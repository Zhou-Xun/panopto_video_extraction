1
00:00:14,260 --> 00:00:26,080
My gosh, if anything.

2
00:00:26,280 --> 00:01:01,960
I mean. Yeah, yeah, yeah, yeah, that's true.

3
00:01:02,070 --> 00:01:05,760
Yeah. Because of money. Yeah.

4
00:01:30,830 --> 00:02:14,820
I think it's because, you know, that's also the second question.

5
00:02:20,430 --> 00:02:27,120
I, I'm open to something I don't I really like to get started.

6
00:02:31,510 --> 00:02:38,320
First of all, I returned the first assignment in Prague as I know it.

7
00:02:38,890 --> 00:02:48,670
This is as sort of the note intentionally in some sense, a difficult assignment, which is why it's so small and sort of I want you to know,

8
00:02:48,670 --> 00:02:57,990
when you engage in the question of did you get an understanding of the difference between the constituents, the answer is no one.

9
00:02:58,480 --> 00:03:01,840
Let's talk about it now. Two, I'm happy to have it with the business people.

10
00:03:01,870 --> 00:03:07,719
I want to make sure you get it, because the more you understand this idea that we are both when we're communicating,

11
00:03:07,720 --> 00:03:11,080
we're both engaging with people's analytical system,

12
00:03:11,080 --> 00:03:20,500
feeding them facts, statistics, risk factors or stuff like that, and engaging with people's emotional, experiential system.

13
00:03:21,350 --> 00:03:27,000
Is sort of core to the whole idea of this course, like doing, you know, what are our objectives?

14
00:03:27,020 --> 00:03:33,139
We understand spend the rest of the class doing doing this communication, thinking about how we communicate that test result.

15
00:03:33,140 --> 00:03:36,620
How do I communicate a decision to somebody?

16
00:03:36,620 --> 00:03:45,349
How do I communicate a crisis to someone? But in each of those situations, you've got to have an understanding both of the analytical,

17
00:03:45,350 --> 00:03:50,360
the relevant information, but also how is this person's experiential knowledge really playing out in that context?

18
00:03:51,080 --> 00:03:59,590
So. We just pause if you have questions about the assignment, if you have questions about the concept, let's I'm happy to engage with it now.

19
00:04:01,350 --> 00:04:06,840
Otherwise I'm happy to meet one on one with people, start meetings, go through your assignment, whatever it takes.

20
00:04:07,650 --> 00:04:13,140
This is really worth engaging in right now because it's going to help you moving forward for the rest, of course.

21
00:04:14,250 --> 00:04:26,220
So any questions? Yeah, I think I was confused about the difference between inputs and outputs.

22
00:04:28,080 --> 00:04:31,190
And I did go back and I read about inputs and outputs.

23
00:04:31,830 --> 00:04:34,170
I thought I had it right, but I didn't have it right.

24
00:04:34,170 --> 00:04:41,040
And then so like part of my paper, you comment are going to take into account people's experiences,

25
00:04:41,040 --> 00:04:46,620
maybe like have food poisoning before or something like that. But I thought that would be considered an output.

26
00:04:47,460 --> 00:04:48,700
Well, so.

27
00:04:53,060 --> 00:05:02,990
I always struggle with how to describe the language here in a way that's clear because things can be both in the sense that you take a simple example.

28
00:05:03,680 --> 00:05:08,810
If I tell you this is relevant to today's conversation, today's reading.

29
00:05:09,260 --> 00:05:12,710
If I tell you that you are at high risk of the X.

30
00:05:14,840 --> 00:05:22,910
That is an analytical import in the sense that you're asking your analytical brain to understand what that category means to.

31
00:05:24,690 --> 00:05:28,230
Think about it. Okay. That means that you have a higher risk than other people do.

32
00:05:28,260 --> 00:05:32,470
How high is that? Whatever. It may also generate fear.

33
00:05:34,610 --> 00:05:37,220
You're going to have a real emotional reaction to that information.

34
00:05:38,420 --> 00:05:49,910
But that's giving somebody a result that says you are at high risk is a very different thing than as several people commented about today.

35
00:05:50,330 --> 00:05:59,420
You know, having the experience of watching someone you love contract that disease, you're not necessarily consciously evaluating.

36
00:05:59,480 --> 00:06:02,850
What does that mean for my probabilities. You are just affected by.

37
00:06:04,610 --> 00:06:10,610
And that experience of that shooting happening or not happening is more part of the experience.

38
00:06:14,340 --> 00:06:20,780
But the challenge of. You know, you had food poisoning.

39
00:06:21,530 --> 00:06:30,150
Let's say. That's an output in one sense of the choice to eat that food.

40
00:06:31,370 --> 00:06:34,700
What is an input into your risk perceptions from that point forward?

41
00:06:34,910 --> 00:06:45,260
You either had that experience of getting six or you didn't do it the rare meat or undercooked egg.

42
00:06:45,800 --> 00:06:52,570
And you know something happened to you or didn't. And it's that. Question of what happened to you or didn't happen to you.

43
00:06:52,580 --> 00:06:59,420
That is what the experiential system is processing as its input into everything that you're going to think about moving forward.

44
00:06:59,970 --> 00:07:11,930
If we think evolutionarily about where that comes from, but this is the same process that the wildebeest has when it goes to the watering hole.

45
00:07:12,350 --> 00:07:15,290
And there is either a predator there or there isn't a predator there.

46
00:07:16,370 --> 00:07:20,300
And how it feeds back in terms of whether they go back to that watering hole or they go someplace else.

47
00:07:21,930 --> 00:07:30,570
Not high analytical human thinking. It's much more fundamental learning the way every living being learns.

48
00:07:31,050 --> 00:07:33,780
It does something. Something happens or it doesn't happen and then changes.

49
00:07:35,720 --> 00:07:43,700
So that's the distinction I'm trying to drive out here because we have to pay attention to our experiences just as much as we

50
00:07:43,700 --> 00:07:50,840
pay attention to the kind of analytical inputs that we learn in an academic environment or in a patient handout or whatever.

51
00:07:51,880 --> 00:07:57,350
I wouldn't consider that projection of my experience to be an output.

52
00:08:01,100 --> 00:08:02,690
If you are. So this is again,

53
00:08:03,170 --> 00:08:13,070
the distinction between input and output is complicated because you can have experiential learning that you then consciously think about.

54
00:08:14,570 --> 00:08:16,190
And if you start to consciously think about it,

55
00:08:16,190 --> 00:08:21,820
then all of a sudden you're taking that experience and you're making it and it's something that you're analyzing consciously.

56
00:08:22,730 --> 00:08:27,650
So some people, for example, in their musing about the food poisoning and their assignment about the food poisoning,

57
00:08:28,250 --> 00:08:36,320
talked about, okay, I went to this restaurant then I'm not sick and.

58
00:08:37,510 --> 00:08:48,970
Then decided because it looked bad that this wasn't a safe restaurant, that kinds of place where that kind of food isn't something that I would want.

59
00:08:49,360 --> 00:08:51,160
And so it made some very conscious choices.

60
00:08:51,500 --> 00:08:57,040
Well, now we're sort of mixing and matching here because you have the unconscious learning of how I did this thing.

61
00:08:57,040 --> 00:09:03,550
And then it turned out that you're also sort of analyzing it from, okay, what was the risk factor was that I didn't cook it much.

62
00:09:03,760 --> 00:09:11,979
Was it this is a lousy dish. And as soon as you shift from that unconscious learning into trying to analyze what

63
00:09:11,980 --> 00:09:15,700
is the risk factor that you're going to consciously be aware of in the future?

64
00:09:16,690 --> 00:09:23,870
Now you're engaging your analytical skills. This.

65
00:09:26,350 --> 00:09:30,760
The reason I'm emphasizing this is not so much because there's a clean line.

66
00:09:30,970 --> 00:09:34,950
This is always analytical and this is only the differential. Is that the.

67
00:09:36,950 --> 00:09:41,870
Experience I have, particularly in public health, although also in medicine to a substantial degree,

68
00:09:42,770 --> 00:09:49,820
is that we as health professionals tend to just simply forget about anything having to do with experience.

69
00:09:51,130 --> 00:09:55,570
We tend to think that all of our job is just throwing facts and risk factor information

70
00:09:55,570 --> 00:10:02,570
and words that people and then our job is done because given the analytical system,

71
00:10:02,590 --> 00:10:10,950
everything you can possibly need to understand this risk. We engage in the process of communication designed with that in mind.

72
00:10:12,470 --> 00:10:19,640
We're going to be unfortunately surprised when people say, yeah, I heard you and I don't care and I'm not going to do that.

73
00:10:20,080 --> 00:10:34,280
Like, we'll get a COVID vaccine or freak out when their experience doesn't seem to align with the words and the facts and the

74
00:10:34,280 --> 00:10:43,339
statistics that we're giving that experience of having me give people all the number and they still went ahead and did the thing.

75
00:10:43,340 --> 00:10:48,760
Whatever it is that put them at risk is at the heart of a huge amount of public health communication problem.

76
00:10:49,790 --> 00:10:51,020
And this is why.

77
00:10:52,090 --> 00:11:00,310
Because if we put experience or emotional learning in contrast with analytical learning, the thing that usually wins is the experience.

78
00:11:03,010 --> 00:11:06,610
You don't anticipate that? That's what I want us to practice.

79
00:11:07,240 --> 00:11:12,890
This is about practice. You may have read your thing and you're like, Yeah, but what about this?

80
00:11:12,940 --> 00:11:16,560
And if you want to engage with me on that, that's totally fine. This is a this is an art.

81
00:11:17,370 --> 00:11:21,390
It's not like there's a clean checkbox. Of course, this is always analytical.

82
00:11:21,460 --> 00:11:30,870
Well, I take that that if you say to me, I'm experiencing this statistic, I'm going to have a little bit of let's talk about how that's happening.

83
00:11:31,440 --> 00:11:37,710
But but even then, there is a concept underneath that in the sense that.

84
00:11:40,680 --> 00:11:44,870
I'm. Driving.

85
00:11:45,650 --> 00:11:49,820
I'm driving, I'm driving. And each day I drive and each day I go through the same route.

86
00:11:49,820 --> 00:11:53,580
And sometimes that route has lots of traffic on it, sometimes it doesn't.

87
00:11:54,620 --> 00:11:58,670
And sometimes I'm experiencing the traffic statistics.

88
00:11:59,730 --> 00:12:02,010
There is such a thing as experiential learning in that sense,

89
00:12:02,010 --> 00:12:07,800
where we come to an understanding of how likely something is, by how frequently it appears in our experience,

90
00:12:08,610 --> 00:12:15,090
and we build up a statistical understanding of, yeah, you know what, I can't drive past this way at 5:00 because it's going to have,

91
00:12:15,270 --> 00:12:18,690
you know, the backup is going to be yearlong, but I can be there for.

92
00:12:21,880 --> 00:12:25,300
That's one that's a translation of experience into statistics.

93
00:12:28,860 --> 00:12:35,490
Other thoughts or questions. Again, I'm more than happy to talk about this because it's going to pay dividends for the rest of this course.

94
00:12:42,420 --> 00:12:48,330
Okay. Somebody, it's not a great place to put the technical terms with the examples that you're give.

95
00:12:49,710 --> 00:12:54,690
Sure. So I'm just going to rattle off a whole bunch of different terms.

96
00:12:55,470 --> 00:12:59,640
You mean different types of inputs? Yeah. Like put. Put in the names of the things.

97
00:12:59,950 --> 00:13:05,249
Yep. So statistics is straightforward.

98
00:13:05,250 --> 00:13:08,970
Like X percent of people have this or case counts.

99
00:13:09,390 --> 00:13:15,600
We can be thinking about, you know, there were 2000 cases of food poisoning in Washtenaw County in the last month.

100
00:13:15,840 --> 00:13:21,770
Those kinds of things. Factual knowledge.

101
00:13:21,840 --> 00:13:27,660
So I would put things in that like, you know, many cases of food poisoning are caused by.

102
00:13:28,790 --> 00:13:32,450
Bacteria like salmonella or E.coli.

103
00:13:32,780 --> 00:13:40,000
And we have actual scientific knowledge about these things and they exist in the environment and we can kill governments if they grow.

104
00:13:40,010 --> 00:13:43,680
They cause things that we may have factual knowledge about, you know?

105
00:13:45,350 --> 00:13:49,940
What is or is not safe food handling. So that gets into like instructions.

106
00:13:50,750 --> 00:13:54,830
I know that certain things need to be refrigerated. Other things don't need to be refrigerated.

107
00:13:56,120 --> 00:14:04,250
I understand the concept of shelf stable, like, okay, this this salsa can sit on a shelf in a jar, but if once I open it,

108
00:14:04,550 --> 00:14:13,250
if I get contaminated and then for bad, etc. those are analytical facts and knowledge about how the world works.

109
00:14:13,250 --> 00:14:22,250
What is or is not safe, what might be safe. Another term which we will use a lot in this course is risk factor.

110
00:14:22,880 --> 00:14:30,380
And what I mean by that broadly. Again, I'm over generalizing the term, but what I mean by that is some thing,

111
00:14:30,560 --> 00:14:40,700
some characteristics that distinguishes people or products or whatever that are more

112
00:14:40,700 --> 00:14:45,200
versus less risky or more versus less likely to have bad things happen to them.

113
00:14:46,130 --> 00:14:54,050
This includes and many of you wrote about things like people who are immunocompromised

114
00:14:54,650 --> 00:15:01,130
or older and you have more susceptible if they look one kind of risk factor.

115
00:15:04,000 --> 00:15:16,540
There may be characteristics of some types of food may be more or less susceptible to

116
00:15:16,540 --> 00:15:20,979
being contaminated and not coming up with a great example off the top of my head.

117
00:15:20,980 --> 00:15:28,240
But the idea being here, being that there's you're sorting things into more versus less risky parts.

118
00:15:29,050 --> 00:15:33,730
Anything that falls into that is knowledge that can you can use to help you decide.

119
00:15:34,600 --> 00:15:37,870
Are you at risk? Is this food safety or not?

120
00:15:40,990 --> 00:15:43,810
I'll shift out of food for a moment. It is about driving risk.

121
00:15:44,290 --> 00:15:54,130
Whatever is a risk factor, it's safer to drive when it's dry and sunny than it is to drive when it's icy or wet.

122
00:16:03,490 --> 00:16:07,090
That's a pretty broad range of the terms that were put in the analytical pile.

123
00:16:08,180 --> 00:16:14,660
On the experience. And for the most part, this is more straightforward in the sense of there's experience.

124
00:16:14,870 --> 00:16:19,909
There's events. Bad things happen or there's nonevent.

125
00:16:19,910 --> 00:16:26,209
Bad things don't happen. And those can happen to you or they could happen to people who are close to you,

126
00:16:26,210 --> 00:16:31,400
or they can happen to people who can you see as they can happen to people who you see on the media.

127
00:16:34,990 --> 00:16:38,740
All of which are just variants of the same experience.

128
00:16:38,810 --> 00:16:49,000
It's closer to you or further away. I would say the one other thing that tends to drop into those two things extend a pop into the experiential space.

129
00:16:50,200 --> 00:16:58,300
Some of the you mentioned either directly or indirectly, what I call sort of cultural norms or behavioral norms.

130
00:17:00,310 --> 00:17:07,600
What I mean by that is more sort of the unconscious associations that we have about what's normal and what's not.

131
00:17:11,150 --> 00:17:23,270
In this society right now, it is normal to consume raw fish in a variety of ways, in sushi, in other kinds of countries.

132
00:17:23,660 --> 00:17:34,670
We see it everywhere. It is normalized. It is much less normalized to consume that.

133
00:17:36,250 --> 00:17:45,230
But there certainly are cultures around the globe in which all our food and so that familiarity or

134
00:17:45,920 --> 00:17:51,000
that's a thing that we need or that's the thing that we don't eat kind of thing is what I'm getting at.

135
00:17:51,140 --> 00:17:56,450
And it's how do you do it from the experiences, what you grew up on, what you see in the restaurants?

136
00:17:57,020 --> 00:18:03,889
What's normalized? Oh, that's, that's again, it's not conscious.

137
00:18:03,890 --> 00:18:11,060
Like if you're, if you raise this to consciousness and you say this is a thing that is okay to eat, all of a sudden you're in the analytical space.

138
00:18:11,840 --> 00:18:16,250
But if we're just talking about something, serves that to you.

139
00:18:16,850 --> 00:18:22,010
You can go, you eat this or you go, Well, of course we eat this.

140
00:18:23,340 --> 00:18:30,620
Unconsciously. That's that's what I'm getting at. And by the way, that applies to all different other kinds of risks.

141
00:18:31,430 --> 00:18:36,860
Like I'll shift back to driving for a second.

142
00:18:39,350 --> 00:18:44,750
Right now in the United States, most highways have speed limits of 70 miles per hour.

143
00:18:46,880 --> 00:18:51,860
When I was growing up, most highways had speed limits of 65 miles an hour.

144
00:18:52,970 --> 00:18:56,050
I remember when the national speed limit changed.

145
00:18:56,720 --> 00:19:02,030
People started driving 70 or 70 rather than 55 or 60.

146
00:19:02,630 --> 00:19:09,000
And it felt near. It just wasn't an analytic.

147
00:19:09,210 --> 00:19:15,360
It was just like, Oh, you don't go that fast. Like, the road shouldn't be moving past me that fast because that's not what we do.

148
00:19:17,210 --> 00:19:24,140
And it took time. Cumulative exposure to driving at that speed before.

149
00:19:25,960 --> 00:19:33,720
I got used to it. And now I notice, as if for whatever reason, I'm driving 55 because of the traffic.

150
00:19:33,730 --> 00:19:38,650
It feels slow to me. That's not a conscious thing.

151
00:19:38,660 --> 00:19:47,810
It's an unconscious normalization of this space in this time, how fast should the trees be going past me kind of thing?

152
00:19:48,810 --> 00:19:51,020
That's experiential. Learning about what is sort of.

153
00:19:52,530 --> 00:19:57,420
Calibration we have of speed and risk and what is okay and what is not okay to be doing in a given situation.

154
00:19:59,170 --> 00:20:09,150
That's the kind of experiential sort of norming that I'm talking about that help you with the problem.

155
00:20:09,390 --> 00:20:13,200
Let me just say one thing. Let me get it right.

156
00:20:13,620 --> 00:20:18,990
This particular event is not the critical thing. This is me trying to teach you a skill.

157
00:20:20,260 --> 00:20:27,320
I'm not sure about food that much we care about is can you look at any risk?

158
00:20:29,250 --> 00:20:36,960
And go through that exercise. And if you're going, Hey, I'm not sure I gotta pick something, anything else?

159
00:20:38,550 --> 00:20:42,130
How does it feel driving chick flu?

160
00:20:42,240 --> 00:20:46,110
Doesn't matter. Do not exercise on your head. Start feeling it.

161
00:20:46,110 --> 00:20:55,590
Because that's the exercise that will help you in your career. You'll be writing about communicating about some other risk, then undercooked food.

162
00:20:57,430 --> 00:21:03,459
But if you can go through that at that exercise and quickly come up with what are the facts and what are the statistics

163
00:21:03,460 --> 00:21:11,110
and what are the risk factors and what are the experiences and what are the just all of those things for a new risk,

164
00:21:11,650 --> 00:21:18,740
that skill will help you. That's what I want you to practice more than anything.

165
00:21:18,860 --> 00:21:24,610
That's what we will really benefit you down the road. Yeah.

166
00:21:24,650 --> 00:21:29,270
So I don't mean to be annoying, but the analytical, experiential thing makes total sense.

167
00:21:29,510 --> 00:21:35,530
The inputs and outputs are still confused, but I know the inputs like you are at risk for x, right?

168
00:21:35,900 --> 00:21:44,990
Yeah. But what is an output then? Like. So an output could be anything from an emotion.

169
00:21:45,650 --> 00:21:49,280
I feel safe or I feel at risk.

170
00:21:50,570 --> 00:21:52,610
There are things that generated that emotion to me,

171
00:21:52,610 --> 00:22:01,300
but the end point of I feel like it is safe for me to eat this food or I feel like this is not a thing I want to be trying to do,

172
00:22:01,760 --> 00:22:04,170
which applies to motivation and behavior.

173
00:22:04,220 --> 00:22:10,640
Like so there's a connection here between the degree to which you feel a degree of safety or risk and what you

174
00:22:10,640 --> 00:22:18,440
choose to do or not do either in terms of engaging in the risky behavior or in terms of gauging prevention.

175
00:22:18,950 --> 00:22:22,819
So. We'll talk about this later.

176
00:22:22,820 --> 00:22:26,870
But we all have some degree of.

177
00:22:29,170 --> 00:22:38,460
Misperception related to COVID. That hopefully motivates us to be in preventive behaviors, whether that's.

178
00:22:40,230 --> 00:22:46,500
They seem more worried, more approximation or even just texting like all of those are behaviors which we

179
00:22:46,500 --> 00:22:51,120
might choose to engage in or not based upon our underlying perceptions of race.

180
00:22:51,750 --> 00:22:55,680
I'm not going to bother to do X if I don't feel like it's worth it.

181
00:22:57,570 --> 00:23:04,970
The standard line for the vaccine, if I feel it's super important. So those are.

182
00:23:08,900 --> 00:23:17,990
Acceptance of that process. But as communicators, we control as the input we control is the fact we provide people.

183
00:23:19,640 --> 00:23:26,830
The. At least acknowledgment of the experiences of people and engagement with those.

184
00:23:28,420 --> 00:23:30,549
You've got to be able to control what people's experiences are,

185
00:23:30,550 --> 00:23:37,910
but we need to certainly understand how their experiences are leading into what they're feeling in that moment.

186
00:23:37,990 --> 00:23:47,290
So we also need to experience what it's like in a relationship with like a bad girlfriend and what would that be?

187
00:23:48,040 --> 00:23:51,699
I'm trying to think how we can frame input, not just numbers,

188
00:23:51,700 --> 00:23:59,360
and you're at risk for a disease that this person could being a bad girlfriend or boyfriend and like.

189
00:24:00,710 --> 00:24:06,910
Giving them really visceral experiences with. But could that be like an experiential input?

190
00:24:07,120 --> 00:24:13,630
It can be. I'm trying to think of a better sort of a cleaner example of the way in which we give people experiences.

191
00:24:16,660 --> 00:24:20,860
Usually we don't control experiences, but we can raise experiences to consciousness.

192
00:24:22,070 --> 00:24:40,340
So to take a simple example, the person who says to me, I know my grandfather smoked for 30 years, you know, it's lung cancer.

193
00:24:42,070 --> 00:24:43,550
So why should I worry about?

194
00:24:46,240 --> 00:24:55,960
They're describing experiential learning like they are seeing their grandfather's experience and perceiving a sense of safety from smoking risk.

195
00:24:56,680 --> 00:25:02,740
Coming out of that experience. And my response to that is that's true.

196
00:25:03,350 --> 00:25:10,510
There are a whole heck of a lot of other people's grandfathers out there. We know from the statistics that unfortunately very different outcomes.

197
00:25:11,520 --> 00:25:17,700
And I can't dismiss that person's grandfather's experience because it's true.

198
00:25:18,300 --> 00:25:23,580
Their grandfather did smoke, did have no bad things come from it.

199
00:25:24,000 --> 00:25:35,229
And that is affecting their perception of safety. But I can knowledge it and then also engage with them without a lot of reasons why we

200
00:25:35,230 --> 00:25:37,930
can't trust that their experience is going to be the same as their grandfather's.

201
00:25:39,440 --> 00:25:44,180
Let the statistics show that lots of people have this bad stuff happen to them.

202
00:25:46,720 --> 00:25:50,380
But of a person experiencing something like catching cold they can have a

203
00:25:50,920 --> 00:25:54,879
family member is now being more afraid of it or preceding their risky behavior.

204
00:25:54,880 --> 00:25:58,600
We can't control we can't control it, but sometimes we can share experiences.

205
00:25:58,600 --> 00:26:02,740
So I can't control what can happen to you. I can't necessarily control what's going to happen to your family.

206
00:26:03,010 --> 00:26:07,470
I can give you stories of people who did something.

207
00:26:07,480 --> 00:26:16,210
So for example. There have been lots and lots of different COVID 19 messaging over the last two years.

208
00:26:16,570 --> 00:26:27,330
I have seen patients stories of people who got COVID saved on TV and radio in the center and how awful it was.

209
00:26:27,340 --> 00:26:32,800
This is what happened to me. This is the you know, I was in the hospital for a month kind of thing.

210
00:26:33,160 --> 00:26:44,680
Please go get vaccinated. Don't let this happen to you. Kind of stuff like those kinds of story messages are experiential learning by curious.

211
00:26:46,720 --> 00:26:49,480
That other person went through this awful experience.

212
00:26:49,480 --> 00:26:57,730
And because they're sharing their experience, we get to see a lot of public health messaging that is basically that kind of stuff.

213
00:26:59,380 --> 00:27:01,420
And sometimes it's about the bad stuff.

214
00:27:01,430 --> 00:27:09,340
So this is the example of the person who got COVID and how bad it was and how much they regret not being vaccinated, that kind of stuff.

215
00:27:10,370 --> 00:27:14,390
And sometimes it's the gravity. Enough of this, but sometimes it's the.

216
00:27:17,930 --> 00:27:28,909
You know, it's the mom who's talking about how relieved she is that her kids are fully vaccinated because there's been an outbreak of stuff at school.

217
00:27:28,910 --> 00:27:32,840
And she feels much more confident because now that she knows that her kids are protected.

218
00:27:33,480 --> 00:27:37,300
Kind of stuff. Still an individual experience.

219
00:27:38,950 --> 00:27:46,899
Now, a lot depends upon whether you, as you are watching this story, look at that person and say, yes, she is like me.

220
00:27:46,900 --> 00:27:51,160
I want to be like her like this. How much of a connection you have to that story?

221
00:27:52,690 --> 00:28:00,940
But that is experiential learning as a communication for giving people an another person's experience as opposed to.

222
00:28:02,150 --> 00:28:06,470
The scientists standing up and saying the vaccine is effective, it will protect you.

223
00:28:06,680 --> 00:28:10,700
You have the person saying, you know, I really wish I have the vaccine.

224
00:28:11,940 --> 00:28:17,720
So that is implied. All right.

225
00:28:18,590 --> 00:28:24,620
Let me bookmark this. We can continue to come back to this over and over again because it's really valuable.

226
00:28:25,580 --> 00:28:29,480
But it's a good point to sort of move forward into terms of talking about today,

227
00:28:29,840 --> 00:28:40,340
because today is all about labels like why do we label when do we label the numbers, the data, the situations?

228
00:28:41,680 --> 00:28:47,350
People as being normal or abnormal or high risk or low risk or whatever.

229
00:28:50,980 --> 00:28:54,070
All of which is leading to the question of the day, which is okay.

230
00:28:54,430 --> 00:28:58,959
We can give labels to say, When is doing that a good thing?

231
00:28:58,960 --> 00:29:02,410
And What is doing that a bad thing? The answer is going to be both.

232
00:29:02,440 --> 00:29:06,009
Like, there are some times when it's good and there are some times when it's bad.

233
00:29:06,010 --> 00:29:11,070
And the question is when. But today I want what I want to spend the rest of today talking about is,

234
00:29:11,460 --> 00:29:15,600
okay, what are some examples where labels were either helpful or not helpful?

235
00:29:16,710 --> 00:29:30,480
And to start, I will step out of the health space and hear you were talking about exam grades as a place in which the number wasn't enough.

236
00:29:32,070 --> 00:29:35,460
So let's start there. Yeah, so I kind of gave two examples.

237
00:29:35,700 --> 00:29:41,580
My first one was intro by freshman year, getting a 76 never got a grade that low before,

238
00:29:42,300 --> 00:29:47,910
but when the class average is a 73 and all my friends who came from good high schools and took

239
00:29:47,910 --> 00:29:57,010
AP bio and they also got 78 so that and my second example was American Gulf freshman year.

240
00:29:57,010 --> 00:30:03,239
I also got to be in the class. I was pretty upset until I talked to the professor and he was like, Yeah,

241
00:30:03,240 --> 00:30:07,830
you did great in my class like multiple about BS, but hey again, I didn't do so bad.

242
00:30:08,610 --> 00:30:15,090
So it was I needed the context to actually. See where I kind of stood compared to everyone else.

243
00:30:16,140 --> 00:30:19,170
Amber I remember you were also thinking about the same kind of thing.

244
00:30:20,160 --> 00:30:22,620
Do you have any other examples to bring in? Because I want to comment,

245
00:30:22,620 --> 00:30:31,529
but after both you guys have gone through similar subject previous or just having that class average

246
00:30:31,530 --> 00:30:36,390
can make you feel a little bit better or a little bit worse for even if you did really well.

247
00:30:36,930 --> 00:30:44,489
Like even if your overall standing in goal in any course is to get a B or better and you get a B,

248
00:30:44,490 --> 00:30:51,240
but everyone else got an A, even if you're always happy with the B any other time everyone else gets a name.

249
00:30:51,240 --> 00:30:54,570
Now you feel bad about something even more so.

250
00:30:55,140 --> 00:31:04,680
Yeah. So this is a great non of a process that's super important to help notice that we have multiple different.

251
00:31:05,590 --> 00:31:08,770
Things that we're using to decide whether the number is good or bad.

252
00:31:09,580 --> 00:31:13,330
Actually, it started off with got to 76, I think it was.

253
00:31:13,690 --> 00:31:18,999
Okay. What is before we do anything else? There is reference information in our minds.

254
00:31:19,000 --> 00:31:23,920
It's already coming up. We already know that rate is a 0 to 100.

255
00:31:25,180 --> 00:31:34,870
The hundred is best. And we also have in the back of our minds a classification system we commonly use, at least in this country,

256
00:31:35,830 --> 00:31:46,480
that associates ninety's or as it is or piece seventies are seas, etc. and we have an association of what is a good enough.

257
00:31:47,740 --> 00:32:01,710
Well. So now with anything else you're going 76 drops you into CDC is not the category that you feel as comfortable with as your goals.

258
00:32:02,310 --> 00:32:11,270
So that gives you a negative reaction. By the way, let's notice that if the number categorization scheme was different,

259
00:32:11,270 --> 00:32:16,340
like if we were talking about green scores, if we were talking about something else, it's not on a 0 to 100 scale.

260
00:32:16,970 --> 00:32:19,640
All of a sudden about 70%. You don't have the same knowledge.

261
00:32:21,190 --> 00:32:26,870
But it matters what prior knowledge we have about the range, about the categorizations, etc.

262
00:32:28,220 --> 00:32:33,650
You your first reaction was you had the interpretation based upon.

263
00:32:36,590 --> 00:32:44,040
And then you added that extra label of, Oh, wait, we've got the class average and now your above average,

264
00:32:44,550 --> 00:32:49,520
above average is good at least or close enough to the average if you're not feeling that great.

265
00:32:51,080 --> 00:32:51,890
Change the number here.

266
00:32:53,120 --> 00:32:58,450
We do things with context and I'll come back to this not next class, but the following class will spend a lot of time talking about.

267
00:33:00,170 --> 00:33:03,200
But to foreshadow. I can take any number.

268
00:33:04,400 --> 00:33:09,770
And make you relieved by it or afraid of it without changes, just by changing the context.

269
00:33:10,430 --> 00:33:17,329
That is the power that we have to contextualize numbers. And so we need to think about, okay, what is the implications of using that?

270
00:33:17,330 --> 00:33:26,330
And today is the introduction to that. So the averages are powerful, but notice also and Amber brought this up, this is important.

271
00:33:26,930 --> 00:33:33,110
Sometimes objectively, what we care about is how we are doing relative to others.

272
00:33:34,250 --> 00:33:37,310
But while we're doing relatively well, we should be fine.

273
00:33:38,030 --> 00:33:42,319
And sometimes I don't care what anybody else is doing.

274
00:33:42,320 --> 00:33:45,830
I care about where I sit compared to some absolute standard.

275
00:33:47,800 --> 00:33:56,270
You take a concrete and unfortunate example. Think about BMI.

276
00:34:01,760 --> 00:34:07,010
Right now, whatever your BMI is or whatever a person's BMI is,

277
00:34:08,450 --> 00:34:16,250
the average BMI in the United States is considerably higher than it was multiple decades ago.

278
00:34:17,330 --> 00:34:21,140
We are in a more obese, overweight society than we were.

279
00:34:22,620 --> 00:34:29,550
So you ought to be communicating to people about where they fall compared to the average BMI in the United States right now.

280
00:34:30,120 --> 00:34:34,800
I get nervous about that. I don't I don't like to care where you fall in terms of the average.

281
00:34:34,830 --> 00:34:41,310
I want everybody to be shifted. I want the whole distribution to be shifting towards a healthier weight level.

282
00:34:43,460 --> 00:34:53,960
But if if we like having average sometimes we're going to give people the average statistics say we do in growth charts

283
00:34:53,960 --> 00:34:59,960
for kids like track where you are in terms of what percentile you are in terms of your high doorway that you're growing,

284
00:34:59,960 --> 00:35:04,280
etc. So now the question becomes, do we care about the relative statistic or the absolute?

285
00:35:06,320 --> 00:35:15,510
It's going to matter how we're going to react to. So this is the key theme.

286
00:35:15,740 --> 00:35:19,950
It's not in the context. We never want to give people.

287
00:35:20,160 --> 00:35:22,360
Well, I say never. This is the problem.

288
00:35:22,380 --> 00:35:28,800
I don't want to say never about anything, because whether we give context or not is going to change what people do.

289
00:35:28,800 --> 00:35:31,800
And sometimes that's going to be a positive way and sometimes it's going to be a negative one.

290
00:35:34,120 --> 00:35:37,690
So the next example I want to bring up City.

291
00:35:40,560 --> 00:35:46,140
Do you like eating right? I'm sorry. You're talking about your mom's experience.

292
00:35:47,640 --> 00:35:52,770
Talk about how she came to that experience and then how it played out over time.

293
00:35:52,780 --> 00:35:56,040
Because it's a good example of how to categorize yourself.

294
00:35:56,040 --> 00:35:58,940
Changes what you think is okay to do or not. Yeah.

295
00:35:58,950 --> 00:36:05,520
So my mom was diagnosed with breast cancer, which was 40 and because that's like a really young age to have breast cancer,

296
00:36:06,000 --> 00:36:12,440
it indicates like me and my sister that we are at like a higher risk of developing breast cancer in our lives.

297
00:36:12,450 --> 00:36:17,670
And like I've been told that like my entire life, like my mom stressing out, like my doctor stressing it.

298
00:36:18,540 --> 00:36:21,299
So I'm like, I have a high risk of breast cancer.

299
00:36:21,300 --> 00:36:26,490
And it's like sometimes in my life, maybe kind of like catastrophize like a little bit cause I've been told it so many times.

300
00:36:27,180 --> 00:36:31,800
And then my mom eventually got bracket testing and she tested negative for the practice.

301
00:36:32,340 --> 00:36:40,680
So then that caused my doctors to tell me that, like, my risk isn't as high of risk, but it's still high risk of developing it.

302
00:36:41,040 --> 00:36:49,559
And it's kind of like made the more confused getting older because like, I'm at a higher risk, but I'm not at as high of a higher risk.

303
00:36:49,560 --> 00:36:54,510
So I don't understand how much of a high risk I'm at compared to the general population.

304
00:36:55,350 --> 00:36:59,760
Notice this entire conversation is taking place non quantitative because of this.

305
00:36:59,790 --> 00:37:04,019
Could play this out in number form. And again, I'm not going to say I know what the numbers are,

306
00:37:04,020 --> 00:37:12,749
but let's imagine and I will use some just arbitrary numbers here that the average person's age at risk for

307
00:37:12,750 --> 00:37:20,880
someone over the next 20 years I'll make these people number is going to be if you can fold around number 5%.

308
00:37:22,180 --> 00:37:27,220
And before you know, whether your mom had the BRCA gene, you know,

309
00:37:28,810 --> 00:37:36,170
that knowledge that she had developed breast cancer at age 40 put you at 10% and now that United don't have BRCA.

310
00:37:37,520 --> 00:37:39,160
And so we call it seven.

311
00:37:41,570 --> 00:37:48,140
You get a little bit more precision here because the number is telling you, okay, I'm a closer to this and I'm closer to that.

312
00:37:50,270 --> 00:37:55,970
We use verbal terms. We step away from that. We said we keep meaning.

313
00:37:57,090 --> 00:38:01,590
But we don't necessarily keep precision. So exercise.

314
00:38:04,260 --> 00:38:10,320
I'm going to put a couple of terms up here and leave this one. I'm not talking.

315
00:38:11,280 --> 00:38:16,660
I want you. To jot down these paper on a computer.

316
00:38:16,670 --> 00:38:22,580
Whatever. What number? What percentage number comes to mind for each of these terms?

317
00:38:27,930 --> 00:38:39,180
So. That's why we're likely.

318
00:38:50,750 --> 00:39:01,340
Low risk development and extremely rare.

319
00:39:05,630 --> 00:39:09,480
Warmer than good. What's. What's the number? What's the percentage?

320
00:39:10,010 --> 00:39:13,070
The pops to mind. Each of those terms.

321
00:39:33,010 --> 00:39:36,190
All right? Yeah. Don't have to. Everything.

322
00:39:38,540 --> 00:39:42,620
All right. Let's kick like.

323
00:39:44,680 --> 00:39:57,460
Just chat about 75, 75 others 60, 60, 70, 70 greater than 50, greater than 50.

324
00:39:57,790 --> 00:40:07,090
And we all said something 80%, 80 and this year are higher and you have a lower.

325
00:40:08,880 --> 00:40:13,470
68% range and 60 to 80.

326
00:40:20,380 --> 00:40:24,730
I think if he were.

327
00:40:25,960 --> 00:40:29,650
Really. Lot people come.

328
00:40:30,060 --> 00:40:33,630
I know you guys got them, and I know they're different than these 89.

329
00:40:34,140 --> 00:40:37,340
Yeah. All right.

330
00:40:38,730 --> 00:40:42,980
1990. Somebody's got to be less.

331
00:40:43,120 --> 00:40:48,210
53. 30. There's some.

332
00:40:52,290 --> 00:40:57,260
Anyway. Laura, 30. We just pause for a second.

333
00:40:58,850 --> 00:41:06,320
Let's say you're taking a prescription medication that says that one of those a common side effect is that you get headaches.

334
00:41:06,800 --> 00:41:10,940
Are you actually assuming that the chance are going to get headaches is 80% when it says that?

335
00:41:13,970 --> 00:41:18,620
No. In fact, I suspect you wouldn't be surprised if if the statistic on that was ten.

336
00:41:20,060 --> 00:41:24,820
But only 10% of those people were actually going to be experiencing headaches as a result of taking that medication.

337
00:41:25,570 --> 00:41:30,790
So words like Common are particularly susceptible to the context.

338
00:41:33,980 --> 00:41:41,450
Common side effects are already suppressed, like we're talking ten 20%, mostly because we're talking about the side effects as a small thing.

339
00:41:41,450 --> 00:41:44,360
And not everybody gets side effects or we wouldn't call them side effects.

340
00:41:47,840 --> 00:41:52,310
But if we want to talk about, you know, is it common to get a cold in the winter?

341
00:41:52,370 --> 00:42:00,470
Yeah. Okay. 80% might be perfectly normal because as common as it is, I use that word to mean things that are going to happen to most people.

342
00:42:02,390 --> 00:42:09,170
All right. Low risk. And.

343
00:42:11,450 --> 00:42:18,770
1515 1010.

344
00:42:19,700 --> 00:42:24,110
Okay. Others 11.1.1.

345
00:42:27,250 --> 00:42:36,050
Well, that's right. Anybody have 15, 20 guns.

346
00:42:38,320 --> 00:42:50,950
Okay. We just pause here. Low risk within the range we have here, a 200 times range.

347
00:42:52,030 --> 00:42:58,030
All risks are 4.1% to 20% is a multiplier of 200.

348
00:42:59,940 --> 00:43:03,120
And we use the same term for all of those.

349
00:43:08,000 --> 00:43:11,680
Extremely rare. What do you got? One in a million.

350
00:43:12,160 --> 00:43:18,620
One in a million. Right.

351
00:43:19,200 --> 00:43:22,430
Plus 1%. 1%. All right.

352
00:43:22,460 --> 00:43:27,140
There's a million times ratio after 10%.

353
00:43:27,170 --> 00:43:30,170
Okay. So we get to 5,000,000,000%. 5 million ratio.

354
00:43:31,260 --> 00:43:36,350
That's less than one. Yep. Okay.

355
00:43:36,740 --> 00:43:41,930
I think my point's been made. You're all answering answers, which makes total sense.

356
00:43:42,170 --> 00:43:47,510
We use these verbal terms to mean all of these things.

357
00:43:47,510 --> 00:43:54,150
Like none of these are wrong. I can imagine situations in which each one of these responses is absolutely appropriate.

358
00:43:55,460 --> 00:43:59,870
But as communicators, what this means is that what I'm envisioning,

359
00:43:59,870 --> 00:44:05,899
that number that turned to me and what you're envisioning that term to me are not necessarily going to be the same,

360
00:44:05,900 --> 00:44:10,570
and they're not even necessarily going to be close. So.

361
00:44:12,180 --> 00:44:16,560
Now envision you're in a doctor's office and the doctor's talking about.

362
00:44:17,570 --> 00:44:23,899
A surgery, say it's talking about, you know, there are common side effects of that surge,

363
00:44:23,900 --> 00:44:28,670
complications of that surgery, and that there are extremely rare complications of that surgery.

364
00:44:31,070 --> 00:44:40,830
Are you informed? Are you ready to make an informed decision about the level of risk you would take?

365
00:44:43,020 --> 00:44:49,770
If you're shaking your head and saying no. Acknowledge that the reverse here is that the doctor is coming back to you and saying,

366
00:44:50,280 --> 00:44:54,110
okay, you know, there's a 40% chance that this is going to happen,

367
00:44:54,140 --> 00:45:00,120
a 30% chance that this is going to happen, and a 1% chance that this is going to happen in a one in a million chance that this is going to happen.

368
00:45:02,170 --> 00:45:12,129
And you're forcing that person to engage with those numbers and derive the meaning that they need to get from those numbers purely from the numbers.

369
00:45:12,130 --> 00:45:17,890
And we just spent last time talking about numeracy. So you should all be going, Wait a second, that's really hard.

370
00:45:23,150 --> 00:45:30,270
There are times when these terms are useful. I don't want you to assume that just because we've lots of variants here,

371
00:45:30,270 --> 00:45:36,990
that they're not useful, especially when we are relatively bad at being able to be precise.

372
00:45:37,860 --> 00:45:45,839
There are all kinds of complications, things that could happen that I would intentionally use.

373
00:45:45,840 --> 00:45:52,400
Phrasing like extremely rare to describe. Can I calibrate whether it's one in a million or one in a thousand or 1%?

374
00:45:52,410 --> 00:45:58,350
No, I probably can't. What am I trying to communicate? I'm trying to say, yes, this can happen.

375
00:45:59,100 --> 00:46:04,740
No, it's not likely to happen. But be aware.

376
00:46:06,510 --> 00:46:15,569
Yeah. What I was thinking with the example of I think you said like in the general population, but like a 5% risk.

377
00:46:15,570 --> 00:46:19,160
And I'm like, that cannot be everything. I would be like 10% would be higher.

378
00:46:20,280 --> 00:46:27,089
But there's also the other layer of complexity of like we have some models out there to help us get that number,

379
00:46:27,090 --> 00:46:33,300
but they're based on white, European, male. Absolutely. So there are all these other confounding factors, too.

380
00:46:33,600 --> 00:46:41,709
So. Whenever we move to the number space, then there is the question of how precise those numbers are appropriate those numbers are,

381
00:46:41,710 --> 00:46:46,150
how accurate those numbers are, and the confidence roles we might have.

382
00:46:46,930 --> 00:46:52,389
What I try to highlight here is there is no perfect answer, but if I go to the numbers,

383
00:46:52,390 --> 00:46:56,830
then we're stuck artificially thinking they're more precise and more accurate and more appropriate than they are.

384
00:46:58,090 --> 00:47:02,500
I step to these verbal terms. We run the risk of all of this ambiguity.

385
00:47:05,820 --> 00:47:10,320
So how do we pick which one we're going to do? And that's the question I want to wrestle with.

386
00:47:11,010 --> 00:47:17,190
When are you going to be okay with the idea? That doesn't matter whether somebody knows things at 60 versus 80.

387
00:47:18,000 --> 00:47:21,780
There's lots of situations like that. So, for example, I'll use a very concrete one.

388
00:47:23,850 --> 00:47:29,009
If you are a man who has been diagnosed with prostate cancer and you are going to have radical

389
00:47:29,010 --> 00:47:36,329
prostatectomy removal of the prostate surgery is one of the most likely side effects of that.

390
00:47:36,330 --> 00:47:39,330
Surgery is some form of incontinence.

391
00:47:39,390 --> 00:47:44,460
Urinary incontinence wrestled with the urinary tract happened so most people.

392
00:47:45,410 --> 00:47:49,580
Does it really matter whether that number is 60% versus 80%?

393
00:47:50,720 --> 00:47:55,070
Yeah. Honestly, probably not like I want you to know.

394
00:47:55,610 --> 00:48:00,649
Be ready. This is probably if you happen to be one of the lucky ones, it doesn't happen to degrade thumbs up.

395
00:48:00,650 --> 00:48:07,430
But otherwise, assuming the level of precision here probably doesn't affect most people's decision making.

396
00:48:08,910 --> 00:48:17,430
On the other hand. If it really mattered to you whether you're going to, say, take a medication in order to prevent.

397
00:48:18,880 --> 00:48:26,620
Developing a condition if your low risk category is 15% versus 0.1%.

398
00:48:28,000 --> 00:48:33,460
So they are the precision starts like it makes really different decisions in that context.

399
00:48:34,690 --> 00:48:41,380
So the point here is let's be aware of what the implications are of using these terms or not.

400
00:48:41,710 --> 00:48:45,430
Not saying that you always do and not saying you always don't. But be conscious of.

401
00:48:47,180 --> 00:48:51,860
All right. Moving forward?

402
00:48:52,730 --> 00:49:00,850
Well, actually, let me just use this example, Emily. You were talking about your mom also talking about breast cancer risk,

403
00:49:00,860 --> 00:49:06,080
but in particular comparing her experience with a friend of hers experience and their different decisions.

404
00:49:06,230 --> 00:49:16,370
Yeah. So my mom, when she was in college, my grandmother, she got breast cancer and my mom was one of her main caregivers.

405
00:49:16,460 --> 00:49:24,860
Just bookmarking experiential learning. Yeah. And so because of that, she she is at higher risk for breast cancer.

406
00:49:24,860 --> 00:49:28,700
And she went through that experience, was very concerned about it.

407
00:49:29,840 --> 00:49:37,700
And so her and her friends recently were offered similar medications that.

408
00:49:39,330 --> 00:49:49,470
Would increase risk of breast cancer, but would decrease like symptoms like hot flashes and other things associated with menopause.

409
00:49:49,920 --> 00:49:53,700
And so because my mom use herself, she is at high risk.

410
00:49:53,700 --> 00:49:57,750
She's like, absolutely not. I'm not doing that. Not increasing my risk.

411
00:49:57,870 --> 00:50:02,190
And her friends who didn't have that experience didn't have that risk.

412
00:50:03,000 --> 00:50:07,770
So I'll take the medication and reduce my symptoms.

413
00:50:07,830 --> 00:50:12,210
And again, not knowing the truth, the medical truth of the situation,

414
00:50:12,840 --> 00:50:23,670
it could be that if we ran both your mom and her friends through some objective breast cancer risk calculator, they would spit back the same number.

415
00:50:25,640 --> 00:50:29,930
So there is the analytical side of what is what risk factors do they have or not?

416
00:50:30,260 --> 00:50:33,860
How won't we calculate that? And then there is the experience.

417
00:50:34,400 --> 00:50:39,650
What we're seeing here is the experience led your mom to label herself as being high risk again.

418
00:50:40,160 --> 00:50:43,310
Accurate or not is not the question here. She is owning that label.

419
00:50:44,750 --> 00:50:50,180
And because of the emotional attachment of that label, it's changing what she sees as an appropriate trade off to make.

420
00:50:52,270 --> 00:50:56,050
So the act of using a label is an intervention.

421
00:50:57,920 --> 00:51:04,140
You're changing what that person feels about themself. And that can be good or not good.

422
00:51:06,090 --> 00:51:14,940
Well, on this topic, Marc, you brought up an important piece here, which is, for lack of a better term, the Lake Wobegon effect.

423
00:51:14,940 --> 00:51:23,460
For those of you who may be familiar with that, we are an overconfident species.

424
00:51:23,760 --> 00:51:30,570
We tend to think that we're better than average in everything. So how does that play into how we think about this type of stuff?

425
00:51:31,180 --> 00:51:32,890
Yeah, so I guess kind of background.

426
00:51:32,910 --> 00:51:41,129
I just remembered a paper route or something that I had read in undergrad about some studies that have just been written.

427
00:51:41,130 --> 00:51:46,410
A lot of this paper said about 75% of people say that they're a better than average driver,

428
00:51:48,240 --> 00:51:52,740
which doesn't make sense because, you know, people can't be better than 50%.

429
00:51:54,510 --> 00:51:56,669
And then just kind of think of some other examples of the paper.

430
00:51:56,670 --> 00:52:05,380
I just, you know, goes on to explore how, yeah, that kind of framing of being overconfident in the abilities and assessing of risk where,

431
00:52:05,430 --> 00:52:12,690
you know, kind of individualize it to your personal situation and, you know, to making decisions that might not be advisable.

432
00:52:13,560 --> 00:52:16,950
So clearly there is a general pattern of overconfidence in our society.

433
00:52:17,790 --> 00:52:27,510
I will say it is not universal when we look at health because we also see people who are essentially universally pessimistic.

434
00:52:29,730 --> 00:52:35,910
They perceive themselves to be vulnerable. They expect that if they could get sick, they will get sick.

435
00:52:36,750 --> 00:52:40,970
And their perception of or that if there is a side effect, that they will get the side effects, etc.

436
00:52:40,980 --> 00:52:45,570
And so they are consistently framing themselves as.

437
00:52:46,510 --> 00:52:56,579
I'm going to have the worst thing happen. But you're raising an important point, which is when we are communicating in relative terms.

438
00:52:56,580 --> 00:53:03,360
So we talk about above average or below average, the meaning that somebody takes from that depends upon how they think about themselves.

439
00:53:04,580 --> 00:53:07,880
And we don't necessarily have more control over that.

440
00:53:08,150 --> 00:53:14,320
On the other hand, when we use terms like high risk, higher risk is an absolute thing.

441
00:53:14,330 --> 00:53:18,300
We are saying this is bad. Does it matter whether everybody is bad?

442
00:53:18,330 --> 00:53:21,780
Doesn't matter whether you're average. We're just labeling it as bad.

443
00:53:22,800 --> 00:53:26,410
And so there's a question of the choice of kinds of labels that we might want to hear.

444
00:53:27,090 --> 00:53:30,720
Like, when do we want to say you are above average risk?

445
00:53:31,050 --> 00:53:36,420
And when do we want to say you are high risk people because they're not the same?

446
00:53:37,250 --> 00:53:43,530
Again, there's no simple answer to this question, but I want to raise your awareness of the subtleties of language really do matter to you.

447
00:53:45,960 --> 00:53:48,730
All right. We spend a lot of time talking about the average.

448
00:53:48,940 --> 00:53:56,400
I want to spend most of the rest of today's class talking about categories, because there's a lot of ways in which medicine uses categories.

449
00:53:59,870 --> 00:54:08,569
And start. Let me actually start with you.

450
00:54:08,570 --> 00:54:15,660
If we. No, I don't want to go too deep into this particular task, but there's a piece of it that that's a good place to start.

451
00:54:15,670 --> 00:54:22,660
This is some test about metabolism and then it's categorized and that's where I want it to sort of build off.

452
00:54:22,750 --> 00:54:25,240
So if you can give a little bit of background and then I'll go from there.

453
00:54:25,900 --> 00:54:31,270
So it's based on the reading of like genetic testing that kind of feels like the results are positive or negative.

454
00:54:31,780 --> 00:54:38,410
But in pharmacogenomics testing, you're put into a like for like usually for categories of like,

455
00:54:39,070 --> 00:54:43,530
like ultra rapid metabolize or normal metabolize or intermediate or so.

456
00:54:43,570 --> 00:54:46,990
You're so placed into these categories based on like those findings,

457
00:54:46,990 --> 00:54:51,040
but they're like a little bit more descriptive, but it's based on like enzyme activity.

458
00:54:52,330 --> 00:54:58,570
So presumably the enzyme itself is not flashing super metabolism,

459
00:54:58,930 --> 00:55:07,000
but there's some underlying quantitative measure here of like how much of the chemical is left at a given point of time.

460
00:55:09,970 --> 00:55:15,510
Why are there four categories? Fight on ten.

461
00:55:16,850 --> 00:55:23,120
To. Answer because somebody somewhere decided that they wanted to do it that way.

462
00:55:25,490 --> 00:55:28,640
Any time we categorize an underlying quantitative variable,

463
00:55:29,060 --> 00:55:38,240
we are applying meaning to a variable that did not have that mean we are making certain things seem the same.

464
00:55:38,750 --> 00:55:43,880
Anything that falls into the category of like super metabolism is the same.

465
00:55:44,210 --> 00:55:49,940
Even though it may not be at all the same within that range, anything that's in a different label is different.

466
00:55:50,540 --> 00:55:59,820
Even though objectively it might be super close. So as another example of that, Isabella.

467
00:56:00,690 --> 00:56:06,810
Yes, you were talking about, if I remember the right boyfriend, he went to see value.

468
00:56:08,370 --> 00:56:12,899
This is a good example of the problem of taking a number and categorizing it.

469
00:56:12,900 --> 00:56:16,320
So if you want to talk through that story, I want to I want to expand on it.

470
00:56:17,680 --> 00:56:25,350
Yes. So my boyfriend got tested for diabetes or testing is A1 C level and he's a nutritional sciences major.

471
00:56:25,350 --> 00:56:28,200
So obviously he has some context on the numbers and what they mean.

472
00:56:29,190 --> 00:56:34,110
But the doctor told them didn't even explain or get any kind of context behind his value.

473
00:56:35,220 --> 00:56:41,520
I guess he's considered borderline pre-diabetic, but I read a number 5.7%.

474
00:56:42,570 --> 00:56:46,820
KUNC Yes. You remember?

475
00:56:46,940 --> 00:56:51,190
I think you had it in your music. What's the range for borderline? 5.7.

476
00:56:51,270 --> 00:56:54,560
6.4. 5.7.

477
00:56:54,800 --> 00:56:58,060
6.4. All right.

478
00:56:58,280 --> 00:57:04,580
So had the test result comes out at 5.6 he didn't like.

479
00:57:08,020 --> 00:57:14,020
It's just pots 5.6 versus 5.7 shifts you from normal equals.

480
00:57:14,030 --> 00:57:21,249
You're fine, don't worry about it, everything's good etc. to not normal prediabetic.

481
00:57:21,250 --> 00:57:28,900
All of the meaning that might come along with it. But 5.7 to 6.4 is all the same.

482
00:57:31,960 --> 00:57:45,070
No, it's not. We're taking the underlying variability and creating distinction between 5.6 versus 5.7 are fundamentally different.

483
00:57:46,390 --> 00:57:48,790
That's the message of slapping that label on it.

484
00:57:49,870 --> 00:57:57,249
Whereas in this range is what if we think about and I don't remember what the bottom of the normal range is,

485
00:57:57,250 --> 00:58:03,490
it's something like 4.3 or something like that. If the normal range is 4.3 to 5.6.

486
00:58:06,460 --> 00:58:12,370
I actually do want you to notice whether you're in the middle of this range or you're at 526,

487
00:58:13,090 --> 00:58:17,680
that might be important for you to know how you're getting close to the edge of that robot range versus,

488
00:58:17,680 --> 00:58:23,430
oh, you're just completely in the middle of the normal range. But when we slap the label of normal on something.

489
00:58:24,740 --> 00:58:32,120
We do have people away from being able to remember the number to process the number to think about is this close to the edge or not?

490
00:58:32,420 --> 00:58:43,880
What they get is, I'm fine or I'm not fine. So these rating schemes pop up everywhere and pop up everywhere because.

491
00:58:45,340 --> 00:58:50,590
Absent this, the average person has no idea what they should be thinking about.

492
00:58:50,590 --> 00:58:58,330
5.71 C. What happens to the nutritional science is they have some background.

493
00:58:58,360 --> 00:59:04,989
Yeah, good for them. But the average person certainly doesn't have that. So if we don't give them context, they're like 5.7.

494
00:59:04,990 --> 00:59:09,820
What's that? If we do give them context, whatever we give them is what they're going to remember.

495
00:59:12,160 --> 00:59:18,450
So here are a couple other examples, but I'll give you the fun 1/1. True story.

496
00:59:19,680 --> 00:59:24,690
What? You. So I've already talked about the fact I went through infertility treatment,

497
00:59:25,230 --> 00:59:36,600
went to a presentation by a urologist talking about the tests that they use to measure sperm motility degree to which sperm are moving,

498
00:59:36,600 --> 00:59:38,580
which is a key factor in terms of fertility.

499
00:59:39,760 --> 00:59:48,520
And basically the sample, you put it in a jar, you wait for 24 hours and you see how much left, how much is moving after 24 hours.

500
00:59:49,610 --> 01:00:01,160
That's that's the concept. And normally, arbitrarily, samples are classified into one of four categories.

501
01:00:03,340 --> 01:00:07,720
One at most motility four has the least motility.

502
01:00:09,760 --> 01:00:13,950
Many said. So that's the categorization scheme we use now.

503
01:00:15,720 --> 01:00:19,440
We used to change this at the time. This is a long time ago.

504
01:00:20,010 --> 01:00:23,219
We used to use, rather than a number categorization system.

505
01:00:23,220 --> 01:00:28,170
We used to use a lot of categorization systems, but we changed it because it was just too brutal.

506
01:00:28,420 --> 01:00:38,110
I have to tell. And I got a d per. But let's think about that.

507
01:00:39,530 --> 01:00:45,319
The layers of meaning in that you have the use of the categorization schema that

508
01:00:45,320 --> 01:00:51,200
we have in school is ingrained in us in this educational system that G is back.

509
01:00:53,710 --> 01:00:59,440
Napping automatically, unconsciously during the categorization of this medical test,

510
01:01:00,850 --> 01:01:05,020
not just purely in terms of what is the characteristic of this medical test, but of the person.

511
01:01:05,440 --> 01:01:13,240
Because in school we carried with it. Not just you got this bad test, but the meaning of you are not performing well as a student.

512
01:01:14,690 --> 01:01:19,190
And so it didn't matter whether that was categorized as one through four versus eight through.

513
01:01:21,230 --> 01:01:24,860
And that change was made very intentional and could change the emotional state.

514
01:01:25,370 --> 01:01:32,449
But the patients were going to be in when they got there. Results. Another place where letters are used.

515
01:01:32,450 --> 01:01:40,250
McKayla You were talking about USPS tough guidelines, which is incomprehensible even to those of us who have been taught this stuff.

516
01:01:41,090 --> 01:01:50,000
Yeah. So you get stuff and it's like this kind of governing body that talks about all the different types of preventive measures in health care,

517
01:01:50,750 --> 01:01:54,140
all sorts of different things. And they have a grading system.

518
01:01:54,590 --> 01:02:02,780
A, B, c, d. I basically categorizes whether or not you should recommend different types of screening tests.

519
01:02:02,780 --> 01:02:10,460
So like an A would be a recommended test, be recommended for certain populations or certain considerations.

520
01:02:10,490 --> 01:02:13,670
C is most likely not these absolutely not.

521
01:02:13,670 --> 01:02:18,620
And I is there's not enough evidence. And I was kind of talking about how.

522
01:02:19,960 --> 01:02:25,990
It's just really complicated to, like, really figure out, like as a clinician, like where people fall,

523
01:02:25,990 --> 01:02:32,730
especially with most Amy's or the B and C categories and how tying back to last,

524
01:02:32,740 --> 01:02:39,010
I think the reference group isn't always considered in some of these categories and how that can make it in some

525
01:02:39,280 --> 01:02:45,730
cases population level and not looking at like specific risk factors for people who would be considered high risk.

526
01:02:46,880 --> 01:02:56,270
I mean, at the core is an underlying idea that for some populations that have certain characteristics,

527
01:02:56,930 --> 01:03:00,620
the evidence base is strong, that there is a good reason to do things.

528
01:03:02,630 --> 01:03:05,660
To translate this into context, it's much more risk driven.

529
01:03:06,740 --> 01:03:11,270
Patients who have smoked for many, many years, multiple packs per day.

530
01:03:11,480 --> 01:03:16,430
We can do a calculation. We can figure out how likely they are to develop lung cancer.

531
01:03:17,240 --> 01:03:22,370
They're weighing up on the high end of that spectrum, and it might be very good to recommend for that person,

532
01:03:22,370 --> 01:03:26,630
hey, you should go get a lung screening test, lung cancer screening test.

533
01:03:27,710 --> 01:03:31,160
And on the opposite end of the people who've never smoked, you're like, this is a total waste.

534
01:03:31,550 --> 01:03:33,050
And there is a spectrum in between.

535
01:03:34,010 --> 01:03:40,160
How much risk do you have to have before the guideline says, Hey, it's worth the tradeoff of engaging in this screening test?

536
01:03:40,670 --> 01:03:49,090
And so you're going to slice it up into different categories. But notice again the problem of mapping.

537
01:03:49,400 --> 01:03:55,000
But it's not that a is good and the C is bad and gotten great,

538
01:03:55,000 --> 01:04:01,510
which is why I wanted to start with a story because that is one of the most salient classification systems that we all are experiencing.

539
01:04:03,190 --> 01:04:08,180
It's. Okay. I should feel confident, d.

540
01:04:08,250 --> 01:04:11,300
I should feel confident too. He is real confident.

541
01:04:11,310 --> 01:04:16,520
Don't do this. So we're using the scale to mean something different.

542
01:04:17,350 --> 01:04:26,049
And that's one of the problems here is that if I want you to understand this idea of Gee is a strong don't do that,

543
01:04:26,050 --> 01:04:29,380
maybe a different label might be a better way to accomplish that.

544
01:04:30,730 --> 01:04:39,630
Yes. No, they. But in some sense a cleaner labeling system than the ABCD structure they currently have.

545
01:04:45,460 --> 01:04:50,830
All right. A couple other ones I want to make sure to touch upon with that one.

546
01:04:54,490 --> 01:05:00,760
Let's talk about this. This is a off topic, but it's one of my favorite label domains, so let's play with it.

547
01:05:01,510 --> 01:05:10,170
Alex, you were talking in your music about food labels like Best Buy or Use by type.

548
01:05:10,480 --> 01:05:11,980
So talk a little bit about that. Yeah.

549
01:05:12,160 --> 01:05:22,239
So, so basically I used to read it's used by certain dates, but now it has different labels around like sell, buy or best use buy.

550
01:05:22,240 --> 01:05:26,560
And I know that original good intentions and not to waste food.

551
01:05:26,830 --> 01:05:31,840
But at the time right now, I have to think a lot more about what it means.

552
01:05:32,200 --> 01:05:35,859
And it comes from something you can track. Yeah.

553
01:05:35,860 --> 01:05:42,309
So let's just pause for a moment. Are those labels about alcohol rather everything?

554
01:05:42,310 --> 01:05:48,520
This is all my choice. Let me ask you guys the open ended question. What are those labels actually supposed to be representing?

555
01:05:50,260 --> 01:05:56,500
Grocery store. You see, I say it's got one of those multiple different kinds of labels on it.

556
01:05:57,910 --> 01:06:02,149
What is this measuring? As different for different products.

557
01:06:02,150 --> 01:06:13,370
So there's multiple right answers here. You all have some intuitive understanding of what this is.

558
01:06:14,800 --> 01:06:22,340
For some things, it can mean when you shoot heated fire, it's going to actually, like potentially make you sick if it's something that can be spoiled.

559
01:06:22,370 --> 01:06:30,170
Okay. So that's one category where we've got a for products that are likely to spoil in a short period of time.

560
01:06:31,550 --> 01:06:34,910
There is this is a guidance of essentially spoilage risk.

561
01:06:35,480 --> 01:06:41,150
So examples of products that might fall under that. Something like milk?

562
01:06:41,240 --> 01:06:46,880
No, that's a classic example. Salads are one that comes to my mind like a pre-prepared cut food.

563
01:06:47,240 --> 01:06:52,310
Like I don't want to be able to salad that was prepared three weeks ago, etc.

564
01:06:52,640 --> 01:06:56,500
So there's a there's a risk. Categorization here.

565
01:06:56,520 --> 01:07:04,380
Notice, by the way, that you could communicate that same risk in a more quantitative way rather than having a cut off date.

566
01:07:05,700 --> 01:07:10,920
So there are products in the grocery store that you see that gives you packed on dates.

567
01:07:11,880 --> 01:07:15,090
This is what it was harvested. This is when it was packed, etc.

568
01:07:15,360 --> 01:07:20,399
And so how much time has been since it was created is a measure of the risk.

569
01:07:20,400 --> 01:07:23,960
If it was packed yesterday, you should feel pretty confident. It was packed a month ago.

570
01:07:24,150 --> 01:07:29,970
Less confident. But that we instead said, don't do that.

571
01:07:29,980 --> 01:07:37,170
We set up a threshold. So the implication is it's fine if it's before that date and it's not fine if it's after that date.

572
01:07:37,180 --> 01:07:40,600
I don't know about you, but I have definitely had doubts about that.

573
01:07:40,600 --> 01:07:44,440
Clearly, we're going bad before the date that was on it,

574
01:07:45,430 --> 01:07:49,660
and I thought we had things which I ate a day or two after that date and they were totally fine.

575
01:07:50,380 --> 01:07:55,630
This is not a perfect marker of risk, but we have arrived at that.

576
01:07:56,820 --> 01:08:04,710
All right, so what do you know? I am less worried about spoilage when there's a date, say, on a package of Oreos.

577
01:08:06,300 --> 01:08:10,500
So what's that date for? Yeah.

578
01:08:10,920 --> 01:08:15,440
Think of it as like quality assurance there. Like we can promise it'll be good until this and then of course weird.

579
01:08:15,470 --> 01:08:20,610
So if it's the Oreos are stale, they're not going to kill me, but I may not enjoy them as much.

580
01:08:21,840 --> 01:08:29,070
So now we get into quality experience. And so I might react differently.

581
01:08:30,180 --> 01:08:34,350
Behave differently when I recognize that it's a quality issue versus it's a safety issue.

582
01:08:35,250 --> 01:08:38,760
So we think that things like Best Buy kind of thing.

583
01:08:39,780 --> 01:08:45,210
So by date, are those for the store?

584
01:08:45,960 --> 01:08:53,740
Yeah. Why? I don't know, actually. I started looking back, but my start looking bad.

585
01:08:54,190 --> 01:08:58,940
It might. But if it's prepackaged, like if it's Oreos, I'm not going to know how to see the outside of the packet.

586
01:08:59,420 --> 01:09:03,050
Now, this is about those kinds of dates or more about stock rotation.

587
01:09:04,100 --> 01:09:08,630
You don't want to have the Oreo pack the whole way in the back from five years ago sitting there.

588
01:09:08,990 --> 01:09:11,150
You want to be able to rotate stuff and you need to double track them.

589
01:09:11,450 --> 01:09:18,170
There's a value of that to the store in terms of making sure to rotate stock so that they don't have stuff sitting there for forever.

590
01:09:19,700 --> 01:09:23,160
That's not a risk thing and it's not necessarily a quality thing.

591
01:09:23,180 --> 01:09:28,579
It's just a management of inventory thing. So it'd be more of a risk thing.

592
01:09:28,580 --> 01:09:38,130
Like, for example, like Kroger has like they'll put like big signs on there and say like, oh, expire by tomorrow if you buy for $4 less.

593
01:09:38,810 --> 01:09:43,040
Is that more like a race thing? Because like, what if it's actually already expired?

594
01:09:44,330 --> 01:09:53,750
So usually what that is is there was an original sell by date spoiled it again sometimes this label different

595
01:09:53,750 --> 01:09:58,520
way and they noticed the fact they had a whole bunch of that product that was getting close to the date.

596
01:09:59,090 --> 01:10:01,760
So they put it on sale to get you to buy it quickly.

597
01:10:02,900 --> 01:10:07,670
Because otherwise they're going to have to throw it out because especially for things like meat or dairy products,

598
01:10:07,670 --> 01:10:11,660
etc., once it passes that they have to dispose of it, they cannot sell it.

599
01:10:12,740 --> 01:10:19,370
In fact, if you ever see expired stuff on the shelf, they really want you to tell them that because they have to get it off the shelf.

600
01:10:20,060 --> 01:10:26,150
They can get in trouble if they actually sell it. So those, for example, meat packages that are like, you know,

601
01:10:26,540 --> 01:10:32,990
$3 off if you buy this often are things where the date is tomorrow that they want it out the door today.

602
01:10:34,810 --> 01:10:40,210
Which if you're going to make it today, is probably fine. But if you're intending to hold on to it for a week, let's find.

603
01:10:43,130 --> 01:10:52,130
Why am I talking about this one? Notice that one again with an underlying continuous variable, maybe a risk quality.

604
01:10:52,850 --> 01:10:57,160
But as soon as we start categorizing it, we create binary behavior.

605
01:10:57,170 --> 01:11:01,970
I throw out the milk on this date, even though it might at perfectly fine.

606
01:11:03,540 --> 01:11:08,100
I drink it if it's on this date, even though it might not be okay.

607
01:11:08,940 --> 01:11:12,290
In fact, there's a whole literature about how.

608
01:11:13,530 --> 01:11:18,179
For much of the US food system, we are actually extremely conservative with a lot of stuff that gets passed.

609
01:11:18,180 --> 01:11:24,960
Foods is probably fine. And so the amount of food waste within the US system is.

610
01:11:26,020 --> 01:11:32,300
Elevated, by the way, in which we label things. And that's a trade off.

611
01:11:33,060 --> 01:11:38,070
There are risks here, but we're essentially talking about, you know,

612
01:11:38,310 --> 01:11:44,549
large fractions of our food supply for very little risk benefit because of the way the system is set up,

613
01:11:44,550 --> 01:11:49,050
because we're categorizing and labeling in a way which is not.

614
01:11:51,210 --> 01:11:56,340
All right. So take I would spend all this time talking about labels and when they're good and when they're bad.

615
01:11:56,850 --> 01:12:01,500
Turn it over. Just talk it through. Like, when do you feel like, hey.

616
01:12:02,940 --> 01:12:07,410
No. Lance labels are bad. Give people numbers.

617
01:12:07,650 --> 01:12:10,410
Manage that way. Make sure that they have the underlying data, etc.

618
01:12:10,410 --> 01:12:14,880
And when are you like, no, but we've got to get people levels that can be completely lost without this, whatever.

619
01:12:15,390 --> 01:12:21,980
Just engage with the question of when the labels seem most helpful, when those labels most harmful.

620
01:12:22,620 --> 01:12:35,710
Okay. But that through. Yeah, exactly.

621
01:12:37,180 --> 01:12:41,460
Exactly. Where are you going to regulate?

622
01:12:42,520 --> 01:12:47,579
Like, I think it's like that.

623
01:12:47,580 --> 01:12:57,690
Like what they say. Like it's like, oh,

624
01:12:57,690 --> 01:13:16,190
here's something like like you would still like to teach me like I have my preference

625
01:13:16,200 --> 01:13:33,730
narrows quite interesting but I didn't get any because it just like like,

626
01:13:34,650 --> 01:14:06,970
you know I don't like I like I like my parents and my father.

627
01:14:07,680 --> 01:14:30,710
I feel like I like this.

628
01:14:30,900 --> 01:14:46,350
And it was like like like like like like like like like this seems okay.

629
01:14:50,490 --> 01:14:58,010
So maybe, you know, that's great.

630
01:15:00,030 --> 01:15:05,880
Yeah, I think that might happen again.

631
01:15:07,140 --> 01:15:41,790
It's not like it's just like, oh, you know, it's like, I think it's like anymore.

632
01:15:43,530 --> 01:16:23,740
It's like, I don't know, I think it's yeah, but like I like, I like, I like, I like, like involvement with my music coaching.

633
01:16:24,150 --> 01:16:32,300
But at this point get time I really yeah.

634
01:16:33,560 --> 01:16:43,290
Like if you were you might be able to get better out of my job.

635
01:16:43,570 --> 01:16:57,960
I'm like get it out. Because I do think, like when it comes to my.

636
01:17:02,320 --> 01:17:14,559
It was really helpful. I really kind of like the one on one setting with the station in my opinion.

637
01:17:14,560 --> 01:17:20,200
Like quite unlikely to get this final like a percentage.

638
01:17:22,150 --> 01:17:26,350
It's not like one 5 to 1 chance.

639
01:17:27,310 --> 01:17:31,840
So there's a situation where you're really feeling like you need that precision and you really like those labels.

640
01:17:31,840 --> 01:17:36,380
Not not giving it to them. Yes. Right. The situation.

641
01:17:36,430 --> 01:17:38,770
What's wrong with that? More so badly? Yeah.

642
01:17:38,800 --> 01:17:48,190
We were kind of like with like precision matters versus like when it doesn't like when the stakes are like, I'm eating a cookie.

643
01:17:48,670 --> 01:17:53,920
Like, we can have, like, a label because, like, I'm eating a stale cookie versus a fresh cookie and.

644
01:17:55,110 --> 01:18:00,670
It'll be okay. Talking about risk for breast cancer.

645
01:18:00,850 --> 01:18:05,940
That's like a much higher stakes situation in that position, like matter.

646
01:18:06,310 --> 01:18:09,460
All right. So let me I'm going to be annoying for a moment.

647
01:18:12,060 --> 01:18:20,820
Does that mean that we need to quantify the risks of the problem with anesthesia as you're rolling into the surgery,

648
01:18:21,360 --> 01:18:26,100
or do you want to just be told it's really unlikely that there's going to be a problem?

649
01:18:28,590 --> 01:18:32,000
Must real serious. No, it's not a severity question there.

650
01:18:33,140 --> 01:18:36,440
We talked about like possibly using both in like health care settings.

651
01:18:36,890 --> 01:18:43,190
So we're like it like like an example. Like it's incredibly rare to have an adverse complication,

652
01:18:43,190 --> 01:18:53,570
like 2% like saying something like that because then it could if someone can get both sides, I'm going to flag something that's important.

653
01:18:53,720 --> 01:18:59,650
There is the order saying you. Meaning came first.

654
01:19:00,980 --> 01:19:04,850
Number 10 seconds. Even something as simple as digital.

655
01:19:05,510 --> 01:19:09,170
Does it feel different to say it's really unlikely that this is going to happen?

656
01:19:09,180 --> 01:19:14,720
Like only one or 2% versus one or 2% of people have this happen, which is, you know, that's pretty unlikely.

657
01:19:18,760 --> 01:19:24,010
We process things in order. So what comes first?

658
01:19:24,010 --> 01:19:28,510
If you're hearing it's really unlikely when they get that number,

659
01:19:28,510 --> 01:19:34,180
they're already processing that number as having that meaning because we've given them the meeting from the start.

660
01:19:34,630 --> 01:19:38,110
If we get the number first, they're like, What is that? Two people.

661
01:19:38,110 --> 01:19:42,189
Two people have this happen to them. Oh, my gosh, that could happen to me. Very different framing.

662
01:19:42,190 --> 01:19:47,050
Even those in the verbal term might follow. Again, I'm not saying what is necessarily right or wrong.

663
01:19:47,060 --> 01:19:51,520
Just watch your own reactions as this play out.

664
01:19:52,570 --> 01:19:57,860
Okay, so now what?

665
01:19:58,810 --> 01:20:04,120
So we're going to precision. We've got to get we got to make sure the number is there. But maybe we could do both.

666
01:20:04,420 --> 01:20:07,810
So is there any situation where we don't want to label? Right. Let's be a problem.

667
01:20:08,250 --> 01:20:13,150
It's okay, but we don't want labels. Yeah, I'm getting confused between labels and numbers.

668
01:20:14,080 --> 01:20:17,630
Is a number label. Don't up.

669
01:20:18,150 --> 01:20:20,020
But at least not the way I'm using the term.

670
01:20:20,140 --> 01:20:26,800
Well, I'm thinking about your example a bit like earlier in the semester when you said the numbers became meaningless.

671
01:20:27,190 --> 01:20:34,890
Yeah. Washtenaw County had 110 cases of documented COVID reported in the last day.

672
01:20:35,160 --> 01:20:40,920
Right. What do you feel about that? That's that's a situation you should use a label.

673
01:20:42,030 --> 01:20:47,399
How does it compare to the history or is it a situation in which we want to do what we want?

674
01:20:47,400 --> 01:20:55,920
An absolute label? Do we want to know that that's below average for other counties within the state or compared to some other threshold?

675
01:20:56,550 --> 01:21:00,640
But you're right, it has no meaning. But which meaning I love.

676
01:21:00,720 --> 01:21:08,760
It's like, yeah, if our county is below average in terms of rates within Michigan, but we still have fairly high circulating disease.

677
01:21:08,760 --> 01:21:13,020
We all still need to be taking precautions even though we're below average.

678
01:21:13,500 --> 01:21:21,840
Yeah. I think sometimes avoiding labels when you're trying to compare risks, particularly if they're more similar,

679
01:21:21,990 --> 01:21:28,530
and particularly if the severity of the risks, you know, if you're trying to compare both the likelihood and the severity and those two variables.

680
01:21:28,540 --> 01:21:37,379
So the task here might be different if we're trying to engage in some nuanced comparison situation versus some of, let's say, behavior motivation.

681
01:21:37,380 --> 01:21:41,040
I just want you to be aware that this risk exists and get you to do something about it.

682
01:21:43,040 --> 01:21:53,749
Notice that it just goes back up. BREWER Like, if we're thinking about risk perceptions, you might have one goal and we're trying to get but hey,

683
01:21:53,750 --> 01:21:59,180
if you're we have a different goal labels connected behavior really will get you motivated to do stuff.

684
01:22:00,380 --> 01:22:05,500
It doesn't necessarily calibrate you for decision making. So looks at all.

685
01:22:06,960 --> 01:22:09,480
What motivates you to quit smoking or to get vaccinated?

686
01:22:09,930 --> 01:22:17,430
Or do I want you to be engaging in something that is this treatment right for you kind of situation where the difference is the critical question.

687
01:22:17,940 --> 01:22:21,870
Yeah. All good points. All right, I'm over time. I'm going to let you guys go.

688
01:22:24,110 --> 01:22:28,259
One thing just for you, you'll see this next class.

689
01:22:28,260 --> 01:22:34,100
We're talking about data visual. That's part of who you submit.

690
01:22:35,100 --> 01:22:38,580
Oh, gee, that's visual. Anything good or bad?

691
01:22:38,700 --> 01:22:44,910
I'm here. We're going to have lots of examples. We're going to talk about it. So people really want to see something in a handout.

692
01:22:45,510 --> 01:23:22,000
The health data we did before. So I don't know.

693
01:23:27,920 --> 01:23:31,790
I know it's upstairs to see if I could do it, because, like, I.

694
01:23:32,200 --> 01:23:51,200
We would get it out of my head and try to do it that way.

695
01:23:52,670 --> 01:24:06,410
I'd like to look at everything like, well, my thing is the guy is a great writer and I saw something different.

696
01:24:06,830 --> 01:24:13,180
Like, I don't know, I just walk out.

697
01:24:14,120 --> 01:24:23,270
But again, because I'm thinking of the traditional definition.

698
01:24:24,640 --> 01:24:30,740
But I think that's. Does this mean you're with me?

699
01:24:30,960 --> 01:24:34,130
I just. Did you see my comment on the. On the assignment?

700
01:24:34,580 --> 01:24:38,050
I thought I did, but maybe I did not go back and look. Okay.

701
01:24:38,690 --> 01:24:47,630
Because you said I had no comment about anything. And I have a literal reference to possibly this that I may go back.

702
01:24:47,840 --> 01:24:49,940
Okay. I don't like I don't care about that, Greta.

703
01:24:51,100 --> 01:24:57,899
It is entirely possible, given the way that the grading system works, that I missed that because of where the comments.

