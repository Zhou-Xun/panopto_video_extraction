1
00:00:01,430 --> 00:00:14,489
Okay. So. We're going to be talking about hanging out 21 today and this morning just before class.

2
00:00:14,490 --> 00:00:18,209
I added another slide to the end just with some more references.

3
00:00:18,210 --> 00:00:22,620
So if you downloaded it really, really early, go ahead and download the new copy.

4
00:00:23,250 --> 00:00:28,200
It hasn't changed too much, but it is going to be slightly different towards the end.

5
00:00:29,500 --> 00:00:34,840
And. What else is going on?

6
00:00:34,870 --> 00:00:43,000
We only have one more class December 7th and I've already posted the review materials.

7
00:00:43,890 --> 00:00:53,850
For that. So if you haven't started studying, this is a handout that I'll go over next time and just bring your questions next time.

8
00:00:55,140 --> 00:01:04,830
Okay. But today, I want to give you yet one more tool that I like to have all of my rays know about and use.

9
00:01:04,830 --> 00:01:11,800
I use it a lot. I double checking that we are recording.

10
00:01:12,900 --> 00:01:15,930
Yep. Everything looks good. So let's go ahead and.

11
00:01:17,450 --> 00:01:23,219
Get going with the handout. And you know,

12
00:01:23,220 --> 00:01:27,360
it's always kind of a challenge towards the end of the course when you put together material you

13
00:01:27,360 --> 00:01:31,920
don't want to run over because you can't really have time to make up for it on the back end.

14
00:01:32,730 --> 00:01:36,180
So I suspect we're going to end very early today.

15
00:01:37,270 --> 00:01:40,389
So I'm going to just skip the break with that idea.

16
00:01:40,390 --> 00:01:50,140
Unless it unless I'm completely wrong and I see a slow down and amazingly, I'm just going to blow right through.

17
00:01:50,800 --> 00:01:55,810
And so it's probably going to be less than an hour and a half, but maybe even more.

18
00:01:56,680 --> 00:02:03,100
So this is a topic that's kind of related to things that we've learned already.

19
00:02:03,100 --> 00:02:10,510
Restricted Mean Survival Times. That's a topic that we learned about early on for two simple tests.

20
00:02:11,320 --> 00:02:16,350
And so I want to now extend it to the regression setting.

21
00:02:16,360 --> 00:02:22,690
We didn't do that before. The only regression method that we had for time to event data was the Cox model.

22
00:02:23,670 --> 00:02:29,760
And as you know, based on your last homework, it comes with a lot of baggage, right?

23
00:02:29,760 --> 00:02:40,350
You have to assume proportional hazards holds. You might have to do some diagnostics to see whether proportional hazards assumptions are okay or not.

24
00:02:40,770 --> 00:02:44,970
And so this is an alternative method that doesn't rely on proportional hazards assumption.

25
00:02:45,270 --> 00:02:52,590
It's more recent than Cox. So the Cox model is going to be the one that dominates the literature.

26
00:02:52,890 --> 00:02:56,580
It's going to be the one people know and the one that gets taught.

27
00:02:57,600 --> 00:03:02,790
But this is going to take over, I think is the more popular model, in my opinion.

28
00:03:02,790 --> 00:03:06,930
It's just kind of wait and see. So the.

29
00:03:07,890 --> 00:03:12,450
That particular reference that kind of birthed all this stuff with 2004.

30
00:03:14,040 --> 00:03:17,190
Compared to Cox model, which was, you know, in the seventies.

31
00:03:17,790 --> 00:03:23,939
So it's just a matter of time. And there is software now available in both AH and South.

32
00:03:23,940 --> 00:03:31,380
And when SAS gets its act together and put something in software, you know, it's here to stay and now suddenly everybody's going to be using it.

33
00:03:31,980 --> 00:03:32,230
Okay.

34
00:03:35,310 --> 00:03:44,550
And some of this is going to be handy studying for the exam because there's overlap between the regression methods and what we've learned already.

35
00:03:45,000 --> 00:03:51,389
So what I'm going to do in the first few pages of this is review a little bit of Handout

36
00:03:51,390 --> 00:03:57,030
ten because it's been a while since we thought about these restricted lifetimes.

37
00:03:57,480 --> 00:04:08,400
And so recall that the total restricted mean survival time, which we estimated with Tao or Amnesty, was covered in Handout ten.

38
00:04:09,000 --> 00:04:16,650
And the idea is that you're estimating the expected years of life lived over 12 years of follow up.

39
00:04:18,530 --> 00:04:26,150
And so I'm going to refresh our memory on how it's estimated when there are no covariates and kind of rebuild that intuition up.

40
00:04:26,160 --> 00:04:30,140
And you can consider this pre studying for this material for the final quiz.

41
00:04:32,160 --> 00:04:39,720
So the new thing is going to be to learn how to estimate how arm is T as a function of covariance.

42
00:04:40,170 --> 00:04:49,260
And there's kind of a new cool thing called pseudo observations that I'll teach you briefly to help you with that regression modeling.

43
00:04:49,260 --> 00:04:54,390
And it's kind of a cool little thing that I found useful in a number of situations.

44
00:04:56,910 --> 00:05:04,440
There's a little bit of overlap also with g regression and generalized linear model regression in general.

45
00:05:05,010 --> 00:05:10,589
You can use pseudo observations that are going to teach you how to get it with

46
00:05:10,590 --> 00:05:16,050
either of these packages as if they were true restricted times to event so.

47
00:05:18,860 --> 00:05:23,360
You know, there's going to be a little bit more overlap in that sense with what you've done.

48
00:05:23,900 --> 00:05:30,620
You can use either the identity link or the log link to analyze these tower misty kind of things.

49
00:05:31,100 --> 00:05:34,760
And both are used in practice. I'll show you examples that do both.

50
00:05:37,470 --> 00:05:43,490
The JE approach. Is thought to add a degree of robustness to various calculations,

51
00:05:43,490 --> 00:05:48,740
even with one pseudo observation per individuals that the software packages seem to have.

52
00:05:48,740 --> 00:05:54,740
You use this, but my this is very close to my research area.

53
00:05:54,740 --> 00:06:01,400
I've done a lot of work in this area and so in my own research I haven't seen much difference in simulated performance when using simpler,

54
00:06:01,700 --> 00:06:09,860
generalized linear models as opposed to g e because it's only it's there's only one outcome per individual.

55
00:06:10,280 --> 00:06:18,230
And I actually had a conversation with the author of the 2004 paper that birthed this field.

56
00:06:18,830 --> 00:06:27,080
And he kind of said that they put in this G thing because a reviewer remarked that they thought it would be useful and.

57
00:06:29,510 --> 00:06:34,520
You just do what the reviewer says sometimes. So that kind of got stuck into old software.

58
00:06:34,760 --> 00:06:40,640
But it's not really that important. And does it if you have one pseudo observation per person.

59
00:06:42,160 --> 00:06:46,059
And you can see that for yourself actually by trying both. All right.

60
00:06:46,060 --> 00:06:54,400
So here's the review for Mean Survival Time and how restricted means survival time in the case when there's no censoring.

61
00:06:54,790 --> 00:06:58,780
And I always like to look at the case with no censoring because it gives you a lot more

62
00:06:58,790 --> 00:07:03,130
intuition if you kind of start from that and then move to adjusting for censoring.

63
00:07:03,670 --> 00:07:09,790
So when there's no censoring in the data for your time to have that and you get to observe everybody's time to event,

64
00:07:10,150 --> 00:07:15,160
if you wanted to look at the mean survival time or their mean event time,

65
00:07:15,460 --> 00:07:23,060
you can calculate that just as if you were in your first intro biostatistics course with the sample mean.

66
00:07:23,080 --> 00:07:32,229
So I've changed the access to the ties for the event times, but this is basically like an x bar where the outcomes are the event times.

67
00:07:32,230 --> 00:07:44,059
The several times. And it turns out that this is exactly the same calculation algebraically is that you get the same

68
00:07:44,060 --> 00:07:50,330
algebraic result if you first estimate the kaplan-meier curve and then you look at the area

69
00:07:50,330 --> 00:07:58,610
under Kaplan-meier curve so that you can get the mean survival time either by averaging this

70
00:07:58,610 --> 00:08:04,730
arrival times or looking at the area under the kaplan-meier curve applied to those survival times.

71
00:08:05,150 --> 00:08:16,969
And that connection is really key in being able to handle censoring later because we can certainly estimate the kaplan-meier curve without bias,

72
00:08:16,970 --> 00:08:24,890
even when they're censoring. But we can't get this t bar thing in an unbiased way when they're censoring.

73
00:08:27,310 --> 00:08:37,990
Without doing this kaplan-meier trick, that is. So the Tao restricted means arrival time averages lifetimes over a Tao restricted follow up period.

74
00:08:38,000 --> 00:08:44,799
So something that we've talked about in handout ten is, you know, what are the possible values of town,

75
00:08:44,800 --> 00:08:50,590
what's the maximum possible value of Tao that you can use when you have particular data sets?

76
00:08:50,830 --> 00:08:52,360
And so I'll review that as well.

77
00:08:53,810 --> 00:09:02,450
So survival times larger than Tao are truncated at Tao in the averaging process or are restricted to Tao in averaging process.

78
00:09:02,450 --> 00:09:05,930
So the Tao or Misty becomes the sum.

79
00:09:06,740 --> 00:09:13,760
And for each individual you either see Tao or their event time, whichever is smaller.

80
00:09:13,760 --> 00:09:23,570
So if they lived the entire Tao time you average in a tao for them, but otherwise you average in their event time.

81
00:09:26,430 --> 00:09:32,970
And that turns out that it's exactly the same as calculating the area under the cap a marker up to timetable.

82
00:09:35,670 --> 00:09:39,600
So this is just copied over from handout ten. So blast from the past.

83
00:09:39,600 --> 00:09:49,169
So this is a little made up data set where you don't have censoring and I'm kind of thinking of this tie as survival time in years or death time,

84
00:09:49,170 --> 00:09:52,770
depending on if you're a cup half full or half empty person.

85
00:09:53,520 --> 00:09:57,900
And so if you wanted to get the mean of those death times,

86
00:09:58,290 --> 00:10:06,600
the intro bio stat way of getting that is just to average all of those event times divided by the total, which is ten.

87
00:10:07,080 --> 00:10:12,870
And so the mean survival time is 17.8 years in my little pretend data set.

88
00:10:14,630 --> 00:10:19,520
And if you wanted to look at the Tao restricted means survival time for Tao of ten years.

89
00:10:20,120 --> 00:10:26,420
Then what happens is every time you have an event that's less than ten years, those event times get added in.

90
00:10:26,810 --> 00:10:35,520
But once you reach people who lived at least ten years, their ten year restricted mean survival time, just six in ten for all those values.

91
00:10:35,520 --> 00:10:38,690
So that's the minute ten and their event time.

92
00:10:39,600 --> 00:10:43,560
And so that ends up being 8.5 years for this mini data set.

93
00:10:43,860 --> 00:10:48,710
So that is during the first ten years of follow up, a person lived an average of 8.5 years.

94
00:10:48,720 --> 00:10:53,940
That's the interpretation of the ten year restricted mean for this little made up data set.

95
00:10:54,910 --> 00:10:58,030
Is this kind of coming back to you? Okay.

96
00:11:01,990 --> 00:11:06,270
And you get the same answer if you look at the area under the Kaplan-meier curve.

97
00:11:06,280 --> 00:11:09,700
So this is not an exercise that you do every day.

98
00:11:09,700 --> 00:11:15,429
But the nice thing about Kaplan-meier curves is that they're flat and then they drop and then they're flat and then they drop.

99
00:11:15,430 --> 00:11:23,800
So if you want to get the area under a kaplan-meier curve, you just get all the areas of the rectangles under the curve.

100
00:11:24,430 --> 00:11:27,999
So here's kind of a short little table that helps with that calculation.

101
00:11:28,000 --> 00:11:34,209
So for this data set, here is the height or the kaplan-meier estimate each of these time points.

102
00:11:34,210 --> 00:11:38,950
So that starts off at one when no one's had the event and then it kind of drops from there.

103
00:11:39,280 --> 00:11:46,300
And I've added in this time point ten so that this is now times of interest instead of just event times,

104
00:11:46,720 --> 00:11:54,940
so that we have a stat that we can look at area under the Kaplan-meier curve up to ten as well as a special case.

105
00:11:56,100 --> 00:12:03,059
So these are the KAPLAN-MEIER estimates over time and here is the width of the time interval so we can get the areas of those rectangles.

106
00:12:03,060 --> 00:12:09,360
So between zero and two, the width of the rectangle we would want to figure out is two, right?

107
00:12:09,990 --> 00:12:17,250
And so we have the height, the kaplan-meier curve is one all the way between zero and two and then it drops.

108
00:12:17,580 --> 00:12:25,290
So that first height, times width that gets us the area under the Kappa marker up to two is just two times one or two.

109
00:12:26,020 --> 00:12:32,370
Right. And then the width between two and five years is three years.

110
00:12:32,370 --> 00:12:35,670
Right. So that's here. This is the width of the time interval, three years.

111
00:12:36,330 --> 00:12:41,910
And so if we want to get the height times the width between two and five years,

112
00:12:42,180 --> 00:12:47,820
we multiply the kaplan-meier estimate that is constant and flat between two and five.

113
00:12:47,940 --> 00:12:51,540
So that's the height of our rectangle, here's the width of our rectangle.

114
00:12:51,780 --> 00:13:00,330
And so the area for that rectangles 2.7 And so the area under the Kaplan-meier curve up to five is two plus two plus seven.

115
00:13:01,140 --> 00:13:05,040
So this is kind of how you can get those numbers and it kind of goes all the

116
00:13:05,040 --> 00:13:09,509
way here where these are all the little areas that the rectangles indicate.

117
00:13:09,510 --> 00:13:14,820
The area under the Kaplan-meier curve, you just sum up however many rectangles you need to get to that time point.

118
00:13:16,510 --> 00:13:23,680
And so you can add up all of these height times widths and get the area under

119
00:13:23,680 --> 00:13:27,550
the entire survival curve all the way down to the point when it hits zero.

120
00:13:28,000 --> 00:13:32,530
And if you add all those up, you'll get the same mean that we did with just T-bar.

121
00:13:32,560 --> 00:13:39,070
So the overall survival time is 17.8 years for this data set, and you can calculate it either way.

122
00:13:39,520 --> 00:13:43,230
And this is a calculation that will always be identical.

123
00:13:43,810 --> 00:13:46,870
You'll always get the same answer if there's no censoring.

124
00:13:49,990 --> 00:13:55,059
And similarly, if you just wanted to get the ten year restricted means survival time,

125
00:13:55,060 --> 00:13:59,110
you just add up all the height times, the widths up to ten years.

126
00:13:59,410 --> 00:14:05,740
And for this little data set, that's these first four values and we get the same answer 8.5 years again.

127
00:14:06,100 --> 00:14:09,430
So it works for overall means, it works for restricted means.

128
00:14:09,700 --> 00:14:17,680
There's just this direct link between averaging these types of things and looking at the area under the corresponding kaplan-meier curve.

129
00:14:19,710 --> 00:14:27,030
So that ends up being really handy because with sensor data we can estimate the tower

130
00:14:27,030 --> 00:14:31,200
misty value that we're interested in by calculating the area under the Kaplan-meier curve.

131
00:14:31,230 --> 00:14:38,700
During that follow up period of interest, we can estimate the Kaplan-meier curve even when they're censoring, you know,

132
00:14:39,180 --> 00:14:46,290
by looking at the area, you know, looking at the area of the Kappa marker and getting that unbiased estimate of Tower Misty.

133
00:14:47,010 --> 00:14:54,180
So that's how it's done. We just have to pay attention to where the Kaplan-meier curve is validly defined to

134
00:14:54,180 --> 00:14:59,340
think of what possible values to how we can go up to with these restricted means.

135
00:15:02,850 --> 00:15:05,390
So that this is a reminder how that works.

136
00:15:05,400 --> 00:15:13,620
So if the last event that you observe in the data set, it is an observed failure time, like a death or just, you know,

137
00:15:13,890 --> 00:15:16,800
the type of event that you were trying to analyze,

138
00:15:17,310 --> 00:15:24,870
then the kaplan-meier curve is going to drop to zero at that last event time and will be zero for ever after.

139
00:15:25,230 --> 00:15:29,490
And so the curve is validly defined at all follow up times.

140
00:15:29,490 --> 00:15:34,830
You have an estimate for what that survival probability is all the way through time.

141
00:15:36,430 --> 00:15:40,780
You know, because once it drops down to zero, you can assume that estimate zero forever after.

142
00:15:41,410 --> 00:15:49,030
So in that case, for that particular group, there's no upper limit to what tower could be.

143
00:15:49,480 --> 00:15:52,960
When you're looking at the tower, surge means arrival time for that group.

144
00:15:55,020 --> 00:16:02,760
So you can estimate the mean survival time because you can go all the way out, you know, as long as you need to to the end.

145
00:16:03,150 --> 00:16:09,870
Or you could choose any follow up period of length tower that's shorter than that and estimate its how restricted mean.

146
00:16:13,010 --> 00:16:22,010
However, if the last event in the data sets a sensor value, then you don't have a survival estimate beyond that last observed time.

147
00:16:22,010 --> 00:16:25,580
So the KAPLAN-MEIER does not have that valid estimate beyond this value.

148
00:16:25,940 --> 00:16:31,670
So in other words, if you if your largest event in the dataset is censored,

149
00:16:32,180 --> 00:16:36,889
then your kaplan-meier estimate at that time is going to be something greater than zero.

150
00:16:36,890 --> 00:16:40,250
And you have no way of knowing without because you don't have the data,

151
00:16:40,250 --> 00:16:45,440
you have no way of knowing whether it's going to stay at that value above zero and everybody lives forever,

152
00:16:45,920 --> 00:16:53,120
or if everybody dies immediately afterwards, there's just there's too much range without data to know what's going to happen next.

153
00:16:53,540 --> 00:16:59,790
So that is going to limit where the maximum possible Tao you can use for that group is.

154
00:16:59,790 --> 00:17:06,980
So we can only report Tao restricted means for values of Tao less than that last censored follow up time.

155
00:17:08,900 --> 00:17:14,780
And so we looked at this when we were looking at two sample tests, when we were trying to compare two groups,

156
00:17:15,170 --> 00:17:22,219
we had to know what the maximum possible tower was for each of the two groups, and then we could only use the tower.

157
00:17:22,220 --> 00:17:26,690
That was the smaller of those two values. And we kind of played around with that a little bit.

158
00:17:29,210 --> 00:17:32,420
So the.

159
00:17:33,570 --> 00:17:42,450
Pseudo observation approach is how we're going to be able to move these censored event times to a regression method.

160
00:17:42,840 --> 00:17:50,729
And so the pseudo observation approach was proposed by Anderson at all in 2004 to model Tao or Misti as a function of covariance.

161
00:17:50,730 --> 00:18:02,760
And every time I see the year 2004, I'm so happy because one, my kids are born to Red Sox won the World Series 2004 great year just generally.

162
00:18:03,000 --> 00:18:06,810
All right. But also this paper came out, which is a really fundamental paper, I think.

163
00:18:08,450 --> 00:18:15,499
So the main idea behind the suit observation approach, sorry, Yankees fans is if you had no censoring,

164
00:18:15,500 --> 00:18:20,510
you could model the expected value of these restricted event times.

165
00:18:20,510 --> 00:18:35,209
Mentor of A.I. The mean of those restricted event times using either a G, e or a generalized linear model with either an identity link or a log link.

166
00:18:35,210 --> 00:18:44,690
So here's a where you could write out the model if you wanted to estimate these restricted means in terms of covariates using identity link.

167
00:18:44,690 --> 00:18:49,160
So that means just nothing is interfering between you and this, this random variable.

168
00:18:49,430 --> 00:18:53,749
Or you could use a log link as well. So here's what that would look like.

169
00:18:53,750 --> 00:18:57,319
So this is the outcome that you're modeling and the covariates.

170
00:18:57,320 --> 00:19:07,190
And if you don't have censoring, you would just go ahead and pick one of these things probably and just model whatever these individual mentees are.

171
00:19:08,280 --> 00:19:10,230
Dementia. It's all things are.

172
00:19:11,940 --> 00:19:19,649
And if you go to this approach, you know, you don't have to have any assumptions about the hazard shapes behind the scenes.

173
00:19:19,650 --> 00:19:34,170
By the way, you're going to be assuming probably that these mentalities are approximately normal, which, you know, it's kind of approximately, okay,

174
00:19:34,170 --> 00:19:38,909
there's a bunch of values that tell, but the regression methods, assuming normality,

175
00:19:38,910 --> 00:19:44,399
still work pretty well, but you don't have to assume proportional hazards at all.

176
00:19:44,400 --> 00:19:50,620
So you've kind of gotten away from that assumption. You can't use this approach directly though, when they're sensor data,

177
00:19:50,830 --> 00:19:56,590
so you don't have a value for mentality when you have sensor data for each person.

178
00:19:57,130 --> 00:20:01,540
But there's a way to create pseudo observations that can be used instead of the original

179
00:20:01,540 --> 00:20:08,050
mentality values to fit these regression models when they're centered mentality outcomes.

180
00:20:09,840 --> 00:20:14,850
And what I'm going to do here. I'm not going to really talk much about the theory of these things.

181
00:20:14,850 --> 00:20:19,770
I'm just going to show you how to use them. And I'm going to give you a little bit of intuition about them as well.

182
00:20:20,820 --> 00:20:27,450
But they've they've got a lot of nice properties. So for the either individual in the dataset,

183
00:20:28,020 --> 00:20:36,250
the pseudo observation that corresponds to their mentality outcome looks like this and I'm going to unpack this for you.

184
00:20:36,270 --> 00:20:43,540
So the outcome, the number that you get after doing everything on the right hand side of that equation, I'm going to be calling POIs.

185
00:20:43,560 --> 00:20:47,700
So this is going to end up just being a number for everybody in the data set.

186
00:20:47,700 --> 00:20:51,060
They'll have their own POIs, whatever that number is.

187
00:20:51,660 --> 00:20:58,920
And so how do you get that number? So PN is the sample size of the dataset that you're analyzing.

188
00:20:59,220 --> 00:21:04,190
And so there's an RN over here, and then there's that sample size minus one over here.

189
00:21:04,200 --> 00:21:13,650
So that's appearing in this calculation. And then this a T is actually what we've been estimating all along in Handout ten.

190
00:21:14,550 --> 00:21:21,810
So Armistice, the Tao restricted means arrival time based on the whole cohort, right?

191
00:21:22,230 --> 00:21:30,150
So you're not even using any covariance when you do this, you just get, you know, the area under the kaplan-meier estimate or up total.

192
00:21:30,210 --> 00:21:37,950
That's all this number is. And then this thing over here where you have this minus I in parentheses, that's just notation,

193
00:21:38,280 --> 00:21:46,110
that means that you want to get the tower, Miss T, when you've taken patient eye out of the data set.

194
00:21:47,520 --> 00:21:51,600
So Armistice the terrorists, which would mean for the whole cohort.

195
00:21:51,900 --> 00:21:55,680
And this is the same calculation but without patient I in it.

196
00:21:56,460 --> 00:22:01,650
And so this is this part here is going to make these policies different for everybody

197
00:22:01,650 --> 00:22:05,910
in the data set because you're subtracting off always this end minus one times,

198
00:22:06,180 --> 00:22:10,380
whatever the restrictive mean would have been if you never had patient AI in the data set.

199
00:22:12,380 --> 00:22:16,940
And so this is sort of a weird little thing. So I want to give you some intuition about how this works.

200
00:22:17,710 --> 00:22:24,830
But I will do that in a minute. But you will have a number that's different for every person in the data set from this calculation.

201
00:22:25,160 --> 00:22:32,400
And we're going to use this and we can use it and analyze it as if it's just a regular, normally distributed outcome.

202
00:22:32,420 --> 00:22:35,600
We can use it in regression methods and everything else just fine.

203
00:22:36,770 --> 00:22:42,310
One property that has been proven to be true, and I will give you some intuition for this in a minute.

204
00:22:42,350 --> 00:22:46,640
I know. I keep on saying in a minute, in a minute. In a minute. Sorry, but I will.

205
00:22:47,000 --> 00:22:54,860
And it has the same conditional expectation as the outcome that you would have liked to model if there was no sensory.

206
00:22:56,400 --> 00:23:05,070
So when you analyze these polls, the model is going to act just like you were modeling the uncensored outcomes.

207
00:23:06,140 --> 00:23:15,700
Which is very, very cool. So in other words, we can estimate parameters from the desired model where, you know,

208
00:23:15,700 --> 00:23:25,749
you wanted to just have the mentor tie here by fitting the model with the the numbers profile that you get from this formula and

209
00:23:25,750 --> 00:23:35,770
the baiters that you are interpreting can be interpreted just as if you had modeled uncensored data that you really don't have.

210
00:23:35,950 --> 00:23:42,340
You don't have uncensored data. But you can get parameters as if you did have uncensored data with this trick.

211
00:23:43,320 --> 00:23:49,820
Isn't that cool? And you can do the same thing if you prefer to use the log link.

212
00:23:49,830 --> 00:23:56,700
I'll show you an example of how you get manuscript worthy sentences using the identity link and how it changes when you use the log link.

213
00:23:57,300 --> 00:24:00,300
And so same thing if you want.

214
00:24:00,300 --> 00:24:05,990
If you really had uncensored data and you were using the log link, you'd have a model like this.

215
00:24:06,000 --> 00:24:09,840
The parameters are going to be interpreted differently from the ones with the identity link.

216
00:24:11,190 --> 00:24:19,950
And you can actually just fit using the the pseudo observations, these numbers that you get from this calculation over here.

217
00:24:21,400 --> 00:24:27,809
And whatever parameter estimates you get over here where you fit the log link with these

218
00:24:27,810 --> 00:24:33,000
pseudo observations are could be interpreted as if you were modeling the end sensor data.

219
00:24:33,600 --> 00:24:42,060
So these feeder observations are really, really powerful. And I have to say I was quite impressed when I met the guy who came up with this idea.

220
00:24:42,690 --> 00:24:47,489
You know, I was just saying, you know, it's so hard to get new, great fundamental ideas in this field.

221
00:24:47,490 --> 00:24:52,800
And he's like, I have something. And he showed me this and I've been using it ever since.

222
00:24:56,340 --> 00:25:01,860
He actually got the idea from a method called Jackknife.

223
00:25:02,250 --> 00:25:05,170
And I don't expect to go, Oh, I've heard of that, you know.

224
00:25:05,460 --> 00:25:14,950
But if you hear the word jackknife and you were curious, he kind of was thinking along the same lines when he came up with these ideas.

225
00:25:14,980 --> 00:25:18,090
So for some people, they'll be like, Oh yeah, jackknife.

226
00:25:18,090 --> 00:25:22,500
That makes sense. And they won't even need you to justify anything else because they know the jackknife so well.

227
00:25:22,980 --> 00:25:30,210
But for you, I'm just letting you know in case you see the term that this is one of those examples of how to have a jackknife.

228
00:25:32,130 --> 00:25:35,370
So here's again the just the formula for the pseudo observation for person.

229
00:25:35,370 --> 00:25:40,740
I just remember. And we again, we need to.

230
00:25:42,070 --> 00:25:47,690
Only use to observations for appropriate maximum possible towers.

231
00:25:47,710 --> 00:25:49,930
How does that work in the regression setting?

232
00:25:51,650 --> 00:26:02,910
So we need both the maximum possible tao to be okay for this whole cohort, but also for the cohort without personally.

233
00:26:03,320 --> 00:26:10,250
So the maximum possible towers kind of are going to be driven by the maximum possible Tao for,

234
00:26:10,970 --> 00:26:16,190
you know, all of the possible data sets where you take out one person at a time.

235
00:26:17,000 --> 00:26:25,610
So it must be a valid choice for estimating the restricted mean survival time without personal unit for every single possible person you could remove.

236
00:26:27,340 --> 00:26:30,129
And so the nice thing is, you know,

237
00:26:30,130 --> 00:26:36,700
it doesn't cause too much of a problem because at least one of the two largest event times is going to be included in this thing.

238
00:26:36,730 --> 00:26:39,280
So you only remove one person at a time.

239
00:26:39,880 --> 00:26:45,700
So when you're figuring out the maximum possible tally, you only need to think about the largest two values now.

240
00:26:46,510 --> 00:26:52,510
And it's going to be the next to largest value that's going to be restrictive.

241
00:26:52,510 --> 00:26:59,350
So there's no restriction on tower at the largest two event times are both, you know, the event you're modeling like deaths.

242
00:27:00,280 --> 00:27:10,090
So in that case, the curve will go down to zero and there's it'll be valid for every possible data set.

243
00:27:10,600 --> 00:27:14,440
Even if you're removing one of the largest two values,

244
00:27:14,440 --> 00:27:17,169
you're still going to have a curve estimate that goes down to zero and there

245
00:27:17,170 --> 00:27:21,400
won't be a restriction for either that does largest to people that are removed.

246
00:27:23,500 --> 00:27:27,250
And if both of the largest two event times are censored,

247
00:27:27,610 --> 00:27:33,250
then the maximum possible total is going to be the smaller of those two values because there will be one data set

248
00:27:33,790 --> 00:27:42,430
where the largest censored values are removed and you'll only be able to estimate up to the next two largest value.

249
00:27:44,980 --> 00:27:54,100
And if only one of the largest to event times is censored, then the there should be a ban here.

250
00:27:54,100 --> 00:28:03,600
Then the maximum possible total is that value. So it doesn't.

251
00:28:03,870 --> 00:28:14,040
So in the regression setting, using these pseudo observations doesn't really restrict the amount of choices for the maximum possible tao.

252
00:28:14,070 --> 00:28:22,830
It nudges it slightly down from a situation where you weren't using any covariates at all, but it's still pretty reasonable.

253
00:28:24,650 --> 00:28:26,840
So when I was first learning this method,

254
00:28:27,020 --> 00:28:33,110
the thing that helped me understand this intuition the most was to again go back to the no censoring case for me.

255
00:28:33,110 --> 00:28:37,160
I always want to go back to a simple case. I understand. Well, see what it's doing.

256
00:28:37,820 --> 00:28:44,510
So if you have no censoring and you're trying to calculate these pseudo observations, then what are the ingredients?

257
00:28:44,510 --> 00:28:49,430
We need to know the sample size because we have end times the research.

258
00:28:49,430 --> 00:28:50,899
You mean for the whole cohort minus?

259
00:28:50,900 --> 00:28:57,950
And what is one times the restricted mean where you take out person I but you also need to have these restricted mean survival time.

260
00:28:57,950 --> 00:29:01,340
So this is the way it would look for the whole whole cohort.

261
00:29:01,940 --> 00:29:07,729
If you don't have censoring it's just the the sum of the minimum of 20 over.

262
00:29:07,730 --> 00:29:15,400
And so we saw that done by hand once. Right. And taking out person.

263
00:29:15,430 --> 00:29:17,570
I I'm making just, you know,

264
00:29:17,740 --> 00:29:25,950
the same notation I made up where it's just the average of the mentality across everybody in the data set except for personal eyes.

265
00:29:25,960 --> 00:29:31,540
That's this minus eyes taking out personal. And so the formula for that would look something like this.

266
00:29:31,540 --> 00:29:39,489
It's the sum over everybody in the data set. Unless your person I their mental tie and then they're at their total number of

267
00:29:39,490 --> 00:29:42,970
people that you're averaging is now and minus one when you take out person I.

268
00:29:44,960 --> 00:29:53,090
And so the pseudo observation for person I when you kind of put all these ingredients together is end times this first.

269
00:29:54,370 --> 00:30:01,520
Uh, average over everybody. Minus and minus one times the average over everybody but person I.

270
00:30:02,360 --> 00:30:12,860
And so you can kind of see that these ends cancel out in the first term and the end minus ones cancel out in the second term.

271
00:30:13,220 --> 00:30:22,310
So you're looking at the difference between everybody's mentality and everybody's mentality except personally.

272
00:30:22,610 --> 00:30:27,230
So the only one that doesn't cancel is person eyes mentality I.

273
00:30:28,360 --> 00:30:35,230
So what does this all mean? So what? This means that a pseudo observation, if you don't have any censoring at all in your data set,

274
00:30:35,560 --> 00:30:43,810
a pseudo observation is identical algebraically to the mentality for that person that you wanted to analyze.

275
00:30:45,840 --> 00:30:52,770
So if you have sensor data and you create pseudo observations, you're just replacing the entire observed data set.

276
00:30:52,840 --> 00:31:00,360
It's you haven't changed anything at all. The pseudo observations are going to algebraically turn out to be the original data set values.

277
00:31:01,580 --> 00:31:08,180
Right. That's very, very comforting. But it also kind of explains how when you do have censoring,

278
00:31:08,660 --> 00:31:15,590
you can do these same calculations where we're use area under kaplan-meier curves and they're

279
00:31:15,590 --> 00:31:24,260
trying to estimate as close as they can the data set value that would have been observed.

280
00:31:25,270 --> 00:31:27,880
It's not perfect, but that's the goal of what they're trying to do.

281
00:31:31,120 --> 00:31:38,530
Oh, and I guess I said this out loud, but it's in your hand out to pseudo observation reduces to the actual data value that you wanted to model.

282
00:31:39,010 --> 00:31:40,240
If you don't have censoring.

283
00:31:42,550 --> 00:31:49,780
So when there are sensory observations, pseudo observations have the same conditional expectation as the original data values,

284
00:31:50,320 --> 00:31:54,040
but they aren't necessarily consistent with the observed data any more.

285
00:31:54,550 --> 00:31:58,360
So you don't have this, you know, when you've got the kaplan-meier curve involved,

286
00:31:58,360 --> 00:32:04,990
you suddenly don't have this nice easy algebra where everybody cancels but the data value, you know, mentality.

287
00:32:05,560 --> 00:32:14,170
But it's pretty close. So when you actually look at the pseudo observations, they can be smaller than or greater than the observed event times,

288
00:32:14,170 --> 00:32:17,800
even if that event time was completely observed for that person.

289
00:32:18,190 --> 00:32:22,250
The pseudo observations are going to jiggle it around a little bit on average.

290
00:32:22,360 --> 00:32:29,830
The means going to be the same as if it wasn't censored, but it does add this kind of extra little variability thing when there's sensor data.

291
00:32:31,430 --> 00:32:38,659
So pseudo values may be less than the observed censoring times, which is a little bit annoying because, you know,

292
00:32:38,660 --> 00:32:47,930
in the original dataset you got to see that an event time was at least this big pseudo observations aren't necessarily making that.

293
00:32:49,010 --> 00:32:54,670
Consistently happen. On average, though, it's going to be correct across the data set.

294
00:32:54,680 --> 00:33:00,950
So the betas, the parties are going to be the important thing that you analyze after you convert everything, the pseudo observations.

295
00:33:01,250 --> 00:33:06,110
And those babies are going to act like the betas that you would have gotten if you didn't have censoring.

296
00:33:08,360 --> 00:33:11,419
So I don't want you to worry too much about these little details.

297
00:33:11,420 --> 00:33:13,040
But when you look at this sort of observations,

298
00:33:13,040 --> 00:33:19,490
when you see that I want you to know that can happen and not to worry that the method is still working.

299
00:33:20,180 --> 00:33:25,819
So whatever these issues are about studio values being smaller or greater than the observed

300
00:33:25,820 --> 00:33:30,200
event times it averages out across the dataset so that the parameter estimates are valid.

301
00:33:31,390 --> 00:33:36,520
And there's been a lot of theoretical work on this that's been done. So it's a settled issue.

302
00:33:36,910 --> 00:33:46,420
Don't worry about it. Okay? I promise. So I've got a data set here that I've used for years too, so we can see an example of how this works.

303
00:33:46,420 --> 00:33:51,850
And there's both are inside software available, so I will be showing you both of those.

304
00:33:52,360 --> 00:33:56,890
So this is a data set that's ancient because I was using it even when I was a postdoc.

305
00:33:57,280 --> 00:34:09,400
So I'm telling you old and there were 1229 patients in this breast cancer study that were either treated with long or short duration chemotherapy.

306
00:34:09,880 --> 00:34:13,080
And so they were trying to decide, you know, are.

307
00:34:14,290 --> 00:34:19,239
Does the lung, the lungs. Duration, chemotherapy is going to be more unpleasant.

308
00:34:19,240 --> 00:34:22,720
Are you actually living longer when you do that? That seems important.

309
00:34:23,920 --> 00:34:29,020
So the outcome variables are always for over survival time in months.

310
00:34:29,110 --> 00:34:35,470
So months is the unit. And I know whenever I see OS, I think of like bone because I think that's an abbreviation for bone,

311
00:34:35,830 --> 00:34:41,650
but it's actually overall survival and the overall survival sensory indicators just OS see.

312
00:34:43,520 --> 00:34:45,049
And there are covariates here.

313
00:34:45,050 --> 00:34:53,740
So we want to estimate the expected months of life over nine follow up years, which is 108 months according to these risk factors.

314
00:34:53,750 --> 00:35:02,809
So there's the treatment group Long T X stands for long duration chemotherapy and versus short duration.

315
00:35:02,810 --> 00:35:10,730
And for all of these, it's going to be a one if you actually have that condition.

316
00:35:10,730 --> 00:35:14,270
So it's going to be a one if they run the long duration therapy and zero.

317
00:35:14,270 --> 00:35:22,669
Otherwise there's this e.R. Underscore post that stands for er status positive versus negative.

318
00:35:22,670 --> 00:35:29,600
So it'll be a one if they have that positive er status size and a score GTI in disguise underscore

319
00:35:29,600 --> 00:35:35,719
two is a variable that says whether the tumor size was larger than two centimeters or not.

320
00:35:35,720 --> 00:35:42,830
So it's a one if that's true, zero otherwise. And then they have node group for the number of positive nodes.

321
00:35:42,830 --> 00:35:48,080
It was a categorical variable 1 to 3 depending on how many positive nodes you have.

322
00:35:48,590 --> 00:35:52,490
And then age ten is age measured in decades.

323
00:35:53,150 --> 00:35:58,790
And so a unit increase in age ten is going to correspond to an increase of a decade in age.

324
00:36:01,170 --> 00:36:11,280
And here's just a little bit of a summary of restricted means survival time analysis for this data set.

325
00:36:11,580 --> 00:36:16,139
I haven't done anything with modeling the multivariable parts of it,

326
00:36:16,140 --> 00:36:24,209
but this is kind of like the handout ten version of things where you kind of use proc life test and this arm t command.

327
00:36:24,210 --> 00:36:33,930
So this is something that you did in the earlier homework, maybe homework four and you've got your time with o-C Strata, the long treatment group.

328
00:36:33,930 --> 00:36:36,960
So here's kind of the Kaplan-meier curves for those two groups.

329
00:36:37,470 --> 00:36:40,020
And so the long treatment group has got is in red.

330
00:36:40,500 --> 00:36:48,570
So they have better survival over time compared to the short duration and the curves are kind of coming together at the end.

331
00:36:48,990 --> 00:36:52,380
And there's actually it's not because of necessarily lack of data.

332
00:36:52,530 --> 00:36:59,520
There's a lot of data here and they really are kind of coming close to each other in the end.

333
00:36:59,970 --> 00:37:08,370
So over however many years, whatever the benefit of that long duration treatment seems to be petering out towards the end here.

334
00:37:08,760 --> 00:37:12,870
So this isn't necessarily a shape you would associate with proportional hazards.

335
00:37:13,470 --> 00:37:18,299
So it kind of motivates using a different regression modeling approach.

336
00:37:18,300 --> 00:37:24,160
So restrictive means makes sense. Here's actually the output from, you know,

337
00:37:24,180 --> 00:37:29,129
the restricted mean survival time option from proc life test so that for the long

338
00:37:29,130 --> 00:37:37,709
duration treatment on average they lived 82.6 ish months and over that follow up period.

339
00:37:37,710 --> 00:37:47,040
Right and then in the short duration group, they lived 75.2 ish months over that again over that 108 month follow up period.

340
00:37:47,040 --> 00:37:55,650
Because the Tao is always important in this story, they're only looking at the restrictive mean up to that time.

341
00:37:57,240 --> 00:37:59,820
And the difference between the curves is very, very significant.

342
00:38:00,360 --> 00:38:06,690
You know, so there's estimates and p values and things that you would have used based on hand out ten.

343
00:38:08,090 --> 00:38:15,889
And this is just copying that again. So confidence intervals for the long duration chemotherapy group.

344
00:38:15,890 --> 00:38:22,940
And in SAS, you're kind of calculating that yourself. So this is the estimate plus or -1.9, six times the standard error.

345
00:38:23,420 --> 00:38:31,760
So you can get those. And the short duration chemotherapy group, kind of a similar calculation with that standard error over here gives you these.

346
00:38:33,110 --> 00:38:45,830
And the restricted mean difference. You can also get that estimate and confidence interval down here.

347
00:38:46,100 --> 00:38:50,830
So in that instance, you kind of had to do this by hand and ah, they'll calculate this for you.

348
00:38:53,680 --> 00:39:00,160
So in our similar code, again, just reviewing what we didn't hand out, ten were the only covariates, the treatment group.

349
00:39:01,810 --> 00:39:02,799
This is so nice.

350
00:39:02,800 --> 00:39:12,640
You get that the confidence limits are given to you and even the area between the curves are given to you with confidence, limits and p values.

351
00:39:12,640 --> 00:39:16,600
So this kind of matches up with what you had to do by hand if you were a SAS user.

352
00:39:19,030 --> 00:39:25,830
All right. So so these are kind of the numbers that we would be paying attention to if we wrote manuscript worthy sentences.

353
00:39:26,580 --> 00:39:30,530
And so here is what we would have done based on Handout ten alone.

354
00:39:31,440 --> 00:39:37,650
So something along the lines of this that in unadjusted analysis for mortality during nine years of follow up,

355
00:39:38,040 --> 00:39:47,279
those taking a long versus short duration chemotherapy lived an estimated 82.6 months and 75.2 months.

356
00:39:47,280 --> 00:39:51,000
And empathetically I have their confidence limits respectively.

357
00:39:51,660 --> 00:40:00,030
The average gain of 7.4 months of life for those in the long duration chemotherapy arm was statistically significant with confidence interval P-value.

358
00:40:03,780 --> 00:40:07,079
So now that was all review.

359
00:40:07,080 --> 00:40:13,320
So consider yourself kind of such studied again for that part of survival analysis.

360
00:40:13,680 --> 00:40:17,040
But now we have these tools to do regression.

361
00:40:17,370 --> 00:40:21,840
So how does that work? How did I not fix this animation?

362
00:40:21,880 --> 00:40:24,390
Okay, so there's the output, but I haven't given you code yet.

363
00:40:24,900 --> 00:40:33,840
So here's this is creating the variables of interest that I described earlier and using proc r.

364
00:40:33,840 --> 00:40:38,460
M. S t. RG So rejection means real time regression.

365
00:40:39,420 --> 00:40:44,340
We put in our Tao and I just put in Tower of 108.

366
00:40:46,510 --> 00:40:50,110
I think that the data actually might have gone a little bit longer than that.

367
00:40:50,110 --> 00:41:00,189
But I just put in this round number four years and we have the class statement is going to be used for Node group,

368
00:41:00,190 --> 00:41:06,520
but all the other variables are binary and you can choose to either have them in the class statement or not, but.

369
00:41:07,560 --> 00:41:09,510
They're already coded as zero one variables.

370
00:41:09,510 --> 00:41:15,120
So I just put the node group only in there with the reference group of one for the smallest number of positive nodes.

371
00:41:16,110 --> 00:41:21,299
Here. The here's the model statement. The model statement looks just like you would do for Cox models.

372
00:41:21,300 --> 00:41:26,080
Really? I accept you now you have to choose a link.

373
00:41:26,890 --> 00:41:32,620
And so this first output is related to doing a linear link.

374
00:41:36,350 --> 00:41:40,010
And what is his method? Peavy. I think it.

375
00:41:41,270 --> 00:41:45,820
I think it's just for pseudo, this must be PV for pseudo value.

376
00:41:46,490 --> 00:41:52,040
So I don't know what the other options are because I haven't just looked at this code in detail.

377
00:41:52,040 --> 00:41:55,070
But this is doing the pseudo observation approach that I've been talking about.

378
00:41:56,940 --> 00:42:00,360
And size does fit a model in the background.

379
00:42:01,380 --> 00:42:07,050
Following the advice from the paper, we're really that was only mentioned in the paper to satisfy one reviewer.

380
00:42:07,060 --> 00:42:15,750
So if you just used a regular generalized linear model, assuming normally two submitted outcomes independence, it would give you very similar results.

381
00:42:17,220 --> 00:42:27,150
All right. So here's the output and the parameter estimates over here, confidence limits and P values and so on.

382
00:42:27,150 --> 00:42:32,520
So there's a lot of interesting stuff going on in this data set to interpret.

383
00:42:32,970 --> 00:42:40,680
The treatment is still significant and this is what will kind of interpret these parameter values in the minutes.

384
00:42:40,730 --> 00:42:50,910
But by giving the game away, the treatment effect is still significant and that months of life saved over 108 months of follow up.

385
00:42:50,910 --> 00:42:56,820
It looks pretty similar to what we saw in the unadjusted analysis that's it's still in that ballpark.

386
00:42:58,950 --> 00:43:06,930
The more nodes you have that are positive, the worse things go for you, for your restricted mean survival time.

387
00:43:07,290 --> 00:43:12,060
That's what these negative values are saying. And that matches intuition.

388
00:43:12,930 --> 00:43:22,530
If you are, are. If you have an er positive tumor, then you live on average a few months longer and greater.

389
00:43:22,530 --> 00:43:25,590
Tumor size isn't great for you. Negative is bad.

390
00:43:26,130 --> 00:43:30,540
And if you are older, this is marginally significant.

391
00:43:30,540 --> 00:43:38,910
But if you're older, it looks like you tended to live a little longer by a touch for every decade older you were.

392
00:43:39,390 --> 00:43:49,280
So here's the fitted model, and this interpretation of the model is going to be almost like studying for the exam as well, right?

393
00:43:49,290 --> 00:43:56,489
Because it's all look the same. So if you have an identity link here where you're just estimating the mean of

394
00:43:56,490 --> 00:44:00,600
this restricted survival time and you want to interpret some of these things,

395
00:44:00,600 --> 00:44:10,700
let's just get a little practice in. So using what you know about interpreting models with different links with and

396
00:44:10,700 --> 00:44:13,790
don't look at your handout for the answer because the handbook will be there.

397
00:44:14,000 --> 00:44:18,989
The answer will be there. Don't look. Okay, just look up. So what's the 108 month research?

398
00:44:18,990 --> 00:44:24,320
You mean survival time based on this model? If you have a 50 year old.

399
00:44:25,360 --> 00:44:34,750
With long duration chemotherapy, zero positive nodes and an ER negative tumor that was less than two centimeters wide.

400
00:44:35,050 --> 00:44:39,130
So that's giving you all the information on the coverage in this model.

401
00:44:39,580 --> 00:44:50,230
So if you're trying to estimate the 108 month restricted mean survival time for this type of a person, you know, what are the what's the formula?

402
00:44:50,290 --> 00:44:54,790
So I'm going to take you through each of these terms. You tell me, is it in the formula or not in the formula?

403
00:44:55,750 --> 00:45:06,330
All right. So is the intercept in this formula, yes or no? It is in the formula because we're not comparing to people or stuff cancels out.

404
00:45:06,570 --> 00:45:10,080
We're estimating outcome for a single person.

405
00:45:10,530 --> 00:45:14,700
So the intercept has to be there if you're estimating the outcome for a single person.

406
00:45:15,360 --> 00:45:21,570
Right. So when you're comparing two different types of people, the intercept will cancel out in that comparison.

407
00:45:22,050 --> 00:45:26,460
But when you're estimating something for a single person with different characteristics,

408
00:45:26,760 --> 00:45:30,510
no matter what kind of model you're using, the intercept is going to be there.

409
00:45:30,660 --> 00:45:37,380
Okay, so good. Do we need this parameter for the long duration treatment?

410
00:45:39,890 --> 00:45:44,530
What kind of treatment is this person on? They have long duration treatment.

411
00:45:44,550 --> 00:45:48,820
So does this parameter go in there or no? Yes.

412
00:45:48,830 --> 00:45:52,909
It goes in there because it's coded as a one of their own long duration treatment.

413
00:45:52,910 --> 00:46:01,400
So this will have to be in there because there will be this number times of one for that person to get this estimated outcome over here.

414
00:46:02,090 --> 00:46:07,310
What about the Node Group two and the Node Group three?

415
00:46:08,580 --> 00:46:15,870
So just as you recall, no group one is the one that had zero positive nodes.

416
00:46:17,070 --> 00:46:21,920
So do we need these parameters to estimate? The outcome for this person?

417
00:46:22,640 --> 00:46:30,290
No, because this indicator that they have more positive knows both of these indicators will be zero.

418
00:46:30,290 --> 00:46:35,120
So these won't be numbers that show up in your formula for estimating this outcome for that person.

419
00:46:35,960 --> 00:46:39,020
All right. What about this, er, positive thing?

420
00:46:39,770 --> 00:46:44,540
So says here we're looking at someone with a er negative tumor.

421
00:46:44,570 --> 00:46:48,860
So this is going to be coded as a zero for this person.

422
00:46:49,130 --> 00:46:52,380
Right. And so this parameter won't be used. They won't.

423
00:46:52,400 --> 00:46:55,910
That won't be in the formula size greater than two.

424
00:46:57,990 --> 00:47:01,440
This person had less than two centimeters wide. So do we need this one?

425
00:47:02,160 --> 00:47:05,220
No, we don't, because it'll be multiplied by zero for this person.

426
00:47:05,910 --> 00:47:09,959
Right. Hey, perk up. This is the tricky one. The only tricky one in this one.

427
00:47:09,960 --> 00:47:14,370
And that's this age ten variable. So this person is 50.

428
00:47:14,970 --> 00:47:18,370
So what is the formula for the person for this term here?

429
00:47:21,680 --> 00:47:26,170
So part one. Do you need at all this parameter?

430
00:47:26,530 --> 00:47:31,259
Yes or no? We need the parameter.

431
00:47:31,260 --> 00:47:36,720
They had some age and we're not comparing anything, so it's not going to be canceling out with some other thing.

432
00:47:37,170 --> 00:47:40,680
So we're going to need it. So what do we plug in for this age?

433
00:47:40,680 --> 00:47:45,520
Ten in our formula? Oh.

434
00:47:45,540 --> 00:47:48,900
Was that you? You were telling me the answer. I thought you were raising your hand.

435
00:47:48,950 --> 00:47:53,040
Yeah, but I always, you know.

436
00:47:53,310 --> 00:48:02,250
Yeah, that worked for both, actually. Very efficient of you. So, yeah, you plug in five because we had coded age ten to be age in decades.

437
00:48:02,250 --> 00:48:06,720
So a 50 year old would have a value of five for this. So the formula.

438
00:48:07,890 --> 00:48:15,000
Would involve these terms that red. Right, mainly because these other covariates were zero for that person.

439
00:48:16,430 --> 00:48:23,569
And when we put in the parameter estimates, that's the estimate down here,

440
00:48:23,570 --> 00:48:35,750
we plugged in a five for the ten value and that person would live on average 91.8 months over a 108 day follow up period,

441
00:48:35,750 --> 00:48:44,450
which is actually pretty decent. You know, you've lived most on average, you haven't lived the entire follow up period, but you do pretty well.

442
00:48:46,460 --> 00:48:52,310
All right. So again, I just have the fitted model here, but and don't look down too hard at your slide because you'll see the answer.

443
00:48:52,310 --> 00:48:57,350
But the question now is what's the average of months?

444
00:48:58,560 --> 00:49:08,310
Of life gained over eight months when using long versus short duration chemo, adjusted for other factors in the model.

445
00:49:09,450 --> 00:49:20,670
So there's. So key element of this question first is that you're comparing something you're comparing long versus short duration chemotherapy,

446
00:49:21,840 --> 00:49:26,400
but nothing else about risk profile been said other than you're adjusting.

447
00:49:27,440 --> 00:49:33,679
So you're assuming that these whoever you're comparing, they have identical node group variable.

448
00:49:33,680 --> 00:49:41,180
They have identical error, positive value, they have identical age, they have identical size greater than two.

449
00:49:41,690 --> 00:49:47,560
The only thing that's different is whether they're on a long versus short duration chemotherapy.

450
00:49:48,530 --> 00:49:54,320
So I'm going to do the same thing. Do we need the intercept term in our formula?

451
00:49:56,380 --> 00:50:02,650
No, we don't because it's going to cancel out when we're comparing the long and the short because they'll both have the intercept.

452
00:50:03,760 --> 00:50:10,530
Do we need this 7.6198 variable number in our formula?

453
00:50:12,220 --> 00:50:16,600
Absolutely. We do need this because it's going to be this variable.

454
00:50:16,600 --> 00:50:24,489
It's going to be a one when we're estimating the life for the long duration therapy person.

455
00:50:24,490 --> 00:50:30,370
And it's going to be a zero when we're estimate for the short duration chemotherapy person.

456
00:50:30,730 --> 00:50:38,200
So it won't cancel. We're going to need this term and we're going to have a one when we take the first

457
00:50:38,530 --> 00:50:42,160
type of person and subtract out the second time a person with short duration.

458
00:50:43,730 --> 00:50:47,959
All right. Everything else in the mall, we're talking about a person that has similar covariates.

459
00:50:47,960 --> 00:50:52,040
So do we need this -12.7?

460
00:50:52,730 --> 00:50:58,280
No. It cancels out whatever this value was. It doesn't even matter if it was a one or a zero for this.

461
00:50:58,640 --> 00:51:05,900
Same here. Doesn't matter if it was a one or a zero for this where it's the same type of person in the first estimate.

462
00:51:05,900 --> 00:51:09,440
In the second estimate that we're comparing. So we don't need this.

463
00:51:09,440 --> 00:51:12,710
We don't need this. We don't need this or this or this.

464
00:51:13,370 --> 00:51:17,330
Everything cancels out because the rest is assumed to be the same.

465
00:51:18,830 --> 00:51:23,600
For the profile with the long treatment versus the short treatment adjusted for other stuff.

466
00:51:25,520 --> 00:51:31,760
So the only thing that's left is this long duration treatment parameter.

467
00:51:33,260 --> 00:51:46,540
And so on average, you gain 7.6 ish months of life if you had the long duration chemotherapy and were followed for that 108 months.

468
00:51:48,200 --> 00:51:56,020
All right. And so, of course, this interpretation really depends on that 108 months, because if you change,

469
00:51:56,260 --> 00:52:06,130
if you had further data and you were to go out to, you know, a hundred years, it would you would have accrued a lot more life savings.

470
00:52:06,550 --> 00:52:14,680
So the interpretation for these tenants is absolutely have to have the value of tile that you used in your analysis to be interpretable.

471
00:52:15,220 --> 00:52:19,840
So at 7.6 out of 108 months followed, that was gained.

472
00:52:20,930 --> 00:52:22,670
For this long duration group.

473
00:52:24,540 --> 00:52:31,769
And so if we want to have confidence limits and p values for these kinds of interpretations, we need our estimate statements.

474
00:52:31,770 --> 00:52:36,360
And fortunately, we worked through what the formulas were already.

475
00:52:36,390 --> 00:52:43,110
So here is the first example where we were looking at the 108 month erm as t for a

476
00:52:43,110 --> 00:52:47,219
50 year old with long duration chemo's zero positive no zero negative tumor size.

477
00:52:47,220 --> 00:52:51,330
That was small. And when we did that formula we needed the intercept,

478
00:52:51,420 --> 00:52:58,650
we needed the lung treatment and we needed the age ten parameter multiplied by five for the 50 year old.

479
00:53:00,450 --> 00:53:05,309
And then this second estimate statement's going to be the average months of life gained

480
00:53:05,310 --> 00:53:09,570
over 108 months when using long versus short duration chemo just for other factors.

481
00:53:09,570 --> 00:53:13,860
And the only parameter we needed was this long treatment.

482
00:53:14,780 --> 00:53:19,399
Parameter. And so here are the results.

483
00:53:19,400 --> 00:53:21,049
And we did this by hand.

484
00:53:21,050 --> 00:53:26,900
It's always nice if you can do it by hand, because then you can automatically check your estimate statement coding to make sure you got it right.

485
00:53:26,900 --> 00:53:29,299
That's always very comforting. So here's the estimate.

486
00:53:29,300 --> 00:53:35,450
We estimates we got by hand for those two cases, but now we have confidence limits and we have P values.

487
00:53:35,450 --> 00:53:38,930
So if we wanted to write sentences about these, we can do so.

488
00:53:42,110 --> 00:53:49,070
So here's a manuscript worthy sentence using the identity link with tasks that over a nine year follow up period,

489
00:53:49,370 --> 00:53:56,930
patients on the long duration chemotherapy arm lived an average of 7.6 months longer than those on the short duration chemotherapy arm,

490
00:53:57,250 --> 00:54:02,240
an analysis that also adjusted for age era status, tumor size and the number of positive nodes.

491
00:54:02,360 --> 00:54:12,319
Confidence interval P-value. And this would be an odd sentence to put in the manuscript because we don't usually talk about one patient at a time.

492
00:54:12,320 --> 00:54:15,640
But, you know, here's an example of what you could write for that.

493
00:54:15,650 --> 00:54:23,000
So for a 50 year old breast cancer patient with zero positive notes and an er negative tumor less than two centimeters in diameter,

494
00:54:23,360 --> 00:54:30,860
the estimated number of months lived during the 108 follow up months was 91.9 months when treated with long duration chemotherapy.

495
00:54:31,070 --> 00:54:34,440
Confidence. Interval. P value. Oh.

496
00:54:34,680 --> 00:54:40,290
And take out this word longer because that doesn't fit there. It's I think that's just a copy paste error.

497
00:54:40,980 --> 00:54:45,270
So just cross out longer because you're just talking about one person.

498
00:54:45,270 --> 00:54:48,630
You're not comparing anything in the sentence. Sorry about that.

499
00:54:49,780 --> 00:54:55,730
Can't believe I didn't see that before. All right.

500
00:54:57,920 --> 00:55:05,600
So you can actually look at the pseudo observations and there's no reason you would have to do this.

501
00:55:05,600 --> 00:55:14,930
But I thought you might be curious. So here is saving out the pseudo observations and printing them so that you can look at them.

502
00:55:15,680 --> 00:55:25,720
So. Actually, there might be a reason you'd want to save and use them because you could do things like heat maps or something with these.

503
00:55:25,990 --> 00:55:32,469
And it's as if you're looking at heat maps that are kind of roughly getting at the distribution of the outcomes in the data.

504
00:55:32,470 --> 00:55:36,220
So you can actually use this to do observations for more than just the modeling.

505
00:55:36,610 --> 00:55:40,690
You can print them out and kind of plot them and, you know, just kind of get an idea.

506
00:55:41,020 --> 00:55:47,709
And so this is The O.C. was the actual, you know, X I value.

507
00:55:47,710 --> 00:55:58,690
And here's the Delta IV value in the Sierra observation is kind of it's kind of similar to the observed value, but with a little wiggle room.

508
00:55:58,690 --> 00:56:04,030
So you'll see things like a pseudo observation that's less than the observed value and all that kind of stuff.

509
00:56:04,450 --> 00:56:07,720
And so they're just a little bit off from what you observe.

510
00:56:09,070 --> 00:56:22,629
So looking at this, just briefly, this person who actually lived 109.8 months, when you get their pseudo observation,

511
00:56:22,630 --> 00:56:31,150
it seems like their their lifetime based on their covariance, maybe was a little bit longer, you know.

512
00:56:32,670 --> 00:56:42,410
Uh. No other really interesting, censored values to interpret here, but this one must have had covariates that,

513
00:56:42,920 --> 00:56:47,150
you know, made them seem like they would have lived longer after censoring.

514
00:56:52,040 --> 00:56:55,790
Same deal for our, uh. In our.

515
00:56:57,610 --> 00:57:07,810
There's a little bit more code here. The strategy is going to be to create the pseudo observations with this pseudo mean function.

516
00:57:08,910 --> 00:57:16,120
So everything is in this package. Pseudo. And so you're going to get the pseudo observations.

517
00:57:16,930 --> 00:57:26,110
I'm calling them PEO, just like I did in the handout with code like this with the three main function and this is what they look like.

518
00:57:26,110 --> 00:57:30,639
So this is again the EXI variable, the Delta IV variable.

519
00:57:30,640 --> 00:57:39,070
And now here's the same pseudo observations and these pseudo observations, then you put it into the next function.

520
00:57:39,820 --> 00:57:47,770
So I'm going to use G Pack just like everybody else using G, even if you don't need it.

521
00:57:50,800 --> 00:57:58,090
Although I think I attached our code that shows you what it looks like if you don't use G and just use a regular generalized linear model.

522
00:57:58,090 --> 00:58:02,230
If you want to see that it's in it's on canvas. But I didn't want to put it in the handout.

523
00:58:02,740 --> 00:58:04,000
It's very similar result.

524
00:58:04,720 --> 00:58:14,770
So you have here's a regression model, the formula and we're putting it in the GC function again just like we did when we were doing G for the class.

525
00:58:16,660 --> 00:58:20,290
And your homework that you're working on is doing this to write.

526
00:58:21,150 --> 00:58:28,220
And then the results, and I've kind of just put them in pretty format so you get the same thing you would from start.

527
00:58:28,260 --> 00:58:31,350
So the estimate, the confidence limits and the p value.

528
00:58:31,770 --> 00:58:38,220
But I've kind of maneuvered it to to look pretty. And you can kind of modify this code to do the same.

529
00:58:39,390 --> 00:58:46,380
With labels and so on. And so this is not algebraically the same as what you get in SAS.

530
00:58:46,390 --> 00:58:55,810
There's some slight difference in the algorithms and I didn't dive deep into the code for each to see exactly what the differences were.

531
00:58:56,200 --> 00:58:59,600
It's it's not about the G.

532
00:58:59,620 --> 00:59:03,399
It's there's something else that's going on, but it's close enough.

533
00:59:03,400 --> 00:59:08,590
I mean, you're getting pretty much the same estimates. They're just not algebraically the same.

534
00:59:08,590 --> 00:59:13,210
But the science is very similar, so I'm pretty comfortable with either package for doing this.

535
00:59:15,500 --> 00:59:20,240
And then for contrasts to get the same contrast statements we did in as I have a little bit

536
00:59:20,720 --> 00:59:28,220
of code here using this estimable function from the G models package just so you can sort of.

537
00:59:28,220 --> 00:59:34,820
But the contrasts are set up similarly. It's just an R, you have to put in values for every parameter, whether you need it or not.

538
00:59:35,330 --> 00:59:40,610
So this is in the order that the things were put into the model statement and, you know, the,

539
00:59:40,610 --> 00:59:44,870
the numbers we needed to multiply by each of the parameter estimates to get the formulas.

540
00:59:45,290 --> 00:59:50,870
Again, I've made it a little bit pretty so that you could see something that looked similar to self in the output.

541
00:59:53,070 --> 00:59:56,070
And there's this next part.

542
00:59:56,460 --> 01:00:00,630
If you really just like using the identity link and you liked this manuscript worthy sentences,

543
01:00:00,630 --> 01:00:05,040
you can just stick with the identity link, but people also play with the log link.

544
01:00:05,040 --> 01:00:08,820
And I had to fix this animation because you don't know what the code is for that.

545
01:00:11,250 --> 01:00:18,120
But there it is. And so the only thing I've changed here is that this link equals log.

546
01:00:18,720 --> 01:00:26,880
And in the estimate statements, if you use the log link, you have an exponential rate, all the resulting formulas to get something interpretable.

547
01:00:26,940 --> 01:00:30,090
And I'll show you how to write sentences about what you get.

548
01:00:30,860 --> 01:00:40,349
So these are now the estimates. And whenever so you've seen log links before, right, for count models and things.

549
01:00:40,350 --> 01:00:48,389
And so when we were estimating parameters from count models, it was always some kind of multiplicative effect on the estimated outcome.

550
01:00:48,390 --> 01:01:00,390
Right. So here as well with a log link, these are going to be multiplicative kind of interpretations that we need to get for the E to the betas.

551
01:01:00,780 --> 01:01:06,720
And so I'll show you how to write sentences in a second here. So some log link.

552
01:01:06,990 --> 01:01:12,299
Here's the formula. But now it's with the log link here.

553
01:01:12,300 --> 01:01:23,310
So all the estimates have shifted around. And so what's the formula for estimating the 108 month armistice for a 50 year old with long duration chemo?

554
01:01:23,310 --> 01:01:26,610
Zero positive nose, ear, negative tumor size? Listen to 70 meters.

555
01:01:27,030 --> 01:01:30,209
This is the same person. But there's nothing tricky that's changed.

556
01:01:30,210 --> 01:01:32,430
This is the same person we were talking about before.

557
01:01:33,210 --> 01:01:40,500
But now if we're trying to estimate that same quantity with the log link, you know, we're doing E to the all of the stuff we did before.

558
01:01:41,220 --> 01:01:50,520
So everything on the inside here is updating the coefficients from this model fit with the log link,

559
01:01:51,000 --> 01:01:59,400
but it's basically the same betas that we're dealing with here with the Times five or the age and decades variable,

560
01:01:59,700 --> 01:02:06,920
you know, and we're pulling off the same intercept term, treatment term, age term.

561
01:02:06,960 --> 01:02:13,710
All the others had zeros, you know, but we have to exponential now to get to that same estimate because we use the log link.

562
01:02:14,650 --> 01:02:19,670
And so we have a fairly similar result. To what we got before.

563
01:02:19,700 --> 01:02:24,260
They live a pretty long time at 108 months. Using this log link as well.

564
01:02:26,380 --> 01:02:33,820
And when. You want to interpret treatment effect?

565
01:02:34,990 --> 01:02:36,760
I changed the question a little bit.

566
01:02:37,420 --> 01:02:45,010
And then what is the formula for the fold change for a 108 month pharmacy comparing long versus short duration chemo?

567
01:02:45,370 --> 01:02:51,390
So the effect size with the log link is a fold change, a multiplicative effect when you use the log link.

568
01:02:51,400 --> 01:02:56,590
So I have to ask the question that way and it's the same parameter that's key.

569
01:02:56,950 --> 01:03:02,859
But we're now looking at E to the beta for that treatment effect and getting 1.10.

570
01:03:02,860 --> 01:03:09,820
So this is the multiplicative effect on the restrict mean survival time comparing long versus short duration.

571
01:03:11,060 --> 01:03:16,350
So in on this scale with the log rank, you can talk about, you know,

572
01:03:16,370 --> 01:03:24,650
there's a 10% longer restricted mean survival time when you're in the long duration group compared to the short duration group.

573
01:03:25,160 --> 01:03:30,889
And so, you know, this is a little bit of a different way of describing the treatment effect that we than we did with the

574
01:03:30,890 --> 01:03:36,920
identity link where we just talked about the number of months extra you lived during that follow up time.

575
01:03:36,950 --> 01:03:47,210
Now we're talking about, you know, how much longer with this 10% here you lived on the long duration therapy.

576
01:03:47,630 --> 01:03:57,860
So estimate statements are going to look pretty similar, except you have to have these exposed exponents here to get stuff we need.

577
01:03:59,150 --> 01:04:04,790
And here are the same terms that this is going to look fairly similar to what we got before.

578
01:04:05,060 --> 01:04:08,060
I didn't compare the confidence limits.

579
01:04:08,420 --> 01:04:11,749
Log Linke is making slightly different assumptions behind the scenes,

580
01:04:11,750 --> 01:04:17,569
so I don't think that these necessarily line up exactly with the 95% confidence interval you got with the identity link,

581
01:04:17,570 --> 01:04:23,870
but there's still reasonable confidence limits. 95% of the time this procedure would cover the true.

582
01:04:26,050 --> 01:04:36,370
Estimated lifetime. And then here is the of the fold change estimate that we got by hand in the confidence limits

583
01:04:36,370 --> 01:04:43,029
and p values so we can write sentences that have to do with this 10% increase in your research.

584
01:04:43,030 --> 01:04:51,409
You mean survival time. All right. And so based on the log link, you can write something like this.

585
01:04:51,410 --> 01:04:54,650
So this is a sentence that we didn't really have before.

586
01:04:54,800 --> 01:04:58,310
Based on the identity link, but we can with the log link.

587
01:04:58,310 --> 01:05:04,210
So during nine years of follow up patients taking long duration chemotherapy lived 10% longer.

588
01:05:04,310 --> 01:05:10,910
That comes from the 1.10 multiplicative effect, then those taking short duration chemotherapy.

589
01:05:10,910 --> 01:05:13,430
In an analysis that also adjusted for age, your status,

590
01:05:13,430 --> 01:05:19,489
tumor size and the number of positive nodes confidence interval P-value And look I'm just looking down,

591
01:05:19,490 --> 01:05:23,780
I forgot to take this longer out of this and that's copy and paste error.

592
01:05:24,050 --> 01:05:26,540
So just scratch out that word longer there.

593
01:05:27,060 --> 01:05:32,600
I'll try to remember to do that and repost the handout because I know that only a few people are live here,

594
01:05:32,600 --> 01:05:39,290
so maybe you know that they'll never they'll know because they'll listen to their recording, but they'll be like, Oh, this sounds perfect.

595
01:05:39,290 --> 01:05:41,540
I don't know what she's talking about. Wink, wink.

596
01:05:42,520 --> 01:05:48,370
So for a 50 year old breast cancer patient with zero positive nodes, this is the same exact sentence.

597
01:05:48,370 --> 01:05:53,050
Except I've updated the numbers based on what we got from the log link model.

598
01:05:57,640 --> 01:06:06,580
Here's Arco kind of doing the same thing. Changing the link to log and I've kind of maneuvered it so that you kind of get a nice

599
01:06:06,580 --> 01:06:10,560
table that looks like SAS that you can edit whenever you have to use this yourself.

600
01:06:12,470 --> 01:06:21,280
Uh. And.

601
01:06:22,420 --> 01:06:28,420
Why did I make what I bought? I think I just put it in a pretty table here, but I don't think I've got anything different from what was.

602
01:06:30,460 --> 01:06:35,690
Why did I make it pretty? I can't remember. This is the same as this.

603
01:06:35,930 --> 01:06:39,919
So here's the like, Frankenstein output that I put together, but I guess I don't know.

604
01:06:39,920 --> 01:06:44,120
I wanted to show it in pretty. I can't remember why I made this slide, except it's slightly prettier.

605
01:06:45,060 --> 01:06:49,670
All right. And then contrasts. Here.

606
01:06:49,760 --> 01:06:55,370
Here is the contrast for those two situations where you've got the log link involved here.

607
01:06:55,370 --> 01:07:01,430
So you have two exponential results to get the full change and the complex limits of the P value.

608
01:07:01,440 --> 01:07:06,980
So you can when you use the log like you can kind of edit this to get something pretty for your output.

609
01:07:09,100 --> 01:07:12,130
All right, so we're almost to the end of this handout, so.

610
01:07:14,690 --> 01:07:18,139
If I had had more time this morning, I think I would have you been added more slides?

611
01:07:18,140 --> 01:07:27,080
So this restricted mean model stuff and these pseudo observations, they have become very practical and there's many,

612
01:07:27,080 --> 01:07:32,959
many papers that kind of you could go from here and and, and use.

613
01:07:32,960 --> 01:07:37,700
And so I brought it forward a couple of them. Yeah.

614
01:07:37,700 --> 01:07:41,899
Yes, I am authored on both of them so they came to mind pretty quickly.

615
01:07:41,900 --> 01:07:49,040
But you can do restricted mean models and address dependent censoring are using these

616
01:07:49,040 --> 01:07:53,270
kind of approaches in the second paper is really cool because it has a software link.

617
01:07:53,840 --> 01:07:59,659
So if you really need to deal with dependent censoring and there's the approach is kind

618
01:07:59,660 --> 01:08:04,010
of using this inverse way it's we that was one of our extra topics we talked about.

619
01:08:07,660 --> 01:08:18,670
To hand out to go. Also not covered on the exam, but it's using an inverse weight approach with pseudo observations and multiple imputation actually.

620
01:08:18,880 --> 01:08:27,070
And there's software to do it kind of at this link and it hasn't been picked up or put into mainstream SAS or anything yet.

621
01:08:27,070 --> 01:08:31,149
But it's fairly recent work in the scale of when SAS actually adds things.

622
01:08:31,150 --> 01:08:34,870
And so there's that there's ways to do that.

623
01:08:34,870 --> 01:08:41,049
There's also extensions for using pseudo observations to model recurrent events.

624
01:08:41,050 --> 01:08:44,230
And if I had more time, I would have put links to that as well.

625
01:08:44,650 --> 01:08:49,870
But a quick Google, you'll see that this stuff is really taken off and there's a lot of interesting

626
01:08:49,870 --> 01:08:55,030
methods that depend on pseudo observations to analyze challenging sensor data.

627
01:08:57,070 --> 01:09:06,010
So that's more or less all I wanted to get into your tool kit for this kind of regression.

628
01:09:06,460 --> 01:09:11,020
And so my summary kind of the take is should be restrictive means evolved time regression

629
01:09:11,020 --> 01:09:15,100
methods are an attractive alternative to Cox Proportional hazards regression.

630
01:09:15,400 --> 01:09:18,400
There's no assumptions about proportional hazards required.

631
01:09:19,430 --> 01:09:30,080
So it it's it seems to be like a safer analysis in that sense that you're not going to be worried about violating proportional hazards.

632
01:09:30,470 --> 01:09:34,040
Now, there are advantages to the proportional hazards model.

633
01:09:34,520 --> 01:09:43,460
You know, there are situations where you can get a more precise estimate with, you know, smaller P values for the treatment effect.

634
01:09:43,490 --> 01:09:47,000
There are those things that can happen in special cases.

635
01:09:47,000 --> 01:09:56,390
But for the most part, this is a big win that you don't have to have that assumption and you'll still have a valid inference with these models.

636
01:09:56,750 --> 01:10:04,229
And so. When I have a new GSR ray and they already know Cox models, they tend to try that first.

637
01:10:04,230 --> 01:10:06,960
But if they do, some clots and proportional hazards don't look good.

638
01:10:07,230 --> 01:10:12,810
I send them straight to this method and we end up publishing this based on a restrictive means of time model.

639
01:10:14,400 --> 01:10:15,420
And in my opinion,

640
01:10:15,420 --> 01:10:22,650
the effect size related to restrictive mean models are more interpretable than hazard ratios available for proportional hazards models.

641
01:10:23,370 --> 01:10:27,449
Hazard ratio, I mean, you know, because you've had to deal with them.

642
01:10:27,450 --> 01:10:32,309
Hazard ratios are kind of a tricky thing to understand. And last time,

643
01:10:32,310 --> 01:10:39,870
I think we saw an example where there was a super interesting hazard ratio when we're looking at pair time to severe vision loss outcomes.

644
01:10:40,080 --> 01:10:42,150
That meant almost nothing clinically.

645
01:10:42,510 --> 01:10:50,309
And so when you look at it in terms of restricted mean months, lives saved over some period, it's much more interpretable.

646
01:10:50,310 --> 01:10:55,380
You get a much more clear idea of what is the clinical impact of differences between groups.

647
01:10:56,130 --> 01:11:02,220
So that's another way. That's another reason I think that these are going to really overtake Cox models in the future.

648
01:11:02,940 --> 01:11:07,150
So I make sure all of my graduate student research assistants know these approaches.

649
01:11:07,620 --> 01:11:14,010
And now I have given you a way to dig into this as well for your own research.

650
01:11:14,430 --> 01:11:18,030
So now you were, you know, almost everything my GSI race.

651
01:11:18,030 --> 01:11:24,510
Now, maybe, maybe not all of it, but, you know, you've gotten a lot further in these last three handouts.

652
01:11:25,440 --> 01:11:30,060
All right. So that is it. I'm going to stop the recording.

653
01:11:30,480 --> 01:11:37,230
But if you have any questions, you can come up and ask me if you're working on your homework or I'm happy to just give this time to you.

654
01:11:38,100 --> 01:11:44,900
All right. But that's all for. The last handout other than the review handout.

655
01:11:47,620 --> 01:11:48,730
It's a big moment.

