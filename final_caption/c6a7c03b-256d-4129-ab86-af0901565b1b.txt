1
00:00:00,570 --> 00:00:04,630
I know this one. All right, everybody.

2
00:00:04,680 --> 00:00:08,640
Good afternoon. Going to be 3:00 on a Wednesday.

3
00:00:10,830 --> 00:00:16,830
All right. This is already difficult for me. I checked my mark at least three times to make sure that we're supposed to be here at three and at 2:00.

4
00:00:17,040 --> 00:00:21,480
So I try to follow Tim Johnson, and I realize that's not always going to happen.

5
00:00:21,690 --> 00:00:31,319
So this should be interesting. Mile range quiz on the syllabus really, really hard.

6
00:00:31,320 --> 00:00:36,300
And one game of Farkle out of everybody at this point, hopefully.

7
00:00:37,950 --> 00:00:41,880
Right. So that's the only stipulation for this class so far.

8
00:00:42,740 --> 00:00:47,980
But this is for fun. But you do get one percentage point every time you play goal every week.

9
00:00:48,120 --> 00:00:51,780
So easy way to get like 12% of your grade.

10
00:00:53,310 --> 00:01:01,170
My question, anybody sort of the homework concerns the homework?

11
00:01:02,220 --> 00:01:06,390
No. All right. Cool. That's all right.

12
00:01:06,390 --> 00:01:09,930
We've really talked about how we ought to follow this.

13
00:01:10,170 --> 00:01:16,230
We're going to pull them to your studio.

14
00:01:17,400 --> 00:01:20,940
All right. Here is the question for today.

15
00:01:22,200 --> 00:01:25,740
Instructions. Good. All right. All right.

16
00:01:26,010 --> 00:01:36,330
Get out here. A musician or a musical group that I listen to often is and remember, multiple words have to be connected with an underscore.

17
00:01:37,050 --> 00:01:40,650
Oh, no, you don't. I'm not trying to work out. This is just a response. You don't need underscores.

18
00:01:41,130 --> 00:01:48,800
So what do you listen to? What makes you productive, makes you happy, makes you whatever.

19
00:01:49,940 --> 00:02:00,600
I'm curious. And I should do this to. Dr. Ron, I have a good question.

20
00:02:00,690 --> 00:02:09,120
You bet. Is there a way to see if we submitted the likes FARKLE surveys every week, just in case I forget if I did it or not.

21
00:02:09,380 --> 00:02:14,460
You know, I will tell you what I will check for this week. And if I don't see somebody, I let them know.

22
00:02:15,210 --> 00:02:18,330
But I checked the other day and there were like 50 people who had done it.

23
00:02:18,340 --> 00:02:22,200
So odds are it's submitting. If you do the four.

24
00:02:22,660 --> 00:02:39,200
Okay, I'll let people know. Is it?

25
00:02:47,820 --> 00:02:52,950
Unit uses.

26
00:03:05,300 --> 00:03:22,250
It's easy cases. Who's on the charts.

27
00:03:33,000 --> 00:03:36,670
Come on, folks. Thanks for the show.

28
00:03:40,020 --> 00:03:43,020
I'm really creative because it's hard to find buyers at 653.

29
00:03:43,080 --> 00:03:48,540
They would be so proud to be.

30
00:03:53,000 --> 00:04:07,840
Since house prices rose.

31
00:04:11,910 --> 00:04:23,920
Tough on China.

32
00:04:23,920 --> 00:04:34,690
Here we go, area 41. One more you haven't done to do it.

33
00:04:40,060 --> 00:04:44,150
Know. There we go.

34
00:04:44,610 --> 00:04:49,640
All right. Let's start with all the way through the Senate.

35
00:04:51,880 --> 00:04:55,670
All right. Let's see what we can.

36
00:05:00,740 --> 00:05:06,500
Oh. Is it going to go this slow? Oh, my God. That's interesting.

37
00:05:08,280 --> 00:05:12,070
Right to speak.

38
00:05:12,440 --> 00:05:30,500
Just a moment. Do you know where this place is?

39
00:05:32,240 --> 00:05:35,600
Right here. Oh. Now go out there.

40
00:05:42,380 --> 00:05:52,060
But, you know, honestly, I think you're just really just.

41
00:06:02,240 --> 00:06:07,400
I just want to show this car.

42
00:06:08,810 --> 00:06:25,980
I think it's kind of interest to you because you have something to do.

43
00:06:28,560 --> 00:06:43,760
Oh, seriously, you. It's going to be.

44
00:06:47,220 --> 00:06:55,980
This is how I see myself.

45
00:06:57,570 --> 00:07:03,820
Do you keep.

46
00:07:05,250 --> 00:07:19,360
James Taylor is the leader and it's going at this show.

47
00:07:19,900 --> 00:07:35,160
But I think we need to how to shoot those moments here.

48
00:07:36,200 --> 00:07:44,250
How long can we.

49
00:07:50,810 --> 00:08:02,450
Thanks to BBC still around the country.

50
00:08:05,870 --> 00:08:10,270
Okay, great.

51
00:08:11,970 --> 00:08:16,430
So I just heard her name.

52
00:08:16,430 --> 00:08:23,140
Beethoven. I know you don't want to mention your couple.

53
00:08:40,230 --> 00:08:45,190
My husband is very.

54
00:08:54,440 --> 00:09:09,760
You can call for the money at the end.

55
00:09:09,830 --> 00:09:16,820
There you go. Savings. So. Oh, well, it's good to know.

56
00:09:16,910 --> 00:09:24,610
I am always interested in, uh, what kind of music people listen to, because it's a big part of national security checks and stuff.

57
00:09:25,760 --> 00:09:30,920
Did you submit your answer? I did. I was the 87 person musical on Broadway.

58
00:09:31,400 --> 00:09:42,220
Most incredible thing I've ever seen. So if you ever get a chance to see Hadestown again, it's a Greek tragedy told in a bar, but modernized.

59
00:09:43,610 --> 00:09:50,960
It's amazing anyway, but I knew left in 80% of these people, so I don't feel so old.

60
00:09:52,010 --> 00:09:55,479
Thank you. All right, I'm going.

61
00:09:55,480 --> 00:09:59,600
We'll look some of those up when I have time. Let's do some statistics now.

62
00:10:00,860 --> 00:10:06,400
Let's go back to 601 at 601.

63
00:10:07,070 --> 00:10:15,160
You've all played Farkle now. Was structured is inadequate.

64
00:10:15,780 --> 00:10:20,850
When I roll all six states in the game of Farkle, what's the probability that I get nothing?

65
00:10:21,750 --> 00:10:26,610
What do you think? The first time you play Farkle, all the dice come up.

66
00:10:26,610 --> 00:10:30,210
What's the probability? You get a zero and it goes to the other player?

67
00:10:34,080 --> 00:10:37,260
0% to one or 2.0, for goodness sakes.

68
00:10:38,680 --> 00:10:42,700
What do you think it is? It's.

69
00:10:54,510 --> 00:11:06,720
I know the answer is yes. And you don't need Joe Namath right now.

70
00:11:06,810 --> 00:11:10,320
Just use your intuition. Not looking for the right answer here.

71
00:11:10,350 --> 00:11:16,150
What do you think? You don't go on the Internet and look it up online like 60.

72
00:11:25,050 --> 00:11:51,900
Every time you get out, you know, there's at least 40 some people who say, you know, it's so humid in here.

73
00:11:53,280 --> 00:12:00,190
It was already know that already. All right. And let's see what people thought here.

74
00:12:04,880 --> 00:12:11,190
It was pretty good. The right answer is be honest.

75
00:12:11,700 --> 00:12:14,730
It is tough. I agree.

76
00:12:14,850 --> 00:12:18,950
All right. So I want you to spend about 10 minutes now.

77
00:12:19,100 --> 00:12:25,370
What is the exact answer? What's the probability of getting a favorable give and take in 601?

78
00:12:26,030 --> 00:12:31,100
You all loved that first month combinatorics, right?

79
00:12:31,820 --> 00:12:36,440
What's the probability of getting nothing on your first rule with Farkle?

80
00:12:36,980 --> 00:12:42,320
Can you figure out exactly what the answer is? So take about 10 minutes.

81
00:12:42,320 --> 00:12:46,790
You can talk to each other. You dance to each other questions.

82
00:12:47,240 --> 00:13:06,860
Do you have any intuition how you can figure this problem out? I really don't know if those are the numbers.

83
00:13:10,430 --> 00:13:23,290
There's a lot of information in this world.

84
00:13:26,690 --> 00:13:34,400
So three days in a row, you could only have two parents.

85
00:13:35,830 --> 00:13:47,270
It was going to be two parents for, you know, six of these two parents or something.

86
00:13:48,890 --> 00:13:58,050
And then. Would you have more on Wall Street?

87
00:13:58,780 --> 00:14:09,640
Oh, yes. Yeah. Okay. So you never know what I'm gonna do here, you know?

88
00:14:10,110 --> 00:14:13,130
Yeah. So.

89
00:14:16,340 --> 00:14:26,450
So. Two games in a row.

90
00:14:37,860 --> 00:14:51,820
Okay. I'm sure it's nothing. Yeah, yeah, yeah, yeah.

91
00:14:55,470 --> 00:14:59,580
Going. Probably getting a little emotional.

92
00:15:01,470 --> 00:15:14,980
So then we have to subscribe to the he's speaking and it has to be the first thing you notice if you get word to.

93
00:15:17,290 --> 00:15:20,680
I'm not used to four numbers that you're six.

94
00:15:21,820 --> 00:15:28,110
You can't say it.

95
00:15:30,340 --> 00:15:35,060
So. 16. So say that you didn't like mistakes.

96
00:15:37,200 --> 00:15:45,440
You should be sure it's 320.

97
00:15:46,250 --> 00:16:10,120
So I think this is very little to try to have all the requirements of the United States of America.

98
00:16:10,120 --> 00:16:17,900
Each one of us is about 30 something between my daughter.

99
00:16:19,120 --> 00:16:30,460
So we have a train. This is a 6.231 to come out of it.

100
00:16:33,460 --> 00:16:37,240
It's similar as some of us have wonderful you.

101
00:16:37,420 --> 00:16:42,520
No, we do have enough reading to see what everything happens for us to be like.

102
00:16:43,910 --> 00:16:51,920
Oh, you're right. You can also. You can do things like that because.

103
00:16:52,150 --> 00:17:04,960
Because then you have five. You don't have to do that. I think when you get down to zero two, you need to you need to get to the other one.

104
00:17:06,870 --> 00:17:13,520
Well, let me just because this is only 5.7 points.

105
00:17:15,340 --> 00:17:21,090
You can do that. Okay. So that's why.

106
00:17:21,560 --> 00:17:31,299
Yeah. And then this is in the article that we use to, you know, we have to look like,

107
00:17:31,300 --> 00:17:41,900
you know, trying to consider all these rules, you know, nothing at all.

108
00:17:43,020 --> 00:18:05,830
So, so. Well, we have to want to find out the only way to get a goal for zero zero to, say, one over 14.

109
00:18:05,890 --> 00:18:27,950
Because once you choose a pair of pants for 2.6, you have three, four years that you can say, you know, you do that.

110
00:18:33,210 --> 00:18:43,740
But you're saying that she's going to throw stones.

111
00:18:45,940 --> 00:18:55,150
Okay. I mean, the problem is it's a matter of time.

112
00:18:57,720 --> 00:19:04,590
You know, it's not just the truth.

113
00:19:04,620 --> 00:19:07,199
It's going to be the least of all of those things.

114
00:19:07,200 --> 00:19:18,030
Let's look at 101 over six, which is to say we know it's greater than whatever she saw or just, you know, over say she's only one.

115
00:19:19,890 --> 00:19:24,690
No one said it was sexist. Oh, that's just.

116
00:19:24,690 --> 00:19:32,340
That's interesting. What are you all right?

117
00:19:32,460 --> 00:19:36,690
It's 320. We'll take it as far as we got here.

118
00:19:37,020 --> 00:19:42,180
Right. So it took me about 5 hours to figure this damn thing out.

119
00:19:44,820 --> 00:19:48,120
I didn't want to give up. I knew I could figure this out. I'm a smart person.

120
00:19:49,680 --> 00:19:54,270
I knew the right answer because I wrote a simulation because I would have to figure this kind of stuff out.

121
00:19:54,710 --> 00:19:58,220
So that's right. How do you approach a problem like this?

122
00:19:58,230 --> 00:20:02,490
How did you learn how to do this and so on? What is the probability?

123
00:20:05,740 --> 00:20:09,670
You saw the hospital conditions. All right. So.

124
00:20:10,630 --> 00:20:14,380
No, I haven't. I have not used a piece of shot. And I think ten years.

125
00:20:16,860 --> 00:20:25,220
That's just stupid. All right. So the probability of something happening, it's going to be some event space over the entire event space.

126
00:20:25,240 --> 00:20:29,800
What's. What's the bottom here? Six.

127
00:20:35,980 --> 00:20:41,680
Right. Six possibilities on the first guy. Second one third, one fourth, 1161.

128
00:20:43,270 --> 00:20:46,390
So how do we figure this? This is the hard part.

129
00:20:47,110 --> 00:20:52,140
How do we figure out what percentage of all of those possible combinations lead to a firm?

130
00:20:54,310 --> 00:20:58,720
So again, whenever I try to learn this stuff is I always draw a picture.

131
00:21:00,160 --> 00:21:03,370
I didn't get six days here. Why don't you. Three, four, five, six.

132
00:21:05,430 --> 00:21:08,490
Did anyone try to write out what a circle looks like and see any patterns?

133
00:21:09,680 --> 00:21:14,600
Right. So what patterns did you see? You have talked to unique doubles and case.

134
00:21:15,420 --> 00:21:21,130
You had to have two unique pairs that weren't ones or fights. That really helps us figure this out a lot.

135
00:21:22,570 --> 00:21:28,810
So you could have something like 2 to 3 or four and six.

136
00:21:30,040 --> 00:21:36,790
You have to have a roll comes out with two pairs and then two singles and none of the numbers can be one or five.

137
00:21:36,800 --> 00:21:46,290
That will always give you a sample. But of course, I could have had two, three, four, two, six, four.

138
00:21:46,530 --> 00:21:54,090
Right. So how do I figure out all the different possible ways that I can get two pairs and two singles?

139
00:21:54,840 --> 00:21:58,870
Another six? Again.

140
00:21:58,870 --> 00:22:03,290
It took me 5 hours. It's so basic.

141
00:22:03,860 --> 00:22:07,820
Right. So there are lots of people who have time on their hands and they've done this and it's on the Internet.

142
00:22:26,220 --> 00:22:30,390
It's simply six factorial over 22 factorial with Victoria, 164.

143
00:22:31,750 --> 00:22:35,670
Six things. How many times are going to get to that one, too of that one? And one of the other two.

144
00:22:37,560 --> 00:22:40,910
And there are lots of other derivations out there. Right.

145
00:22:41,460 --> 00:22:47,940
So this is the number of particles that you can get out of all possible six deaths.

146
00:22:48,780 --> 00:22:57,240
Believe me, that didn't come to me right away. I do things like I think, well, thank God, six days.

147
00:22:59,100 --> 00:23:02,249
How many ways does the first die?

148
00:23:02,250 --> 00:23:05,930
You have to look. There's 4 seconds, right?

149
00:23:05,940 --> 00:23:13,860
4 to 6 numbers will keep me on my pants are getting a but I can't get a one or a five so I've got a six here in the denominator.

150
00:23:15,220 --> 00:23:20,400
I know where the four is coming up in there. And then I said, well, once I know this number.

151
00:23:21,550 --> 00:23:25,390
There's only one out of six possible ways I can get a pair. Right.

152
00:23:26,620 --> 00:23:31,030
And then I said, Well, that means there's. Then once I have that number.

153
00:23:31,360 --> 00:23:35,440
Conditioning of that number, that means there are three out of five.

154
00:23:36,940 --> 00:23:40,660
Is that will work out here like 2 to 4.

155
00:23:41,500 --> 00:23:48,040
Then this has to be the same number. And then I have one one out of four and one out of three.

156
00:23:49,750 --> 00:23:55,600
That didn't work. I thought I was a genius, but this does not get to that answer.

157
00:23:55,610 --> 00:24:01,489
So lots of fool websites, folks who like to do this kind of stuff.

158
00:24:01,490 --> 00:24:07,700
These commentaries, I think are fascinating. If you do that math, what is that math?

159
00:24:09,900 --> 00:24:17,740
And the exact fraction. Let's see, we got six times.

160
00:24:17,740 --> 00:24:27,670
Five times four times 38 and over two times two and one times one to get this right.

161
00:24:28,690 --> 00:24:36,810
If Florida cancels both. That's right.

162
00:24:37,340 --> 00:24:41,760
That means the numerator.

163
00:24:42,350 --> 00:24:46,720
So should be another six here round.

164
00:24:47,820 --> 00:24:51,270
So there are six possible ways to write.

165
00:24:51,290 --> 00:24:57,150
I started with the twos. You could start with the threes. So there's actually another six there.

166
00:24:58,480 --> 00:25:02,190
I'm going to do a lecture on this. But three, is that right?

167
00:25:03,780 --> 00:25:07,020
There is no easy answer. There should be another.

168
00:25:07,860 --> 00:25:18,000
You should be able to shoot it six times that.

169
00:25:25,980 --> 00:25:35,040
There's supposed to be a six here. Oh, crud.

170
00:25:35,220 --> 00:25:42,290
Anyway, you cancel an extra times. Two times when it comes to the opposite.

171
00:25:43,680 --> 00:25:49,800
Six tutorial is six, five, four, three, two, one. Yeah, I'm doing too many steps in my head.

172
00:25:51,990 --> 00:25:57,090
It's about the usual ones. Right. Let's do this each.

173
00:25:59,620 --> 00:26:06,180
2166.

174
00:26:07,800 --> 00:26:13,800
I'm sure they were right.

175
00:26:13,920 --> 00:26:36,030
So this whole thing is six times, five times three times two times six six 216, five of which is 16.

176
00:26:41,260 --> 00:26:44,350
That doesn't look great. Oh, that's right.

177
00:26:48,010 --> 00:26:51,430
Oh, this is why I don't wear dark green shirts in this class anymore. Look at this.

178
00:26:53,620 --> 00:27:00,220
So I had a professor. She was wonderful. Her name was Barbara McKnight, and she retired from Seattle a couple of years ago.

179
00:27:00,910 --> 00:27:04,420
I was a GSI for T.A. in Michigan too long.

180
00:27:05,230 --> 00:27:10,060
And she loved to use the white board and the black markers.

181
00:27:10,810 --> 00:27:14,320
And one day she hit her face, but she didn't know it.

182
00:27:16,450 --> 00:27:20,769
And so I and my friend sat there the whole time as the kids in the class wondering what to do.

183
00:27:20,770 --> 00:27:23,290
And we let her stand up there for an hour with black marker on her face.

184
00:27:24,160 --> 00:27:29,229
But anyways, there was a wonderful skit done by the students a couple of years ago pretending to be a barber at night,

185
00:27:29,230 --> 00:27:33,250
and they were basically writing on their face while they were lecturing. But you had to be there.

186
00:27:33,260 --> 00:27:37,210
But this is the joy of teaching, and this is why we don't chalkboards anymore.

187
00:27:39,190 --> 00:27:45,309
I don't like to do math. I like to do simulations and computers again are really helpful.

188
00:27:45,310 --> 00:27:49,920
EG If you don't want a program in our room, if you haven't seen enough are in our curriculum,

189
00:27:50,680 --> 00:27:56,379
I guess you'll see some of it now that we have the data science concentration.

190
00:27:56,380 --> 00:28:03,640
But I wrote a function that takes a roll of six dice and tells me whether or not I got a sample or not.

191
00:28:04,240 --> 00:28:07,270
And then I just did that facility at times.

192
00:28:08,020 --> 00:28:14,350
So again, I wrote a function called Circle. That's a function of again, this has to be a vector of six numbers from 1 to 6.

193
00:28:15,220 --> 00:28:17,830
I could I guess I could have done an error check on that, but I didn't.

194
00:28:19,150 --> 00:28:23,740
Again, if someone types in less than six numbers, the thing crashes and says you have to do it again.

195
00:28:24,910 --> 00:28:31,180
The first thing I did was check for a one or a five. Again, I probably knowing what I wrote up here probably could have been a shorter way to get the

196
00:28:31,180 --> 00:28:36,339
computer to check for a circle because I knew I had to have two pairs and no fives are ones.

197
00:28:36,340 --> 00:28:41,620
But anyways, I checked for a one or a five. So again, the first thing I did was I tabulated.

198
00:28:42,730 --> 00:28:47,890
I said, give me the vector of six numbers and tell me how much of each of the integers is in that role.

199
00:28:48,760 --> 00:28:49,870
So again,

200
00:28:49,870 --> 00:28:59,430
I added on an extra one through six and took off a minus two plus one because if the rule came up with no fives table and R won't give me a five,

201
00:28:59,440 --> 00:29:04,150
which is still on the table of the other numbers anyway. So this gives me a table of all six numbers and some of them are zeros.

202
00:29:04,840 --> 00:29:13,389
So I want to make sure that there are no ones and there are no fives and make sure that at most there are pairs.

203
00:29:13,390 --> 00:29:16,480
In my role there are no triples and so forth. Right?

204
00:29:16,990 --> 00:29:20,920
So in the table I can't have anything tabulated to appear more than twice.

205
00:29:22,060 --> 00:29:26,230
I checked for a run. The other way a run can happen is if all six of them have a one in the table.

206
00:29:26,270 --> 00:29:35,830
Right. So make sure that I don't have the largest number in the table being a one in the return for three pairs, which takes a little bit too hard.

207
00:29:36,460 --> 00:29:39,040
And then I have to make sure that none of those have happened.

208
00:29:41,050 --> 00:29:49,600
And so again, that's what a function an hour looks like if the last line spits out the answer for that function.

209
00:29:49,600 --> 00:29:53,730
So again, if I put that in our circle.

210
00:29:54,560 --> 00:30:02,110
Oh, what's the circle? Let's say we got two, one, three, two, three, five, six.

211
00:30:02,110 --> 00:30:05,310
One, two, three, four, five and a two. Right.

212
00:30:05,320 --> 00:30:10,690
So there we don't have a circle. We have a one and a five. It doesn't count of any points.

213
00:30:10,690 --> 00:30:15,669
If you really want to fun programing, you can figure out what's the probability of a circle.

214
00:30:15,670 --> 00:30:20,230
Eventually, there's someone out there who will say. Who has figured that out?

215
00:30:21,550 --> 00:30:27,100
So that's false, right? There's no sample there. But as we just learned, any time I have.

216
00:30:30,550 --> 00:30:37,330
Something like that. So two pairs that aren't ones and fives and singletons for the other two numbers are going too far.

217
00:30:39,220 --> 00:30:45,370
And so I just wrote a little simulation that just says 50,000 times, this runs fast enough.

218
00:30:46,450 --> 00:30:50,980
Right now, I have no circles. For 50,000 rolls.

219
00:30:51,460 --> 00:30:57,980
I'm going to roll the dice and see whether or not I get a vertical or not and just increment every time I see a purple.

220
00:31:03,810 --> 00:31:09,850
So as I kept thinking, I got the right answer when I did my simulations.

221
00:31:09,870 --> 00:31:17,459
I was disappointed to see that I was wrong. So again, you have to run this a lot of times to get to the level of accuracy you need for this.

222
00:31:17,460 --> 00:31:26,400
You need about a hundred thousand rolls. But I had 50,000 here. But in these 50,000 simulations, I got about 4.02 to 4 four of the 50,000.

223
00:31:26,500 --> 00:31:33,040
And again, here's the right answer at this point. No, two, three, one, four. Anyway, that code's on there.

224
00:31:33,060 --> 00:31:40,590
If you want to see what I did. If you've never written a function in R, you should. Important skill that we probably will do it this semester.

225
00:31:41,820 --> 00:31:46,560
Fun was mass. Fun was 601 no longitudinal at all in elements.

226
00:31:46,600 --> 00:31:56,280
But I like to figure out these probabilities whenever there's cards or dice or something we should be smart about.

227
00:31:56,550 --> 00:31:59,730
All right. It's so hot in here.

228
00:32:01,320 --> 00:32:04,800
Some of us are bringing a fan, and I think I've opened that door. This is pretty busy.

229
00:32:06,720 --> 00:32:11,040
All right, let's talk about longitudinal data. We've got an assignment to a week from today already.

230
00:32:11,040 --> 00:32:16,470
Boy, I would guess that's ambitious, but again, it should be pretty straightforward.

231
00:32:17,940 --> 00:32:20,100
We're going to talk about how we take longitudinal data,

232
00:32:20,640 --> 00:32:25,650
make it look like the data that you would view it analyzed in your introductory classes in 656,

233
00:32:25,650 --> 00:32:28,940
51 or even, you know, t tests, score, research, resistance.

234
00:32:29,580 --> 00:32:34,130
So really no new methods here. We want to see what I'm talking about.

235
00:32:34,140 --> 00:32:39,130
So I would canvas color.

236
00:32:40,170 --> 00:32:43,530
There we go from the beginning.

237
00:32:44,340 --> 00:32:47,370
All right. So you guys already know how to analyze longitudinal data.

238
00:32:47,380 --> 00:32:52,980
If the number of observations per person has to be called paired data.

239
00:32:53,640 --> 00:33:05,000
And hopefully, you know this. But more generally, a longitudinal voyage only kicks from longitudinal or we call repeated measures.

240
00:33:05,150 --> 00:33:12,410
And if we repeatedly measure the same people, data occur whenever several independent units contribute an outcome of interest to time.

241
00:33:13,580 --> 00:33:17,300
Again, if you if you're using my lecture slides on your computer, you're more than welcome to do that.

242
00:33:18,170 --> 00:33:21,020
I fiddled around some today, so if you downloaded them earlier,

243
00:33:21,080 --> 00:33:28,180
you should have changed a little bit on some things that were missing, but not in this set of slides.

244
00:33:28,190 --> 00:33:34,729
I think the first one, a pre-post study, is a very common approach to designing the study of something.

245
00:33:34,730 --> 00:33:39,200
When we have a baseline measure for everybody and then usually we have an intervention

246
00:33:39,200 --> 00:33:42,800
or something happens and we want to see what happens in some future point in time.

247
00:33:43,340 --> 00:33:47,630
So everybody's got a set of observations at the beginning and at the end of the study.

248
00:33:48,080 --> 00:33:51,560
But again, there's correlation. There is a repeated measures kind of study.

249
00:33:52,370 --> 00:33:56,419
A crossover study is used a lot in clinical trials. I shouldn't say it's used a lot.

250
00:33:56,420 --> 00:33:58,880
It can be used in clinical trials. It is complicated.

251
00:33:59,960 --> 00:34:07,100
It's when we have two or more interventions, exposures, drugs, whatever you want to call them, we want to compare them to each other.

252
00:34:07,640 --> 00:34:12,050
Instead of taking groups of people and giving them one of the drugs or the other.

253
00:34:12,800 --> 00:34:16,340
We let everybody take every drug in some randomized order.

254
00:34:16,640 --> 00:34:19,970
Right, to get rid of the artery. And that's called a crossover.

255
00:34:20,240 --> 00:34:25,580
So if we have two agents of interest, we might say that person is getting a will watch them out.

256
00:34:25,580 --> 00:34:31,340
They'll get B, that person will get B once the drug has been washed out, we'll give them a again.

257
00:34:31,350 --> 00:34:34,970
We're comparing what happens to people when they get those interventions.

258
00:34:36,380 --> 00:34:42,810
So within person kind of comparison. So my question to all of you is why would we do such a thing?

259
00:34:43,830 --> 00:34:52,890
What is the value of longitudinal data as opposed to just getting a cross-section of data and analyzing it like you would in 650 or 651?

260
00:34:55,420 --> 00:35:08,290
What the longitudinal data give us. Yes, you can reach the causal effect or like you can get rid of the confounding factors.

261
00:35:08,740 --> 00:35:17,050
All right. So if I measure the same person more than once and I compare that person's to measures to each other,

262
00:35:18,170 --> 00:35:24,340
have controlled for all the other factors in that person, their age, their race, their socioeconomic status.

263
00:35:25,480 --> 00:35:30,490
So one of the reasons we follow people over time is to deal with confounding.

264
00:35:30,490 --> 00:35:35,440
What we talk about is efficiency. Confounding adds noise to data.

265
00:35:36,010 --> 00:35:40,930
If we can take that noise out, we have a better chance of seeing the signal we're interested in.

266
00:35:41,410 --> 00:35:48,840
So what digital data are a way to control? For unmeasured variability, we've put it that way.

267
00:35:49,350 --> 00:35:59,819
Other thoughts? Yes. Things like Simpsons Paradox, where like as you go over time, the trend might be said Simpsons Paradox.

268
00:35:59,820 --> 00:36:01,470
Or are you talking about regression to the mean?

269
00:36:02,840 --> 00:36:10,040
Mr. Simpson The paradox where if you have the overall population, tends to look like if you pull everyone up, the trend is one way.

270
00:36:10,670 --> 00:36:18,560
I'll have to think about that. I've ever seen Simpson's paradox in the context of longitudinal data that it alleviates Simpson's paradox Oh,

271
00:36:19,070 --> 00:36:23,450
I'm going to look that does that is not an answer.

272
00:36:23,450 --> 00:36:29,930
I was expecting a mr. Simpson's paradox in the context of a I think I know what you're saying.

273
00:36:30,560 --> 00:36:34,640
It's the trend changing.

274
00:36:36,350 --> 00:36:42,020
Yes. We can see trends through time. All right. So if you're interested in changes over time,

275
00:36:44,030 --> 00:36:52,100
one way you might want to do that is to see if people themselves are changing over time rather than getting a cross-section of different time points.

276
00:36:54,210 --> 00:36:58,920
Again, we might be interested in if some people change faster than others.

277
00:37:01,340 --> 00:37:07,410
I might have a very steep slope. Well, some of us might have a very flat slope, and we can't do that.

278
00:37:07,670 --> 00:37:15,650
We just can't do that in a cross-sectional study at all. We can't look at those inverse sometimes affect their thoughts.

279
00:37:15,980 --> 00:37:19,280
Yes. Sometimes not everyone sees an effect at the same time.

280
00:37:19,290 --> 00:37:24,419
Kind of like what you're talking about. Speeding a cross-section at one time. You may not get an accurate representation of people responding.

281
00:37:24,420 --> 00:37:24,870
Guess.

282
00:37:25,160 --> 00:37:34,670
So again, if time is a factor and it needs to be incorporated, we want to see when people if people don't have the same effect at the same time,

283
00:37:34,670 --> 00:37:38,660
sort of the same thing as slopes is we want to see when people change.

284
00:37:39,170 --> 00:37:45,350
And that may differ by different individuals. So we're used to data where we fit one slope across the data.

285
00:37:45,350 --> 00:37:52,670
We're going to talk about random slopes. We're going to talk about where every person has their own individual slopes and they come from a population.

286
00:37:54,320 --> 00:38:00,050
You can't do that with a set of cross-sectional data results.

287
00:38:02,680 --> 00:38:08,980
Wonderful. All right. I have some homework. So for example, if you have looked at the slides.

288
00:38:11,000 --> 00:38:15,380
In front of you right now. What would you tell me? What's the relationship of time and the outcome here in this plot?

289
00:38:19,300 --> 00:38:23,380
Any relationship. It's like a big mass of planets, doesn't it?

290
00:38:25,150 --> 00:38:33,040
So what have you assumed there? What happens when I tell you that?

291
00:38:34,360 --> 00:38:38,590
Is there an association with time and. Yeah, huge association.

292
00:38:39,370 --> 00:38:44,319
All of those people went up because I didn't tell you that there was a baseline measure for

293
00:38:44,320 --> 00:38:47,830
everybody in the study at a certain point in time and then a future observation later.

294
00:38:49,420 --> 00:38:54,340
We use this example in longitudinal classes because it's very pronounced.

295
00:38:55,000 --> 00:38:58,210
I've been doing this for 23 years. I've never seen data like this.

296
00:38:59,410 --> 00:39:06,670
Usually you can see a trend in the longitudinal data without being shown that this is a very unique, unique situation.

297
00:39:06,850 --> 00:39:16,300
But it emphasizes the point that longitudinal data give you information that the cross-sectional study may not demonstrate for you.

298
00:39:16,750 --> 00:39:21,640
So when I tell you that things are paired here right away, you're thinking of, wow, there is a difference here.

299
00:39:21,760 --> 00:39:29,260
You're mentally looking at a change, right? You're looking at those slopes and you're saying they're all positive, right?

300
00:39:31,790 --> 00:39:35,630
So for data like these, you already know what to do.

301
00:39:36,230 --> 00:39:41,030
You already know how to analyze longitudinal data when they're in pairs. It's called a parity test.

302
00:39:41,750 --> 00:39:47,300
Hopefully you have seen a parenting test. Unfortunately, this class jumps unless you took 600.

303
00:39:48,140 --> 00:39:57,140
This department jumps right into regression and doesn't do a more cursory investigation of some of the methods we teach to the rest of the school.

304
00:39:57,830 --> 00:40:03,560
But let's talk about a pair t test and some notation here. So why is going to be the outcome?

305
00:40:03,770 --> 00:40:09,630
I've got two sets of subscripts, one for the person and one for the time point.

306
00:40:09,650 --> 00:40:12,980
So J is one or two. I is the individual.

307
00:40:14,540 --> 00:40:19,580
Each individual has an outcome that is normally distributed right.

308
00:40:19,910 --> 00:40:23,390
New sub j. There's a mean for time one and there's a mean for time two.

309
00:40:24,620 --> 00:40:31,100
And right now I'm going to assume constant variance. All the observations have the same variance regardless of the person or the time point.

310
00:40:32,060 --> 00:40:36,080
We're going to change that. As we move along the semester, we'll start getting into patterns and variance.

311
00:40:37,790 --> 00:40:41,420
But in this simple situation, we have to wipe out the scenes to compare.

312
00:40:42,590 --> 00:40:44,570
Are the means the same or are they different?

313
00:40:45,230 --> 00:40:53,630
I want to see if there's a change and I assess a change by seeing if on average Timepoint point one is different than the average for 10.2.

314
00:40:55,430 --> 00:40:58,909
The problem, though, is that there are core, there is correlation here.

315
00:40:58,910 --> 00:41:02,540
We have two observations. We don't have a two sample teachers problem here.

316
00:41:03,050 --> 00:41:05,930
We don't have a group of people in this group than a group of people in this group.

317
00:41:06,620 --> 00:41:11,420
There's a natural connection between one observation and one group with the others.

318
00:41:12,260 --> 00:41:20,899
And so we call that within subject correlation. And within subject correlation is something you yet have not probably seen in your classes,

319
00:41:20,900 --> 00:41:25,940
because we saw one of the primary assumptions you make is that every observation

320
00:41:25,940 --> 00:41:30,200
is independent of every other observation and right one observation per person.

321
00:41:33,470 --> 00:41:41,000
So what the protest does and it's something we're going to talk about in this class is it actually does something called derived variables.

322
00:41:41,330 --> 00:41:47,930
It takes the data at hand and comes up with a summary measure per person so that we have one measure per person.

323
00:41:48,560 --> 00:41:52,940
And once we have that, I'm back to my T tests,

324
00:41:52,940 --> 00:42:00,590
my case where tests whenever regression independence is now back in play because I've collapsed a series of observations into one number.

325
00:42:02,000 --> 00:42:05,299
The issue is, is what should that one number be when you have multiple measures?

326
00:42:05,300 --> 00:42:12,890
And that's what we'll talk about before we leave today. So the drive variable in this case is to take the was in for some difference.

327
00:42:13,160 --> 00:42:21,139
The second observation minus the first or the other way around. And those differences have I mean and some variance that I'm going to subscript with.

328
00:42:21,140 --> 00:42:28,100
DX. So again, not the same means as the original data. And now I only have a subscript II per person.

329
00:42:28,250 --> 00:42:36,450
There is no time anymore. And the hypothesis that the differences on average should be zero versus they're not right.

330
00:42:37,180 --> 00:42:41,710
If there's some sort of effect, we should see a mean that is significantly different from zero.

331
00:42:43,200 --> 00:42:48,689
So we've got an independent observations. You're going to estimate new by the sample mean of the differences and you're

332
00:42:48,690 --> 00:42:53,250
going to estimate the variance of the differences by the sample variance.

333
00:42:57,160 --> 00:43:03,680
The correlation has gone away because it's accounted for in the variance of the differences.

334
00:43:03,680 --> 00:43:08,890
So we didn't have to think about estimating correlation or dealing with it because the variance of a difference.

335
00:43:10,040 --> 00:43:14,340
Is the sum of the variances minus two times the covariance.

336
00:43:14,440 --> 00:43:18,230
Again, one of those important formulas that we see in the introductory classes.

337
00:43:18,740 --> 00:43:27,470
So again, with constant variance, I've got a sigma squared plus a sigma squared covariance is correlation times, standard deviation.

338
00:43:27,770 --> 00:43:32,540
There's two of those. And so we end up with this formula two sigma squared times, one minus rho.

339
00:43:33,820 --> 00:43:37,180
That is the variance of the difference, right?

340
00:43:37,810 --> 00:43:41,200
It is not sigma squared plus sigma squared. Two sigma squared.

341
00:43:41,800 --> 00:43:46,360
It is two sigma squared times something. That's a function of how much correlation there is.

342
00:43:47,470 --> 00:43:52,990
So within an individual correlation is a nuisance parameter. We don't need to estimate it, but it impacts what we do.

343
00:43:53,260 --> 00:43:58,090
We have to deal with it in our inference, and the way we deal with it here is to simply take the difference.

344
00:43:58,390 --> 00:44:02,460
Right? It's like what you said. It's taking out the confounding by taking the difference.

345
00:44:02,470 --> 00:44:08,980
We get rid of the correlation in most human studies and in most everything we do this semester.

346
00:44:09,670 --> 00:44:17,319
Although correlation in concept can be negative. Most of the time we think of correlation as being positive within such a correlation.

347
00:44:17,320 --> 00:44:22,480
A positive. If my observations are higher than average, they tend to stay higher than average.

348
00:44:23,110 --> 00:44:26,470
If my observations are lower than average, they tend to stay lower than average.

349
00:44:26,480 --> 00:44:32,290
You don't get someone who starts low and becomes high and goes low. Correlation that's negative is very rare in human studies.

350
00:44:32,290 --> 00:44:39,790
I don't think I've ever seen that example. So if we have positive correlation, they go back to this formula.

351
00:44:40,210 --> 00:44:45,670
If we have a positive correlation, the variance of the difference is less than two sigma squared.

352
00:44:46,030 --> 00:44:51,460
It's reduced. Right. So we have this what I just said right here.

353
00:44:51,850 --> 00:44:57,820
The variance of the difference is less than the sum of the variances. This is the pairing effect.

354
00:44:58,510 --> 00:45:05,230
This is if I had two groups and I'm comparing them to each other. So pairing helps to reduce the variability in what I'm going to do.

355
00:45:05,830 --> 00:45:09,190
That's a good thing. If the correlation is positive.

356
00:45:11,200 --> 00:45:16,120
And again, what is the purpose of Roe? What is Roe? You've learned Roe probably in the multivariate normal setting.

357
00:45:16,120 --> 00:45:22,300
It's, you know, correlation coefficient. It's the thing that's off the diagonals and also very normal distribution.

358
00:45:23,440 --> 00:45:28,330
In this class we're going to start talking about the correlation coefficient has a greater purpose.

359
00:45:30,660 --> 00:45:38,640
Correlation is a ratio of how variable someone's observations are with themselves relative to everybody else.

360
00:45:39,090 --> 00:45:46,440
So it's a measure of I'm measured. It's a it's a quantifying unmeasured characteristics of people.

361
00:45:46,980 --> 00:45:53,730
You might hear about latent latent effects or random effects for to use the term random effects in this class a lot.

362
00:45:54,540 --> 00:45:56,970
But again, latent simply means not observed.

363
00:45:58,200 --> 00:46:06,960
So the reason my observations look like my observations look more like each other than my observations look like somebody else's.

364
00:46:08,430 --> 00:46:14,130
And that's because of all these inherent characteristics of me that can't be measured or weren't measured.

365
00:46:15,120 --> 00:46:20,340
And we might think of variables that impact someone's outcomes, but we can never possibly measure everything.

366
00:46:21,360 --> 00:46:28,750
So correlation explains the variation of each person's observations with everybody else, right?

367
00:46:29,160 --> 00:46:32,820
It's the overall noise of the data relative to how noisy a person's data are.

368
00:46:33,540 --> 00:46:40,680
Again, if my data are tightly clustered with each other relative to everybody else's, they're highly correlated.

369
00:46:41,630 --> 00:46:46,580
If my observations are spread out in a cloud and everybody else's are, there's not a lot of correlation.

370
00:46:47,120 --> 00:46:52,070
So there's a relative clustering there. How are my observations relative to everybody else's data?

371
00:46:54,350 --> 00:47:03,140
A protest began. The statistic is your traditional wall test that you should be able to think about without having to memorize anything.

372
00:47:04,610 --> 00:47:06,739
I take my best estimate of meal,

373
00:47:06,740 --> 00:47:14,120
which is the sample mean compare it to the null hypothesis value which is zero and I scale it by the sampling variability.

374
00:47:15,350 --> 00:47:18,800
So I got the sample mean -0 divided by this thing.

375
00:47:19,070 --> 00:47:22,970
And again, this is just the sample standard deviation of the differences.

376
00:47:25,370 --> 00:47:32,330
You could compute the variance of the original observations and you could estimate the correlation, but it's the same thing.

377
00:47:32,690 --> 00:47:40,520
So again, the point of the test statistic takes care of the correlation implicitly by simply taking the sample standard deviation of the differences.

378
00:47:41,420 --> 00:47:46,880
And if that test statistic is the null is true, then it has a T distribution with capital N,

379
00:47:47,390 --> 00:47:58,910
so that's the number of pairs of people minus one degree of freedom, because you had estimate the mean and we will get an estimation of the variance.

380
00:48:00,470 --> 00:48:08,150
So let's consider two study designs to compare one agent, maybe a placebo to another agent that's active and designed.

381
00:48:08,150 --> 00:48:16,210
One is to observe capital and participants. They all get the control and then all of them get the intervention.

382
00:48:16,220 --> 00:48:19,790
And again, we can talk about randomizing the order to get rid of the ordering effect.

383
00:48:20,930 --> 00:48:23,840
What we typically do in a randomized controlled trial is now that.

384
00:48:25,270 --> 00:48:31,750
We take and participants and give them the control and we take them participants and give them the intervention.

385
00:48:32,590 --> 00:48:39,549
So design one is paired, design two is two, sample t, test two, independent groups everything.

386
00:48:39,550 --> 00:48:44,320
And design two is independent. Not so in design one.

387
00:48:44,890 --> 00:48:49,870
So both studies are designed to compare the mean outcome in the intervention group to the mean outcome in the placebo group.

388
00:48:50,530 --> 00:48:59,170
As I said, when I teach in director biostatistics, I tell people that design one uses the paired t test design to use as a two simple T test,

389
00:49:00,040 --> 00:49:05,620
and then I go and work with my collaborators who have been out of grad school for 1020 years.

390
00:49:06,160 --> 00:49:10,090
Should I use a pair of t test or a two simple t test? They have no idea why.

391
00:49:10,240 --> 00:49:15,310
Right? Don't fully grasp the idea of pairing because we don't really teach the way.

392
00:49:15,580 --> 00:49:19,180
Now, if you've ever thought about the why, why do I have to do a purity test?

393
00:49:19,180 --> 00:49:23,560
What can I do to sample t test with paired data? What am I doing wrong there?

394
00:49:24,460 --> 00:49:28,870
It's actually not that bad. Who cares?

395
00:49:30,010 --> 00:49:34,120
Why do we have to have a purity test and into a sample t test when really both our

396
00:49:34,120 --> 00:49:38,410
comparing and observations of one treatment in that observations with the other treatment.

397
00:49:39,310 --> 00:49:45,520
What is the correlation doing? Because both the t test to sample teachers and pair t test they have the same numerator.

398
00:49:47,310 --> 00:49:52,560
The mean of the differences is the difference in the means. If we have the same number of observations which we do here in both groups.

399
00:49:54,480 --> 00:50:00,000
So the numerator is fine. It's the denominators that change in the paired t test.

400
00:50:00,180 --> 00:50:06,070
The denominator again is the standard variance of the differences divided by n right.

401
00:50:06,090 --> 00:50:10,020
Sigma squared over n type thing. Square. We're going to get standard deviation.

402
00:50:11,250 --> 00:50:17,670
But for the two sample comparison, it's the variance of the mean of the first group,

403
00:50:18,270 --> 00:50:24,000
plus the variance of the mean in the second group and then square root because we want a standard deviation.

404
00:50:25,260 --> 00:50:31,350
If there's positive correlation, if the pairing leads to positive correlation between the two groups,

405
00:50:31,680 --> 00:50:37,860
then as I said in a previous slide, that means that this denominator is going to be smaller than this denominator.

406
00:50:39,140 --> 00:50:44,710
If the pairing is effective. My test, autistic will have a smaller denominator.

407
00:50:46,070 --> 00:50:50,990
That's a good thing, right? Small denominator means bigger test statistic being smaller p value.

408
00:50:51,530 --> 00:50:55,790
You're more likely to find a significant result if the pairing is effective.

409
00:50:58,660 --> 00:51:04,970
No. You just don't want a pair of people on their astrological sign and what their favorite musical group is.

410
00:51:05,840 --> 00:51:14,749
Right. Because there is a price to pay with parents and the price to pay is the degrees of freedom that we talk about,

411
00:51:14,750 --> 00:51:18,290
which, again, most people we talk to have no idea what degrees of freedom mean.

412
00:51:18,290 --> 00:51:26,629
And sometimes I'm not sure I do either. But with a protest I have and there's minus the sample mean I had to estimate.

413
00:51:26,630 --> 00:51:33,740
So I have n minus one degrees of freedom. Right. How much useful information do I have in a two sample t test?

414
00:51:33,740 --> 00:51:37,700
It's two times, then minus one. Twice the degrees of freedom.

415
00:51:38,300 --> 00:51:39,470
Degrees of freedom are good.

416
00:51:41,180 --> 00:51:49,160
So relative to a two sample t test, a pair of t test will gain power because it has a smaller denominator in the test statistic,

417
00:51:50,840 --> 00:51:58,309
but that just such a stick will be compared to a null distribution that has wider tails for t test.

418
00:51:58,310 --> 00:52:02,030
A T distribution looks like a normal distribution, except it's spread out more.

419
00:52:02,030 --> 00:52:08,740
It has wider tail, heavier tails. But who cares about degrees of freedom?

420
00:52:09,370 --> 00:52:17,800
Because if you have a large enough sample size of and is large enough to assume it enough is the T distribution looks like a normal distribution.

421
00:52:19,430 --> 00:52:23,180
So this degrees of freedom issue is rigor is irrelevant.

422
00:52:24,700 --> 00:52:31,170
So in general, if you have enough data positive within cluster correlation or individuals,

423
00:52:31,220 --> 00:52:37,600
we think of an individual as a cluster of data because they're correlated will increase power for between group comparison.

424
00:52:39,190 --> 00:52:45,270
When we talk in December, if this is one of the facts that you can still recite to me and research to me a year later,

425
00:52:45,280 --> 00:52:49,600
maybe in 69, I'll stop by and I'll ask you something you learned in 699.

426
00:52:50,500 --> 00:52:52,390
I hope this is one of them that you can remember.

427
00:52:53,350 --> 00:53:01,480
Positive correlation increases your power to find a difference between groups, between conditions, between drugs, between whatever.

428
00:53:02,890 --> 00:53:06,700
Right? This is why we do matched case control studies.

429
00:53:07,330 --> 00:53:14,650
The matching reduces the variability in the data and induces correlation in the data and therefore hopefully gives me more power.

430
00:53:19,090 --> 00:53:25,330
So the picture I showed you earlier where things didn't look like there was any association and then everybody had a positive slope,

431
00:53:26,530 --> 00:53:30,489
I made up those data, but if I analyze them and r just want to show you what I'm talking about here,

432
00:53:30,490 --> 00:53:34,630
because often we talk about theory and we never talk about how it looks like in reality.

433
00:53:35,680 --> 00:53:40,570
Again, I had a set of y knots and a set of y ones. So first time point, second time point.

434
00:53:41,290 --> 00:53:51,519
And I said to a t t test in r, I got a test statistic that was enormous, almost 14 again on 24 degrees of freedom, there were 25 pairs.

435
00:53:51,520 --> 00:53:56,950
And that plot and the p value is astronomically small, six times ten to the -13.

436
00:53:58,220 --> 00:54:05,840
Again with 24 degrees of freedom, the critical value was 2.64, which of course, is much, much further away from 13.

437
00:54:05,840 --> 00:54:09,260
Right. That's why we get a small p value. They do a two simple t test.

438
00:54:09,260 --> 00:54:13,160
Why am I pointing to the screen but to a t test?

439
00:54:13,190 --> 00:54:18,410
And those two sets of observations. Again, I'm going to use equal variance, the assumption of equal variances.

440
00:54:19,370 --> 00:54:23,269
I don't know why we have to worry about unequal variances. I teach people that.

441
00:54:23,270 --> 00:54:27,590
And then the degrees of freedom is like 47.32. But the [INAUDIBLE] does that mean?

442
00:54:28,550 --> 00:54:33,000
I just always stick with variance. Equal equals true degrees of freedom is twice as big.

443
00:54:33,740 --> 00:54:40,580
The test statistic again is much smaller. That denominator was bigger for a two simple t test.

444
00:54:42,040 --> 00:54:48,130
They get a P-value again, very, very small relative 2.5 but the critical value is pretty close.

445
00:54:48,850 --> 00:54:52,570
Again, remember, these two numbers are large enough.

446
00:54:53,380 --> 00:54:59,570
What's the critical value in a normal distribution for this question? But you know.

447
00:55:01,790 --> 00:55:06,620
What's the appropriate critical value their normal distribution here 1.96.

448
00:55:07,310 --> 00:55:12,830
I always tell people to choose that magical value. So the degrees of freedom doesn't really change much.

449
00:55:12,950 --> 00:55:18,410
It's around two four critical value, right? So again, the question is, who cares?

450
00:55:19,340 --> 00:55:24,530
They get a significant result. Right. I mean, investigators aren't going to care.

451
00:55:25,340 --> 00:55:28,730
I'm going to report the results and say we don't have nearly a significant difference here.

452
00:55:29,540 --> 00:55:36,080
It's a small sample problem. So if I throw away a lot of the data and I just fix the first eight observations,

453
00:55:37,280 --> 00:55:43,520
so the first eight people in my dataset, again, it's a random sample because there was no ordering the protest.

454
00:55:43,790 --> 00:55:51,740
That's seven degrees of freedom. There's a test statistic, highly significant to sample t test the test statistic.

455
00:55:52,190 --> 00:55:57,710
Again, I can drive this. It's just a little bit of a final five, but the p values become very distinctly different.

456
00:55:59,450 --> 00:56:08,240
So again, in large samples, whether you do to sample t test or impair t test, if there's a strong signal, it's going to show up.

457
00:56:08,990 --> 00:56:12,290
It's in small samples we get worried about there's a gain in power.

458
00:56:13,490 --> 00:56:17,890
The gain in power here is enormous. Right.

459
00:56:18,080 --> 00:56:23,000
The difference in the test statistics, this 12.6 versus 1.97.

460
00:56:23,180 --> 00:56:26,990
Remember, the numerators were the same, just a difference in the means.

461
00:56:27,290 --> 00:56:33,889
It's the denominators. The denominator here is much smaller because the correlation is so large.

462
00:56:33,890 --> 00:56:39,830
Remember, it's two sigma squared times one minus rho. I think Rho in this stupid example was like point nine.

463
00:56:40,760 --> 00:56:49,340
We don't usually get that much. Usually when we pair observations, we're lucky if the correlation is like .1.15.

464
00:56:50,540 --> 00:56:53,520
So the effect of the pairing is much less pronounced.

465
00:56:53,540 --> 00:57:01,070
This is a really ridiculous example here where the correlation is so strong that the pairing is out and that's that's great.

466
00:57:01,460 --> 00:57:04,880
But so keep that in mind, too, as we start analyzing real data,

467
00:57:04,940 --> 00:57:10,970
we're not going to see this this magnitude of difference because correlation just isn't that strong in human data.

468
00:57:12,980 --> 00:57:22,100
So as I say here, although majority of this course will focus on repeated measures, there are lots of ways to get correlated data.

469
00:57:23,420 --> 00:57:29,960
And I was talking with Walter Dempsey one day and he's again, if you don't know Walter, he works in these micro randomized trials.

470
00:57:30,030 --> 00:57:34,460
He's doing some really cool stuff. But he said my grad student doesn't understand random effects.

471
00:57:35,330 --> 00:57:39,709
And I said, that's because they don't teach random effects. We teach them random effects in longitudinal data.

472
00:57:39,710 --> 00:57:45,650
And they think that if they don't have longitudinal data, they don't have correlated data. But there are lots of ways to get correlated data.

473
00:57:45,650 --> 00:57:51,770
We're going to focus on longitudinal data. But again, if you have observational data, you worry about confounding.

474
00:57:52,640 --> 00:58:00,170
So one way you induce correlation is to match people on similar characteristics, age and severity of disease and so forth.

475
00:58:00,560 --> 00:58:10,610
When you match, you induce correlation. But again, what we did shows is that that correlation, if it's big enough, it reduces variance.

476
00:58:11,900 --> 00:58:18,070
But if it's really small. The benefit is washed out by the degrees of freedom that you lose.

477
00:58:19,750 --> 00:58:22,880
So this is why you don't just match on anything you can think of.

478
00:58:23,500 --> 00:58:28,330
You should match on variables that you think are going to reduce the variability in what you're looking at.

479
00:58:29,740 --> 00:58:33,850
And this isn't trivial. I talked to lots of folks who think they're going to match on 20 different variables.

480
00:58:34,510 --> 00:58:37,570
That's a waste of time. There's not a lot of benefit there.

481
00:58:38,700 --> 00:58:43,280
Group randomized trials in graduate school. I worked on methods for group randomized trials, which I hadn't heard of,

482
00:58:44,430 --> 00:58:49,950
and that's where we don't randomize individuals, but we randomize groups of individuals.

483
00:58:50,910 --> 00:58:55,440
And it's really useful in behavioral interventions, often in classrooms or schools.

484
00:58:56,310 --> 00:59:01,500
If you want to keep kids from smoking, you might have some educational program that gets integrated into the school.

485
00:59:02,670 --> 00:59:07,270
You can't randomize kids. Really to the intervention or the control.

486
00:59:07,330 --> 00:59:10,540
Because if they're in the same classroom, it's really hard to do that.

487
00:59:11,440 --> 00:59:14,319
So we're just going to take that entire classroom and randomize them and we'll

488
00:59:14,320 --> 00:59:17,350
take another classroom of students and randomized them to something else.

489
00:59:18,190 --> 00:59:21,470
But once you do that, all those people in the same classroom are correlated.

490
00:59:21,490 --> 00:59:25,570
Their observations look more similar than the responses from another classroom.

491
00:59:25,900 --> 00:59:31,970
So we can induce a correlation through a group randomized trial. Again, lots of ways that we can use correlation.

492
00:59:31,990 --> 00:59:35,049
I used to work at the dental school and at the dental school.

493
00:59:35,050 --> 00:59:40,690
They would measure every person's tooth, every team, every tooth. You should go to the dentist every six months.

494
00:59:41,620 --> 00:59:43,660
If you don't go to the dentist every six months, you're crazy.

495
00:59:45,520 --> 00:59:51,040
There are huge associations with periodontal disease and all kinds of bad things later in life.

496
00:59:52,720 --> 00:59:59,500
There is a not insignificant proportion of the population who are credentials, which means they have no teeth.

497
01:00:00,720 --> 01:00:07,390
I never would have believed that until I worked at the dental school. But anyway, go to the dentist or go to the go to go to get a cleaning.

498
01:00:07,390 --> 01:00:13,330
Right. Get your teeth cleaned. If you go to a periodontal exam and they're going three, three, 3 to 1.

499
01:00:13,330 --> 01:00:17,380
If you have that. Anybody, please. Yes, right.

500
01:00:17,890 --> 01:00:25,480
They're measuring the pocket. How deep the pocket is. When they press on your gums, they want to see how many millimeters that thing goes down.

501
01:00:25,480 --> 01:00:28,330
And if you get a four or five, that's bad.

502
01:00:29,440 --> 01:00:36,580
Anyways, think about measuring all of the teeth in your head and getting six different measurements on every tooth.

503
01:00:38,120 --> 01:00:42,830
There's lots of correlation going on there, nothing longitudinal. I went and got my teeth measured.

504
01:00:43,250 --> 01:00:47,000
Right. But we can induce correlation through lots of different ways because every person's

505
01:00:47,000 --> 01:00:51,620
teeth right next to it probably impacts the tooth next to it and so forth.

506
01:00:52,970 --> 01:00:58,820
We're not going to talk about sample size or design of studies, but it's a real big issue because we've got two questions here.

507
01:00:59,270 --> 01:01:03,470
How many people should I study and how many times should I study them?

508
01:01:04,940 --> 01:01:09,750
And a previous student and pharma looked at how often, what's the frequency rate?

509
01:01:10,010 --> 01:01:16,220
Think about the outcome. Should I should I have people coming in every month, every six months, only once a year?

510
01:01:17,450 --> 01:01:20,500
All those sorts of questions influence the power of the study.

511
01:01:20,510 --> 01:01:24,080
How quickly does there a change? If change takes a year.

512
01:01:25,630 --> 01:01:31,900
Was really necessary to measure them every month if you're just going to get the same number barely changing.

513
01:01:32,110 --> 01:01:35,920
Those sorts of questions are important. Longitudinal studies are complicated.

514
01:01:37,090 --> 01:01:47,430
All right. That's slide number one. And today is mostly an introductory day, except for one for the rest of the states.

515
01:01:48,000 --> 01:01:52,500
That's really 4:00. Which one was Farkle?

516
01:01:52,500 --> 01:01:59,310
Right. That's the problem with this show from beginning to end.

517
01:01:59,370 --> 01:02:04,020
Somebody is actually complicated. Okay. Let's go back to this example here of two observations.

518
01:02:04,320 --> 01:02:11,760
25 people, everybody was seeing a change from their first point at time green to a later time point margin yellow.

519
01:02:13,140 --> 01:02:15,480
Let's talk about what longitudinal data look like.

520
01:02:16,140 --> 01:02:23,400
This is one of these concepts that people seem to glom onto, and it's really part of me working with investigators.

521
01:02:23,910 --> 01:02:31,320
So I apologize for those awful colors. But here are ten of the 25 folks that I mean, these are simulated data.

522
01:02:32,370 --> 01:02:41,489
The way the data were collected by the investigator is there are two rows for every person, row one, is it two and then row two, is it two?

523
01:02:41,490 --> 01:02:47,610
Because one observation corresponds to the earlier time point and the second observation corresponds to the later time point.

524
01:02:47,910 --> 01:02:52,560
And then the third column tells me what the value was and I can show you all the data.

525
01:02:53,310 --> 01:02:56,870
We call this the long format. The data go like this, right?

526
01:02:57,240 --> 01:03:04,470
If you have 100 people followed ten times, the data sets are going to get really long because one roll per time point per person.

527
01:03:04,500 --> 01:03:09,910
Right. That is usually how data are collected.

528
01:03:10,060 --> 01:03:12,910
That is usually not how data are analyzed, unfortunately.

529
01:03:13,390 --> 01:03:19,630
So one of the joys of your job is you work with a longitudinal study that lets you get to the folks beforehand,

530
01:03:19,870 --> 01:03:26,140
which I have done in the past, is you're going to have to create what is called the wide format of the data.

531
01:03:27,460 --> 01:03:34,240
And the way we like to have data is one role is for that person and all of their observations go off to the right.

532
01:03:35,020 --> 01:03:41,270
So we call that the wide format, right? And so here is a wide format type of data.

533
01:03:41,290 --> 01:03:48,850
Most of the data sets that you have to analyze are analog, not because I hate you and I want you to have to make the wide format,

534
01:03:49,570 --> 01:03:54,130
but that's the way the data sets where or when they were used in the data analytics.

535
01:03:54,760 --> 01:04:00,070
There might be a couple of white ones. So if you got a wide dataset you're looking for.

536
01:04:00,340 --> 01:04:06,790
All right, I'll show you some code that I use. But again, this is something that is the bane of our existence.

537
01:04:07,120 --> 01:04:10,669
Three quarters of it is getting the data into the computer.

538
01:04:10,670 --> 01:04:18,909
Right. And then a quarter of the time we analyze. But we want one role per individual, and each time point adds additional columns to the dataset.

539
01:04:18,910 --> 01:04:28,360
And so often the time points in your data sets are fixed like everybody is measured at time, one at six months time, two is a year and so forth.

540
01:04:28,870 --> 01:04:34,180
Here, unfortunately, people were measured at different time points. So I have to keep track of the time points as well.

541
01:04:34,330 --> 01:04:43,030
But that an outcome for time one and time to get number of rows should be the number of people, not the total number of observations.

542
01:04:43,870 --> 01:04:50,589
So this dataset should have 25 rows. That's 5025 Rose.

543
01:04:50,590 --> 01:04:53,780
Not 50 rose. Right.

544
01:04:53,870 --> 01:05:01,670
So just as we're going through the semester and for a homework assignment, eventually you're going to probably have to make go from a long to a wide.

545
01:05:01,790 --> 01:05:04,970
But then once you've done that, every other homework, you're already set.

546
01:05:05,630 --> 01:05:11,060
So what did Tom and those Husky said to me today?

547
01:05:12,140 --> 01:05:15,630
Your last model should not be the first mile, you said. Right.

548
01:05:16,220 --> 01:05:19,760
The last thing you report on a data set should not have been the first thing you did with the data.

549
01:05:20,690 --> 01:05:30,980
The first thing you should do with the data is to explore because any picture should explain the significant or insignificant result you see later on.

550
01:05:32,210 --> 01:05:40,550
I just I should have should I should have in a screenshot, I just met with a collaborator who has a really cool longitudinal study going on and

551
01:05:40,550 --> 01:05:45,470
somebody was analyzing the data for them and the two groups on top of each other.

552
01:05:46,130 --> 01:05:50,030
Right. And the analyst came back with with a significant result.

553
01:05:52,180 --> 01:05:59,740
Like there's there's something wrong here. Well, I did the blah, blah, blah test like, well, the blah blah blah test does not line up with the data.

554
01:06:00,160 --> 01:06:05,500
Right. So exploratory data analysis are really important because we all make mistakes

555
01:06:05,770 --> 01:06:11,110
and we sometimes the computers give us too many p values to choose from.

556
01:06:11,110 --> 01:06:14,170
Right. So things you should do with longitudinal data.

557
01:06:14,620 --> 01:06:20,499
You should look at the mean at each time, like you should look at the mean of each group at each time, point and standard deviations.

558
01:06:20,500 --> 01:06:26,890
Analyzes of the data. You want to look at the minister deviation of within individual changes.

559
01:06:27,020 --> 01:06:33,160
Right. We want to see how people are changing over time. And you also want to get a feel for how much correlation there is.

560
01:06:34,210 --> 01:06:38,050
Again in most of the day, you're going to analyze when I see your homework assignments next week,

561
01:06:38,560 --> 01:06:44,020
I don't think any of you are going to have a correlation coefficient and we're close to 2.9, but that's what I created.

562
01:06:44,020 --> 01:06:47,979
These data sets, the dataset that I have here. So again, I have a summary at time.

563
01:06:47,980 --> 01:06:53,290
One, there was an average of 7.6. It's I'm sure there was an average of 12.7.

564
01:06:54,220 --> 01:06:57,430
And so the change over time was 5.1.

565
01:07:01,180 --> 01:07:05,350
Again, here is the standard deviation of the first measured, the same deviation of the second.

566
01:07:05,770 --> 01:07:12,460
Here's the standard deviation of the difference. So if you square this, you get about 25 plus 25.

567
01:07:13,330 --> 01:07:19,390
So the standard deviation of the one plus the other is about scored a two times five.

568
01:07:19,510 --> 01:07:27,120
That's like seven. It's 1.9 because again, remember, it's two sigma squared times one minus rho.

569
01:07:27,720 --> 01:07:31,020
The two sigma squared was about seven square root. It is about 49.

570
01:07:31,680 --> 01:07:35,160
It was cut down to 1.9 because of this correlation coefficient.

571
01:07:35,370 --> 01:07:40,389
Right. One minus point nine is 0.1. The differences have a variance.

572
01:07:40,390 --> 01:07:43,480
That's point one of the some of the variances.

573
01:07:44,140 --> 01:07:48,910
There's an effect of the matching here, the period. I also scaled things.

574
01:07:48,920 --> 01:07:52,390
Remember that people were measured for different amounts of time. This is a weird data set.

575
01:07:53,110 --> 01:07:57,520
So I tried to scale the changes by how long each person was followed to try and account for that.

576
01:07:58,720 --> 01:08:01,690
It was 2.6, bringing a measure of time and so forth.

577
01:08:02,110 --> 01:08:09,280
But again, just to get a feel for how things are changing over time and I can see already that there's going to probably be a significant difference.

578
01:08:09,730 --> 01:08:15,220
Remember, we want a ratio of about two. The critical value is around two, even with a T distribution with a certain degree of freedom.

579
01:08:15,790 --> 01:08:22,929
So probably going to see some significant results here. I prefer pictures always to numbers.

580
01:08:22,930 --> 01:08:28,500
And I've asked you a number of assignments to give me a couple or maybe just one plot that helps you to tell the story here.

581
01:08:29,920 --> 01:08:34,780
In those histograms, box plots and scatter plots are really useful here, just very simple numeric summaries.

582
01:08:36,400 --> 01:08:41,430
I did a box plot of all the first time point measurements in a box, thought about the second time measurements.

583
01:08:41,440 --> 01:08:44,650
Again, you can see there's some skewness going on here, right?

584
01:08:44,670 --> 01:08:47,800
Maybe not us here, but certainly things are going up.

585
01:08:49,810 --> 01:08:52,830
And again, that either quartile ranges, not a huge amount of overlap.

586
01:08:52,840 --> 01:08:56,830
Again, telling me that maybe I have some some hope here of finding a significant difference.

587
01:08:59,320 --> 01:09:04,620
Here is a box plot of the differences. So again, checked the difference in I mean, a box plot.

588
01:09:04,630 --> 01:09:10,540
Again, you can see the median is five. Even the smallest value down here in the whisker is nowhere near zero.

589
01:09:10,870 --> 01:09:13,270
The distribution of the differences is far from zero.

590
01:09:13,510 --> 01:09:22,030
Even if I scale them appropriately by follow up again, getting something that's probably going to lead you to significance.

591
01:09:24,160 --> 01:09:30,640
You can see already that things are probab well, maybe they're normally distributed and we talk a lot about normality.

592
01:09:31,750 --> 01:09:39,430
And folks that I work with are very worried about the normality of their data because we pound that into individuals.

593
01:09:39,430 --> 01:09:44,320
Right. And maybe that's pretty symmetric, except for this weirdo out here who had a change of 12.

594
01:09:47,260 --> 01:09:52,540
I don't worry about normality. Why don't I worry about normality?

595
01:09:53,890 --> 01:10:03,230
What helps me. There's a certain something you learned last year that says, I don't have to worry about the normality of the data.

596
01:10:04,590 --> 01:10:09,450
In large samples. Central limit theorem the central element.

597
01:10:09,450 --> 01:10:15,900
Often I'm dealing with means, right? And the central limit theorem tells me that I don't care if you have enough data.

598
01:10:16,350 --> 01:10:19,980
The sampling distribution is pretty close to what it would have been if the data were normal.

599
01:10:20,790 --> 01:10:24,120
So lots of things that we do are robust and normality.

600
01:10:24,690 --> 01:10:29,669
Again, a colleague of mine might disagree with me, but I don't worry a lot about normality.

601
01:10:29,670 --> 01:10:34,860
It's fun to point that out and you can do a scatterplot to assess correlation.

602
01:10:35,850 --> 01:10:40,560
Again, this is just the 45 degree line. It's not really important. All the observations are up here.

603
01:10:41,430 --> 01:10:46,100
You can see that they're above so that the second time point is bigger than the first for everybody.

604
01:10:47,160 --> 01:10:56,030
How do you assess correlation in this picture? But tells me that the correlation is really, really big.

605
01:10:58,790 --> 01:11:03,640
The. Anybody.

606
01:11:06,650 --> 01:11:12,450
I don't know if the correlation is point nine. Versus point one.

607
01:11:14,240 --> 01:11:17,340
Anybody? Yeah. Windsor close. Right.

608
01:11:17,370 --> 01:11:23,580
So I bought the house, squished there on the line. I like to say these things because you got to explain this to people out there in the world.

609
01:11:24,510 --> 01:11:30,030
I teach intro class that people think because the slope is so big that everything's correlated.

610
01:11:30,960 --> 01:11:38,310
Well, no, it isn't. Isn't the slope. It's how well does the line explain the observations, how cluster they're on the line.

611
01:11:38,670 --> 01:11:44,249
So a scatterplot helps me look at correlation by seeing how much how tightly the points are clustered

612
01:11:44,250 --> 01:11:48,240
around each other around this this line that I could visualize even if it weren't in the plot.

613
01:11:49,050 --> 01:11:53,220
Right. Individual trajectories are also very useful as longitudinal data.

614
01:11:53,910 --> 01:11:59,430
Right. In this one, it's very useful to use something called a spaghetti plot.

615
01:12:00,800 --> 01:12:09,540
And what happens is, is when you plot everybody's trajectories over time, you basically end up with a table full of hard spaghetti, right?

616
01:12:09,560 --> 01:12:13,070
Ones kind of around this way. One's going this way and you can't figure anything out.

617
01:12:14,930 --> 01:12:19,430
Be careful with spaghetti plots. I think there are better ways to tell a story.

618
01:12:20,150 --> 01:12:26,030
Don't just do a spaghetti plot because you can. And I'm going to show you some spaghetti plots here in a second.

619
01:12:26,040 --> 01:12:31,549
And they're totally worthless. I think, again, you're looking for trends here.

620
01:12:31,550 --> 01:12:39,270
It's very useful. But if I had ten observations on everybody and things were kind of going up and down, it's really hard to see anything.

621
01:12:39,290 --> 01:12:44,420
So, again, it has this name, spaghetti plot. When I was a graduate student.

622
01:12:44,540 --> 01:12:53,900
Spaghetti plot was the only name of a plot that had some sort of, you know, common others violin plots and, oh, my God, waterfall plots.

623
01:12:54,890 --> 01:12:58,670
It's just crazy. But anyways, so you're going to probably do a spaghetti fight in this class.

624
01:12:58,670 --> 01:13:01,800
I'm not telling you not to just think about it.

625
01:13:01,820 --> 01:13:08,330
It can actually tell you anything useful in your analysis, because I think I have told you to maybe share with me one or two plots.

626
01:13:08,690 --> 01:13:14,670
You probably can come up with a better plot than a spaghetti plot. All right.

627
01:13:16,250 --> 01:13:23,950
Oops. I want to cover this set of data.

628
01:13:23,950 --> 01:13:37,220
This set of data. Data is plural. The data are drives me crazy when I see the news and some genius tells me that the data is secure.

629
01:13:37,890 --> 01:13:42,360
The data is not is. The data are all over.

630
01:13:42,360 --> 01:13:46,530
By the time I'm dead and buried, it will be singular. So I'm fighting the good fight here.

631
01:13:48,240 --> 01:13:51,270
We're going to use the labor pain data as an example in a lot of my lectures.

632
01:13:51,270 --> 01:13:57,030
This is a dataset that was in a textbook and for some reason it's been useful to me over the years and I continue to use it.

633
01:13:57,870 --> 01:14:05,760
So these are data randomized controlled trials in and out the business of a treatment to reduce pain during childbirth.

634
01:14:07,200 --> 01:14:08,669
And so at the beginning of labor,

635
01:14:08,670 --> 01:14:16,100
women were randomized to receive either this new pain medication or an inactive control, which I find hard to believe.

636
01:14:16,110 --> 01:14:22,139
But maybe women were willing to be randomized. Why didn't we randomized them at the time?

637
01:14:22,140 --> 01:14:26,790
We told them about the study. Why did we wait until she went into labor to randomize?

638
01:14:30,920 --> 01:14:35,050
Because if you tell them to perform C-sections or something.

639
01:14:35,060 --> 01:14:40,400
Yes. Again, if you work in clinical trials and someday when you randomized somebody,

640
01:14:40,400 --> 01:14:45,570
they're in that group, regardless of what happens later, intend to treat.

641
01:14:45,590 --> 01:14:53,310
We call that. And when you randomize people and then the treatment starts months later, people change their minds.

642
01:14:54,310 --> 01:15:00,690
I don't really want to be in the study. Right. So we randomized as close to the point the treatment starts as possible.

643
01:15:01,860 --> 01:15:09,240
Here I find it a little bit bizarre. I would not be wanting to go into labor and have a doctor come up to me and talk to me of being randomized.

644
01:15:10,500 --> 01:15:14,520
That's the last thing I'm concerned with right now. But anyway, there's some cool data.

645
01:15:14,820 --> 01:15:18,899
Each woman was then asked and so now again, the woman's in labor.

646
01:15:18,900 --> 01:15:25,799
And every 30 minutes I'm going to ask her about her pain on a scale from 0 to 100 zero,

647
01:15:25,800 --> 01:15:29,250
meaning no pain, 100, meaning the worst pain that could ever be experienced.

648
01:15:29,880 --> 01:15:36,090
And again, we're trying to see over time if the treatment does something to pain versus this inactive control.

649
01:15:36,510 --> 01:15:41,640
So we have data on 43 women who received treatment and 40 women who received the control.

650
01:15:42,090 --> 01:15:45,330
So again, it was randomized. We must have lost a couple of women along the way.

651
01:15:46,680 --> 01:15:52,020
And we also know the age of the woman and whether or not this is the first child that she's going to suffer.

652
01:15:54,720 --> 01:15:58,700
And so here are the data for two of the women. So, again, this is in the long format.

653
01:16:00,060 --> 01:16:09,300
We have three entries for this person. Given the idea of 33, she was measured every half hour, three times, and there were pain scores.

654
01:16:09,600 --> 01:16:16,200
And there's her age. And one means that this is her first child that she's delivering.

655
01:16:17,400 --> 01:16:20,400
Again, we had hoped to have seven measurements on every woman.

656
01:16:20,700 --> 01:16:25,270
Again, this is an unusual study, I imagine. I don't know, because the details are not in the textbook.

657
01:16:25,740 --> 01:16:30,600
What happened to the rest of this woman's measures? Why wasn't she measured at hour two?

658
01:16:31,950 --> 01:16:35,970
She probably gave birth, right? That's probably what happened.

659
01:16:36,120 --> 01:16:40,259
But I don't know. I can imagine there were probably some women who had complications.

660
01:16:40,260 --> 01:16:43,790
They had to like, you know, we're done with our study. We got to figure something else out.

661
01:16:43,800 --> 01:16:47,730
But anyway, we're going to assume that the same data are due to childbirth.

662
01:16:50,220 --> 01:16:59,100
And then here is the number 75. She for woman had to go through labor a little longer than the first woman in her pain.

663
01:16:59,310 --> 01:17:05,850
She's on the inactive control and her payment, her pain continually went up and up and up and up for three and a half hours.

664
01:17:09,540 --> 01:17:17,270
And I'd rather have the data that there's ID 33 and then I have a series of pain measures every half hour that are in the same row.

665
01:17:17,980 --> 01:17:24,270
But so here's a spaghetti plan. Here's the trajectory of pain scores for women in the placebo group.

666
01:17:25,890 --> 01:17:30,540
Figure out what's happening over time in these women. It's probably not.

667
01:17:30,540 --> 01:17:36,380
I can't even see a trajectory. Even if I had solid lines or different colors or different everybody was in a circle.

668
01:17:36,390 --> 01:17:41,020
It's really hard to see what's going on here. But that's what they're getting.

669
01:17:41,280 --> 01:17:45,270
It's the spaghetti, right? Spiral spaghetti. Here's the intervention group.

670
01:17:46,470 --> 01:17:50,610
Now, comparing these two plots, I think I can see right that the intervention group,

671
01:17:50,610 --> 01:17:57,030
women are tending to stay on low pain scores, whereas there's lots of high pain scores in the placebo group.

672
01:17:58,900 --> 01:18:03,580
So I also was was interested in how much followup do we have on each woman.

673
01:18:03,580 --> 01:18:10,690
And so if you're in the treatment group, in the civil group, how many people contributed a measurement that half an hour, an hour and so forth?

674
01:18:10,960 --> 01:18:13,180
And you can see by the time we get to three and a half hours,

675
01:18:13,660 --> 01:18:19,090
we don't have a lot of data here relative to when we started again, probably because these women are giving birth.

676
01:18:19,090 --> 01:18:22,180
But should we be concerned? Why are the data missing?

677
01:18:23,050 --> 01:18:27,010
Do we still have a representative sample? What is it about these women that they were giving?

678
01:18:28,000 --> 01:18:34,210
They had stayed labor longer than women who gave birth maybe after one and a half hours.

679
01:18:34,660 --> 01:18:37,530
Does that affect our inference? This is a big deal.

680
01:18:37,540 --> 01:18:44,860
Longitudinal data because we have out all the time to call us to follow up again a short period of time here.

681
01:18:45,340 --> 01:18:50,470
I don't think women were getting up and saying, you know what I've done with this study? It probably was because of childbirth.

682
01:18:52,870 --> 01:18:57,070
I think this is a much better representation of what's going on in the two groups.

683
01:18:57,670 --> 01:19:02,950
And so what I did was I computed the mean in each timepoint for the placebo group, and I computed,

684
01:19:02,950 --> 01:19:07,329
I mean, at each time point for the intervention group, and then I computed a sample standard deviation.

685
01:19:07,330 --> 01:19:14,649
And this is just like two standard deviations on either side. So right away I can see that anything I do statistically,

686
01:19:14,650 --> 01:19:20,980
any model that I share is probably going to show a difference because by the time we get out here, we see a big difference.

687
01:19:21,820 --> 01:19:27,820
But again, these women out here are not all the women who started the study.

688
01:19:29,150 --> 01:19:34,190
So there seems to be an effect of the intervention. But is there something about Labor?

689
01:19:34,820 --> 01:19:39,140
It's also going on here. But again, this is I think this is a much easier picture to look at.

690
01:19:39,500 --> 01:19:45,050
So summarizing the groups with their means rather than looking at a spaghetti plot, there's probably something going on here.

691
01:19:46,190 --> 01:19:52,040
I could do a box plot again here as a distribution of pain scores at time.

692
01:19:52,530 --> 01:19:57,950
30 minutes for the intervention group. And again, you can see over time that the median doesn't change much.

693
01:19:58,880 --> 01:20:05,870
We see some outliers here, but we tend to stay very low with pain scores relative to the control group.

694
01:20:07,490 --> 01:20:11,830
We can see these bars going up in terms of their medians, right.

695
01:20:12,150 --> 01:20:15,650
Probably a big difference. Again, no statistics.

696
01:20:15,980 --> 01:20:21,410
Nothing fancy here. But in person, correlation gets a little bit tricky here.

697
01:20:22,040 --> 01:20:30,689
So again, we've got seven measurements. We can talk about all the possible correlations of time, one with time, two, three, four, five, six and seven.

698
01:20:30,690 --> 01:20:38,570
Right. There's lots of correlation coefficient going on here and I split it up by Treatment Control Group and I also

699
01:20:39,110 --> 01:20:43,820
decided that I would just compute correlation coefficients on the women who were in all seven timepoints.

700
01:20:44,810 --> 01:20:52,370
And so this is what I got. Again, really hard to look at again, if we look at the correlation of the first time point with all the later time points,

701
01:20:52,640 --> 01:20:58,490
we can see that the correlation is going down over time. That's a general property of longitudinal data.

702
01:20:58,820 --> 01:21:00,350
We think as time goes on,

703
01:21:00,980 --> 01:21:06,740
our measures are just less related to each other because of whatever is happening in our lives with the intervention or whatever.

704
01:21:08,310 --> 01:21:15,650
You know, if we look at one hour versus that channel, the later time points, again, we see this general decay in correlation.

705
01:21:16,770 --> 01:21:19,830
Is something we believe is very common in longitudinal data.

706
01:21:20,280 --> 01:21:26,400
And the same thing is true for the control arm. Again, this looks really this is really hard to look at.

707
01:21:27,600 --> 01:21:30,270
I don't really think that I need all this correlation coefficients.

708
01:21:30,780 --> 01:21:36,060
I might think that there's a correlation coefficient that corresponds to every half hour leg.

709
01:21:36,750 --> 01:21:42,510
I call that a leg one. And so that's sort of like looking at just these observations here.

710
01:21:44,010 --> 01:21:46,680
So again, these are these are bouncing around.

711
01:21:46,680 --> 01:21:52,200
It's pretty noisy, but I'm thinking that maybe there's some average correlation for all of those values.

712
01:21:52,680 --> 01:21:55,620
And so that's a leg one correlation. It isn't the average of these.

713
01:21:56,310 --> 01:22:04,049
What I did was I took everybody's time one half and then one, and then I put the ones down here and put the 1.5.

714
01:22:04,050 --> 01:22:08,340
So I created two columns of data where every observation is lagged by one time when.

715
01:22:09,300 --> 01:22:16,050
And then a computed information coefficient. And so the last one correlation here is .74, 0.92.

716
01:22:17,040 --> 01:22:23,819
Oh, I just said you'd never see 0.9 in data. There is one time point apart in the leg.

717
01:22:23,820 --> 01:22:27,750
Two correlation in talking about these numbers here.

718
01:22:28,230 --> 01:22:31,230
It's 2.5, 5.8, you getting lower going down.

719
01:22:32,000 --> 01:22:38,880
We believe that correlation goes out over time. You know, these are all the sorts of things that you could possibly do with your own data.

720
01:22:39,150 --> 01:22:45,840
Oh, my God. I've run over for the last slide.

721
01:22:45,840 --> 01:22:50,030
Is the one for your homework. Look at the last slides.

722
01:22:50,040 --> 01:22:54,540
We'll go over them on Friday. As I said, I'm trying not to be a lecture hound on Friday.

723
01:22:55,110 --> 01:22:59,700
It's going to be more working. And so we'll go through those slides and work on some R.

724
01:23:00,150 --> 01:23:05,130
I'd say that our code for creating these plots, what I did and what we could do in the homework.

725
01:23:05,280 --> 01:23:09,720
Right. Sorry, but that's.

