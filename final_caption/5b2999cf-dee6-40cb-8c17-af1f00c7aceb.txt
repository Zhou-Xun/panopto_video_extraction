1
00:00:01,350 --> 00:00:15,239
So as I mentioned on Tuesday, we will have the view next Tuesday and we will have the review in this room.

2
00:00:15,240 --> 00:00:19,420
But be free before the sessions. Your exam.

3
00:00:19,420 --> 00:00:25,230
We also on posted will also be in this auditorium and both sections.

4
00:00:25,830 --> 00:00:39,990
We will be the exam year for the review and exams we will develop together for the review will begin for the audit exam.

5
00:00:41,220 --> 00:00:46,950
And while we need to let you decide and we with the solutions,

6
00:00:48,210 --> 00:00:57,530
so what we plan to do in the review is we will solve some of the questions from the past,

7
00:00:57,530 --> 00:01:14,100
the exams, and we are hoping that students will sort of send us e-mail saying that, oh, you know, who do sort of bold water slides.

8
00:01:15,740 --> 00:01:25,809
X Y. Imagine the E because some concepts are not clear or moving forward.

9
00:01:25,810 --> 00:01:36,750
The thought of the interpretation of the regression quotients when you go over the site of the least squares estimation.

10
00:01:38,310 --> 00:01:42,990
So we will call it the square sense. And please, as I mentioned,

11
00:01:42,990 --> 00:01:57,540
please send us your questions by Saturday and midnight and we will collect all those questions and then we will decide like,

12
00:01:57,540 --> 00:02:01,079
you know, how much time you sort of give to those questions.

13
00:02:01,080 --> 00:02:14,460
But so far it's time we should spend five weeks at sea, since you will have all the like you know, you will have the last three years exams.

14
00:02:17,680 --> 00:02:25,450
Fast Exam said. The solutions, but there are problems that we need to decide what can we do?

15
00:02:27,660 --> 00:02:31,049
So even in the event that we don't get any questions from you,

16
00:02:31,050 --> 00:02:41,940
we will be then sort of designing the review according to what we think will be important and beneficial for everybody.

17
00:02:42,840 --> 00:02:49,850
So we may end up doing more problems during the review or we may end up like, you know.

18
00:02:51,450 --> 00:02:56,860
Do that all relation. But if we have questions coming we will try to address those.

19
00:02:56,880 --> 00:03:06,510
And the reason why we are standing up to send by Sakharov is because we've done something similar better than the flu where let the penis.

20
00:03:07,800 --> 00:03:14,080
So then we can then study it. And so that's designed to do that.

21
00:03:16,830 --> 00:03:27,120
Makes sense. And any any other suggestions or any other any comments that anything that you would like to see us through the links.

22
00:03:31,210 --> 00:03:34,310
But other than what I mentioned. Okay.

23
00:03:35,480 --> 00:03:41,320
Okay. So any questions for me on the on the topic that.

24
00:03:44,880 --> 00:03:48,030
Yes. You have the question now. Okay.

25
00:03:48,810 --> 00:04:01,950
So one other thing I will mention. So Peterson and I debated 00y on Tuesday because like every day after lecture

26
00:04:01,950 --> 00:04:10,800
we kind of meet and b because we really try to run the courses almost like a,

27
00:04:10,830 --> 00:04:18,120
you know, sort of, as I said, a carbon copy of the other one other section.

28
00:04:18,540 --> 00:04:24,059
So we both debated a lot about whether we should start module.

29
00:04:24,060 --> 00:04:28,620
Excluding modules is a heavy module.

30
00:04:30,360 --> 00:04:39,510
And all along that I have been telling you, please wait for now, trust me on this distributional result and we will prove it.

31
00:04:39,930 --> 00:04:50,490
So that's what module F is module methods we will prove like, you know, the B and the distributions of it's a C are the independence and, and so on.

32
00:04:51,210 --> 00:05:00,990
So we debated for a while and then we decided that probably the best interest of the

33
00:05:00,990 --> 00:05:07,140
student is that we even if we have a little bit of time left at the end of this lecture,

34
00:05:07,470 --> 00:05:15,120
we will not stop. Why do that? Because you will be too distracted thinking about the exam.

35
00:05:15,240 --> 00:05:23,580
And and plus, it's it's it's like I said, it's it's a it's a dense, calm module.

36
00:05:24,030 --> 00:05:29,250
So we will we will do it after the exam.

37
00:05:31,140 --> 00:05:40,130
We start fresh. But once again, you see that today when I kind of talked about the month in multiple linear regression,

38
00:05:40,490 --> 00:05:45,110
I'm beginning to see the same thing, you know, at this point, trust me.

39
00:05:46,400 --> 00:05:49,640
And and then we will through this result.

40
00:05:52,310 --> 00:06:00,610
So by the time we prove the that you already know the results, but we will kind of come back and formally prove the distribution.

41
00:06:00,630 --> 00:06:15,500
So okay. So with that, if there are no questions, let's go to where we left on Tuesday.

42
00:06:16,220 --> 00:06:28,000
So we had talked about. In the multiple linear regression context, like we had derived the least worse estimate.

43
00:06:28,290 --> 00:06:39,389
We had talked about properties of Y had we had talked about what happens when you add the

44
00:06:39,390 --> 00:06:44,160
full line assumption for the normality and the independence assumptions on the errors.

45
00:06:45,870 --> 00:06:59,459
And we showed that Y you had or a has a normal distribution with a mean meta

46
00:06:59,460 --> 00:07:05,340
and variance convenience made matrix sigma square h where H is the hat matrix,

47
00:07:05,340 --> 00:07:16,169
the prediction matrix? And then we had also talked about properties of the residual vector and shown that the

48
00:07:16,170 --> 00:07:26,040
mean and the variance of the residuals are derived from the mean and the variance.

49
00:07:26,040 --> 00:07:35,640
And then under the full line assumptions are the distribution assumption of normality of the errors.

50
00:07:36,450 --> 00:07:53,819
We established that the residuals have a normal distribution between zero and variance covariance matrix sigma squared times I minus identity minus h,

51
00:07:53,820 --> 00:07:57,090
where once again h is the hat matrix.

52
00:07:57,360 --> 00:08:02,210
It's symmetric, I've important, so I minus eight is also symmetric item four.

53
00:08:02,220 --> 00:08:08,760
And so we stop here and I think I left that with the question and we had a little bit

54
00:08:08,760 --> 00:08:14,250
of discussion around like I asked you like do we know that the errors are independent,

55
00:08:14,880 --> 00:08:20,670
are the residual is independent? And then so it is no, they're really very much, you know, or related.

56
00:08:20,850 --> 00:08:30,540
And it's easy, immediate to see from this expression here.

57
00:08:32,310 --> 00:08:46,530
There basically as you as we saw that the variance of variance covariance matrix of the residuals is sigma squared i.e. minus E.

58
00:08:46,830 --> 00:08:59,080
So the correlation between or I should write the whole medians between epsilon I hat and Ypsilanti had the I

59
00:08:59,080 --> 00:09:17,340
intended residuals would be given by minus c must wear h i t would be j is the I did the element of the hat matrix.

60
00:09:18,120 --> 00:09:25,440
Okay, so that's what we had left. So now sitting in.

61
00:09:28,940 --> 00:09:33,290
Oh. Okay. Oh. Okay. Sure.

62
00:09:38,130 --> 00:09:54,900
Oh, but there's a difference. It's. I found it then.

63
00:10:00,210 --> 00:10:16,450
One more over. Oh, yeah. Okay.

64
00:10:19,960 --> 00:10:26,140
So that's the that's why we had left.

65
00:10:31,750 --> 00:10:41,920
Okay. So now we are going to talk about the estimating, the variance.

66
00:10:43,750 --> 00:10:59,080
So if you recall in simple linear regression, the we had talked about MSE as an estimate of a sigma squared.

67
00:10:59,290 --> 00:11:03,880
Sigma squared is the true error variance.

68
00:11:04,660 --> 00:11:10,209
So the variance of the epsilon or good.

69
00:11:10,210 --> 00:11:23,190
The two errors is must. Dimes when talking about the Vector Epsilon Sigma, Square Times, the identity matrix.

70
00:11:23,850 --> 00:11:27,210
So how do I get an estimated of Sigma Square?

71
00:11:27,960 --> 00:11:44,170
Once again, this has the same form as the sigma squared from simple linear regression feeds SCC all over the degrees of freedom of SC.

72
00:11:44,340 --> 00:11:48,210
In other words, the MSP. But now.

73
00:11:52,110 --> 00:11:56,810
But. No. No. What is the denominator?

74
00:11:56,850 --> 00:12:05,970
What are the degrees of freedom of SC? The degrees of freedom of SC in the multiple linear regression model.

75
00:12:06,480 --> 00:12:09,720
Now I am estimating more parameters. Correct.

76
00:12:10,530 --> 00:12:16,410
So I have a greater loss of freedom.

77
00:12:16,740 --> 00:12:27,629
So the degrees of freedom is going to be equal to n minus the number of parameters that I'm estimating in this model,

78
00:12:27,630 --> 00:12:44,070
which is B minus one plus one, B minus one coefficients corresponding to the obedience beta x1 x2 to XP minus one blasting intercept beta.

79
00:12:44,310 --> 00:12:51,540
So I have been all B minus one plus one B barometers that I'm estimating in the regression model.

80
00:12:52,050 --> 00:12:57,900
So the degrees of freedom is going to be N minus B for MSI.

81
00:12:58,770 --> 00:13:06,060
But as you can see, it has the exact same form as the sigma had squared from the simple linear regression model.

82
00:13:06,750 --> 00:13:18,090
And once again, the B, so it's basically the you can also write it as the inner product of the sum of squares,

83
00:13:18,090 --> 00:13:23,069
of the residuals, which is basically in vector notation.

84
00:13:23,070 --> 00:13:28,470
It's written as an inner product of epsilon hat transpose and epsilon hat.

85
00:13:29,070 --> 00:13:39,960
But this is essentially sum of squares, of the residues in some notation divided by and minus B.

86
00:13:41,010 --> 00:13:48,330
So the same form. But the degrees of freedom accounts for the fact that I'm estimating parameters.

87
00:13:49,320 --> 00:13:58,140
So once I know Sigma Hat Square, then again what is variance is the estimated variance of beta hat.

88
00:13:58,170 --> 00:14:03,570
Remember beta hat is now all B across one vector.

89
00:14:05,100 --> 00:14:09,650
Yes. Because I am estimating be parameters in the regression model.

90
00:14:09,660 --> 00:14:17,130
So what is the dimension of the variance for variance matrix of beta hat?

91
00:14:17,910 --> 00:14:28,230
It would be p cross B. So this is the dimension of the variance obedience matrix.

92
00:14:42,500 --> 00:14:48,200
And that's going to be given by sigma squared times x transpose x inverse.

93
00:14:51,960 --> 00:14:56,500
We derived this expression. Okay.

94
00:14:58,240 --> 00:15:07,330
And I think I asked you this question. Does this look like something you saw in a seller and someone who said, yes, because the SLR,

95
00:15:07,330 --> 00:15:20,090
we saw that the variance of the estimated variance of the of the snow veto on her is sigma squared divided by this x sum of squares of the excess.

96
00:15:20,100 --> 00:15:34,809
So the extreme its inverse is like the f x but in a, you know, in in that's sort of in higher dimension in vector notation.

97
00:15:34,810 --> 00:15:39,490
So, so that's the variance of B to her.

98
00:15:39,820 --> 00:15:46,960
Now this is a symmetrically by B matrix and sigma squared is a is a scalar, it's a number.

99
00:15:48,700 --> 00:15:53,350
What is the kind of like from an interpretation point of view?

100
00:15:53,590 --> 00:15:58,300
And I'm going to show you an example at the end.

101
00:15:59,560 --> 00:16:03,520
So what what do I have on the mean diagonal of this matrix?

102
00:16:04,090 --> 00:16:05,860
On the mean diagonal of this matrix,

103
00:16:05,860 --> 00:16:15,790
I have the variances of the variance of beta not had variance of beta one had billions of beta, two had and so on.

104
00:16:17,760 --> 00:16:32,969
Yes. So these are things you can also get sort of you can construct that statistic by taking down the diagonal elements of

105
00:16:32,970 --> 00:16:43,470
this matrix and taking the square root of design elements to investigate the standard of the V that we did that we have.

106
00:16:44,640 --> 00:16:54,180
What about the off government elements? The off cabinet elements are the four variances.

107
00:16:55,950 --> 00:17:00,120
Correct. So the full variances of what they mean.

108
00:17:00,210 --> 00:17:07,210
But I am estimating, you know, B barometers in the linear regression market.

109
00:17:07,740 --> 00:17:12,990
Suppose I have I'm just thinking about BMI, gender as full body.

110
00:17:12,990 --> 00:17:28,290
It's in the model. So what the archival elements will give you are the full variances between the estimated coefficients for age and BMI,

111
00:17:28,410 --> 00:17:32,610
age and gender, BMI and gender and so on.

112
00:17:37,880 --> 00:17:38,900
Everybody with me.

113
00:17:42,980 --> 00:17:57,620
So that's how I will get the full variances between the estimated coefficients estimated by damaged estimates from the linear regression model.

114
00:17:58,670 --> 00:18:07,730
So here so I mean, at this point, it's kind of a very easy, maybe even silly question.

115
00:18:08,090 --> 00:18:28,400
But if I, if I, I mean, like a toothless version of the beta data hacks from my billing integration model are uncoordinated the estimated.

116
00:18:33,390 --> 00:18:42,860
Barometers of the betterment of estimates from a multiple linear regression model are unpredicted.

117
00:18:44,040 --> 00:18:59,910
True or false? Fox generally falls, right.

118
00:19:00,570 --> 00:19:06,600
Because the four variances do not necessarily get in is going to be non-zero.

119
00:19:08,580 --> 00:19:18,870
Okay. Yeah, there may be certain situations, but off there has zero obedience, but in general none.

120
00:19:19,380 --> 00:19:22,800
They will be equally lifted. Okay.

121
00:19:22,800 --> 00:19:28,320
So now let's talk about testing a single point with you.

122
00:19:28,330 --> 00:19:43,650
So just like in simple linear regression, we can still construct the statistic to test the null hypothesis that a single matriarchy is equal to zero.

123
00:19:46,210 --> 00:19:56,410
Versus an alternative that a two sided alternative that it's non zero and we can do this for any gay equal to

124
00:19:56,410 --> 00:20:04,930
zero one up would be minus one so we can test people not equal to zero versus better not not equal to zero.

125
00:20:04,930 --> 00:20:10,510
We can both be down on equal to zero versus beta one, not equal to zero and so on.

126
00:20:10,630 --> 00:20:15,400
So we can do this for every K from zero to B minus one.

127
00:20:18,190 --> 00:20:22,690
You already saw in SLR how we construct this test district.

128
00:20:22,700 --> 00:20:25,540
So basically it's the point estimate.

129
00:20:26,860 --> 00:20:41,010
This D is calculated value is given by the point estimate that they had divided by the standard error of grade that they had,

130
00:20:41,020 --> 00:20:48,940
or in other words, the square root of the estimated variance of that they had under the null hypothesis.

131
00:20:49,750 --> 00:20:57,480
This has a T distribution with degrees of freedom and minus B in.

132
00:20:57,520 --> 00:21:10,180
In the case of SLR, this P was equal to two corresponding to the intercept and slope in the model.

133
00:21:11,470 --> 00:21:21,280
So in the multiple linear regression scenario, this D statistic has a T distribution with degrees of freedom and minus B under the null.

134
00:21:24,100 --> 00:21:33,339
We had shown that just a few slides back that the least squares estimate the beta had has a normal

135
00:21:33,340 --> 00:21:39,850
distribution with mean beta and variance covariance matrix sigma squared expression max inverse.

136
00:21:40,000 --> 00:21:53,079
Under the full line assumptions where we also have some normality of data, some because we showed that beta had the least squares estimate.

137
00:21:53,080 --> 00:21:56,490
There is essentially a linear combination of the wise.

138
00:21:57,290 --> 00:22:01,179
If y is normal, a linear combination of y, you go to normal.

139
00:22:01,180 --> 00:22:02,920
So we have established this result.

140
00:22:03,430 --> 00:22:15,160
So we use this result and under the null beta is zero like any single, the single component of beta that you are testing is zero.

141
00:22:15,430 --> 00:22:19,420
So essentially this has a D distribution.

142
00:22:21,190 --> 00:22:27,220
There is one other thing that we need in order to establish the P distribution,

143
00:22:28,060 --> 00:22:35,240
and that's something we will prove again in Modulus, which is the distribution of S.

144
00:22:35,240 --> 00:22:45,639
It's E and I have said this before in the context of SLR that SCC over sigma squared has a chi

145
00:22:45,640 --> 00:22:56,380
square distribution with degrees of freedom in minus P and minus two in the context of SLR.

146
00:22:56,800 --> 00:23:02,860
And in general, when there are P minus one whole body, it's in the model.

147
00:23:03,970 --> 00:23:16,780
The SNC or Sigma Square has a Chi Square distribution with degrees of freedom and minus B and beta head and sigma hat square are independent.

148
00:23:20,830 --> 00:23:29,680
This bullet point is the one that I haven't proof yet.

149
00:23:32,500 --> 00:23:40,150
And this is the bullet point that I keep on saying that trust me on this and we will prove this in my duty if.

150
00:23:48,740 --> 00:24:09,350
So given beta hep is normal and given that it says the word muscular has a chi square distribution and beta had sigma squared independent.

151
00:24:10,640 --> 00:24:19,850
It follows that this ratio of beta had QI and the standard unit of beta had key.

152
00:24:19,850 --> 00:24:26,719
This ratio has a T distribution with degrees of freedom N minus B and once again in

153
00:24:26,720 --> 00:24:32,420
minus ft because we are estimating P parameters compared to the in minus doing SLR.

154
00:24:33,140 --> 00:24:38,210
This is what is reflected in the P distribution. So all good so far.

155
00:24:38,870 --> 00:24:47,899
What would be interpretation here? So that's going to be what is the mean sort of distinction?

156
00:24:47,900 --> 00:24:52,309
We know how to how to perform this test, but what about the interpretation?

157
00:24:52,310 --> 00:25:00,980
So the interpretation in the context of multiple linear regression is is the capability associated

158
00:25:00,980 --> 00:25:09,770
with the outcome after adjusting or controlling for all other cool variables in the model,

159
00:25:10,580 --> 00:25:16,190
another way of saying it is that holding all the other pool variance constant.

160
00:25:16,850 --> 00:25:19,310
So the kind of interpretation we will give is,

161
00:25:20,540 --> 00:25:39,130
is there a direct association between E and we after adjusting for gender and nutrition status, nutritional dietary intake and.

162
00:25:42,290 --> 00:25:46,780
And of. And so on.

163
00:25:48,250 --> 00:25:55,960
So the interpretation is that all these I met Justin for the for the company

164
00:25:55,970 --> 00:26:01,570
that they want to make an inference on the association between that quality and

165
00:26:01,570 --> 00:26:08,200
outcome and wanting everything else in the model constant or adjusting or

166
00:26:08,200 --> 00:26:14,110
controlling for all the other covariates in the model so that the interpretation.

167
00:26:17,980 --> 00:26:28,200
No. Oh. Sorry.

168
00:26:28,210 --> 00:26:37,680
I just realized some things. Give me a second.

169
00:26:46,272 --> 00:26:50,192
So now so we talked about the kiddies.

170
00:26:50,742 --> 00:26:57,252
Now we're going to talk about the FTC in the context of multiple linear regression.

171
00:26:58,812 --> 00:27:04,872
So first, let's recall this partitioning of the sums of squares.

172
00:27:06,162 --> 00:27:16,502
So just as in simple linear regression, we saw that the total variation in the response as this y can be partitioned into the.

173
00:27:17,022 --> 00:27:27,102
The variation, the sum of squares attributed to the regression class, the sum of words, the attributed to error.

174
00:27:27,522 --> 00:27:43,091
Similarly here also we can partition is this y and right is this y as the sum of ss e plus ss are but ss are is the

175
00:27:43,092 --> 00:28:02,592
variation accounted for by the by the regression model and ss e is the variation that is left unexplained by the model.

176
00:28:02,622 --> 00:28:10,001
So once again, ss r is the variation accounted for by the regression model.

177
00:28:10,002 --> 00:28:18,822
But now the regression model has B unknown parameters, including the intercept.

178
00:28:20,022 --> 00:28:33,572
Okay? And the residual variance that is, or the variance that is left unexplained and accounted for by the regression model is s as e.

179
00:28:33,582 --> 00:28:42,852
So once again ss y can be partitioned into a CNS Assad same formula as in simple linear regression.

180
00:28:44,172 --> 00:28:51,792
And here I'm writing it in the summation of format like before.

181
00:28:52,842 --> 00:28:55,512
But when we go into module F,

182
00:28:58,062 --> 00:29:08,482
I will write this in matrix notation and you will see that how I kind of work from there to get the distributions of SS are an SS eat.

183
00:29:09,072 --> 00:29:22,152
But the bottom line is for for now, I don't need anything else other than the fact that this partitioning of the total variation in Y still

184
00:29:22,152 --> 00:29:32,052
holds in the MLA situation and it has the same formula and you can write this this way as SSR plus C,

185
00:29:33,192 --> 00:29:40,512
so if I remind you about that now let's look at the F test for the MLA model.

186
00:29:42,312 --> 00:29:47,802
So in addition to testing the individual regression for accretion,

187
00:29:49,572 --> 00:30:02,172
the SS R in a C gives us a way to test all the coefficients together and here is how I do it.

188
00:30:03,612 --> 00:30:18,191
So for now, let's consider that the null hypothesis is that none of the core variants explain the big nationwide blocks of thought.

189
00:30:18,192 --> 00:30:22,152
And I think that there is at least one full body and that it seems that we.

190
00:30:22,722 --> 00:30:28,032
But there are different ways you can sort of phrase this hypothesis.

191
00:30:29,322 --> 00:30:42,222
Another way that you could place this hypothesis is that the null hypothesis is that there is no regression effect, meaning that once again,

192
00:30:42,282 --> 00:30:56,052
none of the Bulgarians explain what is, although they think that there is some regression after some meaning, maybe one, maybe two.

193
00:30:58,002 --> 00:31:06,882
And we don't know. We only know that there is some contribution by the police.

194
00:31:08,122 --> 00:31:20,532
So, no, tell me if I want to write this hypothesis in terms of the regression work to sense how bright it did.

195
00:31:21,522 --> 00:31:28,452
So I'm going to first solicit. And from you before I.

196
00:31:32,012 --> 00:31:33,872
Say it out loud.

197
00:31:35,342 --> 00:31:44,402
If you recall that there was a question I think you asked this question when we talked about this earlier and when I talked about this,

198
00:31:44,402 --> 00:31:48,432
the next best seller, there was a question like, oh, we know how we do this.

199
00:31:48,482 --> 00:31:49,382
Why do we need that?

200
00:31:50,252 --> 00:32:01,172
And I had said that in the simple Indian edition, one faith clearly doesn't matter because the square feet is F and they are equally valid this.

201
00:32:01,952 --> 00:32:09,482
But in the market with linear regression, one thinks there are two different kinds of tests.

202
00:32:09,482 --> 00:32:13,022
They are going to give you different kinds of information.

203
00:32:14,962 --> 00:32:17,121
In the multiple linear regression context.

204
00:32:17,122 --> 00:32:26,632
Just in the previous slide, we saw what the biggest stuff is that each core believed peak than one at a time.

205
00:32:26,722 --> 00:32:34,312
This the null hypothesis that feedback equal to zero versus alternative between naught equal to zero.

206
00:32:34,402 --> 00:32:44,682
And you can actually even do one sided alternatives, but each will be taken one at a time, and you can do it for each year.

207
00:32:46,432 --> 00:32:55,522
It's completely efficient. You can do it for each game from zero to P minus one.

208
00:32:56,902 --> 00:33:04,582
But now we want to test this hypothesis that none of the covidiots explain the variation and why that's my non.

209
00:33:05,302 --> 00:33:09,292
So in terms of the regression coefficients, how can I write the null hypothesis?

210
00:33:20,002 --> 00:33:27,792
Say it again. They not. Okay.

211
00:33:27,792 --> 00:33:35,262
So that's one answer. You asked me one question. I think in the last lecture about.

212
00:33:36,282 --> 00:33:40,902
Do I include that? No need to ask that question.

213
00:33:44,572 --> 00:33:49,022
Yeah. Liberals. I guess Justin Bieber can pursue.

214
00:33:59,542 --> 00:34:09,952
Difficult times. Okay.

215
00:34:13,362 --> 00:34:22,842
So, Sophia, are you going to device to answer that if you don't believe it, that I've been through this?

216
00:34:23,112 --> 00:34:26,142
Okay. So let me take that.

217
00:34:27,162 --> 00:34:30,852
So the null hypothesis is I'm collecting all that answers.

218
00:34:31,602 --> 00:34:35,772
So null hypothesis is beta one equal to beta two.

219
00:34:36,792 --> 00:35:02,682
So. The null hypothesis is that beta long equal to zero beta equal to zero, beta b minus one equal to zero one in other ways.

220
00:35:02,812 --> 00:35:18,082
I mean, I can write this succinctly as BTP equal to zero for all key from one to p minus one.

221
00:35:26,702 --> 00:35:35,462
So I'm just for the sake of argument because there was an initial answer like that, it better not be equal to zero.

222
00:35:35,462 --> 00:35:51,302
Also included. And the point is beta not equal to zero is not part of the NOL because remember what better not be what?

223
00:35:51,452 --> 00:35:58,692
What does none of the covidiots explain? Then you should invite me or there is no regression effort.

224
00:35:58,712 --> 00:36:05,252
What does that mean? That corresponding to each of the core beliefs, the coefficients that I have.

225
00:36:05,642 --> 00:36:13,052
They are all simultaneously equal to zero, but beta not be still in the null model.

226
00:36:14,612 --> 00:36:17,042
The intercept. So you have.

227
00:36:17,102 --> 00:36:29,282
It's almost like not knowing anything, not knowing, not having any information about the X or basically all of the x's are useless.

228
00:36:29,282 --> 00:36:37,232
So they don't contribute to my explaining big issue in y, but there is still that beta not in the null model.

229
00:36:37,712 --> 00:36:40,802
So this is not there.

230
00:36:46,132 --> 00:36:51,952
Okay. So that's the null. What is the alternative?

231
00:36:57,822 --> 00:37:01,242
I had that done. You had it adequate? Yeah.

232
00:37:01,832 --> 00:37:07,962
We wouldn't need COVID-19 people to say no, because that's what you do.

233
00:37:10,702 --> 00:37:20,992
But in the not in that not a hypothesis. No, I mean but because the demand is quite low.

234
00:37:21,052 --> 00:37:25,222
Yeah, but but that's not part of the hypothesis. Okay.

235
00:37:25,222 --> 00:37:29,752
I'm going to go now. You have the non model y.

236
00:37:29,922 --> 00:37:36,742
Y y you don't want to be there, not less X in the night and B then what happens versus the nature is like that.

237
00:37:37,222 --> 00:37:43,941
But that's not part of the null hypothesis. So what about the alternative?

238
00:37:43,942 --> 00:37:47,182
Now, who wants to tell me what? The alternative hypothesis, baby.

239
00:37:51,482 --> 00:37:56,242
That is exactly so.

240
00:37:56,282 --> 00:38:08,342
The alternative hypothesis is that at least one matriarchy is not equal to zero.

241
00:38:09,872 --> 00:38:17,732
The key is, again, from one up to be minus one.

242
00:38:21,452 --> 00:38:33,062
So that's the alternative. Another way of saying the alternative is it's either we don't want even Article two zero or

243
00:38:33,062 --> 00:38:40,772
BW is not equal to zero or B to B minus one up with a B minus one is not equal to zero.

244
00:38:41,732 --> 00:38:47,162
But basically what we are saying is that at least one b duckie is not equal to zero.

245
00:38:47,522 --> 00:38:51,792
So now before I move on to the actual construction of the F test.

246
00:38:53,642 --> 00:38:57,872
Can somebody tell me if I needed the null?

247
00:39:03,882 --> 00:39:12,942
Based on the FBC. Do I know which orbit it is significant?

248
00:39:13,302 --> 00:39:24,682
I don't. All I know that there is some Gobi Desert or there is some regression effort.

249
00:39:25,032 --> 00:39:34,272
So now do you see why in the MLR context, it makes a whole lot of sense to golf both about the best and the deepest,

250
00:39:35,112 --> 00:39:43,782
and they give different kinds of information. So the actors don't know which ones.

251
00:39:44,172 --> 00:39:48,042
Yeah. You've got the data, the null hypothesis.

252
00:39:48,172 --> 00:39:52,002
Using the f this. All I know is that there is the de.

253
00:39:52,302 --> 00:39:57,161
So in another way people refer to it is that the regression is significant.

254
00:39:57,162 --> 00:40:09,222
The model is significant. But I don't know what bar or what will be the thing the more he is explaining the variation.

255
00:40:11,832 --> 00:40:15,372
And the individual tests will give me that information.

256
00:40:20,832 --> 00:40:22,272
Makes sense, everybody.

257
00:40:22,482 --> 00:40:35,862
So so the bottom line is these two tests are really relevant in the MLA context and they give different kinds of information in the SLA context.

258
00:40:36,432 --> 00:40:46,032
I mean, you know, they are kind of their equivalent. If I do one, I know basically the other.

259
00:40:49,742 --> 00:40:57,212
Questions here. Okay. So now what about the test?

260
00:40:58,442 --> 00:41:05,852
So once again, here is the and what people from the MLA model.

261
00:41:07,142 --> 00:41:12,652
So we can again partition the sums of squares.

262
00:41:12,662 --> 00:41:22,442
Remember, for my total variation in why is this why I will I can write it does is this are

263
00:41:22,442 --> 00:41:32,192
classicists e so for the regression model the sum of 30 this is R and the error is ss e.

264
00:41:33,602 --> 00:41:44,522
What about the degrees of freedom? So I have for for the s is why it's n minus one for is this r?

265
00:41:44,642 --> 00:41:59,922
I am estimating p barometers in the model, so it's b minus one and as this E is in minus B, okay.

266
00:42:00,632 --> 00:42:15,632
And then I have MSR, which is the ratio of SSR divided by B minus one and MSE is C divided by N minus B,

267
00:42:17,972 --> 00:42:25,592
and the F statistic is the ratio of MSR to MSE.

268
00:42:32,142 --> 00:42:37,721
And we will show in module F that this, to be sure,

269
00:42:37,722 --> 00:42:46,241
has an F distribution with degrees of freedom B minus one in the numerator and minus B in the denominator.

270
00:42:46,242 --> 00:42:56,142
So this is what we were sure by establishing the distributions of SSR is the C and the independence.

271
00:42:56,712 --> 00:43:10,452
Okay. And so MSR is basically my SSR or B minus one and my MSE is SSP over and minus.

272
00:43:14,162 --> 00:43:27,752
So this is the ANOVA table for multiple linear regression and we will show that SSR,

273
00:43:29,432 --> 00:43:35,432
MSR and MSE have both good distributions and they are independent.

274
00:43:36,312 --> 00:43:45,272
Therefore, it follows that the ratio of those two has F distribution with degrees of freedom v minus one and then minus b.

275
00:43:48,522 --> 00:44:00,102
Okay. All good. So now let's look at one other important quantity or statistic.

276
00:44:00,612 --> 00:44:06,192
That's the coefficient of determination of the R squared that we talked about in the context of linear regression.

277
00:44:06,462 --> 00:44:09,822
So just like in a sense, SLR, we still have as I said,

278
00:44:10,212 --> 00:44:17,681
it's this why you put with Assad plus this is e that is Assad is now the variation in Y

279
00:44:17,682 --> 00:44:29,892
explained by the B minus one covariance matrix of B minus one for variance or B barometers,

280
00:44:30,402 --> 00:44:34,751
and the R squared we have defined as a societal.

281
00:44:34,752 --> 00:44:46,212
What is this? Why in SLR it's the exact same quantity in MLA also, and the interpretation is exactly the same.

282
00:44:46,602 --> 00:45:01,572
Except now we will see that it's the percent of variation in y accounted by all the four idiots in the model, not just a single covariant as in SLR.

283
00:45:02,442 --> 00:45:10,571
So it's accounted the percent of variation that is explained by all the sorry.

284
00:45:10,572 --> 00:45:15,192
You can be minus one for variants in the model. Okay.

285
00:45:15,432 --> 00:45:24,161
And what about one minus R squared? It's the percent of variation that is not accounted for by the model and one minus R squared.

286
00:45:24,162 --> 00:45:31,272
If you do like one step off algebra because of the partitioning offsets y you can.

287
00:45:31,722 --> 00:45:36,071
So one minus SSR over is this y you can, right.

288
00:45:36,072 --> 00:45:39,432
It does. Is this e over this one.

289
00:45:39,642 --> 00:45:51,042
Because I know that this is why is SSR plus this e so it's the percent of variation in Y that is not accounted for by model.

290
00:45:51,072 --> 00:45:56,982
And we also know that Oscar lies between zero and one if there is no error in the data.

291
00:45:57,732 --> 00:46:09,522
So if we are in the deterministic world, if there is no epsilon, then y is equal to x beta and you get output equal to perfectly equal to one.

292
00:46:10,302 --> 00:46:18,882
You'll get a perfect fit on on a on a straight line, no fluctuation around that.

293
00:46:19,512 --> 00:46:29,502
If on the other hand, there is no slope and as we saw in linear regression and in the context of MLR,

294
00:46:29,842 --> 00:46:43,001
if the null hypothesis of no regression effect are now before we can see them all being contributed to line variation in Y or in other words,

295
00:46:43,002 --> 00:46:49,962
all the victims in the model are equal to zero.

296
00:46:51,132 --> 00:46:58,382
Then what do you have? Then you are back to the non model under the F for the f.

297
00:46:58,392 --> 00:47:04,121
Did you go back to the non model and then you have Y?

298
00:47:04,122 --> 00:47:07,962
I had the equality y bar equal to we do not have.

299
00:47:08,262 --> 00:47:17,742
And what do you have. You have r squared equal to zero because all the beta keys are zero.

300
00:47:19,452 --> 00:47:22,602
Okay. So, so it lies between zero and one.

301
00:47:23,292 --> 00:47:31,872
We know that. Now a few more points about R squared in the context of multiple linear regression.

302
00:47:33,672 --> 00:47:42,642
We did not have this issue in SLR because we had only one yet, but no R squared has to be interpreted with some caution.

303
00:47:44,082 --> 00:47:58,662
It so happens that if you are adding covariates into the model, the regression sum of squares, the SSR will either increase or stay the same.

304
00:48:01,062 --> 00:48:05,321
Think about it conceptually. So you are adding, albeit in the model.

305
00:48:05,322 --> 00:48:12,402
What would happen to the explained variation if we either increase,

306
00:48:12,822 --> 00:48:21,222
if the whole we are adding really does have an effect on y or it is still the same.

307
00:48:21,222 --> 00:48:30,522
If you know this well we need that added do not really in fact the do not really explain the issue in y so is this R will

308
00:48:32,232 --> 00:48:45,222
increase or C the same so monotonically increasing what happens to s this e is c is what is left unexplained by the model.

309
00:48:47,422 --> 00:48:51,412
So as you increase, as you add forward, it's to the model.

310
00:48:51,712 --> 00:48:55,432
The SSD will either stay the same or will decrease.

311
00:48:56,122 --> 00:49:07,882
Why? Because. Is this why the total variation in why remains the same no matter whether you add or drop for this?

312
00:49:07,882 --> 00:49:12,052
Because it is not model dependent. S is y is what?

313
00:49:12,832 --> 00:49:16,042
Summation y minus y bar squared?

314
00:49:16,072 --> 00:49:20,122
It's the total variation in y. It's not model dependent.

315
00:49:20,662 --> 00:49:28,672
Is this why is this. The thing is this are keeps on increasing or staying the same as four variants are added to the model.

316
00:49:28,672 --> 00:49:33,412
So is this e what is left unexplained? Either stays the same or decreases.

317
00:49:34,312 --> 00:49:43,852
So what will be the impact on us? Could ask will always increase or stay the same as you are adding more and more variables into the model.

318
00:49:45,982 --> 00:49:50,152
Now, is that necessarily good then?

319
00:49:50,212 --> 00:49:53,241
So is no. And we talked about this.

320
00:49:53,242 --> 00:50:00,862
I don't know, someone asked that question and I again refer back please come back and talk about like a like

321
00:50:00,862 --> 00:50:10,342
a sort of a statistic that adds a penalty to how many more we need to have in the model.

322
00:50:11,242 --> 00:50:18,082
So the R really doesn't pay attention to is it a big model or is it a small model B model?

323
00:50:18,082 --> 00:50:26,332
Meaning like an account that does a full day is in the model. So a less parsimonious or is it a smaller model?

324
00:50:28,162 --> 00:50:37,462
But, you know, we are still explaining almost sort as more of the percent variation in Y.

325
00:50:39,442 --> 00:50:42,501
So R-squared is not a good tool to compare models.

326
00:50:42,502 --> 00:50:45,982
It will always choose models with more poor videos.

327
00:50:46,852 --> 00:50:54,682
But as I increase the number of orbits, chances are that we start to feed the noise rather than the real signal.

328
00:50:57,172 --> 00:51:05,662
This is called overfitting and there are many, many pitfalls of overfitting.

329
00:51:07,072 --> 00:51:19,972
The biggest is that as you start, you know, increasing the number of audience, you are prone to start fitting the noise and misclassify the model.

330
00:51:20,362 --> 00:51:28,352
So, so we know that R squared is not necessarily a good metric, a good measure to compare across model.

331
00:51:28,402 --> 00:51:33,822
So what can we do? This is what we can do.

332
00:51:33,832 --> 00:51:42,542
So we get added penalty. So that's the concept of adjusted r squared.

333
00:51:42,622 --> 00:51:50,272
I, I know that there was a question like this, but I can't remember in what context this came up.

334
00:51:50,872 --> 00:51:57,082
So here is a penalized version of what square nor that are squared.

335
00:51:57,112 --> 00:52:00,442
The way I wrote it is one minus associate versus Y.

336
00:52:01,972 --> 00:52:07,462
So the adjusted R-squared is where I put the penalty.

337
00:52:08,452 --> 00:52:14,872
I see that, you know, you have to pay a price for inclusion of unnecessary variables in the model.

338
00:52:16,042 --> 00:52:28,042
So the penalty is that depending on the number of parameters that you are estimating, you kind of add that as a penalty.

339
00:52:28,042 --> 00:52:36,441
So the adjusted R squared is given by one -60 divided by N minus five it in minus be remember is the degrees

340
00:52:36,442 --> 00:52:45,022
of freedom of the chi square associated with this AC and the denominator is this y you want in minus one.

341
00:52:46,492 --> 00:52:53,932
This is a statistic that is often used to compare between or across models.

342
00:52:54,982 --> 00:53:06,052
And here is the intuition. So consider like is this y is always, you know, stays the same, doesn't change because it's not model dependent.

343
00:53:06,892 --> 00:53:11,302
So let's see how you know this this penalty is working.

344
00:53:11,962 --> 00:53:15,112
Consider the numerator of the SC over n minus b.

345
00:53:17,032 --> 00:53:31,372
So once you have added or once you have included all the correct necessary variables in the model, add the adding additional variables that are maybe,

346
00:53:31,372 --> 00:53:41,752
you know, kind of noise variables unnecessary really do not explain the signal but kind of are fitting the noise.

347
00:53:42,532 --> 00:53:47,202
Then what will happen is that adding those variable?

348
00:53:47,272 --> 00:54:00,292
This will lead to Diame decreasing SSP rate because they're not really contributing to the it's bringing the total variation to you.

349
00:54:00,712 --> 00:54:12,472
You might see a tiny decrease in SSI in the meanwhile, what is happening as you increase B or as you add more poverty?

350
00:54:12,472 --> 00:54:24,172
It's in the model that SS C over in minus B increases as being prices, minus B decreases one over what's happening.

351
00:54:27,862 --> 00:54:36,982
As being grizzlies minus B decreases one over an minus B increases, right.

352
00:54:37,222 --> 00:54:46,942
And the net negative of a C over N minus B increases, sorry, decreases.

353
00:54:48,532 --> 00:54:52,792
So then consequently what happens the adjusted R squared decreases.

354
00:54:59,052 --> 00:55:10,562
Yes. So in theory, uh, the, the variables,

355
00:55:10,562 --> 00:55:21,272
the possibilities that maybe are kind of noise variables contributing very little to explaining the variation in why they are,

356
00:55:21,902 --> 00:55:26,672
that you are actually paying a penalty for any such variables.

357
00:55:28,652 --> 00:55:40,142
And that's why the I just did ask where is the much better metric to compare across models and in theory,

358
00:55:40,472 --> 00:55:44,102
the model with the largest adjusted R squared,

359
00:55:44,792 --> 00:55:54,332
we have only correct variables like the only kind of explanatory necessary variables and no noise for vehicles.

360
00:55:56,512 --> 00:56:09,952
So that's the utility of adjusted R squared in the model in a model in the multiple linear regression context because you can compute across.

361
00:56:14,392 --> 00:56:25,792
Models. And you will see that we will use this statistic that just $2 for later on when we talk about model selection.

362
00:56:27,652 --> 00:56:36,642
Okay. So I am going to stop here and see if there are any questions and then we can take a break and talk about examples.

363
00:56:37,702 --> 00:56:41,212
Yes. Is there any particular region that you have in mind this?

364
00:56:43,612 --> 00:56:46,972
Yeah. So I think that for me was just.

365
00:56:48,892 --> 00:56:56,002
Yeah. No, the reason once again is basically what is N minus the N minus is the degrees of freedom associated with that system.

366
00:56:57,082 --> 00:57:02,102
That's what. Okay.

367
00:57:02,252 --> 00:57:05,612
Any other questions? Yes.

368
00:57:10,942 --> 00:57:20,652
Okay. So the question is, is there, is there a US standard for like what is the US?

369
00:57:21,202 --> 00:57:34,851
Right. So this is once again like the big question is similar to like what is the when you say large sample, what, what, how large Islam does it 30.

370
00:57:34,852 --> 00:57:41,722
Is it 40? So there is little this one size fits all type of answer to this question.

371
00:57:42,202 --> 00:57:50,132
But what I can tell you so definitely if you have asked where greater than 85% it great but

372
00:57:50,282 --> 00:57:59,182
but even in real world in real applications very seldom especially in observational studies,

373
00:57:59,572 --> 00:58:09,232
very seldom do you get experts that high. And they're given differs from discipline to discipline in social science, behavioral science.

374
00:58:09,232 --> 00:58:15,052
But you're, you know, measuring a lot of like behavioral factors.

375
00:58:15,472 --> 00:58:21,802
And, you know, typically our will be in the range of 40%, 45%,

376
00:58:22,282 --> 00:58:30,532
because there may be a lot of unmeasured variables that contribute to your understanding, big issues about them.

377
00:58:31,342 --> 00:58:38,272
In control of the experiments, of course, the R-squared will be maybe, you know, much larger.

378
00:58:38,302 --> 00:58:48,472
Yeah, typically. So there is a kind of on one end sort that fits all and fits across disciplines that,

379
00:58:49,792 --> 00:58:55,021
you know, like, oh, this R-squared is like a 40% absolute is unacceptable,

380
00:58:55,022 --> 00:59:03,382
you know, because like I said, in social behavioral sciences, you'll often encounter studies with figures like that.

381
00:59:04,862 --> 00:59:17,672
So it's better to not use all square than absolute pressure, but rather as a relative measure comparing across the artistic aspect.

382
00:59:19,972 --> 00:59:24,772
As a penalized version of the Oscar for that sense.

383
00:59:24,892 --> 00:59:31,432
But if you come and tell me, Oh, I got an offer of 80%, I believe I would be jubilant like that.

384
00:59:33,202 --> 00:59:37,032
But as I'm saying that you have to respect the if.

385
00:59:37,042 --> 00:59:41,872
On the other hand, you look more in the top 40 because it's just 10%,

386
00:59:41,902 --> 00:59:48,682
then I would say that all that money is lousy because it's really leaving 90% of the Venetian audience.

387
00:59:49,372 --> 01:00:03,741
So it's a it's a matter of like, you know, kind of judgment would be like if you tell me that I don't want to get upwards of 10%.

388
01:00:03,742 --> 01:00:12,862
So I would probably come back and say, you know, you probably haven't mentioned a lot of things or you haven't probably, you know,

389
01:00:12,892 --> 01:00:24,201
analyzed a lot of other variables that would impact why I would tend to think about, oh, are there variables that we just filed when we did really?

390
01:00:24,202 --> 01:00:30,492
But other variables that could be left out of the model, other variables that you didn't even collect data.

391
01:00:32,542 --> 01:00:36,531
So, so I think maybe one of the extremes like what?

392
01:00:36,532 --> 01:00:43,462
I cannot believe what you think. What is good in one discipline me may not be achievable in another.

393
01:00:45,422 --> 01:01:02,092
Does it have? Yeah.

394
01:01:02,372 --> 01:01:12,892
Yeah. If you had the same number of companions. Yes, but and you were saying that they differed in the the poor reviews that could improve it.

395
01:01:13,132 --> 01:01:20,652
Yes. Then he. You can't qualitatively because you know you have to basically want us well adjusted.

396
01:01:20,662 --> 01:01:25,802
R-squared is accounting for it or is the possible right?

397
01:01:26,262 --> 01:01:34,182
Like in other words, like and you see like when we talk about model selection, model parsimony is an important consideration.

398
01:01:34,692 --> 01:01:44,411
Like, you know, if you have a model with 50/50 and you are explaining 70% of the mediation where there's a

399
01:01:44,412 --> 01:01:52,092
competing model involved and I'm just thinking about like five over the it is keeping you,

400
01:01:52,832 --> 01:02:00,432
you know, 65, 60% of the like with the output is 65%, 60%.

401
01:02:00,432 --> 01:02:04,122
Then the question becomes, I mean,

402
01:02:04,122 --> 01:02:15,312
do you want to compromise on the parsimonious simplicity of the model who gets to make any piece of 50% in a square or not?

403
01:02:15,312 --> 01:02:23,441
So so that's that's the consideration, for example, at just aspect.

404
01:02:23,442 --> 01:02:27,192
But you, if you have the exact same number of total idiots, then yes.

405
01:02:29,232 --> 01:02:34,572
You know. Any other questions?

406
01:02:38,342 --> 01:02:42,872
Thank you, Peter. All great discussion. So let's take a break.

407
01:02:42,902 --> 01:03:15,662
It's 911. We've come back at 921. That was.

408
01:03:39,865 --> 01:03:46,125
So let's talk about this example.

409
01:03:51,765 --> 01:04:02,744
So here is a study that was carried out to examine the relationship between body weight that was measured

410
01:04:02,745 --> 01:04:15,135
in pounds and the covariates are age in ears and height in inches among 12 children aged 6 to 12 years.

411
01:04:16,515 --> 01:04:32,985
So, you know, first I read the data in and then I'm going to run a bunch of regression Model X using this dataset.

412
01:04:33,345 --> 01:04:44,895
So the first thing that I do is I run a stellar model to estimate the effect of age on weight.

413
01:04:46,035 --> 01:04:50,745
So here it is. Using the Ellen functioning are.

414
01:04:51,795 --> 01:04:57,045
Here are the results from running the simple linear regression model.

415
01:04:57,525 --> 01:05:08,174
So the things that I'm going to highlight for you are the coefficient for age 3.6 for with

416
01:05:08,175 --> 01:05:18,525
the standard error of 0.95 and this statistic which is obtained by dividing 3.64 by .95,

417
01:05:18,525 --> 01:05:28,695
I get like 3.81 and compare it with the distribution with degrees of freedom.

418
01:05:29,685 --> 01:05:43,425
Remember my n is 12. So the p statistic has a distribution and the degrees of freedom and minus two well to ten.

419
01:05:44,085 --> 01:05:48,195
So I commit 3.814 with the distribution with degrees of freedom.

420
01:05:48,405 --> 01:05:52,275
Then and I get a p value of one, two or three for one.

421
01:05:53,025 --> 01:06:04,785
I also want to point to the tension to the R squared here, 15926 and an adjusted R-squared of 1.5.

422
01:06:05,805 --> 01:06:11,325
The F statistic here is 14.55 degrees of freedom, one and ten.

423
01:06:11,505 --> 01:06:14,685
And the P value is point or three for all seven.

424
01:06:15,765 --> 01:06:32,775
In the simple linear regression context, I know that the square of P is equal to F, so if you take 3.814 square D, you should get 14.55.

425
01:06:35,355 --> 01:06:48,375
Okay. So now I also been 95% confidence intervals for beta, not then beta one.

426
01:06:49,365 --> 01:07:00,945
So here are the lower and upper limits of the confidence intervals for the intercept and the slope corresponding to it.

427
01:07:01,815 --> 01:07:09,705
And for each the 95% confidence interval goes from 1.51 to 5.77.

428
01:07:10,815 --> 01:07:13,985
So as you can see and other like, you know,

429
01:07:13,995 --> 01:07:24,074
we talked about sort of comparing the confidence interval and doing the test of hypotheses using

430
01:07:24,075 --> 01:07:28,725
either the P value method or the rejection ridden method of the confidence interval method.

431
01:07:29,055 --> 01:07:38,025
And here you get basically the same conclusions because the confidence interval, 95% confidence interval does not include zero.

432
01:07:39,845 --> 01:07:46,875
Right. So you would reject the null hypothesis as you did with the p value.

433
01:07:47,325 --> 01:07:50,655
Okay. All good. So let's talk about the interpretation.

434
01:07:51,675 --> 01:08:01,905
So the interpretation from this SLR model, remember, we have to talk about the units, the direction,

435
01:08:03,495 --> 01:08:14,805
and we are our interest is focused on the slope because we want to know the association between age and weight.

436
01:08:17,385 --> 01:08:24,965
And we also have to talk about be careful to talk about like, you know,

437
01:08:25,005 --> 01:08:30,525
when we talk about the inference that this is what is happening or on an average.

438
01:08:33,645 --> 01:08:48,915
So here is the interpretation. Beta one had is 3.64 with a p value of one, two or three, four and a 95% confidence interval of 1.51 to 5.77.

439
01:08:49,485 --> 01:09:08,355
So the interpretation is this, that we estimated that among children aged 6 to 12 years, one year higher age is significantly associated.

440
01:09:08,895 --> 01:09:16,065
I mean, what the p value and the confidence interval say that there is a significant association,

441
01:09:17,205 --> 01:09:33,225
one year high rate is significantly associated with 3.64 higher weight on an average and the 95% confidence interval is 1.51 to 5.77.

442
01:09:37,515 --> 01:09:51,195
And it's also customary to put the p value in parentheses typical 2.34 to denote the significance of association.

443
01:09:51,885 --> 01:09:59,025
So one year high age is significantly associated with 3.64 higher weight on an average.

444
01:09:59,625 --> 01:10:03,255
What about the R-squared from this model?

445
01:10:04,305 --> 01:10:16,185
59%. So we are seeing that age explains 59.26% of the variance of weight of the total variation in B.

446
01:10:17,295 --> 01:10:30,975
Okay. Although. Now let's use let's run a simple linear regression model to estimate the effect of height on weight.

447
01:10:31,845 --> 01:10:37,135
So again, no height only.

448
01:10:37,365 --> 01:10:43,815
And here is the proportion or high 1.07 with the standard it at a point to fall.

449
01:10:44,535 --> 01:10:53,725
These statistics are obtained by dividing 1.07 by 1 to 4 and I get a value of 4.436.

450
01:10:53,745 --> 01:11:03,255
Compare it to a T distribution with ten degrees of freedom corresponding to n minus two and I get a p value of one below one,

451
01:11:03,255 --> 01:11:17,095
two, six and the r squared from this model is 66.3% adjusted r squared is 62.9% and the F statistic is 19.67.

452
01:11:17,685 --> 01:11:28,395
On one end, ten degrees of freedom. And you can convince yourself that 19.67 is equal to the squared of 4.436.

453
01:11:33,975 --> 01:11:46,604
Okay. We also obtained the 95% confidence interval of the intercept and slope from this model to

454
01:11:46,605 --> 01:11:59,655
the 95% confidence interval for the height coefficient is given by .53 gamma 1.61 once again,

455
01:12:00,645 --> 01:12:05,595
and the 95% confidence interval does not include zero four once again,

456
01:12:05,595 --> 01:12:13,975
meaning that there is a significant association between height and weight based on this alarm, the interpretation.

457
01:12:14,925 --> 01:12:26,655
So we have to talk about the units direction and be careful about saying that this is what is happening on average.

458
01:12:27,255 --> 01:12:39,975
So beta one hat is 1.7 with the p value of 0.13 and a 95% confidence interval from one five 3 to 1.61.

459
01:12:39,975 --> 01:12:49,815
So what is the interpretation that among children aged 6 to 12 years, comparing to children would be four in height by one inch.

460
01:12:51,015 --> 01:12:59,925
The taller individual has an estimated mean B that is 1.07, significantly higher.

461
01:13:03,635 --> 01:13:16,715
Okay. So I just raised the sentence a little differently than when I talked about pun intended.

462
01:13:17,135 --> 01:13:25,685
But note here that in none either neither of the two statements I am I'm being careful not to talk about

463
01:13:25,895 --> 01:13:31,865
sort of tend not to talk about for everyone who maybe is indeed or for everyone who had been in prison.

464
01:13:32,975 --> 01:13:42,665
Hi. So I'm I'm trying to be cautious and be respectful about all I mean, because I don't know what the design of the study was.

465
01:13:42,845 --> 01:13:51,755
So I'm kind of giving up, making a statement that gives the sense of a cross sectional comparison.

466
01:13:52,745 --> 01:13:57,425
Okay. Unadjusted output is 66.3%.

467
01:13:57,425 --> 01:14:05,795
So says Hyde explains 66.3% of the total variation in rate.

468
01:14:07,565 --> 01:14:21,335
Okay. All good. So now what I do is I feed the multiple linear regression model with both height and weight simultaneously in the model.

469
01:14:23,105 --> 01:14:32,375
So I have better not have better one hat and beta two had.

470
01:14:34,235 --> 01:14:38,675
Yes. Some question about the two of the two unadjusted parts.

471
01:14:38,675 --> 01:14:41,765
Great. You already have. So one of those 2.6 billion,

472
01:14:41,855 --> 01:14:50,525
2.66 of them together in their argument is a fact a of something greater is about that there is somehow covariance between them.

473
01:14:51,215 --> 01:14:58,385
Absolutely. And the math doesn't work out that directly.

474
01:14:58,895 --> 01:15:11,615
So basically what what you are observing is that part of the variation that is being explained by is let's see,

475
01:15:12,515 --> 01:15:26,285
part of it could be attributed to height. And as you will see, in fact, let me throw a question to the to the to the class.

476
01:15:26,765 --> 01:15:32,405
So can you tell me, without looking at this question estimates, can you tell me,

477
01:15:32,885 --> 01:15:41,855
given even what you just saw with the R-squared, for the two similarly many different models?

478
01:15:44,975 --> 01:15:48,025
Can you you can even see qualitatively.

479
01:15:48,515 --> 01:15:59,285
Can you tell me what would happen to the more efficient estimates of Eden High in the market building integration model?

480
01:16:01,835 --> 01:16:08,915
Going to make a guess. They might be defeated by the.

481
01:16:15,135 --> 01:16:19,285
Go back to even more first principles bases.

482
01:16:21,565 --> 01:16:33,135
Any. Any intuition as to. How like how that would get reflected in the corporate investments.

483
01:16:35,475 --> 01:16:46,635
So like some of the difference between here could be explained by you, the difference between the mean difference between like one inch like.

484
01:16:46,725 --> 01:16:51,235
So we should expect that. But I think just for.

485
01:16:53,485 --> 01:17:04,065
Does that make sense to everyone? So, I mean, even if I said in even more plain English.

486
01:17:04,925 --> 01:17:09,295
Basically I'm seeing a pretty big impact of AIDS, right?

487
01:17:09,745 --> 01:17:12,715
Because 3.6 or something like that.

488
01:17:14,695 --> 01:17:31,015
So I would expect that if some part of that appeared to be explained by HUD, then I would expect that 3.64 to drop down.

489
01:17:34,395 --> 01:17:39,525
The effect will still be there. Possibly.

490
01:17:40,185 --> 01:17:55,685
But maybe not like the people for. So that's kind of intuitively how you would argue or like what's going on.

491
01:17:56,675 --> 01:18:02,045
So now let's look at the model. So this is a multiple in elevation model with both height and age,

492
01:18:02,495 --> 01:18:11,314
and here are the coefficient estimates for each it's 2.25 as opposed to the 3.64 that

493
01:18:11,315 --> 01:18:24,845
we saw with the standard it of .937 the statistic 2.25 value point or five six.

494
01:18:25,535 --> 01:18:46,475
What about CHI 172 standard error 126 the statistic 2.768 and the develop one or two multiple are squared 78%.

495
01:18:49,125 --> 01:18:56,355
I just did ask a 73% statistic 15.9, five, one, two and 90 with the freedom.

496
01:18:59,365 --> 01:19:02,815
With a p value of 0.401099.

497
01:19:03,385 --> 01:19:08,065
So before going into the interpretation, let's first talk about this EPS statistic.

498
01:19:08,095 --> 01:19:13,734
Now, this EPS statistic is testing the null hypothesis.

499
01:19:13,735 --> 01:19:26,875
That qualification for it is zero and coefficient for height is zero versus the alternative that at least one of these two are different from zero.

500
01:19:27,505 --> 01:19:37,075
And based on this EPS statistic, we would reject the null hypothesis and we would conclude that at least one is significantly different from zero.

501
01:19:38,185 --> 01:19:42,085
So in other words, the regression model is significant,

502
01:19:43,525 --> 01:19:52,165
but at this point we don't know which one is significant or whether voters are, you know, significantly different from zero.

503
01:19:52,645 --> 01:19:59,005
That these tests, on the other hand, tell us the significance of the individual coefficients.

504
01:19:59,005 --> 01:20:18,955
And based on these values, as we can see, it's now in the adjusted model in the multiple linear regression model is actually not significant anymore.

505
01:20:20,005 --> 01:20:30,025
At the at the alpha equals 2.5 level or one could say like it's very marginally significant, weakly.

506
01:20:31,645 --> 01:20:41,425
But if you, if I'm using like a, you know, alpha equal to 1 to 5, basically it is not significant but is height still remains significant.

507
01:20:43,135 --> 01:20:50,154
So you see the distinction between this test and the T test in the MLA setting.

508
01:20:50,155 --> 01:21:01,485
The F test kind of gives you the overall significance of the model and the T test tells you about the individual regression coefficients.

509
01:21:01,495 --> 01:21:05,975
Yes. Is it possible for the F statistic to tell you that at least once again?

510
01:21:10,465 --> 01:21:24,315
Only if it's it. It may happen due to an artifact of the data in such a way that each one is sort of contributing marginally.

511
01:21:24,325 --> 01:21:29,154
But overall, when you look at the sum of squares, it sort of makes the significance.

512
01:21:29,155 --> 01:21:37,735
But but that would be more of an artifact. Okay, here are the 95% confidence intervals.

513
01:21:38,245 --> 01:21:43,975
And now, as you can see, the 95% confidence interval for each confidence zero.

514
01:21:47,735 --> 01:21:50,945
So you would not reject the null hypothesis.

515
01:21:52,625 --> 01:22:01,445
The 95% confidence interval for height still has still does not contain zero.

516
01:22:01,595 --> 01:22:15,605
So we we ended up not we ended up rejecting the null hypothesis corresponding to the t test for height interpretation.

517
01:22:16,245 --> 01:22:20,345
So now as you can see,

518
01:22:21,695 --> 01:22:34,075
the mean sort of addition to the to those statements is when we are talking about the effect of age we see adjusting for height.

519
01:22:34,085 --> 01:22:38,705
When we are talking about the effect of height, we are seeing adjusting for it.

520
01:22:39,125 --> 01:22:43,645
So beta had for ages 2.5 with the p value of one, two, five, seven.

521
01:22:43,655 --> 01:22:47,555
So we say we estimated that among children aged 6 to 12 years,

522
01:22:47,555 --> 01:22:57,335
one year height AIDS is not significantly associated with 2.25, higher weight on average adjusting for height.

523
01:22:59,435 --> 01:23:13,745
So this is often the MLR model is often referred to as adjusted analysis, whereas the SLR model is referred to as the crude or unadjusted analysis.

524
01:23:16,085 --> 01:23:25,505
What about height? We see that among children aged 6 to 12 years, comparing to children who differ in height by one inch.

525
01:23:25,835 --> 01:23:33,905
The taller individual has an estimated mean week that is 4.72 significantly higher.

526
01:23:34,235 --> 01:23:41,375
Adjusting for age, the unadjusted output is 70%.

527
01:23:42,515 --> 01:23:47,345
So together, age and height. Explain 70% of the variability in weight.

528
01:23:50,355 --> 01:23:55,305
Now we compared the parameter estimates from the SLR and the MLA model.

529
01:23:55,305 --> 01:24:02,835
So the SLR model, remember, is the pooled. Or on a dusted.

530
01:24:08,895 --> 01:24:28,735
And the MLR model is the adjusted. So I collect the results from the SLR and MLR model, and I want to point your attention to this versus this.

531
01:24:31,265 --> 01:24:37,235
And this versus this.

532
01:24:42,065 --> 01:24:52,115
So what is going on? So in the crude analysis, it was significantly associated with weight.

533
01:24:53,585 --> 01:25:01,385
But when we adjusted for height in the MLA model, it was no longer statistically significant.

534
01:25:02,465 --> 01:25:20,345
The p value is .057. It turns out when you compare the estimated coefficients for both aid in height between the SLR and the MLR model,

535
01:25:21,125 --> 01:25:30,094
as you can see, even for height in the SLR model, the unadjusted or the crude estimate was 1.7.

536
01:25:30,095 --> 01:25:43,445
In the MLR model, that dusted estimate was .72 for it it was 3.64 in the unadjusted and 2.25 in the adjusted model.

537
01:25:43,445 --> 01:25:56,734
So what is happening that just did? Estimates for both height and age are attenuated smaller in absolute value compared to the unadjusted estimates.

538
01:25:56,735 --> 01:26:03,394
So that's what you see getting reflected here in homework.

539
01:26:03,395 --> 01:26:12,095
Three, there's a problem, but actually we asked you to derive an adjustment factor,

540
01:26:12,095 --> 01:26:17,285
the relative amount of difference between the honored between the adjusted and unadjusted coefficients.

541
01:26:17,285 --> 01:26:21,035
And by examining the adjustment factor,

542
01:26:21,035 --> 01:26:28,355
you'll be able to predict whether the crude coefficients will be attenuated after I just spent or whether they will get larger in magnitude.

543
01:26:31,505 --> 01:26:41,675
That the problem number number two. But the point is that the adjusted estimates for as you see here,

544
01:26:42,245 --> 01:26:52,085
we both are attenuated compared to the uncontested estimates and because they were both positive.

545
01:26:52,895 --> 01:27:02,615
So essentially they get smaller in magnitude compared to the on adjusted estimates.

546
01:27:04,945 --> 01:27:19,945
Finally, you can also kind of do the hand calculation and you can also compare these values.

547
01:27:20,245 --> 01:27:29,995
But let's now look at that tested R-squared. I tested R-squared from the SLR models where 55% and 62.9%.

548
01:27:30,385 --> 01:27:35,875
And that just did our square from the MLR model is 73%.

549
01:27:36,475 --> 01:27:53,934
So although we added one extra covariate in the model and in that model, it was no longer significant.

550
01:27:53,935 --> 01:28:02,665
But as you can see, it still explains some part of the variation in Y.

551
01:28:04,105 --> 01:28:16,285
So that's what is going on. And I think the last slide is you can compute the point estimates and make inference, you know,

552
01:28:17,575 --> 01:28:28,015
by by hand and you can sort of use either R or you can use Emmalin SAS to do the matrix computations.

553
01:28:28,015 --> 01:28:33,525
And so here are a few lines of code, but that's for checking.

554
01:28:36,595 --> 01:28:40,165
That's it. Any questions?

555
01:28:49,215 --> 01:28:52,665
Comments? No. Okay.

556
01:28:52,905 --> 01:28:56,145
So can we do something very quickly?

557
01:28:56,385 --> 01:29:15,615
I was I was thinking of kind of giving you 5 minutes to work on this small example, very easy example, but let's do it together.

558
01:29:16,965 --> 01:29:21,794
So this is an in-class exercise, understanding, adjusted regression coefficients.

559
01:29:21,795 --> 01:29:31,904
I have a regression model for birthweight fitted to a sample of data below and what will vary it's about center that the sample mean.

560
01:29:31,905 --> 01:29:48,525
So my y is Bartlett and my x's and the two x are centered mothers weight gain and center gestational age.

561
01:29:51,525 --> 01:29:55,815
Good. And I have.

562
01:29:57,405 --> 01:30:04,125
So here is my model. Better not plus better. One time centered mothers weight gain plus time center gestational age.

563
01:30:04,125 --> 01:30:11,864
And I ran a multiple linear regression model to the data and obtained these coefficient estimates.

564
01:30:11,865 --> 01:30:20,115
I should really have had some of this be done up at is 3500, but the one had these 30 beta two had this hundred.

565
01:30:21,615 --> 01:30:25,125
So here are a bunch of simple questions.

566
01:30:25,815 --> 01:30:31,245
So the first question is, what is the average portrait in this sample of children?

567
01:30:32,955 --> 01:30:36,465
Who wants to tell me? I gave you the the estimates.

568
01:30:40,875 --> 01:30:44,285
They did it. 3500, right.

569
01:30:44,295 --> 01:30:54,705
Because it's basically better not had what would be the predicted birth rate for each of the following hypothetical babies.

570
01:30:55,575 --> 01:31:07,875
John. John's mother's weight gain was one kilometers above the average and gestational age was one week above the average.

571
01:31:08,745 --> 01:31:13,905
Remember that both mother's weight and gestational age are centered,

572
01:31:14,925 --> 01:31:25,275
meaning that I have taken each of those and subtracted the mean mother's weight gain and mean gestational age.

573
01:31:25,365 --> 01:31:35,935
And that's how I get the standard for using. Yes, the units omega 3 seconds o sub 3500 took basically bonds.

574
01:31:37,275 --> 01:31:44,115
So do not false alarms. I'm sorry.

575
01:31:45,765 --> 01:31:50,955
I'll send gestational age is 18 weeks.

576
01:31:55,165 --> 01:32:10,825
Makes sense, right? Yeah. And Mother Sweet is possibly I should have now put the mother's Wheaties we begin is possibly.

577
01:32:14,515 --> 01:32:18,815
Okay. What is up? So now what?

578
01:32:18,835 --> 01:32:23,035
You tell me. What would be the predicted birth weight for each of the following babies?

579
01:32:28,225 --> 01:32:37,005
Seated in 4x1 is. Actually, you know what I do have I do have the web you need written here.

580
01:32:37,365 --> 01:32:42,015
So let me try that. Sorry. I don't know why I did not pay attention to.

581
01:32:42,405 --> 01:32:50,435
The Bartlet is in Grams. Mother's Week is in.

582
01:32:51,215 --> 01:32:59,034
Mother's Weekend is in KG. Sorry.

583
01:32:59,035 --> 01:33:05,035
I did have it written here, but I.

584
01:33:05,665 --> 01:33:09,145
And gestational age is in weeks. I messed it up completely.

585
01:33:09,325 --> 01:33:14,575
No, the decision later was fine. Gestational age in weeks.

586
01:33:17,705 --> 01:33:22,315
Does that help? Does that make sense? I will put that in the in this.

587
01:33:28,265 --> 01:33:33,785
I'm canvas. So guess who wants to tell me what would be the predicted pathway for John?

588
01:33:37,195 --> 01:33:40,555
In terms of the coefficient estimates.

589
01:33:48,505 --> 01:33:55,135
His mother's weight gain was one kg above the average and the discussion lead was one week above the average.

590
01:33:59,075 --> 01:34:08,945
So if I plug in the regression model. Don't.

591
01:34:09,495 --> 01:34:14,745
Don't give me number. You can tell me in terms of the coefficients.

592
01:34:17,575 --> 01:34:25,345
Beta now PAC plus beta one had plus beta two had use.

593
01:34:28,705 --> 01:34:34,945
Everybody see why that is? So that's 3630.

594
01:34:35,575 --> 01:34:43,345
What about Jane? Mothers with Jane was zero zero kilograms above the average and gestational age was one week.

595
01:34:44,125 --> 01:34:53,455
So it would be better not had plus zero times better on her plus one times better to have.

596
01:34:55,865 --> 01:34:59,585
Yes. 3600.

597
01:34:59,795 --> 01:35:08,165
What about Bob? Mothers weighed in with one kg above the average and gestational age was zero week about the average.

598
01:35:08,885 --> 01:35:13,745
So it's better not have a black widow on her.

599
01:35:14,615 --> 01:35:18,515
Plus, better to have times zero.

600
01:35:21,305 --> 01:35:30,484
So this is equal to 3530. And what about Betty Mathers?

601
01:35:30,485 --> 01:35:34,205
Weight gain was zero kilograms above average and gestational age was zero.

602
01:35:34,235 --> 01:35:39,825
We care about that, but it should be done on pad plus several times better.

603
01:35:39,825 --> 01:35:46,685
One had plus zero times better to have. So it's 3500.

604
01:35:51,385 --> 01:36:01,285
Yes. Is the mother speaking different from Betty's mother speaking for these three babies, John, Jane and Bob.

605
01:36:01,495 --> 01:36:10,415
And based on the expressions here, remember, it's one corresponds to mother's again.

606
01:36:11,485 --> 01:36:24,115
So John for John. Yes, it is. For Jean mothers, weight gain was zero.

607
01:36:25,705 --> 01:36:32,425
Above average, it's a norm. And for Bob, yes, it was.

608
01:36:35,405 --> 01:36:43,515
One kg above average. And it's easy to figure out from looking at these expressions.

609
01:36:45,995 --> 01:37:01,805
Okay. So again, you can complete the rest of this people and convince your friends that apart from John Ford,

610
01:37:01,835 --> 01:37:06,995
Jane and Bob, you can write the difference in terms of the confusion.

611
01:37:07,005 --> 01:37:19,025
So I'll stop here, I'll post this and then if you want us, I may come back and talk about this in the review as well.

612
01:37:21,305 --> 01:37:57,125
Okay. Thank you. I feel like I.

613
01:37:57,995 --> 01:38:02,855
And yes, again, I just keep out of.

