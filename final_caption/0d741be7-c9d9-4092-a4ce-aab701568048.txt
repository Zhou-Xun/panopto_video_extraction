1
00:00:08,630 --> 00:00:11,250
Hello. Welcome to this lecture on

2
00:00:11,250 --> 00:00:13,815
hypothesis testing with
continuous outcomes.

3
00:00:13,815 --> 00:00:16,890
In this lecture, we'll discuss
how we compare data from

4
00:00:16,890 --> 00:00:18,510
more than two groups using

5
00:00:18,510 --> 00:00:21,390
a method known as
analysis of variance.

6
00:00:21,390 --> 00:00:23,720
Number call that a
two-sample T-tests

7
00:00:23,720 --> 00:00:25,160
which used to assess if

8
00:00:25,160 --> 00:00:26,630
the average of a
continuous measure

9
00:00:26,630 --> 00:00:28,835
different among two groups.

10
00:00:28,835 --> 00:00:31,460
For example, is the
average blood lead level

11
00:00:31,460 --> 00:00:33,320
of children living in
Southeastern Michigan

12
00:00:33,320 --> 00:00:35,630
different from the
average blood lead level

13
00:00:35,630 --> 00:00:38,150
if children who live in
Southwestern Michigan.

14
00:00:38,150 --> 00:00:40,805
In other words, we wanted to
know if we're child lives

15
00:00:40,805 --> 00:00:43,790
as any association with
their blood lead level.

16
00:00:43,790 --> 00:00:45,650
Now, what if we wanted
to test whether

17
00:00:45,650 --> 00:00:48,110
the average of
blood lead level or

18
00:00:48,110 --> 00:00:49,610
any continuous measure would be

19
00:00:49,610 --> 00:00:52,715
different among more
than two groups?

20
00:00:52,715 --> 00:00:54,890
In our example, perhaps
we want to look at

21
00:00:54,890 --> 00:00:58,010
average blood level in children
in southeastern Michigan,

22
00:00:58,010 --> 00:01:00,275
Southwestern Michigan,
Northwestern Michigan,

23
00:01:00,275 --> 00:01:03,570
and Northeastern Michigan
for a total of four groups.

24
00:01:03,570 --> 00:01:05,050
So in this module,

25
00:01:05,050 --> 00:01:06,800
we'll walk through
such a test to answer

26
00:01:06,800 --> 00:01:10,725
this question known as
analysis of variance or ANOVA.

27
00:01:10,725 --> 00:01:12,750
So we're learning
objectives are first,

28
00:01:12,750 --> 00:01:15,530
to apply statistical
inference concepts to

29
00:01:15,530 --> 00:01:17,390
one continuous outcome
that's measured

30
00:01:17,390 --> 00:01:19,580
on more than two
groups of people,

31
00:01:19,580 --> 00:01:21,230
to then correctly
state the null and

32
00:01:21,230 --> 00:01:23,180
the alternative
hypothesis that we use in

33
00:01:23,180 --> 00:01:24,890
ANOVA and to identify

34
00:01:24,890 --> 00:01:27,650
the statistic that
is used in ANOVA.

35
00:01:27,650 --> 00:01:30,620
Now as I said, imagine you
have more than two groups,

36
00:01:30,620 --> 00:01:33,500
you have a total of K populations

37
00:01:33,500 --> 00:01:36,560
and you wish to compare
the means of these groups.

38
00:01:36,560 --> 00:01:37,980
So in this figure here,

39
00:01:37,980 --> 00:01:41,090
we have K populations and
each of them has a mean

40
00:01:41,090 --> 00:01:42,950
represented by the
Greek letter Mu with

41
00:01:42,950 --> 00:01:45,920
the subscript for
that population.

42
00:01:45,920 --> 00:01:48,275
The null hypothesis states that

43
00:01:48,275 --> 00:01:50,720
all of the groups
have the same mean,

44
00:01:50,720 --> 00:01:52,865
there are no
differences among any

45
00:01:52,865 --> 00:01:55,775
of the populations with
respect to their means.

46
00:01:55,775 --> 00:01:59,570
Now, in ANOVA, the
alternative hypothesis

47
00:01:59,570 --> 00:02:01,040
is more complex.

48
00:02:01,040 --> 00:02:03,075
The alternative states that

49
00:02:03,075 --> 00:02:06,930
the means differ for
at least two groups.

50
00:02:06,930 --> 00:02:08,990
We say that, two or more of

51
00:02:08,990 --> 00:02:10,910
the population means are unequal.

52
00:02:10,910 --> 00:02:12,785
Note that the
alternative does not

53
00:02:12,785 --> 00:02:15,050
say that all of the
means are equal.

54
00:02:15,050 --> 00:02:18,290
We simply say that there
is a difference somewhere,

55
00:02:18,290 --> 00:02:19,700
it could be among two means,

56
00:02:19,700 --> 00:02:21,305
it could be among three means,

57
00:02:21,305 --> 00:02:23,555
or it could be among
all four of them means.

58
00:02:23,555 --> 00:02:27,675
All of those are comprised in
the alternative hypothesis.

59
00:02:27,675 --> 00:02:29,885
We'll returned to the manuscript

60
00:02:29,885 --> 00:02:32,525
published by Pace and
colleagues in 2017,

61
00:02:32,525 --> 00:02:34,040
looking at whether consumption of

62
00:02:34,040 --> 00:02:35,540
sugary beverages in adults has

63
00:02:35,540 --> 00:02:38,875
an association with their
level of calorie intake.

64
00:02:38,875 --> 00:02:42,230
In the manuscript, there were
actually three populations.

65
00:02:42,230 --> 00:02:44,810
Again, the first population
that we looked at earlier,

66
00:02:44,810 --> 00:02:46,430
was US adults who
prefer consuming

67
00:02:46,430 --> 00:02:48,485
less than one sugary
beverage per day.

68
00:02:48,485 --> 00:02:50,600
The second population,
was US adults

69
00:02:50,600 --> 00:02:52,850
who prefer consuming one
to two sugary beverages.

70
00:02:52,850 --> 00:02:56,870
Today, there's now a third
population which is US adults,

71
00:02:56,870 --> 00:02:58,430
who prefer consuming
more than two

72
00:02:58,430 --> 00:03:00,080
sugary beverages per day.

73
00:03:00,080 --> 00:03:01,580
Each of these groups,

74
00:03:01,580 --> 00:03:04,250
has a mean calories
per day represented by

75
00:03:04,250 --> 00:03:05,330
the Greek letter Mu and a

76
00:03:05,330 --> 00:03:08,185
subscript denoting
that population.

77
00:03:08,185 --> 00:03:10,159
Therefore, our null hypothesis

78
00:03:10,159 --> 00:03:11,780
is that all three populations,

79
00:03:11,780 --> 00:03:15,680
have the same average calories
per day or that Mu 1,

80
00:03:15,680 --> 00:03:18,505
is equal to Mu 2,
is equal to Mu 3.

81
00:03:18,505 --> 00:03:21,320
Whereas, the alternative
again states that at

82
00:03:21,320 --> 00:03:24,155
least two of these means are
different from each other.

83
00:03:24,155 --> 00:03:26,240
So therefore, this hypothesis and

84
00:03:26,240 --> 00:03:29,390
the alternative really
states that Mu 1 is

85
00:03:29,390 --> 00:03:32,610
different from Mu 2 and
or Mu 1 is different from

86
00:03:32,610 --> 00:03:36,090
Mu 3 and or Mu 2 is
different from Mu 3.

87
00:03:36,090 --> 00:03:39,200
We have three pairwise
comparisons of means

88
00:03:39,200 --> 00:03:43,300
that are included in this
alternative hypothesis.

89
00:03:43,300 --> 00:03:45,920
Now, these are the
data that appear

90
00:03:45,920 --> 00:03:48,335
in the manuscript for
the three populations.

91
00:03:48,335 --> 00:03:49,715
In the right-hand column,

92
00:03:49,715 --> 00:03:50,780
you see the sample size,

93
00:03:50,780 --> 00:03:52,280
the number of subjects that we've

94
00:03:52,280 --> 00:03:53,960
sampled from those populations.

95
00:03:53,960 --> 00:03:55,940
The third column shows you

96
00:03:55,940 --> 00:03:57,560
the mean calories per

97
00:03:57,560 --> 00:04:00,545
day with the center
deviation in parentheses.

98
00:04:00,545 --> 00:04:02,420
Now, we see that the
sample means for

99
00:04:02,420 --> 00:04:04,865
the three groups are
different from each other.

100
00:04:04,865 --> 00:04:06,290
But again, the question is,

101
00:04:06,290 --> 00:04:08,150
do these three sample
means differ from

102
00:04:08,150 --> 00:04:10,085
each other enough relative

103
00:04:10,085 --> 00:04:11,800
to the standard deviations

104
00:04:11,800 --> 00:04:13,950
and the sample size that we have?

105
00:04:13,950 --> 00:04:15,530
Now to discuss how an

106
00:04:15,530 --> 00:04:17,930
overworked specific
to this example,

107
00:04:17,930 --> 00:04:20,030
I'm going to reduce
the sample sizes in

108
00:04:20,030 --> 00:04:22,520
each of the samples
by a factor of 100.

109
00:04:22,520 --> 00:04:24,590
So let us imagine that
the first sample,

110
00:04:24,590 --> 00:04:26,495
only has 24 observations.

111
00:04:26,495 --> 00:04:28,575
The second group,
has 12 observations.

112
00:04:28,575 --> 00:04:31,500
The third group, has
six observations.

113
00:04:31,500 --> 00:04:33,660
The first thing we do in ANOVA,

114
00:04:33,660 --> 00:04:35,494
is compute the average

115
00:04:35,494 --> 00:04:38,660
across all of the
observations in the data set.

116
00:04:38,660 --> 00:04:42,355
So again, I have 24 observations
with a mean of 1,782,

117
00:04:42,355 --> 00:04:45,235
12 observations with
a mean of 2,007,

118
00:04:45,235 --> 00:04:48,445
and six observations
with a mean of 2,143.

119
00:04:48,445 --> 00:04:53,885
This produces a grand mean
of 1,898 calories per day.

120
00:04:53,885 --> 00:04:58,010
This is the average for
all 46 observations.

121
00:04:58,010 --> 00:05:00,470
Now, if the null
hypothesis is true,

122
00:05:00,470 --> 00:05:02,930
that all populations
have the same mean,

123
00:05:02,930 --> 00:05:04,340
then sample means from

124
00:05:04,340 --> 00:05:06,830
these three samples should
all be close to each

125
00:05:06,830 --> 00:05:08,660
other and I'll be close to

126
00:05:08,660 --> 00:05:11,120
this grand mean that
we just computed.

127
00:05:11,120 --> 00:05:13,820
So we can visually
represent this fact.

128
00:05:13,820 --> 00:05:16,190
I showed you three
horizontal blue lines

129
00:05:16,190 --> 00:05:18,470
representing the means
for the three groups,

130
00:05:18,470 --> 00:05:21,365
and I show you the distance
that each has from

131
00:05:21,365 --> 00:05:23,120
the dashed yellow line which is

132
00:05:23,120 --> 00:05:25,850
the grand mean across
all three groups.

133
00:05:25,850 --> 00:05:28,700
It is the length of
these arrows that

134
00:05:28,700 --> 00:05:30,185
quantifies how distinct

135
00:05:30,185 --> 00:05:31,985
the three groups are
from each other.

136
00:05:31,985 --> 00:05:33,920
The larger these arrows get,

137
00:05:33,920 --> 00:05:36,320
the more the blue lines move from

138
00:05:36,320 --> 00:05:38,555
the dashed yellow line signifying

139
00:05:38,555 --> 00:05:41,615
that there are differences
among the groups.

140
00:05:41,615 --> 00:05:45,900
So therefore, the signal
or the evidence in ANOVA,

141
00:05:45,900 --> 00:05:47,180
is the average of

142
00:05:47,180 --> 00:05:49,085
the squared distances that

143
00:05:49,085 --> 00:05:51,875
each sample mean is
from the overall mean.

144
00:05:51,875 --> 00:05:53,960
Now, each group also receives

145
00:05:53,960 --> 00:05:56,570
a weight equal to that
group sample size.

146
00:05:56,570 --> 00:05:58,489
When I compute my statistic,

147
00:05:58,489 --> 00:06:01,325
I want to account for how much
data come from each group.

148
00:06:01,325 --> 00:06:03,830
If one's group has
very few observations,

149
00:06:03,830 --> 00:06:05,510
it should contribute less to

150
00:06:05,510 --> 00:06:08,825
the statistic than a group
that has a large sample size.

151
00:06:08,825 --> 00:06:12,980
So if I use X-bar with a
dot subscript to represent

152
00:06:12,980 --> 00:06:14,960
the overall mean across all of

153
00:06:14,960 --> 00:06:17,340
the data and I use X-bar 1,

154
00:06:17,340 --> 00:06:19,580
X-bar 2, and X-bar 3 to represent

155
00:06:19,580 --> 00:06:22,805
the corresponding means
of each of the samples.

156
00:06:22,805 --> 00:06:25,310
Then the signal, is
the average squared

157
00:06:25,310 --> 00:06:28,775
distance that each sample
mean has from the grand mean.

158
00:06:28,775 --> 00:06:31,545
So you can see I have
X-bar 1 minus X-bar.

159
00:06:31,545 --> 00:06:33,380
representing the distance of

160
00:06:33,380 --> 00:06:35,810
the first group mean
from the overall mean,

161
00:06:35,810 --> 00:06:37,250
I square that and multiply it by

162
00:06:37,250 --> 00:06:39,350
24 for the 24-hour observations

163
00:06:39,350 --> 00:06:41,750
and I have corresponding
computations

164
00:06:41,750 --> 00:06:43,295
for the second or third group.

165
00:06:43,295 --> 00:06:45,770
I add up these three
numbers and then compute

166
00:06:45,770 --> 00:06:49,850
an average and I divide by
3 minus 1 instead of 3.

167
00:06:49,850 --> 00:06:53,345
So if we are comparing
the means of K groups,

168
00:06:53,345 --> 00:06:56,240
the general formula
for this computation,

169
00:06:56,240 --> 00:06:57,620
is shown in front of you.

170
00:06:57,620 --> 00:06:59,315
We call this computation

171
00:06:59,315 --> 00:07:02,330
the mean sum of squares
between the groups.

172
00:07:02,330 --> 00:07:05,585
It quantifies the
overall differences

173
00:07:05,585 --> 00:07:08,425
that we see among the
groups in the data.

174
00:07:08,425 --> 00:07:10,910
Now, this computation
is an average

175
00:07:10,910 --> 00:07:13,820
and the denominator used
to compute the average,

176
00:07:13,820 --> 00:07:16,685
is known as the degrees of
freedom in this calculation.

177
00:07:16,685 --> 00:07:18,620
Degrees of freedom is a term used

178
00:07:18,620 --> 00:07:20,210
by numerous statistical tests.

179
00:07:20,210 --> 00:07:21,770
We'll see this term again in

180
00:07:21,770 --> 00:07:25,115
a few slides when we talk
about deriving a p-value.

181
00:07:25,115 --> 00:07:26,930
For my specific example

182
00:07:26,930 --> 00:07:28,895
using the data from
Pace and colleagues,

183
00:07:28,895 --> 00:07:30,680
the computation for
the mean square

184
00:07:30,680 --> 00:07:33,080
between ends up to be
a number just over

185
00:07:33,080 --> 00:07:37,220
400,000 which really has no
interpretation at this point.

186
00:07:37,220 --> 00:07:38,645
This is my signal

187
00:07:38,645 --> 00:07:40,340
telling me whether
there are differences

188
00:07:40,340 --> 00:07:43,690
among the groups and
now I ask myself,

189
00:07:43,690 --> 00:07:45,305
is this signal big enough

190
00:07:45,305 --> 00:07:48,575
relative to the amount of
noise I have in my data?

191
00:07:48,575 --> 00:07:50,735
The amount of noise in the data,

192
00:07:50,735 --> 00:07:54,410
is quantified by each of
the individual observations

193
00:07:54,410 --> 00:07:56,330
and how far each of those is from

194
00:07:56,330 --> 00:07:58,505
the mean of their specific group.

195
00:07:58,505 --> 00:07:59,925
So the noise of the data,

196
00:07:59,925 --> 00:08:01,670
is quantified by the
length of each of

197
00:08:01,670 --> 00:08:03,710
these arrows which
are the distance

198
00:08:03,710 --> 00:08:05,060
of each observation or

199
00:08:05,060 --> 00:08:08,000
the deviance from the
mean of their group.

200
00:08:08,000 --> 00:08:11,120
So I have 24 observations
from the first group,

201
00:08:11,120 --> 00:08:12,200
I add up how far each of

202
00:08:12,200 --> 00:08:14,635
those observation is from
the first sample mean.

203
00:08:14,635 --> 00:08:16,910
I have 12 observations
in the second group,

204
00:08:16,910 --> 00:08:19,399
so I add up their
squared differences,

205
00:08:19,399 --> 00:08:21,890
and I have six observations
in the third group and add up

206
00:08:21,890 --> 00:08:25,280
their six squared
distances from their mean.

207
00:08:25,280 --> 00:08:28,640
I then add all three of
these numbers together.

208
00:08:28,640 --> 00:08:34,870
This is the total amount of
noise quantified in my data.

209
00:08:34,870 --> 00:08:38,150
Now, we take this sum and
again like with mean,

210
00:08:38,150 --> 00:08:40,535
square, between, we
compute an average.

211
00:08:40,535 --> 00:08:42,560
There were 42 observations in

212
00:08:42,560 --> 00:08:44,780
this computation which
is the sample size.

213
00:08:44,780 --> 00:08:47,420
But we don't divide by
42 instead we divided by

214
00:08:47,420 --> 00:08:50,045
42 reduced by the
number of groups.

215
00:08:50,045 --> 00:08:51,930
Which in this case, was three.

216
00:08:51,930 --> 00:08:53,150
So in front of you, you see

217
00:08:53,150 --> 00:08:55,775
the general equation
for an ANOVA that has

218
00:08:55,775 --> 00:08:57,680
three groups in
which we're trying

219
00:08:57,680 --> 00:09:00,760
to quantify the amount
of noise in the data.

220
00:09:00,760 --> 00:09:03,845
We call this the
mean squared arrow.

221
00:09:03,845 --> 00:09:05,645
Again, this quantifies
the amount of

222
00:09:05,645 --> 00:09:08,195
skepticism that we
have in our data.

223
00:09:08,195 --> 00:09:10,970
This number also has
a degrees of freedom,

224
00:09:10,970 --> 00:09:13,040
the number that we use
for the averaging.

225
00:09:13,040 --> 00:09:16,970
Again, this will play a
role in a few slides ahead.

226
00:09:16,970 --> 00:09:20,870
If we apply this formula to
the data from pay settle,

227
00:09:20,870 --> 00:09:24,590
we get a mean squared
error just under 400,000.

228
00:09:24,590 --> 00:09:26,675
Now, like a t-test,

229
00:09:26,675 --> 00:09:29,570
the test statistic is
the ratio of the signal

230
00:09:29,570 --> 00:09:32,660
in the noise or the
evidence in the skepticism.

231
00:09:32,660 --> 00:09:33,920
A compute the ratio of

232
00:09:33,920 --> 00:09:35,780
the mean square
between relative to

233
00:09:35,780 --> 00:09:39,800
the mean squared error and
I call this an F-statistic.

234
00:09:39,800 --> 00:09:41,830
Now the next step, is to take

235
00:09:41,830 --> 00:09:44,375
our statistic and
convert it to a p-value.

236
00:09:44,375 --> 00:09:46,330
This p-value then
tells me whether or

237
00:09:46,330 --> 00:09:48,370
not to reject the
null hypothesis.

238
00:09:48,370 --> 00:09:50,020
Now, when a two sample t-test,

239
00:09:50,020 --> 00:09:52,525
the p-value came from
a normal distribution.

240
00:09:52,525 --> 00:09:55,345
For ANOVA, we no longer
use a normal distribution,

241
00:09:55,345 --> 00:09:58,270
we use a distribution
called an F-distribution.

242
00:09:58,270 --> 00:10:01,660
But just like a t-test
we want large values of

243
00:10:01,660 --> 00:10:03,430
our test statistic in order

244
00:10:03,430 --> 00:10:05,605
to reject the null hypothesis.

245
00:10:05,605 --> 00:10:07,840
So if the F-statistic is large,

246
00:10:07,840 --> 00:10:11,620
the observations tend to
cluster in their groups making

247
00:10:11,620 --> 00:10:13,600
the groups distinct from
each other rather than there

248
00:10:13,600 --> 00:10:16,120
being a lot of overlap
among the groups.

249
00:10:16,120 --> 00:10:18,235
So returning to the example

250
00:10:18,235 --> 00:10:19,450
of whether or not consumption of

251
00:10:19,450 --> 00:10:20,830
sugary beverages and adults is

252
00:10:20,830 --> 00:10:23,425
associated with level
of calorie intake,

253
00:10:23,425 --> 00:10:27,415
we get enough statistic
for this example of 1.05.

254
00:10:27,415 --> 00:10:28,930
Again now, this question is,

255
00:10:28,930 --> 00:10:30,800
is the statistic large enough

256
00:10:30,800 --> 00:10:33,410
for me to reject the
null hypothesis?

257
00:10:33,410 --> 00:10:36,740
Now, unlike a t-test where we
had a critical value of 2,

258
00:10:36,740 --> 00:10:39,920
we do not use that
value with ANOVA.

259
00:10:39,920 --> 00:10:43,230
The p-value in this
example, is 0.36.

260
00:10:43,230 --> 00:10:47,195
In general, the p-value computed
for an ANOVA statistic,

261
00:10:47,195 --> 00:10:49,850
is based upon the
specific enumerator and

262
00:10:49,850 --> 00:10:51,485
denominator degrees of freedom

263
00:10:51,485 --> 00:10:52,970
that we're seeing in your data.

264
00:10:52,970 --> 00:10:54,710
Again, these degrees
of freedom are

265
00:10:54,710 --> 00:10:56,570
a function of the sample size and

266
00:10:56,570 --> 00:10:58,130
the number of groups which will

267
00:10:58,130 --> 00:11:00,515
change from data-set to data-set.

268
00:11:00,515 --> 00:11:04,135
Therefore, the
F-statistic will change.

269
00:11:04,135 --> 00:11:07,260
Therefore, we fail to
reject the null hypothesis,

270
00:11:07,260 --> 00:11:08,600
we conclude that we lack

271
00:11:08,600 --> 00:11:10,865
evidence that there
is an association

272
00:11:10,865 --> 00:11:12,665
between sugary drink consumption

273
00:11:12,665 --> 00:11:15,010
and daily caloric consumption.

274
00:11:15,010 --> 00:11:16,825
As I've emphasized earlier,

275
00:11:16,825 --> 00:11:19,190
this does not mean that
we conclude that there is

276
00:11:19,190 --> 00:11:22,475
no association between
sugary drink consumption

277
00:11:22,475 --> 00:11:24,290
and daily caloric consumption.

278
00:11:24,290 --> 00:11:26,885
We never accept the
null hypothesis.

279
00:11:26,885 --> 00:11:28,670
In this case, we simply lack

280
00:11:28,670 --> 00:11:30,170
enough evidence and that is

281
00:11:30,170 --> 00:11:32,450
because we have a
sample size of 42.

282
00:11:32,450 --> 00:11:34,040
It's very difficult to detect

283
00:11:34,040 --> 00:11:35,650
differences between three groups

284
00:11:35,650 --> 00:11:37,040
when we only have a total of

285
00:11:37,040 --> 00:11:39,850
42 observations in our data-set.

286
00:11:39,850 --> 00:11:42,800
Now, the results from
an ANOVA are often

287
00:11:42,800 --> 00:11:45,830
displayed in what we
call an ANOVA table.

288
00:11:45,830 --> 00:11:48,550
This table has six columns.

289
00:11:48,550 --> 00:11:50,705
The first column
will always tell you

290
00:11:50,705 --> 00:11:53,510
the source of the
signal and the noise,

291
00:11:53,510 --> 00:11:55,310
between groups is the signal,

292
00:11:55,310 --> 00:11:57,355
within groups as the noise.

293
00:11:57,355 --> 00:12:00,350
The second column, is what
we call the sum of squares.

294
00:12:00,350 --> 00:12:01,760
This was the denominator of

295
00:12:01,760 --> 00:12:03,980
the computation that
I showed you earlier.

296
00:12:03,980 --> 00:12:06,080
We then take an average by

297
00:12:06,080 --> 00:12:08,120
dividing by the degrees
of freedom shown in

298
00:12:08,120 --> 00:12:11,690
the third column producing
the mean-squared between in

299
00:12:11,690 --> 00:12:13,070
the first row and the mean

300
00:12:13,070 --> 00:12:15,755
squared error in the second row,

301
00:12:15,755 --> 00:12:18,590
we then take the ratio of
these two mean squares to get

302
00:12:18,590 --> 00:12:22,300
an F-statistic and the
computer finds a p-value.

303
00:12:22,300 --> 00:12:24,869
So like a 2-sample t-test,

304
00:12:24,869 --> 00:12:26,610
the true elegance of ANOVA,

305
00:12:26,610 --> 00:12:27,920
is often lost in

306
00:12:27,920 --> 00:12:30,920
the computations and the
formulas that I just presented.

307
00:12:30,920 --> 00:12:33,440
All of them are important
for the computer to use.

308
00:12:33,440 --> 00:12:36,860
They often are complicated for
the human brain to digest.

309
00:12:36,860 --> 00:12:38,900
So instead, I'd like to strip

310
00:12:38,900 --> 00:12:40,940
away the mathematics
and instead use

311
00:12:40,940 --> 00:12:43,145
a visual heuristic
that conveys how ANOVA

312
00:12:43,145 --> 00:12:49,736
operates in a SQL
to this lecture.

