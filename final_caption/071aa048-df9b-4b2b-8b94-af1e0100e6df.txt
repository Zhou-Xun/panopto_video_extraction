1
00:00:01,980 --> 00:00:05,730
Okay. So we're going to finish up our discussion of instrumental variables today.

2
00:00:05,790 --> 00:00:19,020
Now, before we dove in on that, let me just see that people have sort of existing questions or things leftover they want to bring up.

3
00:00:25,080 --> 00:00:33,690
Okay. So you saw I posted the new groups, so hopefully everybody's able to move on that word again.

4
00:00:33,720 --> 00:00:40,049
You know, [INAUDIBLE] still have some time, but, um, but probably should be starting to think a little bit about that.

5
00:00:40,050 --> 00:00:43,920
And if you're really stuck, let me know talk and them through some things.

6
00:00:45,390 --> 00:00:50,340
Okay. So instrumental variables for me.

7
00:00:50,340 --> 00:00:54,030
Just make sure everybody has their little patience. Okay.

8
00:00:54,390 --> 00:00:59,700
I actually have extra time to show you to your own DIY version.

9
00:00:59,820 --> 00:01:15,340
That's cool. And basically give you a punchline.

10
00:01:15,350 --> 00:01:29,930
Steve. Sorry. If you give me one of these just middle of the road in the glasses over an interesting

11
00:01:29,940 --> 00:01:34,460
situation with an eye that that ten years ago where I had a retina get detached.

12
00:01:35,210 --> 00:01:41,840
Long story short, I ended up finally having a lens installed in when I had artificial lens,

13
00:01:41,840 --> 00:01:46,430
which created distance vision in eyes for the first time since I was like eight.

14
00:01:47,480 --> 00:01:49,879
So when I can see far end, when I see close.

15
00:01:49,880 --> 00:01:56,930
And so I've actually gotten lazy about movies and classes, but I think that needs to needs to stop because they continue to deteriorate.

16
00:01:58,680 --> 00:02:04,700
Okay. So anyway, we had this idea, the instrumental variable,

17
00:02:05,060 --> 00:02:11,000
just to quickly review this idea in search of the fact that that if we have

18
00:02:11,000 --> 00:02:15,560
confounders between an exposure that we want to relate to an outcome causally,

19
00:02:16,730 --> 00:02:23,420
right? So that basically well, we're not really quite in a particular outcome framework here.

20
00:02:23,420 --> 00:02:29,780
So we're thinking really about just trying to eliminate confounding in some fashion.

21
00:02:30,830 --> 00:02:42,410
And the the issue here is that if we have a another variable Z,

22
00:02:44,270 --> 00:02:54,200
so that Z is independent of the residuals that relate X to Y, then if we do a regression of Z on X,

23
00:02:55,370 --> 00:03:01,729
we can take the predictive values from that regression, regression, regress that on Y,

24
00:03:01,730 --> 00:03:08,090
and then we get coefficient sort of regression coefficients that are unbiased for.

25
00:03:11,080 --> 00:03:17,680
The idea that we would like to have here if we if we somehow had these apps once said to zero.

26
00:03:18,610 --> 00:03:21,940
So a couple of ways to show this.

27
00:03:29,480 --> 00:03:49,360
Purple. Still working. Consumers could be involved here.

28
00:03:53,800 --> 00:03:57,370
Some sort of way over here.

29
00:03:58,550 --> 00:04:21,250
There. But just remember the model here in Portland observed that.

30
00:04:24,370 --> 00:04:28,330
Yeah. And also see it next year, I'm assuming, for the last year.

31
00:04:28,420 --> 00:04:37,110
So it's just a very simple in terms of your aggression will expand that will always rock again.

32
00:04:38,260 --> 00:04:44,400
The full story here we go back on normality because I'm not gonna spend that much time on it, but.

33
00:04:45,720 --> 00:04:51,990
That's right. So. We fit this linear regression.

34
00:05:22,380 --> 00:05:47,960
So. So there's two stands with two stations squares.

35
00:06:06,110 --> 00:06:22,970
So. To.

36
00:06:30,500 --> 00:06:34,190
Since extant now is going to be confounded that these epsilon is my assumption.

37
00:06:35,390 --> 00:06:53,130
It means we get an unbiased estimate of. Of these babies here.

38
00:06:53,190 --> 00:06:57,060
So. So.

39
00:06:57,910 --> 00:07:01,190
Could draw a little picture here. A little. Another example.

40
00:07:01,200 --> 00:07:10,950
But the idea that projecting X out here into space where it's no longer going to be confounded with the residuals.

41
00:07:12,210 --> 00:07:17,950
And so you use that projection then to do that regression as you enter the system.

42
00:07:56,850 --> 00:08:02,130
So just the other way to think about this is that.

43
00:08:12,730 --> 00:08:16,630
I think about the covariance, y and Z.

44
00:08:22,950 --> 00:08:36,150
So I plug in. I've been Place X.

45
00:08:39,110 --> 00:08:56,930
The this. Well done.

46
00:08:56,950 --> 00:09:03,099
Sorry. So I'm going to start with this part here. So very particular covariance of point Z.

47
00:09:03,100 --> 00:09:23,180
I think it does. Covariance of or not speed with one side is absolute zero.

48
00:09:25,340 --> 00:09:29,450
So like in the right data as the covariance beta not.

49
00:09:34,570 --> 00:09:43,770
This covariance beta one was the two.

50
00:09:50,600 --> 00:09:56,720
So. So what's the story here?

51
00:10:01,320 --> 00:10:05,400
00. 2 seconds. So zero.

52
00:10:07,950 --> 00:10:20,700
So this covariance here. Can I just write this?

53
00:10:24,910 --> 00:10:46,330
So to pull out there is one. And this covariance is there is kind of a covariance, but it's here in this assumption.

54
00:10:47,820 --> 00:10:51,330
So we just get that. And so.

55
00:10:55,810 --> 00:11:06,220
So that implies better. One covariance I expect higher.

56
00:11:10,150 --> 00:11:16,280
So. This is just an estimation.

57
00:11:27,470 --> 00:11:45,000
A variances. They one.

58
00:11:47,130 --> 00:12:05,600
So. So the sort of readings on this sort of state that I think there's a little part of my anyway and a little haven't quite worked

59
00:12:05,600 --> 00:12:13,010
out the details on conceptually if you can see it right that basically the description these terms estimate is giving you.

60
00:12:16,340 --> 00:12:21,560
A covariance of.

61
00:12:26,200 --> 00:12:29,380
Why? With Glee.

62
00:12:30,190 --> 00:12:34,839
But we have to get them. When you work out all the other components,

63
00:12:34,840 --> 00:12:40,960
you actually end up with the part here where you're dividing by the colors with axes and which is the, you know, the X network.

64
00:12:42,150 --> 00:12:48,010
So it's going to be a good way to do that.

65
00:13:03,600 --> 00:13:11,740
The first part is most efficient. The second one is study. Statement of fact.

66
00:13:24,330 --> 00:13:31,710
Two questions. Right.

67
00:13:35,650 --> 00:13:43,720
So this method was originally developed back in economics theory back in the 1930s and I think is where it dates.

68
00:13:47,200 --> 00:13:53,430
So. Sorry.

69
00:13:55,940 --> 00:14:02,060
So I think this was one of the motivating examples.

70
00:14:03,210 --> 00:14:08,960
So the and it's basically the sort of measure of elasticity,

71
00:14:09,650 --> 00:14:15,920
which is basically in like many things that come sticks with the same concept, different words.

72
00:14:16,340 --> 00:14:26,720
So this is basically a regression coefficient. It's sort of the, the, the beta coefficient associated with how supply and demand relate together.

73
00:14:27,940 --> 00:14:36,709
And so, so if we think about this, usually take logs because things are sort of considered multiplicative,

74
00:14:36,710 --> 00:14:46,460
typically associated with a log of butter production, the log of the price of butter, how, how the price of butter might affect butter production.

75
00:14:47,900 --> 00:14:55,430
So this elasticity is basically going to be the change in that percent quantity for the change in the percent price.

76
00:14:56,870 --> 00:15:03,500
So the issue here is that Epsilon and Z are kind of strongly correlated due to supply and demand curves.

77
00:15:04,250 --> 00:15:11,840
And I'm not an economist, but for the moment over tend to be something of one.

78
00:16:14,050 --> 00:16:36,320
All right. So the fundamental. This quantity is something price.

79
00:16:38,780 --> 00:16:46,880
Generally you're going to get more things. So we're made the cheaper and we haven't got this one right.

80
00:16:49,820 --> 00:16:57,920
So the thing is, though, this curve, you have to you have to be this sort of thing constant for the intercepts may change over time.

81
00:16:58,190 --> 00:17:11,810
Right. So many other factors. You know, maybe you're going to get a few people more well off at some point in time.

82
00:17:12,710 --> 00:17:16,850
So the underlying leadership may stay the same, but this sort of intersects with more money.

83
00:17:17,660 --> 00:17:23,120
So for a given quantity, they're going to pay a higher price.

84
00:17:26,390 --> 00:17:29,810
And of course, it could be other things and sort of demand things like changes.

85
00:17:30,830 --> 00:17:35,750
So you can imagine over time who's going to be like that.

86
00:17:35,780 --> 00:17:41,870
Now, the problem is we only get to measure these at one point or a counterfactual.

87
00:17:43,100 --> 00:17:54,400
So maybe there is some sort of reduction in price here, here, here and here and here.

88
00:17:55,460 --> 00:18:07,830
So we know that the observed data. It's something that actually might look weird, or at least not very important.

89
00:18:09,460 --> 00:18:22,520
So. So the idea is that.

90
00:18:24,150 --> 00:18:36,420
We want to get something that can kind of hold all these points to a common curve.

91
00:18:44,250 --> 00:18:58,560
We want to do our estimates based on that. So this sort of idea of rejection.

92
00:19:00,680 --> 00:19:05,850
See the statistics resource estimate is also sort of a projection estimate.

93
00:19:06,970 --> 00:19:13,120
So you're kind of shifting in this case, you're kind of shifting supply, but but not demand.

94
00:19:13,120 --> 00:19:18,940
You'd want to have something that would be be constant. I want it to be the single curve.

95
00:19:22,990 --> 00:19:26,320
So, I mean, one candidate for this might be rainfall.

96
00:19:28,190 --> 00:19:33,010
All right. Just General, even today, weather affects prices of agricultural products.

97
00:19:33,970 --> 00:19:37,990
There's usually a lot of more processing maybe than it was in the 1930.

98
00:19:37,990 --> 00:19:42,610
So it's a little less direct, but certainly an impact.

99
00:19:44,770 --> 00:19:50,470
And so the nice thing about rainfall is it takes insurgents, right.

100
00:19:51,250 --> 00:19:54,290
Presumably doesn't really affect demand very much, let's say, for butter.

101
00:19:54,310 --> 00:20:00,710
Now, there may be other things, umbrellas. It might not be so reasonable for them.

102
00:20:01,900 --> 00:20:06,820
So this idea of indulgent eating versus exercise in Haiti is basically a way of sort of saying confounding

103
00:20:06,820 --> 00:20:11,500
versus not confounding in our in our language exogenous or these things that have to be uncorrelated.

104
00:20:12,070 --> 00:20:22,540
Right. So we get to Z uncorrelated with this residual acquire an X and it's it's hopefully explanatory, right?

105
00:20:22,540 --> 00:20:30,939
So that basically there is some association between rainfall and quantity, right?

106
00:20:30,940 --> 00:20:34,750
So maybe you can produce, you know, less butter.

107
00:20:34,840 --> 00:20:40,450
If there's a drought, you can't feed your cattle the way you would normally do so.

108
00:20:42,630 --> 00:20:48,240
So basically then two states, this first estimate progresses better on rainfall,

109
00:20:48,900 --> 00:20:54,390
water price and rainfall and then regresses as predicted price from the rainfall regression on the supply of butter.

110
00:20:55,950 --> 00:21:05,850
So basically sort of trying to do this projection back like what would happen if we'd had sort of, you know, a.

111
00:21:11,930 --> 00:21:17,090
No right. A consistent rainfall, but then change supplies in other ways.

112
00:21:18,020 --> 00:21:23,300
So the resulting data to say the squares estimate is going to be this elasticity measure of butter.

113
00:21:26,800 --> 00:21:31,360
So. Another example which actually is used, I think,

114
00:21:31,360 --> 00:21:39,849
in one of the papers that will sort of wrap up with us at the end is the effect of

115
00:21:39,850 --> 00:21:45,969
service in Vietnam War on survival after the war so not be killed during the war,

116
00:21:45,970 --> 00:21:58,850
but things like increased either direct risks of suicide or other health problems that that would lead to to service.

117
00:21:58,870 --> 00:22:02,500
Okay. Right. I forgot a little. Don't try to be bipartisan there.

118
00:22:03,460 --> 00:22:05,170
Service. No. Was not randomized. Right.

119
00:22:05,170 --> 00:22:14,770
Despite the draft, if you were well off and maybe had better survival long run anyway, then you tended not to get get drafted.

120
00:22:15,940 --> 00:22:20,980
However, your risk was was probably somewhat affected by the randomization.

121
00:22:21,010 --> 00:22:28,970
There was a randomized lottery, so basically day of the randomized, the first sort of end dates called based on demand for the military.

122
00:22:29,770 --> 00:22:34,570
It's a pretty good instrument, right? So it's clearly randomized.

123
00:22:37,660 --> 00:22:40,780
It's, you know, literally it was randomized.

124
00:22:40,780 --> 00:22:48,790
So there's no way to have an association C with it with anything, including whatever residual you might have, one between service and survival.

125
00:22:50,590 --> 00:22:58,300
And but you are more likely to serve your draft and what's called I mean if Biden Trump could get out of it but if you were a little less connected,

126
00:22:58,750 --> 00:23:02,650
you know, and you had a very high number, it was a little bit easier. It just didn't look good.

127
00:23:03,820 --> 00:23:10,780
And depending upon your local political situation, you might or might or might not be able to do that.

128
00:23:10,780 --> 00:23:14,230
Whereas if you were kind of on the edge, it was always a little easier.

129
00:23:15,370 --> 00:23:19,209
So. So it wasn't totally randomized, right?

130
00:23:19,210 --> 00:23:27,700
Because because of the reality of the world, particularly 1960s and the unpopularity of the war, frankly.

131
00:23:28,840 --> 00:23:31,840
But but it did it did have an effect.

132
00:23:33,040 --> 00:23:38,560
Okay. So that's another example of many other possibilities you didn't even think about.

133
00:23:42,740 --> 00:23:49,190
Okay. So talk about variance estimation, but questions come into.

134
00:23:55,510 --> 00:24:06,549
Right. And so. So variance estimation is somewhat tricky because we're not conditioning on on X quite in the same way.

135
00:24:06,550 --> 00:24:11,110
Right. Where a regression of is on this on Y on x hat.

136
00:24:12,040 --> 00:24:17,050
So. So you've got to kind of break this down to to the extent of disease.

137
00:24:18,100 --> 00:24:30,030
And one way to do that right is if we kind of replace our index heads with disease indexes, right?

138
00:24:30,040 --> 00:24:35,710
So the alpha hat is basically Z times this prediction.

139
00:24:36,810 --> 00:24:38,200
These are the predicted values of X.

140
00:24:40,690 --> 00:24:55,570
Well, the predictive values of X come from the multiplying Z by by alpha hint or or regression coefficient to progression of both X, Y and z.

141
00:24:56,350 --> 00:25:01,480
So plugging that in. This is easy.

142
00:25:01,500 --> 00:25:05,340
Transpose the inverse C, transpose x everywhere for x hat.

143
00:25:06,090 --> 00:25:18,480
We get this kind of messy thing down here. We can do some some initial resolving of of it because we can take the Z's transpose Z.

144
00:25:19,190 --> 00:25:22,530
It's being here Z transpose the inverse. So those cancel.

145
00:25:23,250 --> 00:25:27,930
So we're just left with X transpose easy transpose the inverse C, transpose x.

146
00:25:29,730 --> 00:25:33,210
And nothing like is over here. So we're doing the same thing here.

147
00:25:34,530 --> 00:25:42,959
But. But we can rewrite. If we write Z, transpose Z, Z, transpose the inverse city transpose is this pill some Z.

148
00:25:42,960 --> 00:25:48,630
This projection matrix essentially was doing that that thought of drawing that

149
00:25:48,840 --> 00:25:54,290
from the the sort of observe points to push things back to that common line.

150
00:25:55,980 --> 00:26:04,170
Then we can work with this, right? So this, this is actually pretty easy to work with because this is just a weighted squares estimate.

151
00:26:04,590 --> 00:26:04,890
Right.

152
00:26:05,370 --> 00:26:14,010
Where the weight is essentially this if you think of it as a sort of pseudo covariance matrix or pseudo weight matrix of of of this projection piece.

153
00:26:17,770 --> 00:26:23,020
So if we kind of borrow from the survey statistics,

154
00:26:23,020 --> 00:26:37,660
literature and generalized estimating equation literature, we can show that basically if we just take that.

155
00:26:42,600 --> 00:26:49,110
The mean of these diagonals of the observed residuals.

156
00:26:50,940 --> 00:26:59,639
Use those as an estimate a of this V of of the and the variance of them and then embed

157
00:26:59,640 --> 00:27:04,950
them in this so-called sandwich estimate that gives us an estimate of the variance.

158
00:27:06,810 --> 00:27:10,800
So how does that work? Well, it's a little it's a little funky.

159
00:27:12,390 --> 00:27:17,250
So you start by noting that, again, this is solve this score equation, right.

160
00:27:17,250 --> 00:27:25,469
That if I have my beta happen, right.

161
00:27:25,470 --> 00:27:35,550
If I if I set the score equation in terms of beta, two squiggly squares set it to zero,

162
00:27:36,720 --> 00:27:47,190
then my if I, if I set this equation to zero, then, then I get my to surgery square system here.

163
00:27:47,430 --> 00:28:03,110
Right. So I guess so. You know, I've just read that up there.

164
00:28:41,660 --> 00:28:46,670
So it's a zero because we.

165
00:28:58,210 --> 00:30:12,060
On. Backwards.

166
00:30:18,530 --> 00:30:31,559
So this first part here. So the other funny little trick you can do here is that if you think of the score equation as a function, right,

167
00:30:31,560 --> 00:30:40,050
if you take that function and then plug it into the inverse of that function, then you get the original term back, right?

168
00:30:40,980 --> 00:30:46,560
And so if you think about the variance then and this should be sorry, this should be equal sine I don't know that translation didn't work right.

169
00:30:48,210 --> 00:30:52,740
So if I use the delta method here and.

170
00:31:00,450 --> 00:31:05,620
So basically for this outer part, I have this inverse of you, right?

171
00:31:05,640 --> 00:31:13,740
I take the derivative respect to the my data here, and then I just have the variance of this in part to this part here.

172
00:31:13,740 --> 00:31:17,370
So I know you sort of have to think about these uses separately here for this.

173
00:31:18,210 --> 00:31:32,060
Right. So the other part is. I'm taking the inverse of a function here and sandwich in around the variance of the center part.

174
00:31:33,450 --> 00:31:36,500
So it's just a multivariate delta method for variance approximation.

175
00:31:36,740 --> 00:31:42,500
So you probably touched on this at various points, but you can go back and look at it if you want to.

176
00:31:42,500 --> 00:31:46,580
We could you could show it a little more detail, but I'm gonna leave it here for now.

177
00:31:50,170 --> 00:31:56,260
So now I just got to work up this inner part. Right.

178
00:31:56,260 --> 00:31:59,410
So the variance of the score equation now. Right.

179
00:31:59,650 --> 00:32:06,340
It's just that thing up there for here. So the only and this is the one thing that's random here is going to be Y, right?

180
00:32:06,550 --> 00:32:12,880
So this is part of the variance of this,

181
00:32:14,110 --> 00:32:18,940
plus the variance of this minus twice the covariance will this the variance of

182
00:32:19,580 --> 00:32:24,910
x transpose x beta tell is just zero right into the curvature of the zero.

183
00:32:24,920 --> 00:32:28,450
So we just have this part here. And indeed, this is a constant.

184
00:32:30,040 --> 00:32:35,310
So I have a constant times. Oh, I'm sorry.

185
00:32:35,330 --> 00:32:38,450
I guess this is the residual piece, right? Sorry.

186
00:32:38,900 --> 00:32:46,630
So we're just rebranding this as their position. So a variance of Constantine's a residual matrix world here.

187
00:32:46,640 --> 00:32:56,300
So I take the square root, which in matrix terms means I put the matrix part here and then hit it with the inverse.

188
00:32:56,900 --> 00:33:01,060
I should note that the inverse of PMC. I'm sorry, hit it with the transpose.

189
00:33:01,460 --> 00:33:07,010
The transpose a z is z. It's one of its own important, I guess.

190
00:33:07,010 --> 00:33:13,430
Right. So this is equal to the original.

191
00:33:14,030 --> 00:33:19,939
So just leaves me with the variance of these residual pieces here. And if we assume independence across observations,

192
00:33:19,940 --> 00:33:25,700
we just estimate the variance of the vector residuals using this robust estimate of this kind of robust estimate.

193
00:33:26,060 --> 00:33:34,580
So identical to derivations in survey statistics when we have a sort of pseudo maximum likelihood two meters.

194
00:33:35,340 --> 00:33:40,850
And so it's similar to what you would see from getting a survey statistic estimate or for regression model.

195
00:33:42,470 --> 00:33:46,670
It's also comes out of generalization equation theory, right?

196
00:33:46,670 --> 00:33:55,370
So you probably touch on that in six or maybe you're going to be touching on 653.

197
00:33:55,700 --> 00:34:00,440
Sorry, I'm going to be b slightly ahead of you here, but you'll probably see a little bit of it there.

198
00:34:01,400 --> 00:34:09,650
And yet so is all sort of done independently in different, different spots where it all works the same way.

199
00:34:11,440 --> 00:34:21,170
Okay. So that's the way to get R variances later. So what are good instrumental variables?

200
00:34:22,400 --> 00:34:27,139
They're strong related to the treatment or exposure of interest there,

201
00:34:27,140 --> 00:34:31,310
unrelated or maybe more realistically, very weakly related to the outcome of interest a priori.

202
00:34:31,730 --> 00:34:40,040
So conditional on X. And finally that the that's the relationship with this C and Y can be explained entirely through X, right?

203
00:34:40,040 --> 00:34:51,610
So that's. Through the whatever relationship we have here it's explained entirely through acts because the assumption to.

204
00:34:56,290 --> 00:35:01,540
Again, the sort of gold standard for this is a randomized trial, right?

205
00:35:01,600 --> 00:35:04,630
Because the virtue is satisfied by design.

206
00:35:05,620 --> 00:35:10,599
Hopefully, number one works pretty well with the people actually with whatever who whatever the

207
00:35:10,600 --> 00:35:17,380
randomization is doing actually causes the exposure to happen for the treatment to be taken.

208
00:35:20,800 --> 00:35:29,530
So, but, but anything that meets this criteria doesn't have to be randomization can do this, right?

209
00:35:29,530 --> 00:35:37,570
So if there are things that are sort of quasi randomized or or happened to by chance have this, this kind of randomization flavor, sorry,

210
00:35:38,980 --> 00:35:47,530
and they're also associated with the treatment outcome and can work as an instrument so that,

211
00:35:47,590 --> 00:35:54,430
as I mentioned, these conditions often kind of fight against each other. You can have a variable that's strong related treatment exposure,

212
00:35:55,630 --> 00:35:59,050
but there may still still be kind of modest related the outcome independent

213
00:35:59,050 --> 00:36:04,150
of the treatment or exposures you feel condition two or condition two holds,

214
00:36:04,150 --> 00:36:11,440
but then condition one is is pretty weak. So there isn't much you can do about two that's just going to maybe get some reduction.

215
00:36:11,440 --> 00:36:18,250
Perhaps I'm bias but isn't going to is going to resolve it for condition one you might have some

216
00:36:18,340 --> 00:36:24,070
depending on your situation might be able to improve on things through multiple instruments.

217
00:36:25,420 --> 00:36:30,940
So we basically kind of move in this direction. But now it's sort of Z being scalar, it could be multivariate.

218
00:36:32,680 --> 00:36:38,649
So basic idea is you're going to do now this multivariate regression of Exxon Z still good

219
00:36:38,650 --> 00:36:44,670
exact still this scale piece and you put that in for your Tuesdays and squares this year.

220
00:36:46,150 --> 00:36:51,129
So if each one of them is kind of weakly associated but they're not totally correlated,

221
00:36:51,130 --> 00:36:53,500
obviously, if they're very highly correlated, you're going to get much gain here.

222
00:36:54,010 --> 00:36:58,120
But if you're weakly associated and fairly independent of each other collectively,

223
00:36:58,330 --> 00:37:04,870
they can still provide a pretty good prediction of of X based on based on see.

224
00:37:06,040 --> 00:37:16,030
So select. Another common things is price elasticity of cigarets and sort of understand average price and sales of cigarets.

225
00:37:18,160 --> 00:37:24,280
So you could look at things like overall sales tax and and cigaret specific sales tax.

226
00:37:25,690 --> 00:37:31,540
So again, just beside the point, why would these be potential instruments?

227
00:37:34,060 --> 00:37:48,650
Two features. So if you're looking at supply and if you're looking at demand, cigarets overall sales tax shouldn't really does affect price, right?

228
00:37:48,800 --> 00:37:52,550
They both affect price. So in that sense, they're kind of meet the first condition.

229
00:37:53,690 --> 00:37:59,360
What about the second condition? Particularly there might have been a change.

230
00:38:15,530 --> 00:38:21,500
Yeah. I mean, like, you know, one is first one is to basically is to controlling for how expensive or,

231
00:38:21,620 --> 00:38:26,060
you know, kind of like price, you know, expensive goods are in general.

232
00:38:26,090 --> 00:38:31,309
Right. And then and, you know, cigaret specific taxes and kind of I kind of you factor out,

233
00:38:31,310 --> 00:38:35,480
you know, the sensitivity of prices in general specifically for, you know, tobacco.

234
00:38:35,720 --> 00:38:41,960
Right. So that kind of helps do a better prediction, perhaps, of the sort of demand piece.

235
00:38:42,740 --> 00:38:47,000
But what about I'm sorry, of the price piece of the of the.

236
00:38:47,600 --> 00:38:50,749
No, I'm sorry. Yes, that, that. Right.

237
00:38:50,750 --> 00:38:55,670
So. Ah, sorry. My wife an next is here the.

238
00:39:01,840 --> 00:39:08,100
So it's basically the same thing here. But now it's up giving.

239
00:39:14,400 --> 00:39:20,930
So right. So demand for certain sort of cigaret sales is going to be tied to the price.

240
00:39:20,940 --> 00:39:26,480
So we hope that that sales tax will have some impact on price.

241
00:39:26,490 --> 00:39:33,180
If it doesn't, people coming mean the final price is it it's going to be some fixed price per sales tax, right?

242
00:39:33,660 --> 00:39:42,510
People go to sell things. Sometimes if taxes go up, the providers will maybe eat some of their own profits.

243
00:39:42,510 --> 00:39:48,360
Right. They'll try to keep the price from rising quite as much. They'll try to find suppliers that are cheaper.

244
00:39:48,360 --> 00:39:52,080
And in the long term, they may try to change production and cigarets.

245
00:39:52,140 --> 00:39:55,770
It's going to be a little bit of different issue, but but certainly the first part could easily happen.

246
00:39:57,390 --> 00:40:02,640
So so there's there's not a perfect relationship here between X and Z.

247
00:40:03,480 --> 00:40:07,590
But but we do we do hope that there's there's something going on.

248
00:40:07,890 --> 00:40:13,320
Now, what about the issue of the of the residual between C and Epsilon?

249
00:40:29,110 --> 00:40:39,950
So sort of. Cigaret price that to the epsilon is the relation is the residual from cigaret price on demand.

250
00:40:42,400 --> 00:40:45,740
Two. What about that tax? How is that going to be associated with that?

251
00:40:47,680 --> 00:40:52,710
How much it might help you be associated with it. I'm going to use it as an instrument.

252
00:40:55,000 --> 00:40:59,050
So the Turks probably wouldn't be like strong, right?

253
00:40:59,800 --> 00:41:02,230
It probably isn't necessarily related to that.

254
00:41:03,130 --> 00:41:09,790
It could be, particularly if you're looking across, say, states and United States, where these tax policies are very different.

255
00:41:10,420 --> 00:41:15,520
There may be some underlying different behaviors. So it's possible that might fail.

256
00:41:16,570 --> 00:41:21,340
But if you're looking to say within a state and this and these things change, you might fit that.

257
00:41:22,770 --> 00:41:25,200
That assumption made me stronger. Right.

258
00:41:25,200 --> 00:41:33,360
There was nothing that nothing suddenly happened about smoking behavior that was associated with the change in tax status.

259
00:41:34,530 --> 00:41:37,800
Independent of that change in tax. That's right.

260
00:41:38,560 --> 00:41:43,020
So so it sort of somewhat depends, I think, on the type of analysis.

261
00:41:44,910 --> 00:41:48,600
So I've also been to this kind of analysis recently.

262
00:41:48,600 --> 00:41:49,620
So I've been thinking about this.

263
00:41:49,620 --> 00:41:59,220
But but essentially that that relationship between Y and X is may sort of be somewhat if you're looking at, say, Massachusetts versus Texas or,

264
00:41:59,760 --> 00:42:05,850
you know, Massachusetts versus Kentucky, where we have big differences in tax prices and maybe big differences in smoking behavior,

265
00:42:06,270 --> 00:42:11,370
a priority of that, it's possible that the assumption might fail and so you lose the randomization

266
00:42:11,370 --> 00:42:15,660
component there if you sort of like within Kentucky and there's a change in tax policy,

267
00:42:16,590 --> 00:42:21,600
you don't necessarily think that was associated with with the smoking behavior.

268
00:42:22,230 --> 00:42:31,560
So yeah. So just to tie this back to the, you know, the the variation of for some cases, it would be cigaret specific and of the overall sales tax.

269
00:42:32,070 --> 00:42:36,180
No, no excise. The overall sales is a sales price price.

270
00:42:36,690 --> 00:42:41,670
Right. So as the price changes, both of sales tax could be associated with sales taxes.

271
00:42:41,880 --> 00:42:45,570
Okay. Right. So it could also be associated with the cigaret specific tax.

272
00:42:46,530 --> 00:42:50,590
And but they're probably not necessarily highly correlated. Then there's a correlation there.

273
00:42:50,610 --> 00:42:57,850
But so which is. Okay, so much point. So basically the idea again like they the before but.

274
00:42:58,930 --> 00:43:03,040
Yeah. And. Okay. Right. So you sort of getting ahead of me here.

275
00:43:03,430 --> 00:43:14,560
So basically you would you would regress cigaret price on the sales and cigaret specific taxes.

276
00:43:14,800 --> 00:43:18,520
Of course, your body is price related to cigaret consumption.

277
00:43:20,200 --> 00:43:26,170
So now it's multivariate research, sort of highly touted vector notation.

278
00:43:27,250 --> 00:43:36,340
We still get a predictor of X risks on that predict versus Y in a predictor and exactly the same hypothesis,

279
00:43:36,340 --> 00:43:39,390
test and confidence intervals used before go in the same way.

280
00:43:39,700 --> 00:43:45,310
C is just, you know, not just intercepted a scalar but now an intercept into.

281
00:43:47,500 --> 00:44:01,150
To Cuba. It's very important. So you can get better estimates, more stable estimates.

282
00:44:01,780 --> 00:44:37,540
They don't want anyone to speculate as to why these things. What do you think?

283
00:44:37,540 --> 00:44:45,340
I'm doing better if I use both rather than just, say, overall sales tax or or even just the cigaret specific tax.

284
00:44:47,980 --> 00:45:09,459
Really, what I'm trying to do here is predict. Well, they're actually not confounders.

285
00:45:09,460 --> 00:45:12,940
It's the whole goal. But you're sort of starting down the right track.

286
00:45:13,270 --> 00:45:18,880
You have more. So what's that going to help with is that you have a lower variance estimate.

287
00:45:19,180 --> 00:45:28,270
Right, right, right. Suppose we had these suppose we had a Z here that was essentially know enough Z that we've actually perfectly predicted X.

288
00:45:29,170 --> 00:45:34,360
Right. I mean, it's never been a perfect prediction, but this is shrink this variance.

289
00:45:35,190 --> 00:45:38,559
You're going to get a less and less variability, sort of extra variability.

290
00:45:38,560 --> 00:45:46,210
So X I had. And so the better one becomes becomes the estimated better.

291
00:45:48,050 --> 00:45:53,390
Okay. So that's the good thing about it. The bad thing about it is you don't need all the instruments to meet the conditions.

292
00:45:55,010 --> 00:46:00,080
So in particular the they all have to kind of be randomized.

293
00:46:01,310 --> 00:46:05,780
So you have something that isn't really an instrument in the sense of being a priori associated.

294
00:46:06,410 --> 00:46:16,190
You want to think about it associated with counterfactual outcomes, then then you're introducing problems, potentially bias.

295
00:46:18,290 --> 00:46:21,830
No, they don't have to be nonzero. Only one of them has to be non-zero.

296
00:46:22,790 --> 00:46:26,060
But putting in a bunch of things that are essentially noise isn't going to help you much.

297
00:46:26,840 --> 00:46:27,350
So.

298
00:46:27,490 --> 00:46:34,280
So one needs to condition generally when you have something that's got at least some, you know, weak association with X conditional on the other axis.

299
00:46:38,960 --> 00:46:44,280
Okay. So you can extend this, right?

300
00:46:44,360 --> 00:46:51,400
You can include. Of multiple endogenous variables.

301
00:46:51,430 --> 00:47:02,500
Right. So you could use. Right. Maybe you want to think about how, you know, cigaret consumption is based on price but on,

302
00:47:07,070 --> 00:47:10,960
you know, some other factor that it might be associated with with it, with that.

303
00:47:14,850 --> 00:47:17,920
Course now now you've got to have these.

304
00:47:21,750 --> 00:47:28,110
Basically you sort of retain the need to have these fees be independent of this residual,

305
00:47:28,110 --> 00:47:35,490
even if put in multiplexes and you get one of them to work with each of these endogenous variables.

306
00:47:35,520 --> 00:47:41,069
Right. So basically there's got to be one of these these is going to have to be associated with one of the axes.

307
00:47:41,070 --> 00:47:42,810
They have to be associated with all of them.

308
00:47:43,620 --> 00:47:52,980
And they don't not all of these have to be associated with all the axes, but there has to be at least some pair here that you can find.

309
00:47:56,140 --> 00:48:14,350
And you also can include exogenous variables. So these are things that are or not not confounders in the sense of being having some some primary.

310
00:48:16,000 --> 00:48:21,070
Well, they may potentially be confounders, so. But they're not the confounders of interest.

311
00:48:21,460 --> 00:48:23,470
Right. They're not the confounded variable of interest.

312
00:48:23,980 --> 00:48:28,390
Or you may they may just simply they may not be confounders, but you just want to have them in there for variance reduction,

313
00:48:28,750 --> 00:48:34,510
kind of shrink these these epsilon eyes down, which again, enhance the estimate of the dataset.

314
00:48:35,680 --> 00:48:40,850
And you can do that for both. Come on.

315
00:48:41,060 --> 00:48:44,990
So the thing is, now you got to put these variables in the first stage model as well.

316
00:48:45,080 --> 00:48:49,730
So you got to put these WS in the in the right.

317
00:48:54,940 --> 00:49:01,780
So. All right.

318
00:49:03,130 --> 00:49:16,510
Mr. All right.

319
00:49:16,520 --> 00:49:30,220
So in order for this two stage model to be identified, you've got to have at least as many C's as you have Xs.

320
00:49:31,690 --> 00:49:36,160
So you're going to have at least as many instruments as you have endogenous variables.

321
00:49:40,180 --> 00:49:43,940
We'll will speak. So they're just equal to each other.

322
00:49:43,970 --> 00:49:47,990
Right. The simple example we talked about, we had one of each, right. So he would put.

323
00:49:47,990 --> 00:49:56,150
Q were both equal to each other in equal one. So in this case, you can't you can't really test whether these are valid instruments.

324
00:49:57,410 --> 00:50:02,140
In particular, it's this issue of of potential sort of residual indoctrinating.

325
00:50:02,150 --> 00:50:12,440
Right. There's some we can't test if if these these season and residuals are zero.

326
00:50:14,000 --> 00:50:20,390
So now it turns out if we have more instruments than we have.

327
00:50:27,580 --> 00:50:30,640
Endogenous predictors, then we can do a test.

328
00:50:31,780 --> 00:50:42,090
We can essentially test whether we want to fail to reject the hypothesis that this covariance is zero for all of them or the alternative.

329
00:50:42,160 --> 00:50:49,570
B There's at least one of these of these of these instruments that isn't really an instrument.

330
00:50:51,160 --> 00:51:00,510
Can't tell which one necessarily, depending on whether she was at least one step larger, but to a second.

331
00:51:02,130 --> 00:51:05,580
So it's all right to have this.

332
00:51:08,580 --> 00:51:13,540
Well, the first thing is we get we get our instruments at least epsilon.

333
00:51:14,770 --> 00:51:19,580
Right. So basically we have y two stationary squares of two meter, right?

334
00:51:19,590 --> 00:51:28,080
Unbiased s two meters of the data is b one plus this one I which should be independent of these better hats.

335
00:51:29,820 --> 00:51:37,320
So we compute that first and then regress this on Z and W and use a resulting F statistic

336
00:51:38,190 --> 00:51:42,780
for the global hypothesis that all coefficients for the except for the intercept or zero.

337
00:51:44,160 --> 00:51:47,180
Right. So in order for this to not be associated, right.

338
00:51:47,190 --> 00:51:55,169
We need these, these the, the regression of Z on these guys to be zero, right?

339
00:51:55,170 --> 00:51:56,400
That's our assumption.

340
00:51:57,750 --> 00:52:10,640
So, um, so we take that F statistic, multiply it by the number of instruments and then other than our hypothesis, these instruments are endogenous.

341
00:52:11,520 --> 00:52:16,470
This, this JS artistic has this chi square distribution two minus degrees period.

342
00:52:19,070 --> 00:52:23,510
So so clearly people's views are going to work.

343
00:52:24,500 --> 00:52:29,900
So we've got to have Cuba at least, you know, 1.1 bigger than Pete.

344
00:52:32,840 --> 00:52:37,310
And secondly, if you reject the null hypothesis, it doesn't really tell you which one of the instruments is invalid.

345
00:52:39,170 --> 00:52:44,720
So for that, you sort of just have to rely on science. If you just have one extra instrument.

346
00:52:45,740 --> 00:52:49,490
If it's if you have three.

347
00:52:49,670 --> 00:52:52,550
Well, if you search, if you have two or more extra instruments,

348
00:52:53,090 --> 00:52:59,149
you can essentially drop one at a time and then re compute this no hypothesis and look to see.

349
00:52:59,150 --> 00:53:04,940
Well, is there one that we've we've dropped out this one and certainly that hypothesis looks much more reasonable, right?

350
00:53:05,180 --> 00:53:08,870
So maybe that's the that's the problem.

351
00:53:10,310 --> 00:53:14,120
So so that's basically the concept there.

352
00:53:17,990 --> 00:53:26,930
All right. So we'll look at example here in a second. Yeah.

353
00:53:27,230 --> 00:53:44,370
Go back to slide 23. Sure. A question about the sort of equations for like the second line x supply is equal to alpha, etc. does it include the x?

354
00:53:44,970 --> 00:53:49,040
Q i. I think that should be considered.

355
00:53:49,320 --> 00:53:58,160
Okay. Sorry. These were done by pad from sticks.

356
00:54:03,840 --> 00:54:07,440
Sorry. They should be secure from the Q.

357
00:54:09,240 --> 00:54:15,230
Q releases the exes. Yeah.

358
00:54:16,670 --> 00:54:20,120
Sorry. It's nothing down there to see. Like, copy dead in there.

359
00:54:22,850 --> 00:54:27,220
Okay. Sorry. Right.

360
00:54:27,850 --> 00:54:33,700
QUESTION What type of. All right.

361
00:54:33,700 --> 00:54:38,920
I think I'm actually going to move slightly out of order here because I want to I want to show an example.

362
00:54:40,120 --> 00:54:45,550
I'm realizing now that this is going on too long. And the point I want to get at is.

363
00:54:47,860 --> 00:54:57,290
Yeah. So. So we're going to see.

364
00:54:57,330 --> 00:55:01,700
Well, I'll just say this in order to. To get this.

365
00:55:02,360 --> 00:55:12,200
We're going to see that if we have a single binary instrument, a single binary intermediate variable,

366
00:55:13,040 --> 00:55:17,749
and we have our assumptions of super randomization, this exclusion restriction,

367
00:55:17,750 --> 00:55:25,310
the idea that all the treatment goes through the taking, the treatment,

368
00:55:25,490 --> 00:55:32,719
there's no sort of effective randomization assignment independent of treatment, the amount of tenacity sort of notifier,

369
00:55:32,720 --> 00:55:37,850
the idea that this this could apply our average court effect that that I intend

370
00:55:37,850 --> 00:55:41,780
to treat effect in compliance is actually going to equal instrumental variant.

371
00:55:43,670 --> 00:55:51,670
So all. I'll show that a second.

372
00:55:52,690 --> 00:56:02,890
Possibly next week. But I wanted to go through an example with with physician encouragement study.

373
00:56:05,510 --> 00:56:16,030
So. So here we're actually treating the randomization as an instrument clearly meets the first assumption.

374
00:56:16,840 --> 00:56:19,750
That means the second assumption is not perfect. Right.

375
00:56:20,140 --> 00:56:31,660
But there was a tendency for the for the physicians who were getting that extra oversight, if you will, to to change the treatment.

376
00:56:33,220 --> 00:56:40,760
So if we look at the effect of receiving the best practices here, we then we get our Tues,

377
00:56:40,780 --> 00:56:49,660
the score is estimated or if we have this covariance between the depression score and the.

378
00:56:54,380 --> 00:56:58,550
Randomization. It's not very large.

379
00:56:59,720 --> 00:57:01,250
It's about a 10th of a point.

380
00:57:02,060 --> 00:57:11,240
But then if you look at the covariance between the treatment assignment and the actual treatment given, right, that's not all that large either.

381
00:57:11,270 --> 00:57:14,970
So essentially, we sort of boost up this beta one known to be around nine.

382
00:57:15,380 --> 00:57:19,400
And you can see it's actually gonna be pretty close to what I just said.

383
00:57:19,880 --> 00:57:26,180
I'll show why that is. Peter, sorry. Could you explain again how randomization.

384
00:57:26,180 --> 00:57:42,930
That's sort of our condition? Sure. So I did it in words.

385
00:57:50,870 --> 00:57:58,100
So now here are basically the ideas are related to the treatment or exposure of interest, but not related a priori to the outcome.

386
00:57:59,420 --> 00:58:07,820
So the randomization right caused people to change their being, at least some individuals to get treatment changed.

387
00:58:08,190 --> 00:58:18,650
There was a higher rate of of the sort of standard of care in the most of patients that were the doctors getting the extra calls.

388
00:58:19,280 --> 00:58:21,230
Then it was the place of a doctor's work.

389
00:58:21,650 --> 00:58:26,990
So it is related, not perfect, because there's this noncompliance issue, but there is at least an association.

390
00:58:27,920 --> 00:58:32,360
The second part is actually very easy, right, because there's no way it can be related to the upcoming PRIORE.

391
00:58:32,900 --> 00:58:37,370
It's random variable, right? Because it's conditional on the attribute that was actually taken.

392
00:58:38,080 --> 00:58:43,190
Now, this is the dramatization itself, right? Dramatization doesn't condition on anything.

393
00:58:43,190 --> 00:58:50,390
It's just this random variable that sits out there that hopefully determines how your treatment will change.

394
00:58:50,630 --> 00:58:54,360
Right. But it's like unrelated to the outcome. Right.

395
00:58:54,380 --> 00:58:58,490
We made it mean because it was randomized. You couldn't be related to the outcome.

396
00:58:58,700 --> 00:59:02,690
Right. But you got assigned to zero or one had nothing to do with how your treatment,

397
00:59:03,290 --> 00:59:06,290
how your depression was going to be if you were treated versus you weren't treated.

398
00:59:08,380 --> 00:59:14,830
Is its definition fraternization so that it is it is in some sense an idea considered.

399
00:59:19,790 --> 00:59:24,350
We can use the fact this remote variable idea to get at this idea that.

400
00:59:40,880 --> 00:59:46,490
Right. And so that essentially if the periphery correlated and this part down here is one

401
00:59:47,330 --> 00:59:50,730
and the correlation between Y and Z with the same year correlation between X.

402
00:59:51,500 --> 00:59:57,320
So I mean, if there if there had been no noncompliance, then we could just look at our sort of difference in means.

403
00:59:57,880 --> 01:00:02,540
And that would be 80 for the players because everybody is complaining.

404
01:00:04,320 --> 01:00:11,070
So one way to think about it. So essentially what's what's going in Tennessee a little bit with this covariance,

405
01:00:12,390 --> 01:00:23,010
alternative expression that there's sort of weaker association between the outcome and the

406
01:00:23,340 --> 01:00:29,610
treatment assignment is essentially saying you have potentially a lot of noncompliance.

407
01:00:30,810 --> 01:00:35,220
And if there's a lot if it's due to a lot of noncompliance, then this is going to be small.

408
01:00:35,460 --> 01:00:38,970
So you kind of boost it up. Sort of a bias adjustment. Right.

409
01:00:38,970 --> 01:00:46,230
We saw that that beta one times is basically equal this ratio derivation.

410
01:00:46,950 --> 01:00:48,450
So that's another way to think about, you know,

411
01:00:52,680 --> 01:01:05,940
the sort of relatively weak instruments will means that you do the adjustment you're generally going to get an increase away from zero.

412
01:01:06,120 --> 01:01:09,870
It could be this negative. It's negative, right?

413
01:01:09,870 --> 01:01:15,000
Then we're going to kind of move toward a stronger effect because essentially and again,

414
01:01:16,350 --> 01:01:21,600
this is sort of hidden assumption that we're going to make explicit in a bit of.

415
01:01:25,360 --> 01:01:30,880
Exclusion restriction means essentially there's just a small number of folks in there that are really complying.

416
01:01:31,540 --> 01:01:34,780
And so that's getting drowned out by these folks that are having no effect.

417
01:01:35,440 --> 01:01:39,960
So what you want to do is you want to divide by the small value here to kind of, you know,

418
01:01:40,000 --> 01:01:43,330
throw away all those folks that are having no effect and focus on that little group.

419
01:01:44,640 --> 01:01:49,060
So sort of conceptually, that's what's going on. Okay.

420
01:01:49,930 --> 01:01:58,090
So now there is art or packages for this and this ID model fits the traditional instrumental variable model.

421
01:02:00,010 --> 01:02:11,530
So I think that in order to match the paper on this, we're going to match the terminology and idea model.

422
01:02:13,150 --> 01:02:20,530
There's a little bit switching around, so why we means the outcome. I'm going to use D, which is equivalent to S as a sort of intermediate variable.

423
01:02:21,850 --> 01:02:24,010
Right? This is what we're we're sort of on.

424
01:02:24,880 --> 01:02:31,830
We were actually able to observe with the tree that people take and then the the instrument with what they're assigned to.

425
01:02:31,840 --> 01:02:35,320
So you see instead of a. So.

426
01:02:36,070 --> 01:02:41,920
All right. So basically it's the same something we had before.

427
01:02:43,090 --> 01:02:50,590
Right. We would like to know what beta one is here for treatment taken.

428
01:02:51,880 --> 01:03:02,560
But with after getting rid of the this sort of correlation induced potentially by the fact that the people would take the treatment,

429
01:03:03,010 --> 01:03:12,740
their improved outcomes might differ. So I guess there's a couple different functions that fit this model.

430
01:03:16,400 --> 01:03:22,309
So at model you just called, why is the outcome very variable?

431
01:03:22,310 --> 01:03:29,530
They use the the the instrument they call Z and I don't have any other predictors I'm going to put in here.

432
01:03:29,540 --> 01:03:33,380
So I just are the, the WS, I guess in my, my previous slide.

433
01:03:34,670 --> 01:03:44,870
So, okay, so I run that I get my regression and it is even though these are binary, it's, it's going to use linear regression here.

434
01:03:45,710 --> 01:03:53,180
So I'm clear there is an association between C and D, right?

435
01:03:53,180 --> 01:03:57,900
So no, no, no.

436
01:03:58,100 --> 01:04:02,239
Or it's where it's a little weird to think about in the zero one outcomes.

437
01:04:02,240 --> 01:04:05,660
But, but, you know, it's not huge. It's not trivial either.

438
01:04:06,800 --> 01:04:11,060
So, so I'm going to highlight the two things we've, we've talked about.

439
01:04:11,060 --> 01:04:28,190
So ordinarily squares is just the s treated thing that's just regressing y on one D So the two stage li squares, right, is what we just went through.

440
01:04:29,990 --> 01:04:39,049
So these for an element are slightly different sort of fancier versions of internal variables.

441
01:04:39,050 --> 01:04:47,690
Although in this simple case with the with a single predictor or a single instrument, an element in two thirds of the squares correspond.

442
01:04:50,130 --> 01:05:06,080
So. All right.

443
01:05:06,100 --> 01:05:09,550
I think I'm not going to get into that, but.

444
01:05:12,260 --> 01:05:17,150
Okay. So you can see, right, the same estimate that we got.

445
01:05:17,960 --> 01:05:24,280
We just looked at that we now have. This this grant system.

446
01:05:25,560 --> 01:05:40,270
So. All right.

447
01:05:54,980 --> 01:05:59,690
Oh, I see. So the other option is, if you want to have more than one, you are.

448
01:06:00,980 --> 01:06:03,799
I guess it doesn't really matter. It's just it's just how you reformulate it.

449
01:06:03,800 --> 01:06:12,560
To reformulate this as a a model where you do your first stage model here and then you indicate what the instruments are here.

450
01:06:12,560 --> 01:06:19,160
So implicitly, this is this regression of Z on of the on Z and replace you D here.

451
01:06:19,910 --> 01:06:23,570
So I don't know if this gives you the same get the same results we saw before.

452
01:06:25,610 --> 01:06:31,790
So to get the robust standard error, then you need to use IV model.

453
01:06:37,730 --> 01:06:41,210
Call this call this an I.D. model. Function.

454
01:06:42,110 --> 01:06:45,860
So hetero standard error is true.

455
01:06:46,910 --> 01:06:50,060
And then if you had cluster data, you could put that in here and here we don't.

456
01:06:50,120 --> 01:06:54,740
So I'm just an observation in its own cluster.

457
01:06:54,890 --> 01:07:18,740
So it just goes in and directly. So it's not a big difference in terms of the standard error.

458
01:07:23,490 --> 01:07:28,080
Slightly smaller. The point is, it doesn't change all.

459
01:07:35,140 --> 01:07:39,910
So the estimate of the effect of the best practices is somewhat different in two states.

460
01:07:39,910 --> 01:07:44,319
These squares are fully based on our approach from our lessons, I should say.

461
01:07:44,320 --> 01:07:47,410
Class Note six, right? Yes.

462
01:08:02,580 --> 01:08:08,140
So. Our two stage lead squares.

463
01:08:12,610 --> 01:08:17,080
9.920 .892.07.

464
01:08:18,190 --> 01:08:27,390
So will. Go back now and show why this is a probe into this beating model based approach.

465
01:08:28,050 --> 01:08:33,450
Not identical, but basically meets a similar set of assumptions.

466
01:08:35,190 --> 01:08:43,210
So. See the link here.

467
01:08:48,470 --> 01:08:56,140
All right. Well, we can wrap this up. Too much of the time.

468
01:09:08,790 --> 01:09:14,980
Okay. So. Okay.

469
01:09:15,390 --> 01:09:24,100
As I mentioned before, under these assumptions and with a binary instrument, the binary intermediate variable, the two stage least squares,

470
01:09:24,100 --> 01:09:32,010
this computer will be equivalent to this ATC efficient compilers so people should always is

471
01:09:34,650 --> 01:09:41,760
right my to say the least squares estimate her can be written as these ratio of currencies.

472
01:09:43,230 --> 01:09:55,110
So if Z is binary then basically I can rewrite this covariance of y and z, right?

473
01:09:55,110 --> 01:09:59,610
Just as when when za0.

474
01:09:59,610 --> 01:10:01,110
Right, this is going to be zero.

475
01:10:02,010 --> 01:10:08,670
So the expected value of the product is basically going to be whatever the expected value of Y is when Z is one times the probability that Z is one.

476
01:10:09,210 --> 01:10:15,660
Right. So for this product right,

477
01:10:15,660 --> 01:10:21,959
I can rewrite the expected value of Y as these conditional expectations of Y with Z is one

478
01:10:21,960 --> 01:10:28,110
or zero zero times the probability that it's one or zero and expected value of z indicator.

479
01:10:28,110 --> 01:10:34,889
Right? So it's just a probability of C one. So all right.

480
01:10:34,890 --> 01:10:39,670
So the little bit of factorization here I can factor out of.

481
01:10:41,360 --> 01:10:44,540
Hey, you can sort of do it different ways.

482
01:10:44,540 --> 01:10:56,840
I guess if I if I multiply this PFC times, this PFC, I get PFC squared times correctly,

483
01:10:56,870 --> 01:11:00,620
awakenings equal to one and then a PFC times it goes equal to one.

484
01:11:00,620 --> 01:11:10,820
So I factor that out, gives me this piece here and then I'm left with this PFC Times one minus Zito's expected value of white zero.

485
01:11:12,800 --> 01:11:18,020
So, but of course this is just PFC times one minus Z.

486
01:11:18,020 --> 01:11:23,870
So I can, I can, I can factor these out over here and it gives me this.

487
01:11:25,970 --> 01:11:32,960
So I can play the same game, of course, replacing X with Y and get the same result here.

488
01:11:39,170 --> 01:11:49,910
So essentially then this these positions of most of these cancel and so the expected value of B to have then it's just going to be

489
01:11:50,120 --> 01:12:03,550
a difference in the expectations of why between the observed under zero and the observed and one and the expectations of xt001.

490
01:12:03,950 --> 01:12:13,190
So kind of converted this. Well, I'm still still not in its counterfactual here, right through all the discriminatory board.

491
01:12:14,000 --> 01:12:18,500
So to convert back to the instrumental variable world. I'm going to make some assumptions.

492
01:12:20,270 --> 01:12:31,040
So the first one is this. The observed expectation of Y under when Z has some treatment value is equal to its potential outcome.

493
01:12:32,510 --> 01:12:35,550
Does anyone remember what we call that assumption that prior.

494
01:12:38,430 --> 01:12:43,500
We're near the end, but we must push something stable through the.

495
01:12:43,840 --> 01:12:46,290
Yeah. Suit for the stable treatment. All right.

496
01:12:46,290 --> 01:12:52,799
That's basically what you're seeing on the observed data corresponds to the potential outcome under that treatment.

497
01:12:52,800 --> 01:12:59,070
Sorry. So this one.

498
01:12:59,080 --> 01:13:03,480
So the compliance classes are either compliance never takers or always takers.

499
01:13:04,770 --> 01:13:12,590
So what is that assumption? No, no defiance, rights and unconditionally.

500
01:13:13,760 --> 01:13:23,239
And finally that these expectations the potential outcomes if you're never take her and always take her, are going to be going to be equal.

501
01:13:23,240 --> 01:13:32,950
Whether you're saying the treatment or control you called that, what I knew was this was very personal.

502
01:13:35,390 --> 01:13:45,590
I'm sorry. I'm confounded this I want you to know that was that was when I was jumping around exclusion restriction variant.

503
01:13:45,950 --> 01:13:49,720
So the idea is that it's not you're excluding it.

504
01:13:49,760 --> 01:14:01,219
So just a little bit on I guess the terminology, you're excluding the possibility that there's any effect on the outcome through anything other

505
01:14:01,220 --> 01:14:05,060
than through through the treatment of same itself for anything other than the treatment taken.

506
01:14:06,170 --> 01:14:09,800
So you're sort of saying it cannot be so these two expectations have to be equal.

507
01:14:11,520 --> 01:14:25,979
We never take her and always take her. So with that in hand then right, we can basically the Z is equal to one, right?

508
01:14:25,980 --> 01:14:30,030
We know there are compliance. We're weren't always take.

509
01:14:30,720 --> 01:14:36,540
Well guess just generically because we know that that she falls into one of these three categories we can rewrite.

510
01:14:38,740 --> 01:14:45,390
The expected value of Y as the probability of being one of these categories times the expected value of the potential outcome.

511
01:14:45,400 --> 01:14:55,360
Right. So this is this is sort of combining the monitors his mission with this with software and then the same thing down here

512
01:14:55,360 --> 01:15:02,230
which we can then immediately replace y0 with y one and they always take or never take group with an exclusion restriction.

513
01:15:04,870 --> 01:15:17,830
So furthermore, right x is a is a is a binary variable.

514
01:15:18,520 --> 01:15:23,470
So under a suit four we get x10 of one is x one.

515
01:15:24,010 --> 01:15:28,510
We then take it to probability x, y and z with one is the expected value of x one.

516
01:15:29,470 --> 01:15:43,270
So given that you're in treatment or or given that you were assigned a treatment, that means you're going to be compliant and you took it.

517
01:15:44,320 --> 01:15:48,900
Right? So this is the. This is the D or the extreme.

518
01:15:49,680 --> 01:16:04,620
Then you're going to either be a compliant wage taker. If you were assigned to control and took it then.

519
01:16:06,940 --> 01:16:14,890
I'm sorry. So the control value, if control value is one when you're assigned to the treatment.

520
01:16:15,970 --> 01:16:22,550
So your. Spring.

521
01:16:24,530 --> 01:16:38,059
They should be as equals. Zero. Sorry. Right.

522
01:16:38,060 --> 01:16:43,820
So. So this is the expected value of zero grams equals zero, which the probability zeros equal one given this equals zero.

523
01:16:44,330 --> 01:16:48,470
So only way that is zero is equal to one given your assigned control, as if you're an always taker.

524
01:16:53,240 --> 01:16:58,700
So then pulling this together, we subtract this quantity from this quantity.

525
01:16:59,720 --> 01:17:02,960
So the fact that the exclusion restriction means that these terms go to zero.

526
01:17:05,030 --> 01:17:14,360
And I'm just left with the conditional difference in the potential outcomes given compliance status in terms of probability of being a compliance.

527
01:17:16,250 --> 01:17:21,260
And then for the denominator, right,

528
01:17:21,620 --> 01:17:27,290
these probabilities of all these takers disappears when I subtract the fraction of people that

529
01:17:27,290 --> 01:17:30,919
take the treatment under treatment and the fraction of people that take it under control,

530
01:17:30,920 --> 01:17:36,680
I'm just left with a proportion of compliance expense. So that cancels.

531
01:17:37,490 --> 01:17:41,930
So in the end, there's two states these squares misdemeanor of corresponds to the case.

532
01:17:46,190 --> 01:17:51,230
So. All right, you know, we're a little lower, so I will stop.

533
01:17:52,370 --> 01:17:55,790
We'll just reduce again, Andy.

534
01:17:56,810 --> 01:18:00,820
And meantime, we will see you possibly tomorrow or.

535
01:18:02,030 --> 01:18:03,140
Otherwise next week.

