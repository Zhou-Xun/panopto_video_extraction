1
00:00:05,300 --> 00:00:11,370
Okay. Morning, everyone.

2
00:00:14,150 --> 00:00:21,280
So we have some Monday is going to be full break.

3
00:00:21,290 --> 00:00:27,350
Right. So we're going to have our next meeting on Wednesday.

4
00:00:28,220 --> 00:00:31,940
So that won't be a lecture. That will be a practice session.

5
00:00:31,940 --> 00:00:35,210
Problem solving. I'll talk about homeworks and so on.

6
00:00:36,890 --> 00:00:42,080
I'll grade both of your homeworks tomorrow. So if you are anxious, you can look at it.

7
00:00:43,220 --> 00:00:46,430
Uh, before the fall break or during.

8
00:00:47,450 --> 00:01:00,030
And then we'll go all the solutions and. Anything that we might might be helpful to emphasize that will happen on Wednesday then.

9
00:01:00,510 --> 00:01:04,860
So the following Monday that would be the 24th.

10
00:01:05,400 --> 00:01:13,320
That was our tentative date for the midterm. Are there any strong objections to having it on the 24th?

11
00:01:13,980 --> 00:01:17,040
Because that would be kind of a logical point to have it. Okay.

12
00:01:18,170 --> 00:01:26,640
I'll post a couple of previous exams ahead of the practice session so you can look at them.

13
00:01:26,660 --> 00:01:32,430
So basically it's going to be. The the a bunch of.

14
00:01:34,350 --> 00:01:38,850
So most of the exam. I will just ask you to show something I have shown in the lectures.

15
00:01:40,270 --> 00:01:46,300
Uh, and then the one of the items will be a problem similar to home works.

16
00:01:47,830 --> 00:01:57,000
And like I said, we'll talk about problems. I will also make kind of an extended subject index with the.

17
00:01:57,860 --> 00:02:03,200
Pages in the. I will collate all the lectures, all my notes.

18
00:02:04,040 --> 00:02:13,969
I will go carefully through them. And wherever I see something that might be useful, I will make to put it in a subject index.

19
00:02:13,970 --> 00:02:24,050
Right? So it's not going to be a usual one. That's going to be a long one with possibly every single important thing mentioned with the page number.

20
00:02:25,070 --> 00:02:29,620
So that will help you prepare. Yeah.

21
00:02:29,940 --> 00:02:33,430
Yeah. Uh, so we'll have to sign a cheat sheet.

22
00:02:33,530 --> 00:02:37,920
Uh, yeah. Yeah.

23
00:02:40,850 --> 00:02:49,550
So I will be extra careful with things that they are that I haven't talked about during practice or whole works.

24
00:02:59,040 --> 00:03:03,400
Okay. So regarding the.

25
00:03:07,990 --> 00:03:14,590
The previous lecture. So I had a small glitch there.

26
00:03:18,720 --> 00:03:25,740
When I was writing the formula of total probability, I'd remember where exactly this was.

27
00:03:32,180 --> 00:03:35,330
I had a double expectation there instead of.

28
00:04:00,840 --> 00:04:14,020
There it is. Right. So I have. I have an expectation here instead of a probability, because there's clearly an event inside.

29
00:04:15,010 --> 00:04:22,720
So that should be a probability. Okay.

30
00:04:22,730 --> 00:04:29,270
So we can. Use the opportunity to look at the previous lecture.

31
00:04:30,510 --> 00:04:45,480
So what did we have there? So we proved the formula of total expectation, a very useful instrument.

32
00:04:46,800 --> 00:04:54,959
Um. Also mentioned that, uh. The formula of total probability is going to be a particular case of it because

33
00:04:54,960 --> 00:05:00,510
we define conditional probability as expectation on the indicator function.

34
00:05:00,530 --> 00:05:09,059
So if if we have time, I'll, I'll go over this intuition in more details.

35
00:05:09,060 --> 00:05:14,580
But the kind of. I suggest we start using it right away.

36
00:05:17,090 --> 00:05:22,970
Because it's almost never dealt them. So there was one problem where.

37
00:05:24,690 --> 00:05:30,669
That I think illustrates one of the uses of the formula of total expectation

38
00:05:30,670 --> 00:05:36,720
that's typically used when you have a system of random variables and you need to.

39
00:05:37,680 --> 00:05:40,860
Find out something about the behavior of that system.

40
00:05:40,860 --> 00:05:46,590
So then you're conditioned on some of the variables and that makes your life easier.

41
00:05:47,560 --> 00:05:50,370
And so I used an example of killer models.

42
00:05:50,730 --> 00:06:03,030
So if we have time later on, I will probably do another example on the use of formula of total expectation and random processes.

43
00:06:03,900 --> 00:06:05,580
This is another place where.

44
00:06:08,260 --> 00:06:18,220
Of formulas of total expectation and the understanding of a conditional expectation as a random variable is a kind of very pronounced rate,

45
00:06:18,670 --> 00:06:25,510
because in the processes you have a trajectory of the process and oftentimes you summarize the properties

46
00:06:25,510 --> 00:06:32,470
by taking expectation of something in the future while you conditioned on everything in the past.

47
00:06:33,670 --> 00:06:37,840
So that has a special name for processes that's called a filtration.

48
00:06:38,590 --> 00:06:52,420
And so some of the it's used in statistics also not just the probability theory because when you have non parametric or segment parametric estimation,

49
00:06:52,990 --> 00:06:59,320
you have your CDF or some other parameter that is essentially a process.

50
00:07:00,110 --> 00:07:05,540
Yeah. In its argument. Right. It's a function. So you have a functional problem.

51
00:07:05,540 --> 00:07:08,720
And so that's where these properties are.

52
00:07:09,200 --> 00:07:14,450
So survival analysis is one example where things like that are defined.

53
00:07:15,870 --> 00:07:21,359
Through property is a conditional expectation and you may have heard the word martingale.

54
00:07:21,360 --> 00:07:31,470
So this is a special kind of noise processes that are defined using conditional expectation, so they model fair games.

55
00:07:32,070 --> 00:07:38,430
So if you are playing a game, so you have wins and losses over every loop of the game.

56
00:07:38,430 --> 00:07:44,550
So it's a discrete, but there are also continuous processes like that all over the place.

57
00:07:45,060 --> 00:07:49,380
And the noise process is that wherever you are in the game.

58
00:07:50,790 --> 00:08:02,520
So if it's fair, then the next cycle of the game has an expectation of zero for your win or loss conditional on the past.

59
00:08:03,090 --> 00:08:10,500
Right. And that past is a random variable. We are talking about properties of the process.

60
00:08:12,360 --> 00:08:25,060
Okay. Uh, I did not finish it, so it's tempting for me to actually use this as a homework problem or,

61
00:08:25,750 --> 00:08:29,560
uh, kind of return to it later on, perhaps modifying this.

62
00:08:30,970 --> 00:08:36,100
Scenario a little bit. And make it more general.

63
00:08:36,880 --> 00:08:39,790
Then we talked about the kind of.

64
00:08:41,410 --> 00:08:52,360
Intuitive self evident almost properties of the conditional expectations linearity behavior with respect to constants.

65
00:08:54,020 --> 00:09:01,100
So, uh, it was, uh, so we had to spend a little effort, uh, pretending that, uh.

66
00:09:02,010 --> 00:09:05,589
Uh. Uh, so. Not.

67
00:09:05,590 --> 00:09:12,309
Not going according to intuition, but rather using the definition of conditional expectations.

68
00:09:12,310 --> 00:09:19,730
That's the quality of two integrals and the measurability thing to show all of these.

69
00:09:20,080 --> 00:09:26,380
Uh, but, uh, it's kind of, uh, very natural properties for it.

70
00:09:27,010 --> 00:09:32,080
Uh, now we also proved that it preserves ordering.

71
00:09:32,290 --> 00:09:35,620
Uh, so another pretty natural processes.

72
00:09:36,840 --> 00:09:43,890
Um, and then, uh, so we formulated, but, uh, did not have time to actually prove it.

73
00:09:44,280 --> 00:09:48,150
The Monotone Convergence Theorem. So, and this is where.

74
00:09:49,520 --> 00:09:58,730
I will start. Today. Take it into the next lecture.

75
00:10:10,690 --> 00:10:26,170
Okay. So before I do that, let me just say just one more fact that we have improved, but it also seems self-evident to you.

76
00:10:27,310 --> 00:10:36,700
So you probably wouldn't objective. I just mentioned it in the proof, but I will state that there is a fact nevertheless that the limits.

77
00:10:40,710 --> 00:10:52,130
Of the back into girls. Unique.

78
00:10:59,650 --> 00:11:08,709
And limits of the client. So if you have an integral A X, and so that's a sequence of the variables.

79
00:11:08,710 --> 00:11:14,830
DP So these kinds of limits is and goes to infinity.

80
00:11:20,030 --> 00:11:26,510
Now, normally you wouldn't question that. But just for the sake of purity, I will state it.

81
00:11:27,750 --> 00:11:35,020
The proof is not all too difficult, but. Uh, we want to just save some time.

82
00:11:38,170 --> 00:11:56,880
Okay. So. So I have a non-negative sequence of random variables, uh, converging almost surely to a random variable x uh,

83
00:11:56,890 --> 00:12:02,500
also non-negative, uh, converging from below, uh, in a monotonic way.

84
00:12:04,680 --> 00:12:14,610
So that just means that the accent of Omega converges from below to acts of Omega in the calculus sense for all omegas goes,

85
00:12:14,610 --> 00:12:20,219
except perhaps a set of measure zero. However, that measure is defined.

86
00:12:20,220 --> 00:12:23,790
So there is some P behind the almost true statement.

87
00:12:24,840 --> 00:12:31,350
So then we expect the same thing to happen for conditional expectations.

88
00:12:31,560 --> 00:12:45,390
So the one of examiner will converge. Almost surely from below to the expectation of the limiting random variable x.

89
00:12:46,550 --> 00:12:51,770
Okay. So first we exploit the previous theorem.

90
00:12:51,770 --> 00:12:59,600
So we showed that expectation preserves ordering conditional expectation.

91
00:13:05,560 --> 00:13:08,800
So this means that, uh, uh.

92
00:13:08,990 --> 00:13:24,520
My next value. Of the conditional expectation for the next day, for the next member of my sequence is going to be larger than the previous one.

93
00:13:25,330 --> 00:13:30,340
Well, not smaller. So I'm using non strict inequality here.

94
00:13:33,510 --> 00:13:38,020
Right. And because it's. Converging from below.

95
00:13:38,260 --> 00:13:44,740
Uh, by the same. Or I will have these being less equal.

96
00:13:46,430 --> 00:13:55,740
Uh, conditional expectation of X. Now by the Bounded Convergence Theorem.

97
00:14:10,410 --> 00:14:15,690
Uh. So I have, uh. The expected value.

98
00:14:18,050 --> 00:14:25,310
And leaving you on the virgin to some random variable Z, let's say.

99
00:14:39,110 --> 00:14:52,690
And this this is the one from calculus, right? So if you have something, uh, a monotonic sequence that's bounded from above.

100
00:14:53,470 --> 00:14:57,790
So convergence to some limits. So let this limit be a Z.

101
00:14:58,370 --> 00:15:06,790
Now, I also have that X and converges to X, so by the condition of the theorem.

102
00:15:08,050 --> 00:15:11,110
So it's all. Well, everything is almost surely.

103
00:15:12,040 --> 00:15:15,940
Now let's take any event.

104
00:15:19,250 --> 00:15:23,360
There belongs to the sigma algebra that we're conditioning on.

105
00:15:24,830 --> 00:15:35,090
And so then I will have that expectation of y and so they'll be back in the groove right this time.

106
00:15:37,230 --> 00:15:43,860
Why am. So I need to say what I am is.

107
00:15:45,250 --> 00:15:55,870
So why and there's going to be conditional expectations of as usual, uh, x and given uh, the sigma algebra you.

108
00:15:57,190 --> 00:16:13,720
So that's going to converge. It's to the expected value to the integral z, although the set a on the one hand and on the other hand.

109
00:16:17,190 --> 00:16:25,960
Again, there's going to be a limit. It's integral over X and over E that will converge.

110
00:16:28,160 --> 00:16:33,650
To integral of x over a.

111
00:16:35,520 --> 00:16:47,130
Right. So that's that's right. So each of these lines is a limit of a labeled integral, um, of a different one generally.

112
00:16:47,190 --> 00:16:50,460
Right. But I know that because.

113
00:16:51,990 --> 00:16:55,090
Of. Uh, this thing.

114
00:16:57,610 --> 00:17:01,690
The way it's defined by the definition of conditional expectations.

115
00:17:05,200 --> 00:17:14,500
So these two expectations are actually equal, right? By the definition of conditional expectation of something given to you.

116
00:17:14,660 --> 00:17:19,070
Right. And because they're equal, so.

117
00:17:19,490 --> 00:17:25,220
And because the the big integrals have unique limits.

118
00:17:38,150 --> 00:17:43,280
So my expected value. So my integral.

119
00:17:49,030 --> 00:17:57,170
What's making that noise? I don't know. So the right paths, the limits are going to be the same, right?

120
00:18:04,290 --> 00:18:11,409
And this is for any. From the sigma algebra.

121
00:18:11,410 --> 00:18:23,790
You. And now I'm looking at this as a definition.

122
00:18:32,420 --> 00:18:38,900
For what? A conditional expectation of X given you is.

123
00:18:39,620 --> 00:18:44,650
And I see. The Z is this.

124
00:18:46,060 --> 00:18:49,760
There's the expectation because according to the definition.

125
00:18:50,150 --> 00:18:54,379
So there are two integrals that need to be equal.

126
00:18:54,380 --> 00:19:02,570
So one of them is an integral of X of A and another one of whatever.

127
00:19:02,570 --> 00:19:08,840
Right? So Z, according to this previous line, is a conditional expectation of.

128
00:19:11,490 --> 00:19:14,670
Yes. Even you, by definition.

129
00:19:25,090 --> 00:19:32,820
And this is it, right? Because he was was the limit.

130
00:19:34,270 --> 00:19:38,470
Um, of, of, uh, conditional expectations.

131
00:19:40,670 --> 00:19:48,930
And the. So we show that that limit is the conditional expectation of the limiting variable for the sequence exit.

132
00:19:51,980 --> 00:19:57,030
Okay. Then.

133
00:19:57,070 --> 00:20:06,970
So another very useful theorem is that you measurable functions.

134
00:20:07,720 --> 00:20:17,020
I already used that, actually. In the example on the vacuum models.

135
00:20:23,670 --> 00:20:30,040
These. Like Constance.

136
00:20:33,420 --> 00:20:38,490
The. The expectation given the evil.

137
00:20:41,870 --> 00:20:47,560
No, I don't mean that they are actually constant. They just mean that they can be taken out of the expectation.

138
00:20:47,600 --> 00:21:00,770
Right. So in this sense. They can.

139
00:21:04,060 --> 00:21:11,420
The expectation. Now I need to prepare.

140
00:21:12,390 --> 00:21:15,120
For a formal statement on the theory. Right.

141
00:21:15,570 --> 00:21:27,690
So let's say you have random variable X and another random variable Y that these measurable with respect to you.

142
00:21:30,720 --> 00:21:33,800
So acts is not generally measurable with respect to you.

143
00:21:33,810 --> 00:21:45,000
It's just the way. Okay.

144
00:21:45,240 --> 00:21:49,830
Then I need to make sure that I can consider.

145
00:21:51,900 --> 00:21:55,470
Expectation of the product X in the subway.

146
00:22:01,230 --> 00:22:06,299
So exists. Right. And now that I used it.

147
00:22:06,300 --> 00:22:10,530
So let's give a definition in general.

148
00:22:10,530 --> 00:22:17,940
So we will say. That's.

149
00:22:20,210 --> 00:22:24,130
It's back to the valley or something exists.

150
00:22:27,170 --> 00:22:30,630
He's. It's.

151
00:22:31,960 --> 00:22:36,040
The idea of the absolute value of that something is less than infinite.

152
00:22:39,960 --> 00:22:49,350
And that mimics the definition of existence for integrals with the generally infinite limits in calculus.

153
00:22:51,550 --> 00:23:03,420
So I'm assuming namely that the fact that the value of the absolute value of the product is less than infinity.

154
00:23:07,750 --> 00:23:11,470
So then here comes the statement of theorem.

155
00:23:12,190 --> 00:23:23,080
If I'm taking a conditional expectation X times Y given you because Y is unmeasurable and out of the expectation.

156
00:23:32,610 --> 00:23:42,200
So he unveils the proof. So remember, at some point when we introduced simple functions,

157
00:23:42,920 --> 00:23:53,220
we said that we will add that our typical procedure to show something that involves properties of the back

158
00:23:53,270 --> 00:24:01,700
integrals would be to show that they are true for simple functions and then extend this to an arbitrary statement.

159
00:24:04,070 --> 00:24:07,580
By those plus minus operators that we introduced.

160
00:24:08,000 --> 00:24:13,910
So I'll start with the Y being an indicator function.

161
00:24:16,500 --> 00:24:19,980
Let us first. Show.

162
00:24:22,320 --> 00:24:26,160
That theorem is true.

163
00:24:28,710 --> 00:24:33,600
Or why? Being an indicator function.

164
00:24:48,290 --> 00:24:56,479
Okay. So I'll take E and B, just arbitrary events from the Sigma algebra.

165
00:24:56,480 --> 00:25:06,710
You know, this will mean that E intersection B is also in the U right by definition of the sigma algebra.

166
00:25:07,310 --> 00:25:10,880
Now I can introduce indicator functions.

167
00:25:11,600 --> 00:25:21,290
I. E. E. B. And then so we already discussed that.

168
00:25:22,370 --> 00:25:29,420
So the operation of intersection corresponds to the product of indicator function.

169
00:25:29,430 --> 00:25:36,410
So indicator section is going to be a product indicator, a times indicator beat.

170
00:25:38,270 --> 00:25:42,770
Okay. So then I am considering.

171
00:25:45,770 --> 00:25:49,390
Expected value. Um.

172
00:25:49,750 --> 00:25:53,770
So, um. Setting y. The indicator of the.

173
00:25:56,030 --> 00:26:01,250
So this is my indicator function that I want to show I can take out of the integral.

174
00:26:03,670 --> 00:26:10,720
So I am considering, uh, x times y, which is x times indicate a b.

175
00:26:12,040 --> 00:26:20,680
Uh, so and then the girls have a, uh, so this would be the thing I would consider, uh,

176
00:26:21,580 --> 00:26:27,280
when I want to consider the expectation of the project of X and Y conditional on me.

177
00:26:28,510 --> 00:26:44,229
So this is the integral I start with. Um, no, I'll, uh, I am, I can write it is expected value of X times indicates and b times indicate.

178
00:26:44,230 --> 00:26:47,800
Okay. Now, uh, over the whole area.

179
00:26:50,950 --> 00:26:59,170
Right. So they just replace the area by and then to go over the whole omega except that everything outside of A is zero.

180
00:27:01,720 --> 00:27:13,990
So then this is same as indicate the x times expectation x times indicator of E intersection b.

181
00:27:15,930 --> 00:27:19,410
Because of the. Because of this thing.

182
00:27:24,790 --> 00:27:36,070
So also eat and be a part of the you and by definition of conditional expectation.

183
00:27:37,510 --> 00:27:41,790
This is the same thing is. The.

184
00:27:43,080 --> 00:27:49,730
X. You. Over the intersection be.

185
00:28:03,850 --> 00:28:07,870
So you could consider it a formula of total expectation.

186
00:28:09,660 --> 00:28:22,800
So then expanding it further is e indicator the times when you.

187
00:28:25,140 --> 00:28:27,240
Times indicator a.

188
00:28:29,210 --> 00:28:42,580
So again, disassembling the intersection into two indicators and replacing an integral over the intersection of A and B by the integral over.

189
00:28:43,740 --> 00:28:49,530
Nicole Honegger. But then I have the products of two indicators under the integral.

190
00:28:51,400 --> 00:28:55,960
And it's mainly combining charms. That's my plea here.

191
00:28:57,400 --> 00:29:05,850
So I have expected the value of indicate to be times expected when you.

192
00:29:08,950 --> 00:29:26,410
Now, although they said a. Now I'm going to look at the starting point of my play and the endpoint of my play, and I'm looking at this equality.

193
00:29:36,830 --> 00:29:41,000
So I have the quality just restating it.

194
00:29:52,160 --> 00:29:56,990
That's, um. And that's for any e in you.

195
00:30:00,230 --> 00:30:06,830
So now I'm looking at this as a definition for a conditional expectation.

196
00:30:08,270 --> 00:30:11,750
So which one? The. This one, right.

197
00:30:14,290 --> 00:30:22,090
So a conditional expectation of this guy over you is equal.

198
00:30:25,950 --> 00:30:30,580
To. The Times.

199
00:30:31,590 --> 00:30:41,550
E x even you. The reason why this is the case, because it's this guy that's you measurable.

200
00:30:48,620 --> 00:30:51,950
All right, so I have a unmeasurable, random variable.

201
00:30:54,620 --> 00:31:04,719
Right here. That's when being integrated over any set in the sigma algebra.

202
00:31:04,720 --> 00:31:10,390
You gives me an expectation of another non-U.

203
00:31:10,390 --> 00:31:19,110
Measurable random variables. So an integral uh or of x times indicate a b over the same set.

204
00:31:19,180 --> 00:31:29,140
A So that means that the conditional expectation of X times indicate a b is is this the variable?

205
00:31:29,410 --> 00:31:36,140
This is by definition. Of conditional expectation of a U.

206
00:31:42,370 --> 00:31:46,010
Right. And, uh, so that means that, uh.

207
00:31:46,590 --> 00:31:52,510
Uh, I was able. We were able.

208
00:31:55,220 --> 00:32:02,260
To take. And indicator function.

209
00:32:02,830 --> 00:32:11,410
That was our specific way out of the conditional expectation.

210
00:32:14,190 --> 00:32:24,680
So it works for the indicator. The theorem is true, they indicate.

211
00:32:30,980 --> 00:32:34,280
Okay. Now let's extended.

212
00:32:41,800 --> 00:32:45,340
To simple. Why?

213
00:32:48,100 --> 00:33:01,660
Or to any discreet way, for that matter. So how does that work?

214
00:33:01,870 --> 00:33:10,190
So if Y is discrete. Then I can write it in the general form.

215
00:33:10,550 --> 00:33:15,650
Right. The discrete function of a discrete random variable.

216
00:33:16,190 --> 00:33:22,820
So I know that in order for Y to be a discrete random variable, there's got to be a partition.

217
00:33:24,010 --> 00:33:32,940
Of a case. So they are part of the you and the petition of Omega.

218
00:33:34,960 --> 00:33:38,140
So this is for key and.

219
00:33:39,320 --> 00:33:43,100
The election of the sense is a partition of the.

220
00:33:48,890 --> 00:33:55,340
Ahmed Uh, and then my random variable y takes values.

221
00:34:00,250 --> 00:34:04,780
Wiki on AK respectively writes.

222
00:34:11,210 --> 00:34:13,580
This is what means that y is discrete, right?

223
00:34:13,850 --> 00:34:21,260
And I can certainly write them in the general form that we have for discrete variables on the previous line.

224
00:34:23,030 --> 00:34:30,380
Now let's do the products now of X times Y given you.

225
00:34:31,380 --> 00:34:38,540
And now I'm going to use the linearity of expectation and the form for the Y.

226
00:34:39,740 --> 00:34:45,680
So that allows me to spell out this thing is.

227
00:34:47,060 --> 00:34:53,300
So I have the sum of a key expectation of the sum, the sum of the expectations.

228
00:34:54,050 --> 00:35:07,240
That's the linearity part. So I have e x times indicator a key given you and y key, right.

229
00:35:07,240 --> 00:35:10,720
So for instance went out of the expectation.

230
00:35:13,270 --> 00:35:24,159
And so what I end up with for each term in this sum is the previous statement

231
00:35:24,160 --> 00:35:32,350
that we proved so because a key parts of the you so it's a deal measurable

232
00:35:32,350 --> 00:35:46,450
functions and for the indicator ones I can already take them out so that's going to be indicator key times expected value of text given the.

233
00:35:48,980 --> 00:35:55,550
And that's what this guy is. And.

234
00:35:57,050 --> 00:36:05,390
So let's just collect the terms. So I have expected value of X giving you times.

235
00:36:06,140 --> 00:36:10,700
That's kind of a common term now too, and it doesn't depend on K.

236
00:36:12,250 --> 00:36:18,070
So it's out of the sun. Then I have some. Why?

237
00:36:22,790 --> 00:36:30,770
Key times indicator of the key indices by definition.

238
00:36:32,170 --> 00:36:36,200
Why? Right. So summarizing.

239
00:36:36,200 --> 00:36:41,240
I have. This thing equal to.

240
00:36:46,400 --> 00:36:50,390
We'll go to white suspect at the venue of.

241
00:36:51,880 --> 00:36:55,560
Excuse. Okay.

242
00:36:56,080 --> 00:37:06,250
So I was able to, uh, to show that the theorem works for any discrete random variable, including the simple ones.

243
00:37:14,680 --> 00:37:18,879
Discreet. Why, then? I'm not going to show it.

244
00:37:18,880 --> 00:37:22,240
But so we will have by.

245
00:37:23,850 --> 00:37:31,150
The arguments. Simple.

246
00:37:31,160 --> 00:37:34,520
Why ends? Virgin to. Why?

247
00:37:41,820 --> 00:37:49,830
We extend extended. Extended.

248
00:37:53,780 --> 00:38:00,130
To. Non-negative random variable y.

249
00:38:00,340 --> 00:38:06,220
That's not necessary. Discrete rates is the limit of of simple functions.

250
00:38:07,240 --> 00:38:17,910
So we showed that we can always approximate. The random variable by simple functions and non-negative ones.

251
00:38:18,340 --> 00:38:23,380
Then using. Plus minus three this.

252
00:38:27,800 --> 00:38:43,680
We can extend it to determine why. This would be.

253
00:38:45,700 --> 00:38:50,250
End of proof. He.

254
00:38:52,440 --> 00:38:59,909
So that's that's basically one of the main instruments to work with conditional expectations.

255
00:38:59,910 --> 00:39:06,719
And this is the instrument that helped us in the previous lecture simplify the derivation for the random

256
00:39:06,720 --> 00:39:19,530
the scheme when X was the number of the ideas and the variables key and we conditioned on the X.

257
00:39:24,370 --> 00:39:35,750
Okay. So now. It was actually developing tools up to this point to prove the following theorems.

258
00:39:36,140 --> 00:39:43,920
So remember we considered. The conditional expectation.

259
00:39:45,910 --> 00:39:56,070
As projection. So we had some random variable X.

260
00:39:58,050 --> 00:40:01,230
So in this case, it's F.

261
00:40:02,650 --> 00:40:06,040
Functions measurable with respect to some sigma algebra f.

262
00:40:06,550 --> 00:40:16,790
Then we had the subspace. That's just basic functions measurable with respect to the sigma algebra you.

263
00:40:17,860 --> 00:40:31,660
And then we use discrete variables to develop the intuition that conditional expectations will be, uh, characterized by two things.

264
00:40:32,320 --> 00:40:36,340
So first. Um.

265
00:40:36,920 --> 00:40:40,490
So, uh. Why is the closest?

266
00:40:44,510 --> 00:40:48,890
Element two acts on is you.

267
00:40:49,190 --> 00:41:00,650
So that was the one thing. And we also know from this geometric intuition that for any.

268
00:41:04,820 --> 00:41:08,300
Uh. That that ax. That this.

269
00:41:09,240 --> 00:41:13,280
X is a minus Y, right.

270
00:41:14,970 --> 00:41:20,550
Is going to be orthogonal to this view. In other words, that's the.

271
00:41:22,680 --> 00:41:30,090
Scalar product X minus Y and any z in the s u is going to be zero.

272
00:41:32,310 --> 00:41:39,810
But we never showed it actually in general for, uh, based on the general definition for the conditional expectation.

273
00:41:40,860 --> 00:41:48,210
Right. So the properties that we just derived, uh, that we can take measurable variables,

274
00:41:48,570 --> 00:41:59,100
you measurable variables out of the expectation and other things they actually gives a give us the opportunity to now show these, uh.

275
00:42:00,510 --> 00:42:04,290
In general for the tree, a conditional expectation.

276
00:42:05,970 --> 00:42:12,060
So let's formulate this is the sub in the allergy serum.

277
00:42:17,100 --> 00:42:22,290
So that's that's the two right there. So I'm going to move to first.

278
00:42:23,070 --> 00:42:27,960
So let's say I have X when the variable.

279
00:42:29,580 --> 00:42:33,900
I have. Why? That is you miserable.

280
00:42:40,340 --> 00:42:46,130
Meaning that y belongs to the space you owe you immeasurable functions.

281
00:42:47,480 --> 00:42:54,800
So then. The scalar product, the FETs minus Y and Z.

282
00:42:56,500 --> 00:43:02,090
He's going to be zero for any Z. That's also your measurable.

283
00:43:05,070 --> 00:43:09,660
Now remember how we define scale a product. It's simply an expectation of the product.

284
00:43:11,540 --> 00:43:23,710
Of the random variables. So let's write that down according to definition.

285
00:43:25,640 --> 00:43:32,270
So that's the expected value of X minus Y times Z.

286
00:43:33,050 --> 00:43:36,860
So that's by definition of a scalar product.

287
00:43:41,090 --> 00:43:46,979
So now I'm using linearity of the expectations.

288
00:43:46,980 --> 00:43:50,150
So that's e x times the.

289
00:43:52,250 --> 00:43:55,970
Minus Y times c.

290
00:44:00,190 --> 00:44:08,680
So then I can write each expectation in there by the formula of total expectation.

291
00:44:09,780 --> 00:44:18,800
So that's expected value. X times z given you?

292
00:44:21,120 --> 00:44:24,660
That's the first term mine was.

293
00:44:26,660 --> 00:44:35,260
The second one. I probably don't need to.

294
00:44:37,120 --> 00:44:40,690
Right that they keep it. This is why times the.

295
00:44:45,410 --> 00:44:50,240
Now this guy. So Z, is you measurable?

296
00:44:54,700 --> 00:45:01,460
Then I have. This is the time's expected value.

297
00:45:03,190 --> 00:45:08,180
You. Inside the outer expectation.

298
00:45:09,560 --> 00:45:13,490
So I'm taking a unmeasurable variable Z out of the expectation.

299
00:45:15,410 --> 00:45:18,980
And also this thing is why.

300
00:45:22,980 --> 00:45:29,070
So they, uh, collecting the things and having.

301
00:45:30,090 --> 00:45:40,150
Uh. Expected value of Z times y minus expected.

302
00:45:41,430 --> 00:45:56,650
Of the Times. Why? That's a zero. This is it.

303
00:46:05,080 --> 00:46:15,340
So we showed the second property. Uh, now using the second property, I can actually show the first one.

304
00:46:18,860 --> 00:46:23,380
Used to. We can now show up.

305
00:46:32,130 --> 00:46:37,010
It's so we'll call it the minimum distance. Yeah.

306
00:46:46,160 --> 00:46:50,120
So what do we have? So I have random variable x,

307
00:46:51,470 --> 00:47:08,360
i have random variable y that is expected value of x given some sigma algebra you I have z uh being a you measurable random variable.

308
00:47:09,200 --> 00:47:14,190
So both the y and Z. You miserable.

309
00:47:22,700 --> 00:47:27,180
So then. Why is.

310
00:47:30,030 --> 00:47:36,140
The closest. Two?

311
00:47:36,500 --> 00:47:40,290
Yes. Of ABC.

312
00:47:50,550 --> 00:48:00,850
I know it's a weird way to formulate it like that. So Z is just an arbitrary element of the space of all you measurable functions.

313
00:48:01,420 --> 00:48:04,659
And I'm looking for all of them for the closest one to X.

314
00:48:04,660 --> 00:48:09,040
So that's the Y, right? That's just what I mean by this language.

315
00:48:13,660 --> 00:48:23,950
So prove. Let's look at the square distance between X and see.

316
00:48:26,240 --> 00:48:30,530
Right. Doesn't mean what I'm trying to minimize the distance or the scope of one.

317
00:48:30,980 --> 00:48:43,760
So it's actually. It's a little bit tidier to see that the square of the one in the distance is defined as a scalar product of x minus Z with itself.

318
00:48:46,810 --> 00:48:49,930
That's just how we defined distances.

319
00:48:51,100 --> 00:48:58,360
That is expected value of X minus Z squared.

320
00:48:58,690 --> 00:49:02,140
Again, that's based on how we defined.

321
00:49:05,250 --> 00:49:12,780
Scalar products for random variable and we had the notation for something like that.

322
00:49:13,290 --> 00:49:19,500
So that's the norm of the difference between X squared.

323
00:49:23,520 --> 00:49:29,740
Okay. So. Then let's continue writing it.

324
00:49:34,340 --> 00:49:39,410
And within have right. It is expected value.

325
00:49:40,730 --> 00:49:46,280
And then so I will. Do X minus.

326
00:49:46,280 --> 00:49:51,940
Y plus. Y minus C.

327
00:49:54,130 --> 00:50:03,160
And that thing is going to be square. I just do plus minus y inside the square.

328
00:50:04,270 --> 00:50:17,770
It certainly doesn't change anything. I can, though, dissect that expectation into a quadratic form where I will have x minus.

329
00:50:18,680 --> 00:50:21,890
Well, I square. That's one thing.

330
00:50:22,940 --> 00:50:26,990
Then I will have y minus z squared.

331
00:50:31,320 --> 00:50:40,720
That's another thing. And then I would have expected value.

332
00:50:42,830 --> 00:50:49,730
X minus y times, y, y, z.

333
00:50:50,810 --> 00:50:58,150
And I have two of those rates. So now.

334
00:50:59,130 --> 00:51:03,550
Now we can see how the previous theorem works beautifully.

335
00:51:05,550 --> 00:51:11,460
So my z and Y are both u measurable function and so is the difference.

336
00:51:12,590 --> 00:51:19,640
Between Y and Z. So Y minus Z is measurable.

337
00:51:29,400 --> 00:51:37,480
Okay. Then.

338
00:51:41,320 --> 00:51:45,280
I also have the outside analogy theorem.

339
00:51:46,240 --> 00:51:54,240
That's. The scalar product of X minus Y.

340
00:51:54,870 --> 00:52:03,680
And you. Measurable functions in a measurable function that's going to be a zero.

341
00:52:10,100 --> 00:52:14,810
So that's by the previous theorem. And because of that.

342
00:52:15,740 --> 00:52:30,590
So this whole term is zero. So that's a consequence of the analogy theorem.

343
00:52:31,070 --> 00:52:37,940
So now I'm left with, well, um, x minus Y Square doesn't have the Z.

344
00:52:38,360 --> 00:52:42,890
So remember, my initial intent was to minimize that distance over Z.

345
00:52:43,460 --> 00:52:48,050
So I only have the one of the terms that depends on Z.

346
00:52:49,610 --> 00:53:02,810
That's this one. Right? And it's clear that the expectation of something squared is going to be non-negative in the smallest possible.

347
00:53:05,160 --> 00:53:18,780
I can make it, uh, when I make it zero and then take a minimum of Z in the s u and because.

348
00:53:20,650 --> 00:53:25,680
Why is also you know as you then.

349
00:53:30,470 --> 00:53:40,320
Then why? Then I can take the z equals y minimizes.

350
00:53:45,680 --> 00:53:49,780
Is the. This station.

351
00:53:52,070 --> 00:53:57,790
And minimizes it to zero rates. When they take these vehicles.

352
00:53:57,790 --> 00:54:11,200
Why? And this is what we tend to write, that it's actually the way that minimizes the distance.

353
00:54:15,220 --> 00:54:19,950
Okay. So we finally completed the, uh.

354
00:54:21,680 --> 00:54:33,220
The interpretation of conditional expectation now in general as a projection was to add those properties that we can exploit in the future.

355
00:54:35,490 --> 01:01:01,330
So let's have a break. Okay.

356
01:01:02,140 --> 01:01:16,320
Shall we continue? So let's now look at the variance.

357
01:01:21,420 --> 01:01:25,500
And specifically the additional variants.

358
01:01:28,020 --> 01:01:40,890
It is now that we define conditional expectation with respect to sigma algebra because we can write variances on expectation, um, using expectations.

359
01:01:41,340 --> 01:01:52,140
So we have variance of X is actually expected value of X squared minus the square.

360
01:01:54,170 --> 01:02:04,820
Of the expectations. So I can now define conditional variance by simply turning this formula into one for conditional expectations.

361
01:02:13,520 --> 01:02:23,650
So we will say that variance of X given you is going to be expected value.

362
01:02:25,150 --> 01:02:34,650
Of X squared, even you minus the squared expectation of x given you.

363
01:02:43,590 --> 01:02:50,480
Now that I have defined it. I can derive.

364
01:02:51,490 --> 01:02:58,270
Again, a pretty useful law, and that's the law of total variance.

365
01:03:07,590 --> 01:03:17,999
So we had a low of total expectation that simply meant that I can first condition a sigma algebra you or random variables

366
01:03:18,000 --> 01:03:26,160
that generated and then take amount expectation and that would be the expectation of the original random variable.

367
01:03:26,460 --> 01:03:29,550
Now with variance, it's it's a little bit trickier.

368
01:03:31,060 --> 01:03:41,550
So namely the variance of X. Uh, so if I'm partitioning it, so it's going to be an expectation.

369
01:03:42,800 --> 01:03:47,010
Of the variants. Of X given you?

370
01:03:48,760 --> 01:03:55,570
Right. And if I may make what we did for the formula of terrible expectation, this would be the end of it.

371
01:03:56,200 --> 01:04:01,750
But it's not the end of it. There's a second term here that's oftentimes ignored.

372
01:04:03,740 --> 01:04:10,160
So that's the variance. Of the expectations.

373
01:04:11,520 --> 01:04:18,100
It's given you. Okay.

374
01:04:18,430 --> 01:04:21,580
It's easy to remember once you see this. Right.

375
01:04:22,000 --> 01:04:25,270
Uh, expectation of the variance plus variance of the expectation.

376
01:04:26,140 --> 01:04:29,740
Now, that second term, it's also overlooked.

377
01:04:30,310 --> 01:04:37,150
And I will probably discussed today an example of, uh, where this is used.

378
01:04:38,980 --> 01:04:47,290
So when you are estimating and one parameter and there is another parameter in your model as well,

379
01:04:47,770 --> 01:04:52,840
um, so if you ignore the variability of the estimate of your second parameter,

380
01:04:53,170 --> 01:05:00,340
then you are going to be biased and the variance is on the estimate and your confidence intervals will be smaller.

381
01:05:01,210 --> 01:05:12,340
Right. So you need both terms. Um, but before we go there, uh, let's just prove, uh, the law of total variance.

382
01:05:17,610 --> 01:05:24,400
And of course. So if you, uh, if you never have the, the, uh, uh,

383
01:05:25,200 --> 01:05:33,730
law of total variance in your 601 because you doesn't have to be a sigma algebra, it could just be an event.

384
01:05:34,590 --> 01:05:47,870
Um. So you could write an equivalent of of this expression in a less general form in the 601 context.

385
01:05:47,870 --> 01:05:52,490
So that's also something continuous. Okay, so let's start.

386
01:05:53,260 --> 01:05:56,390
Um, I mean, in general, it's clear how to proceed, right?

387
01:05:56,810 --> 01:06:01,920
So I already have the law of total expectation, so I'm going to use it.

388
01:06:02,450 --> 01:06:08,030
But in order to apply it to variance, I need to express variance through expectations.

389
01:06:08,720 --> 01:06:13,490
And we just have that in there. Uh, definition of conditional variance.

390
01:06:13,490 --> 01:06:17,900
There are all the expectations on the right of the conditional variance.

391
01:06:19,330 --> 01:06:22,420
So I will first write.

392
01:06:22,420 --> 01:06:31,140
It is. Of X squared minus the squared expectation of x.

393
01:06:34,510 --> 01:06:42,040
Then I will use the law of total expectation for each of the terms.

394
01:06:42,790 --> 01:06:49,190
So here I will have expected the value of expected value of x squared given u.

395
01:06:51,940 --> 01:07:06,750
That's low of total expectation for the first term. And then I have the law of total expectations for the second one.

396
01:07:07,740 --> 01:07:12,340
Uh, but that's implied inside. Uh, is, this is expectation squared.

397
01:07:12,340 --> 01:07:15,720
The nature of them is expectation.

398
01:07:16,560 --> 01:07:19,590
Of the. Expected to value.

399
01:07:23,530 --> 01:07:30,450
Of X you. Okay.

400
01:07:32,140 --> 01:07:45,300
That's that's one thing. And then I'm going to add and subtract one term that's going to be minus expected value of the squared.

401
01:07:47,020 --> 01:07:53,190
Expectation condition x giving you an to.

402
01:08:00,100 --> 01:08:04,670
The same term. The.

403
01:08:07,660 --> 01:08:11,350
Right. So nothing changes. Of course, the size, subtract and the same thing.

404
01:08:12,380 --> 01:08:15,970
Uh, but I now get to interpret.

405
01:08:16,430 --> 01:08:20,660
So the inside of this expectation.

406
01:08:26,420 --> 01:08:32,130
And the. And this thing as well.

407
01:08:41,980 --> 01:08:49,030
So the first one is, so what's happening inside?

408
01:08:49,030 --> 01:08:58,330
So I have expected the value of x squared minus a conditional one minus the square of the expectation of x.

409
01:08:58,510 --> 01:09:04,300
So that was by definition variance of x, given you.

410
01:09:07,520 --> 01:09:16,940
And the second term. Uh. So let's say my expect the value of x given to you is y as usual.

411
01:09:17,450 --> 01:09:21,380
Right. So I have expected value of y squared.

412
01:09:21,740 --> 01:09:29,570
That's on the second line. I have minus square of the expectation of y on the first line.

413
01:09:29,870 --> 01:09:39,800
So that gives me that this is the variance of y and my wife is e of x giving you.

414
01:09:53,970 --> 01:09:57,170
Right. So this is. Why?

415
01:10:00,100 --> 01:10:05,890
So. And now the only thing I have left is summarize it.

416
01:10:06,280 --> 01:10:20,800
So the first one is actually expected when you write the all to expectation and on the right they have variance of x given you that's this thing.

417
01:10:22,960 --> 01:10:28,660
And then I have variance of the expected value of x given the.

418
01:10:37,450 --> 01:10:41,660
Okay. So this is. It's. Now.

419
01:10:50,240 --> 01:11:00,530
You can see here once again, uh, being emphasized that the expected value of giving you is random variable, so it has a variance to it.

420
01:11:00,530 --> 01:11:05,600
Right. Uh, that's why this second term is, is negligible.

421
01:11:06,050 --> 01:11:15,400
So speaking of interpretations, uh, so if I have, if my ex is related to is an estimate of something, right?

422
01:11:16,520 --> 01:11:22,220
Um, and if to get an estimate, I actually need.

423
01:11:23,700 --> 01:11:32,090
Another estimate. So that's usually happens when my parameters are split into two sections.

424
01:11:32,110 --> 01:11:35,519
Write the parameters of interest and the nuisance parameters.

425
01:11:35,520 --> 01:11:40,860
That's how it's called in statistics. And we estimate the nuisance parameters.

426
01:11:42,660 --> 01:11:47,400
We have to estimate them, but we're not interested in them.

427
01:11:47,910 --> 01:11:54,030
There are different tricks how to get rid of them. There are different kinds of likelihood that the do that.

428
01:11:55,290 --> 01:11:59,339
So if you have statistics for epidemiology, uh,

429
01:11:59,340 --> 01:12:07,470
you may have seen a problem where you have case control studies, uh, major case control studies, remember?

430
01:12:08,640 --> 01:12:13,500
And they are. So you have matched pairs and you want to, uh.

431
01:12:14,980 --> 01:12:19,380
Uh. So estimates, uh, logistic uh, model.

432
01:12:19,770 --> 01:12:23,880
But uh, um, uh, recognizing that there is a matching there.

433
01:12:26,960 --> 01:12:31,320
So what you could do is you can, uh,

434
01:12:31,370 --> 01:12:42,400
just treat the number of the pair or the label of the pair as a categorical variable and write your logistic regression as a categorical covariance.

435
01:12:42,410 --> 01:12:47,030
Right? So each matched pair will be a level of that categorical covariate.

436
01:12:47,700 --> 01:12:58,610
Uh, so the, uh, the estimating, the, these uh, regression coefficients attached to these categorical variables is a nuisance.

437
01:12:58,880 --> 01:13:05,630
So you want the relationship between members of the pair that they're represented by binary and the variables.

438
01:13:06,320 --> 01:13:11,420
Uh, if you do that, does anybody know what happens to your data of interest?

439
01:13:14,850 --> 01:13:23,280
It's going to be biased. If you have a single cover area with a batter and you have a major players design, a case control study,

440
01:13:23,610 --> 01:13:31,290
and you see the usual logistic regression with uh, there being a kind of categorical covariance.

441
01:13:31,560 --> 01:13:38,910
So instead of data, uh, asymptotically rate unbiased estimates, you get two times better.

442
01:13:39,940 --> 01:13:44,860
And if you have more covariance, that's going to be even more complicated.

443
01:13:45,370 --> 01:13:53,520
But bottom line, you have a biased estimate of the reason why that happens is because the dimension of your, uh,

444
01:13:53,650 --> 01:14:02,680
parameter vector, uh, related to that categorical covariance, it grows with the same rate, the sample size, right?

445
01:14:02,680 --> 01:14:13,300
And information fails to accumulate behind each of the baiters that you use to categorize, uh, uh, your categorical covariates with dummies.

446
01:14:13,960 --> 01:14:17,710
Right. And that that weights. It's not classical theory.

447
01:14:17,710 --> 01:14:22,420
Information leaks. Consistency is not guarantee that you even have a biased estimate.

448
01:14:22,870 --> 01:14:27,160
So what do you use for, uh, matched pairs design?

449
01:14:27,490 --> 01:14:35,049
You use conditional logistic regression. So this one, uh, this is one possible way to get rid of the nuisance.

450
01:14:35,050 --> 01:14:44,350
So another way is profile likelihood. So for those of you who have survival analysis, uh, so with the Cox model, we use so called partial likelihood.

451
01:14:44,740 --> 01:14:48,300
That's a profile likelihood actually where you profile out the nuisance.

452
01:14:49,180 --> 01:14:54,310
Yeah. And it happens to work well without a bias. So it sometimes works, sometimes doesn't.

453
01:14:54,700 --> 01:15:00,040
That's not classical theory. You have to examine each particular case separately.

454
01:15:00,700 --> 01:15:13,150
Now, uh, what I wanted to, uh, allude to looking at this formula is that it could be used to interpret, uh, variances in these situations.

455
01:15:13,840 --> 01:15:21,760
And let's say so the conditioning is like, uh, knowing your nuisance parameters.

456
01:15:21,760 --> 01:15:31,450
So if you pretend that you know your nuisance parameters and you get a variance by simply taking a subset, a sub matrix of your information matrix,

457
01:15:31,450 --> 01:15:38,049
let's say if you are using a likelihood based method inverted, then interpreted as the variance of interest,

458
01:15:38,050 --> 01:15:42,730
you are underestimating it because you're ignoring the variance associated with the nuisance.

459
01:15:43,000 --> 01:15:48,880
So the second term will then be the variance associated with the nuisance.

460
01:15:50,650 --> 01:15:54,760
Um. Yeah. So if we have time later on,

461
01:15:54,760 --> 01:16:03,100
I will go over these examples in more detail because we will have maximum likelihood and generalization of this principle,

462
01:16:03,760 --> 01:16:07,780
uh, towards the end of the course. Okay. So this is it for today.

463
01:16:23,420 --> 01:16:32,690
Uh huh. Yeah. Let me quickly here.

464
01:16:32,840 --> 01:16:34,090
Stop the recording.

