1
00:00:22,830 --> 00:00:27,870
Oh, God.

2
00:00:31,100 --> 00:00:52,230
She says she could that.

3
00:01:14,330 --> 00:01:21,190
Excuse me.

4
00:02:21,400 --> 00:02:27,940
I'm trying to make sense of it.

5
00:02:43,970 --> 00:02:49,140
You will get a few more people coming in soon, but we'll get started.

6
00:02:51,210 --> 00:02:56,490
I am 75% of the way through grading the assignments.

7
00:02:57,840 --> 00:03:01,139
So let me pass on some general stuff.

8
00:03:01,140 --> 00:03:04,350
Hopefully I'll be done enough so that I can at least come to you today.

9
00:03:04,360 --> 00:03:07,680
I certainly will be at talks by tomorrow. So I want to talk about that a little bit now.

10
00:03:11,090 --> 00:03:19,819
The main thing that people get challenged on for this first and the reason why it's both right at the beginning of this course and not worth that

11
00:03:19,820 --> 00:03:30,560
much is it's easy to get sucked into one particular thing without recognizing the breadth of different things that each of these systems deal with.

12
00:03:31,540 --> 00:03:37,710
So what I often see is. People will write about something that is right.

13
00:03:39,460 --> 00:03:45,010
I'll just pick something random. Like I'll talk about logistics.

14
00:03:45,910 --> 00:03:50,350
Add something to the electrical system to engage with or formulate.

15
00:03:51,190 --> 00:03:55,300
And that's perfectly accurate. And there will be lots of details, examples of that.

16
00:03:56,320 --> 00:04:06,580
But amid all the other different kinds of of of types of inputs that that system might deal with, I don't there's no sort of master list.

17
00:04:06,580 --> 00:04:10,060
I'm not tricking off and saying, Oh, you only got five out of seven, therefore you got X.

18
00:04:10,900 --> 00:04:15,850
But what I'm looking for is your understanding of the breadth of different types of things systems deal with.

19
00:04:16,790 --> 00:04:23,450
And so if you look at your assignment, so I have not just the number of grades but comments on every assignment,

20
00:04:23,900 --> 00:04:27,660
separate comments for the analytical system and for the experiential system.

21
00:04:28,440 --> 00:04:33,319
If you look at it and you go, Oh, right, I know that one, I just didn't engage with it.

22
00:04:33,320 --> 00:04:34,580
And five, if I can do that.

23
00:04:34,580 --> 00:04:45,280
And if you look at it and you go, Oh, that's different or broader or more curious, you know, I'm not sure I thought it was.

24
00:04:47,770 --> 00:04:52,929
So at the time to talk with me, I want to make sure you're really understanding that part of the reason why this is right

25
00:04:52,930 --> 00:04:56,890
at the beginning of the course is we're going to spend a lot of the rest of this course.

26
00:04:58,500 --> 00:05:05,610
Thinking very intentionally. How do we shape a communication to engage with the analytical system?

27
00:05:06,530 --> 00:05:09,589
To engage with the experiential system.

28
00:05:09,590 --> 00:05:14,450
And the more that you understand the breadth of different kinds of things that these systems deal with,

29
00:05:14,990 --> 00:05:18,320
the more you're going to understand the opportunities that we have as communicators to work on.

30
00:05:18,800 --> 00:05:23,720
And what we're getting into today and for next class is things like, well, we're starting with numbers.

31
00:05:25,190 --> 00:05:30,680
Numbers are analytical, things like, okay, so certainly I'm going to engage with them in that sense.

32
00:05:32,180 --> 00:05:36,290
But to what degree are we also trying to engage emotional responses?

33
00:05:36,860 --> 00:05:45,210
To what degree are we also trying to engage? Other kinds of unconscious associations that might come up from something.

34
00:05:46,510 --> 00:06:00,090
But the other thing I'll say is, if I may paraphrase from many different characters and movies, etc., everyone, please not panic.

35
00:06:02,130 --> 00:06:08,970
The reason why this assignment is the way it is is because I want to make sure that you know that

36
00:06:08,970 --> 00:06:12,450
you know it or that you recognize that you don't necessarily know it as cleanly as you should.

37
00:06:13,140 --> 00:06:19,440
So that we can engage. So that I can work with you. So that you can build upon that and understand it.

38
00:06:19,440 --> 00:06:22,800
Before we get into the next assignment, it starts the dialog.

39
00:06:23,220 --> 00:06:28,950
Let's start to be sort of how do we implement these ideas in the context of changes in the next assignment,

40
00:06:28,950 --> 00:06:36,930
which comes up in, I think three weeks or so? You mean making essentially the test results returned letters?

41
00:06:38,450 --> 00:06:41,850
No test result is inherently a number.

42
00:06:44,300 --> 00:06:49,370
But there is an experiential and emotional component that we also have to pay attention to.

43
00:06:49,430 --> 00:06:55,669
So that's why I want to make sure everybody's got the sort of foundation in place before we get to those assignments,

44
00:06:55,670 --> 00:07:00,979
before you're thinking through, okay, so what do I need to have in my letter to accomplish a different kind of goal?

45
00:07:00,980 --> 00:07:04,780
So we might have. So no panic.

46
00:07:05,260 --> 00:07:09,310
If you look at your assignment, you'll get this really good.

47
00:07:10,510 --> 00:07:14,290
I'm talking and hopefully they make sense from the comments that are there.

48
00:07:14,290 --> 00:07:20,020
But please let them get. Of. For today.

49
00:07:21,700 --> 00:07:27,940
It's sort of a mixture of different stuff I want to do. Some of it has to do very specifically with the stuff that was in the readings.

50
00:07:28,570 --> 00:07:36,610
The specific types of numbers that we're talking about. But some of it is also just sort of the broader engagement in the idea that.

51
00:07:39,230 --> 00:07:46,820
The way in which we react to that is a function not just of what the data are, but how they are presented.

52
00:07:48,570 --> 00:07:53,640
And we have some choices that can be made about what gets emphasized or not.

53
00:07:54,000 --> 00:07:58,530
I will end the day thinking about this broad idea that several articles brought up about the reference class

54
00:07:58,530 --> 00:08:03,380
and how does this pop up and why is it such an important issue when we're thinking about community and health?

55
00:08:05,430 --> 00:08:12,790
But to stir, if you want to go sort of simple and make sure we engage with sort of what are these things,

56
00:08:12,790 --> 00:08:23,050
these probabilities that we're talking about? Yeah, you're like, What the [INAUDIBLE]?

57
00:08:23,710 --> 00:08:26,980
You actually gave an example, which I think is a nice place to start here.

58
00:08:28,330 --> 00:08:33,910
You're talking about cocoa, like couple parts.

59
00:08:35,350 --> 00:08:40,270
You can get chocolate bars that are in cocoa, 70% growth over there.

60
00:08:41,080 --> 00:08:45,460
Talk a little bit about what made you bring up that example because I think it's a good place to start this conversation.

61
00:08:47,060 --> 00:08:53,350
So I wrote the example because when we were reading about like 30% of rain happening tomorrow and no idea what that meant,

62
00:08:54,730 --> 00:08:59,110
I would understand what the 30% of cocoa means in a chocolate bar.

63
00:08:59,120 --> 00:09:02,550
It's like, what is it being compared to?

64
00:09:02,560 --> 00:09:10,660
Like, I understand the concept of math in a chocolate bar, but I don't understand a concept of math in comparison to precipitation.

65
00:09:10,900 --> 00:09:18,690
Right. And so, you know, part of the key thing you represent here is you zoomed in on an example like a chocolate bar is a thing.

66
00:09:19,140 --> 00:09:23,430
It is physically bounded by its size.

67
00:09:23,430 --> 00:09:29,940
It's like we can take it, we can move it, we can boil it down to its component, chemicals, etc. We can analyze it.

68
00:09:29,940 --> 00:09:33,299
There is no ambiguity about, you know, is the wrapper the candy bar?

69
00:09:33,300 --> 00:09:45,060
No, the wrapper is not the candy bar, the wrappers, the wrapper. When we get in the true sense, then the question becomes, what is this thing?

70
00:09:45,060 --> 00:09:48,240
That is the denominator of the.

71
00:09:50,460 --> 00:09:57,540
Notice, by the way, that you would think of candy bars in a more probabilistic way.

72
00:09:58,800 --> 00:10:14,820
So, you know, this example, Willy Wonka golden ticket with this idea, like you have a contest and there's a prize to them in the chocolate bars.

73
00:10:16,080 --> 00:10:22,350
Now, what's the denominator? What is the chance that you will get that winning ticket?

74
00:10:23,310 --> 00:10:27,840
It's not the individual candy bar that's know that's no longer the denominator.

75
00:10:28,290 --> 00:10:35,350
That is each individual instance, each moment. Now the question becomes, well, how many candidates are there?

76
00:10:38,110 --> 00:10:45,340
And is the rate of winning tickets within the candy bar a fixed proportion?

77
00:10:45,880 --> 00:10:52,450
So let's say every out of every hundred candy bars that get created, one of them has a winning ticket.

78
00:10:53,290 --> 00:10:58,030
Or is this there is a fixed number of winning tickets.

79
00:10:58,600 --> 00:11:02,229
And the more candy bars that get created, the lower your chances are.

80
00:11:02,230 --> 00:11:06,030
And that that's the randomness, by the way.

81
00:11:06,070 --> 00:11:09,820
That is obvious. Think on another more modern example.

82
00:11:10,780 --> 00:11:15,970
How likely are you to have a winning ticket if you play the lottery?

83
00:11:18,400 --> 00:11:24,100
Is that a fixed proportion or is that a function of how many other people are buying tickets?

84
00:11:28,120 --> 00:11:35,050
In one sense, it's a bit proportionate. Either they draw the numbers, there is a fixed number of possible combinations.

85
00:11:35,440 --> 00:11:43,990
You either get that combination or you do not. But the value of winning is a function of how many other people buy tickets,

86
00:11:44,620 --> 00:11:51,640
because the odds that you're going to have that prize by yourself versus have to split it is a function of how many other people are playing.

87
00:11:52,330 --> 00:11:55,780
And that assumes that this is random, which of course it's not an honor.

88
00:11:56,380 --> 00:12:01,300
But I want to actually just start with the candy bar example because it helps us get into this.

89
00:12:01,420 --> 00:12:02,770
What are we talking about here?

90
00:12:04,510 --> 00:12:13,270
And when we're talking about treatment, success, or the likelihood that you as an individual experience a particular side effect.

91
00:12:14,180 --> 00:12:25,280
We're more in that lottery there than we are in the candy bar exam because you're not 30% pregnant.

92
00:12:27,400 --> 00:12:31,780
Or dead or itchy or cured.

93
00:12:32,750 --> 00:12:36,390
But we're not talking proportion of a person.

94
00:12:38,000 --> 00:12:41,910
A person. Experience is a binary outcome.

95
00:12:41,920 --> 00:12:44,920
You get it. You don't win the lottery.

96
00:12:45,100 --> 00:12:58,600
You don't. So we use probabilities both to make proportions of a fixed thing, but also to represent this population level idea of.

97
00:13:00,020 --> 00:13:08,990
How in the larger population, how many of those individual instances have this binary thing happened with the winning ticket?

98
00:13:09,960 --> 00:13:17,290
The pregnancy. Yeah, whatever. And then the question is, well, how do we define that phenomenon?

99
00:13:17,590 --> 00:13:26,499
If you have something like a clinical trial where you have an absolutely fixed denominator, we are enrolling 10,000 patients in this clinical trial.

100
00:13:26,500 --> 00:13:33,610
It becomes much cleaner. But your world doesn't like that.

101
00:13:34,020 --> 00:13:39,850
How many people we talk about actually talk about this as a concrete example?

102
00:13:40,630 --> 00:13:51,340
How effective is a COVID 19 vaccine? We think about the reference class there, how we define effectiveness.

103
00:13:53,270 --> 00:14:01,370
Well. In one sense, the reference class is the set of people who have been vaccinated.

104
00:14:02,920 --> 00:14:07,960
So we can look at that set of people and how many of that set of people contract COVID 19.

105
00:14:10,100 --> 00:14:14,200
Lots of problems with that. Let's pick this apart. What's the problem with thinking about that?

106
00:14:14,200 --> 00:14:21,300
Simple. I just thought.

107
00:14:21,420 --> 00:14:27,059
Yeah. And look at everybody. Hey, say what you want.

108
00:14:27,060 --> 00:14:33,480
I'll use that as like everybody who's been vaccinated as a reference class, but you're not going to, like, look at everybody.

109
00:14:33,840 --> 00:14:37,469
So if you're trying to find who has gotten test, I mean, somebody will go get tested.

110
00:14:37,470 --> 00:14:39,600
Don't report it. Some people. Yeah. So.

111
00:14:39,900 --> 00:14:45,680
So you have a in theory you could do this if we tracked every person in the world, but you can't do that problem.

112
00:14:46,800 --> 00:14:50,970
So that's one source of problem. And by the way, the people who.

113
00:14:52,340 --> 00:14:58,940
Get tested in a hospital where they have to report the data versus the people who get tested at home and don't report it.

114
00:14:59,630 --> 00:15:05,110
Screws up our data and those are not random. But there's an even more simple.

115
00:15:05,120 --> 00:15:12,649
Yeah. I mean, the results are kind of aggregating the average population's response for not the average person.

116
00:15:12,650 --> 00:15:17,260
And the whole data that might have been entirely relevant to you, for example, are allergic.

117
00:15:17,270 --> 00:15:21,469
Yeah. So we're going to spend a lot of time talking about this problem. It's a good introduction to it.

118
00:15:21,470 --> 00:15:30,500
Like. We are not all identical. So when we take the whatever statistic we create about the effectiveness of the COVID 19 vaccine,

119
00:15:31,550 --> 00:15:35,990
there is always the question of, Well, am I like that?

120
00:15:37,130 --> 00:15:41,390
To what degree does this population number actually represent?

121
00:15:41,390 --> 00:15:50,510
My individual likelihood we are building consciously or unconsciously a sense of similarity or dissimilarity.

122
00:15:51,600 --> 00:15:56,110
In our own minds that connects us to that. I have a very simple example of this.

123
00:15:57,550 --> 00:16:03,580
I know that I went to my transplant. I was diagnosed with myelodysplastic syndrome at age 28.

124
00:16:04,730 --> 00:16:08,330
All the papers that I could look up about success rates and treatment, survival,

125
00:16:08,330 --> 00:16:14,610
etc. The median the median age of the patients in those trials were in their mid-sixties.

126
00:16:18,260 --> 00:16:31,640
So what a does not it would be what meaning does this population data have given that I have the same condition,

127
00:16:32,600 --> 00:16:41,900
but I am not the same age, I may not be in the same health, etc. That mapping is a reference class problem like I don't have.

128
00:16:43,260 --> 00:16:47,190
A thousand people who are like me and all of the relevant characteristics.

129
00:16:47,200 --> 00:16:51,000
I have a thousand people who have some similar characteristics, but not others.

130
00:16:51,810 --> 00:16:55,650
And so we are left with that. How informative is this question?

131
00:16:56,550 --> 00:17:01,410
In fact, that was one of the very first questions that I asked my doctors when they got into this.

132
00:17:02,340 --> 00:17:06,870
How much should I trust these data as being relevant to me?

133
00:17:07,440 --> 00:17:13,560
And by the way, we ought to have the same conversation about the fact that both medical data, most clinical trial data, is predominantly white.

134
00:17:14,520 --> 00:17:21,360
There are absolutely clear questions in terms of the applicability of those data to other populations.

135
00:17:22,500 --> 00:17:26,159
And even that's the finding that why are we talking about race,

136
00:17:26,160 --> 00:17:32,040
which of course is just a socially defined construct, is this genetics, is this environment, is this diet.

137
00:17:32,580 --> 00:17:41,520
All of those kinds of things may be similar or different, and they affect the ability of the underlying data to an individual.

138
00:17:42,670 --> 00:17:51,270
The original question is, what is the problem with assuming that this whatever statistics we get,

139
00:17:52,050 --> 00:17:58,800
COVID 19 vaccines is an accurate representation of my personal life or personal chances?

140
00:18:01,480 --> 00:18:06,800
There's one more I want to bring up right now. You.

141
00:18:07,060 --> 00:18:11,970
Everybody get vaccinated at the same time. You know.

142
00:18:12,570 --> 00:18:16,650
So is it time compressed?

143
00:18:18,060 --> 00:18:25,950
Like some people got vaccinated earlier. They have more data points in time when they were vaccinated that which would be

144
00:18:25,950 --> 00:18:30,210
informative towards the effect of measuring the effectiveness of the vaccine.

145
00:18:30,870 --> 00:18:34,470
Other people got vaccinated yesterday. They may not even have any protection yet.

146
00:18:36,570 --> 00:18:45,450
We're not even getting into the idea that vaccination protection is not a constant thing but might be waning over time,

147
00:18:46,290 --> 00:18:49,770
as we discovered if we have booster shots.

148
00:18:51,180 --> 00:18:53,940
So all those things are basically changes to the reference class.

149
00:18:56,550 --> 00:19:02,820
Changes to what the denominator is of the statistic that were supposedly saying this is you.

150
00:19:05,610 --> 00:19:12,150
Just to use one more example of this, because it's an important point. Let's go back to the conversation we had last class about contraception.

151
00:19:15,500 --> 00:19:24,460
What's the reference class for effectiveness in that context? You need to not only be thinking about time, how long are you somebody?

152
00:19:25,250 --> 00:19:30,740
But you need to be thinking about questions of level of sexual activity because your

153
00:19:31,010 --> 00:19:36,710
likelihood of having an unintended pregnancy is a function of how often someone is having sex.

154
00:19:38,550 --> 00:19:42,420
You need to be thinking about is this.

155
00:19:44,020 --> 00:19:47,920
Quality use about someone who's on the pill.

156
00:19:49,360 --> 00:19:55,630
Is this all people? Is this all people who are actually taking their pill regularly the way they're supposed to?

157
00:19:56,800 --> 00:20:03,460
They're not saying that. And obviously, the risk if you don't take your pills and if you do.

158
00:20:04,570 --> 00:20:08,920
Which, by the way, is a huge issue in the context of measuring efficacy of things like HIV suppression.

159
00:20:11,310 --> 00:20:14,580
If your answer regimens is a fundamental change in the reference class.

160
00:20:17,520 --> 00:20:22,330
So again, there's no simple answer here. Why am I harping on this?

161
00:20:22,720 --> 00:20:26,080
Because I want you always questioning what you see as existence.

162
00:20:27,880 --> 00:20:30,940
What's the right question? Who are these people? What is the denominator?

163
00:20:33,740 --> 00:20:38,719
Always asked that question, not just in terms of of characteristics,

164
00:20:38,720 --> 00:20:45,800
in terms of fit to whatever the individual or the population is if you're trying to generalize that information to.

165
00:20:50,800 --> 00:20:54,970
Because it's not irrational for someone to say, for example.

166
00:20:58,530 --> 00:21:04,740
This may not work that well for everybody, but because I have X, Y and z characteristic, it might still be the right thing for me.

167
00:21:06,800 --> 00:21:10,610
Now you may be overconfident and that's its own problem, and you can pay with that.

168
00:21:11,180 --> 00:21:17,570
But there are times in which the right choice for an individual is not the same thing as the average.

169
00:21:18,740 --> 00:21:28,500
Most effective or at least risky thing for the population. Running forward with this idea.

170
00:21:30,960 --> 00:21:38,440
Riley you were talking to using it about sort of like who is in this stuff as part of this conversation.

171
00:21:38,480 --> 00:21:41,760
I just want to give you a chance to talk a little bit more about that. Sure.

172
00:21:42,750 --> 00:21:49,430
So kind of going back to the numeracy readings that we did on Tuesday or from Monday and Tuesday.

173
00:21:49,450 --> 00:22:00,269
Yeah, I personally have a hard time connecting to numbers like one in ten and 130 and like putting a face to those

174
00:22:00,270 --> 00:22:08,520
numbers like was it a perfect study where all the patients didn't have any additional risk factors or,

175
00:22:08,520 --> 00:22:13,440
you know, did they have previous heart conditions or are they smokers?

176
00:22:13,440 --> 00:22:21,319
All the above. And seeing like. If you were to read that, could you relate to the statistics more if they added information like that,

177
00:22:21,320 --> 00:22:28,610
or would it be harder for you to understand and connect? So there's two pieces of where Bradley is bringing up that I want to highlight.

178
00:22:28,760 --> 00:22:34,220
One is where I think you're emphasizing it, which is. Reader.

179
00:22:34,320 --> 00:22:40,899
A knowledge about the population make those numbers seem more trustworthy,

180
00:22:40,900 --> 00:22:46,450
more applicable to you, the personal connection you feel to that information.

181
00:22:47,950 --> 00:22:56,020
The second is the homogeneity versus heterogeneity of the population itself.

182
00:22:57,450 --> 00:23:02,520
If everybody in particular, let's say, has two cardiovascular medications.

183
00:23:03,550 --> 00:23:09,970
He's in a very narrow age range and they're all have similar weights and they all have similar blood pressures and they all are either smokers or not.

184
00:23:10,570 --> 00:23:16,090
Similarity increases reliability for people who fit that.

185
00:23:18,940 --> 00:23:22,629
And if you don't fit that model, then you're left with the weight.

186
00:23:22,630 --> 00:23:27,010
Does this apply to me? In the real world, people are not the same.

187
00:23:27,820 --> 00:23:37,540
So big real world trials, whether experimental or observational, have enormous heterogeneity.

188
00:23:37,540 --> 00:23:41,229
You are different than I am in a variety of ways. We're both in this trial.

189
00:23:41,230 --> 00:23:45,160
Well, how do we think of the US as a little bit different?

190
00:23:47,980 --> 00:23:51,490
So you're right, this is sort of a core tension.

191
00:23:52,720 --> 00:23:56,500
The more narrow we can make our denominator upper class,

192
00:23:57,250 --> 00:24:02,950
the more applicable the number is to people who are like that and the less applicable that number is to everybody else.

193
00:24:04,390 --> 00:24:11,290
When we do a big, broad trial that includes everybody, in one sense, it's applicable to nobody.

194
00:24:14,640 --> 00:24:18,420
Because nobody is the average or essentially no one is the average.

195
00:24:19,170 --> 00:24:24,120
I'll come back to that later on because there are some real good examples. In medical science.

196
00:24:24,960 --> 00:24:35,070
We made very large scale decisions about risk interventions based upon massive populations and assumed it applied to everybody.

197
00:24:36,150 --> 00:24:40,090
And there's no weaknesses in that. Why don't we start here?

198
00:24:40,090 --> 00:24:43,180
Because you're raising a key question. Always ask the question.

199
00:24:43,180 --> 00:24:44,920
You know who is in the study? Why?

200
00:24:44,950 --> 00:24:51,790
Why do I think this study, population or this whatever the population is, where this data is coming from applies to me.

201
00:24:53,680 --> 00:25:00,250
And related to that, I see it you were talking about it sometimes a similar issue,

202
00:25:01,270 --> 00:25:05,050
less about these things like health issues, but more about populations.

203
00:25:05,530 --> 00:25:08,380
Yeah. So I was kind of talking about the idea that.

204
00:25:10,370 --> 00:25:17,540
I suppose we brought up the example like BMI and how it's used to talk about target like weight related diseases.

205
00:25:17,840 --> 00:25:25,550
But it's like this research that shows that specifically in Asians, for example, you're more at risk at a lower BMI.

206
00:25:25,580 --> 00:25:30,270
So if the doctors come and tell you and say that you're at a normal BMI, you're good for Asians.

207
00:25:30,270 --> 00:25:36,680
So that's not necessarily true for me. Right. But often that's not widely known even among medical professionals.

208
00:25:37,670 --> 00:25:44,300
So let's break this down in terms of reference class. But the core problem here is, in theory, one could have.

209
00:25:45,540 --> 00:25:49,469
And there's all kinds of problems with BMI, and I'm just gonna wave my hands and forget about them.

210
00:25:49,470 --> 00:25:56,250
Because for the purposes of this discussion, we're going to assume that this is something we might actually want to use, and we can argue about that.

211
00:25:57,180 --> 00:26:04,380
But we have population of different genetic ancestry.

212
00:26:05,710 --> 00:26:14,380
Cool. If you look at them, those populations have different distributions of weight and height, i.e. BMI.

213
00:26:16,450 --> 00:26:21,190
What is the use you're describing in BMI as a proxy?

214
00:26:21,370 --> 00:26:25,510
We don't actually care about BMI. Let's just acknowledge this right now. Nobody actually cares about BMI.

215
00:26:26,530 --> 00:26:29,860
BMI is a proxy for health.

216
00:26:30,370 --> 00:26:35,770
It's a poor proxy, but we use it because it's correlated with things that we might care about.

217
00:26:39,050 --> 00:26:47,350
I think he was looking when the commission is reading off this number they're making and translate that number into a judgment of potential health,

218
00:26:47,350 --> 00:26:52,600
risk, healthiness or not, whatever. And that judgment is poorly calibrated.

219
00:26:54,110 --> 00:26:57,259
When we take the average whole population predominantly white,

220
00:26:57,260 --> 00:27:02,540
because that's where those numbers came from originally and apply it to, for example, the Asian population.

221
00:27:03,980 --> 00:27:06,350
We could imagine having multiple tables.

222
00:27:07,590 --> 00:27:16,300
Multiple essentially mappings of number to a meaning that would be differently tailored for different populations.

223
00:27:16,320 --> 00:27:21,450
We do that all the time, by the way, for certain things like sex.

224
00:27:21,810 --> 00:27:31,290
And by the way, I'm using sex here, not gender, because I'm meaning the underlying biological chromosomal sex as being associated with height,

225
00:27:31,500 --> 00:27:37,170
weight, etc. But we could do so in terms of genetics.

226
00:27:37,230 --> 00:27:40,470
Power would be appropriate to do so in other ways.

227
00:27:42,180 --> 00:27:46,230
That would in one sense make the number more meaningful.

228
00:27:48,110 --> 00:27:56,240
It also means that we would have to be really confident in our ability to categorize people and be able to say,

229
00:27:56,510 --> 00:28:02,300
you use this table and you use that table because if we screw that up, we make the problem even worse.

230
00:28:05,010 --> 00:28:13,640
It is one of the core problems with using race and account as a categorization.

231
00:28:15,230 --> 00:28:18,680
What is it that is a proxy for? What is it? We think that it's underlying measurement.

232
00:28:22,890 --> 00:28:27,300
If it's something about genetics, then we have to have the conversation at that level.

233
00:28:27,310 --> 00:28:32,310
But boy, that is using a socially created construct to mean something that it is not.

234
00:28:35,470 --> 00:28:42,280
And if what we mean is something else, say, environment or other things associated with that, then we have to have that conversation too.

235
00:28:45,280 --> 00:28:49,320
I will come back to this issue of race later on.

236
00:28:49,330 --> 00:28:55,540
This is a theme that is really an important and very critical question in a variety of risk issues.

237
00:28:56,860 --> 00:29:01,930
So if we're talking about population statistics as being kind of problematic,

238
00:29:02,830 --> 00:29:07,900
could you also argue the other way that getting narrower and narrower is also problematic

239
00:29:07,930 --> 00:29:12,420
because every single person is a different person and there's inherent randomness.

240
00:29:12,430 --> 00:29:17,440
You can't predict the future. Well. Be careful here.

241
00:29:17,560 --> 00:29:25,740
So there's a very specific idea here that. We see correlations between characteristics.

242
00:29:26,220 --> 00:29:33,030
They may be inherited characteristics. They may be temporary characteristic, like what is your blood pressure today to future outcomes?

243
00:29:33,330 --> 00:29:36,600
That's the whole concept behind building predictive models.

244
00:29:38,560 --> 00:29:43,420
Those models create something that is like a probability we can.

245
00:29:45,420 --> 00:29:53,559
But it does not tell us what will happen to you. So there's the inherent uncertainty in the outcome, but not necessarily.

246
00:29:53,560 --> 00:29:59,450
I mean, we can have very predictive models. Then pick up the reality that, for example,

247
00:29:59,450 --> 00:30:06,169
people who smoke lots and lots of cigarets over long periods of time are far more likely to develop lung cancer.

248
00:30:06,170 --> 00:30:09,710
The people who do not. That is predictive power.

249
00:30:09,980 --> 00:30:16,850
That's information we can use, even though there are always people who have smoked for 30 years who don't get cancer.

250
00:30:19,110 --> 00:30:26,640
So you're absolutely right. The more order we make this as a problem, the narrower we make this.

251
00:30:27,620 --> 00:30:35,059
The less generalizable it is, but at least in theory, its potential, there is value to tailored estimates of your risk.

252
00:30:35,060 --> 00:30:38,240
And so that's why I'm setting this up, is there's a whole.

253
00:30:39,490 --> 00:30:45,690
Part of modern medicine and modern risk, which is about risk estimation, building tailored estimates.

254
00:30:45,700 --> 00:30:49,070
I say, what is your risk? What's your risk?

255
00:30:49,100 --> 00:30:54,340
What is your risk? You want to find out what your personal risk of colorectal cancer is?

256
00:30:54,370 --> 00:30:58,149
Go online type colorectal cancer risk calculator and you put in a whole bunch of

257
00:30:58,150 --> 00:31:01,330
stuff and you will get different numbers depending on what those answers are.

258
00:31:04,350 --> 00:31:08,070
And then the question is great, what do we do with that?

259
00:31:09,210 --> 00:31:15,990
How much do I see the reference class of those statistics which are tailored based upon the inputs that are put into it,

260
00:31:17,070 --> 00:31:24,570
but not tailored on everything else that's real about us and providing information about what's possibly going to happen to me.

261
00:31:26,750 --> 00:31:30,930
And. I don't want to minimize what Andrew's being up here because.

262
00:31:31,140 --> 00:31:42,230
So I'll give you an example. I'm working right now with a doctoral student whose interest is in cancer screening decisions in older adults.

263
00:31:42,920 --> 00:31:48,860
So like 75 year olds, 80 year olds, not 50 year olds, like we're not arguing about you have your first colonoscopy.

264
00:31:50,000 --> 00:32:02,360
We're talking about two people who are 75 or 80. And the guidelines for such kinds of decisions are increasingly tailored in the sense that

265
00:32:02,360 --> 00:32:09,050
they say you should look at this person's health and estimate their life expectancy.

266
00:32:09,530 --> 00:32:16,100
And if they are expected to live for more than ten years, and it's probably worth doing another colorectal cancer screening.

267
00:32:17,090 --> 00:32:19,250
But if their life expectancy is less than that,

268
00:32:19,670 --> 00:32:24,980
the risks and costs associated with the screening are probably going to outweigh any benefits they might get.

269
00:32:25,810 --> 00:32:26,860
So don't do the Graham.

270
00:32:28,090 --> 00:32:38,079
So what we're talking about is a medical decision being tailored at the individual level by some model that takes into account your age,

271
00:32:38,080 --> 00:32:41,440
your health, your weight, your blood pressure, all of these kinds of things.

272
00:32:43,620 --> 00:32:48,660
Question is entrusted. When you use it.

273
00:32:50,990 --> 00:32:55,280
And what she's finding is even when you talk to the doctors, the answers vary from.

274
00:32:55,300 --> 00:33:04,310
Oh, absolutely true. And you have no way. Huge variance of how often people are having their decisions being driven off of this algorithm.

275
00:33:05,930 --> 00:33:07,280
That's a different kind of risk mitigation.

276
00:33:10,240 --> 00:33:19,120
But it goes to this question of can we actually use population information which these models are incorporated to predict something and do we want to?

277
00:33:19,360 --> 00:33:23,770
Are we comfortable with that? What meaning do we get from these types of individual predictions?

278
00:33:25,860 --> 00:33:32,900
All right. I'm going to keep moving on because there's lots of good examples here. Oh, yeah.

279
00:33:32,910 --> 00:33:41,160
Stepping out of health for a second. Uh. Uh, are you talking about floods?

280
00:33:41,850 --> 00:33:46,230
Yeah. Oh, I was trying to think of another example.

281
00:33:46,360 --> 00:33:51,720
Um, I was having a hard time after reading the reading about, like, the weather.

282
00:33:53,130 --> 00:34:01,800
30% chance of precipitation. What that means. And I went to undergrad at Michigan State, and I was there in 2018 when we had, like, a ton of flooding.

283
00:34:01,800 --> 00:34:05,790
Like half the campus was flooded. They thought they were going to have to evacuate people out of dorms.

284
00:34:05,790 --> 00:34:13,320
So it was really bad for a couple of weeks. I mean, I was in a class about natural hazards and like natural disasters at the time,

285
00:34:13,650 --> 00:34:19,170
and we spent a lot of time talking about that and whether or not we thought that this cause, the floodwaters hadn't crested yet.

286
00:34:19,170 --> 00:34:26,280
Even when we got to that point. And in this class we were talking about like, you know, is this like a 50 year flood?

287
00:34:26,280 --> 00:34:30,509
Is this a 100 year flood? Is this a thousand year flood or, you know, whatever?

288
00:34:30,510 --> 00:34:35,489
And I was just thinking about how that's really kind of misleading to talk about in that way,

289
00:34:35,490 --> 00:34:38,760
because it doesn't necessarily mean that if we have 100 year flood,

290
00:34:38,760 --> 00:34:46,470
that for the next 99 years like we're safe from a flood of like this bad it's that statistics and that's really hard to wrap your head around.

291
00:34:46,800 --> 00:34:54,540
Yeah. So what is that reference class that is implied when we talk about something as being in a hundred year flood plain.

292
00:34:56,790 --> 00:35:00,899
You want to take a shot at it? I don't know. Okay. Specific.

293
00:35:00,900 --> 00:35:01,049
I mean,

294
00:35:01,050 --> 00:35:09,200
depends on the basically regional data depending upon climate conditions and like non-human interference with like the watershed or stuff like that.

295
00:35:09,210 --> 00:35:20,730
So basically blank slate type of. So notice a few caveats that are here, some defined space without human interference.

296
00:35:20,730 --> 00:35:23,959
Like we're recognizing the fact that if I build a building or a dam or other things,

297
00:35:23,960 --> 00:35:32,340
that changes the dynamics of water and that might change what happens. But it's a population level statistic, it says.

298
00:35:33,530 --> 00:35:36,650
If all else held equal, which of course, it never is.

299
00:35:39,250 --> 00:35:41,410
This type of flood,

300
00:35:41,830 --> 00:35:56,210
this space ought to get flooded on average one time per hundred years over what the amount of time that we've have in human recorded history.

301
00:35:56,230 --> 00:36:03,280
Like, it's not like we have a denominator here that is like we can't do a monte Carlo simulation and.

302
00:36:04,330 --> 00:36:07,350
Run different weather patterns over at the same place over the same time.

303
00:36:07,360 --> 00:36:11,590
Like we're essentially creating a time denominator that doesn't exist.

304
00:36:12,250 --> 00:36:22,560
We don't actually get a chance to know how often things are happening over long periods of time because we don't observe long periods of time.

305
00:36:22,570 --> 00:36:26,260
I don't have weather forecasts for this location over the last 10,000 years,

306
00:36:27,400 --> 00:36:30,520
which is really the denominator we might want if we wanted to do eventually

307
00:36:30,520 --> 00:36:35,080
hundred year flood or 50 year flood or thousand year flood kind of statistic.

308
00:36:37,840 --> 00:36:47,290
So you're right. It's confusing, but it's confusing in the way that it's the same way that happens when we talk about translate

309
00:36:47,290 --> 00:36:53,410
populations that level statistics to individual statistics like this individual plays.

310
00:36:55,710 --> 00:37:01,530
Can have the bad thing the flood happened to just like this individual person can have the bad thing happen to it,

311
00:37:02,310 --> 00:37:03,900
regardless of what the likelihood is.

312
00:37:05,560 --> 00:37:14,320
And as we learn more, we may be changing our estimates, just like as we measure what happens in terms of whether we may be changing our.

313
00:37:14,620 --> 00:37:19,630
Oh, no, this isn't a 100 year flood plain. It's a 50 year flood plain because floods here are happening more often than we thought.

314
00:37:25,380 --> 00:37:28,610
Yeah. I'm just. I'm an anthropology person.

315
00:37:28,750 --> 00:37:32,040
This is crazy. The how like myth. I know so out of there.

316
00:37:32,260 --> 00:37:34,410
So, like, myth is used as a way of risk.

317
00:37:34,800 --> 00:37:40,030
So if I say there's a giant sea monster who always happens here, there's one area I'd never build a house in this cliff.

318
00:37:40,140 --> 00:37:46,560
Yeah, they realize there's always, like, giant natural disasters that happen every maybe 200,000 years.

319
00:37:46,890 --> 00:37:55,020
They use myth as a way of saying, Oh, no, it's not actually out of scope because it allows me to make a connection that's important to make here.

320
00:37:55,800 --> 00:38:02,250
Why do we make associations between particular locations where things happen and risk?

321
00:38:02,910 --> 00:38:06,000
What you are describing is experiential learning.

322
00:38:07,430 --> 00:38:15,510
I go back to the analytical versus experiential. The underlying experience of this is a place where bad things happen frequently.

323
00:38:15,690 --> 00:38:21,060
I may not have a logical explanation for it, like my analytical brain is going, Why the heck is this always happening here?

324
00:38:21,580 --> 00:38:23,550
Why is this always happen to me?

325
00:38:24,450 --> 00:38:31,920
We come up with stories as a way of explaining to our analytical side the reality that our experiential side has already processed,

326
00:38:32,370 --> 00:38:36,120
that this place has bad stuff happened to it.

327
00:38:37,320 --> 00:38:41,850
I thought maybe totally random, like because of random chance.

328
00:38:42,090 --> 00:38:50,010
Some political or place might have lightning hit it and then get flooded and not have an earthquake like some place has to have all that stuff happen.

329
00:38:51,560 --> 00:38:55,030
But our analytical side doesn't like randomness.

330
00:38:55,040 --> 00:39:03,079
We build causal explanations. And so part of what we're talking about, and we'll get to this in about I want to say about four weeks,

331
00:39:03,080 --> 00:39:09,090
we start talking in this course about what what's called mental models. We're talking about his mental models of risk.

332
00:39:10,800 --> 00:39:14,190
What do we believe is causing the outcomes that we're observing?

333
00:39:16,530 --> 00:39:23,270
So. No, that's not off of. You know, I want to bring this up.

334
00:39:24,520 --> 00:39:28,410
Um. Oh, yeah. This is probably a good time. Um.

335
00:39:29,480 --> 00:39:36,230
This is silly, but it actually has a key point. I'm going to show you a clip from an old episode of The Daily Show.

336
00:39:38,780 --> 00:39:46,550
It is ridiculous in one sense, and yet a good example of something that really does exist in the real world.

337
00:40:00,240 --> 00:40:04,530
She was going ugly for the Clintons. Nobody with any expertize.

338
00:40:04,570 --> 00:40:06,820
Oh, let me get him to stop. Let me set you back up here.

339
00:40:07,300 --> 00:40:16,050
This was a clip at the time when the CERN Large Hadron Collider was about to be started in Switzerland.

340
00:40:16,070 --> 00:40:22,600
This was a huge collider and there was some actual questions being raised about whether we actually

341
00:40:23,110 --> 00:40:29,919
crashed these high speed particles together and created certain types of physics particles.

342
00:40:29,920 --> 00:40:37,090
There would be some unanticipated consequences of that. So this is a conversation around the risks of starting a Large Hadron Collider.

343
00:40:40,730 --> 00:40:44,990
So physics says the slightest risk of any danger whatsoever.

344
00:40:45,170 --> 00:40:49,190
Oh, really? What about this guy on the slab 2 hours together?

345
00:40:49,220 --> 00:40:52,930
Some theories suggest they might collapse into the black hole.

346
00:40:53,070 --> 00:41:00,440
Eventually, it would convert the earth. That's also why the one man with the courage to speak out against the experiment.

347
00:41:00,560 --> 00:41:03,799
I think those are things about the physics teacher.

348
00:41:03,800 --> 00:41:07,340
Wide variety of science classes are very high school.

349
00:41:07,400 --> 00:41:12,610
So roughly speaking, what are the chances the world is going to be destroyed in 1 million?

350
00:41:12,650 --> 00:41:26,510
1 billion? Well, the best way to say right now is about a woman's chance of 0% chance.

351
00:41:26,870 --> 00:41:34,760
50. 50. Zero. Looked like it was time for a science of at least try to dazzle me with this child, this horrible.

352
00:41:35,300 --> 00:41:38,600
So to make life I can. I think this goes on.

353
00:41:38,690 --> 00:41:43,920
Well, what happens in each one elucidates the principles, the chances of 5050 chance.

354
00:41:43,940 --> 00:41:50,270
You need to come back to this 5050 thing if we want to have something can happen.

355
00:41:51,500 --> 00:42:01,640
And then something necessary happened is that you either happen or three or four weeks before the press conference.

356
00:42:02,890 --> 00:42:13,540
Not sure that's how you want to. This phenomenon, the 5050 phenomenon.

357
00:42:14,480 --> 00:42:16,730
Is a known thing in the risk literature.

358
00:42:17,570 --> 00:42:26,030
If you ask people a question, how likely is X to happen and you ask them to give you a number between zero and 100 represent probability,

359
00:42:26,510 --> 00:42:32,389
there is always somewhere between eight and 20% of the population.

360
00:42:32,390 --> 00:42:38,280
That will give you an answer. 50 always. What is being represented here.

361
00:42:38,310 --> 00:42:47,750
There's something really important. What is this guy, Walter Wagner talking about when he keeps coming back to this idea of.

362
00:42:51,990 --> 00:42:58,630
Okay. You've seen the binary outcome as the probability of it happening, but it's not quite how that works.

363
00:42:59,620 --> 00:43:03,880
It's not how that works at the population level. Yeah.

364
00:43:03,910 --> 00:43:12,450
There is a underlying truth. In the sense that the outcome for the individual is binary.

365
00:43:14,830 --> 00:43:19,030
So I don't get 50% anything.

366
00:43:19,060 --> 00:43:23,350
The world is not 50% here or not. Either it's here or it isn't.

367
00:43:25,250 --> 00:43:34,480
And. One of the most fundamental types of risk communication is this possibility communication.

368
00:43:35,290 --> 00:43:46,080
The bad thing might happen. And there are times when we are really only trying to achieve possibility communication.

369
00:43:47,760 --> 00:43:53,280
I'm not actually going to try to calibrate you on probability. I just want you to know that this might exist and the flood.

370
00:43:53,310 --> 00:44:00,510
This is what brought me to think about this. The flood is one of them. Like, yes, we may think about how likely isn't the flooding going to happen,

371
00:44:00,930 --> 00:44:06,000
but on the other hand, there is a certain type of should I have an emergency plan for my house?

372
00:44:06,810 --> 00:44:10,710
Yeah, I should. Do I know what? The likelihood that it's going to be flooded?

373
00:44:11,130 --> 00:44:21,670
No. Does it matter? No. Sometimes we can get lost in the probability when what we really need to be doing is to engage in that in that possibility.

374
00:44:22,770 --> 00:44:26,150
Now. This is silly. It's over-the-top, etc.

375
00:44:26,160 --> 00:44:36,860
But I don't want to. I want to make sure that we don't get so enamored of our numbers that we forget that sometimes the goals of our

376
00:44:36,860 --> 00:44:42,019
communication is just to remind people that this thing might happen and maybe you should take some precautions,

377
00:44:42,020 --> 00:44:49,100
or maybe you should look for it, like monitor, make sure that it might happen, and if it does respond to it, go.

378
00:44:49,220 --> 00:44:53,870
I just have such a relevant example that they everybody listen to the podcast, the big one.

379
00:44:55,190 --> 00:45:01,580
It's like built into the probability of a really big earthquake that could happen.

380
00:45:01,940 --> 00:45:11,540
Mentioned another domain in which we often hear years like we're going to have a big earthquake every 200 years or 300 years or whatever.

381
00:45:11,540 --> 00:45:14,000
Same problem as the flood statistics. Yeah. Yeah.

382
00:45:15,080 --> 00:45:24,409
And so they go and they talk to all these experts and amateur experts, preppers, you know, building managers,

383
00:45:24,410 --> 00:45:30,440
urban planners, and just get all the different perspectives on what they're doing to prepare or not doing to prepare.

384
00:45:31,700 --> 00:45:36,910
Good. Super interesting, because depending on who they talk to, you know, the risk level changes and.

385
00:45:38,390 --> 00:45:45,950
You feel like we're measured responses about how you can be adequately prepared, and then you get like on the extreme end of the spectrum,

386
00:45:45,950 --> 00:45:51,620
like preppers who think it's going to happen tomorrow and they have like a year's worth of supplies ready.

387
00:45:51,770 --> 00:45:59,770
So super interesting. Yeah. To take an example of this, we're getting a little bit ahead of topics in this course, but it's useful to bring it up now.

388
00:46:01,320 --> 00:46:08,190
One of the the sides of what is often described as risk management is basically calculating expected value.

389
00:46:08,580 --> 00:46:12,240
How likely is this thing to occur versus what's involved in trying to prepare?

390
00:46:14,160 --> 00:46:19,140
There's a really salient public health example in which this failed spectacularly,

391
00:46:20,010 --> 00:46:28,709
which is the Deepwater Horizon oil spill in the Gulf of Mexico, in which a calculation was made of how likely X, Y and Z were to occur,

392
00:46:28,710 --> 00:46:35,550
and because the probability was so low, in particular shut off type valve was not installed, that had it been installed,

393
00:46:35,550 --> 00:46:40,080
would have cut off the flow of oil and would have basically made sure that the spill ever happened.

394
00:46:41,310 --> 00:46:51,840
But because that probability was defined as sufficiently small, it wasn't worth it to engage in the proper in the in the precautionary actions.

395
00:46:52,110 --> 00:46:58,650
And so the terminology that you will hear in policy, which we'll come back to later on, is the precautionary principle.

396
00:46:59,640 --> 00:47:06,900
And the idea of the precautionary principle is that if something might occur and it will be catastrophic.

397
00:47:08,150 --> 00:47:15,500
The fact that it is unlikely to occur should not be by itself justification not to take protective action.

398
00:47:17,450 --> 00:47:25,009
Obviously, you can take this on to the most extreme version. We could be spending billions of dollars trying to protect against things that have

399
00:47:25,010 --> 00:47:28,910
an extremely low likelihood of occurring and that would have huge societal costs.

400
00:47:29,930 --> 00:47:33,590
But there is a logic behind the precautionary principle.

401
00:47:33,750 --> 00:47:38,209
Like, there are certain types of things that we ought to prepare for we ought to guard against,

402
00:47:38,210 --> 00:47:44,420
even though they're super unlikely, because the consequences of being wrong are so catastrophic.

403
00:47:46,100 --> 00:47:49,370
That's another place where the possibility becomes more salient and probably true.

404
00:47:49,900 --> 00:47:54,980
Yeah. Reminds me of like the. The Challenger shuttle launch.

405
00:47:55,000 --> 00:48:03,829
Yeah. Um, another example. Yeah. How, like, they knew that there was a probability that, like, the weather wasn't right for the launch and everything,

406
00:48:03,830 --> 00:48:11,450
and it just seemed, as, you know, that was that particular example.

407
00:48:11,450 --> 00:48:17,150
And again, I won't go into the details. There's a that's a really good analysis of what happened with the challenger,

408
00:48:18,380 --> 00:48:27,500
including basically that the failure occurred in a meeting between the engineers and the launch leaders.

409
00:48:28,220 --> 00:48:36,320
And they boiled it down to a PowerPoint slide. And some critical information about the reference class was omitted in the PowerPoint slide.

410
00:48:36,320 --> 00:48:41,930
And so they did not actually understand how likely it was that the O-rings were

411
00:48:41,930 --> 00:48:45,680
going to fail and that there was going to be a catastrophic failure of the rocket.

412
00:48:47,120 --> 00:48:55,400
This is actually very salient to this, like they talked about it in generally terms and possibility terms rather than defining the reference class.

413
00:48:56,650 --> 00:49:01,330
Um. I have so much stuff to get through here.

414
00:49:03,430 --> 00:49:08,960
Let me move forward. At first.

415
00:49:11,470 --> 00:49:17,190
And these are. You were talking in your musing about.

416
00:49:18,570 --> 00:49:25,410
That question of sort of like one in ten, how likely is this to happen and then the lived experience of it happening?

417
00:49:27,590 --> 00:49:34,850
Yeah. So I was saying how it seems like one in ten for some people it's easier to like I guess understand it doesn't.

418
00:49:35,910 --> 00:49:37,860
But that one an ex doesn't bring,

419
00:49:37,860 --> 00:49:46,110
I guess panic or emotions or additional emotions to a situation because I think it's like, oh, one out of ten people.

420
00:49:46,410 --> 00:49:50,400
I'd have to be really unlucky for that thing, that reading or.

421
00:49:51,240 --> 00:49:58,230
Yeah. So basically I was talking about that in my lived experience about five years ago I had surgery and my surgeon in the post-op,

422
00:49:58,380 --> 00:50:07,470
the pre-op surgery told me that there was like, Oh, what a chance of me having either my bottom lip or my chin permanently numb.

423
00:50:07,680 --> 00:50:12,470
And I thought like, oh, one in ten, chances are slim. Like, It'll never happen to me.

424
00:50:12,480 --> 00:50:19,320
I'll have to have really bad luck happened while it happened and I kind of was cautioning like I just had

425
00:50:19,320 --> 00:50:25,530
bad luck or was either one in ten and the statistics or the probability of that happening was correct.

426
00:50:26,220 --> 00:50:32,520
And so that piece that you just said was I just unlucky was this statistic.

427
00:50:32,520 --> 00:50:43,090
Right, is what I want to talk about next. We have statistics that say things are unlikely to occur, but they do occur to somebody.

428
00:50:43,600 --> 00:50:48,940
So when they occur and they do, the right question to ask is.

429
00:50:50,510 --> 00:50:52,340
You know what? I just don't like it.

430
00:50:52,370 --> 00:50:59,479
What is this happening in the normal frequency within the population where something fundamentally wrong in our estimation of

431
00:50:59,480 --> 00:51:12,620
how likely this is a real world public health example of this is the phenomenon known as cancer clusters monitor populations.

432
00:51:14,240 --> 00:51:22,010
You notice, hey, this particular community, this particular zip code, this particular area seems to have a lot of people with cancer.

433
00:51:24,970 --> 00:51:29,620
We're exactly the same situation and this is but somebody is going to get cancer.

434
00:51:29,620 --> 00:51:34,120
And by random distribution, they're going to be some places that have fewer cancers and some places that have more cancers.

435
00:51:35,470 --> 00:51:41,620
But when we see that population concentration, the right question to ask is.

436
00:51:43,110 --> 00:51:47,790
You know, how plausible is that? We would see a place like this by chance.

437
00:51:48,540 --> 00:51:53,759
And is there any causal factor that we might need to associate with something that's going on here?

438
00:51:53,760 --> 00:51:56,040
The water supply is contaminated. There's whatever.

439
00:51:58,240 --> 00:52:05,080
It's the same problem, except moved from the individual patient to, you know, environmental risk management.

440
00:52:07,740 --> 00:52:11,580
We can't actually know because I can't go back and figure out.

441
00:52:14,540 --> 00:52:18,470
Was this right or not? Because the statistic itself is a population estimate.

442
00:52:19,250 --> 00:52:23,160
But. Unfortunately.

443
00:52:24,330 --> 00:52:29,010
We tend to pay a lot more attention to the situations in which the bad thing is there.

444
00:52:29,250 --> 00:52:34,710
The cancer clusters. We pay very little attention to the random places where nobody gets cancer.

445
00:52:35,190 --> 00:52:39,630
Who everything's great. Well, no, it's the same problem on the opposite side, too.

446
00:52:42,360 --> 00:52:45,950
So it's this question I want to spend a lot of time on for the rest of the day.

447
00:52:45,960 --> 00:52:50,070
Like, how do we know when things are right or not?

448
00:52:52,140 --> 00:52:56,560
And. Into all kinds of different places.

449
00:52:58,510 --> 00:53:03,510
Elizabeth, you were talking about this, like thinking about. Will a treatment work for me?

450
00:53:03,530 --> 00:53:10,700
Kinds of questions. Which is a similar kind of question, like rather than the risk side, which is what you are facing, it's the positive side.

451
00:53:11,310 --> 00:53:17,160
That's the same problem. So how did this come up for you personally?

452
00:53:18,910 --> 00:53:27,940
This really came up for me with the situation of when new epilepsy treatments would come out and getting all positive about,

453
00:53:28,150 --> 00:53:32,709
Oh, maybe there's something that this time won't work for me since they're not positive.

454
00:53:32,710 --> 00:53:40,130
But then realizing yes, but also needing to consider diabetes, which is just another variable.

455
00:53:40,150 --> 00:53:48,700
And generally in research, it's not like you're going to see a whole study that was just for people with type one diabetes,

456
00:53:49,330 --> 00:53:55,090
even if it's a small enough sample size that it was people that were young, for example.

457
00:53:55,220 --> 00:53:58,600
Often times epilepsy is more common in younger individuals.

458
00:53:58,900 --> 00:54:04,719
There's still not enough people with Type one diabetes to make that one whole study based on that.

459
00:54:04,720 --> 00:54:11,380
And it was the idea of is it worth it to try something that might not work for me?

460
00:54:11,740 --> 00:54:17,559
Because there's always the possibility of having these really frustrating side effects from medications

461
00:54:17,560 --> 00:54:24,040
and the concept of if this study doesn't even really apply and maybe this medicine would have no effect,

462
00:54:24,460 --> 00:54:30,970
should I try? Or is it just not even worth it to? Because could I be the outlier that doesn't have anything happen that's positive?

463
00:54:31,390 --> 00:54:43,900
Yeah. And notice that your decision here is a function of how consequential it is if you try it and it doesn't work.

464
00:54:45,260 --> 00:54:50,540
So if this was a you had to choose a surgery. Once you choose the surgery, you couldn't have the other thing.

465
00:54:50,750 --> 00:54:54,950
If there was no possibility it's this or that. No ambiguity.

466
00:54:55,190 --> 00:55:01,220
It's a very different situation. Then if you could try something for a week and if it works, you'll know.

467
00:55:02,480 --> 00:55:06,860
And then if it doesn't work, you can get off of it right away and decide to do something else or whatever.

468
00:55:07,620 --> 00:55:12,470
Like so the permanence, the medical cost, the financial cost,

469
00:55:12,770 --> 00:55:17,479
all of those things enter into this tradeoff decision where we might be more willing

470
00:55:17,480 --> 00:55:23,330
to engage in uncertainty when we get to find out whether or not it's working or not.

471
00:55:24,080 --> 00:55:28,770
And we get to change it. And much less willing to deal with uncertainty.

472
00:55:30,060 --> 00:55:36,299
When those things are permanent, when we get to have our one hand, if I can use the poker analogy from the previous,

473
00:55:36,300 --> 00:55:39,540
because we get out one hand and then we either win or not and we're done.

474
00:55:40,800 --> 00:55:49,230
So I have my own personal example of this kind of question, which came from honestly one of the weirdest interactions I've ever had as a patient.

475
00:55:51,270 --> 00:55:55,680
It was right as I was about to start my chemotherapy.

476
00:55:57,300 --> 00:55:59,860
One of the most common problems of chemotherapy is nausea.

477
00:56:02,810 --> 00:56:06,440
There are lots of different applications that are sometimes used to help patients with nausea.

478
00:56:07,790 --> 00:56:13,730
I literally can picture this now in my brain sitting sitting at the table and the doctor comes in.

479
00:56:14,860 --> 00:56:17,860
And puts three different pill bottles in front of me.

480
00:56:18,730 --> 00:56:24,870
We. It says each of these are things that people that might help you with your nausea.

481
00:56:25,920 --> 00:56:35,640
Go try them. I have never before since had a moment where basically the doctor said, go play with meds.

482
00:56:37,110 --> 00:56:41,420
But that was basically what the doctor was enjoying. Why?

483
00:56:42,760 --> 00:56:44,229
Because in that context,

484
00:56:44,230 --> 00:56:50,770
one of the things that we know is that people respond very differently to different types of medications in terms of how well they manage nausea.

485
00:56:52,270 --> 00:56:58,810
And so because this is one of those, you can try it and you will know whether it's helping you or not.

486
00:56:58,990 --> 00:57:06,040
And you can change if it isn't working, it makes sense to try it because you can always make a difference.

487
00:57:06,970 --> 00:57:12,210
So that's what I did. And three medications were.

488
00:57:13,960 --> 00:57:19,210
In Benadryl and Lorazepam are at about.

489
00:57:22,110 --> 00:57:25,870
How busy? Did absolutely nothing for me. I thought I was taking sugar pills.

490
00:57:26,100 --> 00:57:29,190
I have no sense of benefit whatsoever.

491
00:57:29,820 --> 00:57:33,720
I know other people for whom it's great. They didn't do a damn thing for me.

492
00:57:34,440 --> 00:57:37,920
Benadryl did help the nausea and put me to sleep.

493
00:57:41,630 --> 00:57:49,610
And as a parable of my savior, I will tell another story about that medication later on in this course.

494
00:57:50,570 --> 00:57:57,820
But. This this how do we respond to uncertainty is a thing that comes up in a lot different situations and.

495
00:57:59,720 --> 00:58:04,130
You know, if you're talking about. I know other people who like.

496
00:58:06,770 --> 00:58:13,730
You're talking about. Knee replacement. You don't get to undo your knee replacement surgery.

497
00:58:15,180 --> 00:58:20,940
So it's a different conversation than if you have a treatment that you try something

498
00:58:20,940 --> 00:58:24,910
for a while on that and that by the way another domain in which this kind of.

499
00:58:26,410 --> 00:58:31,120
Average versus individual stuff plays out. Think about antidepressants.

500
00:58:32,250 --> 00:58:35,940
A how a lot of antidepressants that are currently in use. Why?

501
00:58:36,030 --> 00:58:39,150
In part because, again, we have the heterogeneity of response.

502
00:58:39,840 --> 00:58:49,110
So even if we might say which 82% on average is the most likely to give a beneficial response to a particular, say, a patient with.

503
00:58:50,150 --> 00:58:51,500
Mild to moderate depression.

504
00:58:53,520 --> 00:59:00,600
Beyond the ground experience is that some people get great responses from the thing that doesn't work for most people and vice versa.

505
00:59:01,660 --> 00:59:09,460
So you often see patients switching anti-depressants over time until they find one that works.

506
00:59:09,490 --> 00:59:14,610
I know several people who've had that experience. Same kinds of problems mapping.

507
00:59:15,110 --> 00:59:22,350
Not that the data are wrong. On average in the reference class of people with mild to moderate depression.

508
00:59:23,160 --> 00:59:29,500
This drug is working for more than this drug use. But that doesn't mean that it's the right drug for everybody.

509
00:59:33,340 --> 00:59:36,710
All right. What time is it?

510
00:59:38,080 --> 00:59:45,270
Really? I need to move. Another thing I wanted to bring up on this.

511
00:59:48,430 --> 00:59:51,870
We'll stay on this for now. You know.

512
00:59:54,410 --> 00:59:57,920
You were talking about a particular case that I remember was about.

513
00:59:59,620 --> 01:00:09,460
Partner. Stepfather? Yeah. My partner's step father was diagnosed with stage four pancreatic cancer, which has, like, a very bad prognosis.

514
01:00:10,150 --> 01:00:14,650
Average 2 to 5 months. And he wasn't doing well.

515
01:00:14,650 --> 01:00:24,310
So people think everyone was expecting him to, like, at least be on, like, the bottom 50%.

516
01:00:25,720 --> 01:00:32,170
And, um, and then he met with an intelligent oncologist was like very optimistic and it seemed like,

517
01:00:32,170 --> 01:00:36,040
I think, like to this day I don't understand why because he never gave it.

518
01:00:37,330 --> 01:00:42,340
He was maybe like translating some statistics to some optimism, but like what those were, I don't know.

519
01:00:42,340 --> 01:00:48,790
So something was kind of lost. And then by the time he started the treatment that I was really optimistic by,

520
01:00:48,790 --> 01:00:56,490
it had progressed so far that then the oncologist said, No, you have two or three days like you have to stop this treatment.

521
01:00:56,500 --> 01:01:03,329
It's like back and forth without the communication of the numbers that would be like translated into optimism or not.

522
01:01:03,330 --> 01:01:06,790
It was like very confusing and a very like disjointed experience.

523
01:01:07,250 --> 01:01:14,050
Yeah, prognosis is a space where medicine is particularly bad and being accurate.

524
01:01:15,580 --> 01:01:24,430
And I mean, if we think about it, prognosis itself is a application of the population to individual problem that is fundamentally problematic.

525
01:01:24,430 --> 01:01:29,830
Like you are a person, you will either be alive on a given day or you will not be alive on that given day.

526
01:01:30,160 --> 01:01:34,960
When we talk about prognosis or let's say we talk about treatment effect, like we say,

527
01:01:35,530 --> 01:01:45,850
this treatment gives end of stage kids, you know, stage four cancer patients on average, six more months to live.

528
01:01:46,450 --> 01:01:49,480
Those kinds of statements pop up and we talk about those kinds of treatments.

529
01:01:51,480 --> 01:01:57,150
It is not literally the case that taking a vacation is going to buy you six months is that there is a distribution.

530
01:01:57,630 --> 01:02:00,240
And when we look at that distribution, that shifted,

531
01:02:00,570 --> 01:02:06,180
but some people are still dying immediately and some people are still living a long time, regardless of whether they take it or not.

532
01:02:09,280 --> 01:02:16,930
I wanted to bring this up because your experience is having sort of like wildly varying changing messages about,

533
01:02:16,960 --> 01:02:20,290
okay, now we're optimistic this is going to be good. Now we're not so optimistic.

534
01:02:20,290 --> 01:02:24,160
Can be terrible. In the end, if I remember right, this sort of it ended up being very average.

535
01:02:27,600 --> 01:02:33,660
Is another piece of the puzzle here in the sense that it's probably a reference class problem

536
01:02:33,900 --> 01:02:38,220
in the sense for really took the full set of patients who have a particular condition.

537
01:02:39,000 --> 01:02:44,550
That average is actually quite reliable. It helps us to be less influenced by one particular test result.

538
01:02:46,850 --> 01:02:51,100
Or one particular piece of risk factor that's that's guiding us.

539
01:02:51,230 --> 01:02:56,300
The reason why we build models to predict risk, to predict prognosis, etc.,

540
01:02:57,170 --> 01:03:01,880
is in part because they are more reliable on average than human beings are.

541
01:03:03,590 --> 01:03:07,430
Human beings are often influenced by, Oh, this good news here and now all of a sudden.

542
01:03:08,210 --> 01:03:12,980
Of course this isn't going to happen to me. Or you get the bad news and it flips the other way.

543
01:03:14,400 --> 01:03:16,200
Whereas the truth is, on average,

544
01:03:16,200 --> 01:03:21,810
we don't take into account all of the different things that are predictive models that are associated with what's likely to happen to us.

545
01:03:24,820 --> 01:03:35,680
So I want to spend a couple minutes focusing you in on one table from the start in our article.

546
01:03:37,700 --> 01:03:44,029
Which talks about the concepts of absolute risk reduction and relative risk reduction.

547
01:03:44,030 --> 01:03:49,999
And I hope this is all clear to you guys, but it is definitely not clear to most people.

548
01:03:50,000 --> 01:03:55,040
So it is worth taking 5 minutes and let's make absolutely sure we are clear on what these things are.

549
01:03:58,070 --> 01:04:02,120
So I'm going to use an example just drawn from the table I have here.

550
01:04:02,120 --> 01:04:09,400
So. They had some outcome in this case, the handcuffs.

551
01:04:11,370 --> 01:04:14,240
And it was absolute great.

552
01:04:19,920 --> 01:04:27,420
Point to part something but really matter for the moment what the super something is because the same denominator through all of these statistics.

553
01:04:27,750 --> 01:04:33,360
So let's say this was 1000 people. 10.2 out of a thousand people that.

554
01:04:37,690 --> 01:04:43,080
But this is the controls. So that we had treatment group.

555
01:04:44,170 --> 01:04:48,200
You look at that treatment group and let's for the moment assume the denominator is the same.

556
01:04:48,210 --> 01:04:52,290
We got 10,000, 10,000 people. And these people.

557
01:04:55,450 --> 01:05:00,070
I'm trying to die. Now, what do we already know here? The treatment did something.

558
01:05:01,550 --> 01:05:05,500
The rate. Is less of a question.

559
01:05:05,810 --> 01:05:10,670
How much worse? How do y represent that? So low.

560
01:05:12,280 --> 01:05:16,800
Relative risk. Is the division.

561
01:05:19,180 --> 01:05:28,990
9.00 label Riesling. You divide it by 8.88.

562
01:05:34,700 --> 01:05:44,020
That represents a not to talk about this in terms of so how what does the reduction in proportion if one is equivalent?

563
01:05:46,730 --> 01:05:50,530
28 is the relative risk. In the treated group.

564
01:05:52,790 --> 01:05:56,590
When we talk about relative risk, we don't usually talk about it in terms of proportion terms.

565
01:05:56,600 --> 01:06:01,760
We usually use phrases like a 12% reduction.

566
01:06:04,170 --> 01:06:07,470
Well, I'm going to put a percentage point here.

567
01:06:09,440 --> 01:06:14,840
Even though it's incredibly confusing, because I don't mean that 12 out of 100 people are dying.

568
01:06:16,100 --> 01:06:21,950
What I mean is that compared to this baseline rate of 2.2.

569
01:06:26,620 --> 01:06:31,570
12% of the 10.2 are no longer dying.

570
01:06:34,890 --> 01:06:45,000
He said that again, out of the 10.2 who were dying in the control room, 12% of that 10.2 are no longer dying.

571
01:06:45,810 --> 01:06:51,000
We have changed the outcome for 12% of the people who previously would have been affected.

572
01:06:52,870 --> 01:07:01,500
For a much clearer way to represent this. Absolute risk reduction.

573
01:07:02,550 --> 01:07:05,880
So I kept using this denominator here. So let's say there's 10,000 people.

574
01:07:08,340 --> 01:07:13,560
1.2 of those 10,000 people is having their outcome changed.

575
01:07:15,300 --> 01:07:21,000
1.2 people. Has their outcome changed because of the treatment?

576
01:07:25,250 --> 01:07:37,190
Time. Dilute away. 1.2 dies only in the control group, and everybody else was fine either way.

577
01:07:38,660 --> 01:07:46,590
So we do this intervention, let's say, on 10,000 people, exactly 1.2 people out of them gets any better.

578
01:07:50,510 --> 01:07:59,150
We're going to talk about statistics in terms of cancer screening interventions on a lot of different type of things.

579
01:08:00,140 --> 01:08:04,280
One of the things you're going to see over and over again is that when we really talk

580
01:08:04,280 --> 01:08:08,360
about the absolute risk reduction from much of public health or much of medicine,

581
01:08:08,720 --> 01:08:15,220
the numbers get really small. The absolute risk reduction on mammography is a lot smaller than you think it is.

582
01:08:18,370 --> 01:08:26,110
I believe we should do it. We need to own. What is the likelihood that something is actually changing somebody's life that are being attacked?

583
01:08:27,490 --> 01:08:35,200
And I'm going to put all these things together for a moment. In the context of cardiovascular disease.

584
01:08:37,300 --> 01:08:44,380
And Stafford's stands are one of the most commonly prescribed medications to reduce level of cholesterol on your blood.

585
01:08:45,040 --> 01:08:50,050
High cholesterol is associated with all kinds of cardiovascular risks, strokes, heart attacks, etc., etc.

586
01:08:56,540 --> 01:09:04,080
Statins first came out. The huge population trials.

587
01:09:05,140 --> 01:09:11,620
Of people with cardiovascular risk factors randomized against statins.

588
01:09:12,010 --> 01:09:17,770
Some of them did look at heart outcomes like this or heart attacks or strokes.

589
01:09:19,840 --> 01:09:24,310
When they looked, they saw significant differences.

590
01:09:25,600 --> 01:09:28,320
The treatment recommendations came out from those trials to say.

591
01:09:29,450 --> 01:09:36,950
If you were in the population who was included in this trial, there was a benefit to you of being on a about.

592
01:09:39,160 --> 01:09:44,410
This population was people of many different ages. Some people had super high cholesterol levels.

593
01:09:44,420 --> 01:09:46,450
Some people had only moderately high cholesterol levels.

594
01:09:46,900 --> 01:09:51,970
Some people had other risk factors like high blood pressure or elevated weight, and others didn't.

595
01:09:53,490 --> 01:09:57,300
All would represent the heterogeneity of the population.

596
01:10:00,030 --> 01:10:03,030
And the statistics are right. I'm not minimizing. I'm not arguing the statistics.

597
01:10:03,030 --> 01:10:03,960
The statistics show.

598
01:10:04,230 --> 01:10:11,780
When we looked across the whole population, there was a significant reduction in the negative outcomes for the people who got prescribed status.

599
01:10:13,890 --> 01:10:19,410
And then a number of years later, somebody said, Wait a second. We know all these other factors matter to.

600
01:10:21,800 --> 01:10:26,620
So they did a risk stratification analysis and broke apart the population.

601
01:10:27,160 --> 01:10:31,780
The people who had lots of other risk factors versus fit. What did they find?

602
01:10:31,900 --> 01:10:36,040
Same data. Not. Not a new study. A reanalysis of the original data.

603
01:10:37,710 --> 01:10:45,450
When you changed the reference class to be the high risk people, only people had lots of these other risk factors.

604
01:10:46,410 --> 01:10:50,580
The benefit of steroids was many times larger than the average across the population.

605
01:10:50,940 --> 01:10:59,430
Those people got a lot of benefit. If you look at people didn't have those other risk factors, there was no benefit, no significant effect.

606
01:11:01,450 --> 01:11:09,700
And yet all of the people who met that original study definition had been recommended to start studies.

607
01:11:10,720 --> 01:11:15,700
We as a society had been paying for them to be. They had been living with the side effects of statins, etc.

608
01:11:17,620 --> 01:11:22,180
So when we redefined the reference class for that group.

609
01:11:24,210 --> 01:11:29,690
We sometimes expose the reality that there was one group who really, really was going to benefit a lot.

610
01:11:30,080 --> 01:11:35,780
No problems prescribing stuff for them. And another group who honestly had gotten more harm than benefit.

611
01:11:39,630 --> 01:11:42,540
And the core of the problem was defining the reference class.

612
01:11:46,820 --> 01:11:53,630
That's why we're spending so much time talking about reference class here, because when you do these calculations, everything depends upon.

613
01:11:54,910 --> 01:11:59,620
Which are drugs classes. If you define it to the people who are most likely to be affected,

614
01:11:59,980 --> 01:12:06,340
you're much more likely to erase vivid effects, and it only applies to that if you define it really broadly.

615
01:12:07,550 --> 01:12:14,180
You may get small effects, but that effect is not necessarily likely to be the same across a population.

616
01:12:14,180 --> 01:12:19,280
And we get stuck with the same thing that Elizabeth and many others here are talking about, like, is this going to apply to me?

617
01:12:19,280 --> 01:12:21,950
And I'm going to fit what these data are saying.

618
01:12:24,570 --> 01:12:31,560
No simple answer to this, but it comes up over and over again, especially as we in public health, we tend to think of the population level.

619
01:12:33,140 --> 01:12:36,860
We tend to take the big data and say, does this work or not?

620
01:12:40,870 --> 01:12:46,049
But the trust that we ought to have in our interventions depends a lot on whether or not we

621
01:12:46,050 --> 01:12:50,610
think this is going to apply equally to everybody versus whether there are underlying variants,

622
01:12:50,730 --> 01:12:56,969
whether it could be location, transit, floods, there could be genetic characteristics that could be body types,

623
01:12:56,970 --> 01:12:58,830
that could be all these different kinds of things.

624
01:12:59,980 --> 01:13:05,400
They're going to keep coming back to this question of does it apply to me over and over and over again?

625
01:13:06,370 --> 01:13:11,680
Now, the last thing I want to say before I'm short on time, but I want to take 2 minutes on this AMA.

626
01:13:13,230 --> 01:13:18,980
You brought up who was your partner in fear of flying and that you've talked about a lot.

627
01:13:18,990 --> 01:13:22,020
But there's one piece that I particularly want to make sure that we touch upon today,

628
01:13:22,680 --> 01:13:27,600
which is when your partner sort of talked about, well, wait a second, I know that this is unlikely, but what if I'm the one?

629
01:13:27,850 --> 01:13:34,680
Yeah, yeah. We went on a vacation a few months ago, which involve flying, and we didn't realize you're flying until we were on the plane.

630
01:13:36,360 --> 01:13:43,430
Once we landed, I was like. I was like, well, I knew the statistics of being harmed in any way in a flight are really low.

631
01:13:43,440 --> 01:13:49,770
So I looked up and offered the statistics and framed it like a one in 1.2 million chance of your plane

632
01:13:50,310 --> 01:13:56,190
crashing and 1.1 in like 12 million chance of like dying in a plane crash or something like that.

633
01:13:56,550 --> 01:13:58,920
And one thing like that's like we talked about before,

634
01:13:58,920 --> 01:14:05,490
like my partner only heard like I could be got one rather than this is so unlikely that I should really not worry about it.

635
01:14:06,690 --> 01:14:10,709
Yeah. I was like, and I want to bring this up because that phrasing,

636
01:14:10,710 --> 01:14:15,540
what if I'm the one who has pops will pop up multiple times when you talk to people about risk.

637
01:14:17,040 --> 01:14:26,459
It's it's, again, this possibility versus probability, but it acknowledges that the risk exists and doesn't engage in the and I should

638
01:14:26,460 --> 01:14:30,690
feel better because it's so unlikely engages as a precautionary principle.

639
01:14:30,690 --> 01:14:34,769
I think like it's like I still want planes to be safe. Even though it's really rare.

640
01:14:34,770 --> 01:14:42,240
I feel like we might choose to engage in some degree of protective behavior, even if it's a really unlikely event.

641
01:14:45,080 --> 01:14:49,730
So today is a kind of complicated day and there's no clean takeaways from it.

642
01:14:50,000 --> 01:14:55,460
This is, again, sort of setting up all of the complexities of this stuff as we dove in over the course of the next week or two.

643
01:14:55,490 --> 01:15:02,900
Okay. So here's your number to you. I want you to be thinking of all this nuance and complexity as we're engaging in that stuff.

644
01:15:04,100 --> 01:15:11,120
Take the last couple of minutes class and just turn your neighbor and ask this question, like, where is this the problem?

645
01:15:11,270 --> 01:15:15,770
Where is this problem? That's going to be really silly. What are situations?

646
01:15:16,310 --> 01:15:19,460
Examples of health risk numbers. Where.

647
01:15:20,530 --> 01:15:26,540
The reference class is often unclear where you know the number, but you don't necessarily know what it's referring to.

648
01:15:27,190 --> 01:15:30,190
Let's just sort of remind ourselves of how common this problem is.

649
01:15:30,190 --> 01:15:37,660
It's okay. Take a couple minutes. I got it.

650
01:15:41,740 --> 01:15:48,580
You know, I'm not waiting to tell you.

651
01:15:49,780 --> 01:15:56,640
It's like I I'm not thinking about, you know,

652
01:15:57,010 --> 01:16:20,410
I think you might be sitting here thinking about the whole thing is really what they're doing differently.

653
01:16:22,120 --> 01:16:30,370
I think that would be inflation right now anyway.

654
01:16:33,640 --> 01:16:38,160
So how do you do that?

655
01:16:39,070 --> 01:16:53,170
Because the other side of it to have obviously 2021 with the different.

656
01:16:53,170 --> 01:17:17,430
So what is the dichotomy to my boyfriend is every day because I mean there's not one extreme but I tell you something I'm going to go out there,

657
01:17:17,530 --> 01:17:30,290
like I say this week, I mean, was just like, well, yeah, something like that.

658
01:17:31,900 --> 01:17:42,459
And it's sort of like scheduled my graduation.

659
01:17:42,460 --> 01:17:45,460
Yeah, it's like, I like you. I only know that because I literally.

660
01:17:48,670 --> 01:17:53,950
That's my only family here.

661
01:17:58,450 --> 01:18:15,219
You're supposed to go rather y yeah. Because it would seem like, you know, it doesn't like to say that you're just like, yeah, exactly.

662
01:18:15,220 --> 01:18:24,070
I mean, that's important because it was ever so you're more like this one.

663
01:18:25,030 --> 01:18:28,140
I mean, really for you though.

664
01:18:28,450 --> 01:18:35,560
So like I can directly attribute statistically specific events to climate change.

665
01:18:36,010 --> 01:18:57,489
So like keep an eye on things like climate change, software systems that, you know, it's like, it's like, you know,

666
01:18:57,490 --> 01:19:40,520
for things like this class I really like students started talking about everything all the time was like, I don't know how it works.

667
01:19:40,640 --> 01:19:50,900
Like a very I was like, talk about the reference and like the reference classroom.

668
01:19:51,590 --> 01:20:01,900
Like a lot of this question for next week's class,

669
01:20:03,190 --> 01:20:11,360
next class about labels in which we attach labels for the meanings that come with those good Advair.

670
01:20:12,190 --> 01:20:17,380
But as we start reading about labels in the back of your mind, this question of reference class.

671
01:20:20,380 --> 01:20:24,100
Rainbows were always feel a click on request.

672
01:20:24,100 --> 01:20:33,610
I want you to be thinking about it when World War when really using a label potentially more problems because it steps us away from thinking about

673
01:20:33,610 --> 01:20:44,350
does this number of is this risk in this class is relevant for me or not that's setting up what you're going to be reading about for next class.

674
01:20:44,740 --> 01:20:48,520
And if anything comes up, I should get the same exact you guys.

675
01:20:48,520 --> 01:20:53,260
This is the same. So if you want to follow up, please reach out to me.

676
01:20:54,070 --> 01:20:57,340
This is really foundational stuff, so if you're not confident of it,

677
01:20:57,340 --> 01:21:01,420
I very much like to take the time and help you through it because it'll help you all the way.

678
01:21:03,610 --> 01:21:19,830
All right. Yeah, yeah, yeah, yeah, yeah, yeah, yeah.

679
01:21:20,350 --> 01:21:24,890
I think biggest.

680
01:21:24,940 --> 01:21:30,460
Really? Tired.

681
01:21:31,460 --> 01:21:36,180
Okay. Sounds great.

682
01:21:39,190 --> 01:21:42,530
I don't know, cause I feel like.

683
01:21:46,720 --> 01:21:53,890
I, like, you know, like, I don't want to make it out.

684
01:21:55,370 --> 01:22:01,380
I have a dream about, you know, things like that, you know?

685
01:22:01,790 --> 01:22:09,050
And it's like there's never been a classroom.

686
01:22:11,810 --> 01:22:22,430
And can you look, I get somebody like I was like I was like, oh, did you hear that noise?

687
01:22:23,150 --> 01:22:29,870
I was like, Yeah. And I am well aware of the fact that I have a high frequency, so I did not hear again.

688
01:22:30,800 --> 01:22:37,670
I remember when I was younger and I could hear whether somebody had a TV on and the decades since.

689
01:22:37,670 --> 01:22:41,160
I could hear that. Yeah. All right.

690
01:22:41,180 --> 01:22:47,680
Thank you so much. And sorry for your reports in.

691
01:22:53,460 --> 01:22:58,500
Everyone I talk to question commentary about why I've been.

692
01:23:07,440 --> 01:23:12,920
But yes, you know, submitted today, I'm pretty much done with it.

693
01:23:13,050 --> 01:23:17,950
I just said, okay, I will take a look at it.

694
01:23:18,000 --> 01:23:21,510
I think I saw the submitted. So I have to. I don't think I've got to go.

695
01:23:22,920 --> 01:23:31,290
But thank you. Thank you. Yes.

696
01:23:32,970 --> 01:23:41,360
I think it's hurting you more. That's hurting. This is the problem with these early warning classes.

697
01:23:42,820 --> 01:23:48,280
I have after teaching them for many years, I have just gotten used to the reality that there is.

698
01:23:50,270 --> 01:23:54,650
There are people who are here always early because of whatever is their life.

699
01:23:54,690 --> 01:23:59,770
They're there. They're here. There is the people who are always late because they run late.

700
01:24:00,190 --> 01:24:09,670
And then there is the random drift sprinkling of people who appear at random moments because of whether it's traffic.

701
01:24:10,310 --> 01:24:17,139
I have had my own moments where like, I'm usually here a half an hour ahead just because of the flow in terms of when I'm leaving the house,

702
01:24:17,140 --> 01:24:26,440
etc. I have had two or three days in which like everybody is in the room and I like open the door, I'm coming.

703
01:24:28,080 --> 01:24:35,190
Has Yeah. Got stuck on the highway for 5 minutes, couldn't move and couldn't do any of it.

704
01:24:35,620 --> 01:24:43,480
So be it. Um. Anyway, uh, yeah, I like.

705
01:24:43,530 --> 01:24:47,500
I'm not sanguine about that. At least you remember now.

706
01:24:47,880 --> 01:24:52,590
I know. I like to look away. I always have to remember, right?

707
01:24:57,530 --> 01:24:57,770
You.

