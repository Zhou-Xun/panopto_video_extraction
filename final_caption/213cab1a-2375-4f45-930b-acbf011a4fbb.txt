1
00:00:10,710 --> 00:00:17,280
Hi, everyone, thanks for joining me here to finish up the lecture on research ethics.

2
00:00:17,280 --> 00:00:22,560
I'll try to be try to be quick and hit on a couple of points that I didn't quite get to during lecture.

3
00:00:22,560 --> 00:00:33,690
So as a quick reminder, I introduced the concept of the Belmont report, which was released in response primarily to the Tuskegee syphilis experiments.

4
00:00:33,690 --> 00:00:39,240
It introduced three basic principles that were important for performing ethical research.

5
00:00:39,240 --> 00:00:44,920
Those principles were respect for persons, then deficient injustice.

6
00:00:44,920 --> 00:00:52,900
We talked about how these three ethical principles then allowed us to view

7
00:00:52,900 --> 00:00:58,780
research that that we inherently feel is is not correct or is wrong is immoral.

8
00:00:58,780 --> 00:01:03,220
And now can actually pinpoint the ways in which those research projects were bad.

9
00:01:03,220 --> 00:01:06,610
Right. So we did this for the Tuskegee syphilis study.

10
00:01:06,610 --> 00:01:20,800
We also did this with the more recent example of the scientists that created a genetically modified human embryos to try to create HIV resistance.

11
00:01:20,800 --> 00:01:29,460
So we use those principles. We also talked about how academic misconduct can be problematic as well.

12
00:01:29,460 --> 00:01:35,250
Fabrication, data, falsification and plagiarism. And we gave examples of here.

13
00:01:35,250 --> 00:01:39,390
We talked about the the clinical trials, the cancer clinical trials at Duke.

14
00:01:39,390 --> 00:01:45,720
That were based on academic fraud. And we talked about, you know, for example, that the example of Theranos, right.

15
00:01:45,720 --> 00:01:54,410
Where an entire company was built up around academic misconduct, falsifying data.

16
00:01:54,410 --> 00:01:58,730
So we also discussed the concept of ethical gray zones in research, right?

17
00:01:58,730 --> 00:02:04,060
The examples that we had previously mentioned were clear ethical violations.

18
00:02:04,060 --> 00:02:08,740
But we discussed that in reality. It's not always clear what's right and what's wrong.

19
00:02:08,740 --> 00:02:15,790
And so I gave two examples here of clinical trials in developing nations and then also incidental or secondary research findings.

20
00:02:15,790 --> 00:02:20,830
And when you apply those those three ethical principles that were laid out in the Belmont report.

21
00:02:20,830 --> 00:02:31,510
Valid arguments can be made for and against how how each one of those ethical principles is being upheld within the research.

22
00:02:31,510 --> 00:02:35,920
So it's clear that, you know, research is not always clear cut, right, that that it's not.

23
00:02:35,920 --> 00:02:39,670
That's not always right or always wrong. These gray zones exist.

24
00:02:39,670 --> 00:02:44,380
They exist more often than we often believe they do.

25
00:02:44,380 --> 00:02:49,270
And so how then can we determine if a research project should go forward?

26
00:02:49,270 --> 00:02:55,330
We clearly can't rely on just the primary investigator deciding.

27
00:02:55,330 --> 00:03:01,260
Right. As we've seen in these cases, in that situation, that's where you can often go off the rails, right.

28
00:03:01,260 --> 00:03:07,180
That somebody is not doing it. Somebody could still do research, even though it's a violation,

29
00:03:07,180 --> 00:03:15,340
because they've rationalized in their mind that it's OK or they can be be tempted by having to have important results.

30
00:03:15,340 --> 00:03:21,070
So here I want introduce the concept of institutional review boards, which which you may or may not have heard of I Arby's.

31
00:03:21,070 --> 00:03:27,320
So these are entities that exist at places that do research, be it companies or universities.

32
00:03:27,320 --> 00:03:30,960
They're responsible for reviewing proposed research projects.

33
00:03:30,960 --> 00:03:35,920
So this is before a project gets done. It's a proposal of what would be done.

34
00:03:35,920 --> 00:03:44,110
And their job is not to judge necessarily the scientific accuracy or the know how important the research is, whether the research should be funded.

35
00:03:44,110 --> 00:03:50,760
Their job is to protect the rights and the welfare of any human subjects that would participate in it.

36
00:03:50,760 --> 00:03:57,130
So importantly, it's an independent committee, right. So these are these are not people that are tied to the research project itself.

37
00:03:57,130 --> 00:04:06,520
And they also, you know, they should be not have any financial or or friendships or anything with the people involved.

38
00:04:06,520 --> 00:04:11,200
Right. Because that would bias their opinions, potentially at least five members,

39
00:04:11,200 --> 00:04:15,550
primarily faculty people that are familiar with with doing research and understand

40
00:04:15,550 --> 00:04:21,190
these concepts of research ethics from a range of different academic disciplines.

41
00:04:21,190 --> 00:04:24,070
At least one person not being affiliated with the institution.

42
00:04:24,070 --> 00:04:29,770
I think this is important, right, because you have to remember that the institution beat you avam or beat a company,

43
00:04:29,770 --> 00:04:36,400
you know, stands to gain by doing the research financially. Reputation stands to gain by doing the work.

44
00:04:36,400 --> 00:04:42,420
And so having somebody who is not a part of that institution being involved is important.

45
00:04:42,420 --> 00:04:47,010
Having diversity in terms of race, gender, cultural backgrounds, very important as well.

46
00:04:47,010 --> 00:04:51,450
Why? Well, because as we've seen in some of these other examples,

47
00:04:51,450 --> 00:05:01,620
it's one of the ways that you can go off the rails is you can exploit certain populations, populations that are already having challenges.

48
00:05:01,620 --> 00:05:06,450
Right. And so you can get them involved in research more easily.

49
00:05:06,450 --> 00:05:10,140
They can be exploited. They can be coerced into being involved. Right.

50
00:05:10,140 --> 00:05:19,910
So having representatives that can say can identify how these the plan might be problematic in that regards is important.

51
00:05:19,910 --> 00:05:27,730
So here you have the goals of specifically you have Ms. IRP. And so I have a link there if you're if you're more interested.

52
00:05:27,730 --> 00:05:33,400
But to review and oversee research to ensure that it meets ethical principles and complies with federal regulations,

53
00:05:33,400 --> 00:05:40,060
state laws and university policies, right. So that people need to be very up to date on what's expected.

54
00:05:40,060 --> 00:05:47,170
And this is also the important part to that, that assisting researchers in the design and conduct of sound research in support of U.

55
00:05:47,170 --> 00:05:51,040
Of M's mission to develop and disseminate new knowledge in the public interest.

56
00:05:51,040 --> 00:05:58,390
So part of what the ISB does is they're going to look at the application and they are going to identify potential ethical considerations and harms.

57
00:05:58,390 --> 00:06:07,180
Right. And they can they can respond or reply with those to the actual investigator and try to help them come up with ways of of

58
00:06:07,180 --> 00:06:17,100
first acknowledging and recognizing that that these harms might exist and then coming up with ways to get around them.

59
00:06:17,100 --> 00:06:27,040
So admittedly, a hum. You know, IRA, bees are often viewed as sort of being a very cumbersome or overbearing part of the research process.

60
00:06:27,040 --> 00:06:34,390
But they've got a difficult job to do. And so I grabbed this this picture here of a very, very basic question.

61
00:06:34,390 --> 00:06:40,060
Is an activity research involving human subjects? You would think this would be a simple yes or no.

62
00:06:40,060 --> 00:06:46,330
But what you can see is this this flowchart that sort of works its way through to try to figure out,

63
00:06:46,330 --> 00:06:51,310
you know, it does it does a research study actually involve human subjects.

64
00:06:51,310 --> 00:06:52,300
So it's not always clean. Right.

65
00:06:52,300 --> 00:07:02,170
These things are built up over time as people considered more and more the potential dilemmas that can go into creating research projects.

66
00:07:02,170 --> 00:07:10,900
But these are all, again, built in with the concept that the IRP is trying to look at respect for persons benefits

67
00:07:10,900 --> 00:07:22,140
since injustice and that the that these things are being met for the subject participants.

68
00:07:22,140 --> 00:07:28,410
So little thing or change up here to end up the discussion of ethics.

69
00:07:28,410 --> 00:07:35,150
And specifically because we're in a class about data. You know, we're having something going on right now.

70
00:07:35,150 --> 00:07:39,150
This this idea of big data and how that can play into ethics.

71
00:07:39,150 --> 00:07:46,200
And so, you know, what exactly does big data mean? You know, if you if you ask different people, you'll get different answers.

72
00:07:46,200 --> 00:07:50,940
But regardless, there's a concern about, you know, what the uses of of big data are.

73
00:07:50,940 --> 00:08:01,830
And so, you know, you can define big data as large, complex, versatile sets of data are constantly changing because new ones are coming up.

74
00:08:01,830 --> 00:08:10,470
You know, the data collection can be, you know, through Internet access, mobile devices, social media.

75
00:08:10,470 --> 00:08:15,690
And then I think more commonly, we think of things like electronic health records or that are they're capturing information.

76
00:08:15,690 --> 00:08:21,510
So it's vast amounts of information that are being captured on people.

77
00:08:21,510 --> 00:08:27,660
And the key with big data in terms of of of sorts of research is that it can be repurposed, right?

78
00:08:27,660 --> 00:08:32,070
It can be repurposed for for research purposes.

79
00:08:32,070 --> 00:08:37,500
But there are other reasons that people might repurpose it that aren't quite so noble.

80
00:08:37,500 --> 00:08:44,280
I love this picture that I found it in a jam article that sort of shows the complexity of this this big data idea.

81
00:08:44,280 --> 00:08:49,080
You know, you can see so much, you know, in this picture that's built into it.

82
00:08:49,080 --> 00:08:54,840
Right. So I often operate in this world right here of of electronic medical records that that

83
00:08:54,840 --> 00:09:00,310
can tell you things about a person's health history and also maybe their demographics,

84
00:09:00,310 --> 00:09:04,920
you know, and then, you know, there are other places where information can be captured.

85
00:09:04,920 --> 00:09:11,710
Things like genetics like 23 and me or ancestry.com. There's also repositories where people will upload data they've gotten back.

86
00:09:11,710 --> 00:09:17,430
I mean, you've potentially heard in the news about how the Golden State killer was captured, right.

87
00:09:17,430 --> 00:09:23,280
By looking through these these repositories of genetic information people had had posted.

88
00:09:23,280 --> 00:09:28,110
And it wasn't that the actual goldens, a killer, had put his DNA up there, but a relative of his had.

89
00:09:28,110 --> 00:09:33,750
Right. And so, again, I don't think that's the reason that people posted it there.

90
00:09:33,750 --> 00:09:36,360
But this is a repurposing of this big data. Right?

91
00:09:36,360 --> 00:09:43,640
You can even see, you know, things like LinkedIn and Zillow saying something about your socioeconomic,

92
00:09:43,640 --> 00:09:47,530
you know, coming over here and like, what's your social media history looks like?

93
00:09:47,530 --> 00:09:57,300
All right. So there's a lot of information being captured that this sort of ideal or idealized use of this for research is that,

94
00:09:57,300 --> 00:10:02,930
you know, once you have this big data, it allows you to really look at people on a granular level.

95
00:10:02,930 --> 00:10:03,210
Right.

96
00:10:03,210 --> 00:10:12,420
If you're trying to do predictions of of outcomes and risk in and trying to figure out what's the best potential therapeutic strategy for somebody,

97
00:10:12,420 --> 00:10:16,890
the more granular you can get, potentially, the more accurate the predictions can be.

98
00:10:16,890 --> 00:10:22,290
And you can see, you know, once once you start thinking about how granular this data can become by a granular I mean,

99
00:10:22,290 --> 00:10:30,570
just just lots of different variables being measured on people across a range of different topics.

100
00:10:30,570 --> 00:10:33,900
You know what the question is, has come up like, you know, how much is too much, right?

101
00:10:33,900 --> 00:10:39,730
Once you have all this data put in the same place. Right. And it's been collected.

102
00:10:39,730 --> 00:10:42,900
Yeah. It can be put towards good research use. But we have to be careful. Right.

103
00:10:42,900 --> 00:10:50,970
There is the potential for it to be misused and there are unintended consequences for tracking so much data on people.

104
00:10:50,970 --> 00:10:55,660
And so, you know, here I just you know, here's that. Here's a Web site that that had examples that I found.

105
00:10:55,660 --> 00:11:01,410
And, you know, for example. Right. You know, in something like Uber, you know,

106
00:11:01,410 --> 00:11:10,500
there were instances of of Uber employees being able to basically harass or track individuals

107
00:11:10,500 --> 00:11:15,330
because there was real time and historical tracking of of what people's trips were.

108
00:11:15,330 --> 00:11:25,350
Right. There were instances of individuals in police departments who were looking up law enforcement databases for personal purposes.

109
00:11:25,350 --> 00:11:31,980
Right. So, you know, trying to find out information on somebody that they encounter in their personal

110
00:11:31,980 --> 00:11:35,260
life or somebody that they're a friend of theirs encounters in their personal life.

111
00:11:35,260 --> 00:11:39,870
Right. Not in not the intended use of having that information stored.

112
00:11:39,870 --> 00:11:44,670
And also, you know, at places like AT&T or Morgan Stanley, you know,

113
00:11:44,670 --> 00:11:55,470
there were some pretty high profile cases and examples that showed up where employees there sold Social Security numbers and information,

114
00:11:55,470 --> 00:11:58,740
customer information to third parties to personally benefit. Right.

115
00:11:58,740 --> 00:12:07,500
So certainly not the intended use. And then this was sort of not not not funny for for Jane Slater, but it shows some of the unintended consequences.

116
00:12:07,500 --> 00:12:14,490
Right. So Fitbit tracks people's heart rate and gives some information on exercise.

117
00:12:14,490 --> 00:12:21,900
And I will I will let you read about how the the unintended consequences of of her her boyfriend wearing a

118
00:12:21,900 --> 00:12:30,360
Fitbit and then and then being tracked and having unaccounted for physical activity spiking at 4:00 a.m.

119
00:12:30,360 --> 00:12:35,370
So needless to say, right. There is high potential with big data in terms of research.

120
00:12:35,370 --> 00:12:40,260
But we have to consider the ethical consequences right off of once this data is collected.

121
00:12:40,260 --> 00:12:46,120
It's out there and people can potentially use it in ways that we're not intending.

122
00:12:46,120 --> 00:12:51,310
So to summarize, you know, you could take an entire course, right, on ethics,

123
00:12:51,310 --> 00:12:55,330
you could you could major in ethics, in research, and you could be a bioethicist.

124
00:12:55,330 --> 00:12:58,570
You know, I've had one lecture here to talk to you a little bit about it.

125
00:12:58,570 --> 00:13:03,490
And hopefully some of it was new because I know you've learned about ethics before,

126
00:13:03,490 --> 00:13:08,650
but it's something that's important to consider and something important to consider before research even begins.

127
00:13:08,650 --> 00:13:11,200
We have the Belmont report,

128
00:13:11,200 --> 00:13:22,220
which is which is critical because it provided some criteria that we can now pinpoint specific violations of ethical research.

129
00:13:22,220 --> 00:13:26,840
And so, you know, we think about Tuskegee, we think about that.

130
00:13:26,840 --> 00:13:32,600
The fact that the gene editing as an ideal, ideal or not ideal,

131
00:13:32,600 --> 00:13:38,690
notorious examples of of research misconduct, but misconduct can also be academic misconduct.

132
00:13:38,690 --> 00:13:42,920
Right. And that's that's potentially even the more common one at this at this point.

133
00:13:42,920 --> 00:13:48,110
And there are there are personal public health and policy implications to scientific misconduct.

134
00:13:48,110 --> 00:13:51,380
And that's the reason that we want to avoid it.

135
00:13:51,380 --> 00:14:00,829
And just understanding that this this new world of big data that we live in presents some new challenges for scientific ethics.

