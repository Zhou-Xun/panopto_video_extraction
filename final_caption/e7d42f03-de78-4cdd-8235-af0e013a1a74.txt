1
00:00:06,570 --> 00:00:17,310
Good afternoon, everybody. Thanks for patience. Today, we are going to dove into a week of technically more challenging lectures.

2
00:00:17,670 --> 00:00:20,760
These are about maximum likelihood and restricted maximum likelihood.

3
00:00:21,420 --> 00:00:28,870
But before I do this, I just want to say that if you have not registered on the plasma platform, please do that.

4
00:00:28,890 --> 00:00:39,000
I currently see there are about like 33, 35 people who have registered and I think there are about 50, 40 some people in this class.

5
00:00:39,000 --> 00:00:44,400
So I encourage those ones who have not registered to register because we will be answering a lot of questions through that platform.

6
00:00:49,190 --> 00:00:52,520
And also, are there any burning questions I should answer right now?

7
00:00:53,030 --> 00:00:58,890
Yes. I'm sorry.

8
00:01:01,880 --> 00:01:05,660
Oh, I did not unlock it. Okay, that's.

9
00:01:07,160 --> 00:01:11,700
Good point. Oh, that's right.

10
00:01:13,760 --> 00:01:17,490
Yeah. You guys are too kind. You guys should just drop me an email about this.

11
00:01:18,360 --> 00:01:23,520
All right? I apologize for that. And hopefully you can see that now. And I'll give you, like, one minute download the site.

12
00:02:45,640 --> 00:02:49,440
All, everybody has a size. Okay.

13
00:02:49,480 --> 00:02:52,690
Thanks for the suggestion. So why don't we get started?

14
00:02:54,940 --> 00:02:59,170
In the past few lectures, we have been setting up the stage for this one.

15
00:03:00,340 --> 00:03:09,580
Arguably, this is one of the most important lectures in this class because it will be introducing the notion of generalized squares.

16
00:03:10,390 --> 00:03:15,710
Clearly, it's generalizing what you've learned in 650, and that's called ordinarily squares, right?

17
00:03:15,730 --> 00:03:21,430
So we're going to have some extraordinary squares. We don't call them that, but it's called generalized squares.

18
00:03:22,060 --> 00:03:28,780
The second thing you will learn today at a relatively higher level is called the restricted maximum likelihood.

19
00:03:29,080 --> 00:03:34,240
You know the word a maximum Likert. And we will be trying to introduce the meaning of restricted maximum likelihood.

20
00:03:34,810 --> 00:03:39,550
This technique is going to be applicable not only for longitudinal analyzes

21
00:03:40,030 --> 00:03:44,380
but broadly for like semi parametric and non-pro measure smoothing modeling.

22
00:03:45,940 --> 00:03:50,170
I will not cover those topics, but that's a reason for us to introduce this early on.

23
00:03:51,570 --> 00:04:00,270
And I want to say that our starting point is the general media model where we have a learn the notations and here

24
00:04:00,270 --> 00:04:06,210
the distinction between the EML and read more will be about how do we estimate the various covariance parameters?

25
00:04:07,320 --> 00:04:11,340
It will spill over into how do we estimate the betas.

26
00:04:11,550 --> 00:04:17,880
But we will make that point clear. So before we start, I will still like to introduce the learning objectives.

27
00:04:18,540 --> 00:04:25,139
Again, these slides are trying to give you a direction or self-evaluation.

28
00:04:25,140 --> 00:04:32,580
You know, I have I got these points. So. So there are four objectives which will be introducing three parts today.

29
00:04:33,570 --> 00:04:41,580
Part one. Part two in part three. So the first objective is that let's derive the maximum Likert estimate for your model with correlated errors.

30
00:04:41,940 --> 00:04:47,790
So maximum likelihood you should not be new to you media model, you should not be new to you.

31
00:04:49,290 --> 00:04:57,030
But the new thing is the correlate errors. And in the context of the longitudinal data notation, number two.

32
00:04:58,660 --> 00:05:04,570
The objective is to describe the need why a variant of maximal effort is needed.

33
00:05:05,800 --> 00:05:13,030
And that is going to be made clear in terms of reducing the bias of estimating the variance governance parameters.

34
00:05:14,110 --> 00:05:18,909
Number three is trying to this you need to be able to describe the objective

35
00:05:18,910 --> 00:05:24,280
function that this criteria that this procedure Remo is trying to optimize.

36
00:05:25,060 --> 00:05:28,990
This is a clearly a more technical point, but I think it's only through a technical point.

37
00:05:28,990 --> 00:05:35,620
You can understand the differences. And number four, we will be presenting using our code.

38
00:05:35,620 --> 00:05:37,779
And can you carry those our code, you can use data,

39
00:05:37,780 --> 00:05:45,190
you can run them is trying to give you an example that hey eml and Remo estimate errors actually can be different.

40
00:05:45,280 --> 00:05:50,590
Okay. So those are the four objectives. So why don't we start with part number one?

41
00:05:51,430 --> 00:05:59,650
It is trying to derive the maximum Likert estimation for neonate models with independent or dependent observations.

42
00:05:59,950 --> 00:06:05,380
So look, we will be taking a two step approach. We will be deriving something that should be very familiar to you.

43
00:06:05,650 --> 00:06:10,690
But just under the newly introduced notation with the two double indices like I and J.

44
00:06:11,080 --> 00:06:15,040
So we will be starting with independent ones and then we'll move on to the dependent ones.

45
00:06:15,310 --> 00:06:23,380
Hopefully this can build a bridge for you to understand that all the things we talk about here falls into the general umbrella of EML.

46
00:06:32,520 --> 00:06:40,710
So first let's recall, what's the model we are studying? So the model is called in the model and usually the specified by two parts.

47
00:06:40,920 --> 00:06:47,400
The first part is what we call the mean model. We model that for the vector responses.

48
00:06:48,430 --> 00:06:56,440
On the subject. I given all the information we have on the subject, it can be modeled via a simple new near term called exit beta.

49
00:06:57,190 --> 00:07:00,130
If you need some help in terms of compensation, I can do that now,

50
00:07:00,400 --> 00:07:08,950
which is to basically write out why I which is why I won that other, why I comma and I here.

51
00:07:09,400 --> 00:07:15,610
So this is basically how we put all the response measurements in the single column vector.

52
00:07:15,670 --> 00:07:18,670
Right. And S.I. here, it's a matrix.

53
00:07:18,910 --> 00:07:24,230
It should be a dimension. And I by. And I'm happy.

54
00:07:24,320 --> 00:07:28,550
Right? So each row should be an occasion.

55
00:07:32,140 --> 00:07:39,330
And each column should be one covered. All right.

56
00:07:39,430 --> 00:07:42,820
So this is an antibody and beta naturally.

57
00:07:43,390 --> 00:07:47,170
It has to be compatible with EXI. So it should be P by one.

58
00:07:47,170 --> 00:08:01,220
Right. And this is how we model the main. Number two, we will need to describe how the residuals or how the outcomes are correlated within the person.

59
00:08:01,730 --> 00:08:05,420
So this is card is that this is described by the covariance matrix.

60
00:08:05,870 --> 00:08:10,280
So what this says is that given all the information we have about converts,

61
00:08:10,550 --> 00:08:16,879
we may still want to acknowledge that there may be something called variation among pairs of

62
00:08:16,880 --> 00:08:22,490
measurements of the outcomes when that person and this is neatly described by the covariance matrix.

63
00:08:23,180 --> 00:08:29,809
So again, by covariance matrix, just to review, this is a big matrix of dimension.

64
00:08:29,810 --> 00:08:33,880
And I by and I and by the way, you have forgot what it means.

65
00:08:33,890 --> 00:08:37,580
It just means how many observations we have obtained from a subject.

66
00:08:37,580 --> 00:08:42,620
I and this can differ by people, hence the index. So here it is.

67
00:08:42,620 --> 00:08:52,720
A matrix with a diagonals. If you have four, this is just the variance of y one given XY, right?

68
00:08:53,350 --> 00:08:59,589
So this is to say what's the marginal variance of the outcome for subject at the first occasion,

69
00:08:59,590 --> 00:09:05,230
given all the information we collected about the covariance and for other pairs, for other locations.

70
00:09:05,500 --> 00:09:08,940
On the after, again, we just change 1 to 2 to 3 to 4, right?

71
00:09:09,190 --> 00:09:16,600
And so on and so forth. And for an off diagonal elements say if this is the first row, fourth column,

72
00:09:16,960 --> 00:09:25,660
then this is going to be a representing the covariance of y one and y four, given all the information we have about this person.

73
00:09:26,560 --> 00:09:33,660
Right. So this is the characterize. If you pick the pair of measurements from the first and fourth occasion, what's the amount of variation there?

74
00:09:33,700 --> 00:09:38,790
Right. Previously in 650, you do not consider this to be anything other than zero.

75
00:09:38,800 --> 00:09:42,180
But in this class we're going to consider this to be possibly non-zero.

76
00:09:42,210 --> 00:09:49,020
That's the whole point. And this whole thing, we have decided to give it a name called Uppercase Sigma.

77
00:09:49,590 --> 00:09:57,210
So usually when I write these, I often put a vertical line there because I just want to distinguish from the sum end, you know?

78
00:09:57,960 --> 00:10:02,400
And by the way, do you know who invented the summit like equals one n?

79
00:10:03,840 --> 00:10:09,000
This notation Who invented this? Now you guys know this person.

80
00:10:11,980 --> 00:10:16,480
So this guy invented this notation. So don't blame me anyway.

81
00:10:16,900 --> 00:10:21,850
So we're trying to distinguish the coverage matrix from the summit, so hence the additional vertical line.

82
00:10:22,030 --> 00:10:26,010
But in the later, I clearly cannot do that.

83
00:10:28,000 --> 00:10:32,230
So I just went on to say that when I write that the by hand I put a vertical line there.

84
00:10:34,240 --> 00:10:43,930
A final thing here is that just like how we have modeled the main structure by a small number of unknowns, the betas.

85
00:10:43,990 --> 00:10:53,060
Right. We can also assume that the variance covariance matrix can take very simple form and the theta is what we call the parameters for the currents.

86
00:10:53,680 --> 00:10:57,910
Hence the point of the two. It's a vector of covariance parameters.

87
00:10:58,150 --> 00:11:01,400
And I just use a very generic number.

88
00:11:01,420 --> 00:11:04,570
Q there to represent how many unknowns that could be.

89
00:11:04,600 --> 00:11:07,630
We will give you some examples, but this is a general big picture.

90
00:11:15,080 --> 00:11:26,090
All right. So examples I want to give you some examples about, you know, Sigma Theta, where theta is of dimension Q by one.

91
00:11:26,420 --> 00:11:34,130
And I believe this is probably the first time if you have not worked with lots of data before to encounter this parameter for covariance.

92
00:11:35,180 --> 00:11:39,350
So let's just consider some simple situations. Suppose the data are a balance,

93
00:11:39,350 --> 00:11:46,730
which means that everybody has the same number of measurements and the measurements occur at a common occasions and for everybody.

94
00:11:47,100 --> 00:11:53,480
Okay. So sometimes we can consider the unstructured covariance matrix, which means that.

95
00:11:54,720 --> 00:12:00,060
If a person has NN measurements, how many pairs do you have?

96
00:12:01,820 --> 00:12:06,710
And she's too right. And how many occasions are there?

97
00:12:08,150 --> 00:12:14,990
And right. So these are like you JNJ prime that are different and these are choosing JNJ prime that is same.

98
00:12:15,410 --> 00:12:18,410
So the total number of parameters there are going to be this one.

99
00:12:21,320 --> 00:12:25,740
I calculate that for you, but you can verify this is really correct.

100
00:12:26,510 --> 00:12:33,950
So the total number of parameters in this unstructured covariance will be and times in plus one over two.

101
00:12:34,190 --> 00:12:38,540
If you have RN equals four right, then this number is what, four times?

102
00:12:39,980 --> 00:12:43,580
Four plus one divided by two. What is this, ten?

103
00:12:44,000 --> 00:12:48,910
Okay, so temperament is a lot. But the good thing is that it is totally.

104
00:12:48,920 --> 00:12:50,260
You cannot be wrong. Right.

105
00:12:50,300 --> 00:12:56,600
You did not make any assumptions about how the diagonals are identical or not and how the after having elements are similar or not.

106
00:12:56,690 --> 00:12:59,749
Right. You can not be wrong, but that comes with a price.

107
00:12:59,750 --> 00:13:02,660
Which means you have known as the estimate, right?

108
00:13:03,020 --> 00:13:09,800
If you have a small number of subjects, you know you've got to be stretching the date a lot to be able to estimate so many parameters.

109
00:13:10,130 --> 00:13:14,360
That's why sometimes we're going to lower Q to some reasonable number.

110
00:13:14,660 --> 00:13:19,400
And this is one example, not only example, but one example. This is called compound symmetry.

111
00:13:20,360 --> 00:13:27,440
So what compound symmetry means is that for this sigma matrix, I actually can write on the side.

112
00:13:27,470 --> 00:13:33,320
I cannot. So you have the diagonal elements that are like sigma squared.

113
00:13:39,540 --> 00:13:49,320
And you have the off the ground and it's like, what? So yes I wouldn't say squared because that can be negative.

114
00:13:50,220 --> 00:13:56,220
The me raise this. As.

115
00:13:56,350 --> 00:14:01,030
As as. Or some people use row or whatever.

116
00:14:02,080 --> 00:14:08,440
So here you have two unknowns, the variants that's common to all occasions, regardless of when you measure it.

117
00:14:08,860 --> 00:14:15,640
It's the same across all occasions and also the common covariance, regardless of regardless of what pair of measurements you picked.

118
00:14:15,730 --> 00:14:19,060
Right? So Q now equals two instead of ten.

119
00:14:19,420 --> 00:14:27,700
In the previous bullet point. And now we are in a situation where you as a modeler can be wrong relative to the.

120
00:14:29,330 --> 00:14:33,770
To the gods. To truth. So we run the risk of Mississippi's.

121
00:14:33,770 --> 00:14:38,569
Find this day. We will discuss the consequence and how to do how to counter this.

122
00:14:38,570 --> 00:14:42,830
But I just want you to know this is a fact that by reducing Q from 10 to 2,

123
00:14:43,040 --> 00:14:46,910
when an equals four, you do run the risk of missing specifying the structure.

124
00:14:49,000 --> 00:14:56,350
So with point number three, I want to conclude that we just want to give you two examples of how signal I can be parameter rise by Q parameters.

125
00:14:56,860 --> 00:15:04,160
And we call those parameters theta the variance covariance parameters. Any questions so far?

126
00:15:11,610 --> 00:15:18,389
Okay. So here we have only talking about the army and the coherence.

127
00:15:18,390 --> 00:15:27,750
And clearly not every distribution is uniquely determined by the me and the coins, but within the family of distribution,

128
00:15:27,750 --> 00:15:32,930
which we called multivariate sales and distribution, once you know you once you know the man,

129
00:15:32,940 --> 00:15:40,800
once you know the various coins, you know the entire distribution. And for the sake of deriving the maximum likelihood estimate and the REMO,

130
00:15:40,950 --> 00:15:46,320
we will be constraining ourselves to this family called multiple or Gaussian and then we will relax that.

131
00:15:46,500 --> 00:15:58,570
Okay, so. Before I end the slightest on emphasize what are the parameters that we primarily want to use data to inform the beta?

132
00:16:00,060 --> 00:16:06,990
Those are the ones you are very familiar with regression parameters and the theta, because theta uniquely determines the sigma.

133
00:16:08,670 --> 00:16:11,910
We can say we want to estimate the sigma i's, which is a fine.

134
00:16:32,460 --> 00:16:38,190
So with that, we will be reviewing something called the maximum likelihood.

135
00:16:40,290 --> 00:16:45,120
Maximum likelihood. Do you guys know who invented this and when it was invented?

136
00:16:48,140 --> 00:16:59,860
The person starts with the F. And the year is 19, I believe, and I think 1992 I'm sorry, 1919, sorry, 12, I believe.

137
00:17:01,060 --> 00:17:08,200
But the theory was formally proved in 1950s by this person will come from Berkeley.

138
00:17:08,920 --> 00:17:12,149
But who is this person? Fisher.

139
00:17:12,150 --> 00:17:15,540
Yes. Who can be that old Fisher?

140
00:17:16,590 --> 00:17:19,950
And and whenever you run into a test. F test?

141
00:17:20,130 --> 00:17:27,380
What's that f? FISHER So so there are lots of things named after him.

142
00:17:28,820 --> 00:17:33,379
So maximum likelihood, despite it being a 110 year old approach.

143
00:17:33,380 --> 00:17:38,660
It is very general approach. You use that every day. Here is how it operates.

144
00:17:39,230 --> 00:17:44,870
Suppose you're given a full distribution about how you would have generated data.

145
00:17:45,900 --> 00:17:55,800
Right. And you ask, what is the value of bait and theta that can make the data you saw to be most likely?

146
00:17:58,980 --> 00:18:02,730
Okay. So post the data is.

147
00:18:04,290 --> 00:18:07,530
Actually, I do have that example here. So.

148
00:18:08,070 --> 00:18:12,270
Okay. Suppose you don't know whether the coins in the world are two sided.

149
00:18:13,480 --> 00:18:16,990
Or one sided with both heads.

150
00:18:17,440 --> 00:18:21,470
One side, sorry, two sided, but three possibilities, right?

151
00:18:22,180 --> 00:18:25,270
Both our heads, both the tells and one has one tail.

152
00:18:26,020 --> 00:18:29,680
Now, if I flip a coin once and it lands, heads, head.

153
00:18:31,100 --> 00:18:36,100
Now. What's your MLA estimate? If I use a coin with two heads.

154
00:18:37,250 --> 00:18:43,400
Or did I use a coin with two tails or did I use a coin with one had one tail, which was most likely.

155
00:18:48,780 --> 00:18:52,710
Trust your intuition. So let me ask that question again.

156
00:18:53,250 --> 00:19:00,760
Three possibilities. A coin with two has a coin with two tails a coin with one heads on tail.

157
00:19:01,600 --> 00:19:06,550
I took one coin which you do not know the truth, you know.

158
00:19:06,610 --> 00:19:12,909
You don't know which one to use, but the outcome is head and please tell me your maximum.

159
00:19:12,910 --> 00:19:17,480
And I could estimate. Of what type of coin I have.

160
00:19:25,150 --> 00:19:31,030
Okay. So let's just calculate this, right? Should be very simple. So you have three unknowns.

161
00:19:32,830 --> 00:19:36,250
Theta equals. Two headed.

162
00:19:37,760 --> 00:19:43,290
Coin. Or to tell.

163
00:19:44,480 --> 00:19:48,460
Coin. And to coin a normal coin.

164
00:19:50,210 --> 00:19:53,210
Which is one head and one tail.

165
00:19:53,440 --> 00:19:57,750
Right. So now you have a probability table.

166
00:20:03,940 --> 00:20:07,840
Where the second row represents the probability of you seeing the data.

167
00:20:09,750 --> 00:20:13,520
Under the parameter. And these are the, um.

168
00:20:13,950 --> 00:20:18,120
Thetas. Suppose this is one, two, three. Okay. One, two, three.

169
00:20:20,100 --> 00:20:25,200
So if you have. So suppose that both. So either sides will land with equal probability.

170
00:20:25,200 --> 00:20:28,560
Okay. So suppose the data is actually.

171
00:20:30,550 --> 00:20:34,160
Flip once. And.

172
00:20:35,510 --> 00:20:41,270
Landed head. Okay. Now let's work out this table.

173
00:20:45,200 --> 00:20:49,670
So if it is a two headed coin, what's the probability we see this outcome?

174
00:20:55,570 --> 00:21:00,920
100%. What if we have a two headed coin?

175
00:21:00,950 --> 00:21:04,340
What's the probability we see an outcome with with head?

176
00:21:05,840 --> 00:21:09,300
0%. And 50% here.

177
00:21:09,420 --> 00:21:16,409
So what is the maximum likely to. You would guess that's a genco.

178
00:21:16,410 --> 00:21:19,500
We'll have a two headed coin that was used. Right.

179
00:21:20,700 --> 00:21:24,360
So the reason why you feel suspect that I'm tricking you is because you are

180
00:21:24,450 --> 00:21:29,730
bringing a strong prior that you probably have never seen a coin with two heads.

181
00:21:30,790 --> 00:21:35,540
But that's not true in my world. I have seen both to tell, to coin, to tell a coin.

182
00:21:36,040 --> 00:21:42,190
So you and I may disagree based on prior, but if you are only going to use a likelihood, our solution should be the same.

183
00:21:43,540 --> 00:21:47,880
So this is the very simple example. To the maximum likelihood.

184
00:21:47,890 --> 00:21:52,590
Hopefully this has not bored those people who are more theoretically oriented.

185
00:21:52,600 --> 00:21:58,900
But still I think this is a good example of teasing out teasing out data and prior.

186
00:22:00,310 --> 00:22:02,500
So this goes back the point to our reiterate.

187
00:22:02,740 --> 00:22:10,270
So the fundamental idea is trying to find in the context of the model what are the beta and theta values that maximize the likelihood.

188
00:22:10,720 --> 00:22:17,470
So you've got to be able to do something like what we have. You've got to be able to understand what are the possible values balance you can take.

189
00:22:19,960 --> 00:22:22,030
Second, you've got to be able to work on this table.

190
00:22:22,030 --> 00:22:27,850
And clearly when you have continuous beta theta, you don't have a table, you have you if densities.

191
00:22:29,330 --> 00:22:34,790
So you need to write out a density and which we have decided to focus on multiple Gaussian.

192
00:22:35,210 --> 00:22:42,800
So if you plug in a beta and theta, you should be able to calculate what's the density of observing the data at hand.

193
00:22:44,510 --> 00:22:48,860
Any questions before we move on? A pause for like 30 seconds just to think about it.

194
00:23:25,590 --> 00:23:33,330
Okay. I don't see any questions. So I'm going to move on to this slide, which provides just a summary of what we just discussed.

195
00:23:37,050 --> 00:23:47,430
We need to write out the probability of the observed data given the unknowns, and then we produce the estimated beta hat or stick my hat.

196
00:23:48,500 --> 00:23:54,650
As our Emily and I want to bring you to the tension here, which is a Sigma Theta hat here.

197
00:23:55,700 --> 00:24:05,419
So this means that once you've decided your particular way of prom arising your various covariance matrix,

198
00:24:05,420 --> 00:24:10,610
like in the compound symmetry setting, you had only two parameters, right, Sigma Square ness.

199
00:24:11,180 --> 00:24:16,760
So once you have estimated the sigma squared ness, you will be able to determine the entire matrix.

200
00:24:17,480 --> 00:24:25,580
So sometimes we just write this, which means you asked and you plug in the estimate of how you have parameter tries the various programs.

201
00:24:38,980 --> 00:24:45,160
So that's still the review. Now, let's start with the independent one here.

202
00:24:45,170 --> 00:24:55,000
We will be dealing with new models and we will be assuming making some assumptions that are likely less realistic.

203
00:24:55,420 --> 00:24:59,290
But instruction will instruct.

204
00:24:59,290 --> 00:25:05,110
Instruction, I would say, should be constructive for us, presenting the more complicated situation.

205
00:25:05,620 --> 00:25:15,430
So here I'm going to ask you to focus on a situation where you have collected data from a series of cross-sectional studies,

206
00:25:15,790 --> 00:25:20,710
repeated ads in different occasions. Let me unpack that a little bit.

207
00:25:22,690 --> 00:25:31,870
Suppose that you have, say, two occasions, one and two, right in longitudinal study, you would follow one person.

208
00:25:33,000 --> 00:25:38,490
Into the second. Right. So, you know, one person was measured two times.

209
00:25:39,030 --> 00:25:48,959
But in the setting we're considering, we will be trying to say we will be trying to focus on this is one person collected at the first

210
00:25:48,960 --> 00:25:55,110
time points say the baseline and we have another person collected at the second occasion.

211
00:25:55,110 --> 00:25:55,920
They are not related.

212
00:25:56,670 --> 00:26:08,070
So if we do this like we collect four people, let's say five people from first time point, we collect like five people from the second time point.

213
00:26:08,370 --> 00:26:13,080
So in total there will be like ten people who are totally not related.

214
00:26:13,950 --> 00:26:20,490
So that we can assume their measurements are independent. Right. So this is what we call cross-sectional studies.

215
00:26:20,910 --> 00:26:26,940
And the reason why we are doing this is trying to work with the notation we have introduced.

216
00:26:27,120 --> 00:26:39,720
Remember, we have two indices, the subject I and occasions j but with independence assumption for the measurements within you know between the JS.

217
00:26:44,010 --> 00:26:50,700
So the data from this. This one here and data from this one here.

218
00:26:51,650 --> 00:26:56,120
Uh, Jay Prime. These two will be independent here.

219
00:26:56,130 --> 00:27:03,900
The I. Although still being the same. They can be just how you would have labeled different people in two occasions.

220
00:27:04,470 --> 00:27:11,820
Suppose for the first five people I just labeled them as one, two, three, four or five for the second occasion,

221
00:27:12,030 --> 00:27:18,509
which just label again a completely different set of five people to be 1 to 3, four or five.

222
00:27:18,510 --> 00:27:25,020
Right? It is just how we have labeled them. And then we can assume that yyy2 are independent.

223
00:27:31,380 --> 00:27:44,670
So this is to say that we have made the assumption that Sigma II here is going to have diagonal values and all these locations are going to be zero.

224
00:27:44,910 --> 00:27:59,060
So not correlate at all. I want to pause for a moment because this sounds like a slight deviation from protocol.

225
00:27:59,540 --> 00:28:14,500
But it is helpful to introduce the estimate for. Actually, to be more precise, I think maybe I just used to.

226
00:28:16,930 --> 00:28:23,480
Thanks. This is probably more precise to relate these two situations.

227
00:28:33,510 --> 00:28:37,410
So to conduct maximum likelihood, we got to write down the likelihood.

228
00:28:38,750 --> 00:28:45,310
So what is the likelihood? Well, in this case, we're not going to do something crazy, just plain simple regression here.

229
00:28:46,100 --> 00:28:52,340
And we will make an additional assumption that's the variance at the occasions will be the same.

230
00:28:57,840 --> 00:29:08,130
This is an assumption we make just for illustrating the idea that something like that is not necessarily good or bad for any any data set.

231
00:29:08,550 --> 00:29:16,860
We can test whether the variance can be said to be equal, but here it's only for illustration and for one observation.

232
00:29:16,860 --> 00:29:26,249
We have this. We can write down the likelihood, which is the probability or other the density of observing.

233
00:29:26,250 --> 00:29:32,720
Why j given the covered. For particular values of the unknowns.

234
00:29:33,620 --> 00:29:38,210
The unknowns are the sigma's and the betas and here have been as buried here.

235
00:29:42,690 --> 00:29:51,670
She represents transpose up. So this is just following how you would have written univariate Gaussian.

236
00:29:52,740 --> 00:30:02,370
This is the realized value. Of the outcome you care about.

237
00:30:09,150 --> 00:30:15,270
And this is the meme which is dramatized by Excited JT Peter.

238
00:30:16,050 --> 00:30:24,960
And here it is the unknown variants. Okay.

239
00:30:25,680 --> 00:30:30,840
And what's in front of this? Exponential serves as normalizing constants.

240
00:30:31,140 --> 00:30:35,100
So if you integrate over all possible values while here, it'll integrate to one.

241
00:30:36,400 --> 00:30:43,600
All right. So with that, we have written down like the likelihood for one data point, right?

242
00:30:44,230 --> 00:30:51,130
But we do have multiple occasions. We do have multiple subjects and that and we assumed they are independent.

243
00:30:51,670 --> 00:30:58,059
And as you have learned that for independent observations, so densities can be multiplied together to describe the joint distribution.

244
00:30:58,060 --> 00:31:03,880
Hence we do here. We do. Which color?

245
00:31:04,700 --> 00:31:09,940
How about. Growing here. Let's see it.

246
00:31:10,000 --> 00:31:13,670
Okay. So. We have these two things.

247
00:31:18,700 --> 00:31:22,450
This is to multiply.

248
00:31:22,450 --> 00:31:25,510
Over what? Over subjects.

249
00:31:30,450 --> 00:31:33,900
Subject IDs and this is to modify over.

250
00:31:35,380 --> 00:31:44,350
Occasions. Because of our unique setting, we have made the assumption that all these measurements are independent.

251
00:31:44,350 --> 00:31:47,740
So we can do this right now.

252
00:31:48,760 --> 00:31:54,100
Once you do this, you should be able to obtain this particular formula.

253
00:31:54,880 --> 00:32:00,280
And this one essentially is just the likelihood with the logarithm.

254
00:32:00,520 --> 00:32:03,520
Right. And you will get this here.

255
00:32:05,080 --> 00:32:10,330
This thing is just, again, the sum over subjects and occasions.

256
00:32:10,660 --> 00:32:13,720
And this K essentially is a total number of observations.

257
00:32:14,500 --> 00:32:23,590
It's defined as this one. So you have I from one to capital N and you have lowercase N for each person.

258
00:32:23,860 --> 00:32:33,579
So this is a total number observation. And our goal is trying to view this thing as a function of Beaton's sigma squared.

259
00:32:33,580 --> 00:32:39,190
And then you can apply some optimization algorithm to obtain what's the optimizing beta and sigma squared?

260
00:32:39,580 --> 00:32:55,650
That's the idea of maximum likelihood. So once you solve that particular optimizing function, you will get something like this.

261
00:32:57,210 --> 00:33:00,600
So you may be telling me that because this is something that's still unfamiliar.

262
00:33:00,870 --> 00:33:04,010
Unfamiliar to me. True.

263
00:33:04,020 --> 00:33:10,350
But actually it is. Just a more, less compact version of this.

264
00:33:14,550 --> 00:33:22,660
And I think there should be no excuse if you have not seen this. So this is the commendation you have seen when you were learning.

265
00:33:22,680 --> 00:33:33,120
650. What we have done here is just to diligently work out exactly what they mean by working over subjects and working over the occasions.

266
00:33:33,540 --> 00:33:36,540
So these two are exactly the same. Right.

267
00:33:38,530 --> 00:33:43,240
And we called this estimate. Our audience really squares.

268
00:33:44,170 --> 00:33:55,690
Right. Why is it ordinary? Because we did not consider any codependence between the pairs of measurements within a person.

269
00:33:55,840 --> 00:34:03,220
Right. So everything is independent. And what we have done is to minimize the objective function.

270
00:34:08,840 --> 00:34:14,360
Okay. So later, I think you have also learned the cause marker theorem,

271
00:34:15,680 --> 00:34:22,520
which is to say that the ordinary square is the best unbiased estimate for the unknown.

272
00:34:23,180 --> 00:34:26,810
Right? Even if the girl's an assumption, does not hold right.

273
00:34:27,380 --> 00:34:36,020
So this is to say, although this estimate of beta halves has been derived under the Gaussian assumption,

274
00:34:36,470 --> 00:34:41,150
it does not mean that this estimate it only works when the girl's an assumption holds.

275
00:34:42,020 --> 00:34:50,930
And similarly, when you will be seeing what we call generalized squares, they will be derived under the multiverse Gaussian assumption.

276
00:34:51,260 --> 00:34:55,310
But they do work even if the multi very Gaussian assumption does not hold.

277
00:34:55,430 --> 00:35:03,830
So just 2% of their. And the philosophy is really simple, right?

278
00:35:03,860 --> 00:35:10,520
We just work for work from one to work, start working from something we're familiar with and generalize to more challenging situations.

279
00:35:12,380 --> 00:35:16,700
Now, that's the part for the independent observations.

280
00:35:17,300 --> 00:35:23,240
More exciting stuff. How about the maximum likelihood for new models with correlated observations?

281
00:35:24,560 --> 00:35:34,040
It is a little bit involved and I would say a little bit challenging if you're hoping to sort of get all the technical details in a single lecture.

282
00:35:34,790 --> 00:35:43,580
But so what I'm going to encourage you is to focus on the overall plan, overall strategy of how we have how we can obtain maximum likelihood.

283
00:35:44,210 --> 00:35:52,160
And you will be having homework problems trying to work out the maximum likelihood or at least to work with these steps.

284
00:35:53,420 --> 00:35:55,750
So that's the first thing.

285
00:35:55,760 --> 00:36:05,650
And the second thing is that you want to pay attention to the distinction now between how we would obtain this, the various covariance estimates.

286
00:36:06,440 --> 00:36:10,150
And we will have to come to competitors.

287
00:36:10,880 --> 00:36:15,650
The first one would be called a Sigma hat with EML.

288
00:36:16,490 --> 00:36:25,220
So this will be obtained just using the approach we just did write out likelihood and find the sigma that's going to maximize the likelihood.

289
00:36:26,290 --> 00:36:33,840
There will be a second very good competitor. It will be called Rambo and we will be talking about how to get there.

290
00:36:35,760 --> 00:36:41,940
And the reason why I want to present this way is that you want to focus on point number two.

291
00:36:43,860 --> 00:36:52,060
So in these models. When you are trying to find the best beta, right?

292
00:36:52,660 --> 00:36:57,020
Look, you have an objective function that's beta sigma.

293
00:36:57,040 --> 00:37:04,350
Of course, it depends on your data to write. So if you are just going to hold Sigma.

294
00:37:05,470 --> 00:37:13,810
How that trying to find the best debater. It turns out it turns out that the better hat is going to be.

295
00:37:16,460 --> 00:37:20,730
Something like. How do I raise this?

296
00:37:21,510 --> 00:37:28,500
Good. Something like. A function of sigma here.

297
00:37:41,460 --> 00:37:48,810
First edition, I'll be surprising that this best bidder, should it depend on whatever sigma we have fixed.

298
00:37:49,220 --> 00:37:55,020
Right. And next thing is that you should notice this f.

299
00:37:56,510 --> 00:38:04,010
I have just written that as a single function, which means that regardless of what values Sigma is held to that function,

300
00:38:04,010 --> 00:38:07,250
a form that maps the sigma to beta had is the same.

301
00:38:09,990 --> 00:38:17,460
Okay. So for simplicity, later on, we will be just writing Beta Sigma hat here.

302
00:38:21,440 --> 00:38:27,490
And you can replace this one by whatever choice you may have.

303
00:38:27,530 --> 00:38:30,770
Say, Peter Hart. You know.

304
00:38:31,810 --> 00:38:35,800
B or better hat. W Right.

305
00:38:37,580 --> 00:38:40,970
The question is which one is better? You can plug in the.

306
00:38:42,570 --> 00:38:47,100
To be the variance covariance matrix. You can plug in w as various coherence matrix.

307
00:38:47,520 --> 00:38:54,720
Which one is going to give you a better way that right now envision that V can be this one?

308
00:38:57,870 --> 00:39:03,570
And W can be this one. The question is, which one gives you a better, better estimate?

309
00:39:04,310 --> 00:39:11,050
Right. So that is how we are going to proceed regarding the estimation.

310
00:39:12,070 --> 00:39:22,360
So just as a recap, we know that both beta and sigma are unknown and our strategy will be first to work

311
00:39:22,360 --> 00:39:27,219
out the mathematical relationship between the arbitrary unknown sigma to the beta.

312
00:39:27,220 --> 00:39:34,330
How to estimate. The optimizing won. And it turns out that mapping is of the same form.

313
00:39:37,720 --> 00:39:41,170
So it's always taking square. Okay, that's not true, but it's always taken.

314
00:39:41,170 --> 00:39:45,100
So to sum it up, regardless of what Sigma putting their same operation.

315
00:39:45,910 --> 00:39:53,170
Now, moving on to a third point, that given that we do have the choice of plug in whatever sigma, we found it reasonable.

316
00:39:53,830 --> 00:40:01,060
These are two most reasonable options the EML one, the maximum likely one or the REMO one, and we will be discussing which one to choose.

317
00:40:01,750 --> 00:40:19,290
And often the best one is Remo. And this is a challenging slide, and I do encourage you to consider whether you have understood logic here.

318
00:40:37,200 --> 00:40:46,470
So with that plan, let's proceed with writing out the function we are trying to optimize as we have a.

319
00:40:47,550 --> 00:40:56,360
You know. Discussed. We are now working with Correlate observations that actually involves the multi very Gaussian density.

320
00:40:57,380 --> 00:41:00,560
So here it is actually the log likelihood.

321
00:41:01,800 --> 00:41:06,000
And this looks very intimidating. But let me explain what they are.

322
00:41:06,840 --> 00:41:10,750
So first, because it has been locked, right?

323
00:41:10,800 --> 00:41:22,590
So there is no exponential here. Everything are on the near scale and all the multiplications converge to, you know, summations.

324
00:41:23,010 --> 00:41:29,760
So let's look at this one. Right. Again, this is to characterize the.

325
00:41:32,260 --> 00:41:37,190
Distance. The Mohan and Obus distance or the multiverse?

326
00:41:37,190 --> 00:41:42,450
Standardized distance, if you still recall. Between this and the mean, which is.

327
00:41:52,550 --> 00:42:01,430
Right. What's the distance between the two? But we also need to account for the fact that within the Y of vector they may be correlated.

328
00:42:01,430 --> 00:42:06,530
And that correlation was described by Sigma II. So we needed to use this to standardize it.

329
00:42:07,160 --> 00:42:16,520
So after all, this is the, uh, how, you know, there's a distance.

330
00:42:18,820 --> 00:42:24,970
Right in front of it. There are a few constants actually, not constants.

331
00:42:25,000 --> 00:42:28,560
So the first one is a constants. I will not talk about it.

332
00:42:29,820 --> 00:42:37,650
The second one is similar to what you would get when you have a single lie.

333
00:42:38,580 --> 00:42:43,870
And so this is the log of the determinant of sigma.

334
00:42:45,910 --> 00:42:56,260
So it's called the log of generalized variance. And here K it is basically representing the total number of observations.

335
00:42:56,860 --> 00:43:02,110
So if one person has two, if not the first three observations, then take was five, two plus three.

336
00:43:02,290 --> 00:43:11,140
Okay. And the goal is to find the values of the parent sigma that maximizes function.

337
00:43:11,800 --> 00:43:14,950
To highlight this, I'm going to point out where are the unknowns?

338
00:43:15,370 --> 00:43:19,930
So this is unknown. This is unknown.

339
00:43:22,710 --> 00:43:28,640
Of course, this is same thing here. So these baited signals are known.

340
00:43:32,690 --> 00:43:37,550
Now we will proceed as as a plan says to whole sigma constant.

341
00:43:46,080 --> 00:43:54,510
For now. Okay. So then whatever is going to maximize this thing needs to minimize this quantity in the curly brackets.

342
00:44:11,350 --> 00:44:16,749
So when you solve for this particular equation, which will do in the next lecture,

343
00:44:16,750 --> 00:44:21,280
not now, because again, I remind you today it's about high level idea.

344
00:44:21,310 --> 00:44:23,020
I don't want you to be lost in the woods.

345
00:44:23,920 --> 00:44:34,150
So if you solve for this equation, as promised, we have found a function that relates whatever sigma you hold.

346
00:44:34,750 --> 00:44:37,900
Well, here you have all the people here, so I'm just going to write it out here.

347
00:44:41,690 --> 00:44:47,270
That maps whatever sigma you have the side effects to the best baiter hat.

348
00:44:48,920 --> 00:44:57,560
So this thing is again one of the most important formula you need to know in this class.

349
00:44:58,790 --> 00:45:08,910
And this is called generalized least squares. It doesn't hurt to pause and recall that we do learn.

350
00:45:08,920 --> 00:45:12,070
We do understand there is a thing called ordinarily squares.

351
00:45:12,550 --> 00:45:20,290
So this generalized these squares is different because it involves these these little things sigma ising here.

352
00:45:30,870 --> 00:45:34,330
Okay. So that's called jail.

353
00:45:34,340 --> 00:45:40,670
All this. Now.

354
00:45:40,910 --> 00:45:44,690
What if. Sigma equals. Just diagonal.

355
00:45:45,470 --> 00:45:48,650
Sorry. Diagonal.

356
00:45:49,220 --> 00:45:56,700
Sigma Square. I hear. So this djilas will be a less.

357
00:45:58,160 --> 00:46:04,420
It is that simple. Okay.

358
00:46:04,460 --> 00:46:10,220
What is if you have Sigma II, that's like sigma one squared.

359
00:46:10,550 --> 00:46:18,430
Sigma two squared. That the sigma four squared. Just say that you and I equals an equals four.

360
00:46:18,460 --> 00:46:24,070
Okay, just as an example. Then what we get will be called.

361
00:46:29,100 --> 00:46:32,340
W Have you anybody heard about this? WLS.

362
00:46:33,560 --> 00:46:42,100
It's actually called weighted scores. And everything else are like zero here.

363
00:46:43,960 --> 00:46:55,420
So this provides you with a kind of a set of a category to describe, you know, increasingly complex notions of estimates.

364
00:46:56,320 --> 00:47:00,490
Here you are. That's a vintage point of looking from top to bottom.

365
00:47:00,490 --> 00:47:03,880
Right. You have learned now there's this thing called generalized V Square.

366
00:47:04,120 --> 00:47:10,250
It can be specialized to us, something we're familiar with or wittily square something.

367
00:47:10,270 --> 00:47:14,470
Some of you may be familiar with. The only difference you need to do need to.

368
00:47:15,460 --> 00:47:18,820
The only difference is how you have specified the structure of sigma.

369
00:47:22,280 --> 00:47:27,080
Now I want to remind everybody that this thing here is not a real estimate.

370
00:47:27,140 --> 00:47:35,200
Right. If Sigma II is unknown.

371
00:47:38,650 --> 00:47:42,340
Sure. But I have to say the unknown.

372
00:47:44,710 --> 00:47:47,860
And fixed are two different notions.

373
00:47:48,610 --> 00:47:53,020
You can fix an unknown thing on the arbitrary quantity, but you may not know it.

374
00:47:54,250 --> 00:47:59,770
So by deriving better hat we have decided to fix at an arbitrary sigma high.

375
00:48:00,310 --> 00:48:05,770
But it's still unknown. So you cannot estimate something and.

376
00:48:05,980 --> 00:48:08,380
And tell me. Hey, this estimate depends on something I don't know.

377
00:48:08,830 --> 00:48:18,220
So later on, we will be discussing to replace the sigma here with actual estimates, and that will be a legitimate estimate for beta.

378
00:48:40,190 --> 00:48:45,050
So now this point, this slide basically can be summarized into that.

379
00:48:45,500 --> 00:48:51,080
You can write down the beta hat estimate.

380
00:48:52,950 --> 00:48:58,530
Using a map computing whatever sigma you have decided put in there to better hat.

381
00:48:58,770 --> 00:49:03,030
So this mapping is the same. What are some properties?

382
00:49:03,300 --> 00:49:07,620
These are just some high level summaries and it's quite easy drive.

383
00:49:08,250 --> 00:49:20,590
So for example, it is unbiased that, you know, if you do one study and repeated and repeated four 100 times, right, every time you apply this smarter.

384
00:49:21,270 --> 00:49:25,260
So upon repeated use, this beta hat will be right on target.

385
00:49:25,830 --> 00:49:35,670
That's what's unbiased means. Second, in statistics, you've got to be talking about the uncertainty associated with the estimate.

386
00:49:35,880 --> 00:49:41,910
Right. It would be a fool to assume that something is known for sure, at least in science.

387
00:49:42,210 --> 00:49:49,200
So there will be some uncertainty in quantification. One important one is what we call the sampling variance covariance.

388
00:49:50,430 --> 00:49:56,460
So here in terms of terminology, we are again using variance coherence.

389
00:49:57,300 --> 00:50:02,610
But for better hats. Remember, beta hat is a adapter.

390
00:50:06,490 --> 00:50:08,139
And each one depends on data.

391
00:50:08,140 --> 00:50:18,310
And clearly we can talk about how different components would be correlated with each other and that mathematical form can be worked out like this.

392
00:50:18,700 --> 00:50:31,780
So it is this one. Again, it should not be too difficult to figure out that what is sigma r equals sigma squared times identity.

393
00:50:33,640 --> 00:50:40,900
So this actually will be just. Sigma Square.

394
00:50:40,900 --> 00:50:44,670
It's. This one.

395
00:50:44,970 --> 00:50:50,850
So here I am using compound notation and trying to remind you something you probably have learned.

396
00:50:51,360 --> 00:50:58,169
I think I've learned in 650. Right. So in special cases, when Sigma I's are diagonal with the same elements,

397
00:50:58,170 --> 00:51:01,680
you will be able to simplify this to something you have already been very familiar with.

398
00:51:04,730 --> 00:51:08,000
And two technical marks.

399
00:51:08,000 --> 00:51:13,700
The first one is that this covariance for beta is going to be exempt if you have the goals and assumption.

400
00:51:14,180 --> 00:51:21,560
But if you don't have the girls in assumption in the first place using some asymptotic argument which I now go over,

401
00:51:22,310 --> 00:51:26,780
you still can have approximately this covariance formulation formula,

402
00:51:27,290 --> 00:51:32,930
but you do need a lot of subjects for a fixed number of measurements per subject.

403
00:51:35,790 --> 00:51:41,460
Okay. It's 355. Why don't we take 5 minutes break and then come back at 4 p.m.?

404
00:52:58,660 --> 00:53:02,670
I said. Right.

405
00:53:02,820 --> 00:53:07,710
So obviously. See, I've been a the.

406
00:53:15,010 --> 00:53:33,230
So for example. Follows in whatever is crucial to satisfy this criterion.

407
00:53:36,470 --> 00:53:41,860
And I do. So let me ask you, what's the.

408
00:53:46,310 --> 00:54:02,160
What's the merits? So in sort of mysterious our small band.

409
00:54:12,210 --> 00:54:15,440
Up here. We're not using a consultation because you have.

410
00:54:17,640 --> 00:54:21,080
But now you do know that something is not necessarily close.

411
00:54:23,550 --> 00:54:27,140
They have. So there.

412
00:54:29,230 --> 00:54:32,460
I would say this. We do require.

413
00:54:33,170 --> 00:54:37,490
Somehow an approximation. By this current situation.

414
00:54:44,840 --> 00:54:48,500
Energy was built. You may consider this central.

415
00:54:54,000 --> 00:55:03,010
Yes. So he probably. But still, it's not the start of.

416
00:55:10,080 --> 00:55:13,540
So that's why that is number one, we are trying to.

417
00:55:16,700 --> 00:55:25,910
I thought you it this one. It really hurts.

418
00:55:26,240 --> 00:56:03,010
So actually, supposing. So when you compare this with the process.

419
00:56:05,060 --> 00:56:46,740
No, sir. Then this is the. Just to understand.

420
00:57:06,970 --> 00:57:38,680
Resistance to resistance. Yeah, I actually.

421
00:58:08,340 --> 00:58:12,460
What's left. So. Basically ideas.

422
00:58:18,000 --> 00:58:21,850
You can try to see. But the idea here is that.

423
00:58:24,620 --> 00:58:28,970
You lie there? Lie. One.

424
00:58:49,990 --> 00:58:54,680
People would read this. That.

425
00:59:06,760 --> 00:59:12,800
Let me be that careful here. So I understand. Can I again?

426
00:59:14,790 --> 00:59:17,890
Basically the idea is that. In situations.

427
00:59:55,560 --> 01:00:09,540
All right, everybody, let's continue. So so far, we have on a high level, try to derive how the generalized view squares, and we know a few facts.

428
01:00:09,900 --> 01:00:14,700
The first is when you are holding Sigma II to some arbitrary values,

429
01:00:15,000 --> 01:00:21,530
you can work out the formula of how the beta hat is related to all the information you have.

430
01:00:21,540 --> 01:00:28,830
It's this formula. Second, we can work out the variance formula for beta hat here.

431
01:00:30,120 --> 01:00:38,550
And in the next slide what I'm going to say is that they reduce to some familiar notions, as you have encountered in 650.

432
01:00:39,660 --> 01:00:47,070
This happens when you set sigma to be particular values like sigma squared times identity.

433
01:00:49,560 --> 01:00:57,390
Okay. And here number three is the it is a technical point, which is to say,

434
01:00:57,720 --> 01:01:02,640
depending on what sigma I am putting there, you may have a better or worse beta hat estimate.

435
01:01:05,040 --> 01:01:09,630
So first you have to know that. So regardless of what Sigma II plug in, there is always I'm biased.

436
01:01:10,260 --> 01:01:17,010
So it is not a not whether one estimates it has a less bias than we prefer that it is rather

437
01:01:17,010 --> 01:01:22,740
the variability of the beta hat that determines the best choice of the sigma one plug in.

438
01:01:25,320 --> 01:01:33,630
Theoretically, it can be shown that when the sigma you plug in is the actual true variance covariance.

439
01:01:36,700 --> 01:01:47,680
I put a zero there. I indicate truth. Then if you have made that guess correctly, your estimate of beat ahead will be the best.

440
01:01:48,830 --> 01:01:55,190
Clearly the challenge is that how do we know what you have guessed is the correct one?

441
01:01:58,850 --> 01:02:08,330
But in general it is a theoretical result. It says that the following difference will always be a positive semi definite matrix.

442
01:02:09,110 --> 01:02:19,820
So let me unpack that a little bit. First, we are plug in an arbitrary choice of the variance covariance matrix for everybody.

443
01:02:20,180 --> 01:02:27,200
It is called VI, right? And as we know, when you plug that in, you get a generalized squares.

444
01:02:28,420 --> 01:02:33,300
And then you can calculate that parents. The second one is the.

445
01:02:34,560 --> 01:02:42,600
Should I say zero here? Second one, you plug in all the semis using the true various currencies as if your God or goddess.

446
01:02:43,180 --> 01:02:51,960
Right. And that difference ought to be something that resembles a positive or at least a non-active value in scalar context.

447
01:02:52,470 --> 01:03:01,230
But because we are dealing with a vaporized beta out, so we have to talk about what we call positive semi definite matrix.

448
01:03:01,500 --> 01:03:05,790
It is analogous to a negative number in one dimension.

449
01:03:06,630 --> 01:03:18,000
Okay. So in other words, if you made a very bad guess about the variance covariance matrix like you plug in the very crappy vine there,

450
01:03:18,540 --> 01:03:26,130
you would not get a very good variance, which means that a smarter guy or gal will be able to get a better head estimate.

451
01:03:26,160 --> 01:03:30,840
That's a less variable, which means he or she have used the data more efficiently.

452
01:03:39,120 --> 01:03:44,400
So this brings us naturally to the question is that how do we estimate these sigma i's?

453
01:03:47,820 --> 01:03:54,230
Two approaches. There will be a male.

454
01:03:54,740 --> 01:03:58,880
There will be Remo. But let's talk about, um.

455
01:03:59,330 --> 01:04:03,590
Just say. And they'll approach maximum likelihood, as we know.

456
01:04:04,010 --> 01:04:13,930
They like it a lot. Like a function is a function of theta and sigma, or if you will, sigma theta with the data in here.

457
01:04:15,860 --> 01:04:18,980
You want to maximize it. Want to find.

458
01:04:28,410 --> 01:04:33,900
You want to find the beta hat and theta hat that maximize this thing?

459
01:04:35,060 --> 01:04:39,190
Oh, by the way, do you guys know this notation? Argo, Max. No.

460
01:04:39,880 --> 01:04:43,780
So. Well, you guys know the max, right?

461
01:04:43,810 --> 01:04:50,970
Say, Max of theta. What does this mean? So you can Max oversee it and find the highest value of F.

462
01:04:51,420 --> 01:04:52,530
That should be very simple.

463
01:04:53,790 --> 01:05:04,800
But if you say Arc, Max, now you're trying to say, okay, across all the thetas you can take what is the theta value that achieved that maximum f.

464
01:05:06,030 --> 01:05:17,210
So I usually do this anyway, just as a notation. And this will be helpful if later on you'll be typing like later it will be basically arc maps here.

465
01:05:18,710 --> 01:05:26,360
So returning to the thing I am pointing towards, this is a for maximum Likert approach.

466
01:05:26,630 --> 01:05:32,360
It is a living fishers way to death, you know, just maximize everything with respect to it.

467
01:05:32,930 --> 01:05:39,440
But is this a good idea? It turns out that this may not be a good idea, but I will elaborate later on.

468
01:05:41,610 --> 01:05:47,880
So again, if we are able to do this, we will be able to plug in these sigma housing there and then get better hat.

469
01:05:49,680 --> 01:05:54,000
And this whole thing should be only dependent upon data.

470
01:05:57,850 --> 01:06:01,990
I apologize. This is really hard to write on this very tilted surface. Let me try to do this again.

471
01:06:18,810 --> 01:06:21,850
So this is a true estimate. Okay.

472
01:06:44,780 --> 01:06:51,560
So now what are some property with this as now with estimated.

473
01:06:52,160 --> 01:06:58,250
Rather than assume weather rather than the constant known or fixed variance covariance matrix.

474
01:06:59,240 --> 01:07:03,090
So we say now Peter had is a consistent estimate of theta.

475
01:07:04,700 --> 01:07:15,080
Notice the small difference in how I call it previously called beta, how it is unbiased when you hold the Sigma II to certain particular values.

476
01:07:15,860 --> 01:07:21,740
Now it is only consistent, which in plain English it says with very high probability,

477
01:07:21,950 --> 01:07:27,440
the speed of hat will be very close to the population regression from a beta.

478
01:07:28,420 --> 01:07:36,660
For increasing sample size. N Which means that if you have a lot of subjects data to estimate the speed of a heart, you should not be wrong.

479
01:07:36,670 --> 01:07:42,160
Very frequently you should be with high probability close to to the thing you are trying to estimate.

480
01:07:42,700 --> 01:07:47,410
That's the mathematical notion of consistency.

481
01:07:49,390 --> 01:07:55,000
Okay. You do want to estimate it to behave this way, right? Because if you are spending more money tracking data, it better get to the truth.

482
01:08:05,790 --> 01:08:11,550
So again, here is a statement that I probably will need to skip for now.

483
01:08:12,000 --> 01:08:16,680
But if you're willing to assume goals and errors, then you do have on business.

484
01:08:17,250 --> 01:08:21,210
So the previous statement applies to general situation, even when the Gaussian does not hold.

485
01:08:24,600 --> 01:08:28,830
Second, we also can calculate the.

486
01:08:29,870 --> 01:08:38,660
Approximate sampling distribution of beta out. And this will depend upon the central limit theorem that is used here.

487
01:08:39,260 --> 01:08:49,910
But the bottom line is you will be able to calculate the variance covariance matrix for beta hat even if the sigma i's are estimated themselves.

488
01:08:55,150 --> 01:08:58,180
So this is the theoretical.

489
01:09:00,280 --> 01:09:06,250
Theoretical Barron's organs for better heart.

490
01:09:06,820 --> 01:09:11,980
And and I have to say that this one is actually the truth.

491
01:09:12,450 --> 01:09:15,490
Okay. Question.

492
01:09:16,780 --> 01:09:20,320
Is this variance covariance some number you can.

493
01:09:21,840 --> 01:09:25,590
Just put on your paper report when you are doing data analysis.

494
01:09:26,980 --> 01:09:32,230
If Sigma I's unknown. You cannot write because you don't know this.

495
01:09:32,740 --> 01:09:37,750
So when you plug in. Sigma I my in here.

496
01:09:38,260 --> 01:09:42,790
This whole formula becomes what we call the estimate or.

497
01:09:45,090 --> 01:09:51,090
Of variance covariance. For Peter had.

498
01:09:53,270 --> 01:09:56,659
So these two concepts are, I know, very, very close.

499
01:09:56,660 --> 01:10:01,100
And I hoped when I was first learning that somebody was telling me this difference.

500
01:10:01,820 --> 01:10:05,600
But I just want to say that this is a theoretical quantity.

501
01:10:06,650 --> 01:10:15,260
It's good we have this. But when you're trying to report the various Covance, you've got to plug in an estimate of that sigma.

502
01:10:34,760 --> 01:10:39,110
So this has been a very technically challenging part.

503
01:10:39,380 --> 01:10:45,620
I would like to provide you guys with a summary, so at least you can verify that these are the things we talked about.

504
01:10:47,630 --> 01:10:58,730
Summary for part one. So first, we have introduced generalized least squares by working with a multivariate Gaussian distribution assumption.

505
01:10:59,920 --> 01:11:03,640
So they can be derived as maximum likely estimate.

506
01:11:06,820 --> 01:11:11,830
Second, the sampling distribution for the beta at beta had estimate,

507
01:11:13,330 --> 01:11:19,870
especially with the sigma I estimate plugged in, they will be approximately Gaussian.

508
01:11:20,680 --> 01:11:27,670
I have to acknowledge here it is a little bit hand-waving because I do not want to spend a lot of time on proving the central limit theorem.

509
01:11:28,570 --> 01:11:34,930
So this is just, again, as I said, a high level understanding of the statistical property here.

510
01:11:37,810 --> 01:11:44,110
Number two for the estimation of Sigma I as we have discussed.

511
01:11:45,160 --> 01:11:51,740
Surely nobody wants to be wrong. So you want to put the fair and square and strong variance covariance matrix assumption to be minimal.

512
01:11:51,760 --> 01:11:57,380
Remember the letter Q? Right. If it's unstructured, it is a large number.

513
01:11:57,400 --> 01:12:00,520
But if we're willing to make some assumptions, it can be much smaller.

514
01:12:01,330 --> 01:12:07,460
So there when your s when your analysis relies on an estimate, it sigma II,

515
01:12:07,780 --> 01:12:16,450
you do have to make a conscious choice of whether you want to go with a very complicated Sigma II or some simplified format of Sigma.

516
01:12:17,350 --> 01:12:20,950
So the idea is that there is no free lunch for being correct.

517
01:12:20,980 --> 01:12:28,990
You will sacrifice some statistical money to estimate those huge number of parameters in the unstructured variance covariance.

518
01:12:32,730 --> 01:12:42,030
So all the statistical properties we have been talking about will mostly be true when the number of subjects is large,

519
01:12:42,870 --> 01:12:50,610
i.e. when the central limit theorem roughly kicks in. Of course, it's very hard to know when it kicks in, but you want that to be large.

520
01:13:01,980 --> 01:13:15,830
And also. Somebody in the office actually asked me, what is the difference between time series analyzes and longitudinal data analyzes?

521
01:13:16,760 --> 01:13:21,140
So what I would say is that it is a difference in the end here.

522
01:13:22,310 --> 01:13:31,250
Often when you are monitoring subjects, you can have, say, 100 people and for them, you follow the follow their trajectory over time.

523
01:13:34,180 --> 01:13:39,810
And so on and so forth. Right. But four time series.

524
01:13:40,110 --> 01:13:44,580
So this is for longitudinal data analysis for time series.

525
01:13:48,610 --> 01:13:53,530
Often you follow one subject.

526
01:13:56,100 --> 01:14:04,940
For a long time. So you have lots of measurements.

527
01:14:06,700 --> 01:14:10,750
Over time that's measured here. You know what I mean?

528
01:14:12,550 --> 01:14:17,430
So this is. You know, one important how to say.

529
01:14:18,630 --> 01:14:28,160
Prac a difference when you're trying to choose methods. When you have data that comes from multiple subjects, many subjects but shorter measurements,

530
01:14:28,170 --> 01:14:31,620
you want to consider longitudinal analyzes which the situation here.

531
01:14:32,070 --> 01:14:38,880
So this is say that when you have large in these beta had estimates should be good well approximated.

532
01:14:39,270 --> 01:14:45,960
But if you're in a situation where the number of measurements per subject is small and you have few subjects,

533
01:14:46,230 --> 01:14:54,410
then that's very tough and you got to be able to you got to be willing to make some assumptions about the sigma to simplify the estimation.

534
01:14:58,520 --> 01:15:05,700
I think I'm running out of time for talking about the overall overview of restrictive medicine it.

535
01:15:06,320 --> 01:15:13,190
So for this I will be continue talking about Remo in the next lecture and I will be posting

536
01:15:13,190 --> 01:15:20,000
a recording on the technical note which is hand out for me that is posted on the website.

537
01:15:20,720 --> 01:15:27,590
So that one was recorded last year and I plan to go over the technical note in class again.

538
01:15:27,830 --> 01:15:33,470
So for some of you who are trying to make some preparations about next steps here, you can you can watch that.

539
01:15:34,250 --> 01:15:38,510
But if you don't have time, no worries. I will go over that again with that.

540
01:15:38,780 --> 01:15:40,100
Thanks, everybody, and have a good day.

