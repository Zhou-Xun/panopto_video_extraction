1
00:00:01,310 --> 00:00:06,600
This is like an outdoor one.

2
00:00:06,870 --> 00:00:10,140
Yes. Was there a last sparkle day, the Friday before Thanksgiving?

3
00:00:11,190 --> 00:00:17,090
Technically, no. Oh, no. So let me fill you in on that.

4
00:00:18,070 --> 00:00:24,990
No, I think that was our last real your last and is liable.

5
00:00:27,180 --> 00:00:31,830
All right. 15.

6
00:00:31,860 --> 00:00:35,910
All right. Less than half. Seems to be the trend.

7
00:00:38,390 --> 00:00:42,480
I have 75 students in my class next semester. Environmental, 75 people.

8
00:00:43,680 --> 00:00:46,950
But it's only. It's like a math problem, but only half of them show up.

9
00:00:49,260 --> 00:00:53,580
Should I take two people off the waitlist? Oh, it's a weird problem, huh?

10
00:00:54,300 --> 00:00:57,390
Optimization. How many folks should we enroll in my class, anyway?

11
00:00:57,870 --> 00:01:00,080
You should personally do all that before and ask them if they're fine.

12
00:01:00,930 --> 00:01:05,100
I say you should personally e-mail them before and ask them if they're planning on attending.

13
00:01:05,160 --> 00:01:08,430
Should email every person individually? Yes. Like a plan.

14
00:01:08,430 --> 00:01:11,940
Like single tickets where they purposely oversell. Oh.

15
00:01:12,730 --> 00:01:18,090
Oh, yeah, yeah, yeah, yeah. I know how you do it. You sign up for all the electives, and then you drop a couple.

16
00:01:18,120 --> 00:01:21,570
I get it. I get it. But anyways.

17
00:01:22,260 --> 00:01:25,380
All right. All right. We have three classes left.

18
00:01:25,590 --> 00:01:29,820
One, two, three. Today. Friday. And expensive.

19
00:01:30,810 --> 00:01:35,520
And there is no class that last Friday. A week from Friday. Because you have an exam.

20
00:01:36,270 --> 00:01:41,900
So that opens up. You know, it's taken on Friday. Where to go here?

21
00:01:41,910 --> 00:01:48,809
I'm getting ahead of myself. All right. Exam two said the grades were almost identical.

22
00:01:48,810 --> 00:01:53,760
I did a scatterplot of Exam one versus Exam two in a fit of regression lines.

23
00:01:53,760 --> 00:02:02,819
Not perfect, but awful either. So you had to make some modifications to the way I word things.

24
00:02:02,820 --> 00:02:08,910
I wasn't clear enough. But overall, like, again, I think you guys did really well on an average.

25
00:02:09,870 --> 00:02:18,180
We talk about averages, right? We have one more homework assignment, one more test,

26
00:02:18,390 --> 00:02:25,320
and we only have three more questions since we kind of slowed down a little bit, which was perfectly fine.

27
00:02:26,070 --> 00:02:32,830
Topic six, which is the bootstrap and our missing data and I have not yet put on the screen.

28
00:02:32,850 --> 00:02:36,690
There we go. We're going to ixnay the bootstrap right now.

29
00:02:37,500 --> 00:02:41,070
We're going to talk about missing data. That's far more important, I think, for longitudinal studies.

30
00:02:42,390 --> 00:02:46,980
The homework assignment, therefore, has been pared down.

31
00:02:49,210 --> 00:02:54,850
A little bit. So on canvas I've put an updated version of this homework assignment.

32
00:02:54,850 --> 00:03:00,340
If you downloaded something a long time ago, we're not going to do the bootstrap in this homework assignment.

33
00:03:01,330 --> 00:03:07,480
So question one is irrelevant. I just crossed that out there for my own personal ability to keep it, maybe next year,

34
00:03:09,850 --> 00:03:12,879
but you're all going to analyze the same data set in this homework assignment.

35
00:03:12,880 --> 00:03:16,690
So we're done with that. Everybody gets a random sample of datasets,

36
00:03:17,290 --> 00:03:25,780
so we're all going to use this dataset that looks at individuals with arthritis and to see how long it takes them to walk 50 feet.

37
00:03:26,140 --> 00:03:29,760
That's a measure of severity of arthritis. And some folks got a treatment, some didn't.

38
00:03:30,820 --> 00:03:36,520
There's lots of missing data in that dataset, so it's perfect for the topics that we're going to use this week.

39
00:03:37,520 --> 00:03:44,690
So question one is about the homework assignment, essentially asks you to look at different ways to look at these data.

40
00:03:45,230 --> 00:03:50,030
So look at the data as they are called out and available data analysis.

41
00:03:50,510 --> 00:03:56,240
I'm going to ask you to reanalyze the data in which you exclude individuals who had any missing time points.

42
00:03:57,550 --> 00:04:03,760
So again, you're only going to analyze individuals who had all of the measures in the study available in the data.

43
00:04:04,420 --> 00:04:07,959
And that's not what we do with longitudinal data and what some people were doing on the homework assignments.

44
00:04:07,960 --> 00:04:17,410
Maybe the beauty of longitudinal data is you can drop out of the study and come back in and supply data at the time points at which you can.

45
00:04:18,340 --> 00:04:21,280
And we don't have to exclude you from all of the all of the outcomes.

46
00:04:22,810 --> 00:04:26,260
So when asked to do that and see what happens and then we're going to use an imputation method

47
00:04:26,260 --> 00:04:31,690
that's really bad we're going to talk about because you will see it in published research.

48
00:04:32,080 --> 00:04:35,440
You will see you will work with investigators who use this method.

49
00:04:35,920 --> 00:04:42,249
And it's called last observation carried forward. So somebody drops out of your study and you're missing three time points at the end.

50
00:04:42,250 --> 00:04:45,430
What do you do? You just carry over the last observation.

51
00:04:47,020 --> 00:04:50,349
Why would you do that? Why does that make sense?

52
00:04:50,350 --> 00:04:54,669
Right. But that's what happens a lot. So we're going to look at what happens because that's the easy.

53
00:04:54,670 --> 00:05:00,370
It's easy. That's why we do it. But we'll look at what happens to the analysis of the data when we do that.

54
00:05:01,180 --> 00:05:06,729
That's homework six. We're going to work on it together in class.

55
00:05:06,730 --> 00:05:08,590
So Friday, I'm not going to record lecture.

56
00:05:09,820 --> 00:05:16,060
Those who come here get to work on it with me and to each other, and those who choose not to can work it on their own.

57
00:05:16,360 --> 00:05:23,050
That's, that's cool. But I'm not going to record what we do in class so people can sit at home and do what we did.

58
00:05:23,080 --> 00:05:28,510
So anyway, essentially, I want you to work on these methods in a seminar, homework assignment.

59
00:05:28,510 --> 00:05:33,040
Everybody is going to get 50 points because we're all doing the same thing for the most part.

60
00:05:33,040 --> 00:05:40,000
So again, I want you to use these methods before we disappear for the semester.

61
00:05:40,240 --> 00:05:42,460
So there's the homework assignment. It's on canvas.

62
00:05:43,420 --> 00:05:54,130
We'll work on it closely on Friday and maybe the following Wednesday, depending on time in which I talk here, Exam three.

63
00:05:55,000 --> 00:05:59,080
So it's very unorthodox for me to have another exam so quickly.

64
00:05:59,710 --> 00:06:11,200
But this isn't as much of a test as it is having you analyze a set of data and as I heard some of you talking so on the canvas home page,

65
00:06:11,560 --> 00:06:17,110
I've added a link down here past the six topics says final exam links.

66
00:06:18,360 --> 00:06:24,960
And so there are a set of data and there's a description of the exam and let's go through the description.

67
00:06:25,200 --> 00:06:30,689
And as promised, you have been filling in diligently your results from Farkle.

68
00:06:30,690 --> 00:06:36,780
Every week I have created a dataset from what you have entered to this point.

69
00:06:38,640 --> 00:06:42,390
If you don't do Farkle anymore this semester, it's not going to impact your grades.

70
00:06:43,170 --> 00:06:50,060
If you want to play Farkle somewhere, that's great, too. It was to get you guys to do this so that I have something at the end of the semester.

71
00:06:50,070 --> 00:06:53,610
And you did that. Thank you. This is ever going to come up. Maybe it won't.

72
00:06:55,580 --> 00:06:59,260
Oh, but I do have a copy of that.

73
00:06:59,370 --> 00:07:02,960
All right. So this is the description file that's at the link right now.

74
00:07:03,650 --> 00:07:09,380
It's my first document and are marked down very, very basic.

75
00:07:12,290 --> 00:07:16,190
So as I said, I took all of the sparkle data. Lots of typos.

76
00:07:16,310 --> 00:07:20,390
Well, I did some data cleaning. I got an experience with data cleaning a little bit there.

77
00:07:20,690 --> 00:07:26,890
Oh, I can make you bet. 125 said, okay.

78
00:07:27,490 --> 00:07:32,920
Yeah. Okay. So I took the circle data that you that you and I.

79
00:07:32,950 --> 00:07:41,170
So there's 56 individuals in this dataset and have put them into in our dataset from what you entered in Qualtrics.

80
00:07:41,830 --> 00:07:45,980
The data is already in R, it's already in long format.

81
00:07:46,000 --> 00:07:49,300
I don't want you to spend any time with all of the machinations with data.

82
00:07:50,350 --> 00:07:56,560
I want you to, because we're going to take a test on these data. So here's what the data look like.

83
00:07:56,700 --> 00:08:04,620
I just did enough rows so that you could see there's one person with 11 weeks of information and then the subject two starts mixed.

84
00:08:04,620 --> 00:08:07,589
Everybody up so they're not ordered by Eddie's in any way.

85
00:08:07,590 --> 00:08:13,499
So hopefully that, you know, nobody is you can't tell who you are unless you look at maybe some of the information.

86
00:08:13,500 --> 00:08:17,250
Again, there's one person who's age is 54. She's the only one.

87
00:08:17,250 --> 00:08:23,070
I think maybe you'll you'll be able to pick out anyway. So there's an indicator of time week.

88
00:08:24,220 --> 00:08:28,630
There's an indicator of whether you said you knew how to play circle before the class to cluster of the semester or not.

89
00:08:28,640 --> 00:08:32,770
Yes or no? There's your age at the beginning of.

90
00:08:32,780 --> 00:08:38,930
So somebody had a birthday during the semester and your birth situation might have changed, but it moved it back.

91
00:08:39,480 --> 00:08:47,030
I meant. I meant I meant for it. To be constant within individual periods has changed.

92
00:08:48,770 --> 00:08:54,260
My score is the score you got when you played the game. Keep score is the score of the computer.

93
00:08:55,190 --> 00:08:59,720
And then winner indicates whether or not you had the higher score or not, whether you got to 10,001st or not.

94
00:08:59,910 --> 00:09:07,040
Right. So again, we have some continuous outcomes here and we have a binary outcome.

95
00:09:08,360 --> 00:09:11,570
And so you have the data now. I'm giving you the data now.

96
00:09:12,290 --> 00:09:16,100
Please do not wait until you open up the exam to think about this.

97
00:09:17,750 --> 00:09:26,700
I've also told you how I want you to model the data. So there are a series of nine questions, each requiring a numeric answer.

98
00:09:26,940 --> 00:09:33,620
That is not correct. Yeah, I changed that when I got a series of nine questions.

99
00:09:33,630 --> 00:09:39,600
Some are numeric, some are not. Some are multiple choice. Okay, look at the correct document.

100
00:09:39,600 --> 00:09:43,740
I think different documents on canvas anyway, followed by a final question.

101
00:09:43,750 --> 00:09:47,250
So there's ten questions on this exam. The first nine are problems.

102
00:09:47,460 --> 00:09:52,590
The last one asks you to submit your code. So please don't exit the exam.

103
00:09:53,010 --> 00:09:56,700
Make sure you save a couple of minutes in the 2 hours to get this document.

104
00:09:57,000 --> 00:09:59,910
Just drag it into that window and you're done.

105
00:10:00,690 --> 00:10:07,860
So it opens on Friday, a week from Friday at noon, and it'll close at 5 p.m. on the following Tuesday of exam week.

106
00:10:07,860 --> 00:10:13,230
Hopefully that gives everybody time to do this when they aren't too stressed out about other tests.

107
00:10:15,390 --> 00:10:20,910
Once you have an exam, you have under 20 minutes. So again, please start by 3 p.m. on Tuesday.

108
00:10:21,720 --> 00:10:27,840
After 2 hours it will shut down. I'm going to ask you to sit for most.

109
00:10:29,380 --> 00:10:34,600
So here are the models. You should have these models already programed before you open up the exam.

110
00:10:35,140 --> 00:10:40,030
Right. So model says using a marginal model model the average score for a person.

111
00:10:40,030 --> 00:10:47,349
So my score right is a function of the main effects of continuous week, continuous age and this indicator of circle.

112
00:10:47,350 --> 00:10:53,740
Yes or no, no interactions. And you should use a working exchangeable correlation matrix.

113
00:10:54,100 --> 00:10:57,759
Hopefully all of that means something to you at this point in this class.

114
00:10:57,760 --> 00:11:06,190
I hope Model B says using a marginal model model, the average score for the computer as a function of main effects of continuous week, age and Farkle.

115
00:11:06,190 --> 00:11:10,690
The same variables. No interactions working exchangeable correlation.

116
00:11:12,100 --> 00:11:16,890
Model C says using a marginal model model, the logic of the probability that a person wins their game.

117
00:11:16,900 --> 00:11:22,389
So that winner variable as a function of the main effects of week H and Farkle.

118
00:11:22,390 --> 00:11:31,930
No interactions and exchangeable correlation. And then model this as using a conditional model model the logic of the probability

119
00:11:31,930 --> 00:11:36,059
a person wins their game as a function of the main effects of continuous,

120
00:11:36,060 --> 00:11:41,070
weak, continuous age and for new interactions. And you should include a random intercept.

121
00:11:43,030 --> 00:11:46,570
So those are the four models. So you have the data.

122
00:11:47,770 --> 00:11:51,330
By the time you take the test, you have written the R code that fits these four models.

123
00:11:52,240 --> 00:11:59,080
And from the output of those four models, I'm going to ask you questions. It's about two questions per model, two or three.

124
00:11:59,260 --> 00:12:04,340
Right. To get to the test to get to nine. That's the test.

125
00:12:05,760 --> 00:12:08,460
And I'm hoping that this isn't too arduous.

126
00:12:08,470 --> 00:12:16,870
It's more to see whether more and more time, if you know how to use the methods we've talked about in this class, applied to a dataset.

127
00:12:18,090 --> 00:12:21,750
And then you're done. Right. Questions?

128
00:12:22,620 --> 00:12:25,920
Yes, sir. Data. Clean data. Are they clean?

129
00:12:26,550 --> 00:12:31,680
They are so clean. There are no missing data.

130
00:12:31,920 --> 00:12:35,030
There are no negative images or anything weird. Nope.

131
00:12:35,160 --> 00:12:38,490
Everything is super clean. I am a really nice guy to work with.

132
00:12:39,960 --> 00:12:43,380
So, no, I don't want you to deal with any of that. There shouldn't be any weird data.

133
00:12:44,580 --> 00:12:49,110
If you see them, let me know. But I think I always seem to make a mistake.

134
00:12:49,110 --> 00:12:56,370
But I think I've cleared out most of the problems. All the Farkle scorers, there's no negative Farkle scores or anything weird like that.

135
00:12:57,000 --> 00:13:00,390
So I think you should be good. All right.

136
00:13:02,210 --> 00:13:09,630
And that's and that's the test. So again, that will happen a week from Friday and go through the Tuesday of exam week.

137
00:13:10,650 --> 00:13:14,460
And you are done with longitudinal data.

138
00:13:17,780 --> 00:13:22,700
All right. On to some lectures from.

139
00:13:24,930 --> 00:13:27,000
I'm a little. I'm sorry. I'm a little off.

140
00:13:27,800 --> 00:13:36,510
I had an incident yesterday at work, and there was a threat of a shooting at a minor high school yesterday where my daughter goes to high school.

141
00:13:37,260 --> 00:13:45,180
So. I'm a little preoccupied these days with all the events that happen in our lives anyway.

142
00:13:46,500 --> 00:13:50,610
But we as a show must go on here. What's.

143
00:13:53,480 --> 00:13:59,450
From beginning to end. So if you have downloaded the slides in the past, I made some slight changes.

144
00:13:59,450 --> 00:14:04,760
I found a couple of typos. I said, I'm biased when I'm biased, which is an important typo not to make.

145
00:14:06,440 --> 00:14:11,900
So they're on canvas now. So we're going to try to lecture C, which has to do with missing data and B has to do with the bootstrap.

146
00:14:12,890 --> 00:14:20,870
Again, if you look at those, if you want to use the bootstrap to learn about it, take 685 next semester in my class we'll talk about the bootstrap,

147
00:14:21,980 --> 00:14:30,860
but the concept of missing data and I'm not going to get into lots of a missing data, of course, which we do offer.

148
00:14:30,860 --> 00:14:35,290
We didn't offer, did we offer last year? You guys were probably too early in your career.

149
00:14:35,300 --> 00:14:38,810
AS No, that was run Little's fields. He's getting close to retirement.

150
00:14:39,800 --> 00:14:40,970
No one seemed to have picked it up yet.

151
00:14:40,970 --> 00:14:49,400
At least did a master's level tech missing data anyway common in longitudinal studies because we are following individuals prospectively over time,

152
00:14:49,730 --> 00:14:53,030
possibly many times for maybe a long period of time.

153
00:14:53,960 --> 00:14:58,970
And human beings, unlike mice in cages, are hard to keep track of.

154
00:15:00,260 --> 00:15:04,190
They just don't show up in hours, break down.

155
00:15:04,190 --> 00:15:11,810
They write all kinds of things. The kids get sick, they move away entirely, they lose interest, right?

156
00:15:12,440 --> 00:15:19,310
We're all really excited to join a study. And so we see that there are ten surveys to fill out and then we're not so interested anymore.

157
00:15:21,380 --> 00:15:27,860
Again, if they're biologic samples, whether it's blood serum, saliva, etc., etc., they get lost, they get destroyed.

158
00:15:28,400 --> 00:15:30,140
They just didn't get stored properly.

159
00:15:30,350 --> 00:15:36,860
And then two years later, when we go to analyze all these terrific, terrific biomarkers, we find out that, in fact, the samples of things that send.

160
00:15:37,960 --> 00:15:43,300
So when you're doing a lot of self-study, you have to think about how much of the data are missing because less data just means less power.

161
00:15:44,620 --> 00:15:49,580
And you also have to think really hard about why the data are missing, because less random means more bias.

162
00:15:50,200 --> 00:15:56,770
So those are that, again, the tradeoff between bias and variance that occur in everything that we do are really important here.

163
00:15:58,540 --> 00:16:06,070
So recall with generalized and just ordinary linear models, we had a concept of what we call the incomplete case analysis.

164
00:16:06,100 --> 00:16:12,940
So again, this was back in the day when you had one outcome per person, and if someone was missing a covariate or they were missing an outcome,

165
00:16:13,450 --> 00:16:18,220
they just got thrown out of the analysis because you needed both variables in order to do anything.

166
00:16:19,520 --> 00:16:22,600
Right. So only interviews with the outcome and all the covariates could be included.

167
00:16:22,990 --> 00:16:28,629
And that's pretty standard default in in our or any software package.

168
00:16:28,630 --> 00:16:32,980
But you did make some assumptions there with longitudinal data methods,

169
00:16:32,980 --> 00:16:37,690
we have two levels of completeness for each individual we have to think about within each timepoint.

170
00:16:37,780 --> 00:16:41,770
So if you think about the long arm, the white format, you got one time point.

171
00:16:42,340 --> 00:16:48,520
Is everybody there? And among the templates, how many templates does each person have?

172
00:16:49,420 --> 00:16:55,570
So the elegance of longitudinal data methods is that we can use all the data that are complete within a time point,

173
00:16:56,470 --> 00:17:02,830
so we can include individuals for the time points at which they have information, even if they don't have data for all time points.

174
00:17:03,440 --> 00:17:11,850
Right. But the question is, should we? What do we think about people who are missing data relative to people who don't have missing data?

175
00:17:12,840 --> 00:17:15,930
Are they both from the same random sample of individuals?

176
00:17:16,770 --> 00:17:21,330
Again, just to give you an example of this type of missing is some of the labor pain data that we looked at.

177
00:17:22,230 --> 00:17:30,330
It's been a while, but when we are doing linear mix models, again following women during labor and their pain scores from 0 to 100,

178
00:17:30,960 --> 00:17:36,210
I put circles at trajectories where suddenly the woman no longer had observations any more pain scores.

179
00:17:37,410 --> 00:17:41,670
Again, more than likely, these of the study had a systematic form of missing this.

180
00:17:42,390 --> 00:17:44,070
That's a problem that probably came first.

181
00:17:45,090 --> 00:17:51,750
But anyways, you can see that every woman has followed for a certain period of time and then we no longer have information on some of them.

182
00:17:52,230 --> 00:17:53,580
And the same way in the treatment group.

183
00:17:54,180 --> 00:18:01,530
Again, harder to see, but there are trajectories where things suddenly end and maybe a two and a half hours for the circle there.

184
00:18:01,680 --> 00:18:06,809
There's one up there, also two and a half hours. So there's some missing that's going on here.

185
00:18:06,810 --> 00:18:13,650
We don't have complete information on all these women. For instance, this woman here, we don't know what her pain score had been.

186
00:18:13,650 --> 00:18:16,740
Had we continued to follow her, we have no idea.

187
00:18:17,250 --> 00:18:22,170
And again, the silliness of last observation carried forward is why would you think that it would stay at a flatline?

188
00:18:23,100 --> 00:18:29,360
That's what this observation carried forward since same way here, this woman is going up and up and then she drops out.

189
00:18:29,400 --> 00:18:31,950
Why would you suddenly things that her pain score would stay flat?

190
00:18:32,670 --> 00:18:38,800
But that's a press observation carried forward, since this is a specific type of missing this.

191
00:18:38,820 --> 00:18:42,510
Again, this is more of this nomenclature than for anything for you to worry about at this point.

192
00:18:42,990 --> 00:18:50,760
But when we have a person who drops, who has a missing observation and all future observations are missing, that's called monitoring.

193
00:18:50,760 --> 00:18:57,360
Missing us. Got to put a name on it, I guess. We talk about folks said I just use that word to drop out.

194
00:18:57,480 --> 00:19:04,140
Once a person leaves the study and is never heard from again. For the research, we consider them to be dropouts.

195
00:19:04,150 --> 00:19:11,220
We also talk about less to follow up. And if you're taking survival analysis next semester, you'll hear a lot about loss to follow up.

196
00:19:12,450 --> 00:19:16,980
We don't know what happened to this person after they left our study. We need those in those outcomes.

197
00:19:17,880 --> 00:19:21,150
So when the missing nurse pattern is more sporadic, which is more common,

198
00:19:21,150 --> 00:19:24,600
people come in and out of their outcomes sometimes are just randomly missing a visit.

199
00:19:24,600 --> 00:19:30,030
Or, like I said, one of their vials in the freezer broke and all the information was lost.

200
00:19:30,450 --> 00:19:36,929
We talk about non-monetary messiness, but the labor pain data had a march on missing this pattern,

201
00:19:36,930 --> 00:19:39,930
and I believe the data set in homework six also is monitoring.

202
00:19:42,870 --> 00:19:46,230
So again, bias versus variance. What are we worried about with missing data?

203
00:19:46,740 --> 00:19:50,670
Well, the impact of missing data is obvious with regards to variance.

204
00:19:51,480 --> 00:19:56,340
When we have less less of the outcomes or missing covariance, we have less information, less certainty.

205
00:19:56,730 --> 00:20:04,959
Right. We have less statistical power. When we design a longitudinal study and we get a sample size, we say to adjust it, this should bring that in.

206
00:20:04,960 --> 00:20:12,250
That's a great example of longitudinal study design, looking at development of food allergies in a birth cohort over time.

207
00:20:12,550 --> 00:20:16,900
But the sample size I gave them was very, very liberal.

208
00:20:16,900 --> 00:20:21,700
In fact, it's very, very liberal. I assumed every child at three years would have an outcome,

209
00:20:22,300 --> 00:20:28,720
which we know that over three years there are lots of reasons why you stop participating in a study when you have a newborn,

210
00:20:29,090 --> 00:20:37,240
but it's not usually your biggest priority. But anyway, so when we talk about power, we talk about the last time point,

211
00:20:37,420 --> 00:20:41,980
how many people are going to going to finish our study and have all of the outcomes available to us?

212
00:20:42,370 --> 00:20:45,460
When we start losing outcomes, we start losing statistical power.

213
00:20:46,700 --> 00:20:56,030
With bias. The impact of missing this is less obvious, and we talk a lot about imputation methods with missing data.

214
00:20:56,600 --> 00:21:01,370
Imputation is a way to get the missing data to reappear so that our power goes up.

215
00:21:01,820 --> 00:21:07,010
And it's also a way to fill in the missing data with valid observations.

216
00:21:07,270 --> 00:21:10,910
Right. That's the big issue. What do you fill in missing data with?

217
00:21:10,940 --> 00:21:14,580
I work with investigators all the time who have missing data.

218
00:21:14,600 --> 00:21:18,110
Well, why don't you just compute then time impute them with what?

219
00:21:19,580 --> 00:21:23,030
We have 20 people in this dataset. What do you want me to impute on?

220
00:21:24,140 --> 00:21:33,110
It's really not a straightforward process, but if you have missing data, we can have biased results in our analysis.

221
00:21:34,040 --> 00:21:38,419
If the missing is the systematic, again, this was where the typo in my slides was.

222
00:21:38,420 --> 00:21:44,260
I said, I'm biased. Are people dropping out of your study or missing observations because of the treatment they're getting?

223
00:21:45,940 --> 00:21:50,070
Is it because of how severe their outcome is? I feel really lousy.

224
00:21:50,080 --> 00:21:54,670
I've been in this study for a year. I don't feel any better. Or are you getting really sick?

225
00:21:55,360 --> 00:21:58,960
Why would I continue? Right. It's not working for me.

226
00:21:59,470 --> 00:22:05,320
Well, that's a problem if you're leaving the study, because the outcome we're interested in is getting worse.

227
00:22:05,320 --> 00:22:08,320
And that's the reason you're leaving. That can lead to some bias.

228
00:22:08,560 --> 00:22:16,060
Is it other characteristics? Is it a certain group of individuals, a certain age, certain sex, maybe socioeconomic relationship?

229
00:22:17,560 --> 00:22:21,310
And worse case scenario is, is that due to factors we didn't even think to measure?

230
00:22:22,420 --> 00:22:28,809
Can we even not even can we not even figure out why people are leaving the study or not showing up or why they're missing?

231
00:22:28,810 --> 00:22:35,590
This is occurring. And so now we're going to get a little technical here and use some of the missing

232
00:22:35,590 --> 00:22:42,430
data world technology R notation and talk about likelihood based methods.

233
00:22:42,760 --> 00:22:45,070
So again, we're talking about methods that use a likelihood.

234
00:22:46,740 --> 00:22:52,240
Remix models, linear models, generalized models, generalized linear mixed models, anything but g.

235
00:22:52,260 --> 00:22:56,040
Amber G is a marginal marginal model approach.

236
00:22:56,610 --> 00:23:02,309
So that's the end for these slides. This again, is just again to give you an introduction to the idea of bias versus variance.

237
00:23:02,310 --> 00:23:10,590
We are missing data. Why do we care from a statistical standpoint, let's examine a little more deeply what we're talking about.

238
00:23:12,270 --> 00:23:18,270
If any of you had any exposure to missing data terms, missing completely at random, missing at random.

239
00:23:18,270 --> 00:23:27,960
And of course, they make they sound very similar, right? Undergrad class, you're here, you're in 620 and we talked about or 620.

240
00:23:28,350 --> 00:23:32,400
I keep forgetting we have all these new courses. So talk about immediacy.

241
00:23:32,610 --> 00:23:37,260
What's that you talked about? Well, we're going to talk about it again.

242
00:23:37,650 --> 00:23:43,340
This is a this I think is one of the hardest. Sets of terminology to get your heads around.

243
00:23:43,640 --> 00:23:49,920
Why did I close all of our plants? And I can be honest with you, when I got into grad school,

244
00:23:49,920 --> 00:23:59,370
I had very little exposure to this and I didn't know what the heck MKR was and MKR and things like so from the beginning.

245
00:23:59,370 --> 00:24:04,420
So let's try and make these things a little clearer. Right.

246
00:24:04,750 --> 00:24:07,150
And you'll probably see this as 699 next semester.

247
00:24:07,270 --> 00:24:14,620
This is usually a topic that gets reviewed because it's not covered deeply in our curriculum, at least that unlike a dedicated course.

248
00:24:16,420 --> 00:24:19,749
Likelihood based methods. So we have a joint distribution.

249
00:24:19,750 --> 00:24:26,260
Why I again member is a vector of outcomes on a person. We're talking about a joint distribution, multivariate normal, for example.

250
00:24:27,310 --> 00:24:31,300
We think that these observations have a distribution that is conditional on covariance.

251
00:24:33,430 --> 00:24:36,810
So these are the outcomes. There are covariates in the mean model, right?

252
00:24:36,820 --> 00:24:43,270
We think about a linear mixed model. We say that the expected value of Y is a function of a linear combination of covariance.

253
00:24:43,840 --> 00:24:48,730
We can incorporate random effects like again, we're talking about multivariate normal distribution.

254
00:24:49,420 --> 00:24:54,370
There are parameters in the mean model, the fixed effects, and there are parameters in the random effects.

255
00:24:54,610 --> 00:24:58,600
So the variances of the random effects or correlation coefficient, those sorts of things.

256
00:24:59,560 --> 00:25:03,459
So this is a model for the outcomes, this is the model we've been fitting that we analyze in.

257
00:25:03,460 --> 00:25:08,410
R We also need to think about why observations are missing.

258
00:25:08,410 --> 00:25:15,850
So we need a model for the mRNAs. So again, array is a vector of missing is indicators for every observation.

259
00:25:15,850 --> 00:25:23,590
For every observation we have a binary variable, it's a one if that observation is missing or zero if it is not.

260
00:25:24,490 --> 00:25:28,090
Okay. So again, if we have a Y, we also know that R zero.

261
00:25:28,090 --> 00:25:33,040
If we don't have a Y, we don't have the Y, but we have that our right, ours is one.

262
00:25:34,330 --> 00:25:37,930
So again, this is a of you can think of this as a logistic regression model.

263
00:25:38,830 --> 00:25:42,010
Y is what's the probability that an observation is missing?

264
00:25:42,700 --> 00:25:46,810
Well, it could be a function of the observation. The outcome then it corresponds to.

265
00:25:48,080 --> 00:25:52,550
It could be a function of covariates in the mean model, either in the fixed effects or the random effects.

266
00:25:53,450 --> 00:25:57,319
It could be covariates that we didn't even think to measure, or maybe that we measured.

267
00:25:57,320 --> 00:26:02,660
Not because they go in the mean model, but because they explain why observations are missing.

268
00:26:03,500 --> 00:26:06,750
So this is important. There are lots of reasons why there's missing this.

269
00:26:06,750 --> 00:26:13,390
That could be due to the outcomes covariates in our model or other covariates that are not in our models for outcomes.

270
00:26:13,400 --> 00:26:16,400
And then there are parameters, right? This is a logistic regression model.

271
00:26:17,090 --> 00:26:20,870
There's a parameter up here corresponding to a core like age.

272
00:26:21,140 --> 00:26:24,770
There's also a parameter known here corresponding to age and are different parameters.

273
00:26:26,030 --> 00:26:31,130
This is explaining the messiness for X and this is explaining our axes related to the outcome.

274
00:26:33,170 --> 00:26:38,450
So we've got these two things here. This is some sort of, you know, bunch of binary things.

275
00:26:38,450 --> 00:26:44,680
And this could be maybe a healthy, very normal. We're also going to now take our vector of outcomes.

276
00:26:45,820 --> 00:26:49,780
Again, all this is theoretically right. We don't have the missing values in data.

277
00:26:49,780 --> 00:26:51,100
We never have the missing values.

278
00:26:51,460 --> 00:26:57,820
But if you think about it, theoretically, what you had planned to have, you can partition the vector of outcomes into those that are observed.

279
00:26:58,120 --> 00:27:03,700
Y I was an oh so superscript and those that are missing with a capital m superscript.

280
00:27:05,230 --> 00:27:11,650
So again those in the observed vector have an r i indicator zero meaning they are observed again are

281
00:27:11,650 --> 00:27:18,220
is indicator of missing this and are as one for the observations and for vector of missing outcomes.

282
00:27:21,140 --> 00:27:24,960
Our observed data. Will be.

283
00:27:26,190 --> 00:27:29,959
The outcomes that are observable. But we have all of the our values, right.

284
00:27:29,960 --> 00:27:33,180
And as I just said, even if Y is missing, we still know that our is one.

285
00:27:33,840 --> 00:27:38,490
So we have all of the missing US indicators and we only have the observed lives we're missing.

286
00:27:38,970 --> 00:27:46,950
We don't have the missing values. So let's talk about the density for the observed data that we have at hand.

287
00:27:47,370 --> 00:27:48,629
What is the distribution,

288
00:27:48,630 --> 00:27:56,940
joint distribution of the observed outcomes and the missing US indicators conditional on everything else covariance in X, covariance and Z,

289
00:27:57,330 --> 00:28:01,569
covariance and w the regression parameters in the mean model,

290
00:28:01,570 --> 00:28:09,630
the regression of the parameters omega and the random effects, and then C are the parameters modeling the missing S itself.

291
00:28:10,440 --> 00:28:14,819
So then joint distribution up there, that's a 6 to 1 concept right there from biostatistics six.

292
00:28:14,820 --> 00:28:18,360
So on the joint distribution, if we ignore all the conditioning,

293
00:28:19,620 --> 00:28:30,390
the binary distribution of y super oh and right is simply the marginal of y times, the distribution of our given y, that's a joint distribution.

294
00:28:32,020 --> 00:28:38,620
And then I'm going to integrate out all the missing data, the distribution over the missing data, because I want the observed.

295
00:28:40,090 --> 00:28:45,999
So that algebra right there is simply taking a joint distribution and turning it into a marginal times,

296
00:28:46,000 --> 00:28:50,710
a conditional and integrating over the missing data. Right.

297
00:28:51,880 --> 00:28:58,120
What's the point of all that? We're going to look at that formula and look at different forms of missing ness.

298
00:28:58,360 --> 00:29:03,010
So the first type of missing data mechanism is called missing completely at random.

299
00:29:03,730 --> 00:29:07,420
So if missing, this is unrelated to everything in the outcomes.

300
00:29:07,870 --> 00:29:13,060
So if missing is has nothing to do with the missing outcomes and it has nothing to do with the outcomes you've measured.

301
00:29:14,710 --> 00:29:23,080
Then we say business is completely at random, and that means that the missing US mechanism conditional on everything is equal

302
00:29:23,080 --> 00:29:27,550
to the missing is missing this mechanism without the wide conditioning on the Y.

303
00:29:28,820 --> 00:29:34,130
It's independent of why, given all the other stuff that's called myths completely at random.

304
00:29:35,210 --> 00:29:39,620
So with MKR missing, this can still be a function of observed covariance.

305
00:29:40,400 --> 00:29:45,470
So again, MCI MKR doesn't mean that missing this is simply 5050.

306
00:29:47,660 --> 00:29:51,170
It happens with respect to the model on covariance, for example.

307
00:29:53,080 --> 00:29:57,610
So MKR can be imposed by design based upon features of study participants.

308
00:29:58,480 --> 00:30:03,700
You may think of a longitudinal study in which you say, You know what? For those people, I'm not going to measure time to.

309
00:30:04,240 --> 00:30:07,790
And for these people, I'm not going to measure time for. Okay.

310
00:30:08,180 --> 00:30:12,169
So you can systematically input missing this into a longitudinal study as long

311
00:30:12,170 --> 00:30:18,080
as it's conditional and not on certain covariate that you're going to measure. Why do we care?

312
00:30:18,620 --> 00:30:23,330
Well, because again with MKR, what happens to what we do when we analyze?

313
00:30:23,730 --> 00:30:27,980
All we have at hand are the observed wise when we do our analysis.

314
00:30:30,570 --> 00:30:37,320
Do we get the right estimates for the regression parameters which modeled all of the what was going on on all the ones.

315
00:30:38,040 --> 00:30:41,819
So the joint distribution of my observed outcomes and then missing those indicators

316
00:30:41,820 --> 00:30:46,000
conditional on everything else is that formula I wrote up on the previous page.

317
00:30:46,020 --> 00:30:50,040
Right. A joint is simply a marginal times, a conditional.

318
00:30:50,040 --> 00:30:55,650
And then we get rid of the missing data dependent on the missing data. So that second line there moves into the third line.

319
00:30:56,520 --> 00:31:04,110
I've taken if we have NCAR, then this section right here, I can't.

320
00:31:04,650 --> 00:31:09,090
No, don't do that. This right here is the missing data mechanism.

321
00:31:09,090 --> 00:31:12,360
I said it wasn't conditional on Y anymore. It's MKR.

322
00:31:13,620 --> 00:31:17,220
So we now have this. There's no conditioning on the outcomes anymore.

323
00:31:17,910 --> 00:31:27,150
And then I have this other piece right here, over here. And so what happens is that this has nothing to do with the parameters I'm interested in.

324
00:31:27,170 --> 00:31:31,760
There's no betas, there's no omegas, there's no random effect parameters.

325
00:31:31,760 --> 00:31:44,060
There's no mean effect parameters anywhere. And so the likelihood that we have is proportional to the likelihood that we've been using in this class.

326
00:31:44,330 --> 00:31:49,280
We simply modeled the observed wise in whatever model we wanted to choose.

327
00:31:49,700 --> 00:31:53,870
Okay. So again, theoretically, MKR.

328
00:31:55,050 --> 00:31:59,370
Leads to unbiased estimates for the parameters that we're interested in.

329
00:32:00,090 --> 00:32:05,850
So any likelihood based method will lead to unbiased estimation.

330
00:32:06,330 --> 00:32:13,930
As long as the messiness is MKR. It's a pretty strong assumption.

331
00:32:16,320 --> 00:32:22,860
But it's a assumption that is made all the time, not because it's valid, but because it lets us do what we normally do.

332
00:32:22,860 --> 00:32:27,959
We just ignore the missing outcomes, we just analyze what we have, and that is perfectly fine.

333
00:32:27,960 --> 00:32:32,490
As long as the missing data mechanism is in the air, doesn't depend on the outcomes.

334
00:32:34,050 --> 00:32:43,920
However, suppose that the reason future observations are missing is because of the outcomes we have measured.

335
00:32:44,880 --> 00:32:48,870
I can think of somebody whose score is getting worse and worse and worse, indicating worsening health.

336
00:32:50,070 --> 00:32:56,430
And then they drop out of the study. Well, those future outcomes may be dependent upon what was already happening to that person.

337
00:32:57,870 --> 00:33:05,610
So that's NPR. So we now say missing this is related not only to covariates, but also to the outcomes that we have measured.

338
00:33:06,210 --> 00:33:14,820
But it has nothing to do with the unmeasured outcomes. So the reason someone didn't come into the clinic isn't because of how sick they are now.

339
00:33:16,060 --> 00:33:22,210
He didn't come into the clinic because of how sick they were before. Very practical terms.

340
00:33:23,440 --> 00:33:31,629
So that is Emma. So now the missing data mechanism, again, this is again some sort of logistic regression model that has all of these things in it.

341
00:33:31,630 --> 00:33:38,920
And there are parameters that is reduced to only conditioning on the observed outcomes, not the missing outcomes.

342
00:33:40,040 --> 00:33:46,040
But a spoken issue, all kinds of covariance, some that are in the model for Y and some that are not.

343
00:33:47,730 --> 00:33:51,090
So we also are sometimes because we need more words.

344
00:33:51,690 --> 00:33:57,690
It's also referred to as ignorable messiness. And that again is because if we do some algebra again,

345
00:33:57,690 --> 00:34:03,870
here is the likelihood we have at our disposal for all of the data we have collected, the outcomes observed in the messiness.

346
00:34:04,590 --> 00:34:15,049
But again, this is from before. We can write that as again, we're going to take the missing values conditional on the observed times,

347
00:34:15,050 --> 00:34:20,750
the density of the observed times, the density of the missing is conditional on Y.

348
00:34:21,500 --> 00:34:25,040
And then we're going to integrate up the missing data. So, again, all of this.

349
00:34:26,150 --> 00:34:32,030
Is simply an attempt to get back to this. And what we end up with is a product of two likelihoods.

350
00:34:33,140 --> 00:34:36,170
We end up with a likelihood for the observed outcomes.

351
00:34:37,870 --> 00:34:42,580
Congressional. Again, this is the thing we've always been doing. This is my linear, mixed model.

352
00:34:43,890 --> 00:34:50,220
How do I model the observed way is as a function of covariates and random effects, effects, effects and random effects.

353
00:34:51,150 --> 00:34:54,900
And then we have this other question here. Again, this is the missing data mechanism.

354
00:34:56,370 --> 00:35:00,940
No observation is missing conditional on observed outcomes and covariates.

355
00:35:01,140 --> 00:35:08,220
Again, there's parameters of this model. So the linear mixed model times may be some sort of logistic regression model.

356
00:35:10,470 --> 00:35:14,460
So the observed neighborhood, like I said, the data likelihood is the product of these two things.

357
00:35:14,700 --> 00:35:18,330
The likelihood that we've always been using and what are essentially weights.

358
00:35:20,700 --> 00:35:24,450
Right. So inference for the things we're interested in.

359
00:35:25,470 --> 00:35:27,840
Beta parameters again are the fixed effect parameters.

360
00:35:28,050 --> 00:35:33,900
Omega R The random effect variances or correlation coefficient is still unaffected by the missing

361
00:35:33,900 --> 00:35:39,840
this mechanism because it factors out of the likelihood we can just focus on that stuff.

362
00:35:40,260 --> 00:35:45,080
The other stuff doesn't have beta and omega in it. There's going to be no reason.

363
00:35:45,090 --> 00:35:50,880
However, they are useful. They are weights so we can ignore them or we can we can model them.

364
00:35:51,240 --> 00:35:55,830
But again, we're not going to get into this question too much on missing data, like I said.

365
00:35:57,060 --> 00:36:03,570
But we can think of a model here. We can come up with weights and try and see how that affects our inference.

366
00:36:05,440 --> 00:36:08,919
The worst case scenario. So again, we've come up with these terms.

367
00:36:08,920 --> 00:36:13,300
Somebody came up with these two horrible terms and they are and and they are.

368
00:36:15,340 --> 00:36:21,760
These concepts allow us to justify what we're doing when we analyze the observed data and ignore the missing data.

369
00:36:22,870 --> 00:36:26,620
If missing this is also dependent upon the values that are missing.

370
00:36:28,150 --> 00:36:31,570
Then we have what we call missing.

371
00:36:31,580 --> 00:36:37,450
Not in random. I was going to say not missing at random. Missing not at random and a r.

372
00:36:38,680 --> 00:36:42,100
So how can we account for missing this if we don't have the data that explain it?

373
00:36:42,550 --> 00:36:55,100
That's a big problem. So with M&A are M&A are missing not at random likelihood based methods can but aren't necessarily and have incorrect inference.

374
00:36:55,490 --> 00:37:00,950
Right. We just don't know don't know how it's going to impact what we're going to do.

375
00:37:01,730 --> 00:37:08,900
So we typically don't assume and are because that families are scratching our heads as if we've done something valid or not.

376
00:37:09,950 --> 00:37:13,640
And so many are also referred to as non ignorable missing this.

377
00:37:14,540 --> 00:37:19,580
So there's MKR and then there's ignorable missing us and then there's not ignoring the signals.

378
00:37:20,690 --> 00:37:24,080
If you'd rather stick with those terms, I'll start with them.

379
00:37:25,640 --> 00:37:30,860
Again, when I was in grad school, I had a really hard time trying to get my head around all this and stay away from all those integrals.

380
00:37:32,390 --> 00:37:34,600
Here's an example of what we're talking about here.

381
00:37:34,610 --> 00:37:39,980
So MKR would occur because a participant fails to come into their clinic because the car broke down.

382
00:37:41,170 --> 00:37:44,540
Right. You could think of a covariant that says kind of broke down, yes or no.

383
00:37:45,680 --> 00:37:51,290
MKR says the participant is declining, is declining with regard to primary outcome.

384
00:37:53,060 --> 00:37:57,530
They come anyway to your clinic. They give you an outcome, but then they don't come back.

385
00:37:59,080 --> 00:38:05,170
And then there says, well, the first participant isn't feeling well and doesn't come now, nor do they ever come back.

386
00:38:06,670 --> 00:38:13,180
So you can see that more and more are missing at random and missing not at random are really close to each other.

387
00:38:14,760 --> 00:38:22,210
So how do you know that missing of an outcome is due to the current missing outcome versus only the previous observed outcomes?

388
00:38:23,830 --> 00:38:31,420
Yes, there is that. You've lost out, though. Is it an air versus an air just like one time in the future?

389
00:38:31,450 --> 00:38:41,200
Yeah. It's conditional on what you've already observed in the past, basically.

390
00:38:41,710 --> 00:38:49,300
So like if this is a Tuesday, Thursday kind of a thing, on Tuesday they could be an hour and then on Thursday it could be anything else.

391
00:38:50,110 --> 00:38:55,360
Sure. Cool. The problem is, is you don't you can't really figure that out, right?

392
00:38:55,370 --> 00:38:59,080
You can't tease that out with any sort of test or data analysis method.

393
00:38:59,350 --> 00:39:01,149
So just to clarify, then, what was just said.

394
00:39:01,150 --> 00:39:08,970
So the are it's conditioning on all previous stuff, but not the current outcome because you don't have it in you.

395
00:39:09,010 --> 00:39:12,330
So American conditions on the current outcome. Okay.

396
00:39:12,700 --> 00:39:19,100
And they are not conditional words. Yes.

397
00:39:19,910 --> 00:39:26,690
So that it's a challenging there's again, there's no way to really figure that out from any sort of data analysis method.

398
00:39:26,930 --> 00:39:34,240
It's an assumption. So if you were at the end of a study, then would everyone be and then or.

399
00:39:35,400 --> 00:39:40,340
Well, usually you kill the you kill the study that's going to be a black version is a humorous part.

400
00:39:40,410 --> 00:39:47,280
Like if you finish the study, you don't consider anything future being missing because you define that to be the end of the study.

401
00:39:49,880 --> 00:39:58,340
That helped. I mean theoretically in life I guess there whole and then they are but for the purposes of research collection they've all been observed.

402
00:39:59,210 --> 00:40:04,430
You have to find it. You have to find. When to follow people every month for a year or month.

403
00:40:04,430 --> 00:40:09,320
12 after we're done with month 12. If they have all the observed data, there's nothing missing there.

404
00:40:11,390 --> 00:40:18,350
So very challenging problem and there's lots and lots of cool papers out there, but we're not going to get into in this class.

405
00:40:18,350 --> 00:40:21,680
That, again, would be great for a master's level course in missing data.

406
00:40:24,640 --> 00:40:32,260
So linear, mixed models, as I just said, and generalized linear mixed models, as well as linear models and generalized linear models.

407
00:40:32,260 --> 00:40:41,710
If you get rid of the mixed for there both likelihood based methods, we start with a likelihood to take the log and take the derivative and so forth.

408
00:40:42,550 --> 00:40:48,060
That's some likelihood analysis using observed outcomes as valid under MKR and MRA.

409
00:40:48,100 --> 00:40:49,990
I just showed you if you have a likelihood.

410
00:40:51,230 --> 00:40:58,280
Then we just arrived that what we do when we analyze the observed data leads to valid things through a likelihood based approach.

411
00:40:59,820 --> 00:41:04,200
Analysis using observed outcomes may or may not be valid under our.

412
00:41:06,170 --> 00:41:10,020
So this is one of the restrictions of G. G is not a likelihood based method.

413
00:41:10,040 --> 00:41:13,700
We never specify a joint distribution when we use Genie.

414
00:41:15,120 --> 00:41:19,560
So analysis using observed outcomes is always valid under M.S. They are.

415
00:41:22,190 --> 00:41:28,410
But. Validity for MMR had to be justified through that likelihood.

416
00:41:29,690 --> 00:41:41,480
And since we don't have a likelihood, we can't justify Gee's validity under a air missing at random again.

417
00:41:42,740 --> 00:41:50,030
We talk about bias in this theoretic of classes, but you always have to ask yourself, how much bias are we talking about?

418
00:41:50,990 --> 00:41:57,530
Right? If the outcomes are on the scale of ten in, the bias of an estimate is point to 10.2.

419
00:41:57,920 --> 00:42:01,280
Sorry, Greg, that's not a lot of bias. There is some bias.

420
00:42:01,350 --> 00:42:07,340
But again, I had some Q cool simulations to show you what happens when data are missing at random,

421
00:42:07,350 --> 00:42:10,370
missing completely at random, and how they affect what we do.

422
00:42:11,180 --> 00:42:15,410
We'll see if we have time for that again to look at what we're talking about here theoretically.

423
00:42:15,440 --> 00:42:18,520
So, gee. You're safe.

424
00:42:18,530 --> 00:42:25,130
If the data are MKR, you're not necessarily safe under any other data missing data mechanism.

425
00:42:26,750 --> 00:42:33,140
And that is not to ever sway someone from using G versus a linear mixed model.

426
00:42:33,560 --> 00:42:40,040
I don't want to hear someone say, Well, I'm always going to use a land based model because it's less requires less missing those assumptions.

427
00:42:40,850 --> 00:42:52,389
And that's not the way to go. Because let's be quite honest how many times our data really M-A are versus and they are they're probably and

428
00:42:52,390 --> 00:42:59,900
and they are they're probably missing that a random for many cases but again just to get more specific here,

429
00:43:00,350 --> 00:43:05,780
as I've said with G.E., we don't specify a multiple we don't have a multivariate Poisson distribution.

430
00:43:06,320 --> 00:43:11,510
We have marginal Poisson distributions. And then we try to correlate things through this this working correlation structure.

431
00:43:12,320 --> 00:43:17,720
So we specified marginal models for the mean of the variance, and we have this working correlation matrix.

432
00:43:17,810 --> 00:43:19,210
So when we set G,

433
00:43:19,220 --> 00:43:28,820
we're assuming that marginally each observation has a certain mean conditional on this missing this indicator as well as the variances.

434
00:43:29,570 --> 00:43:34,460
And that is only valid with m CAIR and CAIR.

435
00:43:34,820 --> 00:43:45,020
There's no NCR with an error. We know that the missing US mechanism as a function of observed outcomes, this function is going to have to be included.

436
00:43:45,080 --> 00:43:48,170
Unlike the normal case.

437
00:43:48,620 --> 00:43:53,299
When I showed you earlier, that thing was like a weight that could be ignored in May.

438
00:43:53,300 --> 00:43:57,800
Ah, that we can't be ignored anymore. It shows up in the estimating equation.

439
00:43:58,460 --> 00:44:04,340
And so there's a whole body of research on weighted methods and lots of papers.

440
00:44:04,340 --> 00:44:11,240
And that method, if we want to talk about how we could use G, if we have any, and if we have MMR,

441
00:44:11,870 --> 00:44:16,970
then we have to take our G function and tweak it to account for the missing this which can't be done.

442
00:44:17,990 --> 00:44:24,440
Yeah, we still have time in this class to try and attempt that to look at this another way.

443
00:44:24,440 --> 00:44:27,800
And this was a way that helps me understand these things better when I was learning them.

444
00:44:29,090 --> 00:44:33,409
How do we look at missing this when you have missing completely at random?

445
00:44:33,410 --> 00:44:39,770
What are you saying? You're saying that participants with fully observed outcomes are a random sample from the population.

446
00:44:40,920 --> 00:44:45,060
And you're saying that participants with missing outcomes are also a random sample from the population?

447
00:44:46,150 --> 00:44:50,410
It just didn't happen to to get them. But they're ready. They could be a random sample from the population.

448
00:44:50,770 --> 00:44:56,260
So the observed outcomes. Can be thought of as a random sample of all of the outcomes.

449
00:44:57,560 --> 00:45:05,550
And therefore the observations, the observed outcomes are simply a random sample from what we could have observed if there had been no mRNAs.

450
00:45:06,320 --> 00:45:10,600
So we still have a random sample from the population, essentially when there's a fire.

451
00:45:11,210 --> 00:45:13,360
So we get unbiased, but certainly it's inefficient.

452
00:45:13,370 --> 00:45:21,529
We lost some data in terms of the analysis, but a few of em are less than the participants to have fully observed.

453
00:45:21,530 --> 00:45:24,950
Outcomes are no longer a random sample from the population.

454
00:45:26,090 --> 00:45:30,800
The analysis is only valid if the joint distribution of all outcomes is correct.

455
00:45:32,180 --> 00:45:37,190
We assume when we fit a model again, we're assuming when we go and fit the exam.

456
00:45:38,120 --> 00:45:44,720
When you go fit a certain model to the Farkle data, you are assuming that model is correct and you're estimating the parameters from that model.

457
00:45:45,170 --> 00:45:49,490
So we've made an assumption that our models in the linear mix models in generalize intermix models are correct.

458
00:45:50,540 --> 00:45:55,550
And so therefore we have validity under MKR because we've gotten the right joint distribution.

459
00:45:56,680 --> 00:46:03,669
We don't want to assume our models are correct and gee, we don't have a joint model and therefore reality doesn't exist under MMR.

460
00:46:03,670 --> 00:46:05,560
We don't have a random sample anymore.

461
00:46:05,590 --> 00:46:14,110
The observations we have are a biased sample, but they come from a certain group of individuals who cared to have observed data.

462
00:46:16,910 --> 00:46:20,390
The end? Yes.

463
00:46:20,960 --> 00:46:29,540
So when you say that I'm NGO and are valid under and they are uniquely alone and I'm biased.

464
00:46:30,380 --> 00:46:37,160
I'm biased, yes. You mean models? Those models that are adjusting for the the factors that are missing.

465
00:46:37,160 --> 00:46:38,210
This is conditional on.

466
00:46:39,440 --> 00:46:46,639
No, I'm saying that if if I give you a dataset that have missing observations and you just go fit like you have in the homework,

467
00:46:46,640 --> 00:46:53,450
somebody who had missing data already in your homework, if you just go ahead and fit your model, yeah, your results are valid.

468
00:46:54,350 --> 00:46:58,370
As long as the data that are missing are either missing completely at random or at random.

469
00:46:59,680 --> 00:47:03,160
You didn't have to do anything else about the missing this. You don't have to model. It didn't have to think about it.

470
00:47:04,240 --> 00:47:07,780
You just ignored it. Right. You just analyze the outcomes you had.

471
00:47:08,650 --> 00:47:14,120
Yeah. The parameter estimates are unbiased.

472
00:47:15,890 --> 00:47:21,920
As long as the missing. This is completely at random or at random, so it might be a misunderstanding.

473
00:47:21,920 --> 00:47:24,559
There's another difference between missing and random.

474
00:47:24,560 --> 00:47:31,270
Missing not at random is that you have that you have the observable factors which are related to missing this.

475
00:47:31,280 --> 00:47:38,240
So let's divided into two pieces. Missing completely at random means you have all of the covariates necessary.

476
00:47:38,870 --> 00:47:43,190
The outcomes don't matter in terms of missing us. That's missing completely at random.

477
00:47:44,390 --> 00:47:50,900
It's completely missing. And random says you need covariates and the observed outcomes they impact missing this.

478
00:47:52,030 --> 00:47:55,630
Okay. So it's a very subtle difference between MKR and MSR.

479
00:47:56,770 --> 00:48:00,610
The question is whether the outcomes you have observed explain the messiness.

480
00:48:03,560 --> 00:48:08,330
But you do need those covariates in the model for elementary.

481
00:48:08,380 --> 00:48:11,660
Yeah, of course that's. That's true of anything, right? Any regression model.

482
00:48:11,660 --> 00:48:15,560
We said when we put covariates in the model, we think we have the right model.

483
00:48:16,430 --> 00:48:19,730
If you have the wrong covariates in the model, I mean, you're just be.

484
00:48:20,030 --> 00:48:25,610
But if you assume your model is correct and then you collect data to estimate the parameters,

485
00:48:26,630 --> 00:48:31,280
okay, then as long as the messiness fits into one of those two bins, you're good.

486
00:48:32,450 --> 00:48:37,160
Yes, sir. I think the confusion is I think this was sexy.

487
00:48:37,700 --> 00:48:42,200
You had like a five with different covariates for missing this.

488
00:48:43,340 --> 00:48:54,220
So I think. Is that like what you're saying is like, would you need those extra the so are you talking about how there is two Greek letters one for.

489
00:48:54,340 --> 00:48:59,000
Yeah, that there's a beta and there's a side.

490
00:48:59,420 --> 00:49:08,890
Right. Every time I. I wasn't thinking anything, but it's okay.

491
00:49:08,920 --> 00:49:15,340
I was just trying to help. And, you know, I think it may ah and mkr I did, I didn't, I didn't understand the difference between them either.

492
00:49:16,030 --> 00:49:20,410
One conditions on covariates, the other conditions on covariates and the observed outcomes.

493
00:49:21,000 --> 00:49:28,290
Okay. As long as the missing ness is due to data you have in your dataset.

494
00:49:30,000 --> 00:49:37,560
And you're fine. But the minute the missing, this is due to things that aren't in your dataset, which of course you don't know what they are.

495
00:49:38,100 --> 00:49:43,200
This is why we assume we have them. Because the minute you say, Oh, wait a minute, missing,

496
00:49:43,200 --> 00:49:51,360
this is probably due to some other factor that we didn't even think about or it's a due to the future outcomes that they're not going to give to us.

497
00:49:52,440 --> 00:50:00,630
Kind of stuck, right? Very stuck. So again, we assume M&M Air and Air.

498
00:50:01,850 --> 00:50:04,790
We assume those lots of the time, not because they're valid.

499
00:50:05,870 --> 00:50:11,030
But because they let us do what we need to do and believe that we're getting valid results.

500
00:50:14,700 --> 00:50:19,219
Are you clear? Siberian. Yes. Those two things. Right. One conditions only on covariates.

501
00:50:19,220 --> 00:50:24,380
One conditions on covariates and the observed outcomes. So a little bit stronger.

502
00:50:25,220 --> 00:50:31,020
We need more of the data. But in either case, likelihood based methods are fine.

503
00:50:31,950 --> 00:50:35,780
Marginal models, things like g are only valid with them.

504
00:50:36,270 --> 00:50:40,550
Are. Because with GE you need to model the messiness.

505
00:50:41,870 --> 00:50:47,090
If you have MRI, you need to models are missing this and those become weights in the estimating equation.

506
00:50:49,470 --> 00:50:57,660
And the genie functions in our do allow you to incorporate leads into something we we don't have time to do this semester.

507
00:51:04,680 --> 00:51:11,820
So if you have missing data and I think I'm going to throw in one more imputation method that's a little more straightforward.

508
00:51:12,480 --> 00:51:15,720
There's a package in our called mice. Yeah.

509
00:51:16,160 --> 00:51:19,600
Yeah, I use my ass all the time. No one knows how it works.

510
00:51:20,140 --> 00:51:25,270
That's the problem, right? Again, didn't exist when I was in grad school.

511
00:51:25,570 --> 00:51:28,990
There was no way to impute data when I was in grad school.

512
00:51:29,020 --> 00:51:32,530
We had a rough time anyway.

513
00:51:34,060 --> 00:51:38,140
Be careful when you impute imputation is a really cool idea.

514
00:51:38,740 --> 00:51:44,110
It is not a solution to a poorly designed study that has all kinds of missing data.

515
00:51:45,510 --> 00:51:51,850
Again, we're not going to get into all of those issues in this class for lack of time.

516
00:51:52,090 --> 00:51:56,440
But let's talk about what we mean when we talk about computing here.

517
00:51:58,300 --> 00:52:03,460
So a major issue for missing data in longitudinal data is the loss of efficiency.

518
00:52:03,820 --> 00:52:06,940
We just lose power because we don't have all the data we thought we would.

519
00:52:07,210 --> 00:52:12,270
So can we find a way to fill in the missing data so that our sample size can be increased to what it would have been?

520
00:52:13,840 --> 00:52:21,070
They also find that a lot of a lot of fields out there still don't like using computer data to make conclusions.

521
00:52:22,310 --> 00:52:29,450
I think you've going to convince an investigator that you're going to promote a policy based upon data that you made up.

522
00:52:30,510 --> 00:52:35,900
Yeah, that's the cynical view. Again, if you make it up smartly.

523
00:52:37,950 --> 00:52:45,960
It actually does a really nice job. So how do we fill in the missing data in a smart way that leads to valid inference and unbiased inference?

524
00:52:46,380 --> 00:52:52,560
So again, we've assumed that these missing indicators are actually is a function of observed covariance.

525
00:52:52,800 --> 00:52:59,400
So we just said this in a question earlier. Or observe covariance and the observed outcomes as they are.

526
00:53:00,660 --> 00:53:08,760
So can we estimate the parameters in this model and then generate standard or predicted values for the missing outcomes?

527
00:53:09,750 --> 00:53:19,560
That's what the imputation says. Can I somehow model the outcomes in the missing ness so that I can use that model and get parameter estimates?

528
00:53:20,400 --> 00:53:24,120
And can I plug in the things I need into the formula that I've measured?

529
00:53:24,960 --> 00:53:28,620
Multiply them by the parameters and compute an expected value.

530
00:53:28,950 --> 00:53:33,630
And I can plug that in for a missing value. Lots of ways to improve data.

531
00:53:34,860 --> 00:53:40,740
Some are more complex than others. Mice is a conditional regression models.

532
00:53:40,890 --> 00:53:44,280
I think back and forth get even though it really works, it's really confusing.

533
00:53:44,670 --> 00:53:48,450
I think purposely so. Caveats for imputation.

534
00:53:48,510 --> 00:53:52,640
Again, you need enough data to fill in the missing. This is missing.

535
00:53:52,650 --> 00:53:59,730
This is due to three covariance. You have to have enough people to fit that regression model, to model the messiness,

536
00:54:00,120 --> 00:54:06,720
and you have to have those covariance right when you've got all this missing data because the study was just done poorly.

537
00:54:08,100 --> 00:54:13,920
Imputation is not going to help. And I don't usually do imputation because I usually work with people with small datasets.

538
00:54:14,430 --> 00:54:23,370
So you need a lot of data to feel like you're fitting a model that is valid enough for you to fill in the missing data with a sufficient confidence.

539
00:54:24,150 --> 00:54:29,010
And you have to assume that you have identified the correct model for the missing and the covariates,

540
00:54:29,010 --> 00:54:33,240
and maybe the outcomes that go into that model get lots of assumptions.

541
00:54:35,200 --> 00:54:40,779
The simplest amputation method. It isn't even require a model is what I just said.

542
00:54:40,780 --> 00:54:47,530
Less observation carried for LCF. So any missing outcome is imputed by the most recently observed outcome in the past.

543
00:54:48,340 --> 00:54:55,149
So this looks a little more may be common sense and monitor missing ness when someone is observed and then they stop.

544
00:54:55,150 --> 00:55:00,220
So you just fill them in going forward. But what do you do with the people who are missing sporadically?

545
00:55:00,760 --> 00:55:03,820
Does it make sense to just carry that one over if the next one was really high?

546
00:55:05,080 --> 00:55:10,810
Again, think about what what we're doing here, especially common when dropouts occur monotonically like that.

547
00:55:12,730 --> 00:55:19,630
But this observation, this method makes a very strong and a wildly unlikely assumption,

548
00:55:20,140 --> 00:55:23,980
and that is that future outcomes for a person are perfectly correlated.

549
00:55:24,790 --> 00:55:29,770
If I get a 12, I'm going to get a 12 forever, right?

550
00:55:29,920 --> 00:55:35,950
Not even with a little bit of noise. No noise. Perfectly correlated by the previous observation.

551
00:55:37,470 --> 00:55:42,420
So that is a ridiculous assumption because nothing is measured perfectly in human beings.

552
00:55:43,380 --> 00:55:46,080
But there are other problems. So there's a bias problem.

553
00:55:46,110 --> 00:55:54,610
We got the wrong outcomes probably, and the variability in the imputed dataset is much less than it would be with the observed outcomes.

554
00:55:54,630 --> 00:56:00,320
Right. Everybody is no, there's no variability within a person for those future outcomes.

555
00:56:00,330 --> 00:56:04,740
It's just the same number. We know that there's always going to be variability within a person over time.

556
00:56:05,370 --> 00:56:06,690
We've been doing that all semester,

557
00:56:07,650 --> 00:56:14,370
so that leads to underestimation of standard errors because the data are not as noisy as they should be in reality,

558
00:56:14,370 --> 00:56:18,819
if you hadn't measured all of the outcomes. And so there was in-person correlation.

559
00:56:18,820 --> 00:56:22,330
Again, to look at this a different way with in-person correlation is artificially inflated.

560
00:56:22,690 --> 00:56:27,009
If somebody has the same observation many times that that intra class correlation

561
00:56:27,010 --> 00:56:31,270
goes up or intra individual correlation goes up and then infects senators.

562
00:56:32,200 --> 00:56:36,880
So there's a bias problem and there's a variance problem with this observation carried forward.

563
00:56:39,230 --> 00:56:43,370
So why do we do it? As I said, it's easy.

564
00:56:44,240 --> 00:56:53,060
Don't have to think. There is a myth out there that less observation carried forward leads to what people call tell me or conservative.

565
00:56:54,020 --> 00:57:00,200
You know, it doesn't overstate the treatment of it. Well, how do you know that, Laura?

566
00:57:00,200 --> 00:57:03,350
Type one error. I'm not going to find a significant result.

567
00:57:05,730 --> 00:57:08,760
At the expense of lower time to air. It's not true.

568
00:57:09,120 --> 00:57:12,389
There's no truth to that. And I'd love to find a paper.

569
00:57:12,390 --> 00:57:18,690
I know if anyone's ever read the paper to to conclusively speak to an implied audience as to why this is not a good approach.

570
00:57:20,010 --> 00:57:23,400
So even I'm going to ask you to do it in the homework assignment.

571
00:57:24,270 --> 00:57:28,230
Never use this method with missing data in longitudinal data.

572
00:57:28,530 --> 00:57:35,720
I also will read manuscripts where they will say, well, we only did the Lisa observation carried forward as a sensitivity.

573
00:57:35,730 --> 00:57:41,790
That's sensitivity analysis. That is a bad word these days in statistics.

574
00:57:42,000 --> 00:57:48,490
Sensitivity analysis. Right. That just means you did some other stupid method to make sure that what you have worked.

575
00:57:48,510 --> 00:57:53,280
I find in a lot of applications, right. There are good sensitivity analyzes out there.

576
00:57:54,240 --> 00:57:58,070
But people will say they use this method as a sensitivity analysis for their other results.

577
00:57:58,080 --> 00:58:02,970
What's using the observed data like? Well, how does that justify what you did?

578
00:58:03,000 --> 00:58:06,610
Because it's a dumb method to begin with. So estimates are biased.

579
00:58:06,610 --> 00:58:11,880
Standard error is wrong. So there are better ways to impute data.

580
00:58:14,200 --> 00:58:19,900
That aren't that hard to implement, and nowadays there are libraries to do all this.

581
00:58:21,430 --> 00:58:26,200
So let's look at the labor PIN data. So we had labor pain data, had monitoring missing.

582
00:58:26,320 --> 00:58:29,710
So once a woman didn't have a pain score, she didn't have any future pain scores.

583
00:58:30,550 --> 00:58:33,970
And what I looked at was the percentage of missing outcomes over time for both arms.

584
00:58:34,180 --> 00:58:41,110
So again, the first observation, 30 minutes, everybody's there, everybody's there for an hour, and then we start seeing dropouts.

585
00:58:42,460 --> 00:58:48,820
And again, we worried that there is if there is dropout, we hope that it's similar across both treatment arms.

586
00:58:49,360 --> 00:58:52,660
We certainly don't want dropout due to treatment or, you know, missing. That's due to treatment.

587
00:58:55,820 --> 00:58:58,920
Looks like maybe the control arm is has higher can in the study.

588
00:58:58,920 --> 00:58:59,970
This is a strange study.

589
00:59:00,690 --> 00:59:06,720
But I imagine the women in the control arm are dropping out because they're being really high and they also were giving birth.

590
00:59:07,890 --> 00:59:12,060
So at the end of the study, we've lost half of the outcomes. That's the other thing to show investigators.

591
00:59:12,420 --> 00:59:18,120
You have lost half of your people. The sample size is not the number of observations.

592
00:59:19,350 --> 00:59:24,840
It's a strange combination of the number of people and how many observations you have and the correlation.

593
00:59:25,740 --> 00:59:29,280
So there's a loss of statistical power there, especially at the final time point.

594
00:59:30,090 --> 00:59:32,580
Many times people will collect longitudinal data,

595
00:59:33,180 --> 00:59:38,970
but what they really want to do is look at the change from the beginning of the study to the end of the paired t test kind of thing.

596
00:59:40,830 --> 00:59:45,600
You don't have a lot of pairs by the end of the study. If you have less than half of the Paris left.

597
00:59:46,140 --> 00:59:53,310
So lots of power loss here. This plot we've seen before, these are just the again, the observed.

598
00:59:53,520 --> 01:00:00,020
These are the observed data. Here are the means. And I put standard error intervals and I connect to them.

599
01:00:00,030 --> 01:00:06,899
So it's not really a band, a confidence band, but we see that the mean is going up in the placebo in the placebo group and it's

600
01:00:06,900 --> 01:00:11,490
staying pretty flat in the treatment group over time based upon the observed outcomes.

601
01:00:12,510 --> 01:00:14,550
Let's do some amputation in a stupid way.

602
01:00:15,900 --> 01:00:22,139
I'm going to say every woman in the study and I'm going to say as soon as they drop out, take the previous observation and keep filling that in.

603
01:00:22,140 --> 01:00:27,180
So now I have a complete dataset. Every woman has all of the observations that every time point.

604
01:00:28,680 --> 01:00:32,970
And I did the same sort of plan. Computers means in the same way.

605
01:00:33,510 --> 01:00:37,820
So there's the mean for the observed. And here's the mean for Pelosi.

606
01:00:39,570 --> 01:00:46,440
And this is, again, fits into the mythological story that I've seen, is folks say, well, if there's a difference.

607
01:00:46,890 --> 01:00:49,200
Right. Using the observed data makes the difference bigger.

608
01:00:49,860 --> 01:00:54,480
But if I use less observation carried forward, it's going to naturally move the two groups closer together.

609
01:00:55,400 --> 01:01:00,050
That just happened in this dataset. And it's not this is no proof here.

610
01:01:00,890 --> 01:01:09,320
This is just one study where the control group happened to have happened to move down when we did last observation carried forward.

611
01:01:11,240 --> 01:01:14,810
So we see that the control group has a trajectory that changes less over time.

612
01:01:15,710 --> 01:01:21,590
When we filled in the missing data using less observation carried forward than just analyzing the observed data.

613
01:01:22,210 --> 01:01:26,090
So. So it's likely that those with the highest pain scores in the control arm dropped out.

614
01:01:26,750 --> 01:01:30,000
They probably were also close to giving birth. Missing.

615
01:01:30,000 --> 01:01:34,260
This could be M.R. It could be an error, but we have no way to figure that out.

616
01:01:37,030 --> 01:01:40,989
Several better ways and valid ways to improve the data. I said, not covered in this course.

617
01:01:40,990 --> 01:01:43,840
I'm going to try and do a couple of before the semester is over.

618
01:01:45,120 --> 01:01:52,500
And remember that every time you impute data, your results are conditional upon that one imputation.

619
01:01:53,040 --> 01:01:56,310
We'd like to average over the distribution of imputations.

620
01:01:58,390 --> 01:02:07,180
So the way to do that is certainly we can't average over the entire space of imputations, but we can do several data sets with imputations.

621
01:02:07,750 --> 01:02:14,080
So impute the data smartly, get results, impute them again, get a new set of results and do better hats.

622
01:02:15,210 --> 01:02:20,460
Third time can do better hats and you can average those results across several imputations.

623
01:02:20,760 --> 01:02:24,420
And so that's called multiple imputation for an obvious reason.

624
01:02:25,860 --> 01:02:28,980
The question is how many imputation should you do? And no idea.

625
01:02:29,970 --> 01:02:35,940
Five is seems to be enough. I don't know where five came from except that we have five fingers.

626
01:02:36,770 --> 01:02:40,350
Um. But you certainly should never impute the data once and be done.

627
01:02:41,220 --> 01:02:47,549
If you want to do imputation, you need to imputation several times and then you need to smartly average again.

628
01:02:47,550 --> 01:02:53,820
You have to get an average of eight ahead. You also have to figure out what is the standard error of that data.

629
01:02:54,090 --> 01:02:57,480
It's not just the average of the standard errors from the five analyzes.

630
01:02:57,960 --> 01:03:04,020
There's something called Reuben's Rules Down Reuben that created the correct variance computation.

631
01:03:04,920 --> 01:03:08,760
So you have to worry about variability within the data and variability across imputations.

632
01:03:09,510 --> 01:03:13,680
Again, the formula exists and it's not complicated, but that needs to be done.

633
01:03:16,060 --> 01:03:23,770
The other challenges with imputation are that if you do this for an applied analysis that goes into a manuscript.

634
01:03:24,760 --> 01:03:29,410
You have to make sure you can always reproduce those exact imputed datasets.

635
01:03:30,900 --> 01:03:34,920
Because it's no fun to get reviews that come back and ask you to do some changes.

636
01:03:36,490 --> 01:03:40,690
And then you didn't keep track of the imputed data and you have to re impute

637
01:03:40,690 --> 01:03:43,690
and you get slightly different results just because you have no imputations.

638
01:03:44,440 --> 01:03:49,630
That's no fun. Why is your standard error 1.03 when it used to be 1.04.

639
01:03:50,850 --> 01:03:54,390
But I don't know. Does it matter? Yes, it does.

640
01:03:54,400 --> 01:03:58,420
To someone who does understand statistics, they wonder why you changed.

641
01:03:58,430 --> 01:04:00,580
What are you doing? Do you know what you're doing?

642
01:04:01,580 --> 01:04:10,540
Okay, so computing again is really useful, but if you're going to use an application, make sure you can recreate the imputed data sets you used.

643
01:04:11,650 --> 01:04:15,820
It's just like anything. Whenever you re sample, you do the bootstrap.

644
01:04:15,820 --> 01:04:18,360
If you know that method, you've got to keep those.

645
01:04:18,370 --> 01:04:24,700
And if you work with studies that use imputed data sets, you will see that they will include several imputed datasets for you.

646
01:04:25,830 --> 01:04:34,280
They will record all that. Can keep track of that so that everybody is using the same computer data sets so that we can compare across analysis.

647
01:04:36,650 --> 01:04:42,790
That's it. Ten after four. We'll see you on Friday.

648
01:04:43,920 --> 01:04:47,230
And it's no longer our cloud studio. Our studio cloud.

649
01:04:47,320 --> 01:04:55,420
It's opposite no matter what a dumb name it's now.

650
01:04:55,420 --> 01:05:07,180
Posit Cloud. I've put some code up there that we're going to start working with on Friday, so get ready to analyze some data, stupid passwords.

651
01:05:08,660 --> 01:05:13,480
Public knowledge was very prevalent when I used.

