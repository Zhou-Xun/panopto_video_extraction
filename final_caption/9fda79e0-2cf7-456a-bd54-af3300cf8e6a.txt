1
00:00:01,490 --> 00:00:04,580
All right, that's all. That's all from me. Any questions?

2
00:00:05,950 --> 00:00:12,470
All right. We'll talk more about mid term probably next Monday.

3
00:00:13,070 --> 00:00:20,190
Next Monday? We still have class next Wednesday, no. Okay. Okay.

4
00:00:20,220 --> 00:00:27,450
Let's continue. So last time we were discussing how reversible Markov chain or reversible might.

5
00:00:43,060 --> 00:00:51,490
So there are a few things from general to special properties, a bold of reversible.

6
00:00:51,820 --> 00:00:57,940
So the idea of time reversible is you look at the back sequence off mark, off chain, right?

7
00:00:58,330 --> 00:01:04,690
So one thing is the most a general property we can establish is if you have a markov chain,

8
00:01:05,230 --> 00:01:09,580
regardless what kind of a property that Markov training has.

9
00:01:09,580 --> 00:01:16,240
But the reversible sequence is also has the Markov property.

10
00:01:17,140 --> 00:01:31,700
So we that's a general rule, that's a very general property back more often.

11
00:01:34,720 --> 00:01:38,950
And as such, it also has Markov property.

12
00:01:46,690 --> 00:01:51,340
So the second thing, we're taking this a little bit more.

13
00:01:53,830 --> 00:01:59,620
To a more specific case if we're looking at the back or sequence of according Markov chain.

14
00:02:04,690 --> 00:02:23,750
The sequence often meaning the underlying Markov chain will be you reduce of in any positive recurrent all of these.

15
00:02:25,040 --> 00:02:38,090
One of the so if you look at the back a sequence, there has to be an infinite sequence, which means that the boarding mark option has to you know,

16
00:02:38,900 --> 00:02:43,010
if you look at the back or sequence that the beginning of the vector sequence,

17
00:02:43,580 --> 00:02:52,309
the the it's commonly assumed or it's always implicitly assumed that it's already reaches the stationary distribution.

18
00:02:52,310 --> 00:02:59,900
So that's the special kind of assumption when we talk about back or sequence on the board.

19
00:03:00,100 --> 00:03:06,469
Mark Okay. Right. So it's well, the backwards sequence of avoiding Markov chain, well,

20
00:03:06,470 --> 00:03:15,440
always starts or always already in the station or a distribution station or distribution is a unique.

21
00:03:15,890 --> 00:03:24,230
And so if we look at this that we can actually calculate the back or sequence the,

22
00:03:24,440 --> 00:03:31,080
the kernel, the transition kernel of the back or sequence is defined by this.

23
00:03:31,280 --> 00:03:37,250
Q I'm trying to hide the j he.

24
00:03:46,410 --> 00:03:53,850
CGI provided by George II.

25
00:03:56,640 --> 00:04:07,800
So this is another general property that's for all these according to Markov chain, right.

26
00:04:08,190 --> 00:04:13,950
If you have a ballpark of change, if I look at the back more sequence than they are already.

27
00:04:13,960 --> 00:04:19,860
So if you look at the state marginal distribution on each random variable,

28
00:04:20,460 --> 00:04:25,470
the number random variable that they are already in these stationary distribution

29
00:04:26,280 --> 00:04:32,850
represented by page and by then the back or sequence can be conveniently computed by this.

30
00:04:33,960 --> 00:05:05,070
And then based on this we can we define reversible Markov by explicitly requesting the q ij equal to PJ Q.

31
00:05:09,140 --> 00:05:16,340
Therefore the detailed balance information. All right.

32
00:05:16,520 --> 00:05:25,129
So obviously the first one is true for Odin Mark off chain, second one, that's true for the old, the aborted Markov chain.

33
00:05:25,130 --> 00:05:35,720
But, you know, according Markov chain, it's just a subset of the third one is even special for just this subset of the awarding Markov chain.

34
00:05:36,170 --> 00:05:44,990
Nevertheless, this special we call this this this special type of the Markov chain exists and now we call them time reversible Markov training.

35
00:05:45,380 --> 00:05:48,650
So they are characterized by this detailed balance equation.

36
00:05:49,370 --> 00:05:51,830
Hi. I do. I.

37
00:05:58,940 --> 00:06:13,430
And again, the point is this particular sad that this special class of Markov chain is solely determined by the transition kernel because the PIs,

38
00:06:13,520 --> 00:06:20,930
the stationary distribution is determined by the transition crown, but according property is determined by the transition curve.

39
00:06:21,320 --> 00:06:26,180
And then this equation obviously also determined by the transition kernel.

40
00:06:26,180 --> 00:06:31,729
So this is how we define it, define a type of reversible Markov chain.

41
00:06:31,730 --> 00:06:35,060
So this class of Markov chain is special.

42
00:06:35,090 --> 00:06:37,850
We're going to use it in important applications.

43
00:06:38,300 --> 00:06:48,020
But before that we're talking about how do you identify what is the property of this time reversible Markov chain.

44
00:06:48,560 --> 00:06:53,840
So so the proposition is that to.

45
00:06:55,400 --> 00:07:04,850
So the proposition to actually turn things a little bit so how we so how do we so the general question posed

46
00:07:04,970 --> 00:07:16,370
is how we identify how we make a judgment on the underlying according Markov chain is this time reversible.

47
00:07:17,210 --> 00:07:25,760
So the the proposition to a lot like a kind of it looks trivial it says if you can't

48
00:07:25,760 --> 00:07:39,920
find a solution for this side of excise such that excise by day people tax PGI.

49
00:07:40,490 --> 00:07:49,580
All right. So what you have now is a solution, a set of solution and some transition kernel.

50
00:07:49,580 --> 00:07:53,480
So you are given the Markov chain in the form of a transition kernel.

51
00:07:54,410 --> 00:08:06,110
And if you happen to see there exists an excise, it's a set of the numbers such that this detail balance equation satisfies and furthermore.

52
00:08:08,050 --> 00:08:12,980
I decide it's an opportune one. Obviously, they have to be positive.

53
00:08:22,690 --> 00:08:26,210
I think again, we are.

54
00:08:26,440 --> 00:08:38,420
Then the summation of this excised strip equal to one meaning excised stray could be a valid could be a valid distribution not from right.

55
00:08:38,440 --> 00:08:45,250
If you cannot have negative numbers, you can have you cannot have individual state greater than one.

56
00:08:47,680 --> 00:08:52,660
So given all these conditions, is the detailed balanced equation satisfied?

57
00:08:53,080 --> 00:08:57,250
Now, we cannot assert actually the the underlying mark.

58
00:08:57,260 --> 00:09:14,860
Okay, so this is the conclusion, the underlying see because giving this you can check the irreducibility,

59
00:09:15,460 --> 00:09:21,960
the null recur, the positive recur and the periodic properties, the underlying Markov chain.

60
00:09:21,970 --> 00:09:28,000
And then we should say the underlying operating margin is quite simple.

61
00:09:34,120 --> 00:09:45,790
All right, so if you have that property so now so we're using the we first use the property define the time reversible Markov chain.

62
00:09:46,090 --> 00:09:50,900
And in the proposition to turn that a little bit or wrong let's say you have.

63
00:09:52,860 --> 00:09:59,110
If we check up the mark office, identify that office, and this is a tiny reversible mark.

64
00:10:02,640 --> 00:10:10,950
Well, the only thing we need to check is if we use this detailed balance equations,

65
00:10:20,400 --> 00:10:28,470
then they should it should not conflict with where it should be compatible with the general theory.

66
00:10:28,800 --> 00:10:37,620
Right. So how do we calculate that? So the other way to say it, if if you find the solution x, y, i j this set up the solution.

67
00:10:38,190 --> 00:10:46,150
Does it satisfy the theorem? Meaning if the general way how we compute.

68
00:10:49,050 --> 00:10:55,780
The general way about how we compute a stationary distribution is not violated.

69
00:10:56,640 --> 00:11:00,240
So how do we channel the computer stationary distribution?

70
00:11:00,660 --> 00:11:07,950
So first that you make a judgment that is the underlying Markov chain is supporting Markov chain also that you can check the picture.

71
00:11:08,070 --> 00:11:09,690
So you can do the same thing here.

72
00:11:10,470 --> 00:11:18,360
And then once you confirm that the underlying Markov chain is a Baltic, then you are supposed to solve the friction like this.

73
00:11:18,390 --> 00:11:25,050
Right. Hi, J. In court to see those.

74
00:11:36,020 --> 00:11:41,989
Right. So the page has to be. So the way to remember this is pretty simple.

75
00:11:41,990 --> 00:11:44,990
So if you're thinking about the Markov chain is still.

76
00:11:47,030 --> 00:11:48,670
And I know, right.

77
00:11:48,700 --> 00:11:58,419
The editor of the station or distribution, the ones that transition to X plus one, that particular variable still follows stationary distribution.

78
00:11:58,420 --> 00:12:02,920
Therefore, the x j page has this equation it has to hold.

79
00:12:03,820 --> 00:12:15,260
Okay. We also said this is going to be unique. So all we need to check is, is the detailed balanced equation actually satisfied this right here?

80
00:12:15,280 --> 00:12:21,010
Two things are not in conflict with each other. All right, Cowie.

81
00:12:22,450 --> 00:12:32,900
So, first of all, you see, if we have the side by j equal to i.

82
00:12:33,170 --> 00:12:41,530
J. J. So we starting point is this detailed balanced equation from the proposition to right.

83
00:12:41,570 --> 00:12:52,470
We try to make it look like this. So for the I and what I need to do is summation with all respect, all eyes at the same time.

84
00:12:52,480 --> 00:12:58,030
So thinking about you're writing this for this detailed balanced equation for all possible eyes.

85
00:12:58,600 --> 00:13:04,240
So on the right hand side, because of this, yeah, that's pretty straightforward.

86
00:13:04,300 --> 00:13:09,740
So if you're sum over all the I's the detail balance equation on the right hand side, you should also.

87
00:13:10,420 --> 00:13:14,650
The left hand side, you don't have anything to do.

88
00:13:17,830 --> 00:13:21,040
The right hand side, though real lies.

89
00:13:21,040 --> 00:13:27,310
The summation is with the respect that we must respect this destination.

90
00:13:28,240 --> 00:13:32,170
And then there. So you can take that out.

91
00:13:33,670 --> 00:13:39,150
And a summation on the guy. And then for any Markov.

92
00:13:40,900 --> 00:13:49,120
If you're starting from. Sorry. This is I think starting from is where.

93
00:13:50,540 --> 00:13:50,869
Okay.

94
00:13:50,870 --> 00:14:01,790
So the transition start from a J and then you are basically some over oh possible destinations, then they follows this one should be equal to one.

95
00:14:01,790 --> 00:14:05,360
So this is the same thing as you look at a transition matrix.

96
00:14:05,690 --> 00:14:10,100
The role some has to be one. This is more general.

97
00:14:10,190 --> 00:14:16,010
This is you don't have to constrain yourself into a finite state space market is to be

98
00:14:16,550 --> 00:14:24,770
username counterprogramming so I just the times one so therefore you get this form.

99
00:14:28,230 --> 00:14:31,980
Which means what? Which means the exiled J.

100
00:14:34,760 --> 00:14:38,750
Satisfied the general, according to immigration.

101
00:14:46,610 --> 00:14:51,180
I just called a Christian. Yeah.

102
00:14:51,190 --> 00:15:07,370
This one. This one satisfies. Furthermore, the execs are valid, the distribution.

103
00:15:07,790 --> 00:15:14,390
They add up to one and they individually each of the member is bounded between zero and one.

104
00:15:15,660 --> 00:15:22,650
So the next argument has to be the awarding distribution or the stationary distribution is unique.

105
00:15:23,940 --> 00:15:28,300
We measure that we can to prove that this is the property above.

106
00:15:28,980 --> 00:15:37,350
If you have a unique stationary distribution and this solution gave you a solution, that means the tie exchange.

107
00:15:37,380 --> 00:15:41,170
So this idea. And then because of the uniqueness.

108
00:15:41,780 --> 00:15:51,810
Because of the uniqueness of.

109
00:15:55,160 --> 00:16:08,180
Therefore, we're saying if the detailed balance equation satisfied, then that it must represent the true underlying stationary distribution.

110
00:16:09,440 --> 00:16:19,009
So it's an overall simple argument because all we did is trying to put all the detailed, relevant,

111
00:16:19,010 --> 00:16:27,560
detailed values equation together and get this more general form of stationary distribution equation or awarding equation.

112
00:16:28,010 --> 00:16:35,720
And then that's trivially follows from the property of the marvel, therefore we have.

113
00:16:37,160 --> 00:16:51,020
Okay. So, so the implication of this theorem is or this result is just simply if you're true, you can solve for the detailed balance equation.

114
00:16:51,320 --> 00:16:56,270
You don't need to repeat this process to calculate the more general form of it.

115
00:16:56,600 --> 00:17:11,420
It's automatically satisfied. And so the other way to say that this detail follows the equation not only defines the time of reversible Markov chain,

116
00:17:11,750 --> 00:17:20,240
and it can be also used to to attack the time reversible problem.

117
00:17:23,720 --> 00:17:35,360
So the one more thing we need to say is the theorem one in the lecture notes.

118
00:17:37,400 --> 00:17:41,330
So again, this is a theorem presented as a way to.

119
00:17:43,130 --> 00:17:47,570
Assess if underlying Markov chain is a type of reversible one.

120
00:17:47,750 --> 00:18:01,120
And I don't want to emphasize this theorem one too much because in usual cases this is a1i think I will give it first.

121
00:18:01,740 --> 00:18:21,310
I was thinking of working ABC for which you i j equal to zero always implies GDI zero.

122
00:18:22,670 --> 00:18:32,600
Right. So so this is stronger than either the accessibility or they have a communication relationship.

123
00:18:32,600 --> 00:18:40,399
If you don't have a one step transition, if a one step transition from I to J is zero, then J to Y has to be zero.

124
00:18:40,400 --> 00:18:45,020
So this is a stronger condition for which. So but this on the line mark.

125
00:18:45,030 --> 00:18:50,180
Okay, so already avoid it. But you also have this so.

126
00:18:51,460 --> 00:19:00,820
Is time reversible? If and only if.

127
00:19:02,920 --> 00:19:06,760
So, then the description next. A little bit geometric.

128
00:19:10,380 --> 00:19:19,980
But it's somewhat easier to understand. So if you look at this flow is the thinking about you're not content in holding states together.

129
00:19:20,070 --> 00:19:23,760
So we call this a stage diagram, right?

130
00:19:23,760 --> 00:19:28,950
So sorry for what I say.

131
00:19:29,280 --> 00:19:43,830
So each of these circles are at a stage and then this flow here represents the probability from one to 2 to 3, 3 to 4, three, 4 to 1.

132
00:19:44,190 --> 00:19:58,730
And then because if as so so this requirement is saying if you have ap1, two equal to zero, it has to be, then you all.

133
00:19:58,830 --> 00:20:01,990
You must also have a flow from 2 to 1, right?

134
00:20:02,010 --> 00:20:06,390
Because if 2210p12 has to be zero.

135
00:20:06,600 --> 00:20:16,830
This is a constraint. So we must have this this kind of as well, the double arrow connecting a pair of states.

136
00:20:21,330 --> 00:20:27,659
All right. So you can look at this flow like clockwise on a counterclockwise, right.

137
00:20:27,660 --> 00:20:36,840
So you're thinking about the probabilities floating so that the statement is just basically trying to say,

138
00:20:39,930 --> 00:20:57,690
if I'd only if starting by starting from any path, the path is, well, we just set like the probability flow here and the path.

139
00:21:01,060 --> 00:21:04,000
Back to II has the same probability as.

140
00:21:11,750 --> 00:21:22,160
So if I'm thinking about my one I think any path back to one so there is one way is doing it clockwise and then the other way.

141
00:21:22,460 --> 00:21:29,750
Another path it's doing it counterclockwise has the same probability.

142
00:21:30,590 --> 00:21:33,950
You have the same.

143
00:21:37,900 --> 00:21:55,090
Same probability as the reverse of all pos i.e.

144
00:21:57,510 --> 00:22:09,870
So that is you have to keep eye to eye, one high speed high, one too high to do so on and so forth.

145
00:22:10,080 --> 00:22:15,060
I thought you have I came back to often.

146
00:22:15,960 --> 00:22:18,180
So those are transition probabilities.

147
00:22:18,180 --> 00:22:27,900
Once that transition probability, this is basically pi 1p12 types p two, three times p three, two, four and then types of forward two one.

148
00:22:28,020 --> 00:22:35,090
Right, right. So I want to Iook back to one massive po to the same.

149
00:22:35,100 --> 00:22:39,180
So P I would go backwards.

150
00:22:39,330 --> 00:22:56,970
So doing this counterclockwise or two ik and then I came to sorry i k minus one and so on and so forth and on backwards we get to I two.

151
00:22:57,690 --> 00:23:04,260
Right. And finally, as I want to point, I want to I'm sorry.

152
00:23:05,400 --> 00:23:16,620
So basically per my term, you're just flip the, the destination and then the, the starting point, right?

153
00:23:16,620 --> 00:23:25,130
So this is on I k to IV and on the right hand side you flip them over so you get I do, I carry the same thing turn by turn and flip it.

154
00:23:25,470 --> 00:23:32,480
So that will represent the reversible. And so if this is satisfy the underlying.

155
00:23:34,520 --> 00:23:39,410
So this is a very long narrative to say. So for all states.

156
00:23:48,260 --> 00:23:54,979
So if this property holds for all states, then the underlying mark of Cheney is time reversible.

157
00:23:54,980 --> 00:24:02,920
Mark of Cheney. This is it almost for one to try as he.

158
00:24:03,910 --> 00:24:12,500
This time reversible. Right. So this is the condition if and only if sufficient, on the necessary condition for this underlying article.

159
00:24:12,520 --> 00:24:23,570
So if you find if you have a timer and reversible Markov chain with this property picture, you could do zero on the PGI zero.

160
00:24:23,920 --> 00:24:37,180
Now, if you'll find a path for arbitrary paths like this type of the circle, the reversible, the circle has the same probability defined by this.

161
00:24:37,180 --> 00:24:45,130
And on the right mark is the reversible mark. I would say this is not very useful in practice.

162
00:24:47,200 --> 00:24:58,780
The easier way to find. To show some some on the right mark it is time reversible is actually solving for PI page.

163
00:24:59,320 --> 00:25:01,690
Just treat them like water Markov chain. Right.

164
00:25:01,750 --> 00:25:11,380
So you need to make the document, the Gordian Markov chain and then test for the the detail balance equation that's more efficient.

165
00:25:11,980 --> 00:25:20,520
This one does tell you something like. Very important.

166
00:25:20,890 --> 00:25:25,320
A geometric symmetry, but it's not really useful.

167
00:25:25,350 --> 00:25:28,890
It's not really a kind of a shortcut to find the mark.

168
00:25:29,130 --> 00:25:32,130
Nevertheless, it's interesting. So we see why this is true.

169
00:25:34,690 --> 00:25:38,310
But again,

170
00:25:38,820 --> 00:25:49,830
I'm saying this is not the maybe not the best way to find a stationary distribution or for making a judgment on like it's not the most efficient way,

171
00:25:50,880 --> 00:25:59,910
but it's a useful way. This is probably one of the most important properties.

172
00:26:00,060 --> 00:26:05,969
Right. All right. So first of all, let's see if this is a necessary condition.

173
00:26:05,970 --> 00:26:09,900
So if we we know that says time reversible, then.

174
00:26:13,470 --> 00:26:17,360
Then he. I like to.

175
00:26:17,400 --> 00:26:23,430
We can. So if we already know everything is time reversible, let's just say that he came to.

176
00:26:24,150 --> 00:26:41,330
Okay. Plus one must be equal to the pi I pay plus one times probability I pay plus one to take another divided by probability.

177
00:26:41,990 --> 00:26:48,090
Okay, so you could what you could do is replace one off.

178
00:26:48,150 --> 00:26:52,830
So this is a increasing number so that you can replace the left hand side with this

179
00:26:52,830 --> 00:26:59,459
expression so that basically it's replace the with the destination pi k plus one.

180
00:26:59,460 --> 00:27:03,840
So those are that from the bigger number I K plus one,

181
00:27:03,840 --> 00:27:09,090
two K that's what you have on the right hand side and then you get a bunch of the ratios, right.

182
00:27:09,390 --> 00:27:16,220
So if you're just plug in this expression to the, to the left hand side, you will find all of these of PI.

183
00:27:16,230 --> 00:27:20,700
The PI's canceled all eventually, and then you get in the right hand side.

184
00:27:21,060 --> 00:27:28,350
So that's a simple algebra if. So this is the direction this is and this is.

185
00:27:28,600 --> 00:27:31,830
So it's this direction.

186
00:27:32,490 --> 00:27:49,770
If it's if it is the necessary condition, if the if the chain C is reversible, then this follows.

187
00:27:50,550 --> 00:27:57,510
And then my plug in each term on the left hand side, you will see the cancelation of all the PI's and then you get the right hand side.

188
00:27:57,830 --> 00:28:04,230
I'm not doing any right now. You kind of check it yourself up three in a class.

189
00:28:04,590 --> 00:28:08,280
So let's for sufficiency.

190
00:28:09,030 --> 00:28:18,899
So if we have that property can we say the on the line Markov chain is type V is the time

191
00:28:18,900 --> 00:28:31,170
reversible mark off chain so sufficiency and we are all right so we got to have all the

192
00:28:31,170 --> 00:28:41,340
transitions we can you know there is a simple drag we can clapping all these keys together

193
00:28:41,790 --> 00:28:54,659
and as a case that transition probability right so what we do here is just the eye to eyes.

194
00:28:54,660 --> 00:29:00,660
Okay, so let's say these are the case that comes General.

195
00:29:00,900 --> 00:29:10,680
So what I use it the way I use this notation is, as I said, just clapping everything together and then call this is a case that transition curl.

196
00:29:10,800 --> 00:29:16,410
Right? So that's from I k to I k possible. So this is a goes through I to I1.

197
00:29:16,410 --> 00:29:24,870
I want to I to I talked to i3 all the way out to and so well I mean it doesn't need

198
00:29:24,870 --> 00:29:34,710
to be very accurate to this is the case type transition kernel multiplied by two I.

199
00:29:37,970 --> 00:29:41,150
This one equals five in a statement.

200
00:29:41,210 --> 00:29:45,260
So we have four. So we have the equation for any path.

201
00:29:45,650 --> 00:29:56,510
So it follows you have the IPA and then you multiply that by this i k to try to increase that transition.

202
00:29:59,110 --> 00:30:06,400
Right. So this is just a simple way to rewrite the equation by kind of like clipping things together.

203
00:30:14,580 --> 00:30:20,430
And now we're going to say, well, like the cake goes to take the cake.

204
00:30:28,450 --> 00:30:38,680
To the K. Coast to coast.

205
00:30:46,970 --> 00:30:50,600
Just. It's useless this trying.

206
00:30:50,620 --> 00:30:53,740
It doesn't pretend it's just representative.

207
00:31:04,900 --> 00:31:12,110
Alright. So I will intentionally make this cake also infinity because this is true for any path.

208
00:31:12,520 --> 00:31:23,829
So if K Prime goes to infinity, we now have a state because the underlying Markov chain is is awarded to Markov chain and then this come from

209
00:31:23,830 --> 00:31:38,550
the standard accorded theorem is1 Newport SPI iki1k goes to k prime goes to infinity times i k y equal to.

210
00:31:38,830 --> 00:31:42,880
So this one it's only the best in Asia matters pi.

211
00:31:43,660 --> 00:31:54,870
Yeah. So what we got here is a verification of detailed balance equation, right?

212
00:31:54,940 --> 00:32:00,430
So that the detailed balance equation holds for any of these.

213
00:32:01,600 --> 00:32:09,400
I. K i. Right.

214
00:32:09,490 --> 00:32:15,280
So this one definitely it's not from the by just case that translation.

215
00:32:15,480 --> 00:32:19,090
Right. So this has to be a different number. The hate crime.

216
00:32:19,090 --> 00:32:23,410
So you can do this multiple times. That equation still holds.

217
00:32:23,890 --> 00:32:29,010
Yeah. So just thinking about looking that forever, right.

218
00:32:29,020 --> 00:32:32,710
So you can we can make multiple loops, not just a single.

219
00:32:33,160 --> 00:32:40,959
Because that if you're just do a single loop, you cannot invoke this, um, this argument.

220
00:32:40,960 --> 00:32:47,970
The K prime goes to infinity. But if you do multiple loops, that equation still holds, right?

221
00:32:48,010 --> 00:32:51,190
Because this holds for one single, it will host four.

222
00:32:51,280 --> 00:33:03,040
And the arbitrary loops I like, if you're take k prime goes to infinity, then the first two this is the basically the statement of awarding theorem.

223
00:33:03,520 --> 00:33:10,480
Um, you can run that train long enough they will converge that this transition probability case that

224
00:33:10,870 --> 00:33:17,710
K primes that transition probability will converge to the a stationary distribution therefore.

225
00:33:18,070 --> 00:33:27,760
So this is what we show here is that particular condition implies the detailed balance equation.

226
00:33:27,880 --> 00:33:37,480
Therefore, if you have that property for any event from the proposition to the one,

227
00:33:37,480 --> 00:33:42,460
we just prove detail balanced accretion satisfies on the right Markov chain must be.

228
00:33:45,370 --> 00:33:51,550
A tiny reversible mark. Jim gains. Any questions?

229
00:33:57,530 --> 00:34:02,870
Yeah. I think the way to think about it is kind of reversible.

230
00:34:02,870 --> 00:34:11,290
Markov chain does do something. Special properties, especially this type of the geometric.

231
00:34:14,920 --> 00:34:22,770
Liking the probability flow for the bank or as are the same for any path.

232
00:34:22,780 --> 00:34:27,670
It's not just between a pair of this can.

233
00:34:32,430 --> 00:34:37,050
Again. This is not the way you should. Yes, please. I just have a quick notation question.

234
00:34:37,500 --> 00:34:41,130
So are the arrows in the colors for the states from eye to eye?

235
00:34:41,610 --> 00:34:46,320
For example, are those interchangeable items or do they need something different?

236
00:34:48,720 --> 00:34:52,410
You have the cover? Yeah. Are those those mean the same thing? Is the arrow?

237
00:34:52,590 --> 00:34:59,850
Or is it a different. It is trying to distinguish this is not the one step transition.

238
00:35:00,270 --> 00:35:03,420
I think that's the only reason I did it with.

239
00:35:03,660 --> 00:35:09,150
The comma instead of the arrow is trying to reserve for one slight transition.

240
00:35:09,480 --> 00:35:13,480
So this one, this. Okay, prime step transition. Um.

241
00:35:14,290 --> 00:35:24,950
Yes, good question. Usually we don't I think this is a just in this particular case, we have to emphasize this is a prime spot.

242
00:35:24,970 --> 00:35:28,120
This is the ones that for one, two of them separated.

243
00:35:30,130 --> 00:35:33,360
But yes, you're right. It's useless.

244
00:35:34,180 --> 00:35:40,250
I should be consistent. I'm. Thank you.

245
00:35:47,350 --> 00:35:55,780
Yeah. If I start over, I wouldn't use K prime. I would use a different notation like M or something, but I'm already half way there.

246
00:35:55,780 --> 00:36:01,180
So K prime is better than this encryption at all.

247
00:36:03,850 --> 00:36:09,300
Great. Anyway.

248
00:36:11,490 --> 00:36:18,270
So practically the best way to make a judgment call.

249
00:36:18,400 --> 00:36:23,310
But as I said, I think that the best way to make a judgment,

250
00:36:23,490 --> 00:36:29,370
make assessment is in the time of reversible Markov chain is first thing you may

251
00:36:29,370 --> 00:36:37,500
want to find on a stationary distribution and then check the detail balance.

252
00:36:38,370 --> 00:36:46,139
Not through this. So impossible to use this, but this is the conditions that you need to check.

253
00:36:46,140 --> 00:36:52,830
It's pretty complicated, right? So you can check all possible paths.

254
00:36:53,520 --> 00:37:01,620
That's pretty that's a pretty that's a pretty challenging task to do in general.

255
00:37:03,360 --> 00:37:19,140
All right. So last time I talk a little bit about why we care about time, reversible change, and so today we can see that application in details.

256
00:37:20,100 --> 00:37:28,050
Um, probably there is a lot of usage in of Markov chain in, in statistics,

257
00:37:28,650 --> 00:37:36,780
but from the application we're going to talk about today is probably instrumental to modern statistics,

258
00:37:36,780 --> 00:37:41,330
especially the Bayesian statistics, that is the Markov chain Monte Carlo.

259
00:37:43,410 --> 00:37:50,030
So in this case, we want to design a mark of chain with a target distribution in mind.

260
00:37:50,210 --> 00:37:56,400
I'm trying to sample from it to get to that, get to there.

261
00:37:56,460 --> 00:38:06,060
So the argument when you first talk a little bit about so we're going to start with the lecture selection of Markov changes in the column.

262
00:38:07,290 --> 00:38:21,390
So we first you can talk a little bit about the Monte Carlo algorithm, which among Carlo.

263
00:38:26,760 --> 00:38:38,120
So first it is. It's actually not in the domain of stochastic process, but in general, I think it's important to know total amount.

264
00:38:42,770 --> 00:38:58,020
And so Monte Carlo method is typically used to solve the following.

265
00:38:58,200 --> 00:39:02,430
It's a numerical method used to solve the following problem.

266
00:39:02,730 --> 00:39:12,520
So if you're trying to estimate that beta from a model that's a parameter model parameter, you may have a parametric model, which can be a reason.

267
00:39:12,520 --> 00:39:20,430
There's an expectation h some x, so h is a function.

268
00:39:20,640 --> 00:39:26,640
And so we assume there is a nicely behaved the function, an axis, a random variable.

269
00:39:27,350 --> 00:39:36,450
Right. So you're basically trying to compute the expectation of a random function of X is a random function.

270
00:39:36,450 --> 00:39:45,929
Right. And so if that's the thing that you want to get and then the computation is analytically and tractable,

271
00:39:45,930 --> 00:39:54,630
meaning you cannot do the integration, what you could do is turn this into the following.

272
00:39:57,120 --> 00:40:08,730
So let's assume that each of x is the x only take a while for now only takes discrete values just for the piece of the argument.

273
00:40:08,790 --> 00:40:12,240
But in in practice, x can be continuous always.

274
00:40:13,650 --> 00:40:18,120
So if X. Is a disgrace.

275
00:40:21,030 --> 00:40:27,359
It's a disgrace so that this expectation can be read.

276
00:40:27,360 --> 00:40:33,540
And that's kind of sum over all possible values, let's say.

277
00:40:34,440 --> 00:40:38,550
Oh, possible. Wow. It was okay. Um.

278
00:40:40,230 --> 00:40:43,520
H j the probability.

279
00:40:46,980 --> 00:40:52,330
So that's how you do the calculation for discrete rather than a function.

280
00:40:52,400 --> 00:40:57,080
So you know, the JS probability acts equal to change.

281
00:40:57,110 --> 00:40:58,940
So yeah, there's nothing fancy here.

282
00:40:59,690 --> 00:41:09,170
The issue about this is if you're having trouble to evaluate the probability X equal to j, then you could replace this.

283
00:41:09,680 --> 00:41:13,700
And then this would take a frequency interpretation of the probability.

284
00:41:13,910 --> 00:41:25,580
Right. So this can be interpreted as a frequency of X.

285
00:41:27,490 --> 00:41:29,800
So why this is helpful? Now you've got this.

286
00:41:32,240 --> 00:41:44,180
Designed as the cat well statistical device nowadays there you can give them fancy names like Monte Carlo machines,

287
00:41:46,070 --> 00:42:05,540
everything being, machine learning. And then you learn the machine channel this random variables and solve for, say, this machine.

288
00:42:08,690 --> 00:42:15,500
Usually he changes everything. Same thing. It's true also for machine learning in general.

289
00:42:17,120 --> 00:42:24,020
So what you could do is to draw in samples from.

290
00:42:27,470 --> 00:42:31,460
Why are these samples of x right.

291
00:42:32,090 --> 00:42:38,810
And apply more large number.

292
00:42:41,720 --> 00:42:53,510
So what we got there is the probability going to be executive j going to be replaced by the frequency in the sample than the random sample.

293
00:42:54,050 --> 00:43:07,930
And now what we know there is. So so the law of large number here, there are two different versions, I think in this particular case, Monte Carlo.

294
00:43:08,140 --> 00:43:14,680
And we don't really understand which version of this article both are applicable.

295
00:43:15,970 --> 00:43:27,370
So there are so you should know for, for those of you are taking, um, 801 word.

296
00:43:29,380 --> 00:43:31,780
So this is the weak law of large number.

297
00:43:31,900 --> 00:43:41,780
So if you draw a lot of samples from the X and then this X and going to be converted to the mean here, we can turn that into an.

298
00:43:47,810 --> 00:43:55,580
H X FA minus B, right?

299
00:43:55,580 --> 00:44:06,350
So this is some poll you take each sample and then compute h of x the, the sample mean of the Ajax and the thena gonna converge.

300
00:44:07,430 --> 00:44:14,880
So that's just the. So this is the weak law.

301
00:44:15,420 --> 00:44:33,850
Right. So this said said. And then there is also a strong law, sufficient Ajax.

302
00:44:35,940 --> 00:44:39,600
That's more straight forward to the problem that has happened.

303
00:44:40,350 --> 00:44:43,400
It's a probability one. All right.

304
00:44:43,410 --> 00:44:53,820
So this some poll average from the the random sampling from my own Monte Carlo, everything will converge to the theory that was probability one.

305
00:44:54,360 --> 00:44:59,130
So the requirement here is really the idea.

306
00:45:02,400 --> 00:45:11,550
So you need to know how to draw on how to make realization of acts according to the underlying distribution.

307
00:45:12,060 --> 00:45:15,510
P acts equal to J even. It's hard to calculate.

308
00:45:15,660 --> 00:45:21,240
Maybe you don't know how to calculate this, but if you have the ability to draw the I.D. sample,

309
00:45:21,540 --> 00:45:28,379
then the law and the law of large number, either the either the weak one or the strong one.

310
00:45:28,380 --> 00:45:33,180
No guarantee. So say that you're going to get the right answer.

311
00:45:33,690 --> 00:45:38,190
You're going to converge the estimate are going to converge to the correct answer.

312
00:45:38,200 --> 00:45:46,110
If you have a large enough sample, size is large enough and solvable.

313
00:45:49,750 --> 00:45:56,860
So it's just some jargon to make connection.

314
00:45:56,860 --> 00:46:00,870
And so this is a multi column is nothing more than this stuff.

315
00:46:05,890 --> 00:46:18,170
And here the emphasis is the requirement you need I need something else and something here.

316
00:46:22,160 --> 00:46:52,870
So. So the distribution of ads needs to be fully built and to do that.

317
00:46:57,080 --> 00:47:03,010
So there are some empty. Right.

318
00:47:03,010 --> 00:47:08,410
So. So you need the ability to to do that. And then this is the requirement.

319
00:47:12,260 --> 00:47:24,740
So the challenge of this. So although it is a simple concept, um, there are, there are a lot of cases this algorithm may not be applicable.

320
00:47:25,850 --> 00:47:48,589
And so the challenge here is what is the sampling, distribution or giving the distribution of facts is only known up to a constant,

321
00:47:48,590 --> 00:48:06,590
normalized and constant to see if this is the first time you encounter this,

322
00:48:06,590 --> 00:48:13,280
you may feel like we're only like a constant the way how hard the problem could be.

323
00:48:13,790 --> 00:48:19,249
All right.

324
00:48:19,250 --> 00:48:26,390
So this challenge that frequently appears in Bayesian calculation.

325
00:48:32,260 --> 00:48:41,320
So what do I mean by that? So in facing confrontation, you often see this type of space.

326
00:48:48,160 --> 00:48:51,820
So you want to calculate something you want to off from.

327
00:48:52,990 --> 00:49:01,990
Usually the target distribution you want to draw is from the posterior distribution of the interview index.

328
00:49:01,990 --> 00:49:02,410
X I'm sorry.

329
00:49:03,220 --> 00:49:12,990
So you want to draw from this distribution and then the base rule tells you what you can do is you feel that I'm supposed to be able to figure.

330
00:49:13,780 --> 00:49:20,350
So this part is called as a prior. Usually you will assume I used the computer prior.

331
00:49:20,510 --> 00:49:26,260
Well, at least the computation you evaluate the prior is typically not the problem.

332
00:49:26,380 --> 00:49:32,020
Not always, but typically not the problem. The x given theta is the likelihood.

333
00:49:32,230 --> 00:49:36,650
So those parties on the like tractable.

334
00:49:37,480 --> 00:49:42,820
The thing about this is you need to calculate the the prime.

335
00:49:43,870 --> 00:49:57,350
The prime. So in the denominator though, you need to sum over all possible fetus.

336
00:49:57,410 --> 00:50:05,260
So instead of on, you know, maximum likelihood, just calculate that the maximum value of x given to you.

337
00:50:05,950 --> 00:50:18,280
Here you need to do integration. So in numerical analysis, integration is always more difficult than the optimization problems.

338
00:50:18,560 --> 00:50:29,630
So this kind of. But given the fact. So if you're you still not appreciating, this is hard to do.

339
00:50:29,840 --> 00:50:33,290
Let's give you an example. Let's say the visa is a binary.

340
00:50:34,880 --> 00:50:38,420
They say visa is a binary factor.

341
00:50:43,180 --> 00:50:47,890
So it's taking values like that measured all the back.

342
00:50:48,280 --> 00:50:51,490
So each value is either zero or one.

343
00:50:51,950 --> 00:50:59,020
Okay, I'm just trying to give you example, you have to fully appreciate this is impossible to calculate.

344
00:50:59,860 --> 00:51:02,950
All right. So the 0101 or 111.

345
00:51:03,250 --> 00:51:06,640
So how many values you need to. So these are p dimensional.

346
00:51:15,770 --> 00:51:27,380
How many values you need to some over there. So you need to sum over all possible value of p p is a p dimensional binary factor.

347
00:51:28,340 --> 00:51:32,709
How many values you need you. Come on.

348
00:51:32,710 --> 00:51:40,830
I know this is it. To be very good to do anything exponential is hard, right?

349
00:51:41,100 --> 00:51:45,660
So easily. Can you imagine you do to Cuba?

350
00:51:46,020 --> 00:51:51,209
Well, we can imagine you would do to with a ten. It's only a thousand value carry measure.

351
00:51:51,210 --> 00:51:57,860
You do to us with 100. So anything get to a higher dimension that's not even considered higher dimension.

352
00:51:57,870 --> 00:52:07,500
Nowadays, if you only have 100 binary values to the 100 that you cannot, I don't think anybody can.

353
00:52:07,590 --> 00:52:13,770
It's it's considered an astronomical calculation, even in the discrete case.

354
00:52:14,970 --> 00:52:21,270
So this distribution then becomes you can only calculate the P field.

355
00:52:21,330 --> 00:52:35,100
So usually Bayes in this way is proportional to that x given theta meaning they can the the normalizing constant is intractable.

356
00:52:37,230 --> 00:52:46,380
And then with this type of B limitation, you can now theoretically draw from the the distribution.

357
00:52:46,530 --> 00:52:53,129
You cannot create the distribution. Right? You cannot even draw from this this distribution.

358
00:52:53,130 --> 00:52:56,790
If you don't know this, you don't know.

359
00:52:56,880 --> 00:53:03,540
So so if your know every term this is called the distribution is falling now.

360
00:53:03,840 --> 00:53:12,360
But if you only know something up to a normalizing constant and then I just showed you this could

361
00:53:12,360 --> 00:53:21,210
be often happens in a high dimensional situation or in this type of a situation sometimes.

362
00:53:21,390 --> 00:53:25,640
Well, if you can consider a continuous random variable, then, you know,

363
00:53:25,770 --> 00:53:31,500
I think this is the argument that also extends to the condition continuous random variable.

364
00:53:31,710 --> 00:53:35,130
You don't need to get to a p you go to hundred your your.

365
00:53:37,010 --> 00:53:41,780
They use surrender to say you cannot do the calculation even like that mentioned like ten.

366
00:53:41,810 --> 00:53:46,260
And then you do. Trying to do numerical integration.

367
00:53:46,260 --> 00:53:52,470
Given this the normal marginal E for every single thing, it's it's impossible.

368
00:53:52,500 --> 00:54:01,110
So if you start doing research and I, you should realize to do numerical integration,

369
00:54:01,110 --> 00:54:09,030
once that motion is possible, it's usually considered to be the world in two dimensions.

370
00:54:09,040 --> 00:54:17,370
It's okay. It's still considered too doable to get to three dimensions of trying to do three dimensional numerical integration.

371
00:54:18,420 --> 00:54:21,540
Good luck. And you can find a very good numerical.

372
00:54:22,020 --> 00:54:27,790
So in that case is also in for the continuous situation.

373
00:54:27,840 --> 00:54:34,290
So we can generally consider the the distribution only not up to a concern.

374
00:54:34,530 --> 00:54:39,420
So this one even we want to use the Monte Carlo idea, it can now be used.

375
00:54:39,960 --> 00:54:44,280
All right. So this is a motivator.

376
00:54:45,030 --> 00:54:50,250
And then there's this very reason it's a computational limitation,

377
00:54:50,520 --> 00:55:02,760
is why Bayesian statistics is get to the point the push out of the the mainstream of statistics from like 1920

378
00:55:02,850 --> 00:55:14,460
since Fisher and to 19 probably eighties seventies until the emergence of the Markov chain Monte Carlo algorithm,

379
00:55:14,820 --> 00:55:20,970
which is fundamentally important to solve this type of the problem.

380
00:55:22,830 --> 00:55:32,840
Okay. So, so what we have established this such so if you're not what distribution or target distribution up to a normalizing constant in this form,

381
00:55:33,300 --> 00:55:38,430
you cannot use the naive Monte Carlo or the so-called rejection some point.

382
00:55:38,670 --> 00:55:44,670
Right. So you need to use CMC.

383
00:55:45,540 --> 00:55:50,980
But. But why CMC works.

384
00:55:51,760 --> 00:55:57,130
There are two different ways with it. There are two difference the facts we need to explain.

385
00:55:57,340 --> 00:56:01,570
One is why you can handle the normalizing constant.

386
00:56:01,720 --> 00:56:04,740
So this is a probably we should talk about next Monday.

387
00:56:05,020 --> 00:56:13,040
But first of all, what is. The now, in theory, there is an issue here.

388
00:56:13,190 --> 00:56:17,420
So if you're drawing samples from a markov chain.

389
00:56:20,180 --> 00:56:23,330
Are you still drawing the I.D. samples?

390
00:56:25,580 --> 00:56:30,739
So you need a little bit theory so that the solution, as we all know nowadays,

391
00:56:30,740 --> 00:56:37,580
we can be with the Markoff train, with the stationary distribution being that target distribution,

392
00:56:37,940 --> 00:56:45,500
meaning for example, in this situation, we although we don't know the normalizing constant,

393
00:56:45,890 --> 00:56:55,250
but we can guarantee that the the stationary distribution is the the posterior distribution.

394
00:56:55,300 --> 00:57:01,460
Okay. So that's part of the oh, this algorithm, the genius, those are with them.

395
00:57:02,030 --> 00:57:07,940
But even that's true why the Monte Carlo approach works.

396
00:57:08,180 --> 00:57:12,950
That's the second level of the question, but we can answer that question first.

397
00:57:16,070 --> 00:57:25,550
So there is law of largest number for large object or of Gordin Markov chain.

398
00:57:25,880 --> 00:57:35,720
So if you draw things from Markov chain, one thing you have to be very clear the samples are drawn, are not independent anymore.

399
00:57:36,740 --> 00:57:40,800
But does it matter? Well, sometimes it matters, right?

400
00:57:40,820 --> 00:57:50,390
I mean, depends on the the application for law of large number though a well, we can argue it doesn't matter that much.

401
00:57:51,540 --> 00:57:56,870
Um, as a matter of fact, there's a multiple versions of law of large number.

402
00:57:57,170 --> 00:58:04,790
Mostly you study the simple version of the IAB version in the 601.

403
00:58:05,090 --> 00:58:09,710
So some of you study eight or I don't really know.

404
00:58:09,740 --> 00:58:15,350
So the you should see the version of independent, but not identically distributed.

405
00:58:15,530 --> 00:58:24,560
So there is a you can remove the second requirement that even there being not identical distributed if there is a version.

406
00:58:25,310 --> 00:58:28,190
Right. So there is all kinds of the constraints,

407
00:58:28,470 --> 00:58:34,940
the technical constraints like a variance exists or barriers as a bond or that sort of thing also can be relaxed.

408
00:58:35,300 --> 00:58:44,770
So the law of large number for Markov chain. And it's also, I think, towards the end that we're going to at least learn the the concept of martingale.

409
00:58:44,780 --> 00:58:47,870
So those are the law of large number for marking goals.

410
00:58:48,260 --> 00:58:58,480
And the large, large number of mark of chains actually relax that independence, constrict, constrain for the influx number.

411
00:58:59,730 --> 00:59:23,480
And okay, so let taxonomy be and of course a trade where stationary distribution distribution.

412
00:59:25,350 --> 00:59:34,280
Hi again we cannot talk about these things in the in the script space.

413
00:59:36,140 --> 00:59:42,020
If age is a bounded function only requirement based upon.

414
00:59:47,620 --> 01:00:04,270
On state space. On the state space. Thomas speaks of the agency that it follows.

415
01:00:05,530 --> 01:00:23,950
So we get this strong law version. So the probability Clemens goes at any age and say, Oh, sorry, sorry, I want to run.

416
01:00:25,900 --> 01:00:29,780
Copaxone divided by one. So. So we're calculating.

417
01:00:29,800 --> 01:00:36,310
So we collect samples from the Markov chain and now we just like that the regular Monte Carlo,

418
01:00:36,610 --> 01:00:43,389
we get the dependent samples, but we just calculate the sample mean and then that sample me.

419
01:00:43,390 --> 01:00:53,050
It will converge to the expected value high of eight times.

420
01:00:54,400 --> 01:01:03,790
Right? So this is what we want. We are calculating the Ajax under the stationary distribution.

421
01:01:04,360 --> 01:01:07,090
So. So the left hand side is a simple average.

422
01:01:07,420 --> 01:01:17,350
The right hand side is a number, this is an expectation and that the random variable will converge to this number was probability one.

423
01:01:18,400 --> 01:01:21,850
Okay, so this is a lawful large number from our screening,

424
01:01:24,430 --> 01:01:34,240
it looks just like the law of large number for these samples, although the sign off is a little bit different.

425
01:01:34,300 --> 01:01:41,379
So if this is true, then if we try to calculate anything under the and trying to trying to calculate anything.

426
01:01:41,380 --> 01:01:47,740
And so the only random function, the expectation under the stationary distribution that we can just draw samples

427
01:01:47,740 --> 01:01:52,330
from the chain and then invoke this law of large number and then be done with it.

428
01:01:53,500 --> 01:02:03,220
Right. Um, well, we're not going to do the, the rigorous proof, but this is pretty obvious,

429
01:02:03,280 --> 01:02:12,730
I should say that the conclusion is because this is the core of this argument is if you just compute the expectation,

430
01:02:13,060 --> 01:02:17,320
then the correlation is not really matters that much.

431
01:02:17,320 --> 01:02:21,520
Right. We're not saying but this.

432
01:02:21,610 --> 01:02:26,980
Well, I mean, the argument is that the sample average is truly well.

433
01:02:28,660 --> 01:02:32,139
So these come from some sort of a linear average.

434
01:02:32,140 --> 01:02:37,210
And then you see this x one plus x two. No, I don't want to write this.

435
01:02:37,600 --> 01:02:46,750
So let's say it's a script sketch of the sketch.

436
01:02:53,590 --> 01:02:59,620
So that's assuming. So obviously this, this one is in practice,

437
01:02:59,620 --> 01:03:07,840
how you use it is you run the Markov chain long enough and then there's so-called a burn in the area, the purity that you want to discard this.

438
01:03:08,140 --> 01:03:17,410
So the sketch of the proof says let's just assuming you can start the Markov chain starting

439
01:03:17,770 --> 01:03:28,479
assuming I'm I'm see starts with a stationary distribution so if a state I'm turning

440
01:03:28,480 --> 01:03:33,040
to the stationary distribution that will stay in the stationary distribution forever

441
01:03:33,100 --> 01:04:04,900
right and then left AJ and the node number of visits to the state J in and ans right.

442
01:04:04,930 --> 01:04:10,820
So well I mean this is just keep tracking the visit is state right?

443
01:04:10,870 --> 01:04:18,820
So this is a like a drawing the sample is the same thing. But for the purpose of the proof, we want to count how many times.

444
01:04:18,940 --> 01:04:27,760
So this is a front. Well, you will see this is irrelevant because we're going to calculate the frequency later on and stops.

445
01:04:30,070 --> 01:04:40,750
And then if you trying to calculate the sample average, the first to do is I want to it right.

446
01:04:40,870 --> 01:04:44,819
So so you. This one is not calculating.

447
01:04:44,820 --> 01:04:54,870
The average yet is calculate the sum and then the goal is calculating the average later on because the theorem sets the sample average.

448
01:04:55,320 --> 01:04:58,890
But the sum here. This is a way to calculate.

449
01:04:58,950 --> 01:05:04,800
So. So this is basically the Riemann integration versus the Labbadia integration.

450
01:05:04,950 --> 01:05:11,060
So the Roma integration you can think about, you are counting on the x axis, right?

451
01:05:11,100 --> 01:05:14,670
x1x2x3x4. So on, so forth.

452
01:05:14,910 --> 01:05:18,420
There is this natural x axis or the timeline.

453
01:05:18,540 --> 01:05:24,090
You start counting by the same thing for, especially for the finite, and you see that equivalence.

454
01:05:24,450 --> 01:05:29,310
You can count how many times the attacks hit.

455
01:05:29,580 --> 01:05:34,350
Number one. Number two. Number three. So. So you can come from the Y axis as well.

456
01:05:34,750 --> 01:05:44,000
Right. Is not may not be continuous and it may not have a natural explanation, but it's a valid concrete process.

457
01:05:44,010 --> 01:05:48,090
And so this one equals summation of all possible.

458
01:05:48,300 --> 01:06:00,090
So you're counting from the Y axis like all of a integration and you have AJ and this is the number of times of visit state J and then complex events.

459
01:06:01,110 --> 01:06:05,820
So they are absolutely equal, right? It should be.

460
01:06:06,090 --> 01:06:15,330
There's no reason they are not equal. So these are the really the key point is they're really just realizing counting from the X is the same.

461
01:06:15,360 --> 01:06:23,220
Well. Counting from the time versus counting from the state space gave you the same answer for any finite.

462
01:06:24,030 --> 01:06:30,260
So on the right hand side, though, I know that this is the all possible values of J, right?

463
01:06:30,300 --> 01:06:35,100
So they could be in finite state space. I don't need to do anything.

464
01:06:35,700 --> 01:06:44,940
But why are you trying to calculate the average one over and then this one divided by ten here?

465
01:06:47,580 --> 01:06:52,200
So the issue here is if you take. So those are still equal, right?

466
01:06:52,380 --> 01:06:57,060
Those those things. So this is the sample average import to this one.

467
01:06:58,320 --> 01:07:14,510
So far, we don't have an explanation yet. But if you take that and goes to infinity, we fail, take the limit and goes to an entity that size.

468
01:07:15,270 --> 01:07:24,360
You have a statement about all synthetic behavior of this sample mean the right hand side.

469
01:07:24,360 --> 01:07:41,770
So what is this? So what is this limit?

470
01:07:44,160 --> 01:07:48,750
So the limit here is actually the frequency the Markov chain visit state.

471
01:07:48,880 --> 01:07:51,930
J Yes, by definition that's it.

472
01:07:52,710 --> 01:07:57,900
And unless this frequency converge to high object.

473
01:07:57,930 --> 01:08:05,250
Very good. So this is the UN 40 theorem. Hi, J.

474
01:08:07,230 --> 01:08:21,930
I think they were just saying. Well, noticeably, the correlation doesn't play a role in this in the whole proof.

475
01:08:23,100 --> 01:08:29,610
It actually does because it's kind of being swamped in the wording theory.

476
01:08:29,730 --> 01:08:34,290
We we should be involved right here where you invoke the warning theorem.

477
01:08:34,380 --> 01:08:39,020
Right. The this frequency will converge to X equal to J.

478
01:08:39,030 --> 01:08:42,540
So that's one of the interpretation of the theorem.

479
01:08:42,540 --> 01:08:46,920
What does that quantity means? The probability acts equal to J, right.

480
01:08:50,690 --> 01:08:55,490
So it's not just arbitrary correlation. What gave you this type of the results?

481
01:08:55,910 --> 01:08:59,630
This correlation is constrained to a quick markov chain.

482
01:09:00,290 --> 01:09:04,220
So it's in place, but it's important. All right.

483
01:09:04,370 --> 01:09:11,840
So even you have a correlated sample that you still get a more of large number for Markov chain.

484
01:09:12,260 --> 01:09:24,290
And now we can use this idea then. So knowing that you can't use Markov chain to do a monte Carlo, it's actually the first step.

485
01:09:24,860 --> 01:09:30,040
And then the second step is how I. And that's much harder to answer.

486
01:09:30,050 --> 01:09:42,770
So. Well, I think people know the mathematical result of the law of large number for Markov chain for very long time but on to 1950s sixties.

487
01:09:43,100 --> 01:09:51,200
So again, I think the most important information in statistics is not made by statistician.

488
01:09:52,040 --> 01:09:59,210
So the the, the two famous people associated with Markov chain Monte Carlo methods,

489
01:10:01,040 --> 01:10:08,120
Metropolis and Hastings, they are both physicists and that's a physicist.

490
01:10:08,450 --> 01:10:11,810
Do deal with this kind of probability stuff.

491
01:10:12,770 --> 01:10:23,190
David We probably don't don't have time to talk to the details of the Markov Monte Carlo.

492
01:10:23,220 --> 01:10:27,890
But so we're going to talk two versions of Margot Mollycoddle.

493
01:10:28,340 --> 01:10:32,510
One is the Metropolis Metropolis Hastings algorithm.

494
01:10:32,870 --> 01:10:40,400
The other is a group sampler, which is a special case of the Metropolis pasting algorithm.

495
01:10:40,910 --> 01:10:44,360
So the idea would be just construct the Markov chain now.

496
01:10:44,630 --> 01:10:53,570
So now it's dealing with a very specific problem so that if you can draw, you can using Monte Carlo.

497
01:10:54,830 --> 01:10:58,460
Well, so far what we know is we can use a monte Carlo method to.

498
01:11:01,270 --> 01:11:08,980
Based on the Markov chain to do some polling and then still get the a large sample property law of large number.

499
01:11:09,370 --> 01:11:14,439
So the question is really how can you do all from a distribution?

500
01:11:14,440 --> 01:11:17,920
You only know OP to a normalizing constant.

501
01:11:19,840 --> 01:11:24,770
Right. So the idea is construct a type of reversible mark already.

502
01:11:25,750 --> 01:11:28,540
And then that's a really kind of a genius idea.

503
01:11:28,540 --> 01:11:36,040
I don't even you know, I think a lot of case like put yourself into that situation is that there's kind of a natural bond.

504
01:11:36,040 --> 01:11:38,950
I don't think that's very natural, I think.

505
01:11:39,220 --> 01:11:47,590
But if you look at the the you know, the the progress of the algorithm from its very native form to the current form,

506
01:11:47,890 --> 01:11:57,100
they're going through multiple phases. So it's really fun to see how this works, but you have to wait until next Monday.

507
01:11:57,880 --> 01:12:07,120
Okay. So so next Monday, probably the last lecture will cover the materials in the will will cover in the interview.

508
01:12:09,760 --> 01:12:13,749
Okay. I, I will have some work today, but no offense.

509
01:12:13,750 --> 01:12:22,120
All overall, just this. I think it will.

