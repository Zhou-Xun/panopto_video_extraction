1
00:00:09,970 --> 00:00:13,600
Good afternoon, everybody. So why don't we get started? Thanks, everybody, for coming person.

2
00:00:14,770 --> 00:00:25,150
So today we are going to finish the discussion about maximum likelihood and Remo and we will be then dove into oh five,

3
00:00:25,630 --> 00:00:32,680
which is a specific set of techniques and discussions about how can we model the mean.

4
00:00:33,740 --> 00:00:38,500
First, let me deal with some logistics. Assignment.

5
00:00:38,770 --> 00:00:45,460
So homework one has just been assigned and it is going to be due October 5th.

6
00:00:46,300 --> 00:00:50,860
11:59 p.m. So this is just trying to remove any ambiguity about the timing.

7
00:00:51,880 --> 00:01:01,040
So if you click in here. All right.

8
00:01:02,240 --> 00:01:07,370
You can see that this homework comprises of a few different parts.

9
00:01:07,400 --> 00:01:14,300
The first one is serial methods questions. Remember, the course name is Theory and Application Elder.

10
00:01:14,810 --> 00:01:20,150
So we do need to do some work regarding the derivation of important facts.

11
00:01:20,720 --> 00:01:28,900
And this one is going to. Sort of help you practice how to derive maximum likelihood and read more.

12
00:01:29,380 --> 00:01:36,910
But rest assured, this is extremely simple. Setting probably does not need you to use algebra, just simple calculation.

13
00:01:37,300 --> 00:01:41,830
The key thing here is to hit home the definition of minimal.

14
00:01:41,860 --> 00:01:50,050
We will be doing a review after this. Now the question here is about number two is about the notations.

15
00:01:50,680 --> 00:01:53,950
So again, we were covering a lot of notations.

16
00:01:54,280 --> 00:02:01,269
And I want to use question two as an opportunity for you to get a feel of what do these beta represent?

17
00:02:01,270 --> 00:02:11,080
What do these sigma represent? Question three is a simulation study, so you will need to use whatever programing language is most familiar to you.

18
00:02:11,100 --> 00:02:12,089
Ah, python.

19
00:02:12,090 --> 00:02:22,770
Whatever the idea is trying to show you do gain some efficiency when doing a statistical testing or estimation when you account for correlation.

20
00:02:24,890 --> 00:02:29,750
Finally, this is a part of where I believe most of you have done very well in forming teams.

21
00:02:30,080 --> 00:02:36,500
And this is just a kind of one of many problems I would put on the homework.

22
00:02:37,130 --> 00:02:43,250
Again, they do not need to submit anything, but I believe that it is better to act early.

23
00:02:43,250 --> 00:02:49,700
So these things will be some good instructions for you to try to plant the data on the data.

24
00:02:50,090 --> 00:02:56,270
So at least this will have some momentum with regard to the project.

25
00:02:56,570 --> 00:03:02,570
And as we move along the course, we will spend more and more time discussing projects.

26
00:03:05,590 --> 00:03:09,940
I suspect not many of you have looked at homework, so I'll just close here.

27
00:03:09,940 --> 00:03:14,170
And if you have any questions, please let us know.

28
00:03:14,740 --> 00:03:24,730
Second PSA. So this one platform is really something I want to popularize among the class.

29
00:03:25,210 --> 00:03:35,350
I do not know. You know, this is anonymous post and we we do have some concrete answers posted there by our wonderful assignment, Simon being there.

30
00:03:35,800 --> 00:03:42,550
So if you have any questions regarding projects or homework, I think the first place to go is this place.

31
00:03:42,820 --> 00:03:45,970
And you can see how other people's questions got addressed.

32
00:03:46,300 --> 00:03:49,870
And I and I think that's a probably better use of all of our time.

33
00:03:50,170 --> 00:03:57,790
And if you still find some questions to be best addressed by email emailing to me and GSI, please feel free to do that.

34
00:03:57,820 --> 00:04:00,820
I do ask you to exercise some kind of judgment there.

35
00:04:02,340 --> 00:04:06,620
All right. Okay.

36
00:04:06,650 --> 00:04:13,190
So before I get started with the actual techno material, any questions that I can address so far?

37
00:04:23,130 --> 00:04:27,300
Okay. I don't see any hands here, so I'm going to proceed to the actual material here.

38
00:04:50,970 --> 00:05:00,840
So we will first do some quick review of EML and Remo because the last time we talked about this was Wednesday.

39
00:05:00,850 --> 00:05:03,420
So it doesn't hurt to refresh the memory a little bit.

40
00:05:04,160 --> 00:05:15,060
Um, first, we're going to list a few questions to answer and hopefully this serve as a, um, some objectives for you.

41
00:05:16,290 --> 00:05:21,810
So first. Um. How does.

42
00:05:23,300 --> 00:05:26,690
The General's at least square beta had looked like.

43
00:05:29,900 --> 00:05:36,290
As a function. Of the variants currencies.

44
00:05:38,960 --> 00:05:52,970
Okay. So that's the first one. The second question essentially is if we denote beta hat as beta, you know, sigma here.

45
00:05:54,720 --> 00:06:01,560
I'm just going to rise to give my with understanding that we're going to collect all individuals, various currencies.

46
00:06:01,610 --> 00:06:04,890
I'm going to use curly brackets to represent the collection across all the people.

47
00:06:05,580 --> 00:06:11,940
So then the question is which? Sigma tries to plug in here.

48
00:06:16,260 --> 00:06:19,520
All right. And what choices do we have?

49
00:06:22,950 --> 00:06:33,430
And Mel versus Remo, right? And three, you know, how do we.

50
00:06:35,790 --> 00:06:51,700
Evaluate. A relative.

51
00:06:56,770 --> 00:07:04,400
Advantage. Advantages of Amal versus Remo, right?

52
00:07:06,230 --> 00:07:17,560
What criteria? And before.

53
00:07:22,160 --> 00:07:26,660
We were going to talk about, you know, Remo and how.

54
00:07:27,960 --> 00:07:34,410
It's defined. And we will talk about two perspectives.

55
00:07:35,070 --> 00:07:46,400
One is transformation based. The other is integration based.

56
00:08:11,160 --> 00:08:15,270
So I'm going to put an asterisk here because this is new. And we were going to talk about this today.

57
00:08:17,790 --> 00:08:22,440
And finally, you know. How do we compute them?

58
00:08:31,540 --> 00:08:35,220
Okay. So I think that I'll leave out any questions.

59
00:08:35,620 --> 00:08:44,139
So I think these questions are something that I would ask if I am the I am in the first few lectures learning this.

60
00:08:44,140 --> 00:08:49,460
Right. You know. If we're going to estimate beta, how does that look like?

61
00:08:50,150 --> 00:08:58,010
Second, if we know how it looks like, it turns out the answer is it depends on any sigma plug in in the deterministic way.

62
00:08:58,340 --> 00:09:02,180
Then the question then is which one to plug in? You have many choices.

63
00:09:02,870 --> 00:09:06,980
Two major competitors are the maximum likelihood and the Remo.

64
00:09:07,940 --> 00:09:13,640
Three, then you need to have some objective measure to determine which one is a better choice.

65
00:09:14,420 --> 00:09:20,050
So that's the third question. What criteria? Right. Number four before, you know.

66
00:09:21,780 --> 00:09:28,470
Of course, this is probably, I would say, parallel to two and three what is Remo and how it is defined.

67
00:09:28,860 --> 00:09:32,070
And there is more of a question how to.

68
00:09:32,990 --> 00:09:36,650
Conceptually understand this estimate here.

69
00:09:37,160 --> 00:09:40,870
And we have talked about the transformation based perspective.

70
00:09:40,880 --> 00:09:49,820
We will introduce integration based perspective. So we will review that number five in practice, right?

71
00:09:50,000 --> 00:09:52,550
How do we compute these eml raimo?

72
00:09:53,420 --> 00:10:04,249
So number five, I'm only going to touch that on a conceptual level because I can ask you the same question when you are doing R right.

73
00:10:04,250 --> 00:10:07,820
You put in parentheses whites held x ray.

74
00:10:08,450 --> 00:10:14,299
What did you do to get the betas? Well, it's really working hard to get you those betas right.

75
00:10:14,300 --> 00:10:19,760
You know, initialize and then climb the hill of the likelihood and then do the Newton Ralph's or whatever.

76
00:10:19,760 --> 00:10:23,030
Right. So it's rather complicated inside that function.

77
00:10:23,360 --> 00:10:29,480
And I think my goal here is trying to show that we often will need to use those iterative algorithms as well.

78
00:10:30,080 --> 00:10:35,900
And I will not talk about the particulars of the algorithm, but hopefully this can give you some guidance.

79
00:10:36,290 --> 00:10:47,230
Any questions that I should have added? Okay, I why don't we start with this?

80
00:10:47,590 --> 00:10:55,090
So first, how do these estimate look like? So I want to remind you guys that these are in handouts.

81
00:10:56,350 --> 00:11:05,180
I'm going to use Blue to indicate answers. Okay. Handouts. Oh, for a or for be actually has a derivation.

82
00:11:08,080 --> 00:11:13,990
So it looks like this, right? I do need to check the notes.

83
00:11:16,780 --> 00:11:46,690
So. I hope I got this right.

84
00:11:57,870 --> 00:12:04,140
So this is the form. And the key takeaway is that first we got this.

85
00:12:05,250 --> 00:12:15,180
From. Two steps, write a six sigma, then be solve.

86
00:12:16,260 --> 00:12:19,990
For the best. Peter.

87
00:12:20,890 --> 00:12:26,810
But easing what criteria, though? Based on what criteria?

88
00:12:31,200 --> 00:12:35,400
Well, two choices. The. Long likelihood.

89
00:12:36,900 --> 00:12:38,760
Or modified a log likelihood.

90
00:12:42,420 --> 00:12:49,290
While the answer will be a lot like it, because as you'll see, the modify one doesn't depend on beta, so you have nothing to optimize over for beta.

91
00:12:53,720 --> 00:12:57,260
Okay. Just let me get about, like, likelihood.

92
00:12:58,900 --> 00:13:10,070
Right. So that's the first point. But second, we see that a beta had can be read in beautifully as some deterministic function of.

93
00:13:11,010 --> 00:13:15,000
Whatever you plug in. So that leads us to the second question.

94
00:13:21,140 --> 00:13:29,330
Which one to plugging here? So a represent answer here.

95
00:13:30,080 --> 00:13:33,230
So you can do it. You can do either.

96
00:13:39,340 --> 00:13:50,700
But Remo is preferred. Then that leads to question number three.

97
00:13:50,710 --> 00:13:55,250
Why? Well, on what grounds? So often.

98
00:13:56,140 --> 00:14:02,400
Peter Hart with Peter. Oh.

99
00:14:04,810 --> 00:14:08,800
I wish there's a undo button on my pen, which is not the case, unfortunately.

100
00:14:10,370 --> 00:14:17,800
Um. Is less biased.

101
00:14:21,570 --> 00:14:27,140
In finite samples. Okay.

102
00:14:27,260 --> 00:14:38,110
So the criteria is really the bias. Then some of you may ask Jamaica, you know.

103
00:14:39,170 --> 00:14:44,570
How do these two compare? The one with the remote plugged in, the other with male plugged in.

104
00:14:44,960 --> 00:14:49,580
How do they compare when you have a large sample size with a fixed number of covariates?

105
00:14:50,180 --> 00:14:56,150
The answer will be they will be very similar because both will be consistent. In that case, a bias are approaching zero.

106
00:14:56,600 --> 00:15:02,390
So that's why I was emphasizing that these biases often appear less in finite samples.

107
00:15:02,750 --> 00:15:07,290
For Remo. Okay.

108
00:15:07,500 --> 00:15:13,620
Number four, how was Remo defined in the first place?

109
00:15:17,140 --> 00:15:29,610
So the motivation. Is that Mel is so ignorant of the fact that theta itself needs to be estimated.

110
00:15:32,400 --> 00:15:37,650
So when you estimating sigma by email ignores the uncertainty in estimating beta.

111
00:15:52,520 --> 00:16:00,880
Now Remo comes to the rescue. You know, basically acknowledges.

112
00:16:04,870 --> 00:16:11,400
The uncertainty. In estimating beta.

113
00:16:17,200 --> 00:16:27,250
Okay. By doing what? By adding one additional term in the objective function, which gives us the modified log likelihood.

114
00:16:27,880 --> 00:16:31,930
So for that, I'm going to give you the page number and copy that very quickly.

115
00:16:37,070 --> 00:16:40,650
Of handouts. Okay.

116
00:16:41,080 --> 00:16:45,460
Okay. So the additional term is the one over.

117
00:16:46,820 --> 00:17:04,370
To log. You don't have to remember this, but conceptually this is the explicit penalty term added by the Read More objective function,

118
00:17:04,370 --> 00:17:09,200
which we called a modified likelihood. And by doing so, it reduces bias.

119
00:17:11,120 --> 00:17:18,410
And speaking of the objective function, we have alluded to the fact that.

120
00:17:19,470 --> 00:17:28,950
We transform the original data so that the objective so the likelihood of the transform data will only depend on sigma, not beta.

121
00:17:32,770 --> 00:17:37,980
To be more precise. Let me use another card to do this recall.

122
00:17:38,400 --> 00:17:47,610
EML is using a log likelihood where I use this l to represent log likelihood and it will be a function of beta and sigma's.

123
00:17:49,790 --> 00:17:54,510
And the data, right? So what you do is that you held the data.

124
00:17:54,540 --> 00:18:01,799
You hold the data constant and then find the beta in the sigma lies that you want and maximize this thing by doing it.

125
00:18:01,800 --> 00:18:13,350
Transform data. We are going to say we look at another objective function which I know denote as L star.

126
00:18:13,590 --> 00:18:17,300
L star is different from L. And what does it depend on?

127
00:18:17,750 --> 00:18:21,800
Absolutely not, Peter. So it has to depend only on I.

128
00:18:22,880 --> 00:18:27,170
Based on data that's now transformed. So the data gods, it's an asterisk.

129
00:18:28,750 --> 00:18:32,230
Now by maximizing Remo, we got the Sigma II and.

130
00:18:33,340 --> 00:18:36,730
And the only difference between these two things.

131
00:18:37,930 --> 00:18:48,940
It's just that this additional term. So if you recall this process, what we did is to work with the likelihood of a transforming data.

132
00:18:50,050 --> 00:18:55,830
And the. Consequences that we got an objective function that does not depend on beta.

133
00:18:57,070 --> 00:19:01,360
And that sounds like integration. You integrate, Al, it's not relevant.

134
00:19:01,780 --> 00:19:05,620
Hence our second perspective of defining Remo.

135
00:19:06,070 --> 00:19:12,820
I will cover this in the next page, but I want to finish discussing the the fifth point here.

136
00:19:13,090 --> 00:19:17,350
So we will be covering integration based perspective.

137
00:19:18,160 --> 00:19:21,520
Finally, how do we compute EML and Remo?

138
00:19:22,150 --> 00:19:29,650
Again, I am offering you a recipe. This is usually called the coordinate descent or ascent, if you will.

139
00:19:30,580 --> 00:19:36,100
So you initialize theta. Sorry, let me raise this.

140
00:19:37,090 --> 00:19:41,710
Let's say let's initialize sigma I to be.

141
00:19:44,670 --> 00:19:49,750
Our eyes and eye by an eye. So for everybody, the initial value is going to be identity.

142
00:19:49,800 --> 00:19:57,090
This may not be true because the actual data may indicate some correlation among the measurements from one person.

143
00:19:58,020 --> 00:20:04,590
But you can set this initial value and then figure out the beta one.

144
00:20:05,760 --> 00:20:14,840
At the first iteration. So you know how to do this. You plug in initial value here?

145
00:20:16,340 --> 00:20:19,480
And they got beaten out. And then this is a challenge.

146
00:20:19,490 --> 00:20:27,020
Instead you're going to solve for Sigma two next iteration.

147
00:20:28,030 --> 00:20:35,600
What's the best one in there? You got to fix. Hold beta at this particular value.

148
00:20:37,010 --> 00:20:48,290
So this one often requires up to my optimizer that's building our other programs because often we have Sigma I.

149
00:20:49,500 --> 00:20:59,400
As parameter rise by a small number of parameters and by holding beta constant at the prior value or the current value.

150
00:20:59,850 --> 00:21:07,800
All the unknowns are belt theta. So if you throw this whole thing in, which is a function of theta, hopefully optimizer can solve it.

151
00:21:08,850 --> 00:21:12,270
So this works for Remo and EML.

152
00:21:12,690 --> 00:21:15,870
Okay. It just depends on what objective function you started with.

153
00:21:17,790 --> 00:21:23,280
So with that, before I dove into the integration based perspective of defining Remo.

154
00:21:24,000 --> 00:21:28,410
Do we have any questions about this general roadmap or.

155
00:21:29,550 --> 00:21:32,880
Common questions. Happy to answer any. If you have.

156
00:22:06,340 --> 00:22:13,660
And by the way, in homework number one, the first question is going to ask you to derive a few facts,

157
00:22:14,410 --> 00:22:20,650
which I know is going to sound like toy examples, but this hopefully will actually get you to work on the.

158
00:22:22,620 --> 00:22:23,490
And the definition.

159
00:22:24,120 --> 00:22:38,759
So I think the homework question is ix12x and these are all ID with a density function that's not you can say say normal zero or mu sigma squared,

160
00:22:38,760 --> 00:22:45,240
right. And now your task is how do we estimate sigma hat the eml will be?

161
00:22:49,970 --> 00:22:54,760
One overran times. This one. Right.

162
00:22:56,200 --> 00:23:03,070
So this is EML and the REMO will be minus one here.

163
00:23:07,350 --> 00:23:11,940
Clearly you have learned this is probably the thing of choice.

164
00:23:13,470 --> 00:23:23,040
All we ask and homework is okay. Now you have a much more abstract and higher level perspective of what is.

165
00:23:23,580 --> 00:23:28,290
My claim is that you can derive this thing using either the transformation

166
00:23:28,290 --> 00:23:31,950
based approach or the integration based approach that I'm going to talk about.

167
00:23:32,550 --> 00:23:35,730
And if you can just do the integration based approach, that's golden.

168
00:23:36,000 --> 00:23:40,080
If you want to do the other way and that's golden if you want to do both, all the power to you.

169
00:23:40,410 --> 00:23:43,830
So the idea is that to derive this is real.

170
00:23:44,460 --> 00:23:48,720
So simple enough example, right? Not too surprising, but hopefully this can illustrate the point.

171
00:24:06,680 --> 00:24:12,350
All right. So now let's just cover the integrated likelihood perspective for defining Remo.

172
00:24:13,220 --> 00:24:19,370
It is actually pretty straightforward. What I will do is trying to give you a sense of what these steps are.

173
00:24:19,850 --> 00:24:23,570
I am in no way of making myself miserable by driving everything here.

174
00:24:23,990 --> 00:24:26,090
I'm just trying to guide you through this note.

175
00:24:26,390 --> 00:24:37,070
And I think you do need to watch the video that I have put online, I believe, last Monday or Tuesday, to get a sense of the actual derivation.

176
00:24:37,550 --> 00:24:42,050
But these are the technical details. If you choose, you can follow.

177
00:24:42,530 --> 00:24:47,630
As I said, when you are doing the homework, the integration actually follow the same logic.

178
00:24:47,840 --> 00:24:56,540
But because we were only dealing with a scalar situation, you actually do not need to deal with a complicated vector or matrix operation.

179
00:24:56,900 --> 00:25:03,770
So my hope is that by doing the homework you got idea of Remo, but without going through the,

180
00:25:04,100 --> 00:25:07,640
you know, all the technical challenge of dealing with these algebra.

181
00:25:08,510 --> 00:25:16,340
For this one though, I still want to give you this opportunity for those who are capable or who are willing to go through these derivation.

182
00:25:16,760 --> 00:25:22,640
So the fundamental idea is that we can start with these like heard.

183
00:25:23,060 --> 00:25:28,610
So I have used a capsule and this is just the not logged version.

184
00:25:28,640 --> 00:25:37,010
Okay. Just original likelihood, as we know, l depends on beta and sigma.

185
00:25:40,910 --> 00:25:46,340
So what we do is just integrate our data as in the first row.

186
00:25:48,470 --> 00:25:56,280
Okay. So from the first equation to the second, we basically plug in.

187
00:25:58,480 --> 00:26:06,700
The definition. Of the likelihood, the Gaussian likelihood, which I will not repeat here a second.

188
00:26:07,120 --> 00:26:15,050
You factor l terms. That's our.

189
00:26:16,800 --> 00:26:20,790
Irrelevant. To better.

190
00:26:22,980 --> 00:26:25,990
Okay, because you do not need to use those terms when integrate over beta.

191
00:26:27,100 --> 00:26:32,620
And the real technical challenge is to figure out these three steps.

192
00:26:33,130 --> 00:26:36,400
I treat them as optional so I will not cover them.

193
00:26:37,090 --> 00:26:43,900
But if you actually have extra time or wants to take up this challenge, you can try to derive this.

194
00:26:44,170 --> 00:26:47,650
And I'm happy to answer any questions through office hours.

195
00:26:48,340 --> 00:26:56,950
But all these things in the middle are trying to get to the final point where you can see that this term looks quite familiar.

196
00:26:58,470 --> 00:27:05,870
This is. This is the same.

197
00:27:08,830 --> 00:27:22,830
Additional term. We saw in the modified.

198
00:27:26,720 --> 00:27:43,630
A large likelihood. Based on the transformation approach.

199
00:27:57,810 --> 00:28:01,710
By the same, I mean, you probably have to take the law, right?

200
00:28:02,170 --> 00:28:03,150
Yeah, take the law.

201
00:28:09,200 --> 00:28:20,510
So this claim is to say that although we were integrating with beta to eliminate beta from the objective function, it's a totally different approach.

202
00:28:21,050 --> 00:28:26,600
But what gave it gave us was a same objective function compared to what we saw before.

203
00:28:26,840 --> 00:28:30,980
Hence, it's not surprising that the resulting rental estimate will be the same.

204
00:28:31,430 --> 00:28:34,610
So these are just two different paths towards understanding the same thing.

205
00:28:35,150 --> 00:28:42,080
I believe this one is more technical, so less intuitive compared to the transformation based approach.

206
00:29:28,530 --> 00:29:36,110
Okay. Any questions? All right.

207
00:29:37,250 --> 00:29:41,720
So with that, we will be diving to.

208
00:29:43,150 --> 00:29:50,620
Kind of the actual model specification and choices. So we're going to switch gears to get a sense of where we are now.

209
00:29:51,190 --> 00:29:57,430
So this is the home page of the canvas website. We started with a introduction and.

210
00:29:58,420 --> 00:30:01,900
General introduction to the new models with correlated errors.

211
00:30:02,470 --> 00:30:10,940
We also talking about Gaussian distributions. Then in the past two weeks or one week, rather,

212
00:30:11,630 --> 00:30:20,450
we have been talking about how to estimate media models using techniques what we call maximum likelihood or even better.

213
00:30:20,450 --> 00:30:25,380
Remo. So as you can see, this is a big umbrella of how you would do things.

214
00:30:25,410 --> 00:30:33,600
This is a framework. It does it still does not show us for any particular data set how we would build a model,

215
00:30:33,630 --> 00:30:38,970
how we would choose among many different choices of specifying the model in the various model.

216
00:30:39,510 --> 00:30:47,400
So in the next part of this class, we will be using the framework of General Nina model,

217
00:30:47,880 --> 00:30:54,000
using the framework of generalized least squares and what you learned regarding Remo.

218
00:30:54,330 --> 00:30:57,660
To apply those knowledge, to do actual modeling.

219
00:30:58,200 --> 00:31:05,980
So this is where we are today. We are going to use a simple technique to analyze the meme model.

220
00:31:05,990 --> 00:31:15,220
So it's about the exact time speed apart and it's called analysis of response profiles in the next.

221
00:31:16,630 --> 00:31:23,080
Two or three lectures. We will be covering more complicated ways of modeling the mean.

222
00:31:24,280 --> 00:31:31,510
And then we will move on to model the covers. So hopefully by presenting the framework first,

223
00:31:31,990 --> 00:31:38,710
you will get a sense of the arsenal of tools you would need to actually fit the model for any particular data set.

224
00:31:41,570 --> 00:31:44,540
So I'm going to switch to handouts. Oh five.

225
00:31:46,040 --> 00:31:57,890
So this lecture note has been, at least to me, is relatively straightforward, less technical compared to what you've seen before.

226
00:31:58,870 --> 00:32:09,430
Um. The emphasis will be on trying to apply the notation you learned and knowledge you learned to the specific modeling task here.

227
00:32:10,480 --> 00:32:11,710
So what are the objectives?

228
00:32:12,490 --> 00:32:24,370
So the first objective is to describe the scientific hypothesis that often comes with analysis of response profiles and by response profiles.

229
00:32:24,820 --> 00:32:32,910
I mean trajectories over time. Over time.

230
00:32:33,150 --> 00:32:42,560
Okay. So that's called profile. And if you will, let's make it more specific mean trajectories over time.

231
00:32:44,450 --> 00:32:53,810
And particular emphasis will be placed on interaction the group by time interaction so the group can be treatment group.

232
00:32:54,380 --> 00:32:58,580
For example, I am on a new drug. You are on the placebo, right?

233
00:32:59,300 --> 00:33:04,130
Do our trajectory on average defer since time zero.

234
00:33:04,400 --> 00:33:08,780
If they do, then that may indicate the drug is doing something differently.

235
00:33:10,520 --> 00:33:16,340
Second, we will formulate these hypotheses using notation.

236
00:33:17,380 --> 00:33:26,740
Apologies. There should be no us here because yeah, we will try to put everything into the general, you know, model framework.

237
00:33:27,220 --> 00:33:34,150
So, you know, you can apply the notation to answer the question you posed in the first place.

238
00:33:34,900 --> 00:33:44,950
And there we will be using the TLC trial data for those who have figured out it's about the, you know, drug to reduce the black light level.

239
00:33:47,110 --> 00:33:51,399
And technically we will be distinguishing. So I mean. Cody and reference make reference.

240
00:33:51,400 --> 00:33:54,220
So coding schemes, I think these are pretty straightforward.

241
00:33:56,620 --> 00:34:02,890
Even if we do not cover the final point today, I think we can talk about them pretty quickly in the next one.

242
00:34:04,300 --> 00:34:12,640
So to recap formula hypotheses, the second a represent hypotheses in mathematical notation.

243
00:34:14,440 --> 00:34:19,900
So why it is important to formulate hypotheses so it has something to do with science, right?

244
00:34:20,260 --> 00:34:23,590
What is science about? You know, you put forward a hypotheses.

245
00:34:26,360 --> 00:34:33,680
Right. And then you collect data. And then you decide.

246
00:34:36,050 --> 00:34:41,120
Is evidence against. We have policies.

247
00:34:47,140 --> 00:34:58,130
And then you draw a conclusion, right? So this is not necessarily a new process.

248
00:34:58,970 --> 00:35:00,820
For example, if you work,

249
00:35:01,340 --> 00:35:11,120
if you know people if you work in genomics data you have you can fly a lot of information without even be clear about what what you want to do.

250
00:35:11,810 --> 00:35:17,690
So that's where the kind of the multiplicity or fishing expedition analogy comes into play.

251
00:35:18,590 --> 00:35:24,260
You collect data and you find something interesting and I say, Hey, actually, that's the way I was thinking about.

252
00:35:24,770 --> 00:35:34,380
And it's not. It's not. So that's where that's why it's important to pre-specified hypotheses and collect data.

253
00:35:36,850 --> 00:35:43,630
Use statistical tests or procedures to determine whether you have enough evidence against hypotheses.

254
00:35:44,740 --> 00:35:51,390
And then you draw a conclusion. And remember, science has never once and for all process its iterative process.

255
00:35:51,400 --> 00:35:59,230
You refine the understanding so often you would need to plug it into forming a new hypotheses and then start a whole process again.

256
00:36:01,280 --> 00:36:09,110
Okay. So what's the consequence of not formulating a hypothesis in the first place?

257
00:36:09,830 --> 00:36:13,700
So I want to this is not on the slides.

258
00:36:13,760 --> 00:36:19,750
I just thought about this now. So there is a fallacy called. Texas sharpshooter fallacy.

259
00:36:20,320 --> 00:36:23,840
So there is a guy who claims he is a sharpshooter, right?

260
00:36:25,000 --> 00:36:28,930
So in the night, so in his village, there's a big barn.

261
00:36:29,120 --> 00:36:33,790
You know, he basically shoot randomly on the barn. And of course, they'll be bullet holes.

262
00:36:33,820 --> 00:36:40,720
Right. So when the on the day break, you know, he just to find where the bullet hole is and then draw the bull's eye around it.

263
00:36:40,960 --> 00:36:43,960
Right. And then when people come out to work.

264
00:36:44,290 --> 00:36:49,209
So. Hey, hey, guys. I am a sharpshooter. Look, all the bullet point.

265
00:36:49,210 --> 00:36:57,710
Bullets are around the bullseye appended. But those villagers did not know this guy painted bull's eye.

266
00:36:57,980 --> 00:37:01,730
Sorry. Bull's eye. Around the bullet points.

267
00:37:02,090 --> 00:37:11,250
Sorry, the bullet holes after he had shot them. Right. So if he has a target and then you ask him to shoot, maybe he is not that good.

268
00:37:11,820 --> 00:37:15,030
So that's the point of this pre specification hypothesis.

269
00:37:15,690 --> 00:37:18,850
You got to be willing to be wrong. You got to be willing to say, Hey, this is Target.

270
00:37:18,870 --> 00:37:20,820
Let's shoot and see if you can shoot at it. Right.

271
00:37:21,240 --> 00:37:29,070
So in general, it's extremely important when you're conducting scientific studies to say what's hypothesis.

272
00:37:29,100 --> 00:37:33,569
Otherwise, you're just going to be willing to be changed by whatever new information you see and claim.

273
00:37:33,570 --> 00:37:41,430
That's the original thought you have. So with that, we do want to go back to the longitudinal data setting.

274
00:37:44,120 --> 00:37:50,900
So why do we introduce these techniques called, um, analysis of response profiles?

275
00:37:51,230 --> 00:38:00,350
There are a few. First, we want to start with a relative minimal set of restrictions on the mean responses over time.

276
00:38:00,860 --> 00:38:05,900
And on the cover ans among the repeated measurements. So this is not surprising, right?

277
00:38:06,800 --> 00:38:11,420
We have said we want to model the mean and we have also said we want to model the sigma.

278
00:38:12,110 --> 00:38:17,300
And what is a situation where we do not place any assumptions on them?

279
00:38:19,150 --> 00:38:27,960
So. When you do know one place, a lot of assumptions in general, you require some structure of the data.

280
00:38:30,000 --> 00:38:32,220
We were going to be focusing on these settings.

281
00:38:32,730 --> 00:38:40,370
First, the timing of the repeated measurements are common to all individuals in the study and the same number of repeated measures for individual.

282
00:38:40,410 --> 00:38:43,790
So using one single word. Balance.

283
00:38:46,290 --> 00:38:48,210
This technique we will introduce, though,

284
00:38:48,390 --> 00:38:58,320
will work with situations where some data points for some people will be missing under an original balanced design.

285
00:38:58,980 --> 00:39:26,010
So this indeed can handle missing data. So this method and analysis of response profiles, it is a flexible method.

286
00:39:26,190 --> 00:39:35,610
It often works for the setting where individual belongs to groups like treated versus non treated or different age groups.

287
00:39:37,680 --> 00:39:45,270
We will not be talking about continuous cohorts at this time because this method does not work too well with continuous takeovers.

288
00:39:45,300 --> 00:39:51,150
It does require categorization of people into different groups where the group membership does not change over time.

289
00:39:54,800 --> 00:40:03,650
And it is also flexible because we will not specify how the main trajectory will be linear, quadratic, cubic.

290
00:40:04,040 --> 00:40:09,080
We will not be specifying those. We are willing to be very flexible in that, in that.

291
00:40:14,700 --> 00:40:19,440
So let's look at what we mean here. This is the.

292
00:40:22,730 --> 00:40:28,300
A plot with eight dots for the four dots on the two on the top.

293
00:40:28,310 --> 00:40:32,450
These are the main values of the outcome in this case.

294
00:40:32,450 --> 00:40:37,280
It's the mean blood level of four people in the placebo group.

295
00:40:38,220 --> 00:40:41,360
At a week zero, week one and week four, week six.

296
00:40:41,390 --> 00:40:45,900
Right. So you can see that relative to the group that took the drug.

297
00:40:47,340 --> 00:40:50,400
The placebo group seems to have higher blood level.

298
00:40:50,950 --> 00:40:59,570
Right. So the fact that we can calculate the mean and then connect these dots using segments.

299
00:41:00,860 --> 00:41:04,970
Is representing the fact that we can use a very relatively flexible method.

300
00:41:07,050 --> 00:41:12,090
To compare the two. Without making any Nini or quadratic assumptions.

301
00:41:13,020 --> 00:41:16,620
So here this is going to be called response profile.

302
00:41:20,260 --> 00:41:29,520
Four placebo group. And this is the response profile.

303
00:41:33,340 --> 00:41:54,400
For the treat a group. So without using any statistical math, you can see that, hey, the two groups seems to be very different.

304
00:41:54,430 --> 00:42:03,280
They are not parallel. They started roughly the same place because of randomization, because when they were recruited to study this,

305
00:42:03,850 --> 00:42:09,130
the study manager said, Hey, we flip a coin to assign you to one or two groups.

306
00:42:09,610 --> 00:42:16,120
So this started roughly an equal place. And then it seems that the drug is working and then reduced about that level dramatically.

307
00:42:16,660 --> 00:42:22,700
And then it went up because the equilibrium the body needs to achieve.

308
00:42:22,720 --> 00:42:26,920
So let's start at other places would be released into the bloodstream.

309
00:42:26,920 --> 00:42:31,870
So it goes up. Okay. So you don't need a statistic to see this difference.

310
00:42:32,560 --> 00:42:36,850
And our goal is actually trying to use this simple example.

311
00:42:37,850 --> 00:42:41,990
To say that when the signal is less strong, is this when they're actually quite close?

312
00:42:42,290 --> 00:42:47,750
How do you claim that there is significant evidence against parallel trend?

313
00:42:48,080 --> 00:42:54,290
So that's that's our goal, right? Because I think I told some of you this doesn't have to repeat again.

314
00:42:55,130 --> 00:43:02,240
If the if the signal is super strong like this, you probably don't need a statistician, you know, of a first grader will tell this.

315
00:43:02,270 --> 00:43:06,770
Yeah. If there is no signal, if a signal is very weak, nobody can do anything.

316
00:43:09,070 --> 00:43:12,190
It is in the middle where statisticians thrive. Okay.

317
00:43:12,550 --> 00:43:22,090
So when you have some signal, when you when your data is not a mess, you actually can figure out a way to say test a hypothesis efficiently.

318
00:43:23,430 --> 00:43:28,020
So we're just going to do exactly that. How to consider the hypotheses and how to formulate them.

319
00:43:30,590 --> 00:43:39,430
So to summarize, what we just did is to calculate the arithmetic may of the sponsors at each occasion and join them by a series of line segments.

320
00:43:39,440 --> 00:43:54,520
So very simple. Some questions we can ask by using that data.

321
00:43:54,790 --> 00:43:59,890
First, can we characterize a pattern, a change in the mean response over time in the groups?

322
00:44:00,640 --> 00:44:06,520
Right. And can we determine whether the shapes of the mean response profiles differ among the groups?

323
00:44:11,350 --> 00:44:15,459
So what we will be illustrating will be based on two groups, but it is generic.

324
00:44:15,460 --> 00:44:21,400
So if you have three groups or four groups or five groups, this kind of scheme also works.

325
00:44:42,910 --> 00:44:46,450
A side note the row of baseline response measurements.

326
00:44:48,490 --> 00:44:59,020
So you will come across this issue repeatedly so it doesn't hurt to explicitly discuss this in the TLC trial.

327
00:44:59,920 --> 00:45:04,720
Kids or people were randomized at baseline to receive the drug or not.

328
00:45:06,220 --> 00:45:10,310
So it's not surprising that based on that plot. The means are similar.

329
00:45:12,910 --> 00:45:18,010
In longitudinal studies. It is the is is not that based on comparison that's of interest.

330
00:45:18,220 --> 00:45:21,970
Rather it's the difference in how people change.

331
00:45:22,980 --> 00:45:26,010
From the baseline between the two groups would be of interest.

332
00:45:27,060 --> 00:45:28,770
So to put in more succinctly,

333
00:45:29,280 --> 00:45:36,720
the objective is to compare the patterns of change in the blood level from the baseline over time across treatment groups.

334
00:45:37,980 --> 00:45:44,370
So in essence, the first or the baseline measurements, well, that was a blood level measurement, right?

335
00:45:44,370 --> 00:45:51,640
So it's still an outcome. It's not too conceptually different from other outcomes you would measure later in the study,

336
00:45:53,290 --> 00:46:00,790
but it has some uniqueness because those were collected in this study prior to the randomization.

337
00:46:01,330 --> 00:46:04,570
So they enjoy the benefit of being balanced between the three groups.

338
00:46:07,670 --> 00:46:16,280
However, this may not be the case in observational studies where you do not have the luxury of randomizing people.

339
00:46:23,520 --> 00:46:31,140
So for example, if your goal is trying to study whether the baseline smoking status would impact the trajectories.

340
00:46:33,050 --> 00:46:36,650
Well, good luck to you if you can randomize smoking status after, you cannot.

341
00:46:37,160 --> 00:46:43,280
So by grouping people into smoking versus not smoking, you have already introduce some kind of bias between the two groups.

342
00:46:43,790 --> 00:46:53,010
So statistically, you will need to account for that difference. Second.

343
00:46:53,820 --> 00:46:58,870
Actually, it's not second points for this point here regarding the baseline measurements.

344
00:46:58,920 --> 00:47:04,230
So this has mostly everything to do with study execution.

345
00:47:04,710 --> 00:47:08,310
So whenever you're trying to, say, spend $2 million to collect data,

346
00:47:08,640 --> 00:47:13,980
you got to be very specific about who are the people you're going to collect information upon.

347
00:47:14,640 --> 00:47:19,290
Often not the entire entire population, but rather a segment of the population.

348
00:47:19,770 --> 00:47:27,870
So there when you're reading papers or when you're trying to define cohort, you want to be extremely clear about the inclusion criteria.

349
00:47:28,740 --> 00:47:32,100
Some people like to call this inclusion exclusion criteria.

350
00:47:33,150 --> 00:47:41,430
As his colleague inclusion criteria. It's just simpler. So remember that when you plotting the distribution that based on measurements,

351
00:47:41,970 --> 00:47:50,070
they probably represents the study execution that has constrained the ranges of many different measurements.

352
00:47:51,870 --> 00:47:54,450
So those are some quick notes about baseline measurements.

353
00:47:56,600 --> 00:48:08,510
Returning back to the hypothesis that we can pose for this kind of longitudinal study, we will going to focus on and repeated measurements.

354
00:48:10,990 --> 00:48:14,620
On the small number of groups, say two groups for illustration.

355
00:48:15,940 --> 00:48:22,710
We often can ask three questions. Question number one.

356
00:48:23,640 --> 00:48:32,400
Are the mean response profiles similar across the groups in the sense that the main response profiles are parallel.

357
00:48:33,390 --> 00:48:37,240
Key word parallel. Okay.

358
00:48:37,510 --> 00:48:42,570
So if. If compare the two curves, they're parallel.

359
00:48:42,580 --> 00:48:48,930
It means that the change from the baseline will always be identical regardless of which group you're in.

360
00:48:49,590 --> 00:48:54,470
Right. That's a definition parallel. You know, the change are going to be synched.

361
00:48:56,370 --> 00:49:01,610
So this is a kind of null hypothesis regarding the interaction.

362
00:49:01,620 --> 00:49:04,890
So parallel means there is no group by time interaction.

363
00:49:06,390 --> 00:49:13,410
And often this is the most important hypothesis you want to ask using longitudinal data.

364
00:49:20,740 --> 00:49:26,960
In more detail. Essentially.

365
00:49:28,010 --> 00:49:31,790
It is asking the patterns of change relative to the baseline.

366
00:49:39,670 --> 00:49:43,270
And it's often the key for the longitudinal study.

367
00:49:45,100 --> 00:49:50,659
Often the million dollar question, if you will. I said three questions.

368
00:49:50,660 --> 00:49:55,760
So the next two questions often are of secondary interest.

369
00:49:56,390 --> 00:49:59,540
They do have some relevance, but of secondary secondary interest.

370
00:50:06,920 --> 00:50:11,960
So let's go into them. Question number two. Time may affect.

371
00:50:13,620 --> 00:50:21,090
Now we are operating under the hypothesis that the two curves are parallel.

372
00:50:22,080 --> 00:50:26,370
Well, they can be parallel a quadratic. They can be parallel cubic.

373
00:50:26,460 --> 00:50:35,640
Right. Well, this is parallel to constant. So if it's causing over time, it means that time plays no effect in changing the average outcome.

374
00:50:37,020 --> 00:50:41,750
Not too shocking to know. So this is what we call time effect. Next.

375
00:50:42,820 --> 00:50:51,430
Group effect. You can have the change over time and under the assumption that these two curves a parallel.

376
00:50:52,600 --> 00:50:56,500
The group that asks, do they have vertical shifts?

377
00:50:58,830 --> 00:51:07,550
Here. I'm showing you actually two curves, one with the empty dots, the other with solid dots.

378
00:51:07,560 --> 00:51:11,460
You cannot see them. You cannot separate them apart because they overlay with each other.

379
00:51:12,510 --> 00:51:16,770
This is to say that regardless of what group you in, the mean levels will be the same.

380
00:51:17,130 --> 00:51:20,490
So here it is a null hypothesis of no group effect.

381
00:51:30,820 --> 00:51:38,979
Some quick comments, as I have alluded to when we were formulating or presenting the question number two in question.

382
00:51:38,980 --> 00:51:42,550
Number three, remember, too, is about the time, in fact.

383
00:51:44,540 --> 00:51:51,530
Three is about group effect. We were making the implicit assumption that the two curves are parallel.

384
00:51:53,670 --> 00:51:59,720
What if it's not parallel? Then it does not make too much sense to only talk about time.

385
00:52:00,610 --> 00:52:09,570
Effect or group effect separately. And this reflects general principle that when there is interaction among them,

386
00:52:09,840 --> 00:52:17,740
you often do not interpret that may effect unless you are focusing on a particular segment of population.

387
00:52:17,760 --> 00:52:23,040
So fixing access to particular value, the effect of Z is right.

388
00:52:24,500 --> 00:52:27,100
And you can see the effect is constant across the US.

389
00:52:28,190 --> 00:52:36,830
So again, if you do not have the parallel ness between the two curves, in general, we are not interested in the may effect.

390
00:52:41,230 --> 00:52:46,840
And point on before, I don't want to cover them. Essentially it is how you would interpret interaction term.

391
00:52:47,110 --> 00:52:54,760
You always have to say for a particular value of the confounder you want to communicate.

392
00:52:55,060 --> 00:53:00,520
The effect of the covered of interest is of certain magnitude.

393
00:53:00,910 --> 00:53:04,030
If that confounder change to another level, the effect may be different.

394
00:53:04,060 --> 00:53:08,660
So that's the interaction. Okay.

395
00:53:09,500 --> 00:53:18,800
Why don't we take a five minute break and come back at around 359 and we will continue to finish some more slides of this lecture.

396
00:58:59,980 --> 00:59:03,310
All right. Okay, everybody. So why don't we get back to work?

397
00:59:06,470 --> 00:59:10,720
Okay. So let's see some action regarding annotations.

398
00:59:12,770 --> 00:59:17,509
I'm going to jump to slides 22 because I was reviewing the slides.

399
00:59:17,510 --> 00:59:20,700
I realized that I was repeating some of the.

400
00:59:22,960 --> 00:59:27,770
Some of the kind of. Statement about the study.

401
00:59:27,780 --> 00:59:30,470
So I'm just going to jump directly to the mathematical observations here.

402
00:59:30,500 --> 00:59:34,580
Hopefully, this can for this, you can see some actions about how to follow the models.

403
00:59:35,060 --> 00:59:39,510
I'm going to enlarge this. So consider this structure right here.

404
00:59:39,530 --> 00:59:46,290
You can see a table with two groups, freedom and then group and at each of the end time points.

405
00:59:46,290 --> 00:59:53,070
So you're going to have separate means. So here I am using one to represent for the treated group.

406
00:59:53,090 --> 00:59:57,020
What's the average outcome? What's the average bottle level at the first occasion?

407
00:59:57,290 --> 01:00:04,940
If you go from left to right, clearly they represents the meaning of the lower level or the second third up to the first occasion.

408
01:00:05,330 --> 01:00:07,610
Similarly, you do this for the control group.

409
01:00:08,000 --> 01:00:14,240
Now, if you take the difference vertically, they will give you what we call Delta Delta one, Delta two, and Delta N.

410
01:00:16,280 --> 01:00:21,200
In Greek letters, Delta often represent difference. So hopefully this is easy to remember.

411
01:00:22,940 --> 01:00:31,220
Before we leave this slide again, as I promised when I was talking about response profiles or other more specifically being response profiles,

412
01:00:31,610 --> 01:00:37,070
I mean the vector of means across all the occasions.

413
01:00:37,430 --> 01:00:40,940
And you see that here, you don't have a index.

414
01:00:43,440 --> 01:00:48,950
It is shared by all the people in group, in the trade group as input.

415
01:00:48,990 --> 01:00:55,140
Bullet point number one or shared by all the people in the control group as in bullet point two.

416
01:00:55,740 --> 01:01:02,310
Right? So this is some assumption we've made because to present this idea, we want to simplify things dramatically.

417
01:01:02,700 --> 01:01:07,200
We consider these people are now going to be different beyond the treated versus non treated.

418
01:01:07,350 --> 01:01:13,140
Okay. And clearly you can argue that, hey, in real data, there are so many other factors that may make the two groups different.

419
01:01:13,350 --> 01:01:16,830
True. That's where we will be introducing more complicated models.

420
01:01:17,340 --> 01:01:22,950
But for now we're going to focus on contrasting the treated versus non treated.

421
01:01:25,270 --> 01:01:32,650
So by parallel of the two main profiles, clearly it indicates that all the delta are the same.

422
01:01:33,190 --> 01:01:37,780
So just to for completeness, let me just read these bullet points with you.

423
01:01:38,920 --> 01:01:41,710
So what's the what's the main scientific question?

424
01:01:42,220 --> 01:01:48,820
We want to compare how the novel treatment do relative to the control in terms of the change of the Marine response at the time.

425
01:01:49,300 --> 01:01:54,130
Okay. Um, we said that it's about the group by time interaction.

426
01:01:54,520 --> 01:02:01,960
It is really an interaction. Look, if you look at the hypotheses, it is saying all the changes are going to be the same.

427
01:02:03,630 --> 01:02:09,120
Okay. So regardless of what time point you're in, you're at like occasion, one or occasion.

428
01:02:10,570 --> 01:02:15,400
The difference is same. So no time by group interaction.

429
01:02:20,640 --> 01:02:29,250
And for this single equation in bullet point number three actually implies N minus one constraints, right?

430
01:02:29,280 --> 01:02:32,339
Because you've got to make sure that each pair are going to be the same.

431
01:02:32,340 --> 01:02:35,820
And this is encoded by n minus one constraints.

432
01:02:36,030 --> 01:02:40,080
So for this test, you will it will have n minus one degrees of freedom.

433
01:02:41,160 --> 01:02:42,720
If this now is rejected,

434
01:02:43,050 --> 01:02:51,270
then the claim is that the two groups have none parallel mean response profiles and the patterns of change over time differ in the two groups.

435
01:02:53,040 --> 01:02:58,660
By the way, can you ever say you accept a hypothesis? No.

436
01:02:58,680 --> 01:03:01,750
Right. Okay? Yeah.

437
01:03:01,780 --> 01:03:10,270
After we do not say something, no hypotheses. My favorite example is to contrast Newton's law versus general relativity.

438
01:03:12,480 --> 01:03:18,030
Okay. So for people living in 1800s or 7000s, did they accept Newton's law?

439
01:03:18,750 --> 01:03:21,980
Yeah, maybe they did, but they did it wrong. Okay.

440
01:03:23,880 --> 01:03:28,950
Why so incredulous in rigorous statistical terms?

441
01:03:29,430 --> 01:03:37,350
What they did is that at Newton's age of measurements, of observation, they did not collect accurate enough data to reject.

442
01:03:38,900 --> 01:03:44,060
The. No, that's a f equals end times. And then comes better measurements.

443
01:03:45,020 --> 01:03:52,360
What are the observations that. Triggered Einstein to invent the general relativity.

444
01:03:54,040 --> 01:04:00,009
So homework for you. But the idea is that it collected more data to show that.

445
01:04:00,010 --> 01:04:05,770
Hey. Ethical them or Newton's law does not make sense in these settings.

446
01:04:05,800 --> 01:04:10,050
Right. So it is then they reject the general.

447
01:04:10,690 --> 01:04:18,549
General, the fact that they reject the hypothesis. Newton's law captures everything in terms of the movement of the celestial objects.

448
01:04:18,550 --> 01:04:25,930
Right. So. So then did you accept Einstein's general relativity?

449
01:04:28,480 --> 01:04:37,900
To be rigorous. You probably should not. But in a most kind of everyday sense, you probably accepted.

450
01:04:38,350 --> 01:04:42,820
Why is that? Well, we have not produced data that refutes that.

451
01:04:43,740 --> 01:04:47,820
But does that mean we never will? We don't know. So anyway.

452
01:04:47,880 --> 01:04:53,310
Same thing here. When you do not have enough evidence against us, um.

453
01:04:54,180 --> 01:04:57,180
Just say that we do not have enough evidence to.

454
01:04:58,840 --> 01:05:04,600
To say this may not be true, you know. Anyway, just a side note, I'd like to share this perspective.

455
01:05:06,310 --> 01:05:16,770
Okay. All right. So as we have said, if you have collected enough evidence, all we can say is that the null may be rejected.

456
01:05:16,770 --> 01:05:21,990
Or if we yeah. We reject the NA so there might be some interruptions.

457
01:05:22,260 --> 01:05:26,610
But do we know by rejecting all that how the two groups differ in the trajectories?

458
01:05:26,640 --> 01:05:31,580
We do not. And this test is what we call omnibus test a.

459
01:05:32,730 --> 01:05:39,460
What does this word mean? Uh, just means that it's kind of works for very general situations,

460
01:05:39,910 --> 01:05:46,570
but the price you pay is that it's not specific enough to tell you how the null hypothesis is wrong.

461
01:05:46,730 --> 01:05:49,750
Right. Hmm.

462
01:05:52,940 --> 01:05:59,570
So this actually motivates our people to build alternatives.

463
01:06:00,110 --> 01:06:03,800
Alternative hypotheses, that's more specific.

464
01:06:03,830 --> 01:06:10,400
Look, if you if I flip to the previous slide, the alternative hypothesis essentially is everything but.

465
01:06:10,520 --> 01:06:17,150
Right, everything, but they are equal. But clearly, you can assume that, hey, these differences are going to trend in the early or cosmetically.

466
01:06:17,180 --> 01:06:19,960
Right. So those are more specific alternatives.

467
01:06:19,970 --> 01:06:28,250
If you construct those alternatives, if you reject them, then you know, it is the data is favoring those quadratic or cubic alternatives.

468
01:06:32,090 --> 01:06:44,630
So in that spirit, we will need to sort of mature a little bit by using what we have formulated in terms of general need model,

469
01:06:45,170 --> 01:06:50,600
i.e. using the betas to characterize these weird things.

470
01:06:51,950 --> 01:06:56,150
We did not talk about Delta in our formulation of General Neenah model.

471
01:06:56,930 --> 01:07:03,800
And my claim is that you can use general model generally in any model to formulate this novel pretty easily.

472
01:07:04,970 --> 01:07:10,100
This may be trivial to some of you, but I still want to go over this just so you can see how this is done.

473
01:07:11,910 --> 01:07:14,940
So this brings us to part two. Okay.

474
01:07:15,330 --> 01:07:22,770
So recall in part one, we were focused on talking about the primary scientific questions of interest.

475
01:07:22,860 --> 01:07:26,430
Three questions, the group by time interruption. That's a big one.

476
01:07:27,660 --> 01:07:34,110
It concerns a parallel whether the two curves from the three groups are parallel or if you have three groups, whether they are all parallel.

477
01:07:34,770 --> 01:07:40,640
Then the question number two is about whether time has an effect, whether the trajectory goes up or down or.

478
01:07:42,480 --> 01:07:49,260
Kept constant. Number three, it's a group effect whether the groups defers in their trajectory levels.

479
01:07:49,620 --> 01:07:53,310
And often we focus on the first one.

480
01:07:54,690 --> 01:08:00,240
So here we are going to again use a same data set.

481
01:08:00,780 --> 01:08:06,000
But to use the notations, we have learned to call.

482
01:08:06,240 --> 01:08:11,310
We have decided to formulate the model like this.

483
01:08:12,810 --> 01:08:17,180
And I am using this word very liberally. Okay. Appropriate choices of IXI.

484
01:08:17,190 --> 01:08:19,499
And in any particular data analysis,

485
01:08:19,500 --> 01:08:25,049
you've got to figure out what is IXI that the other guy was thinking about or the investigator was thinking about.

486
01:08:25,050 --> 01:08:36,440
Right. So here is quite generic. And we will use this kind of framework to formulate the know that there is no group or timing direction.

487
01:08:37,160 --> 01:08:41,990
Now, let's look at this particular example here. So.

488
01:08:42,930 --> 01:08:49,030
We have two groups. And say three occasions.

489
01:08:49,030 --> 01:08:53,830
So three is I'm just I just made up this number three. And how many Paramus would you need?

490
01:08:54,610 --> 01:09:04,320
Well, if you use the previous notation, clearly you would need me one to me to to like new three t right.

491
01:09:04,360 --> 01:09:08,700
And you repeat this for the control group. See.

492
01:09:08,700 --> 01:09:17,490
Sorry. Oh. And venturing too close to the boundary here.

493
01:09:18,300 --> 01:09:23,050
So six unknowns. But they are not problem based on beta.

494
01:09:23,070 --> 01:09:30,540
So our task is trying to write things down using six betas to represent these things.

495
01:09:31,230 --> 01:09:36,220
So here's what we do. First we need to decide the design matrix.

496
01:09:36,430 --> 01:09:41,380
My claim is that the design matrix has six columns with columns corresponding to.

497
01:09:45,170 --> 01:09:52,250
Group equals one. Times I occasion equals one.

498
01:09:52,850 --> 01:09:57,440
So. I grew up.

499
01:09:59,240 --> 01:10:03,710
Actually, I regret writing this all down here, but hopefully this helps you to.

500
01:10:09,820 --> 01:10:16,130
I. Group equals one.

501
01:10:16,760 --> 01:10:19,760
J equals three. Okay.

502
01:10:20,840 --> 01:10:30,800
So for the first three columns are this and for the next three columns they are group equals two and j goes one.

503
01:10:31,100 --> 01:10:38,240
The first occasion and group equals to j goes to and I.

504
01:10:38,270 --> 01:10:44,100
Group equals to shake all three. Actually, I should write.

505
01:10:45,630 --> 01:10:58,020
More precisely. G equals one.

506
01:10:58,650 --> 01:11:02,310
G equals two. G equals two.

507
01:11:03,930 --> 01:11:07,530
So GI is indicating the group membership now.

508
01:11:07,830 --> 01:11:11,700
So this is column 1 to 3, four, five, six.

509
01:11:22,730 --> 01:11:26,080
Okay for people in the first group, you know,

510
01:11:26,300 --> 01:11:30,830
the final three columns are always going to be zero because the indicator function are always going to zero.

511
01:11:31,370 --> 01:11:37,069
And for data points that appeared at the first occasion.

512
01:11:37,070 --> 01:11:41,810
So it's the first row, right? First occasion. This is the.

513
01:11:42,980 --> 01:11:47,629
Jay goes to this is Jay goes three, right? So if you cannot recall this,

514
01:11:47,630 --> 01:11:57,770
my only response to that is just remember that the design matrix for every individual has no rows and corresponding to any occasions.

515
01:11:58,100 --> 01:12:04,820
In this case, everybody had three occasions. So row, row, one for the first occasion and so on and so forth.

516
01:12:05,690 --> 01:12:08,920
So in the first first row, we're just going to have one here, right?

517
01:12:08,930 --> 01:12:12,410
Because that indicates the first occasion and everything else will be zero.

518
01:12:12,890 --> 01:12:20,720
Knowledge equals to knowledge equals three. So if you proceed to the second row, only the second column indicating the second occasion will be one.

519
01:12:21,800 --> 01:12:32,630
And similarly, if you plug in guy, if you plug in this whole thing for a person in the second group, you'll recover the second second design matrix.

520
01:12:37,320 --> 01:12:42,310
So. What was my claim? My claim is that you can use data to represent those views.

521
01:12:42,580 --> 01:12:48,250
So here is what we have. If you multiply everything by beta one to beta six.

522
01:12:52,150 --> 01:12:58,850
Right. And you can immediately get all those. Means right.

523
01:12:58,850 --> 01:13:03,049
Because you multiply exi by beta. So this is what we do. Exile by beta.

524
01:13:03,050 --> 01:13:06,470
We do this for group 1/1.

525
01:13:09,010 --> 01:13:15,370
So for four for a person in group one, then the final three business does not have anything, right?

526
01:13:15,370 --> 01:13:17,980
So it's just a beta one, beta two, beta three.

527
01:13:18,250 --> 01:13:25,540
And similarly, if you multiply the x CI for the second group, you only have the beta four, beta five, beta six.

528
01:13:26,350 --> 01:13:35,560
So essentially we have used beta one, the beta three representing the means for people in the first group across all three occasions.

529
01:13:35,800 --> 01:13:45,640
And similarly for beta form beta five and beta six. And to calculate the difference, essentially you need to figure out that they correspond to.

530
01:13:47,440 --> 01:13:50,530
PETER Four minus. Peter one. Peter two minus. Peter five.

531
01:13:50,860 --> 01:13:56,410
Peter three minus. Peter six. So these are the Delta one, Delta two and Delta three.

532
01:13:57,370 --> 01:14:13,979
Nothing special here. So if we're going to test this now, we're going to need to use some statistical procedures before we do so.

533
01:14:13,980 --> 01:14:18,570
We often need to rewrite this now in terms of a compact form.

534
01:14:20,660 --> 01:14:38,540
Which is going to be written this way. I'm just going to give you like 30 seconds to verify that by setting this to zero.

535
01:14:38,930 --> 01:14:42,470
You recover this now. Yeah.

536
01:14:43,510 --> 01:15:46,159
Just try. So essentially you got this.

537
01:15:46,160 --> 01:15:52,010
And then the first row you just remove these terms with zero, the multiplier.

538
01:15:52,400 --> 01:15:57,140
So in the first you get into one minus beta two equals beta for minus beta five.

539
01:15:57,930 --> 01:16:08,060
All right. In the second one, you get beta one minus beta three equals beta four, minus beta six.

540
01:16:08,750 --> 01:16:16,069
So if you in the terms, you should see that they're actually same thing. Here.

541
01:16:16,070 --> 01:16:22,430
I want to say that for this null hypothesis, we have two equal sides to connect them.

542
01:16:23,030 --> 01:16:28,280
So basically two constraints. Okay. L has rent to two rents.

543
01:16:28,580 --> 01:16:34,940
So this correspondence is important. You are using L to indicate the set of equalities you want to test.

544
01:16:44,050 --> 01:16:49,000
So I assume that in 650 you have learned the technique of testing general hypotheses.

545
01:16:49,300 --> 01:16:57,070
So this falls into similar lines. The only difference is that we just now need to write down the hypotheses using the

546
01:16:57,070 --> 01:17:01,240
general formulation with possible correlated data so nothing else is too different.

547
01:17:04,310 --> 01:17:09,140
So we were operating based on the assumption that everybody had the same number of measurements.

548
01:17:09,290 --> 01:17:13,790
There were no missing data. So what if one person has missing data?

549
01:17:15,380 --> 01:17:24,200
So this is one example, right? We were saying originally everybody contributed three data points, but what if the third one is is missing?

550
01:17:24,830 --> 01:17:33,110
So you can simply just remove that row from from that person's design matrix.

551
01:17:33,380 --> 01:17:39,650
Clearly, some of you who have learned missing data will tell me, hey, Jim, because that's not legitimate in many different situations.

552
01:17:39,680 --> 01:17:44,180
True. But if you're willing to assume it's completely random, then that's the way to do it.

553
01:17:44,870 --> 01:17:50,810
So here I will not dove into the missing data issue, but rather say that if you want to forcefully.

554
01:17:51,810 --> 01:17:55,320
Produce an analysis of response profiles.

555
01:17:55,770 --> 01:17:59,250
You can operate by deleting that row for that person.

556
01:18:18,020 --> 01:18:23,180
So we have parameterized the means or the differences in one way or the.

557
01:18:24,400 --> 01:18:27,920
There's one thing we don't like, which is this now looks really ugly.

558
01:18:27,940 --> 01:18:30,640
You know, it's better when minus waiting for something like this.

559
01:18:30,940 --> 01:18:38,620
Can we just use very clean hypotheses like beta beta five equals beta six equals zero or something like that.

560
01:18:39,070 --> 01:18:45,310
So it turns out that you can do this, but you need to use another system of so another system coding.

561
01:18:45,910 --> 01:18:52,480
So this is what we do. I'm going to write down all the columns in the design matrix.

562
01:18:54,670 --> 01:18:56,680
So column one will be intercept.

563
01:19:00,530 --> 01:19:10,609
Column two will be an indicator of a person in addition to column three will be a person a data point at the occasion.

564
01:19:10,610 --> 01:19:14,810
Three. The third one will be I the group indicator.

565
01:19:15,260 --> 01:19:24,950
The group membership is one. The fifth column will be at the second occasion and the group indicator is one.

566
01:19:24,950 --> 01:19:28,400
Group membership is one. So I run out of space again.

567
01:19:28,790 --> 01:19:34,730
I need to be better at this. So g equals three and g i equals one.

568
01:19:35,090 --> 01:19:44,310
Right? So this correspond to this one. Just like this.

569
01:19:44,790 --> 01:19:52,810
Oh, did I miss anything? Yeah.

570
01:19:52,820 --> 01:19:56,630
I missed something here. Okay.

571
01:19:56,790 --> 01:20:06,279
That's fine. So let's look at this.

572
01:20:06,280 --> 01:20:13,450
Right? So for the first group, The Intercept just means regardless of the occasions in the three rows, you just put one there.

573
01:20:13,450 --> 01:20:21,580
Right? So that's what we did here. Second column is only going to be one for the second occasion, which is in the second row, hence the one here.

574
01:20:21,820 --> 01:20:24,880
It is not the second occasion, either first or third row.

575
01:20:25,120 --> 01:20:31,929
It'll be zero for the final one. Same thing. Right. So you got these three columns cover for the fourth column.

576
01:20:31,930 --> 01:20:35,890
Yes. Indicating group membership being one. Right. So these are all ones.

577
01:20:37,600 --> 01:20:44,020
Then you're going to talk about Fifth Column, which is for the second occasion for people in the group,

578
01:20:44,020 --> 01:20:52,600
1/2 occasion because in the second row and this person indeed was in the first group.

579
01:20:52,720 --> 01:21:01,000
Right. So this value is one. However, for the zero, here they are zero because they appeared in the first or third occasion.

580
01:21:01,000 --> 01:21:04,480
So this criteria, which equals two, was not satisfied.

581
01:21:05,260 --> 01:21:12,280
Similarly, you can try to fill out these numbers if you change to another group.

582
01:21:12,280 --> 01:21:17,950
So the second group, so the first three columns because they do not depend on G at all, right.

583
01:21:18,280 --> 01:21:20,349
The group membership at all. So they should remain the same.

584
01:21:20,350 --> 01:21:29,049
Hence you just copy them down while for the final two three columns they all need what g equals one.

585
01:21:29,050 --> 01:21:33,550
I belong to the first group. Right, because these people are in the second group, so they are all zeros.

586
01:21:34,480 --> 01:21:39,520
The bottom line is that we again can fully represent the mean structures.

587
01:21:40,390 --> 01:21:44,450
So this one is called reference. So coding the meaning of this will be clear upon.

588
01:21:44,450 --> 01:21:46,870
We're interpreting the coefficients.

589
01:21:47,290 --> 01:21:56,410
So what we do again is trying to multiply these old things by beta one, two, three, four, beta, five and better six.

590
01:21:56,950 --> 01:22:08,090
Similarly for this one to. And we are going to ask, okay, what are the values in terms of these new betas?

591
01:22:08,840 --> 01:22:12,420
It turns out that we can derive this. So this should be really simple to do.

592
01:22:12,440 --> 01:22:16,010
All you need to do is just putting excited times beta.

593
01:22:16,040 --> 01:22:21,500
This is for the second group and you can do this for the first group.

594
01:22:25,760 --> 01:22:34,030
Now if we are going to contrast the men at the same occasion one but for the two different groups you can see that the difference is made of four,

595
01:22:34,070 --> 01:22:39,440
right? So let's draw a figure. If I use horizontal axis as time.

596
01:22:44,500 --> 01:22:48,850
I'm going to draw, say, a group of number twos trajectories.

597
01:22:48,850 --> 01:22:52,900
So these are going to be this one, this one and this one.

598
01:22:53,530 --> 01:22:58,570
So the height here will be what? So this one indicates group.

599
01:22:59,770 --> 01:23:03,190
Two. And let's use a cross to indicate group one.

600
01:23:05,170 --> 01:23:12,730
So this one is what, Peter? One. And this one is what is a better one plus better to.

601
01:23:16,110 --> 01:23:19,310
This one is better one plus better three. Right.

602
01:23:20,850 --> 01:23:29,370
So if you connect them by segments, this is the main response profile of a group to let's start let's then talk about the.

603
01:23:32,330 --> 01:23:35,720
Main profile, main response profile for the group at number one.

604
01:23:36,140 --> 01:23:42,110
If you look at this relative to Beta Wong here, it's going to add beta for for simplicity.

605
01:23:42,110 --> 01:23:47,990
Let's assume these are positive. Right? So just say it shoots up to here.

606
01:23:48,810 --> 01:23:52,090
All right. So this segment is of Lance Peter for.

607
01:23:52,300 --> 01:23:56,210
Right. So it's representing this value.

608
01:23:56,660 --> 01:24:04,890
How about here? So at the second occasion.

609
01:24:05,950 --> 01:24:08,790
The difference now is going to be better for plus better it, right?

610
01:24:08,810 --> 01:24:15,370
Because if you take this one and this one take the difference, the difference will be better four at plus, better five.

611
01:24:18,180 --> 01:24:25,320
Similarly, if you do the difference between the final row here, you will get better four plus better six.

612
01:24:25,890 --> 01:24:37,660
Now you connect them by segments. You can see that the differences are better for, better for plus better, five and better five plus better six.

613
01:24:37,660 --> 01:24:42,280
Clearly, to make the two curves parallel. You just got to be having better.

614
01:24:42,280 --> 01:24:53,569
Five equals better. So six was been a zero. So this is the reason why we often use this population cleaner.

615
01:24:53,570 --> 01:24:57,830
Simpler. And you write less. Fewer Greek letters.

616
01:25:00,200 --> 01:25:07,280
So we will stop here and at the next lecture we will be content to finish out the rest of these slides, which I believe is pretty simple.

617
01:25:07,760 --> 01:25:12,890
And then we will be moving on to a handout. 0405 BNC.

618
01:25:13,310 --> 01:25:45,200
Thanks, everybody. Have a good day. Let me.

