1
00:00:08,700 --> 00:00:12,600
Okay. And then. Yeah, we have.

2
00:00:12,960 --> 00:00:23,330
This is our demo site. And then let's go over the course syllabus together because the content of the syllabus itself.

3
00:00:23,350 --> 00:00:29,950
But here the insert, the very first slide, the introduction.

4
00:00:29,950 --> 00:00:34,550
This is the introduction. The title is actually action.

5
00:00:35,530 --> 00:00:40,889
And so this slide, we included some very important information from the syllabus.

6
00:00:40,890 --> 00:00:46,480
And of course, you guys can later after the lecture, probably you may want to go over the syllabus in itself.

7
00:00:46,630 --> 00:00:50,620
I need help. So because lots of useful information, important information.

8
00:00:51,370 --> 00:00:58,200
But here on this slides, we actually included some some important information here.

9
00:00:58,210 --> 00:01:11,350
Let's go over them together. So first of all, while we have two sessions, right, so the same as six, you have two sessions.

10
00:01:11,950 --> 00:01:18,969
So the session one is being taught by Professor Min and of us while I'm teaching

11
00:01:18,970 --> 00:01:27,190
session two here and these are our emails and then we have two jazz eyes.

12
00:01:27,520 --> 00:01:36,070
So how is administrative help? So these are their emails and so their rules are going to be very similar to a 650.

13
00:01:36,320 --> 00:01:41,590
So oh, but by the way, so this course does not have a course pro that does not have final product.

14
00:01:41,680 --> 00:01:46,899
And so I don't know whether that's going to make the course more enjoyable or less,

15
00:01:46,900 --> 00:01:54,750
but this is just done just because I know there is no final word app.

16
00:01:54,760 --> 00:01:58,840
But but there will be a final exam. So the major final exam.

17
00:02:01,750 --> 00:02:04,900
Oh, by the way. And then a thank you for this session.

18
00:02:05,020 --> 00:02:14,709
So I mean, thank you for your trust in me over there to thank it is now you are taking that statement session now.

19
00:02:14,710 --> 00:02:21,310
I think this semester we have rather even a dispute in numbers between this 2000 less semester.

20
00:02:21,310 --> 00:02:28,240
I think the numbers were not not equally distributed, but last time I checked, 66 is so true.

21
00:02:28,240 --> 00:02:32,340
It seems that the two sessions had dropped with the same number of you.

22
00:02:32,350 --> 00:02:41,229
Almost okay on. And also when you try to well email is the best way to duration.

23
00:02:41,230 --> 00:02:51,010
So we read in the emails. While it is better to always include biostatistics or two in the subject to avoid you know that your email being buried.

24
00:02:52,170 --> 00:03:00,300
My mailbox can also double check the email address so I do not use DI.

25
00:03:00,670 --> 00:03:05,110
Matt Madison I'm not in school while this is email address for our school of medicine.

26
00:03:05,110 --> 00:03:15,909
So so do not use this account name or some people send nostrils went to the wrong password

27
00:03:15,910 --> 00:03:23,740
to this this one which I I checked the the my my you may start you do account regularly.

28
00:03:25,710 --> 00:03:29,260
Okay. And the meeting time.

29
00:03:29,310 --> 00:03:32,980
Well, it's. The lectures are Tuesday and Thursdays.

30
00:03:33,000 --> 00:03:35,640
One, two, three. Right. This is a lot about her.

31
00:03:35,760 --> 00:03:48,600
650 This video was early like 8 a.m. in the morning, which was quite painful, especially when the date when the date had become shorter.

32
00:03:48,720 --> 00:03:52,020
So. So this this is a lot better.

33
00:03:52,260 --> 00:03:55,320
One, two, three. And office hours.

34
00:03:55,650 --> 00:04:02,610
We haven't decided the location yet, but we will reserve some rooms for office hours.

35
00:04:02,940 --> 00:04:07,290
Last lost to us. I don't remember the exact reason why we decided it.

36
00:04:07,290 --> 00:04:11,140
I mean, Dr. Banerjee and I, we.

37
00:04:11,340 --> 00:04:17,990
I don't remember the exact reason why we decided to hold office hours in our own offices, but, you know, there are long lines, right?

38
00:04:18,000 --> 00:04:23,250
So oftentimes there were long lines, but this time we will book I loop.

39
00:04:23,280 --> 00:04:26,700
So I already sent out an email to the room.

40
00:04:26,850 --> 00:04:34,920
I haven't heard back. So it's probably because of the beginning of the semester, lots of requests, but it was I have the information I'll advocate.

41
00:04:37,710 --> 00:04:42,330
So these are office hours and instructor always arranges outs hours.

42
00:04:42,690 --> 00:04:52,440
So we try to make sure that we have all day hours every day throughout the week and similar to and please feel free to go through any office hours.

43
00:04:52,440 --> 00:05:01,259
So you if you're enjoying this session, you can go to the Dr. Johns office hours and vice versa of the student from the other

44
00:05:01,260 --> 00:05:06,000
session can have to come to my office our and also we share our four gases as well.

45
00:05:07,980 --> 00:05:17,040
Right and then for textbook, we do have a required textbook.

46
00:05:17,670 --> 00:05:22,050
So is studies or inference the second edition by Casella and Berger.

47
00:05:23,430 --> 00:05:34,200
So so I guess if you don't have it, you probably just you survive the course.

48
00:05:34,200 --> 00:05:41,160
But but I think it's it's actually good if you have a copy like either, you know,

49
00:05:41,190 --> 00:05:48,300
a hard copy or 20 called I don't know I got think there's 20 called available but

50
00:05:48,410 --> 00:05:55,920
but anyway so I think it's it's a good idea to have a copy of the textbook because,

51
00:05:56,400 --> 00:06:00,330
you know, of course we won't have time to discuss every single detail.

52
00:06:00,330 --> 00:06:04,320
So lots of things we have to limit up to speed.

53
00:06:05,790 --> 00:06:12,839
Of course, we are going to try to explain, you know, every logic and the logic behind the concepts and the results.

54
00:06:12,840 --> 00:06:20,880
But still, it's sometimes it's better to read a detailed example of a detailed discussion in the textbook.

55
00:06:22,830 --> 00:06:25,950
So the topics covered there are four major topics covered.

56
00:06:27,510 --> 00:06:33,930
So first data reduction and then point estimation, then others testing and then interval estimation.

57
00:06:33,930 --> 00:06:37,770
So this all these topics are about inference.

58
00:06:38,070 --> 00:06:43,110
So these are based on things.

59
00:06:43,170 --> 00:06:47,430
So one oh by the way, so I think you guys use the same textbook for sex a lot, right?

60
00:06:47,440 --> 00:06:54,660
So okay, that's great. Now I saw that. So yeah, so then many a copy of that book.

61
00:06:55,860 --> 00:07:03,239
And so for cameras we will use camera side a lot I just showed you and the slides will be

62
00:07:03,240 --> 00:07:11,910
posted on canvas before lectures and the lectures will be recorded and uploaded to canvas.

63
00:07:12,840 --> 00:07:23,340
But we have not a guarantee that, you know, every a lecture is something because sometimes there were some technical problems if I turn this on,

64
00:07:23,340 --> 00:07:26,820
but sometimes I, I can Kazmierczak attack the recordings.

65
00:07:27,380 --> 00:07:35,160
So if we run into some technical difficulties, then then the cooler lecture might not be available or part of it may not be available.

66
00:07:35,160 --> 00:07:45,690
So, so, but we will try to record all the actors and cameras and homework assignments, so we will have weekly assignments.

67
00:07:46,050 --> 00:07:53,520
So I think the assignments will be posted on every Thursday and the Thursday following Thursday.

68
00:07:54,480 --> 00:08:00,390
And then while we have. Yeah.

69
00:08:00,410 --> 00:08:14,100
We have two exams, right? Yeah, that's. And we will post larger slides, we'll post homework solutions.

70
00:08:14,180 --> 00:08:19,290
Also, we will make announcements on canvas, which I think you will receive.

71
00:08:20,420 --> 00:08:23,850
Eric There are some important things.

72
00:08:23,880 --> 00:08:34,770
So while today is January, the 5th of January is Thursday and the midterm is February 23rd, Thursday in class, I have examined class.

73
00:08:35,040 --> 00:08:38,450
Oh, by the way, I think it's actually 1 to 250.

74
00:08:38,490 --> 00:08:46,110
Okay, so it's up. So that's the Thursday right before spring break.

75
00:08:46,650 --> 00:08:50,040
So so then you have.

76
00:08:50,790 --> 00:08:54,240
Yeah. So that you can have a more relaxing spring break.

77
00:08:54,390 --> 00:08:59,080
Not worry too much about taking an exam after coming back to it.

78
00:08:59,520 --> 00:09:04,299
And there there are a few days that we do not have lectures.

79
00:09:04,300 --> 00:09:06,030
So one is winter break.

80
00:09:06,390 --> 00:09:19,540
So it's a one week winter break for this particular Tuesday and Thursday we won't have lectures and also lectures on March 21st will be canceled in

81
00:09:19,710 --> 00:09:30,300
our I'm not sure how many of you guys are noticing or are going to as many of our faculty members and students as including our business major.

82
00:09:30,980 --> 00:09:34,650
About bout conference in Boston has sticks.

83
00:09:36,730 --> 00:09:43,900
So I don't remember exactly what of the stands for hold. Maybe we can have to take a very quick look since we we're connected to the Internet.

84
00:09:50,070 --> 00:09:54,000
Yeah. It's, you know, our spur meeting. So, you know our stands for.

85
00:09:57,780 --> 00:10:01,920
Yes. Eastern North American Region International about.

86
00:10:02,890 --> 00:10:07,870
Magic Society. Okay. So. So we have spring meeting every every year.

87
00:10:08,080 --> 00:10:11,770
This is a really big meeting for for all the founding members.

88
00:10:11,770 --> 00:10:17,530
And it still does have statistics. So many of us will go attend the meeting.

89
00:10:18,550 --> 00:10:23,560
So for then for this particular lecture on 21st, we will cancel that.

90
00:10:24,700 --> 00:10:28,930
And then the last lecture is on April 18th. That's a Tuesday.

91
00:10:29,350 --> 00:10:32,470
So the final exam is on Wednesday, April 26th.

92
00:10:33,670 --> 00:10:37,200
This is a scheduled by the university. So of.

93
00:10:39,190 --> 00:10:42,610
We will find a room. So the room has not been scheduled yet.

94
00:10:42,700 --> 00:10:49,580
So we will. Once we are all here.

95
00:10:51,670 --> 00:11:00,069
Okay. And for this course, I believe all of you, not most of you, if not all, have taken six.

96
00:11:00,070 --> 00:11:05,770
So what, six or what? So that's a four requisites or some equivalent?

97
00:11:05,920 --> 00:11:16,989
Some other, including, of course. And if you haven't taken six or one, just let's have a have a chat after lecturing and you can come to see me.

98
00:11:16,990 --> 00:11:21,940
And then we can we can talk about whether this is the right course for you because we,

99
00:11:22,270 --> 00:11:30,490
we do need knowledge from six or what in probability theory and also we we need a some basic calculus and Ms.

100
00:11:30,670 --> 00:11:42,970
Salsburg. And if we're grading so the homework assignments will account for 20%.

101
00:11:44,020 --> 00:11:48,700
And homework is assigned on every Thursday, starting from next week.

102
00:11:49,000 --> 00:11:58,180
And the due date is the next Thursday. The college and the submission is through campus, so we will create assignments on that,

103
00:11:59,960 --> 00:12:07,330
submitting our homework in the midterm and a final exam for 2% each.

104
00:12:07,720 --> 00:12:10,930
So. So it's quite happy. The two examples are quite heavy.

105
00:12:10,930 --> 00:12:20,260
So this is similar in Fermat to a single one and we will provide more details later as we progress.

106
00:12:25,520 --> 00:12:28,720
And there will be no makeup midterm.

107
00:12:28,730 --> 00:12:37,709
So if you have to miss the midterm, then the weight of the midterm will be shifted over to the final exam.

108
00:12:37,710 --> 00:12:46,120
But this is definitely not recommended because that's going to make a final exam account for 80% of your course grade results.

109
00:12:46,400 --> 00:12:50,870
That's just too much, unless it's absolutely necessary.

110
00:12:50,870 --> 00:13:01,640
Synthesis definitely not recommended. And midterm exits without legitimate reason will even count it as zero.

111
00:13:04,630 --> 00:13:09,940
So yeah. So Instructor I mean this course is very similar to six, six or one.

112
00:13:10,180 --> 00:13:18,100
This is a continuation of six a lot. So it's very similar and also one to emphasize academic integrity.

113
00:13:18,130 --> 00:13:23,110
So all your homework exams must be on your own.

114
00:13:23,140 --> 00:13:26,450
We definitely encourage you guys to discuss those problems.

115
00:13:26,450 --> 00:13:32,600
So I think there will also be study group study groups, right?

116
00:13:32,620 --> 00:13:37,180
So for similar to six 5601, there will be study groups.

117
00:13:37,540 --> 00:13:42,969
So your you're strongly encouraged to attend those study groups and that you're

118
00:13:42,970 --> 00:13:48,190
strongly encouraged to discuss homework problems with your with your friends.

119
00:13:49,090 --> 00:13:56,080
So that's totally fine. Discussion is totally fine and is strongly encouraged by the way you submit your homework.

120
00:13:57,100 --> 00:14:04,030
Your submission needs to based on your own understanding of your own writing can whatever is worth doing is worth doing well.

121
00:14:08,740 --> 00:14:10,660
So okay.

122
00:14:10,660 --> 00:14:23,520
And you are not allowed to share any piece of your draft or final homework or classmates and any of your hand should be based on the understandings.

123
00:14:28,890 --> 00:14:33,629
I also want to emphasize that feedback is very important.

124
00:14:33,630 --> 00:14:41,520
So we definitely encourage you guys to ask questions whenever you feel that.

125
00:14:42,150 --> 00:14:46,260
If you have a question, it's just just, you know, stop me or raise your hand.

126
00:14:46,410 --> 00:14:49,980
I would be very, very happy to answer any questions you have.

127
00:14:51,000 --> 00:14:54,600
And this actually, sometimes this helps the whole class.

128
00:14:55,320 --> 00:14:58,770
Oftentimes it helps the whole class to understand the feelings better.

129
00:15:00,390 --> 00:15:08,879
So and also having discussions with fellow students, drawing office hours, and we strongly encourage guys to do that.

130
00:15:08,880 --> 00:15:15,660
And there will be definitely no negative consequences having interaction with the students.

131
00:15:17,220 --> 00:15:23,740
He's trying to do office hours. Okay.

132
00:15:23,740 --> 00:15:28,180
So that's actually what the syllabus is about.

133
00:15:28,180 --> 00:15:37,810
Any questions before we walk? Okay.

134
00:15:41,370 --> 00:15:46,920
Now. Now, let's take a look at the rest of the few slides here.

135
00:15:47,070 --> 00:15:49,230
These provide us on the big picture.

136
00:15:49,410 --> 00:15:57,690
What sort of inferences about what this course is about before we look at concrete concepts, concrete definitions.

137
00:15:58,770 --> 00:16:05,850
So first, what is statistical inference? So inference is a really big word.

138
00:16:05,970 --> 00:16:08,430
It covers lots of things in statistics.

139
00:16:09,060 --> 00:16:17,310
It's more inference is actually the foundation of both the study of causes, and it's definitely the thing that we will focus on.

140
00:16:17,340 --> 00:16:26,040
We'll be focusing on this person and by the end of the course after seeing all these different techniques.

141
00:16:26,520 --> 00:16:36,100
So we should be able to use the techniques we learned to construct something, something cool.

142
00:16:36,120 --> 00:16:43,320
Like, for example, the GM I you guys are probably taking 1651 as well, right?

143
00:16:43,510 --> 00:16:48,510
And as most of you are taking that. So that's of course, on generalize in your models.

144
00:16:49,950 --> 00:16:54,350
So and GM of linear regression as well.

145
00:16:54,360 --> 00:16:56,730
So linear linear mode as well.

146
00:16:56,740 --> 00:17:09,300
So these are different tools that we can use and to make an inference and here the course inference here this is more general coverage of inference.

147
00:17:09,420 --> 00:17:12,860
It doesn't care about it doesn't focus on a particular model.

148
00:17:13,030 --> 00:17:17,189
Again, so 54 focus on linear regression model sensitivity.

149
00:17:17,190 --> 00:17:24,840
One, Focus on generalize. These are two special model modeling, some special type of data.

150
00:17:25,470 --> 00:17:30,900
But in this course for inference, we are going to see some general result from general theory.

151
00:17:33,840 --> 00:17:39,590
These results in theory do not only apply to just one particular model, but they generally apply.

152
00:17:39,670 --> 00:17:41,880
So that's the purpose of this course.

153
00:17:43,860 --> 00:17:51,600
And inference is actually a process of the process of drawing conclusions on the online distribution of the data.

154
00:17:52,200 --> 00:17:59,760
So if you think about I mean, we kind of measure if you took a society with me, so we kind of measured this in suitability.

155
00:18:00,390 --> 00:18:09,600
So instead of sticks, usually we do not know the, the, the disputation that generates the data or we do not know the public.

156
00:18:09,600 --> 00:18:19,829
For example, let's so we are interested in estimating how people's income is associated with their work depends on their education level,

157
00:18:19,830 --> 00:18:25,710
their gender, their race, their birth age, for example, all these variables.

158
00:18:26,580 --> 00:18:31,500
Now, of course, we are not able to collect data from the whole population.

159
00:18:31,860 --> 00:18:41,010
So let's say we are interested in studying this association for outcome for the US population.

160
00:18:41,580 --> 00:18:45,209
However, we are not able to clear data from the whole US population.

161
00:18:45,210 --> 00:18:50,430
Right. What we are able to do is we are only able to cloud data from a very small sample.

162
00:18:50,430 --> 00:18:55,770
As a sample size of a thousand, we are able to collect data from a thousand individuals.

163
00:18:56,220 --> 00:19:05,670
That inference is the process of making a conclusion about the population using the small sample size, most small set of data on cloud.

164
00:19:06,420 --> 00:19:08,340
This whole process is called inference.

165
00:19:09,900 --> 00:19:18,780
So while you can interpret this or think of this as sort of you have data, you have data based on the data and you have a process and you make infers.

166
00:19:19,110 --> 00:19:24,540
Then you in the end, you can make some scientific conclusion about the things that are interesting.

167
00:19:24,540 --> 00:19:28,290
For example, how income depends on different variables.

168
00:19:30,320 --> 00:19:46,440
So. Okay. And yeah, this is sort of repetitive, but you've alluded to the process of drawing conclusions or making statements about publication,

169
00:19:46,920 --> 00:19:56,400
using a random sample from that population. So in this course, we are going to focus on the so-called random sample, the random sample,

170
00:19:58,320 --> 00:20:10,160
and we will use these small allure of case letters to denote the concrete value of a data point for each small X represent.

171
00:20:10,170 --> 00:20:17,969
You can think of this as a number, but we just use larger to denote, for example, X, Y and z point of 5xx and could be 4 to 7.

172
00:20:17,970 --> 00:20:29,190
Right? So these are concrete numbers, concrete data points where we call them realizations and we use capital letters to denote run the variables.

173
00:20:29,610 --> 00:20:37,920
So these are variables and small X is a realization of that capital x and we assume we have a random sample.

174
00:20:38,280 --> 00:20:43,559
I run the sample means that the sample is representative of the, of the population.

175
00:20:43,560 --> 00:20:46,650
Right. So it's up. There's no bias in the sample.

176
00:20:48,000 --> 00:20:54,720
Okay. So this, this is what inferences about it actually.

177
00:20:54,990 --> 00:21:05,790
Oh, I think we could go to that. Remember that we had, we have this picture here so we can actually talk about inference using this as well.

178
00:21:05,790 --> 00:21:11,010
So this is our drawing here.

179
00:21:12,000 --> 00:21:15,060
So this is our population and this is the whole population.

180
00:21:15,450 --> 00:21:20,520
And we have a huge number of subjects which are not to collect data on every single individual.

181
00:21:20,790 --> 00:21:24,980
So what we do is we take a random sample, we select a sample lesion.

182
00:21:25,380 --> 00:21:28,650
So it's a much smaller side of individuals.

183
00:21:29,400 --> 00:21:38,150
And now we're able to collect data from this very small number of individuals, and then we will calculate the so-called strata.

184
00:21:38,160 --> 00:21:40,140
Later, we're going to talk about what a statistic is.

185
00:21:40,440 --> 00:21:46,620
So we're going to talk about some summaries, some numerical summaries based on the data from this small sample.

186
00:21:47,130 --> 00:21:59,100
And then we are going to use this small sample summaries calculate a bit small sample kind of to approximate or to estimate the unknown population.

187
00:21:59,370 --> 00:22:05,880
Let's say we are interested in estimating the population mean, let's say the average age or mean age of the population.

188
00:22:06,600 --> 00:22:15,690
So we kind of small sample, we calculate the average age of this small sample and we use that as an estimate of the population,

189
00:22:16,320 --> 00:22:21,240
average age, mean age, and this process is called inference.

190
00:22:21,570 --> 00:22:25,260
So we are drawing conclusions about this population.

191
00:22:35,680 --> 00:22:40,030
And then let's take a look at some of the basic applications.

192
00:22:40,960 --> 00:22:47,860
So we will use as we mentioned, we will use capital letter X to denote run the variables and we assume they follow some

193
00:22:47,860 --> 00:22:54,370
distribution and their distribution is denoted by this F and sometimes to make things clear,

194
00:22:54,370 --> 00:23:02,410
we have this subscript to denote it is a distribution for variable x and theta is the parameter, right.

195
00:23:02,440 --> 00:23:13,120
So to make things concrete. So for example, this F X theta could be your sigma square where theta is important.

196
00:23:13,120 --> 00:23:21,850
New Sigma Springs, if you have normal distribution that you have normalized, that you have theta has 2 to 2 components.

197
00:23:22,780 --> 00:23:28,960
But anyway, this is not the PDF where PPMO depends on whether X is continuously discrete.

198
00:23:29,680 --> 00:23:34,660
Now small x. This is the data. There are realizations of this run of variables.

199
00:23:34,990 --> 00:23:40,240
So when we see small matters, these are concrete numerical values, although we still use letters.

200
00:23:40,240 --> 00:23:47,260
But but these are concrete numerical values. And of this voltage x.

201
00:23:47,680 --> 00:23:52,840
This is a random sample. This is a factor. This is a random sample of this population.

202
00:23:53,830 --> 00:23:57,320
And then it's kept while its distribution is given by.

203
00:23:57,340 --> 00:24:04,170
Here we use this capital. Sorry, the boat it acts to denote this is not pdaf or PMA for a bag.

204
00:24:04,450 --> 00:24:06,760
Not a single, not a schemas.

205
00:24:08,800 --> 00:24:17,350
Now, if we have a random sample, which means that x ies if they are I.D., they are independent and identically distributed.

206
00:24:17,650 --> 00:24:20,920
That's what I.D. stands for. Independent identity disputed.

207
00:24:21,640 --> 00:24:33,400
So assuming they are I.D., then the joint PDF is given as the product of the individual PDF based on each individual right.

208
00:24:33,430 --> 00:24:36,820
So you guys must have seen this in six or one.

209
00:24:38,950 --> 00:24:44,830
And again here is that in practice. And over here we use only only to know the parameter space.

210
00:24:45,070 --> 00:24:49,360
So it's a space where theta lives in right where they lost.

211
00:24:52,420 --> 00:24:52,720
Okay.

212
00:24:55,030 --> 00:25:07,630
And about six while this last house tried to house a the Saudi one not so different but the difference between 611 essential two so is six or one.

213
00:25:09,100 --> 00:25:15,760
We kind of assume the knowledge of theta in making probabilistic statements about this random variables.

214
00:25:18,550 --> 00:25:21,850
So for example, let's take a look at this.

215
00:25:22,440 --> 00:25:26,860
Like this is a very typical six or one question. And so for example,

216
00:25:26,860 --> 00:25:38,919
let's say we have a sample size in and these are capital this this random variables there are if we follow Bernoulli distribution well the success

217
00:25:38,920 --> 00:25:45,640
probability P zero and then a typical a very typical six so well question is

218
00:25:45,940 --> 00:25:50,430
okay what is the probability that the sum of this is less than or equal to?

219
00:25:50,440 --> 00:25:54,280
I'll give a number of the sum here as it stands for.

220
00:25:54,610 --> 00:26:01,540
So it's why is Bernoulli random variable so we can consider it as a success and a failure that we're feeling.

221
00:26:01,870 --> 00:26:05,769
So the sum of this is, as a number of successes result here,

222
00:26:05,770 --> 00:26:13,610
then a typical question would be what is the probability that the number of citizens is less than or equal to the number?

223
00:26:13,610 --> 00:26:18,140
Again, so of that is so a one well from six.

224
00:26:18,190 --> 00:26:25,269
Well, we know that in the sum of four moving around the variable, follow the binomial distributions and then once you know this follow binomial.

225
00:26:25,270 --> 00:26:33,000
Now of course you can calculate the probability that this is less than or equal to a given published number, which is because this greater.

226
00:26:33,010 --> 00:26:36,460
So it's some of the math zeros.

227
00:26:37,150 --> 00:26:40,330
So this is a typical six. So what question?

228
00:26:41,800 --> 00:26:45,980
So again, for six one question, we kind of assume that this P zero is right.

229
00:26:46,030 --> 00:26:53,410
So we know, for example, P zero is equal to one half so that we have a fair coin or p zero equal to one third,

230
00:26:54,130 --> 00:26:57,760
so that the probability of success is is one third.

231
00:26:58,630 --> 00:27:02,890
But anyway, so it's a it's a is a known quantity. So is so.

232
00:27:02,890 --> 00:27:08,890
So tool is different. Is this or tool we assume that this parameter theta, this is unknown.

233
00:27:09,700 --> 00:27:16,480
We don't know the parameter that it's just like a first of the population we population average, we don't know what the average is.

234
00:27:17,260 --> 00:27:21,910
So what we are trying to do is that we call our data and we try to estimate that average.

235
00:27:22,180 --> 00:27:30,700
We try to estimate this data where we try to say something about this and by collecting the data.

236
00:27:30,910 --> 00:27:34,400
So this is the typical thinking of sex or truth.

237
00:27:34,990 --> 00:27:41,830
So we are not bothered. We can take six, six, the linear regression as an example.

238
00:27:41,830 --> 00:27:48,640
So in linear regression you have the regression Y, a bunch of X and you have a regression coherence beta.

239
00:27:49,030 --> 00:27:58,720
The betas are the things that we do not know that are values we do not know, and we will to conquer data that use the data to estimate the values.

240
00:27:59,350 --> 00:28:05,200
So this is the typical six or two question. So.

241
00:28:08,530 --> 00:28:16,690
For example, we assumed that the data was generated by a PDF or PMF that belongs to this class.

242
00:28:17,320 --> 00:28:22,230
So this is actually the so called a model. So model this.

243
00:28:22,240 --> 00:28:29,150
The terminology model is based on one of the most widely used services that is restaurant as to who model.

244
00:28:29,180 --> 00:28:34,839
So what do we mean by model? A model is actually a family of description.

245
00:28:34,840 --> 00:28:43,360
We're a collection of descriptions. So for example, we consider all the normal distributions, then that's a model.

246
00:28:43,360 --> 00:28:47,350
So we are assuming a they of a formal distribution. That's your model.

247
00:28:48,400 --> 00:28:56,600
Or if you are seeing your data, for example, that is Fusion, the family of Explorer, the student that that's it's not so generous.

248
00:28:56,600 --> 00:29:02,799
Speaking a model is just a collection of PDFs of something different.

249
00:29:02,800 --> 00:29:05,820
That's what, of course the theta values can value.

250
00:29:05,840 --> 00:29:11,170
We do not know what value what to say about who generates the data.

251
00:29:11,320 --> 00:29:21,520
So this is our model. We assume that our data is generated by a particular member in this in this collection of this version.

252
00:29:21,850 --> 00:29:32,380
So for example, for example, let's say we are assuming that X followed Bernoulli with parameter theta and this is a model, right?

253
00:29:32,590 --> 00:29:38,230
So this is our model assumption. We assume X followed by Luna distribution, but we don't know what theta is.

254
00:29:38,800 --> 00:29:42,070
What we know is that theta must be a number between zero and one.

255
00:29:42,070 --> 00:29:45,420
It's a success probability, so it must be some number between zero one.

256
00:29:46,000 --> 00:29:50,490
So this is what this closing is our water.

257
00:29:50,500 --> 00:29:54,610
So we assume we follow Bernoulli where theta belongs to theta is between 0 to 1.

258
00:29:55,300 --> 00:30:03,129
Well, this is our practice base and then we will have to data because we don't know what this is.

259
00:30:03,130 --> 00:30:07,630
Probably it is, but we are fascinated. We like to know what it is.

260
00:30:08,080 --> 00:30:17,889
So what we do is we estimate a theta. We can estimate theta just like a linear regression.

261
00:30:17,890 --> 00:30:24,280
We estimate in the betas, inclusions, betas and that we can perform this test about.

262
00:30:25,600 --> 00:30:34,150
You can also again connect this to the testing for a B2C linear regression model to test whether the data is equal to zero or not.

263
00:30:34,840 --> 00:30:41,680
And also, we want to construct confidence intervals who are present and also we want to make a prediction through the data.

264
00:30:44,580 --> 00:30:59,280
So for example, let's say that let's say, well, we tried to estimate a theta estimate of the success probability that let's say we collect this data.

265
00:30:59,760 --> 00:31:03,930
So the typical question is, okay, after seeing, you know,

266
00:31:03,930 --> 00:31:09,590
2 hours and then followed by three tails after a while, let's say we toss the coin five times.

267
00:31:09,600 --> 00:31:15,329
We see this this particular sequence of outcome. Then the question is, what are the probability?

268
00:31:15,330 --> 00:31:22,350
That is what what, what? What is the probability of hat or what the probability of.

269
00:31:22,560 --> 00:31:30,000
Yeah, well, the probability of, of getting a hat. And so this is a typical question I've seen sort of all.

270
00:31:30,000 --> 00:31:38,100
QUESTION So would the data now we assume our model so our model says that you toss a coin and follows a panel of discussion.

271
00:31:38,100 --> 00:31:41,549
Whiskers has probably two theta. Now we have our data.

272
00:31:41,550 --> 00:31:46,860
We toss a coin five hats that with our data to estimate theta.

273
00:31:47,310 --> 00:31:51,420
So that's probably so that's more by point estimation,

274
00:31:52,800 --> 00:31:59,490
we mean that if we provide just a single value as ask me so ask me to theta to be four or five where

275
00:31:59,490 --> 00:32:08,819
we ask this is reporter for 22 points one isolation and also we can prefer what was asked about theta.

276
00:32:08,820 --> 00:32:15,180
So for example for a coin, if you think is a fair quality you could follow have hypothesis you are not but this is would it be

277
00:32:15,180 --> 00:32:20,970
about equal to one half and so those exact probabilities equal one half so that you have a fair coin.

278
00:32:21,720 --> 00:32:29,820
So then after all this data, you can test that hypothesis to see whether or not you can see with what have or not.

279
00:32:31,110 --> 00:32:34,350
Now, so you're able to construct a confidence intervals for theta.

280
00:32:34,560 --> 00:32:43,500
This is so called interval estimations. So the idea behind interval estimation is that now of course,

281
00:32:43,500 --> 00:32:48,600
Theta was excited probably to say that these are not kind of we climb data in order to estimate the theta,

282
00:32:49,260 --> 00:32:56,489
but if you only look at a point as major, if you only provide a point asking how you estimate the probability to be equal to,

283
00:32:56,490 --> 00:32:59,070
let's say based on review to include us.

284
00:33:00,090 --> 00:33:08,520
So based on this particular data now of over five files you get to pass result if you calculate theta, then that would be two over five.

285
00:33:08,910 --> 00:33:17,160
That's 40% or point four. But of course we do not believe theta is exactly equal 2.4 because if you throw the coin,

286
00:33:17,160 --> 00:33:22,500
you will get a different, different, different results, different outcomes.

287
00:33:23,070 --> 00:33:29,580
So it's important as well. We can definitely estimate a theta by a matter, by value, by single value,

288
00:33:29,790 --> 00:33:39,000
but we do not believe that there is just exactly a single that I so that this actually leads us to the so called interval estimate rather than,

289
00:33:39,380 --> 00:33:43,680
you know, estimating theta to be point four, we actually provide an interval for theta.

290
00:33:43,680 --> 00:33:54,389
So although I don't know what theta is, what the true theta is, but I know with a very high confidence that it must be between one or 2.6.

291
00:33:54,390 --> 00:33:59,520
LESSING Right. So you provide a interval rather than a single numerical value.

292
00:33:59,520 --> 00:34:09,659
So this is the so called interval estimation. And also we have a prediction of future data.

293
00:34:09,660 --> 00:34:24,050
So now given the series of coin tosses, so let's say here, after observing this five outcomes, can we predict what the outcome is from next course?

294
00:34:24,770 --> 00:34:31,710
All right. So this is another typical signal to.

295
00:34:34,510 --> 00:34:44,130
Okay. So these are some of the. Yeah, well, this is a this is this is the last night of the wife shows 18.

296
00:34:44,880 --> 00:34:55,810
I think this is the last slide of this lecture. So this sort of gives us an idea what we're going to see in 602.

297
00:34:56,020 --> 00:35:04,150
So they're going to learn tools to make port estimation and testing, construct scenarios and make predictions.

298
00:35:04,990 --> 00:35:09,130
So any questions before we move on to next slide?

299
00:35:39,450 --> 00:35:48,390
Okay. And then we're going to start talking about some formal concepts instead, as do inverse.

300
00:35:48,720 --> 00:35:52,440
The very first important concept is for so-called sufficiency.

301
00:35:58,520 --> 00:36:03,860
Sufficiency. Sufficiency.

302
00:36:03,860 --> 00:36:09,410
Actually, the fundamental idea comes from the idea of data reduction.

303
00:36:10,370 --> 00:36:16,310
This is actually the first major hobby where we're going to be focusing on right now.

304
00:36:16,730 --> 00:36:19,880
First part of the actress. So debt reduction.

305
00:36:19,890 --> 00:36:22,310
So let's take a look at what we mean by data reduction.

306
00:36:23,770 --> 00:36:31,750
So again, we are going to use the small letters, the lowercase letter, to denote the realizations of these capital letters, wrote the variables.

307
00:36:33,400 --> 00:36:38,380
Now, what data reduction means is that okay, maybe we.

308
00:36:39,710 --> 00:36:53,630
If you look at of a concrete example before we look at abstract notation, so let's say that you are interested in estimating the success probability.

309
00:36:53,870 --> 00:37:04,970
I toss a coin right or probably has got a hat and lesser you toss a coin 100 hops and of course you observe a sequence of has and tails.

310
00:37:05,780 --> 00:37:17,420
And of course, one way to store the data is to store all these different has until all you like h h t t h so you can store all these 100 data points.

311
00:37:18,140 --> 00:37:25,640
Or you could summarize data points. For example, you just calculated the sort of like you define has as long tail as zero.

312
00:37:25,850 --> 00:37:31,670
Then you can take the sum of this 100 numbers and you only store a single number, right?

313
00:37:31,910 --> 00:37:38,240
So then when it comes to estimate the success probability, the single numbers.

314
00:37:38,320 --> 00:37:43,700
Well, intuitively, it is good enough, because as long as I know that I know this 100 trials,

315
00:37:43,700 --> 00:37:48,889
I have, for example, 66 assets, then my estimate it would be 60 over 100.

316
00:37:48,890 --> 00:37:57,770
That's point six. It doesn't matter which like which of these hundred files are has and which of these 100 trials are tails, it don't matter much.

317
00:37:58,220 --> 00:38:02,000
So we do not need to store all these 100 data points.

318
00:38:02,300 --> 00:38:06,050
So this actually is the idea behind the so-called data reduction.

319
00:38:07,250 --> 00:38:13,670
So if we define a function was to include this as a function of the.

320
00:38:14,780 --> 00:38:19,460
Numerical values and of this function has lower dimension.

321
00:38:19,880 --> 00:38:25,460
So originally we have n data points. So if you cost 100 times, we will have 100 data points.

322
00:38:26,270 --> 00:38:33,430
But this function, it has a lower dimension. Just like if you simply take or up that you want to have a schema that does not matter.

323
00:38:34,970 --> 00:38:41,690
So we wish to, to, to summarize, they had two well, so the summary is simpler than the original data.

324
00:38:41,690 --> 00:38:47,870
So that is the dimension used is no longer this should be smaller than the original data points.

325
00:38:48,890 --> 00:38:56,540
But on the other hand, we will need to keep all the information about theta that is contained in our original data.

326
00:38:57,020 --> 00:39:11,410
We do not want to lose the information. So for example, let's say again, let's consider let's consider, you know, age.

327
00:39:11,440 --> 00:39:21,400
If you climb a people's age from like from 100 individuals and if you only take the average of these age and a store to that single number,

328
00:39:21,820 --> 00:39:27,520
have a single number, of course, has smaller dimension. So it's a it's a it's a summary of the original data.

329
00:39:27,760 --> 00:39:32,860
But if you look at that number alone, it doesn't tell you how. Spratt How spread out the data is.

330
00:39:33,310 --> 00:39:40,140
So we don't then we're going to do know the range of that, for example, the range of that action.

331
00:39:40,750 --> 00:39:49,030
So when we do data reduction, we will learn also to keep all the information about theta in the original data.

332
00:39:50,700 --> 00:40:03,660
So that's the reduction. And then the typical way, one typical way to do a DNA reduction, that's through the so-called statistics of statistics.

333
00:40:04,410 --> 00:40:08,190
There are two ways of understanding or interpreting statistics.

334
00:40:08,580 --> 00:40:10,300
So here, let's take a look at the first way.

335
00:40:10,330 --> 00:40:22,080
This is probably not the most intuitive way and also is probably the way that it will used the most later throughout the programing,

336
00:40:22,170 --> 00:40:32,690
later in your your job, your career. So of studies, generally speaking, it's just a function of the data, just a function of the rhythm here.

337
00:40:33,210 --> 00:40:36,720
These are the wrong variables from individuals.

338
00:40:37,380 --> 00:40:41,040
So a statistic is simply a function of this X.

339
00:40:42,390 --> 00:40:48,030
And so because these acts, they are rather Marable's and so T is a function so which is also a run.

340
00:40:48,480 --> 00:40:56,700
So a statistic is a rhythm error. This is one thing that we need to be very clear as the routine is a random variable is a function of the data.

341
00:40:57,810 --> 00:41:00,209
So then because it's a function of the data,

342
00:41:00,210 --> 00:41:09,270
then of course it would define and defines a form of data reduction or summary because we mentioned that data summary is just a function of the data,

343
00:41:09,840 --> 00:41:15,510
because a statistic is a function of the data. So defense defines data reduction.

344
00:41:16,050 --> 00:41:26,760
So for example, well, this is, for example, let's say that we have this running variables representing passing the coin.

345
00:41:27,180 --> 00:41:32,640
So each side is either had or tail or failure or success.

346
00:41:33,660 --> 00:41:36,960
And then can you propose any statistic?

347
00:41:37,710 --> 00:41:44,490
T x. She can propose to any student.

348
00:41:46,530 --> 00:41:59,490
Any. So, like, the statistic, it's like.

349
00:42:00,030 --> 00:42:03,510
Yeah, like a statistic would just be something put together. Right.

350
00:42:03,840 --> 00:42:07,770
It was a summing them together, so. Oh, yeah, definitely. That's that's definitely one example.

351
00:42:07,770 --> 00:42:11,190
So here, maybe the question needs to generate a draft.

352
00:42:11,430 --> 00:42:15,239
And is that so any function you give me, that's a statistic.

353
00:42:15,240 --> 00:42:22,040
And if I actually really actually function is s for example, you can take a lesson you can take of the average, you can take the product.

354
00:42:22,050 --> 00:42:26,880
I'll build those X and you can, you can sum the first two plus x, one plus x.

355
00:42:27,060 --> 00:42:32,490
That's also a function of all these acts. Either ignore all the rest of x, but still that's a function of the science.

356
00:42:32,820 --> 00:42:37,110
So we can you can give me a function as one plus x two. That's also a statistic.

357
00:42:37,500 --> 00:42:42,300
So any function of this exercise you give me that says that.

358
00:42:44,300 --> 00:42:53,600
Now, of course the sign up for this exercise seems to have been a reduction can be.

359
00:42:53,940 --> 00:43:00,499
Well okay so that's one way of looking at a data reduction.

360
00:43:00,500 --> 00:43:08,070
That's through a statistic. Another way is through a partition of the sample space.

361
00:43:08,090 --> 00:43:10,550
This is a little bit abstract.

362
00:43:12,200 --> 00:43:23,720
I mean, this is not the understanding of this is not essential for the understanding of the, you know, the expanse of the latter half of the course.

363
00:43:24,230 --> 00:43:37,130
But because this course is based on six a what and this actually contains the central idea of what a probability it itself is a mathematical work.

364
00:43:37,700 --> 00:43:44,900
So it's worthwhile to look at this interpretation of the introduction so it can be viewed as a partition of the sample space.

365
00:43:45,140 --> 00:43:51,290
Now, this becomes a little bit of mathematical of determined by a statistics.

366
00:43:51,470 --> 00:43:55,040
So let's take a look at what we mean by let's take a look at a concrete example.

367
00:43:55,760 --> 00:44:03,850
So suppose now we have EXI, we have, I think, four newly written variables for holy month.

368
00:44:07,310 --> 00:44:12,920
And the P is a number between zero and one, and then we define as the density.

369
00:44:12,950 --> 00:44:18,110
This is simply the sum of these three X size, and this is that statistic.

370
00:44:19,490 --> 00:44:25,490
Now, let's take a look at what the domain and a what are the red is for cheap.

371
00:44:27,040 --> 00:44:37,310
You know how I assemble spaces partitioned by cheap. This is actually illustrative by this slide over here, by this table right here.

372
00:44:38,300 --> 00:44:43,670
So we have three I mean, each X eye is a it is a protrusion curve.

373
00:44:43,790 --> 00:44:51,710
So each side takes that of zero one. Now, in combination, if you look at all these three in combination, we have these eight different combinations.

374
00:44:53,630 --> 00:44:55,850
So this is Azure, the sample space.

375
00:44:56,060 --> 00:45:02,740
So symbol space, if you'll recall, a sample space, small space is just a collection of all the possible values of this run.

376
00:45:03,760 --> 00:45:06,890
That's what a sample space is now.

377
00:45:07,190 --> 00:45:13,460
So the simple space for this three x ise, that's the collection of all the possible values.

378
00:45:13,850 --> 00:45:17,630
So that's the closure of this eight deferred combination.

379
00:45:17,660 --> 00:45:22,280
This is the sample space. So this is a sample space.

380
00:45:23,780 --> 00:45:30,110
And then if you look at T of X, the sum of this is a statistic.

381
00:45:30,560 --> 00:45:37,700
Now, if you take the sum of this X that we have these different values corresponding to this edge combinations.

382
00:45:38,570 --> 00:45:44,360
But if you look at this different values realize that there are only like one,

383
00:45:44,540 --> 00:45:52,940
two, three and a 44 not so zero, one, two and three, only on the 44 matters.

384
00:45:54,350 --> 00:46:05,389
So this then is what we mean by a data reduction is actually a partition of the sample space by a statistic because

385
00:46:05,390 --> 00:46:15,110
we have partitioned the sample space into this for different regions depending on the value of this statistic,

386
00:46:16,820 --> 00:46:26,330
modulo statistic. So that these are equal or a combination along is a single region in a sample space.

387
00:46:26,990 --> 00:46:35,060
But these three, they belong to the same region because they lose because of all these three outcomes.

388
00:46:35,240 --> 00:46:38,030
These two, the statistical value equals one.

389
00:46:38,690 --> 00:46:49,730
And similarly in a similarity, these three combinations, they we partition them into the same region because they form in these two t equal to two.

390
00:46:50,210 --> 00:46:53,210
And then the last one is a separate region.

391
00:46:53,210 --> 00:46:58,670
So it corresponds with you. So the other was this study.

392
00:46:58,730 --> 00:47:05,810
You give me a statistic. In this case you give me the sum, the based on the value of this statistic.

393
00:47:06,530 --> 00:47:10,939
Here I have 44 values based on this, 34 values I partition.

394
00:47:10,940 --> 00:47:14,990
I'm able to partition the sample space. Here are the eight combinations.

395
00:47:15,650 --> 00:47:19,250
I'm able to partition a simple space into four different regions.

396
00:47:20,150 --> 00:47:26,470
And so and then this is actually what we mean here.

397
00:47:27,710 --> 00:47:32,660
The data region can be viewed as a partition of the sample space determined by statistic.

398
00:47:33,350 --> 00:47:41,600
This is a data reduction because in the original sample space you have eight different elements and two different elements.

399
00:47:42,210 --> 00:47:50,190
Now, if you look at the the statistic by the summary, you only have 44 matters, only 44.

400
00:47:50,240 --> 00:47:54,290
And this is a reduction of the data. Okay.

401
00:47:54,290 --> 00:47:59,180
So now let's take a, you know, a bit as break window and resume at a two.

402
00:48:05,156 --> 00:48:16,076
And I just listen to my car. Okay, so let's continue the discussion of this example.

403
00:48:17,726 --> 00:48:29,156
So here we actually I mean, this example is very good in terms of understanding honesty as a way of party, ah, partitioning the sample space.

404
00:48:29,186 --> 00:48:34,376
So as we mentioned, the original sample space is given by this content, this eight elements.

405
00:48:35,236 --> 00:48:38,936
If you can think of this as you know, this is the original sample space, right?

406
00:48:38,936 --> 00:48:41,516
And then we kind of partition this into.

407
00:48:44,466 --> 00:48:58,656
I've been to a lot paid to parties in those into three different regions and based on the value of the statistic, based on value.

408
00:48:58,956 --> 00:49:04,806
And so then the pony lives are just a mathematical, more mathematical way of saying that.

409
00:49:04,836 --> 00:49:11,856
So, you know, we use square the town square party to denote the distinct value of this statistic as well.

410
00:49:12,036 --> 00:49:17,016
This T this is for some X. And so this this denotes a clash of distinct values.

411
00:49:17,016 --> 00:49:25,316
So then in our case, that's zero one, two, three, and then this a of T, right?

412
00:49:25,326 --> 00:49:36,696
Depending on what value small t the student takes, we have the corresponding region as well in 0813283 depending on the matter.

413
00:49:38,046 --> 00:49:45,216
So when you report a data and so instead of reporting the original data, this eight elements,

414
00:49:45,226 --> 00:49:50,526
the different combinations, you could report the value of the statistics.

415
00:49:51,186 --> 00:49:59,526
So for example, let's say that you tell me that your key, the song is you want something would love.

416
00:50:01,146 --> 00:50:07,596
And then of course, well, I don't know which one of the three.

417
00:50:08,106 --> 00:50:12,906
It must correspond to one of these three outcomes, but I don't know which one it corresponds to.

418
00:50:13,986 --> 00:50:18,365
But if your interest is to estimate theta the success of probability,

419
00:50:18,366 --> 00:50:27,996
then it doesn't matter much because your amazement would be just one over one, one out of a31 success out of the three trials or one over three.

420
00:50:28,566 --> 00:50:35,766
So I don't need to know where this one occurs, whether one occurs in the first trial of the first, second or third.

421
00:50:36,426 --> 00:50:39,815
So when you asked me to estimate the sixth, that's probably the oldest.

422
00:50:39,816 --> 00:50:42,606
If I know this obviously is one divided by three.

423
00:50:43,546 --> 00:50:53,166
And so in this sense, indeed, I mean, reporting the value of statistics indeed reveals nothing because you can't worry about,

424
00:50:53,436 --> 00:50:57,346
you know, these three distinct combinations. You just need to know.

425
00:50:57,576 --> 00:51:00,666
Audience one Success out of three clients.

426
00:51:01,836 --> 00:51:05,495
So this is. So is that reporting your regional data?

427
00:51:05,496 --> 00:51:09,606
We fully report this value of the statistic. So.

428
00:51:12,326 --> 00:51:20,666
So this is a reduction of the data. And then essentially we partition the small space into different regions in the partition.

429
00:51:20,996 --> 00:51:27,836
These are coarser that the original sample space because a reinforced assembly space has an added.

430
00:51:29,096 --> 00:51:33,716
But now we partition the sample space into four different subsets.

431
00:51:34,856 --> 00:51:37,966
So t is similar x.

432
00:51:40,706 --> 00:51:47,606
So in the end, then a data reduction can be achieved by calculating the statistics value of the statistic.

433
00:51:48,116 --> 00:51:58,586
So a statistic that if you look at this whole example here, what this example says is that statistic can be interpreted as a way of data reduction.

434
00:51:59,186 --> 00:52:05,876
So if you keep if you device that has to if give me a statistic that means you in your way to reduce the data.

435
00:52:06,326 --> 00:52:12,596
And then I just look at the distinct value of the statistic that's a reduction compared to the original data.

436
00:52:12,836 --> 00:52:20,106
So that's one way to interpret the statistic from a data reduction from the partition of the sample space.

437
00:52:21,296 --> 00:52:28,136
And this is actually a not so straightforward interpretation, a more straightforward interpretation, as we mentioned.

438
00:52:28,646 --> 00:52:32,006
Just consider a statistic as a function of the data.

439
00:52:32,006 --> 00:52:37,466
Whatever function you define, that's a statistic. That's probably the easiest way to interpret statistic.

440
00:52:38,096 --> 00:52:46,886
So we have this interpretation of data reduction because this is actually mathematically this is how that as is defined,

441
00:52:47,576 --> 00:52:50,606
is defined in terms of partition some.

442
00:52:51,276 --> 00:52:55,765
But we do not need to worry too much about that because that's related to the property,

443
00:52:55,766 --> 00:52:59,956
the matter interpretation, the probability of run of variables.

444
00:52:59,966 --> 00:53:06,256
But I need to quadruple that. Okay.

445
00:53:06,356 --> 00:53:10,376
So a statistic then provides us a way to reduce the data.

446
00:53:11,456 --> 00:53:20,546
So, of course, it makes data simpler. But on the other hand, well, on one hand, we want to make the data simpler while the reduce on the Y.

447
00:53:20,546 --> 00:53:23,606
The other hand, we want to keep the information in our radial there.

448
00:53:23,846 --> 00:53:26,966
So if you if you define a very simple statistic,

449
00:53:26,996 --> 00:53:33,836
part of the study does not really content much information from the original data, then this is not a good test it has to keep.

450
00:53:34,256 --> 00:53:40,346
So, for example, let's say you are you are interested in estimating the population, average age.

451
00:53:40,466 --> 00:53:43,616
You collect age data points from 100 individuals.

452
00:53:44,606 --> 00:53:53,425
Then if you define as the density, if that's just the first data that I so like data from there for a subject that's

453
00:53:53,426 --> 00:53:59,215
definite statistic that is a very important it has to be because because it

454
00:53:59,216 --> 00:54:06,535
does not get much information about data so so we would like to device that is

455
00:54:06,536 --> 00:54:11,276
that simplify the data and also to to keep as much information as possible.

456
00:54:17,606 --> 00:54:21,016
So this actually a list of here we have our two goals here.

457
00:54:21,026 --> 00:54:23,956
So will I troisieme our data into a similar forum.

458
00:54:25,316 --> 00:54:31,676
On the other hand, we want to keep all the information about data that is content in the original data.

459
00:54:33,956 --> 00:54:37,526
So how can we achieve the second goal? Keep all the information.

460
00:54:39,236 --> 00:54:44,216
This adds at least to the concept of sufficiency sufficient statistics experience.

461
00:54:45,926 --> 00:54:57,655
So a statistical t of x while function elevator contains all information about what we were do have this to contain all the information about

462
00:54:57,656 --> 00:55:11,996
data in the original data rather was not given this t x even tracks that knowing the full data does not provide any full information about it.

463
00:55:13,796 --> 00:55:18,176
So again, we can think of this in terms of estimating the success probability.

464
00:55:19,136 --> 00:55:26,996
So if you toss the quote 100 times and if you get 60 out of 160 pass or 60 successes,

465
00:55:27,986 --> 00:55:32,896
and if you tell me, okay, so out of this 100, I get a 60 successes.

466
00:55:33,926 --> 00:55:42,895
Then given this 60 given this number, then I don't need to know exactly which trial is the hat.

467
00:55:42,896 --> 00:55:48,596
We to trial is a help in order for me to assume this sounds complete because I'm not asked.

468
00:55:48,596 --> 00:55:53,696
Men will be 60 over 100 knowing exactly which trial.

469
00:55:53,876 --> 00:55:57,416
Give me a hat, which I'll give you until it doesn't matter much.

470
00:55:58,196 --> 00:56:12,265
So this is an example where given the statistic was people the song and knowing the full data and is knowing exactly which is I'll give you fail,

471
00:56:12,266 --> 00:56:17,426
which I'll give because it does not provide any further information about a success probability.

472
00:56:17,996 --> 00:56:22,136
So in this case, team called a sufficient distance.

473
00:56:23,636 --> 00:56:28,336
So vision means that well we can after knowing the statistic over.

474
00:56:28,376 --> 00:56:32,846
No, it's not. We can safely discard all the original data.

475
00:56:34,166 --> 00:56:40,586
That's not a content, any useful information and we can just focus on the t x in itself.

476
00:56:40,856 --> 00:56:44,306
So this is what we mean by a sufficient statistic.

477
00:56:46,046 --> 00:56:50,936
So it contains sufficient information or all information in the data to make a

478
00:56:51,056 --> 00:56:56,276
statement about data so we can get all the information about data to see no read only.

479
00:56:57,656 --> 00:57:07,646
So then, but this, this is sort of intuition behind the concept of sufficiency, but how do we define this more mathematically?

480
00:57:10,376 --> 00:57:14,216
So this is the formal definition of a sufficient statistic.

481
00:57:15,086 --> 00:57:20,216
So as a statistic, t is called a sufficient statistic for theta.

482
00:57:21,236 --> 00:57:27,356
For theta. If the condition of this fusion of the whole sample x,

483
00:57:27,956 --> 00:57:37,646
if the whole sample x given the value of t does not depend on theta anymore, that's just not a formalization of intuition.

484
00:57:37,646 --> 00:57:39,746
We just talk about the idea, we just talk about.

485
00:57:40,046 --> 00:57:48,476
So given X the distribution as a given cheap X, the disclosure on the x does not depend on theta anymore.

486
00:57:49,886 --> 00:58:06,986
So in other words, not the conditional pdf or part F of x given t of executing this activity is a function that of course is function x x,

487
00:58:07,166 --> 00:58:11,635
but it is not a function of theta and so it does not depend on theta.

488
00:58:11,636 --> 00:58:19,616
So it's independent of in this case we see that this T of x,

489
00:58:19,826 --> 00:58:26,426
this is a sufficient disclosed because it contains all the information about theta in the original data.

490
00:58:28,166 --> 00:58:38,366
So knowing what that means, that means knowing t if you give me this t knowing this t, then the distribution of x does not depend on theta anymore.

491
00:58:38,576 --> 00:58:42,776
So knowing the concrete, a small x value really doesn't help me.

492
00:58:44,366 --> 00:58:49,666
It's in estimating theta. Okay.

493
00:58:49,916 --> 00:58:54,376
So this is up five here.

494
00:58:55,046 --> 00:59:02,876
So connected between the definition and the concept. So the definition is just sort of a more formal way of saying the concept.

495
00:59:03,086 --> 00:59:08,966
So is that is a mathematical definition of it's based on conditional distribution.

496
00:59:10,316 --> 00:59:12,956
So now let's take a look at our example.

497
00:59:13,226 --> 00:59:23,686
So sufficient a statistic example to suppose we have here we have in Bernoulli trials with a source has probably a P and if we want to estimate

498
00:59:23,696 --> 00:59:36,056
the success probability but based on these different data points and we want to show that the sun is a sufficient statistic for for P,

499
00:59:37,136 --> 00:59:42,506
we have actually sort of implicitly to use this result because this is very intuitive, very intuitively.

500
00:59:43,266 --> 00:59:50,726
This should be a sufficient statistic. So if you tell me the total number of successes, then they shouldn't matter much.

501
00:59:51,026 --> 00:59:56,406
Exactly. Which is this success? We is a failure in terms of estimating the success probability.

502
00:59:56,816 --> 01:00:07,736
So now we are going to formally prove this. Using that or checklists, using the definition will show that indeed this is a sufficient version.

503
01:00:11,146 --> 01:00:15,916
So now because we only have that definition available. So the proof has to be based on the definition.

504
01:00:16,756 --> 01:00:22,786
So then let's just check the condition of those billion of X, given the statistic.

505
01:00:24,106 --> 01:00:28,696
So first we will need to obtain let's obtain the distribution of this.

506
01:00:28,696 --> 01:00:32,956
T is from the capital.

507
01:00:34,666 --> 01:00:39,386
Sorry, not supposed to show the keys as we know this.

508
01:00:39,466 --> 01:00:44,205
This is the sum of all these x eyes. And so x eyes follow Bernoulli.

509
01:00:44,206 --> 01:00:50,326
So Assad must follow a binomial distribution. So this is very easy to figure out.

510
01:00:50,866 --> 01:01:00,736
And then if we want to use domination sufficiency to check is sufficient, then we need to calculate the conditional distribution,

511
01:01:01,936 --> 01:01:10,066
conditional equivalence model, and then we need to show that this does fusion, India does not.

512
01:01:10,276 --> 01:01:18,466
And so after that we can claim that T substitution is based on that decision and that's what we are going to do next.

513
01:01:23,586 --> 01:01:27,306
So let's find out the condition of his vision.

514
01:01:30,786 --> 01:01:34,446
So because we have to work out under this vision, this mountain.

515
01:01:36,906 --> 01:01:40,446
So the condition of this vision now because X is discrete.

516
01:01:40,956 --> 01:01:44,886
So the confusion like this is just the probably is probably mass function.

517
01:01:45,246 --> 01:01:49,886
So is the probability that acts equal to X, the T equal to people?

518
01:01:51,396 --> 01:01:57,126
And then it's well, if we just start, we are just writing this, writing this out.

519
01:01:57,186 --> 01:02:01,925
So capital X equals y x x equal to slacks.

520
01:02:01,926 --> 01:02:11,396
Right? And given that a sub is equal to cheat. And then based on the definition of conditional probability, you have seen this is six or what?

521
01:02:11,826 --> 01:02:18,186
That this conditional probability is actually equal to the probability of the drawn event.

522
01:02:18,816 --> 01:02:25,686
And this to draw the event divided by the probability of this this condition here.

523
01:02:26,436 --> 01:02:32,196
Right. About this problem. So this is a very simple 6a1 knowledge.

524
01:02:33,726 --> 01:02:37,146
And now let's take a look at this ratio here, this problem.

525
01:02:39,066 --> 01:02:43,146
So for this probability, now we need to look at a two situations.

526
01:02:44,226 --> 01:02:50,696
So because here we have two events, what event says that all this exercise equal to small exercise?

527
01:02:50,706 --> 01:02:53,856
Another event assess that the sum of exercise equal true t.

528
01:02:55,886 --> 01:03:03,026
Now in the case that indeed, if this small excise subroutine, if this is the case, if this is true,

529
01:03:04,196 --> 01:03:11,756
then you can see that if we call this event a if we call this event a be that event, a implies event,

530
01:03:11,786 --> 01:03:18,086
because then under this if this is true that this if this is true,

531
01:03:18,086 --> 01:03:25,826
then event A implies event B and because because if the sum of a small excise is equal to T,

532
01:03:26,216 --> 01:03:31,856
then because they sense that of this different capital excise equal to all the small excise.

533
01:03:32,186 --> 01:03:44,426
Now, of course, the small capital excise must be equal boutique. So in this case implies B, that is the probability of a problem in this case.

534
01:03:44,976 --> 01:03:48,385
So. We have more data.

535
01:03:48,386 --> 01:03:56,686
So if this is true if this is true, then the event, a implies event be right.

536
01:03:57,056 --> 01:04:09,696
And then a probability that a intersection of A and B is equal to just the problem or probability

537
01:04:09,746 --> 01:04:16,176
in the probability that A and B the two event is just equal to the probability of that.

538
01:04:16,226 --> 01:04:26,546
So that's why we have here I mean this because the P event is sort of redundant once we know A and B must occur.

539
01:04:27,806 --> 01:04:33,486
So the probability of A and B both occurs at just the probability event.

540
01:04:34,626 --> 01:04:39,926
Right. So then in this case, then we have then the regional becomes symbolic of this guy.

541
01:04:42,366 --> 01:04:45,876
If this is not true, that means otherwise. So.

542
01:04:46,116 --> 01:04:55,256
So. You. So if otherwise, if the slow exercise does not sound to teach,

543
01:04:55,886 --> 01:05:02,036
that means if rather an event be cannot occur simultaneously because even assess that of the capital

544
01:05:02,426 --> 01:05:08,786
equal to small and side event be you says that a small x the sum of capital x equals multiple.

545
01:05:09,686 --> 01:05:20,156
But we know that the sum of small excite does not equal v cannot occur simultaneous may not only is the probability of a b is equal to zero.

546
01:05:21,086 --> 01:05:32,756
And so that's why otherwise we have zero. Does everybody follow the argument?

547
01:05:34,976 --> 01:05:39,926
Feel free to let me know if I have questions. Okay, so then we.

548
01:05:40,166 --> 01:05:50,756
We have these two cases. Now let's proceed. Let's focus on this numerator here, this new and then this numerator probability.

549
01:05:58,116 --> 01:05:59,616
Now because this exercise,

550
01:05:59,616 --> 01:06:10,086
their idea for a new trial so that the probability of all of these events is equal to the product of each event and of of course,

551
01:06:10,746 --> 01:06:16,596
because if I follow Bernoulli's or the PDF or PMF is given by sort of given by this,

552
01:06:16,986 --> 01:06:19,296
and then if we modify all of these,

553
01:06:19,746 --> 01:06:28,866
we have the P to power some of the exercise one those people for the power at minus some x y and I've got some of the exercise equal to.

554
01:06:28,956 --> 01:06:34,115
She's right that's that's actually because now we are considering this this case so

555
01:06:34,116 --> 01:06:38,826
the sum of the exercise routine so that's why you have to have the sub on next time.

556
01:06:39,636 --> 01:06:42,746
See all the team and some exercise team.

557
01:06:46,996 --> 01:06:55,666
So this is not the numerator and then the denominator here, this denominator here,

558
01:06:56,086 --> 01:07:04,606
this is the probability that the song is equal because we know that some of the capital was followed by not good disclosure.

559
01:07:04,816 --> 01:07:12,466
So of course we are able to find that the probability that a binomial or other variable e equal to a small team that's just given by this sky,

560
01:07:13,066 --> 01:07:21,426
and that if we take the ratio of this to if we take the ratio of this and this, we get this number which.

561
01:07:29,396 --> 01:07:35,356
Okay. And then. The condition of this building.

562
01:07:35,526 --> 01:07:38,736
But because, again, just recall what we are doing, right?

563
01:07:38,746 --> 01:07:49,696
So we are using the definition of sufficient statistic to check that the sum of the site is a sufficient statistic for for theta or for the society.

564
01:07:50,626 --> 01:07:52,336
So we are using definition to check that.

565
01:07:52,606 --> 01:08:00,256
So in other words, we need to calculate the conditional probability and we need to argue that the use of probability does not depend on P anymore.

566
01:08:01,276 --> 01:08:08,236
Now let's take a look at the probability so we have calculated the probabilities equal to this guy.

567
01:08:09,076 --> 01:08:16,486
If this is true, if the small XYZ subroutine, otherwise we have calculated it would be equal to zero.

568
01:08:18,646 --> 01:08:28,746
But in either case you see that. In either case I know this case where this case the ratio does not depend on P is individual g.

569
01:08:29,866 --> 01:08:37,516
So this that by definition this tells us that the sum is a sufficient statistic.

570
01:08:39,586 --> 01:08:43,936
This finish the proof and this is a proof based on the definition.

571
01:08:45,646 --> 01:08:55,876
So this indeed confirms our our intuition sort of intuition that this sum of X high, this is sufficient statistic for success.

572
01:09:03,226 --> 01:09:12,356
So. Well, this does not I don't think we need to worry too much about this.

573
01:09:12,356 --> 01:09:16,736
Not let's let's skip it because it's it's kind of confusing to have this note here.

574
01:09:16,766 --> 01:09:22,376
Maybe later we will come back to this. But this is a quite a confusing not at this place.

575
01:09:24,536 --> 01:09:29,296
So any questions about this proof or the concept? Definitions of statistical.

576
01:09:38,976 --> 01:09:45,246
Okay. And then we'll go introduce a theorem for sufficient studies.

577
01:09:45,436 --> 01:09:51,786
So previously, I mean, we usually talk about the definition and then we look at an example where we use the definition to show.

578
01:09:52,056 --> 01:09:58,716
Well, some of it is some statistical probability, distribution, success, probability.

579
01:09:59,346 --> 01:10:07,806
Now we are going to introduce a sort of a general theorem that can be used to to check whether a person has a sufficient or not.

580
01:10:08,766 --> 01:10:13,625
So, oh, by the way, we keep the theorem number so that this is, as would say,

581
01:10:13,626 --> 01:10:19,566
number of textbook assumptions, which makes it make it easier to refer to in the textbook.

582
01:10:20,616 --> 01:10:31,326
So here's a post. This is the drawing pdf of Capital X and a q is the PDF of the statistic.

583
01:10:31,956 --> 01:10:33,996
Well, the statistic of course, has on this filter.

584
01:10:34,086 --> 01:10:43,926
So we, we measure the routine is a function of the data is a function of capital X, not because capital actually is a random variable.

585
01:10:44,016 --> 01:10:48,186
Now of course, TS also run the variable and TS are another variable.

586
01:10:48,426 --> 01:11:02,006
We can never talk about its distribution. And so here the Q is the extrusion or the PDF or P, then if T is a sufficient statistic.

587
01:11:02,646 --> 01:11:05,376
So P is as a visual statistic for theta.

588
01:11:05,436 --> 01:11:17,476
If the following is true, if for every x this ratio is constant as a function of theta, this ratio is, of course.

589
01:11:18,456 --> 01:11:23,796
So now, before we look out of the proof, let's take a look at what this theorem implies.

590
01:11:23,826 --> 01:11:31,116
What it implies is that in order to check whether a start is sufficient, we need to know two quantities.

591
01:11:31,746 --> 01:11:39,606
One is the distribution of the original data, and one is the distribution of the statistic.

592
01:11:41,586 --> 01:11:46,316
So once we know this two distributions, we just need to know what.

593
01:11:46,376 --> 01:11:52,086
We just need to calculate the ratio of this two and to see if the ratio is equivalent of that.

594
01:11:52,596 --> 01:11:59,616
If that is if it if it is independent of data that t is sufficient.

595
01:12:00,066 --> 01:12:05,466
Otherwise it's not. So this is how this can be applied.

596
01:12:07,476 --> 01:12:11,006
Now, let's take a look at how this theorem can be proved.

597
01:12:11,016 --> 01:12:16,296
I mean, the proof of theorem. Again, it's application of the definition.

598
01:12:16,626 --> 01:12:20,496
So this theorem in itself is essentially the definition.

599
01:12:20,886 --> 01:12:22,566
It's just a simplified A definition.

600
01:12:22,666 --> 01:12:32,466
Little bit later, I'm going to tell them what it is, because so far about so visual statistic, what we have is only the definition.

601
01:12:33,246 --> 01:12:37,146
So in order to prove the theorem, of course, we have to go back to the foundations.

602
01:12:37,656 --> 01:12:45,186
So using the definition of sufficiency, use of the definition sufficiency we need,

603
01:12:45,186 --> 01:12:55,896
while the definition says that he is sufficient if the condition distribution of X given t does not depend on theta.

604
01:12:56,656 --> 01:13:01,926
Now let's just look at the definition. Let's calculate the condition of the spirit, the conditional diffusion.

605
01:13:01,926 --> 01:13:06,366
Again, I mean, this calculation is very similar to the baloney example.

606
01:13:07,266 --> 01:13:14,376
So this is the probability is equal to the probability of an event, a event to be divided by the probability of event.

607
01:13:14,976 --> 01:13:23,676
And that's that's the conditional probability of a given B and A then now for this event A, this event B.

608
01:13:24,606 --> 01:13:35,026
Now, if this is true. If t t function at a small adult is equal to small t, then that means A implies B,

609
01:13:35,056 --> 01:13:45,356
right because was a occurs was a occurs not because t actually see who t then t at a capital x must be in control.

610
01:13:46,186 --> 01:13:54,316
So let me is if this is true, then A implies B, that means the probability of A and B is equal to the probability to A in itself.

611
01:13:54,676 --> 01:14:02,536
So that's why we have that's why we have here in this case is just become probably five eight divided by the probability.

612
01:14:04,576 --> 01:14:13,126
So if this is not true, that it means otherwise if this is taxi's not able to t that means event b, event b is not true.

613
01:14:13,486 --> 01:14:20,656
Now, of course, the probability of A and B is equal to zero, because if an B's not does not occur, never occurs.

614
01:14:21,106 --> 01:14:34,606
So this then is this is equal to zero. And then if we look at this probability here, this is the P mf.

615
01:14:34,986 --> 01:14:41,176
Now, by the way, so the proof here where we're only focusing on the discrete case, this this makes the argument easier.

616
01:14:41,236 --> 01:14:46,576
Of course, this can be extended. There is a proof for a continuous case as well.

617
01:14:46,576 --> 01:14:49,996
But try to see the central idea here.

618
01:14:50,026 --> 01:14:59,286
We just focus on this great case. Now for discrete case, this probability is equal to the now that's just PMT.

619
01:14:59,696 --> 01:15:10,786
And then the denominator here of course, is not CnF or T, so that's one T is equal to otherwise is equal to zero.

620
01:15:13,396 --> 01:15:24,076
And then as we mentioned but if so, of course, this zero does not depend on theta for sure has nothing to do with theta.

621
01:15:25,066 --> 01:15:33,466
So then if this guy does not depend on theta, then then this conditional probably due to a argument of data,

622
01:15:34,096 --> 01:15:37,476
then by definition is not sufficient uncertainty.

623
01:15:38,596 --> 01:15:47,926
So this has reduced the result because the theorem says that if this ratio is not theta, then he is a sufficient statistic.

624
01:15:48,766 --> 01:15:50,776
And so that's exactly what we have proved.

625
01:15:51,046 --> 01:15:59,386
So because zero does not depend on theta and that is, of course, if this does not involve the idea that this conditional,

626
01:15:59,836 --> 01:16:05,686
this future does not involve theta, then we choose to use this company as a proof.

627
01:16:07,456 --> 01:16:12,006
So essentially the theorem is still the definition for it.

628
01:16:12,616 --> 01:16:23,146
Sort of simplify the definition a little bit. I mean, this slides try to emphasize that, yeah, this last try try tried to and make it explicit.

629
01:16:23,566 --> 01:16:27,766
So why this? So this theorem is useful.

630
01:16:29,086 --> 01:16:40,426
Let's be honest. Useful because if you use the definition, the original definition to show consistency is not consistent, to show sufficiency.

631
01:16:40,846 --> 01:16:44,956
If you use the original definition, just like what we did here.

632
01:16:45,086 --> 01:16:51,976
So inside of proof, if we use the original definition, then we would have to separate these two cases.

633
01:16:52,576 --> 01:16:58,216
But rather this is chief routine. We're not and again, we have to look at this two cases.

634
01:16:59,776 --> 01:17:06,285
However, I mean, realizing that the second the case, I mean, that the problem is always equal to zero.

635
01:17:06,286 --> 01:17:11,796
So it really doesn't matter at all whether we look at the second case or not.

636
01:17:11,806 --> 01:17:16,426
We could only just focus on the first case. That's essentially what the film says.

637
01:17:16,666 --> 01:17:22,086
The theorem says that, okay, let's forget about this case, because in this case the problem is always zero.

638
01:17:22,096 --> 01:17:30,546
So why bother to look at that? So we only look at the first case and we only look at this, which is an event of theta that.

639
01:17:30,586 --> 01:17:37,486
TS So we supposed so that that is the difference between the definition and the theorem.

640
01:17:38,566 --> 01:17:46,785
So the definition we have to separate these two cases. But about the theorem 6.21.

641
01:17:46,786 --> 01:17:52,726
True, but only to look at that particular issue, this particular issue here.

642
01:17:54,676 --> 01:18:00,376
But essentially, again, we we need to be clear on that. Six, 662.4 is just the definition.

643
01:18:00,616 --> 01:18:06,856
It's just that we do not need to separate the two separately to look at all two cases using the definition.

644
01:18:07,906 --> 01:18:14,206
Okay. And then the previous note that we skipped, that's actually exactly the same point.

645
01:18:14,926 --> 01:18:19,386
So that's why I started this this put in here, I mean, so that it could fizzle.

646
01:18:20,986 --> 01:18:26,086
So because I mean, when this is not true, then the probability is just equal to zero.

647
01:18:27,316 --> 01:18:29,086
It has to be shown in the proof.

648
01:18:29,326 --> 01:18:38,716
So because we have we have seen that if if this is not true, that means otherwise that neutrality is always is always equal to zero.

649
01:18:38,746 --> 01:18:42,196
That's exactly what this note means here.

650
01:18:42,526 --> 01:18:46,766
So when this is not true, this is always equal to zero. So that means.

651
01:18:46,786 --> 01:18:55,276
Well, of course, zero does not depend on theta. So that means when we look at a sufficiency, we don't even need to look at this case at all.

652
01:18:55,426 --> 01:18:59,625
Because. Because the original is not going to work for the usual probability.

653
01:18:59,626 --> 01:19:05,836
He's not going to say that anyway. So. So that's that's what this note says.

654
01:19:06,406 --> 01:19:11,816
So we don't need to consider the this particular case. And that leads to the theorem.

655
01:19:11,836 --> 01:19:17,056
So let's just look at this region in order to tell whether t's sufficient or not.

656
01:19:25,246 --> 01:19:33,816
Any questions about this film? Okay.

657
01:19:33,846 --> 01:19:39,155
And then let's take a look at one example, essentially the same example.

658
01:19:39,156 --> 01:19:45,306
But now we are using instead of using the definition, how long is this you zero to show a sufficiency.

659
01:19:45,846 --> 01:19:54,216
It's the same problem. So we suppose that we have this random variables followed Bernoulli because that's probably the P of.

660
01:19:55,736 --> 01:19:59,635
This is not you. In this case, theta is equal to P.

661
01:19:59,636 --> 01:20:03,926
So I guess here you could, you could use that abuse a few days later.

662
01:20:08,696 --> 01:20:19,236
Okay. And then we want to show that in the sun is a sufficient statistic for you.

663
01:20:19,556 --> 01:20:24,566
So it's the same problem. Now, we want to use the theorem to show that it is sufficient.

664
01:20:25,676 --> 01:20:36,326
So in order to apply a theorem, we actually just need to show that the ratio of his true this ratio.

665
01:20:36,776 --> 01:20:47,366
This is the ratio of the drawing to a PDF of the original data and of the pdf of the statistic that we're looking at.

666
01:20:48,286 --> 01:20:55,596
The. So we look at the ratio and we just need to show that the ratio is that's not enough.

667
01:20:57,506 --> 01:21:01,286
So then we just need to work out the top down to the bottom.

668
01:21:02,066 --> 01:21:08,696
So the numerator we draw the pdf because each x i follow for Louis.

669
01:21:09,356 --> 01:21:15,416
And so then the draw, the pdf is just the product of all this promoted math.

670
01:21:15,776 --> 01:21:18,385
So then first, if you simplify it,

671
01:21:18,386 --> 01:21:29,816
you get this expression them for key because we know that capital T followed by normal description is the sum of four who can develop a holy idea.

672
01:21:29,816 --> 01:21:42,176
Bernoulli like there was one followed by this fusion. And then for a normal distribution, we know that pure math is given by you by this, right?

673
01:21:42,186 --> 01:21:47,126
And then we can now we find both this and this, we're going to take their ratio.

674
01:21:50,656 --> 01:21:54,706
Okay. So that would just take the rest of this, too.

675
01:21:55,546 --> 01:22:01,616
That's the regional capital of the region. So on.

676
01:22:04,126 --> 01:22:11,866
So here this. This is. Okay. So this is just not the same guy just up and coming over here.

677
01:22:12,326 --> 01:22:16,306
And then this guy is just this guy.

678
01:22:16,336 --> 01:22:19,426
But replacing T with a sub of xyz.

679
01:22:20,326 --> 01:22:26,236
Now, we need to simplify this and try to show that this does not change anymore.

680
01:22:30,166 --> 01:22:35,625
So let's see how to. Oh, this is actually quite straightforward.

681
01:22:35,626 --> 01:22:38,816
So. So, I mean, this guy council with this guy and this guy council with this guy.

682
01:22:38,836 --> 01:22:44,206
So what is left is just one over this and this one over this.

683
01:22:44,896 --> 01:23:03,696
And of course, this does not depend on. And this shows that the capital t the sun is actually a sufficient statistic.

684
01:23:04,986 --> 01:23:07,356
So if you compare this proof to the previous proof,

685
01:23:08,616 --> 01:23:15,636
we can see that this proof is slightly similar because this proof we only look out at the ratio of the stream.

686
01:23:16,026 --> 01:23:25,946
Now if you look at the previous proof using the definition, if we go back to the previous proof here.

687
01:23:34,446 --> 01:23:39,816
So previously when we used the definition, now we had to separate these two cases.

688
01:23:39,836 --> 01:23:51,156
Now whether the sum of exercising is equal to T or otherwise, not because the definition or because there is a conditional probability involved.

689
01:23:51,456 --> 01:23:58,266
So the calculation here is slightly more complex. She?

690
01:24:01,476 --> 01:24:06,606
Okay. That's one example. Now, let's take a look at another example.

691
01:24:08,616 --> 01:24:21,036
That's normal distribution with no parents. So suppose we have idea sample from normal distribution with no power.

692
01:24:21,106 --> 01:24:24,636
So see, my square is not only the beam varies. I don't care.

693
01:24:26,656 --> 01:24:35,956
And we want to show that the sample mean is a sufficient statistic for several means of surveillance.

694
01:24:36,346 --> 01:24:49,336
So what this means is that, well, the implication of this is that let's say that you are trying to estimate the population average average income.

695
01:24:50,116 --> 01:25:02,616
So an average income and you can't data you can't to income data from, let's say, 1000 individuals and you're trying to estimate a while.

696
01:25:02,656 --> 01:25:11,715
Let's say we assume that the income for a normal distribution and then you could store the original,

697
01:25:11,716 --> 01:25:14,896
you know, a thousand data points on people's income,

698
01:25:15,496 --> 01:25:23,056
or you could simply calculate the assembled average of this 1000 data points and then just store that a single data points.

699
01:25:25,426 --> 01:25:31,276
And it turns out that now because this sample mean is a sufficient statistic.

700
01:25:31,966 --> 01:25:37,636
So if you simply store the meat itself after a couple assemble average a request for average,

701
01:25:38,236 --> 01:25:43,936
you can completely and safely ignore discard the original 1000 data points.

702
01:25:44,986 --> 01:25:49,696
Those data points do not provide any additional information about the meat anymore.

703
01:25:50,386 --> 01:25:58,906
So that's the implication of this zero. So because sample because the sample mean is a sufficient statistic.

704
01:25:59,176 --> 01:26:03,676
So once we calculate the sample mean the original data,

705
01:26:04,186 --> 01:26:10,785
the original data does not contain any information about any additional information about you anymore.

706
01:26:10,786 --> 01:26:14,116
We can safely discard the original data.

707
01:26:15,106 --> 01:26:19,796
So that's what this implies.

708
01:26:19,846 --> 01:26:24,226
Now let's use the theorem to prove this result.

709
01:26:26,356 --> 01:26:31,815
Now to to use the theorem to prove. Well, of course, we, we, we are looking on a normal distribution.

710
01:26:31,816 --> 01:26:42,316
So we need to do the coverage based on normal PDF we call it normal pdf is given by the PDF Normal.

711
01:26:42,316 --> 01:26:54,316
This fusion is given by this. So this is, this is actually just a PDF from all of us.

712
01:26:54,886 --> 01:27:02,865
Now, in order to use the theorem to prove that the t the sample average is a solution.

713
01:27:02,866 --> 01:27:10,076
So let's see. We need to calculate this reach. And we need to show that this is a function of X alone.

714
01:27:10,096 --> 01:27:21,075
So it does not depend on. Does not depend on. So in order to counter the ratio,

715
01:27:21,076 --> 01:27:29,006
of course we need to know what are the numerator is and what when the denominator is the numerator, it's just the normal pdf.

716
01:27:29,266 --> 01:27:36,615
So that's very easy to get out. Now we need to know what the bottom the denominator is.

717
01:27:36,616 --> 01:27:46,006
The denominator is the pdf of the statistic where the symbol average, the symbol average of some normal variables.

718
01:27:46,316 --> 01:27:50,736
So we know that the sum of average or normal relevant, it will still for a normal solution.

719
01:27:51,076 --> 01:27:58,006
And so that makes things a lot easier. So what is Q a pdf of the sample average?

720
01:28:00,376 --> 01:28:05,476
So while we follow normal, this usually is still for normal.

721
01:28:05,476 --> 01:28:07,456
And then we need to figure out the parameter.

722
01:28:13,456 --> 01:28:22,066
So recall that the sample average because original exi followed normal, those fusion would be new and very similar square.

723
01:28:22,396 --> 01:28:29,606
So the symbol average followed from the student with the same we we'll put a virus divided in sample size.

724
01:28:30,646 --> 01:28:41,146
That's that's just a fact from 6 to 1 and then because t follow this normal distribution then the PDF of T

725
01:28:41,146 --> 01:28:50,176
of course is given by we just need to replace the CMA square in the original video if I see my square again.

726
01:28:50,626 --> 01:28:57,766
And that gives us this PDF file.

727
01:29:00,276 --> 01:29:03,856
Okay. And then.

728
01:29:07,916 --> 01:29:13,166
Okay. So that the numerator I mean the true on a PDF that's because we have this data right.

729
01:29:13,226 --> 01:29:17,456
So the going to be the app is just the product of this, all these individual PDFs.

730
01:29:18,896 --> 01:29:27,736
So after we now know we got both the top of the results and the bottom of the calculator, the original, right.

731
01:29:27,746 --> 01:29:40,986
So, so and to show that it is actually a constant it does not depend on the so in order to show the ratio does not depend on view.

732
01:29:41,396 --> 01:29:46,016
Now we need to a while let's use some very slight algebra.

733
01:29:46,436 --> 01:29:50,755
So here for the drawing, the PDF, we are going to do the following thing.

734
01:29:50,756 --> 01:29:54,806
So drawing a PDF again is the product of this individual PDFs.

735
01:29:56,126 --> 01:30:10,526
Then because here is the same guy multiplied by in times that we have this guy to the power to the power 90 and over to right.

736
01:30:10,526 --> 01:30:15,476
So that's that's just this guy and then the product of this guy.

737
01:30:16,076 --> 01:30:22,276
Now this is the exponential function. So the product values are the exponential becomes this source a sun.

738
01:30:23,756 --> 01:30:33,745
And then for the sum here we are going to make this trick.

739
01:30:33,746 --> 01:30:37,736
Well, we'll apply this trick so we we minus x bar then plus x bar.

740
01:30:38,506 --> 01:30:48,146
I think we have seen this in 650 use this so 50 so that here we have this guy plus this guy, this square.

741
01:30:49,316 --> 01:31:09,506
So then we have so this thing then or or so I'm going to look at this whole thing which is becomes sub five from one to in x i.

742
01:31:11,576 --> 01:31:23,996
Five squared plus two times x I'm class x bar times are plus x bar minus me all

743
01:31:23,996 --> 01:31:31,136
square the plaza across to across from these x minus x bar times x bar minus me.

744
01:31:34,036 --> 01:31:46,996
It's not some of this whole thing. And then now if I separate to the source, it's going to be x I minus x bar square.

745
01:31:49,066 --> 01:31:56,686
Plus the sum x bar minus New Square one plus two times.

746
01:31:56,686 --> 01:32:02,556
Some acts are about as far as far as my dispute.

747
01:32:05,506 --> 01:32:17,436
But this is just out. And then realizing that now for this song, for this, I'm here because X bar minus me does not depend on some level.

748
01:32:17,506 --> 01:32:22,516
It does not depend on. So the sum is if actually just for this guy.

749
01:32:24,166 --> 01:32:33,256
The song. But this guy must be zero. Right? So because this is a song of exile minus some of x, I find some X bar, which is also some kind of song.

750
01:32:33,466 --> 01:32:38,896
So this is equal to zero. So in other words, this God here, this is equal to zero.

751
01:32:39,076 --> 01:32:43,936
So that's why the cross harmonies up here and then this sum over here.

752
01:32:44,176 --> 01:32:48,166
This is a song of the same thing. So that it's ten times.

753
01:32:48,256 --> 01:32:52,006
That's right. That's that's why I hear mountaintops. This is where.

754
01:32:55,236 --> 01:32:58,356
But this is just not something quite a simple algebra.

755
01:33:01,956 --> 01:33:11,766
Okay, so after we got here, then let's calculate the ratio between the two densities and this is the distribution.

756
01:33:18,296 --> 01:33:27,386
Okay. And then. So the table you just calculated is equal to this, and then the bottom of this.

757
01:33:28,166 --> 01:33:37,226
Now, if we take the ratio, then we realize that, oh, here, this guy, this part can cancel with this part.

758
01:33:39,016 --> 01:33:39,886
Cancel this party.

759
01:33:40,366 --> 01:33:52,906
And it's what is left is just not just this part inside this building and then this part in this part, if you combine them when you get this.

760
01:33:56,316 --> 01:34:03,716
Then if you look at a final expression, we realize that there is no meal anymore in this fashion.

761
01:34:05,336 --> 01:34:08,246
So the idea was this ritual does not depend on me.

762
01:34:08,596 --> 01:34:16,916
So you cannot really solve them by zero six according to this result, because this region does not matter to you.

763
01:34:17,606 --> 01:34:21,656
So capital T is a sufficient statistic.

764
01:34:22,766 --> 01:34:30,196
And so the sample average is indeed a sufficient statistic. She?

765
01:34:32,896 --> 01:34:40,096
This is how the theorem can be applied to check whether this is sufficient or not.

766
01:34:44,926 --> 01:34:48,646
So any questions about this example or this? Two examples.

767
01:34:55,576 --> 01:35:04,046
Understand why we want a statistic, because it seems like she tried to make them inferences, which are the inference about the population rate,

768
01:35:04,106 --> 01:35:09,326
for example, like we're not actually going to throw in data because we have a difficult system.

769
01:35:11,636 --> 01:35:14,956
I'm sorry, I didn't hear the latter part of the question clarity.

770
01:35:14,966 --> 01:35:19,136
So your question was like, why we care about why do we care about something?

771
01:35:20,846 --> 01:35:33,655
Okay. So that's that's a good question. So. So first of all, I mean, it's not just about a statistic is a fundamental concept.

772
01:35:33,656 --> 01:35:40,276
So everything you see, first of all, our display is called a statistics statistic.

773
01:35:40,286 --> 01:35:43,906
So, so bad about the statistic.

774
01:35:43,916 --> 01:35:49,616
Now I'm talking about lots of analogies. And so the studies, I'm talking about the functional data, right?

775
01:35:49,826 --> 01:35:52,826
So we care about statistic because they are summary of the data.

776
01:35:53,666 --> 01:36:00,156
So pretty much everything you use in making inference, that's a statistic.

777
01:36:00,176 --> 01:36:07,736
For example, you say it's 50, you try to estimate a balance so you have better hat like this classmate or Emily or his classmate.

778
01:36:08,546 --> 01:36:13,886
650 doesn't talk about and. So. So this is where as major that's been a hat.

779
01:36:14,456 --> 01:36:19,526
That's a summary of data. You can look at how to base our data. And so speed of hat is a statistic.

780
01:36:19,916 --> 01:36:26,516
So pretty much everything we use and you can you can consider it to be a functional data and that is a statistic.

781
01:36:27,506 --> 01:36:35,366
Now, first statistic of this division is noticeable for there are differences.

782
01:36:35,576 --> 01:36:37,646
So here we're talking about some visual statistic.

783
01:36:37,656 --> 01:36:43,496
And later on we'll talk about complete and later along with how about I see a very well different different type of statistic.

784
01:36:43,976 --> 01:36:49,406
So sufficient statistic is important, at least for one major reason.

785
01:36:49,406 --> 01:36:53,365
That is well, let's take the well again.

786
01:36:53,366 --> 01:36:56,336
Let's take that the burglar as an example.

787
01:36:56,636 --> 01:37:06,296
So if we are trying to estimate the success probability for any of your client data, if you throw a coin for a hundred hats,

788
01:37:07,106 --> 01:37:13,676
you can choose to record these 100 data points or you can choose to just record of the surveillance statistic.

789
01:37:15,076 --> 01:37:22,705
So sufficient statistic, it means that once you know the sufficient statistic, you don't care much about the original data.

790
01:37:22,706 --> 01:37:27,986
No, real data doesn't help you anymore in terms of estimating the quality that you're interested in.

791
01:37:28,826 --> 01:37:33,476
So that's the, the, the, the power or the usefulness of sufficient statistic.

792
01:37:34,106 --> 01:37:42,356
So it is, it is sufficiently adequate to summarize all the data as our all the information from the original data.

793
01:37:43,816 --> 01:37:51,836
That's why it's called a sufficient statistic. That's not one of the major reasons that we talk about something.

794
01:37:52,136 --> 01:37:57,896
Of course, there are other mathematical reasons, but this is more intuitive reason why we talk about it.

795
01:37:58,766 --> 01:38:04,556
So it gives you a way to to summarize the data without lose any information.

796
01:38:08,616 --> 01:38:11,976
Okay. So we are out of time, so we will stop here.

797
01:38:12,996 --> 01:38:14,666
You know, if I could just.

