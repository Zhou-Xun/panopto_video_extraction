1
00:00:01,570 --> 00:00:07,230
There we go. All right. So slide number five.

2
00:00:08,500 --> 00:00:14,010
They said page number five. So again, this was way back when I was trying to motivate.

3
00:00:14,020 --> 00:00:17,530
What correlation does it appeared setting? No, longitudinal here.

4
00:00:19,000 --> 00:00:23,170
Everybody is longitudinal. Everybody has a pair of observations. But the question was this.

5
00:00:23,350 --> 00:00:27,190
There was this notation here, what is exercise in a matrix? Is it a vector?

6
00:00:27,520 --> 00:00:34,770
It is a vector. All right, so what else was on that?

7
00:00:35,460 --> 00:00:40,470
Just to make it clear to the person who asked, I'm confused about notation for X, Y and beta.

8
00:00:41,520 --> 00:00:45,330
So again, there's an intercept. And there's a group difference.

9
00:00:45,360 --> 00:00:53,460
So The Intercept is one of the groups, the mean of one of the conditions and beta one was the difference between the two conditions on average.

10
00:00:54,540 --> 00:00:58,710
So everybody has an intercept and whether we have a free or imposed.

11
00:01:03,330 --> 00:01:07,830
So that's all I have there. If we want to talk more later, we can.

12
00:01:09,390 --> 00:01:13,470
And if I didn't explain to whoever asked and wants to come up later, I'm happy to talk about that too.

13
00:01:17,040 --> 00:01:22,230
Moving on then to correlated data methods. Let's do number three.

14
00:01:22,260 --> 00:01:25,079
Number two, I want to do a little bit of writing on the web, on the web,

15
00:01:25,080 --> 00:01:31,860
where the computer sandwich variant estimate is essentially what are the drawbacks of using them and what kinds of situations that we expect.

16
00:01:31,860 --> 00:01:37,650
The sandwich variants estimates to be smaller than versus larger than model based variance estimates.

17
00:01:38,910 --> 00:01:46,050
So the three, five, three people came into my office hours, phrased this as When would I use sandwich senators?

18
00:01:48,570 --> 00:01:51,750
Rarely. Um.

19
00:01:53,720 --> 00:01:59,010
This is a challenging topic. We use them all the time and DJ.

20
00:01:59,200 --> 00:02:03,580
As you start to learn, gee, hopefully you're going to see how it's connected to what we're doing now.

21
00:02:04,180 --> 00:02:10,450
People use robust janitors all the time in G.E., but they don't use them in goals and goals.

22
00:02:10,480 --> 00:02:15,010
G.E. is simply goals with not normal outcomes.

23
00:02:15,580 --> 00:02:19,360
So I don't really get it why we have the two different approaches.

24
00:02:19,360 --> 00:02:32,720
But anyway, if correlation is nothing more than a nuisance to you, the way it's correlation is simply a nuisance.

25
00:02:32,740 --> 00:02:36,880
You know, that correlation is going to make the standard errors wrong.

26
00:02:38,180 --> 00:02:44,300
And you don't care about the correlation otherwise. Then I would use independence.

27
00:02:45,260 --> 00:02:51,190
Can you get a sandwich standard here? I use independence because I know my estimate is consistent.

28
00:02:53,160 --> 00:02:58,620
And I use the robot standard error to fix the correlation and of the standard errors that wasn't accounted for.

29
00:03:01,810 --> 00:03:04,840
An alternative approach is to say, Well, Tom, that's dumb.

30
00:03:06,790 --> 00:03:10,540
I know there's a correlation structure in the data and I'm going to model it appropriately.

31
00:03:10,540 --> 00:03:13,600
I think it's compound symmetric. I think it's auto regressive.

32
00:03:15,340 --> 00:03:21,070
That also gives me an estimate of the correlation. I want to know what the model, the data are telling me in terms of correlation.

33
00:03:21,370 --> 00:03:26,139
Is it 0.5 for the first two points? Is it point six as a .01?

34
00:03:26,140 --> 00:03:27,220
What is the correlation?

35
00:03:28,810 --> 00:03:35,050
So there's no point in using sandwich standard errors if you want to know what the correlation is because you have to model it to get an estimate.

36
00:03:36,520 --> 00:03:42,580
If you think your model is correct for the correlation structure, then you may as well use the model base standard errors.

37
00:03:43,060 --> 00:03:52,900
If you've used a model, you may as will continue with it. I like to fix a sandwich standard errors after the model based because the sandwich

38
00:03:52,900 --> 00:03:58,450
generators again tell me any residual correlation that wasn't accounted for in my model.

39
00:03:59,050 --> 00:04:07,360
So if I assumed a compound symmetric and the standard errors between the model and the sandwich method if they're very different from each other,

40
00:04:07,510 --> 00:04:11,620
that means the residuals are telling me that compound symmetry didn't cover all the correlation.

41
00:04:12,130 --> 00:04:15,870
There's more there. So I use it more as a check.

42
00:04:16,260 --> 00:04:21,390
Is compound some symmetry doing enough to account for the correlation that I like those standard errors?

43
00:04:21,690 --> 00:04:28,679
So it's more for me an internal check. Please do not report both because I guarantee you folks are going to pick the one

44
00:04:28,680 --> 00:04:32,870
that smaller because it makes their t statistic bigger and the p value smaller.

45
00:04:32,910 --> 00:04:40,500
Right. People are going for smaller p values. So is there a drawback?

46
00:04:41,160 --> 00:04:45,930
The only drawback I can think of for a sandwich standard error is that it's a large sample concept.

47
00:04:46,350 --> 00:04:50,940
We are trying to estimate a matrix with a set of data.

48
00:04:51,720 --> 00:04:59,380
If you have five observations per person and you only have 20 people, can you really estimate a five by five matrix using the residuals?

49
00:04:59,400 --> 00:05:06,150
Probably not. It's really, really noisy. So that's the only drawback I can see is in small samples.

50
00:05:06,150 --> 00:05:14,070
In small samples, you need a model, folks. And small samples models are useful because we don't have enough data to just be free with everything.

51
00:05:14,550 --> 00:05:16,830
Right. So that's the only time.

52
00:05:16,830 --> 00:05:23,430
I think there's a drawback when you don't have enough data to use the residuals to estimate what might be a fairly large matrix.

53
00:05:26,310 --> 00:05:35,610
When will we expect a sandwich? Standard variances to be smaller versus larger than the model based ones depends on what's supposed to happen.

54
00:05:37,240 --> 00:05:42,399
And let's go back to my simulation, coach. We love simulations at this point, so let's see.

55
00:05:42,400 --> 00:05:50,340
This sucks. So I have the simulation code for simulate a normal glass.

56
00:05:50,340 --> 00:05:58,350
So we're going to simulate normal outcomes. We're going to allow either compound symmetry or auto aggressive one correlation structure.

57
00:05:58,360 --> 00:06:01,900
We're going to simulate data to have correlation.

58
00:06:02,770 --> 00:06:09,370
And then we saw what happened when we ignored the correlation. And so to refresh your memory on this code, again,

59
00:06:09,370 --> 00:06:15,640
all I am is again specifying a bunch of correlation parameter one correlation parameter arrows point six.

60
00:06:16,180 --> 00:06:21,639
I said everybody has a compound symmetry for their own observations with a certain

61
00:06:21,640 --> 00:06:26,380
group and time structure for the fixed effects that simulates the data for both groups.

62
00:06:28,630 --> 00:06:35,290
And then I went ahead and set ordinarily squares. So ignoring the correlation, I got a sandwich standard error for that.

63
00:06:36,430 --> 00:06:41,200
I then fit compound symmetry, so I'm going to get the model wrong.

64
00:06:41,880 --> 00:06:44,770
I assuming independence, I'm going to get the model right.

65
00:06:46,420 --> 00:06:50,890
But I'm still going to finish a sandwich, Senator, with the model that has the right senators.

66
00:06:52,240 --> 00:06:59,620
And then I just put it on to a result which I didn't have last time, but I'm going to take results for oil and glass and put them together.

67
00:07:00,850 --> 00:07:07,659
So that's the way this works. So I ran all that code. Now, again, this is one simulation.

68
00:07:07,660 --> 00:07:12,080
This is an across many, many simulations. So things are always a little bit funky from simulation.

69
00:07:12,080 --> 00:07:15,760
A simulation, but we're going to call it my RAZ.

70
00:07:16,090 --> 00:07:19,120
That's the complete, complete data. That's the company. It's.

71
00:07:23,350 --> 00:07:31,070
That's right. The top one is oil.

72
00:07:33,510 --> 00:07:38,579
So the top one is all else. There in my estimates, there are the senators.

73
00:07:38,580 --> 00:07:42,150
I know they're wrong. They don't account for the correlation.

74
00:07:42,630 --> 00:07:47,970
So the sandwich senators are using the residuals to tell me if there's correlation needs to be in the standard terms.

75
00:07:49,140 --> 00:07:53,790
So to answer that question, we can see again, as I said, time is a within person comparison.

76
00:07:55,840 --> 00:07:58,750
By not accounting for the correlation. My senator is too big.

77
00:08:00,010 --> 00:08:04,180
And so the sandwich standard error accounts for the correlation and brings it back down to where it should be.

78
00:08:05,440 --> 00:08:09,580
So the question of when will sandwich sandwich standards be bigger or smaller?

79
00:08:09,610 --> 00:08:12,790
It depends on what's your fixing, right?

80
00:08:12,820 --> 00:08:20,140
Is the answer you have right now in the wrong direction? So I can't tell you whether sandwich centers will be bigger than or smaller this year.

81
00:08:20,590 --> 00:08:23,980
Model based. It depends on what the model is and what the data are.

82
00:08:24,880 --> 00:08:28,480
Again, the group effect. We see a standard error of about 0.5.

83
00:08:29,050 --> 00:08:33,490
I shouldn't say that. Five, four, eight. And the sandwich says .58.

84
00:08:34,120 --> 00:08:40,540
So the sandwich standard error is bigger because again, group is a between person comparison.

85
00:08:40,540 --> 00:08:44,920
The standard fare from the model is too small when I ignore the correlation.

86
00:08:46,840 --> 00:08:50,650
Conversely, in the GLC model, I have modeled everything correctly.

87
00:08:52,550 --> 00:08:59,270
So there are the standard errors from the model. Those standard errors should be close to the right answer because I got the model right.

88
00:08:59,930 --> 00:09:03,800
So the sandwich standard errors again with sampling variability here.

89
00:09:04,080 --> 00:09:11,840
They're darn close to the model based because it's telling me they're telling me the residuals have no correlation left.

90
00:09:12,110 --> 00:09:16,970
You've already accounted for it in your model. So that's why I can use those two columns as sort of a check.

91
00:09:17,850 --> 00:09:21,320
Okay. So they should be similar now because I got the model right.

92
00:09:26,400 --> 00:09:29,700
Two questions about sandwich senators. Again, Juncker will warned me.

93
00:09:29,920 --> 00:09:32,220
He said, You don't want to show this so early in the semester,

94
00:09:33,180 --> 00:09:41,129
but here I am doing it because we're going to see it again in about three weeks when we get to the bus and sandwich.

95
00:09:41,130 --> 00:09:46,290
Senators are just a really nice way to use residuals to check on the correlation.

96
00:09:46,500 --> 00:09:50,070
So you've never model correlation before, and this is the first time you've done this in regression,

97
00:09:50,850 --> 00:09:58,230
so it's just a nice extra little fix sandwich standard errors can be used in linear regression.

98
00:09:58,300 --> 00:10:00,000
Again, remember, there's weight in these squares.

99
00:10:01,000 --> 00:10:05,940
Now when you did those residual plots, you want to see if there was variability in some of the plots.

100
00:10:05,940 --> 00:10:10,410
There was a concern over time rate, sandwich, standard errors and to take care of that.

101
00:10:11,610 --> 00:10:19,439
So again, it's just trying to use the residuals to get a better handle on the variability so that the standard errors are correct,

102
00:10:19,440 --> 00:10:27,400
either larger or smaller. So use them as you wish.

103
00:10:27,410 --> 00:10:33,040
I. I like sandwich standard senators because all I needed to learn was linear regression.

104
00:10:33,440 --> 00:10:39,310
And now I've got a sandwich estimate for the standard errors I don't need to worry about jealous jiggles.

105
00:10:39,320 --> 00:10:45,799
It's just an extra step for me. But again, if I want to estimate the correlation, I should model an estimated in the conference.

106
00:10:45,800 --> 00:10:53,780
So. So maybe you've noticed right now that the estimates for OS,

107
00:10:55,010 --> 00:11:03,050
the estimates for GLAAD are exactly the same sandwich standard errors for all of us and Germans are exactly the same.

108
00:11:04,170 --> 00:11:10,980
And that isn't an error. So we're going to talk about that shortly, too, because some of you have brought this up to me in class and I haven't.

109
00:11:11,730 --> 00:11:17,100
This is a peculiar thing about jails that I have spent hours sifting through the Internet,

110
00:11:18,000 --> 00:11:24,810
and the only people who have tackle this problem are very mathematical people who want to work in matrix spaces that I don't want to be in.

111
00:11:26,310 --> 00:11:32,870
So I haven't found a nice longitudinal data based person to explain the theory.

112
00:11:32,880 --> 00:11:37,290
But anyway, so it's not a fluke here that those estimates are the same for both approaches.

113
00:11:39,690 --> 00:11:47,390
It's a very unique feature of the data. But before we get there, there was one other question.

114
00:11:48,720 --> 00:11:51,600
Someone said, I'd like to see an example of a matrix form for jealousy,

115
00:11:51,610 --> 00:11:56,040
especially when individuals have different dimensions for their design matrices.

116
00:11:56,310 --> 00:12:06,030
All right. So instead of being hypothetical, I thought I would use one of the datasets that some of you wrote to analyze that same problem.

117
00:12:08,680 --> 00:12:14,400
And that's what's not that dataset. All right.

118
00:12:14,850 --> 00:12:19,260
So those of you who have the cholesterol dataset, I'm going to just use this as an example here.

119
00:12:19,560 --> 00:12:26,520
So, again, we had folks who were either given a placebo, a SIBO or this drug called cannabidiol.

120
00:12:27,600 --> 00:12:32,069
They were measured and we made a treatment.

121
00:12:32,070 --> 00:12:35,340
So baseline six, 12, 20 and 24 months of follow up.

122
00:12:37,140 --> 00:12:40,530
And so when we looked at the means over time and the two groups,

123
00:12:41,340 --> 00:12:46,170
I picked this dataset mostly because things are linear, some injury time is a continuous variable here.

124
00:12:46,440 --> 00:12:49,650
Instead of trying to get a bunch of dummy variables with categories.

125
00:12:49,830 --> 00:13:00,899
So metric time is continuous. I've got two groups. The other reason I want to use these data is because here is the data.

126
00:13:00,900 --> 00:13:04,950
Here are the data. A Lots of people have all five measurements.

127
00:13:05,370 --> 00:13:09,690
But then you start to see individuals. This individual is missing their last measurement, this residuals.

128
00:13:10,020 --> 00:13:12,630
So we have people with varying numbers of observations.

129
00:13:13,140 --> 00:13:22,080
And more importantly, as we get down here, there are people of lots and there's actually a few folks who come in stereo.

130
00:13:22,440 --> 00:13:27,660
We have people who have a missing middle observation, and then that's very uncommon in longitudinal studies.

131
00:13:28,110 --> 00:13:34,260
Usually it's because people leave the study and we can't get any more data on them. This is probably some lab error, right?

132
00:13:34,320 --> 00:13:40,810
The measurement, maybe the sample got lost or something, but so differing amounts of measurements on every individual.

133
00:13:40,830 --> 00:13:44,670
Let's use these data to investigate some of the metrics.

134
00:13:46,900 --> 00:13:54,130
Again, matrix algebra is something that you just have to keep doing and it gets more comfortable over time.

135
00:13:54,760 --> 00:13:59,160
Or is my whiteboards? Oh, yes, sir.

136
00:13:59,200 --> 00:14:02,600
You opened it. Everybody got it.

137
00:14:03,430 --> 00:14:08,680
So let's let's try. And and this is something I haven't done very well in this class.

138
00:14:08,680 --> 00:14:10,990
And I'm realizing it as I'm giving you the homework assignments.

139
00:14:12,900 --> 00:14:20,110
Any time you fit a model in our data, you should be able to write out that model on a piece of paper with betas and access lines and so forth.

140
00:14:21,320 --> 00:14:28,070
Think about the model that you're fitting to the data, because I can guarantee you sometimes you end up realizing it's not the model you want.

141
00:14:28,220 --> 00:14:35,210
I have done this. So I probably should have made you write out every model that you were sitting in these homework assignments.

142
00:14:35,250 --> 00:14:40,460
I suppose I still could. All right.

143
00:14:41,150 --> 00:14:47,630
So the cholesterol dataset. So I'm going to say right now, why is that same old notation?

144
00:14:48,140 --> 00:14:57,020
It's going to be cholesterol. For.

145
00:14:58,910 --> 00:15:06,410
So cute. I. I don't want to say time.

146
00:15:11,670 --> 00:15:16,040
This visit is. One, two, three, four, five.

147
00:15:19,070 --> 00:15:37,590
I j his mum. Only noon here again.

148
00:15:39,060 --> 00:15:42,090
She is going to be a member of.

149
00:15:43,110 --> 00:15:51,800
One, two, three, four, five. Everybody has a group membership and.

150
00:15:56,890 --> 00:16:03,620
No. Read on or check back later when I hit reload.

151
00:16:07,610 --> 00:16:11,540
I don't want to know our age group of subjects.

152
00:16:13,610 --> 00:16:16,850
And so I decided to say that zero is going to be a placebo group.

153
00:16:17,450 --> 00:16:20,840
So that's the reference. And what is the treatment group?

154
00:16:21,050 --> 00:16:24,380
We're going to Canada, I hope. Canada. Canada and.

155
00:16:26,830 --> 00:16:32,740
And we have a model here. We believe that someone's cholesterol measurement it and there's a G.

156
00:16:36,640 --> 00:16:41,860
It's really right. Just get rid of me. Is an intercept.

157
00:16:43,070 --> 00:16:47,570
Plus an effective group. That's an effective time.

158
00:16:50,370 --> 00:16:56,800
Plus possible interaction. And again, we're going to have.

159
00:16:58,440 --> 00:17:02,610
We have the the main structure and then we have noise around.

160
00:17:02,610 --> 00:17:09,440
That means from person to person that I think time for us to.

161
00:17:14,480 --> 00:17:20,450
We're going to start with. Unfortunately, the AIDS are nested within groups here, and this is the one drawback of this dataset to use in class.

162
00:17:21,590 --> 00:17:26,570
There's a subject one in group two and there's a subject one in groups or whichever one.

163
00:17:26,900 --> 00:17:33,710
But I'm going to focus on the first person in group one. They have five measurements, so everything has Dimension five here.

164
00:17:33,920 --> 00:17:37,100
So again, I just want to emphasize that we're talking about that person.

165
00:17:39,770 --> 00:17:44,180
Keep scrolling to match notes.

166
00:17:44,310 --> 00:17:51,990
All right. So for grade one in group one.

167
00:17:55,510 --> 00:18:00,050
You know that we have a vector of observations, so it's harder to do a PowerPoint.

168
00:18:00,070 --> 00:18:04,710
I always a vector is one tilde underneath and a matrix is always to tell the hundreds.

169
00:18:06,570 --> 00:18:10,630
So I can see my writing. So again, they have y11, one, two.

170
00:18:10,680 --> 00:18:16,110
These are all cholesterol measurements. Five transpose.

171
00:18:18,490 --> 00:18:24,240
There is. If there is an X matrix for this person.

172
00:18:26,600 --> 00:18:31,090
Can I have five measurements each of them as it is modeled with an intercept.

173
00:18:31,100 --> 00:18:36,240
That's better not. This is the next variable is group.

174
00:18:37,140 --> 00:18:44,370
This is group one. So that was group blessing. So they had zeros at the time.

175
00:18:45,000 --> 00:18:53,670
The baseline is zero. Then we were at six, 12, 20 and 24 and treating time is continuous here in this model.

176
00:18:55,080 --> 00:19:00,989
And then I have an interaction group times time since group is zero there is are all zeros.

177
00:19:00,990 --> 00:19:08,639
There are two. There is a vector of residuals somewhere to copy.

178
00:19:08,640 --> 00:19:12,330
I want to build up to be 1112.

179
00:19:12,500 --> 00:19:19,530
We try to stay real. The here. The one for the new ones like transformers.

180
00:19:21,980 --> 00:19:26,490
You think that this is these are normally distributed with a mean of five zeros.

181
00:19:27,770 --> 00:19:34,820
It's a vector. And there is a correlation matrix, which we subscript by our very sort of various covariance matrix.

182
00:19:35,510 --> 00:19:42,050
Again, that is a one that I. Each person has their own matrix.

183
00:19:42,890 --> 00:19:49,430
The structure is the same across, folks. It's just the number of observations, whether it's four by four or five by five or three by three.

184
00:19:49,970 --> 00:19:56,570
That's why we subscript it here. And so in this case, because I have all five observations,

185
00:19:57,860 --> 00:20:08,830
I am going to say that if I want to sit in on a aggressive correlation structure and I have one row row squared to the fourth strike.

186
00:20:09,740 --> 00:20:13,970
The first observation has this much correlation with all future observations.

187
00:20:15,860 --> 00:20:20,000
That's row and one square in a row cubed.

188
00:20:21,070 --> 00:20:25,240
Square. No one can do this role.

189
00:20:26,430 --> 00:20:32,680
The square is still standing. Huge square roll.

190
00:20:37,290 --> 00:20:41,220
Again, this is very monotonous but important.

191
00:20:43,350 --> 00:20:48,570
Right. So that's an auto regressive religious instruction,

192
00:20:48,580 --> 00:20:52,960
if that's what it would effectively make telling you that's what you're supposed to do with the cluster.

193
00:20:53,000 --> 00:21:02,690
That is that. I'm just saying if you wanted to do that. Right. If I go to ID number 37 and Group one.

194
00:21:05,140 --> 00:21:10,930
So person number 37 only has the first two observations and they have three missing observations.

195
00:21:13,150 --> 00:21:20,010
Don't worry about. So for 37.

196
00:21:22,890 --> 00:21:28,060
One. Can we have a vector of observations?

197
00:21:30,010 --> 00:21:33,460
I was going to say eight 3737.

198
00:21:35,620 --> 00:21:41,170
All right. I'm going to fly a 37 137 to.

199
00:21:42,040 --> 00:21:48,160
And the other three observations. There is an X matrix.

200
00:21:49,690 --> 00:21:53,049
Again, I have two observations. There is an intercept there.

201
00:21:53,050 --> 00:21:58,660
In the first group, there is a reference. I have time zero and time six.

202
00:21:59,680 --> 00:22:12,220
And again, the interaction is all zeros. So again, the overall fit is taking all of these X matrices and putting them on top of each other.

203
00:22:13,660 --> 00:22:17,480
So as long as I have four columns, I can put these on top of each other.

204
00:22:17,500 --> 00:22:27,510
They don't have to have the same number of rows. And likewise this personal assurance voters for being picky there.

205
00:22:28,390 --> 00:22:35,730
And this person is a vector of errors. It's error 37 one and error 37 to.

206
00:22:37,060 --> 00:22:43,810
Suppose we believe that's normally distributed again with a mean vector of two zeros and.

207
00:22:46,880 --> 00:22:55,480
And a variance covariance matrix which are subscript by 37 because in this case the sigma matrix is not five by five.

208
00:22:56,690 --> 00:23:02,690
It's two by two and sigma squared once here.

209
00:23:03,560 --> 00:23:06,830
And the two observations are one visit a part.

210
00:23:07,820 --> 00:23:12,110
So that's grow and grow according to an auto regressive model. It looks like compound symmetry, right?

211
00:23:12,530 --> 00:23:15,650
But it is also aggressive when you only have two observations.

212
00:23:20,590 --> 00:23:30,579
So again, when I have this overall big sigma matrix for all of the data, remember I said that this is Sigma one and Sigma two, Sigma three.

213
00:23:30,580 --> 00:23:35,170
That is all the way through Sigma. And there are zeros everywhere here.

214
00:23:37,500 --> 00:23:42,380
Does it matter that they're not the same dimension? It all fits together when you put them together.

215
00:23:44,940 --> 00:23:50,350
And that just to make this abundantly clear. Let's talk about one person who is missing something in the middle.

216
00:23:50,890 --> 00:23:58,510
So as I pointed out, we're going to talk about this at 64.

217
00:23:58,960 --> 00:24:04,240
So a second row from the bottom. They had the first two measurements and then they had the last measurement.

218
00:24:04,540 --> 00:24:10,540
So here we've got three observations. But again, we have to worry about the order they're in in order to do this correctly.

219
00:24:10,550 --> 00:24:18,010
But the computer, again, if you tell the computer which visitors for which person, at what time, point it knows to do all this for you.

220
00:24:20,410 --> 00:24:24,340
So for looks 64, 64,

221
00:24:24,340 --> 00:24:39,120
and the second group to that person is a secretary of observations by 60 416342 and y 64

222
00:24:39,130 --> 00:24:44,350
five the fifth one for the third and the fourth are going to leave a space for them.

223
00:24:44,350 --> 00:24:51,160
We don't say anything. We should just put in what we have to a set of residuals.

224
00:24:51,370 --> 00:24:54,790
Oops. No, before we do that, we have a design matrix.

225
00:24:57,530 --> 00:25:02,300
64 is a matrix intercept for all three observations.

226
00:25:03,770 --> 00:25:08,270
Now in the other group. So they have a one for all the group membership.

227
00:25:09,350 --> 00:25:16,130
We have zero for the first measurement. Second measurement was at six and the last measurement was at 24.

228
00:25:19,210 --> 00:25:22,510
And then, of course, the interaction is zero 624.

229
00:25:25,460 --> 00:25:29,000
We have a vector of residuals. Groups.

230
00:25:30,160 --> 00:25:36,520
She is 60 416242624.

231
00:25:40,550 --> 00:25:49,460
So again, we believe has a normal distribution centered around zero vector of three zeros and a variance covariance matrix.

232
00:25:49,470 --> 00:26:00,640
I'll call sigma 64. Here is sigma 64 sigma squared times an auto regressive matrix once along the diagonals.

233
00:26:01,150 --> 00:26:06,730
So the first two observations are one visit apart. So they have a correlation of row to the one.

234
00:26:08,290 --> 00:26:16,120
The first and the fifth observation are four visits apart. So this is road of the fourth force.

235
00:26:16,780 --> 00:26:20,960
And the second and the fifth observation are three measures, the first three visits apartments.

236
00:26:20,980 --> 00:26:29,410
So that's cute. And again, I can put that in this overall sigma matrix and everything is estimated correct.

237
00:26:34,210 --> 00:26:41,950
Yes. A simple question, but are the sigma squared and the role is the same across people?

238
00:26:42,070 --> 00:26:49,270
They are. Okay. So if my if my neighboring observations have a correlation of point eight, so do yours.

239
00:26:49,570 --> 00:26:53,320
So do yours. Yes. It is not subject specific.

240
00:26:56,020 --> 00:26:59,230
If you want subjects, you can't even do that.

241
00:27:01,030 --> 00:27:05,350
Randomize X models. Again, their goals is pretty restrictive.

242
00:27:06,970 --> 00:27:10,360
Yes. There are things you can do.

243
00:27:10,360 --> 00:27:14,620
For instance, I don't know if you can get to the level of everybody's own individual correlation,

244
00:27:15,250 --> 00:27:18,620
but you could say that the two groups have different correlation coefficients.

245
00:27:19,330 --> 00:27:24,400
Maybe the gender group has more or less correlation than folks in the placebo group.

246
00:27:25,030 --> 00:27:29,230
So I could have a row, one in a row, two with a lot of aggressive structure on each.

247
00:27:29,980 --> 00:27:35,650
You probably could fit compound symmetry for the placebo group and auto regressive for the other groups.

248
00:27:36,340 --> 00:27:40,840
If you really want to want to have a lot of fun with programing, you can do all that.

249
00:27:41,650 --> 00:27:50,890
But again, rarely is that of interest. There is no need to get to that level of precision unless that is specifically what you want to estimate.

250
00:27:52,220 --> 00:27:57,880
Okay. So I don't mean to say again that modeling correlation is a waste of time.

251
00:27:58,660 --> 00:28:06,900
It is useful, but don't go down a rabbit hole of 10,000 different correlation structures trying to find the one that has the smallest AIC.

252
00:28:08,320 --> 00:28:11,889
You're never going to find the right models. You just want to find a good one.

253
00:28:11,890 --> 00:28:17,140
Right. Other questions on the matrix algebra.

254
00:28:19,800 --> 00:28:23,190
You don't have to do you don't have to do this on the test. But yes, sir.

255
00:28:23,460 --> 00:28:28,320
So the time point or like the time points are like it's one through five, right?

256
00:28:28,440 --> 00:28:35,610
And like the second there's. So between the second and third, there's like the correlation is like a factor of row, right?

257
00:28:35,910 --> 00:28:40,090
But it's on like the second and third are like six months apart.

258
00:28:40,090 --> 00:28:44,490
The third and the fourth are eight months apart. Does that matter? So does that matter?

259
00:28:45,560 --> 00:28:52,070
So his question has to do whether I use a standard air one in which visit is the time and the exponent.

260
00:28:53,280 --> 00:28:57,330
Or a continuous air model where months is the distance.

261
00:28:58,200 --> 00:29:02,220
So does it matter? Perhaps. Right. I don't know what the right correlation structure is.

262
00:29:02,820 --> 00:29:10,920
So instead of instead of again, the exponent is based upon the difference in the visit numbers.

263
00:29:11,430 --> 00:29:17,940
And like you said, well, visits that are one visit a part, there's differing time amounts between those visits.

264
00:29:18,570 --> 00:29:21,840
So I could have said, well, this is these are six months apart.

265
00:29:23,850 --> 00:29:28,770
And boy, what is the first versus the last was a 24.

266
00:29:30,160 --> 00:29:34,510
Yes. I think this would be this could be 24. It's going to be 24.

267
00:29:35,110 --> 00:29:39,880
And then these two visits were. Six versus 24.

268
00:29:39,970 --> 00:29:49,030
So 18. So I could have a model like that in which the exponent is based upon the time difference rather than the visit difference.

269
00:29:50,330 --> 00:29:56,720
I don't know which one's right. You could fit both of them. Look at AC and see which one maybe fits the data better.

270
00:29:57,380 --> 00:29:59,480
All right, so again, it's up to you to decide.

271
00:29:59,510 --> 00:30:05,570
Do you think correlation decays so much that you have to write about the actual time or is the visit sufficient?

272
00:30:06,080 --> 00:30:08,090
This is a this is a huge amount of decay.

273
00:30:09,150 --> 00:30:16,890
I can guarantee you about 24 months that I'm four zero, whereas before it was road to the fourth, there's still going to be some correlation there.

274
00:30:17,370 --> 00:30:20,879
So. Depends on what the truth is.

275
00:30:20,880 --> 00:30:36,500
And you never know the truth. I don't. If matrix algebra is not your friend, at least make it someone something you want to be around once in a while.

276
00:30:37,610 --> 00:30:44,840
We're going to do a lot more of it with with random effects models, and there's going to be some extra layer of matrices going on.

277
00:30:46,220 --> 00:30:52,490
So, again, I could never ask you to derive stuff, but I do want you comfortable with the modeling.

278
00:30:53,330 --> 00:30:59,990
It's a lot easier to do this stuff with with matrices than it is with y j for each one of them.

279
00:31:02,630 --> 00:31:09,380
All right. So I think I answered all the questions squared.

280
00:31:09,830 --> 00:31:16,390
All right. So let's go to this fascinating results that I showed you in the simulations.

281
00:31:19,060 --> 00:31:28,280
Our the economy has stopped. That's right.

282
00:31:28,370 --> 00:31:39,140
So I simulated that down with the simulated 100 participants 50 and each group five time points for every person.

283
00:31:39,830 --> 00:31:44,300
Every person has all of their measurements for a certain mean structure.

284
00:31:45,080 --> 00:31:48,560
I'm going to say compound symmetry with a correlation coefficient of six.

285
00:31:49,340 --> 00:31:51,920
So I draw the area anyways and I simulate the data.

286
00:31:52,580 --> 00:32:03,140
And again, once I did all that in one simulation, once I took the results from fitting again, the data are truly correlated.

287
00:32:04,050 --> 00:32:10,070
I. I. It ordinarily squares ignoring correlation and I use a glass model with compound symmetry in it.

288
00:32:11,450 --> 00:32:16,489
I'm going to go down that again.

289
00:32:16,490 --> 00:32:22,190
Boyle's sandwich sanitaire from their model generally a Leslie squares with.

290
00:32:27,280 --> 00:32:33,160
With compound symmetry. And then I threw a Sanders senator on top of that as well.

291
00:32:40,930 --> 00:32:48,820
And this is what I got. So I have a whale estimates, an ambulance estimates.

292
00:32:48,870 --> 00:32:55,420
Now, as I said, in class over and over. But Willis and Grace are both unbiased.

293
00:32:55,810 --> 00:32:59,110
They both have the same needs. They both have mine data.

294
00:32:59,770 --> 00:33:09,010
It doesn't mean they're equal. And bias is this theoretic concept that makes us feel good that we're doing something right.

295
00:33:10,900 --> 00:33:12,190
On average. Right.

296
00:33:12,430 --> 00:33:19,320
But from study to study, depending upon how much variability there is in the data, you might be far from from beta or you might be close to better.

297
00:33:21,610 --> 00:33:25,120
In this case, we get exactly the same answers.

298
00:33:25,930 --> 00:33:32,250
I thought I didn't. I got the same coefficient estimates, different standard errors in the same sandwich servers.

299
00:33:34,360 --> 00:33:40,090
And so this is a unique design aspect. I am not telling you that OS and glass are the same all the time.

300
00:33:42,190 --> 00:33:49,000
They are the same. Probably should write this down, but not right now.

301
00:34:10,570 --> 00:34:23,250
Your folks. Okay.

302
00:34:23,400 --> 00:34:30,840
Here we go. So I said that the expected value of some regression parameters from all elements

303
00:34:32,100 --> 00:34:37,370
is equal to the expected value of the event from the generalized squares.

304
00:34:37,380 --> 00:34:45,520
And I said that both of those are beta. So again, let's.

305
00:34:47,510 --> 00:34:53,930
X transpose x inverse x, transpose y and be in a hat.

306
00:34:54,110 --> 00:35:04,360
She tells x transpose sigma inverse x inverse x transpose sigma inverse watch.

307
00:35:06,500 --> 00:35:12,230
So the question is, is, is it a hat policy?

308
00:35:13,420 --> 00:35:15,310
Equivalent to being ahead.

309
00:35:15,510 --> 00:35:24,970
She knows, as I said, I have sifted through the Internet for more hours than I probably should have to find a nice answer for this.

310
00:35:26,140 --> 00:35:31,930
And so looking at the two matrix equations, of course, the natural response is, well, stupid.

311
00:35:31,930 --> 00:35:37,000
When Sigma Z identity wrecked the Sigma Z identity, it drops out and the two things are the same.

312
00:35:37,300 --> 00:35:51,640
So yes, but that's independence. So when are those two estimates the same?

313
00:35:51,880 --> 00:35:55,540
When the correlation matrix or the matrix is not the identity.

314
00:35:55,840 --> 00:36:02,660
Again, Times Sigma squared. And the answer is answer.

315
00:36:08,450 --> 00:36:13,220
If Sigma. This kind of symmetric.

316
00:36:18,050 --> 00:36:23,940
And. Stigma has the same dimension.

317
00:36:28,790 --> 00:36:35,630
We're all subjects. Which I have always called balanced.

318
00:36:41,740 --> 00:36:45,760
So if you have balanced data, Vauxhall shall refer to this as a panel study.

319
00:36:46,090 --> 00:36:53,860
A panel study is one in which people are specifically measured every census, every month, and everybody gives us all their data.

320
00:36:54,730 --> 00:36:58,930
So if you have all the observations for every individual.

321
00:37:01,060 --> 00:37:08,210
It doesn't matter if you use council on cemetery or not, you'll get the same answer for using independence.

322
00:37:08,260 --> 00:37:12,700
Remember, an independence matrix is compound symmetric which just got zero.

323
00:37:13,180 --> 00:37:15,820
Instead of some value for row after day.

324
00:37:17,380 --> 00:37:23,650
So in a situation in which you have all the observations and some of you have these datasets and maybe you noticed this and maybe you didn't,

325
00:37:24,490 --> 00:37:32,560
but if you fit A.L.S. model to the data and you set compound symmetry to the data, you should have gotten the same coefficients.

326
00:37:33,920 --> 00:37:39,470
You also get the same stand with senators. And again, I'm not going to get into that in this class, but.

327
00:37:40,630 --> 00:37:47,770
An intuitive way to think about this cause again, I can't find a nice there is a manuscript on canvas that I loaded for everybody.

328
00:37:47,980 --> 00:37:52,720
If you want to read one person's attempt to explain this to us, it's there.

329
00:37:54,760 --> 00:37:59,860
It's a little challenging, but it requires some theorems that I didn't even learn when I was in grad school.

330
00:37:59,860 --> 00:38:05,409
But they are out there in the world. Please don't ever.

331
00:38:05,410 --> 00:38:08,470
If you want to offer this to yourself, please don't do the brute force algebra.

332
00:38:08,740 --> 00:38:13,240
Don't make a compound symmetric matrix and do those two matrix matrix computations.

333
00:38:13,810 --> 00:38:17,890
I made that as an extra credit question one year from my class.

334
00:38:18,610 --> 00:38:22,989
It took them hours. It took them hours. And I did not mean for that to happen.

335
00:38:22,990 --> 00:38:26,050
So please don't do the brute force algebra. That is not the way to prove this.

336
00:38:27,040 --> 00:38:31,600
There has to be some concepts about symmetric matrices and and so forth matrix basis.

337
00:38:32,110 --> 00:38:38,880
Anyway. It doesn't matter what the true correlation structure is.

338
00:38:41,150 --> 00:38:46,670
I'm not saying if the data are compound symmetric, this is true. It doesn't matter what the correlation structure is.

339
00:38:47,420 --> 00:38:52,310
If you have an equal number of observations for every person and you set a compound symmetric matrix,

340
00:38:52,910 --> 00:38:57,290
you'll get the same coefficient estimates as you would assuming independence.

341
00:39:00,170 --> 00:39:01,850
Let's go back against simulating air.

342
00:39:04,100 --> 00:39:11,419
If I simulate my data and have an error one structure so wrong I am going to say compound symmetry and independence.

343
00:39:11,420 --> 00:39:14,900
So both models are wrong. They are explanation like one.

344
00:39:15,890 --> 00:39:19,310
So the truth is that truth in the data is an error one structure.

345
00:39:19,610 --> 00:39:23,060
I'm going to sit well as in glass with compound symmetry.

346
00:39:33,770 --> 00:39:42,559
Right. And I still get identical answers with the two models with different standard errors,

347
00:39:42,560 --> 00:39:46,910
though the standard errors are a function of the correlation structure that you assume.

348
00:39:49,340 --> 00:39:52,040
But then the same with senators, because you have panel data,

349
00:39:52,940 --> 00:39:57,200
the sandwich standard errors are able to balance everything else and you get back to the same answers.

350
00:40:05,240 --> 00:40:11,090
Each time we get to what else I want to talk about today.

351
00:40:13,890 --> 00:40:18,520
All right. This is fun algebra, but I know it's not so much fun anymore.

352
00:40:18,680 --> 00:40:22,090
I think I'm gonna pass on it. Um.

353
00:40:24,400 --> 00:40:29,490
You can set up some people. Can you just set up the problem for you?

354
00:40:29,710 --> 00:40:39,250
Again, a few folks want to try a little bit of algebra. In a very simple setting to show yourself that the answer relates to what I just told you.

355
00:40:39,420 --> 00:40:44,940
You really do. So if you have.

356
00:40:44,940 --> 00:40:49,040
Why I j. Is an intercept only model.

357
00:40:51,860 --> 00:40:57,230
So you're just estimating an overall mean greater, not just an overall mean across all observations.

358
00:40:58,790 --> 00:41:02,830
Mm hmm. It was 1 to 2.

359
00:41:02,860 --> 00:41:06,550
And let's just take two observations.

360
00:41:10,740 --> 00:41:19,400
Religion. Why the right of a prime is rule so that everybody has the correlation matrix.

361
00:41:19,410 --> 00:41:23,940
Again, it looks like compound symmetry, but there's nothing else you can do with two observations.

362
00:41:24,110 --> 00:41:34,760
It can't be anything else. What is the lowest estimate?

363
00:41:37,790 --> 00:41:45,110
But we do know. And what is the estimate of or not?

364
00:41:48,450 --> 00:41:56,870
Stems from comments in which a large. And you should get the same better hand, right?

365
00:41:57,030 --> 00:42:07,610
So if you want to try that and ask me later, you can you don't need a lot of matrix down.

366
00:42:07,860 --> 00:42:14,230
Matrix algebra leads you to what you know the answer is. The answer is, is lots of them.

367
00:42:15,820 --> 00:42:20,650
Two observations per person. There are nine people. There's a total of two and observations.

368
00:42:21,280 --> 00:42:24,580
I'm just going to take the overall mean of all the observations.

369
00:42:25,870 --> 00:42:31,150
This one and equals one or two. Right now the hat is just the mean.

370
00:42:31,660 --> 00:42:39,219
Everything. But you're going to get that answer verbatim, not hacked no matter what company is used to map out cemeteries.

371
00:42:39,220 --> 00:42:45,070
So see if you can give that answer if you want to try it. And I have a little bit of the matrix pregnant again.

372
00:42:45,070 --> 00:42:49,720
You're going to have to invert Sigma. Right. That's why I picked only two observations.

373
00:42:50,410 --> 00:42:57,840
I don't really want to invert a three by three matrix anymore in my life. There are formulas out there for the inverse of a symmetric matrix.

374
00:42:57,850 --> 00:43:03,190
But anyway, so try that out and you should get the same data out at.

375
00:43:04,680 --> 00:43:08,470
For both. There's.

376
00:43:12,010 --> 00:43:17,240
Turkish. Both for a while as insurance.

377
00:43:17,870 --> 00:43:22,460
Right. It's irrelevant. Roe is irrelevant in the computation.

378
00:43:23,420 --> 00:43:27,740
Even though it showed up in the weight matrix for jails. Fallout.

379
00:43:39,540 --> 00:43:43,330
All right. Homeworks number two. Everybody has turned in over at number two.

380
00:43:45,410 --> 00:43:48,870
And if you happen to still have time on a nice guy.

381
00:43:50,460 --> 00:43:56,910
But what I want to just review quickly with everybody is to find out what you actually did.

382
00:44:02,810 --> 00:44:07,740
To do sports. I seem to not have animals.

383
00:44:14,940 --> 00:44:18,090
Oh, because I looked at it for a frame. So it's that.

384
00:44:18,600 --> 00:44:32,880
So we have these 11 datasets. Well, I want you all to do is to wake up and let's find out what correlates and structures you actually decided on.

385
00:44:33,000 --> 00:44:37,230
And I did. There we go. All right. Deficit number one.

386
00:44:37,320 --> 00:44:41,610
What did you end up using? Anybody? I don't know.

387
00:44:41,610 --> 00:44:47,740
The right answer. There isn't a right answer. Don't be afraid to share the compound to my truck.

388
00:44:48,220 --> 00:44:53,260
So someone to come down symmetric. I guess I'll leave it like this.

389
00:44:53,500 --> 00:44:57,340
Here we go. Anybody decide on air one?

390
00:45:00,310 --> 00:45:06,250
One's willing to tell me. I'm going to assume nobody did. Anybody just using dependents with a sandwich estimate or.

391
00:45:10,440 --> 00:45:13,860
There a two. What seemed to work well.

392
00:45:17,610 --> 00:45:23,189
How much room somebody is to come out symmetric. Anybody else here?

393
00:45:23,190 --> 00:45:28,750
What? I'm surprised. So far. We got two for the one that doesn't usually work with longitudinal data.

394
00:45:33,290 --> 00:45:37,070
Number three. They are one one.

395
00:45:38,000 --> 00:45:43,130
Wow. Lots more confidence there, but he's still a little bit.

396
00:45:44,540 --> 00:45:48,650
All right. Complex number four CDs for kids.

397
00:45:49,710 --> 00:45:52,780
Tom has symmetric. You already.

398
00:45:53,150 --> 00:45:56,460
Now you go near one. All right, we got some dissention. All right.

399
00:45:56,600 --> 00:46:03,290
That's 5 seconds left.

400
00:46:04,040 --> 00:46:11,420
That seems to be the overwhelming choice there. Yeah. Number six, it's not an excuse.

401
00:46:11,690 --> 00:46:17,380
Here we go. Okay. Number seven or eight are 11.

402
00:46:18,530 --> 00:46:22,610
I can take his hand right now.

403
00:46:22,700 --> 00:46:29,360
Heroin. Heroin number 9910.

404
00:46:29,920 --> 00:46:36,780
You know what I am? I'm chewing gum vocabulary.

405
00:46:37,740 --> 00:46:41,970
Some people think they are.

406
00:46:42,410 --> 00:46:45,410
Anybody ever use a sandwich ultimatum for any of their answers?

407
00:46:46,670 --> 00:46:55,670
No, because I didn't really show you how to properly do that to the controls.

408
00:46:55,940 --> 00:47:02,570
There we go. All right. So again, a lot more on symmetric than I would have thought.

409
00:47:04,160 --> 00:47:07,340
And then we got three data sets where people don't quite agree.

410
00:47:08,240 --> 00:47:17,420
So that's great, right? I'll be curious to see what you all come up with for your random effects model.

411
00:47:17,930 --> 00:47:27,860
So, compound symmetry. Can also be written as a random effects model and a version of everyone can be written as a random effects model.

412
00:47:28,580 --> 00:47:37,350
So I'll be curious for those of you that picked one, if your random effects approach users to the same rates.

413
00:47:37,610 --> 00:47:42,530
Cool. That's what I wanted to know. There was one other thing.

414
00:47:45,550 --> 00:47:48,550
I got to use 10 minutes. I got 8 minutes. All right.

415
00:47:48,970 --> 00:47:56,860
We are going to start. I guarantee you, if I don't start, the whole semester's going to go away pretty quickly here.

416
00:47:58,000 --> 00:48:04,420
I'm trying to take moving train. So let's transition at least to the beginning and I'll review it all again.

417
00:48:04,420 --> 00:48:14,600
Of course. On what we call a random effects model.

418
00:48:15,860 --> 00:48:21,409
So for homework number three, you're going to analyze these data one more time, I promise.

419
00:48:21,410 --> 00:48:28,760
It's the last time you have to look at these datasets. We're going to use a different approach for the correlation structure.

420
00:48:29,630 --> 00:48:33,290
We're going to do indirectly through what we call random effects.

421
00:48:34,910 --> 00:48:40,280
So there's that glass model. Again, we went through this earlier today with that example.

422
00:48:40,700 --> 00:48:46,100
You've got a measurement for every person and every time point you put those into a vector for each person.

423
00:48:47,120 --> 00:48:54,110
We say that this vector is a mean plus noise around each of the values of each of the means.

424
00:48:54,770 --> 00:48:58,190
And each person has a design matrix depending on what the covariance are.

425
00:48:59,330 --> 00:49:05,330
And we put correlation. The only where the only place correlation shows up is in the errors.

426
00:49:05,630 --> 00:49:09,170
And because the errors are correlated, then of course why it's correlated.

427
00:49:09,530 --> 00:49:15,560
But strictly speaking, we are modeling the correlation of the errors when we talk about compound symmetry error one of the.

428
00:49:17,380 --> 00:49:23,070
Right. And this matrix has possibly lots of correlations, right?

429
00:49:23,080 --> 00:49:27,820
The first time, the second, the first with the forest, the forests with the seventh, all kinds of different correlations.

430
00:49:28,960 --> 00:49:35,560
And that's an unstructured kind of approach. But what correlation really is, if you've never seen any correlation correlational yet,

431
00:49:35,560 --> 00:49:40,960
because you probably have it is correlation is actually a ratio of two quantities.

432
00:49:41,500 --> 00:49:48,880
It lets me talk about how much are each person's observations related to each other look like each other,

433
00:49:49,870 --> 00:49:53,380
versus how much are we spread out from each other.

434
00:49:53,770 --> 00:50:01,820
So if my values are very far apart from another person, I have high within correlation.

435
00:50:01,840 --> 00:50:05,890
My observations look like mine, but they look very distinct from somebody else.

436
00:50:07,100 --> 00:50:13,200
As the data get more and more noisy. Then I can't figure out who is all right in the data.

437
00:50:13,800 --> 00:50:18,240
And so there the correlation within is very small. There's too much noise.

438
00:50:18,240 --> 00:50:26,160
There's a lot of variability within if my observations vary a lot from them within me and when overlap with other people.

439
00:50:27,030 --> 00:50:33,540
So correlation pictorially, visually, it's like clustering.

440
00:50:34,080 --> 00:50:38,060
Do my observations tend to cluster with them, yours, with yours and so forth?

441
00:50:38,400 --> 00:50:44,100
Can I visualize where every person is in the dataset, or do they all sort of come on top of each other?

442
00:50:44,670 --> 00:50:51,390
And that's the between versus within variability. And those two components also go into correlation.

443
00:50:52,650 --> 00:51:00,990
So here are the labor pain data again, and I use this one more time, then we move on to non continuous data in labor pain.

444
00:51:01,080 --> 00:51:09,510
We've got two groups of women every 30 minutes. We had a measurement of of how much pain she was experiencing during labor.

445
00:51:09,840 --> 00:51:15,930
And we have, you know, roughly 43 and 40, about half and half who get a treatment of their control.

446
00:51:16,530 --> 00:51:19,530
And we have two covariates, which we really haven't used up to this point.

447
00:51:21,030 --> 00:51:25,490
And this is where spaghetti plots have some utility and you might want to use them in homework.

448
00:51:25,500 --> 00:51:31,710
Number three, if you didn't use them yet again, we're looking for variability.

449
00:51:33,210 --> 00:51:38,580
Correlation is when I can distinctly figure out where every person is.

450
00:51:38,820 --> 00:51:45,120
That would indicate a lot of correlation. So there is a lot of within person variability here.

451
00:51:45,660 --> 00:51:50,460
There's, you know, there are some trajectories, but we can look at where they all started.

452
00:51:50,730 --> 00:51:54,420
The women all started at very different points in time. We expect that rate.

453
00:51:54,600 --> 00:51:57,669
We think that pain is something that we perceive.

454
00:51:57,670 --> 00:51:59,460
Then everybody has a different tolerance.

455
00:51:59,940 --> 00:52:07,110
So we believe that maybe the women have different baselines and then they all move off maybe with different trajectories over time.

456
00:52:07,770 --> 00:52:11,340
So there's a lot of variability there for us. There's a mean line.

457
00:52:11,880 --> 00:52:14,760
If we want to set a line, there's a mean line somewhere in there, right?

458
00:52:15,510 --> 00:52:21,300
And likewise for the intervention group, there's maybe less variability at the outset.

459
00:52:23,100 --> 00:52:26,100
That's kind of strange, right? Because we randomized these women.

460
00:52:26,100 --> 00:52:29,339
You think there would be just as much period? I guess it's half minutes, half an hour. Keep forgetting.

461
00:52:29,340 --> 00:52:33,760
There is no baseline here. And then so forth.

462
00:52:36,640 --> 00:52:42,700
So the question that I was trying to get to here is we're trying to figure out is why does pain vary between these women?

463
00:52:42,730 --> 00:52:50,080
Is it due to their treatment that they received or is it due to other things about the woman that we haven't measured?

464
00:52:51,040 --> 00:52:54,700
The same stimulus can be experienced very painful by some, less painful by others.

465
00:52:55,510 --> 00:53:01,360
And we probably can't even figure out really why some of us experience pain more severely than others do.

466
00:53:02,500 --> 00:53:08,020
We can't measure these things. So you might have heard of latent class models or latent defects.

467
00:53:08,290 --> 00:53:13,610
Latent simply means unobserved. It's all the features of a woman that would explain, right?

468
00:53:13,630 --> 00:53:19,330
We'd like to have a bunch of covariance that we could adjust for, but we don't have those covariates.

469
00:53:19,720 --> 00:53:24,520
They're all latent. And so we put those all into a random effect.

470
00:53:25,210 --> 00:53:32,620
So it's a common to assume that these latent characteristics come from a distribution, and on average there is no effect.

471
00:53:32,620 --> 00:53:41,109
So there's mean zero. But every person's perception of pain varies across the normal distribution, unknown variance.

472
00:53:41,110 --> 00:53:47,080
And again, we assume normality to make our lives easy right now in terms of maximum likelihood, estimation and so forth.

473
00:53:49,540 --> 00:53:50,890
So I want to set this model here.

474
00:53:51,340 --> 00:53:58,990
This is the model that we've all been fitting to, the homework data sets as well, the intercept time effect, group effect interaction.

475
00:53:59,440 --> 00:54:06,220
And we have a residual effect. Each of the residuals has a point out here.

476
00:54:08,230 --> 00:54:14,320
If I talked about correlation so that each person marginally has a time effect to group effect interaction,

477
00:54:14,590 --> 00:54:17,740
and there's an error that marginally comes from a normal distribution.

478
00:54:19,960 --> 00:54:27,880
So let's focus on the intercept. Now, beta not suppose that beta not is actually the mean of a distribution of intercept values.

479
00:54:28,660 --> 00:54:31,870
Right. I started out with a certain pain point and half an hour.

480
00:54:32,530 --> 00:54:40,809
They started out a different pain value and so forth such that this be zero I so fixed effects.

481
00:54:40,810 --> 00:54:44,470
We use Greek letters and we start talking about random effects. We use Arabic letters.

482
00:54:44,800 --> 00:54:49,870
So little b0i quantifies the deviation from this is like for everybody.

483
00:54:49,870 --> 00:54:53,210
This is the overall population intercept, right?

484
00:54:53,290 --> 00:54:57,190
When I fit this model, everybody has the same intercept.

485
00:54:57,610 --> 00:55:00,660
But I think that's an average of many different intercepts.

486
00:55:00,670 --> 00:55:08,320
Some people are higher or lower than others. And so there's a deviation that we think comes from a normal distribution centers around zero.

487
00:55:08,770 --> 00:55:10,120
And again, there's some variability.

488
00:55:10,270 --> 00:55:18,160
So you picture the intercept and then everybody has their own intercept that varies across that population intercept.

489
00:55:19,690 --> 00:55:26,230
So the model looks like this. Now I have a in search of this little B zero and I.

490
00:55:27,650 --> 00:55:34,330
So we actually have two things that are random. Now we have the error and we have this.

491
00:55:34,340 --> 00:55:39,360
This intercept. This intercept describes me.

492
00:55:39,690 --> 00:55:44,089
I, I this b zero.

493
00:55:44,090 --> 00:55:47,150
I applies to all of my measures, right?

494
00:55:47,420 --> 00:55:54,770
As time goes on. J This piece of information applies to every one of my observations, since it's where I start out.

495
00:55:55,790 --> 00:56:01,600
So we have this model here with one extra component in which this and this are assumed to be independent.

496
00:56:03,200 --> 00:56:06,560
So my noise has no relationship to where I started out.

497
00:56:08,360 --> 00:56:13,130
And the two errors are independent. There are no correlated errors anymore.

498
00:56:13,820 --> 00:56:18,640
Correlation is gone. So all of my ears are independent.

499
00:56:19,330 --> 00:56:23,080
And I also believe that this this thing here is independent of all my errors.

500
00:56:24,040 --> 00:56:29,170
So what have I done now? I've partition to the original error, which I'm going to call Epsilon rather than E.

501
00:56:30,490 --> 00:56:33,640
This was normal Sigma Epsilon squared into two pieces.

502
00:56:34,030 --> 00:56:40,780
The variability of my observations is due to the variation between individuals.

503
00:56:41,290 --> 00:56:46,210
This is describing my intercept relative to somebody else's intercept relative to somebody else's intercept.

504
00:56:46,480 --> 00:56:55,900
How do we all vary from each other versus the residuals which tell how much my observations jump around within me?

505
00:56:57,410 --> 00:57:01,400
So this is the variability within individuals. This is the variation with individuals.

506
00:57:01,670 --> 00:57:06,800
And the total variability of my data is my own variation and the noise.

507
00:57:07,460 --> 00:57:10,850
So Sigma Epsilon squared is the sum of those two components.

508
00:57:11,600 --> 00:57:15,530
And you may hear about variance components, models. Here's what we're doing.

509
00:57:15,530 --> 00:57:22,640
We're taking the variance. When I only had one observation per person, all I could talk about was between person variability.

510
00:57:22,940 --> 00:57:28,790
How much does everybody's observation varied? But now, when I have repeated measures over time within a person,

511
00:57:29,090 --> 00:57:35,150
I can also talk about how my observations vary within themselves and then relative to how we vary from each other.

512
00:57:36,810 --> 00:57:42,900
Regardless of whether or not we include this random intercept, what is the average value of why?

513
00:57:44,210 --> 00:57:50,330
What's the expectation of this formula here? It's again, they're all independent of each other.

514
00:57:51,170 --> 00:57:55,760
This has been zero. This has been zero. We come back to the same population.

515
00:57:55,760 --> 00:58:01,590
Mean. Probably what someone was there.

516
00:58:02,730 --> 00:58:04,830
So population mean is the same.

517
00:58:07,850 --> 00:58:17,239
So whether I fit linear regression with this random component or without it, I'm still saying on average this is what somebody's high value is.

518
00:58:17,240 --> 00:58:20,840
It's their treatment group and time and the interaction.

519
00:58:21,440 --> 00:58:24,480
Furthermore, what is the variance of each observation?

520
00:58:28,880 --> 00:58:34,490
Oh, it's time I set my own on the set of purpose or because I want to do a poll everywhere.

521
00:58:36,560 --> 00:58:40,090
It's important, isn't it? That's right. Let's take the variance.

522
00:58:40,090 --> 00:58:47,190
This thing. What's the variance of a sum? It's the sum of the variances plus all the covariance.

523
00:58:47,190 --> 00:58:53,480
Since I don't want covariance as that's why I assume this is independent of this.

524
00:58:54,440 --> 00:59:01,010
That gets rid of any possible cool variances. The variance of this is simply the sum of all the variances of each of these terms.

525
00:59:01,610 --> 00:59:06,079
A lot of them are fixed. There's no variance here.

526
00:59:06,080 --> 00:59:10,040
There's no variance here, here, or here. There's variance here and there's variance.

527
00:59:10,670 --> 00:59:15,880
So the variance of y is the variance of this thing, plus the variance of the error.

528
00:59:15,890 --> 00:59:20,300
And that's the total variance, as I said before. So we call this a marginal model.

529
00:59:21,110 --> 00:59:24,320
You'll hear a lot now about marginal versus conditional models.

530
00:59:25,180 --> 00:59:30,260
It's a marginal model because we marginalize or integrate over the distribution of the random effects.

531
00:59:31,910 --> 00:59:38,300
But within this model there is a second mean and variance known as the conditional mean and variance.

532
00:59:38,750 --> 00:59:43,370
And this is subject specific. Right.

533
00:59:44,820 --> 00:59:49,050
So I have a description of the population. This is the population line.

534
00:59:51,120 --> 00:59:57,630
And like I said, everybody has a line, a set of values that's above or below the population.

535
01:00:00,690 --> 01:00:05,639
This is my fitted line to my data. It's conditional on my random intercept.

536
01:00:05,640 --> 01:00:12,690
We call this random thing, right? So there is e of y over the entire population.

537
01:00:13,260 --> 01:00:19,380
Everybody has their own mean though. They have subject, specific, mean conditional on whether they're random, b not b not is.

538
01:00:20,280 --> 01:00:24,690
And because now we're conditioning on this thing, now I know what the line thing is.

539
01:00:25,530 --> 01:00:31,319
The only variance left is the error variance. The problem is, again, remember, these are latent.

540
01:00:31,320 --> 01:00:36,540
We don't know what they are. So how can I possibly come up with someone's I can estimate beta or not?

541
01:00:36,900 --> 01:00:38,550
I can estimate all these betas,

542
01:00:39,570 --> 01:00:45,960
but I don't know what the variance of this is because I don't have them right and I know how to estimate this thing here.

543
01:00:46,800 --> 01:00:51,300
So we have to figure out if there's a way we can figure out what those are.

544
01:00:51,300 --> 01:00:55,440
So we don't talk about estimating this. We estimate parameters.

545
01:00:55,440 --> 01:00:59,550
This is not a parameter. The only parameter is its variance.

546
01:01:00,750 --> 01:01:04,950
This thing isn't a quantity. It's a random draw from a population.

547
01:01:05,400 --> 01:01:10,560
Right. Was mean zero and variance data not squared. So we have to figure out what those are.

548
01:01:11,010 --> 01:01:14,460
And I think I'm going to want to stop there because my alarm told me to. Yeah.

549
01:01:14,520 --> 01:01:17,190
And we'll talk about correlation after the exam.

550
01:01:17,820 --> 01:01:30,030
So if you want to look ahead, the model I just showed you is equivalent to symmetries in glass and then a slightly different way.

551
01:01:30,180 --> 01:01:34,950
So ruminate on that as you're trying to remember everything from the first two modules.

552
01:01:37,440 --> 01:01:54,620
And let's find out something else about each other today. And it's actually really something need to know.

553
01:01:57,210 --> 01:02:00,360
Here we go. Here's the question for today.

554
01:02:00,630 --> 01:02:14,400
This is a fun one. Food that I detest and I avoid eating is what makes sure I'm not sure of.

555
01:02:14,400 --> 01:02:18,780
The answer is. There we go. So.

556
01:02:28,630 --> 01:02:41,980
It's. She's.

557
01:03:04,210 --> 01:03:22,910
Like. What's the problem?

558
01:03:23,140 --> 01:03:28,200
Folks, in 19 of you have introduced this theory.

559
01:03:31,800 --> 01:03:40,440
But we need to look at this. I'm going to wait for a few more minutes.

560
01:03:43,350 --> 01:04:01,830
Oh, stop. Please, please. You could do more at once and create more food when you just wait till 35.

561
01:04:01,940 --> 01:04:05,790
It looks to me like two more people.

562
01:04:05,790 --> 01:04:10,340
Two more things like that. It's probably like eating generally.

563
01:04:10,740 --> 01:04:19,230
There we go. That's fine. Because she said a real thing.

564
01:04:25,610 --> 01:04:29,250
I like because I know making chickens.

565
01:04:29,550 --> 01:04:59,640
That was how they fit into the whole thing is that instead of food, you can share like lots of vegetables here, folks.

566
01:04:59,640 --> 01:05:13,800
Well, there go the turkey. I mean, I hate hotdogs from the best, which includes peanut butter.

567
01:05:13,830 --> 01:05:17,520
Oh, really? Peanut butter and cheese.

568
01:05:17,520 --> 01:05:33,660
Is that fair? Put. And never had dairy free will keep on your hands.

569
01:05:35,730 --> 01:05:39,270
I am shocked that they've heard of beets on the last day of class.

570
01:05:40,710 --> 01:05:45,000
Beets, garlic, all of God's stuff.

571
01:05:45,310 --> 01:05:50,940
Oh, my God. Oh, wow.

572
01:05:51,620 --> 01:06:03,340
You know, I'm sure size is very specific.

573
01:06:04,680 --> 01:06:16,350
Okay, so he was able to improvise for some reason.

574
01:06:16,810 --> 01:06:21,240
I was like, oh, I'm with you on that one.

575
01:06:21,420 --> 01:06:39,150
Okay. All right. So I was waiting for cilantro with somebody with cilantro.

576
01:06:39,490 --> 01:06:48,600
Like, most shrimps have a huge.

577
01:06:57,570 --> 01:07:06,000
Anonymous. I give someone some bonus this instead of me.

578
01:07:06,930 --> 01:07:11,460
All right. Thank you very much. Good luck with the test.

579
01:07:12,450 --> 01:07:17,220
And number three is out there for me as well. So we'll see you next Wednesday.

580
01:07:20,200 --> 01:07:20,550
See?

