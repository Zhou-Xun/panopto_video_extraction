1
00:00:09,310 --> 00:00:14,020
Okay. Right. So go ahead and get started.

2
00:00:15,250 --> 00:00:19,750
Before I dove in, I wanted to give everyone a chance. I know we've been kind of zooming along here a little bit.

3
00:00:19,750 --> 00:00:23,799
So if you had I mean, you've had office hours and people would come in and ask questions and actually going to

4
00:00:23,800 --> 00:00:27,400
spend a little bit of time on one section that folks were a little confused with today.

5
00:00:27,400 --> 00:00:34,450
But if you do have things you want to raise and that would be a good time.

6
00:00:42,070 --> 00:00:49,170
So in general I try to start classes class out this way. So I think so.

7
00:00:51,640 --> 00:00:59,440
So. So if you sort of have things you want to bring in that particularly, do you think it might be broader than just you?

8
00:01:00,160 --> 00:01:07,630
Any questions? Or if you think it's just you, then there's, you know, I'll try to have this sort of class.

9
00:01:08,890 --> 00:01:15,290
It's just been. All right.

10
00:01:15,320 --> 00:01:22,670
So this is stepping back a little bit to our discussion of the randomized trial setting.

11
00:01:23,900 --> 00:01:31,100
And I had a few notes on this about how they actually estimate the sampling variance in the randomized trial,

12
00:01:33,230 --> 00:01:37,100
mostly taken from the Toobin's and Rubin text here.

13
00:01:38,060 --> 00:01:42,080
But the text actually does just sort of bounce around a little bit.

14
00:01:42,080 --> 00:01:46,640
And I wanted to just sort of maybe sort of walk this whole back and see if I could clarify any questions.

15
00:01:47,450 --> 00:01:50,210
So so again, in the randomized trial,

16
00:01:50,960 --> 00:01:56,470
you don't have to worry about all the propensity score stuff and all the things we've just been working on sort of in this past week and a half.

17
00:01:56,490 --> 00:02:01,549
We can actually just work on the observed data, right? So they use treatment and control.

18
00:02:01,550 --> 00:02:08,690
I use one and zero, but sort of the difference in the means and treatment of difference between treatment and means of control.

19
00:02:10,070 --> 00:02:13,760
So so they get that variance.

20
00:02:17,330 --> 00:02:26,990
So they're, they're going back to 633, which is the general, this general form here,

21
00:02:28,190 --> 00:02:32,150
which includes a covariance that we can't really directly estimate.

22
00:02:32,900 --> 00:02:35,960
But just talking about the first two terms, which we can.

23
00:02:39,600 --> 00:02:55,830
You should write this on the board. This is a true parents.

24
00:03:19,010 --> 00:03:42,970
That's. And half the literary population.

25
00:03:58,120 --> 00:04:04,540
So so we're trying to find estimates for this quantity.

26
00:04:06,010 --> 00:04:11,260
So for the first one, it's pretty simple, right?

27
00:04:11,260 --> 00:04:20,570
We can just use the observed. Variance for the treatment and for the control.

28
00:04:23,660 --> 00:04:28,820
Right. So that's it's no mystery. There is the so the action usually in this third term.

29
00:04:29,060 --> 00:04:33,170
Right. Which we can't do because we never observed why one and why zero together.

30
00:04:33,170 --> 00:04:37,580
So we can't get that difference. So we can't look at the variance of that difference.

31
00:04:38,870 --> 00:04:43,280
Or conversely, if you think about the covariance of those two together.

32
00:04:44,300 --> 00:04:47,420
So. All right.

33
00:04:47,430 --> 00:04:53,570
So they point out, if that's constant, then essentially that s squared term is zero.

34
00:04:54,680 --> 00:05:00,169
So we only have to have the first two terms of the variance, which because that has to be an unbiased manner,

35
00:05:00,170 --> 00:05:09,329
is that they call this naming because it sort of comes from the 1930s word from from Jersey maybe,

36
00:05:09,330 --> 00:05:13,490
and really sort of sorted out a lot of that randomized trial idea.

37
00:05:15,110 --> 00:05:24,920
So, um, so this is very commonly used, as they point out, even when the assumption about a treatment effect may be known to be inaccurate.

38
00:05:26,030 --> 00:05:29,630
So one thing is that it's conservative.

39
00:05:31,220 --> 00:05:34,830
So that is a, you know, generally considered to be a a nice feature.

40
00:05:34,880 --> 00:05:37,210
You can't get something that's that's unbiased in general.

41
00:05:37,220 --> 00:05:44,900
At least get something that's conservative so that you tend to you're in the side of making type two or type one errors.

42
00:05:46,030 --> 00:05:51,470
But there's some debate, I suppose, about whether that's really exactly the right thing to do, but generally that's taken to be a sensible approach.

43
00:05:55,490 --> 00:06:00,080
So they go on, of course, to note that, you know, large samples and so forth,

44
00:06:00,080 --> 00:06:05,510
because we're using typically these statistics to construct confidence intervals and P value.

45
00:06:09,140 --> 00:06:21,860
So and then of course, if we're looking at the super population estimate, which is more common really, then this extra term goes away.

46
00:06:21,980 --> 00:06:25,700
So this is indeed the right estimate of super population variance.

47
00:06:29,750 --> 00:06:37,950
So questions of. Okay.

48
00:06:37,950 --> 00:06:44,160
So the last of this section, which I think was where there might have been some confusion, they considered two different alternative estimates.

49
00:06:45,090 --> 00:06:48,600
You know, they couldn't do it in the reverse order that I would have done it.

50
00:06:49,920 --> 00:07:01,240
So. Their first estimate or the second when they talk about maps to this concert treatment effect.

51
00:07:01,260 --> 00:07:17,340
Right. So they basically point out that, well, in the absence of the assuming treatment effects consent, so that last term zero,

52
00:07:19,110 --> 00:07:29,249
then you can essentially get a more efficient estimate than this one here using the the treatment in control groups,

53
00:07:29,250 --> 00:07:33,330
because under that assumption, those two values have to be the same.

54
00:07:33,810 --> 00:07:40,800
So you can use the pooled variance estimate her and the pooled variance estimate is going to be tend to be smaller.

55
00:07:42,000 --> 00:07:47,250
So. So they can.

56
00:07:56,140 --> 00:08:06,010
You can you can use this as an alternative. So essentially just replacing that, those separate estimates with the pool variance estimate up here,

57
00:08:06,440 --> 00:08:09,580
you just get the pool of some of the squares and divided by ten minus two.

58
00:08:14,000 --> 00:08:20,600
So the means can be different, but the variances in the two groups have been the same because of this constant to meet them.

59
00:08:20,600 --> 00:08:28,040
There's no way for those populations and your disability to talk about the control and then the shift to treatment.

60
00:08:28,610 --> 00:08:36,050
So that's entirely a shift. So whatever is going on with that variance in the control group, it's going to be exactly the same in the treatment group.

61
00:08:36,980 --> 00:08:47,700
So that's a watch there. And then finally.

62
00:08:50,550 --> 00:08:59,700
They then consider the situation where you may have treatment effect heterogeneity so that you're not assuming constant, constant changes.

63
00:09:00,900 --> 00:09:08,940
So in that case, you've got kind of an upwardly biased estimate of right, because that last piece has to be positive.

64
00:09:09,660 --> 00:09:16,130
So if that's not zero, then it has to be greater than zero.

65
00:09:16,140 --> 00:09:23,310
And so the resulting variance is going to be if you just ignore that term and use the estimate or it's going to be too big.

66
00:09:24,360 --> 00:09:37,570
So. So if you did know that variance covariance, you could work with this estimate.

67
00:09:39,430 --> 00:09:44,650
And this actually is is greatest when the correlation is one.

68
00:09:45,910 --> 00:09:49,959
So, you know, if you if you want to pick other values, I think I did that in the homework.

69
00:09:49,960 --> 00:10:03,790
You kind of see that they shrink down. But when the correlation is one, then the estimate you would use here, this is just becomes one.

70
00:10:03,790 --> 00:10:06,990
And then you're left with terms that are all that's to equal.

71
00:10:07,870 --> 00:10:13,570
Right. So you can get obviously the first two terms in the usual fashion.

72
00:10:16,120 --> 00:10:26,710
And in the last term, that is going to be the squared difference in those little standard deviation measures.

73
00:10:26,720 --> 00:10:30,010
Square root of the variance. So.

74
00:10:32,860 --> 00:10:38,470
So if these are equal, then you kind of default back to the original misdemeanor.

75
00:10:38,920 --> 00:10:45,280
Otherwise it's going to be a little it's going to be smaller. So you can basically, as you point out here,

76
00:10:46,300 --> 00:10:51,280
you can construct confidence intervals and reside that are tighter than those you would get

77
00:10:52,180 --> 00:10:56,410
if you use just this standard name and estimate without compromising our sample validity.

78
00:10:56,710 --> 00:11:02,290
But remember that the confidence interval for the population. So you've got the entire population.

79
00:11:03,100 --> 00:11:09,910
So. So again, for the super population, you still want to go ahead and use the full thing.

80
00:11:11,350 --> 00:11:15,050
So. Okay.

81
00:11:15,060 --> 00:11:17,730
So it's still going to be conservative in four. It was less than one.

82
00:11:18,870 --> 00:11:25,860
But, you know, again, we're sort of aiming for and it's a little too little complicated.

83
00:11:26,910 --> 00:11:30,360
It sort of drives into this population versus super population.

84
00:11:31,800 --> 00:11:36,090
But I kind of wanted to just sort that through because I think there was some confusion when people were reading it.

85
00:11:36,840 --> 00:11:40,890
Some people. Okay.

86
00:11:48,060 --> 00:11:57,990
Okey doke. So you set that aside and go on to finishing our discussion of matching.

87
00:12:01,440 --> 00:12:07,010
So the last thing I wanted to talk about here is, is this is using matched pairs.

88
00:12:07,020 --> 00:12:17,880
So we looked at some of these previous approaches where we use the fully weighted estimator and then

89
00:12:17,890 --> 00:12:22,290
we did a little bit in this direction where we did sort of stratified based on propensity scores.

90
00:12:22,890 --> 00:12:25,470
But this is going to sort of go all the way into this idea of matching.

91
00:12:26,310 --> 00:12:30,480
And conceptually idea is to find two observations with identical covers with different treatments.

92
00:12:31,530 --> 00:12:33,509
Right. So if you think everything about confounding,

93
00:12:33,510 --> 00:12:39,450
you've measured and now you've got two people that are identical on those measures with two subjects who are identical,

94
00:12:40,050 --> 00:12:44,550
then then you should just be able to look at the difference between them.

95
00:12:44,770 --> 00:12:53,549
Right, and treatment wins and control. And then if you average that over a whole bunch of of people with similar values that represent

96
00:12:53,550 --> 00:12:57,990
the values in the population you actually get should be able to get an estimate of causal effect.

97
00:12:59,010 --> 00:13:06,930
Right. So specifically, we're going to put the notation in a second and that we're essentially taking as our predicted value for the counterfactual,

98
00:13:07,290 --> 00:13:16,740
this matched next element. So particularly we start with a bunch of treatments, field individuals, match control individuals to them,

99
00:13:17,160 --> 00:13:24,030
those match control individuals for an estimate of the counterfactual under control for that treated individual.

100
00:13:25,800 --> 00:13:31,500
So of course this doesn't magically solve the problem of unmeasured confounders.

101
00:13:34,230 --> 00:13:39,340
I think when a lot of this early work came out about 15 to 20 years ago, there was a little bit of confusion about that.

102
00:13:39,360 --> 00:13:43,830
Somehow people thought there was some sort of magic in the propensity score that dealt with that, but it doesn't.

103
00:13:44,340 --> 00:13:49,440
So I think that's pretty clear now. But but but that sort of remains an issue.

104
00:13:50,340 --> 00:13:56,070
But it really is an attempt to kind of recreate this randomized trial conditions as best as could be done in the absence of randomization.

105
00:13:56,220 --> 00:14:01,940
Right. So. In a matter of fact, we didn't I didn't take the time on it.

106
00:14:01,940 --> 00:14:06,620
But there are some sections and it's got to be textbook. Talk about mesh randomization.

107
00:14:06,620 --> 00:14:10,609
So this idea that you sort of get more efficient, you still get unbiased,

108
00:14:10,610 --> 00:14:18,800
but improve efficiency by matching individuals or create individuals that are fairly similar to the same treatment control.

109
00:14:18,860 --> 00:14:20,549
So that that assignment,

110
00:14:20,550 --> 00:14:26,570
that randomized assignment you don't get to really unmeasured confounding but improves efficiency because you're sort of you know,

111
00:14:26,660 --> 00:14:30,410
matching people that are you're assigning treatment in a way that that are going to

112
00:14:30,920 --> 00:14:37,220
kind of force even in small samples the the access to be be pretty well balanced.

113
00:14:38,060 --> 00:14:42,500
So so this is sort of a version of this post hoc where you can't actually assign the treatment,

114
00:14:42,500 --> 00:14:49,980
but at least to try and sort of look and see what happening, you know, allowing for the fact there may be still unmeasured failures.

115
00:14:50,600 --> 00:14:55,069
It's also really simple to explain to non statisticians because it fits into this whole randomized trial idea.

116
00:14:55,070 --> 00:15:00,560
Right. And so we've got these people that lined up, they're identical except in terms of this treatment or exposure.

117
00:15:01,190 --> 00:15:08,930
So now we can sort of think about these differences, advocate as having this be the same thing as if it were a randomized experiment.

118
00:15:10,160 --> 00:15:13,370
So I think that's another plus for it.

119
00:15:17,090 --> 00:15:23,540
So in practice, although I've certainly read about this, I realize I haven't done many examples of it.

120
00:15:23,540 --> 00:15:26,959
You sort of see that I was sort of when I go through this example,

121
00:15:26,960 --> 00:15:35,180
I sort of had it was to some sort of encoding thing but hum, but that is that is another, another issue.

122
00:15:35,180 --> 00:15:51,290
It's also I think there I haven't totally explored the the set of available packages on or even other statistical software but it it it does

123
00:15:51,290 --> 00:15:56,989
require a lot of things will encounter in this class are still there aren't necessarily a ton of like packages that are ready for you to

124
00:15:56,990 --> 00:16:06,799
run right there's definitely I mean is people getting them Masterson's that biostatistics I would expect you to sort of be able to get

125
00:16:06,800 --> 00:16:13,280
under the hood understand what's going on anyway but but this is a case where you may not really be able to sort of went to one direction,

126
00:16:13,280 --> 00:16:17,090
but to run a package really you would with no set regression.

127
00:16:18,410 --> 00:16:29,090
So okay. So just to say a little more about the conceptual part of this here, it doesn't really rely on a model for propensity of assignment, right?

128
00:16:30,320 --> 00:16:35,300
But you do have to have some kind of algorithm in most cases. And again, there's a whole world of matching algorithms.

129
00:16:35,750 --> 00:16:41,630
I could teach a one or two critic class matching algorithms or someone good, I think.

130
00:16:42,500 --> 00:16:45,160
And we're not going to do that because we don't want to talk about.

131
00:16:45,830 --> 00:16:52,520
So we're just can look at that a couple of simple approaches but but there so there has to be this sort of algorithmic assignment,

132
00:16:52,820 --> 00:16:57,049
algorithmic construction, you know,

133
00:16:57,050 --> 00:17:01,700
except if we have like four, six, eight categories describing your entire career,

134
00:17:01,700 --> 00:17:05,659
it's inconceivable that you could do sort of good people then work from there.

135
00:17:05,660 --> 00:17:12,070
But but if you're in that setting, you're also not a case. You're probably doing a good job at controlling for unmeasured confounders, right?

136
00:17:12,140 --> 00:17:20,810
You want to have XP as hypertension possible? Usually it can also be kind of inefficient, have relatively few cases which I think this is.

137
00:17:20,810 --> 00:17:26,480
You can sort of view this as sort of automatically trimming for similar to some of the stuff we talked about in the last class.

138
00:17:28,820 --> 00:17:35,580
Alternatively, you can kind of expand the definition of matching like how close to the has to be at the risk of introducing confounding right.

139
00:17:35,840 --> 00:17:41,120
So. So that it's a bit of a tradeoff there.

140
00:17:42,200 --> 00:17:46,009
All right. So it's sort of a conceptual discussion.

141
00:17:46,010 --> 00:17:51,120
We'll get into the mechanics here a little bit more. But before I do that, questions.

142
00:17:56,940 --> 00:18:07,460
Okay. So we'll sort of start out the simplicity, which is 1 to 1 matching, and we're going to focus on matching treatment cases with controls.

143
00:18:08,700 --> 00:18:10,499
So in order to do one dimensional matching,

144
00:18:10,500 --> 00:18:18,270
we have to have at least as many controls then as we have treatment cases typically, maybe a whole lot more so.

145
00:18:19,370 --> 00:18:27,730
So if we are in that situation and we can consider doing this so notation reference, things get a little ugly.

146
00:18:28,680 --> 00:18:34,560
I think the book is slightly over. Does it sort of try to trim it a little bit here and there, but kind of have to stick with it.

147
00:18:36,540 --> 00:18:48,880
So I'm going to let this this I see be an index of the control case such that the you know, these are usually going to be vectors, not scalar, right.

148
00:18:48,900 --> 00:18:57,690
So that the vector of observations for eiph control is matched up with this MCI,

149
00:18:58,530 --> 00:19:05,490
I'm sorry for its treatment is matched up with this and S.I. for the the MCI control.

150
00:19:07,140 --> 00:19:14,370
So so once you have this, then you're basically going to be estimating that counterfactual under the control, right?

151
00:19:14,370 --> 00:19:20,250
You're basically going to say our estimate of the counterfactual for that by observation and treatment,

152
00:19:21,030 --> 00:19:26,880
how they receive control is the observed value from this individual.

153
00:19:28,680 --> 00:19:37,430
So. Just a couple of little puzzle pieces, so much more.

154
00:19:40,670 --> 00:19:45,920
So. So, yeah.

155
00:19:49,110 --> 00:19:56,490
So let's suppose I have three cases in treatment cases.

156
00:20:00,550 --> 00:20:33,570
And. Treatment.

157
00:20:39,810 --> 00:20:47,320
And that's say, have. Working closely.

158
00:20:56,420 --> 00:21:09,270
You're. Controls and.

159
00:21:21,460 --> 00:22:18,660
And it's a control group in your. Yeah.

160
00:22:21,060 --> 00:22:26,380
Yeah, yeah. It's all right.

161
00:22:26,530 --> 00:22:47,340
So. All right.

162
00:22:47,350 --> 00:22:51,420
So this really is a moment of setting where I could imagine this would be based on the access.

163
00:22:52,380 --> 00:22:56,670
So. This.

164
00:22:59,800 --> 00:23:09,570
Sort of thing. So I'm going to match the first case in the treatment to the first case control the second guy.

165
00:23:13,820 --> 00:23:16,840
Nothing down here. Just.

166
00:23:21,220 --> 00:23:26,700
Actionscript this. So I will match.

167
00:23:28,220 --> 00:23:32,210
From the second case to the fourth case. And then.

168
00:23:39,090 --> 00:23:49,530
So I've got these pieces that match so I can match the third piece in the treatment case to the control.

169
00:23:50,100 --> 00:23:53,120
And this would match in the treatments over here.

170
00:23:53,130 --> 00:23:57,490
So. So basically the matching here.

171
00:23:58,880 --> 00:24:06,370
So 1131 -1.4.

172
00:24:11,280 --> 00:24:20,070
So write for the show. Ultimately, you're looking just at the brilliant piece here and I combine them on the way to treatment.

173
00:24:21,600 --> 00:24:30,690
It's like four control, three with.

174
00:24:36,280 --> 00:24:41,480
One, two, one, two. So.

175
00:24:44,860 --> 00:24:50,090
All right. So you can think of this as. This is essentially.

176
00:25:13,130 --> 00:25:24,480
You can see evidence that. I'm good for the treatment side, but for the control side.

177
00:25:29,800 --> 00:25:42,700
The control side. So his first observation is really he has to be here for observation or the testimony of the second.

178
00:25:45,970 --> 00:25:49,240
The Second Circuit optimization process.

179
00:26:38,320 --> 00:26:50,380
So right so everyone see here is one emcee in to see is four and three C is two.

180
00:26:51,370 --> 00:26:55,209
In this example, it's sort of focusing on this little thing down here.

181
00:26:55,210 --> 00:27:00,370
So that's index he decided the control groups and of course I just want to add three for the friends.

182
00:27:06,090 --> 00:27:13,260
So, you know, I would say we think the difference of those shoulder stage, but at this point in terms of this conceptual idea.

183
00:27:13,790 --> 00:27:28,399
Okay. So. So I guess one issue, right, is if I only had sort of one observation,

184
00:27:28,400 --> 00:27:33,370
I would say with both sexes were one in the control group and I had several observations on the treatment group.

185
00:27:34,210 --> 00:27:37,330
It could be trouble with this exact matching cause I've used it up.

186
00:27:38,140 --> 00:27:44,920
Right. So sort of talk about extension to this. So and obviously that's a low dimensional rate.

187
00:27:45,820 --> 00:27:55,059
So I've worked out a simple example here with, you know, essentially for for way more than normal to lie to two binaries.

188
00:27:55,060 --> 00:28:02,260
But if you get ten binaries now you're to the 10th possible categories maybe,

189
00:28:02,610 --> 00:28:08,140
which you probably observe if it's continuous, then obviously there's another issue as well.

190
00:28:09,070 --> 00:28:15,850
Okay. So all right, so then the ace, then basically just.

191
00:28:27,870 --> 00:28:34,429
19 less familiar than in space.

192
00:28:34,430 --> 00:29:13,900
Face of despair for some of his. It used to be that the.

193
00:29:16,590 --> 00:29:19,920
Right. So look at the average of these three.

194
00:29:29,690 --> 00:29:38,120
And the variances to meter basically is going to look at the variance of these differences.

195
00:29:40,460 --> 00:29:51,740
Well, I think there's a little and then divide by one, too. So I think there's a typo in the mean or it should match their derivation of this,

196
00:29:51,950 --> 00:29:56,330
which is from Chapter ten, which I don't think I meant to upload, which I haven't done.

197
00:30:06,470 --> 00:30:12,620
It's basically a lot of somewhat tedious algebra, so I'm not going to go through it here.

198
00:30:13,160 --> 00:30:17,209
But, you know, so once you have the varying system in a row, you can do the usual game.

199
00:30:17,210 --> 00:30:24,920
Crazy statistics that use usages to construct confidence intervals is a normal approximation.

200
00:30:25,730 --> 00:30:33,139
So all that good stuff. So a couple of things.

201
00:30:33,140 --> 00:30:44,030
One is that we're assuming throughout here with for bias and variance that the control is really given by this true value.

202
00:30:44,750 --> 00:30:48,170
And so the matching data kind of correspond to a pairwise randomized experiment.

203
00:30:52,660 --> 00:31:02,230
And so you can use that variance estimate. And now there is I guess if you get into the machinery of this, it does tend to be a little conservative.

204
00:31:03,550 --> 00:31:11,950
So the expected value of this is the true variance, plus a term that's basically of order in minus one.

205
00:31:11,950 --> 00:31:17,349
So large samples becomes unbiased, would be a little conservative.

206
00:31:17,350 --> 00:31:20,740
It's more restrained. It's more samples. So.

207
00:31:24,950 --> 00:31:29,229
Okay. So we were just doing like a regular randomized trial.

208
00:31:29,230 --> 00:31:36,840
We couldn't estimate the variance between like the counterfactual and sort of like the US one zero term.

209
00:31:38,950 --> 00:31:44,530
Right. So are we able to do something like that in this situation?

210
00:31:45,370 --> 00:31:53,139
Well, I think that's the same issue you sort of get. This is a potentially a kind of a conservative estimate or this is this is sort

211
00:31:53,140 --> 00:31:57,130
of related to the the same issue that we saw before in the randomized trial.

212
00:31:57,400 --> 00:32:12,760
Okay. So it's it's not exactly the same because this actually is true, even the population in France, but it's also because there's a yeah.

213
00:32:13,060 --> 00:32:17,800
There's a little extra approximation piece that gets lopped off.

214
00:32:18,820 --> 00:32:26,799
So right. I think there's this I think that I should go back and look at it, but I don't think this is all the more from observed data.

215
00:32:26,800 --> 00:32:28,680
So I think it is kind of the same issue. Okay.

216
00:32:35,950 --> 00:32:44,950
Also, if you are in a matched randomized trial, say this will tend to be a more efficient estimator than the non matched randomized trial setting.

217
00:32:46,120 --> 00:32:51,640
So because essentially you're sort of able to control of the X's.

218
00:33:00,390 --> 00:33:04,220
You can. All right.

219
00:33:05,500 --> 00:33:12,400
So in most practical settings, there won't be enough of any exact matches among the covariates.

220
00:33:12,600 --> 00:33:17,020
Right. So you either would have to throw away a huge amount of data or just work with a small

221
00:33:17,020 --> 00:33:21,790
set of observations and you could match exactly which besides being inefficient,

222
00:33:21,790 --> 00:33:35,019
also maybe not very representative. So in a game, right, you want to we're trying to share this this idea of confounding in this, right.

223
00:33:35,020 --> 00:33:41,200
So that means that the treatment's independent of potential outcomes jointly given that

224
00:33:41,500 --> 00:33:47,600
given X so the higher dimension X is generally the more plausible assumption is right.

225
00:33:47,680 --> 00:33:54,310
So, so we're going to take a step back here and define a matching based on minimizing some sort of generic distance function.

226
00:33:56,290 --> 00:34:05,520
So instead of this excited minus X JP zero right here, right, if you subtracted those vectors and each other, we just get a vector of zeros.

227
00:34:06,550 --> 00:34:11,470
We're going to allow it to have some. We're going to sort of try to find the one that's the best match for the demo.

228
00:34:13,600 --> 00:34:20,030
So. Okay, so I'm not going to this.

229
00:34:20,320 --> 00:34:23,890
I'm leaving. This is very generic for the moment. We'll get into the.

230
00:34:26,480 --> 00:34:29,270
Couple of approaches here in a second, but.

231
00:34:33,850 --> 00:34:42,370
So we have the problem then with this sort of multiple matching problem that kind of starts to rise up here.

232
00:34:42,510 --> 00:34:50,260
Right. We may not be able to have a single control match to a single observation, a particular single trial, and be optimal for multiple observations.

233
00:34:51,520 --> 00:34:55,749
So so it is, you know,

234
00:34:55,750 --> 00:35:03,940
ultimately this decision we would make would be could happen for different sort of be putting this control into different treatments.

235
00:35:04,090 --> 00:35:09,680
So this is sort of deal with through optimal allocation, right?

236
00:35:09,730 --> 00:35:14,020
We sort of try to find the allocation that minimizes this distance.

237
00:35:15,220 --> 00:35:21,340
So it's impossible to solve this directly unless the samples are really small because essentially is sort of this product of the ends,

238
00:35:22,150 --> 00:35:26,090
the sample sizes, possible accommodations to consider. Right.

239
00:35:27,470 --> 00:35:35,070
I guess our little moment or example we were looking at with.

240
00:35:39,790 --> 00:35:43,360
The car seats, that's over 10 million combinations.

241
00:35:44,350 --> 00:35:49,150
So one simple approach is this idea of sort of greedy or here's the available matching algorithm.

242
00:35:50,530 --> 00:35:58,090
So you start off with your first treatment option and then you just sort of looked at the best one that minimizes it.

243
00:35:59,620 --> 00:36:06,129
And then you drop that observation out and you look for the second best for the second observation.

244
00:36:06,130 --> 00:36:09,880
What's the best among the remaining? You just keep going on down.

245
00:36:10,630 --> 00:36:18,820
And remember, of course, that there are actually should be a such that not a comma.

246
00:36:29,010 --> 00:36:40,399
The Court. So we have more controls by definition than the treatment cases.

247
00:36:40,400 --> 00:36:45,890
We're making an assumption. So we will be able to have this work.

248
00:36:47,030 --> 00:36:50,419
But you can see that we are using if we're using them up, we still have this issue.

249
00:36:50,420 --> 00:36:53,870
We started trying to this wonder with matching. So we're using the map along the way.

250
00:36:54,680 --> 00:36:57,830
So the ordering of how we order these treated cases plays a role.

251
00:37:00,740 --> 00:37:06,830
And so it is, Rubin suggest, already by propensity scores, the largest, the smallest since again,

252
00:37:06,920 --> 00:37:13,430
we always have the situation that the treated propensities will be larger on average than the control entities.

253
00:37:14,270 --> 00:37:19,190
So so these high propensities to treat will sort of be reversed in the control group.

254
00:37:19,880 --> 00:37:23,010
So we kind of want to we want to use those as sparingly.

255
00:37:25,370 --> 00:37:32,690
So we want to save those. So we kind of use those up first because those are going to be the hardest ones to match.

256
00:37:33,200 --> 00:37:38,720
And then what's left should be easier to match with the lower in treatment individuals.

257
00:37:41,380 --> 00:37:49,890
So. And when you've done the matching, you're sort of back to the exact matching.

258
00:37:53,010 --> 00:37:56,850
And you can go back and use the same two.

259
00:38:06,250 --> 00:38:14,800
So there is one issue. If you have multiple controls that match to everything, the equidistant, retreat or observation could happen exactly as you do.

260
00:38:15,610 --> 00:38:18,850
Then you have two different alternatives.

261
00:38:20,080 --> 00:38:21,820
One is to pick one at random.

262
00:38:23,320 --> 00:38:31,630
The other is to sort of use the meaning of all the observations to kind of reduce variance, which makes the sort of resulting inference conservative.

263
00:38:33,310 --> 00:38:41,049
But it also uses apples observations. So picking one at random leaves more controls for future matching.

264
00:38:41,050 --> 00:38:44,550
So that's usually what's what's done. So.

265
00:38:47,060 --> 00:38:55,220
Okay. So any to your side the issue I haven't talked about how to actually do the match any any questions at this.

266
00:39:05,030 --> 00:39:13,430
So. All right. So how do we determine this distance? So two real common approaches or mahalanobis distance and Euclidean distance.

267
00:39:14,810 --> 00:39:22,459
So Mahalanobis the idea here is that you have two vectors with some variance covariance matrix and

268
00:39:22,460 --> 00:39:26,420
you're going to look at the difference and essentially divide by that variance and take the square root.

269
00:39:26,450 --> 00:39:34,099
So it's a little bit like a Z type statistic for the variance here because we have treatment control groups separately.

270
00:39:34,100 --> 00:39:39,260
We're just going to take the average of those of those two variance good range matrices, right?

271
00:39:39,800 --> 00:39:49,640
X is the size of the actions of the same. So this would both be such a cube like U matrices and Euclidean basically does the same thing.

272
00:39:49,640 --> 00:39:56,000
Except that doesn't worry about the covariance as it just sort of works with the diagonal of this variance covariance matrix.

273
00:39:57,530 --> 00:40:11,520
So. You spend a little bit of time.

274
00:40:30,320 --> 00:41:15,500
So. The Romans talk about this a little bit.

275
00:41:15,500 --> 00:41:34,530
I thought it. That's 3 seconds here.

276
00:41:35,640 --> 00:41:40,920
So this manhole notice distance idea allows the correlation to move points closer.

277
00:41:51,800 --> 00:42:06,450
In the direction of the coalition. So.

278
00:42:17,330 --> 00:42:27,050
Let's suppose we have sort of a credit illustrator in a 43% crazy extreme correlation.

279
00:42:27,090 --> 00:42:30,920
2.9 anodized.

280
00:42:34,890 --> 00:42:42,700
Variances. So there is a wider network.

281
00:42:46,350 --> 00:42:58,670
And I suppose I have a treatment case here that's centered at the origin, and it's not considered to be trolls.

282
00:43:00,810 --> 00:43:05,610
One way down the diagonal.

283
00:43:05,610 --> 00:43:08,940
Five, seven, five, five.

284
00:43:12,990 --> 00:43:16,260
And then one on the.

285
00:43:17,760 --> 00:43:30,760
Right. X axis. So I wrote down the inverse and this.

286
00:43:32,290 --> 00:43:42,180
Right. So it's one over the determinant of the values.

287
00:43:45,610 --> 00:44:05,540
So. So basically the professional lies that.

288
00:44:16,670 --> 00:44:20,750
Right. He was there to the piece.

289
00:44:53,620 --> 00:44:58,060
So if I look at the differences between these. So.

290
00:45:07,170 --> 00:45:13,620
Let's start with the Phi Phi Phi Phi.

291
00:45:17,490 --> 00:45:26,000
And so I compute. You hear?

292
00:45:50,500 --> 00:45:54,840
One of the major complications.

293
00:46:01,810 --> 00:46:07,170
And it's something that's basically about 26.3 inside experiments.

294
00:46:23,630 --> 00:46:27,360
I look at it.

295
00:46:46,370 --> 00:47:23,470
Yeah. So that's basically from the larger value installation from the negative emitters.

296
00:47:26,120 --> 00:47:29,230
So well founded 84.

297
00:47:29,240 --> 00:47:40,270
It's going to be 4.1 for about 9.8. So which one of these is better matches?

298
00:47:40,270 --> 00:47:43,810
You know. I find the photograph.

299
00:47:45,790 --> 00:47:51,740
Right small back. Yeah.

300
00:47:57,220 --> 00:48:16,799
Recently we've been here. Now.

301
00:48:16,800 --> 00:48:27,300
What about Euclidean distance? So.

302
00:48:33,800 --> 00:48:37,580
So that. And so now I just have.

303
00:48:44,600 --> 00:48:50,340
I remember now I'm thinking it's just the diagonals of this part here, right?

304
00:48:50,420 --> 00:48:55,190
So I still have five times five.

305
00:48:56,220 --> 00:49:09,670
But what's my. The inverse the three things over two five over 2522542.

306
00:49:10,210 --> 00:49:13,800
Well, if you have a six, five, five, go to the inside. Oh, no.

307
00:49:14,660 --> 00:49:17,950
I mean, yeah. What's that going to be?

308
00:49:19,450 --> 00:49:24,220
Identity. Identity? Said there.

309
00:49:29,080 --> 00:49:45,550
All right. So that is three or 5 to 7 and then right for the distance for this.

310
00:50:02,010 --> 00:50:08,100
Yeah. Which is a very scary 16 four.

311
00:50:10,550 --> 00:50:14,100
So basically. You're.

312
00:50:16,760 --> 00:50:22,960
Zero. Where so.

313
00:50:24,010 --> 00:50:32,810
Okay. So you're putting distance between. So we sort of draw a picture here.

314
00:50:39,080 --> 00:50:43,170
So. And I this.

315
00:51:19,810 --> 00:51:27,480
All right, so my anxiety down here is zero and.

316
00:51:31,630 --> 00:51:54,330
So I had this kind of. So I don't know what kind of discipline.

317
00:51:55,410 --> 00:52:07,820
Distribution looks like a of quite high. So basically this spot ends up here versus.

318
00:52:09,540 --> 00:52:15,120
So this is next. When I asked James.

319
00:52:18,120 --> 00:52:23,550
It's five zero.

320
00:52:24,680 --> 00:52:27,800
Trying to see. That.

321
00:52:29,390 --> 00:52:33,410
This space here is going to be denser than space here.

322
00:52:34,520 --> 00:52:39,650
So. Correlation part.

323
00:52:41,730 --> 00:52:50,040
Makes sense. But of course, if you go to the theme park, you're essentially just ignoring this and just commuting completely distances.

324
00:52:52,890 --> 00:52:55,590
So this one's closer. To what?

325
00:53:08,890 --> 00:53:16,780
So I guess some of this exercise is to just also allow you to remind that when you're dealing with drugs that are not so efficient,

326
00:53:16,840 --> 00:53:25,120
that they say, well, I'm actually right. I mean, zero zero seems like four zero is closer than five five.

327
00:53:25,360 --> 00:53:30,330
Certainly if you ignore correlation, that's true. But this one correlates.

328
00:53:33,580 --> 00:53:41,500
They're uncorrelated. This would be cool, but these things are highly correlated.

329
00:53:42,520 --> 00:53:50,440
We sort of think of these as being more like each other because that's when it has to take a lot of the same information.

330
00:53:52,690 --> 00:53:59,560
So the fact that they're more equal to each other is a bit more important than just that.

331
00:54:00,280 --> 00:54:07,080
The. The decision involved with this is quite disappointing as well.

332
00:54:08,100 --> 00:54:12,780
Overall population are very correlated person unusual in the sense that there.

333
00:54:14,550 --> 00:54:20,430
So that's talking through the conceptual part of that.

334
00:54:22,650 --> 00:54:37,830
Okay. Okay.

335
00:54:37,830 --> 00:54:42,120
So that's questions about this. We.

336
00:54:45,360 --> 00:54:57,870
If I had to calculate the year over year student deficit, you write the guidance from the data taking them so that's.

337
00:54:59,140 --> 00:55:08,910
So we. So VM is this thing here?

338
00:55:10,020 --> 00:55:21,600
So assuming I have Bivariate X, I got a whole bunch of them and I'm averaging those two and it just turned out to be the case.

339
00:55:22,050 --> 00:55:26,220
But. Class One World.

340
00:55:26,220 --> 00:55:34,650
So. Good question.

341
00:55:38,190 --> 00:55:43,580
Anything else? All right.

342
00:55:45,050 --> 00:55:48,530
So. All right. So matching with replacements.

343
00:55:48,540 --> 00:55:55,249
I've been talking about just doing this 1 to 1 thing. But what if we just use a single control and that's a single treatment?

344
00:55:55,250 --> 00:55:59,410
If there are a number of controls available with similar distance metrics, they all can be used in analysis.

345
00:56:00,680 --> 00:56:06,350
Right. So we actually can go through and do this with replacement part.

346
00:56:07,940 --> 00:56:24,569
So. Okay.

347
00:56:24,570 --> 00:56:31,070
So basically. I was running out of observation, so I'm just using the capital.

348
00:56:31,070 --> 00:56:35,530
I'm here to represent asset rather than a value. So think about it.

349
00:56:35,540 --> 00:56:43,630
The set of all controls that are match, so give and treatment observation. So I guess a raise that sort of trivial is the way before.

350
00:56:43,670 --> 00:56:52,520
But if we had two controls with identical axes or in the axes are also identical to axes on the treatment side,

351
00:56:53,090 --> 00:57:00,469
then that would be a case where we, you know, those, those two together would represent that set be a fixed number.

352
00:57:00,470 --> 00:57:04,640
M Or very by treatment subject. I'm going to sort of assume the latter.

353
00:57:04,640 --> 00:57:10,400
And if you're in the former, you just replace mine with them. So they're a little vague, bigger houses obtained.

354
00:57:12,800 --> 00:57:17,330
So it could be if you're sort of using a fixed number, you sort of find the controls with the smallest values.

355
00:57:18,620 --> 00:57:32,140
Or you could have, oh, my observations within a certain distance measure, you may want to comment on what the difference like y one might be.

356
00:57:32,150 --> 00:57:44,990
I would, I would say preferable to another. All right.

357
00:57:45,070 --> 00:57:49,870
So what could happen here? That can't happen here.

358
00:57:52,390 --> 00:58:01,060
Yes. I think like depending on, you know, if you have a certain observation that, you know, is very different from all the others.

359
00:58:01,780 --> 00:58:06,889
Right. You could have, you know, a certain kind of set of features.

360
00:58:06,890 --> 00:58:11,830
Right. Which, you know, ah, have a much greater distance rate until the person.

361
00:58:12,000 --> 00:58:15,010
Right. So these false values could be really big.

362
00:58:16,510 --> 00:58:23,620
Right. If we're fixing them. So if we say let's say five again, if you've got some treatment cases where there's not good matches in the controls,

363
00:58:24,070 --> 00:58:26,890
by the time you get to the fifth best match it look might look very different.

364
00:58:28,240 --> 00:58:31,990
Whereas if you're here, maybe there's, you know, one or two matches that look good.

365
00:58:32,530 --> 00:58:37,300
And so you would you just live with the two that are that sort of disease measure.

366
00:58:38,080 --> 00:58:44,070
There are some advantages. I mean, some advantages there's a sort of have to do things like maybe do a bit of averaging for some of the clothes,

367
00:58:44,080 --> 00:58:50,620
one variances, two meters for this, whereas if this is fixed, then it's not so the case.

368
00:58:50,620 --> 00:59:01,510
But, but in general, I sort of prefer the second approach unless you're really quite sure that you have a large number of good matches for everybody.

369
00:59:03,520 --> 00:59:08,080
So I think I lopped off a little bit of.

370
00:59:10,490 --> 00:59:13,880
An equation they didn't finish now. Um.

371
00:59:14,960 --> 00:59:18,650
But. Well, we'll fix it after this first.

372
00:59:19,250 --> 00:59:23,360
So you can estimate this counterfactual now or it's the average of all these.

373
00:59:26,550 --> 00:59:32,250
So instead of taking Y minus a single match, you take a Y minus the meat of these match values.

374
00:59:33,750 --> 00:59:43,590
And you can also rewrite that. These were the better.

375
00:59:50,910 --> 00:59:56,400
Time for the old man conversation. And you have every interview, every word.

376
00:59:57,060 --> 01:00:04,920
Okay, everybody, that's still for. I think that's a technology that it's just not been replaced.

377
01:00:04,930 --> 01:00:12,780
I mean, this is not as good as a chalkboard. I tried using the, you know, the electronic end of their lives with my handwriting.

378
01:00:12,930 --> 01:00:16,590
And I should check as I hadn't tried a couple of years, but with my handwriting.

379
01:00:17,310 --> 01:00:20,430
The control I have on these things is bad.

380
01:00:20,610 --> 01:00:30,600
IPA marginal. Nothing in the past is bad.

381
01:00:32,310 --> 01:00:36,750
As we talk. It doesn't mean we're out of it.

382
01:00:52,920 --> 01:00:56,160
So there.

383
01:01:03,290 --> 01:01:09,390
I'm not sure the treatment runs. Right after the.

384
01:01:12,910 --> 01:01:27,940
Actions. And then the ruins claim it and rewrite this as what's going on here.

385
01:01:29,680 --> 01:01:41,350
So it's. All right, so I have just our treatment.

386
01:01:42,860 --> 01:02:02,050
Ally is basically. The number of observations in the control arm that are part of this controlled control group.

387
01:02:03,100 --> 01:02:07,629
So I'm not really sure I see a big advantage to that.

388
01:02:07,630 --> 01:02:16,900
But if I am coding, I just used this, but it stays there too, so, so close to our various estimates.

389
01:02:17,970 --> 01:02:23,709
It's pretty straightforward, just the variance of the ways it requires and in the treatment and then the

390
01:02:23,710 --> 01:02:32,710
variance of the controls divided by the average number of controls that you use.

391
01:02:34,780 --> 01:02:42,910
And if you can use this ally notation and you set that to zero for cases, right, you kind of do sort of your I think with zero zero.

392
01:02:45,610 --> 01:02:52,660
Now the problem with this variance estimate or not, you'll see this in the example for the to to do that ignores multiple matching.

393
01:02:53,800 --> 01:03:00,340
So there's gonna be a correlation across pairs that's in this share the same control so we're not doing all the numbers

394
01:03:00,340 --> 01:03:05,490
small and they're not much overlap this variance estimate should be good and the actual sample I'm going to look at,

395
01:03:05,830 --> 01:03:19,140
that's not the case. So it doesn't turn out to be quite so good. So okay, so questions about that is pretty similar.

396
01:03:19,150 --> 01:03:25,420
This previous idea just, you know, you just can replace the single match with the mean of the multiple matches.

397
01:03:30,380 --> 01:03:34,160
Now we might want to wonder. So.

398
01:03:34,180 --> 01:03:39,440
So I guess one one thing that's important here I didn't I didn't emphasize it earlier,

399
01:03:39,440 --> 01:03:43,280
probably should have, is that we're just looking at the treatment effect among the treated.

400
01:03:44,060 --> 01:03:50,390
Right. We've really just been looking we're looking at this causal effect, but only among those individuals that receive the treatment.

401
01:03:51,800 --> 01:03:57,350
Right. And randomized trials. And that's fine because those basically should be exchangeable with the people in control.

402
01:03:57,350 --> 01:04:00,649
Right. And just trying to to do that, the randomization.

403
01:04:00,650 --> 01:04:06,260
But in Non-Randomised studies, there might be some systematic differences between the folks who got the treatment and got it.

404
01:04:06,560 --> 01:04:09,740
They didn't get the treatment. Even if you're getting at this causal effect,

405
01:04:09,740 --> 01:04:16,040
it's sort of even this idea of the effect modification that that the treatment might be a little bit different and different

406
01:04:17,000 --> 01:04:23,360
in different individuals and different types of effect of the treatment may be different in different types of individuals,

407
01:04:24,290 --> 01:04:33,500
but you can run the scheme to the way around, right? We can essentially go back and match treated cases to controls to get our estimate for

408
01:04:34,410 --> 01:04:47,090
why one the initial outcome and for individuals in the control group so so everything

409
01:04:47,210 --> 01:04:53,240
kind of just runs the same way as of now we're summing over the over the control

410
01:04:54,170 --> 01:05:01,190
individuals and subtracting their value off from the mean of the match treatments.

411
01:05:04,270 --> 01:05:10,000
Yes. So before we were looking at a treatment for be treated, we needed more controls and treated.

412
01:05:10,030 --> 01:05:14,229
So we wanted to look at it. Controls would be reversed.

413
01:05:14,230 --> 01:05:19,510
You need more treated the control. But because of doing that, you wouldn't have to worry about that anymore.

414
01:05:19,720 --> 01:05:22,900
Okay. So we're going to allow multiple matches.

415
01:05:24,280 --> 01:05:33,490
So so that does avoid that problem at the cost of introducing a lot of potential correlation.

416
01:05:35,590 --> 01:05:40,140
So. Okay.

417
01:05:40,150 --> 01:05:43,900
So that's. But, yes, that's a that's a good point.

418
01:05:44,110 --> 01:05:55,719
So obviously, you can't have a situation where you have multiple you have both Moore and Lobo.

419
01:05:55,720 --> 01:05:58,200
I guess it's not true that they're identical in size.

420
01:05:58,210 --> 01:06:06,910
And they would you could you could match, you know, the same 1 to 1 either way, but essentially perfect matches the whole way through.

421
01:06:07,060 --> 01:06:17,680
So. So what? But again, even even for this piece.

422
01:06:17,830 --> 01:06:28,030
Right. Because we're allowing multiple controls to have to go to an individual.

423
01:06:28,040 --> 01:06:34,060
We can we can sort of bounce this around. Right. That set group, different different individuals.

424
01:06:35,770 --> 01:06:39,790
And then we can combine these two to get an overall causal effect estimate.

425
01:06:41,350 --> 01:06:47,860
Right. To get the effect to the treated group and the control group.

426
01:06:49,300 --> 01:07:05,110
And then we want to weight them by because presumably, if this if this population is indicative of this sort of distribution of controls, then.

427
01:07:08,850 --> 01:07:13,710
And this is a squared term here that we should get that variance.

428
01:07:14,300 --> 01:07:22,640
Assuming correlate that ignoring correlation between the treated in control matches.

429
01:07:23,780 --> 01:07:31,689
There should be a squared there. We can get this. But that's a potential problem.

430
01:07:31,690 --> 01:07:41,030
Right? And that caught the guys I said are already with the correlations because we having the same individuals, different matches show up.

431
01:07:42,500 --> 01:07:48,360
So I'm actually going to suggest using bootstrapping here. So another little extra piece.

432
01:07:50,780 --> 01:07:59,300
Okay. So that's kind of the sort of generic description discussion that is going to then apply this.

433
01:08:00,560 --> 01:08:10,850
Any questions at this point? Those of you watching at home would just have to come to office hours.

434
01:08:14,900 --> 01:08:18,350
Okay. So, all right, you want to see a little bit of bootstrapping, right?

435
01:08:18,350 --> 01:08:24,020
So how many of you heard bootstrapping before? A few hands.

436
01:08:24,950 --> 01:08:36,350
So. So those of you that have it is basically this part of this is sort of a way that's very empirically sort of algorithmically driven.

437
01:08:36,370 --> 01:08:44,629
We estimate variance in very general settings. And basically the idea is that I'm not going to get into the theory,

438
01:08:44,630 --> 01:08:54,290
but sort of conceptual part of it is that you have your sample, then you essentially re sample from that sample with replacement.

439
01:08:55,610 --> 01:09:01,640
So some individuals will not appear in that sampling and others will appear multiple times.

440
01:09:02,780 --> 01:09:11,359
And then you take and get your point estimate from that re sampled dataset, and you do that many,

441
01:09:11,360 --> 01:09:18,559
many times and there are sort of different, different twists and turns on this.

442
01:09:18,560 --> 01:09:23,060
But the sort of most basic approach is to use the variability from those results.

443
01:09:23,210 --> 01:09:28,130
We sampled this four meters to estimate the variance of the statistic that you're actually interested in.

444
01:09:29,210 --> 01:09:33,320
It tends to be conservative, tends to sort of overestimate variance in a lot of cases,

445
01:09:34,040 --> 01:09:40,400
but it's a good way to kind of deal with correlation and other complications when it's would be very hard to deal with analytically.

446
01:09:40,700 --> 01:09:49,790
So it's huge and sort of considered one of the maybe the single most important innovation in statistics in the second half of the 20th century,

447
01:09:50,870 --> 01:09:57,770
because it does sort of, you know, wow, practical variance estimation in a lot of cases where it would have been hard before.

448
01:09:58,440 --> 01:10:03,590
So also a lot of people sort of think very you know,

449
01:10:03,770 --> 01:10:09,190
it sort of opened the floodgates for thinking about estimation techniques because really where we're just focused,

450
01:10:09,200 --> 01:10:13,580
point estimation is sort of the variance of that, which maybe that's been overdone.

451
01:10:13,760 --> 01:10:18,140
Maybe that's the case. But but that's the that's the situation.

452
01:10:18,890 --> 01:10:21,890
So basically here in this case, it's basically I'm doing here, right?

453
01:10:21,890 --> 01:10:26,390
So from each set of treatment controls you we sample with replacement and.

454
01:10:29,420 --> 01:10:38,990
So. So I did notice within each set of treatment and controls, what if I just ignored treatment and control and just sampled the whole sample?

455
01:10:39,080 --> 01:10:53,310
What would that imply? But I would just be like the central government here, so we would be less vague on that question.

456
01:10:55,110 --> 01:11:00,630
Would I no longer be conditioning on if I did, that wasn't the assignment right?

457
01:11:01,110 --> 01:11:03,089
So I'm going continue to condition on the assignment,

458
01:11:03,090 --> 01:11:11,160
particularly the which is what I want to do in this setting and also one of the conditions on the sample size in each of those two assignments.

459
01:11:11,700 --> 01:11:16,380
So I'm only going to re sample within the treatment control group.

460
01:11:17,760 --> 01:11:25,920
So once I've done that, then I just go ahead and compute these matched estimates and treatment control, combine them.

461
01:11:27,930 --> 01:11:39,120
So I just noted this spectrum condition on that and then I just do this a bunch of times with a bunch, typically at least 200 for variance estimation.

462
01:11:39,120 --> 01:11:44,340
If you're going to use things like empirical percentiles, you might want to go more toward thousand.

463
01:11:45,000 --> 01:11:51,840
Obviously more is better, but also takes well. You'll see that I have finally finished the bootstrapping results,

464
01:11:51,840 --> 01:11:58,110
which I had not finished last night when I sent out where Monday night when I sent out the the notes.

465
01:11:58,320 --> 01:12:04,350
So it because it it took several hours to do so.

466
01:12:06,150 --> 01:12:11,850
Then again, you get this sort of you just look at the empirical variance, right, where you could use percentiles.

467
01:12:13,470 --> 01:12:17,010
I did 200, so I'm just going to work with the normal approximation from the variance.

468
01:12:19,970 --> 01:12:24,080
Okay. So that's bootstrapping. All right.

469
01:12:24,080 --> 01:12:30,320
So under our example, all right. So in this case, my treatment is, are the kids in the car seat?

470
01:12:31,070 --> 01:12:38,450
I have not quite to this not quite twice as many in the control group.

471
01:12:38,450 --> 01:12:40,790
So I do have enough donors to do a 1 to 1 match.

472
01:12:42,410 --> 01:12:50,210
So I'm actually using this mahalanobis distance and the greedy algorithm and I got an effect estimated that's actually now smaller,

473
01:12:51,710 --> 01:12:55,820
still in the direction I expect, but no longer reaching statistical significance.

474
01:12:57,380 --> 01:13:13,570
But is this exactly comparable with the previous estimate or. Yes or no?

475
01:13:21,470 --> 01:13:26,720
So my previous estimate was this overall causal effect using the propensity score approach.

476
01:13:29,060 --> 01:13:32,090
Isn't different because those are specifically the affected on the train.

477
01:13:32,270 --> 01:13:38,630
Precisely. Right. So a little hint is, whenever these questions the answer to these kinds of questions, there's always no.

478
01:13:40,340 --> 01:13:43,340
I read somewhere that in the headlines, that's also the thing.

479
01:13:43,340 --> 01:13:51,739
You know, it's like a newspaper. If you're reading on the Internet, whenever there's a question on the headline, the answer is no.

480
01:13:51,740 --> 01:13:55,850
It's not always true. And it's different because it's right.

481
01:13:55,850 --> 01:14:01,720
It's the treatment effect among the treated. So spoiler alert.

482
01:14:02,530 --> 01:14:07,170
I mean, who? The control group. It doesn't change much. So. Okay.

483
01:14:07,180 --> 01:14:10,230
So there may be some packages out there.

484
01:14:10,240 --> 01:14:12,670
There's definitely packages that calculate the distance,

485
01:14:12,670 --> 01:14:20,650
but not using the difference in the in the at least easily that I could see in the in the vectors themselves.

486
01:14:20,740 --> 01:14:26,080
So I just wrote my own little very badly designed function.

487
01:14:26,380 --> 01:14:34,570
I probably should have that in there as well, but I didn't. So that's going to be whatever that is in the first of all, I should add also,

488
01:14:34,570 --> 01:14:45,460
I taught myself as plus in 1996 during the summer, so I just moved down to ah.

489
01:14:45,640 --> 01:14:50,020
So I have not have the benefit of the kind of advanced training many of you will have.

490
01:14:50,020 --> 01:14:54,640
So you will see a lot of garbage code. But it works for me.

491
01:14:55,690 --> 01:14:58,810
I'm not saying it's the right way to do it, but it is a way to do it.

492
01:14:59,680 --> 01:15:03,400
Okay, so I'm going to generate my my covariance matrix, right?

493
01:15:04,570 --> 01:15:09,610
I kept this is continuous. Well, not re continuous, but it's of, you know, word for level, categorical variable.

494
01:15:10,300 --> 01:15:16,840
And then these are all dichotomous, maybe not ideal because they're sort of in line normality assumption,

495
01:15:16,840 --> 01:15:24,730
but we do have a large sample so we can kind of get away with it using the same propensity model I've been using.

496
01:15:26,020 --> 01:15:34,650
So I wanted to reorder my my data to match the propensity score.

497
01:15:34,660 --> 01:15:39,580
Right. We sort of talked about the fact that you get more efficient matching machines if you use a simple greeting with one algorithm.

498
01:15:40,360 --> 01:15:46,329
We want I want to want to do that. So so I had to do that.

499
01:15:46,330 --> 01:15:50,290
I had to bring this through for all of the things I would use.

500
01:15:50,290 --> 01:15:54,850
So my career, it's my outcome and my treatment.

501
01:15:56,380 --> 01:16:02,410
And then I separated out data sets for folks in treatment and the folks in control looked at my.

502
01:16:07,660 --> 01:16:12,580
So I wanted to complete well, I don't know if I really need this, but I did compute the means to use them.

503
01:16:13,210 --> 01:16:18,640
So what I did do is, however, compute this variance covariance matrix, right?

504
01:16:18,640 --> 01:16:21,990
So I get the covariance matrix for x1 coherence for x zero.

505
01:16:22,870 --> 01:16:26,260
Take take the mean of the two in inverted.

506
01:16:28,020 --> 01:16:31,300
And I'm only going to use mahalanobis distance here. I didn't use Euclidean.

507
01:16:31,630 --> 01:16:42,670
We could certainly play with them and see what happens. So I compute is now how much distance to turn matching.

508
01:16:44,050 --> 01:16:50,710
So here I just wanted to say I have no N1 observations on treatment in our control.

509
01:16:52,840 --> 01:16:56,590
I need to do a little trickier and realize I don't have IDs on these things.

510
01:16:57,310 --> 01:17:02,260
So I created a little I.D. on the control group to allow me to do to do the matching.

511
01:17:05,690 --> 01:17:13,760
Okay. So here's the meat of the matching algorithm. So I'm going to go through each of the three observations,

512
01:17:15,530 --> 01:17:25,309
and I'm just creating a little vector here each time that kind of holds the smell of this distance for for the case.

513
01:17:25,310 --> 01:17:29,450
And now each time I'm going to go through, I'm going to keep shrinking my next night.

514
01:17:30,110 --> 01:17:40,940
So I have to play a little game here. So I shrink this vector each time I go through that, each time I take a case out of the control group,

515
01:17:42,290 --> 01:17:44,360
and then I'm going to run through the control observations,

516
01:17:45,440 --> 01:17:57,920
fixing my current treatment X and then looking at the melanomas distance with respect to the J control.

517
01:17:58,580 --> 01:18:02,780
And I have a little extra piece here because I had to create that little index.

518
01:18:02,900 --> 01:18:07,400
So I only want to look at the X's that are actually real X's.

519
01:18:09,790 --> 01:18:15,250
So. All right. So then I figure out and I could have done.

520
01:18:16,030 --> 01:18:20,230
So there are usually potentially a bunch of observations here. So they're all identical.

521
01:18:20,620 --> 01:18:25,660
I don't think there's a good way to. There's pretty much sort of randomized difference within the data set.

522
01:18:25,670 --> 01:18:31,750
So I didn't worry about picking one at random. I just sort of picked the first the first one that would pop up from this.

523
01:18:34,930 --> 01:18:44,430
Yeah. Well, I'm sorry that this is just comparing them the. The minimum distance.

524
01:18:44,430 --> 01:18:51,600
I'm sorry. That's our agreement. So we'll be using this. This is sort of just this because you don't have to do this for the matching.

525
01:18:51,600 --> 01:18:58,200
I just want to do this to kind of show how the actual my whole was distances changed with respect as as I went through the list.

526
01:18:58,200 --> 01:19:02,909
So that was that was just sort of a little instructional piece. So the actual matching is right here.

527
01:19:02,910 --> 01:19:06,540
So I went in and just ordered those by these values.

528
01:19:06,540 --> 01:19:09,270
Oftentimes there are ties and I just would pick a where the first one was.

529
01:19:10,230 --> 01:19:16,680
And, and so this one is actually the ID, so I'm putting it in this whole J thing.

530
01:19:19,830 --> 01:19:29,100
And then I wanted to see which observation I actually took out and I dropped that cycle through again and again.

531
01:19:29,160 --> 01:19:36,920
So each time I went through looking to see which x sub j matches best with my x, I say, okay,

532
01:19:36,920 --> 01:19:46,830
this was this observation get rid of the next j on the x zero and keep going through till I end up with this little.

533
01:19:49,960 --> 01:19:55,090
Syria was going to shrink down to just become the number of controls, minus a number of cases at the end.

534
01:19:56,170 --> 01:20:01,610
Still a couple thousand observations. Okay.

535
01:20:02,350 --> 01:20:07,329
So so how will this work? So I was I was holding on to this matchy thing.

536
01:20:07,330 --> 01:20:10,570
So you can see where these are zeroes is basically these all the X's are identical.

537
01:20:11,680 --> 01:20:16,420
I think one basically sort of means roughly about one opposite. One of those nine things didn't match.

538
01:20:17,170 --> 01:20:20,400
There were some a little tougher than near the end.

539
01:20:20,410 --> 01:20:25,960
I was completely sort of running when the things bashed really well with the ones that didn't sort of had more serious problems.

540
01:20:27,310 --> 01:20:30,460
So but it did seem to work pretty well.

541
01:20:31,570 --> 01:20:35,710
My difference in age was that almost a year shrunk to about a third of the year.

542
01:20:36,550 --> 01:20:43,060
The percent total way about a 70% difference basically went to almost zero and the airbag difference went to complete zero.

543
01:20:43,330 --> 01:20:50,430
So it matched perfectly with things does. So.

544
01:20:52,830 --> 01:20:57,540
So I used this multiple matching with a caliper match of I'm sorry.

545
01:20:57,550 --> 01:21:03,690
So that's the, that's the control group and they get it fully. Let's see.

546
01:21:03,990 --> 01:21:08,850
Kind of running out of time here. All right.

547
01:21:08,860 --> 01:21:12,210
So I'll touch on this. The beginning of class next week.

548
01:21:12,260 --> 01:21:15,600
People don't read it, but I just mentioned before.

549
01:21:16,470 --> 01:21:24,780
So, okay, I have a box out tomorrow, so I'll be seeing you then.

550
01:21:25,230 --> 01:21:28,650
Not blending in the homework to Friday.

551
01:21:30,000 --> 01:21:35,130
Sure, I'll get it right way. But is there a chance that you might not at night?

552
01:21:36,120 --> 01:21:37,920
I think I could do that. Okay. All right.

553
01:21:38,430 --> 01:21:48,360
I'll move the deadline to 1159 that Friday, since I initially thought about having done so, I actually was going to have it do with you.

554
01:21:49,140 --> 01:21:52,290
So you got a bit of a break, which is why I said I'd do it. And I did.

555
01:21:52,290 --> 01:21:59,390
I screwed up the date, so that's fine, right? Maybe we'll keep going that point forward.

556
01:21:59,880 --> 01:22:09,870
This allows you to 2000000.

557
01:22:10,600 --> 01:22:10,920
Because.

