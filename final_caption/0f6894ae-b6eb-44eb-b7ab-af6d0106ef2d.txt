1
00:00:09,330 --> 00:00:46,320
Yeah. Such an interesting survey.

2
00:00:46,500 --> 00:00:50,040
Everybody's got its shares and two empty chairs.

3
00:00:50,050 --> 00:00:57,600
Three sets scared people off or their own kids.

4
00:01:00,960 --> 00:01:05,760
All right. Good morning. Good morning. Excited to be here?

5
00:01:05,790 --> 00:01:11,610
Yes. In a bid to watch the Golden Globes last night, nobody.

6
00:01:11,970 --> 00:01:15,090
There's 80 people in this room. Nobody wants to go alone. Okay.

7
00:01:16,110 --> 00:01:20,820
We want us to just extend it anyway.

8
00:01:20,850 --> 00:01:27,480
Not that it's relevant to the world. It's fun to look at people and criticize for commentary.

9
00:01:29,320 --> 00:01:34,410
Anyway, Hallmark homework is now due on Monday.

10
00:01:34,980 --> 00:01:40,110
A lot of you should be able to have it done by Friday. If you don't want to spend your weekend working on it, please do.

11
00:01:40,110 --> 00:01:47,170
So there's no penalty if you want to wait until we've talked about this material today to do the homework assignment,

12
00:01:47,260 --> 00:01:53,069
but we still can't find a room for my office hours as a guest for this class.

13
00:01:53,070 --> 00:01:58,740
So she doesn't have office hours today, but I think that's going to materially affect anybody.

14
00:01:58,740 --> 00:02:00,190
If it does, please let me know.

15
00:02:01,380 --> 00:02:08,640
But as soon as this behemoth associates can find a room that's available for an hour and a half for lunch, I will let you know.

16
00:02:09,270 --> 00:02:17,130
My office hours will be happening tomorrow, 330 in my office and ask two questions.

17
00:02:20,600 --> 00:02:24,080
Other things, issues, concerns.

18
00:02:24,920 --> 00:02:29,780
If you're here on the way, you're still on the wait list and you're here again.

19
00:02:29,780 --> 00:02:33,080
I apologize to every one of your friends and yourself.

20
00:02:33,720 --> 00:02:37,780
Camera two I have no power over this.

21
00:02:37,790 --> 00:02:44,450
As you can see, this room is full. I can't increase the cap because it's just not going to work right now.

22
00:02:45,320 --> 00:02:51,920
So I apologize to a couple of people who may not ever get it on this bus as a distinct possibility.

23
00:02:54,460 --> 00:02:59,990
I wouldn't leave. It's because I'm awesome and everyone wants to be here. I and a lot of you need to graduate.

24
00:03:00,530 --> 00:03:05,690
So that is a real concern. But this room does have a cap and we do have to stick with.

25
00:03:09,820 --> 00:03:14,170
No class on Monday should all be going to the symposium.

26
00:03:14,290 --> 00:03:21,490
The stuff we have going on for Martin Luther King. Let's use your time as you feel you need to use it.

27
00:03:25,210 --> 00:03:30,340
All right. Let's talk about water hearts and make some tests.

28
00:03:30,390 --> 00:03:34,450
I will get to the survey. Some of you may have already tried to go in and answer the survey.

29
00:03:35,200 --> 00:03:38,770
That's cool, but I will check. Welcome.

30
00:03:39,880 --> 00:03:44,490
Morning. All right, we'll scream.

31
00:03:47,340 --> 00:03:55,510
So I hope that as this class goes along right now, it should seem pretty straightforward.

32
00:03:56,110 --> 00:04:00,880
We're taking mostly T tests, compared T tests, chi square tests,

33
00:04:01,030 --> 00:04:07,600
and we're showing you an alternative to those methods that don't require normality as an assumption.

34
00:04:08,810 --> 00:04:13,990
So again, hopefully the first homework assignment is not too arduous and is mostly a review with a little add on.

35
00:04:15,070 --> 00:04:18,459
So we're going to talk about the classic statistics problem. We'll start with this.

36
00:04:18,460 --> 00:04:23,260
I've got two groups of individuals or two populations, and I want to go through different groups.

37
00:04:24,700 --> 00:04:26,770
And so in order to do that, non parametric,

38
00:04:26,920 --> 00:04:34,690
we're going to start with the concept of a rank which again you won't have enough training here and I asked you to rank five observations.

39
00:04:34,690 --> 00:04:42,340
You could do that right now. You don't need a degree in statistics to rank numbers, but we like formulas and statistics.

40
00:04:42,730 --> 00:04:51,520
So if I have a vector of observations X one through x and then each of them as a rank are survived for the eighth observation.

41
00:04:52,090 --> 00:04:53,890
And we define it by this formula here.

42
00:04:54,250 --> 00:05:02,920
So every observation has a rank that is simply a sum of the number of observations that are as large as their value or larger.

43
00:05:05,080 --> 00:05:14,680
So again, just basic indicator function. Again, a very, I think, needless mathematical formula, but we can give you one for what a rank means.

44
00:05:15,190 --> 00:05:24,250
It's simply a sum of indicator variables. So to be clear here, rank is defined across the vector x.

45
00:05:24,790 --> 00:05:28,240
Each observation has a rank that is a function of the entire vector.

46
00:05:29,050 --> 00:05:32,470
This makes sense. I can't rank an observation by itself.

47
00:05:32,560 --> 00:05:37,390
It's a relative quantification relative to everybody that's in that sector.

48
00:05:38,080 --> 00:05:47,350
So it ranks as we're going to get to are dependent on the rank of one observation is dependent upon the rank of another after another space.

49
00:05:47,630 --> 00:05:54,410
So we use that notation. There are supply of x the vector, but it is a computation for one observation.

50
00:05:54,430 --> 00:05:58,150
So again, if we have unique values, we want to receive unique values in our dataset.

51
00:05:58,900 --> 00:06:04,030
But of course then the largest observation has a rank of RN and the smallest observation has a rank of one.

52
00:06:04,030 --> 00:06:09,669
I cannot lecture in a scarf with a choke here.

53
00:06:09,670 --> 00:06:17,500
All right. So smallest as one up through the integer and for the largest observation function and our called rank.

54
00:06:19,090 --> 00:06:25,300
This is our package there. So there I give you an example of five observations, three, seven, one, 12 and six.

55
00:06:25,900 --> 00:06:29,080
And as you say, rank of that vector, it gives you back to the ranks.

56
00:06:29,350 --> 00:06:34,059
So again, three is the second in order of the observations.

57
00:06:34,060 --> 00:06:40,420
So it is rank two and so forth. And you've all set in order statistics or statistics are tied up with ranks.

58
00:06:40,930 --> 00:06:51,970
The first order statistic as rank one with the second statistics, rank two they are all tied together, but you can get the ranks of observations.

59
00:06:53,590 --> 00:06:59,290
One issue, though, is that in most data sets there aren't necessarily going to be all unique values.

60
00:06:59,290 --> 00:07:05,470
There will be what we call ties. I mean, any time we have observations that take the same value.

61
00:07:06,370 --> 00:07:10,720
So in the definition that I just gave you for a mathematical computation of rank,

62
00:07:12,200 --> 00:07:16,480
every observation that's tied will get the maximum possible rank you could have.

63
00:07:18,400 --> 00:07:26,350
So, for example, if I have a vector of ten, seven, seven and five, I think we'll all agree that five has the smallest rank.

64
00:07:28,150 --> 00:07:33,160
And look at rank one, ten gets rank four. What do we do with these two folks?

65
00:07:33,160 --> 00:07:36,280
They're both sevens. There's a two and a three in there.

66
00:07:36,280 --> 00:07:39,370
There's ranks two and three there. We have two assigned to those two observations.

67
00:07:39,370 --> 00:07:40,020
What do we do?

68
00:07:41,110 --> 00:07:49,270
The formula I just showed you said that seven gets a rank three because again, rank is the number of observations equal to or greater than that value.

69
00:07:49,780 --> 00:07:53,590
There are three values in this dataset that are equal to or greater than seven.

70
00:07:54,310 --> 00:08:02,020
So each of those observations would get a rank of three under the definition that I just showed you, this is a typically what we do.

71
00:08:02,020 --> 00:08:09,130
However, again, if you want to follow that idea in R, there is an argument in rank.

72
00:08:09,270 --> 00:08:10,600
Cult ties that method.

73
00:08:11,800 --> 00:08:17,950
And if you say twice that method equals max, then you get back a four, three, three and one for the ranks of those four observations.

74
00:08:19,300 --> 00:08:23,290
That is not the default, though. The default for taste that method.

75
00:08:23,300 --> 00:08:27,550
In fact, until I started teaching this class again, I forgot this even argument even existed in racks.

76
00:08:28,720 --> 00:08:31,180
The default is to compute the average.

77
00:08:32,620 --> 00:08:40,179
And so if you just say rank of X with those four observations there, you'll see that sevens get an average rank.

78
00:08:40,180 --> 00:08:43,750
There's a two and a three in the ranks and we take the average of those two.

79
00:08:43,760 --> 00:08:48,700
We call them in rank. The sevens get a value of two five.

80
00:08:49,690 --> 00:08:52,450
This is the standard approach used in most of the rank tests.

81
00:08:52,450 --> 00:08:59,560
In fact, I don't ever remember seeing a ranked test presented any other way, so that's why it's the default.

82
00:09:01,510 --> 00:09:05,680
Another example to make clear what we're talking about here is a vector of seven observations.

83
00:09:05,680 --> 00:09:09,940
We see two types, there's two sevens and there are two twos.

84
00:09:10,780 --> 00:09:19,540
And so I have now told R to give me two rows. One is ranks using the max rank and the second row is the rank using the the average sort of in rank.

85
00:09:19,940 --> 00:09:29,560
And so again, you can see that four two sevens would get rank six and five and six excuse me.

86
00:09:30,160 --> 00:09:38,320
So the max rank would be six and the average is the average of five and six or 5.5 mid rank and so forth for the two twos.

87
00:09:39,610 --> 00:09:42,519
And this holds forward with any number of observations.

88
00:09:42,520 --> 00:09:52,030
If I have three times again three, five, six and seven and then those observations would all get rank six increment of those three values.

89
00:09:52,350 --> 00:09:58,180
All right. So against another way of saying here is the tied ranks are the average of the two adjacent ranks.

90
00:09:59,190 --> 00:10:07,089
We will put in those values and the average approach, it says for the most common way, it's the only way I know when we do wilcoxon rank.

91
00:10:07,090 --> 00:10:13,600
So statistics, wilcoxon side rank, statistics, any rank based method, we typically use the metrics.

92
00:10:13,600 --> 00:10:21,850
Again, it's just important for the theory. Talk about the distribution of these ranks in a second and how you handle ties affects the distribution.

93
00:10:21,850 --> 00:10:25,540
What's the probability of a rank 2.5, for example?

94
00:10:27,400 --> 00:10:32,290
So there we are. What do we know about ranks? And ranks are random variables.

95
00:10:32,680 --> 00:10:40,300
They have distributions. So if I have a vector x one through x and from a continuous distribution.

96
00:10:40,510 --> 00:10:42,610
So we're going to get away from the world of ties.

97
00:10:42,610 --> 00:10:50,410
Right now, if we believe values come from a continuous distribution and the probability that they get exactly the same number twice is zero.

98
00:10:50,920 --> 00:10:54,540
Right. This measure zero on that with two periods, huh?

99
00:10:55,060 --> 00:11:00,670
As a result, each of these ranks follows a discrete uniform distribution.

100
00:11:00,670 --> 00:11:05,320
The probability that any rank takes a number one through n is one over n.

101
00:11:05,530 --> 00:11:09,760
They're all equally likely right with given no other information.

102
00:11:09,970 --> 00:11:15,100
The probability that an observation has rank one or rank two or rank three is simply one of.

103
00:11:16,380 --> 00:11:24,770
That's a discrete uniform distribution. And therefore the expectation of a rank is simply a formula of expectation.

104
00:11:24,770 --> 00:11:31,730
That's one over n times the sum of the first n integers, and we all know the sum of the first n integers is n times and plus one over two.

105
00:11:32,600 --> 00:11:37,940
And therefore the average rank makes sense. Is the average of the lowest rank and the highest rank.

106
00:11:39,130 --> 00:11:44,650
And that is the mean of a discrete uniform distribution of the upper bound divided by two.

107
00:11:46,420 --> 00:11:57,820
And likewise, the variance of the ranks is the formula for variance, and it involves the sum of the first integer integer squared.

108
00:11:58,480 --> 00:12:01,600
And we all know there's a formula for the sum of the squares of the integers.

109
00:12:02,140 --> 00:12:06,820
And if we go through a little bit of algebra beyond that step there, you'll get n squared minus one over 12.

110
00:12:07,270 --> 00:12:11,710
The square of the upper bound of the uniform distribution. Minus the square of the lower bound divided by 12.

111
00:12:11,920 --> 00:12:20,310
That's the variance of the distribution. So even if I didn't tell you, was uniform in mathematics produced that point.

112
00:12:20,770 --> 00:12:24,820
However, as I said earlier, these are not ID.

113
00:12:26,500 --> 00:12:29,620
Our ones who are in are not independent. And why?

114
00:12:30,070 --> 00:12:31,810
Because, again, ranks are relative.

115
00:12:32,320 --> 00:12:38,860
I can't figure out the rank of one observation in the rank of any one observation impacts the ranks of the other observations.

116
00:12:40,170 --> 00:12:45,850
However, we're going to get into this as we talk about permutation tests, the entire vector ranks.

117
00:12:45,880 --> 00:12:54,580
So this is the vector here is simply some shuffling of the integers one through n every observation is going to get some number one through.

118
00:12:54,610 --> 00:13:03,730
And as a rank, that vector is simply one of the factorial possible ways that I can compute these numbers.

119
00:13:05,030 --> 00:13:08,430
And so you can think of every shuffling of these integers.

120
00:13:09,240 --> 00:13:13,500
There's a distribution out there, every possible permutation of the first and integers.

121
00:13:14,910 --> 00:13:18,600
My ranks are one realization of all and factorial of those.

122
00:13:19,710 --> 00:13:28,140
And that will become useful for permutation tests. I can think about promoting my data to create a distribution for anything that I'm doing with Rex.

123
00:13:30,490 --> 00:13:36,940
Into homeless down a little bit further. Although each rank has marginal probability one over end.

124
00:13:37,240 --> 00:13:46,950
You can see that the ranks have joint probabilities. So the probability of any two ranks and joint is simply a conditional times a marginal rate.

125
00:13:47,880 --> 00:13:49,860
This has marginal probability one over ten.

126
00:13:50,980 --> 00:13:56,469
Once I know the rank of one observation, there are only one minus one possible ranks for the other observation.

127
00:13:56,470 --> 00:14:02,560
Next observation. So this is probability one and minus one, giving us that probability right there.

128
00:14:03,850 --> 00:14:07,450
And I want to move in to getting what we think about the correlation of these things.

129
00:14:07,570 --> 00:14:10,570
This will be important in a few slides.

130
00:14:11,560 --> 00:14:16,360
So the sum of the first and integers is N plus one over two.

131
00:14:16,360 --> 00:14:21,130
I want to call it T. It's a constant that has no variance.

132
00:14:22,290 --> 00:14:29,890
So some is zero. We know that. However, this is also the sum of ranks.

133
00:14:30,790 --> 00:14:35,050
Some of the first and integers is also the sum of the possible ranks in my dataset.

134
00:14:36,280 --> 00:14:39,700
So we can use that to think about covariance.

135
00:14:40,630 --> 00:14:45,000
So we know that the variance of the sum of all the ranks has to be zero.

136
00:14:45,460 --> 00:14:48,190
We also know that the sum of a bunch of random variables,

137
00:14:48,250 --> 00:14:54,550
the variance of a sum of random variables is the sum of the variances plus the sum of all the currencies.

138
00:14:55,840 --> 00:14:57,970
And we know that each rank has the same variance.

139
00:14:58,840 --> 00:15:04,720
So there's and of these being added together and there's end times N minus one of these things being added together.

140
00:15:05,960 --> 00:15:09,090
So these are all the same. Every time or not forever.

141
00:15:09,090 --> 00:15:10,800
I think the answer is the same.

142
00:15:11,700 --> 00:15:18,180
And so we do all the algebra and we find out that the ranks, again, they're not independent, they have negative covariance.

143
00:15:18,420 --> 00:15:23,240
It is ten plus one over 12. It's to do with that.

144
00:15:23,420 --> 00:15:29,030
I don't know what covariance means as well as I do know the ratio of covariance to variance.

145
00:15:29,030 --> 00:15:36,169
That's correlation. So the correlation of any two ranks is again there covariance divided by the square

146
00:15:36,170 --> 00:15:42,950
root of the product of their variances and they both have the same variance. So it's just that sure not to be negative one over n minus one.

147
00:15:44,360 --> 00:15:49,309
So rents are correlated negatively, right? As an observation becomes more and more likely at a higher rank.

148
00:15:49,310 --> 00:15:54,470
The observer other observations are going to get lower ranks. They go in opposite directions just a little bit.

149
00:15:55,340 --> 00:16:00,860
And in fact, what's important is that this correlation is a function of the sample size.

150
00:16:00,860 --> 00:16:07,190
And if you have enough data, this correlation is essentially zero for officers.

151
00:16:07,760 --> 00:16:12,500
This correlation is zero. So that I really now look at I'm looking at a sum of idea, random variables.

152
00:16:12,770 --> 00:16:19,280
The sum of the ranks can be thought of as a sum of add a little bit kind of right and that's useful.

153
00:16:19,280 --> 00:16:23,720
We know lots of theorems about the sum of 80 random variable, so I haven't discerned.

154
00:16:24,380 --> 00:16:29,360
If correlation is zero, that does not mean it is independent trait other than normal random variables

155
00:16:30,150 --> 00:16:33,950
by adding a little hand-waving there with zero correlation and dependance.

156
00:16:33,950 --> 00:16:42,049
Yes. Well we'll, we'll talk about how this leads independent studies and this will help us again.

157
00:16:42,050 --> 00:16:45,560
I'm never going to quiz you on this. I'm not going to ask you to prove this.

158
00:16:46,160 --> 00:16:50,480
It's just going to help us because I guess and we will do even more hand-waving later.

159
00:16:50,790 --> 00:16:54,019
Right. So, yes, I have not proved independence.

160
00:16:54,020 --> 00:16:58,700
I mean, I can't prove that asymptotically they're independent, proved asymptotically that they have correlations here.

161
00:17:01,010 --> 00:17:08,740
What is the Wilcoxon rank sum test? It is based on the sum of ranks, and it was created by Mr. Wilcoxon.

162
00:17:09,340 --> 00:17:13,420
So there we have the name right there. It's also referred to.

163
00:17:14,770 --> 00:17:20,620
There is a test called the Mann-whitney EU test that I'll talk about a little bit today and a little bit later.

164
00:17:21,730 --> 00:17:28,180
MANN And Whitney also came up with a test statistic for comparing two populations to see if they're different or not.

165
00:17:28,780 --> 00:17:34,270
It turns out that their statistic and Mr. Wilcoxon statistics are simply formulas of each other.

166
00:17:34,600 --> 00:17:37,600
They're just scales and shows that each other. So they're the same test.

167
00:17:37,960 --> 00:17:45,040
So there's the man. When you use the wilcoxon like some some people talk about the wilcoxon mann-whitney do you want to give everybody credit?

168
00:17:45,550 --> 00:17:52,780
But I'll refer to it as though wilcoxon like some test. And this is a test used really to answer the question.

169
00:17:52,930 --> 00:17:58,270
Do observations from one group tend to be larger or smaller than observations from another group?

170
00:17:59,330 --> 00:18:07,010
That's really the question of interest of interest rate. If you picture if you can picture two distributions drawn on a piece of paper is one

171
00:18:07,010 --> 00:18:12,020
of them moving to the right from the other is one sitting underneath the other one.

172
00:18:12,620 --> 00:18:16,669
Those sorts of questions. So again, back to our assumptions here.

173
00:18:16,670 --> 00:18:20,209
We have a bunch of idea observations from one distribution,

174
00:18:20,210 --> 00:18:30,110
one population affects and of those and we have a sample of observations and of those from another population or another distribution,

175
00:18:30,110 --> 00:18:34,730
we don't know what they are. We don't want to specify them if we don't have to.

176
00:18:35,900 --> 00:18:37,160
So roughly speaking,

177
00:18:37,190 --> 00:18:45,680
a one sided wilcoxon rank sum test is trying to determine whether or not the two populations are the same in terms of their seeds.

178
00:18:46,280 --> 00:18:51,350
Or is it? Observations from one of the groups tend to be larger than observations from the other groups.

179
00:18:51,890 --> 00:18:57,170
That's pretty vague. There's no parameters. This is not parametric.

180
00:18:57,590 --> 00:19:02,750
So what do we mean by tend to be larger when we talk about the alternative hypothesis?

181
00:19:04,070 --> 00:19:07,880
Again, the easiest way to think about it is let's start with number two.

182
00:19:09,380 --> 00:19:17,330
The shift alternative simply says the two distributions have essentially the same shape, but one of them is moved to the right after the other.

183
00:19:17,600 --> 00:19:21,110
I could have two exponential distributions. One is just to the right of the other.

184
00:19:21,110 --> 00:19:27,020
Two normal distributions. One moves to the right of the left of the other in terms of its location parameter.

185
00:19:28,900 --> 00:19:33,420
This is a specific example of what is known as stochastic dominance,

186
00:19:33,430 --> 00:19:41,230
and that just says that the two groups are equal under the null versus one of them is stochastic, larger than the other.

187
00:19:43,150 --> 00:19:47,950
And again, you may have learned stochastic literature in one of your classes and never seen it ever, ever again.

188
00:19:48,310 --> 00:19:52,810
This is the only time in my life I've ever used this concept again in this class.

189
00:19:53,200 --> 00:19:57,910
And so a distribution function effects are said to be stochastic only larger if the CDF.

190
00:19:59,020 --> 00:20:02,979
Of this group is less than or equal to the CDF of the other group at all.

191
00:20:02,980 --> 00:20:07,580
T. And there must be strict inequality.

192
00:20:07,580 --> 00:20:16,250
At least one value of t. So think about two CDs going from 0 to 1 and one dominates the other.

193
00:20:17,060 --> 00:20:20,570
And maybe they touch each other or it's up and down.

194
00:20:21,940 --> 00:20:26,650
Impress each other with anger and a shift alternative.

195
00:20:26,920 --> 00:20:30,910
Like I said, if there's a shift alternative, you're going to have stochastic dominance.

196
00:20:33,850 --> 00:20:35,650
Again, if I go to a collaborator.

197
00:20:37,760 --> 00:20:42,380
And say to them, you know, we're going to talk and we're going to see if the two distributions are sarcastically ordered.

198
00:20:43,340 --> 00:20:48,530
We can forget about that collaboration, right? That's not really useful to most folks.

199
00:20:49,340 --> 00:20:52,850
So think about it. Can we tell you these things that I want you to think more deeply?

200
00:20:53,270 --> 00:20:57,320
Folks who use wilcoxon rank some tests all the time because they say, well, I don't want to think about normality.

201
00:20:58,170 --> 00:21:04,820
And what's your alternative? You're trying to accept in favor of or reject the null right to this alternative.

202
00:21:05,720 --> 00:21:11,660
So to make life a little simpler, sometimes we talk about the wilcoxon rank, some test being a test of equal medians.

203
00:21:12,020 --> 00:21:15,200
Do the two groups have the same median versus do they have different meanings?

204
00:21:17,480 --> 00:21:19,940
Most people can conceptualize that they know what a median is.

205
00:21:20,450 --> 00:21:27,530
This is true if we talk about the shift alternative because you observe the two distributions are just different way of shift.

206
00:21:29,630 --> 00:21:37,310
But just keep in mind that, you know, is it reasonable to think that if we have two distributions, they differ by a shift?

207
00:21:37,550 --> 00:21:40,550
That's the reason they're different. So you are making that assumption.

208
00:21:41,780 --> 00:21:45,680
And you could have, again, think of a scenario in which two groups have equal medians.

209
00:21:46,930 --> 00:21:52,570
But in fact, the Wilcoxon rank some test will be very, very large in terms of its statistic and reject them both.

210
00:21:53,680 --> 00:21:59,080
You can have two groups with two populations of equal medians, but their distributions are very different from each other.

211
00:22:00,940 --> 00:22:04,240
We could also reject their. Excuse me.

212
00:22:06,100 --> 00:22:16,330
So another way to think about the Wilcoxon sum test is it's comparing this alternative and let's look and let's get rid of the start right now.

213
00:22:16,540 --> 00:22:19,570
Let's just talk about this part here. We have a continuous distribution.

214
00:22:20,800 --> 00:22:27,910
The NULL says that if I take one observation from each population, it's a coin toss as to whether which one's bigger.

215
00:22:29,950 --> 00:22:37,150
And that would say that they probably come from the same group. If I going round to these observations and it's 5050 guess as to which one is bigger.

216
00:22:37,750 --> 00:22:39,340
And frankly, I have one population.

217
00:22:40,330 --> 00:22:47,020
But if this probability doesn't matter which one I'm talking about here, if I pull one observation from one group and from the other,

218
00:22:47,350 --> 00:22:51,290
if this group is more likely to have bigger observations, more likely than I have.

219
00:22:51,310 --> 00:22:55,240
Right. Then I believe there's different populations going on.

220
00:22:57,130 --> 00:23:04,240
They throw in this additional piece here for discrete distributions because in discrete distributions we do have the there is right there's positive.

221
00:23:04,240 --> 00:23:08,260
Ms. The two observations would be exactly the same value.

222
00:23:09,010 --> 00:23:10,720
So that's there as well. Right.

223
00:23:10,840 --> 00:23:18,370
So just three makes fewer assumptions about the relationships of these two distributions, their shapes and their shifts and so forth.

224
00:23:18,850 --> 00:23:22,249
And again. Following this right here.

225
00:23:22,250 --> 00:23:24,470
I think this is something that I can explain to folks.

226
00:23:24,920 --> 00:23:31,070
Hey, we're interested in seeing if observations are more likely to be larger in one group versus the other.

227
00:23:31,790 --> 00:23:39,020
And what we're saying right there in medical applications, we an investigator might say,

228
00:23:39,050 --> 00:23:42,530
you know, is the outcome under one treatment more likely to be higher?

229
00:23:42,530 --> 00:23:45,589
Better is higher is better than outcomes.

230
00:23:45,590 --> 00:23:52,430
If I give folks a different treatment right now, on average, we didn't say on average what you said in terms of the distribution.

231
00:23:54,410 --> 00:24:01,100
And as I said, the reason for stating things a little more complex with that additional half of a probability there,

232
00:24:01,460 --> 00:24:06,800
like I said, is because in a discrete distribution, we want to be able to cover all distributions.

233
00:24:07,040 --> 00:24:09,080
We're trying to be distribution free here in this class.

234
00:24:10,770 --> 00:24:15,780
But again, when X and Y come from continuous distributions, we have zero probability of equality.

235
00:24:17,250 --> 00:24:22,950
And then we could just think above the hypothesis with the first portion of each hypothesis.

236
00:24:24,810 --> 00:24:27,629
And again, just to show you what we're talking about here in the case,

237
00:24:27,630 --> 00:24:37,380
when we have a discrete distribution and let's say each of the we have possibilities that X is zero, X is one or X is truth.

238
00:24:37,380 --> 00:24:44,070
There's three possible values and we have three probabilities that add up to one and the same for Y.

239
00:24:45,610 --> 00:24:52,630
Then we could do a little bit algebra to say what's the probability that observations from X are bigger than Y?

240
00:24:53,170 --> 00:25:01,420
And that's we just enumerate every possibility where X is bigger than Y and we do a bunch of l0 to in terms of peanut P one and P,

241
00:25:01,690 --> 00:25:08,770
A and B, and likewise there's a little algebra around what's the probability of equality between X and Y?

242
00:25:09,520 --> 00:25:18,190
Probability of zero one is another complicated formula, and if we add them together, you'll see that everything.

243
00:25:19,510 --> 00:25:23,919
So you can see that everything in there is negated by everything.

244
00:25:23,920 --> 00:25:26,950
And this only for this one half here.

245
00:25:27,700 --> 00:25:33,530
So. She should be alive anyway.

246
00:25:34,930 --> 00:25:42,820
And this entire quantity here adds up to one half. So it works for this great distribution's, very pathologic example.

247
00:25:43,240 --> 00:25:47,140
I can't imagine any data that would exactly follow a distribution like that.

248
00:25:47,530 --> 00:25:49,840
But that's why we need it there.

249
00:25:52,500 --> 00:25:59,819
So another reason that we might think about the hypothesis test three here in terms of just saying just one observation,

250
00:25:59,820 --> 00:26:02,790
does one population tend to have values bigger than the other?

251
00:26:03,730 --> 00:26:12,840
Because the problem, again, is this mann-whitney test that talks about getting started with no telling how to talk about this.

252
00:26:14,380 --> 00:26:18,810
There is the wilcoxon rank sum test such as SIC, which I haven't really told you what it is yet,

253
00:26:19,650 --> 00:26:27,000
but it's a sum of ranks and there's a mann-whitney used to test and there's the mathematical relationship between the two.

254
00:26:27,530 --> 00:26:37,290
So Wilcoxon rank sum statistic is simply the other one shifted by this one over to talk about m being defined as a function of W.

255
00:26:38,970 --> 00:26:44,220
If I take the mann-whitney statistic which again I haven't shown you,

256
00:26:44,700 --> 00:26:50,430
if you take the mann-whitney statistic and divide by m times m the two sample sizes and the two groups,

257
00:26:51,300 --> 00:26:54,900
you're going to get an estimate of this probability and this is the probability in the null.

258
00:26:56,280 --> 00:27:02,460
So the mann-whitney u i like because it gives me an estimate that goes with the hypothesis.

259
00:27:03,730 --> 00:27:10,420
One of the challenges of non parametric tests, ranked based tests is if we talk about things like this,

260
00:27:11,020 --> 00:27:14,350
there isn't a parameter estimate that goes into the test.

261
00:27:15,890 --> 00:27:18,310
So it's a little bit challenging sometimes, right?

262
00:27:18,830 --> 00:27:24,430
When we do a Wisconsin ring some tests, there isn't necessarily a parameter estimate that goes with the test.

263
00:27:24,590 --> 00:27:33,290
It was in the test statistic. But the nice thing about the Mann-whitney U is it does give me an estimate of the probability.

264
00:27:33,290 --> 00:27:38,000
Essentially, what's the probability that group X is observations bigger than Y?

265
00:27:38,570 --> 00:27:42,860
The probability goes from 0 to 1. If that probability estimate is 0.8.

266
00:27:43,850 --> 00:27:47,330
Right. That's suggesting to me that probably I'm going to reject the null hypothesis.

267
00:27:47,340 --> 00:27:54,890
There has to be some statistics going on. But if you have done any work with RC curves,

268
00:27:57,620 --> 00:28:06,860
C curves are a way that we take a continuous test measure and see if it discriminates between one group and another.

269
00:28:08,130 --> 00:28:11,620
So we've kind of flipped the problem. I have a continuous measure.

270
00:28:11,670 --> 00:28:16,890
How does it describe two groups? We've been talking about two groups that have the same continuous measure.

271
00:28:17,900 --> 00:28:20,810
So this equation right here, this is a probability.

272
00:28:21,740 --> 00:28:29,690
This probability is the area under the curve of an operating characteristic receiver operating characteristic curve.

273
00:28:30,560 --> 00:28:35,840
And as I just said, RC curves are used to summarize a sensitivity and specificity that you see in the

274
00:28:35,900 --> 00:28:40,100
language of a continuous diagnostic marker for identifying a binary disease state.

275
00:28:40,640 --> 00:28:44,510
Again, what is our secret trying to do is saying, I've got two populations.

276
00:28:44,810 --> 00:28:47,930
You can draw these two distributions. How much do they overlap?

277
00:28:49,430 --> 00:28:55,280
The two populations don't overlap by much. That says this measure is different between the two groups.

278
00:28:55,490 --> 00:29:00,230
Or do the two populations overlap? If you picture to see there are two identities on the top of each other.

279
00:29:00,740 --> 00:29:10,430
That's what these are. So the Mann-whitney U statistic is used a lot in the research under the receiver operating characteristic curve services.

280
00:29:13,430 --> 00:29:21,620
So what is the Wilcoxon rank sum statistic? It's based on computing the sum of the ranks in group one.

281
00:29:23,150 --> 00:29:26,840
When we compute ranks pooling all of the data together.

282
00:29:27,500 --> 00:29:30,200
So we rank the data under the null hypothesis.

283
00:29:30,620 --> 00:29:36,110
We say we don't know what group each observation belongs to for simply going to rank them all among each other.

284
00:29:37,520 --> 00:29:43,370
I will then add up the ranks corresponding to one group inside, which is group one and which is group two.

285
00:29:43,970 --> 00:29:50,840
And again, if observations from Group one tend to be larger than those from group two, then the sum of the ranks should be large.

286
00:29:51,980 --> 00:29:57,260
A large sum of ranks in one group should tell me I have evidence to reject the null.

287
00:29:59,250 --> 00:30:02,480
So is that right? So again, technically here,

288
00:30:02,780 --> 00:30:12,620
let's take all of the observations X ones or X and Y one through Y and put them in one vector Z and under the null hypothesis.

289
00:30:13,220 --> 00:30:18,080
All these observations come from the same group. So I put them together and the wilcoxon rank.

290
00:30:18,080 --> 00:30:21,280
Some statistic says, right.

291
00:30:21,710 --> 00:30:29,540
I head up the ranks one through n again in my notation here one true and the correspond to the axis.

292
00:30:30,380 --> 00:30:36,110
So you do have to keep track of the group membership. So we put them all together.

293
00:30:36,110 --> 00:30:40,270
We rank them as one vector, and then we only add up the first and ranks.

294
00:30:40,280 --> 00:30:47,580
We add up the ranks corresponding to group X. And I just said, I'll have to write.

295
00:30:50,130 --> 00:30:55,860
So under the null hypothesis, as I said, we treat both of them as coming from one single continuous distribution.

296
00:30:55,890 --> 00:31:02,160
F And therefore the expectation of the average rank across all of the observations in Z,

297
00:31:02,160 --> 00:31:07,620
all M plus and observations is simply the average of the lowest rank.

298
00:31:08,460 --> 00:31:18,090
The highest rank. And therefore the expectation of the sum of the ranks is simply the sum of those expectations, right?

299
00:31:18,450 --> 00:31:22,470
If each of them has expectation this their end of them here is the expected value.

300
00:31:23,490 --> 00:31:25,620
The variance, unfortunately not so fun to go through.

301
00:31:28,080 --> 00:31:35,670
15 years ago I probably would have made this a homework assignment, but 15 years later we learned that maybe that's not the best use of your time,

302
00:31:36,930 --> 00:31:40,110
but the variance looks very much like the meaning of simply skills.

303
00:31:40,110 --> 00:31:44,280
By M and by 12 instead of two looks very much like a uniform.

304
00:31:44,610 --> 00:31:51,110
Your hands cut that way. So I have a test statistic.

305
00:31:52,520 --> 00:31:58,010
I know it's mean and I know experience and getting to the point of getting a p value.

306
00:31:59,090 --> 00:32:04,340
That's what we want to get here. Again, I didn't talk about estimation of a parameter here.

307
00:32:05,240 --> 00:32:09,950
I didn't talk about estimating the median or anything. I just immediately went to a test statistic.

308
00:32:10,460 --> 00:32:14,660
The t test is nice because x bar and y bar are in the statistic.

309
00:32:15,290 --> 00:32:19,610
They're part of what I'm doing and nothing going on here with this.

310
00:32:20,090 --> 00:32:24,690
So there are challenges with these non parametric methods. Again, what is a p value?

311
00:32:24,710 --> 00:32:27,770
You all know what a p value is, right? It's the probability.

312
00:32:28,940 --> 00:32:34,760
That's right. How much of the distribution is above my observed value in the sampling distribution under the null?

313
00:32:35,750 --> 00:32:40,520
Where does my observation lie? How much of it is to the right and a one sided value anyway?

314
00:32:40,710 --> 00:32:43,760
Right to know the null distribution.

315
00:32:44,180 --> 00:32:52,160
What's the null distribution of the wilcoxon rank sum statistic when the two populations are the same?

316
00:32:52,760 --> 00:32:58,670
Right. That's what I need to know. What is the null distribution? I've told you a mean and a variance, but I haven't told you a distribution.

317
00:33:00,410 --> 00:33:06,440
This is where the idea of permutation test specifically comes into play before we talk about permutation tests in general.

318
00:33:07,460 --> 00:33:14,360
But these axonal distribution is followed by the fact that each possible ordering of the ranks has the same probability.

319
00:33:16,500 --> 00:33:23,069
So the probability of these integers is simply one over m plus one every.

320
00:33:23,070 --> 00:33:27,410
If you think of the integers one through amc+ and again you need four.

321
00:33:27,420 --> 00:33:32,760
No ties here to make the series easy. Right now we have the integers one through m plus and.

322
00:33:34,380 --> 00:33:38,640
There are. And plus and possible permutations of those integers.

323
00:33:39,710 --> 00:33:53,670
And my ranks are one possible permutation. So the probability is one over all those possibilities and possible and plus m there are impossible ways,

324
00:33:53,670 --> 00:34:01,830
as many possible combinations of assigning those ranks to group one combination formula from internal statistics.

325
00:34:03,120 --> 00:34:09,060
Again, consider an example, very simple example. Here I have four observations two from one group, two from the other.

326
00:34:10,260 --> 00:34:16,560
So in this case, there are six distinct ways to assign two ranks to the one that I'm going to add up the ranks.

327
00:34:17,430 --> 00:34:20,580
So what's the null distribution of the pattern? Like some statistic.

328
00:34:22,260 --> 00:34:25,770
So there's three and there's one, two, three and four ranks one, two, three and four.

329
00:34:27,480 --> 00:34:32,070
I could have one, two, three and four. I could have ranks three and four assigned to group one.

330
00:34:32,490 --> 00:34:35,550
It's only one possible way to get to seven. So one out of six.

331
00:34:36,180 --> 00:34:44,730
There's only one possible way to get a six. That's rank four plus rank two. There's two ways to get five, one and four, two and three and so forth.

332
00:34:45,450 --> 00:34:49,500
So the know, the distribution of the sum of two ranks and group one,

333
00:34:49,980 --> 00:34:59,220
there are five possible values of that sum and each of them has it is a probability and they all add up to one of our screen distribution here.

334
00:35:02,550 --> 00:35:07,590
So there it is. If I am, I have an observed value of ranks for group one.

335
00:35:08,580 --> 00:35:16,319
I have to enumerate every possible sum of ranks for Group One and see where my number lies in all of those possible values.

336
00:35:16,320 --> 00:35:20,910
And there there are probabilities. Not going to do that by hand.

337
00:35:22,650 --> 00:35:28,790
We're going to have 100 observations. If there are 60 and one group you're going to think of every way that I can take 60

338
00:35:28,830 --> 00:35:33,420
integers from 1 to 100 and add them up and figure out what those probabilities are.

339
00:35:34,310 --> 00:35:41,700
And then this idea was thought of a long, long, long time ago when we didn't have computers to do this sort of thing.

340
00:35:43,170 --> 00:35:47,280
So Mr. Wilcoxon visited it by hand. Or we do.

341
00:35:47,280 --> 00:35:50,310
What we always do is computations are difficult.

342
00:35:51,360 --> 00:36:01,650
We hope that large sample theory gets us out of a problem and so we can do that with the Wilcoxon makes some tests and any basically any rate test,

343
00:36:01,890 --> 00:36:08,490
linear rate test that we'll talk about later. So looking at number four, again, equation four, we can see that this test statistic,

344
00:36:08,700 --> 00:36:13,200
as I said earlier and a little hand-waving here, I'm not going to be real precise.

345
00:36:13,470 --> 00:36:16,530
It's nearly the sum of independent, random variables.

346
00:36:16,950 --> 00:36:24,390
Each of the ranks has a little bit of correlation, negative correlation, and now we've got implicit observations.

347
00:36:25,560 --> 00:36:35,640
So we can expect, again, not trivial theory, but we can expect that if I take my rank, some statistic,

348
00:36:36,750 --> 00:36:43,320
shift it by its mean divide, by its standard deviation, the that thing is hopefully approximately normal zero one.

349
00:36:45,080 --> 00:36:48,950
That's great, because then I'm done. Because now I have a test statistic whose.

350
00:36:48,950 --> 00:36:55,030
No distribution. I know. Zero one, I can use my critical value 1.96 and off I go.

351
00:36:55,530 --> 00:37:02,590
Okay, so it's no longer exact, it's approximate. But again, this doesn't rely upon the normality of the data.

352
00:37:03,490 --> 00:37:07,210
This relies upon the normality of the sum of ranks.

353
00:37:08,380 --> 00:37:11,620
And the theory behind the sum of ranks, again, is not trivial.

354
00:37:11,620 --> 00:37:17,430
If showing you just snippets here. There's a whole book I had written a long time ago.

355
00:37:17,440 --> 00:37:25,150
I'm a theory of rank this, but we can think of a standardized version of the sum some statistic my sum of ranks in group one.

356
00:37:26,470 --> 00:37:31,299
It's mean that I just showed you earlier vitamins a squared and its variance that I showed

357
00:37:31,300 --> 00:37:37,210
you earlier and hopefully that is normal zero one with enough with enough observations.

358
00:37:39,550 --> 00:37:47,890
Again, this is just a bunch of math for saying I take this shifted and scaled some and I see where it lies in a standard normal distribution.

359
00:37:47,920 --> 00:37:53,410
Like how much of a standard normal zero one is to the right of this shift and scaled like some statistic.

360
00:37:55,300 --> 00:37:59,000
I have that. Anybody.

361
00:37:59,900 --> 00:38:04,700
Have you ever seen a continuity correction presented to. Yes.

362
00:38:04,710 --> 00:38:09,040
In which class? Not very interested to fix it.

363
00:38:09,560 --> 00:38:15,020
So any other class, anybody talk about kind of digressions anymore?

364
00:38:15,980 --> 00:38:19,220
I mean, I'm. Yes, sir. That's for 12th. Which.

365
00:38:19,370 --> 00:38:23,830
Which is what? Just statistics. Okay. Okay, cool.

366
00:38:24,440 --> 00:38:32,150
I got it. Got it. Okay. When I was in grad school, my professor was already going continuity corrections.

367
00:38:32,990 --> 00:38:39,080
And here I am, 23 years later, doing the same thing. Remember that we have a discrete distribution here.

368
00:38:39,320 --> 00:38:42,760
The rank some statistic, right, is the sum of integers.

369
00:38:42,800 --> 00:38:44,330
It's got a discrete distribution.

370
00:38:45,530 --> 00:38:51,740
The theory behind continuity corrections says that, well, you know, you're approximating a discrete distribution with a continuous one.

371
00:38:53,120 --> 00:38:53,660
Quite right.

372
00:38:55,160 --> 00:39:06,110
And I don't even know who first came up with the idea, but folks said to make these P values more accurate, it's actually makes them conservative.

373
00:39:06,650 --> 00:39:13,790
So we're trying to avoid rejection when we shouldn't be. Of the ninth ethicist we take.

374
00:39:15,550 --> 00:39:21,500
Our observe statistic and we subtract on the half. When we start doing the normal approximation.

375
00:39:23,840 --> 00:39:29,740
So. When we used the Wilcoxon rank sum test in our.

376
00:39:33,050 --> 00:39:41,140
I'm getting way ahead of myself. Everything I've shown you here is in the absence of ties.

377
00:39:43,080 --> 00:39:44,360
The theory, the average,

378
00:39:44,370 --> 00:39:51,870
the mean and the standard deviation are they will concentrate on some statistic in the presence of ties is vastly more complicated.

379
00:39:52,290 --> 00:39:56,340
I need to know where the ties are. How many observations are tied?

380
00:39:56,610 --> 00:40:01,280
All of that goes into the minimum variance. So it gets very, very complicated across my one.

381
00:40:01,500 --> 00:40:10,400
Perfect. So most computer packages, if you have ties in your data, if you have observations that are equal to each other,

382
00:40:10,970 --> 00:40:16,340
most computer packages will not be able to compute all of the permutations.

383
00:40:16,670 --> 00:40:22,220
Deciding on the exact most computer packages will not compute the exact distribution when there are none.

384
00:40:23,150 --> 00:40:27,470
And so it follows that they fall back on this approximation for simple approximation.

385
00:40:29,410 --> 00:40:34,210
R has also decided that it will default to using a continuity correction.

386
00:40:36,040 --> 00:40:42,840
And I don't like that new directions. So every time I do the Wilcoxon rank sum test, I have to tell it not to do a continuity correction.

387
00:40:43,500 --> 00:40:47,340
So there's an argument in our and the wilcoxon test.

388
00:40:47,340 --> 00:40:49,830
I'll show you. I say correct. This is false.

389
00:40:51,150 --> 00:40:59,460
Now, if you've already done homework, one, because you're on top of things and you didn't do the count, you did the continuity correction.

390
00:41:00,210 --> 00:41:05,850
Just leave it, please. We're talking about a p value of 0.72 versus .78.

391
00:41:06,640 --> 00:41:11,340
Right. It's just as a practice. Don't do any kind of continuity corrections.

392
00:41:12,690 --> 00:41:21,480
And again, those of you who have had me as a teacher already, if you use a continuity correction and you've got a P value of 0.05 to.

393
00:41:23,060 --> 00:41:26,960
And then you take away the continuity correction and they get a p value point for seven.

394
00:41:28,070 --> 00:41:32,550
What are you going to do? What's your investigator going to want you to do?

395
00:41:34,080 --> 00:41:38,010
So that's the only time continuity corrections might come into play here, right?

396
00:41:38,040 --> 00:41:41,130
Is when the P value shifts over that stupid line of final five.

397
00:41:42,510 --> 00:41:45,650
Don't ever get into that practice. Right?

398
00:41:45,690 --> 00:41:52,769
Don't pick the method to get the p value. So whether you want to use continuity corrections or not, that's up to you.

399
00:41:52,770 --> 00:42:00,400
I tend not to use them because I just don't know why they were ever created and neither is the default in our view,

400
00:42:00,430 --> 00:42:08,670
where that sometimes I forget that is there there is a coin package and ah I have yet to use,

401
00:42:09,360 --> 00:42:12,870
but you can do permutation tests in the presence of ties.

402
00:42:13,110 --> 00:42:16,920
So again, the wilcoxon rank sum test is a permutation test.

403
00:42:17,660 --> 00:42:23,250
If you think of computing the integers one through M plus and to come up with an L

404
00:42:23,250 --> 00:42:28,410
distribution so you can get exact exact distributions of types should you want to.

405
00:42:28,770 --> 00:42:34,260
Someone need a library for that. All right, survey time.

406
00:42:36,390 --> 00:42:39,900
So, again, I'm trying to figure out the easiest way for everybody to do this.

407
00:42:40,680 --> 00:42:49,050
Um, there's the QR code, and if you need it, you might want to have the link on your computer or something in general.

408
00:42:50,520 --> 00:42:57,720
Questions on there. Again, I apologize to those of you that are colorblind or have a little bit difficulty with in color.

409
00:42:58,320 --> 00:43:05,040
The question is what color pants in my work, you know, it's not.

410
00:43:07,480 --> 00:43:12,150
Is it safe? Evers Yeah. Oh, yeah. Since it figures we'll save the pants for another time.

411
00:43:13,350 --> 00:43:16,890
So just answer if my favorite team has the Packers, tell me that's the Packers.

412
00:43:17,970 --> 00:43:28,770
Don't tell me that. Some of the other teams don't say the Seahawks do not.

413
00:43:28,770 --> 00:43:31,860
76 anyway.

414
00:43:32,010 --> 00:43:38,339
All right. Whatever question is up there, answer it. Shoot to whoever.

415
00:43:38,340 --> 00:43:42,890
Skipton is thrilled. And so we left.

416
00:43:43,170 --> 00:43:50,460
They saw the same question. So if they sit still. No, no, you really don't see the other questions.

417
00:43:50,790 --> 00:43:56,340
Oh, I wonder if the QR code doesn't work after I change the question on canvas is the same.

418
00:43:58,290 --> 00:44:03,119
Okay, well, I'm going to figure this out again. This isn't meant to be problematic.

419
00:44:03,120 --> 00:44:07,170
This is just to keep you guys coming to my fantastic class. So like on canvas also.

420
00:44:07,230 --> 00:44:12,450
So yes, I will figure that out because I do want to change it.

421
00:44:12,450 --> 00:44:15,750
I don't want to have to destroy and create surveys every other day.

422
00:44:16,590 --> 00:44:19,950
All right. Shoot. No, from there. All right.

423
00:44:21,060 --> 00:44:24,630
That's a lecture to cover.

424
00:44:24,990 --> 00:44:29,670
Another question. Yeah, whatever. So are you saying that you want to.

425
00:44:29,760 --> 00:44:39,569
I will compute the what will compute the exact distribution, except in the case of ties in which only in the case of ties, I'll use the last contact.

426
00:44:39,570 --> 00:44:43,620
Correct. I think there also might be a limitation to the sample sizes.

427
00:44:45,060 --> 00:44:48,090
There's a there is no that's Fisher's exact test. I think so.

428
00:44:48,090 --> 00:44:51,510
I think as long as you don't hit. So if you have only unique values in your dataset.

429
00:44:52,440 --> 00:44:54,730
I will give you the exact distribution p value.

430
00:44:55,440 --> 00:45:05,460
So in that case, the continuity correction argument is obviously not used any again, because it only is used in a large sample approximation.

431
00:45:06,150 --> 00:45:11,820
Huh. Yeah. So I guess I always say false for the current, that kind of digression.

432
00:45:11,820 --> 00:45:19,020
But I guess if I only have again, you're rarely going to have a large dataset where everything is unique unless you're out to five or six decimals.

433
00:45:19,020 --> 00:45:28,890
And then the outcome is, yes, it's usually going to be at least one time and again if you have enough data, right?

434
00:45:29,790 --> 00:45:32,520
So the larger sample approximation is really, really good.

435
00:45:33,690 --> 00:45:37,530
Thank goodness for a large sample approximations are a lot of our lives would be very difficult.

436
00:45:39,990 --> 00:45:45,940
I talked about a one sided great. We find our observed value and we see how much of the normal distribution is to the right of it.

437
00:45:46,080 --> 00:45:48,080
You can think of a two sided, devalue,

438
00:45:48,960 --> 00:45:55,780
two sided alternative just says that the two are equal in distribution versus the fact that one of them is different from me.

439
00:45:55,800 --> 00:45:59,300
One of them dominates the other. But we don't know which one. Right.

440
00:45:59,820 --> 00:46:03,020
And. Again, it's the same sort of concept.

441
00:46:03,320 --> 00:46:05,290
You think about where your statistic is,

442
00:46:05,800 --> 00:46:12,010
what's the reflection on the other side of the distribution to get those two values, just like what's the normal concept?

443
00:46:13,360 --> 00:46:17,050
So let's do where we at 1046.

444
00:46:17,560 --> 00:46:26,890
So if we want to do this and you are going to do this again, many who may have already done this, there's a package called line.

445
00:46:27,520 --> 00:46:31,750
Again, you can get it from this repository. It's in a package. It's on canvas.

446
00:46:31,780 --> 00:46:35,469
If you want to practice with this on your own. Again,

447
00:46:35,470 --> 00:46:43,390
not a hugely exciting public health or science while maybe trying to discriminate between types of wine might be something you're interested in.

448
00:46:44,920 --> 00:46:51,010
But we have three types of wine, and then they have all sorts of measures on these wines their alcohol content,

449
00:46:52,420 --> 00:46:56,140
magnesium levels, phenols in the data, their color and their hue and so forth.

450
00:46:56,740 --> 00:47:04,719
And so someone might say, well, can is are there a combination of characteristics that help me figure out what type of wine we're talking about?

451
00:47:04,720 --> 00:47:11,800
And I don't remember what the three types of liner should know now that I have forgotten, we're doing a two sample test right now.

452
00:47:11,830 --> 00:47:16,900
We're going to take out wine number three. So we're focusing on wines one and two.

453
00:47:18,010 --> 00:47:25,899
And again, here is a histogram now of the magnesium levels in the wine type one and type two.

454
00:47:25,900 --> 00:47:29,889
Unfortunately, the Y X-axis here are not of the same scale here.

455
00:47:29,890 --> 00:47:35,680
So it's hard to compare these two extremes now that I think about it in terms of if we can think out before we do a test,

456
00:47:36,460 --> 00:47:41,840
before you do a test, you should have a picture in front of you. It tells you whether or not your test makes sense.

457
00:47:43,610 --> 00:47:52,129
Again, we see one type two tends to have values out here, 142 160 there's a really little in Wayne one so maybe there's some indications here that I'm

458
00:47:52,130 --> 00:47:56,660
going to reject the null hypothesis that say that may be wanting to ask about the values.

459
00:47:57,170 --> 00:48:02,030
And we can see here that they have different distributions of these in these histograms right here.

460
00:48:02,510 --> 00:48:06,680
Right. This one's a little more maybe normal. This one is certainly skewed.

461
00:48:08,390 --> 00:48:19,440
So we can still compare them. But if you would teach us to compare these two values, these two distributions, should I?

462
00:48:24,600 --> 00:48:30,660
Would you use the T test on these two histograms? You're going to go out in the real world in a few months.

463
00:48:30,960 --> 00:48:37,540
Some of you. College, grad school, the real world. Would you would you do a t test here?

464
00:48:42,350 --> 00:48:47,770
So if you think no, why wouldn't you do a test? What do you see that concerns you?

465
00:48:49,110 --> 00:48:55,470
Not normally in a normality. Right. Again, I'm not trying to be difficult here.

466
00:48:56,670 --> 00:49:01,350
You're going to encounter in a case like this, you're going to get two groups and you're going to have a data set and you're going to histograms.

467
00:49:01,350 --> 00:49:05,249
You're going to go home. I was told that I need normally distributed data to do a T test.

468
00:49:05,250 --> 00:49:17,640
You definitely don't see that for weight type two. Do I need normality of the data for the T test to always work?

469
00:49:19,510 --> 00:49:23,050
No. And the answer is no. And why not?

470
00:49:25,150 --> 00:49:27,390
Because data doesn't correct.

471
00:49:27,760 --> 00:49:36,760
The t test uses the sample means the sample means are normally distributed or approximately by the central limit theorem.

472
00:49:38,570 --> 00:49:43,580
I don't need normally distributed data to have a good result from a test or a valid APR.

473
00:49:43,580 --> 00:49:48,770
Valid. Again, it's nothing to do with parametric tests.

474
00:49:48,770 --> 00:49:55,760
It has to do with you as practicing statisticians. Don't let folks tell you you can't use a t test because the histograms look funky.

475
00:49:57,290 --> 00:50:05,300
All you need is normality of the sample means to do it. You need normality of the data to necessarily get valid results are approximately right.

476
00:50:06,080 --> 00:50:07,480
Everything we do is approximate.

477
00:50:07,970 --> 00:50:14,450
I hated a you know, this is one thing that people hate when I teach intro bias that I'm going to tell them you're not going to get the right answer.

478
00:50:15,740 --> 00:50:16,490
You don't you don't know.

479
00:50:16,490 --> 00:50:21,950
If you get the right answer right, you're going to get an answer and you're going to hope that it's pretty close to the right thing.

480
00:50:21,950 --> 00:50:25,280
Right. This is why our jobs are great. We can be approximately right.

481
00:50:25,400 --> 00:50:29,840
Right. So again, I have no problem using a T test here.

482
00:50:29,870 --> 00:50:33,070
There's probably enough data here that the central limit there is to help me. Yes, sir.

483
00:50:33,110 --> 00:50:38,710
I look the as they explain why they will follow normally for for the B statistic,

484
00:50:38,720 --> 00:50:43,490
I'm thinking about the numerator for normality, but the denominator also has to follow chi square.

485
00:50:43,610 --> 00:50:48,110
Correct. So the square is a little is is true when it is normal.

486
00:50:48,110 --> 00:50:51,890
So I'm just thinking like if it is not normal, we will follow Chi Square or not.

487
00:50:52,670 --> 00:50:54,350
That's a good question. Asymptotically.

488
00:50:57,290 --> 00:51:02,990
I'm going to I'm going to throw in an idea that says, yes, I think asymptotically, I think things are going to be okay.

489
00:51:03,110 --> 00:51:08,570
That's a really good point. Remember, the denominator deals with variances. That means we're talking about fine square distribution.

490
00:51:09,980 --> 00:51:16,790
The other thing to remember, too, is that. Even if I do have normal distributions.

491
00:51:21,250 --> 00:51:26,200
There's an unequal variance problem in teach us. I told you in the homework to use variance equally as variance is true.

492
00:51:26,200 --> 00:51:32,170
Right? Equal variances. Remember that the degrees of freedom.

493
00:51:34,390 --> 00:51:37,970
Gets it gets a little bit complex in an equal sample sizes.

494
00:51:38,080 --> 00:51:48,250
And remember, those are approximations as well. So and again, if ever in your life you get a set of data where everything is exactly normal,

495
00:51:48,250 --> 00:51:54,280
please call the rest of the world because you have discovered something no one else has, right?

496
00:51:55,750 --> 00:51:59,660
Nothing is normally distributed to the most part, at least with human data.

497
00:51:59,680 --> 00:52:02,709
I don't know about other other definitions. Wow.

498
00:52:02,710 --> 00:52:06,190
That was a digression. All right. We've got two histograms here.

499
00:52:06,190 --> 00:52:11,080
We want to know if observations of magnesium tend to be larger in one group versus the other.

500
00:52:13,440 --> 00:52:18,210
So there are two ways you can use that syntax and r any way to do a wilcoxon Wrexham test.

501
00:52:19,920 --> 00:52:23,700
One way is to define the X values in the Y values.

502
00:52:24,330 --> 00:52:29,760
One group, the other group. And then to say do a wilcoxon test of the X values and the Y values.

503
00:52:31,020 --> 00:52:37,800
Again, the default alternative is a two sided. And right now we're going to do a greater than one sided alternative.

504
00:52:38,580 --> 00:52:47,700
So that's one way to do a concentric sum test. Again, the reason I like a T test is because it gives me a statistic that I can get my head around.

505
00:52:48,030 --> 00:52:52,230
It's a difference of means scales. But what does this mean?

506
00:52:54,100 --> 00:52:58,000
33. 81.5 W Well, that's just some drinks, right?

507
00:52:58,330 --> 00:53:10,130
So some of the ranks in one of the groups. There's a massively small p value to the continuity correction because they didn't do what I say I do.

508
00:53:11,060 --> 00:53:15,049
I would normally say correct equals false here, but it tells you to do the kind of regression.

509
00:53:15,050 --> 00:53:19,160
But again, this p value is so ridiculously small it doesn't really matter what I do.

510
00:53:19,520 --> 00:53:24,740
I'm going to get something less simplified if that's my threshold for significance, right?

511
00:53:25,100 --> 00:53:31,700
That tells you that there's the alternative. Of course, the other way to do it is in one step is yes in.

512
00:53:31,880 --> 00:53:40,610
Since the alternative is like one side of here, which one is this thing is because it is saying that the second group is bigger.

513
00:53:41,410 --> 00:53:46,010
Line two hopefully. I mean, that's what I see here is wait, two is bigger, right?

514
00:53:46,370 --> 00:53:49,939
This is what them like to look like if you swap them.

515
00:53:49,940 --> 00:53:55,430
That's a good question. I'm going to put this in our cloud and we'll get to it.

516
00:53:56,270 --> 00:54:01,700
So our question is, is what if I got the groups mixed up? I don't think you're going to get this.

517
00:54:01,700 --> 00:54:07,790
You're not going to get the right answer. Right. It'll probably say that X is not greater than Y, because why is that X?

518
00:54:10,190 --> 00:54:16,040
That's a great question. And it just says right here, right, be careful about the ordering.

519
00:54:16,190 --> 00:54:19,819
The ordering of attributes does matter, especially in a one sided alternative,

520
00:54:19,820 --> 00:54:23,060
because you're saying one of groups tends to be it'll have larger values in the other.

521
00:54:25,730 --> 00:54:31,850
And remember to. This is the sum of the ranks in the first group.

522
00:54:33,720 --> 00:54:36,030
So if you want the sum of the ranks in the other group,

523
00:54:36,300 --> 00:54:41,040
you've got to have you've got to shift who's group language to can doesn't affect the inference.

524
00:54:41,050 --> 00:54:46,110
Yes, sir. So if we're reporting results, does that statistically mean a whole lot?

525
00:54:46,530 --> 00:54:49,170
So we shouldn't be reporting that? I don't think you should.

526
00:54:52,000 --> 00:55:03,690
I am of the feeling that remember that a p value is nothing more than a test statistic translated to a01 scale right from intensive purposes.

527
00:55:04,680 --> 00:55:08,749
I never give anybody a test statistic. Now.

528
00:55:08,750 --> 00:55:11,600
I just got a revision back request from an applied journal.

529
00:55:12,380 --> 00:55:20,780
We did four regression models and the reviewer said it would be really helpful if you gave me the statistics and the degrees of freedom for those for.

530
00:55:21,860 --> 00:55:29,680
Are you kidding me? You want to know that the statistic was 35.7 and seven and 25 degrees of freedom, right?

531
00:55:30,350 --> 00:55:36,080
No, you don't. But anyway, I don't think test autistics have an applied value in most settings,

532
00:55:37,670 --> 00:55:41,150
but you will find that folks want them because they think they should have them.

533
00:55:41,660 --> 00:55:46,130
I don't think giving kind of square statistics with degrees of freedom is at all helpful for most individuals.

534
00:55:47,780 --> 00:55:52,070
And T values do have their limitations. Be like confidence intervals whenever possible.

535
00:55:52,940 --> 00:55:57,430
If we have a parameter. But I'm. What does this mean?

536
00:55:57,460 --> 00:56:01,090
Is that big enough? I don't know. I need to know the normal distribution.

537
00:56:01,540 --> 00:56:05,130
I remember reading of numbers 1 to 2.

538
00:56:05,140 --> 00:56:09,010
This is why I like to work in R rather than show you are marked on results.

539
00:56:09,010 --> 00:56:15,040
I don't even know the sample size of two groups, but you're adding up numbers 1 to 100.

540
00:56:15,040 --> 00:56:21,130
You're sure it's going to look big, right? Just because you're ending up numbers that get big so doesn't mean anything.

541
00:56:22,590 --> 00:56:28,020
The other way to do the test instead of creating two two lines of code where you can create

542
00:56:28,020 --> 00:56:32,850
the two vectors of observations is simply is the regression type syntax that R uses a lot.

543
00:56:33,360 --> 00:56:36,510
And here is my outcome model on the two groups.

544
00:56:37,080 --> 00:56:43,890
So right away, right away, our knows that this is the outcome and this is the group indicator.

545
00:56:44,920 --> 00:56:52,300
And it gives you the same result right there. So but again, keep track of the two types.

546
00:56:53,080 --> 00:56:58,540
This again are when our CS a categorical variable is fixed.

547
00:56:58,690 --> 00:57:02,140
Which one is group one and which one is group two? Right. Usually alphabetically.

548
00:57:02,710 --> 00:57:05,230
So keep that in mind when you're doing all this.

549
00:57:05,800 --> 00:57:10,930
That's why maybe it's sometimes it's nicer to do this way because you know what's group one and what's group two?

550
00:57:11,720 --> 00:57:14,860
But this is a lot simpler in terms of writing code.

551
00:57:17,180 --> 00:57:24,740
Again. How did our get this how did our get a rank some statistic of 3380 1.5.

552
00:57:27,370 --> 00:57:30,729
This tells me there had to be a tie. No, no.

553
00:57:30,730 --> 00:57:36,090
Because it's very it's divided by Trump. All right.

554
00:57:36,100 --> 00:57:39,850
So how did it computable considering some statistics? Right.

555
00:57:40,480 --> 00:57:44,200
So and there's my the wilcoxon test.

556
00:57:44,680 --> 00:57:49,270
Oh, here. I got the links of the two again. How many observations are there in the first one group?

557
00:57:49,690 --> 00:57:53,260
How many in the second group. I want the ranks.

558
00:57:55,130 --> 00:58:06,320
Of all of the magnesium levels. I want the ranks across both groups and then I added up the ranks corresponding to the first line type.

559
00:58:07,640 --> 00:58:11,240
And if you find out, you get 50, 51, 51.5.

560
00:58:11,870 --> 00:58:16,100
That is not 33, 81.5. So what happened?

561
00:58:18,920 --> 00:58:24,710
And that's because the Wilcoxon test, remember, it's related to the man when you use.

562
00:58:26,320 --> 00:58:33,880
And so what our spits out is the sum of the ranks shifted by the average rank in that group.

563
00:58:36,280 --> 00:58:41,290
So if I take my sum of ranks, subtract off this number, I get 33, 81.5.

564
00:58:43,000 --> 00:58:52,300
So just keep that in mind that what they are calling W is actually something that's that's shifted by that's the sum of the rank shifted by something.

565
00:58:52,360 --> 00:58:57,500
Again, I don't really care because I don't use this. Use this number.

566
00:58:58,250 --> 00:59:04,010
I just want to know where it lies in the north, and that's no distribution and it's much, much larger than any.

567
00:59:04,370 --> 00:59:06,830
Again, how do I get this p value?

568
00:59:08,570 --> 00:59:17,870
I commute to the integers one through n plus m for every one of those permutations, I compute the sum of the values given to one one.

569
00:59:18,740 --> 00:59:23,630
I make a histogram of all those possible permutations and I see where my observed value lies.

570
00:59:24,150 --> 00:59:28,130
It is so far in the right of the distribution. I have the few values incredibly small.

571
00:59:28,850 --> 00:59:36,460
So it's a permutation test. So this thing right here is actually the man Whitney used to tastic.

572
00:59:37,150 --> 00:59:42,580
I take it the sum of the ranks and subtract off the average rank in that group.

573
00:59:42,820 --> 00:59:49,719
I get them in Whitney statistics. And so if I take as I told you earlier, if you take the man,

574
00:59:49,720 --> 00:59:56,050
when you use statistics supplied by the product of the two sample sizes, you'll get a number of between one and zero.

575
00:59:57,640 --> 01:00:05,890
It's a probability. It's the probability that if I take two, if I draw one value from each group.

576
01:00:07,450 --> 01:00:12,760
What's the probability that I get a larger value from group Y versus group X?

577
01:00:14,930 --> 01:00:19,220
The probability is 0.8. 1.5 is the No.

578
01:00:19,700 --> 01:00:26,270
If I pull one observation from each group and I have a coin toss as to which one will be bigger, then they're coming from the same group.

579
01:00:26,870 --> 01:00:35,270
So as this number moves away from point five or toward one, that tells me that the Y observations are tend to be larger.

580
01:00:35,780 --> 01:00:40,850
This value moves down from point five. It tells me that the other group tends to have larger observations.

581
01:00:42,240 --> 01:00:47,790
This is an AUC value and if you learn anything about AOC, the null value for AOC is 0.5.

582
01:00:48,750 --> 01:00:53,970
Because again, that tells me that these two observations don't really discriminate between the two groups.

583
01:00:53,980 --> 01:00:55,650
They tend to come from the same population.

584
01:00:56,780 --> 01:01:07,520
But this again, if I were to make an oral secret of where magnesium is a diagnostic test to figure out which wine we're talking about.

585
01:01:09,800 --> 01:01:16,310
And it does a nice job of discriminating between these two groups of whites because group whites tend to have larger values.

586
01:01:16,310 --> 01:01:29,050
The next leader. So again, you see is a probability beyond its its estimation of area under the first half.

587
01:01:29,060 --> 01:01:36,470
So let's say we want to do this on our own. I told you how to do the wilcoxon rank, some test, the distribution.

588
01:01:36,740 --> 01:01:40,220
I just primitive things that we're going to do that now.

589
01:01:40,580 --> 01:01:45,320
So let's check out the man and where this statistic matches the simulation based estimate of this probability.

590
01:01:46,430 --> 01:01:50,420
So I've got an indicator of belonging to each of the two types of wine.

591
01:01:52,810 --> 01:01:58,370
Again until I started teaching in this class again, I've never used which I never knew the function existed.

592
01:01:59,240 --> 01:02:07,390
It's amazing things that exist in art now, so it's greater right now is going to be a holder for what I'm going to do next.

593
01:02:07,660 --> 01:02:11,210
So it's just a vector of zeros right now, a thousand times less.

594
01:02:11,470 --> 01:02:21,070
So for a thousand times, what I'm going to do is randomly sample from the first group and then we sample from the second group.

595
01:02:23,690 --> 01:02:31,220
And see if X is greater. And I'm going to take the magnesium values that belong to X.

596
01:02:32,060 --> 01:02:39,770
Are they greater than the magnesium values from Y or at least equal to their half their.

597
01:02:42,150 --> 01:02:45,580
Oh. And then I just take the averages.

598
01:02:45,850 --> 01:02:55,270
So I'm doing a thousand draws from the X's on the lines and I'm saying in a thousand draws, what's the probability?

599
01:03:04,060 --> 01:03:15,000
Is it time to figure? She's fascinating.

600
01:03:15,000 --> 01:03:22,370
When you look at your code in front of 75 pairs of eyes, you realize that it's questionable.

601
01:03:23,190 --> 01:03:29,860
All right. And here's why. Type one. Here's one. Type two. That a lot of large values in one type to right.

602
01:03:31,860 --> 01:03:35,640
When Type one kind of stops here. Let's look at lower values, though.

603
01:03:45,270 --> 01:03:50,700
So in a thousand draws. So I'm going to draw an X and I'm going to draw a Y.

604
01:03:51,980 --> 01:03:57,410
Let's see if the magnesium corresponding to X is bigger than Y magnesium or at least equal to.

605
01:03:59,250 --> 01:04:06,710
So this will be a one. This will be a one if the x value is larger than or equal to the line you.

606
01:04:07,100 --> 01:04:12,080
And then I'm going to take the average across a thousand draws. How often is X bigger than Y?

607
01:04:12,860 --> 01:04:20,770
And it's about 80%. Just maybe, I think mag one just just bigger than two.

608
01:04:20,870 --> 01:04:24,440
I mean yeah. As a larger media, I'm pretty sure. Yeah.

609
01:04:24,560 --> 01:04:27,889
On Instagram it was like like mag one. It hasn't been. No.

610
01:04:27,890 --> 01:04:37,610
Isn't the alternative that I needed to watch to make sure we understand what the alternative is?

611
01:04:37,760 --> 01:04:47,870
We meaning me? I'm sure the alternative is that X is bigger than Y, right?

612
01:04:52,220 --> 01:04:55,400
That's all right. I got a little bit of rewriting to do here.

613
01:04:55,520 --> 01:04:59,480
There is a conflict somewhere in my notes here between everything that I've written.

614
01:05:00,150 --> 01:05:11,160
But so it looks like X is bigger than Y. Regardless, let's not lose sight of what I'm trying to tell you.

615
01:05:11,170 --> 01:05:16,920
Here is the mann-whitney statistic is computing a probability that one group,

616
01:05:17,220 --> 01:05:22,050
a draw from one group, tends to be as large or larger than the other group.

617
01:05:22,650 --> 01:05:28,500
And across a thousand draws, we had a probability close 2.8, which was close to that number.

618
01:05:28,940 --> 01:05:34,020
And I guess I'm interpreting things as the President X is bigger than white.

619
01:05:35,730 --> 01:05:43,650
Wow, that's amazing. If you look at that picture, that picture, and you said which group is going to tend to have larger values?

620
01:05:49,050 --> 01:05:51,990
Very interesting. To be continued next Wednesday.

621
01:05:53,040 --> 01:05:58,980
What I want to get to, though, before we run out of time is the nice thing about the Wilcoxon rank sum test.

622
01:05:59,690 --> 01:06:04,220
It is often use of ordinal data, ordinal data that are really are different.

623
01:06:04,710 --> 01:06:12,360
Here you have something where the measure in everybody is poor, fair, good or excellent, and people will give those numbers one, two, three and four.

624
01:06:13,440 --> 01:06:17,190
No investigators all the time and say, well, why don't we use a two sample T test?

625
01:06:18,840 --> 01:06:23,160
What's the average of fare? Poor, good and excellent. Right.

626
01:06:23,590 --> 01:06:26,940
Isn't very appealing. So there is ordering here.

627
01:06:26,940 --> 01:06:30,120
But there isn't numeric values for these.

628
01:06:31,020 --> 01:06:34,110
Well, if there's ordering, we can do ranks.

629
01:06:34,590 --> 01:06:38,250
We don't need numbers. We know that poor is below fare.

630
01:06:38,790 --> 01:06:47,140
We know fares below good and so forth. So this might be a sense testing whether or not outcomes tend to be better in one group, right?

631
01:06:47,190 --> 01:06:51,120
Do we tend to be toward the higher the higher categories than others?

632
01:06:51,870 --> 01:06:56,790
And so here we can compute the ranks. We don't need to ascribe numbers to fare poor, good and excellent.

633
01:06:57,930 --> 01:07:02,370
And again, all of what I'm saying is true. And the other challenges, there's lots of ties, right?

634
01:07:02,390 --> 01:07:07,380
If every person can only have one or four values and you've got 200 people in the dataset,

635
01:07:07,650 --> 01:07:12,120
there's going to be a tie, which is naturally going to be so.

636
01:07:12,600 --> 01:07:15,870
But with ordinal data rank, some tests have a really nice, I think,

637
01:07:15,870 --> 01:07:23,090
utility because we don't have to get around this question of a, you know, how do I compute means comedians from categories.

638
01:07:23,100 --> 01:07:26,880
I have to give them numbers. And those numbers are usually very arbitrary.

639
01:07:28,770 --> 01:07:32,790
There is an estimated for the shift, if you think of a shift alternative here,

640
01:07:32,790 --> 01:07:37,590
if you think of the two distributions simply being one looks like the other, but moved over to the right.

641
01:07:39,100 --> 01:07:42,400
And so Hodges and Lehman got credit for this one.

642
01:07:43,210 --> 01:07:50,260
And so we have the Hodges Lehman estimate here, and it's defined as the median difference among all possible pairs.

643
01:07:50,680 --> 01:07:57,310
So one observation from each group, but I have a difference if I take the median of all those pairs of differences.

644
01:07:58,810 --> 01:08:04,570
That's that's Delta hat. So again, there is a pair of this is an estimate.

645
01:08:05,560 --> 01:08:13,660
There is a parametric estimate here of the median difference between the two groups that may or not, might, might or might not be valuable to you.

646
01:08:14,320 --> 01:08:19,660
I've never used it, but it is reported and that's what it is.

647
01:08:19,690 --> 01:08:25,690
It's the median difference among all possible pairs of one observation from one group and one from the other.

648
01:08:27,280 --> 01:08:37,510
Again, if you want to do it totally non-intuitive, you got to say confidence interval equals true in the argument for one Cox to test.

649
01:08:39,040 --> 01:08:44,589
I don't know why that was, how it's programed, but it gives you an idea of the shift here.

650
01:08:44,590 --> 01:08:48,250
It's about 14 units in New Zealand between the two groups.

651
01:08:49,150 --> 01:08:55,480
Comedian. I don't know what to do with that if that's clinically meaningful in the world of wine.

652
01:08:56,440 --> 01:09:01,240
Magnesium content. And that's it for today.

653
01:09:01,900 --> 01:09:06,830
Again, I'm not going to try and cram every day with lecture notes until 1120.

654
01:09:06,850 --> 01:09:12,810
If you don't want the topic within or the toughest so the homework assignment doesn't ask you to do most of this,

655
01:09:12,820 --> 01:09:19,720
it simply asks you to apply the facts and test in a couple of settings here in more depth as the semester goes down.

656
01:09:21,340 --> 01:09:25,770
So no class on Monday for next year.

657
01:09:25,780 --> 01:09:33,700
Next Wednesday I have officers will never see you again if you can't come to office.

658
01:09:33,700 --> 01:09:39,010
Hours of admissions sometimes if you want to talk to me and office hours don't work for you, please email me.

659
01:09:39,070 --> 01:10:18,040
And I'm happy to talk to folks so that it would be easy for me to tell you the same answer.

660
01:10:18,040 --> 01:10:30,760
The same thing, I think, is often 1000000 yes.

661
01:10:30,790 --> 01:10:37,510
What smart ones?

662
01:10:38,410 --> 01:10:41,980
Are you really out?

663
01:10:44,710 --> 01:11:07,750
Oh, yeah. Oh, yeah, yeah, yeah, yeah, yeah, yeah.

664
01:11:08,020 --> 01:11:16,120
So we'll see what happens. It was totally blown away if I didn't know.

665
01:11:16,300 --> 01:11:21,520
Right? Sorry.

666
01:11:21,520 --> 01:11:25,140
You were my last words to say.

667
01:11:25,410 --> 01:11:31,990
You know, we need to keep coming.

668
01:11:32,830 --> 01:11:42,670
I would think that they probably should have a few argument.

669
01:11:42,670 --> 01:11:47,980
You're officially on the waitlist. I did. You can email me.

670
01:11:48,100 --> 01:11:52,090
So I sent emails. Everybody in the way. Those that I was talking to.

671
01:11:54,830 --> 01:12:03,470
But if I give you a pen of my just once, you're, you know, identical to you.

672
01:12:04,550 --> 01:12:14,330
And I imagine I saw it. I got it for you. And those people who going to be coming become.

673
01:12:14,690 --> 01:12:24,080
It is, of course. Of course, if it works out between some security postures, some priority or something,

674
01:12:24,800 --> 01:12:34,270
I'm advised that if I edit all this the same stuff six times, I'll check my emails because the courts all me.

675
01:12:34,380 --> 01:12:38,930
She had it. Oh, yeah.

676
01:12:40,090 --> 01:12:44,790
Yeah. Oh, yeah. I saw your name.

677
01:12:44,840 --> 01:12:54,520
Let me check. And I know that the client invoking that is likely to involve 250 people.

678
01:12:56,040 --> 01:12:59,209
Yeah. Yes, I suppose it could be because I'm a signatory.

679
01:12:59,210 --> 01:13:03,070
So what do you expect me to argue with right now?

680
01:13:03,530 --> 01:13:09,520
What do you think is going on? And that is that there should be something.

681
01:13:09,970 --> 01:13:15,980
Okay. So there's not. And also sources like some people would do so.

682
01:13:16,070 --> 01:13:22,970
And so if you believe the data observations aren't in the same quarterback and they play in the same games,

683
01:13:24,430 --> 01:13:29,390
you think the designs are too small, too large? I think it's your inference.

684
01:13:30,490 --> 01:13:32,380
Can you reason out what you think is going on?

685
01:13:34,340 --> 01:13:42,140
Well, currently I'm thinking it's just telling me that patterns are not accurate and I would just run the dependency she was having.

686
01:13:43,070 --> 01:13:46,250
Could you tell me what you think the independent test would do?

687
01:13:46,880 --> 01:13:55,380
Can you tell me ahead of time, do you think? Just the lack of independence can't make the P values too small or too large.

688
01:13:55,500 --> 01:14:00,960
Is there anything you can reason through to tell me? What does what does.

689
01:14:00,970 --> 01:14:05,130
So we're talking about correlation. What does correlation do? Yes.

690
01:14:06,660 --> 01:14:12,220
And it's to make you calm. I am not grading for certain.

691
01:14:13,080 --> 01:14:23,580
I'm grading for an answer that you always think about it. So if you can reason through the these are due to variability.

692
01:14:24,300 --> 01:14:29,490
The double digit. How does the sampling distribution vary? Is the correlation?

693
01:14:29,910 --> 01:14:33,490
Because, you know, distribution get tighter, get wider.

694
01:14:34,310 --> 01:14:38,010
And you think through those. Now.

695
01:14:38,010 --> 01:14:41,190
My bio said students should be able to reason through this.

696
01:14:41,400 --> 01:14:44,790
I don't know how statistics students have been taught this this idea.

697
01:14:46,650 --> 01:14:50,130
Okay. So keep thinking and keep asking me questions.

698
01:14:52,980 --> 01:14:57,540
I've got to single you out. I would want your memory.

699
01:14:57,990 --> 01:15:01,590
Of course. Oh, I guess, like a few questions.

700
01:15:01,590 --> 01:15:09,410
Like the last thing they were talking about. Like. Of, like you use the content, like, options.

701
01:15:10,470 --> 01:15:15,110
Stuff like out of confidence or the role. Is there any way of getting that or you just.

702
01:15:15,410 --> 01:15:19,990
Just get like that? Remember, a confidence interval applies to a population parameter, right?

703
01:15:20,010 --> 01:15:25,260
Um, we don't have a population parameter, so we're not really ever getting into the confidence interval.

704
01:15:25,620 --> 01:15:31,140
I guess, like, I was thinking, like, since it calculates every single like possible like immediate difference,

705
01:15:31,230 --> 01:15:34,580
if you could just get like that like 5%, 95%.

706
01:15:34,590 --> 01:15:40,800
Although I don't know if that's actually that's. Exactly. There are ways to confidence intervals with permutations.

707
01:15:41,880 --> 01:15:51,370
I don't ever see them use the practice. Know, mostly because when people use non parametric tests, they're not interested in parameter estimation.

708
01:15:51,400 --> 01:15:53,740
They're interested in statistical significance.

709
01:15:55,860 --> 01:16:05,980
So also earlier we talk about like example T tests and how even if the histogram says it's not normal, it's like, you know,

710
01:16:06,070 --> 01:16:16,330
large enough sample size, it's like, does that mean that like, um, so we'll talk to us is like better if you have like a smaller sample size.

711
01:16:16,810 --> 01:16:25,060
So that's the argument is that in small samples it wilcoxon test B is better to go with because the large sample isn't going to work out.

712
01:16:28,040 --> 01:16:31,310
That's about the only time I would have somebody use it. Okay.

713
01:16:32,030 --> 01:16:36,679
What I don't like is when people look at histograms of ten observations and

714
01:16:36,680 --> 01:16:40,670
they think that's going to tell them whether normality is questionable or not.

715
01:16:40,700 --> 01:16:44,300
I mean, you can't expect ten operations to produce a bell shape.

716
01:16:44,720 --> 01:16:51,770
Mm hmm. So, sure, you can use them. I just don't like that people use the histograms as a way to testify.

717
01:16:53,390 --> 01:16:59,090
So these are things that you would like decide to use, like you will see your thought on the side,

718
01:16:59,090 --> 01:17:06,620
like, oh yeah, well, Willcox seems like a better way to go then. So I was going to talk about more of this on Wednesday after you guys do the exam,

719
01:17:06,620 --> 01:17:12,010
and not that I don't wanna talk about it now because I don't have a good answer for you.

720
01:17:12,020 --> 01:17:16,070
Well, how do I decide?

721
01:17:17,270 --> 01:17:20,810
I honestly do a parametric test most of the time.

722
01:17:21,930 --> 01:17:27,450
Again because I'm going to report parameter estimates. I mean, difference regression parameter something.

723
01:17:29,630 --> 01:17:33,740
I have done wilcoxon rank some tests. I've done Fisher's exact test instead of a square test.

724
01:17:33,740 --> 01:17:36,860
There's on Fisher's exact test is a non parametric.

725
01:17:41,080 --> 01:17:47,230
Mostly to appease reviewers, mostly to appease folks who want me to do that because they don't want me to assume morality.

726
01:17:48,300 --> 01:17:53,950
Yeah, I know you said a lot about like you're fine with sort of like assuming morality there.

727
01:17:54,220 --> 01:17:59,200
Do people in like real life have a big issue with that?

728
01:18:00,640 --> 01:18:04,750
They are going over a lot of like reviewers and collections and you will run

729
01:18:04,750 --> 01:18:09,300
into reviewers who have all kinds of concepts as to what statistics it's okay.

730
01:18:10,870 --> 01:18:19,720
And is it it is a challenge of our profession is. You know I will have reviewers ask me to report 17 few values.

731
01:18:20,550 --> 01:18:26,350
No, it's not what we do. But at the same time, I want my collaborators to get their works published.

732
01:18:26,360 --> 01:18:30,110
Right. So you will run into this? How to balance that all the time?

733
01:18:30,130 --> 01:18:34,630
Yeah. Picking your battles, sort of the exactly is like, I don't really agree with this, but it's not really.

734
01:18:35,630 --> 01:18:40,580
Creating any false information is a challenge.

735
01:18:41,300 --> 01:18:48,220
It's a very much of a challenge because what we teach individuals, I think, is sometimes a bit right.

736
01:18:48,710 --> 01:18:52,070
We teach you guys to do residual class in linear regression, right?

737
01:18:52,770 --> 01:18:56,500
We've never, ever done a residual plan in practice.

738
01:18:56,510 --> 01:18:59,749
I don't have time to write. There's so many other things we're doing.

739
01:18:59,750 --> 01:19:03,280
So some of this is useful and it's not bad.

740
01:19:03,300 --> 01:19:06,410
It's just that, you know, I'm not going to do it.

741
01:19:06,970 --> 01:19:10,240
Sure. I assume that the residuals are headcounts and variance,

742
01:19:10,320 --> 01:19:18,850
and already I just make that assumption and I go forth with my model not only when I send them, especially the 650 instructors.

743
01:19:20,150 --> 01:19:27,530
Yeah. I think on Wednesday I'm going to talk about this issue because I don't want to I don't want to say that I never use non parametric tests,

744
01:19:27,530 --> 01:19:34,010
but I think it's an important question as to when when do I use them because I don't always have the right answer.

745
01:19:34,520 --> 01:19:41,300
Actually I was thinking like better because to have like different hypotheses on over in 2006.

746
01:19:41,310 --> 01:19:48,400
And so maybe if you're like summarize, I want you to like test the difference between the base and my theory.

747
01:19:48,710 --> 01:19:54,140
You might want to go for 2/10. Yeah, but they're on to test a medium or correct.

748
01:19:54,140 --> 01:19:58,850
If you want to report medians and test for a difference and you want to assume there's a shift, that's.

