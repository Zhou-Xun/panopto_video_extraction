1
00:00:34,490 --> 00:00:42,050
Okay. So we're GSI here.

2
00:00:42,920 --> 00:00:48,110
All right. You probably come to the front and introduce yourself.

3
00:00:50,930 --> 00:00:54,770
Hi. I'm a student.

4
00:00:55,970 --> 00:00:59,299
Yes, I do. Semester. And very nice to meet you guys.

5
00:00:59,300 --> 00:01:03,170
And if you have any questions, you can ask me or Professor Wood.

6
00:01:03,170 --> 00:01:09,079
And he's very nice. Yeah. All right. Thanks. So I hear no complaints.

7
00:01:09,080 --> 00:01:12,500
So we'll start the office for this week.

8
00:01:13,040 --> 00:01:16,459
One, stay after the class and then the the Friday.

9
00:01:16,460 --> 00:01:25,340
So you should. Well, we haven't set it up yet, but you should be able to see the Zoom link before Friday.

10
00:01:25,340 --> 00:01:28,730
So if you have questions, just use the Zoom link.

11
00:01:31,950 --> 00:01:38,190
We don't expect homework in probably two weeks or so.

12
00:01:39,060 --> 00:01:51,670
So but I think. You know, there is a period of trying the class a little bit to large.

13
00:01:52,060 --> 00:01:55,180
You know, usually this large class, there are two sessions.

14
00:01:55,510 --> 00:02:06,549
But I mean, I don't want to say something about, you know, you should either take it or drop it, but you do have a period of time to try this out.

15
00:02:06,550 --> 00:02:12,100
If it's not for your liking, feel free to, you know, pick another you can.

16
00:02:12,100 --> 00:02:15,450
Another one. Okay.

17
00:02:15,690 --> 00:02:21,150
So let's continue. Last time we talk about learning scenarios.

18
00:02:21,750 --> 00:02:28,410
So there's roughly speaking, there are three types of machine learning scenarios.

19
00:02:28,710 --> 00:02:36,010
So those three fundamental types of learning known as supervised learning.

20
00:02:36,030 --> 00:02:40,860
So the learning by example, in this case, you have labeled examples.

21
00:02:40,890 --> 00:02:51,390
You have output inputs from the training data provided to you, and then you should be able to learn the relationship between the input and output.

22
00:02:51,540 --> 00:02:56,280
And then you are with them can do prediction classification in that sentence.

23
00:02:57,210 --> 00:03:00,260
The second type is unsupervised learning.

24
00:03:00,270 --> 00:03:05,750
So the learning without example. So you are giving unlabeled data.

25
00:03:05,760 --> 00:03:08,160
So this is the dataset you're going to analyze.

26
00:03:09,060 --> 00:03:17,130
So the key question is machine learning questions associate with unsupervised learning typically is clustering.

27
00:03:17,160 --> 00:03:22,380
Do you see the group structure, um, from the data at hand?

28
00:03:23,730 --> 00:03:32,370
Can you do that monitoring reduction? So this is sort of if you're familiar with that sort of a technique like principal component analysis,

29
00:03:32,370 --> 00:03:38,610
can you present your current data with low dimensional representations?

30
00:03:38,700 --> 00:03:46,259
So we'll get there. We'll talk about that. So the the key characteristic of those things are you don't have to label the data.

31
00:03:46,260 --> 00:03:49,520
No. Right. So there's the there is no distinction.

32
00:03:50,370 --> 00:03:59,070
We don't distinguish input and output. Right. In the way we're trying to find the pattern from the data into unsupervised learning.

33
00:03:59,340 --> 00:04:03,420
We're trying to find the group structure. So that's one one of the pattern.

34
00:04:05,040 --> 00:04:14,500
So that's learning without example. The third type, which we will not talk about in this class, is reinforcement learning.

35
00:04:14,520 --> 00:04:19,120
So this is last time we set is learning through interactions.

36
00:04:19,180 --> 00:04:26,880
Right. So usually the typical scenario is that you play a game, a chess game, a goal game.

37
00:04:27,180 --> 00:04:33,060
So you have opponent. So they are going to play a strategy to you.

38
00:04:33,330 --> 00:04:38,850
In general, I think you don't have to have a a game type of scenario.

39
00:04:39,120 --> 00:04:45,520
You can think about the clinical trials in this. A lot of the scenarios the environment gave you feedback.

40
00:04:45,540 --> 00:04:48,600
So that's sort of the component in this case.

41
00:04:48,960 --> 00:04:53,880
So the the key characteristic is learning through interactions.

42
00:04:54,330 --> 00:05:04,250
So you make some decision and then the environment gave you feedback and then you use this feedback to inform the next position.

43
00:05:04,260 --> 00:05:13,410
So this is a more from the traditional artificial intelligence line of thinking, but has been pretty.

44
00:05:15,150 --> 00:05:19,230
Important topic recently, but we are not talking about this.

45
00:05:19,740 --> 00:05:23,820
So. So those are learning scenarios.

46
00:05:24,960 --> 00:05:33,560
So last time we said. So machine learning is a scientific study of algorithms.

47
00:05:35,090 --> 00:05:39,610
And then at the same time we form our statistical training.

48
00:05:39,620 --> 00:05:42,710
We talk a lot about models, right?

49
00:05:42,740 --> 00:05:51,920
So these are the probably the most confusing either terminologies or concepts in the field.

50
00:05:51,920 --> 00:05:58,100
When you talk about what is an algorithm, what is the model, then you get people confused.

51
00:05:58,220 --> 00:06:01,980
There is a reason to wait. All right.

52
00:06:01,980 --> 00:06:05,010
So we are not trying to clarify.

53
00:06:05,010 --> 00:06:09,010
This is is in certain degree, it is impossible to clarify the.

54
00:06:10,260 --> 00:06:16,650
But you can see some of the difference. All those verses.

55
00:06:23,190 --> 00:06:34,740
All right. So, Mo, those AI algorithms are all have more broader impact in the in than what they have in the context of a machine learning.

56
00:06:34,860 --> 00:06:40,829
Right. We talk about model, not necessarily just talking about a statistical model.

57
00:06:40,830 --> 00:06:46,290
You can have a physics model. We can have a chemistry model, a chemical reaction model, so and so forth.

58
00:06:47,430 --> 00:06:51,659
Algorithm is the same thing. You can have these algorithms.

59
00:06:51,660 --> 00:06:53,850
We have to deal with uncertainty.

60
00:06:53,850 --> 00:07:00,830
So some of the statistical algorithm, machine learning algorithm, there are also simple algorithms like sorting algorithms.

61
00:07:00,870 --> 00:07:04,230
This is the probably the first thing you gonna learn in computer science.

62
00:07:04,950 --> 00:07:13,250
So for general science. You can't distinguish these two pretty clearly.

63
00:07:13,850 --> 00:07:21,640
So the algorithm is essentially a set. Of computational instructions.

64
00:07:29,080 --> 00:07:33,670
Just take in your input and then provide output to produce.

65
00:07:38,800 --> 00:07:43,120
I'll put. From input.

66
00:07:46,980 --> 00:07:51,690
So the key thing here is algorithm is usually a computational procedure.

67
00:07:52,090 --> 00:08:00,390
Right. You give me your input, I give you output. So mathematically speaking, it's also, you know, this is synonymous with function.

68
00:08:00,590 --> 00:08:08,100
Right. You gave me a if it's a there is a mathematical function, you give me your input and then my function speed out the output right there.

69
00:08:08,100 --> 00:08:15,400
So unique output should be. Then that same algorithm is also.

70
00:08:18,990 --> 00:08:27,330
So that equivalence a little bit. Well, I mean, the relevant here is we're going to say finding machine learning algorithms ended

71
00:08:27,330 --> 00:08:33,720
up with just finding the proper functions to do your machine learning task.

72
00:08:34,230 --> 00:08:41,190
Right. So this is a set of computational. But the emphasis here is in the.

73
00:08:42,970 --> 00:08:47,350
You know, the practical computational purpose, they're usually a salve.

74
00:08:47,590 --> 00:08:56,350
So you see bioinformatics, pipelines, they are you know, there is usually multi steps of instruction,

75
00:08:56,350 --> 00:09:00,850
not just the instructions and multi steps of blocks off instructions.

76
00:09:00,850 --> 00:09:05,070
How do you process your data and give you the well in some sense,

77
00:09:05,090 --> 00:09:14,050
like cooking recipe is then right to produce from your ingredients to your final output, like a cake or a dish.

78
00:09:15,400 --> 00:09:20,260
So that's the algorithm. This algorithm is pretty easy to understand,

79
00:09:20,500 --> 00:09:29,800
but usually I think what it make it difficult to grasp or what people means by algorithm is we also have the concept of model.

80
00:09:31,900 --> 00:09:41,500
What is a model? Right. I'm not trying to give you a, you know, a universally accepted definition for the model,

81
00:09:41,860 --> 00:09:49,960
but for the purpose of this class, it's fair to say this model is really a description.

82
00:09:55,650 --> 00:10:16,810
Often data generated. They're seemingly unrelated.

83
00:10:16,890 --> 00:10:24,780
There is the rightfully so on the model of the way I think about it is a set of assumptions.

84
00:10:25,260 --> 00:10:29,670
It's about your assumption on how the data at hand is generated.

85
00:10:30,780 --> 00:10:34,980
All right. So so this will be a description of the data generative mechanism.

86
00:10:36,150 --> 00:10:41,820
Again, this sort of a definition is broader than just statistical models.

87
00:10:42,150 --> 00:10:47,940
So you can think about physics model like Newton's law or the more fancy model.

88
00:10:47,940 --> 00:10:56,910
That doesn't matter. I think Einstein's general relativity, this is all kind of what describe how objects interact with each other.

89
00:10:56,930 --> 00:11:03,740
Right. What the law at a certain sense if you understand the model, you know how the data generated.

90
00:11:03,770 --> 00:11:10,650
If you know Newton's law, you should understand how this any two objects.

91
00:11:11,520 --> 00:11:16,659
Well. Too broad to say that to be correct in physics.

92
00:11:16,660 --> 00:11:26,860
But you understood. So the point is, if you understand the underlying assumption, if you assume the model is correct, you can simulate data from it.

93
00:11:27,370 --> 00:11:33,490
Right. So that's what we call a state of general algorithms.

94
00:11:33,970 --> 00:11:40,210
So in the way, this is not computational instruction, this is a bunch of assumptions.

95
00:11:43,260 --> 00:11:48,880
Assumption is probably a weak words, but you can have like, you know,

96
00:11:48,920 --> 00:11:59,550
the definition of relationships, but those are just put into the, um, the umbrella of assumptions.

97
00:12:00,180 --> 00:12:08,690
Um. In statistical model, we're usually dealing with, we're usually dealing with uncertainty.

98
00:12:08,690 --> 00:12:13,250
So we need to know why, how the uncertainty arise, right?

99
00:12:13,670 --> 00:12:18,409
So we're typically thinking about example this, right?

100
00:12:18,410 --> 00:12:27,380
So the algorithm versus model, the best way to probably understand this, everybody should understand this is the least a square.

101
00:12:31,760 --> 00:12:37,880
Which is the algorithm. If you've got a bunch of points, it tells you if you have a bunch of points.

102
00:12:40,020 --> 00:12:43,670
At least the square tells you how to find a straight line.

103
00:12:43,680 --> 00:12:49,739
So minimize the distance. The scatter in the points to the line.

104
00:12:49,740 --> 00:12:56,850
So this is the least the square is a simple function is also a very easy algorithm.

105
00:12:57,480 --> 00:13:01,710
In terms of the model, you would say these the square is a modal.

106
00:13:02,250 --> 00:13:07,710
It doesn't really tell you how the data generated. But what is a model for.

107
00:13:09,510 --> 00:13:13,590
Sometimes we're dealing with the situation. So this is a probably the linear model.

108
00:13:18,680 --> 00:13:26,210
In a way you don't necessarily need to apply the lease, the square and.

109
00:13:28,600 --> 00:13:39,790
Only if you have a mold to it do you see a scatter point that you can always apply some sort of least square to find the optimal right.

110
00:13:41,450 --> 00:13:45,890
The important thing, though, is that doesn't really tell you all the data generated.

111
00:13:46,040 --> 00:13:52,310
It doesn't really tell you what the relationship between the line and the points.

112
00:13:52,760 --> 00:13:56,480
There is no interpretation just by. Itself.

113
00:13:57,230 --> 00:14:01,540
What gave the meaning of the interpretation is usually through the model.

114
00:14:03,290 --> 00:14:04,350
Think about it. Right.

115
00:14:04,370 --> 00:14:15,140
So the model, though, it says, okay, my data points are generated from some sort of the normal distribution, which Expedia defines that straight line.

116
00:14:15,650 --> 00:14:23,410
And now we can have Sigma Square describe the scanner's right distance.

117
00:14:23,870 --> 00:14:27,500
They are scattered from them. The regression regression line.

118
00:14:29,140 --> 00:14:39,010
The connection between the two is if you happen to have this model and then if you have follow, say, maximum, maximum, maximum likelihood.

119
00:14:39,010 --> 00:14:43,300
Principle two Trying to find the best fit of the x beta.

120
00:14:43,570 --> 00:14:48,150
Then your solution becomes. The loose the square algorithm.

121
00:14:48,450 --> 00:14:48,780
Right.

122
00:14:49,260 --> 00:14:57,930
I think there's there's there's a lot of analyzes that just tell you, if you just run the least a square, you don't have to have a normal assumption.

123
00:14:58,560 --> 00:15:00,700
You know, there is a lot of situations.

124
00:15:00,970 --> 00:15:14,260
The algorithm can apply more than you don't have a normal assumption have these this type of the linear model the situation.

125
00:15:15,850 --> 00:15:25,750
That tells you a little bit of the difference between the two and that you shouldn't confuse the two in a statistical model.

126
00:15:26,020 --> 00:15:31,419
We are trying to understand the mechanism, how the data generated in most of the statistics.

127
00:15:31,420 --> 00:15:39,850
That's the case to at least from the way we study the we if we were trying to be with the bold, we're trying to understand how the data,

128
00:15:41,140 --> 00:15:48,010
if we view the model, then at least we can simulate from the model that you can never simulate from an average.

129
00:15:49,190 --> 00:16:00,380
Okay. So the algorithm, you can see this is a way to fit the data to feed them all to the data.

130
00:16:00,860 --> 00:16:03,500
Right. You have a mold the that you have observed the data.

131
00:16:03,770 --> 00:16:09,860
Now you need to find out what is the model parameter in this case, what is the theta, what is Sigma Square?

132
00:16:10,460 --> 00:16:16,400
And then the process to feed the model to the data, you need to find the algorithm.

133
00:16:18,450 --> 00:16:22,290
Okay. Um. So they are connected.

134
00:16:22,500 --> 00:16:25,829
If you only have a model that's probably useless.

135
00:16:25,830 --> 00:16:36,790
I can describe a very complicated process, like my thinking on, you know, on how things are generated.

136
00:16:36,810 --> 00:16:40,830
But that's irrelevant if I cannot feed them.

137
00:16:40,830 --> 00:16:50,550
Although to observe the data to prove I have the I have an appropriate assumption or I have a good model of that model.

138
00:16:51,000 --> 00:16:57,510
So having an algorithm is important to validate your model, to learn from your model.

139
00:16:57,960 --> 00:17:02,340
But the model is really how you can interpret the everything.

140
00:17:03,120 --> 00:17:08,879
Okay. So, so well, I mean, at this we're going to go through this throughout the semester.

141
00:17:08,880 --> 00:17:17,040
But at this point, if you can understand, if you can appreciate the model, one of two different things, I think that's my goal.

142
00:17:17,430 --> 00:17:24,030
Okay. And then they are always intertwined together.

143
00:17:25,410 --> 00:17:34,590
Sometimes if you have a model like this, all very, very complicated, and then there is no numerical recipe, you can fit the model to the data.

144
00:17:35,280 --> 00:17:44,960
And this is a useless model. A lot of busy models are deemed useless before we have them to feed that sort of a model.

145
00:17:44,990 --> 00:17:55,760
Before the M.S., M.S., we can do a lot of things. So and if you have an hour with them, though, the current situations, people give you algorithm.

146
00:17:56,360 --> 00:18:05,330
But the algorithm, if you can think about this, is really trying to feed that the fee, the underlying assumption to that data.

147
00:18:06,230 --> 00:18:12,200
Right. So there is a trend that or at least that there's there's something we cannot control

148
00:18:12,200 --> 00:18:16,880
where all the people invent algorithms without saying there are multiple ways.

149
00:18:17,240 --> 00:18:22,430
So there are some limitations with that type of approach. This is a very common in machine learning.

150
00:18:22,700 --> 00:18:31,399
So you understand the algorithm. You can reproduce the algorithm, but you don't necessarily can explain what the model,

151
00:18:31,400 --> 00:18:35,900
what what the underlying model is, how the data generative algorithm is.

152
00:18:36,140 --> 00:18:39,530
So this is usually known as the black box algorithm.

153
00:18:39,530 --> 00:18:49,870
So there is no clear. Description on the data genre algorithm, but still that makes its a valid algorithm.

154
00:18:50,410 --> 00:18:57,130
Just for machine learning is still a valid thing to do because are a scientific discipline to study algorithm.

155
00:18:58,900 --> 00:19:09,730
I think the discussion here is just trying to give you a sort of opinion or to tell you what what it is.

156
00:19:09,820 --> 00:19:16,390
Right. I mean, in the in the world of machine learning, you're going to constantly struggle with algorithm.

157
00:19:16,720 --> 00:19:19,090
What is algorithm, what is not a model?

158
00:19:19,330 --> 00:19:28,600
You may find a certain algorithm like the deep learning algorithm we will get into touch on later this semester.

159
00:19:29,650 --> 00:19:37,510
As powerful as it is, you usually don't have a way to try to make sense out of it, right?

160
00:19:37,510 --> 00:19:46,839
So if you are just trying to do prediction, that's fine. But if you are trying to understand in certain situations, for example, in the in the,

161
00:19:46,840 --> 00:19:52,020
in the molecular biology situation, you feel a lot of gene expressions to a deep learning model.

162
00:19:52,030 --> 00:19:57,520
And then you're trying to predict. The outcome of a disease.

163
00:19:57,910 --> 00:20:02,230
At the end of the day, you may do the prediction pretty accurately,

164
00:20:02,440 --> 00:20:09,320
but you still don't understand how the genes, so many different genes, actually being relevant to that disease.

165
00:20:09,370 --> 00:20:13,710
The mechanism is unknown because that's not a model.

166
00:20:13,720 --> 00:20:18,940
That's usually just a description of our model is completely different thing.

167
00:20:21,110 --> 00:20:26,090
A different. So the statistical model usually is more specific than this.

168
00:20:26,330 --> 00:20:29,840
It's not just a description of a data generated mechanism.

169
00:20:30,170 --> 00:20:33,770
So usually we can further characterize it too.

170
00:20:34,010 --> 00:20:38,510
So if you have a covariates and you have predictors, you have outcome.

171
00:20:38,900 --> 00:20:50,390
Then the statistical model usually referred to the underlying joint distribution that's on Y X being the predictor, Y being the outcome.

172
00:20:50,780 --> 00:20:54,620
Right. Sometimes we. So this is the joint distribution.

173
00:20:54,620 --> 00:21:02,450
The obviously is a model. If we gave you that joint distribution, then given Y well, given X on Y, you can simulate.

174
00:21:02,840 --> 00:21:10,430
Right. You give the joint distribution that you can run the our session to simulate from this distribution if your distribution is specified.

175
00:21:10,910 --> 00:21:15,500
So this is sometimes equivalent to just given Y given that.

176
00:21:16,370 --> 00:21:24,700
So the conditional distribution usually is you like in the regression model, we don't really need to know the distribution of acts.

177
00:21:25,280 --> 00:21:31,310
We just need to know the distribution of Y given that. Right.

178
00:21:31,370 --> 00:21:40,340
The point here is for statistical model. If you know the density, the drawing density or the conditional density, they usually specify the model.

179
00:21:40,940 --> 00:21:45,469
Okay. The algorithm use usually trying to.

180
00:21:45,470 --> 00:21:50,500
Well, I mean, if you have a model that usually you automatically get into,

181
00:21:50,900 --> 00:21:56,600
you can find an algorithm based on some principles, either maximum likelihood or Bayesian.

182
00:21:56,900 --> 00:22:03,230
So those will tell you the procedure to derive the algorithm to feed the model to the data.

183
00:22:04,030 --> 00:22:07,130
Yeah. Okay.

184
00:22:08,690 --> 00:22:17,240
Maybe fuzzy a little bit here. When we get to the the detail, the contacts, I think I'll try to make it clear.

185
00:22:17,630 --> 00:22:21,920
Any questions? Okay.

186
00:22:22,100 --> 00:22:25,880
So there are two different things. One more thing.

187
00:22:25,970 --> 00:22:33,140
Okay. I think I mentioned that point. If you have a mold or you cannot fit, that's useless.

188
00:22:33,800 --> 00:22:38,690
Right. And then if you have an algorithm, you cannot explain the underlying mold.

189
00:22:38,690 --> 00:22:42,260
Or sometimes it's okay that this is just a common practice.

190
00:22:42,500 --> 00:22:53,030
But there are more interaction between the two. Sometimes you have a mold all too complex to be fed with modern computing algorithm.

191
00:22:53,330 --> 00:23:01,160
You may modify your model to a degree that you can actually get answer from it.

192
00:23:01,820 --> 00:23:11,630
To be able to fit exemple that is again in the in the in the Bayesian case before we have an CMC algorithm.

193
00:23:11,960 --> 00:23:16,240
You see a lot of so called the conjugate priors. Right.

194
00:23:16,260 --> 00:23:22,650
So those things are appealing because they are compute, they computable, they're computable.

195
00:23:22,740 --> 00:23:30,090
If you have a conjugate prior, then you can find the you can find the necessary output through analytic calculation.

196
00:23:30,870 --> 00:23:36,059
It's not because the conjugate prior makes a lot of sense in reality.

197
00:23:36,060 --> 00:23:39,660
In practice, it's usually is because you can fit them.

198
00:23:40,020 --> 00:23:45,540
So that's a model is use of all your has to be able to find the algorithm to fit it.

199
00:23:45,600 --> 00:23:51,690
So that's kind of an important that's actually is an important factor.

200
00:23:51,750 --> 00:24:00,050
So again, trying to emphasize you have too complicated that a model cannot you cannot fit, then that's useless.

201
00:24:00,060 --> 00:24:03,470
Yes. Can you speak up? Yeah.

202
00:24:03,600 --> 00:24:12,260
I should give you this. You like when you must have, like, a basketball.

203
00:24:13,860 --> 00:24:22,260
I'll. Yeah. Good question.

204
00:24:22,270 --> 00:24:29,800
I think if you are a really come from a sane scientific background.

205
00:24:29,990 --> 00:24:37,260
But yes, I think traditionally you come up with a model and then you fit the model to the data.

206
00:24:37,270 --> 00:24:43,240
So this has been the practice of physics. There's has been you know, it's been like 100 years of.

207
00:24:44,450 --> 00:24:54,770
But a lot of there's things are changing now for the reason it's we're trying to study the very complex system.

208
00:24:55,280 --> 00:25:04,130
Well, I still think that's a good at least this is a good principle to to conduct your conduct your science.

209
00:25:04,340 --> 00:25:08,930
But again, not a lot of things are scientific discoveries.

210
00:25:09,830 --> 00:25:18,140
For example, if you're trying to predict something will happen later on or where your current

211
00:25:18,670 --> 00:25:23,030
the knowledge is really limited and then you're trying to do prediction,

212
00:25:24,050 --> 00:25:32,840
what you what you can do is based on the pattern. So what I'm trying to say is you don't need to understand the mechanism to do the prediction.

213
00:25:33,320 --> 00:25:46,100
Example of this is you can think about, you know, human start doing navigation for very long time without really understand the astronomy.

214
00:25:46,280 --> 00:25:54,410
Right. So we but they know how to use Star as a pattern to guide their the direction they go.

215
00:25:54,680 --> 00:25:59,980
Right. So to make sure they don't get lost. In the sea.

216
00:26:00,050 --> 00:26:01,940
So so these are there similarity.

217
00:26:01,940 --> 00:26:13,730
They're they're some of the practice like diagnosis, prediction classification we have to do every day without fully understand their mechanism.

218
00:26:14,360 --> 00:26:21,560
Right. So in that case, you may come up with the out the algorithm without knowing the detail of the mechanism.

219
00:26:21,710 --> 00:26:26,030
We cannot delay everything until we we understand everything but start doing things.

220
00:26:26,030 --> 00:26:30,080
That's just impractical. That's being said.

221
00:26:30,500 --> 00:26:41,330
If you do understand the mechanism, usually you will have a much better algorithm there, some more precise get back to the navigation example, right?

222
00:26:41,690 --> 00:26:49,600
So the more physics, you know, the better navigation tools you kind of have nowadays you have GPUs, right?

223
00:26:49,610 --> 00:26:59,209
So the GPS is actually based on, you know, to the tiny bit of the relativity, the Einstein's general relativity theory.

224
00:26:59,210 --> 00:27:04,910
So the thing, you know, in terms of a model, well, it gave you a better algorithm.

225
00:27:05,880 --> 00:27:09,090
So that's kind of sort of also the interaction.

226
00:27:09,150 --> 00:27:17,670
This is a bit philosophical. I think a lot of people will be fine to say you can go out and just study out with them, but.

227
00:27:19,410 --> 00:27:23,900
Might I personally? Would disagree with that.

228
00:27:23,920 --> 00:27:33,940
It's the right way to do. If you do science of scientific discovery, you still need to rely on multiple practically.

229
00:27:33,940 --> 00:27:39,010
If you go to a business, you're just trying to predict what's going to one customer are going to come.

230
00:27:39,460 --> 00:27:47,050
What kind of you know the the titles the other you know that the music titles a customer going to like.

231
00:27:47,320 --> 00:27:50,400
You don't have to understand the psychology. Right.

232
00:27:50,410 --> 00:27:55,210
So you could do that. It's a good question. All right.

233
00:27:56,170 --> 00:28:03,130
It's a bit philosophical. But if I use this terminology, that's what I meant.

234
00:28:04,330 --> 00:28:08,709
Another important thing is we are in this department.

235
00:28:08,710 --> 00:28:16,250
At least a lot of courses are give you train in terms of this part of the equation.

236
00:28:16,350 --> 00:28:22,000
Once this part of the world, the modeling world, not so much on the algorithm.

237
00:28:22,240 --> 00:28:27,700
If you are in the computer science department, then that you are over exposed to it through the algorithm.

238
00:28:28,150 --> 00:28:34,870
I think peer from here to understand the algorithm, it's natural.

239
00:28:36,000 --> 00:28:43,070
Okay. So we'll try to see how it works in terms of machine learning.

240
00:28:49,240 --> 00:28:55,450
So in 601 Utah or another applied sequence we talked a lot about.

241
00:28:57,740 --> 00:29:03,860
You know how to come up with models in this form or or this form.

242
00:29:05,300 --> 00:29:09,730
Right. So there's different regression models. There are.

243
00:29:13,100 --> 00:29:23,390
So in a way are you already know, a lot of algorithm, but now you need to apply this algorithm to specific tasks we have here to discuss.

244
00:29:24,990 --> 00:29:28,920
All right. Good. Any other questions? Good.

245
00:29:29,370 --> 00:29:35,610
All right. So we're going to start for the second lecture, which is the learning theory.

246
00:29:36,000 --> 00:29:47,880
This is a bit of. This tells you a little bit about the general guiding principle in supervising.

247
00:29:56,620 --> 00:30:00,910
So it tells you what is a good algorithm? What is the optimal algorithm?

248
00:30:01,780 --> 00:30:04,959
How do we evaluate the algorithm in general?

249
00:30:04,960 --> 00:30:10,240
In practice, it's more rather general setting. It applies to any types of algorithm.

250
00:30:10,840 --> 00:30:17,290
Let it be, you know, a deep learning algorithm, a support vector machine, so on and so forth.

251
00:30:17,500 --> 00:30:30,960
So. Well. All of these learning algorithms for the optimal, at least for supervised learning, starts from the statistical physician theory.

252
00:30:45,040 --> 00:30:49,150
So the physician problem here is trying to.

253
00:30:52,300 --> 00:30:55,390
Tackle this issue. So you need to make a decision.

254
00:30:55,510 --> 00:31:04,000
Right. So the decision being as simple as do you need to bring an umbrella when you leave home?

255
00:31:06,080 --> 00:31:10,940
You know, this sort of accusation in the presence of the uncertainty.

256
00:31:10,940 --> 00:31:14,720
The uncertainty is you don't really know it's going to rain or not.

257
00:31:15,130 --> 00:31:21,020
Right. So this is a simple decision. A lot of decisions is more complicated.

258
00:31:21,020 --> 00:31:25,670
An important decision like treatment, decision for disease or.

259
00:31:27,680 --> 00:31:36,440
You know, any type of statistician you can think of. In real life, there is always a factor of uncertainty present.

260
00:31:36,530 --> 00:31:43,099
So you need to make a decision. So this this theory is related to it's very simple, by the way.

261
00:31:43,100 --> 00:31:48,700
It's not it's not difficult at all. To guide you.

262
00:31:48,700 --> 00:31:51,970
What is the optimal position should be?

263
00:31:52,250 --> 00:31:54,010
Okay. There are some very simple.

264
00:31:54,140 --> 00:32:05,620
The set of the statistical the theorem is is the following is has a long similarity to the settings of the to the supervisor learner.

265
00:32:05,710 --> 00:32:10,440
So first of all, we're going to have predictors, right?

266
00:32:10,450 --> 00:32:19,659
So we're going to have X in the domain of our P so that just basically say X is a P vector.

267
00:32:19,660 --> 00:32:25,810
So you have a P different features for, for you to make a single prediction.

268
00:32:27,720 --> 00:32:34,700
So this is usually called input. Features or feature back to.

269
00:32:39,510 --> 00:32:45,180
Predictors. Marriott's.

270
00:32:48,320 --> 00:32:57,000
Depends on the context. Right. So in the machine learning word, this is feature vector is probably after use.

271
00:32:57,020 --> 00:33:01,190
But if you're from a regression background, that's perfectly fine.

272
00:33:01,190 --> 00:33:14,809
These are just the predictors of virus. Um, you're going to have an outcome, but it's a single Y come from R so as it could be a vector obviously.

273
00:33:14,810 --> 00:33:22,610
But I think in most of the problems without loss of generality here, we just consider the outcome in the regression.

274
00:33:23,870 --> 00:33:33,800
Value, like somebodys blood pressure. You're trying to use gene expression information to predict the blood pressure or.

275
00:33:35,910 --> 00:33:40,380
Why can be pylori like disease. Case control status.

276
00:33:40,770 --> 00:33:45,000
Disease. None. Disease status. So this is known as outcome.

277
00:33:50,790 --> 00:33:55,949
Predictive value output or classified value.

278
00:33:55,950 --> 00:33:59,819
So those are classified into certain outcomes.

279
00:33:59,820 --> 00:34:05,310
So you need to have these two, right? So this is a this is the same set up of supervised learning.

280
00:34:05,490 --> 00:34:12,570
So you have the input then you're trying to we're trying to see something about the output.

281
00:34:13,620 --> 00:34:18,570
So for statistical decision theory, the third component is actually the multiple.

282
00:34:20,650 --> 00:34:24,120
So we need to know what we don't have to know.

283
00:34:24,130 --> 00:34:27,340
But there is a relationship between X and Y.

284
00:34:28,210 --> 00:34:33,670
You may assume they are independent. You are assuming some sort of a relationship to.

285
00:34:35,940 --> 00:34:43,730
It's actually just using the center console. Some of.

286
00:34:45,940 --> 00:34:52,280
On X and Y. So if you don't like the work model though,

287
00:34:52,310 --> 00:35:01,430
so this is a basic relationship how the X and Y are connected so the model can be completely hidden from right.

288
00:35:01,430 --> 00:35:07,129
So you don't know, but you can learn them all. You could try to estimate the model.

289
00:35:07,130 --> 00:35:10,370
So the relationship between X and Y.

290
00:35:10,760 --> 00:35:15,920
So as we know, this is defined by distribution or.

291
00:35:19,520 --> 00:35:22,550
So sometimes we write he acts on y, right?

292
00:35:22,630 --> 00:35:26,150
So sometimes we assume the relationship front.

293
00:35:26,360 --> 00:35:32,510
So writing in this form Y because of x plus epsilon.

294
00:35:33,050 --> 00:35:36,740
So this is equivalent, but this is more general representation.

295
00:35:37,340 --> 00:35:41,970
Right. So this one has more assumptions. It is a more detailed mode.

296
00:35:42,050 --> 00:35:45,620
This is a very abstract. The model just saying that's one.

297
00:35:46,040 --> 00:35:50,720
This one has more assumptions. And you have to realize this.

298
00:35:50,930 --> 00:35:59,150
So this is an additive model. Right. So the random arrow is added to a median function in terms of F of X.

299
00:35:59,720 --> 00:36:02,360
Right. So this is not your only choice. Right.

300
00:36:02,420 --> 00:36:09,500
In the different model, though, in the different set of assumptions, you could assume there there's a small decrease in power.

301
00:36:09,630 --> 00:36:16,060
Right. So it's time. That's maybe a potential relationship between acts of.

302
00:36:16,930 --> 00:36:28,180
Here. It's just trying to say you could. The true model is have different levels of specification, but for the purpose of statistical decision theory,

303
00:36:28,600 --> 00:36:32,470
you could assume that the general relationship between X and Y.

304
00:36:33,330 --> 00:36:37,560
So you know this notice the fourth one.

305
00:36:46,340 --> 00:36:51,410
So we need a learning algorithm or a prediction algorithm.

306
00:36:56,820 --> 00:36:59,940
So far, it's independent. Okay.

307
00:37:00,300 --> 00:37:04,950
A learning algorithm is simply defined as a function mapping from the.

308
00:37:07,670 --> 00:37:10,940
Input, too. All right.

309
00:37:11,270 --> 00:37:18,310
So the way I write this is this function takes the input speed of output.

310
00:37:18,830 --> 00:37:26,350
Right. So that's. If you observe something, why?

311
00:37:26,360 --> 00:37:30,030
Let's say dagger equals b f.

312
00:37:32,710 --> 00:37:38,470
Baxter. So the X Factor is the.

313
00:37:42,390 --> 00:37:49,530
Some value trying to prove that right and that if you feed into the function X, this is your learning algorithm.

314
00:37:49,530 --> 00:37:53,790
Your conviction algorithm acts as speed output, white decker.

315
00:37:53,790 --> 00:37:58,739
So that's your predictive value. That's your outcome from your learning.

316
00:37:58,740 --> 00:38:02,340
How so? You need to have a component here.

317
00:38:03,010 --> 00:38:07,740
Component of the learning algorithm we define as a functional.

318
00:38:08,680 --> 00:38:11,730
Pretty mathematically valid, right?

319
00:38:11,750 --> 00:38:21,300
So we define algorithm as a bunch of procedures for taking the inputs and outputs that satisfy the satisfy balance.

320
00:38:23,550 --> 00:38:26,790
And then finally. All right.

321
00:38:26,910 --> 00:38:34,290
So so far, these two things are these that they don't really interact with each other.

322
00:38:34,560 --> 00:38:42,450
So you can come up with a new algorithm or any function, taking input and then give you output.

323
00:38:42,720 --> 00:38:53,310
How do you know which one is better? So that's the key question, which is in some way, I think ultimately you want to know the optimal.

324
00:38:54,900 --> 00:38:59,700
Right. So you're not intuitive since you want the algorithm that's most accurate.

325
00:39:00,300 --> 00:39:03,390
But how do you evaluate the accuracy? That's the question.

326
00:39:05,040 --> 00:39:08,910
So you need to act in something to be able to do that.

327
00:39:10,160 --> 00:39:13,490
So you need to tell me, how do you evaluate accuracy?

328
00:39:14,090 --> 00:39:17,420
So this is the role of so-called function.

329
00:39:18,090 --> 00:39:21,800
So this is five one of the most important things.

330
00:39:21,830 --> 00:39:24,870
It's a function. Taking our.

331
00:39:26,110 --> 00:39:29,720
And. Um.

332
00:39:29,900 --> 00:39:34,270
Maybe just. Skip to.

333
00:39:35,360 --> 00:39:46,830
So this is known as the loss function. Besides this, the mathematical notation it takes.

334
00:39:48,770 --> 00:39:59,000
A predicted value. Oh, I think it's a true value and then the predictive value and then tells you how accurate they are.

335
00:39:59,530 --> 00:40:05,799
So the last function in the sense you already know that you know some some value lost function.

336
00:40:05,800 --> 00:40:10,090
If the Y and the white back are the same, your loss is zero.

337
00:40:10,450 --> 00:40:16,510
And then, um, the larger the discrepancy is, the larger the loss should be.

338
00:40:16,930 --> 00:40:22,780
But it's up to you. How do you define the level of discrepancy?

339
00:40:22,780 --> 00:40:30,430
And then the larger the the level of the loss, you want to penalize that discrepancy.

340
00:40:30,640 --> 00:40:37,690
All right. So let me just first of all right. So this one is also equal to one, Y and F.

341
00:40:38,910 --> 00:40:42,090
To the Max Dagger.

342
00:40:44,340 --> 00:40:47,420
Marcus. Okay. Can you see?

343
00:40:47,430 --> 00:40:51,620
I actually think this. That's not very.

344
00:40:54,160 --> 00:40:57,970
It's not very good. Make sense.

345
00:40:58,720 --> 00:41:06,290
So for example, that is well in in regression we always use square loss example of not.

346
00:41:06,790 --> 00:41:09,970
So this is let's say, accuracy.

347
00:41:10,240 --> 00:41:17,590
So this tells you how to quantify or how to evaluate or how to assess our accuracy.

348
00:41:18,040 --> 00:41:29,080
So this is a accuracy quantification. This is the key component for making decisions, for evaluating algorithms in this case.

349
00:41:29,360 --> 00:41:40,030
Right. So the example of this.

350
00:41:43,510 --> 00:41:50,240
The square loss. So we're all familiar with this.

351
00:41:50,260 --> 00:41:55,180
This is the the square also spilled into the least the square outwards.

352
00:41:55,600 --> 00:42:10,180
Right. So this is triangle. So l y and y you have that being the fitted value, y being the true value is defined as y y square.

353
00:42:14,790 --> 00:42:20,669
So usually there is no constraint what kind of the loss function you can use.

354
00:42:20,670 --> 00:42:27,730
You can actually use their. The general principle is in regression problems.

355
00:42:27,740 --> 00:42:43,600
If you have something is precisely predicted that you should have lost zero and then you have that small atomic relationship is expected.

356
00:42:43,730 --> 00:42:46,850
The larger the discrepancy, the larger the loss.

357
00:42:47,120 --> 00:42:55,100
But other than that is so volatile that monotonically and then the zero adds no discrepancy

358
00:42:55,460 --> 00:43:00,560
other than this there is probably no constrains what kind of the last function you could use.

359
00:43:00,980 --> 00:43:10,129
So but nevertheless, this is an important component for regression problems in I should say if the other

360
00:43:10,130 --> 00:43:14,200
prediction problem is usually called the regression problem in machine learning.

361
00:43:14,240 --> 00:43:17,870
So this is just I use this two words interchangeably.

362
00:43:18,260 --> 00:43:26,030
So for prediction type of the problems is otherwise specified, we usually use the square loss.

363
00:43:28,510 --> 00:43:33,100
There is a different story for classification problems.

364
00:43:33,220 --> 00:43:40,840
The different algorithms may use different type of the loss function or get there so of one type of the.

365
00:43:42,670 --> 00:43:48,510
Loss function people use for. It's a binary classification.

366
00:43:48,530 --> 00:43:55,780
So this is an example to. So for example, two example two is called a01 loss.

367
00:43:57,180 --> 00:44:01,110
This is a four binary classification. Right. So you are.

368
00:44:01,380 --> 00:44:04,800
So all this objects that can be classified either zero or one.

369
00:44:05,220 --> 00:44:08,920
If you get classification. Correct. Again.

370
00:44:08,920 --> 00:44:12,310
You don't get loss his means you have a loss. Zero.

371
00:44:12,430 --> 00:44:16,870
If you have a wrong answer, in this case, it's supposed to be zero.

372
00:44:17,230 --> 00:44:20,799
You classify that to one or you if you have one.

373
00:44:20,800 --> 00:44:26,670
And then you classify that to zero. You have a one loss.

374
00:44:27,710 --> 00:44:31,160
Right. So the one unit, two floors and then that's it.

375
00:44:31,340 --> 00:44:35,810
So this is a very simple loss. And on this, this is a commonly used the lost function.

376
00:44:37,000 --> 00:44:49,000
On that sort of note, you can think about hypothesis testing type of the problem is that it I don't my intention is not

377
00:44:49,000 --> 00:44:55,410
to try to confuse you if some of you are studying 602 they don't use this type of the language.

378
00:44:55,420 --> 00:45:01,059
But if you're doing hypothesis testing and then trying to classify if the data

379
00:45:01,060 --> 00:45:06,010
is generated from either the the null hypothesis or alternative hypothesis,

380
00:45:06,340 --> 00:45:14,170
you can think if that lost function is a01 loss or not answer it's is not because you are penalize.

381
00:45:15,470 --> 00:45:23,570
The two different misclassification. Very different. You pop you penalize false positive, more severe.

382
00:45:24,600 --> 00:45:27,870
And then the false negative. Right.

383
00:45:28,080 --> 00:45:37,140
So that's usually the practice. Just an example to say not only the loss function you see from the binary classification, it's zero one false.

384
00:45:39,040 --> 00:45:45,150
Um. All right. So now we have all the components here.

385
00:45:46,410 --> 00:45:53,020
How do we evaluate an algorithm? Now, we should know is we should know input and output.

386
00:45:53,040 --> 00:45:57,540
That's not a question. There's you always have those things well defined.

387
00:45:59,530 --> 00:46:08,019
To be able. So the goal is really to evaluate that you are learning algorithm for finding the optimal learning

388
00:46:08,020 --> 00:46:14,589
algorithm to be able to find the molecule one you should be able to evaluate if I gave you two outcomes,

389
00:46:14,590 --> 00:46:25,260
right? So those things are related and then you need to, to be able to evaluate this that you should be able to do that.

390
00:46:25,270 --> 00:46:30,430
What you need is actually you understand, you do not need to know them, all that sort of relationship.

391
00:46:30,880 --> 00:46:35,550
So that's the truth. I know you need to tell me how you.

392
00:46:39,610 --> 00:46:44,890
How do you evaluate the accuracy? So those are the components we have, right?

393
00:46:45,220 --> 00:46:49,150
You don't know everything. But to be able to find the optimal.

394
00:46:49,510 --> 00:46:52,960
That seems like what you need to do and then see.

395
00:46:53,140 --> 00:46:58,480
So these are the five components I see on how they work together.

396
00:47:01,940 --> 00:47:15,870
Any questions so far? So at this point, you can take a mindset like.

397
00:47:19,150 --> 00:47:26,180
You know. There is always a true relationship between acts of what I'm trying to say something about the model.

398
00:47:27,110 --> 00:47:36,230
There is a true relationship between X and Y and assuming you know that it's just a hypothetical setting.

399
00:47:41,820 --> 00:47:45,360
All right. So with all these components.

400
00:47:45,720 --> 00:47:49,480
So the question is, how do we find the optimal?

401
00:47:49,530 --> 00:47:55,580
How should we make decisions? I'm being.

402
00:48:02,190 --> 00:48:12,579
The opportunity. Equivalent.

403
00:48:12,580 --> 00:48:18,510
The question is, I gave you a function of being deep learning, being a SVM.

404
00:48:19,680 --> 00:48:26,590
How do you evaluate which one is better for what you were doing before your particular application?

405
00:48:28,080 --> 00:48:38,960
Um. So you need to understand that you're probably not going to be an algorithm that predicts everything accurately everywhere.

406
00:48:39,170 --> 00:48:49,420
Right. So if you want to. Well, obviously there is an absolute that's the point you want to achieve with that is the loss is zero.

407
00:48:49,430 --> 00:48:57,150
Every. Right. That's just impractical in the presence of uncertainty, in the presence of intelligence.

408
00:48:57,580 --> 00:49:02,230
So what's the second facet? So that's not a tenable let's don't think about it.

409
00:49:02,590 --> 00:49:07,690
What is the alternative? What is the logical way to find that out?

410
00:49:08,560 --> 00:49:13,340
I think this is pretty. Intuitive principle.

411
00:49:14,470 --> 00:49:18,130
So the principle here. You cannot spell.

412
00:49:20,360 --> 00:49:24,020
So let's minimize. So the minimizing in the.

413
00:49:29,440 --> 00:49:36,980
Wall. For some people, some narrow.

414
00:49:42,610 --> 00:49:53,840
Prediction or classification. In layman's terms.

415
00:49:54,140 --> 00:50:04,730
So this is basically trying to say I don't expect the algorithm to work perfectly every time, but on average.

416
00:50:05,750 --> 00:50:11,660
This particular algorithm should be more accurate than any other things.

417
00:50:12,290 --> 00:50:16,250
Right. So the keyword on that and then this.

418
00:50:18,100 --> 00:50:22,300
Gave rise to this notion of these practical laws.

419
00:50:22,930 --> 00:50:30,250
And then we have the notation that we have. We can actually write this so called expected a prediction narrow.

420
00:50:32,120 --> 00:50:37,480
ISO epi also known as a risk in is statistical.

421
00:50:41,370 --> 00:50:47,750
So this is defined as. Expecting the value of X and Y.

422
00:50:49,880 --> 00:50:55,410
And then if you take the loss. Why? And offer.

423
00:50:56,820 --> 00:51:00,649
All right. I'll explain in a minute. All right.

424
00:51:00,650 --> 00:51:07,790
So there is a lot of things going on here. So you need to have an algorithm.

425
00:51:07,800 --> 00:51:19,310
So this is a evaluation. So far this expression is the evaluation of the risk or the expected the predicted error of a certain learning algorithm.

426
00:51:19,380 --> 00:51:28,460
F. Here i. And then how do we evaluate the overall performance?

427
00:51:29,420 --> 00:51:37,520
So what we can't do is at least mathematically, we can evaluate all possible outcome.

428
00:51:37,520 --> 00:51:43,160
And number two. So this is the loss and then we compute expectation.

429
00:51:46,210 --> 00:51:53,860
Intuitively that sort of claims to averaging out the potential inaccuracy.

430
00:51:54,430 --> 00:52:05,440
So if we can make this this so called so the expected prediction error or low, then we say this algorithm is good.

431
00:52:06,500 --> 00:52:13,180
Right. That's a pretty simple principle to work on.

432
00:52:14,170 --> 00:52:22,060
The things to note here, although it's not explicit written, if you're trying to evaluate this expectation,

433
00:52:22,630 --> 00:52:28,500
the expectation is taken with respect to the joint distribution of X one.

434
00:52:28,510 --> 00:52:35,410
So you need to know the model without the mode of you cannot evaluate this.

435
00:52:36,610 --> 00:52:40,000
Right. So. So as we said, you need to know the actual.

436
00:52:41,350 --> 00:52:51,220
And then so far we do know that in practice, if you don't know that, can you still evaluate this sort of thing?

437
00:52:54,310 --> 00:52:57,550
So this is some statistics going to come to.

438
00:53:00,670 --> 00:53:09,780
To come to save you. So this expectation usually if you'll make further assumptions on how the the something that

439
00:53:09,790 --> 00:53:16,090
they that you come from you can approximate this type of expectation using sample averaging.

440
00:53:16,930 --> 00:53:21,440
Right. So that's in practice how we do it. But for now, let's assume just.

441
00:53:22,620 --> 00:53:30,120
And then in that case, you don't even know. You even need to know the the relationship between the the true distribution x,

442
00:53:30,120 --> 00:53:37,500
y rights of the the law of large number or the the central theorem can help you out there.

443
00:53:37,800 --> 00:53:43,380
But for now, let's assume you do know and this is the way how you evaluate the algorithm you get.

444
00:53:44,900 --> 00:53:54,620
And if you can evaluate the individual algorithm, then there is a very simple way to to find the general representation for the optimal.

445
00:54:04,710 --> 00:54:11,550
So the app had for these purposes probably just more it's a bit of notation.

446
00:54:12,240 --> 00:54:17,380
Our main. Uh, in.

447
00:54:20,670 --> 00:54:43,350
Expect. So this is trying to say, if you're familiar with our I mean this is just trying to minimize over oh possible f.

448
00:54:43,530 --> 00:54:48,990
So suppose you have a candidate of a family of candidates living there.

449
00:54:49,890 --> 00:54:53,490
Which one is optimal then for each of them.

450
00:54:53,670 --> 00:54:57,450
You evaluate this. An expected prediction.

451
00:54:57,450 --> 00:55:03,270
Narrow. Right. And then. Clock is not right.

452
00:55:04,410 --> 00:55:16,530
And then you find the one, the optimal one among this within this family is the one that achieved the smallest predicted.

453
00:55:17,660 --> 00:55:22,420
I expected a predicted prediction error. Right. So that's straight forward.

454
00:55:23,680 --> 00:55:29,559
So usually in this class, we we we have this notation as well.

455
00:55:29,560 --> 00:55:40,020
You should consider asking the family. Again you could potentially compare across the family is so that the family is just a set

456
00:55:40,020 --> 00:55:47,030
of learning algorithms so they're really no constraint on what's the family you should use.

457
00:55:47,030 --> 00:55:54,890
But usually we all we can see there are certain, I'd say deep learning algorithm trying to find the parameters.

458
00:55:55,760 --> 00:56:02,540
We, we constrain ourselves to the linear prediction function, then we try to find.

459
00:56:02,810 --> 00:56:08,010
So within the family you can do that across different families, you can still do that.

460
00:56:08,060 --> 00:56:16,400
So this is all valid. So the AFF is sort of just consider the candidates you have here.

461
00:56:24,200 --> 00:56:28,170
All right. So that's all you need to know. So this is a pretty straightforward.

462
00:56:28,670 --> 00:56:34,310
So if you know them all that we know that they will come to review our magnetism.

463
00:56:34,670 --> 00:56:38,659
And then you should be able to evaluate which algorithm is good.

464
00:56:38,660 --> 00:56:45,280
We chat with them. It's. Not good. Right. But the question here, we're going to turn that.

465
00:56:45,500 --> 00:56:48,800
So so this again, this is talking about the principle.

466
00:56:50,060 --> 00:56:53,820
The principle is out there, I don't think any questions.

467
00:56:53,840 --> 00:56:58,100
If you have a question here, we should discuss very carefully.

468
00:56:58,100 --> 00:57:02,440
Yes, please. Yes, we do at this point.

469
00:57:02,530 --> 00:57:07,990
If you know the truth, all of this is how we we will. Yeah, that's a that's a very good question.

470
00:57:08,350 --> 00:57:11,730
And now we need to see how we can get around that.

471
00:57:21,350 --> 00:57:25,060
Okay. So the principle is clear.

472
00:57:27,610 --> 00:57:32,530
Now we get to the practice. How do we do that?

473
00:57:33,350 --> 00:57:50,010
How do we apply? So to connect this system to the supervisor are examples.

474
00:57:50,150 --> 00:58:01,180
So you have labels. One of the things from the mathematical statistics on what you should know is the total expectation.

475
00:58:01,210 --> 00:58:04,740
All right. So if you're trying to calculate.

476
00:58:07,330 --> 00:58:10,840
Expected value based on the drawing distribution x.

477
00:58:10,840 --> 00:58:14,170
Y. L y.

478
00:58:15,400 --> 00:58:24,040
I fell for x. This one equals the following.

479
00:58:25,410 --> 00:58:30,220
So you could actually do it for eight. So this is saying to do what?

480
00:58:30,390 --> 00:58:38,320
Expectations. All right. So this is just integrate integrate over the joint distribution of X Y.

481
00:58:38,340 --> 00:58:43,980
So if you think about X and Y are two dimensions, this is a two dimensional integral.

482
00:58:44,370 --> 00:58:51,570
But you could do that iteratively based on the so called the total expectation lol.

483
00:58:51,840 --> 00:58:56,860
So what you can do is you can do. This is Max.

484
00:58:57,850 --> 00:59:09,169
And then you first do y given. So there in our expectation you do two expectation.

485
00:59:09,170 --> 00:59:14,310
First, the expectation is which is you conditional, right?

486
00:59:14,340 --> 00:59:23,260
Consider the x axis and then just integrate y and then the outer expectation you integrate all x.

487
00:59:23,270 --> 00:59:27,260
So this is a total expectation. You should see that in 601.

488
00:59:36,690 --> 00:59:42,440
Given. Right.

489
00:59:42,720 --> 00:59:48,620
It doesn't it doesn't change the whole thing. It's just a woman's representation.

490
00:59:50,450 --> 01:00:05,410
So the question of finding the right. Finding the optimal algorithm requires you to minimize or expected loss before the left hand side.

491
01:00:05,650 --> 01:00:10,900
Right. So you can equivalently doing this for the right hand side.

492
01:00:12,070 --> 01:00:19,660
Right. So the left hand side equals the right hand side. So what the significance of that is, you could actually just.

493
01:00:20,640 --> 01:00:24,510
Minimize this in the expectation.

494
01:00:25,750 --> 01:00:29,530
For arbitrary acts, then you'll find the relationship.

495
01:00:29,860 --> 01:00:40,200
Right. So. Because if for every individual acts, for arbitrary acts, for in their expectation,

496
01:00:40,210 --> 01:00:47,440
there's a conditional expectation which is a regression function is a minimize, then it's minimize to everywhere.

497
01:00:47,520 --> 01:00:53,219
So this is a basically what we did for regression for supervised learning.

498
01:00:53,220 --> 01:00:58,710
How we handle the learning data is guided by this principle again.

499
01:00:58,720 --> 01:01:01,870
So let me just write what I meant by that.

500
01:01:05,640 --> 01:01:18,120
So. Okay. So the point is, if you want to minimize the left hand side, you can minimize four arbitrary acts that are one right.

501
01:01:18,570 --> 01:01:21,720
Because this is a in their function is a function of x.

502
01:01:21,780 --> 01:01:29,660
If you for arbitrary X, you'll minimize the expectation that it's minimize the for overall x.

503
01:01:29,820 --> 01:01:35,610
So this is known as the Bayesian decision theory because it's always conditional on X.

504
01:01:36,000 --> 01:01:45,330
You make your learning function decision depends on the observed the data you have, the training data that you have at hand to fit it.

505
01:01:45,720 --> 01:01:49,790
Not potentially you don't see the data you don't see.

506
01:01:49,800 --> 01:01:55,230
So this is known as Bayesian decision. This is also known as the regression function.

507
01:01:56,270 --> 01:02:00,660
Okay. So to minimize this. We can.

508
01:02:01,560 --> 01:02:10,210
Instead of just minimize. The inner life you live in acts.

509
01:02:13,650 --> 01:02:17,760
Now why have facts?

510
01:02:19,150 --> 01:02:23,580
Given access. Four arbitrary acts.

511
01:02:26,090 --> 01:02:30,740
Right. So minimize this. It's a little bit simple because.

512
01:02:31,040 --> 01:02:36,020
Why? Because this is sort of a univariate distribution.

513
01:02:36,050 --> 01:02:40,010
We suspect the expectation is taken with respect. Why?

514
01:02:40,010 --> 01:02:44,490
Given Max. So that's what it means.

515
01:02:44,740 --> 01:02:52,140
Why? So you don't need to know the distribution of acts, but rather focus on the distribution of Y given x.

516
01:02:52,180 --> 01:03:00,100
So it's focus on Y instead of y. That's what we have been doing for regression problem, linear regression, logistic regression.

517
01:03:00,520 --> 01:03:03,980
We have always been specifying modal up to this point, right?

518
01:03:05,730 --> 01:03:11,459
It's the same thing. If you have a candidate, algorithm have a different.

519
01:03:11,460 --> 01:03:17,190
Algorithm different. Prediction algorithm, classification algorithm.

520
01:03:18,120 --> 01:03:23,760
You can evaluate them by just conditional on the X based on your training sample.

521
01:03:24,420 --> 01:03:36,239
Okay. So this is the point here. So you can minimize instead of the whole thing with the model specify the y axis inside

522
01:03:36,240 --> 01:03:41,970
of you is sufficient to specify the mode of just using the conditional distribution.

523
01:03:42,660 --> 01:03:51,320
And that right here, we can just minimize this. If you don't know this, that's fine.

524
01:03:52,010 --> 01:03:56,419
Just take my words for granted. I think of this a little bit theory.

525
01:03:56,420 --> 01:04:00,970
Just help you to give you a sense of confidence.

526
01:04:00,980 --> 01:04:06,450
Well, we did what we're going to do or what we have done is it's valid.

527
01:04:07,180 --> 01:04:14,240
Right. So if you put a specific functional form of loss function.

528
01:04:15,500 --> 01:04:19,670
So this gave you or even simplified solution.

529
01:04:20,360 --> 01:04:25,510
So if you lost. If the ALP is.

530
01:04:26,430 --> 01:04:42,860
Square walls. So minimize this is equivalent to say the act is essentially a conditional expectation.

531
01:04:42,870 --> 01:04:45,980
Why? Right.

532
01:04:46,030 --> 01:04:50,590
So you could try this at home yourself.

533
01:04:50,800 --> 01:04:54,730
If you're having trouble, you come to my office. I'll work. I'll show you how this is done.

534
01:04:55,210 --> 01:04:59,380
So. So you need to realize this is. This is a function.

535
01:04:59,650 --> 01:05:07,630
So the absence of X, I'd say to you that this equals Y computing.

536
01:05:10,560 --> 01:05:14,070
Right. So this is a function, this is a valid function.

537
01:05:14,490 --> 01:05:20,190
And then this is still it's just gave you a simpler, simplified form.

538
01:05:20,640 --> 01:05:25,830
So it doesn't really tell you what the expectation, what this function is actually does.

539
01:05:26,310 --> 01:05:31,260
But this is very informative in the way how you approach.

540
01:05:32,190 --> 01:05:37,620
All. So the app had supposed to be y given.

541
01:05:37,630 --> 01:05:42,250
So first of all understand that this conditional expectation as a function.

542
01:05:43,080 --> 01:05:50,380
It's actually a wonderful function. Is there a way to write these things is equivalent to write from here.

543
01:05:50,770 --> 01:05:58,630
Okay. Secondly, this is a basically saying if you have a loss function, then, oh, sorry.

544
01:05:58,670 --> 01:06:03,550
Then if you have the lost function being the square loss, then at the end of the day,

545
01:06:03,970 --> 01:06:09,790
what you can do to satisfy the statistical position theory you only need to find.

546
01:06:10,830 --> 01:06:15,000
The conditional expectation likely will. Okay.

547
01:06:15,810 --> 01:06:20,010
So this actually informed two branches of approach.

548
01:06:21,330 --> 01:06:26,040
So long for the ultimate learning algorithm.

549
01:06:26,700 --> 01:06:31,890
One is being probabilistic. The other is being completely function.

550
01:06:32,280 --> 01:06:41,360
So that's how the. So the whole theory that you inform us what is the position, we cannot do the job later on.

551
01:06:41,510 --> 01:06:50,820
General Fine and I were. Again if you're confused by this statement.

552
01:06:53,330 --> 01:07:02,790
Like me. No, I'm trying. So. If you've been doing six a week, we show this is essentially the.

553
01:07:04,940 --> 01:07:12,920
This is a this is a conclusion, but it's also the way we define the modern way to define conditional expectation.

554
01:07:15,530 --> 01:07:20,420
All right. Never mind. Right.

555
01:07:20,490 --> 01:07:30,450
Let's take this for. So how this is relevant to our practice of washing our.

556
01:07:32,170 --> 01:07:39,910
At the end of the day, I think what we accept that is the general principle is how we define the model.

557
01:07:40,420 --> 01:07:48,309
Right? So let's constrain ourself in the in the in a rather simple scenario,

558
01:07:48,310 --> 01:07:54,940
we have a loss function being defined as a square loss or dealing with a prediction and then that,

559
01:07:54,950 --> 01:07:59,950
you know, the optimal function is expected by the Y, given X.

560
01:08:02,130 --> 01:08:09,010
That's a function of outside the acts. Right.

561
01:08:09,030 --> 01:08:17,330
So that's the decision. That's universal. True. The issue, though, is just like the previous question.

562
01:08:18,260 --> 01:08:21,300
While you find optimal hunger, if you truly know that.

563
01:08:23,350 --> 01:08:28,810
We don't know the Volvo, so we don't really know. We cannot pinpoint exactly expected.

564
01:08:29,110 --> 01:08:36,350
Why give an ax? If you know the multiple means, you know the probability, why give an ax then?

565
01:08:36,370 --> 01:08:40,480
This is a done. This is a byproduct. Yes.

566
01:08:40,490 --> 01:08:45,620
If you know, the the the distribution. Now you can calculate the expectation.

567
01:08:45,950 --> 01:08:49,880
Simple. That's not what I meant. So this is the way.

568
01:08:50,480 --> 01:08:57,860
How probabilistic. So this is the first way to find algorithm is called a problematic.

569
01:08:59,630 --> 01:09:15,400
All. Supervise.

570
01:09:21,120 --> 01:09:29,160
Okay. All right. So this is very much similar than what you have seen in statistic courses.

571
01:09:29,490 --> 01:09:32,790
So we are going to make an assumption of the data.

572
01:09:32,790 --> 01:09:40,080
Generally, these are the mechanism. We are going to say of the data why given that following normal.

573
01:09:40,170 --> 01:09:44,310
So that's a linear model. Right. So in this case.

574
01:09:47,070 --> 01:09:52,720
Spicy by. The family, given that.

575
01:09:54,520 --> 01:09:58,030
Right. So if you do that, they gave you.

576
01:09:59,940 --> 01:10:04,140
Optimal learning algorithm. You don't need to look for them anymore.

577
01:10:04,290 --> 01:10:09,330
So if you make the distributional assumptions, then the optimal algorithm is all there.

578
01:10:09,690 --> 01:10:12,990
You don't need to. So this gives out.

579
01:10:23,370 --> 01:10:33,290
If you assume the model is a linear model, then if your loss function is square walls, then you don't need to do any fancy machine learning.

580
01:10:33,300 --> 01:10:37,050
So the the expected function is is your optimal.

581
01:10:38,280 --> 01:10:42,220
Apparently this is not the machine learning Carling is doing, right.

582
01:10:42,240 --> 01:10:46,920
So that means they are not in this school of probabilistic machine learning.

583
01:10:47,610 --> 01:10:55,140
But nevertheless, I think what we have already known is already a value of the machine learning approach, right?

584
01:10:55,530 --> 01:11:02,490
To start with, from the data, from the review mechanism, trying to specify a model and then trying to get things done.

585
01:11:03,850 --> 01:11:10,150
The second school of doing supervised learning is known as the functional.

586
01:11:13,740 --> 01:11:17,130
Functional. Supervise.

587
01:11:20,330 --> 01:11:25,670
And then that supposed to be the four things that we focused mostly on in this pause,

588
01:11:26,180 --> 01:11:31,520
because these are not halt in the we we're not going to start from a specify a

589
01:11:31,520 --> 01:11:37,130
problem in most cases not going to start by specifying a problematic model.

590
01:11:37,520 --> 01:11:41,300
We're going to take this alternative approach in this course.

591
01:11:42,620 --> 01:11:47,900
What is the course? So sorry. What is the term of the strategy?

592
01:11:48,380 --> 01:11:53,360
Well, knowing this is being a expectation, maybe not really relevant.

593
01:11:53,840 --> 01:11:58,850
Right. So as we said before, the expectations.

594
01:12:00,670 --> 01:12:07,330
That's complicated thing. Usually we can get away by approximating.

595
01:12:09,000 --> 01:12:14,100
This quantity by using the sample averaging by using the law of large number.

596
01:12:14,400 --> 01:12:30,660
So is the way we can circumvent circumvent the the specification of the data generated model but rather focus on funding this the ultimate access.

597
01:12:32,730 --> 01:12:38,820
Right. So how do you find this, though, if you define the loss function?

598
01:12:43,260 --> 01:12:48,680
Than they expect. Can you actually see how much trouble, Stevens?

599
01:12:50,390 --> 01:12:53,780
Read this. My eyes. So that was why.

600
01:12:56,080 --> 01:13:08,790
Given have full access. So you can approximate this by the sample average.

601
01:13:11,910 --> 01:13:18,840
Um. Right some.

602
01:13:18,840 --> 01:13:22,410
Also this way there is a turning point.

603
01:13:22,740 --> 01:13:29,610
So this is not always true expectation. The approximate value, the sum of everything is not always true.

604
01:13:29,910 --> 01:13:38,880
You do make some assumptions like only repeated points are while independent, you probably not necessarily make that strong assumption.

605
01:13:39,540 --> 01:13:50,640
You know, all of the cases we have law of large numbers or correlated, some even correlate in some polls there are still some and both can be valid.

606
01:13:51,180 --> 01:14:02,470
So. So this is a way to. To be able to do that, make you to focus on finding the optimal path rather than focus on the.

607
01:14:04,780 --> 01:14:10,220
The distribution, finding the distribution, if you can evaluate you.

608
01:14:10,630 --> 01:14:16,900
I'm not saying this is exactly the way we're going to talk about cross-validation or overall all of those things.

609
01:14:17,320 --> 01:14:21,700
But this is the fundamental recipe to get away from.

610
01:14:24,670 --> 01:14:28,180
You know, the requirement of a mall alleyway at this point.

611
01:14:29,050 --> 01:14:35,110
Yeah. So we could get away by using simple averaging as a vehicle.

612
01:14:35,560 --> 01:14:39,980
And then we can focus on finding. Different ask.

613
01:14:40,550 --> 01:14:45,080
So why you talk about F? So the probability has constraints.

614
01:14:45,110 --> 01:14:50,750
You have to be, you know, those satisfy all kinds of constraints, probability holes.

615
01:14:50,760 --> 01:14:55,370
That has to be one. And then the probability of an empty set has to be zero.

616
01:14:55,370 --> 01:15:00,140
Those constraint doesn't apply for expected values.

617
01:15:00,440 --> 01:15:04,070
Those are just function. So usually it's unconstrained.

618
01:15:04,340 --> 01:15:07,970
So you can think about any family of functions, right?

619
01:15:08,300 --> 01:15:13,550
So that actually defines different learning algorithms like deep learning algorithms.

620
01:15:13,550 --> 01:15:18,380
That's a classical function you want to consider. There's a lot of interaction terms.

621
01:15:19,340 --> 01:15:23,270
Linear function is probably the simplest one you can start with.

622
01:15:23,270 --> 01:15:28,160
They are all additive and so that's a different viewpoint.

623
01:15:28,700 --> 01:15:33,380
So instead of specifying why, given the focus on.

624
01:15:36,920 --> 01:15:44,910
Funding. Optimal habitat using bye bye assuming.

625
01:15:50,420 --> 01:15:58,080
Some sort of a low off large number. Some. So people usually get away with this.

626
01:16:00,170 --> 01:16:12,980
Sorry. If you cannot see. But this presents a completely different viewpoint on Capitol Hill where.

627
01:16:13,850 --> 01:16:22,220
Problem on our hands. Right. So now we've got to focus on fighting this random function instead of the probability distribution.

628
01:16:22,760 --> 01:16:28,730
And and then you can evaluate for that particular function based on wall number.

629
01:16:29,270 --> 01:16:33,290
So this is a basically represent two different schools.

630
01:16:34,310 --> 01:16:40,640
In terms of finding the optimal finding the machine learning algorithm.

631
01:16:42,020 --> 01:16:46,290
And then they are always got intertwined people.

632
01:16:46,310 --> 01:16:54,740
Some people might be thinking they're doing statistical learning, but usually actually in this and people doing.

633
01:16:56,240 --> 01:17:02,660
You know, deep learning that definitely in this school because the function they start with is the fun.

634
01:17:02,870 --> 01:17:10,270
So you will get to see all kinds of the functions people use to finding the optimal function, right?

635
01:17:10,280 --> 01:17:15,080
So the deep learning function, for example, is highly nonlinear functions.

636
01:17:17,240 --> 01:17:22,070
Any questions. So this is two philosophy, two schools, two ways to do things.

637
01:17:23,300 --> 01:17:27,560
We're going to do one little bit of comparison today. So it's 2 minutes left.

638
01:17:31,200 --> 01:17:34,820
So specify the probability.

639
01:17:34,890 --> 01:17:43,960
Why? Given that it's more than just. Spicy specify as I said, why the.

640
01:17:44,350 --> 01:17:48,170
If you specify the model but these are generally multiple.

641
01:17:49,400 --> 01:17:52,520
The learning algorithm is a byproduct, right?

642
01:17:52,940 --> 01:17:57,080
By specify the algorithm to use, specify the model.

643
01:17:59,990 --> 01:18:09,400
Usually not. There are so many different. Probability distributions can give you give rise to the same expected function.

644
01:18:10,000 --> 01:18:19,090
Y given that. So specify finding the optimum, although it doesn't really tell you a lot about the underlying data generated out.

645
01:18:19,880 --> 01:18:24,050
Right. So this is all again, to our initial discussion.

646
01:18:24,050 --> 01:18:33,780
Let me give you some insight on all the data generated, but it's not completely interpretable in the sense, you know, if you want,

647
01:18:33,810 --> 01:18:41,900
you get something from your prediction algorithm and then you're trying to get back to say this is how data generated may not be valid.

648
01:18:42,530 --> 01:18:54,980
However, the let's call this the now the probability machine learning versus F.A., which is the function of machine learning.

649
01:18:55,850 --> 01:19:04,340
So the PMO is very restrictive in the sense of the probability distribution you have.

650
01:19:06,050 --> 01:19:09,320
You cannot handle this, you limited. Right.

651
01:19:09,410 --> 01:19:13,430
So you have done 601. That's a prerequisite.

652
01:19:13,440 --> 01:19:17,280
So how many distributions in of distribution?

653
01:19:17,820 --> 01:19:24,270
Well, I think most of the distributions you learn because they can easily underlie the calculation.

654
01:19:24,630 --> 01:19:29,220
So there are some really, truly bizarre hard to compute.

655
01:19:29,250 --> 01:19:35,760
The distributions are probably not in our. There's not a lot of things you can say about this type of.

656
01:19:37,060 --> 01:19:45,580
So they are restricted in that sense. But the benefit of this is that if you specify the distributions, the usually documentations are all.

657
01:19:47,000 --> 01:19:52,730
The FBI now on the other side is very gentle.

658
01:19:53,240 --> 01:19:57,380
As I said, it actually doesn't constrain it doesn't constrain the underlying.

