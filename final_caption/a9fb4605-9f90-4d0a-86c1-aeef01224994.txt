1
00:00:01,500 --> 00:00:31,970
I thought, you know, I would just say, yeah, yeah, yeah, yeah, yeah, yeah.

2
00:00:31,980 --> 00:00:39,450
Like, I know one of those things I was like, I can't refuse.

3
00:00:40,350 --> 00:00:43,980
Yeah, yeah, yeah.

4
00:00:45,670 --> 00:00:56,490
So I replied, like somebody like a messenger and everything like that.

5
00:00:56,880 --> 00:01:00,120
It's good. It's a good dog. It's okay.

6
00:01:00,450 --> 00:01:03,500
It's okay. You're fine.

7
00:01:03,560 --> 00:01:10,680
I was in the throes of feeling like, I think I'm not crazy.

8
00:01:10,980 --> 00:01:20,550
That's when I started to say, I know, I know.

9
00:01:21,960 --> 00:01:45,000
But then I realized that some places it was a little bit different, you know, stuff like that.

10
00:01:48,570 --> 00:01:55,800
Because, you know, I'm like, I don't know.

11
00:02:06,630 --> 00:02:12,840
I don't think I could have definitely recognized something like that.

12
00:02:12,900 --> 00:02:31,810
More people like me like, really like it was good on the global stage, right?

13
00:02:31,850 --> 00:02:36,360
I like to be very shortly going back.

14
00:02:36,360 --> 00:02:47,070
Everybody can see that I learned the results assignments over the weekend.

15
00:02:47,810 --> 00:02:54,000
It's a long time since the last people would have been in trouble.

16
00:02:54,000 --> 00:02:58,530
If you struggle to stay, if you're confused by things, please do reach out to me.

17
00:02:58,530 --> 00:03:08,220
I really would like to touch base with you. I try to get a lot of comments, so you should have some very specific feedback that we think is unclear.

18
00:03:08,520 --> 00:03:12,420
Please reach out to me electronically or names, etc.

19
00:03:13,410 --> 00:03:25,200
Some of the common things that I saw, probably the most common thing was not paying enough attention to what the scale of the graphic means.

20
00:03:27,120 --> 00:03:34,590
So yeah, if you're working with CD4 count, the action is in the 200 to 500 range.

21
00:03:34,920 --> 00:03:40,200
But it matters whether people know that normal goes up to 1500 changes.

22
00:03:40,200 --> 00:03:43,890
The way in which you're going to react to a change of 100 or 200 or something like that.

23
00:03:44,370 --> 00:03:48,329
So showing that range is actually important to calibrate people to get.

24
00:03:48,330 --> 00:03:55,710
You're already relatively low compared to other things beyond that.

25
00:03:56,780 --> 00:04:03,780
A lot of small stuff. But I mean the challenge with these types of situations is really wrestling

26
00:04:03,780 --> 00:04:07,590
with choices and not everybody made the same choices and that's totally fine.

27
00:04:08,670 --> 00:04:12,840
Some people focused on certain standards, other people focused on other standards, and that's good.

28
00:04:13,740 --> 00:04:18,240
And you can make those choices very consciously when you're doing this in practice.

29
00:04:19,750 --> 00:04:29,110
I want to be focusing on what you know, to use the example on the reference point for what health water systems are designed to,

30
00:04:29,280 --> 00:04:34,590
you know, are what standard they have to meet. You don't want to focus on the standard for what bottled water has to meet.

31
00:04:35,070 --> 00:04:36,840
What do I want to focus on the standard of?

32
00:04:38,190 --> 00:04:45,870
Yeah, we don't actually want any of this and that tension between those different standards is part of what was the point, the purpose of so anyway.

33
00:04:47,000 --> 00:04:48,020
Take a look at the comments.

34
00:04:48,080 --> 00:04:57,140
If you haven't already, you love to have individual conversations with you, if you have any questions about where we're going.

35
00:04:58,500 --> 00:05:02,100
So we're here this week. Next week is fall break.

36
00:05:02,100 --> 00:05:10,410
So there's no class a week from today because of fall break. We've got one class on Thursday and then my other trip is the following Tuesday.

37
00:05:10,650 --> 00:05:19,709
So two weeks from today, there is no class of you're having this sort of burst of mental model stuff.

38
00:05:19,710 --> 00:05:24,150
So we're going to spend a lot of time today not sort of introducing the mental models idea,

39
00:05:24,150 --> 00:05:29,190
but showing you concrete examples in which the problem clearly is a mental model problem.

40
00:05:30,870 --> 00:05:34,980
And I gave you a bunch of examples in the readings you gave me more in amusing.

41
00:05:35,940 --> 00:05:37,679
This is going to keep coming back up,

42
00:05:37,680 --> 00:05:45,560
but I'm not going to dwell on it too long because of the focus of the mental models approach is really just understanding the conceptual,

43
00:05:45,570 --> 00:05:52,230
conceptual differences between what an expert knows, what the audience knows, zooming in on, what that difference is.

44
00:05:53,670 --> 00:05:58,500
And so what we're going to wrap up today is the question of the day, which is already up there, which is okay.

45
00:05:59,490 --> 00:06:05,160
Is it okay for us to be given communications that are focused and not complete?

46
00:06:06,780 --> 00:06:12,810
How comfortable are we with a communications design that doesn't have everything,

47
00:06:13,020 --> 00:06:18,450
that only has the thing that we think is the critical misconception or misunderstanding?

48
00:06:18,450 --> 00:06:21,470
And somebody is bent over? When is that going to be acceptable?

49
00:06:21,500 --> 00:06:27,950
When is it not? In terms of next assignments.

50
00:06:29,720 --> 00:06:32,510
After we're done with mental models today,

51
00:06:32,510 --> 00:06:38,780
we're going to go back to thinking about what medical decision making context for a little bit into your next assignment.

52
00:06:38,780 --> 00:06:40,310
Again, is this decision A,

53
00:06:40,940 --> 00:06:51,560
you've got a choice that somebody has to make between two different options that have risks associated with benefits associated with them.

54
00:06:51,770 --> 00:06:56,630
I'm going to be making basically a summary table which shows multiple risks and benefits together.

55
00:06:57,590 --> 00:07:01,489
We'll go through lots of examples. We'll talk about this. But again,

56
00:07:01,490 --> 00:07:08,510
I want you thinking about what's the decision that you might want to explore with the kind of thing

57
00:07:08,510 --> 00:07:12,620
that you're particularly interested in because you get to pick a topic on this one within constraints.

58
00:07:14,120 --> 00:07:24,440
And if you need if you're wondering about examples. So some classic examples in the in the medical literature, things like cancer treatment decisions,

59
00:07:25,520 --> 00:07:32,750
breast conserving surgery versus mastectomy and breast cancer surgery versus radiation and prostate cancer.

60
00:07:34,730 --> 00:07:38,130
Why is my phone? Yeah.

61
00:07:38,280 --> 00:07:49,780
Yeah. Surgery versus physical therapy for like a knee replacement or knee injury type things or hips.

62
00:07:52,960 --> 00:08:02,490
You got. You know, birth control choices and all of the many different complicating options there, etc.

63
00:08:03,630 --> 00:08:12,660
But I am wide open. So if there's a particular type of decision, medical or environmental, that has that kind of risk tradeoff that you'd like to do,

64
00:08:13,170 --> 00:08:18,120
just grab me for a second and suggest it, and I can quickly give you some feedback as to whether that's a good idea.

65
00:08:19,650 --> 00:08:25,050
To the extent that I will be shaping what you pick, it will be to make your job easier.

66
00:08:26,420 --> 00:08:29,510
This can get ugly and complicated facts.

67
00:08:30,020 --> 00:08:37,960
So for example. I don't even want to guess how many birth control options are out there at this point.

68
00:08:39,680 --> 00:08:42,590
I'm going to have you zoom in on two.

69
00:08:45,160 --> 00:08:51,670
Because trying to do this conversation around many options at once is way more complicated than trying to do an A versus B choice.

70
00:08:52,450 --> 00:08:57,000
So we're going to make this simple. But you can choose what it is you're interested in.

71
00:08:57,930 --> 00:09:01,980
So anyway, this is coming down the pike. It's not coming up for another three or four weeks.

72
00:09:02,520 --> 00:09:06,900
I just want to tell Tony where we're going today.

73
00:09:07,260 --> 00:09:10,260
Let's talk about mental models and.

74
00:09:11,730 --> 00:09:13,800
If many different things are going to come up today,

75
00:09:14,280 --> 00:09:19,650
including at least one point into day in which if I do it right, somebody in this room is going to gasp.

76
00:09:21,200 --> 00:09:28,290
So we'll see if this happens. But let's start with what we're talking about.

77
00:09:28,620 --> 00:09:31,740
I'm not sure I've completely followed the article of your references,

78
00:09:31,750 --> 00:09:37,620
but it seemed like an interesting idea about the people, not the models, but the environmental influences.

79
00:09:38,300 --> 00:09:47,129
You know, the article is talking about that's one type of condition that Ethiopia, which is basically just their exposure to like red clay soil,

80
00:09:47,130 --> 00:09:55,830
like because they tend to walk barefoot and how they don't continue to use rabbit like interventions or strategies

81
00:09:55,830 --> 00:10:02,430
like wearing shoes or just like make sure you have like thorough washing of your feet after working because they

82
00:10:02,430 --> 00:10:09,270
believe that it's like the genetics of it is very deterministic and that they have the susceptibility that's being

83
00:10:09,270 --> 00:10:15,120
passed down through their families because they're seeing their family members also developing this condition.

84
00:10:15,120 --> 00:10:20,190
So they're like, Oh, well, my sibling had it then. Like, I don't need to wash my feet because it's just going to happen anyway.

85
00:10:21,030 --> 00:10:24,100
So at the core of this is a fatalism.

86
00:10:25,620 --> 00:10:33,180
It doesn't matter what I do if I believe that this condition is caused by genetics,

87
00:10:33,360 --> 00:10:36,659
something that I cannot change, then why should I modify my behavior?

88
00:10:36,660 --> 00:10:43,630
Because it won't have any impact. I think of other examples where that kind of genetic fatalism pops up.

89
00:10:48,480 --> 00:10:56,140
I certainly have heard in the context of diabetes. So now you're out.

90
00:10:56,160 --> 00:10:59,190
You know, my family has diabetes. Obviously, that's going to happen to me.

91
00:11:00,000 --> 00:11:05,820
And that undermines motivations to change behavior change I had.

92
00:11:08,560 --> 00:11:14,830
That's it. That's that's a nice, focused example of a problem that pops up in a lot of different situations.

93
00:11:15,280 --> 00:11:20,110
If you're a mental model, attribute something to a factor that you don't have control over.

94
00:11:21,400 --> 00:11:25,690
That really is a fundamental shift in terms of whether or not you think you can change anything.

95
00:11:26,920 --> 00:11:36,670
So here's an example that I actually have a mentoring somebody in the Depression Center right now.

96
00:11:39,010 --> 00:11:45,959
What a model of oppression. What causes depression?

97
00:11:45,960 --> 00:11:47,630
But does depression mean?

98
00:11:49,420 --> 00:11:59,690
And their research is basically saying, in contrast to mental models, a model of what this might call sort of the bio psycho model.

99
00:11:59,980 --> 00:12:03,640
Your biological factors or genetic factors are chemical factors.

100
00:12:04,360 --> 00:12:10,589
And depression is caused by these. Factors which are specific to the individual.

101
00:12:10,590 --> 00:12:15,650
Like if you have brain chemistry imbalance, then you are likely to develop depression.

102
00:12:17,440 --> 00:12:21,700
Versus a mental model that says depression is a signal.

103
00:12:23,320 --> 00:12:26,890
Depression is a signal that something is wrong in your life and you need to figure out what that is.

104
00:12:30,920 --> 00:12:35,600
If you think it's the first one, what do you think your belief is going to be about?

105
00:12:37,150 --> 00:12:40,000
If you start medications, how long you're going to need to take them.

106
00:12:42,130 --> 00:12:49,900
If the chemical imbalance is you, that would lead you to believe that that's not going to go away by itself.

107
00:12:50,560 --> 00:12:55,180
And so the mental model may lead you to believe that you're going to need to take medications for the rest of your life.

108
00:12:55,930 --> 00:13:01,479
If you frame this around depression as a signal and you can change whatever the thing is that you think is causing,

109
00:13:01,480 --> 00:13:07,150
you know, that that is signaling about. Then when that's done, you're done.

110
00:13:08,000 --> 00:13:11,319
So you have the possibility. And this is what he's exploring.

111
00:13:11,320 --> 00:13:15,639
I don't want to say we know this, but the possibility that the framing we break,

112
00:13:15,640 --> 00:13:22,360
the moral model we bring to what is a depression, can change people's motivations to engage with therapy,

113
00:13:22,360 --> 00:13:24,550
their beliefs about how long they're going to have to be on therapy,

114
00:13:25,030 --> 00:13:34,299
whether they want medication therapy versus cognitive therapy is those reactions to treatments will be modified by what they believe.

115
00:13:34,300 --> 00:13:38,380
The thing is much like what his example is talking about.

116
00:13:40,780 --> 00:13:42,579
That's one thing I wanted to talk about.

117
00:13:42,580 --> 00:13:50,709
Make sure we got to today how our mental conceptions of what is the disease or this condition and what causes it and shape our behaviors.

118
00:13:50,710 --> 00:13:55,260
And we'll come back to that next.

119
00:13:55,270 --> 00:13:58,509
A number of you. I mean, obviously, I gave you this term in Article three,

120
00:13:58,510 --> 00:14:03,310
but a number of you commented on it and expanded upon it in terms of context of pharmacology.

121
00:14:03,820 --> 00:14:16,809
So let's talk about that one. And just to get started, you raised one quick point, which I think is the good place to start this, which is I remember.

122
00:14:16,810 --> 00:14:23,260
Right, look, we have temperature as in how hot or cold is it outside today?

123
00:14:23,770 --> 00:14:29,380
And we have global temperature as a measure of global climate.

124
00:14:30,010 --> 00:14:42,860
And those things are not the same, for example, in 19 to you.

125
00:14:42,910 --> 00:14:55,180
But I think it's also useful to help you understand change like weather and climates not being cold just yeah.

126
00:14:55,570 --> 00:15:02,250
And of people doesn't know that right um if we change degrees where to doesn't

127
00:15:03,210 --> 00:15:09,280
like doesn't change and it doesn't have a feel it's it's not a big deal.

128
00:15:09,730 --> 00:15:22,150
So, so like, so people's mental model like understanding climate change, we probably need to clarify the definition of this.

129
00:15:22,660 --> 00:15:26,680
So this is a combination mental model and the availability of things.

130
00:15:26,680 --> 00:15:33,070
So we now talk about both of these. So part of the problem is the mental model, one I think to bring up.

131
00:15:34,600 --> 00:15:44,830
The what is your mental model of the relationship between how warm or cold it is outside versus global temperature and climate?

132
00:15:45,700 --> 00:15:55,530
If you. If your model says if the world is warming, then it can't be cold.

133
00:15:57,650 --> 00:16:02,840
You get thrust with very specific concrete examples of that being wrong.

134
00:16:03,470 --> 00:16:11,210
So you break that link. So, okay, we have a polar vortex here in Michigan and it's freezing like really cold.

135
00:16:12,230 --> 00:16:18,740
That must mean that global warming doesn't exist. Kind of like because your mental models said, hey, you should.

136
00:16:18,740 --> 00:16:21,770
Crosby And then you're observing the opposite of that.

137
00:16:27,020 --> 00:16:37,400
So and then there's the availability piece in the sense of and this right is it's 48 degrees outside or is it 50 degrees outside?

138
00:16:37,610 --> 00:16:44,450
Honestly, I don't care. Those two things do not matter in terms of whether or not I'm wearing a jacket today.

139
00:16:46,030 --> 00:16:56,980
But a equivalent difference in global average temperature is an enormous change.

140
00:16:58,950 --> 00:17:01,980
The climate. That would be intolerable.

141
00:17:03,020 --> 00:17:06,830
So we have the same measurement degrees centigrade,

142
00:17:07,760 --> 00:17:14,659
very different implications because it's only variability from the range of variability of temperature outside

143
00:17:14,660 --> 00:17:22,130
in Ann Arbor and the range of variability of global average temperature over millennia are radically different.

144
00:17:22,980 --> 00:17:31,160
So that's the availability side of this problem. But there's a couple of different ways in which this pops up.

145
00:17:32,180 --> 00:17:35,390
So I remember everything that we talked about.

146
00:17:37,380 --> 00:17:40,370
You know, both Mark and Sidney, you were talking about this.

147
00:17:41,270 --> 00:17:46,040
Is there anything that that's come up that I haven't talked about yet or that you want to expand upon?

148
00:17:47,180 --> 00:17:52,819
Oh, I did talk about the concept of like the disconnect between like current,

149
00:17:52,820 --> 00:17:56,990
whether someone's experiencing it for like not understanding how that relates to climate change.

150
00:17:56,990 --> 00:18:03,310
Also talked about like some conversations I had with my friends, just like how in terms of like, you know,

151
00:18:03,560 --> 00:18:08,930
it has gas emissions that it really doesn't matter what the individual does because there are like large

152
00:18:09,170 --> 00:18:14,390
corporations or big industries that are producing like a lot more and so feeling like it's kind of pointless.

153
00:18:14,600 --> 00:18:25,040
So there's another mental model piece. It's the concept you have of climate is something that can be only influenced by organizations, by nations.

154
00:18:25,910 --> 00:18:34,430
Then why does it matter what car I drive? Why does it matter how much I turn off by lights or, you know, engage in specific behavior?

155
00:18:34,430 --> 00:18:37,490
Is that like really, really small barrel places?

156
00:18:39,170 --> 00:18:47,750
Be related to carbon. Notice that this, by the way, is the same core challenge.

157
00:18:50,000 --> 00:18:53,720
Even outside of climate. It's the public health challenge like.

158
00:18:54,780 --> 00:19:03,810
Does it matter if we change how much processed meat you eat on an individual basis?

159
00:19:05,130 --> 00:19:09,510
Is that going to change the obesity epidemic? No, it's one person.

160
00:19:09,810 --> 00:19:14,190
But if we change that same thing across millions of people, then maybe it does.

161
00:19:15,780 --> 00:19:20,940
And so there's the one to many problem in the context of all the behaviors.

162
00:19:21,270 --> 00:19:26,130
Another one too many problem in the context of most public health issues.

163
00:19:27,340 --> 00:19:32,140
Where action needs to occur at an individual level, but we want it to have population level impact.

164
00:19:35,830 --> 00:19:42,790
Any other stuff. Tiger, if you get a number of you talked about sort of that bathtub analogy.

165
00:19:45,190 --> 00:19:51,250
The idea that you keep pouring stuff in, you're not committing if you can't pour it all out instantly, instantly.

166
00:19:52,160 --> 00:19:56,170
Now, this is a momentum analogy that's supercritical in the context of climate.

167
00:19:58,330 --> 00:20:06,790
If you believe that the net, let's say let's take it in terms of carbon in the atmosphere.

168
00:20:06,790 --> 00:20:19,840
If you believe that if we were to stop putting more carbon into the atmosphere today, that the atmosphere would immediately start to change.

169
00:20:21,490 --> 00:20:30,760
Then you can take a wait and see kind of an approach to behavior because you could push it right up to the edge and then back it off.

170
00:20:32,680 --> 00:20:41,710
Whereas if you conceive of the climate system like a freight train going 60 miles an hour.

171
00:20:44,020 --> 00:20:48,980
And it's going to take it 30 minutes and five miles to come to a stop.

172
00:20:49,630 --> 00:20:54,520
Once you put the brakes on, you have a very different concept as to when you need to do action.

173
00:20:58,280 --> 00:21:02,930
We're taking a system level analogy and applying the momentum assumption to it.

174
00:21:03,860 --> 00:21:13,159
Either is there momentum or not? I can change our moment, our desire to act in a lot of ways, in which a lot of different systems,

175
00:21:13,160 --> 00:21:18,470
in which we assume that we can change things instantaneously, and whereas in fact, we can't.

176
00:21:24,370 --> 00:21:28,480
On that vein, Annie. Hypertension.

177
00:21:31,720 --> 00:21:35,770
Yeah. So I was sort of reading the article about climate change.

178
00:21:35,770 --> 00:21:44,860
It was making me think about treating things like hypertension that tend to be pretty asymptomatic and not feel immediate to people

179
00:21:44,860 --> 00:21:51,880
and be something that you kind of have to look for if you want to see the signs of change or worsening or just persistence,

180
00:21:52,960 --> 00:22:00,220
which I think makes it something that's harder to treat just because it ends up being not front of mind for people and not feel as impactful.

181
00:22:01,210 --> 00:22:09,420
So let's expand this for a second. Can you feel when your high blood pressure is high?

182
00:22:13,300 --> 00:22:17,050
Typically. Say that again? Not typically.

183
00:22:17,770 --> 00:22:21,639
Usually. That's really, really. But let's get into the nuance. What do you mean by not typically.

184
00:22:21,640 --> 00:22:22,810
When could you feel it?

185
00:22:23,620 --> 00:22:31,240
If it's severely high, sometimes you get like headaches and blurry vision, but most of the time it has to be like really, really high for that.

186
00:22:32,400 --> 00:22:35,880
So that's one model. What's a lay model?

187
00:22:36,090 --> 00:22:40,200
What's a naive model? Are there are there beliefs that you could.

188
00:22:40,740 --> 00:22:46,710
And do we use language that talks about. Yeah, I think there are people who get stressed out.

189
00:22:46,860 --> 00:22:54,479
Like I can tell my blood pressure is really high right now. So there's the stress to blood pressure, I think that is often discussed.

190
00:22:54,480 --> 00:23:02,610
So whether it's anger, whether it's stress, there's this belief that.

191
00:23:04,650 --> 00:23:10,020
I can't let the experience of that stress is an experience of blood pressure.

192
00:23:11,020 --> 00:23:15,340
And you be careful with my language here because there is.

193
00:23:16,550 --> 00:23:20,750
At the system level associations, people who are more stressed.

194
00:23:22,320 --> 00:23:25,350
Are more likely to have higher blood pressure.

195
00:23:25,620 --> 00:23:33,600
But that's not the same thing as saying if I get upset right now that my blood pressure is in proportion to what I am feeling right now.

196
00:23:36,810 --> 00:23:42,000
Why am I bringing this up? And I want to go back to where we started with Amy's example.

197
00:23:43,100 --> 00:23:52,220
If you believe that, if you are if that there is a link between what you are feeling in the current moment and your blood pressure.

198
00:23:53,550 --> 00:24:00,270
And you are feeling fine in the current moment. What does that tell you about whether or not you need to take your blood pressure medications?

199
00:24:03,420 --> 00:24:11,250
It's not that hard to get to the point where if you believe that there is a feedback loop and you're on the good side of that feedback loop,

200
00:24:11,820 --> 00:24:20,310
it would undermine your motivation to engage in appearance in protective behaviors and dietary changes, whatever.

201
00:24:24,010 --> 00:24:28,060
So this is one of these weird situations, but it's a really important example.

202
00:24:30,070 --> 00:24:32,310
It's actually really, really important.

203
00:24:32,410 --> 00:24:41,770
We're going to talk about something like hypertension to reinforce the idea that you cannot know how your blood pressure is without measuring it.

204
00:24:43,780 --> 00:24:50,379
So that's the way this is often talked about, like when we go let somebody go to the doctor and their blood pressure is high and it's measured.

205
00:24:50,380 --> 00:24:53,470
It's hot. There's a conversation often about measuring.

206
00:24:55,940 --> 00:24:59,840
I've had some of these conversations myself. High blood pressure.

207
00:25:01,630 --> 00:25:07,340
Runs a little higher than you might expect it to be. But.

208
00:25:08,990 --> 00:25:14,060
A critical piece, which I have never had a health care provider say to me is.

209
00:25:16,280 --> 00:25:22,340
You can't actually feel this. The only way you will know what's happening is by moving.

210
00:25:27,910 --> 00:25:36,790
It's not weird to say that somebody and yet it's a critical piece here because if they believe that they can feel it, then what am I going to do?

211
00:25:36,820 --> 00:25:42,370
I'm not going to measure it. I'm just going to measure what I feel. I'm going to monitor the symptoms that I experience.

212
00:25:43,120 --> 00:25:45,370
And that's going to tell me whether or not I need to do this or not.

213
00:25:46,570 --> 00:25:54,730
And notice, by the way, it's the same kind of issue pops up in a variety of other contexts when we're talking about medication adherence.

214
00:25:58,050 --> 00:26:07,740
Whether we're talking about health medications, whether we're talking about pain management, whether we're talking about supplements,

215
00:26:08,250 --> 00:26:17,190
there's all kinds of things where it actually really matters whether we ought to be linking this to your experience or not.

216
00:26:24,700 --> 00:26:28,930
So maybe this is the time to bring up this conversation. I'm going to tell you a story.

217
00:26:29,530 --> 00:26:34,810
This is an absolutely true story. One of the things that happened to me.

218
00:26:35,920 --> 00:26:45,670
While I was going through my transplant, but I had substantial amounts of chemotherapy.

219
00:26:46,570 --> 00:26:50,950
Chemotherapy? One of the most common side effects of chemotherapy is nausea.

220
00:26:52,180 --> 00:26:58,300
So. This was a substantial issue.

221
00:27:00,650 --> 00:27:06,170
Have I talked about the nausea meds before? I'm suddenly wondering if I brought this up the three and I got to pick one.

222
00:27:07,100 --> 00:27:13,579
So if you remember from that conversation I ended up picking out of,

223
00:27:13,580 --> 00:27:22,540
the more generic name is lorazepam, which is an anti-anxiety medication is a benzodiazepines.

224
00:27:24,590 --> 00:27:28,340
Which has a effect on reducing nausea for many people.

225
00:27:28,370 --> 00:27:29,210
It certainly did for me.

226
00:27:30,680 --> 00:27:37,400
So immediately after my chemotherapy and for an extended period of time afterwards, it took a long time for that idea to start to go away.

227
00:27:39,770 --> 00:27:44,750
I was taking. As a parent, I have pretty much regular schedule.

228
00:27:45,660 --> 00:27:50,910
I think, if I remember correctly, I had one milligram pills and I was taking four times a day.

229
00:27:52,610 --> 00:27:56,780
Substantial dose and it worked great.

230
00:27:58,570 --> 00:28:03,810
But as the recovery progressed, I started to have less of that good.

231
00:28:03,880 --> 00:28:05,140
That's what's supposed to happen.

232
00:28:05,930 --> 00:28:14,050
So Canada, which we started, you know, I was saying, Hey, I'm getting better, etc. and they said, okay, so we should,

233
00:28:14,620 --> 00:28:24,850
you know, you should start, you know, have a conversation about tapering you off with medications and about that and.

234
00:28:26,980 --> 00:28:30,580
We had that conversation. I mean, it took a while for us to have that conversation.

235
00:28:31,000 --> 00:28:35,319
I'd actually the night had been sort of like, I'm the president and then it had been just sort of hit or miss.

236
00:28:35,320 --> 00:28:38,500
And then it was really sort of like, maybe I'm not really having much night.

237
00:28:39,510 --> 00:28:44,760
So by the time we actually had that conversation, that whole day, they before I pretty much had no idea.

238
00:28:47,040 --> 00:28:50,490
And so I went home and I was like, Yeah, you know what?

239
00:28:50,760 --> 00:28:55,610
I haven't had any nausea for, like 48 hours. And let's see how this goes.

240
00:28:56,670 --> 00:29:03,440
So I quit. Oh. I heard at least one gasp.

241
00:29:05,630 --> 00:29:08,870
What's wrong with that? Some of you know the answer to this.

242
00:29:10,100 --> 00:29:13,730
Withdrawal from benzodiazepine. It can be dependent on your dose.

243
00:29:13,940 --> 00:29:17,330
Harmful if not really hard on the typical cold turkey.

244
00:29:17,810 --> 00:29:21,900
In worst case, deadly. It could have killed me.

245
00:29:22,650 --> 00:29:31,110
I will say that particular night, my experience was imagine you ever watched a video at two times speed.

246
00:29:32,010 --> 00:29:36,650
Imagine mine awake with insomnia for 8 hours, watching videos at two times speed.

247
00:29:36,900 --> 00:29:43,750
That was roughly what that night was like for me. I survived it, but it could have killed me.

248
00:29:45,690 --> 00:29:59,770
California, this story. I was told to taper off the meds as he's given instructions, very clear instructions and off.

249
00:30:01,220 --> 00:30:09,350
I didn't follow that instructions. Why? Because you feel better.

250
00:30:10,380 --> 00:30:17,140
I felt that imagine that we weren't talking about medications and Lorazepam.

251
00:30:17,820 --> 00:30:27,680
We were talking about muscle pain and ibuprofen. I would have been perfectly fine.

252
00:30:28,880 --> 00:30:34,040
In fact, it might have been optimal to dose myself according to the level of symptoms.

253
00:30:35,500 --> 00:30:42,940
That mental model existed in me and I carried that mental model to this new context.

254
00:30:45,590 --> 00:30:55,399
The failure. No one not one person ever said to me, the reason we need you to taper is that we didn't.

255
00:30:55,400 --> 00:30:59,480
Cold is dangerous. Why didn't they tell me that?

256
00:31:00,140 --> 00:31:02,300
Because it wasn't. It's not that they didn't know it.

257
00:31:03,050 --> 00:31:09,379
I'm quite confident that every person who I talked to knew that it was built into their mental model.

258
00:31:09,380 --> 00:31:16,250
When they were saying the words you need to taper off, they in their minds, they were thinking because it's dangerous.

259
00:31:17,480 --> 00:31:21,260
Because their mental model included the danger associated with.

260
00:31:22,380 --> 00:31:25,890
It said to me and I have this competing mental model.

261
00:31:27,550 --> 00:31:34,330
That is accurate in a different context. That said, look, you know, it's based upon the level of symptom.

262
00:31:34,330 --> 00:31:43,270
This is symptom management and obtaining by personal point the symptom that I don't have the symptom I don't necessarily take my pain medication.

263
00:31:43,540 --> 00:31:55,100
So why doesn't that apply to you? This is the way mental models can really fundamentally change the way in which you communicate.

264
00:31:56,030 --> 00:32:03,530
Helping you learn what is the critical piece that distinguishes the expert from the layperson on this project?

265
00:32:03,560 --> 00:32:07,910
The critical piece here was not about management of symptoms.

266
00:32:07,910 --> 00:32:12,870
The critical piece here was the danger of stopping.

267
00:32:14,190 --> 00:32:17,310
And that's the piece that I never stopped until it was too late.

268
00:32:23,730 --> 00:32:29,430
They didn't need to tell me about what it was. They didn't need to tell me all kinds of stuff about management of it.

269
00:32:31,260 --> 00:32:39,219
I got it. Except for that. And that's where this stuff is going to be really powerful is when we start

270
00:32:39,220 --> 00:32:43,030
to realize what is it that tells us why we're doing the things we're doing,

271
00:32:43,130 --> 00:32:50,590
not just what the instruction is? Why is this what the instructions are?

272
00:32:51,730 --> 00:32:54,160
So now let's keep talking about medications.

273
00:32:54,350 --> 00:33:04,299
So we brought up the opioid question and you were talking about some experience, and I didn't get all the details here.

274
00:33:04,300 --> 00:33:08,680
But you've been working on opioid messaging and this really resonated with you.

275
00:33:08,860 --> 00:33:14,770
Yeah. So this summer I interned on like a project called Public Health called the Opioid Legislative Analysis.

276
00:33:14,810 --> 00:33:19,750
We basically reviewed like every bill that came through the Michigan legislature that mentioned opioids,

277
00:33:20,200 --> 00:33:26,500
and we tried to figure out if they were like risk factors for opioid misuse or protective factors for opioid misuse.

278
00:33:27,310 --> 00:33:31,890
And one of the ones that came through was this non-opioid directive forms.

279
00:33:31,900 --> 00:33:35,979
And we originally thought that like, oh, this is like a protective factor, amyloid misuse,

280
00:33:35,980 --> 00:33:41,860
because it's like people can sign it and then they won't use opioids and then they want it to address misuse.

281
00:33:42,550 --> 00:33:47,720
But then we actually read this article because it sent to us and it was saying that it's not a protective factor.

282
00:33:47,720 --> 00:33:51,010
If anything, it's a risk factor for adverse outcomes from surgery.

283
00:33:51,430 --> 00:33:57,280
So then we were like really confused about like did it know where to put it? Because for some outcomes it could be a risk factor.

284
00:33:57,310 --> 00:34:07,530
For other outcomes, it could be a protective factor. And that is what we've doing here is we're linking different mental models.

285
00:34:07,550 --> 00:34:12,350
One is the model, model of opioids and potential for addiction.

286
00:34:12,350 --> 00:34:17,020
And what is it that you believe about how someone gets fatigued?

287
00:34:18,620 --> 00:34:22,240
So. One model year is.

288
00:34:22,780 --> 00:34:30,650
Well, let me let me add feminine questions. Can you get addicted by taking one pill of an opioid?

289
00:34:35,770 --> 00:34:40,030
So there's a no there's a maybe there's a link, but whatever the answer is,

290
00:34:40,030 --> 00:34:42,430
I mean, so there's the scientific answer and there's what you might believe.

291
00:34:42,460 --> 00:34:51,730
But whatever the answer is, notice how critical that is in terms of the way you start to think about what is it safe to do and was it not safe to do?

292
00:34:52,600 --> 00:34:57,150
And if this isn't clean and opioids, do you have the same reaction about Cigarets?

293
00:35:00,030 --> 00:35:04,020
When you start to get nicotine dependance with your first cigaret.

294
00:35:08,880 --> 00:35:12,630
Aw. All right, let's set the complex, the alternate model.

295
00:35:13,440 --> 00:35:21,179
Is this a situation in which there is no impact for a while and then there's some kind of a threshold where if you cross that threshold,

296
00:35:21,180 --> 00:35:31,729
then the risk starts. So if I play this in cigaret terms you could smoke one cigaret a day and nothing happens to you.

297
00:35:31,730 --> 00:35:35,390
But if you smoke half a pack a day, you get addicted. Kind of thinking.

298
00:35:39,400 --> 00:35:44,680
Or is it duration? You do one cigaret at once.

299
00:35:45,430 --> 00:35:54,890
But if I do a month of it, I can't. So now we're into know is this cumulative dose is this.

300
00:35:55,940 --> 00:35:59,940
Time. Is this any exposure?

301
00:36:00,240 --> 00:36:03,420
What's the critical factor that leads to the negative outcome? Yeah.

302
00:36:03,570 --> 00:36:08,640
Yeah. Another thing I forgot to mention, but like also like the history of opioid use or history of, like, smoking.

303
00:36:08,820 --> 00:36:12,299
So if someone's, like, never taken opioids before, they might be like, oh,

304
00:36:12,300 --> 00:36:17,940
like it's more of a risk factor to have the adverse outcomes from surgery that is like develop opioid misuse.

305
00:36:18,200 --> 00:36:23,150
If someone's like previously had substance use problems in the past, they might think like getting in it,

306
00:36:23,520 --> 00:36:28,400
like taking a break, doing surgery could cause me to relapse and develop over it.

307
00:36:28,410 --> 00:36:35,430
Misuse again and not like that might be more of a risk to them than like the adverse surgical outcomes in like their mental model that's coming in.

308
00:36:35,430 --> 00:36:36,209
Or on top of that too.

309
00:36:36,210 --> 00:36:42,780
I mean, I think there's a lot of environmental social risk factors do that are completely independent of the medicine itself where I mean,

310
00:36:42,780 --> 00:36:45,980
I think characterizing a lot of opioid overdoses as depths of despair,

311
00:36:45,990 --> 00:36:52,230
kind of where either there's unemployment or other factors and stuff where it's much more than just pharmacological interaction.

312
00:36:52,740 --> 00:37:01,860
Which takes us back to the beginning here in terms of is the model of addiction that you hold in your mind, one that is a purely chemical one,

313
00:37:01,860 --> 00:37:07,550
a biological one, and the only risk factor is the amount of exposure you've had or the pattern of exposure again,

314
00:37:08,280 --> 00:37:15,839
or is your concept of this that there is an interaction between the your chemical exposure and other factors,

315
00:37:15,840 --> 00:37:20,010
whether they be social support or environmental context or whatever.

316
00:37:21,380 --> 00:37:27,380
That matters here in terms of whether or not you're going to perceive, let's say,

317
00:37:27,380 --> 00:37:34,040
having your second surgery in which you might be prescribed opioids as a thing,

318
00:37:34,040 --> 00:37:41,190
that is going to potentially be a very dangerous thing to do because you expect that you're going to get addicted versus something that you see.

319
00:37:41,210 --> 00:37:48,360
Yeah. No, this isn't going to be a problem because X, Y or Z. And again, I want to separate here the question of what is the truth?

320
00:37:49,640 --> 00:37:58,780
From the implications of whatever somebody's mental model is on their behavior at this complex questions here about what is the the relationships

321
00:37:58,790 --> 00:38:07,310
between all of these different factors and the likelihood that any individual is going to engage is in going to experience addiction.

322
00:38:08,300 --> 00:38:12,290
I bring it up, however, because and this is why I had you guys read that article.

323
00:38:13,830 --> 00:38:20,730
When somebody holds perceptions of there is a danger, you get this potential compensatory behavior.

324
00:38:21,300 --> 00:38:28,800
So they're acknowledging the risk of opioid addiction and engaging in protective behaviors,

325
00:38:28,810 --> 00:38:37,230
in this case avoiding opioids in a context in which that puts them at risk of adverse surgical outcomes or other kinds.

326
00:38:37,230 --> 00:38:42,540
Or just simply. The negative of experiencing uncontrolled pains.

327
00:38:46,700 --> 00:38:50,960
And I, I mean, I sympathize with this.

328
00:38:51,170 --> 00:38:58,180
Like, I don't know what I would do if I had a surgery in which it was significant.

329
00:38:58,210 --> 00:39:07,120
I was going to be prescribed Nokia. I really don't because I can feel both of these things inside my mind going, Oh, I don't want this exposure.

330
00:39:09,250 --> 00:39:16,770
But I don't want this other, you know, this pain either. Why am I emphasizing this one?

331
00:39:17,850 --> 00:39:25,630
Because. Until we acknowledge both of both of these pieces at once and can bring them to the surface.

332
00:39:26,410 --> 00:39:31,150
Talking about only one of them isn't going to actually solve this problem.

333
00:39:31,210 --> 00:39:38,790
People are not blank slates in this space. Everybody carries around some kind of mental model of addiction.

334
00:39:40,520 --> 00:39:45,410
What their mental model is going to influence how they respond in these situations.

335
00:39:48,130 --> 00:39:53,970
After. There is a lot of more people now than there were five years ago.

336
00:39:54,690 --> 00:39:58,350
We're talking to health care providers and said, no, don't give me this opioid.

337
00:40:00,350 --> 00:40:04,790
And maybe that's a good thing in many circumstances and maybe it's a bad thing in many circumstances.

338
00:40:04,790 --> 00:40:10,700
But either way, it's a thing and it's a mental model, things that we going to have to wrestle with as society.

339
00:40:17,150 --> 00:40:22,710
On a related note, it's a fella. You were talking about antibiotics.

340
00:40:24,010 --> 00:40:28,300
And we have mental models of antibiotics, too. So if you may.

341
00:40:29,140 --> 00:40:36,040
Yeah, I was talking about antibiotics and how most of us know that we need to take a full dose.

342
00:40:36,450 --> 00:40:45,070
I feel like my experience doctors always communicate that you need to take the full dose, but also on the other end, drawing the line of.

343
00:40:46,240 --> 00:40:51,130
Taking too much antibiotics and taking antibiotics. Help me fight off particular infections.

344
00:40:52,420 --> 00:41:00,399
For some context, I have really bad allergies, so I get frequent sinus infections and so I always find myself taking antibiotics for these infections.

345
00:41:00,400 --> 00:41:08,290
But when is it considered too much that is harmful for my body, but also good enough to help my body fight off the infection?

346
00:41:08,680 --> 00:41:12,580
Yeah. So there's several mental models that are relevant in this space.

347
00:41:14,020 --> 00:41:19,720
So this concept of cumulative antibiotic exposure.

348
00:41:21,520 --> 00:41:34,900
Comes up in our food supply. And the desire to limit the amount that animals that are being raised for food are exposed to antibiotics.

349
00:41:35,800 --> 00:41:45,550
It comes up certainly in the societal level. How frequently are are we prescribing antibiotics for acute infections?

350
00:41:48,880 --> 00:41:58,570
And it comes up at the individual level of do you believe that there is a differential, either effectiveness or risk?

351
00:42:00,010 --> 00:42:07,990
If you like, Isabella, have been exposed repeatedly to antibiotics versus this is your first exposure.

352
00:42:08,170 --> 00:42:09,970
I'm in the spotlight. I'm in the same place you are.

353
00:42:10,380 --> 00:42:17,200
Like during my transplant for two years, I was on prophylactic, not in response to an effective prophylactic.

354
00:42:17,440 --> 00:42:22,980
Antibiotics, antifungals and antivirals. So.

355
00:42:24,330 --> 00:42:30,209
Here is a piece of my mental model of my risk. The next time I take an antibiotic or something else, I'm like, Yeah,

356
00:42:30,210 --> 00:42:35,080
but I've already been exposed to this stuff and maybe it's different than maybe it's not.

357
00:42:35,100 --> 00:42:42,030
But whatever that belief is, it changes how I feel about that next prescription, just in many ways, like what you're describing.

358
00:42:44,350 --> 00:42:47,330
So I'm not sure there's a simple answer here.

359
00:42:47,350 --> 00:42:54,560
I honestly, truly do not know what the scientific answer is in the context of whether there is cumulative effects at the individual level.

360
00:42:54,580 --> 00:43:00,790
Clearly, there are cumulative effects of the societal level in terms of the development of antibiotic resistant organisms.

361
00:43:03,760 --> 00:43:11,830
But either way, it's going to shape how. When I have an ear infection, do I make the phone call?

362
00:43:15,650 --> 00:43:19,260
Or. Try and get a prescription or not.

363
00:43:21,410 --> 00:43:27,100
And people who have the experience of chronic repeated types of infections.

364
00:43:27,110 --> 00:43:35,840
And yet this could be antibiotics, it could be fungal infection, whatever you're having this mental model context conversation with yourself like.

365
00:43:37,440 --> 00:43:42,450
You know, last time it's happened. But, you know, is it okay to keep doing that?

366
00:43:43,950 --> 00:43:50,999
And for some people that will trigger them to immediately call and say, hey, I get these on a regular basis and let's do this.

367
00:43:51,000 --> 00:43:56,790
I know this works, so let's keep doing this. And for other people, based upon their mental model, that will trigger more and more resistance.

368
00:43:57,300 --> 00:43:59,340
Maybe I shouldn't do it this time. It should wait.

369
00:44:08,270 --> 00:44:17,090
Notice a lot of these model things are things that will influence us on that tipping point of action.

370
00:44:19,160 --> 00:44:23,330
It's not that we're not aware. It's not that we don't recognize that there's a choice.

371
00:44:24,920 --> 00:44:30,950
It's that what we believe is causing the outcome of what we believe the consequences

372
00:44:30,950 --> 00:44:35,300
of the of the behavior are going to be influencing whether we go there at all.

373
00:44:37,640 --> 00:44:43,010
So I want you to particularly think about mental models in situations of insurance.

374
00:44:46,480 --> 00:44:56,590
That's a domain in which this tends to come up. Also we might get you.

375
00:44:59,080 --> 00:45:07,190
Oh, yeah. Let's do that one. Back up. We're talking about your mom and.

376
00:45:09,320 --> 00:45:13,520
Different mental models about very different medications. Oh.

377
00:45:14,890 --> 00:45:21,160
I remember when I was growing up in middle school and high school, you know, and you get like a really bad cough or bad cold.

378
00:45:21,550 --> 00:45:25,660
And it was a lot more common for doctors to prescribe cough medicine with codeine.

379
00:45:26,590 --> 00:45:30,309
And then also, you know, you get your wisdom teeth out. They give you Tylenol with codeine.

380
00:45:30,310 --> 00:45:39,280
And she would never fill the prescriptions for me just because she didn't want me on that stronger, you know, like a painkiller.

381
00:45:39,280 --> 00:45:48,690
She didn't want me codeine. However, she was prescribed muscle relaxers from a previous injury like years prior, and she kept them.

382
00:45:48,700 --> 00:45:51,520
She didn't finish the medication. She kept them.

383
00:45:52,090 --> 00:45:58,030
And, you know, playing sports in high school and stuff, you come home from the practice and you're really sore.

384
00:45:58,240 --> 00:46:05,230
Should give me like a quarter of a pill. So it's just interesting to think about her not being okay necessarily with

385
00:46:05,720 --> 00:46:10,629
doing a prescription that was written to me for like in a condition that I like.

386
00:46:10,630 --> 00:46:19,220
It was there was a reason for it, but she was okay with giving me a medication that had been prescribed to her, probably because she'd taken them.

387
00:46:19,320 --> 00:46:23,080
I knew what they felt like. And you know what's more comfortable with that?

388
00:46:23,410 --> 00:46:31,270
Yeah. So. But as with most of these things, it's layers upon layers of mental models.

389
00:46:31,780 --> 00:46:41,440
So the first layer is, as you're talking about. These are different medications and your mom's perception of risk or safety.

390
00:46:42,780 --> 00:46:46,020
Is likely driven, at least in part by experience.

391
00:46:46,560 --> 00:46:52,440
So she had experience with these muscle relaxers, had not had a negative experience.

392
00:46:52,830 --> 00:46:56,370
That leads to a perception that notice backward but still going back to that experiential learning.

393
00:46:56,890 --> 00:47:01,050
But did the thing nothing bad happen. Therefore they must be safe kind of thinking.

394
00:47:01,440 --> 00:47:04,080
So that's what you're describing with those models, muscle relaxants.

395
00:47:04,410 --> 00:47:12,480
Is experiential learning of safety that then is carried forward not just from her continuing to take them, but giving them to you.

396
00:47:15,690 --> 00:47:18,870
I would be curious. You may not know had she ever taken Cody.

397
00:47:19,670 --> 00:47:27,150
Um. I'm not sure. She is a pretty interesting script to you on the medical field.

398
00:47:27,180 --> 00:47:36,209
She works for a chiropractor who does not necessarily agree with kind of Western medicine and that sort of thing.

399
00:47:36,210 --> 00:47:45,330
So I'm not really sure if she's ever taking part. So my guess here is that there is a mental model in her understanding of pain.

400
00:47:47,360 --> 00:47:50,960
It's very much oriented against chemical directions.

401
00:47:51,950 --> 00:47:55,970
And perceives that, rightly or wrongly.

402
00:47:57,080 --> 00:48:03,350
Strong pain relievers like codeine as something to be avoided, if at all possible.

403
00:48:05,550 --> 00:48:12,810
And that's part of the mental model it that most other people might hold strong mental

404
00:48:12,810 --> 00:48:18,450
models about how awful it is to be in pain and how critical it is to have good pain relief,

405
00:48:18,450 --> 00:48:22,300
to minimize suffering, etc. And that would make them more likely to take it.

406
00:48:22,320 --> 00:48:30,270
Like, I don't want this to all be about addiction. Like some of this is about your mental models, about understanding the depth of impact that.

407
00:48:31,400 --> 00:48:34,400
Physical conditions or mental conditions can have on you.

408
00:48:36,640 --> 00:48:39,940
A second layer, however, layered on to that is.

409
00:48:41,450 --> 00:48:44,240
What you're describing is.

410
00:48:46,560 --> 00:48:57,480
You know, from a different perspective, the ways that mental models lead into medication abuses, period, regardless of what the type of medication is.

411
00:48:57,870 --> 00:49:02,159
So your mother and again, I'm going to use what you describe.

412
00:49:02,160 --> 00:49:07,350
I don't want I don't want this to come across as picking on your mother because I have innumerable other examples I could draw.

413
00:49:07,740 --> 00:49:13,650
Your mother kept in medication that was prescribed for one particular person for one particular purpose,

414
00:49:14,280 --> 00:49:18,150
and kept it in the House and did not perceive a risk to doing so.

415
00:49:20,500 --> 00:49:27,400
Including a risk for you or anybody else fighting it and consuming it in a inappropriate manner.

416
00:49:27,970 --> 00:49:31,750
And if you ever want to talk, you know, this may not seem that critical when we're talking about muscle relaxants,

417
00:49:31,750 --> 00:49:38,590
but if we want to talk about ADHD medications or if we want to talk about codeine or if we want to talk about other drugs,

418
00:49:38,590 --> 00:49:40,660
which are often more directly abused,

419
00:49:41,440 --> 00:49:50,950
having it in the house is it's risk is part of the risk because it opens the door for someone else you to find it, take it and use it.

420
00:49:52,190 --> 00:49:57,460
So there's real conversations around is it safe to keep the medication in the house

421
00:49:58,060 --> 00:50:02,740
that are driven by people's mental models of what might happen or what not if.

422
00:50:04,570 --> 00:50:09,250
Your mental model is only grounded in you.

423
00:50:11,560 --> 00:50:16,370
That's one thing. If your mental model is grounded, then, look, I have small kids who like to open things.

424
00:50:16,390 --> 00:50:23,260
That's one whole set of risks that you might pay attention to if your mental model includes or doesn't include the idea

425
00:50:23,260 --> 00:50:28,750
that you have teenagers who might want to take that medication and either use it themselves or give it to somebody else.

426
00:50:28,930 --> 00:50:33,550
That's a whole nother set of risks that they might be paying attention to or not paying attention to.

427
00:50:36,750 --> 00:50:44,120
I've done a little bit of work in the context of. Of managing post-surgical opioid risk.

428
00:50:44,840 --> 00:50:47,090
And I think the thing that has driven that.

429
00:50:49,130 --> 00:50:58,220
Stuck with me the most was how much the people I was working with really were grounded on the is the medication in the house.

430
00:50:59,750 --> 00:51:06,080
And do you think it's ever okay for anybody other than you to take it for any purpose whatsoever?

431
00:51:08,270 --> 00:51:16,320
Both of those are mental model ideas. And so the intervention I was part of was basically a disposal intervention.

432
00:51:16,890 --> 00:51:20,310
We weren't getting into risk communication. We were just getting into the.

433
00:51:20,610 --> 00:51:32,800
If you're done, get rid of it. And here's how. Which is not necessarily the way I think most people think about it, because the thought is the risk.

434
00:51:33,150 --> 00:51:38,530
The moment is the moment of giving it to someone as opposed to having it in the house.

435
00:51:40,770 --> 00:51:51,120
But then you have to ask the question, well, okay, you know, why do we think about a medication that has potential danger associated with it that way?

436
00:51:51,510 --> 00:51:56,790
And when you think about a knife or a gun that is in the house in a different part.

437
00:51:59,260 --> 00:52:07,960
And so people don't like. And that's also a mental model issue of is in the house does not equal risk and how do you think about.

438
00:52:13,690 --> 00:52:17,950
As I said, I had lots of examples for today. Oh, I don't want to skip over this one.

439
00:52:19,180 --> 00:52:25,400
Let's go back to the sunscreen one. Number of people wrote about this.

440
00:52:25,430 --> 00:52:29,090
I dropped down a few names. I'm sure I missed some people I know.

441
00:52:30,170 --> 00:52:33,590
However, you talked about, as you know, you talked about it. Sure.

442
00:52:33,590 --> 00:52:41,800
I missed a couple other people. Just reactions.

443
00:52:42,520 --> 00:52:45,880
How many of you were surprised when you read that?

444
00:52:47,650 --> 00:52:53,480
I was. So what was the mental model before?

445
00:52:54,920 --> 00:53:01,610
If you think of surprise as a marker of changing from one mental model to another.

446
00:53:02,810 --> 00:53:07,660
Let's describe what the mental models were and what the mental model that this article suggests is true.

447
00:53:07,670 --> 00:53:11,209
And again, I don't claim to know the scientific accuracy of this.

448
00:53:11,210 --> 00:53:15,590
I'm sure there is more nuance here than was represented by a thousand word article,

449
00:53:16,070 --> 00:53:19,910
but I'm not going to ignore the fact that we're oversimplifying here.

450
00:53:20,570 --> 00:53:27,200
But what's the naive model that says everybody should be wearing sunscreen?

451
00:53:32,290 --> 00:53:37,930
UV rays exposure causes or increases risk for melanoma.

452
00:53:38,340 --> 00:53:45,250
Right. So we have the environmental map. Skin cancer being caused by U.V. ray exposure.

453
00:53:47,910 --> 00:53:52,710
Which is scientifically accurate, scientifically accurate, at least for some people.

454
00:53:53,730 --> 00:53:58,380
So amount of exposure equals is in some degree related to risk.

455
00:53:59,470 --> 00:54:03,180
Minimize that exposure by wearing sunscreen.

456
00:54:05,630 --> 00:54:10,270
Thereby reducing the risk of cancer. This is a good thing.

457
00:54:11,630 --> 00:54:17,330
And by the way, let's be an important piece here. And everybody can get cancer.

458
00:54:18,700 --> 00:54:20,170
That's a key piece of this model.

459
00:54:20,950 --> 00:54:28,959
The idea that cancer risk exists within all, because if we believe that there are certain people who can't possibly get cancer,

460
00:54:28,960 --> 00:54:35,890
then you end up with a very different kind of conception. What's the change that this article is suggesting?

461
00:54:43,780 --> 00:54:49,320
What's the modification to the metal off? Yes.

462
00:54:50,060 --> 00:54:56,130
Increased the amount of melanin inside. It was probably the least developed.

463
00:54:57,270 --> 00:55:03,000
That's a perfectly fair way of summarizing it that the critical new factor here is

464
00:55:03,750 --> 00:55:10,170
concentration about talent in a sense of how dark the skin is and that that is in some way.

465
00:55:11,930 --> 00:55:17,450
Proportionate to the level of risk that when there's darker skin, you have less risk.

466
00:55:17,450 --> 00:55:20,000
When you have virus skin, you have more risk. Yeah.

467
00:55:20,330 --> 00:55:25,729
And then the other piece that I took from that as an epidemiologist specifically was the magnitude of risk,

468
00:55:25,730 --> 00:55:31,490
that the magnitude for people of color is so low that even reducing their risk

469
00:55:31,490 --> 00:55:36,050
further is reducing a risk that's already ridiculously low to begin with.

470
00:55:36,060 --> 00:55:39,320
So what's the actual absolute value of that?

471
00:55:39,350 --> 00:55:40,669
So a key piece here.

472
00:55:40,670 --> 00:55:49,700
What you're bringing up, which is important, is that another part of the mental model that you hold is that the baseline risk is also okay.

473
00:55:49,760 --> 00:55:54,080
So there's differences both in baseline and in marginal effect.

474
00:55:55,820 --> 00:56:02,240
So it's not the case that. Everybody gets the same degree of benefit.

475
00:56:02,540 --> 00:56:06,410
It's just that some people start at a higher risk of they got out of here and other people start here.

476
00:56:06,450 --> 00:56:09,350
Get out of here. It's a you're starting at different places.

477
00:56:10,070 --> 00:56:16,670
And the magnitude of the change is substantially different for people with lighter skin versus darker skin.

478
00:56:18,030 --> 00:56:21,270
Thus undermining essentially motivation at two levels.

479
00:56:21,330 --> 00:56:25,170
One, you haven't had much risk in the first place. And two, it's not gonna make much of a difference.

480
00:56:27,390 --> 00:56:32,290
Now, let's also throw in what's the flaw with what I would say?

481
00:56:33,420 --> 00:56:37,290
A flaw. There are multiple flaws here with this mental model.

482
00:56:38,130 --> 00:56:42,480
Why might someone want to wear sunscreen even if they have dark skin?

483
00:56:44,040 --> 00:56:48,780
Reduce the effects of aging. We don't only care about cancer.

484
00:56:50,340 --> 00:56:59,100
So there are other outcomes that we can totally reasonably say are important that also motivate use of sunscreen.

485
00:56:59,280 --> 00:57:05,909
And if your mental model includes both risks of cancer and these other kinds of factors,

486
00:57:05,910 --> 00:57:10,230
whether that's skin consistency or effects of aging or wrinkles, whatever.

487
00:57:11,790 --> 00:57:18,509
Now all of a sudden we have a more complicated balance of, well, how much do I care about those things?

488
00:57:18,510 --> 00:57:22,200
Versus How much do I care about cancer mortality?

489
00:57:22,440 --> 00:57:25,889
And is the cosmic alcoholic cosmetic?

490
00:57:25,890 --> 00:57:28,440
But I don't that's not a pejorative term. I'm just describing.

491
00:57:28,980 --> 00:57:34,410
Are those benefits worth the time and money and effort that I would put in to wear sunscreen?

492
00:57:34,410 --> 00:57:39,090
If the answer is yes, now we're done. Go ahead. Let's use it. I think there's no problem with that.

493
00:57:40,080 --> 00:57:47,510
But why am I bringing this up? The conversation often is only about cancer risk.

494
00:57:48,730 --> 00:57:56,790
You don't actually often go there, at least at the public health communications like beauty companies will go there.

495
00:57:56,810 --> 00:58:02,000
Beauty companies will talk about protecting your skin against, you know, wrinkles all the time.

496
00:58:02,000 --> 00:58:06,140
But public health people want not nearly as much.

497
00:58:07,370 --> 00:58:11,270
So is that because that stuff doesn't matter to us?

498
00:58:15,200 --> 00:58:21,650
Oh, so personal and personal example is I actually never used the word sunscreen,

499
00:58:21,660 --> 00:58:30,230
but now we learn a lot of things because over the summer I would ask the patient who gave me a facial and she was like, Do you use sunscreen?

500
00:58:30,380 --> 00:58:35,110
And she was also an American. And I told her no. And she said she understands why.

501
00:58:35,120 --> 00:58:42,589
However, my facial ingredients that I use are affected by the sun, and if I don't put on sunscreen,

502
00:58:42,590 --> 00:58:46,640
even half of the active ingredients that I'm using are just going away.

503
00:58:46,700 --> 00:58:52,040
And I'm like, Oh, say no more. Now, wear sunscreen only on my face.

504
00:58:53,300 --> 00:58:56,930
Which, by the way, now you have another separate benefit,

505
00:58:57,260 --> 00:59:06,610
which is the interaction effect of the sunscreen with other products that you may be using, which is also a benefit that we can care about.

506
00:59:07,730 --> 00:59:14,570
So part of the reason for this as a transition point in this course is I want us to get started

507
00:59:14,570 --> 00:59:21,410
thinking about what are the outcomes that are associated with behaviors and to recognize that.

508
00:59:22,920 --> 00:59:29,190
Moving from just how do I communicate about an outcome to how do I balance outcomes?

509
00:59:29,190 --> 00:59:34,049
And the way you value outcomes may not be the same way I value outcomes that we need to

510
00:59:34,050 --> 00:59:39,180
start to bring in values into the conversation about what is the right thing to do.

511
00:59:40,230 --> 00:59:51,090
Is it going to be informed? It is not irrational for two people both to have the knowledge that you just described and for one person to say, wow,

512
00:59:51,090 --> 00:59:54,959
this is a really important reason for me to wear sunscreen and another person to say,

513
00:59:54,960 --> 00:59:59,190
yeah, I just don't care about that and therefore do not wear sunscreen.

514
01:00:00,120 --> 01:00:03,450
And both of them might be making the appropriate choice for them.

515
01:00:06,680 --> 01:00:17,180
Because that's a value driven choice. And if one person ends up with wrinkles and the other one doesn't, they may both be, in one sense, happy.

516
01:00:23,110 --> 01:00:26,660
This mental model. I mean, we could do a long course about another bottle.

517
01:00:26,680 --> 01:00:31,560
I'm not doing it because I don't think it's about the method so much as this insight.

518
01:00:31,810 --> 01:00:34,719
Okay. We have to zoom in on all the different pieces here,

519
01:00:34,720 --> 01:00:40,810
like the different pieces between is it okay we have a mental model or is it okay to have this in the house?

520
01:00:41,350 --> 01:00:45,040
Is it okay for me to give this to somebody else? Is this okay?

521
01:00:46,260 --> 01:00:50,700
For me to do, even though it's not going to give me this benefit because it's giving me something else.

522
01:00:54,900 --> 01:00:59,680
Is it okay for me to take this now? Is it okay for me to take this a long time?

523
01:00:59,700 --> 01:01:03,240
Is it okay for me to take this if I don't have a symptom?

524
01:01:03,930 --> 01:01:08,790
I need to take this if I don't have a symptom. We've all of these things have already come up today.

525
01:01:09,690 --> 01:01:17,880
This is the heart of the mental model of idea. What I'm pushing for here is sort of a reflection.

526
01:01:19,390 --> 01:01:26,860
That you will not assume that just because you know something, that's what needs to be communicated.

527
01:01:27,730 --> 01:01:31,360
Part of the metabolic approach is the idea that we have to understand what the audience

528
01:01:31,360 --> 01:01:35,680
understands or doesn't understand and then deconstruct what that's going to lead them to do.

529
01:01:37,440 --> 01:01:43,169
And though we haven't talked about this, I do want to spend a little bit of time talking about the dioxin study.

530
01:01:43,170 --> 01:01:48,690
I was part of which you guys read about, if only because I think it's a super clean example of this.

531
01:01:52,610 --> 01:01:55,870
Jimmy We did the whole medical model thing for that project.

532
01:01:55,880 --> 01:01:59,820
Like all these we did like I was actually interviewed.

533
01:01:59,930 --> 01:02:06,620
By the way, expert interviews in that context included scientists, regulators,

534
01:02:06,980 --> 01:02:13,280
and at least one person who was an environmental activist who is in big court fights with the

535
01:02:13,280 --> 01:02:20,900
company that had with that had put toxins there in the first place because it wasn't about value.

536
01:02:20,960 --> 01:02:23,990
It was about their knowledge of what is the problem here.

537
01:02:25,270 --> 01:02:34,750
And what was incredibly clear when we did that was all the experts had very different opinions about what the actual.

538
01:02:35,850 --> 01:02:42,630
Health effects of dioxin exposure were ie how much exposure you had to get before you got?

539
01:02:42,660 --> 01:02:45,690
What kind of impact? Everybody had different opinions about.

540
01:02:48,040 --> 01:02:52,720
But when we got to the where is this stuff in the environment, everybody was exactly the same.

541
01:02:55,690 --> 01:03:04,270
And it was all driven by that specific understanding that dioxins are hydrophobic, do not dissolve in water.

542
01:03:05,680 --> 01:03:12,690
And so even though lots and lots of people were really worried about the water supply,

543
01:03:13,960 --> 01:03:19,570
drinking water supply, etc., none of the experts were like, Yeah, we're going to get to the water supply.

544
01:03:21,650 --> 01:03:27,170
Plus it's going to bond with. It doesn't matter where it went into the Obama, it's going to bond with something and stay put.

545
01:03:31,090 --> 01:03:34,840
And. Now.

546
01:03:35,350 --> 01:03:45,340
I actually did a community meeting after we did this research in Portland, Michigan, with residents and.

547
01:03:47,590 --> 01:03:51,740
In the end, it was a really simple meeting because we zoomed in on this one point.

548
01:03:51,820 --> 01:03:55,030
I said, Look, there is scientific disagreement here,

549
01:03:55,240 --> 01:04:04,810
but the thing that everybody is agreed upon that it seems really critical here is this it can't get from the river down to the groundwater.

550
01:04:05,200 --> 01:04:07,330
There is no pathway that that occurs.

551
01:04:10,690 --> 01:04:16,330
You can have arguments about which animals it's going to be in or which plants it's going to be like, but it can't get from here to there.

552
01:04:17,590 --> 01:04:22,510
And that was not something that any of those residents who were at that meeting had really ever understood.

553
01:04:26,820 --> 01:04:31,110
I still to this day don't really know how I would try and develop a communication about that.

554
01:04:33,110 --> 01:04:37,220
Because you can't just sort of say to people, no, the groundwater is safe.

555
01:04:37,550 --> 01:04:41,150
That's not going to do it. You've got to replace it with some other mental model.

556
01:04:42,470 --> 01:04:45,740
So this is in the paper. But let me let me ask you another question.

557
01:04:47,060 --> 01:04:52,370
What are the things? What were the analogies that people were making in that context?

558
01:04:57,210 --> 01:05:00,840
You draw upon Carolyn, with the Erin Brockovich example?

559
01:05:01,900 --> 01:05:06,280
Or I don't know what the chemical was. So was it the same chemical?

560
01:05:06,400 --> 01:05:09,910
I honestly don't know if it was hexavalent chromium, I think. Okay.

561
01:05:10,420 --> 01:05:17,070
Which is to say, oh, no, this is dioxin. But yeah, that got into the groundwater and it was like a big cover up.

562
01:05:17,080 --> 01:05:24,940
And I mean, we certainly had lots of it got into the groundwater and there was a bigger cover up stories.

563
01:05:25,510 --> 01:05:33,620
So it's not like that story hasn't been told. And even if we didn't have a company involved, like.

564
01:05:36,540 --> 01:05:41,110
The kinds of things that caused groundwater contamination. Oh.

565
01:05:42,070 --> 01:05:53,670
Like farming, runoff farming, what happens in Michigan, in particularly rural areas of Michigan, like not that far from in Saginaw farming.

566
01:05:54,220 --> 01:06:01,350
We had a heck of a lot of people in that community who had very specific real world experience with farming runoff,

567
01:06:01,360 --> 01:06:06,250
whether we're talking about pesticides or fertilizer or other chemicals used in that process.

568
01:06:06,280 --> 01:06:14,230
They were already quite concerned about groundwater issues and industrial runoff issues because it was something they were already talking about.

569
01:06:15,610 --> 01:06:22,300
So they were taking models that they already used on a daily basis and applying it to a new thing.

570
01:06:22,990 --> 01:06:30,100
And if we were talking about hexavalent chromium or if we were talking about one four dioxane, those models would be pretty darn accurate.

571
01:06:32,710 --> 01:06:35,140
Except in this case, we're not.

572
01:06:36,170 --> 01:06:42,410
We're talking about something that behaves fundamentally differently in the environment, which leads to all kinds of weird implications.

573
01:06:43,830 --> 01:06:53,040
Like. When you're talking about stuff that gets taken up in water, all kinds of fruits and vegetables have contamination risks.

574
01:06:53,570 --> 01:06:59,700
If you're talking about something that doesn't, most fruits or vegetables don't because they don't take it up.

575
01:07:01,050 --> 01:07:06,390
You want to eat the tomato, you just grow in that soil, wash it, get the dirt off of it, and you're fine.

576
01:07:08,760 --> 01:07:13,350
It turns out there, however, there is. I can't remember. This is in the paper. There is one particular type of.

577
01:07:15,650 --> 01:07:19,180
Actionable that does take out the assets. You know.

578
01:07:21,600 --> 01:07:26,050
It's silly that this time of year. Pumpkins.

579
01:07:29,500 --> 01:07:32,530
Who commits, I believe, is the is the family name.

580
01:07:32,900 --> 01:07:36,830
Pumpkins is the most common one. They will take it up. Don't ask me why I'm not a killer.

581
01:07:36,880 --> 01:07:41,660
I have no understanding of how this can be, but it happens to be true.

582
01:07:41,680 --> 01:07:50,739
So, like, if you had a garden in the riverbank soil where there was that contamination, you want to get zucchini, you're fine.

583
01:07:50,740 --> 01:07:56,380
You want to get tomatoes, you're fine. As long as you wash it like it's the dirt that you got to worry about but don't grow about.

584
01:08:00,880 --> 01:08:05,590
But this is, again, a mental model issue, like giving people the Y, not just the instructions.

585
01:08:10,090 --> 01:08:16,640
So I want to wrap this up. With a few more examples I can bring up.

586
01:08:18,260 --> 01:08:22,370
Oh. All right. I'm not going to let this one go just because I have a good story for it.

587
01:08:23,510 --> 01:08:27,920
This is my problem. I have too many stories to tell us.

588
01:08:30,410 --> 01:08:35,180
Sorry. I'm trying to think of what I was. You were talking about pregnancy and age.

589
01:08:35,450 --> 01:08:41,479
Oh, yeah. So I was talking about, like, women over 35 being pregnant,

590
01:08:41,480 --> 01:08:48,960
considering to be like I advanced for our increased risk because of things like an advanced maternal age that just because you.

591
01:08:48,980 --> 01:08:52,080
Just for a second. Let's just hold on.

592
01:08:52,090 --> 01:09:00,280
For the lived experience of a 35 year old woman being slapped with the label of advanced maternal age.

593
01:09:00,920 --> 01:09:04,330
Oh, no. Worse. It's. It's like geriatric pregnancy.

594
01:09:04,550 --> 01:09:08,500
Oh, I'll tell you, this lady is like a last name term, but.

595
01:09:08,500 --> 01:09:13,030
Yeah, exactly. I was hospitalized, by the way, with our second kid.

596
01:09:13,480 --> 01:09:18,090
Her due date was like five days after her 35th birthday.

597
01:09:18,100 --> 01:09:20,880
So she got that label. She would not be.

598
01:09:23,020 --> 01:09:28,329
And that's kind of what I was talking about is like the disconnect between, like, medical professionals being like, this is concerning.

599
01:09:28,330 --> 01:09:32,500
You're an increased risk of pre-term labor C-section, birth defects,

600
01:09:33,490 --> 01:09:40,479
two different complications that women not having that same perception of themselves and feeling like when they're

601
01:09:40,480 --> 01:09:47,350
mental models about like well I would like when they when is timing of pregnancy correct for me in my life.

602
01:09:47,590 --> 01:09:53,260
Yeah. And so like kind of like that disconnect between like the medical professionals and

603
01:09:53,260 --> 01:10:00,399
then like people's experiences and also the idea of like women who who are older,

604
01:10:00,400 --> 01:10:06,490
who like end up going through fertility treatments, thinking that like, you know, that pregnancy is like a very, like natural thing.

605
01:10:06,490 --> 01:10:12,190
It's an expected part of life and not like they're willing to take on these increased risks of fertility treatments to be able to,

606
01:10:12,610 --> 01:10:16,570
you know, not go through like not being a mother. So like the kind of toss ups.

607
01:10:16,720 --> 01:10:18,760
Yeah. So there's there's several mental models here.

608
01:10:18,760 --> 01:10:23,380
I just want to call out one which you've just touched upon is whatever your mental model is about,

609
01:10:23,620 --> 01:10:32,410
the relationship between age and fertility is going to influence lifestyle decisions about when or if you choose to try to get pregnant.

610
01:10:33,640 --> 01:10:41,140
And the consequences here are, as you say, the likelihood of experiencing infertility and all of the things that might come with that.

611
01:10:43,260 --> 01:10:47,250
Two. There is the pregnancy complication.

612
01:10:47,550 --> 01:10:54,580
If one gets pregnant. The Association, whatever you believe about the relationship between age and pregnancy.

613
01:10:54,580 --> 01:11:00,820
And by the way, this goes back to something we've already talked about, which is what do you think, that I need a better life?

614
01:11:01,390 --> 01:11:14,650
What do you think that relationship looks like? If you think that the risk issue age relationship is linear, that leads to one sets of beliefs.

615
01:11:14,650 --> 01:11:21,400
And if you believe the relationship is, I find a better environment by and then woops now all of a sudden I'm at really really high risk.

616
01:11:22,310 --> 01:11:28,430
That leads to a different set of police behavior. And the reason I bring this up is notice that the way.

617
01:11:30,000 --> 01:11:42,450
The use of a label and the use of a hard age threshold of, say, advanced maternal age is if the due date is after the woman's 35th birthday.

618
01:11:42,840 --> 01:11:51,840
But if it's three, four and nine months, oh, then you're exactly the same in terms of the labels we attach to you as a 22 year old.

619
01:11:52,890 --> 01:11:57,870
Yeah. Of. We all probably can figure out that that's not.

620
01:11:59,410 --> 01:12:02,530
So that use of a label is creating a binary.

621
01:12:02,530 --> 01:12:10,050
You're at risk. You're not at risk kind of conversation. And it's something that underneath is continuous in at least some form.

622
01:12:10,940 --> 01:12:16,960
And a 34 year old does have a much higher risk of pregnancy complications than a 21 year old.

623
01:12:18,160 --> 01:12:21,850
Not much higher here. Let's be careful. You guys can call me on this.

624
01:12:21,850 --> 01:12:25,220
I'm using a verbal term here. We all know how bad verbal terms are.

625
01:12:25,840 --> 01:12:28,930
How much is much more like I wait in my hands.

626
01:12:28,930 --> 01:12:39,620
I don't know, but. The system treats this as a we need to do X if this threshold is hit.

627
01:12:40,710 --> 01:12:50,190
Whereas the underlying conversation in many ways ought to be one in which there is a creation of attention to both of these questions.

628
01:12:51,420 --> 01:12:55,380
Based upon age and frankly, a whole lot of other factors because it isn't just.

629
01:12:58,500 --> 01:13:01,860
Yeah. So that's another one of the connections I wanted to make,

630
01:13:02,160 --> 01:13:11,730
which is mental models are often both embodied in and affected by the labels that we use to things.

631
01:13:13,410 --> 01:13:19,560
Risk factor is about as direct a language for mental model as we get.

632
01:13:22,010 --> 01:13:26,059
You have three risk factors for cardiovascular disease is basically telling somebody,

633
01:13:26,060 --> 01:13:35,210
there are three pieces of my model that you have that are leading me to some bullet in my mental model that says you are high risk.

634
01:13:35,570 --> 01:13:42,080
And that high risk thing is qualitatively different than this low risk thing that's over here as opposed to a continuous conversation.

635
01:13:46,670 --> 01:13:51,840
It gets complicated. So I don't have a simple sort of take away for today.

636
01:13:53,430 --> 01:13:57,840
We'll spend the last couple of minutes talking in small groups about this question of.

637
01:14:00,560 --> 01:14:11,690
We can't do that. But all of the stuff we've talked about today can really get boiled down to really specific pieces of understanding that shift

638
01:14:11,690 --> 01:14:20,570
people's willingness to engage or not engage in a behavior to share that medication or not to wash their feet or not to.

639
01:14:22,490 --> 01:14:30,770
Do an extra maternal screening or not. Is it okay?

640
01:14:31,950 --> 01:14:35,390
A lot of communications like this are not complete.

641
01:14:36,170 --> 01:14:41,570
Is it okay for us to imagine? Our task is to message right and use the open example.

642
01:14:41,930 --> 01:14:46,570
The our message is maybe, you know,

643
01:14:46,760 --> 01:14:52,579
get it out of your house or the antibiotic Congress that context to specifically target

644
01:14:52,580 --> 01:14:58,760
something about make sure to finish your medications and I'm going to make this up.

645
01:14:58,760 --> 01:15:07,819
I don't know if this is paper accurate, but and don't worry if you're going to have to do this again the next time you get prescribed antibiotics,

646
01:15:07,820 --> 01:15:10,970
as long as you're continuing to do take them as directed.

647
01:15:12,650 --> 01:15:18,140
What do we have to bend this to? A much larger conversation about all the risks.

648
01:15:19,210 --> 01:15:22,610
So I think a couple of minutes, just talk about yourself.

649
01:15:24,380 --> 01:15:26,390
If you like this approach, do you hate this approach?

650
01:15:26,420 --> 01:15:33,020
Are there things that come up that making you nervous about zooming in on one specific piece of somebody's mental model and communication?

651
01:15:34,410 --> 01:15:48,110
Okay. That's what I want to end on today. We'll keep coming back to. Right.

652
01:15:49,060 --> 01:15:59,700
I mean, I know I know is already causing problems and stuff like that.

653
01:16:00,710 --> 01:16:07,280
I'm sure that a lot of these people change their minds.

654
01:16:07,600 --> 01:16:19,120
And I think like I think because I feel like it is.

655
01:16:19,910 --> 01:16:29,570
I don't think people minorities like that.

656
01:16:38,220 --> 01:17:05,700
I think it makes me feel like I can only tell you one thing and I feel like I don't like that.

657
01:17:10,330 --> 01:17:16,870
Right. Oh, I think it's that.

658
01:17:17,470 --> 01:17:30,210
And it doesn't matter what people are looking at.

659
01:17:32,120 --> 01:17:37,150
Life was like.

660
01:17:38,590 --> 01:18:04,810
Yeah. Right. You know, when it comes to your car, when you know exactly what you are giving them, makes me like, do that.

661
01:18:05,030 --> 01:18:12,400
Like that. Yeah, I know how I got out.

662
01:18:12,620 --> 01:18:28,250
And I think I was very surprised by that.

663
01:18:32,220 --> 01:18:47,450
And I mean, I think like first before, you know, it was like a high risk thing.

664
01:18:47,450 --> 01:18:53,240
Like. Like I like a lot.

665
01:18:54,050 --> 01:18:59,640
Yeah. Right. Yeah.

666
01:19:00,910 --> 01:19:06,830
Oh, yeah. I like that.

667
01:19:08,750 --> 01:19:13,400
And also the idea that you can get.

668
01:19:14,570 --> 01:19:19,880
I've read that. All right. It's time to wrap up now.

669
01:19:22,370 --> 01:19:31,759
As we go through the rest of this course, I encourage you to bring up the question of America bottled whenever it seems relevant,

670
01:19:31,760 --> 01:19:36,050
whenever it seems like we didn't really understand this relationship,

671
01:19:36,830 --> 01:19:45,350
this connection, this idea, this characteristic, and the puts on the risk, what makes them feel safe, etc., can come up over and over again.

672
01:19:45,350 --> 01:19:47,050
And lots and lots of different contexts.

673
01:19:48,110 --> 01:19:55,459
And all of our situations in which what we're trying to accomplish when we're communicating about risk is really to correct the mental model,

674
01:19:55,460 --> 01:20:02,960
not so much to give people a number. So next class we're going to wrestle with when do we need to give people numbers?

675
01:20:04,820 --> 01:20:09,229
When is it important to have numbers or when is it important to just be malleable

676
01:20:09,230 --> 01:20:13,170
or when is it important to just get into relationships like these mental models?

677
01:20:14,510 --> 01:20:20,270
So we're going to talk about that. And after that, we're going to sort of transition through a little bit of the shared decision making space.

678
01:20:20,270 --> 01:20:29,960
Moving towards that next assignment, we'll talk about issues of decisions and we'll talk about prevention, screening decisions and things like that.

679
01:20:30,920 --> 01:20:36,950
But that's what we're gonna do. So I will see you on Thursday doing a weather.

680
01:20:39,530 --> 01:20:44,390
Because it's. I got a bunch of them.

681
01:20:49,100 --> 01:21:11,270
So you're just like, they're going to do it. I mean, I think Sally is more like, believe it or not, more.

682
01:21:11,270 --> 01:21:19,520
Please. Yes. I mean, I don't know, I.

