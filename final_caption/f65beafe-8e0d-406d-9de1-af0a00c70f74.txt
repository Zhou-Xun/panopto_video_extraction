1
00:00:02,040 --> 00:00:11,910
So we are going to pick up from where we left off in simple linear regression and we are going to talk about is it just the.

2
00:00:13,880 --> 00:00:17,990
Let's talk about about the next homework.

3
00:00:18,910 --> 00:00:27,050
I wasn't planning to burn in the lecture because this is a really warm up for homework.

4
00:00:27,380 --> 00:00:38,090
But if you have questions, please feel free to talk to me after class or I do have to go back to my office for a meeting or, you know, email me.

5
00:00:38,480 --> 00:00:46,790
So this homework I wasn't planning to talk about because it's it's really kind of a normal sort of.

6
00:00:47,780 --> 00:00:57,950
But you are. Feel free. Feel free to ask me questions, you know, like outside lecture via email or trying to come and meet me in my office.

7
00:00:58,580 --> 00:01:15,280
Okay. So we are going to talk about balance, estimation and introduce the very important concept or method of least squares.

8
00:01:16,610 --> 00:01:20,600
Then we are going to talk about properties of the process, the meters.

9
00:01:22,320 --> 00:01:33,780
And establish, like, you know, the kind of the foundation, you know, love from where we can extend through multiple linear regression.

10
00:01:34,110 --> 00:01:39,060
And then we have a small example that I'll sort of like discuss.

11
00:01:39,690 --> 00:01:50,610
Okay. So in terms of the parameter estimation, so let's hear some preliminaries.

12
00:01:52,020 --> 00:01:58,200
So the truth is that we have done what can we go on?

13
00:01:58,290 --> 00:02:03,780
These are my two parameters. Remember, these are fixed but unknown.

14
00:02:04,500 --> 00:02:21,690
And under the simple linear regression model I have the mean of y given the fixed x is better not plus b down ixi.

15
00:02:22,830 --> 00:02:27,030
I know x because this is part of my data.

16
00:02:27,420 --> 00:02:30,450
I know the experiment, but I don't know beta not on beta one.

17
00:02:30,450 --> 00:02:39,460
So that's why the true mean is not observable because big enough and video on and on for I don't know what the true meaning.

18
00:02:40,440 --> 00:02:48,360
Similarly the two errors epsilon I which according to the model.

19
00:02:49,170 --> 00:02:57,930
So remember the model is this why you want to be able to not love beta one x plus

20
00:02:57,930 --> 00:03:10,710
epsilon nine so the two errors y minus the condition mean of my y given the fixed dex,

21
00:03:11,340 --> 00:03:18,720
this is also not observable. What for do we need?

22
00:03:19,680 --> 00:03:25,580
So we need to estimate us for beta naught and be drawn.

23
00:03:25,590 --> 00:03:31,079
So these are denoted by hat on on on on top of beta.

24
00:03:31,080 --> 00:03:39,330
Not to the so beta not pac and beta one hat are the estimated hours and these are computed using

25
00:03:39,390 --> 00:03:46,380
observables meaning work with a computer based on my data time based on the observed values of x1,

26
00:03:46,500 --> 00:04:04,110
x and y. Y hat is the estimated mean of y given a fixed value of x and plugging in to the

27
00:04:04,110 --> 00:04:10,689
expression here for y I have I can get as beaten up had plus beta one hat times six size.

28
00:04:10,690 --> 00:04:15,750
So these are the estimated means or fitted values.

29
00:04:16,470 --> 00:04:20,250
So why are these? These are observed.

30
00:04:21,660 --> 00:04:30,730
But why have. Our estimate did look good.

31
00:04:31,360 --> 00:04:34,689
And what is Epsilon? I had epsilon.

32
00:04:34,690 --> 00:04:39,580
I had this the observed y minus the estimated Y.

33
00:04:41,470 --> 00:04:51,640
So these are called estimated energies or in more popularly called residuals.

34
00:04:52,120 --> 00:04:56,380
So as I mentioned before, the two errors are not observed.

35
00:04:56,470 --> 00:05:10,330
These are not observed, but the estimated errors which are based on the the the observables.

36
00:05:11,530 --> 00:05:17,230
So these are these are estimated from your data.

37
00:05:17,240 --> 00:05:18,220
These are residuals.

38
00:05:18,910 --> 00:05:33,970
One notation that I would like I want to bring to your attention is that your textbook uses E eye to denote the estimated errors or the residuals.

39
00:05:34,450 --> 00:05:38,620
I prefer using the hack notation on top of Epsilon i.

40
00:05:38,860 --> 00:05:45,400
So I for using epsilon i that to emphasize that it is an estimate that it are so close to course

41
00:05:45,880 --> 00:05:53,980
I'm going to use epsilon I had instead of the e i in your textbook to denote the residual.

42
00:05:59,450 --> 00:06:13,430
Okay. So now here is to bring the sort of the bring the fold estimation in context.

43
00:06:14,480 --> 00:06:16,309
Consider a deterministic model.

44
00:06:16,310 --> 00:06:26,030
And, you know, like the most is the easiest one is to consider a model for temperature C is in degrees Celsius and F is in Fahrenheit.

45
00:06:26,450 --> 00:06:35,659
And I have data on CnF on on various days and let's say we are trying to feed this equation

46
00:06:35,660 --> 00:06:43,190
if I equal to B does not plus beta one times C and D I now if I give you data about two days

47
00:06:45,830 --> 00:06:51,170
on which the temperature in centigrade Boston and the temperature in Fahrenheit plus 50 and

48
00:06:51,170 --> 00:06:58,220
another day of the temperature in centigrade was 20 and the temperature in Fahrenheit is 68.

49
00:06:59,060 --> 00:07:04,580
If I ask you, can you determine whether or not and we one. Yes, no.

50
00:07:05,510 --> 00:07:09,380
If I give you data for two days, you can write.

51
00:07:09,560 --> 00:07:12,560
Because what? There are two unknowns. Better not be the one.

52
00:07:13,010 --> 00:07:18,740
You can write two equations using the data for these two days.

53
00:07:19,100 --> 00:07:26,989
So basically all you have to do is to solve the system of linear equations and you can obtain beaten up in beta one.

54
00:07:26,990 --> 00:07:34,850
Exactly. And in fact, like you can write the equation as because we all know the relationship between sending them in Fahrenheit from that,

55
00:07:34,850 --> 00:07:39,800
but you can actually plug in these values and solve the system of linear equations

56
00:07:40,070 --> 00:07:51,830
and you will get basically nine over five C equal to or three plus 32 is equal to F,

57
00:07:53,930 --> 00:07:58,730
so that would be the relationship between. CNF or in other words.

58
00:08:05,310 --> 00:08:10,620
Or in other words better not is 32 and beat them on his nine over five.

59
00:08:11,730 --> 00:08:12,390
Okay, good.

60
00:08:13,560 --> 00:08:24,930
So if I if I gave you the data for two days, you can just solve the system of linear regressions and you can determine better not than beta one.

61
00:08:25,680 --> 00:08:30,610
Now, let's consider a simple linear regression model where there is noise.

62
00:08:30,630 --> 00:08:35,080
There is this sort of calibrator component.

63
00:08:35,100 --> 00:08:40,230
However, do you want to see this Epsilon II guy sitting there?

64
00:08:41,970 --> 00:08:50,830
So if I give you a simple linear regression model, like better not let me go on its side is the main component.

65
00:08:50,850 --> 00:09:01,170
I also mentioned in, you know, a couple letters back that this is sometimes this is also referred to as the system at the bottom part of the model.

66
00:09:01,740 --> 00:09:06,590
Or we can think about this as the deterministic part of the model.

67
00:09:06,600 --> 00:09:15,840
This is more like your, you know, nine over five C plus 32 equals the F with basically nine over five.

68
00:09:16,140 --> 00:09:19,620
632 is the deterministic part of the model.

69
00:09:19,620 --> 00:09:23,160
But in that model, there is no error.

70
00:09:23,250 --> 00:09:34,680
There is no noise. That is no accident. So you know exactly the relationship here that epsilon I is sitting there and

71
00:09:35,220 --> 00:09:42,540
and actually sort of contributing noise than the matter to the relationship.

72
00:09:42,930 --> 00:09:46,079
So what is the effect of noise if there is no epsilon?

73
00:09:46,080 --> 00:09:53,970
Right. As you saw in the previous slide, we could use Amy to say y appears to estimate beta not in beta one and be done

74
00:09:54,780 --> 00:09:59,400
so for the sister of linear equations and be done like in the previous slide.

75
00:09:59,610 --> 00:10:05,520
But due to the presence of epsilon i that noise die.

76
00:10:05,850 --> 00:10:17,510
The estimation becomes more complicated because now there will be random aberrations around that beta naught plus beta one side.

77
00:10:19,380 --> 00:10:33,690
And that could throw me off. So I need a method to estimate the parameters which uses all data points or data points.

78
00:10:38,780 --> 00:10:43,590
Okay. So how do I do that? Everybody fine.

79
00:10:43,640 --> 00:10:52,040
Yes. I just this is a kind of a technical question based on the fact that you can pick one or two is are there any methods,

80
00:10:53,900 --> 00:10:58,129
are there any methods that aggregate these kind of pairwise slopes?

81
00:10:58,130 --> 00:11:05,510
And given that the results are. I'm not sure I think of you.

82
00:11:05,870 --> 00:11:10,070
So you you said, you know, if there was no voice, you could use.

83
00:11:10,310 --> 00:11:14,750
And two points. Yeah, but let's say their argument is still Israel.

84
00:11:15,130 --> 00:11:21,860
I mean I mean, we're going to do something else. But is there a way to, like, use pairwise calculations of slopes?

85
00:11:22,910 --> 00:11:32,990
Okay, so everybody for the question. So if there was no noise like in this thing, you look for an example like we know exactly,

86
00:11:33,380 --> 00:11:38,420
you know, you can eat one district line and be done if there is noise.

87
00:11:38,930 --> 00:11:42,860
The question is, can we let the data? I'm just making it up.

88
00:11:42,870 --> 00:11:45,920
There are six points.

89
00:11:46,610 --> 00:11:49,780
Then there are six to choose beers.

90
00:11:50,780 --> 00:11:56,600
Right. Or you actually like combinations is used less so.

91
00:11:58,040 --> 00:12:07,640
So again, you get a pretty steep climb to each of the possible beers and get the best line fit.

92
00:12:08,780 --> 00:12:18,660
The answer is no because there is still going to be like the I mean, if you behave like if you were saying, you can add.

93
00:12:20,340 --> 00:12:28,799
Aggregate results are similar? No, because that error component is still going to be kind of lurking there.

94
00:12:28,800 --> 00:12:36,060
And you will not get exact the the predicted points to align exactly on the straight line.

95
00:12:36,060 --> 00:12:44,969
Even if you can get that some point because of the random noise there will be you are going to

96
00:12:44,970 --> 00:12:55,920
be sort of off your your predicted value would be never on your observe your observed values.

97
00:12:55,920 --> 00:13:00,840
We never align exactly use the predictive values.

98
00:13:02,650 --> 00:13:06,070
Yeah. So? So the answer, like you don't even know.

99
00:13:08,600 --> 00:13:18,440
Okay. And that's that's basically how the, how the hatred kind of, um, you know, it messes up the, the, the.

100
00:13:19,790 --> 00:13:27,740
So you have to find a method that uses all the data points.

101
00:13:29,660 --> 00:13:36,080
Okay. So let's see how to do that. So I'm going to talk about a method of estimation.

102
00:13:37,100 --> 00:13:46,100
Um, and basically the idea is just to kind of bring home the point that, that I was trying to make with that.

103
00:13:46,100 --> 00:13:57,380
QUESTION So if the, if not for Ypsilanti, as I mentioned, we could use any 2xy beers to estimate the beaten up and beat go on.

104
00:13:57,830 --> 00:14:05,480
And the main point there is and if I do that still, all the points would align perfectly on the straight line.

105
00:14:06,410 --> 00:14:10,010
But if I use that approach that you mentioned, I could do that,

106
00:14:10,280 --> 00:14:17,330
but then all the points will not align perfectly on the straight line still because of that error.

107
00:14:18,260 --> 00:14:24,500
Okay. So let's let's go to the method of estimation.

108
00:14:25,100 --> 00:14:33,920
So here is the scatterplot of Y versus X, you know, some some data like you can think.

109
00:14:36,170 --> 00:14:45,469
So this is a hypothetical data of Y versus X. And so I have one, two, three, four, five, six, seven, eight, nine, ten data ones.

110
00:14:45,470 --> 00:14:50,960
This circles here denote my observed x y values.

111
00:14:51,590 --> 00:15:02,840
So now if you look at the scatter simplistically kind of coming back again to your question, I mean, I'm trying to use all the data points,

112
00:15:03,170 --> 00:15:16,550
but I would think about an infinite number of straight lines of the form B to not plus one excite the condition mean of y you give an x,

113
00:15:18,170 --> 00:15:25,280
I could think about infinite number of possibilities of straight line, say the blue line, the green line, the red line, and maybe others.

114
00:15:26,180 --> 00:15:31,550
And then these lines differ in their slopes and intercepts.

115
00:15:32,780 --> 00:15:37,940
So I could I could think about many, many possibilities.

116
00:15:38,750 --> 00:15:43,580
But the idea is all of these many possibilities.

117
00:15:44,180 --> 00:15:50,419
How do I find the line that best fits the data?

118
00:15:50,420 --> 00:15:53,630
Or in other words, the best describes the scatter?

119
00:15:55,370 --> 00:15:59,720
So I know I can I can fit many lines.

120
00:16:01,850 --> 00:16:11,990
And if I did the piecewise sort of approach and I get that could be another possibility of a line.

121
00:16:12,290 --> 00:16:16,250
But will that be the best line? That's it is null.

122
00:16:17,240 --> 00:16:27,200
So I need a method that selects the best version or the best line, or in other words, the best version of me to not have them be that one hat.

123
00:16:27,770 --> 00:16:32,120
How do I do that? And what what do I mean by best?

124
00:16:32,360 --> 00:16:47,750
That's another important question. So here is the method of estimation and it is called this is a very, very fundamental method in statistics.

125
00:16:47,990 --> 00:16:56,240
This is called the least squares method of estimation or some benefit to refer to as ordinary squares or this.

126
00:16:56,840 --> 00:17:12,139
So the idea is this. So again, let's take the, the, the scatterplot in the previous slide, the Y versus X and I have one,

127
00:17:12,140 --> 00:17:21,980
two, three, four, five, six, seven, eight, nine, ten points corresponding to those thin black dots.

128
00:17:22,550 --> 00:17:28,100
And see, for example, the the blue line here.

129
00:17:28,190 --> 00:17:32,960
So I live line here is my best line.

130
00:17:33,410 --> 00:17:45,080
I mean, I don't know how I find it yet. I tell you, the idea is this it's best by virtue of some of this.

131
00:17:45,680 --> 00:17:56,540
So the idea is I choose the line that minimizes the sum of squares of the vertical distances from the observed points to that line.

132
00:17:56,930 --> 00:18:04,760
So let's take an example. So let's take, for example, this point here.

133
00:18:08,030 --> 00:18:12,020
This point here. Here is an observer.

134
00:18:16,310 --> 00:18:20,360
One corresponding to X equal to roughly about seven.

135
00:18:20,960 --> 00:18:33,800
Okay. So the vertical distance from this observe that the Y to the blue line is given by this distance.

136
00:18:35,000 --> 00:18:46,580
Similarly, I have another point that is this y equal to roughly about two for two quarters.

137
00:18:46,760 --> 00:18:57,170
An x is six. And the vertical distance of this line from this point, from the bold blue line is this.

138
00:18:58,370 --> 00:19:02,840
And for each point, here is another point.

139
00:19:05,330 --> 00:19:16,190
The vertical distance of this point from the blue line is this It's not smaller compared to the previous two distances, but there is a distance.

140
00:19:16,190 --> 00:19:20,690
And this one here is very, very close to the blue line.

141
00:19:22,160 --> 00:19:27,710
And the vertical distance is this very small segment here.

142
00:19:28,460 --> 00:19:40,310
So each point, as you can see, do not fall exactly some good, but they do not fall on the bold line.

143
00:19:42,650 --> 00:19:48,680
So there's a vertical distance. And the idea is that you choose some.

144
00:19:49,490 --> 00:19:59,750
Let me point out, some distances are small, some distances are large, some are positive, some are negative.

145
00:20:01,100 --> 00:20:10,700
Okay. So I need to choose a line that minimizes the sum of squares of these vertical distances from the observed points to the line.

146
00:20:11,150 --> 00:20:18,799
So that's the criterion. So the method of least squares or the least puts estimate years of beta naught

147
00:20:18,800 --> 00:20:25,400
and beta one are the estimates which minimal minimize the sum of squares error.

148
00:20:25,940 --> 00:20:29,839
Why am I calling this some exclusive? What is this?

149
00:20:29,840 --> 00:20:33,040
Let's take this. This distance. What is this distance?

150
00:20:33,050 --> 00:20:40,760
This point? This gives me the observed y and what is this point here?

151
00:20:40,910 --> 00:20:48,049
It is the estimated Y based on this line.

152
00:20:48,050 --> 00:20:53,120
So this is according to my definition epsilon.

153
00:20:53,120 --> 00:21:00,710
I had the residual similarly for this find again,

154
00:21:02,060 --> 00:21:18,680
this is the epsilon that is it's a different point epsilon d hat corresponding to the Y being this value and the Y had been the value on the line.

155
00:21:19,340 --> 00:21:25,790
So I have epsilon one had epsilon do have epsilon three get up the epsilon ten hat for these data points.

156
00:21:26,660 --> 00:21:33,380
Epsilon I had the one that I denoted by by that point is positive.

157
00:21:35,060 --> 00:21:46,610
What about the Epsilon G hat that's going to be negative because the observed value is below the below the quote unquote difficult line.

158
00:21:48,170 --> 00:21:52,100
So some are going to be positive, some are going to be negatives.

159
00:21:52,850 --> 00:22:02,959
So why am I squaring the as the epsilon r is because exactly for the reason that some are going to be positive.

160
00:22:02,960 --> 00:22:10,610
Some are going to be negative. So I want to square them to get the distance of the of the points from the fitted

161
00:22:10,610 --> 00:22:21,050
line and then add them up that these what s this e denotes sum of squared error.

162
00:22:21,090 --> 00:22:31,700
So that is my as s e and my goal is to minimize that quantity.

163
00:22:33,950 --> 00:22:37,729
Does that make sense? Conceptually, it's it's very simple.

164
00:22:37,730 --> 00:22:44,690
The idea of all the metropolis squares is conceptually a very simple idea.

165
00:22:45,080 --> 00:22:49,000
And we all know that all brilliant ideas are infinitely simple.

166
00:22:49,910 --> 00:23:00,590
So. So here, like geometrically, what we're doing is we are saying that I want to minimize that sum of epsilon I have squared and which is

167
00:23:00,590 --> 00:23:09,110
basically the sum of squared residuals or than the sort of aggregate of the vertical distances of the,

168
00:23:09,110 --> 00:23:15,750
of the one from the free deadline. And so that's what we are going to do and the meaning.

169
00:23:15,860 --> 00:23:20,550
Mises says we will find the values of beta not happen beta one head.

170
00:23:20,560 --> 00:23:26,350
We'd solve the following system of equations. So this is an bringing idea from calculus.

171
00:23:26,530 --> 00:23:36,339
You're all familiar with this. So I take the affordability of SASE with respect to based on what can be done.

172
00:23:36,340 --> 00:23:42,880
One Haggard he quit goes to zero. So I have two equations I thought for both and I get best of it.

173
00:23:43,350 --> 00:23:56,320
The words for beat them all and beat the one. If you would be fine with that idea before I kind of do this math, I want you to get the concept first.

174
00:23:56,890 --> 00:24:02,110
And the concept is very simple geometrically when you think about it.

175
00:24:02,770 --> 00:24:07,239
Okay, so, so that so now that's what we are going to do.

176
00:24:07,240 --> 00:24:17,170
We are going to take this assessee big the first because with respect to data not pattern, we don't have any quick those to zero and solve for beta.

177
00:24:17,440 --> 00:24:31,690
So here are my first estimates. So first let me write what it says is and what is a sissy it's some remember it's a so it says he is.

178
00:24:38,920 --> 00:24:43,389
Okay. And I'm going to plan this scene as y minus y.

179
00:24:43,390 --> 00:24:47,780
I had square. Okay.

180
00:24:48,500 --> 00:25:01,580
And then y minus what is why you had better not had minus beta one had it so.

181
00:25:02,870 --> 00:25:06,630
So this is my estimate, right?

182
00:25:07,070 --> 00:25:11,480
So now I d hmm. I from one blend.

183
00:25:14,000 --> 00:25:18,440
So now I take the derivative of this quantity with respect to be on, not have.

184
00:25:19,190 --> 00:25:22,570
So I need with that to figure. So here is the first equation.

185
00:25:22,580 --> 00:25:27,409
So I have like negative two are two.

186
00:25:27,410 --> 00:25:40,680
Let's first go by steps two and then the derivative of this expression with respect to be done, pet y is constant drops off minus beta not had.

187
00:25:42,420 --> 00:25:46,310
So I get a minus from there, minus two.

188
00:25:47,270 --> 00:25:51,990
And then I had beta one hat times six.

189
00:25:52,010 --> 00:25:58,280
Right? So that again drops off because doesn't involve beta not.

190
00:25:58,640 --> 00:26:12,590
So I'm left with minus two summation y minus beta not had minus one having side and he quit that to zero to get the first equation right.

191
00:26:15,180 --> 00:26:18,990
So that's that's all I did. Okay.

192
00:26:19,020 --> 00:26:23,300
What about the second one? The second one with respect to the hat.

193
00:26:23,670 --> 00:26:26,790
So I have again, negative two.

194
00:26:27,390 --> 00:26:32,920
Negative for the same reason. Yes, that's. What we did.

195
00:26:32,940 --> 00:26:36,089
I did the work. I it into the negative too. Okay.

196
00:26:36,090 --> 00:26:40,470
So it's a pick up and it's good.

197
00:26:40,710 --> 00:26:50,970
So the decks of all of the debates of excess squared or minus exist.

198
00:26:51,210 --> 00:26:56,070
Let's see. So what is the x of minus x is where it's.

199
00:26:58,010 --> 00:27:03,640
Do it and then minus dimes, minus one for minus two x.

200
00:27:05,390 --> 00:27:12,550
Okay. So I just upload that. So the minus comes from the fact that within the expression you have a -15.

201
00:27:12,580 --> 00:27:16,850
Not that. So the minus starts from there. Okay.

202
00:27:17,540 --> 00:27:21,880
Any other questions? Okay.

203
00:27:22,300 --> 00:27:25,300
What about with respect to Beethoven hat?

204
00:27:26,230 --> 00:27:34,990
So again, the same thing. So I have a negative two and then I have also the expression y minus beta not

205
00:27:34,990 --> 00:27:40,570
had minus Beethoven had big side because it's a game of x squared is two x.

206
00:27:42,190 --> 00:27:52,329
But now I also have to take the derivative with respect to beta one half of this expression.

207
00:27:52,330 --> 00:28:04,420
So the y part drops of the beta, not bad drops off, but the beta one had the sidebar with the negative sign, which actually gets you the negative do.

208
00:28:05,290 --> 00:28:12,670
So when I take the derivative of that with respect to Beethoven hack, I have an inside also from that.

209
00:28:13,000 --> 00:28:19,059
So the second equation is negative to summation I from one to an x side y minus.

210
00:28:19,060 --> 00:28:24,130
We do not have mine as Beethoven had been six. I equate that to zero to solve.

211
00:28:25,690 --> 00:28:34,270
Okay, everybody. Everybody with me, I'm just applying like simple rule from calculus.

212
00:28:35,230 --> 00:28:39,400
Okay, so I have these two equations, one and two I'm going to solve.

213
00:28:40,510 --> 00:28:45,580
So from 1/1, let's look at one.

214
00:28:46,210 --> 00:28:57,850
So one is a minus two and I don't have to care about because on the right hand side or here on the left hand side in zero.

215
00:28:58,180 --> 00:29:12,200
So if I simplify this, so what I have summation Y minus in times B does not peg minus.

216
00:29:12,220 --> 00:29:15,700
We go on hard times.

217
00:29:16,000 --> 00:29:19,780
Summation X Y is equal to zero.

218
00:29:20,650 --> 00:29:24,430
This is what I have from the first equation.

219
00:29:25,600 --> 00:29:29,140
Yes. Okay.

220
00:29:29,320 --> 00:29:39,050
So I'm just going to write that here. I wish there was a beat that I could show this and.

221
00:29:39,460 --> 00:29:42,490
Okay. Here. So what is the mission? Why?

222
00:29:42,490 --> 00:29:46,150
I can write it as little in dimes. Why bother?

223
00:29:49,040 --> 00:29:57,829
Minus end times be done on pad minus Beethoven on hat dimes submission essay.

224
00:29:57,830 --> 00:30:03,650
I can write this in time. Six bar equal to zero.

225
00:30:08,510 --> 00:30:12,430
So big is better not had on the right hand side.

226
00:30:12,440 --> 00:30:20,570
So what do I have and then divide by in so I have we do not have is equal to y bar minus B one had time same spot.

227
00:30:24,020 --> 00:30:29,100
Yes. Okay.

228
00:30:29,430 --> 00:30:49,510
Just some simple algebra. Right to compete.

229
00:31:01,060 --> 00:31:08,320
Okay. So better not had these wiper miners with the one had them six by.

230
00:31:08,710 --> 00:31:20,740
Good. This is what I have from one. Now, what I'm going to do is substitute this beta notepad into the equation tool, which is here.

231
00:31:21,790 --> 00:31:28,329
So basically what I do is I dig that equation and I just plug in.

232
00:31:28,330 --> 00:31:32,500
Better not yet. So I have minus.

233
00:31:32,500 --> 00:31:36,120
I mean, again, don't worry about the minus two because it drops off.

234
00:31:36,130 --> 00:31:39,940
I have zero on the other side. So I have some mission.

235
00:31:39,940 --> 00:31:45,219
I from one to n, excite y minus B do not hurt.

236
00:31:45,220 --> 00:31:48,430
I just plug in that bad news for y minus y bar.

237
00:31:48,430 --> 00:31:55,130
Plus we don't have ex-parte. And then I also had from equation to minus video on her side.

238
00:31:55,720 --> 00:32:01,000
Right. So I just plug being data, not head into the second equation.

239
00:32:01,360 --> 00:32:05,530
And I get this. That's equal to zero.

240
00:32:06,430 --> 00:32:09,430
Well, good. All good. Yeah. So what?

241
00:32:09,430 --> 00:32:20,440
What, what? Why have not. So what? I now I'm sort of separating the terms, so therefore I have summation ixi y minus y bar from this fourth part here.

242
00:32:23,140 --> 00:32:33,129
Is equal to. I take the egg spot -60 times between had part on the right hand side.

243
00:32:33,130 --> 00:32:40,510
So I have on the right hand side. Beta one hand summation ixi x minus x bar.

244
00:32:40,510 --> 00:32:43,810
So I'm just separating the terms. That's what I did.

245
00:32:44,680 --> 00:32:50,740
Okay, now it's simple.

246
00:32:51,400 --> 00:32:58,990
So then I know what I do is I divide both sides by summation, except say minus X bar.

247
00:32:59,410 --> 00:33:09,490
And so what I have is beta one had is excess x y you what is this x when it says x, y,

248
00:33:09,820 --> 00:33:19,510
is this guy here summation excite y minus y bar and it says x is summation excite x say minus x bar.

249
00:33:23,010 --> 00:33:33,240
It says six only involves x, x, x, y, involves cross product between x and y.

250
00:33:36,840 --> 00:33:40,060
Okay. Everybody follow that.

251
00:33:41,010 --> 00:33:44,700
Those Olympic steps, right? Nothing. Nothing difficult.

252
00:33:45,000 --> 00:33:50,160
I'm just, you know, separating terms, collecting terms and doing things like that.

253
00:33:52,580 --> 00:33:55,970
Yes. Can I go forward, everybody, with me? Okay.

254
00:33:56,180 --> 00:33:59,660
So now let's talk about businesses X, Y and S6.

255
00:34:00,140 --> 00:34:06,200
This is this X minus six. These are interesting quantities.

256
00:34:06,710 --> 00:34:11,600
And I'm going to tell you to do this as a home exercise.

257
00:34:12,800 --> 00:34:24,860
Sure. That is this X, Y. This is how we noted in the previous slide that it's summation ixi y minus fiber.

258
00:34:26,630 --> 00:34:30,560
It's a fun two lines of algebra.

259
00:34:30,590 --> 00:34:39,590
Do it as a home exercise. You can show that an alternative form of physics y is where you take the same growth brought out.

260
00:34:39,590 --> 00:34:44,000
So X and Y and some of them are what I want to add.

261
00:34:44,060 --> 00:34:51,950
So in other words, it's summation y, A-minus Y, but it's say minus X, but it's very, very easy to show that.

262
00:34:52,250 --> 00:35:06,350
Okay. I want you to do it at home. And it says six is you can also in the previous slide, we noted, as X should say it say minus X,

263
00:35:06,350 --> 00:35:12,770
but you can also write it as some mission x sign minus x bar square.

264
00:35:15,620 --> 00:35:31,310
Okay. So again, just and although we are writing these expressions by just algebraic a simplification and collecting done.

265
00:35:32,840 --> 00:35:39,230
Please, please do it as a form exercise. You see how simple it is.

266
00:35:40,190 --> 00:35:48,740
And it will also help you understand the sort of the meaning of the significance of this stool does.

267
00:35:49,310 --> 00:35:53,090
So now let's peer at the alternative expression,

268
00:35:53,240 --> 00:36:04,270
for as this is of sex without going to the next two bullet points do not look familiar like this is X, Y and Z.

269
00:36:05,660 --> 00:36:13,930
Does it ring a bell? So the stakes were high.

270
00:36:14,410 --> 00:36:17,560
Remember, we talked about the stamp of obedience.

271
00:36:19,870 --> 00:36:23,709
So if you take one over in minus one, that is a six by.

272
00:36:23,710 --> 00:36:27,760
It's basically the sample obedience that we had to be finding modulate.

273
00:36:29,440 --> 00:36:38,200
And if you take this X and divide it by minus one, then it's essentially the sample variance.

274
00:36:38,530 --> 00:36:51,549
We would also define this in module. So the slope data, one hat, the slope estimate there is like the sample move,

275
00:36:51,550 --> 00:36:58,540
it is divided by the sample variance of X like of an expression like that.

276
00:37:00,790 --> 00:37:07,780
So that's a very, very interesting way of looking at the slope.

277
00:37:08,620 --> 00:37:14,050
So it's the sample covariance of x, y divided by the sample variance of X,

278
00:37:14,560 --> 00:37:26,260
and this is what the result or the expression or the of these squares estimated for beta one signifies.

279
00:37:28,810 --> 00:37:36,070
When you are doing this whole exercise, it's helpful and also for you know, and farther on.

280
00:37:36,310 --> 00:37:40,210
It's helpful to keep in mind that the sample centered variables is zero.

281
00:37:40,240 --> 00:37:46,809
So if you take some of y minus Y divided zero, if you take some of it same minus X, but it's equal to zero.

282
00:37:46,810 --> 00:38:00,220
And that's very easy to see by because some of the again, even some of its say minus everybody's summation inside minus summation in spot end times.

283
00:38:00,280 --> 00:38:06,010
So any spar and summation excite is equal to any spar.

284
00:38:06,020 --> 00:38:12,610
So this to do this drops off foods and x bar minus X bar.

285
00:38:12,730 --> 00:38:17,930
So you get a zero. Okay.

286
00:38:19,040 --> 00:38:34,340
So. So that's it's just X, Y and is a six.

287
00:38:35,210 --> 00:38:50,000
So now we are going to add and do a little bit of sort of reviewing this and also a bit of review to get into the proper estimation.

288
00:38:50,030 --> 00:38:53,150
Suppose we add in sample size pairs of data.

289
00:38:53,240 --> 00:39:01,100
So Y from one where it says our fixed values, y is our randomly selected from the population at a given its value,

290
00:39:01,400 --> 00:39:06,620
we assumed a linear relationship between y units given by y equals to be it or not.

291
00:39:06,620 --> 00:39:17,449
Plus, we don't excite the Epsilon nine where we assumed at this point that the Epsilon I's have mean zero and constant variance sigma squared.

292
00:39:17,450 --> 00:39:24,679
So they are coming from a C from the same underlying population that has a mean zero and a constant median sigma squared.

293
00:39:24,680 --> 00:39:27,770
But we have in addition what that distribution is.

294
00:39:28,280 --> 00:39:34,370
And also at the very least, we assume that the errors are uncoordinated.

295
00:39:35,120 --> 00:39:36,470
We don't need independence.

296
00:39:36,800 --> 00:39:46,460
So given these assumptions and given the linear overall linear relationship, we can estimate better not the one based on the data.

297
00:39:46,820 --> 00:39:52,850
And we found that the least squares estimate there is far better not and we don't want a given by y

298
00:39:52,850 --> 00:39:59,659
bar minus beta one had expr and for me w on the least worse estimate is explaining what is a six.

299
00:39:59,660 --> 00:40:03,560
This is what we derived using the method of the squares.

300
00:40:04,910 --> 00:40:16,190
Now that the expression sub for beta not happen beta one have given these values depend on the

301
00:40:16,190 --> 00:40:22,880
particular random sample drawn from the populations of what in the expression for beta notepad,

302
00:40:23,360 --> 00:40:27,470
what is fixed and what is end of it is what is fixed, right?

303
00:40:28,160 --> 00:40:37,430
But why but is random and they don't happen is random because these are derived from the or estimated from the particular random sample.

304
00:40:38,030 --> 00:40:42,409
So these quantities are considered random variable.

305
00:40:42,410 --> 00:40:48,440
Similarly with this x with beta one hat is this x is fixed, but it is x.

306
00:40:48,440 --> 00:41:00,890
Y is not is this x, y is random because item y in there which basically has a is coming from like has a has a random component.

307
00:41:00,920 --> 00:41:12,350
The Epsilon is there, so it has a distribution. So the idea is that once we plugging observe data, we have estimates.

308
00:41:12,560 --> 00:41:16,190
So the estimate is I'm going to make this distinction.

309
00:41:16,190 --> 00:41:21,410
I don't know if you have gotten into this in 6 to 1 already,

310
00:41:21,650 --> 00:41:33,780
but estimate there is the expression for V does not have an beat that one had based on the observables estimate is

311
00:41:33,780 --> 00:41:46,339
given the date that head you can plug in numbers in those expressions and get a numerical value of B does not happen.

312
00:41:46,340 --> 00:41:59,510
We don't want to have so do that estimate based on your data at that you would collect from the same population a random sample and I need to do this.

313
00:42:00,320 --> 00:42:07,490
You compute your estimates. Somebody is using phthalates and other than another sample, not the same one.

314
00:42:08,360 --> 00:42:20,569
Then they will get different estimates because different thing like the value of the samples are different, but the estimates are these expressions.

315
00:42:20,570 --> 00:42:25,190
Okay, so bottom line is this quantities are considered random.

316
00:42:26,450 --> 00:42:30,350
So what, what is the implication then?

317
00:42:30,890 --> 00:42:36,110
A typical goal of the analysis is to inform, draw inference on the values of beta,

318
00:42:36,110 --> 00:42:44,239
not beta one supported by the data as well as testing particular values of beta not be the one to consistent with the beta.

319
00:42:44,240 --> 00:42:51,590
So we are going to kind of do both inference as well as hypothesis testing and to draw inference.

320
00:42:53,420 --> 00:43:06,010
So either get interval estimates, estimates of, you know, variation of of B do not have we don't have a precision construct,

321
00:43:06,020 --> 00:43:11,839
tests, etc. We need to know the sampling distribution of the estimates we do not have.

322
00:43:11,840 --> 00:43:18,770
We don't want hat. So the kind of questions we are interested in addressing is what are the average values of

323
00:43:18,770 --> 00:43:24,379
beta not happen beta one had if we were able to take repeated samples of the population.

324
00:43:24,380 --> 00:43:28,190
So I have 20 students in this section i.

325
00:43:28,410 --> 00:43:38,860
And each of you and I say, okay, here is like, let's say we are trying to study some aspect of biology media.

326
00:43:39,780 --> 00:43:43,750
I'm a student in the university, two armies.

327
00:43:44,010 --> 00:43:51,600
Let's some something like that. But I send each of you to collect the sample from that population.

328
00:43:52,320 --> 00:44:00,240
So you collect that. It's a random sample. And let's I tell you, like each of you get a sample of, say, 50.

329
00:44:00,570 --> 00:44:06,450
So you draw a sample of, say, 50, you draw a sample of say 50, you draw a sample of to.

330
00:44:06,540 --> 00:44:12,870
Everybody has the sample right from the same population.

331
00:44:13,830 --> 00:44:22,140
And you can put it to me and and a variance based on your temple and let's

332
00:44:22,170 --> 00:44:26,160
be a try and I'm going to make it even more aligned with the dollar problem.

333
00:44:26,490 --> 00:44:43,830
So let's say from that population, we are trying to sort of study the association between GPA in freshman year versus then

334
00:44:43,920 --> 00:44:53,970
I can see that an association between IQ and freshman year GPA or so like you calculate,

335
00:44:54,340 --> 00:45:03,070
use the method of least person. You know, we do not that could happen we don't had for that median relationship based on the incident

336
00:45:03,870 --> 00:45:09,509
and each of you do that do you think you will get the exact same values of we do not happen.

337
00:45:09,510 --> 00:45:12,989
We don't want to have that.

338
00:45:12,990 --> 00:45:17,760
There is no because look, each of you have a different sample.

339
00:45:18,690 --> 00:45:24,050
So I want you to get this very, very clear.

340
00:45:24,060 --> 00:45:29,500
So when we are talking about the sampling estimated of these estimations.

341
00:45:29,850 --> 00:45:35,190
So sampling, distribution of this estimate, those people are part and beta one part.

342
00:45:35,550 --> 00:45:41,430
We are relying on that repeated sampling from the same population.

343
00:45:42,000 --> 00:45:54,690
And he fite collected the 28 peers like there are 20 you know students enrolled in the section and I send each of you who will do this study.

344
00:45:54,870 --> 00:46:00,900
So if I collect the 28 peer so we do not have and beta one hat from this class.

345
00:46:01,530 --> 00:46:12,629
Then I did the sampling distribution of the beaten path and beta one head because now I can kind of construct a histogram of the beta,

346
00:46:12,630 --> 00:46:20,070
not have it be the one that values. And I can talk about the average values of those 28 beers.

347
00:46:20,460 --> 00:46:29,190
I can talk about the variation in the values of beta not had beta one had from one random sample to another,

348
00:46:29,340 --> 00:46:32,340
from your sample to your sample and so on.

349
00:46:32,730 --> 00:46:36,240
So that's what I mean by the sampling distribution.

350
00:46:36,540 --> 00:46:47,969
And given the model assumptions, it turns out that we can derive the expected value of this beta now pack and we don't want that.

351
00:46:47,970 --> 00:46:51,630
And the variance of this beta not having beta one heads.

352
00:46:52,920 --> 00:46:59,340
And it turns out that these are the expressions we will we will prove this.

353
00:47:00,060 --> 00:47:03,060
But I want you to get this first.

354
00:47:03,060 --> 00:47:08,400
Conceptually, what does the sampling distribution mean and what does it mean to talk about

355
00:47:08,400 --> 00:47:14,070
the average values of these estimates and the variation in these estimates?

356
00:47:15,720 --> 00:47:20,730
So here is kind of a pictorial representation of the same concept.

357
00:47:21,420 --> 00:47:26,700
Since expected value of beta not had equal to beta, one had an expected value of beta naught.

358
00:47:26,700 --> 00:47:31,350
One had this beta one. I have included it. I'll prove it.

359
00:47:32,070 --> 00:47:39,960
So we know that the estimated hours will give estimates that are centered at the true values.

360
00:47:40,860 --> 00:47:48,630
So on an average, I will get the target, you know.

361
00:47:51,400 --> 00:47:57,639
So if we don't want him to be the one. And if I did need a histogram of the observations of beta one.

362
00:47:57,640 --> 00:48:04,390
Huh? When we dig beneath repeated samples from the population, then.

363
00:48:07,420 --> 00:48:18,760
Then we are going to get a histogram like this on the on the left hand side, which is centered at the true unknown value beta one.

364
00:48:19,330 --> 00:48:26,860
However, we have only one particular data set, so only one particular sample,

365
00:48:27,370 --> 00:48:38,649
and we are only able to observe one value of the slope or one value of the intercept, not the whole distribution of the slope and intercept.

366
00:48:38,650 --> 00:48:44,260
So I would only be able to see that if I did repeated sampling from the from the same population.

367
00:48:44,260 --> 00:49:00,010
So conceptually, it's very important to get this and then, you know, sort of derive the formulas for variance of these discourse estimates.

368
00:49:00,250 --> 00:49:07,660
So variance of beta knockout meetings of beta one that tell us how much spread there is in this histogram.

369
00:49:09,550 --> 00:49:15,070
And that's the key. That's the key of the sampling distribution.

370
00:49:15,490 --> 00:49:18,490
We will derive the formulas for variance.

371
00:49:18,490 --> 00:49:26,890
We do not have an readings we don't have, but it turns out that these formulas will also involve Sigma Square.

372
00:49:27,400 --> 00:49:33,300
And remember what Sigma squared is. Sigma squared is the variance of the errors to errors.

373
00:49:34,480 --> 00:49:38,470
That's what we had defined. Yes. I'm coming to you in a minute.

374
00:49:38,980 --> 00:49:45,220
So so we don't have to all to estimate the sigma squared before we can draw inference.

375
00:49:46,240 --> 00:49:50,080
And we will do this by plugging in an estimate of sigma squared.

376
00:49:51,280 --> 00:49:54,370
So how do we get an estimate of sigma squared?

377
00:49:54,760 --> 00:49:57,100
That is also something that's coming up.

378
00:49:57,280 --> 00:50:04,390
So the bottom line is, in this in this sort of conceptual slides, I wanted to bring home the idea of sampling, distribution of bitterness,

379
00:50:04,750 --> 00:50:15,310
pardon me, the one hand, and then derive the expressions for the expected values and the variance of the least squares, estimated hours.

380
00:50:15,820 --> 00:50:24,610
And also, you know, kind of establish that the variance of the square meters really involve Sigma Square,

381
00:50:24,850 --> 00:50:29,410
which is another unknown parameter because it's the variance of the two errors.

382
00:50:29,710 --> 00:50:34,120
So I will also need to estimate that sigma squared.

383
00:50:34,810 --> 00:50:39,100
So I'm going to talk about all of these things.

384
00:50:39,280 --> 00:50:43,210
So, yes, no question. What's the y axis?

385
00:50:45,440 --> 00:50:49,790
What's the y axis on this graph? This is a histogram of this.

386
00:50:49,790 --> 00:51:00,110
I'm plotting the sort of what? So conceptually what you have each of these are, you know, a set of bars denote.

387
00:51:00,260 --> 00:51:05,329
So if I have 28 sets of a set of, you know,

388
00:51:05,330 --> 00:51:17,930
like my beta one half corresponding to each of the random sample for the 20 w I'm constructing the histogram based on those beta one had values,

389
00:51:18,320 --> 00:51:22,190
so there's no y axes. But I'm counting the frequency set.

390
00:51:22,520 --> 00:51:28,460
You get five, you get five, another person gets three.

391
00:51:28,760 --> 00:51:39,379
So of the 28 beta one had values from the data from the class that are then five that are five threes and so on.

392
00:51:39,380 --> 00:51:42,590
So that's what the how of the histogram, you know.

393
00:51:44,450 --> 00:51:49,250
Okay, any questions. No.

394
00:51:49,970 --> 00:51:57,710
Okay. So now we are going to go to the properties of the least squares, estimated hours.

395
00:51:58,280 --> 00:52:06,590
It's 856. So let me just mention this briefly and then we go for a break.

396
00:52:07,160 --> 00:52:15,710
So just a few definitions before we go to prove these properties.

397
00:52:17,480 --> 00:52:26,220
I suppose payback is an estimated of Capetown. So to hack, as I mentioned, is random.

398
00:52:26,240 --> 00:52:33,590
It's depen depends on random sample of data and that is the estimated estimate.

399
00:52:33,590 --> 00:52:37,910
And this is fixed generally unknown to value.

400
00:52:37,920 --> 00:52:42,350
So that is the true unknown and that is the estimate there.

401
00:52:43,160 --> 00:52:46,610
There are various criteria that it used to evaluate estimates.

402
00:52:46,910 --> 00:52:50,900
Two very important concepts which you'll get into in 6 to 1.

403
00:52:51,500 --> 00:52:59,210
You haven't seen this yet in 6 to 1, right? Yeah. So that this is always kind of a little bit of at this point, disconnect.

404
00:53:00,080 --> 00:53:03,200
But you will talk about all of these things in 6 to 1.

405
00:53:03,380 --> 00:53:11,450
I have to use them. So I define the very basic and the minimal in terms of these quantities.

406
00:53:11,750 --> 00:53:15,650
And then I carry forward but you see in more detail in 6 to 1.

407
00:53:16,130 --> 00:53:25,030
So bias, this is one important concept. What is the bias of of an estimate there get ahead.

408
00:53:25,400 --> 00:53:39,440
It's it's the basically the expected value of theta, heck minus theta, which is the true unknown value or the true unknown parameter.

409
00:53:39,650 --> 00:53:48,800
So we seem to have this an unbiased estimator of paper up if the expected value of Theta Hat is equal to picked up.

410
00:53:49,970 --> 00:53:53,200
Sampling variance is a rather important concept.

411
00:53:53,210 --> 00:53:59,930
So variance of take that is, you know, you'd be fine the same way as would define a median.

412
00:53:59,930 --> 00:54:11,260
So it's the expected value of picked ahead minus the expected value of minus the expectation of payback squared.

413
00:54:11,300 --> 00:54:24,379
Actually, it's the expected value of the deviation of take that had from its mean squared mean squared either this is another important concept

414
00:54:24,380 --> 00:54:33,290
and means greater that is basically the sum of variance and squared of the bias so big is had ahead less bias picked ahead squared.

415
00:54:33,710 --> 00:54:41,120
Often there is a tradeoff between bias and sampling variance, which is measured by the MSE so mean squared.

416
00:54:41,120 --> 00:54:49,570
And it is denoted by. So you think sort of this concept, this definition,

417
00:54:49,670 --> 00:54:57,649
I should say our goal in the next few slides will be defined in the context of this and are expected

418
00:54:57,650 --> 00:55:03,430
value be done on cap expected value of beta one have been of we do not have evidence of beta one here.

419
00:55:03,800 --> 00:55:19,670
So our goal in the next few slides will be define the expectation and the variance of the with estimate dose is to make the complete the hat here.

420
00:55:20,030 --> 00:55:30,259
Basically, you can think about that in our plan because I'm interested in beta and we don't have to estimate the cost of the

421
00:55:30,260 --> 00:55:41,360
honeymoon beta and beta one and I would apply these definitions and find the expectation variance of these estimates.

422
00:55:41,960 --> 00:55:47,030
Okay. So we will take a break and we will be back at 910.

423
00:55:55,420 --> 00:56:01,210
We have to keep asking about. Yeah, it's just.

424
00:56:02,730 --> 00:56:06,640
Yeah, the time has come. Okay, let me just.

425
00:56:10,206 --> 00:56:18,676
Okay. So. Everybody but.

426
00:56:20,456 --> 00:56:32,406
Okay. So now to establish or to derive the expected expected value and the variance of this estimates,

427
00:56:32,786 --> 00:56:46,126
I want to remind you about the module, the expected value of some sort of random variables that we had reviewed in module.

428
00:56:46,586 --> 00:56:54,985
So we wanted to aim be constants. Recall that expected value of a y is equal to expected value y.

429
00:56:54,986 --> 00:57:01,706
So if it's if A's are constants, then you can sort of get the A's outside the expectation.

430
00:57:03,506 --> 00:57:10,376
And also the expected value of some of the random variables is the sum of the expected value.

431
00:57:10,386 --> 00:57:21,746
So you can interchange the summation and the expected the expectation and note that this result does not require the independence among the y's.

432
00:57:21,746 --> 00:57:26,216
So nothing, no independence assumption is needed.

433
00:57:27,116 --> 00:57:34,496
So here is the claim that I'm going to make and then I'm going to prove this result.

434
00:57:34,496 --> 00:57:45,205
So I claim that the least squares estimated beta one had is an unbiased estimated of beta one and remember three slides down.

435
00:57:45,206 --> 00:57:51,716
We defined what unbiased means on board that they're not estimated is unbiased.

436
00:57:51,716 --> 00:57:57,295
If any expect is shown, it is equal to the true unknown barometer.

437
00:57:57,296 --> 00:58:02,816
So I have to show that expected value of beta one hat is equal to beta one.

438
00:58:03,326 --> 00:58:11,606
So here is the proof. Oh.

439
00:58:11,876 --> 00:58:16,206
How come my proof is not showing up? Oh, here.

440
00:58:19,656 --> 00:58:33,336
Okay. So here is the proof. So far as the expected value of beta one had I by the job I did I believe it was estimated of beta one.

441
00:58:33,786 --> 00:58:39,216
So beta one had been replaced by its physics value about SSX.

442
00:58:39,216 --> 00:58:43,936
So this is the biodiversity method of bitcoin.

443
00:58:45,066 --> 00:58:48,816
And so I have to take the expectation of this.

444
00:58:49,116 --> 00:58:57,576
Remember that exists fixed. That is nothing random about it so it's a six in the denominator is you can think of it more like a

445
00:58:57,576 --> 00:59:06,426
constant so I can get it outside the expectation sign so I can write it as one over SSX or in other words,

446
00:59:06,426 --> 00:59:19,716
it's a six to the power negative one multiplied by expected value of this X-Y because that is the only random mark in the expression for beta one hat.

447
00:59:20,526 --> 00:59:32,466
Okay, so and so it's equal to again one of our associates expected value of now I break up and write the expression for ssx y.

448
00:59:32,466 --> 00:59:38,436
So is this x y is some of y equal to one when y you say minus x bar?

449
00:59:39,726 --> 00:59:43,446
Yes. So it's like a cross product done.

450
00:59:46,436 --> 00:59:58,176
Now I can apply the result in the previous slide, but if you have a sum of random variables,

451
00:59:58,176 --> 01:00:02,765
then expected value of the sum is equal to some of the expectation.

452
01:00:02,766 --> 01:00:06,726
So you can interchange the expectation and the summation side.

453
01:00:06,996 --> 01:00:19,026
And that's exactly what I do here. So here from here I basically take the expectation sign inside the summation.

454
01:00:19,506 --> 01:00:31,926
So I have a six to the power negative one summation I from one to in sum of expected y gain six sine minus x bar.

455
01:00:31,926 --> 01:00:39,366
And remember once again it's say minus x where speak. So it's like a constant so I can bring it outside of the expectation side.

456
01:00:39,606 --> 01:00:51,906
So I have x sign minus x bar times summation at 40 x minus x bar times expected value of y big the sum of terms like that from I from one to N.

457
01:00:53,796 --> 01:00:57,336
Yes. So from here to here.

458
01:00:57,666 --> 01:01:08,346
Then again, the carry forward the one over six summation I from one to an x sign minus x bar,

459
01:01:08,646 --> 01:01:19,656
but now expected y expand y expected y is what given the fixed x based on my regression line if we do not work with them on each side.

460
01:01:22,486 --> 01:01:28,786
Right. So I just replace that with this expression.

461
01:01:30,196 --> 01:01:36,886
So now from here to here, all I do is I expand.

462
01:01:38,026 --> 01:01:48,196
So I have one. What is this X times within this large bracket, square bracket.

463
01:01:48,496 --> 01:01:59,446
I have beta, not dime's summation X9 minus x bar plus beta one times summation ixi times, ixi minus x bar.

464
01:01:59,836 --> 01:02:07,786
So I'm just expanding terms. Okay, so what is this quantity here?

465
01:02:07,816 --> 01:02:10,876
This is like ssx by definition.

466
01:02:12,826 --> 01:02:18,976
Yes. And what is this here? That zero.

467
01:02:19,366 --> 01:02:24,496
Right. Because I'm taking the sum of the centered X's.

468
01:02:24,646 --> 01:02:31,785
So that zero. So the first one drops off. I'm only left with ssx inverse times.

469
01:02:31,786 --> 01:02:36,855
Beta one is this x. So the ssx inverse and this is x cancels off.

470
01:02:36,856 --> 01:02:44,296
I'm left with bitumen. So the x, what I, I've proved is that the expected value of Beethoven had is equal to Beethoven.

471
01:02:46,756 --> 01:02:53,986
Okay. Now no expected value of beta not hacked.

472
01:02:54,856 --> 01:02:58,846
So again, I claim that beta not had is an unbiased estimation of bitcoin.

473
01:02:58,846 --> 01:03:03,225
Ah. So I have to prove that expected value of beta not pac is equal to beta.

474
01:03:03,226 --> 01:03:09,555
Not so here is the proof. What is beta not hacked?

475
01:03:09,556 --> 01:03:15,106
The list was estimated it's given by y bar miners beta one hacking bar.

476
01:03:15,136 --> 01:03:18,646
I have derived this so I could take expectation of this guy.

477
01:03:20,836 --> 01:03:34,186
So once again applying formulas for expectation so I can write it says expected value of y bar minus expected value of beta one had time same bar.

478
01:03:36,166 --> 01:03:41,535
Okay, so in this expression x five is fixed.

479
01:03:41,536 --> 01:03:49,546
So this it's like on the stand. C Okay, so now from here to here, what do I have?

480
01:03:50,506 --> 01:03:53,686
I have expected well, what is y bar?

481
01:03:53,746 --> 01:03:57,466
Y bar is the sample mean monitoring summation y?

482
01:04:00,966 --> 01:04:10,576
Okay. So I have expected value of y, but is one of more than expected value of summation y.

483
01:04:10,596 --> 01:04:19,846
Again, I can interchange the summation and expectations. So what I have is one regarding summation I from one to an expected value of y.

484
01:04:19,846 --> 01:04:24,365
Yeah. So this is the first term and the second term is still expected. Value of beta one hat band.

485
01:04:24,366 --> 01:04:29,406
Six bar. I don't do anything with that kid.

486
01:04:32,736 --> 01:04:45,176
So now I use the model definition and I can write this first one here as 1 to 1 and some ish and I from one to an expected value of what is why,

487
01:04:45,186 --> 01:04:48,216
according to the SLR model is better, not because we don't excite.

488
01:04:48,216 --> 01:04:55,776
Plus it's selenite. So I've just sort of written that expression of the model and what is the second term.

489
01:04:56,256 --> 01:05:02,315
So expert is like a constant, correct? So I take expert out of the expectation side.

490
01:05:02,316 --> 01:05:05,676
So I have export times expected value of beta one hat.

491
01:05:06,546 --> 01:05:11,736
But just in the previous slide I prove that Biederman had is an unbiased estimation of Beethoven.

492
01:05:12,756 --> 01:05:15,966
So what is the expected value of Beethoven had its Beethoven.

493
01:05:16,236 --> 01:05:23,226
So I the second term reduces to Beethoven time so far from here to here.

494
01:05:27,136 --> 01:05:37,816
Okay. So now what with the first arm one opening expected value of be off it's a constant

495
01:05:37,816 --> 01:05:42,435
so I have bitten off expected value of between each site again it's a constant.

496
01:05:42,436 --> 01:05:45,976
So between each site. What is the expected value of epsilon i?

497
01:05:47,026 --> 01:05:53,536
My assumption is zero. So the drops off and I have minus beta one expert from the second term.

498
01:05:54,916 --> 01:05:57,796
So now what happens to this guy here.

499
01:05:58,306 --> 01:06:18,736
So I this guy is one of our N times in between our last video on dime's expired sorry plus beta one not be a beta one times and in summation site.

500
01:06:24,446 --> 01:06:27,896
And then the second term is they don't want to inspire. Okay.

501
01:06:28,166 --> 01:06:33,516
So I have beaten up plus Beethoven expire minus Beethoven expire.

502
01:06:33,536 --> 01:06:38,336
So the plus and the minus with domain names, Bach again soaks up and left feel better paternal.

503
01:06:41,346 --> 01:06:48,946
Good. So I want to prove that we do not hap is an unbiased estimator of we do not.

504
01:06:49,876 --> 01:06:56,156
Any questions? Okay.

505
01:06:57,656 --> 01:07:00,146
Now the variances they have to compute.

506
01:07:00,626 --> 01:07:11,906
So once again, I want to remind you about the variance of the sums of random variables that we, um, we looked at in module.

507
01:07:13,796 --> 01:07:22,945
So we wanted to let those be constants, then recall that the variance of a y is a squared variance of the poor variance

508
01:07:22,946 --> 01:07:27,596
of x with the y is weakening and being the constant outside the full variance.

509
01:07:27,596 --> 01:07:33,656
It's eight times for variance x, y, and then the variance of the sum of random variables,

510
01:07:33,716 --> 01:07:48,386
i.e. y can be written as the sum of the variances plus two times this sum of the audiences feared.

511
01:07:49,616 --> 01:07:55,256
In particular, if I had variance of e1y1 plus eight y2,

512
01:07:55,826 --> 01:08:08,906
then I can write it as iran squared variance y one e2 square variance y two plus two times a1 two times the for Williams between yny1 and y2.

513
01:08:10,736 --> 01:08:17,546
So that's the general formula for variances off for variance of sums of random variables.

514
01:08:17,846 --> 01:08:23,216
What happens if the Y is a uncorrelated, then the covidien stance drop off.

515
01:08:24,296 --> 01:08:29,276
So the then what I'm left with and it becomes easier.

516
01:08:29,636 --> 01:08:38,836
Variance of the sum of uncoordinated random variables.

517
01:08:38,846 --> 01:08:43,616
Y can be written as the sum of the variances.

518
01:08:45,386 --> 01:08:52,785
Okay, so I'm going to use this result, so forth.

519
01:08:52,786 --> 01:08:58,616
Then going to drive the variance of beta one half.

520
01:08:58,736 --> 01:09:05,936
And I'm claiming that the variance of beta one hat is sigma squared divided by x.

521
01:09:06,266 --> 01:09:11,576
So here is the proof. So what is beta?

522
01:09:11,576 --> 01:09:13,195
One happens again. It is this.

523
01:09:13,196 --> 01:09:24,656
Explain what is the things I need to compute the variance of this we call again the x is fixed so is this x is like a constant and using that

524
01:09:24,656 --> 01:09:34,735
formula variance of four constant times the random variable is equal to the constant squared times the variance of the random variable.

525
01:09:34,736 --> 01:09:38,486
Here the constant is one over six. So what do I have?

526
01:09:38,516 --> 01:09:47,426
I have one of what is this, x squared or in other words, it's a six to the bar negative two times the variance of six y.

527
01:09:49,376 --> 01:09:49,766
Yes.

528
01:09:50,306 --> 01:10:06,355
So I get even the forward I have one of what is this x squared variance of I guess a six fi assumption white sine minus x bar and and exam minus x,

529
01:10:06,356 --> 01:10:18,626
but is my like my constant. So I can then from here to here I can using again using the sort of the result from the previous slide.

530
01:10:18,986 --> 01:10:28,135
I can write it as one over as a squared. Some of the ease squared right here is ixi minus x bar.

531
01:10:28,136 --> 01:10:33,026
So exam minus x bar square times the variance of y.

532
01:10:37,266 --> 01:10:40,806
Right. Okay. What is radiance of why?

533
01:10:43,986 --> 01:10:50,886
There will be two variants of the be variants of the error terms, which is sigma squared.

534
01:10:53,316 --> 01:11:02,856
It's fixed. Remember, we assume that that the variance of the errors do not depend on the values of X.

535
01:11:03,306 --> 01:11:08,406
It's a balanced and sigma squared. I mentioned that part is spoiler.

536
01:11:08,556 --> 01:11:13,836
Most kids this the assumption. So it's variance of y is sigma squared.

537
01:11:14,706 --> 01:11:19,775
So basically I have summation exam minus x y squared, times sigma squared.

538
01:11:19,776 --> 01:11:23,196
I bring that sigma spread outside the summation sign.

539
01:11:24,186 --> 01:11:27,606
And what in summation ixi minus x y squared.

540
01:11:32,916 --> 01:11:47,146
So. What is this?

541
01:11:48,676 --> 01:11:53,086
This is a physics, right?

542
01:11:53,776 --> 01:11:57,976
So I have a six divided by six squared times sigma squared.

543
01:11:58,576 --> 01:12:06,946
So what do I get? I get sigma squared divided by six, which is the variance of beta one half.

544
01:12:10,036 --> 01:12:20,416
Okay. What is building so big does not have so the least squares estimated for this intercept.

545
01:12:22,086 --> 01:12:30,676
I this is a slightly more complicated expression than the variance of the square estimated for the slope.

546
01:12:31,936 --> 01:12:38,356
So I claim the variance of B do not have a signal square one of the six plus one divided by six.

547
01:12:38,356 --> 01:12:44,626
So let's see. How do we get that? What is beta not care?

548
01:12:44,656 --> 01:12:49,996
What is the square estimated for the intercept its y bar minus Beethoven had dates bar.

549
01:12:51,496 --> 01:12:55,036
So I write it as variants of that quantity.

550
01:12:56,026 --> 01:13:03,616
X bar once again is fixed. It's like a constant. The random components are vibrant bit act in this expression.

551
01:13:04,036 --> 01:13:09,166
So I use the obedience formula like two slides back the results.

552
01:13:09,166 --> 01:13:21,466
So I d does variance y bar plus x bar square variance beta one hat minus two times x bar already in supply bar and veto on her.

553
01:13:21,886 --> 01:13:28,096
So I'm just using this result from the variance formula here.

554
01:13:30,906 --> 01:13:34,816
Right. Those are six. Okay.

555
01:13:35,226 --> 01:13:44,426
So. So I wrote the expression for the for the variants.

556
01:13:44,966 --> 01:13:52,016
Now in the next slide, I will show that this Pavilion's term here is zero.

557
01:13:53,666 --> 01:14:06,416
I will show that in the next slide. So if that is zero, then what does the billions of people not it reduces to its variants of y bar.

558
01:14:07,076 --> 01:14:20,696
What is y bar? Y bar is once again summation one or more in summation y each of these ys have variance sigma square.

559
01:14:21,476 --> 01:14:43,025
So what is variance of y bar? It's one of our ends we're dives some of season not quit in dimes so in Sigma Square so I have the meanings

560
01:14:43,026 --> 01:14:50,376
of five three sigma squared over in one end from the numerator cancels out with one end in the denominator.

561
01:14:50,376 --> 01:14:55,286
And what is the the second term means by squared variance of bitcoin hack.

562
01:14:55,296 --> 01:15:02,646
So I have a bar split and I have a rule in the previous slide that the variance of Beethoven had two Sigma Square viruses.

563
01:15:02,646 --> 01:15:07,536
Six. So I just plug that game and I'm done.

564
01:15:08,826 --> 01:15:17,765
I have shown that the variance of beta not had is this expression, but I have to ensure that this full variance is zero.

565
01:15:17,766 --> 01:15:21,396
So let's go ahead and show that this obedience is zero.

566
01:15:22,926 --> 01:15:27,606
So here is my claim that the Corbet is between Weimer and between heck is zero.

567
01:15:29,286 --> 01:15:32,676
So I doubt the expression for beta one half.

568
01:15:32,796 --> 01:15:39,716
So I have obedience of y bar. And remember bitcoin had a six virus versus six.

569
01:15:39,726 --> 01:15:44,856
So it's a six. Why I write a summation sign minus x by the way,

570
01:15:45,216 --> 01:15:57,125
you know I have defined as a6y in the expression for the least squares estimated for beta one, and it's the denominator is a six.

571
01:15:57,126 --> 01:16:06,576
So is this x once again is like a constant. I can give it outside of the summation and the whole variance.

572
01:16:06,816 --> 01:16:07,426
So I have one.

573
01:16:07,456 --> 01:16:21,306
What is a six some ixi minus x bar obedience y bar with y i can I that y cannot either because once again x minus x bar is like a constant.

574
01:16:21,696 --> 01:16:28,716
The only random quantities in this expression are the y bar and y.

575
01:16:31,456 --> 01:16:37,156
So that all the confidence will come from their X minus x where it's like a constant.

576
01:16:37,186 --> 01:16:41,206
So I had one of what is a six hour machine X minus y bar,

577
01:16:41,596 --> 01:16:50,986
and I have dance like obedience of y by reply one obedience of y bar reply to obedience of y bar with y three and so on.

578
01:16:53,326 --> 01:16:57,346
Right, everybody with me.

579
01:16:57,616 --> 01:17:00,826
So let's take one, one of those terms.

580
01:17:01,696 --> 01:17:06,556
So let's say pavilion. So y you bond with y one.

581
01:17:07,876 --> 01:17:11,356
What is y ma y bar is obedience of.

582
01:17:12,586 --> 01:17:21,526
So y buddy's 1 to 1 in summation y covariance of that done with y one.

583
01:17:21,736 --> 01:17:27,106
So now let's look at this. The one over and you know, no big deal comes out of the pavilion site.

584
01:17:27,406 --> 01:17:34,246
So I have an expression like y one plus y to slightly y in obedience of that with y one.

585
01:17:36,046 --> 01:17:40,115
So if I again apply the formula results for obedience,

586
01:17:40,116 --> 01:17:49,546
so I have a term like obedience of y you want with y one obedience of why do we apply one obedience of white three with white one and so on.

587
01:17:51,076 --> 01:18:02,356
What happens to that sum the y's by virtue of the uncoordinated and the y that all the time coordinate.

588
01:18:02,896 --> 01:18:13,096
So for billions of y do with y one is zero obedience of y3y1 you feel obedience apply in with y, one is zero.

589
01:18:13,096 --> 01:18:17,886
The only thing that stays is the whole brilliance of y one.

590
01:18:17,896 --> 01:18:22,336
We did so well. We didn't survive on y Iran.

591
01:18:22,996 --> 01:18:26,206
And that is what is the variance of y one?

592
01:18:28,216 --> 01:18:34,466
Yes. Everybody picked me.

593
01:18:38,466 --> 01:18:45,306
So I only have a. Did everybody follow the logic?

594
01:18:47,316 --> 01:18:57,116
So I only have dance like obedience off why I with it says and a multiple of one.

595
01:19:01,306 --> 01:19:09,426
So it must win over em. Okay.

596
01:19:09,906 --> 01:19:20,086
Well, we didn't. So why, why, why we did self is variance of y which is sigma squared because variance so all

597
01:19:20,106 --> 01:19:34,386
variance of why I we it says for any is equal to the variance of what I for all only.

598
01:19:42,176 --> 01:19:47,835
So I have seen one spread over NN and then I have the one, what is it?

599
01:19:47,836 --> 01:19:55,006
Six. But what stays within the submission size sign is submission.

600
01:19:55,016 --> 01:20:02,636
It's say minus message by one thing and it's zero because I'm taking the sample center beepers so it drops off.

601
01:20:03,566 --> 01:20:17,966
I am left with zero. So I have proved that the coordinates of y bar with beta one had zero and plugging that being I got the variance of beta not had.

602
01:20:20,786 --> 01:20:26,766
Any questions here? No.

603
01:20:27,066 --> 01:20:43,356
Okay. So now, lo and behold, I have gotten expressions for both the radiance of bitterness part and the brilliance of big doe on head.

604
01:20:44,256 --> 01:20:51,316
But. There is sort of more of an issue here.

605
01:20:53,026 --> 01:20:56,716
The variance of meter one hat is sigma squared over ssx.

606
01:20:56,806 --> 01:21:03,796
I know is this is from data but do I know sigma square and all this is the unknown error variance.

607
01:21:04,336 --> 01:21:08,656
The true error videos. Same with beta one had.

608
01:21:08,956 --> 01:21:13,935
Same with beta nampak. So I do x bar the sample mean of x's.

609
01:21:13,936 --> 01:21:20,866
I know ssx. I know the sample size, but kind of the pain in the neck is the sigma squared.

610
01:21:20,926 --> 01:21:28,335
That's still unknown. I don't have a numerical value for that, so I need to estimate this sigma squared.

611
01:21:28,336 --> 01:21:32,836
Otherwise I'm not going to get in get a number for these variances.

612
01:21:33,736 --> 01:21:43,066
Okay. So I now have to estimate the sampling variance of this regression.

613
01:21:43,696 --> 01:21:47,986
So basically generally sigma squared is unknown.

614
01:21:48,166 --> 01:21:52,336
I don't know the name and the sigma squared.

615
01:21:53,386 --> 01:22:13,426
So in SLR, in simple linear regression, we use this estimate for sigma squared and it's given by one over in minus two sum of the squared residuals.

616
01:22:13,546 --> 01:22:19,456
So some of epsilon I had squared remember our friend Epsilon I up.

617
01:22:20,086 --> 01:22:22,666
These are the estimated errors of the residuals.

618
01:22:23,146 --> 01:22:36,646
So I take the sum of the squared residuals divide by n minus two to get an estimated of Sigma Square the true unknown in opinions.

619
01:22:37,816 --> 01:22:49,305
This is also defined as a this this guy here the summation epsilon I had squared remember I call this the sum of squared

620
01:22:49,306 --> 01:23:06,646
errors it says E s is C divided by n minus two and this is defined or called as m s e or the mean squared error.

621
01:23:13,996 --> 01:23:20,026
Well, Epsilon I had once again is the observed minus the estimated Y.

622
01:23:20,776 --> 01:23:24,166
It's the it's it's it's the residual of the estimated error.

623
01:23:24,166 --> 01:23:32,386
So Sigma Head Square reflects the average squared distance between the observed and the predicted wise.

624
01:23:34,786 --> 01:23:38,296
What would be the implication of having a large sigma head square?

625
01:23:38,686 --> 01:23:42,856
Basically, if if the sigma had square is large, then what?

626
01:23:42,856 --> 01:23:46,396
What that implies is that there's a larger amount.

627
01:23:46,426 --> 01:23:52,006
It's a large amount of the scatter in the wise and all that best fit line.

628
01:23:54,546 --> 01:24:05,976
So higher does Sigma head square? That means the best feed line that I have, but still a lot of scatter and on that best food line.

629
01:24:06,126 --> 01:24:23,906
So that's the implication. Now recall that the sample variance is, you know,

630
01:24:24,056 --> 01:24:35,635
we are going to talk about a couple situations and sort of kind of try to explain as to why nobody asked me that question.

631
01:24:35,636 --> 01:24:39,626
Like, you know, where does this n minus two come from suddenly?

632
01:24:39,926 --> 01:24:43,555
So let's talk about where does this come from?

633
01:24:43,556 --> 01:24:51,536
So we call when mean the true population meaning is unknown.

634
01:24:52,346 --> 01:25:00,746
In module eight, we established that the sample variance is given by you.

635
01:25:00,956 --> 01:25:14,906
You have a sample? Why? Why, why? Why? In you estimate the mean by the unknown mean mu by the sample mean so y bar is an estimate for the

636
01:25:14,906 --> 01:25:26,246
meal and the sample variance is given by one neighboring minus one summation y minus y squared.

637
01:25:26,246 --> 01:25:35,546
So you take the deviation of each by from the sample mean of the wise squared them up some them up and divide by n minus one.

638
01:25:36,026 --> 01:25:51,116
So we in module we sort of said that and I said that we you you will prove that this is an unbiased estimate of when milk is unknown.

639
01:25:51,746 --> 01:25:58,826
Now in the simple linear regression setting when estimating the sample variance of error the epsilon.

640
01:26:00,056 --> 01:26:08,186
So I have the sample x1y1x2. I do x and y in the estimated mean is given by beta not head plus beta one haddix.

641
01:26:08,226 --> 01:26:11,965
I know how to calculate the least squares estimates.

642
01:26:11,966 --> 01:26:22,166
We do not have to be the one hat, and I'm seeing that the sample variance is given by this variance of white hat is

643
01:26:22,166 --> 01:26:32,126
given by one good in minus to the sum of squares of the residuals or the MSE here.

644
01:26:32,456 --> 01:26:41,515
So the question is, as you can see, there's a difference in the divisor n minus one in the previous expression in minus two.

645
01:26:41,516 --> 01:26:45,206
Here, the question is, why is this difference?

646
01:26:46,616 --> 01:26:56,546
So here is the here is what's going on at this point, more from like a heuristic conceptual point of view later on,

647
01:26:57,746 --> 01:27:08,455
using distribution purely, we proved this formally and by the time you also catch up on 6 to 1, not so.

648
01:27:08,456 --> 01:27:15,546
The minus two in the denominator comes from the number of lives of freedom in the s

649
01:27:15,776 --> 01:27:24,866
in the degree of freedom b f is the number of observations that are free to vary.

650
01:27:25,376 --> 01:27:32,396
Another way that you can think about it is it's the sample size or the effective sample size,

651
01:27:32,396 --> 01:27:37,436
the number of contributions minus the number of constraints.

652
01:27:38,126 --> 01:27:41,366
So let's look at this a bit more critically.

653
01:27:41,756 --> 01:27:52,826
Here is an example. Four degrees of freedom in SSP. Suppose we observe y1y do y in a new the unknown population mean is the norm.

654
01:27:55,316 --> 01:28:03,206
Okay, so you've done one population mean is known then basically b we have nothing to estimate.

655
01:28:03,506 --> 01:28:08,946
You know, everything is known about the distribution, the mean is gone so we can actually compute the variance.

656
01:28:11,726 --> 01:28:18,046
So nothing is being spent up to estimate of an unknown freedom.

657
01:28:18,266 --> 01:28:27,376
So the degrees of freedom is equal to the sample size. Then suppose now next we have two observations.

658
01:28:27,386 --> 01:28:33,746
An equal to do y. One Y to the mean pople in the population is unknown, new y is unknown,

659
01:28:33,746 --> 01:28:46,826
and we're estimating it based on these the sample of size two and it will be estimated by y bar y, but is y one plus y two divided by two.

660
01:28:50,636 --> 01:28:55,646
So we have you we have estimated the unknown population mean.

661
01:28:55,736 --> 01:29:01,406
But here is the here is the sort of a revelation.

662
01:29:01,766 --> 01:29:08,876
So once y you on and y you bar are norm then.

663
01:29:09,966 --> 01:29:13,776
Why do we lose faith? Why we cannot that.

664
01:29:15,456 --> 01:29:23,586
There are two of the reasons. You know what somebody does with a mean based on my two observations is in five.

665
01:29:23,886 --> 01:29:27,696
And the first observation was three.

666
01:29:28,806 --> 01:29:32,676
Basically. Then why it is determined.

667
01:29:34,776 --> 01:29:39,156
So nothing. There is no room for why to do that.

668
01:29:40,026 --> 01:29:42,126
Let me just come to your question.

669
01:29:42,666 --> 01:29:55,926
So there is one degree of freedom laws in the SCC neutral estimation of why bar five spend up one degree of freedom to estimate that unknown meaning.

670
01:29:56,346 --> 01:30:03,876
So the so the degree of freedom becomes effective sample size n minus one.

671
01:30:06,666 --> 01:30:10,416
No question. So how can you?

672
01:30:11,606 --> 01:30:15,886
It has to be 140. No oranges.

673
01:30:17,276 --> 01:30:25,446
No, no, no hope. I mean, I'm saying that this is sort of the state of being going backwards in the sense that I sent it back.

674
01:30:25,446 --> 01:30:30,986
But so, like, the idea is this and you get confused as you think of it this way.

675
01:30:31,466 --> 01:30:38,256
So like I have two points to make up once why one why do and the mean is population millions of.

676
01:30:39,866 --> 01:30:43,586
I am I have to estimate one I move one.

677
01:30:44,546 --> 01:31:01,126
My effective sample sizes and I'm using up one degree of freedom to estimate that one I'm doing in beta a when I see free to vary it say the

678
01:31:01,206 --> 01:31:13,316
given that y but now becomes fixed then you really cannot get like your you know why do becomes fixed you are going to becomes predetermined.

679
01:31:13,316 --> 01:31:21,326
That's another way of thinking about it. If you want to think about it from the point of view of one things for a for example of

680
01:31:21,326 --> 01:31:30,416
size and genetic n and you could estimate be one photometer then basically ten minus one,

681
01:31:30,756 --> 01:31:44,455
you are using not one degree of freedom. So some so maybe think about it that way if if it confuses you to like how can you know why but before it no,

682
01:31:44,456 --> 01:31:48,916
you don't know why, but before you actually compute from the sample.

683
01:31:48,926 --> 01:31:58,676
But the idea is that, you know there is no weekly for to like to once you know like if you have only a sample of face do

684
01:31:59,576 --> 01:32:10,616
it's it's it gets fixed because you have used up one degree of freedom to on the mean in the SLR.

685
01:32:11,186 --> 01:32:15,076
So I have pairs of data it's one by 1% it's true.

686
01:32:15,086 --> 01:32:21,745
Why do it's and why am I an estimated beta not had been the one hat so I have used up two

687
01:32:21,746 --> 01:32:30,536
degrees of freedom to estimate these two are known parameters of intercept and slope.

688
01:32:31,526 --> 01:32:42,336
So the degree of freedom becomes and the effective sample size minus the number of the.

689
01:32:45,156 --> 01:32:48,666
Unknown parameters I have estimated. So minus two.

690
01:32:49,536 --> 01:33:03,396
So that's why Four Sigma had square. The denominator has n minus two due to the fact that I have used up like two degrees

691
01:33:03,396 --> 01:33:09,996
of freedom to estimate the intercept and scope for implementing this model.

692
01:33:11,976 --> 01:33:15,216
It can be shown and we will not show at this point,

693
01:33:15,546 --> 01:33:24,906
but it can be shown that sigma had squared as defined in the previous slide is an unbiased estimated of Sigma Square.

694
01:33:25,506 --> 01:33:40,426
So this is an unbiased estimate or. Okay.

695
01:33:40,906 --> 01:33:55,246
So all these results that I've talked about, including the the degree of freedom, can be so nicely, so elegantly, so easily understood.

696
01:33:55,606 --> 01:34:04,006
If I went back to distribution purity and if I just move the applying results from distribution purity.

697
01:34:04,366 --> 01:34:11,806
And this is where it's always a kind of a challenge at this point when you are very early on and so on,

698
01:34:11,806 --> 01:34:16,756
because you haven't really gone over those distribution query results.

699
01:34:17,386 --> 01:34:22,756
So I have to kind of tell you that like sort of conceptual arguments,

700
01:34:22,756 --> 01:34:32,566
but but later on when you have progress in 6 to 1 in distribution purely and when we are in multivariable or multiple linear regression model,

701
01:34:32,986 --> 01:34:40,456
we will come back to these results with general results and actually have been using distribution theory.

702
01:34:40,486 --> 01:34:51,936
And then you would see that things are much more clean and elegant then you don't have to really rely on these heuristic arguments.

703
01:34:52,346 --> 01:34:58,906
Okay. So I'm going to end with this very simple study, simple example,

704
01:35:00,646 --> 01:35:09,376
where a study was conducted to examine the relationship between a child's age and the amount of the child sleeps for night.

705
01:35:09,766 --> 01:35:11,686
So this is a really small dataset.

706
01:35:11,686 --> 01:35:22,096
A sample of an equal to 13 children was collected and the average number of minutes slept for a night over a one week period was recorded.

707
01:35:22,696 --> 01:35:28,426
I have also uploaded the dataset in canvas if you want to play with it.

708
01:35:28,436 --> 01:35:35,656
As I said, it's a very small position so you can reading the data and create an our dataset.

709
01:35:36,376 --> 01:35:42,076
And the first thing we did is compute descriptive univariate statistics.

710
01:35:42,466 --> 01:35:44,265
So here it is again,

711
01:35:44,266 --> 01:36:02,896
sample size and here are the sample means of each and and sleep time over a one week period and here of the standard deviations and also the minimum.

712
01:36:03,256 --> 01:36:17,656
So the minimum age in the study was 4.4 years and the minimum number of minutes slept was 461.75 over a one week period.

713
01:36:17,986 --> 01:36:27,615
And the the question is like we will fit a simple linear regression model to these data.

714
01:36:27,616 --> 01:36:32,686
The first thing I want to point out is how did we get this 2.775?

715
01:36:32,926 --> 01:36:43,516
This is the sample standard deviation of eight. So how would I get that if I did not have any information about why?

716
01:36:43,846 --> 01:36:52,906
So then this is by definition, this would be it says six divided by in -112.

717
01:36:53,236 --> 01:37:05,776
Okay. And it's a six is remember sum of I'll take each eight subtract the mean age of the of the sample squared and then think the sum overall 13.

718
01:37:06,386 --> 01:37:11,656
Yes, that's appropriate. Oh.

719
01:37:11,656 --> 01:37:20,356
So square root of that. Yes. Thank you. Yeah, square root of that or 2.77 square will give me a61n minus one.

720
01:37:21,196 --> 01:37:27,046
Okay. Now, so that's basically what I was doing here.

721
01:37:28,306 --> 01:37:33,916
And so the square root of sample variance would be 2.77.

722
01:37:34,456 --> 01:37:37,786
Then I have a scatter plot of age versus sleep.

723
01:37:38,296 --> 01:37:45,526
And here is the scatter of the data. So if I kind of visually did the best fit line, it will be like this.

724
01:37:45,976 --> 01:37:54,556
So let's now use the simple linear regression model to compute between what happened with the one hand.

725
01:37:54,946 --> 01:38:06,916
That's the end in functioning R and using the l m function I or the the coefficients or the estimates of we've done our pattern beta one head.

726
01:38:07,246 --> 01:38:10,785
So here we've done our peg or the square.

727
01:38:10,786 --> 01:38:23,566
Estimated of the intercept is six 46.48 and the least good estimate of the slope is -14.04.

728
01:38:26,296 --> 01:38:31,906
And we will come to the standard error P-value and then the significance testing later on.

729
01:38:32,146 --> 01:38:41,296
One other thing that I want to point your attention to is this residual standard and 13.15 on 11 degrees of freedom.

730
01:38:42,106 --> 01:38:47,566
So what is this, 13.15? This is my MSP.

731
01:38:51,766 --> 01:39:01,486
Okay. So this is my summation Epsilon I had squared divided by 30 minus two, which is what gives me that 11.

732
01:39:03,256 --> 01:39:14,626
Okay. So all of these other quantities, the multiple are split adjusted output X statistic, we haven't gone through those yet, so keep them for later.

733
01:39:17,026 --> 01:39:21,856
And then I cleared in our dataset containing the original data and the fitted values I can get,

734
01:39:22,036 --> 01:39:29,536
the fitted values, the white hat based on my return on pack plus beta one hat time set sun.

735
01:39:30,106 --> 01:39:36,226
I have numbers numerical values for beta Northampton beta one had based on my data

736
01:39:36,226 --> 01:39:42,166
and I have the extends the age so I just plug them in and get the white hat.

737
01:39:42,166 --> 01:39:49,426
So those are my white hats and the next thing I do is on the scatter plot.

738
01:39:49,726 --> 01:39:58,336
Now I superimposed this line so this red line is basically my best fit and line.

739
01:39:59,746 --> 01:40:09,555
Okay, so the next two, three slides I'm going to end here are essentially, you know,

740
01:40:09,556 --> 01:40:17,086
kind of verifying by hand calculation that you would get this exact same estimates

741
01:40:17,086 --> 01:40:22,246
but are not had and beta one had that you got using the elm functioning arm.

742
01:40:22,486 --> 01:40:29,056
If you were to do the hand calculations using the expressions for this x y in this y in SSX.

743
01:40:29,566 --> 01:40:37,426
So I want you to kind of like verify that it's more of verification steps and I'm going to end here.

744
01:40:38,836 --> 01:40:43,576
Okay. Questions for me.

745
01:40:44,176 --> 01:40:49,995
Yes. Okay. Actually, can we keep a while?

746
01:40:49,996 --> 01:41:00,196
I close this. Go ahead and ask my question, but let's step out of and answer that question.

747
01:41:01,906 --> 01:41:06,276
Maybe, maybe, maybe just announce the question so that everybody hears.

748
01:41:06,286 --> 01:41:11,206
And then if I need to send it to the rest of the class, I'll do the post.

749
01:41:12,766 --> 01:41:19,096
So just just announce your question and then we'll leave for the next instructor.

