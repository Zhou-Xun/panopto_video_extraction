1
00:00:00,780 --> 00:00:04,520
All right. Good afternoon, everybody. Why don't we get started? So people will be.

2
00:00:04,530 --> 00:00:12,100
Come in. So. I don't know how many of you have registered, like after Monday,

3
00:00:12,610 --> 00:00:22,540
but I'm still going to go over some basics because very briefly, what I'm going to do is just to show you.

4
00:00:23,700 --> 00:00:36,340
That if you were not here on Monday, you need to try to form your teams by click this link, which will guide you towards the date.

5
00:00:36,390 --> 00:00:43,570
And also there are lots of data links you want to consult when you decide which data to work on so that

6
00:00:43,620 --> 00:00:51,840
that link can be found in the first slides and also in the canvas website where if you click assignment,

7
00:00:52,920 --> 00:00:54,810
if I do student view, this is how we can see it.

8
00:00:54,930 --> 00:01:03,270
You can just do the assignment proposal, you can click this and there are lots of very detailed instructions there.

9
00:01:03,870 --> 00:01:11,490
If you have any questions, please feel free to ask mpesa. I'm also going to leave some time for your questions now if you have any.

10
00:01:21,720 --> 00:01:26,910
No primary audio source. So this has been problem seems to be.

11
00:01:29,100 --> 00:01:38,070
So when I was checking these, these are not working. But you do see these are working, right?

12
00:01:41,040 --> 00:01:51,480
Okay. Are any questions about the team information or other aspects of the class?

13
00:01:55,240 --> 00:01:59,680
All right. If not, then let's continue with the class.

14
00:02:03,840 --> 00:02:08,710
Oh, by the way, if you have not signed into the piazza, please consider doing that.

15
00:02:08,730 --> 00:02:12,540
So if you click here. Oh, actually, I don't know why I can not play here.

16
00:02:12,810 --> 00:02:19,400
So if you click there, you should be able to log in there and you need to sign up using the passcode.

17
00:02:19,620 --> 00:02:21,480
Go blue. All right.

18
00:02:22,660 --> 00:02:34,330
So today's lecture is the first introduction to how to understand longitudinal data, how to use mathematical annotations to represent them,

19
00:02:35,050 --> 00:02:42,190
and some very high level introduction to how we will model them so that you will see a lot of counting.

20
00:02:43,270 --> 00:02:51,190
It's like little bit mechanical, but this is the first set of notations you will be learning.

21
00:02:53,490 --> 00:02:57,630
The focus will be linear models for longitudinal continuous data.

22
00:02:58,260 --> 00:03:04,050
There are a few objectives. We will use about five parts or four parts to go through these objectives.

23
00:03:04,410 --> 00:03:12,810
I like to go through them with you now. So the first one is how to how to classify longitudinal data structure using proper terminologies.

24
00:03:13,080 --> 00:03:18,600
We have briefly touched on them in the first class and this is our opportunity

25
00:03:18,600 --> 00:03:23,310
to revisit and hopefully to familiarize yourself with the terminologies.

26
00:03:23,820 --> 00:03:27,540
The second is how do we use mathematical annotations to represent them?

27
00:03:28,320 --> 00:03:33,920
The reason is that we don't want to. We are lazy. I cannot speak for you, but mathematicians are lazy.

28
00:03:33,930 --> 00:03:39,479
They don't want to use very verbose language to describe a very simple concept.

29
00:03:39,480 --> 00:03:47,940
So they invented these notations. We're going to do the same, and hopefully by practicing using them, these will become more and more familiar to you.

30
00:03:48,810 --> 00:03:58,020
Number three, we will use a very simple example to explain that correlation among the repeated measurements will violate a

31
00:03:58,020 --> 00:04:06,390
fundamental assumption dependance and that will be the important consideration when we're designing a new methods.

32
00:04:07,410 --> 00:04:14,700
And your task or your objective is to understand what are the consequences of ignoring these correlations.

33
00:04:14,700 --> 00:04:17,760
So we will use a very simple numerical example until illustrate.

34
00:04:18,870 --> 00:04:26,800
Finally, as we will embark on a journey of modeling, we will briefly classify the approaches we'll be taking.

35
00:04:27,480 --> 00:04:31,980
So one of the key distinction we will be making.

36
00:04:33,950 --> 00:04:38,690
Is substantive versus nuance parameters.

37
00:04:39,230 --> 00:04:41,640
So substantive will be something of scientific interest.

38
00:04:41,720 --> 00:04:48,980
Newest notions will be something that's typically not of primary scientific interest, but has to be included during the modeling process.

39
00:04:50,370 --> 00:04:54,240
All right. So that's the plan for today. So let's start with the part one.

40
00:04:54,840 --> 00:04:57,960
We will have a few slides going through the terminologies.

41
00:04:58,770 --> 00:05:01,920
They can be very common to you.

42
00:05:02,850 --> 00:05:11,340
We have like you learn them in your regression classes, but it doesn't hurt to go through them in this particular context.

43
00:05:11,790 --> 00:05:17,760
So often we will refer to units as individuals or subjects and.

44
00:05:19,290 --> 00:05:24,030
For the measurement times. We often refer them to occasions or times.

45
00:05:24,570 --> 00:05:31,980
And for example, individuals can be repeatedly measured at different occasions or different times.

46
00:05:33,860 --> 00:05:37,190
And that defines longitudinal studies.

47
00:05:37,610 --> 00:05:44,360
And we also talked about balanced versus unbalanced study designs.

48
00:05:44,630 --> 00:05:52,030
And these are some more detailed slides about them. So the first one is what we call balanced longitudinal data structure.

49
00:05:55,680 --> 00:05:59,060
Just bear with me. I may find. Here we go.

50
00:06:00,140 --> 00:06:04,150
Great. So the first is called balanced, a longitudinal data structure.

51
00:06:04,160 --> 00:06:13,670
What are the examples? For example, we can rapidly measure self-reported pain scale at baseline and six follow up measurements, right?

52
00:06:13,670 --> 00:06:22,340
So everybody will be measured with the same amount of same number of occasions, with the same spacing between these occasions.

53
00:06:22,670 --> 00:06:28,190
Number two, similar situation, but in different context of human understanding, human growth.

54
00:06:29,450 --> 00:06:34,880
The important thing is that they have the same number of repeated measurements at the common set of occasions.

55
00:06:35,240 --> 00:06:38,990
So often if you draw the what we call spaghetti plot,

56
00:06:39,320 --> 00:06:46,310
you will see something like this and you will find plenty of example later part in the later part of this lecture.

57
00:06:46,790 --> 00:06:51,170
So. Often. These are the T.

58
00:06:51,200 --> 00:06:56,150
This is the Y. And this is zero 1 to 3 four.

59
00:06:56,160 --> 00:07:02,840
Right. So you can plot the data for each individual and they are observed at multiple occasions.

60
00:07:03,350 --> 00:07:15,800
And you have another person say that's observed at the lower levels, but the same number of measurements at exactly the same set of occasions.

61
00:07:16,490 --> 00:07:21,430
All right. So that's what we call balanced. Now moving on.

62
00:07:23,890 --> 00:07:30,640
There are other things that we call on balance, which is typically what we have in the in real along between all studies.

63
00:07:31,330 --> 00:07:35,830
So what are some of the examples that we call on balance, longitudinal data?

64
00:07:35,980 --> 00:07:42,100
First, for example, the measurements are not obtained at the common set of occasions.

65
00:07:42,100 --> 00:07:50,710
Right? For example, if you ask people to come into the study to get something measured, you do not tell them an exact date for them to come in.

66
00:07:50,860 --> 00:07:56,470
They will make individual appointments and the exact date of the visits will not be the same.

67
00:07:57,550 --> 00:08:00,520
And this is quite common when you're dealing with, say,

68
00:08:00,520 --> 00:08:06,040
electronic health records or if you use some criteria to identify a certain subset of patients.

69
00:08:06,280 --> 00:08:08,730
And then you look back in history and find the data.

70
00:08:08,740 --> 00:08:16,840
Often you cannot make sure that all the measurements were taken at the same set of a common set of occasions.

71
00:08:16,840 --> 00:08:21,580
Right. And other times these.

72
00:08:23,860 --> 00:08:28,180
Not common set of occasions are caused by design considerations.

73
00:08:28,210 --> 00:08:32,740
There are designs like a staggered design or a stepwise design.

74
00:08:33,910 --> 00:08:38,770
So these are by definition, by design not to have measurements occurred at the same set of occasions.

75
00:08:40,510 --> 00:08:44,440
So that's the first possibility of having unbalanced data.

76
00:08:44,740 --> 00:08:48,940
Second, we have a very pervasive problem, missing data.

77
00:08:49,630 --> 00:08:57,790
And because you are following people over time and it is likely that certain people decided not to continue with the study and for ethical reasons,

78
00:08:57,790 --> 00:09:00,780
we cannot force a person to come back to study, get things measured right.

79
00:09:01,210 --> 00:09:07,870
So if that happens, we do not get to measure what was scheduled to be obtained from this person at a certain set of occasions.

80
00:09:07,870 --> 00:09:13,700
Right. So when missing data occur, clearly some people will have less measurements while other people have more.

81
00:09:14,290 --> 00:09:26,470
And this is what we call a missing data. Often this class I call, I referred to these settings as incomplete data, which are pretty much equivalent.

82
00:09:29,080 --> 00:09:39,159
The only distinction I want to make is that too often this kind of incomplete data situation occurs when we want to obtain certain data,

83
00:09:39,160 --> 00:09:58,670
but somehow we cannot get it. There is a even more subtle reason of how unbalanced data structure can occur.

84
00:09:59,180 --> 00:10:04,190
And if you are tired of reading the words, I can explain to you with a few visuals.

85
00:10:04,670 --> 00:10:08,780
So essentially it is has something to do with the definition of ten zero.

86
00:10:09,830 --> 00:10:16,700
So I'm going to. Provide us some fake examples of a few people's data.

87
00:10:22,160 --> 00:10:37,790
So in this example, suppose this is a cat in time. Let me see what I can.

88
00:10:37,790 --> 00:10:48,980
Move it closer to me. Okay. So suppose we follow.

89
00:10:50,480 --> 00:10:58,380
You know, many different people. We got measurements. All right.

90
00:10:58,410 --> 00:11:04,120
This is for one person. Oh, by the way, I will be sharing all the annotated slides.

91
00:11:04,120 --> 00:11:11,680
So if you feel you have something that's not recorded on your own slide, I'll have them after a pause to and for another person.

92
00:11:13,480 --> 00:11:26,559
Let's say this one, right? So if we define time zero as the left in most measurements,

93
00:11:26,560 --> 00:11:34,120
clearly in this case the measurements are balanced because they have the same number of measurements per person at the common set of occasions.

94
00:11:34,600 --> 00:11:38,980
However, in certain studies, the ten zero is defined differently.

95
00:11:39,010 --> 00:11:46,960
It is not defined with respect to the hand of time, but with respect to certain events that will be important in scientific studies.

96
00:11:48,070 --> 00:11:54,790
This is, for example, for this person. If I say define important events, say I think this example.

97
00:11:55,480 --> 00:12:01,510
Suppose there is a study that examines the changes in body fat in girls before and after menarche.

98
00:12:01,780 --> 00:12:07,359
Baseline measured prior to menarche with annual follow up measurements of body fat right to different people.

99
00:12:07,360 --> 00:12:10,870
They may have totally different times when in Morocco would happen.

100
00:12:11,110 --> 00:12:17,860
So say for the first person, we have that event of interest happening here.

101
00:12:20,100 --> 00:12:25,040
For another person. You know, maybe the event happens here, right?

102
00:12:27,470 --> 00:12:33,790
So in those situations, actually what people want to do is to do the following realignment.

103
00:12:34,630 --> 00:12:38,900
You see if I can select. Okay.

104
00:12:38,940 --> 00:12:42,990
Wish me good luck on this. So let's see.

105
00:12:52,810 --> 00:13:01,870
So select. I think that's the. Or should I use a lasso tool?

106
00:13:02,970 --> 00:13:08,830
Have a lasso select. Okay. So. Really?

107
00:13:10,610 --> 00:13:18,920
Okay. What I want to do is just to copy this. Just bear with me for a moment.

108
00:13:31,280 --> 00:13:34,970
Okay. So copy. And Pest.

109
00:13:38,790 --> 00:13:45,760
Colby. Okay. And. Paste Yeah, there we go.

110
00:13:46,510 --> 00:13:52,900
So in this case, if we want to align the data, we will want to do something like this.

111
00:13:54,970 --> 00:14:01,910
Or should I just put it at the bottom? Maybe that's easier to see. So what I have done essentially is that for the same person.

112
00:14:06,550 --> 00:14:17,830
For the same person OC. We just moved the data, shift the data towards the left for the reason that we want to align this time, point to our.

113
00:14:18,730 --> 00:14:23,150
But now if you're comparing B and B prime. So A and B prime.

114
00:14:26,100 --> 00:14:30,489
If you only have the data like A and B, prime is a data structure balance.

115
00:14:30,490 --> 00:14:31,560
So on balance and Y.

116
00:14:33,990 --> 00:14:41,760
The question I'm going to ask is, do they have a common set of occasions for these measurements after the shift of Person B's data?

117
00:14:44,490 --> 00:14:49,920
No. Right. Because you will not have any observation here or here.

118
00:14:50,190 --> 00:14:53,130
And for a person here, you don't have any observations here or here. Right.

119
00:14:53,850 --> 00:15:01,020
So this is a subtle reason why sort of measurements could appear balanced or balanced, depending on how you define ten zero.

120
00:15:01,830 --> 00:15:10,940
All right. Happy to answer any question you may have now. All right.

121
00:15:10,940 --> 00:15:13,830
So that's a simple. So in summary.

122
00:15:15,170 --> 00:15:23,600
You know, most of the national data are not balanced and not complete, and we need to have methods that can deal with them.

123
00:15:24,980 --> 00:15:32,780
So in many of the models that we will introduce, they will be able to handle both balanced and balanced data.

124
00:15:32,810 --> 00:15:39,920
Clearly for a balanced data situation that's a cleaner dataset, and we will have some special methods for them as well.

125
00:15:43,150 --> 00:15:52,210
All right. So notations again, these are like, you know.

126
00:15:53,190 --> 00:15:58,380
Shorthand for describing the data structure. So what you see here are people.

127
00:16:00,240 --> 00:16:11,280
And small and occasions. What you have been learning in six 5651 is often that you only have this one as the outcome.

128
00:16:12,530 --> 00:16:22,220
Everybody has one bead continuous so beat and I'm continuous and this expansion column wise is new to this class.

129
00:16:23,270 --> 00:16:34,010
So in this table we can see that everybody has the same number of measurements and currently we are not dealing with a missing data.

130
00:16:34,400 --> 00:16:42,350
And in this case we're just assuming that every measurement measurements from different people were obtained at the same set of occasions.

131
00:16:42,710 --> 00:16:47,390
That's why we did not have information about the actual timings of these measurements.

132
00:16:48,980 --> 00:16:56,250
Now. We will use vectors to denote these data.

133
00:16:56,850 --> 00:17:02,400
So for I so I indicate index the subject, we will have a column vector.

134
00:17:02,760 --> 00:17:06,630
So in this class, by default, a vector is a column vector.

135
00:17:23,270 --> 00:17:33,139
So for this particular event, we have end by one because everybody in this case had an measurements and in longitudinal data analysis.

136
00:17:33,140 --> 00:17:41,600
So LDA represents longitudinal data analysis. I'll be using that acronym frequently, so please remember it.

137
00:17:42,540 --> 00:17:52,590
What's the main scientific interest? Usually it's in the main responses, i.e. how do we explain the changes in the mean response over time?

138
00:17:53,070 --> 00:17:58,800
Right. Because you have the measurements over time. So hopefully you can use measurements from more than one person to understand this.

139
00:17:59,940 --> 00:18:05,339
And especially we want to understand how these changes would depend on certain important

140
00:18:05,340 --> 00:18:10,410
factors like the treatment group or other baseline factors or the ten varying factors.

141
00:18:12,090 --> 00:18:18,270
A few more notation. So this one. M.J. e y.

142
00:18:18,270 --> 00:18:28,020
AJ So e represents expectation. Y J represents the JTH measurements obtained at the Jth occasion for the person.

143
00:18:28,590 --> 00:18:32,420
Now, notes M.J. does not depend on I in this case.

144
00:18:32,430 --> 00:18:37,560
I'm just assuming that regardless of who you are, the man at the particular occasion will be the same.

145
00:18:38,580 --> 00:18:43,979
That may not necessarily be the case because people may differ in their baseline information.

146
00:18:43,980 --> 00:18:47,790
So in the following point we have an additional index.

147
00:18:48,330 --> 00:18:56,690
Mu IJA So this is to indicate that possibly different people may have different mean values at the same occasion.

148
00:18:57,300 --> 00:19:04,710
Right. Now, by the way, do you guys know why the parameters are represented using Greek letters?

149
00:19:10,890 --> 00:19:16,320
So long story short, basically, whenever you see Greek letters, they represent unknowns.

150
00:19:16,650 --> 00:19:21,450
These are mysterious quantities. So you got to use whatever observation you have to infer them.

151
00:19:21,450 --> 00:19:24,960
You can get close to them, but you can never observe directly.

152
00:19:25,680 --> 00:19:31,319
So the game here is that how can we use information collected in the longitudinal

153
00:19:31,320 --> 00:19:36,450
study like Wise and in the future covers to inform these Greek letters?

154
00:19:36,710 --> 00:19:41,040
Okay. So often when the thing is written in Greek letter, it's something of interest.

155
00:19:42,910 --> 00:19:46,930
And a few more terminology.

156
00:19:46,940 --> 00:19:50,890
So in the final bullet point, I'm referring to this as conditional me.

157
00:19:51,670 --> 00:19:57,540
So this word conditional often means that we're conditioning on a certain individual level.

158
00:19:57,560 --> 00:20:03,250
Covariance. Okay. And this is signified by the use of the index sign.

159
00:20:08,240 --> 00:20:14,750
Now with this, we have represented the responses for the eighth person across all the occasions.

160
00:20:15,350 --> 00:20:20,020
We have a few more annotations. So again, these concepts should be very familiar to you.

161
00:20:20,030 --> 00:20:27,330
And, you know, my goal here is just trying to review with you and also get you familiar with the index here.

162
00:20:27,350 --> 00:20:32,330
That's really something you want to understand here.

163
00:20:33,400 --> 00:20:44,860
Sigma squared the variance of the response at the Jth occasion and we have the pairwise covariance it is called

164
00:20:44,860 --> 00:20:51,190
conditional because you can conditioning possible you you can condition possible some possible covariance.

165
00:20:51,580 --> 00:20:56,649
And finally if you normalize them by the marginal standard deviations, you get the correlation.

166
00:20:56,650 --> 00:21:03,940
So nothing special here. The bottom line is that you will see later on that for a better.

167
00:21:04,180 --> 00:21:08,970
Oh, by the way, when I'm writing by hand, clearly I don't have the magic of writing in boldface.

168
00:21:09,010 --> 00:21:14,740
Okay. So what I'm writing to represent the vector, I put a tilt underneath it to represent the vector.

169
00:21:15,250 --> 00:21:20,510
So in every race, you tilt. Underneath a letter that's representing a vector.

170
00:21:21,290 --> 00:21:26,750
So you have y i1 y i to that at the end.

171
00:21:26,930 --> 00:21:29,390
So if you don't see a tail, that means that scalar. Okay.

172
00:21:30,140 --> 00:21:37,610
And what we have been talking about that is that we can assess the variation of y one across different people.

173
00:21:38,680 --> 00:21:47,940
For the same occasion. Right? So you can talk about Sigma J squared and also you can talk about, say, the covariance between the two.

174
00:21:48,040 --> 00:21:51,610
So that's sigma one, two, right? Actually, Jake was one.

175
00:21:52,270 --> 00:21:59,650
You can tell about how they are going to be cohering. And when you normalize them by the set deviation, you can also talk about correlation.

176
00:21:59,680 --> 00:22:06,700
One, two. So these notations are setting the stage for discussing possible correlations among the responses.

177
00:22:10,270 --> 00:22:16,090
This brings us to this more magnificent representation of this covariance matrix.

178
00:22:16,930 --> 00:22:27,460
This is probably one of the most pivotal quantity in this class, which is used to explicitly represent the possible variation among the responses.

179
00:22:28,300 --> 00:22:32,360
It is, again, it's just a notation game.

180
00:22:32,380 --> 00:22:40,540
It's just a count accounting, you know. So for whatever things that appear on the diagonal, it is going to be representing variance.

181
00:22:40,540 --> 00:22:42,300
For whatever thing that's here,

182
00:22:42,310 --> 00:22:52,570
it's going to represent covariance and they are going to be identical because coherence is inverting to the ordering of the two variables.

183
00:22:53,540 --> 00:22:58,690
Okay. So if you use notation we just introduced before, actually you will have this, right.

184
00:22:58,810 --> 00:23:03,450
I think it's probably squared. Did I get it wrong? Yeah.

185
00:23:03,480 --> 00:23:10,970
Scared? Or let me just, uh, probably redefine this.

186
00:23:12,490 --> 00:23:16,640
Eraser. Okay. So.

187
00:23:19,650 --> 00:23:27,630
Just bear with me. It's still new to this classroom. So why Sigma equals Sigma J squared in the previous slide.

188
00:23:33,990 --> 00:23:45,650
Okay. This is also a prelude to that to many of the many algebraic operations you'll be doing in this class.

189
00:23:45,660 --> 00:23:52,410
And I assume that all of you have taken 650 and 651 and you should be at least familiar with the common operations,

190
00:23:52,410 --> 00:23:58,080
like inverse, like funny eigenvalues, like, like the compositions to some degree.

191
00:23:58,590 --> 00:24:03,060
But we will be review them, reviewing them when they're needed.

192
00:24:04,770 --> 00:24:29,290
Okay. So in this slide, essentially, I'm just going to say that this is the normalized version.

193
00:24:34,880 --> 00:24:38,270
Covariance and is also called correlation matrix.

194
00:24:52,460 --> 00:25:01,070
So for example, if this value is like 0.8, then this means that the correlation between the first and second measurement across people is point eight,

195
00:25:01,370 --> 00:25:06,529
indicating that they're very similar statistically. And for this one,

196
00:25:06,530 --> 00:25:14,930
it can be low because it is representing a correlation between the first and final measurements and because they are further apart in time.

197
00:25:15,140 --> 00:25:21,740
So the correlation may be maybe lower. Some examples.

198
00:25:23,030 --> 00:25:26,150
This data set will be referred to repeatedly in this class,

199
00:25:26,520 --> 00:25:33,970
primarily because of its it's beautiful that we don't have missing data in this case and it can illustrate many different methods.

200
00:25:34,730 --> 00:25:43,010
So it doesn't hurt to introduce now. Interesting. Yeah. So here I'm showing you a subset of the data where the first column represented.

201
00:25:45,280 --> 00:25:50,160
The second column represents Freeman Group. It's actually called placebo.

202
00:25:51,370 --> 00:25:57,320
As a presenter, Sima. So this is the new treatment.

203
00:25:57,380 --> 00:26:07,420
This is the control. And in this study, it is it has recruited a cohort of LED exposed children,

204
00:26:07,420 --> 00:26:16,120
and they're trying to study whether this new treatment schema is going to help have the body to remove the dirt from the blood.

205
00:26:16,270 --> 00:26:19,510
And as you know, lettuce is is toxicol.

206
00:26:20,350 --> 00:26:26,350
So in the next four columns, they represent the represent measurement.

207
00:26:26,890 --> 00:26:32,950
For example, if we look at a first person. You see all these values here.

208
00:26:33,250 --> 00:26:38,710
So using our annotation, this will be y 79 one.

209
00:26:39,430 --> 00:26:44,650
Y 79 two. Y 79 three.

210
00:26:45,900 --> 00:26:53,670
Why is 79 four right? So I know that there may be some discordance between these two numbers,

211
00:26:54,180 --> 00:27:02,550
and we will be talking about that because the names of the variables are actually indicating the actual timings of these measurements.

212
00:27:02,910 --> 00:27:08,820
But these one, two, three, four are again indexing the measurements from the from earlier to later part of the study.

213
00:27:09,390 --> 00:27:15,150
And similarly, we can apply the mutation for other subjects.

214
00:27:15,450 --> 00:27:24,120
And our goal is trying to compare how the average response would change between between these two groups.

215
00:27:25,110 --> 00:27:32,630
Okay. So what are some interesting scientific questions? Often people are interested in evaluating the treatment effect.

216
00:27:33,960 --> 00:27:35,580
There are a few possibilities.

217
00:27:35,610 --> 00:27:43,080
The first one is that how well we testing the null hypothesis that the immune responses at every time point coincide in the two treatment groups.

218
00:27:43,620 --> 00:27:54,190
So here. Midge, as is representing the main response for a subject in the treatment group receiving the drug Saxena.

219
00:27:55,490 --> 00:27:59,020
And notice here is not index by eye.

220
00:28:00,890 --> 00:28:06,230
It is just assuming that's the main. In the succeeding group.

221
00:28:06,350 --> 00:28:16,070
The case in jail is shared across people. Okay, so if you look at the second quantity, this is the same quantity, but for the control group.

222
00:28:16,610 --> 00:28:22,070
So. In addition, we are requiring this equality to hold for all the occasions.

223
00:28:22,370 --> 00:28:27,729
So this is a really stringent, you know. Set of equalities.

224
00:28:27,730 --> 00:28:38,440
It is saying that can we use a data to give us evidence against this null hypothesis that the new drug does not work at all at any of the occasions?

225
00:28:38,450 --> 00:28:41,840
Right. So this is one example.

226
00:28:42,790 --> 00:28:48,459
A second kind of hypothesis is something that we will be talking about a lot.

227
00:28:48,460 --> 00:28:50,950
Is that how about a change from baseline?

228
00:28:51,340 --> 00:29:01,950
So if you look at the quantity on the left, this is indicating the change in the mean response from the baseline to the occasion.

229
00:29:02,080 --> 00:29:07,420
Right. But for the treatment group and the same quantity for the control group.

230
00:29:07,780 --> 00:29:14,020
So this is saying that hay may be the starting point for the three groups are not the same.

231
00:29:21,700 --> 00:29:24,700
But the change. But the change are the same, right?

232
00:29:36,780 --> 00:29:49,230
I should warn. So to me, this is a more more liberal null hypothesis.

233
00:29:49,250 --> 00:29:57,690
So if you visualize this. For example, this is one, two, three, four.

234
00:29:58,070 --> 00:30:03,729
Right. So we can have this. Or you can't.

235
00:30:03,730 --> 00:30:21,240
And you can have this. Right. So the cross is representing the control group and the empty circle is representing the treatment group.

236
00:30:22,200 --> 00:30:28,170
So clearly at each occasion, the cross and empty circle does not do not overlap.

237
00:30:28,620 --> 00:30:34,230
Right. So the first nine passes will be too stringent and possibly not too much interest.

238
00:30:35,820 --> 00:30:41,280
While for the second the null hypothesis. As you can see, I'm drawing the two curves parallel.

239
00:30:41,530 --> 00:30:53,670
Right. So the changes relative to the first one are the same. Now some simple stuff.

240
00:30:53,720 --> 00:31:03,050
So these are the summaries. And what I did here is that I just calculated the responses for each of the occasions.

241
00:31:06,320 --> 00:31:09,410
All right, Jay varies from 1 to 4.

242
00:31:09,410 --> 00:31:15,320
And in the parentheses, these are the sample standard errors.

243
00:31:16,470 --> 00:31:24,450
So you can plot these things. So in this plot, I am showing you the main trajectories of the responses over time.

244
00:31:25,260 --> 00:31:30,840
So this one is the placebo. This one is the new treatment.

245
00:31:34,030 --> 00:31:42,730
So it seems that the treatment is actually doing something earlier in the follow up period and then the blood level seems to be increasing.

246
00:31:43,450 --> 00:31:50,350
Right. And I think one of the explanation is that when the drug concentration is quite high in, it can help the body remove the lead.

247
00:31:50,500 --> 00:32:00,250
But when that drug concentration decreases, the body is trying to achieve some kind of equilibrium, so that will reenter the bloodstream.

248
00:32:00,280 --> 00:32:01,360
So that's why it's going up.

249
00:32:02,170 --> 00:32:09,520
And by the way, here, the higher the value is, the the worse it is, because you don't want to have high level of blood lead in there.

250
00:32:14,130 --> 00:32:17,420
So we'd talk about me. How about the covariate?

251
00:32:17,610 --> 00:32:23,460
Oh, this is. I don't know what's happening here.

252
00:32:23,910 --> 00:32:27,720
So this is not to. Is a bit murky here.

253
00:32:27,810 --> 00:32:32,220
But hopefully I can explain the idea in the very clear and straightforward way.

254
00:32:33,000 --> 00:32:40,620
So in this pairwise scatterplot, essentially, I am trying to assess the correlation.

255
00:32:41,430 --> 00:32:45,090
Let's focus on the top one. So essentially, this is why I won.

256
00:32:45,180 --> 00:32:53,460
This is why to OC for everybody you can extract the pair values ya1 way two and then you can plot them.

257
00:32:53,970 --> 00:32:58,110
So if there are 30 people, there will be 30 dots in the top plot, right?

258
00:32:58,380 --> 00:33:02,240
What do you see? Clearly hyper.

259
00:33:03,750 --> 00:33:06,840
Correlated right now if you.

260
00:33:08,750 --> 00:33:11,970
Look at another. Scatterplot.

261
00:33:12,630 --> 00:33:15,840
Let's try to find a more interesting one. How about this one?

262
00:33:15,870 --> 00:33:19,140
So this is why I won. Why I.

263
00:33:20,580 --> 00:33:29,520
For? I guess. So this is trying to take the first and the final measurement from the same person, then plot those scatterplot again.

264
00:33:29,820 --> 00:33:36,390
You can see, hey, it's still highly correlated, but probably a little less than the top one, but still it's quite highly correlated.

265
00:33:37,140 --> 00:33:42,390
And you can interpret these plots similarly across all the pairs of y's you can get.

266
00:33:43,410 --> 00:33:45,840
So this is what we meant empirically.

267
00:33:45,840 --> 00:33:53,700
That's often the measurements obtained from same person is correlated and our techniques need to address these correlations.

268
00:33:55,590 --> 00:34:01,409
So by calculating those values we can see that in the bottom matrix.

269
00:34:01,410 --> 00:34:06,330
These are basically correlation matrix. You can see that it is highly correlated.

270
00:34:07,010 --> 00:34:18,300
You know, if it is y one and y to the correlation is .83 and it is why one and why four.

271
00:34:18,810 --> 00:34:23,580
It is .76 a little bit smaller, but not to too small.

272
00:34:29,070 --> 00:34:44,790
All right. Any questions should be very straightforward. Okay.

273
00:34:44,810 --> 00:34:49,760
Let's continue them. This brings us to part three.

274
00:34:49,760 --> 00:34:57,390
So currently. We are trying to set a stage for introducing new methods.

275
00:34:57,840 --> 00:35:04,200
Why do you want to sitting here for another? Two or three months to learn new stuff.

276
00:35:04,210 --> 00:35:05,560
And there are reasons to do so.

277
00:35:05,710 --> 00:35:17,050
And this is a super example, super simple example, hopefully to communicate to you the idea of the need for methods that can deal with correlation.

278
00:35:17,920 --> 00:35:22,360
So let's look at this quantity. Suppose we.

279
00:35:23,300 --> 00:35:29,090
Are taking the difference. Between the means obtained from the second occasion and the first occasion.

280
00:35:29,090 --> 00:35:34,910
And that difference is going to be -13. I believe this is in the treatment group.

281
00:35:35,120 --> 00:35:40,190
So good, you know, it helps reduce the level.

282
00:35:42,220 --> 00:35:47,270
And. Point estimate is never enough, right?

283
00:35:47,310 --> 00:35:54,900
Because if we are only going to get an estimation you don't need sitting here, I think it's quite easy to do.

284
00:35:54,930 --> 00:36:01,110
I think one thing that's kind of unique in statistics is that we do have a very

285
00:36:01,110 --> 00:36:05,960
good set of trainings understanding the uncertainty of of any estimate or.

286
00:36:07,560 --> 00:36:16,650
Arguably the reason why statisticians still have a job is that we do have better ways to calculate uncertainty or what we call inference.

287
00:36:17,010 --> 00:36:25,500
So whenever, you know, people say they're doing machine learning, I ask them, is a machine learning just people doing statistics?

288
00:36:25,590 --> 00:36:34,320
If they care about uncertainty. So anyway, I think long story short, what this example will be about is not about point estimate.

289
00:36:34,500 --> 00:36:40,170
It is about the inference, i.e. the assessment of uncertainty around this estimate.

290
00:36:41,130 --> 00:36:46,590
So now let's look at this. What's the variance estimate?

291
00:36:47,230 --> 00:36:51,300
Right. So we can just do a simple calculation. Well, what's the estimate itself?

292
00:36:51,660 --> 00:36:58,290
It is just taking the difference in terms of the sample means, and we can represent that using this formula.

293
00:36:59,970 --> 00:37:06,390
We have the variance of the first occasion. We have the veterans of the second occasion, but we have an additional term, which is this one.

294
00:37:09,510 --> 00:37:12,690
Negative two multiplied by times.

295
00:37:12,870 --> 00:37:17,340
Sigma one two. Sigma one, two is covariance between the first and second measurements.

296
00:37:17,370 --> 00:37:27,250
Right. So this additional term is going to account for correlation among the first to repeat the measurements.

297
00:37:28,060 --> 00:37:37,150
So I believe in most of the model you have seen in 650 and 651 sigma one two is zero.

298
00:37:42,380 --> 00:37:46,670
But now we're going to entertain ourselves with the possibility that these are not zero.

299
00:37:47,660 --> 00:37:55,430
And often. Passive. As we have seen in the example data earlier.

300
00:37:56,940 --> 00:38:02,960
Now. We're just going to do a very simple illustration so that are you using data?

301
00:38:02,980 --> 00:38:11,770
The sample variance is going to be 25.2 for the first occasion, 2850 8.9 at the second occasion, much larger variability.

302
00:38:12,730 --> 00:38:16,810
And for that sample covariance matrix, that's 15.15 point five.

303
00:38:16,870 --> 00:38:20,200
So again, it's positive as we have seen the scatterplot.

304
00:38:20,800 --> 00:38:24,430
All right. So the correct estimate of the.

305
00:38:25,470 --> 00:38:30,540
Veterans will be this plugging everything to the formula, putting on the hats.

306
00:38:30,570 --> 00:38:39,629
Right and this Barron's is 1.06 and because that sigma one to estimate its to be positive if you

307
00:38:39,630 --> 00:38:45,960
ignore that then you will get an incorrect assessment of the uncertainty which is going to be 1.68.

308
00:38:46,620 --> 00:38:51,210
So this is about 1.6 times larger than it should be. So it's overly pessimistic.

309
00:38:52,490 --> 00:38:55,550
Okay. So what's the implication of this example?

310
00:38:56,180 --> 00:39:03,370
We have one estimate. If this person is great school, he or she can do it.

311
00:39:03,400 --> 00:39:08,880
I think. Right. But you do have two different ways of assessing the uncertainty.

312
00:39:08,910 --> 00:39:14,580
The first way is the honest way, which is to account for possible correlation between the first and second measurements.

313
00:39:16,750 --> 00:39:19,930
That's the way we want one to arrive at.

314
00:39:20,140 --> 00:39:23,740
The second way is to totally ignore the correlation between the two measurements.

315
00:39:24,370 --> 00:39:37,290
It turns out that. The person who ignores the correlation is going to think the variance is 1.68 rather than what it should be.

316
00:39:37,290 --> 00:39:44,280
1.6, right. So this person is overly pessimistic, meaning he or she did not make full use of data.

317
00:39:45,180 --> 00:39:48,750
Hence an inefficient estimate of the beta.

318
00:39:51,800 --> 00:39:56,110
Um. So what confers this advantage?

319
00:39:57,670 --> 00:40:03,910
As you have seen it is because of the correlation. So that's that's kind of the intuition intuitive statement.

320
00:40:04,210 --> 00:40:09,190
At the end of the slide, it says that longitudinal studies are powerful,

321
00:40:09,850 --> 00:40:16,419
often more powerful to study changes in the main because everybody has his or her own past as a

322
00:40:16,420 --> 00:40:23,290
control and often a person's presence status is going to be correlated with this person's past.

323
00:40:23,530 --> 00:40:29,710
And that positive correlation helps with help reduce some of the variance of estimate.

324
00:40:33,070 --> 00:40:36,790
And I'm going to pause for a minute and see if we have any questions.

325
00:40:52,280 --> 00:41:02,060
All right. You guys are awesome. So. So far, we have not introduced any cohorts or complicated culverts.

326
00:41:02,480 --> 00:41:08,660
We do have culverts. That's the treatment group, placebo or eczema at the baseline.

327
00:41:08,690 --> 00:41:12,770
It does not change over time. It is just binary and that's simple.

328
00:41:13,520 --> 00:41:20,690
So in that simple setting, you have already have had a taste of the importance of a kind of a correlation.

329
00:41:21,440 --> 00:41:23,020
But we are more adventurous.

330
00:41:23,090 --> 00:41:31,910
We are going to introduce a more complete set of notations for representing culverts that may or may not be constant over time.

331
00:41:32,360 --> 00:41:39,890
All right. So that's what we call models for longitudinal data.

332
00:41:41,090 --> 00:41:44,180
You can consider this. 650, but for your data.

333
00:41:44,480 --> 00:41:49,790
Okay. But it's more fun. So our goal is trying to introduce a general.

334
00:41:52,910 --> 00:41:56,600
General model for longitudinal continuous response data.

335
00:41:56,900 --> 00:42:01,940
Some of you may be thinking, Hey, Genco, what's the difference between general and generalized?

336
00:42:03,540 --> 00:42:10,360
Because both will be written as your own. If you cannot recall this acronym, you need to come to me.

337
00:42:10,380 --> 00:42:16,470
Okay, so, Jill, I'm in this class. Indeed, it can be confusing, but whenever possible I'm going to distinguish them.

338
00:42:17,160 --> 00:42:21,420
So Jill in general is going to refer to generalized.

339
00:42:24,630 --> 00:42:32,430
OC generalize and model and this is to deal with continuous or non continuous outcomes.

340
00:42:34,620 --> 00:42:42,840
So this primarily appeared in 651 and I will be reviewing them using one lecture if you are not familiar with this.

341
00:42:43,200 --> 00:42:46,290
If you have, forget everything, Julian. You've probably want to attend a lecture.

342
00:42:46,620 --> 00:42:54,000
Lecture for General Nino model. So this general refers to the fact that we have correlations.

343
00:42:57,470 --> 00:43:06,590
So. So it is in this sense that we are moving away, moving from independence to correlation that we call it general.

344
00:43:07,040 --> 00:43:07,370
All right.

345
00:43:07,760 --> 00:43:15,650
I know this is some unfortunate naming convention, but I just want to make this extremely clear so that no confusion will have been down the road.

346
00:43:16,460 --> 00:43:21,470
Okay. So you will have no excuse to tell me that. Hey, you know, I do not know the difference.

347
00:43:22,340 --> 00:43:34,770
Okay. So what we will be considering are responses that generally have distributions

348
00:43:34,770 --> 00:43:39,150
are approximately symmetric without excessive long tails or skewness outliers,

349
00:43:39,390 --> 00:43:42,510
because in those settings some special techniques will be needed.

350
00:43:42,810 --> 00:43:50,370
And second, the general Nino model will serve as the absolute foundation for what you'll learn when we're trying to generalize.

351
00:43:52,000 --> 00:43:54,490
John to deal with dependent data.

352
00:43:56,300 --> 00:44:04,070
And a goal stated simply is that we want to make inference about the parameters of interest often in Greek letters beta just to.

353
00:44:05,010 --> 00:44:10,290
Give you some taste of the great leaders while recognizing the likely correlation structure of data.

354
00:44:11,160 --> 00:44:15,480
So I want you to really understand this, and it's not necessary now, but if you are reviewing the slide,

355
00:44:15,750 --> 00:44:22,140
I want you to once in a while refer to this statement just to orient yourself.

356
00:44:26,770 --> 00:44:30,010
Let's see where we are because we are probably 5 minutes.

357
00:44:32,350 --> 00:44:40,360
So the break I want to see if we want to take early break or not. Yeah.

358
00:44:40,370 --> 00:44:46,210
Why don't we take five minute break and we will continue after that. So we will continue at 350.

359
00:49:36,470 --> 00:49:44,480
Test. Test. Okay. All right. I'm very worried that the sound is not recorded in the video.

360
00:49:44,510 --> 00:49:50,930
Let's see what's going on after this. But I do see there is a bar light, so this indicates the sound is being recorded, I think.

361
00:49:52,040 --> 00:49:58,390
Okay. All right. Okay.

362
00:49:58,690 --> 00:50:02,980
Let's see. All right.

363
00:50:02,990 --> 00:50:08,510
Some more notations. I don't I agree it's not the most interesting stuff, but we do have to go through them.

364
00:50:08,510 --> 00:50:13,250
It is like you've got to be able to read the map. You got to be able to know these languages.

365
00:50:13,640 --> 00:50:19,350
So why do we need to introduce a little more complicated notation?

366
00:50:19,550 --> 00:50:24,140
It is because, as we said, there are many data sets that are not necessarily balanced.

367
00:50:25,040 --> 00:50:27,619
For example, we may not have the same number of measurements.

368
00:50:27,620 --> 00:50:34,279
Hence we need to introduce index Y and indicating different number of measurements per person.

369
00:50:34,280 --> 00:50:41,660
Right. So and John can be like three and Alex can be like two.

370
00:50:42,760 --> 00:50:46,480
We often just say, hey, and he calls blah.

371
00:50:47,450 --> 00:50:55,130
And I equals one too. And right. So this notation indicates that the number of occasions may change over time and time.

372
00:50:55,790 --> 00:51:02,590
Okay. So for one person to one tie, two to t i for say.

373
00:51:02,600 --> 00:51:06,750
Right. And its actual value may be different.

374
00:51:06,770 --> 00:51:12,480
So in the previous example, we have this a0146, right?

375
00:51:12,500 --> 00:51:18,830
In the example it is because they represent the actual timings of these measurements.

376
00:51:20,060 --> 00:51:24,190
These are going to be indexed by J here, right?

377
00:51:24,260 --> 00:51:29,960
Because J goes from 1 to 4 indicating the occasion we're talking about.

378
00:51:30,770 --> 00:51:34,590
Right. Now a special case.

379
00:51:35,130 --> 00:51:38,220
What if we do have a common set of occasions?

380
00:51:38,940 --> 00:51:42,810
So that's when you can ignore the Index II here.

381
00:51:46,710 --> 00:51:55,950
So you can see that in the data example we saw earlier, we just remove the I in the index and that will be fine because the same.

382
00:52:03,390 --> 00:52:14,520
Let's just focus on one or two on this slide. So essentially, we have to talk about the covariates and this indeed take maybe a little bit practice.

383
00:52:17,370 --> 00:52:21,880
So it's a bit more complicated than the response first. For Exigé.

384
00:52:24,660 --> 00:52:36,720
It is a vector OC and it is representing the set of cohorts at the JTH occasion for subject I and it can be excited j one to excise JP.

385
00:52:37,200 --> 00:52:43,190
So P is indicating the number of information you collected on this person at the occasion.

386
00:52:43,920 --> 00:52:47,630
Right. Okay.

387
00:52:47,810 --> 00:52:57,140
And here it is p by one again, as I said, by default, any adventure without an explanation, it will be a column vector.

388
00:52:59,060 --> 00:53:02,620
And there are in general two types of converts.

389
00:53:02,630 --> 00:53:07,580
The first type is called timing variant. The second is called time variance.

390
00:53:09,970 --> 00:53:15,830
Let's return to the response here. The only difference is that we're now dealing with and I buy one.

391
00:53:15,880 --> 00:53:20,770
So that's the. Now is different across people.

392
00:53:20,780 --> 00:53:30,020
So the length of this vector is going to be different. How about coverage on all the occasions?

393
00:53:30,350 --> 00:53:33,650
So remember we've just talking about Exigé, right?

394
00:53:33,950 --> 00:53:37,820
This is a vector of coverage for the occasion.

395
00:53:38,330 --> 00:53:43,810
A subject I. But we do have multiple occasions for a person, so this is how we do it.

396
00:53:44,320 --> 00:53:52,370
Remember, if we have a column vector, if we do x prime, so we just rotate it.

397
00:53:52,910 --> 00:53:59,230
It'll be a row vector, right? Okay.

398
00:53:59,620 --> 00:54:05,460
And this is what we have here. It is a row of it's.

399
00:54:10,680 --> 00:54:15,540
If we repeat the same process for all the occasions, we will have these information as well.

400
00:54:18,380 --> 00:54:27,460
Okay. And we will be using EXI to represent this entire set of coverage for all the people at all their occasions.

401
00:54:29,260 --> 00:54:31,060
And what's the dimension of this matrix?

402
00:54:35,610 --> 00:54:45,690
So it is now by essentially the number of occasions multiplied by the number of converts obtained from each occasion.

403
00:54:56,890 --> 00:55:09,050
So. Next, we're going to just connect the responses and the covers we have here on the left hand side of the equations.

404
00:55:09,440 --> 00:55:13,520
The set of responses. Y y y. To up to y and I.

405
00:55:13,580 --> 00:55:17,790
Right. And on the right hand side, we're going to unpack a little bit.

406
00:55:20,510 --> 00:55:25,690
This is a response. And these are what we call systematic.

407
00:55:28,770 --> 00:55:35,620
Covert effect. And this is what we call random errors.

408
00:55:39,290 --> 00:55:52,380
It is representing human ignorance. Because whatever cannot be explained by covers will put them into their.

409
00:55:56,510 --> 00:56:00,170
So let's focus on the systematic component first. Look.

410
00:56:01,060 --> 00:56:04,840
The first equation is just to look at the first. First occasion, right.

411
00:56:08,360 --> 00:56:14,300
It is relayed in the information collected at the first occasion to the outcome you observed at the first occasion.

412
00:56:14,660 --> 00:56:18,470
So why? One is the outcome. You want to explain them using p covariates.

413
00:56:19,580 --> 00:56:26,000
As I recall, this is always going to be one indicating the first occasion, the second index,

414
00:56:26,450 --> 00:56:31,940
while the the third index is going from one to P indicating the P sets of information you created.

415
00:56:32,510 --> 00:56:37,550
And we are doing a combination of these information and trying to explain the level of why here.

416
00:56:37,860 --> 00:56:44,560
Right. And because on the left hand side, we're using what I want, we're not dealing with expectation y one.

417
00:56:45,070 --> 00:56:50,800
So for any actual values of these responses, there will be random error as indicated by EIA here.

418
00:56:53,220 --> 00:56:58,140
For short and because statisticians sometimes can be very lazy.

419
00:56:58,140 --> 00:57:04,890
We just use this notation, essay one prime beater to indicate that.

420
00:57:06,560 --> 00:57:14,360
Son. Okay. So if you repeat this process for the second occasion, up to the final occasion for this person,

421
00:57:15,080 --> 00:57:24,800
you do have the same set of interpretations of trying to predict the response in terms of systematic effect and random errors.

422
00:57:25,640 --> 00:57:31,940
And the quantity of interest is this vector beta OC beta one through beta.

423
00:57:36,470 --> 00:57:40,850
And sometimes we choose to write models in this way.

424
00:57:41,240 --> 00:57:52,450
But what's the difference between the two? The difference is that on the left hand side of the bottom of this box,

425
00:57:52,450 --> 00:57:57,340
the equation, equation, the box, we are dealing with expectation of wise error.

426
00:57:57,610 --> 00:58:05,180
So it is the average of the response at JTH occasion, given a particular value of culverts.

427
00:58:05,590 --> 00:58:09,820
And there you just assume it's related culverts.

428
00:58:10,600 --> 00:58:15,100
And we have assumed that's the expectation of the IJA is zero.

429
00:58:16,130 --> 00:58:24,400
Actually, let me be more precise here. Eraser.

430
00:58:30,840 --> 00:58:34,360
Exigé. Equals zero.

431
00:58:35,050 --> 00:58:43,390
So we may have error when we are representing or measuring y j, but we assume on average those errors are going to be centered around zero.

432
00:58:44,320 --> 00:58:52,570
So the difference between the first set of equation and the equation in the box is that the equation in the box only talks about the mean trend.

433
00:58:53,520 --> 00:58:59,010
Put together by having all the occasions in one simple equation.

434
00:58:59,400 --> 00:59:06,360
This is the a model for the response, which includes the systematic effect and the random error here.

435
00:59:06,810 --> 00:59:39,790
All right. Any questions? Yeah.

436
00:59:39,800 --> 00:59:43,670
So the question is, is this column wise just for one person?

437
00:59:44,090 --> 00:59:52,890
Yes, it is. Yeah.

438
00:59:52,900 --> 00:59:56,590
So the question is whether it's of dementia and I by one. Yes, it is. Yeah.

439
00:59:59,410 --> 01:00:02,660
Any other questions? Okay.

440
01:00:08,760 --> 01:00:15,990
So essentially you will if you need to review this, this is the entire Matrix operation that enters here.

441
01:00:17,040 --> 01:00:23,270
So when you are looking at this set of equations, you do feel that, hey, it is somewhat like 650, right?

442
01:00:23,280 --> 01:00:30,269
Y equals x items better. Plus here it is that we are just having a lot of information packed into this equation

443
01:00:30,270 --> 01:00:36,690
where y is vector x is and I buy p matrix and beta's vector here and e itself is a vector.

444
01:00:36,870 --> 01:00:41,850
And that is pretty important because we will be talking about the covariance of the vector here.

445
01:00:42,480 --> 01:00:53,060
So whenever you have a vague memory about these equations, please refer to slide 25 and slide 26 and just try to close this.

446
01:00:53,220 --> 01:00:57,150
You know, we have this so you look at it and put aside and then try to write down those notations.

447
01:00:57,540 --> 01:01:01,260
I think in general, they will be very good first step to have some memory.

448
01:01:01,560 --> 01:01:04,170
And some people told me that. H And so why do I have to remember this?

449
01:01:04,530 --> 01:01:08,820
Well, if you go to go into friends and you hope to speak in French, you've got to remember some words, right?

450
01:01:09,390 --> 01:01:18,380
Remember how to construct sentences. So, sure, I don't speak French, but this these are equivalents of how do you form sentences.

451
01:01:18,390 --> 01:01:22,020
You have to have these building blocks. And I think I highly recommend that practice.

452
01:01:22,230 --> 01:01:25,650
If you are not familiar with this. Okay.

453
01:01:33,920 --> 01:01:41,810
All right. So with this mathematical notation, we do need to start talking about something that's called the distributional assumptions.

454
01:01:42,500 --> 01:01:47,510
Essentially, as we have alluded to, exit beta is what we call systematic.

455
01:01:47,780 --> 01:01:53,760
You give me a vector of x i. I output you something and that's a deterministic number.

456
01:01:53,980 --> 01:01:59,370
Peter here is considered a fixed number, but when you're doing measurements, there will be.

457
01:02:00,620 --> 01:02:05,150
You know, distributions of measurements. Right. Where does that readiness come from?

458
01:02:05,510 --> 01:02:14,210
That's coming from. So EIA is representing deviations from those main values, and that's the distribution we're going to talk about.

459
01:02:30,790 --> 01:02:34,480
So this is an error for random error.

460
01:02:40,500 --> 01:02:49,030
That's the first thing. The second thing is that. Often we also talk about the distributional assumption.

461
01:02:49,050 --> 01:02:54,810
So if I write this, it's representing a shorthand for distribution because I'm too lazy to write down the entire thing.

462
01:02:55,950 --> 01:03:03,210
Assumption. So we can talk about why, Ira but because why is a.

463
01:03:04,810 --> 01:03:09,880
Some of these, too. And this is considered as a fixed quantity, fixed but unknown.

464
01:03:14,070 --> 01:03:18,220
Right. So if you are talking about the distribution, that's why I.

465
01:03:18,880 --> 01:03:23,270
And the distribution if I. Should I put something here?

466
01:03:24,690 --> 01:03:28,110
Okay. These two things are related.

467
01:03:28,320 --> 01:03:33,440
So F here represents the distributional assumptions, but the relation is quite simple.

468
01:03:33,450 --> 01:03:41,490
It just a shift in terms of the main. If you take the distribution, I shift to in the main by exit beta, you got distribution y.

469
01:03:41,880 --> 01:03:45,600
So whenever I'm talking about distribution, these two things are actually interchangeable.

470
01:03:45,900 --> 01:03:52,200
You just have to be very clear about what is a distribution for y y you know one, you know the other.

471
01:03:53,990 --> 01:03:58,130
Is that clear? All right. Get.

472
01:04:05,360 --> 01:04:07,040
So that's what this slide is about.

473
01:04:07,040 --> 01:04:15,320
And usually we will introduce the notation UI, which again is a vector by itself to represent that systematic component.

474
01:04:15,710 --> 01:04:21,720
All right. And I will not try to overexplain the site.

475
01:04:21,740 --> 01:04:27,900
Basically what I just talked about. So this is about the main.

476
01:04:29,070 --> 01:04:33,680
I mean of why? How about the veterans?

477
01:04:36,430 --> 01:04:45,150
Coburn's. So this is what the slide is about.

478
01:04:46,170 --> 01:04:51,300
Okay. This may be something new to you. First.

479
01:04:52,820 --> 01:04:56,300
New notations. So Sigma, I hear capital sigma.

480
01:04:57,080 --> 01:05:00,559
So this is a matrix of dimension.

481
01:05:00,560 --> 01:05:04,639
And I buy and I. It is representing.

482
01:05:04,640 --> 01:05:12,980
That's how the responses are going to be correlated given the set of information collected for each person at each of the occasions.

483
01:05:14,360 --> 01:05:17,690
You have seen this matrix before. Let me remind you.

484
01:05:18,860 --> 01:05:35,329
I'm going to scroll back. So the top matrix is one example of sigma I it it just that we don't have the variance covariance matrix index

485
01:05:35,330 --> 01:05:41,750
by subject because in this case we're just talking about the coherence matrix within one treatment group.

486
01:05:43,760 --> 01:05:51,200
And this is the notation we talk about in this case because we did not talk about covariates.

487
01:05:51,440 --> 01:05:56,450
So we don't need a condition covered, but in complicated situations.

488
01:05:56,690 --> 01:06:01,460
We do need to talk about the covariance, given the correct information.

489
01:06:02,540 --> 01:06:07,580
So let me return to where I stopped, which I believe is slide 28.

490
01:06:13,800 --> 01:06:17,460
There we go. Okay.

491
01:06:17,480 --> 01:06:20,150
So here is another thing that I just talk about, right?

492
01:06:20,160 --> 01:06:28,520
So if you have concerns of why a vector, given the covariance, it is going to be the same thing as the covariance matrix.

493
01:06:28,520 --> 01:06:37,360
What you see given the cohorts that. And often this is assumed to be just the cover and for the error itself.

494
01:06:38,860 --> 01:06:42,670
Because we assume that CIA is independent of itself and.

495
01:06:49,260 --> 01:06:56,480
Moving on for the third bullet point. The reason why we index sigma by I can be of many reasons, right?

496
01:06:56,940 --> 01:07:00,570
For example, what if the dimensions are not same? You have three measurements.

497
01:07:00,580 --> 01:07:03,899
I have five. So the sigma as well be of different dimensions.

498
01:07:03,900 --> 01:07:07,320
Of course we will need to index sigma by. All right.

499
01:07:07,320 --> 01:07:12,630
That's the first reason. The second reason is that, hey, different people may have different.

500
01:07:14,870 --> 01:07:21,430
You know. Different trends or different ways of how the areas may be correlated.

501
01:07:21,940 --> 01:07:25,300
That may be a little bit more challenging situation,

502
01:07:25,630 --> 01:07:32,620
but often it's manifested in the fact that it is conditional on one particular person's covert information.

503
01:07:32,860 --> 01:07:38,530
Your covert information may be different from my you know, and we may have different coverage structure.

504
01:07:38,920 --> 01:07:43,270
So that's the second way or second reason why stigma needs to be.

505
01:07:43,270 --> 01:07:50,650
And that's why. Bullet point number four, they indeed can be the same with the same number of measurements.

506
01:07:50,860 --> 01:07:53,980
If we do not have any covers to be incorporated into the model.

507
01:07:57,310 --> 01:08:07,920
And. Often we will need to specify the Sigma II structure through some general distribution of assumptions.

508
01:08:08,490 --> 01:08:13,440
You have learned in 650 a very prominent distribution called Gaussian distribution.

509
01:08:13,980 --> 01:08:20,840
I often don't like to use the word normal because if you say something as normal, it means other things are abnormal.

510
01:08:20,850 --> 01:08:25,470
Right? But did you really believe Gaussian distribution is normal?

511
01:08:26,400 --> 01:08:32,850
Different people have different opinions, but in this class I often refer to these normal distributions Gaussian.

512
01:08:32,850 --> 01:08:36,690
But. There may be some situation there used interchangeably.

513
01:08:36,720 --> 01:08:40,890
Just want to give you a guidance of how to read these sites.

514
01:08:44,520 --> 01:08:51,390
We will be doing some technical derivations about multivariate Gaussian distribution.

515
01:08:52,260 --> 01:09:01,739
The goal is really trying to extend what you've learned in 650 to a setting where you have to talk about many goals and variables together.

516
01:09:01,740 --> 01:09:09,030
And there will be mean structures, there will be variance, covariance matrices, and that is totally a different level of operations.

517
01:09:09,420 --> 01:09:18,360
And there, I believe I have a recorded video I will share with you that that video and also we'll be talking about the technical aspects a little bit.

518
01:09:19,350 --> 01:09:27,270
Unfortunately, this part is about technical, but I do believe it is kind of the backbone of many of the distributions we'll be doing.

519
01:09:27,900 --> 01:09:31,470
So I'm going to say that you can do it and give it a bit effort.

520
01:09:32,400 --> 01:09:38,400
But we will be talking about multi year distribution, you know, not in this lecture, in the future lectures.

521
01:09:44,020 --> 01:09:49,600
So any questions? I want to make sure that I have some time for you to ask questions before I move on,

522
01:09:50,080 --> 01:09:53,560
because we only have like a few more slides left and that will be done for today.

523
01:10:04,340 --> 01:10:08,060
All right. Don't worry. If you have questions after class, just post them on Piazza.

524
01:10:08,360 --> 01:10:18,170
Happy to respond to them. The final part essentially is a kind of classification of the kind of model the activities will do in this class.

525
01:10:18,680 --> 01:10:24,470
And they basically represent different parameters we have been talking about.

526
01:10:24,830 --> 01:10:28,990
We talk about lots of Greek letters, right? The Beatles, the muse, the Sigma's.

527
01:10:31,490 --> 01:10:34,970
Yeah. This is also a Greek letter. Just so you know.

528
01:10:35,330 --> 01:10:40,580
So we talk about mu beta. How do we model each of these things?

529
01:10:41,240 --> 01:10:46,830
So we will be modeling the mean. We will also be modeling the variants coverings.

530
01:10:50,590 --> 01:10:54,940
So let's start with the main sorry, this is calling here.

531
01:10:55,780 --> 01:10:58,720
There will be two approaches to modeling the main.

532
01:10:58,930 --> 01:11:08,800
The first one is what we call analysis of profile response profiles, and it requires the data to be balanced, which is indeed a very ideal situation.

533
01:11:09,250 --> 01:11:17,409
And it works quite a well because it allows for arbitrary patterns and we will be talking about that.

534
01:11:17,410 --> 01:11:25,630
And that's going to build some intuition for the second sets of models, which we call parametric or semi parametric models.

535
01:11:26,590 --> 01:11:31,210
So these models are going to provide some parsimonious description of trends and cover defects.

536
01:11:32,650 --> 01:11:39,790
For example, in the previous data set you saw you have plotted the dots for four occasions for each of the treatment group.

537
01:11:40,150 --> 01:11:44,050
Do you believe they're in here or do one have a more flexible curve for that?

538
01:11:44,590 --> 01:11:51,910
So that's a place where we've got to introduce some assumptions, say a line would suffice or a quadratic curve would be good.

539
01:11:52,690 --> 01:11:56,530
So those are the questions about how do we select models for the main trend.

540
01:11:57,610 --> 01:12:06,160
Indeed, there will be some smoothing occurring, especially if we have a lot of data points or if we have unbalanced data points.

541
01:12:06,550 --> 01:12:14,290
And the benefit of doing so, as you see, is that we can reduce the number of unknowns in the curve if we assume here that.

542
01:12:14,290 --> 01:12:19,960
So I'm only going to need the intercept the slope for quadratic three parameters and so on and so forth.

543
01:12:20,320 --> 01:12:26,770
And this can work for say, ten measurements, which would be great because you're smoothing over the data points.

544
01:12:29,430 --> 01:12:35,970
And in this situation, we will need to distinguish two different sets of parameters.

545
01:12:36,510 --> 01:12:42,750
The first set of parameters is what we call substantive parameters or in general parameters of interest.

546
01:12:43,140 --> 01:12:56,150
Usually they come in the form of beta. In the example we saw that beta essentially is the treatment effect.

547
01:12:56,510 --> 01:13:02,030
Does the new drug change the rate with which the main level?

548
01:13:03,650 --> 01:13:11,450
Very, very over time. Right. So that is the primary parameter of interest.

549
01:13:17,560 --> 01:13:21,700
All right. Now, how about modeling the variance covariance matrix?

550
01:13:21,940 --> 01:13:27,370
So in general, there will be again a few different approaches.

551
01:13:27,970 --> 01:13:31,770
The first one is just trying to do the totally unstructured version,

552
01:13:31,780 --> 01:13:36,880
because if you have, say, five measurements per person and they will be how many pairs?

553
01:13:37,180 --> 01:13:40,360
Five. Choose two. Right. So this is a number we saw here.

554
01:13:40,780 --> 01:13:49,300
We see here. This is a quadratic thing anyway. And this works for measurements that were collected at a common set of occasions.

555
01:13:49,780 --> 01:13:55,150
And that's what we set up, balanced data. And if any calls of five.

556
01:13:55,630 --> 01:13:58,990
What's this number? Ten.

557
01:14:04,940 --> 01:14:09,440
How about any calls? Ten.

558
01:14:13,910 --> 01:14:17,880
45. Okay. How about 15?

559
01:14:26,750 --> 01:14:31,850
I'm going to pull out the calculator. My point is just trying to say that's actually huge.

560
01:14:32,750 --> 01:14:39,050
Not his, but bigger. Right.

561
01:14:39,470 --> 01:14:45,170
So, you know, if you have more and more data points, you will have a lot more parameters.

562
01:14:45,410 --> 01:14:49,520
Right. And you'd better have more observations than parameters.

563
01:14:50,060 --> 01:14:54,230
And if you're dealing with 15 measurements per person, then you have to at least have 100 or five people.

564
01:14:55,430 --> 01:15:03,460
And this is a problem with unstructured coherence, which is that if you are trying to be very liberal or trying not to be wrong,

565
01:15:03,590 --> 01:15:09,020
so you are trying to use the totally flexible core and structure there is going to be price you're going to pay.

566
01:15:09,410 --> 01:15:16,880
Which means that there are lots of unknowns introduced into the model. So to counter that, there are many different alternatives.

567
01:15:17,210 --> 01:15:20,210
The first one is called covariance pattern models.

568
01:15:20,660 --> 01:15:28,490
So these are based on two observations. That observation and far apart are less correlated and they in general have roots in time series modeling.

569
01:15:29,330 --> 01:15:35,690
Just to give you a sense, there is one structure, one pattern called a exchangeable.

570
01:15:39,890 --> 01:15:44,660
Which means the correlation can be written as like row, row, row for three measurements.

571
01:15:46,360 --> 01:15:53,330
All right. So this is a huge simplification compared to an otherwise unspecified appearance governance structure.

572
01:15:53,860 --> 01:15:57,160
Only one parameter for the correlation. Otherwise, you would need. How many?

573
01:16:00,630 --> 01:16:03,710
The more than one I want. Definitely. So, one, two, three, three. Right.

574
01:16:03,750 --> 01:16:07,920
So instead of three parameters for correlation, you have now one product for correlation.

575
01:16:08,280 --> 01:16:16,110
And if you recall, in the data example we saw earlier, the correlation was roughly point eight seems to be constant regardless of which pair of shoes.

576
01:16:23,050 --> 01:16:28,960
Like the one about him. So if you can save some number of parameters by assuming the correlation all constant,

577
01:16:29,620 --> 01:16:32,830
hopefully the data will estimate this is going to be around point eight.

578
01:16:33,250 --> 01:16:36,940
Then you are saving lots of degree of freedom to get that the correlation.

