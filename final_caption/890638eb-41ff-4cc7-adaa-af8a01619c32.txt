1
00:00:03,150 --> 00:00:13,650
Hello welcome to both 266 lecture for a liquidity select trolley about be recording to make it more smooth and compact.

2
00:00:14,550 --> 00:00:18,120
So please watch this version if you haven't seen one before.

3
00:00:18,120 --> 00:00:22,559
So today we're going to talk about daily frequency estimation method.

4
00:00:22,560 --> 00:00:26,640
In the first method, we're going to talk about this maximum diagram method.

5
00:00:27,090 --> 00:00:31,169
So maximum like is a general framework for estimating model parameters.

6
00:00:31,170 --> 00:00:35,969
And we find a set of primary values that maximize the probability of of data.

7
00:00:35,970 --> 00:00:44,700
So an interesting fact about this maximum likelihood is that we're just using the probability density or mass functions as a function,

8
00:00:44,700 --> 00:00:49,919
but consider them as a function of parameters rather than the theta outputs.

9
00:00:49,920 --> 00:00:53,910
Clean more in the next slide. And this is applicable to many different problems.

10
00:00:54,450 --> 00:00:59,999
So the likelihood is introduced artificially in the 1925 and it measures the goodness

11
00:01:00,000 --> 00:01:06,180
of the specific parameterization of this statistic statistical models to data samples.

12
00:01:07,170 --> 00:01:16,170
So basically the function itself, the form is exactly the same form with the joint probability distribution, PDF or PMF of the sample.

13
00:01:16,800 --> 00:01:21,420
But you're considering that as a functional parameter because a likely function

14
00:01:21,420 --> 00:01:26,580
assumes that you observe the data and you you assume that data is fixed by your time.

15
00:01:26,590 --> 00:01:30,360
It could vary. So that's the that's how you view the later function.

16
00:01:30,360 --> 00:01:35,780
So it's a function of parameters and it tries to render video but fixed value as the observed value.

17
00:01:35,800 --> 00:01:41,670
So you can live with the biostatistics test to material to have a comprehensive, comprehensive understanding.

18
00:01:41,670 --> 00:01:46,680
But here we're going to. Covered that part.

19
00:01:47,580 --> 00:01:51,570
So one example we can talk about is the likelihood of hourly frequency.

20
00:01:51,640 --> 00:02:00,870
So so you consider the sample of chromosomes and X of them are type A and the point of interest is the frequency.

21
00:02:01,020 --> 00:02:07,290
So here, if you look at these, this look at the binomial distribution, this this may look familiar.

22
00:02:07,290 --> 00:02:15,030
This is exactly binomial, probably this function. But in that case, we usually consider them as so and and t unknown.

23
00:02:15,250 --> 00:02:25,110
An X is a variable. So you usually see this is a function of variable x, but now we're taking this same function of form,

24
00:02:25,110 --> 00:02:28,350
but they're looking at a function of P rather than function of X.

25
00:02:28,410 --> 00:02:29,520
So that's the difference.

26
00:02:30,030 --> 00:02:38,970
And in this case, we observe if A was observed four times in the out of ten chromosomes, what is the likely daily frequencies?

27
00:02:39,060 --> 00:02:46,080
If you take this function in the show as a function of frequency P and see how that function behaves.

28
00:02:46,080 --> 00:02:51,300
So that's the the first step for the maximum likely estimation.

29
00:02:52,080 --> 00:02:59,820
So if you do that, you can figure out what what can be the maximum like estimate.

30
00:02:59,830 --> 00:03:04,020
So in this example, you can enumerate the some possible values,

31
00:03:04,020 --> 00:03:10,140
values of P and one minus P and you can calculate based on this situation, you can calculate the likely value.

32
00:03:10,470 --> 00:03:17,310
And these are the values. Among these values you can see that the p equal point four which happened to be actually four

33
00:03:17,310 --> 00:03:24,630
out of ten so exit X divided by and has the highest likely and among the values we try.

34
00:03:24,960 --> 00:03:32,140
But does it hold across all the values to understand that we need to throw this like

35
00:03:32,290 --> 00:03:40,650
function across across different parameter P so we can do this with the some exercise here.

36
00:03:40,980 --> 00:03:44,580
So, so let's allow the notable.

37
00:03:46,910 --> 00:03:58,180
And the first case we're trying to do here is that, you see, we are we consider this a likely function.

38
00:03:58,190 --> 00:04:05,479
So we're just making this function. And so including quoting is as a function here is a same form, this form exactly.

39
00:04:05,480 --> 00:04:13,380
Encoding the equation here. And after that, let's set at the parameter so and equal ten and X equal four and A,

40
00:04:13,400 --> 00:04:19,219
we're now trying hundred different values of piece and try to visualize that.

41
00:04:19,220 --> 00:04:23,510
So all these pairs would be quite good at the likelihood.

42
00:04:23,870 --> 00:04:27,910
Okay. And then we visualize the likely function here.

43
00:04:27,920 --> 00:04:41,640
So. So what we are doing is that we just make us quick dataframe and show the P by like function and see how they look like.

44
00:04:42,180 --> 00:04:48,370
And they the function look like this. Okay. And that's what that's what the next slide shows.

45
00:04:48,370 --> 00:04:58,200
So this is likely function. So like a function basically tells that are the data the likely.

46
00:04:58,440 --> 00:05:10,170
So. So this is the probability of this data observing when the true parameter is different value through time there's a different value.

47
00:05:10,180 --> 00:05:14,650
So then this value is exactly actually 0.4.

48
00:05:14,670 --> 00:05:25,890
So this is in the maximize at the peak or 0.4, but they also have a likelihood value value like non non-zero likely value in many other parameters.

49
00:05:27,400 --> 00:05:34,270
So we can ask this question. So these like a function tells that the data is most probably P 0.4.

50
00:05:34,300 --> 00:05:45,220
So in terms of if we just take this like a function as a, as a quantitative measure of how likely the data is well under different parameters,

51
00:05:45,230 --> 00:05:52,600
then most light was probably payment is equal to 0.4, but also P equal to 2.8 for peak or 0.2.

52
00:05:53,590 --> 00:05:57,459
It does have some likely value there.

53
00:05:57,460 --> 00:06:00,550
That means that there is a these are the possibilities.

54
00:06:02,290 --> 00:06:06,130
So maximum local estimation is basically as a two step.

55
00:06:06,160 --> 00:06:15,240
You write down the likely function with which likely function should be proportional to the to the actually probable probably tension.

56
00:06:15,250 --> 00:06:21,040
This function doesn't have to be exactly normalizing to one because we're not using

57
00:06:21,040 --> 00:06:25,900
the fact that the likely function integrates into one across the parameter values.

58
00:06:26,290 --> 00:06:34,840
No, you know, you can find a value that maximized the likely value then that that it becomes a maximum they could estimate.

59
00:06:35,260 --> 00:06:40,660
So in principle, this problem is applicable for any problem we are likely to function exist.

60
00:06:42,430 --> 00:06:48,610
So maximum that I could estimate is basically the procedure of finding the parameter that maximizes the likely function.

61
00:06:48,940 --> 00:06:53,019
And this is called MLI and this is basically optimization problem.

62
00:06:53,020 --> 00:06:57,610
When you optimize the selected function, you can analytically optimize it.

63
00:06:57,910 --> 00:07:02,290
If the closed form solution doesn't exist, you can try to numerically optimize.

64
00:07:05,950 --> 00:07:14,019
So if you wanted to find a political solution, how to do it so you can directly try to optimize the likely function,

65
00:07:14,020 --> 00:07:17,020
but usually like a function is a multiplicative form.

66
00:07:17,020 --> 00:07:23,469
So optimizing like a function usually is harder, so you can take a log of them.

67
00:07:23,470 --> 00:07:31,720
Then all the multiplicative form becomes some some a summation form in a maximizing likelihood is equivalent to maximizing low likelihood.

68
00:07:31,720 --> 00:07:41,770
So lower likelihood is a typically easier way to deal with and maximizing look alike function is a typical way to for the MLP problem.

69
00:07:42,340 --> 00:07:52,959
So and you can try to calculate the derivative of lower likely by taking the derivative and try to find the or find zeros for for this derivative.

70
00:07:52,960 --> 00:08:00,070
And that becomes that they will contain maximum they could lead you to be more specific.

71
00:08:00,070 --> 00:08:06,250
You can try to see the changes of the likelihood values and make sure that the likelihood that derivative

72
00:08:06,250 --> 00:08:12,610
actually changes from the negative positive values to negative that becomes the likelihood value.

73
00:08:12,610 --> 00:08:17,679
And you can try to use a second derivative to avoid any central point and so on.

74
00:08:17,680 --> 00:08:23,960
But those are more technical details. You don't you don't necessarily have to know about it.

75
00:08:23,980 --> 00:08:34,240
So let's consider three cases on how to calculate the maximum like estimates in the frequency estimation problem first.

76
00:08:34,930 --> 00:08:42,339
So this is a case where we observe each Jaleel then you can represent your data is as you know to end

77
00:08:42,340 --> 00:08:50,320
with the there's a and chromosomes available then this becomes binomial distribution by binomial PMF.

78
00:08:50,830 --> 00:09:00,309
But you can also represent as a, you know, without the entries in that you can represent the data as a sequential data with 001

79
00:09:00,310 --> 00:09:04,860
to a series of coin flips flips that doesn't that actually have order in the case.

80
00:09:04,890 --> 00:09:14,260
You want to represent the likelihood into this form these and these actually are the same except for there is a binomial coefficient and as I said,

81
00:09:14,950 --> 00:09:20,709
likelihood function shouldn't, shouldn't matter with this normalizing coefficient.

82
00:09:20,710 --> 00:09:28,960
So these are practical equivalent, but we're going to use the first form to maximize it in this to obtain the analytical solution.

83
00:09:29,530 --> 00:09:38,739
So we have this likely function and you can take the lower likelihood function in this way just by taking the lower the log of the previous situation.

84
00:09:38,740 --> 00:09:47,260
And you can take the derivative of this way and this is a constant with respect to P so it goes away in Excel in P becomes x,

85
00:09:47,260 --> 00:09:52,690
d by by P and this part becomes this. So you can, you can try to find the zero.

86
00:09:52,690 --> 00:09:59,200
So you just find the zero and they'll try to represent the P with respect x and then this is a, this is the root.

87
00:10:00,250 --> 00:10:12,040
And you can make sure that this is indeed maximizing the likelihood by, by making sure that the DC is there with is actually decreasing function.

88
00:10:12,040 --> 00:10:20,530
But I'll skip that part so that that part is not necessary for this class, although you may need it for the six or two.

89
00:10:21,520 --> 00:10:27,010
So another way to do through the optimization is you can.

90
00:10:29,260 --> 00:10:31,530
They use a numerical solution here.

91
00:10:31,540 --> 00:10:38,650
So even though this problem can have an analytical solution exist, some problem doesn't have an analytical solution.

92
00:10:38,660 --> 00:10:43,780
It is those who can find them only using numerical method in more general problem.

93
00:10:44,770 --> 00:10:53,409
So forcing this in this case is a single dimension optimization where you need to find the parameter.

94
00:10:53,410 --> 00:11:03,129
P It's a single dimensional. P So in this case you can use this optimize a function that is built in R two to apply the printer equation,

95
00:11:03,130 --> 00:11:06,880
which is the most widely used, a single dimensional optimization problem.

96
00:11:06,880 --> 00:11:18,640
So you can take the by a six by six or 615 to to learn about more about the numerical optimization that I'm going to show how you do it.

97
00:11:18,670 --> 00:11:23,590
So here, basically, we're trying to optimize this function, likely function,

98
00:11:24,130 --> 00:11:32,350
but I took the negative because most of the optimization function assumes that the problem is a minimization problem, that the maximization problem.

99
00:11:32,680 --> 00:11:38,139
So here we're just taking it's changing the sign to convert the problem into minimization problem.

100
00:11:38,140 --> 00:11:43,900
So when we when you feed the function, so optimize the function, take the first time it takes a function.

101
00:11:44,140 --> 00:11:49,120
Okay? Then the first function is a function you want to optimize, you want to minimize.

102
00:11:49,180 --> 00:11:53,169
So this is function and you can give a interval.

103
00:11:53,170 --> 00:12:00,670
So if you do this, basically it does give you the answer that the optimized the minimum parameter,

104
00:12:00,910 --> 00:12:06,819
which is a low frequency or is a point for, and that this is the negative log likelihood value here.

105
00:12:06,820 --> 00:12:10,149
So well, in this case, likely not lower likelihood.

106
00:12:10,150 --> 00:12:15,070
I actually try to use the likelihood, but you can also take the localized food if you want.

107
00:12:16,420 --> 00:12:21,459
Okay. So that's another way to optimize so you don't have to use this,

108
00:12:21,460 --> 00:12:27,850
but I'm just showing it showing you that the analytical solution and numerical solution converge in this case.

109
00:12:29,660 --> 00:12:37,700
So how do we evaluate so the the maximum like an estimate or so a maximum local estimate or has a

110
00:12:38,360 --> 00:12:46,819
properties of these three properties unbiased this consistency and efficiency in these three perspectives,

111
00:12:46,820 --> 00:12:52,790
Emily has a very good syntactic properties. So what it means that it's unbiased estimates.

112
00:12:52,940 --> 00:13:00,200
So if you have a maximum likely estimate, that means that the expected value estimate is same as a true value.

113
00:13:00,380 --> 00:13:04,100
So the Emily is always the function of observer data.

114
00:13:04,460 --> 00:13:14,240
So if the observed, you know, observed data can, you know, you can draw the observed theta multiple times from if you know the true parameters,

115
00:13:14,720 --> 00:13:23,270
then if you calculate these estimate in the expected value of them will be always the same as a true value.

116
00:13:23,270 --> 00:13:31,610
So that's, that's a very nice property and consistency means that if you have a large sample size,

117
00:13:31,910 --> 00:13:36,860
then the your estimate approaches a true value as more data is gathered.

118
00:13:36,860 --> 00:13:44,600
So in the, in the previous example, if you have a ten samples and if you have four, if the total time is 2.4,

119
00:13:44,930 --> 00:13:50,090
sometimes you can have your estimate could be .3.4 depending on what the X values are.

120
00:13:50,420 --> 00:13:52,459
But if the any is like million,

121
00:13:52,460 --> 00:14:04,090
then it the Emily estimate should should be very should converge to very close 2.4 because that is a consistent estimate,

122
00:14:04,250 --> 00:14:09,790
which should give a lot that quick value approach to the other large size.

123
00:14:10,520 --> 00:14:15,829
And the third is the efficiency. So variance of estimate is that at the theoretical lower bound asymptotically.

124
00:14:15,830 --> 00:14:25,250
So if you have a larger sample size, then your the variance is error is as possible, as small as possible.

125
00:14:25,250 --> 00:14:35,149
So then you can that it's a it's hard to find a better estimate here than Emily in terms of the efficiency.

126
00:14:35,150 --> 00:14:45,260
So you can you can not the the standard in other words, standard error of the estimate is pretty, pretty precise.

127
00:14:45,590 --> 00:14:49,100
Okay. So you can look at those six or two material later.

128
00:14:49,100 --> 00:14:53,630
So if you have if you took six or two material, this should be familiar with you already.

129
00:14:53,840 --> 00:14:57,680
If you haven't in that, these are a little bit new, but you will eventually learn this.

130
00:14:59,010 --> 00:15:08,100
So summary though, this natural estimate here where you have just a x divided by NN excuse count the proportion of a sequence of

131
00:15:08,100 --> 00:15:16,049
particular type and the MLA gives the ideal solution here and MLA provides a justification on why you should.

132
00:15:16,050 --> 00:15:19,020
The natural estimate are when you calculate the frequency.

133
00:15:19,080 --> 00:15:28,260
So you may take a greater Oh, I'm going to use x divided by and as my estimate and if somebody asks the Y now you can use the maximum like estimation.

134
00:15:28,260 --> 00:15:31,709
It's one of the justification. Okay, let's move on.

135
00:15:31,710 --> 00:15:43,110
So we consider the class case where we observe the individual chromosomes and this doesn't seem tricky, but what if we observed the genotypes?

136
00:15:43,380 --> 00:15:47,010
How about we observe parent offspring three So what's the difference?

137
00:15:47,670 --> 00:15:56,399
So second case is when we observe the genotypes of we're going to use a notation and over AJ as a number of genotypes.

138
00:15:56,400 --> 00:16:06,210
So if you have a homogeneous level 111111 genotype, this is heterozygous and this is homozygous genotypes.

139
00:16:06,660 --> 00:16:10,350
And if you have a genotype count, you can also calculate the genotype frequency.

140
00:16:10,360 --> 00:16:16,559
So now these is observed data. So in this case, how do you calculate the maximum that I could estimate?

141
00:16:16,560 --> 00:16:26,290
So there is a question well before doing that, you can actually if you have a genotype count, you can't you can't have early counts.

142
00:16:26,290 --> 00:16:30,570
So you can say that, oh, I observed this genotype.

143
00:16:30,580 --> 00:16:41,340
So then one one genotype contains a2l2, a two, observational one and a one hetero heterozygous genotype is a one observation that little one.

144
00:16:41,340 --> 00:16:47,760
So you can currently count this way you can calculate the frequencies this way and the number of is to n.

145
00:16:47,760 --> 00:16:59,010
So one way to argue that oh well you can calculate this and little alu count and you can estimate the frequency based on the count.

146
00:16:59,010 --> 00:17:04,590
So that's a good argument. Is it is it true? Does it give us some answers that that's what we would like to check?

147
00:17:05,310 --> 00:17:10,650
So how do we do it? So basically you have a genotype data here.

148
00:17:10,890 --> 00:17:18,180
And to calculate the like estimate, you just need to calculate the likely function in this case.

149
00:17:18,280 --> 00:17:22,830
Right. So in this case, there is a two.

150
00:17:23,340 --> 00:17:28,049
So the question is estimating a live frequencies, but here you have a genotype frequency.

151
00:17:28,050 --> 00:17:33,120
So how do we use LTE frequencies to get the genotype frequencies?

152
00:17:33,300 --> 00:17:36,780
To do that, you need to make another assumption, which is how do you own KG?

153
00:17:36,780 --> 00:17:42,269
We learn the last lectures, so we use them.

154
00:17:42,270 --> 00:17:48,329
Then you can represent the likelihood it's a function of just the single parameter P and occlusal one minus P.

155
00:17:48,330 --> 00:17:51,950
Then you can represent the likelihood this way it.

156
00:17:53,070 --> 00:17:56,940
Then you can get the only solution here by taking the low likelihood.

157
00:17:57,600 --> 00:18:05,759
So local actually becomes a little more complicated because the, the, the equation is a little more complicated.

158
00:18:05,760 --> 00:18:13,170
But basically you can represent this as a function of P if you just change the Q to one minus P here.

159
00:18:14,380 --> 00:18:17,471
Uh. Just give me a second.

160
00:18:23,202 --> 00:18:30,402
Okay. So I paused the lecture a little bit because I realized that the equation was incorrect.

161
00:18:30,552 --> 00:18:37,902
So now you should see the qualification here and know hopefully this makes more sense.

162
00:18:38,592 --> 00:18:44,262
So this is a log likelihood function from this, like the function here.

163
00:18:45,552 --> 00:18:51,252
And you can calculate the derivative of this log like likely function.

164
00:18:53,302 --> 00:19:00,982
Here. By taking the derivative with with respect to P now you have a P that is and one divided by one plus two.

165
00:19:01,492 --> 00:19:04,882
So which is basically the net.

166
00:19:05,092 --> 00:19:09,472
This is natural l based on the natural count.

167
00:19:09,472 --> 00:19:21,022
But you can also represent there's a genotype count. So the actual MLP is indeed can is a same to the natural estimate are based on daily count.

168
00:19:21,382 --> 00:19:27,322
The reason is that these counts are sufficient statistic for calculating MLP.

169
00:19:27,322 --> 00:19:31,401
So I think you learned what the sufficient statistic is already so that there was

170
00:19:31,402 --> 00:19:37,972
underlying region and but these are these can be driven by the genotype count.

171
00:19:38,422 --> 00:19:48,022
Okay. So in the unrealized impulse type case, the natural estimate or where we count the proportion is based on the genotype count,

172
00:19:48,022 --> 00:19:57,202
gives a identical solution with the first case in maximum likelihood, provide just a justification of using the actual estimate.

173
00:19:58,192 --> 00:20:02,842
So we can do the same thing here is just to make sure that this is indeed actually working.

174
00:20:03,202 --> 00:20:08,842
So you can calculate that you know, the same the genotype is the likelihood the here.

175
00:20:09,202 --> 00:20:17,001
And you can you can have a little count genotype count that that is basically, you know,

176
00:20:17,002 --> 00:20:26,512
five individuals and you observe the one the four times and you can draw the like your function in this way.

177
00:20:26,692 --> 00:20:30,972
Okay. So then it's again, the maximizing and.

178
00:20:32,052 --> 00:20:39,232
When the PS a 0.4. So you may you may think that this is actually exactly the same as this, but there is a one difference.

179
00:20:39,612 --> 00:20:41,982
One difference is that the Y values are different.

180
00:20:41,982 --> 00:20:50,891
So the like it a value is becomes different because this quantifies a different, you know, probability.

181
00:20:50,892 --> 00:20:58,852
So these are probably the most function is self is different, but the relative proportion of them are actually incredibly the same because because of

182
00:20:58,872 --> 00:21:04,842
these are these are actually that represent the same amount of information basically.

183
00:21:06,102 --> 00:21:08,872
So it should be a function of frequencies.

184
00:21:09,132 --> 00:21:18,102
And the one interesting thing is that you can also change these as a zero for for example, because these still should give the same l l account.

185
00:21:18,492 --> 00:21:25,182
So still the minority count is four. And if you throw this, you're I'm sorry to four and one.

186
00:21:25,542 --> 00:21:29,922
So in this case I'm sorry you need to keep the total number of individual same.

187
00:21:30,462 --> 00:21:34,682
Then you see the same. Value here.

188
00:21:34,832 --> 00:21:40,892
Okay. So that that is one interesting point you may want to see.

189
00:21:41,312 --> 00:21:44,761
And you can obviously, you can do 0203.

190
00:21:44,762 --> 00:21:47,792
This this should still give a similar account.

191
00:21:48,242 --> 00:21:55,532
So you see different value. Just the y y axis becomes different, but the relative value becomes the same.

192
00:21:56,402 --> 00:21:59,402
So that was the one interesting aspects.

193
00:21:59,852 --> 00:22:03,802
So now a tricky part. So how about you have a parent offspring pairs.

194
00:22:04,262 --> 00:22:09,052
So if you have a parent opportunity opus prepares, it becomes a little trickier.

195
00:22:09,062 --> 00:22:14,552
So in that case, you have a different.

196
00:22:16,552 --> 00:22:24,771
You know you're pairs of the of three individuals of the possible observation of possible cases observation

197
00:22:24,772 --> 00:22:32,272
is that you can have a nine different possibility of a of a pair of apparent or pleasant genotypes,

198
00:22:32,572 --> 00:22:38,242
but among them eight. So if the parent type relative to and the offspring of one one, this is impossible.

199
00:22:38,242 --> 00:22:40,732
So this as a zero probability, this as a general problem.

200
00:22:40,852 --> 00:22:47,302
So you have seven different cases and for each of the pairs you date, they belong to one of them.

201
00:22:47,302 --> 00:22:52,161
So you can have a count of how many belong. Say you want a two and so, so on.

202
00:22:52,162 --> 00:22:57,982
Then you, you have this a seven dimensional count and that's your observed theta.

203
00:22:58,372 --> 00:23:07,191
Okay. Then you can now you need to calculate the what's the probability of each of the cases that that's a little trickier.

204
00:23:07,192 --> 00:23:12,562
But you can try to calculate the what is the probability if you assume that a low frequency is a P,

205
00:23:13,282 --> 00:23:19,492
so in a similar frequency of the ll1 is ap1 and the two is ap2.

206
00:23:19,912 --> 00:23:28,042
Then what is the probability of this case? For example, in this you can try to enumerate all possible parental pairs,

207
00:23:28,042 --> 00:23:34,842
but the one missing information here is that you don't have the maternal so well and the other the other parent genotypes here.

208
00:23:34,852 --> 00:23:42,442
So one way to think about this is that this is a p square and that this is to be two and that this is a Q square.

209
00:23:42,892 --> 00:23:49,131
And but you're inheriting only one genotype, a11l to the offspring and the other.

210
00:23:49,132 --> 00:23:52,432
The offspring is just a sample to from earlier frequencies.

211
00:23:52,852 --> 00:24:01,431
So that's that's one way to think about it. So in this case, this property is ap1 square, but the other ones should be sampled from the population.

212
00:24:01,432 --> 00:24:07,192
So under the p one comes in here and save the P one square and the p two are the in this case the P,

213
00:24:07,342 --> 00:24:12,022
the only two must be it inherited for the other from the other parent.

214
00:24:12,022 --> 00:24:15,772
So you multiply two and they fill them together. This becomes p one square.

215
00:24:16,222 --> 00:24:18,112
Same thing. You have ap2.

216
00:24:18,232 --> 00:24:26,931
So in this case, to pick one, p two among tucuman p two, you have a half a probability that those are a one is inherited here.

217
00:24:26,932 --> 00:24:38,902
So multiply point five and then that becomes a P1, P2 and a p1 l is should be also in notice of this P1 score at times.

218
00:24:39,232 --> 00:24:45,682
So in this way, you can construct this table and this is the probably at the beach possible observation,

219
00:24:46,132 --> 00:24:53,692
then you can use this multi nominal probability again and to to represent l frequent represent the likelihood.

220
00:24:53,962 --> 00:24:57,682
It's much more complicated now. So this is the likely model.

221
00:24:57,952 --> 00:25:04,462
You should we should write that you can write everything is a function of p one because appp2 is a one minus p one.

222
00:25:05,702 --> 00:25:08,751
And. Then this.

223
00:25:08,752 --> 00:25:16,072
This becomes all the parameter you need. And this is ap1 hat you're going to be.

224
00:25:17,592 --> 00:25:24,912
The cut culturally. So one interesting fact is that now you actually have a four chromosomes.

225
00:25:25,182 --> 00:25:33,612
Right. But the if if you see the numerator denominator, it's accounting A1 three times, A2 three times.

226
00:25:33,672 --> 00:25:37,542
If you add the BPP and see eight three, three times in.

227
00:25:37,552 --> 00:25:41,202
So on a53, three times a63 times a73 times.

228
00:25:41,502 --> 00:25:52,642
But a four is counted only two times because these a four those are not have enough information to the as much information.

229
00:25:52,642 --> 00:26:00,191
There's other pairs to tell about the L frequencies because there is some ambiguity involved.

230
00:26:00,192 --> 00:26:04,661
So this is the interesting part. We can talk in the class more if you have questions.

231
00:26:04,662 --> 00:26:12,192
But the that that's very interesting part here so that this natural estimate in this case a natural estimate or

232
00:26:12,192 --> 00:26:18,761
where we count the proportion of badly parts of a type and Emmeline no longer keep identical solution in this case.

233
00:26:18,762 --> 00:26:22,542
We expect the Emily to be more accurate. So let's look at this.

234
00:26:22,902 --> 00:26:30,912
So the parent offspring pairs, let's say we observe that these pairwise genotype count, this is obviously more than five individuals.

235
00:26:31,572 --> 00:26:41,262
But let's look at these cases and you can backtrace the counter this way and you can print the value into three by three,

236
00:26:41,532 --> 00:26:47,261
count the matrix to just know that that was a parent or a parent of specific parents.

237
00:26:47,262 --> 00:26:51,492
You know what that looks like and the likely function looks like there.

238
00:26:51,512 --> 00:26:54,882
So we can just the code up this like a function this way.

239
00:26:56,212 --> 00:27:02,572
And we use a similar strategy to evaluate the likelihood in various way of various

240
00:27:02,572 --> 00:27:09,422
values of P So now we just evaluate the likelihood and maximum likely estimate.

241
00:27:09,892 --> 00:27:17,792
We're just plugging, plugging in the equation here and calculate the maximum likelihood frequency frequency in this case.

242
00:27:17,842 --> 00:27:24,442
I actually managed to come up with a contradiction to give our MLA P point four.

243
00:27:24,442 --> 00:27:27,442
So this is similarly is you could actually put in four.

244
00:27:28,222 --> 00:27:36,102
However, if we calculate a little a frequency, just assuming that these are independent samples,

245
00:27:36,112 --> 00:27:41,632
this is a night night naive estimate who you can calculate for the low frequencies.

246
00:27:41,992 --> 00:27:46,252
The naive estimate is not 2.42 37.5%.

247
00:27:46,702 --> 00:27:52,372
And let's see the likely function to see which one fits the data better.

248
00:27:52,912 --> 00:27:58,972
So and this is the likely function you can draw, and the blue one is the natural estimate over here.

249
00:27:58,972 --> 00:28:12,522
So blue is natural estimate in the red is. The Emily Emily in Emily is a it fits that with a likely function better so that that is a better estimate.

250
00:28:13,272 --> 00:28:22,512
Okay. Well, so you can do the same thing for the do the numerical optimization to make sure that this also is a maximize at the peak or 0.4.

251
00:28:23,882 --> 00:28:32,092
Okay. So that's the. That's the paradox in pairs and it's a has interesting aspects.

252
00:28:32,602 --> 00:28:37,372
And if you compare the variance of those three cases, it's more clear.

253
00:28:37,382 --> 00:28:46,052
So in the first example if you observe and single chromosomes a haploid that the number of observations end so you just the

254
00:28:46,282 --> 00:28:55,732
the the variances p q the by n if you have a diploid a number of submission is two n actually in terms of number of areas.

255
00:28:55,732 --> 00:29:02,482
So you divide by two it so that these and these are actually the same estimate is just a number of observations different but there are cases

256
00:29:02,482 --> 00:29:12,292
estimate is quite different because you actually have a for n haplotypes but only three and haplotype is useful to estimate the frequency.

257
00:29:12,622 --> 00:29:17,451
And the heterozygous heterozygous pair, which is a four is less informative than others.

258
00:29:17,452 --> 00:29:28,822
So it doesn't contribute to the to the bit reducing the variance as much as other other pairs of genotypes.

259
00:29:28,822 --> 00:29:34,492
So that that is a very interesting observation. And this is this is the variance.

260
00:29:37,542 --> 00:29:44,292
So now let's briefly talk about Emily, the numerical method.

261
00:29:44,592 --> 00:29:47,742
We're going to talk about the EMR specifically.

262
00:29:47,952 --> 00:29:54,222
So this is a very simple example. So now let's assume that we don't observe the genotypes.

263
00:29:54,262 --> 00:29:58,751
Okay. The cases that we assume that the random sample of a hundred individuals,

264
00:29:58,752 --> 00:30:04,812
we do not observe the types directly, but we observe phenotypes in the phenotypes of recessive.

265
00:30:04,932 --> 00:30:11,652
So we know that the four individual expresses a recessive phenotype assertion that the phenotype is controlled by a single variant.

266
00:30:12,132 --> 00:30:20,921
In that case, well, you can guess that the well at the probably genotype frequency of the the research phenotype is 4%.

267
00:30:20,922 --> 00:30:28,452
So you may think that, oh, but you had one bacterium where maybe the daily frequency is up 20%.

268
00:30:28,602 --> 00:30:33,102
Is that right? Okay. So how do we do it with a maximum likely estimate.

269
00:30:33,942 --> 00:30:39,192
And one way to do it is a year ago. And when you have an observed genotypes.

270
00:30:40,212 --> 00:30:44,802
So in this case, the reason why this problem is harder is because there's a missing data.

271
00:30:44,922 --> 00:30:52,542
So in the when you have a missing data, calculating the analytical solution is not always feasible in this case.

272
00:30:52,892 --> 00:30:56,592
It may be possible in general, and it's not always.

273
00:30:57,042 --> 00:31:03,942
So let's say theta is of parameters to be estimated. An X is observed theta and z's in unobserved theta.

274
00:31:04,182 --> 00:31:12,012
Then this is a complete theta likely, which is usually analytically tractable if you have both observed and unobserved theta.

275
00:31:12,372 --> 00:31:19,062
But the likelihood that the data you're actually need to optimize the parameter is this because unobserved data is not observed.

276
00:31:19,272 --> 00:31:22,752
So you need to calculate the likelihood based only on the observed data.

277
00:31:23,022 --> 00:31:29,982
And it has a D summation form which is a usually makes the equation much more complicated and harder to track.

278
00:31:30,582 --> 00:31:36,252
So finding Emily for this data analytically is a harder and often intractable.

279
00:31:37,242 --> 00:31:44,502
So how do we solve this problem? We use the expectation maximized scenarios in which it's called the M algorithm to solve this.

280
00:31:44,892 --> 00:31:53,072
So this is a this is a covered in six or two later in the lecture theoretically

281
00:31:53,082 --> 00:31:58,512
so an empirical and the actual computation aspect is a couple of is 615.

282
00:31:58,782 --> 00:32:04,932
But we're going to go through a really quick overview of this which and so let's consider a set of starting parameters.

283
00:32:05,382 --> 00:32:12,011
And the strategy is that you start with the some initial part of me that doesn't doesn't have to

284
00:32:12,012 --> 00:32:20,442
be necessary accurate and you use these guess estimated parameters to estimate the complete data.

285
00:32:20,442 --> 00:32:25,602
So there's unobserved data in the try to estimate the observed data and use these

286
00:32:26,592 --> 00:32:33,552
estimated complete data to update the parameters and keep keep doing this steps.

287
00:32:33,912 --> 00:32:40,152
This may feel very new to me and very strange to me if you haven't used the EMR routine.

288
00:32:40,182 --> 00:32:44,442
But this works pretty well actually. So how does it work?

289
00:32:44,442 --> 00:32:47,831
So step one is basically starting a parameter,

290
00:32:47,832 --> 00:32:52,332
so we can start with a variable point of view and let's say just I'm going to start

291
00:32:52,962 --> 00:32:56,952
the start the so with the assumption that the live frequency is between five.

292
00:32:57,192 --> 00:33:02,832
Okay. And how does it work? So let's start let's try this.

293
00:33:03,132 --> 00:33:11,802
So first, I'm going to assume that the frequency 2.5 and you know, instead what you're going to do is that what I do not know is that I know that,

294
00:33:11,802 --> 00:33:16,512
that the genotype frequency of the recessive genotype, I don't know the,

295
00:33:18,172 --> 00:33:24,002
the frequency of go two to the eight, the homozygous genotype and heterozygous genotype for it.

296
00:33:24,052 --> 00:33:27,851
It involves the dominant delta. So how do we estimate that?

297
00:33:27,852 --> 00:33:32,171
So you can use those or use this parameter.

298
00:33:32,172 --> 00:33:40,932
I'm going to tell you frequencies point five and try to use a Bayes theorem to estimate the the unobserved value.

299
00:33:40,932 --> 00:33:50,352
That's what you do. So in this case, what you need to do is that I need to know the relative frequency of these a

300
00:33:50,532 --> 00:33:56,412
genotype in a had a homozygous genotype in the the heterozygous genotype here.

301
00:33:58,032 --> 00:34:07,032
And you can use this definition of conditional the probability to estimate the probability of each genotypes in this way.

302
00:34:07,812 --> 00:34:15,221
So because you know the probability of small a similar genotype already so you just need to calculate

303
00:34:15,222 --> 00:34:23,172
this relative R norm so relative of a frequency of these 96 normal individuals in this way.

304
00:34:23,682 --> 00:34:23,882
Okay.

305
00:34:24,732 --> 00:34:36,642
So if you do the first iteration, if the if you assume that this LV frequency point five basically are the frequencies of 25% and 50% for this one.

306
00:34:37,092 --> 00:34:45,072
So what he means that the a capital AA in the heterozygous genotype is a lovely 1 to 2 ratio between them.

307
00:34:45,372 --> 00:34:52,451
So if you when you divide these 96 individuals that the first round you're going to have a 64 heterozygous

308
00:34:52,452 --> 00:34:58,841
genotypes in the top 32 dominant genotypes because the 1 to 2 ratio and you can keep to ten,

309
00:34:58,842 --> 00:35:07,992
then you can we estimate a little frequency based on this. Now your new early frequencies that you're you're observing 68.

310
00:35:08,952 --> 00:35:15,502
So you know that again. So for four recessive genotypes, so eight alleles and 64 least.

311
00:35:15,522 --> 00:35:21,671
So 70 to 72 L is observed out of 200 LDL.

312
00:35:21,672 --> 00:35:25,422
So your dual frequency is 36% then.

313
00:35:25,422 --> 00:35:29,352
Now through this table, again with a low frequency, 36%.

314
00:35:29,592 --> 00:35:38,052
Now you're going to have a new genotype count. And you can we estimate the low frequencies and that's that's what you can do.

315
00:35:38,052 --> 00:35:42,822
So this is my step doing so. Mr. You're calculating this at the.

316
00:35:44,042 --> 00:35:50,942
We estimate in early frequency and feed into step to this equation and keep keep going between them.

317
00:35:51,752 --> 00:35:59,792
If you do this strategy, if you keep doing this, what's going to happen is like this.

318
00:35:59,792 --> 00:36:08,152
I'm going to probably show in the actual notebook. So the way how you quoted is that you this is just as showing how the likely changes.

319
00:36:08,152 --> 00:36:12,962
So at this part this is not necessary part. In this part these are checking the.

320
00:36:14,702 --> 00:36:21,722
Checking the convergence of the algorithm to figure out the stopping criterion is that this is not the core part.

321
00:36:21,842 --> 00:36:30,812
The core part here is each step where you need to calculate these two two probabilities here.

322
00:36:30,942 --> 00:36:43,742
Okay. So the two two probabilities of a probability of the two of you two genotypes and you re just you re estimate the expected count of our capital,

323
00:36:43,772 --> 00:36:49,292
a capital genotypes in heterozygous genotypes, assuming your parameter estimate is correct.

324
00:36:49,652 --> 00:36:57,242
Now initially you start with a very low parameter, but you anyway use it to estimate your count.

325
00:36:58,232 --> 00:37:04,772
And these are just, you know, you're keeping the track of them all the time, you know, just to visualize the result.

326
00:37:05,282 --> 00:37:09,962
And after that, you update your P of A and we estimate this.

327
00:37:09,962 --> 00:37:16,292
So these two line and this line is basically a call line for the E step and step respectively.

328
00:37:17,162 --> 00:37:27,992
So that is the code. Okay. And if you try to run this algorithm here, this is a table you are seeing and it's shown also here.

329
00:37:28,292 --> 00:37:39,062
Okay. So at the first round, the P is 0.5 and this is 32 and 64.

330
00:37:39,242 --> 00:37:45,601
Next time P that at the point 36 and the genotype is more homogeneous, you know,

331
00:37:45,602 --> 00:37:52,172
type account increases because a little frequency decreases for the recessive a little

332
00:37:52,712 --> 00:37:58,712
and it goes down and down and it basically converges at a low frequency of 20%,

333
00:37:58,712 --> 00:38:01,922
which we conjectured initially. So without.

334
00:38:03,942 --> 00:38:08,502
Without using the hardware and buggy calibration across all three genotypes.

335
00:38:09,882 --> 00:38:21,142
The together we just assumed that the won't be clearly works for just you know just just for this these two genotype.

336
00:38:21,162 --> 00:38:37,752
This still works nicely and it converges to the the as conjectured earlier frequencies so to do summary so we looked at the agent as a maximum

337
00:38:37,752 --> 00:38:46,212
likely the likely estimate and we looked at the low frequency estimation using the maximum naked estimates for these three different cases.

338
00:38:46,632 --> 00:38:52,282
And we also look at the very brief overview of the algorithm to understand how the CMA

339
00:38:52,382 --> 00:38:57,402
region works when you estimate the frequencies for the recessive values without genotypes.

340
00:38:57,702 --> 00:39:08,082
So we can give you some of these in the in-person lecture and do more exercise regarding the EMI, which in the class.

341
00:39:08,382 --> 00:39:10,062
Thank you for your attention by.

