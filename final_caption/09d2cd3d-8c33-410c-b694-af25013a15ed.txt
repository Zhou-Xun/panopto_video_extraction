1
00:00:00,960 --> 00:00:05,820
I realize the club started at 41.

2
00:00:06,240 --> 00:00:10,170
All right. All right, everybody.

3
00:00:10,170 --> 00:00:18,840
Happy Monday, Monday, Tuesday, Wednesday, Mike.

4
00:00:21,740 --> 00:00:29,630
And it's Wednesday. All right, let's come together here. Exam grades, as I told you, are released.

5
00:00:30,360 --> 00:00:33,460
You have how I graded the exam.

6
00:00:33,480 --> 00:00:37,200
I tried to be as generous as I could trying to help folks out.

7
00:00:38,040 --> 00:00:44,760
If you feel that I was mean or wrong, I would be happy to discuss it with you.

8
00:00:46,180 --> 00:00:49,770
Um, I don't want to go through what.

9
00:00:49,950 --> 00:00:53,010
What the why the right answers. Were they what they were.

10
00:00:53,820 --> 00:00:57,890
Right now, I'm not prepared to do that with everything else that we're doing up.

11
00:00:59,820 --> 00:01:04,920
But we will talk about specific problems on Friday, should you all wish to do so.

12
00:01:07,300 --> 00:01:11,410
That's what I want to say about the grades overall. I think we're very good. I know they were very good.

13
00:01:11,950 --> 00:01:17,380
There were few individuals who were on the left tail there. We want to bring those scores back up.

14
00:01:19,060 --> 00:01:28,150
Otherwise, I thought the hardest problem was the three pictures and telling me which correlation

15
00:01:28,150 --> 00:01:31,240
structure was demonstrated or whether there was an aggressive independence.

16
00:01:31,990 --> 00:01:36,450
Um, that was probably the one that was answered at least.

17
00:01:36,460 --> 00:01:40,690
Right? And maybe folks who got it right hunted down.

18
00:01:41,620 --> 00:01:55,950
So I think we need to go through that, but not right now. Other concerns, thoughts I saw most people took most of the 90 minutes to do it.

19
00:01:55,960 --> 00:02:05,790
So true. No. Some people say no, but I'm trying to figure out for the next test if this was the appropriate length.

20
00:02:07,480 --> 00:02:12,610
Yeah. No one was screaming in 90 minutes that they weren't ready to to have a close.

21
00:02:12,650 --> 00:02:16,190
Okay. All right.

22
00:02:16,210 --> 00:02:22,060
Exam one's done. It also helps me understand because I you know, I see a little bit of your analysis skills in the homework assignments,

23
00:02:22,060 --> 00:02:27,400
but I don't really get to see how you understand some of the concepts and this is the way I'm doing it this semester.

24
00:02:27,400 --> 00:02:30,860
So it's helpful for me to. To see those.

25
00:02:31,190 --> 00:02:34,970
See whether we're on the right track. All right.

26
00:02:35,150 --> 00:02:39,790
We will have one more test. With that.

27
00:02:39,790 --> 00:02:46,120
Let's talk about the calendar for the semester. I know you had a homework to do next Wednesday.

28
00:02:47,040 --> 00:02:51,840
And we haven't even really talked about linear mix models except for the crappy job I did last week.

29
00:02:53,920 --> 00:02:57,810
The calendar has been I've tried to change it. I've been spending a lot of time this morning.

30
00:02:59,370 --> 00:03:05,130
Number three, it's the calendars, been updated in canvas, all the due dates and so forth.

31
00:03:06,140 --> 00:03:10,230
Number three, it's going to mess up my Wednesday and Friday plans.

32
00:03:11,300 --> 00:03:14,570
We've got to talk about linear mix models today on the surface.

33
00:03:15,590 --> 00:03:19,040
Talk a little bit about our and how to do number three on the seven.

34
00:03:19,910 --> 00:03:22,940
I want to finish our linear mix models on the 12th.

35
00:03:25,610 --> 00:03:29,810
So that means you should be ready to finish the homework sometime around the 14th.

36
00:03:31,750 --> 00:03:35,620
Because I'm such a nice guy. I said, let's give them the weekend.

37
00:03:35,620 --> 00:03:48,160
Plus, I have office hours on Monday, the 17th. So you have until the 17th at 5 p.m. I think is the official time that homework number three is due.

38
00:03:50,620 --> 00:03:53,710
I might be wrong. It was the 17th during fall break or.

39
00:03:53,740 --> 00:04:01,420
Yeah, it was a small break. Well, then I'll have to make it to 15th, not forget I asked.

40
00:04:03,670 --> 00:04:09,130
That is fall break and I'm happy to make it to the 19th.

41
00:04:09,880 --> 00:04:14,500
Yeah. Okay. That means I am not moving the next homework assignment, the fourth one.

42
00:04:15,040 --> 00:04:18,249
So keep that in mind. You've got a lot of time for homework number three.

43
00:04:18,250 --> 00:04:23,860
But number four is going to come, right? So homework number four is due on the 26th.

44
00:04:23,860 --> 00:04:26,919
A week later, I will change that.

45
00:04:26,920 --> 00:04:33,549
I totally forgot about the break. All right. But this homework, I think making this homework due next Wednesday is really not going to work.

46
00:04:33,550 --> 00:04:39,220
Well, you know, you're just going to do the homework and not really get much time to absorb the concepts.

47
00:04:39,220 --> 00:04:51,100
So. All right. So I will change that to be the 19th if you all have gone to wherever you're going to go, that means the second exam.

48
00:04:52,210 --> 00:04:57,190
My plan was to have one exam at the end of every month. Well, that's not going to work.

49
00:04:59,320 --> 00:05:05,110
So Test two is now on the 11th of November, a couple of weeks later.

50
00:05:08,440 --> 00:05:13,510
I also want to point out for in October, there is no class on the 21st at all.

51
00:05:14,710 --> 00:05:19,360
I have an all day retreat with some university stuff that I have to go to.

52
00:05:19,360 --> 00:05:24,459
So it's in the syllabus, but I don't think it was in to canvas yet or I didn't see it.

53
00:05:24,460 --> 00:05:36,340
So 21st there is no class. So that kind of also gets gets into making thing is to take your second exam November 11th, same process.

54
00:05:39,100 --> 00:05:41,320
Is it valuable to not have class that day?

55
00:05:44,040 --> 00:05:52,169
I'm happy to meet and learn more material since we would normally be sitting in the classroom if it were in a class exam.

56
00:05:52,170 --> 00:05:55,420
Anyway, I am giving you those days, not the lecture.

57
00:05:55,440 --> 00:06:00,329
Okay, so no lecture on the 11th because you're going to have the same sort of format test opens

58
00:06:00,330 --> 00:06:08,069
on Friday and you have until Tuesday to finish it up and then Thanksgiving comes as usual.

59
00:06:08,070 --> 00:06:15,360
And then we have no time left because there's like two weeks to cover everything else and then a sample of the semester.

60
00:06:16,560 --> 00:06:22,580
All right. So I think the only real duties that changed were homework three and exam number two.

61
00:06:22,590 --> 00:06:26,010
Everything else I've had to change again.

62
00:06:26,010 --> 00:06:33,750
My plan was Wednesday lecture. Friday are going to have to change that up a little bit right now to get back on track.

63
00:06:34,590 --> 00:06:39,060
But you all come every day, so that's irrelevant, right?

64
00:06:39,120 --> 00:06:47,430
Right. Okay. So homework number three is not due for a while and definitely not due next Wednesday

65
00:06:48,810 --> 00:06:52,950
and I'm sure some time will also appreciate not creating an overflow break.

66
00:06:53,020 --> 00:06:58,290
So everybody wins, right.

67
00:07:01,560 --> 00:07:04,860
So have you heard the story about the NYU professor who was fired?

68
00:07:05,790 --> 00:07:08,880
Yes. What do you think?

69
00:07:09,030 --> 00:07:16,829
No. Some people say what, he was fired because he taught an organic chemistry class and the grades were

70
00:07:16,830 --> 00:07:21,270
too harsh and students were worried they were no longer to get into med school.

71
00:07:22,380 --> 00:07:27,840
So they complained and he was fired. Notice what I'm saying now?

72
00:07:30,240 --> 00:07:35,010
He was a tenured professor at Princeton and was retired and was now teaching at NYU.

73
00:07:35,010 --> 00:07:40,440
I think simply to teach, earn some money or have fun, whatever he was in his retirement.

74
00:07:42,330 --> 00:07:46,380
So it's an intriguing issue in academia.

75
00:07:48,870 --> 00:07:57,330
How do we make you guys happy but in a way that still maintains integrity of of what we do.

76
00:07:57,510 --> 00:08:01,440
So go check that out. The Internet is here if you're interested.

77
00:08:02,040 --> 00:08:06,390
There's got to be more to the story. I can't imagine that was the only reason he was fired.

78
00:08:07,530 --> 00:08:17,550
Usually really mean professors are really mean people. And he probably was, you know, a bad side for somebody else as well who had more power anyway.

79
00:08:18,900 --> 00:08:24,630
But the professors are a little concerned if that's going to set a precedent for other places

80
00:08:26,100 --> 00:08:31,920
to use that sort of information and how we get promoted and are kicked out of the university.

81
00:08:34,110 --> 00:08:38,009
So that's intriguing as I give you your exam grades.

82
00:08:38,010 --> 00:08:43,650
Right? Um, all right.

83
00:08:44,820 --> 00:08:50,280
Let's keep going on linear mixed models, then. I don't think I had any other things on my list.

84
00:08:51,390 --> 00:09:00,840
Uh, today is Yom Kippur, which is the day in which Jews fast to atone and get ready for the next year.

85
00:09:00,840 --> 00:09:04,640
And, uh. I am not Jewish, but my family is.

86
00:09:05,540 --> 00:09:12,230
So I felt in support of them and I'm a little loopy.

87
00:09:14,240 --> 00:09:20,360
So we'll see how this goes today. I'm trying to remain focused here.

88
00:09:20,840 --> 00:09:26,479
All right. Again, we've talked about modeling the correlation structure strictly through the error

89
00:09:26,480 --> 00:09:33,110
of structure so exchangeable or regressive others that you might want to use.

90
00:09:34,220 --> 00:09:37,430
Now we're going to start talking about a different way to approach correlated data.

91
00:09:39,840 --> 00:09:44,250
Oh, my gosh. Start beginning.

92
00:09:45,180 --> 00:09:53,040
All right. So I did what all bad professors do, and they try to cram the last minutes of their class into anything they possibly have.

93
00:09:53,100 --> 00:09:57,180
Instead of just going away. So I flew through some of these slides.

94
00:09:57,570 --> 00:10:01,380
And, boy, I didn't even know what I was talking about. So let's step back again.

95
00:10:03,150 --> 00:10:06,150
This was just a review of what the generalized these squares model was.

96
00:10:06,270 --> 00:10:09,330
We've all been looking at this. We have a regression model.

97
00:10:10,260 --> 00:10:15,840
There are fixed effects where I've been going, fixed effects and there are mirrors.

98
00:10:16,050 --> 00:10:19,050
And now the errors within a person are possibly correlated.

99
00:10:19,440 --> 00:10:22,950
There's a vector of them and they have some correlation structure.

100
00:10:23,370 --> 00:10:30,989
And so far we've been putting correlation structure on those errors again, to help with our inference, less so than our estimation.

101
00:10:30,990 --> 00:10:34,200
Estimation is consistent regardless of what we use.

102
00:10:36,810 --> 00:10:43,560
What we have been implicitly talking about, though, is that there are now two components of variability in our data.

103
00:10:44,730 --> 00:10:49,410
When we had one observation per person, we could only look at how people varied among each other.

104
00:10:50,600 --> 00:10:54,680
But now when we get repeated measures on each individual, we can see how how they are.

105
00:10:56,540 --> 00:11:01,280
Yes. How observations vary within the same person. Why do my values change over time?

106
00:11:02,540 --> 00:11:05,810
Relative to why are my observations different from somebody else's?

107
00:11:05,830 --> 00:11:10,430
So between subject variability and within a subject or with an individual variability.

108
00:11:11,870 --> 00:11:16,850
We're going to start talking about these two pieces because now we can estimate them with repeated measures.

109
00:11:17,150 --> 00:11:26,630
So labor pain data, we've been looking at these data again, forgot to tell you, I added a little bit of a few few more slides to the slide deck.

110
00:11:27,290 --> 00:11:31,429
There are simply plots. Again, I felt like I wasn't explaining things well.

111
00:11:31,430 --> 00:11:33,770
And when I want to look back and say, well, maybe we can use the picture.

112
00:11:33,770 --> 00:11:39,380
So if you had the old slide deck, there are going to be a couple of slides that just have additional plots.

113
00:11:39,680 --> 00:11:44,009
But I haven't changed the text. Again.

114
00:11:44,010 --> 00:11:51,840
Here is a spaghetti plot looking at each individual trajectory of the pain scores for the placebo women of the control group women.

115
00:11:52,500 --> 00:11:59,130
And then I changed this plot to simply superimpose if I fit a least squares model.

116
00:11:59,190 --> 00:12:02,220
Assumed independence. Right. What we could do in G was jealous.

117
00:12:02,550 --> 00:12:05,370
I don't care about the standard errors right now. I just want estimates.

118
00:12:05,940 --> 00:12:15,690
So I set a essentially an l am with a group variable time as categorical and then an interaction of time and group.

119
00:12:16,080 --> 00:12:21,450
So what I've done is essentially set a mean at each time points and then I connected them.

120
00:12:21,450 --> 00:12:25,710
So it isn't a curve, it's a bunch of lines connected with each other.

121
00:12:26,400 --> 00:12:34,380
And so, again, we see for the population of placebo women, there seems to be a general upward trend in their pain scores.

122
00:12:34,770 --> 00:12:39,960
And they start out at about a 20. And they reach about 80 by the time we get to three and a half hours.

123
00:12:41,370 --> 00:12:54,269
That's the population average. But I did this three times and shucks, can't quickly rotate.

124
00:12:54,270 --> 00:13:00,990
How can I quickly rotate in?

125
00:13:01,650 --> 00:13:07,290
Look at that. It was rotated on my Mac. But this one's big.

126
00:13:07,290 --> 00:13:11,580
It. It's not weird.

127
00:13:12,680 --> 00:13:16,140
It is. It looks fine on the one that you already uploaded.

128
00:13:16,170 --> 00:13:19,379
Yeah, I know. That's why.

129
00:13:19,380 --> 00:13:23,970
I don't know why this computer's decided that it needs to rotate them back.

130
00:13:25,500 --> 00:13:28,530
All right, we're going to go here from this slide. All right.

131
00:13:28,800 --> 00:13:35,400
So again, there is the population or our samples from the population of placebo women, untreated women.

132
00:13:35,850 --> 00:13:38,309
And so there is the population fitted average again.

133
00:13:38,310 --> 00:13:44,940
But we see some women in this group who are distinctly different from the population mean they both start out fairly low,

134
00:13:45,630 --> 00:13:50,550
the same intercept, but they have distinctly different slopes in how they increase with their pain over time.

135
00:13:51,880 --> 00:13:59,430
And so again, we typically use the mean curve as a way to predict or give a guess as to what a person's scores might be over time.

136
00:14:00,150 --> 00:14:03,120
And so that line isn't really a good description of these two women.

137
00:14:03,360 --> 00:14:07,770
There's something about these women that makes them different from the rest of the population.

138
00:14:08,970 --> 00:14:12,270
Don't have it necessarily. Right. It's latent. We talk about these things.

139
00:14:13,380 --> 00:14:18,660
And again, here was the skinny spaghetti plot for the women who were treated with pain medication.

140
00:14:18,900 --> 00:14:25,260
And the pain score was typically flat, maybe just a slight increase, but a much, much lower than the control women.

141
00:14:26,640 --> 00:14:30,299
But again, we can look at a couple of women and unfortunately it's blue instead of green.

142
00:14:30,300 --> 00:14:35,070
But these are two women in this group with distinctly different intercepts.

143
00:14:35,940 --> 00:14:42,720
They started out at half an hour at very different points, values of pain and then pretty flat.

144
00:14:42,810 --> 00:14:47,370
But we see maybe a slight change for 82 at the end, but again,

145
00:14:47,370 --> 00:14:52,560
distinctly different from the population average and distinctly different from each other.

146
00:14:53,670 --> 00:14:57,299
And so we're going to use the repeated measures to try and get at these differences and

147
00:14:57,300 --> 00:15:03,630
estimate them what is the deviation of this woman's intercept from the population?

148
00:15:03,960 --> 00:15:10,500
And maybe they have different slopes because again, as I had on my slides last week,

149
00:15:11,100 --> 00:15:18,810
we think of pain as being very subjective and there are just lots of inherent factors that we probably can't measure we can't even collect data on.

150
00:15:19,890 --> 00:15:27,000
That might explain why some women start out with more sensitivity to pain or experience pain in a different way over time.

151
00:15:27,720 --> 00:15:31,410
Again, we call these this collection of unmeasured things, latent.

152
00:15:32,100 --> 00:15:35,640
There are things we like to control for if we had them, but we cannot.

153
00:15:36,840 --> 00:15:44,730
And so we want to get around that. And what we do is we say that these latent factors come from a distribution in an average.

154
00:15:44,820 --> 00:15:48,030
There is no weight in effect. On average, it's mean zero.

155
00:15:49,230 --> 00:15:50,310
There's some variability.

156
00:15:52,410 --> 00:16:01,320
And for mathematical simplicity, we assume a normal distribution is it makes the math easy, at least in the setting right now.

157
00:16:02,970 --> 00:16:07,170
So normal. Do we believe that latent characteristics truly follow a normal distribution?

158
00:16:07,170 --> 00:16:10,590
I have no idea. But mathematically, that's what we assume.

159
00:16:10,860 --> 00:16:14,340
Just like we assume the errors are normally distributed right in a linear regression model.

160
00:16:15,840 --> 00:16:22,920
So we're trying to fit the following model. The ministry time is continuous now on this model, whereas I have in my plots, I did categorical,

161
00:16:23,220 --> 00:16:32,010
but there's a time effect, continuous variable for group and then possible slope differences between the two groups.

162
00:16:32,580 --> 00:16:42,330
And then we get the different descriptions and then the errors are normally distributed with mean zero and variance.

163
00:16:42,510 --> 00:16:46,649
Sigma Epsilon squared. So let's focus on beta, not up there.

164
00:16:46,650 --> 00:16:51,240
The Intercept. Again, that is where the population on average starts out.

165
00:16:53,010 --> 00:17:00,120
But we could think of that better not as actually being the mean of everybody's intercepts, their personal intercepts.

166
00:17:02,370 --> 00:17:06,689
The way we model everything is we say there's the population average, just like we do.

167
00:17:06,690 --> 00:17:10,290
There's everybody has a population average and then there's a deviation from it.

168
00:17:11,460 --> 00:17:15,360
So everybody has a deviation from the population intercept, bid or not.

169
00:17:15,930 --> 00:17:20,509
And those deviations again have mean zero and they have some variance.

170
00:17:20,510 --> 00:17:26,190
So I'm going to use Tao not squared, not for The Intercept because we're going to talk about random slopes in a second.

171
00:17:28,020 --> 00:17:33,090
And again, this quantifies how there is variability between individuals.

172
00:17:34,050 --> 00:17:37,590
How do individuals vary between their pain scores?

173
00:17:37,590 --> 00:17:41,100
And we're going to quantify it as a random intercept where they start out.

174
00:17:45,690 --> 00:17:49,230
So we modify the models to throw in a random intercept for everybody.

175
00:17:49,890 --> 00:17:54,000
So this is your first instance of what we call a linear, mixed model.

176
00:17:54,300 --> 00:17:58,590
It's a mixture of fixed effects and random effects. That's where the mixture comes from.

177
00:17:59,970 --> 00:18:06,000
The errors are random. You've always been fitting or a random thing, but you've never really done more with it.

178
00:18:06,170 --> 00:18:10,880
But so in this case, now we have a new component called Be Zero.

179
00:18:11,010 --> 00:18:16,320
And that is a random effect. And because we have both random and fixed effects, we call this a mixed model.

180
00:18:16,920 --> 00:18:21,780
It's linear, right? We have continuous outcomes. Everything is linear right now for the mean.

181
00:18:23,160 --> 00:18:31,530
We assume that everybody's random intercept is independent of the noise around the values, the errors.

182
00:18:32,970 --> 00:18:37,770
And we also right now, we're going to assume that the errors are independent within person.

183
00:18:37,890 --> 00:18:46,950
This is different from the generalized squares. The correlation is going to occur through the random intercepts, no longer through the errors.

184
00:18:47,490 --> 00:18:51,180
So after we have random effects, there is no more correlation.

185
00:18:51,790 --> 00:18:56,100
And right now, we're going to assume that the errors are independent within a person.

186
00:18:57,750 --> 00:19:01,200
So as I said before, we've partitioned the original error.

187
00:19:02,300 --> 00:19:04,580
When we did give us generalized squares,

188
00:19:05,090 --> 00:19:13,760
we separated into two pieces the variability between individuals through their intercepts and the DNA percentages through the errors,

189
00:19:14,240 --> 00:19:19,610
so that the total variation of all the pain scores is the sum of these two components.

190
00:19:19,910 --> 00:19:24,050
Again, you may hear variance components models. That's where that comes from here.

191
00:19:25,220 --> 00:19:30,680
But regardless of whether we use random intercepts or we do not, on average,

192
00:19:31,370 --> 00:19:41,180
a woman's pain score is exactly that linear model there because the errors have been zero and the random intercepts have mean zero.

193
00:19:42,440 --> 00:19:45,200
So we're still fitting the same population average model.

194
00:19:51,930 --> 00:20:02,220
Furthermore, the overall variability, as I just said in the YS is a component of the variability between and the variability within.

195
00:20:05,660 --> 00:20:13,889
Where do my own for go? Let's do a little algebra. Begin to realize in my slides I thought it might skip over the seltzer but algebra.

196
00:20:13,890 --> 00:20:22,020
But again, we're all at different places in our comfort with all of this stuff in the way there is.

197
00:20:24,370 --> 00:20:30,910
So just to be explicit here, Heinz score for subject I at times has variance.

198
00:20:32,810 --> 00:20:39,860
So that's variance of X to. Well, this was T first steps some I've already forgotten.

199
00:20:41,820 --> 00:20:47,390
Resume intercepts, plus a time effects.

200
00:20:47,510 --> 00:20:55,880
They don't want James Spader to group effects, plus the interaction of time and group.

201
00:20:57,910 --> 00:21:00,950
Right. So I just figured out why I was in my model.

202
00:21:02,420 --> 00:21:06,930
All the fixed effects have variance. Zero.

203
00:21:07,320 --> 00:21:10,980
They are not random. They are a number. They are a quantity.

204
00:21:11,910 --> 00:21:20,730
So all the facts the facts don't contribute to variance so much with the variance of beta not i plus epsilon i j +20.

205
00:21:23,840 --> 00:21:29,510
Plus two times the covariance of between I and AIG come.

206
00:21:32,570 --> 00:21:36,920
This is why I assume that the errors and the intercepts are independent.

207
00:21:36,950 --> 00:21:40,640
If I didn't assume independence, then there's another variance component there.

208
00:21:41,450 --> 00:21:45,710
So assuming independence, we make those have zero.

209
00:21:47,260 --> 00:21:53,830
And I'm left with eight squared plus where they call it Sigma E squared.

210
00:21:55,360 --> 00:22:00,050
Okay. So important to that. Again, I'm never going to ask you to derive that.

211
00:22:00,070 --> 00:22:05,420
Let's put it in the trash and then we're going to ask you to do that again.

212
00:22:05,440 --> 00:22:14,210
But it's important for you to see where that comes from. Right.

213
00:22:14,340 --> 00:22:27,559
And so this model right here is the expected value of the pain score is what we call a marginal model.

214
00:22:27,560 --> 00:22:32,360
We've marginalized over the distribution of the random intercepts with integrated it up.

215
00:22:35,840 --> 00:22:39,170
Everything works out really nicely because we've got normal and normal.

216
00:22:39,440 --> 00:22:41,809
So all the math, all the integration works out really well.

217
00:22:41,810 --> 00:22:50,120
So I don't want to go through the integrals, but essentially we're averaging over all the random effects to come up with a marginal mean model,

218
00:22:51,290 --> 00:22:55,250
but within our model is a conditional or a subject specific model.

219
00:22:56,900 --> 00:23:00,709
So now we're conditioning on knowing what someone's random intercept is.

220
00:23:00,710 --> 00:23:05,420
If I know what their intercept is, it's no longer random, it's fixed.

221
00:23:06,080 --> 00:23:14,700
And so now it shows up in the mean model. And because of that, a random intercept is now fixed.

222
00:23:14,710 --> 00:23:21,520
It has no variance. When we conditional on it, it is known there is no variance and the only variance is Sigma Epsilon squared.

223
00:23:22,710 --> 00:23:26,020
So what I'm telling you now is I fit a line.

224
00:23:26,040 --> 00:23:29,220
Every woman has their own line. The same slope.

225
00:23:30,720 --> 00:23:36,800
Different intercepts. And there's a variance between those lines of Sigma Epsilon squared.

226
00:23:37,280 --> 00:23:41,150
So you can picture a bunch of lines that are parallel to each other with different intercepts.

227
00:23:41,750 --> 00:23:45,430
Those are conditional means. That's a woman's mean.

228
00:23:45,440 --> 00:23:50,390
Again, the conditional model says that woman with that random intercept has this pattern.

229
00:23:51,580 --> 00:23:55,390
The other model says averaging across all women. And here's what we see.

230
00:23:56,140 --> 00:23:59,730
Right. We don't know what the peanuts. Pinatas are.

231
00:23:59,730 --> 00:24:02,730
They're random. We don't have them.

232
00:24:02,730 --> 00:24:07,860
They're latent, right? They're not parameters. So we don't talk about estimating them.

233
00:24:09,720 --> 00:24:13,440
They don't have a value. They're random variables, right?

234
00:24:17,080 --> 00:24:26,470
And as I told you now, because we assumed the errors within a person were independent of each other, there's still correlation within a subject.

235
00:24:28,390 --> 00:24:34,090
And then it becomes from this formula here. So the correlation of outcomes from the same subject are no longer part of the error term, right?

236
00:24:34,090 --> 00:24:39,669
So the correlation comes from the random intercepts. So let's derive what the correlation is.

237
00:24:39,670 --> 00:24:44,590
And again, I have this line here with some algebra. I think we should do some algebra.

238
00:24:46,240 --> 00:24:53,320
This is, I think, one of the most important derivations, I think in random effects models is this one right here.

239
00:24:55,410 --> 00:25:01,720
And it's always good practice. It maybe helps you see why we have community troops.

240
00:25:01,780 --> 00:25:05,210
This is why we teach you 6001 and 6026.

241
00:25:05,230 --> 00:25:14,790
It is important to. So variance of two observations from the same person.

242
00:25:14,790 --> 00:25:18,270
Right. So I've got a. Y. I. J. I. K.

243
00:25:20,040 --> 00:25:29,070
We all know that covariance is the difference between the expected value of the product and the product of the expected values.

244
00:25:33,240 --> 00:25:37,530
Right. And once I get the covariance, I can scale by the square or the variances to get a correlation.

245
00:25:42,020 --> 00:25:46,650
We've already done this on the slides, the expected value of AIG, and it came Wednesday.

246
00:25:47,670 --> 00:25:54,710
So it didn't just kept mind. And more.

247
00:25:55,790 --> 00:26:01,120
I chased. It's been a nut again.

248
00:26:01,350 --> 00:26:05,280
The random intercept has expectations. Zero. So it disappears.

249
00:26:05,940 --> 00:26:10,050
And you're left with Chitty. Chitty.

250
00:26:10,100 --> 00:26:16,110
Chitty Chitty. Chitty. Chitty Chitty Bang.

251
00:26:17,670 --> 00:26:21,650
And of course, then we can all substitute AJ for a case.

252
00:26:25,560 --> 00:26:36,840
I feel I can tell you I came to the same group effect, but a different time.

253
00:26:36,930 --> 00:26:42,910
Right. So the two observations marginally have the same mean, except for the fact that there are different time limits.

254
00:26:47,180 --> 00:26:50,030
Now we have to figure out this is where the correlation comes into play.

255
00:26:51,860 --> 00:26:57,260
So the expected value of the product of these two observations is not the product of their means.

256
00:26:59,270 --> 00:27:02,690
If they were them different than the covariance of zero.

257
00:27:03,830 --> 00:27:10,520
So we have to figure out what is the what is the expectation of the product of these two things.

258
00:27:11,040 --> 00:27:17,499
Right. So as the expected value. Uh, better not be.

259
00:27:17,500 --> 00:27:23,310
Not. I don't want me to cheat.

260
00:27:23,340 --> 00:27:31,360
I need a three to 9ggi plus it's error.

261
00:27:31,690 --> 00:27:39,050
So that thing times. Better, not better that I just made it once.

262
00:27:39,060 --> 00:27:47,070
He I came in close to a speed of 3 to 5 plus.

263
00:27:47,070 --> 00:27:51,059
It scared me. Legible.

264
00:27:51,060 --> 00:27:56,880
Is that pretty legible? It's like an exclamation point, doesn't it?

265
00:28:00,700 --> 00:28:07,140
Right. So there are going to be squared terms for some of these things in here.

266
00:28:09,330 --> 00:28:18,420
And the expected value, for instance, of being a squared is the variance because it has a mean zero.

267
00:28:19,500 --> 00:28:27,630
And so that's where these variance components are going to come into play. So much to the.

268
00:28:30,620 --> 00:28:34,160
So I'm going to put all the fixed effects times, all the fixed effects first.

269
00:28:34,610 --> 00:28:40,820
So that's the expected value of this first year. Changing the speed of two.

270
00:28:41,200 --> 00:28:45,090
I need a three. G.

271
00:28:45,340 --> 00:28:49,820
G. I. Better, not better.

272
00:28:49,840 --> 00:28:57,750
One, two. I came close to those two key groups.

273
00:28:57,760 --> 00:29:02,340
There we go. So I'm going to put all of that fixed effect and save that times.

274
00:29:02,350 --> 00:29:08,170
The other fixed part. Plus.

275
00:29:09,690 --> 00:29:15,299
Random intercepts for the first observation at time.

276
00:29:15,300 --> 00:29:18,990
J Our times. K What's the second one? Why does it matter?

277
00:29:20,720 --> 00:29:25,470
Great to see.

278
00:29:27,600 --> 00:29:31,680
Three. G. G. G. G.

279
00:29:33,440 --> 00:29:40,310
So now I am taking the random intercept from the case timepoint and multiplying it by everything from the jail standpoint.

280
00:29:41,690 --> 00:29:48,140
And then conversely, I'm going to take the random intercept from type J and multiply it by everything from time k.

281
00:29:54,440 --> 00:30:00,020
KING Kate Plus.

282
00:30:04,410 --> 00:30:10,710
Sometimes the two random intercepts times each other. So I got that right there.

283
00:30:16,520 --> 00:30:26,450
So what's. But I say this in words.

284
00:30:26,780 --> 00:30:33,580
The expected value of this first term here, there's no dependance there.

285
00:30:33,590 --> 00:30:36,860
The expected value of this product is the product of their expected values.

286
00:30:39,250 --> 00:30:49,980
So that's the expected value of this thing. It's probably a smart way to cut and paste.

287
00:30:52,560 --> 00:31:01,650
I could save myself some writing time. I'm a bit shy.

288
00:31:05,170 --> 00:31:09,550
She was put.

289
00:31:15,050 --> 00:31:19,970
So that first term, the expected value of a product is simply the product of the expected values.

290
00:31:24,070 --> 00:31:33,890
What's the expected value of that? What's random in that statement.

291
00:31:39,070 --> 00:31:44,680
What's random in that part. Anybody.

292
00:31:47,810 --> 00:31:51,830
He zero zero is random. Everything else is fixed.

293
00:31:53,680 --> 00:31:56,829
So we've got a fixed thing times a random thing.

294
00:31:56,830 --> 00:32:01,210
What's the expected value of beta? Not a zero.

295
00:32:02,470 --> 00:32:08,630
So that whole thing is zero. And the next one, the whole thing is zero.

296
00:32:09,230 --> 00:32:25,310
The expectation is zero. So expectation of a number times the expected value of beta 920, which was the expected value of beta, not a squared.

297
00:32:25,640 --> 00:32:29,360
And because it has a mean zero, the expected value squared is the variance.

298
00:32:43,100 --> 00:32:49,970
Skin Cancer Growth management. So this is the expected value of YJ times.

299
00:32:49,990 --> 00:32:53,980
The expected value. Okay, I. Right. This thing is right there.

300
00:32:55,000 --> 00:32:58,450
This is that plus that amount squared.

301
00:33:00,610 --> 00:33:06,400
That's the covariance. Worst times.

302
00:33:06,970 --> 00:33:10,770
So where we are now. So now under the current systems.

303
00:33:12,070 --> 00:33:17,260
So the covariance. Vijay might like.

304
00:33:18,930 --> 00:33:26,310
Is the idea of what I think of I came from minus all of that.

305
00:33:28,810 --> 00:33:35,230
Which is simply stated that square. Everything else goes away. All the fixed stuff gets subtracted out.

306
00:33:35,400 --> 00:33:39,070
Right. Variance is not affected by the mean.

307
00:33:39,340 --> 00:33:43,370
Okay. So it's covariance isn't affected by the inside.

308
00:33:46,220 --> 00:33:56,930
Though so far. So therefore. The correlation between these two things is their covariance.

309
00:33:58,890 --> 00:34:08,590
Divided by the square root of their variances. Reason variants variance.

310
00:34:10,010 --> 00:34:31,280
Variance why I like. What's the variance of why I j they do not squared plus sigma Mazin is an epsilon a spirit.

311
00:34:32,350 --> 00:34:35,379
So as a square root for one of them, there's a square root for the other one.

312
00:34:35,380 --> 00:34:37,390
You multiply it together, the square roots go away.

313
00:34:38,050 --> 00:34:47,560
So the correlation of any two observations in the same person is the ratio of the between variability and the total variability.

314
00:34:57,360 --> 00:35:06,270
There is no time component there. The correlation of any two observations, regardless of adjacent care, it's the same correlation.

315
00:35:06,810 --> 00:35:10,290
What is this? Which is compound symmetry?

316
00:35:13,670 --> 00:35:18,470
So with normal data, this is really important with normal outcomes so far in this class.

317
00:35:23,300 --> 00:35:26,920
A random intercept model. That's.

318
00:35:27,650 --> 00:35:31,610
So there's got to that last step there. Answer that.

319
00:35:32,510 --> 00:35:42,050
A random intercept model with normal data is equivalent to a generalized squares model, assuming compound symmetry for the errors.

320
00:35:45,570 --> 00:35:49,000
So far, we've assumed the errors are independent. You don't have to.

321
00:35:49,020 --> 00:35:52,290
You can have random effects and correlated errors.

322
00:35:52,300 --> 00:35:56,610
You can go crazy here. You can say, I have three random terms.

323
00:35:56,850 --> 00:36:00,750
And I think that there is some sort of Toeplitz structure for the air, for the errors.

324
00:36:02,010 --> 00:36:09,460
We rarely do that. It's too complicated. And certainly do not fit a random intercept model.

325
00:36:10,710 --> 00:36:13,090
And there's some compound symmetry for the errors.

326
00:36:14,100 --> 00:36:18,960
If you do that, the computer will tell you that it can't do both because they're both trying to do the same thing.

327
00:36:19,530 --> 00:36:30,450
You can estimate both the glass structure and the random variance component that rightfully so, goals put structure on the error correlation.

328
00:36:30,930 --> 00:36:40,260
Random effects models. Don't they assume independence most of the time for the errors and then throw in random effects to handle the correlation?

329
00:36:42,150 --> 00:36:45,060
Again, I've never done both. I don't know why you would do both.

330
00:36:45,930 --> 00:36:51,780
If you ever speak in SAS lingo, there is a repeat of the repeated statement and a random statement.

331
00:36:52,700 --> 00:36:57,809
And when I used to teach this class, students would have a horrible time trying to figure out which one should I use to use both?

332
00:36:57,810 --> 00:37:02,820
What's in the random sample once and then repeat the statement. I would just tell them Don't do both.

333
00:37:03,240 --> 00:37:09,010
Take one of them. So you have learned how to do structure and goals.

334
00:37:09,030 --> 00:37:12,840
And now I've shown you your first random effects model, which is a random intercept.

335
00:37:16,290 --> 00:37:22,860
We can also perhaps think that the lady characteristics don't just impact where a woman starts out with her pain,

336
00:37:23,670 --> 00:37:27,270
but maybe the leading effects change her pain over time.

337
00:37:29,010 --> 00:37:34,050
Right. So that means that we think that there are slopes that vary among the women.

338
00:37:34,830 --> 00:37:38,340
And so now we're going to focus on the beta ones, the parameter on time.

339
00:37:39,270 --> 00:37:43,260
So now we suppose that this thing is actually the mean of a distribution of slopes.

340
00:37:44,400 --> 00:37:48,299
But really what we're going to say is that each of these little b1i so one now

341
00:37:48,300 --> 00:37:52,050
to go with data again data the Greek letters are for fixed effects typically,

342
00:37:52,590 --> 00:37:55,920
and the lowercase letters are for random effects.

343
00:37:56,670 --> 00:38:04,190
We believe that there are deviations, there's a population slope, and then maybe some women going faster or slower than other women.

344
00:38:04,260 --> 00:38:07,980
And so there is a deviation from the slope is little we want to see.

345
00:38:08,610 --> 00:38:16,050
And so now we're going to write the modeling simply again as an intercept, plus a population slope, beta one,

346
00:38:16,890 --> 00:38:21,870
plus a deviation for each person, beta one, eight times time, and then everything else is the same.

347
00:38:24,090 --> 00:38:30,150
Oh, there's that darn interaction term. Should I put a random effect in the interaction term?

348
00:38:32,130 --> 00:38:36,830
Right. I'm asking you.

349
00:38:38,600 --> 00:38:48,890
It gets really challenging. Do I also think that there should be a random adjustment for the difference in the slopes between the two groups?

350
00:38:50,000 --> 00:38:55,310
So this model right here says there's two population averages, one for the placebo, one for the intervention slope over time.

351
00:38:56,570 --> 00:39:03,230
And then there is this overall adjustment for every woman, but that adjustment isn't different for the two groups.

352
00:39:03,890 --> 00:39:09,740
I have assumed that the variability between the slopes is the same for the placebo group in the intervention.

353
00:39:12,020 --> 00:39:18,020
I could put in another random effect. There have been not 3b3.

354
00:39:18,020 --> 00:39:26,510
I haven't done that today, but you could do that if you so desire there.

355
00:39:26,850 --> 00:39:31,280
The intercepts are fixed. They do not. There is no random intercept right now in this model.

356
00:39:32,120 --> 00:39:38,420
Again, same assumptions. We're going to assume that the random slope deviation b1i and the errors within a person are independent.

357
00:39:39,350 --> 00:39:46,850
And we believe that the errors within a person are independent because it makes the math easy, not because we believe it's realistic.

358
00:39:47,780 --> 00:39:56,460
Okay. Again, regardless of whether we have random slopes or not, because they have mean zero, we get the same marginal model.

359
00:39:58,350 --> 00:40:04,370
Whether it was a random intercept or a random slope. We get the same overall population average model.

360
00:40:07,040 --> 00:40:16,380
But the variance of any one observation YJ right gets to various components, the variability between and the variability within.

361
00:40:17,510 --> 00:40:21,230
But this is what's really important about this model. We fit random slope models all the time.

362
00:40:21,680 --> 00:40:24,860
And again, no one told me this when I was in grad school, but look what the variance does.

363
00:40:26,880 --> 00:40:33,900
What happens to the variance over time? The changes and what direction?

364
00:40:36,440 --> 00:40:44,840
It gets bigger. Time squared is positive, right? So as time goes on, because we believe that each woman has her own slope,

365
00:40:46,610 --> 00:40:51,200
we're also making the assumption that these observations have variability that increases over time.

366
00:40:51,470 --> 00:40:53,210
In fact, it goes quite dramatically with time.

367
00:40:54,990 --> 00:41:01,430
Just an artifact just to realize that that we're assuming and not just at variance, this is not a constant variance model anymore.

368
00:41:01,430 --> 00:41:11,400
It's variance changes with one of the covariance changes the time settle that look at it within this model however again is a conditional.

369
00:41:12,400 --> 00:41:22,570
Um, I mean model. And if I know the deviation for an individual, a woman theta 1ib1i excuse me then I know that what her slope is,

370
00:41:22,570 --> 00:41:31,030
it's the population average slope plus her deviation. And so now I'm talking about what do I expect for a given woman's trajectory over time,

371
00:41:31,540 --> 00:41:35,350
not the population, but for a woman with a given random effect?

372
00:41:36,190 --> 00:41:40,880
Again, it doesn't. It is it no longer has a mean because it's fixed for conditioning at it.

373
00:41:43,210 --> 00:41:46,240
I remember I had a professor in grad school who was mean as [INAUDIBLE].

374
00:41:46,750 --> 00:41:51,880
Taught me a lot though, but he always said your caller doesn't understand conditioning.

375
00:41:53,020 --> 00:42:00,190
And he was right. It took me a long time as a student to get around the fact that it was, you know, whether it's in there or not.

376
00:42:00,580 --> 00:42:03,580
But when you conditioned on something that's random, right now it's fixed.

377
00:42:04,030 --> 00:42:08,950
And so it goes into the mind model. And because it's fixed, it has no variance anymore.

378
00:42:09,430 --> 00:42:16,030
So now what we're saying is every woman has their own line and those lines vary among each other.

379
00:42:16,390 --> 00:42:22,240
I am on Sigma Epsilon squared and again, we're not going to do the algebra again.

380
00:42:22,240 --> 00:42:28,680
Once was enough, but if you go through the same steps, but now there's a bunch of TS floating around here.

381
00:42:28,690 --> 00:42:38,380
What's a t j tick? What you're going to get is a correlation structure that looks like that, and it's ugly, but it isn't constant over time.

382
00:42:39,190 --> 00:42:43,150
The correlation of any two observations is a function of when they were observed.

383
00:42:45,820 --> 00:42:46,780
But again,

384
00:42:46,780 --> 00:42:53,350
you can look at that function a little bit and you'll see that it actually decreases over as the two observations get further and further apart.

385
00:42:54,100 --> 00:42:58,059
So this is mimicking sort of an or one structure.

386
00:42:58,060 --> 00:43:04,660
It's a different version of it, but it is essentially some sort of continuous a one structure error,

387
00:43:04,660 --> 00:43:10,420
one structure, whereas as the observations move apart in time, their correlation decreases.

388
00:43:11,630 --> 00:43:16,760
So again, has a nice interpretation there that it's sort of mimicking the idea that correlation decreases over time.

389
00:43:18,420 --> 00:43:24,050
That's because the variance is increasing over time. Which gets back to one of the questions on the test.

390
00:43:25,430 --> 00:43:30,140
All right. We don't typically fit a random slope only model.

391
00:43:31,680 --> 00:43:38,130
Will often shoot random intercept models and will often fit random intercept and slope models.

392
00:43:39,840 --> 00:43:43,050
So we'll take a random intercept and then maybe we'll throw on a random slope.

393
00:43:43,650 --> 00:43:49,050
But I don't know of anyone who has ever said they're just going to hit a random, random slope model with fixed intercepts.

394
00:43:49,680 --> 00:43:58,420
So that model looks just like all the other models, except now we have we have the population average, right?

395
00:43:59,760 --> 00:44:03,990
And then each woman has deviations from the line, both in her intercept and her slope.

396
00:44:04,740 --> 00:44:08,399
And then there's an error term where it gets a little more complicated.

397
00:44:08,400 --> 00:44:17,070
Now we have two random effects, although those random effects are independent of the errors.

398
00:44:17,970 --> 00:44:26,340
So these two folks are uncorrelated with the errors and we still say that the errors are independent.

399
00:44:28,260 --> 00:44:32,350
These two things, right? It is. They have a binary normal distribution.

400
00:44:32,350 --> 00:44:37,710
Now we've got two random variables that have normal distributions. They don't necessarily need to be independent.

401
00:44:39,990 --> 00:44:47,280
So these two random effects now have a mean vector of zero and their own correlation matrix.

402
00:44:47,530 --> 00:44:50,880
This is not the correlate D is not the correlation of the observations.

403
00:44:51,420 --> 00:44:55,110
It's the correlation of the random effects within the model.

404
00:44:56,650 --> 00:45:01,660
So we have to think about structure for D, so they each have their own variants off the diagonal.

405
00:45:02,020 --> 00:45:09,280
But what about on the diagonal? Excuse me? Off the diagonal. So the covariance of a random intercept and a random slope.

406
00:45:09,280 --> 00:45:14,370
Zero. Or should there be a correlation or covariance term off the diagonal?

407
00:45:16,050 --> 00:45:20,730
What do we usually think about intercepts and slopes? If my intercept is way up here, what happens to my slope?

408
00:45:24,720 --> 00:45:30,060
No change goes down, right. We believe a regression to the mean sort of a concept.

409
00:45:30,390 --> 00:45:37,110
If I start out low, I just naturally kind of drift up to the mean except for in extreme situations so often.

410
00:45:37,200 --> 00:45:43,499
And if you were to look at your 650 models, there's usually there's a call variance term for the intercept in the slope.

411
00:45:43,500 --> 00:45:49,120
It's usually negative. Okay. So if the intercept is way down here, the slope tends to go up.

412
00:45:49,130 --> 00:45:54,800
The data are just not going to go further down. And likewise, if the intercept is really high, the slope is negative.

413
00:45:54,810 --> 00:45:59,750
So we usually allow for there to be a correlation between the random intercept and the random slope,

414
00:46:00,110 --> 00:46:02,210
because many times they're going to be negatively correlated.

415
00:46:03,170 --> 00:46:09,820
If someone is just much higher than everybody else, they tend to drift down to the population average.

416
00:46:09,830 --> 00:46:23,870
Not always, but often. But you can force so you can tell the default in most computer packages is to have a covariance term in this matrix here.

417
00:46:24,800 --> 00:46:31,340
If you strictly want to force the covariance between the random intercepts and the slopes to be zero, you have to tell the computer that.

418
00:46:32,180 --> 00:46:39,940
And so we'll go through that with our. Again, we can start talking about comparing those two models.

419
00:46:40,240 --> 00:46:46,480
Should I allow for the random or substance ups to be correlated, or should I force them to have a correlation of zero?

420
00:46:47,200 --> 00:46:59,070
You can compare those two models to decide that you use the data. So just more generally now you can have a gazillion random effects.

421
00:47:00,000 --> 00:47:03,060
And this is where it gets really important in this class.

422
00:47:03,600 --> 00:47:05,340
We're talking about longitudinal data.

423
00:47:06,270 --> 00:47:21,810
The only random effects are usually time in the intercept, but you can have random effects for anything which looks fine here.

424
00:47:22,500 --> 00:47:27,719
I could have had a random group effect. I again.

425
00:47:27,720 --> 00:47:31,290
What are we saying? If I had a little G, not II or ag1,

426
00:47:31,290 --> 00:47:40,590
I guess it is little G that says that there's a variance component for one of the groups so that the two groups have different variances.

427
00:47:41,730 --> 00:47:45,000
So you do that. That's is the that's not constant variance.

428
00:47:45,000 --> 00:47:50,040
Again, the variance changes depending on whether I'm in the placebo group or the intervention group.

429
00:47:50,730 --> 00:47:54,340
And I don't I think that's not that implausible in these data.

430
00:47:54,840 --> 00:48:03,990
Let's go back here. I think a lot of the labor pain in the intervention, they tended to have a pretty low level of pain.

431
00:48:04,110 --> 00:48:10,320
There wasn't a lot of noise. Um, but with the placebo group, there seemed to be a lot of noise in the pain scores.

432
00:48:10,800 --> 00:48:15,630
So you could have a random group effect, you can have a random effect for anything.

433
00:48:17,500 --> 00:48:26,430
Just keep that in mind as you move along in future classes that random effects are used for lots of things other than just longitudinal data.

434
00:48:26,910 --> 00:48:32,670
And I there I am, but the general formulation of a linear mixed model.

435
00:48:32,670 --> 00:48:40,470
So this is the classic formula. Back in the day it was called the Laird and Wear model for James Layer and then layered,

436
00:48:41,760 --> 00:48:47,670
one of the few things and statistics I learned that was named after a woman for sure, and it was named after a guy, right?

437
00:48:48,330 --> 00:48:51,420
But that was called the layered wear model. Again, I don't think anyone calls it anymore.

438
00:48:51,420 --> 00:48:58,500
It's just a linear, mixed effects model. There is a fixed component which you've seen before.

439
00:48:59,400 --> 00:49:02,820
There is a random component. That's why it's mixed.

440
00:49:03,360 --> 00:49:08,730
And then there's the errors. So we have a design matrix for the fixed effects.

441
00:49:10,210 --> 00:49:18,430
So again, that was in in our model that's column a once a group indicator, time indicator and an interaction.

442
00:49:20,710 --> 00:49:23,740
We then have a design matrix for the random effects.

443
00:49:25,090 --> 00:49:33,670
And if I have a random intercept and slope model Z is a column of ones for the intercepts and a column of times for the random slopes.

444
00:49:33,880 --> 00:49:40,720
It's the same sort of approach. So fixed, random and then and then the errors.

445
00:49:42,050 --> 00:49:47,570
So we get a vector again of the mean parameters. And as I've shown you, there's, there's the design matrix, right.

446
00:49:48,170 --> 00:49:57,980
There's one row for every time point. And then the random parts of the model are the random effects and the errors.

447
00:50:01,310 --> 00:50:06,140
We have a certain number of random effects and this is a cube x matrix.

448
00:50:06,230 --> 00:50:14,120
D is a cube cube matrix of variances along the diagonal and all the covariance is of the random effects if you want to have them.

449
00:50:14,840 --> 00:50:21,590
And then Z is simply again for every timepoint. Which covariates are you allowing for random effects to go with them?

450
00:50:23,340 --> 00:50:34,139
The one thing now that is different is that the vector of errors I'm going to denote there variance covariance as I often are.

451
00:50:34,140 --> 00:50:37,920
I can't read it that often.

452
00:50:37,940 --> 00:50:45,930
Ari is an identity matrix. Yes. So can does that mean that every fixed effect has a random effect?

453
00:50:45,940 --> 00:50:51,870
Also in the recycle question, the never ending, never ending debate in a random effects model.

454
00:50:53,190 --> 00:50:58,710
If I have a fixed effect, if there's a covariate in X, does it have to go in Z?

455
00:50:58,740 --> 00:51:01,020
That's what you're asking, right? The answer is no.

456
00:51:02,520 --> 00:51:09,930
It's possible for instance, it's possible that although the two groups have different variances, they have the same mean.

457
00:51:10,960 --> 00:51:15,010
So that group wouldn't be in the main structure, but it would be in the random effect structure.

458
00:51:17,620 --> 00:51:25,030
Again, most of the time in this class, if you have a if you have a random effect, you're going to have a fixed effect for it.

459
00:51:26,110 --> 00:51:32,530
So if you have a random slope, you should have a fixed slope. If you have a random intercept first, you're going to have intercept in the model.

460
00:51:32,540 --> 00:51:37,390
We always use intercepts. But mathematically, you can do anything.

461
00:51:37,630 --> 00:51:41,320
You can have any covariance in X and any covariance and Z.

462
00:51:41,890 --> 00:51:45,280
But often they're the same. So not. But they don't have to be.

463
00:51:48,240 --> 00:51:53,370
This again, this covariance matrix of the errors I've written in generally as a covariance matrix,

464
00:51:53,610 --> 00:52:00,750
the errors can have any correlation structure you want. 99% of the time it's going to be an identity matrix, times sigma squared.

465
00:52:03,550 --> 00:52:09,960
But we're not calling it Sigma II anymore. Because the variance sigma is the variance of the data.

466
00:52:10,350 --> 00:52:14,920
The whites. And in the ghost model, they were they were the same thing.

467
00:52:15,220 --> 00:52:20,740
But so again D the random effects are the eyes of the between subject variability.

468
00:52:21,160 --> 00:52:27,520
Why do women vary between each other and the within?

469
00:52:27,520 --> 00:52:34,780
Is the noise that's left over. That's the errors. So the resulting model.

470
00:52:34,780 --> 00:52:37,540
I've written this up to you, but not in matrix notation. Right.

471
00:52:39,250 --> 00:52:45,979
The the marginal mean of the vector of lies is simply exciting speed because everything else has been zero goes away.

472
00:52:45,980 --> 00:52:54,940
It's the variance of y, as I said, is sigma I sigma I is now decomposed into two pieces, the between and the within.

473
00:52:56,560 --> 00:53:01,870
So I know this is the within component.

474
00:53:02,560 --> 00:53:14,540
This is the between component. But again, the variance of why I if you take the variance of this thing right here, it's the variance of this,

475
00:53:14,900 --> 00:53:22,250
which is zero because it's fixed plus the variance of this, which is Z times,

476
00:53:22,250 --> 00:53:26,930
the variance of this thing times the transpose of Z right squared in variance.

477
00:53:27,710 --> 00:53:35,240
So you've got a Z variance of this Z transpose plus the variance of the errors, which is right.

478
00:53:35,720 --> 00:53:44,420
So that's where this formula comes from. This is squaring the set, the covariance times, the variance covariance matrix of the random effects plus.

479
00:53:44,420 --> 00:53:48,940
All right. What line is just some information for you?

480
00:53:49,120 --> 00:53:58,900
If you remember from probably 601, the total variance of something can be decomposed into the variance of a mean plus, the mean of a variance.

481
00:53:59,320 --> 00:54:04,000
Remember that lovely formula? Yes, it applies right here.

482
00:54:04,870 --> 00:54:08,860
Those two components are actually the variance of the means. It's the between.

483
00:54:09,250 --> 00:54:13,480
How do the variance of the mean the conditional means? How do they vary from each other?

484
00:54:15,170 --> 00:54:23,600
As well as an average variability. So just that fits really nicely into that formula.

485
00:54:23,990 --> 00:54:29,809
Oh, can I just answer his question? There is Z. I can be a subset of the columns, etc. Can you included keywords, words, indexed?

486
00:54:29,810 --> 00:54:36,080
Say it's not very common, but it can be done. The parameter estimation remains the same.

487
00:54:37,490 --> 00:54:41,780
All I've done is figured out a different way to model the variance structure of the data.

488
00:54:43,400 --> 00:54:49,310
So beta hat is the same formula was before and the variance of beta is the same as before.

489
00:54:49,550 --> 00:54:59,360
There's a weight matrix that quantifies the correlation of the weights and I just write Sigma II as a function of this omega,

490
00:54:59,360 --> 00:55:07,430
which are all the other parameters, right? Sigma Squared Epsilon, the variance of the random effects, which could be an intercept boundary slope.

491
00:55:09,410 --> 00:55:17,270
So the challenge again is in order to get beta hat, I need to know what are those variance terms?

492
00:55:17,690 --> 00:55:22,010
What is the variance of the random intercepts? What is the variance of the errors?

493
00:55:23,660 --> 00:55:28,040
And that has taken some people part of their careers to figure out.

494
00:55:28,070 --> 00:55:31,280
Back in the day when these models didn't exist. Right.

495
00:55:31,340 --> 00:55:36,620
Laird and where made their money and developing how you estimate those things.

496
00:55:36,650 --> 00:55:40,310
It isn't straightforward and it's beyond what I want to teach in this class.

497
00:55:40,880 --> 00:55:47,180
But the formula itself is very easy program programing, a computer to do something that we're going to do in class.

498
00:55:47,750 --> 00:55:54,750
I took a lot of a lot of effort. It's 4:00, 4:00 for all of that.

499
00:55:55,620 --> 00:56:02,560
All right. Want to talk about a few other things we're going to do poll everywhere because we always do.

500
00:56:03,240 --> 00:56:11,889
It's very important. And I just got a new Apple Watch, so my phone will go as well, I'm sure.

501
00:56:11,890 --> 00:56:13,030
But let's talk again.

502
00:56:13,030 --> 00:56:22,270
Let's talk about the next set of slides so that we can get at least as far as we can before you jump into the homework time from beginning.

503
00:56:22,420 --> 00:56:32,830
All right. As I said earlier in the first set of slides, we do not estimate the random intercepts.

504
00:56:33,640 --> 00:56:36,910
We estimate the variance of those random intercepts.

505
00:56:37,360 --> 00:56:44,139
The variance is the parameter. But we're going to come up with something called a prediction.

506
00:56:44,140 --> 00:56:51,160
It's I'm going to fill in everybody's latent B's penalized, for example, with something called the prediction.

507
00:56:52,840 --> 00:56:59,620
Then we don't say estimate. We're going to come up with a number, but the number is a prediction, as you're going to see in the formulas here.

508
00:57:00,490 --> 00:57:04,750
Oh, boy. All right. So again, generalized squares versus linear mixed models.

509
00:57:05,920 --> 00:57:12,100
We've got our model there for a linear, mixed model, fixed plus random components plus error.

510
00:57:12,460 --> 00:57:17,260
We decompose the variance into these two pieces. So again, what do I need to estimate?

511
00:57:17,260 --> 00:57:19,990
I need to estimate the betas in the fixed effects.

512
00:57:20,740 --> 00:57:29,860
I need to estimate the parameters and D and I need to estimate usually one parameter in R that sigma squared times the identity in GLS.

513
00:57:29,860 --> 00:57:34,630
We didn't do any of the decomposition. We just immediately went to Sigma II and put some structure on it.

514
00:57:36,730 --> 00:57:46,300
As I've just re-emphasized here, the variance of the random effects is this D in our I couldn't be anything we want it to be,

515
00:57:46,840 --> 00:57:49,899
but it's usually just an identity matrix. Times Sigma Square.

516
00:57:49,900 --> 00:57:55,780
We assume the errors are independent. Once we model the variability through the random effects, there's nothing left in years.

517
00:57:58,040 --> 00:58:06,360
But you can do anything you want mathematically. For the most part. And one feature of an item that goes beyond glass.

518
00:58:06,380 --> 00:58:11,330
So if you ask the question of Why do I need to learn this time, you've already showed me how to deal with correlation.

519
00:58:11,330 --> 00:58:15,710
Can I please go do something else? This does have some benefit.

520
00:58:16,070 --> 00:58:22,070
So one feature of the linear mix based model is that we can get these subjects specific estimates.

521
00:58:23,060 --> 00:58:28,040
Glass only gives me the population average and then it fixes inference.

522
00:58:29,850 --> 00:58:34,920
But I can get these conditional. I want to know every woman's life, for example.

523
00:58:35,730 --> 00:58:40,980
So the value of Z time CPI is the difference between the mean of an individual.

524
00:58:41,490 --> 00:58:48,510
So the population mean is data. The zibi is the deviation from that population mean.

525
00:58:49,920 --> 00:58:54,000
But again, those BI are latent. We don't know what they are.

526
00:58:54,780 --> 00:59:01,950
We could never measure them. So in order to make this useful, we have to come up with some number to plug in for by.

527
00:59:04,740 --> 00:59:11,050
And I think we'll stop there with huge suspense for Friday.

528
00:59:11,070 --> 00:59:21,060
All right. So we're going to start predicting these babies and we're going to do it through the basic Bayes Bayes formula.

529
00:59:22,020 --> 00:59:26,160
And why does Mr. Bayes come into play in our lives beyond Bayesian statistics?

530
00:59:26,850 --> 00:59:35,780
This is a classic example of, again, something we see in 601 that's really useful in this case, but that's not as important as full everywhere.

531
00:59:35,790 --> 00:59:42,810
So we'll get to that on Friday. We can cover everything again.

532
00:59:42,820 --> 00:59:45,840
On Friday, we're going to start fitting models.

533
00:59:45,900 --> 00:59:53,620
We're going to start fitting these random episode models to the data to see see what you guys come up with everywhere.

534
00:59:54,060 --> 01:00:05,160
So get out your phones, including myself. So there it is.

535
01:00:07,110 --> 01:00:10,679
All right, here is the question and instructions.

536
01:00:10,680 --> 01:00:19,560
Activate three settings. Remember that you are the host of a brand new late night talk show.

537
01:00:20,760 --> 01:00:26,399
Who would be your first guest since this hypothetical liquid?

538
01:00:26,400 --> 01:00:31,860
They not be alive anymore. It could be anybody you wish if you could reincarnate them.

539
01:00:31,860 --> 01:00:38,730
Sure, why not? Who would you not determined to on camera in front of the nation?

540
01:00:41,370 --> 01:00:46,730
You're Jennifer Hudson. You just got your own new show. That's for. Who would you want to have?

541
01:00:55,430 --> 01:01:03,130
So. In.

542
01:01:10,410 --> 01:01:56,160
He. Well, it's hard.

543
01:02:07,730 --> 01:02:39,770
She? Peter.

544
01:02:51,610 --> 01:03:08,620
Yeah. You come out and you can't really remember.

545
01:03:08,620 --> 01:03:11,980
It's got to be sort of interesting to the rest of the world, to your show is going to get canceled.

546
01:03:36,480 --> 01:03:46,440
I'm going to get 30. He can't leave until we get the search results.

547
01:03:53,640 --> 01:04:12,020
We're going to have to go to another country that are going to be extremely committed to our people.

548
01:04:12,220 --> 01:04:16,920
All right. You guys are forcing me to give up. All right.

549
01:04:16,950 --> 01:04:21,030
There's two things. This is what I have to do.

550
01:04:22,550 --> 01:04:28,680
No doubt. There's only one.

551
01:04:32,540 --> 01:04:38,180
No. So let's do it.

552
01:04:42,870 --> 01:04:53,750
That is. I doubted that she would be canceled.

553
01:04:55,970 --> 01:04:59,090
Scott Seeger, a sensation.

554
01:05:01,130 --> 01:05:07,970
Kim Jong un. Oh, my goodness.

555
01:05:09,260 --> 01:05:20,300
All right. What's this?

556
01:05:21,480 --> 01:05:25,850
You know how to do it, you know?

557
01:05:29,720 --> 01:05:36,010
You already got your grades, folks. Okay?

558
01:05:52,660 --> 01:06:07,780
No, this it's the same person as the sandwich variants which are works like a bird's eye freeze.

559
01:06:14,680 --> 01:06:23,040
Marvel no one had taken.

560
01:06:32,600 --> 01:06:38,760
He's just I want to know who these people are.

561
01:06:38,790 --> 01:06:47,310
I hear these Snickers and laughs. Oh, come on. My guess is someone just wanted to get out of here.

562
01:06:48,870 --> 01:06:53,370
Right. Thank you. I will share these with you, and I encourage you to check out the metropolitan area.

563
01:06:54,900 --> 01:06:58,980
All right. Yes. Right here. Yeah.

