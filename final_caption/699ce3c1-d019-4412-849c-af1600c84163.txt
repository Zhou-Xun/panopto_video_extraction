1
00:00:00,270 --> 00:00:08,400
Then I think we will pick up from where we left.

2
00:00:09,510 --> 00:00:26,160
So we, we kind of stopped right before talking about, uh, the leftist and how you could use some squares to construct the.

3
00:00:26,700 --> 00:00:45,299
So we will sort of pick up from there. So recall that in module E, we did define that F distribution and we basically,

4
00:00:45,300 --> 00:00:53,820
if I have x one square distributed as a I square with degrees of freedom,

5
00:00:53,820 --> 00:01:04,260
the F1 and x2 squared is a chi square with degrees of freedom d f2 then x1 squared and x2 squared are independent random variables.

6
00:01:04,260 --> 00:01:21,270
Then the ratio of x1 squared to x2 squared scaled by their respective degrees of freedom is an F distribution with degrees of freedom D, F1 and F2.

7
00:01:21,270 --> 00:01:27,930
So the distribution has two parameters that characterize the distribution,

8
00:01:28,290 --> 00:01:37,739
and the two parameters are the numerator and denominator degrees of freedom denoted by D f1 v F2,

9
00:01:37,740 --> 00:01:44,970
which correspond to the biggies of freedom of the respective chi squares in the numerator and denominator.

10
00:01:46,050 --> 00:01:58,709
And since F is the issue of the Laker x1 squared x2 squared random variable, it only takes on positive values.

11
00:01:58,710 --> 00:02:02,220
So this is a positive value, random variable.

12
00:02:03,000 --> 00:02:14,729
And the connection to the P distribution, we all talked about this and in module eight we knew this that a square of a T distribution is an F

13
00:02:14,730 --> 00:02:23,130
distribution with the numerator degrees of freedom one and then denominator degrees of freedom equal to DFT.

14
00:02:23,430 --> 00:02:28,200
VF is the corresponding degrees of freedom of the distribution.

15
00:02:28,200 --> 00:02:33,090
So this is a of the connection.

16
00:02:33,840 --> 00:02:37,049
And you can also show this by brute force algebra.

17
00:02:37,050 --> 00:02:43,230
In fact, in the in the context of the SLR, this show it by brute force algebra.

18
00:02:43,510 --> 00:02:50,220
But these two are I mean, this is the connection between the T and the F.

19
00:02:51,420 --> 00:03:01,590
Okay. So now let's go back to our framework hypothesis testing framework in simple linear regression.

20
00:03:02,040 --> 00:03:08,610
And you know, as we saw in the sort of describe and level up that we are going to be,

21
00:03:08,970 --> 00:03:15,930
the primary hypothesis of interest is always like the hypothesis about the slope.

22
00:03:15,930 --> 00:03:25,169
We talked about the inference and hypothesis testing for the intercept, but typically this is of primary interest.

23
00:03:25,170 --> 00:03:35,580
So like here is my null hypothesis between equal to zero beta one is the is the slope of the simple linear regression model.

24
00:03:35,940 --> 00:03:41,520
And and I'm just going to, you know, kind of right behind you.

25
00:03:41,790 --> 00:03:54,560
This is my model. And the alternative is that we have a two sided alternative with they one not required to do so.

26
00:03:54,570 --> 00:04:04,070
Recall that we did develop that statistic to conduct this test of hypothesis of the scope and the p statistic.

27
00:04:04,070 --> 00:04:14,000
The calculated value of the P statistic is basically the point estimate with the one head divided by the standard error of the one head,

28
00:04:14,360 --> 00:04:19,340
or in other words, the square root of the estimated variance of beta one hat.

29
00:04:19,790 --> 00:04:29,719
And we know that the standard error of beta one had is given by the square root of the MSE over so six.

30
00:04:29,720 --> 00:04:41,540
So we just plug that in and this has a distribution with degrees of freedom and minus two Y responding to and the N minus two comes from the

31
00:04:41,540 --> 00:04:49,939
fact that the sample size is an and the minus two comes from the fact that we're estimating the two parameters in the linear regression model.

32
00:04:49,940 --> 00:04:59,410
We are not going to be sensible. So now let's cast this in the context off of the null and void hypothesis.

33
00:04:59,440 --> 00:05:04,219
So under the null model, it's under beta one equal to zero.

34
00:05:04,220 --> 00:05:13,700
What happens to this equation? So I have the model is y equal to beta not plus epsilon nine right under the now.

35
00:05:14,990 --> 00:05:18,070
Okay. So what is the estimate?

36
00:05:18,270 --> 00:05:24,650
Least squares estimate. Therefore beta are not in this case it's y bar.

37
00:05:25,010 --> 00:05:32,570
That's your best. I mean, you know, you can also sort of argue from intuition, but this is the least with estimated it's y bar.

38
00:05:32,870 --> 00:05:36,020
So y hat is equal to IBR.

39
00:05:37,760 --> 00:05:42,770
Correct. So what is under the null hypothesis?

40
00:05:42,770 --> 00:05:51,050
What is is this e s this e is always summation y minus y hat square.

41
00:05:52,010 --> 00:05:58,999
So now I plug in in place of y, I had a plug in y bar so I get under the null.

42
00:05:59,000 --> 00:06:03,440
The s e is summation y minus y by square.

43
00:06:05,000 --> 00:06:08,540
You know, this this quantity should look familiar to you.

44
00:06:08,540 --> 00:06:23,119
Right? And we had argued before that this is what this is in the in the in the SLR model, original SLR model.

45
00:06:23,120 --> 00:06:27,829
This is the SS SSD, the total sample squared.

46
00:06:27,830 --> 00:06:40,370
So if you remember last week we had argued that you can think about the SSD as an SSD from the null model and that and that.

47
00:06:40,370 --> 00:06:45,440
Lo and behold, that's what it is. What about the alternative model?

48
00:06:45,710 --> 00:06:50,720
So under H one, when beta one is not equal to zero,

49
00:06:50,840 --> 00:06:55,350
then you have this model back y are equal to be Dennard plus beta one excite that's

50
00:06:55,350 --> 00:07:02,239
excellent and the least squares estimate the sum of B Donovan with the one you

51
00:07:02,240 --> 00:07:10,469
can obtain and basically then you plug it in so you get y I had to go to beta not

52
00:07:10,470 --> 00:07:17,870
pad plus beta one had x side and SS e is summation of y minus y had squared.

53
00:07:17,870 --> 00:07:24,350
So all good so far. So we have this is C under the null and we have the SC on the alternative.

54
00:07:24,980 --> 00:07:31,910
Now the question is how do we use some sub squares to decide what the statistic for testing the

55
00:07:31,910 --> 00:07:37,550
null that beta one equal to zero versus the alternative that between is not equal to zero.

56
00:07:38,930 --> 00:07:51,410
Okay. So the next slide is once again, you have to trust me kind of slide at this point, but we will do this in module F.

57
00:07:52,940 --> 00:08:04,070
So now I need to construct a test statistic which involves Beethoven and for which we can derive the sampling distribution.

58
00:08:05,270 --> 00:08:22,520
Okay, so this is what we will show in module F that under the null hypothesis is this you

59
00:08:22,730 --> 00:08:28,430
sigma squared has a chi square distribution with n minus one degrees of freedom.

60
00:08:29,210 --> 00:08:35,360
It's this r over sigma squared has a chi square distribution with one degree of freedom.

61
00:08:35,360 --> 00:08:45,400
And this is I mean in SLR, I should mention because then we will show in module F the general result.

62
00:08:47,680 --> 00:08:50,290
For SSR and SS.

63
00:08:50,290 --> 00:09:01,960
Each of our sigma squared has a chi square within minus two degrees of freedom, and it also turns so that SSR and its AC are independent.

64
00:09:02,050 --> 00:09:10,630
This is also key to our, you know, constructing the EPS statistic.

65
00:09:11,620 --> 00:09:15,729
SS r is the quantity.

66
00:09:15,730 --> 00:09:23,020
Remember I say that we need a statistic statistic which involves we go on and for which we can derive the sampling distribution.

67
00:09:23,380 --> 00:09:30,580
So SSR depends on data, her video on her.

68
00:09:35,360 --> 00:09:40,519
It says e also implicitly depends on B the one head because it's a C involves

69
00:09:40,520 --> 00:09:47,090
why a hat and why a hat involves big are not hat as well as Beethoven hat.

70
00:09:47,990 --> 00:10:01,590
Okay so so the key now if you if you sort of this we will for in future slide these results about the some substance the distributions of this answer

71
00:10:01,600 --> 00:10:20,360
of and also that is this on an associate are in hard independent who now applying the the result in about a distribution in module e what can we say?

72
00:10:20,540 --> 00:10:27,980
We can see that the D sure you can apply this result that issue of independence scale.

73
00:10:29,330 --> 00:10:31,970
But the scaling is being done by the degrees of freedom.

74
00:10:32,240 --> 00:10:38,210
But the issue of two guys squared and the variables that are independent fields and F distribution.

75
00:10:38,750 --> 00:10:53,060
So we are going to focus on SS are and SS E and the independence of SSR and SS E to construct the F statistic in this scenario.

76
00:10:54,080 --> 00:10:59,390
So under age, not since SS are and SSI are independent,

77
00:11:00,170 --> 00:11:10,510
we will take the ratio of these two sums up squares scaled by that appropriate degrees of freedom and we get an f so SSR

78
00:11:10,510 --> 00:11:18,620
over sigma squared divided by what is the degree of freedom officer so one in simple linear division two divided by one,

79
00:11:18,620 --> 00:11:30,200
I should say. And then in the denominator we have a CFC over sigma squared divided by its degrees of freedom, which is n minus two.

80
00:11:31,940 --> 00:11:39,230
This ratio is distributed as an F with degrees of freedom one comma in minus two,

81
00:11:39,860 --> 00:11:48,799
because the upstairs this quantity is a square with one degree of freedom and the downstairs scale

82
00:11:48,800 --> 00:11:54,650
by degrees of freedom in the downstairs is a guy squared with n minus two degrees of freedom.

83
00:12:01,290 --> 00:12:06,780
So I get an F with this is a freedom one comma and minus two gifs.

84
00:12:07,890 --> 00:12:13,630
So are you saying. No, no, no.

85
00:12:13,640 --> 00:12:19,440
Not the sigma square in the numerator. So it must be from the upstairs and downstairs cancel.

86
00:12:19,640 --> 00:12:23,150
The one is corresponds to the degrees of freedom officers are.

87
00:12:26,580 --> 00:12:34,230
Why do I get 1.4 degrees of freedom? Okay, so if you remember from the partitioning of the sunset squares.

88
00:12:34,920 --> 00:12:38,070
Let's go back. So is this. Why is.

89
00:12:50,880 --> 00:13:00,390
So is this why is equal to SSR plus SS e so the same suspect partition that way and the degrees of freedom also correspondingly

90
00:13:00,390 --> 00:13:08,760
partition that we so the total degrees of freedom is equal to the degrees of freedom of SSR of freedom of SSD.

91
00:13:08,970 --> 00:13:15,270
We argued I mean, you see, like when we prove in module F, like we sort of prove it generally,

92
00:13:15,570 --> 00:13:20,310
but we now we are arguing is that it says C has degrees of freedom in minus two.

93
00:13:20,790 --> 00:13:24,120
So in minus one, minus and minus two gives me the one.

94
00:13:24,360 --> 00:13:27,719
But it really comes from looking generally both.

95
00:13:27,720 --> 00:13:39,720
See, when we prove this, it comes from the fact that if there are be barometers in the linear model, then it's t minus one.

96
00:13:44,200 --> 00:13:51,550
Okay? Yes. If we only had the statistics to check if I wanted to start reading and writing.

97
00:13:52,150 --> 00:13:56,530
Okay. And that's a great question. And it's coming up the next slide.

98
00:14:01,150 --> 00:14:07,030
Okay. So now let's go to.

99
00:14:10,690 --> 00:14:13,930
Okay. So everybody clear here? Yes.

100
00:14:15,640 --> 00:14:17,170
It is delicious. It has good.

101
00:14:19,590 --> 00:14:32,400
At the end of each of those sitting in the food distribution, it seems now that they have insurance under age, not under the non.

102
00:14:33,180 --> 00:14:38,620
Yeah. I mean and it's they're not under the null hypothesis.

103
00:14:38,620 --> 00:14:53,040
So that's key. So under the null this is key because remember, like if, if you go back to the general framework for hypothesis testing,

104
00:14:53,040 --> 00:15:02,819
we say that we have to construct the test statistic such that we have a reference distribution or a

105
00:15:02,820 --> 00:15:11,850
sampling distribution that we can reference that test statistic against under the null hypothesis.

106
00:15:12,660 --> 00:15:21,270
And and again, you know, I'd probably say more than we needed to do here, but just go back to the general hypothesis testing framework.

107
00:15:21,480 --> 00:15:28,740
And the idea is we need a different distribution to calibrate our calculated value of the test statistic against

108
00:15:28,740 --> 00:15:38,000
that distribution under not why are there none again p value one what we see value is like is the non false.

109
00:15:38,010 --> 00:15:45,629
Then what is the chance of observing something as ah more extreme than what we calculated.

110
00:15:45,630 --> 00:15:46,800
That's the best of the thing.

111
00:15:46,800 --> 00:15:57,750
So everything that we are doing is under the null hypothesis, you know, calibrating the statistic, seeing how much extreme value we got.

112
00:15:58,980 --> 00:16:07,830
And so if it's on the null, any other questions and thank you for all the questions.

113
00:16:07,830 --> 00:16:16,320
This is good discussion. And that's you know, this is very important for the game to do to learn from each other.

114
00:16:16,650 --> 00:16:29,550
So what are we doing there then? Under the null in this ratio, scaled ratio has the F distribution.

115
00:16:29,790 --> 00:16:36,959
What is the upstairs quantity in the quantity in the numerator is this are divided

116
00:16:36,960 --> 00:16:44,610
by c must were over one one coming from for from the degrees of freedom of SSR.

117
00:16:44,790 --> 00:16:49,320
And as you can see, the C must be from the numerator and denominator just cancel out.

118
00:16:49,890 --> 00:16:53,129
So we are left with SSR divided by one. What is that?

119
00:16:53,130 --> 00:16:59,610
That is MSR. We define this as the mean square regression model.

120
00:17:00,300 --> 00:17:04,080
And what is a C over in minus two that is MSE.

121
00:17:06,690 --> 00:17:18,120
Okay, so the ratio of the MSR to MSE has any distribution with degrees of freedom one comma minus two, again under the null hypothesis.

122
00:17:20,040 --> 00:17:30,929
So the implication is that we can test this null hypothesis by computing f then assessing how reasonable it is to think that a value,

123
00:17:30,930 --> 00:17:37,139
at least as extreme as it would have arising from this distribution.

124
00:17:37,140 --> 00:17:44,550
So this f, this F is my calculated value.

125
00:17:53,600 --> 00:18:00,110
Calculated value of the statistics, and this is the theoretical distribution.

126
00:18:06,060 --> 00:18:16,920
Which is basically my reference distribution or the sampling distribution that I'm doing to calibrate this value against.

127
00:18:18,030 --> 00:18:25,620
Okay. So now here is the procedure.

128
00:18:27,930 --> 00:18:32,370
We talked about the rejection region and the p value method.

129
00:18:33,510 --> 00:18:36,659
Mm hmm. So the procedure is this.

130
00:18:36,660 --> 00:18:43,620
You would compute this EPS statistic based on the data you can compute MSR equals compute MSI.

131
00:18:44,010 --> 00:18:47,370
So the ratio of them is up to them. A C is the F statistic.

132
00:18:48,210 --> 00:18:55,780
Then you can compute the P value. So what would you have to compute the p value?

133
00:18:55,800 --> 00:18:59,550
What would you look at? So let's say here is the F distribution.

134
00:18:59,550 --> 00:19:07,470
Remember, the F distribution is a positive distribution or like F is a positive value random variable.

135
00:19:07,890 --> 00:19:26,610
So if you have calculated F statistic is this one did this point here, then your p value is the area to the right of this calculated F statistic.

136
00:19:28,320 --> 00:19:34,800
So it's this shaded area. So that would be my p value.

137
00:19:40,500 --> 00:19:52,440
The critical value will be. We will be derived only from the right hand side of the distribution, the right date of the distribution,

138
00:19:52,950 --> 00:20:04,679
because the statistic of the if distribution is a positive value distribution and if the P value is less than 1 to 5,

139
00:20:04,680 --> 00:20:07,860
then it would be the null, otherwise it would fail to be the null.

140
00:20:09,450 --> 00:20:13,499
So a few points to note the f first is two sided.

141
00:20:13,500 --> 00:20:15,840
Meaning you are always doing this test.

142
00:20:28,120 --> 00:20:39,220
But that if distribution is one thing because it is a positive valued random variable, the only fixed value above zero.

143
00:20:44,100 --> 00:20:47,580
And he replied the P-value and the reduction needed method.

144
00:20:50,660 --> 00:21:03,980
Yeah. So that's the, you know, sort of the essence of the crux of the F test and the hypothesis testing is that this now coming to that question,

145
00:21:04,610 --> 00:21:10,189
why is it we already had a peaked test or actually before that?

146
00:21:10,190 --> 00:21:13,820
Let me sort of put it into perspective.

147
00:21:13,830 --> 00:21:19,490
So in the SLR, we had constructed the pretest.

148
00:21:23,330 --> 00:21:33,410
And the question is, firstly, let's answer the question, how are the speakers who lived in how are the aftershocks related?

149
00:21:36,350 --> 00:21:43,729
So in simple linear regression, it turns out that they are deeply valid, they are exact.

150
00:21:43,730 --> 00:21:57,559
They would be you the exact same inference if you recall that this statistic was basically the point estimate based on had divided by the standard

151
00:21:57,560 --> 00:22:07,070
input of beta one had so the implicit base square root of MSE versus x and this has a T distribution with that minus two degrees of freedom.

152
00:22:07,520 --> 00:22:12,860
The F statistic calculated value is msaa over MSE.

153
00:22:12,860 --> 00:22:18,550
This has an F distribution with degrees of freedom. One for my minus two from module.

154
00:22:18,770 --> 00:22:29,570
We noted that the square of the T is equal to the F with one and the freedom in terms of distribution.

155
00:22:30,050 --> 00:22:39,230
But it turns out that you can also sure by brute force algebra that the F statistic

156
00:22:40,070 --> 00:22:50,660
so this is algebraically the X statistic is exactly equal to the square in SS in SLR.

157
00:22:51,920 --> 00:23:03,559
So this is from module here you know that distribution these are equivalent these squared is distributional equivalent to F four degrees

158
00:23:03,560 --> 00:23:15,770
of freedom one ended but here in SLR you can even algebraically show that the test statistics f and these squared are exactly the same.

159
00:23:16,160 --> 00:23:19,580
So here is like a small group.

160
00:23:19,580 --> 00:23:23,240
So here is this square. What is p?

161
00:23:24,050 --> 00:23:29,209
P is beta one had divided by the square root of MFC over six.

162
00:23:29,210 --> 00:23:32,570
So I will square it and I'll write it as we go on.

163
00:23:32,570 --> 00:23:38,750
Had squared divided by MSE over a six.

164
00:23:40,340 --> 00:23:52,940
Yes. And what is beta one hat from the least squares method of estimation.

165
00:23:52,940 --> 00:23:56,180
I did have beta one had a cystic fibrosis.

166
00:23:56,660 --> 00:24:14,900
Right. So I squared it. This is six Y with divided by six where I mean the denominator in this C over is C6.

167
00:24:17,950 --> 00:24:23,990
Yes. So what do I have?

168
00:24:25,400 --> 00:24:35,780
I have this done. So to me, this is it's like we're divided by is this X?

169
00:24:37,710 --> 00:24:46,670
Guys in this. That's despair.

170
00:24:47,870 --> 00:24:51,440
What is it? It is.

171
00:24:53,390 --> 00:24:57,080
And this are what I see.

172
00:25:02,440 --> 00:25:10,300
In other words, a cis are divided by one or what messy?

173
00:25:16,600 --> 00:25:25,980
Okay. What is the source of? Is this all by definition is so this is the depression, some obscure side.

174
00:25:26,490 --> 00:25:39,630
So the deviation of white hat from the overall mean so why I had minus y bar square some of this this can.

175
00:25:43,210 --> 00:25:51,770
Now I know why. What you have is I'm just going to plug in the expression for why I had why I had this beaten on pat.

176
00:25:51,770 --> 00:25:58,390
Does that go on, Pat? I'm sexy and I'm going to plug in the least squares estimates for beta naught and beta one.

177
00:25:58,900 --> 00:26:07,270
So what is beta not pack beta not pad is why bar miners beta one had a bar.

178
00:26:10,950 --> 00:26:14,520
Okay. Last Beethoven had been 60.

179
00:26:15,720 --> 00:26:24,870
So I heard plus Beethoven had nine, six minus y bar squared of the whole thing.

180
00:26:28,270 --> 00:26:35,009
It would be fine. I did not jump any steps.

181
00:26:35,010 --> 00:26:39,230
I just, like, loved being the square.

182
00:26:39,300 --> 00:26:45,400
50 for all. Yes.

183
00:26:51,380 --> 00:27:07,080
So the Y bars get canceled and I'm left with this is summation I from 1 to 10 beta one had squared times IXI minus six bars.

184
00:27:12,530 --> 00:27:19,700
So in other words this is better one had where time sex.

185
00:27:26,750 --> 00:27:36,680
Yes. So now if you plug this quantity in the numerator of SSR, what do you get?

186
00:27:36,680 --> 00:27:39,890
You get. Sorry, I'm running out of space.

187
00:27:41,300 --> 00:27:48,920
And just going to bring it here. So I have beta 112 times.

188
00:27:48,920 --> 00:27:52,160
It's this x over a MSI.

189
00:27:59,960 --> 00:28:09,020
Which is the same as in fact, I did not mean to simplify the dice with statistics.

190
00:28:09,650 --> 00:28:15,050
So this one and this 150 are the same.

191
00:28:22,640 --> 00:28:28,820
Yes. So these were the same.

192
00:28:32,410 --> 00:28:36,070
In this simple linear regression that algebraically the same.

193
00:28:41,260 --> 00:28:47,860
Okay. Everybody with me questions.

194
00:28:52,660 --> 00:29:02,830
Okay. So this is more this is a stronger result than the distribution prevents you then seeing that in simple linear regression,

195
00:29:02,830 --> 00:29:06,070
the statistic displayed is algebra.

196
00:29:06,500 --> 00:29:10,579
You get into the statistic f. Okay.

197
00:29:10,580 --> 00:29:13,910
So then that's that's fine. That's good.

198
00:29:14,360 --> 00:29:21,380
Now the question.

199
00:29:22,820 --> 00:29:33,590
So. I did get asked, like, you know, we have the data, so why do we need the exodus?

200
00:29:35,400 --> 00:29:42,600
Because we see that the reference null distributions of the F.A. Square statistics are equivalent.

201
00:29:42,720 --> 00:29:49,860
So why do we need the evidence? So, yeah, so I'm going to argue from a more angle.

202
00:29:49,870 --> 00:30:03,660
So firstly, the advantage of the T test related to the F this is remember the F this day said the the the alternative is always two sided.

203
00:30:03,900 --> 00:30:09,750
So with the F test, you cannot perform one sided tests like this.

204
00:30:10,620 --> 00:30:20,249
But the alternative is greater one greater than zero. So for that did this the advantage with the T test is you can perform one sided

205
00:30:20,250 --> 00:30:26,129
alternative like this and you basically have the same computed t statistic.

206
00:30:26,130 --> 00:30:30,960
But your rejection region now is no one sided.

207
00:30:31,140 --> 00:30:37,590
Like your rejection region would correspond to the like date of the distribution of the distribution.

208
00:30:37,590 --> 00:30:45,750
If you were interested in testing an alternative, the beta one is less than zero, then you would go to the left then of the distribution.

209
00:30:46,050 --> 00:30:55,920
Bottom line is you can do a one sided test using that these statistics which you cannot do using the F statistical good.

210
00:30:56,310 --> 00:31:04,950
But then again, why do we need to f test the reason why we need the f test and which will not be apparent in simple linear

211
00:31:04,950 --> 00:31:12,390
regression because we have only one for video and the B and the F this and we can give you equivalent inference,

212
00:31:13,350 --> 00:31:17,760
but even a context of multiple for radius.

213
00:31:19,410 --> 00:31:23,790
So like if I have x wanted to see export for 40 years,

214
00:31:24,570 --> 00:31:32,910
what the F test will allow you to do is jointly test for the effect of expire next

215
00:31:32,910 --> 00:31:40,620
to 60 exporting the model whereas the D this will allow you to test individually

216
00:31:40,620 --> 00:31:49,709
x1 individually x2 individually three individually s for what the f first allows

217
00:31:49,710 --> 00:31:57,570
you to do is taken together the simultaneous effect of X1 needs to export.

218
00:31:58,710 --> 00:32:04,410
Is that like the simultaneous effect of this covariance?

219
00:32:04,410 --> 00:32:11,610
Is that significant d in explaining the variation in y?

220
00:32:12,720 --> 00:32:25,500
So if this is a general test that allows you to test for the significance of the model when there are multiple already, it's more than one go.

221
00:32:25,500 --> 00:32:31,440
We need another way of talking about it is that you can actually test for multiple slopes.

222
00:32:31,740 --> 00:32:41,250
So I'm kind of going to jump and write this, but you'll see it become more deferent when we are in module E.

223
00:32:41,460 --> 00:32:53,010
So with the F this what you can do is you can test on all hypotheses like this between equal to zero beta two equal to zero beta three equal to zero

224
00:32:53,010 --> 00:33:05,700
where beta one beta two beta three corresponds to the coefficients for x1 x2 extreme the model versus the alternative that at least one is non zero.

225
00:33:09,000 --> 00:33:14,970
So that's what the F test allows you to do best for multiple slopes.

226
00:33:25,250 --> 00:33:34,340
To be tested simultaneously. Okay.

227
00:33:34,640 --> 00:33:39,500
So that's the advantage of the FTC. Once again, in the simple linear regression context.

228
00:33:39,500 --> 00:33:48,490
It sort of is, you know, isn't a big, big deal because they are equivalent and we have only one 4 billion.

229
00:33:48,500 --> 00:33:58,510
So that equivalent. But in the multiple linear regression context, each of them gives you different kind of information.

230
00:33:58,910 --> 00:34:02,270
They have to give you the overall picture.

231
00:34:03,560 --> 00:34:13,190
And the PTC allows you to go and make inference about specific or maybe it's individualistic and one at a time.

232
00:34:13,340 --> 00:34:18,649
Yes. So can I ask something differently? So I guess I see this as one of the hypothesis.

233
00:34:18,650 --> 00:34:22,670
Yeah. So under the like, economic policies are always using the wine bar as food.

234
00:34:22,680 --> 00:34:26,200
Wine. Under the null hypothesis.

235
00:34:26,200 --> 00:34:40,060
Yes. In which case ss r is. SSR, which is why I which is why by this point I am so there is no space in the sum of.

236
00:34:44,190 --> 00:34:49,290
No, but I'm. No, I think there's a.

237
00:34:52,310 --> 00:34:59,240
SSR is a wine bar minus winery that under not so under age.

238
00:34:59,360 --> 00:35:12,020
So I know what to wear identical. So SSR under the knowledge model like you can think about the SS Y as an SS under the Normandy.

239
00:35:15,150 --> 00:35:19,830
Yes. But yeah. I'm also asking the SS. Are we zero?

240
00:35:20,610 --> 00:35:26,730
No, it will not be. No, it will not be.

241
00:35:27,690 --> 00:35:38,129
It will not be algebraically equal to zero. Let's let's actually, you know, like this is this is a kind of idea,

242
00:35:38,130 --> 00:35:46,330
but it becomes much more clearer when you can actually algebraically write the expressions and do that.

243
00:35:46,360 --> 00:35:52,200
So maybe just wait till I kind of prove these results.

244
00:35:53,640 --> 00:36:02,040
And in the context of a simple linear regression, things are things are simple.

245
00:36:02,610 --> 00:36:11,010
But at the same time, like, for example, with the T versus F, I still don't see the relevance clearly.

246
00:36:11,460 --> 00:36:16,830
But in the multiple linear regression, you actually see the relevance.

247
00:36:18,870 --> 00:36:25,800
So let's, let's wait and I'll, I'll show you.

248
00:36:25,860 --> 00:36:29,430
Like, it's not as if it'll equal to zero.

249
00:36:34,010 --> 00:36:41,390
Okay. So now let's talk about F, R squared versus F.

250
00:36:41,990 --> 00:36:46,430
So the other question is, F is a direct function of our square.

251
00:36:48,920 --> 00:36:54,170
It turns out that the F statistic is all squared divided by one minus R squared

252
00:36:54,170 --> 00:37:02,900
times the issue of the degrees of freedom of the SC or what is this all?

253
00:37:04,670 --> 00:37:08,840
Again, this is something that you can show algebraically.

254
00:37:08,840 --> 00:37:13,460
So the question is, well, if F is a direct function of our square,

255
00:37:14,420 --> 00:37:19,460
then the overall leftist is equivalent to testing R squared adjusting for the degrees of freedom.

256
00:37:20,060 --> 00:37:24,230
Why can't we do a test in terms of the Ostrower?

257
00:37:24,230 --> 00:37:33,020
Because in SL are once again once again only in SL are the null hypothesis that beta one

258
00:37:33,020 --> 00:37:46,160
equal to zero versus beta one is not equal to zero is equivalent to doing a pistol R squared,

259
00:37:49,280 --> 00:38:00,800
which is, you know, we talked about the Capuchin diabetes in Oxford equal to zero versus the alternative that R squared is greater than zero.

260
00:38:04,900 --> 00:38:11,680
You remember, our spot is a quantity, a number between zero and one.

261
00:38:13,450 --> 00:38:16,720
So these two are equally valid. Then why not?

262
00:38:17,710 --> 00:38:28,660
Why not do a test based on R Square? And the reason why we don't do a test based on R squared, rather, on that on the F,

263
00:38:28,900 --> 00:38:37,090
using the sum of squares is because analytically like remember we need a sampling distribution.

264
00:38:37,210 --> 00:38:45,460
Right. So analytically we actually have to have a different distribution.

265
00:38:45,820 --> 00:38:55,070
And it is difficult or it is impossible to get a analytically tractable distribution for this R-squared.

266
00:38:55,070 --> 00:38:59,010
That's where it is, is a is used as a summary statistic.

267
00:38:59,020 --> 00:39:09,610
But is this F the MSR or what? MSE You can establish a distributional property for that F, which is an F distribution.

268
00:39:09,910 --> 00:39:16,750
So the reason why we do a test using the issue of the sum of squares, of the mean squares,

269
00:39:17,050 --> 00:39:30,730
is because we can get an analytically tractable distribution for the behavior of the sum of squares through the F.

270
00:39:39,830 --> 00:39:45,770
Which we don't get with the US with. So that's why we do the.

271
00:39:49,460 --> 00:40:00,410
We are going to finish this with the UK small example and then we will go for a break.

272
00:40:01,030 --> 00:40:11,300
So the example that we are going to talk about is a study that was carried out in order to evaluate the relationship between Bartlett and SGA levels.

273
00:40:12,320 --> 00:40:16,400
Data were collected on a total of 31 mothers to be on their newborns.

274
00:40:17,510 --> 00:40:21,920
And we are picking a straight line regression model for this analysis.

275
00:40:22,250 --> 00:40:27,530
I have uploaded the data file, which is stored as a permanent sized dataset.

276
00:40:27,800 --> 00:40:31,460
And then and if you want to kind of replicate that analysis,

277
00:40:31,790 --> 00:40:43,700
so we are going to create a simple linear regression model using the L m function in R, and you can also do a like verify using hand calculations.

278
00:40:44,420 --> 00:40:47,840
We are going to opt in the statistic and corresponding P values.

279
00:40:48,200 --> 00:40:57,739
And essentially that's the point of this exercise to show you the using a small dataset how

280
00:40:57,740 --> 00:41:03,680
the statistic and these statistics are related in the same being a regression point based.

281
00:41:04,250 --> 00:41:17,420
So here is the dataset. Again, the sample size n equal to 31 and the outcome is with measured in grams per 100.

282
00:41:17,870 --> 00:41:31,720
And the x will radiate or the single predictor in the simple in any version model is SGA level measured as MG every 24 hours based on union samples.

283
00:41:33,680 --> 00:41:37,720
So we installed and loaded a package called site.

284
00:41:37,730 --> 00:41:44,030
This is a very useful package to do sort of elevated descriptive statistics.

285
00:41:44,570 --> 00:41:57,490
And check if there's any missing data. So the first set of slides is essentially giving you a descriptive statistics for the dataset.

286
00:41:57,500 --> 00:42:02,420
So I'm just going to circle a couple of things that we are going to use.

287
00:42:02,990 --> 00:42:10,879
So here is Birthweight 4.7. The mean is 32 and the standard deviation is 4.7.

288
00:42:10,880 --> 00:42:16,910
For this is my Y and then my X is a sphere level.

289
00:42:17,270 --> 00:42:25,009
So again, based on a sample of size, 31 mean is 17.23 and the standard deviation is 4.75.

290
00:42:25,010 --> 00:42:31,160
And then we also have the medians, the range mean max and so on.

291
00:42:32,330 --> 00:42:37,729
Okay. So first we feature simple linear regression models in the end function.

292
00:42:37,730 --> 00:42:41,960
So again, this is a familiar output for you.

293
00:42:42,170 --> 00:42:54,290
I'm going to mar a few things. So the estimate column gives the least squares estimates for beta note and beta one.

294
00:42:54,620 --> 00:43:12,950
So the intercept, the estimate for the intercept is 21.52 and the slope is 1621608 with standard errors given by 2.6 to 1.14.

295
00:43:13,490 --> 00:43:19,010
So that statistic remember, this is the column that we hadn't looked at before.

296
00:43:19,280 --> 00:43:25,339
So this is the column that gives you the key statistics for the fourth line.

297
00:43:25,340 --> 00:43:35,090
It's the statistic corresponding to the hypothesis for the intercept equal to zero versus a two sided alternative.

298
00:43:35,480 --> 00:43:45,950
And then the second line, the 4.1 fold is the P statistic corresponding to a test of the slope equal to zero.

299
00:43:46,930 --> 00:43:59,240
And this last column here gives the P values for the deaths of the intercept and the slope.

300
00:44:00,170 --> 00:44:03,650
So what is the what is the.

301
00:44:04,160 --> 00:44:10,370
Let's come to the next slide and I'll ask you the specific question, but let's remember these quantities.

302
00:44:11,180 --> 00:44:16,880
One other thing that I want to highlight here, the residual standard error on 29 degrees of freedom.

303
00:44:17,540 --> 00:44:28,040
What is this, the residual standard error. This is the square root of MSE, right?

304
00:44:28,340 --> 00:44:33,020
So this is the square root of MSE 3.82.

305
00:44:33,800 --> 00:44:37,040
And where is the 29 degrees of freedom coming from?

306
00:44:37,520 --> 00:44:40,880
31 minus. Minus two. Yes.

307
00:44:41,690 --> 00:44:50,870
Okay. Multiple R squared. Remember, this is the percent of variation explained by SGA level.

308
00:44:51,230 --> 00:44:58,730
So here are squared is point c seven or to resumption interpretation in a minute.

309
00:44:58,940 --> 00:45:08,470
And then the last of this tick I'm going to highlight is the F statistic, 17.16 on one and 29 degrees of freedom.

310
00:45:13,090 --> 00:45:28,600
Okay. So the first thing before we move on to the next slide is this f f value, 17.16 is the squared of another quantity on this slide.

311
00:45:29,560 --> 00:45:40,030
What is that the p value? The 4.14 if you squared 4.14, you will get this 17.16.

312
00:45:42,170 --> 00:45:47,360
As we saw in the simple linear regression model that these F is equal to square.

313
00:45:49,100 --> 00:46:04,210
Okay. Now you can calculate all of these by hand using the formula for building our pad and with the one hand and using some subscripts.

314
00:46:04,220 --> 00:46:11,370
This is x, why is this x? This is Y and all the, you know, sort of descriptive summary statistics for X and Y.

315
00:46:11,390 --> 00:46:14,690
I'll leave this up to you to kind of convince yourself.

316
00:46:15,920 --> 00:46:19,370
Let's go to the slide.

317
00:46:19,370 --> 00:46:28,609
So another way of getting that F statistic is through the ANOVA function and the ANOVA function,

318
00:46:28,610 --> 00:46:40,489
we give you also the breakdown of the partition of the sum of squares, the way we had partition that says Y star.

319
00:46:40,490 --> 00:46:49,190
And I see it shows that partitioning and the corresponding analysis of variance stable based on the sums of square degrees of freedom,

320
00:46:49,190 --> 00:47:02,990
the mean square and the value. So this to 50.57 is a are this for 23.43 is my SSP.

321
00:47:04,970 --> 00:47:13,970
The degrees of freedom for SSR is one because I have only one predictor SGA and the total degrees

322
00:47:13,970 --> 00:47:26,090
of freedom is the degrees of freedom for is this Y is in minus two which is 2931 minus two.

323
00:47:26,900 --> 00:47:31,570
Correct. When nine. Okay.

324
00:47:32,330 --> 00:47:36,440
So. So you've got amazing notices.

325
00:47:48,770 --> 00:47:57,890
It's Jesse. It's 29. So now the mean square for division is two 50.57 divided by one.

326
00:47:58,550 --> 00:48:05,850
And the mean square for error is four 23.43 divided by 29, which gives me the 14.6.

327
00:48:05,850 --> 00:48:28,370
So the ratio of these two one deep, this is my 17.16 I calibrated against an F distribution with degrees of freedom one by 29 and get this p value.

328
00:48:32,690 --> 00:48:35,820
Yes. Okay.

329
00:48:36,990 --> 00:48:43,590
Now again, you can convince yourself using hand calculation.

330
00:48:45,630 --> 00:48:52,590
Now, let me let let's do this part and then we will for a break because the other things they going to shorten.

331
00:48:53,310 --> 00:49:00,030
So now I am asking you estimate the marginal and conditional variance of rate.

332
00:49:01,260 --> 00:49:06,059
You already saw the saw the descriptive statistics.

333
00:49:06,060 --> 00:49:10,170
You saw the feet from the alarm function.

334
00:49:10,170 --> 00:49:16,350
You saw the I know what the above. So here is the question.

335
00:49:16,800 --> 00:49:21,320
The first question asks estimate the marginal variance of pocket.

336
00:49:21,600 --> 00:49:26,310
So let's first try to understand what does marginal variance mean?

337
00:49:29,400 --> 00:49:32,549
Does somebody have any guess? What does MARTIN?

338
00:49:32,550 --> 00:49:42,860
Obedience and small. Maybe another good way of saying it is like two buildings.

339
00:49:48,820 --> 00:49:52,540
So think about the context where you don't have any eggs.

340
00:49:52,990 --> 00:50:05,640
You don't have any information about the whole body. It just as a best guess, an intelligent guess for why would be Viber.

341
00:50:06,220 --> 00:50:11,410
So here I'm asking you like if there was no eggs, what is the variance of why?

342
00:50:21,950 --> 00:50:27,370
They begin. Yes.

343
00:50:28,870 --> 00:50:34,509
Right. And you can just simply go through the sample and calculate the SSP.

344
00:50:34,510 --> 00:50:43,870
I mean, one and, you know, sort of divided by N minus one or you can go back and look at your descriptive statistics.

345
00:50:44,890 --> 00:50:50,260
Slide. So we got here is my descriptive statistics.

346
00:50:50,980 --> 00:51:02,550
Slide. So what was and this 4.74 is the standard deviation.

347
00:51:03,300 --> 00:51:09,690
So I just squared it. That gives me the marginal variance.

348
00:51:10,350 --> 00:51:22,650
So basically it's 4.7 for Square and that is where it is 22.46 seven.

349
00:51:27,260 --> 00:51:36,230
Okay. And you can then again use calculation to establish this.

350
00:51:37,070 --> 00:51:41,780
Now I'm asking estimate the variance of board weight conditional on this fuel level.

351
00:51:41,780 --> 00:51:44,359
So now I'm bringing as your level information.

352
00:51:44,360 --> 00:51:58,340
So I'm saying like what would be the variance of Y, but given your level first before telling me what the how would you estimate this?

353
00:51:59,030 --> 00:52:06,560
Can you sustain me? Will this number be smaller or larger than the 22.467?

354
00:52:10,520 --> 00:52:14,780
In this. Are you from intuition?

355
00:52:20,870 --> 00:52:23,870
Okay. That's a that's a finance head.

356
00:52:23,870 --> 00:52:32,270
But can you kind of even go back to more basics, more first principles, type of argument?

357
00:52:44,950 --> 00:52:49,720
Do you have a you do have an answer, you know.

358
00:52:52,550 --> 00:52:57,830
Yes. So what this is to me, the conditional variance kind of looks like, you know,

359
00:52:57,830 --> 00:53:03,979
you fix your X and then you have some kind of Y estimate you get from that and then you kind of sample around that,

360
00:53:03,980 --> 00:53:08,980
which is basically you're the sigma squared is going to be less than total.

361
00:53:09,620 --> 00:53:16,100
Thank you. That that's that's a beautiful explanation because you're going to get it.

362
00:53:16,370 --> 00:53:27,500
So maybe I can draw a picture here. So this is why this is X and let's do this again.

363
00:53:33,080 --> 00:53:39,000
So the total, the marginal variance is basically I have blocked out this far.

364
00:53:39,350 --> 00:53:46,249
I'm only looking at the Y axis and the range or the distribution of the dispersion of the Y.

365
00:53:46,250 --> 00:53:55,969
So that gives me the 22.4, six, seven and the condition the buildings of birth with conditional on its shear level.

366
00:53:55,970 --> 00:54:06,440
So variance of y given a fixed x is basically let's add this is my feet sticks x not so I am going to go and look at.

367
00:54:08,730 --> 00:54:20,310
Sort of the b the variance of the Y values for the peak stakes not.

368
00:54:23,980 --> 00:54:27,950
So this is going to be and you can sort of do it for given it.

369
00:54:27,950 --> 00:54:31,360
It's not. So this is, of course, going to be smaller.

370
00:54:33,340 --> 00:54:43,210
And. Exactly. So. So you would given an X nor did they believe informative that you would have a guess about what it why it is.

371
00:54:43,450 --> 00:54:48,069
So you are looking at deviation, supply, values and all that.

372
00:54:48,070 --> 00:54:57,660
Y. Which is essentially your white hat from the regression model.

373
00:54:58,710 --> 00:55:02,970
So lo and behold, what is this? Conditional Vivian's.

374
00:55:03,420 --> 00:55:06,510
It's actually the MSE.

375
00:55:10,480 --> 00:55:19,660
So this is the embassy. And that message we saw was 14.6.

376
00:55:19,900 --> 00:55:26,230
In other words, we had also seen it from the additional 3.8 inch square.

377
00:55:27,250 --> 00:55:40,540
So given a value of X and the range of values for Y is much smaller than what the total range of Y is, if we did not have any information about that.

378
00:55:42,550 --> 00:55:48,220
Okay. Does not depend on your level.

379
00:55:48,250 --> 00:55:51,200
Carry out an appropriate t test. Of course it does.

380
00:55:52,240 --> 00:56:03,069
So we just saw that, you know, the point estimate was for Beethoven was one six or eight and the standard error was .146.

381
00:56:03,070 --> 00:56:06,130
So the statistic is the issue of those two quantities.

382
00:56:06,730 --> 00:56:14,530
4.14 with the beaver and it's distributed as D with 29 degrees of freedom.

383
00:56:15,490 --> 00:56:27,220
So you calibrated against that D and you get a P value of a point or or two seven, which is less than much less than 1 to 5.

384
00:56:27,670 --> 00:56:37,360
So you would reject the null and conclude that there is evidence to believe that part could depend on SGA level.

385
00:56:40,030 --> 00:56:46,290
Okay. Then you then have to determine whether SGA levels are associated with birthweight.

386
00:56:46,820 --> 00:56:54,700
So I would point your attention to the and what you will hear.

387
00:56:56,410 --> 00:57:06,700
So here is once again the F statistic, which is the ratio of MSR over MSE.

388
00:57:07,150 --> 00:57:17,500
And we showed that D 17.16 is the square of that D, which was 4.1 for square.

389
00:57:18,730 --> 00:57:26,380
So my F statistic calculated value is 17.16 and a p value with a p value of one or two.

390
00:57:26,980 --> 00:57:44,020
So yes, based on the F statistic, also I would reject the null and conclude that SGA levels are birthweight is associated with SGA levels.

391
00:57:45,010 --> 00:57:52,540
So just highlight on the from the analog table and the simple inverted version output.

392
00:57:53,260 --> 00:57:58,060
And then lastly, what percentage of the variation in Birchwood is explained by SGA level?

393
00:57:58,070 --> 00:58:01,090
So what am I asking about? I'm asking with the R squared.

394
00:58:01,900 --> 00:58:09,460
Right. And the R squared from this model was between three seven, I believe.

395
00:58:09,610 --> 00:58:18,880
Let's go back there. So here is the monster blast 12.37.

396
00:58:19,380 --> 00:58:38,430
So the interpretation is that 37% of the variation in Berkeley is explained by.

397
00:58:47,090 --> 00:58:52,670
SAGAL That. Okay.

398
00:58:53,060 --> 00:58:57,400
So that's it. Questions?

399
00:58:59,450 --> 00:59:03,500
Yes. There was another Ask Kurt Cobain adjusted. Yes.

400
00:59:03,710 --> 00:59:06,860
Okay. Thank you. So the ad dusted off. Good.

401
00:59:07,730 --> 00:59:10,970
We are going to talk about it, but later on.

402
00:59:11,630 --> 00:59:13,910
But let me tell you conceptually what it is.

403
00:59:13,910 --> 00:59:23,450
So it does so again, in the in the context of simple linear regression, these all become sort of these are difficult to see.

404
00:59:26,210 --> 00:59:37,280
And that's why am seeing quite a few things that live video and often in the market building elevation domain and we would sort of move these things.

405
00:59:37,760 --> 00:59:46,640
But conceptually, this is what happens as you add more and more variables in the model.

406
00:59:47,630 --> 00:59:52,760
Your artwork will actually stay the same or will be increasing.

407
00:59:53,300 --> 01:00:00,709
So there is with each addition of variables, whether it's some, it is significant or not.

408
01:00:00,710 --> 01:00:05,180
R-squared is going to be monotonically non decreasing.

409
01:00:06,710 --> 01:00:09,740
So that idea of adjusted R-squared is if like it,

410
01:00:09,770 --> 01:00:14,959
let's see if I have a model with 50 covariates and my R squared is I'm making

411
01:00:14,960 --> 01:00:29,420
this up is 65% and I have a model with ten covidiots and my R-squared is 63%.

412
01:00:30,290 --> 01:00:40,969
So to get any movement of 2%, I'm adding like 40 mode over to its I am having to add 40 more to it.

413
01:00:40,970 --> 01:00:53,629
So like from a in terms of a model selection, parsimony versus gain, which one is better and the I add just a dash,

414
01:00:53,630 --> 01:01:01,710
but what it does is it actually puts a penalty based on the number of all variants in the model.

415
01:01:01,740 --> 01:01:10,219
So it's kind of trying to make a compromise as to, okay, is it what, you know, kind of is that 2% gain,

416
01:01:10,220 --> 01:01:17,510
what it might be making the model so much complex by adding for the other four variants,

417
01:01:18,290 --> 01:01:25,250
if you only compared the squares between the models, then you would choose the one with 65%.

418
01:01:25,730 --> 01:01:31,820
But what I just did ask for will do it will put a penalty for those additional 40.

419
01:01:31,820 --> 01:01:37,760
That will be it and it will adjust for the number of variables in the model.

420
01:01:37,760 --> 01:01:44,240
So based on that dusted off square, you will end up choosing like the more parsimonious model.

421
01:01:44,690 --> 01:01:50,009
So that's what it does. But we will talk about it more formally asking how do we do that?

422
01:01:50,010 --> 01:01:54,260
Just okay, any other questions?

423
01:01:59,950 --> 01:02:15,410
Okay. Let's take a break. And when we come back, let's come back at 921 and we will do a very quick review of Matrix Algebra.

424
01:02:15,430 --> 01:02:22,300
This is usually be assigned as read on your own review on your own type of module.

425
01:02:23,170 --> 01:02:26,410
But I do not want to start no Julie today.

426
01:02:28,060 --> 01:02:36,070
So we are going to use the rest of the class time and do a very quick review of the of module D.

427
01:02:36,250 --> 01:02:46,930
But the key is to remember that this is a review that you should do on your own because when we come back on Thursday,

428
01:02:46,930 --> 01:02:50,079
we are going to start writing them.

429
01:02:50,080 --> 01:02:53,680
And I want you to make connection. Okay.

430
01:02:53,830 --> 01:02:57,730
So let's take a break. Please come back at 921.

431
01:03:15,071 --> 01:03:21,191
Do a very quick review of matrix results.

432
01:03:22,121 --> 01:03:28,151
And please make sure that this should all be.

433
01:03:32,161 --> 01:03:44,481
Very familiar with materials for you, but just want to refresh your memory memory and please make sure that like,

434
01:03:44,521 --> 01:03:52,471
whatever we don't get to talk about you do it on your own at home.

435
01:03:54,121 --> 01:04:04,951
Okay. So as I mentioned that we're going to heavily use a matrix algebra in multiple linear regression.

436
01:04:05,821 --> 01:04:15,271
So we're going to talk about different types of matrices, matrix properties, matrix operations, random vectors and so on.

437
01:04:17,401 --> 01:04:36,211
So a vector is an area of numbers, for example, and the vector be a Q cross one dimensional vector denotes a vector because few fewer and one column.

438
01:04:36,571 --> 01:04:39,631
And it's an area of numbers like B1, B2, BQ.

439
01:04:48,411 --> 01:04:54,901
Okay. Note that all vectors are shown to be column vectors.

440
01:04:54,921 --> 01:05:03,921
Unless stated otherwise. The Matrix, on the other hand, is a two dimensional rectangular area of numbers,

441
01:05:03,921 --> 01:05:09,051
and it is usually denoted by the number of floors and the number of columns.

442
01:05:09,441 --> 01:05:13,100
So here is an example of our plus C matrix.

443
01:05:13,101 --> 01:05:25,191
That is our rows and C columns. And it's an area of numbers like e1112 A1 C is the first row a21222282 sees the second

444
01:05:25,191 --> 01:05:34,401
row and e d refers to the element in the I through and the column of the matrix.

445
01:05:35,931 --> 01:05:41,010
Okay. What about some special effects of matrix?

446
01:05:41,011 --> 01:05:46,551
Some a scalar, a one by one or a single number?

447
01:05:46,551 --> 01:05:54,051
Is is a matrix can be viewed as a matrix, a one by one matrix or one by one vector.

448
01:05:54,471 --> 01:05:59,211
A square matrix is one where there are equal number of rules and columns.

449
01:05:59,781 --> 01:06:05,570
A diagonal matrix has only elements on the main diagonal that are non-zero.

450
01:06:05,571 --> 01:06:13,431
So here is an example of a diagonal matrix where as you can see, only the diagonal elements 540 do non zero.

451
01:06:13,431 --> 01:06:17,151
The rest of the matrix has all elements equal to zero.

452
01:06:17,691 --> 01:06:21,201
What is an identity matrix? It's a square diagonal matrix.

453
01:06:21,201 --> 01:06:31,940
The width wants along the mean diagonal and it's a very important matrix in statistics and it is

454
01:06:31,941 --> 01:06:42,951
denoted by capital I with subscript equal to the number of rows or number of columns of the matrix.

455
01:06:43,431 --> 01:06:48,111
And since it's a square matrix, so the number of fluid is equal to the number of balance.

456
01:06:48,111 --> 01:07:00,861
So you can describe this identity I matrix with dimension in this case four because into four rows and for balance and the reason why it's

457
01:07:00,861 --> 01:07:17,391
such an important matrix in statistics is that if B is in our cross C matrix and if you pre multiply B by an identity matrix of dimension R,

458
01:07:17,451 --> 01:07:27,471
then you get back B and similarly, if you post multiply B by an identity of the matrix of dimension C, then you get back.

459
01:07:27,471 --> 01:07:39,141
B So pre multiplying or force multiplying the matrix by an identity matrix, all appropriate dimension gives you back the matrix itself.

460
01:07:40,551 --> 01:07:48,831
Some special other special matrix is Annapurna and triangular matrix is one such that the idea,

461
01:07:48,871 --> 01:08:04,791
the limit of the matrix is 009 greater danger and a lower triangular matrix is one way and the idea element is zero for all are less than two.

462
01:08:05,271 --> 01:08:10,701
So here are examples of upper triangular and lower triangular matrices.

463
01:08:11,961 --> 01:08:18,711
And so these are really important matrices that we will see in statistics and other one.

464
01:08:18,711 --> 01:08:24,830
Another definition is a symmetric matrix such that e.g. equal to a die.

465
01:08:24,831 --> 01:08:31,581
So the element in the i2 and the on is equal to the element in the j true and i twala.

466
01:08:31,911 --> 01:08:35,421
So here is an example of a symmetric matrix. As you can see.

467
01:08:35,781 --> 01:08:43,551
Of course, leaving aside the diagonal elements, here is the element in the second row,

468
01:08:43,551 --> 01:08:50,810
first column of V, and that is equal to the element in the first row.

469
01:08:50,811 --> 01:09:06,501
And second column of is. So a symmetric matrix will always have ID equal to HDI for all did not equal by any diagonal matrix is symmetric because

470
01:09:06,501 --> 01:09:15,231
the diagonal element matrix is one which has only non-zero elements in the DI Winnie and restored to the zero matrix.

471
01:09:15,951 --> 01:09:32,211
By definition, all elements are zero. A vector one or the one vector is a vector if all the elements are equal to one,

472
01:09:32,601 --> 01:09:46,901
and usually for a vector also u denoted by the the start of the vector and the dimension used as a substitute the middle.

473
01:09:47,031 --> 01:09:56,721
Except one is again a matrix, a rectangular array of numbers that are all one.

474
01:09:56,731 --> 01:10:00,561
So here is the one matrix.

475
01:10:02,781 --> 01:10:13,010
Matrix functions. What is the transport? Transport of a matrix is essentially are kind of inverting the matrix on its own

476
01:10:13,011 --> 01:10:20,571
inside where then the columns of a transpose become the rows of E and vice versa.

477
01:10:21,681 --> 01:10:25,521
So here is a to transpose a.

478
01:10:26,841 --> 01:10:30,201
What you do is the rows become columns.

479
01:10:31,491 --> 01:10:36,291
So in a transpose, the first column is one, three, four.

480
01:10:36,681 --> 01:10:40,551
The second column is 210 and third column is 579.

481
01:10:42,681 --> 01:10:51,481
If is equal to a transpose, then that means a symmetric matrix addition proceeds element wise.

482
01:10:51,601 --> 01:11:09,560
So if you have two matrices A and B of dimension of the same dimensions, then E plus B is obtained by doing element Y's addition of the two matrices.

483
01:11:09,561 --> 01:11:21,731
But this is only defined for matrices of the same dimension or of the same order vector in a product sum of element wise product.

484
01:11:21,741 --> 01:11:32,391
So if you have a vector A which is a three plus one vector with evenly 283 and a vector B, which is a gain of three plus one with elements b1,

485
01:11:32,391 --> 01:11:46,850
b2, b three then e transpose b the or the inner product of E transport, then b is obtained by taking the sum of EIB EI over EI from 1 to 3.

486
01:11:46,851 --> 01:11:58,221
Another way of writing a transpose would be I think it is a draw vector and maybe I should use a few conventions here and notation.

487
01:11:58,701 --> 01:12:03,651
Let me just explain that. So you can write the transpose as.

488
01:12:13,611 --> 01:12:22,371
As a rule vector like this Iran, A, a, c and so that in a productive way transposed with these submission.

489
01:12:22,371 --> 01:12:35,660
The idea I from one group to a few conventions and I m used to writing a transpose like this.

490
01:12:35,661 --> 01:12:43,941
So this is another way of writing transpose e and a super state of like a slash.

491
01:12:46,611 --> 01:12:55,251
So oftentimes if I'm writing and it's so ingrained in me that I love, yeah, that's one thing and then the other.

492
01:12:56,211 --> 01:13:04,071
But I'll try to sort of adhere to like the E and sub superscript of T and another thing is

493
01:13:04,581 --> 01:13:15,111
vectors are often written as either like a bald face or a kind of a squiggly at the bottom.

494
01:13:17,181 --> 01:13:23,951
This or that's another. We in reach.

495
01:13:26,421 --> 01:13:29,731
A vector is written and for a matrix.

496
01:13:30,321 --> 01:13:48,951
Again, it's written either as a boy this or sometimes you'll see in walks like a sort of a dash underneath the matrix.

497
01:13:49,191 --> 01:14:01,251
So just things to be a of matrix product has the matrix the product C of two matrices and D

498
01:14:02,061 --> 01:14:10,131
equals the MM side equals the inner product of the I through of E and the G column of B.

499
01:14:11,721 --> 01:14:18,561
So essentially what you do is here is if this is A and this is the B,

500
01:14:19,191 --> 01:14:26,660
then the way the matrix multiplication goes is the first rule for the E multiplied

501
01:14:26,661 --> 01:14:33,831
by the first column of B that gives you the one one element of the product matrix,

502
01:14:34,641 --> 01:14:42,171
the fourth column of B with the second column of B, you do the one to it element of the product matrix and so on.

503
01:14:42,831 --> 01:14:48,111
So it's only defined when the number of columns in E equals the number of rule, then be.

504
01:14:49,311 --> 01:14:56,661
Okay. So, so that's the that's that's the requirement.

505
01:14:57,081 --> 01:15:03,201
But it the way you calculate the Matrix product is not an element wise product,

506
01:15:03,201 --> 01:15:15,531
but it's the inner product of the I to open the summit functions, multiplication of scalar proceeds, just element wise.

507
01:15:16,551 --> 01:15:31,101
So the matrix inner product E, transpose E and matrix outer product E, transpose both of the inner and outer products are going to be symmetric.

508
01:15:32,841 --> 01:15:36,410
Here is another important quantity that we will use a lot.

509
01:15:36,411 --> 01:15:42,441
In addition is trace of three or four matrix.

510
01:15:43,191 --> 01:15:49,611
So three, four, four matrix that denotes the sum of the elements along the main diagonal of the.

511
01:15:49,821 --> 01:16:00,491
So we can write it as summation II, where II is the either diagonal element or IGF element of the matrix.

512
01:16:00,571 --> 01:16:18,831
The rules of matrix operators commutative law e plus be equal to be plus the scalar e multiplied by b is going to be element y's

513
01:16:18,831 --> 01:16:32,821
multiplication because literally is a scalar and that will be equal to we don't see distributive laws e multiplied with B plus C,

514
01:16:32,841 --> 01:16:36,501
you can distribute it and and write it as a B-plus.

515
01:16:36,651 --> 01:16:43,881
C similarly, both multiplying the sum of two matrices, B and other matrix.

516
01:16:43,881 --> 01:16:46,931
D all so you can write it as B, D plus.

517
01:16:47,421 --> 01:16:56,931
So these are called the distributive laws, associative laws, A, plus B, plus C, E, plus B plus C.

518
01:16:57,891 --> 01:17:12,921
And in terms of the product of A, B, multiples multiplied by C, you can write it as pre multiplying B, C by E.

519
01:17:13,071 --> 01:17:27,201
So this also holds, but only then these matrix multiplications are valid or in other words, the dimensions of A, B and C form,

520
01:17:27,201 --> 01:17:37,641
form B, matrix, multiplication, transpose plus transpose of E plus B, you can write it as a transpose plus B transpose.

521
01:17:38,031 --> 01:17:44,541
And here is another one that we are going to rely on.

522
01:17:44,541 --> 01:17:52,430
So the product of AB, if you transpose that, then it is B transpose times.

523
01:17:52,431 --> 01:17:55,911
E transpose, okay.

524
01:17:56,721 --> 01:18:08,511
And extending it, father a, b, c transpose is you basically take the product of the transpose matrices backwards,

525
01:18:08,691 --> 01:18:12,141
c transpose times, we transpose time saved articles, etc.

526
01:18:15,871 --> 01:18:21,151
Okay. Linear dependance a set of vectors x1 x2.

527
01:18:21,541 --> 01:18:36,151
Extremely excu is linearly dependent. You are scalar c1 c2 52 CQ non-zero such that summation C.K. X is equal to the vector zero.

528
01:18:38,601 --> 01:18:44,481
This is the big. This is the definition of linear dependance of two vectors.

529
01:18:47,571 --> 01:18:54,891
So we need the linear dependance of vectors, not two vectors like ubiquitous here.

530
01:18:55,551 --> 01:18:59,450
If 000 is the only set of scalar satisfying one,

531
01:18:59,451 --> 01:19:10,340
then x1 x2 excuse are said to be linearly independent and if it's wanted to exclude linearly independent,

532
01:19:10,341 --> 01:19:16,441
then then there is no excuse that can be expressed as a linear function of the other itself.

533
01:19:18,361 --> 01:19:22,921
Okay. So once again, this is a property that we're going to use a lot of,

534
01:19:23,661 --> 01:19:32,301
not a result that it would be used a lot in handling a depression, context, linear independence.

535
01:19:32,341 --> 01:19:46,261
Then what is the rank of a matrix? The rank of a matrix denoted as ranking equals the number of linearly independent columns or rows of E.

536
01:19:47,611 --> 01:20:02,491
So for any matrix a row rank and column rank but equal, if the matrix is of dimension are C,

537
01:20:02,941 --> 01:20:10,111
then the rank of E has to be less than or equal to the minimum of R pharmacy.

538
01:20:14,931 --> 01:20:26,391
If ease of dimension are by C and ran Coffey is the minimum of ANSI then is said to be full gram.

539
01:20:28,311 --> 01:20:43,971
Full length means that the matrix has depending on you know like a which one is smaller or key.

540
01:20:44,451 --> 01:20:54,740
But the matrix has uh, either are linearly independent rows or c linearly independent columns.

541
01:20:54,741 --> 01:21:00,571
It's the minimum of ANSI, but because that's like full length means.

542
01:21:00,931 --> 01:21:04,580
Yes. Say it again.

543
01:21:04,581 --> 01:21:08,660
I mean, that was about 2 minutes or something.

544
01:21:08,661 --> 01:21:12,411
That is enough to make it make it in in vertical. Yeah, but.

545
01:21:13,281 --> 01:21:22,971
Well no, not really. Let's, let's come to the but but that's the, that's the basic criteria or the like.

546
01:21:22,971 --> 01:21:27,371
If it's less than two, then it's not important. Yeah.

547
01:21:29,151 --> 01:21:42,501
If he is across the sea and then he is less than the minimum of our policy, then he is rank deficient.

548
01:21:44,011 --> 01:21:48,341
So if it is rank deficient, then it will not be workable.

549
01:21:50,571 --> 01:22:01,041
Continued rank. So here is a matrix E where the elements are given by 1243065 313.

550
01:22:01,491 --> 01:22:06,411
So the rank of this matrix is two.

551
01:22:07,191 --> 01:22:19,521
Because as you can see that if you add the second row with two times the force,

552
01:22:19,731 --> 01:22:26,151
so second column the two times the first column, then you get the third column.

553
01:22:26,331 --> 01:22:32,900
So this is an example where I would see that the columns of a have a linear

554
01:22:32,901 --> 01:22:41,481
dependance because this is the third column and and this is the second column.

555
01:22:42,261 --> 01:22:48,401
This is the first column for the third column can be expressed as a linear combination of the first and second columns.

556
01:22:51,621 --> 01:22:56,591
That is what makes it less than full rank.

557
01:22:57,441 --> 01:23:05,681
So there are only two columns here that are linearly independent of each other, or in other words, within any set of two columns.

558
01:23:05,691 --> 01:23:11,751
Neither is a linear combination of the other. Here is another example.

559
01:23:13,311 --> 01:23:21,351
This matrix b13 5.51152.5 and negative two negative six negative ten.

560
01:23:21,771 --> 01:23:25,521
This word kicks heads of N equal to one.

561
01:23:25,911 --> 01:23:33,621
Because if you take the first column and if you multiply that by three, then you get the second column.

562
01:23:34,491 --> 01:23:42,561
And if you multiply the first column by five, you get the third column.

563
01:23:45,471 --> 01:23:52,251
So this is an example. All these things rank is only one.

564
01:23:54,671 --> 01:23:59,711
So. Some flaws.

565
01:24:03,251 --> 01:24:14,441
The rank of three is between one and minimum of ANSI, but art is the number of floors and sees the number of fallen off the matrix set.

566
01:24:14,741 --> 01:24:24,761
The rank of the product of EB vary and the matrices is less than equal to the minimum of rank and lengthy.

567
01:24:26,831 --> 01:24:32,591
Multiplying by a full rank matrix never reduces rank determinants.

568
01:24:33,131 --> 01:24:38,141
There is some notation. The again determinant is an important one to that.

569
01:24:38,951 --> 01:24:43,751
We will sort of fuze in as in in the context of linear regression,

570
01:24:44,051 --> 01:24:54,791
the determinant of E is denoted by E and two bars around D or end of the re absolute value as written.

571
01:24:55,361 --> 01:25:09,520
So it's kind of a summary measure about the you can see like a summary quantity for E and the determinant of E is calculated based on all factors.

572
01:25:09,521 --> 01:25:12,731
For a matrix you cross to matrix B,

573
01:25:14,681 --> 01:25:26,211
you can write the the determinant of in terms of the pull factors of B and this is something that I would just say,

574
01:25:26,231 --> 01:25:30,041
like pull back and give you one more time in, in detail.

575
01:25:30,821 --> 01:25:34,661
But the idea is that it is the equal to the weighted,

576
01:25:34,661 --> 01:25:44,141
some of the pull factors for any do or any column so that some that's the determinant for a two by two matrix.

577
01:25:44,141 --> 01:25:53,290
It's very easy to calculate the determinant. So if you have a matrix table with the elements of even one in one to a21 or two two,

578
01:25:53,291 --> 01:26:00,730
then the determinant of this two by two matrix is the product of the diagonal elements A1 wanted to

579
01:26:00,731 --> 01:26:11,981
do minus the product of the of diagonal elements or reverse diagonal elements A1 two and 8 to 1.

580
01:26:12,671 --> 01:26:19,631
Okay. So that's very easy for the for a two by two matrix.

581
01:26:19,691 --> 01:26:23,651
Here are a few miscellaneous results about determinants.

582
01:26:23,681 --> 01:26:27,971
And so if A's full rank, then determinant of would be non zero.

583
01:26:29,441 --> 01:26:33,911
If A's then deficient, then determinant of E is zero.

584
01:26:36,101 --> 01:26:45,761
If it's triangular, then the determinant of E is just the product of the diagonal elements.

585
01:26:47,191 --> 01:27:08,230
Okay, in versus if a is a full rather then the inverse of e denoted by e inverse or e superscript negative

586
01:27:08,231 --> 01:27:18,131
one is such that e time saying inverse is equal to inverse find c equal to identity matrix.

587
01:27:18,131 --> 01:27:20,830
So basically if is full rank,

588
01:27:20,831 --> 01:27:30,821
then the inverse of B is a matrix such that pre multiplying or multiplying E by that matrix gives you the identity matrix.

589
01:27:31,301 --> 01:27:37,001
If is real deficient, then inverse does not exist.

590
01:27:38,381 --> 01:27:42,551
So this again we will use in the context of linear regression.

591
01:27:42,941 --> 01:27:46,551
Yes. To. Like inverses and determinants.

592
01:27:46,551 --> 01:27:51,501
All this could be applied to super inverses only applied for square metrics.

593
01:27:51,891 --> 01:27:56,240
And that's why I was like, when you ask that question, is that the only requirement?

594
01:27:56,241 --> 01:28:02,361
I said, That is proven to be true, but for the inverse it only applies sports green matrices.

595
01:28:04,461 --> 01:28:08,361
For non similar matrices, a unique inverse exists.

596
01:28:12,351 --> 01:28:20,150
There is I mean, I don't want to digress too much, but later on, even in statistics,

597
01:28:20,151 --> 01:28:26,871
you will learn that there is also a quantity called the gene birth, sort of generalized inverse.

598
01:28:27,861 --> 01:28:40,161
And it is it is it is defined in the context of actually in the what let's not go there.

599
01:28:41,271 --> 01:28:52,341
So for for now, for this class, just pay attention to the fact that it has to be full rank in order to be inverted.

600
01:28:52,821 --> 01:28:56,941
We are not going to go into the gene, but steady to be in 650 adults.

601
01:28:57,021 --> 01:29:02,271
So they started mentioning, but let's not go there.

602
01:29:03,291 --> 01:29:08,450
Generally the inverse is given by the transport of the full factor matrix divided by the determinants.

603
01:29:08,451 --> 01:29:11,601
Once again, for a two by two matrix, it's very easy.

604
01:29:14,301 --> 01:29:24,891
You can get the inverse of the matrix two by two matrix E by picking like the whole factors of each element.

605
01:29:25,401 --> 01:29:34,961
Eddy Arranging them in as, you know, inverse idea and then dividing it by the determinant of e okay.

606
01:29:37,491 --> 01:29:44,751
Results. Suppose that these are full rank then inverse of inverse gives you back.

607
01:29:44,751 --> 01:29:54,381
E multiplying a bio scalar key and taking the inverse.

608
01:29:54,441 --> 01:29:57,581
Basically it's like this. This is a scalar.

609
01:29:57,591 --> 01:30:09,891
You get back one of our key scene work and the relationship between transport and inverse is if you take the inverse of a transpose,

610
01:30:09,891 --> 01:30:12,891
you can write it as transport of being both.

611
01:30:14,541 --> 01:30:24,681
Suppose that the MBA of full that then inverse of the product of A and B is being worse time seeing bus.

612
01:30:24,981 --> 01:30:27,121
So again, the order is switched.

613
01:30:28,191 --> 01:30:39,321
Other important matrices, really very important matrix in statistics is the projection matrix and we will see this in the context of regression a lot.

614
01:30:40,731 --> 01:30:43,851
So let's define some other important matrices.

615
01:30:43,851 --> 01:30:53,001
The idea important matrix, the matrix is said to be either important if the square of the matrix C is equal to E,

616
01:30:54,231 --> 01:30:58,341
if I do important, then I minus the is also important.

617
01:30:59,151 --> 01:31:10,371
And a prediction matrix is or a matrix be said to be a prediction matrix of the squared of B is B and these are also symmetric,

618
01:31:10,851 --> 01:31:20,271
meaning v transposes equal to P. It turns out that if these are prediction matrix, then I minus b's also prediction matrix.

619
01:31:22,961 --> 01:31:30,580
Or put another matrix so bees in orthogonal matrix if the matrix square feet and

620
01:31:30,581 --> 01:31:37,941
speech transpose is the identity matrix or P transpose B's identity matrix.

621
01:31:37,941 --> 01:31:44,471
So in other words, we are going to be playing P by its transpose into the identity matrix.

622
01:31:46,271 --> 01:31:50,861
Note that any two of these three implied the third.

623
01:31:51,941 --> 01:31:55,061
Okay, so the prediction matrix is particularly important.

624
01:31:55,241 --> 01:31:58,931
Going to be important for us. Keep that in mind.

625
01:32:00,731 --> 01:32:15,551
Matrix derivative. So if X is a q plus one vector, we d define d of x defined by, you know, element wise.

626
01:32:16,861 --> 01:32:27,341
And so the defining the real line, then the first derivative of g with respect to x,

627
01:32:28,091 --> 01:32:36,461
the vector derivative is basically going to be element wise, taking derivatives of that column vector or rule.

628
01:32:36,461 --> 01:32:41,081
Victor What about the second derivative?

629
01:32:42,071 --> 01:32:57,551
So delta D Deluxe Deluxe transpose, essentially we'll take element y's second derivative of G with respect to x and x transpose.

630
01:32:57,851 --> 01:33:08,831
And this gives you a matrix like this derivative of inner products.

631
01:33:09,431 --> 01:33:14,201
These results are very analog as to what happens in the scalar setting.

632
01:33:15,071 --> 01:33:29,560
Then the likes of X transpose E when it gives you E and differentiating vectors with respect to vector.

633
01:33:29,561 --> 01:33:37,061
So then the likes of x transpose e but e is a matrix gives you back the matrix z.

634
01:33:37,301 --> 01:33:43,721
So these results should look very, very analogous to what happens in the scalar setting.

635
01:33:45,311 --> 01:33:53,711
And then I have one other result that I want to show you is if you have this more on vector derivative,

636
01:33:53,711 --> 01:34:01,120
so if you have B as are prosecutrix, and if you take the first derivative of B,

637
01:34:01,121 --> 01:34:11,411
the matrix part of B with the vector X and take the vector derivative of that quantity with respect to x, then you get the back, the matrix b.

638
01:34:14,091 --> 01:34:22,191
Once again, I am going to highlight the results that we are going to need are in quadratic form.

639
01:34:22,201 --> 01:34:27,681
If either square matrix then the scalar extremely x is known as a quadratic form.

640
01:34:28,191 --> 01:34:34,250
Once again, I'm going to highlight this because you will see like then we the results about SSR is

641
01:34:34,251 --> 01:34:41,091
that the independent distribution we are going to write this are a see as quadratic forms.

642
01:34:43,221 --> 01:34:57,141
If we symmetric then the first derivative, the vector derivative of x transpose x with respect to x give you two times x.

643
01:34:57,411 --> 01:35:03,381
Once again, this is a very important result that we are going to use from like vector derivatives.

644
01:35:03,381 --> 01:35:17,020
And this should also look very analogous to the scalar goldilocks of a square in the scalar setting is to then say X.

645
01:35:17,021 --> 01:35:23,600
So and then I'm just going to end with some notation.

646
01:35:23,601 --> 01:35:36,141
If Y is a random vector where Y is of dimension and one, and then we return as y one, y y n where all of these are random,

647
01:35:36,561 --> 01:35:42,811
then the mean of the vector is basically the element y's mean all the elements

648
01:35:42,811 --> 01:35:52,761
wise expectation of y y to y it and the covariance of y or the variance of y.

649
01:35:53,661 --> 01:36:02,361
Sometimes this can be a bit confusing because when we are talking about the variance of a vector y, it's a matrix, it's not a single number.

650
01:36:03,291 --> 01:36:12,830
And the diagonal elements of this matrix are the variances and the of diagonal elements are the four variances.

651
01:36:12,831 --> 01:36:17,331
So maybe this is an important so that so the definition still holds,

652
01:36:18,321 --> 01:36:26,871
but you have to appreciate that y is no a vector and not a scalar laser and groth one vector.

653
01:36:27,651 --> 01:36:35,571
And when we talk about the covariance, this is also sometimes referred to as the variance or variance matrix.

654
01:36:38,851 --> 01:36:46,531
So the covariance, the matrix and it is such that the diamond alignments have the variances,

655
01:36:47,131 --> 01:36:51,931
but the of diagonal elements are the whole variances between white and white.

656
01:36:52,141 --> 01:36:59,341
For all I know, if you go to the functions of a random vector, twice a random vector,

657
01:37:00,121 --> 01:37:07,891
and is a matrix of constants, then expectation of EVA is eight times the expected value of y.

658
01:37:07,891 --> 01:37:19,681
Once again, same as sort of in the scalar domain and variance of EVA is e times variance of y times it transforms.

659
01:37:20,221 --> 01:37:30,811
So the difference now is is a bit matrix and but it's fixed because it's a matrix of constants and why is it another vector?

660
01:37:30,961 --> 01:37:43,230
And then finally, if Y has a military gradient, if Y is a vector and it is a multivariate normal distribution with a mean mil,

661
01:37:43,231 --> 01:37:49,921
which is a vector and variance covariance matrix capital sigma which is a matrix then e

662
01:37:49,921 --> 01:37:57,451
times y that either matrix of fixed constants has a normal distribution like in miles.

663
01:37:57,451 --> 01:38:03,840
I believe normal would mean equal to eight times view and covariance matrix equal to

664
01:38:03,841 --> 01:38:11,311
eight times sigma times eight transpose where these are matrix multiplication products.

665
01:38:11,671 --> 01:38:19,411
So I'm going to stop here. Like I said, very sort of quick review of these matrix algebra results.

666
01:38:19,801 --> 01:38:22,831
Please go and you know, kind of review them one more time.

667
01:38:23,221 --> 01:38:27,090
We'll use these results in module to date.

668
01:38:27,091 --> 01:38:30,191
Our thank you.

669
01:38:40,751 --> 01:38:41,001
Well.

