1
00:00:02,430 --> 00:00:05,810
As well as more fiction.

2
00:00:09,790 --> 00:00:13,000
Yes, exactly.

3
00:00:13,350 --> 00:00:28,700
I think more second years, just just consider or not what to expect or what the like.

4
00:00:28,840 --> 00:00:34,600
And then maybe after they get started here.

5
00:00:34,600 --> 00:00:37,410
I'm so happy today to have John Rice.

6
00:00:37,530 --> 00:00:48,479
John was a problem six years ago, probably some years ago, something like that experienced in Rochester and was in Colorado.

7
00:00:48,480 --> 00:00:52,260
And he's just rejoined the department in September, August.

8
00:00:52,800 --> 00:01:01,830
Let's talk about this work. Something for everyone.

9
00:01:03,360 --> 00:01:12,689
Yeah. So today I'm going to be talking about some work that was motivated by cancer prevention and actually has some resonances with cancer methods.

10
00:01:12,690 --> 00:01:20,219
But the data application is going to be a little bit kind of more adjacent, I guess, tangential to cancer.

11
00:01:20,220 --> 00:01:36,660
So so the motivation for for this work was studies of vaccine hesitancy and children particularly hesitancy among the parents.

12
00:01:37,560 --> 00:01:44,700
So cervical cancer and a number of other cancers can be prevented very effectively by HPV vaccination.

13
00:01:45,840 --> 00:01:53,850
And this is something that's been true for a while now. The HPV vaccine is relatively new, 15, 20 years in the grand scheme of vaccines,

14
00:01:53,850 --> 00:02:02,009
but it's been proven to be be pretty effective in cancer prevention, but it has to be administered early because of its STDs.

15
00:02:02,010 --> 00:02:09,720
So the earlier the vaccines demand, the more protection the individual has against subsequent cancers.

16
00:02:10,770 --> 00:02:14,190
So we're interested in a couple of questions when we look at research in this area.

17
00:02:14,460 --> 00:02:21,540
So on one hand, we want to understand what proportion of people are going to be the hardcore are not going to get vaccinated.

18
00:02:21,540 --> 00:02:25,950
But we also want to understand the time to vaccination among the other people.

19
00:02:26,640 --> 00:02:31,410
So the people who are going to vaccinate, we want to understand sort of the rate of vaccination.

20
00:02:34,680 --> 00:02:41,400
So in particular, the data that we had in a couple of these studies are that a lot of the data that's

21
00:02:41,490 --> 00:02:47,129
that's generated for these things in this study comes from state level databases.

22
00:02:47,130 --> 00:02:54,780
So they're called immunization information systems or IHS, and they are huge data sources.

23
00:02:54,780 --> 00:02:59,190
So they're very valuable in that sense. They have a ton of a ton of information in them.

24
00:03:00,180 --> 00:03:07,920
However, they are often pretty restricted as far as their their use goes because of privacy concerns.

25
00:03:08,330 --> 00:03:18,640
These groups are worried about things like individuals being re identified from from a data set and you guys are all familiar with Concept II.

26
00:03:19,560 --> 00:03:24,600
So dates are high. So anything more specific than a year is considered health information.

27
00:03:24,600 --> 00:03:31,319
So they're very they're very reluctant to share some of that, some of that data, which we need to get these times.

28
00:03:31,320 --> 00:03:36,180
So when we do travel analysis, we need time to events and we need to derive those from dates.

29
00:03:38,130 --> 00:03:46,410
So so the idea was, well, if they're not going to share the individual patient data or IPD, maybe they would share form of aggregated data.

30
00:03:46,410 --> 00:03:51,300
So aggregate data is something like summary statistics or contingency tables.

31
00:03:51,630 --> 00:04:00,150
It's, it's a summary of the data that doesn't allow you to sort of individually identify participants in the in the research.

32
00:04:00,150 --> 00:04:05,459
Because actually this this data wasn't collected for research purposes. But another kind of things feature.

33
00:04:05,460 --> 00:04:10,980
It's collected for more for sort of administrative purposes, public health purposes, not so much for research.

34
00:04:11,920 --> 00:04:17,410
Um and it is six things like that so means allegations.

35
00:04:17,650 --> 00:04:24,760
Um, and this can be of course stratified by at least discrete covariate values so we can look at males versus females.

36
00:04:25,330 --> 00:04:35,830
Different age groups, things like that. Um, so we do have some existing progression methods that don't require IP, things like prison regression.

37
00:04:35,830 --> 00:04:41,520
You can have group data binomial group binomial regression, logistic, you can, you can do that without IPD,

38
00:04:42,340 --> 00:04:50,220
but they either don't allow for sort of a cure fraction discretely or they don't allow for differential follow up spoken for Cummings there.

39
00:04:51,820 --> 00:05:00,420
So the the whole idea of how to event analysis in the area vaccine research is is a little bit less common.

40
00:05:00,430 --> 00:05:08,409
I think one of the things I worked on when I was at Colorado was for making this more palatable to the researchers,

41
00:05:08,410 --> 00:05:12,880
because everybody a lot of applied researchers are very comfortable with logistic regression.

42
00:05:13,660 --> 00:05:16,990
So that's they understand that they know what that odds ratio is.

43
00:05:17,920 --> 00:05:22,780
But survival or time to event is a little bit harder for for some of them to get to grips with.

44
00:05:23,490 --> 00:05:29,440
Um, so the problem is that this, uh, this data really is time to imagine.

45
00:05:29,460 --> 00:05:35,170
It's sometimes it's even an age scale. So we're looking at HPV and they thought, yeah,

46
00:05:35,440 --> 00:05:42,310
age is the relevant time scale because we want to get people fully vaccinated before the age of 13 is the goal.

47
00:05:43,480 --> 00:05:50,200
So we need to know when that's happening. But when we do these studies, they have a finite horizon in time.

48
00:05:50,200 --> 00:05:52,059
So they have to stop at a certain point.

49
00:05:52,060 --> 00:05:58,150
And then at that point, some individuals may or may not be vaccinated and some of them may be younger than 13, so we can consider them censored.

50
00:05:58,360 --> 00:06:04,540
So it's important to account for that in the analysis. Um, so there's a few,

51
00:06:04,540 --> 00:06:09,369
there's a few situations I mentioned where IPD in 80 are kind of you can kind of get away

52
00:06:09,370 --> 00:06:14,799
with the aggregated data version and focus on regression was was one that I mentioned.

53
00:06:14,800 --> 00:06:20,170
So that can be equivalent to some Cox regression models in certain cases.

54
00:06:20,770 --> 00:06:28,990
So you can set a course on regression with zeros and ones outcomes, time to event being sort of a log of that being an offset term.

55
00:06:29,380 --> 00:06:35,560
So there's, there's, there's ways you can kind of manipulate things to get a IPD miles to work with aggregated data.

56
00:06:35,980 --> 00:06:42,040
Um, but in general, it's, it doesn't work for the mixture cure model, which is what I'm talking about.

57
00:06:42,740 --> 00:06:50,319
Um, so what I'm going to propose is a set of likelihood based procedures to estimate essentially a cure rate mixture,

58
00:06:50,320 --> 00:06:54,030
cure model parameters when IPD is unavailable.

59
00:06:55,830 --> 00:07:05,610
So criminals, if anyone's unfamiliar, allow for analysis of time to event data when there is a fraction of non susceptibles in the population.

60
00:07:06,210 --> 00:07:11,370
So these are people who essentially have an infinite time to events, so they'll never experience the events of interest.

61
00:07:11,370 --> 00:07:16,019
So in the vaccine context, this is people who are kind of maybe with non anti-vaxxers,

62
00:07:16,020 --> 00:07:18,810
so they're not going to get their kid vaccinated sort of regardless.

63
00:07:19,800 --> 00:07:28,680
And we do know the fraction that's in that group is PI fraction that is susceptible is one minus pi and they have a survival function if not bar.

64
00:07:29,490 --> 00:07:38,450
So that means the, the marginal survival function has this form here and that's really the source of our difficulties as far as estimation goes.

65
00:07:39,690 --> 00:07:48,569
Um, so some notation here again in the, in the context of times of event, this is all fairly, fairly standard.

66
00:07:48,570 --> 00:07:52,229
So, uh, we don't observe the actual times of events.

67
00:07:52,230 --> 00:07:55,620
So, so this is one of the important features of survival analysis.

68
00:07:56,430 --> 00:07:58,530
We only observe a rise censored version of it.

69
00:07:58,530 --> 00:08:05,490
So that's the minimum of the events of interest, time and sensory, which is assumed to be independent, given covariance.

70
00:08:06,570 --> 00:08:10,860
And then we have this delta variable that's equal to one.

71
00:08:10,860 --> 00:08:14,880
If the observed time is an event, time equal to zero if it's a censoring.

72
00:08:15,510 --> 00:08:21,180
So that just tells us whether or not we've observed in the past. So the event here is a person getting vaccinated.

73
00:08:21,180 --> 00:08:24,900
Yes. So the time to vent is. From birth.

74
00:08:24,990 --> 00:08:31,320
It's like the age we could be free. It could be age. It could also be time from the start of the study or something like that.

75
00:08:31,400 --> 00:08:36,050
So. So it's different sort of applications. But yes, it could be.

76
00:08:36,060 --> 00:08:39,360
That's a good point. Is the time origin is important to identify.

77
00:08:39,870 --> 00:08:48,150
Um, in the HPV context, it is age. Um, but in the example I'll show, it's going to be the beginning of the season.

78
00:08:48,600 --> 00:08:57,870
So time to event. There is times of flu vaccine. Um, so, so actually is, is these pairs of y i and Delta IV.

79
00:08:57,970 --> 00:09:03,120
Um, and then we have a sample size of, of. Um, so, so that's IPD.

80
00:09:03,480 --> 00:09:10,800
But the aggregated data form could be variable because the only requirement is to avoid sharing the individual level data.

81
00:09:11,430 --> 00:09:15,629
Um, so the assumption that, that I'm making with this work is,

82
00:09:15,630 --> 00:09:23,670
is essentially that we can request from the data custodians what whatever we need as far as the form of the data.

83
00:09:24,160 --> 00:09:28,410
Um, so that gives us a little bit more flexibility as far as the methods that we're, we're working with.

84
00:09:30,940 --> 00:09:35,919
And again, the basic idea is just to sort of sort of like a dimension reduction problem.

85
00:09:35,920 --> 00:09:41,980
We want to get certain. We want to get as much information as we can with a lower dimensional summary of the data.

86
00:09:43,810 --> 00:09:49,810
So one of the special cases that we can think about with this, with this model is an exponential time to event.

87
00:09:50,540 --> 00:09:56,710
Um, and this is one of those cases where the IPD and the 80 can coincide.

88
00:09:57,760 --> 00:10:08,020
So this is the single time I mentioned where we can have the outcome being total number of events and an offset for the log of the total time at risk.

89
00:10:08,530 --> 00:10:23,590
So, so this is sort of a grouped data plus on regression model fit and this is again unavailable to us when we have a mixture of model line here.

90
00:10:24,400 --> 00:10:30,280
Um, so when we are an extra care model, we're interested in estimating sort of two sets of parameters,

91
00:10:30,280 --> 00:10:34,749
one pertaining to the, the probability of being susceptible,

92
00:10:34,750 --> 00:10:42,970
which is PI and the other which I'm calling PSI here, which is parameter sizing the time to event distribution of the susceptible.

93
00:10:44,230 --> 00:10:48,640
So the long likely it for this model can be decomposed into two terms,

94
00:10:49,030 --> 00:10:53,560
one of which corresponds to the censored individuals, one for the, for the event individuals.

95
00:10:54,100 --> 00:11:01,960
Um, and the censored individuals are denoted here with the h nots and the event individuals are noted with h one.

96
00:11:03,130 --> 00:11:07,360
And this, this kind of, this, this log of this sum.

97
00:11:07,360 --> 00:11:12,100
Here is the reason that we have problems estimating to do with aggregate data,

98
00:11:12,430 --> 00:11:17,770
because this doesn't this doesn't break down nicely when you sum up overall the

99
00:11:17,770 --> 00:11:21,550
individuals because it's a nonlinear function of the data and the parameters.

100
00:11:22,160 --> 00:11:31,480
Um, and in the case of the event times, you can obviously pull out this one minus pi, but then you have the, the density function itself,

101
00:11:32,200 --> 00:11:39,460
which in a certain special case is okay, but in general will also contain nonlinear functions of the data and parameters.

102
00:11:40,430 --> 00:11:44,600
Um, so the special case is, is the exponential model again.

103
00:11:45,730 --> 00:11:49,360
So when we're in the exponential model with a accurate fraction.

104
00:11:50,470 --> 00:11:57,620
Um, we can, we can break down the low likelihood such that we can use aggregate data,

105
00:11:57,620 --> 00:12:00,900
a certain form of aggregated data to estimate the parameters in small.

106
00:12:00,910 --> 00:12:08,080
We don't need the individual patient data. And that's if the censoring time is the same for everybody.

107
00:12:08,290 --> 00:12:15,969
So if we have the same censoring time across individuals and we have the exponential distribution for the time,

108
00:12:15,970 --> 00:12:22,660
so that among the susceptibles, then we can essentially get away with estimation using two summary statistics,

109
00:12:22,660 --> 00:12:26,379
one of which is the number of events and the sample,

110
00:12:26,380 --> 00:12:35,710
which is in sum so delta or dense up one here we can get of course in and not from the sample size minus N1.

111
00:12:37,150 --> 00:12:43,630
And then the mean time at risk among the susceptibles or sorry, among the those with an event.

112
00:12:43,750 --> 00:12:48,100
So that's why one are here. So this in this case,

113
00:12:48,100 --> 00:12:56,340
the the estimates that you obtain from the IPD analysis will be the same exact estimates that you would get from from the aggregated analysis.

114
00:12:56,350 --> 00:12:59,630
So this is a nice, a nice example of where we are.

115
00:12:59,650 --> 00:13:05,110
We're sort of okay with aggregated data without further sort of, uh, manipulation.

116
00:13:06,500 --> 00:13:13,430
If you have covariance in any of these models. So that is something that's kind of for future work.

117
00:13:14,060 --> 00:13:17,640
But in general, yes, that's that's for something like it.

118
00:13:17,870 --> 00:13:22,790
When you see if you've seen core models previously, you often see covariates in pi, for example.

119
00:13:23,720 --> 00:13:28,130
And that would be fine in something like if you if you had the individual level data.

120
00:13:29,180 --> 00:13:32,870
But when we don't. Yeah. So that's something I'm still working on.

121
00:13:33,140 --> 00:13:41,930
What, what, what we can do is we can stratify by covariance and we can end up if they're discrete and then we can do sort of.

122
00:13:42,530 --> 00:13:47,990
It's analogous to like the group binomial situation. I'll, I'll talk more about that in the discussion as well.

123
00:13:48,770 --> 00:13:51,020
Um, but yes, in principle,

124
00:13:51,020 --> 00:13:59,600
you can have covariance attached to really any of these parameters and that could become important in terms of getting a good model fit.

125
00:14:01,830 --> 00:14:11,280
So again, in general, we don't have this this nice equivalency between the aggregate data and the individual patient data.

126
00:14:12,630 --> 00:14:23,430
So we in terms of the log likelihoods. So the proposal here is to make a polynomial approximation to the log likelihood function.

127
00:14:24,000 --> 00:14:35,940
Um, where we're, we're going to be able to essentially use approximate or approximately sufficient statistics to estimate our model parameters.

128
00:14:36,470 --> 00:14:40,799
Um, and so the example here is, is for a wild distribution.

129
00:14:40,800 --> 00:14:45,030
So that's a generalization of the exponential. Uh, has, the exponential is a special case.

130
00:14:46,200 --> 00:14:51,179
And you can see here that there's going to be some, some nonlinear terms, uh, in the,

131
00:14:51,180 --> 00:14:55,790
in the log likelihood function, even for the, the events, the individuals with an event.

132
00:14:56,850 --> 00:15:02,940
Because what we saw before was in the special case of a common censoring time in the exponential distribution,

133
00:15:03,390 --> 00:15:07,560
uh, we can, we can use the aggregate in a sort of directly,

134
00:15:07,560 --> 00:15:11,400
but here we have this, this nonlinear term in the,

135
00:15:11,730 --> 00:15:18,590
in the event log likelihood and we still have the problem with it with the nonlinear and the censored by likelihood.

136
00:15:19,080 --> 00:15:28,469
Um, because again, what happens is you can't sort of simplify this in any way where you have sufficient statistics

137
00:15:28,470 --> 00:15:32,550
that summarize all the information that's in the data with respect to the parameters.

138
00:15:33,230 --> 00:15:36,330
Um, so, so the, the,

139
00:15:36,690 --> 00:15:46,080
the idea here comes from some literature and sort of the big data framework where they need to process a lot of data very quickly.

140
00:15:46,600 --> 00:15:50,850
Um, so they're, they're interested in kind of computational efficiency,

141
00:15:50,850 --> 00:15:59,940
but we're going to use sort of the same techniques to allow us to use the to, to avoid having to obtain the individual level data.

142
00:16:02,020 --> 00:16:08,050
So the particular polynomial approximation that we're going to use is called trebuchet polynomials,

143
00:16:08,500 --> 00:16:11,950
and they're defined recursively in this way and they have some,

144
00:16:12,190 --> 00:16:13,450
some nice properties,

145
00:16:13,720 --> 00:16:23,060
but but essentially what they're doing is just providing a certain degree of accuracy in terms of an approximation to a function.

146
00:16:23,080 --> 00:16:28,180
And our function here is we're thinking of this as a function of the size of the data.

147
00:16:28,600 --> 00:16:32,950
So there's there's times we can approximate functions of other variables, for example.

148
00:16:32,950 --> 00:16:37,450
But here we're thinking of this as a function of, of y the outcome.

149
00:16:38,510 --> 00:16:43,120
So that's an important thing to think about because when we go through and maximize a likelihood,

150
00:16:43,120 --> 00:16:48,490
we turn that into a function of the parameters of a were what we're approximating here as a function of the data.

151
00:16:49,720 --> 00:16:57,190
So so there's some, some kind of aspects of, of the transformation or sorry of the approximation that we have to specify.

152
00:16:57,190 --> 00:17:01,750
We have to specify an interval over which the that the approximation is going to be valid.

153
00:17:02,800 --> 00:17:07,570
And that is kind of an important question to to figure out in terms of the application.

154
00:17:08,500 --> 00:17:15,700
And then we also have to pick a degree. So this K here, a capital K is a degree of polynomial.

155
00:17:16,630 --> 00:17:23,350
And that's going to that's going to determine the the number of summary statistics that we need to obtain.

156
00:17:24,370 --> 00:17:31,089
And then what we end up with is this linear combination of these types of coefficients.

157
00:17:31,090 --> 00:17:39,730
And then the variable X here, because again, we have to transform we have to transform X to the interval, -1 to 1.

158
00:17:40,750 --> 00:17:46,329
But fortunately this is all been well-studied and has implemented in some some R packages.

159
00:17:46,330 --> 00:17:51,040
The ones that I use is perhaps not so fortunate.

160
00:17:51,040 --> 00:17:54,310
So we can take advantage of kind of existing code for this.

161
00:17:56,230 --> 00:17:59,930
So here are some examples of how these approximations work in practice.

162
00:17:59,950 --> 00:18:03,760
So on the left we have the age not function.

163
00:18:03,770 --> 00:18:09,550
So this is essentially the log of the contribution for a single individual who is censored.

164
00:18:10,300 --> 00:18:19,900
And then y here is the observed time of censoring and it's kind of hard to see, but there's a, there's a black line that's the true function.

165
00:18:20,140 --> 00:18:24,880
And then the approximations are and these these colorful lines.

166
00:18:25,900 --> 00:18:36,250
So with increasing degree of polynomial, you can see that in general, we get a closer fit to the true the true function there.

167
00:18:36,880 --> 00:18:40,570
We can start with kind of the degree to which is which is just quadratic, right?

168
00:18:40,570 --> 00:18:42,100
That's not a, that's not a great fit.

169
00:18:42,610 --> 00:18:51,910
Um, so, so there's, there's this idea of this tuning parameter K and we need to use that to get a sufficiently good approximation,

170
00:18:52,210 --> 00:19:00,140
but we don't want to be too large because that creates potentially numerical problems and also will require more,

171
00:19:00,160 --> 00:19:03,880
more data from the, from the data custodians.

172
00:19:04,570 --> 00:19:08,590
Um, and then the right hand side here is essentially the same story we have.

173
00:19:08,590 --> 00:19:13,030
This is for an individual with an event at time y here.

174
00:19:14,080 --> 00:19:22,330
And again, you can kind of see with increasing degree we get better sort of better fits to the true the true log likelihood.

175
00:19:24,090 --> 00:19:31,350
So the to to to try to manipulate this because, again, as I mentioned, when we have larger K,

176
00:19:32,190 --> 00:19:36,870
we end up potentially in some numerical problematic areas because you have a very

177
00:19:37,050 --> 00:19:42,750
high degree of probably no meal you have you can run into issues with again,

178
00:19:42,750 --> 00:19:46,980
you're taking large powers of numbers so things get very large, very small.

179
00:19:47,880 --> 00:19:53,220
So so we think about we want to think about ways to maybe improve the approximation.

180
00:19:54,420 --> 00:20:00,300
So, so one of the things to think about is the fact that on that X axis, on the previous slide,

181
00:20:01,120 --> 00:20:07,890
the horizontal axis, I guess I should say, because it's labeled Y, those values are not all equally likely.

182
00:20:08,520 --> 00:20:14,490
So when we have data, essentially those what we're looking at is the distribution of Y given Delta,

183
00:20:15,240 --> 00:20:18,780
which has this form here for survival data with independent censoring.

184
00:20:19,470 --> 00:20:22,380
So G here is just the distribution of these censoring times.

185
00:20:24,090 --> 00:20:29,760
So what I wanted to do was kind of look at what does this function look like if we sort of expand

186
00:20:30,210 --> 00:20:36,840
and contract the the X or the horizontal axis according to the actual distribution of the data?

187
00:20:36,840 --> 00:20:42,329
Because we kind of want to have a better approximation where there's a lot of data so we

188
00:20:42,330 --> 00:20:46,379
don't really care so much maybe about approximating in the tails and things like that.

189
00:20:46,380 --> 00:20:48,360
We still want an okay approximation there,

190
00:20:48,360 --> 00:20:56,040
but we really want to do well where the data is sort of the most dense so we can think about sort of transforming the

191
00:20:56,040 --> 00:21:00,480
data and then approximate and then reversing the transformation when we actually go through our estimation procedure.

192
00:21:01,760 --> 00:21:05,870
So on the on the left, here is what you saw on the previous slide.

193
00:21:05,880 --> 00:21:12,590
So this is just the, um, transformed H functions or h functions with the non transformed argument.

194
00:21:13,190 --> 00:21:18,470
And then in the middle I've applied this, this inverse CDF transform to the, to the x axis.

195
00:21:18,650 --> 00:21:25,790
So, so it's just if you, if you notice, there's the same sort of limits on the, on the horizontal axis here,

196
00:21:25,790 --> 00:21:32,390
it runs from 0 to 1.8 for this answer to observations and point forward to 5.4 for the event observations.

197
00:21:32,690 --> 00:21:36,110
But it's just been transformed. It's just been nonlinear,

198
00:21:36,150 --> 00:21:43,760
least stretched and compressed in a way that's intentionally designed to show sort of where the data is in the central column.

199
00:21:43,760 --> 00:21:45,530
So this is the CDF transform.

200
00:21:45,980 --> 00:21:54,680
And so really what's happening here is between each tick mark on the horizontal axis is sort of a fixed quantile of the data.

201
00:21:54,680 --> 00:22:01,129
So, so what we're seeing here is that a lot of the data for for this in this case,

202
00:22:01,130 --> 00:22:06,680
when, when Delta Zero is kind of low on this on this horizontal axis.

203
00:22:06,980 --> 00:22:13,310
So we do maybe want to have a better approximation in this region, because that's where a lot of the data is.

204
00:22:14,210 --> 00:22:21,860
And similarly here for the central observations, we kind of have a lot of data, again, sort of where this thing peaks actually,

205
00:22:21,860 --> 00:22:27,560
because it's sort of stretched the the x axis of the horizontal axis, such that 1.5 is kind of in the middle here.

206
00:22:29,120 --> 00:22:32,299
The problem is we don't know what that CDF is, right.

207
00:22:32,300 --> 00:22:38,990
So so one of the common transformations that you've probably all encountered and stats classes so far is a log transformation.

208
00:22:39,260 --> 00:22:49,040
So that's sort of approximately getting at this idea of where is the data, but it also has has another factor can have another effect,

209
00:22:49,610 --> 00:22:56,440
which is if you look at the log transform applied in the case of the event, this looks very quadratic, right?

210
00:22:56,450 --> 00:23:03,710
So the more sort of polynomial a function is to begin with, the better the approximation is going to be when we apply a polynomial approximation.

211
00:23:04,280 --> 00:23:06,709
And you can see here that, that that's very true.

212
00:23:06,710 --> 00:23:14,720
So this is after we've applied a log transform to the horizontal axis, approximate the function, then transform back.

213
00:23:14,990 --> 00:23:22,070
We end up with these kinds of pictures. It really helps for the event observations, not so much for the censored ones.

214
00:23:22,340 --> 00:23:28,070
So again, there's a couple of things going on there, one of which is where is the data located?

215
00:23:28,700 --> 00:23:33,290
And we want to do well when the data is. We want to do well in areas where the data is really dense.

216
00:23:33,800 --> 00:23:40,310
But also it's good to have it's good to use polynomial approximations with functions that are close to polynomial.

217
00:23:40,580 --> 00:23:48,800
So again, on the on the, on the log scale, this, this function for the sensor was, which is very close by and very close to by direct.

218
00:23:49,040 --> 00:23:53,810
So you can see that even with the lowest degree approximation that we looked at, we're doing very well.

219
00:23:54,170 --> 00:24:05,810
So so this is something that's still sort of ongoing work as far as understanding potentially other transforms that could be could be helpful.

220
00:24:06,380 --> 00:24:11,960
But for now, the log transform is sort of the the method to deal with with these issues.

221
00:24:13,410 --> 00:24:17,160
Yes. You can explain the Chevy Chase on the next.

222
00:24:19,530 --> 00:24:24,230
So what is? So they're not just linear, so to say.

223
00:24:26,050 --> 00:24:30,430
Yeah. So, so, so we start off with minimum income quadratics, right?

224
00:24:30,430 --> 00:24:41,260
Yeah. So, so the, so the first one is linear, the secondary one would be quadratic because you'd multiply two you times you minus one.

225
00:24:41,350 --> 00:24:44,770
So B two you squared minus one. Okay. And then.

226
00:24:45,790 --> 00:24:49,020
Go up to the third degree and I would. It just keeps going up.

227
00:24:49,030 --> 00:24:57,820
Yeah, but. But it. It's weird because you can get an advantage from from looking at these these actual so.

228
00:24:57,860 --> 00:25:06,479
So the coefficients see here are sort of just very simply they're sort of averages of the value of the function.

229
00:25:06,480 --> 00:25:17,310
So the C not term is literally just like the average value of the function at a set of points called the the Chevy Shove nodes.

230
00:25:17,640 --> 00:25:23,880
And those are chosen in a certain way to kind of span the, the -1 to 1 axis.

231
00:25:24,450 --> 00:25:28,900
Um, you know, C so ahead of time you have to choose.

232
00:25:28,980 --> 00:25:37,500
So we have to calculate it. Um, so, so the way that, that we implement the estimation is, uh, within,

233
00:25:37,500 --> 00:25:43,500
within sort of the outer optimization, we have to calculate this approximation for each iteration.

234
00:25:43,860 --> 00:25:48,600
So we use current parameter estimates to sort of calculate what C is.

235
00:25:49,530 --> 00:25:58,739
Um. But yeah. And so there's some, some nice features of this approximation as well that involve things like they have balance on accuracy.

236
00:25:58,740 --> 00:26:05,940
So you know how big and the error be in and in the case of, of approximation with this kind of polynomial.

237
00:26:06,540 --> 00:26:15,300
Um, but yeah, so, so those coefficient C are really just versions of, of averages of the value of the function.

238
00:26:15,480 --> 00:26:23,400
So if you think about something like, like a Taylor approximation, which is kind of where I started with this, um, that's going to be accurate.

239
00:26:23,400 --> 00:26:29,640
And in one specific area, it's not guaranteeing you accuracy over a whole range, which is really what we need.

240
00:26:30,120 --> 00:26:38,140
Um, so, so it's, it's, and the other thing is the Taylor approximation relies on derivatives, right?

241
00:26:38,160 --> 00:26:44,580
So that can become pretty cumbersome if you're taking derivatives of, you know, greater than second order.

242
00:26:45,780 --> 00:26:53,160
And what this does is that all it depends on is the functions value a set of points, a discrete set of points.

243
00:26:53,670 --> 00:26:57,420
Um, and that's where that, that tuning parameter K comes.

244
00:26:57,420 --> 00:27:02,910
And, um, because K determines the number of points that you have to evaluate the function at.

245
00:27:03,270 --> 00:27:08,100
So this is another thing that the sort of the, the literature is interested in,

246
00:27:08,250 --> 00:27:13,170
in the big data context is not having expensive function evaluations repeatedly.

247
00:27:13,470 --> 00:27:21,090
So, so the lower degree of K, the lower degree K, the fewer points you have to evaluate the function that in our case,

248
00:27:21,090 --> 00:27:27,210
the function evaluation is not super expensive and we're not really so worried about computational aspects anyway.

249
00:27:27,690 --> 00:27:35,880
Um, but that's, that's another thing that K is, is doing is controlling the number of function evaluations to get these approximations.

250
00:27:41,450 --> 00:27:48,439
Yeah. So, so here's, here's where we end up. So we have the championship approximations as a polynomial approximation to our log likelihood

251
00:27:48,440 --> 00:27:53,870
function that we apply sort of separately to the sensor and the observed contributions,

252
00:27:54,710 --> 00:28:06,890
event contributions. And what we have then is a sum, a linear combination of powers of y and again, y is the observed time.

253
00:28:07,670 --> 00:28:12,170
So whether it's censored or an event, that's what Y represents.

254
00:28:12,530 --> 00:28:18,170
So now because we have a sum applied to a linear combination,

255
00:28:18,290 --> 00:28:29,570
we can pass the sum over and through and we can get an expression like this where we now have a function of, so that's not sufficient.

256
00:28:29,930 --> 00:28:38,900
Do we have a function of these summary statistics? So we're approximating the full log likelihood with a linear combination of our summary statistics,

257
00:28:39,140 --> 00:28:50,000
which in this case are again the number of events and the sample moments within each of the censored and observed groups.

258
00:28:51,380 --> 00:28:58,550
So and this is why we say that the larger the K is, kind of the more the more numerical trouble we could have.

259
00:28:58,550 --> 00:29:01,220
Because as you raise y, it's a higher and higher power.

260
00:29:01,220 --> 00:29:06,200
As you get larger numbers, you have numerical inaccuracies, things, errors, compound, things like that.

261
00:29:06,590 --> 00:29:09,260
But there are things we can do to mitigate that.

262
00:29:09,260 --> 00:29:21,140
But again, it's it's just something to be aware of in terms of you could get greater accuracy, but you could also have numerical problems.

263
00:29:22,540 --> 00:29:35,740
So the way we implement this in practice is to essentially just nest the polynomial approximation within any standard optimization algorithm.

264
00:29:36,460 --> 00:29:44,440
So I use optim and r, um, but the difference is that now inside that, that optimization,

265
00:29:44,830 --> 00:29:49,200
we have this step where we're using the polynomial approximation given the current

266
00:29:49,240 --> 00:29:53,410
parameter estimates to obtain the actual lot like the function that we're,

267
00:29:53,410 --> 00:30:00,010
that we're optimizing. So, so that's kind of that's kind of the, the unique feature of this,

268
00:30:00,010 --> 00:30:07,420
of this method is that we apply the approximation sort of within the context of an optimization algorithm.

269
00:30:08,200 --> 00:30:12,219
Um, and as I mentioned, we have a couple of tuning parameters here.

270
00:30:12,220 --> 00:30:17,770
We have K, which is the degree and we have the interval that we're applying the approximation over,

271
00:30:18,820 --> 00:30:22,080
which again, K is a little bit simpler to understand, right?

272
00:30:22,090 --> 00:30:25,570
The more the higher K is, the more summary statistics we have,

273
00:30:25,990 --> 00:30:31,600
greater the accuracy we should she and higher degree of the polynomial that we're using to approximate

274
00:30:32,860 --> 00:30:37,570
A and B is a little bit harder because how do we know what the range is that we need to approximate?

275
00:30:38,230 --> 00:30:43,780
Right. And actually, the approximation can be fairly sensitive to these these choices.

276
00:30:44,770 --> 00:30:50,200
So currently I'm using the order statistics of the minimum, the maximum of the wise.

277
00:30:50,500 --> 00:30:56,440
So that's yet another piece of information that we would need to have from the data, the holders of the data.

278
00:30:56,880 --> 00:30:59,740
Um, and that works, I think. Okay.

279
00:31:00,250 --> 00:31:09,420
But that probably could take some more sort of thought too to know sort of what the best range is to apply the approximation over.

280
00:31:10,750 --> 00:31:15,190
And then of course, we have to think about inference, right? We have to think about standard errors.

281
00:31:15,190 --> 00:31:25,389
We have to think about P values. And there's, there's this nice paper in biometrics that basically says that under certain conditions,

282
00:31:25,390 --> 00:31:30,820
you can treat an approximate log like the like a regular log, like for purposes of inference.

283
00:31:31,180 --> 00:31:35,700
And of course, that's great. The problem is, you know, one of those conditions, right?

284
00:31:35,710 --> 00:31:42,670
How do you verify? Um, so that's something that's still kind of to be, to be detailed.

285
00:31:43,750 --> 00:31:50,500
But again, the better the approximation to the log likelihood, the better the approximation for your inference.

286
00:31:51,490 --> 00:31:57,250
And what I found sort of in simulations is that whatever degree you need for good estimation,

287
00:31:57,250 --> 00:32:00,460
you need a little bit higher degree to get good inference. I'll show what I mean.

288
00:32:00,700 --> 00:32:05,200
And some of the examples. Yes. How long does a stick to run?

289
00:32:06,170 --> 00:32:10,930
That's a good question. So it's it's fairly quick. It's not as quick as if we had.

290
00:32:11,440 --> 00:32:15,940
You think it would be quicker than if you had the individual data, right? That's not necessarily the case.

291
00:32:15,940 --> 00:32:21,490
I think because of that step where you're doing the approximation because it, you know, to calculate those types of coefficients,

292
00:32:21,970 --> 00:32:27,430
um, again, it's fairly quick, but you have to do that every time, every, you know, every time you iterate.

293
00:32:27,850 --> 00:32:31,870
Um, but yeah, it's, it's not, it's not terribly slow.

294
00:32:31,900 --> 00:32:37,150
Um, I guess I'm trying to think sort of as a benchmark to fit.

295
00:32:38,930 --> 00:32:42,610
I don't know. A few dozen of these would probably take a few minutes. It's yeah, it's.

296
00:32:42,620 --> 00:32:48,680
It's not it's nothing crazy. So your you're proximate most likely to get back to them?

297
00:32:49,160 --> 00:32:53,450
Oh, yes. And those are for the lambdas in the PIs.

298
00:32:53,450 --> 00:33:01,549
But you're into so that's all in theta here. So that's so so so so PSI here is essentially sort of a combination of championship

299
00:33:01,550 --> 00:33:08,570
coefficients and some other it's really that's all coming from the approximation.

300
00:33:09,020 --> 00:33:10,730
So that's a function of the parameters.

301
00:33:11,890 --> 00:33:17,380
So what I what I'm doing is a sort of partitioning such that there's a function of the parameters as a function of the data,

302
00:33:17,590 --> 00:33:21,070
and there they're just linearly combining. Okay.

303
00:33:21,460 --> 00:33:26,920
So because what's what we're doing with the approximation is that all the all the parameters are coming into that.

304
00:33:27,870 --> 00:33:34,829
So so then so I here is the it's just what comes out of that approximation procedure.

305
00:33:34,830 --> 00:33:39,660
And that's going to be a function of theta, which is the PI's and the Lambdas and whatever else you have.

306
00:33:43,680 --> 00:33:51,060
So to to examine this sort of in practice, we looked at some simulations here.

307
00:33:51,540 --> 00:33:55,319
So this is 200 simulations with different settings of parameters.

308
00:33:55,320 --> 00:34:01,650
You don't have to go through all the examples. I'm going to present a couple of of slices of the results here.

309
00:34:03,460 --> 00:34:08,170
We're in the the parametric survival settings. So I had to pick a well, you always have to pick a distribution.

310
00:34:08,860 --> 00:34:11,770
But we looked at inverse Gaussian, Weibel and Gamma,

311
00:34:12,760 --> 00:34:20,530
and the inverse Gaussian is kind of like a log normal as far as it has a density and sort of goes to zero at zero,

312
00:34:20,530 --> 00:34:26,500
whereas the Weibel and Gamma don't necessarily have that feature with certain areas of the parameter space.

313
00:34:27,430 --> 00:34:33,489
But I looked at high values 2.25, 1.5, and that was motivated by the actual the data application.

314
00:34:33,490 --> 00:34:39,390
And one of the groups and the net application there was at the pie value was estimated to be about 0.5.

315
00:34:39,400 --> 00:34:43,270
So so that's somewhat of a realistic setting.

316
00:34:44,110 --> 00:34:47,680
And then MU is just sort of arbitrarily one and two that can be scaled.

317
00:34:48,520 --> 00:34:57,100
So one of the things that that I'll talk about with the real data example is rescaling your data to obtain better sort of numerical properties.

318
00:34:58,120 --> 00:35:04,030
So the scale is kind of arbitrary. And then the CV here is the coefficient of variation of that distribution.

319
00:35:05,410 --> 00:35:11,080
And then I use the exponential distribution for the censoring times and looked at a few sample sizes.

320
00:35:11,470 --> 00:35:16,180
So the data example that I'm going to show is going to be towards the lower end of the spectrum.

321
00:35:16,180 --> 00:35:25,780
But really the the method is sort of probably best suited for these larger sample size settings because in the state AI systems,

322
00:35:27,040 --> 00:35:31,240
we potentially have access to a lot of patients in the data that I'm working on getting permission to use.

323
00:35:31,990 --> 00:35:35,410
It's like 30,000. So so these can be very big samples.

324
00:35:36,580 --> 00:35:40,660
But for the for the purpose of the simulations, we looked at sort of a smaller range.

325
00:35:41,620 --> 00:35:44,500
And then for the analysis, since I generally the data,

326
00:35:44,710 --> 00:35:51,790
I can use the IPD analysis to sort of benchmark like what we should be getting or what we would be getting with individual level data.

327
00:35:52,270 --> 00:35:58,510
And then we use the aggregated data method with the cheryshev approximation with degrees ranging from 4 to 20.

328
00:36:00,330 --> 00:36:05,240
So this is 4.5 and I think equals one.

329
00:36:05,250 --> 00:36:12,540
So this is the cure probability estimates at various sample sizes and across each of these distributions.

330
00:36:13,320 --> 00:36:20,160
And then the x axis shows sort of the degree of polynomial. And then we're looking at both sort of the temperature approximation by itself.

331
00:36:20,460 --> 00:36:29,640
And then that same transformation or that same approximation with the log transformation applied prior to doing the fitting.

332
00:36:30,930 --> 00:36:38,610
And what you can see here is, is a lot of very good performance for the smaller sample sizes, even with lower degree polynomials.

333
00:36:38,610 --> 00:36:42,329
Right. Or for like an eight degree polynomial,

334
00:36:42,330 --> 00:36:51,030
we have pretty much on target estimation for both sort of the values of the estimates as well as the inference.

335
00:36:51,030 --> 00:36:54,239
Because if we think about the as well, I guess I'm not showing the standard errors here,

336
00:36:54,240 --> 00:36:59,729
but it has the same sort of spread as the true the true estimate.

337
00:36:59,730 --> 00:37:08,130
So that's that's important. We do we do want to end up with with very close properties to the true the true estimates here are the IPD estimates,

338
00:37:09,690 --> 00:37:12,809
but we also see some heterogeneity with respect to the distributions.

339
00:37:12,810 --> 00:37:17,070
Right. So so the inverse gauss in particular shows some sort of weird behavior.

340
00:37:18,660 --> 00:37:21,510
And again, this is something that you can sort of look at.

341
00:37:21,510 --> 00:37:28,589
And in application you can fit each of these models because what's nice about this method, it doesn't depend on a particular distribution,

342
00:37:28,590 --> 00:37:35,250
it just depends on having some close form of that density and survival function that you can use in the approximations.

343
00:37:36,330 --> 00:37:42,450
So the other thing that's that's kind of interesting here is the sample size effect, right?

344
00:37:42,660 --> 00:37:47,880
Because as we increase the sample size, these lower degree polynomials do do pretty poorly.

345
00:37:48,690 --> 00:37:55,860
So one of the things that that Ogden metric of paper about approximate inference talks about

346
00:37:55,860 --> 00:38:01,890
is the the requirement for something like our K tuning parameter to grow the sample size.

347
00:38:02,130 --> 00:38:06,480
So I think that's what we're seeing here is just sort of as the sample size grows,

348
00:38:06,750 --> 00:38:12,600
you need a larger degree of polynomial to adequately approximate that that objective function.

349
00:38:14,810 --> 00:38:20,280
And then this is the the same kind of box plots for the for the estimates of the mean.

350
00:38:20,310 --> 00:38:26,210
So this is the mean of the distribution of times to events among the susceptibles.

351
00:38:26,930 --> 00:38:30,220
And again, we see fairly similar results here.

352
00:38:30,230 --> 00:38:34,880
I think there's a little bit more sort of for a preference for the lower degree polynomials.

353
00:38:35,720 --> 00:38:38,959
And oftentimes, the this log transformation really does help.

354
00:38:38,960 --> 00:38:46,220
I mean, you can look, for example, you know, at this year for for a sample size of a thousand, the advertising distribution,

355
00:38:46,220 --> 00:38:54,200
the the log transform version of the approximation performs quite a bit better than than the unfree version.

356
00:38:54,440 --> 00:38:59,179
And that's just getting at probably some of the things that I showed with when you transform it,

357
00:38:59,180 --> 00:39:05,150
it might be kind of more quadratic and then you can approximate on that scale and transform back and you should do better overall.

358
00:39:06,740 --> 00:39:11,330
But again, I mean, we also see that there's there's certainly some issues with larger sample sizes

359
00:39:11,330 --> 00:39:18,080
in particular as far as you you you may need degree 12 or 16 polynomials.

360
00:39:18,200 --> 00:39:21,850
And that starts to get a little bit like, wow, that's that's high, right?

361
00:39:21,860 --> 00:39:26,060
I mean, so so that's another thing to think about is, you know, what's what's realistic here.

362
00:39:26,240 --> 00:39:31,940
And, of course, the numerical side of things. So just the background.

363
00:39:31,940 --> 00:39:39,499
So moving into the real data here, this is a data set on flu vaccination at Denver Health,

364
00:39:39,500 --> 00:39:46,430
which is a safety net hospital system and Denver area collected in the 20 1920 flu season.

365
00:39:47,330 --> 00:39:52,970
So this is, as I said, on the lower end of the sample size spectrum with about 250 parents who were surveyed.

366
00:39:53,690 --> 00:40:03,650
And what they wanted to do with this study was to determine the effect of parental attitudes about vaccination on the vaccine behavior.

367
00:40:03,680 --> 00:40:08,690
Right. So did they or did they not get their their child vaccinated for flu during that flu season?

368
00:40:09,890 --> 00:40:15,320
And they collected data on demographics, of course, time of vaccination, which is our outcome.

369
00:40:15,920 --> 00:40:23,300
And then the vaccine hesitancy variable, which is derived from a form or an instrument called the PAC B,

370
00:40:24,020 --> 00:40:31,099
which is just a set of questions that are intended to assess somebody's hesitancy about vaccination.

371
00:40:31,100 --> 00:40:35,540
So this is sort of a standardized way of of registering that.

372
00:40:35,540 --> 00:40:39,190
And that's the this variable is either zero.

373
00:40:39,410 --> 00:40:48,410
So somebody classified as either hesitant or not hesitant. So the plan here is, since we have access to the IPD in this case,

374
00:40:49,250 --> 00:40:53,870
we can both conduct the analysis on that data and we can artificially caution

375
00:40:53,870 --> 00:40:59,120
to get our aggregate data and compare what we see with the to the two methods.

376
00:41:00,470 --> 00:41:04,940
So I use the Kaplan-meier plot to look at the distribution of the data and

377
00:41:04,940 --> 00:41:09,620
then the kind of the IPD log likelihood to get these sort of ideal estimates,

378
00:41:09,620 --> 00:41:14,210
right. So these are the estimates that we would obtain if we had the individual level data.

379
00:41:14,870 --> 00:41:20,390
And remember, the goal here is to do see how well we can do without having to have that IPD.

380
00:41:21,230 --> 00:41:29,240
So we're assuming a viable distribution for now and I rescale the data was in the form of time was in days so days since the start of the flu season.

381
00:41:30,140 --> 00:41:38,750
I'm going to rescale that to year just to to you know because when we have days the flu season is I think like 200 days long or something.

382
00:41:39,440 --> 00:41:43,940
So if you think about this random variable that can take values on that scale,

383
00:41:44,240 --> 00:41:47,360
when you start to take these higher moments, you end up with really large numbers.

384
00:41:48,350 --> 00:41:53,420
So so we rescale to get sort of maybe better, better numerical stability.

385
00:41:54,950 --> 00:41:58,220
And one feature of this data is that everybody sensor at the end of the flu season.

386
00:41:58,790 --> 00:42:06,529
So everybody has the same sensor any time. So so the approximation really only needs to deal with the, the, the events observations.

387
00:42:06,530 --> 00:42:10,129
So we should see better performance than in the simulations.

388
00:42:10,130 --> 00:42:15,470
Whereas the censoring time was random and because we have a smaller sample size,

389
00:42:15,470 --> 00:42:23,150
we're going to look at sort of profile like the confidence intervals instead of wild because I was finding that even with the IPD,

390
00:42:24,020 --> 00:42:29,330
the performance of the well, it just wasn't working because the samples were too small.

391
00:42:30,350 --> 00:42:32,180
So, so this is what the data looks like.

392
00:42:33,470 --> 00:42:40,880
So the, the upper line here is the distribution of the survival estimated survival functions for the hesitant group.

393
00:42:41,690 --> 00:42:44,510
And then the lower line is for the not specific group.

394
00:42:44,900 --> 00:42:56,210
So you can see a clear separation here indicating that the those who are hesitant are in fact vaccinating at a lower rate than those who are not.

395
00:42:56,480 --> 00:42:58,130
And that's important to know,

396
00:42:58,460 --> 00:43:06,440
because it's not necessarily clear that this vaccine hesitancy variable as collected through this instrument translates into behavior.

397
00:43:06,440 --> 00:43:11,450
But we're seeing in this data at least, that it does. So overlaid on the kaplan-meier curves.

398
00:43:11,450 --> 00:43:19,560
Here are the IPD fits with the. Why bowl mixture care model and for the most part, they seem to fit pretty well.

399
00:43:20,010 --> 00:43:24,719
So so this is a situation where it could make sense to to look at the aggregated

400
00:43:24,720 --> 00:43:30,090
data and compare with the IPD because the IPD is doing sufficiently well as far as,

401
00:43:30,510 --> 00:43:34,050
um, matching up with, with the kaplan-meier curves.

402
00:43:36,180 --> 00:43:40,560
So this is the estimates that we obtained using the aggregated data and then this so so that's

403
00:43:40,770 --> 00:43:47,100
the degree of the polynomial we're using is on the x axis and this infinity spot is for the IPD.

404
00:43:48,180 --> 00:43:55,110
So, so what we're seeing here is, is this is a pretty, a pretty uninteresting figure on the surface, right?

405
00:43:55,110 --> 00:44:00,179
Because these things all at the same. But that's good because that means that even with these lower degree polynomials or

406
00:44:00,180 --> 00:44:04,440
matching up pretty well with both the both the point estimate and the inference.

407
00:44:04,820 --> 00:44:09,030
Right. So these are the bars are sort of the 95% confidence intervals.

408
00:44:10,080 --> 00:44:16,080
And again, even with a low degree polynomial, pretty much across the board, we're doing really well.

409
00:44:16,770 --> 00:44:19,220
And I do think that's partly because we have a common censoring time.

410
00:44:19,290 --> 00:44:28,229
So when you have, uh, depending on how many since your observations is going to determine the significance of the importance of that,

411
00:44:28,230 --> 00:44:34,410
that contribution to the line likelihood, um, and it could be a lot potentially.

412
00:44:35,280 --> 00:44:41,560
So it's when we're not approximating that at all like in this case we should do better in the,

413
00:44:41,580 --> 00:44:50,729
in terms of the estimation sort of at the end of the day, the only, the only issues to that that are noticeable here are down in this log,

414
00:44:50,730 --> 00:44:58,049
uh, the coefficient, variation, parameter estimation where we kind of need a degree for the polynomial to get a

415
00:44:58,050 --> 00:45:03,180
good estimate and maybe a degree six polynomial to get a good thing or sorry,

416
00:45:03,180 --> 00:45:06,390
but inference with the likelihood function.

417
00:45:06,960 --> 00:45:12,120
So, so this is, this is really, I think a good a good result here.

418
00:45:12,120 --> 00:45:18,929
As far as in this dataset, we see that the aggregated data estimates match up very well in terms of both point

419
00:45:18,930 --> 00:45:24,510
estimates and inference with what we would obtain would be with the individual level data.

420
00:45:25,320 --> 00:45:32,190
Um. Yes. So if you don't have the individual level data, like how do you know which datum?

421
00:45:33,730 --> 00:45:42,129
Why? Well, that's a good question. And what the nice feature is that since we're approximately like this anyway,

422
00:45:42,130 --> 00:45:48,160
we can just sort of different models and we can compare based on something like AIC or something like that.

423
00:45:48,670 --> 00:45:53,710
Um, the question is again, how, how good is the approximation?

424
00:45:54,250 --> 00:45:57,760
Um, so, you know, when you're doing model comparisons and things like that, you're,

425
00:45:57,760 --> 00:46:03,339
you're typically not thinking that those AIC values are a lot like the maximize log,

426
00:46:03,340 --> 00:46:07,700
like your values or whatever are sort of they're sort of more fixed, right?

427
00:46:07,720 --> 00:46:12,280
But in our case, it's a little bit less definite.

428
00:46:13,060 --> 00:46:16,690
But in principle, I think that's that's the approach you can take to, to choose.

429
00:46:17,320 --> 00:46:23,090
Um, but what's, what's unfortunate is that we wouldn't necessarily have access to this, right?

430
00:46:23,110 --> 00:46:28,810
Yeah. You couldn't necessarily look at a plot like that. So you kind of have to rely on some some good type measures, I think.

431
00:46:32,130 --> 00:46:39,420
Yeah. So so again for this data sets, we we've seen that the aggregate assessments are pretty good.

432
00:46:39,810 --> 00:46:45,180
They do pretty well as far as point estimates. And then France is a small sample size.

433
00:46:45,180 --> 00:46:50,190
So it's not necessarily generalizable. It's very particular patient population.

434
00:46:50,820 --> 00:46:53,190
Um, what we saw in terms of,

435
00:46:53,190 --> 00:47:03,120
of actual sort of conclusions was maybe a significant difference in the two groups in probability of being a never vaccinated, right.

436
00:47:03,120 --> 00:47:09,089
So if we look at this, these panels here the non has an in-group has a much lower probability of being

437
00:47:09,090 --> 00:47:12,840
an ever vaccinated as estimated by these models than the hesitant group,

438
00:47:13,140 --> 00:47:20,310
although they have very similar time in times to vaccination given that they are not in that in that non susceptible group.

439
00:47:20,910 --> 00:47:23,700
So that's kind of an important and interesting finding here.

440
00:47:24,270 --> 00:47:31,129
Um, we see really good performance for the approximation even with lower degree polynomials, which again it's partly a function of,

441
00:47:31,130 --> 00:47:39,570
of the smaller sample size, um, but also a function of the fact that we have a common censoring time.

442
00:47:39,660 --> 00:47:42,270
So the approximation is not having to do as much work there.

443
00:47:43,350 --> 00:47:51,720
Um, so just to kind of wrap up here, we, we've developed this method for fitting permaculture,

444
00:47:51,840 --> 00:47:58,760
parametric mixture care models to aggregate data so we can get away with not using the IPD by,

445
00:47:58,800 --> 00:48:02,820
by taking advantage of these summary statistics that are just sample moments of the data.

446
00:48:03,480 --> 00:48:09,750
Um, we use the of approximation to, to carry this out, which includes sort of estimates of the error.

447
00:48:09,960 --> 00:48:17,970
So you can tell for a given data set, you know, is this going to be a sufficiently good approximation because potentially each iteration of,

448
00:48:17,970 --> 00:48:21,180
of your log likelihood maximization you could output these error estimates,

449
00:48:21,220 --> 00:48:25,140
you can see if it's kind of shooting off or, you know, if it's if it's actually okay.

450
00:48:25,770 --> 00:48:31,950
So that's something I want to work on a bit more is, is incorporating some, some version of that into the algorithm.

451
00:48:32,550 --> 00:48:44,520
Um, still thinking about these tuning parameters as far as K and this, this interval A and B A to B and again, we have this,

452
00:48:44,520 --> 00:48:52,500
this option of looking at different distributions as well, which again becomes important because in the parametric survival world,

453
00:48:53,220 --> 00:48:59,790
we have to be more careful about choosing distribution because with censoring, uh, you have more,

454
00:48:59,790 --> 00:49:05,549
more problems with this specification than with when you don't have censoring because you,

455
00:49:05,550 --> 00:49:09,420
because you need to sort of say what's going on in the tails of the distribution.

456
00:49:10,230 --> 00:49:20,160
Um, so again, with, with larger K, we could potentially have more trouble with estimation, but it could be a better approximation.

457
00:49:20,730 --> 00:49:28,080
So some extensions that I wanted to look at are again, more richer parametric families.

458
00:49:28,080 --> 00:49:33,420
So generalized gamma with three parameters piecewise exponential hazard would be the more general.

459
00:49:33,950 --> 00:49:36,740
Um, and then regression modeling as well.

460
00:49:36,750 --> 00:49:42,959
So we could certainly do something with group data, but continuous coverage would be a bit harder and then some,

461
00:49:42,960 --> 00:49:50,110
some other approximations that could, they could be used or other polynomial approximations that may have different properties.

462
00:49:50,370 --> 00:49:56,340
And then there's even other ways to obtain approximate spatial statistics that may or may not work in our scenario.

463
00:49:57,090 --> 00:49:58,835
Um, but then I just wanted to.

