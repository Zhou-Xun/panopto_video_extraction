1
00:00:00,930 --> 00:00:08,060
Okay. So so today we'll start our exploration in.

2
00:00:11,480 --> 00:00:17,000
In our whole process. This is a one off the first one, not the money.

3
00:00:19,340 --> 00:00:25,970
So the way we discuss Markov process is like the classification of stochastic process.

4
00:00:26,600 --> 00:00:32,150
So we classify according to the index that we need to classify according to the state space.

5
00:00:32,480 --> 00:00:46,080
So the first encounter here, we will discuss the finite state space and a discrete marginal chain.

6
00:01:00,710 --> 00:01:13,190
So sometimes when people talk about Markov chain, they are already thinking about implicitly the name implicates and implicated.

7
00:01:13,220 --> 00:01:17,750
We're talking about discrete time, stochastic process.

8
00:01:18,960 --> 00:01:27,040
Complex and the length of the chain. So if it's a continuous time, where you from Markov process.

9
00:01:27,560 --> 00:01:34,459
But first thing first, this is such a famous name in stochastic process.

10
00:01:34,460 --> 00:01:39,230
We first need to give a destination for.

11
00:01:41,770 --> 00:01:46,510
For this under the Markov property as well. Okay.

12
00:01:46,570 --> 00:02:19,210
So the definition is we're talking about, at least in the current context of the screen, time of the train with finite or comfortable state space.

13
00:02:24,470 --> 00:02:27,630
All right. So this is a justifying the classification.

14
00:02:27,650 --> 00:02:31,880
What specific type of stochastic process we're going to talk about if we.

15
00:02:33,830 --> 00:02:39,700
Under the name of finite or composting space Markov chain.

16
00:02:40,130 --> 00:02:50,540
So the fact the support of the random, the composing random variables in this stochastic process are with finite or comfortable state.

17
00:02:50,550 --> 00:02:55,580
Space means we can map the state space into integer values.

18
00:02:56,420 --> 00:03:08,989
Right? So it could be finite, or you might, but you are comfortable in so we can make those the states possible states each member random,

19
00:03:08,990 --> 00:03:13,520
the variable can take two integers. So that's important for the notation.

20
00:03:14,240 --> 00:03:18,380
Okay. So this saying, first of all, is a stochastic process.

21
00:03:27,230 --> 00:03:33,470
All right. Now we need to specify the additional property of this classic process.

22
00:03:36,200 --> 00:03:41,810
Such that for all states and the old times.

23
00:03:50,510 --> 00:04:00,469
So kind of a universal. So the following conditional probability equation.

24
00:04:00,470 --> 00:04:04,100
Satisfied. So the probability.

25
00:04:04,970 --> 00:04:08,870
Okay. So first on. I forgot to write the notation here.

26
00:04:09,470 --> 00:04:25,050
It's one week, right? I. 020 infinity to denote this is a finite collection of random variables that satisfy the discrete time.

27
00:04:25,080 --> 00:04:39,840
So we have the the integer value index and then we have each of the X1 we support in finite spaces in finite or combo space.

28
00:04:40,440 --> 00:04:51,370
All right. So with this we have X plus one equals J conditioning x zero.

29
00:04:51,370 --> 00:04:59,849
We can do that now, x one or two, x and so on and so forth, all the way up to x.

30
00:04:59,850 --> 00:05:15,180
And you can both say you get to I mean, so this is trying to describe this conditional on all the previous states of a markov chain at the time.

31
00:05:16,200 --> 00:05:29,310
And plus one, if your condition on all of the states is the same as the probability of X and you can see.

32
00:05:32,180 --> 00:05:37,820
All right. So this equation is not not some Markov property.

33
00:05:45,970 --> 00:05:57,100
All right. So as we have been familiar with the modern definition of conditional probability and then this particular statement,

34
00:05:57,490 --> 00:06:01,690
if your conditional more thing is equivalent to conditional on just X,

35
00:06:01,690 --> 00:06:07,600
and that's actually a conditional as a probability statement of a conditional independence relationship.

36
00:06:08,170 --> 00:06:17,560
Right. So what exactly about conditional independence or relationship is is fully encoded in this probability statement.

37
00:06:17,950 --> 00:06:34,450
But alternatively, we can write this as the Markov property can be equivalently represented as a conditional in the current state.

38
00:06:42,370 --> 00:06:44,769
So this is a equivalence that I'm going to write.

39
00:06:44,770 --> 00:07:00,690
So as an X plus one will be independent of x0x1, so on and so forth, all the way up to x and minus one if we conditional access.

40
00:07:01,690 --> 00:07:04,930
So knowing the value,

41
00:07:04,990 --> 00:07:15,520
knowing the with the knowledge of X and everything else happens in the past before the Markov chain become irrelevant

42
00:07:15,520 --> 00:07:22,780
to the current to the future state or the current state depends on if you are labeled and an is the present.

43
00:07:22,780 --> 00:07:30,610
Let's use the this terminology conditional on X then that's the present state, the future state,

44
00:07:31,720 --> 00:07:37,990
right, the field, the run, the variable x and plus one indicate the future state and then the past.

45
00:07:37,990 --> 00:07:41,030
The state becomes independent.

46
00:07:42,100 --> 00:07:48,940
It's not an unconditional independence, so you need to write this condition.

47
00:07:49,360 --> 00:07:59,590
And that's fully what these, you know, the property of the probability conditional probability equation because that's.

48
00:08:01,170 --> 00:08:09,150
Okay. So it's pretty easy to remember if your label present, past and future.

49
00:08:09,420 --> 00:08:17,880
So this is basically saying given the present state, given the present, the past and the future becomes irrelevant.

50
00:08:18,150 --> 00:08:21,960
Right. That's all.

51
00:08:22,200 --> 00:08:25,200
I think that's that's a pretty strong statement.

52
00:08:26,430 --> 00:08:32,340
You should realize, actually, in statistics, money independence statement are, you know,

53
00:08:32,610 --> 00:08:39,569
we make this thing we make that sort of assumption all the time, you know, applications and they give you a false impression.

54
00:08:39,570 --> 00:08:49,820
That's, you know, it's just a normal assumption to make, but it's actually a very strong assumption in in the context you make.

55
00:08:50,630 --> 00:08:56,270
This one. This, too. But this is. We're going to change the.

56
00:08:56,970 --> 00:09:02,470
The. The details of the Markov chains.

57
00:09:02,470 --> 00:09:09,280
We made later on discuss Markov chains in continuous state space.

58
00:09:09,550 --> 00:09:16,959
We may discuss the Markov processing continuous indices, right,

59
00:09:16,960 --> 00:09:25,910
so that using the continuous time Markov process, but this the essence of the Marcos property remain the same.

60
00:09:25,930 --> 00:09:33,570
So you can always understand the Markov property as given to present the possible future are independent.

61
00:09:33,610 --> 00:09:36,940
The future doesn't depend on the. Yeah, sounds better.

62
00:09:37,000 --> 00:09:43,230
The future doesn't depend on the past given the present.

63
00:09:43,240 --> 00:09:55,990
So what you do in the present time are small, not supposed to be something as purely mathematical, not not philosophical, but that's what it is.

64
00:09:56,530 --> 00:10:05,020
And of questions. This is important, right? So the definition is just trying to emphasize this conditional independence relationship, right?

65
00:10:06,190 --> 00:10:11,440
In such a process, the other things.

66
00:10:11,440 --> 00:10:15,130
So this is a generally true for all the Markov process.

67
00:10:15,520 --> 00:10:20,409
And then we can. Okay, I should say here.

68
00:10:20,410 --> 00:10:23,920
So this is a this is a property is generally true for all the Markov chains.

69
00:10:24,310 --> 00:10:32,560
Even you can expand to other Markov processes that you know, it's not in the finite state space,

70
00:10:32,890 --> 00:10:38,980
not in the discrete time, but it's the following property we're going to discuss.

71
00:10:39,700 --> 00:10:42,340
A special properties is more specific.

72
00:10:42,730 --> 00:10:51,730
So we're not may not be true for all the Markov process, but for the discussion of this clause, we need to focus on some additional properties.

73
00:10:51,730 --> 00:10:56,050
We may add on to the Markov, the Markov chain.

74
00:10:57,560 --> 00:11:05,510
So one of the typical properties we add on is so called time homogeneous assumption.

75
00:11:09,170 --> 00:11:15,950
So this is site of one. Let's put a star in here.

76
00:11:16,100 --> 00:11:23,680
That means not as general for not knowing that general for all these mark operations.

77
00:11:25,850 --> 00:11:32,290
So this time I'm working on something not homogeneous.

78
00:11:33,140 --> 00:11:39,890
I think we're using a homogeneous, homogeneous Markov chain.

79
00:11:46,130 --> 00:11:55,820
So, you know, the the right hand side is X one plus one equals J and then conditional on X and plus I.

80
00:11:56,300 --> 00:11:59,930
So that statement depends on and you're right.

81
00:11:59,960 --> 00:12:09,040
Although the present and in the future is pretty clear from that notation by seemingly depends on the transition.

82
00:12:09,050 --> 00:12:16,250
So the transit from an to unpleasant one depends on what time exactly time point you are.

83
00:12:16,370 --> 00:12:33,320
The Markov chain is right. And if you can Johno or you write the probability x plus one equals j, conditional on x and y is invariant with respect.

84
00:12:33,680 --> 00:12:41,660
We call this anchor time and right. So this is a one state of transition once one time transition from end to end plus one.

85
00:12:41,960 --> 00:12:49,130
But one can be different, right? You can think about an equal to one and you go to 2 to 3, so on and so forth.

86
00:12:49,580 --> 00:12:55,850
So it's possible this particular conditional probability could be.

87
00:12:58,770 --> 00:13:08,670
Could be different at different time points. But if there is a virus and it's not a function of anchor point, we call this.

88
00:13:09,120 --> 00:13:26,400
So this is a PJ night invariant or constant with respect to this anchor time.

89
00:13:26,550 --> 00:13:39,170
And so this is not something that we call this is a time homogeneous Markov chain.

90
00:13:39,180 --> 00:13:43,940
Right. So over the transitions time, a homogeneous means what?

91
00:13:43,980 --> 00:13:47,280
So there is some sort of a stable.

92
00:13:50,370 --> 00:13:55,410
Probability law is governing this the running of the Markov chain.

93
00:13:55,500 --> 00:14:05,370
Right. So at the time, regardless what time you have, if you're thinking from the time point until OnePlus one,

94
00:14:05,370 --> 00:14:11,610
you obtain the same probability you you draw from the same condition of distribution.

95
00:14:11,620 --> 00:14:21,080
So that's why it's important. So usually this for a lot of the questions that a lot of the problems we're being we're.

96
00:14:22,340 --> 00:14:27,410
We can conquer in them Markov chain. This goes without saying.

97
00:14:29,660 --> 00:14:34,910
But, you know, that's a that's really kind of a special additional assumption.

98
00:14:35,450 --> 00:14:39,200
It's not generals and all these are chiefs.

99
00:14:43,310 --> 00:14:48,850
So this is called the time homogeneous property or time coordinates.

100
00:14:50,390 --> 00:14:54,020
Okay. All right.

101
00:14:54,140 --> 00:15:05,940
On top of that now, we kind of narrow down to discuss time, homogeneous, discrete time, discrete state space, moral dimension.

102
00:15:06,380 --> 00:15:08,090
So in this context,

103
00:15:15,950 --> 00:15:34,880
all this probabilistic dynamic of this Markov chain going to be going to be car and or kind of state is controlled by the side of the probabilities.

104
00:15:35,210 --> 00:15:45,990
Right. So those conditional probabilities, if you know your current state is I, what is the probability you have to get to the other another state?

105
00:15:46,060 --> 00:15:51,260
J Right. So I. J So this is a set of probabilities.

106
00:15:51,830 --> 00:16:05,390
It's known as transition probability. So this set of probabilities, knowing us usually call it one step,

107
00:16:08,120 --> 00:16:12,380
whereas the size of this one stack, because there's always from end to end plus one.

108
00:16:12,860 --> 00:16:29,870
Okay. Now we're going to discuss how if we can figure out like two or three step transitions so as a one step transition of probabilities.

109
00:16:35,870 --> 00:16:39,380
The fancy name is sometime some people call this a kernel.

110
00:16:40,700 --> 00:16:48,740
So one, let's say in transition col transition probabilities will refer to this side of, uh, conditional probabilities.

111
00:16:56,190 --> 00:17:07,050
So one of the things is because we consider both the case of finite and uncomfortable many states space.

112
00:17:07,530 --> 00:17:10,980
So this set notation is.

113
00:17:13,490 --> 00:17:16,330
Universally applicable in this context.

114
00:17:16,340 --> 00:17:25,220
But if you only consider the the finite state space, then we can have a better representation of the transition curve.

115
00:17:25,580 --> 00:17:33,130
Right? So it's probably double the start for.

116
00:17:35,950 --> 00:18:00,990
I stayed Mark on the train so we can express and represent this transition colonel into a matrix.

117
00:18:01,650 --> 00:18:15,270
So this is known as a transition matrix. So you are given a transition matrix.

118
00:18:15,270 --> 00:18:18,940
So automatically, no, you're dealing with a finite state space.

119
00:18:21,520 --> 00:18:24,640
So this the matrix layout is pretty simple.

120
00:18:25,000 --> 00:18:33,190
So you need to first label all the states horizontally and vertically, so you have a state zero.

121
00:18:33,490 --> 00:18:39,400
So usually we start with state zero, one, two, so on and so forth.

122
00:18:39,430 --> 00:18:44,290
But last one has to be and so you have to have finite. If you have a finite, you cannot write.

123
00:18:44,530 --> 00:18:53,320
So we don't have a such a thing with, you know, matrix, which is for inside dimensions.

124
00:18:53,530 --> 00:19:01,890
So 012. So, so this is the role number.

125
00:19:01,900 --> 00:19:05,050
This is a column number. So you have ap00.

126
00:19:07,030 --> 00:19:14,070
That means the probability in one step transit from state zero to state zero.

127
00:19:14,080 --> 00:19:22,960
So you may remain in the state from the Markov chain point of view, you did a transition, but you transition back to your original state.

128
00:19:23,080 --> 00:19:35,080
Right. So all the diagonals will be this p11, two P and you have p01.

129
00:19:35,740 --> 00:19:42,340
So you can understand this. Your start, the P, the accent equal to zero and plus one.

130
00:19:43,090 --> 00:19:46,090
This one, what is that probability? That's p01.

131
00:19:46,540 --> 00:19:51,729
And then so on and so forth. That's at the end of the P integral.

132
00:19:51,730 --> 00:19:56,980
So you can feel all these matrix according to accordingly.

133
00:19:58,420 --> 00:20:08,710
Here's some properties with. So this is a V. So in this case, according to this notation, there will be a plus one by plus one matrix.

134
00:20:10,690 --> 00:20:14,980
So there are certain properties for this matrix.

135
00:20:15,640 --> 00:20:28,210
For example, if you're starting the accent s I done so for the I throw so p I say k a.

136
00:20:32,480 --> 00:20:38,500
This is not right. Yes, I saw it.

137
00:20:40,840 --> 00:20:47,710
All right. So actually so this notation is trying to track the song of a girl.

138
00:20:47,740 --> 00:20:53,980
Right. So for that, I throw I some over older columns, a case column.

139
00:20:54,460 --> 00:20:59,200
So this one's okay? Equal to zero two and this one has to be one.

140
00:21:01,920 --> 00:21:10,740
Right. So this is a required by the conditional distribution because if you are in this state, for instance,

141
00:21:13,170 --> 00:21:20,580
you are in the state by the next step, you have to the Markov chain had to align in the valid state, right.

142
00:21:20,850 --> 00:21:26,190
If you are enumerate all possible state cake with a zero, they are mutually exclusive events.

143
00:21:26,520 --> 00:21:36,240
If you're counting all possible events that probability so that the probability has to all the probabilities of two add up to one.

144
00:21:37,170 --> 00:21:47,880
Yeah. So there are always some is a kind of strain for oh these homogeneous transition kernels for finite state homogeneous transition kernels.

145
00:21:49,140 --> 00:21:53,070
Do we have a constraint on the column? Some. Ah.

146
00:21:53,090 --> 00:21:56,440
Oh, they're calling me to add up to one. Good.

147
00:21:56,740 --> 00:22:00,010
There is no constraint. In general, there is no constraint.

148
00:22:01,420 --> 00:22:04,600
You could end up in, I think, your state.

149
00:22:05,890 --> 00:22:11,740
Yeah. But so that that one doesn't have a constraint. That's not the conditional distribution in general.

150
00:22:13,480 --> 00:22:16,750
Obviously, the entries has to be non-negative.

151
00:22:16,780 --> 00:22:22,420
You can have zero and then the entries cannot be greater than one.

152
00:22:23,750 --> 00:22:27,630
Okay. Okay.

153
00:22:27,950 --> 00:22:32,900
So that's all we need to say about the definition of the mark off chain.

154
00:22:32,930 --> 00:22:38,350
There are a few things. Remember, there's the only core thing here is the mark of property.

155
00:22:38,350 --> 00:22:41,660
And given the present that the possible futures are independent.

156
00:22:42,170 --> 00:22:44,160
So there's a conditional independence statement.

157
00:22:44,630 --> 00:22:52,160
Time homogeneous is really an added property and we know the terminology of transition probability and the transition kernel,

158
00:22:52,250 --> 00:22:59,100
or by default this means one state. All right.

159
00:22:59,110 --> 00:23:02,169
So this is the first time we gave the formal definition.

160
00:23:02,170 --> 00:23:15,510
But as a matter of fact, we have learned the stochastic processes we have studied before into this category of smart contracts.

161
00:23:17,470 --> 00:23:26,050
So the question is, can we prove it? According to the new method and more specifically, how we shoveled out.

162
00:23:31,660 --> 00:23:42,180
So this would be the example people use.

163
00:23:45,610 --> 00:23:49,270
All right. So we just talk about simple, random walk.

164
00:23:51,160 --> 00:23:58,060
Let's see if we can show you what a simple, random walk is. A one dimensional, simple, random walk.

165
00:24:09,320 --> 00:24:15,830
Okay. So how would you do this? In general, we only have the the definition of Markov chain, right?

166
00:24:15,860 --> 00:24:24,260
So you, if you want to show something, is a markov. You have to match the definition of the specific or the specification of the specific.

167
00:24:25,350 --> 00:24:28,510
So casting process to the definition of mark.

168
00:24:30,440 --> 00:24:35,600
So to do that, we can write as a plus one.

169
00:24:38,180 --> 00:24:44,590
So this is a from the definition of simple random one as zero loss.

170
00:24:44,710 --> 00:25:01,130
So this is a I'm actually using a I'm not saying a zero has to be zero, but I want to unplug one side.

171
00:25:01,220 --> 00:25:12,380
I i the Karzai is the individual whose so those are i d so if you define as plus one in this way,

172
00:25:13,430 --> 00:25:26,180
then you can write us on the plus one equal to s on the plus side, the plus one, right?

173
00:25:26,510 --> 00:25:30,499
So the ends, the plus one,

174
00:25:30,500 --> 00:25:43,170
that's the future placement of your Markov chain at the time and point and plus one is the present position plus a random variable that's the,

175
00:25:43,490 --> 00:25:49,520
the, the move and the particular move you're going to make in one step.

176
00:25:51,740 --> 00:25:59,460
So that means what? So all the information. So if you're conditioned now, you need to think about the conditional independent statement.

177
00:25:59,690 --> 00:26:09,170
So if we conditional on X and the randomness of X as a plus one only depends on this side

178
00:26:09,350 --> 00:26:17,240
and plus one has nothing to do with I some minus one as a minus two as a minus three,

179
00:26:19,310 --> 00:26:23,440
so on, so forth. So that establish a conditional independence.

180
00:26:23,540 --> 00:26:39,500
So just by inspecting this definition, we can say so this implies as a plus one as independent of as zero.

181
00:26:40,490 --> 00:26:48,170
That's one conditional, right?

182
00:26:48,740 --> 00:26:58,910
If you give me as and then none of these same cannot be provide additional information for some plus one.

183
00:27:00,560 --> 00:27:12,680
Right. So the only randomness come from the site and plus one which is a random variable, has nothing to do with us zero two as a minus one.

184
00:27:15,880 --> 00:27:27,670
So that's sufficient to say this is satisfies the mark of property and therefore the simple random walk is a mark of training.

185
00:27:33,780 --> 00:27:38,280
Right. So. So in most cases,

186
00:27:38,280 --> 00:27:43,859
if you want to classify stochastic process is a mark of chain you usually do is that

187
00:27:43,860 --> 00:27:49,200
as things like this establish the condition of independent of this relationship.

188
00:27:56,100 --> 00:28:08,680
Okay. Any questions? So let's use the same technique to see if Carlton Watson processed the mark.

189
00:28:09,670 --> 00:28:14,980
Well, it is because the definition, so-called plus size, is a mark of process.

190
00:28:15,490 --> 00:28:26,920
But let's just double check it with the definition of mark checking again.

191
00:28:27,040 --> 00:28:33,130
So if you want to start with this formal argument, you need to write it out,

192
00:28:33,280 --> 00:28:44,290
find a way to to connect that the definition so called was probably in process.

193
00:28:46,030 --> 00:28:54,399
So one of the definitions we can write that's relatively easy or to establish the

194
00:28:54,400 --> 00:29:00,280
connections is this one so that for the Z and plus one for the future generation,

195
00:29:00,820 --> 00:29:13,210
how do you generate a Z and plus one? All you do is you need to go through all the offsprings in the current generation, which is the Z, right?

196
00:29:13,270 --> 00:29:18,550
So this is equal to one. And then come the offsprings.

197
00:29:19,360 --> 00:29:24,640
He's the one this generation offspring produce individually.

198
00:29:25,150 --> 00:29:30,160
So that's where you have this accident, my right.

199
00:29:30,310 --> 00:29:35,770
So X and I represent the individual in the ninth generation.

200
00:29:36,940 --> 00:29:39,420
But the offspring is generated by the.

201
00:29:41,490 --> 00:29:48,720
But this particular individual, that sort of generation, and then this comedy would like it to be an individual.

202
00:29:49,110 --> 00:29:56,010
So that was always outside the is the offspring distribution.

203
00:29:56,200 --> 00:29:59,310
Right. So it's just think about the tree.

204
00:30:01,350 --> 00:30:05,820
So if this is the only generation you're going through every individual,

205
00:30:06,330 --> 00:30:13,680
and then comes a number of the offspring, but each individual is equal to one or two or so and so forth.

206
00:30:13,680 --> 00:30:18,780
So you add them up, you get the number of offsprings in the plus one generation.

207
00:30:19,710 --> 00:30:23,640
And then this definition, it's a very clear right?

208
00:30:23,640 --> 00:30:28,100
So you should treat OnePlus one and the ZM plus one and see it and as random variable.

209
00:30:28,110 --> 00:30:31,980
So this is a compound random variable problem is not trivial.

210
00:30:32,370 --> 00:30:40,250
Nevertheless see this shows if you want to count the the Z and plus one,

211
00:30:40,260 --> 00:30:56,040
if you want to estimate or you want to find the realization of Z and plus one, it going to be irrelevant to the Z, one, z to two to Z minus one.

212
00:30:56,730 --> 00:31:11,010
If you conditional of Z, you don't need any more information from the previous generation, conditional on Z if you already know Z.

213
00:31:11,910 --> 00:31:26,520
So I probably forgot to say that conditional independent statement is so very different than unconditional independence statement.

214
00:31:26,550 --> 00:31:32,820
Right? So you should realize Z and plus one and then Z one, zero plus one and Z two.

215
00:31:32,820 --> 00:31:35,920
They are dependent. Okay.

216
00:31:37,480 --> 00:31:41,799
The reason they're easy funds for you and because you're conditional on the additional

217
00:31:41,800 --> 00:31:47,530
thing that says the Z is in other way to say all of the information from Z one,

218
00:31:47,530 --> 00:31:55,480
two and minus one. All of these information related to the Z and plus one is encoded in the Z.

219
00:31:56,260 --> 00:31:56,510
Right.

220
00:31:58,720 --> 00:32:15,700
We don't talk this a lot in basic statistics, but conditional independence and undefined and just unconditional independence has a drastic difference.

221
00:32:15,940 --> 00:32:19,059
Okay, let's go back to the random walk example.

222
00:32:19,060 --> 00:32:31,360
If I think about as I'm plus one and I say as for I'm less than so unconditionally, there are dependent.

223
00:32:31,360 --> 00:32:36,249
How do you show their dependance? Because you can't use the definition.

224
00:32:36,250 --> 00:32:41,500
So this is just consi. I coach for 1 to 1 plus one.

225
00:32:43,960 --> 00:32:48,010
And then this part, this y equals one.

226
00:32:48,280 --> 00:32:56,710
And I suppose you already see the share the random variables up to the point and.

227
00:33:09,700 --> 00:33:13,410
Right. So you can be compulsive. So they share some randomness.

228
00:33:14,500 --> 00:33:17,110
Right. So the path is overlap the so-called.

229
00:33:17,920 --> 00:33:27,219
So if you're trying to compute unconditional independence or you want to calculate the covariance is obviously it's not zero, right.

230
00:33:27,220 --> 00:33:37,900
Because they share a common path. So they're they are not independent, but they are conditionally independent.

231
00:33:38,080 --> 00:33:53,050
If I give you and because I if I conditional on the the payment status and this path is no longer random, the information is fully encoded in this.

232
00:33:53,710 --> 00:34:05,260
So you so it's pretty obvious you can see the two random variables are dependent, but they can be conditional independent.

233
00:34:06,850 --> 00:34:10,720
So the next question is I actually I'll sign this on the course.

234
00:34:11,050 --> 00:34:19,150
Do you ever seen a to run? The variables are actually independent, but conditionally.

235
00:34:20,240 --> 00:34:23,600
Dependent and that things happen.

236
00:34:26,290 --> 00:34:31,690
So to run. So this is to run the variables as one and the same.

237
00:34:32,740 --> 00:34:36,990
So these two are. Unconditionally.

238
00:34:37,140 --> 00:34:41,590
They are dependent, but you can find the right random variable conditional on that.

239
00:34:41,610 --> 00:34:44,640
That becomes conditional independent.

240
00:34:45,240 --> 00:34:49,470
I'm asking a similar question, but in a different way.

241
00:34:50,130 --> 00:34:56,940
Do you ever seen two random variables? There are unconditionally independent, but they are conditionally.

242
00:34:57,120 --> 00:35:00,720
Actually, you can commit. If you're conditional on something, they become dependent.

243
00:35:04,470 --> 00:35:08,070
All right? No, I would assume that is not possible.

244
00:35:11,070 --> 00:35:16,160
So if you just learn the probability, I think from a mathematical point of view, you're sinking.

245
00:35:16,170 --> 00:35:23,040
The two things don't share any information. How could they become dependent conditionally conditional on something?

246
00:35:23,220 --> 00:35:27,450
Why? If they share something information, it's actually possible.

247
00:35:29,760 --> 00:35:35,940
It happens all the time in genetics and then a lot of other causal inference cases.

248
00:35:36,330 --> 00:35:40,110
So think about these two patterns.

249
00:35:40,440 --> 00:35:45,989
So they're kind of a random individuals from the population.

250
00:35:45,990 --> 00:35:53,100
So their DNA, we say, are independent. Let's say it's part, you know, so they are independent.

251
00:35:53,100 --> 00:35:57,520
Their DNA is are on the correlate. They don't share anything common.

252
00:35:57,630 --> 00:36:01,170
That's a rough statement, but it's rock and shore.

253
00:36:02,100 --> 00:36:08,280
So this the DNA of a mother and the father you can consider independent.

254
00:36:09,000 --> 00:36:13,890
Okay. And then they produce offspring. Right.

255
00:36:13,920 --> 00:36:21,590
So this offspring. If we condition along the the DNA of the offspring.

256
00:36:22,790 --> 00:36:31,670
What going to happen. So knowing the DNA of the offspring, because the offspring is part of the father and then part of the mother.

257
00:36:31,910 --> 00:36:39,040
If you know something about the offspring's DNA, then you know something about the father's DNA.

258
00:36:39,050 --> 00:36:42,660
And then, you know, some fathers, you know. You know, fathers DNA.

259
00:36:42,680 --> 00:36:46,290
If it's something come from the father, then, you know, that's study.

260
00:36:46,460 --> 00:36:53,000
It's not coming from the mother in this sense it doesn't conditional on the offspring.

261
00:36:53,720 --> 00:36:57,500
To the DNA is from fathers and mothers become dependent.

262
00:36:58,980 --> 00:37:05,360
Okay if that's puzzling there is a even more famous example.

263
00:37:07,350 --> 00:37:10,880
Dependance is probably the, you know, the most difficult.

264
00:37:11,390 --> 00:37:16,040
So I'm glad to know that there are three types of events.

265
00:37:16,160 --> 00:37:26,690
So the first one, this one, the event one is there is a sprinkler, probably get the grass, the grass wet.

266
00:37:27,380 --> 00:37:33,880
All right. So this is a so this is a, you know, like a sprinkler sprinkler.

267
00:37:35,420 --> 00:37:38,990
So the event to here would be nowadays a rain.

268
00:37:40,700 --> 00:37:47,900
There is a passing. Right. And then so these two things are I think we should always.

269
00:37:50,040 --> 00:37:56,340
You know consider their independent are operating a sprinkler independent of the well.

270
00:37:57,360 --> 00:38:05,280
Maybe not, but let's continue. Let's consider, you know, the schedule is two things are independence.

271
00:38:05,520 --> 00:38:11,320
And then the observation is st is the ground or the grass so that you run.

272
00:38:11,400 --> 00:38:16,570
So really the grass is what? All right.

273
00:38:16,930 --> 00:38:20,080
So these two things are unconditionally independent.

274
00:38:20,200 --> 00:38:25,990
But if you observe, the grass is wet, you know, either this happened or this happened.

275
00:38:26,380 --> 00:38:31,810
Right. If you rule out one. So, you know, there there is a there is no passing rate.

276
00:38:32,380 --> 00:38:38,890
And that means your sprinklers are probably a malfunction. They just make the grass wet.

277
00:38:39,150 --> 00:38:46,920
All right. So conditional on these two things become so this is the important case in causal inference.

278
00:38:46,930 --> 00:38:54,430
Both of these are because two things can two independent factors can cause the same phenomenon.

279
00:38:55,330 --> 00:39:01,390
Right. Although they're apparently independent. If you can rule out one thing, the other becomes the only choice.

280
00:39:01,710 --> 00:39:04,750
That's the dependance by observing this.

281
00:39:05,260 --> 00:39:08,640
So this is known as collider structure.

282
00:39:09,160 --> 00:39:16,600
I could see two arrows going two without cause. So you very interesting graphical models of causal inference.

283
00:39:16,600 --> 00:39:27,010
You're going to see a lot of these things but so this is this is why when you say something is dependent or not dependent, you always have a context.

284
00:39:27,430 --> 00:39:31,600
It's conditional on something or not conditional, right?

285
00:39:32,060 --> 00:39:40,900
So a lot of causal inference techniques like Mendelian randomization or the instrument variable

286
00:39:40,900 --> 00:39:46,090
analysis depends on this type of the structure of conditional independence or consent,

287
00:39:46,180 --> 00:39:50,860
unconditionally independence.

288
00:39:51,040 --> 00:39:57,730
So, okay, so just you need to appreciate more about the.

289
00:40:01,340 --> 00:40:06,260
The specific the context of the problem.

290
00:40:07,550 --> 00:40:10,790
So the example three is gambler's rule.

291
00:40:10,970 --> 00:40:19,460
So you don't need to really show the gamblers. There is a mark of change because it's a special case of random walk.

292
00:40:19,490 --> 00:40:21,140
So you can use the same technique.

293
00:40:21,620 --> 00:40:33,470
So the point of writing that number is the wrong problem is to show you could actually write out the transition matrix for gamblers rule.

294
00:40:33,870 --> 00:40:37,310
All right. So we have two barriers.

295
00:40:37,670 --> 00:40:41,630
One, that is zero. That's in bankruptcy threshold.

296
00:40:41,930 --> 00:40:49,310
You don't have money, you cannot gamble. And then there is no exit strategy.

297
00:40:49,370 --> 00:40:58,820
That is the money. If you win at money's at a certain target, the gambler and wealth discipline.

298
00:40:59,480 --> 00:41:03,050
It's a very decent you know, he will stop gambling.

299
00:41:04,730 --> 00:41:10,910
So in that case, you could write this transition matrix accordingly.

300
00:41:11,000 --> 00:41:15,230
So you have the transition probability, the peonage.

301
00:41:16,490 --> 00:41:21,020
All right. So all the state space, you can naturally order them.

302
00:41:21,170 --> 00:41:25,129
So there is a zero to C, right?

303
00:41:25,130 --> 00:41:29,420
So everything in between 1 to 3 and so on, so forth.

304
00:41:31,250 --> 00:41:35,720
And now we can take a look of this what this transition matrix look like.

305
00:41:36,650 --> 00:41:42,110
One So from, you know, if you're in the state zero.

306
00:41:43,440 --> 00:41:51,560
What is the probability of transit to a state one? Sorry.

307
00:41:52,460 --> 00:41:55,700
You're a it's a trick question.

308
00:41:56,700 --> 00:42:03,020
Your Anastasia. So you don't have the money. What is the probability you will translate to zero?

309
00:42:03,090 --> 00:42:06,380
Yes, every state is zero.

310
00:42:07,790 --> 00:42:12,610
But you have a constraint question. So here is one. So we call this absorbing the status, right?

311
00:42:12,620 --> 00:42:15,800
Once you enter into the zero in stating the zero, whatever.

312
00:42:16,720 --> 00:42:20,620
Right. It's different in for this one.

313
00:42:20,860 --> 00:42:26,180
So if you're in the state one, what is the probability you'll get into a state zero?

314
00:42:26,950 --> 00:42:30,669
You lose of that. I'm not. We know that is with probability.

315
00:42:30,670 --> 00:42:36,060
Q What is the probability you stay in the original state one?

316
00:42:37,300 --> 00:42:42,930
No, that's impossible because like the simple random walk, you have to move out.

317
00:42:43,540 --> 00:42:47,110
So 0p000.

318
00:42:48,130 --> 00:42:51,510
And then similarity this.

319
00:42:51,910 --> 00:42:56,469
Well, we know all the other things are zero because we have the rules on constraint.

320
00:42:56,470 --> 00:43:02,020
Q Plus P is already one. There's no entries can be not in zero three.

321
00:43:03,460 --> 00:43:13,840
And I can do this for every single. So they have a pretty there's kind of like a bun, the bandit matrix, right?

322
00:43:13,840 --> 00:43:19,990
So the the queue up just shift is a diagonal.

323
00:43:22,780 --> 00:43:25,570
q0p and so on and so forth.

324
00:43:25,750 --> 00:43:34,600
So when we get back to state, see again, this is absorbing state because once you get the state, see your romance, they see you don't gamble anymore.

325
00:43:34,630 --> 00:43:40,480
So your fortune remains a constant. That's the way to explain this mathematically.

326
00:43:40,780 --> 00:43:45,220
So you can easily write up this transition matrix.

327
00:43:45,820 --> 00:43:48,510
And then this is how this is very useful.

328
00:43:48,520 --> 00:43:58,360
Later on, we're going to see the ability to write out the transition probability, transition kernel or transition matrix in this complex.

329
00:44:00,640 --> 00:44:04,780
And then for the general case of random walk, if you want.

330
00:44:04,930 --> 00:44:13,420
There is no boundary, no barrier, or even in the case that you have only a single barrier like zero,

331
00:44:13,690 --> 00:44:25,840
but no barrier as see, this becomes an infinite state space situation and that you cannot write a transition matrix.

332
00:44:25,870 --> 00:44:34,570
Nevertheless, you can still present the so in that case, let's say just have one single barrier and zero.

333
00:44:34,960 --> 00:45:00,910
In that case, you should write to zero zero equal one and that API iron plus one is key and then API minus one is q and then equal to zero otherwise.

334
00:45:01,940 --> 00:45:13,420
Right. So this is the if we remove the barrier, obviously if we see barrier.

335
00:45:17,670 --> 00:45:21,490
Then this is how you present. Everything else is zero.

336
00:45:21,510 --> 00:45:26,850
So any PJ this is defined as any PJ Right.

337
00:45:26,850 --> 00:45:30,060
So any one slight transition. So only these are.

338
00:45:30,090 --> 00:45:31,050
So those are true.

339
00:45:31,500 --> 00:45:44,160
The caveat is you cannot arrange into a Atrix anymore because you have have finite elements that we have comfortable having these restraints on.

340
00:45:46,920 --> 00:45:54,060
All right. So for this for finite state space, you have to absorbing space as a zero on the sea.

341
00:45:54,630 --> 00:46:03,150
Right. So those absorbing stages has the signature here in zero zero, where you go to one and then you go to one.

342
00:46:03,480 --> 00:46:07,530
In this case, if only one barrier, you have only one absorbing state.

343
00:46:07,980 --> 00:46:13,430
Right. You don't have the the upper barrier anymore.

344
00:46:13,470 --> 00:46:16,500
You remove essentially a single absorbing state.

345
00:46:21,910 --> 00:46:26,590
All right. So those are what we have studied before.

346
00:46:27,100 --> 00:46:30,880
We're going to see a few more examples today.

347
00:46:31,840 --> 00:46:42,160
Yeah, if you're going to do that. The example again is the important contribution of Markov training in science.

348
00:46:43,220 --> 00:46:47,360
Especially in generics. Is a note on the right.

349
00:46:47,360 --> 00:46:55,100
FISHER Model. FISHER Is that the wrong.

350
00:46:55,310 --> 00:47:04,100
FISHER That's what we no longer want the famous statistician to actually call himself a geneticist or nowadays,

351
00:47:04,100 --> 00:47:09,110
as you know, very controversial about his research.

352
00:47:09,590 --> 00:47:13,700
But he is actually a professor of genetics in Cambridge.

353
00:47:16,700 --> 00:47:21,020
It is actually very interesting, I think.

354
00:47:22,430 --> 00:47:26,900
At the same time, there is another Bayesian is Geoffrey Hora.

355
00:47:28,660 --> 00:47:39,160
Harold is his name, I think is probably the most famous position, but he's not also not in the mathematics department or statistics department.

356
00:47:39,160 --> 00:47:43,180
He's major. I think he's a major research area is actually Earth Science.

357
00:47:43,690 --> 00:47:52,599
So it's pretty funny. I think most of the statistics or the most of statisticians actually work from a pretty specific domain.

358
00:47:52,600 --> 00:47:55,680
Science is very different than today's.

359
00:47:58,330 --> 00:48:03,640
The training more like math rather than science.

360
00:48:05,170 --> 00:48:11,200
Okay. Anyway, so we what we're trying to talk about is the right feature on the right side.

361
00:48:12,050 --> 00:48:18,790
I mean, this is just operations, which is.

362
00:48:20,560 --> 00:48:30,700
So Ray Fisher Modell is one of the early accountant trying to understand diversity of human genetics.

363
00:48:31,400 --> 00:48:39,640
Right. It's not necessary human genetics, actually. There's just the diversity of organisms, so minded inheritance.

364
00:48:39,850 --> 00:48:49,420
And so we all inherit copies of DNA from our parents and the passage down to the next generation.

365
00:48:51,010 --> 00:49:01,360
Here, the randomness. There's a couple of things in science, in genetics, in terms of the randomness of this this material inheritance.

366
00:49:01,360 --> 00:49:04,870
Right. And which copy of a Leo you're going to pass on.

367
00:49:05,230 --> 00:49:12,400
The other thing is so this is called a recombination event where you have an

368
00:49:12,400 --> 00:49:18,220
egg and then the sperm they get together and then there is a meiosis going on.

369
00:49:18,280 --> 00:49:22,900
So while I'm not trying to tell you too much about the genetics,

370
00:49:23,200 --> 00:49:30,310
but the process will randomly pick so called a copy of DNA either from your father or mother.

371
00:49:30,970 --> 00:49:35,140
Pass on to you at a particular location, at a location.

372
00:49:36,070 --> 00:49:42,490
So that's one type of randomness. It's determined by the Mendel's law.

373
00:49:42,970 --> 00:49:54,010
The other randomness is called a mutation. So this we usually see what's called germline mutation, or so cancer is kind of a mutation, right?

374
00:49:54,010 --> 00:49:58,780
So the DNA mutated into a different copy and cause all kinds of issues.

375
00:50:00,010 --> 00:50:08,800
So there are multiple forces can change the the randomness of the DNA material from one generation to the other.

376
00:50:09,250 --> 00:50:21,580
And then this is a time trying to understand the so called random making behavior and how they create this randomness in a genetic pool.

377
00:50:23,260 --> 00:50:28,390
So they set up a mathematical model. It's actually a pretty simple one if you're see.

378
00:50:29,260 --> 00:50:34,960
But that does explain a lot of phenomenons we observe in in genetics.

379
00:50:35,530 --> 00:50:43,270
So the right fish are most always trying to you just call it constant population model so they can see they're a population.

380
00:50:47,120 --> 00:50:54,050
Just carry one single genetic marker. Or you can just say we focus on just that one single genetic moniker.

381
00:50:54,590 --> 00:51:02,060
A population of constant size.

382
00:51:04,920 --> 00:51:11,910
Of the youths. So a video is a target in genetics.

383
00:51:12,120 --> 00:51:17,790
It's just basically means that the nature of the copy of genetic marker you're interested in.

384
00:51:18,300 --> 00:51:28,110
So usually we can see there is a marker of two different versions either the small A or the castaway and a loss of.

385
00:51:30,020 --> 00:51:38,460
So basically just two states, right. Either small A or in California.

386
00:51:40,430 --> 00:51:49,490
So they are and call these individuals. Each of the individual carry a version of either small Leo or kept to a.

387
00:51:55,390 --> 00:52:01,990
Okay. And so this is the problem we're interested in.

388
00:52:02,560 --> 00:52:17,080
So we cannot define and. Most number of the alien capital field illegals.

389
00:52:24,010 --> 00:52:34,330
In the end of the generation. So we have a.

390
00:52:35,380 --> 00:52:40,580
So thinking about the the population going through generations.

391
00:52:40,600 --> 00:52:43,870
So you have first generation, second generation, third generation.

392
00:52:44,380 --> 00:52:48,730
Each of that generation will talk about how they obtained their values.

393
00:52:49,150 --> 00:52:58,690
But in each generation, we kind of keep track the number of the total number of the capital in the population.

394
00:52:59,220 --> 00:53:02,860
Okay. So, so this is what we are interested in.

395
00:53:07,150 --> 00:53:15,160
So we need to specify how the Leo's transmit or inherited from the previous generation.

396
00:53:15,520 --> 00:53:30,070
So this is called the inheritance model. All right.

397
00:53:31,720 --> 00:53:43,930
So this isn't really trying to get into the randomness and try to describe some of the randomness happens in genetics.

398
00:53:46,810 --> 00:53:58,480
All right. First time for the. So think about this as a kind of a simulation process, a procedure that you should say.

399
00:53:59,140 --> 00:54:12,550
So you can start the simulation with generation one or generation zero, which you have a bunch of money on the wall and then a bunch of capital.

400
00:54:16,150 --> 00:54:20,980
All right. So this is a generation one. We know this is in the population.

401
00:54:21,730 --> 00:54:28,720
All right. So we're going to define the inheritance model, just like, you know, follow basically follow Mendel's law.

402
00:54:29,170 --> 00:54:32,170
However, we have some constraints. So.

403
00:54:32,170 --> 00:54:35,800
So this is the generation one. We want generation two.

404
00:54:36,190 --> 00:54:41,800
And then the generation two is actually we want.

405
00:54:43,350 --> 00:54:47,870
Just want to make sure I. Okay.

406
00:54:47,990 --> 00:54:55,160
So the generation two has a kind of strain, has had the same number of individuals as generation one.

407
00:54:55,160 --> 00:55:00,770
So we need to produce and copies of these allows for generation two.

408
00:55:01,640 --> 00:55:14,180
And on the way the model this Mendelian transmission is using a sample with replacement kind of a scheme.

409
00:55:14,540 --> 00:55:21,920
So every time you just randomly sample from the previous generation and then pass it to the next generation.

410
00:55:22,310 --> 00:55:29,060
So I would say this is a pretty close to how it Leo's passing from one generation to the other.

411
00:55:29,570 --> 00:55:37,160
Right so there are different views on this as actually leads to later on in the license process.

412
00:55:37,490 --> 00:55:41,120
But the basic randomness is the same. So what do you do?

413
00:55:41,120 --> 00:55:50,300
So you just need to draw and copies from this of your pool and then pass it down to the next generation as to the first store.

414
00:55:50,330 --> 00:55:58,549
And for example, if I can draw the first one for the first individual I draw correctly just drawing it from the next one,

415
00:55:58,550 --> 00:56:01,970
I might get a small a template type doing it.

416
00:56:02,330 --> 00:56:06,470
So how could you describe this mathematically?

417
00:56:06,710 --> 00:56:10,730
So this is a kind of a binomial sampling problem, right?

418
00:56:11,210 --> 00:56:19,070
It's just simple with replacement. You never you know, you take it, you don't never actually take things away.

419
00:56:19,070 --> 00:56:24,410
You just make a call. You point to a particularly on that, make a copy of that.

420
00:56:25,250 --> 00:56:27,410
So this is what happens in genetics.

421
00:56:27,980 --> 00:56:45,190
So if you understand this process, like my verbal description of this, then you can say so you can write the I can write.

422
00:56:45,230 --> 00:57:00,800
So this is the probability. And then this is the same sort of that dynamic from generation to generation X plus one equal to J given x equal to I.

423
00:57:01,250 --> 00:57:02,240
So what that means?

424
00:57:02,540 --> 00:57:19,850
So if the other generation has a copy of capital, and what is the probability the plus one generation has jay copy of capital and B Right.

425
00:57:20,150 --> 00:57:21,020
So what do you do?

426
00:57:21,410 --> 00:57:31,670
This is actually pretty simple based on what I described, because, you know, the number of you can view the capital, Amy know, is kind of a success.

427
00:57:32,270 --> 00:57:42,200
You're just flip the coins and times and then you get Jay copy or that means Jay numbered jay numbers of successes.

428
00:57:42,800 --> 00:57:48,200
Okay. And what is the frequency you get from the previous generation?

429
00:57:49,850 --> 00:57:55,100
It must be so. So first of all, you need to have a choose.

430
00:57:55,100 --> 00:57:57,830
JAY So that's the number of success you have.

431
00:57:59,900 --> 00:58:05,810
And I want you to consider what this a probability to actually gather a little from the previous generation.

432
00:58:06,410 --> 00:58:09,590
I think you should agree with me. This is I over.

433
00:58:09,590 --> 00:58:14,690
And so that's basically the frequency of the capital elite from the pool.

434
00:58:15,530 --> 00:58:19,700
From the pool, the generation. So that has to be.

435
00:58:19,700 --> 00:58:26,600
JAY And that multiplied by one a minus nine.

436
00:58:28,430 --> 00:58:41,030
Yeah, it's a binomial sampling problem, that's all. So it's a pretty simple not say no more.

437
00:58:42,470 --> 00:58:49,470
It's it's kind of a, you know, simple mathematical model, right?

438
00:58:49,490 --> 00:58:56,330
It's from generation to generation is just binomial sampling nothing more.

439
00:58:56,390 --> 00:59:05,750
But that does characterize the nature of randomness of Mendelian you Harrington's right so what you can think

440
00:59:05,750 --> 00:59:13,600
about this call P so the different viewpoint so this is the viewpoint that from the generational perspective,

441
00:59:13,610 --> 00:59:19,310
you put your focus on the number of copies from the other perspective from individual.

442
00:59:19,700 --> 00:59:26,810
If you look at this, then, you know, the, the small and the first generation has.

443
00:59:27,950 --> 00:59:33,950
So you can actually calculate the probability how many individuals this particular radio generates.

444
00:59:34,490 --> 00:59:40,490
You only generates a small feel. And then that dynamic is pretty simple as well.

445
00:59:41,420 --> 00:59:46,760
And then you ask what the secondly what the three. So if you track these.

446
00:59:47,090 --> 00:59:53,140
They were get some sort of a tree structure that's related to the, quote, licensed structure.

447
00:59:53,150 --> 00:59:59,330
We're not talking about this, but this. This particular perspective is extremely straightforward.

448
00:59:59,660 --> 01:00:05,990
And then that is the right fissure module. So what the Fisher model tells us.

449
01:00:06,500 --> 01:00:12,150
So if you have something like this, first of all, this is a markov chain, right?

450
01:00:12,200 --> 01:00:24,380
So you generate that. And plus, one generation only depends on the the random variable in the previous generation X and nothing more.

451
01:00:25,790 --> 01:00:34,910
So first of all, if you can write this transition kernel, this is a variety of way the transition kernel, if you are halfway done, this is a.

452
01:00:36,230 --> 01:00:51,200
Markov chain situation. Based on this, we can tell there are two absorbing statements.

453
01:00:53,410 --> 01:00:58,820
Can you see that? Two absorbing states? For Exxon here.

454
01:00:59,720 --> 01:01:07,710
So think about Exxon. Exxon take all the values from either zero to end and the population size remains constant.

455
01:01:07,730 --> 01:01:12,640
So what are the two absorbing states? Sorry.

456
01:01:13,460 --> 01:01:18,660
I think you're mostly right. Just speak up. Very good.

457
01:01:20,250 --> 01:01:25,860
So if you have a zero colonies. This whole population, war and peace.

458
01:01:25,890 --> 01:01:34,140
This zero call. This means the population is homogeneously old and skeptical of all smart young people.

459
01:01:34,500 --> 01:01:43,550
And then the if you have capital, while every other accent you go to and that means this is a it's not just dominating this,

460
01:01:43,560 --> 01:01:49,290
it's completely over the capital.

461
01:01:49,890 --> 01:01:55,290
Then there is no variability based on that sampling scheme, based on this transition, Colonel.

462
01:01:55,710 --> 01:02:05,220
Right. So every time you do all this, I divided by and will be either zero or one.

463
01:02:05,310 --> 01:02:09,570
So there is no variability. So there are two absorbing states.

464
01:02:10,470 --> 01:02:15,600
And because there are two absorbing states and there is a markov chain.

465
01:02:15,750 --> 01:02:26,790
Now you can ask the question if this is the only dynamic in the population to think about this population genetics problem.

466
01:02:27,930 --> 01:02:36,540
Then you see the number of capital elite all kind of varies from generation to generation.

467
01:02:36,960 --> 01:02:44,070
But just like gamblers rule problems, eventually they make hits zero states or island states.

468
01:02:44,520 --> 01:02:48,690
So this behavior in population genetics are called a genetic drift.

469
01:02:49,740 --> 01:02:59,250
So this drift is mathematically is describe the war that's determined by this binomial sampling formula.

470
01:03:01,020 --> 01:03:08,040
Because you have a markov chain, you can either simulate that genetic drift.

471
01:03:08,550 --> 01:03:16,040
Right. So that's pretty simple. Or you can ask the more profound question, like, can you expect the whole line?

472
01:03:16,050 --> 01:03:22,920
If it is a genetic drift, then you can ask the question, is the drift eventually going to get into the absorbing states?

473
01:03:23,450 --> 01:03:26,910
Right. So can is that possible that you never.

474
01:03:26,940 --> 01:03:33,690
So this is a this is the like the stochastic process problem we encountered in gamblers

475
01:03:33,690 --> 01:03:39,840
or is there is a possibility that legal will never reach the absorbing state,

476
01:03:39,990 --> 01:03:44,340
but is the probability is not zero?

477
01:03:44,820 --> 01:03:48,750
Right. So we should be able to answer this question in a few weeks.

478
01:03:49,240 --> 01:03:53,370
Okay. Time. Do we have a reset? Okay. We have more topics.

479
01:03:55,950 --> 01:04:02,819
So. So this is.

480
01:04:02,820 --> 01:04:06,150
Describe the. The genetic drift.

481
01:04:07,500 --> 01:04:16,650
Um. Okay.

482
01:04:17,250 --> 01:04:20,549
Um. So that's important. So. So.

483
01:04:20,550 --> 01:04:27,180
Modern day population genetics. Um, I would say, you know, it's all the.

484
01:04:27,180 --> 01:04:31,940
The. Over the Maldives population genetics, Maldives after this.

485
01:04:32,360 --> 01:04:37,940
There is no significant difference than this basic assumption, but they can relax on a few.

486
01:04:42,120 --> 01:04:49,380
Requirement in, for example, the constant population assumption often relaxed.

487
01:04:51,300 --> 01:04:54,810
But that's not a you should steal Markov chain, you're going to change.

488
01:04:54,810 --> 01:05:00,750
But in here. Right. So the only error is that the population size from the previous population, that's the pool.

489
01:05:01,290 --> 01:05:08,490
If you change the population size constraint, it's still Markov chain.

490
01:05:09,900 --> 01:05:21,710
But what change the and it's going to depends on the population size the anchor point of that the generation that you have right.

491
01:05:21,720 --> 01:05:24,810
So you can see there is some sort of exponential.

492
01:05:26,520 --> 01:05:32,640
So the my point is it's still a markov chain, but not going to be a time homogeneous Markov chain.

493
01:05:32,730 --> 01:05:39,810
The transition kernel are going to be time dependent because the population size is time dependent if you don't have that constraint.

494
01:05:39,990 --> 01:05:49,380
Okay. So that's one of the relaxation. The other relaxation is introduce more randomness into this process.

495
01:05:50,350 --> 01:05:53,640
Uncle Paul is buying oatmeal sample.

496
01:05:53,920 --> 01:06:01,780
So this is. I'm trying to mimic Manderley. An inheritance, so we could actually talk about this today.

497
01:06:03,790 --> 01:06:07,260
It's just more like, how do you make your multiple?

498
01:06:09,600 --> 01:06:16,620
I introduce other things. So the other things that we're trying to mimic in this mathematic model of this is mutations.

499
01:06:17,430 --> 01:06:20,740
So what is a mutation? It's just basically DNA change.

500
01:06:20,760 --> 01:06:29,380
So we all know that DNA has four types of base pairs, a tag.

501
01:06:29,550 --> 01:06:35,460
So the mutation can happen if a change to a TIA or vice versa.

502
01:06:36,720 --> 01:06:41,190
So this type of the mutation, a mutation will prevent this.

503
01:06:41,640 --> 01:06:46,209
Well, from mathematical terms, it's just the way you think about it.

504
01:06:46,210 --> 01:06:51,390
If you introduce a mutation into this is you may be trying to copy something,

505
01:06:51,390 --> 01:06:59,310
but doing the copying process, you can introduce this mutation process as well.

506
01:06:59,430 --> 01:07:13,620
Right. So let me just say, if I'm right, fish are all way mutation.

507
01:07:17,940 --> 01:07:27,960
So in this one, we don't have for copy software, you only have two on either capital 80 or a x small or capital here.

508
01:07:29,820 --> 01:07:40,140
So how do we add two on top of this? So we still keep the copying process intact and we still do some whole with replacement.

509
01:07:41,670 --> 01:07:41,920
Okay.

510
01:07:42,450 --> 01:07:53,250
So if we do, sample is replacement, but when you copy a little, you don't directly say the second generation, the next generation, get the exact copy.

511
01:07:53,610 --> 01:07:59,610
You introduce another coin flip to determine if you're going to keep this audio or mutate this.

512
01:07:59,610 --> 01:08:03,530
Are you? Right on top of the coffee processing.

513
01:08:03,690 --> 01:08:09,510
And I am trying to make the mutation process independent of this copying process.

514
01:08:10,590 --> 01:08:18,250
Right. So that's actually pretty simple. We just introduced.

515
01:08:32,730 --> 01:08:41,240
And in early. How?

516
01:08:43,270 --> 01:08:52,060
I was Haitian. That's all right.

517
01:08:52,180 --> 01:08:57,100
So consider you have copied of the old.

518
01:08:58,690 --> 01:09:05,740
So this is mutations that can change it to a cat of the old with probability.

519
01:09:09,420 --> 01:09:14,770
I said you could read and similarity.

520
01:09:14,770 --> 01:09:24,520
You can be fine. But they need the capital aid to somewhat deal with probability because of the.

521
01:09:25,510 --> 01:09:29,500
So we immune doesn't need to be the same.

522
01:09:29,680 --> 01:09:34,870
I know there is no constraint with this. Right. So they don't have to add up to one or anything like.

523
01:09:35,110 --> 01:09:40,390
It's just some or just observation in the in the context somewhat.

524
01:09:40,630 --> 01:09:44,200
Some of the video tend to mutate more than the other.

525
01:09:45,190 --> 01:09:54,940
Okay. So if you introduce this additional on the stand, then how do you calculate your.

526
01:09:58,090 --> 01:10:05,330
And how do you calculate the the probability of you and the the transition probability think.

527
01:10:10,020 --> 01:10:13,650
So the way to think about this is more mathematical.

528
01:10:13,680 --> 01:10:23,010
Right. So what you want to do is actually kind of a bundle that is copying process into the copying process.

529
01:10:23,340 --> 01:10:31,650
All right. So we're still going to say the. We still have the concept of population probability.

530
01:10:33,500 --> 01:10:43,430
So the reason I have j colonies in plus one generation is because I successively get j copies of capital and people.

531
01:10:43,820 --> 01:10:48,310
But now more careful about this probability of getting.

532
01:10:50,300 --> 01:10:58,850
Now we call this a zero year road to the J one -0 to the J is the probability sorry of the mind.

533
01:11:01,940 --> 01:11:13,400
So what is the row here? So the low here is what is the probability you get a capital angel from the previous generation?

534
01:11:14,370 --> 01:11:17,850
During this coping and rotation process.

535
01:11:17,970 --> 01:11:24,800
So you need to consider all. So that makes the problem easier.

536
01:11:24,850 --> 01:11:34,450
So they are still binomial some point except for the success.

537
01:11:34,450 --> 01:11:40,860
Probability is no longer just depends on the the call copy you can call size.

538
01:11:41,920 --> 01:12:02,250
So the row here is the probability of getting a of the conditional x and we call it to j.

539
01:12:02,320 --> 01:12:09,899
Right. You go to?

540
01:12:09,900 --> 01:12:13,340
I'm sorry. I was using the wrong for my friend.

541
01:12:14,160 --> 01:12:21,030
All right, how do we get this? So, first of all, you know, there are two different ways you can get a capital on the go.

542
01:12:21,810 --> 01:12:26,770
First, you actually sampled an alien.

543
01:12:27,180 --> 01:12:31,310
So that is. And then that alien doesn't mutate. Right.

544
01:12:31,380 --> 01:12:35,410
So that's probability we can say, is I divided by one?

545
01:12:36,450 --> 01:12:40,110
And then there is no notation from capital 80 to small.

546
01:12:41,250 --> 01:12:44,910
So that's one minus, you know, limitations.

547
01:12:46,560 --> 01:13:03,210
Yeah. So this is the first way. The second way it's a completely mutually exclusive way is you actually get a small but a mutated back to the way.

548
01:13:03,300 --> 01:13:07,150
So that would be. All right.

549
01:13:08,580 --> 01:13:16,770
I know that's a roll. That's pretty much it. There is no other way you can get a deal, cut a deal from the previous generation.

550
01:13:19,340 --> 01:13:25,450
This is? Well, I think you can still see that.

551
01:13:25,460 --> 01:13:34,790
Obviously, there's still a markov chain. The the transition probability, the J is still depends on only I.

552
01:13:35,060 --> 01:13:39,230
But there are two additional parameters and we'll be sorry.

553
01:13:39,710 --> 01:13:44,420
Yes. One, we are introducing this. So these are mutually.

554
01:13:44,600 --> 01:13:48,910
Well, gave you additional variability from mutation.

555
01:13:49,760 --> 01:13:55,420
And then that changed that Markov chain, the structural change significantly.

556
01:13:56,590 --> 01:13:59,620
All right. Let me ask you, do we still have absorbing state?

557
01:14:01,820 --> 01:14:07,230
Yes. What are the absorbing state which isn't zero and right.

558
01:14:07,310 --> 01:14:11,840
But I'm arguing there are no longer absorbing state. We have an equilibrium now.

559
01:14:11,840 --> 01:14:17,360
Possibly that's different. Equilibrium is not the same as absorbing states.

560
01:14:17,360 --> 01:14:20,690
Absorbing state is always probability one that one not ever.

561
01:14:21,080 --> 01:14:32,270
That's very good. So. So there is no more absorbing state because easily you can see even you are dominant by one deal.

562
01:14:33,230 --> 01:14:40,639
You can still have a different aleo in the next generation because of this mutation

563
01:14:40,640 --> 01:14:46,760
process will reintroduce the different nodule back into the population pool.

564
01:14:47,240 --> 01:14:56,120
So in mathematical terms, you don't have the p00 equal to one anymore.

565
01:14:56,210 --> 01:14:59,690
So you could. You're welcome to try. This is not going to be the case.

566
01:15:00,020 --> 01:15:03,050
Even this one is zero IQ.

567
01:15:03,550 --> 01:15:06,770
This is zero. This one. It's not a zero. Right.

568
01:15:06,800 --> 01:15:17,140
If I equal to zero and then you want to calculate the p i j equal to zero, then this is not going to be the case outcomes.

569
01:15:17,840 --> 01:15:24,620
So this is a no. Okay. Andrew J j equal to zero and then row here.

570
01:15:24,980 --> 01:15:34,430
It's none zero. Right. So you're going to introduce we will introduce the the back.

571
01:15:35,270 --> 01:15:39,380
Right. Very good. So there may be a equilibrium state.

572
01:15:39,440 --> 01:15:43,350
So we're going to talk about what that means later on in the in the lecture.

573
01:15:43,370 --> 01:15:48,140
So now you have a markov chain. If you run long enough, what going to happen?

574
01:15:48,800 --> 01:16:00,500
Right. So what happens is you can see the frequency of, you know, after certain generations of the Markov train run long enough going to stabilize.

575
01:16:01,180 --> 01:16:05,360
Right. So that's the equilibrium state. But that's not absorbing the state.

576
01:16:05,720 --> 01:16:10,220
Okay. Very different actually, in terms of Markov chain.

577
01:16:10,430 --> 01:16:16,680
So drastically different. So the two things are basically mutually exclusive.

578
01:16:16,700 --> 01:16:21,860
If you have a swarming state, you know, I'm talking about we cannot talk about equilibrium state.

579
01:16:21,920 --> 01:16:25,960
Okay. But that's down the road. Okay.

580
01:16:26,030 --> 01:16:34,510
We're at the time. So next time. So those are the basic things I expect you can do now.

581
01:16:34,550 --> 01:16:43,850
So really, your basic description of the process, we should be able to show that process is a mark of process or not.

582
01:16:44,720 --> 01:16:51,590
You should be able to write the transition. So next time we start considering this problem is the step.

583
01:16:51,800 --> 01:16:54,320
So so far we just talk about transition kernel.

584
01:16:54,320 --> 01:17:01,790
Once that transition kernel is not sufficient for us to understand the food that now make up the stochastic process,

585
01:17:01,790 --> 01:17:09,260
for example, from whence that transition kernel. And we get to arbitrary understand transition kernel before we get a stationary

586
01:17:10,020 --> 01:17:15,500
of the marginal distribution for and the old system number rhythm variables.

587
01:17:16,970 --> 01:17:22,280
But that's the next Monday. Okay, I'll see everybody.

588
01:17:22,790 --> 01:17:29,360
That's what expected the quiz. That's a small yes, but yes please do that.

589
01:17:30,860 --> 01:17:33,500
Right. Don't do that. Don't give me the physical.

