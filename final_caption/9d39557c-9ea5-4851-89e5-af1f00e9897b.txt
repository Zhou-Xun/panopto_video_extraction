1
00:00:08,820 --> 00:00:15,330
During election years, you may find your news feeds full of polls about who's going to win the election.

2
00:00:15,870 --> 00:00:23,159
Sometimes you'll see that those polls are correct. That being accuracy, sometimes they're really not right.

3
00:00:23,160 --> 00:00:30,330
And you get very misled. Other times you see that there's 12 polls out there and they each say different things.

4
00:00:31,110 --> 00:00:36,540
In this particular video, we're going to talk through the concepts of both accuracy and precision.

5
00:00:36,960 --> 00:00:43,200
Do you get the answer right? And if you ask the question many times, are you getting the similar answer each time?

6
00:00:44,500 --> 00:00:50,080
We'll talk about the definitions of those different measures and look at some examples of calculating them.

7
00:00:50,860 --> 00:00:57,550
Finally, we'll also talk about the usefulness of looking at consistency across different measures and epidemiologic studies.

8
00:00:59,240 --> 00:01:05,060
So when thinking about accuracy and precision, I often find it helpful to imagine a game of darts.

9
00:01:05,600 --> 00:01:10,370
Now, as you guys mostly know, right, you're taking your darts and you're aiming at the bull's eye in the middle.

10
00:01:11,290 --> 00:01:17,490
Now. If I was an amazing dart player, my dartboard would likely look like that on the left.

11
00:01:17,500 --> 00:01:22,420
I would be very accurate. Always getting at the bull's eye and very precise.

12
00:01:22,420 --> 00:01:30,220
Like every time I threw it, I was hitting the center. In reality, my particular dart board most likely looks like the one you see in the center,

13
00:01:30,550 --> 00:01:38,440
where on average I'm getting at the center, but my each throw is so noisy that it's really scattered around the whole board.

14
00:01:39,190 --> 00:01:42,790
And finally, there are some people who may be very precise.

15
00:01:42,790 --> 00:01:48,040
They can always hit the same place, but unfortunately that place is never the center of the bull's eye.

16
00:01:49,820 --> 00:01:58,910
Now, as you can imagine, in order to have good data for an epidemiologic study, you really want your data to both be accurate and precise.

17
00:02:00,750 --> 00:02:04,830
So let's start by first talking about the concept of accuracy,

18
00:02:05,280 --> 00:02:11,460
and we'll do that within the context of classifying an exposure or an outcome as a binary measure.

19
00:02:12,000 --> 00:02:17,370
So in other words, we're asking the question who has that exposure outcome and who does not?

20
00:02:18,320 --> 00:02:26,780
Now in epidemiology, the two measures that we use to assess how accurate our data are when we're using a binary measure are sensitivity,

21
00:02:27,140 --> 00:02:33,800
which asks the question How good is my test at correctly identifying people with a disease or an exposure?

22
00:02:34,490 --> 00:02:42,050
And specificity, which is how good we are at correctly identifying those without a disease or without an exposure.

23
00:02:43,670 --> 00:02:49,100
In order to estimate the sensitivity and specificity of some sort of test or screening measure.

24
00:02:49,400 --> 00:02:54,229
We often use a gold standard that's known to measure something without error,

25
00:02:54,230 --> 00:02:59,330
to tell us if our measurement here, I'm calling it a screening test, is getting the right answer.

26
00:03:00,460 --> 00:03:09,370
By comparing against the gold standard, we can cross tabulate our data into a two by two table screening test versus gold standard.

27
00:03:10,150 --> 00:03:13,840
Now, this breakdown here is showing us that in cell A,

28
00:03:13,840 --> 00:03:22,030
we have our true positives where a screening test is accurately capturing people who truly have either the exposure or the outcome.

29
00:03:22,630 --> 00:03:28,930
Similarly, in Cell D, I just wanted to call out that this is the true negative cell in that situation.

30
00:03:28,930 --> 00:03:36,970
Our screening test is accurately identifying those people who are negative by the gold standard, either for their exposure or their outcomes.

31
00:03:38,710 --> 00:03:45,970
Now using the data and the comparison between a gold standard and our test, we can estimate sensitivity,

32
00:03:46,300 --> 00:03:52,070
which you likely recall is the number of true positive samples that we correctly identify as positive.

33
00:03:52,510 --> 00:03:58,600
Divided by the total true positive samples set another way sensitivity is the

34
00:03:58,600 --> 00:04:03,610
proportion of truly positive samples that we correctly identify as positive.

35
00:04:05,040 --> 00:04:13,709
Similarly, you may recall that specificity is defined as the number of negative samples correctly identified as negative divided by the total

36
00:04:13,710 --> 00:04:23,180
number of true negative samples or specificity reflects the proportion of truly negative samples that we correctly identify as negative.

37
00:04:24,910 --> 00:04:33,430
Sensitivity and specificity are very useful for categorical data, but much of the data that we use in epidemiology is actually continuous.

38
00:04:34,000 --> 00:04:36,580
So what do we use in those particular situations?

39
00:04:37,360 --> 00:04:43,420
Well, similar to the past slides, what we really want is some sort of gold standard for continuous data.

40
00:04:44,020 --> 00:04:51,370
And if we have that, then what we can do is we can make a scatterplot of our measurements versus the gold standard measurements.

41
00:04:52,680 --> 00:05:02,040
Often in the literature, people report correlations between the gold standard and the measurement in order to assess accuracy there.

42
00:05:02,060 --> 00:05:09,870
The idea is, is that if the screening tool is very useful, then it should increase or decrease in perfect harmony with the gold standard.

43
00:05:11,080 --> 00:05:18,580
Now, as you can see in this particular situation, the observed data is very strongly correlated with the gold standard.

44
00:05:19,090 --> 00:05:22,960
However, there is an offset of 4.4.

45
00:05:23,680 --> 00:05:31,870
What that means is that our observed value is always reading 4.4 points lower than what the gold standard is reading.

46
00:05:32,820 --> 00:05:36,250
Similarly, we observe a slope of the line.

47
00:05:36,270 --> 00:05:39,750
In other words, it's not on the 1 to 1 line that's there in a dash.

48
00:05:40,470 --> 00:05:49,320
What we see is that for every unit change in our observed data, there is actually a 1.2 unit change in the standard.

49
00:05:50,310 --> 00:05:54,420
Now, while the strong correlation suggests that our measurements will be useful,

50
00:05:54,810 --> 00:05:59,550
we also can see that there's a need for calibration or adjustment of our measure

51
00:05:59,790 --> 00:06:04,620
if we really want to get truly accurate estimates of our exposure or outcome.

52
00:06:06,110 --> 00:06:13,450
Now let's start thinking about estimates of precision again, starting first with a binary measures of either yes or no.

53
00:06:14,360 --> 00:06:19,280
Here. I'm showing you a classic example from the literature where they had two different

54
00:06:19,280 --> 00:06:25,250
examiners looking at whether or not the same children had dental cavities or caries.

55
00:06:25,790 --> 00:06:34,790
And the purpose of this testing was to figure out how precise or repeatable or reproducible a test was between different scorers.

56
00:06:35,510 --> 00:06:42,590
Now, the table shown here has the status of carries either present or absent by the first examiner on the columns.

57
00:06:43,040 --> 00:06:49,670
And that's been crossed tabulated against Kerry's present or not, by the second examiner shown in the rows.

58
00:06:51,000 --> 00:06:59,520
One method of testing how precise or repeatable a test is, is to calculate the percent agreement between the different evaluators.

59
00:07:00,240 --> 00:07:06,660
Effectively, this asks the question of how often repeated classifications result in the same answer.

60
00:07:07,760 --> 00:07:14,780
To estimate percent agreement. You divide the number of times that both examiners agree here for 280 children,

61
00:07:14,780 --> 00:07:19,610
both being scored with cavities and ten children both being scored without cavities,

62
00:07:20,060 --> 00:07:25,250
and divide that total by the total number of children evaluated here being 300.

63
00:07:26,250 --> 00:07:36,110
As a result, you can see that there's very high precision or repeatability in this particular dataset, with a percent agreement of almost 97%.

64
00:07:37,660 --> 00:07:44,590
One limitation in the percent agreement is that some of the agreement we see will happen just due to chance alone.

65
00:07:45,220 --> 00:07:49,630
The cap of statistics is an approach that's very similar to percent agreement.

66
00:07:50,140 --> 00:07:57,670
But instead of just saying, how often are they agreeing, it's actually asking how much is the agreement above and beyond chance alone?

67
00:07:59,020 --> 00:08:05,170
In order to estimate the Kappa statistic, we must first figure out what we would expect to happen by chance alone.

68
00:08:06,300 --> 00:08:15,270
If the information is independent, then the chance that both interviewers would score a child with no carries is the probability of examiners one

69
00:08:15,270 --> 00:08:20,670
scoring someone without Kerry's times the probability of that examiner to score as a person without carries.

70
00:08:21,360 --> 00:08:30,450
Similarly, the likelihood of both examiners scoring a child with carries is the probability of examiners one scoring any child with Kerry's times,

71
00:08:30,450 --> 00:08:33,930
the probability that examiner two scores any child with Kerry's.

72
00:08:34,620 --> 00:08:40,440
Now, each of these calculations is shown at the bottom of the slide, so you can really see where those numbers came from.

73
00:08:42,090 --> 00:08:48,540
By combining information on the observed percent agreement, along with the percent agreement that's due to chance alone,

74
00:08:48,840 --> 00:08:53,040
we can estimate the capacity to stick using the formula at the bottom of this table.

75
00:08:54,060 --> 00:08:59,310
Based on our particular data we see here that our Kappa statistic is 0.66.

76
00:09:00,240 --> 00:09:05,580
Now, for those of you who haven't worked with Kappas before, you might be thinking to yourself, Is that any good?

77
00:09:06,690 --> 00:09:10,530
Well, to some extent it depends on who you ask, because it is subjective.

78
00:09:11,040 --> 00:09:19,440
But here is a list that has a relatively good rule of thumb for you based on this scale with a Kappa statistic of 0.66.

79
00:09:19,740 --> 00:09:22,920
Our test for caries was actually considered to be quite good.

80
00:09:24,550 --> 00:09:30,910
So now that we've seen how to estimate precision for categorical data, what is the parallel in continuous measures?

81
00:09:31,750 --> 00:09:35,260
Well, one measure that's commonly used is the correlation statistic.

82
00:09:35,800 --> 00:09:40,000
This again shows us how tightly to measures are tracking with one another.

83
00:09:40,750 --> 00:09:45,850
Here you can see that we have extremely limited scatter in our data and a very high

84
00:09:45,850 --> 00:09:50,830
correlation coefficient suggesting high precision or repeatability of our measure.

85
00:09:51,920 --> 00:09:59,180
Another common way of looking at the precision of continuous data, our notions of percent error or coefficient of variations,

86
00:09:59,780 --> 00:10:07,430
both of which are trying to get at how different repeated measures are on average as compared to the mean value of the measure.

87
00:10:07,970 --> 00:10:13,010
In other words, what fraction of the value seen is likely due to error alone?

88
00:10:14,570 --> 00:10:16,760
Finally, before we wrap up this video,

89
00:10:16,760 --> 00:10:24,530
I just wanted to touch on the notion of consistency in the situation where there are no gold standards and our tests perform in perfectly.

90
00:10:24,980 --> 00:10:30,620
One approach is to use multiple measures of the same construct, be it an exposure or an outcome,

91
00:10:30,950 --> 00:10:34,760
in order to get a better estimate than our single measure alone.

92
00:10:35,540 --> 00:10:40,670
So let's go back, for example, to that scenario of trying to assess Alzheimer's disease.

93
00:10:41,330 --> 00:10:51,110
In that situation where there is no gold standard, we might use multiple tests of mental performance on memory and language and social ability,

94
00:10:52,010 --> 00:10:57,410
each of which has our own limitations in order to get a stronger overall measure of health.

95
00:10:58,810 --> 00:11:04,060
So in conclusion, we talked about two different concepts, one being accuracy and the other precision.

96
00:11:04,600 --> 00:11:10,510
Accuracy is the ability of a measure to distinguish who has a disease or an exposure and who does not.

97
00:11:11,200 --> 00:11:19,450
We talked about the use of sensitivity and specificity for categorical data and interception slopes or correlations for continuous data.

98
00:11:20,650 --> 00:11:24,700
While precision is how reproducible or repeatable measure is.

99
00:11:25,150 --> 00:11:32,379
And for that we used percent agreement and Kappa statistics for categorical data as well as correlations,

100
00:11:32,380 --> 00:11:35,680
percent error and coefficients of variation for continuous data.

101
00:11:37,010 --> 00:11:40,940
Now that we understand a little bit better as to how to assess the quality of our data,

102
00:11:41,240 --> 00:11:47,030
we're going to talk about how errors in our measure can impact our epidemiologic findings in the next videos.

