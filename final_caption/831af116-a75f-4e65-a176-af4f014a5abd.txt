1
00:00:00,180 --> 00:00:08,280
Not one word for my friend. So I'm going to look at that.

2
00:00:08,310 --> 00:00:12,050
I did. All right. Dr. Johnson said he had ten three, four, five, six, eight.

3
00:00:12,540 --> 00:00:17,660
All right. The Beatles. Almost 20 people. You don't look good.

4
00:00:18,030 --> 00:00:25,470
All right. As I told you guys before, I had a faculty meeting that I have to be at 430.

5
00:00:26,010 --> 00:00:33,870
So when I am class, I'm going to disappear very quickly. So I can't answer questions after I talk with you today.

6
00:00:34,260 --> 00:00:40,020
I apologize. It's been an office hours or emails if you have more questions.

7
00:00:40,800 --> 00:00:45,300
So I got about 12 review questions for the exam.

8
00:00:45,300 --> 00:00:54,570
So that's going to take the entirety of today, I think. And then we'll regroup in two weeks for the rest of the semester and finish up.

9
00:00:56,310 --> 00:01:00,870
According to Junko Wu, they will not do James until after the break.

10
00:01:01,890 --> 00:01:05,610
So you're ahead of them there, but they are covering something right now that we're not covering.

11
00:01:06,000 --> 00:01:13,920
And we'll cover something that they're not covering. So anyway, let's get started.

12
00:01:14,230 --> 00:01:18,090
They said, I got to get out of here and cover everything.

13
00:01:18,180 --> 00:01:22,589
So Elms, let her mix models that generalize estimating equations.

14
00:01:22,590 --> 00:01:27,840
That's what some of the test and the concepts that apply to those two ideas.

15
00:01:27,840 --> 00:01:32,520
And based upon the things you guys still want me to talk about. Here we go.

16
00:01:33,870 --> 00:01:53,580
So this is going to be all handwriting today. So I'll do my best to go slowly and coherently so that we can go through everything with some factor.

17
00:01:53,610 --> 00:02:01,139
That's a linear, mixed model.

18
00:02:01,140 --> 00:02:05,910
We have normal outcomes. Everything is normal here. The random effects are normal, the errors are normal.

19
00:02:06,300 --> 00:02:21,300
So that way is normal. And if you don't like all this notation, memorize the refitting models that look like an intercept.

20
00:02:23,040 --> 00:02:38,370
Plus a group effect has a time effect if you're treating time as continuous interactions and will go as far as a random intercept in a random.

21
00:02:43,140 --> 00:02:48,450
Right. So the whole premise that we are fitting a linear regression model basically with group time

22
00:02:48,450 --> 00:02:53,280
interaction and then random effects to deal with the repeated measures part of the data.

23
00:02:57,610 --> 00:03:03,010
These two things right here by very normal distribution, which means zero.

24
00:03:04,950 --> 00:03:10,090
Variance covariance matrix D. Now, Dean called.

25
00:03:10,400 --> 00:03:14,690
There's the variance for the random intercepts. There's the variance for the random slopes.

26
00:03:17,420 --> 00:03:21,190
And there is the covariance, if we want that. Data.

27
00:03:21,190 --> 00:03:29,229
01201. And this thing here is normal.

28
00:03:29,230 --> 00:03:33,309
Zero sigma squared. You can put it.

29
00:03:33,310 --> 00:03:43,660
Yeah, it's right. JS independent of being not high would be one I.

30
00:03:45,490 --> 00:03:50,760
So all I am saying.

31
00:03:51,700 --> 00:03:55,060
So errors have nothing to do with the random effects.

32
00:03:55,570 --> 00:03:59,650
We make those independent because that, of course, makes all the calculations a little bit nicer.

33
00:04:00,340 --> 00:04:06,150
And we have no reason to believe why the residual would have any correlation with effects.

34
00:04:11,300 --> 00:04:15,650
And we've got a group indicator. It's a.

35
00:04:18,700 --> 00:04:26,530
The. She was timing.

36
00:04:29,850 --> 00:05:05,179
Of measurement. And most of the times in longitudinal studies that they're designed prospectively,

37
00:05:05,180 --> 00:05:10,670
we usually plan to measure people at the same time, month to month, six months, 12, so forth.

38
00:05:11,360 --> 00:05:16,180
So it could be just the same number for every person as that measurement.

39
00:05:16,520 --> 00:05:30,950
Again, doesn't have to be. So let's start talking about between and within subject variability or in the future.

40
00:05:34,070 --> 00:05:59,620
So for a person. So for a given person, every one of their wages has the same intercept, random intercept in the same random slot.

41
00:06:00,880 --> 00:06:07,900
That's the whole point. That's how we get correlation induced within a person because those values all contain similar or similar quantities.

42
00:06:18,480 --> 00:06:24,360
Yes. But they have different E.J., every every value within a person has this deviation.

43
00:06:30,360 --> 00:06:41,990
So therefore, it is the one I want to go through each.

44
00:06:42,450 --> 00:06:47,570
Am I depending how many measurements there are, which.

45
00:06:50,990 --> 00:07:03,470
Well, I mean, it's easier to quantify communication outcomes.

46
00:07:07,090 --> 00:07:20,280
Within an individual. So sigma squared quantifies how measurements within the same person differ from each other.

47
00:07:21,330 --> 00:07:29,100
So Sigma Square as a measure of within subject variability. And that is why, because everything else is is contained within a person.

48
00:07:30,780 --> 00:07:34,800
Right. So as a sigma squared deals with within subject variability.

49
00:07:37,770 --> 00:07:42,590
But every individual is.

50
00:07:54,670 --> 00:08:18,740
But every person has a different. During an intercept. A random sort. So all these pairs of random quantities.

51
00:08:22,640 --> 00:08:25,730
When I say how comes?

52
00:08:27,380 --> 00:08:32,510
Protection varies between individuals.

53
00:08:36,860 --> 00:08:41,180
Right. And that's this key matrix right here.

54
00:08:41,900 --> 00:08:47,300
So D is quantifying how much people differ from each other in terms of their outcomes.

55
00:08:49,110 --> 00:08:53,330
Right. So sigma squared is the within variability.

56
00:08:53,780 --> 00:08:57,950
D quantifies the variability between individuals.

57
00:09:02,770 --> 00:09:12,520
You. You're going to want to make sure all this is clear because then we're going to apply it to GPA, which looks a little bit different.

58
00:09:13,420 --> 00:09:20,210
So. Put another way. The marginal variance of any observation.

59
00:09:20,220 --> 00:09:24,380
Y. J. Oops.

60
00:09:24,620 --> 00:09:27,740
Where do they mention that? I'm going back to a randomness. That model.

61
00:09:30,780 --> 00:09:32,790
Oh, I'll have to go to that.

62
00:09:43,790 --> 00:09:51,470
So put it another way the total variance of any observation in the study across people over time, any observation in the dataset,

63
00:09:51,770 --> 00:09:57,860
the total variability in the outcomes is sigma not squared plus sigma squared theta not squared plus sigma.

64
00:09:59,930 --> 00:10:03,290
We've taken the total variance and we've partitioned it into two pieces.

65
00:10:03,770 --> 00:10:14,550
The between and that was in. But for a given person's values, if I condition on their random intercepts,

66
00:10:14,910 --> 00:10:19,260
if I know what their random intercept is, it's no longer a random quantity.

67
00:10:20,880 --> 00:10:40,730
This variance is simply sigma squared. So the total variance is across all individuals.

68
00:10:43,810 --> 00:10:57,760
This is within individual. And therefore the leftover part must be between and that is data, not squirt.

69
00:10:58,460 --> 00:11:04,990
Squared. Again, trying to show you different ways of trying to get your head around what the scene variability is and what between variability is.

70
00:11:04,990 --> 00:11:08,770
Yes. So that's in a rhythm intercept model for a variance of why.

71
00:11:09,430 --> 00:11:15,310
If you have a random slope, you would be adding and know one squared and subtracting two times.

72
00:11:15,610 --> 00:11:21,700
So again, a random. Intercept and slope model.

73
00:11:31,550 --> 00:11:39,110
Squared plus donut squared plus two squared theta.

74
00:11:39,110 --> 00:11:49,930
One squared. Now we have a random effect times time so that variance component, the variance is T squared times the variance, right.

75
00:11:54,310 --> 00:12:01,420
You know, just going back to this model right here, the variance of why i j this is all fixed.

76
00:12:01,630 --> 00:12:18,550
Everything is fixed up until here. So the variance of Yha ops and people in a random intercept slope model with theta zero one equals zero.

77
00:12:18,550 --> 00:12:27,220
If the two random components are not correlated and the variance of the sum is simply the sum of the variances,

78
00:12:27,220 --> 00:12:31,960
the variance of this plus t squared times the variance of this plus the variance of that.

79
00:12:33,530 --> 00:12:34,609
If these are correlated,

80
00:12:34,610 --> 00:12:43,130
then you've got two times the covariance on all of those formulas from from again I'm not going to have you do any of this computationally.

81
00:12:43,430 --> 00:12:46,520
I hope you realize that's not on the test. It's good to know.

82
00:12:46,730 --> 00:12:49,910
I want you to know it, but I'm not going to give you a bunch of results and say,

83
00:12:49,910 --> 00:12:55,670
what's the covariance of the first and third observations that's just mean and useless.

84
00:12:55,670 --> 00:12:59,230
And I'm not mean nor useless. Right?

85
00:13:00,050 --> 00:13:04,400
But that's how we get variance. Variance of this thing is just a variance of a sum.

86
00:13:04,550 --> 00:13:08,420
And then some of the variances plus two times covariance is where they exist.

87
00:13:11,040 --> 00:13:15,540
So again, we've now got between subject variability.

88
00:13:16,710 --> 00:13:20,490
That has this constant term not squared plus a component of time.

89
00:13:21,210 --> 00:13:25,800
The variability between subjects is increasing quadratics over time.

90
00:13:26,370 --> 00:13:34,440
That's what a random slope model is doing. There are two components of the between subject variability, the random intercept and the slope.

91
00:13:36,010 --> 00:13:44,220
And so the variability is increasing quite quickly. And this is why things are hard to fit when these ties are, you know, 36 to 120, 240.

92
00:13:44,670 --> 00:13:47,129
You got to square that and then multiply it by something.

93
00:13:47,130 --> 00:13:53,820
You're getting huge variances unless you scale the time variable, too, to not have things fill up so much.

94
00:13:53,880 --> 00:13:57,300
All right. So that's how that all works.

95
00:13:59,820 --> 00:14:08,610
So the circled part is between the sigma squared is the within component and the total is the total variability of all the observations of analysis,

96
00:14:08,610 --> 00:14:15,669
all the outcomes. Again down here.

97
00:14:15,670 --> 00:14:19,510
This is the conditional variance to try and connect it to James,

98
00:14:19,510 --> 00:14:24,490
although we're not covering jail terms on the exam, but again, this is a conditional variance.

99
00:14:25,610 --> 00:14:28,640
Given a specific person's latent traits.

100
00:14:29,240 --> 00:14:34,750
Their observations vary by segment squared, and that's a conditional variance.

101
00:14:34,790 --> 00:14:44,120
When we conditioned on the random effect, there's a way to withstand variability and to make that clear some more.

102
00:14:44,390 --> 00:14:47,770
What's the expected value of YJ?

103
00:14:47,770 --> 00:14:51,290
AJ Again, conditioning on someone's in a random intercept and random slope.

104
00:14:57,920 --> 00:15:04,550
So what is the expected value of is a given? I know what the random intercept value is and what the random slope component is.

105
00:15:06,110 --> 00:15:11,060
So again, we have the fixed card. The expected value of the fixed components is just fixed.

106
00:15:12,290 --> 00:15:16,400
And because the random intercept is now known, it is a zero.

107
00:15:16,940 --> 00:15:25,250
It's known it has a value b, not I. So within a person, once they know their random components, they have an intercept.

108
00:15:26,210 --> 00:15:29,450
That's the population part, plus the individual part.

109
00:15:33,760 --> 00:15:45,530
Plus, again, I had that group effect. I had a time of s t i j I had an interaction.

110
00:15:46,560 --> 00:15:49,910
G g they have the random slope component.

111
00:15:53,780 --> 00:15:59,719
Can the expected value of this conditioning on the random intercepts and random slopes keeps those parts

112
00:15:59,720 --> 00:16:04,280
end of the formula in the errors drop out because that has an expected value of zero and still random.

113
00:16:10,120 --> 00:16:14,620
So there's a intercept for every person and there is a time component.

114
00:16:14,630 --> 00:16:37,490
So we've got what's. So every individual in Group A and Ruby has their own Intersect and every person and group A or B has their own slope,

115
00:16:38,300 --> 00:16:45,980
which is better to those beta three plus beta beta, not beta one.

116
00:16:46,010 --> 00:16:50,330
I. The one. I don't want that.

117
00:17:00,920 --> 00:17:05,480
Today there's an intercept and there's a slope.

118
00:17:06,170 --> 00:17:13,940
The intercept is a function of the person and which group they're in. The slope is a function of which person we're talking about in which group.

119
00:17:13,940 --> 00:17:17,280
There it is, guys. Zero.

120
00:17:17,300 --> 00:17:22,370
We have a slope of beta two plus B, not one, b not one, b one.

121
00:17:22,370 --> 00:17:32,029
I can't even if I can't look at that and immediately see that B and every person in

122
00:17:32,030 --> 00:17:37,940
Group B has a slope of beta two plus beta three plus their random component timestamp.

123
00:17:42,790 --> 00:17:49,670
So this is a subject specific. Specific intercept.

124
00:18:00,000 --> 00:18:07,170
And that's the subject specific slope. If we condition on the random effects there.

125
00:18:07,170 --> 00:18:13,910
No. So let's interpret data to.

126
00:18:17,790 --> 00:18:27,210
So better to quantify. Change in average.

127
00:18:29,430 --> 00:18:36,760
She's come. Which.

128
00:18:56,420 --> 00:19:03,350
Characteristics of one unit change in t.

129
00:19:05,590 --> 00:19:08,590
Changes. Why buy an animal being a two? And what does it mean?

130
00:19:08,590 --> 00:19:12,370
That means we have to account for the Laden's slope effect.

131
00:19:12,820 --> 00:19:17,230
Peter one I and which group there is. So again, one group has g of zero.

132
00:19:17,800 --> 00:19:24,640
They were left with beta two plus the random slope part. So better to quantify quantifies the change in average outcome among individuals

133
00:19:24,640 --> 00:19:28,990
and group who have the same lean characteristics conditional on who they are.

134
00:19:30,250 --> 00:19:35,050
Again, these member the Leighton Effect is just this constellation of all the covariates we didn't measure.

135
00:19:35,650 --> 00:19:39,580
That might explain why people have different outcomes. We didn't measure them.

136
00:19:40,090 --> 00:19:43,930
So we collectively put them into a random intercept type type formulation.

137
00:19:44,770 --> 00:19:51,010
So Beta two has in this model right now, Beta two has a conditional interpretation or a subject specific interpretation.

138
00:19:51,610 --> 00:19:59,799
We didn't really go through this in our analysis we did in James because as I have said,

139
00:19:59,800 --> 00:20:06,100
if we take the expectation of them out of the conditional mean, we get the marginal mean.

140
00:20:11,060 --> 00:20:14,090
So if we take the expected value of everything I wrote up in here.

141
00:20:15,730 --> 00:20:18,980
They get better, not plus better.

142
00:20:19,000 --> 00:20:23,830
One guy can. Now the random intercept has mean zero.

143
00:20:23,890 --> 00:20:36,020
It is expectation zero. And the random slope has means are in a drop zone.

144
00:20:37,380 --> 00:20:54,340
So now we have a marginal new. So we can talk about data to again identify as.

145
00:20:57,500 --> 00:21:03,260
A change in average. Don't count.

146
00:21:10,310 --> 00:21:15,930
Among all individuals. In gravity.

147
00:21:22,630 --> 00:21:28,750
This is what you've seen in your other classes. When you do change in t changes what?

148
00:21:28,750 --> 00:21:36,130
I mean, I'm out there to to for anybody. Anybody repeat the slope and group B is bad A two plus B and three.

149
00:21:36,850 --> 00:21:40,960
But so this has a population average interpretation.

150
00:21:42,310 --> 00:21:47,950
But again, we never worried about this because because if everything's linear, all the expectations,

151
00:21:48,250 --> 00:21:53,920
everything comes out in the conditional interpretation and the marginal interpretation are the same.

152
00:21:57,010 --> 00:22:01,150
So when you fit a linear mix model, you're still getting a population average coefficient.

153
00:22:02,500 --> 00:22:11,040
Even if we never thought about it that way. Right. Jan I'm reviewing that only to connect it to Glamour's.

154
00:22:11,520 --> 00:22:15,660
This stuff wasn't covered in Section three, so I'm not going to test you on it.

155
00:22:16,820 --> 00:22:21,830
Because I know you're still you're still getting your heads around it. It's a hard concept, right?

156
00:22:22,370 --> 00:22:29,050
But it's these concepts then carry over to times where things are no longer going to carry through nicely.

157
00:22:29,060 --> 00:22:33,350
And we get to different interpretations with parameters that are not the same.

158
00:22:37,740 --> 00:22:43,200
All right. So I've talked a little bit about between and within subject variability in lambs,

159
00:22:43,500 --> 00:22:53,250
how random effects are used to parse out some of the total variation as of between component and relative to the words that are used one bit.

160
00:22:56,560 --> 00:22:59,280
Right. So I'll try a little alignment. New topic.

161
00:23:11,780 --> 00:23:23,840
So let's just again, I like to stick with random intercepts just because the math doesn't get quite as complex, especially with correlation.

162
00:23:26,160 --> 00:23:29,250
YJ is intercepts.

163
00:23:29,270 --> 00:23:33,320
Plus a group of friends plus a time effects.

164
00:23:34,070 --> 00:23:43,850
I guess I mean a constant across individuals. So okay these are three plus percent intercepts.

165
00:23:49,660 --> 00:24:11,600
You. The correlation of any two observations from the same person is the same in both observations.

166
00:24:11,610 --> 00:24:20,710
I've just changed the time point J and time point K. Can we derive this in class algebra?

167
00:24:20,720 --> 00:24:28,780
It's covariance over the square root of the variances and it turns out to look like this at any two time points.

168
00:24:29,020 --> 00:24:33,340
The two observations have a correlation that is constant. There is no time in that equation.

169
00:24:34,480 --> 00:24:37,960
It's the same number. It's the ratio of the between variability.

170
00:24:43,940 --> 00:24:50,300
And the between. Your abilities plus the role within.

171
00:24:56,450 --> 00:25:04,520
It's a percentage and we don't have a negative correlation in most longitudinal, at least with a random US X model says total.

172
00:25:05,060 --> 00:25:08,810
How much of the total variability in the outcomes is due to the between variability?

173
00:25:09,440 --> 00:25:17,889
And so a correlation it's. So we called that drill for any time timepoints.

174
00:25:17,890 --> 00:25:22,960
And again, it's the same for every individual. There is not different correlations for different people.

175
00:25:24,590 --> 00:25:32,070
So as I just said, correlation. It's a measure of.

176
00:25:34,140 --> 00:25:38,390
How much? Total variance.

177
00:26:02,820 --> 00:26:15,670
W between. That's between. Correlation is a measure of how much of the total variance of outcomes is due to the variability between individuals.

178
00:26:20,020 --> 00:26:25,570
So again, as the between variability goes up, the correlation goes up.

179
00:26:30,480 --> 00:26:34,860
If all of us look so distinct from each other that our data does tend to cluster into

180
00:26:34,860 --> 00:26:39,540
different parts and our observations are very correlated with each other within a person.

181
00:26:41,200 --> 00:26:48,149
That's because we're all varying from each other by such a great amount. You can see it in the formula, right, as is said, and it goes to infinity.

182
00:26:48,150 --> 00:26:57,230
The correlation goes to one. And as between subject variability goes down and our data all start to overlap more and more with each other.

183
00:26:57,890 --> 00:27:01,990
There's no distinct clustering that's around them.

184
00:27:02,000 --> 00:27:03,290
The correlation is going down.

185
00:27:05,960 --> 00:27:14,630
If if there is no between subject variability, if they turn out a zero, the correlation is exactly zero that there is no between.

186
00:27:15,380 --> 00:27:18,170
There is no between some variability. There's just a massive data.

187
00:27:33,970 --> 00:27:43,510
So again, random intercepts explain over and over again many different ways between subject variability.

188
00:27:53,910 --> 00:28:03,780
So the random intercepts we think of explaining between subject variability at time zero maintains the baseline measure in a longitudinal study.

189
00:28:05,500 --> 00:28:13,050
And the random slopes. Explain between Southern variability and later tablets.

190
00:28:27,190 --> 00:28:33,500
Okay. So correlation again, correlation is quantifying between subject variability.

191
00:28:33,830 --> 00:28:42,350
I know we talk about within subject correlation. But within such a correlation is due to how much variability there is between individuals.

192
00:28:50,110 --> 00:28:51,340
So talked about random effects.

193
00:28:51,550 --> 00:28:58,600
And again, if we have a random slope to go back to this question, if we have a random slope, the correlation equation is much more complicated.

194
00:28:59,200 --> 00:29:02,590
The covariance has time in it, the variances have time in it.

195
00:29:02,980 --> 00:29:11,980
And the formula again is in the slides. When I presented this to you guys, the correlation is a function of time and it actually decays over time,

196
00:29:12,130 --> 00:29:15,010
looking very much like auto regressive type correlation structures.

197
00:29:15,890 --> 00:29:25,420
So again, the correlation is still some component of between subject variability relative to some total variability measure,

198
00:29:26,230 --> 00:29:33,190
regardless of how many random effects you have. It's just very simple to do with a random better model.

199
00:29:36,860 --> 00:29:40,370
Questions are about correlation, how their animals are doing that.

200
00:29:44,150 --> 00:29:52,380
All right. Election results.

201
00:29:54,870 --> 00:30:02,230
So I gave this just a short amount of time in lecture. And again, this is a topic that you probably could spend a semester on.

202
00:30:02,250 --> 00:30:06,780
There are textbooks written on this very complicated topic, theoretically.

203
00:30:09,990 --> 00:30:13,690
So let's be explicit here. She's a model.

204
00:30:13,710 --> 00:30:20,580
One is the model we had earlier steps.

205
00:30:22,520 --> 00:30:25,830
So I guess what I did, I switched to an ex.

206
00:30:27,110 --> 00:30:30,240
So it doesn't matter, huh?

207
00:30:30,300 --> 00:30:33,690
Why did I do that? All right, so we have some callbacks.

208
00:30:36,180 --> 00:30:43,980
We have. Jenny, this is not a lot of time.

209
00:30:44,830 --> 00:30:51,600
J j. And I want to clear this up again.

210
00:30:51,600 --> 00:30:55,560
I was someone brought up in the poll over questions.

211
00:31:10,220 --> 00:31:18,260
The fixed affects the x matrix are the intercepts discovery x CPL notation because then I call my design matrix x.

212
00:31:18,920 --> 00:31:24,470
Let's call it something else. Let's call it I don't know.

213
00:31:25,190 --> 00:31:34,280
Oh my goodness. It's not so again, there's just covariance h blood pressure, something continuous.

214
00:31:35,450 --> 00:31:39,080
So the fixed effects are intercept, discovery and time.

215
00:31:41,330 --> 00:31:45,680
My z, however, is an intercept and time.

216
00:31:49,430 --> 00:31:52,250
Z and X do not have to have the same variables.

217
00:31:53,600 --> 00:31:59,960
Computationally, theoretically, I could have nothing in the Z matrix that's in the x matrix and vice versa.

218
00:32:02,030 --> 00:32:06,620
I could say, well, you know what, I think that she should be in the fixed effects in a linear fashion.

219
00:32:06,920 --> 00:32:11,210
But I think that t should be quadratic in the random effects. I could do that.

220
00:32:12,290 --> 00:32:18,560
Computer doesn't care. They're just numbers. The interpretation, however, gets nasty.

221
00:32:19,430 --> 00:32:27,470
How would you explain that to someone? So many times, if we have a random slope, we have a fixed slope component.

222
00:32:31,080 --> 00:32:38,430
But they do not have to be the same quantity. Anything in the fixed effects and or it doesn't have to be in the random effects and vice versa.

223
00:32:38,430 --> 00:32:45,860
The random effect matrix. And often they overlap a lot because that's just the interpretation.

224
00:32:47,180 --> 00:32:50,810
Makes a lot more sense to do that. Right.

225
00:32:51,380 --> 00:32:55,370
So. It's.

226
00:32:57,840 --> 00:33:03,570
So we're going to say that this first thing here is a normal zero set and not square distribution.

227
00:33:04,110 --> 00:33:14,430
We're going to make them independent. Let's just say this has its own normal distribution at and again, that's normal zero sigma squared.

228
00:33:15,180 --> 00:33:23,460
And we're right that the random intercept is independent of the random slope is independent of the errors.

229
00:33:23,490 --> 00:33:29,250
Everything's independent. No covariates term in that D matrix, which is three variance components here.

230
00:33:34,500 --> 00:33:37,800
And I want to compare that to a second model in which I don't have a random slope.

231
00:33:50,750 --> 00:33:55,030
So my question is this Can I throw away the random stuff? Do I really mean that?

232
00:34:03,860 --> 00:34:08,080
Its two models are nested within each other. Everything in model two is in model one.

233
00:34:08,470 --> 00:34:37,530
There's just one additional component in models. The whole thing.

234
00:34:44,250 --> 00:34:48,900
Model one is one more parameter at variance parameter that isn't contained in model two.

235
00:34:56,360 --> 00:35:07,400
So we could talk about this is just. That says that under the null hypothesis, the variance component is exactly zero.

236
00:35:08,720 --> 00:35:13,190
If something with mean zero has variance zero, it is exactly zero all the time.

237
00:35:14,150 --> 00:35:20,840
And that essentially means it's gone versus the alternative that says no, this variance component is not equal to zero.

238
00:35:22,390 --> 00:35:26,710
And you know, let's be honest here. We're going to the negative side.

239
00:35:27,070 --> 00:35:34,870
That would be kind of silly. So we've learned how to compare invested models.

240
00:35:34,960 --> 00:35:40,000
We use the likelihood ratio test, say negative to log times,

241
00:35:40,000 --> 00:35:49,930
to log likelihood and little H log likelihood in the model to the log likelihood to make it fit in the right direction.

242
00:35:53,050 --> 00:35:56,230
That's right. That's going to be my test statistic.

243
00:35:59,090 --> 00:36:04,370
This is a metric that we use to compare models. How big are the likelihoods relative to each other?

244
00:36:07,490 --> 00:36:10,690
And there's well tests and they're scored tests and they all have the same sort of flavor.

245
00:36:12,260 --> 00:36:15,620
The question then is how do I get a P value? How do I do inference?

246
00:36:18,630 --> 00:36:22,470
What's the distribution of the likelihood ratio test statistic?

247
00:36:24,000 --> 00:36:27,990
Statistics are no.

248
00:36:31,440 --> 00:36:36,270
And it's no no distribution. And where this number might lie in that distribution gives me a p value.

249
00:36:41,590 --> 00:36:56,950
And in your theory classes. We've talked about under irregularity conditions the just testing.

250
00:36:58,870 --> 00:37:09,910
Stick. There's a chi square with one degree of freedom distribution, the chi square distribution.

251
00:37:10,060 --> 00:37:14,110
The degrees of freedom is the number of parameters that they differ by, which is one in this case.

252
00:37:17,820 --> 00:37:23,610
What's the critical value of Chi Square when distribution? Why now?

253
00:37:24,960 --> 00:37:29,010
Does it have a two year, 3.84? We know that it's about four.

254
00:37:29,040 --> 00:37:35,999
Right. The critical valve in a normal distribution is to identify square a squared, square one as a square in our normal.

255
00:37:36,000 --> 00:37:40,590
So it's about four, but it's 3.84. So it happens to be after 22 years.

256
00:37:41,760 --> 00:37:46,180
Those things never leave your brain. It's kind of like this the checkout people at the grocery store.

257
00:37:46,440 --> 00:37:50,760
They know the codes of all the vegetables and the fruit just never goes away.

258
00:37:51,660 --> 00:37:58,320
So if we had regularity conditions holding here, we would look this up in a five square one distribution and compare it to 3.84.

259
00:37:59,250 --> 00:38:01,020
But we don't have regularity conditions here.

260
00:38:06,270 --> 00:38:31,980
Violator regularity conditions because H.A. is on the boundary or testing are right at the boundary value.

261
00:38:32,970 --> 00:38:37,620
Now is a problem in all of this theory that you have learned so far.

262
00:38:43,460 --> 00:38:46,790
So what are we going to do here? Well, we have two options.

263
00:38:46,820 --> 00:38:48,920
We can figure out what the distribution is.

264
00:38:52,000 --> 00:39:03,880
And so, again, there's some very difficult theory, but it shows that actually, in this case, the no distribution.

265
00:39:07,190 --> 00:39:15,320
Is 5050 next year, then chi square with zero degrees of freedom and okay square with one degree of freedom.

266
00:39:18,920 --> 00:39:22,940
So picture a chi square one with a spike in zero.

267
00:39:23,540 --> 00:39:26,840
50% of the time at zero. 50%. And it's a chi square.

268
00:39:28,730 --> 00:39:33,320
So that's the normal distribution. And you can figure out what the critical value is in that number distribution.

269
00:39:33,320 --> 00:39:39,080
And you can do inference. You can do that. This is the only time when it's a 5050 mixture of Chi Square.

270
00:39:39,230 --> 00:39:44,540
If you want to test for inclusion of other random effects in other more complicated models,

271
00:39:45,170 --> 00:39:49,700
the mixtures of the chi squares and the degrees of freedom is a horrible, horrible task.

272
00:39:49,700 --> 00:39:55,040
And it's not programed by anybody that I know of. Everyone has written code if they do it themselves.

273
00:39:55,310 --> 00:40:02,270
And as I told you, if you ever use SAS, SAS gives you p values for random effects tests and they're wrong.

274
00:40:02,330 --> 00:40:06,530
I think they're still wrong to use the likelihood ratio standard theory.

275
00:40:06,800 --> 00:40:13,370
Some programmer just said, Oh, I know how to do this, and they just typed in it and I can't believe it hasn't been fixed.

276
00:40:14,510 --> 00:40:18,710
To fix it would be very, very hard. Again, they should just eliminate it altogether.

277
00:40:19,160 --> 00:40:28,820
So we can't really do hypothesis testing for random effects without using some really heavy machinery that would require us to do our own programing.

278
00:40:29,510 --> 00:40:35,989
So we throw away the hypothesis testing and we do it as next best thing.

279
00:40:35,990 --> 00:40:52,980
And that's B I see. Another way to do this.

280
00:40:53,000 --> 00:40:54,890
There's no p value. There's no inference here.

281
00:40:55,340 --> 00:41:02,360
We just compare it to numbers and we pick the one that is smaller and I see smaller likelihood adjusted for penalized complexity.

282
00:41:06,410 --> 00:41:14,750
So that's why we use AIPAC when we try to decide whether to include random effects or not, because the hypothesis testing is just very,

283
00:41:14,750 --> 00:41:29,540
very complex and violates standard likelihood theory, excellent plant theory, right where we at quarter to quarter to five, when will that be fixed?

284
00:41:31,430 --> 00:41:36,490
All right. So that's all I'm really going to talk about again, about selection of random effects.

285
00:41:36,500 --> 00:41:44,810
We use ABC, ABC, NBC, because this testing just isn't really feasible for us in most settings.

286
00:41:48,390 --> 00:41:55,200
So then we talked about prediction of these random quantities.

287
00:42:09,380 --> 00:42:14,810
So again, these two quantities are not parameters.

288
00:42:15,290 --> 00:42:19,340
They are random. They're random variables that come from a distribution.

289
00:42:20,630 --> 00:42:43,570
So we don't estimate them. I think we might see that there's huge variability in the baseline measurements, that there's a random intercept necessary.

290
00:42:44,260 --> 00:42:47,470
Why does someone have a really huge intercept relative to everybody else?

291
00:42:47,950 --> 00:42:54,600
I don't know. It's some latent constellation of factors that's quantified in that random intercept.

292
00:42:55,300 --> 00:43:01,310
It's not a perfect measure of of what is going on, but at least it shows us who differs from whom by a lot.

293
00:43:01,330 --> 00:43:09,800
We just don't really know exactly what. You can make a guess.

294
00:43:15,120 --> 00:43:31,070
She's. So we might say, well, we have some data.

295
00:43:31,080 --> 00:43:35,569
Can we use the data that the person gave us to think about maybe what that random intercept

296
00:43:35,570 --> 00:43:42,979
might have looked like had we been able to collect it and reduce the idea of lumps less linear,

297
00:43:42,980 --> 00:43:53,810
unbiased predictors. So this idea that this produces reduce the prediction.

298
00:43:59,710 --> 00:44:09,830
Speaks volumes. Well, it's not too that we call that thing the tilde, and that would be a hat just to begin with.

299
00:44:09,830 --> 00:44:12,800
But hats around parameter estimates are on top of parameter estimates.

300
00:44:13,280 --> 00:44:19,610
So vital there is the expected value of what VII might be conditional on the outcomes that that person gave us.

301
00:44:22,100 --> 00:44:27,170
Can I show you the derivation of that idea? That is that's a posterior mean.

302
00:44:28,340 --> 00:44:32,350
We can think about putting priors. We can think about that being a conditional mean.

303
00:44:32,360 --> 00:44:39,980
We can get a conditional mean from the joint divided by the marginal of those concepts from six so on and the Bayesian class if you took them.

304
00:44:41,090 --> 00:44:45,559
So again, we call this an empirical Bayes estimate because again it comes from Bayesian concepts,

305
00:44:45,560 --> 00:44:53,209
but we don't use Bayesian methods, we don't put priors on everything, just put priors on the random effects we estimated.

306
00:44:53,210 --> 00:44:57,980
Again, it's empirical because the Beatles are estimated with frequentist methods.

307
00:44:59,120 --> 00:45:05,280
It's the random effects that have a Bayesian interpretation. And so it's not fully Bayesian.

308
00:45:06,840 --> 00:45:12,580
Bayesian will tell you that it's not being seen at all. That's because they're snobs.

309
00:45:13,130 --> 00:45:16,680
You're right. You shouldn't say that on a recording.

310
00:45:16,680 --> 00:45:21,240
So they are just losing stuff all the time.

311
00:45:22,020 --> 00:45:26,220
All right. So we saw in lecture.

312
00:45:32,940 --> 00:45:36,930
What's the tweet on the bottom that we're on, the chopping and shovels of here?

313
00:45:37,200 --> 00:45:41,520
We have these predictions. This thing is a weighted average.

314
00:45:48,520 --> 00:45:53,770
Of the person's data. So a person's random effects or a weighted average of their data.

315
00:46:12,370 --> 00:46:17,810
All of the data. What I'm trying to figure out where someone's random intercept lies.

316
00:46:19,010 --> 00:46:25,730
I should think about where their data are relative to all the data, and it's some weighted average of those two quantities.

317
00:46:33,030 --> 00:46:41,040
And the weights are based on our two favorite quantities which in.

318
00:46:42,690 --> 00:46:56,700
In between. Subject variability and this is D in this segment squared errors.

319
00:47:01,970 --> 00:47:07,770
If De is large now again. We talk about large in a matrix sense.

320
00:47:08,430 --> 00:47:12,960
If it's just a scalar, if the between subject variability is large.

321
00:47:15,740 --> 00:47:24,389
Relative to that within. So in between is very large.

322
00:47:24,390 --> 00:47:32,970
If everybody's observations look very distinct from everybody else, then the random inertia ship, the random intercept,

323
00:47:33,960 --> 00:47:39,150
should resemble more of their own data than everybody else's data because everybody is so distinct.

324
00:47:39,150 --> 00:47:43,830
Right? So if DX is large relative to sigma squared, then the prediction.

325
00:47:50,900 --> 00:47:54,910
It resembles really simple. That person's data.

326
00:48:01,700 --> 00:48:08,350
More than comes. But the ultimate of the others.

327
00:48:15,700 --> 00:48:23,180
Be is small. Sigma squared.

328
00:48:24,200 --> 00:48:28,400
If there's very little between subject variability, if all the data are just on top of each other.

329
00:48:32,630 --> 00:48:43,940
Until the. Why don't you do it?

330
00:48:44,080 --> 00:48:57,090
That should resemble. More than in-person nuptials.

331
00:49:02,320 --> 00:49:04,960
Gender equations for all of us. Exactly. Works.

332
00:49:04,970 --> 00:49:11,650
Again, the weights are matrices and we have multiple random effects, but the concept holds the same way.

333
00:49:12,380 --> 00:49:18,790
We're trying to see how distinct my data are from everybody else and when I figure out what the random effects should be.

334
00:49:19,270 --> 00:49:22,870
How much credence do I give to my own data? How much should I believe the other data?

335
00:49:22,880 --> 00:49:26,620
Mark And that's all a function of these two components of variability.

336
00:49:27,310 --> 00:49:30,710
This is cool stuff. Yep.

337
00:49:30,870 --> 00:49:37,200
All right. If there's one question that relates to our code that I will try and get to before the end of class.

338
00:49:39,270 --> 00:49:45,580
That's it for linear its models. How they quantify the two components of variability.

339
00:49:45,600 --> 00:49:49,950
What prediction of random effects looks like? How we decide whether to have random effects in a model?

340
00:49:52,550 --> 00:49:59,320
Interpretations. That's everything. Again on the exam.

341
00:50:00,190 --> 00:50:05,080
There are computational questions. They don't need anything more than a calculator.

342
00:50:05,710 --> 00:50:11,230
You will not need our code and sufficient and deep derivations.

343
00:50:11,890 --> 00:50:18,310
A very obvious question would be that if I gave you the track of two components of variability, what's the correlation coefficient?

344
00:50:19,090 --> 00:50:25,780
I just showed you the formula a few minutes ago. Might be on the test piece, stuff like that.

345
00:50:25,840 --> 00:50:33,220
Those are the sorts of when I mean computation. They require a calculator with plus minus divide, sometimes maybe an exponent.

346
00:50:34,660 --> 00:50:40,990
But that's it. I just don't have time to have you guys drive a lot of stuff.

347
00:50:44,200 --> 00:50:47,620
All right. So let's talk about Gina. Generalized estimating equations.

348
00:51:01,660 --> 00:51:06,580
So in this class we dealt primarily it was for sun or binary data.

349
00:51:09,600 --> 00:51:27,310
Comes. Since we're getting repeated measures on people.

350
00:51:33,680 --> 00:51:37,370
We need to account for our friends between.

351
00:51:40,370 --> 00:51:46,620
And what's happened. Subject variability.

352
00:51:47,590 --> 00:51:52,540
So G is doing this in a slightly different way.

353
00:51:53,050 --> 00:52:05,340
Very much like we were doing with glass. So again marginally for each observation why I it.

354
00:52:06,990 --> 00:52:14,729
Let's start with Bosnia. Let's just let's stick with percent. We might assume marginally that every observation in the dataset,

355
00:52:14,730 --> 00:52:25,110
every outcome comes from a baseline distribution with a mean subscripts, i and J so that these observations have mean.

356
00:52:25,860 --> 00:52:36,600
The YJ may have a marginal variance, but that's the hallmark of a Western distribution means equal to the variance.

357
00:52:41,180 --> 00:52:44,630
And then we say that the mean is related to covariates.

358
00:52:45,080 --> 00:52:53,900
Yes, sir. Sir, I had a question for the interpretation of the random rule about random variable way.

359
00:52:54,380 --> 00:53:05,150
No, it's for the PI. It's not that you use for them. So you said then the key is to resemble the data morally y or the meaning outcome.

360
00:53:05,360 --> 00:53:08,800
Yeah. I thought that should be. It should be the estimation of the.

361
00:53:09,840 --> 00:53:13,430
The group mean. Yeah, that's what I need. Okay.

362
00:53:13,460 --> 00:53:16,600
The group mean is estimated from all the data, right? Yeah.

363
00:53:16,970 --> 00:53:25,090
So because I would think in the extreme case when T is zero, then I should not resemble anything like why I write it.

364
00:53:25,160 --> 00:53:28,640
Should just close to zero. Should be zero. Sorry. No, no, no.

365
00:53:28,730 --> 00:53:32,510
That's again. When I say all the data, I mean, what do we do with all the data? We get a group mean.

366
00:53:33,920 --> 00:53:40,250
Or we get some regression coefficients, right. Yeah. So but we use all the data to get some parameters.

367
00:53:40,380 --> 00:53:45,740
Yeah. You're being a little more precise than I am, probably.

368
00:53:46,560 --> 00:53:52,940
Which is good. We're in prison or binary. So, for example, Sonya's a mean.

369
00:53:52,940 --> 00:53:59,989
We don't model the mean as a function of covariates. We model the log of the mean because that's the canonical link.

370
00:53:59,990 --> 00:54:04,129
And it also leads to means that are always positive.

371
00:54:04,130 --> 00:54:07,730
We can't have negative means in a Western model.

372
00:54:07,850 --> 00:54:13,760
So we say again that, you know, there's some I'll say some covariates, x one,

373
00:54:15,290 --> 00:54:22,270
some covariant x two at every time point for every person or in some regression model, no random effects.

374
00:54:23,550 --> 00:54:24,710
The strikes effects are.

375
00:54:44,750 --> 00:54:52,140
So we're going to go back to what we said earlier with cylinder mix models, which is the total variability of all the outcomes in the dataset.

376
00:54:53,480 --> 00:55:01,910
It's very strict. It has to match. It's there means that is the total variable, the total variability of why I say.

377
00:55:10,490 --> 00:55:16,639
So we throw in a working correlation matrix. Well, again, what did I say?

378
00:55:16,640 --> 00:55:25,160
Correlation does correlation quantifies the magnitude of the between subject variability relative to all the variability.

379
00:55:25,490 --> 00:55:31,160
So the working correlation matrix is put in there to quantify.

380
00:55:34,040 --> 00:55:37,370
The pursuit of.

381
00:55:39,880 --> 00:55:56,940
Between subject variability. Do.

382
00:56:10,860 --> 00:56:15,610
And correlation is used to quantify the between subject variability.

383
00:56:15,630 --> 00:56:18,810
Everything left over. We don't really model it, we don't really talk about it.

384
00:56:19,620 --> 00:56:22,890
But all of the leftover is that within subject variability?

385
00:56:33,200 --> 00:56:51,390
I'm sorry that. So again, what is the generalizing, a generalizing, generalized estimating equation for our regression parameters?

386
00:56:53,300 --> 00:57:01,970
Takes this very horrendous looking form, but it is very similar to everything you've seen in all the other regression modeling has taken.

387
00:57:02,390 --> 00:57:08,730
It's a vector, that's vector. That's something that's vector for every person.

388
00:57:08,750 --> 00:57:13,880
We can view these things, we multiply them and then we add them all up because everybody is independent from one another.

389
00:57:18,680 --> 00:57:23,140
These are the derivatives. Of.

390
00:57:25,980 --> 00:57:31,440
Model with respect to Peter.

391
00:57:31,680 --> 00:57:41,650
This is like the design matrix X and a linear regression model. That.

392
00:57:55,010 --> 00:57:59,810
This is the variance covariance matrix of the observations and we can write it in this form.

393
00:58:00,800 --> 00:58:01,520
This will get along.

394
00:58:01,520 --> 00:58:08,690
The diagonals will have the position variance formula and then also the diagonal will have whatever's in the working correlation matrix,

395
00:58:09,470 --> 00:58:17,210
which could be a lot of aggressive. It could be exchangeable, however. But again.

396
00:58:19,240 --> 00:58:26,910
This. The these right here are the total variance. And this right here is the between.

397
00:58:32,520 --> 00:58:36,900
So the concepts in the linear mix model are there, but they're done in a slightly different way.

398
00:58:38,380 --> 00:58:42,040
Right. This doesn't produce a joint distribution of all the observations.

399
00:58:42,850 --> 00:58:49,600
Marginally. My observations come from a Poisson distribution, but jointly they don't come from anything.

400
00:58:50,110 --> 00:58:53,500
We don't have a joint distribution. We have a quasi likelihood.

401
00:58:54,430 --> 00:58:59,140
We never come up with a true distribution, multivariate distribution for my outcomes.

402
00:58:59,640 --> 00:59:10,270
And so there's a whole body of theory that relates likelihood methods to quasi likelihood methods which can be recovered in another course.

403
00:59:12,670 --> 00:59:17,530
But the point is that. Right, so there are beaters in this thing.

404
00:59:17,530 --> 00:59:19,560
There are battles that we have to solve for.

405
00:59:21,730 --> 00:59:30,220
And what we end up with is the same sort of formula that we have seen in other settings of regression models, whether it's genomes or limbs.

406
00:59:48,550 --> 00:59:52,420
And this looks like x transpose x inverse x transpose y.

407
00:59:53,440 --> 01:00:00,610
Now there's a weight matrix for the correlation and the expert is just the derivatives of the mean with respect to the parameters.

408
01:00:01,360 --> 01:00:09,460
So it has that same sort of flavor of weighted regression. And where do we throw in all the hints?

409
01:00:11,310 --> 01:00:17,350
Because there's all kinds of hints going on here. We've got to figure out what's in the correlation matrix, what's why is there an estimated.

410
01:00:18,370 --> 01:00:22,540
But then we end up putting estimates of everything in there. So that's the that.

411
01:00:28,470 --> 01:00:31,870
You can't do this formula too many times in your graduate careers.

412
01:00:31,890 --> 01:00:38,390
What is the variance of theta? That's right.

413
01:00:39,060 --> 01:00:43,590
Variance. The only thing that's random and random and all of that is the Y part.

414
01:00:43,680 --> 01:00:47,670
Everything else is fixed. So you've got a quadratic form.

415
01:00:47,730 --> 01:00:54,060
The variance of something times y is that something times the variance of y, times that something transpose.

416
01:00:55,770 --> 01:01:00,480
So let's write all that out. This will relate to one of your questions, I promise.

417
01:01:22,870 --> 01:01:31,980
Why? Right. So I've carried everything over and I have the variance of my life to carry everything over to the other side.

418
01:01:32,580 --> 01:01:38,870
And again, it's there's a little bit of algebra that has to go on here to get the sum to look like what I want it to look like.

419
01:01:38,880 --> 01:01:43,320
But trust me. It looks like that.

420
01:01:44,840 --> 01:01:50,280
We have that. And we repeat this thing on the left hand side once again.

421
01:02:00,800 --> 01:02:23,430
So that's the variance. If the various covariance matrix of my observations exactly matches what I thought it was.

422
01:02:25,410 --> 01:02:34,440
That's what's in this. Look, we got everybody music, you know.

423
01:02:36,120 --> 01:02:39,169
Maybe a hint that thing should make a Greek letter.

424
01:02:39,170 --> 01:02:42,650
I know the name of my head. If those two things are equal.

425
01:02:44,420 --> 01:02:49,670
If this is equal to the inverse of that lambda lambda.

426
01:02:49,670 --> 01:02:57,300
Thank you. Thanks, Mike. That's exactly what it is. If those two things are equal, then these cancel out with each other.

427
01:03:00,150 --> 01:03:08,110
And now I have the sum of Delta. I transpose and inverse delta again that matches one of these inverses.

428
01:03:08,130 --> 01:03:11,160
All of this goes away from all of that.

429
01:03:19,370 --> 01:03:36,480
Let's. So if I got the model right, that is the variance covariance matrix of mega regression parameters.

430
01:03:41,820 --> 01:03:47,010
That model based. Our call.

431
01:03:47,180 --> 01:03:52,860
So many people in the world call us naive. Why is it naive to believe that your model is correct?

432
01:03:52,890 --> 01:03:58,560
I don't know that we get so call this naive.

433
01:04:04,570 --> 01:04:16,420
If I don't want to believe my model. So it's a variance of why I tilde if I worry about this not holding, but whether that's true.

434
01:04:20,110 --> 01:04:28,260
The variants of beta cannot. You call this the model based?

435
01:04:33,700 --> 01:04:41,290
So if the model is not right, if I cannot do all this cancellation, then the variance goes back to the model based variance.

436
01:04:42,940 --> 01:04:51,430
Times this thing inside the brackets, sum equals one and delta transpose.

437
01:04:53,400 --> 01:05:06,250
Inverse. During the time that it was hit the tests again times the model based variance.

438
01:05:11,880 --> 01:05:17,460
So if I don't believe my model, I still have this to be the variance. And that's why we call it a sandwich estimate.

439
01:05:18,060 --> 01:05:20,070
Right. There's something on both sides, the bread.

440
01:05:20,070 --> 01:05:26,160
And there's thing in the middle called the meat or the whatever sandwich components you want to call it.

441
01:05:28,620 --> 01:05:32,730
And I have to put in an estimate here. I don't use the model to estimate it anymore.

442
01:05:33,360 --> 01:05:43,250
I estimate it from the residuals. And in the G role that's called SDI, and that is simply the outer product of the residuals.

443
01:05:44,540 --> 01:05:50,250
Right, which I put that.

444
01:06:00,950 --> 01:06:10,450
Transpose in the right direction. The second one you. This is not the sum of the squared residuals, right?

445
01:06:10,930 --> 01:06:15,490
It's not a row vector times, a column vector. This is a column vector times a row vector.

446
01:06:15,520 --> 01:06:21,700
And up with a matrix. I take every residual times, every residual.

447
01:06:21,790 --> 01:06:24,850
And along the diagonal, I get the residuals squared.

448
01:06:25,090 --> 01:06:33,129
If the residuals have mean zero, then squaring them gives me an estimate of the variance in all the off diagonal terms.

449
01:06:33,130 --> 01:06:37,690
Give me some idea of covariance. So the residuals here, I used to fill that in.

450
01:06:40,250 --> 01:06:44,760
And so this is called the sandwich sheet.

451
01:06:48,730 --> 01:06:53,020
You know that some people like to call it robust.

452
01:06:58,990 --> 01:07:06,790
So one of the questions on the pole everywhere was, are the model based and the sandwich based estimates different?

453
01:07:07,570 --> 01:07:13,650
And yes, they are very different. The sandwich estimate here uses the model based variance twice.

454
01:07:13,660 --> 01:07:18,160
And then to put something in the middle there, there are very select instances,

455
01:07:18,160 --> 01:07:23,020
as I tried to point out in class, when the model based in the sandwich will be the same.

456
01:07:24,010 --> 01:07:26,020
It's a very rare occurrence.

457
01:07:26,680 --> 01:07:34,450
It only occurs when the design matrix has dummy variables and you have a balanced set of data where everybody's measured at the same points in time.

458
01:07:34,810 --> 01:07:39,070
That is the only time all of the math will lead to the same.

459
01:07:39,580 --> 01:07:45,729
Essentially, in that case, we get the same cancellation cancellation we got earlier, but nine times out of ten,

460
01:07:45,730 --> 01:07:51,910
99 times out of 100, they are not going to be the same unless you have a very unusual design for your study.

461
01:07:57,780 --> 01:08:02,430
Some of those other questions I haven't hit upon.

462
01:08:03,210 --> 01:08:11,770
You're dying to know. I must tell you, you've done a very nice job of asking me questions that reflects most of the stuff on the exam,

463
01:08:12,970 --> 01:08:17,550
which tells me maybe, you know what I'm buying, what I think is important or what is, and I'm teaching what's actually on the test.

464
01:08:17,560 --> 01:08:24,280
So anyway, I think this has been a really nice review of everything I find important.

465
01:08:25,330 --> 01:08:33,520
Again, make sure you know how to interpret coefficients. If I give you the results of a binomial Poisson regression model and I ask you,

466
01:08:33,520 --> 01:08:38,710
you know, what's the what's what's the difference between the means of the two groups,

467
01:08:39,700 --> 01:08:44,800
you know, that you might have to exponentially add something or do something to get to the right answer.

468
01:08:47,490 --> 01:08:55,470
And I think that's everything. And it's quarter after. Yes. I'm just a quick question about the idea of like marginal or conditional interpretations.

469
01:08:55,920 --> 01:09:04,290
So just to confirm that in linear I's mixed models, there is we can't the marginal in the conditional interpretation of the same.

470
01:09:04,500 --> 01:09:07,530
So for generalized linear mix models, that doesn't that's not true.

471
01:09:07,710 --> 01:09:15,330
That is you are you are right and there's one caveat with Poisson.

472
01:09:15,390 --> 01:09:20,670
Again, this is not on the test that technology allows for linear mixed models.

473
01:09:21,690 --> 01:09:25,620
The parameters from a linear mix model have a conditional and a marginal interpretation.

474
01:09:25,620 --> 01:09:29,730
They're the same for binary models.

475
01:09:31,050 --> 01:09:36,180
The conditional interprets the coefficients from a marginal model and a conditional model.

476
01:09:36,450 --> 01:09:41,690
They do not have the same interpretation. I see that again, the coefficients in a Guillaume.

477
01:09:42,780 --> 01:09:47,670
When you have binary data, those parameters never have a marginal interpretation.

478
01:09:48,240 --> 01:09:57,480
The marginal parameter is very different from what you get from Jay Leno for a Poisson model, a dilemma with based on outcomes.

479
01:09:59,580 --> 01:10:02,310
The coefficients have a conditional interpretation.

480
01:10:03,120 --> 01:10:10,380
It turns out that all of the parameters except for the intercept also have a marginal interpretation.

481
01:10:12,580 --> 01:10:15,850
So for normal outcomes, everything conditional is marginal.

482
01:10:16,980 --> 01:10:22,620
For Poisson, everything conditional is marginal except for the intercept.

483
01:10:23,820 --> 01:10:30,570
And for binary data. Everything that is a conditional parameter does not have the marginal interpretation of that.

484
01:10:32,570 --> 01:10:41,059
It was from all to nothing. And again, it's just because of the mathematics and the fact that the logit is just a nasty equation.

485
01:10:41,060 --> 01:10:47,990
It is non-linear. It's very non-linear. And it makes everything just not we like linear models.

486
01:10:49,190 --> 01:10:56,389
So what would you say was the case where The Intercept doesn't have a marginal internal in a

487
01:10:56,390 --> 01:11:03,260
Poisson in the The Intercept you get from the Poisson glm does not have a marginal interpretation.

488
01:11:03,700 --> 01:11:11,760
I mean. The marginal intercept is related to it, but it is not exactly the same thing.

489
01:11:19,160 --> 01:11:23,860
All right. That is it. I will see you in two weeks.

490
01:11:23,870 --> 01:11:31,479
Hard to believe. Two weeks after you take the test, it will be in my office as asked on Friday during the class period.

491
01:11:31,480 --> 01:11:36,340
2 to 330. If you're going to take the test, then you want to ask me questions or if you just want to stop by and ask questions.

492
01:11:37,420 --> 01:11:42,310
What counted as extra office hours? I do have office hours on Monday.

493
01:11:43,000 --> 01:11:48,000
If you're going to wait until Tuesday to take the test, I'll be in my office hours.

494
01:11:48,500 --> 01:11:59,336
Either way, it's good luck to see you in a couple weeks. Just things like this rather than trying.

