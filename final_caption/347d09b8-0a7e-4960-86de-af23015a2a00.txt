1
00:00:00,360 --> 00:00:05,630
Kind of good stuff there now.

2
00:00:07,770 --> 00:00:15,210
Today, we're going to start talking about homework number three, which is still on Sunday, October 15th.

3
00:00:16,770 --> 00:00:24,140
As always, we are going to cover this homework into labs today and next Monday.

4
00:00:24,150 --> 00:00:31,390
Okay. Weird. The problems that we are going to see today.

5
00:00:31,410 --> 00:00:34,950
Hopefully you are going to be able to complete our homework number.

6
00:00:35,520 --> 00:00:38,880
Problem number one without a problem. Okay.

7
00:00:39,510 --> 00:00:44,250
So let's just begin. This is the outline for today.

8
00:00:44,970 --> 00:00:48,450
It's it's mom. I know we're going to try to make it all.

9
00:00:49,560 --> 00:00:54,930
We are going to see some plus on model versus chrome plated for tomorrow.

10
00:00:54,960 --> 00:01:02,260
Then I'll make up the final Miyamoto. Then negative nine on the only some flying parents on that inflated make up this

11
00:01:02,280 --> 00:01:08,910
binomial for b we have on our word with the same example throughout the lol.

12
00:01:09,270 --> 00:01:19,840
However, we just want to make sure that it's noted that these kind of models are not the best for the dataset that we are using.

13
00:01:19,860 --> 00:01:23,669
However, we are just going to pretend they are okay, just for the sake of the examples.

14
00:01:23,670 --> 00:01:29,290
Or maybe not. Yes, ma'am. We are just going to. Go with it.

15
00:01:30,790 --> 00:01:36,340
Okay. So for the class for today, we're going to work with the data set that it's already loaded.

16
00:01:36,910 --> 00:01:48,819
But the data set involves chronic obstructive pulmonary disease physicians who were retrospectively asked about the number of

17
00:01:48,820 --> 00:01:56,620
exacerbations that they have experienced in the previous year so that the number of exacerbations will be our outcome variable.

18
00:01:56,830 --> 00:02:04,900
Since that outcome variable, the number first is an integer that it tells like a positive integer to a one, two, three, four and so on.

19
00:02:05,710 --> 00:02:10,660
A cold model must be or should be very accurately appropriate for these data.

20
00:02:11,740 --> 00:02:22,220
We will explore the Association of wealthy but soon to be our main the US with the number of exacerbations reported in the previous years.

21
00:02:22,840 --> 00:02:27,400
Seeking to account some other covariates as well.

22
00:02:29,180 --> 00:02:35,960
These are pretty solid, does it? If there's one thing, it's a continuous variable that measures like it,

23
00:02:36,350 --> 00:02:44,470
like the thickness of the low, that measures the frequency of the wall in the loans.

24
00:02:44,480 --> 00:02:52,100
And then at some point when I create Thomas blinds in this wall thickness between 1.4 and 1.75.

25
00:02:53,030 --> 00:02:57,440
Okay. So the data side, you just heard it.

26
00:02:58,010 --> 00:03:01,370
So we are all talking in the same language.

27
00:03:01,940 --> 00:03:10,399
Our outcome variable is going to be the number of exacerbations is a x ac underscore account.

28
00:03:10,400 --> 00:03:13,459
Here we have all of these are covariates.

29
00:03:13,460 --> 00:03:23,480
We have age, whether they are male or female, the covariance that we are reporting on a lot of, whether they are as is or not on this counter,

30
00:03:23,810 --> 00:03:32,930
the type of they use to measure like they won't thickness on what our pre-COVID but I will say this which is a continuous variable.

31
00:03:33,950 --> 00:03:41,420
Okay. Based on that, first we are going to run some fossil on C or inflated model.

32
00:03:41,570 --> 00:03:43,640
So you see inflated for some water.

33
00:03:44,210 --> 00:03:51,740
So for this first part above the clouds, where we're going to go is first we are going to feed our regular Poisson model,

34
00:03:52,160 --> 00:03:57,260
then we're going to feed a or inflated for someone, and then we're going to compare these.

35
00:03:57,260 --> 00:04:06,410
So how do we compare these? So using I don't know how to say this for good volume best.

36
00:04:08,060 --> 00:04:14,280
Okay. So first, let's just feet up with someone.

37
00:04:14,430 --> 00:04:21,050
How do we freed up a submarine subs? First of all, we've got the problem that we are using is Jen.

38
00:04:21,990 --> 00:04:28,410
The data set is the data set that contains like, you know, one variable for each variable in the dataset,

39
00:04:28,410 --> 00:04:33,810
including the outcome, and each row corresponds to each space patient.

40
00:04:34,830 --> 00:04:41,340
Here we are just using this class a statement. We are just using the reference level that we want.

41
00:04:41,640 --> 00:04:45,060
In this case, since those countries are categorical variable,

42
00:04:45,060 --> 00:04:51,660
I want to create as reference level for that variable because that may be easier for me to understand, to interpret the results.

43
00:04:52,080 --> 00:04:58,470
But this line, I mean, is just for setting reference variables for categorical files.

44
00:04:58,830 --> 00:05:03,090
The really important line is here. So model it's as you will, right?

45
00:05:03,090 --> 00:05:03,870
Any mother.

46
00:05:04,180 --> 00:05:15,030
This is my outcome variable against all of my predictors and then here backslash remember these as well here but this lot and then distribution these

47
00:05:15,030 --> 00:05:24,990
equals Poisson for a Poisson regression model and then right and that C the outcome should look something like this if you're running it in source.

48
00:05:25,350 --> 00:05:30,480
So we have a table that is called analysis of maximum likelihood parameters estimates.

49
00:05:30,840 --> 00:05:41,640
I'm here. I have all of the parameters estimates. The estimate based on that error, the 95% confidence interval, then the world test for each,

50
00:05:42,030 --> 00:05:47,640
the wald test for each of the parameters and then the P value associated to each parameter.

51
00:05:48,630 --> 00:05:56,940
Very important here that US logistic regression, for example, these values that you have here are the role estimates.

52
00:05:56,940 --> 00:06:03,070
So for example, these 0.52 is the beta corresponding to all three.

53
00:06:03,510 --> 00:06:08,010
However, since we are working with logistic, I'm sorry, we are also regression.

54
00:06:08,400 --> 00:06:15,570
If you want to make some judgment call regarding the mean, you have to do it with our correct.

55
00:06:16,590 --> 00:06:19,680
Just be clear of these instance.

56
00:06:19,890 --> 00:06:21,959
These are the RockPile matters.

57
00:06:21,960 --> 00:06:28,650
If you want to make some assumptions or some conclusion, sorry about the mean, you need to use exponential those parameters.

58
00:06:29,520 --> 00:06:34,410
Okay, so that's just it. Fossil model. All right.

59
00:06:34,620 --> 00:06:37,649
Now we are going to feed us zero inflated.

60
00:06:37,650 --> 00:06:42,280
What someone. In order to see what inflated what what's on what.

61
00:06:42,320 --> 00:06:48,800
I'm going to do three things. The first one is going to be on how to feed them on the second one.

62
00:06:48,830 --> 00:06:55,910
When we whenever we are running sewer inflated for stormwater or even a sewer inflated negative binomial model,

63
00:06:57,080 --> 00:07:02,570
you have to take into account that what whatever we are doing is running two models in parallel.

64
00:07:02,930 --> 00:07:08,600
So for example, if I said we are going to feed a contemplated Poisson on water, it's because I have a Poisson model.

65
00:07:08,780 --> 00:07:13,120
And then I also have a logistic regression for the excess of zeros in that Poisson.

66
00:07:13,460 --> 00:07:25,820
Correct. Okay. So in order to feed a zero inflated plus all for a suspect feed that was inflated for some second set,

67
00:07:26,180 --> 00:07:30,920
I'm going to look at the logistic regression behind that zero inflated puzzle.

68
00:07:31,340 --> 00:07:35,120
And then I'm going to choose the significant covariates in that model.

69
00:07:35,480 --> 00:07:42,150
And then three. And then the first step is based on that, like, significant covariates.

70
00:07:42,170 --> 00:07:46,360
I'm gonna reseed this zero inflated plus. Okay, so.

71
00:07:46,390 --> 00:07:50,940
So your inflated almost always means that you need to rethink the model.

72
00:07:50,960 --> 00:07:57,530
Okay. So if you had once you look at the output of the logistic regression, you choose only the significant variables,

73
00:07:57,530 --> 00:08:02,290
and then you refit this zero inflated plus all based on those significant violence.

74
00:08:02,450 --> 00:08:05,450
Okay. Okay.

75
00:08:05,630 --> 00:08:14,240
So first, a set of those three steps, right, as you inflated fossil record looks exactly the same as before.

76
00:08:14,420 --> 00:08:19,070
The only difference is that here in the distribution, I don't write fossil.

77
00:08:19,100 --> 00:08:26,150
I write. I write as IPI. I'm sorry, c i b which is c translated was okay.

78
00:08:26,810 --> 00:08:33,290
And then here. So that's the first difference in the model center distributions, though.

79
00:08:33,290 --> 00:08:42,079
So you inflated Poisson. And then also, since we need to take into account the logistic regression that we are running behind here,

80
00:08:42,080 --> 00:08:44,330
we are also fitting the logistic regression.

81
00:08:44,540 --> 00:08:52,940
So, so to feed better logistic regression, use the command zero model and then here put all of the covariates.

82
00:08:53,270 --> 00:08:56,509
Notice that that's a big difference for all of the covariates.

83
00:08:56,510 --> 00:09:02,390
So I have wald females if they are smoking age on the other two covariates.

84
00:09:03,260 --> 00:09:12,800
Very important in the model statement. I have to make sure I have my outcome variable and then equal and then all of my covariates.

85
00:09:13,160 --> 00:09:18,950
However, in the zero model, which is the logistic model, I only need to list all of the credits.

86
00:09:19,250 --> 00:09:25,490
I do not need to list the outcome again. If you do it, then he's going to give you like some weird error.

87
00:09:26,030 --> 00:09:34,459
Does that make sense? Okay. When we run this model, we're going to have two outputs the outlook for the cold model,

88
00:09:34,460 --> 00:09:39,140
which will be the Poisson model, and the outcome for the logistic regression.

89
00:09:39,590 --> 00:09:46,280
Okay. So step one is just feed A0 inflated plus a step to look at the covariance.

90
00:09:46,700 --> 00:09:50,870
So like the significant covariance and then etc., etc., it's received them.

91
00:09:51,620 --> 00:09:58,190
So for this step two, we are going to look at the significant covariates in the logistic regression model.

92
00:09:58,310 --> 00:10:01,310
That's very important. So we have two outputs,

93
00:10:01,610 --> 00:10:09,550
the output for the count variable noise that I know that this is the output for the count variable because in the title it doesn't says anything,

94
00:10:09,590 --> 00:10:14,540
it doesn't specify anything, it just says analysis of maximal light estimation.

95
00:10:14,930 --> 00:10:24,410
However, the second table, it also it will say something like analysis of maximal living zero inflation parameter estimation,

96
00:10:24,710 --> 00:10:29,150
but zero inflation means that that's the output for the logistic regression.

97
00:10:29,960 --> 00:10:33,830
So based on this, I'll look for the logistic regression.

98
00:10:34,220 --> 00:10:36,440
I'm going to look at all of these P values.

99
00:10:37,070 --> 00:10:46,970
I'm going to select the significant P values, meaning that I'm going to select all of these that are less than 5%.

100
00:10:47,330 --> 00:10:56,930
In this case, if you notice that all of these values, the only one that is based on 5% is this variable, which is f ab one BP.

101
00:10:57,230 --> 00:11:05,510
And then I'm going to select these variable as my only significant predictor for the logistic regression in this inflated Poisson.

102
00:11:05,990 --> 00:11:11,180
Does that make sense? Very important. Is that in the second step, when you are choosing variables,

103
00:11:11,450 --> 00:11:18,280
make sure you're choosing variables based on the outcome of the logistic regression, not on the outcome of the fossil model.

104
00:11:18,530 --> 00:11:22,130
Okay, how about we feed the model?

105
00:11:25,330 --> 00:11:34,180
Only we received no notice that this it's the exact exact same code for the model.

106
00:11:34,210 --> 00:11:38,800
I have my outcome variable and all of the covariance, all of them.

107
00:11:39,250 --> 00:11:48,550
However, for this zero model, for the output, for the logistic regression, I only I'm only going to feed it with the significant predictors.

108
00:11:49,560 --> 00:11:53,100
Does that make sense? And then these will be your final model.

109
00:11:53,940 --> 00:11:59,760
These here will be your fight on what will be your zero financier inflated plus on.

110
00:12:00,650 --> 00:12:05,130
Does that make sense? Any questions so far? Okay.

111
00:12:05,760 --> 00:12:08,760
And then these last line, the output line.

112
00:12:09,000 --> 00:12:21,390
It's because we are trying to compare a four surrogate action with a C or inflated regression, and that's we are going to do it using not one test.

113
00:12:22,020 --> 00:12:25,709
So we need the output for those regressions.

114
00:12:25,710 --> 00:12:33,450
I'm going to do we need the output of those two regressions, the normal regression of the students at a phosphorylation for that test.

115
00:12:33,720 --> 00:12:40,410
So here I'm just saving the output for later using the test here.

116
00:12:41,610 --> 00:12:53,430
Okay. Once I have done this, which means I already run up so regression and I already run see translated for someone else.

117
00:12:53,880 --> 00:13:02,420
What I'm going to do is just. He's just run this source code.

118
00:13:02,420 --> 00:13:06,130
This is a macro dot. Calculate the room test.

119
00:13:06,400 --> 00:13:11,240
Okay. All of these lines of codes are exactly the same.

120
00:13:11,250 --> 00:13:15,229
You don't need to change because this is just fine. If that's what they want.

121
00:13:15,230 --> 00:13:19,400
That's the only thing that you need to take into account are these two numbers,

122
00:13:19,400 --> 00:13:26,360
nine and seven here again, because these are these depend on the parameters that you are using.

123
00:13:26,660 --> 00:13:32,150
So the first one corresponds to the plus all regression model.

124
00:13:32,510 --> 00:13:38,690
So if I look at the Poisson model, I have one penis, that's one parameter male.

125
00:13:38,690 --> 00:13:42,350
That's my second parameter, still is more in middle age.

126
00:13:43,430 --> 00:13:49,639
So one, two, three, four, five, six, seven and then seven.

127
00:13:49,640 --> 00:13:55,630
But this has to five are less than seven, eight and then US zero, that's 913 total.

128
00:13:56,150 --> 00:14:04,790
So wait, I'm sorry, this is one, two, three, four, five, six, six.

129
00:14:05,060 --> 00:14:08,720
Yes, seven variables in total. So that's the seven that I have here.

130
00:14:09,230 --> 00:14:15,140
And then for the logistic regression, I have to think one more model that has one variable.

131
00:14:15,440 --> 00:14:19,729
So that's nine, seven plus the other two. Because for the logistic regression,

132
00:14:19,730 --> 00:14:24,200
I have to account for the same variables because the variables in the logistic regression

133
00:14:24,440 --> 00:14:29,120
but in logistic regression we only encounter one significant variable plus the intercept.

134
00:14:29,390 --> 00:14:32,390
So that's seven plus two equal to nine.

135
00:14:32,570 --> 00:14:40,010
Does that make sense? So just make sure you change these two numbers depending on how many variables you having each one.

136
00:14:41,160 --> 00:14:45,680
Those that make sense. Okay.

137
00:14:47,190 --> 00:14:50,790
And then when I run that test.

138
00:14:53,860 --> 00:15:02,300
My output table will look something like this. We're going to pair the three different models that we are running here.

139
00:15:02,320 --> 00:15:08,410
We're going to choose the unadjusted in general. And then the other adjusted test will give you the test.

140
00:15:08,650 --> 00:15:17,260
It's got these three points. In this case, it's 4.2, the p value and then it's going to tell you which model do you prefer.

141
00:15:17,530 --> 00:15:23,980
In this case is that it is telling you that whatever you should use to fit the data is this zero inflated plus?

142
00:15:24,490 --> 00:15:31,630
So it's literally going to tell you here which model to use, either super inflated for sale or the Poisson regression itself.

143
00:15:33,680 --> 00:15:38,110
Does that make sense? Any questions so far?

144
00:15:42,600 --> 00:15:46,680
Okay. Then the big question is how do we do these in our.

145
00:15:46,890 --> 00:15:50,790
So remember, we are we are doing a lot of things.

146
00:15:50,790 --> 00:15:58,020
First, we are going to feed up what's our aggression. Then we're going to feed a series later for regression and for the second a step.

147
00:15:58,050 --> 00:16:01,140
I have to do three things, you know, feed this, you translate it.

148
00:16:01,290 --> 00:16:05,820
So then look at the covariance of the logistic and then rethink this, your inflated plus.

149
00:16:06,510 --> 00:16:12,630
And then there's a third this that is compare this to model step what's on the student inflated fossil using the view on best.

150
00:16:13,170 --> 00:16:19,320
Okay so how do we do all of that in um, so first thing first,

151
00:16:19,470 --> 00:16:26,700
let's just see a simple question regression from my formula, which is the same formula that we use like same.

152
00:16:29,790 --> 00:16:34,619
Yeah. The same structure that we used last week. So here I have my outcome variable.

153
00:16:34,620 --> 00:16:44,489
And then here are all of my predictors. And then to run a logistic regression, I'm when I use the Glynn function, I'm going to pause my formula.

154
00:16:44,490 --> 00:16:50,820
And then in formulae, I'm going to specify that I'm going to use opposite regression.

155
00:16:51,570 --> 00:16:57,630
The only difference between this code and last week's code is that last week here we have logit.

156
00:16:57,750 --> 00:17:02,010
Now we're going to feed up our regression. But other than that, it's exactly the same.

157
00:17:02,760 --> 00:17:07,950
Okay. When we said this was our regression, the outcome would look something like this.

158
00:17:08,280 --> 00:17:18,060
So for each of our variables, we have the estimate, the standard error, the Wald test, and then the P value.

159
00:17:18,630 --> 00:17:22,230
And then it's exactly the same case as we did before.

160
00:17:22,620 --> 00:17:25,970
And ah, these estimates are the wrong estimates.

161
00:17:25,980 --> 00:17:30,460
So for example, basis is beta one. This is not e to the power of veto one.

162
00:17:30,540 --> 00:17:31,590
These is beta.

163
00:17:32,670 --> 00:17:42,090
However, it seems we are running upwards or regression if you want to say if we want to conclude something or the mean if you need to exponent,

164
00:17:42,090 --> 00:17:46,430
change those properties. Correct. Oh, and I will be.

165
00:17:46,640 --> 00:17:52,540
So this is the first step. Just run a little logistic regression and save the output, and that's it.

166
00:17:52,820 --> 00:17:59,530
How am I saving the output? Just a New Zealand function asylum in this case minus the wasn't.

167
00:18:01,920 --> 00:18:05,510
Okay so going to step we are going to see plus your inflated what's.

168
00:18:06,300 --> 00:18:10,530
So the feet are still inflated plus we need a new library.

169
00:18:11,010 --> 00:18:19,440
So the first thing is make sure you install these new perpetual PCL and then called the progression through your script.

170
00:18:19,590 --> 00:18:25,750
Okay. Because that glm function does not allow us to run simulated fossil.

171
00:18:26,000 --> 00:18:37,850
So we need to install a new library above the first and then and then we are going to create the formula for our inflated.

172
00:18:38,340 --> 00:18:42,930
So the formula looks very similar to the formula before I have my outcome here.

173
00:18:43,380 --> 00:18:47,040
And then I have this field and then I have all of my variables.

174
00:18:47,640 --> 00:18:54,300
And then after all of my variables, I'm going to include this symbol, which is like a vertical line.

175
00:18:56,280 --> 00:19:02,220
But in your keyboard, you should find by holding shift and then press the button.

176
00:19:03,000 --> 00:19:11,640
Okay. And then after this line, I'm specifying all of the covariance that will go into the logistic regression.

177
00:19:12,060 --> 00:19:16,710
In this case, since this is a step one, I want to include all of the covariance.

178
00:19:17,130 --> 00:19:20,700
Then I'm going to look at the output and then I'm going to repeat them all based on the output.

179
00:19:21,120 --> 00:19:24,510
So in this case, here I have my outcome all of my covariance.

180
00:19:24,810 --> 00:19:31,590
Then I have the vertical line and then I have all of the covariance for the both software, the logistic regression.

181
00:19:32,190 --> 00:19:36,280
Okay. And then to run the pseudo inflated plus plus.

182
00:19:36,280 --> 00:19:43,260
So in this library there is a function that is goal zero, use e i NFL.

183
00:19:44,760 --> 00:19:47,850
The first thing is going to require this formula here.

184
00:19:48,360 --> 00:19:55,530
Then the distribution is going to be Poisson. Okay. Because we can also see, for example, negative binomial is inflated models.

185
00:19:56,310 --> 00:20:02,880
So the distribution is going to be a on and then the data is the dataset that we already have loading columns.

186
00:20:03,750 --> 00:20:10,110
And then when you run this function and you do the summary of this function that I have here,

187
00:20:10,110 --> 00:20:20,160
feed the fossil feed that C translated the plus off the output looks something like this and then you have to have two big tables.

188
00:20:20,460 --> 00:20:22,830
The first table, it's for the count,

189
00:20:22,920 --> 00:20:32,310
which means which means the first table is giving you the parameters for the regression for the Poisson regression model and then a second table.

190
00:20:34,420 --> 00:20:39,460
That will look something like this. It's going to be it's going to have these like money symbols here.

191
00:20:40,750 --> 00:20:44,710
It's going to give you the estimates for the logistic regression behind.

192
00:20:45,370 --> 00:20:52,600
Okay. Based on this table, based on the logistic regression, I'm going to look at the P values.

193
00:20:53,350 --> 00:20:58,240
I'm going to select the significant variables in this case, same result.

194
00:20:58,270 --> 00:21:06,100
The significance, the only significant variable in this case is this predictor f, a, v, one p,

195
00:21:06,460 --> 00:21:15,280
and then I'm going to receive this inflated Poisson, but with the condition that the logistic regression would only take into account discovery.

196
00:21:15,670 --> 00:21:19,630
Okay. Does that make sense? Okay.

197
00:21:20,440 --> 00:21:25,630
So start this step. Rethink the serial inflator model.

198
00:21:26,080 --> 00:21:30,190
Look at my formula. It changed a lot for the regression model.

199
00:21:30,190 --> 00:21:35,170
I have my outcome on all of my covariance and then I have these vertical lines.

200
00:21:35,410 --> 00:21:39,220
And after the vertical line and all, I only have the.

201
00:21:42,080 --> 00:21:50,570
They significant variable. Okay. And then I repeat that, um, I just you again, I see your inflated, um, function.

202
00:21:50,840 --> 00:21:56,270
I mean, you formula mixing usually equal plus on is the same data set.

203
00:21:56,420 --> 00:22:00,650
And then I just look at the output. Does that make sense?

204
00:22:01,770 --> 00:22:07,510
Anyway. She knows how to do it on our. Oh, great.

205
00:22:08,270 --> 00:22:15,280
Oh. Okay. I'm in for the long test.

206
00:22:15,640 --> 00:22:21,730
That same library. Remember that in r you need to install these library.

207
00:22:21,970 --> 00:22:27,130
P s c l these same library has a function for the boring press.

208
00:22:27,670 --> 00:22:32,100
And then I'm just about to pass out from my two models this year.

209
00:22:32,110 --> 00:22:39,490
Inflate a model and then the plus one and that C and the output of that function will look something like this.

210
00:22:40,540 --> 00:22:48,190
We are going to see. R Well, I'll take a look into the first row which is called over there real life,

211
00:22:48,970 --> 00:22:56,260
the estimation of that one test and then here is going to give you the p value.

212
00:22:56,770 --> 00:23:05,470
In this case, I look at the people in the region who hypothesis these output output us that the first model when we set

213
00:23:05,590 --> 00:23:12,010
the function in this case notice up the order API and we first we see translated plus so it's better.

214
00:23:12,400 --> 00:23:16,510
Okay, how do I know that it's the only model one.

215
00:23:16,510 --> 00:23:23,560
It's bigger than model two which means is better model one for me is the first one like positive so far.

216
00:23:24,940 --> 00:23:28,180
So make sure you're taking into account those things.

217
00:23:30,160 --> 00:23:35,730
Questions so far. All right.

218
00:23:37,800 --> 00:23:42,150
That's everything for what's our regulation and steering clear of buzzword regression.

219
00:23:42,630 --> 00:23:47,790
Now, let's take a look and do negative right on those models.

220
00:23:48,270 --> 00:23:56,160
So for negative binomial, we are going to call it the road negative polynomials models and also negative binomial with its plans.

221
00:23:56,580 --> 00:24:03,390
So first, let's just feed our regular negative binomial model without response, just a basic negative binomial model.

222
00:24:03,960 --> 00:24:15,390
So what we are what we would like to do is the following first three are negative binomial model and then feel disabled.

223
00:24:16,070 --> 00:24:20,880
That's it. But what you want to do feed a negative binomial model and see now this table.

224
00:24:20,940 --> 00:24:23,549
This table is I want you to estimate,

225
00:24:23,550 --> 00:24:36,330
put people in power of 0.25 and then beta for my predictor of intrinsic which is healthy and then the 95% confidence interval of this thing.

226
00:24:36,870 --> 00:24:44,220
So I want to be able to estimate this. Now, this is not trivial because the output for all of these models is Josie Beta.

227
00:24:44,460 --> 00:24:51,030
So I need to do something because I want the estimate of E to the power of 2.25 beta and also the confidence interval.

228
00:24:51,480 --> 00:24:58,860
So the question is by itself not trivial. And then based on this, I want to test the null hypothesis.

229
00:24:59,100 --> 00:25:07,559
I want to test these hypotheses. My normal requirement is that you do a power of 0.25 be my beta.

230
00:25:07,560 --> 00:25:11,030
For one thing, this is equal to one or the other.

231
00:25:11,310 --> 00:25:15,150
Hypotheses hypothesis is that these features are available, but it's different than one.

232
00:25:15,450 --> 00:25:18,990
So I also want to be able to test for those things. Okay.

233
00:25:19,470 --> 00:25:25,920
So first let's just feed on negative binomial order. The logic behind source is pretty similar.

234
00:25:27,150 --> 00:25:32,190
So in this case I have exactly the same as I did before I have my class.

235
00:25:32,190 --> 00:25:36,430
A statement just for referencing my is common variable which is identical.

236
00:25:36,810 --> 00:25:41,190
I have my model, which is my outcome against all of my predictors.

237
00:25:41,520 --> 00:25:46,200
And then here this deviation is going to be B for negative binomial.

238
00:25:46,650 --> 00:25:59,340
That's the only difference. Until here, if I would have that run line here, this will only fit the negative binomial model.

239
00:25:59,730 --> 00:26:05,880
However, I want also the estimate of e to a power of 0.25 based on.

240
00:26:05,970 --> 00:26:11,130
All right. So this part here, these estimates here, it's doing that.

241
00:26:11,910 --> 00:26:18,630
So this first part is just going to feed on. I got the binomial this estimate second part right before the run.

242
00:26:18,900 --> 00:26:22,470
It's doing the e to the power of 0.25 beta.

243
00:26:23,520 --> 00:26:35,610
Okay, so what we're doing here is remember that we want to be able to estimate this thing even if our of 0.25 made our world famous.

244
00:26:36,540 --> 00:26:41,130
So what we are going to do in source is first uses estimate.

245
00:26:45,000 --> 00:26:49,740
Come on. I went to school on that estimate.

246
00:26:50,280 --> 00:26:54,660
It's very flexible and will allow us to basically estimate anything that we want.

247
00:26:55,380 --> 00:26:59,940
How was this actually made? What is that for each of the predictors so far?

248
00:26:59,970 --> 00:27:06,120
Our model. I'm going to give you a number. So, for example, this is my formula, right?

249
00:27:07,770 --> 00:27:11,730
I have my output and then this is equal to an intercept.

250
00:27:12,860 --> 00:27:22,460
Glass made one for wall thickness floors, made a two for H.

251
00:27:23,180 --> 00:27:28,040
APPLAUSE Better three I think is male, so on and so forth.

252
00:27:28,700 --> 00:27:37,219
So if I want the estimate of E to 0.25 beta thickness made a wall thickness.

253
00:27:37,220 --> 00:27:41,180
I'm sorry. This estimate function works us.

254
00:27:41,480 --> 00:27:48,140
I need to be able to tell for each of my covariance a number.

255
00:27:48,680 --> 00:27:56,900
So to create 0.25 vedha v of wall thickness, that doesn't include the intercept.

256
00:27:56,960 --> 00:28:02,120
So intercept I'm going to set it to zero. That's that does not include H.

257
00:28:02,270 --> 00:28:06,230
So the H variable, it's also going to be set at zero.

258
00:28:06,620 --> 00:28:11,570
That does not include male or sex or gender in this case.

259
00:28:11,720 --> 00:28:18,290
I'm also going to fit this to zero and all of the other covariates are also fit to zero because I don't need them.

260
00:28:18,290 --> 00:28:23,629
In this case, the only one that it needs its wall thickness and wall thickness.

261
00:28:23,630 --> 00:28:27,020
I need C 0.25 wall thickness.

262
00:28:27,230 --> 00:28:32,150
So that means that I'm just going to set well the units to be 0.25.

263
00:28:33,080 --> 00:28:39,230
That will eat you on the amount that will review the estimate of 0.25 beta world thickness.

264
00:28:39,530 --> 00:28:44,480
However, I need to the powers that be so to do it is a part of the thing.

265
00:28:44,870 --> 00:28:49,450
Backslash x2x1. Initiate backslash.

266
00:28:49,640 --> 00:29:01,880
It's too expensive. Does that make sense? Makes sense.

267
00:29:02,490 --> 00:29:05,490
Okay. Notice, this is very flexible.

268
00:29:06,240 --> 00:29:18,930
So, for example, if I would ask you to estimate for some reason, let's say that makes sense in this is in our U e to the power of four times h.

269
00:29:22,450 --> 00:29:32,139
What would change in the code is that in this case, this wall thickness will set to zero and then H will be set to four and then to exponential.

270
00:29:32,140 --> 00:29:35,800
To exponential. I have backslash p xp.

271
00:29:36,190 --> 00:29:44,470
All right. Does that make sense? So it's very flexible. You can test like basically anything that you can make linear reporting of here.

272
00:29:45,820 --> 00:29:55,730
Okay. When I run that, I'll have two outputs.

273
00:29:55,880 --> 00:30:01,970
The first output is the negative binomial model, and then the second output is the estimate.

274
00:30:01,970 --> 00:30:05,160
E to the power of c 2.25 Vega.

275
00:30:05,780 --> 00:30:13,840
Well, so from here I have figured all of these estimates that in half are right.

276
00:30:13,850 --> 00:30:20,030
Estimates are. The beta one had better translate as you have, however.

277
00:30:20,040 --> 00:30:25,970
Well, I also have the standard error, the confidence interval, the Wall Chi Square test,

278
00:30:26,660 --> 00:30:31,640
which will tell you the hypothesis whether those estimates are equal to zero or not.

279
00:30:31,940 --> 00:30:35,490
And the p value. Right. On that. Well, that's it.

280
00:30:36,000 --> 00:30:44,330
And then for my hypotheses e to the power of 0.0. 25 B to the power of veto be missed.

281
00:30:44,520 --> 00:30:54,930
I'm going to have one more table that is called because one does estimate result on for each of the estimates here if you.

282
00:30:55,980 --> 00:31:03,450
So this is just a small example because in this example, I only require you to do it to the power of 0.25 beta.

283
00:31:03,720 --> 00:31:10,920
However, if you were to estimate that plus I also when I estimate E through a power of four times h,

284
00:31:11,130 --> 00:31:14,100
you can create like multiple estimates here, not only one.

285
00:31:14,550 --> 00:31:20,810
In that case, this table will have multiple rows, one for each, like estimate that you want.

286
00:31:20,820 --> 00:31:34,520
In this case, since I only want one, I estimate well, I only get my two rows one for 0.25 more P and then one for e e s p p to the power of 0.25 to.

287
00:31:34,530 --> 00:31:41,850
Okay. Okay. And then here you have the estimate and then the 95% confidence interval.

288
00:31:42,600 --> 00:31:48,190
This means that. The way to read this table would be that.

289
00:31:50,300 --> 00:31:55,910
E to the power of 0.25 meter wall thickness.

290
00:31:57,260 --> 00:32:00,750
So they seen the estimate of this thing.

291
00:32:01,070 --> 00:32:05,960
It's 1.18 and the confidence interval of that thing.

292
00:32:06,230 --> 00:32:10,460
It's 1.03 and 1.35.

293
00:32:12,270 --> 00:32:16,610
Does that make sense? Okay. Question.

294
00:32:18,190 --> 00:32:21,620
Let's say. I have the hypothesis.

295
00:32:24,710 --> 00:32:34,040
Oops, I have my null hypothesis. Is that e to the power of 0.25 b wall thickness is equal to one.

296
00:32:34,520 --> 00:32:39,830
And my alternative hypothesis is that nothing it's different than one.

297
00:32:40,290 --> 00:32:48,410
Okay. Based on these outlooks, do I reject or do I fail to reject?

298
00:32:53,870 --> 00:33:00,540
What? Do I reject or do I fail to reject only based on this output?

299
00:33:05,000 --> 00:33:09,470
Reject. Right. And how can I tell just by looking at this output?

300
00:33:09,890 --> 00:33:14,360
Because the confidence interval does not have the one right.

301
00:33:14,630 --> 00:33:18,710
It goes from 1.03 to 1.35.

302
00:33:18,980 --> 00:33:23,540
This interval does not contain one. Since the interval does not contain one,

303
00:33:23,540 --> 00:33:31,040
which is like the thing that I'm facing here to the right hand side of the new hypothesis, I reject the hypothesis.

304
00:33:31,520 --> 00:33:34,910
Okay. If, for example, this.

305
00:33:37,010 --> 00:33:40,760
Lower bound of the confidence interval will be 0.9.

306
00:33:41,180 --> 00:33:45,920
In that case, my example would have the one. So I will fail to fail to reject.

307
00:33:46,340 --> 00:33:51,590
Okay. So remember that you can also do not reject based on my confidence intervals.

308
00:33:52,940 --> 00:33:57,470
Okay. Now how we do all of these?

309
00:33:58,730 --> 00:34:01,860
All right. Yes.

310
00:34:01,880 --> 00:34:10,070
All right. So we need this package to do the contrast matrix for each of the bar of 0.25 theta data.

311
00:34:11,210 --> 00:34:16,420
And then first, I'm going to see negative final model.

312
00:34:16,460 --> 00:34:26,580
I have my formula here, which is exactly the same formula is as I use before my out of all of my predictors, I'm going to feed on that the binomial.

313
00:34:26,600 --> 00:34:30,500
I'm using the formula and the negative minus.

314
00:34:30,860 --> 00:34:36,410
Okay. The GNN function does not allow us to feed on negative binomial.

315
00:34:36,680 --> 00:34:41,150
We need to use glenn the negative and the formula.

316
00:34:41,480 --> 00:34:47,330
My formula here and then my data. And that's when I run that line.

317
00:34:49,250 --> 00:34:51,640
I have a table that looks something like this.

318
00:34:51,650 --> 00:35:02,390
I have all of my Bible, my covariates here, my estimates, the standard errors, you know, the Walters for each of my Bibles.

319
00:35:02,840 --> 00:35:09,800
And then here in this other part of the code, I'm not showing you the output here.

320
00:35:09,830 --> 00:35:14,060
I just have it here.

321
00:35:14,800 --> 00:35:19,800
And you just change colors. This line right here.

322
00:35:21,630 --> 00:35:26,640
What I'm doing here is that I'm taking the exponent of all of these parameters.

323
00:35:27,000 --> 00:35:31,690
Okay. That's why I'm doing that. Just because, remember, these are real parameters.

324
00:35:31,710 --> 00:35:36,480
If I want to conclude something about the mean, I have to do e to the power of that thing.

325
00:35:36,600 --> 00:35:39,690
So that's why I'm deleting that line. Okay.

326
00:35:40,260 --> 00:35:48,150
And then to test for E to a power of 0.25 beda will take this.

327
00:35:49,200 --> 00:36:00,150
The logic is more or less the same. I need this package to create my complex matrix and then the logic behind this is basically the same.

328
00:36:01,230 --> 00:36:08,010
So for each of the parameters of my model, I'm going to set my variables either zero one or the value that I want.

329
00:36:08,820 --> 00:36:13,900
In this case, you have to make sure that. The numbers that you've said here.

330
00:36:15,350 --> 00:36:20,240
A line in the exact order to the variables that you have here.

331
00:36:20,480 --> 00:36:27,680
Okay. And remember that the first number that you said here, it's for The Intercept.

332
00:36:28,880 --> 00:36:32,720
And then after that, it's all of your variables. So for example.

333
00:36:32,800 --> 00:36:40,820
Oops, my first variable is Walgreens. So zero for dangerous and 3.25 for wall payments.

334
00:36:41,270 --> 00:36:44,750
And then all of my other variables are set to zero.

335
00:36:45,230 --> 00:36:49,100
Correct. And then. Okay.

336
00:36:50,330 --> 00:36:56,000
And then to, you know, actually create the estimate of E to the power of that thing.

337
00:36:56,420 --> 00:36:59,670
I'm going to use the function g lhc.

338
00:37:00,770 --> 00:37:06,520
It's going to. Is going to ask you for two things.

339
00:37:06,520 --> 00:37:14,350
The first wife, because then they got the binomial model. So these number that these names that I have here for negative binomial minus plane

340
00:37:14,830 --> 00:37:22,330
is just might make up his mind on but is just my name is by no one and then is

341
00:37:22,330 --> 00:37:27,489
going to ask you not only for the final number but also for like all of those

342
00:37:27,490 --> 00:37:33,580
numbers to set the contrast my face which I have here in my contract in this model.

343
00:37:33,790 --> 00:37:39,250
So that's what I have here. And then really only positive here.

344
00:37:39,370 --> 00:37:44,770
Like if you do it like this, it will give you the estimate of 0.25 billion.

345
00:37:45,040 --> 00:37:51,460
However we have we want you to a power of that thing. So here what I'm doing is that it's exponentially.

346
00:37:52,570 --> 00:38:01,330
Because I was eating a out of that thing. So these line here will give you the E to the power off.

347
00:38:04,310 --> 00:38:07,670
All right. Does that make sense?

348
00:38:07,820 --> 00:38:16,380
So far. Good. The outcome of this function here will look something like this.

349
00:38:17,250 --> 00:38:26,560
You have your power of 0.25 meter wall thickness and then the 95% confidence interval.

350
00:38:31,700 --> 00:38:43,370
Just for the sake of comparison, you can see that these values are not exactly the same size being one of you having the size,

351
00:38:43,640 --> 00:38:49,100
but they are pretty, pretty close. And the only difference is just a minor difference in the functions that they use.

352
00:38:49,310 --> 00:38:53,810
But they are pretty close. I mean, they are close to like two before decimal place or something else.

353
00:38:54,050 --> 00:39:00,080
So it's not exactly the same, but they are pretty, pretty close. So like I said, you can use SAS or R whichever.

354
00:39:00,500 --> 00:39:04,700
It's going to be fine. Okay then.

355
00:39:05,960 --> 00:39:09,500
We sit on our table. We have our estimate.

356
00:39:11,000 --> 00:39:19,760
1.18 49 and then we also have our confidence interval based on the confidence interval.

357
00:39:19,790 --> 00:39:24,470
We conclude that we want to reject the null hypothesis because this contains one,

358
00:39:26,300 --> 00:39:31,880
and then that interpretation of this thing will be something this variable.

359
00:39:31,910 --> 00:39:47,000
WOLPE This is measuring. MM So for every 0.25 MM increase in the variable in the mix, then the equations in my dataset are expected to half.

360
00:39:47,000 --> 00:40:01,700
And then my estimate 1.18, 14, 49 times more reported outcome in my cases it's also relations adjusted for the other variables see them.

361
00:40:02,270 --> 00:40:13,879
So for every 0.25 increase in my variable in the wall thickness, my subjects as subjects in my dataset,

362
00:40:13,880 --> 00:40:20,540
my patients are expected to have 1.8 49 times more exacerbations.

363
00:40:20,540 --> 00:40:26,150
Remember that exacerbations is our outcome and then adjust for the other factors.

364
00:40:26,240 --> 00:40:31,940
I'll remember that we need to adjust for it because we are taking into account compared to them on.

365
00:40:34,110 --> 00:40:41,480
Any questions so far? Okay.

366
00:40:42,980 --> 00:40:46,520
So here. I'm just.

367
00:40:48,080 --> 00:40:51,520
From. Here to hear.

368
00:40:52,060 --> 00:40:57,100
This is just the same that we said before, 4.25 millimeter increase.

369
00:40:57,110 --> 00:41:05,860
Increase. And then this first part is just the analysis we did about whether this is a significant result or not.

370
00:41:05,980 --> 00:41:10,720
Okay. So that's like we that we've all the already with the confidence interval, we can conclude that.

371
00:41:12,610 --> 00:41:16,930
Okay. Example belts all for an active mine.

372
00:41:17,050 --> 00:41:22,840
You know, we want to run a negative binomial with its playing terms.

373
00:41:23,000 --> 00:41:30,800
Okay. So. In this case, we want to rerun that same negative binomial model.

374
00:41:31,160 --> 00:41:35,330
But we want. We would like to add some explaining to tell parents.

375
00:41:35,330 --> 00:41:43,670
So that continues. While they do not want to see 1.4 on the other one meeting.

376
00:41:43,670 --> 00:41:51,129
1.7. This is his key.

377
00:41:51,130 --> 00:41:54,280
And what is what is it clear?

378
00:41:54,310 --> 00:41:59,190
What is slime means? Okay.

379
00:41:59,580 --> 00:42:07,649
I'm going to give you a small explanation of a slight not in the negative binomial, but in the linear model in general,

380
00:42:07,650 --> 00:42:12,300
because I think it's easier to understand that way, but it works the same way for the other variables.

381
00:42:15,330 --> 00:42:29,010
So let's, let's say, okay, let's say you have a simple linear regression model like this, simple linear regression.

382
00:42:29,520 --> 00:42:33,329
I mean, it works for negative binomial and for for Poisson for the others.

383
00:42:33,330 --> 00:42:41,850
But I think it's simpler to use it in a simple linear regression model under my.

384
00:42:48,450 --> 00:42:59,770
And then I feed my data. And then I feed the data and it looks something like this.

385
00:42:59,820 --> 00:43:04,020
So this is only my dataset because these are all like dots in my dataset.

386
00:43:05,250 --> 00:43:10,440
If I were to do a linear regression model, remember that it's linear.

387
00:43:10,740 --> 00:43:17,100
So like, I can only feed something by, like, either inside, like, or maybe slightly or maybe even like this.

388
00:43:17,580 --> 00:43:24,600
However, if you look at this like pattern right here, I can try to approximate these using two lines.

389
00:43:26,640 --> 00:43:36,780
One line that goes like here, for example, and that other line that goes like here it's the same linear because I'm feeding these using lines.

390
00:43:37,260 --> 00:43:41,400
So this is my first line. I'm using my second line. This is called last line.

391
00:43:42,060 --> 00:43:47,400
So I'm using a linear regression model, but instead of feeding one line, I'm going to feed two lines.

392
00:43:48,000 --> 00:43:51,570
And these value right here in my covariate.

393
00:43:52,770 --> 00:43:58,060
It's call these value right here.

394
00:43:59,730 --> 00:44:07,940
It's called not. So, for example, when they tell you one, when this tells you like feed our binomial model, we do not.

395
00:44:08,040 --> 00:44:12,779
It means that I want like something like this maybe doesn't need to be like these drama.

396
00:44:12,780 --> 00:44:22,020
I think this is just like to make sense of what we are doing. But I want to feed something linear before 1.4 and then after 1.4.

397
00:44:22,270 --> 00:44:25,440
Okay, wait. I have two knots.

398
00:44:25,710 --> 00:44:33,600
Then I have like three partitions before 1.4 between 1.4 and 1.7 and then after 1.7.

399
00:44:34,320 --> 00:44:39,570
Does that make sense? That's what we are doing here in order for this to make sense.

400
00:44:39,870 --> 00:44:44,330
Not is that I'm dividing my covariate. I'm not dividing my outcome.

401
00:44:44,340 --> 00:44:50,640
That's very important. I'm dividing my covariate. This not meeting the x axis, not in the Y axis.

402
00:44:51,100 --> 00:44:55,020
And for this to make sense, this covariate needs to be a continuous one.

403
00:44:55,470 --> 00:44:58,890
That. That is why we are using wall thickness. Because it's continuous.

404
00:44:59,430 --> 00:45:09,890
Does that make sense? Oh. So we want we will try to create some explains for the negative binomial model.

405
00:45:11,960 --> 00:45:19,110
So. Great those supplies. The first thing I'm gonna do is my data set right now looks something like this.

406
00:45:19,500 --> 00:45:23,510
So I have y I have wall thickness.

407
00:45:23,520 --> 00:45:27,300
I have age. I have male, and I have other covariates.

408
00:45:27,660 --> 00:45:34,050
What I'm going to do is I'm going to add two terms to that data set one for each plan.

409
00:45:34,180 --> 00:45:40,680
Okay. So this is my dataset and I would like to add two more violence to that dataset.

410
00:45:40,740 --> 00:45:44,100
One for each plan. How do I read this line?

411
00:45:44,130 --> 00:45:51,120
It's very easy. So this is so for example, the phrase is blank at one point for.

412
00:45:53,300 --> 00:45:57,560
So what I'm doing here is if the right.

413
00:45:57,650 --> 00:46:01,940
Maybe this is easier with a number right here.

414
00:46:05,080 --> 00:46:11,290
Okay. So let's say I have one subject in my dataset which was thickness value.

415
00:46:11,290 --> 00:46:17,080
My X variable, which I would like to make offline is flying on has a value of 0.9.

416
00:46:17,410 --> 00:46:22,420
That means that my first discipline is plane number one will have a value of zero.

417
00:46:23,830 --> 00:46:29,740
And then if it is 1.5, I would like this to be 0.1.

418
00:46:30,710 --> 00:46:37,160
Why? Because this is 1.5 -1.4.

419
00:46:37,760 --> 00:46:46,990
Okay. But then if it is less than 1.4, I would like to be I would like to be I would like to that would be a zero.

420
00:46:47,470 --> 00:46:54,880
And then if it is greater than 1.4, I want the difference between the actual value and then my not, which is 1.4.

421
00:46:55,090 --> 00:46:58,210
Okay. So that's what I'm doing here.

422
00:46:58,270 --> 00:47:06,970
First, I ask whether those variables are greater than 1.4 and if they are being assigned the actual value.

423
00:47:07,330 --> 00:47:10,750
Example 1.5 minus might not. Okay.

424
00:47:10,900 --> 00:47:20,280
Otherwise it's just going to assign us. You go those that make sense and then are for the is flying at 1.7.

425
00:47:20,290 --> 00:47:26,680
I'm going to do exactly the same. So let's say, for example, for my second test plane.

426
00:47:29,480 --> 00:47:33,320
This person here that has a wall thickness of 0.9.

427
00:47:33,740 --> 00:47:42,710
The second line is also going to be zero. But the second person who has a wall thickness of 1.5, 1.5.

428
00:47:42,740 --> 00:47:51,590
The second line, it's also going to be a zero. But then if I have someone in my dataset, for example, weed out 1.8,

429
00:47:52,040 --> 00:48:00,680
then this person is going to have a 0.4 in the first line and then 0.1 in the second line.

430
00:48:00,770 --> 00:48:01,520
Does that make sense?

431
00:48:03,710 --> 00:48:14,930
And so for my first six life, if the value of the world being less is better than mine for a sign, see if it is greater than one point for a sign.

432
00:48:14,930 --> 00:48:22,399
The difference between the value on 1.4 I'm 47 is like saying much if the value

433
00:48:22,400 --> 00:48:28,100
of the world thickness is less than 1.790 and if it is greater than 1.7,

434
00:48:28,100 --> 00:48:34,910
a sign the difference between the value of the world thickness on the value of this 1.7.

435
00:48:37,200 --> 00:48:39,900
Does that make sense with these numbers that I have here? Okay.

436
00:48:40,290 --> 00:48:46,200
So what I'm doing here is just adding those few new columns or variables into my dataset.

437
00:48:47,160 --> 00:48:56,370
And then once I have that, once I have that, what I'm going to do is run a negative binomial.

438
00:48:56,670 --> 00:49:00,540
So then I have the binomial. Let's use this.

439
00:49:00,870 --> 00:49:04,560
This looks very. This looks like a lot.

440
00:49:05,150 --> 00:49:09,000
Let's just. Let's just focus firstly on this board.

441
00:49:10,200 --> 00:49:16,520
Okay. So. So these four right here is just creating the negative binomial model.

442
00:49:16,910 --> 00:49:21,770
So I have my model is saying this is my outcome and then I have all of my covariance.

443
00:49:22,160 --> 00:49:28,460
But notice. But I added my two new variants so I have one less which is here.

444
00:49:28,730 --> 00:49:32,760
And I also have my first test flight and I also have my second flying.

445
00:49:33,050 --> 00:49:37,580
So I just them as covariates in my model that T and that creates.

446
00:49:37,730 --> 00:49:42,840
I got the binomial test lines. That's all. Okay.

447
00:49:43,170 --> 00:49:49,840
And then here I just have this division negative, like. But they made up the triangle model.

448
00:49:51,130 --> 00:50:03,730
Now what we would like to do, it's to remember the E to the power of 0.25 beta wall thickness.

449
00:50:05,020 --> 00:50:08,620
So we will like to estimate this again.

450
00:50:09,070 --> 00:50:17,799
However, when I have a splint, that's kind of a problem because I'm dividing this outcome because I'm sorry,

451
00:50:17,800 --> 00:50:26,410
this covariate was wanting this into three parts before 1.4 between 1.4 and 1.7, and then after one point.

452
00:50:27,160 --> 00:50:30,910
So for each part, I need to create a new estimate, a state.

453
00:50:31,270 --> 00:50:35,710
That's why here I have three estimates for before 1.4.

454
00:50:36,160 --> 00:50:40,630
Between 1.4 and 1.7. And then after 1.7.

455
00:50:41,320 --> 00:50:47,950
Okay. So that's why here I have the three different mistakes. Does that make a little bit of sense?

456
00:50:48,670 --> 00:50:52,390
And then notice that the logic of this is exactly the same.

457
00:50:52,600 --> 00:50:56,410
So here are the three. I'm just going to put these here.

458
00:50:57,130 --> 00:51:03,310
This is before 1.4. This is 1.4 and 1.7.

459
00:51:04,480 --> 00:51:10,090
Then this is greater than 1.7.

460
00:51:10,720 --> 00:51:18,550
So what I'm doing here is for the 1.4, I need one thing less because that's my base variable and then neither of these plans.

461
00:51:19,640 --> 00:51:24,980
Because he's before 1.4. 41.4 2.47.

462
00:51:25,100 --> 00:51:31,130
I needed wall thickness on my first line because I need 1.4 to take value but not the other one.

463
00:51:31,580 --> 00:51:34,880
And then greater than 1.7, I need my previous ones.

464
00:51:35,480 --> 00:51:40,010
My three days will be less. And then the first is slide and then the second line.

465
00:51:40,620 --> 00:51:47,690
All right. But the logic is exactly the same. All of the other viable ports are to zero because that's like I'm not interested in that.

466
00:51:50,290 --> 00:51:55,270
Okay. And then the outcome the outcome will look something like this.

467
00:51:55,390 --> 00:52:02,920
First of all, you have your maximal likelihood table, which include all of the estimates in my model.

468
00:52:03,580 --> 00:52:08,010
Notice that I have estimates for each of these slides. This is a huge table.

469
00:52:08,020 --> 00:52:12,910
We just included the first rows and then the whole important part is the second one.

470
00:52:15,280 --> 00:52:20,890
So in this case, notice that I have more rows.

471
00:52:21,580 --> 00:52:29,770
The first on the second one corresponds to 0.25 more thickness and then 0.25 fold thickness on my first slide.

472
00:52:29,770 --> 00:52:36,640
And then 0.25. My first my first line on my second line on the logic is exactly the same.

473
00:52:37,990 --> 00:52:43,180
Please note is that these column,

474
00:52:43,180 --> 00:52:52,450
these names are telling you whether they are row estimates or e to the power off and we are interested in E to the power off.

475
00:52:52,750 --> 00:52:58,180
That's why I am only using these things that are easy instead of the other ones.

476
00:52:58,450 --> 00:53:02,110
And then I have my estimates on my confidence intervals.

477
00:53:02,800 --> 00:53:06,790
Okay. I am going to be analysis of these is exactly as we did before.

478
00:53:07,060 --> 00:53:17,740
So for example, in this case, 1.16 will be the S for every 0.25 to increasing times.

479
00:53:18,160 --> 00:53:23,980
In all vagueness, I would expect my patients to have 1.16 times more.

480
00:53:24,040 --> 00:53:31,780
My outcome, more my outcome, if they're wanting this is less than 1.4 because that's the first one.

481
00:53:32,620 --> 00:53:43,900
Okay. However, for example, here, this shouldn't be 1.7 and should be 1.4 on 1.7.

482
00:53:45,850 --> 00:53:53,559
However, if I'm interested in someone who has like I want the year between that value, then instead of using this,

483
00:53:53,560 --> 00:54:01,420
I will use the second one because that's my estimate for people who are in that range of wall thickness.

484
00:54:03,080 --> 00:54:13,120
Oh. In our.

485
00:54:14,860 --> 00:54:18,780
We will do this in. A similar way.

486
00:54:18,800 --> 00:54:24,470
So the first thing we're doing is we are creating variables into the data set before these slides.

487
00:54:24,740 --> 00:54:34,790
So here we have the same biological logic. If one thing this is greater than 1.4, then I see my value -1.4.

488
00:54:35,000 --> 00:54:40,160
Otherwise it's S0 and then the same for 1.6, but just creates this lines.

489
00:54:40,640 --> 00:54:46,120
And then once I create this pipelines to run the models just add those variables to the model,

490
00:54:46,970 --> 00:54:51,950
just add them plus my first value on my second variable and run them all on that.

491
00:54:51,950 --> 00:54:55,969
C And then you see I make up the binomial model,

492
00:54:55,970 --> 00:55:04,190
I have my formula and then here I'm just X1 initiated because I have the rover row estimates and if I would like to make inference,

493
00:55:04,190 --> 00:55:16,550
I need to explain those things. We have our full set of our table with all of our estimates and then more importantly

494
00:55:17,810 --> 00:55:23,290
to run those like confidence matrix in our what we need to do is directly the process.

495
00:55:23,300 --> 00:55:24,950
So for example, for the first one,

496
00:55:25,460 --> 00:55:33,980
remember I got the first one right here is the Intercept and then 0.25 for the first data and then for the second flight for the first line,

497
00:55:33,980 --> 00:55:41,390
it will be zero for the intercept, 0.25 for my data variable and 0.25 for my first flight.

498
00:55:41,690 --> 00:55:51,740
And then for the third one the same. The only difference is that now here I have one more time and that's if you have to run back three times.

499
00:55:54,920 --> 00:55:58,240
Unfortunately, it's very late.

500
00:55:58,250 --> 00:56:02,180
I'm sorry. There's just something that we are missing.

501
00:56:02,180 --> 00:56:09,350
It's c inflated negative binomial regression, but it is very similar to what we did in the plus model.

502
00:56:09,530 --> 00:56:16,520
Okay. You need to run it, feed the logistic, look at the logistic output and then re feed the negative binomial.

503
00:56:17,270 --> 00:56:25,970
These lights are like here. Oh, right.

504
00:56:30,810 --> 00:56:34,020
See you next week. Remember, I have office hours right now.

