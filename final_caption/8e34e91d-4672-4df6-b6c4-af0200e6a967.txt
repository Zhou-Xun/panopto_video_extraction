1
00:00:04,180 --> 00:00:12,980
My. Do you have?

2
00:00:27,230 --> 00:01:08,820
Yeah, I think. You.

3
00:01:15,140 --> 00:01:20,820
I don't like anybody.

4
00:01:59,780 --> 00:02:03,900
Last week. I was.

5
00:02:10,220 --> 00:02:14,960
Good morning, everybody, and welcome back.

6
00:02:20,510 --> 00:02:23,900
I hope you all have a great, great drive. A good break.

7
00:02:27,430 --> 00:02:31,060
I have a positive ahead of of what to say fantabulous one.

8
00:02:31,780 --> 00:02:36,100
So my son was on the homecoming court and he won homecoming queen on Friday.

9
00:02:37,450 --> 00:02:41,350
I was very tempted to show you all a picture, but I resisted the urge.

10
00:02:42,950 --> 00:02:49,430
Oh. I'm going to show you off.

11
00:02:55,090 --> 00:02:58,690
See, I tried to resist, but.

12
00:03:01,030 --> 00:03:06,179
See if I can pull it up on Facebook. Really fair. No.

13
00:03:06,180 --> 00:03:11,050
I just want Facebook. The school I don't want. I'm not Mylan.

14
00:03:12,260 --> 00:03:15,270
High school plays for.

15
00:03:17,240 --> 00:03:20,770
CFL, just pull it up. The area.

16
00:03:22,840 --> 00:03:26,470
Go away. I can't see my son.

17
00:03:26,980 --> 00:03:34,260
So that's my son. He wore. He was very excited.

18
00:03:34,830 --> 00:03:38,280
He says, I was jumping up and while I knew I was jumping up and down.

19
00:03:38,280 --> 00:03:47,890
But. He says that his friends were like, Your mom was so cute dropping up, you know, because we had to go out there with them.

20
00:03:48,430 --> 00:03:52,300
So that was my fun. Thank you for hearing me.

21
00:03:52,620 --> 00:03:58,149
I am the Queen Mother for what I can only I can only pull the Queen Mother thing for one more day,

22
00:03:58,150 --> 00:04:01,510
which is today, because then there'll be a full week and they'll be obnoxious.

23
00:04:05,980 --> 00:04:12,910
So on Saturday he wakes up with this crown and he's like, Make me breakfast.

24
00:04:14,140 --> 00:04:17,200
I'm like, Yeah, whatever. I'm nothing.

25
00:04:19,730 --> 00:04:27,970
So. So today I guess I'll take my queen mother hat off and we'll be talking about indicators.

26
00:04:29,830 --> 00:04:38,080
So indicators actually brings us into our to the next step, step for gather incredible data.

27
00:04:38,500 --> 00:04:44,530
So we'll talk a bit about what step four is and then we'll talk about indicators.

28
00:04:46,420 --> 00:04:51,190
Announcements if you weren't here on last Thursday.

29
00:04:51,190 --> 00:05:03,750
It's been a week crazy. Last Thursday, I announced that the quiz will be open until Sunday at 11:59 p.m.

30
00:05:04,410 --> 00:05:07,860
So if you haven't done the quiz, which I haven't looked to see.

31
00:05:09,710 --> 00:05:15,360
You have a few more days to get it done. And I think that's about it for now.

32
00:05:15,540 --> 00:05:25,220
But you do have your next part of the evaluation plan coming up and out.

33
00:05:25,230 --> 00:05:31,140
But you had you still have a few weeks, so I'm not talking about it just yet, but just a heads up.

34
00:05:32,250 --> 00:05:39,450
It will come quicker than you think. So if you haven't started working in your groups on this next part,

35
00:05:40,500 --> 00:05:49,560
I encourage you to do so and it will pretty much cover from purpose and to data collection.

36
00:05:53,620 --> 00:05:58,689
Right. So review. So a week ago, I know you've held this in your head.

37
00:05:58,690 --> 00:06:09,819
It was fantastic. We had a fabulous week talking about validity and how much we have confidence and the work that we're doing in terms of its

38
00:06:09,820 --> 00:06:20,560
ability to tell us what we need to know and confidence that what we are seeing and what we're observing is more than likely true.

39
00:06:22,150 --> 00:06:29,480
You know, there's always with science. And you know, the scientific method, there's always some limitation.

40
00:06:30,110 --> 00:06:37,100
So we never know for sure if it's true, but validity pertains to the confidence that it is likely true.

41
00:06:38,150 --> 00:06:40,910
On Thursday we talked about external validity,

42
00:06:41,900 --> 00:06:48,680
which is the degree that the degree to which we can take what we learned and apply it to different situations.

43
00:06:49,580 --> 00:06:55,250
In this case, we talked about different threats to external validity.

44
00:06:56,000 --> 00:07:02,210
One, the interaction of selection and treatment is it try to make this a little bit.

45
00:07:02,600 --> 00:07:07,610
Now, I'm not going to do it today. I will do it another day that I was going to put it out there for you all.

46
00:07:07,610 --> 00:07:10,700
But I have given that it's been a week.

47
00:07:12,220 --> 00:07:17,090
I don't make any assumptions today, but I may do it on Tuesday.

48
00:07:18,350 --> 00:07:26,330
But threats to external validity relate to interaction of selection and treatment.

49
00:07:26,720 --> 00:07:36,650
So selection pertaining to those people who participate it and the extent to which what we

50
00:07:36,650 --> 00:07:44,000
see in that group can be related to another group interaction of setting and treatment,

51
00:07:44,360 --> 00:07:57,500
the extent to which the setting in which the evaluation in the program took place, the extent to which we can take where that program took place.

52
00:07:58,460 --> 00:08:03,500
And then translate it to other settings.

53
00:08:04,380 --> 00:08:06,180
And then testing and treatment.

54
00:08:06,480 --> 00:08:18,900
The extent to which testing or any kind of instrumentation had an influence on the outcomes we see in the US can affect our

55
00:08:18,900 --> 00:08:26,970
ability to see the same results in a in a different situation and then multiple treatment effects the threat that will.

56
00:08:28,380 --> 00:08:32,490
We might have other interventions going on at the same time,

57
00:08:32,880 --> 00:08:39,720
and that may be interfering or contributing rather to the outcomes that we're observing and so making it.

58
00:08:44,300 --> 00:08:49,910
A bit complicated, too, to be able to achieve those results in a difference in a different circumstance.

59
00:08:50,660 --> 00:08:55,549
So those are the threats that we talked about upcoming.

60
00:08:55,550 --> 00:09:01,850
We'll talk about construct validity as well, but we'll get more into that when we get into measures.

61
00:09:04,000 --> 00:09:09,310
So where are the new step? As I mentioned, we're in gather incredible data.

62
00:09:11,920 --> 00:09:16,360
So we you know, step three is really about what are we going to do with this evaluation?

63
00:09:17,140 --> 00:09:24,620
You know, step two is what is the program? And then step three takes us to focusing on the evaluation.

64
00:09:24,950 --> 00:09:29,600
What part of the program are we going to focus this work on?

65
00:09:30,080 --> 00:09:37,760
So from there on, once we have our questions, once we have, you know, our evaluation purpose,

66
00:09:38,450 --> 00:09:44,840
the next step is, well, how are we going to get the information we need to answer these questions?

67
00:09:46,560 --> 00:09:55,290
And then so the goal of step four is really about gathering data to be able to answer the questions.

68
00:09:58,050 --> 00:10:02,260
Sometimes you know, depending on. Um.

69
00:10:03,680 --> 00:10:07,970
You know, the people you talk to in a research world.

70
00:10:08,970 --> 00:10:13,470
Some people and I tend to do this, use data and information interchangeably,

71
00:10:14,310 --> 00:10:19,680
but just know that sometimes there is a strict division between data and information.

72
00:10:20,010 --> 00:10:25,380
Information is the it comes from the interpretation of the data analysis.

73
00:10:26,580 --> 00:10:31,739
I like to give that disclaimer because I tend to go back and forth and I tend

74
00:10:31,740 --> 00:10:37,530
to favor information because I think it's just it provides a little bit more.

75
00:10:37,740 --> 00:10:43,050
It's it provides for me a reminder that we are doing this work for a purpose.

76
00:10:44,680 --> 00:10:45,880
So that's my disclaimer.

77
00:10:46,750 --> 00:10:59,830
But ultimately, what this what this step is about is compiling information or data that the evaluation team and stakeholders perceive as high quality,

78
00:11:00,190 --> 00:11:03,460
trustworthy and relevant for answering the questions.

79
00:11:04,120 --> 00:11:09,700
So give thinking about some of those values that we stated at the beginning of this course.

80
00:11:10,120 --> 00:11:14,260
Um, utility, feasibility, propriety.

81
00:11:14,500 --> 00:11:19,240
We want data that we know are are going to help us.

82
00:11:19,510 --> 00:11:25,690
They're going to be useful. So they're going to help us answer a question. High quality and trustworthy.

83
00:11:25,690 --> 00:11:34,600
So we are making sure that we are being as rigorous as we can, accounting for the context of the evaluation,

84
00:11:35,140 --> 00:11:44,230
meaning, you know, some of the, you know, potential resource allocations that may affect what we can do.

85
00:11:45,160 --> 00:11:53,170
Any issues with participants and making sure that we are being ethically minded toward the

86
00:11:53,170 --> 00:12:00,880
program for the program but as long so what we're doing is trying to make sure that in.

87
00:12:02,220 --> 00:12:07,770
In consideration of those things that we are producing the best quality data we can.

88
00:12:09,980 --> 00:12:17,330
So again, it's but in principle, it is similar to any other research study you would do.

89
00:12:18,110 --> 00:12:21,680
So a lot of the things, if you've taken research methods, will feel familiar.

90
00:12:22,190 --> 00:12:32,570
You still take time to make sure you have high quality data, making sure that you're using rigorous methods to develop measures.

91
00:12:33,950 --> 00:12:43,670
All those things are true. We just add on the layer for evaluation that there are stakeholders involved, there's issues of utility involved,

92
00:12:45,290 --> 00:12:52,220
you know, additional ethical issues that relate to treatment and access to treatment.

93
00:12:55,170 --> 00:13:02,310
So why is credible data important? Bad data leads to bad decisions.

94
00:13:04,230 --> 00:13:12,000
Frankly. So if we have data that is not high quality, it's not credible data.

95
00:13:14,330 --> 00:13:17,750
We can make inferences that are incorrect.

96
00:13:18,050 --> 00:13:23,990
There are. Poor inferences about a program.

97
00:13:24,650 --> 00:13:34,130
So, for example, say we are evaluating a program that was successful in reducing agency levels for type two diabetics.

98
00:13:35,530 --> 00:13:42,840
But really because of the quality, the data being poor quality, there actually is no program.

99
00:13:42,850 --> 00:13:47,710
There actually is no effect. And we've, you know, incorrectly.

100
00:13:50,040 --> 00:13:53,400
Incorrect incorrectly determined that that program.

101
00:13:54,300 --> 00:13:57,750
Can, um, reduce ANC levels.

102
00:13:59,030 --> 00:14:05,420
So the implications of that is that we invest resources and money into a program that doesn't work.

103
00:14:06,360 --> 00:14:08,940
And there are people who, as a result,

104
00:14:09,180 --> 00:14:17,190
are not getting the treatment that they need to reduce their agency levels or think about vice versa or visa versa.

105
00:14:17,350 --> 00:14:27,420
Whenever you have a program that does reduce agency levels, but because of the quality of the data,

106
00:14:29,610 --> 00:14:34,830
you're unfortunately able to identify that that program is successful.

107
00:14:35,340 --> 00:14:42,180
And so, again, people are denied treatment that actually would help them and reducing a1c levels.

108
00:14:44,320 --> 00:14:49,930
And we already know that just because of the way data collection works,

109
00:14:51,010 --> 00:15:00,430
there always are going to be some potential ways in which, even with the most rigorous design, we might miss something as it is.

110
00:15:01,290 --> 00:15:12,900
So imagine what might happen if we're, in addition, not taking time to produce as credible of data as possible.

111
00:15:15,620 --> 00:15:25,279
And then also it's important to think about credible data because especially when you're working with stakeholders who understand how data,

112
00:15:25,280 --> 00:15:29,810
how data works, how data analysis and data collection methods work,

113
00:15:31,730 --> 00:15:37,309
if you are not using as rigorous methods as possible and you're not able to

114
00:15:37,310 --> 00:15:41,720
explain the choices that you've made and justify the decisions that you've made,

115
00:15:42,380 --> 00:15:47,660
that is unfortunately a way of compromising your dissemination.

116
00:15:48,260 --> 00:15:53,240
So you may have a program that actually you may be evaluating a program that works,

117
00:15:53,840 --> 00:15:59,480
but because of your methods and the flaws in the methodology without justification,

118
00:16:00,860 --> 00:16:09,229
you can't actually get that information out there in a way that allows for replication in other settings or situations.

119
00:16:09,230 --> 00:16:13,040
Right. So it's important to have credible data.

120
00:16:13,880 --> 00:16:26,240
And even when you are making kind of choices in terms of rigor, you need to understand how to justify if you're making some compromises,

121
00:16:26,930 --> 00:16:34,040
because again, in real life, you're going to need to make some compromises to what we consider that gold standard.

122
00:16:34,040 --> 00:16:42,350
Right, that artsy. If you think about design and starting from there, more likely than not, you're going to make some kind of compromise.

123
00:16:42,370 --> 00:16:45,700
So you need to be able to justify why do we make this compromise?

124
00:16:46,120 --> 00:16:56,180
How might that affect the quality of the data? So the different components that we'll talk about over the next few weeks.

125
00:16:57,710 --> 00:17:07,520
Today, we'll talk about identify indicators. We'll also, in the future, talk about finding and developing measures, determining sampling.

126
00:17:08,390 --> 00:17:12,410
And we'll talk about selecting data collection methods and sources.

127
00:17:17,220 --> 00:17:22,470
But in general, how can you improve data credibility?

128
00:17:22,480 --> 00:17:28,350
So how do we start in terms of making sure that we have as credible data as possible?

129
00:17:29,760 --> 00:17:37,240
So there's five things that you can do. One, you can incorporate stakeholders at all stages of data collection.

130
00:17:40,240 --> 00:17:47,830
So for example, you know, when stakeholders are involved in defining and gathering data that they find credible,

131
00:17:48,130 --> 00:17:52,990
they'll be more likely to accept the evaluations conclusions, as I just stated.

132
00:17:54,760 --> 00:18:00,490
And then also even in thinking about credibility from a career lens.

133
00:18:01,780 --> 00:18:08,950
The more you have your stakeholders or your investor parties involved in data collection,

134
00:18:10,240 --> 00:18:20,050
that can help with ensuring that the way that data is being collected is meaningful to the communities in which you're working.

135
00:18:21,450 --> 00:18:28,920
So that, for example, might help with more truthful answers from community members.

136
00:18:31,120 --> 00:18:33,400
So it's important to have your stakeholders involved.

137
00:18:35,380 --> 00:18:43,330
Second, we can improve data credibility by using reliable and ballot measures to collect information.

138
00:18:44,020 --> 00:18:53,830
So again, we've talked a little bit about validity and confidence, reliability, which we'll talk about in more in depth.

139
00:18:54,040 --> 00:19:01,870
We'll talk more in depth later. It's really about how reliable and consistent is the measure.

140
00:19:02,530 --> 00:19:11,860
So we want to be able to use. So we want to use measures that consistently get you the same responses so that we know that it is a accurate measure.

141
00:19:12,340 --> 00:19:17,950
And we also want to get ballot measures, measures that we can have confidence in.

142
00:19:18,850 --> 00:19:25,720
And again, we'll talk about construct validity when we start talking more about measures.

143
00:19:27,800 --> 00:19:31,340
Three. We want rigorous data collection procedures.

144
00:19:32,710 --> 00:19:35,890
And this is so I'm going to tell you as an evaluator.

145
00:19:36,190 --> 00:19:42,370
This is surprisingly something that can kind of get to the back of your brain.

146
00:19:43,030 --> 00:19:52,450
Not because. Not because necessarily that you don't know how to engage in rigorous data collection.

147
00:19:52,450 --> 00:20:00,730
But it's been my experience that if it comes to relationships, sometimes you're in relationship with a client.

148
00:20:02,110 --> 00:20:05,410
I know for me sometimes that relationship takes precedence,

149
00:20:06,130 --> 00:20:14,350
and so I forget to do things that take up time but are important for rigorous data collection training data collectors.

150
00:20:15,130 --> 00:20:20,110
That's one that. I don't necessarily I don't necessarily have a problem with.

151
00:20:20,560 --> 00:20:31,780
But it does take time. You have to stop, um, and ensure that data collectors are being consistent about the way that they are.

152
00:20:33,790 --> 00:20:38,170
Asking questions, responding to answers and etc.

153
00:20:39,790 --> 00:20:45,090
And again. Can be very can be difficult, especially if you're working as part of a team.

154
00:20:46,170 --> 00:20:55,440
I'm working on a project now where there are three, four foot four of us doing interviews.

155
00:20:56,250 --> 00:21:04,110
And the way that I interview is different than the way that my colleague, the way he interviews or another colleague she interviews.

156
00:21:05,520 --> 00:21:11,970
And that includes the order in which we might ask questions, how we go about doing a semi-structured interview.

157
00:21:13,140 --> 00:21:23,400
So some, especially when you're administering open ended tools, like interview tools, it's important to talk about as a team.

158
00:21:25,170 --> 00:21:29,040
What things are we going to allow? What things are we not going to allow?

159
00:21:29,940 --> 00:21:40,229
Like, are we going to allow kind of pivots into different conversations that might be relevant but not necessarily reflected in the question.

160
00:21:40,230 --> 00:21:46,950
For example, are we going to respond with what types of probes or prompts?

161
00:21:48,420 --> 00:21:55,260
So it's important to be as consistent as possible regardless of the person who is doing the data collection.

162
00:21:57,560 --> 00:22:12,830
You know, and knowing where there's room for variation pilot testing, this is another one where formally you would test with a sizable sample.

163
00:22:12,980 --> 00:22:16,370
Not enough. Not as big as the sample in the real study.

164
00:22:18,200 --> 00:22:22,820
But sometimes you don't have access to the people that you need for pilot testing.

165
00:22:23,120 --> 00:22:31,100
So you have to think about what can I do if there's really not a pilot test sample that I have access to?

166
00:22:31,580 --> 00:22:35,690
So how can you, for example, use your stakeholders to review tools?

167
00:22:37,160 --> 00:22:40,940
Have, you know, participants one or two review tools.

168
00:22:42,260 --> 00:22:45,560
So there's ways to go about it without kind of.

169
00:22:47,150 --> 00:22:50,480
Compromising relationship or data quality.

170
00:22:52,570 --> 00:23:01,840
Sampling is another one. Sampling also can be actually a difficult one to handle and evaluation projects because

171
00:23:01,840 --> 00:23:09,310
typically you don't have access to a control group or just an automatic comparison group.

172
00:23:10,060 --> 00:23:17,020
So you do have to sometimes be creative to be able to have that rigorous data collection in respect to sampling.

173
00:23:17,590 --> 00:23:25,750
And so we'll be able to get we'll get into different ways that you can sample to help it be as rigorous as possible.

174
00:23:28,340 --> 00:23:35,300
The fourth way to improve data credibility is using multiple data collection methods and sources.

175
00:23:39,360 --> 00:23:44,220
I'll say more about this, but generally in.

176
00:23:46,340 --> 00:23:50,930
Design. We engage in something called triangulation.

177
00:23:52,170 --> 00:24:05,100
Where you have multiple sources of data and you compare the results across the different data collection methods.

178
00:24:05,340 --> 00:24:16,030
So you might have interviews. So I can give you the one on the research study I was just talking about on COVID.

179
00:24:17,670 --> 00:24:22,500
It's not a COVID vaccine. Uptake.

180
00:24:24,370 --> 00:24:28,270
And community based strategies. So we have surveys.

181
00:24:30,880 --> 00:24:39,380
We have interviews and these surveys are with. People who are implementing the program.

182
00:24:41,480 --> 00:24:48,350
Why would why we're doing it at the implementer level. I won't give it to you, but we're we're actually focusing on the implementers.

183
00:24:49,400 --> 00:24:55,770
We have interviews with those implementers. But then we also are looking at secondary data.

184
00:25:00,890 --> 00:25:09,630
Including materials that they're using. Like posters and whatnot.

185
00:25:11,850 --> 00:25:21,060
And from that, we will be able to identify maybe there will be, you know, understand where we're seeing results in common.

186
00:25:22,000 --> 00:25:30,370
And where we are seeing differences in results and using the different data sources to try and understand why we're seeing those differences.

187
00:25:30,820 --> 00:25:37,059
So when we're triangulating data, we're not asking the exact same questions across a method,

188
00:25:37,060 --> 00:25:43,360
but we are asking similar questions across a method where we might ask them, for instance, in a different way.

189
00:25:43,750 --> 00:25:52,600
And that just allows us to be able to again capture commonalities across the different data sources and account

190
00:25:52,600 --> 00:26:01,300
and control for the impact that different data techniques or different methods might have on the results.

191
00:26:02,970 --> 00:26:16,320
And then finally, the most boring but very important part of an improving data credibility is routine error checking as part of data quality control.

192
00:26:17,220 --> 00:26:20,430
How many people buy arrays? Hands have actually clean data.

193
00:26:22,670 --> 00:26:33,980
So a few people have clean cleaned, like sat in an Excel file, made sure that the results looked just like what it looked like on the survey.

194
00:26:34,010 --> 00:26:38,690
Changing the coding from yes to one, those kinds of things.

195
00:26:40,030 --> 00:26:43,390
Absolutely boring. So boring.

196
00:26:43,840 --> 00:26:51,280
Absolutely boring. So important. Anybody who's done it knows it.

197
00:26:51,730 --> 00:26:55,640
Even though I do have a coworker, she loves doing it. I'm like, more power to you.

198
00:26:55,660 --> 00:26:59,890
You could do all of it for the whole center. I don't care because it's boring.

199
00:27:00,730 --> 00:27:06,490
But it is important because you switch. You switch too many once to zeros or twos.

200
00:27:06,790 --> 00:27:11,020
You just change a significant or not.

201
00:27:11,350 --> 00:27:15,100
Yeah. You just change something that might have been significant to. Not significant.

202
00:27:17,650 --> 00:27:24,010
So it's important to not only have that data claiming process be rigorous.

203
00:27:24,880 --> 00:27:29,050
It's helpful to have multiple people doing it, at least the doing the data checking.

204
00:27:31,750 --> 00:27:40,870
In order to ensure that you have the best quality data set, and that's either qualitative or quantitative data.

205
00:27:41,920 --> 00:27:47,590
So some things you can do, have somebody else check after you take a few surveys,

206
00:27:47,590 --> 00:27:53,350
check randomly, randomly a few surveys, and check to see that they were entered accurately.

207
00:27:54,400 --> 00:27:57,820
Checking a transcript against the audiotape.

208
00:27:58,570 --> 00:28:08,230
These things are easy to kind of not do when you are in the midst of a busy project, but important for analysis.

209
00:28:11,240 --> 00:28:14,930
So any questions about just the generalities of step four?

210
00:28:14,960 --> 00:28:19,790
So we're collecting good data. All right.

211
00:28:20,270 --> 00:28:27,140
So what we're going to do is spend the rest of the time doing talking about indicators.

212
00:28:28,640 --> 00:28:37,340
And let me get my time. Thank you. No more announcements for those who follow British politics.

213
00:28:37,640 --> 00:28:42,410
You might have noticed. Liz Truss. Stepped down after 44 days.

214
00:28:43,820 --> 00:28:49,100
As Prime Minister. Which is crazy. But anyway, I just read that this morning.

215
00:28:49,100 --> 00:28:53,180
I was like, What? I'm obsessed with British. I should move to Britain.

216
00:28:54,500 --> 00:28:59,020
But if I didn't, I wouldn't be here right now. And I want him.

217
00:28:59,020 --> 00:29:02,240
I can't see either way anyway.

218
00:29:03,010 --> 00:29:07,390
So we're going to talk about indicators. So what is an indicator?

219
00:29:08,710 --> 00:29:10,420
An indicator is a specific,

220
00:29:10,420 --> 00:29:20,470
observable and measurable piece of information that's used to exert to determine the extent to which a program is being implemented as expected.

221
00:29:21,540 --> 00:29:27,510
Process indicators and achieving the intended outcomes outcome indicators.

222
00:29:28,760 --> 00:29:35,020
So again, though, this. I put the extent to which it is doing this.

223
00:29:40,570 --> 00:29:44,780
So you want your indicators. To be.

224
00:29:46,990 --> 00:29:59,650
In some respects you want them to be neutral or open and you wanted to be able to tell you what what you're seeing versus I'm telling you,

225
00:29:59,770 --> 00:30:02,560
yes, it's doing something or no, it is not.

226
00:30:04,390 --> 00:30:13,540
You want it because you want them to be able to make judgments based on what you're seeing, leaving room for you to be able to interpret.

227
00:30:16,780 --> 00:30:23,830
To be able to interpret why you're seeing what you're seeing. So to me, you know, in principle is the difference between.

228
00:30:25,520 --> 00:30:35,420
So say I have two studies. Or I have one study rather.

229
00:30:41,660 --> 00:30:45,170
And save the. From one approach.

230
00:30:48,300 --> 00:30:52,670
And from one approach, I'm saying that here's the gold standard.

231
00:30:52,680 --> 00:30:57,970
This is where you got to reach. So what happens if I don't reach that?

232
00:30:58,930 --> 00:31:06,370
If I if I leave it close in and it's a yes or no option either did what I said I was going to do or didn't say what I was going to do.

233
00:31:07,420 --> 00:31:15,040
The problem is, is that it can kind of pigeonhole you into simply determining, okay,

234
00:31:15,040 --> 00:31:22,570
it's it's not good because it didn't reach that gold star, that point, the benchmark that we were trying to reach.

235
00:31:23,710 --> 00:31:28,490
But. If you leave it open and do it again.

236
00:31:28,870 --> 00:31:32,720
Admittedly, this is more of a philosophical difference.

237
00:31:35,240 --> 00:31:41,370
If you leave it in, please excuse my drawing. But if you leave it open ended.

238
00:31:43,790 --> 00:31:49,830
That gives you a little bit more room. To discuss OC will.

239
00:31:52,470 --> 00:31:59,040
It may not have reached where we thought we were going, but to what extent did we reach it?

240
00:32:00,480 --> 00:32:07,140
Why didn't we reach it? It can also tell you what good things allow us.

241
00:32:07,860 --> 00:32:16,350
You know what facilitators facilitated us giving some improvement, even though it wasn't as much improvement as we expect.

242
00:32:18,560 --> 00:32:21,410
And I pushed that point just because I think in public health,

243
00:32:21,410 --> 00:32:26,240
sometimes we get really caught into if it didn't work the way we thought was going to work, it didn't work.

244
00:32:27,200 --> 00:32:30,380
But we miss a lot of learning that way.

245
00:32:30,890 --> 00:32:40,190
So we want our indicators to be able to accommodate that neutral position where it's not just good or bad,

246
00:32:40,190 --> 00:32:45,080
but it is something like we're observing something, some kind of change.

247
00:32:49,870 --> 00:32:54,610
And so in terms of thinking about the process of an evaluation project.

248
00:32:55,850 --> 00:32:59,720
It's really important that we focus some time on our indicators.

249
00:32:59,990 --> 00:33:06,170
Why? Because our indicators are going to tell us what kind of measures we're going to use.

250
00:33:07,550 --> 00:33:11,870
And then from there, it's going to tell us what methods we're going to use.

251
00:33:12,830 --> 00:33:17,030
And then from there, tell us what kind of analysis we're going to do.

252
00:33:18,010 --> 00:33:22,480
So it's not just a throwaway like, okay, well, this is how we tell that something happened.

253
00:33:23,770 --> 00:33:28,180
It allows us to it impacts the rest of the process.

254
00:33:33,980 --> 00:33:39,980
So ideally, we're going to develop indicators before we identify the methods.

255
00:33:41,240 --> 00:33:45,620
Again, this seems obvious, but when you're kind of caught up in the process,

256
00:33:45,620 --> 00:33:53,480
sometimes you and I have admittedly been known to do this myself, where I go from a valuation question.

257
00:33:54,490 --> 00:33:58,390
Two methods. And then try to go backwards.

258
00:33:59,400 --> 00:34:05,250
And identify indicators realizing, oh, crap, I can't, actually.

259
00:34:06,270 --> 00:34:09,360
I actually can't answer the question with the data that I have.

260
00:34:11,950 --> 00:34:13,600
Very easy to get caught up in that.

261
00:34:15,300 --> 00:34:26,310
So ideally we are thinking, right in reference to the question, what cam what will indicate the answer to the question?

262
00:34:31,230 --> 00:34:40,380
So the way we we start off with that is that we really think about what are our evaluation questions of interest.

263
00:34:41,630 --> 00:34:48,410
So say, you know, we're this nice looking cartoon gentleman here and we are value.

264
00:34:49,430 --> 00:35:00,710
We are evaluating a program, we think, with a question mark over our head that the program activities reach the correct audience.

265
00:35:01,620 --> 00:35:09,360
So what? What was our reach? Well, we need to know what kind of information do we need to know to answer this question?

266
00:35:09,990 --> 00:35:17,160
And so you might remember I said that when we think when you think about questions, you want to keep them a little open.

267
00:35:17,490 --> 00:35:24,570
You don't want to make the questions so super specific that you can't actually identify indicators.

268
00:35:24,950 --> 00:35:31,950
That's why when you think about phrasing your question, you want to kind of keep it the back of your mind.

269
00:35:31,980 --> 00:35:37,530
Okay, I have to identify indicators that allow me to observe the answer to this question.

270
00:35:39,610 --> 00:35:46,540
But that's what indicators simply tell us what is going to indicate that we are seeing the answer to this question.

271
00:35:47,350 --> 00:35:52,220
And again, neutral. It could be. No change.

272
00:35:52,250 --> 00:35:57,680
Change. It can be positive change. A negative change with with indicators.

273
00:35:57,680 --> 00:36:04,910
We're really trying to just identify what's going to tell us that we're seeing the answer to this question.

274
00:36:08,710 --> 00:36:18,360
So some common examples of indicators. Participation rates, incident rates, all in public health at least.

275
00:36:18,360 --> 00:36:25,600
Incidence rates. Prevalence rates. Took me a long time to figure out the difference between indicators and prevalence, by the way.

276
00:36:27,810 --> 00:36:31,260
So many things to learn in public health. So if you feel that way, it's okay.

277
00:36:31,560 --> 00:36:35,930
We all feel that way, especially when we're first learning counts.

278
00:36:36,480 --> 00:36:41,490
Frequencies are other indicators. We might see proportions.

279
00:36:42,780 --> 00:36:52,560
And then attitudes and opinions, which are more, you know, often more qualitative, but still an indicator that we could incorporate.

280
00:36:57,060 --> 00:37:06,180
One thing I also want to wanted to make sure I know is that we often need to use multiple indicators for any one evaluation question.

281
00:37:06,720 --> 00:37:14,670
And again, because if we're relying just on one indicator that can distort how well a program is working.

282
00:37:15,120 --> 00:37:20,910
So again, thinking about that triangulation. If we only have one answer.

283
00:37:23,110 --> 00:37:26,920
That might be due to the form of data collection that we use.

284
00:37:27,580 --> 00:37:33,940
So having multiple answers to multiple indicators in the same way.

285
00:37:35,490 --> 00:37:40,290
It's helpful so that we are getting a well-rounded answer to our question.

286
00:37:45,070 --> 00:37:48,720
Okay. So what are good indicators?

287
00:37:50,730 --> 00:37:55,200
So when I say good again, I mean more like high quality indicators.

288
00:37:57,420 --> 00:38:05,610
So you want them to be specific. You want them to be measurable, practical, relevant, useful, equitable.

289
00:38:06,770 --> 00:38:11,720
So when we say specific indicators, they should be detailed enough to be usable.

290
00:38:13,080 --> 00:38:21,240
They should tell us enough information that we can actually link it to the evaluation question and link it to some aspect of the program.

291
00:38:23,100 --> 00:38:31,410
They should be measurable. We should be able to observe them either through quant or qual methods.

292
00:38:32,300 --> 00:38:36,980
And they should be able to be assessed vis a vis some kind of standard.

293
00:38:39,870 --> 00:38:42,899
And what that standard is will be determined in the future.

294
00:38:42,900 --> 00:38:51,990
But we should be able to take the results from the data that's associated with the indicator and be able to say,

295
00:38:51,990 --> 00:38:56,310
okay, this was negative, this was positive change, etcetera, etcetera.

296
00:38:59,600 --> 00:39:04,560
They should be practical. So you should be able to collect them on a timely basis.

297
00:39:04,580 --> 00:39:07,640
They should be feasible as well.

298
00:39:09,200 --> 00:39:18,980
We should be able to look at you know, we should be able to collect the data in a way that is that fits within the parameters of our resources.

299
00:39:20,580 --> 00:39:24,540
They should be relevant, so they should be aligned with program activities.

300
00:39:25,080 --> 00:39:32,639
Um. Meaning the actual program not this is not in that I just want to make sure that it's

301
00:39:32,640 --> 00:39:36,480
clear that what I'm talking about is that they should be aligned with the program.

302
00:39:37,660 --> 00:39:44,840
And in in contrast, they shouldn't be indicators that relate to something else.

303
00:39:44,860 --> 00:39:49,240
And I have an example that in a little bit they should be useful.

304
00:39:50,660 --> 00:39:57,080
So again, they may be interesting, but if they aren't useful to the stakeholders,

305
00:39:57,560 --> 00:40:03,350
then you might consider cutting that particular indicator when you're thinking about,

306
00:40:03,350 --> 00:40:06,590
okay, I have all these indicators, which ones do I actually move forward?

307
00:40:07,130 --> 00:40:14,150
So one. One characteristic that you can use to make that decision is a useful.

308
00:40:15,600 --> 00:40:18,540
Now for your paper or your plan assignment,

309
00:40:18,540 --> 00:40:25,740
it'll be a little different because you'll have to kind of make assumptions about what a client, my team useful.

310
00:40:26,720 --> 00:40:31,430
Just so you know. And that's okay for you to have to make that.

311
00:40:32,920 --> 00:40:39,700
Assumption for the purposes of the assignment. But if you weren't doing the assignment and you were working with a real client.

312
00:40:41,040 --> 00:40:45,030
You might you might stop. And it's totally appropriate to stop.

313
00:40:45,270 --> 00:40:52,140
Talk to a client. Okay. Are these indicators useful to you or are they going to help you make the decisions you need to make?

314
00:40:54,320 --> 00:41:02,450
And again, even if you've already shown the questions, you want to make sure that every step of the process you're working with your stakeholders.

315
00:41:03,080 --> 00:41:11,100
And then finally, good indicators should be equitable. And in this case, from a Cree lands,

316
00:41:11,100 --> 00:41:19,620
what we're talking about is that it should be able to accommodate an analysis that will allow you to identify inequities in groups and subgroups.

317
00:41:20,160 --> 00:41:23,760
So in other words, you want to be able to desegregate the data.

318
00:41:24,180 --> 00:41:28,620
And even with qualitative data, what that means is that.

319
00:41:31,020 --> 00:41:41,970
You want to be able to have indicators that can leave room for exploring qualitative data based on subgroup identity, for example.

320
00:41:43,500 --> 00:41:50,670
You know, you might do interviews with rural groups versus urban groups,

321
00:41:51,540 --> 00:41:55,650
and your indicators shouldn't be so specific that it doesn't allow you to do that.

322
00:41:58,120 --> 00:42:06,100
But again, that's an example. So you have to think about when you're crafting crafting the indicators, you have to make sure that.

323
00:42:08,330 --> 00:42:13,760
You're being specific where you need to, but allowing room for disaggregation where you can.

324
00:42:18,080 --> 00:42:21,830
Hopefully that makes a little bit more sense as you kind of practice writing them.

325
00:42:23,880 --> 00:42:25,810
So there's different kinds of indicators.

326
00:42:25,830 --> 00:42:33,600
I mean, there's also input indicators, but for the purposes of the class, we're going to focus on process and outcome indicators.

327
00:42:34,590 --> 00:42:38,730
Your process indicators, they measure their programs, activities and outputs.

328
00:42:39,240 --> 00:42:46,530
Just like with evaluation questions and we talk about, you know, fidelity, reach those,

329
00:42:46,530 --> 00:42:52,980
deliver your indicate your process indicators are hitting the same places in the logic model.

330
00:42:54,370 --> 00:43:00,129
With outcome indicators. Those measure whether the program is achieving effect.

331
00:43:00,130 --> 00:43:05,380
It expected changes in the short and intermediate and long term.

332
00:43:06,420 --> 00:43:19,020
Um. So I might even edit this just to kind of be more in line with the definition and the principles, just if it achieving some effect.

333
00:43:21,480 --> 00:43:29,000
Because again, you want it to be neutral. So how do we develop indicators?

334
00:43:34,270 --> 00:43:39,310
Well, the first place you definitely want to start is looking at the valuation questions in the logic model.

335
00:43:40,450 --> 00:43:47,200
So some questions you can ask yourself what are important dimensions of the program and its desired outcomes?

336
00:43:49,070 --> 00:43:53,960
Um, so you might have a program that is taking place.

337
00:43:54,800 --> 00:43:57,860
The Safer Sex program. My go to.

338
00:43:59,260 --> 00:44:14,580
Ozone House. In Ypsilanti. If I identify indicators related to safer sex, that's an important dimension of the program.

339
00:44:15,300 --> 00:44:27,210
Is it so much an important dimension of the program that it takes place at Ozone Houses, main facility or the drop in facility?

340
00:44:27,720 --> 00:44:30,960
Maybe not. It may not be important.

341
00:44:31,110 --> 00:44:41,219
Maybe it is to the client. So part of determining what an important dimension is is understanding what's important to the client,

342
00:44:41,220 --> 00:44:45,540
but also what's important to make the program work.

343
00:44:47,660 --> 00:44:53,300
If you know, if I'm trying to reduce condom use and I take away all all.

344
00:44:54,570 --> 00:44:57,690
You know, questions about condom use and how that was presented.

345
00:44:57,720 --> 00:45:05,490
Well, that I'm missing an important dimension. You also want to look at what changes are the focus of the evaluation.

346
00:45:09,740 --> 00:45:15,600
Excuse me. I'm like. Excuse me. So what changes are the focus of the evaluation?

347
00:45:15,750 --> 00:45:22,050
Are we looking for change? In what population are we looking for change in a certain time frame?

348
00:45:23,160 --> 00:45:31,230
These are different areas that we can look at within the logic model to identify indicators of interest.

349
00:45:32,250 --> 00:45:35,670
What processes are important to the implementation of the program?

350
00:45:36,180 --> 00:45:48,450
Again, taking that Ozone House example. Is it important that it takes place in one location versus another where it may be?

351
00:45:49,440 --> 00:45:52,440
That, you know, issues of transportation are important.

352
00:45:52,440 --> 00:45:57,390
So maybe that is an important process that, you know, transport,

353
00:45:57,500 --> 00:46:02,129
you have a way for youth to get to Ozone House regardless if it's at their drop in center,

354
00:46:02,130 --> 00:46:06,510
which is downtown Ypsi versus their main facility, which is.

355
00:46:08,380 --> 00:46:12,400
Not Downtown and bright by Easter University.

356
00:46:12,520 --> 00:46:20,280
Michigan University. And then finally, what context specific dimensions need to be measured to tell the story of the program.

357
00:46:21,330 --> 00:46:24,330
This is one that is not always as clear cut.

358
00:46:25,230 --> 00:46:29,850
And so sometimes understanding if we need to measure indicators related to context,

359
00:46:30,390 --> 00:46:36,540
is about really getting the know why and how a program was designed.

360
00:46:36,780 --> 00:46:47,580
So you might need to work with a client to understand that perhaps having a steering committee planet was an important context specific dimension.

361
00:46:48,760 --> 00:46:54,250
And in that case, you would ask you would have an indicator that relates to that steering committee.

362
00:46:57,330 --> 00:47:00,490
And so think about it is what, you know, thinking about.

363
00:47:00,510 --> 00:47:05,160
Does it need an indicator? One thing that you can consider is.

364
00:47:06,330 --> 00:47:11,819
What if this program were being replicated with this context?

365
00:47:11,820 --> 00:47:18,300
Need to be included in order for it to be replicated in the way that we're doing it.

366
00:47:20,420 --> 00:47:25,010
So again, if you email, your client says, well, that steering committee is also important.

367
00:47:26,900 --> 00:47:31,430
What sometimes it is them. You should have an indicator related to the steer committee.

368
00:47:37,290 --> 00:47:41,120
So once you have kind of a you know, you've thought about some of these questions,

369
00:47:42,270 --> 00:47:48,870
you can also start to think about the constructs or concepts that are in the evaluation question.

370
00:47:49,140 --> 00:47:57,120
And so indicators connected to them. So now, you know, I've talked about constructs and public health constructs.

371
00:47:57,300 --> 00:48:03,110
They're the key concepts of a given theory. Even if.

372
00:48:04,150 --> 00:48:09,250
You're looking at the evaluation question, and it's not based on a formal theory.

373
00:48:09,640 --> 00:48:17,110
There's usually a theory of change being considered. So you can think about whether they're kind of key concepts from that theory of change.

374
00:48:21,190 --> 00:48:24,980
So for example, if I have a question that's about okay.

375
00:48:25,000 --> 00:48:29,200
To what extent does this program improve parent youth communication?

376
00:48:29,920 --> 00:48:33,190
Well, there are certain constructs that are associated with parent.

377
00:48:33,190 --> 00:48:37,870
You've communication. Um, parents intention to talk to you.

378
00:48:39,370 --> 00:48:42,880
You need parents actually talking to youth.

379
00:48:44,350 --> 00:48:48,130
Then you meet youth actually talking with their parents.

380
00:48:49,460 --> 00:48:56,150
And then you need youth perceiving something out of that conversation with their parents.

381
00:48:56,690 --> 00:49:02,540
So those are different constructs related to kind of the broad idea of parent youth communication.

382
00:49:03,140 --> 00:49:06,230
And I might just choose to measure two of those.

383
00:49:07,960 --> 00:49:15,490
In this case, on my measure, I might write an indicator related to their communication parents communication behavior,

384
00:49:16,300 --> 00:49:19,540
and then youth's perception of parents communication.

385
00:49:22,630 --> 00:49:30,180
Does that make sense? So I'm kind of breaking down. The concepts or the constructs in the in the question.

386
00:49:31,460 --> 00:49:34,910
And identifying kind of pieces that relate to that construct.

387
00:49:36,750 --> 00:49:37,950
Pieces of information.

388
00:49:44,010 --> 00:49:53,190
And then what you're doing is thinking about how with those indicators illustrate the observation you expect to measure or document.

389
00:49:55,290 --> 00:50:05,400
So again, here's this question. Did the Internet say and so this question relates to a program we talked about before, the Internet safety program.

390
00:50:06,120 --> 00:50:10,680
So did it lead to an increase in communication between parents and youth about Internet safety?

391
00:50:11,400 --> 00:50:15,600
So the question I'm thinking about when I see that evaluation question is, well,

392
00:50:15,600 --> 00:50:21,660
how am I going to know that the Internet safety program led to an increase in communication?

393
00:50:21,960 --> 00:50:32,640
What is it that I need to see in order to know that I'm looking at communication between parents and youth, much less an increase in communication?

394
00:50:35,440 --> 00:50:42,850
So here are two example indicators. Percentage of parents who have talked about Internet safety with their child after the intervention.

395
00:50:43,870 --> 00:50:48,460
So I know that, you know, if I want to know that there was an increase in communication,

396
00:50:48,790 --> 00:50:53,410
I'm going to have to see that parents have talked with their kids after the intervention.

397
00:50:53,950 --> 00:50:57,490
And when I say I need to see it, I don't mean literally. I need to see it.

398
00:50:57,670 --> 00:51:02,340
I mean, I need to measure it. And the same as with the second.

399
00:51:03,360 --> 00:51:11,640
The second indicator, the number of youth who report their parent has talked to them about Internet safety after the intervention.

400
00:51:12,240 --> 00:51:17,760
So again, I know that if I want to measure increase in communication,

401
00:51:18,150 --> 00:51:23,640
I'm going to want to see youth reporting that their parent has talked to them about Internet safety.

402
00:51:24,480 --> 00:51:28,890
And so a note that the indicator also it accounts for time.

403
00:51:29,400 --> 00:51:34,290
So something that happened after the intervention, you know, thinking about validity.

404
00:51:34,770 --> 00:51:41,820
Thinking about, you know, when we talked about ambiguous temporal, blah, blah, blah, no precedents.

405
00:51:44,000 --> 00:51:47,080
It's so unnecessarily long. Cracks me up, even.

406
00:51:48,680 --> 00:51:55,400
That's gonna stick with me forever, I guess. And note that this indicator is a neutral.

407
00:51:55,430 --> 00:51:59,300
It doesn't say and this is a comment. This is one actually I make.

408
00:51:59,310 --> 00:52:02,870
I have to continually check myself in doing it.

409
00:52:03,290 --> 00:52:08,750
Note that it's neutral. It doesn't say a greater percentage of parents.

410
00:52:09,820 --> 00:52:13,210
Have talked about Internet safety with their child after the intervention.

411
00:52:14,760 --> 00:52:17,880
Because the indicators are neutral. It's just telling me, what am I seeing?

412
00:52:18,870 --> 00:52:26,490
It's telling me what to look for. Then in the event in that interpret interpretation phase,

413
00:52:27,300 --> 00:52:39,330
that's when we get into the increase decrease and of adding a value or an assessment onto the indicate what we see.

414
00:52:40,170 --> 00:52:54,340
Right. You also want to make sure that when you're developing indicators that they accurately reflect evidence of the construct or relationship.

415
00:53:00,310 --> 00:53:12,300
So. And think about it this way you want when you see that answer, when you see what you've collected in relationship to the indicator,

416
00:53:12,870 --> 00:53:21,420
you want it to be able to reasonably relate to what it is you're trying to find evidence about.

417
00:53:22,420 --> 00:53:30,220
So consider the value. I have an evaluation question, and it's about assessing the relationship between breastfeeding and child health.

418
00:53:30,700 --> 00:53:34,450
I mean, it could be, you know, to what extent. Um.

419
00:53:35,450 --> 00:53:41,510
To what extent does. This program on breastfeeding.

420
00:53:42,320 --> 00:53:47,200
Improve child health. So I might have to indicators.

421
00:53:47,200 --> 00:53:53,110
One related to breastfeeding initiation. One related to breastfeeding duration.

422
00:53:54,240 --> 00:54:00,350
So initiation you to measure. You know, usually people know.

423
00:54:01,070 --> 00:54:08,000
Yeah. I you know I try breastfeeding that's their they even like a nurse could collect in the hospital

424
00:54:08,930 --> 00:54:16,040
so it because it is easier to measure initiation it's a more common indicator that's used.

425
00:54:16,520 --> 00:54:22,700
Um. You know, in contrast, breastfeeding duration is harder to measure.

426
00:54:23,600 --> 00:54:27,140
I did a project years ago related to.

427
00:54:29,150 --> 00:54:37,040
You know, relate it to low birth weight. And, you know, we brought up breastfeeding duration as a potential indicator.

428
00:54:38,420 --> 00:54:46,060
When we worked with our stakeholders, they said that's going to be a hard one for our because the program was related to.

429
00:54:49,060 --> 00:54:55,030
Not doulas, but some type of other support service for for parents.

430
00:54:55,900 --> 00:55:03,370
And part of what they were doing was promoting breastfeeding. So when we said, okay, well, you know, we want to measure.

431
00:55:04,400 --> 00:55:08,180
The extent to which participants are continuing to breastfeed.

432
00:55:08,180 --> 00:55:14,540
And they say it's really hard for people to know when they how long they breastfeed at a given moment.

433
00:55:14,810 --> 00:55:19,790
It's also hard for them to determine or remember when they stop breastfeeding.

434
00:55:20,630 --> 00:55:28,070
So that's, you know, so there was kind of some pushback from from the stakeholders about gathering that because it was going

435
00:55:28,070 --> 00:55:35,570
to require more work on the part of the participant and the person providing the caregiving support.

436
00:55:37,840 --> 00:55:47,740
But if you're really trying to measure improved child health, the fact that they start is not really a good measure of breastfeeding.

437
00:55:47,770 --> 00:55:52,300
You know, the benefit of breastfeeding. The benefit of breastfeeding comes from duration.

438
00:55:53,580 --> 00:55:58,110
So you have to think about whether or not so in terms of writing your indicators,

439
00:55:58,110 --> 00:56:05,790
you have to think about is this actually a good indicator to use to be able to measure what I'm saying?

440
00:56:05,790 --> 00:56:11,710
I'm trying to measure. Does that make sense?

441
00:56:16,670 --> 00:56:21,400
Maybe. Maybe not. Kind of. I don't know. All right.

442
00:56:22,040 --> 00:56:25,220
Hmm. Thank you. Now.

443
00:56:26,420 --> 00:56:34,220
Here's where we get it. Yeah. You want to make sure you want to make sure that the indicator is pointing you toward evidence.

444
00:56:35,490 --> 00:56:41,340
That is accurately telling you it accurately reflects the contract or relationship.

445
00:56:42,210 --> 00:56:46,320
So you want to measure things that actually.

446
00:56:47,990 --> 00:56:50,960
That best match what it is you're trying to measure.

447
00:56:51,200 --> 00:57:00,650
So if I'm trying to measure improved child health, the duration of breastfeeding, it's what's more important than initiation.

448
00:57:03,200 --> 00:57:07,580
And so sometimes that indicator is going to be more complicated to to measure.

449
00:57:08,150 --> 00:57:12,290
But it's a better measure of what you're trying to determine.

450
00:57:18,130 --> 00:57:26,290
So some considerations. Um. You know, share one, do stakeholders agree with the chosen indicators?

451
00:57:27,520 --> 00:57:31,509
And then, you know, if there is some kind of conflict, you know,

452
00:57:31,510 --> 00:57:36,910
I mentioned the the conflict or the pushback we got on measuring breastfeeding duration.

453
00:57:37,360 --> 00:57:45,970
Sometimes as an evaluator, that means you need to do some educating, but you also may need to be doing some compromising.

454
00:57:47,930 --> 00:57:53,540
Another consideration are indicators linked to the program, logic model and evaluation questions.

455
00:57:54,600 --> 00:58:01,060
So theoretically. Ideally. Your logic model should drive your questions.

456
00:58:01,480 --> 00:58:06,190
So there should not be any conflict between the logic model and the questions.

457
00:58:07,040 --> 00:58:10,040
And so your indicator shipped far right in line.

458
00:58:11,780 --> 00:58:17,270
You know, you should be able to start with the logic model, see logically how the questions came from.

459
00:58:18,050 --> 00:58:22,370
And from there on, you should be able to see how the indicators came from the questions.

460
00:58:25,970 --> 00:58:29,930
Another consideration. Do you have a way to measure and collect the data for the indicators?

461
00:58:32,190 --> 00:58:35,580
Now. I think that when an example of breastfeeding duration came into play.

462
00:58:36,900 --> 00:58:40,940
They felt we didn't have a way to measure the indicator, so they didn't want to measure.

463
00:58:42,250 --> 00:58:44,950
Or at least they felt we didn't have a credible way of measuring it.

464
00:58:47,270 --> 00:58:52,940
And then how will the chosen indicators affect the cost and the length of time our evaluation will take?

465
00:58:54,310 --> 00:58:57,850
Especially if you're thinking about indicators with time considerations.

466
00:58:58,860 --> 00:59:07,710
Um. Do we have the time? Like if I have an indicator that relates to measuring something six months after the intervention took place?

467
00:59:08,070 --> 00:59:14,760
Again, I need to have money to be able to do that. Just like with the questions, you have money to do a six month follow up.

468
00:59:21,050 --> 00:59:32,440
So some common challenges with indicators. Not link to program activities focusing on outputs instead of outcomes.

469
00:59:33,550 --> 00:59:39,760
I'm going to talk about those in a second. Other common challenges indicators are unclear.

470
00:59:40,690 --> 00:59:45,700
So they don't specify what's being measured. You have too many indicators.

471
00:59:46,990 --> 00:59:52,480
You do want a few for every question, but you don't want like 20 or ten for each question.

472
00:59:54,370 --> 01:00:00,850
And so that might require you, if you have a lot of indicators, to really refine your list.

473
01:00:02,730 --> 01:00:09,570
Lack of a data source. So you might come up with a really good indicator but have no feasible way of obtaining that data.

474
01:00:10,870 --> 01:00:20,980
So those are three common challenges. And a fourth challenge is having indicators that are not linked to the program and its activities.

475
01:00:22,500 --> 01:00:27,870
So for example, I have a evaluation question about the program,

476
01:00:29,040 --> 01:00:35,730
and the question is about the relationship between safer sex education and changing condom use.

477
01:00:38,610 --> 01:00:43,790
So perhaps. There's some cover.

478
01:00:43,790 --> 01:00:49,520
You know, there's some discussion about partner violence within the curriculum.

479
01:00:50,240 --> 01:00:53,630
And so I say, well, I'm going to have an indicator number of youth that report.

480
01:00:53,660 --> 01:00:56,660
New Skills in Assessing Risk for partner violence.

481
01:00:57,020 --> 01:01:03,410
After all, it's part of the safer sex education. Um, you know, I'm going to go ahead and ask it.

482
01:01:05,300 --> 01:01:11,540
But if the effort of the program is not geared toward partner violence, it may be helpful to know.

483
01:01:12,020 --> 01:01:16,669
But. It's not really concentrating on the effort of the program.

484
01:01:16,670 --> 01:01:19,910
The effort of the program is geared toward condom use.

485
01:01:22,060 --> 01:01:26,500
So a more appropriate indicator would be a number of youth that report new condom use.

486
01:01:31,660 --> 01:01:36,820
Another challenge is focusing on outputs instead of outcomes.

487
01:01:37,240 --> 01:01:41,530
Remember your outputs simply tell you the deliverables that emerged from the program.

488
01:01:41,800 --> 01:01:45,250
They don't actually tell you if there was some sort of change.

489
01:01:46,210 --> 01:01:53,230
So again, you have this question related to the relationship between safer sex education and changing condom use.

490
01:01:54,890 --> 01:01:58,640
So one indicator that's more closely the.

491
01:02:00,140 --> 01:02:04,310
More output related number of youth that complete the education series.

492
01:02:05,060 --> 01:02:11,570
It is helpful to know. It measures participation, but it doesn't really tell you about the outcomes.

493
01:02:13,010 --> 01:02:18,170
A little better number of you that measure intention to engage in condom use.

494
01:02:18,650 --> 01:02:25,660
So it's telling you a little bit more you're getting to some outcomes are getting to short term outcomes because it's measuring attitude.

495
01:02:26,790 --> 01:02:35,020
Ideally, you're going to have some outcome related indicators that relate to the actual behavior change.

496
01:02:35,030 --> 01:02:42,110
In this case, you want to measure behavior change, you know, the condom use specifically.

497
01:02:42,590 --> 01:02:46,070
So number of youth that report new condom use.

498
01:02:47,460 --> 01:02:57,570
So again, it's helpful to have output indicators, but those belong with process indicators and you don't want to put all your eggs in that basket.

499
01:02:57,750 --> 01:03:03,420
You want to make sure that if you're doing an outcome evaluation, you have outcome indicators.

500
01:03:07,890 --> 01:03:11,970
So I want to spend the rest of the time just showing you some examples.

501
01:03:13,350 --> 01:03:17,030
We have about 15 minutes left. So. Yes.

502
01:03:28,360 --> 01:03:37,720
So yeah, good question. Good question. So, um, variables come into play when you actually start talking about measurement.

503
01:03:38,440 --> 01:03:47,750
So they are so a variable. You will create a variable related to the indicator, but it's more about context.

504
01:03:48,350 --> 01:03:52,580
So the variables, the actual value.

505
01:03:53,960 --> 01:03:57,050
Of the of the data from the data collection.

506
01:03:59,680 --> 01:04:04,140
Yeah. So it's really just like. The terms.

507
01:04:04,260 --> 01:04:07,710
It's almost like the terms change as you get further down the process.

508
01:04:09,010 --> 01:04:14,080
But it's more closely tied to the actual data collection and analysis.

509
01:04:17,380 --> 01:04:22,870
It's a good question that I was trying to find a sly. I made a slide that had all three of them on and I couldn't find it.

510
01:04:24,390 --> 01:04:27,530
Which was annoying. Could have just remade it.

511
01:04:27,530 --> 01:04:31,250
But my. I was like, Nah, I'm okay.

512
01:04:32,730 --> 01:04:40,360
So I'm glad you asked that. Any other questions? All right.

513
01:04:41,600 --> 01:04:47,210
So we're going to revisit this Internet safety program, if you remember, from our logic modeling unit.

514
01:04:48,150 --> 01:04:48,660
Um.

515
01:04:50,030 --> 01:05:03,080
This is a program to promote Internet safety among youth by not only influencing youth behavior, but influencing parent behavior and media behavior.

516
01:05:03,440 --> 01:05:15,260
Local media. So we'll go through a few examples of process and outcome indicators.

517
01:05:16,930 --> 01:05:26,370
Right. So you might. So one of the questions related to this evaluation, did the Internet safety trainings reach the intended audience?

518
01:05:27,450 --> 01:05:30,540
Okay. So, you know, we're thinking about here.

519
01:05:30,540 --> 01:05:35,430
We're thinking about reach. That aspect of process.

520
01:05:38,030 --> 01:05:43,940
So one way use that we're reaching because we have both youth and parent.

521
01:05:44,750 --> 01:05:48,980
So youth who attend and attended at least one training.

522
01:05:49,920 --> 01:05:54,230
And then we also have. The proportion of adults.

523
01:05:57,460 --> 01:06:02,430
From. Yeah.

524
01:06:02,570 --> 01:06:05,840
The proportion of adults from a particular county.

525
01:06:05,840 --> 01:06:12,350
So say the intervention is meant to focus on Buffalo Colony, Wisconsin.

526
01:06:13,430 --> 01:06:17,750
And we want to know how. To what extent do we reach adults in that county?

527
01:06:19,080 --> 01:06:27,180
So you can think about, you know, indicators that tell you we reach people in the right place, that we reach people in the right age group.

528
01:06:30,420 --> 01:06:35,460
Just to tell us a little bit more about how well we reached the people we were trying to reach.

529
01:06:38,360 --> 01:06:45,060
Another example to WebEx for this question To what extent were the media releases dispersed as intended?

530
01:06:45,080 --> 01:06:51,590
Remember that part of this intervention was about media and changing how the media talked about Internet safety.

531
01:06:52,540 --> 01:06:57,310
So the number of media releases that went out.

532
01:06:58,740 --> 01:07:04,590
Number of people who saw our media releases opinions of program staff.

533
01:07:05,370 --> 01:07:09,660
So here's an issue that's here is an indicator that gets to.

534
01:07:11,390 --> 01:07:19,600
Dose delivered. And then in contrast.

535
01:07:21,340 --> 01:07:35,490
Dose received. Even here we have opinions of program staff, so we're learning more about how the program was delivered.

536
01:07:37,100 --> 01:07:44,200
Or deliver. Here's the third example.

537
01:07:44,470 --> 01:07:49,030
So was the space functional and conducive for implementing the safety training?

538
01:07:51,990 --> 01:07:55,350
Don't know. You know, so you could look at the physical space.

539
01:07:55,530 --> 01:07:59,040
So here's one ratio of chairs, the program participants.

540
01:08:02,310 --> 01:08:07,920
Seems kind of lightweight, but. If people can't sit down, they're probably not paying attention to you.

541
01:08:08,700 --> 01:08:14,940
And it's probably also associated with this next indicator level of satisfaction among program participants.

542
01:08:16,290 --> 01:08:25,200
You know, you may want to know that because if they weren't sitting down that may drive dissatisfaction and that the program components itself.

543
01:08:26,720 --> 01:08:30,710
You might also have opinions again from program implementers.

544
01:08:31,160 --> 01:08:36,410
And notice that here's a quant and a quantitative.

545
01:08:38,040 --> 01:08:41,270
Indicator. This could be quite.

546
01:08:45,850 --> 01:08:51,190
Or it could be qualitative. And this is the same way you could you could quantify this.

547
01:08:52,650 --> 01:08:57,320
But. Likely he might want this to be a qualitative indicator.

548
01:08:58,780 --> 01:09:02,320
So you can have both quant and qual for one question.

549
01:09:07,710 --> 01:09:11,130
So now we'll kind of pivot to outcome indicators.

550
01:09:12,700 --> 01:09:18,730
In this case, the media sources in Wisconsin increased their reporting on Internet safety after the intervention.

551
01:09:19,300 --> 01:09:27,170
So, again, note we're looking at time. We want to know what changed after the intervention.

552
01:09:28,470 --> 01:09:34,290
So possible indicators number of youth who report having seen more media reports on Internet safety.

553
01:09:36,090 --> 01:09:41,700
So, you know, again, that's getting at, you know, receiving the intervention.

554
01:09:43,530 --> 01:09:48,340
We need to know. Who saw what got put out there.

555
01:09:48,940 --> 01:09:53,430
Proportion of media sources in that hat. Oops.

556
01:09:54,530 --> 01:09:59,180
Proportion of media sources that have increased reporting on Internet safety.

557
01:10:01,300 --> 01:10:05,320
And then change in total number of media reports on Internet safety.

558
01:10:06,010 --> 01:10:09,610
So again, we have media sources increase their reporting.

559
01:10:09,910 --> 01:10:17,200
We have this we have objective measures related to reporting,

560
01:10:18,160 --> 01:10:28,210
both from the perspective of the media that are releasing the reporting and the youth that are target it with the reporting.

561
01:10:30,380 --> 01:10:37,670
So you want your indicators again to be well-rounded. You don't want to just put all your eggs in one indicator because.

562
01:10:38,980 --> 01:10:45,820
If I just say I take out the I have no way of taking out the number of youth on here like me.

563
01:10:45,820 --> 01:10:53,620
I'm trying to like block it on my screen as if you can see that. Let's say we just we don't have the first one say we only have the second.

564
01:10:54,670 --> 01:10:59,410
So, yes, the media sources might be increasing reporting.

565
01:11:00,330 --> 01:11:07,110
But what we're what's being distorted is the extent to which people are actually seeing it that are supposed to see it.

566
01:11:08,530 --> 01:11:13,059
So you want something to be well rounded because you don't want to distort the

567
01:11:13,060 --> 01:11:20,230
findings to say we don't say you don't see the media sources and we just determine,

568
01:11:20,230 --> 01:11:25,210
oh yeah, the media sources. Um, they were increased.

569
01:11:25,930 --> 01:11:30,070
But, and then we, you know, at the end, youth are more.

570
01:11:31,150 --> 01:11:37,450
More aware of Internet safety. Well, we don't have a way of knowing.

571
01:11:38,540 --> 01:11:42,290
Was it because of the media sources? Because we don't know if they actually saw it or not?

572
01:11:43,520 --> 01:11:46,790
So you want to make sure you're kind of covering your bases that way.

573
01:11:49,430 --> 01:11:52,969
When I do it, I'm kind of thinking about, okay, who?

574
01:11:52,970 --> 01:11:58,710
How could one poke a hole in my argument? That's literally what I ask myself.

575
01:12:00,340 --> 01:12:04,120
Okay. So how can I prevent them from poking a hole in my argument?

576
01:12:04,570 --> 01:12:14,070
Okay, I need evidence of this as well. So I'm almost thinking through how the program is supposed to work and how the implicate,

577
01:12:14,080 --> 01:12:17,170
like the outcomes of that program is supposed to work as well.

578
01:12:18,800 --> 01:12:24,920
As I'm crafting these indicators. And then finally, the final example.

579
01:12:25,760 --> 01:12:32,030
Did the Internet safety program lead to an increase in communication between parents and youth about Internet safety?

580
01:12:32,630 --> 01:12:42,710
So again, this is the same principle. So if I want to know if the increase in communication well, that came for that communication to be therapeutic,

581
01:12:42,950 --> 01:12:47,089
both parents and youth need to perceive something about that communication.

582
01:12:47,090 --> 01:12:54,310
Right. Because you know. If parents say they're talking to kids are not hearing them, then what's the point?

583
01:12:55,530 --> 01:13:03,480
So I'm going to fight. I want to make sure that I have some measurement of parents as well as the youth.

584
01:13:04,270 --> 01:13:08,530
So when I say that the indicators need to be well rounded, that's just another example.

585
01:13:09,630 --> 01:13:15,510
You know, again, this thinking through, how is this how is this program supposed to function?

586
01:13:15,540 --> 01:13:19,950
And again, you can look when it gets to thinking about how the program functions.

587
01:13:20,920 --> 01:13:23,830
You can go back to your theory of change now.

588
01:13:24,580 --> 01:13:30,910
That's why I said, regardless if you put you do the fight the theory of change formally, it really does help.

589
01:13:30,970 --> 01:13:34,030
If you just drive out for your own reference.

590
01:13:35,190 --> 01:13:38,400
Because that helps you when you get to the stage and indicators.

591
01:13:42,120 --> 01:13:45,510
Right. Any questions about indicators?

592
01:13:46,620 --> 01:13:50,130
And you want them to be neutral. You want them to be well rounded.

593
01:13:50,940 --> 01:13:57,960
You want them to be specific and associated with your evaluation questions.

594
01:14:02,630 --> 01:14:07,370
All right, so I'll just summarize. So for goal for.

595
01:14:08,760 --> 01:14:14,850
It's really about compiling information that we can treat as high quality and trustworthy and relevant.

596
01:14:16,910 --> 01:14:20,060
And then when it comes to indicators, we want them to be specific.

597
01:14:20,570 --> 01:14:24,500
We want them to be observable. And we want them we want them to be measurable.

598
01:14:30,790 --> 01:14:34,550
Right. If no questions.

599
01:14:35,390 --> 01:14:38,900
Again, that's all I have for today. Just again,

600
01:14:38,900 --> 01:14:50,300
make sure if you haven't done the quiz you have until Sunday night and if you haven't thought about your groups since you did that logic model.

601
01:14:50,900 --> 01:14:56,600
It's time to give them my email. It's back to work. All right.

602
01:14:58,520 --> 01:15:07,400
Have a good weekend. Is mama keep saying this for the people who don't who are from Michigan is getting called if you ain't got your coat.

603
01:15:08,600 --> 01:15:16,720
You think this is something, just wake. It's gonna get colder and it's gonna be the holidays.

604
01:15:20,050 --> 01:15:23,190
Yeah. Okay. This one.

605
01:15:44,910 --> 01:15:52,640
Also, I'm going to embarrass myself on the show tonight, which is why I have.

606
01:16:22,450 --> 01:16:28,870
Yeah, yeah, yeah, yeah.

607
01:16:33,810 --> 01:16:40,440
Oh, yeah, yeah.

608
01:16:50,290 --> 01:16:57,730
What? I don't know. Now, are you saying that is just an update for.

609
01:17:02,950 --> 01:17:08,700
Oh, thank you. It's temporary, but I know why.

610
01:17:10,000 --> 01:17:17,740
Because my hair is gray. No, no, this is.

611
01:17:17,980 --> 01:17:24,430
This is all. It's gray. So I said, well, let me see what it'll look like if I put it more free.

612
01:17:28,240 --> 01:17:33,520
I mean, I'm embracing my age, but what really?

613
01:17:34,210 --> 01:17:41,440
I don't know. It's. It's getting gray. I know, I know.

614
01:17:44,980 --> 01:17:49,690
But I.

615
01:17:51,100 --> 01:17:54,820
I think it's I think I would be the queen mother because I'm.

616
01:17:54,910 --> 01:17:59,620
Because I'm big, thick, sweet. That first word refers to the person you're talking about.

617
01:17:59,960 --> 01:18:03,080
Oh, you have a friend that.

618
01:18:04,150 --> 01:18:07,240
But. But, you know. Oh, wait, I don't know.

619
01:18:07,930 --> 01:18:11,860
Oh, man, you're killing me. I don't. I don't know. Maybe I'm the king, mother.

620
01:18:12,950 --> 01:18:15,970
Oh, you are not the queen. Oh, yeah.

621
01:18:16,660 --> 01:18:20,500
Not in the past, too. I don't know. Well, I did watch the crown.

622
01:18:21,010 --> 01:18:26,139
Yes, I have. Oh, I'm not British. That's why I'm like, wait, I know this right now.

623
01:18:26,140 --> 01:18:30,040
Confused all these years. I love British history.

624
01:18:30,080 --> 01:18:33,190
It's. I don't know why, but I do. It's great.

625
01:18:34,550 --> 01:18:38,260
Oh, you're welcome to your show.

626
01:18:40,460 --> 01:18:59,870
So she. I was just talking to somebody about that candy.

627
01:19:00,620 --> 01:19:06,919
We passed. We passed, but I passed by the grocery store with with my husband.

628
01:19:06,920 --> 01:19:16,340
I might have to swing by that today. Oh, you poor that. Well, we were just driving by and I said, Oh, my student must've told me about that,

629
01:19:16,340 --> 01:19:19,220
that grocery store, because that's one I had never been to before.

630
01:19:19,370 --> 01:19:24,460
Oh, he was on by the time I got back to say, oh, you know, you're not playing around.

631
01:19:25,130 --> 01:19:31,210
I'll take a few more. You're really good. Yeah, yeah. Well, that's good.

632
01:19:31,320 --> 01:19:37,540
That's good. Can you just fly back like these things are nice?

633
01:19:37,960 --> 01:19:44,350
Yeah, they really do help. You know, it's pretty small coffee, which I think, you know, they're good.

634
01:19:47,400 --> 01:19:53,410
If they give me a temper, then I'll. If you buy how many backpacks of you buy, how many?

635
01:19:54,130 --> 01:19:57,420
Like the whole case, which is like 24 bags all around it.

636
01:19:57,850 --> 01:19:58,390
Do you buy that?

