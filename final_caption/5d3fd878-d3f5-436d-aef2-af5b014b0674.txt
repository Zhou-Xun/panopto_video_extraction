1
00:00:00,750 --> 00:00:12,740
All right, so why don't we get started? So today we have basically one very simple goal, which is trying to finish the final lecture given by myself.

2
00:00:12,750 --> 00:00:19,080
And then on Wednesday, we will be having two guests lectures, both of whom are here today, actually.

3
00:00:19,620 --> 00:00:23,430
So you guys want to introduce yourselves?

4
00:00:24,210 --> 00:00:32,730
Oh, hi. Hi. People ask me and like everyone here, my name is Kira and I'm a third year.

5
00:00:35,990 --> 00:00:39,860
All right. We have also another guest lecturer you probably were familiar with.

6
00:00:40,380 --> 00:00:45,090
Well. Yeah.

7
00:00:45,100 --> 00:00:48,159
So both of them will be given a guest lecture.

8
00:00:48,160 --> 00:00:56,709
I believe each one of them will have about 40 minutes. So they will they are considering a career probably involving teaching in various format.

9
00:00:56,710 --> 00:01:07,480
So, um, you know, they will be practicing partly and also trying to teach you guys a little bit about longitudinal modeling using margin models.

10
00:01:07,780 --> 00:01:10,870
So that's what we will be doing on Wednesday.

11
00:01:10,870 --> 00:01:18,400
I will be here as well. So if you if you or your friends who are actually in this class are interested, please join.

12
00:01:19,210 --> 00:01:22,240
And I think the lectures will be recorded, of course, will not be exams,

13
00:01:22,810 --> 00:01:27,280
but I think I know that they will be very appreciative if you guys can show up in person.

14
00:01:27,970 --> 00:01:35,110
All right. Today, it's about missing data, but before that, I just want to say a few logistic things.

15
00:01:37,030 --> 00:01:42,760
First, homework done before I posted today. And the due date actually is on December 17th.

16
00:01:43,240 --> 00:01:47,050
And so you have more than two weeks to do it if you choose to.

17
00:01:47,440 --> 00:01:54,350
So essentially it is an optional homework. If you click it, you will see two questions.

18
00:01:54,370 --> 00:02:05,950
The first question is a theoretical question. It is trying to get at the robust variance kind of concept for G estimation.

19
00:02:06,340 --> 00:02:13,149
So it is a seemingly very simple exercise, but I think if you can work this out independently,

20
00:02:13,150 --> 00:02:18,200
I think you will master the concept pretty well if you choose to do so.

21
00:02:18,200 --> 00:02:26,740
The first question of a two is a data analysis exercise, so it will kind of give you an opportunity to practice the fitting of G.

22
00:02:28,240 --> 00:02:31,690
So what about the grading?

23
00:02:32,140 --> 00:02:38,950
Essentially, if a person choose not to do homework for the final project will be 40%.

24
00:02:41,560 --> 00:02:48,400
So it is basically comprised of the original 40% of the project's credit and also 8% for the homework

25
00:02:48,400 --> 00:02:54,640
full credit that if a person thinks that he or she can achieve a higher score on homework four,

26
00:02:55,180 --> 00:02:58,600
then clearly you want to get that at 8%, right?

27
00:02:59,050 --> 00:03:07,330
Because you may get a low scoring project. So if you want to do that, do the homework for and then try your best on the project.

28
00:03:07,960 --> 00:03:14,050
So that's another strategy. But clearly you can balance your time with the credit to want to get.

29
00:03:16,150 --> 00:03:21,550
And I think the goal here is trying to give you some flexibility in adjusting the workload

30
00:03:22,090 --> 00:03:29,200
while allowing yourself others to practice the older technique if they choose to do so.

31
00:03:30,460 --> 00:03:35,510
So any questions among audience? About this grading scheme.

32
00:03:43,320 --> 00:03:46,800
All right. Number two, projects.

33
00:03:47,010 --> 00:03:51,629
So we are pretty much one week away from project presentation.

34
00:03:51,630 --> 00:03:54,510
And I just want to provide a few explanations.

35
00:03:56,370 --> 00:04:04,380
So if we click this form, you will see that there are 12 teams we have for the entire next week to do this.

36
00:04:04,770 --> 00:04:09,630
And for next week, you're not required to submit a not required to submit a final report.

37
00:04:09,960 --> 00:04:18,930
So all you need to do is to present preliminary analysis the best you can and show the audience, which means the other students in the class.

38
00:04:19,350 --> 00:04:22,079
And GSI and myself. What you've done.

39
00:04:22,080 --> 00:04:30,180
And we will be able to see whether you guys can articulate the question, the solution, the finding, and the limitations.

40
00:04:30,690 --> 00:04:37,800
I have read proposals and I think most of them are extremely awesome and I have seen some very creative use of data set.

41
00:04:38,460 --> 00:04:42,180
Some of you have come to my office hours for comments and I have given them.

42
00:04:42,180 --> 00:04:49,560
I think, as you can see, I'm in general very supportive of you just applying the techniques in the class to do the analysis.

43
00:04:50,280 --> 00:04:59,700
But if you do one, have more conversations, you know, I can be here today after class for 30 minutes if you want to discuss.

44
00:05:01,170 --> 00:05:06,809
So each team will be present presenting a few slides.

45
00:05:06,810 --> 00:05:10,410
You guys can choose however you want to do the presentation task.

46
00:05:10,410 --> 00:05:14,700
You can have one person take over or you can have like four people or three people rotate.

47
00:05:14,790 --> 00:05:22,680
That's your decision. But definitely consider what's the best way to optimize the kind of the effectiveness of the presentation.

48
00:05:24,290 --> 00:05:28,940
The order will be randomized, so I will release the order on Wednesday.

49
00:05:30,050 --> 00:05:38,900
So you guys will have an expectation about the presentation order finally before the class or before the.

50
00:05:40,000 --> 00:05:43,530
Class that your team will be scheduled present.

51
00:05:43,600 --> 00:05:48,160
Please put your slides the link to the slides in this column.

52
00:05:49,300 --> 00:05:55,090
For what reason though? First I can take a look at during the presentation, I can look back and forth.

53
00:05:55,670 --> 00:06:00,360
The second is that it's also an opportunity for your classmates to learn, you know,

54
00:06:00,790 --> 00:06:05,020
the skills you have developed in terms of data analysis and thus a presentation.

55
00:06:05,530 --> 00:06:14,230
And I find it very effective for everybody to have a copy and just to look at the numbers and those are the equations.

56
00:06:15,250 --> 00:06:21,640
So this is a rough plan, and I think all the specific instructions will are already specified here.

57
00:06:22,000 --> 00:06:28,420
So I will not repeat them for the final report because we will not have the time to talk about these logistics.

58
00:06:30,220 --> 00:06:34,930
During the presentation, I am going to say that, you know, please control the number of pages,

59
00:06:34,930 --> 00:06:40,600
total pages to ten, because it will be a lot of reading for both myself and the GSI.

60
00:06:40,990 --> 00:06:49,750
And we want to give you the most pertinent comments as possible, because for the final project report that determines your final report.

61
00:06:50,410 --> 00:06:53,560
Great. Historically, we have lots of variation.

62
00:06:53,770 --> 00:06:58,120
There are some very excellent report, some very disorganized reports.

63
00:06:58,510 --> 00:07:04,090
So if you want to sort of get a sense of what kind of we want to read,

64
00:07:05,440 --> 00:07:13,120
please feel free to jump by the office hours or talk to me or GSI or generally ask on Piazza.

65
00:07:13,600 --> 00:07:21,340
And I think that as long as you try your best, I think that you should be able to produce a logical and readable report.

66
00:07:24,530 --> 00:07:30,649
And also all the reports will be graded on the following criteria like criteria.

67
00:07:30,650 --> 00:07:32,120
By criteria, we will break them.

68
00:07:32,450 --> 00:07:41,630
So, for example, has the report described the scientific question or statement aims clearly that that's worth ten points?

69
00:07:42,050 --> 00:07:45,260
Okay. I can provide a grade like nine or like three.

70
00:07:45,590 --> 00:07:48,770
Okay. So it depends on how that is being communicated.

71
00:07:49,250 --> 00:07:57,980
Second description of an analytic approach. For example, if a person says, Hey, I'm trying to do the individual level trajectory prediction.

72
00:08:00,020 --> 00:08:07,900
That accounts for between subject, subject area abilities and then this person using mixed model, then that's immediate 10% off.

73
00:08:07,910 --> 00:08:13,760
Okay, for this question because clearly that demonstrated that this team has not understood the technique.

74
00:08:15,050 --> 00:08:25,460
So use of data has the team figured out that data has what kind of advantages and disadvantages?

75
00:08:25,790 --> 00:08:36,139
And I want to say that it is totally okay for data set to be a little bit raw to be a little bit I wouldn't say problematic, but of some issues.

76
00:08:36,140 --> 00:08:41,870
For example, missing data, for example, sample size, for example, number of measurements per person.

77
00:08:42,260 --> 00:08:45,260
And I think that, you know, first and foremost,

78
00:08:45,260 --> 00:08:50,809
I think I want to encourage you all to be a bit more adventurous instead of like optimizing for grades.

79
00:08:50,810 --> 00:08:59,570
Although I know that's one of the most important goals, I think I will be generally lenient on the data issues as long as as long as

80
00:08:59,570 --> 00:09:03,379
I see you figuring out what's the part of data that's suitable for analyzes.

81
00:09:03,380 --> 00:09:11,150
What's part of data is have issues that you cannot overcome in the short period time as like a communicate that honestly I will not penalize that.

82
00:09:11,510 --> 00:09:18,020
But if you clearly hide some issues that can tell like within 5 seconds and that does not speak well to that report.

83
00:09:21,290 --> 00:09:25,940
So. That's appropriate use of concepts and methods.

84
00:09:25,970 --> 00:09:29,760
I think that's probably the clearest clue.

85
00:09:29,810 --> 00:09:35,870
You want to use the right method for the right question. Second, the last interpretation results for use in public health decision making.

86
00:09:36,230 --> 00:09:39,980
So this one is a little bit, you know, depending on the question.

87
00:09:39,980 --> 00:09:45,710
So I would not say there is specific role as long as I can tell exactly what's the

88
00:09:46,310 --> 00:09:50,360
relevance of the question and why you want to provide certain kind of answers.

89
00:09:50,630 --> 00:09:54,230
Not my ultimate answer, but certain kind of answers as first step towards that.

90
00:09:54,260 --> 00:09:59,090
Answering that scientific question, that's that's considered a good interpretation.

91
00:09:59,480 --> 00:10:05,390
Finally, the quality presentation. It includes writing the report and delivering the oral presentation.

92
00:10:06,020 --> 00:10:11,780
So why is this worth 10%? After all, communication is one of important goals.

93
00:10:12,110 --> 00:10:19,460
And I do not want to sort of say that you want walk out of the classroom without talking a complete sentence about your project.

94
00:10:20,570 --> 00:10:25,700
So I do want to encourage you guys to practice a little bit before the presentation and also to, you know,

95
00:10:26,360 --> 00:10:33,170
revise the final report to the degree you can so that the final report is logical and coherent.

96
00:10:35,210 --> 00:10:38,920
And I sometimes receive questions about, Hey, June,

97
00:10:39,290 --> 00:10:44,130
there are some results that's going to be changed between the presentation and actually final report.

98
00:10:44,150 --> 00:10:44,840
Is that okay?

99
00:10:45,740 --> 00:10:54,680
I say that's okay because you know, it is the presentation probably is one of the most important occasions you guys can spot any inconsistencies,

100
00:10:54,680 --> 00:10:59,720
you know, on the standing of the project. So you can smooth that out while you were.

101
00:11:00,140 --> 00:11:05,660
You guys are preparing the final report. So for the oral oral presentation,

102
00:11:06,770 --> 00:11:16,520
what I am looking for is generally a structure and certain level of specificity of the techniques and data analysis results.

103
00:11:17,330 --> 00:11:23,000
Of course, if you change completely like 90% of results, then that is going to be a red flag.

104
00:11:23,540 --> 00:11:27,050
But otherwise, if you change a little bit, I think that it will be okay.

105
00:11:30,940 --> 00:11:35,290
So happy to answer any questions you have any regarding the project.

106
00:11:41,810 --> 00:11:48,580
Okay. Now let's just switch to the lecture.

107
00:11:48,920 --> 00:11:52,610
So for this particular handout, it's now 11.

108
00:11:53,330 --> 00:12:03,350
Our goal is trying to cover at a high level the mission data issues, the definition of different classes of missing data mechanisms.

109
00:12:03,710 --> 00:12:07,730
And one example that's probably a little bit more advanced example of how to

110
00:12:07,730 --> 00:12:14,270
deal with missing data in the context of G models and pattern mixture models.

111
00:12:14,630 --> 00:12:22,850
As you will see, many techniques are actually involved by faculty in this department or advisors of faculty in this department.

112
00:12:23,210 --> 00:12:27,140
So essentially this is a very classical field.

113
00:12:27,860 --> 00:12:32,600
And I think that as some of you have asked me frequently, how do I deal with missing data?

114
00:12:33,500 --> 00:12:35,390
Some of these techniques are very useful.

115
00:12:37,160 --> 00:12:43,760
So but when you are doing your own projects, I understand that you guys have not fully been taught our mission data.

116
00:12:44,090 --> 00:12:49,100
So as long as you provide a certain level of justification of the missing data mechanism, that's okay.

117
00:12:50,060 --> 00:12:56,090
And if you are even going to use like multiple imputation already, then that's that's more power to you.

118
00:12:56,930 --> 00:13:00,110
But I would treat that as a surprise. That's not what I expect.

119
00:13:00,380 --> 00:13:10,250
But try your best. If you do like missing complete random kind of analysis, you've got to write out all the possible limitations.

120
00:13:10,380 --> 00:13:14,300
Okay, so what's the learning objective for today?

121
00:13:15,140 --> 00:13:20,420
Two objectives. The first one is to classify the different kinds of missing data mechanisms.

122
00:13:20,750 --> 00:13:24,350
The first one is called M car missing completely at random.

123
00:13:24,360 --> 00:13:28,430
The second is missing at random, and the final one is not missing at random.

124
00:13:29,180 --> 00:13:37,430
So these are some somewhat mechanical words, and we will try to put them in mathematical terms and give you some examples where they are.

125
00:13:38,030 --> 00:13:47,990
So if you can tell, if you can answer yourself, what are these mechanisms after the course, you will have understood the definitions.

126
00:13:48,890 --> 00:13:54,050
Number two, explain the implications of missing data on longitudinal data analyzes.

127
00:13:54,290 --> 00:14:00,679
So essentially, we will be I'll give you guys one example of two different sets of results depending

128
00:14:00,680 --> 00:14:04,550
on whether you account for missing data that is based on way weighted G actually.

129
00:14:05,630 --> 00:14:10,250
So let's get started with the first part. The first part is on our missing data.

130
00:14:10,490 --> 00:14:13,610
So the most important thing is about patterns and mechanisms.

131
00:14:15,790 --> 00:14:21,520
So generically there are a monotonic monotone with non monotone missing data patterns.

132
00:14:21,880 --> 00:14:28,300
So in the monotone missing data pattern, individuals have missing on one verbal awesomeness, all the following observations.

133
00:14:28,660 --> 00:14:35,320
That is, mRNAs is nested visually. This is what it looks like if you order the variables.

134
00:14:36,070 --> 00:14:40,990
So order the variables in the columns by the percentage of mRNAs.

135
00:14:42,130 --> 00:14:47,500
Okay. So you put the variable with the least mRNAs, so that's the most common and so on and so forth.

136
00:14:47,980 --> 00:14:56,470
And also re-order the individuals on the rows so that the individuals with the most complete information variables are at the top.

137
00:14:56,740 --> 00:15:01,960
You will have something like this, right? By monotonic mRNAs, it means that.

138
00:15:03,310 --> 00:15:11,530
If you look at this particular subject, it has observed information up to three and then missing information afterwards.

139
00:15:11,950 --> 00:15:16,720
So this is what we call monotone listeners.

140
00:15:22,780 --> 00:15:28,820
The second. Class is called no monotone or intermittent mRNAs.

141
00:15:29,150 --> 00:15:34,600
So if you look at this particular know. Table.

142
00:15:34,740 --> 00:15:42,840
So it is showing you for three measurements on any subject time point one, two and three.

143
00:15:43,350 --> 00:15:47,340
And it is allowing such kind of occasions. Right. Missing.

144
00:15:47,790 --> 00:15:51,899
And then this person got measured again. Missing.

145
00:15:51,900 --> 00:16:00,870
Missing. And then that measured again. Right. So monitor missing is basically excludes row number three, five, six and seven.

146
00:16:01,140 --> 00:16:09,090
So number and so all into minute missing this is more how to say what general and theoretically it

147
00:16:09,090 --> 00:16:13,800
is somewhat more difficult to deal with and we are not going to talk about how to deal with this.

148
00:16:14,640 --> 00:16:20,670
But this is the terminologies to talk about missing the mechanism.

149
00:16:20,670 --> 00:16:29,880
It is important to introduce a few notations that can be quite confusing at the beginning, but I'll try to try to explain what they mean.

150
00:16:30,540 --> 00:16:36,600
So in general, when people are studying missing data, they first put forward the complete data,

151
00:16:36,990 --> 00:16:41,460
which means that what if we measure everything we want to measure?

152
00:16:41,820 --> 00:16:45,750
What is the value? And clearly every value like the outcomes will be random.

153
00:16:45,750 --> 00:16:52,710
So it will be why? So just imagine that's a collection of vectors from subjects.

154
00:16:54,760 --> 00:17:02,170
And then I from one end and why I could be a vector where I indicate using the tilt underneath.

155
00:17:03,880 --> 00:17:08,680
So these are called complete data and it is split into two parts.

156
00:17:09,190 --> 00:17:14,290
Often y i obs and a y i miss.

157
00:17:16,250 --> 00:17:23,960
Okay. So then people just write down like, you know, these two parts separately.

158
00:17:27,540 --> 00:17:33,330
Just mechanically write them down. So this is what we have here.

159
00:17:34,260 --> 00:17:39,569
What this does is trying to start from the complete data indicate which part

160
00:17:39,570 --> 00:17:45,300
of the data is not observed to indicate which part of data is not observed.

161
00:17:45,670 --> 00:17:55,110
Often we will introduce this indicator verbal RJ, which are is short for response.

162
00:17:55,110 --> 00:17:58,430
Right. Short of a response.

163
00:17:58,970 --> 00:18:06,920
And RJ. So this one is introduced specifically for the longitudinal setting where I for the subject change for the occasion.

164
00:18:07,310 --> 00:18:13,310
So it equals one. Then it means that it's observed. If it equals zero, then the occasion.

165
00:18:13,550 --> 00:18:20,040
You have a missing. Okay. So then put together what is the observed data?

166
00:18:20,220 --> 00:18:32,910
Well, you do observe. Which for each individual, which occasion have this person's outcome information, say, I have our information here.

167
00:18:33,610 --> 00:18:41,110
The first part essentially is the sub vector of the observed outcome in the entirety of this lecture.

168
00:18:41,830 --> 00:18:46,810
I don't believe I put in a lot of covariates. It is because that often we.

169
00:18:48,320 --> 00:18:52,550
In this lecture, at least we consider only the missing outcome situation.

170
00:18:53,210 --> 00:18:57,170
So when you're wondering where do the covers go?

171
00:18:57,890 --> 00:19:03,380
Consider them been conditioned upon. So we are looking at individuals with within the same covariance strata.

172
00:19:06,540 --> 00:19:13,080
So missing there. The mechanism basically concerns the distribution of the indicators.

173
00:19:14,150 --> 00:19:27,530
Ah. Given the wise. Which is to say that given the entirety of the.

174
00:19:28,460 --> 00:19:36,740
A complete information, the outcomes. What's the chance that we observe the, you know, response indicators?

175
00:19:37,650 --> 00:19:41,240
Right. So it is that's why we call this missing data mechanisms.

176
00:19:42,110 --> 00:19:43,670
And in the next bullet,

177
00:19:44,330 --> 00:19:53,120
I was just having a disclaimer that everything is being conditioned upon x so we assume no missing coverage unless otherwise stated.

178
00:19:53,480 --> 00:19:57,230
A missing covariance is another piece and I don't think I have the time to tackle.

179
00:19:57,530 --> 00:20:03,590
And I think yeah, that's pretty actually challenging from many of the real studies though.

180
00:20:05,750 --> 00:20:13,629
So first. Given this need to study the conditional distribution now,

181
00:20:13,630 --> 00:20:23,380
given why we need to classify them into three broad classes of missing data mechanism, the first one is called missing completely random.

182
00:20:23,410 --> 00:20:27,100
Mathematically, you can see this is just like R and Y independent.

183
00:20:27,400 --> 00:20:29,560
Right. And why is this important?

184
00:20:30,340 --> 00:20:39,400
It is to say that whether or not, you know, the observation is missing has nothing to do with the underlying true outcome.

185
00:20:43,760 --> 00:20:48,860
Right. The second class is called missing at random.

186
00:20:49,580 --> 00:20:55,819
So. The probability of argument y. Remember, on the left hand side, the Y is complete information.

187
00:20:55,820 --> 00:21:03,620
On the right hand side, these are the observed information. This equation says people can have massive responses,

188
00:21:04,040 --> 00:21:13,400
but that that the probability of missing is happening has only everything to do with the observed information.

189
00:21:18,010 --> 00:21:21,340
Finally not missing at random, which is to say that.

190
00:21:23,170 --> 00:21:32,830
The probability of the missing indicators or the response indicators, given the complete information, depends on those information that's missing.

191
00:21:34,820 --> 00:21:39,470
So whether or not the observation as observed, depends on the quantities you are not able to observe.

192
00:21:44,250 --> 00:21:47,870
So some examples are missing completely random.

193
00:21:47,880 --> 00:21:53,400
For example, patients missed a scheduled visit because of bad weather or car out of service.

194
00:21:53,850 --> 00:21:59,730
So you ask yourself whether a probability of ah given y is probability of all right.

195
00:22:00,610 --> 00:22:06,450
You know. If this reason the missing this is true, then.

196
00:22:07,460 --> 00:22:12,500
Because whether or not out of service likely is a random, complete random event.

197
00:22:12,890 --> 00:22:17,290
So this kind of assumption is going to be plausible. Number two.

198
00:22:19,120 --> 00:22:23,590
Older people may have older people may have a higher chance of dropping out of study.

199
00:22:24,460 --> 00:22:33,400
Suppose the age is observable. So this means that you know the chance of a person missing the outcome measurement.

200
00:22:34,260 --> 00:22:38,880
Is dependent upon the age which is observing information.

201
00:22:39,950 --> 00:22:48,270
Right. If that's the only factor that's determining business patterns, then this is missing Edwin finally not missing our random,

202
00:22:48,960 --> 00:22:53,340
for example, subjects jump out because they have poor treatment outcomes or they die.

203
00:22:54,540 --> 00:22:59,790
So this is to say that the fact that we do not see a patient coming back to the clinic to take

204
00:22:59,790 --> 00:23:06,390
certain kind of measurements is depend on the outcome we wanted to have measured on this person.

205
00:23:06,450 --> 00:23:09,970
I. Maybe this person just recent got worse.

206
00:23:10,360 --> 00:23:14,500
But because we did not measure this person's information, we have no way to tell.

207
00:23:15,500 --> 00:23:20,070
It is just a plausible assumption. Right.

208
00:23:20,430 --> 00:23:30,270
So I think I sort of asked, you know, what's the suppose are is the indicator of whether a student is in 653 in person.

209
00:23:30,300 --> 00:23:39,190
Right. And suppose Y is a grade. So what's the relationship between the two?

210
00:23:50,320 --> 00:23:56,820
Well, I hope I hope I hope that this is true. Sorry.

211
00:23:56,850 --> 00:24:03,680
This is true. I hope.

212
00:24:04,160 --> 00:24:07,860
But could the assumption could be violated anyway.

213
00:24:08,420 --> 00:24:12,700
So. For these reasons.

214
00:24:12,700 --> 00:24:21,100
As you can see, whenever you see missing data, you have an additional step to ask yourself, Hey, how should I deal with the missing data?

215
00:24:21,400 --> 00:24:24,670
Is one of three assumptions more plausible in the current setting?

216
00:24:25,630 --> 00:24:33,380
So you know. If you do not have to deal with missing data, please just try your best to avoid missing data.

217
00:24:33,860 --> 00:24:39,530
So that's why we say that it is quite important to have a good conduct of the studies.

218
00:24:40,220 --> 00:24:48,500
In my experience, all the best collaborations occur with a very strong study team that execute the protocols,

219
00:24:49,130 --> 00:24:54,170
sort of engage the study participants very well. So the missing data rate is kind of low.

220
00:24:56,310 --> 00:25:02,580
And I think in those cases is usually a good it's a good situation for the statisticians.

221
00:25:04,170 --> 00:25:12,090
But in my in my experience, they also some studies that it's kind of like have people missing the information at the end of the study.

222
00:25:12,360 --> 00:25:16,890
So it's going to be challenging. And I think then that's where all these techniques are going to come in.

223
00:25:16,900 --> 00:25:25,830
You know, how can a rescue this? Right. So it is you know, it is probably one of the reasons why statisticians have jobs,

224
00:25:26,640 --> 00:25:34,680
because these are like techniques that can rescue certain like $1,000,000,000 effort, you know, after some missing missing data issues.

225
00:25:35,910 --> 00:25:40,470
But do remember that although we can fix this using certain techniques,

226
00:25:40,860 --> 00:25:45,780
but whenever you can try to encourage your collaborators or principally investigators to

227
00:25:45,780 --> 00:25:51,540
make sure that they do the best practices to collect as complete information as possible.

228
00:25:53,030 --> 00:26:00,740
So those are the conceptual pieces. Second part is about certain specific models we can play with.

229
00:26:01,220 --> 00:26:06,260
And this part is going to be a little more technical and we'll provide certain examples.

230
00:26:08,390 --> 00:26:13,250
There are a few models to model a missing data.

231
00:26:13,610 --> 00:26:19,070
So the first one is called selection model. It basically tries to model this.

232
00:26:23,060 --> 00:26:31,500
The second model is called pad mixture model. It basically tries to model this.

233
00:26:33,080 --> 00:26:42,770
I'll explain. So the first model is to say, hey, given the complete information, how do we invest?

234
00:26:42,770 --> 00:26:45,680
How does the missing data occur and can we model this?

235
00:26:46,250 --> 00:26:54,260
The second kind of model is to say, okay, let's have a we do know for every individual what's the missing data pattern, right?

236
00:26:54,560 --> 00:27:04,220
If you scroll back to this particular slide, you do know for three occasions how many people fell into different rows.

237
00:27:04,610 --> 00:27:12,019
So we do observe the patterns and then the task is continued by modeling the condition,

238
00:27:12,020 --> 00:27:16,220
the distribution, the outcomes, the information, given those kind of patterns.

239
00:27:16,580 --> 00:27:19,800
So the first model.

240
00:27:19,820 --> 00:27:29,020
Have you guys heard about a person called Heckman? And this guy won Nobel Prize because of Heckman selection model.

241
00:27:30,270 --> 00:27:34,770
Do you know who invented second model? Well, yes.

242
00:27:35,580 --> 00:27:38,760
So that is our part anyway.

243
00:27:38,760 --> 00:27:44,850
So that's why I say people invent great things and not too far away, actually.

244
00:27:45,270 --> 00:27:49,889
So finally, you know, those are two models.

245
00:27:49,890 --> 00:27:58,170
Finally. There will be one example to demonstrate to you, you know, you know, certain complications about not missing at random.

246
00:27:59,310 --> 00:28:07,890
So to set up these models, we need to talk a little bit about how to say ignore ability.

247
00:28:08,830 --> 00:28:13,220
So we are now into the modeling modeling space.

248
00:28:13,240 --> 00:28:18,459
How do we do the model? The first one is about the low cost function.

249
00:28:18,460 --> 00:28:23,520
Remember, like the function. Mostly it's about observed information.

250
00:28:23,530 --> 00:28:32,950
So on the left hand side, we're only going to ask what's the probability, joint probability for the observed outcomes and the response indicators.

251
00:28:34,420 --> 00:28:41,680
And because this one y obs is obtained by what?

252
00:28:42,280 --> 00:28:45,610
By removing the Y Ms. from the entire complete vector of information.

253
00:28:46,000 --> 00:28:49,090
So you just write down the entire thing and then degrade that y miss out.

254
00:28:49,270 --> 00:28:52,660
Right. So this is totally legitimate.

255
00:28:53,710 --> 00:29:02,980
And the difference between the selection model and the pandemic model essentially is on how we decompose the thing in the middle.

256
00:29:03,810 --> 00:29:05,430
For the selection model.

257
00:29:06,120 --> 00:29:14,940
Essentially, you sort of decompose by putting this out first and then do the conditional distribution for the pattern mixture model is the other way.

258
00:29:17,460 --> 00:29:25,760
So. One thing that actually amazed me when I was learning these differences is that this is such a simple thing to do.

259
00:29:26,860 --> 00:29:34,370
And if you just learn by logic, there's no criteria for choosing which decomposition is better.

260
00:29:35,200 --> 00:29:37,650
But it is when you are trying to do data analysis,

261
00:29:37,660 --> 00:29:44,320
trying to understand which one will have more interpretation advantages that it decide which way to go.

262
00:29:44,800 --> 00:29:53,590
So that's the lesson that do not rely on the you have to develop a certain kind of taste or reason why certain models need to be developed.

263
00:29:54,550 --> 00:29:59,680
We will talk about each one of them. So selection model.

264
00:30:00,010 --> 00:30:04,809
This is called optional slide, but I do want to go through them. So in the selection model,

265
00:30:04,810 --> 00:30:08,950
we will need to model the marginal distribution of the outcome and the conditional

266
00:30:08,950 --> 00:30:12,820
distribution of the missing response given they are given the outcomes.

267
00:30:13,690 --> 00:30:21,459
So they are quite nice because the model this thing here I am conditional X, but as I said,

268
00:30:21,460 --> 00:30:26,140
whenever you don't see an X in any of these conditional distributions, they are always conditional upon.

269
00:30:26,920 --> 00:30:30,880
So it is because the second part. Sorry, this part.

270
00:30:33,730 --> 00:30:38,890
You're modeling something like a regression model, right? Because you can condition covariance and do whatever model you want.

271
00:30:40,380 --> 00:30:43,080
So it's actually quite popular, however, to do this.

272
00:30:43,080 --> 00:30:52,340
It often involves lots of computation, which I will not detail, and the results may heavily depend on the modeling assumptions and identified below.

273
00:30:52,350 --> 00:30:58,180
That can be difficult to characterize. And I believe the final statement likely needs to be updated.

274
00:30:58,200 --> 00:31:06,150
There are many software now that can set Bayesian models. So indeed, this kind of model is relatively computationally intensive.

275
00:31:07,110 --> 00:31:10,889
The second class model is based on another decomposition rate.

276
00:31:10,890 --> 00:31:16,350
So as we call it, the needs to model this thing first and then why OBS?

277
00:31:16,350 --> 00:31:19,910
Why Ms. And the missing patterns?

278
00:31:20,280 --> 00:31:25,170
So the first part is basically the marginal distribution of the missing response indicators.

279
00:31:25,560 --> 00:31:30,480
This often is easier to do because you do observe the patterns.

280
00:31:31,200 --> 00:31:37,050
Number two, it is a conditional distribution outcome is missing response patterns.

281
00:31:37,380 --> 00:31:44,820
What does that mean? Well, because it's conditional upon R and R determines what kind of person you are based on the Ms. and the other pattern.

282
00:31:45,210 --> 00:31:49,410
For example, if I if we have five measurements scheduled for each person,

283
00:31:49,890 --> 00:31:56,010
I am the person I am the kind of person who comes into the study every time without fail.

284
00:31:56,040 --> 00:32:02,370
Right. So it is going to pose a model for people like me who showed up in every visit.

285
00:32:03,180 --> 00:32:09,360
There is another kind of person who only showed up for once, let's say for twice the first and second occasion.

286
00:32:10,170 --> 00:32:15,630
Out of the five scheduled and then never showed up from the third occasion on.

287
00:32:16,110 --> 00:32:25,380
Right. So we have more than one such people. And then the second bullet is to say, hey, let's put another model onto this kind of people.

288
00:32:25,800 --> 00:32:35,370
So essentially is for stratifying people by the mission data patterns and then impose models in each of the strata strata up.

289
00:32:35,850 --> 00:32:40,050
So it comes with certain I to say.

290
00:32:41,600 --> 00:32:45,350
Disadvantages, I would say so. For example, the interpretation,

291
00:32:45,350 --> 00:32:52,220
the regression coefficients is conditional on the missing response patterns and has less attractive interpretation in practice.

292
00:32:52,640 --> 00:32:56,100
So you may be wondering, where did I talk about regression coefficients?

293
00:32:56,120 --> 00:33:00,589
I just want to say that I have assumed that we can condition x.

294
00:33:00,590 --> 00:33:06,500
So if you condition acts you can do regression. And I have ignored all the x in this.

295
00:33:06,830 --> 00:33:13,610
Most of that's in this slide. So that's what I meant. You can model this, but the model depends on what stratum a person is in.

296
00:33:17,100 --> 00:33:23,430
However, Pandemic's model has been kind of quite popular because it sort of helped people understand

297
00:33:23,820 --> 00:33:30,480
the missing data mechanism and assumptions and and would also have some identifiable issues.

298
00:33:34,130 --> 00:33:39,110
So for this one, actually, I'm going to indeed Skip, because this is a bit more complicated.

299
00:33:43,450 --> 00:33:49,120
Okay. I want to jump to this one. Ignore ability.

300
00:33:53,790 --> 00:33:58,710
So this is the single most important concept of missing data.

301
00:33:59,160 --> 00:34:01,960
At least when you first start to learn missing data.

302
00:34:02,670 --> 00:34:11,190
Analyzes claim missing a random missing complete random are ignorable for likelihood based inference.

303
00:34:11,190 --> 00:34:17,580
That is the likelihood based inference using only the observed data is invalid under and they are an MHC MKR.

304
00:34:18,270 --> 00:34:19,740
So I'll explain what that means.

305
00:34:20,220 --> 00:34:27,060
There are some notations here involving some Greek letters, but hopefully by me explaining it, it will be clear to you.

306
00:34:28,740 --> 00:34:41,400
So let's get started. The first one is that, remember, this is the observed data like likelihood way obs is the part of the outcome that's recorded.

307
00:34:41,910 --> 00:34:49,800
Ah is the response patterns you do observe both of them. I have introduced two vectors, theta and phi.

308
00:34:50,100 --> 00:34:56,429
Okay, so and in general Phi Phi Alpha represent some nuisance parameters or something.

309
00:34:56,430 --> 00:35:00,240
You don't care too much. Theta often represent the parameter of interest,

310
00:35:00,570 --> 00:35:11,040
so I am distinguishing them precisely because theta often is hit for this model in the outcome model and phis potentially

311
00:35:11,040 --> 00:35:20,339
modeling the missing missing data pattern mechanism because often you do not care about how for this particular study,

312
00:35:20,340 --> 00:35:25,680
the missing data occurred. Right, because we do another study and this data pattern may be different, the mechanism may be different.

313
00:35:26,100 --> 00:35:31,560
So the fire is introduced to accommodate whatever which future the study may have.

314
00:35:32,610 --> 00:35:36,990
So let's write down this entire likelihood again.

315
00:35:38,130 --> 00:35:42,960
The inside essentially is probability of y ops, y Ms.

316
00:35:44,110 --> 00:35:51,540
And. Ah. It just. It just follows this particular petition.

317
00:35:51,910 --> 00:36:01,800
Sorry. Decomposition. Okay.

318
00:36:01,950 --> 00:36:08,340
So you have these it has these two terms. And now, look, theta has been introduced into here.

319
00:36:09,390 --> 00:36:12,470
It is often representing, say, regression coefficients.

320
00:36:12,480 --> 00:36:14,520
Again, I am ignoring the covers here,

321
00:36:14,820 --> 00:36:22,920
but theta often represents how the outcome is related to the keywords and this is kind of the primary parameter, primary interest.

322
00:36:23,610 --> 00:36:29,280
The second part, as you can see it, is characterizing, given the complete information about the outcome,

323
00:36:29,280 --> 00:36:32,640
what's the joint distribution of the response patterns?

324
00:36:33,060 --> 00:36:37,170
And that can be a nuisance because that's not we want to study.

325
00:36:37,180 --> 00:36:41,970
It has nothing to do with the health of the person. How that's determined by risk factors.

326
00:36:42,450 --> 00:36:46,560
So this is going to be influence in that second model.

327
00:36:48,090 --> 00:36:51,930
Now, under certain assumptions, we can further simplify this.

328
00:36:52,260 --> 00:36:57,270
For example, under missing at random. Now you have to recall what's missing in a random.

329
00:36:57,570 --> 00:37:02,100
It is to say that the chance you observe certain part of the information,

330
00:37:03,090 --> 00:37:08,640
although in theory it must depend on all the information, but it may ah allows you to simplify it.

331
00:37:09,270 --> 00:37:19,530
It allows you only to depend on observed part of information. So that means for the second term here, you can simplify this to this one, right?

332
00:37:21,980 --> 00:37:25,890
Now I am highlighting that term. Here.

333
00:37:26,730 --> 00:37:31,030
Now, what do you see? Does the term involve Ms.

334
00:37:32,530 --> 00:37:38,110
It does not involve women's, but the integral is with respect to what women's right.

335
00:37:38,710 --> 00:37:44,130
So what you can do is basically to. Take this up and throw this out.

336
00:37:45,840 --> 00:37:54,150
While the first term in the integrated depends on what meant. So you do have to put in there so what that if you have done this and then.

337
00:37:55,120 --> 00:37:59,639
If I'm going to use green color. So the green part with the Y.

338
00:37:59,640 --> 00:38:04,170
Ms. integrate it out. What is that? It is just this thing, right?

339
00:38:14,370 --> 00:38:22,890
And this is just what's factored out. Now, I need you to focus on this piece that I circle.

340
00:38:23,890 --> 00:38:32,260
Which is to say that the observed, the likelihood can be factored into two parts where one part the first term why

341
00:38:32,260 --> 00:38:36,280
OP's given said it has everything to do with the parameter you want to infer,

342
00:38:36,700 --> 00:38:41,589
while the second part has nothing to do with what you want to infer.

343
00:38:41,590 --> 00:38:49,750
Clearly this needs to have a few assumptions, which is to say that one assumption that simplest is theta and the five are.

344
00:38:50,960 --> 00:38:55,040
They do not have anything in common. They are completely separate parameters.

345
00:38:56,500 --> 00:39:02,390
So. When you are in foreign theater, the second term can be ignored.

346
00:39:03,140 --> 00:39:10,370
Okay, so to summarize, ignore ability comprised of comprises of two parts.

347
00:39:10,490 --> 00:39:17,340
The first one is missing at random. Well, I should say a sufficient condition for.

348
00:39:18,630 --> 00:39:24,720
Ignore ability is that you have missing a random. So this simplification can happen.

349
00:39:25,350 --> 00:39:33,030
The second one is that Theta and Phi are not overlapping, I think, in the original Rubin paper.

350
00:39:35,460 --> 00:39:42,900
I believe in 1970. I could be wrong, though. Let me search.

351
00:39:52,540 --> 00:39:57,710
So 1976. 1976.

352
00:40:00,960 --> 00:40:07,660
So I believe. She defined the term orthogonal.

353
00:40:09,110 --> 00:40:12,300
I think a cell phone set on fire is not overlapping.

354
00:40:12,320 --> 00:40:18,200
So when those when they are in the third and final overlapping, he calls it ignorable.

355
00:40:19,060 --> 00:40:23,960
Okay. So this is all you need to know about ignore ability.

356
00:40:24,260 --> 00:40:29,000
It will take many different forms in real data analysis, but this is the most fundamental piece.

357
00:40:29,810 --> 00:40:35,030
Just an anecdote. Do you know if we reviewed this paper when Rubin submitted this paper to biometric?

358
00:40:36,620 --> 00:40:42,910
Actually it all. So Raul, it all just graduated from his Ph.D.

359
00:40:44,140 --> 00:40:56,630
In 1975, I think. The reason why I know this is because what little told me so.

360
00:40:58,100 --> 00:41:05,300
So let's see. Okay. So he graduated in 1974, and then he moved on to do a post-doc.

361
00:41:06,830 --> 00:41:10,250
I believe with certainty. I think with.

362
00:41:11,840 --> 00:41:17,980
Uh. At the University of Chicago, actually.

363
00:41:17,990 --> 00:41:29,080
Yeah. Assistant Professor All right. So I think it is during the period when he was working with David Cox and David Cox was editor of Biometric.

364
00:41:29,800 --> 00:41:35,220
And he got this paper from Rubin and he basically said, well, he was right.

365
00:41:35,230 --> 00:41:41,230
Was doing the right. Right. You want to review this paper and then write with you this paper.

366
00:41:41,680 --> 00:41:47,890
And then rather than Rubin become probably one of the best pair of friends on this topic.

367
00:41:48,490 --> 00:41:54,580
So the interesting things do happen and and do not refuse the opportunity to review whenever I ask it.

368
00:41:55,480 --> 00:42:00,070
So anyway, this is a fun story. All right.

369
00:42:01,720 --> 00:42:06,910
So to summarize clearly, if you have missing complete random, this entire simplification can happen.

370
00:42:08,680 --> 00:42:21,340
So essentially you know and they are essentially in they are is much weaker condition compared to most MKR and you know both are called ignore voice.

371
00:42:21,370 --> 00:42:28,510
If you have four and five do not overlap. I recall that theta is a parameter of interest phi as a parameter for the missing data mechanism.

372
00:42:29,800 --> 00:42:32,620
You may ask what is theta and PHI overlap?

373
00:42:34,420 --> 00:42:42,040
So the general, the conventional wisdom is that you can still use the first term and ignore the second term,

374
00:42:42,040 --> 00:42:45,610
but you do lose certain kind of efficiency when you're estimating theta.

375
00:42:46,300 --> 00:42:47,810
I will not talk too much about that.

376
00:42:47,830 --> 00:42:55,720
It's a little bit more subtle and I think if you later on will be doing some data related research, I think you will need to understand this.

377
00:42:58,350 --> 00:43:13,460
Okay. So I don't think I would talk too much about this one.

378
00:43:13,910 --> 00:43:22,010
But I think you should know that a g may work under missing data settings.

379
00:43:22,430 --> 00:43:28,460
But the assumptions are quite strong. It requires the missing complete a random assumption.

380
00:43:29,870 --> 00:43:39,980
One question is does g work? If we have missing out random, which is to say that missing the indicators depends on some information that we observed.

381
00:43:40,220 --> 00:43:44,180
How do we do that? They will. You will you guys will need a way to g.

382
00:43:54,180 --> 00:43:57,690
I mean, I do think that we we may have one example to do that.

383
00:43:57,780 --> 00:44:15,139
So I can revisit that perhaps. Oh, I don't have an example on that.

384
00:44:15,140 --> 00:44:23,210
So yeah. So I think to me, you know, this is the simplest message that I want to communicate.

385
00:44:23,930 --> 00:44:28,580
Gee, I need some modification if you have missing random assumptions.

386
00:44:37,190 --> 00:44:43,620
All right. We talk about mmr ignore bility and g on the are.

387
00:44:44,500 --> 00:44:48,670
There is a final quite. How is it?

388
00:44:48,670 --> 00:44:54,200
Quite vexing. Part, which is not missing at random.

389
00:44:54,270 --> 00:44:58,040
So in general, some people will call it not ignorable.

390
00:44:59,250 --> 00:45:04,110
No response. Why is that called this way? Well, non-response just means missing data, right?

391
00:45:04,140 --> 00:45:13,470
It's just a very general way to say that non ignorable basically means that you cannot conduct the like of a factorization as we did before.

392
00:45:15,020 --> 00:45:23,750
So in general, it means that this particular condition, distribution of the response patterns depends on certain missing information.

393
00:45:24,140 --> 00:45:27,590
And because we never observe the missing information.

394
00:45:27,600 --> 00:45:31,770
So it's almost impossible to it's just impossible to model that.

395
00:45:31,790 --> 00:45:37,250
You have to impose certain kind of assumptions. Now you're saying is more plausible given your background knowledge.

396
00:45:42,080 --> 00:45:49,790
So for not missing random in general, you will need to specify again this for distribution.

397
00:45:51,640 --> 00:46:00,070
And because you cannot, as I said, estimate a missing data mechanism depends on value that has not been that has not even been observed.

398
00:46:00,850 --> 00:46:08,710
The results in general, in general heavily depends on the modeling assumption you put onto the missing data mechanism.

399
00:46:10,860 --> 00:46:14,140
So for example. Let me see.

400
00:46:14,250 --> 00:46:24,210
I think this is pretty simple example, actually. Example one Suppose you are trying to estimate the income as a function of various covariates.

401
00:46:24,240 --> 00:46:30,660
Right. And you will not have the income information from many people, but it could be a mix of reasons.

402
00:46:30,930 --> 00:46:33,719
First one is that my income is not in business.

403
00:46:33,720 --> 00:46:39,390
So whenever this person, always whenever this person got a questionnaire, this person would never respond.

404
00:46:40,970 --> 00:46:48,170
Regardless of income level. So the chance of person responding may not depend on the actual income that he or she did not report.

405
00:46:49,580 --> 00:46:53,750
But it could also be likely that people with higher income or low income,

406
00:46:54,410 --> 00:46:59,240
they probably do not want to reveal wealth or do not want to tell people that they poor.

407
00:46:59,270 --> 00:47:05,810
So the missing this quote, can be dependent upon on the response they never put into the questionnaire.

408
00:47:07,150 --> 00:47:10,540
So that's one example of the normal response.

409
00:47:13,460 --> 00:47:18,480
Second example, again, it's a no ignore bill of no response.

410
00:47:18,500 --> 00:47:23,840
Consider a longitudinal clinical trial with interest in modeling health related quality of life.

411
00:47:24,320 --> 00:47:30,350
So quality of life is measured every three months by self-report and a detailed multi item questionnaire.

412
00:47:31,610 --> 00:47:34,950
So items must include ability to carry out everyday activities.

413
00:47:34,970 --> 00:47:42,890
Outlook to pay and so on. There may be a lot of missing data even on subjects who do not drop out because they may not choose to self-report.

414
00:47:44,060 --> 00:47:50,900
For example, if subjects were sicker, were in more pain, do not respond then we may have non ignorable.

415
00:47:50,900 --> 00:47:56,690
No response, right? Because I am in so much pain I am struggling to even get out of the bed.

416
00:47:57,080 --> 00:48:02,000
Why do I bother to help you finish your study? It's of my secondary priority.

417
00:48:02,570 --> 00:48:10,870
So if those situations happen, you should not treat the individuals who responded the same as people who have not responded.

418
00:48:10,910 --> 00:48:14,630
Because you will only be if you ignore that possible difference.

419
00:48:14,990 --> 00:48:21,800
You will only be analyzed in the subset of data that is produced by people who are in general less sicker.

420
00:48:22,960 --> 00:48:26,080
Less sick and in less pain.

421
00:48:26,380 --> 00:48:29,470
So that can cause bias in the analysis or result.

422
00:48:33,040 --> 00:48:37,670
Uh. So. Let's.

423
00:48:38,020 --> 00:48:42,220
I think I have two more slides before we take the break. After the break, we do have one example.

424
00:48:42,790 --> 00:48:53,290
One more example. So for this one, I just want to give you a quite simple illustration of how the now ignoble non-response can happen.

425
00:48:53,830 --> 00:48:56,950
Suppose the outcome for this is not longitudinal data.

426
00:48:57,030 --> 00:49:00,670
Okay. We just have y sample from normal zero one.

427
00:49:01,240 --> 00:49:04,540
Suppose your goal is trying to infer the mean.

428
00:49:05,030 --> 00:49:11,920
Right. Okay. What's the truth? It's zero. Because we're sampling the wise from a population of standard normals.

429
00:49:12,610 --> 00:49:21,400
And however we impose the following missing data mechanism, we only so our equals one indicates that we have a response, right?

430
00:49:21,850 --> 00:49:28,479
So the probability of a person providing response is dependent upon the underlying value.

431
00:49:28,480 --> 00:49:33,430
Y Positively. So the higher the value of Y is, the more likely the person is respond.

432
00:49:34,760 --> 00:49:44,030
So if for one person we generate one Y and then we produce two equals zero, then that data did not get recorded.

433
00:49:44,450 --> 00:49:51,760
Okay. So this generally gives us a probability of missing data around 33% on average.

434
00:49:52,300 --> 00:49:56,420
So now let's look at these two plus. So the top one is complete data.

435
00:49:56,440 --> 00:50:01,170
Well, I am only simulating 1000 data points. So the ME is about zero.

436
00:50:01,200 --> 00:50:06,880
Not exactly zero. That's due to the random variation. But if I only look at the data.

437
00:50:09,030 --> 00:50:15,270
You know that has our equals one. Then the mean in that subpopulation is .84.

438
00:50:16,050 --> 00:50:21,960
So it is biased upwards and it's not surprising because people with higher value y tends to respond.

439
00:50:22,290 --> 00:50:29,340
So the lesson here is that this is simple example, that even if you're inferring a single parameter like the meaning,

440
00:50:29,670 --> 00:50:36,420
you should still consider how to produce a plausible mRNA in the mechanism.

441
00:50:37,590 --> 00:50:41,670
OC you may be wondering OC Hey, can we model this thing?

442
00:50:42,180 --> 00:50:46,480
I equals one given why. Probably not.

443
00:50:46,560 --> 00:50:53,460
Right. Or I should say not. Okay. Because for those individuals whose why did not got observed.

444
00:50:54,000 --> 00:50:58,140
True. You have the R equals zero because this person has no response.

445
00:50:58,560 --> 00:51:01,830
But you didn't. You do not have their true y information.

446
00:51:01,960 --> 00:51:08,070
Right. And of course, you have individuals who have a response r equals one and you have measure their.

447
00:51:08,910 --> 00:51:16,200
So again, this model is this de the logic thing is there because I did a simulation.

448
00:51:16,560 --> 00:51:19,710
But when you see the data, you only see the data with observed wise.

449
00:51:21,730 --> 00:51:23,780
So that's what I said here. Just to summarize.

450
00:51:24,970 --> 00:51:33,580
In general, we will not be able to distinguish between now ignorable non-response to a normal way, which was what we did and MCI with non normal one.

451
00:51:34,090 --> 00:51:38,880
What do I mean by that? Well, the data you see will only be like this.

452
00:51:38,980 --> 00:51:43,150
Right now I provides you two alternative explanations.

453
00:51:44,370 --> 00:51:51,600
One explanation is that the population has the standard normal distribution, and then a subset of them responded.

454
00:51:52,950 --> 00:51:54,930
This is what we did and this is what we saw.

455
00:51:55,860 --> 00:52:05,490
The other possibility is that, hey, the population distribution is going to be look looking like that skewed and normal and everybody can observe.

456
00:52:06,120 --> 00:52:13,060
Right. I.e. the missing sorry, the observed individuals are representative, the missing data,

457
00:52:13,330 --> 00:52:18,910
the the people who did not have a y recorded a no different from the people who have their y recorded.

458
00:52:19,420 --> 00:52:24,370
So this will be their true population distribution if we increase the sample size.

459
00:52:25,030 --> 00:52:32,890
So that is what I meant. We have two competing explanations of the blue histogram and using the data, you cannot distinguish the two.

460
00:52:34,100 --> 00:52:40,820
So this creates a lot of difficulty and clearly you will need to use your substantive knowledge.

461
00:52:41,030 --> 00:52:51,070
If Y is measuring income, then it is plausible to assume that there may be certain, you know, non ignorable non-response.

462
00:52:52,160 --> 00:53:00,920
Anyway, in general, the way people deal with this is to impose a range of models for this missing data mechanism and see how the results would change.

463
00:53:01,220 --> 00:53:05,090
In this case, it's to see how the mean would change depending on how you specify this model.

464
00:53:05,360 --> 00:53:09,879
It is called sensitivity analysis. So let's take a five minute break.

465
00:53:09,880 --> 00:53:15,880
And in the 17 minutes after that, we will just be providing one example on pandemics,

466
00:53:15,910 --> 00:53:20,470
geology, and a very, very, very brief intro to multiple imputation.

467
00:59:51,770 --> 00:59:58,580
All right, everybody. So let's get back to work. We have some additional mileage to cover.

468
00:59:59,330 --> 01:00:04,879
The first one is the iPad in Mixergy. So this thing is combination of pad and mixture model.

469
01:00:04,880 --> 01:00:08,670
And we have not talked too much about pattern mixing model. We have talked a lot about G.

470
01:00:09,560 --> 01:00:15,930
So we'll try to C give you some example a sense actually how to how to use them and the code to provide options.

471
01:00:15,950 --> 01:00:22,129
Yes. But I think that you're not required to do any homework on this, so you should be fine.

472
01:00:22,130 --> 01:00:26,870
But later on, if you try to use this technique, we probably need to search first in our packages.

473
01:00:28,730 --> 01:00:36,290
Example, it is a longitudinal study and it's trying to collect the smoking status data on 403 young adults at baseline,

474
01:00:36,290 --> 01:00:40,460
which is a year zero and a few years after year two and five and seven.

475
01:00:41,120 --> 01:00:47,660
And the question the investigators are trying to investigate is the pattern of smoking prevalence over time in this cohort,

476
01:00:48,020 --> 01:00:51,170
it is going up or down or stable.

477
01:00:51,680 --> 01:00:58,700
So the response variable is why it just takes of one. If a person is a smoker at time j equals 0257.

478
01:00:59,090 --> 01:01:06,770
Everybody has these four variables. Ideally, you know, whether a person is smoking at these time points and there are a few predictors.

479
01:01:06,950 --> 01:01:12,860
First, based on age, what is the age of the person? When this person entered the study, it is centered at 20 years old.

480
01:01:13,550 --> 01:01:18,710
So excise equals zero, meaning this person was 20 years at the baseline.

481
01:01:19,130 --> 01:01:26,660
And there are two more converts indicating the education level at baseline, the highest educational attainment.

482
01:01:27,500 --> 01:01:32,090
It is a categorical variable. The reference category is the college graduate.

483
01:01:32,720 --> 01:01:37,880
The second. So for three categories you have two dummy variables.

484
01:01:38,540 --> 01:01:42,499
The second one is the indicator of less than high school education.

485
01:01:42,500 --> 01:01:49,340
And third one is the the second indicator is the indicator for some college completed.

486
01:01:49,340 --> 01:01:54,049
So we have three predictors that are at baseline and let's take a look at data.

487
01:01:54,050 --> 01:02:03,320
So we have four occasions year 0257 and we do can calculate the fraction of people who are smokers.

488
01:02:03,890 --> 01:02:07,490
And let's look at the final column. It is number of observed people.

489
01:02:07,490 --> 01:02:12,020
So this goes from four three down to 275.

490
01:02:12,050 --> 01:02:17,660
So we do have a lot of people who dropped out of study at the end of the end of the follow up period.

491
01:02:19,070 --> 01:02:25,370
So the percentages were calculated with a denominator shown on the final column here.

492
01:02:25,460 --> 01:02:35,510
So, you know, if we have like it is sort of number divided by 275, that's 363 36.4%.

493
01:02:35,990 --> 01:02:44,980
So it's just trying to calculate some honest proportions. So from this raw calculation, it seems to me that the percentage is going up a little bit.

494
01:02:45,900 --> 01:02:49,650
But how do we assess whether this upward trend is significant or not?

495
01:02:50,700 --> 01:02:55,620
One thing that's a little bit unclear here is that, hey,

496
01:02:55,830 --> 01:03:00,870
we have not really talked about missing data patterns because the people who chose not to stay in the study,

497
01:03:01,230 --> 01:03:06,030
they who are they are they tend to be smokers or not to be smokers.

498
01:03:06,150 --> 01:03:13,500
Right. So that's a kind of question that needs to be answered when you are doing analysis of trend.

499
01:03:13,590 --> 01:03:18,299
Right. Because it's kind of a it's a lot of things being wrapped together.

500
01:03:18,300 --> 01:03:31,470
We need to unwrap them. So we were thinking about doing this model, using this approach that just.

501
01:03:32,840 --> 01:03:34,280
Considered missing completely random.

502
01:03:34,490 --> 01:03:40,670
So we do not treat people who did not continue in the study differently than the individuals who chose to be in the study.

503
01:03:41,420 --> 01:03:46,070
So we think we can do it just to fit this kind of model with a few covariates.

504
01:03:46,610 --> 01:03:50,210
Why, as you again indicate smoking or not for person at high.

505
01:03:50,420 --> 01:03:54,290
It's just the time. So it can be 0257.

506
01:03:54,680 --> 01:04:03,970
X age is baseline age. And excited too is is how I see the is a high school education.

507
01:04:04,900 --> 01:04:08,350
The final one is the is some college education.

508
01:04:09,870 --> 01:04:20,759
Right. The reference category is college graduate. So when we do this analysis, you can see that this term that involves time is positive,

509
01:04:20,760 --> 01:04:28,930
indicating that the trend for the probability of the odds of smoking goes up and the p value is significant there.

510
01:04:29,100 --> 01:04:32,290
It's based on the score test. Okay.

511
01:04:33,940 --> 01:04:37,900
So this is the first analysis. It is a very, very naive analysis.

512
01:04:38,460 --> 01:04:45,490
Okay. It's not consider any missing data issues. So a natural question is how do we can we believe this result?

513
01:04:45,550 --> 01:04:49,810
All right. So there are we need to do something more nuanced.

514
01:04:50,320 --> 01:04:55,030
So one approach is based on this pattern X model. So let's first define the patterns.

515
01:04:55,900 --> 01:05:01,840
We should use a indicator called die. So die is the final measurements.

516
01:05:03,960 --> 01:05:08,910
Sorry to see the indicates the time of the last measurements from each subject.

517
01:05:09,150 --> 01:05:16,890
Right. So if a person. It's like measure to measure, not measured measure.

518
01:05:16,990 --> 01:05:21,240
This person's day equals to. How about this?

519
01:05:22,680 --> 01:05:25,860
This person's D equals five. How about this?

520
01:05:26,280 --> 01:05:30,930
This person die also equals five. Because it's only accounting for the final one.

521
01:05:32,040 --> 01:05:37,950
It's only accounting for the final one. So by doing this definition, we are reducing number of patents.

522
01:05:37,980 --> 01:05:41,740
Clearly there can be many more than four patents, right?

523
01:05:41,810 --> 01:05:45,720
If you count all the prisms, absence of the measurement, that is some occasion.

524
01:05:46,770 --> 01:05:50,420
But here we're just trying to group these people together and do one simple analysis.

525
01:05:50,430 --> 01:05:55,080
We will show whether the results are different from the naive analysis we saw earlier.

526
01:05:56,460 --> 01:05:59,640
Clearly, if you have this kind of people, die equals seven.

527
01:05:59,970 --> 01:06:03,390
You have this kind of people. The die also equal seven.

528
01:06:04,500 --> 01:06:10,890
All right. But we're grouping them together just to focus on fewer missing data.

529
01:06:11,250 --> 01:06:12,780
Strada. Strada. Okay.

530
01:06:17,610 --> 01:06:26,370
So the only information we can tell from this data is that if this goes to anything after two year or two, is is missed, is missing.

531
01:06:27,240 --> 01:06:30,720
If Dick was five, then the anything after your five is missing.

532
01:06:33,980 --> 01:06:39,230
So let's look at this table. So, how to read this table? Time goes this way, right?

533
01:06:39,260 --> 01:06:45,590
You're zero. You're to year five. You're a seven. You know, there are three different kinds of people.

534
01:06:46,130 --> 01:06:50,530
So. Let's look at, say this one seven.

535
01:06:52,990 --> 01:06:56,890
So the people who has the said that means we have.

536
01:06:58,100 --> 01:07:08,240
These people have the final measurement. So they contribute to this percentage, which calculates that at year seven, what's the fraction of smokers?

537
01:07:09,360 --> 01:07:16,290
Well, these people we'll have baseline for sure because this is the first measurement of the study.

538
01:07:16,740 --> 01:07:20,220
But they may or may not have measurements at the year two, year five.

539
01:07:20,250 --> 01:07:30,420
But if they do, they will contribute to the fractions calculated at the year five, the fraction calculated to OC and fraction calculated year zero.

540
01:07:30,990 --> 01:07:35,430
Right. So to summarize, the way to read this paper,

541
01:07:35,790 --> 01:07:41,219
read this table is that you have four blocks indicating the four times and in each

542
01:07:41,220 --> 01:07:46,320
of the four blocks you have four kinds of people contributing to the fractions.

543
01:07:47,340 --> 01:07:50,730
If you see a question mark here, let's say, how about this one?

544
01:07:51,210 --> 01:07:54,480
How do we interpret this? It's a question mark. Why?

545
01:07:54,990 --> 01:07:58,410
Well, we are looking at people who have the equals five.

546
01:07:59,330 --> 01:08:05,420
But people have who has died because of that kind of people did not have the final measurement.

547
01:08:06,220 --> 01:08:14,020
So these people will not have a. Fraction of smokers calculated at year seven in the final roadblock.

548
01:08:14,770 --> 01:08:20,890
I'm going to pause here for a little bit because I know that can be a little bit confusing if you're looking at this for the first time.

549
01:08:41,980 --> 01:08:45,300
So let's look at the first roadblock. Okay.

550
01:08:45,310 --> 01:08:51,580
So there are four kinds of people. People who only stayed for the first two measurement.

551
01:08:52,060 --> 01:08:57,760
People who stayed with the fact were the second year to measurement obtained.

552
01:08:58,180 --> 01:09:01,239
People who stayed for the five year measurement obtained.

553
01:09:01,240 --> 01:09:07,990
And finally people were stayed with the seventh year follow up measurement obtained.

554
01:09:07,990 --> 01:09:13,530
Right. So what do you see here? Well, what I see here is that, hey,

555
01:09:13,920 --> 01:09:19,530
people would choose to stay in a study for not only the baseline measurement seems

556
01:09:19,530 --> 01:09:25,020
to be smoking more than people who only choose to stay in the study for one time.

557
01:09:26,040 --> 01:09:30,840
So these are the people who say, hey, I in the study that said, I don't want to come back ever again.

558
01:09:31,150 --> 01:09:34,950
Right. These people, they came back again.

559
01:09:35,250 --> 01:09:39,030
Right. At year two, 85, 87.

560
01:09:39,270 --> 01:09:43,080
Right. Of course, this kind of people may have come back a year to year five.

561
01:09:43,860 --> 01:09:48,030
Again, we're indicating the last measurement a person can come back.

562
01:09:48,750 --> 01:09:54,120
So the difference why difference I saw is that it seems to me that people would choose not to come back.

563
01:09:54,120 --> 01:09:57,730
Are people. At best, at least.

564
01:09:58,940 --> 01:10:07,320
Who do not tend to smoke. Okay.

565
01:10:07,810 --> 01:10:11,110
So it seems to be there is a at least based on the first roadblock,

566
01:10:11,110 --> 01:10:16,560
there's a little bit evidence that people would choose to study, may need some help and they do want to quit smoking.

567
01:10:16,570 --> 01:10:21,790
It's just you can they can not help themselves because nicotine is very addictive.

568
01:10:22,450 --> 01:10:26,950
So they need more help from the study. But for people who are relatively less prone to study.

569
01:10:27,220 --> 01:10:32,290
From the smoking, let me just say, okay, this is enough for me. I don't need too much help from the study and they just don't come back.

570
01:10:33,430 --> 01:10:38,640
Let's look at the second, second row, second row block here.

571
01:10:41,740 --> 01:10:50,060
What are we looking at? It is you're two now, right? For this kind of people who only showed up for the first measurement.

572
01:10:50,480 --> 01:10:53,660
Of course, they do not show up in a year or two.

573
01:10:54,440 --> 01:11:00,200
So there is a question mark there because they we cannot calculate the fraction of smokers for them because they're gone.

574
01:11:00,680 --> 01:11:10,420
All right. For people who are like showed up year two, they showed a baseline and they showed up again and we calculate and then they disappear.

575
01:11:10,570 --> 01:11:15,460
Okay, so these are the people who are in this role. They showed up a year or two and then they disappear.

576
01:11:15,820 --> 01:11:20,440
The fraction of people who were smoking is 51%, right?

577
01:11:20,710 --> 01:11:24,670
If you follow the same logic, if you calculate the fraction of.

578
01:11:26,000 --> 01:11:31,920
People who. Showed up in your five, but then disappeared.

579
01:11:32,190 --> 01:11:37,620
These people tend to smoke like 43.90% time of year to.

580
01:11:38,820 --> 01:11:49,040
Right. And if you do same logic, the same calculation here, that fraction is 34.40% for people who showed up for the final follow up.

581
01:11:50,080 --> 01:11:55,030
So in this case, it is suggesting something that is quite the opposite,

582
01:11:55,060 --> 01:12:05,290
which is that people who showed up at the second visit and then disappeared are people who are who smoked a lot more a second year.

583
01:12:05,860 --> 01:12:09,250
So it seems to be opposite story compared to the first roadblock.

584
01:12:13,520 --> 01:12:16,840
So this is so this is a little bit counter-intuitive.

585
01:12:16,850 --> 01:12:22,339
I cannot provide a simple biological reason or other reason to suggest that.

586
01:12:22,340 --> 01:12:31,000
Just some impure conversation. So a quick question.

587
01:12:31,180 --> 01:12:34,420
Based on this table, do we think the data are missing completely random?

588
01:12:48,420 --> 01:12:53,909
Do you think that they are missing computer? And what's the what?

589
01:12:53,910 --> 01:13:03,620
Why do you think that? What? Why do you think. Yes. Let me flip back to this table there.

590
01:13:41,070 --> 01:13:46,440
Well, the simple way to see this is basically to ask whether you have y independent of.

591
01:13:47,760 --> 01:13:56,140
VII. For example, I. And this does not seem to be the case for many of the roadblocks.

592
01:13:58,620 --> 01:14:02,280
So then let's find a somewhat more complicated model.

593
01:14:02,580 --> 01:14:07,610
It is called pandemic strategy. And, you know.

594
01:14:10,100 --> 01:14:17,270
We need to make a few assumptions. The first is that. Given the relatively small number of subjects dropping out at this stage,

595
01:14:17,270 --> 01:14:21,620
we will assume the effects of age and education do not depend on dropout patterns.

596
01:14:22,700 --> 01:14:27,680
It is because of the pattern mixing model. You do need to model this and model this.

597
01:14:29,740 --> 01:14:32,830
Right. So, of course, you have covered.

598
01:14:33,080 --> 01:14:40,510
I'm going right there. So clearly, if you have covers, there's a question of whether the X and R would interact.

599
01:14:40,510 --> 01:14:52,410
And this assumption is to say, let's not assume interaction. And number two in the set of cover, it's we do have the term cot time.

600
01:14:53,070 --> 01:15:01,800
So it is trying to investigate whether a time there is a time trend, but there is a subset of people who only had one measurement.

601
01:15:02,550 --> 01:15:07,950
These people are these people with the zero. They only showed that baseline and they never came back again.

602
01:15:08,470 --> 01:15:14,130
Right. So we can never rely on a time trend analysis based on these individual alone.

603
01:15:14,460 --> 01:15:23,790
So what we will do is to combine individuals with the zero and individuals with vehicles two together as a strata strata.

604
01:15:23,920 --> 01:15:28,280
Okay. So this is a second thing we do. As I would have said.

605
01:15:29,480 --> 01:15:34,490
Then we fed this model. This model is the pattern mixture model.

606
01:15:35,000 --> 01:15:38,060
How can you tell? Well, it is trying to model. Why?

607
01:15:38,390 --> 01:15:41,990
Given given the hours and cause.

608
01:15:42,470 --> 01:15:50,990
Right. And to do this, you basically ask, what are the regression coefficients for all these different?

609
01:15:54,080 --> 01:16:04,190
Missing the patterns. And we have simplified the number of missing a reduced number of missing data patterns to to a few, like the 0 to 5 with seven.

610
01:16:05,150 --> 01:16:12,920
And then we have the term for the time we have the time trend with the two kinds of patterns combined and we

611
01:16:12,920 --> 01:16:22,970
have the time trend for the individuals who showed up for the year five follow up and not the year seven.

612
01:16:23,240 --> 01:16:28,180
And then we have the age effect college education in these terms.

613
01:16:29,930 --> 01:16:37,270
So. This is basically the model you want you to specify for doing pattern extra G and to do this.

614
01:16:37,630 --> 01:16:40,900
Essentially, this is a SAS code to do this.

615
01:16:40,900 --> 01:16:45,310
I will not to talk too much about that. What's the result?

616
01:16:45,970 --> 01:16:54,880
If you look at the result, essentially now the time effect has a p value of 0.0553.

617
01:16:55,980 --> 01:17:04,110
So this goes from Non-Significant. So this goes from significant before using naive analysis to Non-Significant at this time.

618
01:17:04,650 --> 01:17:12,960
So this means that by doing more careful analysis, we do not see the time trend effect here.

619
01:17:18,400 --> 01:17:22,300
So this is not to say that this is the only model we can fit.

620
01:17:22,410 --> 01:17:30,219
Clearly, you can try out the model, too. But what I want to show here is that we have a sharp contrast between the declaration,

621
01:17:30,220 --> 01:17:34,030
the significance, depending on what missing data assumption you want to do.

622
01:17:34,750 --> 01:17:39,340
Remember, when we're doing this pattern mission model, we are not assuming missing, incomplete, random.

623
01:17:39,790 --> 01:17:46,120
But when we were doing the analysis does just take all the information we have for the simple model.

624
01:17:46,660 --> 01:17:50,920
It is a missing complete at random data Nazis. So these two results are different.

625
01:17:51,310 --> 01:18:00,160
And for a careful data analyst, you need to take into account whether the assumption of missing could be random is plausible.

626
01:18:00,460 --> 01:18:03,580
And we claim that that assumption does not seem to hold.

627
01:18:05,020 --> 01:18:10,900
It is 4:03 20. I just want to say that all things that's left is multiple imputation.

628
01:18:11,230 --> 01:18:16,750
And I do want to say it's very brief and I will not have time to cover it.

629
01:18:18,010 --> 01:18:21,940
So I will leave them to on reading summarize.

630
01:18:22,300 --> 01:18:26,110
So missing data somewhere missing data is a rule rather than exception.

631
01:18:26,110 --> 01:18:33,940
And there are three classifications missing completely random, missing at random and not missing at random.

632
01:18:35,080 --> 01:18:39,970
And most importantly, not A.C.O.R.N. or more generally not ignorable.

633
01:18:40,330 --> 01:18:42,819
Response are typically fundamental,

634
01:18:42,820 --> 01:18:52,660
non identifiable and inferences made is possible after making certain unverifiable assumptions about non-response process.

635
01:18:53,140 --> 01:18:59,710
And in addition, it can be quite difficult to determine whether missing data are missing random or not ignorable.

636
01:19:00,490 --> 01:19:08,920
So, you know, there are short courses or talks in Circumferences that's about sensitivity analysis.

637
01:19:09,250 --> 01:19:15,310
It is all trying to produce certain kind of principles where when you encounter these missing data,

638
01:19:15,640 --> 01:19:22,120
how do you assess whether the results you obtained is sensitive or not sensitive to the assumptions you have made?

639
01:19:22,600 --> 01:19:24,790
So those will be more advanced topics,

640
01:19:25,000 --> 01:19:34,450
and hopefully this lecture provides you some basic definitions and examples to be able to understand or possibly be interested in those topics.

641
01:19:35,350 --> 01:19:39,880
All right. I'm going to stop here. That's all I have today. And thanks, everybody, for your attention.

642
01:19:40,630 --> 01:19:44,500
We'll see you guys on Wednesday for two guests for our guest lectures.

643
01:19:46,830 --> 01:19:47,100
All right.

