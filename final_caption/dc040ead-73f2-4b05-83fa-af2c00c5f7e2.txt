1
00:00:02,490 --> 00:00:17,600
Okay. So today my goal is to get you a little bit more ready for the online canvas quiz that's happening next Wednesday.

2
00:00:17,610 --> 00:00:21,200
And so right now it's closed.

3
00:00:21,680 --> 00:00:28,580
I am still writing the exam. So right now, no questions are in my head necessarily.

4
00:00:29,610 --> 00:00:37,710
All I've done is prepare this first sheet of information, which you would see upon opening the quiz.

5
00:00:38,160 --> 00:00:43,590
And so there's just a couple of things that I wanted to make sure you know.

6
00:00:44,130 --> 00:00:48,240
So the quiz is going to open at 6 a.m. and it's going to close at 11 a.m.

7
00:00:48,240 --> 00:00:54,030
So it's a five hour window. And you can use the entire 5 hours if you wish.

8
00:00:54,330 --> 00:00:59,730
That is up to you. It's written to be a two hour exam. I expect it to take 2 hours or less.

9
00:01:00,510 --> 00:01:05,370
But I'm paranoid about technical difficulties that can happen when you're doing stuff online.

10
00:01:05,820 --> 00:01:11,520
And so I have a big window so that if anything happens, you have time to even restart the quiz over again if you have to.

11
00:01:12,150 --> 00:01:18,250
All right. So that extra time here to allow for technical difficulties that might arise during the exam.

12
00:01:19,350 --> 00:01:28,800
But I haven't put a time restriction on how long you work the exam, so I don't intend this to be a time pressure situation for anybody.

13
00:01:29,550 --> 00:01:34,960
You won't need this entire time in case you have questions during the exam.

14
00:01:34,980 --> 00:01:38,970
I'm going to be available via cell phone. This is my cell phone number.

15
00:01:40,290 --> 00:01:46,309
Right. And so you would please text me a phone number where I may reach you, and I will get back to you as soon as possible.

16
00:01:46,310 --> 00:01:50,150
So you text me. I will be monitoring my texts.

17
00:01:50,150 --> 00:01:59,200
I won't be checking email because I'm going to be looking at texts that don't email text, and then I will call you back as soon as I can.

18
00:01:59,210 --> 00:02:06,920
Depending on how many people are queued up with questions. In past terms, I have had no problem reaching out right away.

19
00:02:07,010 --> 00:02:12,739
It's not like there's a huge queue of people with questions, but if I don't get back to you right away,

20
00:02:12,740 --> 00:02:20,740
it's because I'm just finishing up with someone else's question. And the the format for the quiz is going to be.

21
00:02:21,490 --> 00:02:24,880
Well, actually, they have a new quiz structure.

22
00:02:26,000 --> 00:02:35,240
That I have not played with before. So last year at this time, canvas quiz structure you've probably played with before.

23
00:02:35,480 --> 00:02:40,700
When I started writing the quiz this time, they had other options for types of questions that you could do.

24
00:02:41,330 --> 00:02:49,430
And so it might. The way the questions look on your screen might be a little different from what you've seen.

25
00:02:49,580 --> 00:02:52,100
If you haven't taken a canvas quiz this year, I don't know.

26
00:02:52,850 --> 00:02:57,740
But I'm thinking of playing with that format because it gives you other options for types of questions you can ask.

27
00:03:00,310 --> 00:03:09,280
And so I may play with that. But the overall structure, if I can't make it similar to the practice squares, I I'll go back to that.

28
00:03:09,400 --> 00:03:12,940
You have a choice of using the student quiz format as well, not the new one.

29
00:03:13,690 --> 00:03:19,389
And so my intent is to make it look as similar as possible to the practice quiz so that you'd have several files,

30
00:03:19,390 --> 00:03:23,470
bar code and output that you can download onto your computer.

31
00:03:24,870 --> 00:03:28,540
So you will not be programing yourself on the quiz?

32
00:03:28,560 --> 00:03:33,480
I don't intend for programing in a quiz format to be a thing.

33
00:03:34,080 --> 00:03:40,970
I will give code and output. I might not give you all the correct code and output.

34
00:03:40,980 --> 00:03:50,130
I mean, you have to know enough about how to interpret code and output to see if I'm doing the right analysis or to find the right output.

35
00:03:51,060 --> 00:03:57,870
All right. But everything that you need will be provided to you to answer the questions.

36
00:03:57,870 --> 00:04:03,270
You won't have to really study programing other than how to read it and interpret it.

37
00:04:03,750 --> 00:04:13,079
So you'll have files with code now put in fast and files with code and output for ah, for each question you can look at both types of output.

38
00:04:13,080 --> 00:04:16,830
You don't have to pick. They're all going to be available to every person.

39
00:04:18,550 --> 00:04:21,340
All right. So that's also the way it was on the practice quiz.

40
00:04:23,450 --> 00:04:29,510
So you can download those and kind of have them on the side of your screen as you're reading the questions.

41
00:04:29,660 --> 00:04:37,830
So that's what I recommend doing. And then in terms of preparing for the exam, remember that you're allowed to use one page of notes.

42
00:04:37,830 --> 00:04:46,350
Front and back is kind of your cheat sheet or if you like, just one page, the usual typical eight and a half by 11 inch size.

43
00:04:47,130 --> 00:04:51,090
No, like microscope, please.

44
00:04:51,420 --> 00:04:59,490
You know, you can write small, but let's not get carried away with like adding extra devices to your item to be able to read it.

45
00:05:00,810 --> 00:05:05,580
You can also have blank pieces of paper, however many you feel you need for taking notes during the exam.

46
00:05:05,670 --> 00:05:16,620
And I never anticipate the exam breaking down, but maybe once or twice a term a student,

47
00:05:16,860 --> 00:05:22,080
you know, one or two students will have something weird happen and they need to start again.

48
00:05:22,500 --> 00:05:26,940
So if you want to kind of keep track of your answers during the exam, it's not a bad idea.

49
00:05:28,400 --> 00:05:33,220
Just in case you have to redo. You'll need a calculator.

50
00:05:35,110 --> 00:05:39,130
So have a calculator with you.

51
00:05:39,850 --> 00:05:46,570
All right. So this is going to be the same page of instructions that you'll see when you open up the quiz and.

52
00:05:48,740 --> 00:05:54,810
All right. Okay.

53
00:05:54,820 --> 00:05:59,360
And let's see. I think that's.

54
00:06:01,150 --> 00:06:04,780
All the administrative stuff that I meant to go over.

55
00:06:04,900 --> 00:06:08,920
We don't have class on Monday because it's the study day.

56
00:06:09,400 --> 00:06:14,950
But I will still have office hours Monday and Tuesday. I won't have office hours this Friday because I'll be traveling.

57
00:06:15,880 --> 00:06:24,110
But I will have office hours Monday and Tuesday and there's no lap next week.

58
00:06:24,160 --> 00:06:29,530
It doesn't make sense to have labs since my name Tuesday or, you know, no classes.

59
00:06:29,530 --> 00:06:34,150
And then we have the exam. So lab over zoom again the following week.

60
00:06:36,310 --> 00:06:42,250
Okay. Are there any questions about the kind of administrative. Part of this.

61
00:06:46,740 --> 00:06:52,240
Okay. Hearing none. I'm going to go ahead. And open up the review handout.

62
00:06:52,660 --> 00:06:59,530
And again, I can speak easily for the whole class period going over the review handout,

63
00:06:59,530 --> 00:07:08,019
but this is meant to be interactive and this is your chance to ask questions, kind of solidify any ideas.

64
00:07:08,020 --> 00:07:15,250
Hopefully you've started studying at least a little bit and so your questions are being formulated and I know the little smirks I'm seeing mean that,

65
00:07:15,520 --> 00:07:19,510
you know, don't feel bad if you haven't started studying. I know you're still working on the homework.

66
00:07:21,060 --> 00:07:29,160
So the the topics to study are lecture handouts one through eight labs, one through four homeworks, one through three homework.

67
00:07:29,520 --> 00:07:34,860
Three is due on Sunday. I've kind of made them do on Sunday pretty commonly.

68
00:07:35,310 --> 00:07:41,010
No late homeworks will be accepted after the deadline because we need to post the solutions.

69
00:07:41,700 --> 00:07:48,509
So this is one of the few times during the course where I can't really give homework extensions

70
00:07:48,510 --> 00:07:54,840
because the the solutions are needed to study with for the for the rest of the class.

71
00:07:55,200 --> 00:07:57,330
But I do drop the lowest homework.

72
00:07:57,990 --> 00:08:06,180
So if I if you find that you aren't doing your best on this homework because of time constraints, this might be that homework that gets dropped.

73
00:08:06,930 --> 00:08:15,470
So. So those those solutions will be posted probably early Monday morning by the GSA.

74
00:08:16,370 --> 00:08:20,180
Okay. If not Sunday night. I mean, I don't think they'll be up that late.

75
00:08:22,460 --> 00:08:28,160
And as I was kind of looking over the slides this morning, it occurred to me that.

76
00:08:29,590 --> 00:08:35,709
I should. Probably start with later slides.

77
00:08:35,710 --> 00:08:40,420
The, uh, the slides that have to do with the models that are really, really new.

78
00:08:40,420 --> 00:08:46,240
This is stuff you've never seen before. And so I want to make sure I cover that, but there's a bit of an overview of,

79
00:08:46,630 --> 00:08:50,800
of just what we've covered, and I want you to appreciate how much you've learned.

80
00:08:51,670 --> 00:08:55,240
So I'm going to go over these next to next few slides just to kind of.

81
00:08:56,640 --> 00:09:01,770
Remind you how much you've learned. And then we're going to I'm going to jump ahead and then come back to the early slides.

82
00:09:02,640 --> 00:09:12,209
So you have a learn to work with binary data and what outcomes can be independent or outcomes can be correlated.

83
00:09:12,210 --> 00:09:16,260
We've learned how to deal with matched binary data in a number of ways,

84
00:09:16,920 --> 00:09:23,100
so we looked at matched case control studies, conditional odds ratios, McNamara's test,

85
00:09:23,610 --> 00:09:32,280
and if you have more than a two by two table or more than one matched control to one or more matched cases,

86
00:09:32,670 --> 00:09:37,650
then you've learned how to use Cochrane to handle estimates and tests for those match sets.

87
00:09:38,730 --> 00:09:43,110
You've also learned to look at the campus statistic to assess agreement.

88
00:09:43,740 --> 00:09:47,640
You've learned logistic regression where the outcomes are independent.

89
00:09:48,300 --> 00:09:53,970
So there's no dependent. Yes, no outcomes. Just everybody has their own independent outcome.

90
00:09:55,170 --> 00:10:01,500
And you've we kind of reviewed general model usage from whatever your prerequisite was,

91
00:10:02,460 --> 00:10:07,140
including odds ratio calculation from the parameters adjusting for confounding and interaction,

92
00:10:07,680 --> 00:10:14,219
choosing forms of the continuous predictor and special cases that reduce to other well known estimates and test.

93
00:10:14,220 --> 00:10:18,060
And I'm going to kind of review these in separate slides.

94
00:10:19,850 --> 00:10:27,200
And also says our code and output interpretation, including how to get confidence intervals or formulas involving more than one parameter.

95
00:10:31,780 --> 00:10:37,719
And special topics that were kind of covered in 523 that you may not have seen before and

96
00:10:37,720 --> 00:10:43,940
might require you to spend more time on are linear spline terms and regression models.

97
00:10:43,960 --> 00:10:56,410
So allowing not just a continuous variable in the model, but also allowing the association to change at a certain cut point in the predictor.

98
00:10:57,640 --> 00:11:04,480
And so you've played actually a lot with that for homework three where you have these spline terms that have come in,

99
00:11:04,810 --> 00:11:11,200
but supply in terms can be used in any modeling setting and we'll be leaning on them heavily later in the course.

100
00:11:11,200 --> 00:11:15,880
So this is going to be a place I again, I haven't written the questions yet,

101
00:11:16,360 --> 00:11:22,000
but I can see myself putting spline terms and having to model contrast or,

102
00:11:22,090 --> 00:11:31,450
you know, recognize where I've put appropriate contrasts and output that you will need to find and interpret correctly based on supplying terms.

103
00:11:31,450 --> 00:11:37,150
We need to own this skill. You've been introduced and you've practiced with it a little bit already,

104
00:11:37,510 --> 00:11:42,730
but it's going to be a big player later in the course, so I can just see myself hitting this heavy.

105
00:11:43,630 --> 00:11:46,120
Okay, so study this a lot.

106
00:11:47,720 --> 00:11:55,910
We also talked about diagnostic testing and RLC curve analysis, and you may have had a little bit of this in a prior course,

107
00:11:55,910 --> 00:12:01,190
but I don't think any of the courses would have covered RC curve analysis to the extent that we have.

108
00:12:01,760 --> 00:12:07,969
So again, this is something that might appeal to me as I'm writing because I know it's a

109
00:12:07,970 --> 00:12:13,280
new spin on the material you've had before related to binary outcomes analysis.

110
00:12:14,570 --> 00:12:22,580
We also learned how to perform meta analysis and how to do diagnostics with a funnel plot.

111
00:12:22,670 --> 00:12:28,280
This is also something that is a little bit of a new spin on the on the binary outcome kind of material.

112
00:12:28,760 --> 00:12:32,360
And I'm a little bit in love with plots and funnel plots.

113
00:12:32,420 --> 00:12:40,249
So I haven't written the exam yet, but I love funnel plots and detecting bias, and I think they're underutilized.

114
00:12:40,250 --> 00:12:43,610
So I could just see myself going, Here's a funnel plot.

115
00:12:44,090 --> 00:12:50,930
You better interpret this correctly now. So I just know myself and how much in love I am with funnel plots.

116
00:12:52,490 --> 00:13:00,260
Propensity score matching is another application that you were introduced in this class that you may not have seen before.

117
00:13:00,830 --> 00:13:08,800
And this is super useful. We didn't actually have a homework problem that that used propensity scores.

118
00:13:08,950 --> 00:13:17,860
So when I'm you know, when I'm writing questions, I might very well ask, you know, what is a propensity score?

119
00:13:17,870 --> 00:13:27,740
How do you set it up? But I probably won't have codeine output, you know, because you didn't really play with that on your own.

120
00:13:28,190 --> 00:13:37,759
But I may very well ask questions related to the intuition of what they are, how you set up a model to calculate a propensity score.

121
00:13:37,760 --> 00:13:42,650
And, you know, how do you match people using propensity score or something like that?

122
00:13:45,470 --> 00:13:51,620
We also learned. So we actually learned a lot of stuff even within the logistic regression context.

123
00:13:51,620 --> 00:13:57,349
I think that in your home departments they sometimes wonder, I'm just wasting your time teaching stuff you already know.

124
00:13:57,350 --> 00:14:00,680
But we actually have a lot of little features in here, what we were reviewing.

125
00:14:01,280 --> 00:14:05,720
So we learn how to do multiple imputation for missing covariates in lecture.

126
00:14:06,500 --> 00:14:08,660
We have not had a homework on this.

127
00:14:09,050 --> 00:14:19,340
So again, I'm not likely to give you code and output, but I may very well ask you intuition about multiple computation.

128
00:14:19,340 --> 00:14:22,370
And you know, when is it helping you avoid bias?

129
00:14:22,370 --> 00:14:25,310
How does it work? You know, that kind of stuff.

130
00:14:26,180 --> 00:14:30,319
But I probably won't get to the code now point part because you just didn't have time to play with that.

131
00:14:30,320 --> 00:14:35,990
But the like. But just the description of what it is, why it's used, what kinds of bias can you avoid?

132
00:14:36,320 --> 00:14:40,370
Stuff like that that's known on the non-technical part of it.

133
00:14:41,690 --> 00:14:46,790
Fair Game or and I do love this topic. I mean, I love all these topics.

134
00:14:46,790 --> 00:14:56,300
That's why I cover them. So but the, the parts that are new and that involve skills, I need you to know I might hit hard.

135
00:14:57,840 --> 00:15:04,170
There's too many to cover them all. So, you know, I can't ask questions about all of these, but these will all be in my mind as I'm writing.

136
00:15:07,460 --> 00:15:07,810
All right.

137
00:15:07,880 --> 00:15:16,610
And then entirely new stuff is porcine negative binomial and corresponding zero weighted regression models for count data, which will review shortly.

138
00:15:17,450 --> 00:15:25,999
So you're going to need to know when when you have code that is modeling a mean when you have code,

139
00:15:26,000 --> 00:15:33,230
that's modeling a rate why you need to do one or the other for different scenarios.

140
00:15:34,400 --> 00:15:41,870
All of all of that is a big take away from the course how to learn and apply and interpret these models correctly.

141
00:15:43,840 --> 00:15:47,070
Offset terms are kind of a new concept you haven't seen before.

142
00:15:47,080 --> 00:15:50,230
Offset terms. I'll go over them later in.

143
00:15:51,630 --> 00:15:54,710
The hand out. Actually, do I have offset?

144
00:15:54,960 --> 00:15:59,030
Yeah, I think I do, yes. And so you need to know when to use them.

145
00:15:59,040 --> 00:16:01,890
You need to know based on how they're coded.

146
00:16:02,460 --> 00:16:12,270
If you're interpreting rates per person, year per 100,000 person years, you know, what is the rate that the code is?

147
00:16:13,440 --> 00:16:26,580
Talking to you about. So there there's algebra and setting up these offset terms that control what the rate is in terms of the per follow up time.

148
00:16:27,300 --> 00:16:35,170
Okay. So you need to know how that works. When to use them changing units and interpretation of the results.

149
00:16:35,860 --> 00:16:41,469
So we've you're practicing with model building right now where these models that's definitely going to

150
00:16:41,470 --> 00:16:48,500
be a feature that this is very much a setting where because you've had so much practice and homework.

151
00:16:48,880 --> 00:16:56,830
I foresee giving you code and output and you kind of go in through it and figure out what's going on with the questions based on that code and output.

152
00:16:58,440 --> 00:17:10,350
So adjusting for confounding, you'll need to know how to set an interaction, terms and contrast or estimate statements that help you estimate,

153
00:17:11,760 --> 00:17:16,980
you know, formulas that are needed for describing a full change or meeting of interest.

154
00:17:18,830 --> 00:17:26,299
Choosing forms of the continuous predictor. So here's another setting where linear spline terms may be in the code,

155
00:17:26,300 --> 00:17:32,750
and output is something you need to evaluate or quadratic terms and how these might show up in formulas.

156
00:17:32,750 --> 00:17:39,230
Again, for us, any means or rates or for changes or something, you need to kind of know how to how to set those up.

157
00:17:41,210 --> 00:17:47,360
And you need to know about this topic of modeling excess zeros, you know.

158
00:17:48,590 --> 00:17:53,810
Why is this an issue and how do you choose whether to do it or not?

159
00:17:55,600 --> 00:17:59,830
And how it's done code and output wise, how you look at that.

160
00:18:01,330 --> 00:18:10,870
Understanding when models are nested in this case and and, you know, whether liquid ratio tests are possible to use versus not, you know.

161
00:18:11,830 --> 00:18:20,380
Interpreting the models, so estimating multiplicative terms from parameters in describing how these relate to the interpretation of data,

162
00:18:20,860 --> 00:18:24,249
I you'll see in the practice exam I often have, you know,

163
00:18:24,250 --> 00:18:30,010
which is the most appropriate manuscript worthy sentence and I'll have choices for you to pick through.

164
00:18:30,010 --> 00:18:36,570
And you need to, you know, know what the code and output is doing to pick the most manuscript work.

165
00:18:36,700 --> 00:18:45,640
Most appropriate manuscript where this sentence. Estimating mean cancer rates from the model.

166
00:18:46,540 --> 00:18:52,719
Again, these would be based on estimate or contra statements that I would provide to you.

167
00:18:52,720 --> 00:19:01,900
But I you know, I tend to. Like blast many, many contrast and estimate statements and output.

168
00:19:01,900 --> 00:19:07,150
And so there's lots there for you to choose from and you have to know the code and output that's relevant.

169
00:19:11,030 --> 00:19:16,880
Since our code and output interpretation, including how to hit confidence intervals for formulas involving more than one parameter.

170
00:19:18,230 --> 00:19:21,260
Okay. So that's kind of you've learned a lot, actually.

171
00:19:21,260 --> 00:19:24,260
That's just the overview of how much you've learned so far.

172
00:19:24,740 --> 00:19:31,130
And so what I want to do now, just to make sure we have time to go over it, is I want to kind of skip ahead.

173
00:19:31,940 --> 00:19:39,620
I think it's a round slide. Gosh, we do a lot here.

174
00:19:41,270 --> 00:19:48,139
Yes, I'm 60 because I want to make sure I go over the stuff that's the most new to you as material,

175
00:19:48,140 --> 00:19:52,700
this count outcome data and this is related to your homework as well.

176
00:19:53,450 --> 00:19:58,760
So you might also have questions ready to go based on what you've been learning so far.

177
00:20:03,900 --> 00:20:14,700
All right, so count outcome data and you might have counts with or without different person years contributing to the counts.

178
00:20:15,840 --> 00:20:21,629
All right. So we've had examples of both. We have the doctor in the doctors data set.

179
00:20:21,630 --> 00:20:31,470
When we were first learning about count data, we looked at number of deaths per person year and then number of deaths per 100,000 person year.

180
00:20:31,980 --> 00:20:35,310
And so we had to deal with an offset.

181
00:20:35,970 --> 00:20:43,320
And in the person this is the thing I meant to say, the fish data, you know, in the fish,

182
00:20:43,770 --> 00:20:49,570
the fish catching fish dataset, the handout that goes over negative binomial and zero inflated models.

183
00:20:50,100 --> 00:20:58,379
You didn't have different amounts of person time that were measured in the dataset, so you didn't have an offset.

184
00:20:58,380 --> 00:21:00,990
You were just assuming the mean number of fish.

185
00:21:00,990 --> 00:21:08,819
So you've seen both and in your homework you kind of have another example where the offset wasn't leaned upon.

186
00:21:08,820 --> 00:21:15,100
Right. Because you were you're modeling in the current homework, the number of exacerbations that occurred in the previous year.

187
00:21:15,120 --> 00:21:18,240
So you have at the same amount of follow up for every person in your dataset.

188
00:21:18,780 --> 00:21:23,250
So you've seen different versions of all of all of this.

189
00:21:26,150 --> 00:21:30,380
And so this is usually how you write these models.

190
00:21:30,620 --> 00:21:36,860
So the typical outcomes are the number of deaths per person year or you've seen number of exacerbations during a year.

191
00:21:37,430 --> 00:21:44,690
And the way I think about these models and then I've been encouraging you to think about these models,

192
00:21:45,200 --> 00:21:53,569
is that the log of the count is something you are thinking of with these count outcomes, right?

193
00:21:53,570 --> 00:22:02,170
So that's the movie and. Depending on if you have different follow up for each person in the data set.

194
00:22:02,170 --> 00:22:05,469
You may or may, you may or may not need this offset term.

195
00:22:05,470 --> 00:22:13,030
So log in. I is the natural log of the number of person years in group I or individual I.

196
00:22:13,030 --> 00:22:16,240
If it's individual level data in the in the data set,

197
00:22:17,890 --> 00:22:23,920
so it's not needed in the model if follow ups equivalent for everyone and this is what the offset term is.

198
00:22:24,550 --> 00:22:27,640
So and then I'll get to this part in a minute here.

199
00:22:27,640 --> 00:22:36,400
So the reason it's so important to have this term, if you have different numbers of follow up years for each individual in your data set,

200
00:22:36,820 --> 00:22:41,050
is that it's not fair to compare what affects the mean.

201
00:22:42,390 --> 00:22:46,410
If some people were watched for longer than others.

202
00:22:46,770 --> 00:22:50,219
So, for instance, your homework, everybody.

203
00:22:50,220 --> 00:22:54,000
The counts were measured over the previous year. So it was the same for everybody.

204
00:22:54,570 --> 00:23:00,120
But I have had many data sets where you're watching for exacerbations prospectively.

205
00:23:00,690 --> 00:23:08,730
And if you don't take into account that some people are occurring exacerbations over two years and some are recurring exacerbations over five years,

206
00:23:09,060 --> 00:23:13,770
you're going to get all the wrong associations of what causes more exacerbations.

207
00:23:14,710 --> 00:23:26,290
You really have to set it up to be a fair playing field so that if there are factors that influence having more, you know, more exacerbations,

208
00:23:26,290 --> 00:23:32,679
you're you're making it a fair comparison that you're only comparing exacerbations

209
00:23:32,680 --> 00:23:39,730
per some common unit of follow up time rather than the mean number of exacerbations.

210
00:23:41,640 --> 00:23:48,360
So if you if everybody has the same follow up time to collect the counts,

211
00:23:48,360 --> 00:23:55,560
then this part is not used to offset isn't in the model and you'll be able to tell from my code whether I had one or not.

212
00:23:56,340 --> 00:24:01,560
Right. Because there's some kind of there's there's options for the offset in both packages.

213
00:24:02,310 --> 00:24:05,340
If the offset isn't there, you're modeling the mean count.

214
00:24:09,190 --> 00:24:16,190
If the offset is there, you're modeling the rate per some person, years of follow up.

215
00:24:16,210 --> 00:24:19,660
It kind of depends on how you created this variable.

216
00:24:21,320 --> 00:24:26,530
And so in both cases, all you're seeing in the output are the estimates of this stuff.

217
00:24:26,540 --> 00:24:31,610
The data is for the model. And so the output is going to look the same.

218
00:24:33,240 --> 00:24:37,700
You're going to have to know from the code whether the offset was used or not.

219
00:24:37,710 --> 00:24:42,120
The debate is are going to just be put in the output and there's nothing to distinguish.

220
00:24:42,120 --> 00:24:47,030
Those paid as clearly is coming from one or the other model.

221
00:24:47,040 --> 00:24:50,940
You have to know from the code that whether that offset term was there or not.

222
00:24:52,080 --> 00:25:01,580
And so if the offset term is there, it models the log of the rate of the events per some unit of person.

223
00:25:01,830 --> 00:25:08,280
A follow up. And if the offset is not there, it's modeling the log of the count.

224
00:25:10,890 --> 00:25:16,260
In either in-group of its categorical variables or for person either continuous variables in the model.

225
00:25:20,760 --> 00:25:28,440
So here's this is a busy slide. It's a bit of a flow chart to help you think about how the modeling selection goes.

226
00:25:28,500 --> 00:25:37,110
So the first question is, you know, when you when you're considering your model,

227
00:25:37,350 --> 00:25:45,000
if you're starting from a Poisson kind of model, that was the first handout where we looked at counts is the the variability.

228
00:25:46,000 --> 00:25:52,470
That the person assumes. Is it appropriate or is there more variability than what the Poisson distribution would have?

229
00:25:52,480 --> 00:26:01,780
And so the null hypothesis when you're trying to figure out if there's over dispersion or not is, you know, whether the variance is equal to the mean.

230
00:26:02,050 --> 00:26:09,120
So this is what the Poisson distribution assumes, or the variance is either bigger or smaller than the mean.

231
00:26:09,130 --> 00:26:10,810
So there's k parameter.

232
00:26:10,810 --> 00:26:18,310
This negative binomial dispersion parameter allows the variability to be either smaller or larger than what the person distribution would assume.

233
00:26:20,250 --> 00:26:27,030
And so when you're trying to decide between the person model and the negative binomial model.

234
00:26:28,300 --> 00:26:36,280
Then you kind of do this hypothesis test. This is the one situation where you can use a likelihood ratio test to pick which model

235
00:26:36,280 --> 00:26:42,070
style you're going to use there nested with this K parameter being the one parameter,

236
00:26:43,420 --> 00:26:46,780
you know, one degree of freedom element your testing.

237
00:26:47,170 --> 00:26:53,889
And so if you compare the two models, all the same predictors except for one is Poisson,

238
00:26:53,890 --> 00:27:01,180
one is negative binomial and likely the ratio fails to reject the null hypothesis than the Poisson is appropriate, you know,

239
00:27:01,510 --> 00:27:09,729
and it's likely to raise your test rejects and I parties to the negative binomial is appropriate and so that's kind of

240
00:27:09,730 --> 00:27:15,700
the way the decision process works if you're only thinking about the variability of the person and if it works or not.

241
00:27:16,510 --> 00:27:26,950
So from there, you know, you sort of have picked your your path of whether you're going to go forward with Poisson or negative binomial.

242
00:27:27,190 --> 00:27:34,810
And so the next question in the decision rule is kind of, you know, do you have more zeros than that distribution would allow?

243
00:27:35,810 --> 00:27:40,100
And so the only. Did I miss?

244
00:27:42,070 --> 00:27:50,300
I think I. I think it's you. Oh, right. I think I've had this slide for years that I've never noticed that before, so I misspelled 1/10.

245
00:27:50,320 --> 00:27:55,570
But it is the I'm going to double check that after class. Maybe I'm just having a dyslexia moment here.

246
00:27:56,380 --> 00:28:08,170
So you use this one test to to see, you know, whether there's a reason to think you need to have a zero inflated model so you fit the Poisson model.

247
00:28:08,170 --> 00:28:13,030
If you're in this part of the flowchart, you and then you fit the zero inflated person model,

248
00:28:14,380 --> 00:28:17,560
you know, so you've added in the zero model to the zero inflated person model.

249
00:28:18,340 --> 00:28:21,940
And the one test is supposed to tell you which model is closer.

250
00:28:22,970 --> 00:28:27,470
You know, to fitting the data well. And it's it's.

251
00:28:29,410 --> 00:28:33,030
It's not a likely two ratio test. It's some other test you can't do.

252
00:28:33,050 --> 00:28:37,720
Likelihood ratio test for comparing a person to a zero inflated puts on.

253
00:28:38,650 --> 00:28:45,309
And. If, you know, people have been using P values for this long test for years and saying, you know,

254
00:28:45,310 --> 00:28:52,360
if you fail to reject the null hypothesis, go ahead with the person or if you reject the null hypothesis, use zero inflated Poisson.

255
00:28:52,360 --> 00:28:56,319
But just, you know, as I was preparing for this course during the summer,

256
00:28:56,320 --> 00:28:59,950
I came across an article that says the P values for the wrong test aren't great.

257
00:29:01,280 --> 00:29:05,030
So I think it's more of a rule of thumb than a firm thing.

258
00:29:05,420 --> 00:29:14,450
And so you you know, they'll usually tell you which model fits better and you can make your choice based on which model fits better.

259
00:29:14,450 --> 00:29:17,770
But if you want to lean on the people, I use that fine.

260
00:29:17,810 --> 00:29:21,710
That's fine. You'll always be in the direction of which model fits the data better.

261
00:29:22,220 --> 00:29:30,650
But even with a non-significant p value result, I think there's a little bit of ambiguity if the p values should be the decisive role.

262
00:29:32,590 --> 00:29:36,730
And I think most people you bump into on the street still think PE values are okay.

263
00:29:38,760 --> 00:29:47,400
So it's kind of the same decision process if you find that you need to start from the negative binomial model because of over dispersion.

264
00:29:47,410 --> 00:29:55,799
So if you end up on this path. So remember you got here by doing a liquid ratio test and if you reject the null hypothesis,

265
00:29:55,800 --> 00:30:04,380
when you're comparing the person to the negative binomial, then you need to allow for this extra variability by moving to this path.

266
00:30:04,800 --> 00:30:14,850
And so then you again ask if there are zeros in your data set and use this one test and it's kind of the same process from there.

267
00:30:14,860 --> 00:30:20,460
So you you'll get a p value and a recommendation of which fits the model better.

268
00:30:21,430 --> 00:30:27,159
And if, you know, if you fail to reject the hypothesis, then probably the negative binomial is fine.

269
00:30:27,160 --> 00:30:31,540
But if you reject the null hypothesis, the zero play, the negative binomial is fine.

270
00:30:32,570 --> 00:30:39,710
It's the same situation, though. This one article kind of convinced me the values aren't great, so it's not.

271
00:30:40,370 --> 00:30:43,519
You know, if you decide to deviate from the P value advice,

272
00:30:43,520 --> 00:30:48,950
I think it's probably it's okay for you about the reasons for for using one model or the other.

273
00:30:53,250 --> 00:31:04,650
So for this handout, I've kind of gone to, again, the doctors dataset just because it gives you more practice with this offset.

274
00:31:05,280 --> 00:31:10,469
And so the outcome variable is the number of lung cancer deaths captured in

275
00:31:10,470 --> 00:31:21,000
deaths and the the offset l years underscore 100,000 was calculated with this.

276
00:31:21,000 --> 00:31:27,300
So you would see this in the code, either an R or SAS, you would see where this variables created.

277
00:31:27,720 --> 00:31:31,800
And depending on what you divide by here or if you divide by anything at all,

278
00:31:32,190 --> 00:31:38,909
that is the rate per person follow up units that you're looking for in your interpretation.

279
00:31:38,910 --> 00:31:46,530
So this particular algebra dividing by 100,000 is letting us interpret death

280
00:31:46,530 --> 00:31:50,879
rate per 100,000 person years instead of the death rate per single person.

281
00:31:50,880 --> 00:31:53,310
Your follow up if you didn't divide by this number.

282
00:31:56,970 --> 00:32:07,110
And you know, often we do this rescaling because the death rate per one single year of follow up for a person is usually so tiny.

283
00:32:07,530 --> 00:32:11,640
It just doesn't. It's not something that people would read easily.

284
00:32:12,360 --> 00:32:17,280
So this kind of puts it on a scale where is easy for people to understand what's happening.

285
00:32:19,030 --> 00:32:22,150
So this dataset had a smoking status.

286
00:32:23,430 --> 00:32:26,660
Yes, no age with five age groups.

287
00:32:26,670 --> 00:32:35,370
And I think that we also at the end of the post on handout, we had this age squared in there as well.

288
00:32:35,370 --> 00:32:42,029
So I went ahead and included that variable as well, and we'd only run this with four signs.

289
00:32:42,030 --> 00:32:47,950
So we're going to see what negative binomial looks like here. So here's the data set.

290
00:32:48,980 --> 00:32:55,700
Same as we had in the past. One hand out. And I've got the count, the death counts here in blue.

291
00:32:55,700 --> 00:32:59,140
And notice there's no zero counts in this dataset at all, right?

292
00:32:59,150 --> 00:33:07,580
There's no single zero count in this data set. So I immediately, immediately am ruling out the need for zero inflated models.

293
00:33:07,580 --> 00:33:11,959
There's certainly not excess zeros because there's not a single zero in the data set.

294
00:33:11,960 --> 00:33:18,020
So for this review handout, I'm not doing any of the zero inflated models just because we're looking at this data set,

295
00:33:18,020 --> 00:33:23,270
but you can review your handouts where we did cover zero inflated models to review that material.

296
00:33:24,710 --> 00:33:31,610
So, so let's get some intuition again about whether you need the offset or not.

297
00:33:31,610 --> 00:33:35,750
So if we were just modeling the blue column of counts.

298
00:33:36,720 --> 00:33:46,650
And ignoring all the person years of follow up. You know, if we didn't have an offset at all, how would we how would the science be affected?

299
00:33:47,310 --> 00:33:56,800
I mean, because these numbers are so different for the years of follow up, i, i it would seriously bias your analysis.

300
00:33:56,820 --> 00:34:05,530
So, for instance, there are a lot, a lot of people in this youngest age group of smokers.

301
00:34:05,550 --> 00:34:15,630
Look at how many that might be the largest group of any in terms of how long, you know, how many person years contributed to this count of 32.

302
00:34:17,150 --> 00:34:24,049
Right. If you ignore that last call, then you might think that that group, you know,

303
00:34:24,050 --> 00:34:33,710
if you're just looking at the mean counts and estimating the mean as your outcome, you might think that group has worse mortality than the nonsmoker.

304
00:34:34,310 --> 00:34:39,020
Fine. That makes sense. The nonsmoker, older nonsmoker.

305
00:34:39,470 --> 00:34:42,740
You know, all of these groups any age.

306
00:34:44,310 --> 00:34:47,360
Of nonsmokers. This 32 is higher.

307
00:34:47,370 --> 00:34:55,449
Right. So you might think that that. This youngest age group has worse mortality than any of these others.

308
00:34:55,450 --> 00:34:59,660
But it's not. That would be incorrect, right?

309
00:34:59,680 --> 00:35:04,390
Look at this highest age group. There were 31 deaths.

310
00:35:04,810 --> 00:35:12,580
But from watching many, many fewer person years more, those deaths could have accrued.

311
00:35:13,450 --> 00:35:16,960
So if you ignore the follow up years,

312
00:35:17,410 --> 00:35:26,770
you would have a completely nonsense interpretation of how how much mortality impacted this group compared to this group.

313
00:35:28,690 --> 00:35:34,839
All right. So you could fit the model without an offset and and find the mean for these groups.

314
00:35:34,840 --> 00:35:42,490
But there's no scientific value to that. To compare mortality in each of these groups, you have to put them on an equal playing field.

315
00:35:43,360 --> 00:35:48,670
And so instead of looking at accounts, poor per whatever, this number was for each of the group.

316
00:35:50,050 --> 00:35:54,430
You need to be looking at the number of deaths per some common person.

317
00:35:54,430 --> 00:35:59,230
Years of follow up. And so that's why we create this offset.

318
00:35:59,650 --> 00:36:05,920
This now we'll be able to look at the number of deaths per hundred thousand years of follow up,

319
00:36:06,220 --> 00:36:11,020
and that will be applied equally to all of these different combinations of covariates.

320
00:36:11,020 --> 00:36:19,090
So we can really see, you know, it's smoking bad or how does mortality work for these different age groups in relationship to one another?

321
00:36:24,080 --> 00:36:31,610
And so in sense you'll see the offset here and you'll know you'll have to look in the code how this variable was created,

322
00:36:31,610 --> 00:36:35,010
just, you know, the right interpretation of it. Right.

323
00:36:35,550 --> 00:36:42,990
And the output. So you'll actually see the offset variable here as you know, saying it was used.

324
00:36:43,590 --> 00:36:45,569
And then here is here's the output.

325
00:36:45,570 --> 00:36:53,190
And so these parameters are going to be interpreted with respect to the the number of deaths per 100,000 person years.

326
00:36:53,730 --> 00:36:59,030
Right. So here's a review of how it works.

327
00:36:59,510 --> 00:37:02,720
This is very similar to a hand that we had before.

328
00:37:03,230 --> 00:37:08,710
But I'm I want to give you practice with formulas and how to set up contrast statements.

329
00:37:08,720 --> 00:37:13,740
Right. So this is. You know, from the output.

330
00:37:14,160 --> 00:37:20,280
This is the the log of the lung cancer deaths per 100,000 person years.

331
00:37:20,850 --> 00:37:26,399
So this is the outcome interpretation based on having the offset and I've just copied the

332
00:37:26,400 --> 00:37:30,250
parameter estimates for the intercept and all these different terms that were in the model.

333
00:37:30,300 --> 00:37:33,360
Okay. And I've color coded a couple of these.

334
00:37:34,440 --> 00:37:37,530
I'm focusing on the smoking variable. So that's in red.

335
00:37:37,530 --> 00:37:43,700
And then I have all these things in blue because they come up and they're going to help us with keeping track of what cancels and what doesn't.

336
00:37:45,320 --> 00:37:54,620
All right. So if I want to know the actual number of lung cancer deaths per person, years in the smoking group.

337
00:37:56,790 --> 00:38:02,030
Then this is the formula. That is applicable, right?

338
00:38:02,060 --> 00:38:10,040
So if I'm actually trying to get that number of deaths per hundred thousand person years, I'm going to need all of these terms in there in some form.

339
00:38:11,540 --> 00:38:16,909
The smoking variable that I'm you know, because they're smokers, the smoker variables are going to be a one.

340
00:38:16,910 --> 00:38:19,940
So of the code for all the variables will be in your materials.

341
00:38:19,940 --> 00:38:23,060
So smoking was quoted as a one. You saw the data that.

342
00:38:23,940 --> 00:38:32,380
And so one gets plugged in here and here. And the age group 55 to 64 was coded as age equals three.

343
00:38:32,450 --> 00:38:36,370
That would also be described in in the in the dataset for the problem.

344
00:38:36,910 --> 00:38:46,299
And so you need to plug in three for age and for age squared you're multiplying three times three and then smoker by age there's a one here,

345
00:38:46,300 --> 00:38:50,530
there's a three, four age. That's how the formula comes up and a comment.

346
00:38:50,620 --> 00:38:56,410
So this is basically how it works out. If you had to program a contrast statement for it,

347
00:38:56,770 --> 00:39:06,160
you would have each of these like intercept would be a you'd have intercept one, you'd have the smoking parameter with a one,

348
00:39:06,700 --> 00:39:18,660
you'd have a three times the age parameter and a three squared times the age squared parameter and a three times the interaction parameter.

349
00:39:18,670 --> 00:39:24,070
Right. That's how you would set up that contrast to get the same number and confidence intervals and things like that.

350
00:39:25,440 --> 00:39:32,620
Right. And it's something that people people really struggle with when they're trying to step or contrast statements.

351
00:39:32,780 --> 00:39:36,899
It's the same question comes up every year as students are studying.

352
00:39:36,900 --> 00:39:40,020
And that is, when do I use the intercept? When do I not use the interest?

353
00:39:40,290 --> 00:39:43,589
Do you know that's the that that really throws people?

354
00:39:43,590 --> 00:39:46,560
And of course, I'm going to have both options for every question.

355
00:39:47,100 --> 00:39:50,580
So you're going to need to know when to use the intercept, when you don't use the intercept.

356
00:39:51,180 --> 00:39:57,390
So whenever you're estimating the outcome. You're going to have the intercept, right?

357
00:39:58,880 --> 00:40:04,700
When you're comparing groups with some kind of a ratio, the intercept always cancels.

358
00:40:06,760 --> 00:40:18,100
So when like odds ratio multiplicative factors that involve a ratio, you know, anytime you do a ratio, the intercept is going to cancel out.

359
00:40:18,100 --> 00:40:25,150
I mean, you have to know what cancels out, what doesn't cancel out. But if you have if you're trying to estimate the outcome the way we are here.

360
00:40:26,770 --> 00:40:33,740
You need to have all the features in the model. And plug in the numbers for the coverage so the intercept will be there.

361
00:40:33,750 --> 00:40:40,320
So when we're estimating that lung cancer deaths per 100,000 person years, there's no ratio in that formula.

362
00:40:40,620 --> 00:40:46,080
We're putting all the terms in here and of course, exponentially because we model a log scale.

363
00:40:46,440 --> 00:40:56,220
Okay. And so similarly for the nonsmokers, if we're looking at the lung cancer deaths per 100,000 person years,

364
00:40:57,090 --> 00:41:01,200
the main thing that changes is that we plug in a04 smoker.

365
00:41:01,560 --> 00:41:05,580
And so these two terms with the blue go away.

366
00:41:07,500 --> 00:41:12,590
Right. So we'll still have because we're estimating the count per hundred thousand person years.

367
00:41:12,600 --> 00:41:17,459
That's like estimating the mean outcome. We're going to have E to the intercept.

368
00:41:17,460 --> 00:41:20,730
So that's there and then this is zero.

369
00:41:20,730 --> 00:41:24,959
So we don't need that, but we need three times the parameter for H.

370
00:41:24,960 --> 00:41:29,490
We need three squared for the parameter times H squared.

371
00:41:29,730 --> 00:41:38,430
This goes away because Smucker is zero. And so we get 433 and Samad lung cancer deaths 400,000 person years.

372
00:41:40,870 --> 00:41:44,949
Okay. So here's where a ratio is happening. So this is look at this sentence.

373
00:41:44,950 --> 00:41:54,999
This looks almost like part of a manuscript where the sentence so among those aged 55 to 64, that's the same age equals three group smokers have.

374
00:41:55,000 --> 00:42:02,319
And here's the formula times the number of lung cancer deaths per 100,000 patient years of follow up.

375
00:42:02,320 --> 00:42:08,700
So how do I know there's a ratio involved here? Because I've got a times there's some multiplicative factor here.

376
00:42:09,220 --> 00:42:16,000
One group has this many times the number of lung cancer deaths per.

377
00:42:17,050 --> 00:42:23,580
Hundred thousand patient years of follow up. So smokers versus non smokers this time.

378
00:42:23,580 --> 00:42:33,580
This is doing that. And so for the formula, you have the two formulas for the Count's 400,000 years here.

379
00:42:33,590 --> 00:42:43,400
So you can sort of see everything is going to cancel when you take the ratio except for these blue terms, these, you know, these elements with blue.

380
00:42:44,730 --> 00:42:52,620
So that's where this formula comes from. We have the the blue 1.44 that was that went with smoker this you know this term

381
00:42:52,620 --> 00:42:59,669
here and then we have the .3075 times three that came from the smoker by age.

382
00:42:59,670 --> 00:43:06,030
So when you take the ratio and you're looking at, you know, smokers versus non smokers,

383
00:43:06,030 --> 00:43:10,470
it's going to be the terms with smokers that smoker that don't cancel out.

384
00:43:12,050 --> 00:43:15,740
But everything else, if it doesn't involve smoking, will cancel out in these ratios.

385
00:43:17,570 --> 00:43:21,830
So you don't tend to have an intercept when you're comparing groups.

386
00:43:23,360 --> 00:43:32,480
Right. When you're comparing a count per something versus a count per something in two different groups, you have a lot of cancelations, actually.

387
00:43:35,440 --> 00:43:39,400
Right and you can actually check it. I always think it's fun to double check, you know.

388
00:43:40,900 --> 00:43:48,350
But there's this same. Right here if you multiply it by the.

389
00:43:49,690 --> 00:43:53,409
You know, the number of nonsmoking deaths per 2000 person years.

390
00:43:53,410 --> 00:43:57,400
You'll get that same count we got for the smoker death for hundred thousand person years.

391
00:43:58,960 --> 00:44:02,320
All right. So this this is like a fold change or multiplicative factor.

392
00:44:02,320 --> 00:44:04,240
We've said a lot of different terms about it.

393
00:44:05,350 --> 00:44:14,380
And the and this times is like a word to look for because we're often saying, you know, the word times when we've got a ratio.

394
00:44:19,710 --> 00:44:23,430
Right. And because there's this interaction.

395
00:44:24,410 --> 00:44:29,540
You know, we only looked at one age group. We used this, we looked at this age group three.

396
00:44:29,540 --> 00:44:31,339
But because of this interaction, you know,

397
00:44:31,340 --> 00:44:39,230
this multiplicative positive increase in the number of lung cancer deaths per 100,000 person years for smokers versus non smokers,

398
00:44:39,920 --> 00:44:45,320
you know, it changes by the age groups. So this is the formula for the general pattern and it's plotted here.

399
00:44:45,740 --> 00:44:52,280
So this is the same fold change or multiplicative effect that we saw where we just looked at it for three earlier.

400
00:44:54,400 --> 00:44:55,690
But, you know,

401
00:44:57,070 --> 00:45:05,830
you could calculate that by hand and see what it was for any of these age groups just to check that you're in the right place of the output.

402
00:45:06,760 --> 00:45:09,280
You know, when you have your estimate in contrast statements,

403
00:45:09,280 --> 00:45:14,409
they're always going to have the number that you can calculate by hand in that contrast statement.

404
00:45:14,410 --> 00:45:19,479
And that's one way to double check. You're in the right place in the output to get the answer you need.

405
00:45:19,480 --> 00:45:27,370
It's knowing how to do know which term you're actually needing to estimate will help you find the right output.

406
00:45:28,200 --> 00:45:31,770
For the confidence intervals are p values that go along with that estimate.

407
00:45:31,780 --> 00:45:35,990
Right. So don't be afraid to double check by hand.

408
00:45:36,770 --> 00:45:42,570
So you would know from the previous slide, you know, we did it for age equals three by hand here.

409
00:45:43,460 --> 00:45:52,790
And it can be handy to to know which number you would get by hand so that you can find the right contrast statement in your output.

410
00:45:52,910 --> 00:45:56,270
It's just a good way to double check that you're looking in the right spot.

411
00:46:02,090 --> 00:46:12,180
All right. So here is example code of how to do.

412
00:46:13,580 --> 00:46:18,680
You know, the to get the estimates and the confidence intervals for each of those points on that plot.

413
00:46:19,710 --> 00:46:28,370
So just so I have labels, but you know, when I'm writing the exam, I'm not going to give you cute labels telling you what I'm doing.

414
00:46:28,380 --> 00:46:33,990
I'm going to have like it'll be some generic name, like contrast one, contrast to contrast three.

415
00:46:34,410 --> 00:46:40,850
So these labels are here to help you study, but I'm not going to fill in the interpretations for you.

416
00:46:40,860 --> 00:46:45,300
You need to know what the interpretations are based on the code and the output.

417
00:46:45,750 --> 00:46:51,000
So it's a bit of a crutch that I put this in here for you just so you can learn what these are doing.

418
00:46:51,570 --> 00:46:57,750
But you're going to have very generic labels, you know, contrast one contrast to and you have to know what it's doing.

419
00:47:00,030 --> 00:47:08,980
All right. And the the output, you know, these are the the values.

420
00:47:08,990 --> 00:47:12,770
I've even got the formulas that we were using.

421
00:47:12,770 --> 00:47:15,520
So this is for age equals one, two, three,

422
00:47:15,530 --> 00:47:22,610
four or five where you find these things in the output and there's confidence limits and p values that go along with these.

423
00:47:23,630 --> 00:47:24,890
And I just have a kind of a note here.

424
00:47:24,890 --> 00:47:35,660
You know, for these oldest two age groups, they didn't have these multiplicative factors significantly different from from one.

425
00:47:35,660 --> 00:47:42,350
So, you know, the the differences are kind of diminishing for the older age groups between.

426
00:47:43,960 --> 00:47:45,310
I think I have a sentence here.

427
00:47:46,460 --> 00:47:54,890
Smokers aged 75 to 84 were observed to have essentially the same lung cancer death rate per hundred thousand person years as nonsmokers in this plot.

428
00:47:54,920 --> 00:48:01,040
See how it's close to one. But for these last two groups, the P values really weren't significantly different from one.

429
00:48:01,040 --> 00:48:06,590
So you could kind of make that same claim for this age group four based on.

430
00:48:07,800 --> 00:48:16,030
It's marginally significant, I guess. You know, I would still think maybe there's a difference there based on this p value point of five people love.

431
00:48:16,050 --> 00:48:23,310
So you could add a little bit of ambiguity there of where that's whether that's significantly different from one or not.

432
00:48:26,740 --> 00:48:31,899
All right. And so for someone doing our you know, this is what the code would look like here.

433
00:48:31,900 --> 00:48:40,780
Here's where the offsets being created. So you could find that here and you would know what age square it is because it's being created over here.

434
00:48:41,170 --> 00:48:47,700
The sun model. I couldn't fit the output underneath.

435
00:48:47,700 --> 00:48:58,169
So here's kind of the output that you would see in the R code and here is the the code for the various contrast, the same one.

436
00:48:58,170 --> 00:49:04,979
And again, I've got nice labels here telling you what I've done, but in your exam you would just have contrast one,

437
00:49:04,980 --> 00:49:08,550
contrast two, and you'd have to know which one is appropriate for which question.

438
00:49:14,770 --> 00:49:28,760
Okay. So we did talk a little bit about goodness of fit and the there is this deviance.

439
00:49:29,480 --> 00:49:38,790
I think we had one slide on this. But this the only time that this is really useful is if all of your predictors are either binary or categorical.

440
00:49:38,810 --> 00:49:43,969
So in the doctors dataset, that was the one dataset we've seen where that was the case.

441
00:49:43,970 --> 00:49:50,720
We had all categorical or binary predictors. And so the you can actually get a p value for goodness to fit.

442
00:49:51,770 --> 00:50:00,500
And so for the for this particular goodness of fit table, the deviance and the degrees of freedom.

443
00:50:03,320 --> 00:50:06,560
We'll let you calculate a p values.

444
00:50:06,590 --> 00:50:12,650
This value is under the null hypothesis is a chi squared with five degrees of freedom.

445
00:50:13,040 --> 00:50:17,700
And if you get a p value greater than 0.05, that indicates that you can you know,

446
00:50:17,780 --> 00:50:22,100
there's reasonably good fit here compared to the model that would have perfect prediction,

447
00:50:22,520 --> 00:50:26,120
fully saturated, identical, you know,

448
00:50:26,120 --> 00:50:38,120
to like remember if you remember how if you put in the same number of parameters as rows of your data set that you get perfect prediction.

449
00:50:38,450 --> 00:50:43,009
So this deviant statistic is always comparing the fit of your current model to

450
00:50:43,010 --> 00:50:46,640
the model that has the same number of parameters as groups in your dataset.

451
00:50:49,320 --> 00:50:54,660
So. So this particular result for the doctors data sets that this model,

452
00:50:54,660 --> 00:51:03,810
this final model that I've been working with this whole time fits reasonably well compared to the perfect model with ten parameters in it.

453
00:51:06,640 --> 00:51:11,080
Okay. So we haven't seen the negative binomial model applied to this dataset.

454
00:51:11,080 --> 00:51:19,600
So here it is. So how do you know it's the negative binomial model we've got just equals and b here we're using the same offset.

455
00:51:19,600 --> 00:51:25,630
So we're still going to be interpreting the rate of, you know, the number of deaths per 100,000 person years.

456
00:51:26,980 --> 00:51:34,610
And again, I've got nice, cute little titles here. I'm not going to have titles in in your in the exam.

457
00:51:34,630 --> 00:51:40,660
You're going to have to know what I'm doing based on the cold, without my titles telling you what the code is.

458
00:51:43,560 --> 00:51:47,280
So here's the comparison between the Poisson model and the negative binomial model,

459
00:51:47,610 --> 00:51:53,820
and this is the one time you can choose between models using the likelihood ratio test.

460
00:51:53,850 --> 00:51:59,549
These models are nested where there's only one parameter the the k the negative

461
00:51:59,550 --> 00:52:03,960
binomial dispersion parameter that's different between these two models.

462
00:52:04,380 --> 00:52:10,700
So like good race, your test here has a very small difference.

463
00:52:10,710 --> 00:52:19,860
I mean, this is crazy how identical the log likelihoods are between these two even to four decimal places.

464
00:52:20,340 --> 00:52:23,370
The Poisson models vastly prefer to the negative binomial model.

465
00:52:23,370 --> 00:52:31,310
It fits the data just fine. And the AIC, if you look at the AIC, the AIC for the Poisson model is also smaller.

466
00:52:31,320 --> 00:52:35,670
So the AIC agrees in this case that some models fine.

467
00:52:36,600 --> 00:52:43,920
So that's how we choose. And the R code and output would look like this.

468
00:52:45,040 --> 00:52:48,609
And for this particular example, there were no zeros in the data set at all.

469
00:52:48,610 --> 00:52:53,020
So I didn't move forward with how to test for excess zeros.

470
00:52:54,130 --> 00:52:59,800
The earlier PERSAN model is the best model of of all four count models that we've looked at.

471
00:53:01,600 --> 00:53:05,379
Just a reminder that liquid ratio tests are not suitable for comparing zero inflated

472
00:53:05,380 --> 00:53:09,250
models and their non-zero inflated counterparts because the models aren't nested.

473
00:53:09,260 --> 00:53:12,970
So you have to, you know, use the wrong test.

474
00:53:13,870 --> 00:53:20,200
All right. So I'm going to go back. This is, I think, the end of the count.

475
00:53:21,710 --> 00:53:27,920
Model reviews. So the on in the negative binomial is your inflated count miles the stuff that you're working on for your homework.

476
00:53:27,920 --> 00:53:33,650
Now, this is the end of my review for that material. Did you have any questions that you wanted to.

477
00:53:34,520 --> 00:53:42,200
I've been talking and talking to sort of feeding your brain as much as I can, but this is meant to stir your brain if you have questions.

478
00:53:42,200 --> 00:53:48,350
So would you like to ask some questions about the count stuff or the rate stuff?

479
00:53:53,430 --> 00:54:03,370
I'm just scanning eyeballs here. I think everybody is hoping someone else will ask a very insightful question.

480
00:54:03,640 --> 00:54:20,520
Yes, I guess that's. So okay.

481
00:54:20,560 --> 00:54:27,969
So that sends a good question to the questions about parameter interpretation and we don't have probabilities of anything.

482
00:54:27,970 --> 00:54:39,280
Now, probabilities are going to end up being tied to binary outcomes because means of zero one outcomes or probabilities.

483
00:54:39,280 --> 00:54:50,530
So we won't be looking at probabilities, but we will be doing is either having, you know, what's the mean count if we don't have any offset term.

484
00:54:52,070 --> 00:54:58,610
What's the number of counts per some person your unit.

485
00:54:59,740 --> 00:55:03,310
If we've got an offset term. And so.

486
00:55:06,400 --> 00:55:11,410
In logistic regression. When you talk about the mean of the outcome, it is a probability here.

487
00:55:11,410 --> 00:55:15,340
It really is just the mean of account when you don't have the offset term.

488
00:55:18,300 --> 00:55:21,420
When you're logistic regression, you're talking about odds ratios.

489
00:55:21,420 --> 00:55:26,969
So you frame your sentences to be about this. Odds is so much times higher than that.

490
00:55:26,970 --> 00:55:35,460
Odds right here. It's the multiplicative factor for one group having higher counts than another.

491
00:55:38,720 --> 00:55:43,790
So this is kind of the most useful slide for how to word that kind of stuff.

492
00:55:44,750 --> 00:55:51,200
So you're either estimating a count. This is like the mean number of lung cancer deaths per.

493
00:55:52,480 --> 00:55:58,060
You know, 100,000 person years. And this per part just came because we had an offset.

494
00:55:59,580 --> 00:56:03,270
Of the log follow up years divided by 100,000.

495
00:56:05,270 --> 00:56:13,940
And so in logistic regression, this would have been like, you know, estimating probabilities to if that's that that's the mean for logistic,

496
00:56:13,940 --> 00:56:25,610
this is the mean for count models when you have an offset here, this is like the counterpart to odds ratios for these count models.

497
00:56:25,730 --> 00:56:31,670
So when you're comparing groups in logistic, you're talking about odds ratios here,

498
00:56:31,670 --> 00:56:38,870
you're talking about some multiple, some multiplicative effect on the counts.

499
00:56:38,870 --> 00:56:45,110
When you move from the risk factor on top to the risk factor on the bottom of your ratio.

500
00:56:47,190 --> 00:56:50,579
So there's lots of different ways to say that this is one way to say it.

501
00:56:50,580 --> 00:57:01,500
You know, that smokers have some number times, you know, the number of lung cancer deaths when you compare smokers to nonsmokers.

502
00:57:01,510 --> 00:57:06,000
So smokers at the top of the ratio, nonsmokers are at the bottom of the ratio.

503
00:57:06,570 --> 00:57:11,760
And this 1.60 is the part that you're getting from the model.

504
00:57:14,280 --> 00:57:18,120
So but there's lots of ways you could write this and you kind of need practice, right?

505
00:57:18,120 --> 00:57:22,439
Because you could say some tables would have like fold change, I think, in your homework.

506
00:57:22,440 --> 00:57:25,230
Did I use that or you were supposed to come up with something like that.

507
00:57:25,530 --> 00:57:30,960
But you can talk about a fold change like this is the fold change comparing smokers to

508
00:57:31,260 --> 00:57:35,430
smokers when you're estimating the lung cancer deaths per hundred thousand person years.

509
00:57:36,480 --> 00:57:42,389
So kind of getting practice of how to phrase these things and it can be written different ways.

510
00:57:42,390 --> 00:57:46,020
And unfortunately, I'm going to be the author of all the manuscript worthy phrases.

511
00:57:46,020 --> 00:57:52,020
So kind of knowing when you read a phrase that it's appropriate or not, you know,

512
00:57:52,020 --> 00:57:55,980
you kind of kind of you have to get used to how these things are written now.

513
00:57:57,760 --> 00:58:03,250
So I tend to use multiplicative factors for changes or times when I'm writing my sentence.

514
00:58:03,730 --> 00:58:06,730
Since. All right.

515
00:58:07,150 --> 00:58:10,780
Odds ratio will never appear in these sentences for count models.

516
00:58:10,780 --> 00:58:15,730
And unfortunately, every year there are students who just use odds ratios for everything.

517
00:58:15,970 --> 00:58:17,800
Does it matter if they're modeling accounts or not?

518
00:58:18,130 --> 00:58:24,640
And they get super frustrated when they pick an odds ratio sentence and everything else was in the sentence was correct,

519
00:58:24,640 --> 00:58:30,280
but the odds ratio part and they got marked wrong because it was a multiplicative effect, not an odds ratio.

520
00:58:33,000 --> 00:58:37,860
Does that give you a sense of kind of how to look and interpret?

521
00:58:38,670 --> 00:58:43,170
So that's going to be you know, if that's the part that's giving you the most trouble,

522
00:58:43,680 --> 00:58:51,360
then that's going to be the part you put on your cheat sheet to help you real quick with some of this language of how these get written.

523
00:58:54,350 --> 00:59:00,160
Yeah. Could you speak just a little louder?

524
00:59:00,170 --> 00:59:03,770
My hearing is not a young person's hearing, so.

525
00:59:11,080 --> 00:59:15,070
So the question is, does it have to be person years or you could compare like one year to the other.

526
00:59:15,520 --> 00:59:24,310
So here but notice I don't have year is a covariate that in the model.

527
00:59:24,880 --> 00:59:30,340
Right. So. Years is just in the model.

528
00:59:30,340 --> 00:59:33,340
This offset thing is just saying how many?

529
00:59:33,550 --> 00:59:44,710
How long did we have to observe the count, you know, but it's not in there as a covariate where you estimate a parameter for it.

530
00:59:46,480 --> 00:59:55,960
So I don't have a way for this model to talk about, you know, as years increase, do I get more events?

531
00:59:56,560 --> 01:00:00,010
It's just not set up to analyze that.

532
01:00:00,370 --> 01:00:07,240
If you really wanted to do that, you know, you could have years as a covariate,

533
01:00:08,650 --> 01:00:16,840
but it doesn't really make sense to try that in this context because of course, the longer you watch someone, the higher the counts are going to get.

534
01:00:17,780 --> 01:00:20,780
That's just the key. How counts don't go down.

535
01:00:21,830 --> 01:00:24,860
So the longer you watch, the more chances you are to get counts.

536
01:00:24,860 --> 01:00:32,530
So no one ever put stuff like years. In these models because the result is.

537
01:00:33,700 --> 01:00:42,850
Like very predictable. So they only put it in as an offset and there's no parameter that gets estimated with that.

538
01:00:43,390 --> 01:00:47,920
So that kind of goes back to this. There's no Baidu that's multiplied here.

539
01:00:48,790 --> 01:00:54,570
It's just trying to give you this. Maybe appreciating this truly.

540
01:00:55,050 --> 01:00:59,970
You need to be flute and log algebra. And I may have had this in the main handout,

541
01:00:59,970 --> 01:01:07,860
but if I moved this to the other side and it was log of the mean minus the log of the and I log

542
01:01:07,860 --> 01:01:14,130
algebra would say it's equivalent to the log of mu I over and I and then it would look like the,

543
01:01:14,970 --> 01:01:19,680
the interpretation would look just like the thing you would expect to see for a rate.

544
01:01:21,150 --> 01:01:24,360
So there's nobody here. It's not something.

545
01:01:25,550 --> 01:01:35,180
It really is just so that this stuff can be interpreted as a multiplicative factor on the rate of deaths per a common follow up period.

546
01:01:41,100 --> 01:01:45,180
Okay. All right. These are great questions.

547
01:01:48,930 --> 01:01:56,360
What's next? So I moved to the earlier part of the handout.

548
01:01:58,990 --> 01:02:03,470
Okay. So I haven't given you a break, and, uh.

549
01:02:04,780 --> 01:02:07,890
You know there's choices, right? You probably need a break.

550
01:02:07,900 --> 01:02:11,200
Honestly. There's a risk.

551
01:02:11,470 --> 01:02:16,420
I won't get to everything and hand out, but that's okay. You know, this is just a guideline.

552
01:02:16,450 --> 01:02:19,420
There's stuff in the handout. You can read it on your own if you need to. So let's take it.

553
01:02:19,660 --> 01:02:23,470
Let's just do a five minute break so that we have a higher chance of getting to everything.

554
01:02:23,830 --> 01:02:29,290
So let's meet again at 908 by my iPhone.

555
01:03:27,810 --> 01:03:31,860
No as great questions as you would like. Would you like me to put this on?

556
01:03:42,470 --> 01:03:45,900
I'm a little closer so I can see what he's doing. This is right. Yes.

557
01:03:51,030 --> 01:04:04,050
So. What's your zero model before you go on?

558
01:04:08,740 --> 01:04:12,860
Well, if you're. Well, if it's not crazy. Okay.

559
01:04:14,210 --> 01:04:17,860
So it does look wacky. And my guess is that you have too much stuff in your.

560
01:04:19,260 --> 01:04:23,070
Because that's there's convergence issues if you try to throw everything in there.

561
01:04:24,030 --> 01:04:31,810
And it's I think it. Whenever there's convergence issues, it's almost always binary of categorical variable and zero.

562
01:04:32,910 --> 01:04:36,470
And so I suspect you're trying to do that. Yeah.

563
01:04:36,480 --> 01:04:44,170
So when you go to the. What? This is the.

564
01:04:49,230 --> 01:05:00,480
I don't see the film where we're. Will be so.

565
01:05:02,110 --> 01:05:08,020
Oh. So this is naked body, obviously, as a model as all. That's when you were dreaming of a woman with the problem, too.

566
01:05:08,680 --> 01:05:13,940
Okay. Problems? Yeah. So this is still.

567
01:05:15,670 --> 01:05:20,980
If this is still just negative binomial, so that part should still run.

568
01:05:24,070 --> 01:05:30,390
So what's going on? Oh, okay.

569
01:05:30,540 --> 01:05:43,820
Also, first of all, I've seen the type. So.

570
01:05:47,330 --> 01:05:49,360
Or this is something that comes up a lot.

571
01:05:49,370 --> 01:05:57,550
So when you've got this wonderful, it's treated like a variable without thinking about the ingredients of what makes up.

572
01:05:59,780 --> 01:06:06,460
So look at the algebra for what makes. And for every if you want to.

573
01:06:09,460 --> 01:06:19,460
Also I think you the security. Is 1.1.2.

574
01:06:21,790 --> 01:06:28,940
So. If you have to set a date.

575
01:06:34,560 --> 01:06:38,540
Oh. Oh. This is having a.

576
01:06:51,850 --> 01:06:55,480
So that's what I think. I think the same thing. I think it might be.

577
01:06:56,720 --> 01:07:00,430
Yes, but that's why we need to double check that for sure.

578
01:07:08,380 --> 01:07:14,920
Yeah. So it can go from 0 to 134 I guess.

579
01:07:15,260 --> 01:07:22,390
Okay. Yeah, that's going to fix some stuff right there because it doesn't have enough data between zero and one to really estimate those things.

580
01:07:22,920 --> 01:07:26,070
That's probably what it is. I mean, really?

581
01:07:26,790 --> 01:07:31,450
I mean. This problem was nobody between zero and one.

582
01:07:31,470 --> 01:07:36,840
I don't know if I could do it, but that has to be okay.

583
01:07:44,590 --> 01:07:49,329
All right. So we're going to get back to work. That was super, super quick break.

584
01:07:49,330 --> 01:07:55,479
But I want to make sure I give you whatever I can from the handout and chances to ask questions.

585
01:07:55,480 --> 01:07:59,350
So for for these earlier slides that I'm coming back to now.

586
01:08:00,400 --> 01:08:07,420
So what slide number is this? You can find me at slide five. This is going over all the binary outcome.

587
01:08:07,430 --> 01:08:13,730
Oh, yeah. The nested models.

588
01:08:16,600 --> 01:08:21,010
For the count staff. Okay.

589
01:08:21,400 --> 01:08:27,460
So the question from the back of the room is, you know, how do you figure out if your models are necessary or not?

590
01:08:28,660 --> 01:08:32,229
So and this is this is a good question. We wrestle with this.

591
01:08:32,230 --> 01:08:37,690
Right. So here's some general guidelines for this.

592
01:08:39,320 --> 01:08:46,310
Yep. First thing, the two models you're comparing have to be fit on the same people.

593
01:08:47,780 --> 01:08:53,089
So in your homework you felt you had to deal with this issue, right?

594
01:08:53,090 --> 01:08:58,350
Because there were some people who had missing emphysema. And the model.

595
01:08:58,350 --> 01:09:03,560
All the models that had emphysema variables in them would drop anybody with missing emphysema.

596
01:09:04,900 --> 01:09:12,450
So a model with the full dataset versus the model with just the people who have complete emphysema,

597
01:09:12,460 --> 01:09:18,480
they'll never be nested because you're not fitting them on the same. Data set the same people.

598
01:09:18,660 --> 01:09:22,530
You have to have the identical sample size, the same people in the model.

599
01:09:24,200 --> 01:09:29,030
So that's something that you just picked up recently because of your homework and missing data.

600
01:09:30,170 --> 01:09:31,969
All right. So so from here on,

601
01:09:31,970 --> 01:09:39,360
the rest of my discussion is assuming you've got the same data set and the same number of people and the two models you're comparing.

602
01:09:39,380 --> 01:09:42,870
Okay. We're going to build from there now. So.

603
01:09:44,250 --> 01:09:54,420
If you have the same model style, like a four count models, if you are still doing distributions person for both models.

604
01:09:56,280 --> 01:09:59,910
That's necessary to even start asking about whether.

605
01:10:02,480 --> 01:10:09,790
I'm a backup because I'm going to talk myself to a corner when I know that you can compare negative binomial and poisson with like the ratio test.

606
01:10:10,060 --> 01:10:14,660
So. See how hard this is even to explain.

607
01:10:14,990 --> 01:10:18,770
Okay, so start from the same modeling framework.

608
01:10:18,770 --> 01:10:22,130
Suppose you're doing per sign model one puts on model two.

609
01:10:22,820 --> 01:10:31,370
How do you tell if something's nested or not? It's an easy question if one model has.

610
01:10:32,930 --> 01:10:39,680
All the parameters and the reduced model just results from removing one of your variables.

611
01:10:41,160 --> 01:10:47,610
Those models are nested whenever you're removing one variable from a long list.

612
01:10:48,300 --> 01:10:51,630
Nothing else about the models are changing. That's going to be nested.

613
01:10:54,310 --> 01:10:58,790
Okay. All right. Let's add one more degree of difficulty here.

614
01:11:00,280 --> 01:11:08,380
There's one case when it's not obvious that you're just removing one variable that we've seen, and that's when you're comparing.

615
01:11:09,970 --> 01:11:19,090
A categorical variable that has several levels, you know with the class statement or your resort factor in are to.

616
01:11:20,640 --> 01:11:25,770
An ordinal variable where you use just one variable.

617
01:11:26,010 --> 01:11:29,310
You're kind of like treating it like continuous. You're not using your class statement.

618
01:11:29,640 --> 01:11:33,330
You get one parameter estimate instead of a bunch.

619
01:11:34,450 --> 01:11:36,190
I've categorical estimates.

620
01:11:36,700 --> 01:11:45,799
So it's not clear because you're using the same variable name and you're just treating it either as a categorical with your class or as fact,

621
01:11:45,800 --> 01:11:50,980
not, fact or not. It's really hard to tell. That that's nested.

622
01:11:50,980 --> 01:11:54,540
But it is. And we showed algebra.

623
01:11:55,090 --> 01:12:03,100
Algebra is not the favorite thing, but we did actually show algebra for a simple case to show you how one was a subset of the other.

624
01:12:04,210 --> 01:12:10,930
It's you can algebraically write it so that one variables the ordinal version of the variable and

625
01:12:10,930 --> 01:12:16,360
you've got a couple other terms that are added to get algebraically to the categorical version.

626
01:12:17,750 --> 01:12:25,549
So you just need to trust that you've seen it algebraically worked out once and just own that for any other time.

627
01:12:25,550 --> 01:12:29,990
You're comparing the same variable as a categorical versus an ordinal.

628
01:12:31,590 --> 01:12:35,220
Those are nested always, regardless of which type of model you're fitting.

629
01:12:36,360 --> 01:12:43,110
That algebra always is possible, although if you make it a harder case, it'll take me hours to prove it to you.

630
01:12:44,600 --> 01:12:49,320
Okay. If you've got a continuous variable.

631
01:12:51,070 --> 01:12:55,840
And you try to chop it up into categories or binary.

632
01:12:57,100 --> 01:13:00,550
There's no algebra that'll make those nested. It's just not possible.

633
01:13:00,910 --> 01:13:06,810
That's a general thing. I'll never be able to get from one.

634
01:13:07,170 --> 01:13:12,420
You know, I'll never be able to figure out what the continuous values would have been from the other.

635
01:13:13,740 --> 01:13:19,410
Predictors that you've chopped up in some way. You've removed some information about what the variable is.

636
01:13:19,410 --> 01:13:27,570
Somehow you can't get from one to the other. All right.

637
01:13:27,890 --> 01:13:34,370
So that's when you're in the same. Like model, like porcelain versus porcelain.

638
01:13:34,370 --> 01:13:42,130
And the only thing you're changing is the predictors. So the next level of difficulty.

639
01:13:44,460 --> 01:13:52,040
Okay. I've shown you one case where a smart switch from porcine to negative binomial.

640
01:13:52,050 --> 01:13:58,080
But I told you it was nested. And I didn't really have a detailed proof.

641
01:13:58,080 --> 01:14:02,850
I said there's one parameter difference between the way the models fit in the background.

642
01:14:03,870 --> 01:14:11,939
And so it's that what is the dispersion parameter that the negative binomial uses but the, the behind the scenes,

643
01:14:11,940 --> 01:14:19,680
the likelihoods you can, it'll be reduced to the Poisson likelihood if that one parameter is zero.

644
01:14:20,130 --> 01:14:24,870
And so without really good proof, because the algebra is beyond what you would be interested in,

645
01:14:24,870 --> 01:14:31,050
I promise you that one case is a nested model and you wouldn't know it except that I've told it to you.

646
01:14:31,350 --> 01:14:40,680
And you will trust this. It would not have been obvious to you other than the fact that I told you it is nested.

647
01:14:43,000 --> 01:14:46,630
So I expect you to believe me. All right.

648
01:14:47,020 --> 01:14:54,330
And then there's this other situation where we've talked about, like Pakistan model.

649
01:14:54,340 --> 01:14:58,390
And then there's something a leap to a zero in platypus on model.

650
01:15:00,950 --> 01:15:07,500
And I've told you, those are not nested. But there's at least some intuition in that case to help you remember it.

651
01:15:08,670 --> 01:15:13,010
You've moved from having one model statement to two model statements.

652
01:15:13,020 --> 01:15:16,240
You haven't just added one variable to the mix.

653
01:15:16,560 --> 01:15:24,960
You've added an entirely different logistics style model to the mix, and there's no way to nest those.

654
01:15:24,970 --> 01:15:33,270
There's stuff going on behind the scenes in the estimation that just doesn't let you get from one model to the other in an algebraic way.

655
01:15:33,390 --> 01:15:40,200
I can't write out one as a special case that the other in any reasonable way when I'm looking at the likelihood formulas.

656
01:15:41,870 --> 01:15:48,160
So. Carol. The person model is not nested within the zero latitude Poisson model.

657
01:15:49,000 --> 01:15:53,440
The negative binomial model is not nested within the zero to negative binomial model.

658
01:15:54,750 --> 01:15:58,680
You don't go from one model statement to two model statements and have nesting be true.

659
01:16:02,340 --> 01:16:07,760
All right. Have a covered all the cases. I think that's actually a pretty decent overview.

660
01:16:08,810 --> 01:16:12,790
Of how to tell. All right.

661
01:16:16,850 --> 01:16:21,410
Yeah. The question. Yes.

662
01:16:28,230 --> 01:16:31,610
What is this blind term and why do we need it? Good question.

663
01:16:31,620 --> 01:16:35,900
I think I was five for that, so let me find it. I'm sure I do.

664
01:16:36,360 --> 01:16:40,580
Yeah. Uh, let's see here. Let's jump to that.

665
01:16:40,590 --> 01:16:44,800
So where are we? To a.

666
01:16:46,650 --> 01:16:54,880
Way, way down here. Look at all these slides.

667
01:16:55,290 --> 01:17:08,060
I really worked hard for you guys. Gosh, I have to use control.

668
01:17:17,580 --> 01:17:24,569
Oh, here it is. Okay. All right.

669
01:17:24,570 --> 01:17:30,740
So. So spline terms.

670
01:17:31,220 --> 01:17:34,940
What are they? Why do you need them? All right, so.

671
01:17:37,260 --> 01:17:41,070
It's another version of the predictor in the model. First off.

672
01:17:42,470 --> 01:17:48,860
So. When you have like a continuous variable in the model.

673
01:17:50,720 --> 01:17:55,760
Say age, we've used age or actually let's just do the emphysema because you've been doing that in your homework.

674
01:17:55,760 --> 01:17:59,650
So you have emphysema in your model. So how is that interpreted?

675
01:17:59,660 --> 01:18:09,710
You're basically saying for every unit increase in emphysema, the effect on the outcome is the same, whether you're going from 0 to 1 or.

676
01:18:10,980 --> 01:18:22,200
55 to 56. You're saying that's the same and you have ways to in your other classes, you've found ways to kind of change that.

677
01:18:22,200 --> 01:18:27,180
But it's been mainly by creating categories of the variables, right?

678
01:18:27,540 --> 01:18:30,929
So instead of having continue with emphysema in your old classes, you would say,

679
01:18:30,930 --> 01:18:36,749
well, what if I make a parameter for the, you know, emphysema between zero and ten,

680
01:18:36,750 --> 01:18:40,260
between ten and 20, and you would create a categorical version,

681
01:18:40,560 --> 01:18:47,160
and that would let you have different estimates for the impact of emphysema on the outcome according to the category.

682
01:18:47,160 --> 01:18:54,180
But that if throwing a lot of information away in the data there because you're treating everybody in the same category is the same.

683
01:18:55,810 --> 01:19:05,320
So spline terms are kind of something in between a continuous variable and a categorical one from the same information.

684
01:19:05,950 --> 01:19:12,849
And so what happens is, so here in this kind of general code I'm allowing,

685
01:19:12,850 --> 01:19:18,729
I keep on saying we can create linear spline terms, allowing bands at these different points.

686
01:19:18,730 --> 01:19:24,550
So I needed to describe a little better what I mean by bends and so on. So this in your homework you had a bend.

687
01:19:24,700 --> 01:19:27,760
What were the values I had? You do bend. That was a ten and 35.

688
01:19:28,510 --> 01:19:33,730
And so I had it to 1041 at 3532, something like that, you know.

689
01:19:34,510 --> 01:19:38,530
And so what is this doing? So this would be emphysema minus ten times the indicator.

690
01:19:38,530 --> 01:19:42,370
Emphysema is greater than ten and you get a beta that goes with that.

691
01:19:43,870 --> 01:19:47,740
So what happens is you have your emphysema variable.

692
01:19:49,150 --> 01:19:55,719
And before ten, it's the only emphysema variable that's in play saying, you know,

693
01:19:55,720 --> 01:19:59,260
for a single unit increase in emphysema, what's happening to the outcome?

694
01:19:59,650 --> 01:20:01,660
It's just that one parameter for emphysema.

695
01:20:01,930 --> 01:20:10,390
And the reason it's the only one that comes into play is that this is a10 variable that only turns on when emphysema is greater than ten.

696
01:20:10,930 --> 01:20:19,360
So if when you're less than ten, this is a zero. And this one where T2 is 35, it only turns on once emphysema is greater than 35.

697
01:20:20,020 --> 01:20:28,570
So when you have emphysema less than ten, these two parts are zero and these parameters don't apply.

698
01:20:30,010 --> 01:20:38,110
So the. So the emphysema parameter is the effect of one year to increase in emphysema when you're less between zero and ten.

699
01:20:38,110 --> 01:20:44,860
You know the effect that has on the outcome and I'm assuming it's the same for everybody in that range,

700
01:20:44,980 --> 01:20:49,090
that if you go up by one, that's the same effect on the outcome.

701
01:20:51,540 --> 01:20:57,510
All right. So once you hit ten, I have two parameters that turn on.

702
01:20:57,630 --> 01:21:03,210
I have the emphysema, one I started with. But then I also have this one become a one.

703
01:21:03,690 --> 01:21:07,000
And I have the empathy value minus ten here.

704
01:21:08,500 --> 01:21:13,600
And you kind of have to pay attention to the ingredients, not just focused on the variable name being in your model.

705
01:21:13,630 --> 01:21:17,560
You have to pay attention to the ingredients. So once you go past ten.

706
01:21:19,110 --> 01:21:27,090
You're plugging in whatever the emphysema value is here, you it's you have minus ten here in your formulas.

707
01:21:27,100 --> 01:21:31,620
You need to kind of pay attention when you're plugging into contrast statements, what these things are.

708
01:21:32,130 --> 01:21:39,000
So what does it mean? It means that for every one unit increase in emphysema, once you're past ten,

709
01:21:40,020 --> 01:21:47,610
the beta from the emphysema term plus the beta for this term is influencing the change in the outcome.

710
01:21:48,090 --> 01:21:55,260
So suddenly, once you hit ten, I have the possibility of saying there's a it's worse or it's better.

711
01:21:56,410 --> 01:22:02,830
You know, it can change from the people who had emphysema in the in the 0 to 10 range.

712
01:22:02,830 --> 01:22:08,420
I can change the scientific story based on what this parameter is, that it's either better or worse.

713
01:22:08,420 --> 01:22:13,920
So one unit change. Relative to the people that had emphysema, less than ten.

714
01:22:14,790 --> 01:22:19,320
So it allows me to kind of. Let the data speak to.

715
01:22:19,330 --> 01:22:24,680
Is there a point at which emphysema the effect of having more and more emphysema stabilizes?

716
01:22:24,700 --> 01:22:29,080
Maybe it's getting terrible. I'm not talking about the homework. This is all hypothetical.

717
01:22:29,740 --> 01:22:33,940
But is there some point in emphysema where, you know, it's you can't get any worse?

718
01:22:33,940 --> 01:22:38,710
You're already awful like you've bottomed out on how worse it's going to get for you with your health.

719
01:22:38,740 --> 01:22:45,010
Is there some this would allow you to kind of have a place in the model where it can't get any worse?

720
01:22:45,010 --> 01:22:48,010
This is as bad as it's going to get and have you still walking.

721
01:22:49,680 --> 01:22:55,290
You know, so you could imagine different shapes of how that pattern might grow over time, maybe when.

722
01:22:55,530 --> 01:22:59,339
And again, I'm not talking about the homework because your homework and that data set is

723
01:22:59,340 --> 01:23:03,450
really kind of it's it's a it's an interesting little strange little data set.

724
01:23:03,840 --> 01:23:07,260
But you can imagine all kinds of hypothetical things that you could have seen.

725
01:23:07,590 --> 01:23:12,749
So, for instance, maybe when you just have a little emphysema, it's not affecting your exacerbations at all.

726
01:23:12,750 --> 01:23:18,540
It's just like, you know, it's there. It's kind of brewing, but it's not really having an immediate impact on your outcomes.

727
01:23:19,170 --> 01:23:26,790
And then maybe there's this period where as it gets worse, you have, you know, more exacerbations,

728
01:23:27,840 --> 01:23:37,260
but maybe then there's at some point in time where, again, your your lungs are just awful and it can't get any worse.

729
01:23:37,260 --> 01:23:41,190
I mean, it's just adding fuel, fuel to the fire.

730
01:23:41,190 --> 01:23:46,259
But you're topped out. You just maybe you're in the hospital all the time.

731
01:23:46,260 --> 01:23:50,610
I mean, there's some upper limit, right? How bad it can get and have you still alive.

732
01:23:51,810 --> 01:23:55,290
So this blind allows you to tell that story with the data.

733
01:23:56,130 --> 01:24:01,140
If you don't have some way to have more than one parameter, tell that story.

734
01:24:01,140 --> 01:24:06,210
You're going to be assuming the same story all the way through for all the values of emphysema.

735
01:24:08,730 --> 01:24:15,780
Okay. And if you use categories, you know, if you just had categories between zero and ten,

736
01:24:16,050 --> 01:24:24,960
then you're assuming there's no difference in your outcomes within the whole range of that category that whether you have zero or 10% emphysema.

737
01:24:25,350 --> 01:24:28,350
Same in terms of the predicting your outcomes.

738
01:24:28,890 --> 01:24:32,730
And so when you have these wide categories, there can be differences, right?

739
01:24:32,760 --> 01:24:38,790
So that's that's all you did in your earlier classes because it was easy to learn how to do.

740
01:24:39,360 --> 01:24:42,480
But we're growing. We need this ability.

741
01:24:42,840 --> 01:24:46,499
And it's going to be very important later when we talk about trends over time,

742
01:24:46,500 --> 01:24:52,139
we're going to want to have trends over time where we have kind of like bends over time

743
01:24:52,140 --> 01:24:55,500
that reflect the way the data is changing over time and not have it just be flat,

744
01:24:55,500 --> 01:25:01,900
flat, flat, flat, flat with categories. So I.

745
01:25:02,380 --> 01:25:07,760
So in this particular slide, you know, as you're choosing terms of the continuous predictors, blinds are very much in the mix.

746
01:25:07,780 --> 01:25:13,630
I want you to own this skill and be able to use it. This is going to be this is going to make your career better.

747
01:25:13,930 --> 01:25:22,780
To know and be able to be fluent in spline terms. And so estimates of odds ratios in logistic regression or multiplicative terms

748
01:25:22,780 --> 01:25:26,829
and Poisson regression will require calculate calculation of an appropriate

749
01:25:26,830 --> 01:25:36,100
Formula one spline terms are used and you need to pay attention to the ingredients of this online term and not just think the variable itself is,

750
01:25:36,520 --> 01:25:41,410
you know, untouched. When you're plugging in values for emphysema, you would actually have to,

751
01:25:41,980 --> 01:25:47,200
you know, do that whatever that number for emphysema minus the bend number is.

752
01:25:48,100 --> 01:25:54,760
And whether it's greater than that bend number to decide what to multiply by that parameter in any formula.

753
01:25:54,820 --> 01:26:00,720
Right. So you've.

754
01:26:01,870 --> 01:26:04,930
You're doing this in homework. Three, you're having to estimate.

755
01:26:05,970 --> 01:26:10,049
You know, what happens to outcomes for this or that scenario.

756
01:26:10,050 --> 01:26:17,459
And you're using contrast statements in homework three and you need to practice and it

757
01:26:17,460 --> 01:26:21,810
can happen in any spline terms can be useful in any model that's a regression model,

758
01:26:22,080 --> 01:26:29,610
even linear regression that you learn in earlier classes. Okay.

759
01:26:33,140 --> 01:26:40,690
Let's see. All right. So shall I go back? Or are there other topics that are just like you want to make sure I get to?

760
01:26:45,530 --> 01:26:49,100
Okay. All right. Let me go back just to give you ideas for questions.

761
01:26:49,970 --> 01:26:53,150
So this Slide five now. All right. So.

762
01:26:56,040 --> 01:27:02,010
Matched binary outcome data. So this is something I don't know if you had this in other courses in in the past or not.

763
01:27:02,030 --> 01:27:06,390
I, I don't know if you had this in, in 522, for example.

764
01:27:06,900 --> 01:27:13,650
So we've looked at this in a few different contexts. We've looked at McNamara's test for 1 to 1 matching,

765
01:27:14,460 --> 01:27:20,550
and we've looked at mental handle methods for different numbers of cases to controls that was in handout four.

766
01:27:20,940 --> 01:27:24,360
And then totally unrelated analysis.

767
01:27:24,360 --> 01:27:33,419
We've looked at Kappa agreement analysis where there was correlation in these outcomes because maybe different raters were saying yes or no,

768
01:27:33,420 --> 01:27:38,160
there was the disease. All right. So there's an alternative way to look at match pairs data.

769
01:27:38,550 --> 01:27:44,070
End three. You've had homeworks that have looked at the same dataset in different ways using these methods.

770
01:27:44,070 --> 01:27:47,940
So you could sort of see what was the difference in interpretation from these various methods.

771
01:27:48,870 --> 01:27:49,860
Will Eventually.

772
01:27:50,100 --> 01:27:56,430
The reason I like to review this in this course is because eventually we're going to deal with regression models for these same scenarios.

773
01:27:57,450 --> 01:28:02,880
And we haven't gotten there yet. But this is kind of the background context for what we'll do that.

774
01:28:03,360 --> 01:28:07,470
And so we had odds ratios that were continu condition.

775
01:28:08,010 --> 01:28:13,680
So are you saying continuous from last conversation? We have conditional odds ratios when you have matching.

776
01:28:14,730 --> 01:28:23,340
And so if you remember from way back when when we were talking about this, you know, we had tables that looked like,

777
01:28:23,610 --> 01:28:29,340
you know, the case member status exposed, not exposed, the control member status exposed, not exposed.

778
01:28:29,580 --> 01:28:35,010
And so each count is talking about a count that's classifying a pair of people.

779
01:28:37,170 --> 01:28:45,140
And we had, you know, the conditional odds ratio was looking at the discordant cells.

780
01:28:45,240 --> 01:28:50,100
Remember how this subscript works? I think that the first subscript was.

781
01:28:52,270 --> 01:28:59,140
Whether the. Oh, here it is. The case exposure status, whether they were exposed or not exposed.

782
01:28:59,230 --> 01:29:03,850
So here is the case that was exposed. Here in the bottom, the case was not exposed.

783
01:29:04,360 --> 01:29:07,720
And the second subscript was the control exposure status.

784
01:29:08,080 --> 01:29:13,750
So on top, the control was not exposed and in the bottom the control was exposed.

785
01:29:13,840 --> 01:29:18,069
So that's kind of how the subscripts worked. We called these cells here.

786
01:29:18,070 --> 01:29:21,700
The discordant cells is when one was exposed and the other wasn't.

787
01:29:23,450 --> 01:29:30,250
And so the ratio of the discordant cells is is looking at a conditional odds ratio.

788
01:29:30,260 --> 01:29:39,809
So it's taking into account the matching. And there's formulas for companies that, you know, you may need to know how to find the stuff in the output.

789
01:29:39,810 --> 01:29:45,900
I have this in there just so that, you know, you can have them, but you don't need formulas really,

790
01:29:45,900 --> 01:29:52,590
other than I think it's always handy to know how to create the odds ratio so you can find where the right term is in your output.

791
01:29:52,590 --> 01:29:54,840
When you're looking for confidence intervals and P values,

792
01:29:56,010 --> 01:30:06,690
there's a McNamara's test that will that you can look for to test the null hypothesis that you have the same

793
01:30:06,690 --> 01:30:15,720
chance of the case member being exposed in the control member not versus the the flip for the discordant sounds.

794
01:30:15,750 --> 01:30:23,070
So you have a p value to see whether that's significantly associated with, you know, the exposure status.

795
01:30:25,960 --> 01:30:36,520
And confidence in her roles. And I think that, you know, the interpretation part is the part we have to that's the muscle we need to exercise.

796
01:30:36,530 --> 01:30:39,310
I think finding the right part in the output is probably okay.

797
01:30:39,670 --> 01:30:47,350
So here's an example table where Radiologist one and two are saying whether the conditions absent are present.

798
01:30:47,350 --> 01:30:51,280
So they agreed that it was absent 60 times for 60 patients.

799
01:30:51,670 --> 01:30:56,800
They agreed it was present for 25 people and then they disagreed 15 times.

800
01:30:58,030 --> 01:31:04,329
And so the can you know, when you're looking at this data and you're doing this ratio of discordant cell counts,

801
01:31:04,330 --> 01:31:08,649
five over ten, you know, how do we write about that conditional odds ratio?

802
01:31:08,650 --> 01:31:18,340
So here is one way. So the odds ratio conditional that the assessor is radiologist to when assessed as absent versus present is five over ten.

803
01:31:20,210 --> 01:31:33,810
That's one way to write it. And then here would be the, uh, the p value for McNamara's test that would go along with, you know,

804
01:31:33,810 --> 01:31:38,730
is that significantly different from one sense analysis gives insufficient data suggests there's

805
01:31:38,730 --> 01:31:44,220
a different odds of radiologist two giving an absolute assessment compared to Radiologist one.

806
01:31:49,660 --> 01:31:55,050
That's also an appropriate statement. Because, you know.

807
01:31:56,530 --> 01:32:00,429
That does it. When you're looking at the differences in discordant cells,

808
01:32:00,430 --> 01:32:07,240
you're kind of also talking about differences in probabilities of saying absent versus present for one or the other.

809
01:32:11,690 --> 01:32:17,300
Okay. So the source code and output is is here are.

810
01:32:20,070 --> 01:32:28,080
I've got Cochran mantel Hansel as the option here to give that the McNamara's test here.

811
01:32:30,520 --> 01:32:36,549
And here is the way the R code and output would look for the same kind of stuff.

812
01:32:36,550 --> 01:32:41,080
And McNamara's test is nicely labeled here in the R output.

813
01:32:43,650 --> 01:32:52,710
Whereas here you have to know that this is the conditional odds ratio, confidence interval, and that this is the p value for McNamara's test.

814
01:32:55,380 --> 01:32:59,670
All right, so are. This.

815
01:32:59,670 --> 01:33:05,489
Our output gave you the p value for McNamara's test, but you get confidence intervals and and that sort of thing.

816
01:33:05,490 --> 01:33:10,010
You have to have code relying on the mental Hansel method as well.

817
01:33:10,020 --> 01:33:13,890
So here's how the data would be put in and coded in our.

818
01:33:14,980 --> 01:33:20,200
And you would have to kind of, again, kind of look through all this output to find what's relevant.

819
01:33:20,200 --> 01:33:23,439
Here is the here's the initial odds ratio. See how easy it is to find.

820
01:33:23,440 --> 01:33:27,670
If you do it by hand first, you can kind of find it and zoom in and then there's the confidence limits.

821
01:33:34,320 --> 01:33:37,320
Okay. And then I just wanted to remind you,

822
01:33:37,320 --> 01:33:44,070
this is kind of a fact that I'm a little bit in love with you knowing because it's addressing what

823
01:33:44,070 --> 01:33:50,190
bias you would see if you don't take into account the correct analysis that handles pairing.

824
01:33:50,610 --> 01:33:58,370
And so if you have paired match outcome studies and you analyze them using traditional methods for independent outcomes,

825
01:33:58,770 --> 01:34:03,570
so here is that it's kind of the same data,

826
01:34:04,080 --> 01:34:11,430
but you're acting like you have 200 outcomes and you've got radiologists 1 to 2 here and absent present here.

827
01:34:11,790 --> 01:34:24,270
And you, you know, you've done nothing to kind of even identify which of these counts are talking about the same person, right?

828
01:34:24,690 --> 01:34:30,750
So if you do, if you set up your data like this and you try to do an odds ratio,

829
01:34:31,350 --> 01:34:35,580
it is always going to be the case that that odds ratio is biased towards one.

830
01:34:36,270 --> 01:34:45,060
And so if you had done that for this last example, you know, you would have had an odds ratio instead of 0.5 when you took into account pairing,

831
01:34:45,360 --> 01:34:49,140
you have an odds ratio that's creeping up towards one no association.

832
01:34:49,410 --> 01:34:55,410
And so this is a this is a feature of you need to pick the right analysis or you're going to get

833
01:34:55,410 --> 01:35:01,230
an analysis result that's biased away from discovering an association you're interested in.

834
01:35:05,890 --> 01:35:12,640
All right. So Kappa statistic is also using data on independent binary outcomes.

835
01:35:12,640 --> 01:35:20,680
And so this is kind of the same set up where you have this could be reviewer one, reviewer two or whatever.

836
01:35:20,740 --> 01:35:23,890
Two different methods that you want to see how well they agree.

837
01:35:24,860 --> 01:35:30,520
And so here's method one is agrees that there is it could be a positive diagnosis.

838
01:35:30,740 --> 01:35:34,370
Some other criteria, maybe a tumor grade. They both agree on that tumor grade.

839
01:35:35,000 --> 01:35:41,090
And here is agreeing that negative prognosis or some other tumor grade.

840
01:35:41,510 --> 01:35:49,070
This is basically a general set up for how you want to know what's the probability of agreeing and if it's really good agreement.

841
01:35:49,730 --> 01:35:53,810
So what they're agreeing, whether it's positive or negative, is a in the cells.

842
01:35:54,230 --> 01:36:01,850
That's when they're agreeing. And the Kappa statistic isn't really distinguishing between the A and the D cells.

843
01:36:01,850 --> 01:36:12,600
It lumps all of these together as agreeing. And so the observed agreement is A-plus D over the total sample size for the table.

844
01:36:14,790 --> 01:36:20,159
And the Kappa statistic kind of takes into account, you know,

845
01:36:20,160 --> 01:36:25,560
the agreement that you would expect just from chance alone when you have the the margins in this table.

846
01:36:27,070 --> 01:36:32,920
Again. I don't want you to get too bogged down in the formulas here. This formula is useful because you can get it by hand.

847
01:36:32,920 --> 01:36:38,140
You know, I don't want you to get too bogged down. And in these other formulas, you're going to have code and output.

848
01:36:40,150 --> 01:36:43,209
But the cap is statistic. Just in general,

849
01:36:43,210 --> 01:36:47,440
it's trying to see whether that observed agreement is appreciably different

850
01:36:47,440 --> 01:36:52,150
from what you would expect from chance alone when you have these cell margins.

851
01:36:54,880 --> 01:37:02,380
And you'll get a number. And then there's this rule of thumb or cap of values that I had in the handout way back when.

852
01:37:02,770 --> 01:37:06,670
And guess what? For your homework that I gave you, you had a negative Kappa.

853
01:37:06,700 --> 01:37:15,520
It was that bad. The agreement between home FMV and in-person FMV was awful, at least based on our little mini data set.

854
01:37:16,420 --> 01:37:20,170
And you don't see it too often, but we got one in negative Kappa.

855
01:37:20,170 --> 01:37:24,489
Was that bad? Most of the time you're seeing agreement.

856
01:37:24,490 --> 01:37:29,530
That's a little better. That's the only time in my career I remember seeing a negative Kappa.

857
01:37:29,530 --> 01:37:34,629
Although I tease, I say my my in-laws. If you measured their agreement, it would be negative.

858
01:37:34,630 --> 01:37:39,540
But I've never actually collected that data. That was meant to be a joke. But yeah, you did see one like that.

859
01:37:43,410 --> 01:37:53,700
And so this agree statement and sass is going to and this test kappa is going to give you what you need for the campus statistics so it'll be there.

860
01:37:53,700 --> 01:37:58,130
And I think it's pretty well labeled in the South output.

861
01:37:58,140 --> 01:38:01,379
I'm not going to do anything silly like covering up standard output labels.

862
01:38:01,380 --> 01:38:02,520
These labels will all be there.

863
01:38:02,520 --> 01:38:11,790
You can find what you need pretty easily and but you might have some kind of sentence that you need to identify as interpreting the data correctly.

864
01:38:11,790 --> 01:38:15,660
So here's a sentence that's an example of interpreting this data correctly.

865
01:38:15,930 --> 01:38:20,879
The campus statistic indicates good agreement between radiologist assessments significantly higher

866
01:38:20,880 --> 01:38:26,400
than chance alone would indicate with the Kappa statistic confidence interval and P value.

867
01:38:28,350 --> 01:38:31,530
And I guess I took this this good from here.

868
01:38:32,580 --> 01:38:37,020
I don't see myself testing you on which adjective was correct.

869
01:38:38,340 --> 01:38:46,110
In fact, if I. Now that I'm thinking about it, if I make you pick as a manuscript worthy sentence,

870
01:38:46,110 --> 01:38:50,580
I'll copy this table for you so you don't have to remember all this and have it in your notes.

871
01:38:54,410 --> 01:38:59,330
And here's what it looks like for the hour code where the estimate is kind of in the middle here.

872
01:38:59,750 --> 01:39:04,370
We're looking at the unweighted cap, but we didn't really cover the weighted cap on this class here.

873
01:39:04,370 --> 01:39:11,840
It's the same because it's just a two by two table. So the estimates in the middle, the confidence limits and then the p value.

874
01:39:14,050 --> 01:39:17,440
For the Kappa statistic has a little bit more code, but it's very well labeled.

875
01:39:17,440 --> 01:39:21,129
Again, this is not my label. This is the output from R that you get.

876
01:39:21,130 --> 01:39:24,250
It's very well labeled that you're finding the P value for the Kappa.

877
01:39:27,500 --> 01:39:32,090
All right. So that was kind of the quick review on dependent binary outcomes.

878
01:39:32,090 --> 01:39:39,050
And we saw that later in the course we'll learn how to model those with regression for these kind of different settings.

879
01:39:40,760 --> 01:39:47,660
And so the rest of the handout is kind of reviewing logistic regression and all the little special things we learn how to do with it.

880
01:39:48,470 --> 01:39:58,310
So if we don't get to the end, that's okay. So the typical outcomes are yes versus no or diagnosis one versus diagnosis two.

881
01:39:58,310 --> 01:40:03,170
And the logistic regression kind of looks like this guy. I've only put one little covariate here.

882
01:40:03,890 --> 01:40:15,050
If you've got a binary covariate x one like this, you're actually doing an analysis that corresponds exactly to other ones that you've learned.

883
01:40:15,060 --> 01:40:22,550
So if you are looking at this analysis of beta one, when you've got a binary predictor here, a01 predictor here,

884
01:40:23,930 --> 01:40:34,490
that's that score test for this parameter is algebraically identical to either a two sample proportion test like comparing p one hat, p two hat.

885
01:40:35,540 --> 01:40:41,870
And that P value is also equivalent to the p value we get for a chi squared analysis for a two by two table.

886
01:40:42,680 --> 01:40:49,490
So that's the score test here. So we usually just interpret the wall test, not the score test or the likely ratio test.

887
01:40:49,500 --> 01:40:55,850
We just look at this wild test that's given in the output and that's going to be close to but not algebraically identical to these two.

888
01:40:56,630 --> 01:41:02,150
So the I'm a little bit in love with how these all these analyzes relate to one another.

889
01:41:02,420 --> 01:41:05,930
I know that about myself and I kind of want you to know it too.

890
01:41:05,960 --> 01:41:10,340
So I could see myself saying, You know what?

891
01:41:10,340 --> 01:41:12,110
Analysis corresponds to what?

892
01:41:12,830 --> 01:41:20,120
Logistic regression is a catch all for many of the analysis we've seen, and we've certainly seen me do that in your homework for meta analysis.

893
01:41:20,120 --> 01:41:27,200
Right. I had you read you made an analysis using logistic regression and say which one corresponds to the homogeneity test,

894
01:41:27,470 --> 01:41:33,410
which analysis corresponds to estimating the meta analysis overall study odds ratio?

895
01:41:33,410 --> 01:41:36,440
Right. So you've seen my brain do that.

896
01:41:38,880 --> 01:41:43,040
And that I haven't read the exam yet, but that's a good indication of how my brain works.

897
01:41:43,050 --> 01:41:48,550
I want you to know how to do these things in different ways. And how those work.

898
01:41:50,860 --> 01:41:53,050
All right. So here's our old Vitamin C example.

899
01:41:53,530 --> 01:41:58,870
I'm going to go through this quickly because this is the part that I feel like you really know well from earlier courses.

900
01:41:59,260 --> 01:42:03,730
So we have the two simple proportion test. You're not going to do this by hand.

901
01:42:03,740 --> 01:42:07,810
You will have code and output for all of these things.

902
01:42:08,320 --> 01:42:12,280
Pearson's Chi Square test. And then finally, just looking at this data.

903
01:42:13,350 --> 01:42:16,409
By hand getting results. You won't have to do anything by hand.

904
01:42:16,410 --> 01:42:24,510
And how the same stuff works with logistic regression, I'm going to show output saying it corresponds, but let's just pause on this slide.

905
01:42:24,540 --> 01:42:26,760
This is a very useful summary slide, I think.

906
01:42:28,560 --> 01:42:35,490
So remember in logistic regression, the mean of the zero one outcome is actually the probability of the outcome.

907
01:42:35,670 --> 01:42:39,450
So we are still modeling a mean in that sense or some function of the mean.

908
01:42:39,660 --> 01:42:47,700
We're logging the modeling, the mar, the log, the natural log of the mean of the zero one outcome over one minus the mean, it's zero one outcome.

909
01:42:48,480 --> 01:42:53,580
If we want to estimate P hat, there's a formula to get there, but there's also SAS code.

910
01:42:53,580 --> 01:43:02,040
If I had if I'm asking for estimates of P hat, I'll be asking for it in the code and having p heads spilled out for you to pick from.

911
01:43:04,110 --> 01:43:06,840
So if there's no interaction terms in the model.

912
01:43:08,680 --> 01:43:18,489
Then the event, the odds ratio for having the outcome equals one for a person with a coverage that's one unit higher versus not.

913
01:43:18,490 --> 01:43:23,440
One unit hires each of the beta half. That's if there's no interaction terms in the model.

914
01:43:23,770 --> 01:43:30,240
If there is an interaction term in the model. Then the odds ratio is the formula.

915
01:43:30,270 --> 01:43:36,710
It's not just each of the data. It's each of the data, plus some other stuff here.

916
01:43:36,720 --> 01:43:40,080
So I need you to be able to figure out what these formulas are.

917
01:43:41,150 --> 01:43:47,240
So an interaction between x, j and Coover Z in the model, the event odds ratio,

918
01:43:47,420 --> 01:43:51,800
you know, the odds of having one versus, you know, for having the one outcome.

919
01:43:52,100 --> 01:43:58,460
Comparing a person with cover it X plus one versus X is not just involving beta j anymore,

920
01:43:58,940 --> 01:44:04,010
but it's involving whatever their z value is times the interaction parameter.

921
01:44:04,460 --> 01:44:14,840
And there's algebra here for Y that is, you know, if you've got I know if you've been shown this in your earlier classes, but if somehow,

922
01:44:14,840 --> 01:44:23,329
some way you've avoided learning this for longer than like a quiz duration, you know,

923
01:44:23,330 --> 01:44:30,380
if you study for the quiz but immediately lost to the intuition, yeah, you're going to have to know this now.

924
01:44:30,950 --> 01:44:35,240
We're going to be using this for the rest of your very long and prestigious careers.

925
01:44:35,780 --> 01:44:39,829
So you need to know how to do these formulas and get these contrast statements correct.

926
01:44:39,830 --> 01:44:43,040
So here's just a reminder of how that algebra works.

927
01:44:43,550 --> 01:44:48,980
Here's the interaction term. And for the person with X plus one here, we're plugging X plus one.

928
01:44:48,980 --> 01:44:57,710
Everywhere we see that covariate. So we see that Cooper here and we see it here multiplied by this Z value, whatever their Z value happens to be.

929
01:44:58,820 --> 01:45:03,980
And if we have x jake with X, we are plugging in x everywhere we see that value.

930
01:45:03,990 --> 01:45:10,520
So X here, next here. And so we have this e algebra all the time.

931
01:45:10,520 --> 01:45:13,489
We're looking at the ratio of these and seeing what cancels.

932
01:45:13,490 --> 01:45:20,720
So what's a really quick way to figure out what cancels when you have ease and then a whole bunch of stuff in the power?

933
01:45:20,930 --> 01:45:30,020
Well, you just take the the power for the numerator, you know, all the beta stuff and you subtract out all the power for the bottom stuff.

934
01:45:31,360 --> 01:45:36,099
Anything that cancels through that subscript through that subtraction is going to go away.

935
01:45:36,100 --> 01:45:42,190
So maybe not is going to be canceled out with this one big 1x1 canceled out here.

936
01:45:42,970 --> 01:45:49,450
And then when we multiply this out, we're going to have a beta j x and a beta had j so the beta that J X cancels out

937
01:45:49,450 --> 01:45:53,499
with this and we have a lot of more cancelations until we get to this term,

938
01:45:53,500 --> 01:45:59,080
we're going to have a beta hat interaction, Z, times X and the beta interaction Z times, which is one.

939
01:46:00,080 --> 01:46:06,110
The Z times x part is going to cancel, but the beta hat times x times one will not.

940
01:46:06,890 --> 01:46:17,640
That's where this comes from. So this needs to be an ingrained skill now, not just like a cramming for the test skill.

941
01:46:17,970 --> 01:46:28,440
We need to lean on this kind of. Logic to get our science right and to have interesting, nuanced science in our papers and in our stories we give.

942
01:46:28,440 --> 01:46:30,800
It talks and posters and all that stuff.

943
01:46:34,700 --> 01:46:40,909
So the ratio of those to God odds, it's going to be a formula now and you're going to have because it involves more than one parameter,

944
01:46:40,910 --> 01:46:47,060
you're going to have to use contrast to estimate statements for different values of Z.

945
01:46:47,780 --> 01:46:58,399
You know what this odds ratio looks like? So this one doesn't have this particular doesn't have many interesting interactions.

946
01:46:58,400 --> 01:47:04,960
But I'm going to just blow through this code and output mainly because it's just the two by two table case.

947
01:47:05,000 --> 01:47:13,010
You can read that on your own. Oh, by the way, again, I made these nice pretty labels of, oh, this is an odds ratio in your exam.

948
01:47:13,010 --> 01:47:16,999
I'm not going to tell you if it's an odds ratio or a full change, that crutch is going away.

949
01:47:17,000 --> 01:47:20,590
You're going to have to know from context what it is. Right.

950
01:47:22,590 --> 01:47:27,970
Uh. Let's see, we. We're running out of time, so I'm trying to pick. Good stuff here.

951
01:47:31,560 --> 01:47:35,160
I think this is another one where you can review in the privacy of your own room.

952
01:47:35,160 --> 01:47:42,840
We saw this in lecture two. Yeah.

953
01:47:43,200 --> 01:47:47,790
This is all stuff we've kind of. It'll be good to read it on your own.

954
01:47:52,160 --> 01:47:57,860
This is sort of reminding us about, you know, mental hansell and how you do mental Hansel.

955
01:47:58,830 --> 01:48:02,670
At odds ratios due to logistic regression. So you've done that before.

956
01:48:02,670 --> 01:48:06,450
This is just looking at the meta analysis store. Again, you've you've done that before.

957
01:48:07,720 --> 01:48:12,970
All right. Here's. Here's a useful places to start just because, again, I'm in love with plots.

958
01:48:12,980 --> 01:48:17,350
I love funnel plots. I love you knowing what a funnel plot does, how to interpret it.

959
01:48:17,860 --> 01:48:23,600
So put a little circle on this slide and say Susan hasn't run the exam again, but she knows she's in love with this plot.

960
01:48:23,620 --> 01:48:34,600
So make sure you are able to view a funnel plot and interpret correctly whether there's some kind of bias issue with the main analysis with the poop.

961
01:48:36,930 --> 01:48:45,569
Uh, here's again just focusing on having odds ratios vary according to a value of a confounder,

962
01:48:45,570 --> 01:48:51,090
but this is really just repeated from the interaction slide we just talked about and just some.

963
01:48:52,310 --> 01:49:01,719
Notes about. You know, big red practices is a good investment of odds ratios and logistic regression and multiplicative terms and Poisson regression,

964
01:49:01,720 --> 01:49:06,640
which will require calculation of an appropriate Formula one and interaction terms are used

965
01:49:06,640 --> 01:49:14,590
practice this so so I kind of emphasize interactions blind terms know how how this works.

966
01:49:17,770 --> 01:49:21,090
Note some propensity score matching kind of. No.

967
01:49:22,690 --> 01:49:28,509
You know how what it's about why you do it, what are the outcomes?

968
01:49:28,510 --> 01:49:31,990
What are the predictors? You know, kind of just review that material.

969
01:49:32,890 --> 01:49:40,360
Diagnostic test formulas. I am in love with sensitivity, specificity, positive, negative, predictive value and RC curves.

970
01:49:40,660 --> 01:49:45,970
You had practice on your homework. I know this. I love this about the material for the course.

971
01:49:46,660 --> 01:49:48,940
So review this well.

972
01:49:50,700 --> 01:49:57,330
So for instance, I mean, I could totally see myself throwing up in a rosy curve with two curves and saying which one's the better biomarker?

973
01:49:58,170 --> 01:50:02,850
And asking questions like, Can you tell which threshold to use just based on the plot alone?

974
01:50:03,920 --> 01:50:07,850
You can't tell from the plot alone generally which threshold to use, by the way.

975
01:50:08,000 --> 01:50:12,680
You learn that from your homework. So I can see myself throwing up a plot and asking questions about it.

976
01:50:12,980 --> 01:50:18,850
I just know that about myself. Uh oh, boy.

977
01:50:18,850 --> 01:50:22,960
We're out of time. Multiple amputation again. Know what it is?

978
01:50:23,260 --> 01:50:31,050
Why? It's important. You know, I've kind of got some little bit of review here.

979
01:50:33,980 --> 01:50:40,370
So sorry about the quick review, but I did know that we would run out of time, so I at least saved it for the material you're most familiar with.

980
01:50:40,380 --> 01:50:45,980
I'm glad we did so much with account outcomes because that's hard stuff you're just learning to own.

981
01:50:46,970 --> 01:50:55,880
I can stay after if you have some burning questions that have been raised just now from blowing through this part of the handout.

982
01:50:57,560 --> 01:51:03,020
If you have questions between now and the quiz, please post them on Piazza.

983
01:51:03,920 --> 01:51:10,500
I will be traveling over the weekend, so the fastest way to get answers are going to be the gases.

984
01:51:10,520 --> 01:51:18,860
See them in addition to me. Don't send them to my email because you might not hear from me for a couple of days while I'm out of town on.

985
01:51:19,070 --> 01:51:24,350
I don't know how good Mackinac Island is with this simply the internet because I've never been there.

986
01:51:24,860 --> 01:51:32,569
But they don't have cars. I'm saying they don't have cars. So it might be bad technology, maybe not as good there.

987
01:51:32,570 --> 01:51:40,490
I'd have no idea. Anyway, good luck studying and I will see you.

988
01:51:40,490 --> 01:51:46,850
I have office hours Monday and Tuesday. I might see you then or otherwise. I won't see you on Wednesday because you're taking the quiz at all.

989
01:51:49,120 --> 01:51:50,530
Okay. That's it, right?

