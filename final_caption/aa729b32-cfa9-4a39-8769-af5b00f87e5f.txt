1
00:00:10,650 --> 00:00:20,240
Okay. Good morning, everyone. So it's the last optional lecture.

2
00:00:20,250 --> 00:00:27,330
So if you review it, you have seen that I posted four sets of practice problems.

3
00:00:29,370 --> 00:00:34,810
So I encourage you to try to solve them. Because that's.

4
00:00:36,490 --> 00:00:41,020
Can we prepare you for the problems on the final exam?

5
00:00:41,030 --> 00:00:50,830
So the final exam will be similar to midterm. So there will be theoretical questions from the lectures as well as some problems for a choice.

6
00:00:52,030 --> 00:00:57,820
So the only difference is that you will have extra credit instead of.

7
00:00:59,600 --> 00:01:03,830
A choice of points for the final.

8
00:01:04,130 --> 00:01:07,970
Now the solutions I will post this week for all the problems.

9
00:01:10,810 --> 00:01:14,110
But do try to solve that without looking anywhere.

10
00:01:17,710 --> 00:01:22,090
Ask me questions during the office hours. Okay.

11
00:01:22,090 --> 00:01:34,330
So last time. We basically looked at the classical proofs of the central theorem.

12
00:01:36,650 --> 00:01:40,910
The law of large numbers using characteristic functions.

13
00:01:46,740 --> 00:01:59,040
And the basic idea was, was a Taylor series keeping the first term of the characteristic function for the long lunch numbers and the second term,

14
00:02:00,120 --> 00:02:04,050
while making sure the first one is zero for the.

15
00:02:05,940 --> 00:02:18,030
The central theory is and the central fact that we relied on was the many theory that we had the two different scenarios.

16
00:02:18,030 --> 00:02:24,090
But that basically said the convergence calculus type convergence of characteristic functions is.

17
00:02:25,650 --> 00:02:30,570
Equivalent to wi convergence of random variables.

18
00:02:32,680 --> 00:02:39,450
That have those characteristic functions. Okay.

19
00:02:39,480 --> 00:02:50,340
So, uh, but the way we looked at those central limit theorems, we looked at them using a scalar random variable.

20
00:02:51,270 --> 00:03:00,260
So our x. Is as we go into the multivariate setting, is now a vector.

21
00:03:07,900 --> 00:03:22,770
Okay. And. And, uh, so we do have some.

22
00:03:24,660 --> 00:03:35,200
Is now a vector and we want some way to reduce the multivariate central limit theorem.

23
00:03:42,650 --> 00:03:46,460
Variants yields to univariate.

24
00:03:49,600 --> 00:03:53,680
Now, of course, this is not for free.

25
00:03:53,730 --> 00:03:56,840
So we would have some kind of exit.

26
00:03:57,210 --> 00:04:06,360
We need some kind of flexibility in the statement to be able to generalize it from a univariate case to multivariate,

27
00:04:06,360 --> 00:04:09,000
because it has a lot more degrees of freedom.

28
00:04:10,140 --> 00:04:19,080
So we need some kind of a uniform or functional statement in the univariate case to be able to play the multivariate things.

29
00:04:19,580 --> 00:04:23,910
And that statement is called the Kramer World Device.

30
00:04:26,570 --> 00:04:31,130
But before I formulated. So we have a sample.

31
00:04:33,600 --> 00:04:38,370
Simple random sample rate to have a random variables.

32
00:04:44,400 --> 00:04:50,250
It's where it goes from one to, let's say, an.

33
00:04:51,680 --> 00:04:55,040
And we'll go to infinity and the limit in case.

34
00:04:56,620 --> 00:05:04,870
Uh, the, um, the x, I read the variables, so it has components because it's a vector.

35
00:05:05,530 --> 00:05:12,730
So that's the first component. The second one and the end component.

36
00:05:13,060 --> 00:05:26,130
It's a column vector. Now the characteristic function of a vector, um, with, uh, t is an argument.

37
00:05:27,690 --> 00:05:31,050
So that's an expected value of E.

38
00:05:32,500 --> 00:05:35,590
So i t transpose times x.

39
00:05:40,210 --> 00:05:45,220
And this is on the complex played right because I have an I hear.

40
00:05:54,460 --> 00:06:05,130
K and T transpose times x is basically scalar product, so it's a sum of GK Times X.

41
00:06:08,680 --> 00:06:12,240
Okay. Right where? Excuse.

42
00:06:13,240 --> 00:06:16,960
So two things and I call it the vector.

43
00:06:19,270 --> 00:06:23,410
So it's the sum of the components of X with, uh, some coefficients, right?

44
00:06:23,900 --> 00:06:31,479
Um, and uh, it returns a number, uh, on the complex plane, which is a bivariate thing, essentially,

45
00:06:31,480 --> 00:06:39,580
right, with some rules on how you manipulate the real and the imaginary components of it.

46
00:06:40,840 --> 00:06:45,800
Okay. So in this setting. So the Kramer.

47
00:06:47,340 --> 00:06:54,960
We'll devise. It's given by the following theorem.

48
00:06:58,740 --> 00:07:04,230
Namely that I have convergence of x and weakly x.

49
00:07:05,100 --> 00:07:11,620
Both are vectors. Now. If and only if.

50
00:07:14,650 --> 00:07:23,680
He transposed times X and converges weekly to T transpose Times X.

51
00:07:25,050 --> 00:07:31,080
So these two things scalar is of course, because it's a scalar product.

52
00:07:33,120 --> 00:07:41,980
Yeah. Not just for the fixed somehow t, but for any t.

53
00:07:45,530 --> 00:07:52,070
You need to mean they are, uh, whatever the dimension of X is.

54
00:07:58,560 --> 00:08:05,980
Okay. So I need to separate the. Let's say M was the dimension of X, right?

55
00:08:07,010 --> 00:08:11,600
Because I don't want to confuse the size of the sample with the dimensions of the X Factor.

56
00:08:14,210 --> 00:08:18,710
And if it's am, then what would be part of our.

57
00:08:23,190 --> 00:08:33,960
Okay. So the functional part of the argument is that convergence is supposed to take place for any T, for any vector t.

58
00:08:37,630 --> 00:08:42,640
So that's a uniform statement. So it's actually many, many convergences in there, right.

59
00:08:43,060 --> 00:08:48,610
Because there are unaccountably many tees that are part of our.

60
00:08:54,370 --> 00:09:01,340
Okay. And, uh. Let's prove that thing.

61
00:09:02,270 --> 00:09:10,130
So it's not difficult. It's just a matter of putting brackets in the right place and let's manipulating their arguments.

62
00:09:10,820 --> 00:09:16,040
Let's say I call y t a variable that.

63
00:09:17,160 --> 00:09:21,600
Uh, isn't the right part of the theorem statement?

64
00:09:21,660 --> 00:09:28,800
Right. And I have y t I have y and that's the sub limit variable.

65
00:09:30,060 --> 00:09:33,440
So it's again the same t times x and.

66
00:09:35,560 --> 00:09:40,360
That basically converts a vector to a scalar using some decisions.

67
00:09:42,360 --> 00:09:50,330
Let's say I want to get the. Characteristic function of what it is.

68
00:09:50,500 --> 00:09:54,370
So I need a different argument because my t is already.

69
00:09:55,920 --> 00:10:02,520
Taken. So he was tall then by definition of a characteristic function.

70
00:10:02,700 --> 00:10:09,510
It's going to be the expectation that the title and the random variable.

71
00:10:12,580 --> 00:10:15,850
Okay. So that's univariate so far.

72
00:10:18,790 --> 00:10:22,880
Right. Then I can't recall what way it is.

73
00:10:23,750 --> 00:10:27,040
And I'm going to have to. I.

74
00:10:27,430 --> 00:10:32,320
How and why is she transpose times x?

75
00:10:37,240 --> 00:10:45,020
So he's a scaler. So these events tough.

76
00:10:47,440 --> 00:10:52,270
81 and so on. Right now tell times t.

77
00:10:54,540 --> 00:11:01,469
Transposed, right? It's a vector dial turns to one dial tends to turn.

78
00:11:01,470 --> 00:11:05,760
And so this kind of object three acts as a vector.

79
00:11:07,380 --> 00:11:12,510
When we do an inner product, we, we get a scalar.

80
00:11:13,360 --> 00:11:21,670
Terms there. And what I do here is I will put brackets.

81
00:11:22,870 --> 00:11:24,190
Like this. Right.

82
00:11:25,600 --> 00:11:34,930
And I noticed that the object on the right so this thing is the characteristic function and they evaluate evaluated and this argument,

83
00:11:35,440 --> 00:11:45,360
this vector argument. Characteristic function of X.

84
00:11:47,160 --> 00:11:50,670
If I knew that, I would times to.

85
00:11:59,140 --> 00:12:07,000
And so this is the key observation because I have a unique variance.

86
00:12:13,120 --> 00:12:21,339
Characteristic function on the left of the argument tell and it's basically equal to the

87
00:12:21,340 --> 00:12:30,320
multivariate characteristic function and the argument that is titled Times D now uh,

88
00:12:30,490 --> 00:12:36,129
by the definition and the construction of the characteristic function.

89
00:12:36,130 --> 00:12:40,690
So both tile in d or arbitrary.

90
00:12:44,330 --> 00:12:49,930
So in case of Tao, it's a scalar. It's a case of cheats, of actor.

91
00:12:52,570 --> 00:12:59,330
And so. Down times to ease an arbitrary victor.

92
00:13:10,500 --> 00:13:18,860
Right. So my lady theory works. In any case, be that univariate multivariate.

93
00:13:20,380 --> 00:13:25,690
So to build the correspondence between the univariate and the multivariate situation,

94
00:13:26,140 --> 00:13:33,760
I will just rely on this relationship I can frame because that's the core of this theorem.

95
00:13:40,580 --> 00:13:58,650
And by the Levy Theorem. So convergence of x and two x is equivalent to convergence of the characteristic functions.

96
00:14:00,120 --> 00:14:03,180
It's an arbitrary argument.

97
00:14:13,190 --> 00:14:17,750
Well, it is an arbitrary effect argument. So this is for energy.

98
00:14:22,340 --> 00:14:28,080
Then I also have by the. The quality.

99
00:14:30,220 --> 00:14:41,470
And this is equivalent because my left and right part of this convergent statement each are equal respectively to the parts of the star.

100
00:14:42,670 --> 00:14:50,350
So that would be the characteristic function of y and g of Tao.

101
00:14:53,110 --> 00:14:58,170
Building to fly. Variety of town.

102
00:15:01,220 --> 00:15:06,400
Right. And I'm actively using here the fact that, uh, uh,

103
00:15:06,650 --> 00:15:16,750
so tao te entitled times to arbitrary arguments because these are statements for a family when t is arbitrary, right?

104
00:15:16,790 --> 00:15:21,020
Not for specific t and same thing in all the other statements.

105
00:15:22,560 --> 00:15:26,360
And this is going to be for any time and to.

106
00:15:30,480 --> 00:15:35,520
Even though in this last statement he's considered as hicks, but it's still an arbitrary term.

107
00:15:36,120 --> 00:15:42,390
So that convergence is taking place for any T and for any of the above.

108
00:15:43,080 --> 00:15:54,390
And once I have that, I can use Levy the second time now on this type of statement.

109
00:15:54,690 --> 00:16:17,020
Right. Getting this is equivalent to y in t weekly to y t.

110
00:16:19,340 --> 00:16:28,910
Now for any tea. And this is basically what we intended to prove.

111
00:16:29,120 --> 00:16:45,200
So that's the end of the statement. Now this simple observation actually allows us to prove the multivariable central theorem.

112
00:16:46,370 --> 00:16:50,260
But the way. So note.

113
00:16:53,410 --> 00:16:56,380
How to use the cream of old device.

114
00:17:07,310 --> 00:17:15,620
Now the immediate example of its use will be the proof of the multivariate sensor limit theorem because we'll use the Craveable device to.

115
00:17:17,380 --> 00:17:22,060
Reduce it to the univariate KLT that we have already proved.

116
00:17:23,410 --> 00:17:29,620
Right? But there are other, uh, central theorems that you potentially may encounter.

117
00:17:30,630 --> 00:17:38,950
And so the general way to deal with it, uh, would be the following.

118
00:17:38,950 --> 00:17:43,150
So we need, uh, so first.

119
00:17:46,960 --> 00:17:52,010
Instead of first. I'll just put out one. Right.

120
00:17:52,190 --> 00:17:58,879
So we have x m, right. So that's an estimate or a sample either way.

121
00:17:58,880 --> 00:18:05,930
Right. So that we want to find. The distribution of asymptotic distribution of.

122
00:18:06,470 --> 00:18:11,810
So we form, uh, t transpose times except.

123
00:18:16,360 --> 00:18:19,870
And then the limiting random variable of that thing.

124
00:18:22,660 --> 00:18:27,670
We can find it and it's going to be two transpose times X, right?

125
00:18:31,360 --> 00:18:38,439
There is a second thing, uh, as we find the limiting random variables.

126
00:18:38,440 --> 00:18:41,890
So this is going to be, well, some distributions, right?

127
00:18:42,340 --> 00:18:47,650
With some mean, let's say zero, right. And some variance sigma.

128
00:18:51,180 --> 00:19:01,650
And we need to convert it to a statement about x not not about the T transposed because we need this including distribution of x.

129
00:19:03,290 --> 00:19:09,259
So here. Uh. Well, arbitrary fax has been zero.

130
00:19:09,260 --> 00:19:10,690
That's. That's still a zero.

131
00:19:10,700 --> 00:19:20,329
But we need to recognize the variance of X in these terms that we get in the central limit theorem applied to this variable.

132
00:19:20,330 --> 00:19:25,460
Right? So we need to get rid of the T and have the variance of x.

133
00:19:26,270 --> 00:19:29,960
So how does the sigma look like?

134
00:19:30,320 --> 00:19:37,760
Well, so the covariance of t transpose times x looks like a sandwich.

135
00:19:39,410 --> 00:19:49,920
Um. So that's. The transposed.

136
00:19:50,690 --> 00:19:56,200
Times the variance of X. Times.

137
00:20:00,790 --> 00:20:06,280
Right. And so here you need to recognize this is the sigma.

138
00:20:13,100 --> 00:20:22,550
But that lies the. How do you call the middle part of the segment, which I don't know, currently, maybe.

139
00:20:26,200 --> 00:20:32,900
Maybe the most important part of the sandwich. It's. They need.

140
00:20:43,960 --> 00:20:51,710
This is. Covariance of x and the and state.

141
00:20:55,370 --> 00:21:02,780
The same thing distribution. Yes.

142
00:21:04,480 --> 00:21:10,230
Whatever the distribution is. Zero. Oh, that's right.

143
00:21:10,240 --> 00:21:14,959
For. The. Statistic.

144
00:21:14,960 --> 00:21:20,120
Right. So drop the cheese in the sandwich. Just focus on the middle part.

145
00:21:26,320 --> 00:21:34,660
Okay. So with that in mind, so a first example of the use of the Kramer world is the multivariate.

146
00:21:41,870 --> 00:21:54,120
Still classic gold. SEAL team.

147
00:22:02,050 --> 00:22:10,090
So I have a simple it's I, uh, from one to an idea.

148
00:22:12,310 --> 00:22:22,690
Random factors. Such that they have existing expectations.

149
00:22:35,440 --> 00:22:39,310
Just the notations. Usual notation for expectation.

150
00:22:40,770 --> 00:22:45,300
And we have the variants of parents.

151
00:22:47,650 --> 00:22:51,360
That's why. Variants of X.

152
00:22:52,790 --> 00:22:58,300
That's the expectation of. The.

153
00:22:59,260 --> 00:23:03,600
That's one. The center version of the random variable.

154
00:23:05,180 --> 00:23:10,000
Times. It's transposed.

155
00:23:10,240 --> 00:23:14,170
Right. So that thing is a matrix because it's an outer product.

156
00:23:14,270 --> 00:23:17,850
Right. That's. Row Times column.

157
00:23:23,050 --> 00:23:32,830
Not all the times. Column, column times are. Small.

158
00:23:34,100 --> 00:23:44,620
This is a matrix. And we need it to exist.

159
00:23:44,620 --> 00:23:49,670
Right? Otherwise I don't have a statement. For the theorem.

160
00:23:49,830 --> 00:23:55,980
So then. We have the square or the van.

161
00:23:59,120 --> 00:24:04,210
Then an empirical expectation of X right for one over and.

162
00:24:09,140 --> 00:24:15,970
So. Some of them.

163
00:24:17,570 --> 00:24:22,370
What is it? So analyze the size of my sample for it. So some of it's I.

164
00:24:24,750 --> 00:24:28,560
My last meal. So this is getting good weekly.

165
00:24:28,830 --> 00:24:37,800
So because all the variables are centered. So we'll have a normal distribution with zero mean and with covariance of.

166
00:24:39,030 --> 00:24:47,490
Fix. Okay.

167
00:24:47,510 --> 00:24:57,920
Prove. So I'm looking at a multivariate weak convergence statement.

168
00:24:58,640 --> 00:25:11,220
Right, that I need to prove. I know I have a univariate statement just like this one for every component of X by the univariate code.

169
00:25:11,330 --> 00:25:17,299
You know, my, I'm just following this recipe, right?

170
00:25:17,300 --> 00:25:29,090
So for the univariate collection of random variables by multiplying them, um, as an in the product with some coefficients.

171
00:25:30,440 --> 00:25:39,050
Uh, they and uh, applying the, uh, univariate central limit theorem to this product,

172
00:25:39,800 --> 00:25:46,790
recognizing the middle part of it, dropping the everything in the sandwich except the middle part.

173
00:25:47,510 --> 00:25:54,500
And that would be my covariance by the Kramer device in this inverted distribution.

174
00:25:54,920 --> 00:26:03,750
So let's make that happen. So we will do tee times.

175
00:26:04,950 --> 00:26:08,970
And the variable of interest is this one, right?

176
00:26:09,600 --> 00:26:14,419
Because I want to show that this one there somewhere. Times.

177
00:26:14,420 --> 00:26:20,430
This one. And let's see what it is.

178
00:26:22,420 --> 00:26:29,500
So I have to transposed. Uh, so I have an implant or square or the van.

179
00:26:30,880 --> 00:26:34,170
So I have some. I want to and.

180
00:26:36,290 --> 00:26:40,400
I have the sense that random variables inside minus male.

181
00:26:46,700 --> 00:26:54,200
So now it's all linear. So my t transpose now gets inside.

182
00:26:58,670 --> 00:27:10,190
So I from one talk and then inside I have the teacher and spouse driving right to the sty and to the mirror.

183
00:27:15,280 --> 00:27:18,570
So this goes by the univariate Keelty.

184
00:27:25,660 --> 00:27:29,740
As I have arranged for scalar and the variables in brackets.

185
00:27:31,880 --> 00:27:44,030
That goes to normal distribution with min zero and variance of g transpose times x.

186
00:27:50,090 --> 00:27:55,120
No my variance or covariance for it.

187
00:27:55,130 --> 00:28:01,360
That's the same thing in the multi variants. So in this case, it's still variance.

188
00:28:01,480 --> 00:28:10,090
It's a variance. To transpose times X turns into T transpose times the variance of x.

189
00:28:12,040 --> 00:28:18,550
Times. T Right. So I recognize that this is the sandwich and this is the middle part of it.

190
00:28:19,600 --> 00:28:30,310
And by Cramer Walz. My multivariate convergence statement is.

191
00:28:32,250 --> 00:28:38,810
That the original random variable that I multiplied by t.

192
00:28:40,450 --> 00:28:44,140
Transposed. So that's what it was.

193
00:28:44,500 --> 00:28:49,090
Right. And Bills weekly to.

194
00:28:52,560 --> 00:29:03,930
Multivariate normal with mean zero in covariance being the middle part of the send, which I get in the course of my Kramer world arguments.

195
00:29:05,310 --> 00:29:19,080
So that's the two variants of X. And this is what we intended to show.

196
00:29:19,150 --> 00:29:35,830
So that's the. And of course. There are other versions of this central limit theorem.

197
00:29:38,030 --> 00:29:43,460
Not just the multivariate, but, uh, relaxing some of the classical assumptions.

198
00:29:44,570 --> 00:29:54,890
And what we will formulate in this course is two theorems that reflects the assumption that exercise are identically distributed.

199
00:29:58,090 --> 00:30:01,120
So we want to consider CEO G.

200
00:30:03,540 --> 00:30:11,960
For generally. Not identically distributed.

201
00:30:22,230 --> 00:30:32,810
It's still independence. The hipsters.

202
00:30:36,220 --> 00:30:46,660
When doctors. We will later can see the examples.

203
00:30:46,810 --> 00:30:52,270
There's also examples in the homework on the use of such theorems.

204
00:30:54,150 --> 00:30:58,470
Not the homework. The two sets of problems. I forgot to tell you that, uh.

205
00:30:59,380 --> 00:31:06,340
So because we have four sets of problems ahead of us, I decided not to give any more graded homeworks.

206
00:31:08,260 --> 00:31:14,470
But I still call those practice problems kind of homeworks and solutions.

207
00:31:15,880 --> 00:31:21,960
For. Later on. And the first one is Lindenberg.

208
00:31:26,100 --> 00:31:41,180
SEAL team. Now usually the there are a number of those sealed tees bearing the name of whoever came up with the right conditions.

209
00:31:41,270 --> 00:31:44,740
Right. So there's the Lindbergh version.

210
00:31:44,810 --> 00:31:48,530
There's the fuller version.

211
00:31:48,540 --> 00:32:00,350
So if you remember one of the recommended books for now and throughout your 76 Life is Fellas, two volumes.

212
00:32:01,820 --> 00:32:05,090
And then there's this level off and there are some others as well.

213
00:32:17,270 --> 00:32:24,170
So we'll start with STI. Uh, from one to an independent.

214
00:32:24,800 --> 00:32:28,700
Random factors? Not necessarily.

215
00:32:32,760 --> 00:32:46,140
Identically distributed. So now, uh, if you look at how the, uh, serum was structured.

216
00:32:47,980 --> 00:32:55,590
As far as normalization condition girls. Right.

217
00:32:56,130 --> 00:33:00,000
So where there's this square or the van actually come from.

218
00:33:01,590 --> 00:33:08,879
That comes from a consideration that the variance of this random variable is finite, right?

219
00:33:08,880 --> 00:33:16,140
So we want it to stabilize the variance. That's why the constant here is sometimes called variance stabilizing.

220
00:33:17,230 --> 00:33:22,330
Uh, normalization. Right. Uh, and sometimes more generally, variance.

221
00:33:22,330 --> 00:33:28,200
Stabilizing transformation. In this particular case.

222
00:33:29,610 --> 00:33:41,009
And by the way. Yeah. So in this particular case, my exercise where I had random variables.

223
00:33:41,010 --> 00:33:45,870
So one normalization for everything worked perfectly.

224
00:33:46,560 --> 00:33:50,390
But if I have XYZ, not necessarily identically distributed.

225
00:33:50,410 --> 00:33:56,610
So that would mean that they have their own variances. And I may want to.

226
00:33:57,180 --> 00:34:00,450
So one constant for all the exercise may not work.

227
00:34:02,030 --> 00:34:02,390
Right.

228
00:34:02,400 --> 00:34:16,730
So I may want to normalize exercise individually and have a exercise specific constant so not to work through to keep the generality of that argument.

229
00:34:17,690 --> 00:34:27,500
Uh, people often say that suppose x i's are already normalized so that the variance, so that I can use a lot of large numbers on the variance.

230
00:34:29,510 --> 00:34:32,810
And then there is no constant here. Right.

231
00:34:33,050 --> 00:34:39,590
So one other thing I can say that well they may have different uh, uh, expectations.

232
00:34:39,590 --> 00:34:43,400
So one mu will not work for, for this whole term.

233
00:34:44,000 --> 00:34:52,399
Right then. So again, to keep the generality, we can say that suppose they are all without loss of generality centered.

234
00:34:52,400 --> 00:34:55,879
So I subtracted from every exercise.

235
00:34:55,880 --> 00:35:06,820
Right. So this is to say without loss.

236
00:35:10,140 --> 00:35:21,570
Reality. SEAL the mean values of x rays and zeros.

237
00:35:24,820 --> 00:35:28,840
And, uh, they are already.

238
00:35:35,680 --> 00:35:41,830
The normalized. So they had.

239
00:35:45,590 --> 00:35:48,800
The. One of the largest numbers.

240
00:35:51,830 --> 00:36:16,010
Works for the various. So that that leaves some work, uh, uh, basically open for you if you're using the,

241
00:36:16,050 --> 00:36:23,480
in the work, if you have, uh, the random variables, independent samples, but uh,

242
00:36:23,990 --> 00:36:30,620
when the variables are not necessarily identically distributed, so that leaves some work for you,

243
00:36:30,620 --> 00:36:36,770
it's easy to, to make them centered, but you need to figure out how to normalize them so that.

244
00:36:38,530 --> 00:36:42,540
You can apply the Lindbergh. Fear so.

245
00:36:42,550 --> 00:36:48,550
And by the way, one of the situations where you might want to use it.

246
00:36:51,770 --> 00:37:01,100
Just make a note. So if you have a regression of Y all into the area, it's x.

247
00:37:01,970 --> 00:37:06,750
So this is a response. This is covariance.

248
00:37:11,790 --> 00:37:16,720
You might think of a model that generates from the joint distribution.

249
00:37:16,950 --> 00:37:21,810
Why? I am excited by it and that would be simple.

250
00:37:22,560 --> 00:37:30,480
And in this context that you can say that I have a regression with the random variables.

251
00:37:30,630 --> 00:37:37,740
Right. But you can also say that, um, so the fix is clearly it's fixed, for example,

252
00:37:37,740 --> 00:37:42,420
by design of experimental or just because you want to consider conditional inference.

253
00:37:47,350 --> 00:37:55,780
So then, uh, your y given x are not identically distributed.

254
00:37:59,640 --> 00:38:08,270
Right, because every subject has. A specific vector of covariates and it by definition modifies the distribution of the Y.

255
00:38:09,200 --> 00:38:16,549
So this would be one of the cases where you may want to consider Lindeberg and that includes linear regression.

256
00:38:16,550 --> 00:38:29,120
So if we have the time, we'll go over an example. Okay.

257
00:38:29,140 --> 00:38:33,990
So now I'm talking about the Lindbergh Theorem, right?

258
00:38:34,000 --> 00:38:37,030
So you need to start formulating the conditions.

259
00:38:38,830 --> 00:38:44,680
So we have limited the conditions. This is the one that gave theater in its name.

260
00:38:50,580 --> 00:39:03,300
And that is for any absolute greater than zero sum II from one to and that's over the sample values expectations.

261
00:39:04,740 --> 00:39:09,300
Of normal size squared.

262
00:39:11,600 --> 00:39:15,910
Over the area where this norm.

263
00:39:16,070 --> 00:39:19,370
So these are vectors, right? So this is the Euclidean norm.

264
00:39:21,150 --> 00:39:24,570
So some of the squares of the components.

265
00:39:26,670 --> 00:39:30,810
Over the area where it is known is greater than epsilon.

266
00:39:31,500 --> 00:39:36,510
So this is supposed to go to zero is and goes to infinity.

267
00:39:38,010 --> 00:39:42,540
Right. So now that exercise is centered, right.

268
00:39:43,110 --> 00:39:47,730
So expectations of these guys is a variance, right?

269
00:39:49,270 --> 00:39:57,549
So this condition has a sense of limiting the deviations of X as a random variable.

270
00:39:57,550 --> 00:40:01,270
So it's conditional on the variance, essentially.

271
00:40:06,670 --> 00:40:13,660
So it limits. The only areas.

272
00:40:16,470 --> 00:40:24,690
The variances is there are many random variables here with different distributions.

273
00:40:29,230 --> 00:40:33,820
So you don't want it to be too variable for the central theorem to work?

274
00:40:34,390 --> 00:40:44,230
That's a general condition, right? And then the second is that you have a log of large numbers for covariance.

275
00:40:46,710 --> 00:40:54,590
Works. Right. So you have some of the variances of.

276
00:40:56,750 --> 00:41:00,620
Now, again, so normalization constant is already included.

277
00:41:00,620 --> 00:41:03,860
So there's no one over and before the sum.

278
00:41:06,050 --> 00:41:12,410
So they had girls as girls to infinity to some kind of a sigma.

279
00:41:13,640 --> 00:41:31,500
This is a career in this matrix. So here I have a note that because it's already normalized.

280
00:41:39,260 --> 00:41:52,520
There's no. This.

281
00:41:59,730 --> 00:42:09,040
Okay. And then we finally get to. The main statement of the theorem again without the one over.

282
00:42:09,040 --> 00:42:23,110
And so if I sum up the exercise and this would go as in goes to infinity to a normal zero sigma.

283
00:42:25,560 --> 00:42:36,050
Random variable, and this is a multivariate statement. And so there's non-classical theorems.

284
00:42:36,590 --> 00:42:42,980
They go without proof, even though it's not a very difficult thing that we just don't have time for it.

285
00:42:45,830 --> 00:42:53,120
Okay. So then there is another note that Linda Burke.

286
00:42:56,250 --> 00:43:03,450
In addition. So where is it?

287
00:43:03,990 --> 00:43:09,440
Like this? Right. That's the one here. It's not that easy to verify.

288
00:43:09,450 --> 00:43:17,580
Right? So we need to consider some integrals over areas where normal X is greater than epsilon.

289
00:43:17,770 --> 00:43:21,420
Very clear what this is, right? So we need a simpler condition.

290
00:43:25,170 --> 00:43:35,970
It's not so easy to verify. In practice.

291
00:43:39,940 --> 00:43:43,110
And so people were looking for simpler conditions.

292
00:43:54,100 --> 00:44:01,120
So it's. And there's no free lunch, right?

293
00:44:01,150 --> 00:44:05,950
If you want a simpler condition, then that usually means it's going to be a stronger one.

294
00:44:07,600 --> 00:44:15,550
And it is the case here with the example of such condition that is left for north.

295
00:44:23,220 --> 00:44:35,130
So this is a guy most known for the optimal control theory, but he apparently left his imprint in central limit theorems as well.

296
00:44:39,120 --> 00:44:50,249
So we will formulate a test condition of the couple of notes and then we'll show that if I have enough conditions satisfied,

297
00:44:50,250 --> 00:44:58,740
then I this will yield the condition as well so that the life is stronger condition than.

298
00:45:22,670 --> 00:45:26,270
Okay. And the theorem is exactly the same, except for that condition.

299
00:45:30,070 --> 00:45:37,450
So. Same.

300
00:45:39,520 --> 00:45:52,190
Statements. In the book.

301
00:45:56,630 --> 00:46:03,540
Know to. Except for. One.

302
00:46:10,080 --> 00:46:16,510
And so the. One star of the show would be.

303
00:46:25,170 --> 00:46:38,020
Same context, same everything. So we have is that by definition is some, uh, from one to and.

304
00:46:39,890 --> 00:46:43,770
Expected value. Normal.

305
00:46:44,170 --> 00:46:50,370
I. They come to power as an expectation of this whole thing.

306
00:46:51,390 --> 00:46:59,880
Where else is greater than to. And the condition is that there exists an ass.

307
00:47:01,760 --> 00:47:06,140
Such that. Oh, yes.

308
00:47:06,480 --> 00:47:09,480
Those two zero is paying the bills to the.

309
00:47:28,010 --> 00:47:33,950
No, actually, we can say that is greater than zero in the state of the theorem.

310
00:47:35,480 --> 00:47:40,220
We just need to find the ask large enough so that this works.

311
00:47:42,640 --> 00:47:45,790
And it's usually. Yes, it's greater than to.

312
00:47:49,000 --> 00:47:53,230
So we essentially what happened is that we traded.

313
00:48:03,480 --> 00:48:13,840
Sum of expectations. That's where it's over where X is great on the map.

314
00:48:13,840 --> 00:48:20,020
So things like this. Uh, we traded it for a simple expression.

315
00:48:20,090 --> 00:48:24,700
So these are simply. Well, it's not, uh.

316
00:48:26,280 --> 00:48:30,270
So I hesitate to call these moments because there's not an infant jersey.

317
00:48:30,450 --> 00:48:42,020
Right. Um, and it's, it's a similar condition, similar to moments, uh, rather than a complicated integral with epsilon involved.

318
00:48:51,210 --> 00:48:55,740
Okay. Um. Steve.

319
00:48:57,920 --> 00:49:01,480
So. And the proof.

320
00:49:05,580 --> 00:49:12,270
Is essentially of the facts that implies Lindbergh does once that's done.

321
00:49:12,780 --> 00:49:20,360
I'm invoking the Lindbergh. Uh, Central Limit theorem and have the Central Limit Theorem statements.

322
00:49:26,870 --> 00:49:36,340
Lies. This is what we need to prove right now.

323
00:49:36,910 --> 00:49:41,410
The thing, uh, in the Lindbergh condition.

324
00:49:41,770 --> 00:49:46,300
So this thing. I'm going to apply some inequalities, right?

325
00:49:46,630 --> 00:49:50,890
Because this is an integral of some function.

326
00:49:51,040 --> 00:49:54,150
The normal size square over this area.

327
00:49:54,160 --> 00:50:03,850
Right. And I'm going to exploit this area to, uh, uh, uh, make an inequality that major rises this integral.

328
00:50:03,850 --> 00:50:12,200
Right. So. I could, of course, immediately put Epsilon Square instead of x y square.

329
00:50:12,210 --> 00:50:24,060
But that's not enough. So I have for the area of integration.

330
00:50:30,000 --> 00:50:35,430
So that's done over X greater than epsilon.

331
00:50:35,620 --> 00:50:45,479
Right. In the lindenberg. Now from there I can write.

332
00:50:45,480 --> 00:50:51,540
It is normal that I divided by epsilon is greater than one.

333
00:50:54,900 --> 00:51:01,080
Then I will take. Both sides.

334
00:51:04,950 --> 00:51:12,610
So the power. Yes.

335
00:51:12,650 --> 00:51:21,880
Minus two. And they want this power to be greater than zero.

336
00:51:22,750 --> 00:51:27,610
So this is where, uh, uh. S great alarm too came from.

337
00:51:27,680 --> 00:51:36,730
Right. Remember earlier. So then this would give me.

338
00:51:37,450 --> 00:51:50,679
It's I. So the power is minus two over epsilon to the power of s minus two, and it remains to be greater than one.

339
00:51:50,680 --> 00:51:53,890
Because my power is not negative. It's positive.

340
00:51:57,720 --> 00:52:05,060
Right. Then from there. I will multiply both sides.

341
00:52:13,470 --> 00:52:17,750
Science is the norm of exercise squared.

342
00:52:19,550 --> 00:52:31,460
This will give me an x squared is less than norm of x squared times.

343
00:52:32,240 --> 00:52:46,850
No it is minus two divided by epsilon as minus two and that's part of the minus two and the excise squared cancels out.

344
00:52:46,850 --> 00:52:52,790
They just have normal like side to s. All were absent.

345
00:52:54,240 --> 00:53:07,820
Yes. Miles two. So finally I can use this inequality in the lindeberg conditions.

346
00:53:09,410 --> 00:53:14,330
So we'll have some cry from one trip and expect an.

347
00:53:15,790 --> 00:53:20,380
Of its, uh, squared over the area.

348
00:53:20,590 --> 00:53:24,100
It's, uh, greater than epsilon.

349
00:53:27,230 --> 00:53:31,490
This is less of equal. Two.

350
00:53:32,930 --> 00:53:38,720
So instead of x y squared, I can substitute this guy.

351
00:53:38,870 --> 00:53:46,690
Right. And once I do that, I can drop the integral over a specific area.

352
00:53:46,720 --> 00:54:01,080
Make it into a level where. So that would leave me with some, uh, if I want an expected value.

353
00:54:03,020 --> 00:54:08,910
I. So this. Divided by.

354
00:54:10,580 --> 00:54:25,430
And so it's still. This is so I notice that what I have in the numerator is the less right.

355
00:54:25,530 --> 00:54:33,000
And that's how it was defined in the condition of a theory was that it last goes to zero and goes to infinity.

356
00:54:38,230 --> 00:54:42,520
This is a less over. Yeah. So, yes, minus two.

357
00:54:43,240 --> 00:54:47,760
And this goes to zero as time goes to 20.

358
00:54:50,530 --> 00:55:12,430
By the condition of the serum. And so then.

359
00:55:13,260 --> 00:55:19,000
Then the condition is satisfied. So by the Lindbergh.

360
00:55:25,400 --> 00:55:34,150
You're. We have.

361
00:55:36,560 --> 00:55:39,830
We multilayer in statement, just like the Lindbergh Theorem, right?

362
00:55:39,830 --> 00:55:46,180
So that's the end of proof. Okay.

363
00:55:46,340 --> 00:55:53,420
So let's have a five minute break. Talk a little bit more after the break about it.

364
01:01:29,760 --> 01:01:39,480
Okay. Shall we continue? So just, uh, the fact that I want to make as a note.

365
01:01:44,340 --> 01:02:02,560
Probability which he. Functions.

366
01:02:10,910 --> 01:02:15,380
That may sometimes be a useful tool of, uh, major rise in something.

367
01:02:18,490 --> 01:02:21,520
That says that the function.

368
01:02:23,440 --> 01:02:27,360
I don't know. Let's say if. Alpha.

369
01:02:29,030 --> 01:02:38,819
It has the form of expected value. Absolute value of x taken on for the whole thing.

370
01:02:38,820 --> 01:02:51,720
Taken to one over. So you may recognize the functional norm here.

371
01:02:54,090 --> 01:03:01,140
Well, it's not so important. So it's a particular case of a norm and the quality, if you will.

372
01:03:03,620 --> 01:03:07,700
So is an increasing function.

373
01:03:18,450 --> 01:03:23,350
Also. When all of this non-negative.

374
01:03:39,870 --> 01:03:43,500
Okay. Let's prove it.

375
01:03:48,510 --> 01:03:55,770
So we have that. Anything taken to the power of our is a convex function.

376
01:04:02,850 --> 01:04:10,140
When R is greater equal to one. So if you exclude one, that would be a strictly convex.

377
01:04:12,750 --> 01:04:18,840
Right. I said if R is less than one, you are taking root of some kind.

378
01:04:18,840 --> 01:04:25,550
So roots are like this. So that would be a concave function.

379
01:04:25,850 --> 01:04:29,540
Um, but so convex is one like this.

380
01:04:30,050 --> 01:04:36,770
And that starts from one where it's just the line to power is larger than one.

381
01:04:38,730 --> 01:04:58,560
And we have JOHNSON Inequality. The sense that there is a systematic bias for either convex functions in the case of convex functions.

382
01:05:00,430 --> 01:05:10,240
So the expected value of the function is actually greater equal than a function of the expected to value.

383
01:05:15,980 --> 01:05:18,980
The function in this case is something taken to our.

384
01:05:23,290 --> 01:05:27,910
Okay. So then I'm going to take a bunch of.

385
01:05:31,240 --> 01:05:39,130
Variable notation here. Right. So I'll take some data that's greater than Alpha.

386
01:05:42,010 --> 01:05:58,540
I'll take this one to be taken to the power of for the arguments and I'll take are equal to the divided by alpha.

387
01:05:58,570 --> 01:06:08,590
So this is all simultaneous. And having taken that, I'll plug that into the Jensen and the quality.

388
01:06:12,390 --> 01:06:15,600
So what do I have after that?

389
01:06:15,690 --> 01:06:26,310
So I have expected the value of now, instead of the absolute value of the dollar that I have asked, to hold further to what my dog is.

390
01:06:27,530 --> 01:06:34,790
The and this is taken to power of r r is beta divided by alpha.

391
01:06:36,850 --> 01:06:45,469
Now I have these grades equal. So my r is made, uh, divided by alpha inside.

392
01:06:45,470 --> 01:06:51,800
I have an expectation of. In the end, my thought is it's time for.

393
01:06:53,850 --> 01:06:58,650
This is what I have. Now, of course, also cancels out from the left part.

394
01:07:00,620 --> 01:07:06,380
Then I have expected value. Oh, yes.

395
01:07:07,490 --> 01:07:18,740
Taken to be to. And so I can also take one over the.

396
01:07:26,440 --> 01:07:31,920
Tate. Of both spots.

397
01:07:38,340 --> 01:07:45,440
This is going to be great. Equal. Experts that value.

398
01:07:47,070 --> 01:07:53,680
It's also. And Beethoven of all time times.

399
01:07:53,680 --> 01:07:57,400
One of the Beatles. So that would be one of our favorite.

400
01:07:59,030 --> 01:08:03,260
I started with just, uh, better be greater.

401
01:08:04,040 --> 01:08:16,490
And then Alpha and this is basically showing that if I take a greater argument of my function F, then I get something greater.

402
01:08:20,880 --> 01:08:31,020
And the prove. That's just the not useful in the quality.

403
01:08:35,850 --> 01:08:45,150
Do you tell them off then? So we talked about uh, uh, normalization and centering random variables.

404
01:08:45,170 --> 01:08:51,930
Ex That was kind of kind of behind the curtain because we already started formulating the,

405
01:08:52,590 --> 01:08:59,070
uh, those non-classical theorems, uh, with being already normalized.

406
01:09:00,030 --> 01:09:06,630
Um, so we need some practical argument on how that happens.

407
01:09:08,040 --> 01:09:14,230
So node. It's absorbing.

408
01:09:18,600 --> 01:09:26,830
Is Asian. Variance.

409
01:09:26,950 --> 01:09:43,960
Stabilizing for stabilizing variance. So let's say I have a statistic.

410
01:09:49,740 --> 01:09:56,650
But I'm considering G and. That's some of why I.

411
01:09:58,840 --> 01:10:01,900
And so that was why I.

412
01:10:04,300 --> 01:10:08,740
Once again without loss of generality.

413
01:10:16,470 --> 01:10:20,340
The expected values of y are being zero, right?

414
01:10:21,320 --> 01:10:36,990
So they are center. So then suppose I have.

415
01:10:38,670 --> 01:10:42,450
The consistent estimate for the variance.

416
01:10:59,750 --> 01:11:03,350
The estimate is yes and squared.

417
01:11:04,940 --> 01:11:08,920
That's the variance. Of tea and.

418
01:11:10,960 --> 01:11:15,280
And it's a some because of independence.

419
01:11:16,900 --> 01:11:21,560
So why are. Independent.

420
01:11:23,210 --> 01:11:27,830
That's not. Identically distributed.

421
01:11:27,950 --> 01:11:38,120
So that's our case for the theorems. So the variance of independent the variables is going to be some of the variances of.

422
01:11:39,140 --> 01:11:46,160
That was random variables, but essentially it's not singular square good for all.

423
01:11:46,910 --> 01:11:50,720
So it's indexed. It's variable specific, right? It's indexed by.

424
01:11:54,320 --> 01:12:00,100
We are sigma pi squared is. It's why I.

425
01:12:03,740 --> 01:12:07,100
Then. So the normalized estimation.

426
01:12:09,780 --> 01:12:25,750
This to me. The. These, let's say, when we take tea in and divided by a consistent estimate of its sigma.

427
01:12:27,080 --> 01:12:31,640
And then the variance as we do that. So the variants of the mix will be one.

428
01:12:32,990 --> 01:12:36,050
And so that means it's stabilized.

429
01:12:39,100 --> 01:12:46,210
So a practical way to stabilize is basically divide by a consistent estimate of the square root of the variance.

430
01:12:48,460 --> 01:13:00,910
Then. So the seal tea then would look like an over and well, we'll go to normal zero.

431
01:13:01,120 --> 01:13:04,210
And then either one or identity matrix depending on with.

432
01:13:05,100 --> 01:13:08,610
Whether this is a factor, the variable.

433
01:13:11,290 --> 01:13:21,100
So this is the most common way to do it. And this is it for today.

434
01:13:21,120 --> 01:13:25,080
So next time we will do M and Z.

435
01:13:25,230 --> 01:13:35,420
Estimation Introduction. And Z estimation.

436
01:13:35,420 --> 01:13:45,850
And this is our last topic. This is it.

437
01:13:48,580 --> 01:13:49,030
The.

