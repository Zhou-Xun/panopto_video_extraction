1
00:00:09,140 --> 00:00:12,000
Hey. This is Matthew Zawistowski.

2
00:00:12,000 --> 00:00:13,770
Welcome back to our analysis of

3
00:00:13,770 --> 00:00:16,815
the MPLS repeated
measures dataset.

4
00:00:16,815 --> 00:00:18,450
In this lecture, we'll be fitting

5
00:00:18,450 --> 00:00:19,530
some of the mixed models

6
00:00:19,530 --> 00:00:22,170
we've been discussing to
the reading test scores.

7
00:00:22,170 --> 00:00:25,050
We'll also compare how
these models fit to

8
00:00:25,050 --> 00:00:28,454
standard linear regression
that ignores correlation.

9
00:00:28,454 --> 00:00:31,260
As a reminder, the MPLS dataset

10
00:00:31,260 --> 00:00:33,480
consists of reading test
scores collected in

11
00:00:33,480 --> 00:00:36,090
a repeated measure
fashion on students from

12
00:00:36,090 --> 00:00:39,595
the 5th-8th grade in the
Minnesota public school system.

13
00:00:39,595 --> 00:00:42,545
The first goal of our
longitudinal data analysis

14
00:00:42,545 --> 00:00:44,090
is to determine if scores change

15
00:00:44,090 --> 00:00:45,965
over time within students

16
00:00:45,965 --> 00:00:48,610
as they progress from
the 5th-8th grades.

17
00:00:48,610 --> 00:00:51,765
We're going to fit three
models to this dataset.

18
00:00:51,765 --> 00:00:54,290
A simple linear regression
model that ignores

19
00:00:54,290 --> 00:00:56,785
the within student
correlation of test scores,

20
00:00:56,785 --> 00:00:59,960
a random intercept model
that allows variation in

21
00:00:59,960 --> 00:01:01,010
the intercepts of

22
00:01:01,010 --> 00:01:04,085
subject-specific regression
lines and finally,

23
00:01:04,085 --> 00:01:07,550
a random intercept, random
slope mixed model that allows

24
00:01:07,550 --> 00:01:09,320
both the intercept and slope to

25
00:01:09,320 --> 00:01:12,680
vary amongst subject-specific
regression lines.

26
00:01:12,680 --> 00:01:15,515
Let's start with the
simple linear regression.

27
00:01:15,515 --> 00:01:17,270
This would be
equivalent to treating

28
00:01:17,270 --> 00:01:19,190
the test scores as independent or

29
00:01:19,190 --> 00:01:21,140
pictorially ignoring
the lines that

30
00:01:21,140 --> 00:01:23,410
connect test scores
on the same student.

31
00:01:23,410 --> 00:01:25,400
Remember that these data violate

32
00:01:25,400 --> 00:01:28,265
the independence assumption
for simple linear regression.

33
00:01:28,265 --> 00:01:30,980
But it's still instructed
to see what happens.

34
00:01:30,980 --> 00:01:33,200
The lm function in R is

35
00:01:33,200 --> 00:01:36,100
used to fit the simple
linear regression model.

36
00:01:36,100 --> 00:01:38,585
Here's the resulting, our output

37
00:01:38,585 --> 00:01:40,190
and fitted regression line.

38
00:01:40,190 --> 00:01:42,320
The fitted regression parameters,

39
00:01:42,320 --> 00:01:45,065
or 183.9 for the intercept,

40
00:01:45,065 --> 00:01:47,360
and 4.4 for the slope.

41
00:01:47,360 --> 00:01:48,845
According to the model,

42
00:01:48,845 --> 00:01:51,020
the mean test score increases by

43
00:01:51,020 --> 00:01:54,340
4.4 points for each grade.

44
00:01:54,340 --> 00:01:56,210
Let's also pay attention to

45
00:01:56,210 --> 00:01:59,000
the estimate for the random
component of this model.

46
00:01:59,000 --> 00:02:00,905
The residual standard error,

47
00:02:00,905 --> 00:02:02,330
that is the estimate for

48
00:02:02,330 --> 00:02:06,185
the standard deviation of
the residuals is 19.53.

49
00:02:06,185 --> 00:02:07,850
Remember that number,

50
00:02:07,850 --> 00:02:09,140
because this is
where we're going to

51
00:02:09,140 --> 00:02:10,280
see the biggest difference when

52
00:02:10,280 --> 00:02:12,485
comparing to the mixed models.

53
00:02:12,485 --> 00:02:14,780
The interpretation of this model

54
00:02:14,780 --> 00:02:16,730
is that the fitted
slope and intercept

55
00:02:16,730 --> 00:02:19,130
regression line parameters
define the mean or

56
00:02:19,130 --> 00:02:21,835
predicted test scores
across grade levels.

57
00:02:21,835 --> 00:02:24,005
The actual observed
test scores are

58
00:02:24,005 --> 00:02:26,645
independently distributed
about the regression line

59
00:02:26,645 --> 00:02:29,030
according to a normal
distribution with mean

60
00:02:29,030 --> 00:02:33,055
zero and standard
deviation of 19.53.

61
00:02:33,055 --> 00:02:37,055
Now let's fit the random
intercept mixed model.

62
00:02:37,055 --> 00:02:39,110
The random intercept model is

63
00:02:39,110 --> 00:02:41,015
fit using the LMER function in

64
00:02:41,015 --> 00:02:43,400
R. Random intercept terms are

65
00:02:43,400 --> 00:02:46,730
indicated using the
parentheses notation as shown.

66
00:02:46,730 --> 00:02:50,675
The one indicates an
intercept and sub ID tells

67
00:02:50,675 --> 00:02:52,490
R which variable defines

68
00:02:52,490 --> 00:02:54,925
the level of the
repeated measurements.

69
00:02:54,925 --> 00:02:57,020
Here's the R output for the

70
00:02:57,020 --> 00:02:59,000
fitted random intercept model.

71
00:02:59,000 --> 00:03:02,695
Let's walk through each section
of results individually.

72
00:03:02,695 --> 00:03:06,245
First, we are given numerous
measures for model fit,

73
00:03:06,245 --> 00:03:08,690
including AIC and deviance,

74
00:03:08,690 --> 00:03:09,980
quantities you have previously

75
00:03:09,980 --> 00:03:12,355
encountered for
regression models.

76
00:03:12,355 --> 00:03:14,210
Next, R gives us

77
00:03:14,210 --> 00:03:15,740
the five-number summary for

78
00:03:15,740 --> 00:03:18,400
the residual or
random error terms.

79
00:03:18,400 --> 00:03:20,060
Remember, we assume that

80
00:03:20,060 --> 00:03:21,965
these would be
normally distributed.

81
00:03:21,965 --> 00:03:26,090
The five-number summary allows
us a quick lens to assess

82
00:03:26,090 --> 00:03:28,130
whether the distribution
of residuals is

83
00:03:28,130 --> 00:03:31,010
roughly centered at
zero and symmetric.

84
00:03:31,010 --> 00:03:33,200
Up next is the summary of

85
00:03:33,200 --> 00:03:35,485
random components for the model.

86
00:03:35,485 --> 00:03:37,445
The line with sub ID

87
00:03:37,445 --> 00:03:39,530
corresponds to the
random intercepts.

88
00:03:39,530 --> 00:03:42,200
The quantity 18.295 is

89
00:03:42,200 --> 00:03:44,585
the point estimate
for Sigma B zero,

90
00:03:44,585 --> 00:03:46,070
the standard deviation for

91
00:03:46,070 --> 00:03:48,625
the distribution of
random intercepts.

92
00:03:48,625 --> 00:03:52,295
The line width residual
corresponds to the random errors,

93
00:03:52,295 --> 00:03:54,695
or the Epsilon ij in the model.

94
00:03:54,695 --> 00:03:59,180
The quantity 5.4 is the
point estimate for sigma E,

95
00:03:59,180 --> 00:04:00,800
the standard deviation of

96
00:04:00,800 --> 00:04:03,004
the distribution
of random errors.

97
00:04:03,004 --> 00:04:05,630
These random component
parameters are typically

98
00:04:05,630 --> 00:04:08,209
not the focus of a
longitudinal data analysis.

99
00:04:08,209 --> 00:04:11,240
There are sometimes referred
to as nuisance parameters.

100
00:04:11,240 --> 00:04:13,310
We need them to perform inference

101
00:04:13,310 --> 00:04:15,530
and they're estimated
as part of the model.

102
00:04:15,530 --> 00:04:17,210
Even if your analysis does not

103
00:04:17,210 --> 00:04:19,010
explicitly include
these quantities,

104
00:04:19,010 --> 00:04:20,990
you should at least know
where the point estimates are

105
00:04:20,990 --> 00:04:22,295
located in the output

106
00:04:22,295 --> 00:04:24,575
and understand how
to interpret them.

107
00:04:24,575 --> 00:04:26,780
Finally, we have a summary of

108
00:04:26,780 --> 00:04:28,535
the fixed effect parameters.

109
00:04:28,535 --> 00:04:30,500
These are usually what
we're most interested

110
00:04:30,500 --> 00:04:33,250
in for longitudinal
data analysis.

111
00:04:33,250 --> 00:04:36,260
The estimate column gives
the fitted values for

112
00:04:36,260 --> 00:04:37,670
the intercept and slope

113
00:04:37,670 --> 00:04:39,530
of the population
regression line.

114
00:04:39,530 --> 00:04:41,990
The table also includes
the standard error

115
00:04:41,990 --> 00:04:44,785
in t-statistic for the
regression parameters.

116
00:04:44,785 --> 00:04:47,135
You might notice
something is missing.

117
00:04:47,135 --> 00:04:49,340
The LMER function
does not provide

118
00:04:49,340 --> 00:04:52,280
p values corresponding
to the t-statistics.

119
00:04:52,280 --> 00:04:54,380
The reason is that
the distributions for

120
00:04:54,380 --> 00:04:56,845
these t-statistics are
not well understood.

121
00:04:56,845 --> 00:05:00,260
Regardless, t-statistics
with larger magnitudes,

122
00:05:00,260 --> 00:05:02,390
positive or negative indicates

123
00:05:02,390 --> 00:05:04,490
stronger evidence in
favor of rejecting

124
00:05:04,490 --> 00:05:06,050
the null hypothesis that

125
00:05:06,050 --> 00:05:09,440
the corresponding regression
parameters equal to zero.

126
00:05:09,440 --> 00:05:12,440
The fitted fixed effect
parameters define

127
00:05:12,440 --> 00:05:13,850
the population regression line

128
00:05:13,850 --> 00:05:15,935
shown here in the black line.

129
00:05:15,935 --> 00:05:17,990
Let's take a minute to interpret

130
00:05:17,990 --> 00:05:21,280
the random components of this
model using some pictures.

131
00:05:21,280 --> 00:05:23,780
The subject specific
regression lines are

132
00:05:23,780 --> 00:05:26,090
assumed to be independently
distributed about

133
00:05:26,090 --> 00:05:28,190
the population regression
line according to

134
00:05:28,190 --> 00:05:29,870
a normal distribution with mean

135
00:05:29,870 --> 00:05:32,825
zero and standard
deviation of Sigma b_0,

136
00:05:32,825 --> 00:05:35,675
which is estimated to be 18.3.

137
00:05:35,675 --> 00:05:38,990
Note that it's the subject
specific lines rather than

138
00:05:38,990 --> 00:05:40,760
individual observations that are

139
00:05:40,760 --> 00:05:43,205
distributed about the
population regression line.

140
00:05:43,205 --> 00:05:44,630
This is a major difference

141
00:05:44,630 --> 00:05:46,805
compared to the
simple linear model.

142
00:05:46,805 --> 00:05:49,445
Next, conditional
on a given student,

143
00:05:49,445 --> 00:05:51,920
the test scores for that
student or independently

144
00:05:51,920 --> 00:05:55,220
distributed about that students
subject-specific line,

145
00:05:55,220 --> 00:05:57,545
shown here is the
dashed blue line.

146
00:05:57,545 --> 00:05:59,480
The test scores are
assumed to follow

147
00:05:59,480 --> 00:06:02,795
a normal distribution with
standard deviation Sigma_e,

148
00:06:02,795 --> 00:06:05,765
which we have
estimated to be 5.4.

149
00:06:05,765 --> 00:06:09,305
This figure is showing the
concept of partitioning error.

150
00:06:09,305 --> 00:06:11,525
First, with between
subject error,

151
00:06:11,525 --> 00:06:13,355
denoted by the
black normal curve,

152
00:06:13,355 --> 00:06:15,215
and then by within subject error

153
00:06:15,215 --> 00:06:17,405
denoted by the
likely normal curve.

154
00:06:17,405 --> 00:06:19,235
In this specific example,

155
00:06:19,235 --> 00:06:21,680
we see that there's more
variation due to between

156
00:06:21,680 --> 00:06:25,245
subjects error than random
within subject error.

157
00:06:25,245 --> 00:06:26,970
The fixed effects solution,

158
00:06:26,970 --> 00:06:28,985
the focus of our analysis.

159
00:06:28,985 --> 00:06:30,680
Let's compare the estimates and

160
00:06:30,680 --> 00:06:32,180
standard errors we get from

161
00:06:32,180 --> 00:06:33,710
standard linear regression in

162
00:06:33,710 --> 00:06:35,615
the random intercept mixed model.

163
00:06:35,615 --> 00:06:38,360
The red boxes indicate
the point estimates for

164
00:06:38,360 --> 00:06:41,300
the population intercepts
and slopes from each model.

165
00:06:41,300 --> 00:06:42,860
Immediately, you should notice

166
00:06:42,860 --> 00:06:44,990
that they are very similar.

167
00:06:44,990 --> 00:06:48,080
Similar in fact, that
the regression lines are

168
00:06:48,080 --> 00:06:51,125
nearly indistinguishable
when plotted with the data.

169
00:06:51,125 --> 00:06:52,640
The two methods produce

170
00:06:52,640 --> 00:06:55,175
nearly identical population
regression lines

171
00:06:55,175 --> 00:06:57,860
relating mean test
score to grade level.

172
00:06:57,860 --> 00:07:00,080
Let's shift our attention now to

173
00:07:00,080 --> 00:07:02,315
the standard errors of
the point estimates.

174
00:07:02,315 --> 00:07:04,160
Here we see the big difference.

175
00:07:04,160 --> 00:07:06,260
The standard errors
for both parameters

176
00:07:06,260 --> 00:07:08,115
are smaller for the next model.

177
00:07:08,115 --> 00:07:09,940
The reason for this is based on

178
00:07:09,940 --> 00:07:12,505
the unexplained or random
error in each model,

179
00:07:12,505 --> 00:07:14,380
that is being used in
the computation of

180
00:07:14,380 --> 00:07:16,785
these regression parameter
standard errors.

181
00:07:16,785 --> 00:07:18,560
As we previously mentioned,

182
00:07:18,560 --> 00:07:21,335
the random errors have an
estimated standard deviation of

183
00:07:21,335 --> 00:07:25,205
19.53 in the simple
linear regression model.

184
00:07:25,205 --> 00:07:27,560
But in estimated standard
deviation of only

185
00:07:27,560 --> 00:07:30,575
5.4 in the random
intercept model.

186
00:07:30,575 --> 00:07:33,710
The mixed model is essentially
attributing some of

187
00:07:33,710 --> 00:07:35,420
the overall error to between

188
00:07:35,420 --> 00:07:36,830
subject error because of

189
00:07:36,830 --> 00:07:38,675
the correlation of
repeated measures,

190
00:07:38,675 --> 00:07:41,645
rather than attributing
it all to random error.

191
00:07:41,645 --> 00:07:45,440
The effect of this change in
standard errors is reflected

192
00:07:45,440 --> 00:07:49,010
in a comparison of t-statistics
between the two models.

193
00:07:49,010 --> 00:07:51,410
Remember that the effect sizes or

194
00:07:51,410 --> 00:07:52,700
point estimates of

195
00:07:52,700 --> 00:07:55,280
the regression parameters
were nearly identical.

196
00:07:55,280 --> 00:07:57,500
The differences in
t-statistics is

197
00:07:57,500 --> 00:07:59,915
primarily due to the
standard errors.

198
00:07:59,915 --> 00:08:03,200
The t-statistic for grade
effect increases from

199
00:08:03,200 --> 00:08:06,740
2.153 in the simple
linear regression model,

200
00:08:06,740 --> 00:08:10,460
to 8.564 in a random
intercept model.

201
00:08:10,460 --> 00:08:13,520
Remember that we can roughly
equate t-statistics with

202
00:08:13,520 --> 00:08:15,680
larger magnitude to
stronger evidence

203
00:08:15,680 --> 00:08:17,555
against the null hypothesis.

204
00:08:17,555 --> 00:08:20,030
Therefore, despite
the two models giving

205
00:08:20,030 --> 00:08:21,950
similar point estimates for

206
00:08:21,950 --> 00:08:23,885
change in test scores by grade,

207
00:08:23,885 --> 00:08:26,015
the evidence for
rejecting the null

208
00:08:26,015 --> 00:08:28,415
is much stronger based
on the mixed model.

209
00:08:28,415 --> 00:08:30,590
This example
highlights the impact

210
00:08:30,590 --> 00:08:31,760
of properly accounting for

211
00:08:31,760 --> 00:08:35,450
correlation can have on your
analysis and inference.

212
00:08:35,450 --> 00:08:38,045
Now, let's fit a
random intercept,

213
00:08:38,045 --> 00:08:41,180
random slope model
to the MPLS data.

214
00:08:41,180 --> 00:08:43,670
Recall that the random
slope allows for

215
00:08:43,670 --> 00:08:47,030
variation in the slopes of
the subject specific lines.

216
00:08:47,030 --> 00:08:50,105
Further acknowledging
between subject differences

217
00:08:50,105 --> 00:08:52,910
that should not be
attributed to random error.

218
00:08:52,910 --> 00:08:55,730
To add a random
slope to the R code,

219
00:08:55,730 --> 00:08:57,425
we include the grade variable

220
00:08:57,425 --> 00:08:59,615
in the list of
random effect terms.

221
00:08:59,615 --> 00:09:02,360
Note that the grade variable
is now included among

222
00:09:02,360 --> 00:09:05,600
both the fixed effects
and random effects.

223
00:09:05,600 --> 00:09:07,550
Here is the R output for

224
00:09:07,550 --> 00:09:10,460
the fitted random intercepts,
random slope model.

225
00:09:10,460 --> 00:09:11,960
The point estimates for

226
00:09:11,960 --> 00:09:14,630
the fixed effect parameters
are nearly identical to

227
00:09:14,630 --> 00:09:16,730
those from both the
simple linear regression

228
00:09:16,730 --> 00:09:19,055
and the random
intercept mixed model,

229
00:09:19,055 --> 00:09:21,680
which should not come
as much of a surprise.

230
00:09:21,680 --> 00:09:24,080
Remember that the
random slope brought

231
00:09:24,080 --> 00:09:25,310
some additional parameters to

232
00:09:25,310 --> 00:09:27,200
the random component
of the model.

233
00:09:27,200 --> 00:09:29,330
We now have an
additional estimate

234
00:09:29,330 --> 00:09:30,770
of the standard deviation among

235
00:09:30,770 --> 00:09:35,885
the random slopes Sigma b1
here estimated to be 2.63.

236
00:09:35,885 --> 00:09:38,480
Note that the standard
deviation estimate

237
00:09:38,480 --> 00:09:40,310
for random error Sigma e,

238
00:09:40,310 --> 00:09:43,265
has dropped slightly from
the random intercept model.

239
00:09:43,265 --> 00:09:45,680
It is now 4.28 as compared

240
00:09:45,680 --> 00:09:48,410
to 5.4 in the random
intercept model.

241
00:09:48,410 --> 00:09:50,645
This means that some
of the random error

242
00:09:50,645 --> 00:09:52,115
in the random intercept model,

243
00:09:52,115 --> 00:09:54,545
has been absorbed by
the random slope terms.

244
00:09:54,545 --> 00:09:57,980
That is, some of what we
previously called random error,

245
00:09:57,980 --> 00:09:59,480
can instead be attributed to

246
00:09:59,480 --> 00:10:02,195
the between subject
variation in slopes.

247
00:10:02,195 --> 00:10:04,400
Remember that the
random slope brought

248
00:10:04,400 --> 00:10:05,600
some additional parameters to

249
00:10:05,600 --> 00:10:07,520
the random component
of the model.

250
00:10:07,520 --> 00:10:09,860
We now have an
additional estimate of

251
00:10:09,860 --> 00:10:12,075
the standard deviation
among the random slopes,

252
00:10:12,075 --> 00:10:15,915
Sigma_b1 here
estimated to be 2.64.

253
00:10:15,915 --> 00:10:18,340
Note that the standard
deviation estimate

254
00:10:18,340 --> 00:10:19,375
for random error

255
00:10:19,375 --> 00:10:20,680
Sigma e has dropped

256
00:10:20,680 --> 00:10:22,825
slightly from the
random intercept model.

257
00:10:22,825 --> 00:10:25,630
It is now 4.28 as compared

258
00:10:25,630 --> 00:10:28,590
to 5.4 in the random
intercept model.

259
00:10:28,590 --> 00:10:30,980
This means that some of
them random error in

260
00:10:30,980 --> 00:10:32,360
the random intercept
model has been

261
00:10:32,360 --> 00:10:34,655
absorbed by the
random slope terms.

262
00:10:34,655 --> 00:10:38,300
That is, some of what we
previously called random error,

263
00:10:38,300 --> 00:10:39,980
can instead be attributed to

264
00:10:39,980 --> 00:10:42,885
the between subject
variation in slopes.

265
00:10:42,885 --> 00:10:46,270
We are also provided with an
estimate of the correlation

266
00:10:46,270 --> 00:10:49,750
between random intercepts in
random slopes in students.

267
00:10:49,750 --> 00:10:51,730
Here, we see that
this quantity is

268
00:10:51,730 --> 00:10:54,600
estimated to be negative 0.74.

269
00:10:54,600 --> 00:10:57,440
The negative correlation
implies that students with

270
00:10:57,440 --> 00:11:00,515
higher intercepts tend
to have smaller slopes.

271
00:11:00,515 --> 00:11:02,870
That is, students
with higher starting

272
00:11:02,870 --> 00:11:04,280
test scores tend to have

273
00:11:04,280 --> 00:11:07,265
smaller increases in
test scores over time.

274
00:11:07,265 --> 00:11:09,080
We fit three models to

275
00:11:09,080 --> 00:11:12,260
the same repeated
measure MPLS data.

276
00:11:12,260 --> 00:11:15,635
A simple linear regression
that ignored correlation.

277
00:11:15,635 --> 00:11:18,110
A random intercept
mixed effect model that

278
00:11:18,110 --> 00:11:21,035
allowed for between student
variation and intercepts.

279
00:11:21,035 --> 00:11:22,655
Finally, the random intercept,

280
00:11:22,655 --> 00:11:24,860
random slope model
that further allowed

281
00:11:24,860 --> 00:11:27,410
for between student
variation in slopes.

282
00:11:27,410 --> 00:11:30,320
Thinking about how test
scores change over time,

283
00:11:30,320 --> 00:11:31,700
all three methods give

284
00:11:31,700 --> 00:11:33,440
nearly identical estimates for

285
00:11:33,440 --> 00:11:36,080
the increase in mean
test score over time.

286
00:11:36,080 --> 00:11:38,000
It is the inference based on

287
00:11:38,000 --> 00:11:40,490
the models that produces
very different results.

288
00:11:40,490 --> 00:11:42,785
The evidence for a
change in test scores

289
00:11:42,785 --> 00:11:46,155
is much stronger for
the mixed models.

