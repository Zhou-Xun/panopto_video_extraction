1
00:00:39,470 --> 00:00:53,830
Okay. Good morning, everyone. So we basically finished our exposition of the main instruments.

2
00:00:56,000 --> 00:01:03,320
So the first half of the course, this was just the basics of probability mathematics.

3
00:01:04,340 --> 00:01:17,780
And in the second half of the course, we looked at convergence and various types of relationships between convergences and techniques around them,

4
00:01:17,780 --> 00:01:21,650
and this is the main instrument to study properties of estimates.

5
00:01:22,640 --> 00:01:32,390
So now in this lecture we will introduce the two most common ways to derive estimate is.

6
00:01:33,170 --> 00:01:43,940
So sometimes you will find that an estimate is simply proposed out of the blue by somebody and then the properties are proved.

7
00:01:44,030 --> 00:01:51,950
But that's not the preferred way because we would ideally have a recipe how to

8
00:01:51,950 --> 00:01:56,580
deal with it in some general framework and how we come up with good estimate.

9
00:01:56,700 --> 00:02:01,820
So the M and Z estimate, this is an example of such a recipe.

10
00:02:04,510 --> 00:02:08,510
So it's not. And algorithms.

11
00:02:10,960 --> 00:02:18,940
So recipes a kind of thing that still requires theoretical work, but just gives you direction and steps that you need to follow in general.

12
00:02:18,940 --> 00:02:26,620
So this is the case, for example, with the algorithms that you may have had in your previous courses.

13
00:02:28,750 --> 00:02:35,829
So it's not a given that you will arrive at a reasonable for a particular problem,

14
00:02:35,830 --> 00:02:43,360
but at least the theory gives you a recipe how you would approach it, and you can try following it.

15
00:02:44,860 --> 00:02:56,410
But before we go there. So in the last lecture, we considered the multivariable central limit theorems,

16
00:02:56,950 --> 00:03:07,060
including in its classical form, where observations were just barely scratching the surface of this field.

17
00:03:08,380 --> 00:03:14,290
The classical one is when we have had independent, identically distributed estimates.

18
00:03:15,460 --> 00:03:23,440
And so the non-classical is when some of these assumptions are violated.

19
00:03:25,900 --> 00:03:40,140
There are. And so in our particular case, we looked at where the at the situation where the assumption of identical distribution is violated,

20
00:03:40,150 --> 00:03:42,850
but the observations are still considered independence.

21
00:03:43,900 --> 00:03:58,180
Perhaps even more important are cases where there is some dependance, but it's weak enough so that information still accumulates with the sample size.

22
00:03:58,180 --> 00:04:04,510
But we are not delving into theorems like that for like of the time.

23
00:04:05,050 --> 00:04:10,630
The independence covers the bulk of the statistical theory.

24
00:04:13,800 --> 00:04:27,090
Usually, right? An example where independence is violated is uh, uh, survival analysis or analysis of rates where you observe population over time.

25
00:04:27,300 --> 00:04:32,700
And uh, so one to make some inference about the rate of a certain disease,

26
00:04:33,480 --> 00:04:37,170
uh, or the rates of death from a certain disease occurring in the population.

27
00:04:37,890 --> 00:04:42,090
And then of course because it's one of the same population or different points in time,

28
00:04:42,090 --> 00:04:50,250
there is dependance there uh, is, it's the same population contributing to each point,

29
00:04:51,630 --> 00:05:01,560
but that dependance can be ignored because of a special serum, uh, that central serum that allows for this dependance to exist.

30
00:05:04,140 --> 00:05:12,600
So we had two conditions when we looked at Dependent, threatened them at the Independent,

31
00:05:12,600 --> 00:05:16,890
but not identically, not necessarily identically distributed random sample.

32
00:05:17,700 --> 00:05:26,110
One condition was Lindbergh. And then we defined the somewhat more practical condition, uh,

33
00:05:26,220 --> 00:05:34,320
that was called off and we showed that ripping off is actually a bit stronger than lindeberg, but easier to verify.

34
00:05:34,320 --> 00:05:42,840
So it wasn't usual trade off there. The question that's somewhat hanging in the air right now.

35
00:05:42,840 --> 00:05:49,890
So we had a classical central limit theorem situation.

36
00:05:54,190 --> 00:05:58,450
Let's go see g situation.

37
00:06:01,300 --> 00:06:13,480
Where we had essentially independent, identically distributed variables or random vectors if we're considering its multivariate version.

38
00:06:14,260 --> 00:06:17,830
And then we have the lynda bird.

39
00:06:21,060 --> 00:06:39,320
Uh. Or of. A version of the situation where, uh, uh, so the identical distribution is not available, right?

40
00:06:39,320 --> 00:06:44,960
So this is crossed out. There is some dependance.

41
00:06:45,410 --> 00:06:45,670
Right.

42
00:06:45,920 --> 00:07:00,799
But of course, we can look at the classical, simple random sample of random factors with identical distribution and independence and ask the question,

43
00:07:00,800 --> 00:07:04,160
how would diplomats look for a situation like this?

44
00:07:05,860 --> 00:07:13,059
Uh, expecting that probably it's a particular case, but we will see that it's not quite a particular case.

45
00:07:13,060 --> 00:07:21,490
And there is some payment to make for the ability to handle non-identical distributions.

46
00:07:24,340 --> 00:07:29,620
Um, so I'm looking at the classical Keelty situation.

47
00:07:29,620 --> 00:07:35,710
So there I have excise uh, from one and.

48
00:07:38,070 --> 00:07:42,720
These are the random variables.

49
00:07:44,670 --> 00:07:49,260
Now we have a somewhat different formulation for the classical C,

50
00:07:49,770 --> 00:07:56,579
T and the PENARTH because we said because of the lack of identical distribution, one estimate,

51
00:07:56,580 --> 00:08:08,819
the one normalization for the all of the exercise may not work and we may need some because of the exercise,

52
00:08:08,820 --> 00:08:17,160
because of the non-identical distribution of I may have their own variances and may need to be normalized separately and so on.

53
00:08:18,150 --> 00:08:23,820
Um, so let's do the recipe for the classical situation.

54
00:08:24,270 --> 00:08:26,190
So we have a test statistic.

55
00:08:28,100 --> 00:08:46,850
For the mean let's say team and it's one of and some of I from one to an ex I my this move right so the classical situation the idea we have need.

56
00:08:48,870 --> 00:08:59,540
You write that each. So that's expected value of X with any index.

57
00:09:00,290 --> 00:09:07,489
Let's see, x one. Right. So how do we get the test statistics?

58
00:09:07,490 --> 00:09:10,920
So we center random variables.

59
00:09:12,600 --> 00:09:19,880
And. Divided by an.

60
00:09:21,850 --> 00:09:28,390
Right. That's how I want it represented as an empirical expectation.

61
00:09:32,870 --> 00:09:35,990
So this is about the point where we need notation for it.

62
00:09:46,600 --> 00:09:54,580
So in your previous courses, did you have a notation for sample mean other than something like Neal with a have.

63
00:09:59,940 --> 00:10:03,999
Okay. I'm not hearing anything. So one for the patients.

64
00:10:04,000 --> 00:10:09,780
So this is the true expectation. Right. E And so the empirical.

65
00:10:11,460 --> 00:10:20,400
So we can do like this, right? There's one other notation for empirical expectation that comes from.

66
00:10:28,140 --> 00:10:33,430
So this is true. Expectation.

67
00:10:33,800 --> 00:10:38,040
This is an empirical. Expectations.

68
00:10:38,920 --> 00:10:43,060
There's notation coming from the theory of empirical processes.

69
00:10:53,820 --> 00:11:00,330
And they are convenient because we oftentimes when we look at different statistics,

70
00:11:00,330 --> 00:11:09,810
it's a difference between difference between the empirical distribution and the true distribution and expectations taking with respect to them.

71
00:11:10,410 --> 00:11:19,020
So in order to kind of flesh out and bring to the fore the essence of comparison of true versus empirical.

72
00:11:19,470 --> 00:11:23,520
So they have annotations of the the true expectations.

73
00:11:24,450 --> 00:11:28,470
So let's say integral of x DP.

74
00:11:29,640 --> 00:11:35,210
That's the one. So they write it as P.

75
00:11:36,290 --> 00:11:50,770
Yes. So it's meaning there p is the distribution of x and PE, the beautiful one right with the double bar is just like E with a double bar, right?

76
00:11:51,230 --> 00:11:55,270
That's an operator that's acting on the x like this.

77
00:11:56,560 --> 00:12:06,010
Right. So when I have the empirical expectation, so I have one over and some over, I write x i.

78
00:12:07,880 --> 00:12:15,950
That's the analog of it. And this is written as an empirical measure, acting on X.

79
00:12:20,220 --> 00:12:25,380
Where is the empirical measure assigning mass one over and the rights to.

80
00:12:27,010 --> 00:12:30,579
Values of exercise that are observed.

81
00:12:30,580 --> 00:12:35,710
Right. And then observed the then true minus empirical.

82
00:12:42,990 --> 00:12:48,600
Or the other way around. So it can be written as B minus P and.

83
00:12:50,070 --> 00:13:02,300
You have to know next. This is just for your information, because in modern papers, often times this notation is used instead of the expectation.

84
00:13:03,950 --> 00:13:11,930
And like I said, it, uh, basically, uh, uh, fleshes out the difference between the true distribution and the empirical one.

85
00:13:12,440 --> 00:13:18,960
And this is the empirical process, by the way. Okay.

86
00:13:20,640 --> 00:13:32,390
So with that in mind. So I can write these is the empirical expectation of why, right?

87
00:13:39,230 --> 00:13:45,020
Why is the sentence? Random variable.

88
00:13:47,370 --> 00:13:52,150
Okay. We still need to normalize it to stabilize the variance.

89
00:13:52,800 --> 00:13:56,070
So let's look for the variance.

90
00:13:58,720 --> 00:14:06,370
So that would make it. What? Some of I.

91
00:14:09,510 --> 00:14:25,050
Why? Right. Because if you remember in the Linda Burke version of The Theorem, we just have a sample of the variables in the condition,

92
00:14:25,740 --> 00:14:33,750
and each random variable already absorbs the centering and the normal is zation that stabilizes the variance.

93
00:14:34,290 --> 00:14:39,420
So we just did the same during. So we need to normalize the variance.

94
00:14:39,990 --> 00:14:43,860
So we compute the variance of g.

95
00:14:43,860 --> 00:14:56,910
And this is as. And squared right where I am is sigma over the square with the Venn.

96
00:15:00,080 --> 00:15:08,860
The normalization. Goes like this.

97
00:15:08,860 --> 00:15:14,420
So just divide t n by an estimate of its standard deviation.

98
00:15:15,640 --> 00:15:30,220
These would be sum from one two, and it's a minus new of the square of the firm in the sigma in this form.

99
00:15:30,670 --> 00:15:37,180
This is our new x pie that will go into the condition.

100
00:15:44,820 --> 00:15:55,229
Because in this form I have a very stabilizing transformation and I have centering and that's, uh, and I have a sum of,

101
00:15:55,230 --> 00:16:04,170
uh, since centered and normalized three variables, just like the level of condition requires for it.

102
00:16:10,800 --> 00:16:15,030
Now the level of condition is about, uh, uh, l s.

103
00:16:15,570 --> 00:16:28,260
So that's the function we introduced. And so we can write it in this case is.

104
00:16:29,690 --> 00:16:32,719
So I need to do some. I for one too.

105
00:16:32,720 --> 00:16:38,750
And and I need the expected value of my rent.

106
00:16:38,750 --> 00:16:41,780
The variable taken to a power greater than two.

107
00:16:41,780 --> 00:16:52,120
Right. So as is greater than two. And I'll take it as two plus Delta, where Delta is some number greater than zero.

108
00:16:52,540 --> 00:16:58,030
Now from the properties, from the non decreasing character of the functions,

109
00:16:59,320 --> 00:17:10,899
we can say that if we found that the condition is satisfied for some delta, it will also be satisfied for some larger delta.

110
00:17:10,900 --> 00:17:23,060
And we'll also be satisfied for a smaller one. So the peace that does not depend on the variables will be in the denominator.

111
00:17:23,930 --> 00:17:27,740
Right. And it's to the power to plus delta to the power.

112
00:17:28,910 --> 00:17:34,400
Right. And then I have expectations so that that's I minus me.

113
00:17:36,380 --> 00:17:42,230
Two plus Delta. Right.

114
00:17:44,430 --> 00:17:48,600
So it's just the sum of the new expected values of the new X.

115
00:17:50,320 --> 00:17:58,980
Taken to the power of two plus. As the above condition requires.

116
00:17:59,700 --> 00:18:03,660
And what we can say is the denominator.

117
00:18:08,320 --> 00:18:12,850
Is the only part that depends on them. And we need to go to zero.

118
00:18:12,910 --> 00:18:27,350
Right. That's the level of condition within. And denominator ease and um, to the power of uh, delta over to.

119
00:18:27,380 --> 00:18:32,150
Right. Yeah.

120
00:18:32,580 --> 00:18:36,700
Well. So let's just write that down.

121
00:18:37,960 --> 00:18:42,520
So because of the I.D., I have.

122
00:18:44,080 --> 00:18:48,350
And times the. Expected value.

123
00:18:50,270 --> 00:18:55,090
It's one minus me. So listen, they'll tell.

124
00:18:57,280 --> 00:19:04,010
Over. And one last delta over to.

125
00:19:08,320 --> 00:19:11,610
Times. Sigma two.

126
00:19:12,150 --> 00:19:15,560
So plus delta like this, right?

127
00:19:15,790 --> 00:19:19,980
And then. So I have the expectation.

128
00:19:30,740 --> 00:19:38,750
In the denominator. I have a constant. Of course, both situations assume that variance exists.

129
00:19:39,950 --> 00:19:45,010
So Sigma exists. And then I have an two delta over to.

130
00:19:45,260 --> 00:19:53,600
Right. So this guy. Of course, goes to zero and then goes to infinity.

131
00:19:53,600 --> 00:19:57,560
And that's the only piece in the condition that depends on that.

132
00:19:57,650 --> 00:20:03,030
Right. But that condition would not be satisfied if this expectation does not exist.

133
00:20:03,090 --> 00:20:07,520
Right. Um. So. For.

134
00:20:10,130 --> 00:20:14,930
They'll ask to go to zero and there's to infinity.

135
00:20:14,930 --> 00:20:22,490
That's the level of condition we need. The expectation.

136
00:20:24,980 --> 00:20:33,140
It's one minus new two plus delta to exist rates to be less than infinity.

137
00:20:34,310 --> 00:20:38,200
Now the classical seal t.

138
00:20:43,740 --> 00:20:52,710
The only thing we required was the existence of variance, which is x one minus mu to two.

139
00:20:52,870 --> 00:21:09,459
Right. Less than infinity. And if two plus delta or some, uh, non-negative delta is less than infinity, then it would follow.

140
00:21:09,460 --> 00:21:18,850
Right? Uh, so high order moment existence implies the existence of a smaller order moment, right?

141
00:21:19,300 --> 00:21:24,760
So which means that life on offer requires something more than the classical.

142
00:21:27,200 --> 00:21:30,860
Right. So if conditions.

143
00:21:41,030 --> 00:21:49,770
Is stronger. Just the existence.

144
00:21:54,690 --> 00:22:00,730
Of the variance. In the C in the classical seal t.

145
00:22:08,890 --> 00:22:17,080
Right. And you could consider it as a payment for being someone with relaxing the assumption of.

146
00:22:18,930 --> 00:22:24,840
Identical distributions. Okay.

147
00:22:28,260 --> 00:22:33,390
Now the next and last topic is M and Z estimation.

148
00:22:42,380 --> 00:22:49,070
So because of the remaining time. So we won't be able to cover it in great detail.

149
00:22:49,710 --> 00:23:01,490
But we do have time for some general facts with some proofs, and we do have the time to lay out the general recipe that I was talking about.

150
00:23:02,690 --> 00:23:09,670
So the, uh. Recipe or philosophy.

151
00:23:16,140 --> 00:23:19,260
And it's the estimation.

152
00:23:25,400 --> 00:23:28,910
Did anyone talk about Hammond's estimation previously to you?

153
00:23:30,510 --> 00:23:33,559
I know the courses. You certainly know Emily.

154
00:23:33,560 --> 00:23:41,840
And this would be one prime example of it. But you did not approach it the way we are here.

155
00:23:42,940 --> 00:23:47,979
As you'll see from the start with the likelihood of great likelihood with something

156
00:23:47,980 --> 00:23:53,200
that depends on the seventh or eighth was essentially a random function.

157
00:23:54,010 --> 00:23:59,310
So we don't start with the random function. So we start with.

158
00:24:01,390 --> 00:24:13,850
With defining the trauma of. Right.

159
00:24:13,850 --> 00:24:18,530
And in general, this is some collection of the variables.

160
00:24:25,410 --> 00:24:27,540
From a practical point of view,

161
00:24:29,700 --> 00:24:38,280
it's probably almost always a collection of independent from the variables where some of them are unobserved and are used to induce dependance.

162
00:24:38,310 --> 00:24:42,540
If your model really describes some independent data.

163
00:24:44,400 --> 00:24:54,870
But that's. So in general we have some random variables may not be accountable collection if you have a process in your true models.

164
00:24:56,380 --> 00:25:01,810
But that's, uh. And the distribution is, uh, parameterized, right?

165
00:25:01,810 --> 00:25:14,630
So there's a parameter space. Sator again could be a functional space if you consider see non parametric estimation.

166
00:25:16,090 --> 00:25:20,440
Because then you estimate a function and the true function is is.

167
00:25:21,880 --> 00:25:25,480
A functional parameter and parameters.

168
00:25:29,000 --> 00:25:37,520
Or elements of that space. And then there is a true parameter that you, of course, don't know.

169
00:25:40,330 --> 00:25:45,940
And that's the one with the star. And that's also part of the parameter space.

170
00:25:53,440 --> 00:25:57,490
So the second step is to characterize.

171
00:26:02,700 --> 00:26:11,910
The true parameter is a solution. True problem.

172
00:26:17,970 --> 00:26:27,820
Now the word to everywhere and using it to indicate that the sample values do not participate in that statement right away.

173
00:26:28,530 --> 00:26:34,020
So it's about the true model. It's totally probability theory type thing.

174
00:26:34,470 --> 00:26:38,100
Same for the true problem.

175
00:26:38,340 --> 00:26:48,560
Right. So I just want to create artificially some kind of a problem that will have to stars a solution.

176
00:26:50,010 --> 00:26:55,230
And there are two problems that I have in mind.

177
00:26:56,490 --> 00:27:16,530
The maximization problem. And, uh, according to this problem I'm representing, Theta Star is a point of maximum.

178
00:27:20,150 --> 00:27:23,930
Of a theta belonging to the parametric space.

179
00:27:25,180 --> 00:27:28,450
Of some function of theta.

180
00:27:30,450 --> 00:27:36,100
Okay. Where am. Is some.

181
00:27:37,650 --> 00:27:43,830
Function. Meaning nonrandom rates.

182
00:27:47,240 --> 00:27:53,950
True function. And note.

183
00:27:56,120 --> 00:28:01,720
You. The. And expectation.

184
00:28:06,010 --> 00:28:15,270
Over. Yes. Right. So X was the collection of random variables that defined the true models.

185
00:28:20,720 --> 00:28:29,360
So that's the true variable in your model. Uh, this is, uh, not always the case, but most of the time.

186
00:28:39,230 --> 00:28:44,150
Okay. And then there could be an equation, uh, solving.

187
00:28:53,000 --> 00:28:57,980
And there I am representing my solution.

188
00:28:59,900 --> 00:29:05,900
My true parameter value is a solution to the equation.

189
00:29:08,720 --> 00:29:18,530
Some. So theta equals zero where I can say the same thing about the psi.

190
00:29:19,760 --> 00:29:23,370
This is some. Function.

191
00:29:24,700 --> 00:29:37,730
Brendan. General functional rights of the distribution of X.

192
00:29:37,750 --> 00:29:42,580
This is just another way to put the previous sentence that was said about the M.

193
00:29:43,720 --> 00:29:47,620
So expectation of X is some functional of the distribution of X.

194
00:29:55,360 --> 00:30:03,610
So it's again, expectation over. Most of the time.

195
00:30:11,580 --> 00:30:21,330
Right. So as long as we're looking at the situation at the point where we formulate our true model, this exercise seems to be.

196
00:30:23,140 --> 00:30:27,790
Kind of inconsequential, right? Yeah. It's not clear why we're doing it.

197
00:30:28,060 --> 00:30:31,080
Uh, because of that lack of a world where you have to star.

198
00:30:31,600 --> 00:30:41,260
So why even presenting it as a solution? Because that bridges us to and gives us the recipe to derive estimates.

199
00:30:41,380 --> 00:30:58,690
Right. So breach to estimate us. And these will be denoted by, say, with a hat.

200
00:30:58,690 --> 00:31:06,430
So they are based on a sample value and another to get those we approximate.

201
00:31:10,170 --> 00:31:20,210
So this is what? One, two, three. So Proposition eight.

202
00:31:22,380 --> 00:31:27,280
Either am or. Sigh.

203
00:31:28,330 --> 00:31:35,730
By. They sample based.

204
00:31:41,580 --> 00:31:48,670
Counterparts. Right.

205
00:31:48,690 --> 00:31:54,800
And you can see why the. They almost always.

206
00:31:57,960 --> 00:32:07,260
A suggestion here that both men's side B some expectations of the tour and the very blacks that this is

207
00:32:07,260 --> 00:32:16,680
instrumental because it's very easy to approximate a true expectation by something that's based on the sample,

208
00:32:16,930 --> 00:32:20,460
simply replacing it by the empirical expectation.

209
00:32:22,890 --> 00:32:28,400
So if. And. Sigh fi.

210
00:32:29,510 --> 00:32:36,180
He's. Over. It's then it is natural.

211
00:32:40,830 --> 00:32:50,270
To folks to made them. Is.

212
00:32:51,310 --> 00:32:56,320
And. And. I am.

213
00:32:57,470 --> 00:33:00,570
Uh. Imperial expectations.

214
00:33:07,370 --> 00:33:13,040
So we basically take the true expectation and replace it by the empirical one.

215
00:33:13,790 --> 00:33:19,800
And this is approximation. Based on the samples.

216
00:33:33,680 --> 00:33:38,390
Okay then. So why is this reasonable?

217
00:33:43,900 --> 00:33:48,750
So the hope is like this. Um, so I want.

218
00:33:49,480 --> 00:33:53,860
Mm hmm. So this approximation is a random thing.

219
00:33:53,890 --> 00:33:57,430
Right. Because I replaced M and sci fi.

220
00:33:57,670 --> 00:34:04,810
Some functions that are now random variables because they depend on the sample x i go from one to m.

221
00:34:08,410 --> 00:34:20,600
So if I were in the approximating things in the calculus sense, I would need some convergence of email and tie in to the site.

222
00:34:20,620 --> 00:34:25,209
Right. That's the essence of approximation as that goes to infinity.

223
00:34:25,210 --> 00:34:37,000
That would be an approximate sequence of functions. But because they are random, I need this convergence in some probabilistic sense.

224
00:34:38,500 --> 00:34:42,280
So we want approximation.

225
00:34:45,060 --> 00:34:48,150
To be reasonable in the sense.

226
00:34:57,960 --> 00:35:10,330
That's. Uh. So the and minus and go to zero in probability.

227
00:35:14,040 --> 00:35:19,660
Maybe even almost truly if you want. But in some probabilistic sense.

228
00:35:22,250 --> 00:35:27,410
And these are both some functional norms.

229
00:35:32,590 --> 00:35:38,200
Because we are talking about convergence of functions rather than convergence of some numbers.

230
00:35:40,810 --> 00:35:48,370
And so one of the most common examples is the supernormal.

231
00:35:57,900 --> 00:36:02,340
So that would be the super over sata.

232
00:36:03,770 --> 00:36:07,130
Right. And. And. I'll say to.

233
00:36:08,030 --> 00:36:14,060
Minus theta upside and minus upside of theta.

234
00:36:23,700 --> 00:36:30,990
And because so whenever we use the soup norm, this another word for, uh, the convergence.

235
00:36:33,310 --> 00:36:38,130
Uh, so that has the sense of uniform convergence.

236
00:36:55,360 --> 00:36:58,990
So uniform. Typically, it's not a rigorous term.

237
00:37:02,050 --> 00:37:08,350
But it usually means simultaneously for values of a certain thing in a certain class.

238
00:37:08,350 --> 00:37:12,250
Right? He or we mean simultaneously for all of us.

239
00:37:32,530 --> 00:37:37,910
In states state again rigorous tests if you want the rigorous it's the emergence and so.

240
00:37:44,120 --> 00:37:49,130
Okay. So then we get to define the estimates now.

241
00:37:56,440 --> 00:38:00,519
Is solutions. So one of them,

242
00:38:00,520 --> 00:38:07,090
if we're looking at the function M So that's going to be the point of maximum or the theta

243
00:38:07,630 --> 00:38:14,020
now of this function of theta because a man is a function that's based on the sample.

244
00:38:15,190 --> 00:38:20,919
The solution to this problem will also be random and based on the sample.

245
00:38:20,920 --> 00:38:24,900
And so that's going to be an estimate. Right.

246
00:38:24,910 --> 00:38:28,540
Or we can define it is.

247
00:38:31,360 --> 00:38:36,480
Solutions to the equation. Say and I'll say to.

248
00:38:38,290 --> 00:38:45,740
Equals, right? And this guy will be called Mr. Nader.

249
00:38:50,470 --> 00:39:00,380
This will be called z estimate of. Now the reason for the name.

250
00:39:00,390 --> 00:39:05,420
So for them it's clear that's from the word maximum for the Z.

251
00:39:06,050 --> 00:39:13,250
The letter probably comes from the fact from the theory of estimating equations and from the fact they aren't.

252
00:39:14,420 --> 00:39:16,999
So if you can see the maximum likelihood estimation,

253
00:39:17,000 --> 00:39:25,580
where AM is somehow related to your likelihood that you usually do the score equation and scores are usually denoted by a letter Z.

254
00:39:25,640 --> 00:39:35,590
Right. Now if you're not deriving the score from the like.

255
00:39:35,950 --> 00:39:40,630
I guess this is confusing notation and confusing history,

256
00:39:40,660 --> 00:39:50,040
but this is how they're denoted for you because the score for maximum likelihood is usually you if you are within this theory, right?

257
00:39:51,940 --> 00:40:03,389
But anyway. So the scores from estimating equations are often Z's right, and the scores in statistical testing like in the T test, for example.

258
00:40:03,390 --> 00:40:08,150
Right. That's also the Z statistic. That's normal.

259
00:40:08,150 --> 00:40:12,690
Zero one. It's. Okay, then.

260
00:40:13,290 --> 00:40:16,290
So the hope is that.

261
00:40:19,340 --> 00:40:31,040
That's because my yeah man goes to them in some sense and sigh and goes to say in some sense they're the solutions.

262
00:40:33,760 --> 00:40:43,290
This should be. That's the solution of the problem that involves a man should go to the solution.

263
00:40:46,820 --> 00:40:55,550
To not sit and or say in equal zero should go in some sense.

264
00:40:58,760 --> 00:41:08,800
And the solutions will be the estimates rate. That stayed in my head would go in some sense for it.

265
00:41:08,850 --> 00:41:23,160
So it's clear what sense we want. We want it to be consistent so it goes in probability to the solution of maximum am and cycles zero.

266
00:41:23,550 --> 00:41:30,690
And the solution of those is the true parameter value. Right.

267
00:41:30,860 --> 00:41:37,790
So in other words, once again, so the big picture is that, uh, so we start with the true problem.

268
00:41:38,750 --> 00:41:44,300
We characterize the true parameter value as a solution to some true problem,

269
00:41:44,840 --> 00:41:53,480
and we approximate this true problem using values of the sample, most often by replacing expectations by the empirical.

270
00:41:55,060 --> 00:42:08,830
And then because the true problem is approximated well in some sense by the approximating problem that's now random and based on the sample,

271
00:42:09,130 --> 00:42:15,160
then we hope that the solutions to the problem behave the same way and converge.

272
00:42:15,820 --> 00:42:20,830
The solution to the approximate problem converges in probability to solution of the true problem.

273
00:42:23,730 --> 00:42:39,160
So that's that's the philosophy. Now the you can consider the framework of M estimation and Z estimation separately,

274
00:42:39,220 --> 00:42:45,280
but also you could connect them and connecting them would be representing.

275
00:42:49,490 --> 00:42:53,560
We can. Present.

276
00:42:59,180 --> 00:43:03,660
And this to me is. The estimate.

277
00:43:05,220 --> 00:43:11,550
Right. So I could always. Uh, take the function, Cy.

278
00:43:14,120 --> 00:43:23,760
Is. The M. That depends on the parameter theta of theta.

279
00:43:25,260 --> 00:43:30,840
Yeah, it's, uh. Uh. So hoping that this is the necessary condition.

280
00:43:34,840 --> 00:43:45,660
Conditions for the maximum. Say it to be maximum.

281
00:43:50,470 --> 00:43:55,530
I'll say to. He's.

282
00:43:56,960 --> 00:44:00,360
So I say to. Equals zero.

283
00:44:04,340 --> 00:44:16,390
What would they say? So then if this works, then my estimate can be representative this as the estimate that an example of this would be.

284
00:44:16,430 --> 00:44:19,460
So you have a maximum likelihood you derive the score equation.

285
00:44:20,150 --> 00:44:27,590
Now looking at the score equations and finding a theta that satisfies it, that would be the z estimation problem.

286
00:44:32,720 --> 00:44:38,200
Now. The other way also works. So we present.

287
00:44:43,010 --> 00:44:47,590
The estimate is an estimate of.

288
00:44:50,820 --> 00:44:54,690
So you can, for example, always take.

289
00:44:58,280 --> 00:45:05,240
And we'll say to. Equal to minus some kind of a norm of say.

290
00:45:07,380 --> 00:45:11,760
The norm is not negative. It's equal to zero by the axioms of norm.

291
00:45:13,230 --> 00:45:24,270
If. And only if. The argument. Is zero rights inside equal and zero is the Zestimate.

292
00:45:27,060 --> 00:45:35,070
And this is not the only way. There is, of course, infinitely many ways to represent one thing as the other.

293
00:45:39,830 --> 00:45:44,780
Infinitely many. Presentations like this.

294
00:45:53,860 --> 00:45:57,220
The only other note about Dave is that.

295
00:45:59,490 --> 00:46:11,860
So it's. So it's not always that you can represent the score equation is.

296
00:46:13,590 --> 00:46:19,470
So if you already have a score equation, it's not always that you can represent it as a maximum function.

297
00:46:27,210 --> 00:46:40,990
Except through the normal arguments. Okay.

298
00:46:41,000 --> 00:46:45,350
So the most common procedure of the most common.

299
00:46:49,690 --> 00:46:57,320
We? Specify the recipe a little bit.

300
00:47:06,400 --> 00:47:09,700
So I have a random variable x.

301
00:47:10,630 --> 00:47:15,310
I have some am theta of x.

302
00:47:17,820 --> 00:47:24,550
Where? Yeah, I'd say it's a function.

303
00:47:32,880 --> 00:47:37,620
And then so we can define of theta.

304
00:47:38,510 --> 00:47:44,730
There's an expected value of. And say to thanks.

305
00:47:52,460 --> 00:47:59,510
So then my end of theta will be the empirical expectation.

306
00:48:01,430 --> 00:48:08,310
Of these functions. This is.

307
00:48:10,400 --> 00:48:15,600
Well, either one of em and some of I.

308
00:48:17,800 --> 00:48:28,540
Two. Of course, I. Or it's if we're using empirical process notation, that's B of X of M of x.

309
00:48:35,530 --> 00:48:40,010
He and. This guy is.

310
00:48:40,250 --> 00:48:45,290
So this is B and this is B. To.

311
00:48:49,660 --> 00:48:54,850
Same with ACI. So you can say that I have a function.

312
00:48:56,140 --> 00:49:10,230
I. Of X. This is a function for the Z estimate and we can define PSI.

313
00:49:11,600 --> 00:49:14,690
Is. Empirical expectations.

314
00:49:23,260 --> 00:49:30,380
If I. A12x writes This is one of and some of the high.

315
00:49:32,100 --> 00:49:38,010
I say this is the same as and.

316
00:49:39,040 --> 00:49:48,270
Phys ed. And the tall one.

317
00:49:55,090 --> 00:49:59,150
He's expected to. The truth.

318
00:50:01,430 --> 00:50:08,690
Eight A12x. He sees in a different foundation.

319
00:50:10,190 --> 00:50:26,250
She later. Okay.

320
00:50:29,570 --> 00:50:32,770
So that was the general framework. Yeah.

321
00:50:32,780 --> 00:50:43,850
And uh, so the first fact will be a theorem that gives general conditions for the estimate to be consistent.

322
00:50:59,490 --> 00:51:03,330
And the connection between the.

323
00:51:03,720 --> 00:51:10,320
So the logic of the theorems will be that because we cannot represent them estimates,

324
00:51:10,320 --> 00:51:20,490
the estimate and the other way around so we can prove the theorem like consistency in a static distribution.

325
00:51:20,820 --> 00:51:23,220
For one of them, let's say for the estimate.

326
00:51:23,820 --> 00:51:32,340
And then they argue that because of the connection between them, we can just prove the result for the other one,

327
00:51:32,340 --> 00:51:37,130
for the Z estimate, by checking the conditions of the theorem for the M estimate.

328
00:51:37,170 --> 00:51:47,550
So that's going to be the logic. And will formulate the conditions for consistency and go the proof probably after the break.

329
00:55:02,230 --> 00:57:51,840
Yeah. Okay.

330
00:57:52,650 --> 00:58:09,530
Shall we continue? So we said that the ingredients are to a good estimate of that amount approximates em in some probabilistic sense.

331
00:58:10,190 --> 00:58:16,280
So these conditions will be called uniform law of large numbers.

332
00:58:27,280 --> 00:58:32,140
So why we. Why it's not enough to have a simple law of large numbers.

333
00:58:32,650 --> 00:58:37,210
Point wise. Right. So we could require that now.

334
00:58:37,990 --> 00:58:48,610
So my function as a man for a particular theta, uh, follows the law of large numbers and converges in probability to some m of theta,

335
00:58:48,970 --> 00:58:53,620
and that for every theta point Y is in the parameter space.

336
00:58:54,460 --> 00:59:00,520
That won't be enough because we are characterizing the solution as a solution

337
00:59:00,520 --> 00:59:07,550
to maximizing m and maximizing a function depends on its behavior everywhere.

338
00:59:07,600 --> 00:59:14,229
Right. I guess if I have a point of maximum, but then my function increases.

339
00:59:14,230 --> 00:59:18,100
I need to know that it increases probably above that maximum.

340
00:59:20,530 --> 00:59:28,300
Two. So it would matter, right? The global behavior of my function, not just point by point.

341
00:59:29,140 --> 00:59:33,940
So that's why there is the word uniform that's key here.

342
00:59:35,440 --> 00:59:39,340
And that means that we're taking the norm.

343
00:59:41,640 --> 00:59:50,280
In the parameter space of the difference between a man of theta minus m of the aid.

344
00:59:50,280 --> 00:59:58,760
And this is all the theta. This is supposed to go in probability to zero, right.

345
00:59:58,780 --> 01:00:03,540
So this is a global. Property.

346
01:00:08,970 --> 01:00:16,560
The functions. Yeah, man.

347
01:00:16,780 --> 01:00:28,310
And that is needed. Four maximization problems.

348
01:00:34,240 --> 01:00:36,059
Because from some convergence of functions,

349
01:00:36,060 --> 01:00:42,600
we want to imply convergence of solutions to the maximum problem and those solutions to the maximum problem.

350
01:00:43,110 --> 01:00:50,100
They depend on the behavior of these functions everywhere. That's why the condition is a bit stronger than point wise.

351
01:00:52,620 --> 01:00:56,820
Okay. The second condition is called the proximate.

352
01:01:01,100 --> 01:01:09,850
Maximize the. Okay.

353
01:01:10,540 --> 01:01:16,630
So you say to enhance ease and maximize our.

354
01:01:21,440 --> 01:01:26,170
Of the function. A man. Right.

355
01:01:26,320 --> 01:01:30,000
That's by definition how it was defined. Right.

356
01:01:30,010 --> 01:01:36,220
And the true parameter value is not a maximizer.

357
01:01:37,980 --> 01:01:43,220
In general. Of their men.

358
01:01:45,980 --> 01:01:50,060
But we want these two values.

359
01:01:50,390 --> 01:01:57,890
So the man of his heart and the man of faith are to be sufficiently close in probability.

360
01:01:59,210 --> 01:02:10,850
And the condition is, well, if I plug in the estimate, the theta in here, that is exactly the function that maximizes that man.

361
01:02:11,270 --> 01:02:16,970
So that's the largest possible value for a man that I can achieve in my parameter space.

362
01:02:17,690 --> 01:02:28,100
Right. And if I plug in am and I'll see the star, theta star, the true parameter value into the approximate function.

363
01:02:28,950 --> 01:02:32,480
And I'm getting something suboptimal for the man. Right?

364
01:02:33,200 --> 01:02:41,240
So generally a smaller value. But I want them to be sufficiently close so that these differences.

365
01:02:43,010 --> 01:02:47,810
Asymptotically, in probability. Uh, yeah.

366
01:02:48,410 --> 01:02:55,610
Uh, that it goes to zero. Right. So the op of one is something.

367
01:02:58,700 --> 01:03:02,270
That goes to zero in probability.

368
01:03:09,240 --> 01:03:17,820
So it's a concept similar to the one in calculus when we considered things that go to zero.

369
01:03:20,270 --> 01:03:27,350
Except that the letter indicates that this is considered yes, convergence in probability rather than calculus type convergence.

370
01:03:31,800 --> 01:03:38,880
And yet another condition is the separated maximum.

371
01:03:50,420 --> 01:03:54,410
And will first do it in a graphical sense. Right.

372
01:03:54,420 --> 01:04:00,100
I can define the. Perhaps a neighborhood.

373
01:04:07,130 --> 01:04:17,560
Of, say, the star. And that's the set of values of theta such that the difference between.

374
01:04:19,620 --> 01:04:24,880
Theta Star and theta. So that's the distance between. So you don't see the star.

375
01:04:30,680 --> 01:04:42,710
Well outside of the it, I should say. So if I do less of equal epsilon, that would be a neighborhood of Theta Star.

376
01:04:43,190 --> 01:04:48,409
If I do agree to equal Epsilon, that's everything other than the neighborhood, right?

377
01:04:48,410 --> 01:04:55,320
So that's outside of the neighborhood. Then suppose I have some function like this.

378
01:04:56,880 --> 01:05:01,170
And let's say this is my arm and.

379
01:05:03,430 --> 01:05:08,860
It's maximized its say the star by the construction of M.

380
01:05:10,030 --> 01:05:21,640
I create an absolute neighborhood around it so I have absolutely here have absolutely here so that's the interval.

381
01:05:25,450 --> 01:05:38,460
So that's the neighborhood. They just started the absolute right.

382
01:05:39,050 --> 01:05:43,490
And what I am interested in is the outside.

383
01:05:45,320 --> 01:05:56,730
So this area. Then of course, I have the maximum.

384
01:05:58,590 --> 01:06:09,240
That's right here. And this is and I'll say the star again by construction because Theta Star is the solution to the maximum problem.

385
01:06:11,970 --> 01:06:15,900
Right. So it would be separated if.

386
01:06:16,620 --> 01:06:21,919
So if I take the values in my outside neighborhoods.

387
01:06:21,920 --> 01:06:27,380
So the ones that shaded. And if I can squeeze.

388
01:06:29,840 --> 01:06:35,510
The condition is that we can squeeze a line.

389
01:06:40,800 --> 01:06:57,100
Between. And of, say, the star and maximum of a seat outside of this neighborhood.

390
01:07:07,990 --> 01:07:15,330
They tell. For any absolute greater than zero.

391
01:07:18,040 --> 01:07:22,299
So that means that so I can define.

392
01:07:22,300 --> 01:07:26,290
So this is a particular line in this picture.

393
01:07:38,590 --> 01:07:44,340
Such that. So my. Maximum value.

394
01:07:49,080 --> 01:07:57,090
Um. Of theta. Of theta in this neighborhood outside.

395
01:07:59,780 --> 01:08:04,170
Epsilon. It's going to be. Less than.

396
01:08:08,340 --> 01:08:15,010
See the star. So that's one way to say it.

397
01:08:16,000 --> 01:08:26,410
Right. And I see that my values, uh, in these outside neighborhoods, they are in this particular example right here,

398
01:08:26,770 --> 01:08:33,760
right here, right in between the maximum value outside of the epsilon.

399
01:08:34,000 --> 01:08:37,000
Right. Which is these two values. Right.

400
01:08:37,930 --> 01:08:41,910
And the value of the maximum. I can put a line between them.

401
01:08:42,100 --> 01:08:48,390
Right. So this is where we call the maximum.

402
01:08:48,960 --> 01:08:52,800
There's a separate one. There's one other form of that condition.

403
01:08:53,880 --> 01:09:03,340
So this is one form. And another form is the Epsilon Delta language.

404
01:09:03,350 --> 01:09:16,730
So for any epsilon greater than zero, there exists ETA greater than zero such that and of theta is going to be less.

405
01:09:18,340 --> 01:09:21,610
Then and. They just are.

406
01:09:22,980 --> 01:09:28,830
Minus eight. And this for any state such that.

407
01:09:30,190 --> 01:09:37,660
In the north, the distance between theta star and theta is greater equal lives.

408
01:09:38,650 --> 01:10:00,500
So it's another form of this condition. Then we can make a note that separated maximum.

409
01:10:06,830 --> 01:10:22,060
Implies that the maximum is unique. Because if it's not unique, I can separate it.

410
01:10:22,230 --> 01:10:29,770
Right. So if I have a situation where let's say I have two maximum and they are the same level.

411
01:10:31,990 --> 01:10:39,879
Like this. Right. Then I can take the sufficiently small epsilon.

412
01:10:39,880 --> 01:10:46,750
So I zoom in on the first one. Then the values outside will have a maximum.

413
01:10:46,750 --> 01:10:51,490
That's exactly equal to this one, and I won't be able to squeeze a line in between them.

414
01:10:57,960 --> 01:11:11,300
So when maximum is North Sydney? The separated.

415
01:11:16,350 --> 01:11:32,310
Maximum condition is evaluated. It can also happen if I have an asymptote.

416
01:11:33,030 --> 01:11:37,140
So for example, if my function looks like this.

417
01:11:38,580 --> 01:11:44,639
Well, it attains the maximum at a certain level and then never attains it.

418
01:11:44,640 --> 01:11:50,970
But as it doesn't go like this, I won't be able to separate the maximum either or.

419
01:11:51,010 --> 01:12:03,610
Right. Because well, I can't find the data such that the symbol over the rest of it is strictly below this point.

420
01:12:08,210 --> 01:12:14,390
And that means that the separated max condition is somewhat stronger than the unique maximum group.

421
01:12:15,110 --> 01:12:22,520
So that means for consistency we are acquiring a bit more than the unique maximum.

422
01:12:29,800 --> 01:12:36,810
It's another note. We are requiring.

423
01:12:44,540 --> 01:12:48,990
More. You need connection here.

424
01:12:57,770 --> 01:13:03,530
Okay. So. I will give the proof next time.

425
01:13:05,700 --> 01:13:09,870
So then let me just state. The serum.

426
01:13:11,580 --> 01:13:15,020
So theta in half will go in probability.

427
01:13:15,040 --> 01:13:25,030
Two Theta Star. Next.

428
01:13:36,900 --> 01:13:48,250
And I should say that the. The uniform law of large numbers is pretty hard to verify, and it's a pretty strong condition.

429
01:13:48,430 --> 01:13:52,110
So we will later formulate a way to relax its.

430
01:13:54,510 --> 01:13:55,590
This is it for today.

