1
00:00:44,990 --> 00:00:49,760
Okay. So the stuff related to calculus.

2
00:00:50,030 --> 00:01:01,760
So I'm working on a document. The House will have a refresher.

3
00:01:25,410 --> 00:01:31,030
That's because. Okay.

4
00:01:31,810 --> 00:01:35,040
So just like I did for the midterm.

5
00:01:35,920 --> 00:01:39,520
So there will be a detailed thing with table of contents and so on.

6
00:01:40,900 --> 00:01:44,590
And to do this, I will go through all of the course.

7
00:01:44,590 --> 00:01:50,920
And as I do that, I will mark myself things that I basically take for granted some calculus.

8
00:01:51,550 --> 00:01:56,670
And I'll make a refresher with all of them explained. Okay.

9
00:01:57,210 --> 00:02:01,110
That's that's one thing. So another thing.

10
00:02:01,120 --> 00:02:08,710
Uh, so another document. Uh. We have questions and answers.

11
00:02:09,930 --> 00:02:19,710
So I'm basically collecting anything. I receive an email and sometimes noting them as as you ask them during office hours.

12
00:02:20,610 --> 00:02:33,819
So this is also coming. And it won't be complete until the very end of the course,

13
00:02:33,820 --> 00:02:43,360
but at least it will be mostly there and I will update it closer to the final so that it is complete in the end.

14
00:02:44,890 --> 00:02:51,460
It's another thing then. So as far as more problems of different kinds.

15
00:02:52,150 --> 00:02:56,380
So there's now four sets of problems or.

16
00:02:58,470 --> 00:03:01,690
That I posted and I posted them.

17
00:03:03,390 --> 00:03:24,940
In final exams folder. They're on the canvas.

18
00:03:28,050 --> 00:03:37,050
On the exams. Okay.

19
00:03:37,110 --> 00:03:49,980
So some of them you can solve right now, and for some of them, uh, some further material in the future, lectures will be required.

20
00:03:51,340 --> 00:04:01,610
Um. And so because this is far more problems than there would be for a single homework that we have time for.

21
00:04:02,360 --> 00:04:05,600
So I decided we won't do another grade, that homework.

22
00:04:06,830 --> 00:04:11,720
So this is going to be eight. And what will happen with this one?

23
00:04:12,170 --> 00:04:17,660
So I'll let everybody pick and choose what they want to solve from the four sets.

24
00:04:19,100 --> 00:04:28,340
I'll keep the suspense for a while, then Pulse Solutions, and I'll let everybody discuss these things during office hours.

25
00:04:28,960 --> 00:04:36,590
And if needed, we can arrange another optional time for a seminar where I go through the solutions I've never seen.

26
00:04:38,510 --> 00:04:44,630
Okay. So we can briefly look at those problems.

27
00:04:50,470 --> 00:04:55,390
So this is sort of the first set of problems.

28
00:04:55,840 --> 00:05:01,360
So I found that perhaps I should talk a little bit about the homeworks.

29
00:05:02,920 --> 00:05:06,840
So the homework two and three were pretty good.

30
00:05:06,850 --> 00:05:12,190
So the only problem where there were some issues was the one with a maximum.

31
00:05:12,640 --> 00:05:20,510
Um. Of a bunch of variables that was I think from homework three it.

32
00:05:21,760 --> 00:05:26,050
So there was and that's a maximum.

33
00:05:27,200 --> 00:05:35,700
I. One, two, and of some variables x.

34
00:05:35,710 --> 00:05:47,640
I. And so the typical issues with that was to assume, for example, that a man is bound.

35
00:05:47,640 --> 00:05:57,630
It's. It was expected the value of exide's assets.

36
00:06:04,290 --> 00:06:22,700
And this is not true. Because if you think about the maximum, so it's basically the largest or the statistic rates from a bunch of random variables.

37
00:06:23,030 --> 00:06:38,230
If you take for example. It's a fairly normal distribution, regardless what mean and variance.

38
00:06:41,220 --> 00:06:44,270
The, uh. Um. And, uh.

39
00:06:44,720 --> 00:06:48,080
We'll go to infinity. Asymptotically. Right.

40
00:06:54,110 --> 00:07:02,540
And that's because as as you and I ran the variables to its independence identical to distributed.

41
00:07:02,540 --> 00:07:10,750
There's a larger and larger chance that they will cover all of the support for the normal distribution that support this supplemental.

42
00:07:19,630 --> 00:07:30,320
And also. It is not true that so is expected to value its.

43
00:07:32,030 --> 00:07:37,100
Exists the. The variance exists.

44
00:07:40,090 --> 00:07:47,120
This is also not true. In general, it's.

45
00:07:53,180 --> 00:08:00,650
Because. If Saya or the moment exists.

46
00:08:09,610 --> 00:08:13,600
The lower order moments.

47
00:08:20,980 --> 00:08:32,040
Exist, but not vice versa. In general.

48
00:08:35,470 --> 00:08:40,390
So in other words, the existence of an expectation does not guarantee existence of variables.

49
00:08:40,390 --> 00:08:43,440
And so. Uh.

50
00:08:43,930 --> 00:08:47,200
So. That's about it.

51
00:08:47,680 --> 00:08:51,700
Uh, nothing in homeworks two and three caused problems.

52
00:08:52,930 --> 00:08:59,840
Apart from this one, mostly. Okay.

53
00:09:00,380 --> 00:09:04,760
So back to the four sets of problems.

54
00:09:05,660 --> 00:09:28,410
So let's take a look. So in the first one and in general.

55
00:09:28,410 --> 00:09:35,730
So I find that the most difficulty so far is proves that involves some kind of inequalities.

56
00:09:37,230 --> 00:09:46,110
And that's related to different kinds of convergence. So I had a few problems with this in mind.

57
00:09:46,770 --> 00:09:55,740
Uh, so first one is, uh, so if the variance goes to zero, that's sufficient for convergence and probability to zero.

58
00:09:58,860 --> 00:10:07,350
So that's one thing and it's basically an application of the emissions that leads to that then.

59
00:10:07,920 --> 00:10:13,320
So we had a bunch of uh, uh, continuous mappings statements.

60
00:10:15,090 --> 00:10:25,919
And uh, so the second one is try to prove that if I have convergence in probability for two random variables,

61
00:10:25,920 --> 00:10:35,920
then that product also converges in probability. So there's kind of a bunch of inequalities and the idea that uh, uh,

62
00:10:36,390 --> 00:10:43,680
you can separate things into two kinds of events that is bigger than the previous one.

63
00:10:44,160 --> 00:10:47,790
And each is an event of the type greater than the epsilon over two.

64
00:10:47,820 --> 00:10:52,530
So we had this idea a couple of times already.

65
00:10:52,540 --> 00:11:01,380
So that is a very useful one in the number of proofs and uh, at least the way I did it in this one also.

66
00:11:04,180 --> 00:11:10,570
Then, uh, so finding, uh, characteristic functions.

67
00:11:12,250 --> 00:11:17,800
So that's basically taking expectation of E to a t x,

68
00:11:18,610 --> 00:11:28,300
then recognizing that how they are related to moments and then finding the moments of random variable using the characteristic function.

69
00:11:29,050 --> 00:11:37,300
Right? There could be eventually a bunch of problems like these where things are relatively easy and in a closed form.

70
00:11:37,300 --> 00:11:43,900
Right. And that relates to the usual distributions like this one is exponential, right?

71
00:11:45,340 --> 00:11:56,110
So that could be similar for, I don't know, the distributions that are easy, uh, uniform, uh, uh, fossil and binary.

72
00:11:56,800 --> 00:12:01,090
So all of this works well for this type of problem.

73
00:12:01,270 --> 00:12:05,120
So this is just an example then, um.

74
00:12:08,170 --> 00:12:11,990
So here is another one that needs inequalities.

75
00:12:12,070 --> 00:12:21,190
And so you have a sequence of random variables and for some sequence you have

76
00:12:21,490 --> 00:12:27,760
some sequence of numbers right from the set of natural numbers such that.

77
00:12:31,190 --> 00:12:34,250
Okay. So I missed the statement of the problems.

78
00:12:37,490 --> 00:12:54,970
It's probably the. Yeah.

79
00:12:55,130 --> 00:13:05,330
This is what I missed. So you have the maximum between two numbers going to zero, right?

80
00:13:05,810 --> 00:13:11,530
So in case the subsequent. So they have a sequence of random variables.

81
00:13:11,530 --> 00:13:15,940
Then you have a sequence of natural numbers that skip a bunch of them in between.

82
00:13:16,750 --> 00:13:23,500
So if you take the maximum over the difference between accept the previous

83
00:13:23,680 --> 00:13:30,580
sequence number and the ones that fall into this interval of numbers that I skip.

84
00:13:31,690 --> 00:13:42,160
So if this thing goes to zero, almost truly in the sub sequence of those, almost surely to some random variable.

85
00:13:42,790 --> 00:13:49,090
Then the original random variable also goes almost surely to x, right?

86
00:13:50,930 --> 00:14:01,129
So we had a theory in the lectures that if if exam converges in probability to X, then you can choose a sequence that converges almost surely.

87
00:14:01,130 --> 00:14:08,690
Right? So here it's not even available that and converges in probability to x.

88
00:14:08,780 --> 00:14:13,570
Otherwise this will be. Right.

89
00:14:13,620 --> 00:14:18,090
But this condition with the maximum actually does that.

90
00:14:19,770 --> 00:14:24,660
The truth is that we do have almost full convergence again.

91
00:14:25,950 --> 00:14:30,690
The solution is pretty short and involves inequalities of the usual.

92
00:14:51,740 --> 00:14:58,880
And so another thing is the taking inverse of a characteristic function.

93
00:14:59,420 --> 00:15:02,990
So if I give you a characteristic function, find the random variable that.

94
00:15:05,750 --> 00:15:08,780
This is a characteristic function of. Now,

95
00:15:08,780 --> 00:15:13,909
I know I told you that we're not using the general formula for the universe because it

96
00:15:13,910 --> 00:15:20,000
requires a bunch of knowledge from complex variable calculus that most of you don't have.

97
00:15:20,960 --> 00:15:27,440
But sometimes it's possible to do more elementary mathematics like series and so on.

98
00:15:28,640 --> 00:15:39,590
And in this particular case, there's a hint that you can relate these characteristic function to a geometric progression, basically.

99
00:15:41,030 --> 00:15:45,829
So if you want to do this, invert the characteristic function by elementary means.

100
00:15:45,830 --> 00:15:55,580
You need to recognize that it is E of its x for some kind of x and is that x is discrete.

101
00:15:56,480 --> 00:16:04,490
So you can recognize that form using series and expanding the characteristic function somehow.

102
00:16:05,970 --> 00:16:18,260
Right. This particular case, if you recognize that this formula is can be passed as some kind of a sum over a geometric distribution,

103
00:16:19,370 --> 00:16:24,390
and this would allow you to identify the X. Behind it.

104
00:16:29,550 --> 00:16:35,770
And then so some of these problems are from past final exam like this one.

105
00:16:36,580 --> 00:16:45,980
So let X have a distribution. Explain why the law of large numbers and central theory don't work to estimate the mean.

106
00:16:46,060 --> 00:16:49,600
Right. And that's because the true meaning does not exist, right?

107
00:16:52,780 --> 00:17:01,940
Let alone develop the various rights. The central limit theorem of the variance does not exist.

108
00:17:03,260 --> 00:17:06,650
We can do it. Lot of large numbers if the mean is infinitely large.

109
00:17:08,240 --> 00:17:11,470
But in the end, this is a calculus fact. Right.

110
00:17:11,840 --> 00:17:15,860
When integrals converge and when they diverge.

111
00:17:16,040 --> 00:17:24,610
Right. So four just like with series, right?

112
00:17:24,630 --> 00:17:30,030
So you need the thing under the integral to go to zero fast enough in the tails.

113
00:17:30,930 --> 00:17:45,809
So that convergence happens and that happens when you have tables behaving like one over x one plus epsilon for some epsilon greater than zero.

114
00:17:45,810 --> 00:17:51,150
So this is not going to be the case here because as you take expectation, you multiply by X.

115
00:17:51,830 --> 00:18:01,400
So the whole thing will behave as a one over X in the tail and that's to slow convergence to zero for the integral to exist.

116
00:18:09,500 --> 00:18:22,580
That is. So this is a characteristic of a distribution that's called coefficient of variation, and that's about how s shape the distribution is.

117
00:18:23,930 --> 00:18:29,210
Right. So it's a ratio of sigma to the mean.

118
00:18:29,390 --> 00:18:42,230
Right. So if you have a distribution that's approaching a constant rate and the general trend of variable, then Sigma would be going to zero, right?

119
00:18:42,230 --> 00:18:45,890
So then efficient, the variation would be going to zero.

120
00:18:46,190 --> 00:18:50,660
And you will have just the opposite of what looks like this.

121
00:18:50,960 --> 00:18:54,980
Right. Simply a jump. A single jump.

122
00:18:55,370 --> 00:18:59,840
And as you make it more spread out.

123
00:18:59,930 --> 00:19:07,480
So your sigma, your. And if one jump will turn into a mass function and so on.

124
00:19:07,620 --> 00:19:20,990
Right. And then it also matters for the CV where the Middle East or how far to the right of this whole action is taking place.

125
00:19:21,770 --> 00:19:31,680
Right. So an obvious estimate for it is, uh, well, the sample, uh, uh, standard deviation of the sample mean the ratio, right?

126
00:19:33,480 --> 00:19:37,980
So trying. And the problem is about arguing for.

127
00:19:38,470 --> 00:19:46,770
Consistency. Right. And of course, so you have laws of large numbers for numerator and denominator.

128
00:19:46,930 --> 00:19:50,070
Right. But making that work in combination.

129
00:19:51,730 --> 00:19:57,910
Using properties of convergence and probability space can do this.

130
00:19:58,540 --> 00:20:07,240
So that's the first set. So the second set, so towards.

131
00:20:08,260 --> 00:20:11,440
So we have the few lectures we have ahead of us.

132
00:20:12,730 --> 00:20:24,570
We will move into the nonstandard central limit theorems with some dependance, uh, and perhaps for uh, with independence.

133
00:20:24,580 --> 00:20:29,890
But when, uh, uh, when the variables say not identically distributed.

134
00:20:32,640 --> 00:20:40,550
And then we'll move to the two general tools of war recipes of deriving good estimates.

135
00:20:40,590 --> 00:20:51,490
And that's the so-called Z estimation where an estimate is a solution to some equation that involves sample values and estimation.

136
00:20:51,720 --> 00:21:00,000
And basically a related thing where you you have a function that depends on simple values and you need to maximize it to get the estimates right.

137
00:21:00,000 --> 00:21:04,930
Then of course, the prime example of that is the. Right.

138
00:21:04,970 --> 00:21:09,130
So there are a bunch of problems. Uh, where?

139
00:21:09,370 --> 00:21:16,870
Uh. So you just use Emily theory to derive an estimate and show it's, uh,

140
00:21:17,110 --> 00:21:25,450
consistent and derive their symbolic distribution, uh, for something simple, like an exponential.

141
00:21:26,810 --> 00:21:30,380
And so all the examples could be put on.

142
00:21:30,830 --> 00:21:40,850
So whenever you can get things into closed form. So that's kind of a thing that you would consider.

143
00:21:41,550 --> 00:21:46,850
Towards the end of the course? Not right now, but I still put it out.

144
00:21:48,780 --> 00:21:54,870
And so the problem that sounds easy but isn't easy.

145
00:21:55,260 --> 00:21:59,900
Yeah, that's proof that median estimate.

146
00:22:00,110 --> 00:22:10,050
It's the median. Let's say you have an absolutely continuous random variable X, you have a medium for the distribution of X, right?

147
00:22:12,390 --> 00:22:17,310
So prove that this is a consistent estimate of the true medium.

148
00:22:18,750 --> 00:22:24,410
So the point that separates. The 3.5 percentile.

149
00:22:25,460 --> 00:22:31,640
Of of the distribution system and establish its asymptotic distribution.

150
00:22:31,700 --> 00:22:37,370
Right. So at this point, we don't even know whether it's normal or not, and that will be normal.

151
00:22:39,000 --> 00:22:47,020
So the variance needs to be derived and so on. So it's spelled out a little bit into some problems.

152
00:22:49,690 --> 00:22:56,080
And there are a bunch of representations that's again, related more to the stuff that towards the end of the course.

153
00:22:57,600 --> 00:23:04,530
For example, showing that the medium as an estimate is a solution to this equation.

154
00:23:05,910 --> 00:23:14,720
So that casts it as a Z estimate also shows that the median is an estimate,

155
00:23:14,730 --> 00:23:23,910
a is a maximizer to this kind of function that basically relates the problem to M estimation.

156
00:23:24,390 --> 00:23:30,000
We will have some general theorems for Z and estimation.

157
00:23:30,010 --> 00:23:34,290
So the trick is to apply it in this specific case.

158
00:23:34,290 --> 00:23:40,710
Right? And that will yield, uh, consistency and asymptotic normality.

159
00:23:41,610 --> 00:23:48,570
So at this point it seems as if quite. Surprising result, a non-trivial result.

160
00:23:48,920 --> 00:23:53,290
So how would you derive this entire distribution for the medium?

161
00:23:53,410 --> 00:23:58,270
Why the variances related to the PDA for the variable x?

162
00:24:00,730 --> 00:24:03,300
That's a pretty useful exercise.

163
00:24:04,250 --> 00:24:13,740
And it's an example of a problem that would not show up in a six on one course because it's nonstandard and needs some materials.

164
00:24:15,220 --> 00:24:19,330
It's coming. So that's the second one.

165
00:24:19,500 --> 00:24:27,730
So the third. So, again, something nonstandard.

166
00:24:30,310 --> 00:24:34,270
So it is known that ax is uniformly distributed.

167
00:24:35,400 --> 00:24:41,950
Uh, but I don't know. The parameter of this distribution parameter is the end of the interval.

168
00:24:43,240 --> 00:24:49,000
All right. So how do you get the family for that private?

169
00:24:49,040 --> 00:25:04,280
Uh. And, uh. Solving problems, not solutions.

170
00:25:05,090 --> 00:25:11,100
So now. So that this estimate is consistent.

171
00:25:11,820 --> 00:25:17,820
Now it is not a regular and will leave because it's the border of the support.

172
00:25:18,430 --> 00:25:21,180
It's just for the regular one.

173
00:25:21,720 --> 00:25:31,880
All the classical theorems, they need a parameter to be the internal points of their in of some kind of interval in the parameter space.

174
00:25:31,890 --> 00:25:35,760
So this is not going to be the case here. It's right.

175
00:25:37,750 --> 00:25:42,310
The age of of the support for that distribution.

176
00:25:45,000 --> 00:25:55,380
Finds this important distribution of it. Again, at this point, it's not very clear how to do that.

177
00:25:55,530 --> 00:26:08,080
There are some students here. And I want you to suffer just a little bit trying to do that before you look at the solution.

178
00:26:10,000 --> 00:26:13,270
So this is how the learning experience works.

179
00:26:16,450 --> 00:26:25,060
And then the fourth is kind of so I could have given it as a project to do, but we don't have the time for it.

180
00:26:27,370 --> 00:26:30,399
And it's a game and a nonstandard estimate there.

181
00:26:30,400 --> 00:26:37,480
In this case, I say my parametric estimate that it's a special case of smoothing techniques that's called moving average.

182
00:26:38,410 --> 00:26:50,020
So if you have a sample of a random variable, simple random sample from some random variable with some distribution.

183
00:26:52,120 --> 00:26:54,100
Yeah. It's absolutely continuous.

184
00:26:56,090 --> 00:27:07,680
And so you're basically using the empirical distribution function to estimate this distribution, non-prime metrically,

185
00:27:08,330 --> 00:27:18,200
but you're not happy with it because it's too variable and too junky and may be too granular, especially with small samples.

186
00:27:19,580 --> 00:27:29,890
So you want a smooth estimate before it's in the smooth estimate that would be estimating the PDF at some point x.

187
00:27:30,920 --> 00:27:40,290
Uh, by, uh, essentially taking, uh, the app, taking the view that you,

188
00:27:40,850 --> 00:27:46,640
if you have an empirical distribution function and you know, if you differentiate the smooth version of it,

189
00:27:47,130 --> 00:27:50,230
this is how you get the PDF right,

190
00:27:50,270 --> 00:27:55,190
but you can represent the derivative has a difference over the length of the interval in

191
00:27:55,190 --> 00:28:03,380
which the difference is taken in this window than the interval slide from left to right.

192
00:28:04,530 --> 00:28:09,350
Uh, because you are taking all points that are available to you, right?

193
00:28:09,440 --> 00:28:14,510
So that's. Sliding over the x axis.

194
00:28:15,050 --> 00:28:22,950
So this will create a smooth version of the PDF as an estimate.

195
00:28:24,370 --> 00:28:32,260
That's a moving average estimate from the PDF now show that this estimate is is consistent

196
00:28:33,220 --> 00:28:40,390
and like with all the smoothing estimates so you have a bias variance tradeoff usually.

197
00:28:40,660 --> 00:28:51,220
Right. And the precision of this approximation depends on how large the interval is, but also with large intervals you get bias.

198
00:28:54,510 --> 00:29:01,440
So for things to be precise, you need to let the size of the interval go to zero syntactically.

199
00:29:01,440 --> 00:29:10,130
But you can have that very fast because then you will just resort to the non parametric estimate.

200
00:29:10,980 --> 00:29:16,410
So there's usually a delicate plane. How fast these are, it's called bandwidth.

201
00:29:16,410 --> 00:29:28,290
The size of that interval goes to zero. And so for consistency, it's required to go to zero, uh, for the right.

202
00:29:28,290 --> 00:29:32,969
And so the consistency needs that the variance go to goes to zero.

203
00:29:32,970 --> 00:29:41,640
So this is advisable, right? So for it you just need the A and to go to zero for the variance.

204
00:29:41,640 --> 00:29:45,870
You need it to go to zero in such a way, not so fast.

205
00:29:45,870 --> 00:29:48,960
So that in times a goes to infinity.

206
00:29:53,100 --> 00:29:59,730
Because if you take the bandwidth to be too small, there will be too few points in it in that interval.

207
00:30:00,090 --> 00:30:03,420
And there's not enough sample size for consistency.

208
00:30:04,260 --> 00:30:09,090
So for consistency, you need information to grow asymptotically, right?

209
00:30:09,570 --> 00:30:13,250
So you shrink the interval for having bias in this,

210
00:30:13,860 --> 00:30:21,300
but you shrink it so that the number of observations in the interval still accumulates asymptotically at a certain rate,

211
00:30:21,930 --> 00:30:31,950
and that makes your variance goals go to zero. So that's basically dividing the expectation of this estimate.

212
00:30:32,550 --> 00:30:36,030
Now, of course, the this whole thing is is binary, right?

213
00:30:36,030 --> 00:30:42,930
So you have newly trials, you have each, uh, member of the sample ex.

214
00:30:42,940 --> 00:30:46,320
I either falls into this interval or doesn't.

215
00:30:47,130 --> 00:30:52,500
This is the count of how many fold divided by the total count of observations.

216
00:30:53,430 --> 00:31:03,020
So everything is done using their nuclear trials here up until this point, and it's a little bit further.

217
00:31:03,030 --> 00:31:07,730
But towards the end of the question. So you would need.

218
00:31:09,040 --> 00:31:16,760
Correlations as well. So argue it as it's a consistent estimate.

219
00:31:16,800 --> 00:31:24,660
Uh. Get, uh, asymptotic distribution.

220
00:31:25,740 --> 00:31:32,770
So again, uh, uh, based on the central limit theorem of some kind, right?

221
00:31:32,800 --> 00:31:41,840
But, uh, rate of convergence is the normalization that would stabilize the variance is not a standard rate.

222
00:31:41,850 --> 00:31:50,040
It's not just square to them. It's, it involves the bandwidth and how it goes to zero and so on.

223
00:31:51,160 --> 00:31:54,240
But then you can show that this thing will.

224
00:31:55,150 --> 00:31:59,200
We are still at the new normal with f being the.

225
00:32:01,670 --> 00:32:07,550
Sigma and then so you recognize that.

226
00:32:08,240 --> 00:32:18,860
So as you create this estimate, it's a function. And because it's a function and it depends on simple values as an estimate.

227
00:32:19,070 --> 00:32:23,090
So it's essentially a statistical process. So it has some correlation structure.

228
00:32:23,570 --> 00:32:27,650
So there's correlation between, uh.

229
00:32:28,730 --> 00:32:32,720
So if you have this estimate at the point. At the point why?

230
00:32:34,940 --> 00:32:41,570
So there's a variance between those estimates, right?

231
00:32:42,920 --> 00:32:54,950
And so using multivariate version of the central limit theorems and the sympathetic theory would allow you to get the permissions associated with it.

232
00:32:56,760 --> 00:33:00,360
For two points. Of course, it can be generalized.

233
00:33:02,130 --> 00:33:05,160
Into a of multivariate settings with more than two.

234
00:33:05,160 --> 00:33:08,460
But so it's enough for an exercise to do it with two.

235
00:33:10,070 --> 00:33:13,560
Uh, there is another quality that's needed for that.

236
00:33:13,740 --> 00:33:18,880
It's called equality. So in passing, there's a problem to prove it.

237
00:33:21,760 --> 00:33:27,100
And most inequalities are consequences of genes.

238
00:33:27,310 --> 00:33:32,780
So it's just finding. How to formulate them, Jensen.

239
00:33:33,520 --> 00:33:38,870
This. Then find the minimum distribution.

240
00:33:44,810 --> 00:33:48,140
And I think this is it as far as problems go.

241
00:34:40,260 --> 00:34:50,610
Okay. Then I wanted to briefly point you with two pieces of theory that are related to the material in this course.

242
00:34:52,400 --> 00:35:01,250
We don't really have the time for. So one is related to stochastic processes.

243
00:35:11,610 --> 00:35:20,460
And so when we define a stochastic process so it's basically a set of friend the variables indexed by.

244
00:35:22,360 --> 00:35:26,680
A continuous parameter that is usually the time.

245
00:35:40,400 --> 00:35:45,560
So you have an uncountable collection of random variables here.

246
00:35:45,830 --> 00:35:49,700
Now, of course it can be a process and discrete time.

247
00:35:49,700 --> 00:35:55,250
Then you just have a sequence. Right. So the things we were looking at like that same.

248
00:35:58,020 --> 00:36:04,530
And from one to infinity, so it can be considered as a process in discrete time.

249
00:36:27,260 --> 00:36:33,890
Okay. So then if we, let's say, start observing the process at time zero.

250
00:36:35,970 --> 00:36:39,530
And behave somehow. It starts from a certain value then.

251
00:36:41,560 --> 00:36:46,900
Has some trajectory. So this is a trajectory of a process.

252
00:36:55,390 --> 00:36:59,320
Uh, and that's essentially a sample.

253
00:37:00,730 --> 00:37:11,870
And the one from, uh. This friend variables it's to.

254
00:37:17,660 --> 00:37:22,880
You can choose a particular point in time. Let's say I have a specific T.

255
00:37:25,250 --> 00:37:31,520
And then let's say this is the current time I'm sitting here.

256
00:37:31,970 --> 00:37:36,010
I observed what happened to the process up until this point, but I don't know.

257
00:37:36,050 --> 00:37:39,070
The future is that I'm not a prophet. Right?

258
00:37:41,480 --> 00:37:49,520
So that creates a certain kind of processes where there's uncertainty in the future.

259
00:37:49,530 --> 00:37:58,060
It's still a random thing, right? And they can take, for example, an expectation of the future trajectory.

260
00:37:59,500 --> 00:38:08,920
But because I want to make it relevant to the observations I already have, I would condition on what I have observed so far.

261
00:38:09,250 --> 00:38:17,960
Right. So that brings us to. The concept of a history of a process up to time to.

262
00:38:26,110 --> 00:38:29,460
So t it has a special name.

263
00:38:30,450 --> 00:38:37,860
That history is, of course, random. So it has a special name that's called Filtration.

264
00:38:45,570 --> 00:39:01,410
With notation beautiful f t and that's basically a sigma algebra generated by the random variables I observed.

265
00:39:02,910 --> 00:39:10,140
So that's X. Let's say tell. The town doubles from 0 to 80.

266
00:39:14,970 --> 00:39:19,650
Right, because every point in here the value of the process is a random variable.

267
00:39:20,130 --> 00:39:25,650
I observe all the variables from zero to then I can.

268
00:39:26,220 --> 00:39:31,540
In the spirit of this course, I can create a sigma algebra generated by these random variables.

269
00:39:32,280 --> 00:39:36,880
So that's certainly a pretty, uh, rich collection, right?

270
00:39:36,910 --> 00:39:46,360
Because not only. The single one will generate Burrell Sigma algebra in this case a continuous process.

271
00:39:46,360 --> 00:39:59,170
But I have essentially the product of a product over a continuous set of those sigma algebra straight.

272
00:40:00,300 --> 00:40:04,650
But you can think of it. This is all possible.

273
00:40:06,090 --> 00:40:08,300
Probability all possible sets.

274
00:40:09,520 --> 00:40:17,500
Trajectory slide, there's a probability space for them if you want to define the distributional characteristics of the process.

275
00:40:18,970 --> 00:40:29,500
Um. There is a nesting to those signals, of course.

276
00:40:29,530 --> 00:40:29,830
Right.

277
00:40:30,400 --> 00:40:42,040
Because if I move my T to the right, I would be adding more and more variables to the sigma algebra and I would get a bigger one because of that.

278
00:40:42,700 --> 00:40:54,840
So there's a nesting, uh. Of histories so that if time will be part.

279
00:40:55,750 --> 00:40:59,770
50 where it is less than two.

280
00:41:12,480 --> 00:41:16,580
There's an important class of processes that are called martindale's.

281
00:41:21,760 --> 00:41:24,970
That are defined by the following property.

282
00:41:25,600 --> 00:41:34,990
So if I'm sitting at the pointy end, I'm looking at the increment of the process over the next small time interval.

283
00:41:36,890 --> 00:41:47,330
Sci fi condition on the history just before this point when the process is not supposed to go anywhere on average.

284
00:41:48,480 --> 00:41:51,390
Of course, it will always go somewhere because it's random.

285
00:41:53,000 --> 00:42:01,160
Uh, but at least having observed the trajectory, uh, up to a certain point and consider it fixed,

286
00:42:02,030 --> 00:42:08,420
uh, I expect to stay on average at the same place where I am.

287
00:42:09,200 --> 00:42:14,550
Of course, that never happens because it's. Uh, we go.

288
00:42:14,820 --> 00:42:21,120
Uh, but that basically defines or characterizes.

289
00:42:25,960 --> 00:42:32,210
MARTIN Gail's. There's a noise process.

290
00:42:45,630 --> 00:42:50,170
Now. Originally they appeared in card games.

291
00:42:51,340 --> 00:43:01,480
So the card game is fair. And if you consider the increment of a process is how much you win over a certain small time period.

292
00:43:02,170 --> 00:43:08,950
Next cycle of the game, for example, the game is fair regardless of how much your won or lost so far.

293
00:43:10,270 --> 00:43:13,840
You are not expected to win or lose anything, right?

294
00:43:15,080 --> 00:43:23,470
Because it's balanced. Uh, and so for that reason, these processes are pretty good.

295
00:43:24,490 --> 00:43:32,750
Um. You know, to to handle different kinds of estimates, especially on parametric estimates.

296
00:43:33,910 --> 00:43:40,000
Because when you consider some and consistency of them, you take differences right between.

297
00:43:43,060 --> 00:43:47,540
So you expect something to go to zero for consistency and.

298
00:43:50,780 --> 00:44:01,970
So taking differences of the kind the observed, uh, statistic minus the expected value of that statistic, you want properties for that difference.

299
00:44:03,570 --> 00:44:12,630
But taking the difference between observed and expected when estimating is okay should be done for sale on average.

300
00:44:13,860 --> 00:44:19,360
It's a nice thing. That's an instrument to handle asymptotic properties because of that.

301
00:44:20,460 --> 00:44:33,130
So if we take the expected value of x t, so I'm just using it as an example of how complicated things are.

302
00:44:33,150 --> 00:44:41,640
Really sophisticated stuff about stochastic processes can be quickly proved using the fact that we already have.

303
00:44:43,290 --> 00:44:48,180
So we can show that this property implies that.

304
00:44:49,410 --> 00:44:53,100
The expected value of the process at any point in time is actually zero.

305
00:44:54,390 --> 00:45:05,800
So how do we do that? So we can first see the value at the point t is an integral.

306
00:45:07,800 --> 00:45:10,800
All of this of the increments, right.

307
00:45:12,140 --> 00:45:15,880
So that's a calculus thing then.

308
00:45:16,640 --> 00:45:19,820
This is going to be expected, the value of that.

309
00:45:30,350 --> 00:45:34,129
The end. So expectation is a linear operator.

310
00:45:34,130 --> 00:45:38,730
So integral is a sum. So expectation of a sign.

311
00:45:38,780 --> 00:45:50,670
The sum of expectations. So then my expectation gets right to the increment.

312
00:45:52,360 --> 00:46:00,390
And then I can now use a formula of total expectation on it.

313
00:46:02,060 --> 00:46:11,180
You know, free to air condition on anything. And what I will conditioned on is the.

314
00:46:13,560 --> 00:46:20,490
Sigma algebra generated by the process up until the time just prior to tell.

315
00:46:23,370 --> 00:46:26,400
Right. So this is a formula of total expectation.

316
00:46:28,670 --> 00:46:38,210
And I have generated an expression that relates directly to the definition of the process.

317
00:46:38,240 --> 00:46:41,510
If this is true, then the entire expectation is zero.

318
00:46:42,450 --> 00:46:59,920
So that makes the whole thing zero. And another interesting property that makes these processes useful.

319
00:47:00,920 --> 00:47:05,960
For studying this and studying distributions. Use the increments.

320
00:47:11,240 --> 00:47:19,630
So much of the game. Uncorrelated.

321
00:47:24,860 --> 00:47:28,310
This is actually a typical thing about stochastic processes.

322
00:47:29,270 --> 00:47:36,910
Uh, so it seems that I. I said very little about the process, right?

323
00:47:37,540 --> 00:47:46,449
I only said that. So if I'm sitting at the pointy end conditioning and the history, the process is not supposed to go anywhere.

324
00:47:46,450 --> 00:47:53,500
On average. I haven't said anything about the distribution of the process or the other properties and so on.

325
00:47:54,070 --> 00:47:59,980
But that statement already yields a ton of results that characterize these processes.

326
00:48:00,550 --> 00:48:07,930
And one of the most prominent one is that the increments of these properties will be uncorrelated.

327
00:48:10,250 --> 00:48:16,820
So and it's again, an exercise in a similar argument.

328
00:48:17,810 --> 00:48:23,570
So what is the. Correlation between two income.

329
00:48:23,570 --> 00:48:27,290
And so since the mean value is zero, right?

330
00:48:30,270 --> 00:48:40,110
Then I'm just considering an expectation of a product of two increments at different points in time, so and without loss of generality.

331
00:48:40,110 --> 00:48:44,400
So let's say Tao is less than T.

332
00:48:48,850 --> 00:48:53,770
So then I can use a formula of total expectation to that product.

333
00:48:57,070 --> 00:49:02,139
And then a condition on the largest sigma algebra.

334
00:49:02,140 --> 00:49:13,380
I have to. I have two histories, right. I have a history, uh, up to the time point t that I have a history up to the, uh.

335
00:49:14,720 --> 00:49:27,540
Kind of smaller time point. And I can take the algebra that's generated by the random variables all the way to t.

336
00:49:32,950 --> 00:49:37,240
So the s t the textile.

337
00:49:39,550 --> 00:49:58,800
This is going to be. Like this.

338
00:49:59,130 --> 00:50:08,640
Right then. So I know that if Tao is part of 50.

339
00:50:12,280 --> 00:50:21,880
Right. And that means that the X Tower is actually because tower is typically less than two year rates.

340
00:50:22,150 --> 00:50:30,340
So it's basically less an equal to minus than our is a random variable that's going to be measurable

341
00:50:30,340 --> 00:50:38,470
with respect to this sigma algebra and measurable random variables under this conditioning.

342
00:50:46,520 --> 00:50:49,550
They can be taken out because they behave like Constance.

343
00:50:56,270 --> 00:51:09,770
So I can take it out. And then it's the inner expectation that we're going to have as again, relates directly to the definition.

344
00:51:14,100 --> 00:51:19,530
Of the march and gave. So this is equal to zero.

345
00:51:19,680 --> 00:51:32,570
And this will make the whole thing equal to zero. Okay.

346
00:51:32,930 --> 00:51:46,940
And with these the kind of the fruitful volume of things I can extract out of a seemingly non restrictive assumption,

347
00:51:47,960 --> 00:51:52,070
only about an average of some kind of increments and nothing else.

348
00:51:53,160 --> 00:52:01,230
This will actually lead me eventually to the fact that this process is normal.

349
00:52:04,290 --> 00:52:08,220
Under certain conditions. Right. Because what do I have?

350
00:52:08,310 --> 00:52:13,380
I have a process. At some point is essentially a sum of its increments.

351
00:52:14,640 --> 00:52:22,620
I just found that the incumbents are uncorrelated sums of uncorrelated random variables should follow some kind of a central limit theorem.

352
00:52:23,550 --> 00:52:33,350
So that would make the value of the process normal. So this is to say that it's no surprise.

353
00:52:40,470 --> 00:52:47,100
That so integral zero to t d x.

354
00:52:47,430 --> 00:52:51,030
So this is essentially some.

355
00:52:54,400 --> 00:53:15,370
Of uncorrelated increments. So if this thing is normal.

356
00:53:20,620 --> 00:53:33,990
Under certain conditions. And by some.

357
00:53:35,730 --> 00:53:47,960
So. That's right. Because we are already at the point of synthetics because there is infinitely many increments between zero and T,

358
00:53:48,980 --> 00:53:52,640
so I already have a sum of infinitely many uncorrelated variables.

359
00:53:53,450 --> 00:53:59,300
So that is the theoretical theoretically normal as we know from classical seal,

360
00:53:59,750 --> 00:54:03,800
but certainly you need some conditions of the variance existence and so on.

361
00:54:05,810 --> 00:54:17,870
Right. So that means that if you can show your observed minus expected difference, uh, for a certain estimate to be a martingale.

362
00:54:19,740 --> 00:54:25,250
And some parametric estimates about functions. So functions are processes and they have a.

363
00:54:26,640 --> 00:54:36,900
They you can derive the consistency and this Antarctic normality using the properties and limit theorems for this class of processes.

364
00:54:39,750 --> 00:54:51,160
That was just one appetizer. And so they used the cross probability theory and biostatistics, probably mostly aware of the.

365
00:54:53,770 --> 00:54:58,500
Semi-Permanent estimates are. Big time.

366
00:55:06,760 --> 00:55:10,790
Then the second topic that's again, pretty useful.

367
00:55:11,720 --> 00:55:18,530
Uh, for you to eventually consider is, uh, modeling independence.

368
00:55:34,550 --> 00:55:39,230
And the story is related to so-called popular theory.

369
00:55:44,540 --> 00:55:48,230
Copula as a general tool to describe the patterns.

370
00:55:49,280 --> 00:55:52,430
I'm just alerting you to existence of certain things that.

371
00:55:53,760 --> 00:55:57,030
Are useful in model modern statistics theory.

372
00:55:57,070 --> 00:56:01,710
Right. So I don't know how old you were in 2008.

373
00:56:01,860 --> 00:56:05,460
So I probably didn't care much about this stuff.

374
00:56:06,000 --> 00:56:09,690
But there was there was a big crisis back then. Right. The dot com.

375
00:56:11,660 --> 00:56:24,890
The economy went down sharply. Right. That all was claimed was blamed on the so-called Gaussian copula that was used by financial mathematicians,

376
00:56:25,730 --> 00:56:30,440
uh, to, uh, kind of predict and assign credit and so on.

377
00:56:30,500 --> 00:56:36,770
They were econometric models that use these copula, and it was smooth.

378
00:56:36,790 --> 00:56:46,760
And uh, uh, it was claimed that the crisis happened because they were predicting wrong and not taking into account.

379
00:56:47,470 --> 00:56:54,850
Kind of edgy and not smooth effects. So for inadequacy of a model, right.

380
00:56:55,850 --> 00:56:59,450
That's certainly not true. It happened for. Reason put.

381
00:57:01,610 --> 00:57:07,830
That made the press. Are trying to put the blame on the statisticians.

382
00:57:09,810 --> 00:57:22,260
So how do we go about that? So let's start with the with a random variable x and let's say it has a PDF as x of x.

383
00:57:24,200 --> 00:57:36,240
Not. So if I asked you to generate a sample from a random variable x, how would you do it?

384
00:57:40,010 --> 00:57:43,400
I mean, suppose you have a simulation model for something, right?

385
00:57:44,000 --> 00:57:47,150
So this is a basic task that you need to accomplish.

386
00:57:47,870 --> 00:57:50,930
How would you do it? Did they talk about it at all?

387
00:58:02,610 --> 00:58:09,210
Well, here is how people do it. Right. Uh, so I would, uh, so simulate X.

388
00:58:13,760 --> 00:58:17,780
So I would first simulate.

389
00:58:21,470 --> 00:58:24,960
A uniform. Random variables.

390
00:58:30,070 --> 00:58:34,380
On zero one. You get a value for it.

391
00:58:36,040 --> 00:58:40,510
Right. Then, as a second step, I will solve the equations.

392
00:58:45,960 --> 00:58:49,590
So you equals x.

393
00:58:50,040 --> 00:58:59,729
X. Four X. Or in other words, so.

394
00:58:59,730 --> 00:59:05,780
So is inverse. Function of you.

395
00:59:07,810 --> 00:59:11,230
Right. That thing will be as simple.

396
00:59:16,870 --> 00:59:27,850
The random variable x. So the argument why this is the case is basically a six or one type argument.

397
00:59:29,120 --> 00:59:37,220
Uh, because I just want to know the, uh, distribution of this thing, right?

398
00:59:37,250 --> 00:59:42,350
Uh. It's the way it goes.

399
00:59:42,370 --> 00:59:50,670
So let's say since I don't know. Let's say I pretend I don't know that wise uniform.

400
00:59:52,440 --> 00:59:55,530
Um. So let's say I have.

401
00:59:55,950 --> 01:00:01,360
Why? As if X.

402
01:00:02,340 --> 01:00:09,270
Of X writes I take the seed F of X and I plug in the random variable X as an argument.

403
01:00:11,660 --> 01:00:17,760
So what kind of a distribution is this? For the way.

404
01:00:22,090 --> 01:00:31,100
You may recognize kind of a common bias statistics procedure here because this is sort of a p value, right?

405
01:00:32,220 --> 01:00:35,990
That's how it works. You observed some statistics.

406
01:00:37,410 --> 01:00:39,570
And so this is related to P value.

407
01:00:40,050 --> 01:00:48,660
And they may have told you in some of the courses that the P values have under the null hypothesis, a uniform distribution between zero and one.

408
01:00:49,740 --> 01:00:58,200
So that's a similar result. So to get the distribution, I would just go by definition.

409
01:00:58,210 --> 01:01:07,680
So the seeds of Y will be the probability that Y is less equal y.

410
01:01:09,420 --> 01:01:16,110
And because my CDs are not decreasing, that's the same thing as.

411
01:01:18,850 --> 01:01:23,920
Well, first of all, I'll just spell out what and why is this thing?

412
01:01:28,950 --> 01:01:37,650
Then I will. Change this inequality by acting.

413
01:01:39,510 --> 01:01:48,210
By F minus one on both sides of it. And it's a non decreasing function, so it doesn't change the quality.

414
01:01:50,690 --> 01:01:55,580
So the CDF itself is now decreasing and the inverse function is not decreasing because it's just.

415
01:01:57,400 --> 01:02:02,530
Imagine the mirror around with these straight line at 45 degrees.

416
01:02:04,670 --> 01:02:15,160
It's how the universe functions go. And then I see that this probability is a CDF of accidentally of this weird argument.

417
01:02:19,980 --> 01:02:26,410
So that's x of. If it's minus one.

418
01:02:26,770 --> 01:02:29,950
Why? And this is, of course, the Y itself.

419
01:02:29,950 --> 01:02:34,660
Right. Because I'm superimposing the function and its inverse.

420
01:02:35,180 --> 01:02:39,560
Getting the argument back. And that means.

421
01:02:39,570 --> 01:02:52,260
So if my CDF of the Y is in line at 45 degrees, that means my Y is you and it's uniformly distributed.

422
01:03:03,220 --> 01:03:07,360
And values for the Y are between zero and one just because it's a CD, if that's.

423
01:03:10,390 --> 01:03:23,050
Right. So I've shown that, uh, actually, well, my view is equal to or focus on X or x is equal to F minus one a few and then justifies the procedure.

424
01:03:24,800 --> 01:03:36,020
Right. So the idea of calculus is basically generalizing this argument, uh, to multivariate situation.

425
01:03:49,360 --> 01:03:54,400
So let's say two variables leads to the concept of an openness.

426
01:04:03,200 --> 01:04:10,250
Depending on how much of a purist you are, you can tell me that it's a latent word.

427
01:04:12,000 --> 01:04:18,590
The plural, for it is copula. I've seen both.

428
01:04:24,930 --> 01:04:31,440
So how does this work? So I now have to hover around the vector x and y.

429
01:04:40,530 --> 01:04:48,630
And they can do the trick with marginals because I already know that I can have two uniform.

430
01:04:52,030 --> 01:04:56,980
Two uniformly distributed. Random variables.

431
01:04:57,790 --> 01:05:03,870
Let's say you and me. That may have some dependance between them.

432
01:05:04,020 --> 01:05:12,600
But if I'm looking at marginals, I can say that I will take you people's.

433
01:05:14,330 --> 01:05:18,840
Of course I will take the off.

434
01:05:20,170 --> 01:05:33,330
Why? Why? Just in this period of applying the previous argument separately to marginal random variables X and Y.

435
01:05:33,610 --> 01:05:43,530
Right. But because of dependance, there is some dependance between you and V and there was originally presumably some dependance between X and Y.

436
01:05:44,560 --> 01:05:54,780
Right. So to characterize that dependance, I would have to introduce the joint distribution of you and V and that joint CTF is called the copula.

437
01:06:06,230 --> 01:06:11,340
So EDF. Acts of you and the.

438
01:06:17,500 --> 01:06:21,610
With Notation C. The.

439
01:06:29,350 --> 01:06:37,750
Okay. So then the joint distribution of X and Y will be C of.

440
01:06:50,820 --> 01:06:57,870
So they join CD4 is the probability that X is less equal, x and y is less equal y.

441
01:07:01,900 --> 01:07:07,570
And this is the same as probability that is x of x.

442
01:07:08,950 --> 01:07:12,280
So equal thinks of the small one.

443
01:07:12,370 --> 01:07:16,990
Right. Just because of some non decreasing.

444
01:07:19,610 --> 01:07:31,000
Acting. Both sides of the inequality violent decrease in function will change its and if y y levels are equal.

445
01:07:33,110 --> 01:07:38,640
The script for arguments. Then I recognize that.

446
01:07:40,650 --> 01:07:45,180
This guy is you and this guy is me.

447
01:07:46,260 --> 01:07:55,649
So that's the joy in CDF after you leave, after evaluate it again to weird their opinions.

448
01:07:55,650 --> 01:08:00,260
So one is effects of x and another one is stuff. That's why.

449
01:08:12,110 --> 01:08:22,249
So that means they just represented an arbitrary joint distribution between X and Y using a copula,

450
01:08:22,250 --> 01:08:32,240
which is a function of 10101 square with marginal CD substituted as arguments of the C.

451
01:08:34,040 --> 01:08:42,410
So. And this representation. So, first of all, this representation of the joint distribution is always possible.

452
01:09:06,150 --> 01:09:09,840
And then there is the supply here in.

453
01:09:15,230 --> 01:09:19,250
And secondly, it's a useful modeling tool.

454
01:09:30,560 --> 01:09:36,500
Because how you specify see is is about dependance.

455
01:09:43,300 --> 01:09:53,459
For dependance. So there are families that people use for the sea and those families are characterizing the

456
01:09:53,460 --> 01:09:59,490
dependance between the random variables and the marginals can be modeled in a separate way.

457
01:09:59,890 --> 01:10:04,680
Right. You can say that one of them is normal, the other one is logistic, whatever.

458
01:10:04,980 --> 01:10:11,280
So we have the freedom to define models for marginals separately from the model for dependance.

459
01:10:11,850 --> 01:10:16,290
So any parameter sitting in the sea would be some kind of a correlation coefficient.

460
01:10:18,060 --> 01:10:23,880
In the parameters of the marginals will be well means of variances for those marginals.

461
01:10:27,850 --> 01:10:33,480
And if X. Why responsible for.

462
01:10:39,270 --> 01:10:53,860
You know, modeling. There is a deep theory and thick books on how this is done in different situations.

463
01:10:54,310 --> 01:11:05,950
Generalizing this argument because I've done it for continuous one, but there's also a discrete calculus and so on, right?

464
01:11:06,050 --> 01:11:16,450
There are, uh, kind of conditions on the sea that make it so not every function 10101 will be a

465
01:11:16,810 --> 01:11:23,800
left because what's needed is that C is really enjoying distribution of something.

466
01:11:24,610 --> 01:11:28,380
And that means, well, if I put one instead of a.

467
01:11:29,490 --> 01:11:36,290
Uh, one of the arguments I get, the marginal distributions you get the, you and the, the,

468
01:11:36,770 --> 01:11:45,210
and the second derivative over both arguments needs to be negative because that's a joint PDF and so on.

469
01:11:45,880 --> 01:11:50,850
Uh, but there is a whole theory that is spawned by this idea.

470
01:11:56,470 --> 01:12:24,600
So this is it. And I wish you a happy Thanksgiving.

