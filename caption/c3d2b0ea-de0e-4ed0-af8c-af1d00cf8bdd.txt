1
00:00:01,200 --> 00:00:05,550
Today. If you only took if you only took theirs.

2
00:00:07,590 --> 00:00:11,110
If you only took a few things from week four. I'm not going to press you on these.

3
00:00:11,130 --> 00:00:15,450
I just want you to remember, this is our interpretation.

4
00:00:15,780 --> 00:00:17,380
To solidify it in your brain.

5
00:00:17,670 --> 00:00:23,570
Those coefficients that we see, every single regression output, it's going to follow the same logic throughout the rest of the semester.

6
00:00:23,580 --> 00:00:27,330
I don't care if we're talking logistic regression, Poisson regression, multilevel modeling,

7
00:00:27,960 --> 00:00:32,430
structural equation modeling, those coefficients are going to have the exact same interpretation.

8
00:00:32,850 --> 00:00:36,270
All right. So just cemented in your brain.

9
00:00:36,570 --> 00:00:40,530
All right. And I'll keep on asking folks to interpret it for me. And that's great.

10
00:00:40,530 --> 00:00:47,429
And just the practice makes perfect. But the big kid, a big distinction between that bivariate stuff we did at the beginning of the semester.

11
00:00:47,430 --> 00:00:53,700
And this one is we have those other predictors and with multiple regression, all we're doing is holding other things, constant,

12
00:00:54,060 --> 00:00:58,350
taking all of this other stuff into account, holding all of that,

13
00:00:58,350 --> 00:01:03,450
pushing it all to decide what is the association or relationship between my predictor variable.

14
00:01:03,690 --> 00:01:07,710
And we are comparing questions about number one here.

15
00:01:11,330 --> 00:01:13,490
Forgot my clicker, so we'll have to stay over here.

16
00:01:14,210 --> 00:01:20,990
The intercepts often are also a little bit of an oversight or afterthought in terms of our analysis.

17
00:01:21,260 --> 00:01:28,010
But I only showed you one example of how an intercept can have some utility last week, and more often than not,

18
00:01:28,010 --> 00:01:34,940
if you are a little bit creative, you can make that intercept value, which again, so often it's just this kind of throwaway thing.

19
00:01:35,420 --> 00:01:40,520
Who really cares if it's different from zero? All I really care about is are those be the ones to three is whatever.

20
00:01:41,210 --> 00:01:44,540
But we can make it interpretable, we can make it meaningful.

21
00:01:45,200 --> 00:01:55,820
But is again the the interpretation is always the same at its root, when everything else in the model is zero, my intercept is my predicted score.

22
00:01:56,360 --> 00:02:00,860
So what we needed to do is to make sure that there is some sensible values of

23
00:02:00,860 --> 00:02:05,450
zero for all of our predictor variables if we want to change or play with that,

24
00:02:05,870 --> 00:02:09,229
that intercept interpretation questions on that one.

25
00:02:09,230 --> 00:02:13,130
And I actually think I'm going to show you an example in just 1/2. The question is just stuff you.

26
00:02:18,010 --> 00:02:21,100
The last name we can use R squared.

27
00:02:21,100 --> 00:02:28,030
And I know I ran through this fairly quickly at the end of class, but one we can tell you just in general, what's my model doing for me?

28
00:02:28,960 --> 00:02:32,770
How much of that variability in the outcome? In the outcome can I explain? Right.

29
00:02:33,760 --> 00:02:37,570
I realize I see that a lot variability in the outcome variable.

30
00:02:38,140 --> 00:02:41,620
Does anybody want me to clarify what I'm talking about?

31
00:02:44,510 --> 00:02:48,950
Or have our folks at this point, they're starting to understand that when I ask you a question about whatever it is,

32
00:02:48,950 --> 00:02:55,490
whatever health outcome you care about, Larry, what kind of health outcomes you care about anxiety, anxiety, anxiety scores.

33
00:02:55,760 --> 00:03:02,330
I give people an anxiety test. You all completed for me the variability in that, whether you're high, medium or low,

34
00:03:02,600 --> 00:03:06,110
it's trying to predict where somebody is going to land on that spectrum. Right.

35
00:03:06,140 --> 00:03:10,370
That's the variability that I'm talking about. That's what our models are trying to explain.

36
00:03:10,730 --> 00:03:18,410
And so there's R-squared or multiple R-squared, and multiple regression is our effect size for that explained variability.

37
00:03:18,800 --> 00:03:22,730
How much anxiety in this classroom can I explain with my model?

38
00:03:23,180 --> 00:03:27,440
What are the predictors? What are the right predictors to help explain most of that variability?

39
00:03:27,800 --> 00:03:32,060
So what are some of the things that are correlated or associated with anxiety, for example?

40
00:03:32,360 --> 00:03:35,660
And when I include different ones in my models, do I explain more and more?

41
00:03:36,050 --> 00:03:43,280
If I try to think through what those predictors would be as I add, or I make my what I'm out of my model more complex,

42
00:03:43,760 --> 00:03:51,560
I can compare it to simpler versions of my models to see how r squared changes during my model building process.

43
00:03:52,040 --> 00:03:57,560
So as is include additional variables by default, all things being equal,

44
00:03:57,710 --> 00:04:03,850
my r squared should go up or stay the same in the very worst case scenario, but it almost always goes up.

45
00:04:03,930 --> 00:04:10,520
Explain just a little bit more variability. We can use ANOVA like we did just briefly last week because we can test empirically

46
00:04:10,790 --> 00:04:15,710
whether adding new predictors to my model makes it a better fit than my previous one.

47
00:04:18,410 --> 00:04:23,330
Now that simply folks questions about these three.

48
00:04:34,260 --> 00:04:37,360
If you got this, you got a month where you really do.

49
00:04:38,350 --> 00:04:44,670
This is what I you know. All right.

50
00:04:45,300 --> 00:04:48,390
Well, that's good. I'm happy. You're seem happy.

51
00:04:49,380 --> 00:04:53,400
Let's keep moving. I always think this is a good idea, just to remind ourselves.

52
00:04:55,620 --> 00:05:03,660
So we'll do this a little bit in two weeks as well. But our command to create a scale, Clara said, anxiety.

53
00:05:03,930 --> 00:05:07,380
Well, there's not usually just a general measure of anxiety that I can just ask folks.

54
00:05:07,950 --> 00:05:12,330
More often that's due to a survey research, but also a lot of other kind of administrative data.

55
00:05:12,600 --> 00:05:17,670
We're taking multiple items, more multiple responses and combining them in some way, shape or form.

56
00:05:18,170 --> 00:05:20,070
Right. So we're going to do this a lot in this class,

57
00:05:20,310 --> 00:05:26,730
but this is just one example of a variable or a set of variable manipulation that we can do to create a construct,

58
00:05:26,730 --> 00:05:32,639
a construct as this kind of unobservable value that we all have or that whatever

59
00:05:32,640 --> 00:05:36,720
our unit analysis has that we're going to try to use within our analyzes.

60
00:05:37,200 --> 00:05:46,499
So leveraging different pieces of data that we have available to us makes for ideally kind of more interesting and

61
00:05:46,500 --> 00:05:52,170
more nuanced analysis than if we just go with kind of single items that we're using from our data collection.

62
00:05:52,470 --> 00:05:56,430
And I think this is going to be true in social sciences, hard sciences, the local school, whatever it might be.

63
00:05:56,730 --> 00:06:00,990
We can probably think of more interesting constructs than just a single item

64
00:06:00,990 --> 00:06:04,410
that we can ask folks or a single observation or account that we can have.

65
00:06:05,730 --> 00:06:10,530
So just to clarify, what needs to grow means that it creates a new variable.

66
00:06:10,560 --> 00:06:14,730
The means of all the columns. Yeah.

67
00:06:16,680 --> 00:06:26,400
So is that like like, for example, our homework, we're supposed to create predictor variables that we were using because it didn't specify means.

68
00:06:28,170 --> 00:06:36,180
So you can it's I don't care if these means or sounds or if there's other ways that you can conceptualize, but you want your construct to be.

69
00:06:36,390 --> 00:06:42,120
Generally speaking, we do tend to take these. So if you're creating kind of a scale, that's probably what we're going to do.

70
00:06:42,120 --> 00:06:44,009
We did have an exposure to violence or whatever,

71
00:06:44,010 --> 00:06:51,750
but so mean is just a linear transformation of a some writing with some divided things up and divided by whatever.

72
00:06:52,620 --> 00:06:56,440
There could be other ways that you might think of how you're going to combine variables together,

73
00:06:56,520 --> 00:06:59,550
take that information and create something that's more synergistic.

74
00:06:59,910 --> 00:07:05,670
So this is an example. It's probably the most common, but it doesn't have to be the only one.

75
00:07:05,910 --> 00:07:08,340
And in fact, when we talk about factor analysis in a couple of weeks,

76
00:07:08,340 --> 00:07:13,590
we can think of a new way to take information from more disparate measurement approaches.

77
00:07:13,590 --> 00:07:18,510
So like a categorical or a dichotomous variable and a continuous variable and we're actually a couple of

78
00:07:18,510 --> 00:07:26,430
others and see whether or not they can be combined into something meaningful that I can build up to that.

79
00:07:26,430 --> 00:07:36,150
When we're doing this, it doesn't benefit us if we're doing this to the outcome variable to do the same to the predictor combine in the same way.

80
00:07:37,200 --> 00:07:43,850
Ah yes, actually. So depending on what you're trying to use in your model, it might behoove you to think, Alright,

81
00:07:43,860 --> 00:07:51,420
well I have three measures of perceived risk and I want to combine those into one measure of perceived risk.

82
00:07:51,810 --> 00:07:57,930
The reason why this can be helpful is one, you take multiple pieces of information because you're kind of collapsing together.

83
00:07:58,230 --> 00:08:01,680
So that might be more accurate than if we just take the one times two.

84
00:08:02,490 --> 00:08:08,490
If you remember I said last week, all things being equal, a simpler regression model is better than a more complex one.

85
00:08:08,730 --> 00:08:14,730
So if I can take three perceived risk items and combine them into one, that's one predictor versus three predictors.

86
00:08:15,000 --> 00:08:18,420
This is especially true when you have small sample sizes.

87
00:08:18,840 --> 00:08:26,400
You have thousand people in your sample. It doesn't really matter. But you can you can have as many predictions you want up to 2000 when you have 50.

88
00:08:26,640 --> 00:08:32,460
All of a sudden having a half dozen or 12 predictors in your model instead of just two scales can make a big difference.

89
00:08:32,910 --> 00:08:40,320
So by and large, yes, I will use some form of transformation to make scales for both predictors in a number.

90
00:08:44,560 --> 00:08:50,480
Others. Scared.

91
00:08:50,810 --> 00:08:56,910
But I think again, for the we just tried to visualize some of this data, it can be helpful.

92
00:08:56,930 --> 00:09:00,290
You can rely on some of the tests that we're going to run today, which is fine,

93
00:09:00,890 --> 00:09:07,700
but it might be beneficial just in general to take a peek at what am I expecting before I run this complex model.

94
00:09:07,700 --> 00:09:15,440
So don't forget that when we have two variables and we look at them in a scatterplot that makes sense, that maps on to our reality.

95
00:09:15,740 --> 00:09:21,380
If we have three variables and we have kind of a three dimensional point, also maps onto our reality.

96
00:09:21,920 --> 00:09:30,020
As soon as we jump into four or five or more variables, we are now moving in a to a multi dimensional space that we can't visualize.

97
00:09:30,620 --> 00:09:36,139
So you're kind of breaking down a very complex problem into some of its basic

98
00:09:36,140 --> 00:09:42,410
constituents to at least get a sense of what you might expect for your results.

99
00:09:42,920 --> 00:09:48,440
In this case, that anxiety seems to be negatively associated with self acceptance, right?

100
00:09:48,620 --> 00:09:53,690
Generally speaking, our scatterplot show that when I run a model that has four other predictors in it,

101
00:09:53,960 --> 00:09:57,980
I probably should see that negative association between self-acceptance and anxiety.

102
00:09:58,280 --> 00:10:03,320
And if I don't, maybe that's going to give me scratching my head thinking what could be happening here?

103
00:10:04,000 --> 00:10:09,170
All right. So what are some of the things that we can do at a binary level that might inform our multivariate analysis?

104
00:10:10,760 --> 00:10:18,200
Here is our linear regression sequence. I'm going to go ahead and ask people what is what's going on in this very kind of time here?

105
00:10:22,820 --> 00:10:31,900
What we're doing is that one. Is there a regression model?

106
00:10:31,920 --> 00:10:35,430
Yeah. What what's the how would you say this in word? This is code. You know, this code.

107
00:10:35,430 --> 00:10:45,080
But everybody else out there doesn't have to explain this in words except. Self-acceptance.

108
00:10:45,230 --> 00:10:48,890
Age, sex. Is it for one minute?

109
00:10:50,820 --> 00:10:53,940
Or do you think more generally, what are we doing?

110
00:10:54,290 --> 00:11:02,330
What is what is the analysis here? That whole thing you're getting into the interpretation, you don't have that area.

111
00:11:02,750 --> 00:11:09,800
This is simply we are regressing. Someone self-reported anxiety on their levels of self-acceptance, age and sex.

112
00:11:11,100 --> 00:11:18,260
We're using a linear regression model. So the assumption is that there is a linear association between these three variables and our outcome variable.

113
00:11:18,920 --> 00:11:22,310
We are already in a multidimensional space. You cannot visualize this.

114
00:11:22,700 --> 00:11:27,019
I cannot draw you a plot to talk about that association or that relationship,

115
00:11:27,020 --> 00:11:31,340
because there are three variables that we think are going to combine information to predict her outcome.

116
00:11:31,940 --> 00:11:36,140
Right. So that's hard to think about visually, but we know that that's happening kind of behind the scenes.

117
00:11:36,230 --> 00:11:39,680
I want to hear what's going on. Summary All the way down to confidence intervals.

118
00:11:39,890 --> 00:11:46,280
What can you think that's firing me into that?

119
00:11:46,470 --> 00:11:50,209
You know, it's just you're doing the summary check.

120
00:11:50,210 --> 00:11:55,810
What you're seeing the first bullet. Yeah. ANOVA, and then, like you said, the confidence intervals.

121
00:11:56,020 --> 00:11:57,440
What's this one? What's this going to do?

122
00:11:57,440 --> 00:12:06,110
I forget can output one is now putting to see if there is something going on in the relationship between the two variables.

123
00:12:06,710 --> 00:12:17,570
I don't think at tone it doesn't tell you everything that's true, but I think that's just to see in comparing to anxiety what self-acceptance,

124
00:12:18,140 --> 00:12:21,560
how it's correlated basically, so that you're going to get that better.

125
00:12:21,560 --> 00:12:24,740
From summary, Oksana, would you have you had your hand up?

126
00:12:25,910 --> 00:12:37,250
I was going to say I would to see if there is anything in that, even if it doesn't tell us between what it is.

127
00:12:37,290 --> 00:12:42,050
Is standardizing the model right now. Oh yeah.

128
00:12:43,070 --> 00:12:48,530
Is it showing the the groups like Self-acceptance Age?

129
00:12:48,530 --> 00:12:51,650
And that's a different and same thing.

130
00:12:52,870 --> 00:12:57,169
Okay. Yeah. So your summary is going to give you your coefficients. It's going to give you a test.

131
00:12:57,170 --> 00:13:00,260
The model says it's going to give you distribution. Here is the residuals, right?

132
00:13:00,440 --> 00:13:04,490
This is where you're going to get one year change in blah, blah, blah is associated with a change in anxiety.

133
00:13:05,270 --> 00:13:12,710
This ANOVA, remember, is kind of our omnibus test overall of model strip and whether or not our model fits the data well how well?

134
00:13:12,950 --> 00:13:16,820
Well, we kind of look at the R-squared value and think to yourself, is that enough?

135
00:13:17,150 --> 00:13:22,430
But this is also going to partial out the different contributions of each predictor variable, remember?

136
00:13:22,430 --> 00:13:26,240
So it's going to look at the sums of squares relative, the residuals for all three predictors.

137
00:13:26,550 --> 00:13:32,840
And we're going to see if there's any sort of if you want to say extraneous or redundant predictors in the model,

138
00:13:33,170 --> 00:13:39,470
or are they all making unique contributions to the prediction of Amazon?

139
00:13:40,190 --> 00:13:43,639
Right. So, yes, there's going to be very similar information.

140
00:13:43,640 --> 00:13:49,670
You're going to see you're going to find out whether or not a predictor matters. Um, here's where you're going to have the standard interpretation.

141
00:13:50,000 --> 00:13:56,870
Here's where you're going to help to make a decision about, well, is each one of these variable maybe kind of pulling equal weight?

142
00:13:57,140 --> 00:14:02,060
Or can I perhaps make some decisions about pruning back my model, taking some variables out that I wouldn't otherwise need to?

143
00:14:02,480 --> 00:14:07,850
Confidence intervals. You just kind of take those coefficients and give you kind of a range of where they plausibly would lie.

144
00:14:08,360 --> 00:14:13,280
Because with this summary one, you're going to get one point s right at one point estimate.

145
00:14:13,280 --> 00:14:15,770
And that's not going to tell you anything more than what that point estimate is.

146
00:14:16,070 --> 00:14:21,530
Confidence interval says, well, the range, the plausible range for the effect of age and anxiety,

147
00:14:21,680 --> 00:14:26,240
holding sex and self-acceptance consent is within these two bags.

148
00:14:29,620 --> 00:14:40,780
Now. I care about that. Can you imagine this woman was in the asylum where you actually told us what it is?

149
00:14:41,200 --> 00:14:48,280
Oh, this Saturday? Yeah. Sometimes we want to know beyond just the original metrics or units, which we're going to get here in the summary.

150
00:14:48,700 --> 00:14:51,820
We can standardize things by standardizing good graph.

151
00:14:52,990 --> 00:14:58,960
It puts it on a place where you can compare different measurements if they're not comparable.

152
00:14:59,320 --> 00:15:00,100
Yeah. All right.

153
00:15:00,100 --> 00:15:05,380
So instead of having to compare apples to apples, because all three of these variables are very are measured on very different scales.

154
00:15:05,680 --> 00:15:10,510
This is 1 to 5. This is zero to whatever. This is actually the economist variable.

155
00:15:10,950 --> 00:15:16,030
Right. So we can't necessarily compare those if those regression coefficients by themselves.

156
00:15:16,450 --> 00:15:22,960
Instead, we get we can use the standardization to put them all on the same scale on a mean of zero deviation of one.

157
00:15:23,320 --> 00:15:25,150
Heather, can you bring us home with this one?

158
00:15:25,690 --> 00:15:35,440
So that is looking at four two regression models to determine if the model two with the additional items is going to.

159
00:15:37,160 --> 00:15:45,370
Yes, I think it's a good idea to do right. We have an we have two models that we think could possibly work here to compare conceptions of reality.

160
00:15:45,670 --> 00:15:49,900
We think that by all things being equal, it's better to have a simpler model.

161
00:15:50,150 --> 00:15:56,050
So we're going to pair and we're going to compare a model that has just a one predictor to one that has three predictors.

162
00:15:56,440 --> 00:16:01,479
And instead of just saying, Well, yeah, I think number two looks better now we have an empirical test to say yes.

163
00:16:01,480 --> 00:16:07,480
In fact, number two is doing a better job of explaining variability my outcome than a single or bivariate regression model.

164
00:16:07,960 --> 00:16:12,640
But the good stuff here, if you got this, you got a model regression.

165
00:16:13,000 --> 00:16:20,920
You really do. You really do. Couple of messages.

166
00:16:21,170 --> 00:16:24,850
Hmm. I'm not sure, actually, what I want you to tell me, but what's. What do you see in here?

167
00:16:31,850 --> 00:16:36,530
This is their data might be our reconstructed data.

168
00:16:36,800 --> 00:16:47,990
That's what this is. Structure Command is going to give us some really important information, namely right here.

169
00:16:49,710 --> 00:16:53,150
Right. What's the difference between a factor in their variable? No.

170
00:16:58,030 --> 00:17:02,520
So I just left out. What was it? What's the difference between an American effective variable?

171
00:17:02,530 --> 00:17:07,300
What's the difference? How would you tell someone walking by at all metric and switching variable?

172
00:17:15,400 --> 00:17:20,980
I'm actually not sure. Okay. The 100 K the factor is a category.

173
00:17:21,190 --> 00:17:25,180
Categorical variable. And the numeric is obviously a number.

174
00:17:25,990 --> 00:17:31,060
And so then you want to bring on the same playing field again and convert that.

175
00:17:31,300 --> 00:17:36,750
So like you're converting the categorical variable into a factor level to compare with numerical right.

176
00:17:36,760 --> 00:17:42,550
So R is smart as it is, doesn't necessarily always think about variables in the same way that we do.

177
00:17:42,970 --> 00:17:51,310
We want to represent factors. So categorical variables as in as what R is going to call a factor.

178
00:17:51,310 --> 00:17:57,280
So it's going to recognize that there's no inherent order and that in theory the groups are mutually exclusive and exhaustive.

179
00:17:57,670 --> 00:18:02,590
Nobody belongs to more than one group. And in theory, again, we have all the groups represented here.

180
00:18:03,010 --> 00:18:08,889
Contrast that to a number that has a meaningful zero value and goes up at some incremental level.

181
00:18:08,890 --> 00:18:14,020
Right. So it's assuming it's either going to be at any integer or even ratio level of of measurement.

182
00:18:14,470 --> 00:18:17,530
How about an or variable? What do you think an ordinary variable is going to show up here?

183
00:18:29,110 --> 00:18:33,349
So generally speaking, it's going to take a peek at an audio variable that says something like strongly

184
00:18:33,350 --> 00:18:37,730
disagree all the way to strongly agree and it's going to include as a factor. So you got to be careful.

185
00:18:38,030 --> 00:18:42,170
If that's not the way that you want your audio variable to be utilized.

186
00:18:42,860 --> 00:18:47,180
More often than not, we are thinking that there are there are going to be a number.

187
00:18:47,630 --> 00:18:51,320
We're going to combine that information or if we want to use some ordered factor,

188
00:18:52,040 --> 00:18:57,500
then we can be designated as such specifically within some of the analysis that we're going to run like a normal regression model.

189
00:18:57,860 --> 00:19:03,140
And what it's going to do is it's going to break up a factor variable into a bunch of dichotomous comparisons.

190
00:19:03,650 --> 00:19:04,880
We'll get to that in a few weeks.

191
00:19:05,390 --> 00:19:11,570
But knowing that these are the two major kinds of variables that we're going to be working with, there are also logical variables.

192
00:19:11,570 --> 00:19:15,050
We'll show an example today. It's just like whether it's true or false.

193
00:19:15,770 --> 00:19:20,899
But this has huge implications for your regression analysis because it's going to take

194
00:19:20,900 --> 00:19:24,740
a factor variable and it's going to create that dummy coding that we've talked about.

195
00:19:25,550 --> 00:19:28,010
Right. So you're going to see a difference in the output.

196
00:19:28,580 --> 00:19:35,960
If we ran this analysis like this with all these things labeled correctly versus one where it's assuming every single thing is a number,

197
00:19:36,830 --> 00:19:46,190
this variable race has three loads. That means it's going to have how many coefficients, what I included as a factor of two, right?

198
00:19:46,400 --> 00:19:52,640
It's going to create a dummy variable and two dummy variables and we'll have a reference category if you didn't include it as a factor.

199
00:19:52,990 --> 00:20:00,380
There's only going to be one coefficient for race. All things that we can be that we can be mindful of when we're looking at our output.

200
00:20:02,280 --> 00:20:08,620
Which. We got here.

201
00:20:09,610 --> 00:20:15,820
What do these results tell us about the relative levels of violence in each school drought?

202
00:20:17,050 --> 00:20:20,620
Is there a cautionary reading question here?

203
00:20:20,920 --> 00:20:25,090
What do the results tell us about the relative levels of violence in each school?

204
00:20:30,310 --> 00:20:33,379
Remind me which one is an old school idea.

205
00:20:33,380 --> 00:20:37,210
Refers to school, right? Yeah. So then there is a.

206
00:20:44,410 --> 00:20:48,000
Positive. Correlation between what?

207
00:20:53,110 --> 00:20:56,379
Environment in which we're at. Yeah.

208
00:20:56,380 --> 00:21:00,730
All right. So you said you said there's a positive association between which schools?

209
00:21:01,390 --> 00:21:11,690
School. Two for two and four relative to where you live are all relative to a referred category, which in this case would be school one.

210
00:21:12,260 --> 00:21:16,760
Right. So we don't see school one represented here because it was a four category or there were four schools.

211
00:21:16,970 --> 00:21:20,299
Does this look very familiar? I remember doing this analysis.

212
00:21:20,300 --> 00:21:23,230
You did this analysis. All right.

213
00:21:24,010 --> 00:21:32,390
And you submitted what you did is as a novel, you did not do the same analysis as a multiple regression or I guess a mosquito bite regression.

214
00:21:32,810 --> 00:21:40,850
All right. This is simply regressing our exposure to violence variable on our school I.D. We have a factor variable that has four levels schools.

215
00:21:40,850 --> 00:21:41,870
One, two, three, four.

216
00:21:42,410 --> 00:21:49,970
It's going to take one school and create a record category, and then it's going to create dummy variables for schools two, three and four.

217
00:21:50,690 --> 00:21:54,830
That's why we have three coefficients for a four level factor.

218
00:21:55,910 --> 00:22:08,920
All right. As the real said, schools two relative to school one and school for relative to school one have higher levels of violence exposure.

219
00:22:10,280 --> 00:22:21,110
These are actually means because we're talking about categories here, school three relative to school one has a lower level of violence exposure.

220
00:22:21,920 --> 00:22:24,990
And these are significant, significant marginal.

221
00:22:28,710 --> 00:22:44,060
Same results, is there? No. And my question to you answer that question for me at the bottom, what does the means violence score for school I.D. one?

222
00:22:57,150 --> 00:23:01,560
This me first school I.D. is the same as well because I see.

223
00:23:05,260 --> 00:23:12,050
The media know. So this is going to be a residual. So that's going to be the difference between what we observe and the outcome that you said.

224
00:23:14,250 --> 00:23:20,879
That is the intercept. So with a factor variable like this, what's the interpretation of intercepts?

225
00:23:20,880 --> 00:23:25,680
Three slides ago on every coefficient in the model is zero.

226
00:23:26,400 --> 00:23:31,050
That's our predicted value or predicted score. That's what our intercept means.

227
00:23:31,720 --> 00:23:34,980
Doesn't matter what variable we're talking about. We're done decoding.

228
00:23:35,370 --> 00:23:41,460
If you go to school one, you get a zero here. You get a zero here and you get a zero here.

229
00:23:42,600 --> 00:23:45,870
This, my friends, is a very meaningful intercept.

230
00:23:46,590 --> 00:23:53,910
That is our kind of baseline, if you will, level or expected violence exposure for someone who attends school one.

231
00:23:54,270 --> 00:24:02,970
And we can see how people who attend school two or three and four vary about that mean just like a over rat.

232
00:24:03,600 --> 00:24:04,380
And look at this.

233
00:24:04,920 --> 00:24:11,160
So on the left hand side, it's just that described by a command where I'm just thinking descriptively, give me the means across the four schools.

234
00:24:11,340 --> 00:24:14,340
Look at these coefficients, folks. That's the connection.

235
00:24:15,750 --> 00:24:19,820
Do you see them? I think this stuff is really fine.

236
00:24:20,850 --> 00:24:24,300
You didn't think about this when you saw that those coefficients out there,

237
00:24:24,450 --> 00:24:29,250
you didn't think you could just go run some descriptive statistics and see the exact same information?

238
00:24:29,640 --> 00:24:34,350
Absolutely. And they take it. They bundle it all up and they try to confuse us.

239
00:24:34,680 --> 00:24:39,750
Well, we're not confused. We're not confused because it's all the same stuff, right?

240
00:24:39,780 --> 00:24:44,760
Our intercept is just school one. And then they get to either means two, three and four.

241
00:24:45,180 --> 00:24:49,350
It's just our coefficients. That's why I talked about them as means or mean differences.

242
00:24:49,830 --> 00:24:55,230
Right. That's what this is. This regression equation is just reduced down to this categorical predictor.

243
00:24:55,950 --> 00:24:59,169
It's not anything super fancy. It looks fancy.

244
00:24:59,170 --> 00:25:03,149
It looks complicated, but we can think it through, right?

245
00:25:03,150 --> 00:25:05,310
We can make our intercepts interpretable.

246
00:25:05,640 --> 00:25:11,340
And we can also talk about these relative differences in an empirical way rather than just saying, oh, man, really?

247
00:25:11,340 --> 00:25:17,520
Who looks like group two is the highest point? So I remember running this in the last war.

248
00:25:17,850 --> 00:25:23,610
Yeah. When I did the describe by the group 1.5 and 2.5.

249
00:25:24,270 --> 00:25:30,659
4.5. Why? Why would it do something like that? I mean, they had the correct numbers for group one, two, three and four.

250
00:25:30,660 --> 00:25:34,799
But the point in between. That's interesting.

251
00:25:34,800 --> 00:25:39,750
And I talked to other people and they also that's I like I just don't know why I

252
00:25:39,750 --> 00:25:48,990
did that so just ignore the points and the labels are 1.52.5 frequency group one.

253
00:25:49,110 --> 00:25:52,890
Those numbers would be correct. But then there was that group 1.5 and a group 2.5.

254
00:25:53,520 --> 00:25:59,250
So that's I mean, it's going to draw those labels and kind of give it its best guess based on what it reads in.

255
00:26:00,060 --> 00:26:06,600
So I'm guessing it's just how the dataset was read in, and there might be a way to either,

256
00:26:06,600 --> 00:26:12,720
as you're reading it in, to change or to be more explicit about what you want the labels to be.

257
00:26:13,080 --> 00:26:16,260
Or you can always post hoc. Once it's written, you can change.

258
00:26:16,350 --> 00:26:20,040
Instead of calling a group one, you can say School one, school to school three.

259
00:26:20,370 --> 00:26:27,960
So the way around that would have been to read in the categories as or the the label values is true if you're in a successful.

260
00:26:31,690 --> 00:26:38,759
Okay. Um, I'm a little bit confused from the user group to bring forward to the estimates.

261
00:26:38,760 --> 00:26:42,560
Is that just like. Is it?

262
00:26:43,560 --> 00:26:48,240
Is the government being taken over at this coefficient?

263
00:26:48,300 --> 00:26:52,490
You should get that. Take this referent. Subtract this coefficient.

264
00:26:52,540 --> 00:26:58,870
She did that. Many differences which make sense for a.

265
00:26:59,530 --> 00:27:03,400
Right. So what is there? It is. That is going to be very comprehensive.

266
00:27:04,240 --> 00:27:08,230
Any questions about this? Or this.

267
00:27:17,170 --> 00:27:21,909
I think this stuff is going on. All right.

268
00:27:21,910 --> 00:27:30,580
Here we are. We're moving it to the the diagnostics piece, which isn't my favorite, but it's you know,

269
00:27:30,590 --> 00:27:34,659
it's a necessary evil that we're going to have to get ourselves through questions about.

270
00:27:34,660 --> 00:27:38,470
If you watch the video, anything that you want me to make sure that I cover in the next 42 minutes.

271
00:27:43,030 --> 00:27:44,600
Well, I have to have you know, you can always ask me.

272
00:27:47,910 --> 00:28:03,750
And if people watch the video for a low enthusiasm where you really see nonlinearity on the scatterplot.

273
00:28:03,780 --> 00:28:06,900
What is that? What would that look like? Oh, that's a great question.

274
00:28:13,310 --> 00:28:14,180
And asked.

275
00:28:18,200 --> 00:28:30,190
I think it'd be helpful to just generally go through all the assumptions again and oh yeah, explain the various I guess we can go through and.

276
00:28:32,270 --> 00:28:32,840
Yeah, that's great.

277
00:28:33,230 --> 00:28:43,670
And then also just what it entails when you're talking about doing a transformation, is it just one thing or you're doing multiple things?

278
00:28:43,940 --> 00:28:53,430
Sure. We all felt the need for the very first week class.

279
00:28:53,730 --> 00:28:57,300
We all have our biases to armor preconceived notions about how things should work.

280
00:28:57,660 --> 00:29:04,770
I will be very upfront about mine. I detest the great pizza store, but I do not like transformations.

281
00:29:05,310 --> 00:29:13,920
The reason why is because I feel like you lose interpretability. I especially don't like more extreme transformations that are difficult to replicate.

282
00:29:13,920 --> 00:29:18,060
So in two key degrees or when you take logarithms, that type of thing.

283
00:29:18,360 --> 00:29:21,600
That said, they are a very common practice I think is important to know about them.

284
00:29:21,810 --> 00:29:25,590
There are some legitimate reasons where you want to use them, but for me,

285
00:29:25,590 --> 00:29:34,080
the loss of of interpretability is usually so severe that I will counsel folks away from them.

286
00:29:34,200 --> 00:29:37,610
That said, we will absolutely cover them so you know what to do when you see them.

287
00:29:37,620 --> 00:29:42,030
Any other questions? But then reflecting on that.

288
00:29:42,720 --> 00:29:46,230
So here I am telling you that I run these analyzes and I don't like transformation.

289
00:29:46,620 --> 00:29:52,470
Maybe Amy loves transformation. There's nothing better than to use some logarithm on all the different variables in their data.

290
00:29:53,190 --> 00:29:58,890
And that's how Shell approached these analysis issues and what the different results that we might have based on those kind of decisions.

291
00:29:59,250 --> 00:30:03,120
And then talking about the objectivity of this kind of work that we do and what

292
00:30:03,120 --> 00:30:07,319
that means for how you're going to report some of this information and how your

293
00:30:07,320 --> 00:30:12,600
findings could change based on some of these decisions that Tom and I just made you

294
00:30:12,600 --> 00:30:17,770
look down upon the people where people use transformations to make you do that.

295
00:30:18,780 --> 00:30:23,010
I Yeah, honestly, and there's nothing wrong with it.

296
00:30:23,490 --> 00:30:28,260
I mean, in the sense that if you have a legitimate reason to make a, you know,

297
00:30:28,260 --> 00:30:36,960
a variable with us extreme skew to transform it so that it's a little more normally distributed,

298
00:30:37,230 --> 00:30:41,250
which then helps with the residuals in your model to meet some of these assumptions.

299
00:30:41,640 --> 00:30:47,490
I can't fault that, other than I would hate to know that you just did that at the end of six.

300
00:30:47,490 --> 00:30:51,240
Other things that you tried to get to your quote unquote, significant results.

301
00:30:51,570 --> 00:30:54,150
And if you do do that, and this would be for everybody here,

302
00:30:55,350 --> 00:31:00,630
tell us and us can be the stakeholders at a journal or it can be the folks in your community,

303
00:31:00,840 --> 00:31:04,560
or it could be whatever you're doing, be transparent about the way that you're working with data.

304
00:31:05,880 --> 00:31:10,110
Talk about all the different what you might call sensitivity analysis or alternative steps.

305
00:31:10,530 --> 00:31:14,370
Tell us what you did, what went wrong, why you moved in new directions,

306
00:31:14,580 --> 00:31:19,620
and then let any consumers who know what they're talking about take all that on the whole and say,

307
00:31:19,830 --> 00:31:23,069
All right, I buy it or I don't take it in that game.

308
00:31:23,070 --> 00:31:30,030
And in your opinion, and how is that upheld on like a committee if you're like doing research, like how would you do that?

309
00:31:30,660 --> 00:31:37,170
Would it I mean, if you list all the steps, okay, you're transparent about it, but how do you think that leads your research findings?

310
00:31:37,410 --> 00:31:41,309
So it's a great question. I think so often it's a it's an unknown.

311
00:31:41,310 --> 00:31:44,100
So I wouldn't know if you just sent me the final product.

312
00:31:44,370 --> 00:31:50,130
And it only shows me that you ran one analysis that you ever did 60 other things that you wouldn't have thought about,

313
00:31:50,910 --> 00:31:52,290
where it can have some implications.

314
00:31:52,290 --> 00:31:59,650
If I learned that you ran 25 different models before you found the one that, quote unquote worked, what does that mean for our type one error rates?

315
00:31:59,670 --> 00:32:03,000
Right. When we think about family life, they're running a whole bunch of non independent tests.

316
00:32:03,540 --> 00:32:06,690
I'm not surprised. After 25 models you hit the jackpot.

317
00:32:07,050 --> 00:32:09,510
Right? Chance alone would suggest that you did just that.

318
00:32:09,900 --> 00:32:14,250
So people might ask for more stringent parameters for the way that you're running your analysis.

319
00:32:14,820 --> 00:32:21,600
Lower cutoff rate. I've seen journals now expect to have kind of a sensitivity analysis section where they say,

320
00:32:21,750 --> 00:32:26,579
tell us all the other things you tried before you did this for best practice.

321
00:32:26,580 --> 00:32:31,530
But even though people don't typically do it, if you lay everything out in clinical trials will ask you to do this.

322
00:32:31,530 --> 00:32:37,709
If you lay everything out in advance about what you're going to run, how you're going to run it, etc., etc., it's all mapped out.

323
00:32:37,710 --> 00:32:44,820
It's shared publicly. I had to do this. Then when you deviate from that practice, people can see it's like a blockchain, right?

324
00:32:44,850 --> 00:32:50,070
You can see within the ledger that this is what you're supposed to do and then how can you ended up with this result instead?

325
00:32:50,340 --> 00:32:52,170
And then you just have to kind of make your explanation.

326
00:32:52,710 --> 00:32:56,730
So those are some things that we can do in theory that doesn't always work out that way in practice,

327
00:32:56,730 --> 00:33:01,830
but it's getting better and especially I think with clinical trials that have people's lives at stake.

328
00:33:01,830 --> 00:33:05,040
It's nice that you have to have them published and people checking back and stuff.

329
00:33:05,310 --> 00:33:08,250
Yeah, it's interesting because I'm thinking of like community research especially.

330
00:33:08,490 --> 00:33:15,150
So for people that community may not really know the type one area until like, Oh, here's, you know, what I did to get to these steps.

331
00:33:15,510 --> 00:33:20,670
It's almost like there's a protection aspect there of like they may not realize,

332
00:33:20,700 --> 00:33:26,610
like you can be giving them data that's really just not true, essentially because you had to transform in all this.

333
00:33:26,730 --> 00:33:33,059
Yes. And the community members are misled that says, okay, seriously, now this this is this is us.

334
00:33:33,060 --> 00:33:37,170
This is a role that you can play. You are now no longer just a consumer.

335
00:33:37,170 --> 00:33:41,520
Right. You have the the ability to translate some of this information.

336
00:33:41,760 --> 00:33:44,970
You can help people think critically about the data that they have that they receive.

337
00:33:45,270 --> 00:33:47,129
You can explain why you should be wary.

338
00:33:47,130 --> 00:33:55,080
If someone runs an analysis on a subset of their population and claims that whatever they're doing is really effective for everyone,

339
00:33:55,080 --> 00:33:58,680
for example, and sure. To try to poke holes in some of the validity of those claims.

340
00:33:58,680 --> 00:34:00,809
Right. This is where I want folks to be.

341
00:34:00,810 --> 00:34:07,740
Even if you're not running all the analysis, you can at least be that interpreter for folks because again, it's esoteric in many ways,

342
00:34:07,740 --> 00:34:09,840
but I think when you start to explain to folks,

343
00:34:10,440 --> 00:34:14,790
they're going to understand they're going to fail to make a reasonable judgment or at least provide some guidance.

344
00:34:15,260 --> 00:34:20,250
I'd imagine it's the same way that patient education, they're going to have a lot of information that they don't understand.

345
00:34:20,550 --> 00:34:24,570
Your job is to help to break that down into pieces so that they can make informed decisions.

346
00:34:24,930 --> 00:34:31,890
Right. So however, you're going to be working with data, thinking about your end consumer at the and how it's going to be most effective for them.

347
00:34:32,700 --> 00:34:39,880
So, so believe it or not, all this stuff we did last week, we're only doing this one, this last line today and.

348
00:34:39,940 --> 00:34:44,010
It's just to make sure that that we can trust the results that we came up with last week.

349
00:34:44,020 --> 00:34:49,090
We said model two is the best. I think we said that sex was the strongest predictor.

350
00:34:49,900 --> 00:34:53,740
Age was kind of on that. Borderline might not even matter in terms of someone's anxiety.

351
00:34:54,100 --> 00:34:57,790
Those were our conclusions. We explained about 5% of the variability, right?

352
00:34:57,790 --> 00:35:02,230
So about 1/20 of what we need to understand about anxiety with just our model.

353
00:35:02,590 --> 00:35:11,530
Can we trust those results? That's what's that experiment. I forget who has this.

354
00:35:11,530 --> 00:35:14,750
Jenny, let's do it. I'm not going to do it.

355
00:35:16,350 --> 00:35:23,140
So let's walk through a quick linearity. Any questions about the linearity assumption?

356
00:35:30,030 --> 00:35:34,920
His assumption is we have a range of variable in an outcome variable and there should be.

357
00:35:40,850 --> 00:35:47,070
Something we can describe with a straight line. If we cannot describe it with a straight line.

358
00:36:00,700 --> 00:36:07,150
And perhaps something like this makes more sense. Plus, linear regression is not going to be appropriate.

359
00:36:08,080 --> 00:36:17,200
Okay, so there's scattered plots and so scattered plots can be a nice way to detective early on.

360
00:36:17,720 --> 00:36:24,310
And you can also we'll talk about it in a couple of weeks. And I know I keep saying a couple weeks, that's when we look at a nonlinear regression.

361
00:36:24,310 --> 00:36:26,530
You can test models that have the linear assumption,

362
00:36:26,710 --> 00:36:33,100
or you could test nonlinear models to see whether or not perhaps a one that has a curve or even multiple curves is going to do a better job.

363
00:36:33,520 --> 00:36:38,739
And if you get really into this stuff, you can think about multi dimensional models that can go all sorts of different places.

364
00:36:38,740 --> 00:36:46,720
So non parametric models that don't necessarily fit an arc or a straight line and that are trying to map a reality a little bit more closely.

365
00:36:47,460 --> 00:36:51,470
We can we can talk about those as we get a little bit more advanced and stuff like that.

366
00:36:51,850 --> 00:36:57,640
And that's actually what my question is and what you do when you don't have something else.

367
00:36:57,850 --> 00:37:03,400
Yeah. So almost everything that we do here are what we call parametric models, parameters that we know and love.

368
00:37:03,790 --> 00:37:10,120
So these are all based on distributional assumptions that we think map on to reality well enough.

369
00:37:10,840 --> 00:37:16,030
Normal distributions, logistic route distributions, beta gamma distributions.

370
00:37:16,210 --> 00:37:17,710
I have heard of you might not have,

371
00:37:18,070 --> 00:37:27,490
but basically we think in most situations we can find some sort of distributional assumption that kind of maps on to the data.

372
00:37:28,270 --> 00:37:34,390
Now, in practice, if that is true or not, that's when we might start to move into what we call non parametric models.

373
00:37:34,930 --> 00:37:39,010
But most of the modeling analysis will do in this class are based on parametric assumptions.

374
00:37:40,330 --> 00:37:47,080
This one is going to be one of the first parametric assumptions that we make,

375
00:37:47,620 --> 00:37:52,990
that we have a set of residuals that's going to follow a very specific distribution.

376
00:37:53,890 --> 00:37:54,820
Now, distribution,

377
00:37:55,660 --> 00:38:03,219
we like parametric distributions because we know everything about if we have an equation that describes every single thing about that bell curve,

378
00:38:03,220 --> 00:38:08,800
as you see so many times where people lie relative to each other, if you're a standard deviation above or below that type of thing.

379
00:38:09,580 --> 00:38:14,450
That's what makes a lot of this stuff run. Any questions?

380
00:38:14,810 --> 00:38:20,270
The party questions about an area homosexual activity might be a new one.

381
00:38:21,200 --> 00:38:25,900
Questions about this. What is it?

382
00:38:28,100 --> 00:38:38,579
You know, it's kind of there's. This is one example of kind of homeless catastrophe.

383
00:38:38,580 --> 00:38:49,050
If you could think about like a set of power points across the levels of X relative to what we would see that this strike,

384
00:38:49,620 --> 00:38:55,770
the width of the distribution is going to be the same regardless of the level of X.

385
00:38:57,300 --> 00:39:05,190
Okay. So I say in the video, I think I showed you like fitted plots of like a model relative to the residuals and they

386
00:39:05,190 --> 00:39:11,170
just were kind of scattered about a zero line and indicating that they are just there.

387
00:39:11,260 --> 00:39:15,300
There is no real clustering around zero and any kind of pattern.

388
00:39:15,900 --> 00:39:18,240
They're just kind of randomly dispersed across.

389
00:39:18,720 --> 00:39:25,890
We're looking to see if there are any points in our predictor variable or variables where you have a lot

390
00:39:25,890 --> 00:39:32,120
of variability in the residuals relative to some places where there's really kind of a little tighter,

391
00:39:32,120 --> 00:39:36,540
you know, a little easier to visualize like this.

392
00:39:37,050 --> 00:39:42,280
I think. You know, like this.

393
00:39:42,910 --> 00:39:46,370
So you might have predictive value here. You might have a residuals here.

394
00:39:46,390 --> 00:39:53,440
We would expect that each level of the predictor, you're going to have more or less kind of rough, constant distribution and residuals.

395
00:39:54,040 --> 00:39:57,219
We expect them all to kind of cancel out. Right. Is there a residuals?

396
00:39:57,220 --> 00:40:00,250
Again, just the difference between what we predict and what we observe.

397
00:40:01,150 --> 00:40:03,280
Right. We expect them to all kind of cancel out.

398
00:40:03,910 --> 00:40:10,630
But the big question of homicide I was going to ask this is when we look at different levels of action, we go from very low to very high levels of X.

399
00:40:11,260 --> 00:40:14,980
That variability across the residuals is pretty constant.

400
00:40:16,120 --> 00:40:23,110
These are examples of non constant variance or kind of funnel where it starts out pretty wide here and then gets pretty tight.

401
00:40:24,790 --> 00:40:30,220
If I can give you an idea of when you start to see this, it's the extremes of your range of prediction.

402
00:40:31,090 --> 00:40:38,800
So if you have a variable where everybody kind of says or responds and the kind of the same area of the scale,

403
00:40:39,670 --> 00:40:42,730
but then you have parts of the scale that only a few people respond to.

404
00:40:43,510 --> 00:40:46,600
You have less precision of measurement by, generally speaking.

405
00:40:47,020 --> 00:40:49,420
So when you only have a couple of of observations,

406
00:40:49,780 --> 00:40:59,690
the likelihood that you start to see some people kind of at the extremes relative to where the bulk of your observations tend to be, is much higher.

407
00:40:59,710 --> 00:41:04,900
So that's when I typically see hetero scholastic variances.

408
00:41:05,140 --> 00:41:09,760
It's when we have parts of the scale that are used sparingly or that they're really extreme values.

409
00:41:10,040 --> 00:41:14,360
Jenny So is that also scatterplot or. Yeah, yeah.

410
00:41:14,560 --> 00:41:19,690
You don't see that within a scatter plot. So there's a I mean, like how it generates something like, yeah, there's just the scatter.

411
00:41:19,690 --> 00:41:27,280
But yeah, and we'll start. I mean, I think ideally you try to use the example of just like one X value and the outcome variable,

412
00:41:27,280 --> 00:41:31,479
but ultimately we're going to take the whole model and take what are called the fitted values.

413
00:41:31,480 --> 00:41:34,090
So those would be the predicted values across everybody's scores.

414
00:41:34,690 --> 00:41:40,720
Plug all everybody's scores into the the regression model that's going to give me my fitted value relative to the residuals.

415
00:41:45,900 --> 00:41:54,030
Questions about this second one. It's the funkiest one's probably alone, as least familiar to people.

416
00:41:54,060 --> 00:41:59,130
The thing is, it's not going to necessarily ruin your day if it's if some of these assumptions are made.

417
00:41:59,520 --> 00:42:05,280
And we're going to talk about ways to do this, both visually as well as empirically, so that you can take a combination of information.

418
00:42:05,580 --> 00:42:08,630
Or if you don't understand one of them, you just take the one that makes the most sense. Hmm.

419
00:42:09,540 --> 00:42:12,540
That's what I kind of do with some of these normality of errors.

420
00:42:12,780 --> 00:42:21,690
This one is we have, again, what's a residual? We've not heard from Katie Rowe for that.

421
00:42:22,020 --> 00:42:28,829
You're expecting very, very few. The actual repeated in your head until you get it down to a residual of the

422
00:42:28,830 --> 00:42:32,160
expected value relative to the predictive value and take a predictive value,

423
00:42:32,400 --> 00:42:36,600
subtract the of the observed value. And that's what you're that's a residual.

424
00:42:37,470 --> 00:42:40,710
We think that those residual sometimes we're going to go shoot too high.

425
00:42:40,890 --> 00:42:44,580
Sometimes we're going to shoot too low. Ideally, most and most of the times we're right on the money.

426
00:42:45,330 --> 00:42:48,240
But we would expect that across a large enough sample,

427
00:42:48,480 --> 00:42:55,020
you are going to get a mean of zero with some normal distribution or variation around the mean of zero.

428
00:42:56,440 --> 00:42:59,790
Okay, this we will actually estimate this part of our model.

429
00:43:00,060 --> 00:43:04,710
So we'll try it. We will we will use those differences to estimate what that distribution is.

430
00:43:05,070 --> 00:43:10,080
But what we're really looking for is a nice, happy bell curve when we plot those residuals.

431
00:43:12,180 --> 00:43:21,420
Independence. Last one, man I heard from today, Amy, was independent, that your variables are independent or not relating to each other.

432
00:43:23,350 --> 00:43:26,830
If there is a change, variables are there.

433
00:43:29,810 --> 00:43:37,190
Yeah. So the observations should be independent, right? So that the responses from individuals or the whatever data that we're going to use,

434
00:43:37,190 --> 00:43:41,390
the actual unit of analysis, we're assuming that there is no association underlying correlation.

435
00:43:41,690 --> 00:43:49,760
We can see some of those violations when we have cluster data, either because of space, of time, because there are repeated measures.

436
00:43:50,450 --> 00:43:55,590
So we're looking out for those so different ways that we can look at some of the same things who are certain.

437
00:43:55,670 --> 00:43:58,700
We went through almost catastrophe, normality, residuals.

438
00:43:59,300 --> 00:44:05,690
It's never going to be perfect in practice, but we're looking for something that looks kind of happy and normal ish independence.

439
00:44:06,410 --> 00:44:10,130
We're talking this is actually a plot of residuals against each other.

440
00:44:10,640 --> 00:44:16,400
And what we want to see is if we took every single pair of residuals and we looked at the correlation between them,

441
00:44:16,790 --> 00:44:21,920
that there is nothing observable, there's no kind of straight line, there's no grouping or clustering.

442
00:44:22,160 --> 00:44:26,990
We're just expecting this to be more or less kind of a random splatter of dots.

443
00:44:28,250 --> 00:44:31,940
Yeah. Fortunately for this one as well. There's a test for that.

444
00:44:32,090 --> 00:44:35,569
There's a test for this. There's a test for this. So visuals are great.

445
00:44:35,570 --> 00:44:42,350
But if they're not working for you, if you don't if it's not obvious, we can go we can take a look at the the test.

446
00:44:45,670 --> 00:44:52,030
So for each one of those those violations that we were sorry I one of those assumptions that we have we can violate those.

447
00:44:52,450 --> 00:45:03,189
We see a linearity violation there. We showed you examples of the Petoskey nasty city non normality there.

448
00:45:03,190 --> 00:45:05,020
So if we didn't start to see this,

449
00:45:05,080 --> 00:45:11,830
you know how typical of normal distribution or if there seem to be any sort of grouping or clustering in our residual plots,

450
00:45:12,280 --> 00:45:20,290
these are all indications that one of our assumptions could be violated depending on the violation that has implications for our analysis.

451
00:45:20,990 --> 00:45:28,180
Right? We might not be able to trust our coefficients or our standard errors or the R squared.

452
00:45:28,810 --> 00:45:38,370
Okay. Measurement area and access is the one that we really can't test, and that's just the way that we've collected data.

453
00:45:38,940 --> 00:45:43,710
But it is a real life thing. Who said somebody said anxiety or Clara said anxiety earlier?

454
00:45:44,730 --> 00:45:49,889
If I'm not measuring anxiety, but I'm still calling it anxiety, that's a measurement error.

455
00:45:49,890 --> 00:45:52,770
That's an assumption that we're making that I could easily be violating.

456
00:45:53,190 --> 00:45:57,300
But I'm asking you a whole bunch of questions about your, you know, your your life outlook.

457
00:45:57,510 --> 00:46:03,360
And that's not capturing anxiety. We've we've under undercut the analysis in there.

458
00:46:05,190 --> 00:46:10,830
All right. Here's our example from last week and very it's looking familiar at this point.

459
00:46:11,340 --> 00:46:16,980
You've seen us enough. But if we Don, here's our results.

460
00:46:18,420 --> 00:46:28,650
Which one of these is the strongest predictor? And.

461
00:46:34,930 --> 00:46:38,020
Jeffrey Sachs. I know.

462
00:46:43,040 --> 00:46:47,210
I love the game because the people you know.

463
00:46:47,570 --> 00:46:50,570
Mm hmm. Mm hmm. Don't fall into that trap.

464
00:46:51,320 --> 00:47:02,020
P values are kind of. If you want to think about p values, they are individual to the the standard errors of a given predictor.

465
00:47:02,420 --> 00:47:11,000
So we can't compare P values and kind of say which one's going to the strongest 20 and know Eugene regression.

466
00:47:14,090 --> 00:47:17,629
You would think so, but I don't buy that now with this analysis, now this out.

467
00:47:17,630 --> 00:47:25,180
But how come? What is the coefficient for h so small?

468
00:47:27,910 --> 00:47:31,030
Because. Yeah. Well, yeah, exactly.

469
00:47:31,030 --> 00:47:32,500
So these are in their original metrics.

470
00:47:32,860 --> 00:47:38,020
We are not making any sort of distinctions about which is the strongest predictor based on these original metrics,

471
00:47:38,320 --> 00:47:42,490
not if they're scaling differences or that big are. So we would just need to guard it.

472
00:47:42,940 --> 00:47:47,710
Jenny, you are actually right. I think we looked at that last week and we saw that six when we standardize or whatever

473
00:47:48,190 --> 00:47:52,509
did seem to have the biggest variability both from the beta coefficient sizes,

474
00:47:52,510 --> 00:47:55,390
which is a little funny with a dichotomous variable.

475
00:47:55,600 --> 00:48:08,110
But also we don't know the contribution of different ways that we can kind of ascertain the same kind of the same kind of result our residuals.

476
00:48:08,410 --> 00:48:15,160
So now that behind the scenes is going to give you three of them, it's going to take a while to understand the residual,

477
00:48:15,490 --> 00:48:19,870
whereas the stage three of the observed minus predicted standardized residuals.

478
00:48:19,870 --> 00:48:29,409
We like this because, like the term suggests we are, we're we're making a scaling transformation so that they're all kind of on the same scale.

479
00:48:29,410 --> 00:48:33,670
We can start to think about where you are and the distribution of residuals.

480
00:48:33,670 --> 00:48:41,740
Are you a standard deviation above version deviation below? We like that because our assumption is our residuals are normally distributed.

481
00:48:42,160 --> 00:48:50,910
So if I know that you are 1.96 standard deviations above the mean, you were at the 90th percentile, right?

482
00:48:51,160 --> 00:48:57,970
If that's a residuals, 1.96 standard deviations away, that's what's helpful about standardizing our residuals.

483
00:48:59,300 --> 00:49:08,530
All right. So we'll talk a little bit about this to see if, in fact, we have the proper proportion of residuals that are above and below the mean.

484
00:49:09,490 --> 00:49:13,420
All right. It's going to help us kind of get a sense of whether or not we have that normal distribution.

485
00:49:13,930 --> 00:49:18,880
Our student residual is essentially the standard, you know, standardized residual.

486
00:49:19,150 --> 00:49:21,490
Instead, now we're using a student's T distribution.

487
00:49:21,790 --> 00:49:27,880
If you remember those from Furniture State, basically the same kind of thing as a normal distribution has got fatter tails.

488
00:49:28,750 --> 00:49:33,070
And in this case we're using what's called the leverage as a correction factor.

489
00:49:33,580 --> 00:49:38,320
Same so is still very similar interpretation of those standardized residuals.

490
00:49:38,590 --> 00:49:47,080
And now we can expect, you know, roughly 68% to fall in 1 to 1 star deviation below, above and below the mean, for example.

491
00:49:48,700 --> 00:49:52,540
So both of these might be a little bit more helpful than your standard.

492
00:49:52,550 --> 00:49:53,050
I mean,

493
00:49:53,050 --> 00:50:00,760
you're just you're on standardized residual because we're now talking in an language that is going to be transferable across different situations.

494
00:50:01,090 --> 00:50:07,210
So it doesn't matter what model you run, if your standard or if your residuals are standard deviation above it has the same interpretation.

495
00:50:07,390 --> 00:50:16,160
Yeah. Just to clarify, so the command recorded is for a standardized residual in our students for student test, correct.

496
00:50:17,530 --> 00:50:24,459
This is actually I, I forget standard residual. This is going to be just your normal residuals.

497
00:50:24,460 --> 00:50:32,460
This is our student. I forgot to put this on here.

498
00:50:33,030 --> 00:50:39,730
Sorry to have. I forget which one of the commands first.

499
00:50:41,130 --> 00:50:52,110
If you look into it, you can look real quick at the Help menu for Elm and it should tell you which one what it is for standardized residuals.

500
00:50:53,430 --> 00:50:55,560
This last one, which is freeware I forgot,

501
00:50:56,430 --> 00:51:04,110
is just showing you that you can take residuals that are that are calculated by our model and save them as a new variable.

502
00:51:05,100 --> 00:51:09,690
The reason why that's helpful is if we want to say our SNR residuals are normally distributed,

503
00:51:10,080 --> 00:51:15,060
then we might want to just graph or plot our variable or our new column of residuals.

504
00:51:15,810 --> 00:51:21,960
So by doing this and taking my data frame and indexing a new variable that I'm calling student residuals,

505
00:51:22,920 --> 00:51:29,010
just taking those residuals from this model, saving them as a new journal, which I can now manipulate or play with.

506
00:51:32,990 --> 00:51:37,100
L.A. folks are familiar with this concept. Yes.

507
00:51:37,760 --> 00:51:44,960
Any questions about it? It's a very large value, something that we want to that we want to be mindful of,

508
00:51:45,740 --> 00:51:54,620
maybe contrast to in an influential observation about any questions about this from the video.

509
00:51:56,240 --> 00:52:01,580
An outlier. It's just something that's beyond or like kind of far away from the rest of the data points.

510
00:52:02,750 --> 00:52:07,280
An influential observation is polling our regression line more than it should.

511
00:52:08,060 --> 00:52:13,760
It's getting more weight or it's having a bigger effect on the line than the rest of the values in the data.

512
00:52:15,140 --> 00:52:24,030
Okay. Our influential observations are all outliers.

513
00:52:24,600 --> 00:52:34,110
Outliers are not necessarily influential observations. Okay, so you can be kind of way off in space, but still kind of fit with inverted values.

514
00:52:34,890 --> 00:52:40,950
But you're probably you're you're not going to be pulling that line in unexpected directions if you're not an outlier.

515
00:52:42,990 --> 00:52:48,030
I'm going to have a picture of that. Oh, yeah. In this case, where is the outlier?

516
00:52:48,760 --> 00:52:57,370
Not heard from? Who wants to jump in, but what number is the outlier saying, hey, it's an outlier, where are you?

517
00:52:57,420 --> 00:53:00,810
Where would you look for it is to make that determination. Somebody shout it up.

518
00:53:03,550 --> 00:53:10,590
Put it here, let me say. Is that it?

519
00:53:12,450 --> 00:53:15,690
Yeah. What are you doing? You saw something. You're right.

520
00:53:20,200 --> 00:53:25,310
It's very hard to look his way over here.

521
00:53:25,610 --> 00:53:35,380
That's one indication that we have an outlier. Absolutely. Another one is when we look at the either cook's distance or leverage values here.

522
00:53:35,390 --> 00:53:39,590
So coexistence is a good it's a good measure of kind of an outlier.

523
00:53:40,310 --> 00:53:43,970
I think we're looking for values that might exceeded for a good there. The metric is like one.

524
00:53:44,990 --> 00:53:48,740
I think we've done that with number eight here. Right. It is identified as an outlier.

525
00:53:48,740 --> 00:53:54,440
Notice that as a residual, it's in its own kind of original metrics in units.

526
00:53:54,770 --> 00:54:02,090
It's not so obvious. Right. So this is why I would caution you away from just looking at the actual residuals,

527
00:54:02,390 --> 00:54:06,770
if you saw these this line of residuals, you wouldn't necessarily think that it's an algorithm.

528
00:54:07,250 --> 00:54:15,550
But as soon as we look at that as a distance, again, kind of an indicator, an outlier, we see that this is an extreme outcome.

529
00:54:16,340 --> 00:54:24,860
What are we going to do with an outlier like this? Why are you doing all this?

530
00:54:25,490 --> 00:54:30,360
We're going to get rid of this. No, that's not an order made by me, honestly.

531
00:54:30,380 --> 00:54:33,710
So what we're going to do is we're gonna take a look. All right? We're going to see why is this?

532
00:54:34,010 --> 00:54:39,230
Is this observation an outlier? Is it because we made a scaling mistake and we put go to the data alone?

533
00:54:39,530 --> 00:54:44,810
Is there something particularly unique about case number eight here that we want you to just

534
00:54:45,530 --> 00:54:52,420
think through and and make sure that we're accounting for our model with or without that case?

535
00:54:52,750 --> 00:54:55,880
Right. So we don't want to just drop because it's an outlier.

536
00:54:56,540 --> 00:54:58,550
We want to be intentional about some of these decisions.

537
00:54:59,330 --> 00:55:05,840
But it might be that this was just an error, or it might be that this is an observation that really doesn't fit.

538
00:55:06,260 --> 00:55:09,350
And therefore, we might want to just change the way that we talk about our model.

539
00:55:09,680 --> 00:55:14,030
Our model might be generalize to cases that look like one through seven, but maybe not cases.

540
00:55:14,990 --> 00:55:19,309
All right. We also see that this had values that we didn't value.

541
00:55:19,310 --> 00:55:22,580
Number eight has an extreme amount of leverage.

542
00:55:22,820 --> 00:55:26,840
Leverage ranges from zero nor influence to one, like complete influence.

543
00:55:27,860 --> 00:55:32,300
This dotted line is one just cases 137 are in the model.

544
00:55:33,800 --> 00:55:38,100
So it's really. This is the concept.

545
00:55:38,520 --> 00:55:41,640
The start line is when cases 137 are in the model.

546
00:55:42,780 --> 00:55:47,730
This sort of line is when new case eight is also in the model.

547
00:55:48,570 --> 00:55:51,840
What is the distinction? That's huge.

548
00:55:52,530 --> 00:55:53,550
That's huge.

549
00:55:53,880 --> 00:56:03,420
We went from a strong, strong near vertical association between these two variables to one that looks still positive but not nearly as strong.

550
00:56:06,660 --> 00:56:12,780
And we even have a metric for this influence and we see there relative to everything else in the model, these are all kind of below 0.2.

551
00:56:13,200 --> 00:56:17,100
So not tons and tons of leverage or influence. Eight.

552
00:56:17,310 --> 00:56:23,820
Huge, huge. It's almost as if there is age and everything else.

553
00:56:24,730 --> 00:56:30,120
That's kind of what this is saying. So you might have observations like this in your data, but a really small sample sizes.

554
00:56:30,120 --> 00:56:35,670
But not only you might have an outlier neighborhood, you might have a clinic that's that average,

555
00:56:35,680 --> 00:56:38,010
just kind of different, different population, whatever.

556
00:56:38,370 --> 00:56:45,989
You could very easily see some of these data points having an extreme influence on whether that line is positive,

557
00:56:45,990 --> 00:56:50,730
flat, whatever questions about anything you see up here.

558
00:56:53,940 --> 00:56:58,590
Yeah. So. Brain cracking,

559
00:56:58,830 --> 00:57:04,930
but you have a really high value that's a very high rate because of something that

560
00:57:04,930 --> 00:57:09,580
perhaps we didn't account for when we were setting up the skills or our data collection.

561
00:57:11,020 --> 00:57:13,760
So I'm not sure what you mean by identical for me.

562
00:57:13,780 --> 00:57:22,330
It could be like if you were expecting a range of values and someone had responded well, well outside of the expected range.

563
00:57:22,840 --> 00:57:29,380
That could be something that might happen. Yeah, like maybe like an event that happened to just that one person to make them.

564
00:57:29,560 --> 00:57:32,920
Yeah. And then we wouldn't know unless we dove into that.

565
00:57:33,550 --> 00:57:36,970
Yeah. And that's, that's the challenge. So if you, all you have is access to the numbers,

566
00:57:37,720 --> 00:57:43,240
you can certainly end up having to make some difficult decision about what do we do with Case eight here?

567
00:57:43,510 --> 00:57:49,840
Do we keep chasing and do we do we share results for both the analysis with and without case?

568
00:57:50,860 --> 00:57:54,040
And then ultimately, what's our what's our path forward?

569
00:57:54,790 --> 00:58:00,939
But yeah, there are certainly some situations where you could have a very valid point that's a board, an outlier, and very influential.

570
00:58:00,940 --> 00:58:03,100
And you have to decide, well, what am I going to do with it?

571
00:58:06,430 --> 00:58:16,060
In contrast, I think here is my best attempt at a point that is an outlier but not especially influential.

572
00:58:16,570 --> 00:58:27,260
So here number nine is a kind of little bit further away from the rest of the pack, but not so far outside.

573
00:58:27,520 --> 00:58:30,610
But it's not skewing the line at all. Right.

574
00:58:30,910 --> 00:58:34,690
So it's probably an outlier. I couldn't make that very much higher.

575
00:58:35,770 --> 00:58:43,510
But earlier, I mean, we want this to be a little bit closer to one, but just kind of visually, it seems like it's well away from one through seven.

576
00:58:43,750 --> 00:58:46,990
But because it's in line with predictions, it's not especially influential.

577
00:58:47,440 --> 00:58:52,930
It's not changing the way the lines shape or the way the line is is directed.

578
00:58:53,980 --> 00:58:58,860
In contrast, again to eight, which is both an outlier and an influential observation.

579
00:59:05,470 --> 00:59:13,990
Questions about this. It never hurts to plot things if you want to.

580
00:59:16,090 --> 00:59:20,410
But we also have numbers that we can use if we want.

581
00:59:21,010 --> 00:59:27,700
They usually kind of exceed one from distance values that are close to one in terms of our hat or leverage values.

582
00:59:29,950 --> 00:59:34,570
And even if we're not getting this extreme, you know, something that's 0.98, 2.96.

583
00:59:34,870 --> 00:59:40,150
We might see things that if everything else is kind of hovering around point one, one, you have something that's point four,

584
00:59:41,290 --> 00:59:43,899
.35 that might be enough to say, all right, well,

585
00:59:43,900 --> 00:59:50,080
I should really look at this observation and see why it's the different questions, comments, concerns.

586
00:59:54,950 --> 00:59:58,970
Uh, this is just a slide about that kind of leverage.

587
00:59:59,580 --> 01:00:03,560
Um, just I think this is probably the key.

588
01:00:04,100 --> 01:00:07,760
Basically, values that are really far away from the need are going to pull them in.

589
01:00:08,060 --> 01:00:09,830
Right. This is the same that you have.

590
01:00:09,830 --> 01:00:17,090
It's just intuitively something that would pull like if you think about a very, very wealthy person in the sample of incomes, can pull them up.

591
01:00:17,690 --> 01:00:20,810
That's one of our problems with means versus other forms of central tendency.

592
01:00:21,140 --> 01:00:24,740
So leverage is basically looking for those values that are a long ways away from me.

593
01:00:28,040 --> 01:00:30,770
Consistencies are a good metric of outliers.

594
01:00:30,980 --> 01:00:37,670
This is the formula you don't use, other than to see that it is also in some ways connected to those same hat values, those leverage values.

595
01:00:38,000 --> 01:00:40,940
If we again think of those leverage values as a long way away from the mean,

596
01:00:41,270 --> 01:00:49,220
it might not be surprising that our identification of outliers is going to be dependent on those ones that are far away from the mean.

597
01:00:49,940 --> 01:00:55,880
So both of these are stored within those regression objects we can use.

598
01:00:56,270 --> 01:01:02,150
We can save just like with leverage, the leverage, values and quirks, distance.

599
01:01:03,140 --> 01:01:07,610
We can say these as variables, and then it's a lot easier for us to call them up and say,

600
01:01:07,820 --> 01:01:12,200
Well, are any of them bigger than one or any of them greater than 0.6?

601
01:01:12,650 --> 01:01:13,580
Right. Those types of things.

602
01:01:13,590 --> 01:01:20,390
That's where we can use logical statements in order to quickly identify outliers versus trying to scan through all 850 residuals.

603
01:01:24,410 --> 01:01:31,250
Um, so speaking of those residuals, there are some things that we should know, right?

604
01:01:31,250 --> 01:01:37,190
If we believe that that assumption is true that they are normally distribute or on a mean of zero.

605
01:01:37,760 --> 01:01:45,230
We can look at those other standardized residuals or student residuals to see whether or not they have roughly a normal shape.

606
01:01:45,920 --> 01:01:49,500
And if we have too many values that we would consider to be, quote unquote, outliers.

607
01:01:49,510 --> 01:01:53,180
So, for example, like the 5% do,

608
01:01:53,180 --> 01:01:59,690
we see more than 5% of our values at the extreme tables that might suggest we have a few more outliers than we might expect.

609
01:02:00,320 --> 01:02:08,420
Same thing with Cookie. We can look to see if there are any values that exceed one, and then those might be worth some further investigation.

610
01:02:08,750 --> 01:02:12,980
So in a sample of 850, how many values might be greater than than one?

611
01:02:13,250 --> 01:02:19,580
We can do this quickly and easily and ah, we can do it on Thursday so that you can say, All right, I have five outliers that I want to look at.

612
01:02:20,240 --> 01:02:23,660
And is that roughly what I would expect in a sample of 850 people?

613
01:02:24,260 --> 01:02:28,550
Now, last point here is just don't delete them out of offhand.

614
01:02:29,000 --> 01:02:35,959
Take a minute to see whether or not it's a scaling mistake, an entry mistake, whatever that might be explaining.

615
01:02:35,960 --> 01:02:42,020
Some of us are almost going to assess the assumption, I think.

616
01:02:42,020 --> 01:02:48,650
I mean, with this one, the slide is just talking about some of the things that we can do, including making some of those transformations.

617
01:02:48,890 --> 01:02:54,020
If we see that when we look at our predictor variables relative to those residuals,

618
01:02:54,380 --> 01:03:01,010
that the residual variance around the mean of zero is different at different levels of our predictor variables.

619
01:03:02,960 --> 01:03:08,510
That's very jargony. You don't see it happen super, super often in practice,

620
01:03:09,020 --> 01:03:13,579
but it's one of the things that we look out for again when we have a whole lot of

621
01:03:13,580 --> 01:03:18,530
people clustered around one part of our scale and then fewer people at the extremes.

622
01:03:18,950 --> 01:03:28,010
So those anchors that we sometimes have to worry about are very skewed measures are often more likely to have some of these violations which.

623
01:03:29,820 --> 01:03:34,800
This is an example of homeless, good tastic irreverence.

624
01:03:35,070 --> 01:03:39,299
I've never seen someone as bright. Never, never. Once in my life, I've seen what it looks like.

625
01:03:39,300 --> 01:03:45,840
This in practice. This is whatever. Anyways, you take your food values, you compared your residuals,

626
01:03:46,230 --> 01:03:50,430
and we're looking for more or less kind of an even distribution that in values would

627
01:03:50,430 --> 01:03:54,360
be like what I would expect if I put in all of your data into my regression equation,

628
01:03:54,600 --> 01:04:01,509
it's going to spit out a projected value. You can also look at individual variables.

629
01:04:01,510 --> 01:04:07,830
So if you thought that, for example, there is one variable that's kind of thrown off your analysis, you could run kind of the same plot.

630
01:04:08,280 --> 01:04:15,210
This is all back to saving those residuals and taking a look at probably things that you didn't think that you would ever want to do.

631
01:04:15,870 --> 01:04:23,580
But you can do this. You can look at those residuals for outliers, for super high leverage values, for interscholastic error variance.

632
01:04:24,390 --> 01:04:31,410
They're all just waiting for you in those regression objects. Transformations.

633
01:04:32,100 --> 01:04:40,259
One, we find that, for example, we have some of those violations of homeschoolers to see if we have normal distributions,

634
01:04:40,260 --> 01:04:45,210
quote unquote, that don't look so normal. We can make some of these transformations based on what we see.

635
01:04:45,810 --> 01:04:58,380
So if we see skewness, if we see that we want to either pull data out because it's too top Coptic or whatever it is,

636
01:04:58,590 --> 01:05:01,680
the pickiness is too high versus it's too fair.

637
01:05:02,370 --> 01:05:05,910
These different sorts of transformations are available to us.

638
01:05:06,240 --> 01:05:09,840
I would always start with the smallest transformation you can use.

639
01:05:10,780 --> 01:05:14,040
Now, none of these are linear transformations. That's important, right?

640
01:05:14,370 --> 01:05:21,120
Because that's not going to change the shape of the distribution. If I just subtract five from everything, your residual distribution will not change.

641
01:05:22,290 --> 01:05:32,720
Okay. All of these are nonlinear distribution transformations, which is going to make it hard once we start manipulating things to go back.

642
01:05:33,190 --> 01:05:36,710
That makes sense. But this is defensible.

643
01:05:36,800 --> 01:05:44,240
It's used in practice, and it might help you change some of the variables that you either use in your analysis or is your outcome variable.

644
01:05:44,510 --> 01:05:47,690
To get to a model that does not have some of these violations.

645
01:05:48,500 --> 01:05:51,710
Right. The problem is you have to change your interpretation.

646
01:05:52,310 --> 01:06:04,490
You're not. Now, you might now be talking about the square of anxiety instead of the anxiety score or nzd score range from 1 to 5 now might range

647
01:06:04,490 --> 01:06:13,760
from 1 to 25 because we've squared every single person's anxiety outcome score and now we're predicting the squared anxiety score,

648
01:06:14,680 --> 01:06:20,120
you know. So this just refers to the individual.

649
01:06:22,070 --> 01:06:26,190
It can be any variable, so it won't be the residuals per se.

650
01:06:26,210 --> 01:06:31,860
It's going to be the variables that you actually include in your model. So it could be your self acceptance variable.

651
01:06:31,880 --> 01:06:35,030
It could be your age variable. It could be your outcome variable.

652
01:06:35,030 --> 01:06:41,659
Anxiety is variable, for example, because it has that big scale from well and they really ranges from like 14 to 16.

653
01:06:41,660 --> 01:06:46,580
But if we did an entire population of people and we had a whole bunch of people who are emerging adults,

654
01:06:46,580 --> 01:06:50,210
20 to 30, but then five or eight people who are 65,

655
01:06:50,510 --> 01:06:59,510
we we might see a very skewed distribution and we could use like a log of that scale to get people into something that looks a little bit more normal.

656
01:07:03,710 --> 01:07:07,280
I'm dubious, but whatever. People do it. It's okay.

657
01:07:08,450 --> 01:07:13,400
I don't mean to throw you off. No, not to use it. Incomes are notoriously a difficult one.

658
01:07:13,610 --> 01:07:16,430
People take lots of incomes all the time because they're so it's such a big number.

659
01:07:17,480 --> 01:07:23,870
But there are other ways, other things that we might want to do at this point. I do want to get to in a couple minutes here.

660
01:07:26,000 --> 01:07:30,110
These are just this is kind of presaging what we're going to hear about a little bit later on in the semester.

661
01:07:30,650 --> 01:07:36,440
Amy had mentioned that when we turn, we have observations that are not that are correlated with each other.

662
01:07:36,770 --> 01:07:40,879
Usually they're for reasons like this. There are a lot of correlation.

663
01:07:40,880 --> 01:07:41,810
Just means time.

664
01:07:42,170 --> 01:07:49,580
If you if you're asked the same thing in quick succession, those reports are probably going to be more highly correlated than if we spaced them out.

665
01:07:49,970 --> 01:07:59,870
If you live next to people, chances are your your responses to items might be more similar than your responses to somebody who lives in New Mexico.

666
01:08:00,420 --> 01:08:10,610
Right. If you have clusters based on school, based on classroom, based on clinic, based on family, based on work,

667
01:08:11,180 --> 01:08:18,020
profession, all of that might create some similarity and some non independence within your scoring responses.

668
01:08:18,530 --> 01:08:20,870
That's what different models are going to try to account for.

669
01:08:21,110 --> 01:08:26,000
Of all the violations that I've talked about today, this is the one that's going to mucky up the most.

670
01:08:26,630 --> 01:08:32,180
And this is why there are multiple models and new models for dealing with this kind of data.

671
01:08:32,840 --> 01:08:39,800
This is why we have multilevel models and time series models and spatial analytics, because this is such a big deal.

672
01:08:40,760 --> 01:08:43,760
So this is one to test we have. It's the Durban Watson test.

673
01:08:43,760 --> 01:08:52,790
You can test. We'll do that on Thursday. But for a quick glance, if you worried about some of the violations, this is what's going to happen.

674
01:08:53,180 --> 01:08:59,510
So, for example, nonlinearity, if you're if you're trying to fit a linear model to nonlinear data wherever it is brownness,

675
01:09:00,290 --> 01:09:05,610
you're not going to build trust. Your regression coefficients for an inference are okay.

676
01:09:06,020 --> 01:09:10,610
If you have non-normative residuals, you're not able to trust your regression coefficients.

677
01:09:10,970 --> 01:09:14,450
It actually is not going to have a major impact on the test of significance.

678
01:09:15,290 --> 01:09:18,590
So the test might be okay, but their what they're testing is not going to be accurate.

679
01:09:19,460 --> 01:09:25,490
Hetero is good test, isn't it? You're actually your coefficients are going to be pretty active at 945.

680
01:09:26,720 --> 01:09:32,660
Your coefficients are going to be pretty accurate. But I'd be a little bit worried about the inference that you're trying to make and kind of on down.

681
01:09:32,660 --> 01:09:40,730
That's how I want you to read this one more time. If I had if I had my choice, I'd want this one to work.

682
01:09:41,540 --> 01:09:45,920
The ones that are both like the super frowning. This is one just completely missed the vote.

683
01:09:46,070 --> 01:09:50,270
You're not running a linear regression model. That's why Nonlinear Anonymous.

684
01:09:50,270 --> 01:09:56,270
Because they're just using the wrong variables. Like if you could wear off and try to predict anxiety with your foot size or something.

685
01:09:56,750 --> 01:10:01,400
So the ones that we see in practice are these four. These are the ones that we have the most influence over.

686
01:10:02,630 --> 01:10:08,720
And this is the one that we're going to spend lots and lots of time using different kinds of models to help fix.

687
01:10:09,170 --> 01:10:13,690
But be wary of these three other ones. And that's what we're going to do on Thursday request.

688
01:10:14,120 --> 01:10:21,200
We made it through this lecture. You have these resources online as well, which means Thursday is going to be entirely working in our day.

689
01:10:21,770 --> 01:10:24,800
I'm excited about that. I'm excited to see on Thursday.

690
01:10:24,810 --> 01:10:28,220
I hope you all have a wonderful day. This is what we're going to be doing.

691
01:10:28,430 --> 01:10:32,810
Guess what? If you did this last week, you're 95% of the way done.

692
01:10:33,260 --> 01:10:39,300
Because all we're going to do is take that that regression object and we're going to run some of these additional things outside.

693
01:10:39,800 --> 01:10:45,140
I've got to go. What's Skip? No. What's our chemistry and the assignments?

694
01:10:45,140 --> 01:10:48,230
We just executed you by? 11:59 p.m.

695
01:10:48,740 --> 01:10:52,120
Is that correct? That's supposed to be. Yeah.

696
01:10:53,000 --> 01:10:58,250
By and large, I'm pretty lenient on the the times of when you submit stuff.

697
01:10:59,690 --> 01:11:01,130
All right, folks, I appreciate.

