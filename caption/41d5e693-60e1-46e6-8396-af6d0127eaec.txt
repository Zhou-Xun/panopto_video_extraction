1
00:00:07,680 --> 00:00:19,280
Okay. My son.

2
00:00:36,440 --> 00:00:41,880
Is it? I would you?

3
00:00:51,840 --> 00:01:12,030
You. He gets kicked off for being.

4
00:01:19,470 --> 00:01:22,930
Yeah. Yeah.

5
00:01:33,720 --> 00:01:40,690
Yeah, I.

6
00:01:49,850 --> 00:02:04,430
Oh, I see. So it's going.

7
00:02:08,540 --> 00:02:55,110
I doing? I got to go.

8
00:02:56,370 --> 00:03:20,610
She? All right, everyone.

9
00:03:21,090 --> 00:03:25,620
Good morning. Have your attention. We're going to get started here.

10
00:03:26,010 --> 00:03:30,450
So we have a bit of a problem with the screen here.

11
00:03:30,450 --> 00:03:36,080
As you can see, they may know how to lower this more. I can't figure it out here if they have some thoughts.

12
00:03:36,090 --> 00:03:39,540
It doesn't seem to be one of the options on the control panel.

13
00:03:41,010 --> 00:03:44,220
So absent that, I think we're probably going to have to press forward.

14
00:03:45,480 --> 00:03:52,500
And I will the notes are posted, so I guess we'll just have to kind of do all we can with this.

15
00:03:52,500 --> 00:03:55,680
And I will get in touch with tech support after the last day.

16
00:03:55,680 --> 00:03:58,980
Hopefully it won't happen again. So sorry about that.

17
00:03:59,700 --> 00:04:08,250
Before we get started, does anybody have any questions about the last lecture labs, homeworks, anything?

18
00:04:09,290 --> 00:04:14,470
Yes. Are you going to get that out? Figure out how to get sass.

19
00:04:14,950 --> 00:04:18,370
Yeah, you will definitely go over that. I'll make sure that guys know about that as well.

20
00:04:19,580 --> 00:04:27,400
Other questions. All right.

21
00:04:28,450 --> 00:04:34,120
So last time we talked about we're still in our review here.

22
00:04:34,570 --> 00:04:38,890
Before we jump into our first module, which is going to be simple linear regression.

23
00:04:39,820 --> 00:04:48,190
So we've been reviewing the idea of random variables, research questions, outcomes,

24
00:04:48,880 --> 00:04:55,570
exposures, variable types, different ways to summarize these random variables.

25
00:04:56,410 --> 00:05:04,030
We talked about measures of center like the mean and the median and measures of spread, which are the variance in the standard deviation.

26
00:05:06,220 --> 00:05:14,860
And then we talked about when you would use the median versus the mean, and we talked about the median being a little bit more.

27
00:05:15,430 --> 00:05:23,440
We used the term robust. The meeting is more robust when you have a skewed distribution because it will still reflect a better

28
00:05:23,440 --> 00:05:29,710
measure of the center of the distribution than the mean will in the presence of a skewed distribution.

29
00:05:31,900 --> 00:05:36,520
We talked about the definitions of the variance and the standard deviation.

30
00:05:37,090 --> 00:05:44,920
The variance is the average squared distance from the mean, and then the standard deviation is just the square root of the variance.

31
00:05:47,400 --> 00:05:52,559
And then we looked at this example, showing two distributions that have the same center,

32
00:05:52,560 --> 00:05:56,540
but one has a larger standard deviation than the other and it's more spread out.

33
00:05:56,550 --> 00:06:04,770
So this is just demonstrating that the larger the variance or the standard deviation, the more spread out the distribution will be.

34
00:06:07,650 --> 00:06:17,160
Then we looked at our first example. Right. So this was the example of the effect of smoking during pregnancy, on birth weight of newborns.

35
00:06:17,550 --> 00:06:26,459
Right. So this was where we looked at identifying which variables we would use for the response,

36
00:06:26,460 --> 00:06:30,250
in this case birth weight, and then a predictor which is smoking level.

37
00:06:32,250 --> 00:06:35,579
So and we looked at some summary statistics, right?

38
00:06:35,580 --> 00:06:45,149
So I think last time there was maybe some confusion about the ideas of parameters versus statistics.

39
00:06:45,150 --> 00:06:49,110
So we're going to talk a lot about that today. But just notational.

40
00:06:50,130 --> 00:06:58,590
I want everyone to to be clear on the the the hat over a symbol means estimate.

41
00:06:59,040 --> 00:07:05,369
So the symbol here is means of a so that represents the mean of age.

42
00:07:05,370 --> 00:07:08,399
So A is age. And then the hat over.

43
00:07:08,400 --> 00:07:11,810
The mew represents the estimate of age.

44
00:07:11,820 --> 00:07:19,110
We also write that as a bar. So the average age here is 24.9 years.

45
00:07:19,890 --> 00:07:26,670
Similarly with Sigma. Right. So Sigma is the parameter that represents the standard deviation.

46
00:07:26,940 --> 00:07:30,960
So Sigma sub a then is the standard deviation of age.

47
00:07:31,650 --> 00:07:35,940
So Sigma hat is the estimated standard deviation of age.

48
00:07:38,100 --> 00:07:41,730
So and this goes for the other, the other variables as well.

49
00:07:41,740 --> 00:07:50,460
Right? So we can look at the average. The mean smoking rate in this sample is 1.37 packs per day.

50
00:07:50,970 --> 00:07:55,170
The standard deviation of the smoking rate is 0.79 packs per day.

51
00:07:55,500 --> 00:08:02,250
So remember we talked about the variance doesn't have the same units as the as the original variable.

52
00:08:02,250 --> 00:08:05,460
So that's why we like to look at the standard deviation because that puts the,

53
00:08:05,640 --> 00:08:11,850
the units back on the packs per day or the year is or what the original variables units are.

54
00:08:12,270 --> 00:08:14,730
The standard deviation takes those units as well.

55
00:08:16,320 --> 00:08:29,340
So, so again, the key point here is that mu and sigma are the parameters and then mu hat or sigma hat and then something bar are the estimates.

56
00:08:30,030 --> 00:08:40,590
So we're going to talk more about that as well. And we talked about the the the the kind of the crux of this course is, is regression analysis.

57
00:08:40,920 --> 00:08:46,649
And the type of regression analysis that you undertake is going to depend in large

58
00:08:46,650 --> 00:08:53,460
part on the character of the variable that is your response or your outcome.

59
00:08:54,930 --> 00:09:02,370
So I'm going to use these terms outcome response dependent variable interchangeably,

60
00:09:02,370 --> 00:09:06,540
and I'm going to try to use them kind of equally proportionally so that you

61
00:09:06,540 --> 00:09:11,340
all can get an idea of these are these are terms for the same the same thing.

62
00:09:12,600 --> 00:09:14,669
And we'll talk more about that as well.

63
00:09:14,670 --> 00:09:26,550
But we went over a few examples here of different kinds of response variables that could be analyzed using these various regression frameworks.

64
00:09:27,420 --> 00:09:35,370
And the thing that unifies these regression frameworks is that they're all intended to measure the

65
00:09:35,370 --> 00:09:44,820
effect of one or more predictors or independent variables on an outcome response or dependent variable.

66
00:09:46,250 --> 00:09:51,450
So anybody any questions so far? All right.

67
00:09:52,650 --> 00:09:59,310
So as I said, we want to make sure we distinguish between parameters and statistics.

68
00:10:00,660 --> 00:10:09,210
So a parameter is the quantity in the population that you're trying to generally that you're trying to estimate from your sample.

69
00:10:09,600 --> 00:10:18,390
So the population is the large group of individuals that you're trying to make inferences about.

70
00:10:18,420 --> 00:10:30,300
So in our smoking and birthweight example, that would be the entire population of the entire group of women with a first pregnancy.

71
00:10:30,480 --> 00:10:35,280
Right. So that's the population that we would draw that sample from.

72
00:10:36,390 --> 00:10:47,700
So the the the parameter value, the idea is that we can only compute that if we had data or values for everybody in the population.

73
00:10:48,420 --> 00:10:52,350
And I think I mentioned last time that this is a concept.

74
00:10:53,160 --> 00:10:59,610
There's a concept of the census. Right. So the census is where you do measure every single individual in a population.

75
00:10:59,970 --> 00:11:06,330
But typically that's not possible. So the parameter is kind of a a theoretical construct.

76
00:11:07,470 --> 00:11:16,680
So it's the target of our inference. But we don't really know what it is because we don't have information on everybody in the population.

77
00:11:17,520 --> 00:11:23,700
So the statistic or the estimate, so this is when we get the hat or the bar over these quantities,

78
00:11:23,700 --> 00:11:35,850
the parameters is the quantity from the sample that we're using as I guess our best educated guess toward the value of the parameter.

79
00:11:36,510 --> 00:11:45,450
So, so kind of, if you think about the parameter is sort of the target and the statistic is the the guess at that target.

80
00:11:46,020 --> 00:11:52,680
Right. So we're always trying to understand about the parameter, which is the population characteristic.

81
00:11:52,980 --> 00:11:59,850
Using statistics from samples and statistics are calculated from sample data.

82
00:12:00,420 --> 00:12:08,129
So here we have an example, right? So the proportion of high school seniors who have consumed alcohol during the past 30 days can

83
00:12:08,130 --> 00:12:13,290
be estimated based on a carefully selected representative sample from the target population.

84
00:12:13,320 --> 00:12:17,280
So let's think about this. So what's that? What's the parameter here?

85
00:12:17,590 --> 00:12:25,440
They. We want to take a guess. So that's the population.

86
00:12:27,450 --> 00:12:32,870
What about the parameter? So again.

87
00:12:35,440 --> 00:12:38,200
So the proportion of those who consumed alcohol. Yes.

88
00:12:39,850 --> 00:12:46,960
And then the statistic would be the estimate of that proportion that we that we calculate from our sample.

89
00:12:47,350 --> 00:12:54,670
So remember, if we when we looked at the definition of the mean back here.

90
00:12:54,910 --> 00:13:01,870
Right. So the mean is the sum of the individual response values divided by the sample size.

91
00:13:02,290 --> 00:13:06,430
So in the case of the alcohol consumption in high school students,

92
00:13:06,940 --> 00:13:13,960
you would sum up the number of those who reported consuming alcohol and then divide by end.

93
00:13:14,260 --> 00:13:17,350
Right. So why here would be zero or one?

94
00:13:17,770 --> 00:13:21,850
Because somebody either did or did not consume alcohol in the past 30 days.

95
00:13:22,360 --> 00:13:28,370
So and we talked about that. Right. So this is an example where wise are only ones or zeros.

96
00:13:28,810 --> 00:13:33,640
So our parameter is a proportion. So it's between zero and one.

97
00:13:35,660 --> 00:13:41,050
So. Oops. So another question here.

98
00:13:41,060 --> 00:13:45,830
If we draw another sample from that population, does the parameter change?

99
00:13:46,220 --> 00:13:52,930
Anybody want to shout it out? Correct. Yes. The parameter is not going to change the parameters fixed in the population.

100
00:13:53,390 --> 00:13:57,100
Will the estimate be different? Yes, absolutely.

101
00:13:57,280 --> 00:14:02,680
Great. The summit will be different because the estimate is a random variable.

102
00:14:03,280 --> 00:14:07,120
Right. So our sample is composed of random variables.

103
00:14:07,630 --> 00:14:09,130
And then the estimate,

104
00:14:09,400 --> 00:14:18,700
the statistic that we compute from that sample is also a random variable because it's typically some sort of function of the data points.

105
00:14:19,030 --> 00:14:27,510
Right. So when the data points are random, then the estimates are statistics that we calculate from them will also be a random quantity.

106
00:14:27,530 --> 00:14:39,340
So when you re sample or sample again from the same population, that value will change, but it will change in a predictable way.

107
00:14:39,520 --> 00:14:46,660
We're going to talk about sampling distributions, and it gets a little tricky because it gets a little meta because we have data

108
00:14:46,840 --> 00:14:51,160
that have distributions and then we have estimates that also have distributions.

109
00:14:51,460 --> 00:14:54,910
So we'll we'll be more precise about this in a little bit.

110
00:14:56,320 --> 00:15:02,410
So that's what I meant when I'm saying sampling distributions, I'm talking about the sample to sample variation.

111
00:15:02,740 --> 00:15:06,130
Right. So this is the the essence of uncertainty.

112
00:15:06,160 --> 00:15:07,780
So in statistics,

113
00:15:08,170 --> 00:15:18,610
a big part of what we're doing is trying to characterize the uncertainty that's resulting from having only a sample rather than the entire population.

114
00:15:19,270 --> 00:15:24,910
Right. So you can kind of foresee where some of the some of this is going.

115
00:15:25,120 --> 00:15:30,910
So we can think about things like, well, if you had the whole population, you'd know the parameter value, right?

116
00:15:31,210 --> 00:15:39,850
So if you had half the population in your sample, that's going to be a better estimate than if you had a quarter of the population in your sample.

117
00:15:40,090 --> 00:15:42,820
Right. So these are notions of sample size.

118
00:15:43,120 --> 00:15:51,580
So kind of the larger the sample, the closer in some sense you're getting to having an estimate that's computed from the full population.

119
00:15:54,070 --> 00:15:56,290
So the sampling distribution, right.

120
00:15:56,770 --> 00:16:07,030
So when you when you sample individuals from the population and calculate these estimates, as we said, those are random variables, right?

121
00:16:07,040 --> 00:16:14,530
When you repeat this experiment or this sampling, you're going to get a different value for your estimate.

122
00:16:14,860 --> 00:16:22,900
So the sampling distribution then is the distribution of all possible values of the estimate from all possible samples.

123
00:16:24,010 --> 00:16:31,160
So the mean or the center of the sampling distribution is the expected value of the estimate.

124
00:16:31,180 --> 00:16:40,210
So, so expected value is another term for mean. So we've seen that that little symbol E, the capital E, that's the expected value operator.

125
00:16:40,510 --> 00:16:47,050
So when we say the mean or the expected value, we mean we're referring to the same thing then.

126
00:16:47,060 --> 00:16:53,410
So that's one characteristic of the sampling distribution. Then there's also the variance of the estimates, right?

127
00:16:53,420 --> 00:16:56,770
This is the spread in the sampling distribution.

128
00:16:57,190 --> 00:17:00,700
So this is the sampling variance of the estimate.

129
00:17:01,150 --> 00:17:05,250
And then we can take the square root of that to get the standard error.

130
00:17:05,260 --> 00:17:13,330
So if you are familiar with I'm sure you have encountered standard errors in papers before or other courses,

131
00:17:13,660 --> 00:17:17,530
and that gives you an idea of the uncertainty in the estimate.

132
00:17:17,800 --> 00:17:22,330
And it's a function of essentially two things we'll see in a in a moment,

133
00:17:22,630 --> 00:17:30,220
but it's a function of the sample size and the variance in the population of the quantity that you're that you're measuring.

134
00:17:31,310 --> 00:17:36,640
Um, so so just a little detour here, too, to talk about random variables.

135
00:17:36,650 --> 00:17:44,990
So the idea of a random variable is actually quite nuanced and very tricky to define mathematically.

136
00:17:45,710 --> 00:17:51,110
But for our purposes, we're just going to we're just going to kind of hand wave that away and we're

137
00:17:51,110 --> 00:17:54,650
going to say it's a variable whose value is determined by a random experiment,

138
00:17:54,710 --> 00:18:04,100
right? So again, in the example of the smoking and birth weight, where the random experiment would be,

139
00:18:04,430 --> 00:18:09,050
we can say it's drawing the sample of women with their first pregnancy.

140
00:18:09,080 --> 00:18:16,120
Right? So the particular women that we're sampling are going to be different from sample to sample, right?

141
00:18:16,130 --> 00:18:19,130
And that's driving the sampling distribution.

142
00:18:19,130 --> 00:18:25,400
That's driving this idea of the estimates changing between samples.

143
00:18:25,790 --> 00:18:31,880
Right. So the randomness there is who were drawing from the population, right?

144
00:18:33,800 --> 00:18:45,530
So again, the key one of the key things that I want to impart here is that the sample mean sample proportion regression slopes.

145
00:18:45,530 --> 00:18:48,620
When we get to those, they're all random variables.

146
00:18:48,620 --> 00:18:55,249
So this is how we're able to make inference on these quantities because we're estimating

147
00:18:55,250 --> 00:19:03,230
population parameters using finite data and these random variables that the estimates represent,

148
00:19:03,500 --> 00:19:12,890
we can characterize their sampling distributions and that allows us to to to estimate things like the standard error of the estimates.

149
00:19:13,310 --> 00:19:20,120
And this is again, it's it I hope it's not too confusing to say an estimate of standard error of the estimate.

150
00:19:20,120 --> 00:19:25,300
Right. But that I mean, it is a little again, we're getting a little meta there, but it's deep.

151
00:19:25,760 --> 00:19:27,739
I just want to be clear about the terms here.

152
00:19:27,740 --> 00:19:35,600
We're estimating population parameters and then we're trying to understand what are the characteristics of those estimates.

153
00:19:35,600 --> 00:19:43,879
So when we say we're estimating the standard error of the estimate, that's because we don't know the quantities that go into these calculations.

154
00:19:43,880 --> 00:19:49,400
We have to estimate all of them because it all depends on parameters that are governing

155
00:19:49,400 --> 00:19:53,180
the random process that we're observing and that we're trying to understand.

156
00:19:54,710 --> 00:19:59,090
So this last point here is is kind of a key feature of random variables.

157
00:19:59,600 --> 00:20:09,530
It's not predictable. Right? Before you do the experiment, before you draw your sample, you can't predict what that value is going to be.

158
00:20:09,920 --> 00:20:15,080
So that's what makes it random. The opposite of random would be deterministic.

159
00:20:15,470 --> 00:20:19,070
So if something is deterministic, then we know what it's going to be.

160
00:20:19,070 --> 00:20:24,530
We can calculate that by a random variable you can't know in advance.

161
00:20:26,340 --> 00:20:32,249
So the central limit theorem you may have encountered this previously in 521,

162
00:20:32,250 --> 00:20:40,979
I'm not sure, but if not, it's a great result to know it is an asymptotic result.

163
00:20:40,980 --> 00:20:43,350
So asymptotic means large sample.

164
00:20:43,710 --> 00:20:55,620
So as we increase our sample size, essentially what the central limit theorem says is that a great deal of our estimates become normally distributed.

165
00:20:56,250 --> 00:21:01,140
So when I say normally distributed here I am referring to the sampling distribution.

166
00:21:01,440 --> 00:21:07,530
So as the sample size, which we're generally going to call in as in increases,

167
00:21:08,190 --> 00:21:15,870
the sampling distribution of many, many of our statistics or estimates becomes more normal.

168
00:21:16,170 --> 00:21:19,320
So for example, this includes the sample mean.

169
00:21:19,800 --> 00:21:23,100
So as the sample size gets larger,

170
00:21:23,100 --> 00:21:31,200
the sampling distribution of the sample mean becomes closer and closer to a normal distribution sample proportions as well.

171
00:21:31,230 --> 00:21:37,680
Right. Because sample proportions, our sample means there's no difference qualitatively.

172
00:21:37,680 --> 00:21:45,720
It's just when the random variable is a zero and a one. So in the random variables binary, the sample mean is a sample proportion.

173
00:21:46,140 --> 00:21:49,230
Okay. And then the slopes here beta hat.

174
00:21:49,230 --> 00:21:50,910
So that's when we get to a regression.

175
00:21:51,510 --> 00:22:00,030
That's, that's a regression parameter and regression is essentially where we have a mean that can depend on other variables.

176
00:22:02,100 --> 00:22:09,900
So just to reiterate here, the Y bar is the estimate here and new y is the parameter, right?

177
00:22:10,170 --> 00:22:16,080
So the new Y is fixed. The Y bar is random and is estimating new y.

178
00:22:16,470 --> 00:22:28,290
So when we say that the expected value of y bar is new, for example, then that is a trait of the estimates that is desirable.

179
00:22:28,290 --> 00:22:30,270
That would be an unbiased estimate.

180
00:22:30,930 --> 00:22:39,090
So if we have the expected value of the estimate are equal to the parameter that we're trying to estimate, that's an unbiased estimate.

181
00:22:39,160 --> 00:22:44,430
And that's one of the things that we look for when we when we construct these estimate.

182
00:22:46,350 --> 00:22:49,080
So the variance of y bar now, right.

183
00:22:49,380 --> 00:22:58,530
So we talked about the sampling distributions characteristics being the center, which would be its mean new Y and then its spread.

184
00:22:58,920 --> 00:23:04,079
So the central limit theorem tells us that the spread of the sampling distribution,

185
00:23:04,080 --> 00:23:10,620
the variance of the sampling distribution of the sample mean is sigma squared over n.

186
00:23:11,310 --> 00:23:15,270
Okay. So I'm sure you've seen this this quantity before.

187
00:23:15,600 --> 00:23:19,140
So a couple of things to note about this, right?

188
00:23:19,500 --> 00:23:23,240
So I mentioned that this the sampling distribution, right.

189
00:23:23,250 --> 00:23:28,350
We're talking about the sampling distribution of the sample mean depends on these two quantities here.

190
00:23:28,900 --> 00:23:33,480
Right. So Sigma Square is the variance of the underlying random variable.

191
00:23:33,960 --> 00:23:40,500
So the more variable that that underlying random variable is, the more we think noise.

192
00:23:40,800 --> 00:23:49,530
Right. So sigma squared is a measure of noise. So the noisier y is the larger the variance of y bar is going to be.

193
00:23:50,400 --> 00:23:54,000
However, what we have in the denominator is n right.

194
00:23:54,090 --> 00:24:00,630
That's our sample size. That's the number of observations that go into the estimate of the sample mean.

195
00:24:01,200 --> 00:24:07,170
Right. So the larger this is, the smaller the variance of y bar is going to be.

196
00:24:07,890 --> 00:24:15,510
And so this is the idea of as we increase our sample size, what's going to happen to this sampling variance?

197
00:24:15,540 --> 00:24:18,960
Right. It's going to get smaller and smaller and increases.

198
00:24:19,440 --> 00:24:23,730
And as the sampling variance gets smaller, we'll see an example of this.

199
00:24:24,180 --> 00:24:28,740
It's going to shrink the sampling distribution towards the MI.

200
00:24:29,490 --> 00:24:33,120
So it's going to concentrate that distribution towards the mean.

201
00:24:33,270 --> 00:24:34,550
And this is desirable.

202
00:24:34,560 --> 00:24:43,140
We want this to happen because we want to know that as we increase our sample size, we're getting a better estimate of of the mean.

203
00:24:44,430 --> 00:24:55,080
So so the CLTV, what's really what's really wonderful about the Clty is that we don't have to worry about the distributions of the Y eyes here.

204
00:24:55,560 --> 00:25:01,830
The Y eyes can have any distribution. Right? So we talked about the sample proportion, the y eyes and zeros and ones.

205
00:25:01,830 --> 00:25:09,300
That's not normal, right? Normal random variables are continuous, but zeros and ones are discrete.

206
00:25:09,570 --> 00:25:11,280
So it's very, very not continuous.

207
00:25:11,580 --> 00:25:21,600
But it doesn't matter if n is sufficiently large, then the central limit theorem applies and we can use this this wonderful result here,

208
00:25:21,600 --> 00:25:28,680
which is essentially boiling down the entire sampling distribution into just two, two numbers.

209
00:25:28,980 --> 00:25:36,120
There's a center and there's a spread, because if you'll remember, the normal distribution is characterized by two parameters, right?

210
00:25:36,120 --> 00:25:43,500
The mean and the variance. So regardless of what the distribution of the Y, I could have some crazy for,

211
00:25:44,010 --> 00:25:47,670
it could be some bizarre thing that depends on 20 parameters or something.

212
00:25:47,910 --> 00:25:55,830
But if you have enough of a sample, all that the sample means distribution is going to depend on is the true mean,

213
00:25:56,400 --> 00:26:05,130
the variance and then the sample size. So, so this is a really, really nice result that, that gets used all the time in statistics.

214
00:26:06,600 --> 00:26:18,090
And we have kind of the special case of if the Y eyes are normal, then this this normal distribution for the sample mean is exact.

215
00:26:18,450 --> 00:26:25,620
So when I say exact, the the opposite of that is kind of the asymptotic notion I was talking about.

216
00:26:25,620 --> 00:26:30,060
Right. So asymptotic means as the sample size gets large.

217
00:26:30,450 --> 00:26:36,840
So if if the y eyes are in fact normal, we don't have to worry about the sample size getting large.

218
00:26:37,170 --> 00:26:41,970
This will see the seal t will kind of apply regardless.

219
00:26:42,360 --> 00:26:46,560
So the sample size only needs to get large when the y eyes are not normal.

220
00:26:47,430 --> 00:26:51,800
Does that make sense? Any questions so far?

221
00:26:53,210 --> 00:26:58,150
Yes. How do you determine? That's a great question.

222
00:26:58,150 --> 00:27:03,610
In practice, you mean? So okay. So the question is, how do you determine how normal a variable is?

223
00:27:03,970 --> 00:27:06,850
So there are statistical tests for that.

224
00:27:08,170 --> 00:27:17,580
But typically those are applied so much because what can happen is say you're trying to do a T test or something, right?

225
00:27:17,590 --> 00:27:20,470
And you want to make sure your variables are normal before you do that.

226
00:27:20,950 --> 00:27:30,220
If you apply a test for normality and then you either accept or reject that hypothesis, you do two different tests or you can do.

227
00:27:30,680 --> 00:27:35,560
You guys talked about Wilcoxon or last semester non parametric tests.

228
00:27:36,280 --> 00:27:42,340
Does that ring any bells? Anyway, there's you could do a non parametric test if it's not normal or you could do a T test if it's normal.

229
00:27:42,850 --> 00:27:45,850
However, what you've introduced there is another step.

230
00:27:45,850 --> 00:27:56,440
So you've created an additional step in your testing procedure which can interfere with the characteristics of the test that you actually want to do.

231
00:27:56,860 --> 00:28:04,870
So by by having this other test, you can change things like the 5% type one error rate.

232
00:28:04,870 --> 00:28:10,420
We normally want to control that at 5%. If you introduce this other test, you can mess that up.

233
00:28:10,780 --> 00:28:19,210
So so that's a long winded way to say that the tests for normality are they have their place, but I think it's very limited.

234
00:28:19,930 --> 00:28:24,130
Typically what we would do is look at plots, we'd look at histograms, things like that.

235
00:28:25,600 --> 00:28:32,380
Because the other thing with those tests for normality is that they often require very large sample sizes to be accurate.

236
00:28:33,100 --> 00:28:36,380
So so they're not necessarily practical because the problem is, you know,

237
00:28:36,430 --> 00:28:42,250
I think what you're identifying here is that the small sample cases is where we need to be most concerned about normality.

238
00:28:42,910 --> 00:28:48,670
And that's exactly the case where it's very hard to determine because you can picture we'll look at some histograms at some point,

239
00:28:49,390 --> 00:28:55,050
but if you can picture a histogram of of ten data points, it's just going to be a bunch of bars, right?

240
00:28:55,060 --> 00:28:58,210
It's not going to be very easy to to pick out that bell curve.

241
00:28:58,840 --> 00:29:02,440
So so it can be difficult to assess normality and small samples,

242
00:29:03,550 --> 00:29:09,520
but graphical methods are generally a good tool for at least for moderate to large samples.

243
00:29:10,410 --> 00:29:16,440
Great question. Any anybody else? Okay.

244
00:29:16,710 --> 00:29:24,420
So standard error. Right. So this is the way we characterize the sampling distribution or the sampling the sampling distribution spread.

245
00:29:25,720 --> 00:29:28,750
So we abbreviate that as SCC. So standard error.

246
00:29:29,020 --> 00:29:33,610
So it's the standard deviation of y bar and this is another source of a lot of

247
00:29:33,610 --> 00:29:38,590
confusion is the distinction between the standard error and the standard deviation.

248
00:29:39,010 --> 00:29:47,170
So what you'll see, though, is that the standard error is a function of the standard deviation of Y, right?

249
00:29:47,590 --> 00:29:57,160
So the standard deviation of y is sigma y and then the standard error is just sigma hat y divided by the square root of n.

250
00:29:57,580 --> 00:30:05,020
So it's, it's an estimate because it's sigma hat and then it's divided by the square root of n, which is the sample size.

251
00:30:05,680 --> 00:30:09,340
And this is going to have the same units as the original data as well.

252
00:30:09,550 --> 00:30:14,290
So we talked about the standard deviation. We'll have the same units as Y.

253
00:30:14,770 --> 00:30:18,999
Now the standard error will also have the same units because it's just the

254
00:30:19,000 --> 00:30:23,709
standard deviation which has the same units as Y divided by the square root of N,

255
00:30:23,710 --> 00:30:27,610
which doesn't really have any units. So it's going to have the same units as Y.

256
00:30:29,760 --> 00:30:37,530
So this is our chance to look at some histograms and some sampling distributions over here.

257
00:30:37,860 --> 00:30:44,059
So. So. I want to make sure we go through this in detail here.

258
00:30:44,060 --> 00:30:52,400
But on this this plot over here on the on the left is actual samples.

259
00:30:52,700 --> 00:31:03,950
So if we had a sample size of equals 100, this is a histogram showing the distribution of those 100 values across Y.

260
00:31:04,070 --> 00:31:08,540
Right. So these were drawn from a normal zero one distribution.

261
00:31:08,540 --> 00:31:12,380
So a normal distribution with mean zero variance one.

262
00:31:13,700 --> 00:31:21,470
So what you'll see is that if we increase the sample size, we start to see this kind of smooth out.

263
00:31:21,680 --> 00:31:24,110
Right. So this is what I was talking about a moment ago.

264
00:31:24,350 --> 00:31:30,860
If you have fewer data points, it's it can be harder to visually identify what this distribution is.

265
00:31:30,950 --> 00:31:36,049
Right. Whereas you can start to see that bell curve shape when you have a thousand

266
00:31:36,050 --> 00:31:40,459
data points and 100 data points is not I wouldn't call that a small sample.

267
00:31:40,460 --> 00:31:47,720
That's probably a moderate sample, but it's still you can see here, it can be kind of hard to pick out what the shape is.

268
00:31:48,170 --> 00:31:52,639
So you're familiar with histograms, right?

269
00:31:52,640 --> 00:32:00,800
From five. Okay. So the thing with histograms is that there's there's not a single histogram associated with the data set.

270
00:32:00,980 --> 00:32:04,160
You have to pick a width for these bars.

271
00:32:04,610 --> 00:32:08,550
Right. So you can change the width of the bars and get a different histogram.

272
00:32:09,440 --> 00:32:13,190
So that's another thing that can kind of change how these look. That's a little bit of a side point.

273
00:32:14,300 --> 00:32:22,790
What I want you to pick up on here is that these are the same distributions that you're that you're that you're estimating here with this histogram,

274
00:32:22,790 --> 00:32:30,500
because the histogram is also an estimate. It's an estimate of the density function of this random variable.

275
00:32:30,740 --> 00:32:35,540
So it's a much different estimate than when we talk about the means and the variances, because those are simple.

276
00:32:35,750 --> 00:32:41,750
Those are numbers. We call them scalar. But these are essentially functional estimates.

277
00:32:42,260 --> 00:32:50,600
So but they're estimating the same thing, which is the distribution of the wise and the standard deviation here is not changing.

278
00:32:51,170 --> 00:32:56,900
Right. So if we look at the spread of the left panels versus the right panels, it's the same.

279
00:32:57,470 --> 00:33:06,770
It's going to be the same spread whether we have 100 or a thousand individuals in our sample when we go and look at the sample means,

280
00:33:06,770 --> 00:33:12,799
however, and I know it's hard to see this bottom on here, but this is the sampling distribution.

281
00:33:12,800 --> 00:33:21,140
So this is when we've taken a thousand samples of a hundred or a thousand individuals each.

282
00:33:21,290 --> 00:33:29,660
We've repeated that sampling process a thousand times and then we've plotted here the histogram of the sample means, right?

283
00:33:30,020 --> 00:33:32,239
So look, look down here to the x axis.

284
00:33:32,240 --> 00:33:43,190
If you can see it's running from minus point to 2.2, whereas the x axis down here is running from like minus four to almost a four.

285
00:33:43,550 --> 00:33:53,450
So we're shrinking the x axis because we're shrinking and and compressing the distribution of the sample mean.

286
00:33:53,840 --> 00:34:00,410
So we're pushing the sample mean toward its true value by increasing the sample size.

287
00:34:00,770 --> 00:34:07,370
So you'll see that as you go from the top panel to the bottom panel, it becomes a more peaked distribution.

288
00:34:07,520 --> 00:34:13,910
So it's standard deviation is decreasing when we increase the sample size, right.

289
00:34:14,180 --> 00:34:18,800
So the standard deviation is fixed. That's a parameter, right?

290
00:34:19,040 --> 00:34:26,930
But the standard error that we're estimating from the sample is going to be a function of the sample size.

291
00:34:27,200 --> 00:34:32,120
So the standard error is the standard deviation of these plots over here.

292
00:34:32,360 --> 00:34:37,999
And you can see that as this is the sample size gets larger, that's decreasing, right?

293
00:34:38,000 --> 00:34:44,930
So when we have a more kind of peaked distribution that that occupies less space along the X axis,

294
00:34:45,380 --> 00:34:50,060
that's associated with a smaller standard deviation, a smaller variance.

295
00:34:50,570 --> 00:34:53,840
So as even any questions about standard deviation of the standard errors.

296
00:34:54,930 --> 00:35:01,919
Because I know this is a tricky kind of concept, and you'll see this in papers.

297
00:35:01,920 --> 00:35:04,470
You'll see people trip up on this in papers.

298
00:35:05,250 --> 00:35:10,830
So one of the things that that kind of drives me nuts is the format and papers of meeting plus or minus something,

299
00:35:11,430 --> 00:35:15,390
because oftentimes it's not defined if they mean the standard deviation of the standard error.

300
00:35:16,080 --> 00:35:24,330
Sometimes you can tell from the context, but not always. So I'm in favor of more of a format of mean and then parentheses, standard deviation,

301
00:35:24,330 --> 00:35:28,830
because then you can say in your table, legend, mean parentheses.

302
00:35:29,070 --> 00:35:33,150
SD Um, but yeah, it's, it's, it's a confusing point.

303
00:35:33,160 --> 00:35:37,590
So, so again, the standard deviation is the trait of the distribution that you're sampling from.

304
00:35:37,860 --> 00:35:45,540
The standard error is a characteristic of the estimate that you're, that you're calculating from that sample.

305
00:35:47,170 --> 00:35:51,399
So I'm talking about the actual distributions, right?

306
00:35:51,400 --> 00:36:02,110
We've talked about the central limit theorem which characterizes the normality of the sampling distribution as in goes to infinity, so asymptotically.

307
00:36:02,530 --> 00:36:10,300
Right. So there's other sampling distributions that we will encounter, which are things like the student's T distribution.

308
00:36:10,310 --> 00:36:15,880
So we've you've heard about T tests. One sample t test paired t test.

309
00:36:15,910 --> 00:36:20,890
Two Sample T test. So there's the T distribution, which we'll talk about some more.

310
00:36:21,290 --> 00:36:23,710
Then there's the Chi square in the F distribution.

311
00:36:24,070 --> 00:36:32,020
So so kind of the there's there's some correspondences here, but so so the student's t gets to be normal.

312
00:36:32,830 --> 00:36:37,390
So it approaches this, this distribution here, as in goes to infinity.

313
00:36:37,810 --> 00:36:44,290
Right. So we've you've probably encountered the idea of the T test, kind of approaching the Z test as and gets large.

314
00:36:44,800 --> 00:36:52,030
So that's happening there. There's also a characteristic of the F in the chi square that has and goes to infinity

315
00:36:52,450 --> 00:36:56,360
a certain version rescale scaled of the f distribution approaches a chi square.

316
00:36:56,380 --> 00:37:07,840
So there's there's kind of this parallel notion here where the F and the T are sort of finite sample versions of the Chi square in the normal.

317
00:37:08,590 --> 00:37:13,510
And then this is just an example of a couple of, of different sampling distributions that are normal.

318
00:37:13,510 --> 00:37:20,260
And then t so the degrees of freedom here is a characteristic of the T distribution, right?

319
00:37:20,260 --> 00:37:25,630
So the T distribution is characterized by its degrees of freedom and as the degrees of freedom,

320
00:37:25,840 --> 00:37:32,140
which is essentially going to be related to the sample size that we were going to use it as it increases.

321
00:37:32,470 --> 00:37:35,740
We're approaching the normal distribution here and it's hard to see,

322
00:37:36,760 --> 00:37:44,350
but there is a thick black line underlying these that is the infinite degrees of freedom case, which is the normal distribution.

323
00:37:44,740 --> 00:37:49,370
So the furthest away from that is the five degrees of freedom on this plot.

324
00:37:50,650 --> 00:37:54,160
And you might say, well, these look kind of similar, right?

325
00:37:54,160 --> 00:37:57,570
I mean, you can't really distinguish after you get to maybe 20.

326
00:37:58,240 --> 00:38:00,460
Right. So so why do we even need to worry about this?

327
00:38:00,700 --> 00:38:08,560
And the reason that we need to worry about this is because when we're doing significance testing, we're thinking about the tails here.

328
00:38:08,770 --> 00:38:13,990
We're concerned about the area under the curve in the tails of this distribution.

329
00:38:14,380 --> 00:38:15,940
And that's what's plotted here.

330
00:38:16,300 --> 00:38:25,720
So you can see that there's quite a difference between the the curves here for different values of the degrees of freedom.

331
00:38:25,960 --> 00:38:32,500
That's going to be much more relevant in actual testing situations.

332
00:38:33,160 --> 00:38:39,129
And we'll talk more about that as well. So the normal distribution, I'm sure you're all familiar.

333
00:38:39,130 --> 00:38:45,370
I don't know if we need to to belabor this, but it is probably the most important distribution statistics.

334
00:38:45,370 --> 00:38:51,280
I don't think that's hyperbole because it's just it comes up so frequently and it's so useful.

335
00:38:52,610 --> 00:39:01,179
As I said, essentially, you can characterize almost a large majority of the estimates that you can construct well,

336
00:39:01,180 --> 00:39:07,210
ultimately can be shown to have a normal distribution for their sampling distribution, at least asymptotically.

337
00:39:07,720 --> 00:39:09,540
It is continuous. Right.

338
00:39:09,550 --> 00:39:19,270
So we talked about the idea of if the wise are zeros and ones, we can still have a normal a normal distribution as it goes to infinity.

339
00:39:19,900 --> 00:39:23,410
But the normal distribution itself is continuous. Right.

340
00:39:23,530 --> 00:39:30,880
And the reason that this happens, for the reason that we can use the c l t for the Y, even when it's zeros and ones.

341
00:39:31,270 --> 00:39:36,100
Is that the mean of the y's is going to be continuous, right?

342
00:39:36,310 --> 00:39:41,710
So the mean of a bunch of zero and one variables is going to be between zero and one, right?

343
00:39:41,730 --> 00:39:49,210
It's going to be estimating a proportion. So that becomes sort of a continuous quantity as in increases.

344
00:39:50,170 --> 00:39:57,130
So that's that's the way we can kind of approach a continuous distribution, even when we start from a discrete distribution.

345
00:39:58,610 --> 00:40:02,930
So it is bell shaped we can see here and it's symmetric.

346
00:40:03,230 --> 00:40:06,680
We talked about symmetry in relation to means and medians.

347
00:40:07,010 --> 00:40:12,260
The normal distribution is symmetric, which means it has the same value for it.

348
00:40:12,270 --> 00:40:21,890
So this is the density here. Symmetric means it has the same value for the density on this side of of zero as it does on this side of zero.

349
00:40:23,630 --> 00:40:30,709
And the the sigma is here are just characterizing the the spread, right?

350
00:40:30,710 --> 00:40:39,890
So within a single standard deviation on either side of the mean U, we have about 68% of the density, right?

351
00:40:39,890 --> 00:40:44,420
So 68% of the probability is within MU plus or minus sigma.

352
00:40:45,020 --> 00:40:55,400
By the time we get to two standard deviations away from the mean, we're already covering 95% of the probability and then even out to two.

353
00:40:56,150 --> 00:40:59,570
By the time we get to three, it's over 99%.

354
00:40:59,960 --> 00:41:07,730
So the the tails of the normal distribution are relatively thin.

355
00:41:07,730 --> 00:41:13,550
So we don't we don't have a whole lot of probability mass pushing out into the tails.

356
00:41:13,850 --> 00:41:19,580
So that means that it's very unlikely to observe extreme values.

357
00:41:20,000 --> 00:41:23,600
Right. So when we get to talking about things like outliers.

358
00:41:23,990 --> 00:41:31,880
Right. So we can characterize outliers by being unlikely to occur under the distribution that we're modeling.

359
00:41:32,330 --> 00:41:34,280
So when we have a normal distribution.

360
00:41:34,580 --> 00:41:42,050
Outliers may be things that are further than we could say to standard deviations from the mean, because that's already pretty unlikely.

361
00:41:42,200 --> 00:41:47,780
Right. And by the time we get to more than three standard deviations from the mean, it becomes even more unlikely.

362
00:41:48,080 --> 00:41:53,270
However, in a large enough sample, you'll still expect to see some of these values.

363
00:41:53,750 --> 00:41:56,150
What we're saying with the normal distribution, however,

364
00:41:57,050 --> 00:42:03,950
is just that such extreme values are very unlikely to be observed relative to the values in the center.

365
00:42:05,620 --> 00:42:10,450
Any question about the normal distribution. Okay.

366
00:42:13,890 --> 00:42:19,080
Standardization. So you may have encountered this as well,

367
00:42:19,410 --> 00:42:28,350
but the idea of the normal distribution is that because it's symmetric and characterized by its mean and its variance,

368
00:42:29,370 --> 00:42:39,300
we can manipulate any variable, any random variable that we sample in order to calculate normal probabilities.

369
00:42:39,720 --> 00:42:47,160
So we have the these tables and I can just kind of pause and go look at these tables in a minute.

370
00:42:47,160 --> 00:42:56,190
But you may have seen the my first is on the canvas, but they they tabulate the area under the curve of the normal density function,

371
00:42:56,850 --> 00:43:00,380
which is also called the cumulative distribution function.

372
00:43:01,050 --> 00:43:05,040
They tabulate these values for the normal, the standard normal.

373
00:43:05,040 --> 00:43:10,140
So the standard normal is the normal with mean zero and variance one.

374
00:43:11,010 --> 00:43:15,180
And this is useful because we can calculate things like for example,

375
00:43:15,420 --> 00:43:25,170
if we have a Y is distributed normal with mean one and standard deviation two, we want to know the probability that Y is greater than five.

376
00:43:25,740 --> 00:43:35,010
Right. So this is this is something that we can calculate using these tables, even if we don't have access to R, for example,

377
00:43:35,670 --> 00:43:42,030
because R makes it very easy to calculate these these quantities using things like the P know our function.

378
00:43:43,080 --> 00:43:45,540
But in this example, let's, let's talk through this, right?

379
00:43:45,540 --> 00:43:57,840
So if y is normal with mean one and standard deviation of two, what we can do is subtract the mean from y and then divide by the standard deviation.

380
00:43:58,530 --> 00:44:06,660
Right? And we do the same to this value five that we're trying to calculate what, what the probability of y being greater than that is.

381
00:44:07,530 --> 00:44:11,220
So we end up with the probability that Z is greater than two.

382
00:44:11,460 --> 00:44:15,600
So Z is the the standard normal.

383
00:44:15,600 --> 00:44:20,070
We use the letter Z to represent a variable with the standard normal distribution.

384
00:44:21,390 --> 00:44:27,420
So when we subtract off the mean and divide by the standard deviation, what have we done here?

385
00:44:28,110 --> 00:44:36,090
Right. So the mean of this new variable is going to be the mean of y minus one.

386
00:44:36,630 --> 00:44:40,260
So the mean of y is going to be one.

387
00:44:40,260 --> 00:44:42,690
So subtracting one from that is zero, right?

388
00:44:43,080 --> 00:44:52,950
And then the variance of this variable is going to be essentially you can ignore this constant, but if you take the variance of this expression,

389
00:44:53,310 --> 00:45:00,900
it's going to end up being one over four times the variance of this variable, which is going to be two squared or four.

390
00:45:01,230 --> 00:45:06,540
So what you're doing is you're converting this normal random variable with mean one

391
00:45:06,540 --> 00:45:13,500
variance for standard deviation two to a mean zero variance one normal random variable,

392
00:45:13,980 --> 00:45:24,420
right? Because this is another nice feature of the normal distribution is that is closed under normal under linear transformations.

393
00:45:24,780 --> 00:45:32,730
So a linear transformation is when you're just multiplying and adding and subtracting and dividing like because that's the same as multiplying.

394
00:45:33,660 --> 00:45:38,910
So that's a linear transformation. And when we, we say closed under linear transformations,

395
00:45:38,910 --> 00:45:44,880
we just mean that when you do a linear transformation, the outcome is another normal distribution.

396
00:45:45,180 --> 00:45:53,249
So any time you take a normal distribution, apply a linear transformation to it, you end up with another normal distribution.

397
00:45:53,250 --> 00:46:01,409
So this is what allows us to do this kind of standardization and end up with a a standard normal

398
00:46:01,410 --> 00:46:09,809
random variable where we can calculate from these tables what the value of this probability is.

399
00:46:09,810 --> 00:46:14,850
And I'm going to pause here and we're going to take a look at these distribution tables.

400
00:46:15,720 --> 00:46:24,150
So I don't know if anybody's looked at these yet, but these are again, it's it's kind of old school now because we have so much computing power.

401
00:46:24,990 --> 00:46:34,860
But the way to read these is the table entry is the area under the standard normal curve to the left of the indicated Z value,

402
00:46:35,250 --> 00:46:38,340
thus giving probability of Z less than Z.

403
00:46:38,610 --> 00:46:44,730
So what were we looking at? Right. We were looking at the probability that Z is greater than two.

404
00:46:45,240 --> 00:46:48,270
So let's see if we can find out what that would be on here.

405
00:46:50,580 --> 00:47:02,040
So what we want to do is we want to find the value of Z equal to two, so we see Z equal to two right here, and then it's going to be 2.0.

406
00:47:02,610 --> 00:47:11,730
So the point zero is up here. So basically the top line here is going to be the hundredths place.

407
00:47:11,970 --> 00:47:18,530
So this is two. .00, this would be 2.012.02, etc.

408
00:47:19,070 --> 00:47:23,719
So we find a 2.00.977.

409
00:47:23,720 --> 00:47:28,340
Right? But this is not the answer because we don't want the probability that Y is less than that.

410
00:47:28,340 --> 00:47:32,960
We want the probability that Y is greater than that. So we just take one minus this value.

411
00:47:33,610 --> 00:47:37,219
Then that's our probability that Y is greater than two.

412
00:47:37,220 --> 00:47:41,480
Or then sorry, the Z is greater than two. That is everybody followed up.

413
00:47:43,090 --> 00:47:48,850
Yeah. And again, this is something that you can do even if you don't have access to a computer, if you have access to these tables.

414
00:47:49,180 --> 00:47:53,830
But on a computer, on R, you just do the p norm function to to figure these things out.

415
00:47:56,210 --> 00:48:01,250
Okay. So I was just talking about this.

416
00:48:01,250 --> 00:48:09,260
But what if what happens if we have a normal mu and sigma random variable and then we take this this function,

417
00:48:09,260 --> 00:48:18,410
this transformation eight times X plus B for some constants A and B, what is the distribution of Y as we want to shout it out.

418
00:48:20,320 --> 00:48:27,820
You can just say a word. You don't have to say parameters. You can say a word. Exactly.

419
00:48:27,820 --> 00:48:33,760
Yes. Thank you. It's normal. So we we said that any time you take a linear transformation,

420
00:48:33,760 --> 00:48:38,560
which is what this is of a normal random variable, you end up with another normal random variable.

421
00:48:39,460 --> 00:48:46,660
So it's this variable Y here that is equal to a x plus B is still going to be normal.

422
00:48:46,780 --> 00:48:49,840
What's going to happen is it's mean and variance will be different.

423
00:48:50,200 --> 00:48:56,740
The mean will be different because we've added B and the variance will be different because we've multiplied by A.

424
00:48:57,340 --> 00:49:01,600
So if we run through these calculations here, E is the expectation operator.

425
00:49:01,900 --> 00:49:06,250
So E of X is mu. The variance of x is sigma squared x.

426
00:49:07,120 --> 00:49:15,040
Then we take the expectation of why we just have this expectation operator applied to a times x plus B,

427
00:49:15,430 --> 00:49:23,230
so it kind of pushes through in is equal to eight times MU because that's the expectation of X plus B, right?

428
00:49:23,470 --> 00:49:28,450
So the mean is eight times me plus B, then the variance of Y.

429
00:49:28,750 --> 00:49:36,550
Same. Same idea. Here we take the variance of the expression that represents y, a, x plus b, and then that's equal to.

430
00:49:36,760 --> 00:49:45,640
We can break these apart because there's essentially because B as a constant, we can kind of ignore it because the variance of a constant is zero.

431
00:49:46,540 --> 00:49:49,149
And then we just end up with the variance of eight times X.

432
00:49:49,150 --> 00:49:55,270
So what that means, we will remember from, from probability that when we take the variance of a constant times,

433
00:49:55,280 --> 00:50:01,179
random variable is equal to that constant squared times the variance of the random variable.

434
00:50:01,180 --> 00:50:08,170
So a squared times sigma squared x is the variance of this quantity y here.

435
00:50:08,590 --> 00:50:20,380
So y then has the normal distribution with mean a, times mu plus B and standard deviation a times sigma x.

436
00:50:21,130 --> 00:50:23,890
So this is the result I was talking about.

437
00:50:23,890 --> 00:50:33,820
So any linear combination, linear transformation, those are the same things of a normally distributed random variable is itself going to be normal.

438
00:50:34,600 --> 00:50:41,260
So this is why we were allowed to do that standardization to to use our our tables of the normal distribution.

439
00:50:44,060 --> 00:50:46,910
So the tea distribution. Right.

440
00:50:46,920 --> 00:50:54,620
So this is this is going to be a useful distribution when we're doing tests for our regression coefficients, for example.

441
00:50:55,130 --> 00:50:59,870
Right. So and we'll, we'll see why.

442
00:50:59,870 --> 00:51:02,630
But essentially they have this, this form here, right?

443
00:51:02,750 --> 00:51:10,309
So the T distribution is what's used to characterize essentially a ratio of a random of two random variables,

444
00:51:10,310 --> 00:51:15,560
one being a sample mean and the other being essentially a sample standard error.

445
00:51:16,040 --> 00:51:23,480
So the Z here has X bar over sigma, right?

446
00:51:23,780 --> 00:51:26,839
So this is a a parameter.

447
00:51:26,840 --> 00:51:30,590
So Sigma is a parameter. So this is a known quantity.

448
00:51:30,980 --> 00:51:37,459
What distinguishes the Z and the T is that sigma hat is in the denominator for T, right.

449
00:51:37,460 --> 00:51:39,800
And this is might not seem like a big deal,

450
00:51:39,980 --> 00:51:47,480
but when you replace a known quantity by an unknown quantity that you have to estimate that changes the sampling distribution.

451
00:51:47,990 --> 00:51:58,790
So instead of this quantity, this ratio being normal zero one, it then becomes T distributed within minus one degrees of freedom, right?

452
00:51:58,790 --> 00:52:05,810
So we talked about when we estimate the standard deviation or the variance, we divide by n minus one, right?

453
00:52:06,050 --> 00:52:09,650
So we've lost one degree of freedom because we had to estimate the mean.

454
00:52:10,430 --> 00:52:15,470
So we've used the data once essentially already to estimate the mean.

455
00:52:15,680 --> 00:52:22,730
And this idea of degrees of freedom is going to become very important in linear regression, and we'll see that soon.

456
00:52:23,420 --> 00:52:30,110
But essentially the T distribution that is characterized by this degrees of freedom parameter, I know it's hard to see,

457
00:52:31,670 --> 00:52:36,290
but any question so far, especially because it's hard to see this anybody is anybody having trouble?

458
00:52:38,110 --> 00:52:42,910
Hopefully it'll, it'll appear in the notes and again, I'll get this sorted out for next time.

459
00:52:44,500 --> 00:52:53,320
Anyway, so the notation here is that T has the little T distribution with subscript degrees of freedom, right?

460
00:52:53,620 --> 00:52:57,249
So the degrees of freedom is a positive integer.

461
00:52:57,250 --> 00:53:06,040
That's going to be a function of the sample size. And then we can have tables like we just looked at the table for the Z distribution.

462
00:53:07,190 --> 00:53:11,540
We have tables for the tea distribution. Four degrees of freedom up to 30.

463
00:53:12,050 --> 00:53:22,460
Right. So and again, this is important because when we have these smaller sample sizes, there's a bigger effect on the sampling distribution.

464
00:53:24,110 --> 00:53:31,400
So the and I just want to make sure that that I note the reason that we have this kind of this is a test statistic.

465
00:53:31,760 --> 00:53:41,239
So this this quantity here is a test statistic. So if, for example, this would come up if we wanted to test statistically test with a hypothesis,

466
00:53:41,240 --> 00:53:49,970
test whether the mean of X is equal to Mew, for example, this would be the test statistic that we would use to do that.

467
00:53:50,360 --> 00:53:53,660
So that's why these these quantities are important, right?

468
00:53:54,620 --> 00:54:01,160
So they're they're test statistics and we need to know their sampling distribution to get our P values essentially.

469
00:54:03,370 --> 00:54:13,030
So you all can look at the tables on canvas as we just did, and you can kind of play around with those, those distributions.

470
00:54:14,200 --> 00:54:20,679
The the T distribution can be calculated and are using the P t function so that I'll

471
00:54:20,680 --> 00:54:24,880
give you the distribution function for the T with specified degrees of freedom.

472
00:54:25,540 --> 00:54:33,460
For a large number of degrees of freedom. We talked about asymptotically the T distribution is going to approach the normal distribution.

473
00:54:33,880 --> 00:54:43,960
So when T sorry, when the degrees of freedom gets large, you can use the normal distribution as an approximation for the T because they're so close.

474
00:54:44,830 --> 00:54:50,530
So you really only need to worry about the T distribution when you have a smaller sample size.

475
00:54:52,560 --> 00:54:59,760
And then again, we have this form of a test statistic here where it's theta hat.

476
00:54:59,760 --> 00:55:03,000
So hat is an estimate. So we're trying to estimate theta here.

477
00:55:03,360 --> 00:55:13,440
So theta hat minus is expectation divided by its standard error is going to have a T distribution with a certain number of degrees of freedom.

478
00:55:13,680 --> 00:55:22,950
So this is just a very general expression, but we'll see that a lot of the test statistics that we're interested in are going to have this form,

479
00:55:23,220 --> 00:55:29,610
and this is known as a wild statistic when you take a an estimate and divide it by a standard error.

480
00:55:29,790 --> 00:55:36,300
That's a wild statistic. And we'll talk more about that as well.

481
00:55:37,440 --> 00:55:42,839
So the if you'll recall, your to sample t test, right.

482
00:55:42,840 --> 00:55:54,000
So when you have two independent samples of data and you want to compare it, ask the question, do these do these two populations have different means?

483
00:55:54,570 --> 00:56:01,170
Then you can use A to sample T test, right? And then you can think about the critical values for that T test.

484
00:56:01,170 --> 00:56:08,910
So the critical values are the values above which we say this is a significant difference.

485
00:56:09,330 --> 00:56:14,940
Right? So what's tabulated here is four increasing degrees of freedom.

486
00:56:15,330 --> 00:56:22,050
What's the essentially what's the threshold we have to reach with our test statistic to declare a significant difference?

487
00:56:22,650 --> 00:56:30,150
So what you'll see is as the as the degrees of freedom, also sample size increases.

488
00:56:30,660 --> 00:56:35,370
Our critical values are decreasing right up to a point.

489
00:56:35,730 --> 00:56:41,760
So the point five significance level, the critical value for the this is a two tailed test.

490
00:56:42,780 --> 00:56:49,200
The critical value is 1.96 for the normal distribution, but for a smaller sample.

491
00:56:49,380 --> 00:56:55,080
Essentially, what the T distribution is saying is that you need to have even more evidence,

492
00:56:55,590 --> 00:56:59,870
right, to be able to say you have a significant difference because it's a small sample.

493
00:57:00,180 --> 00:57:04,950
We need to be more conservative about declaring a difference to be significant.

494
00:57:05,640 --> 00:57:12,420
So this is and this is kind of a nice formalization of that intuitive notion that with a smaller sample,

495
00:57:12,690 --> 00:57:17,550
you're you're a little bit less certain, right, about what you're estimating.

496
00:57:18,270 --> 00:57:22,650
So and again, we can also quantify that with things like the standard error of the mean,

497
00:57:22,980 --> 00:57:27,930
the area of the mean is going to be a lot smaller with a sample size of 100.

498
00:57:28,140 --> 00:57:37,920
And with the sample size of five. Right. So you have more certainty about the estimates that you're that you're constructed there.

499
00:57:40,350 --> 00:57:48,510
So the chi square in the f, I talked about how those are kind of analogous to the normal and the T,

500
00:57:48,750 --> 00:57:57,690
so the F is kind of a finite sample version that in large samples becomes equivalent to a chi square.

501
00:57:58,050 --> 00:58:04,740
Right. So when we have squared sample values, for example, the sample variance.

502
00:58:04,740 --> 00:58:08,549
Right, that is a sum of squares. Right.

503
00:58:08,550 --> 00:58:16,830
Because we saw the formula for the sample variance is the sum of the squared differences from the mean divided by n minus one.

504
00:58:17,180 --> 00:58:24,000
Right. So when we have statistics like that, the F and the Chi Square are typically good approximations.

505
00:58:24,450 --> 00:58:30,480
And we have some examples over here, but you can see that they are always positive.

506
00:58:31,020 --> 00:58:35,940
So these are always positive because a squared value can't be negative.

507
00:58:36,100 --> 00:58:42,390
Right. We're not dealing with imaginary numbers here. So all every time we square something, it's going to end up being positive.

508
00:58:42,600 --> 00:58:46,950
So these distributions are all on the positive, real line.

509
00:58:48,210 --> 00:58:55,170
So the notation here, so this is a chi, the Greek letter chi here squared, and then the subscript is going to be the degrees of freedom.

510
00:58:56,520 --> 00:58:58,110
So the properties here,

511
00:58:58,110 --> 00:59:06,960
the expected value or the mean of the chi square with the f degrees of freedom is equal to DF so its mean is equal to its degrees of freedom.

512
00:59:08,160 --> 00:59:17,160
So it's skewed to the right. What you'll encounter in many cases is with positive random variables, they're often skewed to the right.

513
00:59:17,400 --> 00:59:21,180
So things like income, right, that's skewed to the right typically.

514
00:59:22,090 --> 00:59:26,970
Um, so the, the sample variance, right.

515
00:59:27,630 --> 00:59:38,940
Sigma squared had y that suitably transformed has a chi square distribution when the underlying random variable is normal.

516
00:59:39,540 --> 00:59:47,189
Right. So, so this and again, this is a, this is a transformation of the sample variance.

517
00:59:47,190 --> 00:59:56,550
So it's a little bit trickier to use. But you can, you can get confidence intervals for sigma squared using this result here.

518
00:59:57,240 --> 01:00:04,560
Um, and then kind of a key relationship is that if, if Z is a standard normal random variable,

519
01:00:04,860 --> 01:00:09,480
then the z squared is a chi square random variable with one degree of freedom.

520
01:00:11,810 --> 01:00:15,500
And then you may have seen this in categorical data analysis.

521
01:00:15,500 --> 01:00:23,900
The Chi Square test is is commonly applied to test for independence between rows and columns of a contingency table.

522
01:00:25,070 --> 01:00:31,229
Any questions so far? Okay.

523
01:00:31,230 --> 01:00:38,280
So the F distribution. It's got an extra parameter, but it's going to be kind of analogous to the Chi Square.

524
01:00:38,280 --> 01:00:47,520
So the F distribution has two degrees of freedom parameters, so it's DF one and DF two DF one will typically refer to as the numerator.

525
01:00:47,700 --> 01:00:50,999
Degrees of freedom and DFT will be the denominator.

526
01:00:51,000 --> 01:00:57,810
Degrees of freedom. And this is again similar to the Chi square are going to be skewed to the right asymmetric.

527
01:00:58,200 --> 01:01:08,520
It's always positive. And these these kinds of F distributions come up in situations where we have ratios of squared qualities.

528
01:01:08,880 --> 01:01:15,750
So for example, if you've encountered the test for equality of variances in two populations,

529
01:01:16,110 --> 01:01:20,340
that's going to have that test statistic is going to have an F distribution.

530
01:01:20,820 --> 01:01:31,920
So that's the equal variances test. Well, and again, we have this this nice result here analogously to Z squared having a Chi Square distribution.

531
01:01:32,340 --> 01:01:43,950
When we have one numerator degrees of freedom, the F will have the same distribution as T squared where T has the same denominator degrees of freedom.

532
01:01:44,670 --> 01:01:50,280
Okay, well, this will make more sense when we get to the simple linear regression stuff and

533
01:01:50,280 --> 01:01:54,060
we'll see again that the F distribution comes up a lot and linear regression.

534
01:01:56,900 --> 01:02:00,770
So the two main components of statistical inference,

535
01:02:00,770 --> 01:02:07,130
we've kind of talked about these and I'm sure you've encountered these before, but our estimation,

536
01:02:07,310 --> 01:02:13,550
which involves the idea of confidence intervals to characterize uncertainty and then hypothesis testing,

537
01:02:13,790 --> 01:02:18,380
which is more of a decision making, significant or not kind of question.

538
01:02:20,050 --> 01:02:25,660
So for for this example, we're saying we want to estimate the weight of Detroit male residents.

539
01:02:25,990 --> 01:02:31,180
So we randomly sample 50 male Detroit residents and record their weights.

540
01:02:31,510 --> 01:02:37,420
And we want to find an estimate and a 95% confidence interval for the mean weight.

541
01:02:37,840 --> 01:02:48,310
So the data here is is going to be written as w sub I so i the subscript here is the index for the individual.

542
01:02:48,790 --> 01:02:55,810
So this is kind of like w one, w two, so on and so forth up to W 50 here.

543
01:02:56,020 --> 01:03:03,280
So these are the sample weights that we've obtained from our sampled male residents in our sample of size 50.

544
01:03:04,840 --> 01:03:17,620
So if we go through and we calculate these quantities, our mean, our estimated mean or mu had W or W bar is going to be 159.

545
01:03:18,290 --> 01:03:25,069
Okay. And then our standard deviation estimate is going to be 38, right?

546
01:03:25,070 --> 01:03:30,320
Because we have the same units for the standard deviation that we do for the original random variable.

547
01:03:30,950 --> 01:03:35,390
And then our our 95% confidence interval has this formula.

548
01:03:35,750 --> 01:03:41,930
So it's going to be the mean plus or minus the critical value for the T distribution

549
01:03:41,930 --> 01:03:51,360
within minus one degrees of freedom at 0.975 probability times the standard error.

550
01:03:51,380 --> 01:04:00,440
So we can talk through these things, right? So the mean a sorry, the, the confidence interval is going to be centered around the mean, right.

551
01:04:00,440 --> 01:04:09,050
And that, that, that makes sense. We generally want the confidence interval to be centered around our best guess at the true value of that parameter.

552
01:04:09,440 --> 01:04:13,700
We will encounter situations where the confidence intervals asymmetric.

553
01:04:14,270 --> 01:04:19,430
In this case it's not. In this case it's symmetric. We're adding and subtracting the same quantity.

554
01:04:19,700 --> 01:04:25,700
Right? So you're adding something and subtracting the same thing to get the endpoints of the confidence interval.

555
01:04:26,640 --> 01:04:31,920
The end minus one here is the degrees of freedom in our tea distribution.

556
01:04:32,250 --> 01:04:41,100
So the confidence interval here is coming from the sampling distribution for the the mean here essentially.

557
01:04:42,360 --> 01:04:46,680
And then the 0.975 is.

558
01:04:48,860 --> 01:04:53,570
The it's we're looking at a 95% confidence interval.

559
01:04:53,930 --> 01:05:01,970
So we need to find the critical value that has half of 5% on either side.

560
01:05:02,270 --> 01:05:05,480
So I don't think I have a picture. Oh, I do have a picture here.

561
01:05:05,840 --> 01:05:16,960
So the way that we find these critical values is we want to find the area of alpha is 0.05 here for a 95% confidence interval.

562
01:05:16,970 --> 01:05:27,230
So these these critical values come from the points on the x axis where we can symmetrically enclose one minus alpha.

563
01:05:27,230 --> 01:05:32,720
So 95% of the sampling distribution is enclosed in this shaded region.

564
01:05:33,110 --> 01:05:36,739
So that's where this T value comes from.

565
01:05:36,740 --> 01:05:43,460
So this is the critical value that multiplies the standard error of W bar.

566
01:05:43,820 --> 01:05:48,800
So you can see that as the standard error shrinks if we had a larger sample.

567
01:05:49,780 --> 01:05:57,460
This confidence interval is going to become more narrow because you're adding and subtracting a smaller quantity, right?

568
01:05:57,850 --> 01:06:02,530
So then if we go through and we we plug in these these values that we have,

569
01:06:02,830 --> 01:06:11,140
we have the estimate for the mean we can pull this t critical value from an appropriate table or from R for example,

570
01:06:11,470 --> 01:06:19,720
we have our estimated standard deviation for the weights and then we just divide by the square root of our sample size.

571
01:06:20,080 --> 01:06:25,690
And then we end up with this interval. 148 to 169 Right?

572
01:06:25,690 --> 01:06:35,350
So this is the 95% confidence interval for the mean weight based on our sample of 50 Detroit male residents.

573
01:06:35,980 --> 01:06:41,770
So the confidence intervals are another kind of tricky concept.

574
01:06:41,770 --> 01:06:52,540
I think there's there's often an impulse to say it's a 95% chance that the true parameter value is in that interval, and that's not correct.

575
01:06:53,080 --> 01:06:59,080
So what, what then? The reason that's not correct is the true parameter value is fixed.

576
01:06:59,800 --> 01:07:03,310
So it's either inside this interval or it's not for a particular sample.

577
01:07:03,310 --> 01:07:11,620
Right. So the probability, zero or one, there's no notion of a probability of this interval enclosing the true parameter value.

578
01:07:12,070 --> 01:07:19,060
Really. We need to think about the confidence interval as also being a random quantity, right?

579
01:07:19,210 --> 01:07:25,900
So it's a random variable because it's a function of a sample mean and a sample standard deviation.

580
01:07:26,620 --> 01:07:35,230
So when you take another sample and you recalculate the confidence interval, you're going to get a different interval, right?

581
01:07:35,680 --> 01:07:45,159
So the that, the correct interpretation of the confidence interval is that if you repeat this experiment

582
01:07:45,160 --> 01:07:51,370
a large number of times and each time calculate a confidence interval in this manner,

583
01:07:51,880 --> 01:07:57,080
95% of those are going to have the true parameter inside them.

584
01:07:57,310 --> 01:08:01,780
Right. So you guys are familiar with that. Just again, this is a tricky concept.

585
01:08:01,780 --> 01:08:08,500
I think a lot of people misunderstand. So it's good to have a firm grasp on on the interpretation of the confidence interval.

586
01:08:09,430 --> 01:08:15,340
So that is how we think about a confidence interval. And it's not as it's a little bit of a more nuanced interpretation,

587
01:08:15,610 --> 01:08:22,390
but essentially what we're doing with the confidence interval is just conveying the uncertainty in our estimate,

588
01:08:22,750 --> 01:08:32,320
right through the wider the confidence interval, the less certainty we have about our estimate being close to the true value, right?

589
01:08:32,590 --> 01:08:37,510
So as the confidence level, right. So that's the 95% part.

590
01:08:37,810 --> 01:08:41,890
The interval width is going to increase as the confidence level increases.

591
01:08:42,340 --> 01:08:52,030
So if you wanted to create a 99% confidence interval, it would be wider than this because this T critical value would be larger.

592
01:08:52,330 --> 01:08:52,629
Right.

593
01:08:52,630 --> 01:09:02,680
If we look at our picture here, if Alpha becomes 0.01, which of course one to a 99% confidence interval, we'd have to push this shaded area out.

594
01:09:03,730 --> 01:09:07,090
Right, to be able to get that area covered here.

595
01:09:07,360 --> 01:09:14,410
So these critical values would become larger. So then that will increase the width of the confidence interval here.

596
01:09:16,700 --> 01:09:20,029
So. Guess so. That's one feature, right?

597
01:09:20,030 --> 01:09:29,929
Is this t critical value that contributes to the width of the confidence interval increasing in is going to result in narrower confidence intervals.

598
01:09:29,930 --> 01:09:32,720
Right. Because PN is in the denominator here.

599
01:09:33,080 --> 01:09:41,060
So as in increases, this quantity is going to get smaller and that's going to shrink the width of our confidence interval.

600
01:09:41,660 --> 01:09:48,469
And of course you can see as well that another thing in the in the numerator here is the variance of our random variable.

601
01:09:48,470 --> 01:09:53,360
So if that's lower, then we're going to have a narrower confidence interval as well.

602
01:09:53,990 --> 01:09:58,460
Any questions about confidence intervals? Yes.

603
01:09:59,030 --> 01:10:09,010
What do you mean by that interview with. So so the so the width of the confidence interval is essentially captured by this term here.

604
01:10:09,020 --> 01:10:15,130
So T is in -1.975 times the standard deviation of W hat.

605
01:10:15,350 --> 01:10:24,290
So that's, that's what's creating the width of the confidence interval. So this t critical value has a confidence level associated with it.

606
01:10:24,290 --> 01:10:27,950
So 95% here is our confidence level.

607
01:10:28,520 --> 01:10:33,170
So when we increase our confidence level, the example I used was 99%.

608
01:10:33,980 --> 01:10:39,260
This t critical value is going to increase as well because if we look at our picture here,

609
01:10:39,500 --> 01:10:44,990
we want to enclose instead of 95% of the area here, we want to enclose 99%.

610
01:10:45,260 --> 01:10:48,880
We're going to have to push out these these these lines here.

611
01:10:48,890 --> 01:10:54,800
Right. So taking close that that area, we have to have a larger critical value.

612
01:10:55,070 --> 01:11:03,200
So that's going to increase the width. Right. And the intuition there is to be more confident about something.

613
01:11:03,230 --> 01:11:07,100
You have to allow more possible value. Right. Given the same data.

614
01:11:07,310 --> 01:11:13,340
So that's the intuition there. Good question. But yes, can you remind us of the R code?

615
01:11:13,340 --> 01:11:22,970
We used to get that T critical value. So that would be a Q T, and then you plug in 0.975 and then degrees of freedom is the second argument.

616
01:11:26,420 --> 01:11:33,830
And these these t critical values are again available in our or in the I think, the tables on the campus site as well.

617
01:11:36,380 --> 01:11:40,070
So so some more some more words about confidence intervals here.

618
01:11:40,490 --> 01:11:48,410
We can think of them as sort of denoting a plausible value for the are plausible interval for the true mean.

619
01:11:48,650 --> 01:11:52,850
And this is not just limited to means. We can have confidence intervals for the variance.

620
01:11:52,910 --> 01:11:56,030
Right. We talked about that when we talked about the Chi Square distribution.

621
01:11:57,230 --> 01:12:01,010
The values in the interval should be compatible with the data. That's the whole idea.

622
01:12:01,010 --> 01:12:12,620
Here we have a sample and we want to calculate sort of a range of values for this parameter that will be compatible with the data that we sample.

623
01:12:14,150 --> 01:12:21,920
And the kind of the formal way to think about this is its values for which we would not reject the null hypothesis.

624
01:12:22,430 --> 01:12:30,290
So we can think about confidence intervals as being sort of a counterpart to hypothesis testing in this way.

625
01:12:30,830 --> 01:12:33,830
Right. So you remember the.

626
01:12:34,910 --> 01:12:42,620
Well, we'll see. I guess we'll see an example here in a minute. So before we get to that, we can talk about the idea of hypothesis testing.

627
01:12:42,830 --> 01:12:47,390
So I've just introduced the notion that confidence intervals and hypothesis testing are related.

628
01:12:47,750 --> 01:12:56,250
So we're going to talk about hypothesis testing for a moment. So hypothesis testing, we're going to end up with sort of a decision that we're making.

629
01:12:56,430 --> 01:13:03,180
We're either going to reject or fail to reject the null hypothesis and some examples.

630
01:13:03,340 --> 01:13:10,680
So so a hypothesis in statistics is a little bit different than a hypothesis that you might find in a science experiment.

631
01:13:10,680 --> 01:13:17,580
Right? It's typically in the form of kind of something that you want to to.

632
01:13:18,540 --> 01:13:22,650
Test, I guess where is the hypothesis? And science is often kind of what you're expecting to see.

633
01:13:23,220 --> 01:13:30,360
So so in statistics, a hypothesis could be the mean weight of newborns is equal to 3.1 kilograms.

634
01:13:30,900 --> 01:13:37,260
That's a that would be a null hypothesis that we could test by collecting data on the weight of newborns.

635
01:13:37,260 --> 01:13:45,840
Right. And then then calculating means the mean time to clearing headache is the same for those who are treated with medication A versus B.

636
01:13:46,530 --> 01:13:54,690
So this would be an example of a two sample situation because we would have a sample treated with a sample treated with B.

637
01:13:55,050 --> 01:14:04,170
So this would be a two sample testing situation. This first example would be a one sample test because we're testing a fixed value.

638
01:14:04,500 --> 01:14:12,210
So this is an important distinction. When we have a fixed value that we're testing against, that's that's typically a one sample scenario.

639
01:14:12,960 --> 01:14:22,710
Whereas when we're testing the mean between two groups or more than two groups, that would be a two or more sample scenario.

640
01:14:23,010 --> 01:14:30,450
Right? And the reason that that distinction is important is because 3.1 kilograms here has no variance.

641
01:14:30,750 --> 01:14:38,340
Right? The variance is zero. But when we calculate the means for group A and group B, those or random variables,

642
01:14:38,340 --> 01:14:43,680
and you have to account for the fact that those are random and not fixed.

643
01:14:44,370 --> 01:14:46,920
So so this is an important distinction there.

644
01:14:47,970 --> 01:14:54,930
And then this, this this last hypothesis is one that's going to become very important in our linear regression conversation.

645
01:14:55,680 --> 01:15:01,020
Smoking during pregnancy affects birth weight after accounting for maternal age.

646
01:15:01,650 --> 01:15:05,309
So then we can think about things like the outcome variable here.

647
01:15:05,310 --> 01:15:09,480
The dependent variable here is going to be birth weight, right?

648
01:15:09,480 --> 01:15:17,580
We're talking about smoking being a factor or an independent variable that may influence birth weight.

649
01:15:17,970 --> 01:15:26,430
And again, we want to be careful. I mentioned last time about causal statements, so we're typically going to talk about associations.

650
01:15:26,640 --> 01:15:31,320
So even affects is probably a stronger term than we might want to use here.

651
01:15:31,680 --> 01:15:38,009
But we can say it because smoking precedes birth weight, right?

652
01:15:38,010 --> 01:15:43,980
The smoking during pregnancy temporally precedes the birth weight of the of the infant.

653
01:15:44,850 --> 01:15:47,640
And then after accounting for maternal age,

654
01:15:48,030 --> 01:15:55,740
that language should tip you off to this being a regression problem and as I should tip you up once we get to the regression conversation.

655
01:15:56,160 --> 01:16:00,600
So this is when we talk about adjusting for a variable, right?

656
01:16:00,600 --> 01:16:04,500
This is what we mean here, too, after accounting for maternal age.

657
01:16:04,860 --> 01:16:10,680
And we'll look at an example. So so in our example about the Detroit residence, right.

658
01:16:10,800 --> 01:16:16,470
So we have our sample mean, which is 159 and our sample standard deviation, which is 38.

659
01:16:16,950 --> 01:16:23,940
Right. So a CNN reporter claims that the mean weight of male residents of Detroit is 175.

660
01:16:24,360 --> 01:16:27,900
And we want to know, do the observed data support the reporter's claim?

661
01:16:28,820 --> 01:16:32,600
So what analytic approach is appropriate for those questions? They may want to shout it out.

662
01:16:43,170 --> 01:16:47,430
I know you all have seen this. The T test.

663
01:16:48,510 --> 01:16:58,320
So one sample t test. So the the null hypothesis here is that the mean weight is equal to 175.

664
01:16:58,590 --> 01:17:01,650
So we have a one sample, right? We have one sample.

665
01:17:01,920 --> 01:17:05,280
And then we have a fixed value that we're comparing the mean to.

666
01:17:05,550 --> 01:17:08,990
So this is going to be a one sample t test. Right.

667
01:17:10,110 --> 01:17:15,090
So our null hypothesis is that the mean weight is equal to 175.

668
01:17:15,450 --> 01:17:20,630
The alternative here is that the mean weight is not equal to 175.

669
01:17:20,640 --> 01:17:29,490
So this is a two sided test. Okay. When we have a greater than or less than sign here, this becomes a one sided test.

670
01:17:29,490 --> 01:17:38,820
But for this, we're talking about a two sided test. So to carry out this testing procedure, we're going to need to specify the null alternatives.

671
01:17:39,120 --> 01:17:44,940
The significance level, we talked about that. So that's the probability of a type one error.

672
01:17:45,270 --> 01:17:49,410
And we'll talk more about that as well. But here we're saying that's .05.

673
01:17:49,680 --> 01:17:57,720
So when we say that the significance level is .05, that's the mirror image to a 95% confidence interval.

674
01:17:58,230 --> 01:18:04,980
Right. So then we want to calculate the test statistic, which is this ratio here.

675
01:18:05,340 --> 01:18:10,409
So it's the difference between our observed or estimated weight,

676
01:18:10,410 --> 01:18:18,510
mean weight and the null hypothesized mean weight divided by the standard error of that mean weight.

677
01:18:19,230 --> 01:18:24,610
Right. So. So the intuition here is this is a distance, right?

678
01:18:24,630 --> 01:18:29,700
This difference between the the hypothesized and the estimated weight is a distance.

679
01:18:30,240 --> 01:18:39,160
And then when we divide by the standard error of the mean, we're changing the units of that distance into being units of standard errors of the mean.

680
01:18:39,840 --> 01:18:50,190
So this is all going towards standardizing our observed data into a form that we can relate to a known distribution.

681
01:18:50,420 --> 01:18:55,770
In this case, it's the T distribution with NW -149 degrees of freedom.

682
01:18:56,520 --> 01:19:01,680
Then we calculate this test statistic and we get -2.95 in this case.

683
01:19:02,880 --> 01:19:11,700
So essentially in statistics, when we have a large value in absolute value terms of a test statistic that is

684
01:19:11,700 --> 01:19:17,100
generally associated with is generally not consistent with the null hypothesis.

685
01:19:17,610 --> 01:19:20,040
So if we calculate the P value here,

686
01:19:20,340 --> 01:19:32,570
which in the case of a two sided test is the sum of the tail probabilities on both sides of this value, then we end up with .0049, right?

687
01:19:32,610 --> 01:19:37,080
Because 0.0049 is less than 0.05.

688
01:19:37,350 --> 01:19:41,190
Our conclusion is to reject the null hypothesis.

689
01:19:41,580 --> 01:19:48,240
So the data is not consistent with the claimed mean weight of 175.

690
01:19:48,750 --> 01:19:57,920
We estimated 159. The combination of our estimated weight, our estimate is standard error and then this statistical procedure.

