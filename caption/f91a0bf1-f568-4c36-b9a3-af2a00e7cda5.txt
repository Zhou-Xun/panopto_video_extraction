1
00:00:35,150 --> 00:00:46,620
Okay. Good morning, everyone. So let's briefly recall what we had in our previous lectures.

2
00:00:50,360 --> 00:00:58,770
So we have. Started by giving a formal definition.

3
00:00:59,340 --> 00:01:09,640
So in the lectures prior to the previous ones, we kind of tried to develop the intuition for conditional expectation using uh,

4
00:01:10,350 --> 00:01:22,440
expectations with respect to sigma algebras generated by discrete random variables and them motivated by what we understood.

5
00:01:22,950 --> 00:01:28,710
In this analysis, we essentially gave a formal definition to it.

6
00:01:31,140 --> 00:01:34,800
And so this was.

7
00:01:39,180 --> 00:01:49,200
So two things need to happen for a random variable to be a conditional expectation of another one given some sigma algebra.

8
00:01:51,820 --> 00:01:53,049
So first of all,

9
00:01:53,050 --> 00:02:01,690
the conditional expectation is defined as a random variable and it's defined as one that's measurable with respect to this sigma algebra.

10
00:02:02,230 --> 00:02:10,870
So in case this sigma algebra is discrete, so we already know that the class of such random variables would just be.

11
00:02:12,850 --> 00:02:24,400
So each of them is identified by specifying a vector of values on the events where the random variable is supposed to take a fixed value.

12
00:02:26,370 --> 00:02:35,489
And then the second condition was that essentially so we dropped the p of a from the denominator,

13
00:02:35,490 --> 00:02:41,250
anticipating that some of them will be zero eventually. Not in the discrete case though.

14
00:02:42,080 --> 00:02:49,350
Um. Uh. So it's basically says that if we take an additional conditional expectation of something

15
00:02:49,350 --> 00:02:53,400
that's already a conditional expectation with respect to the same sigma algebra,

16
00:02:53,610 --> 00:02:58,200
then nothing changes, right? So that's the equality of the two integrals.

17
00:02:58,800 --> 00:03:09,210
So the expectation of y over the area is where well is the conditional one is same as expectation of the original and the variable over the area a.

18
00:03:12,150 --> 00:03:18,620
Okay. That was it. And so it's the second one that's essential.

19
00:03:20,520 --> 00:03:34,450
And so we have in addition to this formal definition, so we have in mind some of the properties that we found for the discrete ones, namely that the.

20
00:03:38,740 --> 00:03:48,130
That the conditional expectation is the closest random variable to a given 12x.

21
00:03:48,220 --> 00:03:48,879
For example,

22
00:03:48,880 --> 00:03:58,360
if we take conditional expectation of X given you so that conditional expectation will be the closest to X in the class of measurable functions.

23
00:03:58,750 --> 00:04:03,670
So in the sense that defines a projection, right?

24
00:04:03,670 --> 00:04:07,240
So when we find an element of a bigger space.

25
00:04:09,480 --> 00:04:14,010
Of a smaller space that's closest to any element of a bigger space.

26
00:04:14,010 --> 00:04:22,110
So that's by definition a projection. So there is one other property of the projection, if you remember.

27
00:04:24,290 --> 00:04:33,110
Uh, because on the one hand, so the y is the closest element on this basis you to the ex.

28
00:04:33,470 --> 00:04:45,830
On the other hand, we know that such elements are characterized by the source of the knowledge of the vector x, minus y, y, minus x to the space of s.

29
00:04:45,830 --> 00:04:48,860
You write to any vector in the space as you.

30
00:04:48,920 --> 00:04:53,900
So this would be your uh, geometric, uh, intuition.

31
00:04:54,230 --> 00:04:58,010
Uh, but we haven't proved that for conditional expectation.

32
00:04:58,020 --> 00:05:08,900
So this, uh, comes later when we have, uh, established the properties that we need to actually prove something like this.

33
00:05:11,470 --> 00:05:15,220
Um. Okay.

34
00:05:15,220 --> 00:05:27,610
So then so we considered an example of a uniform distribution and we defined another binary random variable,

35
00:05:28,030 --> 00:05:33,669
the simpler one, right, than the sigma algebra that's generated by the binary random variable.

36
00:05:33,670 --> 00:05:44,409
So that has four elements and they all negate self to sense where my random variable takes values zero and one respectively and then the,

37
00:05:44,410 --> 00:05:47,980
the empty set. Yeah.

38
00:05:48,300 --> 00:05:56,560
And so we found what the corresponding random variable is that represents the conditional expectation.

39
00:05:58,630 --> 00:06:09,820
Okay. And it's basically looking within the SATs defined by the Sigma algebra that's being conditioned on and finding the,

40
00:06:10,360 --> 00:06:17,470
uh, conditional expectation in the usual six or one sense within each of these sets.

41
00:06:18,100 --> 00:06:23,200
And that represents the value of the new random variable rate.

42
00:06:23,200 --> 00:06:34,180
And also another thing to note is, uh, so it basically illustrates the definition and the meaning of the conditional expectation.

43
00:06:34,190 --> 00:06:39,280
So it's taken with respect to Z and Z's and binary random variable,

44
00:06:40,040 --> 00:06:46,750
then the conditional expectation will also be a binary random variable right on the same since defined in the same sets,

45
00:06:47,140 --> 00:06:49,510
but with different values in general.

46
00:06:54,750 --> 00:07:03,060
And we just thought when you went through the trouble of taking in the girls that represent conditional expectations, we also noted that.

47
00:07:03,930 --> 00:07:16,049
So these integrals. So in the case where my omega is a number so they represent the theory that was used in calculus to average functions.

48
00:07:16,050 --> 00:07:22,160
Right. So that that has the same meaning.

49
00:07:22,970 --> 00:07:34,100
So then another useful theorem was that if the sigma algebra is already the one where X is defined,

50
00:07:34,100 --> 00:07:37,700
so x is measurable with respect to F then conditioning.

51
00:07:37,700 --> 00:07:40,970
And it doesn't change anything in the sense that I get X back.

52
00:07:42,250 --> 00:07:47,040
Um. Yeah. This was simply a consequence they had.

53
00:07:47,710 --> 00:07:54,790
So we're thinking of Sigma. Algebras in the conditional expectations is as a restriction on the original one.

54
00:07:55,030 --> 00:07:59,140
Right. And that's how, uh. That's also.

55
00:08:00,970 --> 00:08:04,640
Uh. Builds on the intuition here.

56
00:08:04,970 --> 00:08:11,750
Right. So the sue can be considered the restricted space compared to the s f, right.

57
00:08:12,620 --> 00:08:19,670
And the fact that X is different from Y is because I can't actually take s.

58
00:08:20,360 --> 00:08:27,200
X is as close as element to x because I'm only allowed to look in the restricted space.

59
00:08:27,200 --> 00:08:30,400
And that's my definition of the conditional expectation.

60
00:08:30,410 --> 00:08:38,000
But if my restricted space is basically the space where X is defined.

61
00:08:38,390 --> 00:08:45,920
So if I'm conditioning on F, then the closes the element within this space as f2x is x itself.

62
00:08:47,000 --> 00:08:50,390
So that's what the.

63
00:08:53,610 --> 00:09:02,340
This theorem says. And so there will be generalizations of the theorem later on.

64
00:09:02,350 --> 00:09:15,360
So we will show that basically if I conditional a sigma algebra, you and I have some complicated expressions with a bunch of random variables.

65
00:09:16,080 --> 00:09:26,639
So if there are any random variables in that expression that are already measurable with respect to you, so they will behave like a constant.

66
00:09:26,640 --> 00:09:31,049
We will be able to take them out of the conditional expectation.

67
00:09:31,050 --> 00:09:34,680
So that would be a generalization of this theorem.

68
00:09:35,730 --> 00:09:40,800
Right. We also discussed the meaning, the physical meaning of mean values.

69
00:09:42,330 --> 00:09:55,260
That's that's a useful thing for quickly solving a problem based on that meaning, namely that the expectation is a point of balance.

70
00:09:55,500 --> 00:09:55,860
Right.

71
00:09:56,610 --> 00:10:08,849
So if if we have conditional expectations, so we look for an average for a usual expectation within the restricted space, uh, uh, within the sets.

72
00:10:08,850 --> 00:10:12,420
Right. Things intuitive this ah symmetry,

73
00:10:13,110 --> 00:10:21,630
then the expectation will be right in the middle based on that definition because it's the midpoint that gives balance.

74
00:10:22,520 --> 00:10:31,739
Uh, so we noted in passing that once we define the conditional expectation, we can also define conditional probability using expectation.

75
00:10:31,740 --> 00:10:38,870
So that was the. Uh, the intuitive definition of probability is the indicator.

76
00:10:40,610 --> 00:10:49,219
And then we formulated a theorem that is a formula of total expectation that will eventually

77
00:10:49,220 --> 00:10:55,790
serve both the formula of total expectation and the formula of total probability.

78
00:10:55,790 --> 00:11:01,520
Eventually, when we show how it's linked to probabilities, when we take indicator functions.

79
00:11:02,400 --> 00:11:08,330
Right. But we haven't proved it. So I'm going to just copy to the new lecture.

80
00:11:14,980 --> 00:11:24,990
And we didn't approve. Somehow it doesn't want to sink here.

81
00:11:33,290 --> 00:11:53,940
Put up a new one. So property is of conditional expectation.

82
00:11:56,340 --> 00:11:59,630
Let me restate this theorem.

83
00:12:01,820 --> 00:12:10,290
So that means that I can take expectation of X by steps, right?

84
00:12:10,310 --> 00:12:17,360
And first conditioning on some sigma algebra and then taken an unconditional outer expectation.

85
00:12:17,570 --> 00:12:28,340
And we discussed what the conditional expectation and other intuition for it, namely we can consider expectation as a functional, right?

86
00:12:28,340 --> 00:12:32,900
So functional is something that takes a function and spits out a number.

87
00:12:34,100 --> 00:12:43,520
So in a sense, it's something that if we translate it to the terminology and the probability theory, right?

88
00:12:43,520 --> 00:12:54,110
So X is a function and acting by an expectation on X, if it's unconditional, that leaves that number because expectation is a function of.

89
00:12:55,790 --> 00:13:06,260
So another intuition kind of attached to it is that we essentially eat all the randomness in the X and I get in on the random number as a result.

90
00:13:07,190 --> 00:13:11,540
Right? So the, the meaning,

91
00:13:11,990 --> 00:13:21,229
the intuition behind the formula of total expectation would be that they don't have to eat that all the randomness of the X all at once,

92
00:13:21,230 --> 00:13:27,920
I can do it by pieces and conditional expectations and that still leaves X as a random variable.

93
00:13:29,630 --> 00:13:39,410
It's a conditional on you, but that's just simple around the variable in the sense that it's more restricted, there's less random listening, right?

94
00:13:39,410 --> 00:13:46,459
So we took in the example, we took a pretty complicated random variable in terms of the sigma algebra

95
00:13:46,460 --> 00:13:52,460
that was the uniformly distributed one that has Morrell as the Sigma algebra,

96
00:13:53,300 --> 00:13:57,170
right? And we took it conditional on a binary random variable.

97
00:13:57,170 --> 00:14:04,670
We convert it into a binary random variable. So that conditional expectation essentially a double the complexity of the

98
00:14:04,670 --> 00:14:09,980
barrel and left us with the sigma algebra that only has four elements in it.

99
00:14:11,300 --> 00:14:17,300
Okay, so let's. Now furnish the proof.

100
00:14:24,230 --> 00:14:33,670
For the formula. I was told the expectation. So we have.

101
00:14:33,940 --> 00:14:44,290
So the conditional expectation we have in the usual notation, that's a random variable Y, it's conditional expectation of X given you.

102
00:14:48,020 --> 00:14:52,370
Okay. And by definition for that, why?

103
00:14:52,700 --> 00:14:59,660
So if I'm taking another conditional expectation of Y with respect to the same sigma algebra.

104
00:15:00,390 --> 00:15:03,620
So if I take integral of y evenly.

105
00:15:04,640 --> 00:15:12,850
So that's by definition. Same as the one for the original when the variable.

106
00:15:15,220 --> 00:15:18,970
All right. So just restated the definition of conditional expectation here.

107
00:15:21,110 --> 00:15:26,299
Right. And this is supposed to happen with respect to any a in the sigma algebra you so

108
00:15:26,300 --> 00:15:33,470
it's essential to keep track of what kind of events this is supposed to happen for.

109
00:15:33,680 --> 00:15:39,170
Right. It's only supposed to happen for a coming from the sigma algebra.

110
00:15:39,180 --> 00:15:42,650
You that's the one used for the conditional expectation.

111
00:15:43,130 --> 00:15:46,520
So for any other events, these integrals do not have to be the same.

112
00:15:48,670 --> 00:15:54,040
Okay. So what I'm going to do in this formula, I'm going to take.

113
00:15:56,370 --> 00:16:00,150
He as the army does. I can always do that.

114
00:16:02,780 --> 00:16:08,570
Right. And then, um. So this is the full army then.

115
00:16:09,740 --> 00:16:12,860
So any expectation of anything? Right.

116
00:16:13,790 --> 00:16:18,560
So as an integral over the full army, that's by definition the.

117
00:16:20,220 --> 00:16:23,730
Expectation of whatever they say is right.

118
00:16:24,870 --> 00:16:29,190
The unconditional one. Because that's the full integrals.

119
00:16:30,480 --> 00:16:36,580
So them. So I can have that expectation of why.

120
00:16:39,190 --> 00:16:47,620
That's what my first term and the definition of quality is.

121
00:16:48,850 --> 00:16:55,780
It's going to be equal to. And my second one will turn into expectation.

122
00:16:57,700 --> 00:17:02,000
Oh, that's. Right.

123
00:17:02,040 --> 00:17:15,060
And I'm now going to. Recall that my why is actually an expected value of x given you again by definition.

124
00:17:16,860 --> 00:17:19,980
That's the way we defined it.

125
00:17:20,700 --> 00:17:24,270
So as a result, I'm just restating this inequality.

126
00:17:24,870 --> 00:17:37,290
So the expectation of the Y, which is E of X with you is same is the expectation of X.

127
00:17:37,290 --> 00:17:41,969
And this is just what we intended to prove, right? So we're very quick thing.

128
00:17:41,970 --> 00:17:48,750
It's simply an interpretation of the definition where we take the events as full one again.

129
00:17:49,800 --> 00:17:57,930
This is where. The formula of total probability comes from now.

130
00:18:00,670 --> 00:18:04,540
Being that simple, it's actually remarkably useful.

131
00:18:05,820 --> 00:18:10,710
Right. And so let's consider an example.

132
00:18:12,850 --> 00:18:25,830
From modeling. For the formula of total expectation.

133
00:18:40,170 --> 00:18:42,100
A formula of total probability.

134
00:18:45,770 --> 00:18:57,650
So when I was talking to you about a problem where, uh, the distributions could have some massive the infinity, and I call this problem cure models.

135
00:19:01,870 --> 00:19:08,920
So these were about. So suppose I treat somebody who has cancer and after treatment.

136
00:19:10,270 --> 00:19:13,300
So cancer in some people eventually comes back.

137
00:19:13,870 --> 00:19:17,800
In some people it doesn't. So here is a, uh, a patient.

138
00:19:18,160 --> 00:19:26,950
And I generally, I assume that I have some, uh, lesions that are left in the patient after treatment.

139
00:19:26,950 --> 00:19:30,820
So these are kind of pockets of tumor that are microscopic.

140
00:19:30,830 --> 00:19:36,330
You don't see them, but they are the ones that eventually give a recurrence of cancer.

141
00:19:36,340 --> 00:19:48,130
And another event, which is of main interest, and I'm assuming in this toy model that there are, um, actually.

142
00:19:51,140 --> 00:19:54,380
X lesions in the patients.

143
00:20:03,900 --> 00:20:07,500
Now just a reminder of our discussion of queue of models.

144
00:20:07,890 --> 00:20:11,640
So I have the, let's say, the survival time.

145
00:20:13,900 --> 00:20:26,560
T that is. And that has some kind of a CDF and the CDF does not reach one.

146
00:20:27,070 --> 00:20:32,950
So it's something less than one. And this is because there's mass sitting at infinity.

147
00:20:36,030 --> 00:20:42,120
That's that's the CDF. It's more common because we're talking about a survival analysis problem.

148
00:20:42,630 --> 00:20:46,590
You all have to know survival analysis to understand these.

149
00:20:47,190 --> 00:20:52,200
Uh, but just for the sake of appropriate terminology.

150
00:20:52,740 --> 00:20:57,570
So one minus CDF is called the survival function.

151
00:20:59,540 --> 00:21:03,730
And there it will look like this.

152
00:21:03,740 --> 00:21:10,160
It will start. At one rate, but then not go to zero.

153
00:21:11,860 --> 00:21:25,170
So that's the survival function of. And we argued that this is the mass seeding at infinity and we called it the probability of cure rate.

154
00:21:25,650 --> 00:21:28,950
So that expresses the fact they have cancer never comes back.

155
00:21:34,370 --> 00:21:43,520
So when we used this notion, this model for the first time,

156
00:21:43,880 --> 00:21:53,630
we actually used it to just illustrate the fact that some of the bad expressions that you take for granted from

157
00:21:53,630 --> 00:22:00,140
the 601 so a lot of them don't work if you assume that there is a probability of mass sitting at infinity,

158
00:22:00,710 --> 00:22:05,840
for example, as one example here for this type of a distribution.

159
00:22:06,080 --> 00:22:14,990
So the integral over the PDF is not going to be equal to one, it's going to be equal to a one minus the mass that's sitting at infinity.

160
00:22:17,000 --> 00:22:24,950
Okay, so what I'm trying to do here, I'm building a model pretty simplistic.

161
00:22:24,980 --> 00:22:36,650
That would be a mechanistic interpretation of a pure model, as we will see, but we will kind of put aside the pure model stuff.

162
00:22:37,100 --> 00:22:44,120
I will just formulate the mechanistic model. I will try to find the distribution for the survival time here.

163
00:22:44,480 --> 00:22:47,840
So I have x lesions in the P in the patients.

164
00:22:47,840 --> 00:22:50,840
So this is a count, right? Number of lesions.

165
00:22:55,900 --> 00:23:01,720
And there could be zero. So X equals zero means.

166
00:23:02,110 --> 00:23:05,230
Well, we basically did our treatment perfectly.

167
00:23:05,560 --> 00:23:09,390
There's no cancer in the person, so it can come back after that.

168
00:23:09,400 --> 00:23:12,970
So that means that we have a cure.

169
00:23:14,090 --> 00:23:31,240
Here's the patient. So we already see that this probability that X is equal to zero will actually give us the probability of cure.

170
00:23:34,440 --> 00:23:46,469
In this context. Now, if I want to model X, that is a count.

171
00:23:46,470 --> 00:23:50,370
What would be a first choice for a model for account?

172
00:23:52,970 --> 00:23:57,780
How do the model counts? Who are some distribution rights.

173
00:23:58,530 --> 00:24:04,700
Okay. So let's. Give a model for X.

174
00:24:08,830 --> 00:24:23,110
Expertize for us on distribution. Now.

175
00:24:23,110 --> 00:24:26,380
How do you remember a pause on distribution?

176
00:24:26,920 --> 00:24:31,330
So most of you probably know the formula for it, right? Um.

177
00:24:32,290 --> 00:24:38,920
How. How? Kind of embedded into her a somewhat more general concept that may be useful to you.

178
00:24:39,660 --> 00:24:54,380
Uh, it's called power series distributions. Okay.

179
00:24:55,490 --> 00:24:59,060
So let's say I have a functional.

180
00:25:01,540 --> 00:25:12,730
E of x. And so I can expand it into a power series with respect to around X equals zero.

181
00:25:16,080 --> 00:25:21,210
Yeah. You could also call it Taylor expansion because they are the scene here.

182
00:25:27,400 --> 00:25:38,020
So and that would be what some say came from zero to infinity x2k over k factorial.

183
00:25:41,970 --> 00:25:53,820
If I can do it to a lot of functions, actually, you know, because a lot of the functions we consider are expandable into a power series.

184
00:25:54,300 --> 00:26:01,350
Now, that can be at this point when I take my favorite function and expanded into a power series,

185
00:26:01,770 --> 00:26:08,370
what I care about for the method to work is that the coefficients of these series are non-negative,

186
00:26:09,840 --> 00:26:13,320
because if they are non-negative, I can interpret them as probabilities.

187
00:26:16,670 --> 00:26:20,990
So I'm looking for an expansion of the time.

188
00:26:21,140 --> 00:26:24,980
Some K from zero to infinity.

189
00:26:25,580 --> 00:26:29,380
Some positions a key times x to K.

190
00:26:29,390 --> 00:26:36,190
This is part a series. Yeah.

191
00:26:36,380 --> 00:26:41,020
For some. Functions.

192
00:26:45,670 --> 00:26:49,240
So my AK is going to be great equals zero for.

193
00:26:50,880 --> 00:26:59,220
In UK. So how do I make probabilities out of the series like this?

194
00:27:03,570 --> 00:27:12,540
I will divide both parts of the above a quality by e2x or multiply by E two minus x.

195
00:27:13,890 --> 00:27:17,970
Then what I'm going to have is that one is equal to some.

196
00:27:19,760 --> 00:27:24,170
K from zero to infinity X to K.

197
00:27:25,540 --> 00:27:29,860
Over K factorial times e to minus x.

198
00:27:30,610 --> 00:27:36,010
Right. I can use the opportunity to introduce a parameter.

199
00:27:36,040 --> 00:27:40,210
So instead of x I will use lambda times x.

200
00:27:43,840 --> 00:27:49,710
And then. So acts against that won't change anything, right?

201
00:27:49,760 --> 00:27:59,650
Because X is just an arbitrary argument. I could use Lambda Times X and what you can see is that this is a normalization condition for something.

202
00:27:59,660 --> 00:28:04,420
I can interpret this probabilities. So these are Poisson probabilities.

203
00:28:22,300 --> 00:28:32,710
You can also look at this formula is as an expectation of one that's supposed to be equal to one.

204
00:28:36,350 --> 00:28:40,429
Okay. So this is how I can get pass on probabilities.

205
00:28:40,430 --> 00:28:44,860
So I will. Just. Restate that.

206
00:28:44,870 --> 00:28:49,169
So it's number 2kx to K. Okay.

207
00:28:49,170 --> 00:28:52,530
Factorial each or minus x times lambda.

208
00:28:56,520 --> 00:28:59,940
Okay. That that gives me a model for the X.

209
00:29:03,480 --> 00:29:07,500
Now I need come back.

210
00:29:07,740 --> 00:29:13,490
So I need to explain how lesions give tumors.

211
00:29:13,500 --> 00:29:16,590
Right? So they apparently stop growing.

212
00:29:16,590 --> 00:29:23,010
And at some point, they they give a sizable, uh, piece of the tumor that I can detect.

213
00:29:23,610 --> 00:29:30,929
Uh, they will generally do it randomly, and each will give a tumor at a different point in time.

214
00:29:30,930 --> 00:29:42,900
Let's say this one has grown to a detectable size at this point, and I, I'll call this random variable y one them.

215
00:29:43,890 --> 00:29:49,080
So this one is, is way too and so on.

216
00:29:49,200 --> 00:29:54,079
Right. So the last one is, let's see. Why?

217
00:29:54,080 --> 00:30:00,239
Three, right. But we're certainly not going to wait.

218
00:30:00,240 --> 00:30:05,220
If we are following this patient up, we will see a tumor when the first one gives it.

219
00:30:05,610 --> 00:30:09,300
Right. So we can see that the.

220
00:30:13,280 --> 00:30:18,490
So now this is a model for. T.

221
00:30:19,680 --> 00:30:22,770
So I will say that t is actually a minimum.

222
00:30:25,590 --> 00:30:31,820
Um. When?

223
00:30:35,160 --> 00:30:38,340
He is between six and zero.

224
00:30:42,000 --> 00:30:46,350
Of the why. Okay.

225
00:30:48,240 --> 00:30:54,330
Right. So t is the time to observe humor.

226
00:31:04,520 --> 00:31:10,080
All the time. When?

227
00:31:11,450 --> 00:31:27,060
The fastest lesion progressed. Okay.

228
00:31:27,300 --> 00:31:30,870
Now, you may notice that there's some ambiguity here, right?

229
00:31:31,650 --> 00:31:40,080
But we already said that we will interpret no lesions as, uh, the, uh, curing of the patients.

230
00:31:40,080 --> 00:31:43,350
So in that case, t is going to be equal to infinity.

231
00:31:43,980 --> 00:31:49,400
So for. X equals zero.

232
00:31:51,870 --> 00:31:56,060
She is, by definition, insanity.

233
00:31:59,790 --> 00:32:07,660
And you could also say that. Y0 is equal to infinity.

234
00:32:07,680 --> 00:32:14,170
This. Particular formulation. Okay.

235
00:32:14,920 --> 00:32:19,570
So that's the model for T. Now let's.

236
00:32:23,500 --> 00:32:29,470
Now the question is what is? The distribution of key.

237
00:32:45,610 --> 00:32:57,010
Now I can see that I can make the assumption that simplifying my life rates that the times for each lesion to progress,

238
00:32:57,730 --> 00:33:02,890
that those wikis are already random variables.

239
00:33:06,490 --> 00:33:19,140
With expectations. Not with expectation with CDV.

240
00:33:20,640 --> 00:33:25,010
Let me just give a distribution. They have, right?

241
00:33:27,820 --> 00:33:37,160
Okay. So now I have the problem of finding and distribution for such random variable.

242
00:33:37,210 --> 00:33:43,720
So by the way, a similar problem was on one of the qualifying exams, a few.

243
00:33:44,770 --> 00:33:49,930
Several years ago. So you have to run the variables.

244
00:33:50,460 --> 00:33:54,820
Uh, well, you have a series of random variables, so that's the wiki.

245
00:33:56,560 --> 00:34:00,490
So these are the random variables.

246
00:34:00,500 --> 00:34:10,270
So you also have a random variable X. So you make the assumption that X is independent of the Y case.

247
00:34:15,730 --> 00:34:27,250
And let's say for the distribution. So it would be enough for me to find, for example, the probability that T is greater than T.

248
00:34:29,590 --> 00:34:36,280
Which would be if we're using survival analysis terminology, it's going to be a survival function for.

249
00:34:46,960 --> 00:35:00,090
Or why minus the CD. Now any suggestion how do how I go about finding this distribution?

250
00:35:08,610 --> 00:35:09,329
And by the way,

251
00:35:09,330 --> 00:35:23,220
there's another name to this formula that's considered totally independently in probability theory from the this this pure model formulation,

252
00:35:23,850 --> 00:35:27,360
independent of biostatistics, it's called the random meaningless scheme.

253
00:35:39,180 --> 00:35:55,930
Probability theory. And it's usually looked at from the point of view of some limiting distributions.

254
00:35:58,360 --> 00:36:04,719
Because if you can increase the number of allies and increase or decrease the acts,

255
00:36:04,720 --> 00:36:09,370
so normalize this situation, somehow there's a limiting distribution to it.

256
00:36:12,380 --> 00:36:16,430
There are some limited theorems for the random meaningless scheme.

257
00:36:19,080 --> 00:36:28,470
So the clue to how to deal with it is use the formula of total expectations.

258
00:36:41,470 --> 00:36:43,180
And like I said, it's a useful tool.

259
00:36:43,180 --> 00:36:50,830
When you have a model, the model is always consists of a bunch of random variables and the mechanistic one to be tractable.

260
00:36:50,950 --> 00:36:57,430
You basically have independence as simplifying assumptions to those random variables.

261
00:36:58,000 --> 00:37:06,880
So even if they are dependent to basically invent random variables that induce dependance to deal with it.

262
00:37:07,660 --> 00:37:21,580
Okay. So I'm looking for the probability that minimum between minimum for K will also equal x.

263
00:37:23,500 --> 00:37:31,140
Why keep? Greater than 80.

264
00:37:33,650 --> 00:37:46,160
Now if I'm using the formula of total expectation so I can condition on the sigma algebra that's generated by x by the person random variable.

265
00:37:46,910 --> 00:37:50,750
So I'm going to take an expectation given X, right?

266
00:37:51,080 --> 00:38:01,489
And remember, we understand this is with respect to sigma algebra that's generated by random variable X, right?

267
00:38:01,490 --> 00:38:05,510
If you remember how sigma algebra generates, we actually.

268
00:38:07,800 --> 00:38:13,140
Look at all possible burial cells in the range of the random variable x.

269
00:38:13,680 --> 00:38:18,990
Look at their prim adjust and populate this sigma algebra with those free images.

270
00:38:19,080 --> 00:38:26,340
That's how it's generated. And inside, I still have an expectation.

271
00:38:27,910 --> 00:38:33,910
So outside. Right. So I have a general expectation, unconditional one outside.

272
00:38:36,680 --> 00:38:41,700
And inside I have minimum. Okay.

273
00:38:41,700 --> 00:38:45,340
Let's equal x, y, k.

274
00:38:48,370 --> 00:38:52,560
And this is greater than 80. Now.

275
00:39:00,800 --> 00:39:06,020
The intuition for dealing with a formula like this.

276
00:39:06,160 --> 00:39:15,410
Well, so like I said, we will later prove that, uh, random variables, uh, conditional.

277
00:39:18,920 --> 00:39:27,320
Conditional on you. Will behave.

278
00:39:31,670 --> 00:39:37,320
Like Constance. Inside.

279
00:39:40,650 --> 00:39:45,700
Expectation of. Whatever given you.

280
00:39:49,250 --> 00:39:52,490
Right. So I'll take it for granted for now.

281
00:39:54,000 --> 00:40:00,910
Uh. With with the promise that we will prove it either today or at the next lecture.

282
00:40:02,180 --> 00:40:12,830
So then what? That means that I can consider inside the expectation I will consider this acts as being a constant.

283
00:40:14,150 --> 00:40:23,390
So the sex is a constant and I have ID variables and my event is that the minimal of them is greater than T.

284
00:40:23,690 --> 00:40:29,120
So how can you reformulate this event? So minimum.

285
00:40:31,110 --> 00:40:34,470
And I'd say, okay, lesser equals, whatever.

286
00:40:34,990 --> 00:40:41,780
So for example. If I'm taking Constance, this is not a random variable anymore.

287
00:40:42,470 --> 00:40:48,320
I have wiki greater than ts, so how can I write this event?

288
00:40:51,310 --> 00:40:59,640
So if at least one white key will be less than tea with this work, that won't be part of the event.

289
00:40:59,650 --> 00:41:07,570
Right. So every single wiki needs to be greater than T for the event for the omega two to belong to this event.

290
00:41:08,350 --> 00:41:17,800
And this means that I will have the intersection right every single four K so you call and.

291
00:41:22,430 --> 00:41:31,130
Wiki is greater than t. This is how this event looks like.

292
00:41:32,520 --> 00:41:37,740
Right. And the probability because of this event.

293
00:41:40,110 --> 00:41:45,900
He's going to be equal to a product of K from zero to N.

294
00:41:47,650 --> 00:41:55,100
Of. The probability that y k is greater than t.

295
00:41:55,430 --> 00:41:59,569
So I defined the cdf y. Okay, so that's a complement.

296
00:41:59,570 --> 00:42:16,630
So one minus cdf of g. This is one minus F of T.

297
00:42:19,530 --> 00:42:24,150
Taken to the power of them because they are all the same.

298
00:42:30,270 --> 00:42:36,660
Okay. So now I can continue writing this thing.

299
00:42:44,720 --> 00:42:49,480
Is. The expected value.

300
00:42:50,970 --> 00:42:58,090
Uh. One minus f of t taken to x.

301
00:43:00,440 --> 00:43:04,460
In my X years for song rates.

302
00:43:06,210 --> 00:43:23,840
With the rate of lambda. Now I'll leave it for you as an exercise, because what you see here is a probability generating function.

303
00:43:23,840 --> 00:43:28,450
Right? Uh. Of all the ports on distribution.

304
00:43:30,250 --> 00:43:35,950
Because I have something to the power of X when I take an expectation it will actually.

305
00:43:37,120 --> 00:43:43,380
Um, and the person probabilities are, uh, powers of something, right?

306
00:43:43,390 --> 00:43:51,190
And they have an exponent here, so it will just get inside those parameters and give me another exponent.

307
00:43:51,190 --> 00:44:00,720
Right. So. Just use the this type of a formula for a different three defined X where.

308
00:44:06,340 --> 00:44:13,930
One minus F, two x. So I can just write with what this is, right?

309
00:44:13,970 --> 00:44:17,210
So it's going to be some. X from zero.

310
00:44:21,640 --> 00:44:31,010
Okay. From zero to infinity, one -52 k times the probabilities.

311
00:44:31,030 --> 00:44:35,260
That's what the expectation of a discrete random variable is.

312
00:44:35,650 --> 00:44:40,930
So these probabilities are poor on and they have them written also.

313
00:44:41,110 --> 00:44:46,120
So there is uh. Uh. Lambda uh.

314
00:44:46,940 --> 00:44:50,540
To the power of the that I keep the ax there.

315
00:44:51,050 --> 00:45:02,960
I wasn't supposed to. So these are the probabilities.

316
00:45:03,020 --> 00:45:22,940
And without the X. If I take x x equals one need to come up with those probabilities.

317
00:45:26,610 --> 00:45:30,920
And so on. So you can have an exercise of finding what this thing is.

318
00:45:31,380 --> 00:45:35,730
And this will give you an expression for the model.

319
00:45:37,790 --> 00:45:52,969
For the final models. And the probability of cure, like I said, will be the probability that the person read the variable takes the value of zero.

320
00:45:52,970 --> 00:46:00,010
So the target is zero. This is an exercise.

321
00:46:08,450 --> 00:46:17,570
Okay. So this is an example of how. Formulas of total expectations are good for modeling.

322
00:46:19,930 --> 00:46:24,940
In finding out the properties of your model. So let's now come back to.

323
00:46:28,060 --> 00:46:32,440
The properties of the conditional the expectations.

324
00:46:34,820 --> 00:46:48,190
So we won't show all of them. Uh, we'll just throw a couple, uh, to give you a sense.

325
00:46:49,700 --> 00:46:56,100
What this is about. So let's say I have a probability space.

326
00:46:56,300 --> 00:47:03,290
I'm in the s p. I have a random variable defined the probability space.

327
00:47:04,340 --> 00:47:08,780
And there is another one that I define as a linear combination.

328
00:47:09,650 --> 00:47:15,420
So A plus B, times x. Um, so as you see,

329
00:47:15,480 --> 00:47:23,370
I'm trying to approach linearity of conditional expectation with this argument and

330
00:47:23,370 --> 00:47:30,360
then I have some sigma algebra u that will be a condition on and the theorem.

331
00:47:32,130 --> 00:47:41,550
Is that the expected value of A was B times x conditional on you.

332
00:47:42,450 --> 00:47:50,019
I can write in a similar fashion. To the usual expectations.

333
00:47:50,020 --> 00:47:55,020
So he plus be times expected. David, you.

334
00:48:06,580 --> 00:48:12,820
You can also see this formula as a consequence. So A and B are constants.

335
00:48:12,880 --> 00:48:15,970
Right. And they are the intuition.

336
00:48:15,970 --> 00:48:25,790
Is that. So regardless of what's the sigma algebra, a new E is it will have events that define nonrandom random variables.

337
00:48:25,910 --> 00:48:31,660
Right. Because what it takes to define a non random constant as a random variable right is

338
00:48:31,660 --> 00:48:37,390
a sigma algebra that has just two elements and the full omega and the empty set.

339
00:48:37,510 --> 00:48:45,940
Right. And that's always part of the you. So you could consider this linearity as a consequence of the property I alluded to.

340
00:48:47,270 --> 00:48:52,880
They had random variables measurable with respect to you.

341
00:48:55,880 --> 00:49:11,860
Not conditional, measurable. Behave like constants under the expectation symbol.

342
00:49:12,670 --> 00:49:16,569
So we consider A and B as being such degenerates.

343
00:49:16,570 --> 00:49:23,650
When the variables that are certainly measurable with respect to you, then we just pull them out.

344
00:49:25,140 --> 00:49:28,890
Of the expectation that we can also do reform improve.

345
00:49:33,850 --> 00:49:37,540
So I'll say that's Apollo's way with the Tilde.

346
00:49:38,140 --> 00:49:42,220
Is there in the variable I'm trying to prove.

347
00:49:43,900 --> 00:49:47,830
It's equal to. Right. So that's a plus B times e.

348
00:49:49,140 --> 00:49:52,890
Of X giving you. Right? So that's the right part of it.

349
00:49:57,180 --> 00:50:06,410
Right. And I have notation. Uh, continue using this notation for the conditional expectation of x given to you.

350
00:50:06,430 --> 00:50:11,830
So that's just the Y. Okay so we'll start with.

351
00:50:13,090 --> 00:50:22,880
So these equal to zero. Right that all I have is expectation of a given you.

352
00:50:24,290 --> 00:50:27,380
And that's clearly a self.

353
00:50:30,760 --> 00:50:36,040
Because. Integral of ADP.

354
00:50:40,340 --> 00:50:47,460
He's keen to grow. A deep.

355
00:50:50,140 --> 00:51:03,750
All of it. That's trivially true. And the way I understand this trivial equality is that.

356
00:51:04,840 --> 00:51:07,729
Well, according to the definition. Right.

357
00:51:07,730 --> 00:51:17,840
So if expectation of any given you is the conditional expectation of a given you and its sequel to a then taking and then to grow over.

358
00:51:18,590 --> 00:51:22,670
So this is this a is my way now right when I'm looking in this expression.

359
00:51:23,780 --> 00:51:29,720
So this is a candidate for the Y that represents the conditional expectation on the left.

360
00:51:29,840 --> 00:51:38,749
Right. And so if I'm taking a conditional expectation of this, why I need to grow a why?

361
00:51:38,750 --> 00:51:44,300
DP That's the same thing as the integral over the original read the variable which is a.

362
00:51:47,690 --> 00:51:54,210
So that's just a definition. Of conditional expectation.

363
00:51:58,830 --> 00:52:11,420
So one of the is is. The Y equals expectation of p given u and A is.

364
00:52:12,790 --> 00:52:18,080
Yeah. This is the itself. And then.

365
00:52:18,700 --> 00:52:26,300
The definition of conditional expectation. Gives me this treaty of quality.

366
00:52:27,020 --> 00:52:36,810
And that in turn means that. Expected value of a constant, even if its conditional expectation is still that constant.

367
00:52:38,010 --> 00:52:46,910
So that was the trivial case. And the second one is when B is not equal to zero.

368
00:52:46,920 --> 00:52:56,730
So then I have read the variable x essential in the expression and define my y is.

369
00:52:57,800 --> 00:53:09,830
Why wait till the minus A or B, which is the same thing as to say that way with the tilde is a plus b times.

370
00:53:09,830 --> 00:53:17,910
Y where. So why is defined as usual of x given to you?

371
00:53:23,250 --> 00:53:29,920
Now. Uh, because subtracting something and dividing by something that's non-zero.

372
00:53:31,380 --> 00:53:40,200
Still keeps the random variable measurable with respect to the sigma algebra that that's being generated by that random variable.

373
00:53:40,200 --> 00:53:47,910
So I can see that both y, tilde and y are still part of the space.

374
00:53:49,110 --> 00:53:54,400
Yes, you. That space of random variables.

375
00:53:58,460 --> 00:54:10,090
Um, measurable with respect to you. Right.

376
00:54:10,120 --> 00:54:18,400
So. I can take an event from that sigma algebra and consider the integral.

377
00:54:21,350 --> 00:54:30,730
Of y tilde over that event threat because y deal is measurable with respect to sigma algebra you.

378
00:54:30,740 --> 00:54:39,680
So that's a legitimate construct for me to look at and it by definition is integral way to go over the set.

379
00:54:39,680 --> 00:54:49,520
E DP Now I know that integrals are linear, so I have the average of integral.

380
00:54:54,450 --> 00:55:08,640
And I recall what I tell these, right. So that that also goes into this and I can write it as a times probability of A was B times integrals.

381
00:55:10,450 --> 00:55:18,639
A wide. Because right now I'm not dealing with conditional expectations or anything.

382
00:55:18,640 --> 00:55:31,180
I'm just dealing with the laidback back integral that so many of the. Now this guy here he is, of course, the expectation of.

383
00:55:32,010 --> 00:55:35,280
X over the set e.

384
00:55:38,520 --> 00:55:48,050
By definition of the way. By definition.

385
00:55:50,670 --> 00:56:03,480
An additional expectation of X given you. And now I can write it.

386
00:56:04,140 --> 00:56:14,940
So when I replace this by integral of X of the same E, I can again use the linearity of integrals and the symbol ID back.

387
00:56:17,630 --> 00:56:24,110
And what? So again, the linearity of integrals.

388
00:56:29,670 --> 00:56:33,260
The girl. So assembling.

389
00:56:37,900 --> 00:56:53,560
All terms back under the integral. So you have an integral over the set a.

390
00:56:54,560 --> 00:56:57,560
A plus B, times x.

391
00:56:58,650 --> 00:57:12,890
DP. And this is by definition because I now have X instead of Y, right in this integral.

392
00:57:13,460 --> 00:57:21,710
So my Y is gonna and just this is where it shows up and I'm just rewriting this.

393
00:57:23,260 --> 00:57:26,850
It's by definition A plus B x.

394
00:57:30,660 --> 00:57:45,620
Over the set e. Right.

395
00:57:45,630 --> 00:57:50,550
And I'm going to look now at this part.

396
00:57:50,550 --> 00:58:02,250
And this part is a definition and the fact that A is any event that belongs to you.

397
00:58:03,950 --> 00:58:11,000
So that by definition of conditional expectation I will have that e plus B times.

398
00:58:12,590 --> 00:58:18,530
Expected value of tax revenue is the same as.

399
00:58:20,470 --> 00:58:24,700
Plus the X. Giving you.

400
00:58:31,830 --> 00:58:48,520
Right. Is this guy is way to the. It's just one deal, though.

401
00:58:55,180 --> 00:59:07,910
And this is what we intended to prove. So I look back, I just used the definitions and linearity of integral.

402
00:59:09,500 --> 00:59:15,280
So I unfolded the integral, then folded it back, having to use the definitions inside that.

403
00:59:17,270 --> 00:59:21,530
So not difficult. Okay, let's have a five minute break.

404
01:01:50,670 --> 01:02:04,250
So. Right.

405
01:02:15,070 --> 01:02:20,260
I'm trying to use a definition here that is given for conditional expectation.

406
01:02:22,240 --> 01:02:25,389
I'm pretending I don't know that expectation of a constant.

407
01:02:25,390 --> 01:02:29,890
This constant. Right. You can consider this as a proof of it.

408
01:02:30,940 --> 01:02:42,650
Right. But. I'm interpreting it in the sense of the second line, the definition of conditional expectation, which turns out to be a trivial equality.

409
01:03:00,200 --> 01:03:05,870
Right. So one of the is is is this guy right?

410
01:03:05,900 --> 01:03:11,870
Because. Right. And another of the is is is the right part of it of this.

411
01:03:14,070 --> 01:03:21,290
Right. And I'm looking at this trivial equality is essentially.

412
01:03:22,990 --> 01:03:28,120
B So this this plays the role of the definition.

413
01:03:28,810 --> 01:03:31,810
I've conditional expectation at this place that all of my.

414
01:03:51,240 --> 01:03:54,930
Right. That's that's eight times of a. Yeah. So why is.

415
01:04:02,930 --> 01:04:07,900
The. Well, this is not the same as.

416
01:04:08,470 --> 01:04:16,870
This is a times P.O.V. Right. But it's still true because of is this because this is just same expression on both sides, right?

417
01:04:33,540 --> 01:04:38,910
Okay. Shall we continue? There's another property.

418
01:04:39,240 --> 01:04:44,760
Another part of linearity is when I have to run the variables.

419
01:04:55,910 --> 01:05:07,520
Two random variables. Then if I take an expectation of X while still giving you.

420
01:05:09,730 --> 01:05:13,180
Then this is going to behave in a linear way.

421
01:05:15,490 --> 01:05:18,490
It's an expectation of it's why I'm giving you plus.

422
01:05:19,660 --> 01:05:22,090
Expectation of X to given the.

423
01:05:30,960 --> 01:05:41,190
And again, it's a combination of of properties of the growth linearity of the grow and definitions of conditional expectation.

424
01:05:42,120 --> 01:05:47,860
So I'll start with. So expected value.

425
01:05:48,760 --> 01:05:58,090
So let's say I have a usual notation. I have Y equals expectation of x I give you.

426
01:05:59,760 --> 01:06:05,110
I being one or two. Then.

427
01:06:05,740 --> 01:06:10,580
So. Then I'm taking.

428
01:06:11,720 --> 01:06:24,620
With everything going there this then I'm taking X, but the entire growth rate of y one plus y two over the area of a.

429
01:06:26,100 --> 01:06:31,270
And it is, as usual, any. That's part.

430
01:06:32,390 --> 01:06:39,280
Of the collection. You. And I can write it by linearity of integrals.

431
01:06:43,830 --> 01:06:50,430
As it's the linearity of conditional expectation that I am trying to prove, but the linearity of integrals is given to me.

432
01:06:50,430 --> 01:07:05,579
Right? I can use it. So whenever I have an integral so I can split it in a linear fashion between one over A and y.

433
01:07:05,580 --> 01:07:16,560
Two of a. And this is actually true for any way, one and way too, including the ones I just defined.

434
01:07:16,580 --> 01:07:23,290
Right. So then, uh, by definition.

435
01:07:23,590 --> 01:07:32,740
Um, so this is supposed to be equal to in the growth of x one over E.

436
01:07:33,970 --> 01:07:40,750
And this is supposed to be equal to the integral of it's two over E, right?

437
01:07:40,750 --> 01:07:48,640
And that's by definition. Of conditional expectation.

438
01:07:48,820 --> 01:07:56,450
So then I have that integral of y one plus y two over the area.

439
01:07:56,450 --> 01:07:59,470
He can be.

440
01:08:00,760 --> 01:08:10,100
So I assemble. These two terms right back.

441
01:08:12,090 --> 01:08:15,750
The. The same integral.

442
01:08:17,910 --> 01:08:25,250
By linearity of integrals. And this will give me.

443
01:08:27,550 --> 01:08:30,640
And then to go over x one plus x two.

444
01:08:33,640 --> 01:08:36,350
Over a. Right.

445
01:08:36,680 --> 01:08:49,370
And just like in the previous rules and now considering this equality of expectations as a definition for what is a conditional expectation of X,

446
01:08:49,370 --> 01:08:56,570
one plus X to give a new right and I see that Y one plus y two plays that role,

447
01:08:57,740 --> 01:09:01,730
in which case this line becomes a definition of conditional expectation.

448
01:09:03,210 --> 01:09:08,580
By definition of conditional expectation.

449
01:09:10,220 --> 01:09:14,090
That's 1.6 to give you.

450
01:09:15,200 --> 01:09:19,010
I have. That's this is why. One last way to.

451
01:09:43,010 --> 01:09:52,390
And this is what we intended to prove. We remember that this was defined as conditional expectation of X one given you.

452
01:09:53,030 --> 01:09:56,030
This was defined as conditional expectation of.

453
01:09:56,030 --> 01:10:17,249
X to. Okay.

454
01:10:17,250 --> 01:10:22,920
So yet another theorem. So I'm not only proving what I'm going to need in the subsequent.

455
01:10:24,020 --> 01:10:36,880
To establish subsequent facts. And the theorem sounds like the conditional expectation preserves.

456
01:10:38,230 --> 01:11:01,230
Ordering. So suppose I have x one and the x two is two and the variables.

457
01:11:03,250 --> 01:11:09,290
Uh, such that X1 is less of equal to almost surely.

458
01:11:10,010 --> 01:11:18,469
Now, in order to explain what all this truly means, I have to give a probability space.

459
01:11:18,470 --> 01:11:24,320
Right. So suppose I have someone be so it's then p almost surely.

460
01:11:27,530 --> 01:11:37,399
That means that X, Y or Farmiga is always less or equal x two or Farmiga with the exception of some sense of omegas that however,

461
01:11:37,400 --> 01:11:42,940
have a probability of zero. So then.

462
01:11:44,740 --> 01:11:52,150
The expected value of X one given you is going to have the same order as the.

463
01:11:56,170 --> 01:12:04,510
So the conditional expectations will be ordered as well if the random variables are ordered, almost truly.

464
01:12:08,240 --> 01:12:16,880
So as a proof. So as usual. So what is defined is conditional expectation of inside given you.

465
01:12:19,410 --> 01:12:29,850
I am taking in a. From that sigma algebra and then by definition of conditional expectations.

466
01:12:30,300 --> 01:12:37,050
So the integral of y i of the area e is going to be the same.

467
01:12:38,600 --> 01:12:43,250
Is the integral of. Its eye over the same area a.

468
01:12:46,650 --> 01:12:52,850
This is. Definition. Conditional expectation.

469
01:12:58,000 --> 01:13:02,290
Now I'm using properties of integrals.

470
01:13:12,130 --> 01:13:16,020
I will see that by the properties of integrals.

471
01:13:16,020 --> 01:13:25,440
Expect that the value of. Why one over a that's by definition expected to of X one.

472
01:13:27,140 --> 01:13:42,440
And to go over a it would be less equal because ax1 is less of equal x two and integrals are do not change.

473
01:13:42,460 --> 01:13:45,520
So that's your homework, one of your homework problems.

474
01:13:48,270 --> 01:13:55,330
To change. By a modification.

475
01:13:58,480 --> 01:14:03,420
Function. On the set of Measure zero.

476
01:14:11,580 --> 01:14:19,030
So this is going to be also equal expected value of X to integral of the area.

477
01:14:19,950 --> 01:14:23,220
And this is by definition same as.

478
01:14:25,470 --> 01:14:28,810
Way to go with a very.

479
01:14:30,730 --> 01:14:34,990
Right then by the year or two of the girls.

480
01:14:38,900 --> 01:14:46,310
I can write these as expects that the value of y two minus y y.

481
01:14:49,750 --> 01:15:00,580
There is great technical zero so assembling but the the difference between theory integrals all the area a.

482
01:15:04,170 --> 01:15:10,470
And this is supposed to be almost erroneous, just like the above expression.

483
01:15:12,780 --> 01:15:19,370
And because a. He is arbitrary.

484
01:15:24,010 --> 01:15:29,040
In the U. And.

485
01:15:30,770 --> 01:15:35,179
Why? One way to measure all of this.

486
01:15:35,180 --> 01:15:47,090
Back to you. I will have that y to minus y.

487
01:15:47,110 --> 01:15:50,640
One has to be great equals zero.

488
01:15:51,000 --> 01:16:01,569
Almost truly. As if this is not the case.

489
01:16:01,570 --> 01:16:05,610
Right, then I will focus on, uh, uh,

490
01:16:05,710 --> 01:16:16,210
on the set a where y two minus y one is less than zero and then the integral will be of the difference of that difference will be less than zero.

491
01:16:18,050 --> 01:16:27,290
So because of the arbitrarily chosen a uh, except perhaps for a that has a probability of a equal to zero.

492
01:16:28,670 --> 01:16:35,810
So yeah, from the expected value of grade equals zero, it actually follows that the function is non-negative as well.

493
01:16:37,980 --> 01:16:41,650
And this is what we. Intended to prove.

494
01:16:55,470 --> 01:16:59,240
And so this theory I needed to prove the next one.

495
01:16:59,240 --> 01:17:03,460
And the next one is a monotone convergence theorem.

496
01:17:13,550 --> 01:17:23,720
So conditional expectation. It sounds like.

497
01:17:23,740 --> 01:17:30,490
So suppose I have a sequence of random variables and that is the negative.

498
01:17:32,720 --> 01:17:37,220
And converging to some random variable x.

499
01:17:38,060 --> 01:17:41,210
Uh, well, p almost true. Right. For some p.

500
01:17:44,000 --> 01:17:50,899
They? Expected the value of its am given.

501
01:17:50,900 --> 01:18:00,830
You will converge in a monotonic fashion from below to the expected value of tax.

502
01:18:02,370 --> 01:18:13,800
When you. With proof of that fact.

503
01:18:14,880 --> 01:18:18,570
Um. I will do it.

504
01:18:18,570 --> 01:18:19,560
And the next lecture.

