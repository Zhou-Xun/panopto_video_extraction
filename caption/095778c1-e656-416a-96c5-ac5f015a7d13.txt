1
00:00:01,000 --> 00:00:06,000
Hello, welcome to part two. I've directed a cyclic graphs.

2
00:00:06,000 --> 00:00:14,000
We'll continue our discussion from last time and start with a review of confounding and selection bias on Dags.

3
00:00:14,000 --> 00:00:25,000
In this lecture, all lecture, we will look at the differences and similarities between confounding and selection bias.

4
00:00:25,000 --> 00:00:30,000
Here is a simple example of compounding Dag W.

5
00:00:30,000 --> 00:00:33,000
Mother's diabetes status is a common cause of a.

6
00:00:33,000 --> 00:00:41,000
And why? This opens up a backdoor statistical association between A and Y.

7
00:00:41,000 --> 00:00:57,000
And we do not have exchange ability in our study. And the reason we don't have exchange ability is because of the common cause of a NY.

8
00:00:57,000 --> 00:01:03,000
Here is a simple example of selection bias on a dagg.

9
00:01:03,000 --> 00:01:10,000
W. Mother's diabetes status is a common effect of a variable associated with the exposure.

10
00:01:10,000 --> 00:01:21,000
Low income and the outcome. Diabetes. If we control for that common effect, we open a back door path between A and Y.

11
00:01:21,000 --> 00:01:31,000
In this example, we also lack exchange ability. But for a different reason than the previous example.

12
00:01:31,000 --> 00:01:38,000
This is an example of both selection bias and confounding being present on IDAG.

13
00:01:38,000 --> 00:01:44,000
W is a common cause or a confounder between A and Y.

14
00:01:44,000 --> 00:01:58,000
It is also a collider. Shared by a variable that is associated with the exposure and with the outcome.

15
00:01:58,000 --> 00:02:03,000
What are some commonalities between confounding and selection bias?

16
00:02:03,000 --> 00:02:10,000
Both are problematic because both imply an open back door path from A to Y.

17
00:02:10,000 --> 00:02:13,000
Hence, in the presence of either or both of these phenomena,

18
00:02:13,000 --> 00:02:20,000
we do not have exchange ability and we cannot measure the causal association between A and Y.

19
00:02:20,000 --> 00:02:26,000
Both also imply the need for modification to a dagg.

20
00:02:26,000 --> 00:02:32,000
In order to achieve exchange ability, we can do this by either statistical adjustment,

21
00:02:32,000 --> 00:02:37,000
restriction of the study population or randomization of treatment.

22
00:02:37,000 --> 00:02:44,000
Remember, randomization is not always feasible or ethical.

23
00:02:44,000 --> 00:02:53,000
What are the differences between confounding and selection bias, fundamentally confounding and selection bias arise from different structural causes?

24
00:02:53,000 --> 00:02:59,000
Confounding is due to a common cause of the exposure and the outcome.

25
00:02:59,000 --> 00:03:04,000
And selection bias is due to conditioning on a collider.

26
00:03:04,000 --> 00:03:17,000
That is a result of the exposure or a variable X associated with exposure or the outcome and a variable associated with the outcome.

27
00:03:17,000 --> 00:03:32,000
Here's a simple example of selection bias here, selection into the study is a common effect that the exposure and the outcome.

28
00:03:32,000 --> 00:03:39,000
This is also a depiction of selection bias, but here censoring status.

29
00:03:39,000 --> 00:03:49,000
Which induces selection bias is a common effect of a variable associated with the exposure and a variable associated with the outcome.

30
00:03:49,000 --> 00:04:04,000
This is also considered selection bias. Here is yet a third example of selection bias whereby censoring status is a direct descendant of exposure.

31
00:04:04,000 --> 00:04:16,000
And a descendant of a variable that is. A cause of both censoring status and the outcome.

32
00:04:16,000 --> 00:04:23,000
So in summary, confounding and selection bias arise from different structural associations,

33
00:04:23,000 --> 00:04:31,000
selection bias arises because of investigator induced selection, uncommon effects and confounding arises because of common causes.

34
00:04:31,000 --> 00:04:38,000
Between A and why of A and why rather.

35
00:04:38,000 --> 00:04:46,000
Next, we'll look at the new topic of information bias on DACS. We learned about information bias in her previous lecture.

36
00:04:46,000 --> 00:04:56,000
These slides will teach us how to represent and understand slipping information bias on dags.

37
00:04:56,000 --> 00:05:03,000
Information bias and its impact on causality. So information bias, sometimes known as measurement bias,

38
00:05:03,000 --> 00:05:12,000
occurs when the association between A and Y is weaken or strengthen as a result of how variables in the study are measured.

39
00:05:12,000 --> 00:05:21,000
This is sometimes also known as measurement error. Information bias can occur in all settings, even in randomized controlled trials.

40
00:05:21,000 --> 00:05:33,000
So it's important to keep that in mind and understand that even RC T's are susceptible to information bias.

41
00:05:33,000 --> 00:05:39,000
Having exchange ability, so controlling for either selection bias or confounding in the presence of

42
00:05:39,000 --> 00:05:44,000
information bias isn't enough to measure the causal association between A and why.

43
00:05:44,000 --> 00:05:48,000
So even if you have exchange ability and even if you randomize,

44
00:05:48,000 --> 00:05:56,000
you may still end up with mis measured variables and hence systematic bias in your study.

45
00:05:56,000 --> 00:06:01,000
Here, we're going to introduce the concept of information bias on Dag's,

46
00:06:01,000 --> 00:06:12,000
which allows us to identify and understand how information bias can affect our estimates in our studies.

47
00:06:12,000 --> 00:06:19,000
So let's say we want to study the effect of cholesterol lowering drugs on the risk of liver disease.

48
00:06:19,000 --> 00:06:22,000
Why do we know that A can be mis measured?

49
00:06:22,000 --> 00:06:31,000
Because some patients may not take the drug or the doctor and may forget to write down that the patient was prescribed the drug.

50
00:06:31,000 --> 00:06:43,000
So we do not have perfect measurement of the exposure status. We represent the true exposure of interest on a Dagg as a and the measured exposure.

51
00:06:43,000 --> 00:06:49,000
So the data we will actually use in our study as a star.

52
00:06:49,000 --> 00:06:58,000
Which will not keep in mind, necessarily equal A because there is some measurement error.

53
00:06:58,000 --> 00:07:02,000
Here's an example of how we might represent that.

54
00:07:02,000 --> 00:07:12,000
You a is some type of measurement error which causes a discrepancy between a star and the true exposure value.

55
00:07:12,000 --> 00:07:19,000
A. The true treatment affects liver disease and measured drub.

56
00:07:19,000 --> 00:07:31,000
You a represents all the factors other than A which determine the value of drug use as measured in the study.

57
00:07:31,000 --> 00:07:43,000
Sometimes in psychology. We think of the a variable here, so the true exposure status, people call that in psychology sometimes a construct,

58
00:07:43,000 --> 00:07:47,000
and we develop instruments to measure those underlying constructs.

59
00:07:47,000 --> 00:07:55,000
The challenge is to use data on the measured indicator, a star to derive inference on the effect of an underlying construct.

60
00:07:55,000 --> 00:08:06,000
In this case, the true drug status on the outcome in question, in this case, true liver disease.

61
00:08:06,000 --> 00:08:14,000
Very often in a study. It won't just be either the exposure or the outcome that is mis measured.

62
00:08:14,000 --> 00:08:22,000
Often there is measurement error in both the measurement of the outcome and the exposure.

63
00:08:22,000 --> 00:08:28,000
So here we're interested in, again, the effect of a on why drug use on liver disease.

64
00:08:28,000 --> 00:08:36,000
But we only have a star and why star measurements of the underlying constructs of the exposure and the outcome.

65
00:08:36,000 --> 00:08:42,000
Because of measurement error, there are no guarantees that a star is equal to a.

66
00:08:42,000 --> 00:08:51,000
And that Y star is equal to Y. So that we are using appropriate measures.

67
00:08:51,000 --> 00:08:56,000
Let's consider the causal risk difference, which is.

68
00:08:56,000 --> 00:09:08,000
Why, if everyone had been exposed equal to one minus the probability that the event occurs at everyone with unexposed.

69
00:09:08,000 --> 00:09:16,000
But what we're actually measuring our studies are the conditional probabilities that Y prime equals one, given that a star equals one.

70
00:09:16,000 --> 00:09:24,000
I'm sorry. Y star equals one. Given that a star equals one minus the probability that Y star equals one.

71
00:09:24,000 --> 00:09:35,000
Given that a star equals zero. So the difference between the treated and untreated in our actual study.

72
00:09:35,000 --> 00:09:46,000
Since a start doesn't equal a and why start doesn't necessarily equal why the causal risk difference is not the association or risk difference.

73
00:09:46,000 --> 00:09:56,000
Generally speaking, this will indicate that our measure of association is not equal or causal measure of association or our causal measure,

74
00:09:56,000 --> 00:10:03,000
the presence of information, bias, exchange ability, the presence of information, but in the presence of information,

75
00:10:03,000 --> 00:10:16,000
bias, exchange ability is is not sufficient for identification of causal effects.

76
00:10:16,000 --> 00:10:24,000
In the lecture, we're going to review the concept of information bias on Dags and look at some more extreme examples

77
00:10:24,000 --> 00:10:31,000
and understand how information bias can be both differential and independent or dependent.

78
00:10:31,000 --> 00:10:45,000
We'll also go through some examples of identifying adjustments, sets on DAGS and put these concepts into practice a little bit.

79
00:10:45,000 --> 00:10:53,995
So I look forward to speaking with you on Thursday.

