1
00:00:09,140 --> 00:00:14,930
Okay. Let's get started. So.

2
00:00:22,650 --> 00:00:26,190
So we went through and ultimately made the algorithm.

3
00:00:28,780 --> 00:00:37,940
In the last lecture. So, uh, so there, there was a question.

4
00:00:38,210 --> 00:00:42,740
So there was a lot of question about how this survey really works.

5
00:00:43,560 --> 00:00:47,660
Uh, I explained a little bit faster, so.

6
00:00:47,660 --> 00:00:51,840
And I decided what you mean is, is not intuitive to understand necessarily.

7
00:00:51,860 --> 00:00:55,969
So I understand all these questions.

8
00:00:55,970 --> 00:01:07,730
So today I wanted to go where they started with them a little bit, you know, in detail and try to, uh, have you on the same page for Flynn.

9
00:01:09,500 --> 00:01:16,820
So the first question I had is that so these are good items.

10
00:01:18,110 --> 00:01:30,770
If you, if you actually look at the code, uh, the server region is maintaining three points.

11
00:01:31,460 --> 00:01:38,960
Okay. So does it mean that it this or this mm.

12
00:01:38,960 --> 00:01:50,090
Algorithm only works for two dimension because in two dimension I say that three three pointer can't keep being tracked in three dimension.

13
00:01:50,510 --> 00:01:53,570
There are four points that are being tracked as you see here.

14
00:01:54,410 --> 00:01:59,030
So, uh, so that, that is the first question that the answer is no.

15
00:01:59,480 --> 00:02:05,660
So no matter how many dimension you have, these are the three points you need to keep track of.

16
00:02:05,930 --> 00:02:10,760
Okay. But that doesn't mean that other points are not used.

17
00:02:11,010 --> 00:02:14,840
Okay. That's a difference. So these are just the three points you need to remember.

18
00:02:15,350 --> 00:02:18,980
Okay. Three points you need to remember to.

19
00:02:19,070 --> 00:02:22,970
To update to the next step. Okay. What does that mean?

20
00:02:23,000 --> 00:02:36,650
Okay. So, uh, so if you if you look at this algorithm there, so basically there are two important things that that's happening here.

21
00:02:37,400 --> 00:02:41,330
One is that we need to find this line, right?

22
00:02:41,810 --> 00:02:49,280
If you find this line, this line is defined as, uh, as a midpoint.

23
00:02:51,110 --> 00:02:58,010
Midpoint meaning average or average of all across all the points, excluding the worst point.

24
00:02:58,790 --> 00:03:08,480
Here is the worst point. So you're not including worst point and then try to find the average of all the points except for the worst point.

25
00:03:09,320 --> 00:03:14,720
Okay. And in the two dimensional cases, it happens to be just average of these two.

26
00:03:14,840 --> 00:03:21,230
Okay. In three dimensional case, this is a you have this shaded area.

27
00:03:21,440 --> 00:03:26,360
Okay. And this this one is the worst point. This shaded area is the.

28
00:03:27,050 --> 00:03:30,200
Those are those contain those three points. There needs to be average.

29
00:03:30,500 --> 00:03:42,320
So midpoint is the middle of this. Okay. So then what we are doing is the draw a line from the highest point or worst point to the midpoint.

30
00:03:42,500 --> 00:03:48,460
Okay. So this is the this is just the two to determine the axes that I want you to optimize.

31
00:03:48,530 --> 00:03:53,960
Okay. So I want you to I want you to try a new point along this axis.

32
00:03:54,170 --> 00:03:57,700
Okay. And what the my strategy is there.

33
00:03:57,980 --> 00:04:00,980
Oh, I'm going to I'm going to reflect this.

34
00:04:01,640 --> 00:04:05,690
Okay. I'm going to find the reflection point, because this is the worst point.

35
00:04:05,700 --> 00:04:09,320
So there's a good, good, good chance that this is a close to the best point.

36
00:04:10,250 --> 00:04:14,399
So you try this. Okay. And in three dimensions.

37
00:04:14,400 --> 00:04:18,590
Same. You try the reflection point. Okay. So when.

38
00:04:18,710 --> 00:04:23,000
When you try the reflection. Okay, then then evaluate that.

39
00:04:23,480 --> 00:04:26,570
And sometimes. Oh, I made a very good decision.

40
00:04:26,960 --> 00:04:32,660
Okay, then how do I know if this becomes the best point?

41
00:04:32,900 --> 00:04:37,970
Okay. If so, this was the worst point that this is the average of of all the other point.

42
00:04:38,330 --> 00:04:47,570
And if this happened to be the best point, this best point, meaning that the function value is smaller than anything else I have observed.

43
00:04:48,440 --> 00:04:52,100
In that case, I made a successful attempt. So I'm going to expand.

44
00:04:52,430 --> 00:05:01,970
I'm going to try even more and see if it makes a better if it makes even better, then I'm going to make this in my new trial point.

45
00:05:02,540 --> 00:05:08,480
If not, I'm going to fall back to just four back to this point.

46
00:05:08,900 --> 00:05:15,920
So that's the. So we we we calculate the midpoint and try the reflection first.

47
00:05:16,190 --> 00:05:20,630
And if the reflection is very successful, then you try the exposure.

48
00:05:21,830 --> 00:05:30,170
If reflection was total failure, meaning that this point is worse than any of these point.

49
00:05:31,340 --> 00:05:35,520
Okay, I. If I reflect, this could be better.

50
00:05:35,520 --> 00:05:41,459
But sometimes the phones are maybe, you know, this is actually always already close to optimum.

51
00:05:41,460 --> 00:05:50,610
So the mean optimum is somewhere in in we side in that case that if you go outside of this pyramid it could the function value will increase.

52
00:05:50,610 --> 00:05:54,030
So in this case, oh, I made a mistake.

53
00:05:54,030 --> 00:05:58,319
I shouldn't have tried it. So I'm going to go back to the original simplex.

54
00:05:58,320 --> 00:06:04,320
But there is no point that we doing this. But so I'm going to I'm going to try somewhere in the middle.

55
00:06:04,320 --> 00:06:07,139
So there is a meta point. There is high point.

56
00:06:07,140 --> 00:06:20,060
So I'm going to try somewhere in the middle making the making my pyramid in a in this a smaller is smaller pyramid compared to the original.

57
00:06:20,370 --> 00:06:28,409
Okay. So that's contraction. Okay. So when you do this contraction, it's possible that is still not good.

58
00:06:28,410 --> 00:06:31,500
So nothing, nothing improves it. Okay.

59
00:06:31,650 --> 00:06:40,320
So this is a still what's the point in the case what you're doing is that oh, I maybe I found this axis was wrong.

60
00:06:40,320 --> 00:06:44,130
I, I thought that this was a good axis to find a neutral point.

61
00:06:44,250 --> 00:06:48,989
I shouldn't have tried. And you tried the axis here, so because likely.

62
00:06:48,990 --> 00:06:52,230
Because maybe. So this is this is a best point here.

63
00:06:53,820 --> 00:07:00,879
So that so maybe maybe this is a my my current.

64
00:07:00,880 --> 00:07:07,770
The simplex is too big. So I want I need to search from the smaller part because my step size is too big.

65
00:07:07,770 --> 00:07:10,440
So no matter where, where it goes, it doesn't work.

66
00:07:10,440 --> 00:07:22,110
So because this was the best point, I'm going to shrink my pyramid or simplex into the smaller, so I'm going to make every side as half.

67
00:07:22,560 --> 00:07:25,980
So that's a multiple contraction. I'm going to restart my hybrid.

68
00:07:26,310 --> 00:07:30,210
Okay. So that is the idea of the another method.

69
00:07:31,470 --> 00:07:37,110
Okay. So to be able to do that, the reason why you need to keep track of three points is this.

70
00:07:37,740 --> 00:07:43,260
So to calculate the, you know, you need to calculate this a plane.

71
00:07:43,260 --> 00:07:47,040
So you definitely need to keep track of worst point, right? You agree with that.

72
00:07:47,630 --> 00:07:55,920
Good. And to be able to do the multiple contraction later, you need to know where is the best point because when you do the multiple contraction,

73
00:07:56,460 --> 00:08:01,860
you know, you should know where the best point is and you're shooting thing every other side for the best point.

74
00:08:02,130 --> 00:08:04,620
In this case, the low means that this is the best point to me.

75
00:08:04,620 --> 00:08:15,659
So, so so to to do that you need to know the best point and for the reason why you need to know.

76
00:08:15,660 --> 00:08:26,760
The second best point is that when you evaluate the reflection, okay, I'm going to check whether this is the worst point or not or but it doesn't.

77
00:08:27,570 --> 00:08:31,110
So you already this is the worst point where you did a reflection.

78
00:08:31,380 --> 00:08:35,280
So this is no longer worst point that you are still you keep track of the simplex.

79
00:08:35,280 --> 00:08:44,069
Now your simplex has been updated. So the worst point here is the second worst point in the previous one, because this was the worst point.

80
00:08:44,070 --> 00:08:48,180
And I'm going to check whether this is the worst point in my simplex.

81
00:08:49,110 --> 00:08:55,350
So then you need to compare with the second worst point and the oh is my value is worse than the second worst.

82
00:08:55,570 --> 00:09:01,440
That makes this a this is the worst point. So that's why you need to keep those to keep track of the second worst point.

83
00:09:02,520 --> 00:09:08,040
So that's that's as far as I can do in terms of the explaining in concept.

84
00:09:08,580 --> 00:09:17,459
And let me go through the code again and let me try to see whether I can explain a little bit better

85
00:09:17,460 --> 00:09:25,530
because the most likely you're you're probably not going to implement another middle wisdom from scratch.

86
00:09:25,530 --> 00:09:33,720
Again, there is a very little case like that, but sometimes you need to know why this failed to converge.

87
00:09:34,230 --> 00:09:37,680
Then if you especially when you use another media algorithm.

88
00:09:38,160 --> 00:09:44,520
So it is of good practice that oh, where these values are evaluated in word where the simplex might be,

89
00:09:45,090 --> 00:09:50,280
and those are stuff I wanted to or need to you to understand.

90
00:09:51,600 --> 00:09:57,750
So okay. So this first part is set up.

91
00:09:58,110 --> 00:10:06,660
So I'm making a simple x one way to make a simple what is the easiest way to make a simple x in an arbitrary dimension?

92
00:10:06,780 --> 00:10:11,609
So let's say let's think about three dimension. Easiest way to make a simple as that.

93
00:10:11,610 --> 00:10:16,290
I wanted to start from here and just add one and one and one.

94
00:10:17,190 --> 00:10:21,050
Then you can make a pyramid like this, right? So this is a pyramid, right?

95
00:10:21,720 --> 00:10:31,950
So then, you know, making four points by adding one in each and each axis is a11 easiest way to make an arbitrary pyramid.

96
00:10:32,280 --> 00:10:35,580
So that's exactly what this was doing. Okay.

97
00:10:36,060 --> 00:10:47,070
Well, what this is doing is that I'm making a matrix with the so so this is the dimensional matrix.

98
00:10:47,070 --> 00:10:57,410
Each column has each each column has a d, d rows and I'm making d plus one columns because I'm,

99
00:10:57,500 --> 00:11:05,820
I'm keeping track of D plus one, then a point and I'm going to copy the same value all over here.

100
00:11:06,480 --> 00:11:17,440
Okay, first. So I'm going. So really I am going to duplicate all value here.

101
00:11:18,940 --> 00:11:22,200
Uh, zero. So start from zero.

102
00:11:22,770 --> 00:11:32,580
And after that, what you're doing is that I'm going to. This means that except for the very last one I'm going to exclude is all the other ones.

103
00:11:33,420 --> 00:11:37,460
I'm going to add a diagonal. Right.

104
00:11:39,760 --> 00:11:46,330
Here. Good time. If you add a diagonal, basically you're making encouraging one for each of the dimensions.

105
00:11:46,810 --> 00:11:53,200
So you're basically making this okay. So just using the fancy way to make a instead of using loop,

106
00:11:54,190 --> 00:11:58,870
this looks a little fancier and probably a little more efficient that that's why we're doing this.

107
00:11:58,900 --> 00:12:05,050
Okay. So after making this simplex, you evaluate this function.

108
00:12:05,170 --> 00:12:13,479
So just I'm going to use a apply function. Apply function is basically what is thought, what is if you don't know how to use it,

109
00:12:13,480 --> 00:12:18,400
apply function is basically apply some same function in each row or each column.

110
00:12:19,030 --> 00:12:27,670
In this case, apply function. You need to give a matrix and if this value is a one, then I'm going to apply this function for each of the row.

111
00:12:28,180 --> 00:12:32,739
If these values are two, I'm going to apply all the function, each of the columns.

112
00:12:32,740 --> 00:12:44,950
So it's a pretty useful function to use the matrix to, to, to run some function that that takes the vector as a inputs.

113
00:12:44,950 --> 00:12:51,700
So F F takes the each of the column as an input and you can provide extra parameter if you want.

114
00:12:52,720 --> 00:12:59,740
So then now y y you will have the function that the y y is a d plus one dimensional

115
00:12:59,740 --> 00:13:10,730
value that the f of f of the you want x1xx1f of x two and f of x plus one.

116
00:13:10,750 --> 00:13:14,350
Right. So that that value will be stored there.

117
00:13:16,450 --> 00:13:26,470
Okay. So, so then, okay, so, so we are, we're keeping track of all variants.

118
00:13:26,500 --> 00:13:32,530
We're not just keeping track of three variants. Three, three values are all points.

119
00:13:32,530 --> 00:13:44,790
I work say variants. We get all the points. Uh, yeah, but yeah, but we're, we're, we're treating three points as a special points.

120
00:13:45,630 --> 00:13:51,060
So three points is a, the worst point. Best point and the second worst point.

121
00:13:51,390 --> 00:13:55,350
We're going to we're going to just remember which one is the worst point?

122
00:13:55,350 --> 00:13:58,440
Which one is best point? Which one is the converse point? Okay.

123
00:13:59,700 --> 00:14:03,210
And at this midpoint, the through line is the is the.

124
00:14:05,600 --> 00:14:14,690
The main point is of it the the average of the all all values except for the worst point.

125
00:14:14,930 --> 00:14:19,370
Okay. And total line is the direction. So I'm going to go to the there.

126
00:14:19,730 --> 00:14:25,280
Okay. So this function looks complicated, but this function is actually very simple.

127
00:14:25,670 --> 00:14:29,840
What I'm going to do is I'm going to evaluate all the function values.

128
00:14:30,330 --> 00:14:35,910
Okay. And I'm going to. Yeah.

129
00:14:36,130 --> 00:14:45,190
I'm going to evaluate everything and I'm going to update, uh, which one is, uh, which one is the worst point?

130
00:14:45,220 --> 00:14:49,960
Which one is the second worst point or which one is the best point?

131
00:14:50,020 --> 00:14:55,629
So that, that's what this is doing. So, so if you read this routine, it's a little bit simple.

132
00:14:55,630 --> 00:15:00,490
It's just a you need to remember that you need to use a global assignment.

133
00:15:00,490 --> 00:15:06,180
Otherwise the store, the value will be only stay valid within the function.

134
00:15:06,220 --> 00:15:10,240
When the function ends, the value disappears.

135
00:15:10,720 --> 00:15:18,520
So I don't recommend using global variable a lot, but sometimes you just need to use the otherwise the the algorithm becomes very, very complicated.

136
00:15:18,820 --> 00:15:23,520
Okay. So now I want to explain this part.

137
00:15:23,910 --> 00:15:30,230
Okay. So so midpoint is basically.

138
00:15:31,370 --> 00:15:40,490
So let's say let's say there's a put you have a period could be the point is just the this value right so you.

139
00:15:42,860 --> 00:15:47,660
What the what this has done is that I'm going to exclude the worst point.

140
00:15:48,240 --> 00:15:53,030
Okay. There are we call false make it you know, make it still stay as matrix.

141
00:15:53,030 --> 00:15:58,820
Sometimes if you exclude the one one column on one row, you can make a vector.

142
00:15:59,240 --> 00:16:02,300
If there's only one one node after that, that's what it does.

143
00:16:03,440 --> 00:16:09,229
So you you take this matrix and for each of the row you are taking the mean.

144
00:16:09,230 --> 00:16:18,620
Because if you take the mean of each mean for each of the each of the row for 311 points, that will make an average right wage of dimension.

145
00:16:20,570 --> 00:16:24,380
And through line is just the subtraction of these two.

146
00:16:24,850 --> 00:16:35,750
Okay. So, so through line is basically the vector that because you're subtracting the midpoint to the line is representing this vector.

147
00:16:35,870 --> 00:16:45,170
So this is the worst point, you know, in a from the this is a meta point and it it just represent this vector.

148
00:16:45,500 --> 00:16:53,990
Okay. So what? So for example, so because of the, let's say, exit, you know, is a what?

149
00:16:54,710 --> 00:17:08,390
Well, that's the you know, it's the worst point. And that's I mean, so midpoint and let's say this color line, let's say the slope is then.

150
00:17:08,600 --> 00:17:13,700
Okay. So let's say the slope as something else.

151
00:17:16,630 --> 00:17:21,940
Hislop has been blessed to be the through line.

152
00:17:23,770 --> 00:17:31,930
So this is better. So because I defined a B as x, you know, minus x m okay.

153
00:17:32,740 --> 00:17:36,130
So exit zero is x and plus be.

154
00:17:38,320 --> 00:17:43,270
Agreed. Right. So if I want to reflect on something here.

155
00:17:44,780 --> 00:17:52,930
Okay. So let's say this is an X, R and R is X and minus B, agreed.

156
00:17:55,140 --> 00:17:58,680
Okay. If I want to do the expansion.

157
00:17:59,280 --> 00:18:05,240
Okay. X are. That's our our start.

158
00:18:05,390 --> 00:18:08,480
Okay. Do the. It's better than I need to go twice.

159
00:18:09,110 --> 00:18:16,400
So x our star is X and minus B times two to be agreed.

160
00:18:18,410 --> 00:18:22,100
If I want to do the contraction here, I want to find something.

161
00:18:22,100 --> 00:18:30,410
Ecstasy. Okay. Ecstasy is X and plus 0.5 B.

162
00:18:32,510 --> 00:18:36,450
Okay. Because it's something in between. Okay.

163
00:18:37,390 --> 00:18:44,860
So did this. They say this is the equation you need to remember to understand this.

164
00:18:45,700 --> 00:18:54,010
Okay. This update next point is basically what is trying to do is I'm going to replace my worst point.

165
00:18:54,580 --> 00:19:00,290
Okay. So. So what is this is the next point, right?

166
00:19:00,320 --> 00:19:06,740
Next point is that I'm doing the sum scale. The step scale could be negative one 0.5.

167
00:19:06,750 --> 00:19:18,320
Negative two. It can be it can be any value. I'm going to evaluate that based on some sum, some coefficient along the line and evaluate this function.

168
00:19:19,100 --> 00:19:22,340
And if this function is indeed a bit better than the.

169
00:19:23,540 --> 00:19:29,750
So if this is a better than the worst point. Okay, I am going to update.

170
00:19:30,170 --> 00:19:35,180
Okay. Update to my new point as a as a next point.

171
00:19:35,660 --> 00:19:44,600
Okay. And I'm going to also update that to my worst point is this one, it doesn't have to be necessarily the worst point,

172
00:19:44,600 --> 00:19:50,000
but I'm just replacing the worst point because I found a better operating than the worst point.

173
00:19:50,090 --> 00:19:54,080
Right. And I'm going to return. True. Oh, I improved it.

174
00:19:54,440 --> 00:19:57,710
We had returned to the image that I updated next point.

175
00:19:58,430 --> 00:20:01,880
So my worst point has been updated that that's what it means.

176
00:20:03,740 --> 00:20:12,950
But when I try this new point and if my new value is actually not improving the my simplex, then I'm going to say just return false.

177
00:20:13,910 --> 00:20:19,340
I'm not going to update it because my new point is worse, so I'm not going to update.

178
00:20:19,580 --> 00:20:22,910
Okay, so that's what this update next point does.

179
00:20:23,480 --> 00:20:29,720
Okay then. So now I need to explain this part.

180
00:20:30,020 --> 00:20:35,780
Okay. So in each step we're evaluating of all function values.

181
00:20:36,020 --> 00:20:40,060
Okay. And. And.

182
00:20:40,960 --> 00:20:44,020
Okay, so. And we.

183
00:20:45,910 --> 00:20:49,510
So. Okay, so we calculate. Okay.

184
00:20:50,360 --> 00:21:03,969
Uh. So we calculate the relative errors here, relative errors,

185
00:21:03,970 --> 00:21:08,110
and see whether relative errors are less than the tolerance and say that's a conversion

186
00:21:08,110 --> 00:21:17,290
converted and if it's converted and the exit otherwise or you update the midpoint.

187
00:21:17,800 --> 00:21:25,470
Okay. And. Uh, update the midpoint.

188
00:21:25,860 --> 00:21:30,750
And after that, what it does is that we're doing the reflection.

189
00:21:31,020 --> 00:21:38,130
Reflection is basically when I do this, I put my negative one in a put the step scale is a negative one.

190
00:21:38,610 --> 00:21:49,530
That's why there's a negative one is here. And then then my, you know, I might have updated my worst point or not,

191
00:21:50,160 --> 00:21:55,800
but if my worst point has been updated, this could be better than the best point.

192
00:21:55,860 --> 00:22:05,250
Does it make sense? So what? Because when I when I updated it up, the next point I just replaced the worst, pointed to something else.

193
00:22:05,610 --> 00:22:08,669
But I don't check whether this is the best point or worst point or what.

194
00:22:08,670 --> 00:22:17,640
I just updated the worst point to something else. So that value could be better than the better than the worst point.

195
00:22:18,160 --> 00:22:28,770
It's better than the best point. And that means that this was a hugely successful OC then oh, I'm going to expand it good because I was successful.

196
00:22:28,770 --> 00:22:35,220
So if the expansion is successful, I'm going to the the worst point could be updated again.

197
00:22:35,430 --> 00:22:39,509
Otherwise just worst point there will stay still still the same.

198
00:22:39,510 --> 00:22:43,140
But the reflect the point the will is all already better.

199
00:22:43,590 --> 00:22:51,990
So I don't have any complaints. And in the in the other case I updated my worst point.

200
00:22:52,680 --> 00:23:00,750
But I mean, my worst point is still worse than the second worst point, meaning that reflection wasn't successful at all.

201
00:23:01,620 --> 00:23:07,499
That means that I need to do something else. So I'm going to do the one dimensional contraction.

202
00:23:07,500 --> 00:23:11,160
So I'm going to try this this point. Okay.

203
00:23:11,910 --> 00:23:20,729
And there is a there's an exclamation mark here, which means that if this returned.

204
00:23:20,730 --> 00:23:23,990
False. What does it mean if you return the falls?

205
00:23:24,560 --> 00:23:28,700
There means I try this point. This point is still worse than the worst point.

206
00:23:29,270 --> 00:23:32,490
Okay, so my worst point is still stay the same.

207
00:23:32,510 --> 00:23:36,170
This influx hasn't changed. So then I'm stuck.

208
00:23:36,590 --> 00:23:39,830
I need you to do something. What do I do? I do multiple contracts.

209
00:23:42,260 --> 00:23:46,700
I do multiple contraction. And how do you do the multiple contraction here?

210
00:23:47,690 --> 00:23:49,880
I exclude the best point.

211
00:23:50,240 --> 00:23:57,500
And except for all, except for the best point, I'm going to take the average between the best point and all the other points.

212
00:23:57,740 --> 00:24:00,740
Okay. That's what it does. Okay. Okay.

213
00:24:00,860 --> 00:24:04,580
And I'm going to reevaluate all the all the all the function value and why.

214
00:24:08,160 --> 00:24:12,180
Okay. So that's the down the middle.

215
00:24:13,020 --> 00:24:16,889
Is that a little more clear? Still. Still very confusing.

216
00:24:16,890 --> 00:24:27,660
Right. So, again, you know, this is not a beautiful algorithm if mathematically saying this is a very practical algorithm.

217
00:24:27,960 --> 00:24:37,110
So that's very important. So, you know, I know that you guys are really good at math and that, you know, the beauty, beauty of the math.

218
00:24:37,110 --> 00:24:44,040
But even you would work with the real data. Sometimes you just need to need some compromise.

219
00:24:44,310 --> 00:24:51,730
Okay. Sometimes. And when you when you have a compromise, you can have a really ad hoc compromise.

220
00:24:52,080 --> 00:25:00,310
So, I mean, juristic that works only in this particular case, whereas its simplicity is actually robust, pretty much all different cases.

221
00:25:00,330 --> 00:25:07,230
So even if it's not mathematically beautiful, heuristics, you know,

222
00:25:07,320 --> 00:25:15,230
try to make it as general as possible as, you know, it just don't focus on your specific use case.

223
00:25:15,240 --> 00:25:18,000
Oh, this is going to be boring. I'll give you this one time use.

224
00:25:18,450 --> 00:25:29,650
Well, you know, although lot cases, it's not when you find a very good, you know, framework to, you know, update your parameter.

225
00:25:29,670 --> 00:25:32,370
What do some optimization that is that becomes very, very helpful.

226
00:25:33,540 --> 00:25:42,479
So if you if you are following some of the machine learning or deep learning method, some kind of idea is is coming out of nowhere.

227
00:25:42,480 --> 00:25:50,380
Like I don't know if you know this things called transform or not in a natural language processing, you know,

228
00:25:50,430 --> 00:25:58,229
that has something called self attention if you know and that that is that is very that looks

229
00:25:58,230 --> 00:26:05,490
like heuristic but it actually learns the structure in the data in a very interesting way.

230
00:26:05,670 --> 00:26:10,979
And until you try and it works in then it change the world of the natural language

231
00:26:10,980 --> 00:26:15,480
processing into the next level so that if you look at if you look at the paper,

232
00:26:16,110 --> 00:26:22,020
that paper was like four years old in a society of 50,000 times, I think.

233
00:26:22,110 --> 00:26:24,900
Right. So it's a it's a really, really good paper.

234
00:26:25,770 --> 00:26:35,680
And, you know, it's not you know, that update rule is a specific, specific algorithm and it looks like heuristic, but it's very general, actually.

235
00:26:36,330 --> 00:26:40,650
So that works at least for natural language processing works really well.

236
00:26:41,790 --> 00:26:52,679
So yeah, that, that, that thing helps in this another meta algorithm although this is doesn't you may not appreciate this is a really good algorithm,

237
00:26:52,680 --> 00:27:04,010
but I think it's really intuitive. So the good algorithm is it is intuitive to understand like this like this algorithm and pretty generalizable.

238
00:27:04,090 --> 00:27:05,370
Yeah right.

239
00:27:05,400 --> 00:27:17,070
Kind of math worth and 7.500 hours of work that you would have thought why do I why don't I use sort of you can you can do searching every time.

240
00:27:17,310 --> 00:27:27,060
Right. But if you thought that you need to pay for the price and know when I time complex that you

241
00:27:27,060 --> 00:27:31,410
need to scan them if you keep track of those three points you don't need to scan everything.

242
00:27:32,040 --> 00:27:36,209
You know I, I if you are interested in this are this implemented.

243
00:27:36,210 --> 00:27:38,730
I don't actually this is not what I implemented.

244
00:27:38,730 --> 00:27:44,610
But there is a whole that you are doing some unnecessary function evaluation here so you can make it faster.

245
00:27:44,610 --> 00:27:49,559
But when you when you, if you deal with like two and three dimension sorting is a point,

246
00:27:49,560 --> 00:27:54,360
but if you deal with like ten dimension or something, sorting could be a slightly less efficient.

247
00:27:55,110 --> 00:27:58,860
Right. Okay.

248
00:27:59,820 --> 00:28:03,690
Any question about this now? The middle with you. Okay.

249
00:28:03,930 --> 00:28:08,010
We're going to evaluate that, how this works at the end. Let's save it.

250
00:28:08,250 --> 00:28:13,170
Okay. I'm going to explain all these different algorithm first. Okay.

251
00:28:13,500 --> 00:28:18,210
Second thing, gradient descent. How many people heard about gradient descent?

252
00:28:18,990 --> 00:28:23,670
Just I heard this work. A lot of people. How many people know what it is?

253
00:28:24,980 --> 00:28:32,160
Okay. So you are in the right class, looks like. Okay, so gradient descent is very simple.

254
00:28:32,160 --> 00:28:38,309
I mean, for you. I mean, people talk about gradient descent and the machine learning that this is a this is

255
00:28:38,310 --> 00:28:43,500
like very standard method and there's a versions of stochastic gradient descent in a,

256
00:28:43,890 --> 00:28:48,240
you know, multiple different version of this graded descent.

257
00:28:48,540 --> 00:28:54,269
And these are what you mean. Selfie is also very popular because it's easy to understand.

258
00:28:54,270 --> 00:28:58,709
It makes sense. Okay. You know, it's not mathematically necessary.

259
00:28:58,710 --> 00:29:02,880
Beautiful. But there it's a little more mathematically sound.

260
00:29:03,280 --> 00:29:06,510
Then. Then.

261
00:29:07,990 --> 00:29:11,050
Another meta algorithm? I think so.

262
00:29:13,100 --> 00:29:21,710
To create a decent algorithm is basically you calculate a gradient gradient is a multidimensional derivative, right?

263
00:29:22,550 --> 00:29:33,680
So if you have an parameter, you can your even if your function is a single scalar function, your gradient will be in dimensional.

264
00:29:34,760 --> 00:29:38,210
So then what it does is that you're updating your.

265
00:29:41,290 --> 00:29:52,060
You're updating your parameters by some note by the direction of negative gradient with a certain certain coefficient.

266
00:29:52,480 --> 00:29:57,520
Why do you do that? Why does it make sense? Okay, so maybe.

267
00:29:57,520 --> 00:30:07,810
Maybe I can explain this way. Okay. So a gradient is a coordinated direction.

268
00:30:08,020 --> 00:30:12,730
So direction of the direction of how function increases.

269
00:30:12,940 --> 00:30:17,980
Right. So by looking at this, you don't know this is a mountain or valley.

270
00:30:18,220 --> 00:30:23,560
Okay. So let's assume this is a mountain going up, then it makes sense.

271
00:30:23,860 --> 00:30:28,060
If you want to going up, you want to add the gradient, right?

272
00:30:28,540 --> 00:30:34,180
So you keep adding it. But we formulated a problem as a minimization problem.

273
00:30:34,690 --> 00:30:40,120
When you try when you think this is a valley gradient is actually the other way, this, this way.

274
00:30:40,210 --> 00:30:44,710
Right. So you need to go down. So you need to subtract the gradient.

275
00:30:44,830 --> 00:30:50,830
Okay. So this is the reason why you're doing the minute minus is just because this is a minimization problem.

276
00:30:52,660 --> 00:30:58,930
Okay. And. Okay, so I understand that I need to go down by how much?

277
00:30:59,230 --> 00:31:07,360
Okay. That is really heuristic. Again, from here, it's not very you know, there is no very good algorithm.

278
00:31:07,600 --> 00:31:16,480
Okay. So so usually a recommended way is the making the step size into some arbitrary constant.

279
00:31:16,570 --> 00:31:22,150
That makes sense. Okay. So which probably is not does not make you happy.

280
00:31:23,170 --> 00:31:27,910
Another way to do it is that you can use this heuristic.

281
00:31:28,390 --> 00:31:39,070
Okay. Okay. The serious thing, if you if you if you do if you just look at this, this is actually very similar to second method.

282
00:31:39,670 --> 00:31:49,120
Okay. But second method, just so you divide by one, but here you need to divide by some constants.

283
00:31:49,120 --> 00:31:53,050
So it's a dividing the square of that and multiply the vector.

284
00:31:53,110 --> 00:31:53,860
So in this way.

285
00:31:53,860 --> 00:32:03,790
So but basically the idea is a very similar to 16 and it does a multiple multidimensional using to the the approximate second derivative.

286
00:32:03,790 --> 00:32:07,210
And I'll try to try to find the rate.

287
00:32:07,450 --> 00:32:10,840
Okay. Find the read. How much you need to go down. Okay.

288
00:32:12,560 --> 00:32:21,590
And is because of calculated Hession is going to be very expensive instead of, you know, doing the proper way for each of dimension.

289
00:32:21,590 --> 00:32:25,390
I'm just using the very simple averaging across this.

290
00:32:25,400 --> 00:32:30,049
So you're basically treating all the dimension equally to calculate the step size.

291
00:32:30,050 --> 00:32:35,990
So that that is the that is the idea behind of this particular equation.

292
00:32:36,320 --> 00:32:46,370
This equation is useful to find the lovely step size if you don't have any ideas of how to do database purposes.

293
00:32:46,370 --> 00:32:54,620
But if you keep using this, sometimes this, this, this value could be too small or too large, so it'll it'll fail.

294
00:32:54,620 --> 00:33:02,629
So usually updating this way, you know, maybe more efficient, but will create a lot of problems sometimes.

295
00:33:02,630 --> 00:33:07,010
It's similar to Newton ethnography. Okay.

296
00:33:07,760 --> 00:33:14,830
So that's the idea. Okay. So gradient descent is not perfect either.

297
00:33:14,840 --> 00:33:23,480
So sometimes. Okay, this is a well, I this happens really rarely, but you can in practice.

298
00:33:24,110 --> 00:33:36,190
Well, in principle you can. So this is you zero one, two, two, three is a is the the actual, actual direction of update.

299
00:33:36,390 --> 00:33:43,090
Okay. So sometimes you can you can just a wander around this way too much in a this is very, very flat.

300
00:33:43,100 --> 00:33:52,909
If you if you end up beings being stuck in a very, very flat area because a gradient descent is a, you know,

301
00:33:52,910 --> 00:34:02,360
making an update the proportional to the slope, then it's going to take a really, really long time, although this is not even the minimum.

302
00:34:02,630 --> 00:34:06,650
Right? So you need to go here, but it just doesn't doesn't go that way.

303
00:34:06,650 --> 00:34:14,180
Right. So and just you can stop in a very, very long time in a very, very flat surface if you get very unlucky.

304
00:34:14,590 --> 00:34:17,809
Okay. This doesn't happen very often, so I'm not to worry about it.

305
00:34:17,810 --> 00:34:25,400
But that is a theoretical concern of those gradient descent in terms of convergence properties.

306
00:34:27,680 --> 00:34:32,840
Okay. The nice thing about gradient descent, the code is really simple.

307
00:34:34,640 --> 00:34:38,240
So this is it. Okay, so this fit into one screen?

308
00:34:38,900 --> 00:34:43,700
Almost. Okay. So create a descent.

309
00:34:44,210 --> 00:34:49,070
You have to have two functions. One is the actual function.

310
00:34:49,670 --> 00:34:52,670
And G is a gradient. You need to provide a gradient.

311
00:34:53,330 --> 00:35:00,500
Gradient is a is a function that should return a vector so that that you should remember.

312
00:35:00,920 --> 00:35:06,140
Okay. And the dimension should be equal to the dimension of input.

313
00:35:06,410 --> 00:35:11,570
Okay. And you can provide a bunch of functions here.

314
00:35:11,660 --> 00:35:18,470
Here you can provide step size. Do I want to update my step size automatically or not?

315
00:35:19,190 --> 00:35:25,380
And what is the total tolerance in the maximum number of iterations and so on?

316
00:35:25,400 --> 00:35:27,260
So you can you can change these parameters.

317
00:35:27,260 --> 00:35:33,920
And when you when you try to see how this method works, you can try to vary this parameter and see what happens.

318
00:35:34,100 --> 00:35:38,600
Okay. So all.

319
00:35:40,950 --> 00:35:47,700
So these are good is is a is a longer than it's necessary but the actually.

320
00:35:49,500 --> 00:35:53,730
What you actually need is, uh.

321
00:35:54,960 --> 00:35:59,220
Is just this. Okay. This is what? This tree line is all you need.

322
00:35:59,560 --> 00:36:12,200
Okay, well, what am I doing? I'm up I'm I'm calculating the new x one new point I want to try based on some gradient instep size.

323
00:36:12,370 --> 00:36:17,230
Okay. And I'm going to evaluate my new function value.

324
00:36:18,250 --> 00:36:23,920
I'm going to evaluate the new gradient and after that, do something.

325
00:36:23,920 --> 00:36:26,799
And after that, I'm going to if this didn't converge,

326
00:36:26,800 --> 00:36:35,140
I'm going to assign this value into zero F digital and digital and redo this iteration until until convergence happens.

327
00:36:35,140 --> 00:36:40,570
So that's the logic. And there are some stepping criteria.

328
00:36:40,570 --> 00:36:44,350
Sometimes your function could be infinite for whatever reason.

329
00:36:44,350 --> 00:36:47,890
Then it doesn't work. So it doesn't. It didn't converge.

330
00:36:48,300 --> 00:36:57,670
I'm spinning them out. If the difference between the F1 and F digital becomes really, really small than that,

331
00:36:58,060 --> 00:37:06,430
I'm going to assume that that converted and it's b if the relative error was

332
00:37:06,430 --> 00:37:10,690
below the tolerance in a C convergence is zero and I break out of the loop.

333
00:37:11,860 --> 00:37:22,720
Otherwise I want to keep doing this. But if I decide to update my step size using this equation in this equation, I can do that.

334
00:37:23,050 --> 00:37:31,000
The way how you do it is exactly just to put plugging in the same equation here, just to calculate the step size this way.

335
00:37:31,000 --> 00:37:35,200
Then in my next iteration, I'm going to use this step size to update my parent.

336
00:37:35,740 --> 00:37:44,720
Okay. So that is an option. Okay. Okay.

337
00:37:45,170 --> 00:37:51,080
So sometimes it could come very slow if your especially your step side is specified incorrectly.

338
00:37:53,240 --> 00:38:04,310
You know, what is that? What is. So what is the advantages and disadvantages of the setting, the step size or too large or too too small?

339
00:38:05,060 --> 00:38:13,750
What do you think? If it's too large, you can overshoot.

340
00:38:15,470 --> 00:38:18,740
Tomorrow, at least for a very, very, very good robot.

341
00:38:19,040 --> 00:38:27,750
So. So here. So here what you have is.

342
00:38:28,950 --> 00:38:35,760
So if you have a step size, very large overshoot means that I have a step size this much.

343
00:38:36,210 --> 00:38:39,240
Then you can just go over here and keep going.

344
00:38:39,240 --> 00:38:42,930
Like this way you never, like, be close to become close to here.

345
00:38:43,050 --> 00:38:49,370
Okay. That's that's the downside in a.

346
00:38:51,410 --> 00:38:55,190
If you step aside, is really, really small. Oh, I'm updating.

347
00:38:55,370 --> 00:39:01,659
I'm going really slow. Like this. Right. So you just need to have a good guess of what?

348
00:39:01,660 --> 00:39:05,660
What my original step says. Otherwise, you can do it over there.

349
00:39:05,690 --> 00:39:09,530
It can. It can be. It can overshoot, or you can go really slow.

350
00:39:09,740 --> 00:39:14,390
Okay. So that's that.

351
00:39:17,650 --> 00:39:20,910
Let's move on to the Newton Newton's method. Okay.

352
00:39:22,240 --> 00:39:25,250
So Newton's method is.

353
00:39:25,270 --> 00:39:34,810
Well, I kind of like the method, but why do we use this arbitrary step size?

354
00:39:35,170 --> 00:39:41,200
That doesn't make sense. We have a we have something mathematically correct way to do it.

355
00:39:41,620 --> 00:39:49,510
Okay. Which is a fashion. Fashion is a second order derivative of a of the function.

356
00:39:49,860 --> 00:39:54,640
Okay. You can do it. You can. Then you have all the second derivative.

357
00:39:54,910 --> 00:39:59,230
Right. Then you can use the second derivative and multiply this.

358
00:39:59,620 --> 00:40:04,270
So basically multiply by inverse of a second derivative, which is a in the Newton.

359
00:40:04,700 --> 00:40:11,319
That's now with a one dimensional. You divide by divide by the second derivative.

360
00:40:11,320 --> 00:40:17,440
Right. So. But here you are not dividing you to multiplying the inverse.

361
00:40:17,440 --> 00:40:20,710
That that's only the difference. Okay. So.

362
00:40:23,210 --> 00:40:30,950
Then this some, especially if you're mathematically flavored person, you may want to argue that this is the right way to do it.

363
00:40:31,400 --> 00:40:35,870
Okay. Uh, that's. That's right.

364
00:40:36,230 --> 00:40:44,540
In some sense, if everything works as expected, but you're evaluating.

365
00:40:44,540 --> 00:40:50,270
So you're evaluating this gradient and gradient yet at the specified point.

366
00:40:50,390 --> 00:40:58,150
Right. So. So we already went through some problems of Newton's algorithm.

367
00:40:58,390 --> 00:41:02,860
So sometimes you're just your approximation at some point is usually good.

368
00:41:02,870 --> 00:41:08,319
Sometimes it could go wrong and that that doesn't guarantee convergence and so on.

369
00:41:08,320 --> 00:41:11,590
So by expectation, it should be efficient.

370
00:41:11,740 --> 00:41:16,180
But there are some career cases. It doesn't work. So that is a problem one.

371
00:41:17,860 --> 00:41:31,670
Okay. But this problem, one, becomes a bigger problem in the multidimensional setting because you're calculating this Haitian matrix.

372
00:41:32,660 --> 00:41:35,920
This Haitian matrix is basically double, double derivative.

373
00:41:35,960 --> 00:41:38,960
All possible combinations of those dimensions.

374
00:41:39,680 --> 00:41:42,740
You're calculating the on a lot of values here.

375
00:41:43,880 --> 00:41:47,810
If some values are not really correct, providing the correct,

376
00:41:48,110 --> 00:41:58,010
correct way to go to the next step, then it's going to, you know, point the wrong direction.

377
00:41:58,790 --> 00:42:02,389
So even though gradient is correct, so, you know,

378
00:42:02,390 --> 00:42:09,890
the what which direction you need to go to after multiplying this inmate's theory into some weird direction.

379
00:42:10,790 --> 00:42:11,090
Okay.

380
00:42:11,480 --> 00:42:22,309
So, well, ideally, you know which direction you should go to and you may know, like how much weight you need to provide for each of the dimensions.

381
00:42:22,310 --> 00:42:27,320
So ideally that's possible, but because you don't know the whole function, how,

382
00:42:27,320 --> 00:42:31,820
how, how all the function looks like across all dimension, there will be a lot of,

383
00:42:32,280 --> 00:42:44,780
you know, unexpected things in the in the happening could be happening and that that could be, you know, making less efficient iteration sometimes.

384
00:42:44,930 --> 00:42:51,379
Okay. So that's the problem. One problem, too, is the question is, is it expensive to compare,

385
00:42:51,380 --> 00:42:56,660
especially when you're talking about really large the major, sometimes pretty decent works we like.

386
00:42:57,810 --> 00:43:00,960
Thousand dimension. Million dimension. Okay.

387
00:43:01,170 --> 00:43:07,320
If you're a lot of machine learning parameters, it's using like million, billion parameters.

388
00:43:08,310 --> 00:43:12,000
You cannot really calculate a million by million. How do you do it?

389
00:43:12,750 --> 00:43:15,920
So especially a high dimensional problem.

390
00:43:15,930 --> 00:43:22,620
This is not a practical solution because the calculation is a matrix requires a lot of computation,

391
00:43:22,770 --> 00:43:29,070
a lot of storage and a you know, it's a printed numerical precision in at least some dimensions.

392
00:43:29,190 --> 00:43:42,060
Okay. And finally, you need to multiply this inverse operation matrix and there is no guarantee that your question matrix is a is non singular.

393
00:43:42,150 --> 00:43:46,830
So you could, you may end up being a singular matrix and that that could cause a problem to.

394
00:43:49,730 --> 00:43:54,260
But mathematically it looks right. Okay, so why don't we try?

395
00:43:54,500 --> 00:44:05,220
Okay, so it's almost the same as I see that I'm doing this calculating the F function and gradient.

396
00:44:05,240 --> 00:44:09,770
Now I'm in the hessian. You need to you need to have a three function.

397
00:44:10,430 --> 00:44:13,940
First function returns as color. Second function returns a vector.

398
00:44:14,240 --> 00:44:25,050
Third function returns a matrix. Okay. Then what I'm doing is just I have hessian, so inverse those the session and multiply by create gradient.

399
00:44:25,070 --> 00:44:30,530
So instead of doing calculate an actual inverse, you can just use your search function to do this.

400
00:44:31,130 --> 00:44:37,490
And you know that. That's it. Everything else looks pretty much the same as a gradient descent, except.

401
00:44:37,510 --> 00:44:42,350
So basically, instead of using step side, you're doing the pressure matrix.

402
00:44:42,620 --> 00:44:49,360
And that that's that. Okay. Okay.

403
00:44:54,590 --> 00:45:03,650
So. Now. Well, many people like Newton method because of mathematical knots.

404
00:45:03,660 --> 00:45:10,490
Correct. But also they realize that calculating the pressure matrix is not ideal.

405
00:45:11,050 --> 00:45:19,910
Right? So for many reasons. So there are a lot of heuristics to mimic what Newton's method are doing.

406
00:45:20,660 --> 00:45:25,400
But doesn't require to have a and function to be explicitly calculated.

407
00:45:25,850 --> 00:45:29,750
Okay. So there are a lot of papers, you know.

408
00:45:30,900 --> 00:45:38,870
Uh, so these are, you know, you know, 50 years ago, this was a really hard copy.

409
00:45:39,050 --> 00:45:43,870
How do I do it? Do this. Right. Okay. The print algorithm became sorry.

410
00:45:43,950 --> 00:45:48,410
The other mediums became very popular. And that.

411
00:45:48,710 --> 00:45:51,860
But that doesn't. That doesn't require that.

412
00:45:51,860 --> 00:45:54,920
That has a very good property. That doesn't require a derivative.

413
00:45:54,930 --> 00:45:58,520
But what if I have a derivative? Encoded in descent is a good way.

414
00:45:59,090 --> 00:46:08,659
But you know what? What do we do better? So, uh, there are a lot of method out there.

415
00:46:08,660 --> 00:46:13,400
PFG s algorithm turned out to be most efficient.

416
00:46:13,790 --> 00:46:28,130
These are the initial of four authors, and we have just started with them was, you know, updated multiple times and LP plus GSP algorithm,

417
00:46:28,280 --> 00:46:34,759
these are golden is considered to be the probably most practical and efficient

418
00:46:34,760 --> 00:46:39,780
average in a lot of that I recommended a lot of a lot of people so just the.

419
00:46:40,730 --> 00:46:45,620
So basically it's all about using gradient descent and try to remember.

420
00:46:45,740 --> 00:46:50,780
So what this is trying to do, I'm going to remember some value in my gradient descent value.

421
00:46:51,350 --> 00:46:56,090
So based on the my history of this gradient, I'm going to estimate my hedging.

422
00:46:56,680 --> 00:47:03,340
Okay. So that's what it's trying to do. And you know, and I try to avoid the corner cases and so on.

423
00:47:03,800 --> 00:47:09,560
We have just argued it works well, but this is a requires a lot of memory if you have a high dimensional case.

424
00:47:10,160 --> 00:47:18,230
And it also does not allow to specify, uh, you know, my, your parameter space.

425
00:47:18,400 --> 00:47:23,330
Okay. So sometimes you apply me to space. I want to have a some bounding box of my parameter.

426
00:47:23,330 --> 00:47:29,630
So my part of me has to be between zero and one for everything, for example, so it doesn't allow that.

427
00:47:29,930 --> 00:47:38,000
So LBA for GSP algorithm, this basically allows the bounding and it also doesn't require a lot of memory,

428
00:47:38,570 --> 00:47:43,459
but it's it works as good as we have for algorithm in general.

429
00:47:43,460 --> 00:47:47,030
So that's why it became very popular.

430
00:47:47,360 --> 00:47:50,910
Okay. So.

431
00:47:52,350 --> 00:48:01,250
Okay, so. So that's that's the introduction of this long history, of this numerical method.

432
00:48:01,670 --> 00:48:06,920
These are all I a method because it's trying to mimic what the method is trying to do.

433
00:48:07,970 --> 00:48:14,300
I could explain some of these, but, you know, these are you know, now the meat is already very boring for many people.

434
00:48:14,520 --> 00:48:19,489
I don't want to, you know, waste your time for most of you.

435
00:48:19,490 --> 00:48:27,620
But these are these are good paper to read. Maybe pages in the paper is is a good, good, good method to use a read.

436
00:48:28,160 --> 00:48:42,890
And this was a pretty restrained compared to others. So but in practice in practice what you can use is an art has a function called optim.

437
00:48:44,300 --> 00:48:51,910
Some of you may have used it I guess with this does provide a very general optimization routine.

438
00:48:52,730 --> 00:49:02,360
So it implements the other middle wisdom PFG or Sodium LP, JSP algorithm as a print algorithm, conjugate gradient and simulate annealing.

439
00:49:03,200 --> 00:49:07,189
These these are these these two algorithm.

440
00:49:07,190 --> 00:49:08,750
I don't think we're going to cover it.

441
00:49:09,380 --> 00:49:16,190
Similarly, delivering a annealing is a monte Carlo method, so we probably can catch up on a little bit at the end.

442
00:49:16,550 --> 00:49:21,320
Conjugate gradient is basically just gradient with the gradient descent with the certain.

443
00:49:21,740 --> 00:49:28,880
This is similar to the Newton I was in basically, but I don't think it's it's useful in general.

444
00:49:29,130 --> 00:49:40,850
Okay. So yeah, so most of the case, so what I recommend is when you don't have gradient, just use another meet.

445
00:49:41,270 --> 00:49:48,560
Okay, no brainer. Okay. If you have gradient, use this.

446
00:49:49,540 --> 00:49:55,400
I will be up for GSP. Okay. If you have a single dimension optimization,

447
00:49:56,000 --> 00:50:01,220
use this printer which is only for working for a single dimensional equation I explained in the last lecture.

448
00:50:01,610 --> 00:50:07,939
Basically it's a combination of coding search algorithm plus parabola approximation.

449
00:50:07,940 --> 00:50:17,700
So that's what it does. Okay. So yeah, so these are these three algorithms are mostly what I would recommend.

450
00:50:17,720 --> 00:50:26,060
I use them a lot actually. So, uh, so then now is the time to try each of them.

451
00:50:26,240 --> 00:50:29,440
Okay. So let me just propose.

452
00:50:30,770 --> 00:50:49,540
Yeah. Let me. Let me reload this. To do the.

453
00:50:51,130 --> 00:50:58,120
Canadian said the I'm loading all the algorithm to compare that, evaluate them to see how they work.

454
00:50:59,350 --> 00:51:04,180
Okay. So the function we're going to try is this.

455
00:51:04,330 --> 00:51:09,340
Okay. Do you know whether what the minimum should be for this one?

456
00:51:11,880 --> 00:51:17,550
Well, it has to make both of them zero. So x one should be 1x2 should be one two.

457
00:51:17,550 --> 00:51:20,790
Right. So but looks complicated function after all.

458
00:51:22,170 --> 00:51:25,740
So let's load this function, define this function.

459
00:51:26,220 --> 00:51:30,810
And what I'm going to do, try first with that.

460
00:51:30,990 --> 00:51:35,470
I'm not going to use the previous 1/1 and just try to because another me the

461
00:51:35,490 --> 00:51:43,220
algorithm has this routine optim it implements this now the media algorithm.

462
00:51:43,670 --> 00:51:48,360
Okay, so let's look at the result. So.

463
00:51:49,910 --> 00:51:56,680
So basically what you can do when you use this period is a parameter initial point.

464
00:51:56,800 --> 00:52:02,260
So my initial point is this when you try to optimize an algorithm, I'm going to start from this point.

465
00:52:02,650 --> 00:52:10,510
Okay? And you need to provide a function for when and you specify method is anatomy.

466
00:52:10,510 --> 00:52:15,580
Then I'm going to use the argument and it does return these values.

467
00:52:15,730 --> 00:52:19,510
Oh this is my optimum upon ometer 1.00063.

468
00:52:19,750 --> 00:52:22,930
Okay. And 1.000112.

469
00:52:22,940 --> 00:52:26,710
So it's pretty close, but it's not really accurate.

470
00:52:27,460 --> 00:52:33,220
My function value is close to zero. Okay. Which is which you would expect is optimal value.

471
00:52:34,030 --> 00:52:37,240
How many function evaluation happened? 165.

472
00:52:37,750 --> 00:52:44,700
Did it converge? Yes. Okay. You can specify tolerance in some other parameters, but, you know, that's pretty much what you can do.

473
00:52:44,710 --> 00:52:51,250
So it does run the other way, the way you can, that you can collaborate a lot of different parameters.

474
00:52:51,250 --> 00:52:56,110
So that's the downside of using this optimal algorithm. This another meta algorithm.

475
00:52:57,130 --> 00:53:05,230
Do you think it's better or worse? Let's see. If you do this, it does.

476
00:53:05,530 --> 00:53:11,290
Minimum values are one one. The f mil is a pretty close to zero.

477
00:53:12,280 --> 00:53:15,160
Converted number of iteration is 139.

478
00:53:16,150 --> 00:53:24,549
So, uh, I mean, this is not actually fair, fair evaluation because it is a number of iteration is not, is probably better.

479
00:53:24,550 --> 00:53:34,209
The number of actual function evaluation is actually more. But, uh, yeah, but, but this seems very comparable actually.

480
00:53:34,210 --> 00:53:42,970
This looks really good. In some view, the main median is actually just tolerance parameters different than idealism is slightly different.

481
00:53:42,970 --> 00:53:50,080
But what I wanted to point, I wanted to make is that the name of the model will be implemented here is really comparable,

482
00:53:50,080 --> 00:53:57,549
a very good algorithm you might want to use in practice. So if you happen to use optimal algorithm, it doesn't work.

483
00:53:57,550 --> 00:54:02,200
Just plug it, copy and paste. Those are those now to me, then try to use it.

484
00:54:02,200 --> 00:54:05,580
That might work better. Okay. Okay.

485
00:54:07,730 --> 00:54:11,059
Uh. So. So. And when you make a new your.

486
00:54:11,060 --> 00:54:16,190
Your new automation algorithm, don't assume that. Oh, there's already something that I implement in mine.

487
00:54:16,520 --> 00:54:20,540
Should my must be worse. No, not necessarily.

488
00:54:20,630 --> 00:54:29,090
But if you make it efficient, then, uh, you know, organize very well, then it can very well to be better then.

489
00:54:29,420 --> 00:54:34,280
Then the already what's, what's built in and ah that's pretty outdated.

490
00:54:34,280 --> 00:54:41,180
Many of the algorithm doesn't get updated the laws. So if you have new ideas in a new environment, those should actually help.

491
00:54:41,510 --> 00:54:44,990
Okay, so let's do more evaluations.

492
00:54:45,500 --> 00:54:49,610
Okay? So let's think about these three functions.

493
00:54:49,610 --> 00:54:52,999
So first one is the same as before. Second function.

494
00:54:53,000 --> 00:54:56,660
Uh, what do you think it should be? Minimize that.

495
00:54:59,620 --> 00:55:01,320
Actually. Yeah.

496
00:55:01,810 --> 00:55:10,870
So if you if you do some math to try to try to take the derivative of each of the to the dimension, you will see that this is a minimum.

497
00:55:10,870 --> 00:55:17,230
This should be minimized that x equal .5.5 into extra, extra equal point five.

498
00:55:17,500 --> 00:55:21,970
Okay. And this one is sinusoidal, but sorry.

499
00:55:24,420 --> 00:55:31,059
To do this. What is the complicated function which doesn't have a global optimum or minimum, obviously,

500
00:55:31,060 --> 00:55:38,410
but just to it's a good, good function to see how these are good and works in some more arbitrary cases.

501
00:55:39,040 --> 00:55:44,649
But in this case, you know that this at least the minimum is greater than negative one, right?

502
00:55:44,650 --> 00:55:51,610
So and because this doesn't have a you know, this may be easily trapped in the local minimum.

503
00:55:51,610 --> 00:55:57,820
So it's it's very possible that you may not reach to the global minimum in this case.

504
00:55:59,260 --> 00:56:07,290
So let's define this three function. Okay. So but we're going to do something a little bit.

505
00:56:08,570 --> 00:56:21,830
Interesting here. Okay. What I'm going to do is that I already defined the F1 and F2 and F3 as a two to take as an x1x2x1x2x3 and so on.

506
00:56:21,840 --> 00:56:32,600
Right. But when you try to plug that into this function, so now the media algorithm or ultimate wisdom,

507
00:56:33,260 --> 00:56:37,070
it doesn't like that function taking multiple parameters.

508
00:56:37,580 --> 00:56:42,650
You know, it, it prefers to have the of parameter as given as a vector like this.

509
00:56:42,830 --> 00:56:52,219
Okay. So instead of so I have this F1 but I'm going to just a defined F1 multi that takes a single parameter which is should

510
00:56:52,220 --> 00:57:00,890
be vector and just call them so F1 multi and F2 motive and have three multi are equivalent to F1 and F2 and have three.

511
00:57:01,820 --> 00:57:05,570
But just yeah.

512
00:57:05,900 --> 00:57:11,720
It is just that multi multiple to represent implementation or a single parliament implementation.

513
00:57:11,990 --> 00:57:15,740
Okay. So I'm going to get back to this why I did that.

514
00:57:15,740 --> 00:57:24,920
But you know, at least it makes sense. I'm going to just visualize how this function should look like.

515
00:57:25,100 --> 00:57:32,810
So just define some some area and evaluate the function and just show as an image.

516
00:57:33,410 --> 00:57:37,910
So and this is F1 and F3 is both of them are two parameter functions.

517
00:57:37,910 --> 00:57:41,840
So you can visualize that into is an image.

518
00:57:42,170 --> 00:57:50,840
And F1 as you expect that this is a minimize a0x to zero and you know and this is a this looks very complicated.

519
00:57:51,060 --> 00:57:54,430
Okay. Yeah.

520
00:57:54,500 --> 00:58:03,440
F2 is a three dimensional, so I'm not going to try to visualize it, but it should be clear that it's a minimize that.

521
00:58:03,440 --> 00:58:08,230
0.5. 0.5. 0.5. Okay.

522
00:58:10,180 --> 00:58:16,690
So first one, we're going to try to coordinate descent, but it isn't that you may have forgotten it,

523
00:58:16,690 --> 00:58:22,180
but basically doing the optimization for each of the coordinate.

524
00:58:22,330 --> 00:58:26,739
Right. So I'm going to provide this F1 F2, F3 is a multi,

525
00:58:26,740 --> 00:58:31,809
multi just the single parameter version and you need to provide the range for

526
00:58:31,810 --> 00:58:35,470
the coordinate descent and the provide the tolerance and see what happens.

527
00:58:35,890 --> 00:58:38,950
Okay. So this may take a long time.

528
00:58:39,220 --> 00:58:42,850
Okay. So this took quite a bit of time.

529
00:58:43,420 --> 00:58:56,590
Let's look at the result. Okay. So first one, I would expect the x one, the excellent x one equal 1x2 equal one should be the the minimal point.

530
00:58:56,600 --> 00:59:05,710
So it looks like it did converge. It required a 4695 iterations in this case.

531
00:59:06,760 --> 00:59:15,110
The second one. Yeah, the second one is a .5.5.5.

532
00:59:15,210 --> 00:59:19,660
Pre close convergence is required on the 11th iteration.

533
00:59:19,670 --> 00:59:30,249
That's interesting. Third one, it just found some some value which may not make sense, but it did find something.

534
00:59:30,250 --> 00:59:39,700
And it's interesting that I gave X the the starting point I gave is is pretty close to zero,

535
00:59:40,120 --> 00:59:45,249
but the minimum point that are found is a lot larger value.

536
00:59:45,250 --> 00:59:50,170
So this is because I use a coordinate coding search.

537
00:59:50,170 --> 00:59:56,410
Ah. What do you mean. It, it did, it did have a problem in finding bracket in some cases I guess.

538
00:59:57,250 --> 01:00:00,550
And it's iteration number of iterations 200.

539
01:00:00,820 --> 01:00:07,720
Okay. Now let's try to allow me to, to, to work on that.

540
01:00:07,990 --> 01:00:10,780
Now domain is extremely fast as you see.

541
01:00:11,290 --> 01:00:23,260
And if you see the result, it did converge much more precisely and required 118 iterations, required 91 iterations, 33 iteration, much less.

542
01:00:24,040 --> 01:00:32,830
And if it did find the right value for a first two example and this one, we don't know whether this is right or wrong.

543
01:00:33,040 --> 01:00:38,979
Right. The way how you how you see whether it's right is that if this is because is assigned times

544
01:00:38,980 --> 01:00:45,639
cosine there is a there is a value that x value that makes this value exactly a negative one.

545
01:00:45,640 --> 01:00:50,680
So if the F of mean is negative one, you can say that, oh, this is this conversion.

546
01:00:50,920 --> 01:00:57,880
In the previous cases, it did, it did find something similar, but it found something different.

547
01:00:58,030 --> 01:01:09,150
Yeah. Okay. So let's use a delta meta algorithm that are implemented in the opt in function.

548
01:01:09,160 --> 01:01:14,440
So if if you don't have the implementation delta with the algorithm we have here, you can use this.

549
01:01:15,280 --> 01:01:18,849
And if you do this well, it's really, it says really fast.

550
01:01:18,850 --> 01:01:22,389
I don't know. Yeah, it, it is really zero.

551
01:01:22,390 --> 01:01:34,150
But it, it ran really fast and but here interesting part is that the first one didn't converge, although we said is converged.

552
01:01:34,420 --> 01:01:43,050
But you know, this is not right. So I don't know what happened in this particular case, but, uh, it just didn't work.

553
01:01:43,060 --> 01:01:48,870
Basically, you started for if you start from the point negative ten, negative ten, it's a,

554
01:01:48,910 --> 01:01:54,969
it's probably too far from the optimal point and it, it was stuck in somewhere, I guess.

555
01:01:54,970 --> 01:02:02,020
So that's what happened. Okay. And so sometimes sometimes some implementation is not because especially optimal

556
01:02:02,020 --> 01:02:05,350
algorithm doesn't have a very good implementation with delta meta algorithm,

557
01:02:05,620 --> 01:02:09,730
I think. So if you want to use another organism, just use the one in the class.

558
01:02:10,720 --> 01:02:16,150
Okay. There. But the second case, it did converge. It did require a lot of function evaluation.

559
01:02:16,150 --> 01:02:20,320
126 in this case, it was 9191.

560
01:02:20,320 --> 01:02:27,580
So it wasn't necessarily most efficient. A third one, it did find a value where Y values a negative one.

561
01:02:27,580 --> 01:02:32,680
So it did converge. And the value seems to be very close to what is found here.

562
01:02:32,680 --> 01:02:35,980
So it looks like it worked okay.

563
01:02:37,820 --> 01:02:42,920
Now. So these are the so these are the method that doesn't require a gradient so far.

564
01:02:43,910 --> 01:02:47,210
Now we are going to evaluate the method that is part of gradient.

565
01:02:48,920 --> 01:02:52,400
Okay. So but I don't want to calculate the gradient.

566
01:02:52,570 --> 01:02:55,700
Okay. So that's the that that is that that is a challenge I had.

567
01:02:56,300 --> 01:03:01,010
So there is a package qualitative. Okay. Which is calculate the.

568
01:03:01,310 --> 01:03:04,970
So calculate the derivative for you.

569
01:03:05,360 --> 01:03:13,460
Okay. So, but if to use the use this so it you calculate the derivative.

570
01:03:13,670 --> 01:03:17,540
Not numerically. It does calculate the derivative.

571
01:03:18,350 --> 01:03:22,760
You know, analytically it are if you give a function like this.

572
01:03:23,450 --> 01:03:29,020
So what that is doing is that you are providing a function like this.

573
01:03:29,030 --> 01:03:29,630
I give up,

574
01:03:30,410 --> 01:03:40,330
I give a function like this and it is it uses symbolic notation and find the what the gradient should be okay for for each of these a function.

575
01:03:40,380 --> 01:03:44,090
So it is convenient. I didn't realize this.

576
01:03:44,210 --> 01:03:51,710
This existed before. But basically what you can do is that oh, I'm going to calculate derivative and calculate the gradient.

577
01:03:51,710 --> 01:04:00,150
But to be able to do that, you need to separate the parameter into this first parameter, second part with a separate parameter.

578
01:04:00,170 --> 01:04:10,390
It doesn't work in this case it doesn't. So F1 multi here, if you give the parameter as a vector, it doesn't know how to do it.

579
01:04:10,400 --> 01:04:15,110
So you need to specify the parameter in a single variable.

580
01:04:15,110 --> 01:04:23,330
So this is how you do it. Okay. So this is and you can also calculate the question using this way, too.

581
01:04:23,930 --> 01:04:30,259
And after that, I to be able to be used in our I you need a single parameter representation.

582
01:04:30,260 --> 01:04:35,480
So I'm going to define Q one multi, two Multijet three multi after finding derivative.

583
01:04:36,170 --> 01:04:46,219
Same thing for the question. So that's what I'm going to do. Of course you can calculate the derivative by yourself and there's no problem.

584
01:04:46,220 --> 01:04:51,620
But there is a package like this so you can use it. Okay. So.

585
01:04:55,160 --> 01:05:04,790
Then let's try to the use of gradient descent where doing the gradient descent providing the F1 multi and multi four of the multi G2,

586
01:05:04,790 --> 01:05:07,890
multi and so on and give us step size.

587
01:05:07,910 --> 01:05:18,420
We're going to update the step size with that equation. To using the the using the the second method.

588
01:05:18,660 --> 01:05:25,560
So using the approximate the second derivative to update the step size, okay and see what happens.

589
01:05:26,490 --> 01:05:34,020
Most of the method worked really fast. How did it work? So it did it did convert as you see.

590
01:05:34,050 --> 01:05:39,930
It did converge. Did you see? And it did converge, you see, because f of mean is negative one.

591
01:05:41,310 --> 01:05:46,830
And uh, it, it returns a step side of these value.

592
01:05:46,860 --> 01:05:49,739
What I care about is the how many iterations did did they require?

593
01:05:49,740 --> 01:05:57,600
This one required a 64 iteration did required it require only three iterations and the it required 30 iteration.

594
01:05:58,020 --> 01:06:03,240
The reason why did this. So why do you think this required a 60 for this required three?

595
01:06:03,810 --> 01:06:06,840
If you look at the function, how the function looks like.

596
01:06:12,260 --> 01:06:17,600
This one has an excellent is a looks like independent measurement x2 and x1.

597
01:06:17,810 --> 01:06:22,820
There's a strong correlation. So the function looks very correlated with the next one.

598
01:06:22,820 --> 01:06:26,600
Next one that they have, they are they don't look independent basically.

599
01:06:27,560 --> 01:06:32,060
And here x1, x2 x three are pretty independent.

600
01:06:32,060 --> 01:06:36,670
So these are eejit easier problem to optimize the first one when there is a

601
01:06:37,170 --> 01:06:42,079
lot of linearity between the well correlation between this and that linear,

602
01:06:42,080 --> 01:06:46,940
but still there's a lot of entangling between the dimension.

603
01:06:47,270 --> 01:06:52,080
Then this is a harder problem. Okay. Okay.

604
01:06:52,440 --> 01:06:59,970
So, so that that that's what the noting and the gradient descent did really well, especially for this problem.

605
01:07:00,120 --> 01:07:05,280
And it also did better than the coding search in terms of number of function evaluation required.

606
01:07:07,590 --> 01:07:11,100
And it also it required only three iterations here.

607
01:07:11,250 --> 01:07:20,430
Okay. How about Newton method that the method took a little bit of time, especially because these are cultural impressions I think.

608
01:07:21,240 --> 01:07:33,450
But ah, so if you see it, it, it did require a lot of iteration six it here only required three iterations on the four iterations.

609
01:07:33,450 --> 01:07:43,220
So if you use a neater method and if you know the exact function, how it looks like that, that usually does really well.

610
01:07:43,230 --> 01:07:51,410
But you know, as I said, you need to provide both the gradient and then sometimes it fails because of the singularity and so on.

611
01:07:51,420 --> 01:08:02,520
So although this is a this looks good in this particular examples and you may want to use it this method, um, if you care about efficiency,

612
01:08:02,520 --> 01:08:07,079
but if you don't want to deal with the cases and deal with the question,

613
01:08:07,080 --> 01:08:15,060
you probably want still want to stick to create a decent or page I need to Mr. like LPA for JSP.

614
01:08:15,270 --> 01:08:26,909
Okay, so LPA projects P is a pretty much the same setting except that I gave kee one multijet two multi that I gave the gradient previously.

615
01:08:26,910 --> 01:08:30,920
When I use optimum function, I only use a function f f of any.

616
01:08:30,960 --> 01:08:39,820
Right now I'm giving the the gradient and except that is pretty much the same and I'm specifying the method is a LPA for DSP.

617
01:08:40,380 --> 01:08:44,520
So those are usually pretty efficient.

618
01:08:45,330 --> 01:08:52,860
So if you use this quite a new method, it, it needs the, it needed a little bit more function evaluation but it was,

619
01:08:52,860 --> 01:08:57,000
it requires only three one, five and eight function evaluation.

620
01:08:57,010 --> 01:09:04,290
So it was a very efficient and it found the right solution for all these three cases.

621
01:09:04,680 --> 01:09:15,560
So it did work. Okay. So, uh, here, the take home message is that if you don't want, don't have a gradient use now the mid,

622
01:09:15,570 --> 01:09:25,709
if you have gradient use LP FC LP, if GSP, that that should be a little bit more efficient, one dimensional case,

623
01:09:25,710 --> 01:09:34,800
use a use print algorithm and you can use optimum function if you don't have a implementation of your particular I mean you want to use,

624
01:09:34,800 --> 01:09:40,050
but if you have your own tailored implementation, then might work better for your problem.

625
01:09:42,390 --> 01:09:55,950
Okay. So so that's the end of the lecture because I'm because I'm not going to talk about all the bells targeted at them.

626
01:09:56,670 --> 01:10:02,130
I just wanted to mention, okay, so this is obviously not everything.

627
01:10:02,250 --> 01:10:11,750
So in for this case these are golden is does not have any assumption of a convexity or the

628
01:10:11,760 --> 01:10:17,790
form if default if you assume the function is convex or or have have certain properties,

629
01:10:17,790 --> 01:10:26,190
then you can leverage those properties to do some more aggressive optimization to assuming the convexity.

630
01:10:26,610 --> 01:10:28,460
And there are others like that. So.

631
01:10:29,970 --> 01:10:39,210
So there are there are sort of those kind of like a linear programing, quadratic programing, say my definite programing and those are you know,

632
01:10:39,240 --> 01:10:49,770
area you need you can burn from day 15 of also there are no matter like admin and those methods are also

633
01:10:49,770 --> 01:10:58,230
very very nice are you going to you may want to know you're usually using the like so yeah those those

634
01:10:58,320 --> 01:11:05,969
convex convex optimization problem you usually use are some are not going to multiply inequality and those

635
01:11:05,970 --> 01:11:14,100
those are theory you need to need and you need to learn a little bit and there is stochastic emergence.

636
01:11:14,370 --> 01:11:19,460
So you may heard of a stochastic gradient descent, which is a grid in the center region,

637
01:11:19,470 --> 01:11:24,540
but if you have a lot of data, then calculating gradient itself takes a lot of time.

638
01:11:24,540 --> 01:11:32,070
But so you basically want to make a small batch of the of the data and they'll calculate the gradient using those small batches.

639
01:11:32,820 --> 01:11:40,350
And that became very effective actually. So that there was well, some popular variation of that algorithm is like Adam.

640
01:11:41,100 --> 01:11:50,330
Adam optimize is probably used a lot and those are just a special specific variation of this gradient is a correct and descent mark,

641
01:11:50,340 --> 01:11:52,799
but similar to LP suggest algorithm,

642
01:11:52,800 --> 01:12:03,920
it does to do some smart thing to approximate the second derivative and try to find a better direction for the gradient descent optimization.

643
01:12:03,930 --> 01:12:09,720
So those are the methods you may want to learn and but not covered in this.

644
01:12:11,000 --> 01:12:16,730
Yes. And as I said, this is an introduction of the numerical method.

645
01:12:17,120 --> 01:12:21,690
Okay. And I think this knowing all these things are very important.

646
01:12:21,710 --> 01:12:25,640
I really believe so. But this is just a start.

647
01:12:26,180 --> 01:12:32,450
And yeah. And and that that's that's all I can I want to say.

648
01:12:32,630 --> 01:12:36,620
Okay. So it is an important problem.

649
01:12:36,890 --> 01:12:46,910
Okay. That's all I want to say. Any, any any question for lecture eight before we have eight, 7 minutes to move on to the lecture nine.

650
01:12:50,850 --> 01:12:54,120
Okay. So this is an optimization. Okay.

651
01:12:54,130 --> 01:12:57,360
So now we're going to move for a different, different topic.

652
01:12:57,840 --> 01:13:02,880
Okay. We're going to come back to optimization when we do the algorithm.

653
01:13:03,090 --> 01:13:09,930
But that that that'll that'll be done after doing this random number generation because this is relevant for homework.

654
01:13:12,260 --> 01:13:17,480
Good. So this is it. Now we're. So some of these will appear in the form of four.

655
01:13:21,250 --> 01:13:27,340
So random number generation. So I think we can do some light introduction.

656
01:13:28,450 --> 01:13:31,720
So random numbers. What is random numbers? So.

657
01:13:31,870 --> 01:13:35,020
Well, random number is something you can never predict.

658
01:13:35,350 --> 01:13:40,630
Okay. It's only imaginary if you're doing a quantum computing.

659
01:13:40,780 --> 01:13:46,899
You can argue that quantum, you know, the values calculate for quantum computing is truly random.

660
01:13:46,900 --> 01:13:57,370
But you will never know. You cannot prove it. Okay. So it's a very hard to generate the one or one or two or test is a random or not.

661
01:13:57,460 --> 01:14:05,980
So there is a site called random and orig, if you use it for doing the group assignment that they claim that they're using some atmospheric,

662
01:14:06,430 --> 01:14:15,280
you know, some, some cosmic, the value to, to generate the random numbers are actually the atmospheric noise.

663
01:14:15,280 --> 01:14:20,380
Right. So. Okay. So pseudo random number.

664
01:14:20,560 --> 01:14:25,330
What is the random number? So random number is not a random number.

665
01:14:25,810 --> 01:14:31,030
It is a deterministic sequence of number. But that it looks like random number.

666
01:14:31,990 --> 01:14:36,940
Okay. So it's a so good random number, pseudo random number.

667
01:14:36,940 --> 01:14:42,610
Generators should be, uh, have a two properties.

668
01:14:43,360 --> 01:14:48,790
One is that you it has to be reproducible sometimes the region is region y,

669
01:14:48,790 --> 01:14:55,840
we are using pseudo random number is that if if everything is random, then you cannot reproduce.

670
01:14:56,440 --> 01:15:01,580
Sometimes you want to have some sort of random looking number.

671
01:15:02,020 --> 01:15:06,999
And I do use use the randomize algorithm, but you want to reproduce, reproduce it.

672
01:15:07,000 --> 01:15:14,560
So to be able to reproduce, you need to start from the seed and the the algorithm should be all deterministic.

673
01:15:15,340 --> 01:15:22,660
But if you don't know the seed, this the for the users, this should look like random.

674
01:15:22,750 --> 01:15:26,990
You cannot predict on this, you know, until you see the seed.

675
01:15:27,510 --> 01:15:34,050
Okay. So that's the good property of the pseudo random number generators.

676
01:15:34,390 --> 01:15:40,660
Okay. So all random number we're talking about here is going to be pseudo random number.

677
01:15:40,840 --> 01:15:48,370
Okay. Just to be clear. Okay. So y random number are useful in statistical method.

678
01:15:48,880 --> 01:15:52,720
Okay. Well, we're using random numbers a lot.

679
01:15:53,020 --> 01:15:59,350
Okay. And when you re sampling from data like permutation and bootstrapping, you're using that.

680
01:16:00,370 --> 01:16:09,459
If you use a simulate the data from using to test the statistical method, you are using random numbers, invasion statistics,

681
01:16:09,460 --> 01:16:16,570
sometimes CMC method like a keep sampling or you know to simulate annealing and you know those random what

682
01:16:16,570 --> 01:16:25,810
kind of algorithm all user applied relies on these random property of these random pseudo random numbers.

683
01:16:26,140 --> 01:16:33,400
Okay. In outside of the statistical statistical area you with.

684
01:16:34,180 --> 01:16:37,340
So there's a there's something called a hashing. Okay.

685
01:16:37,840 --> 01:16:48,969
Hashing is is there something that you, uh, you may want to use to efficiently make a dictionary like a store?

686
01:16:48,970 --> 01:16:52,390
If you know what the python is, you know what the dictionary is.

687
01:16:52,390 --> 01:17:05,080
Basically make a key value pair into memory, but it make it very efficient to store and store and lookup.

688
01:17:05,080 --> 01:17:11,950
So that that's a hash tag, a data structure called hash, and it requires some good hashing function.

689
01:17:11,950 --> 01:17:16,689
Basically, you give a key and you should give a random number that is a you know,

690
01:17:16,690 --> 01:17:24,610
that is also if you read the key random number, meaning that just, you know, random mapping of the key to the key key space.

691
01:17:25,330 --> 01:17:34,540
So if you if you have a good number random generator, you can use that to to generate a very good hash function in the cryptography.

692
01:17:36,250 --> 01:17:40,640
You can use them for encrypting stuff.

693
01:17:40,780 --> 01:17:47,830
So if you have a very good pseudo random number generator plus actually what you can do is that you can have your seed,

694
01:17:48,490 --> 01:17:53,050
okay, to keep a secret and generate a pseudo random number based on the seed.

695
01:17:53,360 --> 01:17:57,160
Okay. Then just to give this a random number.

696
01:17:57,400 --> 01:18:01,300
Okay, then people can access the seed.

697
01:18:01,780 --> 01:18:07,209
Okay. So they cannot reproduce or they cannot, you know, decrypt the information.

698
01:18:07,210 --> 01:18:17,080
But if I have the seed, you can you can you can price those number or you can decrypt them for whatever, whatever way it's needed.

699
01:18:17,770 --> 01:18:29,970
So that's how you can. Use these pseudo random number four or cryptography in general, and that can be used a digital signature or you know,

700
01:18:30,130 --> 01:18:36,610
in many different settings in cryptography is a really interesting area if you're interested in doing it.

701
01:18:36,610 --> 01:18:41,410
But it's very there is a very statistical part in this also are going to be part here.

702
01:18:41,830 --> 01:18:51,690
Okay, so this is a reader cartoon basically, you know, through a random number, the there difficulties that you can never know.

703
01:18:51,700 --> 01:18:57,459
So if someone say, oh, I want to have a random number generator and some someone says nine,

704
01:18:57,460 --> 01:19:02,350
nine, nine, nine, nine, nine, you can guess that that's not random.

705
01:19:02,830 --> 01:19:10,450
Okay, but you can never be sure. Right? So too random number is hard to generate automatically.

706
01:19:10,450 --> 01:19:12,640
Very hard to provide a true randomness.

707
01:19:12,850 --> 01:19:23,440
So we're relying on the pseudo random numbers and this is how you generate a one way to generate the random numbers.

708
01:19:23,710 --> 01:19:31,750
So because we have a we have one minute, I'm going to say that we're going to use a very large prime number to do this.

709
01:19:31,930 --> 01:19:38,200
Okay? And this is just the, uh, yeah, yeah, this is just the introduction.

710
01:19:38,200 --> 01:19:44,680
And I'm going to repeat from this slide and I'll go through the random number part from next lecture.

711
01:19:45,670 --> 01:19:49,150
Well, we don't have a lecture on Monday, just so you know.

712
01:19:49,420 --> 01:19:53,680
Okay. And I'll see you on next Wednesday.

