1
00:00:02,750 --> 00:00:05,880
You don't have that? No.

2
00:00:05,910 --> 00:00:12,930
I mean, I don't think that very question represents. Just reminds you of something.

3
00:00:14,490 --> 00:00:18,000
I'm in Switzerland and there's not a lot of. How's it going?

4
00:00:18,750 --> 00:00:23,540
That's my. Ah, you know, that's the one you just added right.

5
00:00:24,090 --> 00:00:27,180
No. That 670601 is.

6
00:00:27,210 --> 00:00:39,560
I know you do. Yeah, yeah, yeah, yeah. That's my response, which is why I haven't got the 652 exam in this day and age.

7
00:00:39,690 --> 00:00:42,810
I'm like, no, I have a 90%.

8
00:00:42,990 --> 00:00:50,270
Oh, you're right. I'm trying. It seems like it's open for a while.

9
00:00:50,460 --> 00:00:59,040
I thought so. You know, my last time I thought it was in class F in 601.

10
00:00:59,580 --> 00:01:03,900
Oh, no, no, no, no, no, no, no, no.

11
00:01:04,200 --> 00:01:07,919
I don't know. That was not stressful at all. It was like 24 hours.

12
00:01:07,920 --> 00:01:13,080
Oh, okay. I know you're making me nervous. You're like, oh, no, it's an F.

13
00:01:13,960 --> 00:01:20,490
Okay. Yeah, she's like, really old school, like, good in class.

14
00:01:20,850 --> 00:01:24,600
Like, she's really strict about, you know, like, really paranoid about she.

15
00:01:25,320 --> 00:01:33,510
Yeah, yeah, yeah. It's gotten better, better and better.

16
00:01:33,900 --> 00:01:44,190
Getting better. Thinking about the good things, you know, like trying to come up with a list every day of, like, ten good things.

17
00:01:44,340 --> 00:01:50,579
Oh, that's good. You know, just as a 30, you know, Sheryl Sandberg is I've heard the name.

18
00:01:50,580 --> 00:01:54,200
She was the CEO of Facebook. Oh, that's right. She stepped down.

19
00:01:54,200 --> 00:01:59,030
Now she writes about how she starts every day, blessed in three forms of gratitude.

20
00:01:59,040 --> 00:02:02,729
Yeah, I, I've been through a number of things like that.

21
00:02:02,730 --> 00:02:07,110
Yeah. So, like, the leaves are down. Michigan beat Rutgers.

22
00:02:07,410 --> 00:02:12,420
Yeah, I had tailwinds on my bike riding on what else?

23
00:02:13,830 --> 00:02:17,790
I thought of a lot of things on my way and now I can't remember any of them. It was sunny this weekend.

24
00:02:17,820 --> 00:02:20,310
Sunny this weekend. My favorite class this morning.

25
00:02:21,360 --> 00:02:31,020
Some Halloween candy for those of you that earn it today, can you help hear some of the good things?

26
00:02:34,560 --> 00:02:43,870
World Series is over. Terms of service, halfway done, more than halfway down this break.

27
00:02:44,410 --> 00:02:48,640
What else? We're alive for good health.

28
00:02:48,820 --> 00:02:53,750
We went up in the rankings. For which? Rankings of all rankings were number three.

29
00:02:53,970 --> 00:02:58,750
Three? Yeah. Okay, so that's good. Well, that's a good, good.

30
00:03:01,030 --> 00:03:07,210
Okay. Well, anyway, thanks for coming today. This is the second half where you'll see me talking about this subject.

31
00:03:07,480 --> 00:03:13,240
Rigor, reproducibility, transparency. As an announcement I sent you.

32
00:03:15,730 --> 00:03:21,160
It was actually a big opinion piece in The New York Times from Wednesday last week.

33
00:03:21,730 --> 00:03:28,540
Science that asking for solving problems. And did you guys have a cure for depression?

34
00:03:30,280 --> 00:03:39,700
What's the deal with Photoshop in science? And this article contains the things that have been produced.

35
00:03:40,240 --> 00:03:49,000
But I think you're going to be paying for it. Flip it over to me and say that's a new indication.

36
00:03:50,020 --> 00:03:54,230
Yeah. And they got away with it, right. What else?

37
00:03:54,970 --> 00:04:09,050
Who discovered this? There's Elizabeth Dick, a microbiologist who work at Stanford in the Dutch National Institute.

38
00:04:09,890 --> 00:04:16,040
And I guess she's now making a career of discovering fraud because she says

39
00:04:16,040 --> 00:04:20,180
she's been blessed with better than average ability to spot repeating patterns,

40
00:04:20,780 --> 00:04:31,970
which is pretty cool. You know, and so the times when I try to copy something from there, it doesn't work that well because they have,

41
00:04:34,700 --> 00:04:42,020
you know, sort of animated, interactive things on the website and it's just you can't get that in the typical PDF.

42
00:04:42,530 --> 00:04:54,080
But she looks at, you know, some of the and, and, you know, these patterns tell scientists a lot about what's going on.

43
00:04:54,440 --> 00:05:02,450
And then she also has I don't know if it's shown here, some of the pictures they have flipped and so forth.

44
00:05:04,820 --> 00:05:13,640
Still not quite over what's called not Google, but I don't know if those pictures are in here or not.

45
00:05:14,240 --> 00:05:19,450
But I mean, it's really simple stuff that she got away with or that others got away with it,

46
00:05:19,460 --> 00:05:26,690
which is kind of set up an independent foundation or something.

47
00:05:27,020 --> 00:05:34,490
And she's discovered since 2014, image duplication in this country.

48
00:05:34,730 --> 00:05:42,830
Authorities are collection, you know, and this is out of 100,000 that we've looked at.

49
00:05:43,040 --> 00:05:50,510
So this woman is a machine who's looking at these articles, finding these cases in intimate relations by hand.

50
00:05:50,840 --> 00:05:59,140
I would think, first of all, that an eye technique would do the point that, you know, but also whatever.

51
00:05:59,150 --> 00:06:14,480
It's still work. So what do you think about that? What a creative job are company deserves.

52
00:06:15,500 --> 00:06:20,960
It's good that someone's doing that. Why is it so hard to do it?

53
00:06:22,580 --> 00:06:33,500
Good question. I mean, I think if you could figure out a business model, then someone would do who would be interested in paying for it.

54
00:06:36,020 --> 00:06:45,260
Do we talk about who should fund research? Should we put a tax on research?

55
00:06:46,610 --> 00:06:49,760
Should university do something like that?

56
00:06:53,650 --> 00:06:55,750
It's a really good idea that somebody.

57
00:06:56,350 --> 00:07:04,630
I feel like if you came across an author who was doing similar work and you were worried that maybe they were copying you,

58
00:07:06,040 --> 00:07:14,530
then maybe you would pay for it. I would like. I know, but I think I'm the only one of the only way that consists of paying for it.

59
00:07:14,710 --> 00:07:19,120
So sort of authors in an area doing that.

60
00:07:19,120 --> 00:07:24,820
But I mean, if I'm publishing a certain area and I know who my editors are, I'm going to look at their stuff.

61
00:07:24,970 --> 00:07:30,220
Right. So I sort of do it, but I typically don't book for fraud or anything like that.

62
00:07:30,430 --> 00:07:33,430
Yeah. So it's a good idea.

63
00:07:33,460 --> 00:07:38,410
I mean, if you could figure out a business model, it should happen to the government because.

64
00:07:42,260 --> 00:07:52,190
Do you know what my sense is that once there's an awareness that somebody is policemen out there, people tend to behave.

65
00:07:53,300 --> 00:08:05,720
Not everybody, but most people. And if things are such that you can get away with anything, then bad actors are more willing to do that.

66
00:08:05,950 --> 00:08:10,670
So what's our current approach for dealing with this right now?

67
00:08:12,710 --> 00:08:17,920
Go to work. We talked about this last week with you right now.

68
00:08:18,530 --> 00:08:31,250
I mean, so we have this notion that peer review is kind of an insistence on a self-correcting, holistic, comprehensive, if you will, approach.

69
00:08:32,720 --> 00:08:35,810
And, you know, we talked about some failings in the peer review system.

70
00:08:36,260 --> 00:08:47,030
And, you know, in the PowerPoint slides, some of them were listed as described here, as are cognitive fallacies.

71
00:08:47,750 --> 00:08:54,790
This isn't really peer review, but I mean, just some of the biases that we bring to the subject seems to you.

72
00:08:59,310 --> 00:09:03,600
So I'm still very intrigued by this woman. Actually, I wrote about her some years ago as well.

73
00:09:04,080 --> 00:09:16,900
And, you know, I was thinking about all the data that we generate, and it comes from various types of sources, but it's often part of visual patterns.

74
00:09:16,930 --> 00:09:24,870
It's either these blogs or chromatographic piece, or if you're doing the genetics,

75
00:09:24,870 --> 00:09:31,470
it's Google Pixel for lighting, well, different colors, or if you're doing remote sensing things there.

76
00:09:31,890 --> 00:09:38,610
I mean, everything is individual in some ways, and that stuff is so easily manipulated.

77
00:09:40,230 --> 00:09:45,540
How many of you take pictures with your camera mean you can manipulate the pictures,

78
00:09:46,860 --> 00:09:54,900
crop them, take the colors, rotate them, Photoshop things in or out that you like or dislike.

79
00:09:56,250 --> 00:09:57,620
I mean, everybody for.

80
00:10:00,450 --> 00:10:12,360
So yeah, I don't know where it's going to go, but if one of you wants to make clear that, it's like, okay, just figure out how to get paid to do it.

81
00:10:14,880 --> 00:10:25,800
Okay. Well, returning to corrective mechanisms, I put this up here because we need to be aware that we have these mindsets,

82
00:10:25,800 --> 00:10:29,570
these cognitive fallacies when we're looking at research or anything else.

83
00:10:29,580 --> 00:10:34,500
Okay. We talked about this last week. This all make sense to you so we can move forward.

84
00:10:35,430 --> 00:10:43,510
Okay. And you. So we talked about these.

85
00:10:47,330 --> 00:10:56,630
And you know, at the bottom here, some thoughts from some folks at Stanford who are saying, you know, understanding the failure,

86
00:10:56,900 --> 00:11:04,970
our failure to understand our own biases is something that we should understand and we should try to take our big steps.

87
00:11:05,240 --> 00:11:13,370
Okay, so that's great. So how do we do that? So, you know, recognizing the issues is the first thing.

88
00:11:15,350 --> 00:11:19,700
What factors can cause bias? What's wrong with the current system?

89
00:11:20,040 --> 00:11:29,300
We've talked about publisher Perry's economic incentives, the promotions that need that you need to be able to publish.

90
00:11:30,350 --> 00:11:34,940
What's wrong with the current system? We talked about who's getting paid to do it.

91
00:11:35,900 --> 00:11:43,250
Predatory journalist, things like this. What are the advantages and disadvantages and challenges of an open peer review system?

92
00:11:44,060 --> 00:11:47,150
What? I mean open. You know, the identity of everybody. Right.

93
00:11:49,670 --> 00:11:56,930
What structures, incentives, rules might work? What are the same challenges moving forward to improve reproducibility in recruitment?

94
00:11:57,290 --> 00:12:01,650
So all of this is prefaced on, you know, we have a problem.

95
00:12:01,680 --> 00:12:08,600
Houston, Right. Okay. I was wishing for the underdogs to open the series.

96
00:12:08,600 --> 00:12:12,470
That's why I saw that Philadelphia was playing Boston anyway.

97
00:12:13,340 --> 00:12:18,770
You know, peer review is important. Having transparency and credibility is important.

98
00:12:19,430 --> 00:12:23,960
And when we have challenges, we need to figure out how to move forward.

99
00:12:24,980 --> 00:12:29,470
I don't believe we have all the solutions. Okay, to evolve in Brussels.

100
00:12:31,220 --> 00:12:36,740
What news I would suggest in an article are ways to be bias ourselves.

101
00:12:37,880 --> 00:12:41,390
And these are pretty straightforward and I think I talked about this briefly last week.

102
00:12:42,440 --> 00:12:45,950
The Devil's Advocacy. Consider alternative hypotheses.

103
00:12:46,430 --> 00:12:56,300
See if your data works with those, then we typically use sham outcomes sometimes in a study,

104
00:12:56,750 --> 00:13:03,800
and if we get an association with those sham outcomes, something's wrong or it shouldn't work.

105
00:13:04,760 --> 00:13:17,360
I mean, if I'm looking for, I don't know, cognitive outcomes in a study and I put in guy outcomes that my participants are feeling and there's no

106
00:13:17,360 --> 00:13:25,220
possible way that a G.I outcome is associated with some of the factors or various that I'm concerned.

107
00:13:26,360 --> 00:13:31,120
It's a good test, not sufficient, but it's helpful.

108
00:13:33,260 --> 00:13:39,680
We talked about pre-commitment publicly declaring data collection analysis plan before starting a study.

109
00:13:39,950 --> 00:13:47,990
So I lay out my methodology, my inclusion criteria, my stopping group is my data analysis plan, all of this kind of thing.

110
00:13:48,650 --> 00:13:57,110
If you're writing a proposal, people are going to want to see that this is something that you do before you see what the data says.

111
00:13:57,650 --> 00:14:06,950
And so, as I mentioned, I think last week to folks don't typically do this because they want the data to guide them.

112
00:14:06,950 --> 00:14:13,340
A lot of times, especially with data science, it's actually the things that we're seeing, the teams of rivals,

113
00:14:13,700 --> 00:14:22,010
your adversaries to collaborate with an interesting approach and employing data analysis using essentially synthetic datasets.

114
00:14:23,120 --> 00:14:32,540
I guess there have been a number of so-called models, shoot outs or data tests or something like that,

115
00:14:32,540 --> 00:14:37,190
where a synthetic dataset is given to different teams of researchers across the globe.

116
00:14:37,910 --> 00:14:44,360
And they come up with their analysis and it should agree it often does.

117
00:14:45,200 --> 00:14:52,849
And this is a really interesting technique. And the neat thing about synthetic data set is I know what the right answer is.

118
00:14:52,850 --> 00:14:57,740
And so that's different from giving you a real data set, even giving different teams,

119
00:14:58,190 --> 00:15:01,760
different people can come up with different answers, but I still don't know what's true.

120
00:15:03,740 --> 00:15:19,840
How would I generate a synthetic data dataset? Yeah.

121
00:15:19,840 --> 00:15:30,430
I mean, you could use a I but you could use a model to generate it and then contaminated, put noise in it, send it out to people.

122
00:15:30,790 --> 00:15:37,299
The model tells you, Yeah, I have this association between a pollutant and a neurological outcome.

123
00:15:37,300 --> 00:15:48,270
I build that into the data. So it's good. Thoughts about this approach is not routinely done.

124
00:15:52,000 --> 00:15:58,570
This should be done. It's easy for individual scientists to do this should be done, and it's often done.

125
00:15:59,680 --> 00:16:07,890
These two are unusual, rare. Some other things.

126
00:16:10,050 --> 00:16:17,720
A Nobel Prize winner, Levi Strauss, cut off his last name, says ask the right questions.

127
00:16:17,740 --> 00:16:21,570
The scientist is not a person who gives the right answers. He's the one who has the right questions.

128
00:16:23,070 --> 00:16:26,130
So how do you identify the right question?

129
00:16:27,120 --> 00:16:30,629
So he suggests a number of things, localize it. Space and time.

130
00:16:30,630 --> 00:16:33,330
Find its cause, find its mechanism, what's going on.

131
00:16:35,070 --> 00:16:47,400
And this basically brings us to the apotheosis testing our scientific paradigm, where I have a specific question,

132
00:16:48,510 --> 00:16:58,500
I design an experiment test, I see a change in something specific, gives me the answer any way I expect.

133
00:16:59,010 --> 00:17:04,560
I can use statistics to prove a hypothesis testing.

134
00:17:05,730 --> 00:17:10,320
And we're incrementally changing one thing at a time and learning something and moving forward.

135
00:17:11,250 --> 00:17:16,140
So basically the scientific method, this is okay.

136
00:17:16,760 --> 00:17:28,860
It's very granular. I mean, it's not big picture stuff, but the point is something should be tested and if it's testable, others contested this well.

137
00:17:33,140 --> 00:17:40,280
So to have something which is testable, which can be seen as reproducible, others can do it.

138
00:17:40,730 --> 00:17:47,030
And rigorous. We need an experimental design, which is good.

139
00:17:47,800 --> 00:17:51,980
And the five requirements are listed here.

140
00:17:52,790 --> 00:17:58,070
Your designing should be unbiased. Should be precise.

141
00:17:58,700 --> 00:18:02,960
Wide range of accessibility. Simple, elegant.

142
00:18:03,590 --> 00:18:08,350
You should be able to calculate the uncertainty, maybe use of replicates or duplicate specific exam.

143
00:18:10,040 --> 00:18:21,100
There's obviously blinding randomization. So again, in this article, Nuzzo is trying to bring this to easy to understand level.

144
00:18:21,530 --> 00:18:28,130
And in the cartoon two old scientists are talking and saying there's a flaw in your experimental design.

145
00:18:28,910 --> 00:18:32,930
All the mice are scorpions. What is that?

146
00:18:36,280 --> 00:18:39,580
What do you think? Yeah.

147
00:18:41,260 --> 00:18:44,350
They're not Sagittarius. They're not Pisces. What's going on?

148
00:18:47,080 --> 00:18:51,520
Yeah. Okay.

149
00:18:54,380 --> 00:18:59,300
In fact, one of the new videos is talking about selection of animals.

150
00:18:59,300 --> 00:19:02,300
And if all the animals are from the same strain or something like that,

151
00:19:02,570 --> 00:19:08,450
and that strain reacts in a weird way to your essay or something like that, it's not that it's not reproducible,

152
00:19:09,260 --> 00:19:22,549
but it's also kind of like NIH these days wanting to make sure that if it's relevant to have different ethnicities, different genders,

153
00:19:22,550 --> 00:19:30,710
etc. in your experimental design because things that you conclusions that you draw on a subset may not be generalizable.

154
00:19:34,890 --> 00:19:39,910
Anything else I should mention is that you guys read this article.

155
00:19:39,930 --> 00:19:46,470
I gave it to you. So I've already talked about using a scientific method.

156
00:19:46,860 --> 00:19:50,400
Part of the scientific method is a review of the existing literature.

157
00:19:50,670 --> 00:19:54,090
They're not starting from ground zero or building on others.

158
00:19:54,600 --> 00:20:03,960
And you need to ask the next question if you want to think of it that way in the history of analysis in your area.

159
00:20:05,070 --> 00:20:08,550
We talked about peer review a bit, making peer review scientific.

160
00:20:09,240 --> 00:20:14,580
Last week we talked about Danny Drummond, Brennan's Rather review.

161
00:20:16,720 --> 00:20:21,580
Something we haven't talked much about is transparency. This is going to open your eyes a little bit.

162
00:20:22,540 --> 00:20:30,400
We want people to be able to communicate and disseminate results, provide enough information to be able to, in fact,

163
00:20:30,760 --> 00:20:45,010
understand and presumably possibly reproduce the experiment and make all of this information available in these days with supplemental information.

164
00:20:45,460 --> 00:20:50,470
We click on another button when you're reading a journal and you can get dozens,

165
00:20:50,470 --> 00:20:57,040
if not hundreds of pages of information about what was done, stuff that's often available.

166
00:21:00,820 --> 00:21:07,230
Another issue in being transparent is providing good data presentations, and I'll show you a slide on that in a second.

167
00:21:08,880 --> 00:21:18,330
And then having a culture in systems that are structured so that they promote transparency and rigor and reproducibility.

168
00:21:19,440 --> 00:21:27,329
So, for example, if I need help with statistics or data presentation or something like that, I should be able to use university.

169
00:21:27,330 --> 00:21:31,170
Here, let me get some resources to do that.

170
00:21:32,210 --> 00:21:36,800
Nice. So should you when you're writing a tweet.

171
00:21:45,670 --> 00:21:52,900
So NIH really is been promoting this topic of rigor and reproducibility since

172
00:21:54,100 --> 00:21:59,290
I'm going to say about five or seven years or something like that where they

173
00:21:59,290 --> 00:22:04,059
came out with this ninth module added to the eight that you already were involved

174
00:22:04,060 --> 00:22:10,750
in and emphasized that this might be the most important one and that all,

175
00:22:10,750 --> 00:22:19,060
of course is there and know how to enhance reproducibility of research findings through increased scientific rigor.

176
00:22:19,480 --> 00:22:29,140
Transparency. NIH is doing work on the NSF on the order of $100 billion worth of research.

177
00:22:29,380 --> 00:22:38,860
They don't want to waste their money. They have a obligation to promote science and technology and be efficient.

178
00:22:38,860 --> 00:22:47,150
And so. They want to get funding from Congress to do this.

179
00:22:48,800 --> 00:22:54,020
And it's sort of in their own interests to make sure that it's as good as it can be.

180
00:22:55,040 --> 00:23:02,130
So the agencies themselves have self-interest, which is interesting to think about that.

181
00:23:05,300 --> 00:23:10,070
But I think more broadly is kind of the, you know, public perception of the research.

182
00:23:11,570 --> 00:23:19,820
They want to raise awareness, begin a culture shift in the scientific community, given that they've been at this for over five years,

183
00:23:20,480 --> 00:23:32,820
given that we've seen, like the Duke lawsuits and fines, there's certainly increased awareness of this activity.

184
00:23:34,190 --> 00:23:38,540
I'm not sure it's a culture shift yet, but certainly awareness is increasing.

185
00:23:43,530 --> 00:23:49,830
And when I say prompt applicants to consider issues that they may have downplayed or ignored,

186
00:23:50,100 --> 00:23:53,820
what they're talking about here are applicants for research grants,

187
00:23:55,320 --> 00:24:00,780
because this is really directed to scientists who are writing or loans or whatever

188
00:24:00,780 --> 00:24:06,150
types of awards or grant proposals to arrange for an sample for whatever agency.

189
00:24:06,930 --> 00:24:14,100
Okay. So your application for funding needs to discuss.

190
00:24:14,460 --> 00:24:18,240
Now, this is new in an explicit section.

191
00:24:18,630 --> 00:24:21,950
Reproducibility and reproducible.

192
00:24:24,270 --> 00:24:27,570
Before that was not an explicit requirement.

193
00:24:29,130 --> 00:24:32,160
Today there are a couple more slides.

194
00:24:35,050 --> 00:24:41,230
So again, this is kind of for folks writing for and most of you are masters students.

195
00:24:41,230 --> 00:24:45,100
So it's probably doesn't apply immediately.

196
00:24:45,370 --> 00:24:53,480
But you should understand that this is something that NIH is investing in to improve performance overall.

197
00:24:55,560 --> 00:25:11,570
Think that you can refer to that. What's interesting is that the vast majority of scientists have learned this material indirectly.

198
00:25:13,460 --> 00:25:18,700
You know, it has lots of courses. They're mandated, okay.

199
00:25:18,830 --> 00:25:23,360
For trainees, for students and so forth. But ten years ago, they weren't.

200
00:25:27,280 --> 00:25:39,280
And I move on. So again, you know, just updates since this is actually the date it's 2016.

201
00:25:39,280 --> 00:25:42,940
So it's now seven years since this call has been out.

202
00:25:46,280 --> 00:25:50,860
Okay. So I never really defined for you reproducibility.

203
00:25:51,340 --> 00:25:52,660
And here is the definition,

204
00:25:54,010 --> 00:26:01,180
the closeness of the agreement between results of measurements of the same measurement and carried out under changed conditions of the measurement.

205
00:26:02,200 --> 00:26:08,109
In other words, we're reproducing the experiment using different conditions, different principle of measurement,

206
00:26:08,110 --> 00:26:13,030
different method of measurement, different people or animals, whatever, in different locations.

207
00:26:15,250 --> 00:26:21,880
Know that there's a difference between replication, which is using the same experimental method,

208
00:26:22,780 --> 00:26:28,080
maybe the same assays and so forth, and then seeing how close you are to referable.

209
00:26:28,990 --> 00:26:41,060
So reproducibility is much more rigorous record and it would be changed conditions leading to the same conclusion and then.

210
00:26:44,880 --> 00:26:51,060
Now, this does not define high quality research by itself, but it's an instrumental aspect.

211
00:26:51,640 --> 00:26:55,080
High quality research would include the reproducibility aspect.

212
00:26:55,500 --> 00:27:01,680
I don't think it can be reproducible once it's replicable. You need accuracy of repeatability as well.

213
00:27:02,250 --> 00:27:05,940
And then you want to have it as transparent and well.

214
00:27:06,630 --> 00:27:14,820
And there's actually a whole sort of textbook that in how you choose to go to deal with this notion of reproducibility.

215
00:27:16,860 --> 00:27:22,530
We do good on the difference between these two replications reproducible results.

216
00:27:23,240 --> 00:27:28,440
So how many of you have had, of course, an experimental design?

217
00:27:31,740 --> 00:27:32,040
Yeah.

218
00:27:33,120 --> 00:27:46,140
So, you know, experimental design basically came out of the social sciences and it's a way or method to figure out how best to approach a problem.

219
00:27:46,560 --> 00:27:52,800
Typically, you have a controlled experimental factor, you've manipulated and other things are constant.

220
00:27:53,250 --> 00:28:01,650
And you again use the scientific method to try to learn about what's affecting the system.

221
00:28:03,030 --> 00:28:10,290
You know, variants of this are essentially the proper method for inquiring after the properties

222
00:28:10,290 --> 00:28:14,670
of things Isaac Newton said is to deduce them from experiments we've all done.

223
00:28:15,400 --> 00:28:18,510
But that nail doesn't move. You hit it harder, okay?

224
00:28:18,750 --> 00:28:26,520
I mean, there's obviously lots of practical sense, but there's so many choices in experimental design.

225
00:28:26,970 --> 00:28:32,430
Okay. And I've listed a number of them here. Choosing the right controls.

226
00:28:34,660 --> 00:28:41,140
This is a massive problem. If you're doing human subject work, how do you find controls?

227
00:28:43,150 --> 00:28:46,480
What's the most common way to investigate a disease?

228
00:28:46,810 --> 00:28:51,580
Case control method, right? How do you find those controls?

229
00:28:58,520 --> 00:29:03,920
With its prospective study typically more powerful than a retrospective study.

230
00:29:04,790 --> 00:29:08,150
I'm going to recruit everybody that comes into a clinic with a disease.

231
00:29:09,080 --> 00:29:11,480
Okay. Whatever. You're investing diabetes.

232
00:29:11,990 --> 00:29:20,360
We're going to compare them to people without diabetes, maybe match them in, match them in gender and so forth.

233
00:29:20,750 --> 00:29:28,530
I will find those people. How many of you have ever been involved in a story like this?

234
00:29:30,450 --> 00:29:33,540
You have to recruit the right people that come in who are sick.

235
00:29:34,560 --> 00:29:38,170
Then another form. You consent to be in the study. Why not?

236
00:29:38,190 --> 00:29:45,720
Okay. How do you find that healthy people? It's really hard, really challenging.

237
00:29:47,220 --> 00:29:51,540
I mean, we pay them, but still, it's really hard. Really. There are professional subjects out there.

238
00:29:52,600 --> 00:29:55,610
Okay. I don't know that they're suffering anyway.

239
00:29:55,650 --> 00:30:03,450
This is a big deal. Estimating experimental error and statistical power.

240
00:30:04,620 --> 00:30:10,890
You've had statistics, you know how to calculate confidence intervals, do hypotheses, tests and so forth.

241
00:30:11,460 --> 00:30:15,570
Hopefully that's well, we talked about replicates.

242
00:30:16,850 --> 00:30:21,110
What about pulling samples? What does that mean?

243
00:30:26,470 --> 00:30:33,590
But you know. It's up to the other inmates.

244
00:30:34,860 --> 00:30:38,580
Yeah. So it's more kind of like an ecological design sometimes.

245
00:30:38,910 --> 00:30:42,030
There are a lot of issues here. Sometimes you have to do that.

246
00:30:42,630 --> 00:30:52,170
But statistical control care and compared data for mean by peer to peer data.

247
00:30:54,150 --> 00:30:59,160
For example, case control could be paired or it might look that you are more important time

248
00:30:59,160 --> 00:31:03,850
and you at a different point in time and compare the same subject to themselves.

249
00:31:03,850 --> 00:31:07,650
So that's an example of peer to peer also is often much more powerful.

250
00:31:09,120 --> 00:31:14,820
And then lastly, the type of experimental design, whether observational intervention,

251
00:31:14,910 --> 00:31:20,040
randomized controlled trial, different types of study designs, pictorial and so forth.

252
00:31:20,310 --> 00:31:26,520
So all of these are things that affect experimental design.

253
00:31:27,600 --> 00:31:33,370
Anything else? We're talking a lot today. Sorry about the good.

254
00:31:35,490 --> 00:31:42,270
So it's really hard to come up with an experimental design which is rigorous and reproducible.

255
00:31:42,570 --> 00:31:50,540
On your own, you need to build some others. And you also need to be self-critical to do something that you do.

256
00:31:56,510 --> 00:32:03,110
Okay. A couple other concepts. Validity is one which is really not well discussed in the literature.

257
00:32:03,650 --> 00:32:10,880
This is the extent to which a concept, conclusion or measurement is well-founded and likely corresponds accurately to the real world.

258
00:32:11,480 --> 00:32:20,690
And this decision of the concept of conclusion or measurement is valid is based on a lot of different factors.

259
00:32:20,810 --> 00:32:30,350
Some of those are most appropriate. People will say all the time, Oh, this is valid, but it's not really representing the real world.

260
00:32:31,910 --> 00:32:45,290
And I have a whole hierarchy of validation from sort of validated and validated, and some of those terms are listed here,

261
00:32:46,070 --> 00:32:52,730
and they go for various verifications, calibrations, evaluations of validity, validation.

262
00:32:55,010 --> 00:33:01,700
You know, we do a lot of stuff with models, we do a lot of stuff with environmental epidemiological models.

263
00:33:02,120 --> 00:33:05,870
Very few of them are, what I would say, validated at this point.

264
00:33:07,130 --> 00:33:11,810
I would say most of them are verified. You don't change one parameter.

265
00:33:11,850 --> 00:33:21,709
The model output changes the way I expect it to. That tells me that it works the way it's supposed to be calibrated so that it matches the

266
00:33:21,710 --> 00:33:29,330
data that I have evaluated to see how well it matches the datasets that are available to me.

267
00:33:30,260 --> 00:33:33,169
But typically I don't have that many data sets,

268
00:33:33,170 --> 00:33:39,770
and so validity means I have to test it with lots of datasets from all over the country, world, whatever.

269
00:33:40,100 --> 00:33:48,850
It's hard to do. So very few models or analysis will proceed all the way down.

270
00:33:49,520 --> 00:33:56,810
Then you guys have seen this, you know, when we're talking about the efficacy of a vaccine, the latest study on the weekend came out.

271
00:33:57,070 --> 00:34:08,240
The latest COVID vaccine is good for four or five times the amount of antibodies compared to the prior vaccine.

272
00:34:08,720 --> 00:34:18,170
That was just around 50 people. But I don't think that's enough to give me this higher assurance of fluidity.

273
00:34:18,830 --> 00:34:28,130
It's an evaluation. So you're going to see these terms before discussed and distinguish this word.

274
00:34:29,020 --> 00:34:32,360
The difference between evaluation and validity is a different.

275
00:34:35,820 --> 00:34:40,210
I keep that in mind when you're talking about other aspects.

276
00:34:41,990 --> 00:34:45,160
Well, there's so much stuff here. Blinding.

277
00:34:46,660 --> 00:34:50,980
We talked about this. Should the investigator be aware of the outcomes?

278
00:34:54,330 --> 00:35:01,290
This is a real issue. Randomization, we're looking at analysis.

279
00:35:01,800 --> 00:35:07,500
A lot of times I can get rid of potential biases by randomization.

280
00:35:09,240 --> 00:35:16,740
So in other words, if I want to test every one of you, I should not say young people first.

281
00:35:16,860 --> 00:35:23,100
Old people next week. Maybe my instruments shifts during the test, something like that.

282
00:35:23,790 --> 00:35:31,710
Better to reduce blinding and randomization are pretty important elements to reduce these biases.

283
00:35:33,180 --> 00:35:41,460
There's a video clip to if you have some time to do that would be good on this one.

284
00:35:42,850 --> 00:35:47,380
Keep an eye on your hands when you gets a few almond joys if I'd like to encourage them.

285
00:35:48,970 --> 00:35:55,000
Replication we talked about. There are different types of replicates that would start from a slide biological,

286
00:35:55,000 --> 00:36:02,920
technical replicates, sample size, anybody not understand the need for sample size.

287
00:36:03,700 --> 00:36:08,529
So you have enough statistical power to hand out statistics.

288
00:36:08,530 --> 00:36:12,850
Plus, I hope, good outliers.

289
00:36:13,660 --> 00:36:19,750
This is really an interesting issue because we often define outliers using some arbitrary definition,

290
00:36:19,750 --> 00:36:23,680
like three standard deviations away from a median or something like that.

291
00:36:24,640 --> 00:36:29,410
Careful outliers may be the most valuable data points you have.

292
00:36:30,340 --> 00:36:42,280
They may also be the most messed up ones yet. I can't tell that that they need some investigation, exclusion criteria or inclusion criteria.

293
00:36:42,760 --> 00:36:43,690
Those are important.

294
00:36:48,430 --> 00:36:56,770
And again, there's another video clip, which is kind of a funny one because they were excluding female outliers, not while I was here.

295
00:36:59,050 --> 00:37:03,070
And then consideration of relevant biological variables.

296
00:37:04,270 --> 00:37:07,749
And this needs to be included. So in your proposal.

297
00:37:07,750 --> 00:37:16,750
So again, this is something new in an age that if you're writing a proposal, you have to be very explicit on what variables are testing.

298
00:37:20,430 --> 00:37:31,290
Okay. Uh, there's a need to authenticate the key assays or biological or chemical resources that you're using.

299
00:37:32,010 --> 00:37:39,330
Okay. So don't go into your chemical cabinet and pull out a standard that's five years old.

300
00:37:39,630 --> 00:37:42,510
Okay. It's got to be very basic stuff.

301
00:37:44,130 --> 00:37:58,260
And, um, this authentication is important, and there's a lot of guidance on how to do this, which I'm not gonna do.

302
00:38:00,060 --> 00:38:06,120
I talked about producing results, which can be interpreted correctly.

303
00:38:06,750 --> 00:38:11,489
This is kind of interesting. Here's a very simple dataset sample.

304
00:38:11,490 --> 00:38:15,030
Very sample B, who got five or ten values?

305
00:38:16,650 --> 00:38:21,030
And then there are three different representations of the same data.

306
00:38:22,410 --> 00:38:26,940
On the left, is that the mean plus or minus the standard error?

307
00:38:28,690 --> 00:38:33,120
They look very much the same. Etc.

308
00:38:33,600 --> 00:38:37,020
Then we have the mean. I guess we're showing the data points.

309
00:38:37,420 --> 00:38:40,800
Uh huh. Now I can start to see a difference. Right.

310
00:38:41,340 --> 00:38:49,350
Sample is all clustered together. A very precise sample, really, of two separate categories.

311
00:38:51,670 --> 00:38:57,780
You know, the third basically combination of the two.

312
00:38:59,070 --> 00:39:03,780
5%. My data this way. I'm missing this pattern.

313
00:39:04,650 --> 00:39:18,790
Completely misleading. Look at your data point by point.

314
00:39:20,170 --> 00:39:26,770
This is far more transparent than this being or the last one is equally good, I guess,

315
00:39:28,600 --> 00:39:38,860
except it might sort of imply something that might be wrong given the means are showing the open science.

316
00:39:39,010 --> 00:39:43,120
How many of your work is true? You've heard.

317
00:39:44,320 --> 00:39:48,460
Access. Open access journals. Right. Just click on them together.

318
00:39:48,790 --> 00:39:57,940
It's basically the same thing. But for the data and more than that, often with respect to the methods that you use and so forth.

319
00:39:59,800 --> 00:40:07,750
So the idea here is that researchers should share all the material that underlies the reported results.

320
00:40:08,620 --> 00:40:13,780
That includes the methods code, often the datasets.

321
00:40:15,250 --> 00:40:26,770
And again, the fourth here is that this increases transparency and reliability, makes it more equitable, hopefully increases trust and credibility.

322
00:40:28,030 --> 00:40:33,880
So there is a whole term called open science here, which I've taken from a handbook.

323
00:40:34,330 --> 00:40:41,049
Open Science, Inclusive Principles, transparency, reduce participation, accountability in practices,

324
00:40:41,050 --> 00:40:45,520
open publication, data sharing system, community science and so forth.

325
00:40:47,040 --> 00:40:50,950
This is really important. Okay. And something that's pretty new.

326
00:40:53,020 --> 00:41:00,160
The open science folks being scientists have a taxonomy.

327
00:41:00,690 --> 00:41:10,330
Okay. And so what open science encompasses is your lady in this diagram here.

328
00:41:10,510 --> 00:41:18,040
So there's open access, there's open data, there's valuation, there's policies of data science tools and so forth.

329
00:41:18,280 --> 00:41:28,750
It's a whole big system. In part, this is the way a European project called Foster is doing.

330
00:41:29,620 --> 00:41:35,220
This is. They are taking this expansive view.

331
00:41:35,610 --> 00:41:39,600
If I will open access publication, for example, here.

332
00:41:41,310 --> 00:41:48,090
So what's more open data journals or something? Unless the rest of this stuff is also open.

333
00:41:48,480 --> 00:41:52,290
They're not considering it. Open source. It's a subset.

334
00:41:53,520 --> 00:41:57,990
Maybe this is a dream. I don't know. But it's an interesting direction to pursue.

335
00:42:02,240 --> 00:42:10,460
So we're two more slides and we'll be wrapping up. What this has translated to and this is an important take home point,

336
00:42:10,910 --> 00:42:18,920
is something that is starting to perceive now in the US and elsewhere in terms of data and methods.

337
00:42:19,460 --> 00:42:28,790
And it's something called the Fair Principles. There stands for findable, accessible, interoperable and reusable.

338
00:42:29,670 --> 00:42:42,380
Okay. And basically it's a way to structure your data so that other people now or late in the future or far in the future can use it.

339
00:42:44,330 --> 00:42:58,070
And so, you know, these fair data principles are something that I have started to see us being requested to comply with for data repositories.

340
00:42:59,550 --> 00:43:05,660
They typically they're not quite here yet, but they're moving in that direction.

341
00:43:06,920 --> 00:43:18,320
And so you typically have a metadata file which describes all the parameters that were used to generate the file or the data sources,

342
00:43:19,460 --> 00:43:22,940
the methods and so forth, the definitions.

343
00:43:23,360 --> 00:43:35,870
And then you have the data afterwards. And the design of these things is being done in association with the background of these data principles.

344
00:43:40,170 --> 00:43:46,409
So it's a very interesting approach to think about regarding reproducibility because it puts it all out there that I can

345
00:43:46,410 --> 00:43:55,680
come back with a new technique and analyze the data that you collected to verify your results or maybe learn new things.

346
00:43:59,420 --> 00:44:08,750
And many, many of the research projects and products that we're seeing currently are using other people's data.

347
00:44:11,740 --> 00:44:17,530
In public health. Can you give me some examples of giant data studies?

348
00:44:21,070 --> 00:44:28,960
And you've all heard of ventilators, right? Thousands of publications on the state because it's open access.

349
00:44:29,890 --> 00:44:35,230
It's been defined as more or less complying with these principles.

350
00:44:36,520 --> 00:44:40,150
There are any other studies you can do for data sources.

351
00:44:41,660 --> 00:44:46,570
So that's what the study of women across the nation.

352
00:44:47,050 --> 00:44:56,410
Okay. This is older women, for example, or I think there's 1 to 2000 publications for that study.

353
00:44:57,550 --> 00:45:03,520
Nurses study another study. It's also there's lots of studies.

354
00:45:04,780 --> 00:45:18,000
Other types of datasets. All the national data, all the remote sensing data that is collected by certainly tens of thousands of studies.

355
00:45:19,510 --> 00:45:24,460
Again, because they comply with these kinds of principles.

356
00:45:29,680 --> 00:45:41,040
Just going to do it. So my last topic here is just a brain teaser for you.

357
00:45:43,130 --> 00:45:49,670
We we're going to have the perfect studies, the absolute correct information for everything.

358
00:45:50,510 --> 00:45:54,319
I mean, you know, it's it's not we're always going to have uncertainties.

359
00:45:54,320 --> 00:46:00,500
We're always going to have gaps in our understanding. But this is what we approach.

360
00:46:02,920 --> 00:46:11,350
Typically with a name of risk assessment or risk management, we make decisions and policies under uncertainty.

361
00:46:12,130 --> 00:46:20,110
And I think the last reading I gave you was a short piece from one of the, I guess,

362
00:46:20,530 --> 00:46:29,320
luminaries in this field that artificial turf calls how we make decisions in the face of incomplete knowledge and uncertainty.

363
00:46:29,530 --> 00:46:34,149
There's so many examples. For example, we have a terrible disease.

364
00:46:34,150 --> 00:46:35,810
There's two strains, okay?

365
00:46:36,670 --> 00:46:46,930
And we have a vaccine that completely protects you from one strain or a second one that confers 50% protection for both strains.

366
00:46:48,100 --> 00:46:53,880
Which one do you take? You can't take them both. How many will choose a?

367
00:46:57,550 --> 00:47:02,170
I mean, which is. Nobody chooses.

368
00:47:04,970 --> 00:47:11,060
Interesting one. We're going to keep that a rhetorical one here.

369
00:47:13,130 --> 00:47:13,400
You know,

370
00:47:13,400 --> 00:47:21,020
currently we're having a huge debate in the country where they're all getting it right now on self-driving cars where they say for the normal person.

371
00:47:26,450 --> 00:47:29,659
I can give you unlimited number of examples, okay.

372
00:47:29,660 --> 00:47:39,530
Because we just don't know all the outcomes. Think there's uncertainty in the subjective decision that you know is correct?

373
00:47:41,330 --> 00:47:46,820
I can't say one way or the other because it depends on your values or preferences or about history and conservatives.

374
00:47:47,360 --> 00:47:51,500
And we need ways to accommodate that subjectivity.

375
00:47:51,770 --> 00:47:59,150
So there's no right answer here. But you need information to make that decision.

376
00:47:59,870 --> 00:48:09,500
You need to understand it. And the ideas here apply for individual decisions.

377
00:48:09,740 --> 00:48:19,400
They apply for societal decisions. Typically, there are pretty well understood list of factors that influence an individual's decision.

378
00:48:20,210 --> 00:48:30,980
Those are listed here. Whether you're familiar with the outcome, whether the risk is voluntary or not, whether it's huge risk or greater risk,

379
00:48:32,090 --> 00:48:39,160
whether there is risk or benefits or whether there is sort of long term aspects to the likelihood of fatalities,

380
00:48:39,170 --> 00:48:42,710
what you get a benefit, the magnitude of the risk.

381
00:48:43,430 --> 00:48:55,669
And this plot here is trying to compare on a couple of axes the way people would approach different types of risks.

382
00:48:55,670 --> 00:49:02,629
For example, lower level of risk might be the use of food colorings or, you know,

383
00:49:02,630 --> 00:49:13,970
if I wanted to use the spray cans, while huge factors might be or ones that are much more fatal, that still red.

384
00:49:14,420 --> 00:49:18,620
I mean, it's just we're all over the place with respect to how we approach these years.

385
00:49:19,280 --> 00:49:24,930
And it's largely because of these types of factors through the physical particle.

386
00:49:24,980 --> 00:49:29,660
It's interesting the way he puts this into context because, again, you know,

387
00:49:29,720 --> 00:49:38,480
the more familiar you are, for example, maybe with an issue, something that may shift your attitude.

388
00:49:39,390 --> 00:49:46,130
And so this is important in convincing people to behave in a different way, potentially.

389
00:49:51,090 --> 00:49:54,690
Anyway. This is a huge topic. I love this topic. I could talk about it for a long time.

390
00:49:55,070 --> 00:49:59,280
Not going to today because we probably post out of time anyway.

391
00:50:00,240 --> 00:50:07,920
Yes, I am probably going to skip this and we're going to talk.

392
00:50:08,460 --> 00:50:20,130
So thank you, folks. More early next week, rather. You have to do this during Dr. Seuss fourth issue.

393
00:50:20,130 --> 00:50:24,750
Okay. I'll send you out. Here you go. Oh, you don't miss.

394
00:50:25,560 --> 00:50:29,700
Oh, yes. You do need to sign this because it will be.

395
00:50:41,720 --> 00:50:49,190
We're going to meet in a minute. But.

