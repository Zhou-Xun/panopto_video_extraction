1
00:00:04,770 --> 00:00:10,660
Yes. Yes, sir.

2
00:00:20,960 --> 00:00:23,970
Sorry. I'm sorry. My handwriting.

3
00:00:24,630 --> 00:00:28,320
No, I think it's that color. Yeah, that is not the best color.

4
00:00:29,400 --> 00:00:35,130
It's my dress. Yeah. I'll read it off at school anyway.

5
00:00:41,880 --> 00:00:57,630
Yes. You have been embraced in classrooms.

6
00:00:57,630 --> 00:01:04,800
Never met phonics. Do you think I was teaching in this room?

7
00:01:04,800 --> 00:01:08,220
I actually went to IKEA, got a big clock and stuck it in the back.

8
00:01:08,220 --> 00:01:31,420
And it stayed here for, like, two years. And then it disappeared. And I. Just resetting in the back to back.

9
00:01:31,420 --> 00:01:36,660
Let's get started. I say thank you.

10
00:01:39,960 --> 00:01:47,320
You. Oh, thank you.

11
00:01:50,320 --> 00:01:59,410
Thank you very much.

12
00:02:01,130 --> 00:02:10,150
She is she just like you said.

13
00:02:17,730 --> 00:02:40,719
So I had talked to Zachary. I just thought I'd like to talk about something else you can talk about this morning.

14
00:02:40,720 --> 00:02:56,030
I'm not going to talking to you.

15
00:02:58,420 --> 00:03:06,430
You know, she's been with me as long as I can.

16
00:03:08,480 --> 00:03:23,360
I think we should be able to go.

17
00:03:29,530 --> 00:03:55,720
Oh, yeah. Only three of us agreed to do something in public life, and most of my friends are supposed to be in the room.

18
00:03:57,280 --> 00:04:02,679
Charlie wrote me, like you said, and then I volunteered myself.

19
00:04:02,680 --> 00:04:16,800
So I'm going to go somewhere I trust way. But I never had another one like anyone else last night.

20
00:04:17,500 --> 00:04:31,780
All right. Let's get started. Some housekeeping, self-criticism while I'm at the entrance to help me along things.

21
00:04:31,870 --> 00:04:36,219
If your name doesn't exist, let me know.

22
00:04:36,220 --> 00:04:39,879
If you don't like the way it's worded, it can edit. Give it back to me.

23
00:04:39,880 --> 00:04:45,850
I'll get it fixed. You can continue to give these back to me and I won't bring them every day.

24
00:04:45,850 --> 00:04:59,270
Or you can keep them. I don't care. But at least for the first month or so I always have them because it'll will help me do it.

25
00:04:59,980 --> 00:05:02,380
I didn't mention this first class. My apologies.

26
00:05:02,740 --> 00:05:11,470
I am automatically recording the class and so you will see course recordings each day being posted in canvas

27
00:05:12,430 --> 00:05:19,389
one to disclose that so that you know that that's occurring to exactly the sound quality is remarkably

28
00:05:19,390 --> 00:05:25,810
good on the square is it have green dots above them are microphones so the room is actually pretty

29
00:05:25,810 --> 00:05:31,200
decent at this what it does the camera is this thing so you get like really good close ups of my image,

30
00:05:33,520 --> 00:05:38,540
but that's going to be there in case you miss a day or you want to go back and you listen to listen to stuff,

31
00:05:38,560 --> 00:05:44,320
etc. I just want to make sure you know that it's a three.

32
00:05:47,030 --> 00:05:51,020
I would be, you know, thank you for doing your musings.

33
00:05:51,920 --> 00:05:56,570
As you'll see today, we'll go through a lot of them and I'll we'll ask people to share.

34
00:05:58,130 --> 00:06:03,530
Reminder, if you ever want to be writing about something that you don't want to share, that is fine.

35
00:06:03,530 --> 00:06:10,399
Just email it to me directly and I will manage that. And if anything ever comes up where I'm asking you to share something and you realize,

36
00:06:10,400 --> 00:06:13,880
Yeah, you know what, maybe I don't really want to be talking about that.

37
00:06:14,090 --> 00:06:21,380
That's fine. But we're going to have sort of every day I have a few things that I want to bring,

38
00:06:21,650 --> 00:06:25,550
but often it's a lot of like, let's build the connections between the stuff that you've talked about.

39
00:06:27,910 --> 00:06:35,170
That means that you should feel free. This doesn't have to be a gluten free conversation.

40
00:06:35,170 --> 00:06:39,100
Always, like we can go back and forth between each other. I want to I want to bring that to the surface.

41
00:06:39,100 --> 00:06:43,870
Like the structure here is nice because it gets lots of people talking. It gets lots of people talking to me.

42
00:06:44,350 --> 00:06:49,600
And I do want to encourage you to be talking to each other when that makes sense and bring up, hey, what about this?

43
00:06:49,600 --> 00:06:52,780
Etc. So.

44
00:06:53,860 --> 00:06:57,550
Just to raise that to our awareness. And then the last thing.

45
00:07:01,740 --> 00:07:11,150
I don't. Next week, we're going to spend a lot of time thinking about the claim between cognition and emotion.

46
00:07:11,900 --> 00:07:21,530
We're going to set up work today, but really diving into that as you do so start to pay attention.

47
00:07:21,770 --> 00:07:25,729
The reason I'm telling you this now, so I want you to think about this before you're doing the reading,

48
00:07:25,730 --> 00:07:31,520
not just afterwards, but pay attention to your own reactions to the risks around you.

49
00:07:33,390 --> 00:07:38,800
And to what degree they are. Conscious thought.

50
00:07:40,420 --> 00:07:50,829
To what degree they are just experiences or emotions that are popping up in you that you may not necessarily know where they came from,

51
00:07:50,830 --> 00:07:55,840
but you know that you have them either. And by the way, again, on both sides.

52
00:07:57,600 --> 00:08:01,750
You may have beliefs about. Risk.

53
00:08:02,680 --> 00:08:06,520
You may have beliefs or cognitions or emotions about safety,

54
00:08:07,210 --> 00:08:12,250
and the latter is at least as important, if not more important than the former attitude to both.

55
00:08:13,040 --> 00:08:17,050
So what I want you to do is a status of your life going into this and then as a revolution,

56
00:08:17,080 --> 00:08:20,230
hopefully you'll be given some sort of structure to think about this.

57
00:08:20,860 --> 00:08:26,379
The first assignment in this class is due next week, the following week,

58
00:08:26,380 --> 00:08:29,410
and it's going to be all based upon the stuff that we're reading about next week.

59
00:08:30,730 --> 00:08:36,700
But the more that you're getting into this sort of self-awareness about different types of risks around us, the easier that will be.

60
00:08:39,420 --> 00:08:47,610
I think that's it for now. We have a few people who weren't here on Tuesday.

61
00:08:47,910 --> 00:08:53,610
You may have gotten more of a chance to read through the syllabus. Any questions that have come up or how we're going to work?

62
00:08:56,720 --> 00:09:02,700
Chris. Whatever. Moments like this.

63
00:09:02,700 --> 00:09:08,190
I always feel like I was scared too into submission. Please do let me know if anything's unclear.

64
00:09:09,390 --> 00:09:16,170
All right. So today I'm going to take a little bit and dove into some of the stuff in the readings

65
00:09:16,170 --> 00:09:20,430
that you probably looked at and you didn't really process as deeply as I want you to.

66
00:09:22,170 --> 00:09:26,940
And the first one, I hit the screen, it sucked down.

67
00:09:27,090 --> 00:09:32,640
You can't use the way I use this one, and we will do the best we can.

68
00:09:46,240 --> 00:09:49,650
It was this diagram.

69
00:09:53,700 --> 00:10:01,490
All work in the 1970s about organizing risks on a two dimensional structure.

70
00:10:01,780 --> 00:10:04,340
This was one of the figures in the Morgan chapter.

71
00:10:05,570 --> 00:10:14,740
This is old work, but it's really foundational to thinking about the types of risks that we deal with in public health and medicine,

72
00:10:14,900 --> 00:10:18,980
health in general. And the basic idea.

73
00:10:21,080 --> 00:10:28,850
It's called the psychometric paradigm. The basic idea is that you can place risks on essentially two orthogonal dimensions.

74
00:10:29,600 --> 00:10:33,740
One of them is uncertainty. Where?

75
00:10:35,290 --> 00:10:40,540
Things that are uncertain, I know and I don't think is unknown by you.

76
00:10:40,690 --> 00:10:43,959
I mean like unknown by anybody in science.

77
00:10:43,960 --> 00:10:46,090
We don't we don't know anything about it.

78
00:10:47,320 --> 00:10:57,070
Are more scary evokes stronger risk reactions than things which we have great volumes of information about that are borne out.

79
00:11:03,850 --> 00:11:08,990
In the other dimension. Is usually abbreviated as dread.

80
00:11:10,730 --> 00:11:18,440
It captures many things. It's likely to kill you is one of them, but by no means only one.

81
00:11:19,250 --> 00:11:23,480
So dread captures not just mortality or severity.

82
00:11:23,810 --> 00:11:27,200
It captures the frankly grossness factor.

83
00:11:27,370 --> 00:11:31,940
Like, how would this affect you? It captures catastrophic ness.

84
00:11:32,570 --> 00:11:36,110
Like, is this going to affect many people is going to be a big event?

85
00:11:36,950 --> 00:11:43,500
Or is this going to be lots of really small events? To take a concrete example of that dread dimension.

86
00:11:44,700 --> 00:11:51,930
Do more people die in mass shootings or in shootings that just occur with one or two people?

87
00:11:52,930 --> 00:11:58,240
One or two people, energy people. Which one evokes the stronger media response?

88
00:11:59,710 --> 00:12:02,830
Mass shootings. Mass shootings are a dread event.

89
00:12:03,520 --> 00:12:05,980
They are catastrophic. They are large. They are visceral.

90
00:12:07,800 --> 00:12:14,610
The background level of gun violence in our society is not as much of a threat, even despite the fact it kills lots of people.

91
00:12:15,090 --> 00:12:21,630
But the individual pieces of it don't evoke the same level of reaction, so we don't pay that kind of attention to it.

92
00:12:22,690 --> 00:12:30,100
What do we pay attention to most of that. So if you look in that.

93
00:12:31,330 --> 00:12:36,399
All were from the 1970s. What they did was they basically gave people a whole bunch of different routes and asked

94
00:12:36,400 --> 00:12:42,520
them to categorize them and then used this structure to sort of map out where things were.

95
00:12:44,230 --> 00:12:47,920
I don't actually care about what the organization was 50 years ago.

96
00:12:48,640 --> 00:12:53,350
What I care about is for you to think about these two dimensions as we look at risks around us today.

97
00:12:54,970 --> 00:13:01,960
So let's take a simple example. Imagine it is March 2020.

98
00:13:03,430 --> 00:13:07,520
You are hearing about COVID. Where is COVID on this dimension?

99
00:13:10,040 --> 00:13:13,550
There's. First quarter meeting where like.

100
00:13:14,710 --> 00:13:24,700
Or even one because it's a massive, bad pandemic that had the ability to impact millions of people, the whole world.

101
00:13:25,040 --> 00:13:29,799
You know, and we didn't know anything about it. Okay. So easily give you that.

102
00:13:29,800 --> 00:13:32,770
We don't know anything about it. Like Kobani came in March 2020.

103
00:13:33,160 --> 00:13:38,379
We literally have no we don't know if it ever we don't know if we're supposed to be washing our groceries.

104
00:13:38,380 --> 00:13:44,020
Remember when we want groceries. Now, the question here is, is it?

105
00:13:44,260 --> 00:13:46,150
Where does it fit in that dread space?

106
00:13:46,540 --> 00:13:56,230
Because, well, we know historically it hasn't affected millions upon millions of people globally at that moment in time.

107
00:13:57,040 --> 00:14:05,200
How many people have been affected by COVID? And we're talking like March 20, 24 cases might have been measured in hundreds globally.

108
00:14:05,650 --> 00:14:14,800
Total cases in the U.S. were measured in single digits. So there is a dread component to it, but it's not like the mass shooting space.

109
00:14:15,250 --> 00:14:20,510
So, I mean, I would put it like. Here, maybe.

110
00:14:21,320 --> 00:14:26,840
I'm not sure. It's all the way over on the drug scale, but it's definitely where the heck up here on the uncertainties.

111
00:14:27,920 --> 00:14:31,160
Okay. So where's Kovac today?

112
00:14:35,910 --> 00:14:44,340
Yeah I think it moved over more towards trend but way down in uncertainty.

113
00:14:44,580 --> 00:14:49,920
So like down here or just more up here? Um, it's still a moderate one.

114
00:14:50,060 --> 00:14:57,930
Right. So you're saying it's like there? Yeah. As we know more about what's going on, I don't want it to happen, like.

115
00:14:58,590 --> 00:15:03,020
Okay. I would say it's further to the left and right.

116
00:15:03,420 --> 00:15:13,650
Um. Yeah. Or, um. Just cause I think now it's like we think of it as, like, the endemic and so are really.

117
00:15:13,770 --> 00:15:22,079
That's what I hear now is the buzzwords. But, um, and I'm not as worried as like at the beginning about people dying and everything.

118
00:15:22,080 --> 00:15:27,149
It's more just so the fact that we have accumulated immunity through vaccination,

119
00:15:27,150 --> 00:15:34,530
through prior exposure, we now have not only more knowledge about how bad it is that it's not,

120
00:15:34,810 --> 00:15:39,990
you know, we're knowledgeable about how likely we are to die from it, but we also have reduced the mortality rate.

121
00:15:40,920 --> 00:15:48,389
So again, there's multiple dimensions here, like we still have millions of people being affected by it.

122
00:15:48,390 --> 00:15:58,200
So on a on a scale standpoint, we're very hot but on is is likely to kill me standpoint maybe not quite as much as it once was.

123
00:15:59,760 --> 00:16:12,240
Yeah. I think it's so far to the left I me because we do have variants now we don't know how the variants impact because it will increase or decrease.

124
00:16:12,690 --> 00:16:19,100
Notice, by the way. What one person's thought is and what another person's thought may or may not be exactly the same

125
00:16:19,490 --> 00:16:24,920
that we are each looking at the same information that we have and placing more or less weight on it.

126
00:16:25,250 --> 00:16:27,680
So one way to look at this is, hey,

127
00:16:27,680 --> 00:16:34,040
we know about lots of different variants of science has now come to greater understanding about the existence of different variants,

128
00:16:34,880 --> 00:16:37,430
a different way of looking at that same sort of information as.

129
00:16:39,190 --> 00:16:44,889
We knew about what was circulating months ago, but we don't know what is circulating now.

130
00:16:44,890 --> 00:16:49,810
And so there remains a level of uncertainty about, you know,

131
00:16:49,930 --> 00:16:55,460
I just saw an article yesterday about shifting away from the Bay five variant of the Bay for whatever.

132
00:16:56,020 --> 00:16:59,800
It keeps changing. And so there's that keeps reinforcing that uncertainty component.

133
00:17:00,340 --> 00:17:08,740
Absolutely. But my point here is notice that my degree of reactivity to the possibility of folding is a function of this.

134
00:17:09,220 --> 00:17:15,960
The more that it feels like a risk that I have a handle on that we know it might be bad, it might not.

135
00:17:15,970 --> 00:17:20,770
But whatever it is, I know what it is. The less strong my reaction is going to be to it.

136
00:17:23,380 --> 00:17:33,730
This idea that risks change over time is a critical feature, especially when we think about novel risks being faced by society.

137
00:17:34,120 --> 00:17:37,570
And I'll give you a historical example that we've actually done some research on.

138
00:17:39,270 --> 00:17:45,540
Back before COVID. The pandemic in Europe ten years ago was 2008 H1N1.

139
00:17:49,730 --> 00:17:58,100
And I have in my former graduate school office maid works for ran the conference with him and he said, What are you working on?

140
00:17:58,100 --> 00:18:06,589
Blah, blah. Is it. Oh, I have this long. We've been doing this longitudinal surveys with the same panel over and over about H1N1 flu risk perceptions.

141
00:18:06,590 --> 00:18:09,770
I think you have longitudinal data about flu risk perceptions.

142
00:18:10,580 --> 00:18:13,760
And we ended up analyzing it and it showed a really interesting pattern.

143
00:18:14,030 --> 00:18:19,700
So the timetable of H1N1 in 2009 was it showed up.

144
00:18:20,930 --> 00:18:29,150
In April of 2009. Few cases really came in.

145
00:18:30,900 --> 00:18:41,490
An effect on the population in the fall of September was about when the peak transit cases went up and it peaked in the night,

146
00:18:41,490 --> 00:18:45,660
November, December, and then it started dropping off when we had January.

147
00:18:45,660 --> 00:18:51,030
So which is not actually that different from what we might expect to see in many influenza years.

148
00:18:51,960 --> 00:18:56,900
So if we think about the time. This was.

149
00:18:58,930 --> 00:19:05,200
Like April and March of the next year.

150
00:19:06,930 --> 00:19:11,510
That case counts. I like this.

151
00:19:15,420 --> 00:19:23,060
I'm. But like we've had with COVID, it big question was, well, okay, can we get flu shots?

152
00:19:23,210 --> 00:19:27,800
Like are we going to have vaccinations that are specific to H1N1 and when will this be available, etc.?

153
00:19:28,410 --> 00:19:38,660
And just to give you the other anchor point here, the vaccinations that were specific to H1N1 started showing up around late September.

154
00:19:39,540 --> 00:19:44,880
There were about right here. When the vaccinations actually showed up at.

155
00:19:46,500 --> 00:19:52,090
So. Anyway.

156
00:19:53,080 --> 00:19:57,100
My friend and his team have been doing these surveys and they kept asking questions.

157
00:19:57,100 --> 00:20:04,110
Two key questions. One. How likely do you think you are to get H1N1 in the next month?

158
00:20:04,560 --> 00:20:10,170
So this is a measure of short term exposure risk.

159
00:20:12,360 --> 00:20:16,410
Sure. How likely are you to get a vaccine when it becomes available?

160
00:20:17,160 --> 00:20:22,010
What? Behavioral intentions? This is the interesting piece.

161
00:20:23,270 --> 00:20:27,800
The pattern that we observe for risk perception.

162
00:20:27,810 --> 00:20:34,520
The likelihood of being infected mirrored the actual pattern of almost perfect.

163
00:20:35,880 --> 00:20:40,800
Most people in June were not thinking that they were likely to get it when and pretty soon,

164
00:20:41,400 --> 00:20:44,280
which was true because it was very it was not circulating very broadly.

165
00:20:44,700 --> 00:20:48,659
And their risk perception started to pick up when the cases started to really go up

166
00:20:48,660 --> 00:20:54,630
in the fall and followed it back down to what behavioral intentions looked like this.

167
00:20:58,110 --> 00:21:03,480
He. Monotonically decreasing.

168
00:21:04,490 --> 00:21:07,160
People were most interested in getting the vaccine.

169
00:21:07,970 --> 00:21:16,460
When we first interviewed them in like April or May, and the longer they went, the less and less motivated people got it.

170
00:21:17,120 --> 00:21:23,870
Which is why when the vaccine actually showed up in late September to October, there was a few weeks of people lining up for it.

171
00:21:24,470 --> 00:21:28,220
And then there was a surplus. There's a whole bunch of people.

172
00:21:28,240 --> 00:21:31,979
Yeah, we know about this. It's not killing everybody.

173
00:21:31,980 --> 00:21:39,630
I'm not that freaked out by it. I've been living with six months of news about this, so I'm no longer actually motivated to go get the vaccine.

174
00:21:41,910 --> 00:21:46,380
I'm familiar from COVID. And the same problem, right?

175
00:21:46,740 --> 00:21:54,270
That moment when we first engaged with the knowledge of this novel disease and the moment when the vaccines became available,

176
00:21:54,480 --> 00:21:56,550
separated by six or eight months at least.

177
00:21:57,470 --> 00:22:07,670
That difference fundamentally changes our willingness to engage in that risk because it's going from uncertain and sliding down over time.

178
00:22:09,800 --> 00:22:14,150
So this is a pattern we see a lot with novel health risks.

179
00:22:14,600 --> 00:22:19,070
The first moment it's new and scary, it's unknown.

180
00:22:19,490 --> 00:22:21,590
So we engage and we want to do something about it,

181
00:22:21,590 --> 00:22:27,440
but we can't do something about it by the time that the vaccine or the preventive behavior comes into place.

182
00:22:28,280 --> 00:22:34,220
It's not so new anymore. And so now it becomes a lot harder to motivate people to engage in the behavior.

183
00:22:36,770 --> 00:22:44,330
By the way, historically there's an interesting historical novel are some of the things that were like way out here in the 1970s.

184
00:22:45,320 --> 00:22:51,170
Like nuclear power. The power is not way out here anymore.

185
00:22:52,000 --> 00:22:58,190
Like we both we know a lot more about it. We also have 50 years of history without having things blow up.

186
00:22:58,820 --> 00:23:07,370
Despite the fact we've had Chernobyl and Fukushima, it's not like we had events that had substantial nuclear explosion.

187
00:23:07,940 --> 00:23:13,580
And yet you can put that in a larger historical frame so it doesn't feel as risky.

188
00:23:14,540 --> 00:23:19,080
Okay, this might be a little tricky question, but based on that behavioral intentions curve.

189
00:23:19,160 --> 00:23:22,430
Yeah. Wouldn't it? Couldn't you argue?

190
00:23:22,430 --> 00:23:28,729
I'm not saying this is correct. Couldn't you argue that it would actually be advantageous to not release more info and to

191
00:23:28,730 --> 00:23:33,380
keep people in uncertainty because they would be more likely to take proactive action then.

192
00:23:35,330 --> 00:23:39,560
Interesting question. You feel like that's a very difficult ethical question.

193
00:23:39,650 --> 00:23:45,500
It is. And let's bring this a surface. We're going to have these kinds of ethical questions throughout this.

194
00:23:48,900 --> 00:23:57,059
My first ethical response is I'm not comfortable with that because of another component, which is the experience.

195
00:23:57,060 --> 00:24:05,070
And we'll talk about this next week. The experience of being at risk, the experience of facing uncertainty is inherently aversive.

196
00:24:05,970 --> 00:24:09,720
Inherently stressful, it causes its own harm.

197
00:24:10,680 --> 00:24:14,880
So withholding information causes harm.

198
00:24:15,750 --> 00:24:21,899
And we are we're going to always face that tension between bombarding people with information,

199
00:24:21,900 --> 00:24:30,750
which has some value to people that can make them feel better about the situation that they are in versus the fact that the more that somebody knows,

200
00:24:30,930 --> 00:24:34,080
that may then make them less freaked out about it.

201
00:24:34,560 --> 00:24:39,900
Which goes to the larger point of the Brewer article of today,

202
00:24:40,530 --> 00:24:50,339
which is to really pause and think about the different goals that we might have in a situation like what is our goal to provide information?

203
00:24:50,340 --> 00:24:55,410
That is it. We just want the information to be out there, want to be available notice,

204
00:24:55,770 --> 00:25:04,230
and we don't care what they believe about it and we don't care what they do about it.

205
00:25:04,620 --> 00:25:13,770
Our only goal is to share the information versus situations in which we care about beliefs.

206
00:25:13,980 --> 00:25:22,600
We care about either elevating beliefs of risk or decreasing beliefs about risk or calibrated.

207
00:25:22,740 --> 00:25:26,069
So making sure that if you have X, then you have higher.

208
00:25:26,070 --> 00:25:29,250
And if you have some kind of x and you have lower those kinds of situations.

209
00:25:30,640 --> 00:25:38,440
And then distinguishing that from the third category, the programs, which is situations in what we care about, is behavior.

210
00:25:39,100 --> 00:25:43,030
And notice the hand here and we don't care about beliefs.

211
00:25:44,730 --> 00:25:48,600
Like we care about behavior. Period. End of conversation.

212
00:25:50,400 --> 00:25:53,700
I don't actually care whether you believe X or Y. I simply do.

213
00:25:56,730 --> 00:25:59,480
And everybody's rolling for that one, too.

214
00:26:00,810 --> 00:26:09,240
So I'm going to pause and go back to some of your musings, because several of you brought up examples that I think are really appropriate for this.

215
00:26:11,640 --> 00:26:14,790
Try and use this to start to learn names.

216
00:26:15,780 --> 00:26:25,620
Abby. Abby, you were thinking about pharmacist's goals in communicating all the kinds of risks that come up in that job description.

217
00:26:25,650 --> 00:26:29,280
Let's start there. What are pharmacist goals in that? Those kinds of conversations.

218
00:26:29,490 --> 00:26:35,350
So I think our primary goal as the pharmacist is have patients take their medications, which is a behavior.

219
00:26:35,490 --> 00:26:35,880
Right.

220
00:26:36,660 --> 00:26:43,190
And then it's followed by we also want them to understand their medications and believe that the medications are important and then helping them and,

221
00:26:43,200 --> 00:26:47,070
you know, all that good stuff. But our primary objective is take your medication.

222
00:26:47,430 --> 00:26:52,720
Well, so it's interesting because I can imagine situations in which a primary goal is to take your medication.

223
00:26:52,770 --> 00:26:57,450
I can also imagine situations in which your primary goal is to build.

224
00:27:00,040 --> 00:27:03,040
Awareness monitoring.

225
00:27:04,660 --> 00:27:13,900
Watching out for specific side effects. So in situations in which many people will not experience X, but a few people will.

226
00:27:14,620 --> 00:27:18,730
I can think of a personal story I have.

227
00:27:19,900 --> 00:27:29,920
Experienced one of the weird black box warnings on Ciprofloxacin, which is a standard antibiotic.

228
00:27:30,760 --> 00:27:33,070
There is a black black box warning as a sort of like these.

229
00:27:33,280 --> 00:27:39,820
Take this seriously, like major things I took several years ago for something I honestly don't even remember what.

230
00:27:40,450 --> 00:27:46,460
And. Two days after I started taking it, I started to get chalk.

231
00:27:48,150 --> 00:27:53,390
She? It turns out there's a black box warning for tendon damage.

232
00:27:53,870 --> 00:27:57,890
Very rare can come back and affect you for months afterwards, etc.

233
00:27:58,610 --> 00:28:02,090
Oh my doctor, they were like, Get off it. Don't take it. Done.

234
00:28:02,660 --> 00:28:07,610
That's a different kind of pharmacist conversation. That's not a motivate you to take it.

235
00:28:07,940 --> 00:28:11,960
That's. Hey, let's make sure you don't harm yourself by taking it out of conversation.

236
00:28:12,890 --> 00:28:20,150
But that's an important distinction, because the if I go into a conversation and say my goal is to promote adherence,

237
00:28:21,320 --> 00:28:25,670
we're not engaging cognition in the situation.

238
00:28:26,720 --> 00:28:30,830
Let's do this. Let's make sure that you know what you need to do and you know when you need to take it, etc.

239
00:28:31,370 --> 00:28:37,220
But if our goal is to avoid side effect harm, now, all of a sudden we have to be talking about beliefs.

240
00:28:38,420 --> 00:28:42,380
What do I need to think about this? Is it safe? What are the things I need to be doing?

241
00:28:42,620 --> 00:28:47,180
Potentially monitoring, etc.? So that's an example in which you have multiple goals.

242
00:28:48,560 --> 00:28:51,320
And part of what we need to be thinking about is, okay, so.

243
00:28:52,990 --> 00:29:01,120
What about the reality that if I spend time talking to a patient about the different possible side effects of their medication,

244
00:29:01,450 --> 00:29:07,330
am I undermining their motivation to keep taking this medication, which is has a net benefit to them?

245
00:29:08,050 --> 00:29:15,550
But if I don't tell them about these side effects, am I exposing them to risk that they might otherwise be able to manage because they didn't know?

246
00:29:19,440 --> 00:29:24,250
Stuck. That's what's challenging about this is that we have multiple goals.

247
00:29:24,620 --> 00:29:33,130
We got to figure out what we're trying to accomplish in a given moment. A similar kind of context came on.

248
00:29:36,740 --> 00:29:40,399
Caleb brought up an example that is worth bringing.

249
00:29:40,400 --> 00:29:44,180
I'll bring it up anyway, because it's one of the major ones. Informed consent.

250
00:29:46,490 --> 00:29:49,940
You're going to go in for a colonoscopy. You're about to start a research project.

251
00:29:50,090 --> 00:29:53,810
Whatever. What is the purpose of informed consent?

252
00:29:56,410 --> 00:30:01,220
About to start an elective surgery. You're in the hospital, you're in the pre operations phase.

253
00:30:01,640 --> 00:30:06,470
The anesthetist comes out and sits there and talks with you about the fact that you're going to have anesthesia.

254
00:30:07,130 --> 00:30:15,210
Why is the physiologist talking to you? What is the purpose of that conversation?

255
00:30:18,720 --> 00:30:23,610
Is it to get you to have the surgery?

256
00:30:24,360 --> 00:30:28,250
Well. Then why are they talking to you in the first place?

257
00:30:28,970 --> 00:30:35,260
Because they're only going to be bringing up risks that come with it. Is it to get you to not take the surgery?

258
00:30:35,280 --> 00:30:42,210
Well, no, that's obviously not the case. We want you to do this. So we're out of the behavior space.

259
00:30:42,840 --> 00:30:50,280
We got to be in the belief space. So what are the beliefs that we might want in the context of informed consent?

260
00:30:51,840 --> 00:31:00,210
Yeah. That that decision to proceed with whatever they're trying to do is the right decision for them and their specific situation with all.

261
00:31:03,550 --> 00:31:10,180
Andrew, can you also say it's about like the deferment of risk in that you're choosing it so that if something does happen,

262
00:31:10,180 --> 00:31:14,860
the hospital can say, well, you chose to make that decision as opposed to taking responsibility.

263
00:31:15,340 --> 00:31:18,970
I know that sounds really bad, but isn't that part of why they actually do it?

264
00:31:19,270 --> 00:31:27,280
Well, so there is a reasonable side in the sense that we we have an expectation in our society that you are informed of risks,

265
00:31:27,280 --> 00:31:35,670
that you are being that you are undertaking. And since most of us are not anesthesiologists, we don't necessarily know what those are.

266
00:31:35,680 --> 00:31:44,620
So there's a requirement for disclosure. I think I would reframe this and talk about this as regret avoidance,

267
00:31:45,250 --> 00:31:52,960
like we're trying to avoid a situation in which something bad happens and somebody said, Well, wait, you never told me that.

268
00:31:53,140 --> 00:31:55,960
I never knew that that might happen to me.

269
00:31:57,560 --> 00:32:05,510
That's a failure, because if they do know that and they choose to continue, then then there's some degree of acceptance of that risk.

270
00:32:05,510 --> 00:32:09,799
But if I didn't even know that that was possible, maybe I wouldn't have done it.

271
00:32:09,800 --> 00:32:13,370
Or maybe I would have, for example,

272
00:32:14,810 --> 00:32:20,120
made sure to tell you some piece of information about my genetics or the drugs that I'm taking or something

273
00:32:20,120 --> 00:32:24,650
else that would have mitigated that risk because I never knew that it was possible I couldn't do that.

274
00:32:25,850 --> 00:32:33,080
So there is a lot of the informed consent space is about avoiding that kind of negative outcome where you never knew it was possible.

275
00:32:34,430 --> 00:32:45,270
Noted. That conversation is less about probability and more about possibility.

276
00:32:48,340 --> 00:32:58,660
We aren't defining success by the patient being able to give me a quantitative estimate of the exact likelihood of a complication of the anesthesia.

277
00:32:58,990 --> 00:33:01,960
That is not the standard to which we are holding ourselves.

278
00:33:02,590 --> 00:33:09,430
Instead, we're talking about something like Does the patient know that it is a rare possibility that someone might have,

279
00:33:09,430 --> 00:33:15,570
say, breathing problems as a result of anesthesia? If they don't know that, then we have to say, hey, that's a problem.

280
00:33:18,960 --> 00:33:22,650
So that's. We're getting into beliefs here. We have to talk about like what kind of beliefs?

281
00:33:23,710 --> 00:33:28,060
I just believe, like we have very specific beliefs that we might have for it.

282
00:33:29,020 --> 00:33:33,580
Oh, well, I wasn't going to say, but I was gonna to say that they also could serve as a function of trust,

283
00:33:34,450 --> 00:33:42,099
because even visiting doctors for some communities feels risky because of historical situations that have

284
00:33:42,100 --> 00:33:48,370
happened where individuals didn't have the option to have informed me so that they will tell you something.

285
00:33:48,670 --> 00:33:54,520
And we have I mean, there are many, many historical layers here.

286
00:33:56,410 --> 00:34:01,210
Even putting aside the differential patterns of behavior in different communities.

287
00:34:01,710 --> 00:34:09,700
But we have a history of not just paternalism in medicine, but basically imposing of outcomes on people.

288
00:34:10,800 --> 00:34:17,490
You have people you know, historically we've had women who go in for what they think of as electro surgery, who come out with a hysterectomy.

289
00:34:18,950 --> 00:34:28,559
And that's happened. We have in part imposed expectations about informed consent, not just to inform the patient,

290
00:34:28,560 --> 00:34:31,980
but to require a pattern of behavior from the health care provider,

291
00:34:32,990 --> 00:34:39,660
a kind of a disclosure of owning the risks or owning the outcomes that might result from a behavior.

292
00:34:40,890 --> 00:34:44,190
This, by the way, is another major component of risk communication.

293
00:34:44,190 --> 00:34:51,090
And I want to it's relevant for what we're talking about here, because what Breuer is talking about sharing of information.

294
00:34:51,450 --> 00:34:56,010
One reason why we have miscommunications that are about sharing of information

295
00:34:56,640 --> 00:35:02,550
is situations in which I don't actually expect the recipient to act on it.

296
00:35:03,690 --> 00:35:08,010
But by forcing the disclosure. The organization.

297
00:35:08,100 --> 00:35:16,290
The doctor adds that the source is required to actually own that that risk exists and to disclose it.

298
00:35:17,290 --> 00:35:30,580
To take a simple example. If any of you ever gotten or seen your a water quality report.

299
00:35:33,010 --> 00:35:33,400
So what's it?

300
00:35:35,700 --> 00:35:45,780
A lot of data and most people can interpret exactly a lot of numbers about concentrations of various potential contaminants in our drinking water.

301
00:35:47,430 --> 00:35:52,710
Most people have no idea what's good or bad, except that we probably ought to have less of these things.

302
00:35:54,720 --> 00:36:04,410
So why is it that in many jurisdictions it is mandated that the water supply, the water systems send a water quality report to every homeowner?

303
00:36:05,040 --> 00:36:07,920
What is the purpose of that communication?

304
00:36:08,010 --> 00:36:13,440
Do I think that the homeowners are actually becoming more informed about the underlying quality of their water?

305
00:36:14,770 --> 00:36:21,990
Not really. But do I believe that there is a policy purpose for requiring disclosure of that information?

306
00:36:22,020 --> 00:36:32,670
Absolutely. And so the sharing of information in that context, because both requiring the water system to actually own what is the level of.

307
00:36:34,160 --> 00:36:39,980
Are safe, but safe in the water and something that speaks to trust.

308
00:36:41,330 --> 00:36:47,630
I might have greater trust in my water system knowing that this report is being generated, even if I don't understand a word of it.

309
00:36:51,630 --> 00:36:55,350
That's another kind of purpose we might have for communicating risk.

310
00:36:56,190 --> 00:37:03,000
It's not actually about what the user would do with it. It's about ensuring that that information is out there because it changes the system.

311
00:37:05,510 --> 00:37:13,130
And if we think about a lot, there are lots of public policies related to risk disclosure that have those characteristics.

312
00:37:14,570 --> 00:37:22,100
Producers don't want to have to disclose the fact that their product has X and therefore they change their product to make them safer.

313
00:37:23,910 --> 00:37:27,170
Sarah. That's another dynamic.

314
00:37:29,210 --> 00:37:32,780
All right. More examples. Let's do that.

315
00:37:32,780 --> 00:37:39,170
One zero. You know, you're talking about seeing some signs about people dying on the beach.

316
00:37:39,800 --> 00:37:42,470
Yeah, I recently went to the beach and there were, like,

317
00:37:43,160 --> 00:37:50,690
kind of discretely place signs about a recent E coli outbreak and elevated levels of people away on the beach.

318
00:37:51,230 --> 00:37:57,709
And I was kind of taken aback because I had already gone into the water and the beach, I later learned, had been previously closed.

319
00:37:57,710 --> 00:38:04,480
So I felt kind of uncomfortable that that information wasn't more likely to stay away from the beach.

320
00:38:04,880 --> 00:38:09,200
It wasn't posted at the park entrance. Or maybe it was it was like a sheet of paper.

321
00:38:09,740 --> 00:38:15,470
So let's think about this. We have bacterial contamination at a public space.

322
00:38:16,280 --> 00:38:20,060
What is the underlying goals of communication about that information?

323
00:38:20,480 --> 00:38:24,960
So. In the situation when the beach was closed.

324
00:38:25,050 --> 00:38:29,610
Somebody somewhere said, our purpose is behavior.

325
00:38:29,730 --> 00:38:32,790
We want nobody going in the water. Nobody is to be on the beach, period.

326
00:38:33,360 --> 00:38:38,729
Notice, by the way, that this becomes a communication about the fact the beach is closed and the fact

327
00:38:38,730 --> 00:38:42,570
that it's e or the level of concentration or whatever is sort of a secondary point.

328
00:38:43,650 --> 00:38:46,950
But your story is interesting to me because the beach was not closed.

329
00:38:48,710 --> 00:38:54,530
So what's the purpose there? I mean, your experience of it was not happy.

330
00:38:55,930 --> 00:39:00,490
You. You came out of this concerned about what you would had already done.

331
00:39:02,460 --> 00:39:10,130
So what was the goal? Or maybe we can all do a good job of doing what it should have been doing.

332
00:39:10,140 --> 00:39:14,250
But this is a this a classic public health communication. Let's break it down.

333
00:39:14,250 --> 00:39:16,410
What's it doing right? What's it not doing right? Yeah.

334
00:39:16,560 --> 00:39:22,830
I mean, it might seem counterintuitive and it could be poorly done, but it still does influence behavior in the sense of I mean,

335
00:39:22,830 --> 00:39:28,229
so if it was equal, obviously closed, lower concentrations are still not supposed to necessarily drink the water.

336
00:39:28,230 --> 00:39:32,110
They might not want to get the water in your eyes, put your head underwater, stuff like that.

337
00:39:32,130 --> 00:39:35,310
However, it does sound like it wasn't actually communicated.

338
00:39:35,310 --> 00:39:45,060
And again, notice that if our purpose was to say, don't drink the water, then that should have been the primary message of the communication.

339
00:39:45,060 --> 00:39:54,180
Not there is e coli at the beach because if I know that there is e coli, but I don't know that that means that I shouldn't drink the water.

340
00:39:54,630 --> 00:40:00,830
We have failed our primary objective. So, yes, this this gap between beliefs and behavior.

341
00:40:01,050 --> 00:40:06,020
But I can have the belief that there is risk at the beach because there is E coli.

342
00:40:06,840 --> 00:40:12,360
But unless I can map that to the behaviors that we want to do, I might be sitting there freaking out,

343
00:40:12,360 --> 00:40:16,080
but actually continuing to drink the water or do whatever these risky behaviors are.

344
00:40:18,430 --> 00:40:21,580
So specificity. Like what are we?

345
00:40:22,060 --> 00:40:25,880
Is our goal to satisfy disclosure requirement?

346
00:40:25,900 --> 00:40:31,780
They tested it and we're posting the results. Is our goal to say, hey, maybe going to the beach today is a good idea?

347
00:40:32,170 --> 00:40:34,060
Is our goal to say, no, you can't do this?

348
00:40:37,940 --> 00:40:44,990
And this is an example where I'm not sure it was clear in the moment that you were going what the what the public health purpose should have been.

349
00:40:45,320 --> 00:40:49,550
And that's the communication didn't really reflect that well. Uh.

350
00:40:52,840 --> 00:40:56,790
Let's see. Where is next. Oh, Sydney.

351
00:40:59,760 --> 00:41:04,799
The product labels you saw Intuit. Interesting example.

352
00:41:04,800 --> 00:41:10,360
We don't have those kinds of things here, but we've talked about them. So share what you what you saw and then I want to comment.

353
00:41:11,100 --> 00:41:18,659
Yeah. So I was just visiting Chile back in 2018, of course,

354
00:41:18,660 --> 00:41:26,090
and I was in a grocery store and noticed like between one and four, like really big like black stuff, science,

355
00:41:26,160 --> 00:41:32,280
like almost hexagons on like packaging of foods that are like sugars and saturated

356
00:41:32,280 --> 00:41:41,610
fats as well as like beverages and lot of like high in calories on the warning.

357
00:41:41,700 --> 00:41:48,599
But it kind of like very so some have you know for some have to and so from there I kind of made my

358
00:41:48,600 --> 00:41:54,120
decisions based on those shapes and kind of looked for foods that maybe like if I'm comparing a few foods,

359
00:41:54,540 --> 00:42:01,320
one that had like the less number of ex-convicts choose to buy that and also the unhealthier foods were more expensive.

360
00:42:01,620 --> 00:42:06,960
Yeah. So also opted to buy less and stock as well.

361
00:42:07,170 --> 00:42:12,930
So I'm just one from an outside observer. What did you learn from seeing those stop signs?

362
00:42:13,590 --> 00:42:20,580
You learned that someone has classified this product as, say, high in sugar.

363
00:42:22,300 --> 00:42:26,920
You know how many grams of sugar ridden could you?

364
00:42:28,990 --> 00:42:35,290
Calibrate that quantitative knowledge in any way against like how much sugar you normally consume.

365
00:42:35,650 --> 00:42:42,400
Like you think about nutrition labels in the U.S., we get grams of sugar on nutritional labels and we even sometimes get sort of like

366
00:42:42,400 --> 00:42:46,750
recommended daily amounts or maximum levels and we're left doing number calculations.

367
00:42:47,230 --> 00:42:50,680
They took all that out of base of the Fed. If the label is here, you're hot.

368
00:42:53,940 --> 00:43:02,310
And this is a transformation of information that you might have used to calibrate into something that is much more persuasive.

369
00:43:03,360 --> 00:43:07,140
So what happened? You tended to avoid things with lots of signs on.

370
00:43:08,150 --> 00:43:14,510
Not because you were consciously trading off the volumes of, you know, am I eating too much sugar?

371
00:43:14,510 --> 00:43:22,700
But because you had a relatively unconscious reaction to the warning label saying this is a bad thing.

372
00:43:25,580 --> 00:43:34,780
Let's just also pause in the moment. While it is true that consuming excess sugar is a bad thing, we all need sugar to survive.

373
00:43:35,110 --> 00:43:39,150
So it's not like slapping a label on something saying there's led it.

374
00:43:39,970 --> 00:43:47,130
Like there's somebody somewhere making a judgment and imposing a emotional.

375
00:43:49,340 --> 00:43:56,600
Evoking categorization scheme that says this level is good, this model is not good, and translating that into a message.

376
00:43:57,110 --> 00:44:02,180
It is much more about pushing you towards or away from consuming that kind of a belief.

377
00:44:03,410 --> 00:44:08,360
So that was a behavior. Inducing type of communication.

378
00:44:09,340 --> 00:44:15,400
You're not sitting there engaging in. Well, yeah, I know it's got high in sugar, but, uh.

379
00:44:15,970 --> 00:44:22,090
But it's better on this, so I'm going to. No, no, no. You're just, like, avoid it because the communication is set up to do that.

380
00:44:22,780 --> 00:44:31,900
There are lots of warning labels and classification labels that do exactly that, where we take quantitative information and turn it into.

381
00:44:34,030 --> 00:44:39,100
Duchess Charles. Things that are going to make us have emotional reactions.

382
00:44:41,110 --> 00:44:48,459
Yeah. It was part of his strategy, especially just keeping a warning sign without the violent information that

383
00:44:48,460 --> 00:44:53,230
part of the strategy that it was actually inducing some risk of unknown that,

384
00:44:53,230 --> 00:44:56,230
oh, I see the warning sign. I don't know what's underlying it.

385
00:44:56,500 --> 00:45:03,910
And then he avoided first part of the threat. So I don't think it's so much unknown, so much as it's engaging.

386
00:45:04,450 --> 00:45:07,440
And this is why this is a foreshadowing where we're going to go for next week's.

387
00:45:08,190 --> 00:45:15,780
It's in having you engage with the risk at a qualitative level, not quantitative on an emotional level, rather than a cognitive one.

388
00:45:17,860 --> 00:45:22,060
If I remember right, Sydney, the labels were stop sign shaped.

389
00:45:23,140 --> 00:45:27,280
That was intentional. We have emotional associations with things that say stop.

390
00:45:30,460 --> 00:45:34,230
By the way, start paying attention. We look at labels. What is red?

391
00:45:34,240 --> 00:45:38,140
What is green? So for example.

392
00:45:40,570 --> 00:45:49,960
In the grocery store. This is not really on this topic, but in the grocery store products which are making themselves as low fat.

393
00:45:51,250 --> 00:45:56,140
Almost always have some degree of branding that is a particular color of cream.

394
00:45:58,680 --> 00:46:04,400
You look at the low fat. Potato chips in Green Bay and of.

395
00:46:05,820 --> 00:46:09,870
Why? Because we have positive emotional associations with the color green.

396
00:46:10,860 --> 00:46:13,380
And they're translating the quantitative.

397
00:46:14,390 --> 00:46:24,880
Information about the amount of fat into the qualitative emotional feeling of better versus fat, much like the one that you studied yesterday.

398
00:46:25,100 --> 00:46:28,489
Do you think the effectiveness of the information versus changing beliefs?

399
00:46:28,490 --> 00:46:32,150
Class behavior depends also on the level of uncertainty.

400
00:46:32,570 --> 00:46:34,370
So like because I was thinking like with the labeling,

401
00:46:34,370 --> 00:46:39,050
like we see like packs of cigarets that had like warning labels like this could be harmful for your health.

402
00:46:39,260 --> 00:46:43,850
We all understand that. No one's ever like, Oh, I don't understand. Is this like, what should I do?

403
00:46:44,210 --> 00:46:49,390
But then with like the E coli breakout where that's a little bit more uncertain of like the harms of E coli.

404
00:46:49,850 --> 00:46:55,549
Then just saying like, Oh, there's the presence of E coli that could be more confusing without a direct like,

405
00:46:55,550 --> 00:46:58,710
you should do this or you shouldn't do that. On.

406
00:47:04,520 --> 00:47:08,390
I'm not saying there aren't situations in which there's uncertainty components to this.

407
00:47:10,580 --> 00:47:13,760
It's certainly the case. I guess I don't see it so much in.

408
00:47:14,870 --> 00:47:24,140
At Lens as much as it is. Does the Public Health Authority, the source of the communication, to use Brewer's words?

409
00:47:24,650 --> 00:47:32,900
Is there do we know what the right thing for people to do is in situations in which we as the communicator,

410
00:47:33,440 --> 00:47:37,730
can ethically stand on a belief that there is a right answer?

411
00:47:38,240 --> 00:47:41,240
You should quit smoking. Smoking is harmful to you, etc.

412
00:47:41,900 --> 00:47:45,410
We are far more comfortable then going down the pathway of changing behavior.

413
00:47:45,620 --> 00:47:52,400
Being very explicit, being nudging, etc. Is that driven by the level of scientific knowledge?

414
00:47:52,430 --> 00:47:55,520
Well, yes, to some degree, but it's not really the primary issue.

415
00:47:57,640 --> 00:48:02,650
I honestly think that the holy example is an interesting one because we know a lot about E coli.

416
00:48:03,100 --> 00:48:06,220
That is not an unknown risk from a scientific standpoint,

417
00:48:08,980 --> 00:48:15,520
but we're relatively uncomfortable or poor about translating that knowledge

418
00:48:15,910 --> 00:48:22,920
into what is the right level of instructions that we want to give to public.

419
00:48:25,270 --> 00:48:28,030
In part because we get pushback.

420
00:48:28,780 --> 00:48:35,830
People will say, Why are you not letting me go to the beach in a way that we have, at least thus far in our society, gotten to?

421
00:48:35,980 --> 00:48:42,340
We don't have people saying, why are you not willing to let me smoke? Now, 40 years ago, we had more of that conversation,

422
00:48:42,340 --> 00:48:49,360
like the culture of acceptance of smoking risk is fundamentally different today than it was 40 or 50 years ago.

423
00:48:50,430 --> 00:49:00,110
But. When we get into decision making around communicating about beliefs versus communicating about behavior, we're also getting into autonomy.

424
00:49:01,930 --> 00:49:05,350
How much is the authorities saying you can't do this?

425
00:49:06,700 --> 00:49:11,620
Versus leaving it up to you, because if we think this is in our economy space.

426
00:49:12,040 --> 00:49:17,770
Well, you could know that there is E coli and say, yeah, I don't care, I'm just going to go swimming anyway.

427
00:49:19,530 --> 00:49:23,880
And there are times in which we need to have that conversation.

428
00:49:23,910 --> 00:49:25,830
You will hear me later on in the course.

429
00:49:25,830 --> 00:49:33,000
Talk about a very specific example in which I think public health massively overreaches in terms of the way it imposes.

430
00:49:33,210 --> 00:49:37,080
You should not do this when we might choose to accept that risk.

431
00:49:37,890 --> 00:49:43,110
I'm not saying we always should be in the space of truth of communicating about changing behavior,

432
00:49:43,620 --> 00:49:52,020
but we just need to be aware of when we're comfortable doing that and when we're not. Yeah, we're good.

433
00:49:57,370 --> 00:50:03,670
Lisa Yeah, you raise a really interesting point in your views about getting people to care.

434
00:50:04,600 --> 00:50:09,610
So I want to keep probing that. So talk to me about what led you there and then I want to follow up on it.

435
00:50:10,300 --> 00:50:15,390
Well, I was thinking about in both readings that talked about like the purpose of precipitation

436
00:50:15,790 --> 00:50:20,380
is to share information the way that makes people change behavior or change beliefs.

437
00:50:20,800 --> 00:50:26,950
But I was thinking that even with some of the most effective policies or even like talking to a health care worker,

438
00:50:27,280 --> 00:50:34,060
and I'm telling you there a risk for a certain disease or a certain thing, people still will not care.

439
00:50:34,570 --> 00:50:41,920
And if you really think of a saying my mom always says, which is people usually don't care until something bad happens to them.

440
00:50:42,310 --> 00:50:47,410
And I related it to in my high school, I came from like a very small town.

441
00:50:47,740 --> 00:50:49,270
Everybody knew each other in my high school.

442
00:50:49,630 --> 00:50:56,950
And there was a student who got into a car accident and lost his ability to walk because he wasn't wearing a seatbelt.

443
00:50:57,250 --> 00:51:02,020
And it made a bunch of people in my school who were already wearing a seatbelt wear their seatbelt,

444
00:51:02,020 --> 00:51:07,990
because when this happened and affected someone that they know, like, oh, we have to do something, this could happen to me.

445
00:51:08,500 --> 00:51:13,899
So yeah, even though we know the dangers of not wearing a seatbelt and the importance of wearing a seatbelt,

446
00:51:13,900 --> 00:51:19,480
it's hard for many people in my community a tragic event for them to change their behavior.

447
00:51:19,570 --> 00:51:23,200
So a couple of things I want to bring up about. One, car accidents.

448
00:51:24,240 --> 00:51:28,180
Go back over here. Where are our car accidents? Over here.

449
00:51:30,890 --> 00:51:35,850
Do you think I want to put their weight down? We know a lot about currencies.

450
00:51:35,870 --> 00:51:39,380
They are part of our background. But we've been we've had cars for a hundred years.

451
00:51:39,770 --> 00:51:47,180
We're all familiar with them. We all know that accidents happen. We all have received accidents or aftermath of that.

452
00:51:47,190 --> 00:51:49,700
It's not like we don't have exposure to this risk.

453
00:51:49,700 --> 00:51:59,840
On a personal level, that fact of it's familiarity means that it's harder for us to get motivated to engage with it.

454
00:52:00,740 --> 00:52:08,060
Now what happened? We have a very concrete, visceral, personal example that's going to nudge us over this way,

455
00:52:09,830 --> 00:52:16,550
but it's also the case that this is foreshadowed or thought about next week. What you're describing is a type of experiential learning.

456
00:52:18,100 --> 00:52:23,650
You have to know the individual who was affected.

457
00:52:24,160 --> 00:52:27,340
And that is a very different thing than talk about a risk in the abstract.

458
00:52:27,610 --> 00:52:34,540
I can know the car accidents happen. And then there a very different thing when I know someone who has been affected by a car.

459
00:52:35,410 --> 00:52:37,660
I want to be clear here. This can cut in both directions.

460
00:52:38,350 --> 00:52:46,690
So in your case, we probably pay too little attention to car accidents and seatbelt risks, etc.

461
00:52:47,230 --> 00:52:54,250
And so if by knowing someone who is affected that motivated you and your fellow students to pay more attention,

462
00:52:54,250 --> 00:52:57,520
to wear a seatbelt, to drive safer protective actions.

463
00:53:00,530 --> 00:53:04,609
Now I'm going to make this up, but let's image. Actually, I won't make this up.

464
00:53:04,610 --> 00:53:12,710
I tell an unfortunate but very good example of this. Take example of someone who is.

465
00:53:13,910 --> 00:53:18,730
Struck by lightning. Really rare events.

466
00:53:19,930 --> 00:53:24,820
I don't personally actually agree with someone who has been struck by lightning, but if you do,

467
00:53:25,330 --> 00:53:30,400
your perception of that risk is going to have exactly the same process that Denise is talking about.

468
00:53:31,770 --> 00:53:35,910
It's going to be much more salient to you. You're gonna have much stronger reaction to it.

469
00:53:36,240 --> 00:53:38,280
But of course, it's still a really rare thing.

470
00:53:38,640 --> 00:53:45,630
So you might end up being overreactive in some sense, taking being too concerned about it precisely because you have that firsthand knowledge.

471
00:53:47,210 --> 00:53:55,070
So as we engage with this, by the way, on the topic really of your first assignment, as we engage with different kinds of risks,

472
00:53:55,070 --> 00:53:59,360
we have this balance between sort of the cognitive knowledge of how likely is it, how bad is it,

473
00:53:59,360 --> 00:54:07,550
etc., and then that experiential knowledge of Do I know somebody or not I seen this or that, etc.

474
00:54:08,770 --> 00:54:12,190
So great point to bring up because sometimes.

475
00:54:14,320 --> 00:54:21,400
Any of our purposes will depend upon having somebody actually think that this is relevant to them, something that they're going to care about.

476
00:54:25,680 --> 00:54:30,590
I actually do know somebody got struck by lightning and I absolutely have reaction to it now.

477
00:54:31,640 --> 00:54:39,350
And we can go through all of these kinds of things with lots of different rare risks that most people don't have any reaction to.

478
00:54:39,350 --> 00:54:46,760
But if you happen to know somebody who had that happen to them, it's really it changes your perspective of that.

479
00:54:46,780 --> 00:54:50,090
Right. All right.

480
00:54:51,560 --> 00:54:58,680
But more than that.

481
00:55:05,960 --> 00:55:09,570
Mark. Of.

482
00:55:10,910 --> 00:55:14,630
So you were sort of wrestling with the bigger picture.

483
00:55:15,170 --> 00:55:23,900
I remember sort of like the sense of notes I you sometimes about risks in communication seems to be like and the article talks about this

484
00:55:24,740 --> 00:55:32,060
let's convince you that this is okay even though there's a risk of sort of convincing somebody that this degree of risk is tolerable.

485
00:55:32,690 --> 00:55:36,440
Other times it's motivating, particularly protective behavior.

486
00:55:36,710 --> 00:55:43,550
Other times we've got, you know, I'm not necessarily pushing you, but I want to make you be able to make better decisions, etc.

487
00:55:44,990 --> 00:55:48,469
What were you sort of like? You talked about sort of this tension,

488
00:55:48,470 --> 00:55:52,940
but it's sort of I had a sense that there was something underneath that you were still wrestling with and I wanted to come back to it.

489
00:55:53,180 --> 00:55:56,240
Yeah, I think just on impressed me. Is that the.

490
00:55:57,190 --> 00:56:01,990
The messenger is communicating. The risk can be important in this.

491
00:56:03,080 --> 00:56:09,470
It's important to take note of that. I think that the arc I was talking about is, you know, tech companies will basically put out risk communications.

492
00:56:10,700 --> 00:56:16,900
Not always, but often enough method to kind of make you worry less about the side effects and then, you know, other yeah.

493
00:56:17,420 --> 00:56:20,190
Products will kind of be, you know, the inverse where it's, you know,

494
00:56:20,240 --> 00:56:26,000
really trying to communicate as well to fully understand and just kind of understanding where the message is coming from.

495
00:56:26,950 --> 00:56:32,060
So black flag a word you put up there. Fully understand.

496
00:56:35,360 --> 00:56:40,930
Was that being? But what is it to find?

497
00:56:41,020 --> 00:56:46,630
I mean, I'm probing you now because I think you've had the answer, but because I want us to all wrestle with how hard it is to answer this,

498
00:56:47,550 --> 00:56:57,760
how we define fully understanding, say, the risk of a product that has.

499
00:56:58,300 --> 00:57:06,340
Well, I'll use a concrete example. The exposure risk that we get by pumping the gasoline into our car every time that we go to a gas station.

500
00:57:08,250 --> 00:57:20,010
There's risk in that behavior. So should I not care or should I be freaked out and wearing respirator masks every time I pop a gas?

501
00:57:20,530 --> 00:57:23,730
Like what is defining success in this context?

502
00:57:25,670 --> 00:57:32,000
So this is probably impossible in a way. But it is what I mean, impossible in one sense and not in another.

503
00:57:32,030 --> 00:57:40,850
So this is the tension of this larger force. And I turn you and a consumer into an environmental expert.

504
00:57:41,210 --> 00:57:46,020
No, that is impossible. We are not turning our patients into many doctors.

505
00:57:46,030 --> 00:57:50,770
We are not turning our officers in the public in terms of many environmental scientists, etc.

506
00:57:51,340 --> 00:57:54,790
But that doesn't mean we can't have goals. Yeah.

507
00:57:55,240 --> 00:57:58,660
Isn't it about empowerment? Empowerment to do what?

508
00:57:59,380 --> 00:58:03,400
To protect yourself. Okay, so.

509
00:58:06,260 --> 00:58:09,560
What if you can't change whether or not you really focus?

510
00:58:12,320 --> 00:58:19,460
Well, at least you're aware. Yes. And I'm going to be I'm going to be annoying for a moment.

511
00:58:20,660 --> 00:58:25,760
And in being aware, my quality of life has now decreased, I am stressed out about that.

512
00:58:26,660 --> 00:58:35,540
Is that valuable to have changed my experience of life from blissful ignorance into stressed out about this risk that I cannot change?

513
00:58:36,810 --> 00:58:40,470
Is it valuable? We're doing indication, intentional.

514
00:58:41,070 --> 00:58:45,870
We could communicate it. We could not communicate. But isn't it more important to be ethical?

515
00:58:46,920 --> 00:58:50,190
But again, I want the ethics here is a complicated story.

516
00:58:50,940 --> 00:58:54,900
And I'm not challenging the underlying idea that.

517
00:58:56,230 --> 00:59:04,810
Having information can be intrinsically valuable that we have societal expectations of of transparency and disclosure.

518
00:59:05,290 --> 00:59:10,750
But I do think it's it's important for us to take a step back and ask what is defining success?

519
00:59:11,170 --> 00:59:15,120
Why are we doing this communication? Because.

520
00:59:17,090 --> 00:59:22,520
Years ago, I had a conversation with an environmental health science graduate student here.

521
00:59:22,520 --> 00:59:24,740
I was working at the time on a project you'll read a little bit about,

522
00:59:25,250 --> 00:59:34,370
which is involves the fact that a couple hours north of here in Midland, Midland, Michigan, is the international home of Dow Chemical.

523
00:59:35,060 --> 00:59:44,030
Dow Chemical. 60, 70, 80 years ago when we didn't know all the things we know about doing chemical create production

524
00:59:45,500 --> 00:59:52,550
would dump some of the products of their production processes into the local Tittabawassee River.

525
00:59:52,910 --> 00:59:58,610
What we now know is that those byproducts included dioxins. Dioxins have half life measured in decades.

526
00:59:58,610 --> 01:00:03,700
The stuff still around. So we have a question.

527
01:00:05,440 --> 01:00:13,660
Do we need to disclose to the community the level of dioxins that are available, that are present in water, in the soil, etc.?

528
01:00:14,050 --> 01:00:17,290
What is the purpose of that communication? Can they avoid it?

529
01:00:17,320 --> 01:00:26,360
No, they live there. So we get we end up exactly where Carolyn's bringing us up to, which is I can tell you about it.

530
01:00:26,360 --> 01:00:29,390
And now you're freaked out about I can't change it.

531
01:00:29,420 --> 01:00:34,610
So what is the underlying purpose of that communication? And the answer is there is there are answers to that.

532
01:00:34,610 --> 01:00:43,160
But we need to own what those policies are. I mean, I think isn't there a paternalism in deciding what I think you can and can't change?

533
01:00:43,760 --> 01:00:50,410
So if I provide you with all the information, you then have the freedom to decide what you want to adjust or what you want to adapt to.

534
01:00:50,450 --> 01:00:55,310
You might not have the ability to adapt to everything, but I can't know for you which things you can adapt.

535
01:00:55,320 --> 01:00:58,670
Maybe you'll never pump gas again. So notice that there are.

536
01:00:59,210 --> 01:01:03,710
I'm intentionally pushing the envelope in multiple directions here to help us wrestle with this.

537
01:01:06,300 --> 01:01:12,540
Every time I make a choice about what to disclose to you or not, and not even just that I want to disclose it to you.

538
01:01:12,960 --> 01:01:17,100
I am shaping your perceptions and changing your way in which you're going to respond.

539
01:01:17,610 --> 01:01:28,290
That is paternalistic. Every time I choose not to do that, I am intentionally leaving you to deal with whatever the information is in its raw form,

540
01:01:29,220 --> 01:01:32,280
which you may not be able to understand, which you may not be able to use,

541
01:01:32,670 --> 01:01:36,720
which may or may not actually guide you towards the behavior that would make you better.

542
01:01:38,690 --> 01:01:45,410
You can't avoid that one. One of the big pieces I'm going to push in this course is the owning of the responsibility as a communicator.

543
01:01:46,740 --> 01:01:50,730
There is no such thing as an unbiased risk communication.

544
01:01:52,030 --> 01:01:59,709
It doesn't exist. It can't exist because every choice we make about the provider number or not to label

545
01:01:59,710 --> 01:02:04,390
that number or not to organize the information in one way or another is paternalistic.

546
01:02:05,350 --> 01:02:10,500
And so you're asking the right question if it is paternalistic for us to do that.

547
01:02:11,340 --> 01:02:15,659
How do we decide that we should or shouldn't do it? That is a good question.

548
01:02:15,660 --> 01:02:20,130
That's one that we want to wrestle with throughout this course. I don't have a simple answer for it.

549
01:02:21,030 --> 01:02:24,600
But the more that we actually ask, what is our goal?

550
01:02:25,170 --> 01:02:29,660
What is our purpose? Why are we trying to do this? The better we can answer that question.

551
01:02:30,690 --> 01:02:35,340
That's my philosophy. That also means that you and I can disagree.

552
01:02:35,760 --> 01:02:39,540
I can say, yeah, I'm willing to persuade you to do this. And you can push back and say, You know what?

553
01:02:40,080 --> 01:02:44,010
I'm not sure I'm comfortable with that. And that's a fair conversation that we can be having.

554
01:02:45,680 --> 01:02:49,669
Because it's grounded not in some abstract scientific truth.

555
01:02:49,670 --> 01:02:54,950
It's grounded in values. It's grounded in recognizing what are we trying to accomplish?

556
01:02:56,840 --> 01:03:02,180
All right. So if I'm 33 or something like that.

557
01:03:03,350 --> 01:03:07,340
You guys all feel because we're past 930. You can pause me and say, Hey, it's time for the question.

558
01:03:07,730 --> 01:03:15,170
So the question of the day long and it's in pink and it's hard to read from the back.

559
01:03:15,170 --> 01:03:24,080
So let me read it off. Under what circumstances should public health communicators game to change behavior directed?

560
01:03:25,600 --> 01:03:31,520
Versus changing beliefs. Which may, in fact, ultimately change behavior, but not necessarily.

561
01:03:32,970 --> 01:03:39,690
So take a couple of minutes at the table and ask the question What for you becomes the reasons?

562
01:03:40,200 --> 01:03:43,590
Why would you feel comfortable pushing towards behavior directly?

563
01:03:43,920 --> 01:03:47,790
And when would you feel like, No, no, no, we're going to back off, we're going to be in the belief space.

564
01:03:49,260 --> 01:03:53,460
Just wrestle with like what? What's the principles that you would want to hold on to to make that choice?

565
01:03:55,050 --> 01:04:04,500
All right. Like a couple of minutes. I have no idea what I mean.

566
01:04:04,500 --> 01:04:21,570
Like drinking, like, I feel like by mistake, and they're not happy with their thoughts and then be change, I think.

567
01:04:24,340 --> 01:04:34,649
Yeah, yeah. I think there is no. I think as much as you do.

568
01:04:34,650 --> 01:04:47,320
I want to be Silicon Valley Chamber of Commerce and I want to do things like that

569
01:04:49,590 --> 01:04:58,870
every time I think title or something I don't know is but like a lot to think about.

570
01:04:59,070 --> 01:05:19,320
It's a lot of people like, like I was like like other anywhere I think like like what about what's great about it?

571
01:05:19,500 --> 01:05:23,490
I used to be like, oh, might not look good.

572
01:05:24,990 --> 01:05:29,550
So I think a lot about that.

573
01:05:30,150 --> 01:05:42,390
He's not going to let me know either way,

574
01:05:46,470 --> 01:06:01,610
which makes it very questionable efforts in terms of like what I think it's going to do with one particular wrestler.

575
01:06:02,040 --> 01:06:11,429
Yeah, I think that one even like that was a little bit more how like how does this actually kind of career and that's something I'm like,

576
01:06:11,430 --> 01:06:20,420
why so much have you read a lot of that in your family?

577
01:06:20,430 --> 01:06:35,410
Like I'm actually one of these women drawn to it is.

578
01:06:37,200 --> 01:06:49,080
I don't think I guess like I think I think it's pretty much like somebody above me.

579
01:06:51,120 --> 01:07:15,410
Yes. That's like it's like they're not giving you, like, marijuana, you know, like I feel like, though,

580
01:07:15,870 --> 01:07:28,850
that's part of questions like my daughter and don't tell her everything changing my behavior when I made my name and something.

581
01:07:29,800 --> 01:07:59,710
But they're telling me I have a really long term contract because I don't think of it.

582
01:08:02,250 --> 01:08:07,880
But then you have to deal with the repercussions of prison for doing something that's now legal.

583
01:08:07,920 --> 01:08:13,350
And that is there's a whole other stuff that I think that feasible to write.

584
01:08:13,680 --> 01:08:18,370
Like if you make a change, you know, how, how you're thinking about it,

585
01:08:18,700 --> 01:08:32,130
you have no idea because it's like a lot of times, I believe, like society has.

586
01:08:32,890 --> 01:08:42,340
I applied to the club have said that they didn't want to, you know.

587
01:08:42,340 --> 01:08:48,210
But it's like there's no support here. You know how you watch?

588
01:08:50,050 --> 01:08:55,270
Well, I know that you know that people are going to find out.

589
01:08:55,930 --> 01:08:58,989
Otherwise, I don't know. What do you think?

590
01:08:58,990 --> 01:09:07,480
One of the reasons is, I guess, you know, their lifetime.

591
01:09:07,980 --> 01:09:20,910
Let's say that's not true. What exactly are you talking about?

592
01:09:25,690 --> 01:09:43,089
You know, I just started saying not long ago that there are exceptions.

593
01:09:43,090 --> 01:09:47,770
I the question of like the whole conversation about.

594
01:09:50,850 --> 01:10:01,160
Except when he allowed once upon a time.

595
01:10:01,210 --> 01:10:06,710
But. All right, I'll cover my hair.

596
01:10:07,810 --> 01:10:10,880
But I would like to thank you.

597
01:10:11,650 --> 01:10:15,070
Now that you engaged and wrestled with this stuff.

598
01:10:15,070 --> 01:10:18,430
Like, I don't have answers, like we're going to be wrestling with this for the rest of this course.

599
01:10:18,880 --> 01:10:25,000
But that's the point. But spent time early on in this course I began.

600
01:10:25,000 --> 01:10:29,020
And owning the complexity was one of the things in my experiences in teaching this course,

601
01:10:29,230 --> 01:10:36,660
what people tend to find is that like I read a lot of musings in like week two and week three with people like,

602
01:10:36,670 --> 01:10:41,740
I never realized it was so complicated. Yes. Yes, that's the point.

603
01:10:42,540 --> 01:10:47,649
If what you get out of taking this course is an awareness of, it's complicated.

604
01:10:47,650 --> 01:10:51,280
You need to think about it. You need to plan to have some intentionality.

605
01:10:51,790 --> 01:11:00,940
You will already be well ahead of many other people in owning the possibility that what you're doing may or may not be achieving your goals.

606
01:11:02,010 --> 01:11:04,500
So I want to touch upon a couple of things before we wrap up.

607
01:11:04,950 --> 01:11:09,570
One has to go back to this because I realize there was one other piece that I didn't really talk about that I think is important,

608
01:11:10,470 --> 01:11:23,450
which is a component of that dread dimension that's really relevant when we think about health risks as time frame things which are immediate dread.

609
01:11:25,880 --> 01:11:29,630
Things which are postcode and are not.

610
01:11:31,550 --> 01:11:34,580
So what can we do in public health?

611
01:11:35,270 --> 01:11:41,840
Things like obesity, diabetes, cardiovascular disease.

612
01:11:42,820 --> 01:11:47,170
And for the most part, vaccination when we're not talking about living in the middle of a pandemic.

613
01:11:47,740 --> 01:11:57,070
These are issues where the health impact is not just years, but often decades down the road for most people.

614
01:11:58,540 --> 01:12:02,769
We're somehow trying to get people to change their life and not do the thing they

615
01:12:02,770 --> 01:12:07,030
like to eat and have to take a medication or have to change their behavior or,

616
01:12:07,030 --> 01:12:12,910
God forbid, have to exercise. Why do they feel bad now?

617
01:12:13,150 --> 01:12:20,800
No, because some number on some tests that they got done that they never actually saw

618
01:12:21,130 --> 01:12:24,370
is telling them that their cholesterol is high or their blood pressure is high.

619
01:12:24,370 --> 01:12:27,489
And now my doctor is telling me, no, I can't eat the thing.

620
01:12:27,490 --> 01:12:34,629
I like to eat that half hour because we don't dread the stuff.

621
01:12:34,630 --> 01:12:41,070
We don't react to stuff that's downstream. And if you want to make this really concrete.

622
01:12:42,750 --> 01:12:51,410
Climate change. Just downstream as we get it may not be in our lifetime the kinds of some of the impacts of what's happening.

623
01:12:52,770 --> 01:12:58,320
But we want to engage with the risks of climate change.

624
01:12:58,650 --> 01:13:08,610
We have to somehow be motivated to take protective behaviors now for things that will not affect us for many years.

625
01:13:09,090 --> 01:13:15,690
So that's another component of yeah. I think that's another situation where it's also important to personal experiences in terms of

626
01:13:15,690 --> 01:13:22,860
like if you have seen a grandparent or a family member who had something that was long term,

627
01:13:23,160 --> 01:13:30,600
it's a lot easier for you to think about it. You saw your grandpa have diabetes and all the difficulties with that.

628
01:13:30,870 --> 01:13:34,410
And even if, you know, for you it could be 50 years off, it's still.

629
01:13:34,650 --> 01:13:36,200
Wow, it does happen. Yep.

630
01:13:37,560 --> 01:13:49,410
Notice that that's mediated through the degree to which you see that the examples that you are familiar with as predictive of you.

631
01:13:51,190 --> 01:13:59,680
So if I look at another person and I'll pop out of the diabetes example, for the moment I look at another person and think.

632
01:14:01,180 --> 01:14:04,419
Oh, well, of course they had this happen.

633
01:14:04,420 --> 01:14:08,740
And that's because they lived in this particular environment or they engaged in this particular behavior or whatever.

634
01:14:08,890 --> 01:14:16,840
And I don't see myself as like them then that knowledge that that bad thing happened to them isn't going to be as influential on me,

635
01:14:17,380 --> 01:14:23,020
that you use the example of a family member and obviously family brings this a lot more closely, but.

636
01:14:25,220 --> 01:14:29,450
Just simply knowing that something can happen doesn't necessarily make us feel bad.

637
01:14:29,960 --> 01:14:32,960
I just feel motivated unless we see that as personally relevant.

638
01:14:33,740 --> 01:14:38,120
So I just keep raising it. Well, when are we going to see something that's personally relevant?

639
01:14:38,120 --> 01:14:49,710
And what are we not? And the hard part here is the personal relevance of the cost of preventive actions is often immediate and salient.

640
01:14:54,050 --> 01:14:59,190
Shutting down the beach is now. The infection I might get is later.

641
01:15:00,120 --> 01:15:04,290
Wearing the mask is now closing down my business because of COVID.

642
01:15:04,290 --> 01:15:08,820
Is not the person dying from COVID is later.

643
01:15:10,050 --> 01:15:15,840
Certainly the hamburger is now. The heart attack is later.

644
01:15:18,200 --> 01:15:24,230
So when we're trading off. The risk of I don't get to eat what I want.

645
01:15:25,060 --> 01:15:28,400
Now we're trying to do this later.

646
01:15:29,010 --> 01:15:35,210
So that's one big thing that that fits into this framework. It is really important for us to be wrestling with this.

647
01:15:37,220 --> 01:15:40,790
So. What were the things that came up?

648
01:15:41,900 --> 01:15:49,010
As you were talking here. GROSS So when are you most comfortable going straight to behavior?

649
01:15:49,670 --> 01:15:55,760
Let's start on that side. Yeah, I mentioned that I'm most comfortable going straight to try to change someone's

650
01:15:55,760 --> 01:15:59,770
behavior when it directly or directly impacts someone else who they have.

651
01:16:00,770 --> 01:16:05,030
I heard a number of groups talking about this. Is it affecting you or affecting other people?

652
01:16:05,480 --> 01:16:09,799
We are willing to society to say we got we got to enforce behavior.

653
01:16:09,800 --> 01:16:13,490
The more that behaviors are not just individual but their societal.

654
01:16:15,150 --> 01:16:19,110
Pandemics. Infectious diseases are a good example of that, at least in principle.

655
01:16:20,130 --> 01:16:23,220
My behavior doesn't just protect. It's not about protecting me.

656
01:16:23,220 --> 01:16:28,830
It's about protecting others, respecting others. But none of this is complicated.

657
01:16:28,830 --> 01:16:34,470
So I was talking with another group about something like motorcycle helmet laws.

658
01:16:36,280 --> 01:16:41,190
Because wearing a motorcycle helmet affect others. Absolutely.

659
01:16:42,300 --> 01:16:47,250
So in what way? Oh, we're don't know. I mean, so I might not want a helmet,

660
01:16:47,250 --> 01:16:53,580
but if I crash and I get knocked unconscious or someone asks me on the side of the road that ruins important this day.

661
01:16:53,910 --> 01:16:57,110
Societies pay insurance. Traffic accidents.

662
01:16:57,120 --> 01:17:00,570
All kind of stuff. So. So there's a tension here.

663
01:17:00,910 --> 01:17:04,650
But there are clearly some behaviors which instantly affect others.

664
01:17:05,220 --> 01:17:08,610
There are also other behaviors which are more focused on the individual.

665
01:17:08,620 --> 01:17:16,349
But there are obviously echo. I mean, we could look at cardiovascular disease like one person might have a heart attack,

666
01:17:16,350 --> 01:17:19,080
but it affects their family, it affects the health care system, etc.

667
01:17:20,150 --> 01:17:32,300
So whatever we think this dimension of affecting others versus individual is relevant to how quickly we move to the context of being persuasive.

668
01:17:32,570 --> 01:17:41,620
What else? Little context that makes us more willing to go down the behavior pathway.

669
01:17:45,910 --> 01:17:50,020
Yeah. You talked a little bit about the practicality of what you actually want them to do.

670
01:17:50,050 --> 01:17:53,620
So are you asking them to change a long term behavior, in which case it's less likely to be effective?

671
01:17:53,620 --> 01:17:58,689
Or do you just want them to, like get a vaccine or do something? So there's feasibility pieces here.

672
01:17:58,690 --> 01:18:02,349
Like, we love things that you can do now. It's really simple.

673
01:18:02,350 --> 01:18:06,280
It's concrete. Let's just let's just do this long for behavior change.

674
01:18:07,660 --> 01:18:13,780
It's hard to maintain. Like getting somebody to change their diet is not a one time exercise.

675
01:18:13,930 --> 01:18:17,139
This is a lifetime worth of communication and effort,

676
01:18:17,140 --> 01:18:22,440
etc. So we're probably going to need some beliefs underneath that before we're actually going to be successful.

677
01:18:23,500 --> 01:18:31,680
Yeah. The other thoughts came up. Yeah, it sounds like a very simple, uh, simplistic view.

678
01:18:32,130 --> 01:18:43,700
I think the harder it is to change, the harder, the harder the communication is to share and distribute and possibly change behavior.

679
01:18:43,710 --> 01:18:46,920
Maybe the less likelihood that it will be share.

680
01:18:48,450 --> 01:18:49,679
Which is kind of sad in a way,

681
01:18:49,680 --> 01:18:57,600
but like the reality is we have low resources and behaviors are hard to change or beliefs are hard to change is what I meant.

682
01:18:58,770 --> 01:19:02,280
And so behaviors I feel like more often are something that we target.

683
01:19:03,540 --> 01:19:06,719
So there's a journalism component to what you're saying,

684
01:19:06,720 --> 01:19:15,090
which is important to us when it's when we don't believe that simply changing beliefs is going to be an effective way to ultimately change behavior.

685
01:19:15,870 --> 01:19:22,829
We may or may not be willing to impose I'm going to use a very inflammatory language to impose

686
01:19:22,830 --> 01:19:28,470
our choice on others through the language that we use or the communications that we create.

687
01:19:29,830 --> 01:19:35,560
And then we have to stand on that side. I am comfortable being really persuasive about smoking.

688
01:19:37,200 --> 01:19:40,260
You could challenge me on that. You could say, Hey, I really enjoy a cigaret.

689
01:19:40,890 --> 01:19:45,600
I'm still going to stand here and show you graphic images and get you to try and quit smoking.

690
01:19:48,030 --> 01:19:52,200
But I'd better be willing to stand here and own that. Are you going to die on?

691
01:19:52,350 --> 01:20:01,020
Right. And so part of this is a conversation about what hills are we willing to die on from the standpoint of imposing behavior.

692
01:20:01,590 --> 01:20:03,780
Again, I'm using inflammatory language intentionally here.

693
01:20:04,200 --> 01:20:15,419
Imposing behavior on others versus supporting autonomous decisions, knowing that some of those autonomous decisions are going to be people.

694
01:20:15,420 --> 01:20:22,740
And I will pick on the motorcycle helmet, one who will go out, not wear a helmet and crash and die as a result of not wearing the helmet.

695
01:20:24,300 --> 01:20:30,870
We will know that more people will be dead as a result of that choice, and somehow we have to accept that.

696
01:20:33,210 --> 01:20:36,810
So now let's just flip it around.

697
01:20:37,470 --> 01:20:43,380
I'm running out of time. Okay. We keep coming back to this one. This is a team that we can come back to over and over again.

698
01:20:43,890 --> 01:20:48,120
Oh, yes. The class is about experience.

699
01:20:50,070 --> 01:20:53,400
It's two classes are really about experiences that talk about emotion.

700
01:20:53,890 --> 01:20:56,970
The monologue of emotion to the next set not really.

701
01:20:57,000 --> 01:21:02,790
Try to start to pay attention to your own experiences, your own emotions.

702
01:21:03,030 --> 01:21:07,590
What are you conscious of when you're not conscious of that class?

703
01:21:18,060 --> 01:21:22,060
Get it? Think you do.

704
01:21:23,940 --> 01:21:48,490
I will see you out there on the children and families in the 1960s.

705
01:21:48,560 --> 01:21:54,170
Oh, yes. Penn State as a consultant like the former president.

706
01:21:56,820 --> 01:22:00,180
I don't know about you, but.

707
01:22:02,930 --> 01:22:20,630
I know. I know. I felt like I had you know, I didn't want to get to go back.

708
01:22:20,820 --> 01:22:23,990
That's also something else to talk about like that.

709
01:22:24,240 --> 01:22:29,170
If that weren't right, you weren't. And that's what I.

710
01:22:29,910 --> 01:22:39,460
Me. I think you should be straight out of the house.

711
01:22:39,930 --> 01:22:59,570
Oh, really? All right. Let me ask you that, because I can start with you.

712
01:23:01,740 --> 01:23:09,630
But what about these things? One thing that I think about, it was something, right?

713
01:23:09,670 --> 01:23:24,120
Well, sorry. I'm not saying you guys go I love you more than I do is bring up those same thought.

714
01:23:24,360 --> 01:23:28,150
Just make it more abstract. And you argue that, and that's what I'm doing.

715
01:23:28,170 --> 01:23:32,010
Just thinking know I can be, but it cannot not be.

716
01:23:32,010 --> 01:23:35,370
I think a lot depends upon whether your goal is to say, Oh, what's your goal?

717
01:23:35,370 --> 01:23:41,370
Whether you're worried about what they already know. You need to bring that to the surface or whether you have a purpose where that doesn't matter.

718
01:23:42,270 --> 01:23:45,450
And that's kind of dependable, which isn't true either way.

719
01:23:45,520 --> 01:23:48,580
Yes, I do. Okay.

720
01:23:48,690 --> 01:23:52,320
So let me flip the them at that point.

721
01:23:53,490 --> 01:23:59,960
The question is, if you're willing to do it the way you want it, we can do it.

722
01:24:02,220 --> 01:24:10,350
And we kind of made all those decisions based on that information and you just how you see it.

723
01:24:11,550 --> 01:24:24,030
So in that one model, 2024 and I was going, okay, you know, since she's writing and everybody knows that,

724
01:24:24,800 --> 01:24:30,420
hey, that was like, you're never going to write a fireside chat about what I like.

725
01:24:31,420 --> 01:24:35,260
I want to go every single page like that.

726
01:24:35,640 --> 01:24:44,160
Well, I think that's like the bigger question, like talking when we say even like I honestly see it's like there's lots of words.

727
01:24:44,200 --> 01:24:49,430
I don't want to be like short versions.

728
01:24:50,190 --> 01:24:57,300
We don't evaluate outcomes in absolute terms.

729
01:24:57,600 --> 01:24:58,720
We evaluate them in relative.

