1
00:00:02,570 --> 00:00:07,520
Okay. So it's about checking different assumption wrestle for linear regression.

2
00:00:07,520 --> 00:00:19,969
We have as you mentioned, we have major or major confidence linearity, abundance people, the variance and the dramatic effect in nature.

3
00:00:19,970 --> 00:00:30,390
We as we started talking about how to try to independence and we finish these out of this is we are we stopped so let's continue from here the

4
00:00:30,490 --> 00:00:39,500
intimate go out later there is there is a summary slide so that later we're going to summarize all the different ways of checking these assumptions.

5
00:00:39,950 --> 00:00:43,220
But now let's let's just continue from where we left off.

6
00:00:44,240 --> 00:00:51,950
So let's see what are the impact is if we if the assumption is violated.

7
00:00:53,680 --> 00:01:06,610
So when independence assumption is violated. In other words, if not, while this is absolutely an absolute joke, if the error terms are correlated,

8
00:01:06,790 --> 00:01:14,410
are not invented between different individuals, then the better hand, if you look a bit ahead, is still unbiased.

9
00:01:14,860 --> 00:01:18,249
The reason is that no hat is equal to the recall.

10
00:01:18,250 --> 00:01:28,280
That when a hat is equal to. Is equal to.

11
00:01:28,310 --> 00:01:33,820
That's right. Yes.

12
00:01:33,850 --> 00:01:39,720
Is all of that. Okay. So if Peter had a is equal to those that is marriage and Peter had is equal to.

13
00:01:42,650 --> 00:01:45,840
He's managing wife at what? He's Belgian wife.

14
00:01:47,630 --> 00:01:51,110
And this is actually ex Spain.

15
00:01:52,540 --> 00:01:56,030
I am a man. This is equal to bear.

16
00:01:56,620 --> 00:01:59,180
So in other words, the in your mind,

17
00:01:59,200 --> 00:02:08,220
this assumption about anyone else is that it doesn't really about no on business of being a hat and being a hat is still normal,

18
00:02:08,260 --> 00:02:16,569
disputed because it is a linear combination of y and if y not undisputed then better hat and we'll still be normal.

19
00:02:16,570 --> 00:02:22,830
This really. But ACMA Square will be off acting as soon as we are.

20
00:02:22,840 --> 00:02:26,230
We will be biased. He was morally biased.

21
00:02:26,380 --> 00:02:34,210
The reason is that now when we show that we're not going to go into details here, but if you go back to how we should look,

22
00:02:34,230 --> 00:02:39,970
if don't see what we are equal to, she must look at able to see must where this we didn't.

23
00:02:40,390 --> 00:02:43,480
I'm sorry. So this depends on the independence assumption.

24
00:02:44,950 --> 00:02:49,360
So in other words, if something is violated, let's see what's where is biased.

25
00:02:50,590 --> 00:02:57,280
And then if we look at the bearers of better hat, if we calculate awareness of better hat.

26
00:02:59,020 --> 00:03:01,240
Now the hat is given by this.

27
00:03:01,300 --> 00:03:12,400
So you can calculate the errors that this matrix times the variance of y and times this now because the errors they are not independent.

28
00:03:13,090 --> 00:03:16,150
So the sigma is not a diagonal matrix anymore.

29
00:03:17,750 --> 00:03:25,239
So 111. What is our independent that seems the matrix for one when there are not any other

30
00:03:25,240 --> 00:03:33,760
gender correlations among these different wise and then the C makes no value matrix.

31
00:03:34,990 --> 00:03:40,050
So there is no simplification of this matrix anymore. There is no simplification.

32
00:03:40,060 --> 00:03:49,090
So in other words, the variables are equal to this. So if the there is a sequel to this now, if we look out of a C eyes, you have always passed.

33
00:03:51,620 --> 00:03:57,929
If you think you need a rush to use, you are or will still assume that the virus is equal to the sea.

34
00:03:57,930 --> 00:04:02,170
You must wear an x both times and symbols.

35
00:04:03,900 --> 00:04:09,620
Now if you fish linear regression model, this is how R would calculate the variance of the beta.

36
00:04:10,700 --> 00:04:17,870
However, now the appearance of beta is not equal to this, adding so this is not equal to this any more.

37
00:04:21,210 --> 00:04:29,550
So. So then if you look at the center errors given by R, if you look at it, if you construct confidence intervals,

38
00:04:29,550 --> 00:04:37,200
if your kernel level is just based on that, the results from R now the results become invalid.

39
00:04:38,390 --> 00:04:43,700
Because the the center error calculated by R is not in the true center area.

40
00:04:46,910 --> 00:04:58,010
So this is the consequence of of having the independence assumption violated and the questions about this life.

41
00:05:02,280 --> 00:05:11,880
Okay. And then now there are some remedies for, you know, one of these measures I think is valid.

42
00:05:12,750 --> 00:05:16,469
So there is of course, you guys are going, okay, that's the core course.

43
00:05:16,470 --> 00:05:20,730
That's the the last course of the series.

44
00:05:21,390 --> 00:05:29,370
So we have 6050 linear regression to 51 less generalized linear regression and about 3 to 3 that's reviewing the matter or longitudinal data.

45
00:05:30,390 --> 00:05:36,120
But essentially it's about when the independence assumption is violated.

46
00:05:36,690 --> 00:05:40,740
So in biostatistics, that's a very commonly seen case.

47
00:05:40,980 --> 00:05:49,680
So when the when there is some correlation among different individuals, you know, especially in terms of the additional data.

48
00:05:49,890 --> 00:05:58,410
So that's, of course, where you will that will see different ways of dealing with the data, what independence assumption is violated.

49
00:05:59,580 --> 00:06:07,260
We can very quickly measure this, but but we don't need to get into details at all because you guys are going to learn this in 53.

50
00:06:08,940 --> 00:06:15,980
So the one approach is actually to use the so-called mix effects model, the social media effects.

51
00:06:15,990 --> 00:06:19,150
Now, some of you might have seen this term. It's amazing.

52
00:06:19,980 --> 00:06:29,340
So what it does is that when you model the the response here, we have to infer this.

53
00:06:29,640 --> 00:06:33,930
I am the J. Now J here is the in this for the J.

54
00:06:33,930 --> 00:06:40,890
It's well, I guess it's it is better to interpret this in terms of longitudinal data.

55
00:06:41,160 --> 00:06:52,800
So this is the is individual and this is the date time one to from the individual and so on.

56
00:06:53,260 --> 00:06:59,340
And then the model actually allows a random intercept rather than intercept.

57
00:06:59,820 --> 00:07:05,370
So the. So, in other words, the intercept is individual specific.

58
00:07:06,060 --> 00:07:14,460
So each individual has their own intercept, and then the slope is the same for all the individuals.

59
00:07:14,970 --> 00:07:21,810
This is the difference. This is how the correlation among different individuals is accounted for.

60
00:07:24,330 --> 00:07:29,850
Okay. I realized that by just saying this is probably just caused more confusion because.

61
00:07:30,480 --> 00:07:33,580
Yeah, I mean, this lasts is is just not necessary here.

62
00:07:33,600 --> 00:07:37,470
I mean, this is 653, so we don't need to worry about this at all.

63
00:07:38,370 --> 00:07:43,379
So let's yeah, let's, let's not let's not get into yeah,

64
00:07:43,380 --> 00:07:53,610
let's not worry about bunch of others because I'm afraid that we spend 5 minutes here but still trying to cause confusion.

65
00:07:53,640 --> 00:07:58,160
Like what exactly the motto is. Yeah, let's not worry about this.

66
00:07:58,170 --> 00:08:09,010
Let's skip this. It's really not because we do have other more important topics for this course.

67
00:08:09,500 --> 00:08:17,860
And okay, so in 56, 53, you would have definitely learn how to address this issue, like the violation of independence.

68
00:08:18,810 --> 00:08:22,740
There are two approaches. One is mixed model, the other one is the marginal model.

69
00:08:24,540 --> 00:08:27,930
And also some other remedies include that.

70
00:08:28,050 --> 00:08:33,870
You could include time as a quote, but you model the data and.

71
00:08:40,830 --> 00:08:45,090
So yeah. So that's that's that's another solution. You include crime as a covariate.

72
00:08:47,490 --> 00:08:55,510
And also while you might need to resort to some other techniques time series is is,

73
00:08:56,170 --> 00:09:01,110
is an area where it is a topic that is not covered in this department.

74
00:09:01,110 --> 00:09:06,870
How serious data is more widely seen in novels in finance.

75
00:09:07,680 --> 00:09:15,360
So basically you have you have you have reviewed again you have to repeat the matter is from the from the same subject,

76
00:09:15,960 --> 00:09:19,770
but it's much more densely collected.

77
00:09:21,030 --> 00:09:25,200
So there's a special area called Time Series Analysis.

78
00:09:25,200 --> 00:09:33,390
And there you can use those models to model to take care of the correlation among subjects as well.

79
00:09:34,820 --> 00:09:40,870
Okay. Yeah, that's the Inverness assumption.

80
00:09:40,880 --> 00:09:46,690
So recall that an endeavor for independence was actually. I think the most important thing to remember is that.

81
00:09:49,670 --> 00:09:54,080
Although we do have ways, deeper ways of tracking individuals.

82
00:09:54,920 --> 00:10:02,570
However, the most effective way is to have a design of a study to look at how the data are collected.

83
00:10:03,560 --> 00:10:09,240
And usually we have a pretty good idea by looking at how the data on a cloud ID holiday.

84
00:10:09,240 --> 00:10:14,000
In our sample, we had a pretty good idea of whether the assumption is valid or not.

85
00:10:15,230 --> 00:10:19,340
So for example, if you look at again, I'm making of this some example,

86
00:10:19,340 --> 00:10:26,030
if you want to study people's income, if you collect people's income from the same household,

87
00:10:26,030 --> 00:10:33,950
then people living in the same household or in a same neighborhood, their income earning are highly rated or correlated.

88
00:10:34,520 --> 00:10:42,470
So in that case, this assumption may be violated, but if you collect data based on completely random samples,

89
00:10:42,770 --> 00:10:46,730
then you already within that of independence of assumption. This is.

90
00:10:46,760 --> 00:11:00,480
Okay. So. Having said that, now in practice, when people model the data sometimes as you do, oftentimes it's not uncommon at all.

91
00:11:00,530 --> 00:11:07,430
So oftentimes people chose to model explicitly address the mileage of the event as assumption.

92
00:11:08,360 --> 00:11:15,110
For example, sometimes even people know that while some data points in my datasets are acquired

93
00:11:15,110 --> 00:11:18,890
is from the individual within the same household or within the same neighborhood.

94
00:11:19,160 --> 00:11:24,170
That in principle, there is correlation. But some people choose to forget about it.

95
00:11:26,840 --> 00:11:33,170
The reason why? There are many reasons. One reason is that taking that into account would be very complex.

96
00:11:33,530 --> 00:11:39,200
I mean, you may need a very complex model in order to deal with that and to ignore that.

97
00:11:39,260 --> 00:11:45,670
I mean, they make things much easier and assigning them all well, although impressively there is correlation,

98
00:11:45,680 --> 00:11:50,090
but the correlation may not be strong enough to cause any serious concern.

99
00:11:50,690 --> 00:11:59,389
So it's not uncommon that people sometimes even we know there is a correlation, but some people still go ahead.

100
00:11:59,390 --> 00:12:08,870
And I assume if I ignore that correlation and this is sort of justified by the fact that, you know, here, look, Peter had a steal.

101
00:12:08,870 --> 00:12:15,410
I'm biased and so normally disputed, but a stenner error may be affected.

102
00:12:16,400 --> 00:12:23,750
But if the correlation is not a very strong, then the effect on the impact on average may not be,

103
00:12:24,680 --> 00:12:28,210
you know, be large, not be too large to cause any concerns.

104
00:12:28,670 --> 00:12:32,750
So, yeah, that's about independence.

105
00:12:33,920 --> 00:12:45,470
Now comes from various assumption. So recall that we assume that the arms from follow.

106
00:12:49,110 --> 00:12:50,160
Because of the variance.

107
00:12:50,580 --> 00:12:58,500
So in other words, we assume that assume a square is the same for all individuals in the data, that it does not vary for different individuals.

108
00:12:59,940 --> 00:13:03,900
And this is another very important assumption. And people would check this.

109
00:13:04,230 --> 00:13:07,780
We could make a plot of the residual versus the predicted.

110
00:13:07,810 --> 00:13:11,080
What? The in prison for daily life.

111
00:13:16,170 --> 00:13:20,190
Just like a lot of here. So we thought of the residuals versus the fuel value.

112
00:13:21,750 --> 00:13:25,890
And then we look at the plot to see if there is any clear pattern.

113
00:13:26,520 --> 00:13:35,790
So if the assumption is good enough, if this assumption is not violated, then we shouldn't see any pattern in this plot.

114
00:13:38,520 --> 00:13:41,580
Yeah. So in other words, the plots would be wrong in the scattered.

115
00:13:42,180 --> 00:13:51,960
If you look at the partizan divide of absolute versus y hat, then if the cause of the virus is not violated,

116
00:13:52,320 --> 00:13:55,880
then you will see why the expanded evil hat is equal to zero as well.

117
00:13:55,890 --> 00:14:04,140
Then you will see that this dot would randomly scattered around this horizontal line.

118
00:14:06,150 --> 00:14:12,120
But if you see any clear pattern, that is an indication that this caused the virus may be violated.

119
00:14:12,180 --> 00:14:22,710
Just like here. This is a very commonly seen violation of the causal variance assumption or the so-called homogeneity assumption,

120
00:14:25,170 --> 00:14:29,790
so that if you look at here, the s the field value increases.

121
00:14:30,880 --> 00:14:37,630
But there is also increases. So this is a clear indication of the evolution of the concept of our assumption.

122
00:14:39,360 --> 00:14:43,560
So where the absolute value increases would lie ahead.

123
00:14:45,000 --> 00:14:55,380
So implying that the virus is increasing function of the mean and this is quite common especially when you model count were originated in say 51.

124
00:14:56,880 --> 00:15:01,650
You guys will not say for a while. Okay, so here we are talking about a linear regression.

125
00:15:01,800 --> 00:15:07,709
So for example, let's say you buy count data for the loneliness.

126
00:15:07,710 --> 00:15:11,310
For example, we look at the number of, let's say,

127
00:15:11,550 --> 00:15:20,550
the number of cancer cases in let's say in our CDC and our Birx in Detroit, in in Flint, in East Lansing.

128
00:15:20,910 --> 00:15:31,230
So we look at numerous cities. We look at the number of cases of like the number of cancer cases, and this is a count or even a positive integer.

129
00:15:32,700 --> 00:15:42,570
So when we model such data, use a linear regression. Again, it's not uncommon to see that the virus, as we of the residual, depends on the mean.

130
00:15:44,910 --> 00:15:49,650
And so the variance actually increases. So in this case is violated.

131
00:15:50,220 --> 00:15:56,040
And here we want to point out that the residuals are the residuals.

132
00:15:56,100 --> 00:16:01,380
We call that the residuals. These are absent this absence, are heterogeneous.

133
00:16:01,590 --> 00:16:08,500
Heterogeneous. The reason is that we have calculated the variance of Epsilon hat.

134
00:16:09,780 --> 00:16:20,780
If you go back to. Yeah.

135
00:16:20,780 --> 00:16:29,000
This is. Yeah, it's over here. Right. So we have we have calculated the variance of everything we have that's equal to this guy.

136
00:16:30,340 --> 00:16:36,030
Now, if you look at this, that means that there is an absolute white hat that is seamless.

137
00:16:36,030 --> 00:16:39,810
Where comes the high diagonal element of this matrix?

138
00:16:41,820 --> 00:16:49,590
But this matrix and the elements, along with having all these elements, are not necessarily the same, the general speaking.

139
00:16:49,830 --> 00:16:54,150
They are different. So in other words, the.

140
00:16:59,500 --> 00:17:04,930
In other words, the weight of the virus of abs I had will not be the same.

141
00:17:05,350 --> 00:17:10,580
Generally speaking, they're not able to see my square because there is one minus h r involved.

142
00:17:11,050 --> 00:17:18,550
So the virus of this episode y hat are not the same, but they should not have any pattern.

143
00:17:19,060 --> 00:17:26,470
So in other words, if you calculate again, if you calculated or if you make a plot from had was a white hat,

144
00:17:26,830 --> 00:17:30,790
you should have seen this one really run to scatter.

145
00:17:31,350 --> 00:17:34,510
Patrick Well, there shouldn't be any clear pattern in the plot.

146
00:17:36,820 --> 00:17:41,350
Okay. So that is very effective way of checking and causal.

147
00:17:41,350 --> 00:17:45,490
There is an assumption. Now if the causal variance assumption is violated.

148
00:17:45,940 --> 00:17:49,620
Now let's take a look at what will happen.

149
00:17:51,610 --> 00:18:00,760
So if the homogeneity fails, what we need is just another terminology for the cause of various assumption.

150
00:18:01,480 --> 00:18:06,430
If this fails and then beta hat still remain valid.

151
00:18:06,640 --> 00:18:11,710
Well, again, this. That this is because a beta hat is equal to.

152
00:18:14,330 --> 00:18:19,160
That's right. And then if you count Manning's management, Peter had.

153
00:18:24,230 --> 00:18:32,880
His wife. If you count my lady Spanish, I'll bet I had a year out of this.

154
00:18:33,930 --> 00:18:41,930
And this is equal to. Excavator.

155
00:18:43,180 --> 00:18:49,500
But and this this council, this is even better. So Peter had a steal, and I'm biased as major.

156
00:18:49,890 --> 00:18:54,180
This does not. I mean, approval, but it had is unbiased.

157
00:18:54,480 --> 00:18:59,880
This does not depend on costs. There is an assumption at all this has nothing to do with theirs.

158
00:19:00,390 --> 00:19:10,830
So as long as they that foundation y is equal to as times better than the better had is is still a valid as matter by valid.

159
00:19:11,430 --> 00:19:20,370
What do we mean is that it is still unbiased. And so the expectation on my hand is to equal to is an exaggeration that is it better?

160
00:19:20,760 --> 00:19:29,280
That's what we mean by valid. But if you look hard to see why square seems square in this case.

161
00:19:30,540 --> 00:19:39,990
Has a problem. The reason is that if you recall in Russian, like whenever you fill in your Russian, you ask r whatever software to fill in in Russia.

162
00:19:41,070 --> 00:19:47,820
The software, we will automatically make the assumption of maybe those four assumptions made.

163
00:19:47,900 --> 00:19:52,860
Linearity in manners, equal veterans and normality will make those assumptions.

164
00:19:53,700 --> 00:20:00,120
So the results you'll see from our ULLAPOOL or from SAS article, they are based on those four assumptions.

165
00:20:00,600 --> 00:20:05,010
They are already being made by both by the by whatever software you use.

166
00:20:06,960 --> 00:20:17,280
However, if the course where there is assumption is violated, that means when you use are fulfilled model you are making an incorrect assumption.

167
00:20:17,850 --> 00:20:23,900
Rather see my square as in truth the sigma squared is not equal for different individuals.

168
00:20:23,910 --> 00:20:26,130
It's actually very different for different individuals.

169
00:20:26,820 --> 00:20:33,120
So in that case, there is really no sigma squared anymore because each individual has their own seamless square.

170
00:20:33,690 --> 00:20:37,380
There is no unique or identical seamless square.

171
00:20:37,650 --> 00:20:44,940
So there is no CMI square anymore. So in that case, this Seymour Square itself itself is not.

172
00:20:45,150 --> 00:20:52,440
It does not exists. So that's why this CMA square is not a well defined, although, I mean, mathematically,

173
00:20:52,530 --> 00:20:58,110
if you ask our preferred model to fill in data, you will still get an estimate of Sigma Square.

174
00:20:59,670 --> 00:21:06,150
Sigma how to square. But it is just it doesn't estimate the truth to see that squaring was true.

175
00:21:06,450 --> 00:21:10,500
The truth you see must wear as the varies for different individuals.

176
00:21:13,480 --> 00:21:20,140
Okay. And then the virus of hat this becomes invalid.

177
00:21:20,410 --> 00:21:24,760
The reason again is that now if you calculate the variance of Peter had.

178
00:21:25,810 --> 00:21:37,150
So this has been a hack. If you count with the virus, then we have this matrix multiplied by the virus of y by the trust transpose of this matrix.

179
00:21:39,410 --> 00:21:43,460
Not because the cause of the virus assumption is does not hold.

180
00:21:43,910 --> 00:21:48,890
So this virus, a y is no longer the same, a square times.

181
00:21:49,190 --> 00:21:54,530
I don't know The Matrix because there's no causal mirrors under the virus.

182
00:21:54,530 --> 00:22:13,970
All the viruses are different. So in other words, this virus here is no longer if you control this virus here is not equal to this, but of this virus.

183
00:22:15,380 --> 00:22:25,970
This is what are used to calculate the Cinerama to show that there is a better at because this one this virus.

184
00:22:26,420 --> 00:22:31,370
If you simplify it this is equal to similar squared times x transpose test inverse.

185
00:22:33,470 --> 00:22:40,220
So this is the formula that ah we use for calculus generator to estimate variance on beta.

186
00:22:42,680 --> 00:22:47,390
So the results you see from our output will be based on this formula.

187
00:22:48,140 --> 00:22:51,170
But actually, in this case, the virus is in control.

188
00:22:51,650 --> 00:22:58,070
This is not a control. Actually, it's not that the numbers that you see from the arm.

189
00:22:58,370 --> 00:23:01,640
So that's the reason why this variance is invalid.

190
00:23:04,880 --> 00:23:12,020
So because the virus is invalid, then the hypothesis test and confidence interval will become invalid.

191
00:23:14,790 --> 00:23:19,320
So this is the impact of the violation because of Berra's assumption.

192
00:23:29,640 --> 00:23:37,799
Any questions? I'm not sure if I'm going too fast, although we do want us to be real a little bit about it.

193
00:23:37,800 --> 00:23:41,190
We don't want to rush any questions.

194
00:23:45,300 --> 00:23:57,250
Okay. Now let's look at the some solutions that can address that problem.

195
00:23:57,820 --> 00:24:05,680
So one commonly used technique to deal with the violation of coastal areas is a so-called waiting list, a square.

196
00:24:07,840 --> 00:24:13,450
So now suppose we do have independence as all the different individuals they are still in.

197
00:24:14,110 --> 00:24:17,300
However. The viruses are different.

198
00:24:17,320 --> 00:24:24,729
So in other words, now, if we look at the residual for sort of the error and return absent the plus the absence do

199
00:24:24,730 --> 00:24:34,450
follow normal distribution usually mean zero virus given by W but in this case W is not.

200
00:24:35,560 --> 00:24:44,770
In this case it's not previously we are seeing W is equal to I see my squared times are for now, but especially now.

201
00:24:44,800 --> 00:24:50,500
We still think ivana's assumption holds, but the equal variances assumption is valid.

202
00:24:50,620 --> 00:24:56,230
So that W is still a diagonal matrix, a still a dynamo matrix.

203
00:24:56,520 --> 00:24:59,770
What do you do for a diagonal element? They are different.

204
00:25:00,880 --> 00:25:05,440
So they are different individuals. They have different variances. W is not equal.

205
00:25:05,440 --> 00:25:21,760
WJ So in this case not calculate this is better hat and this is not the least squared estimate that we have been using if we calculate this virus,

206
00:25:22,300 --> 00:25:25,030
the virus again while we just did this on the last slide,

207
00:25:25,420 --> 00:25:35,890
so it's equal to guess from the virus of wide where there is of absolute the same which is w then times the principles of that matrix.

208
00:25:36,100 --> 00:25:47,710
So this is a true virus but if you use R then are would still use are what do you see my square humps extra

209
00:25:47,980 --> 00:25:55,690
transpose times x humerus or I would still use this to calculate the standard error which is different from this.

210
00:25:55,690 --> 00:26:04,360
Not so. That's why the output, the conclusion you're actually looking at our outputs would not be correct.

211
00:26:06,010 --> 00:26:16,659
However, in this case, instead of using the square as major, we could use the so-called weighted least squared as major weight.

212
00:26:16,660 --> 00:26:21,490
Here, this is. This is what we missed where we look like.

213
00:26:24,700 --> 00:26:39,600
Okay. So if you compare it to beta hacked, a beta had is equal to x transpose times x inverse times x transpose and y, right?

214
00:26:39,790 --> 00:26:41,710
That's what beta hat is.

215
00:26:42,580 --> 00:26:52,360
Now if you compare a beta had to beta had w the difference is that for beta had w in the middle here we have w inverse and here we have w inverse.

216
00:26:54,970 --> 00:27:02,170
And then the beta had at the least caused me to become a special case by simply taking w equal to the added matrix.

217
00:27:02,320 --> 00:27:08,860
So in other words, here you could put the matrix inverse here and I learned matrix inverse here.

218
00:27:11,520 --> 00:27:17,310
So. So, in other words, speed ahead is actually a special case of fate ahead of.

219
00:27:18,670 --> 00:27:28,180
Special case when a special case when W is indeed the identity matrix so that all the dialog elements are equal.

220
00:27:29,560 --> 00:27:34,740
Okay. So this so-called weighted in this square,

221
00:27:34,750 --> 00:27:42,160
this is a commonly used technique to deal with the cost of the Mirror's violation of cost of mirror assumption.

222
00:27:45,910 --> 00:27:58,870
And then if you calculated the variance of beta W had there been a number that had is actually equal to its equal

223
00:27:58,870 --> 00:28:07,150
to is equal to this time I'm thinking whether maybe we should calculate and go through the calculation here.

224
00:28:08,830 --> 00:28:10,420
This is quite a simple calculation.

225
00:28:11,020 --> 00:28:22,660
So the variance of w being a w hat is equal to this times inverse time variance of y, then times the transpose of that matrix.

226
00:28:30,640 --> 00:28:38,530
And then the very of why we know that it's w that's our assumption is equal to W so this there is a y,

227
00:28:38,560 --> 00:28:42,880
this is equal to W, so this number will cancel with one W move.

228
00:28:43,860 --> 00:28:47,410
And then what is left in the middle you have x first o w inverse.

229
00:28:47,710 --> 00:28:54,310
The times this acts will cancel with which this will cancel with the current.

230
00:28:54,580 --> 00:29:01,750
So what is left with that is x transpose times double the inverse of the x that inverse which is what do we have here?

231
00:29:05,130 --> 00:29:20,040
Which is what we have. So this is the balance.

232
00:29:23,460 --> 00:29:29,430
So in other words, I mean, one solution to deal with violation lock horns, to be honest,

233
00:29:29,850 --> 00:29:36,520
is that you just ask R or SAS or whatever software to send a weighted list squared.

234
00:29:36,900 --> 00:29:45,390
Is that at least square? Because if you drag a field is a square, then the output of center area from ARC you'll not be cracked.

235
00:29:46,770 --> 00:29:51,570
So consequently the confidence intervals have all this to will not be correct.

236
00:29:52,350 --> 00:29:58,889
But if you are to calculate a waiting list to square that bar,

237
00:29:58,890 --> 00:30:03,480
we calculate that this ask major and then are would crackly estimate is

238
00:30:03,480 --> 00:30:09,840
terrorist try this one and then you will still have valid conclusion in the end.

239
00:30:11,340 --> 00:30:13,720
Practice in rhetoric prime comments here it will.

240
00:30:16,020 --> 00:30:23,459
But of course, in this case the problem is that while you are the we do not know what these double eyes are, what will we do?

241
00:30:23,460 --> 00:30:28,260
Know what the letters are, and we need to estimate this.

242
00:30:29,100 --> 00:30:32,639
So but this is going to be all the scope of this course.

243
00:30:32,640 --> 00:30:34,500
So let's not worry too much about that.

244
00:30:35,310 --> 00:30:45,420
But it was you we are was w is estimating or I mean r as long as we are in the right way, are we to estimate this automatically?

245
00:30:48,610 --> 00:30:52,690
So then it's not be a big deal. So this is one solution.

246
00:30:53,380 --> 00:30:57,610
So another solution is the so-called virus stabilizing transformations.

247
00:31:01,560 --> 00:31:05,920
So this is the V.A. or various people, as in transformation.

248
00:31:05,950 --> 00:31:11,570
This can be used when there is evidence against it because then there is assumption change.

249
00:31:12,900 --> 00:31:21,290
This is based on the observation that very often the virus is not a constant, is actually the function of the MIF.

250
00:31:21,810 --> 00:31:29,870
It's a function. So let's take a look at a very, very quick example here.

251
00:31:30,350 --> 00:31:40,580
So from 6:01 a.m., you guys must have learned that the food poisoning is usually the mean is equal to the virus.

252
00:31:40,640 --> 00:31:44,920
Well, the virus is equal to the meat ration. The purpose, as usual.

253
00:31:45,860 --> 00:31:51,740
Now, in that case, now the virus is not causing any more for the individuals.

254
00:31:51,980 --> 00:32:00,980
Now, if you decide to say tomorrow fit the puzzle, the account that I use instead of using puzzle or jail, am I in your rush model?

255
00:32:01,250 --> 00:32:07,459
Now the virus is not equal to limit. So the virus also depends on X because you assume the meaning of the act.

256
00:32:07,460 --> 00:32:12,930
So you are also given an x. So of course the virus assumption is violated.

257
00:32:15,410 --> 00:32:20,270
So there has all been. So in this case, a common practice is to.

258
00:32:22,420 --> 00:32:33,130
So the function G and then we make a transformation such that the variance of the transformation of Y sorry, the transformation of y.

259
00:32:33,640 --> 00:32:38,020
So the beta is of transformation of what is no longer a function of the mean.

260
00:32:38,980 --> 00:32:42,640
We can find such a function, you know, we can find a center function.

261
00:32:44,980 --> 00:32:48,250
And then once you find a certain function, we will see the model.

262
00:32:48,680 --> 00:32:53,110
It's not for the Y, for the regional Y, but A to the transform the Y.

263
00:32:53,110 --> 00:32:59,230
So we first made a transformation Y and then we filled this model in this case.

264
00:32:59,500 --> 00:33:07,090
Now, if you count it in the viewers of this trust from the light because of the transformation, the various does not depend on mean anymore.

265
00:33:07,240 --> 00:33:14,260
So we can approximate a C is a constant. So that's the so-called virus stabilizing transformation.

266
00:33:14,260 --> 00:33:24,100
So it's to try to stabilize the virus so that after transformation variance does not depend on the mean anymore on a problem.

267
00:33:24,100 --> 00:33:35,200
Well, this approach is that now you are modeling the transform transfer y, not at the original Y, so then your estimated value jump here.

268
00:33:35,290 --> 00:33:40,870
So you run the whole vision. It has different meaning from this beta.

269
00:33:41,260 --> 00:33:50,140
I mean, the beta not where you model the original one. So because here the beta, what better means is for y unit changing x,

270
00:33:50,380 --> 00:34:01,090
I will see on average in beta change in y beta much change in what but a here this is for one x change in for one

271
00:34:01,090 --> 00:34:11,380
unit a changing x we will see this gamma much changing transform y you transform so the interpretation change.

272
00:34:11,680 --> 00:34:17,649
So sometimes this this interpretation may not be so straightforward, especially for example,

273
00:34:17,650 --> 00:34:23,310
let's say if you consider a large transformation, pretty exponential transformation or square root of transformation, right?

274
00:34:23,380 --> 00:34:31,570
So then the value to continue would be to interpret this gamma in terms of the log of y or square root of y,

275
00:34:31,810 --> 00:34:38,890
which may not be very straightforward individually, not really, however of a benefit.

276
00:34:39,280 --> 00:34:46,240
That's the advantage that's kept. Caveat One of the benefit is that it makes after this transformation.

277
00:34:48,150 --> 00:34:52,190
The assumption, of course bearers is this is.

278
00:34:55,820 --> 00:34:59,840
No, we can't. While we can consider the assumption to to hold.

279
00:34:59,870 --> 00:35:04,130
Or we can. Well, that had to become some reasonable assumption so that we had the model.

280
00:35:04,460 --> 00:35:09,620
The model results are more trustworthy. So that's the benefit of making such a transformation.

281
00:35:12,090 --> 00:35:19,200
Okay. So that's the so-called we are in a civilizing transformation.

282
00:35:20,250 --> 00:35:23,370
Now, let's take a look at one specific example.

283
00:35:24,810 --> 00:35:28,200
Now, suppose that I follows a Poisson distribution.

284
00:35:29,110 --> 00:35:34,590
So it's account data. So let's consider, for example, the number of cancer cases in different messages.

285
00:35:35,050 --> 00:35:40,470
So these are part of an integer. And you heard of reporting of injure of anonymous people.

286
00:35:40,520 --> 00:35:47,409
Sylvia follows proposal distribution now 651 for generally in your model,

287
00:35:47,410 --> 00:35:54,640
you guys will learn how to model a portal discussion, how to how to well, that's called a possible regression.

288
00:35:55,300 --> 00:36:05,830
However, it's some cases even it's while people assume this puzzle or is counted or people still use literature tomorrow it in that case,

289
00:36:05,830 --> 00:36:10,000
if you use linear regression tomorrow account data there wise account,

290
00:36:10,750 --> 00:36:18,070
then you need to keep in mind imagine the variance if you assume it's also disappearing, the variance depends on the mean.

291
00:36:18,520 --> 00:36:27,790
So the cost we are is is bothered. And in this case we could make the square root transformation.

292
00:36:29,530 --> 00:36:32,620
And this is actually a very stabilizing transformation.

293
00:36:33,490 --> 00:36:38,320
This is based on. The phone, in fact.

294
00:36:38,830 --> 00:36:43,930
So here, let's not worry too much about mathematical details like why we have this approximation.

295
00:36:44,140 --> 00:36:55,180
And this is based on data method, which you guys will learn in 602 next semester, which by the way, I'll be teaching six, six months.

296
00:36:55,640 --> 00:37:03,120
So just to that, you guys have, you know, well, it seems that you just can't get a read on me as well.

297
00:37:03,500 --> 00:37:06,760
Well, you guys will see me again next semester.

298
00:37:07,960 --> 00:37:13,780
This is so true. But if you think so too, we will talk about this in much more detail.

299
00:37:14,120 --> 00:37:18,690
The the transformation method. But for now, let's not worry too much about that.

300
00:37:18,700 --> 00:37:20,350
Let's just not take this as a fact.

301
00:37:21,190 --> 00:37:30,940
So if we take the square root as the transformation, and if we calculate the variance of the square root of Y, then it turns out that.

302
00:37:33,490 --> 00:37:41,980
Oh. While the dollar method tells us that while a prisoner equal to this guy modified by the veterans of war.

303
00:37:42,360 --> 00:37:50,669
Okay. And now for. And then if we go through the calculation, if we apply the square transformation,

304
00:37:50,670 --> 00:37:54,660
it turns out that we are resolving some level of what becomes a constant.

305
00:37:57,770 --> 00:38:05,300
Okay. So this means that a while after the formation of the NSA, why now is a consent or a process and consent?

306
00:38:05,690 --> 00:38:11,780
So the causal matter is indeed a false. We do not need to worry too much about a positive assumption.

307
00:38:12,770 --> 00:38:19,400
So this is a very stabilizing transformation that we can apply in this case.

308
00:38:21,290 --> 00:38:29,749
So in other words, now, if you worry about because of the various assumption where you model counter data, when where you are still,

309
00:38:29,750 --> 00:38:39,290
why follow positive intuition, then you could consider for distress for your life by taking a square root in an event you model.

310
00:38:40,100 --> 00:38:45,200
And then you would, for example, you would fit a model y square root of y equal to x.

311
00:38:45,200 --> 00:38:56,600
I'd have been, I assume such a model then the absolute because of various assumptions what would possibly hold.

312
00:39:00,910 --> 00:39:06,960
So this table here listening to some commonly seen virus stabilizing transfer transformations.

313
00:39:07,450 --> 00:39:13,780
So it depends on how the winners are life depends on the meaning of life.

314
00:39:13,990 --> 00:39:17,170
So it does not depend on being a what if it's just a constant.

315
00:39:17,440 --> 00:39:25,760
Now, of course, we've got ed information. We could contract a lot of life, but he lagares is equal to the median.

316
00:39:25,780 --> 00:39:30,460
Like in the example we just saw that we could take the squirrel transformation,

317
00:39:31,330 --> 00:39:36,820
but even the virus is equal to the square of the meat, the mean squared.

318
00:39:37,150 --> 00:39:42,100
Then we could take luck. Transformation. That's a very stabilizing transformation.

319
00:39:42,460 --> 00:39:49,480
So but these are just, you know, our guidelines.

320
00:39:49,480 --> 00:39:59,350
So we do not have to follow exam because in practice there is no way we are certain the the virus, for example, depends on is equal to that.

321
00:39:59,710 --> 00:40:05,500
That took that to mean to the power three.

322
00:40:05,710 --> 00:40:13,420
So there's there is no way that we are certain about this. So so then these transformations, I mean,

323
00:40:13,420 --> 00:40:18,610
it's just based on I can you make different plots for example you plot the

324
00:40:18,610 --> 00:40:22,990
absolute versus the mean and a see roughly how the variance depends on the mean.

325
00:40:23,170 --> 00:40:26,800
And then you could choose one transformation from this table.

326
00:40:29,040 --> 00:40:35,609
So again, for local well, for no coastal areas, the most effective tool, I guess,

327
00:40:35,610 --> 00:40:43,080
is the plot to plot the residuals versus the theory of the models and to see if there is any Cleopatra.

328
00:40:44,280 --> 00:40:50,460
And one specific Cleopatra that is commonly seen is this one.

329
00:40:50,850 --> 00:40:58,590
So the residual as to the various increases with the means or when you get larger mean you get a larger virus.

330
00:40:58,860 --> 00:41:03,770
And once we see this, we can there are different ways of dealing with this result.

331
00:41:04,830 --> 00:41:08,220
We can use a square for their destabilizing transformation.

332
00:41:09,870 --> 00:41:14,130
We talk about these two ways, and there is another way.

333
00:41:14,190 --> 00:41:18,299
That's the so-called robust center error. So robust.

334
00:41:18,300 --> 00:41:25,620
And they're working their way in a way, a large recall that.

335
00:41:27,770 --> 00:41:34,129
Well, the calls on the areas assumption is violated. As we mentioned, as we mentioned now,

336
00:41:34,130 --> 00:41:40,430
the bearers of better hat if your calculus do cargo needs to square as a meter using are

337
00:41:40,760 --> 00:41:48,630
now the bearers of beta hat is as equal to this guy instead of the one reported about.

338
00:41:49,340 --> 00:41:55,400
Why are we are we use this this formula to capitalize on our error which is not a cracked.

339
00:41:56,000 --> 00:41:59,900
So the crack collision center should be equal to this guy.

340
00:42:01,550 --> 00:42:06,370
So then another way of addressing the violation of causal mirrors is well,

341
00:42:06,380 --> 00:42:11,550
instead of calculating scenario in this way, I would just close in error in this, right?

342
00:42:11,740 --> 00:42:14,930
If I use the crowd follower, then I will get to the crack center error.

343
00:42:15,680 --> 00:42:25,219
So this is the so-called robust center error. The bus in error is also called the sandwich for center error is called so in for because you

344
00:42:25,220 --> 00:42:31,580
know it are three pieces but that these piece this piece and then the bearer is in the middle.

345
00:42:32,210 --> 00:42:37,610
So that's often happens. People also refer to this as the sandwich estimated for standing around.

346
00:42:42,950 --> 00:42:51,010
And you can you this standard ever by. I mean are you going to use this function to get robust internet.

347
00:42:57,660 --> 00:43:09,610
So you guys will definitely see that more about a robust area in, say, 53, 73, but dealing with the violation of independence assumption.

348
00:43:09,630 --> 00:43:20,940
It turns out that what remedies also to use the robust center error is that of the scenario given by dragonfly fitting sigma linear regression.

349
00:43:25,040 --> 00:43:29,320
Okay. Any questions about the cost versus option?

350
00:43:36,380 --> 00:43:39,890
Okay. And then let's take a look at the normality assumption.

351
00:43:43,410 --> 00:43:46,550
For normal, your assumption? Well, there are different ways.

352
00:43:46,560 --> 00:43:51,810
What we call the nomadic assumption means that we assume assume all a normal

353
00:43:51,810 --> 00:43:57,780
distribution would mean and of course the merits and to normal your assumption.

354
00:43:58,680 --> 00:44:09,870
Well, we have different the most widely used way of tracking or something is graphical is graphical tools.

355
00:44:10,170 --> 00:44:18,480
So of course we have we have a formal test, but the graphical tools are perhaps the most widely used way of checking normality assumption.

356
00:44:19,830 --> 00:44:28,170
He's read the box plot and the cuckoo plot. Let's take a look at one by one what he saw his rat on first.

357
00:44:28,620 --> 00:44:37,890
I mean, if all of you guys have seen his crap so we can answer the bigger a plot of his crime, of the residuals of the residual.

358
00:44:38,190 --> 00:44:44,549
Now, you guys probably already have the impression that plus, based on residuals,

359
00:44:44,550 --> 00:44:49,110
this is a very important tool in tracking all these different assumptions,

360
00:44:49,830 --> 00:44:56,850
making assumptions of linearity, assumption of invaders, cause there is now no analogy.

361
00:44:57,120 --> 00:45:01,920
We can always plot of the residual to check all these assumptions.

362
00:45:01,950 --> 00:45:04,470
This is a graphical tool of checking those assumptions.

363
00:45:04,740 --> 00:45:10,530
Of course, we need to be careful plotting the residual versus what I happened in traveling the RV.

364
00:45:10,770 --> 00:45:20,550
You might plot of the residual or partial residuals versus versus each covariate and versus not predicting the about.

365
00:45:21,060 --> 00:45:27,810
Now when we check the closing in mirrors, we will part of the neutral versus y hat and see if there is any clear pattern.

366
00:45:28,470 --> 00:45:33,060
So ideally, when you make this plots, ideally we'll make this plot.

367
00:45:37,250 --> 00:45:41,570
Part of the residual. Ideally, we shouldn't see any clear pattern.

368
00:45:41,990 --> 00:45:43,220
So that's an ideal case.

369
00:45:43,340 --> 00:45:49,880
If you if we do not see any clear pattern, that's an indication that, okay, so there's our assumptions are probably good, good enough.

370
00:45:50,360 --> 00:45:53,690
But if you see any clear pattern, any clear pattern,

371
00:45:54,380 --> 00:46:02,660
you might want to think twice of what assumptions might have been violated and then try to identify revenue to address that.

372
00:46:04,520 --> 00:46:14,120
So here we can make a plot of a scrap like the the absence wise should be normal distributing.

373
00:46:14,120 --> 00:46:17,830
In other words, they should be symmetric, roughly symmetric, bell shaped and light tailed.

374
00:46:18,170 --> 00:46:25,940
I got the plot shown here. Of course, in practice we will never see a perfectly normal distributed his ground.

375
00:46:26,660 --> 00:46:32,300
But sometimes no. But once you see such a sort of huge ground, I think many people would think of this.

376
00:46:32,660 --> 00:46:38,540
Well, it's not exactly normal, but it seems to be okay in public.

377
00:46:39,110 --> 00:46:42,740
You don't need to worry too much about the normality assumption.

378
00:46:45,590 --> 00:46:52,310
Well, the problem of making a huge promise that if you were to win your fair and a large number of individuals together, reliable information.

379
00:46:52,760 --> 00:46:58,190
So if you only have five individuals, ten individuals and she's around, probably doesn't work anymore.

380
00:46:59,480 --> 00:47:08,420
And also, he's wrong because all of the be in size or in other words, here how wide each each each bar is.

381
00:47:08,600 --> 00:47:14,330
So if you if you make light of the bar too wide, if you make the bar too wide,

382
00:47:14,930 --> 00:47:20,590
then probably it's not a very effective in revealing that true fusion residuals.

383
00:47:20,780 --> 00:47:25,880
So we have to select the fin size appropriate.

384
00:47:27,980 --> 00:47:32,420
So while question is why can't we simply plot of the histogram of white eyes?

385
00:47:33,170 --> 00:47:37,250
Why do we have to plot the the piece from of residuals?

386
00:47:37,790 --> 00:47:45,980
Well, the reason is that while the reason is that we call it a what is equal to expired beta plus.

387
00:47:48,140 --> 00:47:53,300
And our assumption is that absolute I follow normal distribution.

388
00:47:54,290 --> 00:48:02,450
Although, I mean, although this one, I would also follow normal distribution because I mean, this is a shift of epsilon because exercise constant.

389
00:48:03,080 --> 00:48:08,240
However, not because each y would have different mean.

390
00:48:08,270 --> 00:48:15,200
So if you put these y eyes that have different means together, then it could be farther.

391
00:48:15,350 --> 00:48:22,640
If you make out his right leg, his back could look very nasty. It's not necessarily, you know, bell shaped, symmetric, anything like that.

392
00:48:23,000 --> 00:48:31,400
So that's why usually when we try to promote it, we do not make part of the Y eye, but we fit a model and we make part of the residual.

393
00:48:31,880 --> 00:48:39,960
The residual. We look at the case where all the residuals. So that's his rap.

394
00:48:39,990 --> 00:48:47,700
Another one is the so-called black spot. And this is what a typical black spot looks like, this part.

395
00:48:48,120 --> 00:48:52,830
But typically, you look at a vertical, you would look at it this way.

396
00:48:53,030 --> 00:48:56,400
So this is a horizontal. He's caught on a box block.

397
00:48:57,540 --> 00:49:03,570
So for normal fusion. Now we know that for normal distribution, the mean is equal to the median.

398
00:49:03,660 --> 00:49:10,110
The median. You see what I mean? And both are equal to zero. So once you plot a his graph of the residual.

399
00:49:10,170 --> 00:49:16,980
So again, here we make a plot of the box, spot of residual of the absolute highest.

400
00:49:19,280 --> 00:49:26,870
And if the normal u assumption approximately hold, then the median here should be roughly around zero,

401
00:49:27,160 --> 00:49:32,270
be very close to zero, and also the first quartile and a third quartile.

402
00:49:32,600 --> 00:49:38,570
This should be roughly we should have roughly equal distance from the meeting out of that.

403
00:49:38,870 --> 00:49:46,160
Well, there shouldn't be too many outliers or worse on the two X, you could have some outliers, but not too many.

404
00:49:47,030 --> 00:50:01,400
Maybe just a few. So if you see such histogram, you're indicates that that assumption approximately hold not a history, but a on the box plot.

405
00:50:07,550 --> 00:50:15,690
It's not like you plot, but maybe maybe you should take a plane that's take a five, six minute break and then we'll.

406
00:50:31,636 --> 00:50:38,535
And this is maybe the most widely used pool you are looking for and is used often.

407
00:50:38,536 --> 00:50:44,596
But Occupy, I think, is probably and arguably probably the most widely used radical tool.

408
00:50:45,106 --> 00:50:48,186
So the Kuku Lodge is a great creation.

409
00:50:48,376 --> 00:50:54,976
Your body is a little involved, but fortunately we do not need to worry too much about creating Google Cloud

410
00:50:54,976 --> 00:51:00,506
ourselves because our or without our software if we ask probably the three of us.

411
00:51:01,456 --> 00:51:13,275
But let's let's first let's see how you pod works cube plus stands for somehow on now what sort of what what

412
00:51:13,276 --> 00:51:21,526
of how it works or how it is created is that now for our sample we look at a standardized residual random Z.

413
00:51:21,526 --> 00:51:28,126
I recall that Z I had this is the standardized residual we defined for the standardized residual.

414
00:51:28,216 --> 00:51:36,076
If you order all the standardized residuals from the smallest for the largest and

415
00:51:36,136 --> 00:51:42,586
that if this original followed normal distribution for a normal distribution,

416
00:51:43,366 --> 00:51:53,656
then we will make a plot of this residuals versus the corresponding compound of a normal this version of a normal normal this year.

417
00:51:53,926 --> 00:51:58,245
Just imagine this model forget about notation for a minute let's just the thing about

418
00:51:58,246 --> 00:52:03,886
this intuitive so you have a symbol you have made you calculate the residuals,

419
00:52:04,366 --> 00:52:12,286
a standardized residuals. You look at let's say you have a thousand or a thousand individuals, so you have a thousand standardized residuals.

420
00:52:13,666 --> 00:52:18,075
And now suppose that indeed the data follow a normal distribution error for one of

421
00:52:18,076 --> 00:52:20,986
the students so that standardized rules are also followed normal distribution.

422
00:52:21,796 --> 00:52:26,566
Now you have this a thousand residuals that are taken from normal diffusion.

423
00:52:27,556 --> 00:52:39,876
Now, if I look at the, for example, the thirties, somehow of this 1000 residuals and I compare it to more of a 30% in the theoretical 30,

424
00:52:39,896 --> 00:52:46,275
30, 40, 30% now a normal distribution, then there should be roughly equal similarity.

425
00:52:46,276 --> 00:52:52,816
If I look at a different person present, I look at a first quarter, but I look at the first quarter.

426
00:52:54,526 --> 00:53:03,046
If I look at my residuals, if I look at first quarter, then the value should be roughly the same as my first quarter out of a normal distribution.

427
00:53:03,526 --> 00:53:09,966
If I look at the median now, the median should be roughly the same as my normal is for the medial monogamous.

428
00:53:10,216 --> 00:53:19,095
I look at a third part how there should be similar to my third quarter earnings, so that's how the coupon is created.

429
00:53:19,096 --> 00:53:23,716
So it looks at different personnel of your sample.

430
00:53:23,716 --> 00:53:34,096
I calculated residuals, standardized residuals and then look at the corresponding personnel from normal as usual and in May,

431
00:53:34,106 --> 00:53:39,616
plus a list and one versus the other. And if normal year assumption holds,

432
00:53:39,616 --> 00:53:46,066
we know that the the standardized residual roughly follows in a normal distribution like after summarization you follow center normal.

433
00:53:46,486 --> 00:53:52,366
So then we can compare the present help from this centralized residual.

434
00:53:52,816 --> 00:53:57,895
We can compare their present house to the personnel of the normal distribution center.

435
00:53:57,896 --> 00:54:07,666
Rubble usual. And if you are close, if ever present now is close, then more than normal.

436
00:54:07,666 --> 00:54:11,416
Your assumption approximately hold. Right. So that's done.

437
00:54:12,276 --> 00:54:18,176
The idea behind it you can plot. So that's why it's called quantile versus now.

438
00:54:19,606 --> 00:54:28,996
But for some people, it's really just like the so-called people in prison now versus prisons and how it works exactly the same way.

439
00:54:29,666 --> 00:54:33,316
So so this is Cuomo versus Cuomo.

440
00:54:33,646 --> 00:54:41,096
How long have plot and if normally assumption hopes that the kuku plot should give us a straight line?

441
00:54:41,806 --> 00:54:47,896
I go here. This is a figure showing what different scenarios, what the coup plot might look like.

442
00:54:48,736 --> 00:54:52,666
If you look at if we look at the very last one here, the very last one.

443
00:54:56,266 --> 00:55:00,286
In this case, this is the residual histogram of the residual.

444
00:55:00,916 --> 00:55:04,036
Well, apparently this is not a perfect normal distribution.

445
00:55:04,396 --> 00:55:10,216
Ira, it does seem to be too bad. It's roughly symmetric with a P in the middle.

446
00:55:10,766 --> 00:55:18,106
This is definitely not perfectly symmetric, but it's in this symmetric not too far away from normal.

447
00:55:18,616 --> 00:55:25,546
So in this case, if we make a peculiar plot, we see that most points we fall along this along this line.

448
00:55:25,786 --> 00:55:32,916
So, look, this plot, it'll make parts of the sample, residual sample on house versus theoretical clubhouse.

449
00:55:33,626 --> 00:55:37,876
Well, sample Wodehouse on the Clone House based on the standardized residuals.

450
00:55:38,656 --> 00:55:43,336
And I'm a theoretical house. These are all now based on standard normal distribution.

451
00:55:44,086 --> 00:55:46,666
So we make a plot of one versus the other.

452
00:55:47,206 --> 00:55:55,096
Even remotely the assumption the possible hold that the two of you should see roughly a straight line for the Q plot.

453
00:55:56,116 --> 00:56:07,816
So this is a good indication of the of the of the model, the assumption that if the assumption is violated, like here we have five scenarios here.

454
00:56:08,116 --> 00:56:11,445
First of all, we have a rather skewed distribution.

455
00:56:11,446 --> 00:56:21,286
That means we have a longer, much longer right tail and we have less skilled, less skilled if we have a much longer level of tail.

456
00:56:22,096 --> 00:56:27,926
And also, we have this like, if you like, this sort of uniform, a disability.

457
00:56:28,186 --> 00:56:37,036
There is no peak in the middle sort of uniform this way. And also, we have such a sort of this fusion, while there is a very happy pig in the middle,

458
00:56:37,036 --> 00:56:42,076
there is no almost no, no, no data points on the two ends.

459
00:56:42,796 --> 00:56:52,056
So it also we have such a like by. I'll find out who does this one has a better had put in the middle.

460
00:56:53,556 --> 00:57:00,216
So for all these different scenarios, you know, if you make a pot will see if you find a criminal plot like in this case,

461
00:57:00,636 --> 00:57:07,806
the Sembawang house, well, here towards the left and the Sembawang house would be larger than the building collapse.

462
00:57:07,806 --> 00:57:12,816
You see that? It goes just upward above this, above the straight line.

463
00:57:14,926 --> 00:57:22,906
So if we see Google BLOCK like these, you know, there is serious deviation from the storyline.

464
00:57:23,416 --> 00:57:35,056
You are an individual occur on the two. If we see serious deviation from the storyline, that's an indication that no assumption is violated.

465
00:57:38,086 --> 00:57:45,586
So what I'm going to process the whole time we should, I think, which receives most of the points should fall along this line.

466
00:57:49,326 --> 00:57:57,576
Okay. That's Q plot. And those are graphical tools like scrap cube cloud and inline box spot, radical tools.

467
00:57:57,606 --> 00:58:03,416
Now we do have a more formal like past testing rather than a major assumption.

468
00:58:03,446 --> 00:58:08,736
Host So this is the so called four patrol walks test.

469
00:58:11,496 --> 00:58:20,316
So how it works is that it examines the so called a correlation or something similar to correlation

470
00:58:20,976 --> 00:58:29,826
between the standardized residuals and the foundation of a standardized residuals assuming normality.

471
00:58:29,856 --> 00:58:41,586
So if we assume normality is true, if we assume that indeed in the data followed normal restriction, then we will calculate this.

472
00:58:41,586 --> 00:58:44,526
This guy, the rule squares, which is calculated this way,

473
00:58:44,796 --> 00:58:54,185
is sort of matters the association or correlation between the centralized residual and expected value of the standardized residual

474
00:58:54,186 --> 00:59:03,606
under normal assumption where this Jean this is just a collection of all the standardized residuals from all different individuals.

475
00:59:04,566 --> 00:59:11,136
So we calculate this quantity and then a low value of this quality.

476
00:59:12,576 --> 00:59:26,766
A low value of this quantity. And it means that the Z I or the Z hat does not a well with this mean under normal distribution on a nervous future.

477
00:59:27,126 --> 00:59:31,716
So if they do not agree well, of course. I mean, the normal assumption is slightly valid.

478
00:59:32,286 --> 00:59:39,906
That's not hold. So in other words, if we see a small band, if we see a little doubt, this is evidence against null hypothesis.

479
00:59:41,256 --> 00:59:48,696
And if you in our you could directly use this test. So and then you look at the P value if the P values Martin equal to five.

480
00:59:49,026 --> 00:59:56,496
Now we do not original not well this is that we can we tend to think oh I mean the normality seems to be a good assumption, residual assumption.

481
00:59:56,946 --> 01:00:03,936
But if the P last four or five, then we might think what to do, how to address this violation of normality assumption.

482
01:00:05,046 --> 01:00:17,316
So that's all a formal test. Now, let's take a look at of the impact, the consequences of violation.

483
01:00:17,946 --> 01:00:23,976
What happens if the normality assumption is violated unless directly according this life.

484
01:00:24,636 --> 01:00:34,896
So if normality does not hold. Now if you look at B there had been a have is due unbiased for the same reason is

485
01:00:34,896 --> 01:00:39,626
now the same reason as before because if you calculate these values of beta hash.

486
01:00:40,026 --> 01:00:45,116
Now let's skip that the detailed expression is equal still able to better.

487
01:00:45,546 --> 01:00:55,656
And this calculation does not depend on the assumption of normality as well as a region of y is equal to x times data as well as we have.

488
01:00:55,866 --> 01:01:04,446
The majority of assumptions cracked that the the beta had is a bias as major so.

489
01:01:04,586 --> 01:01:16,955
So the beta how to remains unbiased and a C was squared had this also remained unbiased because if you check the how we show that to expand I don't

490
01:01:16,956 --> 01:01:25,836
see one square and is equal to similar square that a correlation does not mean that all of mammalogy either it does doesn't require normality,

491
01:01:26,646 --> 01:01:31,836
so it will not affect the Sigma Square of the estimate of similar square.

492
01:01:32,556 --> 01:01:40,386
Now, what about the beer? Is a beta hash there has been a hash is still valid.

493
01:01:40,596 --> 01:01:43,596
The reason is that now if we can count on this, good.

494
01:01:43,716 --> 01:01:47,136
So the theories of beta had better had is equal to.

495
01:01:51,846 --> 01:02:04,066
Is equal to that. So if you come into the areas of this. Is equal to this.

496
01:02:06,936 --> 01:02:15,156
And then the bearers of why. So as long as the bearers of why is equal to sigma squared perhaps I don't know the matrix.

497
01:02:16,416 --> 01:02:23,616
As long as various of why is equal to less, then this whole thing will be simplified to.

498
01:02:26,186 --> 01:02:29,516
This guy. Right. So.

499
01:02:30,086 --> 01:02:37,206
So, in other words, there is no more margin required for this guy to be equal, this guy.

500
01:02:37,616 --> 01:02:44,846
So as long as the subjects they are, they are independent and they have caused marriage equal barriers,

501
01:02:46,076 --> 01:02:51,176
then we will have the appearance of being a had equal. So there is no no, no, you essentially did.

502
01:02:52,166 --> 01:02:55,496
So in other words, various as major is still valid.

503
01:03:05,026 --> 01:03:08,946
Now the. Okay.

504
01:03:09,696 --> 01:03:12,576
So then the hypothesis has a component in intervals.

505
01:03:13,446 --> 01:03:22,596
This becomes well invalid under a small sample size but valid if the sample size is large and a you guys might wonder like why is this case?

506
01:03:23,106 --> 01:03:33,996
Why is the case? Why is this looking because here we found out about the has made it out a bit ahead is still valid now the variance of beta how this

507
01:03:33,996 --> 01:03:42,426
is also still valid then how come that the how well this house and confidence intervals become invalid what the sample size small.

508
01:03:42,876 --> 01:03:51,366
The reason is that now for linear regression if you recall that our output if you recall the quality the

509
01:03:51,466 --> 01:03:58,146
table now look over a table gives you the estimate is beta this one column and then there is a column

510
01:03:58,356 --> 01:04:05,846
center error I want column and then the column t value and then see that the function if you recall

511
01:04:06,096 --> 01:04:15,396
that the column that's called a key for p value where culpability that is based on normal assumption.

512
01:04:15,786 --> 01:04:16,746
No. Now the assumption.

513
01:04:18,686 --> 01:04:31,166
So recall that Tejas fusion is defined as a normal rainbow variable divided by a chi square meter, and that's how we derive the t statistic.

514
01:04:32,156 --> 01:04:36,266
Now that actually relies on normal assumption of energy or something.

515
01:04:37,856 --> 01:04:49,376
So in a linear regression. So this is maybe you just explain a little bit more so in linear regression.

516
01:04:51,566 --> 01:04:57,746
If you look at all the results we have so far, we have results based on t test.

517
01:04:58,046 --> 01:05:02,216
Based on houston, we have a results based on our studies. Avatar's ev statistic.

518
01:05:03,266 --> 01:05:09,836
The reason that we are able to derive t statistic i t news fusion fs evidence fusion.

519
01:05:09,986 --> 01:05:15,635
The reason is precisely because we made the assumption we assumed an error followed normal

520
01:05:15,636 --> 01:05:22,406
this fusion because what we assumed normal was we assumed normality and normal this fusion.

521
01:05:22,406 --> 01:05:25,555
So special people have started normal.

522
01:05:25,556 --> 01:05:31,256
This fusion extensively is well understood, not because the data follow normal.

523
01:05:31,256 --> 01:05:32,756
Those who are error for normal disorder.

524
01:05:32,756 --> 01:05:43,586
Then we are able to derive legislative, state pollokshields fusion absolute to follow after solution if the value assumption is violated.

525
01:05:44,396 --> 01:05:50,326
So basically we do not. Generally speaking, we do not have no well, we are still able to construct historicity.

526
01:05:50,336 --> 01:05:57,926
How do you you guys do numerically calculate statistic but his studies do not follow the utility and a similarly

527
01:05:57,926 --> 01:06:03,935
you are still you're only are able to calculate out statistic you are still can guess you apply your data.

528
01:06:03,936 --> 01:06:07,886
I calculated but in absolute SD no longer follow evidence fusion.

529
01:06:08,936 --> 01:06:20,426
So the reason that we have t this we are an evidence fusion is precisely because we have assumed without or for so many other words now,

530
01:06:20,696 --> 01:06:26,446
without assuming normality, without assuming normality, the key will not follow.

531
01:06:26,476 --> 01:06:31,555
Previous fusion F will not follow evidence fusion. So that's why here the house task.

532
01:06:31,556 --> 01:06:38,876
In a couple of scenarios, general speaking, we are not valid anymore when the sample size is small,

533
01:06:39,206 --> 01:06:47,066
looking right from one for that one sample size is small. So this is still valid if the sample size is large.

534
01:06:47,726 --> 01:06:53,876
This is because of the so-called a central limit theorem, which you guys must have learned since a lot.

535
01:06:54,836 --> 01:07:00,176
And we are going to talk more about this in six or to when we talk about a scenario statistics.

536
01:07:00,746 --> 01:07:05,096
So it turns out that have all this testing and endless chorus intervals,

537
01:07:05,966 --> 01:07:13,826
we are still approximately valid if the sample size is large, even if normality is not so.

538
01:07:13,826 --> 01:07:22,076
In other words, normative assumption. Usually we don't worry as much as the other three assumptions were not assumptions.

539
01:07:24,836 --> 01:07:26,636
Well, it holds, but that's great.

540
01:07:26,816 --> 01:07:36,076
If it doesn't, then we have very easy ways of addressing valid assumptions because, you know, the estimating value is valid.

541
01:07:36,116 --> 01:07:43,916
The errors center error estimate is valid now always has a couple of days intervals.

542
01:07:44,276 --> 01:07:49,286
Now of course, the T distribution is violating the terms of the T anymore.

543
01:07:49,466 --> 01:07:53,816
But we know that the T decision, whether you are freedom is large.

544
01:07:54,626 --> 01:08:01,706
It becomes normal, right? It is very close to normal. So well, the sample size is large and that your freedom will be large.

545
01:08:02,156 --> 01:08:10,976
So that's that's the and the reason why we don't need to worry too much about violation normality assumption.

546
01:08:11,996 --> 01:08:18,865
However, while it may be a problem if your sample size is is small to moderate is not too large.

547
01:08:18,866 --> 01:08:23,996
In that case you could actually consider make some transformation of the response

548
01:08:24,326 --> 01:08:30,295
to make the center error for us to make the the error for more distributed.

549
01:08:30,296 --> 01:08:35,126
You could have consider the square root of transformation or not transformation, things like that.

550
01:08:39,316 --> 01:08:44,536
Okay. So a note.

551
01:08:44,596 --> 01:08:51,046
Well, this is just a well, just just to repeat what I what I said.

552
01:08:51,526 --> 01:08:57,826
Ordinarily some square as major the all our less well asked that's how we derive a

553
01:08:57,826 --> 01:09:05,806
better hat right although this is quite a robust to departure from formality so.

554
01:09:07,096 --> 01:09:16,896
So if we have well if we see violation of normality that all else itself is still unbiased as major and listed.

555
01:09:16,896 --> 01:09:27,186
Standard error is to arise as major of the true standard deviation or the variances most absolutely bits of.

556
01:09:27,736 --> 01:09:31,486
And then well, the sample size is large based on central linear zero.

557
01:09:32,086 --> 01:09:38,906
Now we do not need or too much about that. This has to continue zero because they still approximately hold.

558
01:09:42,826 --> 01:09:49,336
Okay. So this is a table that tried to summarize what we have talked about so called triangles,

559
01:09:49,336 --> 01:09:53,536
assumptions, what of course the cosmologies are when those assumptions are valid.

560
01:09:54,196 --> 01:10:02,355
So for linearity now let's go over this because we have talk about we have covered this in seven lectures.

561
01:10:02,356 --> 01:10:07,996
So I think it's worthwhile to take a big picture view on this.

562
01:10:08,326 --> 01:10:17,446
So linearity or assumption, this is probably the most assumption that we want to verify whether verified.

563
01:10:17,686 --> 01:10:22,486
While other assumptions are also important, however, academia give us an idea.

564
01:10:22,516 --> 01:10:29,786
If Leonardo summaries validated, then basically that means all of what I would do subsequently are meaningless.

565
01:10:30,076 --> 01:10:37,876
And the reason is that, well, we see the model usually, but we have to assume that a model is reasonable.

566
01:10:38,146 --> 01:10:41,266
Is it is can can be used to approximate the truth.

567
01:10:42,256 --> 01:10:48,256
If the model is so dramatically severe that mystified is so far away from the truth.

568
01:10:48,706 --> 01:10:53,086
There is no point of feeling that models the model doesn't really approximate to the truth at all.

569
01:10:53,086 --> 01:10:59,476
So what was the point to it? Will all of a subsequent sort of estimation and inference.

570
01:11:01,066 --> 01:11:07,515
So for linear regression model, of course linearity that's not the how is business model.

571
01:11:07,516 --> 01:11:14,836
So we assume that no, the model right depends on both near-term and in this case,

572
01:11:14,836 --> 01:11:19,965
we have to make sure that we have to check how wide depends on these different

573
01:11:19,966 --> 01:11:27,195
covariates and whether we should include X in itself alone or also squirrel max,

574
01:11:27,196 --> 01:11:32,326
for example. And we could check this by looking at the partial regression plot,

575
01:11:32,956 --> 01:11:42,436
or we could like break the continuous variable into different categories and then check whether linearity of possibly halt.

576
01:11:43,666 --> 01:11:53,956
So there are different ways of checking linearity and if linearity is violated, so then your estimated value is problematic.

577
01:11:54,646 --> 01:11:59,686
So it's not about adding more. And in your past we're confidence intervals, they're not valid anymore.

578
01:11:59,716 --> 01:12:07,786
So so essentially the narrative is after you smile and then whatever you do subsequently, they're minimalist.

579
01:12:07,906 --> 01:12:21,646
There's no point of doing anything for. And well, even the narrative assumption, if you think is violated, then you could transfer X or Y.

580
01:12:22,186 --> 01:12:27,106
So for example, you could consider x squared and a square term in the model,

581
01:12:27,436 --> 01:12:37,216
or you could consider adding a polynomial function of x for equal to consider tries for y other transformation of x or y.

582
01:12:38,626 --> 01:12:47,896
So that in a linearity assumption approximately, of course, we never believe the Y depends on X exactly.

583
01:12:47,896 --> 01:12:55,456
In a linear fashion. Right. So even if we see that in your regression model, we don't we rarely do we believe that this is the true model.

584
01:12:55,966 --> 01:12:59,716
It's just that this model may be good enough to approximate the truth.

585
01:13:00,196 --> 01:13:08,776
So so in that sense, we do not want, you know, very clear, very, very severe violation of the act.

586
01:13:09,496 --> 01:13:16,516
So definitely they are doing something now. Independence of assumption, although there are different ways I'm tagging anyone.

587
01:13:16,666 --> 01:13:25,276
Assumption Part of the most straightforward, most effective way is probably to look at the design and to look at a holiday at our planet.

588
01:13:25,906 --> 01:13:31,996
And by looking on that, we have a pretty good idea whether an event of a subject is reasonable or not.

589
01:13:32,836 --> 01:13:40,846
And again, I want to point out that sometimes, even if there is a clear violation, there seems to a clear violation of independence assumption.

590
01:13:41,146 --> 01:13:47,626
Some people might still go ahead and feel a little more independence assumption.

591
01:13:47,986 --> 01:13:55,606
The reason is that in this case, the Predator estimate is still valid, which means they are still unbiased.

592
01:13:56,086 --> 01:14:04,006
Now the standard error may be problematic, but we could use the robot, the so-called robust and or error is that.

593
01:14:04,336 --> 01:14:14,506
And by using that we are still able to get valid inferences accommodate in scenario in the end of the space of coverage detail coverage in this order.

594
01:14:14,506 --> 01:14:24,795
So let's not worry too much about that. But even as assumption Yuri this is not too much do so 200 problems or we have

595
01:14:24,796 --> 01:14:30,286
different ways of dealing with that and then there is not equal areas of assumption.

596
01:14:30,856 --> 01:14:38,536
So we can check out by making a plot, the residual plot of land versus what happens and if this is violated.

597
01:14:39,656 --> 01:14:50,306
Then the parameter estimate is still about boarded up the test end and the accommodation or housing become problematic.

598
01:14:51,206 --> 01:14:58,526
But again, we have ways addressing those problems. So we could, for example, apply the waiting lists where where we could test for one,

599
01:14:58,886 --> 01:15:05,036
or we could again use the so-called robust center in our true estimate of the corresponding standard.

600
01:15:05,816 --> 01:15:10,526
So this is also not a huge deal. We have different ways of dealing with this.

601
01:15:10,976 --> 01:15:20,276
The violation of equal barriers. Now for mobility, we have a bunch of graphical tools of checking that.

602
01:15:20,486 --> 01:15:28,106
And also we have one formal test now for NorBAC in Europe, and this is the least worrisome assumption.

603
01:15:28,586 --> 01:15:33,056
So of course we will lack normality to hold. We will have this assumption to hold.

604
01:15:33,686 --> 01:15:38,986
What if it's violated? Then we don't need to worry too much.

605
01:15:38,996 --> 01:15:40,805
I mean, there are other ways of addressing that.

606
01:15:40,806 --> 01:15:47,186
If, for example, that comes from being alive and also the results, especially when a sample size is large.

607
01:15:47,646 --> 01:15:58,526
Well, it's not a big deal to have an analysis of validate, but to see what I mean for linear regression is common, a common practice.

608
01:15:58,856 --> 01:16:02,626
People still check normality assumption and a still try to find ways to.

609
01:16:03,056 --> 01:16:10,316
To. To. To. To make a dramatic assumption may hold.

610
01:16:11,366 --> 01:16:15,595
Okay. So so when this assumption is violated,

611
01:16:15,596 --> 01:16:25,586
the estimated value or parameter estimates there are valid the standard errors or asari to the test, the test or a confidence interval.

612
01:16:25,586 --> 01:16:32,095
They become problematic again because for linear regression the test is complex intervals.

613
01:16:32,096 --> 01:16:39,966
They are based on the tedious fusion work after fusion and a both are based on expensive is based on the normality assumptions.

614
01:16:40,286 --> 01:16:50,516
However, if normally a summary is followed, it is not too to big deal because t tells us its large t is close to normal kind of f,

615
01:16:50,996 --> 01:16:54,266
but there are other ways to replace the test.

616
01:16:54,536 --> 01:17:00,116
We have lagged original pass through placed on the outcast, so that's not too much a problem.

617
01:17:02,746 --> 01:17:08,326
Okay. So this is a quick summary of these four assumptions.

618
01:17:08,776 --> 01:17:12,326
Any questions before we look at something?

619
01:17:12,326 --> 01:17:15,676
Examples? Yes. So we transformed why?

620
01:17:15,796 --> 01:17:19,156
So we need to use that in a way to feed them.

621
01:17:20,776 --> 01:17:29,416
So after we transform the Y, we use a new way to fit the model or use the original 1907 format.

622
01:17:29,416 --> 01:17:36,406
We will use the new new way. So, yeah, that's the that's the that's the reason why we transform.

623
01:17:36,466 --> 01:17:40,906
So let's say if your y is I, I make you have an example.

624
01:17:40,906 --> 01:17:50,115
Let's write a skilled have a, have a really long run tale in that a case log transformation is a commonly used transformation so that you would take

625
01:17:50,116 --> 01:17:58,786
a lot of y and then you would you would your model you would use Y as their new response is that of Y and a new fit.

626
01:17:58,786 --> 01:18:02,806
It would fit a linear regression model y. Y you go to speed up plus else.

627
01:18:04,206 --> 01:18:09,666
Just. Any other questions?

628
01:18:16,826 --> 01:18:20,216
Okay. Okay. So we have this slide.

629
01:18:20,496 --> 01:18:24,466
Oh, basically, this is just a summary of what I what I mentioned.

630
01:18:24,476 --> 01:18:28,116
I don't think we need to we need to look at the slides in detail.

631
01:18:28,346 --> 01:18:39,626
This is you guys can look at this. But this is this is a very famous quote from 40 bucks who was a very famous institution.

632
01:18:39,986 --> 01:18:46,256
So all models are wrong, but some are useful. So this is something I tried to emphasize.

633
01:18:46,556 --> 01:18:53,816
So all the models that we learned were you guys will learn in Cincinnati, 51, 53 and in many other persons,

634
01:18:54,206 --> 01:18:59,506
all these models, they are we do not believe they are the truth, like the truth online.

635
01:18:59,576 --> 01:19:07,406
Truth is just that they are good enough where we want to find a model that are good enough to approximate but are not truth.

636
01:19:07,866 --> 01:19:11,396
And so as long as we have a good approximation, then that's.

637
01:19:11,396 --> 01:19:15,515
That's good enough. Okay.

638
01:19:15,516 --> 01:19:22,256
So now let's take a look at the one example once our examples are.

639
01:19:22,986 --> 01:19:32,736
So here we have the response is actually the so-called first expert rating volume.

640
01:19:34,176 --> 01:19:39,676
And this is our response. And then we have a few cameras.

641
01:19:39,676 --> 01:19:49,246
We have age that is continuous and hide that is continuous and male.

642
01:19:49,516 --> 01:19:58,996
That is a sorry, the male this is indicator for for male and then we have smoke variable this indicator for smoker.

643
01:19:59,566 --> 01:20:06,586
So we have we have a few covariates and want to study how the how the the first

644
01:20:06,856 --> 01:20:12,406
expert treat volume depends on what are associated with this few covariates.

645
01:20:15,736 --> 01:20:22,036
So by fitting the model now for a computer, some descriptive statistic, I believe you guys in your product,

646
01:20:22,996 --> 01:20:27,705
you guys probably have already done this, but maybe not at this exact plot.

647
01:20:27,706 --> 01:20:35,746
But you guys have done some descriptive statistics and that's the very first step you or anyone needs to take

648
01:20:36,766 --> 01:20:42,466
in order to start a product to look at the descriptive statistic to get a rough idea what that looks like.

649
01:20:43,156 --> 01:20:44,686
So get us some some rough idea.

650
01:20:44,956 --> 01:20:55,366
So if we create a such a plot on this part, let's not worry about in the first row, in the first column, we can get a read on this.

651
01:20:55,666 --> 01:20:59,325
This is because this is for I.D. in a in a data.

652
01:20:59,326 --> 01:21:04,616
We do have a column that's called idea. And this is this is really not a variable interest.

653
01:21:04,636 --> 01:21:09,196
It's just identity identification number for different individuals.

654
01:21:09,466 --> 01:21:13,336
So we don't need to worry about the first row in the first column.

655
01:21:14,026 --> 01:21:20,116
But if you look at about what the other rules and columns, so here we have different variables.

656
01:21:20,536 --> 01:21:28,996
We have age, height, male, any get over male, energy level, low curve and also F eve.

657
01:21:30,136 --> 01:21:40,456
And then if you look at this diagonal here, it gives us the his graph or the plot or the distribution of these different variables.

658
01:21:40,816 --> 01:21:46,306
And by looking at this, we get to have a very quick idea what are the DOS fusion look like?

659
01:21:46,396 --> 01:21:52,126
For example, for age, all this seems to be an acquired, well disputed, like a bell shaped,

660
01:21:52,216 --> 01:21:58,336
you know, a normal disputed age distribution and a for height as well.

661
01:21:59,266 --> 01:22:07,126
And again, for male, of course, we have just a two categories. And for a smoker we have two categories and for the response, MPV.

662
01:22:07,516 --> 01:22:12,426
But it seems to have also acquired a quite a well-behaved distribution.

663
01:22:12,436 --> 01:22:19,276
Right. So looking out of the the plot, it does seem to be there does seem to be too much of a problem.

664
01:22:19,966 --> 01:22:24,766
Other than that, I mean, this may or may not be a problem depending on the concrete product.

665
01:22:25,966 --> 01:22:33,886
So here we see that if we're smoking by smoking, I mean there's there's like I guess this is probably smoke smoker.

666
01:22:33,886 --> 01:22:39,226
I mean, the number of smokers is is much, much smaller than the number of those smokers.

667
01:22:39,256 --> 01:22:49,095
So so this may or may not to be an issue in a fit of the model because if one category has only a few subjects, let, let's let me make out an example.

668
01:22:49,096 --> 01:22:54,046
So let's say that the ingredient is Adams, which has a thousand individuals.

669
01:22:54,496 --> 01:22:59,506
There are only five smokers and not all the rest are not smokers.

670
01:22:59,806 --> 01:23:07,996
That, if you include smoke smoking status in the model, probably is probably not going to be a well,

671
01:23:07,996 --> 01:23:15,886
you probably have to include a smoking smoking status in the model because it's not going to tell you anything within the spiral individuals.

672
01:23:17,956 --> 01:23:27,826
But if here I mean, this may not be a huge issue because although the number is not large, but it is not, it's not too small either.

673
01:23:27,826 --> 01:23:39,105
So I guess that's okay. And then we can look at the correlation, these numbers, they give the correlation between any pair of these variables.

674
01:23:39,106 --> 01:23:46,876
So for example, this 4 to 7 dog, this is the correlation between this is the rule of age and a column of height.

675
01:23:47,176 --> 01:23:59,865
So this is not correlation between age and height. In height we see that inclusion is pretty high was born eight and this may raise some alarm when

676
01:23:59,866 --> 01:24:06,106
you try to fit a linear regression model because if you include both age and height in the model,

677
01:24:06,406 --> 01:24:11,656
then you might have a so-called follicle inherited problem, which we will cover in the next model,

678
01:24:11,656 --> 01:24:17,896
in the next model, in more detail, or maybe not after that, but we will talk about medical reality.

679
01:24:18,106 --> 01:24:24,346
So if you have a correlation as high as an eight, you really want to be a little bit careful.

680
01:24:25,126 --> 01:24:27,346
Later, we'll talk about ways of aggressiveness.

681
01:24:29,106 --> 01:24:37,206
And then if you look at the correlation between this age and this age and the response as opposed to seven, six,

682
01:24:37,446 --> 01:24:43,026
what it means age, the association between the response there with age is pretty high, which is very good.

683
01:24:43,596 --> 01:24:49,146
And also the correlation between height and middle response is also pretty high.

684
01:24:50,706 --> 01:24:54,426
Okay. So this is by inspecting the descriptive statistics.

685
01:24:54,426 --> 01:25:03,216
Probably the most alarming thing is the high correlation between age and the response and also between I'm sorry,

686
01:25:03,576 --> 01:25:07,596
between a median age and a height between those two covariates.

687
01:25:08,026 --> 01:25:12,306
And just because I'm this like. Okay,

688
01:25:12,306 --> 01:25:19,476
so this is not the plot and then let's fit a main facts model and then obtain this

689
01:25:19,476 --> 01:25:24,096
different before quantities are similar Y and different versions of the residuals.

690
01:25:24,636 --> 01:25:33,336
And then let's check some assumptions now first about this calculation here.

691
01:25:33,336 --> 01:25:40,056
It shows what a subject is most align with a start with an X space.

692
01:25:41,466 --> 01:25:49,266
And we we check that by looking at the leverage recall that I think I have a very beginning of this lecture we define a.

693
01:26:18,656 --> 01:26:25,256
On November. Define that about that leverage?

694
01:26:25,746 --> 01:26:39,816
Probably not in this module. And. Oh, yeah.

695
01:26:40,146 --> 01:26:50,256
Sorry. Okay. So if you look at if we look at the, the h i this is the so-called leverage ratio, which I, this is called leverage.

696
01:26:50,766 --> 01:27:02,736
And we, when we talk about this, we measure the leverage score to measure the how far away each individual X is from from the center,

697
01:27:02,736 --> 01:27:09,215
from the center that is building. This is easier to see for simple linear regression model because indeed h i similar

698
01:27:09,216 --> 01:27:13,596
linear regression model no matter how far away X is from the from the mean.

699
01:27:14,046 --> 01:27:20,826
So it sort of matters how, whether there are outliers or ally in the x direction.

700
01:27:22,326 --> 01:27:26,586
And now we are calculating this h I the leverage.

701
01:27:28,416 --> 01:27:35,676
So here this is actually the this comment just gives the sum describes the density of the

702
01:27:35,676 --> 01:27:44,686
all of the leverage we see that there are subjects with the with the largest our well,

703
01:27:44,736 --> 01:27:47,976
this is the largest average. So if we look out at this subject,

704
01:27:48,396 --> 01:27:53,465
that's why if you want to you could get to the IP number for this reason and then you can

705
01:27:53,466 --> 01:27:58,206
take a closer look at this object to see if there is anything else for that subject.

706
01:27:58,596 --> 01:28:05,466
So this is sometimes well, sometimes people go into such a detail, some that sometimes people don't like.

707
01:28:06,006 --> 01:28:11,375
This slide just shows you about if you want to take a look at mean in a longer extraction.

708
01:28:11,376 --> 01:28:17,886
If there is an outlier you could look at it but not leverage and then you could identify which

709
01:28:17,886 --> 01:28:24,726
some these have sort of the largest leverage and and then see whether there is anything at all.

710
01:28:24,906 --> 01:28:30,566
And then we can examine the courthouse of the absolute pi.

711
01:28:31,596 --> 01:28:42,336
I see that we have only 10 minutes left, so I don't think we need to spend too much time talking about this one.

712
01:28:42,336 --> 01:28:51,436
So the example of the courthouse slide, this is actually to check whether absolute I follow normal distribution of.

713
01:28:57,596 --> 01:29:02,566
Yeah. Whether whether the absolute I follow follow normal distribution.

714
01:29:02,576 --> 01:29:07,916
So we call that to the Q the box part.

715
01:29:10,766 --> 01:29:15,176
Well, the park's part. This is a plot of the original IRA, and this is a plot of abstract.

716
01:29:15,326 --> 01:29:21,746
Abstract. And for both plot the the first part on the third part, the median.

717
01:29:22,016 --> 01:29:29,486
Well, this should be, you know, the first quarter on the third part, all this would be symmetric about a median and we should be roughly around zero.

718
01:29:29,906 --> 01:29:37,946
And so that we can check these courthouse very quickly through to see whether it follows normal rather than normal.

719
01:29:38,276 --> 01:29:44,186
So, for example, here, if we can make the first part out of the meeting, a third part out of abstract,

720
01:29:44,606 --> 01:29:50,696
we see that the median is fairly close to zero and in the first quarter of the third quartile,

721
01:29:50,696 --> 01:29:55,616
they are roughly equal distance have roughly equal distance from the median.

722
01:29:55,616 --> 01:30:03,376
So. So this is again, just a way of checking the margin assumption, which is not as referring to as just drag the making of his rap.

723
01:30:03,476 --> 01:30:08,186
Right. So this is his wrap of the residual.

724
01:30:08,426 --> 01:30:14,366
We can see that. Well, I mean, in practice, I think if you get under his ground, I mean,

725
01:30:14,606 --> 01:30:21,206
we would probably accelerate a little bit because this is a really good indication that there is no violation of normality.

726
01:30:22,316 --> 01:30:29,216
So which is very, very good because now we don't need to worry much about you make a different transformation if things like that.

727
01:30:30,156 --> 01:30:33,596
Yeah, when you build systems like that, how do you decide how many points to make?

728
01:30:34,646 --> 01:30:37,706
So I mean, how do you decide how to build your histogram?

729
01:30:39,626 --> 01:30:42,656
How do you in your histogram I'll make this easier.

730
01:30:42,656 --> 01:30:46,645
How do you bin it. I don't have a bin besides.

731
01:30:46,646 --> 01:31:02,516
Yeah. Oh okay. So. Well I guess here you could, I think there is really no general rule you could try to find bin size in a C the distribution.

732
01:31:04,316 --> 01:31:06,296
Yeah. I guess that's that's typical what I do.

733
01:31:06,356 --> 01:31:13,015
I try, I would try a few different mid-sized but you definitely won't go on the bins has to be too large or too small.

734
01:31:13,016 --> 01:31:21,266
I mean this is a well because you are plotting absolute hat and everyone has the same unit

735
01:31:21,266 --> 01:31:26,516
as Y so you can change the unit of whatever for the magnitude of absolute will also change.

736
01:31:26,936 --> 01:31:33,206
So there is really no rule of saying like what a bin size would would be appropriate for a particular problem.

737
01:31:33,776 --> 01:31:37,796
So you definitely need to be trial field in size C.

738
01:31:41,916 --> 01:31:45,186
Okay. So that's his and then we have the Kuku block.

739
01:31:45,876 --> 01:31:53,886
And so for Cube we see that although I can are a few points to you know to both ends there seems to be to deviate from

740
01:31:53,886 --> 01:32:03,306
the story line but overall overall it is quite a most of the points all along these lines match this line quite well.

741
01:32:03,306 --> 01:32:04,146
So again,

742
01:32:04,146 --> 01:32:15,696
there's an indication that the normality assumption is not a big issue in this case and that we can calculate in a lot the supervised design as well.

743
01:32:15,816 --> 01:32:18,435
So if you come the center guys,

744
01:32:18,436 --> 01:32:28,866
the in a standardized we should roughly follow a normal decision and then you can you have to check because for center normal diffusion

745
01:32:29,196 --> 01:32:38,016
now we know the probability that a Z like the probability that that the larger than one or a third of the two were larger than 2.5.

746
01:32:38,466 --> 01:32:43,836
So you can then you can calculate about the percentage that a standardized residual larger

747
01:32:43,836 --> 01:32:49,056
than one or into another 2.5 to see whether this roughly followed normal the exclusion.

748
01:32:49,446 --> 01:32:52,506
But again, this is really unnecessary.

749
01:32:52,506 --> 01:32:58,296
I mean, once I think most people would just tack that to the block and his rep.

750
01:32:58,986 --> 01:33:04,146
And based on these two, we see clearly that what they're of seems to be a very good assumption.

751
01:33:04,566 --> 01:33:15,246
So so there is a I don't think it's necessary to subtract a specific percentage of Z larger than 100 to 2.5.

752
01:33:15,546 --> 01:33:20,946
But these slides are just illustration. You could do that if you want to get a better idea.

753
01:33:21,936 --> 01:33:25,596
Now, let's go to the partial plots.

754
01:33:26,226 --> 01:33:32,046
Recall that part of a plot these are used to check linearity assumption.

755
01:33:33,636 --> 01:33:39,366
Now, if we have a here, we have part of plots of the response versus H,

756
01:33:39,936 --> 01:33:54,006
the response versus height and the versus any over mil and versus indicator for smoking and a based on the plot here, as we mentioned.

757
01:33:54,546 --> 01:34:02,016
So the narrative holds that in this plot we should see roughly linear trends.

758
01:34:02,706 --> 01:34:07,116
No other trends in the by looking on this plot.

759
01:34:07,956 --> 01:34:15,006
Well, this is actually typical by looking at this plot a year early, it's really hard to say whether it's exactly legal or not.

760
01:34:15,306 --> 01:34:23,195
But for example, if you look at such a plot and it's really hard to by looking at this graph graph to just say, oh,

761
01:34:23,196 --> 01:34:32,316
linearly linear is the seems to be that the association or I mean, there is some commentary in the middle in the middle points here.

762
01:34:32,646 --> 01:34:42,306
And also another thing that is that makes the graph sometimes scatter plot sometimes harder to read is that there are much less points here.

763
01:34:42,876 --> 01:34:47,736
If you look out of the point, there are a few points, but much less compared to here.

764
01:34:48,486 --> 01:34:54,976
However, now these points, because they are larger and sometimes it's a case,

765
01:34:55,416 --> 01:34:59,916
we could easily make the conclusion that, okay, so there is increasing trend,

766
01:35:00,306 --> 01:35:01,866
but we could be I mean,

767
01:35:02,496 --> 01:35:09,786
there could be a mis misreading of the plot because there must last number of observations then the number of observations here.

768
01:35:10,026 --> 01:35:21,036
So the other word, I guess the point I'm trying to make is that reading so scatter plot urinates is quite subjective.

769
01:35:21,456 --> 01:35:23,946
So the conclusion made could be quite subjective.

770
01:35:25,386 --> 01:35:34,566
But here, I mean at least for for this part of the plot, it seems that there there's a clear linear association increase in crypt.

771
01:35:35,136 --> 01:35:41,046
So this seems to be quite clear and then we could make a residual plot.

772
01:35:41,436 --> 01:35:48,666
So this is the the response to residual versus each response and also versus the feel about it.

773
01:35:49,206 --> 01:35:50,796
Now based on the residual plot,

774
01:35:51,546 --> 01:35:59,436
it assumes that the linearity we need to question a little bit about the narrative assumption because there seems to be some contagion.

775
01:35:59,886 --> 01:36:04,776
If you look at this plot over here and this plot over here, there seems to be some courage.

776
01:36:05,466 --> 01:36:15,606
So this is an indication that we want to take a further look at the assumption and of the the the way we look at any narrative assumption is,

777
01:36:17,076 --> 01:36:24,225
well, we can look at it in two ways. One way is we replace height with a of courthouse.

778
01:36:24,226 --> 01:36:28,056
So in other words, we break height into a few categories.

779
01:36:28,746 --> 01:36:33,816
And so and then we use that including high drag as a continuous variable.

780
01:36:34,176 --> 01:36:39,036
We include these categorical variables created based on height.

781
01:36:40,476 --> 01:36:44,326
And then we can make a plot record. Not here.

782
01:36:44,346 --> 01:36:48,546
I'm not making this explicit, but.

783
01:36:48,836 --> 01:36:54,306
But I just want to point out where. Yeah.

784
01:36:55,176 --> 01:36:58,446
So after breaking heart into this different categories,

785
01:36:58,836 --> 01:37:08,496
after reading about how you could make a plot of different estimated coefficients and see whether the trend is linear or not.

786
01:37:10,206 --> 01:37:12,816
So in our example here.

787
01:37:20,416 --> 01:37:30,376
In our example here, you can plot this value, this value of this value versus the corresponding mean or look at a different category.

788
01:37:31,966 --> 01:37:42,886
So this is category. What the reference category has has been a value equal to zero or or a difference equal to zero.

789
01:37:43,346 --> 01:37:53,656
And then you have this category first or second and third, and then you have one for two and you have for the seven six, and then you have 1.33 nine.

790
01:37:53,656 --> 01:38:00,646
That's probably way above. And this is one for two, this one, two, seven, six.

791
01:38:00,946 --> 01:38:12,836
This is 1.39. And so if you look at the trend, the trend that doesn't seem to be strictly linear because the third category in the last category,

792
01:38:12,836 --> 01:38:17,716
in a third category, the increase is much larger than the increase of the previous category.

793
01:38:17,956 --> 01:38:22,486
So this is actually an indication of the values of linearity.

794
01:38:23,056 --> 01:38:37,216
This is one way of tracking it. And and indeed, if you include this categorical height, if you, again, make the the partial residual plot,

795
01:38:37,606 --> 01:38:42,496
now we can see that the dependance seems to be more clear to be linear.

796
01:38:42,496 --> 01:38:48,466
If you look at a partial position, you can see that there seems to be a clear increasing trend.

797
01:38:50,676 --> 01:38:53,616
And a similarity. We can add a quadratic, a flaw.

798
01:38:54,126 --> 01:39:02,136
So instead of breaking height into different categories, we can add a converting turn of height into a model.

799
01:39:02,856 --> 01:39:08,156
And then we found that, okay, so the p value of converted terms is highly significant.

800
01:39:08,856 --> 01:39:16,056
How significant? This is another indication that the dependance of heavy on height may not be linear.

801
01:39:16,386 --> 01:39:22,746
And the better we will look it in the bottom because the climatic term is highly significant.

802
01:39:23,466 --> 01:39:28,056
So this is just another way of looking at that.

803
01:39:28,086 --> 01:39:37,775
So this while this line assess reconcile differences in M2 and M3 but what I think

804
01:39:37,776 --> 01:39:43,296
I'm two and three as would be great with each other because m2 and to model this is

805
01:39:43,296 --> 01:39:51,095
just to use the the categorical height 2 to 3 including the model and then to then in

806
01:39:51,096 --> 01:39:57,146
a review is there seems to be a curvature reciprocal climatic quadratic dependance.

807
01:39:57,576 --> 01:40:06,336
And model three just added this quadratic return and has a p value which also indicates while there may be a the dependance may be quadratic.

808
01:40:06,336 --> 01:40:11,586
So these two models, the observations based on these two models, they agree with each other.

809
01:40:13,896 --> 01:40:25,595
Okay. So let me let let's very quickly finish up the slides because I mean, me and I, we agree that we should finish this slides in this lecture.

810
01:40:25,596 --> 01:40:29,346
So we will probably take another one minute or two. Okay.

811
01:40:29,346 --> 01:40:41,166
So then if we compare the plots of y have versus y y versus the height of end of them, we can see that.

812
01:40:44,166 --> 01:40:52,746
So the first column here, this is why had versus h y had A versus H in linear in in karate model.

813
01:40:53,226 --> 01:40:56,496
Now, of course, leading our model, we see this linear finding quadratic model.

814
01:40:56,496 --> 01:40:59,946
We see this already trend because that's how we fit in the model result.

815
01:41:00,126 --> 01:41:06,396
We use the fit of the model to predicted Y. Of course, then the producer Y has the corresponding relationship height.

816
01:41:06,606 --> 01:41:15,236
This is quite easy to understand. And then if we make a plot of y have versus y y versus y again.

817
01:41:18,036 --> 01:41:21,216
Well, here I mean, this is the interesting observation.

818
01:41:21,216 --> 01:41:30,036
So when we assume a linear regression model to predict y hat and then we plot Y how to versus the true y,

819
01:41:30,846 --> 01:41:35,316
we see there is sort of a correlation over here. I sort of correct over here.

820
01:41:35,796 --> 01:41:43,626
This means that the predicted y hat and an actual Y, there is some some discrepancy.

821
01:41:44,256 --> 01:41:52,386
So this means that your model year linear model to use linear model pretty y hat may not be the best.

822
01:41:54,186 --> 01:42:04,266
Whereas if you use quadratic model to predict y hat, then then was as y, you'll see that now the trend seems to be quite linear.

823
01:42:05,016 --> 01:42:11,256
There is no curvature anymore. And also we have plotted the residual versus height.

824
01:42:11,316 --> 01:42:20,286
Residual versus height in this model and in karate model, only about a week later, to see that there is a there seems to be a pattern.

825
01:42:20,616 --> 01:42:30,136
There seems to be some curvature over here, the residual versus height or after we are equal.

826
01:42:30,146 --> 01:42:31,116
Look already the term.

827
01:42:31,806 --> 01:42:39,726
Now, there doesn't seem to be curvature anymore, but of course, it it seems that the virus just increases with that with with height.

828
01:42:40,266 --> 01:42:46,686
Well, there does seem to be other pattern. There doesn't seem to be any major function and so on.

829
01:42:49,046 --> 01:42:52,136
Okay. Yeah, this is the the last that's last.

830
01:42:52,286 --> 01:42:57,536
As we mentioned, the outlier slides. These are removed from the updated Oracle slides.

831
01:42:57,806 --> 01:43:07,525
So because we are going to cover those in the next modules. So okay, so sorry for the, you know, for, for getting over time.

832
01:43:07,526 --> 01:43:13,076
So yeah. So here and I hope you guys have a great Thanksgiving.

833
01:43:13,256 --> 01:43:15,646
I'll see you guys, everybody.

