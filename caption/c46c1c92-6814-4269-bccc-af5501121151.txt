1
00:00:01,680 --> 00:00:05,390
Okay. So now now we're recording.

2
00:00:05,430 --> 00:00:12,270
I started this a little bit late for anybody who's watching this recording, but you just missed a couple of a few comments.

3
00:00:14,010 --> 00:00:19,230
So this was the paper from, as I mentioned, passed away about ten years ago.

4
00:00:19,620 --> 00:00:24,360
And they looked at all the biomedical research papers that had been published

5
00:00:25,140 --> 00:00:33,210
from 2012 all the way back or 2011 all the way back to the early seventies.

6
00:00:33,570 --> 00:00:40,950
And what they found was there were 2047 retracted articles and they were surprised to find.

7
00:00:40,980 --> 00:00:46,290
So they broke out, broken out into suspected reasons for the retraction.

8
00:00:46,290 --> 00:00:51,810
So errors, plagiarism, duplicate publications or fraud or suspected fraud.

9
00:00:52,440 --> 00:00:58,380
And they actually expected that it was going to be mostly mistakes that contributed to these retractions.

10
00:00:58,980 --> 00:01:01,070
And they were surprised to find a couple of things.

11
00:01:01,080 --> 00:01:13,440
Number one, that the majority of the retracted papers were actually due to fraud or in later years, plagiarism, duplicate publications.

12
00:01:14,130 --> 00:01:15,600
So that was really interesting.

13
00:01:15,600 --> 00:01:23,819
And also that the number of retractions increased over the years and the ones in particular related to fraud increased as well.

14
00:01:23,820 --> 00:01:26,010
The frequency of fraud increased.

15
00:01:26,640 --> 00:01:37,530
And this this graph down here just shows, you know, for the year publication, the percentage of articles retracted for fraud or suspected fraud.

16
00:01:37,770 --> 00:01:41,490
Again, you can see that this is steadily increasing.

17
00:01:42,540 --> 00:01:47,370
So, again, not an uncommon occurrence.

18
00:01:47,970 --> 00:01:55,470
And so you may ask yourself, what are the reasons for people behaving inappropriately in their research?

19
00:01:55,890 --> 00:02:03,629
And I would say one of the biggest one of the biggest sources of of this one of

20
00:02:03,630 --> 00:02:08,550
the biggest contributors to misconduct is the amount of pressure people are under.

21
00:02:08,880 --> 00:02:16,860
So as a graduate student, you have a graduate, you have your thesis project, you're struggling.

22
00:02:16,860 --> 00:02:21,790
It's not going well. You have this pressure to succeed.

23
00:02:21,810 --> 00:02:28,480
You have this pressure to finish. What am I going to do with my life if I don't finish this dissertation?

24
00:02:28,500 --> 00:02:31,940
I'm going have to drop out of school. You have faculty.

25
00:02:31,940 --> 00:02:36,959
You faculty are under a lot of pressure. So as soon as you start on the tenure track,

26
00:02:36,960 --> 00:02:46,890
there is a clock and you need to be able to achieve external funding and a certain number of publications

27
00:02:46,890 --> 00:02:54,510
every single year in order to meet your merit requirements and also to meet the requirements for tenure.

28
00:02:54,510 --> 00:02:59,130
And if you don't meet those requirements, you don't get funding, you don't get an adequate number of publications.

29
00:02:59,580 --> 00:03:04,750
Then you are out after several years if your tenure clock runs out.

30
00:03:04,770 --> 00:03:08,670
So there's a lot of pressure that people are under.

31
00:03:10,140 --> 00:03:14,190
There's also, you know, some people may be interested in fame.

32
00:03:15,180 --> 00:03:25,890
And you wonder about this, Dr. Potti, as we'll talk about in the in the Duke dissection video that we'll discuss.

33
00:03:26,670 --> 00:03:33,930
So some people may be interested in fame or other ulterior motives, such as conflicts of interest may also play a role.

34
00:03:34,620 --> 00:03:41,550
If you recall from the article, one of the things they point out in kind of the discussion of the sabotage article,

35
00:03:42,540 --> 00:03:50,250
they mentioned that there's other forms of misconduct that are more sort of insidious and kind of fly under the radar.

36
00:03:50,610 --> 00:04:00,569
Things like if you have a competitor and you don't want them to succeed and they ask you for a protocol or something like that,

37
00:04:00,570 --> 00:04:08,070
and you don't share the full details of the protocol with them or if you're reluctant to, to help them.

38
00:04:09,660 --> 00:04:19,020
Or there are other things like giving somebody a scathing review of their their grant or their their grant application or their paper.

39
00:04:19,050 --> 00:04:23,670
So as scientists, we function as peer reviewers.

40
00:04:23,670 --> 00:04:32,819
So I send out my research papers to my fellow scientists and they review those papers and provide comments on the paper,

41
00:04:32,820 --> 00:04:39,540
and hopefully they're constructive and useful. And the vast majority of people do behave in an ethical fashion.

42
00:04:39,540 --> 00:04:44,910
But there are situations where there could be these ulterior motives and conflicts of interest.

43
00:04:45,420 --> 00:04:49,500
And fortunately, if you are submitting a grant or a paper for review,

44
00:04:50,940 --> 00:04:55,950
you do find out pretty quickly who the people are that you maybe shouldn't fully trust.

45
00:04:55,950 --> 00:04:59,580
Or if you're just, you know, you feel a little bit of concern around.

46
00:04:59,870 --> 00:05:03,890
Lots of interest. You can always exclude them from the review process.

47
00:05:04,760 --> 00:05:10,820
You have a you know, when you submit a paper, they'll ask you like, who would you like to review your paper and who do you not want to review it?

48
00:05:11,540 --> 00:05:16,400
And so that's kind of a nice aspect of the peer review process.

49
00:05:17,120 --> 00:05:27,980
But, you know, there are situations where somebody may provide a scathing review of your paper just because they they have some ulterior motive,

50
00:05:29,750 --> 00:05:36,020
things like financial benefits. Again, with the video of the deception at Duke.

51
00:05:36,380 --> 00:05:44,720
They talk about whether Duke benefited financially from this behavior of this scientist.

52
00:05:45,140 --> 00:05:49,670
So we'll touch on that. Many times it's just a lack of training.

53
00:05:49,670 --> 00:05:55,850
So people may not realize that changing number, tweaking a number or something like that,

54
00:05:55,850 --> 00:06:01,010
just to get it always just statistical significance is not okay.

55
00:06:01,580 --> 00:06:10,639
And so the NIH and the NSF have implemented these new mandates to require that everybody across the board,

56
00:06:10,640 --> 00:06:15,950
whether it's US students, I have to as faculty, have training.

57
00:06:16,400 --> 00:06:20,120
I have a a training career development award.

58
00:06:20,120 --> 00:06:26,779
Anybody who gets like a K 99 or a one or two types of career development grants, we have to have several hours of training,

59
00:06:26,780 --> 00:06:34,670
and that's another situation that can contribute to misconduct as poorly managed research groups.

60
00:06:34,820 --> 00:06:39,860
So you can imagine that you are one of those people and you feel like you're floundering in your research.

61
00:06:40,370 --> 00:06:49,219
And maybe you have an environment where there's a lot of pressure from your PI to to generate results,

62
00:06:49,220 --> 00:06:54,890
and everybody around you seems to be generating great data and not having any problems and you feel like

63
00:06:54,890 --> 00:07:00,860
you're the only person struggling and you can get that face time with your PI or the lab manager or whoever.

64
00:07:01,280 --> 00:07:09,080
You know, these are the types of situations where people fall through the cracks and are under extreme amounts of pressure and feel compelled to,

65
00:07:09,500 --> 00:07:14,210
you know, fabricate data or other types of things.

66
00:07:15,110 --> 00:07:16,700
Or maybe they're just a bad seed.

67
00:07:16,910 --> 00:07:28,310
You know, I suppose that there are people out there who fit that category, but in general, I think that people have good intentions.

68
00:07:28,820 --> 00:07:36,280
Um, I think that more often than not, their people are not evil, but they have some other motivation,

69
00:07:36,560 --> 00:07:41,450
these other external pressures and factors that contribute to their behavior.

70
00:07:42,680 --> 00:07:48,680
And I'll step back for just a moment and ask you all, have any of you first of all,

71
00:07:48,680 --> 00:07:53,330
what do you think of when you think of like what comes to mind initially when you think of research misconduct?

72
00:07:59,650 --> 00:08:03,370
First thing that comes to my mind was falsification of results or something.

73
00:08:03,550 --> 00:08:11,890
Yeah. Yeah. That's kind of that's like the first thing I think of too was, yeah, somebody changing,

74
00:08:12,040 --> 00:08:17,110
taking a Western blot and like getting rid of some bands that they don't really want people to see or.

75
00:08:17,980 --> 00:08:22,760
Yeah. Changing, throwing in some data that kind of like help make their, that ah.

76
00:08:22,780 --> 00:08:26,380
Make their hypothesis seem to be correct.

77
00:08:28,540 --> 00:08:35,350
How many of you have. Have any of you have any, had any experience or know somebody who has had experience with research misconduct?

78
00:08:36,280 --> 00:08:41,350
Has that ever happened in a research lab you've been in somebody you know, never experienced this.

79
00:08:45,730 --> 00:08:49,100
That's a good thing there, I would say.

80
00:08:51,260 --> 00:08:58,940
I have not personally been involved, like I've never personally witnessed research misconduct.

81
00:08:59,630 --> 00:09:05,070
And in my situation, like people in my lab that I, you know,

82
00:09:05,070 --> 00:09:13,280
in my graduate lab and in my postdoc lab and in the current lab that I'm a part of, people have behaved ethically, which is excellent.

83
00:09:13,280 --> 00:09:27,319
But I know I have colleagues who have it had been the recipient of where somebody has plagiarized their work to just a ridiculous extent,

84
00:09:27,320 --> 00:09:30,680
to the point where you're like, how does this person still even have a job?

85
00:09:31,010 --> 00:09:41,540
That how do they get away with this? Because in the case that I'm thinking about, this individual still is still working as a professor.

86
00:09:41,540 --> 00:09:55,339
Like he plagiarized a bunch of work from another professor and was found out and actually evaluated by for research misconduct.

87
00:09:55,340 --> 00:09:58,459
But he was still working as a professor.

88
00:09:58,460 --> 00:10:01,760
So in this email, when we watch this video,

89
00:10:01,760 --> 00:10:12,200
you'll see this physician who falsified these data that led to these potentially injurious clinical trials is still performing.

90
00:10:12,470 --> 00:10:16,280
He's still working as a physician. So it does kind of blow your mind.

91
00:10:16,880 --> 00:10:21,920
But the professor that plagiarized.

92
00:10:22,760 --> 00:10:27,280
Plagiarized work. Yeah. Yeah. His tenure as I like tenure.

93
00:10:29,050 --> 00:10:32,050
Yeah, well, I don't think he was tenured at the time.

94
00:10:32,050 --> 00:10:35,260
Now that's a thing which is really, really interesting.

95
00:10:38,080 --> 00:10:42,170
You can find a on. Yeah.

96
00:10:42,790 --> 00:10:47,850
Yeah. Even has like two. Profile.

97
00:10:48,360 --> 00:10:53,030
Yeah. Yeah. That's the thing that really just you do.

98
00:10:53,450 --> 00:10:59,600
There are a few situations where the behavior is so bad and so bold that you just

99
00:10:59,990 --> 00:11:06,319
how do these people think that they're going to get away with what they do? And and sometimes they do get away with it.

100
00:11:06,320 --> 00:11:12,830
And I just I don't know how so so yeah, this does these things do happen, unfortunately.

101
00:11:14,170 --> 00:11:14,590
Um,

102
00:11:16,430 --> 00:11:27,530
so here's a quick little case scenario where you're a student research assistant and you've noticed that graphs on to senior colleagues publications.

103
00:11:27,950 --> 00:11:33,290
So two different publications. The same graph shows up in two different publications.

104
00:11:34,920 --> 00:11:38,070
And the senior colleague is a member of the same research team, Iran.

105
00:11:40,020 --> 00:11:44,100
So after checking carefully and reviewing the data, you realize something's wrong.

106
00:11:44,990 --> 00:11:48,000
You made a mistake or you falsified information. So.

107
00:11:49,300 --> 00:11:55,640
Consider this for a few seconds and. What are your thoughts on how how would you handle the situation?

108
00:12:16,410 --> 00:12:20,230
There's really not. There's lots of potential things you could do here.

109
00:12:20,900 --> 00:12:28,950
It's not a right or wrong answer here. So for me, the.

110
00:12:35,170 --> 00:12:38,760
IAC IAC summary.

111
00:12:40,310 --> 00:12:45,260
Yeah. Yeah.

112
00:12:46,160 --> 00:12:49,950
Yeah. Yeah. Yeah.

113
00:12:49,960 --> 00:12:53,530
So. Couple of things that are key here.

114
00:12:53,650 --> 00:12:56,920
One of the the words here that's key is senior.

115
00:12:57,130 --> 00:13:03,130
So this person is a senior colleague, which you imagine that there could potentially be a power dynamic there.

116
00:13:04,450 --> 00:13:07,320
But, you know, maybe you have an excellent relationship with this person,

117
00:13:07,330 --> 00:13:11,540
in which case you would feel comfortable saying, you know, hey, what's going on here?

118
00:13:11,560 --> 00:13:15,219
Like, there's this. I think you made a mistake.

119
00:13:15,220 --> 00:13:19,810
You know, you probably frame it as you wouldn't want to assume that somebody genuinely made a mistake.

120
00:13:21,250 --> 00:13:29,530
And then based on how they behave from there, then maybe you could proceed to talking to your department or another research administrator,

121
00:13:29,830 --> 00:13:33,550
or maybe you are just really uncomfortable with the situation in general.

122
00:13:33,790 --> 00:13:41,800
And so maybe you talk to a research administrator just in private and get their advice about how to handle the situation.

123
00:13:42,640 --> 00:13:52,390
And that's, I think, a really great way to to handle it. You may may be able to encourage a fellow graduate student to report the problem,

124
00:13:53,680 --> 00:14:00,520
but what I would probably do is ask a fellow graduate student whether they what they think,

125
00:14:00,910 --> 00:14:04,600
like show them the two papers and hey, what's your what are your thoughts on this?

126
00:14:04,600 --> 00:14:13,209
And if they agree, like, maybe they will be willing to go to the research administrator as well or,

127
00:14:13,210 --> 00:14:18,250
you know, kind of help you kind of be a sanity check in the whole process.

128
00:14:18,850 --> 00:14:24,190
But, you know, you don't really want to go and ignore that because I think you do you do have an

129
00:14:24,190 --> 00:14:28,450
obligation to report these things or at least figure out if there's a mistake.

130
00:14:29,720 --> 00:14:33,890
I'm wondering if there's also a. The editor.

131
00:14:36,250 --> 00:14:42,430
Yeah. I think depending on how you define yeah.

132
00:14:42,430 --> 00:14:48,480
I think internally, whether it's a mistake, you probably want to handle it internally to figure out what's going on,

133
00:14:48,730 --> 00:14:55,090
whether it was a mistake, whether it was deliberate and you know, so there would be some dealings there.

134
00:14:55,090 --> 00:15:02,920
But then, yeah, you would definitely need to talk to the editor of the journals and one of those papers is probably going to get retracted,

135
00:15:03,310 --> 00:15:07,630
maybe both of them, depending on if they're if the grant belonged in either of the papers.

136
00:15:09,300 --> 00:15:17,190
Um. So. We we kind of laid out like, what are the ethical issues here?

137
00:15:17,190 --> 00:15:22,080
But who is at risk and what are the risks involved?

138
00:15:28,750 --> 00:15:35,140
You think about? People's careers or relationships and.

139
00:15:41,150 --> 00:15:46,610
So obviously the senior colleague, because he'd be at risk in his career, would be at risk.

140
00:15:46,610 --> 00:15:55,340
But wouldn't the research assistant, their career future, kind of be at risk, too, if they knew about it and didn't say anything?

141
00:15:55,980 --> 00:16:06,260
Yeah, yeah, yeah, yeah. It could be, you know, if you see a problem and you don't speak up and it's found out, you know,

142
00:16:06,260 --> 00:16:11,180
it's discovered later that you knew about it and you just let it go or you were even complicit.

143
00:16:11,480 --> 00:16:16,900
Like that's that's a problem. Speaker The reverse would be true too.

144
00:16:16,900 --> 00:16:20,740
Like if you do try to bring it to the senior person and somebody who maybe hasn't.

145
00:16:21,370 --> 00:16:29,100
Your career. And then they're really sad about it. And then they try to sabotage your own efforts or, you know, that support you.

146
00:16:29,110 --> 00:16:32,700
I think that that. Yeah. Or risk on whether.

147
00:16:33,030 --> 00:16:39,440
Your relationship is absolutely in academia is a small it's a small community.

148
00:16:39,450 --> 00:16:43,700
You know, you think about this, it's a big world and there's lots of scientists and whatever out there.

149
00:16:43,710 --> 00:16:49,110
But within your individual research community, it's a pretty small group of people.

150
00:16:49,320 --> 00:16:58,220
And like environmental, health, science, epigenetics, people that I'm that I work with, they all kind of know each other's.

151
00:16:58,380 --> 00:17:01,020
At least we know each other's names and we're familiar with each other.

152
00:17:01,410 --> 00:17:09,420
And when you go to apply for tenure, when you go to again, when you're applying for grants,

153
00:17:09,420 --> 00:17:16,020
these people are reading your grant applications and whatnot. When you apply for tenure, you need people who can write letters of recommendation.

154
00:17:16,950 --> 00:17:25,350
And that's at the faculty level. When you're a graduate student or you know, you're wanting to make some career transition forward,

155
00:17:25,830 --> 00:17:28,830
you will need letters of recommendation, you'll need references.

156
00:17:28,830 --> 00:17:34,830
And these people can be when you're a graduate student and you're looking at your research advisor or whatever,

157
00:17:35,040 --> 00:17:38,070
these people are like in a position of pretty significant power.

158
00:17:38,490 --> 00:17:48,330
And so, yeah, I think it's understandable that there would be some hesitation to, to rock the boat in a,

159
00:17:48,330 --> 00:17:53,970
in a research environment if you feel like it's going to there's going to be a backlash against you.

160
00:17:54,360 --> 00:18:02,610
And that's why a lot of cases, you know, you probably just want to go talk to a research administrator or talk to your department chair,

161
00:18:02,700 --> 00:18:09,140
somebody you're comfortable with, with discussing things with, with and and get their feedback so that.

162
00:18:10,470 --> 00:18:13,740
Can be protected from any sort of backlash. Fortunately,

163
00:18:13,740 --> 00:18:21,479
there are laws now and policies in place to protect both the whistleblower and

164
00:18:21,480 --> 00:18:27,660
also people who are who are the subject of an investigation of misconduct.

165
00:18:27,930 --> 00:18:33,240
Because sometimes, as I'll I have an example, sometimes somebody might be wrongly accused of misconduct.

166
00:18:33,690 --> 00:18:41,400
And so you don't want to destroy somebody's career in the process of investigating that misconduct.

167
00:18:43,980 --> 00:18:54,510
So just a couple of examples to provide some historical context for why we need these policies in place.

168
00:18:55,710 --> 00:19:06,990
The first one was from 1974 and this was William Summerlin used a black felt pen to darken patches of fur on a white mouse.

169
00:19:08,370 --> 00:19:09,750
Basically what he was doing,

170
00:19:09,990 --> 00:19:17,490
he was trying to provide proof that he could transplant skin grafts from to unrelated individuals without an immune response,

171
00:19:17,610 --> 00:19:18,810
without an immune rejection.

172
00:19:19,050 --> 00:19:27,720
So, of course, you probably know that if you do any sort of organ transplant or tissue transplant between two people who aren't related,

173
00:19:28,050 --> 00:19:36,030
the person receiving that transplant, the body's going to recognize that as a foreign foreign tissue and its meaning system is going to attack it.

174
00:19:36,040 --> 00:19:41,460
So you have to give people immune suppressing agents to prevent transplant rejection.

175
00:19:41,820 --> 00:19:50,630
So this guy claimed that he had figured out how to do these transplants without any sort of immune response.

176
00:19:50,640 --> 00:19:53,910
And of course, nobody was able to repeat the guy's work.

177
00:19:54,300 --> 00:20:02,310
And eventually they figured out that he was just taking a marker and, you know, darkening patches of fur and claiming that that was a transplant.

178
00:20:02,320 --> 00:20:06,420
And then when they went in with some rubbing alcohol, they got rid of the transplants.

179
00:20:06,420 --> 00:20:18,659
No problem transplants. So, yeah, these are just really astonishingly bold examples of blatant misconduct.

180
00:20:18,660 --> 00:20:22,160
And I don't know how these people didn't realize that they were going to be caught eventually.

181
00:20:23,610 --> 00:20:33,509
1981, a physician was caught for fabricating data and a study in canine cardiology, and he was at Harvard.

182
00:20:33,510 --> 00:20:43,020
And both the school and the federal investigators found him guilty of scientific misconduct and cut him off from federal funds for ten years.

183
00:20:43,380 --> 00:20:46,860
But he publicly apologized for fabricating data.

184
00:20:46,860 --> 00:20:53,040
Eventually, more than 100 have published research papers over the course of 14 years.

185
00:20:53,490 --> 00:20:57,510
So in a situation like that, like who's the who are the victims?

186
00:20:57,720 --> 00:21:04,170
But do you think loses in this situation because of because of this behavior?

187
00:21:08,960 --> 00:21:12,940
I mean, research off the paper should be traced to.

188
00:21:14,510 --> 00:21:18,470
Yes. Yes. And this. Yeah.

189
00:21:18,510 --> 00:21:23,110
But what I really. Anybody who gets. Yeah.

190
00:21:23,710 --> 00:21:30,820
Yeah. And this is. This is. What we'll see in this this 60 Minutes video.

191
00:21:32,020 --> 00:21:33,100
This is what happened at Duke.

192
00:21:34,150 --> 00:21:45,100
But even the even the inability to reproduce scientific like I don't know how many times you you'll see a paper come out and you're like,

193
00:21:45,100 --> 00:21:48,610
Oh, this is cool. They found this and let's build on it.

194
00:21:48,970 --> 00:21:53,260
Let's see if we can repeat it and apply it to what we've been doing research wise.

195
00:21:53,560 --> 00:22:01,930
And, you know, let's build on this idea and and then the researchers go to repeat their experiments and they can't repeat them.

196
00:22:02,590 --> 00:22:18,150
And this is a huge problem in biomedical research that the ability to reproduce a lot of research findings, it's it's really it's really low feat.

197
00:22:19,150 --> 00:22:23,500
It's very difficult in a lot of situations to to reproduce research.

198
00:22:25,300 --> 00:22:28,629
And that's not necessarily in the vast majority of cases.

199
00:22:28,630 --> 00:22:32,620
That's not because of any sort of misconduct. It's just because of a lot of technical issues.

200
00:22:32,800 --> 00:22:38,680
But then you add on somebody who is just behaving, you know, unethically on top of that.

201
00:22:38,980 --> 00:22:46,200
And it's just like I think about that as a researcher and I you just get so I mean, I'd be so frustrated.

202
00:22:46,210 --> 00:22:53,590
Like, what if I had spent thousands of dollars, tens of thousands of dollars in my lab trying to repeat?

203
00:22:53,980 --> 00:22:59,469
I probably wouldn't spend that much. I'd probably give up at some point, but move on to something else.

204
00:22:59,470 --> 00:23:05,410
But you imagine, like a lot of collective research labs are spending that much money trying to repeat the experiments and whatnot.

205
00:23:05,410 --> 00:23:10,900
So it's a big waste of money. And what about the reputation of the scientific community?

206
00:23:12,160 --> 00:23:21,160
You know, you think about like even amidst the COVID pandemic, we've seen the types of mistrust of science,

207
00:23:21,400 --> 00:23:27,070
of the scientific community among people and situations like this.

208
00:23:27,070 --> 00:23:36,670
I feel like I really don't help in that regard, you know, really harmful thinking about hydroxychloroquine and how it created shortages.

209
00:23:38,050 --> 00:23:42,580
It's actually like demonstration. Yeah.

210
00:23:43,020 --> 00:23:49,320
And. Well, not just like that.

211
00:23:49,500 --> 00:23:52,780
Yeah. Yeah. There wasn't really very much evidence.

212
00:23:52,780 --> 00:23:58,690
I don't think it. Right.

213
00:24:00,360 --> 00:24:12,419
Yeah. Yeah. So I think, you know, speaking in a way that is exaggerating your results or in any way being untruthful,

214
00:24:12,420 --> 00:24:16,830
bending the truth, you know, bending the truth is not it's not true.

215
00:24:17,700 --> 00:24:25,020
If you have to bend the truth and you have a problem. And yeah, so there's a lot of downstream repercussions of these things.

216
00:24:25,920 --> 00:24:34,110
Um, the last example here was, um, Margot O'Toole, who was a post-doc, and Teresa Imani.

217
00:24:34,110 --> 00:24:44,370
She carries a lab at M.I.T. and he accused this professor of falsifying data in a cell paper that she had coauthored with the Nobel laureate.

218
00:24:44,970 --> 00:24:48,810
And there there's a whole congressional panel investigation.

219
00:24:49,320 --> 00:24:51,960
She was banned from federal research for ten years,

220
00:24:52,260 --> 00:25:02,399
but eventually the Office of Research Integrity found that she had not she had not been she had not been involved in any misconduct,

221
00:25:02,400 --> 00:25:07,740
that she was that her her behavior was ethical and everything was was fine.

222
00:25:07,980 --> 00:25:17,969
But and she was cleared of all charges. But you can imagine over this course of this investigation, everything, it was all like made public.

223
00:25:17,970 --> 00:25:19,770
And so she was dragged through the mud.

224
00:25:21,060 --> 00:25:30,860
And sort of in response to this, the regulations concerning the management of these misconduct allegations were announced by federal agencies, um,

225
00:25:31,350 --> 00:25:34,950
which includes protecting, protecting the provision of these provisions,

226
00:25:34,950 --> 00:25:39,540
protecting people who report misconduct and those who are also accused of misconduct.

227
00:25:40,680 --> 00:25:47,430
Anybody think of a good example in recent a very, very recent news story of misconduct and.

228
00:25:51,090 --> 00:26:01,990
Retaliation against whistleblowers and. So if any of you follow that Theranos story.

229
00:26:02,770 --> 00:26:09,909
Yeah. So Elizabeth Holmes just got 11 years in prison for I don't know how much of the time that that's her sentence.

230
00:26:09,910 --> 00:26:12,819
I don't know how long. Shaw, actually. I'm sure they'll appeal it,

231
00:26:12,820 --> 00:26:25,330
but she got a pretty serious sentence for the fraud that she that she perpetuated and was involved in as as a part of Theranos.

232
00:26:25,330 --> 00:26:27,600
And if you've.

233
00:26:27,820 --> 00:26:38,049
There's a there's an excellent book that I think the title has Blood in the name, but there's an excellent book on the whole Theranos thing.

234
00:26:38,050 --> 00:26:46,870
And they talk about how there were whistleblowers who were trying to point out that this technology that she had developed was crap.

235
00:26:46,870 --> 00:26:56,130
And it wasn't, you know, it wasn't performing the way they were claiming it was performing and there was retaliation against these people.

236
00:26:56,140 --> 00:27:00,910
And so it wasn't just like it wasn't just like they made mistakes.

237
00:27:00,910 --> 00:27:09,340
And, I mean, they really went after the people who tried to they tried to speak up about the fraud that was going on.

238
00:27:09,340 --> 00:27:18,819
So. There's also a great podcast called The Dropout, who many of you heard this was like a great binge.

239
00:27:18,820 --> 00:27:23,740
Listen, if you're interested in the whole Theranos thing, it's really fascinating.

240
00:27:25,870 --> 00:27:27,430
Cool. So we will stop.

241
00:27:28,450 --> 00:27:37,179
I'm going to stop my talking for you and we'll watch this 60 Minutes video and then we'll with the last bit of time we have in class,

242
00:27:37,180 --> 00:27:45,730
we'll talk about this. Five years ago, Duke University announced it had found the holy grail of cancer research.

243
00:27:46,150 --> 00:27:50,770
They discovered how to match the patient's tumor to the best chemotherapy drug.

244
00:27:51,310 --> 00:27:54,820
It was a breakthrough because every person's DNA is unique.

245
00:27:55,120 --> 00:28:01,510
So every tumor is different. A drug that kills a tumor in one person, for example, might not work in another.

246
00:28:02,230 --> 00:28:05,590
The research was published in the most prestigious medical journals,

247
00:28:05,680 --> 00:28:12,040
and more than 100 desperately ill people invested their last hopes in Duke's innovation.

248
00:28:12,820 --> 00:28:18,880
In 2010, we learned that the new method was a failure, but one isn't widely known until tonight.

249
00:28:19,720 --> 00:28:28,630
The discovery wasn't just a failure. It may end up being one of the biggest medical research frauds ever wanted to see in dying patients.

250
00:28:28,870 --> 00:28:38,170
The best medical journals and a great university has made a commitment to fight this war against cancer at a much higher level.

251
00:28:38,920 --> 00:28:46,810
Dr. Aneel Cody, featured in this commercial for Duke University, had made a discovery that promised to change the face of medicine.

252
00:28:47,110 --> 00:28:53,650
Genomics will revolutionize cancer therapy. It actually identifies a fingerprint that's unique to the individual patient.

253
00:28:54,130 --> 00:29:00,220
This is sort of like the holy grail of cancer. Dr. Robert Keyworth is Duke's vice chancellor of clinical research.

254
00:29:00,760 --> 00:29:05,140
What's the idea here that this would change the way we thought about treating cancer?

255
00:29:05,590 --> 00:29:09,010
I've never seen such excitement at an institution, and it's understandable.

256
00:29:09,760 --> 00:29:15,550
It wasn't just do. It was excited. 112 patients signed up for the revolutionary therapy.

257
00:29:16,180 --> 00:29:24,730
Hope was fading for Juliet Jacobs when she learned about she had stage four lung cancer and this would be her last chance.

258
00:29:25,120 --> 00:29:33,190
She was my best friend. But that's kind of a cliche. She's she's somebody who after 49 and a half years, I was still madly in love with her.

259
00:29:33,970 --> 00:29:37,360
She and her husband, Walter, were looking into experimental treatments.

260
00:29:37,690 --> 00:29:41,590
They had to choose carefully because there was only time for one.

261
00:29:42,160 --> 00:29:47,950
When you met Dr. Bowden, what did you think? We thought he was going to give us a chance.

262
00:29:47,950 --> 00:29:51,850
He was. It was very encouraging for a patient with no time.

263
00:29:52,210 --> 00:29:55,960
Dr. Pony's research promise. Right. Drug right now.

264
00:29:56,620 --> 00:30:01,990
Fair to say, quote, was a rising star at Duke probably was one of our most important rising stars.

265
00:30:02,770 --> 00:30:07,660
A lot of people were pleased that it was Dr. Pony who made the discovery of a lifetime.

266
00:30:08,170 --> 00:30:11,860
Born in India, he was known as an earnest, modest,

267
00:30:11,980 --> 00:30:19,360
hardworking Rhodes scholar who did research at the University of North Dakota before reaching Duke in 2003.

268
00:30:19,810 --> 00:30:24,970
He was a young man with a big idea, which he explained in an interview for Duke.

269
00:30:25,330 --> 00:30:33,040
And that's the goal was to was to be able to tell a patient with cancer that I'm not just a cancer doctor and a to your particular cancer.

270
00:30:33,460 --> 00:30:38,170
Dr. Pony made the breakthrough in the renowned lab of Dr. Joseph Nevins.

271
00:30:38,590 --> 00:30:42,010
The Nevins lab had built a reputation for important work.

272
00:30:42,490 --> 00:30:49,060
Dr. Nevins saw something in Dr. Brody, and he chose the young researcher to mentor and support.

273
00:30:49,960 --> 00:30:53,110
Very bright, very smart individual.

274
00:30:53,110 --> 00:30:57,260
Very capable. He was a very. Scotland to many, many people.

275
00:30:57,680 --> 00:31:07,790
And to you and to me. When Dr. Poling decoded the genetic makeup of hundreds of tumors, the research created huge computer files of data.

276
00:31:08,300 --> 00:31:18,140
That data was the underlying proof and research papers under the names of Poti and Evans that were a sensation in the top medical journals.

277
00:31:18,500 --> 00:31:22,400
It was going to change medicine. It was going to change how we treat patients.

278
00:31:23,030 --> 00:31:27,350
Doctors everywhere were eager to save lives with the new discovery.

279
00:31:27,740 --> 00:31:37,400
And in the Anderson Cancer Center in Houston, Kevin Coombs and his family began analyzing Dr. Pony's data to verify his results.

280
00:31:37,910 --> 00:31:46,240
And as you dug into the data, what did you find? We started some basic processing and we noticed some things that were really odd that we recruited.

281
00:31:46,340 --> 00:31:52,970
Explain comes in back early are experts in the kind of data created in Dr. Brody's research.

282
00:31:53,510 --> 00:32:01,880
They e-mail their questions to Duke, and Dr. Poti admitted a few clerical errors, but he said that new work can find his results.

283
00:32:02,570 --> 00:32:09,920
Duke proved it had doctors in Evans and Poti applied for patents and started a company to market the process.

284
00:32:10,370 --> 00:32:12,380
They and Duke stood to make a fortune.

285
00:32:13,010 --> 00:32:22,400
Patients enrolled in the clinical trial so that their tumors could be surgically biopsy to be matched with the best drug on it in the Indiana's.

286
00:32:22,760 --> 00:32:29,180
During months of analysis accurately and Combs kept finding errors that they thought were alarming.

287
00:32:29,840 --> 00:32:38,749
One of the things that was especially disturbing was that these types of errors again and again and again, that was far beyond anything.

288
00:32:38,750 --> 00:32:48,290
17. They suspected Dr. Poti had somehow ruled out some of the data and that some of the patients could be getting not the best drug for their tumor,

289
00:32:48,680 --> 00:32:54,920
but the worst that you would be giving patients drugs that would definitely not benefit them.

290
00:32:55,220 --> 00:33:01,000
So there's clear potential for harm. Exactly the opposite of what this was supposed to be.

291
00:33:01,110 --> 00:33:04,790
Yes. So we wrote them and we said this, this this is a big problem.

292
00:33:05,390 --> 00:33:10,220
Baggoley and Coombs eventually concluded that Duke's holy grail was worthless.

293
00:33:10,710 --> 00:33:17,300
The doctors, Evans and Moody, disagreed. I wonder why at that point you didn't say, is the director of the lab?

294
00:33:17,390 --> 00:33:24,530
One stop, too many questions. We have to get to the bottom of this and put a team together to figure that out.

295
00:33:25,460 --> 00:33:31,580
I didn't feel it ever got to that point. I felt that we had addressed the issues that had been raised.

296
00:33:32,600 --> 00:33:39,350
That changed when researchers here at the National Cancer Institute said they, too, were having trouble with the data.

297
00:33:40,100 --> 00:33:47,120
Duke suspended the enrollment of patients and ask an outside review committee to analyze Dr. Brody's discovery.

298
00:33:47,600 --> 00:33:53,240
After three months, the review committee concluded that Dr. Poti was right.

299
00:33:53,960 --> 00:33:57,920
Juanita reaction was an expletive, which I will not repeat here on.

300
00:33:57,920 --> 00:34:01,340
We've gone through the usual channels. We've written letters to the journals.

301
00:34:01,340 --> 00:34:08,840
We've written the article. We succeeded in getting the trial suspended, and somebody investigated and we tried everything we could.

302
00:34:09,710 --> 00:34:17,870
Duke restarted the clinical trials, and that's when Juliet and Walter Jacobs sat down for their first meeting with Dr. Brody.

303
00:34:18,590 --> 00:34:23,360
I'm recording this information. That's a good thing because you've been misled.

304
00:34:24,020 --> 00:34:31,280
The Jacobs were told, based on the research, that the chances of finding the right drug were approximately 80%.

305
00:34:31,970 --> 00:34:38,390
Walter Jacobs says no one mentioned that the clinical trial had been suspended because of so many questions.

306
00:34:38,800 --> 00:34:45,740
And of how many? Trusted because Dr. Pony's work had been vindicated.

307
00:34:46,370 --> 00:34:57,710
But there was just one more thing discovered not by a scientist, but by the editor of a small, independent newsletter called The Cancer Letter.

308
00:34:58,520 --> 00:35:04,250
Goldberg got a tip from a confidential source. Check Dr. Rhodes Scholarship.

309
00:35:04,760 --> 00:35:08,450
It was right there on his applications for federal grants.

310
00:35:08,990 --> 00:35:12,650
Trouble was, it wasn't true. You asked him about it?

311
00:35:13,030 --> 00:35:20,330
Certainly. Ask him about it. What did he say? He said that while it wasn't a Rhodes scholar, as we know the Rhodes Scholar,

312
00:35:21,470 --> 00:35:26,100
it was a fellowship from Australia, from a group of scholars in Australia.

313
00:35:26,130 --> 00:35:40,840
So. Stretch of the truth was at the moment when you realized amazingly, I was still hanging on to the notion of there must be a good explanation here.

314
00:35:41,110 --> 00:35:46,510
Why are you deluding yourself? At that point in time, what is it that you want to believe?

315
00:35:47,350 --> 00:35:55,190
I want to believe that somebody that I have trusted, that was a colleague for the last four or five years.

316
00:35:55,750 --> 00:35:59,050
Someone that I viewed as a friend.

317
00:36:00,390 --> 00:36:10,469
Was who I thought they were. And then you're faced with the reality of them deceived, fearing that reality.

318
00:36:10,470 --> 00:36:19,710
Joseph Nevins, whose own reputation was at stake, reviewed the original data, which had justified the clinical trials for 112 patients.

319
00:36:20,370 --> 00:36:25,770
Dr. Nevins discovered that when the underlying data disproved Dr. Brody's theory,

320
00:36:26,340 --> 00:36:34,559
the data were changed and it became clear that there was no explanation other than rumors was a manipulation and

321
00:36:34,560 --> 00:36:44,040
infiltration of the data manipulation of somebody's credentials and manipulation of a lot of people's trust.

322
00:36:44,610 --> 00:36:48,240
Manipulating data. These were not errors.

323
00:36:48,990 --> 00:36:53,640
That's correct. It simply couldn't be random. It simply couldn't be inadvertent.

324
00:36:54,840 --> 00:36:58,260
It had to be. Based on.

325
00:36:59,150 --> 00:37:08,180
A desire to make something more clear. Is it a close call or is it abundantly clear that the data were fabricated?

326
00:37:09,590 --> 00:37:14,570
Abundantly clear when you switch the data, the theory is proof.

327
00:37:15,260 --> 00:37:18,580
And when you put the data back the way it's supposed to be, the theory.

328
00:37:18,590 --> 00:37:23,720
Second, so theories are done. If you put the data back to work the way it was supposed to be, how could that switch happen?

329
00:37:24,350 --> 00:37:29,400
If it happened by chance, it would be roughly equivalent to an asteroid hitting Earth.

330
00:37:30,560 --> 00:37:36,380
Duke University agreed to tell us this story as a cautionary tale for other institutions.

331
00:37:36,920 --> 00:37:46,430
Vice Chancellor Rob Catlin is implementing new procedures for Duke and also overseeing the retraction of Dr. Brody's papers from the medical journals.

332
00:37:46,760 --> 00:37:51,510
One of the most significant retractions in medical history is examining how both

333
00:37:51,510 --> 00:37:57,350
the prestigious university and outside investigators missed all the warning signs.

334
00:37:58,010 --> 00:38:05,360
How could they have found nothing wrong, nothing suspicious about the work at that point?

335
00:38:06,140 --> 00:38:09,170
They were analyzing events that had been prepared by Dr. Purdy.

336
00:38:09,220 --> 00:38:15,140
So the data showed that God was wanting to produce or same results and then saying our own analysis.

337
00:38:15,530 --> 00:38:21,620
You know, there were people watching this interview who were thinking to themselves like they stood to be wealthy.

338
00:38:22,100 --> 00:38:25,430
The university stood to make a lot of money.

339
00:38:26,030 --> 00:38:30,650
No one wanted to believe that this research was corrupt.

340
00:38:31,340 --> 00:38:36,380
To what extent was that the reason that the warning signs were overlooked?

341
00:38:37,280 --> 00:38:40,249
In my view, it was not the money that was the primary driver.

342
00:38:40,250 --> 00:38:46,940
It was this great opportunity to help people in striving people to say, you know, we've got to make this work because it looks so good.

343
00:38:47,670 --> 00:38:56,090
The patients were told that there was an 80% chance that precisely the right drug for their tumor would be found.

344
00:38:57,380 --> 00:39:01,090
That wasn't true. Do you bear any responsibility for that?

345
00:39:02,020 --> 00:39:07,450
I regret. Some of the issues that were raised along the way.

346
00:39:08,110 --> 00:39:12,490
I didn't recognize earlier that this could have been brought to a halt at an earlier time.

347
00:39:13,810 --> 00:39:18,520
Julian Jacobs died three months after she entered the clinical trial.

348
00:39:18,880 --> 00:39:24,400
Walter Jacobs and eight others have filed suit. In his answer to the Jacobs lawsuit.

349
00:39:24,730 --> 00:39:32,230
Dr. Poti says he was not aware that false or improper information had been included in the research.

350
00:39:32,770 --> 00:39:40,570
Duke has apologized for the trials, and even though the patients hope that they were getting an innovation that could save their lives,

351
00:39:41,050 --> 00:39:47,770
Duke says no one was really harmed because all of them received the standard of care in chemotherapy.

352
00:39:48,340 --> 00:39:52,000
They did not advertise this as a standard of care program.

353
00:39:52,480 --> 00:39:57,160
They advertised this as an advanced clinical trial.

354
00:39:58,920 --> 00:40:06,420
Great results for what happened to my wife. I have to blame Cody and anyone else associated with him.

355
00:40:07,020 --> 00:40:16,430
Who? Knowingly. Promoted and false counterfeit clinical trial exploiting human beings.

356
00:40:17,630 --> 00:40:22,850
Dr. Modi resigned from Duke. He faces an investigation into research misconduct.

357
00:40:23,390 --> 00:40:27,410
He told us in an email that it would be inappropriate for him to comment.

358
00:40:27,860 --> 00:40:36,410
He wrote. My primary concern at all times is and will be the care of patients and seeking new ways to treat cancer.

359
00:40:37,190 --> 00:40:42,860
These days, he's working as a cancer doctor in South Carolina, and if you look online,

360
00:40:42,860 --> 00:40:50,120
you will see that he is celebrated for, quote, his significant contribution to the arena of lung cancer research.

361
00:40:50,780 --> 00:40:59,270
The websites were created with the help of an online reputation consultant, perhaps to put the best face on the available data.

362
00:41:04,750 --> 00:41:13,550
It's. So we just have like a couple of minutes left.

363
00:41:14,780 --> 00:41:20,630
But yeah. What are your thoughts? Have you have you seen you said you hadn't seen or heard of it?

364
00:41:20,920 --> 00:41:30,620
Um, yeah. Pretty insane, huh? Do you think? Um, his mentor, the guy I'm working on his name now, who is running that research lab at the time?

365
00:41:30,620 --> 00:41:36,830
The head of the research lab. Do you think he bears any responsibility? How did you feel about his responses?

366
00:41:42,100 --> 00:41:48,160
There's a little bit of the responsibility, I understand, of wanting to trust your mentee and thinking he's not doing anything.

367
00:41:48,550 --> 00:41:53,050
But when the initial accusations came up, you could have reviewed the data then.

368
00:41:53,260 --> 00:41:57,249
Yeah. And this could have been avoided. Right. Yeah.

369
00:41:57,250 --> 00:42:03,440
It's a tough situation when you're when you're running a research lab, you're responsible for the behavior, you know,

370
00:42:03,460 --> 00:42:09,730
in some regard, at least responsible for the behavior of your research team and the data that comes out of that lab.

371
00:42:11,170 --> 00:42:17,979
And I thought it was I thought it was telling you know, I think in a lot of cases,

372
00:42:17,980 --> 00:42:28,270
we genuinely want to believe that people have good intentions and that people wouldn't behave in such a terrible, terrible fashion.

373
00:42:28,270 --> 00:42:36,460
And maybe Dr. Cody was telling himself this lie that he he wasn't harming anybody and that he was,

374
00:42:37,240 --> 00:42:40,480
you know, who knows what his motivations were in that regard.

375
00:42:41,380 --> 00:42:52,000
But I yeah, this is a great example of a situation where, you know, your research misconduct is directly harming human health.

376
00:42:53,050 --> 00:42:57,160
But again, there are certainly cases where that are not so severe.

377
00:42:57,610 --> 00:43:04,599
But it's always important to even if you think about your research and that it's probably not going to be harmful,

378
00:43:04,600 --> 00:43:10,510
whatever you're doing, it's important to know that we we need to behave with integrity and not most, you know.

379
00:43:12,290 --> 00:43:15,830
Everything needs to be correct and validated.

380
00:43:16,550 --> 00:43:24,530
I don't think the thing they brought in like what they called a third party. I look at it like kind of the NBA is kind of a third party already.

381
00:43:25,490 --> 00:43:32,630
Yeah. But then also like the fact that the third party like supposed to be more objective review.

382
00:43:34,720 --> 00:43:40,230
But they just took that information. And I think that just seems like.

383
00:43:41,610 --> 00:43:45,390
Given the time. Right. Right.

384
00:43:45,960 --> 00:43:50,610
If these if this other research team had found that there were problems with the data,

385
00:43:50,880 --> 00:43:54,600
they should have been able they should have at least gone to these guys and,

386
00:43:55,080 --> 00:44:00,480
you know, figured out if they could address it, if they saw the same problems with the data.

387
00:44:00,510 --> 00:44:05,460
I don't understand how like, they're not objective. Right.

388
00:44:07,580 --> 00:44:15,020
Yeah, it was interesting. And then of course, you have the the added dynamic of you have the every human being on this planet.

389
00:44:15,020 --> 00:44:21,590
I think in general, I mean, every warm blooded human being wants us to find a cure for cancer.

390
00:44:21,860 --> 00:44:30,259
We want to be able to help people and we want to believe that something like that is really is real when you see this extraordinary result.

391
00:44:30,260 --> 00:44:37,730
But I think it just is a cautionary tale to around just having a healthy sense of.

392
00:44:39,510 --> 00:44:47,550
Pessimism, a healthy sense of skepticism about your data, you know, and and data that other people generate and,

393
00:44:48,930 --> 00:44:54,940
you know, try and make sure that you can reproduce it and make sure it's been vetted a million different ways.

394
00:44:54,940 --> 00:44:58,799
And that's something that's going to determine whether people are going into a clinical trial.

395
00:44:58,800 --> 00:45:01,990
So. Yeah. Yeah.

396
00:45:02,280 --> 00:45:10,920
Pretty much wraps up today's class in a couple of minutes over and I appreciate all of your input and some great discussion.

397
00:45:12,180 --> 00:45:18,390
Yeah. Have a wonderful holiday if I don't see you all. And we'll see you after break.

398
00:45:20,430 --> 00:45:20,670
Yep.

