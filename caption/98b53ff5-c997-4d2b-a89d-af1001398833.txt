1
00:00:07,930 --> 00:00:10,030
Good afternoon, everybody. So why don't we get started?

2
00:00:10,690 --> 00:00:17,620
Today we have quite technical capture and the goal is trying to go through the restricted maximum likelihood.

3
00:00:18,700 --> 00:00:25,569
I believe I still have a part to impart three of the and for a to finish and if time allows we'll

4
00:00:25,570 --> 00:00:30,670
go into the technical notes of will be and I think both both slides should be available to you.

5
00:00:32,410 --> 00:00:44,590
It doesn't hurt. To recap where we were in the last lecture recall, we started by introducing the general media model for longitudinal data,

6
00:00:44,980 --> 00:00:48,639
and with the model formulated, we got to do inference.

7
00:00:48,640 --> 00:00:53,740
I estimate parameters of interest and hopefully we can do the estimation the best we can,

8
00:00:53,740 --> 00:00:58,450
i.e. having estimates that are highly efficient with small variances.

9
00:00:58,990 --> 00:01:04,930
So we talked about a very classical method called maximum likelihood or ML for short,

10
00:01:05,830 --> 00:01:12,550
and I promised that we will be talking about a new thing which is called restricted maximum likelihood.

11
00:01:12,790 --> 00:01:18,730
Remo And there I owe you an explanation of what restriction means here, so I will be covering that.

12
00:01:20,050 --> 00:01:25,390
Importantly, these methods defer in their way of estimating variance covariance matrices.

13
00:01:26,230 --> 00:01:33,400
So just as a review of the objectives I'll throw for our objectives.

14
00:01:33,730 --> 00:01:42,040
So the first one is that we need to know how to drive the maximum likelihood estimate, and we have covered that in the last lecture.

15
00:01:42,400 --> 00:01:45,550
So you should ask yourself whether you have a general sense of how this is done.

16
00:01:46,300 --> 00:01:52,900
And two, three, four are going to be the objectives for today. And first, describe why this new thing is needed.

17
00:01:53,020 --> 00:02:05,020
Why bother? And third, how do we how the what is the optimizing criteria for obtaining this estimate are called REMO?

18
00:02:05,860 --> 00:02:11,260
And finally, we will have one example demonstrating the difference between the amount and Remo estimate.

19
00:02:12,070 --> 00:02:16,420
Does this sound like a plan? Okay. All right.

20
00:02:16,440 --> 00:02:49,200
So I'll be jumping forward to the slide where we stopped. Part two Remo for estimating the covariance matrix in neater models.

21
00:02:49,740 --> 00:02:57,030
Just as a quick reminder that we do have this important mathematical fact,

22
00:02:57,030 --> 00:03:05,850
I need to remind you that whenever you're doing estimation of data, those data will be represented by this one.

23
00:03:08,640 --> 00:03:19,920
Where we can be the sigma had eml or it can be the sigma had remo so.

24
00:03:22,970 --> 00:03:31,370
We have a common or generic formula that goes from the assumed variance covariance matrix to the beta estimate.

25
00:03:31,670 --> 00:03:39,380
But we do have two options or more than two BE Well, how do we replace the V by so an estimate of variance covariance.

26
00:03:40,760 --> 00:03:43,910
How do we derive the beta formula?

27
00:03:44,440 --> 00:03:51,670
Um, it is in technical note for b. And offer be.

28
00:03:55,020 --> 00:03:59,430
And it is also covered by the recordings I sent Monday.

29
00:04:05,440 --> 00:04:10,840
What's the date on Monday? Top September 12.

30
00:04:12,650 --> 00:04:19,930
But we will be covering that derivation again. So our goal is trying to do this.

31
00:04:19,940 --> 00:04:23,300
What is this REMO estimate for variance covariance matrix.

32
00:04:27,480 --> 00:04:39,960
So some mathematics here. Recall that when we were doing maximum likelihood, we have to work with a particular local function.

33
00:04:40,080 --> 00:04:48,180
Right. So if it helps you, if you have I equals one to n indicating subjects.

34
00:04:48,210 --> 00:04:56,730
Right. And for everybody, it may have a different number of observations.

35
00:05:05,770 --> 00:05:13,930
Right. So how do we write down the likelihood? Um, basically we need to recall that the outcome from each subject,

36
00:05:14,320 --> 00:05:20,050
forms of that vector that can be assumed to be drawn from both of our Gaussian distribution.

37
00:05:25,050 --> 00:05:29,240
So for one person. I. What's the likelihood?

38
00:05:30,200 --> 00:05:35,030
Well. You do have the data for this person.

39
00:05:37,700 --> 00:05:41,540
You do have the model assumption right for.

40
00:05:42,330 --> 00:06:00,800
Um. The main. And I'm assuming that the variance covariance matrix is sigma I inverse and we copy that.

41
00:06:04,310 --> 00:06:10,470
So this needs to be divided by one or two matters and with a minus sign.

42
00:06:10,490 --> 00:06:14,870
So this forms the kernel of that likelihood.

43
00:06:15,470 --> 00:06:22,830
And we do have some normalizing constants here. Which car should I use?

44
00:06:22,860 --> 00:06:27,590
How about green here? Okay. So what's the normalizing constant?

45
00:06:28,040 --> 00:06:32,070
Well, we have the two pi. To the power of.

46
00:06:32,910 --> 00:06:38,370
And I divided by two. I'm just remembering the call from a memory.

47
00:06:38,370 --> 00:06:45,600
But. And also we have a of AI that determines to the.

48
00:06:46,630 --> 00:06:58,800
No one will have power. So these two things combined together is the likelihood for such a night.

49
00:06:58,830 --> 00:07:07,260
Would you guys agree? Okay. Now, because we are going to assume independence across people.

50
00:07:16,730 --> 00:07:27,200
Then we just multiply everybody together. So in this case, we have written down the entire likelihood, okay, for all the subjects.

51
00:07:28,100 --> 00:07:31,250
And I want to highlight the unknowns in Orange.

52
00:07:32,000 --> 00:07:37,300
So the unknowns are the betas. And the Sigma's.

53
00:07:39,550 --> 00:07:55,310
Okay. So this is how I convince myself it's important to remember this.

54
00:07:55,550 --> 00:08:01,940
That's the function of. Now we will use a notation.

55
00:08:03,820 --> 00:08:06,910
This one book equals the total number.

56
00:08:09,750 --> 00:08:14,430
Number of measurements for all the subjects.

57
00:08:14,700 --> 00:08:21,510
Right. And this is just my shorthand for measurements because it's some overall the number of occasions across all the subjects.

58
00:08:22,740 --> 00:08:29,370
And if you take the log right, you can take a log, then every multiplication will be converted into a summation.

59
00:08:29,760 --> 00:08:35,240
So what you get will be something like this, okay, for which I want now do here for the sake of time.

60
00:08:35,250 --> 00:08:41,469
But hopefully that should be clear to you. In the next few slides.

61
00:08:41,470 --> 00:08:48,040
My goal is trying to communicate that now estimates for.

62
00:08:49,630 --> 00:08:54,240
For variance covariance. It's bias.

63
00:08:57,620 --> 00:09:03,930
In finite samples. Okay.

64
00:09:04,530 --> 00:09:11,400
So to make that point, I want to work with a similar situation, which is to assume that Sigma I.

65
00:09:23,140 --> 00:09:27,140
Because Sigma Square, I and I bought a nice oak.

66
00:09:29,340 --> 00:09:37,200
So this should simplify the discussion. At least we have only one scalar unknown for the variance covariance which is the.

67
00:09:38,400 --> 00:09:44,380
Sigma Square here. So let's see what EML will do in estimating sigma squared.

68
00:09:44,670 --> 00:09:59,180
My claim is that it will be biased. Okay.

69
00:09:59,390 --> 00:10:05,140
Moving on. So. These two are the formula for the ML.

70
00:10:05,990 --> 00:10:09,170
How did I get them? Hand over me. I'll cover them.

71
00:10:09,260 --> 00:10:13,010
But for the sake of any idea, I'll ask you to trust me that these are correct.

72
00:10:13,420 --> 00:10:17,299
Okay. So for the first one, it is really not surprising.

73
00:10:17,300 --> 00:10:23,510
I would not go through with it, but it is just the had matrix multiplied by the vector responses.

74
00:10:24,320 --> 00:10:30,020
How about the second one which shows the maximum like the estimate for the sigma squared.

75
00:10:30,860 --> 00:10:35,870
So in here it is what we call. A word starting with R.

76
00:10:35,930 --> 00:10:42,390
What's that word? Residual.

77
00:10:44,690 --> 00:10:48,450
Everybody. Okay. I hope the guys you guys know this word.

78
00:10:49,190 --> 00:10:52,459
I think you guys do. But you guys just play it sometimes.

79
00:10:52,460 --> 00:11:01,980
You got to help me. All right, so this is the. What's left after you have subtracted the model components.

80
00:11:01,980 --> 00:11:08,640
So the mean from the response. Right. So this is what we do not model and the variance of covariance.

81
00:11:10,020 --> 00:11:14,160
By definition, is trying to account for what's the residual variability there.

82
00:11:14,220 --> 00:11:25,050
Right. Notice in the setting, because we have assumed that sigma I equals sigma squared times I it is to say that.

83
00:11:26,100 --> 00:11:32,640
Despite I have, say, ten measurements for the sake of this presentation, I'm not going to assume those ten measurements are independent.

84
00:11:33,780 --> 00:11:38,190
So you can say that I contribute ten measurements.

85
00:11:38,370 --> 00:11:42,780
You contribute ten measurements, a total of 20 independent measurements in the simplified setting.

86
00:11:44,040 --> 00:11:51,570
So all this summation here, although it appears as a double summation, but it is just trying to sum of all the observations.

87
00:11:53,680 --> 00:11:57,159
How about K here? Recall K equals the sum of.

88
00:11:57,160 --> 00:12:00,350
And I hear. And here.

89
00:12:02,140 --> 00:12:07,210
I think I also made an additional assumption that everybody has the same number of measurements.

90
00:12:07,810 --> 00:12:14,430
So it is basically the total number of measurements, right? Does this feel a bit weird in terms of denominator?

91
00:12:24,310 --> 00:12:27,670
So what's the simple violence prevention estimate or.

92
00:12:29,190 --> 00:12:41,690
For. Like this one. Did you divide by minus one or divide by N and minus one?

93
00:12:41,690 --> 00:12:46,800
Right. And what's the reason for that? What's the reason for that?

94
00:12:48,990 --> 00:12:53,280
We can tell me his inputs on bias. That's actually one of the most important reasons.

95
00:12:55,980 --> 00:13:01,600
Do we have any other reason for doing this? Probably.

96
00:13:04,150 --> 00:13:12,090
You got to speak louder. Sorry. Okay.

97
00:13:15,380 --> 00:13:25,190
To me, I think the UN business is probably the most important reason and I want to ask you why the minus one?

98
00:13:25,190 --> 00:13:28,910
Why not minus two? Why not -2.5?

99
00:13:28,920 --> 00:13:31,910
Why not minus ten? Where does that one come from?

100
00:13:36,930 --> 00:13:44,400
So here in this simple setting, what we what we were trying to do is just trying to estimate the one single parameter for the main.

101
00:13:45,310 --> 00:13:51,280
It's called model degree freedom. You are only using one main to model the exercise.

102
00:13:51,850 --> 00:13:56,140
So one represents the. The main.

103
00:13:57,450 --> 00:14:00,670
Models. Degree of freedom.

104
00:14:02,540 --> 00:14:10,510
Anyway, I'm a little bit ahead of myself, but I just wanted to remind you guys that this appearance of Kay here is really abnormal.

105
00:14:10,940 --> 00:14:14,260
Unless you want to mind is something that's really a dimension of beat out there.

106
00:14:14,700 --> 00:14:18,250
Right. But if we all indulge ourselves.

107
00:14:18,260 --> 00:14:21,490
So let's see. The real consequence of dividing by Kay.

108
00:14:23,200 --> 00:14:29,740
Essentially, we will be able to find mathematically that the maximum likely estimate for sigma squared will be.

109
00:14:30,650 --> 00:14:35,990
Bias upwards or downwards. Relative to truth.

110
00:14:40,400 --> 00:14:47,180
I if we do them, that's not good. We have an estimate that's too small relative to the truth or to.

111
00:14:48,020 --> 00:14:51,280
It's a big relative to the truth. Samarra.

112
00:14:51,680 --> 00:14:55,850
So you will underestimate the liability relative to the truth.

113
00:14:56,330 --> 00:15:05,030
If you do that now. Okay. So that's the the reason why we are going to consider alternative estimates is four sigma squared.

114
00:15:05,690 --> 00:15:09,260
And one thing I want to note here is the magnitude of this bias.

115
00:15:09,680 --> 00:15:14,210
Okay. So we call K is a total number of observations.

116
00:15:14,480 --> 00:15:18,530
Right. And P is a total number of parameters in the model.

117
00:15:19,580 --> 00:15:28,160
Now, let's entertain with two sets of numbers, say CP, which equals like ten P equals like.

118
00:15:30,550 --> 00:15:38,320
One 1005 versus 1000 to 1000.

119
00:15:40,920 --> 00:15:45,810
Yeah. I'll just remove the final one. Actually, let me change them.

120
00:15:51,340 --> 00:15:56,890
So he goes to basically indicate we have in this model and he goes ten basically

121
00:15:57,100 --> 00:16:01,900
means the total number of measurements across all the subjects is ten.

122
00:16:02,020 --> 00:16:07,000
And the second situation is with more data.

123
00:16:07,330 --> 00:16:11,809
So what's this ratio here? Eight divided by ten.

124
00:16:11,810 --> 00:16:12,830
Right. So point eight.

125
00:16:14,150 --> 00:16:21,710
Which means that if you have only ten measurements and you have like two parameters, estimate your estimate of the variance covariance.

126
00:16:22,100 --> 00:16:26,479
Sorry, the variance parameter will be 20% smaller than truth on average.

127
00:16:26,480 --> 00:16:33,830
Right. How about this one? Well, it's still very close to one, I would say.

128
00:16:33,860 --> 00:16:41,360
Right. So when you have a large sample size, which means the logic, the bias is probably.

129
00:16:42,390 --> 00:16:52,580
Tolerable. So that is why we say that this issue of business comes out, you know, is most prominent when you have a small number of measurements.

130
00:16:56,220 --> 00:16:59,970
Or alternatively, we can consider C and D here.

131
00:17:01,560 --> 00:17:06,450
So if you have, say, 100 total number of measurements.

132
00:17:07,080 --> 00:17:11,580
But for one model, that's simple. You have into and slope for the model.

133
00:17:11,610 --> 00:17:15,420
It's really ambitious. You're trying to estimate, say, 50 parameters.

134
00:17:16,470 --> 00:17:20,540
You can go crazy by putting in many splicing phases.

135
00:17:21,270 --> 00:17:28,360
So what's the ratio here for the first one? That ratio is 0.98.

136
00:17:28,380 --> 00:17:31,500
Right. So pretty close to one. How about this one?

137
00:17:32,160 --> 00:17:39,900
Ratio 3.5. So it is to say that if you look at scenario number D, if you have a lot of parameters,

138
00:17:41,490 --> 00:17:46,050
you know your bias, the best underestimating the true variance will be 50%.

139
00:17:46,830 --> 00:17:52,020
If you have fewer parameters, we buyers of underestimating the true fairness will be 2%.

140
00:17:52,650 --> 00:18:03,240
Right. So these scenarios, ABCD is trying to illustrate to you that as long as you can make sure that came out is P divide by K is close to one.

141
00:18:03,990 --> 00:18:07,100
This issue will not be too big. Okay.

142
00:18:07,290 --> 00:18:10,920
So that's why you know this.

143
00:18:11,930 --> 00:18:16,040
Relative magnitude of care should be part of your consideration.

144
00:18:21,590 --> 00:18:27,239
Now let's do this correction. We just divided by another thing came on his feet.

145
00:18:27,240 --> 00:18:36,190
So this will be unbiased. We call.

146
00:18:36,190 --> 00:18:43,690
We have tried to present this issue using sigma i equals sigma.

147
00:18:45,190 --> 00:18:53,080
Sigma squared times the identity matrix. Clearly this is a situation where we have grown out of it.

148
00:18:53,110 --> 00:19:00,940
We want to ask whether this issue persists when Sigma is truly complex, like the off diagonal elements are not zero.

149
00:19:01,690 --> 00:19:08,290
So this should not be surprising that there might be some more issue occurring when you have correlated observations.

150
00:19:10,820 --> 00:19:20,870
And as I have said, regardless of whether sigma is diagonal or not, when K is large relative to the P, this bias issue will not be that severe.

151
00:19:24,430 --> 00:19:34,570
Between the two. For the sake of estimating the various covariance matrix, I would often suggest just go with the remo.

152
00:19:35,260 --> 00:19:39,820
Okay. That is why in general we always use the low over n minus one.

153
00:19:40,120 --> 00:19:43,180
You don't you never use while n I guess so.

154
00:19:43,390 --> 00:19:47,750
My recommendation. Is to use.

155
00:19:48,440 --> 00:19:52,460
Unless you have reason to see us now, I will touch those reasons.

156
00:19:52,760 --> 00:20:02,040
Not in today's lecture, but in future lectures. I want to pause for 30 seconds just to see if we have any questions.

157
00:20:34,760 --> 00:20:42,920
Okay. I don't see any questions, so let's continue. Now some discussion points.

158
00:20:43,670 --> 00:20:51,410
First is that we should recognize that mathematically we can show there are some issues with the bias.

159
00:20:51,980 --> 00:20:58,520
It does not necessarily know. It does not necessarily say how we can fix it or generally what's the problem.

160
00:20:59,180 --> 00:21:03,230
I think hopefully that previous example.

161
00:21:04,280 --> 00:21:11,330
Legitimize this question Why did this bias happen if you use EML, which seems to be a pretty well-established method?

162
00:21:12,500 --> 00:21:17,870
So here are some points. They are quite intuitive, but I think should help you to think.

163
00:21:19,920 --> 00:21:29,790
First, when we are doing the estimation, we what we did was to maximize beta and sigma jointly.

164
00:21:31,700 --> 00:21:37,190
We did not take into account the fact that the regression coefficients is also estimated from data.

165
00:21:39,180 --> 00:21:43,860
Okay. So this is the intuition why the bias occurred.

166
00:21:44,700 --> 00:21:53,750
And then what's the solution then? It is to basically eliminate beta from the likelihood.

167
00:21:55,420 --> 00:22:05,440
So we can propose an alternative objective function that is derived based on the notion that that has nothing to do with beta.

168
00:22:08,420 --> 00:22:19,020
So there are a few ways to do this. The way we will be focusing on, at least in these slides, is about transformation perspective,

169
00:22:19,740 --> 00:22:26,550
which is to transform the outcomes so that the distribution of the transformed outcome does not depend on beta.

170
00:22:27,920 --> 00:22:37,880
So when you're optimizing the likelihood, likelihood of that transform data, you will be able to correct for this fact that you it better from data.

171
00:22:40,100 --> 00:22:50,660
So we'll we'll see how we do it. We call the modified we call the new objective function to be modified likelihood.

172
00:22:51,950 --> 00:22:55,720
But what do we mean by modified? So.

173
00:22:57,390 --> 00:23:00,840
I'll read this first sentence, just to be clear.

174
00:23:01,410 --> 00:23:10,200
Transform data so it has a distribution that does not depend on beta recall when we were writing down the like for y eyes.

175
00:23:10,380 --> 00:23:14,770
Right. You should know that. So the likelihood depends on what?

176
00:23:15,130 --> 00:23:26,610
Depends on both the beta and sigma. Right.

177
00:23:28,000 --> 00:23:33,330
Now, hopefully we can find. A transformation.

178
00:23:34,520 --> 00:23:48,280
Transformation. Of the whys so that what we get in terms of a modify, the likelihood will only depend on sigma.

179
00:23:56,530 --> 00:24:01,600
Where the Y stars are going to be. The transformed responses.

180
00:24:04,090 --> 00:24:12,040
Again, if I call this part A and this part B, we have been doing the maximum like of using A.

181
00:24:13,170 --> 00:24:16,710
But that ignores the fact that data is being estimated as well.

182
00:24:17,340 --> 00:24:25,110
So what seems to be working is that we can transform the Y to some Y stars.

183
00:24:25,500 --> 00:24:31,560
So for the modify, like for those why stars matter does not appear any more.

184
00:24:32,760 --> 00:24:35,820
Okay. So this implant. Implant, clearly you.

185
00:24:38,190 --> 00:24:41,760
Okay. So the question is, what are the Y stars?

186
00:24:42,690 --> 00:24:49,309
You know, what are they? Well, this is what I'm going to issue.

187
00:24:49,310 --> 00:24:56,690
But for some historical note, it first appeared in 1971.

188
00:24:58,620 --> 00:25:01,950
By two people called Pedersen.

189
00:25:05,050 --> 00:25:10,110
And Thompson. Okay.

190
00:25:13,900 --> 00:25:18,370
Again, it's more than 50 years old or, if you will, half century old.

191
00:25:19,240 --> 00:25:26,230
So the solution is basically to take wise stars as residuals from ordinary squares.

192
00:25:36,960 --> 00:25:52,480
Here. It is not the only transformation that can be OC for this beta only depend on sigma, but this is the simplest choice.

193
00:25:53,680 --> 00:25:56,920
If you can recall what you learned a year ago, like 650.

194
00:25:57,430 --> 00:26:04,630
You do recall that the residuals, the estimated residuals are independent from the mean because you have those projection interpretations.

195
00:26:05,080 --> 00:26:14,060
I don't know if you still recall. So the claim I'm going to make is that why a star has a distribution that does not depend on data.

196
00:26:14,420 --> 00:26:19,430
Hence it's entirely possible to write that star as is shown in B there.

197
00:26:22,540 --> 00:26:26,740
When you write that down and when you do log transformation, this is what you got.

198
00:26:33,790 --> 00:26:36,400
I will make a few remarks about this particular equation.

199
00:26:36,880 --> 00:26:42,520
I believe it is really important for you to understand what is going on here without diving into these notation.

200
00:26:43,270 --> 00:26:48,810
First, this is just a plain, simple, maximum likelihood objective, right?

201
00:26:51,000 --> 00:26:55,680
The usual likelihood. Use your likeness.

202
00:27:00,030 --> 00:27:04,020
All right. Second term. The second term is new to us.

203
00:27:05,780 --> 00:27:09,770
Essentially this term is what's unique to Ramo.

204
00:27:12,710 --> 00:27:21,950
We will talk about the interpretation here. But this term's goal is trying to account for the fact that data is estimated rather than known.

205
00:27:47,650 --> 00:27:55,950
Okay. Now in here, you may be wondering how you can still see a better hat here, right?

206
00:27:56,160 --> 00:28:02,100
Because if you can if you want to look at my color, our path dependent, now it's orange.

207
00:28:02,520 --> 00:28:05,760
That's not what I intended. But hopefully you can still see them.

208
00:28:06,060 --> 00:28:11,520
I'm going to change to a blue here just for the sake of presentation.

209
00:28:11,520 --> 00:28:14,970
So I want to highlight to you, where are the unknowns?

210
00:28:15,810 --> 00:28:20,390
Here. Here. How about here?

211
00:28:21,790 --> 00:28:25,690
Can you guys convince yourself that this is a thing that depends on Sigma?

212
00:28:25,720 --> 00:28:30,450
I only. I'll give you 30 seconds to think about it.

213
00:28:30,480 --> 00:28:36,630
The argument my claim is that better had depends only on sigma across people.

214
00:29:09,690 --> 00:29:12,819
So. The way to think about this is,

215
00:29:12,820 --> 00:29:24,610
as I have alluded to you better hats is always obtained as a plug in estimate or where whatever thing you're getting, get four sigma.

216
00:29:25,910 --> 00:29:31,070
You plug in. And that better transformation is universal regardless of what Sigma is.

217
00:29:32,420 --> 00:29:34,190
And that formula is fixed.

218
00:29:34,760 --> 00:29:43,300
So and so my claim is that this entire thing, the first row in the second row in the third bullet point all depends on sigma.

219
00:29:44,600 --> 00:29:47,760
Hopefully we're on the same page here. Any questions?

220
00:29:54,530 --> 00:30:01,050
Now, as we know. Sigma I sometimes can be proud of tries by a few a number of parameters.

221
00:30:01,920 --> 00:30:06,180
So notation wise, we often denote this name by what?

222
00:30:06,720 --> 00:30:18,620
Sigma. Where theta is of a lower dimension.

223
00:30:19,190 --> 00:30:25,730
Okay. So again, I plug in this particular parameterization of the sigma i's.

224
00:30:26,870 --> 00:30:33,860
This entire objective function, which we call modified likelihood, now depends only on theta.

225
00:30:34,460 --> 00:30:40,430
And hopefully you can use whatever optimization function that's available to you to get a theta.

226
00:30:41,030 --> 00:31:01,349
And that's how we do it conceptually. So once we have gets the sigma hat, then you just plug into this particular formula.

227
00:31:01,350 --> 00:31:04,940
You will get a beta harassment. All right.

228
00:31:05,300 --> 00:31:12,920
I know. Point number three is entirely new to you, so that I will provide one additional explanation here.

229
00:31:15,100 --> 00:31:22,270
So how do we interpret the additional corruption term? If we do some simple calculation,

230
00:31:22,810 --> 00:31:31,620
we will be able to show that final corruption term is log of the square root of generalized variance of the beta hat.

231
00:31:31,750 --> 00:31:38,660
Right. Okay so the term has something to do with Peter had estimation uncertainty.

232
00:31:39,640 --> 00:31:48,970
So it is in this sense that the modified likelihood fulfills the promise that we correct for the fact that beta was estimated rather than known.

233
00:31:55,090 --> 00:31:59,920
Again, this is purely for your interpretation of that additional term.

234
00:32:00,490 --> 00:32:06,190
And for this part you will need to know that's a covariance of beta.

235
00:32:06,190 --> 00:32:12,520
How take a particular form. Which if you have a garden, it's on.

236
00:32:15,460 --> 00:32:20,500
Page. Page 18 of the slides.

237
00:32:33,040 --> 00:32:39,370
So in practice, we will just obtain the estimate of the.

238
00:32:41,850 --> 00:32:47,450
Beta using this formula. Okay.

239
00:32:48,070 --> 00:32:54,260
If you sort of replace this by the. And this is what I meant by the beta v here.

240
00:32:57,230 --> 00:33:02,360
Okay. If you plug in, you plug in the sigma.

241
00:33:03,020 --> 00:33:06,620
I have there. You've got beta sigma at.

242
00:33:10,410 --> 00:33:15,480
So my recommendation is that by default we use Remo for estimating Sigma how to bailer

243
00:33:15,990 --> 00:33:23,520
and and as we have seen with the examples about different combinations of K and peer,

244
00:33:24,010 --> 00:33:30,600
they can be very similar when K is large relative to P, some additional terminologies.

245
00:33:31,710 --> 00:33:36,590
So when somebody is talking to you say Remo estimate, right?

246
00:33:41,160 --> 00:33:44,960
Remo was first introduced.

247
00:33:44,970 --> 00:33:48,240
When you are estimating Sigma. I have. Okay.

248
00:33:48,750 --> 00:33:51,899
So it's a Remo estimate of the variance.

249
00:33:51,900 --> 00:33:58,550
Coburn's. That's the entire you know,

250
00:33:59,600 --> 00:34:11,300
we were focusing on the variance covariance matrix however because beta had estimate depends on really what is the sigma estimate you plugged in.

251
00:34:14,240 --> 00:34:21,580
You can plug it in. You can plug in REM. All right. And this speed ahead will be called a REMO estimate.

252
00:34:22,930 --> 00:34:26,530
So when we are talking about reimbursement, I would like to.

253
00:34:29,160 --> 00:34:36,629
Clarify that often it refers to both the various cover and estimate the sigma has and also

254
00:34:36,630 --> 00:34:41,760
the resulting beta harassment with the rental estimate for this various covers plugged in.

255
00:34:43,980 --> 00:34:50,430
Okay. Now, what if we plugged in Sigma EML?

256
00:34:51,900 --> 00:34:55,290
Well, we just call that a maximum of a beta.

257
00:34:56,910 --> 00:35:05,760
So terminology wise when we were referring to Remo or EML would just refer to what is the sigma estimate we plugged in.

258
00:35:07,410 --> 00:35:18,120
Okay. A final clarification.

259
00:35:20,300 --> 00:35:30,020
Why do we call this restricted? We probably can call it a transformed.

260
00:35:31,330 --> 00:35:38,080
But for reasons I will talk about later, this is called restricted.

261
00:35:44,120 --> 00:35:53,040
But here is how I remember it. Restrict our attention.

262
00:35:56,380 --> 00:36:00,580
To Sigma I. You're just lucky to be here, right?

263
00:36:00,940 --> 00:36:05,410
Because maybe we just to restrict the unknown to see my.

264
00:36:06,040 --> 00:36:11,620
So that when we optimizing the objective function, we remedy that bias issue.

265
00:36:18,880 --> 00:36:25,060
Okay. I'm going to pause again for 30 seconds just to see if we have any burning questions.

266
00:36:25,270 --> 00:36:44,840
US. Can somebody try this?

267
00:36:46,760 --> 00:36:54,230
Can you try to speak into this microphone to see if it does work?

268
00:36:54,530 --> 00:36:59,590
Okay. I'm going to throw this to you guys if you ask questions. Okay.

269
00:37:10,180 --> 00:37:18,700
Okay. Well, no worries. If you later on come up with any questions, just come to the office or ask on the piazza.

270
00:37:19,180 --> 00:37:24,520
So that should have covered the definition of Remo and the.

271
00:37:25,000 --> 00:37:27,580
Well, we have a reason for using Remo.

272
00:37:28,150 --> 00:37:40,750
The definition of Remo at the high level and I emphasize that Peter had estimate depends entirely on the sigma estimate you plugged in.

273
00:37:41,830 --> 00:37:46,600
And also we interpreted this particular this additional correction term.

274
00:37:47,530 --> 00:37:57,760
And we have said that this accounts for the fact that we now estimate better instead of assuming it's known.

275
00:37:59,810 --> 00:38:04,430
So I think we're being honest when doing France. Okay.

276
00:38:06,430 --> 00:38:09,160
Finally, we recommend the ramble in general.

277
00:38:09,310 --> 00:38:17,800
And in the next part I'm going to show you Remo and me are truly different, which should not be surprising now.

278
00:38:18,340 --> 00:38:21,520
And indeed they can be very similar in large sample sizes.

279
00:38:22,360 --> 00:38:26,740
So focusing on the difference between Mel and Remo, I'm going to present the example here.

280
00:38:28,630 --> 00:38:33,250
So this is a data set. I believe it's a canvas.

281
00:38:33,250 --> 00:38:40,450
Hopefully you can see it. And by the way, all the code here should be reproducible, which means that if you copy and paste, they should be vulnerable.

282
00:38:41,050 --> 00:38:45,130
As long as you're willing to change the working directory to write one.

283
00:38:46,660 --> 00:38:50,080
This data set is a very clean data set.

284
00:38:50,710 --> 00:38:55,300
So the pigs are representing a missing guinea pigs.

285
00:38:55,450 --> 00:39:02,409
Not real big. I don't know units here. So here the data set is what we call OC.

286
00:39:02,410 --> 00:39:06,350
You have two options. The format is wide or long.

287
00:39:08,620 --> 00:39:13,759
You got to make a choice right along. I explain this?

288
00:39:13,760 --> 00:39:23,240
Right. So one, two, we have six guinea pigs here and the columns represents the measurements of their weight across nine weeks.

289
00:39:24,270 --> 00:39:28,950
Okay. And this is one particular way of representing the data.

290
00:39:29,040 --> 00:39:33,450
And my question for you to guess, if you will, is it wide or long?

291
00:39:38,320 --> 00:39:41,820
If by law I mean vertically. Right.

292
00:39:41,870 --> 00:39:48,550
Right. It's a very wide. So you will be you will need this concept.

293
00:39:50,160 --> 00:39:52,350
You will need to understand this concept very sharply,

294
00:39:52,800 --> 00:39:57,840
which means that if at the end of this lecture you do not know the difference between right and wrong,

295
00:39:58,440 --> 00:40:04,080
you should at least play with the code and then get a sense. Okay. So then what's the long format?

296
00:40:05,640 --> 00:40:16,080
This is just a much more relaxed. We don't have any technical things, so you have much longer data set where the first column is in the second time.

297
00:40:16,110 --> 00:40:22,620
Second column is occasion. The final.

298
00:40:24,750 --> 00:40:28,740
Actually. Let me just write much better here. This.

299
00:40:29,850 --> 00:40:35,780
Slope is kilometer. I want to. It's just so hard.

300
00:40:40,710 --> 00:40:48,110
This is a week. And the final one will be the weight.

301
00:40:51,960 --> 00:40:57,000
Let's try to convert our one pics data to long format.

302
00:40:58,260 --> 00:41:02,320
So. We will need to have how many rows?

303
00:41:06,130 --> 00:41:09,490
Nine rows, right. One, two, three, four.

304
00:41:12,190 --> 00:41:15,280
Nine rose and the weight will be 24.

305
00:41:16,090 --> 00:41:22,270
32. 39 for 2.5 and 72.

306
00:41:23,290 --> 00:41:26,490
If you're a visual person, this is what I'm going to do. Okay.

307
00:41:26,950 --> 00:41:29,950
Here. You're.

308
00:41:32,380 --> 00:41:37,370
Here. So it is very long.

309
00:41:39,550 --> 00:41:47,380
And we will be operating on the long format data. Okay.

310
00:41:48,610 --> 00:41:52,390
Now we can plot data. So these are what we call spaghetti plot.

311
00:41:54,130 --> 00:42:03,560
You can see that there are some rent preservation here. Pigs that were heavy at the beginning tends to stay heavier than other picks picks up.

312
00:42:03,580 --> 00:42:12,940
So. This is this pattern is very commonly observed when you're studying waves and heights, of course, weights.

313
00:42:13,180 --> 00:42:21,340
And we will be using one simple model to model these phenomena, which is to say that we have some rank preservation.

314
00:42:30,350 --> 00:42:37,270
The heavy picks to heavier across time. So what is a model we're going to fit?

315
00:42:38,110 --> 00:42:44,500
The model we're going to fit is why j we call these are the weights.

316
00:42:45,820 --> 00:42:49,420
Or rather wait for pick I an occasion.

317
00:42:50,080 --> 00:42:53,290
You got a number there? We want to explain.

318
00:42:55,350 --> 00:42:59,100
The weight by a few. Well, just one factor at a time.

319
00:43:00,640 --> 00:43:10,690
How do we do that? We assume a new model. Why did I ignore the ice index here?

320
00:43:13,540 --> 00:43:18,910
Well it's because every peg was measure that come and set occasions you have no need to have that so index here.

321
00:43:18,940 --> 00:43:24,749
So. What's the final thing we want out? Clearly, these are not actual storylines, lines.

322
00:43:24,750 --> 00:43:28,960
We have some deviations. Right. So you want to model the humor in this spy?

323
00:43:28,980 --> 00:43:37,910
It's an idea here. And I mean that literally, because whatever things you put into error, I think mostly our ignorance.

324
00:43:37,920 --> 00:43:46,220
I think we should not be. That's a hubris to assume we know everything by modeling the mean and we will assume that.

325
00:43:47,760 --> 00:43:53,350
Sigma i j. Is going to be independent from Sigma Prime.

326
00:43:53,350 --> 00:44:05,610
Jay Prime. I explain this look we have to subject ducks here and I prime which means that we are talking about errors from two picks.

327
00:44:06,600 --> 00:44:11,070
And JJ Prime Day can be the same occasion or different occasions, but at least these are.

328
00:44:13,630 --> 00:44:20,380
Residuals coming from different picks. However, we will assume that if i.

329
00:44:20,700 --> 00:44:23,719
J. Correlation.

330
00:44:23,720 --> 00:44:31,080
F i j emotional i j prime. Will be what will be say theta.

331
00:44:32,510 --> 00:44:40,830
To our reserve the. Also covariance about covariance reserve theta one for the variance.

332
00:44:41,430 --> 00:44:45,180
Okay. So this means that for the same pic I.

333
00:44:46,310 --> 00:44:51,200
The errors occurred at two occasions and they likely are correlated.

334
00:44:52,860 --> 00:44:57,590
Okay. Oh, by the way, have you guys learned this notation?

335
00:44:58,660 --> 00:45:06,580
It just means that statistically independent. Okay.

336
00:45:10,810 --> 00:45:15,520
You guys know who invented this imitation? I think it's Peter.

337
00:45:15,520 --> 00:45:22,020
Do it. Anyway, so.

338
00:45:25,840 --> 00:45:33,160
We will also assume that the variance of Egyptian i j to be signal c the one here.

339
00:45:35,310 --> 00:45:38,790
I will present the following fact to you later on.

340
00:45:38,790 --> 00:45:46,890
That will be a maybe a homework for my question, not for the first one more, but later on for some homework questions or midterm questions.

341
00:45:47,280 --> 00:45:55,800
So the Sigma II, which is the covariance matrix of why I given.

342
00:45:58,830 --> 00:46:02,069
Cohorts the covers of what like t one to t nine.

343
00:46:02,070 --> 00:46:05,310
Right. This will be of the following format.

344
00:46:08,020 --> 00:46:14,880
Theta one. To. Uh, actually, let me check.

345
00:46:29,730 --> 00:46:34,680
Theta one theta 1.01, everything else, theta two.

346
00:46:35,250 --> 00:46:38,850
So this is what we call exchangeable correlation matrix.

347
00:46:40,810 --> 00:46:44,230
This means that we have decided to privatize Sigma.

348
00:46:45,100 --> 00:46:50,930
I buy this one. Okay.

349
00:46:51,410 --> 00:46:54,560
And the main structure be straightforward. So our goal.

350
00:46:55,130 --> 00:47:01,310
Oh, by the way, our goal is trying to estimate what theta one CO2 oc.

351
00:47:21,450 --> 00:47:30,070
Here is how we will fit a model here. We load this library which will enable you to fit a model.

352
00:47:30,820 --> 00:47:36,160
I don't think I will have taught this, but this is just one way of doing the fitting.

353
00:47:36,160 --> 00:47:45,970
The model I just said the model is going to take the weight as the outcome and is going to have weak as a predictor.

354
00:47:46,600 --> 00:47:52,340
And this one is basically to tell the program that if you have it.

355
00:47:53,450 --> 00:47:58,370
Different rows from the long formatted data have the same ID. They are coming from the same pig.

356
00:47:59,990 --> 00:48:05,240
And also this specifies the various covariance matrix we just discussed.

357
00:48:06,020 --> 00:48:09,980
So those are lots of things packed into here. But that's not really the point.

358
00:48:10,730 --> 00:48:15,380
My point is trying to point to you the estimation method by default.

359
00:48:15,710 --> 00:48:21,710
It is going to estimate theta one and theta two using remo by default.

360
00:48:23,150 --> 00:48:33,410
Do the same thing. But now I explicitly say that I don't want to use Remo so that will default, that it will switch to maximum like estimate.

361
00:48:36,100 --> 00:48:42,430
So here you will get a set of one hat estimate. And it's say to to estimate.

362
00:48:47,610 --> 00:48:50,620
And this is a fatal one. That's a male.

363
00:48:50,970 --> 00:48:57,430
And I say to her. And now. And we will be focusing on this quantity.

364
00:48:59,300 --> 00:49:16,040
Um, which is the residual variance. So it turns out that in in many situations we care about zero one minus two.

365
00:49:16,490 --> 00:49:20,240
Why that's the case. I will delay that explanation.

366
00:49:20,950 --> 00:49:26,840
I suppose our goal is trying to estimate this theta one minus eight two and notice the difference here.

367
00:49:30,010 --> 00:49:39,940
So the one from maximum likelihood expectedly is smaller than the estimate from the REMO.

368
00:49:40,690 --> 00:49:46,030
As we have discussed early on, using that particular simple example.

369
00:49:47,950 --> 00:49:54,520
So here I want to conclude the example and we will return to this kind of model

370
00:49:54,820 --> 00:50:00,790
and this particular kind of data when we are talking about genomics models there.

371
00:50:01,690 --> 00:50:06,280
Hopefully you will find it more satisfying about why we care about this parameter.

372
00:50:06,700 --> 00:50:11,560
So for now, my goal is trying to avoid this discussion for the sake of time.

373
00:50:13,030 --> 00:50:20,560
To summarize, we have found that they are different, but they can be very similar.

374
00:50:20,620 --> 00:50:32,050
The reason for that is we do have a lot of observations 432 And we have only two unknowns in the mean model, right?

375
00:50:32,080 --> 00:50:34,210
We have we have only the intercept and slope.

376
00:50:34,690 --> 00:50:44,049
So when you have a large number of observations, differences do exist, but they do not matter too much in practice.

377
00:50:44,050 --> 00:50:51,870
We still recommend you to use this one. Was this one.

378
00:50:51,900 --> 00:51:05,980
Okay. Summary. So the biases of maximum luxury estimates can occur because the maximum likelihood did not take into account the fact,

379
00:51:06,170 --> 00:51:16,940
taking into account that beta has also been estimated from data and we introduced restricted maximum likelihood and the word restricted

380
00:51:17,000 --> 00:51:30,850
means that we are going to switch to an alternative objective function that only depends on Sigma I and there we have defined a remote.

381
00:51:31,980 --> 00:51:38,220
The consequence is that Raimo will produce less biased estimates in finite samples.

382
00:51:39,000 --> 00:51:42,480
You may be wondering, Janko, what's going to happen with infinite samples?

383
00:51:42,990 --> 00:51:46,710
Well, then, K is super large, right? That ratio is going to be very close to one.

384
00:51:47,830 --> 00:51:51,510
In practice, you don't have a lot of you don't have 1 billion samples.

385
00:51:51,520 --> 00:51:55,050
So even if they're they can be very similar.

386
00:51:55,060 --> 00:52:02,470
I still recommend you to use Remo. And that's the that's the that's the point I made.

387
00:52:03,100 --> 00:52:04,749
Okay. Let's take a five minute break.

388
00:52:04,750 --> 00:52:16,510
And when we come back, I will be covering the material in the technical note here, which, again, some fairwarning.

389
00:52:16,510 --> 00:52:27,760
It's a lot of notation. So if you find yourself tuned out, you can try to rewatch the recording or the more complete recording I sent on Monday.

390
00:52:28,360 --> 00:52:40,870
But my goal in the remaining maybe 20 minutes is trying to go through some of the operations and hopefully this can increase the depth,

391
00:52:41,250 --> 00:52:52,210
technical depth of this class a little bit. And I believe this will be very helpful for you to recall the meaning of Remo and EML.

392
00:52:53,200 --> 00:52:56,470
Okay, I'll see you guys at 4 p.m.

393
00:53:05,340 --> 00:54:37,830
Do you mind if I go to restaurant? Okay, I'll go. Come back. Now it looks like it's.

394
00:56:21,260 --> 00:56:31,860
Sure. It's.

395
00:56:40,400 --> 00:56:45,420
Yes. And totally different so that my people. Correct.

396
00:56:46,170 --> 00:56:49,180
The reason why we still need to continue searching for them.

397
00:56:49,390 --> 00:56:54,160
Where does the High Representative? So this is.

398
00:56:55,010 --> 00:57:02,210
Because we're trying to. In the framework of this much implementation this.

399
00:57:04,950 --> 00:57:08,340
We have to use Tunisia. So that's why I said if you have.

400
00:57:09,810 --> 00:57:17,120
The first time. Still you do.

401
00:57:17,150 --> 00:57:20,870
Nine, six, seven, eight, nine, five.

402
00:57:22,960 --> 00:57:27,940
So. So how does that make sense? Of these people.

403
00:57:29,530 --> 00:57:36,170
What just happened to be as one. Just try to make sure that this is a reasonable situation.

404
00:57:46,660 --> 00:57:52,640
This is just. But remember, here is our first.

405
00:57:59,230 --> 00:58:02,340
Yeah. You don't have to have the same.

406
00:58:04,140 --> 00:58:09,850
And the data you have, whatever I go to to so you can do signal checks.

407
00:58:10,880 --> 00:58:14,750
Yeah. So it doesn't. No.

408
00:58:15,920 --> 00:58:21,470
Just one of great. So. It's an assumption.

409
00:58:38,290 --> 00:58:41,860
All right. So let's get back to work. I think we have 20 minutes.

410
00:58:42,070 --> 00:58:50,470
And actually, personally, time the entire detailed lecture of this entire 40, that took one hour.

411
00:58:50,860 --> 00:58:58,120
So I think my goal is trying to, as I said, give you a general sense of how these operations were done.

412
00:58:58,450 --> 00:59:04,480
And I will try to give you some basic formula for you to at least take notes.

413
00:59:05,710 --> 00:59:12,580
And I would then refer you back to the recording I sent earlier, and that should be pretty detail in slow paced recording.

414
00:59:13,510 --> 00:59:16,570
If you don't know where to find it, post samples or let me know.

415
00:59:16,720 --> 00:59:42,870
Okay. So this notice serves as a supplement to all the formula we talk about.

416
00:59:42,870 --> 00:59:52,139
And remember, I have freely used the claim that beta hat can be reaching out as a function of the Sigma II and regardless of what Sigma is,

417
00:59:52,140 --> 00:59:53,790
that formula is going to be the same.

418
00:59:54,210 --> 01:00:02,580
And I also have talked about, you know, the bios, you know, for the beta hat and where do those claims come from?

419
01:00:03,330 --> 01:00:13,770
So this slide well, sorry, this note will rely on some simplifying settings just for the sake of a simple presentation, which I'll now.

420
01:00:15,790 --> 01:00:25,900
Go through. First, we assume data balanced, which means we have the same number of measurements per person and we have common occasions.

421
01:00:36,670 --> 01:00:45,340
Okay. And we assume that the the the observations within the person can be correlated,

422
01:00:46,270 --> 01:00:50,970
but across subjects, the measurements are entirely independent distribution.

423
01:00:51,230 --> 01:00:57,490
We assume the vector of responses will follow a multivariate Gaussian distribution with the mean

424
01:00:57,490 --> 01:01:02,900
characterized by the excited beta and the variance covariance matrix characterized by sigma.

425
01:01:03,000 --> 01:01:12,430
I here again for simpler presentation, I'm going to assume all these sigma i's are going to be the same.

426
01:01:13,090 --> 01:01:21,820
So our goal will be trying to estimate beta and estimate sigma and uh, and that's how we get started.

427
01:01:22,950 --> 01:01:26,110
Um, so I'm going to write down the goal here.

428
01:01:30,220 --> 01:01:35,150
The goal is to derive. That generalized at least squares.

429
01:01:36,560 --> 01:01:41,700
Which we have written as Peter has. Let me.

430
01:01:43,530 --> 01:01:46,980
Make this more consistent with what we just did.

431
01:01:54,170 --> 01:01:58,190
Okay. And also.

432
01:02:13,880 --> 01:02:17,780
Given the beta, how do we get the sigma estimate number to?

433
01:02:18,620 --> 01:02:30,390
But this is for unstructured. Unstructured. Number two, we will provide.

434
01:02:33,640 --> 01:02:43,300
An alternative. Perspective for deriving Remo.

435
01:02:54,430 --> 01:02:58,540
And we call that approach integration. Approach integration.

436
01:02:59,110 --> 01:03:01,550
Basically same story, right?

437
01:03:01,570 --> 01:03:07,420
We want to restrict our attention to Sigma and we just start with the Orient like hood and take an integrate out of the beta.

438
01:03:07,480 --> 01:03:11,950
That's it. Okay.

439
01:03:12,460 --> 01:03:21,730
So quick review. This is a local function we will need to be working with again, as I have a word in the at the beginning of this class,

440
01:03:22,420 --> 01:03:31,960
these are the normalizing constants and the exponential corresponds to the Gaussian kernel with general variance current structure.

441
01:03:32,290 --> 01:03:37,990
And the product here represents the fact that we can assume data from different people are independent.

442
01:03:38,080 --> 01:03:41,740
Right. And if we take a log, this is what we have.

443
01:03:42,400 --> 01:03:49,660
So this is the objective function for deriving the maximum likelihood estimate.

444
01:03:53,980 --> 01:03:57,730
So we are. Nolans are here, here, here and here.

445
01:04:08,190 --> 01:04:17,520
What's our game plan? Our game plan? It's trying to fix sigma.

446
01:04:18,730 --> 01:04:24,420
Gets the optimal. Later that.

447
01:04:27,290 --> 01:04:32,840
Okay. First step number two, fix beta.

448
01:04:33,770 --> 01:04:36,800
Get the optimal.

449
01:04:38,930 --> 01:04:50,910
Sigma hat. Okay.

450
01:04:51,030 --> 01:04:59,870
Fortunately, when Sigma is unstructured, we do have a closed form for this final term but is not always the case.

451
01:04:59,880 --> 01:05:03,360
So in practice we need to do iterative optimization.

452
01:05:04,830 --> 01:05:15,210
But for the sake of just demonstrating to you, this can be done in a special case, I will assume that Sigma is unstructured.

453
01:05:20,750 --> 01:05:31,490
Which means that we do not put a smaller number of parameters theta to characterize the elements which just let each element in sigma to be unknown.

454
01:05:31,910 --> 01:05:35,300
And we do not place any additional restrictions.

455
01:05:37,280 --> 01:05:47,480
So let's start with one, right? Look, in this objective function, we want to take the derivative with respect to beta.

456
01:05:53,080 --> 01:06:04,720
And we get this equation on the right hand side, and then if you just arrange the terms, this will be what you get in the video.

457
01:06:05,380 --> 01:06:11,680
There will be. Actual derivations of all the.

458
01:06:13,330 --> 01:06:26,570
Matrix operations. I do want to take this opportunity to introduce to you a few formula that you will find very handy.

459
01:06:27,720 --> 01:06:31,050
If not in your entire career, but whenever you're trying to read these things.

460
01:06:32,220 --> 01:06:41,540
So for Formula Formula One. Two, three.

461
01:06:46,170 --> 01:06:53,340
For. These are all about taking taking partial differentials with respect to a vector.

462
01:06:54,150 --> 01:06:58,700
Look, we learned this from your calculus one on one, right?

463
01:06:58,860 --> 01:07:05,360
Or multivariate calculus, you do have to take the route of it, but unfortunately it is not scalar often.

464
01:07:05,460 --> 01:07:07,770
Right. Beta can be a vector here.

465
01:07:08,430 --> 01:07:18,240
So if you take a look at the objective function, there are things like something multiply by beta or beta appearing in the entire quadratic form.

466
01:07:18,900 --> 01:07:24,630
Right. And also later on when we're going to take a derivative with respect to the unknown.

467
01:07:26,090 --> 01:07:30,950
Sigma. Hey, it's a matrix. How do you take the derivative with respect to Matrix?

468
01:07:30,980 --> 01:07:38,930
All right, so what I'm going to do is to tell you the exact formula that can help you remove the roadblock when you're doing these calculations.

469
01:07:40,410 --> 01:07:45,450
I. Don't worry. These are all in the recording, so I'm repeating myself.

470
01:07:45,450 --> 01:07:54,650
Really? So assume a beta is a vector of the mentioned p by one, so beta one to beat up.

471
01:07:54,750 --> 01:08:00,700
Okay, so that's the dimension. First.

472
01:08:07,270 --> 01:08:13,260
Let's consider this one. It's a generic notation, so a means arbitrary matrix.

473
01:08:13,810 --> 01:08:17,110
So A is P by p.

474
01:08:17,140 --> 01:08:21,730
Okay. And this will be two times a beta.

475
01:08:29,670 --> 01:08:34,480
How do you check? This is correct. Here is the list.

476
01:08:35,220 --> 01:08:46,450
The trick that worked for me. So the roads will need to be of the same dimension as the unknown that you take a derivative.

477
01:08:47,840 --> 01:08:52,590
So Peter is up by one. You should expect that out.

478
01:08:52,610 --> 01:08:56,120
The C on the right hand side of the equation will need to have pyros.

479
01:08:56,630 --> 01:09:03,080
If not, you're either talking about gradient, which is different from partial differential.

480
01:09:04,430 --> 01:09:08,300
So you want to make sure that this has pros.

481
01:09:11,160 --> 01:09:14,160
Which clearly is a case where IP is aosp IP.

482
01:09:14,550 --> 01:09:19,470
Anyway, if you don't care about this, just have this lecture Hindi.

483
01:09:21,820 --> 01:09:31,300
Number two. If you take the partial differential of the sigma inverse with respect to Matrix.

484
01:09:32,830 --> 01:09:44,170
Again the same truck. If you take a derivative with respect to a n by m matrix y is in by an IP by p well sigma is representing variance,

485
01:09:44,170 --> 01:09:47,530
covariance of and number of measurements within the person.

486
01:09:48,010 --> 01:09:51,639
So the result will need to be a man.

487
01:09:51,640 --> 01:09:55,750
And actually oh actually you should be taking the variable with respect to the inverse of Sigma.

488
01:09:56,560 --> 01:09:59,980
So the outcome, the result will be sigma super beautiful.

489
01:10:03,160 --> 01:10:10,690
Equation number three is taking partial differential of this one with respect to beta.

490
01:10:17,840 --> 01:10:21,950
And anybody care to make a guess? Hey.

491
01:10:22,050 --> 01:10:30,180
Right. Okay, again, same trick. This is P buy one should you should expect p rose in there and it is payback period.

492
01:10:33,860 --> 01:10:53,150
Equation number four. Is day even more exciting.

493
01:10:53,420 --> 01:11:04,210
So. No.

494
01:11:05,110 --> 01:11:09,550
I said it's more exciting. We should not do this. So actually.

495
01:11:10,660 --> 01:11:21,260
Let's do a. So this will be theta time speed to transpose.

496
01:11:24,610 --> 01:11:30,710
Again. The trick is to make sure that the dimension here matches the result.

497
01:11:31,210 --> 01:11:34,360
So theta is p by one.

498
01:11:37,410 --> 01:11:44,910
Right. This is why if you do the transpose, so this one should be P by p, so as p by p, right.

499
01:11:45,480 --> 01:11:50,790
So the result needs to be having the same dimension, which is P by P and which is a case.

500
01:11:53,320 --> 01:12:00,790
I believe with these weapons that you're being handed to you, I think you can derive.

501
01:12:04,070 --> 01:12:09,230
This one. After all, it is all about taking the word with respect, the better.

502
01:12:09,830 --> 01:12:20,510
And I defer the details to the recording. So here, if we take a step back and again, look at this particular formula,

503
01:12:21,140 --> 01:12:31,450
hopefully now you can see that by equating that to zero rearrange in terms moving terms, this is just a function of sigma.

504
01:12:31,460 --> 01:12:36,860
So I have done something like this if you fix.

505
01:12:38,550 --> 01:12:45,150
Sigma. You get this formula, whatever sigma you have plugged in, you get the estimate.

506
01:12:45,600 --> 01:12:51,300
So this is a step one here. All right.

507
01:12:52,200 --> 01:12:58,319
I want to pause for 30 seconds to see conceptually.

508
01:12:58,320 --> 01:13:10,070
Are there any questions? I don't see it.

509
01:13:10,430 --> 01:13:20,390
I don't see him. So let's just move on. We have another set of things to do, which is to take the derivative with respect to the sigma inverse.

510
01:13:21,290 --> 01:13:31,020
So using the. Formula as I just introduced, you will be able to get this and very easily you can see by creating that zero.

511
01:13:32,320 --> 01:13:37,270
The Sigma hat ought to be this particular form.

512
01:13:37,270 --> 01:13:43,840
Right. Remember these sigma. Are there in school versions of the residual terms.

513
01:13:44,230 --> 01:13:50,620
So it is not surprising that to estimate that you will need to absolutely get the residuals right.

514
01:13:50,650 --> 01:14:02,960
Y minus X are beta. So here we have realized step two, which is to get the estimate whenever you have fixed.

515
01:14:04,530 --> 01:14:07,720
Data. All right.

516
01:14:08,710 --> 01:14:17,560
So fortunately you don't have to iterate this more than one time if the segment is completely unstructured.

517
01:14:18,460 --> 01:14:22,390
But we are not in that beautiful, peaceful world always.

518
01:14:22,390 --> 01:14:26,020
Right? So we have to have some algorithm.

519
01:14:26,350 --> 01:14:34,950
If Sigma is parameterized by, say, sigma theta one theta to write or adjust to parameters, right.

520
01:14:34,990 --> 01:14:44,950
It is not unstructured. You put some structure to it by reducing it to a model that's describable by two parameters.

521
01:14:45,850 --> 01:14:49,570
So using the same logic, we can start with some gas of sigma.

522
01:14:50,380 --> 01:14:54,730
Maybe the gas can be better than the identity.

523
01:14:56,870 --> 01:15:01,820
But you can start with that empty. So.

524
01:15:03,790 --> 01:15:08,800
Using that initial value, we get a better house and then with better, how do you get to Sigma hat?

525
01:15:09,340 --> 01:15:13,210
So by iterating you will be able to get the final result.

526
01:15:15,940 --> 01:15:21,980
Test tests. Can you guys hear the. It's not through the audio system.

527
01:15:22,650 --> 01:15:27,910
Test. Test. I'm not sure what's happening here.

528
01:15:30,660 --> 01:15:39,550
I mean, it's just speaking to this. Test squad.

529
01:15:40,940 --> 01:16:21,830
Just bear with me for a moment. Okay.

530
01:16:21,920 --> 01:16:36,190
So battery. So a comment here is that when you are doing the second step there, right,

531
01:16:36,250 --> 01:16:43,510
we will not not have this beautiful formula there, sigma, how it will be parameterized by theta one, so to say.

532
01:16:43,870 --> 01:16:49,239
And then you will need to use some numerical optimization algorithm to actually get those theta

533
01:16:49,240 --> 01:16:55,390
estimates and iterated many times until the changes in the beta or in the state are negligible.

534
01:17:01,560 --> 01:17:12,750
So there we have completed two steps, which is to demonstrate to you the form of the MLI for better heart, and second a form of the optimal sigma.

535
01:17:12,750 --> 01:17:20,340
How to way is complete unstructured. We have also talked about what to do if the sigma a parameter rise by a small number of parameters.

536
01:17:21,540 --> 01:17:26,580
These are the ML. What are the properties? So under sort of regularity conditions.

537
01:17:26,670 --> 01:17:30,190
I believe you have learned them. You probably an inference class, right.

538
01:17:30,430 --> 01:17:37,710
Theoretically so they are consistent, which means that if you have enough sample size,

539
01:17:38,130 --> 01:17:43,950
the chance you're going to make big errors is going to be very small and that is called consistent.

540
01:17:44,790 --> 01:17:49,380
And we will have this particular formula indicating we do have a symptomatology.

541
01:17:50,580 --> 01:17:57,060
And I want to emphasize here that whenever you see a central mean theorem, whatever here is going to be the.

542
01:17:59,580 --> 01:18:07,030
The what? The inverse. Of the information matrix.

543
01:18:09,310 --> 01:18:12,910
For one person, one subject to.

544
01:18:12,910 --> 01:18:20,010
Okay. So here I am.

545
01:18:20,010 --> 01:18:25,590
Piggyback on the, uh, what you have learned in ignorance class.

546
01:18:25,950 --> 01:18:29,960
So I will not have time to explain the entire theory about it.

547
01:18:31,320 --> 01:18:36,270
But without understanding, I still want to emphasize that here it's for one subject,

548
01:18:36,660 --> 01:18:43,460
and there it should guide you to at least disentangle the meaning of and tall.

549
01:18:43,470 --> 01:18:44,610
So end gamma there.

550
01:18:47,190 --> 01:18:59,730
The bottom line is we do have some technique based on central limit theorem to construct the confidence intervals for the beta in the following slide.

551
01:19:01,980 --> 01:19:07,740
This is basically how do you replace those theoretical variances with estimates?

552
01:19:08,190 --> 01:19:12,780
And again, is pretty straightforward by placing sigma by their estimates.

553
01:19:13,140 --> 01:19:16,910
Here I am showing you the maximum Likert estimates.

554
01:19:18,900 --> 01:19:29,129
Because we were driving the maximum likelihood. I think I'm just going to stop here and for the next lecture,

555
01:19:29,130 --> 01:19:39,000
we're going to start from here and go through them very conceptually and then move on to the next part of the lecture.

556
01:19:40,080 --> 01:19:42,870
With that, thank you, everybody, and have a good day. See you next week.

