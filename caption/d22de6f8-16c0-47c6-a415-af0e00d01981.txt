1
00:00:00,210 --> 00:00:03,930
It's going to be the last night day. I know. I know.

2
00:00:04,230 --> 00:00:18,100
I love it. Let's keep saying it. No way did you go to AT&T and back already or is it next?

3
00:00:18,350 --> 00:00:21,860
It's coming up. Okay. That's right. All right. I thought it was November.

4
00:00:31,700 --> 00:00:40,850
I have a bunch of. Like. Websites we can look at to offer some.

5
00:00:57,250 --> 00:02:36,870
Cool. Want to do a presentation.

6
00:02:57,730 --> 00:03:50,170
I should be able to do that, but I might. I don't know.

7
00:03:50,190 --> 00:03:54,490
They're just taking it. So for your advice.

8
00:03:59,650 --> 00:04:08,110
Correct. We do need at least one person to go out and do.

9
00:04:10,170 --> 00:04:14,330
I just think it's helpful. He's up for something on the 30th of.

10
00:04:18,830 --> 00:05:12,110
Everybody going. We'll do it.

11
00:05:12,590 --> 00:05:18,280
Okay. So. So we'll have quiet on the 30th.

12
00:05:18,280 --> 00:05:23,180
And then do you guys want to go? Okay.

13
00:05:23,450 --> 00:05:48,110
Thank you, everybody. Yeah.

14
00:05:49,100 --> 00:05:56,520
How is everything going with. Person's midterm brings everything to.

15
00:05:58,550 --> 00:06:08,980
Oh, yeah, that's a lot. And I know and I don't know in terms of these readings and stuff, like I don't know if people have read all my records or.

16
00:06:09,620 --> 00:06:16,610
It's kind of interesting. I love that the first time someone when just sort of shocking let's take the lead in terms of.

17
00:06:17,640 --> 00:06:27,000
What worthwhile things people have done before. But, you know, so we can just we can just talk through some research of ethics stuff.

18
00:06:27,180 --> 00:06:30,250
And then I have a lot of resources. Despite the through.

19
00:06:32,480 --> 00:06:36,860
And being, you know, to be like new to the system.

20
00:06:36,860 --> 00:06:43,010
And then everybody's kind of going to be like, maybe I'll be in charge of a recent project like that.

21
00:06:43,130 --> 00:06:47,550
So I think that's like a really different frame for thinking about research ethics.

22
00:06:49,480 --> 00:06:53,130
So we can just kind of go through some of those things and then just.

23
00:06:55,440 --> 00:07:00,089
It's like, okay, okay, cool.

24
00:07:00,090 --> 00:07:06,090
So we can talk about research ethics and just sort of starting with like these research ethics fails is the bombshell.

25
00:07:07,620 --> 00:07:19,229
So I think there are so many there are so many examples of these like awful things that happened and that we know about.

26
00:07:19,230 --> 00:07:28,090
We could probably all rattle off a lot of these like really egregious examples and oh, okay.

27
00:07:28,200 --> 00:07:31,229
Kristy confirms she will be in class next week.

28
00:07:31,230 --> 00:07:36,360
So for me to say thank you because I think she feels really that she forgot the previous one.

29
00:07:36,370 --> 00:07:40,400
So. Great.

30
00:07:41,120 --> 00:07:50,600
So anyway, I just wanted to point out, like, none of these are like that long ago in sort of like collective social time.

31
00:07:51,650 --> 00:07:58,969
And I think we also know how fraught our current political time is.

32
00:07:58,970 --> 00:08:12,420
And we just. Need to keep our ethical like wits about us in terms of protecting people who are not able and power to protect themselves.

33
00:08:12,510 --> 00:08:17,579
That's what I'll say. So I don't know if there are some things from that, like bombshell.

34
00:08:17,580 --> 00:08:26,550
These are just different ones that I pulled out. Like I always think about this one of like they were like, Oh, you know what's going to happen if we,

35
00:08:26,820 --> 00:08:32,130
like, transfer this cancer that the mom has to, you know, their child?

36
00:08:34,200 --> 00:08:44,849
It's a particularly shocking one to me, but I would say this issue of known treatments withheld like the Tuskegee study,

37
00:08:44,850 --> 00:08:53,640
which I'm sure we're all familiar with. That, you know, these kind of things can be going on now in more subtle ways.

38
00:08:54,030 --> 00:09:01,769
And I think a lot of us in research or certainly in like in public health research, are very alerted to these like egregious medical violations.

39
00:09:01,770 --> 00:09:06,660
But, you know, it's still can be a gray area.

40
00:09:06,670 --> 00:09:11,640
So I think it's worth like checking in on these like over the top.

41
00:09:13,170 --> 00:09:19,979
You know, violations and like these sort of I think a lot of things fall under this like bizarre study category.

42
00:09:19,980 --> 00:09:23,610
Like, why, why? What's the value?

43
00:09:23,700 --> 00:09:27,600
And I think we can still ask that question, like, what's the value of these,

44
00:09:28,350 --> 00:09:32,190
you know, strange studies like where just want to know this and like, yeah,

45
00:09:32,190 --> 00:09:41,309
maybe they didn't know x rays were really bad, but at the same time, was there also a different method they could have used to know that information.

46
00:09:41,310 --> 00:09:48,840
And so we can talk more about that. Like how intensive does your method need to be in order to know something?

47
00:09:50,130 --> 00:09:58,140
And I feel like where this can kind of come up for us in like our current moment are things like, you know.

48
00:09:59,550 --> 00:10:07,560
Having access to like DNA information is very relatively easy now, even compared to like five years ago.

49
00:10:07,560 --> 00:10:12,120
It's cheaper to get it. People use it for lots of different reasons.

50
00:10:13,720 --> 00:10:16,090
And yet do we need that?

51
00:10:16,360 --> 00:10:23,049
You know, and I feel like a lot of what comes out of this, like, bizarre study of maybe really cool, maybe cutting edge technology.

52
00:10:23,050 --> 00:10:29,410
And I think that's what a lot of people are tempted to do, things that would be ethically questionable maybe.

53
00:10:29,770 --> 00:10:35,440
And that's where I think some of these examples like kind of come out of this like, oh, there's this new cool tech tool.

54
00:10:35,440 --> 00:10:43,899
We don't totally understand it, but look what it can do. And I think that we can still fall like, you know, victim to that trap in terms of research,

55
00:10:43,900 --> 00:10:47,890
because we're all wanting to do like more novel, like better methods, you know,

56
00:10:48,340 --> 00:10:52,270
more comprehensive measurement of like everything because, you know,

57
00:10:52,270 --> 00:10:57,250
everything's important from like genes to, you know, neighborhood structures to, you know, governments.

58
00:10:58,000 --> 00:10:59,770
But like, should we be doing those things?

59
00:10:59,830 --> 00:11:08,920
So I think that's always like even though these are sort of it's the hopefully they seem like out of range examples for now.

60
00:11:09,490 --> 00:11:15,280
I think that the underlying pieces around like technology and just doing

61
00:11:15,280 --> 00:11:20,980
something because we want to see what happens is still still certainly possible.

62
00:11:22,930 --> 00:11:26,440
And I think the other thing that I like about this article is that like it talks about the

63
00:11:26,440 --> 00:11:33,910
complicit and this complicit nature of like the journal editors and the publishing companies,

64
00:11:35,050 --> 00:11:42,670
because like we sort of talked about, you know, like the other day we're talking about like the arsenic life pre publishing piece.

65
00:11:43,720 --> 00:11:51,640
People were like publishing these studies and they weren't really, you know, having much trouble publishing these studies.

66
00:11:51,640 --> 00:12:00,190
And I think that's a piece around reviewing studies, reviewing research proposals.

67
00:12:00,880 --> 00:12:04,420
Certainly if people get to the level of like editors of journals and that kind of thing,

68
00:12:04,420 --> 00:12:12,129
like that's really another important stopping point for studies that may not be ethical,

69
00:12:12,130 --> 00:12:16,630
that may be ethically questionable or have ethically, ethically questionable components to them.

70
00:12:17,860 --> 00:12:21,760
And so I think that that article is helpful. And, you know, I like that pointed that out.

71
00:12:24,010 --> 00:12:28,600
And so I would just like obviously my bias is like I think it's relevant because.

72
00:12:30,360 --> 00:12:37,560
These things can still happen. I don't know if you guys have seen kind of or could think of current examples of like

73
00:12:38,280 --> 00:12:43,230
ethically questionable studies that hit the news or anything or that you've been,

74
00:12:43,860 --> 00:12:48,270
you know, you're not personally involved with, but you think.

75
00:12:49,730 --> 00:12:53,590
Not sure if that would have passed. These sort of.

76
00:12:54,720 --> 00:12:58,560
You know, these like the studies of our day would be bombshells.

77
00:12:59,540 --> 00:13:02,680
This isn't ethically technically wrong, obviously.

78
00:13:04,910 --> 00:13:07,910
It's still. I was going there and I came across.

79
00:13:09,490 --> 00:13:16,580
I do that. And they do a lot of. DAVIES It's it's a Brill Building Hospital.

80
00:13:16,600 --> 00:13:27,399
And so people from the area come in. And a lot of times I don't think I can afford medical care other hospitals can build in this December.

81
00:13:27,400 --> 00:13:33,430
So what's going on? Doctors can figure out the age and there's a whole kids center for children.

82
00:13:34,270 --> 00:13:41,160
Can you stay on campus? And one time into this art project, we got a chance to talk about it.

83
00:13:43,040 --> 00:13:46,219
And it was one that was like, I'm not a lab rat.

84
00:13:46,220 --> 00:13:48,890
And they do this in the nature and they still put it on the wall.

85
00:13:49,670 --> 00:13:55,249
But I'd always got a little weird because even though the companies were signing up to be in the study,

86
00:13:55,250 --> 00:13:59,570
as you know, they have seen, and they're like big knowledge of what's going on.

87
00:13:59,580 --> 00:14:03,440
And that research was published about it.

88
00:14:03,950 --> 00:14:09,260
It's still like some of these companies can't afford to give you any grants are so desperate and stuff.

89
00:14:09,290 --> 00:14:12,710
And the reason they're standing up so crazy, so. All right.

90
00:14:13,720 --> 00:14:17,890
Yeah, everybody. Are they really?

91
00:14:18,510 --> 00:14:22,850
Yes. Absolutely. I mean, that is a huge issue.

92
00:14:23,090 --> 00:14:30,530
And I think that like for different reasons, like it could be economic, like I can get this kind of higher quality care and I could my.

93
00:14:31,880 --> 00:14:34,040
Oh. And the health providers, if they have those.

94
00:14:34,370 --> 00:14:44,800
And also, just like out of desperation, people have, you know, a child who's ill with a rare thing or find their level of desperation is so high.

95
00:14:44,810 --> 00:14:48,650
And I feel like it puts people in just that like.

96
00:14:49,760 --> 00:14:54,290
It's almost like it puts you in a state of like not being able to consent.

97
00:14:55,350 --> 00:14:59,030
Freely because you're so you're like so overwhelmed by everything.

98
00:14:59,040 --> 00:15:02,220
And I think in those cases, like.

99
00:15:03,180 --> 00:15:11,730
I don't know if you've ever seen like we'll talk about the IAB, but like consent forms that are for like single use or.

100
00:15:12,480 --> 00:15:16,860
I mean, COVID like experimental use and drugs.

101
00:15:17,610 --> 00:15:25,350
And just the length of those forms and the risks are so major on those sort of like rare disease.

102
00:15:26,040 --> 00:15:32,280
Prayer, cancer and a friend. But, you know, as a parent, like potentially dying child like.

103
00:15:33,380 --> 00:15:36,300
They're just kind of saying, you know, and I don't know, you know,

104
00:15:36,380 --> 00:15:42,290
how you get around that because I'm sure, you know, being aware and talking to family is a.

105
00:15:45,160 --> 00:15:48,260
But it's really critical. I mean, I've signed up for it and they have.

106
00:15:50,140 --> 00:15:57,790
I love my father. I wish he was doing the kind of things that you that I think you play right.

107
00:15:57,810 --> 00:16:02,410
Oh, that's great. Flip study is an example, so I'll be interested to hear that.

108
00:16:02,410 --> 00:16:07,750
Don't care how much they may like talking points that they made, but also that yeah.

109
00:16:07,760 --> 00:16:13,190
Like it's like where we study, you know, I think a bunch of like, you know, like the see.

110
00:16:14,010 --> 00:16:21,310
Yeah, yeah, yeah. Um, yeah. I think it's, it's just there's just so many ways and subtleties you figure out when you get people.

111
00:16:21,700 --> 00:16:25,629
And I think obviously we've seen well, I think that's the next slide.

112
00:16:25,630 --> 00:16:35,890
Yeah. Is this issue around like the COVID vaccine rate and sort of, you know, who is signing up and who is participating in research?

113
00:16:36,250 --> 00:16:45,600
Who do we need to work for? Like if we don't have representation in our research, you know, samples, we don't know if it works for everybody.

114
00:16:45,610 --> 00:16:50,080
And it's like we all got this experimental approved vaccine.

115
00:16:50,980 --> 00:16:57,040
And I think also like the irony with the vaccine now, too, is just like, you know,

116
00:16:57,130 --> 00:17:02,650
more people have had this vaccine than and like most other approved vaccine trials ever.

117
00:17:03,670 --> 00:17:10,719
So we kind of have so much more data, but we all did it under this experimental use, you know, approval.

118
00:17:10,720 --> 00:17:16,270
And so I think that it's just been interesting over the past, you know, three years looking at.

119
00:17:18,150 --> 00:17:21,180
To have, you know how.

120
00:17:22,360 --> 00:17:32,190
The discussion of this research ethics issues. I have like necessarily bubbled up into a conversation, but.

121
00:17:33,180 --> 00:17:43,220
I think this certainly is. Critically important because it's not only like a totally appropriate response in terms of trust,

122
00:17:43,230 --> 00:17:46,799
in looking at understanding history, but it's also like,

123
00:17:46,800 --> 00:17:58,440
well, really, if people don't participate, like really we're in trouble because we don't know what's going to work and how to kind of build that back.

124
00:17:59,710 --> 00:18:05,490
There's a challenge. But it's certainly a place where research, ethics and debt debate take.

125
00:18:12,380 --> 00:18:17,660
One of the things. So that's just an example of like clinical trials.

126
00:18:19,010 --> 00:18:25,310
And I think that does also reflect that an age point. Like, who is signing up for clinical trials and why?

127
00:18:25,320 --> 00:18:33,720
Because. That's kind of what we have to offer for people who are really in a situation where there isn't a treatment.

128
00:18:35,060 --> 00:18:41,810
But that desperation. I again, I don't know his ethics things.

129
00:18:41,810 --> 00:18:46,720
I'm like, there's really no answer floating out there, but how do we, like, manage it?

130
00:18:46,830 --> 00:18:55,430
It's kind of like a, you know, conflict of interest. We have to manage it. And so that Belmont Report, which is kind of the classic at this point.

131
00:18:56,870 --> 00:19:05,940
The piece around this is this. This is kind of the same and I for sure like is something research or is it practice and sort of like.

132
00:19:06,900 --> 00:19:08,400
Understanding that distinction.

133
00:19:08,400 --> 00:19:15,480
And so some of the things that we were just talking about, like if you have a rare cancer and there's nothing available to be treating,

134
00:19:15,750 --> 00:19:19,770
you know, available to treat you and you go to an age or other trial,

135
00:19:21,150 --> 00:19:26,820
you're going to be like, yes, I'll do whatever it takes because I want to be in that trial and I want to be in the experimental group of that trial.

136
00:19:26,820 --> 00:19:34,440
That's the other thing that I think about is clinical trials. Like, do people really understand that you could be in a treatment as usual group could

137
00:19:34,440 --> 00:19:39,179
be getting the experimental drug and we don't know about the experimental drugs.

138
00:19:39,180 --> 00:19:42,360
So it just really about informed, informed consent.

139
00:19:42,360 --> 00:19:49,290
Informed part of informed consent is like really hard to achieve, I think sometimes so.

140
00:19:49,290 --> 00:19:54,689
But this is sort of this like the well-being of an individual versus like what's research like generalizable research.

141
00:19:54,690 --> 00:19:59,880
And so that example of like X-raying baby's bladders to see like how much they,

142
00:20:00,370 --> 00:20:06,750
you know, fill up and and then the expunge that was like generalizable knowledge.

143
00:20:08,880 --> 00:20:12,390
And then these should and should be familiar to all of us.

144
00:20:12,390 --> 00:20:23,520
But it's always good to get a refresher. These principles that kind of guide all of our research endeavors is that we have this respect for persons,

145
00:20:23,520 --> 00:20:35,010
both like bodily autonomy and like psychological, who choice autonomy that people can choose to be in the study and they can choose not to.

146
00:20:35,100 --> 00:20:38,899
And that's fine. Even if they've started a study some.

147
00:20:38,900 --> 00:20:44,479
So we've all been in that situation of maybe consenting participants and you think you've like got one,

148
00:20:44,480 --> 00:20:50,510
you're like, Oh, we're getting new participants. And then like actually now that I hear more about it, like, no thanks, it's like,

149
00:20:50,570 --> 00:20:54,860
Oh, we don't get that person's point, but like, that's fine, that's their right.

150
00:20:54,860 --> 00:20:59,450
You know, it's hard to as a recent a researcher recruiter particularly role, it's,

151
00:20:59,450 --> 00:21:04,700
I'm sure all of us have been and it's like, oh, come on, just participate, you know?

152
00:21:05,600 --> 00:21:13,069
But so that's always really good to to check in and as well to kind of talk about are there ways that we can,

153
00:21:13,070 --> 00:21:24,049
like redesign some of our study choices to make it more, um, you know, welcoming and reasonable for somebody to participate in?

154
00:21:24,050 --> 00:21:32,860
Like, do we need to do all that crazy technology? Intensive ness in order to actually learn something generalizable from our study.

155
00:21:34,360 --> 00:21:37,989
And so that sort of comes up, I think a lot very practically.

156
00:21:37,990 --> 00:21:40,600
Like, again, if you guys have been involved in research where you're like,

157
00:21:41,920 --> 00:21:46,840
Oh my gosh, we tried this with three people, the study burden is just like too much.

158
00:21:46,840 --> 00:21:50,400
You can't really do all of this. Or like I, recruitment numbers are really low.

159
00:21:50,410 --> 00:21:55,300
Why is that? So we're always like trying to solve these, like practical recruitment problems.

160
00:21:56,500 --> 00:22:02,170
And so I think some of the ways that we respond to those.

161
00:22:03,370 --> 00:22:08,650
Can be tailored make studies more ethically appealing to the participants.

162
00:22:10,000 --> 00:22:13,870
The other beneficence certainly no harm. We want to maximize benefits.

163
00:22:13,870 --> 00:22:14,379
So again,

164
00:22:14,380 --> 00:22:21,430
like the malpractice case where you're taking an experimental drug that we really don't know what's going on like it's because you're desperate,

165
00:22:21,460 --> 00:22:25,240
because the alternative is worse. And then justice.

166
00:22:25,240 --> 00:22:32,559
So this idea is really important, of course, with that clinical trial example of the COVID vaccine, like who is getting this?

167
00:22:32,560 --> 00:22:38,080
Is everybody getting an opportunity to do this study? And is everybody equally burdened from doing the study?

168
00:22:38,080 --> 00:22:42,280
So like, it's not just like NIH research staff who are contributing in the data.

169
00:22:43,630 --> 00:22:47,200
So those are important. And then talk to already about informed consent.

170
00:22:47,200 --> 00:22:53,979
But like this is kind of how they I think if these are the very real ways that those things play out, it's like,

171
00:22:53,980 --> 00:23:05,559
how do we truly inform someone about the scope of the study, the activities of the study, and what is the risk benefit for that person?

172
00:23:05,560 --> 00:23:11,200
And like that's sort of their individual assessment as well as like a study wide presentation.

173
00:23:11,200 --> 00:23:17,410
Like we think as the study designers, there's little risk and there's benefit, but it's not benefit to you directly.

174
00:23:17,950 --> 00:23:21,820
And then the person has to also think about, is this worth my time?

175
00:23:21,970 --> 00:23:27,760
Do I want to go to U of M and park and like have to pay half of my way through the traffic?

176
00:23:27,760 --> 00:23:31,059
And even if you're going to give me a parking pass, like, is that all worth doing?

177
00:23:31,060 --> 00:23:34,210
Whatever you want me to do. So there's a lot around that.

178
00:23:36,250 --> 00:23:40,150
And then of course, selection and recruitment of participants is kind of like that.

179
00:23:40,720 --> 00:23:45,940
Who gets to do it? And then how do you explain things to the people who are participating?

180
00:23:45,940 --> 00:23:54,370
So they're sort of this like very big picture, I would say aspirational because that's one of my favorite words.

181
00:23:54,370 --> 00:23:58,360
But they shouldn't be aspirational. They are true, they are real.

182
00:23:59,050 --> 00:24:03,970
But this is kind of like how they play out, like on the ground in terms of our research, designing experience.

183
00:24:07,210 --> 00:24:18,220
And then I think just this kind of idea of, you know, there's a lot of in like the philosopher ethics debates.

184
00:24:18,220 --> 00:24:22,390
There are sort of these discussions around, we know some information.

185
00:24:22,390 --> 00:24:32,379
And for me, it's like these these studies, these Harlow Harry Harlow studies where they stressed out baby monkeys and like allowed them to have.

186
00:24:32,380 --> 00:24:37,209
So I don't know if you guys all know this, but it's like the wider mother monkey versus the cloth mother monkey.

187
00:24:37,210 --> 00:24:43,000
And so basically this stressed out, these young primates like, you know,

188
00:24:43,000 --> 00:24:49,299
took them from life and their moms took them away from their peers, just really like created stress for them.

189
00:24:49,300 --> 00:24:56,980
And they allowed then the monkeys to receive comfort either from a wider mother that had food,

190
00:24:57,640 --> 00:25:04,390
that gave it food, or a wire like monkey mother frame that had a like a terrycloth on it basically.

191
00:25:04,960 --> 00:25:10,300
And the little stressed out baby monkeys, the question was like that sort of like a Freudian question,

192
00:25:10,300 --> 00:25:14,530
like, is food or love more important for reducing stress?

193
00:25:15,010 --> 00:25:19,870
And so they tested this in these monkeys repeatedly over the long over a long time.

194
00:25:21,100 --> 00:25:30,340
And the monkeys kept going to the cloth mother, instead of the food dispensing mother, why are mothers like me the one with a real monkey?

195
00:25:30,910 --> 00:25:39,639
And what they've learned from that is that this like soothing function that doesn't involve food is actually what the little

196
00:25:39,640 --> 00:25:44,770
stressed out the monkeys would choose and like was more biologically soothing to the monkeys so that they went took their blood,

197
00:25:45,250 --> 00:25:54,670
cortisol, they did all this stuff to these little stressed out baby monkeys. And it's like now we would not allow those experiments to kind of go for.

198
00:25:55,780 --> 00:26:01,570
And at the same time, like we in the sort of early life stress field.

199
00:26:02,770 --> 00:26:05,980
Have learned so much from those monkey experiments.

200
00:26:07,370 --> 00:26:15,680
And in terms of the like, we've learned sort of like what happens sort of naturalistically in stress, like a generalizable research question.

201
00:26:16,040 --> 00:26:18,379
But we also learn like what soothes stress,

202
00:26:18,380 --> 00:26:25,970
like what brings down a little baby monkey's cortisol or therefore like a little baby infant's cortisol who's had early life stress.

203
00:26:26,660 --> 00:26:34,520
And so we've learned things like, you know, you need sort of a supportive, soothing presence,

204
00:26:34,520 --> 00:26:40,010
even if it doesn't involve food to kind of suit the little baby monkeys, the little baby infant.

205
00:26:40,550 --> 00:26:44,510
And so I always think about those studies when I think about research ethics because I'm like,

206
00:26:45,530 --> 00:26:50,560
we know a lot of like we're building on that knowledge, like at some really basic level.

207
00:26:50,570 --> 00:26:55,520
But those studies were pretty unethical to the baby monkeys.

208
00:26:57,860 --> 00:27:03,340
So that's when I particularly struggle with a little bit and you know, we still do mouse studies and, you know,

209
00:27:03,350 --> 00:27:13,280
I think now there, but because of animal research controls, which we'll talk about more, more ethically conducted.

210
00:27:13,520 --> 00:27:18,890
But it's still like do I think there's another question like do we need to know more?

211
00:27:19,220 --> 00:27:22,460
Like, do we know enough about how bad stress is for the body? Right.

212
00:27:22,520 --> 00:27:27,950
Like, do we need more studies of that? And I think that's kind of a bigger question, almost, because like.

213
00:27:28,900 --> 00:27:32,290
When do we stop our generalized knowledge?

214
00:27:32,290 --> 00:27:35,860
Right. Like, can we be like, okay, we're done. We know this.

215
00:27:36,340 --> 00:27:44,590
Moving on. And then another one of this is this. I don't know if you guys have heard about these, but like, of course, the you know.

216
00:27:45,380 --> 00:27:52,970
World War Two Nazis were like the ultimate two evil right figures and did these experiments where they like froze people.

217
00:27:52,970 --> 00:27:56,960
And so see how basically how long it took them to freeze to death.

218
00:27:57,380 --> 00:28:08,120
And so some of that information is used in terms of like airplane design and, you know, life jacket.

219
00:28:09,450 --> 00:28:10,919
Design and that kind of thing.

220
00:28:10,920 --> 00:28:20,370
So like, you know, it informs practice, but then it, you know, it's not it's the least the knowledge is out there, but it was obtained very ethically.

221
00:28:20,460 --> 00:28:23,670
So I think it's like this kind of quandary.

222
00:28:25,560 --> 00:28:29,480
But. I don't know if others have thought.

223
00:28:29,480 --> 00:28:33,050
Oh, and then this Jones one is this this question of like,

224
00:28:34,820 --> 00:28:42,800
everybody thinks they're a good person and everything's sort of like a really ethical researcher and.

225
00:28:44,290 --> 00:28:49,820
I think that that, you know, we've seen like what's acceptable changes over time.

226
00:28:49,840 --> 00:28:55,959
We've certainly seen like recent changes even around like a really concrete example.

227
00:28:55,960 --> 00:28:59,380
Like how many options for gender do you have on there?

228
00:28:59,380 --> 00:29:10,090
Like demographic for, you know, like I would say five years ago to like in 99.999999% of research.

229
00:29:10,750 --> 00:29:19,380
And I think now I think in five years. I just I think that will be, like, shockingly wrong.

230
00:29:20,470 --> 00:29:26,620
Which is very interesting because then you have like ten millions, you know, longitudinal cohort studies to dissect the two options.

231
00:29:26,680 --> 00:29:33,850
So I think that's an interesting example of like a kind of a rapid shift in response to kind of societal shifts.

232
00:29:34,510 --> 00:29:38,320
That has very specific implications for research studies.

233
00:29:41,750 --> 00:29:47,030
You know, and what else? And I think that animals as sentient beings, we can talk more about that.

234
00:29:47,030 --> 00:29:51,770
But like there's definitely again through research and this is the tricky thing,

235
00:29:51,780 --> 00:29:57,170
like as we learn more by conducting sometimes research with and on animals,

236
00:29:57,650 --> 00:30:04,670
we learn more about, oh, animals are are more sentient and aware than we thought.

237
00:30:05,780 --> 00:30:08,469
Now we need to stop doing this research without, you know,

238
00:30:08,470 --> 00:30:15,530
or like we need to really think about how we ethically conduct research with them and why we are doing that in these search.

239
00:30:17,390 --> 00:30:22,040
So I don't know if people have certain thoughts on that but piece and we'll talk more about that.

240
00:30:24,070 --> 00:30:26,550
Anything around this because I think that comes up a lot.

241
00:30:26,580 --> 00:30:35,460
We just you know, it's it's kind of also like knowing history, you know, which is it's hard to know everything, of course, because we're we're I.

242
00:30:37,260 --> 00:30:40,770
The Mental Health Institute. Mm hmm.

243
00:30:41,190 --> 00:30:44,910
And they would talk about them like the.

244
00:30:46,090 --> 00:30:51,520
None of these things if you just put your carry on. So it's a weird.

245
00:30:53,850 --> 00:31:00,970
And I guess they didn't. It seemed like the people in charge of the project.

246
00:31:04,460 --> 00:31:08,770
Time to care. I know. There was a time where there's a snowstorm.

247
00:31:10,480 --> 00:31:21,240
Oh, yeah. You still. And dogs, too.

248
00:31:21,540 --> 00:31:26,520
They discovered. The dogs sometimes. And I know that when they've done that, they done.

249
00:31:30,170 --> 00:31:33,350
But if it does, it's very difficult.

250
00:31:34,580 --> 00:31:38,580
Yeah, yeah, yeah, yeah. And then.

251
00:31:38,970 --> 00:31:44,670
And that's about right. Which is like the tip top of research protocol.

252
00:31:45,490 --> 00:31:48,550
Awareness one. Right.

253
00:31:48,670 --> 00:31:56,860
Because I, I sort of a friend in town here is like a bio biotech person working on like rare cancers.

254
00:31:57,810 --> 00:32:01,799
And so for very, very noble, important crimes.

255
00:32:01,800 --> 00:32:08,330
But they do it in clinical trials a lot of like not every single moment of them, but at some point they get to dogs.

256
00:32:08,340 --> 00:32:15,360
And so listen and I think they do it ethically, but it's like it's so like boring in like a biotech business.

257
00:32:16,410 --> 00:32:24,270
And so I, you know, they have to similarly, like, be very regulated in terms of how they have to use those animals.

258
00:32:24,270 --> 00:32:29,330
But. It's a hard one, I think, because.

259
00:32:30,500 --> 00:32:37,610
What is the ultimate perspective on I could you get there under way and so that's what we've talked about a lot of like we can do

260
00:32:37,610 --> 00:32:47,599
this much before we need to do that no in dollars do you know the mechanism or whatever it is and if you talk to people here,

261
00:32:47,600 --> 00:32:57,080
so there's like folks in DHS, Environmental Health Sciences who do a lot of work on like epigenetics and use mouse models.

262
00:32:58,180 --> 00:33:04,299
And so I always tell a story which is like a good story about like ethical review animal studies.

263
00:33:04,300 --> 00:33:14,170
But Dana, Illinois does work on epigenetics and sort of environmental exposures, complicity in like environmental justice and epigenetics.

264
00:33:14,230 --> 00:33:17,920
So it like it brings like it's very awesome work.

265
00:33:19,360 --> 00:33:23,890
But one of the studies they were running, they were going to feed mice.

266
00:33:24,140 --> 00:33:31,290
It's like a Western diet and then a mouse diet that was more of a mediterranean diet,

267
00:33:31,300 --> 00:33:39,850
like they were going to basically make mouse pellets to feed the mice that had like the same distribution of white fat and fiber and things.

268
00:33:39,850 --> 00:33:45,049
That's like the Western versus. And so the animal IRP basically was like,

269
00:33:45,050 --> 00:33:50,920
you cannot feed the mice this because it's like unethical because it's such a bad diet with the Western diet codes.

270
00:33:51,950 --> 00:33:55,879
And I always think about that story because I'm like, okay, they caught this regulation.

271
00:33:55,880 --> 00:33:59,080
But then I'm like, My gosh, like we're all keeping this diet.

272
00:33:59,090 --> 00:34:05,780
You know, it's not applicable to the lab of mice, like, oh, man.

273
00:34:06,980 --> 00:34:14,690
So, you know, and I think that that. I mean, some sometimes I know they do have to, you know, kill the mice because they look at like,

274
00:34:15,500 --> 00:34:21,260
do your environmental toxins interact with your diet and impact like your brain?

275
00:34:21,290 --> 00:34:26,150
But I think the only way to really answer that is to kill the mice at this juncture.

276
00:34:26,540 --> 00:34:31,940
Like, I can't I don't think they can image the mice or maybe we can get to that point,

277
00:34:31,950 --> 00:34:37,519
have like a little mouse and MRI and like have them wear something, you know, so we didn't have to kill.

278
00:34:37,520 --> 00:34:43,309
But, you know, like you could imagine that ultimately getting to that point where like you put a little electrode,

279
00:34:43,310 --> 00:34:50,090
you know, and I do they do put electrodes in mice. But at some point, you know, they don't come down here.

280
00:34:51,290 --> 00:34:55,070
And I think they can do a lot with artificial intelligence.

281
00:34:57,710 --> 00:35:01,490
Not be able to do more. Somebody was talking about.

282
00:35:04,630 --> 00:35:13,370
And then the phone. Yeah.

283
00:35:13,440 --> 00:35:17,010
That whole clone. Should I have been?

284
00:35:18,140 --> 00:35:21,530
So I don't know. But, yeah, that is like a major.

285
00:35:23,780 --> 00:35:30,340
And actually, you know, we'll talk about, I think maybe on our team Science Week we'll talk about.

286
00:35:32,610 --> 00:35:35,920
The international collaboration and that kind of stuff. But I mean.

287
00:35:36,990 --> 00:35:46,260
Practices and policies and ethical policy and monitoring varies quite a bit across different settings and country and like if you're a business,

288
00:35:46,260 --> 00:35:49,860
if you're in university and so. You know.

289
00:35:51,790 --> 00:35:56,980
It's it's kind of that thing of, you know, with technology that bang, bang, people are going to do it.

290
00:35:58,210 --> 00:36:06,400
But it's very. The other thing I think is fascinating.

291
00:36:08,520 --> 00:36:18,390
Like really disturbing is that you know what the CRISPR technology like the new relatively new like genetic medical alteration technology.

292
00:36:19,170 --> 00:36:23,909
I guess if you're and I'm not a lab scientist, but like I guess if you are laughing science,

293
00:36:23,910 --> 00:36:28,170
it's like the actual physical mechanics of it is not that difficult to do.

294
00:36:29,610 --> 00:36:33,689
And so they're the people who, like, tried that they don't want themselves.

295
00:36:33,690 --> 00:36:37,020
There's like a few like really out there, like, genetically.

296
00:36:37,850 --> 00:36:45,139
Alteration. People think one person died and another person like did something, but it just didn't work.

297
00:36:45,140 --> 00:36:51,140
And he was trying to make himself have like bigger muscles or something, you know, you're just like, Oh my God, where we had ten people.

298
00:36:51,860 --> 00:36:57,170
So yeah, I think it's like that, you know, but that's 22 of technology.

299
00:36:57,500 --> 00:37:00,620
Yeah. I'm like, back to that baby bladder X-ray and study like.

300
00:37:01,610 --> 00:37:03,170
We can do a lot of things.

301
00:37:03,380 --> 00:37:12,170
And like sometimes the way that we get to maybe like scanning little mice with animal eyes on their heads is the trial and error of,

302
00:37:12,530 --> 00:37:16,200
you know, trying out these technologies to see what they can do.

303
00:37:16,220 --> 00:37:19,530
But in some. It was not about.

304
00:37:24,820 --> 00:37:31,330
Oh, yeah, yeah, yeah, yeah. Oh, oh, yeah.

305
00:37:31,390 --> 00:37:34,780
You know what? I should put her in there. I don't.

306
00:37:34,780 --> 00:37:38,709
Even 81. Yeah, she was like an undergrad.

307
00:37:38,710 --> 00:37:45,160
Yeah, right. Yeah. And they get so far, I think at one point they were using it all.

308
00:37:47,880 --> 00:37:53,260
Yeah. That's a great idea. I think we should just watch this one through.

309
00:37:54,110 --> 00:37:57,220
I read the book that the book is called Bad Blood.

310
00:37:57,230 --> 00:38:02,150
I think the documentary was maybe also bad, but it's her name, Elizabeth Holmes.

311
00:38:02,330 --> 00:38:08,150
That's right. Theranos. Yeah. Yeah. And so they were yeah.

312
00:38:08,180 --> 00:38:17,479
They were like, I've been thinking about this, like, competing interests and and and conflict of interest and talk about competing interests.

313
00:38:17,480 --> 00:38:22,190
And in the business context, they had gotten all this venture capital.

314
00:38:22,910 --> 00:38:33,649
They were like under major time an investor, stockbroker, stock owner, pressure to make this product work because she pitched it.

315
00:38:33,650 --> 00:38:37,280
It made sense. It was like logical.

316
00:38:37,280 --> 00:38:40,790
All the it would have had a huge scalable impact if it were true.

317
00:38:41,420 --> 00:38:46,970
And they kept like trying and trying and trying and like there were just some fundamental flaws in their research design.

318
00:38:47,780 --> 00:38:51,880
Didn't work. And so they started thinking Dana and Mike.

319
00:38:52,390 --> 00:38:55,780
Yeah, like you said, we put it in the box and like, put the samples in a box.

320
00:38:55,780 --> 00:38:59,929
Had like a like a little kid, like a magic, so to speak, with.

321
00:38:59,930 --> 00:39:03,500
They're like, here are the boxes that go make these beeping noises, and then you get your sample.

322
00:39:03,850 --> 00:39:10,900
But they were testing the samples the old fashioned way that we know works that is much less fancy and more expensive and slower.

323
00:39:11,440 --> 00:39:17,120
And then they like were like, here's a fancy product that worked for, like the magic box.

324
00:39:17,150 --> 00:39:24,760
But then it all kind of fell apart, I think also because like not only not that's not really as much ethics changing over time,

325
00:39:24,760 --> 00:39:33,640
but it was like the whistle blower thing. So like the people who worked for the company eventually were just like, we can't take this anymore.

326
00:39:33,940 --> 00:39:37,179
And I think it is true also this idea of not consensus.

327
00:39:37,180 --> 00:39:44,590
So like is the financial gain of the stockholders and the venture capitalists that were invested in that company.

328
00:39:46,150 --> 00:39:51,610
One could, you know, reasonable people could disagree maybe about like how far.

329
00:39:52,900 --> 00:39:57,670
You know, to kind of string them along with the hopes that your product is going to work.

330
00:39:58,180 --> 00:40:07,250
What your research products project is going to turn out versus like, oh, we, you know, it's like this like hypothesis testing.

331
00:40:07,270 --> 00:40:11,230
You know, it's like, oh, you know what? We found evidence. Our hypothesis did not work.

332
00:40:11,890 --> 00:40:16,750
And like, in that case, it was a very strong evidence, but they weren't really interested in testing the hypothesis.

333
00:40:16,750 --> 00:40:23,560
They were interested in like having their preferred hypothesis make a lot of money.

334
00:40:24,220 --> 00:40:28,570
And so that's like the ultimate, you know, conflict of interest.

335
00:40:28,570 --> 00:40:33,100
But it really is a great example of all of that.

336
00:40:34,060 --> 00:40:38,920
It's just like watch that movie because it's not really Shakespeare.

337
00:40:39,670 --> 00:40:43,290
That's right. That's totally right.

338
00:40:44,140 --> 00:40:49,510
Oh, my gosh. I can't you know, it's so funny. I was like so fascinated by it was that pre-COVID that that all happened.

339
00:40:51,820 --> 00:40:55,570
If the tribe is still kind of in process, right? Yeah.

340
00:40:55,590 --> 00:41:02,010
She also had like the whole Steve Jobs kind of thing where she wanted to be, you know, and I think, you know, I think there's misogyny.

341
00:41:02,010 --> 00:41:07,260
I think there's like a lot of things going on there. Sort of misogyny slash.

342
00:41:09,650 --> 00:41:14,840
Just weird sexism things. And like her investors are like, ooh, you're this pretty young girl.

343
00:41:14,990 --> 00:41:18,650
Whoa, whoa, whoa. Creepy things of that.

344
00:41:19,520 --> 00:41:23,180
But yeah, but really, I mean, it's just a great example in this.

345
00:41:24,820 --> 00:41:32,150
They were researchers like that for. This is something we have to look at the same.

346
00:41:34,370 --> 00:41:46,110
Do the math. And I think we had I think I put this article in like an earlier like a conflict because it's like fraud,

347
00:41:46,320 --> 00:41:50,309
you know, research, misconduct, fraud, conflict of interest, research ethics.

348
00:41:50,310 --> 00:41:56,340
Like, they're obviously all interconnected. I think in this class they've ended up kind of for various reasons on different weeks, but.

349
00:41:57,810 --> 00:42:03,720
I think there's like subtleties to each kind of part of each one, so it's good to talk about it over time.

350
00:42:04,560 --> 00:42:10,710
But that that article by Tim Caulfield about like the commercialization of university research.

351
00:42:11,750 --> 00:42:19,129
That's something, and I think I did find some slides on this, but that's something that is really you know,

352
00:42:19,130 --> 00:42:28,010
it may or may not affect any of our research personally, but certainly at a university level that happens all the time.

353
00:42:28,010 --> 00:42:35,479
And like maybe less than that, like a lot in engineering and school of information and you know, we might collaborate with these people.

354
00:42:35,480 --> 00:42:39,559
So there's there's a lot to think about around that.

355
00:42:39,560 --> 00:42:46,400
And like these pressures just, you know, you know, your study, you're like, well,

356
00:42:47,630 --> 00:42:53,810
we had a study that showed the intervention worked, but like you also know, like, well, did it have any foibles?

357
00:42:53,810 --> 00:42:57,230
Was it perfect? We don't have five studies showing the intervention work.

358
00:42:57,230 --> 00:43:03,230
But then you have like some investor who was like, I want to do this intervention and I'm going to make it the next big thing.

359
00:43:03,230 --> 00:43:11,330
And like maybe that investor to put it like in very bad terms, like maybe it was like an exercise intervention and you know,

360
00:43:11,330 --> 00:43:14,899
you got really good impact because of some sort of manipulation.

361
00:43:14,900 --> 00:43:19,310
You did like some new way to kind of connect people or activate them or whatever it was.

362
00:43:20,330 --> 00:43:26,090
And then like the why comes and they're like, we want to roll it out nationwide, you know,

363
00:43:26,090 --> 00:43:32,810
like, you know, why anointed, you know, cardiovascular health program, for example.

364
00:43:33,410 --> 00:43:40,310
And you know, knowing everything we know about like, oh, as interventions get, you know,

365
00:43:40,580 --> 00:43:49,700
expanded and away from the original, you know, demonstration project, they can sort of lose fidelity.

366
00:43:49,700 --> 00:43:54,530
Like, how do we think about that translation of research, which we really want to do,

367
00:43:54,590 --> 00:43:59,000
as we talked about last week, but like what's responsible and what sort of ethic?

368
00:43:59,680 --> 00:44:05,470
After that. So I think it's very it's a very real issue.

369
00:44:05,650 --> 00:44:10,000
And that's kind of like kind of like, again, last week, we want our stuff to get out there.

370
00:44:10,000 --> 00:44:16,450
But like when, when together, you know, we don't want to hang on to it too long because then it takes 17 years.

371
00:44:17,080 --> 00:44:24,960
But, you know, managing the pressures to get it out before it's ready for prime time, I think is a research ethics.

372
00:44:24,970 --> 00:44:29,790
Like a little bit of a quandary. So yeah, that's a great example.

373
00:44:29,940 --> 00:44:33,180
I'll totally out of there that they're in a spirit of service.

374
00:44:33,420 --> 00:44:38,350
This is my fantasy is not like the God of Haiti's or something, you know?

375
00:44:38,370 --> 00:44:42,760
Maybe, maybe not. We had an old.

376
00:44:43,450 --> 00:44:46,529
Yeah, yeah. Oh, yeah, yeah, yeah, that's right.

377
00:44:46,530 --> 00:44:50,220
Snapping. Is that what it is? It snaps at the rain scene. Oh, my gosh.

378
00:44:50,550 --> 00:44:53,790
During COVID, I'd never had watched a horrible movie.

379
00:44:53,790 --> 00:45:04,000
Like, literally, I think I saw Wonder Woman maybe was Black Panther, but like during COVID, my kids watched all this, like, chronological order.

380
00:45:04,170 --> 00:45:09,490
Like, here's the order we need to watch them. And I was like, We've got to do it in the order like that.

381
00:45:09,510 --> 00:45:14,430
They happen, you know, it's not the order of things. And then she was like, We've got to make our way through.

382
00:45:14,430 --> 00:45:20,310
Like, I don't know, some like, Thor movie that was, you know, she's like, it's not very good, but like, you have to do it because you learn things.

383
00:45:21,010 --> 00:45:27,299
So I was like, I am opting out of that. And America was like, What do you think?

384
00:45:27,300 --> 00:45:34,780
We thought we were going to do it in the TV show, but Spider-Man and Walt Disney was like, Oh my gosh, yeah.

385
00:45:34,830 --> 00:45:41,100
Oh, it's so funny. Yeah, the Marvel Universe, it's and actually the next Black Panther is coming out with a sister, so I'll see that.

386
00:45:42,330 --> 00:45:47,430
Oh, yeah. Okay. Digression. Yeah, yeah, yeah. Oh, yeah.

387
00:45:48,330 --> 00:45:55,890
So perspective was I was thinking a lot of things about her and family research and how like, it's unimaginable.

388
00:45:56,110 --> 00:46:00,800
But these debates about ethics. I like, for example, my an undergrad.

389
00:46:01,690 --> 00:46:05,670
A lot of research and. American Senator Conrad.

390
00:46:10,590 --> 00:46:22,410
A lot of training to get there. Someone tried to tell me there was something I had to call my boss and my lawyer threw.

391
00:46:28,740 --> 00:46:32,990
But now I'm curious, like where on this day we've been primarily.

392
00:46:37,850 --> 00:46:39,740
People can have these conversations.

393
00:46:41,520 --> 00:46:50,830
On Science Friday, or maybe that in terms of that balancing light, you may have a race or just a different experiment.

394
00:46:52,610 --> 00:46:56,580
You know, for sort of like opening the body and. My.

395
00:47:02,800 --> 00:47:06,270
I don't know. Question.

396
00:47:06,750 --> 00:47:10,570
So there are. I don't.

397
00:47:12,790 --> 00:47:16,770
Like women and I. There is a separate.

398
00:47:19,060 --> 00:47:28,140
On this slide, for example. Like there's a healthy research which is applicable to all social implications of research.

399
00:47:28,800 --> 00:47:33,150
I think. This is actually a good thing, I think that I've seen.

400
00:47:33,750 --> 00:47:39,360
It's a very. I just want to see.

401
00:47:42,430 --> 00:47:46,220
You know, like the genetic research, because that is where going to be most.

402
00:47:47,220 --> 00:47:51,220
First. The clock at.

403
00:47:53,250 --> 00:48:02,090
His daughter is friend. Anyway, I thought it was I kind of looked at my family and that's this whole film and I.

404
00:48:03,420 --> 00:48:10,350
The future for 25 years. But the whole point of it was like, Oh, we're going to get really so good at N.A.,

405
00:48:10,350 --> 00:48:13,860
N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A.

406
00:48:16,010 --> 00:48:19,770
It's accessible both on the screen and the main folders.

407
00:48:22,060 --> 00:48:29,400
Perhaps the year that so. I think that's what makes the genetics and.

408
00:48:31,080 --> 00:48:34,120
But that's. And it's. It certainly sort of.

409
00:48:36,310 --> 00:48:42,110
The system and animal researchers who have been in the towns like the.

410
00:48:43,420 --> 00:48:47,740
It's a policy that happens often have a research.

411
00:48:49,410 --> 00:48:52,550
So I don't know the. Specific.

412
00:48:53,990 --> 00:48:58,550
But you're the one that's very. Dedicated to bioethics.

413
00:48:58,570 --> 00:49:02,560
The animal research discussion. And I just don't.

414
00:49:04,720 --> 00:49:07,800
But a decent salary is the right thing.

415
00:49:08,810 --> 00:49:14,050
And it's kind of the academic, I think. Unless you're hanging out with the clients.

416
00:49:14,190 --> 00:49:22,290
This is a whole. You'll find it here and.

417
00:49:28,380 --> 00:49:34,560
The center in Branson which rides Fisher is part of.

418
00:49:35,360 --> 00:49:38,390
Scott is also part of that.

419
00:49:43,020 --> 00:49:47,660
Consultations I really want to have. An interesting question.

420
00:49:47,690 --> 00:49:51,489
How much of what people work they can talk about?

421
00:49:51,490 --> 00:49:55,450
And that center is very social science and medicine.

422
00:49:58,460 --> 00:50:02,330
I know that you have worked on the foundation for what, about seven something?

423
00:50:03,770 --> 00:50:07,080
I am on that list of their talks in the draft.

424
00:50:08,830 --> 00:50:20,940
But I don't want to gamble. There. I mean, I think one of the points, too, about it being reactive is that changing for time.

425
00:50:22,170 --> 00:50:26,920
That true has. That comes out right? Some nation has grown.

426
00:50:27,760 --> 00:50:38,840
This level of animal cruelty, this complexity of animal life, that there's a flurry of things on Science Friday or here and then legislation that.

427
00:50:40,090 --> 00:50:45,180
It's hard to project when you get a man like that.

428
00:50:45,190 --> 00:50:49,050
But imagine 25 years ago. Altering genetics.

429
00:50:51,900 --> 00:50:55,330
I think the other thing, you know, in terms of the genetics and family planning.

430
00:50:56,420 --> 00:51:00,610
Government has so much to do with anything in life. It's hard to.

431
00:51:04,230 --> 00:51:07,260
And legislate something that's very hypothetical.

432
00:51:08,260 --> 00:51:15,040
You know, but it's like knowing at the same time the brought process that if technology.

433
00:51:16,210 --> 00:51:21,190
That hasn't been tried because it's sort of like the sort of legislation and policy.

434
00:51:23,500 --> 00:51:29,240
Feels like you could be out and about. But at the same time. You know.

435
00:51:29,570 --> 00:51:33,470
So it's really true. Fascinating.

436
00:51:33,490 --> 00:51:41,180
And I think that the bioethicist, like Zeke Emanuel, is one of the people who writes a lot about this,

437
00:51:41,180 --> 00:51:47,060
and they're often like pulling up these ethical quandaries,

438
00:51:48,080 --> 00:51:52,940
and then they're kind of handing it off to like policy regulation bodies like the IAB

439
00:51:52,940 --> 00:51:57,740
that we'll talk about to kind of implement these things and the IAB because it's.

440
00:51:59,230 --> 00:52:03,540
You know, a slower than being body. And it's got to go through somebody's level.

441
00:52:03,550 --> 00:52:08,560
It's like easier for a bioethicist in a way to, like, throw out the discussion points,

442
00:52:09,100 --> 00:52:15,760
but then the people have to really regulate it, are like hampered in a good way at some level by like all these levels of regulation.

443
00:52:15,970 --> 00:52:18,670
Right? So there's that kind of interesting disconnect.

444
00:52:22,090 --> 00:52:28,630
Let's see, let's thinking about, oh, I like this slide because so this is sort of thinking about like applying,

445
00:52:28,870 --> 00:52:32,139
applying this thinking and research ethics challenges to our own research.

446
00:52:32,140 --> 00:52:35,200
So this is like it says, different perceptions of value.

447
00:52:36,070 --> 00:52:40,330
And so we're saying like, oh, here's $50 for join in our pediatric research study.

448
00:52:40,330 --> 00:52:45,520
Healthy kids want it and this one's like $50. Wouldn't even pay my original weekly allowance.

449
00:52:45,520 --> 00:52:51,460
And then this is like the same study. And then this woman who had like four kids, it's like, honey, for $50, you can have one of these kids.

450
00:52:51,490 --> 00:53:00,820
So it's like what is like the, you know, inducement, right, for participating in research or compensation?

451
00:53:01,210 --> 00:53:04,630
And how does that vary depending on the type of study?

452
00:53:05,020 --> 00:53:11,590
And I think this is what I was just talking about to like when does something work well enough and should be disseminated?

453
00:53:12,940 --> 00:53:21,820
And then this challenges to informed consent is really like this. So if we're talking about doing intervention research with vulnerable populations,

454
00:53:24,580 --> 00:53:30,129
how do we kind of recruit them for research in an ethical way that like follows,

455
00:53:30,130 --> 00:53:35,410
that all those principles of beneficence and justice and respect for persons?

456
00:53:36,430 --> 00:53:40,780
And then how do we manage, like some of the challenges to informed consent?

457
00:53:40,780 --> 00:53:43,960
Like if people are vulnerable because they're desperate because their child's ill,

458
00:53:44,620 --> 00:53:50,349
or if people are vulnerable because they like really need the $50 in a way that like this woman doesn't need the $50.

459
00:53:50,350 --> 00:53:59,600
Like $50 means different things to different people. And then I think there's some specific stuff that we need to think about in terms of,

460
00:54:01,190 --> 00:54:05,329
you know, protections for research participants that we've talked already about,

461
00:54:05,330 --> 00:54:07,610
like people who are, you know,

462
00:54:07,880 --> 00:54:14,450
lacking documentation and how that could get them in trouble if they're talking to researchers who would like to pay them for their time.

463
00:54:15,410 --> 00:54:21,080
So that's like a whole issue that I think we grapple with and HPD and like regulations

464
00:54:21,080 --> 00:54:25,010
haven't like caught up with our needs and those are in that area specifically.

465
00:54:26,990 --> 00:54:29,840
And then this is like this undue influence is like that.

466
00:54:32,090 --> 00:54:43,969
So I don't know if people have had, you know, again, experiences around those, but we've discussed this issue a lot is under influence.

467
00:54:43,970 --> 00:54:47,630
Just working with, you know, parents of kids who are in Head Start.

468
00:54:48,170 --> 00:54:51,350
That's like a population that I've worked with a lot and then just sort of saying like,

469
00:54:51,920 --> 00:54:55,220
I would really like to compensate you for your time because like your participants

470
00:54:55,340 --> 00:54:59,540
participation is super valuable because we don't have a lot of data from,

471
00:54:59,540 --> 00:55:08,029
you know, your, you know, kids and yet, you know, I don't want to like coerce people to participate in a research study.

472
00:55:08,030 --> 00:55:15,690
But I, you know, their time is very valuable. So like, how do you, you know, kind of like pitch in at the right level.

473
00:55:17,850 --> 00:55:23,040
That's tough because I also feel like sometimes people devalue, like what their time is worth.

474
00:55:23,910 --> 00:55:33,930
So yeah, it's a really tough line to walk or that I know that $25 that's too much that they're giving us so many hours of their time.

475
00:55:33,930 --> 00:55:37,240
Like, yeah, too much. Like, too much work, you know?

476
00:55:37,680 --> 00:55:39,480
Totally. And if somebody is, yeah.

477
00:55:39,490 --> 00:55:46,020
Like, really passionate about an issue and, like, wants to be about they feel like they're doing something by talking to you,

478
00:55:46,020 --> 00:55:51,509
which they are killing, saying like, Oh, don't you know, don't compensate me.

479
00:55:51,510 --> 00:55:57,390
And I was like, not going to happen to you, but glad that you wanted to do this.

480
00:55:58,530 --> 00:56:03,969
But, yeah. And of course, like financial constraints of the study and everything, you know,

481
00:56:03,970 --> 00:56:08,530
someone's a graduate student and how $50, $50 would be able to compensate people.

482
00:56:09,160 --> 00:56:16,510
Of course. And so then I think, like just thinking about this kind of example, we could just chat through.

483
00:56:17,520 --> 00:56:22,000
So. So this is like our research question.

484
00:56:22,330 --> 00:56:30,670
We're interested in, you know, how it is sort of like violence, exposure, stress and sleep, interrupt,

485
00:56:31,000 --> 00:56:37,870
you know, interact and how my violence and sleep, violence and stress exposure disrupts sleep.

486
00:56:38,140 --> 00:56:43,330
And we're interested in looking at this in a high poverty sample or high poverty setting.

487
00:56:44,320 --> 00:56:55,540
And so I just put up like. You know, some examples of how we might think about these constructs, different ways of measuring, man.

488
00:56:55,790 --> 00:57:05,770
And, you know, obviously there's different ways we could go about measuring these things that are more or less invasive.

489
00:57:07,300 --> 00:57:12,660
But this is like, you know, studies like this exist out there, right, that are NIH funded.

490
00:57:12,670 --> 00:57:19,899
So like I look at this study design, I'm like, okay, like maybe we have a questionnaire.

491
00:57:19,900 --> 00:57:24,100
We're asking people about their home environment. Maybe we, like do a neighborhood assessment.

492
00:57:24,100 --> 00:57:30,220
Or maybe we look at like secondary data and see like what are the crime rates in the community or something like that.

493
00:57:30,700 --> 00:57:33,459
And then we're really interested in look like measuring sleep really well.

494
00:57:33,460 --> 00:57:37,930
So we're like, would you come in for like a sleep lab study and spend the night overnight?

495
00:57:39,010 --> 00:57:45,580
What are the implications for that if we're thinking about a population who's living in a poverty setting?

496
00:57:47,710 --> 00:57:51,490
This is another way to measure sleep, which is a little bit less gold standard.

497
00:57:51,490 --> 00:57:57,280
But I mean, it's pretty much the gold standard because it's the only way to do this outside of a lab.

498
00:57:57,280 --> 00:58:05,850
But like wear a sleep monitor for a week and log your sleep and, you know, tell me what time you're going to bed waking up the data.

499
00:58:05,860 --> 00:58:09,580
Keep asleep diary, and then we'll kind of gather the data to meet with you.

500
00:58:09,590 --> 00:58:17,530
So, like, pretty high touch, right? And like, being responsible for this monitor, maybe it's an expensive thing.

501
00:58:18,280 --> 00:58:24,339
It used to be really expensive. And now they're dropping in price and then stress like, oh, we'd like to measure stress really?

502
00:58:24,340 --> 00:58:27,520
Well, we'd like to take some biological samples in saliva.

503
00:58:27,520 --> 00:58:32,230
We have to do that in a certain time of day so that we know what you're responding to in terms of the stress.

504
00:58:32,980 --> 00:58:38,280
Now, we're going to also ask you some questions about your worries and anxiety and like things that might have happened to you.

505
00:58:38,300 --> 00:58:47,770
So like when you guys think about someone who's living in a high poverty setting and like potentially participating in that study.

506
00:58:49,690 --> 00:58:59,080
What what do we think about, like compensation and design and intensity and potential challenges?

507
00:59:07,070 --> 00:59:19,530
What would you compensate for the study? I was a bit worried about like your average for like traumatized.

508
00:59:23,950 --> 00:59:30,070
And 24 hours. Then the name. At least on average.

509
00:59:43,730 --> 00:59:48,310
Abolish the Senate. But thank you.

510
00:59:57,540 --> 01:00:02,390
And. We're certainly seeking to recruit people and I promising pool rate.

511
01:00:03,180 --> 01:00:06,840
We want to compensate them for this work. Yes, totally.

512
01:00:07,020 --> 01:00:10,799
We are potentially retraumatizing around violence.

513
01:00:10,800 --> 01:00:14,310
Right. You're like going to interview them. What violence is happening in their home.

514
01:00:14,310 --> 01:00:20,680
So. Is that safe to even do? Like, what about domestic violence or controversial violence in the home?

515
01:00:20,690 --> 01:00:30,059
Like. What if that person is there and they know that the participants were tested in the study that could really place the participant in danger.

516
01:00:30,060 --> 01:00:33,150
If that person thinks like, Oh, you're reporting on what happens in the home.

517
01:00:35,200 --> 01:00:37,660
That idea about worrying times, stressful life events.

518
01:00:38,140 --> 01:00:43,660
Just as an aside, if wonder, you know, was asking about this and like asking certainly about depression,

519
01:00:43,660 --> 01:00:48,670
then we all would be aware, like you need a a safety protocol.

520
01:00:48,700 --> 01:00:53,460
Right. If somebody is like you're asking someone if they feel suicidal or have felt suicidal,

521
01:00:53,470 --> 01:00:59,500
you would meet someone on the team who could address that and view it in a timely fashion.

522
01:00:59,590 --> 01:01:02,850
So there's like a lot of complexities around this study, right?

523
01:01:02,860 --> 01:01:13,080
Like this is definitely an are a one level study and yeah, the after graph tracking that.

524
01:01:13,690 --> 01:01:21,100
So what are some what are some ways that we could like maybe think about either tweaking the protocol or taking the payment?

525
01:01:22,570 --> 01:01:30,440
Structure. Equitably and ethically have people participate.

526
01:01:43,650 --> 01:01:48,690
This is a study that people definitely would could get funded like by age.

527
01:01:49,980 --> 01:01:56,020
But what would you like if you were if you were. Cautions or tips.

528
01:01:56,600 --> 01:02:06,960
But you might tell. You're not talking about.

529
01:02:09,130 --> 01:02:21,700
And you know it's a part of the that. This stuff really works.

530
01:02:23,120 --> 01:02:28,400
And even I would go further to say like. Time it.

531
01:02:29,330 --> 01:02:35,140
Right. Similar studies to us that play for our lives.

532
01:02:36,220 --> 01:02:39,250
But we've done you know, I've been involved in studies.

533
01:02:39,250 --> 01:02:49,180
I've done almost all of those other things. And. Like saying oh, you know, to do the act across from.

534
01:02:50,470 --> 01:02:57,040
That's. Later in time, they're supposed to call.

535
01:02:58,230 --> 01:03:02,050
I can. Biological.

536
01:03:05,780 --> 01:03:10,970
It's growing and growing up. What is going to happen? Well, I think that's where the biological samples from could.

537
01:03:12,620 --> 01:03:22,130
People are like. Few years, but I'm much more aware of how are they stored, what they used for use for anything else.

538
01:03:23,350 --> 01:03:34,480
Michigan. We have this one spot that every year it gets basically like a physical card and some those cards are made available for.

539
01:03:36,410 --> 01:03:40,120
We had. To sign an.

540
01:03:41,250 --> 01:03:46,830
It's more than that. Sometimes there's a different.

541
01:03:48,520 --> 01:03:54,460
Not an by. Holy [INAUDIBLE]. But there's something out there, you know, people.

542
01:03:56,320 --> 01:04:02,020
How are those firefighters? And then I think again, like.

543
01:04:03,840 --> 01:04:07,590
You know, first, since we've all sort of probably said something like.

544
01:04:08,230 --> 01:04:15,150
Some of these questionnaires may contain information that might make distress distressing or upsetting and.

545
01:04:15,860 --> 01:04:21,010
Feel free to stop any comments. That comes up to.

546
01:04:23,260 --> 01:04:25,450
You know, the safety plan, which I mentioned.

547
01:04:27,620 --> 01:04:38,180
And I think, you know, thinking about this study design, why it's like we know Super Bowl standard would be like the last 24 hours.

548
01:04:38,460 --> 01:04:45,770
But like, if you're like a researcher who is most interested in like the stress part.

549
01:04:46,310 --> 01:04:49,820
Like, do you need to use the fancy sleep technology?

550
01:04:49,820 --> 01:04:53,180
Or could you be like, okay, we'll do only active or free.

551
01:04:53,210 --> 01:04:57,350
We won't do this like 24 hour overnight work life disrupting.

552
01:04:57,350 --> 01:05:02,590
Like do you provide child care for the person who has cancer? You know, like all sorts of things like that.

553
01:05:02,900 --> 01:05:04,730
That's a really invasive thing.

554
01:05:05,450 --> 01:05:11,960
And it's like, you know, the lowest measure of sleep here, which I've used all the time, is like sleep questionnaires, right?

555
01:05:12,770 --> 01:05:16,790
So like, how intensively do you need to measure your construct?

556
01:05:17,180 --> 01:05:20,480
And is like, is that where you like want to put, you know,

557
01:05:20,510 --> 01:05:25,760
all of your effort getting people to come in for a sleep study that's going

558
01:05:25,760 --> 01:05:30,620
to give you like 15% more information and nuance than like an active graph.

559
01:05:31,130 --> 01:05:36,920
And then after Graph Republic give you 50%, 70% more information than a questionnaire.

560
01:05:36,920 --> 01:05:43,879
So you might want to try for the active graph versus a questionnaire, but like you may not need the full fancy technology of the sleep study.

561
01:05:43,880 --> 01:05:46,070
So it's like that idea too.

562
01:05:46,070 --> 01:05:53,690
And thinking about like our study design and like the intensity with which we recruit people because here we might try to do this study and be like,

563
01:05:54,080 --> 01:05:56,960
Oh my gosh, nobody's coming in for the 24 hour overnight.

564
01:05:57,650 --> 01:06:06,230
Maybe we reconfigure our, you know, protocol and then you'd want to talk to your program officer and talk to, you know, people before you drop that.

565
01:06:07,010 --> 01:06:14,270
But a very good justification for like dropping part of the protocol would be formally nobody.

566
01:06:14,270 --> 01:06:17,960
We haven't gotten anybody to do that sort of through like our first 25 people.

567
01:06:18,800 --> 01:06:22,010
It's just not feasible. I think so.

568
01:06:22,010 --> 01:06:27,160
Yeah. So, yeah, I didn't. I didn't know how to do.

569
01:06:27,690 --> 01:06:32,970
The part. Wow.

570
01:06:33,080 --> 01:06:37,210
To be in the place for me. Yes. I think you need to pay.

571
01:06:37,640 --> 01:06:45,560
I'm pretty sure. But someone who is also steady work to do this.

572
01:06:45,980 --> 01:06:52,400
Okay. Bring it in with me. It was friend of mine, right?

573
01:06:52,550 --> 01:06:55,760
I mean, because I was right there and I was there every day.

574
01:06:56,860 --> 01:07:00,970
You heard me right. How do they come in every day?

575
01:07:01,400 --> 01:07:04,490
Right. Stay there. And then maybe they stay there the whole weekend.

576
01:07:05,240 --> 01:07:10,010
Right. It was great. It's like a week long search.

577
01:07:11,660 --> 01:07:19,819
More than thinking like the research data, like you're getting either getting research data from people who are like coming in on stage or like from

578
01:07:19,820 --> 01:07:25,480
thinking about the sleep data and then commuting what's happening on their commute to kind of lose control over,

579
01:07:25,490 --> 01:07:29,450
you know, but then again, like, where are you studying? I can come and spend a week.

580
01:07:31,880 --> 01:07:35,520
And I think if we think about the why. Like, why do we care?

581
01:07:35,570 --> 01:07:39,110
In this case, community based violence disrupts sleep.

582
01:07:40,900 --> 01:07:46,600
You know, there's very real good reasons why we may not get the people from our population and interests to come in and do.

583
01:07:48,410 --> 01:07:57,140
So. Yeah. Okay. You just say like it's probably yeah, there's some my sleep lab people, friends,

584
01:07:58,040 --> 01:08:03,590
collaborators are in Colorado and they do a lot of like light exposure and sleep.

585
01:08:04,250 --> 01:08:08,450
And my particular collaborator moved there from Rhode Island where we both met.

586
01:08:08,990 --> 01:08:11,360
And she said when she moved to Colorado,

587
01:08:11,840 --> 01:08:19,460
it like totally changed her research world because New England was kind of dark and cold and then Colorado was like so much sunlight.

588
01:08:19,910 --> 01:08:26,030
So she sort of switched to doing these like artificial darkening studies and having people come in for like a

589
01:08:26,030 --> 01:08:32,030
24 hour period of the 30 hour period and seeing like where on the biological clock their circadian rhythm was.

590
01:08:32,390 --> 01:08:34,850
So there's like so much you can do with sleep.

591
01:08:35,930 --> 01:08:40,970
That part of sleep that you've been to, all these other biological measures, light exposure and all this stuff.

592
01:08:40,980 --> 01:08:47,830
But like, where is your research question? Like. I mean, that's always a perfect score.

593
01:08:47,840 --> 01:08:54,950
So yeah, like it. I think without the sleep lab study here, I think people who would be participating in this study,

594
01:08:54,950 --> 01:09:06,230
from my assessment at one wave, it would be at least I think maybe 150 to $200 if you chunk up all those things.

595
01:09:07,190 --> 01:09:14,240
So again, like thinking, scope, research, scope and budgeting grants and stuff like that, that stuff gets expensive.

596
01:09:16,040 --> 01:09:20,270
Okay, um, let's see, we talk a lot.

597
01:09:20,540 --> 01:09:23,569
Oh, we were talking about animals. Let me see.

598
01:09:23,570 --> 01:09:33,410
This is. This is kind of about animals, too. That's another side, though, Amy, that might have some animal welfare stuff, but I knew I had put some.

599
01:09:33,760 --> 01:09:39,700
There have been less what I deeply know. I think we've talked about a few of these things.

600
01:09:39,700 --> 01:09:46,180
I think this sentient being thing is really fascinating because we don't, you know, typically think of mice.

601
01:09:47,020 --> 01:09:55,550
But maybe if you've worked in a lab with mice and you're the person who's like tracking through a snowstorm to make sure their welfare is not enough,

602
01:09:55,570 --> 01:10:09,490
welfare needs are met. Like how sentient, you know, how much do we distinguish between animal types and then those issues around harassment?

603
01:10:09,490 --> 01:10:15,639
And I think that like I remember the thing with like beagles that just happened in the summer.

604
01:10:15,640 --> 01:10:22,959
Maybe there was like a puppy mill of beagles, but I think they were being I can't remember what it, what it was exactly.

605
01:10:22,960 --> 01:10:27,880
But they were, you know, there was like a big case in the news about these dogs are being mistreated.

606
01:10:27,880 --> 01:10:32,290
And I think that they were adopted out. But then it also think like, oh, gosh, these dogs are going to be adopted out.

607
01:10:32,290 --> 01:10:37,600
Like what was their kind of early life stress and how do they you know, what happens with them?

608
01:10:38,530 --> 01:10:43,120
And so this three RS piece, which was one of the it's like I think it emanated from Britain,

609
01:10:44,080 --> 01:10:49,000
but they talk about like if we can replace the meat for animals, let's do that.

610
01:10:49,990 --> 01:10:55,320
If we must use animals, let's reduce the number of that region that we need to use and for refinement.

611
01:10:55,330 --> 01:10:58,989
And so that's that question of like, when do you need to get to the dogs?

612
01:10:58,990 --> 01:11:05,770
Like when how much can you do before you, you know, you need to to test things in animals.

613
01:11:06,220 --> 01:11:11,950
And I would argue with you have to test some things in animals because we just can only know so much, you know,

614
01:11:11,950 --> 01:11:20,410
if we're talking about like life saving cancer drugs and like really, really, you know, critical medical breakthroughs.

615
01:11:21,560 --> 01:11:32,000
At some point you need animals, but like, I don't know, like makeup, testing, maybe you only maybe animals that are very small part of that, you know.

616
01:11:33,650 --> 01:11:36,940
So kind of like when do we get to that point?

617
01:11:36,950 --> 01:11:42,200
And it's important to think I don't know if other reflections. Have you guys both worked in animal labs?

618
01:11:42,200 --> 01:11:45,520
If you worked in animal, I, I really.

619
01:11:48,510 --> 01:11:56,620
Yeah. No, I don't know. There was one person.

620
01:11:57,950 --> 01:12:05,860
Really big. You don't see the horses or planes or something like.

621
01:12:10,000 --> 01:12:19,650
And that real? And that's what got us there.

622
01:12:21,510 --> 01:12:29,530
And. Now to protesters that they know that it's it is it is a lot.

623
01:12:29,530 --> 01:12:36,060
And then of course like veterinary medicine, right. Because of their also you know and again like it's.

624
01:12:36,960 --> 01:12:40,230
For for a lovely, relatively immediate purpose.

625
01:12:41,790 --> 01:12:46,470
Fourth welfare. So it's a it's definitely complicated.

626
01:12:49,780 --> 01:12:58,090
Okay. So I think, oh, here, well we can watch some of the videos and then I had like our little coffee break so we can, should we?

627
01:12:59,620 --> 01:13:05,710
So we take a mini break and then talk about like videos and I. RB I think I just have some other examples.

628
01:13:07,750 --> 01:13:12,350
Okay. So I'll leave it up on the videos and just the 1015.

629
01:13:17,270 --> 01:13:21,820
Run. A stretch.

630
01:13:44,200 --> 01:13:55,660
I feel that I have really bad. The position that they were promoted.

631
01:13:56,480 --> 01:14:01,290
I would just put it as the and I.

632
01:14:04,600 --> 01:14:36,120
You know how it is back. It's too far, and I have to go upstairs.

633
01:14:39,330 --> 01:14:43,140
Let me help you out in just the lab left that we went over.

634
01:14:49,210 --> 01:14:53,170
So it's nice that. Yeah.

635
01:14:54,480 --> 01:14:58,810
I watched that you recorded those two yesterday.

636
01:15:02,830 --> 01:15:12,300
But in the past, we've had some. Skidmore saw.

637
01:15:18,470 --> 01:15:22,010
Are you guys all set for late course stuff for next year?

638
01:15:22,400 --> 01:15:27,550
I'm doing that. I don't know. I definitely don't know.

639
01:15:27,560 --> 01:15:32,030
But I feel like it's coming at me. I just got it. It's gotta be coming soon.

640
01:15:35,190 --> 01:15:38,690
But do you guys have a sense of what?

641
01:15:41,780 --> 01:15:45,800
No, not yet. Okay. I did see on campus they have this.

642
01:15:47,100 --> 01:15:50,100
I believe in the past and it's a very different semester.

643
01:15:50,100 --> 01:15:54,080
So that's kind of what's coming up. Oh, okay. It's been a good year.

644
01:15:57,050 --> 01:16:00,140
On campus. It's like it was me. All the bad stuff.

645
01:16:00,140 --> 01:16:05,280
But it was bad. Yeah. It's like, what happened? There has been.

646
01:16:12,360 --> 01:16:15,600
So you're not very nice. Oh, I think I.

647
01:16:15,820 --> 01:16:27,500
It is that sort of seven years. I don't think that.

648
01:16:31,860 --> 01:16:36,070
Okay. Now we know we couldn't carry on from South.

649
01:16:39,100 --> 01:16:50,709
How about you? And, you know, figuring out doing a little bit of that like I to check in like okay where where do I want to,

650
01:16:50,710 --> 01:16:55,150
like, situate myself for like more and just knowing.

651
01:16:56,860 --> 01:17:00,249
You know, not everything is always off her back each semester.

652
01:17:00,250 --> 01:17:09,560
But it is a lesson to be offered because we. But I think the communication one is.

653
01:17:12,170 --> 01:17:15,590
Laser light with an HP HP as you probably can.

654
01:17:17,370 --> 01:17:20,640
I have a higher status than the average student per.

655
01:17:21,710 --> 01:17:25,730
And some of those are very. I know.

656
01:17:30,610 --> 01:17:37,890
I think that would be going into that and that'll be really good. I know this person tend to be a super popular figure.

657
01:17:38,310 --> 01:17:50,050
So I think there's probably a lot of different opinions. And I want to say he teaches at 830 in the morning, a class like me and I.

658
01:17:51,840 --> 01:17:58,889
Um, yeah. And, and, and does motivational interviewing of patients persons.

659
01:17:58,890 --> 01:18:04,810
But I'm not sure of when it happens. Those are harder to get to.

660
01:18:10,370 --> 01:18:14,900
Motivational interviewing that last year been online or.

661
01:18:15,210 --> 01:18:22,550
Like a hybrid. I don't know how that went, but that's a good one to check in on, too.

662
01:18:22,560 --> 01:18:25,860
So maybe just, you know, in the next. Yeah.

663
01:18:25,870 --> 01:18:29,430
Just being a word and I don't know come at some point soon.

664
01:18:29,480 --> 01:18:37,030
I think that representation of the word should be the strength of the company because.

665
01:18:40,940 --> 01:18:46,430
Next year depends on how.

666
01:18:51,950 --> 01:18:57,740
How did you know we were going to do that?

667
01:18:57,830 --> 01:19:03,560
Well, I think that might. Talking with others in a smaller company, but also.

668
01:19:04,710 --> 01:19:11,760
Planning courses that are. Like, for example, a lot of people see.

669
01:19:13,460 --> 01:19:19,020
Starting to see. Staff. Staff in nursing. Of course, in nursing.

670
01:19:23,250 --> 01:19:26,990
It may have been just split. Yeah.

671
01:19:29,860 --> 01:19:37,240
I guess. I wanted to spend time.

672
01:19:40,010 --> 01:19:43,190
U.S. forces in Afghanistan have begun to be.

673
01:19:47,460 --> 01:19:56,220
And I just went through all. Certainly supports the policy.

674
01:19:59,740 --> 01:20:08,290
And that person a. Hightower and I think their course.

675
01:20:09,110 --> 01:20:13,259
They're considered their cohorts are small.

676
01:20:13,260 --> 01:20:16,260
They're much. So they're classified.

677
01:20:24,700 --> 01:20:30,270
Right. I. I think I found one.

678
01:20:30,420 --> 01:20:34,160
I don't know if it offered, but. Yeah. I think it's.

679
01:20:36,580 --> 01:20:42,490
Yeah. Social work stuff. Definitely better have a. Topic area and yeah.

680
01:20:43,420 --> 01:20:50,260
Yeah I item and. I feel like I've never seen.

681
01:20:50,730 --> 01:20:56,990
Policy and social work are all. It just that around on their Web site.

682
01:20:57,070 --> 01:21:02,500
This is your first. That's my copy that.

683
01:21:02,650 --> 01:21:09,840
You know, the person saying I was the student just kind of pointing out online courses and the fake and.

684
01:21:24,160 --> 01:21:27,890
Oh yeah, she is. She's the. She's social. Yeah, but she's.

685
01:21:29,090 --> 01:21:32,210
Oh. Yeah.

686
01:21:32,220 --> 01:21:45,900
Yeah, that's. Nurses and health scientists have been.

687
01:21:47,560 --> 01:21:55,910
We're. But not being.

688
01:21:59,400 --> 01:22:04,790
I sense that the house is stuck with their thinking that we.

689
01:22:09,030 --> 01:22:14,109
A lot of. Yeah.

690
01:22:14,110 --> 01:22:23,240
And like, how to write, like. Like I said, I was secret that.

691
01:22:25,670 --> 01:22:42,550
I think. I hope this.

692
01:22:51,010 --> 01:22:54,890
It's trying to figure out what we're. Yeah.

693
01:22:56,320 --> 01:23:01,160
So I do have. When they do, they have to be like a desert.

694
01:23:01,700 --> 01:23:05,160
Do they have to be like one of the countries that is.

695
01:23:09,840 --> 01:23:16,520
Christine. It's the person time.

696
01:23:22,590 --> 01:23:25,980
That's the mental health. Just I don't know.

697
01:23:26,800 --> 01:23:31,580
I just take the terms of the policy. You'll kind of.

698
01:23:33,050 --> 01:23:36,970
Daniel, I. I.

699
01:23:41,550 --> 01:23:47,190
This is beyond. Yeah.

700
01:23:47,350 --> 01:23:57,570
She's a. Sort.

701
01:23:58,300 --> 01:24:07,240
I think mental health is sort of like. You know in you are.

702
01:24:11,160 --> 01:24:15,250
So I think that that makes that is a little bit like boxing.

703
01:24:22,780 --> 01:24:26,720
One. I know social work is a bit place.

704
01:24:33,100 --> 01:24:41,120
Testing. Something might have sent.

705
01:24:41,850 --> 01:24:56,280
The support for those units. And forces.

706
01:24:57,910 --> 01:25:01,510
First thing. Like there.

707
01:25:03,500 --> 01:25:08,690
Are potentially. Not.

708
01:25:23,920 --> 01:25:28,670
Psychology. Sometimes.

709
01:25:36,230 --> 01:25:48,130
And. Yeah.

710
01:25:48,880 --> 01:26:04,240
I feel like the psychology. Hey.

711
01:26:05,930 --> 01:26:15,590
I think this is, like, good. Could feed your contents boxes no matter.

712
01:26:17,310 --> 01:26:21,560
But knowing when that. I heard nothing.

713
01:26:22,940 --> 01:26:27,650
And then it's like whether it be awkward and then like cross channel I think said I have.

714
01:26:31,520 --> 01:26:35,610
The time that these. Students to seem like.

715
01:26:54,600 --> 01:26:58,310
Thank you for bringing that up. Yeah. All right.

716
01:26:59,750 --> 01:27:04,170
Yeah, I know that. Didn't like the timing. I think the future turkey.

717
01:27:04,170 --> 01:27:18,430
And I think. I think. I don't know. But that going be Thanksgiving and then all of a sudden I'm kind of cut from that.

718
01:27:19,060 --> 01:27:25,840
You know, it's good to just sort of like, you know, work design under couch and browse and course stuff.

719
01:27:28,210 --> 01:27:38,460
So cool. Okay. So we can I think these are some of the other videos that I thought are can be helpful.

720
01:27:41,720 --> 01:27:47,620
And I wonder yeah maybe this cherry picking data one let me just.

721
01:27:49,460 --> 01:27:55,070
We can just watch this stuff and I'll go here. So these are these other videos?

722
01:27:55,700 --> 01:28:06,429
Not those and each ones, but. They usually have some some like cheesy music.

723
01:28:06,430 --> 01:28:17,020
So let me see. So this is just an example with some like thoughtful questions at the end of like on research ethics moments for grad students.

724
01:28:17,940 --> 01:28:21,180
Oh, there it is. Like this groovy music.

725
01:28:22,510 --> 01:28:29,180
Just leave it on, repeat it. Western results. Now Trump's and I will handle how you do it.

726
01:28:29,600 --> 01:28:33,620
An application deadline is less than two weeks. No, she's not. I'll take care of it.

727
01:28:39,870 --> 01:28:49,860
A citizens must also not only know the results of. A little acting like that was.

728
01:29:02,180 --> 01:29:08,550
I'm going to destroy. But usually they're not the ones that might be good enough for the prime.

729
01:29:11,100 --> 01:29:20,360
Finance. She's like the star, and most of them will never give that much.

730
01:29:33,530 --> 01:29:37,790
I'm missing the entire pregnancy. Yeah.

731
01:29:39,570 --> 01:29:45,400
I'm hoping someone comes and keeps an instrument and there's simply no longer.

732
01:29:47,210 --> 01:29:52,550
I I'm super frustrated and I'm worried about what will happen in the moment.

733
01:30:00,950 --> 01:30:08,040
So. What's the point of.

734
01:30:12,780 --> 01:30:39,080
Right. Like a moment of choice.

735
01:30:41,920 --> 01:30:45,040
Okay. So what is the.

736
01:30:49,830 --> 01:30:53,250
All of these pressures that he did it.

737
01:30:54,780 --> 01:30:58,690
He basically deleted the files, like minded bureaucrats.

738
01:30:59,910 --> 01:31:06,820
I think he did what they wanted for their grandson. You play?

739
01:31:08,110 --> 01:31:19,940
At the site choice appointments. So the pressure is strengthening at the time of the NPI.

740
01:31:22,820 --> 01:31:28,260
It was like trying to get social support from his wife. He was like this new iPhone 5 seconds.

741
01:31:28,310 --> 01:31:29,960
I was pregnant. Like, I'm missing all whole.

742
01:31:30,710 --> 01:31:38,480
So I kind of liked this example because it was just all the different things, strange things in life shape or like moment of choice.

743
01:31:39,800 --> 01:31:47,660
And the pie also my grandma getting there was like, we have to get the right results or whatever when good enough for that type of thing.

744
01:31:48,080 --> 01:31:51,900
But they're an example, like they have to work, they have to be right ones.

745
01:31:51,920 --> 01:31:58,510
And so she made this kind of data choice. That was to remove the ones that didn't like the line.

746
01:32:02,950 --> 01:32:07,620
And so I think that it's just sort of like one small example,

747
01:32:07,630 --> 01:32:17,230
but it's like a recent research in ethics and romance and data that misconduct is like shrinking big data we talked about earlier, too, so.

748
01:32:20,520 --> 01:32:23,550
Just to kind of present that to play out with.

749
01:32:23,560 --> 01:32:27,640
We can see why. It's happening, right?

750
01:32:28,300 --> 01:32:32,820
It's not that they're just like out of the blue or people are bad people. They're just like, Oh, let me just get my husband.

751
01:32:32,860 --> 01:32:37,790
It didn't work. But it was on a different. She explains that the money.

752
01:32:39,760 --> 01:32:44,110
So we can we'll watch another one of those for that. And when we talk about authorship.

753
01:32:44,950 --> 01:32:49,150
I kind of like these videos, but they have a bunch of different ones.

754
01:32:51,670 --> 01:33:00,060
And these other videos around, like the lap dynamics that we saw before where they have like the ugly sweater party and certain things.

755
01:33:00,130 --> 01:33:07,390
So this also what I like about this session, this sequence of videos, is that it does it's like that younger grad student,

756
01:33:08,200 --> 01:33:12,909
she's sort of like doing the work often, and then he's the postdoc in the lab.

757
01:33:12,910 --> 01:33:16,989
So he sort of has like he's like a little bit, not totally,

758
01:33:16,990 --> 01:33:25,209
but like somewhat her boss or he's like runs interference sometimes between the P.I. and the grad student, the Ph.D. student.

759
01:33:25,210 --> 01:33:28,750
And so I think it also like it brings up some of the dynamics of like.

760
01:33:30,000 --> 01:33:35,790
Just pressures to do it, like he felt pressured by the P.I. and then she was kind of like, you know,

761
01:33:35,790 --> 01:33:41,150
it's going to come up like she saw the results were different and then they work different and like, then what happened?

762
01:33:41,160 --> 01:33:44,340
So we can watch another one of those.

763
01:33:44,340 --> 01:33:47,520
I think the authorship ones are good in particular. Um, okay.

764
01:33:47,640 --> 01:33:54,780
So, all right, so there's that coffee and lovely coffee.

765
01:33:55,230 --> 01:33:59,970
Um, ah, a little, I don't know if people saw on the vector like that,

766
01:34:00,900 --> 01:34:06,300
that like now our pseudo cafe is going to be open sometime in November where we will have like a,

767
01:34:07,140 --> 01:34:12,840
I don't know, we'll have the period coffee or something so we can visualize this coffee.

768
01:34:14,040 --> 01:34:23,100
But I did also want to talk about IRP and just go through a little bit of like IRP stuff and like resources here.

769
01:34:23,640 --> 01:34:26,100
And I think I love this. It'll come next.

770
01:34:26,100 --> 01:34:33,990
But like, I think that sometimes we just like twist ourselves into a pretzel trying to, like, meet the needs of the IRP.

771
01:34:34,380 --> 01:34:38,040
And like with that study example we were talking about with the sleep and violence in the community,

772
01:34:38,580 --> 01:34:44,610
like you can imagine the IAB having like a lot to say about how that study was conducted.

773
01:34:45,810 --> 01:34:51,930
And then, you know, keeping in mind like there's still like participant experience to think about.

774
01:34:51,930 --> 01:34:58,530
And so it's really a balance between like doing things that are going to be like promoting the, you know,

775
01:34:58,740 --> 01:35:04,469
the safety of the participants, but then also making it a reasonable experience for the participants.

776
01:35:04,470 --> 01:35:12,510
And so I think we've all kind of been in data collection type experiences where like you're bending over

777
01:35:12,510 --> 01:35:17,670
backwards to do all these different parts of the protocol and like at some point you just have to go,

778
01:35:18,060 --> 01:35:23,490
Oh my goodness, what is the participant experience here? And having to make it like more easy.

779
01:35:23,670 --> 01:35:30,690
And you know, the reason for that being that then more people would participate in the research and then you had that like,

780
01:35:31,350 --> 01:35:37,540
you know, justice and everybody being able to participate. And then so that's kind of this case.

781
01:35:37,900 --> 01:35:45,520
And then this one, like I often and I don't know how many people have gone back and forth between like the behavioral sciences,

782
01:35:45,520 --> 01:35:49,599
research kind of approvals and then the medical school kind of approvals.

783
01:35:49,600 --> 01:35:54,940
But depending on kind of where you're submitting your IAB application, I'll show you some examples.

784
01:35:54,950 --> 01:35:57,219
And they kind of care about different things.

785
01:35:57,220 --> 01:36:06,310
Sometimes it's all driven by like the Belmont Report and those three principles, but how it's operationalized can be can look kind of different.

786
01:36:07,690 --> 01:36:10,929
And so I think we can all appreciate like animal research,

787
01:36:10,930 --> 01:36:18,790
like there's these sort of sub groups of researchers who kind of become acclimatized to the sort of different risks.

788
01:36:19,420 --> 01:36:22,660
And benefits that different types of research protocols can carry with them.

789
01:36:23,170 --> 01:36:26,250
And that's not to say like one group is right and one is wrong.

790
01:36:26,260 --> 01:36:29,320
It's just like where they're like where their research focuses.

791
01:36:29,740 --> 01:36:36,580
And so like, for example, if you were doing a behavioral study of questionnaires and it went to.

792
01:36:37,710 --> 01:36:45,900
A research group who like never did questionnaires about like worry and anxiety, but they took blood samples all the time.

793
01:36:47,310 --> 01:36:53,340
They might think that those questionnaires about worry and anxiety are like really invasive, but not think about the blood draw.

794
01:36:54,030 --> 01:37:01,499
Whereas if you went to a behavioral board that's very interested and usually reviews applications that ask about worry and anxiety,

795
01:37:01,500 --> 01:37:05,639
and you're like talking about doing a blood draw. And I'm like, Whoa, wait, why do you need someone's blood?

796
01:37:05,640 --> 01:37:09,430
Like that might be more invasive. So it's kind of like they're, they're,

797
01:37:09,480 --> 01:37:17,700
those are sort of examples of something that are like commonplace for different types of IAB reviewers, and that's also distorted.

798
01:37:17,700 --> 01:37:26,040
So overall, as you're framing it relates to like who reviews are grant applications in terms of, you know,

799
01:37:26,040 --> 01:37:32,009
what we were talking about last last time, about the time before about like choosing your study section.

800
01:37:32,010 --> 01:37:35,339
That's like used to seeing studies that are like the one you're proposing.

801
01:37:35,340 --> 01:37:42,690
Like that's really important because some people could be like, you know, that 24 hour sleep study is like.

802
01:37:43,320 --> 01:37:49,160
Way too much. But if you went to like a sleep focused group, they'd be like, Why aren't you doing a 24 hour sleep study?

803
01:37:49,170 --> 01:37:51,420
So, you know, it's like, where are your lenses?

804
01:37:51,420 --> 01:38:02,250
And I think the IAB kind of have similar elements of it, and we probably all know our purpose of the IAB.

805
01:38:03,120 --> 01:38:11,790
So the role of IAB really is to keep us honest and our participant safe and avoid all those violations.

806
01:38:13,230 --> 01:38:20,670
Really critical role, the RB application and why we all, if we ever do any research that is generalizable knowledge,

807
01:38:20,670 --> 01:38:26,370
we need to have an IAP application on file even if it is exempt and we'll talk about that.

808
01:38:26,910 --> 01:38:33,330
But we have to have a IAP application that has gone through approval because if we don't,

809
01:38:33,330 --> 01:38:39,450
we could get our institution in big fat trouble and we don't want to be the people to do that.

810
01:38:43,250 --> 01:38:55,790
That industry, FDR. Speaking of fairness, that is a sort of a different regulatory structure, but they do have their own regulatory structure.

811
01:38:55,790 --> 01:39:03,110
And that's really like the FDA monitors industry research that can lead to medications.

812
01:39:03,530 --> 01:39:10,730
And if that kind of research that can lead to medications is happening in a university context that is triggered by their hiring.

813
01:39:12,800 --> 01:39:23,000
So we in public health are like in this funny situation in the middle where because our collaborators are often either ourselves,

814
01:39:23,390 --> 01:39:30,520
we individuals do both types of research and or work with collaborators like in the med school who do this.

815
01:39:30,530 --> 01:39:38,980
And over here in psychology we do this. And so different research protocols are reviewed by different enemies.

816
01:39:39,720 --> 01:39:50,690
And espionage is like technically the unit are that our school is governed by the health by HSBC, which is behavioral sciences.

817
01:39:51,380 --> 01:40:00,900
RB. But we often sometimes need to get our research approved by the med school, IAB, which is IAB Med.

818
01:40:03,120 --> 01:40:09,270
Because of the protocols that we may be doing. So things like, you know, taking blood samples is like.

819
01:40:10,400 --> 01:40:19,510
Doesn't necessarily mean you need to get IAB med approval, but like other things that would be more involved might need IAB approval.

820
01:40:19,990 --> 01:40:27,480
And so sometimes. One can find oneself on the phone like I or be going like, I just need you to review this application.

821
01:40:27,500 --> 01:40:37,100
It's got these elements of it. So like, for example, when we were doing taking a lot of saliva samples from little kids to look at stress hormones.

822
01:40:38,290 --> 01:40:43,809
The Arabs were being driven crazy because if I was the P.I., they'd be like, Well, she got health behavioral sciences.

823
01:40:43,810 --> 01:40:48,700
And this is before a lot of like psych studies were doing sort of saliva sampling.

824
01:40:48,700 --> 01:40:51,970
Now a lot of psych studies also do that. So they're more used to reviewing that.

825
01:40:52,870 --> 01:40:59,829
But it seemed like it needed to go through IAB Med were never like IAB Med or med school collaborators on the application.

826
01:40:59,830 --> 01:41:03,459
So there's sort of all these like decisions around where to research live.

827
01:41:03,460 --> 01:41:06,850
So I would say this is a time to like talk to the, you know,

828
01:41:06,850 --> 01:41:12,580
your advisor and start dealing with this go through as I would start with health, behavior, science, health.

829
01:41:13,680 --> 01:41:17,340
Forget what HSBC stands for. Health Sciences. Behavioral Sciences, I think.

830
01:41:18,270 --> 01:41:23,159
Start there. And just, you know, you'll get used to those reviews.

831
01:41:23,160 --> 01:41:28,080
But that's just something to know that an either or situation or I mean, you say both.

832
01:41:28,970 --> 01:41:32,050
It would be an either or situation. Yes, exactly. Right.

833
01:41:32,060 --> 01:41:36,660
Right. So, you know, and they have sort of just like different structures.

834
01:41:37,890 --> 01:41:41,820
I'm on this board that kind of oversees.

835
01:41:43,740 --> 01:41:48,150
Broadly the IAB process and research compliance at the university.

836
01:41:48,720 --> 01:41:55,140
And so those discussions often are like, well, HSBC does it this way, IAB, but this does this.

837
01:41:55,140 --> 01:42:04,650
And I will say usually RB Med, just because so many of the research dollars go to the med school and most institutions,

838
01:42:04,650 --> 01:42:08,610
including this one, they're kind of setting the tone usually.

839
01:42:08,610 --> 01:42:15,830
And I think it's often the role of HSBC to be like, Wait a minute, didn't think about this, you know, what about our participants?

840
01:42:15,840 --> 01:42:21,450
And so I think that thing about like putting so many requirements on participants often

841
01:42:21,450 --> 01:42:26,610
comes from IAB Med because they're doing much more kind of physically invasive studies.

842
01:42:27,240 --> 01:42:33,090
And there have been some changes recently that have like lightened the burden for behavioral studies,

843
01:42:33,090 --> 01:42:37,770
including studies that are more studies that are exempt than they were previously,

844
01:42:37,770 --> 01:42:40,919
and like consent forms being simplified, which is, I think a good thing.

845
01:42:40,920 --> 01:42:49,500
But I think it's taken like a little bit of pushback by people who do behavioral science research to be like, this is more than we need here.

846
01:42:50,430 --> 01:42:59,759
An exception to that, I think, is clinical trials. And I think we talked a little bit about in if anybody have been some of us have like been in the

847
01:42:59,760 --> 01:43:07,140
clinical trials website and having to put in a lot of information about behavioral clinical research,

848
01:43:07,560 --> 01:43:18,540
clinical trials that has exploded in terms of the amount of kind of documentation and paperwork on the behavioral side that was driven from this side,

849
01:43:18,540 --> 01:43:22,890
and I don't think that's going to go away. So just something to note.

850
01:43:24,200 --> 01:43:31,049
I think you have to also try to be heard and external to say you guys here in Michigan,

851
01:43:31,050 --> 01:43:34,250
are you guys so interconnected with universities that it's usually just Michigan?

852
01:43:34,350 --> 01:43:40,110
They just rely on Michigan desire. Yeah, I know there are a lot of studies that have don't have multiple sides.

853
01:43:40,110 --> 01:43:48,659
And my my personal research experience is much more like we have collaborators at other sites who may or may not collect data.

854
01:43:48,660 --> 01:43:53,690
If they do collect data, it's support for intervention.

855
01:43:53,700 --> 01:44:01,680
I guess one has been for an intervention. But a while ago before behavioral before clinical trials, which I think might have been more of a nightmare.

856
01:44:03,990 --> 01:44:14,430
But yeah, there's a lot of discussion actually in that group that oversee our group about single site IRBs and how to make that the smoother process.

857
01:44:16,020 --> 01:44:19,740
So yeah, there is a lot of research that does happen with.

858
01:44:20,950 --> 01:44:28,090
You know, and for good reason. Like in your population, it's rare and you need to have multiple data collection sites.

859
01:44:28,600 --> 01:44:35,290
That certainly happens a lot. I don't know what you may have healthy minds how they do that.

860
01:44:35,500 --> 01:44:48,020
I'd be interested in. And more time on their hands or each time.

861
01:44:51,920 --> 01:45:00,440
Said. Seems very that's.

862
01:45:02,650 --> 01:45:07,260
Is it because they're like, not father?

863
01:45:09,790 --> 01:45:24,760
Started writing. Because there were so many schools in so many different states, and I been sort of like this.

864
01:45:33,560 --> 01:45:43,610
She was talking about. And so my brother Peter.

865
01:45:46,880 --> 01:45:50,210
Yeah. Yeah.

866
01:45:50,260 --> 01:46:12,590
Yeah. What's the name of that one?

867
01:46:12,620 --> 01:46:21,830
Maybe Edvard. I'm going to I'm going to just because I think I feel like you also get.

868
01:46:23,560 --> 01:46:30,610
More and more of these emails that are like will take over this research function for you, you know?

869
01:46:30,610 --> 01:46:32,440
And it's sort of unclear.

870
01:46:32,620 --> 01:46:41,880
I feel like it's an evolving space to tell, like who's a predatory, you know, research service provider and who could actually be really useful.

871
01:46:41,890 --> 01:46:43,140
Like, I think I got one the other day.

872
01:46:43,150 --> 01:46:49,510
It was like about consent form drafting or something like that and it was like we're we'll need all the elements that you need.

873
01:46:49,520 --> 01:46:54,730
You know, it was very outsourced. But I mean, you can see like this.

874
01:46:55,510 --> 01:47:03,790
Because of all the different many hoops to jump through within a single player being like one.

875
01:47:04,030 --> 01:47:09,990
If you, you know, and I think especially if you're working with know like other hospitals like we did do I remember.

876
01:47:10,000 --> 01:47:16,659
I do. I blocked it out. I grew with like Wayne State really kids who had diabetes.

877
01:47:16,660 --> 01:47:22,479
And that was that just took so much. And I think we got maybe 25 kids out of like 85.

878
01:47:22,480 --> 01:47:26,560
And I was like, was that worth it? I don't know. You know, it just was a lot.

879
01:47:27,560 --> 01:47:39,670
And we had this weird situation, so and yeah, in my previous work we were and was not affiliated with any sort of cancer center or anything.

880
01:47:39,800 --> 01:47:44,260
Oh, for sure. Separately, in addition to ours.

881
01:47:46,330 --> 01:47:50,070
And then working with hospitals and other kinds of taxes.

882
01:47:51,860 --> 01:47:55,220
Just feel like a shift. They also, I think.

883
01:47:57,910 --> 01:48:01,540
And then they had like a ship on your kitchen book or something.

884
01:48:01,990 --> 01:48:06,520
And then they wanted us to reduce our gift cards to our women.

885
01:48:06,770 --> 01:48:12,060
Like, they're like, oh, we have, like, a much lower volume.

886
01:48:13,960 --> 01:48:23,470
I don't know. That was just restricting. But we've been running it with them already for like an hour.

887
01:48:23,620 --> 01:48:27,880
Yeah, I that's really interesting, too, because you think about yeah.

888
01:48:27,910 --> 01:48:32,490
Like it's, it's sort of like a centralization question, like what are the implications?

889
01:48:32,600 --> 01:48:32,900
Yeah.

890
01:48:33,010 --> 01:48:40,930
And could it, could it impact your participants and did they let you keep the gift cards at the same level ultimately, or did you have to pay less?

891
01:48:40,930 --> 01:48:45,690
Or they forced us to pay less for them to have people refer participants to our study.

892
01:48:47,730 --> 01:48:52,990
And it isn't. And it makes sense because like if they, you know, it's like that previous slide,

893
01:48:52,990 --> 01:48:57,810
like if they if their payment level is here and that's sort of typical.

894
01:48:57,820 --> 01:49:05,080
And then like your study comes along with the tip here, they're going to lose all their research participants for this amount to go to your study.

895
01:49:05,140 --> 01:49:15,820
So like, you know, that sort of like equity across study opportunity payments, study payment opportunities is you could see how that could come about.

896
01:49:15,820 --> 01:49:22,330
But then like for your individual participants that you already know and have a relationship with, you know, they're getting paid less, so.

897
01:49:22,390 --> 01:49:25,840
Oh yeah. The ones that were in the study, we kept them there.

898
01:49:25,840 --> 01:49:30,049
And luckily we were in a juncture where only like a couple where we had just consented.

899
01:49:30,050 --> 01:49:35,950
So we had to go back and because the fact that I think we started it, but yeah, it was awful.

900
01:49:36,160 --> 01:49:41,500
So yeah. Yeah, I think we were doing behavioral.

901
01:49:41,650 --> 01:49:46,330
And so I don't know, I think it was also partially they're used to like this drug thing and I don't know,

902
01:49:47,620 --> 01:49:51,410
but is that is that kind of investing a lot of time or.

903
01:49:51,870 --> 01:49:55,260
We're not just. Right, right. Yeah.

904
01:49:56,690 --> 01:50:01,840
And it might be like I decided to do it because they were an important referral site for us.

905
01:50:01,900 --> 01:50:05,080
Right? Is that right? That's a good yeah.

906
01:50:05,230 --> 01:50:07,360
That's a quandary. That's a hard one.

907
01:50:08,080 --> 01:50:15,640
And yeah, it's like they're not giving blood samples or tissue samples, but it's like they're giving it personal information.

908
01:50:15,940 --> 01:50:19,150
Psychological information. Yeah, yeah, yeah.

909
01:50:19,270 --> 01:50:23,080
That's a lot. That is interesting. So I think yeah, I mean, I mean,

910
01:50:23,580 --> 01:50:28,780
I think it's a it is one of these watch the space things because I think that as like also

911
01:50:28,810 --> 01:50:34,000
team science grows and multi study sites and like they're trying to regulate it but.

912
01:50:34,120 --> 01:50:43,779
Been doing it I think like within these kind of clunky systems that like here you know to just like add someone to your IRA

913
01:50:43,780 --> 01:50:52,180
being is an amendment and like it should just be like a button to press that like somehow cross checks with their peers,

914
01:50:52,510 --> 01:50:56,950
training status or something. Like there'd be ways to improve it. But then it's like, that's a big thing.

915
01:50:56,950 --> 01:51:07,120
And actually on this meeting we only need twice a year. It's great to be on, but like the discussion was like, how do we get people to like, you know,

916
01:51:07,120 --> 01:51:11,469
upload versions of their consent forms more because they're required to do it and they don't do it.

917
01:51:11,470 --> 01:51:15,850
So like, what's the stick basically that we're going to use to try to have people do that?

918
01:51:16,480 --> 01:51:23,410
And someone was like, Well, we could consider adding a button and everybody in the call was like, Oh my gosh, no, not a button.

919
01:51:23,410 --> 01:51:29,590
Because that like had so many implications for like all of the different like software and programing and everything.

920
01:51:29,590 --> 01:51:33,600
And yeah, and I was like, oh my gosh, I it's so fun.

921
01:51:33,600 --> 01:51:41,260
Yeah. Yeah, yeah. It's all patches and it's not like a redesign because it would just be like, you know,

922
01:51:41,980 --> 01:51:47,080
so but it's very interesting to witness because these are like all the heads of these different areas.

923
01:51:47,080 --> 01:51:52,650
And I'm like, Oh my gosh, I don't want ever to have your job, but I really appreciate that you're doing it.

924
01:51:52,660 --> 01:51:58,870
And they, you know, they're very super integrity and passionate and, you know, and they do.

925
01:51:58,870 --> 01:52:02,770
And I think I put a little call out to them that I will show you guys, too.

926
01:52:03,880 --> 01:52:15,130
So I think, again, we probably we know these IAB elements, but they are these are there are are good guiding principles with a common rule.

927
01:52:16,270 --> 01:52:21,490
And this is what something that I would find interesting on like a commercial IAB.

928
01:52:22,870 --> 01:52:32,760
I would hope that the composition of the IAB would be similar, and I think that's like a federal rule that they would have to be, but I don't know.

929
01:52:33,870 --> 01:52:42,630
And so here, like, you have to have, you know, faculty, the, you know, the practitioner and the lay laypeople.

930
01:52:43,530 --> 01:52:49,680
And so like, for example, if a big if a commercial IAB is like we're doing a prison based.

931
01:52:51,040 --> 01:52:56,769
Application, which I used to do some work in that space with kids too.

932
01:52:56,770 --> 01:53:00,249
And so that was like such a wild IAB review.

933
01:53:00,250 --> 01:53:06,159
But like you have to have a person from the community that is the vulnerable population that you're doing.

934
01:53:06,160 --> 01:53:08,200
So like somebody who's knowledgeable about space.

935
01:53:08,200 --> 01:53:13,870
So like I wonder kind of what access they might have and maybe they have more access than universities do, I don't know.

936
01:53:14,380 --> 01:53:15,910
But that would be something to think about.

937
01:53:16,360 --> 01:53:22,720
I wonder if there's this competition or like how does that translate into Internet when you're collaborating with international?

938
01:53:23,290 --> 01:53:26,749
Yeah. I think it's a great question.

939
01:53:26,750 --> 01:53:38,110
And I think that. I would love to have John come in and like do a session on that because I think he's really managed a lot of that and,

940
01:53:38,800 --> 01:53:47,880
you know, has a lot of lived experience around that. So. I think that is an evolving space, like rapidly evolving for sure.

941
01:53:50,430 --> 01:53:54,360
And yeah, there's like a certain, like, classes of people.

942
01:53:55,520 --> 01:54:00,169
Or populations that are known, that are vulnerable populations.

943
01:54:00,170 --> 01:54:05,030
And I think there's a like a growing understanding of other kinds of populations,

944
01:54:05,030 --> 01:54:14,690
like cognitively impaired high poverty, you know, but they're not as officially protected yet.

945
01:54:16,670 --> 01:54:28,969
And then this idea of these exemptions, so this is what is kind of latent in terms of IAP like paperwork or submissions for some behavioral studies.

946
01:54:28,970 --> 01:54:36,080
So things like if you're evaluating normal educational practices, that's an IAB exempt activity.

947
01:54:36,080 --> 01:54:42,200
So if you have like an entire school doing a positive behavior support program, for example,

948
01:54:42,800 --> 01:54:50,300
and you collect evaluation data in the course of that, doing that in your school, that's exempt.

949
01:54:51,530 --> 01:54:58,380
But like I think healthy minds would not be under that exempt category because they're,

950
01:54:58,630 --> 01:55:04,940
they're trying to gather generalizable research and data so that something like a difference

951
01:55:06,230 --> 01:55:12,590
here observation of public behavior that's not linked to identifiers is also exempt.

952
01:55:13,250 --> 01:55:21,200
And then this is probably where most of us could could qualify for exempt research exempt attribute

953
01:55:21,290 --> 01:55:26,600
status is that you still have to put forth an application and they have to evaluate that it is exempt.

954
01:55:27,260 --> 01:55:37,940
But the publicly available data, data sets that are de-identified so like big datasets better for using for secondary research, that's an exemption.

955
01:55:40,130 --> 01:55:50,480
And, and those are kind of like just key, key IAB pieces to know about in terms of how you have them.

956
01:55:50,480 --> 01:55:54,900
Does IAB, which one did I want?

957
01:55:56,240 --> 01:56:02,560
So have you guys all logged in to IAB recently?

958
01:56:02,570 --> 01:56:12,110
Research to do and you're like, I hear like my life has reasoned, um, okay, so here, let's see.

959
01:56:12,110 --> 01:56:17,910
I was just going to log in and. That's me.

960
01:56:20,050 --> 01:56:23,740
At the beginning. Oh, come.

961
01:56:28,650 --> 01:56:36,990
This is when I feel like a robot. It's me. Like a good track house where you can know my face and then I get every day.

962
01:56:38,310 --> 01:56:39,120
It's ridiculous.

963
01:56:40,770 --> 01:56:49,319
But anyway, so this is Eve research, and you guys, me, I think, have all been in here, but just to kind of pull it up, do you have your, like,

964
01:56:49,320 --> 01:56:58,410
inbox with the tasks that you may need to do things that are in progress, which is like, I have some old things in progress, nothing urgent.

965
01:57:00,420 --> 01:57:07,470
And then, you know, your you can look at your approved studies that you're on and, you know, just.

966
01:57:08,360 --> 01:57:12,139
Click around. This is how the venues work here.

967
01:57:12,140 --> 01:57:16,130
I wanted to pull some examples of exempt and not regulated.

968
01:57:19,090 --> 01:57:25,060
So for example, this one is a study about maternal technology used during infant feeding.

969
01:57:25,750 --> 01:57:31,239
It's not regulated because we're not collecting any data at this site and we're like a secondary site.

970
01:57:31,240 --> 01:57:37,840
So we've got. So it's a very complicated study and a lot of regulation of videotaping and taking milk samples,

971
01:57:37,840 --> 01:57:43,360
all the stuff like at the site where the data collection is happening. But we're a second site.

972
01:57:44,770 --> 01:57:48,490
There was a lot of discussion around this one because it is involving videos.

973
01:57:49,330 --> 01:57:58,750
But my role because like I have a video coder person, so like my role on those videos is that like I may see some of these videos,

974
01:57:58,750 --> 01:58:01,670
but I don't know who the people are and I will not be ever,

975
01:58:02,080 --> 01:58:08,800
ever able to link those data to any people because I'll be like providing input on like this is how you code the videos.

976
01:58:09,280 --> 01:58:15,610
So it was still able to be done and not regulated except study.

977
01:58:17,640 --> 01:58:23,640
This is one actor, as does Carmen, who I've mentioned before, who's recruiting the grandmas.

978
01:58:25,530 --> 01:58:35,540
So her study, I can't remember here with the exam, but I think it's because she's doing interviews about like non sensitive topics.

979
01:58:35,550 --> 01:58:40,350
I want to say that how that one that's exempt groups. But I wanted to see.

980
01:58:40,350 --> 01:58:44,310
Okay, here we go. I just wanted to pull up one that was exempt.

981
01:58:47,030 --> 01:58:51,710
So where was she? Okay.

982
01:58:53,380 --> 01:59:05,550
There's, like, a little section. Just pull down here.

983
01:59:07,870 --> 01:59:17,229
There's a conflict of interest. Okay.

984
01:59:17,230 --> 01:59:25,870
So here's like application type. So this is where you get into this like decision making and, you know, figuring out where your study goes.

985
01:59:26,260 --> 01:59:29,740
So it's a minimal risk study. It does involve interaction.

986
01:59:32,080 --> 01:59:35,980
And then our exemption category, here we are.

987
01:59:36,460 --> 01:59:38,560
So these are what have kind of recently changed.

988
01:59:38,560 --> 01:59:45,580
So these exemption categories are like this one here because we're doing interviews with adults only and they're sort of not.

989
01:59:47,190 --> 01:59:54,719
Sensitive topics, which one could think like interviewing people around their feeding behavior and raising child rearing is a sensitive topic,

990
01:59:54,720 --> 01:59:59,910
but it's not considered to be a sensitive topic by the center be. So that's where it's sort of an exception.

991
02:00:00,960 --> 02:00:06,320
Sorry. So that's like a long detour into the IAB, but just sort of like that's again,

992
02:00:06,330 --> 02:00:15,260
I also think this is a good example of like the clunkiness of any research because it's just not streamlined and beautiful and you know,

993
02:00:15,330 --> 02:00:19,060
yet it's the same where we spend a lot of time. Okay.

994
02:00:19,990 --> 02:00:24,010
And then the other website I wanted to just pull up, which is not a log in,

995
02:00:24,010 --> 02:00:28,780
but this is the one that I've shot before and this is like where you go to find stuff out.

996
02:00:28,870 --> 02:00:32,440
So that's always just good to remember and always Google Able.

997
02:00:33,190 --> 02:00:39,309
But it's like the compliance word here is kind of key to just be able to find this office.

998
02:00:39,310 --> 02:00:44,500
But also if you type like, you miss peers, research ethics, like you'll get there too.

999
02:00:46,330 --> 02:00:51,280
So this does have a lot of good information about all the different types.

1000
02:00:51,310 --> 02:00:53,350
It has a lot of like decision trees up here.

1001
02:00:53,350 --> 02:00:59,440
I'm thinking of doing this, that and like at the very end, if you need to actual actually call a physical human, you can also do that.

1002
02:00:59,950 --> 02:01:03,310
But there's a lot of travel here in terms of how to shape it.

1003
02:01:03,580 --> 02:01:07,270
That's the nature of regulation of your application.

1004
02:01:10,390 --> 02:01:13,660
Okay. So. Oops. Right.

1005
02:01:13,780 --> 02:01:18,440
Here we go. What was I going to say?

1006
02:01:18,460 --> 02:01:27,150
Oh, the other thing. Like, if you guys get an alert to, like, go in and accept your role on an application,

1007
02:01:27,160 --> 02:01:32,970
like, just take 5 minutes and do it because that can hold up applications for a long time.

1008
02:01:33,070 --> 02:01:36,910
Then the person's like, We're going to submit these students.

1009
02:01:36,910 --> 02:01:40,090
And it's not you guys. It's like the undergraduates usually like. Really?

1010
02:01:40,150 --> 02:01:44,350
That's actually an important email to respond to. So the caveat is like,

1011
02:01:44,350 --> 02:01:48,940
if you guys are working with undergraduate students or other students who are like on

1012
02:01:48,940 --> 02:01:53,649
you need to be on your RB application just like say you're going to get this email.

1013
02:01:53,650 --> 02:01:59,890
It's actually it's not it feels like a form email or like a, you know, spam email, but like you actually need to do that.

1014
02:02:01,180 --> 02:02:08,620
So I feel like that comes up a fair bit. Other stuff like this is more from the.

1015
02:02:08,650 --> 02:02:18,129
So that's all you have on these are just sort of like those guiding questions about where does my research proposal application go in the IAB?

1016
02:02:18,130 --> 02:02:28,450
And then this is just a link to the like federal guidelines around what we regulate research and we want to know like is it research,

1017
02:02:28,480 --> 02:02:32,740
is it this to generate generalizable data? Is it with human subjects?

1018
02:02:32,740 --> 02:02:35,830
How many of them, what age are they? Are they vulnerable?

1019
02:02:35,830 --> 02:02:40,870
Are they a special population that is protected as vulnerable, or are they sort of more generally vulnerable?

1020
02:02:41,410 --> 02:02:45,910
And how identifiable are they? And then is it exempt?

1021
02:02:46,450 --> 02:02:52,899
So these are these exemption categories that like you may not be able to always answer all these questions like right away.

1022
02:02:52,900 --> 02:02:56,260
But when you get into the IAB, there's always these like choice points.

1023
02:02:56,860 --> 02:03:00,969
Name is not it? It's like deciding, okay, what are we doing?

1024
02:03:00,970 --> 02:03:07,120
And that's like again, all of us have been mirror a little bit, but just like as you go into your own research plans.

1025
02:03:08,710 --> 02:03:16,540
It's impossible to like forecast everything but what you want to think about it has, I think, in a way that's helpful is like,

1026
02:03:17,650 --> 02:03:28,810
what do I need to know for the IAB application and how does that then inform our planning for kind of the research protocols that we're doing and.

1027
02:03:30,690 --> 02:03:37,980
How intensive. Like what's the balance between intensity of research protocol and the stuff we can

1028
02:03:38,160 --> 02:03:46,130
learn and like the ethics of doing so and the sort of subject burden of doing so.

1029
02:03:46,140 --> 02:03:50,070
So I feel like those are all like these interactive pieces and you go back and forth,

1030
02:03:50,070 --> 02:04:02,240
but kind of being aware from the get go of planning your whole research project, like what are the things that I need to be considering?

1031
02:04:02,250 --> 02:04:09,330
And like the way that I also think about it too is like you can propose a really amazing research project,

1032
02:04:09,780 --> 02:04:11,910
but then if it gets funded, you got to do it.

1033
02:04:12,270 --> 02:04:21,660
And so that's something that like some people, you know, say like it's great to get an r-1, but like, oh my gosh, then you can't do it.

1034
02:04:21,810 --> 02:04:29,850
So I think that's that balance of like what's feasible, what's meaningful and like what is, you know, equitable and ethical.

1035
02:04:32,650 --> 02:04:36,190
So and that's also a reason to start with like secondary data analysis if you can't.

1036
02:04:36,520 --> 02:04:38,080
Right. Or someone else's project.

1037
02:04:39,840 --> 02:04:46,320
I think, first of all, I don't know if they're all working and then this this is this like issue with like the students.

1038
02:04:46,740 --> 02:04:50,820
Like, we kind of need to have students that kind of, like, complete your peers training.

1039
02:04:51,330 --> 02:04:58,940
And there's that annoying thing where people who worked at places that accept the city training, you start to redo peers.

1040
02:04:58,950 --> 02:05:05,189
Now, at this point, that's also under discussion by that group about like, is there a way to ease this?

1041
02:05:05,190 --> 02:05:09,929
Because here what you have to do is people who are outside collaborators have to get,

1042
02:05:09,930 --> 02:05:13,469
let's call it your best friend account and that they do peers training,

1043
02:05:13,470 --> 02:05:19,600
which is exactly the same content to the city training, but they like don't have crosstalk with each other at this point.

1044
02:05:20,420 --> 02:05:24,200
So it's just one of these things like, I'm so sorry. I know we want to collaborate.

1045
02:05:24,830 --> 02:05:29,930
Can you spend 3 hours mucking around with this, like, really annoying learning system that is the peers training.

1046
02:05:31,100 --> 02:05:38,440
It's a requirement, and it's just one of those things. But they are working to make it a little more streamlined if.

1047
02:05:41,650 --> 02:05:46,120
Okay. And then. Let me see.

1048
02:05:46,140 --> 02:05:49,379
I just wanted to show you guys a few examples of consent form stuff.

1049
02:05:49,380 --> 02:05:56,610
But these next like so this is just I'll click through these really quickly because if we're doing an IAB application,

1050
02:05:57,690 --> 02:06:00,059
you'll know you'll still figure these things out.

1051
02:06:00,060 --> 02:06:06,480
But like, we need to know who the people are, that they're certified, what kind of application we're doing.

1052
02:06:06,900 --> 02:06:15,120
There's just, there's a lot of info here, who the sponsor is or like the funder, what happens here at U of M.

1053
02:06:15,450 --> 02:06:25,200
So that's important because that's driving these questions about like who is quote unquote, engaged in the research and why.

1054
02:06:25,200 --> 02:06:32,489
That's important to know is that, you know, kind of what we're just talking about, like these external IRBs, if it's a U of M IAB,

1055
02:06:32,490 --> 02:06:39,510
they want to know, like, what are we regulating and what sites are like listed, but we don't need to monitor as closely.

1056
02:06:40,230 --> 02:06:46,460
And so that's this issue of like, are we recruiting elsewhere or are we actually running protocols elsewhere?

1057
02:06:46,470 --> 02:06:47,930
Like for our diabetes study,

1058
02:06:47,930 --> 02:06:57,030
we ended up running some protocols in a hotel in Grand Rapids because there's like a U of M satellite campus there that does diabetes care.

1059
02:06:57,660 --> 02:07:02,160
And we just said it's much easier for us to go to Grand Rapids, but we, like, need a space.

1060
02:07:02,160 --> 02:07:07,320
So we like rented a spare hotel room and it was ridiculous, but it was a performance site.

1061
02:07:10,080 --> 02:07:14,920
And then clearly this is like what we're all most familiar with is like, what's our protocol, what's actually happening?

1062
02:07:14,940 --> 02:07:17,999
Are people going to the sleep lab or people giving their saliva?

1063
02:07:18,000 --> 02:07:23,520
Like, How are we storing it? What's our confidentiality of data, etc.?

1064
02:07:24,600 --> 02:07:30,450
And then we also have to lay out the benefits and risks of our protocols.

1065
02:07:30,540 --> 02:07:34,859
And like that, what we've been talking a lot about, like the compensation part,

1066
02:07:34,860 --> 02:07:42,360
it's not a benefit to the study participation, it's purely like a compensation for them participating.

1067
02:07:42,600 --> 02:07:47,670
But it is not a study benefit because then that would be like a coercion situation.

1068
02:07:49,170 --> 02:07:56,460
And it's always important to tell the participants really in an intervention study or for an observational study,

1069
02:07:56,910 --> 02:08:00,750
you may not benefit directly, but this is like for the good of science,

1070
02:08:00,930 --> 02:08:08,969
because even if they're in a clinical trial, a, they may not be an intervention arm and B, the intervention may not work or it could be harmful.

1071
02:08:08,970 --> 02:08:18,700
So it really may not benefit you directly. So like recruitment, you do need an excruciating level of detail.

1072
02:08:19,240 --> 02:08:25,479
You need like, you know, if you're recruiting online or on Facebook, what you've done.

1073
02:08:25,480 --> 02:08:29,260
Like you need to have sort of a we've we've I feel like we've finessed it.

1074
02:08:29,260 --> 02:08:32,680
I don't know if this is true and some of the research that you guys have been doing at recruiting.

1075
02:08:32,680 --> 02:08:41,379
But like we've gotten to the point where we say, like we may say a number of these general phrases and we're like, give them a bunch of phrases.

1076
02:08:41,380 --> 02:08:51,130
And then within those phrases, we kind of pick and choose what ends up on the Facebook ad or whatever it is so that,

1077
02:08:51,130 --> 02:08:57,130
you know, any like flier, for example, needs to be approved and images need to be approved.

1078
02:08:57,130 --> 02:09:02,740
And all of those things like we know why, because there's coercive language we can use or there's like images.

1079
02:09:04,420 --> 02:09:14,319
One of my colleagues does a lot of work on like media use in young kids, like social media, like mobile mobile phone use and stuff.

1080
02:09:14,320 --> 02:09:21,160
And she looks at like tracking of different apps. And so we were looking at this recruitment flier and the word tracking was on there.

1081
02:09:21,160 --> 02:09:27,910
I was like, I'm pretty sure that we're tracking on the flier. And it's more like you're looking at that from an app perspective.

1082
02:09:27,910 --> 02:09:31,600
But like from a recruitment fire perspective, it sounded like a little off.

1083
02:09:33,610 --> 02:09:41,079
And so I was like, Well, how can you, you know, be have integrity and honesty about like what you're studying, which is like parent child media use.

1084
02:09:41,080 --> 02:09:46,219
But, you know, then as they get interested in, in the study, then in the consent form,

1085
02:09:46,220 --> 02:09:49,900
you say, oh, you'll download this thing on your phone that can monitor the time you spend.

1086
02:09:50,350 --> 02:09:53,860
But it's not like we're going to track you from the ad. Right.

1087
02:09:55,920 --> 02:10:06,360
Informed consent. Of course. And then the other thing that even in like survey research, which is, you know,

1088
02:10:06,360 --> 02:10:11,070
one could argue like more straightforward than like the 24 hour lab sleep study.

1089
02:10:11,730 --> 02:10:16,890
You still need to provide like the questions that that that people are going to be answering.

1090
02:10:18,570 --> 02:10:24,780
And I would say that, like, again, if you guys are like in academia, research, career,

1091
02:10:25,920 --> 02:10:36,560
ultimately it's always kind of instructive to like sit in on some IAB reviews and and if you're asked to be an IAB reviewer at some point,

1092
02:10:37,050 --> 02:10:42,060
it is an interesting process because you get exposure like to those different kind of studies

1093
02:10:42,060 --> 02:10:48,090
that people do and you see like variation and like the level of detail that people provide.

1094
02:10:48,090 --> 02:10:55,930
And when they don't provide, like this obsessive level of detail, you get a sense of like, I don't know about your research.

1095
02:10:55,950 --> 02:11:00,030
Like, I don't know that you're actually have thought through these things enough.

1096
02:11:00,060 --> 02:11:03,900
And so I think that's why that IAB process, it is painful,

1097
02:11:04,890 --> 02:11:10,860
but it is really important for just like getting the study team to think through these

1098
02:11:11,220 --> 02:11:17,010
really tiny details because research is like also a lot about the tiny details.

1099
02:11:18,240 --> 02:11:26,160
And so I had served as an IAB reviewer at Brown for like a few years, like maybe four years,

1100
02:11:26,670 --> 02:11:32,820
and just sort of the range of different kinds of research protocols that come through to you just learn a lot.

1101
02:11:33,060 --> 02:11:35,760
And so you just are exposed to like a lot of things.

1102
02:11:37,710 --> 02:11:46,410
Right now in my current life, I'm much happier to be on like the overseeing board compared to like reviewing a ton of applications all the time.

1103
02:11:46,410 --> 02:11:56,130
But I think like Ritesh in this department is on the HSBC, IAB board and you know, so people do this like a great service to do what I would say.

1104
02:11:57,810 --> 02:12:01,380
For a few years. Let's see.

1105
02:12:01,710 --> 02:12:09,870
I wouldn't even. You guys don't need to write this down in your slides, but, like, these are all the different things that you need to address.

1106
02:12:09,990 --> 02:12:16,830
These are not like the sections of your consent form per se, but these are like all the things to address in your consent form.

1107
02:12:17,940 --> 02:12:19,710
And this, this is where I call that GA.

1108
02:12:20,430 --> 02:12:31,620
So if you guys are in the situation where you're on a research team, that is like starting up a new study with this RCR group kind of fun.

1109
02:12:31,620 --> 02:12:37,740
Like my and some other people suggestions are now doing what they call these like study start up consultations.

1110
02:12:38,780 --> 02:12:43,429
And I think I don't know if anybody any. Did you guys ever did they ever come and consult with you guys?

1111
02:12:43,430 --> 02:12:53,270
I don't know. But they there's some groups they kind of came up a few years ago around, like I said, just kind of in the middle, you know,

1112
02:12:53,270 --> 02:12:59,570
like we do some studies that really touch on that school territory and some that really are not and some that are even,

1113
02:13:00,170 --> 02:13:10,250
you know, like I would say, community based research in general is a little like out of scope of both IAB Med and IAB, HSBC.

1114
02:13:10,940 --> 02:13:17,839
I do agree. Like there's always stuff that like we want to do and we're doing and we're doing it for research purposes,

1115
02:13:17,840 --> 02:13:20,060
like to make generalized research knowledge,

1116
02:13:21,020 --> 02:13:29,059
but like it makes the Arby's head explode and that that issue around compensation of people who do not want

1117
02:13:29,060 --> 02:13:35,060
to give a Social Security number for lots of different reasons is like a key issue and it keeps coming up.

1118
02:13:35,060 --> 02:13:39,560
We kind of think we were like, Oh, we've got a solution. And then like another study will happen, like we need that, you know?

1119
02:13:39,560 --> 02:13:44,930
So they go through this whole process and that the payment people here are called each SIP.

1120
02:13:45,230 --> 02:13:52,770
That's like their acronym. And I can't remember what it stands for. Human subjects, incentives and incentive programs.

1121
02:13:52,770 --> 02:13:54,299
That's right. I think is interesting.

1122
02:13:54,300 --> 02:14:03,060
But they also did a thing a few years ago where they like change the type of gift cards you could give and kind of like you were saying to car like.

1123
02:14:04,890 --> 02:14:11,190
It just it messes with the research flow and it becomes more of a hassle for participants.

1124
02:14:11,700 --> 02:14:14,760
I think one of the things around that switch was maybe like.

1125
02:14:15,860 --> 02:14:22,610
The cards didn't renew or something like that. Like we always had these like reloadable visa gift cards and they're like, Oh, you can have that.

1126
02:14:22,610 --> 02:14:26,870
So there's always for these reasonable regulatory reasons.

1127
02:14:27,410 --> 02:14:30,170
But they like trickle down the individual research projects.

1128
02:14:30,920 --> 02:14:37,579
And I would say like this startup consultation thing, it's really great because then OCR gets feedback like,

1129
02:14:37,580 --> 02:14:41,989
this isn't working for this individual research project and we can't find out about

1130
02:14:41,990 --> 02:14:46,880
these really important things we need to know about because you're hampering us.

1131
02:14:47,000 --> 02:14:55,410
And like, let's figure out a way that we can be monitored and be ethical in a documented way that is not hampering the research.

1132
02:14:55,460 --> 02:14:58,820
So I think that this is a really good development.

1133
02:14:58,820 --> 02:15:01,340
And actually in the last call with this group,

1134
02:15:02,180 --> 02:15:08,749
they were just reviewing like we've done a bunch of these study startups and there's always something like how do we store our consent forms?

1135
02:15:08,750 --> 02:15:11,239
Or like, have you thought about this part of the protocol?

1136
02:15:11,240 --> 02:15:16,090
And like, I think they're learning more too as they talk to people who do more varied studies.

1137
02:15:16,100 --> 02:15:22,129
So I would say, like promote this among, you know, people who might be starting up studies.

1138
02:15:22,130 --> 02:15:29,690
And the international thing is an amazing question and I am sure they have not fully digested all the implications.

1139
02:15:31,730 --> 02:15:39,460
That work, even though it happens like I think it's probably kind of idiosyncratic like how it right is context to recent.

1140
02:15:40,420 --> 02:15:46,659
Yes. Yes. So I think there's like a lot a lot in that space.

1141
02:15:46,660 --> 02:15:51,470
But I love they're doing that. They're doing these. Let's see what else.

1142
02:15:52,600 --> 02:15:57,820
I just wanted to offer a few other things here. That's another just one of my things.

1143
02:15:58,180 --> 02:16:06,159
Consent forms. And so, again, like some of us, I'll just pull these two examples up.

1144
02:16:06,160 --> 02:16:11,549
But the range of different types of studies, there's consent forms, there are different types of studies.

1145
02:16:11,550 --> 02:16:14,620
So this is again, like this is a pretty simple study.

1146
02:16:14,800 --> 02:16:18,550
We interviewed parents and parents did some tasks.

1147
02:16:18,640 --> 02:16:22,690
It was a particular population, which is why it's so it's kids with type one diabetes.

1148
02:16:22,690 --> 02:16:27,700
So it went automatically to med school and we also reviewed the medical record.

1149
02:16:27,700 --> 02:16:33,380
So these guys were paid about what they were paid $50 and it was like a one time visit.

1150
02:16:33,400 --> 02:16:42,280
So like relatively very simple and it was like a relatively simple consent form.

1151
02:16:42,280 --> 02:16:50,499
But I think it's still like ten or 11 pages going through, you know, the purpose and who can participate and what you're going to do and etc., etc.

1152
02:16:50,500 --> 02:17:03,370
So you guys all see you can sometimes that's a sort of that's like an IAP met one and then another example here is like the more short form,

1153
02:17:03,870 --> 02:17:08,709
the same kind of idea but like not a specialized population but also kids.

1154
02:17:08,710 --> 02:17:12,940
So I always have kids, so that's always like a vulnerable population, but it's pretty common one.

1155
02:17:13,600 --> 02:17:20,739
And this is videotaping and I was eating food. So eating food is interesting because you have to specify what they're going to

1156
02:17:20,740 --> 02:17:25,180
eat and what they're offered and whether how you can manage allergies and choking.

1157
02:17:25,390 --> 02:17:30,700
Those are like the main things. And we also had weird measures, so here it was a little more involved.

1158
02:17:31,300 --> 02:17:37,030
They really came and eat a snack. Frankly, it's like what they did that the IRB like give us a little more hassle.

1159
02:17:37,030 --> 02:17:43,670
And this was just sort of a little bit more. Broad in terms of consent, for example.

1160
02:17:43,700 --> 02:17:49,969
So just so you guys have those examples and then I wanted to just kind of talk

1161
02:17:49,970 --> 02:17:54,260
a little bit more about like in this public health space and this is when some

1162
02:17:54,260 --> 02:17:59,719
of these like community based projects like just don't fit the mold of like

1163
02:17:59,720 --> 02:18:05,120
existing RV reviews and like what people know to think about per research ethics.

1164
02:18:08,060 --> 02:18:12,709
And so I think this kind of gets to that point of like when we think something works,

1165
02:18:12,710 --> 02:18:18,860
we want to get it in the community and how do we then test how it works in the community?

1166
02:18:18,860 --> 02:18:23,719
So like kind of like you've done in your works are like fidelity and adaptation.

1167
02:18:23,720 --> 02:18:32,420
Like how much, what are the key ingredients of why this thing worked in the first place and how much do we adapted and like the

1168
02:18:32,420 --> 02:18:38,630
research ethics kind of around that or like we want to get the community something that works and how can we do that?

1169
02:18:40,130 --> 02:18:50,050
The idea of control groups. Is often tricky because, you know, is it ethical to not do anything?

1170
02:18:51,220 --> 02:18:59,530
Is that treatment as usual? Like once we get into the community space and we're talking about like high risk vulnerable populations,

1171
02:18:59,530 --> 02:19:05,790
like I would argue it's usually not ethical to do nothing, but maybe you don't have the resources to do something.

1172
02:19:05,800 --> 02:19:12,520
And so that's really a questionable thing. And then around like sustainability, we've talked about that a bit.

1173
02:19:13,720 --> 02:19:19,690
But you know, is it ethical to come in and do like whatever they call a parachuting research and do your project differently?

1174
02:19:20,920 --> 02:19:25,940
So that's like those bigger ethical issues that are not research ethics regulated, right?

1175
02:19:25,960 --> 02:19:30,760
That's like a very, you know, back to that point of like overall ethical people.

1176
02:19:30,760 --> 02:19:37,450
We want to do what we can do, but it's not regulated. So like nobody is forced to do these things in an ethical way.

1177
02:19:39,340 --> 02:19:44,560
Industry partnerships, like just sort of same kinds of things to think about in these different contexts.

1178
02:19:44,590 --> 02:19:50,560
And when we're talking about screening studies, like what's the value of screening,

1179
02:19:50,680 --> 02:19:56,230
how do we connect people to resources if they're doing a study that involves screening for something, what do you do about it?

1180
02:19:58,090 --> 02:20:05,169
So there's that in terms of sort of prevention, science, and then this idea of just like everyday ethics.

1181
02:20:05,170 --> 02:20:13,090
So it's not research research ethics, but it is ethics, regular ethics.

1182
02:20:16,030 --> 02:20:28,180
And then I guess the other things I would say is maybe just two more things about like I, b, I, RB and CBP are like stumbling over it, but.

1183
02:20:30,290 --> 02:20:35,179
There are these requirements in the air be reviewed space that could be seen

1184
02:20:35,180 --> 02:20:40,879
as kind of conflicting or at least like having tension with a true like CBP,

1185
02:20:40,880 --> 02:20:52,220
our approach or community support as I'm talking, but like who is engaged in the research?

1186
02:20:53,240 --> 02:20:59,959
And I have always found it's a tricky balance to both say, like community partners,

1187
02:20:59,960 --> 02:21:07,520
we are valuing your engagement in this research enough to like spend a lot of time on it, but.

1188
02:21:09,490 --> 02:21:20,140
It's kind of an ask to ask you to get a you have a friend account and go through 3 hours of peer's training and be listed on the IAB.

1189
02:21:22,300 --> 02:21:32,280
What is valuable to you about that? But if your site is engaged, like with a capital E in research, from Abby's perspective, then you need to do that.

1190
02:21:32,290 --> 02:21:42,279
And I think institutional agreements like there are these really formal things about working with the university that ultimately can be beneficial,

1191
02:21:42,280 --> 02:21:47,230
especially if you broke that partnership to be sustainable, to be sustained over time.

1192
02:21:48,130 --> 02:21:54,010
That maybe is worth our worth like doing this paperwork, but it's definitely a process.

1193
02:21:54,010 --> 02:21:58,000
So I don't know if others have found this, but like with working with community partners.

1194
02:21:59,660 --> 02:22:03,970
You know, some people I've worked with have been like, sure, yeah, we'll do these agreements.

1195
02:22:03,980 --> 02:22:10,850
You know, we do them with other people. That's fine. Some people are like, Oh my gosh, I totally don't want to be involved in any of that stuff.

1196
02:22:11,120 --> 02:22:15,170
You can come and recruit people. That's fine. I'll give you a letter, but like, don't make me do anything else.

1197
02:22:15,200 --> 02:22:25,729
So I think it's it's it's like kind of it's maybe it's okay that it's unregulated space, but it is a little tricky, too,

1198
02:22:25,730 --> 02:22:30,440
because I think that like on a kind of going towards the community side,

1199
02:22:30,830 --> 02:22:35,790
like you want to also make sure they're doing things that are ethical in the research.

1200
02:22:35,810 --> 02:22:42,320
So for example, we had this woman who is an amazing recruiter at one of our sites,

1201
02:22:42,500 --> 02:22:46,520
just amazing and an awesome like service provider, all these things.

1202
02:22:47,150 --> 02:22:52,010
But you know, and we've gone through like, okay, this is how we're going to consult people for the research.

1203
02:22:52,010 --> 02:22:55,489
But then when she was doing the research consenting,

1204
02:22:55,490 --> 02:23:03,500
it was very clear it was happening differently with like low fidelity to sort of what we had initially laid out in terms of research consent.

1205
02:23:03,980 --> 02:23:08,209
And so it was much more like, Oh, good to be part of this program.

1206
02:23:08,210 --> 02:23:11,500
You're going to participate in this study. And this is kind of what everybody does.

1207
02:23:11,520 --> 02:23:16,579
I think it just wasn't like laying out all that, like, you can stop participating at any time.

1208
02:23:16,580 --> 02:23:19,430
It doesn't impact the services that are getting from the agency.

1209
02:23:19,430 --> 02:23:25,070
And I remember thinking like, wow, like so many people are participating when she's recruiting.

1210
02:23:25,850 --> 02:23:31,399
And it was basically because she was like really going off script and not like

1211
02:23:31,400 --> 02:23:34,940
kind of letting people have an out if they didn't want to do the research part.

1212
02:23:35,000 --> 02:23:42,680
So I think there's this kind of like both directions of training and sort of thinking about like there's,

1213
02:23:43,310 --> 02:23:48,440
you know, a lot of like community team participation is so important,

1214
02:23:49,010 --> 02:23:58,850
but then kind of like how it all plays out and is still under the guise of IAB regulated research, ethical, ethical research.

1215
02:23:58,850 --> 02:24:01,910
That's there's like a lot of process around it, I think.

1216
02:24:04,670 --> 02:24:07,309
I think this is another important thing in life, again,

1217
02:24:07,310 --> 02:24:18,740
with people serving on either these community engaged research is not like a protected status that you have to have somebody on the IAB to review for.

1218
02:24:20,210 --> 02:24:24,680
But I think that people who don't do any kind of community based research and

1219
02:24:24,680 --> 02:24:27,950
are reviewing these proposals that are proposing recommend based research,

1220
02:24:27,950 --> 02:24:36,139
there's like a mismatch potentially. And, you know, again, like it's kind of an emerging space,

1221
02:24:36,140 --> 02:24:44,270
but sometimes you'll have a specific community focused like an NIH are doing a community based call.

1222
02:24:44,540 --> 02:24:48,680
They're going to have reviewers that like review the science of community based research.

1223
02:24:49,100 --> 02:24:53,959
But at like an individual institution level, you may or may not have IAB expertise.

1224
02:24:53,960 --> 02:24:58,840
And so then you get in that situation where I feel like the IAB is asking us to do these 50

1225
02:24:58,880 --> 02:25:03,860
things and then it just becomes not feasible in the study in a context that you proposed.

1226
02:25:05,770 --> 02:25:09,100
So I think that's important. What else?

1227
02:25:10,340 --> 02:25:14,960
I think these are kind of our basic research things. We kind of still need to make sure they're happening.

1228
02:25:17,570 --> 02:25:25,190
The other thing I think I'd just say is that this, you know, at the basis of cdpr is like.

1229
02:25:26,260 --> 02:25:31,780
Sort of. We don't know where we're going yet with this research and it's going to emerge.

1230
02:25:32,290 --> 02:25:36,900
And so like. Exactly.

1231
02:25:37,240 --> 02:25:45,100
Exactly. So you do like you know, you can do like a phased review process or like a promissory note.

1232
02:25:45,100 --> 02:25:51,399
And sometimes I think what happens more and more is that if probably any of our system you've done this,

1233
02:25:51,400 --> 02:25:56,770
but you can do what's called like an umbrella IRP that's sort of like we're going to do a project.

1234
02:25:56,890 --> 02:26:01,870
We aren't planning on collecting data any time soon, but it got funded.

1235
02:26:01,870 --> 02:26:06,340
We're going to do a project, we're submitting this umbrella, IRP.

1236
02:26:07,840 --> 02:26:13,540
These are the people that will be involved. These are the places that we know so far will be involved.

1237
02:26:14,740 --> 02:26:18,550
And that's enough for the funding agency to award the funds.

1238
02:26:19,000 --> 02:26:22,060
And then you can kind of dive into what else you're going to do.

1239
02:26:23,330 --> 02:26:31,580
And I feel like that covers things like focus groups, formative research, you know, like not focus groups for which you will use the data.

1240
02:26:31,970 --> 02:26:38,030
Some of those might be examples. So like there's all these sort of like different ways to work.

1241
02:26:38,990 --> 02:26:42,320
On this. But I do think it's think it's tricky.

1242
02:26:42,410 --> 02:26:51,050
And I think talking to people who put in areas that involve community engaged players and research is important.

1243
02:26:53,680 --> 02:26:59,410
I don't know any insights from. He's.

1244
02:27:04,620 --> 02:27:13,210
Much of the various. He says.

1245
02:27:16,200 --> 02:27:22,070
Like, for example, we've been talking to. Detective work and hard questions are such that.

1246
02:27:24,030 --> 02:27:26,500
In the context of the in Sydney and not actually.

1247
02:27:31,490 --> 02:27:37,730
Not very good at that, which is different things that yeah, I seem to be like very specific with how you were in here.

1248
02:27:40,410 --> 02:27:46,240
Doing that. Between the questions.

1249
02:27:47,700 --> 02:27:59,850
How does your school. Yes.

1250
02:28:00,420 --> 02:28:06,180
And I think that was kind of one day in my head. That was probably really good practice, but not regulated.

1251
02:28:07,260 --> 02:28:11,950
And then my character in the world of making a decision at the time that it.

1252
02:28:13,620 --> 02:28:19,200
So that's not right. Just here to find out about the process and how things work.

1253
02:28:19,260 --> 02:28:23,590
Right now, I'd like more of the education revolution.

1254
02:28:26,970 --> 02:28:31,950
Versus. Doing something that is research, but it's a concept.

1255
02:28:37,340 --> 02:28:41,720
I feel like it's a really good example because it's kind of like it was just.

1256
02:28:41,970 --> 02:28:51,020
Hinges on very small. At first the focus group.

1257
02:28:53,560 --> 02:28:57,940
Yeah. Versus like very focus groups. I feel like we. In the middle.

1258
02:28:57,940 --> 02:29:07,610
I feel like we had one that was about. We interviewed clinicians about what they thought about screening for social determinants of health.

1259
02:29:08,810 --> 02:29:15,120
I can't remember exactly how I feel about our discussion, but I can remember it was probably.

1260
02:29:15,850 --> 02:29:19,300
Happens in this clinic. But like we're talking to.

1261
02:29:20,120 --> 02:29:24,250
I know he did have questions about. But what's.

1262
02:29:27,720 --> 02:29:32,390
If it was properly doctored, it should have been examined.

1263
02:29:33,180 --> 02:29:44,580
It's not regulated. I can remember that they're talking and they're like looking after the like the head of an organization you're talking about.

1264
02:29:49,950 --> 02:29:53,010
And that would probably be exactly that. Right.

1265
02:29:54,910 --> 02:29:59,629
Either. Yeah.

1266
02:29:59,630 --> 02:30:05,720
It is hard. And it's hard to because often like you were.

1267
02:30:06,990 --> 02:30:12,630
Support him. Questions? Your role. How does this happen here?

1268
02:30:12,640 --> 02:30:22,030
So. Like it's funny because like when I think about like things that are not radio, I do know what that's like.

1269
02:30:24,020 --> 02:30:28,370
Not not offensive to have to give a ton of detail on.

1270
02:30:44,490 --> 02:30:51,740
I. Yeah, yeah, yeah. Well, and the flip side, too, because like in our, like in our diabetes study,

1271
02:30:51,740 --> 02:31:00,139
for example, we run, we were like with cutoff at 17.99998 because then when they,

1272
02:31:00,140 --> 02:31:10,549
if they turn 18, then you might have to rethink some of them as adults and to have sort of different applications for lots of things.

1273
02:31:10,550 --> 02:31:20,720
And so I think crossing over that line. 818 is likely to continue to research the domain at least.

1274
02:31:22,380 --> 02:31:28,970
That's one. Created much difficulty still. And I think we did determine because some kids didn't turn 18.

1275
02:31:29,880 --> 02:31:39,720
Across the course of the study. But if their participation had ended later in the year, follow up data from their chart, that didn't matter.

1276
02:31:40,230 --> 02:31:53,400
But when they couldn't participate in what seemed to be. I understand as well.

1277
02:31:53,440 --> 02:31:57,060
And then we didn't really talk about this, but like consent and consent.

1278
02:31:58,510 --> 02:32:02,020
And there are these guidelines of like, you know, really even.

1279
02:32:02,210 --> 02:32:07,300
And again, like I would like but these young kids and we're asking them to do things.

1280
02:32:07,750 --> 02:32:12,590
And if they say, you know, that's technically they've not given their assent.

1281
02:32:12,640 --> 02:32:15,910
So, like, you have to be, again,

1282
02:32:15,910 --> 02:32:21,010
like balanced and mindful about how you're approaching like a three year old

1283
02:32:21,010 --> 02:32:27,100
who's having like a cranky day and they just don't want to do it with like,

1284
02:32:27,550 --> 02:32:34,090
you know, an authoritative, like, okay, great. Now we're going to go in this room, sit down here, as opposed to saying, Will you come?

1285
02:32:34,570 --> 02:32:38,950
Will you sit down here like that? So it's the same thing with like that wording of like.

1286
02:32:40,060 --> 02:32:47,010
Are you keeping the participant, the child participant with you for the protocol and the parent has consented.

1287
02:32:48,200 --> 02:32:53,240
But there's so many different points of protocol that the child might be like, I don't wanna do that and I need to do it.

1288
02:32:54,410 --> 02:32:59,460
So. But that's a good example.

1289
02:32:59,610 --> 02:33:02,759
We did these all these interviews with him before.

1290
02:33:02,760 --> 02:33:08,640
These things kind of got exempt. I feel it was used to be not very common to have this exempt stuff.

1291
02:33:10,560 --> 02:33:15,629
And we did all these interviews with like heads of Head Start agencies about like

1292
02:33:15,630 --> 02:33:20,400
their policies and just kind of getting into like the detail of the policies.

1293
02:33:20,910 --> 02:33:23,910
But then we had sort of a separate section about what do you think?

1294
02:33:24,030 --> 02:33:33,210
Blah, blah, blah. But I think that that's a good example of not regulated per se, exempt, which always like to think about tweaking.

1295
02:33:37,230 --> 02:33:40,540
For the project. Oh.

1296
02:33:45,750 --> 02:33:49,360
The not. Yeah.

1297
02:33:50,090 --> 02:33:59,100
Yeah. And we know from your study you.

1298
02:34:06,660 --> 02:34:11,820
Oh, yes. And getting certificates of company confidentiality.

1299
02:34:12,450 --> 02:34:24,110
If you're asking high schoolers or middle schoolers about like drug use or anything like that, it's a really messy irony situation.

1300
02:34:24,700 --> 02:34:31,840
And yeah, like drawn out and for good reason because you don't want to be the study where the child was like,

1301
02:34:32,770 --> 02:34:39,489
you know, I don't know had something going on. The parents never knew that the child's, you know, like also, of course,

1302
02:34:39,490 --> 02:34:45,040
really good reason to have in your study, you know, good plan for mental health crises.

1303
02:34:46,320 --> 02:34:52,640
In places like. You can text the pie and they will get back to you within a half an hour kind of plan,

1304
02:34:53,390 --> 02:34:57,920
kind of like the, you know, going on vacation with the mouth, with the mice.

1305
02:34:58,850 --> 02:35:05,540
If you're a pi of a study that has ongoing data collection and something happens like you need to tag something, you know,

1306
02:35:05,690 --> 02:35:08,560
if you're out of town and unreachable and you know, you will be unreachable,

1307
02:35:08,570 --> 02:35:13,810
you have to have a person who knows they're responsible for making that call in your absence.

1308
02:35:13,820 --> 02:35:17,090
So. That's certainly something to think about.

1309
02:35:17,320 --> 02:35:24,020
And those are everything like child abuse, you know, mental health, crisis of the individual, etc.

1310
02:35:24,020 --> 02:35:28,310
So those are all like really significant pieces.

1311
02:35:30,030 --> 02:35:33,690
Of hire be and and protocol planning.

1312
02:35:36,120 --> 02:35:40,560
I think that. Oh, I was going to also just say one other thing here, too.

1313
02:35:40,590 --> 02:35:49,079
This is sort of broader. But this thing around screening what if you can't like and control groups it's like if

1314
02:35:49,080 --> 02:35:52,920
you can't provide services if you're working with people who don't have a mental health,

1315
02:35:52,920 --> 02:35:59,729
prevent like where is your sort of ethical obligation end with your participants and how much

1316
02:35:59,730 --> 02:36:04,469
can you kind of provide to your participants that's like beyond the scope of your study.

1317
02:36:04,470 --> 02:36:13,130
And so. You know. These clinical screenings, which is where a lot of the social needs screening has kind of emanated from.

1318
02:36:15,010 --> 02:36:20,820
They're sitting in a clinical setting where people may be more likely to connect with services more easily.

1319
02:36:20,830 --> 02:36:26,560
But if you're doing it kind of in a research context, you're asking about sensitive issues.

1320
02:36:26,860 --> 02:36:31,990
You know, this is kind of let's just talk about but like, I'm prepared to offer resources and do you have a plan?

1321
02:36:33,910 --> 02:36:39,190
And then is the person prepared for you to share this information?

1322
02:36:39,190 --> 02:36:44,990
And that should be in the consent form. So, like. You know, this information is private and confidential.

1323
02:36:45,010 --> 02:36:51,129
The only circumstances under which I would share this information is if you disclose that you might hurt yourself or other people.

1324
02:36:51,130 --> 02:36:56,020
And I would have to tell your mom or I would bring in someone to talk to you about that.

1325
02:36:56,470 --> 02:37:00,600
And so that's. Critical Consent form element two.

1326
02:37:01,500 --> 02:37:07,980
But I do think it goes beyond that like individual self or other harm to this broader peace around.

1327
02:37:08,700 --> 02:37:14,259
I'm working with this population who you know. Her vulnerable for whatever reason.

1328
02:37:14,260 --> 02:37:19,270
And I need to sort of figure out to give her a tailored resource sheet for their community kind of thing.

1329
02:37:23,170 --> 02:37:26,310
What else? I feel like that's the most important thing.

1330
02:37:26,370 --> 02:37:29,740
I think I have a little bit info on, like, research misconduct, but I think.

1331
02:37:32,590 --> 02:37:38,440
Yeah. Any thoughts about this? I feel like this is such a big issue and I don't.

1332
02:37:38,530 --> 02:37:46,840
Again, it's like not to be regulated. Like some of these like individual pieces are be regulated, but like beyond, you know.

1333
02:37:48,350 --> 02:37:54,350
I don't know. There's actually, I should add, there's there are all these new like kid requirements.

1334
02:37:56,240 --> 02:37:59,840
I should add that stuff in here about the army because.

1335
02:38:02,380 --> 02:38:06,700
In my work. We do like a ton of videotaping of kids doing these weird protocols,

1336
02:38:06,700 --> 02:38:10,780
like eating and trying to get into boxes that are locked and all these just weird things.

1337
02:38:12,430 --> 02:38:21,610
But, you know, typically, you know, if you don't have like a nice one in a one way mirror that like we've all seen in sort of those psych studies,

1338
02:38:22,570 --> 02:38:25,570
how do you monitor what's happening in the room with the kid?

1339
02:38:26,050 --> 02:38:30,940
And that's why choking is such an issue with eating studies because like, if you give them Eminem,

1340
02:38:30,940 --> 02:38:37,780
so they're waiting for him at that waiting task and they can eat the M&Ms, they can choke on the evidence and nobody's looking.

1341
02:38:37,780 --> 02:38:41,050
And then it could be like choking and dying and somebody is outside the room.

1342
02:38:41,080 --> 02:38:45,790
So now there's it's just put it into place some additional.

1343
02:38:47,710 --> 02:38:50,470
Safety safeguards for working with kids.

1344
02:38:50,470 --> 02:38:58,840
And one of them is like, you have to always have like three people kind of in the monitoring of child child data collection.

1345
02:39:00,170 --> 02:39:03,430
So that's not like an adult and a kid. Only together in a room.

1346
02:39:03,440 --> 02:39:09,620
So. It's tricky. If you videotape, then it's okay to have one person.

1347
02:39:10,460 --> 02:39:15,220
There's a lot of just interesting. New things that people are thinking through.

1348
02:39:15,220 --> 02:39:18,550
So that's been just like in the last two years.

1349
02:39:21,830 --> 02:39:26,990
So yeah, I don't know. But it's also there's a lot to think about.

1350
02:39:27,440 --> 02:39:34,220
I always think I was tweak this thinking about this session because like there's just so many different.

1351
02:39:36,320 --> 02:39:43,770
Good. Think, to talk about the importance of adverse events and how to talk about your being.

1352
02:39:43,960 --> 02:39:48,110
Yeah. Yeah.

1353
02:39:49,340 --> 02:39:53,330
So, yeah, you know, that's so funny, I think.

1354
02:39:54,460 --> 02:39:58,030
I definitely could add that and I would say like Oreo.

1355
02:39:58,030 --> 02:40:04,120
So those are the terms of like basically like a protocol violation for anyone who Oreo stands for.

1356
02:40:05,590 --> 02:40:15,610
Other reportable incidents so vague, but like things that happen that are just not as exactly out of your control,

1357
02:40:15,700 --> 02:40:22,140
that like we've had things like I think people were like paid off sketchy.

1358
02:40:22,730 --> 02:40:28,410
Or, you know, somebody didn't want to do a questionnaire especially early, so.

1359
02:40:29,710 --> 02:40:39,160
There might be sort of what I would see as like pretty innocuous. Events that are otherwise specified will happen.

1360
02:40:40,270 --> 02:40:48,280
But then the adverse events and after reports from the diabetes study lead to an adverse event.

1361
02:40:49,200 --> 02:40:54,840
It is basically anything that happens to your participants during the time period that they're participating in your study,

1362
02:40:54,870 --> 02:41:01,410
even if it doesn't have anything to do with your study and your brain, go through that decision process about.

1363
02:41:02,800 --> 02:41:07,930
How serious was this event? And that was it. What is the likelihood that it was something related?

1364
02:41:08,320 --> 02:41:13,200
So. Like, for example, these kids this way.

1365
02:41:13,370 --> 02:41:15,260
We had one kid who.

1366
02:41:16,590 --> 02:41:24,569
This could add to adverse events that were totally unrelated and one was like a broken arm and I think one was like a sprained ankle.

1367
02:41:24,570 --> 02:41:28,350
I think there are two at limb injuries for this kid.

1368
02:41:28,980 --> 02:41:38,070
And so I think even the reason that we knew about it is we were reviewing the medical records because the family didn't tell us.

1369
02:41:38,640 --> 02:41:43,740
But we saw the medical record and it was during the time period of our study, which was an intervention study.

1370
02:41:43,800 --> 02:41:46,860
So it was like unrelated to this diabetes.

1371
02:41:48,110 --> 02:41:58,139
But. The adverse events that. I came to understand some of the other things that we have in that study,

1372
02:41:58,140 --> 02:42:04,230
that the child would go to the hospital for diabetes related events and that we had to.

1373
02:42:05,500 --> 02:42:11,880
Making judgments and justifications. I suspect that this type of action.

1374
02:42:20,060 --> 02:42:24,200
And if you have examples. Our research center.

1375
02:42:29,390 --> 02:42:32,770
We weren't allowed to do anything was due to the lack of.

1376
02:42:36,690 --> 02:42:40,190
And they would offer somebody who was going to be behind us.

1377
02:42:41,850 --> 02:42:45,680
I tried to be kind of like just sort of running less in a childlike.

1378
02:42:46,990 --> 02:42:52,820
Confidentiality. Another study.

1379
02:43:08,500 --> 02:43:12,870
Pretty sure prior to that the other day that Tucker had first.

1380
02:43:16,270 --> 02:43:23,580
Reporter. Four things that morning that has heard that.

1381
02:43:26,790 --> 02:43:32,890
That's crude, but like. Living in poverty for.

1382
02:43:33,800 --> 02:43:44,000
Right. Apparently he walked in.

1383
02:43:44,660 --> 02:43:50,930
See. Absent evidence of drug use.

1384
02:44:00,870 --> 02:44:19,630
He's. Distressing situation. Covering the House and the Senate is Asian-American.

1385
02:44:19,780 --> 02:44:27,100
And she was just like, I'm so stressed out and kind of well because I thought that the person.

1386
02:44:28,820 --> 02:44:36,000
Have. Yeah. She said she sat in the trenches like I do.

1387
02:44:37,200 --> 02:44:40,590
You know that understand my experience because it's so stressful.

1388
02:44:41,550 --> 02:44:50,770
We had a lot of procedure before the toddlers study. And so the toddler is kind of flying around, you know, how to think about.

1389
02:44:52,570 --> 02:44:59,770
At the time, it was like, you know, running and somewhere like, you know, that could it didn't end up not nothing happened.

1390
02:44:59,900 --> 02:45:07,880
America's assessment left the house. And then we had a lot of or at least one of.

1391
02:45:11,300 --> 02:45:16,720
It has been acknowledged, but has apparently nearly all that was left behind all the time.

1392
02:45:18,290 --> 02:45:22,430
How could the toddler have grabbed at it? But then he didn't.

1393
02:45:28,560 --> 02:45:35,430
So. It wasn't like it was time to take control of my hands.

1394
02:45:36,190 --> 02:45:45,250
But a two year old. So. Yeah. So we have situations like that, like we had I think this was the same person when she came to the house.

1395
02:45:47,150 --> 02:45:51,900
My six year old daughter, very clearly the parent.

1396
02:46:00,200 --> 02:46:07,350
Republicans have called. Louisville, Kentucky.

1397
02:46:13,200 --> 02:46:28,450
He doesn't mean it. So when you call in and you call the police, or no matter what you call the CPS.

1398
02:46:28,630 --> 02:46:32,680
CPS. Yeah. So I think that we all.

1399
02:46:34,030 --> 02:46:42,839
I think it was one of these situations where like. I need to be able to be mindful of the fact that I'd like sucks.

1400
02:46:42,840 --> 02:46:47,530
Different things out of sentiment is actually very dependent on.

1401
02:46:53,040 --> 02:46:56,640
Like from fishing. But in that case, I.

1402
02:46:56,890 --> 02:47:05,080
I was like a neighbor. But then she went to her and then we had this constraints was like, Oh, who are you?

1403
02:47:05,170 --> 02:47:08,370
Why are you not? You know, this is a pressure to share like.

1404
02:47:10,330 --> 02:47:18,000
It was just like a lot of there's been some kind of favoritism either and associates she was like the neighbors like tax.

1405
02:47:19,620 --> 02:47:23,970
He has about 77 arrangements. You know, I never had a relationship.

1406
02:47:24,280 --> 02:47:30,290
Parents and I think the church. So it's just like I want certain.

1407
02:47:31,370 --> 02:47:34,460
You sit here so I never have worry.

1408
02:47:38,740 --> 02:47:43,750
And I don't know if Afghanistan is dying and how they feel about it. And it was my view of the situation.

1409
02:47:47,790 --> 02:47:53,080
Her dad. The face of these ambiguous landmark situations had.

1410
02:47:54,590 --> 02:48:00,650
We also understand. Yes. Thank you.

1411
02:48:02,480 --> 02:48:06,690
Thinking about what? Yes.

1412
02:48:10,900 --> 02:48:21,490
Family values in that. At the same time, I a research assistant.

1413
02:48:22,860 --> 02:48:27,390
Just because you call doesn't mean that. It's going to happen.

1414
02:48:27,940 --> 02:48:30,970
But that's the only way for now course I that links.

1415
02:48:32,450 --> 02:48:38,040
Is that the. And also.

1416
02:48:40,950 --> 02:48:49,170
He decides. I. Decision.

1417
02:48:58,390 --> 02:49:02,430
Families are. But is totally.

1418
02:49:10,650 --> 02:49:24,370
But no. So that.

1419
02:49:33,540 --> 02:49:37,290
Do you think that's another place where like all of the is.

1420
02:49:39,330 --> 02:49:43,080
Like we have every idea that, you know, you haven't seen, like, your plan.

1421
02:49:47,130 --> 02:49:55,180
What tends to come out that like, you know, like the troubles that you've done. Hopes that something has not been generated right now.

1422
02:50:00,080 --> 02:50:06,390
We had mentioned that actually in terms. Waiting for that response.

1423
02:50:25,380 --> 02:50:28,720
Anything else for a time. Close.

1424
02:50:29,140 --> 02:50:32,170
They're just so. But.

1425
02:50:32,170 --> 02:50:37,530
Yeah. Well, let's see. Next time we'll have Christine and.

1426
02:50:38,310 --> 02:50:42,000
We'll talk about peer reviewing stuff.

1427
02:50:42,000 --> 02:50:48,240
And so hopefully that's going to be some good timing for. You know, for papers.

1428
02:50:50,130 --> 02:50:56,070
And then there is the freedom manual. And again, this is like this been a discussion of like.

1429
02:51:00,550 --> 02:51:04,500
But any questions about whatever you can talk about?

1430
02:51:05,210 --> 02:51:09,940
So I think so because I got her for like an hour or so.

1431
02:51:10,060 --> 02:51:17,090
And then we had. Sure.

1432
02:51:27,850 --> 02:51:35,630
All of these like research from groups contact that there's just there's just a lot there's a lot to consider.

1433
02:51:38,230 --> 02:51:42,790
It's like we don't know what we're going to encounter, you know?

1434
02:51:42,790 --> 02:51:52,390
And then like, it's sort of like also I think as a people can get in their comfort zone and then like your research changes and you're like,

1435
02:51:52,420 --> 02:51:55,860
I don't know anything. What, what, what do I do in this kind of research study?

1436
02:51:55,870 --> 02:52:00,850
You know, it's like very different kind. Like different groups.

1437
02:52:04,170 --> 02:52:08,260
Yeah. Never directly. Oh, yeah, I.

1438
02:52:09,490 --> 02:52:18,520
You. That doesn't mean it's going to be.

1439
02:52:20,300 --> 02:52:25,400
Because it know that they're there to fight three years before they go home tonight.

1440
02:52:28,160 --> 02:52:34,830
But the thing is that. Yeah. And I think, you know.

1441
02:52:37,100 --> 02:52:44,870
Like. So I don't know the one that's for so long right now.

1442
02:52:48,950 --> 02:52:52,670
Got one that is helping people that are.

1443
02:53:03,420 --> 02:53:08,570
The ones that. So to be.

1444
02:53:08,610 --> 02:53:16,400
But I don't know that that is the foundation. But I imagine it.

1445
02:53:20,590 --> 02:53:24,790
I want to be good to see that all that stuff stirred up.

1446
02:53:26,220 --> 02:53:32,980
How about. Yes.

1447
02:53:33,000 --> 02:53:45,730
Beautiful time in history right now. Right.

1448
02:53:46,110 --> 02:53:54,380
Maybe we'll have to talk about like international research, too, because I hear from Dr. David, I should ask I to.

1449
02:53:55,890 --> 02:54:00,510
Yeah, I know, I know. I always kind of do these all these sessions around.

1450
02:54:00,510 --> 02:54:04,840
And I used to have, like, conflict of interest.

1451
02:54:05,550 --> 02:54:11,400
I think it's time that we all kind of came together, but it's like, so much, like, kind of like spreading out now.

1452
02:54:11,550 --> 02:54:16,450
There are different. But.

1453
02:54:19,080 --> 02:54:29,340
There's a lot. But yes, it's good enough that.

1454
02:54:30,900 --> 02:54:33,960
He was so obsessed. I think that was one of those books.

1455
02:54:34,710 --> 02:54:38,740
And so everything I read in front of the key centers like that, I'm just like, super.

1456
02:54:43,790 --> 02:54:47,630
Yeah, but it was good.

1457
02:54:50,310 --> 02:54:57,800
Yes. Oh. So at the beginning of the documentary based on the book as well.

1458
02:55:00,020 --> 02:55:03,350
The theory is that. With more fiction.

1459
02:55:06,150 --> 02:55:12,410
And that is. Well, you said that's the C.

1460
02:55:12,530 --> 02:55:19,360
C, Fred or Siegfried or whatever. Okay. If you completely.

1461
02:55:21,870 --> 02:55:28,440
Oh, yes, she was very particular. She had this sort of feeling of not so much in Pakistan and the drought.

1462
02:55:31,480 --> 02:55:43,930
I think it was. Yeah.

1463
02:55:45,950 --> 02:55:52,760
That's pretty cool.

1464
02:55:53,930 --> 02:55:57,260
I know. I was there one day in my grad.

1465
02:55:58,960 --> 02:56:05,370
Seriously considering like an esoteric topic, like some kind of development or others.

1466
02:56:05,410 --> 02:56:09,810
Remember, like, this is some place. You're so serious.

1467
02:56:09,840 --> 02:56:12,650
You just don't. You know everything. Oh, no.

1468
02:56:12,660 --> 02:56:18,459
But I have this whole theory of, like, in the West, you know, like the history of research or some, like, research.

1469
02:56:18,460 --> 02:56:24,510
And I think something like that was like altered felt like I have this whole curriculum that I pointed out to,

1470
02:56:24,510 --> 02:56:30,569
to I think research methods need to be like kind of into topics research because it's really true.

1471
02:56:30,570 --> 02:56:35,580
It's like these classical documentaries, primaries.

1472
02:56:36,180 --> 02:56:41,840
I don't know if you guys saw the one about. Except for the three brothers.

1473
02:56:43,950 --> 02:56:48,620
In a recent documentary, Romney said, All these things I want to do.

1474
02:56:51,070 --> 02:56:55,550
Yeah. They were separated. Yeah. Yeah.

1475
02:56:55,850 --> 02:57:00,169
Like all participating countries are sick of being separated.

1476
02:57:00,170 --> 02:57:03,940
And I think that, you know, based on the resources that.

1477
02:57:05,810 --> 02:57:12,810
He did it in an armored. Somebody said, My life is almost like a desert.

1478
02:57:14,380 --> 02:57:21,510
And it was like that. Rather than your life was just like doing, which is like being connected with that.

1479
02:57:21,740 --> 02:57:25,760
Yeah. Yeah. So.

1480
02:57:27,500 --> 02:57:35,920
Well, be fun to watch more films. Cool.

1481
02:57:35,930 --> 02:57:58,930
So we were going to fix the perfect. They don't have.

1482
02:58:08,460 --> 02:58:09,370
All right. So.

