1
00:00:02,520 --> 00:00:10,829
Good morning, guys. Well, last year we we had what we were talking about, matrix algebra.

2
00:00:10,830 --> 00:00:18,750
We had a few slides left. But instead of just going over his lines,

3
00:00:18,750 --> 00:00:27,750
I think it's probably better if we talk about a logical linear regression and then whenever we have some results,

4
00:00:31,650 --> 00:00:35,220
then we can go back to the long few slides.

5
00:00:35,610 --> 00:00:39,450
That's probably a better way is how we decided.

6
00:00:39,500 --> 00:00:46,620
Just we're just going over a lot of these slides without seeing the application of any of those results.

7
00:00:49,680 --> 00:00:55,140
So in today's lecture, we are going to move on to modularity here.

8
00:00:55,710 --> 00:01:00,360
So I mean, it's almost a month now since the beginning of the semester.

9
00:01:00,360 --> 00:01:06,200
Now finally we are removing the mobility of symbolic interactions.

10
00:01:07,200 --> 00:01:18,420
It turns out that many of the things when we talk about first symbolic interaction, they can be well, they think they still apply and they are.

11
00:01:19,530 --> 00:01:21,890
Yeah, let's do a 500 mile video. Right?

12
00:01:21,960 --> 00:01:31,830
So with the more detailed coverage of Simple Interactive now, it's a lot easier to be part of our module rewrite.

13
00:01:33,840 --> 00:01:42,700
So while this module is of copying similar to people interpreting the model of transverse matrix,

14
00:01:42,700 --> 00:01:49,829
proper Indigenous leader is that for a lot of Americans is a lot of the other day for me I

15
00:01:49,830 --> 00:01:55,590
think we were talking about are a little bit hard to cover up with your graduate anatomy,

16
00:01:55,590 --> 00:02:04,980
whether it is possible to cover your about matrix or it might be possible or whatnot, but it's going to be really challenging.

17
00:02:05,490 --> 00:02:18,460
So it's a lot easier to do a little bit of this module and then we're going to look at some experience useful sooner or later.

18
00:02:20,430 --> 00:02:29,909
Okay, so simple linear regression.

19
00:02:29,910 --> 00:02:37,170
As we already know about symbolic intervention, it includes only one covariance, only once,

20
00:02:37,170 --> 00:02:48,450
not really as well as symbolic in the module linear regression, it allows multiple coherence.

21
00:02:49,920 --> 00:02:54,270
So you have here while we have p minus one covariance.

22
00:02:54,450 --> 00:03:04,679
So we have if you are going to use this in here now you have and for each subnet like we have,

23
00:03:04,680 --> 00:03:14,580
I1 is i2 and I p minus one and in total we have p minus one x one.

24
00:03:14,590 --> 00:03:21,180
The reason why we use a human as long as a loop is that the total number of data is

25
00:03:21,180 --> 00:03:26,820
equal to the number of number of random data that's equal is it's just easier to,

26
00:03:27,960 --> 00:03:35,550
to, to use P to denote the total number balance. So in that sense, then the number of X we have as P minus one.

27
00:03:35,910 --> 00:03:47,370
So this is what a particle in your Russian is one shimano's one is stronger than or equal to, larger than or equal to one.

28
00:03:49,950 --> 00:03:57,870
So as we measure the many calls out from symbol in your Russian, not only many calls out, but most many things we saw many results.

29
00:03:57,870 --> 00:04:05,700
We saw for a single linear regression. They can be generalized, they can be extended to multiple intervention.

30
00:04:06,030 --> 00:04:13,380
And in this class, we're going to take a look at how these results can be generalized from of impression.

31
00:04:13,950 --> 00:04:19,860
But first, let's consider of the interpretation of multiple linear regression.

32
00:04:20,280 --> 00:04:25,229
What are the differences when we try to interpret regression coefficients?

33
00:04:25,230 --> 00:04:38,280
The balance of if we consider a study of association between weight and this is our response why this is white and height and h.

34
00:04:38,370 --> 00:04:44,430
So now we have two covariates. One this is xx1x2.

35
00:04:44,620 --> 00:04:57,239
Right for the and in the following notation this is probably called x what this is probably called as true, but doesn't matter.

36
00:04:57,240 --> 00:05:00,490
I mean, it's just a label and it's. You call it.

37
00:05:00,670 --> 00:05:04,900
So age is one colvera. The hive is another covariate.

38
00:05:05,740 --> 00:05:15,070
So first, let's let's consider just to build a simple linear regression model using age alone without considering height nor height for phonetics.

39
00:05:15,400 --> 00:05:18,790
So A is equal to age. So this is a simple linear regression model.

40
00:05:19,720 --> 00:05:26,600
So way is equal to such a assembly or regression model based on the covariate.

41
00:05:26,800 --> 00:05:36,410
H And then B is the the mean of the middleweight or the average weight of women weight given the age that's equal to this.

42
00:05:36,430 --> 00:05:40,150
The reason is that the ABS has a values equal to zero.

43
00:05:40,570 --> 00:05:49,540
So leagues notation w given about this. Now we already know how to interpret this better one.

44
00:05:49,850 --> 00:05:58,850
Right. So if we consider, for example, two children, one age, one at age ten and the other at age 11,

45
00:06:00,020 --> 00:06:11,060
and then for the child who at age 11 than the well or for her, you know, for four for the children were eight at age 11.

46
00:06:11,240 --> 00:06:18,120
You know, the average or the mean wage is actually equal to the zero plus beta one five in this page eight.

47
00:06:20,210 --> 00:06:30,220
And similar for children at age ten the average when is actually going to be the one that's ten.

48
00:06:30,680 --> 00:06:39,710
Now, if we look at the difference, if we subtract two, four more than the difference here, beta zero can go to cancel.

49
00:06:40,130 --> 00:06:44,990
And then the difference between this, that's just one beta beta one.

50
00:06:45,440 --> 00:06:49,940
So then theta one then becomes. Now the difference between this guy and this guy.

51
00:06:50,450 --> 00:06:53,660
This is the main difference. The mean difference.

52
00:06:55,280 --> 00:07:08,030
The main difference in weight you wait for each one year higher in age is more like comparing children at age 1920, at age ten.

53
00:07:08,840 --> 00:07:16,330
So this is how we interpret this beta one is right here.

54
00:07:16,940 --> 00:07:26,809
One thing I would like to point out is that so sometimes I mean, when you when you when you look at how people interpret this,

55
00:07:26,810 --> 00:07:37,370
some people might interpret this as the that the change in the mean change in in weight for every one year.

56
00:07:37,370 --> 00:07:41,780
Increase in in age for every one year increase in age.

57
00:07:42,110 --> 00:07:46,940
Now, there is a subtle difference between that interpretation and this interpretation over here.

58
00:07:47,840 --> 00:07:54,020
So if you look at this, the study, this is if from a design point of view,

59
00:07:54,020 --> 00:07:57,900
from an experimental design point of view, this is a cross-sectional design.

60
00:07:57,920 --> 00:08:07,560
So in other words. A child for any child to pick from this from the status that the child is at a fixed age.

61
00:08:08,010 --> 00:08:15,900
So either at age ten or at age 11 or at a 12 year old or child cannot be both at the age of ten and aged.

62
00:08:16,750 --> 00:08:19,710
So it's just for one child. There is only one age.

63
00:08:21,450 --> 00:08:31,800
So then the interpretation that, you know, if like if we interpret this as the mean change in weight for every one year increase in its age.

64
00:08:32,040 --> 00:08:36,330
Sorry for the other mean shedding away it for every one year increase in age.

65
00:08:36,690 --> 00:08:44,940
That interpretation would implicitly imply imply that, you know, it's sort of a longitudinal design.

66
00:08:44,940 --> 00:08:51,030
So the child age actually increase, gradual increase and then for one year increase in age and see this up,

67
00:08:54,200 --> 00:08:58,469
you see you see you see this much bigger one change in the reading.

68
00:08:58,470 --> 00:09:07,430
Wait. And this interpretation here just make about that subtle difference disappear.

69
00:09:07,440 --> 00:09:14,339
So. So this is super clear that this is for each one year higher in age.

70
00:09:14,340 --> 00:09:18,300
I'm comparing the children at age ten to children at age 11.

71
00:09:18,480 --> 00:09:27,930
So for it's why you're hiring age, we have the difference in rate being finalized to be difference in weight.

72
00:09:28,650 --> 00:09:37,530
So that's one thing I wanted to point out. But although, I mean, in practice, you do see if you ask people how to interpret,

73
00:09:37,580 --> 00:09:45,120
you do see that many people would interpret this as simply as the mean change in weight for every one year, increase in age.

74
00:09:46,320 --> 00:09:57,960
But at that, if you want to be super rigorous and that is one answer to implicitly imply that it's sort of lunch, you know, the size of each child.

75
00:09:58,090 --> 00:10:02,850
You can observe a child at age ten, at age 11 and age 12, for example.

76
00:10:02,850 --> 00:10:08,360
And for each one year increase, you will see this. But it's a greater one changing.

77
00:10:08,370 --> 00:10:12,100
I mean, and to be weight. Okay.

78
00:10:13,210 --> 00:10:16,120
So yeah, that's the interpretation.

79
00:10:16,570 --> 00:10:25,570
Now for this simple linear regression model, we actually ignore the weight of the height, so we only fit assembly or fashion model.

80
00:10:26,200 --> 00:10:36,520
Now what about we want to adjust for height as well, because we know that weight also should depend on height, right?

81
00:10:36,580 --> 00:10:44,200
So there should be a strong association in our social, maybe linear or maybe a longer association on the better.

82
00:10:44,230 --> 00:10:48,340
But anyway, so weight should have a strong association with height.

83
00:10:49,990 --> 00:10:57,670
So without them without adjusting for height, this model is, is not a very, very good model.

84
00:10:58,690 --> 00:11:07,960
So, yeah, so, so while this is the so-called confounder, I mean,

85
00:11:08,320 --> 00:11:13,360
I think here we probably do not want to spend too much time talking about the content and confounder.

86
00:11:13,630 --> 00:11:25,240
But anyway, so the age we know that the high age actually could also be strongly associated with the weight because of this.

87
00:11:26,980 --> 00:11:31,980
Now we will have to fit a model that includes both age and height of all.

88
00:11:33,170 --> 00:11:37,340
So this becomes then a multiple linear regression model.

89
00:11:37,430 --> 00:11:42,890
So it has to covariance more than one covariance. It has both age and height.

90
00:11:46,700 --> 00:11:50,540
And then the interpretation of age now becomes different.

91
00:11:51,590 --> 00:11:56,360
So now let's take a look at the is. How do we interpret this fact?

92
00:11:57,320 --> 00:12:02,750
So now if we consider two children age eight over age ten and age.

93
00:12:03,620 --> 00:12:13,970
So if we still plug these ten and 11 into the regression model for the children at age 11, we have been 111.

94
00:12:14,480 --> 00:12:18,900
But we didn't specify by height. So let's say for this for this one child,

95
00:12:18,930 --> 00:12:29,570
the height of his h one now for the second child at age ten would just plug ten into into the model we replace eight by ten.

96
00:12:30,630 --> 00:12:35,480
But for the same child, we don't know the height either. So we refuse to denote the height.

97
00:12:37,490 --> 00:12:48,650
And then if we subtract four from three, well, then we would have to add here 11 beta minus ten data we get at one as it is going to be two one.

98
00:12:48,890 --> 00:12:54,860
But then the difference between does not become three are two times h one minus beta two.

99
00:12:58,240 --> 00:13:09,340
So now, of course, because we didn't say what values H1 and H2 take cause we don't know what this value is, then how how old are we in?

100
00:13:09,580 --> 00:13:12,700
How should we interpret it? Then this this result and this difference.

101
00:13:14,050 --> 00:13:20,170
Now, how do we interpret that? So the way to interpret being of one is that.

102
00:13:25,060 --> 00:13:34,840
Now suppose that we are looking at the two children being one at age ten and one at age 11.

103
00:13:34,850 --> 00:13:38,850
Suppose they have the same height? I suppose they have.

104
00:13:38,860 --> 00:13:46,360
They have the same height. Their height are both equal to the small h, so it is some value of equal to small age.

105
00:13:48,580 --> 00:13:57,160
And then when we look at the average age, we can find both the age and this small age.

106
00:13:57,460 --> 00:14:05,420
Well, a small age doesn't matter what its value is, if it helps.

107
00:14:05,470 --> 00:14:12,850
You could you could apply a concrete number. But if you are, let's just use more age to denote this visit value.

108
00:14:13,630 --> 00:14:25,780
And after plugging in this age that we have, these are one at age 11 and and where to add h small age as our height as my age.

109
00:14:26,170 --> 00:14:30,880
And then for the same child that's age ten and height at small age.

110
00:14:31,870 --> 00:14:41,239
And then if we take the difference between these two equations, then we see that, of course, again,

111
00:14:41,240 --> 00:14:47,050
the intercept counsel and then these two counsel, because we are looking at the same age, madam.

112
00:14:47,500 --> 00:14:51,549
And then what is left is is a better one.

113
00:14:51,550 --> 00:14:55,120
Minus ten. Better one. That's just one. Better one.

114
00:14:56,200 --> 00:15:02,050
So in this case, then beta one is equal to the mean difference in weight.

115
00:15:02,230 --> 00:15:09,160
While we are still talking about any difference in weight. The difference in waiting for each one year higher.

116
00:15:09,160 --> 00:15:12,460
In age for one year. Higher in age.

117
00:15:13,830 --> 00:15:19,830
Well. Hi, Hope. How constant. We are holding all right.

118
00:15:20,760 --> 00:15:24,990
At the cost of matter. So it doesn't matter which high we're looking at, but we are holding it out.

119
00:15:25,920 --> 00:15:33,569
And of course, it. So this is the interpretation of the beta one for monitoring rash model.

120
00:15:33,570 --> 00:15:37,260
When you adjust for we will include more covariance in the model.

121
00:15:37,860 --> 00:15:45,899
So the beta one now becomes the mean change or mean difference in why the y is r is

122
00:15:45,900 --> 00:15:51,780
weighed in our case in Y for every one of your primary age now holding the other one,

123
00:15:51,780 --> 00:15:59,630
constant holding the other coumarin a constant. Now if you have more covariance in the holding, all the optical areas constant.

124
00:16:00,870 --> 00:16:05,430
In this case we say that of beta one.

125
00:16:05,640 --> 00:16:09,750
This is adjusting the effect of age adjusted for height.

126
00:16:09,750 --> 00:16:13,830
So beta one is the other age.

127
00:16:14,790 --> 00:16:18,330
Age effect on weight.

128
00:16:29,560 --> 00:16:44,180
Adjusted for height. So without including height in the model, if we just look how they are, that would be a simple intervention model.

129
00:16:44,770 --> 00:16:54,330
You could just look at this model or. We're this model operate without including height.

130
00:16:55,050 --> 00:17:04,770
So this age, this is just the Marvel effect of age, but how age alone affects the weight.

131
00:17:05,760 --> 00:17:09,570
But now, after including this height.

132
00:17:10,260 --> 00:17:15,450
Now the age effect, the beta one effect. That's not the marginal effect of age anymore.

133
00:17:15,480 --> 00:17:23,790
That's the age, in fact, adjusting for adjusting for height, because height is also strongly associated with weight.

134
00:17:24,270 --> 00:17:28,820
So if we look at. Two children were at the same height.

135
00:17:29,630 --> 00:17:33,020
They live maybe at a different age. We're at the same height.

136
00:17:33,740 --> 00:17:40,100
We look at how how much they are, how large the difference is from their weight.

137
00:17:40,520 --> 00:17:44,040
A large number of different ways. That's done.

138
00:17:45,210 --> 00:17:49,250
Adrian, in fact, adjusting for height by including height in the model.

139
00:17:52,390 --> 00:18:08,450
Any questions so far? Okay.

140
00:18:09,150 --> 00:18:13,530
So now this slides, just summarize what we have talked about.

141
00:18:13,920 --> 00:18:19,950
So for a simple linear regression with only one X, we only want X now.

142
00:18:19,950 --> 00:18:26,820
But you know, one beta one is the mean frequency Y per unit higher in x.

143
00:18:27,960 --> 00:18:33,030
Are you going to hire an X without adjusting for anything?

144
00:18:33,330 --> 00:18:37,890
Right. So beta one you could use the crude or across this slope.

145
00:18:38,730 --> 00:18:47,940
So unadjusted for any marginal effect of x or y for multiple regression.

146
00:18:48,210 --> 00:18:53,670
So here, if you want to write it more explicit, it was the beta one x.

147
00:18:53,700 --> 00:19:02,550
I want to participate in tool and spy tool was once they took x like.

148
00:19:05,420 --> 00:19:15,350
Peter. You know, as one ex, I keep on this one as though, you know,

149
00:19:15,350 --> 00:19:22,880
that we have P minus one X and we have been told only we have been including this being a0p PBS.

150
00:19:26,080 --> 00:19:30,130
And in this case, if you want to interpret this being a one.

151
00:19:32,770 --> 00:19:46,340
Now, Peter, one is the main difference in why per per unit, per one unit higher in xx1 with other x case how constant?

152
00:19:48,280 --> 00:19:50,650
Just like the example we just went over.

153
00:19:50,860 --> 00:20:02,920
So by holding all the either covariates constant or in other words, adjusting for or controlling for all the other covariates.

154
00:20:05,720 --> 00:20:17,030
Because this this now with this multiple linear regression model, the fact of X and Y on average of Y on the mean a y is beta one.

155
00:20:17,210 --> 00:20:22,790
But this, in fact, is after we control for all the other covariates that we included in the model.

156
00:20:31,030 --> 00:20:37,120
This is the how we interpret war, she said, holding by holding the other acts constant.

157
00:20:37,940 --> 00:20:41,379
So four, four, four, two, four,

158
00:20:41,380 --> 00:20:49,210
two subjects for two individuals if they have the same matter for all the other covariates that it is controlling the other categories,

159
00:20:49,630 --> 00:20:51,340
holding the other qualities constant right?

160
00:20:51,370 --> 00:21:00,400
If they have the same matter for all the other covariates, then what are the main differences in why that's not.

161
00:21:03,130 --> 00:21:07,290
That's better. That's the. Okay.

162
00:21:07,290 --> 00:21:12,030
So this is how we interpret model regression model.

163
00:21:12,210 --> 00:21:16,650
Similarly, similarly, you could interpret the observations.

164
00:21:16,830 --> 00:21:27,000
So here we are hitting beta one as an example. Similarly, you could interpret data to be a tool.

165
00:21:27,000 --> 00:21:34,850
That would be the main difference why a per one unit higher in ads to holding all the other acts,

166
00:21:34,860 --> 00:21:39,330
holding all the other coloreds constant broadcasting for only on clearance.

167
00:21:49,930 --> 00:21:53,169
Okay. Now,

168
00:21:53,170 --> 00:22:03,250
the people want it a bit too generous because this bailout here in a symbolic inauguration hotel and this bailout here in the model interaction model,

169
00:22:03,610 --> 00:22:07,060
general speaking, we are not equal. They're not equal.

170
00:22:07,240 --> 00:22:14,800
The reason well, this is very intuitive. I mean, here this is the Martelli effect of X and Y not adjusting for anything.

171
00:22:15,430 --> 00:22:21,410
You simply fate of this single union. But A, here you are just for the article Americas.

172
00:22:22,660 --> 00:22:25,959
So of course, intuitively, mathematically, this beta want this to be No.

173
00:22:25,960 --> 00:22:29,620
One from this two models. They cannot. Generally they do not even.

174
00:22:32,550 --> 00:22:37,050
This has been a way in this vein of what generally would be different.

175
00:22:38,010 --> 00:22:43,590
If you look at Latin America, that they're different. However, in some cases they become the same.

176
00:22:44,580 --> 00:22:52,950
It was special case if is that if all of the other covariates they are uncorrelated with beta one.

177
00:22:53,550 --> 00:23:00,750
So in other words here if in a logical linear regression model in the model, linear regression model,

178
00:23:00,750 --> 00:23:10,900
if all the other x if all these x x case, if they are uncorrelated with this particular x x1 there.

179
00:23:10,950 --> 00:23:15,329
Anthony And while of course we can calculate their correlation, right?

180
00:23:15,330 --> 00:23:19,940
So because each X means you have, once you have the data set,

181
00:23:19,980 --> 00:23:29,970
you can calculate the correlation amount of this X and if all these rest of X they are unrelated with x one,

182
00:23:30,390 --> 00:23:37,140
then this beta one here by feeding the similar linear regression model and this beta one here by fitting this model,

183
00:23:37,140 --> 00:23:40,710
linear regression model, these two beta ones, they are the same.

184
00:23:43,000 --> 00:23:46,990
That's one federal case where beta one little beta was missing.

185
00:23:48,900 --> 00:23:55,590
So another federal case is when all of these are equal to zero.

186
00:23:55,700 --> 00:24:03,990
Right. So when all these are on collision with the white light amidst all the rest, libidos are equal to zero, though this is almost a true case.

187
00:24:04,050 --> 00:24:07,920
Right. So because. Because, of course, when.

188
00:24:07,920 --> 00:24:15,320
When all these bailouts are equal to zeros, then there's no there's there's no these terms anymore.

189
00:24:15,330 --> 00:24:19,440
So. So then the molecule eventually becomes super has a subordinate, right?

190
00:24:19,920 --> 00:24:22,380
So of course then the two parties must be the same.

191
00:24:35,360 --> 00:24:48,140
Look, these are two federal cases, but I think the most important point is that generally speaking, but this a one here and this better one here.

192
00:24:48,770 --> 00:25:00,230
They are different. They are different. In fact, one is without adjusting for the logical methods and one is after adjusting for the other covariates.

193
00:25:00,920 --> 00:25:08,360
So after adjusting for other communities to meet alumni, the real change of age fact, for example, the average weight will change.

194
00:25:11,750 --> 00:25:16,640
Okay, so that's two separate cases where the two meters are equal.

195
00:25:17,270 --> 00:25:22,430
And in practice, while these these two cases rarely occur in practice,

196
00:25:23,090 --> 00:25:30,500
they rarely occur there they are maybe in some extreme cases and in the rarest amount, these are unlikely occur.

197
00:25:30,890 --> 00:25:39,710
On the contrary, I mean, what is likely to occur is that these exercise, they may have high correlation in stature.

198
00:25:40,250 --> 00:25:46,260
So for example, here I see in this in this running example we are using, right?

199
00:25:46,280 --> 00:25:55,280
So here we have age and height and especially for, you know, for for for adolescents or maybe for young kids.

200
00:25:55,730 --> 00:26:00,730
So we know that age and height there are highly correlated and they are highly correlated.

201
00:26:00,740 --> 00:26:07,340
So so it's likely that the covariates there exist some high correlation among these covariates.

202
00:26:08,240 --> 00:26:12,020
And when there is high hydration now the mode of feeding,

203
00:26:12,020 --> 00:26:18,829
while there may be some challenge in feeding the model and whether even this high correlation among different acts among covariates,

204
00:26:18,830 --> 00:26:23,780
we call this medical reality. So look at how they've related.

205
00:26:24,350 --> 00:26:27,560
This can cause lots of troubles.

206
00:26:28,850 --> 00:26:31,220
I think we mentioned that maybe in module eight.

207
00:26:32,150 --> 00:26:41,270
So the reason while maybe it's after we talk about of the use of the matrix notation, it becomes clear, mathematically becomes clear.

208
00:26:42,020 --> 00:26:47,510
But intuitively it is not a very easy to not very difficult to to explain this.

209
00:26:47,690 --> 00:26:53,470
So, for example, let's say we look at 12, right?

210
00:26:55,100 --> 00:26:58,230
Let's say look, we look at this model. And I was.

211
00:27:00,750 --> 00:27:04,700
Google Apps, thanks to. Of course.

212
00:27:04,740 --> 00:27:10,100
Absolutely. Right. Right. We do have this model now.

213
00:27:10,110 --> 00:27:16,320
If x1x2 are highly correlated, let us consider extreme case.

214
00:27:17,130 --> 00:27:23,160
Let's consider extreme case. Let's say that x one and x two are exactly the same.

215
00:27:24,570 --> 00:27:31,320
I didn't tell you when I passed on data to you, but actually in a dataset x1x2 of them are exactly the same.

216
00:27:31,710 --> 00:27:38,580
And let's say you didn't have that. You're just, you know, just a regression model that includes both in the model.

217
00:27:38,880 --> 00:27:45,420
You didn't check the weather whether there is hibernation now because x1x2 are exactly the same.

218
00:27:46,380 --> 00:27:58,440
Then. I could then well then beta 1x1 plus beta 2x2, which is actually going to be the same as we had one.

219
00:27:58,460 --> 00:28:06,230
Let's say I somehow put a five here times x one last beta two at last quantified here comes out.

220
00:28:08,850 --> 00:28:13,920
Right. So if if X one is equal to two,

221
00:28:14,550 --> 00:28:22,890
these two are exactly equal because the higher the -1 to 5 times x one and a plus five kind of adds to the

222
00:28:22,890 --> 00:28:31,160
castle because excellent next we of the same well this implies is that now where you ask try to estimate

223
00:28:31,180 --> 00:28:36,569
better one better to you could ask made a better one as better one better to ask badly or you could ask

224
00:28:36,570 --> 00:28:42,900
made a better one as better one point minus on a five and better two as spiritual plus point of five.

225
00:28:43,710 --> 00:28:52,800
So you can not you need to determine it a lot better. Even the very values that again can make it can give you the same result.

226
00:28:53,100 --> 00:28:57,510
So this is of course, a hypothetical example.

227
00:28:58,020 --> 00:29:02,570
Y there there are also challenges. Well, you know, there exist high value narrowing.

228
00:29:02,910 --> 00:29:09,860
So you can not very precisely estimate beta one beta.

229
00:29:10,800 --> 00:29:18,510
So if you look at then in the end, if you look at the center, error can be out of range very large center at.

230
00:29:23,750 --> 00:29:31,290
Yeah. But bottom line is that high correlation or the so-called Monte Carlo narrative is not uncommon.

231
00:29:31,310 --> 00:29:36,770
It's actually quite a common practice and one there is logical narrative.

232
00:29:37,550 --> 00:29:40,790
Then there there is going to be some challenge on the field of money.

233
00:29:41,270 --> 00:29:48,020
So we need to be very careful before we feed a model to check if there if some ice

234
00:29:48,020 --> 00:29:52,940
there hot varieties and later rubble talk about the poor is going to come about.

235
00:29:54,990 --> 00:29:58,740
The diagnosis of that and also some ways to deal with that.

236
00:30:00,480 --> 00:30:07,380
But for now, let's just keep in mind that medical reality, which means that some of the facts are highly correlated.

237
00:30:07,650 --> 00:30:11,030
This is not a troublesome situation.

238
00:30:11,040 --> 00:30:17,950
So we need to be careful. Okay.

239
00:30:18,050 --> 00:30:30,830
Any questions for you? Now, as we mentioned, for multiple linear regression, it's a lot easier if we use the matrix notation.

240
00:30:31,070 --> 00:30:34,400
Is that rather than here like for example,

241
00:30:35,150 --> 00:30:43,580
here's the explicit write down the expression is but is a lot easier if we use matrix notation

242
00:30:43,580 --> 00:30:52,280
rather than maybe not explicitly so here this one will use Y to denote the response factor.

243
00:30:52,370 --> 00:31:03,530
That's by stacking that observation from the in individual and we have in subjects from individual values from on top of each other.

244
00:31:03,740 --> 00:31:09,469
Those are similar to what we would have in a data saturated dataset.

245
00:31:09,470 --> 00:31:15,200
Each role represent one subject and each column represents one variable.

246
00:31:15,200 --> 00:31:20,140
Let's say there is one column that that is white. Then the column is just this way of entry.

247
00:31:21,040 --> 00:31:26,150
So we have in here observations in subjects.

248
00:31:27,140 --> 00:31:33,140
So that's the Y vector and of the car's volume, the absolute that's just the average form for each subject.

249
00:31:35,550 --> 00:31:49,800
And if we use well, this is the so-called design matrix X design matrix X, so the X and contents are now in rules.

250
00:31:50,070 --> 00:31:56,700
So each row here, each row here represents one subject, one individual in the dataset.

251
00:31:56,910 --> 00:32:02,850
So in total, we have in individuals, we have in rules. And then each column.

252
00:32:02,850 --> 00:32:07,770
So we have P columns here. So the first column, this represents the intercept.

253
00:32:08,580 --> 00:32:15,870
So the intercept for the first individual intercept before assigning individual and intercept for the last individual in the intercept.

254
00:32:16,590 --> 00:32:21,480
And then we have x one, right? This, this is x one, the variable x one.

255
00:32:21,780 --> 00:32:33,620
And then we have x one value for the first subject, x y value for the silence object and x formatted for the last subject and a similarity.

256
00:32:34,000 --> 00:32:39,170
We have different columns. We have different coverage included because here, for example, this is age.

257
00:32:39,180 --> 00:32:42,740
This is in a way. This is way.

258
00:32:42,810 --> 00:32:56,180
So this is this height. This is age. This is maybe gender or maybe maybe education level or maybe race or whatever variables you have here.

259
00:32:56,220 --> 00:33:00,240
Data set. So different columns, they represent different variables.

260
00:33:01,720 --> 00:33:05,820
And in the different rules, they represent different individuals, different subject.

261
00:33:05,850 --> 00:33:12,640
So, so this matrix is the so called design matrix and is NBC.

262
00:33:12,660 --> 00:33:19,980
So here I think it's very important to keep this notion in mind because while we deal with Matrix inside,

263
00:33:20,940 --> 00:33:28,340
we need to be very careful about dimension because dimensionality is a very easy way to check whether in

264
00:33:28,380 --> 00:33:33,360
some of the expression we write out makes sense because we can easily attack the relative dimension.

265
00:33:33,360 --> 00:33:36,599
Makes sense. So you could imagine. Does that make sense?

266
00:33:36,600 --> 00:33:41,930
Then something must be some error in the expression.

267
00:33:41,940 --> 00:33:46,889
So we need to keep the dimension in mind. So this is in rows and columns.

268
00:33:46,890 --> 00:33:51,060
So again, by p matrix and a similar to you all for beta.

269
00:33:51,060 --> 00:33:59,550
Now theta is a p dimensional matter. So beta zero is the intercept and then the other betas.

270
00:33:59,880 --> 00:34:04,050
Where are the betas? Further for different acts, for different clouds?

271
00:34:06,350 --> 00:34:13,070
So with all this notation, then the linear regression model, linear regression model can be expressed as.

272
00:34:14,390 --> 00:34:25,940
Why? Equal to x. The design matrix multiplied by bathos plus absolute gold out of why here is an by one and equal to x.

273
00:34:26,090 --> 00:34:32,360
This is in by buf and the beta that is P by one end of the class.

274
00:34:32,390 --> 00:34:41,790
Armstrong This is an by one. So this is not how we express linear regression model,

275
00:34:41,790 --> 00:34:47,429
but of a linear regression model using matrix notation and then the expression Y is equal

276
00:34:47,430 --> 00:34:55,920
to so that is Y is equal to whereas these foundation x beta class these values of absolute.

277
00:34:56,400 --> 00:35:01,020
Now we know that it is Spanish from absolutely zero zero because that's our assumption, right?

278
00:35:01,020 --> 00:35:04,440
So they expand each on each line that's equal to zero.

279
00:35:05,070 --> 00:35:15,330
And so this is zero. Then what we have is but but but we assume that X is fixed over X, they are numbers.

280
00:35:15,570 --> 00:35:27,400
So this is kind of straight up. So this is how what are the model is we are modeling that the mean of Y as this linear function in terms of data.

281
00:35:29,920 --> 00:35:34,450
So this is how we express the multiple linear regression model using matrix notation.

282
00:35:38,220 --> 00:35:47,020
Any questions? And then forgo sanctions.

283
00:35:47,980 --> 00:35:54,210
Now, here again, let's first look out of the assumption without assuming normality.

284
00:35:54,220 --> 00:35:56,470
So here, if we assume the error,

285
00:35:56,650 --> 00:36:06,430
follow some distinction that this year has been zero has the various covariance matrix sigma sigma squared identity matrix.

286
00:36:09,330 --> 00:36:18,290
Let's call that this zero. This zero. This is this is in by one zero bedroom.

287
00:36:18,920 --> 00:36:28,650
Right. And again, this this this metric that's sigma squared times other metrics.

288
00:36:28,950 --> 00:36:33,030
So only the evidence on a diagonal. That's that's that's one.

289
00:36:34,620 --> 00:36:40,109
So what this means is that the absolute eyes for different individuals, Aboriginal eyes,

290
00:36:40,110 --> 00:36:48,110
they're mutually uncorrelated to because because the the various well here each element even.

291
00:36:48,520 --> 00:36:50,340
So this is you know, this is zero.

292
00:36:50,550 --> 00:37:01,260
So for each element here, for each element of the animal, nothing represents the correlation between the covariance, between the absolute eyes.

293
00:37:01,860 --> 00:37:08,009
Now, because we are assuming that this is a matrix or all of these elements,

294
00:37:08,010 --> 00:37:13,020
they are zeros and they're zeros now because this have some eyes, they are uncorrelated.

295
00:37:14,010 --> 00:37:22,499
So that's one assumption we are making. We we're assuming that aerosolized that are mutually uncorrelated the same as before.

296
00:37:22,500 --> 00:37:27,450
But now with the only difference is that now we are expressing this assumption using this

297
00:37:27,450 --> 00:37:34,590
matrix notation and also the absolute have neither zero and equal variance see y squared.

298
00:37:35,850 --> 00:37:41,220
So the variance of all the x oh the absolute highest we have sigma squared varies.

299
00:37:41,550 --> 00:37:45,810
This is because we modify a six sigma square in the front of that matrix.

300
00:37:46,320 --> 00:37:52,860
Now this of course, is equal to sigma squared, sigma squared and zero here and zero here of that.

301
00:37:54,420 --> 00:38:02,280
So this means that the variance of each absolute I must see my square on the diagonal.

302
00:38:03,630 --> 00:38:05,600
All the covariance is the R zeros.

303
00:38:07,140 --> 00:38:16,710
So again, the this notation here, the only difference is that now they're writing the assumption in terms of matrix notation,

304
00:38:17,040 --> 00:38:22,739
the assumptions are still the same as before. We are assuming absolute eyes for different individuals.

305
00:38:22,740 --> 00:38:27,720
They are they are uncorrelated and also they have new zero and equal variance.

306
00:38:29,630 --> 00:38:36,590
The same assumption as before. We do change the assumption now by overseeing the writing assumption in terms of visuals and vision.

307
00:38:37,610 --> 00:38:41,330
And then this is the manual linear regression model.

308
00:38:41,430 --> 00:38:46,580
So as we mentioned, the foundation is equal to or equal to this and some speed.

309
00:38:47,360 --> 00:38:51,340
Now the variance of Y the virus is Y.

310
00:38:51,350 --> 00:38:57,649
So either is model T while there is one slide.

311
00:38:57,650 --> 00:39:03,560
Talk about the run of. So that suppose why is a drug of action.

312
00:39:03,650 --> 00:39:11,040
Well, this is. This is our rebel rebel leader.

313
00:39:11,200 --> 00:39:17,530
And then the mean matter to me is of why is just the extrapolation of each component

314
00:39:19,930 --> 00:39:26,340
and then the variance of why is while this is how the variance of Y is defined,

315
00:39:26,350 --> 00:39:32,560
so is y minus excitation times the transpose of y minus excitation.

316
00:39:32,950 --> 00:39:36,250
So the variance of why is this embodied in matrix?

317
00:39:37,270 --> 00:39:40,569
This is in matrix is the dimension.

318
00:39:40,570 --> 00:39:55,960
Why is it so? The diagonal elements, these numbers, these are the variance of why I write.

319
00:39:56,320 --> 00:40:00,940
These are the variance of each one and of the off diagonal elements.

320
00:40:01,600 --> 00:40:04,600
For example, here this power dynamics.

321
00:40:04,630 --> 00:40:07,840
This is the covariance of what I want to see.

322
00:40:10,020 --> 00:40:16,910
These are the clearances. So this is the virus of a vector.

323
00:40:17,300 --> 00:40:23,420
Why? Virus of the vector? Why it's again is if not,

324
00:40:23,420 --> 00:40:32,479
why is it a dimension in that ignorance matrix that's in my end matrix with the elements equal to

325
00:40:32,480 --> 00:40:42,050
the variance of each component of what happens over the covariance matrix which in the y models.

326
00:40:42,680 --> 00:40:46,550
So here then I'll.

327
00:40:53,610 --> 00:40:57,390
And then the matter is of why is because why is equal to this.

328
00:40:57,450 --> 00:41:01,890
So that's the variance of this guy, not now behind of this guy.

329
00:41:02,070 --> 00:41:07,040
So by using the power of the awareness, the speed up, that's just a number that's constant.

330
00:41:07,330 --> 00:41:12,990
So, so so the virus has no has no rule.

331
00:41:13,980 --> 00:41:18,750
No rule on this guy. And then it becomes the virus of absolute.

332
00:41:19,200 --> 00:41:26,460
Now, we are assuming that there is an absolute let's see my squared times that fix and that's based on our assumption.

333
00:41:31,080 --> 00:41:38,070
So we can see that now the linear regression under this assumption assuming this now the somebody was know the

334
00:41:38,370 --> 00:41:45,660
linear regression now can be written in this way where they foundation becomes this and variance three times this.

335
00:41:51,180 --> 00:42:05,970
So the other was number one response factor why and I mean this guy there is everything that is written in terms of matrix here.

336
00:42:06,030 --> 00:42:11,340
Keep in mind that we have not assumed of normality and independence yet.

337
00:42:11,760 --> 00:42:16,410
So the only assumption we made so far that's absolute is our mutual admiration.

338
00:42:16,650 --> 00:42:20,220
And that's the highest of the zero. Yeah, it's equal virus.

339
00:42:22,620 --> 00:42:33,510
And under this assumption we are able to rule on sorry if you recall in similarly a regression model we talk about for estimation of this assumption,

340
00:42:33,560 --> 00:42:36,970
we can carry out estimation under this assumption alone. Right.

341
00:42:37,020 --> 00:42:46,820
But that will make inverse the need to assume normality and independence of these different numbers of different individuals.

342
00:42:52,460 --> 00:43:01,740
Okay. And then this is just one simple example I got for our symbol linear regression.

343
00:43:02,330 --> 00:43:08,990
Now we are able to write it in matrix notation as well. So this is simple linear regression in this case.

344
00:43:09,350 --> 00:43:14,629
Now, why again is this an dimensional metric from different individuals now?

345
00:43:14,630 --> 00:43:18,260
S Because we only have two well, only have one.

346
00:43:18,260 --> 00:43:24,589
X So then we have to build. So if we start with a beta, then beta is just a beta as it were.

347
00:43:24,590 --> 00:43:30,800
Beta was intercepted in slope and a similar to the x.

348
00:43:31,220 --> 00:43:38,150
Now S has an intercept. And also it has a column corresponding to x one.

349
00:43:38,150 --> 00:43:42,320
There's just one there. But but now it has two column.

350
00:43:42,350 --> 00:43:45,380
The reason is that we have a we have an intercept in the model.

351
00:43:48,800 --> 00:43:58,260
So this is this is X accident is by a Y is by one line and the X is made from matrix and.

352
00:43:58,800 --> 00:44:02,280
This is just an absolute actor from all the individuals.

353
00:44:02,280 --> 00:44:06,780
This is my one lecture and better than it is to my.

354
00:44:13,230 --> 00:44:22,250
Right. Then as I see the some of the area areas of the square or some square areas, if you recall, how you define is defined in this way.

355
00:44:23,270 --> 00:44:30,440
The sum of absolute I hat square and that's how I see this we talk now using

356
00:44:30,440 --> 00:44:35,980
matrix notation is simply the inner product between absolute and abstraction.

357
00:44:36,260 --> 00:44:44,690
So the absolute of how transpose times absolute is a m by one, m by one vector.

358
00:44:45,290 --> 00:44:50,720
So absolute transpose now becomes this one by in rule vector.

359
00:44:51,350 --> 00:45:01,600
So absolutely as a column. So absolute transpose becomes this rule that this is absolute and principles and limits is absolute.

360
00:45:05,250 --> 00:45:12,080
Like the person in the matrix algebra says that we modify this rule actor by calling macro.

361
00:45:12,140 --> 00:45:15,200
That's just the sum of the element.

362
00:45:15,200 --> 00:45:18,529
Y is part of some of x epsilon times.

363
00:45:18,530 --> 00:45:24,020
S1 Epsilon two times absolute rules. And that becomes this some.

364
00:45:26,130 --> 00:45:39,280
Absolutely. I swear. So this so this is an example, just trying to illustrate that no simple regression model does fall into this category.

365
00:45:39,280 --> 00:45:42,880
We can use matrix notation if you know somebody particular mine.

366
00:45:51,220 --> 00:46:02,470
And now with the assumption, you know, uncorrelated and this this assumption I so apologize are mutually uncorrelated and I mean zero equal,

367
00:46:02,500 --> 00:46:07,660
whereas we are able to estimate the beta now here.

368
00:46:07,670 --> 00:46:14,150
Maybe it's actually easier if we just use this slide instead of writing everything down because there's only the smallest.

369
00:46:16,230 --> 00:46:25,560
Um, so recall that the ordinary least the square is try to minimize it to try to minimize the SC.

370
00:46:25,720 --> 00:46:30,370
That's what only square, least square is and try to minimize the SC.

371
00:46:34,290 --> 00:46:43,499
Now to minimize as I see lots of nutshell using matrix notation, let's derive the estimate of theta.

372
00:46:43,500 --> 00:46:45,840
We're going to have a see how we do that.

373
00:46:47,700 --> 00:46:57,160
Now first let's calculate an associate, but as I see by definition does have some transpose times Armstrong or epsilon.

374
00:46:57,450 --> 00:47:03,990
So Armstrong had that's y minus the predicted y y minus y hat.

375
00:47:04,710 --> 00:47:09,210
But we need to keep in mind that everything now is is in terms of factor were matrix.

376
00:47:09,390 --> 00:47:22,890
So this y is an by one and y hat is also in by one and then transpose that then times one minus y hat.

377
00:47:26,810 --> 00:47:32,210
But this predated wireless. Why have that's actually x times beta hat.

378
00:47:32,270 --> 00:47:36,590
That's simply by pluck, by plugging by replacing the by a better hat.

379
00:47:37,820 --> 00:47:47,860
And that's that's X times better hat. And then while this is transpose four transpose,

380
00:47:47,860 --> 00:47:54,790
we know that awhile if we apply this property of principle so eight plus p transpose is equal to eight transpose us the transpose.

381
00:47:55,810 --> 00:48:07,120
Then here we will have you know, this will be equal to y transpose minus x times theta hat transpose.

382
00:48:08,240 --> 00:48:12,220
Right? This this transpose is equal to this guy.

383
00:48:14,080 --> 00:48:19,670
And then while the transpose a product is equal for this,

384
00:48:19,780 --> 00:48:27,730
the reverse all we need to reverse the order when we calculate the transpose the product and then this guy becomes this guy.

385
00:48:28,000 --> 00:48:42,850
So here this make it work so this transpose becomes this becomes Peter had transpose times X principles that's when

386
00:48:42,850 --> 00:48:54,190
applying to the properties while this is matrix algebra and then we have now if we just subtract this product so we have,

387
00:48:55,240 --> 00:49:03,850
we have things clear, so we have this y transpose minus multiplied by this line that it gives us this

388
00:49:04,630 --> 00:49:13,760
and again this and then this y transpose multiplied by this x times better happen

389
00:49:14,290 --> 00:49:23,350
that it gives us this guy over here one of those times x and a similarly we have

390
00:49:23,770 --> 00:49:28,600
the second guy this term here but of having this Y not it gives us that's.

391
00:49:30,700 --> 00:49:37,600
And again we have this guy modified by this never gives us this.

392
00:49:41,720 --> 00:49:45,700
And of course, here we need to pay attention to the signs.

393
00:49:45,700 --> 00:49:50,670
So we have narrative here, narrative here. So that's why in the end, for the last time, we have a party.

394
00:49:56,410 --> 00:49:59,920
And then if we notice the two things in the middle. So here this is.

395
00:50:01,580 --> 00:50:09,739
There's two things. They are scalar. So here that's why that's the reason why we always need to keep the dementia in mind.

396
00:50:09,740 --> 00:50:17,030
So here the beta, the, the y is actually in by one matrix.

397
00:50:17,420 --> 00:50:28,820
So Y first post that becomes one, but in fact it acts as imagined is going to be that is p by one factor, right?

398
00:50:29,240 --> 00:50:33,649
So if you multiply all the three, the dimension becomes one by one.

399
00:50:33,650 --> 00:50:37,430
So that's a skill that it can use as a scalar.

400
00:50:37,760 --> 00:50:44,270
And it's similar to here. This gives us a skill because both are scalar.

401
00:50:45,170 --> 00:50:48,380
Then this guy is equal to this guy.

402
00:50:53,360 --> 00:50:58,160
Because the transpose of this is equal to I had transposed time the instructions not.

403
00:50:59,810 --> 00:51:04,940
But why? Well, this thing is a scalar event. Of course, the transpose is just equal to itself.

404
00:51:05,060 --> 00:51:11,690
It's just the numbers on the transpose as has no as north the has no effect.

405
00:51:13,970 --> 00:51:23,330
So then the point is that the point is that the the two terms in the middle like this term here and this term here, they are, they are equal.

406
00:51:23,750 --> 00:51:27,080
So that's why we can write it as minus two. I just.

407
00:51:28,180 --> 00:51:37,950
It's. Okay.

408
00:51:38,250 --> 00:51:47,060
So many other words. Now we have read this. See the sum of squares as this function of patents.

409
00:51:48,750 --> 00:51:54,270
If you look at this, if you look at this, this final expression is actually a quantity of functional beta.

410
00:51:54,270 --> 00:51:59,250
So here is involves beta how transpose kind of state and there is matrix in the middle.

411
00:51:59,610 --> 00:52:05,370
But essentially this is a function of data and that there is a linear function of data.

412
00:52:05,790 --> 00:52:13,560
So in combination then we have already a function of data and now we are trying to minimize this contribution,

413
00:52:13,590 --> 00:52:19,049
minimizing the sum of squares, true to estimate, but that is small.

414
00:52:19,050 --> 00:52:27,240
We can minimize that by taking a derivative beta and essentially you get a sum that for band.

415
00:52:30,890 --> 00:52:38,810
Okay. Any questions? Yeah. Well, the doesn't the term on the left in the term on the right just equal one?

416
00:52:41,330 --> 00:52:52,580
So we just that part, the term lifetime by transpose Y is equal to one and then this one and then transpose x,

417
00:52:52,580 --> 00:52:56,900
transpose x and a hat equals one of the.

418
00:52:57,200 --> 00:53:00,380
They do not equal. They are not equal to one.

419
00:53:01,550 --> 00:53:12,680
We're not. They are not what the de imagine is equal to, but the fact that this thing is not able to look with the emergency room.

420
00:53:12,940 --> 00:53:15,999
Oh, okay. So so this.

421
00:53:16,000 --> 00:53:23,320
Why could it be anything? So so the real value of this guy is not able to work, but one would imagine is he is a schemer.

422
00:53:29,320 --> 00:53:41,340
Any other questions? Okay, then let's take a six minute break as you.

423
00:53:46,630 --> 00:53:49,850
Do you really have to deal it?

424
00:53:56,857 --> 00:54:01,867
Yeah. Okay. I think it's. Well, of course, but being on the algebra, that's.

425
00:54:01,867 --> 00:54:07,117
That's important. But on the other hand, it is also important to keep the big picture in mind.

426
00:54:07,117 --> 00:54:11,407
So. So that we do not get lost in the calculation.

427
00:54:11,707 --> 00:54:20,766
So what we are trying to do here is we set up the matrix notation for multiple interaction and that's that's what this is like.

428
00:54:20,767 --> 00:54:26,736
So we set it up. So our ago is actually estimated to be the same as the symbol.

429
00:54:26,737 --> 00:54:35,587
A year ago was we have global interaction. Using this matrix notation, we have estimated this then, but now we are we are deriving this data.

430
00:54:35,737 --> 00:54:42,757
We are having a little fun with one of the feedback habits, one of the benefits that that's what we are doing.

431
00:54:43,867 --> 00:54:46,897
And in doing that we actually need to some assumption.

432
00:54:46,897 --> 00:54:54,607
This is an assumption we make as the absolute rate and as long as they have zero, they have equal.

433
00:54:54,617 --> 00:55:02,977
Yeah, this is assumption we made and under this assumption we are trying to find the estimate beta hat and the way to fun.

434
00:55:02,977 --> 00:55:09,276
Ask me the beta hat. That is to minimize the sum of squares over weighting.

435
00:55:09,277 --> 00:55:21,487
So somewhat or not, these are the parts of the way to find beta beta that is to minimize the sum of squares in the same exact the same idea.

436
00:55:21,637 --> 00:55:25,057
The is a square idea when we talk about somebody or maximum.

437
00:55:25,537 --> 00:55:29,577
So now we are trying to minimize this. SC Now you know, minimize.

438
00:55:29,607 --> 00:55:35,047
SC Of course. So we just need to take in first of all,

439
00:55:35,047 --> 00:55:41,977
we just need to take linearity of that as I see that solidarity to be equal to zero to solve for this equation.

440
00:55:42,607 --> 00:55:46,227
But in order to do that, first we need to calculate as I see, right?

441
00:55:46,237 --> 00:55:49,387
So we do find out what as I see is that's what we did.

442
00:55:50,227 --> 00:55:56,586
So we calculated as as a class now is actually equal to this functional valence and

443
00:55:56,587 --> 00:56:01,447
then you know next what we will do is we will take the derivative of this function.

444
00:56:01,547 --> 00:56:05,977
We decide without matches and, and then saw that, that at the urging.

445
00:56:09,067 --> 00:56:15,227
Okay. So in order to choose the beta hat to minimize.

446
00:56:15,317 --> 00:56:17,027
So we just sat in the community. Community.

447
00:56:17,807 --> 00:56:24,557
But now we need to calculate that the routine, the derivative is equal to this is, as I see we just calculated.

448
00:56:25,037 --> 00:56:35,057
Now we need to calculate that you already know that you earned you've got this is this minus this passenger so we can calculate the original is true.

449
00:56:35,547 --> 00:56:41,017
Now that you review over the first term with Salvator, that's simply zero because it does not depend on that.

450
00:56:41,027 --> 00:56:46,637
I have nothing to do with that. So then the version that is appearing, that's simply zero.

451
00:56:47,537 --> 00:56:52,187
Then we just need to look at the derivative of the cell in terms of the salvator.

452
00:56:52,607 --> 00:56:56,827
So now let's forget about this negative two for a moment because it's just a scale.

453
00:56:57,227 --> 00:57:02,357
This is a is a constant is a scanner. So let's just consider the origin of this guy.

454
00:57:02,657 --> 00:57:19,177
That is this guy right here. Mr. Farage, you know, to counter this derivative immoral to add to while we have a few slides calculating the directive.

455
00:57:19,297 --> 00:57:28,737
So here. Yeah.

456
00:57:29,777 --> 00:57:34,157
So we were just well we use this resound so differentiating factor we the shadow of.

457
00:57:34,397 --> 00:57:40,097
So here if we have a back four X if we look at it as transposed have some matrix eight

458
00:57:40,637 --> 00:57:44,657
then that you already know this guy we're going to have an X that's just the matrix eight.

459
00:57:46,067 --> 00:57:49,607
We will apply this result here.

460
00:57:49,967 --> 00:57:54,307
So here this that x is is the beta hat here.

461
00:57:54,317 --> 00:58:00,706
So I have transpose times this was x transpose.

462
00:58:00,707 --> 00:58:06,077
Hence like we will consider this to be the a matrix in the previous line in this line

463
00:58:06,087 --> 00:58:13,667
here this way that's that's done this matrix here so that if you apply that result,

464
00:58:13,907 --> 00:58:18,337
this is simply equal to eight where an equal X transpose tax mark.

465
00:58:20,557 --> 00:58:23,687
So as we did on the next slide.

466
00:58:28,347 --> 00:58:31,647
Is. Is this. Is this result over here?

467
00:58:32,097 --> 00:58:36,746
I mean, here? Well, I think in this particular case is a scalar.

468
00:58:36,747 --> 00:58:41,397
So. So you can consider it while the resolving is actually this result over here.

469
00:58:42,537 --> 00:58:47,897
I mean, here it is small as a scalar. So that is an actress and Rudy is small.

470
00:58:48,507 --> 00:58:54,177
But of course, I mean, this is a more general result because small a factor can be considered as a special matrix.

471
00:58:54,207 --> 00:58:58,287
A special type of matrix. So then so this.

472
00:58:58,647 --> 00:59:01,977
There is no combination. You just is about.

473
00:59:02,277 --> 00:59:07,637
So by applying that result, we found the dirty side.

474
00:59:08,757 --> 00:59:13,347
And then let's find the derivative of the secondary.

475
00:59:17,837 --> 00:59:24,137
That's the to derivative. Obviously, this is a quadratic flaw.

476
00:59:27,107 --> 00:59:35,117
I mean, in this model T, we got a slide, I think in here.

477
00:59:36,017 --> 00:59:41,237
So if it is a square matrix, then this scalar is known as a quadratic before.

478
00:59:42,287 --> 00:59:47,267
This is because it is a radio function of x2x transpose times eight times x.

479
00:59:48,197 --> 00:59:57,677
So this is a quantity of four. Now for writing a form, if we take the derivative, the generative is actually equal to two any times x.

480
00:59:59,507 --> 01:00:06,467
If we apply that result over here, this is already for if you simply call this matrix in the middle.

481
01:00:06,497 --> 01:00:11,207
This is the eight matrix. I find that rhythm.

482
01:00:11,267 --> 01:00:16,627
This is a matrix. This is an alternative form of a community forum, allegedly.

483
01:00:16,657 --> 01:00:21,817
Also, we have a footnote indicating the result of the community writing forum.

484
01:00:24,717 --> 01:00:29,817
Now the derivative is to house eight x. Two times a.

485
01:00:31,147 --> 01:00:34,887
That's actually over here as well.

486
01:00:35,017 --> 01:00:42,307
Our aim is X transpose times x, so then two times eight, that's two times.

487
01:00:43,717 --> 01:00:47,557
Two times this guy then pops back and it's better hat.

488
01:00:49,717 --> 01:00:53,797
So these are just apply the some results from literally hasbara.

489
01:00:54,067 --> 01:01:00,366
So now we have found the derivative of. This guy endures.

490
01:01:00,367 --> 01:01:04,447
You know, this guy on the radio, the first guy who disappeared, it's simply zero.

491
01:01:05,527 --> 01:01:17,497
So if we put all these results together, now we have a majority of siac that can be called out and there is still a minus truth.

492
01:01:17,707 --> 01:01:22,357
So that's this minus two. Right here as of this moment.

493
01:01:22,687 --> 01:01:27,107
Then the dirigible. This guy. That's two times that.

494
01:01:27,127 --> 01:01:37,117
That's two times this. So in the end, well, we actually by his by applying some matrix algebra,

495
01:01:37,927 --> 01:01:47,527
we actually found the diversity of as I see we have a better at the max we just need to set

496
01:01:47,527 --> 01:01:56,497
this 2 to 0 and a solve for beta and that should give us that beta as Nate has made available.

497
01:01:57,277 --> 01:02:02,287
If we set this to zero and it's only 19.

498
01:02:02,767 --> 01:02:11,617
So if we said of this to zero, this implies that actually it's written over here.

499
01:02:11,857 --> 01:02:19,747
So making the smaller, easier to see. So if we set an equal to zero, then there's a tool kit.

500
01:02:19,817 --> 01:02:24,707
Well, this tool on both sides of the equation can cancel, and it goes to the right hand side.

501
01:02:25,277 --> 01:02:37,756
So if we divide both sides by two and then we have X transpose here we have x beta and minus A minus.

502
01:02:37,757 --> 01:02:53,227
This this is why this is a zero. And of course, this is the same as this one, because if I multiply both sides and the only difference is here is X,

503
01:02:53,227 --> 01:03:03,786
beta minus Y and here is Y, minus x that on the negative side involved but shouldn't matter because because this equation is equal to zero.

504
01:03:03,787 --> 01:03:09,247
So I can multiply both sides by negative one. I still get the same equality.

505
01:03:14,887 --> 01:03:20,017
So in other words, now surging about equal to zero, we have this equation.

506
01:03:22,607 --> 01:03:30,827
And there's a reason and this is typical in referred to as the normal equation in linear regression models is the normal equation.

507
01:03:31,487 --> 01:03:43,337
And literally that's normal when we take six or two and there's a number like when we talk about distribution, then become squared equation.

508
01:03:43,577 --> 01:03:51,587
Well, but anyway, so let's not worry too much about that. So oh, so this is the so called a normal equation, a linear regression model.

509
01:03:53,187 --> 01:03:56,997
And if we solve this equation. If we solve this equation.

510
01:03:57,657 --> 01:04:12,297
Well, this equation here, this line implies X transpose Y equal to or minus x transpose x be had had equal to zero.

511
01:04:14,897 --> 01:04:32,057
Which implies this. And in order to stall for this bit, I had noticed that here we have a matrix x five transpose times x.

512
01:04:32,807 --> 01:04:40,157
Now if this matrix is a vertical, then we are able to multiply both sides by the inverse of this matrix,

513
01:04:41,237 --> 01:04:49,487
and then we are able to solve better hash as the inverse of this matrix times and then this x transpose times flush.

514
01:04:50,657 --> 01:04:54,977
So this is simply by modifying the inverse of x transpose.

515
01:04:54,977 --> 01:05:04,696
Have the X on both sides of the equation. And this is actually under the condition that this is a vertical convertible.

516
01:05:04,697 --> 01:05:13,877
Then we can modify both sides from numerous. And typically it is convertible typically in a datacenter region.

517
01:05:15,877 --> 01:05:20,417
We have data that we use. Typically, this is in verticals.

518
01:05:23,107 --> 01:05:27,267
So then finally we found what Peter has.

519
01:05:28,057 --> 01:05:31,677
And that's not visas or an estimate of data for medical intervention.

520
01:05:33,017 --> 01:05:35,917
We went through all algebra we found out is equal to or less.

521
01:05:37,027 --> 01:05:45,567
Now, one interesting thing is that if you look at this, this guy over here is some matrix here.

522
01:05:45,577 --> 01:05:51,097
If you look at this over is some matrix multiplied by the white line multiplied by some matrix.

523
01:05:53,497 --> 01:05:54,607
And this matrix.

524
01:05:59,257 --> 01:06:16,367
If we want to make things so x and recall that x is MRC matrix, l transpose these p by n axes and by p and x transpose by m what is an entry?

525
01:06:19,417 --> 01:06:24,367
Right. So if we look at the whole thing here, why multiply by a matrix?

526
01:06:25,987 --> 01:06:29,017
Essentially it means a linear combination of y.

527
01:06:29,017 --> 01:06:32,107
So if you want to go back to white power matrix,

528
01:06:32,167 --> 01:06:37,927
that means you're making meaning or combinations of one so that each component recall that a

529
01:06:37,927 --> 01:06:43,627
y is why the Peter has p component repeat of zero in a lot better to after a bit of sheep.

530
01:06:44,347 --> 01:06:51,246
That means each component beta iron is actually a linear combination of the response factor.

531
01:06:51,247 --> 01:06:55,357
What we have seen this in similarly regression, right?

532
01:06:55,417 --> 01:07:03,067
We have we have seen this in symbolic intervention. We saw that we actually show that both beta zero and a beta one is simple linear regression.

533
01:07:03,577 --> 01:07:11,377
They are linear combinations of the response y. If you go back to those months, you let me see that slide.

534
01:07:12,487 --> 01:07:16,417
But back then, we didn't use metric a tool to do that.

535
01:07:16,507 --> 01:07:22,477
The answer is everything was calculated explicitly using using detailed expressions.

536
01:07:22,687 --> 01:07:28,927
We show that both beta zero had, a beta one had, and similarly their regression, the linear combinations of the Y.

537
01:07:29,707 --> 01:07:33,827
And here it turns out this result is much more general.

538
01:07:33,847 --> 01:07:41,677
So for modulo linear regression, the estimated beta hash, this whole matter is indeed a linear combination of this wise.

539
01:07:43,417 --> 01:07:50,527
This has implication that later we are going to when we make users, we are going to assume Y for the normal distribution.

540
01:07:51,367 --> 01:07:55,837
Now, if Y followed normal that any linear combination of Y also followed normal distribution.

541
01:07:56,137 --> 01:08:03,427
So that implies that beta had followed normal solution and similar to linear regression similarly around case

542
01:08:03,787 --> 01:08:10,926
where the beta estimate of beta so that actually that's the result based on which we will make inference,

543
01:08:10,927 --> 01:08:15,457
we will construct confidence intervals, we will test hypotheses and things like that.

544
01:08:16,027 --> 01:08:24,727
So, so but here, I mean, one point is that the beta hat that is the linear combination of the response y.

545
01:08:31,227 --> 01:08:38,687
Now let's take a look at one simple example of a toy example.

546
01:08:40,037 --> 01:08:43,337
To calculate that, I'm using matrix notation.

547
01:08:43,907 --> 01:08:48,257
So here we have a Y equal to zero plus absolute.

548
01:08:48,497 --> 01:08:53,147
Now Y. In this case, we have three individual and is equal to three.

549
01:08:53,177 --> 01:08:57,857
We just have three individual, three subjects in the transaction. So an is equal to three.

550
01:09:04,567 --> 01:09:12,877
And then of course, cars, Vallone, we have the absolute four and the three individuals and here in this case X is equal to this.

551
01:09:13,717 --> 01:09:19,477
So P is equal to two. So equal is what? What this means is that there is no intercept.

552
01:09:19,477 --> 01:09:25,177
If you look at this, these are two covariance are talking about the value of covariance.

553
01:09:25,507 --> 01:09:28,797
There is no know the intercept column. Right.

554
01:09:28,807 --> 01:09:35,197
So we need to include Peter's our column. So in other words, this regression model we are looking at,

555
01:09:35,677 --> 01:09:48,847
if you want to write it as in not in the matrix information that is actually theta one of X of five one plus two x i2 plus absolute.

556
01:09:49,297 --> 01:09:52,897
So there is no intercept in this regression models.

557
01:09:53,077 --> 01:09:59,187
Oh by the way. So here is is a table this this group in beta one there's no there's no beta zero.

558
01:09:59,857 --> 01:10:03,867
There is no intercept. So this is the model we are feeding.

559
01:10:04,267 --> 01:10:18,187
We're feeding the model without including the intercept. So for this model now we are able to quite easily calculate the beta hat by using matrix

560
01:10:18,187 --> 01:10:25,746
notation because beta hat is equal to we've just dragged that beta had any strength over there.

561
01:10:25,747 --> 01:10:38,047
And so that's x transpose times, x inverse times, x transpose times like this what beta have is in order to calculate beta how to,

562
01:10:38,047 --> 01:10:45,936
we will need to figure out what all these matrices are exposed four times x that's actually equal

563
01:10:45,937 --> 01:10:59,737
to x is x is equal to 100010 and then x transpose that's 100010 and that's one X transpose.

564
01:11:03,937 --> 01:11:09,967
And then if you really calculate this product, that's now the first element.

565
01:11:09,967 --> 01:11:14,207
That's that's the first rule times the first column. That's that's one.

566
01:11:14,227 --> 01:11:17,647
Right. And then the second element, that's second the rule.

567
01:11:17,697 --> 01:11:30,947
Those are. Your second row times, the first column, that's zero.

568
01:11:31,757 --> 01:11:37,547
And the first row times, the second column that's zero. And Central has seven column this, that's one.

569
01:11:40,577 --> 01:11:46,397
Okay. So in this toy example, now this matrix is just a two dimensional two by two other two matrix.

570
01:11:49,867 --> 01:12:03,267
Okay. And then extras, most times Y that's equal to X transpose again is 100010 and a y is one one.

571
01:12:03,577 --> 01:12:12,137
Right. And so it's equal to.

572
01:12:15,517 --> 01:12:21,397
One and one. Okay.

573
01:12:21,727 --> 01:12:28,086
So this implies beater hatch equal to now the inverse of the beta.

574
01:12:28,087 --> 01:12:34,807
How does the inverse of this x transpose times x nine times x plus most times y.

575
01:12:35,797 --> 01:12:39,057
So then is this two by two?

576
01:12:39,067 --> 01:12:47,797
And in the matrix has this vector. And that's this factory itself.

577
01:12:49,747 --> 01:13:01,397
So this is what happens. A better hat is just off to the races on the back of this two by one factor and a whole spate

578
01:13:01,397 --> 01:13:06,347
of one better one had a better to have zero equal to one what for this particular dinner that.

579
01:13:11,817 --> 01:13:18,647
And afterwards you can calculated y hat is equal to x times speed.

580
01:13:19,567 --> 01:13:24,657
That's. That's why minus X and Spader had.

581
01:13:27,167 --> 01:13:41,157
And then you can calculate what y habits. Okay.

582
01:13:41,167 --> 01:13:44,227
So that's that's this toy example.

583
01:13:44,227 --> 01:13:48,097
That's. And if you calculate the by hand. Like, how would you do it?

584
01:13:48,697 --> 01:13:52,787
But of course, in practice, there is no way that we able covered in the scanner.

585
01:13:52,787 --> 01:13:56,947
And so once we are doing this, that I think we can we can just use our profit.

586
01:13:56,957 --> 01:14:05,947
A lot of these are assessed within the model catalog for the estimates but essentially but what are were sense

587
01:14:06,337 --> 01:14:13,867
does is actually this procedure here calculating with its different viruses and against different matters.

588
01:14:15,607 --> 01:14:25,307
Any questions so far? Okay.

589
01:14:28,127 --> 01:14:33,017
And generally speaking, okay, that point is about to show us that that will be true.

590
01:14:33,027 --> 01:14:40,347
Zuckerberg But generally speaking, when we call a better how we need this matrix and its transport types X net,

591
01:14:40,367 --> 01:14:47,627
because if you look out the beta had an expression that involved this x x transpose times x matrix.

592
01:14:49,807 --> 01:14:57,277
So this matrix is now because X, if you recall what X is.

593
01:15:00,917 --> 01:15:05,327
Yes. You know, the first column corresponds to The Intercept.

594
01:15:07,737 --> 01:15:14,287
That's the column of all the ones intercept. And the second column, that's the value for the first variable x one.

595
01:15:14,297 --> 01:15:23,537
So that's 11x21. So that's in one piece about x one values for different individuals.

596
01:15:24,077 --> 01:15:36,337
And then we have for last, that's the P minus one covariant.

597
01:15:42,497 --> 01:15:52,727
And this is what an X is. And then a transpose, of course, because many of the rules for the columns and columns to be the rules.

598
01:16:16,207 --> 01:16:32,487
This is x transpose times x. And then if you were indeed to carry out the the the matrix algebra of television now,

599
01:16:32,647 --> 01:16:38,467
for example, the very first element, now the result is give us our next slide on the next page.

600
01:16:38,677 --> 01:16:47,007
For example, if you look at this particular element, this in this is actually the first row times,

601
01:16:47,137 --> 01:16:52,477
the first column multiplied by the first column, the inner in our inner part between the first row and first column.

602
01:16:53,287 --> 01:16:58,697
Of course, it gives us this because because all the elements are one so we can claim a of.

603
01:16:59,137 --> 01:17:04,017
And once you get. Okay, so that's that's the end.

604
01:17:04,567 --> 01:17:08,556
And let's take a look at another element.

605
01:17:08,557 --> 01:17:15,497
For example, let's say let's say and say and this atom on the right,

606
01:17:15,787 --> 01:17:21,987
this is not if you look at this element, this is the first rule, the side of the column.

607
01:17:21,997 --> 01:17:27,607
This element in first wrote in the second part. And then we just will leave it now,

608
01:17:27,617 --> 01:17:36,847
which is defined as the first rule from this matrix and then the second column from from this matrix and then through the inner product.

609
01:17:37,177 --> 01:17:44,347
So that then that becomes the sum of all the elements in the second column, in the same column here.

610
01:17:44,677 --> 01:17:54,147
And that gives us. Right.

611
01:17:54,237 --> 01:18:03,957
In other words, I mean, this this this matrix here does not lead to the exquisite calculation of the product of this tool reduces.

612
01:18:15,307 --> 01:18:19,807
Okay. And that's this matrix.

613
01:18:19,807 --> 01:18:22,237
And in beta, how do we also have this matrix?

614
01:18:23,107 --> 01:18:34,477
Well, there's I'll have X transpose times Y now and stressful times Y is equal to the real world right over here.

615
01:18:34,897 --> 01:18:40,647
This is there's more space X transpose times Y that's ans transpose as we know them.

616
01:18:40,697 --> 01:18:56,486
That's. That's the sky and palms.

617
01:18:56,487 --> 01:19:00,237
yyy1. Y two. And y in.

618
01:19:09,307 --> 01:19:14,137
And then if we calculate, you need to calculate this product.

619
01:19:15,997 --> 01:19:27,067
For example, let's say we look at here, if we look at the second, the second group, the second element, the second.

620
01:19:27,907 --> 01:19:32,917
Q then this is equal to the calculation.

621
01:19:32,917 --> 01:19:36,007
That's the second the rule of this matrix times.

622
01:19:36,007 --> 01:19:47,407
This factor is called entry here so that it becomes x11 times Y, 1x2, one times y2, and then taking the sum of all these, this becomes.

623
01:19:49,987 --> 01:20:00,567
That becomes like. Okay.

624
01:20:00,567 --> 01:20:04,197
So that's how these two movies are calculated.

625
01:20:06,837 --> 01:20:14,747
Okay. Are we okay with this coverage? I'm not sure whether we were going to to slow her a little bit fast.

626
01:20:16,007 --> 01:20:24,517
If you have questions, just let me know. Okay.

627
01:20:29,137 --> 01:20:40,416
And then. Well, okay. So here because in while we calculate if you haven't if you look at the east region

628
01:20:40,417 --> 01:20:46,387
locality B that had involved this inverse matrix inverse of X transpose Hamas.

629
01:20:46,957 --> 01:20:50,317
So when we talk about inverse, we are assuming this to be inverted.

630
01:20:51,427 --> 01:20:55,897
But when we talk about the inverse ability of this matrix, we need to be clear.

631
01:20:55,897 --> 01:21:04,057
What is this what the root of this matrix is? And so this actually then naturally involves the rest of the matrix.

632
01:21:04,567 --> 01:21:12,696
So these concepts we have seen, so we have if the factors are linear involved and there are linearly matter, if any,

633
01:21:12,697 --> 01:21:21,697
you know, combination of not equal to zero is not equal to zero unless all the common collisions are equal to zero.

634
01:21:22,327 --> 01:21:32,917
And also the, the, the rank we say actually is for rank is rank is equal to the minimum number of its rows and number of its columns.

635
01:21:33,127 --> 01:21:37,567
Otherwise, axes around diffusion. Now we review this in our last lecture.

636
01:21:39,757 --> 01:21:44,307
Now, here is one simple example. So now let's take a look at this one simple example.

637
01:21:44,317 --> 01:21:47,496
So what is the root of this matrix for?

638
01:21:47,497 --> 01:21:56,327
For this very simple example? What movies around this? And guess.

639
01:21:59,687 --> 01:22:02,897
This matrix. Was that, too?

640
01:22:03,257 --> 01:22:09,977
Yes. So the reason why is true, because neither are linear combinations.

641
01:22:10,847 --> 01:22:19,447
Okay. Yeah. So here, if don't matter whether because these two items wouldn't matter whether you looking at the rules that the rules were two columns,

642
01:22:19,457 --> 01:22:26,957
let's say two columns. And so if you look at these two columns here, neither one can be written as something more.

643
01:22:27,197 --> 01:22:37,697
Or if you look at ac1 times, this is x1x2 as ac1 times x one plus C to have actually zero.

644
01:22:38,027 --> 01:22:43,637
Now, if you look at this, the only way you can see what the C two both equal to zero.

645
01:22:44,177 --> 01:22:48,587
Are you able to get plus the sum equal to zero?

646
01:22:48,857 --> 01:22:52,307
So these are two factors that are linearly independent.

647
01:22:52,607 --> 01:22:58,307
So that's why this writer is is equal to. So then what about this example?

648
01:23:01,397 --> 01:23:10,477
The room this matrix's. Is it also true?

649
01:23:10,837 --> 01:23:18,397
It's also true. The reason? Because the first two rows are linear combinations of each other.

650
01:23:18,397 --> 01:23:21,717
And then this. The bottom two are. Okay.

651
01:23:21,727 --> 01:23:25,617
Yeah, that's great. So, yeah, you're looking on a different, different rules.

652
01:23:25,627 --> 01:23:29,946
Right? So. So he's probably easier to look at a column because there are only three columns, right?

653
01:23:29,947 --> 01:23:36,997
So if you look at different columns, then let's say X one extra extracts, three three columns,

654
01:23:38,047 --> 01:23:43,747
we see that x two plus and three if we pay you a sum of ads to enact three.

655
01:23:44,887 --> 01:23:49,027
Right. But that gives us column of of all tense events.

656
01:23:49,267 --> 01:23:53,107
If we divide by ten, it's not a reverse x one.

657
01:23:54,947 --> 01:24:01,817
Right. So in other words, now X1 is actually a linear combination of the other two columns, x2 and x3.

658
01:24:04,247 --> 01:24:08,387
And this means about one column is linear accommodation of the other.

659
01:24:09,077 --> 01:24:19,157
And then if you look at extract three X, we three apparently are not dependent so so again here the rabbit is equal to.

660
01:24:20,807 --> 01:24:28,007
These are just two very simple examples showing what a wreck he was pushing.

661
01:24:28,397 --> 01:24:31,997
And then. Okay.

662
01:24:32,237 --> 01:24:41,857
I think we probably don't need to get into so but the rank I mean the rank of X but these are matrix is equal to around its

663
01:24:41,867 --> 01:24:52,097
transpose and also is equal to a rank of this matrix and anaphora is equal to the rank of its transpose matrix this matrix.

664
01:24:52,757 --> 01:25:03,617
So because our normal equation involves x transpose time sex this inverse, right, we will need the inverse of this matrix.

665
01:25:04,037 --> 01:25:07,307
So what we are interested in is the rank of this matrix.

666
01:25:09,137 --> 01:25:13,067
But because the root of this matrix is equal to the rank of the desired matrix.

667
01:25:13,067 --> 01:25:24,616
So we just need to focus on the root of the design metrics and the rank of design matrix is actually equal to this matrix here,

668
01:25:24,617 --> 01:25:29,627
this x transpose time x, this is a p by p matrix.

669
01:25:30,077 --> 01:25:38,207
The reason is that x is p by its transpose right and x is in by p.

670
01:25:39,167 --> 01:25:47,717
So the product then is a p by p matrix. And this matrix PMP matrix is a full rank.

671
01:25:48,317 --> 01:25:51,767
It is rank is equal to the number of these columns.

672
01:25:51,977 --> 01:26:04,547
The number is nobody's rules. So for this a to be full rank now this round has to be equal to P, but based on this fact here,

673
01:26:04,697 --> 01:26:08,447
based on the fact is Israeli is also equal to the rank of matrix.

674
01:26:09,197 --> 01:26:15,447
So in order for this matrix to be in vertical. In order to be able to counter the inverse.

675
01:26:16,137 --> 01:26:28,617
That means that these are metrics. That these are metrics has to be full rec has to be for rec and recall that these are metrics.

676
01:26:29,517 --> 01:26:33,197
These are metrics is an embedded key metrics. Right?

677
01:26:33,417 --> 01:26:37,887
We have individuals and we have P covariates, p columns.

678
01:26:38,037 --> 01:26:45,836
So my P metrics and the UAD the symbol size n is a lot larger, the number of parameters, number of number, covariance.

679
01:26:45,837 --> 01:26:51,387
So you may have, you know, 300 or 500 individuals in the dataset and that's your sample size.

680
01:26:51,387 --> 01:26:56,787
N And you may have 1015 covariates in your data set.

681
01:26:57,147 --> 01:27:08,077
So typically N is larger than P now because the rank is actually the smaller of the number of rules and of the number of columns and the smaller ones.

682
01:27:08,337 --> 01:27:10,947
So typical a P smaller than N.

683
01:27:11,607 --> 01:27:23,247
So that means in order for the for the X, these are metrics to be a full rank, then it has to have full column rank, call them.

684
01:27:25,257 --> 01:27:35,357
So in other words. In other words. Now the important point is that in order for the matrix, for this matrix to be vertical,

685
01:27:36,957 --> 01:27:40,287
so you have to be able to calculate the inverse of this matrix.

686
01:27:41,517 --> 01:27:48,867
Where we look at all of a matrix X, we look out these are matrix X, we call that matrix.

687
01:27:50,577 --> 01:27:57,227
Let's go back to our matrix. Yeah.

688
01:27:57,237 --> 01:28:02,307
This is the design matrix, right? This is are matrix recall that you know, has different column.

689
01:28:02,307 --> 01:28:09,227
This is an intercept column. This is x one covariate, x two covariate and x p minus one.

690
01:28:09,237 --> 01:28:17,577
So in total we have p columns. So for this matrix four for the x transpose time x to be in vertical,

691
01:28:17,577 --> 01:28:24,087
this makes these arbitrary has to be a full rec for call rec in order for it to be full color rec.

692
01:28:24,897 --> 01:28:29,367
That means this different columns. They cannot have a linear combination of a triangle.

693
01:28:30,417 --> 01:28:39,137
They have to be linearly independent. But that's not the point that we are trying to make.

694
01:28:40,137 --> 01:28:53,737
On this slide. Q So no column of X can express a linear combination of the other columns.

695
01:28:55,957 --> 01:29:07,147
This is the important point of in order for your host to be able to carry out the estimation of multiple linear regression model.

696
01:29:07,717 --> 01:29:16,717
So if you look at the different columns of x x measures or if you look at yeah, look how different column is that matrix, there's different columns.

697
01:29:16,957 --> 01:29:20,467
This should be linear, independent, otherwise.

698
01:29:21,577 --> 01:29:25,237
Otherwise we will not be able to cover of an inverse.

699
01:29:25,507 --> 01:29:39,547
Calculate this inverse matrix. So what does the what does it mean by you know, the columns are not linear combination of each other.

700
01:29:39,557 --> 01:29:48,276
It it means that different covariance they well they should be different color matter they shouldn't for example let's

701
01:29:48,277 --> 01:29:57,727
say let's let's consider extreme example I mean this is the extreme but you get I think it helps us to get the idea.

702
01:29:59,017 --> 01:30:01,326
So let's say that in this design matrix.

703
01:30:01,327 --> 01:30:10,947
Lessing this x1x x2, these variables, let's say they are exactly the same their unless they both are h as in both are H.

704
01:30:11,647 --> 01:30:19,386
But I didn't tell you, Wolf, I just gave you this data. In fact, when you have this data set, you don't know as they both are.

705
01:30:19,387 --> 01:30:24,606
H Because both are. H Now of course, this.

706
01:30:24,607 --> 01:30:27,817
H For the first individual, this values are exactly the same.

707
01:30:28,777 --> 01:30:36,127
But then the second row here for this individual, the H, this two numbers for the values are similar.

708
01:30:36,127 --> 01:30:44,917
These two numbers should be exactly the same. So that means these two columns that are duplicates, this one column is duplicates on the other,

709
01:30:44,947 --> 01:30:49,986
of course, then then this column can be written as meaning a combination of the other column,

710
01:30:49,987 --> 01:30:55,687
because you could put all the on the column, a coherence would be zero, and the color coded in for this column could be one.

711
01:30:56,107 --> 01:31:05,487
And you have a linear accommodation. So in this case, if you have duplicate duplicate columns, then x transpose sums.

712
01:31:05,497 --> 01:31:12,487
X, this matrix is not a vertical. So if you want to calculate the inverse, inverse does not exist.

713
01:31:14,227 --> 01:31:19,597
And again we have a fit of model. Well, you get errors from our or from SAS.

714
01:31:20,047 --> 01:31:30,067
This is the same as this line. Depending on which software, they're not even square and they have different exact message, error message.

715
01:31:30,067 --> 01:31:35,827
But essentially it means that this matrix is not a full rank, so that this matrix is not a vertical.

716
01:31:37,717 --> 01:31:40,537
So that's an extreme case where you have duplicate columns.

717
01:31:40,807 --> 01:31:50,286
But imagine a case where they are not duplicates, but they are highly correlated and the collision is very hard, like, let's say edge.

718
01:31:50,287 --> 01:31:55,207
And I don't know, I have an example that's an age and a height for adolescence.

719
01:31:55,207 --> 01:31:57,667
Right. And then they're highly correlated.

720
01:31:59,227 --> 01:32:06,037
And again, when you calculate this matrix model, although is still in vertical, mathematically may still be vertical,

721
01:32:06,667 --> 01:32:12,187
by the way, but inverted you are going to get some huge numbers, some pure numbers.

722
01:32:12,487 --> 01:32:19,537
And that's the thing we always said, like when you have medical imperative and that would be trouble when you fit at the moment,

723
01:32:20,137 --> 01:32:26,697
because when you take an inverse of this matrix, there will be some huge numbers show.

724
01:32:29,877 --> 01:32:35,787
Sometimes you can do so. So this is the so-called clean energy.

725
01:32:39,997 --> 01:32:43,047
Okay. Okay.

726
01:32:43,087 --> 01:32:57,787
So any questions? And that let's take a look at the properties of three square meters and we derive as a meter let's look at the property,

727
01:32:57,787 --> 01:33:03,366
the properties while we still focus on the properties as before and we want to calculate the expansion.

728
01:33:03,367 --> 01:33:07,357
One to calculate the variance of this estimate has less to do than one by one.

729
01:33:07,997 --> 01:33:14,717
But for us, if you calculate these foundation. Now Peter had is equal to this.

730
01:33:14,737 --> 01:33:17,917
This is what we just dropped, right? We just drop it.

731
01:33:18,757 --> 01:33:22,627
Now we come on these foundation. Now they use foundation here.

732
01:33:22,927 --> 01:33:28,747
As we measure, we treat this acts as fixed numbers so we can pull this out of the anxiety.

733
01:33:30,247 --> 01:33:34,237
Now it becomes this guy and multiply by the foundation of y.

734
01:33:38,107 --> 01:33:42,817
And as we said of these foundation, why is equal to x x times better?

735
01:33:44,317 --> 01:33:51,187
And that's the foundation of why that multiplied by the matrix in the front.

736
01:33:54,507 --> 01:33:54,777
Okay.

737
01:33:56,407 --> 01:34:07,117
So then if we look at this guy over here, if we combine this two matrices, an event, this is the inverse of the matrix and this is the measure itself.

738
01:34:07,657 --> 01:34:13,027
Matrix ends up. So this guy. CARLSON This is a what is left is this data.

739
01:34:16,427 --> 01:34:22,997
Okay. So what this means is that now the expectation of being a hack is equal to better.

740
01:34:24,787 --> 01:34:33,637
So Peter had is an unbiased estimate. As we mentioned, for simple linear regression, the beta half, both beta has zero and beta had one.

741
01:34:33,637 --> 01:34:40,327
Their advice as measure of beta. Now here for multiple linear regression, it turns out we have the same result.

742
01:34:41,397 --> 01:34:45,567
So the whole bit of actor Peter Hatch things about Asian women,

743
01:34:47,097 --> 01:34:53,637
what it means is that if you take a different data set so you can take different samples,

744
01:34:53,637 --> 01:35:00,776
you'll calculate a better match that on average you will get a better this true better average,

745
01:35:00,777 --> 01:35:04,677
you will get it back for each status, adding together different matter.

746
01:35:06,297 --> 01:35:17,607
So this is the this is stuff. So what this means is that to have this unbiased.

747
01:35:21,657 --> 01:35:29,967
S major. Okay.

748
01:35:30,357 --> 01:35:37,467
And now let's calculate the merits of beta hatch from the Verizon beta hatch.

749
01:35:39,207 --> 01:35:42,057
Now we have beta hat is equal to this guy.

750
01:35:43,587 --> 01:35:52,917
Now the various beta hat, if we call this matrix matrix right here, we are going to use this result over here.

751
01:35:55,137 --> 01:36:02,217
The bearers of a matrix eight times of actor y is actually equal to eight times the variance, times a transpose.

752
01:36:04,517 --> 01:36:08,867
This is the property of the virus, variants of the vector.

753
01:36:09,497 --> 01:36:20,057
So if we call this matrix A that if we apply a lot of that here, we have a times the variance of Y, times a and this is a principles.

754
01:36:26,947 --> 01:36:35,987
This is a principles. And the theories of why we know that it's equal based on our assumption.

755
01:36:36,047 --> 01:36:41,087
Recall that we've made some assumption that Armstrong variances equals fewer squares

756
01:36:41,657 --> 01:36:46,217
and matrix than is the variance of wise is also seen much more times than the matrix.

757
01:36:48,657 --> 01:36:52,137
So that's why here we have similar sort of other metrics in the middle.

758
01:36:54,997 --> 01:36:58,757
No, but a seamless square is a scooter. I see. One square is just a number is a scooter.

759
01:36:58,757 --> 01:37:08,897
So of course we can put it in front of everything, we can move it to the front and I don't know, matrix in the middle, it has no effect, right.

760
01:37:08,897 --> 01:37:13,067
Because everything, anything multiply by it is still not a thing in itself.

761
01:37:13,697 --> 01:37:27,307
So I know he doesn't play a lot so that what is left, this whole thing here, what is left is equal to a times a transpose where a is equal to.

762
01:37:32,107 --> 01:37:37,177
A transpose is equal to x times, x transpose X times Imus.

763
01:37:41,137 --> 01:37:48,667
Okay. And then if we look at this matrix, then this guy council with this guy.

764
01:37:50,017 --> 01:37:55,657
So what is left is just x times x transpose some extent inverse.

765
01:37:59,267 --> 01:38:07,756
So this is the variance of beta. That is very simply that we see that to use in matrix notation.

766
01:38:07,757 --> 01:38:17,656
I mean of course amount we will have to be familiar with all the matrix algebra in order to, to carry out this calculation.

767
01:38:17,657 --> 01:38:20,267
But once we are familiar,

768
01:38:20,267 --> 01:38:30,947
we become familiar with the iceberg and here it becomes a lot more general and more and easier than using than, than previously.

769
01:38:30,947 --> 01:38:35,837
I mean we compare it to the values in matrix relation.

770
01:38:37,397 --> 01:38:46,757
Okay. So then what we did was we calculated on the A's foundation, we found out about a beta that is unbiased as measure of beta.

771
01:38:47,657 --> 01:38:56,207
And also we calculated the merits of beta and we found the virus is equal to is equal to this.

772
01:38:59,327 --> 01:39:03,797
Then again, here. Now, if we consider the case a medical narrative.

773
01:39:04,097 --> 01:39:11,057
So if if the design matrix columns are highly correlated or highly correlated, that,

774
01:39:11,057 --> 01:39:18,527
as we mentioned, this matrix in the middle becomes almost nine vertical verticals.

775
01:39:18,977 --> 01:39:20,927
That way you calculate an inverse.

776
01:39:21,047 --> 01:39:26,847
As we mentioned, when counted, immersed, you're going to see some huge numbers because it's almost a singular matrix.

777
01:39:26,867 --> 01:39:28,277
You can't really get accurate numbers.

778
01:39:28,727 --> 01:39:39,497
And then that means if you look at the various operator hat, you're going to see that some beta had to have huge numbers.

779
01:39:40,757 --> 01:39:47,837
And again, if you look at an output from R, if you look at the center error column for estimate, you're going to see huge center error.

780
01:39:48,077 --> 01:39:51,527
So that's usually a sign that you have multiple in error.

781
01:39:51,677 --> 01:40:02,057
If you if you see abnormally large center error for your estimate, that's a sign that you may have medical an error in the model made.

782
01:40:03,907 --> 01:40:10,967
You know, mathematically, it comes from the fact that the virus is equal to the inverse of this matrix.

783
01:40:21,957 --> 01:40:25,677
Okay. So you think.

784
01:40:29,397 --> 01:40:37,196
Yeah. I think we should we should be able to finish this slide. So for lease the square as a major, like one very important result.

785
01:40:37,197 --> 01:40:40,647
Well, the result is the so-called August Merkel theorem.

786
01:40:41,757 --> 01:40:51,087
So it shows that the linear at the start of the lease, the square meter is operable in certain sense, is the best in certain sense.

787
01:40:52,017 --> 01:41:02,277
So if we assume for this malleable linear regression model, right, there's the model of looking at and we are assuming, you know, it's material error.

788
01:41:02,277 --> 01:41:06,957
U0, a variance of error equal to C must times the matrix.

789
01:41:08,007 --> 01:41:15,267
Then the beta hat that we do right by minimizing the sun was where an error somewhat squares the beta.

790
01:41:15,267 --> 01:41:21,416
How we derived it is optimal. It is the best, but best in what it says.

791
01:41:21,417 --> 01:41:27,647
Best in the sense that it achieves the minimum variance among all of the linear.

792
01:41:27,657 --> 01:41:34,557
I'm biased as the meters or this is referred to the the blue lagoon.

793
01:41:35,637 --> 01:41:40,227
So the blue the best linear unbiased as matrix for that.

794
01:41:43,677 --> 01:41:47,937
So first of all while beta the is linear, right?

795
01:41:47,937 --> 01:41:55,317
So while the linear is in in in the sense that while we actually that here

796
01:41:56,397 --> 01:42:02,397
the linear is in the sense that it's a linear combination of the response y.

797
01:42:03,667 --> 01:42:04,937
That's what leave me.

798
01:42:05,827 --> 01:42:15,247
So you can cause well, for example, when you can construct any linear combination of this life and a clock and use it to ask me to be here,

799
01:42:15,457 --> 01:42:23,496
and there is nothing prevented from doing that. You can construct an arbitrary accommodation on lot and a college and as the made

800
01:42:23,497 --> 01:42:27,937
it but he's just a rather that as a meter is a good ask me to work as a major.

801
01:42:28,597 --> 01:42:36,007
But what I can believe there is nothing preventing you from from construct, from constructing a arbitrary accommodation and use that to assimilate it.

802
01:42:37,537 --> 01:42:45,187
And it turns out out of this being a hash we derive, first of all, it's a linear escalator, so it's a linear combination of the one.

803
01:42:45,877 --> 01:42:49,627
And a second of all is a bar is that meter as we just construct.

804
01:42:50,017 --> 01:42:55,777
So it's a biased exaggeration is equal to the foundation, equal to the.

805
01:42:56,287 --> 01:43:02,647
So it's on bias. And of course, there are many, many linear asymptote that are biased.

806
01:43:04,267 --> 01:43:07,417
I mean, here we are not going to constrain others,

807
01:43:07,437 --> 01:43:19,737
but there are many other linear escalators that are unbiased that it turns out that the theta hat is the best among so few on last estimation,

808
01:43:20,437 --> 01:43:24,247
best in the sense that it has the minimum barriers.

809
01:43:24,667 --> 01:43:27,157
If you look at all these linear based asymmetries,

810
01:43:27,457 --> 01:43:35,697
you are able to construct that if you're if you look at a better have a better have has the least that has minimal preference.

811
01:43:40,537 --> 01:43:48,967
So for example, that for any you know, let's say beta had a prime this is another meta analysis major that we are as

812
01:43:48,967 --> 01:43:54,367
a beta hat is smaller than or equal to the variance of any other such thing.

813
01:44:01,567 --> 01:44:06,947
Okay. So we are two at times, so maybe I think we should have stopped here in a few minutes to.

