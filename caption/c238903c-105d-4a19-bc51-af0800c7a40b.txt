1
00:00:00,630 --> 00:00:01,980
Okay. So.

2
00:00:07,230 --> 00:00:27,810
So if you recall, we had started talking about different summary statistics and we talked about estimated of the mean and we showed that or you know,

3
00:00:27,990 --> 00:00:39,120
that the estimates of the population mean mean y is basically the sample mean.

4
00:00:40,470 --> 00:00:50,129
So my white hat is the most some of the wise these are the sample observations and we

5
00:00:50,130 --> 00:00:56,340
assume that we are deriving a random simple random sample from some underlying population.

6
00:00:56,340 --> 00:01:02,310
Simple random sample means that basically each of the drugs that we are making from the

7
00:01:02,320 --> 00:01:10,590
population are independent and they are identically distributed from that population.

8
00:01:11,670 --> 00:01:17,579
So when we have a simple random sample from some underlying population,

9
00:01:17,580 --> 00:01:24,450
we can derive estimates of each of the population quantities that we defined previously.

10
00:01:24,450 --> 00:01:28,920
And as the first example, we talked about the estimated of the mean.

11
00:01:29,460 --> 00:01:42,710
So my white hat is the sample mean y bar and I see that applying and the properties are

12
00:01:42,720 --> 00:01:49,799
the results of of expectation and variance like sum of random variables and we you know,

13
00:01:49,800 --> 00:01:53,700
sort of review basic results about expectation and variance.

14
00:01:54,120 --> 00:01:58,200
Expectation of some is equal to some of the expectations.

15
00:01:58,920 --> 00:02:05,970
And then we also reviewed the variance formula and based using those results,

16
00:02:07,230 --> 00:02:17,250
one can show that expectation of y bodies equal to mean y and variance of y bar is equal to sigma y squared over n,

17
00:02:17,250 --> 00:02:21,120
but sigma y squared is the population variance.

18
00:02:21,120 --> 00:02:31,409
This is unknown and little n is the sample size that are the size of the sample that we are drawing from that underlying population.

19
00:02:31,410 --> 00:02:42,900
So everybody fine then here some other review concepts, obedience and obedience estimated to estimate disobedience.

20
00:02:43,860 --> 00:03:02,280
So once again recall that this is unknown sigma y squared typically is unknown and if the population mean, the population mean is m.y.

21
00:03:02,280 --> 00:03:21,179
Remember, if this is norm for some reason then sigma y squared can be estimated as one of what n summation i from one to n y minus mu y squared.

22
00:03:21,180 --> 00:03:33,540
So basically again, you know, like remembering sort of the, the, the definition of variance is some measure of dispersion around the mean.

23
00:03:33,540 --> 00:03:47,070
So if I do know the unknown population mean y y, then I'm going to estimate the unknown population variance as a quantity,

24
00:03:47,080 --> 00:03:55,920
as a dispersion sample squared deviations of y from the population mean.

25
00:03:55,950 --> 00:04:06,329
So that's this quantity here. So if the population means non, however, in most situations the population mean is not known.

26
00:04:06,330 --> 00:04:19,590
Mean Y is unknown typically. And in in in a situation like that, the way we estimate sigma y squared is now,

27
00:04:19,590 --> 00:04:36,329
remember we also have to estimate the population mean and in the previous slide we reviewed the results that if y1y Dubai is not independent,

28
00:04:36,330 --> 00:04:45,930
identically distributed, if they comprise a simple random sample from a from another population with unknown mean mean y,

29
00:04:45,930 --> 00:04:51,719
then you can estimate that unknown mean new y using the sample mean y.

30
00:04:51,720 --> 00:05:03,959
But so we estimate we are using y bar and we plant that in the formula for variance to get an estimate of the variance.

31
00:05:03,960 --> 00:05:17,400
So sigma y squared hat if the population mean is unknown, is given by one word minus one summation are equal to one and y minus five plus square.

32
00:05:18,420 --> 00:05:23,130
So at this point, I'm going to pause and see if there there is a question.

33
00:05:27,200 --> 00:05:30,950
Maybe I'll give a hint. I'm expecting a question here.

34
00:05:34,860 --> 00:05:38,830
Why? Yeah.

35
00:05:39,300 --> 00:05:43,680
So. So did everybody hear the question? This is precisely the question I was expecting.

36
00:05:43,690 --> 00:05:50,260
Why is it that in the first expression I have one overridden and in the second expression, I have one word in minus one.

37
00:05:51,610 --> 00:05:59,670
Okay. So here is the simple answer that I'm going to give now because I need machineries to sort of well,

38
00:05:59,680 --> 00:06:06,040
this formally, which will happen both in 601 and in 650.

39
00:06:06,460 --> 00:06:14,890
But for now, the simple answer is, remember I said, if the population means unknown, I need to estimate that quantity, right?

40
00:06:15,190 --> 00:06:18,700
So and the value estimate is with Y bar.

41
00:06:19,240 --> 00:06:23,860
So basically the concept is I'm using what is called what you'll see later.

42
00:06:24,670 --> 00:06:31,330
I'm using one degree of freedom to estimate that that population mean.

43
00:06:32,020 --> 00:06:36,100
So basically what happens is that's why I,

44
00:06:36,310 --> 00:06:45,490
I have the denominator as my effective in one way you can think of my effective sort of sample size

45
00:06:45,490 --> 00:06:52,149
denominator becomes N minus one because I'm using up one degree of freedom to estimate the new Y.

46
00:06:52,150 --> 00:06:56,290
So that's the simple answer. And we will sort of prove it formally.

47
00:06:56,290 --> 00:07:03,790
You will prove it fast, actually in 6 to 1, and then we'll kind of see it again in 650.

48
00:07:04,550 --> 00:07:11,350
Another way of or maybe I'll tell you a little bit more.

49
00:07:12,310 --> 00:07:21,250
So you will also establish in six or one when you learn about the properties of estimated hours.

50
00:07:22,360 --> 00:07:26,230
So you learn the concept of bias.

51
00:07:28,390 --> 00:07:40,590
And it turns out that if the population mean is unknown, then the sigma y head squared with the divisor is in.

52
00:07:40,600 --> 00:07:46,719
Minus one is an unbiased estimate, whereas of the population variance.

53
00:07:46,720 --> 00:07:53,170
Whereas if you take the divisor as one over ten, it is not going to be an unbiased estimate.

54
00:07:53,350 --> 00:07:58,000
So there are like, you know, this kind of good properties of of this estimate there.

55
00:07:59,500 --> 00:08:02,560
Okay, no, I see hundred of the covariance.

56
00:08:02,770 --> 00:08:06,819
Suppose I have pairs x1y, 1x2y,

57
00:08:06,820 --> 00:08:21,220
two x and y and so in like generically x i is that is the pair of data I have and they are independent identically distributed in the sample sizes.

58
00:08:22,330 --> 00:08:42,250
Well, I don't one to know what is for variance covariance is as we saw in a couple slides back the whole billions is defined by this quantity

59
00:08:42,250 --> 00:08:56,350
here so expectation you take the product of the deviations of y and X from their respective means and take the expectation of that.

60
00:08:56,860 --> 00:09:04,030
So if you if you stare at this quantity basically now I'm saying like me Y and New X are on one, right?

61
00:09:05,500 --> 00:09:09,790
So how do I estimate this based on sample quantities.

62
00:09:10,030 --> 00:09:16,689
So I'm just going to use sort of the sample analogs of these quantities,

63
00:09:16,690 --> 00:09:26,440
the deviation of the wise from from the mean and the deviation of the X's from the mean and estimate the covariance.

64
00:09:26,440 --> 00:09:31,510
So lo and behold, what is the estimated hour for women's x y?

65
00:09:31,810 --> 00:09:37,960
So I have I replaced mean y by y bar and reworks by expr.

66
00:09:38,170 --> 00:09:43,420
So Miller like mimics x is not random, x is fixed right?

67
00:09:43,630 --> 00:09:47,620
So explain is does the sample mean of the x values and y bodies?

68
00:09:47,620 --> 00:09:55,569
The sample mean of the y values and I dig the deviation of each y from y bar each x from x party,

69
00:09:55,570 --> 00:09:59,770
the product, some of the products, and then divide by n minus one.

70
00:10:00,460 --> 00:10:04,960
Okay. So the same concept as in the as in the previous slide.

71
00:10:06,880 --> 00:10:10,150
And so that would give me an estimate of the audience.

72
00:10:10,450 --> 00:10:15,009
What about the correlation or correlation between X and Y?

73
00:10:15,010 --> 00:10:17,530
So what is the definition of correlation?

74
00:10:17,530 --> 00:10:26,320
Correlation is the pool billions of explain why divided by the square root of the product of the variances of X and Y.

75
00:10:27,250 --> 00:10:32,770
Right. So I can I can estimate the correlation how to estimate that.

76
00:10:33,100 --> 00:10:37,760
So this would be basically obedience medians. Of X, Y, Z.

77
00:10:38,930 --> 00:10:45,649
So the estimate, based on the sample quantity divided by I know how to estimate the variance of

78
00:10:45,650 --> 00:10:56,510
y on the seat my wife's equal and the variance of x squared of the whole thing.

79
00:11:01,730 --> 00:11:05,320
Okay. Right.

80
00:11:05,800 --> 00:11:09,580
So that's basically what I'm doing.

81
00:11:10,210 --> 00:11:13,840
Yes, very excellent. Okay.

82
00:11:13,840 --> 00:11:17,890
This is another question I was expecting. Should the variance of X that I had done not.

83
00:11:23,470 --> 00:11:27,310
The beginnings of it. So what happens? Should.

84
00:11:28,910 --> 00:11:34,320
Okay. Well, how many people think it should have had?

85
00:11:35,730 --> 00:11:40,800
I guess it's a bit ambiguous. How many people think it should have a.

86
00:11:41,970 --> 00:11:49,450
Let's see. Yes.

87
00:11:50,230 --> 00:11:53,800
Why? Why do you think it should ever have? Because.

88
00:12:03,280 --> 00:12:13,770
So is it random? Is it x random or.

89
00:12:17,850 --> 00:12:24,720
I guess I'll give you up. I'll give you a soda for 1:30.

90
00:12:25,080 --> 00:12:26,440
Okay. So here is the thing.

91
00:12:26,480 --> 00:12:37,049
So the way making the linear regression model and maybe I should make it a make make it clear as to why deliberately at this point did not look ahead.

92
00:12:37,050 --> 00:12:50,610
But I was hoping for a question. So in the linear regression model like X is considered since X is not a random variable.

93
00:12:54,220 --> 00:12:57,520
So I know it takes I know day spa.

94
00:12:57,520 --> 00:13:07,630
I know the baby and something but I mean. But the reason why I'm going to give you up and give you a half point and maybe here I will pull back.

95
00:13:07,960 --> 00:13:17,050
If you ever feel like this is a general formula that X and Y are both random in that case.

96
00:13:17,050 --> 00:13:20,290
Sure, I need to put that so.

97
00:13:20,320 --> 00:13:25,900
New X is unknown. New Y is unknown. X and y are both random variables.

98
00:13:26,230 --> 00:13:35,680
And so I think the fact that they're but I mean, I want you to remember for this, this is basically for in general,

99
00:13:35,860 --> 00:13:45,220
if both X and Y are random variables in the linear regression context X is not, then that makes it that seems to be fixed.

100
00:13:45,940 --> 00:13:55,510
So given all these know the sample of billions of x, we will always know the minimal case.

101
00:13:56,320 --> 00:14:00,940
Yep. So yeah, this despite being kind of a bit ambiguous,

102
00:14:00,940 --> 00:14:10,479
but for the sake of generality and for the sake of this being a general result, we are arguing that X and Y both random.

103
00:14:10,480 --> 00:14:19,790
And so you can see like per square mile y you have small y you had.

104
00:14:26,980 --> 00:14:31,810
Sometimes measurements of X, including markings that may have made an error.

105
00:14:31,880 --> 00:14:38,110
Yeah, yeah, yeah, yeah. So we have come to that. Yeah. And that's a very valid question.

106
00:14:38,140 --> 00:14:41,740
There's a whole body of literature on recommendation,

107
00:14:41,740 --> 00:14:49,299
more than 60 even models that we are going to talk about with a view that there is no measurement better

108
00:14:49,300 --> 00:14:57,130
already verified positive that is negligible to now there would be a feeling that x is generally not random.

109
00:14:57,340 --> 00:14:58,700
But you are. You are right.

110
00:14:58,700 --> 00:15:09,910
There is there could be situations, for example, we are like lab assays or you're measuring something in a device, medical device.

111
00:15:10,210 --> 00:15:16,690
So there might be sort of B-to-B value that you will find across different labs.

112
00:15:16,690 --> 00:15:20,650
So there might be variation from one to another.

113
00:15:20,980 --> 00:15:26,440
So all of those instances are examples of where the measurement error could fall far.

114
00:15:27,340 --> 00:15:32,440
So in that scenario, you cannot assume that X is fixed.

115
00:15:33,370 --> 00:15:43,240
But generally and this, like I said, a whole body of literature on measurement did a lot of that.

116
00:15:44,180 --> 00:15:47,899
That could be beyond the scope of 650 and 650.

117
00:15:47,900 --> 00:15:52,820
People assume that this is fixed. Okay.

118
00:15:53,780 --> 00:15:57,120
So that's the coordination now.

119
00:15:59,330 --> 00:16:08,000
Now we will quickly introduce four distributions that we are going to heavily use in this class.

120
00:16:08,420 --> 00:16:16,190
The first one is our very well-known normal distribution.

121
00:16:16,220 --> 00:16:21,710
This is the most frequently used distribution in statistics.

122
00:16:22,580 --> 00:16:35,510
And it is a continuous distribution. The density function for the normal distribution has two unknown parameters new and the mean new.

123
00:16:35,510 --> 00:16:39,950
And the Sigma Square and the probability density function.

124
00:16:39,950 --> 00:16:45,139
If Y is given by one over square root of two,

125
00:16:45,140 --> 00:17:04,140
by the time sigma where sigma is the standard deviation of the distribution and times exponential of negative half y minus new over sigma squared.

126
00:17:04,250 --> 00:17:11,510
So y minus nu over sigma is basically the standardization of the random variable.

127
00:17:13,700 --> 00:17:23,599
So like I said, the normal distribution is characterized by these two parameters Mulan Sigma squared, the mean and the variance respectively.

128
00:17:23,600 --> 00:17:30,020
And if we know the mean and the variance, then this is like a better shape curves.

129
00:17:32,880 --> 00:17:36,150
And Sonia.

130
00:17:39,850 --> 00:17:51,610
And if renewable Greenland seemed more square, then a third of Y's distribution lies within plus minus one of a.

131
00:17:53,410 --> 00:18:07,060
95% of the values will lie between new plus minus two sigma, and 99% of the values will lie within new plus -60.

132
00:18:07,300 --> 00:18:20,330
So these are basically. Well-known properties of the of the normal distribution.

133
00:18:20,330 --> 00:18:34,870
It's a battleship garb. And as I mentioned, it's some. So the blue lines are the 95th of the 80.

134
00:18:34,870 --> 00:18:41,410
On the blue lines gap to 95% of the distribution.

135
00:18:42,100 --> 00:18:52,510
The area under the dotted or dashed red lines captured 66% of the normal distribution.

136
00:18:52,840 --> 00:18:59,080
And then plus my new plus minus three sigma scatters greater than 99%.

137
00:18:59,530 --> 00:19:03,759
So as I mentioned, arguably, this is the most important distribution in statistics.

138
00:19:03,760 --> 00:19:07,810
And you will see that this distribution comes up like, you know,

139
00:19:07,840 --> 00:19:21,010
sort of so many times when we are talking about inference in this class and doing important results.

140
00:19:21,400 --> 00:19:24,260
Linear combinations of normals are also normal.

141
00:19:24,400 --> 00:19:35,290
So if Y is a normal random variable and if little and little B are known constants, then e, y plus B also has a normal distribution.

142
00:19:36,070 --> 00:19:40,330
The mean of that distribution is going to be E dimes.

143
00:19:40,330 --> 00:19:48,700
Milk plus beaver meal is the mean of y and the variance of e y plus b would be a squared.

144
00:19:48,700 --> 00:19:53,499
Times sigma squared or sigma squared is the variance of y once again.

145
00:19:53,500 --> 00:20:03,190
How do I get these results? By applying the mean and the variance results that we reviewed last week.

146
00:20:03,580 --> 00:20:13,060
Okay. So just simple applications of those, the random variable Z,

147
00:20:15,430 --> 00:20:27,459
which is standardizing Y based on taking the deviation of Y from its meaning and dividing it by the standard deviation.

148
00:20:27,460 --> 00:20:30,820
So in other words, y minus mu over sigma.

149
00:20:32,320 --> 00:20:38,680
This has this is called a gold standard normal.

150
00:20:39,190 --> 00:20:47,080
And this has distribution, normal distribution with mean zero and variance one.

151
00:20:47,830 --> 00:20:53,170
This once again, is a very, very important distribution.

152
00:20:53,200 --> 00:20:55,749
It's basically like the normal distribution.

153
00:20:55,750 --> 00:21:07,090
It's a it's a standard and scaled version of of the normal random variable Y, and this is called the standard normal.

154
00:21:07,430 --> 00:21:20,110
And if you see that the sort of the standard normal distribution all through this force, how do I get from Y to Z being normal zero one?

155
00:21:21,400 --> 00:21:27,140
By applying which result that the linear combination of normals are normal.

156
00:21:27,220 --> 00:21:30,940
So like if you did I do by minus mu over sigma.

157
00:21:31,300 --> 00:21:36,280
If you take the expectation of minus me over sigma, then what happens.

158
00:21:36,280 --> 00:21:45,040
It's one of the sigma times expectation of y minus mu because new is constant and we know expectation of y is new.

159
00:21:45,160 --> 00:21:48,790
So you have new minus new or sigma zero.

160
00:21:49,120 --> 00:21:59,560
And similarly you can give the variance of variance of y minus new over sigma one of our sigma squared times the radius of why minus two.

161
00:22:00,430 --> 00:22:06,640
And we know from the variance results that the variance doesn't change by adding or subtracting a constant.

162
00:22:07,060 --> 00:22:13,540
So basically what you have is one more significant than speaking supply and variance

163
00:22:13,540 --> 00:22:19,450
of y is equal to sigma squared or sigma squared divided by signals credibility one.

164
00:22:20,590 --> 00:22:28,880
Okay. So I, I see these things all flow because these are results that we should all be familiar with.

165
00:22:30,850 --> 00:22:34,840
I didn't write them. Okay. You got to be right.

166
00:22:35,030 --> 00:22:38,620
Okay, so that's the standard.

167
00:22:38,890 --> 00:22:44,799
That's the normal distribution. The next distribution that we are going to talk about is the Chi Square distribution.

168
00:22:44,800 --> 00:22:54,630
And once again, this is a very important distribution for our for 654 when we develop the inference for a model.

169
00:22:54,640 --> 00:23:07,510
So the notation, a little bit notation here denoted by X, this guy square distribution is written as Chi Square,

170
00:23:07,840 --> 00:23:17,500
this guy symbol squared and then the subscript B, F, B, F refers to the degrees of freedom.

171
00:23:19,570 --> 00:23:26,110
And the characterization of this distribution is.

172
00:23:27,960 --> 00:23:43,920
Square with degrees of freedom equal to the F the mean of their distribution is equal to b.f will get and x.

173
00:23:45,060 --> 00:23:59,760
This random variable corresponding to the chi square thinks only positive values if it is a positively distributed random variable.

174
00:24:01,320 --> 00:24:11,790
One important result is that if the IE is a standard normal.

175
00:24:13,200 --> 00:24:19,710
So in other words, the AI has normal zero one, then spread of the AI.

176
00:24:19,740 --> 00:24:27,840
So spreading a standard normal gives you a chi square distribution with one degree of freedom.

177
00:24:30,610 --> 00:24:38,079
If Z1, Z2 is not independent and it is distributed,

178
00:24:38,080 --> 00:24:50,500
adds a standard normal to normal Zulu one then some of the the squared so on squaring each each is giving me a one degree of freedom chi

179
00:24:50,500 --> 00:25:02,260
square and then I'm adding an independent guy squared one distributions that gives me a i squared with degrees of freedom equal to n.

180
00:25:05,470 --> 00:25:10,480
Yes. So is it safe to say that the square deviation of.

181
00:25:14,110 --> 00:25:17,350
Squared. What we mean by squared deviation.

182
00:25:17,620 --> 00:25:23,920
So you see it because the standard. I kind of represents how a deviation.

183
00:25:25,300 --> 00:25:30,570
Yeah yeah you can these you can think of this could be a -0 spread if you are thinking that way.

184
00:25:30,570 --> 00:25:37,740
But, but the basic thing is that you have already taken the deviation from the mean like you have come from.

185
00:25:37,740 --> 00:25:42,390
Why does the data have already sort of center and scale?

186
00:25:43,320 --> 00:25:49,080
So this is a normal random variable between zero variance one.

187
00:25:49,080 --> 00:25:54,470
So you have already done the same thing and scaling squared of that gives you a nice day.

188
00:25:55,530 --> 00:25:58,979
Okay. I just realized one thing that I forgot to mention.

189
00:25:58,980 --> 00:26:09,360
The mean is equal to the F and the variance of x squared x is chi square with one

190
00:26:09,360 --> 00:26:16,860
degree of freedom is equal to two times the f the two times the degree of freedom,

191
00:26:19,650 --> 00:26:22,229
and it is a positive random variable.

192
00:26:22,230 --> 00:26:31,290
So basically it will always be skewed to the right and depending on the degrees of freedom, you know, the sheep varies.

193
00:26:31,290 --> 00:26:39,860
But if this is zero then the guy's square distribution loop some what I did not really draw maybe that.

194
00:26:42,900 --> 00:26:53,580
So it looks. There is zero.

195
00:26:57,810 --> 00:27:03,510
That's its gift to the right, only to exploitative values.

196
00:27:04,440 --> 00:27:12,380
So this is a distribution that is used extensively in hypothesis testing and confidence intervals involving people.

197
00:27:12,470 --> 00:27:22,860
So we will sort of use this distribution again throughout 650 when we're talking about hypothesis testing for the production.

198
00:27:25,500 --> 00:27:31,680
The key distribution. Again, a very, very important distribution in statistics.

199
00:27:32,100 --> 00:27:44,580
Again, you will see in our regression modeling we will use the P distribution to make inference about the individual regression parameters.

200
00:27:45,360 --> 00:27:49,080
Excuse me. So what is the distribution?

201
00:27:50,220 --> 00:27:59,400
If I have a the standard normal and if I have another random variable x squared,

202
00:27:59,910 --> 00:28:08,550
which is chi square with degrees of freedom equal to the F and give z and square are independent.

203
00:28:08,560 --> 00:28:13,470
So this is a this is a very, very important assumption.

204
00:28:14,130 --> 00:28:19,830
Okay. So I have a standard normal and I have a chi square and they are independent.

205
00:28:20,340 --> 00:28:34,530
Then the ratio of the standard normal to the square root of that square is, you know, standardize by its degree of freedom.

206
00:28:35,010 --> 00:28:40,079
He has a D distribution with degrees of freedom equal to def.

207
00:28:40,080 --> 00:28:48,450
So I have on the numerator the standard normal z and on the denominator I have s divided by square root

208
00:28:48,450 --> 00:29:00,389
of the degrees of freedom for the chi square and the ratio of those to the normal random variable.

209
00:29:00,390 --> 00:29:06,930
And thus, you know, the square root of the chi square standardized by degrees of freedom.

210
00:29:07,470 --> 00:29:11,830
The ratio is a distribution with degrees of freedom equal to the.

211
00:29:13,230 --> 00:29:18,600
This is a symmetric bell shaped graph like the normal.

212
00:29:18,610 --> 00:29:26,030
But the difference between a normal and empty distribution is that the distribution has mott factor pins.

213
00:29:26,580 --> 00:29:33,660
So it looks like. So again again it values from negative infinity to positive infinity.

214
00:29:34,590 --> 00:29:39,120
But it is it has Mott factor is the.

215
00:29:42,490 --> 00:29:46,870
Okay. So. So that's the B distribution.

216
00:29:47,230 --> 00:29:55,900
The mean of the P distribution is zero and the variance of so this is zero.

217
00:29:57,400 --> 00:30:09,010
The variance of for P with degrees of freedom equal to CBF is equal to 240 degrees of freedom.

218
00:30:12,240 --> 00:30:22,710
Divided by the F minus two and four V greater than two, and it's undefined otherwise.

219
00:30:23,850 --> 00:30:29,490
So the variance is always greater than one meaning in contrast to the normal zero one.

220
00:30:29,700 --> 00:30:32,900
This will have much heavier, much properties.

221
00:30:33,480 --> 00:30:43,740
And another very interesting fact and a key fact that people use in making inference is that as the degrees of freedom goes to infinity,

222
00:30:44,640 --> 00:30:54,820
the limit of the distribution as the use of freedom goes to infinity approaches a normal zero one.

223
00:30:54,840 --> 00:31:03,210
So, you know, as the degrees of freedom increases, the distribution starts looking more and more like the normal.

224
00:31:05,220 --> 00:31:12,240
And in all practical purposes, for all practical purposes, if the degrees of freedom is greater than 30,

225
00:31:12,240 --> 00:31:17,850
then the distribution very closely resembles the normal 3 to 1 distribution.

226
00:31:18,870 --> 00:31:28,530
The reason I say that this is a very interesting result, and the key to that is you'd see that in inference the people make use of this a lot.

227
00:31:28,890 --> 00:31:39,870
And oftentimes you might hear, I don't know, probably you already heard people saying, oh, the key statistic is, is the key statistic bigger than two?

228
00:31:42,240 --> 00:31:46,799
And if it's bigger than two, then basically we reject the null hypothesis.

229
00:31:46,800 --> 00:31:55,920
So that why is that two greater than two equal to two important because if as I said, if the degrees of freedom is,

230
00:31:55,920 --> 00:32:04,530
you know, as it becomes larger and for all practical purposes, greater than 30, then the body starts looking like a normal.

231
00:32:04,540 --> 00:32:20,670
What is that do? What does it do? Correspond to 1.96 are the standard like for a standard normal and they are the 90th percentile cutoff.

232
00:32:21,570 --> 00:32:30,960
So 1.92 one. We got to do any reaction that well that these kind of like the normal so is a greater than plus minus two if it is then.

233
00:32:31,870 --> 00:32:43,450
So that's sort of the sort of backwards approximation that you make and because of the value of the set up.

234
00:32:44,110 --> 00:32:46,749
Okay. So you'll see these, you know, as,

235
00:32:46,750 --> 00:32:55,840
as we kind of walk through the course and develop the friends and you will see that you start becoming more and more conversant with,

236
00:32:55,990 --> 00:33:06,890
you know, how to use these results and making. And as I mentioned, that the distribution is used for inference on individual regression parameters.

237
00:33:08,060 --> 00:33:12,230
And then the last one that we are going to talk about is the F distribution.

238
00:33:13,550 --> 00:33:22,940
Once again, this is extensively used in linear regression, mostly for hypothesis testing,

239
00:33:23,300 --> 00:33:30,030
not for single regression parameters, but for testing simultaneously.

240
00:33:32,390 --> 00:33:40,610
The significance of regression parameters, of the significance of our model and what is their distribution.

241
00:33:40,610 --> 00:33:44,659
So if X1 Square has a nice square with degrees of freedom,

242
00:33:44,660 --> 00:33:53,150
von and x2 squared is chi squared with degrees of freedom B of two and x1 square and x2 square.

243
00:33:53,160 --> 00:33:55,770
So these two random variables are independent.

244
00:33:56,870 --> 00:34:07,730
Then the ratio of the two guys squared standardized by their respective degrees of freedom has an F distribution with degrees of freedom.

245
00:34:08,820 --> 00:34:15,230
Now there are two barometers we have. We call them the new monitor Degrees of Freedom and the denominator Degrees of Freedom.

246
00:34:15,620 --> 00:34:25,430
So the two parameters of F are the F1 and B F2, which come from the chi square in the numerator and the chi square in the denominator respectively.

247
00:34:25,820 --> 00:34:29,720
So the F distribution is again characterized by two parameters.

248
00:34:30,560 --> 00:34:36,890
It is the right skewed distribution like the Chi square because it can only be positive values.

249
00:34:37,070 --> 00:34:40,250
On the top you have a positive value than a variable.

250
00:34:40,250 --> 00:34:46,489
On the bottom, you have a positive value than a variable. So it always has to take positive values.

251
00:34:46,490 --> 00:34:48,290
It's a right skewed distribution.

252
00:34:49,400 --> 00:35:00,650
And there's an interesting connection between the F distribution and the P distribution return, so that if you take a T distribution,

253
00:35:01,700 --> 00:35:11,480
the degrees of freedom and squared it, then in distribution it is equal to an F with degrees of freedom.

254
00:35:12,260 --> 00:35:19,870
One former b f. So the numerator degrees of freedom is one and the denominator degrees of freedom is dear.

255
00:35:20,240 --> 00:35:23,390
Yes. What is the.

256
00:35:26,580 --> 00:35:33,060
What is what is. Oh yeah. The be about equal time is what I see in distribution are equal.

257
00:35:33,910 --> 00:35:38,190
Okay. Okay. So even distribution there are.

258
00:35:41,060 --> 00:35:46,070
They are equal in the context of the integration.

259
00:35:46,850 --> 00:35:53,060
People will actually for even more women for that their because also have the same expression.

260
00:35:53,600 --> 00:35:58,180
But here the gender that's often said he needs to be sent this way.

261
00:36:02,200 --> 00:36:16,960
So once again, this is a distribution that is used extensively in hypothesis testing for linear regression.

262
00:36:17,800 --> 00:36:22,030
Okay. Any questions?

263
00:36:31,700 --> 00:36:35,860
No. Yes.

264
00:36:36,280 --> 00:36:46,220
But I still think you need to come to me on this if it's less than to the experience.

265
00:36:47,020 --> 00:36:54,990
Yeah, yeah. I mean that. Yes. Okay.

266
00:36:58,800 --> 00:37:02,340
Yeah. And in distribution equal maybe like one.

267
00:37:02,340 --> 00:37:06,680
One other way that you can help understand.

268
00:37:06,690 --> 00:37:19,020
So what is a B? Remember A B is z over square root of a guy square with degrees of freedom beer divided by beer.

269
00:37:19,050 --> 00:37:24,210
Right. That is a that was a t that is how we defined it.

270
00:37:24,810 --> 00:37:27,930
And Z and guy squared independent.

271
00:37:28,470 --> 00:37:41,550
So if I take the square of this, then I have a Z square and on the bottom I have a guy square with divided by the F.

272
00:37:43,380 --> 00:37:49,500
So what is the square offer of a normal random variable?

273
00:37:52,000 --> 00:37:55,850
It's a chi square. Right.

274
00:37:56,480 --> 00:38:01,160
So I can drive this this square divided by one.

275
00:38:02,000 --> 00:38:10,280
And in the denominator I can write the sky squared divided by the F so this would e that chi

276
00:38:10,280 --> 00:38:16,940
square the degree of freedom one and x or chi square x squared the ties to the degree of freedom.

277
00:38:17,420 --> 00:38:21,530
So that issue of those two is an F with degrees of freedom.

278
00:38:21,530 --> 00:38:28,700
One might be. So that's another way of seeing Y in distribution.

279
00:38:28,700 --> 00:38:33,230
They are equal. Okay, so I'm going to.

280
00:38:33,920 --> 00:38:40,670
So that's the end of module here. I'm going to stop here and see if there are any questions.

281
00:38:44,740 --> 00:38:50,100
No. Okay. So it's 845. Let's take the break now.

282
00:38:50,110 --> 00:38:57,570
Instead of, you know, starting the new module right now and let's come back at 855 at least,

283
00:38:57,820 --> 00:39:04,350
and we'll start talking about a simple linear regression model.

284
00:39:13,769 --> 00:39:22,259
Okay. So. We are going to stop talking about.

285
00:39:25,729 --> 00:39:36,949
Simple linear regression. This is module be in your canvas course website.

286
00:39:37,849 --> 00:39:42,829
So if you go and look under module table now everybody can see the slides.

287
00:39:43,759 --> 00:39:59,849
If you go there then. This module be a set of slides as well as some additional materials are uploaded under module B on canvas.

288
00:40:01,019 --> 00:40:05,669
Okay. So. With that.

289
00:40:06,479 --> 00:40:16,619
Certainly what I'm going to talk about are what is also here is an outline.

290
00:40:17,789 --> 00:40:21,799
So these are the topics that we will learn in module B.

291
00:40:22,589 --> 00:40:26,599
So we'll introduce the simple linear regression model.

292
00:40:27,689 --> 00:40:31,619
We will talk about the interpretation of the parameters in that module.

293
00:40:32,849 --> 00:40:45,089
Then we will first talk about how the estimate is unknown parameters and introduce the method of least squares.

294
00:40:45,449 --> 00:40:56,549
So this is again a fundamental method in simple linear regression, also referred to as ordinary squares.

295
00:40:57,329 --> 00:41:04,649
So we will introduce that and we will derive the square system index in a simple enough regression model.

296
00:41:05,129 --> 00:41:13,469
Then we are going to talk about the properties of these, these squares estimates and.

297
00:41:15,349 --> 00:41:19,368
Once we have established the properties of the experts estimate those,

298
00:41:19,369 --> 00:41:31,549
we will then also introduce an estimate of the variance and sort of wrap up module

299
00:41:31,549 --> 00:41:41,358
be we that topic be the inference for the scene building regression model.

300
00:41:41,359 --> 00:41:49,129
So how do I do the hypothesis? This for many different pattern that we do like a module and see the next one.

301
00:41:50,419 --> 00:41:55,189
The relevant readings are from your textbook chapters one and two.

302
00:41:56,839 --> 00:42:02,749
Okay, so the simple linear regression model, SLR.

303
00:42:04,249 --> 00:42:14,628
So here is the model Y equal to beta, not plus beta one, it's CI plus epsilon,

304
00:42:14,629 --> 00:42:27,469
i.e. as I mentioned that first, let's, let's kind of define one more time what these quantities are.

305
00:42:27,469 --> 00:42:33,319
So why is the response of the dependent variable for the I subject in the study?

306
00:42:33,979 --> 00:42:40,819
Beta non is the intercept of this equation.

307
00:42:41,629 --> 00:42:45,919
This is unknown. I don't know it, so I have to estimate this.

308
00:42:46,879 --> 00:42:51,049
It's fixed, but I'm not seeing the beta one way.

309
00:42:51,149 --> 00:42:55,779
One is the slope of this line. It is fixed but unknown.

310
00:42:56,179 --> 00:43:06,709
What is excite? Excite is the whole period or the predictor variable for the you subject in the study.

311
00:43:08,449 --> 00:43:20,839
Epsilon R is the random error or the noise variable and this is unobservable b we don't observe it.

312
00:43:21,349 --> 00:43:25,728
So what are the observables? The observables are these pairs excited?

313
00:43:25,729 --> 00:43:37,999
Why I from one to little and but in the substitute I denotes the subject IP and I goes from one to land but and is the sample size.

314
00:43:38,119 --> 00:43:53,419
Okay, so that's the that's the model. So a few more kind of bells and whistles, as I mentioned, that people mostly assume that this X is fixed.

315
00:43:56,359 --> 00:44:06,409
I mean, I should say that people assume in this course that X is fixed or in other words, X is not random.

316
00:44:06,889 --> 00:44:11,989
And we just talked about, you know, there may be situations where X, you know,

317
00:44:11,989 --> 00:44:21,769
has measurement error because of differences in axes across labs, because of measurement error from a medical device.

318
00:44:23,119 --> 00:44:28,129
So there is a whole body of literature that talks about measurement error models.

319
00:44:28,369 --> 00:44:32,989
But in this course, we are going to assume that X is fixed.

320
00:44:33,799 --> 00:44:45,319
Okay. One other one other one that I want to mention is that all the inferences that we are going to make are one B,

321
00:44:45,319 --> 00:44:56,659
Shannon on on x one which no meaning that I know the value of x given that x is equal to a little x like a number.

322
00:44:57,559 --> 00:45:06,469
I've been making friends about y or I been, you know, estimate better not going to be the one.

323
00:45:07,549 --> 00:45:12,859
One other thing that I'm going to mention is that so why is the response?

324
00:45:17,169 --> 00:45:27,249
Together. Paternot Plus beta. S Beta on its site is referred to as the systematic component of the modern.

325
00:45:40,379 --> 00:45:46,619
And Epsilon. I referred to the random component of the model.

326
00:45:51,909 --> 00:45:58,869
So in plain English. I The random error of the noise,

327
00:45:59,139 --> 00:46:09,609
the unobservable random variable is what gives the model its stochastic nature and beta

328
00:46:09,789 --> 00:46:14,259
plus beta one say although I don't know better not and beat them on their unknown,

329
00:46:14,259 --> 00:46:18,699
but they are some fixed numbers and same with x.

330
00:46:19,059 --> 00:46:29,609
Exactly. So beta. Not because we don't want it say there's nothing random in it, but so this is called the systematic component of the model sometime.

331
00:46:30,399 --> 00:46:36,639
Okay. Now linear models, what does linearity different?

332
00:46:37,029 --> 00:46:43,779
Linearity, we said refers to the fact that the mean can be defined as a weighted sum of the parameters.

333
00:46:44,769 --> 00:46:55,718
So the expected value of y given side is is equal to some small fixed number.

334
00:46:55,719 --> 00:47:06,489
Little XY is given by summation w Gabe Tuckey and we will talk about what the W keys are.

335
00:47:06,519 --> 00:47:12,128
But it's a it's kind of a weighted average of the beta noting in here,

336
00:47:12,129 --> 00:47:20,229
I don't know the values of beta by the estimate, but I can always write the mean of Y,

337
00:47:20,229 --> 00:47:33,819
given a fixed value of x as the really big advantage of the beta examples of linear models we've covered in more detail later.

338
00:47:33,849 --> 00:47:43,388
But these are all examples of linear model. So where the expected value of Y given in a fixed value of x is beta not growth.

339
00:47:43,389 --> 00:47:54,219
We don't want to say square beta or beta not closed beta one exponential excite or beta not plus beta one times global fun classic.

340
00:47:54,249 --> 00:47:56,719
So these are all examples of linear models.

341
00:47:56,739 --> 00:48:08,289
Y Because the parameters enter the moment in a linear fashion and all we have done is basically on the right hand side.

342
00:48:08,439 --> 00:48:19,749
Instead of having X, we have taken some transformation of the X to achieve better fit to do the data.

343
00:48:20,529 --> 00:48:26,619
Okay. But they are inherently linear models because the parameters enter the model in a linear fashion,

344
00:48:28,269 --> 00:48:32,319
examples of non-linear models not covered in this class.

345
00:48:33,549 --> 00:48:36,819
So the first one is what they expected.

346
00:48:36,819 --> 00:48:44,619
Value of y given a fixed x is exponential beta naught plus beta one ixi.

347
00:48:45,159 --> 00:48:49,028
The second model is where the expectation applied,

348
00:48:49,029 --> 00:48:57,369
given a fixed x is one or one plus one over one plus exponential of negative beta, not be the one side.

349
00:48:57,639 --> 00:49:09,129
And then the last one is the mean of y given a fixed x is given by beta not plus exponential of beta one times its size.

350
00:49:09,699 --> 00:49:20,829
And these are all examples of nonlinear models because the barometers enter the model in a nonlinear fashion.

351
00:49:22,119 --> 00:49:26,709
So they are nonlinear in the pattern with this. Okay.

352
00:49:27,309 --> 00:49:32,289
Questions here, I'm expecting one.

353
00:49:35,359 --> 00:49:38,669
So every time I like I.

354
00:49:38,999 --> 00:49:42,948
I bother and I see there's not a lot I can do.

355
00:49:42,949 --> 00:49:47,599
But he said, I mean, I'm I'm expecting a question here.

356
00:49:55,829 --> 00:50:04,319
So the three nonlinear models that I wrote, if you stared at those three for a for a while.

357
00:50:07,699 --> 00:50:13,219
Do you see a difference between the first one and the versus the second and third?

358
00:50:29,819 --> 00:50:38,128
Yeah. So did everybody hear that? So in the first one, if I did the law transformation, I can make it a linear model, right?

359
00:50:38,129 --> 00:50:46,259
So if I take log off in this model, if I log on both sides.

360
00:50:54,239 --> 00:50:57,509
I get it done on last beat, Darren.

361
00:50:58,559 --> 00:51:06,839
It's fun. So it is transformable to a linear model using the transformation on the Y.

362
00:51:08,279 --> 00:51:22,599
Okay. So this is a this is a transformable 3D model where y is not related to X in a linear fashion, but normal is related to it.

363
00:51:24,549 --> 00:51:31,989
Okay. So in 650, although I said this model is not the body in the stuff in 650,

364
00:51:32,259 --> 00:51:39,309
I can talk about the first model if I make up our formation and I'm kind of like,

365
00:51:39,909 --> 00:51:47,879
you know, do a regression of long y your body, but the second and third model are nonlinear.

366
00:51:47,889 --> 00:51:55,299
I cannot bring them to linear or the transformable model.

367
00:51:58,909 --> 00:52:05,939
Same for the United. Not this one is log.

368
00:52:07,169 --> 00:52:16,198
Yeah. Sorry, I might. I am.

369
00:52:16,199 --> 00:52:20,789
I'm a much better writer on the ball than on the screen.

370
00:52:21,539 --> 00:52:28,679
So this is long and and. Okay.

371
00:52:29,129 --> 00:52:33,949
Any other questions? Yes.

372
00:52:34,909 --> 00:52:39,499
I just want to ask, is the second one exactly. It's the second one.

373
00:52:39,739 --> 00:52:44,509
Is that the second one example?

374
00:52:46,249 --> 00:52:49,908
Not exactly. Not exactly.

375
00:52:49,909 --> 00:52:54,459
But the expression sort of looks like, you know, what you seen.

376
00:52:54,929 --> 00:52:59,359
But but but but the left hand side is different.

377
00:52:59,859 --> 00:53:07,269
It is. You can think of it like a like the expression of a logistic regression model.

378
00:53:07,279 --> 00:53:11,539
But the Y is has a different distribution.

379
00:53:11,659 --> 00:53:18,829
So it's not like normal. So did that answer your question?

380
00:53:18,839 --> 00:53:24,969
Yeah, it looks the expression looks like a logistic regression, but the Y has a different distribution.

381
00:53:28,269 --> 00:53:28,689
Okay.

382
00:53:31,449 --> 00:53:47,289
So now we will talk about assumptions for estimating these unknown parameters between beaten up and based on the sort of assumptions that we make.

383
00:53:49,449 --> 00:53:56,289
We are going to talk about the most fundamental assumptions about the errors first.

384
00:53:56,289 --> 00:54:02,079
And one thing that I want to mention here is first, we will develop the estimation, right?

385
00:54:02,409 --> 00:54:12,879
So you will see that I need less assumptions or less, I should say, extreme advantage for estimation then for hypothesis testing.

386
00:54:13,539 --> 00:54:17,559
So what do I need about the urn, about the errors?

387
00:54:18,699 --> 00:54:28,869
The first two are that just about the first and second moment, or in other words, the mean and the variance of the random error.

388
00:54:29,239 --> 00:54:40,538
Illinois. So I assume that the mean of Epsilon II is equal to zero and the variance of Epsilon II is equal to

389
00:54:40,539 --> 00:54:51,159
Sigma Square for all I for all I meaning like remember epsilon I Epsilon line excellent to epsilon,

390
00:54:51,159 --> 00:54:59,349
three to epsilon. And these are like random errors corresponding to the first, second, third and subject.

391
00:54:59,829 --> 00:55:13,929
And the assumption that I'm making is that these excellence come from underlying distribution that has a mean zero and B is Sigma Square.

392
00:55:14,739 --> 00:55:19,809
Okay. So that's the fundamental assumption I make.

393
00:55:20,169 --> 00:55:24,369
I also assume that these are all reported.

394
00:55:25,719 --> 00:55:30,239
So this is a less stringent assumption that independence.

395
00:55:31,359 --> 00:55:38,859
So what for estimation? What I need is that Epsilon I and Epsilon G are uncoordinated.

396
00:55:38,859 --> 00:55:49,029
Or in other words, what really is between Epsilon and Epsilon G is equal to expectation of the product of Epsilon I, Ypsilanti.

397
00:55:49,719 --> 00:55:56,319
This should be a minus product of the expectations, but the expectations of an eye epsilon zero zero.

398
00:55:56,649 --> 00:56:05,138
So that drops off. So this for radians is equal to zero for all i d i not equal to do so.

399
00:56:05,139 --> 00:56:08,259
These are the two assumptions that I need.

400
00:56:10,479 --> 00:56:15,909
As I said, I'm uncorrelated does not mean independence.

401
00:56:15,909 --> 00:56:20,799
Independence is a much more stringent assumption at this point for estimation.

402
00:56:20,829 --> 00:56:24,009
We only need the first and second moment,

403
00:56:24,279 --> 00:56:32,529
the mean and the obedience of epsilon eyes and the assumption about Epsilon I

404
00:56:32,529 --> 00:56:38,919
and Epsilon way to be uncoordinated later when we consider hypothesis testing.

405
00:56:39,939 --> 00:56:47,889
So when we are doing inference, then we will require further assumptions about the errors.

406
00:56:47,919 --> 00:56:53,498
And specifically the assumptions that we will request are the independence

407
00:56:53,499 --> 00:56:58,389
assumption of the errors and a normal distribution assumption for the error.

408
00:56:58,749 --> 00:57:07,779
So this comes so this is needed for inference and I like hypothesis testing.

409
00:57:10,769 --> 00:57:17,999
In general inference, the independence anomaly gives option, but for the estimation of be cannot be the one.

410
00:57:18,539 --> 00:57:27,769
All be required are that the expectation of Epsilon II is zero and the variance of that similar i.

411
00:57:29,969 --> 00:57:36,449
Is seamless and the depth of an image Indiana quality.

412
00:57:37,289 --> 00:57:43,409
Okay so these imply assumptions about the distribution of wire yet each x.

413
00:57:44,629 --> 00:58:00,329
So remember the model is Y equal to B, Bernard plus beta one X plus epsilon I and the random part of the model is the Epsilon nine.

414
00:58:00,829 --> 00:58:15,408
So if I join those facts about Epsilon I, then what happens to the expected value of Y given a fixed value of x that is equal to beta,

415
00:58:15,409 --> 00:58:20,369
not plus beta on one side because in expectation epsilon is zero, right?

416
00:58:20,389 --> 00:58:25,278
So if I did the expectation on Woodside's expectation of epsilon, I is zero.

417
00:58:25,279 --> 00:58:32,329
So I get the expected value of Y given a fixed value of X to be equal to beaten up by one x.

418
00:58:32,469 --> 00:58:36,639
So it depends only on its side. And of course I need to estimate return.

419
00:58:37,069 --> 00:58:42,409
What about the variance? So epsilon y is the only random quantity.

420
00:58:42,409 --> 00:58:55,369
Correct. And not. And we know that variance doesn't change by adding fixed constants to a random variable.

421
00:58:55,609 --> 00:59:02,299
So the variance of y given a fixed value of x is going to be equal to sigma square.

422
00:59:03,859 --> 00:59:09,799
Right? Equal to the variance of epsilon. We just mean we saw the variance formula.

423
00:59:10,009 --> 00:59:13,009
Right? So that's a variance.

424
00:59:13,279 --> 00:59:19,699
And what happens to y n y the poor variance of that support medians apply.

425
00:59:19,699 --> 00:59:27,049
And why did a will come from the only random part in the model which is Epsilon I Ypsilanti.

426
00:59:27,409 --> 00:59:37,999
So covariance of y-o-y g is equal to the poor variance of epsilon i epsilon d once again applying for variance results.

427
00:59:39,289 --> 00:59:44,229
And that is equal to zero because epsilon I am assuming zero.

428
00:59:47,409 --> 01:00:01,869
Okay. So that's what. So these assumptions about the errors lead to the following about the distribution of Y at each X.

429
01:00:02,049 --> 01:00:08,549
Yes. So in the context of estimation. The random errors can be any distribution.

430
01:00:09,789 --> 01:00:12,949
Zero. Which is confusing. Yes. Yes.

431
01:00:13,259 --> 01:00:21,449
So everybody heard that. So when I said that I don't need the independence or the normality assumption of the areas for estimation.

432
01:00:21,779 --> 01:00:31,469
Precisely what I meant is that for estimation I became count of the errors that come from any distribution,

433
01:00:31,469 --> 01:00:36,599
as long as the mean is zero and the big into Sigma Square.

434
01:00:37,019 --> 01:00:42,119
And if so, then I have to do it on 40 because I don't need the normal data.

435
01:00:43,199 --> 01:00:48,719
I will need that when developing inference, when we hypothesis tests.

436
01:00:49,139 --> 01:00:52,289
And just think of a sneak preview of why you need that.

437
01:00:52,589 --> 01:01:05,239
Remember, what is the distribution? So if I talked about like, you know, baby diamond on on the island off of the air.

438
01:01:05,249 --> 01:01:07,708
So that's why we put in four inside.

439
01:01:07,709 --> 01:01:16,269
We need the normal media, not time but for estimation or in other words, to, you know, estimate the unknown kind of bigger than we don't want.

440
01:01:16,289 --> 01:01:20,519
I don't need that. I'm from of the distribution.

441
01:01:20,669 --> 01:01:24,159
Yes. Just to hold that, the variance has to be defined.

442
01:01:24,929 --> 01:01:30,719
The variance has to be defined. The variance has to be finite and constant.

443
01:01:31,919 --> 01:01:45,449
So another important point for the variance of epsilon, I have to be finite and constant sigma squared for all time.

444
01:01:47,039 --> 01:01:53,759
Again, this is a good question because we will see later on.

445
01:01:53,759 --> 01:01:59,479
Remember, we talked when, you know, talking about like what are things that are coming up in 650,

446
01:01:59,489 --> 01:02:10,319
we said that we are going to assess the adequacy of the model or the adequacy of the assumptions in a linear regression model to residual diagnostics.

447
01:02:10,589 --> 01:02:16,618
And one of their concerns we make is that the variance is constant in belief,

448
01:02:16,619 --> 01:02:21,629
because there may be situations where the variance depends on the value of X,

449
01:02:22,079 --> 01:02:30,539
and we will learn how to assess the adequacy of that assumption to receive the diagnostics because,

450
01:02:32,159 --> 01:02:38,129
you know, as you develop the model, but here's the variance has to be finite and constant.

451
01:02:38,459 --> 01:02:53,069
So let me show you the sort of a this is a plot from your textbook and this might help to put things in context.

452
01:02:54,299 --> 01:03:00,929
So here it is sort of a pictorial of how observations.

453
01:03:00,959 --> 01:03:06,559
This is. Speaker 1.2 from your textbook, how observations are generated.

454
01:03:10,119 --> 01:03:13,509
I should say implicitly generative linear regression.

455
01:03:13,779 --> 01:03:20,169
So let's for the see at this point that you know better now than beta one.

456
01:03:21,129 --> 01:03:30,429
So let's say you. So let's see.

457
01:03:52,569 --> 01:04:00,619
So let's see. You know, this may not be the one. There are some some kind of numbers.

458
01:04:01,129 --> 01:04:04,969
Okay. So this is what is happening.

459
01:04:04,969 --> 01:04:16,189
So we on the Y on the X axis, I have a X and thinking values, let's say hypothetical values, ten, 20, 30.

460
01:04:16,609 --> 01:04:20,749
And on the Y axis, I have plotted the observed values.

461
01:04:21,229 --> 01:04:25,549
The ball circles are the observations of Y.

462
01:04:25,559 --> 01:04:29,448
So here is one of the values. Here is one of the value.

463
01:04:29,449 --> 01:04:35,059
And so on. This observe value of y corresponds to x equal to ten.

464
01:04:37,009 --> 01:04:41,959
This observe value of y corresponds to x equals roughly 25.

465
01:04:41,969 --> 01:04:46,239
So this is of the value of Y at its sequel to ten.

466
01:04:47,689 --> 01:04:51,729
And this is the observed value acquired X equal to 25.

467
01:04:51,739 --> 01:04:55,429
Let's. Okay.

468
01:04:56,179 --> 01:05:04,099
So for a fixed ex, this is the idea for a fixed xy x equal to ten.

469
01:05:06,139 --> 01:05:10,069
The mean is. Oh, and one other thing.

470
01:05:10,069 --> 01:05:19,188
The the sort of the solid fat black line is now I know beta naught and beta one.

471
01:05:19,189 --> 01:05:27,979
Look the numbers right. So the solid black line is given by the straight line equation.

472
01:05:27,979 --> 01:05:33,429
Beta are not plus B go on. S I know beaten up.

473
01:05:33,469 --> 01:05:36,669
I will be the one and I can values.

474
01:05:37,249 --> 01:05:46,249
I get different values of that function beaten up as beta one x and let's see that solid black line characterizes that straight line.

475
01:05:46,609 --> 01:05:49,879
So now here is conceptually what is happening.

476
01:05:50,449 --> 01:06:01,889
So did a fixed x say x equal to ten then the mean is given by, you know,

477
01:06:01,909 --> 01:06:06,919
expected value of y given is we've done our plus beta one x, we just sign the previous slide.

478
01:06:07,249 --> 01:06:12,949
So then for four x equal to ten fixed x equals two then.

479
01:06:16,309 --> 01:06:22,969
The meat is given by B to zero plus contains bitcoin.

480
01:06:24,409 --> 01:06:30,379
Correct. So the beta zero plus ten times beta one will be.

481
01:06:35,439 --> 01:06:39,159
We fall on this black solid line.

482
01:06:39,999 --> 01:06:46,429
And it's this point here. Okay.

483
01:06:47,479 --> 01:07:00,679
So now think about this. Think about a normal distribution around that red circle.

484
01:07:01,579 --> 01:07:05,839
So the mean is at beta, not plus ten times between.

485
01:07:06,679 --> 01:07:11,009
And imagine a normal distribution around that red dot.

486
01:07:11,239 --> 01:07:23,389
That point, which has been which is which is centered around beta zero plus ten beta one with a variance of two.

487
01:07:23,569 --> 01:07:37,329
See, for example. So the absurd feathers of why for any given x are sent pulled from that distribution.

488
01:07:38,139 --> 01:07:46,409
As you can see, the absolute value of Y for x equal to ten is this black solid circle.

489
01:07:46,749 --> 01:07:50,439
It does not fall on that black solid line.

490
01:07:52,959 --> 01:08:00,369
If there was no error, if there was no accident, then we would have fallen on that black solid line.

491
01:08:01,299 --> 01:08:11,019
But because of that error. So there's a it's been sample from the normal distribution that is center that we

492
01:08:11,019 --> 01:08:16,479
do not understand with the one and has obedience itself signals critical to to.

493
01:08:19,509 --> 01:08:29,439
Similarly by his own family. Remember, I assume that the key must wear is constant across all values of x.

494
01:08:29,949 --> 01:08:36,699
So if I do x equal to C 25, same thing.

495
01:08:37,059 --> 01:08:48,329
The mean is better not plus 25 times beta one, which will fall exactly on this black solid line.

496
01:08:48,339 --> 01:08:53,499
So here is beta not plus 25.

497
01:08:58,309 --> 01:09:16,589
Radio on. So imagine a normal distribution around that red circle with variants, same variants as the as before variants equal to two.

498
01:09:17,069 --> 01:09:25,739
And your observed value of y 40 x equal to 25 is sampled from this distribution.

499
01:09:25,739 --> 01:09:37,649
And it turns out the observed value of why is this blue like is the solid sample that I denoted with the blue sample.

500
01:09:38,069 --> 01:09:42,959
So that's the globe. So once again, it does not fall or fall on that bold line.

501
01:09:44,279 --> 01:09:48,389
If there was no error, it would fall exactly on that bold line.

502
01:09:48,989 --> 01:09:54,269
But because of the error, there's a departure of the observed from the.

503
01:09:54,869 --> 01:09:58,709
From the mean. Yes. So.

504
01:09:59,729 --> 01:10:05,529
A picture of the assumption for estimation was.

505
01:10:06,289 --> 01:10:09,909
The first and second moments of. Iraq.

506
01:10:11,209 --> 01:10:16,579
Does that mean that in the context of estimation, you can have different distributions of.

507
01:10:17,549 --> 01:10:25,559
If they have to say, yeah, yeah, you get it, then yes, I sort of shoot it with an answer because it's easier to do to then.

508
01:10:25,979 --> 01:10:29,239
But yes, you can you can do it quick.

509
01:10:29,489 --> 01:10:37,618
For in the politics of estimation, this is basically I'm saying like how people conceptually how you can think about

510
01:10:37,619 --> 01:10:41,659
observations being generated in then this and we're going to bring it back.

511
01:10:41,699 --> 01:10:44,938
Okay. I mean, you know, we never know.

512
01:10:44,939 --> 01:10:53,039
We cannot be there. One here have actually written a bigger one on more than we are really backward to say like how are the lives being generated?

513
01:10:53,939 --> 01:10:59,339
But it's okay if you could be clear on this.

514
01:10:59,819 --> 01:11:03,079
Any other questions? Okay, great.

515
01:11:03,089 --> 01:11:08,279
So so that's the premise.

516
01:11:08,909 --> 01:11:13,019
Now here is a full list of the assumptions for estimation and inference.

517
01:11:14,009 --> 01:11:18,959
So we call this line the end stands for linearity.

518
01:11:19,139 --> 01:11:25,919
So we have why are you going to be to not plus B to one side plus epsilon I linearity is refers

519
01:11:25,919 --> 01:11:32,879
to linearity in the parameters and sort of signifies that the model is correctly specified.

520
01:11:33,989 --> 01:11:38,699
The expected value of Y, given a fixed x is beaten up, does become one x.

521
01:11:38,849 --> 01:11:43,709
So that's the L, that's the L assumption.

522
01:11:44,669 --> 01:11:51,029
What about the high the line? We assume that epsilon i an excellent j are independent.

523
01:11:52,289 --> 01:11:57,869
So this this denotes independence of variables.

524
01:11:58,649 --> 01:12:06,299
Hence Epsilon I and Epsilon G are independent I not equal to g.

525
01:12:07,299 --> 01:12:12,899
And this is just just to make sure that people don't get confused.

526
01:12:12,899 --> 01:12:17,869
So this is the full list of assumptions that I need for both estimation and inference.

527
01:12:17,879 --> 01:12:23,339
I had told a couple slides back, I don't need the independence assumption.

528
01:12:23,339 --> 01:12:25,919
I need just the uncorrelated assumption.

529
01:12:27,479 --> 01:12:35,399
The N stands for normality, which is again an assumption I don't need for estimation, but I need for inference.

530
01:12:35,819 --> 01:12:41,788
And the normality refers to that. The Epsilon A's, the errors have a normal distribution,

531
01:12:41,789 --> 01:12:54,959
which means zero gradient signals where sigma squared is finite and the normality of the error is also

532
01:12:54,959 --> 01:13:02,339
implied because of the result about normal distributions that linear combinations of normals are normals.

533
01:13:03,059 --> 01:13:09,718
So why, given a fixed value, affects also the normal distribution with mean better, not less.

534
01:13:09,719 --> 01:13:12,749
We don't want it so and variance equal decimal square.

535
01:13:13,799 --> 01:13:19,529
Okay so that's the in assumption of the in line.

536
01:13:19,769 --> 01:13:30,958
And what about the E as I'm sure this is the what we were just talking about a couple slides back so we assume the two things.

537
01:13:30,959 --> 01:13:42,239
One is that the variance of Y given a fixed x if I denoted as sigma I squared with an i subscript for the subject,

538
01:13:42,689 --> 01:13:50,069
this is always equal to B is always going to be equal to a constant sigma square.

539
01:13:50,069 --> 01:14:01,409
This is called a constant variance assumption also meaning that the variance does not change with the levels of x and it is finite.

540
01:14:02,009 --> 01:14:05,939
So variance does not change.

541
01:14:12,619 --> 01:14:25,419
We value of. So in other words, the enemies have equal, finite and equal medians.

542
01:14:25,449 --> 01:14:37,539
This is also referred to as understand variance assumption and there is a sort of 40 spoiler big name for it and it's called homeless kids basically.

543
01:14:37,779 --> 01:14:45,339
So homeless kids pick it up and write it, but that's basically it.

544
01:14:45,339 --> 01:14:48,729
So it's a big word, but that's all it means.

545
01:14:49,479 --> 01:14:51,569
So for most kids to sit down.

546
01:15:01,369 --> 01:15:13,239
Getting anybody needed my regression professor who was a very famous British professor and we used to have a big in the morning classes.

547
01:15:13,269 --> 01:15:21,859
Like he said, if you are in a party and somebody is really, really disturbing you, you throw words like this.

548
01:15:22,279 --> 01:15:27,369
He don't believe people get tested. And that's a very good test to get.

549
01:15:27,829 --> 01:15:32,179
So that's basically refers to constant obedience.

550
01:15:32,749 --> 01:15:38,239
Okay, so now a few more things.

551
01:15:38,539 --> 01:15:49,019
Fixed design exercise are pretty elastic, constant while discourse I mentioned it and think of sampling y at each level of x just as we saw like time

552
01:15:49,639 --> 01:16:00,139
from how to do that and the conceptually think about how the observations are being sample and in reality

553
01:16:00,139 --> 01:16:07,368
it's they maybe random as was pointed out that it would be due to measurement error for example body

554
01:16:07,369 --> 01:16:12,619
weight meaning that there might be measurement error in the scheme that you would be that you are using.

555
01:16:12,619 --> 01:16:24,229
The scale is not perfectly balanced and may have natural fluctuation due to body weight, water retention and body weight and so on.

556
01:16:25,699 --> 01:16:34,669
But throughout the course, we are going to assume that either X is fixed or the randomness is X in x is negligible.

557
01:16:35,989 --> 01:16:48,989
So. Model components relationship between X and min of Y is described by a line pin so that you saw the expected value of Y.

558
01:16:48,989 --> 01:16:56,669
Given a fixed value of x is beta not plus beta one excite right and way you can right.

559
01:16:56,669 --> 01:17:06,119
This is because we are equal to beta not plus beta one x plus epsilon nine.

560
01:17:06,539 --> 01:17:13,679
You can write Epsilon I the unknown to error says y minus the mean of y given a fixed x.

561
01:17:14,969 --> 01:17:19,799
We all know that a straight line is determined by two numbers the intercept and the slope.

562
01:17:20,399 --> 01:17:26,039
So what are the interpretations of beta not and beta one beta note is here.

563
01:17:26,549 --> 01:17:31,139
The intercept of the regression line. And how would you get better?

564
01:17:31,139 --> 01:17:42,059
Not if you plug in x equal to zero. The mean of y given x equal to zero is the intercept of the regression line.

565
01:17:42,059 --> 01:17:48,959
So like in practical carpets, basically you are saying something like this is the mean we have for individuals that aid zero.

566
01:17:49,469 --> 01:17:53,489
Does that make sense? Is it interpretable or no?

567
01:17:53,879 --> 01:18:01,229
So we will talk about how to make beta not interpretable, but that's based on the model that we have.

568
01:18:01,229 --> 01:18:05,218
That's the interpretation of probably about what? About beta one.

569
01:18:05,219 --> 01:18:08,039
Beta one is the slope of the regression line.

570
01:18:09,149 --> 01:18:18,209
So it's the change in the mean of y four, you need increase in X just as we would do for a straight line equation.

571
01:18:18,689 --> 01:18:25,949
So in the what is beta one between is two points, it's little x1 and little x x2.

572
01:18:27,089 --> 01:18:34,949
The numerator is the difference in the mean of y for the two values x1 and x2.

573
01:18:35,609 --> 01:18:40,289
And the denominator is the difference in the x1 x2 values.

574
01:18:40,679 --> 01:18:45,149
Okay. So that's, that's how you would define the slope of any straight line.

575
01:18:45,599 --> 01:18:49,349
So that's exactly what beta one is.

576
01:18:49,779 --> 01:18:57,389
And the interpretation is that it's the change in the mean of y per unit increase in x.

577
01:18:59,339 --> 01:19:09,729
Suppose we have two subjects are equal to one and are equal to two and the that before by one unit of x

578
01:19:09,749 --> 01:19:18,929
one saves one is the age here and the two ages are in seven and six and we are regressing weight on age.

579
01:19:19,829 --> 01:19:26,909
So then the difference in the expected value of y between the six year old and the seven year old,

580
01:19:27,239 --> 01:19:32,878
according to the regression model, will be beta one because if you take expected value of y,

581
01:19:32,879 --> 01:19:47,639
given that age is fixed at seven, then you get beaten up plus beta one times seven and the expected value of y486 is beta not plus six beta one.

582
01:19:47,999 --> 01:19:53,519
You take the difference of those two quantities and what you get is beta one.

583
01:19:53,819 --> 01:19:58,108
So basically it's the again back to the slope interpretation,

584
01:19:58,109 --> 01:20:12,749
it's the change in the mean y mean of y for every unit increase in x are the above results dependent on the specific x say values used?

585
01:20:15,689 --> 01:20:19,849
Yes. No. The answer is no.

586
01:20:20,499 --> 01:20:28,659
No, because in linearity we assume that for every unique change in eggs, the mean differences in wise, constant, regardless of the bad will face.

587
01:20:29,589 --> 01:20:34,269
It could be six seven. It could be five, six, ten, 11.

588
01:20:34,989 --> 01:20:38,949
It would be one unit increasing x. Efforts.

589
01:20:41,149 --> 01:20:45,049
This week. I want units of time in one meanwhile.

590
01:20:45,919 --> 01:20:54,108
Okay. So now a couple more points.

591
01:20:54,109 --> 01:21:03,769
Paternot and Bitterman are scale dependent. So when when we are interpreting, they do not to be the one we need to consider.

592
01:21:03,769 --> 01:21:08,809
The units between reflects the magnitude of the X Y Association.

593
01:21:09,579 --> 01:21:18,019
And and this is often of much greater inherent interest than we do not.

594
01:21:19,639 --> 01:21:29,999
As you saw that we could have kind of not non interpretable meaningless interpretations of we don't know but some things

595
01:21:30,049 --> 01:21:38,209
that might be of interest also and we will learn how to what to do to get a more meaningful interpretation of we or not.

596
01:21:40,759 --> 01:21:50,839
So here is an example. Suppose I have y has serum cholesterol and it's as the systolic blood pressure so the

597
01:21:51,139 --> 01:22:00,979
cholesterol units as mg DL and systolic blood pressure the unit is millimeters of mercury.

598
01:22:01,459 --> 01:22:06,529
So I have a model y equal to beta, not blood beta one ixi plus Epsilon II.

599
01:22:07,519 --> 01:22:12,228
And what what are the interpretations of P general beta one?

600
01:22:12,229 --> 01:22:16,459
Remember, they are they depend on the unit.

601
01:22:16,849 --> 01:22:22,638
So the interpretation of Bitterman in plain English is is the mean difference in serum

602
01:22:22,639 --> 01:22:31,579
cholesterol mg part here bar one unit higher or one unit increase in systolic blood pressure?

603
01:22:32,299 --> 01:22:40,788
MM of marketing. So that's the interpretation of midterm on what it would be to not be to know what the interpretation is.

604
01:22:40,789 --> 01:22:48,589
It's the mean serum cholesterol for patients with SBP systolic blood pressure should equal to zero.

605
01:22:49,579 --> 01:22:53,958
And the question here is, is zero SBP possible for life persons?

606
01:22:53,959 --> 01:23:03,649
The answer is clearly no. So how do we get a more meaningful interpretation of beta no?

607
01:23:06,079 --> 01:23:07,368
Here is another example.

608
01:23:07,369 --> 01:23:22,219
I, I wanted to mention one kind of technical point here, but maybe let's go ahead and we can come back because it would be a bit of a digression.

609
01:23:23,059 --> 01:23:31,039
But I will just so when I say mean difference or change.

610
01:23:35,729 --> 01:23:50,099
Why do you need iodine? Is BP not that I'm using the word difference of teams instead of saying increase incident illustrative and that.

611
01:23:50,459 --> 01:24:06,448
So the kind of the technical detail for that is that sort of that you can see change or difference depends on the nature of the study design.

612
01:24:06,449 --> 01:24:16,139
So the that it's an experimental design versus an observational study in an experimental design where

613
01:24:16,149 --> 01:24:22,169
you have gone through and everything else you have made the other than systolic blood pressure,

614
01:24:22,169 --> 01:24:30,479
incidental, etc., you have made it so you make everything the same for the subjects in the study.

615
01:24:32,369 --> 01:24:37,889
Then you. And see dance like genes.

616
01:24:37,889 --> 01:24:42,389
You can see increases depending on whether it is positive or negative.

617
01:24:44,009 --> 01:24:48,329
But in all. But it is not an experimental study.

618
01:24:49,139 --> 01:24:53,049
If it's an observational study, you cannot stop about change.

619
01:24:53,099 --> 01:25:00,309
All you can say is the mean difference in one for every one of the people.

620
01:25:00,719 --> 01:25:14,499
So it's a sophomore and it's a big one. I'm just going to say this and basically remind you that unless you know a lot about the study design this

621
01:25:14,549 --> 01:25:23,479
day showed that it's an observational study and use words like for anything that don't change or even.

622
01:25:25,029 --> 01:25:32,619
Okay. I'm going to sort of stop there. And if you need to elaborate on it more, we'll come back to the to the next, you know.

623
01:25:32,769 --> 01:25:38,189
I'm sorry. You have to. If you are just.

624
01:25:40,149 --> 01:25:48,279
Yeah. BUCHANAN Yeah, it's basically because yeah, that's what it is.

625
01:25:48,289 --> 01:26:01,939
If you've got a patient who suffers one, but even if you access work on phone, the how can I get a guarantee that I have I should put it on this site.

626
01:26:02,279 --> 01:26:12,979
So, so that's why you see it's hard to see the difference rather than, you know, you know, sort of even having to do all that.

627
01:26:16,929 --> 01:26:21,158
Do you play it or is this kind of your thing before?

628
01:26:21,159 --> 01:26:25,959
If you want to meeting, please. Yeah.

629
01:26:26,829 --> 01:26:30,239
I mean, y increases of changes.

630
01:26:30,249 --> 01:26:32,408
Moving to talk about change.

631
01:26:32,409 --> 01:26:43,019
We're arguing that, you know, everything is that move ethnic y you have control control for and we cannot really get into that not that the.

632
01:26:47,139 --> 01:26:54,289
Okay. So now. Same here.

633
01:26:54,919 --> 01:27:01,548
A study of adult males aged 20 to 39 weighed in kg.

634
01:27:01,549 --> 01:27:06,679
Why an age in years X and a modeling Y?

635
01:27:07,429 --> 01:27:14,479
It read as a function of age. So beetle on here I.

636
01:27:14,659 --> 01:27:26,389
I said, you know I have teens here but once again know unless I know that this is all an experimental study, I should be using mean difference.

637
01:27:30,399 --> 01:27:34,359
What about beaten? Not here. So again, we like beta.

638
01:27:34,419 --> 01:27:42,339
Beta not still has the interpretation that it's the mean rate for AIDS zero.

639
01:27:42,399 --> 01:27:49,839
Yes. Not only is this not interpretable here, there is an additional issue.

640
01:27:49,839 --> 01:27:53,769
And what is that? We are looking at the study of Muse.

641
01:27:53,949 --> 01:27:58,719
It is 2239, but we are extrapolating it zero.

642
01:27:59,019 --> 01:28:08,619
So this brings me to a to another point that is very important in regression and that relates to extrapolation.

643
01:28:08,739 --> 01:28:14,949
So generally the inference and the conclusions should respect the range of of the property embedded.

644
01:28:15,369 --> 01:28:19,779
So if you return to the just the previous example, the bit example,

645
01:28:20,619 --> 01:28:32,858
the model that we fitted would apply to the age range 20 to 39 years and that we like we within that age range.

646
01:28:32,859 --> 01:28:42,399
The relationship between age and beat might be very different from that which applies to someone, say, age 65.

647
01:28:43,119 --> 01:28:54,789
The relationship may no longer be linear, and here is an example of how you know, within the observed range of X that 20 to 39,

648
01:28:55,149 --> 01:29:03,039
it is a linear relationship, but how can I guarantee that the linear relationship will still hold outside that range?

649
01:29:03,369 --> 01:29:11,379
And, you know, like outside that 20 to 39 year range, the true model could be something like this,

650
01:29:11,379 --> 01:29:16,749
the dotted red red line, which has a definite curvature.

651
01:29:16,869 --> 01:29:19,989
So the two model might be very different from a linear model.

652
01:29:20,259 --> 01:29:22,929
I do not have any data in that range.

653
01:29:23,409 --> 01:29:37,749
So using that model to predict for somebody whose age is 65 would be totally, totally wrong and I could be totally wrong.

654
01:29:37,749 --> 01:29:45,549
And this and the predictions generally made outside this observe range of the Xs might be very, really erroneous.

655
01:29:45,569 --> 01:29:49,059
So I want to be careful about the extrapolation of.

656
01:29:51,089 --> 01:29:56,249
Standard and convenient to make intercept interpretable. So in the original model, yes.

657
01:30:03,219 --> 01:30:10,659
See. Yeah.

658
01:30:10,929 --> 01:30:16,499
So you got you really do get in a lot to support everyone and everything.

659
01:30:16,509 --> 01:30:20,809
Please be at the mean of law.

660
01:30:22,209 --> 01:30:26,439
But we need for to go on. All these thoughts and stuff.

661
01:30:27,369 --> 01:30:33,849
We talk about like if you know think that more than that someone's spawned though is nothing.

662
01:30:34,329 --> 01:30:44,549
I can remember I see them sort of be careful because it messes up the interpretation from an adjective saying that it seems kind of indefinite.

663
01:30:44,759 --> 01:30:47,999
I think they think it is a state of the art.

664
01:30:48,729 --> 01:30:53,589
So the interpretation in the witness game is not going to.

665
01:30:55,789 --> 01:31:02,629
Okay. So now centering the pool, we need to make the intercept interpretable.

666
01:31:02,659 --> 01:31:17,029
We have the original model Y equal to be. Do not look. We don't want explicit that let's say I define a new variable excised star as XOR minus x bar.

667
01:31:17,419 --> 01:31:25,129
So what is the x bar? X bar is the sample mean of the excess?

668
01:31:27,279 --> 01:31:38,349
A good third one what information I from one learning site now instead of progressing Y on XIV this Y on its high star.

669
01:31:39,879 --> 01:31:47,229
So here is the device model y are equal to beta, not star plus beta one star in size star plus epsilon I star.

670
01:31:48,159 --> 01:31:53,529
We still have the error distributed.

671
01:31:53,769 --> 01:32:02,888
I mean, I put a normal there, but I like, you know, I really should have done without the normal but we still have the distribution

672
01:32:02,889 --> 01:32:09,969
of the error says meaning zero obedience sigma squared and on populated areas.

673
01:32:10,659 --> 01:32:21,759
So the errors are unchanged. But what we have is graphically we have all locations here in the axis.

674
01:32:23,019 --> 01:32:33,488
And what is the implication of that? How does it how does it lead to or kind of like improve the interpretation of the intercept?

675
01:32:33,489 --> 01:32:40,089
So we call in the original model, the interpretation of beta naught is that, you know,

676
01:32:40,089 --> 01:32:50,079
it's the mean read for individuals at end zero because they cannot be equal to the expected value of young given Excel equal to zero.

677
01:32:50,799 --> 01:32:55,509
So what is the mean with AP zero? That is the interpretation for beta.

678
01:32:56,019 --> 01:33:06,758
What happens to the revised model? The revised model, the interpretation of the intercept beta not star is it's the mean of y.

679
01:33:06,759 --> 01:33:14,618
Given exercise start is equal to zero. But what is it say started say study for is ixi minus x.

680
01:33:14,619 --> 01:33:24,219
But we have sent the expert use. So we take each subject, we take that experiment, subtract.

681
01:33:24,219 --> 01:33:29,649
The sample mean of the expelled is from the, you know, forwarded values for them.

682
01:33:29,889 --> 01:33:37,889
So now we have beta not studies. The expected value of y given x is equal to expired.

683
01:33:38,499 --> 01:33:43,929
So what is the interpretation named? The prediction is that the mean rate for the individual reader?

684
01:33:43,929 --> 01:33:54,219
No, it is the mean bit for the individuals with average age average meaning the average age of the sample of my individuals.

685
01:33:54,639 --> 01:34:06,189
So this is a very quick and simple but a but a kind of a smart trick to make the beta not interpretable.

686
01:34:07,539 --> 01:34:19,279
Okay. What happens to the slope by centering the slope, actually.

687
01:34:19,579 --> 01:34:27,379
Interpretation remains unchanged in the original model, so I take it to expand the x1 and x2.

688
01:34:27,949 --> 01:34:36,829
So the numerator is the difference in the mean of wise for the x1 and x2 values divided by the denominator is x2 minus x1.

689
01:34:37,399 --> 01:34:40,849
In the standard model, what happens to the slope?

690
01:34:41,089 --> 01:34:48,049
The slope is basically say I'm doing everything now in terms of the X1 excised stars.

691
01:34:48,799 --> 01:34:59,119
So the main difference in the mean of Y's, given its size, are equal to X2 and excess start equal to X1.

692
01:34:59,809 --> 01:35:03,439
And in the denominator I have x2 minus x1 once again.

693
01:35:04,129 --> 01:35:08,958
So now plugging what is x?

694
01:35:08,959 --> 01:35:27,529
I started so studies it sorry minus x bar and so x5 then becomes x bar plus x2 and and the second term is x is expired plus x1.

695
01:35:28,849 --> 01:35:42,708
Correct. So so in the numerator, I have the difference in the mean of y or responding to expert plus x2 and expert plus x1.

696
01:35:42,709 --> 01:35:47,538
And in the denominator I have x2 minus x1. So what do I do to the denominator?

697
01:35:47,539 --> 01:35:51,829
I added spot because it's a constant I add expert.

698
01:35:51,829 --> 01:35:55,969
So I did expire plus x2 minus x plus x1.

699
01:35:56,179 --> 01:35:58,429
So now how does this expression look?

700
01:35:59,629 --> 01:36:13,099
It looks again like in the center model that it's basically the difference in the mean of y corresponding to x1 prime and it's prime,

701
01:36:13,099 --> 01:36:18,439
but x1 prime is a far plus x1 and x2 prime is a five plus x2.

702
01:36:18,829 --> 01:36:22,499
And the denominator is the difference between X1 dynamics design.

703
01:36:22,579 --> 01:36:27,649
So it's back to the same data one.

704
01:36:28,129 --> 01:36:33,769
So the interpretation in the center model remains the same.

705
01:36:33,769 --> 01:36:39,269
It's exactly equal to make that one. Okay.

706
01:36:39,389 --> 01:36:46,739
So I am out of time. I really wanted to at least start talking about barometer estimation.

707
01:36:47,249 --> 01:36:51,569
But we will pick up from here first. Okay.

708
01:36:54,819 --> 01:37:00,159
And what for? Convertido the Walmart problem set.

