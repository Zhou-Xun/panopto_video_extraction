1
00:00:00,180 --> 00:00:07,400
Go on here. Go.

2
00:00:48,520 --> 00:01:01,810
So I wanted to finish up today on with a little bit of discussion about sort of causal effect versus causal association approach.

3
00:01:06,100 --> 00:01:13,719
So and they said that what kind of sort of finished the core of what I perceive this class to be.

4
00:01:13,720 --> 00:01:17,560
And then we're going to go on to a little bit of discussion about causal inference and

5
00:01:17,560 --> 00:01:22,000
time to time and event settings or more generally sort of time variance settings.

6
00:01:23,440 --> 00:01:31,149
And then I'll finish up the class after after the break and I'll do a little introduction to survival analysis.

7
00:01:31,150 --> 00:01:35,470
I know that everybody's had that, and I'm not going to necessarily anticipate we're going to get 100% out of this.

8
00:01:36,080 --> 00:01:47,590
But if it is, in order to talk about that, if I have a little bit of framework for that to work, then, so okay.

9
00:01:48,610 --> 00:02:00,970
So we talked a little bit about defining direct effects in the causal association or principal stratification setting.

10
00:02:03,640 --> 00:02:09,400
And it's and then starting a little bit with this idea of indirect effects.

11
00:02:11,620 --> 00:02:19,609
Now the, the big difference is that the principal strain of approach, the estimation of direct effects doesn't you doesn't know,

12
00:02:19,610 --> 00:02:27,250
it doesn't require it really doesn't allow the mediator to be sort of conceptually separable, communicable from the treatment.

13
00:02:28,180 --> 00:02:31,540
So we sort of moved into these direct and indirect effect discussions.

14
00:02:32,150 --> 00:02:38,740
We started to define a new form of the potential outcome that now was a function both of the treatment,

15
00:02:38,740 --> 00:02:42,969
a mediator in principle stratification doesn't really do that.

16
00:02:42,970 --> 00:02:46,120
It's just considers a potential outcome of an under a.

17
00:02:47,680 --> 00:02:57,790
And so I would say the under wheel and more generally Jeremy Robbins and others like him a sort of developed and champion this

18
00:02:58,120 --> 00:03:04,450
causal effect approach view this as a as a limitation of the of the principle stratification or causal association approach.

19
00:03:06,040 --> 00:03:09,429
It can really be viewed as sort of sweeping the issue of the mediator outcome,

20
00:03:09,430 --> 00:03:14,620
confounding under the drug, and we'll show that in a bit more explicit detail.

21
00:03:15,520 --> 00:03:20,680
So basically, there's really no coherent definition of indirect effect that could sum to the overall effect.

22
00:03:21,280 --> 00:03:29,139
So this direct and indirect effect added up to the overall effect in the principal stratification approach really doesn't work.

23
00:03:29,140 --> 00:03:34,000
You can see that some of the work that I did and some others have done kind of try to develop

24
00:03:34,020 --> 00:03:39,729
this association idea that that might corresponded some vague sense to an indirect effect.

25
00:03:39,730 --> 00:03:50,740
But it's it's not. And so basically these these two approaches do correspond, though, with respect to the direct effect.

26
00:03:50,740 --> 00:03:56,650
When there's no books, I don't want to know.

27
00:03:56,800 --> 00:04:00,460
Go away and make it go away.

28
00:04:03,220 --> 00:04:07,450
All right, fine. But I think compared to where.

29
00:04:09,040 --> 00:04:15,879
So. So if there's no mediator outcome confounding present and there was a discussion of this and

30
00:04:15,880 --> 00:04:21,670
put in the material that I read so I'm going to kind of go through a little bit of an example.

31
00:04:23,470 --> 00:04:28,570
And this was an example where Rubin basically sort of developed this idea of direct effects and and

32
00:04:28,570 --> 00:04:34,780
made some case that that that was sort of what the really guestimate sensibly and causal inference.

33
00:04:35,830 --> 00:04:50,110
So. Okay so basically so in this setting, we basically have essentially the,

34
00:04:50,800 --> 00:04:57,910
the principal strata here of corresponding to this distribution of M0 and M one basically comes in two flavors,

35
00:04:57,910 --> 00:05:04,959
one where M zero is two and M one is three, and then where M0 is the three of them,

36
00:05:04,960 --> 00:05:11,470
one is four, and then potential outcomes accordingly and then the treatment assignments.

37
00:05:12,640 --> 00:05:19,600
So we're essentially doing randomized assignment and these principal strata are sort of equal in the population.

38
00:05:20,200 --> 00:05:29,559
So that means essentially the population can be divided into four equal equal categories.

39
00:05:29,560 --> 00:05:38,200
So one quarter of the population, each of these four categories that are given here and the red lines here correspond to stuff that we observe.

40
00:05:39,910 --> 00:05:48,310
So we observe, you know, half the population in zero and one and half.

41
00:05:48,970 --> 00:05:53,230
Half the population and zero. Half the population one.

42
00:05:54,370 --> 00:06:02,680
And then since this mediator principle straight are equally distributed because of randomization, we have essentially within each of these.

43
00:06:05,130 --> 00:06:08,670
Zero one. Assignments.

44
00:06:08,970 --> 00:06:13,320
Half of the individuals are two and three and a half or three or four.

45
00:06:13,710 --> 00:06:20,100
And then the job comes just tie in. This has this is a, you know, sort of just the nature of the population.

46
00:06:22,740 --> 00:06:26,160
Okay. So they sort of think that through a little bit.

47
00:06:26,520 --> 00:06:34,810
That said clear. So the red stuff is stuff is what we observe at the black stuff is what we don't observe.

48
00:06:34,830 --> 00:06:42,000
Right. So if somebody decided to control, we can't see their mediator or their outcome of treatment and vice versa.

49
00:06:43,590 --> 00:06:53,100
But since we have this situation where we're able to sort of look down and we can see all the potential outcomes,

50
00:06:53,940 --> 00:06:57,070
then we can see the causal effect of the treatment of the mediator is one.

51
00:06:57,090 --> 00:07:03,000
Right. There's always a difference of one here between the treated mediator values and the control media values.

52
00:07:06,360 --> 00:07:13,590
It's also going to be the observed difference for the meat eaters under treatment and control because it randomization.

53
00:07:14,100 --> 00:07:22,229
Right. And it doesn't really matter because it's always one so similar.

54
00:07:22,230 --> 00:07:25,470
The causal effect of the treatment the outcome is going to be zero. Right.

55
00:07:26,280 --> 00:07:35,670
There's no differences here between the difference between the control value and the and the treated values are always equal to each other.

56
00:07:36,960 --> 00:07:44,520
Of course, the control values can be different, but the change is always zero in randomization or observed.

57
00:07:45,150 --> 00:07:48,700
Difference means also zero. Okay.

58
00:07:49,420 --> 00:07:54,660
Keeping that in mind. It's somewhat different.

59
00:07:54,710 --> 00:07:59,630
Difficult to talk about direct effects in the principal straight setting since we basically don't have any difference here.

60
00:08:00,200 --> 00:08:04,880
Right. This difference in the media values is one that has clearly no effect of the treatment on the outcome.

61
00:08:06,320 --> 00:08:10,100
So Rubin basically argues this is problematic in language direct indirect effects.

62
00:08:11,150 --> 00:08:14,570
So although A is randomized, right.

63
00:08:15,320 --> 00:08:27,110
If we look at the difference in the treated values conditional on meaningful the three, then the difference in the means is going to be negative two.

64
00:08:29,310 --> 00:08:39,639
So. So effectively.

65
00:08:39,640 --> 00:08:44,200
If there's no overall effect, then we shouldn't really expect to see a direct effect.

66
00:08:45,130 --> 00:08:52,720
But yet we do. But Vanderbilt points out in this example,

67
00:08:52,990 --> 00:08:59,140
it doesn't only consider the potential outcomes for the joint distribution of A&M that is a separately beneficial from him.

68
00:09:00,310 --> 00:09:03,910
So this would allow us to define direct and indirect effects could be in the opposite direction.

69
00:09:05,200 --> 00:09:08,830
It also allows consider with me your outcome, the independent and whether confounded.

70
00:09:12,810 --> 00:09:22,230
So. So let's consider two settings the first where there's no mediator outcome confounding and the second where there is.

71
00:09:23,590 --> 00:09:28,470
I guess one thing to point out here is why do we have mediator confounding so.

72
00:09:30,760 --> 00:09:34,839
First. The first thing is to notice I've expanded this table here.

73
00:09:34,840 --> 00:09:37,990
Right? So now I'm allowing for this.

74
00:09:40,780 --> 00:09:46,080
Cross world. No. Come. Right. So what happens? Two.

75
00:09:46,090 --> 00:09:52,030
Why have treatment in zero when multiple three year treatment is one of the three?

76
00:09:53,380 --> 00:10:05,680
So now you also notice here, because of the cross world, I don't necessarily observe these potential outcomes in all of these trades here.

77
00:10:05,860 --> 00:10:12,850
Right. So. When Emma's equal to three.

78
00:10:14,380 --> 00:10:21,350
Right. That requires the. For these these types of individuals.

79
00:10:21,350 --> 00:10:24,460
So in order for people of three have to be under treatment.

80
00:10:25,300 --> 00:10:35,020
So all this can continue to exist and sort of cross what kind of factual since we can't observe it now,

81
00:10:35,020 --> 00:10:43,540
we can't observe it for the Treaty Group and indeed we do when a is equal to 11.1800.

82
00:10:45,100 --> 00:10:54,070
And conversely, for this, this chunk of the population, we can observe it for when it is equal to zero.

83
00:10:55,180 --> 00:11:01,910
Right. Because that and the three. So down here.

84
00:11:02,660 --> 00:11:10,460
Same thing. I just happen to have different values for my classroom outcomes.

85
00:11:30,020 --> 00:11:39,790
So the reason the second situation is confounded. It's.

86
00:12:30,950 --> 00:12:38,060
Right. So basically this potential outcome corresponds to this row here.

87
00:12:44,980 --> 00:12:50,460
No, I'm sorry. It's the bottom row here. So when I was equal to one and was able to four.

88
00:12:52,840 --> 00:12:58,600
Right. That's this group here in Naples.

89
00:12:58,600 --> 00:13:17,270
One in 12. Now, if I hold the treatment constant and change the mediator.

90
00:13:22,530 --> 00:13:37,130
Can anyone pick out what this is going to be? All right, you see?

91
00:13:44,140 --> 00:13:51,870
He's in our bottom table here. Yeah.

92
00:13:58,130 --> 00:14:01,870
Right. That corresponds to this. This table here.

93
00:14:03,610 --> 00:14:07,600
One and three. So that's my potential outcome.

94
00:14:08,490 --> 00:14:13,260
All right. So basically. No confounders.

95
00:14:18,670 --> 00:14:50,210
You should be. You might know also here, certainly from the observed data, these two tables are identical.

96
00:14:50,420 --> 00:14:54,050
Right. In terms of the.

97
00:14:57,210 --> 00:15:02,040
The part that doesn't involve the cross world mediators drawing never in the.

98
00:15:03,660 --> 00:15:07,580
It's there. All the action is over here in this.

99
00:15:10,310 --> 00:15:30,100
Because we will compare potential outcomes based. So you might notice, I guess in the first table that.

100
00:15:32,910 --> 00:15:38,069
Again, these these two quantities correspond to these two expectations.

101
00:15:38,070 --> 00:15:41,250
And there they are, equal rights of change in him.

102
00:15:45,390 --> 00:15:55,000
Has no impact on that potential outcome. So there's no compelling. And because everything was fully randomized.

103
00:16:05,440 --> 00:16:16,080
So. So if we look at our first example.

104
00:16:26,960 --> 00:16:46,170
See. That table up there.

105
00:16:50,140 --> 00:17:29,900
So. Okay.

106
00:17:29,940 --> 00:17:37,570
So. Why is this ten -12 or negative two?

107
00:17:40,700 --> 00:17:44,290
Well, let's go back to this table here. So.

108
00:17:50,660 --> 00:17:54,650
Basically. So these are all equal in the population.

109
00:18:04,280 --> 00:18:21,930
Right. So this is basically. .25 10.5 1055 510, ten.

110
00:18:23,160 --> 00:18:33,900
In the same thing here. Right? Tricky times.

111
00:18:50,690 --> 00:18:55,090
Right. So I'm just averaging these values here.

112
00:18:56,230 --> 00:19:21,800
So I get this controlled, direct effect. But I restrict my analysis to the last two rows of this first.

113
00:19:23,900 --> 00:19:27,230
Then it's a natural, direct effect.

114
00:19:32,910 --> 00:19:36,460
Right. So that's just for no other.

115
00:19:36,470 --> 00:19:46,210
Fixing immediate mediator value. All right.

116
00:19:46,240 --> 00:19:53,970
So that's going to be cool. Well, maybe you.

117
00:20:40,830 --> 00:20:57,160
So when one is one. The median value under M0 is three.

118
00:21:07,860 --> 00:21:15,680
So 841 and equals three is given by ten here. Zero zero.

119
00:21:29,180 --> 00:21:35,960
It's also going to give by equals zero and equals three or 12 and -12 to.

120
00:21:41,190 --> 00:21:45,030
And so an actual indirect effect then.

121
00:21:47,970 --> 00:21:53,900
There's always my total effect. Minus my natural direct effect.

122
00:21:55,100 --> 00:21:58,310
So know just it's going to be zero in that first table.

123
00:21:59,220 --> 00:22:30,200
Zero minus eight. Two is two. So.

124
00:22:33,560 --> 00:22:40,340
So this corresponds to this to the situation.

125
00:22:40,340 --> 00:22:44,450
There's no confounding. So everything seems everything matches up.

126
00:22:44,540 --> 00:22:59,970
But when you have confounding. Then this chorale controlled direct effect this piece of here now becomes.

127
00:23:07,830 --> 00:23:16,070
But now we're going to have. Second table.

128
00:23:37,180 --> 00:23:45,130
98.25 points. Ten plus 2.25 ten plus five.

129
00:23:45,790 --> 00:23:50,560
12.25 12.

130
00:23:52,330 --> 00:23:58,110
411. And then we're going to have the same thing.

131
00:24:00,520 --> 00:24:05,990
But the second piece here. So.

132
00:24:09,230 --> 00:24:38,290
Right. Now we have a controlled direct effect.

133
00:24:39,820 --> 00:24:43,910
Beta is fixed at three. Zero.

134
00:24:45,860 --> 00:24:52,310
So even though we have randomization to the control direct effect estimated for the observed data is going to be biased.

135
00:24:56,640 --> 00:25:10,130
So. That's because essentially we don't have this.

136
00:25:10,970 --> 00:25:21,950
We've been unable to break the mediator outcome confounding. So the methods we described, if this remains, don't get away from that problem.

137
00:25:23,210 --> 00:25:31,130
We still would have to hopefully be able to condition of access that we no longer have that that this this mediator confounding situation here.

138
00:25:31,730 --> 00:25:35,690
So once you start conditioning on covariates,

139
00:25:36,800 --> 00:25:44,120
we were able to we eventually get straight over where this that where things looked like this, not like this.

140
00:25:47,390 --> 00:25:53,060
So that's the assumption. If we don't have that, then of course, we still get we still get biased estimation.

141
00:25:53,780 --> 00:25:56,980
Of course, we talked about such losses and so forth to be okay.

142
00:25:58,100 --> 00:26:11,450
But. But I think what Vanderbilt basically argues is that without being able to break things out into this approach,

143
00:26:11,960 --> 00:26:19,250
you can't get at the media confounding, which is which is critical for estimating direct and indirect effects.

144
00:26:24,150 --> 00:26:30,660
So that's this divide between the sort of. Rubin or parental stratification approach in this province, Greenland, or direct indirect approach.

145
00:26:32,850 --> 00:26:40,380
So my personal opinion on this is that the Rubin approach is sort of easy to understand conceptually.

146
00:26:41,220 --> 00:26:44,910
It's less reliant on testable assumptions in some sense closer to the data.

147
00:26:46,470 --> 00:26:53,790
But it kind of it leads this assumption of this mediator outcome randomization at the cost of not being able to define direct indirect oil,

148
00:26:53,830 --> 00:26:56,730
not militarily defined, direct effect, indirect effects.

149
00:26:58,300 --> 00:27:05,200
So I think to the extent we think about direct effects as being a sum of direct and indirect as

150
00:27:05,200 --> 00:27:13,060
being sort of understandable in the context of something with indirect effects to a total effect,

151
00:27:13,090 --> 00:27:22,870
then even a direct effect measure is perhaps problematic. So but it does avoid making this assumption.

152
00:27:22,870 --> 00:27:26,360
So everything you're talking about out of this situation, including the,

153
00:27:26,650 --> 00:27:34,420
the sort of stratification definition of direct effects, avoids dealing with immediate outcome randomization.

154
00:27:36,040 --> 00:27:39,879
Whereas this over the years I've started to think of the Robin Greenland approach

155
00:27:39,880 --> 00:27:43,000
as sort of closer conceptual to the way we usually think about cause and effect,

156
00:27:44,650 --> 00:27:50,290
particularly when it comes to mediation. But I think each does have their strengths and weaknesses.

157
00:27:52,720 --> 00:27:57,460
So Eric, I wanted to just sort of kick things back a little bit.

158
00:28:00,880 --> 00:28:07,060
So in the principle stratification approach and then we should give us some context,

159
00:28:07,060 --> 00:28:13,170
let's think about the compliance situation because that's a situation that we developed pretty carefully in the principal stratification setting.

160
00:28:13,660 --> 00:28:17,560
And we also discussed a little bit as a direct indirect effect approach alternative.

161
00:28:18,730 --> 00:28:27,030
So. So Principal Streeter, do we can we think of those kind of immutable quality qualities?

162
00:28:27,060 --> 00:28:33,060
So first of all, I guess in the in the, in the mediation, in the compliance setting, what are the principal trainer?

163
00:28:35,360 --> 00:28:38,660
Your compliance is right.

164
00:28:38,780 --> 00:28:48,260
So that's defined by what? Um, how do you know people actually act depending on their every time.

165
00:28:48,710 --> 00:28:52,670
Right. So in what way would these be immutable?

166
00:28:55,420 --> 00:29:04,340
Or not. Suppose we switch to a different treatment?

167
00:29:07,700 --> 00:29:16,630
Do you think the principle straight would stay the same? Probably true, right?

168
00:29:16,650 --> 00:29:20,160
Compliance is probably not a fundamental behavior of human beings.

169
00:29:22,170 --> 00:29:29,490
Right. You're probably not going to always comply with your treatment assignment or not, regardless of what the treatment is.

170
00:29:31,590 --> 00:29:40,890
So so that would be one possibility of of of changing their hand if the treatment stays constant.

171
00:29:42,120 --> 00:29:46,380
You know, imagine that as being a more somebody that really is stable.

172
00:29:46,470 --> 00:29:50,100
Although conceivably things change over time.

173
00:29:50,100 --> 00:29:55,530
But if the treatment isn't changing. Right. Let's say if I mean, what are some reason that you might not comply?

174
00:29:59,890 --> 00:30:05,560
But again, you, you know, learned some things concerning about the treatment.

175
00:30:05,830 --> 00:30:09,010
Right. You could be worried about side effects.

176
00:30:09,230 --> 00:30:16,060
Right. Good experience, side effects. Anybody else, please?

177
00:30:17,160 --> 00:30:33,550
Sorry. Sorry.

178
00:30:33,940 --> 00:30:39,010
Yeah. Informing him that you're getting information that he's planning to shoot.

179
00:30:39,400 --> 00:30:45,130
Yeah. Yes. Okay. I think it probably be closely related to the side effect that he like he would be worried about.

180
00:30:45,640 --> 00:30:51,370
What are some of the things that are just about really just be sure to be religious beliefs?

181
00:30:51,610 --> 00:30:55,060
Yeah. So maybe there are certain treatments that can conflict. Right.

182
00:30:55,070 --> 00:30:58,240
But maybe. I mean, like. I guess like. Yeah.

183
00:30:58,390 --> 00:31:03,540
So far we've treated like behavior and kind of based on your beliefs.

184
00:31:03,550 --> 00:31:07,570
Like you. Yeah, maybe. You know, you just, you know, we're busy.

185
00:31:07,570 --> 00:31:10,570
One day he could sign up, like, show up for him. Right. Okay.

186
00:31:10,570 --> 00:31:17,220
If you just do. Yeah. You're sort of on the right on the track to what I was thinking about, which is that some treatments could be hard to take.

187
00:31:17,230 --> 00:31:23,800
Right? So it may not be that you'd suffer side effects, but the sort of difficulty of of of doing them.

188
00:31:24,010 --> 00:31:31,420
Example for my own personal situation, my, my daughter was very sensitive about pill sizes and so she was unable to take.

189
00:31:31,810 --> 00:31:36,160
Really resistant to taking medication certain medications because they went, you know, large pill.

190
00:31:37,360 --> 00:31:42,400
She was able to get a smaller pill to do it. So she was a kid.

191
00:31:44,620 --> 00:31:48,940
So yeah, so those, those kinds of things.

192
00:31:49,210 --> 00:31:58,540
So as the treatment changed, well, that's probably what was in the medication is no different, the sort of active chemical components.

193
00:31:59,890 --> 00:32:03,160
The delivery system did change. Is that a change in treatment?

194
00:32:04,900 --> 00:32:09,310
Probably from a perspective of principal stratification. So.

195
00:32:10,490 --> 00:32:18,500
Okay. So what about the implication of being able to sort of manipulate mediators and treatments independently in this context?

196
00:32:18,740 --> 00:32:25,670
So right here, the mediator is is what? Legitimate complaints points.

197
00:32:31,970 --> 00:32:37,760
Well, right. So the compliance class is used to form a bit more the direct indirect effect setting,

198
00:32:37,760 --> 00:32:42,860
but the compliance classes form in a combination of treatment assignment and treatment taken.

199
00:32:43,010 --> 00:32:49,010
But when we're in this sort of mediation context, what is the mediator variable, the triggering you're actually.

200
00:32:49,370 --> 00:32:53,100
Yes, it's taken right in the treatment you're assigned to is a remedy.

201
00:32:53,120 --> 00:32:55,720
Is that the actual treatment? Right.

202
00:32:56,360 --> 00:33:07,460
The sort of treatment exposure that you're concerned about A versus M and or if we stick with our standard notation developed.

203
00:33:08,900 --> 00:33:15,860
So, so so what is what is sort of, you know,

204
00:33:15,860 --> 00:33:22,759
manipulate those I can practice we sort of get back to that but we sort of move a little keeping an eye in the

205
00:33:22,760 --> 00:33:29,510
rearview mirror to the to the compliance class setting will need to be able to manipulate that independently.

206
00:33:42,200 --> 00:33:48,470
For example, keeping your troops out, as you say, but making it easier or pushing people to comply more.

207
00:33:50,710 --> 00:33:52,690
Yeah, this is the right track.

208
00:33:52,690 --> 00:34:03,010
So basically you could you could sort of force people to take the drug regardless of whether they were assigned to it or not and vice versa.

209
00:34:04,450 --> 00:34:08,350
So but I think particularly in the setting, if you think about some of the reasons you might not comply,

210
00:34:09,400 --> 00:34:17,380
that that leads to some situations where you might I mean, you might have issues.

211
00:34:18,090 --> 00:34:23,890
And this is going to get to the next question about positivity. So given that that is required, what?

212
00:34:26,200 --> 00:34:32,960
But what might that say about positivity? Not just write down positivity here.

213
00:34:33,980 --> 00:34:52,920
Adaptation. We didn't emphasize this, but this is an important part.

214
00:35:07,510 --> 00:35:15,270
Basically that there's some non-zero probability that somebody in the population will take on all of these factors.

215
00:35:23,910 --> 00:35:28,710
Or I should say really sorry.

216
00:35:33,310 --> 00:35:43,130
Within. But you can get an individual to take some time to apply for these classes.

217
00:35:44,660 --> 00:35:49,610
Yeah, I'm not sure having answer here, but you know,

218
00:35:49,610 --> 00:35:55,730
so minimum income independently is that is a good way to think about that makes sense

219
00:35:55,760 --> 00:36:00,350
like you know a conflict mediator is like some sort of degree like partial mediation.

220
00:36:00,980 --> 00:36:04,670
No, mixed mediation is defined in terms of manipulating things separately.

221
00:36:04,730 --> 00:36:09,280
Right. So. So what about. Let's suppose the side effects are really bad.

222
00:36:09,320 --> 00:36:15,040
Like you die. What would that do to positivity?

223
00:36:16,870 --> 00:36:23,140
For those individuals. Some people wouldn't take this.

224
00:36:23,830 --> 00:36:27,160
Yeah. Yes. So does it have to be zero for those people?

225
00:36:27,340 --> 00:36:31,940
They literally could not survive such. So.

226
00:36:35,680 --> 00:36:43,570
So I think that that would be that is that is sort of the the downside of this.

227
00:36:44,620 --> 00:36:51,460
If you think about these things meaningful more than they really substance have to be many people even if it's a cross world counterfactual.

228
00:36:52,900 --> 00:37:10,020
So. So this was a source of a great deal of of of noise and argument in in this in this area is one of the few parts of statistics where

229
00:37:10,020 --> 00:37:17,280
there really was a full on sort of classic scientific debate where the main people involved actually literally hated each other.

230
00:37:17,880 --> 00:37:28,020
So result of disagreeing over over some some some issues related to the science and it kind of held back the field in some ways a little bit,

231
00:37:28,620 --> 00:37:36,749
although I think over the last 20 years or so, the the students of these people have not carried on that level of animosity.

232
00:37:36,750 --> 00:37:45,510
And so and so now we're we're in a situation of at least a detente, if not a little bit better.

233
00:37:46,760 --> 00:37:50,659
So folks weren't around for this.

234
00:37:50,660 --> 00:37:53,990
But I don't know if he had any thoughts in terms of the sort of.

235
00:37:55,990 --> 00:38:12,850
Comments people want to make. All right.

236
00:38:12,970 --> 00:38:17,799
Well, I'll just finish up by saying that I think it's it's best to think about these

237
00:38:17,800 --> 00:38:20,800
methods producing results that could be interpreted sort of on their own terms.

238
00:38:21,220 --> 00:38:22,600
You know, if they're sometimes convertible.

239
00:38:24,640 --> 00:38:29,500
And I think in practice, the causal association approach has been particularly amenable to answering questions about treatment,

240
00:38:29,500 --> 00:38:36,129
compliance and surrogate marker measures. We didn't talk about about surrogate markers, but that's another big area recall.

241
00:38:36,130 --> 00:38:45,850
The inference plays a role where you sort of try to understand treatment effects on so-called surrogate markers as a as a proxy for an actual outcome.

242
00:38:46,750 --> 00:38:50,469
And and there well, mediation is kind of a logical thing to do.

243
00:38:50,470 --> 00:38:53,910
You think about the surrogate marker as the mediator value, right?

244
00:38:54,490 --> 00:39:03,129
The treatment to go through the mediator affecting, say, biomarker values that are critical to two outcomes like survival or health.

245
00:39:03,130 --> 00:39:20,200
Does the the sort of fact that you don't in some sense you can kind of be agnostic about this about this mediator outcome confounding and really,

246
00:39:20,200 --> 00:39:26,080
really focus on the value as a as a sort of associative marker, if you will.

247
00:39:28,300 --> 00:39:33,850
That I think gives it gives it some value. I think the compliance issue to some extent, the thing we just talked about,

248
00:39:34,720 --> 00:39:41,350
the way that the fact that you can't really manipulate this potentially really in some settings or that it's going to be hard to think about.

249
00:39:41,350 --> 00:39:48,670
That is another another reason why the causal association approach seems to be better there.

250
00:39:51,280 --> 00:39:58,540
And I guess the last thing is the instrumental variables are essentially sometimes a special case of the causal association approach.

251
00:39:58,540 --> 00:40:03,880
So everything which sort of talked about that kind of falls back into that category,

252
00:40:05,200 --> 00:40:12,700
whereas the causal effect approach I think in some sense is sort of truer to the sort of philosophical or

253
00:40:12,850 --> 00:40:19,360
whether you think about it from a sort of more techno philosophical response or just a very sort of average,

254
00:40:19,360 --> 00:40:30,099
you know, as you think about cause and effect learning as a kid, all of them, all of those things sort of lead back to this causal effect approach.

255
00:40:30,100 --> 00:40:31,179
And I think in particular,

256
00:40:31,180 --> 00:40:39,940
it's a matter of answering questions about policy effects or say a social science or an epidemiological context and to some extent, right.

257
00:40:39,940 --> 00:40:46,150
So in these particularly, we sort of get into these these sort of issues about what happens if we change a policy,

258
00:40:46,750 --> 00:40:52,560
for one thing, that seems much more separately meaningful. And secondly, this idea of direct and indirect effects,

259
00:40:52,630 --> 00:40:58,840
it's probably the more critical to ultimate ultimately underlying science and in a lot of these contexts.

260
00:41:00,010 --> 00:41:12,970
So. Okay. All right.

261
00:41:15,130 --> 00:41:18,550
Anything else before we switch gears and start talking? Survival analysis.

262
00:41:26,710 --> 00:41:30,720
All right. Okay.

263
00:41:33,340 --> 00:41:37,480
So again, it's a brief review about 14 pages.

264
00:41:38,230 --> 00:41:47,110
And what's the whole course? So it's going to be pretty brief. And so survival analysis, the time to event data.

265
00:41:47,590 --> 00:41:54,790
So it has a few special features. One is that it's positive arrow of time goes one direction.

266
00:41:54,820 --> 00:41:59,500
Right. So we're not doing time travel here. So everything's got to be positive.

267
00:41:59,500 --> 00:42:06,670
Valued could be zero. But no, even zero is not really a logical situation here.

268
00:42:07,510 --> 00:42:11,919
Imagine you're tracking something from time zero forward. So none.

269
00:42:11,920 --> 00:42:22,570
None. Not only not negative, but actually positive values are a feature of this data can be approximately continuous, truly continuous data.

270
00:42:22,580 --> 00:42:26,200
Of course it doesn't exist, but it's particularly true here if it hits,

271
00:42:26,470 --> 00:42:37,660
since usually you really are talking about something that's usually measured, sometimes somewhat crudely could be reasonably approximate.

272
00:42:37,660 --> 00:42:48,069
So often in clinical trials we know a day, say a treatment starts, you know, a day of death or so.

273
00:42:48,070 --> 00:42:54,129
And if it's over, a multiyear follow up, that's obviously going to be typically in anywhere from the dozens to the thousand.

274
00:42:54,130 --> 00:43:04,330
So treated as continuous is quite reasonable. It could be more discrete, particularly for doing a sort of a follow up, follow up time.

275
00:43:04,330 --> 00:43:13,990
So maybe like the number of years in a multi-year follow up so you know, maybe from 1 to 10 or 20, it's often centered.

276
00:43:15,790 --> 00:43:21,040
So the total elapsed time isn't known if you're looking at literal survival in patients,

277
00:43:23,140 --> 00:43:31,450
unless it's a really terribly, terribly brutal disease or even a very, very long follow up time.

278
00:43:32,260 --> 00:43:35,200
Not everyone will die during the him during the follow up.

279
00:43:35,980 --> 00:43:42,550
So the sort of bright censoring with the follow up ends before these events is observed is pretty common.

280
00:43:43,420 --> 00:43:47,500
You could have left censoring so the event is observed before the beginning of the follow up.

281
00:43:48,400 --> 00:43:53,680
So sometimes in like chronic diseases, we sort of people are sort of brought into a study in middle age.

282
00:43:55,270 --> 00:44:00,110
So there may be a small percentage of individuals that have already experienced, say,

283
00:44:00,110 --> 00:44:06,550
a heart attack or diabetes, but the majority will be experienced for the first time during follow up.

284
00:44:07,870 --> 00:44:12,920
And then you often have interval censoring and usually this discrete. This is an example of this.

285
00:44:13,270 --> 00:44:23,319
So just to take the diabetes example, maybe every year you're sort of re contacting people in your study and saying,

286
00:44:23,320 --> 00:44:29,740
have you been to have been diagnosed with diabetes or type two diabetes than older population?

287
00:44:30,280 --> 00:44:36,099
And people will say yes. So, you know, it happened sometime during that year, but you may not have a precise date.

288
00:44:36,100 --> 00:44:40,390
And even if you did, it would probably be based on their visit to the doctor.

289
00:44:41,050 --> 00:44:46,120
No time when they clicked past some HNC measure or sugar measure in their play in their bloodstream.

290
00:44:46,900 --> 00:44:52,059
So. So it kind of becomes a discrete taste, that kind of thing.

291
00:44:52,060 --> 00:44:58,060
So. So it's basically that case both left and right sensor.

292
00:44:58,070 --> 00:45:03,430
So you know that it was sort of during some, some known interval.

293
00:45:03,970 --> 00:45:10,630
So after the previous follow up, but before the next follow up.

294
00:45:14,860 --> 00:45:21,940
So censoring can be non informative. The censoring events and the at times are independent or informative,

295
00:45:22,510 --> 00:45:31,090
but the censoring that's correlated with the bedtime so very often non informative and can take a

296
00:45:31,300 --> 00:45:37,090
clinical trial or even a lot of observational studies where the to follow up just happens at some date,

297
00:45:38,950 --> 00:45:40,780
then the censoring would be not informative.

298
00:45:41,590 --> 00:45:46,180
It's important to recognize, I think in my at least in my head, I has had a hard time with this for a long time.

299
00:45:46,720 --> 00:45:55,510
The fact that some people have longer events and that you have a cut off time obviously means the old longer events are more likely to be censored.

300
00:45:55,870 --> 00:45:57,550
That is not informative. Censoring.

301
00:45:58,180 --> 00:46:05,620
Informative censoring is where that where the time in which you cut follow up is is somehow related to that outcome time.

302
00:46:06,670 --> 00:46:11,080
So in the case of administrative censoring, that is where you simply stop a study.

303
00:46:11,740 --> 00:46:18,639
And by definition that's not informative because it's a fixed time for everybody and it cannot be related

304
00:46:18,640 --> 00:46:27,820
to the actual like the following informative settings might occur when you're when you lose people.

305
00:46:29,200 --> 00:46:37,480
So so it could be that there is something about the nature of the people you're following that's harder to follow.

306
00:46:38,050 --> 00:46:45,580
And so you may have them drop out in some sort of fashion, then it ultimately may be tied to their end at that time.

307
00:46:46,870 --> 00:46:51,850
So you can imagine going either way in the diaries example,

308
00:46:52,420 --> 00:46:57,070
people that are more likely to be diabetic may be lower income and have other things that make them more difficult to follow.

309
00:46:58,060 --> 00:47:01,850
Conversely, maybe people that are less likely to be diabetic have, you know,

310
00:47:01,870 --> 00:47:05,410
their health is good and maybe they would have little less incentive to stay involved in the study.

311
00:47:06,100 --> 00:47:13,320
So one can imagine sort of very scenarios there. But but that's the distinction for.

312
00:47:13,720 --> 00:47:18,190
There's a there's a literature on informative censoring, actually.

313
00:47:18,190 --> 00:47:25,540
It's kind of closely related to, to, ah, a lot of the work we've done here, a lot of involves sort of propensity weighting and so forth.

314
00:47:25,540 --> 00:47:31,840
But we're, we're going to take a pass on that and just focus on the point setting here.

315
00:47:35,770 --> 00:47:47,110
So some of the key concepts, there's this idea of the survival distribution and it's basically a cumulative distribution function, but one minus that.

316
00:47:48,370 --> 00:47:55,870
So it's the probability of the actual kind of event occurs after some time to accumulate as a distribution function,

317
00:47:55,870 --> 00:48:00,790
as the probability of an event occurs up to some time.

318
00:48:00,820 --> 00:48:06,910
Capital T survival function is one minus that put this in CDs, right?

319
00:48:06,910 --> 00:48:12,580
They go up from 0 to 1. Survival started over by the time one goes down point zero.

320
00:48:15,850 --> 00:48:23,890
So a hazard function. It's somewhat related to the PDF, but it's not exactly the same thing.

321
00:48:24,700 --> 00:48:31,950
In particular, it's defined as the probability of event occurring essentially at an instantaneous time.

322
00:48:31,960 --> 00:48:38,590
So if we think about the probability so the same event has not occurred at time.

323
00:48:38,590 --> 00:48:42,639
T What's the probability that occurs between time and time?

324
00:48:42,640 --> 00:48:46,540
T Plus delta? T Divided by that delta?

325
00:48:46,540 --> 00:48:51,580
T And you let delta t go down to zero. So very much like it's a derivative essentially.

326
00:48:54,580 --> 00:49:03,070
And so that it's essentially a derivative of the log of this survival function.

327
00:49:04,120 --> 00:49:15,100
Or if your pdf of the underlying generating time two event measure is continuous and that's a pdf divided by one minus KDA.

328
00:49:16,660 --> 00:49:19,739
I'm not going to show will is not going to make this class.

329
00:49:19,740 --> 00:49:26,790
I'm just getting this, as a matter of fact. But I'm going to see, I think maybe a little little bit of the concept behind that.

330
00:49:29,660 --> 00:49:32,840
And so the cumulative hazard function, right?

331
00:49:32,900 --> 00:49:38,180
If we imagine this hazard function, it's cost that over time or maybe it varies over time,

332
00:49:39,380 --> 00:49:49,910
but we integrate that up from time starting at zero up to t sort of a measure of the overall risk during this time period.

333
00:49:51,260 --> 00:49:53,809
So obviously, this is constant.

334
00:49:53,810 --> 00:50:04,850
This is just going to be that whatever value that is times t if it's a function, then you sort of get an area under that triangle, right triangle.

335
00:50:04,850 --> 00:50:11,360
If it's non-linear, then you have some more complicated integral calculus to do.

336
00:50:12,380 --> 00:50:24,770
And so. But we also think about this hazard in terms of the lot of the survival function.

337
00:50:27,650 --> 00:50:31,700
Then essentially we're just looking at this this.

338
00:50:35,130 --> 00:50:43,440
This integral of this of of the law of and so by the fundamental theorem the calculus, we just get right back the log this of t.

339
00:50:45,060 --> 00:50:50,600
Right. Integrating the derivative gives us the original function. So.

340
00:50:52,880 --> 00:50:59,240
So this beautiful country can started with this. Okay.

341
00:50:59,250 --> 00:51:03,030
So I already said this, right?

342
00:51:04,440 --> 00:51:09,630
This had a function. Instantaneous rate of hazard is a sort of total rate of experience in the event.

343
00:51:11,070 --> 00:51:14,230
You can relate these expressions together. Right.

344
00:51:14,280 --> 00:51:23,430
So if the cumulative hazard function is the negative of the log of the survival function, the survival function is the.

345
00:51:26,040 --> 00:51:34,640
Exponentially ation of the negative with the cumulative hazard. Similarly of.

346
00:51:36,590 --> 00:51:46,129
Right. We're already in this hazard. This form here. I can write the expected value of T given the T as it occurred by time.

347
00:51:46,130 --> 00:51:57,450
Little T is. Essentially the the integral right expected value under this.

348
00:52:00,940 --> 00:52:12,980
Rate the expected value of. Of team air conditioning on TV and rated the little T over one minus the CTF.

349
00:52:13,310 --> 00:52:26,090
Right. So that's just a survival function which I can then rewrite like this, replacing F with my hazard function and my survival function.

350
00:52:26,510 --> 00:52:37,250
So that's how I will function. Evaluated a T and then the symbol of respect to you from t little t to infinity.

351
00:52:39,400 --> 00:52:47,550
In terms of hazard, in terms of survival. So we generally use two approaches.

352
00:52:48,570 --> 00:52:53,640
One is as parametric and one is as what's called semi parametric.

353
00:52:54,930 --> 00:53:00,809
So accelerated filter time model basically models.

354
00:53:00,810 --> 00:53:10,260
The log of the time to event is some linear function of covariance and then some parametric error term could be normal.

355
00:53:10,440 --> 00:53:21,030
Right. So you get these log normal type distributions. We'll see a weibel type function can also be used here and there's some sort of scaling factor.

356
00:53:21,600 --> 00:53:28,020
We call it gamma. So this would be sort of a known distribution like normal zero one.

357
00:53:28,920 --> 00:53:32,309
This would then be like a sigma, the sort of variance,

358
00:53:32,310 --> 00:53:40,680
and then another situation and then with some shift up or down as a linear combination of coverage.

359
00:53:41,670 --> 00:53:47,400
And that's the log of the time to event. Right. So actually it's quite, quite skewed there when you're back on the actual kind of event scale.

360
00:53:49,420 --> 00:53:50,710
So proportional hazards.

361
00:53:52,210 --> 00:54:06,250
So this is sometimes called the Cox model because David Cox in 1972, I think maybe the single most cited paper in biostatistics developed this idea,

362
00:54:08,020 --> 00:54:17,350
basically noted that you sort of could go back to the accelerate figure time models and maybe you really didn't need to specify the hazard itself,

363
00:54:18,220 --> 00:54:25,570
although it is it is is formed here so you can rewrite these models into this form and then you just don't care about this.

364
00:54:25,570 --> 00:54:38,049
You can really take on any, any form and then you just estimate which you you can then set up a likelihood, you just based on this component.

365
00:54:38,050 --> 00:54:45,970
But there's still this sort of linear predictor that on the log scale, the hazard is still a function of covariance,

366
00:54:46,660 --> 00:54:51,760
but that we don't need to do a sort of baseline hazard to estimate that directly.

367
00:54:53,980 --> 00:54:57,250
Now can be that these things can be put together in a particular way.

368
00:54:57,250 --> 00:55:04,809
The model shortly is this is a proportional hazards model, not all salaried field time models.

369
00:55:04,810 --> 00:55:09,370
And in fact, the only the way we want to really takes on that this is proportional hazards for.

370
00:55:09,850 --> 00:55:20,210
So the walk model is not apportionment as it's more. So I guess we'll see it on the next slide.

371
00:55:21,230 --> 00:55:31,130
So in the error term, in this accelerated period, time model takes on this extreme value, extremely value that should be extreme value.

372
00:55:34,600 --> 00:55:48,580
It's extremely difficult and even more extreme. The street value distribution function.

373
00:55:49,000 --> 00:55:58,540
Right. Which is given by this here. So this is this is sort of the standard standardized extreme value distribution function.

374
00:56:00,010 --> 00:56:04,930
The resulting distribution of times is okay falls away with distribution.

375
00:56:06,660 --> 00:56:10,810
So. Particular.

376
00:56:12,700 --> 00:56:30,480
The if you integrate this. Up to some value t right to get to the CDF, then you're left with one minus either the negative t of the gamma times.

377
00:56:33,430 --> 00:56:38,379
Theta II for the log of Theta II is a so transpose boner.

378
00:56:38,380 --> 00:56:42,740
Theta is going to be this quantity. Right.

379
00:56:42,800 --> 00:56:45,800
Well, SARS is just one minus F, so we have this.

380
00:56:48,230 --> 00:56:53,210
We take the log of the survival function and we just get this quantity here.

381
00:56:55,370 --> 00:56:57,590
And if we differentiate this with respect to T,

382
00:56:59,000 --> 00:57:14,809
we get Gamma Sigma i t to the gamma minus one or gamma t to a t to the gamma minus one here the excite transpose beta, right?

383
00:57:14,810 --> 00:57:18,200
And so this is proportional hazards. Monarch Right. If we sort of remember our definition,

384
00:57:19,220 --> 00:57:28,129
the point is we got to get all of our coverage into this heated linear combination and then of stuff that doesn't have covariance,

385
00:57:28,130 --> 00:57:31,310
which corresponds to a so-called baseline hazard.

386
00:57:31,400 --> 00:57:38,780
Right? So this is set to zero then then this proportional hazard model.

387
00:57:41,300 --> 00:57:46,000
It takes on the school here. So there's an important point, which I probably should have written on the slides,

388
00:57:47,050 --> 00:57:53,470
but for this to be identified, you don't want intercepts in these axes.

389
00:57:54,880 --> 00:58:03,040
Right. Because if you had if this was just an intercept only model, you would essentially be aliasing with gamma here.

390
00:58:04,870 --> 00:58:10,060
Right. Whatever. Either the beta was, either the bait or not was.

391
00:58:10,810 --> 00:58:14,170
You could make it bigger. Maybe even better if you needed beta.

392
00:58:14,170 --> 00:58:18,340
Not bigger. Yeah, maybe smaller. And you'd still get exactly the same value for the hazard.

393
00:58:19,150 --> 00:58:28,930
So. So if you take. If there's no intercepts and there's no covers, then this is just the this is just the proportional hazards model.

394
00:58:29,080 --> 00:58:35,950
You start to have axes that are different than. Right. So some values of X's take on different, different values.

395
00:58:36,430 --> 00:58:40,530
Then you're in a longer alias, you're able to estimate beta.

396
00:58:40,720 --> 00:58:44,530
So. So, no. No intercepts.

397
00:58:45,220 --> 00:58:48,550
The same thing is true for the for the Cox model itself.

398
00:58:50,100 --> 00:59:01,919
So there's no interception in the Cox model. So kind of zooming along here pretty quickly.

399
00:59:01,920 --> 00:59:05,309
Any questions if you've had to write an analysis before?

400
00:59:05,310 --> 00:59:10,860
I apologize. This is all extremely review. If you haven't, maybe a little confusing, but.

401
00:59:17,800 --> 00:59:22,000
Okay. So as I mentioned, we often have these sensor data situations.

402
00:59:24,480 --> 00:59:27,900
Right. So may I observe tea?

403
00:59:27,900 --> 00:59:32,970
And then delta with tea is not the survival time always.

404
00:59:33,780 --> 00:59:37,050
But it could be a censoring time because it's a minimum of these two. Right.

405
00:59:38,070 --> 00:59:44,700
So if you've the event hasn't occurred yet, then you can.

406
00:59:44,700 --> 00:59:48,420
And all you can do is observe that it didn't happen by some time. You.

407
00:59:51,510 --> 00:59:58,200
Conversely, if it occurs and if you do see it, then the censoring time was later.

408
00:59:59,250 --> 01:00:04,350
Right? So you have no way of of mean. So it's basically going to be the minimum of these two values.

409
01:00:05,940 --> 01:00:09,150
So and then a censoring indicator.

410
01:00:09,180 --> 01:00:14,580
Right. So basically. Right.

411
01:00:14,590 --> 01:00:18,720
So I guess I said this were Delta is one that's not censored, is reserved.

412
01:00:19,050 --> 01:00:23,010
And then Delta zero, that's censored. So.

413
01:00:24,830 --> 01:00:30,680
Okay. So this is typical and this is also true that you feed data into a.

414
01:00:35,340 --> 01:00:39,120
Statistical package, there will be your estimation of your survival model.

415
01:00:39,360 --> 01:00:46,470
You've got to tell for each observation whether what it's getting as time as the actual theater time or the sensory time.

416
01:00:48,370 --> 01:00:52,020
Your feet in the middle of movies? Well, yes, by definition, that's what your observing.

417
01:00:55,410 --> 01:01:06,180
Okay. So. All right, so the sex and you are independent and what what how am I getting that assumption that we call that?

418
01:01:10,850 --> 01:01:15,140
Syria. Yes. Right. So this was this idea back up here.

419
01:01:20,230 --> 01:01:27,790
But censoring times in bad times are independent. So if we get that, then we get a fairly manageable likelihood.

420
01:01:30,210 --> 01:01:41,940
Right. So. So the event occurred at time.

421
01:01:41,940 --> 01:01:47,790
T right. I just get the pdf for that observation.

422
01:01:48,000 --> 01:01:52,440
It didn't occur by time t then I know it's greater than that.

423
01:01:52,440 --> 01:02:05,200
Or one minus the ctf. So Delta's one observed with Delta zero it censored and the people I do this and the history briefly.

424
01:02:15,410 --> 01:02:20,000
To get to the final equation. They're right.

425
01:02:20,240 --> 01:02:25,970
One -50, which is my survival time.

426
01:02:27,610 --> 01:02:34,610
I'm sure I can write X minus the the cumulative hazard.

427
01:02:38,170 --> 01:02:50,090
And so. All right.

428
01:02:50,150 --> 01:02:59,720
So my PDF review of my CTF on my as my survival function.

429
01:03:03,230 --> 01:03:20,920
She's. All right.

430
01:03:20,920 --> 01:03:25,120
So basically I differentiate this.

431
01:03:25,120 --> 01:03:28,270
So this goes away. I got a negative sign here.

432
01:03:28,900 --> 01:03:35,290
So the derivative of E to something that's just here's something kind of to review that inside these.

433
01:03:49,130 --> 01:03:56,380
Sorry. There. That's right.

434
01:03:56,890 --> 01:04:01,330
Minus for this. This comes down here and it's really that.

435
01:04:14,510 --> 01:04:26,730
Oh, sorry. Right. That's right. So those things cancel and I'm just really like using the hazard as my actual hazard.

436
01:04:43,720 --> 01:05:03,740
So this gives this vagueness discounts on you. So basically then when I raised this and power won over Delta, I get a.

437
01:05:08,890 --> 01:05:17,770
Hazard raised in the delta i. I get an X or negative of the distribution raised to the.

438
01:05:25,050 --> 01:05:31,080
The win over Delta I. And so those things.

439
01:05:32,930 --> 01:05:40,450
Combine. So that the Delta II plus one of the Delta equals one.

440
01:05:40,600 --> 01:06:05,630
You got cut back. So that's for the full salary fearless man models where right have this full distribution.

441
01:06:06,950 --> 01:06:17,450
Then for my personal hazards models, I really only care about beta.

442
01:06:24,590 --> 01:06:34,760
I just ended up with this year the exit transpose beta and then crucially divided by the

443
01:06:34,760 --> 01:06:42,080
sum of the index j transpose beta where I'm summing over all of the j in a risk set.

444
01:06:42,530 --> 01:06:53,210
The ones that are still at risk for which the event has occurred by time t supply and then multiply that up by all of the observations.

445
01:06:55,880 --> 01:07:05,790
Have. An estimation point, estimation, variance estimation.

446
01:07:05,970 --> 01:07:14,130
We do score equations, we do fisher information, so they give us a second derivative score.

447
01:07:16,540 --> 01:07:20,290
And again. But actually survivor class. I would do this, but it's not.

448
01:07:24,330 --> 01:07:28,500
Okay. So that's. But you're running the software.

449
01:07:28,980 --> 01:07:31,350
That's what it's doing. And that's how it's getting these results.

450
01:07:36,670 --> 01:07:41,860
So I want to talk a little bit too about discrete time two event models just because I tend to actually like them a lot.

451
01:07:41,920 --> 01:07:55,630
And also it's a when we get into some discussion of the problem of using hazard hazard ratios and causal inference,

452
01:07:56,380 --> 01:08:06,040
this will be one way to kind of work out of it. So as I mentioned, you know, many cases data can be left and right truncated.

453
01:08:06,160 --> 01:08:09,340
So we only know the event happened within a given time period.

454
01:08:11,770 --> 01:08:16,390
Since the interval essential data setting. Now nearly all data really is of this form, right?

455
01:08:16,870 --> 01:08:21,849
I mean that we have the day of death that might be continuously well approximated of.

456
01:08:21,850 --> 01:08:25,929
The follow time is long, but it has some advantages for estimation,

457
01:08:25,930 --> 01:08:29,110
even if the follow up time has to be coarsened to fit into a discrete time setting.

458
01:08:33,050 --> 01:08:42,080
So so the basic the basic concept here is that you can basically trick.

459
01:08:45,220 --> 01:08:56,640
The likelihood from a discrete time event model to become a generalized, linear, mixed model, either basically for a any any kind of binomial outcome.

460
01:08:56,650 --> 01:09:08,730
And you can sort of pick, pick your linked function. So if we treat each time tea as having its own constant hazard risk hazard.

461
01:09:08,760 --> 01:09:12,050
So. So this alpha t is proportional.

462
01:09:12,060 --> 01:09:16,310
You could have proportional odds model that basically says the probability of an

463
01:09:16,310 --> 01:09:24,020
event occurring at time t is going to be given by the log odds of that event.

464
01:09:24,170 --> 01:09:27,740
Right. It's going to be given by.

465
01:09:34,550 --> 01:09:43,700
This poverty plus exile, Peter. And then for uncensored individuals.

466
01:09:43,790 --> 01:09:51,260
So we know this event occurred at time t against time being here not continuous but this sort of window slice.

467
01:09:52,880 --> 01:09:56,600
So we know it didn't happen before time t minus one did it happened to T.

468
01:09:57,500 --> 01:10:06,020
So this overall probability is you can work out is the conditional probability of occurring time t given that didn't occur at time t minus one.

469
01:10:06,740 --> 01:10:08,180
And then since it didn't occur at time,

470
01:10:08,180 --> 01:10:19,520
t minus one occurred time t minus two all the way down and out occurring now sort of times less one basically that's given by this h of t times,

471
01:10:19,520 --> 01:10:24,290
one minus these and therefore sensor individuals at time t.

472
01:10:25,250 --> 01:10:28,730
Well, basically, that's exactly the same thing up here.

473
01:10:29,360 --> 01:10:33,019
You now have this conditional probability of not occurring at a time t given that didn't occur at time.

474
01:10:33,020 --> 01:10:45,620
T minus one. So we got to put this likelihood together.

475
01:10:48,270 --> 01:10:56,190
Right. So we basically have the probability that occurs at time t surviving times,

476
01:10:56,190 --> 01:11:05,380
the probability it didn't occur at all the previous time slices for the 74 which is observed to occur and then for the censored subjects,

477
01:11:05,400 --> 01:11:13,080
it's basically one minus the probability over all the time slices to whatever the t the eighth place was observed.

478
01:11:14,700 --> 01:11:17,519
I should mention here this also allows for different follow up times where we don't

479
01:11:17,520 --> 01:11:21,990
necessarily assume every individual has the same number of windows observing.

480
01:11:24,290 --> 01:11:32,119
So so the trick is to note this is equivalent to a logistic regression where the outcomes are zero unless the event occurs at time t,

481
01:11:32,120 --> 01:11:33,080
in which case it's one.

482
01:11:34,220 --> 01:11:42,740
So basically you have a string of zeros and one for uncertain individuals and a string of zeros, complete zeros for sensitive individuals.

483
01:11:45,290 --> 01:11:59,750
Right. So basically you sort of switch the outcome from T to this Y by J, which runs from one to T survive and then.

484
01:12:04,690 --> 01:12:09,050
We have. One minus eight.

485
01:12:10,090 --> 01:12:16,090
Right. So these are all going to be zeros here by definition. And then if it occurs.

486
01:12:18,820 --> 01:12:22,210
We get this. And if it doesn't, we just have these other pieces.

487
01:12:27,370 --> 01:12:34,120
So basically then the standard logistic regression models was up to each observations for each

488
01:12:34,120 --> 01:12:47,500
individuals can then be fit where we have a vector of dummy variables for this t time interval and then.

489
01:12:50,930 --> 01:12:56,030
The vector of or actual covariance of interest when using alpha.

490
01:12:56,150 --> 01:13:03,050
These in some sense are kind of nuisance parameters. So this can be thought of as essentially estimating.

491
01:13:04,190 --> 01:13:11,270
A if we go back to the proportional, proportional model, this is sort of the baseline hazard, right?

492
01:13:12,590 --> 01:13:18,800
Bouncing along over time and then we have some slip up or down when we when we change X.

493
01:13:21,150 --> 01:13:25,620
So this is a very, very robust model here.

494
01:13:25,620 --> 01:13:31,110
I mean, literally it's sort of non parametric on the baseline hazard.

495
01:13:32,160 --> 01:13:37,410
You could be a bit more parametric by using, say, a spine model or even maybe a low dimensional polynomial.

496
01:13:37,430 --> 01:13:46,140
But follow up times are small. But but usually you don't there's there's not a great reason to do that unless you have a pretty modest amount of data.

497
01:13:47,190 --> 01:13:55,660
So. This generally holds to those who have just a few observations within the time window of you want to collapse over that period.

498
01:13:56,080 --> 01:14:06,430
So there are some sort of fiddly things you can do there, but in many cases it's just using a simple vector of dummy variables to explain.

499
01:14:08,050 --> 01:14:13,060
So then the interpretation of the Russian coefficients is the change in the log odds of an instantaneous event occurring.

500
01:14:17,640 --> 01:14:24,150
So because this law, Gosling is not a personal hazards model and there's not really a simple relationship between the

501
01:14:24,150 --> 01:14:28,110
baseline hazard to the survival functions and the critical functions for the level of the covariate.

502
01:14:29,070 --> 01:14:33,120
So I fear there's occasionally a sort of advocate for using the SO got this

503
01:14:33,210 --> 01:14:39,870
complimentary log log link which basically takes the log of one minus event occurring,

504
01:14:40,200 --> 01:14:41,850
takes the negative of that and all of that.

505
01:14:43,140 --> 01:14:49,740
So this really starts to look more actually as a flavor of a label model and thus is a proportional hazards model.

506
01:14:50,700 --> 01:14:57,600
Right. It sort of has a sort of extreme value, kind of inverse of extreme value function approach.

507
01:14:58,740 --> 01:15:02,820
So basically now, you know, you get a personal hazards model.

508
01:15:03,720 --> 01:15:09,720
So the dummy variables are fine for time and for the baseline hazard and you get this exponential piece.

509
01:15:10,860 --> 01:15:16,409
So we actually implement this an example going to look at, it's not critical which one we look at because we're actually moving away from the hazards.

510
01:15:16,410 --> 01:15:24,810
But if you want to estimate hazards using a discrete time event model in this work link is kind of a good idea.

511
01:15:28,080 --> 01:15:30,559
So I should add, I didn't talk much about time during coverage.

512
01:15:30,560 --> 01:15:40,230
It's a bit of a pain in the in the continuous models, but then it's a great time to get models that are really easy to include.

513
01:15:41,340 --> 01:15:49,080
You just let the cover. It's very for each line of data corresponding to a given follow up and it's easy to implement using existing software.

514
01:15:52,680 --> 01:15:57,750
So I know we're close to time. Can I have five more minutes just to finish up?

515
01:16:00,340 --> 01:16:05,250
The Government's held back on this. So the one the one little piece here is you need to convert.

516
01:16:08,100 --> 01:16:11,910
The longer form to actually use this bit of these generalized.

517
01:16:35,380 --> 01:16:43,090
So let's suppose our original data as an IP based on various.

518
01:16:47,740 --> 01:16:58,000
Follow up time and then our sensory indicator because a policy is going to sensor.

519
01:17:03,260 --> 01:17:09,380
Otherwise. So I've got to say four observations.

520
01:17:13,840 --> 01:17:31,330
What I have here is members and I have this file at times and again, these are going to be integers as screen time periods and so observed.

521
01:17:35,270 --> 01:18:01,740
Center. So we can track this data down essentially with a row for every follow up to a data.

522
01:18:12,940 --> 01:18:15,940
And then my ex just gets copied over baseline.

523
01:18:27,590 --> 01:18:34,520
One variable, it's always going to be zero. So my last couple of time and it's one of the observed.

524
01:18:37,730 --> 01:18:43,639
So. Jerry 001 and I had one follow up one too, and it was censored.

525
01:18:43,640 --> 01:18:52,070
So that's a zero that is two are there is and then I have one below zero because of since.

526
01:18:54,970 --> 01:19:04,800
I can understand that something like Joanne affects the family.

527
01:19:06,960 --> 01:19:17,600
I don't know. So that'll give me a lot of odds.

528
01:19:21,090 --> 01:19:25,620
As a great show is going to be needed to figure out what's going to be better access.

529
01:19:29,100 --> 01:19:40,390
It's going to be better here. And then the hazard ratio versus back.

530
01:19:44,800 --> 01:19:52,760
This. If I use the ceiling.

531
01:20:08,900 --> 01:20:12,060
In. Because you like log.

532
01:20:16,230 --> 01:20:34,100
Then. I just need to expend a creative education for this kind of correspondence.

533
01:20:48,960 --> 01:20:52,980
A time period for various things. Things change over time.

534
01:20:58,900 --> 01:21:05,690
Yeah. So that's it. And you just run into that statistic, just put it in to your, your favorite software.

535
01:21:16,230 --> 01:21:25,290
So all right. So we'll go on to sort of putting this into the sort of causal inference setting we get back from today.

536
01:21:25,740 --> 01:21:33,990
I will have office hours today. I want to have them on Thursday and we'll have a good few days off.

537
01:21:33,990 --> 01:21:37,350
I'm sure you all can use it. I know I certainly can.

538
01:21:39,270 --> 01:21:41,250
So. Right.

