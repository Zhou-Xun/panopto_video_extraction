1
00:00:03,290 --> 00:00:06,610
Yeah. It's important.

2
00:00:07,440 --> 00:00:13,250
You're looking for leadership.

3
00:00:16,980 --> 00:00:27,270
All right, go ahead and get started today.

4
00:00:28,210 --> 00:00:32,040
Maybe just the first lady.

5
00:00:32,040 --> 00:00:37,910
Maybe I'll just give you a sample.

6
00:00:37,920 --> 00:00:43,170
Just so I know how to read data and our research, that's kind of our topic.

7
00:00:43,800 --> 00:00:52,500
I have this CSV file I'm using for some of the lectures.

8
00:00:52,920 --> 00:01:05,850
I think there's also our package that has this data, but you can also download it from the one on the side of the files and the data.

9
00:01:05,870 --> 00:01:23,880
So if you want to load it into our biggest source plus solution, we download it and see.

10
00:01:26,120 --> 00:01:36,590
I think it's just in the downloads folder. You can see we can see here that it's in the downloads folder.

11
00:01:36,630 --> 00:01:40,550
Now, if you go to our studio if you want to.

12
00:01:45,000 --> 00:01:50,280
You are still the most important. I mean, you can do read that CC directly.

13
00:01:51,930 --> 00:01:55,870
It's just easier to click on this more data set.

14
00:01:56,640 --> 00:02:04,650
But in my opinion and then there's also the question of multiple options.

15
00:02:04,650 --> 00:02:15,270
I mean, for a CSP or DOT text file or a dot for those types of files, those are just considered from text.

16
00:02:16,470 --> 00:02:26,129
Any of those types of files. They also have options for importing from Excel, Excel spreadsheet or or SAS or state.

17
00:02:26,130 --> 00:02:35,640
And but just like these CSP files are just considered text others to worry about for

18
00:02:35,640 --> 00:02:41,160
now these are just do basically two different functions to read in a text file.

19
00:02:41,670 --> 00:02:47,750
So just to base. And then we saw that it was in downloads and then this is what I,

20
00:02:47,890 --> 00:02:56,220
I just downloaded like it will get mine underscore for the CSP file and then something like this pops up.

21
00:02:56,910 --> 00:03:01,920
So this is like what the raw file looks like on top to input file.

22
00:03:05,640 --> 00:03:11,100
So it's just kind of all the data. Everything is separated by commas and then kind of below that.

23
00:03:12,720 --> 00:03:24,290
It's kind of hard to. Almost any political party can.

24
00:03:40,540 --> 00:03:45,730
Things are important, but yeah, so let's zoom out.

25
00:03:45,750 --> 00:03:54,160
And that's kind of at the bottom is what the data set will look like for you when you merge into when you loaded into bar.

26
00:03:54,460 --> 00:04:01,540
Right. So this is kind of the raw file. This is going to look like the already are I think, you know,

27
00:04:01,540 --> 00:04:13,000
the heading or not adding just means usually that means you include kind of the first the first line has the variable names in your dataframe,

28
00:04:15,850 --> 00:04:26,550
which is usually the if the if, if kind of the columns of your data are labeled in your CSV file, you usually want to do that heading.

29
00:04:26,590 --> 00:04:36,490
Yes, that just means kind of the first row of text in your CSV file should be the variable names of the data frame.

30
00:04:36,700 --> 00:04:46,330
So you can see that here. This is what it looks like that you import that's big enough that you can see here.

31
00:04:46,750 --> 00:04:55,750
It just runs kind of the CSP automatically read that CSP with the right file path.

32
00:04:56,860 --> 00:05:01,180
So it does the same thing. It's just like it's a little bit more convenient, kind of.

33
00:05:02,840 --> 00:05:10,700
Point. Good way to load in your data. And then yeah, there's also this command once you do well for any data frame.

34
00:05:13,030 --> 00:05:22,730
So if you do view this function view with a capital V that allows you to view the data frame,

35
00:05:22,750 --> 00:05:30,230
so it'll, it'll view the data frame in a separate, separate window right here that'll pop up.

36
00:05:30,800 --> 00:05:33,840
And so you can kind of scroll. Scroll.

37
00:05:34,150 --> 00:05:43,930
And so these are the variables like six variables in the data frame and thousand 700 observations.

38
00:05:43,930 --> 00:05:54,100
We can kind of just, you know, scroll around and see what the data frameworks like you can use the view function here with the capital.

39
00:05:54,100 --> 00:05:57,250
V Okay. So let's just.

40
00:06:05,990 --> 00:06:11,540
That's just one way to read. In this case, we file into our studio.

41
00:06:11,540 --> 00:06:14,660
Again, this is probably the most convenient way to do that.

42
00:06:17,960 --> 00:06:21,130
Okay, so let's just ask you to.

43
00:06:23,880 --> 00:06:41,550
The presentation of the meeting and their assignments of the second homeworks do this Friday in the second online quizzes two on 26 looks like Monday.

44
00:06:42,750 --> 00:06:54,000
So I'll try to we'll post the final assignments later this week and then I'll do mine either Friday or Saturday of next week,

45
00:06:56,370 --> 00:07:00,570
September 30th or so, or something like that.

46
00:07:03,810 --> 00:07:09,680
All right. So I guess so today we'll just finish up talking about reading and data files.

47
00:07:09,690 --> 00:07:13,140
I don't think that much there's not that much left to say.

48
00:07:13,290 --> 00:07:22,559
And then if the rest of the lectures, once, you know, people comp data wrangling,

49
00:07:22,560 --> 00:07:35,750
it's just kind of different ways of setting your data, transforming your data and finding variables and things like that.

50
00:07:35,760 --> 00:07:46,650
Just kind of, I guess data, data manipulations, it allows you to do the analysis you want the idea there.

51
00:07:47,790 --> 00:07:51,980
So I think yeah, I think this is actually related to the example.

52
00:07:51,990 --> 00:08:04,440
I just if you want to try this job reminder data set yourself, you can download it from the canvas site, you can do a read on CSP like we just did.

53
00:08:04,440 --> 00:08:13,500
I mean, you can type it indirectly or you can do, I think our studio, if you kind of do what I just did, just kind of.

54
00:08:15,520 --> 00:08:18,940
If you know where the folder that it's located is in is,

55
00:08:19,690 --> 00:08:26,080
just click this import data sent button and you can browse the different folders and click the,

56
00:08:29,230 --> 00:08:33,730
you know, select the the name of the file that you want to import.

57
00:08:33,730 --> 00:08:38,650
And then we can this, this import dataset panel will pop up.

58
00:08:38,650 --> 00:08:45,160
And so it's just kind of a convenient way of importing the CSP file in our studio.

59
00:08:48,700 --> 00:08:53,050
Yeah, the read table function is basically more or less the same.

60
00:08:53,680 --> 00:08:56,889
That's really not since we are mostly the same.

61
00:08:56,890 --> 00:09:08,950
It just, uh. It can just handle other text files, I guess more general types of text file.

62
00:09:08,950 --> 00:09:18,429
So we have a single file called the extension dot text or a dot TSB tab, separated files.

63
00:09:18,430 --> 00:09:29,649
Those those types of things you could do is you can use read that table for any of those, but you can't even use it for a redone CSP.

64
00:09:29,650 --> 00:09:35,950
Sometimes you have to specify the separator yourself.

65
00:09:35,950 --> 00:09:44,140
So separator just means the the character that separates like the individual observations.

66
00:09:44,650 --> 00:09:56,110
So like in a CSP file. So that's a common set of values that is, that stands for the separator, there is a comma.

67
00:09:57,760 --> 00:10:04,420
So if you have another type of separator between the observations, you can, you can use that.

68
00:10:08,080 --> 00:10:14,830
So like that. Yeah, I will mention this again.

69
00:10:14,830 --> 00:10:19,209
I mean, I've said this before. If you want to see the dimensions of your data,

70
00:10:19,210 --> 00:10:24,580
you can use the demo function that tells you that it's 1704 observations with six

71
00:10:24,580 --> 00:10:30,399
variables and it just function as you just want to see kind of the first few rows.

72
00:10:30,400 --> 00:10:39,280
You can actually give it a by default if they get prints out the first six rows of the data frame that you can as a second argument,

73
00:10:39,280 --> 00:10:44,290
you can give it a number if you want to look at or rows or fewer rows.

74
00:10:46,360 --> 00:10:49,650
So if you want to look at the first eight rows, you just put the second arguments.

75
00:10:49,720 --> 00:10:55,000
Eight if you want to look at the first few values of.

76
00:10:55,920 --> 00:11:08,129
Certain variables, you can you can basically access all of the components of a data frame, as you would with a matrix or for individual variables.

77
00:11:08,130 --> 00:11:20,700
You can access the components of the variables the same way you would have that calendar dollar sign country think, get that as a factor.

78
00:11:20,700 --> 00:11:25,050
It's basically it's a fact. It's kind of a factor factor if you look at it.

79
00:11:26,280 --> 00:11:44,509
I think actually. Well now that they actually might be any characters actually since they were actually before they changed the default for 3D table.

80
00:11:44,510 --> 00:11:49,880
So it's actually a character. You can see that here. If we do ACR, I've got minder.

81
00:11:50,240 --> 00:12:02,150
Afghanistan is a not Afghan. The country variable is a it's a character very much that in fact we have another

82
00:12:03,020 --> 00:12:08,330
character variable which is continent and then we have a few other numeric.

83
00:12:09,830 --> 00:12:17,360
So my life expectancy in GDP per capita numeric urine in population are also numeric.

84
00:12:17,360 --> 00:12:21,350
But they did have a treated it was rated as an integer.

85
00:12:28,220 --> 00:12:42,140
Yeah. So again this is based they just recently changed the default frequency has been read that table so this is saying this would read it in as a

86
00:12:42,740 --> 00:12:55,700
character but it's that's the default anyway so it's it's the same thing as it was before this is it's country variable is a is a character variable.

87
00:12:58,930 --> 00:13:00,749
The other thing I just mentioned,

88
00:13:00,750 --> 00:13:10,920
one or two other useful things to note is sometimes this is useful for the skip kind of argument for reading that table or or reading

89
00:13:10,920 --> 00:13:22,380
got CCD that just allows you to if you want to skip a certain number of rows in your text file before you start reading it in,

90
00:13:23,310 --> 00:13:26,730
that allows you to do that. So in this example, I did it.

91
00:13:27,090 --> 00:13:37,250
I skipped the first row. So the first row just has the variable names, so it's not going to read in the variable views.

92
00:13:37,260 --> 00:13:43,350
And then I did header equals false, so it doesn't treat the first row as variable.

93
00:13:44,730 --> 00:13:53,550
So what's that? What that's going to do is to skip the first row and then not automatically put the first row as variable name.

94
00:13:53,560 --> 00:14:02,900
So it's basically. It basically just starts from the first the first row of real data, I guess,

95
00:14:04,010 --> 00:14:14,989
and it ignores the variable names as a default kind of retrieval will automatically assign the variable names of V1,

96
00:14:14,990 --> 00:14:31,760
V2, V3, etc. If you wanted to skip the header like the variable names and the date, the original data file and skip the first two lines.

97
00:14:31,760 --> 00:14:35,690
I guess of data you could do skip equals three. So you can see that here.

98
00:14:37,770 --> 00:14:43,660
It says kind of the first two rows from leaving the above example missing Skip.

99
00:14:44,230 --> 00:14:52,190
Skip just allows you to skip a certain number of rows in the original CSV or text file

100
00:14:52,670 --> 00:15:02,220
before it starts loading that into the data frame you've assigned it to another.

101
00:15:02,240 --> 00:15:06,920
So the name of the data frame here is. DF. So that's the.

102
00:15:08,090 --> 00:15:12,169
It's a risky argument. Yeah.

103
00:15:12,170 --> 00:15:22,309
The only other thing I'll mention is that you can, like,

104
00:15:22,310 --> 00:15:32,530
re read in the text files that are stored online if they're kind of in the format of a CC or a dot text file.

105
00:15:32,560 --> 00:15:38,870
So I think this example here, it's actually stored as a data extension, but it's just a.

106
00:15:47,890 --> 00:15:52,950
If you if you open it up. I think it's just it has the format of just a text file with.

107
00:15:57,800 --> 00:16:03,110
Empty an empty space between the observations.

108
00:16:04,490 --> 00:16:10,550
So, um, so I just.

109
00:16:10,560 --> 00:16:16,100
Yeah, I just put. You can just. You can just put the URL here directly.

110
00:16:16,220 --> 00:16:23,720
But I kind of didn't have space, so I just basically, you know, pasted these two terms together.

111
00:16:25,370 --> 00:16:29,540
So here, it works fine. So this sub is not.

112
00:16:30,260 --> 00:16:35,450
This is related to this the pasting of these two things.

113
00:16:35,460 --> 00:16:39,260
It's not it's not related to the separator and the data file.

114
00:16:39,260 --> 00:16:43,370
It's just it's related. It's related to the paste function.

115
00:16:43,370 --> 00:16:54,650
I'm just trying to create the URL that doesn't have any spaces between the slash an element and so Statler I guess.

116
00:16:54,740 --> 00:16:59,450
Okay. But you can write the full URL name directly.

117
00:16:59,450 --> 00:17:03,080
It just doesn't fit on the slides. I just did it this way.

118
00:17:03,890 --> 00:17:15,049
Okay. And so I think it was able to interpret the separator automatically between different characters, different observations.

119
00:17:15,050 --> 00:17:21,200
And then I did header equals true so that it read in the first line as the variable name.

120
00:17:21,210 --> 00:17:24,980
So this. So this should work automatically.

121
00:17:24,980 --> 00:17:29,450
So now it's bound to is a data frame.

122
00:17:29,450 --> 00:17:33,620
And if you print out kind of the first few rows, it looks like this.

123
00:17:33,830 --> 00:17:49,190
Okay. So you can you can read at least, at least text files directly from an online source that if they're saved as kind of a separate text text file.

124
00:17:49,580 --> 00:18:00,180
All right. Just thought I'd mention that. So I think that's all I'll say about reading in a data sense for now.

125
00:18:00,380 --> 00:18:03,560
For this, you know, for this, at least for this class,

126
00:18:03,560 --> 00:18:12,050
we'll just assume kind of every everything that we're reading is just like a CSP or a regular type of text file.

127
00:18:13,250 --> 00:18:22,700
All right. So the I guess what we'll be talking about the rest of the day is just kind of data wrangling, some people call it.

128
00:18:22,700 --> 00:18:30,620
I guess it's really just kind of maybe you can think of it as data manipulation or data organization.

129
00:18:33,320 --> 00:18:41,000
So so data wrangling is just really the process of organizing your data so that it's useful.

130
00:18:41,000 --> 00:18:46,190
So sometimes a lot of times the kind of the raw data that you have,

131
00:18:46,190 --> 00:18:56,810
it's you need to do a little bit of work to organize it in a way so that you can do the statistical analysis

132
00:18:57,050 --> 00:19:02,270
that you're interested in or the kind of the graphing or data visualization that you're interested in.

133
00:19:02,270 --> 00:19:02,570
I mean,

134
00:19:03,140 --> 00:19:19,670
usually when you have a the goal of this class is to eventually to mostly be able to do some type of statistical analysis or data visualization.

135
00:19:19,670 --> 00:19:35,120
But often the first step is to to organize your data so that you could do this statistical analysis that you're interested in.

136
00:19:35,360 --> 00:19:50,659
Okay. So kind of the amount of work that this involves really kind of varies a lot depending depending on how kind of clean your original dataset is.

137
00:19:50,660 --> 00:19:56,209
Sometimes the original data that you get is pretty messy,

138
00:19:56,210 --> 00:20:04,760
or it might be scattered around a large number of files and you have to try to merge them in some way.

139
00:20:04,760 --> 00:20:08,959
So it really kind of the amount of work can vary a lot.

140
00:20:08,960 --> 00:20:15,950
Sometimes it's it's not it's a small amount of work, sometimes it's a really large amount.

141
00:20:17,570 --> 00:20:26,389
So I guess the kind of the things that we'll be focused on now, I mean, you could you could talk about a lot more,

142
00:20:26,390 --> 00:20:37,040
but since we have kind of roughly one class or one and a half classes to talk about this kind of focus on these topics.

143
00:20:37,040 --> 00:20:47,630
So sub setting data, often you're only interested in a subset of the data or what we want to compute certain summaries,

144
00:20:48,350 --> 00:20:51,409
statistics for specific subsets of the data.

145
00:20:51,410 --> 00:20:56,150
So it's, you know, to me to know how to do that,

146
00:20:58,670 --> 00:21:05,090
especially if you have kind of more complicated subsets of your data that you need to use some emerging data.

147
00:21:05,090 --> 00:21:14,540
So that's basically just kind of merging maybe different data files into a single data file that you might use for for analysis.

148
00:21:15,410 --> 00:21:18,410
So the next topic will be transforming data.

149
00:21:18,410 --> 00:21:30,470
So often for kind of the analysis that you're interested in, you have to kind of derive a variable based on.

150
00:21:32,360 --> 00:21:44,870
One ah are several variables. You kind of have to transform, you know, several of your other variables to kind of get to the outcome of interest.

151
00:21:46,340 --> 00:21:58,190
So I guess that's related to the driving new variables and then kind of at least a few comments about, you know, handling missing data.

152
00:21:58,610 --> 00:22:05,759
So it's often that's often an issue when you're trying to organize your data.

153
00:22:05,760 --> 00:22:12,020
I can often have or some variables which have a lot of messiness.

154
00:22:13,100 --> 00:22:23,569
All right. So let's I think we're just kind of just presented this as just kind of a whole series of different questions

155
00:22:23,570 --> 00:22:31,430
that you might be interested in and how you can how you can answer those questions kind of using different,

156
00:22:33,500 --> 00:22:40,490
different functions. So the first thing, at least in terms of like sometimes it's good to kind of first,

157
00:22:41,330 --> 00:22:47,750
you know, explore your data before you want to think about organizing in a certain way.

158
00:22:47,780 --> 00:22:53,720
So if we look back at this, maybe let's look back at what it looks like.

159
00:22:55,220 --> 00:23:00,290
So this is kind of what the the data set looks like when you first loaded in are.

160
00:23:00,740 --> 00:23:07,970
It's basically just information about different countries at different points in time.

161
00:23:08,330 --> 00:23:20,360
Okay. So like their population, life expectancy and GDP per capita, that's really all that this dataset contains.

162
00:23:23,030 --> 00:23:28,669
So one thing you might be interested in is kind of how many countries are there in the dataset.

163
00:23:28,670 --> 00:23:40,080
So there are 1704 observations in the dataset, but it's a lot of repeat observations for for most of the countries.

164
00:23:40,100 --> 00:23:43,940
So one thing you could do is use this unique function.

165
00:23:44,450 --> 00:23:54,140
So unique if you do unique of a like a vector, it just returns the set of kind of unique elements.

166
00:23:54,680 --> 00:24:03,950
So it returns the returns all of the elements from this vector, but it just deletes kind of all the duplicates.

167
00:24:04,070 --> 00:24:12,380
Okay. So this will return kind of a vector, which is the basically the set of unique values from this event.

168
00:24:12,590 --> 00:24:18,780
So if you take the length of that. And I'll tell you how many countries are in the dataset.

169
00:24:21,320 --> 00:24:27,110
So there's 142 countries. So that tells us that. So that's unusual to know.

170
00:24:27,720 --> 00:24:31,700
Also, there's also a variable that tells you the continent as from.

171
00:24:32,300 --> 00:24:35,960
So this has five continents.

172
00:24:39,050 --> 00:24:44,030
So I think actually, I think this is the way this data is structured.

173
00:24:44,030 --> 00:24:47,090
It kind of lumps North and South American. No.

174
00:24:47,540 --> 00:24:54,950
One country. But there's the Americas. Okay. So you can do a table as well.

175
00:24:54,960 --> 00:25:02,480
Sometimes it's good to get a good idea of kind of how your observations are spread across continents.

176
00:25:03,050 --> 00:25:10,130
So we do table of the continent variable and just tell us how many observations are from each of these five continents.

177
00:25:11,040 --> 00:25:16,129
I guess something that's also kind of useful to do at least when you first load

178
00:25:16,130 --> 00:25:20,510
in a data center just to look at kind of the ranges of the numeric variables,

179
00:25:20,510 --> 00:25:28,270
just to kind of get a sense of what typical values are from each of the numeric variables.

180
00:25:28,330 --> 00:25:39,260
So you can see that here, if you just do all a numeric variable, if you do summary, you just do summary of a numeric vector,

181
00:25:39,270 --> 00:25:48,200
it just it'll give out kind of this five number summary or 36 I guess, since it also throws in.

182
00:25:48,950 --> 00:25:59,810
So it gives the minimum, the first quartile, the median, the third quartile maximum and the median.

183
00:26:00,110 --> 00:26:08,360
Okay. So it's kind of gives you just through the summary function account gives you a decent

184
00:26:08,780 --> 00:26:17,720
idea of what the mean is decent idea what the spread is for that is that numeric variable.

185
00:26:21,630 --> 00:26:32,750
Here you can also do summary basically on the whole data frame itself like this probably becomes less useful

186
00:26:32,750 --> 00:26:41,989
if you have a data frame with a large number of variables and kind of spits out a ton of information,

187
00:26:41,990 --> 00:26:44,090
or it may not even print out everything.

188
00:26:44,510 --> 00:26:54,410
But if you have kind of a moderate number of variables like you have here, where we have six variables, you know, I think this is probably useful.

189
00:26:54,740 --> 00:27:03,890
So for the when you do this, it really only gives kind of useful information for the numeric variables, right?

190
00:27:04,880 --> 00:27:11,620
Is six numbers summary for each of the numeric variables or these character variables?

191
00:27:11,630 --> 00:27:15,260
It doesn't really, really give any useful information.

192
00:27:15,260 --> 00:27:21,530
Just says that it's a character and it has 1704 observations.

193
00:27:23,510 --> 00:27:37,129
Yeah. So this is if we converted this into a factor, it would give some kind of table to, oh, but this is low, this is lower than as a character.

194
00:27:37,130 --> 00:27:41,390
So it doesn't really give us much information. All right.

195
00:27:42,500 --> 00:27:49,819
Okay. So here's also some other things you might be interested in looking at when you're kind of explorer.

196
00:27:49,820 --> 00:27:52,900
First, exploring the data, the data frames.

197
00:27:52,940 --> 00:27:59,630
Sometimes you might be interested in a subset of your data are a certain important subset.

198
00:28:00,170 --> 00:28:05,270
So here, if you just wanted to focus in on a specific country and the U.S.

199
00:28:05,990 --> 00:28:12,140
So I just define this U.S. Underscore that data frame.

200
00:28:13,730 --> 00:28:21,560
So that basically that's a data frame with all the observations that where the country is is the United States.

201
00:28:22,190 --> 00:28:25,430
And so here I'm just looking at a summary of that. Okay.

202
00:28:27,450 --> 00:28:38,330
So here you can get what for some reason look at look like for the this is just for the U.S. observer observations.

203
00:28:39,930 --> 00:28:43,020
You can do this for any any country you like.

204
00:28:43,440 --> 00:28:48,480
So this is how you would subset for Brazil, for example.

205
00:28:50,290 --> 00:29:00,030
Okay. So here's another example. I guess this is a slightly more complicated subset.

206
00:29:00,420 --> 00:29:14,969
So here, let's say we wanted to create a subset which only contained all the years, 1952 or 2007.

207
00:29:14,970 --> 00:29:27,500
And I guess you could have done an over type of logical expression here, but you could also do your interviewers for this and just add these.

208
00:29:28,230 --> 00:29:36,390
Does this belong to this set? Does it belong to either 1957 or 2007?

209
00:29:36,480 --> 00:29:49,230
So this is going to basically return a data frame for all observations where the year is either equal to 1952 or 2007.

210
00:29:49,980 --> 00:29:59,130
Okay. So you can see that here. If we look at the first six rows of this data frame called two years,

211
00:30:00,060 --> 00:30:05,940
you could see look at the years and they're all equal to either like 1957 or 2007.

212
00:30:08,010 --> 00:30:19,110
Okay. So this is something more complicated, a subset list compared to just looking at specific countries.

213
00:30:20,470 --> 00:30:26,730
Okay. So here's an example of where we can use to apply.

214
00:30:26,940 --> 00:30:31,230
Okay. So I think I remember last time to apply.

215
00:30:34,860 --> 00:30:46,920
The way it works is you put like a vector as the first argument and then the second arguments as well.

216
00:30:47,310 --> 00:30:54,720
In this case, it's actually not a factor, but you can it's really any kind of thing that could be thought of as a categorical variable.

217
00:30:55,290 --> 00:31:06,089
So these are all character variables. So you can think of them as it's basically a category, and then you can have a function as the third argument.

218
00:31:06,090 --> 00:31:11,010
So in a plot for each, basically for each level of country.

219
00:31:11,010 --> 00:31:14,970
So for each different country, it applies this function to the.

220
00:31:19,110 --> 00:31:31,829
So this back to the, to this vector where we're only we're looking at kind of one country at a time.

221
00:31:31,830 --> 00:31:47,320
So if we, for example, look at the first country, you would apply this function to the vector of population kind of subset it to the other.

222
00:31:48,510 --> 00:32:02,280
So the first level of country, okay, so for each level of country, this, this two years dollar sign population vectors only has like two, right?

223
00:32:03,150 --> 00:32:11,910
So if you were to subset for each country at a time, this population vector only has its it's a factor of length too.

224
00:32:12,360 --> 00:32:22,440
Okay. So we can see that here. We're just basically if you look at one country at a time, there's only two observations for each country.

225
00:32:22,980 --> 00:32:33,750
And then assuming I didn't explicitly sorted, but everything sorted according to years of 1952 comes before 2007.

226
00:32:33,760 --> 00:32:46,050
So if we look at the difference x two minus x one, that's going to give us the difference in population between 2007 and 1952.

227
00:32:50,320 --> 00:32:58,600
So if we apply if we do this to apply with this function here, it's going to return that for each level of country.

228
00:32:59,320 --> 00:33:03,790
The difference in population between 2007 and 1990.

229
00:33:03,880 --> 00:33:09,190
So you can see that here. So, for example, will change the population change in Afghanistan.

230
00:33:10,210 --> 00:33:17,110
You're about 23 and a half million. For Austria, it's like 1.27.

231
00:33:19,030 --> 00:33:24,010
It's the population change over those 55 years.

232
00:33:25,220 --> 00:33:29,910
Okay. Okay. So let's say another thing.

233
00:33:29,940 --> 00:33:35,960
This is, I guess, an example of adding a new variable to a data set.

234
00:33:35,970 --> 00:33:43,820
So let's say we just it would be convenient to have an extra variable in our data frame two

235
00:33:43,820 --> 00:33:52,070
years that that stored the information for your population change between 1952 and 2000.

236
00:33:52,400 --> 00:34:01,040
So the way you could do that is just to create this new variable in the data frame here, kind of using this notation.

237
00:34:01,040 --> 00:34:05,890
Remember we said you could just use the, the name of the data frame, the dollar sign,

238
00:34:06,680 --> 00:34:10,909
the name of the variable that you want to create, and then you could just assign something.

239
00:34:10,910 --> 00:34:14,910
So as long as the dimensions make sense.

240
00:34:14,940 --> 00:34:18,540
And I'll just add that to the, to the data frame.

241
00:34:18,560 --> 00:34:22,700
So what we've done, what I did here, I just did the red function.

242
00:34:22,700 --> 00:34:37,609
So it's just going to repeat the population change kind of twice so that it's we have the same we have the same

243
00:34:37,610 --> 00:34:48,200
population change repeated twice for each country because there's only one population change between 1952 and 2007.

244
00:34:48,200 --> 00:34:54,770
So just kind of repeat it twice for each for each country.

245
00:34:55,340 --> 00:35:00,799
So you can see that here in that if we look at the two years data frame,

246
00:35:00,800 --> 00:35:06,380
now we have an extra column that that variable is called pop underscore change.

247
00:35:07,020 --> 00:35:12,680
And that gives us the population change between 1952 and 2007.

248
00:35:14,150 --> 00:35:24,889
All right. So this is okay. So I guess the next thing I mention is it's often useful to sort data frames according to certain variables of interest.

249
00:35:24,890 --> 00:35:29,930
So here it's basically sorted by country alphabetically.

250
00:35:30,500 --> 00:35:34,610
And I think within country it's sorted according to year.

251
00:35:34,610 --> 00:35:41,979
So the. Everything is sorted according to country.

252
00:35:41,980 --> 00:35:46,740
Then within country 1952 comes before 2000.

253
00:35:46,780 --> 00:35:54,610
So you may want to sort it in a different way. So let's say we just wanted to sort sorted by year, for example.

254
00:35:54,910 --> 00:36:00,310
So all of the 1952 observations would have come first.

255
00:36:00,320 --> 00:36:08,730
So the way you can do that, at least using kind of the base R type of syntax is, is by using the order function.

256
00:36:08,740 --> 00:36:16,990
So what you can do that is, let's say I'll just call it Gap 1952 and 19, two years.

257
00:36:17,710 --> 00:36:31,150
And I do I just I just to order of two years I use do order of like the the variable that you want to that you want to sort by.

258
00:36:31,270 --> 00:36:34,659
Okay. So basically what this order function does,

259
00:36:34,660 --> 00:36:46,870
it returns kind of a vector of integers that that reorders the data frame so that it so that it's sorted by this variable.

260
00:36:47,020 --> 00:36:50,830
Okay. So that's kind of why we use that notation.

261
00:36:52,060 --> 00:36:56,020
So it's basically like this order is returning a vector integers.

262
00:36:56,020 --> 00:37:01,270
That's it's not for one day not it's not one, two, three, four,

263
00:37:01,350 --> 00:37:10,510
it's a different order so that you kind of reorder the two years data frame according to those integers.

264
00:37:10,990 --> 00:37:20,530
It's, it's everything is sorted according to this variable and it's sorted from like low to high as the default there is.

265
00:37:20,980 --> 00:37:27,490
You can make it do a high to low, but the default is to sort it from from low to high.

266
00:37:27,760 --> 00:37:31,850
Okay. Okay. So this is.

267
00:37:32,780 --> 00:37:35,930
Okay, so you can. So everything is sorted.

268
00:37:40,460 --> 00:37:49,060
It was 1952. First you could, but there's there's a lot of repeat of 1952.

269
00:37:49,070 --> 00:37:54,390
Right. So there's a lot of repeated values in 1952.

270
00:37:54,410 --> 00:38:00,200
So within 1952, you could sort by another variable if you wanted to think.

271
00:38:01,430 --> 00:38:08,299
So, that would be called like sorting by by two variables.

272
00:38:08,300 --> 00:38:18,530
So I sort by year first, but then for those observations with the same value of year, you could sort by another.

273
00:38:19,490 --> 00:38:31,070
So that's, that's basically what's done. So if I sort by, it's just an order of year and comma population, it's going to sort by year first.

274
00:38:31,580 --> 00:38:40,040
But then for like those observations that have the same value of year, it's kind of it's going to sort by population.

275
00:38:40,040 --> 00:38:50,110
So it's going to give, you know, population from lowest to highest, at least within the year for those observations.

276
00:38:50,120 --> 00:38:57,890
And you can see here, this is the this is the country with the smallest population in the year 1952.

277
00:38:59,030 --> 00:39:10,580
And Djibouti is the second smallest population in 1952, at least in this dataset.

278
00:39:13,190 --> 00:39:21,800
Oh, if you wanted, I guess the easiest way to do kind of the reverse is just to do negative.

279
00:39:22,490 --> 00:39:30,740
If I just the order of year and then negative population that sort from year, it's still the year from lowest to highest.

280
00:39:31,340 --> 00:39:39,290
But for those within having the same value of year, it's going to sort by population from highest to lowest.

281
00:39:39,590 --> 00:39:44,990
Okay, you can see that here. It's kind of the bigger countries that start first.

282
00:39:45,260 --> 00:39:49,190
Okay. But still, these are all still at the top of the dataset.

283
00:39:49,190 --> 00:39:54,230
We still have everything is from 1952.

284
00:39:54,560 --> 00:39:59,270
Okay. So that's how you sort by these two variables.

285
00:39:59,270 --> 00:40:08,930
I mean, you could even sort by more than that, but usually there's not too many cases where it would make sense to do that two or three or four,

286
00:40:09,310 --> 00:40:22,040
because you have to have kind of repeated kind of have to have multiple values of the same number four, to make sense to sort by further variables.

287
00:40:22,040 --> 00:40:27,050
But in some cases, it may make sense to do that.

288
00:40:30,440 --> 00:40:36,229
Let's see, this is a little bit of a repeat of what we said in kind of an earlier lecture.

289
00:40:36,230 --> 00:40:49,160
We talked about missing data. I mean, the most common way of handling that is in a stands for not available I think there oh,

290
00:40:52,420 --> 00:40:55,940
I think I don't think there are any missing values of the original data frame.

291
00:40:55,940 --> 00:41:03,200
We're just saying if we wanted to put in a few missing values, you could do it this way.

292
00:41:03,620 --> 00:41:11,360
You assign the first two values of the population variable in a if you printed it out, it would look like this.

293
00:41:17,530 --> 00:41:17,840
Yeah.

294
00:41:19,000 --> 00:41:30,280
I think we mentioned this before is that in a function that that returns basically if you give it a a vector just returns a logical vector order.

295
00:41:30,290 --> 00:41:39,699
It's true if it's true for the positions that happen in in a value and false.

296
00:41:39,700 --> 00:41:46,689
Otherwise I think why I included this it's also useful for for data frames, I would say.

297
00:41:46,690 --> 00:41:57,700
So you can put like a data frame directly inside as the argument in a and it'll return a logical vector.

298
00:41:57,700 --> 00:42:07,630
So it's pretty useful sometimes. So here, if I just did the data frame where the data frame was just the first two rows of two years,

299
00:42:09,910 --> 00:42:13,030
and if I, I put that as the argument of is that in a,

300
00:42:13,420 --> 00:42:20,000
you know, for now something like this it'll be true where there are missing values like I,

301
00:42:20,170 --> 00:42:24,160
like I had for the population variable and then false otherwise.

302
00:42:24,730 --> 00:42:35,680
So let's say that's useful that you can find the number of like the total number of missing values in your your data frame.

303
00:42:35,680 --> 00:42:39,100
So if I do some of is starting a two years.

304
00:42:40,620 --> 00:42:47,400
It turns out three. That just means that there are three total missing observations in the entire data frame.

305
00:42:48,180 --> 00:42:54,270
Okay. But you can also use it for individual variables, like if I did some.

306
00:42:54,840 --> 00:42:59,970
Is that in a the GDP per capita variable?

307
00:43:00,480 --> 00:43:08,910
It's saying that there's no missing values there. So I think just you probably said that before.

308
00:43:08,910 --> 00:43:17,069
I don't know if I I don't know if I mentioned that you can use it on data frames directly so you can put the data

309
00:43:17,070 --> 00:43:27,510
frame as argument is data in a directly and return like a logical vector of the same dimensions as the data frame.

310
00:43:29,520 --> 00:43:39,150
Okay. Okay. So this is, as I said, this is actually a little bit of a new topic.

311
00:43:39,450 --> 00:43:44,970
So this is data wrangling using teams to imply.

312
00:43:45,030 --> 00:43:45,480
All right.

313
00:43:51,000 --> 00:44:03,239
So this is declare as it's a separate package that you have to load and you have to either install it if it's not there and then what it wants,

314
00:44:03,240 --> 00:44:06,470
it is installed. You have to load it in before you can use it.

315
00:44:06,480 --> 00:44:13,590
So that's just you just have to type in library the plot and then run that.

316
00:44:17,130 --> 00:44:30,030
So the player, I guess is probably the most popular package from this collection of our package is kind of referred to as tidy burgers,

317
00:44:30,690 --> 00:44:38,630
which is something that's. He's more popular in five or six years.

318
00:44:39,200 --> 00:44:54,320
It's just kind of a, I guess, a collection of packages for kind of just processing data in certain ways or doing data visualization,

319
00:44:55,100 --> 00:45:00,850
kind of a similar kind of philosophy and syntax, and I think some.

320
00:45:01,970 --> 00:45:10,610
And then also kind of extra useful functions that maybe the base R doesn't do so well.

321
00:45:11,750 --> 00:45:18,610
That's kind of the purpose of that. I mean, I think some of these packages are more widely used in popular somewhat.

322
00:45:19,220 --> 00:45:30,440
Some of them are probably not as popular as others. I'd say the most popular plot to which I guess is actually what came before Tiny Nurse.

323
00:45:31,040 --> 00:45:35,930
But I guess now it's kind of referred to as part of the title universe anyway.

324
00:45:35,960 --> 00:45:43,310
So I'd say probably the most popular ARG plot to any plot would talk about that.

325
00:45:43,520 --> 00:45:47,360
The other ones probably not quite as much.

326
00:45:48,410 --> 00:45:54,680
And then there's some overlap in the packages as well. They do something similar things.

327
00:45:54,680 --> 00:46:03,290
But I think for for deep plot, deep white R is the reason why it's popular, I think.

328
00:46:04,770 --> 00:46:13,700
And it does some a few things more simply or maybe better than what's available in base.

329
00:46:13,700 --> 00:46:19,510
R And then I guess one of the big advance advantages that people like is, is that you can kind of do a,

330
00:46:20,170 --> 00:46:29,180
you know, a collection of data processing steps in kind of a single almost like a single command almost.

331
00:46:29,930 --> 00:46:34,620
Talk about that later today rather than kind of writing at you.

332
00:46:34,880 --> 00:46:41,030
It might be a little bit more awkward in honor where you might have to write a series of commands line by lines.

333
00:46:41,630 --> 00:46:52,610
So I'd say that's probably one of the main reasons why people like, okay, so you could think of deep priorities, just a way of.

334
00:46:56,880 --> 00:47:02,070
Doing different types of data manipulation or data organization.

335
00:47:04,950 --> 00:47:15,830
So, so I guess the one way to think about what people are doing is that kind of most of your data kind of

336
00:47:15,840 --> 00:47:26,430
manipulation tasks can be done by using one of the or by doing by combining these operations in a certain way.

337
00:47:29,430 --> 00:47:40,290
You may have to do a series of these steps, but then if you break it down, it can be done by combining these in certain ways.

338
00:47:41,850 --> 00:47:50,549
So the and then kind of these deep fly ah functions allow you to do that sometimes called

339
00:47:50,550 --> 00:47:57,900
verbs to be applied for basically things that do some type of action on your data.

340
00:47:58,440 --> 00:48:01,910
So I think the main ones are going to be filter.

341
00:48:01,920 --> 00:48:05,340
So that's filter is kind of basically like some setting.

342
00:48:08,280 --> 00:48:14,880
So it's basically keeping rows that have let's satisfy a certain condition.

343
00:48:14,900 --> 00:48:27,060
So the filter, you can think of this as doing the same thing as what subset does in R select is basically choosing a subset of columns.

344
00:48:29,370 --> 00:48:37,200
A range is really related to sorting, mutate is related to creating new variables in some kind of way.

345
00:48:38,340 --> 00:48:48,780
Summarize is just basically creating summaries from different subsets of your data, basically.

346
00:48:48,840 --> 00:49:02,969
So you might, you know, compute the mean for certain subsets of the mean of a variable for first subsets of your data.

347
00:49:02,970 --> 00:49:10,350
So this is kind of doing what T applied is kind of the same type of thing usually.

348
00:49:10,860 --> 00:49:17,909
And then Group BI is just I guess a way of specifying which subsets of rows you want to

349
00:49:17,910 --> 00:49:23,910
focus on set group by kind of often goes together with some as so summarized kind of

350
00:49:24,390 --> 00:49:32,740
tells you which how you want to summarize your data and then maybe group by for specify

351
00:49:32,740 --> 00:49:41,280
kind of which which collection of rows you want to compute the summary statistics for.

352
00:49:42,060 --> 00:49:49,590
So these are the main ones. I think there's a few others, but these are kind of the main ones that you would want to focus on first.

353
00:49:52,600 --> 00:50:00,360
So I think I just mentioned this just as a reminder, like subset basically takes it and takes the root.

354
00:50:00,840 --> 00:50:13,450
Like for this example, subset would take a it creates a data frame where of only observations where the countries

355
00:50:13,490 --> 00:50:20,820
Brazil you could basically do the same thing with filter and deploy or so before you run,

356
00:50:20,970 --> 00:50:24,900
deploy or you have to do this library file first.

357
00:50:25,530 --> 00:50:29,190
If I run this group, if I run this row first.

358
00:50:30,090 --> 00:50:34,680
So once you do that, you can use on file or capabilities.

359
00:50:36,780 --> 00:50:41,160
Okay. So. Yeah, buddy.

360
00:50:42,970 --> 00:50:46,660
Like filter by itself is pretty much doing the same thing.

361
00:50:46,680 --> 00:50:53,140
A subset I think, and we'll see a little bit later on.

362
00:50:54,520 --> 00:50:59,490
This is something called the kind of the pipe operations in deep fly are.

363
00:50:59,500 --> 00:51:08,300
I guess where it becomes useful is that you can kind of combine filter with other operations in kind of a kind of a single state.

364
00:51:08,860 --> 00:51:18,489
And I think that that's really what makes it more useful. So I would say filters pretty much the same thing as a subset.

365
00:51:18,490 --> 00:51:26,020
I guess the only difference is the way it does sub setting for like joint statements.

366
00:51:26,770 --> 00:51:40,540
So if I wanted to subset create a a subset of from the original dataset that only has the a country Brazil and the year later than 1965.

367
00:51:40,540 --> 00:51:50,049
I could I could filter it this way. I just give it the, the name of this the dataframe first than I do with country equals Brazil.

368
00:51:50,050 --> 00:51:55,420
And then you do a comma and then you're rendered in 1965.

369
00:51:55,420 --> 00:52:02,319
So you can do it that way. If you were to use subset, you'd have to do it as like a formal logical expression.

370
00:52:02,320 --> 00:52:04,870
You would have to do with country equals Brazil.

371
00:52:05,290 --> 00:52:17,290
And then and with this ampersand character years trainers in 1965 that's it's really the only main difference between those filtering subsets.

372
00:52:17,290 --> 00:52:28,270
So I don't know maybe some some people find this more intuitive just separating the and conditions by a comma rather than using it as a ampersand.

373
00:52:28,960 --> 00:52:38,500
However, if you if you want to do kind of an overall type of subset, you still have to use the same type of vertical bar operation with one filter.

374
00:52:38,590 --> 00:52:42,790
Okay? So if you're doing an over type of subset, it's the same.

375
00:52:44,860 --> 00:52:48,640
Okay? So the, the next thing we'll talk about is select.

376
00:52:49,300 --> 00:52:55,240
So select just basically allows you to select a subset of variables.

377
00:52:56,410 --> 00:53:02,319
I don't know if it's as useful for this dataset in particular just because there's only six variables,

378
00:53:02,320 --> 00:53:12,850
but it's definitely useful if you have a larger dataset or at least larger in the sense of having a larger number of variables.

379
00:53:12,850 --> 00:53:18,669
Like if you have 500 variations, there's only really ten variables that you're interested in.

380
00:53:18,670 --> 00:53:26,920
Sometimes it's useful to just select those those ten variables that you're interested in and work with that.

381
00:53:27,160 --> 00:53:34,060
So when you view the data frames or look at the first few rows, it's less confusing.

382
00:53:34,460 --> 00:53:40,570
You're going to have to sift through the entire selection of 500 variables.

383
00:53:41,560 --> 00:53:45,710
So here, I've done it.

384
00:53:45,730 --> 00:53:59,620
If we just do this assignment, this creates a new data frame that if it's a new data frame which just has these variables from the original here,

385
00:54:00,070 --> 00:54:06,220
so it takes the original get mind your data frame and it just keeps these three variables.

386
00:54:06,370 --> 00:54:11,590
Okay, so you just put the name of the data frame first and then just the collection of.

387
00:54:13,610 --> 00:54:17,300
Of variable names from that data frame after which.

388
00:54:17,810 --> 00:54:18,830
So let's select.

389
00:54:19,250 --> 00:54:32,300
So select is just allowing to keep a collection of columns so you can actually, in some cases, you might just want to remove a certain column.

390
00:54:33,050 --> 00:54:38,450
In that case, you can do the you can just do the minus sign.

391
00:54:38,450 --> 00:54:48,469
So this is if I instead just do DFS and then do select again finder minus some GDP underscore that

392
00:54:48,470 --> 00:54:56,150
will return a data frame with with all of the original variables except for GDP school account.

393
00:54:56,600 --> 00:55:07,010
Okay. So it just keeps all the variables from the original data frame except for this variable here which.

394
00:55:13,230 --> 00:55:16,440
Okay. So you can even use the colon type of notation so that.

395
00:55:19,800 --> 00:55:25,530
So basically so you can see what's going on here.

396
00:55:25,530 --> 00:55:31,589
We have select with Gap Minor and then the first variable we're selecting is country.

397
00:55:31,590 --> 00:55:35,909
But you can also select quite a range of variables.

398
00:55:35,910 --> 00:55:45,690
This is basically has kind of the same you could think of this as it's kind of the same notation as we did using colons for creating factors.

399
00:55:46,500 --> 00:55:50,730
So this should just means that we're kind of starting from the variable continent

400
00:55:51,180 --> 00:55:55,979
and it's going to keep all the variables up until a GDP underscore attack.

401
00:55:55,980 --> 00:56:01,559
So this is we're going to select country and then kind of the remaining variables that we're going

402
00:56:01,560 --> 00:56:08,280
to select is all of the variables starting from continent all the way to getting your score.

403
00:56:08,280 --> 00:56:12,570
Cat, I forgot. Was that the original?

404
00:56:16,150 --> 00:56:22,450
Oh, I think. Okay. Those were like the last three columns in the original dataset.

405
00:56:22,450 --> 00:56:29,680
So when we did, we selected country and then we did a colon starting from continent up to GDP underscore.

406
00:56:30,490 --> 00:56:36,040
So it's basically selecting all of these. And then it did not select these two columns.

407
00:56:36,040 --> 00:56:40,150
It's kind of like what was going on in this in this example.

408
00:56:41,890 --> 00:56:45,460
So that's kind of just different, different ways of using select.

409
00:56:48,910 --> 00:56:50,920
Okay. This was about to start.

410
00:56:51,640 --> 00:57:02,380
Yeah, I think I'm I'm about to start doing the kind of the pipe notation from something deeply, deeply is especially useful for.

411
00:57:03,100 --> 00:57:13,600
So let's say we want to we, we just basically like the data set that we wanted to look at.

412
00:57:13,600 --> 00:57:16,450
It kind of involves both selection and filtering.

413
00:57:16,450 --> 00:57:28,030
So let's say we wanted to look at we want to create a data frame where the only country was Brazil and then we only want to look at a few,

414
00:57:28,930 --> 00:57:31,990
a few of the columns. We're not interested in the other variables.

415
00:57:32,650 --> 00:57:39,010
So we could first kind of create like this temporary data frame, we call it temp.

416
00:57:40,360 --> 00:57:51,999
So that be a data frame with only the Brazilian observations and then you could do a selection on template for this will be kind of what

417
00:57:52,000 --> 00:58:04,920
this data frame here that's printed here is a only the Brazilian observations with only the year end GDP per capita variables included.

418
00:58:05,170 --> 00:58:08,409
Okay. So that's this is one option for doing that.

419
00:58:08,410 --> 00:58:12,729
So this is this is perfectly fine way of doing that.

420
00:58:12,730 --> 00:58:22,960
But I think it's kind of a better way once we, as we'll see in a few slides,

421
00:58:23,530 --> 00:58:29,589
are an alternative way to do it is just to do kind of a like nested functions.

422
00:58:29,590 --> 00:58:36,010
So we first apply filter in here in the first argument of select.

423
00:58:36,010 --> 00:58:42,400
So this filter will return a data frame with only the Brazilian observations.

424
00:58:42,940 --> 00:58:52,990
And then we as the second and third argument, we include the variables that we we only want to include in this subset in a data frame.

425
00:58:54,490 --> 00:59:06,090
So that's also fine, but there is kind of an alternative notation or syntax for doing this and the plot.

426
00:59:06,220 --> 00:59:08,650
And I think it's especially useful if you want to,

427
00:59:09,070 --> 00:59:16,330
if you kind of have a series of these steps in a row that you want to if you want to do so, this is probably works.

428
00:59:16,330 --> 00:59:21,090
Okay? If you want to do two types of operations, but if you want to do four or five,

429
00:59:21,100 --> 00:59:29,290
six type of data processing steps, this becomes it starts to become a little bit.

430
00:59:31,960 --> 00:59:34,000
I don't know. Little bit annoying.

431
00:59:34,600 --> 00:59:43,630
So the way that a kind of deep air allows you to do these types of things is with this kind of operator that they call the pipe operator.

432
00:59:44,110 --> 00:59:48,690
So this is the, you know, the pipe operator.

433
00:59:48,780 --> 00:59:57,850
It's just this symbol here. And it's age. It's this greater than symbol in the middle, but surrounded by two, 2% signs.

434
00:59:57,940 --> 01:00:04,960
Okay. So this is call them the pipe operator and indeed fly arm.

435
01:00:05,260 --> 01:00:10,540
Okay. So if we wanted to do the same thing that we talked about before,

436
01:00:11,740 --> 01:00:21,100
the way you can do it and deploy are is just to first give the name of the data frame and then you give the pipe operator

437
01:00:21,730 --> 01:00:33,100
and then you do something to it here and you do this and you put another pipe operator and then you do something else.

438
01:00:33,730 --> 01:00:37,540
So the data frame, that's kind of the way to interpret it.

439
01:00:39,670 --> 01:00:43,059
At least that's kind of the way I think it. That's kind of the way I think of it.

440
01:00:43,060 --> 01:00:50,230
So. And this notation means that you start with the data frame, get wider.

441
01:00:50,230 --> 01:00:55,299
Then basically this pipe just means that you're going to do something to it.

442
01:00:55,300 --> 01:00:59,080
So it's mean and then you have a filter here.

443
01:00:59,080 --> 01:01:09,729
So that just means you think of this as creating another data frame, which only which filters by Brazil also only has the Brazilian observer nations.

444
01:01:09,730 --> 01:01:13,830
And then this pipe means you're going to do something else to it.

445
01:01:13,840 --> 01:01:21,459
And so down here is you do this selection for the two variables that you're interested in.

446
01:01:21,460 --> 01:01:34,000
What you wind up at the bottom of these series of pipes is something that that's left over after you've done both filter and select.

447
01:01:34,570 --> 01:01:45,550
So you can see that here. These are the Brazilian only observations with only the year and the GDP per capita included.

448
01:01:47,700 --> 01:01:56,610
Okay. So you can basically think of this as basically just applying a series of functions in a row.

449
01:01:57,160 --> 01:02:10,469
Okay. So doing like if you think of parentheses by parentheses, parentheses as certain functions doing def pipe x,

450
01:02:10,470 --> 01:02:18,240
pipe y, pipe z is the same thing as you first applying the function x to def.

451
01:02:18,690 --> 01:02:29,850
And then from that result, applying y two to x of def and then applying the function z to y of x of def.

452
01:02:30,190 --> 01:02:33,260
Okay. So that's kind of the way of thinking of it.

453
01:02:33,270 --> 01:02:36,280
So. These pipe chains.

454
01:02:36,280 --> 01:02:43,299
That just means you start out with the F first apply X to DF and apply next thing

455
01:02:43,300 --> 01:02:48,790
in the pipe change Y to accept the F and then the next thing in the pipe chain,

456
01:02:48,790 --> 01:03:03,310
you just apply that operation Z to Y of y, except the kind of whatever is kind of left after doing these series of operations.

457
01:03:05,400 --> 01:03:13,600
Okay. So you can actually, once you've load it and you can actually I mean,

458
01:03:13,600 --> 01:03:20,139
I don't usually do it this way, but you could do kind of standard math using a series of pipes.

459
01:03:20,140 --> 01:03:32,830
So if you had this kind of composition of functions, like if you had sine of one half an exponential of sine of one half and rounding that result,

460
01:03:33,340 --> 01:03:37,090
you could you could actually express that as a pipe change.

461
01:03:37,930 --> 01:03:43,839
So here, up here, this is maybe a more usual way of writing it,

462
01:03:43,840 --> 01:03:51,010
which is kind of closer to the way it's, it's written and mass like it is a composition of function.

463
01:03:51,010 --> 01:03:58,570
So you're taking the exponential of the sign of one half and then rounding that complete nearest two digits.

464
01:03:59,440 --> 01:04:02,440
But you could write it as a series of pipe changes.

465
01:04:03,010 --> 01:04:12,370
You're basically starting with one half. The first thing you do to it is take sign, and then the next thing you do to that number is the exponential.

466
01:04:13,300 --> 01:04:18,160
And then the next thing you do that to that number is you rounded to the nearest two digits.

467
01:04:18,160 --> 01:04:24,850
So it's kind of just a way of breaking down a series of operations.

468
01:04:26,060 --> 01:04:29,860
That's the way of thinking about pipe change.

469
01:04:29,860 --> 01:04:35,320
It's kind of breaking down a series of operations kind of.

470
01:04:36,630 --> 01:04:41,370
Kind of clearly step by step, I guess, is that is the way to think about it.

471
01:04:42,670 --> 01:04:48,450
All right. Okay.

472
01:04:48,450 --> 01:04:53,939
So that's. Yeah, I think that's part of the main thing I wanted to say about pipe change.

473
01:04:53,940 --> 01:04:58,710
I mean, we'll see some more examples of how to continue to use this.

474
01:05:01,380 --> 01:05:05,340
And I'll also mention kind of the other main deploy are.

475
01:05:08,180 --> 01:05:15,290
He functions so arranges the basically the way you do sorting and deploy.

476
01:05:16,230 --> 01:05:26,520
So if I wanted to get a data frame that sorted using pattern notation, I could do gap, minder type,

477
01:05:26,520 --> 01:05:34,440
arrange, arrange, and then you give it the name of the of the name of the variable you want to sort by it.

478
01:05:35,040 --> 01:05:39,600
So this is get minor pi arrange.

479
01:05:40,370 --> 01:05:43,609
A statement that returns a data frame.

480
01:05:43,610 --> 01:05:47,120
Actually, it's a data frame sorted by life expectancy.

481
01:05:48,650 --> 01:05:58,430
So if I just put that as the main argument inside the head function, it should just print out the first 4 to 5 given that the second argument.

482
01:05:58,610 --> 01:06:03,349
Number four, value four. You can see that here.

483
01:06:03,350 --> 01:06:10,790
This is pointing out the first four rows of the data frame returned by this state gap.

484
01:06:10,820 --> 01:06:14,050
Mind your type range life expectancy.

485
01:06:14,060 --> 01:06:24,500
So this is basically data frames sorted by life expectancy from low to high.

486
01:06:29,990 --> 01:06:36,020
You can do. High to low.

487
01:06:36,020 --> 01:06:43,849
You just have to do debts, at least around the variable that you want to sort of in descending order.

488
01:06:43,850 --> 01:06:50,809
So I, I've done that here. So I'm in this statement, I'm sorted by the two variables.

489
01:06:50,810 --> 01:07:00,380
So sorted by year first of so, but so that'll be done by default from, from low to high.

490
01:07:02,180 --> 01:07:07,760
And then I'm sorted by life expectancy from high to low, so I'm starting from off.

491
01:07:08,570 --> 01:07:14,180
So since I put this down, yes. C around the life expectancy variable.

492
01:07:18,900 --> 01:07:23,280
So you can see this here you have all the 1952 observations first.

493
01:07:23,730 --> 01:07:32,070
But within the 1952 years, it's sorted by life expectancy from high to low.

494
01:07:35,180 --> 01:07:47,860
Okay. So you can actually, at least within kind of the standard functions you can you can actually sort on functions of variables.

495
01:07:47,860 --> 01:07:56,220
If I want to sort by a variable, which is the product of GDP per capita times population.

496
01:07:56,790 --> 01:08:01,470
You could actually sort that by that variable. And I can do that.

497
01:08:02,760 --> 01:08:08,430
You can do that without the you don't necessarily have to create a new variable that has this value.

498
01:08:08,430 --> 01:08:17,999
So you can see here, this is sort of like high to low according to this product, but it didn't create an extra column in the DataFrame.

499
01:08:18,000 --> 01:08:27,720
So that's kind of useful. If you had that baseline, you probably have to create a separate variable for.

500
01:08:27,870 --> 01:08:31,750
Well, you have to do that.

501
01:08:31,800 --> 01:08:37,950
You could use the order function for a on that product itself.

502
01:08:38,640 --> 01:08:44,970
So that's that's something you could do based on. Well, okay.

503
01:08:45,890 --> 01:08:59,310
Okay. So here's kind of an example where I think the pipe starts to become this kind of pipe operation starts to become a little bit more useful.

504
01:08:59,700 --> 01:09:17,159
So let's say I just wanted to print out the first top ten highest sort of, you know, the top ten countries according to population in 1952.

505
01:09:17,160 --> 01:09:17,730
I wanted that.

506
01:09:18,180 --> 01:09:30,630
I want the top ten countries according to population in 1952, sorted from the largest at the top to the to the number ten at the at the bottom.

507
01:09:30,960 --> 01:09:37,350
Okay. So to do that with this pipe notation, I can just start, I get my order.

508
01:09:37,350 --> 01:09:44,790
The first thing I want to do is filter by year and I want to sort by population from highest to lowest.

509
01:09:45,390 --> 01:09:54,450
That's the second step. And then just returning the only the first ten rows.

510
01:09:54,800 --> 01:09:59,240
Okay. So that's what kind of this series of statements would do.

511
01:09:59,250 --> 01:10:14,100
So I don't have to create kind of intermediate temporary data frames and do this multiple lines or use some kind of a nested type of statement.

512
01:10:14,110 --> 01:10:22,990
I can just write a kind of a collection of single operations separated by pipes.

513
01:10:23,550 --> 01:10:34,080
So this is going to print out like the these are basically the top ten countries in population sorted from highest to lowest.

514
01:10:34,440 --> 01:10:41,430
Okay. I listed in 1952. So this is kind of a nice way of doing it with pipes.

515
01:10:42,480 --> 01:10:48,650
You know, if you want it to do 2007, do that as well.

516
01:10:48,660 --> 01:10:55,050
We just change that the year from 1952 to 27, and that's what the results are.

517
01:10:55,740 --> 01:11:03,360
So the next kind of useful thing I would say is, is mutate.

518
01:11:04,320 --> 01:11:09,020
So you can kind of include that in your your series of pipes as well.

519
01:11:09,030 --> 01:11:20,400
So let's say I wanted to print out the top ten countries according to GDP in 1952,

520
01:11:20,920 --> 01:11:27,770
but also at the same time wanted to create a new variable that's the product of GDP per capita.

521
01:11:28,680 --> 01:11:31,940
Oh sorry. This is. Oh yeah.

522
01:11:31,950 --> 01:11:34,530
We don't have the. Yeah I guess the point was it.

523
01:11:35,430 --> 01:11:47,489
Yeah, the point of this is that there is no GDP variable in the original datasets but we can create it by just multiplying GDP per capita population.

524
01:11:47,490 --> 01:11:58,170
So that's, that's like total GDP. So you can both create a variable and sort by the variable kind of within,

525
01:11:58,950 --> 01:12:04,679
within the pipe change, you don't have to kind of create that variable first separately,

526
01:12:04,680 --> 01:12:10,259
then kind of rerun everything you can create a variable kind of higher up in your

527
01:12:10,260 --> 01:12:17,760
series of pipe change and then also sort by that variable as well kind of later on.

528
01:12:18,550 --> 01:12:21,700
And one year later, pipe change. So it's nice.

529
01:12:21,700 --> 01:12:26,769
You don't have to kind of create two separate, separate series of pipe chains.

530
01:12:26,770 --> 01:12:31,329
You can kind of do everything together. So let's see what we're doing here.

531
01:12:31,330 --> 01:12:38,520
So first again, you start with the name of the data frame first and then I filter by 1952.

532
01:12:38,560 --> 01:12:44,560
So everything's going to be in 1952. And I know I'm creating a new variable called GDP.

533
01:12:45,040 --> 01:12:48,549
So this mutate operation creates new variables.

534
01:12:48,550 --> 01:12:54,970
So and then basically you're giving the definition of the variable here.

535
01:12:54,970 --> 01:13:05,500
So I'm saying a GDP, a GDP variable that I'm going to create is GDP per capita times population.

536
01:13:06,310 --> 01:13:14,050
So that's going when we get the final data frame, it's going to have this new variable called GDP.

537
01:13:14,470 --> 01:13:23,020
Okay. And then in this next step in the pipe chain, I'm sorted by GDP.

538
01:13:24,160 --> 01:13:29,950
And so that's a sorting from low sorry, from highest to lowest.

539
01:13:30,400 --> 01:13:34,299
And then I guess in the last step, I'm just looking at the first ten rows.

540
01:13:34,300 --> 01:13:40,960
So this says that these are the in print out everything.

541
01:13:41,560 --> 01:13:46,690
These are a subset of the top ten sorted according to total GDP.

542
01:13:47,080 --> 01:13:54,040
It didn't turn out that nicely, but you can see here, you see here, this is like GDP here.

543
01:13:55,270 --> 01:14:02,320
This is the last column, the value drop from here.

544
01:14:04,180 --> 01:14:11,020
But that's that if you were to print it out nicely, it would be just like an extra column in your dataframe.

545
01:14:12,190 --> 01:14:22,210
Okay. So let's so let's say since this print ended up nicer, this is basically the same thing.

546
01:14:22,570 --> 01:14:34,390
It's just 27. Instead of instead of 1942, we're filtering according to the year 2007 rather than 1952.

547
01:14:35,080 --> 01:14:39,580
So that's just one of them in the last nature.

548
01:14:40,000 --> 01:14:45,190
Oh yeah. We need to talk about group and summarize.

549
01:14:52,800 --> 01:14:57,260
And I want to try to finish it up.

550
01:14:58,040 --> 01:15:06,320
So. Well, first of all, I guess this this part is not as tricky as we add in.

551
01:15:07,460 --> 01:15:18,290
So we can summarize just it's basically just it's just taking some function of a of a variable of interest.

552
01:15:18,470 --> 01:15:20,320
That's really all it's it's kind of doing.

553
01:15:20,330 --> 01:15:30,319
So, I mean, the most common way it's used is to compute a median or a median or a standard deviation of some of some numeric variables.

554
01:15:30,320 --> 01:15:35,410
So kind of the way you would use it if we added, if we put it in a pipe chain,

555
01:15:35,420 --> 01:15:44,720
if we had like Gap Minder and then filter in 1952 and then it summarize and we kind of give it,

556
01:15:45,110 --> 01:15:52,070
we give a name to kind of the variable that we want the summary measure to have.

557
01:15:53,300 --> 01:15:56,640
And then we say a equals mean of the variable that we're interested in.

558
01:15:56,640 --> 01:16:07,220
And all this is going to do is take a mean of the life expectancy, at least for the data frame that we have after the most recent pipe.

559
01:16:07,310 --> 01:16:18,740
So this is going to give us this code here is going to give us the average life expectancy for all the countries in the year 1952.

560
01:16:19,520 --> 01:16:23,300
Okay. So that's basically all that is doing.

561
01:16:23,780 --> 01:16:32,690
And then the here this would give us the median life expectancy of all the countries in the year 2007.

562
01:16:33,760 --> 01:16:48,830
If you wanted the median, you would just do a median here instead of mean or standard deviation or any kind of the standard statistical summary.

563
01:16:51,760 --> 01:17:00,879
Functions available in R. I would say that the case where it's kind of more useful than you see a lot is when you want

564
01:17:00,880 --> 01:17:07,780
to compute a summary measure according to the levels of a certain categorical variable.

565
01:17:08,530 --> 01:17:12,880
So kind of like the same type of calculation that we do with t apply.

566
01:17:13,360 --> 01:17:20,950
So when you want to do that type of thing, you kind of need a group by type of statement before summarize.

567
01:17:21,520 --> 01:17:28,720
So let's say we wanted to do the mean life expectancy in 1952 kind of separately for each continent.

568
01:17:29,710 --> 01:17:36,790
So for that, again, you would first filter by year 1952, but you do before you summarize,

569
01:17:36,790 --> 01:17:43,270
you have to in Group VI, you need this kind of group by state before you do summarize.

570
01:17:43,720 --> 01:17:50,530
And again, for group by you have to give it the name of the variable that you want to group group it by.

571
01:17:50,540 --> 01:17:56,919
And usually it doesn't really make sense unless it is some categorical variable or at

572
01:17:56,920 --> 01:18:01,930
least a variable where there's kind of many observations that have the same value.

573
01:18:02,620 --> 01:18:13,480
Okay. So could technically be numeric like the year variable in this case, but you would want it to have your variable, which means kind of.

574
01:18:18,370 --> 01:18:23,020
A lot of the values are shared across observations.

575
01:18:24,290 --> 01:18:31,300
So this is kind of the average life expectancy for each of the continents in 1952.

576
01:18:34,180 --> 01:18:37,480
So know this is what it is. This is the same thing for.

577
01:18:41,020 --> 01:18:59,730
20 something. Okay. So I think that sorry, we're out of time and really have any questions or any kind of questions to finish up.

578
01:18:59,740 --> 01:19:10,720
I think I can finish it next time we'll start start talking about merging data and then also also reshape

579
01:19:11,110 --> 01:19:23,200
that topic about kind of reshaping it so much faster than I talked about graphics and data visualization.

580
01:19:24,790 --> 01:19:40,490
I think it's important for us to really have.

581
01:19:53,080 --> 01:20:04,240
Know I think we would going to.

582
01:20:06,290 --> 01:20:11,610
If I were to edit this.

