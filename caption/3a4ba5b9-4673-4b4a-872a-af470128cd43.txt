1
00:00:00,960 --> 00:00:10,440
Because there are other factors, some other factors.

2
00:00:10,440 --> 00:00:19,590
These are your other factors that your mentors, sometimes they make it difficult for you to just terrible things.

3
00:00:20,300 --> 00:00:49,120
But yes. And so you are taking aim at the process and then use your credit in a way that Dr. Johanna says, well,

4
00:00:50,880 --> 00:01:12,270
you should think if you think I'm good, because that is the woman you usually hear of that happening to be struck by.

5
00:01:12,270 --> 00:01:18,330
Now you are heavily on.

6
00:01:19,830 --> 00:01:27,360
Oh, oh, oh, oh.

7
00:01:28,350 --> 00:01:36,780
Okay, let's get started. I go to that and I finish grading your meter till like 1 a.m. last day.

8
00:01:36,790 --> 00:01:40,890
So when I finish, I am so happy that I get it done.

9
00:01:40,980 --> 00:01:49,020
And so I'm trying to work out the, the, my sort of comments and summaries of your midterm.

10
00:01:49,020 --> 00:01:53,280
And I still have a couple of points I need to sort of work out.

11
00:01:53,280 --> 00:01:57,010
And so I will return a midterm search there. There's some of my comments.

12
00:01:57,030 --> 00:02:12,630
So we'll go over here and. So one thing I want to mention is that some of you just wanted to see the sort of the computing time.

13
00:02:13,620 --> 00:02:19,500
Like, give me a very low sort of resolution for this research use in point one.

14
00:02:19,920 --> 00:02:28,200
Of course, you feel 0.1 from two that you don't see much difference, like where you are so friendly for a computer or you just like a computer.

15
00:02:28,200 --> 00:02:31,530
Wrong. You will give more a finer grade.

16
00:02:31,560 --> 00:02:36,840
Like, you know the great point that like the computer does not take a lot of time to write those things.

17
00:02:36,840 --> 00:02:50,069
So it's very fast. I did 10001 and yeah, I so minutes or maybe I were on my like it's like crap computer it doesn't take much time.

18
00:02:50,070 --> 00:02:55,410
I don't I think you must have must have much faster computer than mine because I you know,

19
00:02:55,410 --> 00:03:02,140
the before shoulder like solution before ODA ODA it's quite a nine year.

20
00:03:02,190 --> 00:03:12,810
It's doesn't take much time to learn it but anyway. Anyway I will make comments and and another big thing is the problem three part A

21
00:03:12,840 --> 00:03:21,510
that most of you detect that to say sort of counterintuitive sort of relationship.

22
00:03:21,510 --> 00:03:28,700
I, I think that people just point out these make comments that I really think that I,

23
00:03:29,610 --> 00:03:34,169
I don't know if the age group like 65 plus is the right age group.

24
00:03:34,170 --> 00:03:40,380
That's the first thing I ask. I think there are some kind of heterogeneity problems underneath this age group.

25
00:03:40,740 --> 00:03:50,340
People are 80 or older, may tend to be of how are less beneficial to the vaccine if they are sick,

26
00:03:50,340 --> 00:03:56,820
maybe they're just, you know, because they're, you know, a age that may not help much.

27
00:03:58,320 --> 00:04:03,000
But I don't know, like 65% is really a good cutoff to start.

28
00:04:03,020 --> 00:04:09,780
But you could point out some comments and also the the confounding factor of the number.

29
00:04:09,780 --> 00:04:15,840
In fact, the case is particularly for Omnicom, because there are a lot of breakthrough cases.

30
00:04:15,990 --> 00:04:25,500
People gather, you know, you talk to complete vaccination but they stupid like myself right so stick up part partly covered.

31
00:04:25,770 --> 00:04:34,770
So so it's it's hard to to really say the the vaccine situation and there's also expiry

32
00:04:35,100 --> 00:04:41,429
like usually the vaccine works a factor for a certain period of time and after,

33
00:04:41,430 --> 00:04:46,410
you know, there's there's another vaccination that would work forever.

34
00:04:46,650 --> 00:04:50,850
So anyway, so there are a lot of things that we can discuss there.

35
00:04:50,850 --> 00:04:59,270
But anyway, so, so to to help you, most of people do not commit anything, just point out,

36
00:04:59,280 --> 00:05:06,270
hey, here's the, you know, whatever the the numerical result you found from there.

37
00:05:07,320 --> 00:05:13,110
Anyway, I will return the of the midterm answers there and make some comments on that.

38
00:05:15,570 --> 00:05:23,700
But overall you do very well I think. And so it's not very difficult midterm, I guess, you know, so, so on.

39
00:05:24,510 --> 00:05:34,320
And I'm just back to the MSNBC because I want to wrap it up of this thing this week so that we can look out to the spatial data analysis next week.

40
00:05:36,420 --> 00:05:48,450
So this is something like we did in year 2020 when we start with sort of this pandemic and that

41
00:05:48,450 --> 00:05:56,000
time was thinking about how we could make some contribution to the infectious disease community,

42
00:05:56,220 --> 00:06:07,260
help them to process their data. So at that time we did this massive data and say algorithm and I just want to wrap up this the hour package that

43
00:06:07,260 --> 00:06:16,500
we did for for that and show you how how powerful this MSM is to do this complex sort of modeling dynamic model.

44
00:06:16,890 --> 00:06:25,110
Okay. So of course, in the use of NCMEC, there's always the issue of of big out the blurring, right?

45
00:06:25,110 --> 00:06:28,140
So here is the iterations, right?

46
00:06:30,600 --> 00:06:38,400
So you sample your sample for unconditional distribution in order to hopefully sample from joint distribution.

47
00:06:38,940 --> 00:06:46,259
But this sampling is coming from a markov Chen because as I already described that like give sampling,

48
00:06:46,260 --> 00:06:54,059
right, you sample from one dimension marginal and but you know that you are doing this cycle, right?

49
00:06:54,060 --> 00:07:00,030
So, you know, one by one and you always conditional to the values that you draw and you.

50
00:07:00,230 --> 00:07:08,880
Data point from conditional distribution and conditional distributions are both dimensional distribution and can always find some algorithm to draw.

51
00:07:08,970 --> 00:07:17,410
Right. So the question here is of under which condition that the samples draw from conditions

52
00:07:17,510 --> 00:07:21,650
which will eventually equivalent to a sample draw from joint distribution.

53
00:07:22,010 --> 00:07:30,340
So this breeding rate is actually the the time that under something like geometric,

54
00:07:31,540 --> 00:07:37,759
geometric all this city condition that you can guarantee that the samples were all formed a

55
00:07:37,760 --> 00:07:43,460
conditional distribution well basically equivalent to the sample draw from conditional distribution.

56
00:07:43,820 --> 00:07:45,110
That's what you want. Okay.

57
00:07:45,440 --> 00:07:56,900
Essentially, this is the case that I already mentioned that the you know, the sampling process eventually forgets the initial conditions.

58
00:07:57,020 --> 00:08:02,900
Okay. So so that you have the process like if you have multiple starting points, right?

59
00:08:03,800 --> 00:08:13,370
So eventually the, you know, the there are sort of sort of mixed together.

60
00:08:14,060 --> 00:08:17,180
So this is the time people think that's the brain.

61
00:08:17,930 --> 00:08:24,470
Well, so after this point that all the sample draw from this sort of condition distribution

62
00:08:24,480 --> 00:08:29,390
tend to give you kind of stationary or give you some kind of nice mixture,

63
00:08:29,810 --> 00:08:36,110
and you cannot tell that which pass is coming from which initial balance and

64
00:08:36,110 --> 00:08:42,739
this point that the initial positions have you complete sort of forgotten.

65
00:08:42,740 --> 00:08:48,170
And then after that you can start to use all the samples to make statistical analysis.

66
00:08:48,470 --> 00:08:53,720
Okay, so this is simulation based, the method experience, which is very nice.

67
00:08:53,840 --> 00:09:04,490
I mean, this especially the idea of the deeper learning or below some of the modern sort of data science,

68
00:09:04,730 --> 00:09:12,170
which were a lot of methods are based on simulations using the power of computer to generate some of information, if you like.

69
00:09:12,380 --> 00:09:19,980
Okay. I saying this is wild goals. Okay. Now the question here is, how do you pick up this points?

70
00:09:20,630 --> 00:09:28,100
Okay. Well, how you pick up this very, very critical capital B, so after this B,

71
00:09:28,100 --> 00:09:36,440
that all the data draw from this M60 algorithm will be used for you first, it will be regarded as the sample from joining distributions.

72
00:09:36,860 --> 00:09:43,940
So so you can pick up this to you can pick up, I mean, by eyeballing you can pick up this like 50 solid.

73
00:09:45,080 --> 00:09:50,930
Okay, you can pick up one where you know what a very good pickup as to it in the decision

74
00:09:50,930 --> 00:09:55,910
you can make your judgment right using this kind of trace pol or something like that.

75
00:09:56,600 --> 00:10:01,780
So you say I'm going to pick up 15,000 as the time of Bernie.

76
00:10:01,850 --> 00:10:10,730
Right. But the question here is whether or not this is a reasonable is actually can be confirmed

77
00:10:11,360 --> 00:10:17,929
in some way is more like how you do your diagnostics in the regression analysis,

78
00:10:17,930 --> 00:10:20,809
right. So you establish a NINA regression model.

79
00:10:20,810 --> 00:10:31,040
This is the model that I'm going to use to process from my data to generate some relationships or estimate some, you know, population attributes.

80
00:10:31,250 --> 00:10:36,889
But the one or not that model is right or appropriate or not appropriate right.

81
00:10:36,890 --> 00:10:41,360
So you use a residual analysis to confirm the assumptions.

82
00:10:41,480 --> 00:10:47,690
Okay? You cannot prove the assumptions of mathematical, but you can use your data,

83
00:10:48,020 --> 00:10:53,060
particularly from residual analysis, to confirm this is a very similar idea.

84
00:10:53,300 --> 00:10:59,060
You can give me a number capital B at which you believe that same series whispering.

85
00:10:59,930 --> 00:11:06,890
You can use some of the diagnostic tools to confirm whether or not that's really the case.

86
00:11:07,130 --> 00:11:13,820
So called a conversion diagnostic and output analysis is one of the earliest sort

87
00:11:13,820 --> 00:11:19,700
of work that people developed to to do this sort of convergence diagnostic,

88
00:11:19,970 --> 00:11:30,140
that diagnosis basically mostly to just focus on the the the bearing right whether or not, you know,

89
00:11:30,740 --> 00:11:39,680
the mystery of C three to that kind of point, that very critical after that you can use for your analysis.

90
00:11:39,680 --> 00:11:47,540
So caller is available in our package and provide functions for summarizing plotting output from Markov general called

91
00:11:47,930 --> 00:11:58,340
simulations as well as diagnostic diagnostic tests of convergence to equilibrium distribution or Markov chain basically.

92
00:11:59,180 --> 00:12:03,710
So so that's. The purpose of this coder.

93
00:12:04,820 --> 00:12:12,290
So unfortunately, many people ignore this critical step of convergence diagnosis and take the convergence for granted.

94
00:12:12,290 --> 00:12:17,910
People you know nowadays that you'll see I use them the same see I pick up be equal to 50 solid.

95
00:12:18,860 --> 00:12:22,280
Don't question me. Don't question me about these.

96
00:12:22,520 --> 00:12:26,870
Oh, the conversions which is of course not the right thing to do.

97
00:12:26,870 --> 00:12:34,160
I mean, people have published some papers in the early times to say that, you know, 50,000 may not be enough.

98
00:12:34,370 --> 00:12:38,959
And it depends on how strong memory the process is.

99
00:12:38,960 --> 00:12:43,940
Right? So if you have the process itself,

100
00:12:43,940 --> 00:12:51,530
the model you studied had has a very strong memory that is take long time to think to to forget the initial balance.

101
00:12:51,530 --> 00:13:01,909
Right. So but if you maybe the model is in such a way that you only need to take 50 or 500 iterations to forget out the initial condition.

102
00:13:01,910 --> 00:13:12,290
And it really depends on the autocorrelation, the mark of ten, how strong the memory is to to, you know, really reach the equilibrium condition.

103
00:13:13,340 --> 00:13:14,450
So it's hard to say.

104
00:13:14,450 --> 00:13:22,829
You have to go through this stack analysis really to confirm the choice of whatever the number of B you're going to use as per in the ABC,

105
00:13:22,830 --> 00:13:33,740
we see that like a residual analysis more for model diagnosis in careers of conversion diagnosis using numbers, they should be always encouraged.

106
00:13:33,920 --> 00:13:42,260
Okay, so gay men Ruby's test is used of folder two confirmed.

107
00:13:42,260 --> 00:13:47,450
I pick up value of B from sample pass from multiple starting values.

108
00:13:47,630 --> 00:13:56,810
Okay. So the given ruby in cancer is really like you have three pack like three passes or you have five depends on how you like to start

109
00:13:56,810 --> 00:14:07,370
your multichannel sort of M.S. Then you can compare the two values you go after to be whether or not they are really mixed.

110
00:14:10,040 --> 00:14:20,990
Well, right. So so you can I mean, intuitively, you can imagine that if birding happens,

111
00:14:21,530 --> 00:14:28,700
then this multiple chance to be mixed together because they often forget about the initial conditions.

112
00:14:28,880 --> 00:14:34,120
If they haven't forgot to the full Gulf in their initial conditions, there should be like forks, right?

113
00:14:34,130 --> 00:14:45,590
They have branches going their own way. But if if this process really, you know, bring happen, then all in multiple chances to somehow mix together.

114
00:14:45,980 --> 00:14:49,900
When they mix the well, mix the together, reach the equilibrium point.

115
00:14:50,180 --> 00:14:57,950
All the differences really due to the random sort of random deviations, a random, you know, error,

116
00:14:58,220 --> 00:15:08,900
random marginal error is not really something driven by the initial conditions so that you feel to test you can, you know, reject that.

117
00:15:09,020 --> 00:15:14,660
So, you know, this three process will be really very similar.

118
00:15:14,720 --> 00:15:24,440
Okay, that's the game. And Rubin test is quite popular even as part of these are jacked up so so that

119
00:15:24,440 --> 00:15:29,540
provide this statistic but this statistic itself is not sufficient is one,

120
00:15:30,710 --> 00:15:36,530
you know, computational computationally convenient way to check but that that's not sufficient.

121
00:15:36,620 --> 00:15:44,610
But sometimes people just take this for a like the gold standard like if you've

122
00:15:44,900 --> 00:15:50,690
given Rubin test pass this then barring reaches and that's not true it's more

123
00:15:50,690 --> 00:15:59,329
likely to you know when when the F test passes this sort of the this burning

124
00:15:59,330 --> 00:16:05,720
then it only says that it's more likely to be doesn't really guarantee that.

125
00:16:05,870 --> 00:16:10,220
Okay. So personally I like the weak test.

126
00:16:11,000 --> 00:16:17,060
Yeah, good. Weak test I think is a little bit more intuitive, but takes all the time to process this test.

127
00:16:17,570 --> 00:16:22,790
Then a given Rubin test that is given Rubin test to just give you one statistic.

128
00:16:23,210 --> 00:16:29,840
But the weak test, it gives you multiple test. I will explain to you how this test looks like for this this narrative.

129
00:16:30,560 --> 00:16:36,830
And Pennebaker and Welch test are also very useful.

130
00:16:37,970 --> 00:16:42,740
Okay. Let me just go over this a quick test first.

131
00:16:42,740 --> 00:16:48,650
I personally I like this test. It's give me a lot of intuition what's going on.

132
00:16:48,710 --> 00:17:00,110
Okay. So what happens here is you select two segments from a sequence of sample values with respect to, you know, the first a percent value.

133
00:17:01,660 --> 00:17:13,630
And at last the B values, you know, for example, you can use 10% or 25% and, you know, the peak could be lost 50%.

134
00:17:14,440 --> 00:17:19,120
And you divide this each of the segments into 15 things.

135
00:17:21,610 --> 00:17:28,480
So here this is your P you choose and then you generate an set of values.

136
00:17:30,010 --> 00:17:39,879
Okay then so from this channel, right, so then you can cut this into, oh, you know,

137
00:17:39,880 --> 00:17:47,870
so, so you have this, right, so, so then so here is the initial position.

138
00:17:48,140 --> 00:17:54,520
You choose the, the 50% of the values from this channel.

139
00:17:55,240 --> 00:17:58,240
So that's the p part. Okay.

140
00:17:58,660 --> 00:18:05,650
Then there you look at that. The first part here, you can have 10%, 25, 25%.

141
00:18:06,250 --> 00:18:10,630
For example, you cut this part versus this part, right?

142
00:18:11,320 --> 00:18:22,240
So so this so you've basically divided into two multiple segments, like two segments, for example,

143
00:18:22,780 --> 00:18:33,010
you can have used the first 10% of the channel and then you use a 50 50% of the the segment of the channel as the second part.

144
00:18:33,370 --> 00:18:39,410
Now, you basically can create the the mean, you know,

145
00:18:40,120 --> 00:18:51,520
each divided the segment into 25 beats well subsequent and calculate sample means and half you know sample TV variances for each piece.

146
00:18:51,820 --> 00:19:01,810
Okay so then. So you cut this into a small part of the context into small parts.

147
00:19:03,190 --> 00:19:12,070
Okay. So, so, so for that, for each short a small segment, you can target its meaning and then right.

148
00:19:12,130 --> 00:19:17,830
Because you have this, you know, some kind of data point, each being like,

149
00:19:17,830 --> 00:19:28,190
suppose you cut it in 25 bins each being you have some like 20 or 50 data points depends on the values you have, how you choose.

150
00:19:28,540 --> 00:19:37,250
But anyway, you have some data points where that you can, you know, calculate the sample, mean understand forbearance.

151
00:19:37,270 --> 00:19:41,710
Let's say you have 25, you have the early and late.

152
00:19:42,820 --> 00:19:46,720
So if this station achieves if the process mixed the well,

153
00:19:47,200 --> 00:19:53,380
then the mean from the early segment population should be close to that for the late second population.

154
00:19:53,800 --> 00:20:04,990
Right is this process becomes stationary, it reaches the equilibrium point then you know the early part and later part of should be very close.

155
00:20:05,680 --> 00:20:08,560
Okay. That's place of value. Can to to t test. Okay.

156
00:20:08,980 --> 00:20:22,450
So t test two to sample t test can tell you that's exactly d so give make this score and you use this to sample t test idea you have, you know,

157
00:20:22,450 --> 00:20:29,649
whatever the population mean for the early segment of like 10% of it and then

158
00:20:29,650 --> 00:20:35,230
you have the later part that takes about 50% and you have the to the variance.

159
00:20:35,890 --> 00:20:40,960
This is kind of the t test to sample t test that we usually do.

160
00:20:42,520 --> 00:20:47,079
And under this null hypothesis of the station narrative,

161
00:20:47,080 --> 00:20:52,620
you can prove that this statistic will roughly approximately follow a standard normal distribution.

162
00:20:53,770 --> 00:20:57,070
Then you can use this cutoff plus two.

163
00:20:57,250 --> 00:21:00,430
Minus two you can use you use 1.96.

164
00:21:00,430 --> 00:21:03,230
But this is this is step gnostics, right?

165
00:21:03,280 --> 00:21:15,129
So just like you detect outliers, sometimes you are not really that precise to use like 1.6 in the software I call that or in the literature,

166
00:21:15,130 --> 00:21:23,470
people just sit, you know, just use plus minus two as this cutoff and yes, yardstick for the monitoring.

167
00:21:23,680 --> 00:21:34,920
Okay. So you calculate this from 25 data points from your from your 30 part segment, you have 25 data points for you.

168
00:21:35,380 --> 00:21:48,100
The late segment, you do this, you know, statistic and then you see if this is outside of the plus, minus plus or minus two.

169
00:21:48,700 --> 00:21:58,270
If so, then you can say that if if stationary t achieves that is achieved, then this statistic should be between -2 to 2.

170
00:21:58,600 --> 00:22:06,400
If not, then then you should see something more than 5% to, you know, to go outside of this.

171
00:22:06,610 --> 00:22:17,230
Okay. So so this is essentially so you you create this minus two plus two cold out, gives you this box.

172
00:22:18,130 --> 00:22:23,800
Okay? So you have one point here. It's this that means that, you know.

173
00:22:23,830 --> 00:22:30,550
Yeah, so so the difference between the early segment and later segment for the no difference,

174
00:22:30,640 --> 00:22:39,520
statistically speaking, right, if you go outside of and that means you go beyond the 25% confidence.

175
00:22:39,850 --> 00:22:49,770
All right. A 95% confidence. So you are supposed to have like 5% a point, you know, a legally to also because you create a 95 composite.

176
00:22:50,380 --> 00:22:55,810
But if you have more than 5%, that means the issue of all the convergence.

177
00:22:56,440 --> 00:23:00,310
Okay. So that's really the intuition behind this quick test.

178
00:23:01,480 --> 00:23:12,550
So how do you generate more of that? Okay. That's exactly the the procedure, that quick test, the test that being used in a convergence diagnosis.

179
00:23:14,290 --> 00:23:23,320
So the picture of the week test, the test should be displayed by the plot of many week list scores in terms of different extractions.

180
00:23:23,710 --> 00:23:28,270
So essentially, how do you define these segments to create different types of statistics?

181
00:23:28,300 --> 00:23:34,330
Right. So we are not going to only do one sort of deviation, one one choice.

182
00:23:34,570 --> 00:23:40,810
You do more of a choice and then you sort of getting more and more sort of, uh,

183
00:23:41,830 --> 00:23:51,850
sort of finer checking of the, of the differences by looking at different part of the segment of the chart.

184
00:23:52,000 --> 00:24:03,070
Okay, here, here is the procedure. So consider a sequence of nested channel, given the false channel is the food channel with invaders.

185
00:24:03,280 --> 00:24:12,519
Okay, that's the food chain. So then chain to you a sub Chambliss and minus one that is discarding.

186
00:24:12,520 --> 00:24:20,139
The first thing that is and the the first iteration number is in plus one.

187
00:24:20,140 --> 00:24:24,010
So. So you start with the the canvas aim values.

188
00:24:24,580 --> 00:24:33,700
That's the first ten. You can create a one you have a good week that statistic by the way you I just way now you look at a shorter

189
00:24:33,700 --> 00:24:48,069
ten by discuss discard the first invaders okay so that okay then then you create a even shorter chain.

190
00:24:48,070 --> 00:24:58,240
So channel three is a section this lost and minus two you discard the first end and too many values.

191
00:24:59,260 --> 00:25:03,610
Okay, so you can imagine that here is the first ten.

192
00:25:03,880 --> 00:25:14,350
Okay, then say can ten discard, you know, the this part, then you further discard this, you get to ten three.

193
00:25:15,550 --> 00:25:21,400
Okay, so for every ten you use the same principle to generate the distances.

194
00:25:21,400 --> 00:25:30,310
And basically, as I described, you use them half of this later, ten and 25% or 10% of its first ten.

195
00:25:30,820 --> 00:25:35,530
Okay. To create this this test score statistic.

196
00:25:35,530 --> 00:25:45,550
Okay. You keep doing this, okay. Until you saturated this procedure, until this step that you stop for sure.

197
00:25:45,640 --> 00:25:56,680
Okay? And the first iteration is K plus and and so continues until defined as option contents list of 50 values.

198
00:25:57,130 --> 00:26:01,750
But the folder extraction and resulting a sub is less than 50 of that.

199
00:26:02,170 --> 00:26:07,450
Okay, then you stop. Okay, so. So you stop somewhere here.

200
00:26:07,900 --> 00:26:11,110
The last part that is 50 values or less.

201
00:26:11,200 --> 00:26:20,380
You don't because you believe that based on the actual channel FIFO values that this statistic of course this is not going to be very stable.

202
00:26:20,500 --> 00:26:32,290
Okay, so for each stop, can you take the Z ice scores by definition where A and B is choosing fixed over you look at chance B could be 50%.

203
00:26:32,290 --> 00:26:35,950
It could be 25% as I describe here, or 10%.

204
00:26:36,130 --> 00:26:45,790
Okay. They every time you you cut the first part so you can see that if you interest B 50 solid and then if this is not true,

205
00:26:45,790 --> 00:26:53,499
then you you cut this part, you move this B before down there, you move the B folder back and down and then down, right.

206
00:26:53,500 --> 00:27:02,139
So you can see that and this cutting is purposely designed that you are essentially looking at some

207
00:27:02,140 --> 00:27:10,360
four points that are bigger than the purpose initially choosing B so that you can see where they are.

208
00:27:11,010 --> 00:27:16,810
But this, that this course becomes more sort of stable.

209
00:27:17,380 --> 00:27:26,740
You can imagine that you kind of have one scenario like minus two plus two, maybe all the points are everything here.

210
00:27:26,740 --> 00:27:35,290
That means the B is good choice because no matter how you cut this, you always have very, you know, stable situation.

211
00:27:36,040 --> 00:27:47,770
First segment, dirty segment, that later segment consistently gives you, no, no, no, no, no, no troublemakers right there, everything.

212
00:27:48,220 --> 00:27:57,640
And then you could have a situation at the beginning. There are a lot of points out, but suddenly the points are all inside.

213
00:27:57,790 --> 00:28:06,399
That means you should increase your lead to a point that you can reach some better stationary team.

214
00:28:06,400 --> 00:28:16,300
So that makes you win. Basically means like between convergence or like we can also control mixing from like this in Polish or something like this.

215
00:28:16,390 --> 00:28:23,530
Yeah. So what does it mean me the, the mix it, this is very generic a way to,

216
00:28:24,310 --> 00:28:31,090
to describe basically things states that all the channels are tangled together is it's not the is mathematical

217
00:28:31,090 --> 00:28:42,050
definition is a a word to describe how severe the appearance of the or your sample pass like variable.

218
00:28:42,630 --> 00:28:47,620
But it's rare right? Basically they are all like tangled together, right?

219
00:28:47,920 --> 00:28:53,049
There are little separations so they know this is the biggest the world is.

220
00:28:53,050 --> 00:29:01,020
And you can. Although a mathematical definition is rather a way to describe the appearance or the look of the.

221
00:29:01,050 --> 00:29:05,870
Oh, you're basically the the phenomenal display by the figure.

222
00:29:09,020 --> 00:29:18,390
Okay. You can use equilibrium or stationary or some like more sort of formal statistic thing.

223
00:29:18,810 --> 00:29:23,420
So plotting this Z against the first impression and then you,

224
00:29:24,170 --> 00:29:31,020
you would expect both the point to fall into the barrel -2 to 2 if the stationary achieves.

225
00:29:31,440 --> 00:29:34,950
So this is the part of, you know,

226
00:29:35,310 --> 00:29:44,760
the week this statistic is that I prefer because that gives me a lot of more detailed checking about the stationary than,

227
00:29:46,230 --> 00:29:50,130
you know, the command Rubins f statistic.

228
00:29:50,780 --> 00:29:55,649
Oh sometimes people lazy just to go give my Rubin test.

229
00:29:55,650 --> 00:30:02,520
That is okay. But if you want to really do a serious business that you need to look at this.

230
00:30:03,660 --> 00:30:07,649
Okay. So this is one plot per wired.

231
00:30:07,650 --> 00:30:14,550
Even colder, which is very straightforward, is based on two sample t test idea.

232
00:30:14,670 --> 00:30:21,690
Okay. So Hedberg and walch test is a little bit more complicated, but let's just go over it.

233
00:30:22,290 --> 00:30:30,929
And so this two German statistician propose to test the hypothesis that small values

234
00:30:30,930 --> 00:30:38,520
for a sample values from a station reprocess basically the test this dish narrative.

235
00:30:38,580 --> 00:30:42,800
Okay. So they come up with the way to tested this.

236
00:30:42,810 --> 00:30:53,880
They give you a statistic using this sort of stochastic process of sort of analytics to to create this test of station integrity.

237
00:30:55,020 --> 00:31:07,200
So if null hypothesis is reject basically that this whatever the, the chair you're looking at is not a stationary yet,

238
00:31:07,200 --> 00:31:13,770
then you will, you know, throughout the first 2% of sample the values.

239
00:31:14,070 --> 00:31:23,540
Okay. So you shorten the chain by 10% y pretty percent or that's a arbitrary number and you can choose 15% or even 11%.

240
00:31:23,550 --> 00:31:28,440
I mean, it's but this is what the authors suggested.

241
00:31:28,470 --> 00:31:32,340
You just throw out the 10% of the sample values.

242
00:31:32,610 --> 00:31:39,900
Okay. Then you use the remaining ten to, you know, do this stationary to test.

243
00:31:40,140 --> 00:31:44,760
Okay. This Hedberg approach test. Use the remaining chance to do.

244
00:31:44,850 --> 00:31:53,130
If this null hypothesis of stationary t is reject again based on the 90 90% original sample data.

245
00:31:53,580 --> 00:31:57,240
Now you folder cut the chin by 10%. Okay, then.

246
00:31:57,780 --> 00:32:07,320
Then you keep doing this until either the proportion of chan at least 50% of sample value passes stationary

247
00:32:08,280 --> 00:32:14,980
station enter to test or 50% of sample value have been threw out and to not have half still reject it.

248
00:32:15,000 --> 00:32:23,700
So you stop. So if you can now reach the stationary t, you cannot find a point or trend that gives you stationary to test.

249
00:32:24,690 --> 00:32:30,809
You know that you are not going to fold or if the remaining chance 50% of of the or initial

250
00:32:30,810 --> 00:32:39,660
chance so so you don't do anything further cut more than you know 50% of the origin.

251
00:32:40,230 --> 00:32:49,110
If the leader happens this dish in order to test the fills and the longer round is deemed to achieve this convergence,

252
00:32:49,110 --> 00:32:52,590
then you need to increase your envelope after bearing.

253
00:32:52,740 --> 00:32:59,520
So everything is based on A choosing B and then you do extra imbalance.

254
00:33:00,090 --> 00:33:08,310
After that, you are basically looking at whether or not this imbalance, the channel imbalance, gives you some kind of stationary t you want.

255
00:33:10,920 --> 00:33:19,979
So how much longer you would need to run if this, you know, rejection happens, you do not reach this this narrative,

256
00:33:19,980 --> 00:33:29,670
then the half width in this thing that gives you a empirical sort of rule, how how extra points do you need?

257
00:33:29,910 --> 00:33:32,460
How many extra values you need to run? Okay.

258
00:33:32,520 --> 00:33:41,459
If you do not have a lock to really, you know, conclude a stationary tank in the head of Bergen Welch test,

259
00:33:41,460 --> 00:33:48,180
a quantity called half with is defined as half length of a 95 confidence interval.

260
00:33:48,210 --> 00:33:54,210
Okay. It's also useful to make a decision whether a longer run should be considered.

261
00:33:54,480 --> 00:34:02,970
Okay. So the half west test is conducted as follows based on the proportion of chain that pass this stationary energy test,

262
00:34:03,420 --> 00:34:06,780
the corresponding sample mean and is some part extended arrow.

263
00:34:06,960 --> 00:34:11,310
Can be obtained at a zero for the half wealth of the associate.

264
00:34:11,310 --> 00:34:18,360
90%, 95% of confidence interval for that being is equal to 1.96 times stained area.

265
00:34:18,540 --> 00:34:29,309
That's something we're not for right if they have with is less than ipsum times the sample mean the has the past and the return.

266
00:34:29,310 --> 00:34:39,450
The sample is deemed to estimate the posterior mean with accepted accepted position and ipsum is usually taken to be point one.

267
00:34:40,110 --> 00:34:46,470
If the test the Phils a longer run is deemed to achieve this additionally accuracy for estimate.

268
00:34:46,890 --> 00:34:59,010
So that's basically the half this is trying to to to set up the half width that's to set up to how do you you know to this test.

269
00:34:59,250 --> 00:35:10,530
Okay so in the CO two output there is a suggested number that tells you how many actual value

270
00:35:10,770 --> 00:35:19,740
you need to run to achieve certain you know desired out sort of station are to yes you wish.

271
00:35:20,400 --> 00:35:28,260
Okay so those are besides given Reuben F test this are to test I usually use I feel that they are

272
00:35:28,260 --> 00:35:34,799
pretty informative and give me a better more confidence about convergence diagnosis and both are

273
00:35:34,800 --> 00:35:46,940
available you coder that should be the our packages okay so so that's the the case where we want I'm

274
00:35:46,960 --> 00:35:55,920
sincere but sometimes that maybe give sampler is not really feasible to derive and keep sampling.

275
00:35:56,460 --> 00:36:01,800
It's very nice if you can figure out condition of distribution.

276
00:36:01,800 --> 00:36:05,340
Sometimes you may not be able to do give sampling.

277
00:36:05,550 --> 00:36:10,320
Okay, because you are not able to work out the condition of distribution.

278
00:36:10,320 --> 00:36:13,860
You have to read that something a little more advanced then keep sampling.

279
00:36:14,580 --> 00:36:22,590
And so that's basically me top metropolis hasting of words and so give keep

280
00:36:22,590 --> 00:36:27,959
sampling is a special case of MH algorithm me type of testing algorithm we stay

281
00:36:27,960 --> 00:36:36,450
space of Markov chain is of course causing a product form and the general sampling

282
00:36:36,450 --> 00:36:40,950
scheme requires flexible way of constructing a station or mock of chimneys,

283
00:36:40,950 --> 00:36:46,740
a given station or distribution pie from which sample are expected to raw.

284
00:36:47,070 --> 00:36:56,160
Okay. So basically that the main type of testing algorithm is trying to generate skip sample in the case,

285
00:36:56,160 --> 00:37:02,760
sometimes you are not able to figure out this stationary distribution to generate the samples.

286
00:37:05,460 --> 00:37:12,090
So a simple way to do this is to construct a reversible Chien using Hastings method

287
00:37:12,090 --> 00:37:17,250
used via a substance probably bullet it which allows for arbitrary transition kernel.

288
00:37:18,150 --> 00:37:32,280
Okay. So, so basically what you're trying to do here is I'm not able to sample from the original the I want to sample from W given you and me,

289
00:37:32,670 --> 00:37:42,300
okay this is what I want. Okay. If I'm able to sample from the condition distribution of W giving you an avid, I can use skip sampler.

290
00:37:42,540 --> 00:37:45,809
Okay. But sometimes this distribution is complicated.

291
00:37:45,810 --> 00:37:51,480
I cannot really figure out how the analytical for I don't know how to sample is.

292
00:37:51,750 --> 00:37:58,410
This could be complex so I can use a proposal so I can sample from f tutor.

293
00:37:59,490 --> 00:38:09,090
I'm able to sample from f to that. Of course, roughly speaking, I know that I can sample from a tutor.

294
00:38:09,810 --> 00:38:13,890
I can all sample origin from F, but I'm able to sample from F to them.

295
00:38:14,610 --> 00:38:25,079
But it's too late is not enough. So. So then under certain conditions I could say, well some of general F will be you know,

296
00:38:25,080 --> 00:38:33,440
this is Hastings or some generated a W will be really just same as the one generally from F W.

297
00:38:34,350 --> 00:38:42,330
Okay so so of course some of this will not be like these the sample general from top.

298
00:38:42,450 --> 00:38:46,710
So there's a acceptance probability which Y should be accepted.

299
00:38:47,130 --> 00:38:53,100
Which one sample from F to that should be a separate sample generally from and that's okay.

300
00:38:53,460 --> 00:39:06,210
So, so that's basically I mean very very intuitive thinking me the idea so so you have so that's first you.

301
00:39:06,490 --> 00:39:10,390
You start with the iteration contrary to the arbitrary initial value.

302
00:39:10,870 --> 00:39:15,170
Move the channel to a new value. Find that is generated from density.

303
00:39:15,190 --> 00:39:22,329
Q Okay. And, you know, this is proposal density is like F to the right.

304
00:39:22,330 --> 00:39:26,440
So suppose you are able to, to, to find some.

305
00:39:26,800 --> 00:39:33,100
Q Okay. Of course, you sometimes most of the time you could find something.

306
00:39:33,580 --> 00:39:37,990
I mean, there's always the issue of. Q is good or bad. But if you have really a good.

307
00:39:38,000 --> 00:39:47,150
Q very good F close to F, then you have high chance to generate the samples like desirable samples.

308
00:39:47,170 --> 00:39:54,550
If the formula is very different from F, maybe every sentence visible could be very useful.

309
00:39:54,850 --> 00:39:56,770
So it's really depends on the quality of.

310
00:39:56,800 --> 00:40:05,290
Q In terms how close this proposed, the density close to the actual restriction or distribution, you like to generate the actual one.

311
00:40:05,410 --> 00:40:11,560
Okay. So you give the proposal, it doesn't mean this is you generate the sample from this density.

312
00:40:11,890 --> 00:40:19,780
It doesn't mean that this generous sample is the way you want. Okay, there you will evaluate whether or not this one is something desirable.

313
00:40:19,990 --> 00:40:24,550
Okay. You evaluate the acceptance probability of a proposal move.

314
00:40:24,850 --> 00:40:30,970
Okay. Given this the you know, the probability.

315
00:40:31,180 --> 00:40:38,020
Okay, so wide or not that you are going to accept this or reject this if this move accepted.

316
00:40:38,230 --> 00:40:46,300
Okay. Then you update your sample if they want general from F to the S, the one you like according to rule of order for this symptom.

317
00:40:47,050 --> 00:40:53,050
There you'll say, okay, I have a new update, otherwise you will stay on the old sample.

318
00:40:53,440 --> 00:40:58,780
Okay, so the old sample, you're you're the one general from proposing the distribution.

319
00:40:59,200 --> 00:41:03,370
The new value side right. Is now the one desirable.

320
00:41:03,380 --> 00:41:06,550
So you just, you know, give it up.

321
00:41:06,730 --> 00:41:10,440
You don't need it because this is not what you want. Okay.

322
00:41:10,450 --> 00:41:16,210
So then your update value will stick and the previous one and the change does not move.

323
00:41:16,810 --> 00:41:25,549
Okay. So, so this is essentially is a basic idea or image of hosting algorithm is proposed to

324
00:41:25,550 --> 00:41:30,580
do solve the problem sometimes that you are not able to sample directly from your,

325
00:41:31,300 --> 00:41:36,820
you know, stationary distribution where your your condition of basic distribution you want.

326
00:41:37,330 --> 00:41:41,590
Okay. So how do you do this? Maybe it is too complicated.

327
00:41:41,930 --> 00:41:47,770
It's too much detail. Probably just want to quickly touched on it.

328
00:41:48,160 --> 00:41:55,210
So the queue can need some kind of this sort of symmetry in the transition kernel.

329
00:41:55,480 --> 00:42:03,549
And then this is to your proposed distribution and then you need to, you know, create this our function as you know,

330
00:42:03,550 --> 00:42:10,629
this a substance acceptance probability in a construction and then you generate a uniform

331
00:42:10,630 --> 00:42:17,200
distribution in the General Assembly from your remote distribution and then see whether or not,

332
00:42:17,230 --> 00:42:27,310
you know, the you will be smaller and bigger than our effort to make a decision whether or not to this are and you move is good move.

333
00:42:27,520 --> 00:42:33,760
Okay. So anyway so they are implemented in the R, so you don't need to know all the details.

334
00:42:33,970 --> 00:42:40,870
But what I'm saying here is that actually the there are some Botero version of game sampler

335
00:42:41,170 --> 00:42:46,930
that like top testing algorithm that helps you to solve some of the difficult cases,

336
00:42:47,260 --> 00:42:50,620
need more time to more patients to generate data.

337
00:42:50,620 --> 00:43:02,710
But people have the solution to deal with the case where the conditional distributions are not like do not have close expression.

338
00:43:03,460 --> 00:43:07,300
So of course that we are interested in having high acceptance rate.

339
00:43:07,300 --> 00:43:12,700
So, you know, about 100 out of 100 proposed moves.

340
00:43:13,090 --> 00:43:18,970
I'm able to accept them 90%. So. So you have very high efficiency of movement.

341
00:43:19,570 --> 00:43:30,700
So. So one naive approach suggestion of literature is really reached this sort of high acceptance rate to make the chain move slowly.

342
00:43:30,810 --> 00:43:33,280
Okay. Don't don't make big jump. Right. So.

343
00:43:33,280 --> 00:43:43,690
So if you make big, big jump, then maybe it's very likely system would not be able to take it as a, you know,

344
00:43:43,840 --> 00:43:53,079
of new new move write an eligible new move so that you move the chain slowly and they take deliberate longer time to converge.

345
00:43:53,080 --> 00:43:57,520
But you have higher acceptance. So how do you create this reversible?

346
00:43:57,910 --> 00:44:02,110
I mean, this kernel, right. The symmetric kernel.

347
00:44:02,110 --> 00:44:06,220
I mean, all time people use normal distribution, use t distribution.

348
00:44:06,250 --> 00:44:10,840
And so on and so forth, start that. So you know that literature, people have done that.

349
00:44:11,290 --> 00:44:21,760
You can have other type of the symmetric kernels that you use, but non motivated, non motivated t are quite popular in practice.

350
00:44:24,400 --> 00:44:34,990
So let me just skip this. And so those for the for the for our implementation, which is, you know,

351
00:44:37,810 --> 00:44:49,300
use this are jacks up the our jacks those this the one that we implemented that you don't like

352
00:44:49,310 --> 00:44:54,160
the black box essentially it's through something into it and produce something for your book.

353
00:44:54,730 --> 00:45:04,090
So the the one who is now in the Google and who would per this and use this our Jackson two for the same.

354
00:45:04,420 --> 00:45:09,610
And so it becomes quite a interesting package.

355
00:45:09,610 --> 00:45:10,650
A lot of people use it,

356
00:45:10,660 --> 00:45:19,810
I think more than a lot of the researchers from different places in the world have used use this our package called extended sample.

357
00:45:20,080 --> 00:45:30,100
Okay. Okay. So I'm just going to next lecture about so I'll give you a little quick introduction.

358
00:45:30,250 --> 00:45:35,379
I'm not going to give you a homework, but if you want to do your project using the M.S.,

359
00:45:35,380 --> 00:45:40,240
if you if you have good knowledge about using M.S., you will come to do this.

360
00:45:40,900 --> 00:45:44,390
I will talk about specialty analysis, use M.S., okay.

361
00:45:44,890 --> 00:45:53,200
And but I just I'm not going to give you a homework for this, but if you want to use it, it's very powerful method.

362
00:45:53,370 --> 00:46:01,790
Okay. So just talk about this, particularly a particular sort of work we did in 2020.

363
00:46:02,780 --> 00:46:11,330
So so that based on Mossad, a discussion paper published in July 2020 on the Journal of Data Science.

364
00:46:11,480 --> 00:46:20,930
And so there were five discussions from five university at Chicago, Johns Hopkins, Berkley.

365
00:46:20,930 --> 00:46:25,559
And so and. Five places.

366
00:46:25,560 --> 00:46:35,400
I don't remember all of them, their effect affiliations, but so discussed this not only the paper itself but discussed the, you know, the, the,

367
00:46:35,580 --> 00:46:47,550
the way we did for this MSI implementation for a particularly the forecast in part because that that time nobody knows what's going to happen next.

368
00:46:47,970 --> 00:46:54,720
So people are very curious about a lot of things and want to use a, you know,

369
00:46:55,860 --> 00:47:03,659
statistical model to simulate some possibilities for the future of the pandemic, you know, but anyway, it's quite a fun work to do.

370
00:47:03,660 --> 00:47:11,549
And when we look at retrospectively now and, you know, it's it's something like interesting.

371
00:47:11,550 --> 00:47:17,760
So the quickly the paper get the quite a bit of citation because this is the one that we

372
00:47:17,760 --> 00:47:25,130
first release a software package for people to trying and to improve or to move them.

373
00:47:25,710 --> 00:47:34,320
And the software has to be used by researchers from more than 20 countries, including our application analysis of US COVID data.

374
00:47:35,040 --> 00:47:44,450
And of course, Professor Mukherjee and his collaborators, including some people sitting here,

375
00:47:44,480 --> 00:47:50,280
have used this kind of idea to to do the prediction, everything.

376
00:47:50,280 --> 00:47:53,939
So they do a much better job than what we did at the beginning,

377
00:47:53,940 --> 00:48:05,219
but that we are the first to try to see how this framework can be formulated in CMC sort of context

378
00:48:05,220 --> 00:48:12,210
and using this powerful simulation based approach to make some forecast and to see what happens.

379
00:48:12,690 --> 00:48:22,700
So originally that we use this thing to analyze the data from China because that's the, the,

380
00:48:22,990 --> 00:48:38,160
the beginning of the pandemic that so so I think we start to analyze data in February 2020 and that was just one and a half months of,

381
00:48:38,940 --> 00:48:44,280
you know, sort of into the early stage of the the outbreak of the disease.

382
00:48:44,280 --> 00:48:48,270
So so we have very, very limited data at that moment.

383
00:48:48,660 --> 00:48:51,710
But people are curious what's going to happen next. Yeah.

384
00:48:52,890 --> 00:48:56,640
So in this work, we want to achieve three aims.

385
00:48:56,760 --> 00:49:06,329
First, all we want to use publicly available data from China and CDC to understand the twin of coronavirus pandemic in Hubei province.

386
00:49:06,330 --> 00:49:18,780
That's basically the province has the, you know, the reported three new COVID cases and most severe infection within China.

387
00:49:19,470 --> 00:49:25,860
So that gives us a better quality of data to test this model.

388
00:49:25,860 --> 00:49:35,639
And so then what we did there is really trying to see how we can improve the from the original mode

389
00:49:35,640 --> 00:49:43,350
company model by incorporating some time during quarantine protocols in the model of infection dynamics.

390
00:49:43,590 --> 00:49:54,030
Right. So because, you know, remember that the early time there was quite a bit this control measures imposed by

391
00:49:54,030 --> 00:49:59,879
Chinese government and how to mimic that kind of policy in the control of the disease.

392
00:49:59,880 --> 00:50:04,950
It becomes a, you know, a challenge in this kind of modeling.

393
00:50:04,950 --> 00:50:10,499
And because original serum, all SDR model, all, you know,

394
00:50:10,500 --> 00:50:20,240
build upon the constant infection rate and recovery rate and certainly that how do you modify that the classical CI,

395
00:50:20,250 --> 00:50:27,630
our Model S and Model three to to to reflect actually real reality.

396
00:50:28,020 --> 00:50:34,110
And to that that was unclear to us. So basically we trying to achieve that.

397
00:50:35,130 --> 00:50:40,080
And I think most important contribution from this work is to really provide a house informatics

398
00:50:40,320 --> 00:50:46,290
toolbox while they are software to populate for their own analysis of disease spreading patterns.

399
00:50:46,920 --> 00:50:50,969
Okay. So, so that's something we want to see.

400
00:50:50,970 --> 00:51:00,150
And so what are we we, we have seen this particular work is that we use the proportions rather than counts.

401
00:51:00,570 --> 00:51:10,500
So we use model time series of observed like minute proportions of in fact that cases and remove cases.

402
00:51:10,860 --> 00:51:22,870
So to our compartment are consist of both recovered cases and emitting from under non Markov ESR infectious disease dynamics.

403
00:51:22,870 --> 00:51:25,830
So that's. Exactly what I introduced the beginning of this course.

404
00:51:26,730 --> 00:51:36,820
Then what I haven't talked about so far is how we estimate the model parameter and make a forecast based on this McKinsey algorithm.

405
00:51:36,900 --> 00:51:39,750
Okay, so what do we really want to achieve?

406
00:51:39,750 --> 00:51:53,760
There is not only just obtained the point estimate that we really want to make forecasting and, you know, evaluate the uncertainty of the forecast.

407
00:51:53,850 --> 00:51:57,719
Right. So not just give a one point prediction.

408
00:51:57,720 --> 00:52:01,980
You want estimate the so the the prediction interval.

409
00:52:02,250 --> 00:52:05,700
So what so that people understand the the uncertainty on that.

410
00:52:06,150 --> 00:52:11,310
So that's something we like to we plan to do in the very beginning.

411
00:52:11,460 --> 00:52:22,920
And so that so we need to use some sort of the statistic toolbox algorithm to allow us to really work on this prediction uncertainty.

412
00:52:23,670 --> 00:52:26,730
Okay. So what we did there is trying to, you know,

413
00:52:26,910 --> 00:52:33,129
derived posteriors model parameters and predicted turning points with respect

414
00:52:33,130 --> 00:52:37,650
to credible intervals because that time nobody knows what's going to happen.

415
00:52:38,130 --> 00:52:43,200
And just like the third months after outbreak of the disease,

416
00:52:43,200 --> 00:52:50,909
people want to know when this this whole holy fashion will be peak, that when this will, you know, decay to zero.

417
00:52:50,910 --> 00:52:57,600
We die down to zero. I mean, people really want to know what are the turning points and so on.

418
00:52:58,140 --> 00:53:05,480
So, so, so there were some kind of very critical questions that we'd like to answer by one model.

419
00:53:05,500 --> 00:53:09,660
It is. So finally we created our package ESR model.

420
00:53:11,790 --> 00:53:19,410
So this is very, very familiar sort of model, sort of structure that you have seen before.

421
00:53:19,860 --> 00:53:29,849
So here that the why here is not current is the really the the proportions here because I

422
00:53:29,850 --> 00:53:37,200
said that proportions will allow us to just the population size in the proximal regression.

423
00:53:37,200 --> 00:53:41,000
You need to really work on some kind of, you know, offset.

424
00:53:41,040 --> 00:53:44,040
But if you look at proportions that you don't need to do that.

425
00:53:44,210 --> 00:53:55,590
Okay. So you work on the prevalence of of the so that is more stable actually in the in the model in the forecast.

426
00:53:55,750 --> 00:54:03,140
Okay. So here is the model you have to observe, process the observed number of in fact, the cases for,

427
00:54:03,150 --> 00:54:09,330
you know, the proportion of in fact, cases and proportion of removed cases from system.

428
00:54:09,390 --> 00:54:17,190
Okay. This is very early at work. So what do we really want to achieve here is really to that time to see how we are

429
00:54:17,190 --> 00:54:22,200
going to incorporate time during quarantine protocols into this multi compare model.

430
00:54:22,440 --> 00:54:31,170
Okay. So that we can predict the future rally to see the, you know, the effectiveness of quarantines.

431
00:54:31,300 --> 00:54:36,620
Okay. In the future. Okay. So this is actually model, you know.

432
00:54:36,670 --> 00:54:46,950
Right. So this this is our model, you know, and so you have to sample infection and remove and this of,

433
00:54:47,670 --> 00:54:51,870
you know, three compartment dynamic model described by this,

434
00:54:52,500 --> 00:54:55,239
the system of three differential equation is, you understand,

435
00:54:55,240 --> 00:55:04,650
theta is the disease transmission rate and gamma is the removed rate of this ratio is, you know, production number.

436
00:55:04,980 --> 00:55:13,350
Okay, so just skip there. So, so what do we did in the model here is really used the better distribution goes to Y is proportion.

437
00:55:13,680 --> 00:55:22,430
So so why is the one that we observe in terms of proportion of, in fact, the cases data?

438
00:55:22,950 --> 00:55:31,589
Mm hmm. And so we use a better distribution to model the the proportion of, in fact,

439
00:55:31,590 --> 00:55:37,320
cases and not repeat the distribution to model the proportion of removed the cases.

440
00:55:37,710 --> 00:55:48,410
Those are two things are, you know, no, of course, that if you know the population that you know, you would know y t s which will be cattle.

441
00:55:48,420 --> 00:55:54,960
And mine in this case will be one minus, because y t.i and y tr are proportions.

442
00:55:55,080 --> 00:55:59,550
Okay. Okay. So then you have your late in process data.

443
00:56:00,030 --> 00:56:13,500
Okay. I see that Ty is the latent proponent that the prevalence of infection and theta t r is the underlying proportion of remove.

444
00:56:13,800 --> 00:56:22,440
Okay. So that you model this underlying process by, you know, a mark of process.

445
00:56:22,830 --> 00:56:29,440
Here we use the original distribution because directly because data here is also a proportion print.

446
00:56:30,000 --> 00:56:32,910
So we use the original distribution.

447
00:56:33,120 --> 00:56:45,570
The original distribution is multivariate beta distribution here that you have the know the proportion of people in this susceptible compartment.

448
00:56:46,800 --> 00:56:57,820
Proportion of people in the in fact if in fact just compartment and they t and you know of proportion of people like uh,

449
00:56:58,320 --> 00:57:03,570
left from, you know, in the removal compartment from this infected system.

450
00:57:03,960 --> 00:57:07,340
So that's the one we create. So they are correlated.

451
00:57:07,710 --> 00:57:11,790
This three underline proportions are correlated.

452
00:57:12,270 --> 00:57:21,680
They should, you know, follow a three dimensional by an all beta distribution, which is the original distribution.

453
00:57:22,740 --> 00:57:30,840
So of course that if you in the specification of the originally distribution, you really need to bring in the dynamics, right?

454
00:57:31,050 --> 00:57:36,090
So how they say to those proportions evolve over time.

455
00:57:36,420 --> 00:57:43,770
So that's the place where we bring this theater through this load compartment model.

456
00:57:44,040 --> 00:57:48,960
Okay, so theta here is the solution.

457
00:57:49,380 --> 00:57:54,770
Okay. They are the mean of this directional distribution.

458
00:57:54,780 --> 00:58:05,250
Describe the average pattern, the dynamic pattern from this past time to current time.

459
00:58:05,520 --> 00:58:14,579
You know, that's and this F state of T is nonlinear as you can see that in the mode come from.

460
00:58:14,580 --> 00:58:21,479
All right. So you have this s is going down or your theta.

461
00:58:21,480 --> 00:58:27,660
I would be like this situation and then you you are compartment is always increase.

462
00:58:27,720 --> 00:58:31,530
So those are the nonlinear functions, right?

463
00:58:31,530 --> 00:58:39,980
So the nonlinear functions that you have seen already that are actually a model

464
00:58:39,990 --> 00:58:45,540
by this free differential equation which can be obtained by this explicit.

465
00:58:45,870 --> 00:58:56,010
Right. So the this is approximation a numerical solution of the, the three, you know, differential equation in terms of,

466
00:58:57,630 --> 00:59:05,190
you know, how, how you use the branch of cutoff approximation to solve this the system of differential equation.

467
00:59:05,580 --> 00:59:16,410
So we can say that here you have this the proportion of the three proportions from yesterday plus some additional terms.

468
00:59:16,990 --> 00:59:20,670
Okay. So this really defines Markov process.

469
00:59:20,990 --> 00:59:30,690
So, so how the history affects the current the value of theater T through this directly distribution.

470
00:59:31,260 --> 00:59:35,110
So this is a little more than just what you did for this Nina regression.

471
00:59:35,130 --> 00:59:38,910
All right. So we did this in this station erm process.

472
00:59:38,910 --> 00:59:46,290
Right. So we have this BTC to t minus one plus its own T.

473
00:59:46,320 --> 00:59:52,740
Right. So this is the one you did the this error one gamma process.

474
00:59:53,010 --> 01:00:02,010
Very simple. Well here that the this conditional distribution is coming from two really distribution with

475
01:00:02,010 --> 01:00:09,430
a nonlinear transition structure defined by this system of order of differential equation.

476
01:00:09,480 --> 01:00:16,640
Yeah. Is there any way any corporate plug in this is going to potentially increase in length some covariance?

477
01:00:17,400 --> 01:00:20,970
Is there any way that it does not in any given period?

478
01:00:21,180 --> 01:00:25,200
Yes. I mean, it's but like if you want to incorporate some of it here.

479
01:00:25,200 --> 01:00:31,469
So that's an open problem actually, because you feel you incorporate some covers,

480
01:00:31,470 --> 01:00:37,460
then you probably will destroy the some of this sort of the nice properties.

481
01:00:37,890 --> 01:00:41,640
Yeah. So, so off. I don't know how to do this.

482
01:00:42,120 --> 01:00:51,520
This is open problem. And maybe we have to extend this differential equation to something more flexible.

483
01:00:51,540 --> 01:01:00,210
I don't know how to do that. I wish I could have more knowledge in all the interactive for each equation to see how the can be extended.

484
01:01:00,960 --> 01:01:13,740
And so. But anyway, now we are doing very, very incremental extension and that's already a quite a big sort of impact in many aspects.

485
01:01:13,740 --> 01:01:18,180
But I wish that you can incorporate some of vaccination into this.

486
01:01:19,050 --> 01:01:22,650
And so typical of people trying to incorporate.

487
01:01:22,850 --> 01:01:28,310
Vaccination in an additional compartment rather than a cover into this process.

488
01:01:29,540 --> 01:01:34,489
So yeah, I wish I could spend more time working on this whole thing.

489
01:01:34,490 --> 01:01:39,680
So but anyway, so that's a very open problem in this field.

490
01:01:39,830 --> 01:01:46,370
How do you incorporate Cook covers into this?

491
01:01:46,370 --> 01:01:55,340
And some people are trying to work on a little bit like non non parametric solution on this.

492
01:01:55,490 --> 01:02:08,200
Okay, so you want to do same see I'm CMC is actually built upon the you know by the Bayesian model you need to specify priors right.

493
01:02:08,270 --> 01:02:11,420
So, so you need to space the initial conditions.

494
01:02:11,870 --> 01:02:19,250
So what we're doing here for initial conditions use the first sort of observe the data to you know,

495
01:02:19,430 --> 01:02:30,540
specify initial condition for those are model parameters like that reproduction number in this recovery rate and the,

496
01:02:31,550 --> 01:02:41,120
you know, transmission rate at that time, like in February 2020, we don't have any idea how those values should be set up.

497
01:02:41,570 --> 01:02:51,020
So we looked at, you know, the source data from Hong Kong and then we just just trying to estimate because we believe that at that time this,

498
01:02:51,410 --> 01:02:57,740
you know, COVID 19 is some is a type of coronavirus similar to SA.

499
01:02:57,830 --> 01:03:02,360
So so we're, we're trying to see whether or not we can reverse some, you know,

500
01:03:02,630 --> 01:03:08,450
transmission rate and or some recovery rate and from the data out there.

501
01:03:08,450 --> 01:03:15,409
So it turns out that actually SARS data is a little bit more severe than the COVID 19.

502
01:03:15,410 --> 01:03:27,230
You can see that the data estimate here, one over .1.0117 is almost like what is a 35 or so.

503
01:03:27,740 --> 01:03:38,000
So basically people need 35 to move out from infected case to to to, you know, to our compartment.

504
01:03:38,210 --> 01:03:51,320
Now, I think for COVID 19 usually takes ten days or or will 15 days to two weeks roughly to recover from the infection.

505
01:03:51,620 --> 01:04:03,410
But that time we don't have that. And also the the the average R0 is 3.15, which maybe is a little bit underestimate the actual infection rate.

506
01:04:04,100 --> 01:04:11,480
But anyway, those are the the nowadays we can have much better use of Atlas than the time we were colonies.

507
01:04:11,660 --> 01:04:20,930
Okay so we use diffuse prior put a lot of you know small values here to priors becomes very diffused.

508
01:04:21,050 --> 01:04:25,460
Okay. Those are the way we, you know, specify the priors.

509
01:04:25,700 --> 01:04:32,180
Okay. So what do we really want to do here is for the the prediction.

510
01:04:32,300 --> 01:04:46,730
Okay. So suppose t0 is the last date of the data availability that that time we only have like the data over like from like four or five weeks or so,

511
01:04:46,850 --> 01:04:52,100
we have very, very limited data on time and t zero will be last date of data availability.

512
01:04:52,940 --> 01:05:00,020
The forecast spans over a period of capital t days from T zero plus one to capital T days.

513
01:05:00,470 --> 01:05:08,630
That's the goal we want to do prediction. So now in this context of M.S., we have posteriors.

514
01:05:08,640 --> 01:05:15,830
We can draw M, we first take M draws after bringing from the the training model,

515
01:05:16,250 --> 01:05:27,200
we use this available data to train our this SRM model with this sort of the states states this model structure.

516
01:05:27,650 --> 01:05:30,799
Okay, use our two X to twin.

517
01:05:30,800 --> 01:05:38,060
The model where we need to tell actually is the parameter of the model and the model I just discussed.

518
01:05:38,060 --> 01:05:41,180
And then you use available data to twin this. Okay.

519
01:05:41,480 --> 01:05:49,610
Then you can actually get that estimate at the center of dynamics from one to the T zero.

520
01:05:50,900 --> 01:06:03,680
Now as I say that after this, right after you, you you generate all of the values from this system for each general of M values.

521
01:06:04,250 --> 01:06:11,030
You can go forward to make a prediction because every value after burning will be regarded as eligible value.

522
01:06:11,900 --> 01:06:16,160
That will be the value that you could use as the basis for prediction.

523
01:06:16,580 --> 01:06:20,300
So if you generate M values from this system, right.

524
01:06:20,480 --> 01:06:26,060
So then you could, you know. Use each of them to make a forecast.

525
01:06:26,400 --> 01:06:38,270
Okay. So so for each solution pass you have, you know, capital M out there we explained this s our trajectory in the future of t days.

526
01:06:38,930 --> 01:06:49,520
But M seems the algorithm basically you continue to draw from this latent Markov process using this directional distribution to draw this.

527
01:06:49,670 --> 01:07:00,410
Okay. So then after you draw the, you know, the theta t, then you use a meeting probability to do all proportion of your Y,

528
01:07:00,830 --> 01:07:07,250
particularly you're interested in the proportion of infections, the proportion of re removed from the system.

529
01:07:07,550 --> 01:07:17,660
So that's the one you can you can do. So after that, you, you know, you can you can use put you are to draw this and then you can,

530
01:07:17,660 --> 01:07:23,740
you know, uh, obtained the, uh, the one from summary statistic.

531
01:07:23,900 --> 01:07:26,060
Okay. So that's basically what we did.

532
01:07:26,090 --> 01:07:36,810
So suppose you observe data, the proportion of infection, proportion of, uh, removal from the surveillance system.

533
01:07:37,580 --> 01:07:41,270
Then you can use this data to train your model.

534
01:07:41,270 --> 01:07:48,140
I just describe beta beta and to restore it using C to do that draw in same.

535
01:07:49,160 --> 01:07:57,620
Okay. Then after that, for each solution pass, you can create a future of loss rate.

536
01:07:57,630 --> 01:08:09,140
So, you know, do a lot of things and then you can create a, you can draw this of multiple points.

537
01:08:09,650 --> 01:08:16,730
And so that could help you to get different type of, you know, solution pass.

538
01:08:17,060 --> 01:08:26,150
Okay. So then from each of this solution pass that you, you know, you summarize them using like the meaning or median or something like that.

539
01:08:27,110 --> 01:08:33,890
So you can summarize this multiple draws because you see them as they allow you to draw them off to breed.

540
01:08:33,970 --> 01:08:37,280
Right, to have em. Different solution pass. Okay.

541
01:08:39,710 --> 01:08:44,420
So, so then after that, you can, you know, make a prediction.

542
01:08:44,600 --> 01:08:47,780
Okay. To to to the feature about us.

543
01:08:48,030 --> 01:08:51,960
Yeah. So that's that's basically the idea.

544
01:08:52,040 --> 01:08:59,330
You you do it. And what you're interested here is, of course, to fi some key turning point.

545
01:08:59,780 --> 01:09:05,599
The first turning point is to really figure out the, the, uh, you know,

546
01:09:05,600 --> 01:09:13,280
this so zero acceleration point essentially is the time that the data increment of the,

547
01:09:13,280 --> 01:09:18,770
in fact, cases will or proportions will be smaller than previous days.

548
01:09:19,400 --> 01:09:23,300
So that's the time that you can start to see slow down of the process.

549
01:09:23,630 --> 01:09:31,940
Okay. So that's the the point very the first turning point when you see the acceleration becomes zero.

550
01:09:31,970 --> 01:09:42,560
So after that point that the system becomes a system where, you know, there's the policy.

551
01:09:42,890 --> 01:09:51,260
Some some some someone pressed a break on the system because your your you still have number of the new cases,

552
01:09:51,260 --> 01:09:56,180
but the number of new cases will be smaller than previous days.

553
01:09:56,540 --> 01:10:02,510
So so that basically system is trying to use a brick, you know, to slow down.

554
01:10:02,570 --> 01:10:05,650
So the acceleration becomes deceleration.

555
01:10:05,660 --> 01:10:14,150
That's the first turning point. You can use this multiple solution past to estimate this turning point.

556
01:10:14,180 --> 01:10:20,389
That's exactly D the the second order derivatives that you'd like to I mean, all were, you know,

557
01:10:20,390 --> 01:10:31,700
the the peak of the first order interrupt if you like to figure out what's the time at which the first order to out becomes the maximum at the peak.

558
01:10:32,330 --> 01:10:37,130
So after that time point that the system becomes a deceleration system.

559
01:10:37,400 --> 01:10:45,650
Okay. So that's the first one. The second one is, of course, this zero speed, in fact.

560
01:10:45,860 --> 01:10:55,160
And so when this speed becomes zero. So that's the case where you see the hope of the, you know, the end of this pandemic.

561
01:10:55,340 --> 01:10:55,610
Okay.

562
01:10:55,910 --> 01:11:05,300
So that's basically the time that you use the first first order to to to estimate the zero day time at which the first order corrected becomes zero.

563
01:11:06,620 --> 01:11:14,630
That's the basically the the case that essentially that your AI compartment will

564
01:11:14,870 --> 01:11:21,649
always become the AI compartment always become smaller and smaller because you know,

565
01:11:21,650 --> 01:11:29,320
so. So you know that basically you are cleaning up your eye compartment after that point.

566
01:11:29,710 --> 01:11:36,370
So that's a very important thing. So here is the tip.

567
01:11:36,700 --> 01:11:49,090
You know, do forecast that we did. Okay. So we have data up to sort of the early march and then we of February 27th that we would you know,

568
01:11:49,180 --> 01:12:00,700
we only had about two, two months data from one half from January 13, 2020, up to, you know, the end of February.

569
01:12:00,730 --> 01:12:04,960
Then we want to predict what's going on using this model we proposed.

570
01:12:05,140 --> 01:12:17,080
Okay. Here you have the mean and the median prediction, and then you have the the sort of the confidence spans.

571
01:12:17,500 --> 01:12:21,760
Okay. Generally from this different solution pass.

572
01:12:21,910 --> 01:12:27,520
Okay. So then you predict this sort of the this is the case result of intervention.

573
01:12:27,520 --> 01:12:34,220
If you do not have intervention, what do do what on average, what you will see about the situation?

574
01:12:34,250 --> 01:12:39,970
It seems that the pandemic never will, never would never end.

575
01:12:40,090 --> 01:12:44,410
So there's always quite a bit the infection.

576
01:12:45,310 --> 01:12:48,440
So now how about the learning process?

577
01:12:48,680 --> 01:12:57,340
Those are the data points here attended of in the year from January 13th to the end of February.

578
01:12:58,060 --> 01:13:04,330
You know, the green one tells you how this small do we propose to learn the the pattern

579
01:13:04,330 --> 01:13:10,780
of this you can clear see that the the training is pretty well done right.

580
01:13:10,810 --> 01:13:17,740
You can see the green one that that gives you the McKinsey twinning for this model that we proposed.

581
01:13:18,160 --> 01:13:23,650
And it covers quite a nicely this pattern of those data points we observed.

582
01:13:24,400 --> 01:13:25,660
Now, after you move,

583
01:13:25,820 --> 01:13:33,340
I'll said this from this green part that you basically doing the prediction that there is a lot of uncertainty in this dynamic system,

584
01:13:33,340 --> 01:13:40,300
that you can see that this the interval range is very wide.

585
01:13:40,540 --> 01:13:50,859
So you can clearly see that if you want to do long term prediction, this is very, very risky business to do it because they have a lot of uncertainty.

586
01:13:50,860 --> 01:13:54,710
But if you do a short term prediction, maybe it's doable.

587
01:13:54,720 --> 01:13:57,370
And for long term, is this hard? Okay.

588
01:13:58,990 --> 01:14:14,400
So that's that's something we analyzed and the data we you know, and later on, we calculate the our zero and and all of those that is.

589
01:14:14,410 --> 01:14:26,620
Okay. So but of course, that the you know, that at the end of January, there was a severe so you know that there's a strong control measures coming.

590
01:14:26,620 --> 01:14:33,790
And Chinese government imposed a lot of control measures late January.

591
01:14:33,790 --> 01:14:43,420
And so so now we're really thinking about like how you're going to incorporate this control measure into this system.

592
01:14:44,050 --> 01:14:55,480
And then, you know, of course, that this is, you know, what happens here in the import in terms of imposed this sort of control measures.

593
01:14:55,590 --> 01:14:59,889
Okay. So how do you try to you know, those are milestones, right?

594
01:14:59,890 --> 01:15:02,170
So happened in the Hubei province.

595
01:15:02,680 --> 01:15:11,980
So the question here is, how do you incorporate those interventions into this more comparable model, which seems to these quite announced before it?

596
01:15:12,130 --> 01:15:17,890
Okay. So what we did here is really just introduce this sort of pilot function.

597
01:15:17,890 --> 01:15:25,030
We call this transmission modifier as basically how many people are actually in the in home quarantine.

598
01:15:25,030 --> 01:15:35,540
When you have people who are, you know, in home quarantine, you reduce the size of how does it reduce the size of S compartment.

599
01:15:35,560 --> 01:15:37,830
They become part of susceptible individuals.

600
01:15:37,840 --> 01:15:48,010
If people staying at home not going anywhere, they are basically isolated themself from being infected or having almost zero chance to be infected.

601
01:15:48,490 --> 01:15:51,190
And also that for the in fact,

602
01:15:51,190 --> 01:15:59,079
the people they build up some specialized a hospital to hold those in fact the people that those in fact that people in fact just

603
01:15:59,080 --> 01:16:09,380
people would not be able to walk down the street freely so that the chance of of infection from infectious individuals will reduce.

604
01:16:09,400 --> 01:16:13,350
So we're thinking about this. How do you incorporate that into the model?

605
01:16:13,360 --> 01:16:21,970
Essentially, it can be summarized as a sort of transformation modifier that will reduce basically the.

606
01:16:22,440 --> 01:16:26,040
So originally maybe you have five transmission rate,

607
01:16:26,340 --> 01:16:37,909
but after you do this quarantine in home quarantine or this sort of well-organized hospitalization to find the,

608
01:16:37,910 --> 01:16:44,040
in fact, infectious individuals and put you to hospital, that can reduce this transmission rate quite a bit.

609
01:16:44,070 --> 01:16:46,140
That's what we're thinking at this moment.

610
01:16:46,650 --> 01:16:56,520
So that would build up this pretty into this more comparable model and see how this policy positive rate could affect this prediction.

611
01:16:56,670 --> 01:17:03,480
So we can come up with a different time of time. You can come up with different types of this intervention.

612
01:17:03,930 --> 01:17:09,190
You can have this sort of a we call macro isolation measures.

613
01:17:09,210 --> 01:17:21,510
Do you have one policy that everyone would do this and it changed completely in the way of, you know, the the the the activity people can do.

614
01:17:22,050 --> 01:17:31,980
And so those are sort of the big strong shots from the outside of the system to change this sort of infection rate by somebody with time points.

615
01:17:32,070 --> 01:17:33,360
That's exactly what we do.

616
01:17:33,600 --> 01:17:42,830
Or you can do this micro system where it probably is more like the case in U.S., where people have increased awareness of disease.

617
01:17:43,170 --> 01:17:45,810
They learn how to protect themselves and so, so forth.

618
01:17:46,140 --> 01:17:55,770
So you can have a a smooth decay the way people learn more and people are become more conscious or,

619
01:17:55,770 --> 01:18:00,809
you know, aware of aware of this, you know, the protection they are waiting to,

620
01:18:00,810 --> 01:18:04,230
you know, do something like wear a mask or, you know,

621
01:18:04,680 --> 01:18:15,880
to be socially distancing and to protect themselves rather than having this very strong and discreet things.

622
01:18:15,900 --> 01:18:24,750
Okay. So that after you do this, this is the case with no inter inter sort of intervention.

623
01:18:25,200 --> 01:18:27,870
But if you do intervention. Okay.

624
01:18:28,230 --> 01:18:39,629
So like this, the micro intervention, like if you impose this kind of policy to increase the awareness of of self protection,

625
01:18:39,630 --> 01:18:45,300
we're being social distancing or we're masking or, you know,

626
01:18:45,630 --> 01:18:55,290
reduce this person to person contact using this gradual sort of a fashion of, you know, intervention.

627
01:18:55,290 --> 01:18:59,880
Then you can see that if you take that policy, then here is the predicted.

628
01:18:59,940 --> 01:19:03,270
So you do see the hope that this can be controlled.

629
01:19:03,690 --> 01:19:07,980
So if you do not do anything, this is one with no intervention.

630
01:19:08,550 --> 01:19:11,820
We have forecast of what's going to happen in future.

631
01:19:12,380 --> 01:19:17,670
There it doesn't seem to be going to to seem to end soon.

632
01:19:18,120 --> 01:19:27,150
But if you do this, if you impose this policy of intervention, you will see that sometimes the you know, this is going to end.

633
01:19:27,720 --> 01:19:40,740
If you do this kind of a macro intervention, take a strong sort of control of the restriction of activities so that this is the protected situation.

634
01:19:40,990 --> 01:19:47,820
It has more uncertainty. But eventually, the this thing will will die down.

635
01:19:48,810 --> 01:19:55,560
So those are very useful things. And the very early stage when, you know, there is no vaccine, there is nothing,

636
01:19:55,560 --> 01:20:03,690
and people see what policy would be a useful policy if we use policy, what we're going to see in next three months.

637
01:20:03,920 --> 01:20:06,210
We're use policy be what we're going to see.

638
01:20:07,140 --> 01:20:15,610
You know, so this is a simple modification using this transmission modifier under different policy construct.

639
01:20:16,200 --> 01:20:19,440
And to say effect only the forecast.

640
01:20:19,710 --> 01:20:24,900
So that's why this becomes somewhat popular. And that time maybe now is not that useful.

641
01:20:24,900 --> 01:20:27,750
But I'm just talking about the idea.

642
01:20:28,020 --> 01:20:39,450
And this statistic is statistical toolbox M.S. that helps us to achieve something people have could not figure out how to do at that time anyway.

643
01:20:39,450 --> 01:20:46,230
So so how do we extend this? You know, that will be really something interesting to figure out.

644
01:20:46,350 --> 01:20:49,650
And so time's up.

645
01:20:49,890 --> 01:20:56,610
Should like lecture know since I have always already if you have any question about your data capture.

646
01:20:57,180 --> 01:21:01,920
I'll be happy to talk to you. 330, you know, that's it.

647
01:21:07,600 --> 01:21:10,620
From South Africa?

648
01:21:11,490 --> 01:21:16,800
Yes. Doing commercial.

