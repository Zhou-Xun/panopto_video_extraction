1
00:00:00,600 --> 00:00:05,339
Okay. Before we get started here, you and I were looking through your homework assignments.

2
00:00:05,340 --> 00:00:10,230
I think those were done really well. Thank you. For all of you who asked questions for me,

3
00:00:10,920 --> 00:00:17,220
I realized the two by two table with like the odds ratios was a bit confusing versus what I presented in class.

4
00:00:18,030 --> 00:00:20,579
So definitely change that for a future semester.

5
00:00:20,580 --> 00:00:26,610
But it's also good just to keep in mind that when people do set up a two by two table like where the disease is,

6
00:00:26,610 --> 00:00:29,849
where the exposure is, those can be like literally flipped or in different order.

7
00:00:29,850 --> 00:00:33,809
So just keep that in mind. Okay.

8
00:00:33,810 --> 00:00:46,080
So the two biggest questions that we had were people were wondering about risk and prevalence ratios in a cohort study.

9
00:00:46,350 --> 00:00:48,990
So this is a cohort study, as we mentioned.

10
00:00:52,740 --> 00:01:01,560
And so the thing that we mentioned in this study is that we excluded people who had Type two diabetes at baseline.

11
00:01:02,310 --> 00:01:10,680
So how a prevalence ratio would work is that at that baseline value we would compare coffee drinking habits and type two diabetes prevalence.

12
00:01:11,010 --> 00:01:19,020
But since in this dataset we excluded people at type two diabetes, it would be wrong to say that we can calculate prevalence ratios.

13
00:01:19,860 --> 00:01:23,879
At the same time, I think some of you, you know, like wrote good explanations of that.

14
00:01:23,880 --> 00:01:28,350
So we did give, you know, some partial points back depending on how you wrote that.

15
00:01:30,270 --> 00:01:36,570
But yeah, so again, statistically, risk ratios and prevalence ratios are the same.

16
00:01:36,900 --> 00:01:44,430
You just need to look through how they're defined in this dataset, how we define diabetes and how we define coffee.

17
00:01:44,430 --> 00:01:51,360
Drinking is indicative of a risk ratio because the diabetes is diagnosis.

18
00:01:51,360 --> 00:01:54,360
During the study period was coffee is our study baseline.

19
00:01:55,050 --> 00:01:59,580
So that's kind of the time difference. That's the other thing.

20
00:01:59,580 --> 00:02:06,600
And we didn't take up any points for this, but sometimes when people were talking about risk difference or a prevalence difference,

21
00:02:06,960 --> 00:02:12,120
they put like a percentage for the estimate, but keep the intervals as decimal points.

22
00:02:12,450 --> 00:02:14,159
That's not mathematically wrong.

23
00:02:14,160 --> 00:02:22,049
You know, we got this right, but I generally for a risk difference, I would just put everything as a percentage similarly for like a rate difference.

24
00:02:22,050 --> 00:02:30,360
I'd put everything as like per person years and multiply that up and then have the estimate in the intervals be on the same with the same denominator.

25
00:02:33,900 --> 00:02:44,850
Okay. So this with, um, how many hours did you spend on nurse to make sure I'm still recording?

26
00:02:44,850 --> 00:02:50,429
Guess how many hours you spend on homework? This is sort of the distribution I would like to see.

27
00:02:50,430 --> 00:02:55,799
You know, I think spending 1 to 3 hours is adequate most of the time.

28
00:02:55,800 --> 00:03:00,450
I think that if you are in this range, like getting above 4 hours,

29
00:03:00,840 --> 00:03:09,120
that's definitely a point in time where maybe you should be setting up a time to meet with me to talk about maybe there's something you're doing,

30
00:03:09,120 --> 00:03:10,400
which is it's kind of inefficient.

31
00:03:10,770 --> 00:03:18,570
Maybe this is at the point in time if you're just like really stuck with the coding to send up an email to me or you and or set up again,

32
00:03:18,930 --> 00:03:21,180
come to our office hours just because, you know,

33
00:03:21,180 --> 00:03:27,299
we're not this class isn't designed to make you have to really like spend so much time with the homework.

34
00:03:27,300 --> 00:03:34,740
But I do think like 1 to 3 hours is kind of in general what I see for most students across different terms.

35
00:03:37,010 --> 00:03:42,800
Okay. This has to do with an attributable fraction.

36
00:03:43,220 --> 00:03:46,340
I say we found the influenza vaccine to be 60% effective.

37
00:03:47,510 --> 00:03:51,020
Again. Again, that's an attributable fraction in a case control study.

38
00:03:51,020 --> 00:03:53,060
Which of the following is likely true?

39
00:03:53,600 --> 00:04:01,280
And about half of you got it so that the odds of having influenza was four times as high in the vaccine group compared to the unvaccinated group.

40
00:04:01,850 --> 00:04:07,610
And that's just because things like a vaccine effectiveness, which is an example of an attributable fraction.

41
00:04:07,850 --> 00:04:11,750
The formula for that is one minus an odds ratio or one minus the risk ratio.

42
00:04:12,110 --> 00:04:15,979
And in for case control studies, we predominantly have odds ratios.

43
00:04:15,980 --> 00:04:17,780
So that's why what they think would be correct here.

44
00:04:18,410 --> 00:04:24,980
But, you know, there could be some examples where we do a case control study where the odds ratio approximates the risk ratio.

45
00:04:25,370 --> 00:04:30,760
So maybe this could be correct in some circumstances for like a test,

46
00:04:30,770 --> 00:04:35,480
I would probably provide like a bit more background, but I think this naturally seems like a bit.

47
00:04:39,110 --> 00:04:42,410
The best questions.

48
00:04:45,010 --> 00:04:48,490
Okay. What is not a good reason to use the odds ratio again.

49
00:04:48,610 --> 00:04:54,669
Not take. Questions are always hard to deal with, but it is an appropriate measure for case control studies.

50
00:04:54,670 --> 00:04:58,479
That is true in cohort studies.

51
00:04:58,480 --> 00:05:03,100
With rare outcomes you can approximate a risk ratio that is true.

52
00:05:03,700 --> 00:05:08,440
It can be a measure of last resort if there are insurmountable convergent issues with other models.

53
00:05:08,830 --> 00:05:13,760
I would say that's true and then fail because the formula, it appears to have larger effect sizes and risk ratio.

54
00:05:13,790 --> 00:05:16,840
So I would say that's not a good reason. So this is what I'd go with.

55
00:05:17,080 --> 00:05:23,170
Again, I realize this question was like a bit complicated, but if you get that, these top three ones are true.

56
00:05:23,620 --> 00:05:31,210
This bottom line is, again, it's true, but it's not a good reason to use the the odds ratio.

57
00:05:33,700 --> 00:05:38,740
And again, it's not that it's biased or anything, it's just that it looks like it's more exaggerated.

58
00:05:39,190 --> 00:05:43,180
So a layperson looking at it would just think that an odds ratio shows the stronger effect.

59
00:05:46,590 --> 00:05:51,270
Okay. Collapsed ability. Um, this is the answer.

60
00:05:51,270 --> 00:05:54,870
So the total effect size is somewhere between the stratas Pacific effect sizes.

61
00:05:56,550 --> 00:06:01,140
And it could be that the total effect size equals the stratas specific effect sizes.

62
00:06:01,140 --> 00:06:06,810
Because again, if the strata effect sizes are the same, then the total effect size would by necessity have to be in between.

63
00:06:06,810 --> 00:06:12,060
But that's not always the case. So collapse ability is just kind of like different than confounding.

64
00:06:12,060 --> 00:06:22,290
I mean, it like relates to confounders, but it, you know, collapse ability is something that we study in the presence of confounding.

65
00:06:27,700 --> 00:06:36,550
Okay. So if we are depicting things for a difference, we actually should be using a normal axis.

66
00:06:36,910 --> 00:06:42,910
The log transforms axis or axes. We use four ratio measures.

67
00:06:43,420 --> 00:06:49,690
So like odds ratios, risk ratios, rate ratios, hazard ratios there we would use a log transfer to access that.

68
00:06:49,690 --> 00:06:54,160
We don't need to do that for a risk difference or a prevalence difference.

69
00:06:54,850 --> 00:07:03,610
And that's just because, you know, for differences, everything's symmetrical around zero and that's just what our typical access looks like.

70
00:07:04,210 --> 00:07:09,130
But for log transformations, that's what works well for things where we're symmetrical around one,

71
00:07:09,130 --> 00:07:11,700
where the distance between zero and one is the same as one in.

72
00:07:11,730 --> 00:07:19,210
But again, I think this is just, you know, as you're preparing posters, I think there's no poster session for most of you coming up.

73
00:07:20,020 --> 00:07:24,880
I think graphs of ratios can be great.

74
00:07:26,230 --> 00:07:32,710
Just put them on a log, transform, skip any questions on these questions?

75
00:07:39,320 --> 00:07:44,780
So today we will be getting into causal inference.

76
00:07:45,230 --> 00:07:50,120
This is something it's a buzzword in epidemiology I think became even more popular in recent years.

77
00:07:51,110 --> 00:07:57,530
I think if you go to any of the epidemiology conferences like the Society for Epidemiologic Research,

78
00:07:57,830 --> 00:08:02,240
there will be a lot of like conference on causal inference, short courses and study sessions.

79
00:08:03,770 --> 00:08:12,170
If you're really into it, the summer session in epidemiology here in our department, we have a series of courses on causal inference.

80
00:08:14,440 --> 00:08:18,079
Okay. So the thing to remember about Kathleen Prince is there's no like set criteria.

81
00:08:18,080 --> 00:08:27,400
There's not like a list of of checkboxes for you to say whether something is an appropriate causal inference analysis or not.

82
00:08:27,410 --> 00:08:31,370
It's more as like it's a cure. It's mentioned as an exercise in measurement.

83
00:08:31,370 --> 00:08:35,720
It's more of a process and it's not, you know, like a list of things that you can do.

84
00:08:35,750 --> 00:08:44,330
So I think the point of class for you all today is for me to give you like an intuition on what a

85
00:08:44,420 --> 00:08:50,810
causal inference is in some ways that we view things different from like a causal inference approach,

86
00:08:51,110 --> 00:08:55,700
but is not to tell you that you need to like exactly do these, these certain things.

87
00:08:58,010 --> 00:09:01,549
So this is something that I would like you to do in your small group.

88
00:09:01,550 --> 00:09:05,630
Just think about these different words. Cause, correlation, association, relationship.

89
00:09:06,140 --> 00:09:16,880
How are they the same? How are they different? Again, in the candidates website there is um, if you go to our pages,

90
00:09:19,070 --> 00:09:26,660
there is a link to your Google Drive document where we have all these small parts as well.

91
00:09:27,530 --> 00:09:31,680
So I will just give you like three or 4 minutes.

92
00:09:32,630 --> 00:10:15,410
Think about how these are the same or different. Yeah.

93
00:10:17,040 --> 00:10:21,670
Yeah. Okay.

94
00:10:30,040 --> 00:10:40,190
I mean, that's going to fall and you're going to have.

95
00:10:56,120 --> 00:11:04,010
Yeah. That's. That's. That is South America, literally.

96
00:11:12,100 --> 00:11:18,790
I just felt that.

97
00:11:23,450 --> 00:11:27,440
Yeah, I have to say.

98
00:11:33,350 --> 00:11:38,090
Asking for your help to find.

99
00:11:42,640 --> 00:11:45,790
Thanks so much for.

100
00:11:50,270 --> 00:11:53,510
Off the charts.

101
00:11:53,650 --> 00:11:59,710
They are very fashionable, but they like everything about the.

102
00:12:04,820 --> 00:12:15,540
Like I said, I think. So.

103
00:12:32,620 --> 00:12:38,100
Haven't heard. Literally.

104
00:12:47,900 --> 00:12:52,590
He says. Is that?

105
00:13:14,970 --> 00:13:22,110
Is there. USA.

106
00:13:29,040 --> 00:13:34,450
I have no doubt that.

107
00:13:40,080 --> 00:13:50,760
Okay. Let's bring that together. Let me start with and maybe I will start with Group I, the influencers.

108
00:13:51,450 --> 00:13:55,790
What are your thoughts on these different versions?

109
00:14:00,200 --> 00:14:11,590
That happens. Anybody.

110
00:14:15,460 --> 00:14:20,370
I. Okay?

111
00:14:21,210 --> 00:14:31,840
Yeah. So goodbye. What are your best? Recently had a fall with her four year old.

112
00:14:35,310 --> 00:14:42,920
I agree with her. So.

113
00:14:47,190 --> 00:14:52,740
Got it. Yeah. So I get your things. The first three are a bit more technical relationship, a bit more vague.

114
00:14:53,280 --> 00:14:57,260
Um, let's see. Let's move it to group a the disease.

115
00:14:57,270 --> 00:15:03,660
Did you have any additional thoughts, anything in line or separate from that?

116
00:15:04,080 --> 00:15:10,480
Yeah. Relationship. The anthropologist nation.

117
00:15:14,150 --> 00:15:18,490
Or at least in a situation where. The relationship exists between two very.

118
00:15:19,770 --> 00:15:30,389
Back to direction. Words going. Yeah.

119
00:15:30,390 --> 00:15:33,240
And I think what both groups have said totally makes sense to me.

120
00:15:33,840 --> 00:15:38,070
I do think that relationship and maybe even like association relationship are a bit more vague.

121
00:15:38,400 --> 00:15:41,730
Cause clearly is like a bit more specific on something.

122
00:15:42,210 --> 00:15:45,390
As you mentioned, there needs to be like a temporal relationship.

123
00:15:47,250 --> 00:15:54,690
If you remember from your first epidemiology course, there might be like a breadth of Bradford Hill criteria for like what is a causal relationship.

124
00:15:55,650 --> 00:16:03,120
Again, that's not like a sentence don't kind of thing, but often we use sort of checklists like that and probably abuse those that checklist as well.

125
00:16:03,830 --> 00:16:09,630
I think that correlation I think is a bit more like a correlation coefficient.

126
00:16:09,870 --> 00:16:15,839
But as you're saying, like we're not saying that like one variable comes before necessarily another variable.

127
00:16:15,840 --> 00:16:24,210
We're just looking at how these things interlink. So I would say that in general, I agree with all of you in what you're saying,

128
00:16:25,230 --> 00:16:32,870
but the Adam that I would have to say here is that when we say the word cause in epidemiology, we should be very specific about when we use it.

129
00:16:32,880 --> 00:16:39,180
We should be very careful. And throughout this semester I'll give some examples that one would be somewhat good with good places where we could say,

130
00:16:39,180 --> 00:16:46,800
cause then one would be some bad situations where things like association and relationship builds are going to be you can use those whenever you want.

131
00:16:47,430 --> 00:16:51,300
Again, correlation. I probably would limit it to more of like a correlation coefficient.

132
00:16:53,370 --> 00:16:58,350
Okay. And we always hear things like this, like correlation does not mean causation.

133
00:16:58,740 --> 00:17:04,649
This is a bit of a trite thing at this point in time and there's all sorts of fun graphs

134
00:17:04,650 --> 00:17:11,850
like the number of pilots there are in the world versus the global average temperature.

135
00:17:11,850 --> 00:17:18,660
And sort of there's this inverse relationship. So maybe the pilots were keeping global average temperatures at there.

136
00:17:20,040 --> 00:17:23,849
I think we would disagree with this assumption and we kind of laugh it off.

137
00:17:23,850 --> 00:17:30,540
Again, correlation is not causation. At the same time, sometimes correlation does mean causation.

138
00:17:30,900 --> 00:17:37,160
And sometimes I would say in epidemiology we've been maybe overly careful about making this jump.

139
00:17:37,860 --> 00:17:49,310
So erm Fisher was a statistician, you know there is like the fisheries test, there's all sorts of things that were named after him.

140
00:17:49,920 --> 00:17:59,610
He was the one who came up with like .05 being like our p cut off, he turns out was not that great of a human being.

141
00:17:59,610 --> 00:18:04,649
And I think the American Statistical Association has sort of removed an award that

142
00:18:04,650 --> 00:18:09,480
they used to say was named after him and now it's named after somebody else.

143
00:18:11,400 --> 00:18:16,740
But he was an avid smoker, as many people were in the early 1900s.

144
00:18:16,740 --> 00:18:22,080
And when all these epidemiological studies came out saying that there was this relationship,

145
00:18:22,080 --> 00:18:26,580
this association, this correlation between smoking and lung cancer,

146
00:18:26,910 --> 00:18:36,690
he was very adamant that it was spurious, that he was of the camp, that like correlation does not mean causation in regards to smoking in lung cancer.

147
00:18:38,160 --> 00:18:45,180
It turns out he's wrong. I think that, you know, now it is the standard epidemiological view that smoking does cause lung cancer.

148
00:18:45,540 --> 00:18:50,490
I think for him, he thought there were from like genetic thing that was, you know, the mastermind in all of this.

149
00:18:50,760 --> 00:18:57,600
And I will say that if you read epidemiological studies from like prior to the 1950s or sixties, there's a lot of like,

150
00:18:57,600 --> 00:19:04,350
oh, the genes are the reason why there is there's this reason we don't necessarily believe that any.

151
00:19:05,040 --> 00:19:13,560
Certainly, there can be some genetic relationships to lung cancer, but I don't think it's it's confounding the smoking lung cancer relationship.

152
00:19:15,560 --> 00:19:20,690
So we clearly need a process for deciding what is causal and what is not.

153
00:19:21,410 --> 00:19:27,410
So again, this is not supposed to be a checklist, but for some things for you to keep in mind, of course you want a good sampling scheme.

154
00:19:27,410 --> 00:19:32,240
Like if your sample is heavily biased, then you're not really going to be able to do any causal inference.

155
00:19:32,570 --> 00:19:41,270
You need to have like a good way of measuring your variables. Of course, you need to be able to specify the correct statistical or mathematical model,

156
00:19:41,960 --> 00:19:46,100
like what we were talking about last week with like odds ratios and risk ratios of risk differences.

157
00:19:46,100 --> 00:19:49,430
Maybe if you're doing the wrong thing for the wrong study, that would be bad.

158
00:19:50,540 --> 00:19:54,290
You know, control being able to control for confounders is good.

159
00:19:54,830 --> 00:19:57,860
Being able to use an experiment could be better in many situations.

160
00:19:59,270 --> 00:20:05,149
But we really need to be thoughtful about what does it mean to have a change in your exposure?

161
00:20:05,150 --> 00:20:09,500
And we'll be talking about that later on this this class.

162
00:20:09,500 --> 00:20:15,200
But what a change in exposure that can be kind of tough to reconcile.

163
00:20:15,200 --> 00:20:17,800
It's one thing to compare people who've been vaccinated or not.

164
00:20:18,170 --> 00:20:22,940
That's like an easy exposure to kind of randomize between like, did you get the COVID 19 vaccine or did you not?

165
00:20:24,560 --> 00:20:35,030
But other types of exposures are or things that we traditionally have thought of as exposures in epidemiology like race or age or gender,

166
00:20:35,450 --> 00:20:40,640
like what does it what does that mean? So we'll talk more about that later.

167
00:20:40,670 --> 00:20:43,850
And then we will evaluate biases.

168
00:20:43,850 --> 00:20:51,230
And there's a future class on evaluating biases that we will talk more about that.

169
00:20:52,910 --> 00:20:58,250
Okay. So I'm a big proponent of using drugs, these directed acyclic graphs.

170
00:20:58,250 --> 00:21:03,889
I believe in previous courses you've kind of gone over this as well as you know,

171
00:21:03,890 --> 00:21:08,120
these depicts the statistical associations implied by underlying causal structures.

172
00:21:09,260 --> 00:21:13,850
I think they're very helpful for letting you choose the regression covariance.

173
00:21:15,080 --> 00:21:22,159
The thing to keep in mind is that there's sort of for anything in epidemiology and in any model building,

174
00:21:22,160 --> 00:21:25,160
there's sort of a garbage in, garbage out philosophy.

175
00:21:25,160 --> 00:21:33,680
So for Dag, we need to decide like what are the relationships between certain exposures or independent variables

176
00:21:33,680 --> 00:21:39,170
and confounders or plausible confounders or mediators or all sorts of other things like that.

177
00:21:40,490 --> 00:21:45,680
And you should have like a good theoretical understanding underpinning that.

178
00:21:47,090 --> 00:21:52,940
But, you know, there can be a lot of studies which may be in the past that, oh, this is for sure a confounder.

179
00:21:53,360 --> 00:21:58,579
And the more you look into it as sort of like a house of cards where there's not really

180
00:21:58,580 --> 00:22:02,840
a theoretical basis for something being thought of as the confounder is just that,

181
00:22:03,470 --> 00:22:09,290
you know, there's a certain research group which always put this variable in their model as a confounder,

182
00:22:09,290 --> 00:22:12,559
but if you dig deeper, there's not really a theoretical underpinning to it.

183
00:22:12,560 --> 00:22:16,969
So that's all to say that as you're putting together a DAG,

184
00:22:16,970 --> 00:22:25,040
I definitely would be really purposeful for deciding which variables you

185
00:22:25,040 --> 00:22:31,040
include and how you decide if there's a causal relationship from past lecture.

186
00:22:32,660 --> 00:22:41,660
So how do we do that? We will talk in the next class about mediators and effect modifiers, so we'll kind of just ignore a bit about that for now.

187
00:22:42,200 --> 00:22:47,329
But in general, we want to control for things that are confounders,

188
00:22:47,330 --> 00:22:53,239
but we do not control for mediators or colliders unless we have a specific reason to.

189
00:22:53,240 --> 00:23:00,220
And again, we'll talk more about that next week. So for confounding again, I'm sure you've seen this chart before.

190
00:23:00,230 --> 00:23:04,219
This is the traditional DAG we have where there is an arrow between the confounder and

191
00:23:04,220 --> 00:23:07,790
the exposure and the confounder and the outcome and the exposure and the outcome.

192
00:23:07,790 --> 00:23:12,320
And that directionality is really important because if it's in the opposite direction,

193
00:23:12,650 --> 00:23:16,100
it would be a collider, not a confounder, and we don't control for colliders.

194
00:23:19,160 --> 00:23:25,520
So this is the relationship which is trickiest, the confounder exposure relationship.

195
00:23:26,630 --> 00:23:31,790
Here I have it as a double headed arrow,

196
00:23:32,570 --> 00:23:42,260
but this is not officially dag vocabulary because in in a precise dag you're only supposed to have arrows going one direction, not by directional.

197
00:23:42,790 --> 00:23:48,529
I think sometimes people use this or they'll use like a dotted arrow as a shorthand for saying there's like another variable,

198
00:23:48,530 --> 00:23:53,000
maybe a fourth, the variable which is an ancestor of both the exposure and the confounder.

199
00:23:54,710 --> 00:23:59,600
So this is sort of like a shorthand that we use a lot, but it's not, you know, official.

200
00:24:01,870 --> 00:24:10,729
The symbology. So how do we deal with confounding mental Haynesville equations?

201
00:24:10,730 --> 00:24:15,110
Are you still tight until have operations in like introductory epidemiology?

202
00:24:15,110 --> 00:24:20,120
Sort of. In my life over a decade of doing epidemiological studies,

203
00:24:20,120 --> 00:24:24,680
I have never computed amounts of information in my life, but I'm glad they're still teaching it.

204
00:24:25,430 --> 00:24:32,990
I think that's just a simple example of what we can do with confounders in like a very, very simple example.

205
00:24:33,300 --> 00:24:36,080
I think your tell all for you, direct and direct standardization.

206
00:24:36,380 --> 00:24:44,030
I don't do so much with that, but I know there are some fields which do a lot with that you can stratify based on the confounder.

207
00:24:44,030 --> 00:24:48,169
That's one way of dealing with a contender. But generally we use multivariate models.

208
00:24:48,170 --> 00:24:49,970
And what do we mean by multivariable model?

209
00:24:50,270 --> 00:24:57,530
Like if you're using SAS, you have your outcome, you have your mean independent variable, and then your confounders are just added on.

210
00:24:57,920 --> 00:25:06,560
So we create of an equation where you have the confounders as the other variables.

211
00:25:07,190 --> 00:25:12,100
And I will say that sometimes people use the word multivariate like ending with a T here.

212
00:25:12,740 --> 00:25:15,860
Um, historically that's not been the correct way.

213
00:25:15,870 --> 00:25:21,680
Recurrence. This multivariate means that the outcome has three or more categories,

214
00:25:22,010 --> 00:25:28,760
whereas multivariable means that you have more than one variable on this side of the equation.

215
00:25:29,420 --> 00:25:34,219
I think now at this point in time, people use multivariate more often for this.

216
00:25:34,220 --> 00:25:42,950
So I, you know, I'm a descriptive linguist, so I don't think we need to be too too particular about it.

217
00:25:42,950 --> 00:25:46,310
But just so you know, and then we'll be talking about causal inference methods.

218
00:25:46,640 --> 00:25:51,530
So there's different techniques we can use kind of based off of this multivariable model.

219
00:25:54,890 --> 00:26:01,130
Okay. So this is Owen. He is a statistician out of UCLA.

220
00:26:01,160 --> 00:26:04,340
I've gone to some short classes from him. He has the thing.

221
00:26:04,340 --> 00:26:10,490
When in doubt, take it out. And so that is what we are going to do.

222
00:26:11,300 --> 00:26:17,330
So I would like you to use agreed to that to draw out this debt.

223
00:26:18,710 --> 00:26:23,660
So if you go to it, I mean, this is sort of you need to play around with it.

224
00:26:23,660 --> 00:26:28,250
And if you have a mac versus a P, C, sometimes the directions are a bit different.

225
00:26:29,120 --> 00:26:39,319
But you can launch negative in your browser. They already have like an example of stuff here on PC.

226
00:26:39,320 --> 00:26:45,440
At least you just hover over it and you press like D to delete something and then you can like add in another variable.

227
00:26:46,910 --> 00:26:51,500
And then if you like click on things in a certain order. You get arrows going in that way.

228
00:26:52,250 --> 00:26:56,990
So I would love for you in your groups to do this.

229
00:26:57,320 --> 00:27:02,030
The other thing to note is you should have this model code here.

230
00:27:02,570 --> 00:27:07,640
And if you could, I think I have it in the group document on Google Drive.

231
00:27:07,650 --> 00:27:12,230
But if you could paste your model code into your Google Drive, that be lovely as well.

232
00:27:13,070 --> 00:27:20,180
Mostly I just think this is like a good reference to have in the future as you are like going through different computers and 20 seconds.

233
00:27:25,160 --> 00:27:48,640
Okay. So do you have government? Every year or so, just like one.

234
00:27:49,320 --> 00:27:53,190
If you do not have a computer, do anything either just take at least one.

235
00:28:07,000 --> 00:28:26,550
Just if. Is anyone just having trouble accessing Dalgety or like launching it?

236
00:28:27,030 --> 00:28:37,330
I'm happy to help troubleshoot. Oh.

237
00:28:59,830 --> 00:29:11,860
The. You're.

238
00:29:17,610 --> 00:29:24,790
He had. I think in a lot of.

239
00:29:28,640 --> 00:29:38,100
Oh. Yeah.

240
00:30:11,010 --> 00:30:14,120
Okay. So, I mean, obviously.

241
00:30:39,240 --> 00:30:51,520
But we figured out that. I haven't heard that.

242
00:31:51,000 --> 00:31:57,930
So. So.

243
00:32:04,120 --> 00:32:16,000
I got. Okay.

244
00:32:16,520 --> 00:32:19,490
I think some people are still working on this great thing. They can come together.

245
00:32:28,660 --> 00:32:32,350
Does anyone did anybody in the bureau potatoes or did you finish this?

246
00:32:33,280 --> 00:32:38,170
No. Okay. Maybe I should have had on going group Jane.

247
00:32:38,440 --> 00:32:42,760
Did anybody from your group finish this? Okay.

248
00:32:42,770 --> 00:33:04,050
Well, I'll give you another minute that I have been concerned. So.

249
00:33:25,610 --> 00:33:44,000
Let's hear. What? Just because I want a.

250
00:34:05,530 --> 00:34:25,920
Yeah. For.

251
00:34:49,100 --> 00:35:03,139
Okay. So when I did this, I got and if you go to this like top right section here, you can adjust for different things.

252
00:35:03,140 --> 00:35:07,879
We're probably addressing for the total effect most of the time when I do it on my computer,

253
00:35:07,880 --> 00:35:13,340
I have childhood abuse, education, maternal age, race, ethnicity.

254
00:35:14,480 --> 00:35:18,560
Anybody have any other answers? I might have done the thing a bit.

255
00:35:21,640 --> 00:35:25,110
Okay. So one thing, you know, when you put together Dad,

256
00:35:25,120 --> 00:35:28,840
there are some people who are just able to look at this and in their mind compute like, where's that?

257
00:35:29,980 --> 00:35:36,160
The back path and the back door path. And you know what make sense being a confounder not I'm not one of those people.

258
00:35:36,160 --> 00:35:41,379
I really do need to put an integrity. I use this a lot, so I'd recommend it.

259
00:35:41,380 --> 00:35:42,459
But you know,

260
00:35:42,460 --> 00:35:49,420
some people I think are able to kind of have more of a spatial awareness of these these relationships and aren't always needing to put stuff in.

261
00:35:49,420 --> 00:35:57,520
And the other thing with this is I just made up these relationships of the for the sake of an assignment for you all.

262
00:35:58,090 --> 00:36:05,110
But, you know, theoretically, if you're coming, you're creating your own dad,

263
00:36:05,500 --> 00:36:11,200
you should look to see is there relationship between maternal age and social support and thinking your mind, you know, does that make sense?

264
00:36:11,500 --> 00:36:16,540
Is that in line with the literature? Is there something causally there? And you need to do that for every single area?

265
00:36:16,540 --> 00:36:20,170
So you shouldn't just be like willy nilly putting themselves in random places.

266
00:36:25,390 --> 00:36:34,070
Okay. So when we are thinking about causal inference methods in epidemiology, what do we so typically in the classes that we teach you,

267
00:36:34,090 --> 00:36:36,459
we talk about randomized controlled trials a bit,

268
00:36:36,460 --> 00:36:41,380
but mostly we talk a lot about observational studies like cohort studies, cross-sectional study of case control studies.

269
00:36:42,250 --> 00:36:45,820
And then we analyze those results using multivariable regression models.

270
00:36:47,530 --> 00:36:50,350
But there are some problems with these approaches.

271
00:36:50,360 --> 00:36:55,600
So in observational studies you could have time varying confounding in the article for today was a great example of that.

272
00:36:56,290 --> 00:37:05,110
You could have like an observed or substantially or confounding even from observed variables that there's like some residual confounding there.

273
00:37:06,910 --> 00:37:11,580
And even in randomized controlled trials, there's this difference between, you know, like a,

274
00:37:12,070 --> 00:37:16,809
like what do we do when patients are not compliant with the treatment or if

275
00:37:16,810 --> 00:37:21,340
they're self-selecting or if they change their behaviors based on what goes on?

276
00:37:22,150 --> 00:37:26,530
So even in randomized controlled trials, we could be presented with some difficulties.

277
00:37:27,730 --> 00:37:32,440
So causal inference methods, I think there's in general two different categories of them.

278
00:37:32,890 --> 00:37:38,200
Again, I think sometimes people use causal inference like more broadly to refer to just being thoughtful

279
00:37:38,560 --> 00:37:43,420
about what variables to include in your model and to be using drugs and things like that.

280
00:37:43,750 --> 00:37:49,060
But then other people are, you know, much more narrow minded in their view of what a causal in method is.

281
00:37:49,450 --> 00:37:51,399
And they will have two families.

282
00:37:51,400 --> 00:38:00,490
There's G estimates and then there's instrumental variables will be mostly talking about marginal structure models which are type of GFR models,

283
00:38:00,970 --> 00:38:07,170
but I'll go over a few of these. So the family of G estimation models, they're called different things.

284
00:38:07,180 --> 00:38:12,040
Basically, there were a few different research groups which came up with these topics around the same time,

285
00:38:12,820 --> 00:38:21,010
and so they were kind of like battling it out to see who could come up with the name that would be applied more often throughout the literature.

286
00:38:21,010 --> 00:38:27,880
And I would say all these are kind of used relatively synonymously, although some people might argue there are some differences across them.

287
00:38:29,020 --> 00:38:34,840
The G stands for General because the original people who came up with this thought that this would

288
00:38:35,110 --> 00:38:40,510
eventually be generalized to be the method that we just use all the time in population studies.

289
00:38:40,840 --> 00:38:48,300
That has not really happened. So I think they were maybe, you know, a bit overly optimistic about how often people would use these methods.

290
00:38:48,850 --> 00:38:52,749
But that's where the G comes from for information. Okay.

291
00:38:52,750 --> 00:38:56,820
So this is an example, time period confounding. It's from the article for today.

292
00:38:57,160 --> 00:39:02,200
And basically, if we're looking at social support and depression, we can look at that across time.

293
00:39:03,100 --> 00:39:08,799
So then how would you deal with looking at the social support at time one and its

294
00:39:08,800 --> 00:39:14,620
effect on depression at time to if there is sort of this intervening period?

295
00:39:15,730 --> 00:39:24,430
And like the issue here is that depression at Timepoint one is sort of a a mediator,

296
00:39:24,430 --> 00:39:29,829
but it's also like confounder of the social support at time to and depression at time too.

297
00:39:29,830 --> 00:39:32,200
So there's a lot of things just looking at this model.

298
00:39:32,530 --> 00:39:41,590
If you put this into Dagny, it would spit out an answer, but it doesn't necessarily like Dagny would basically.

299
00:39:41,860 --> 00:39:46,030
Have you ignored depression at time point one and so support at time point two.

300
00:39:46,270 --> 00:39:48,639
But these might be really important things for you to consider.

301
00:39:48,640 --> 00:39:57,040
So basically Dagny would simplify the process and make a more coarse analysis, whereas for us we might want to actually delve into that time.

302
00:39:57,040 --> 00:40:01,429
Very confounding, incorporated. Okay.

303
00:40:01,430 --> 00:40:06,020
So again, we'll be using the G estimation.

304
00:40:06,020 --> 00:40:11,720
And basically what's consistent through all of these is this idea of using propensity scores.

305
00:40:12,350 --> 00:40:17,810
So what is the propensity score or propensity score is the probability of being in a treatment group.

306
00:40:18,350 --> 00:40:24,770
So the thing that students always like across time get confused about is that for propensity score,

307
00:40:25,460 --> 00:40:31,430
your outcome in this logistic regression model is your treatment, which is like counterintuitive.

308
00:40:31,940 --> 00:40:40,370
And we'll go over this very much in depth. But again, I'm using actually the word logistic regression pretty explicitly here.

309
00:40:40,380 --> 00:40:43,400
I know last week I was kind of badmouthing logistic regression.

310
00:40:43,790 --> 00:40:49,010
But one thing logistic regression is very good at, in addition to some of the things you talked about last week,

311
00:40:49,340 --> 00:40:56,450
is it's good at coming up with a propensity score. So a propensity score is your probability of being in a certain group.

312
00:40:56,540 --> 00:41:07,550
So as a simplified example, we have an example of people who are, you know, have diabetes.

313
00:41:08,060 --> 00:41:11,930
Do they get are they the drink, coffee or not?

314
00:41:11,930 --> 00:41:21,360
And then they develop diabetes over time. Our propensity score is like, what's the probability that somebody is a coffee drinker?

315
00:41:21,590 --> 00:41:27,830
So again, it's like flipping it to look at a really in-depth look at what's going on in your treatment group.

316
00:41:30,350 --> 00:41:33,070
And then you have the important things that Nancy,

317
00:41:33,110 --> 00:41:38,840
these that we can come up with these histograms and we can compare what's going on in the control in the treatment group.

318
00:41:39,740 --> 00:41:48,590
So let me walk you through this. So zero one, let's again use our example of coffee drinking as our exposure.

319
00:41:49,190 --> 00:41:52,910
So the zeros are those who do not drink coffee.

320
00:41:53,330 --> 00:41:59,380
The treatment are those people who do your coffee. We that makes sense.

321
00:41:59,390 --> 00:42:04,760
So again, like zeros are the people who you'd be coded as zero in this analysis.

322
00:42:04,760 --> 00:42:06,800
If you did not drink coffee, you'd be coded as one.

323
00:42:07,160 --> 00:42:14,540
In this analysis, if you do drink coffee, then what we do is we look at what other covariates are in the model.

324
00:42:15,020 --> 00:42:25,040
So again, like, what are the confounders, what are the things which are confounding the relationship between our exposure and our outcome?

325
00:42:25,400 --> 00:42:34,940
So this could be age, it could be gender, it could be you know, it could be like your racial, ethnic background.

326
00:42:36,830 --> 00:42:41,090
There's there's all sorts of things which could be considered confounder.

327
00:42:42,410 --> 00:42:50,450
So in this example, what we would do is we would put together a logistic regression model where the outcome was,

328
00:42:52,010 --> 00:42:57,170
whether they drank coffee or not, and then the exposure to all those different confounders that everything.

329
00:42:58,670 --> 00:43:04,850
And then what a logistic regression model consider, you know, hence we don't like odds ratios and you know what?

330
00:43:04,850 --> 00:43:13,280
We're typically interested in epidemiology, but we can also have the programs spit out these propensity scores.

331
00:43:13,820 --> 00:43:17,899
So that's like your probability, you know, from zero to that.

332
00:43:17,900 --> 00:43:23,900
It's not like you do have it or not. It's what's your probability that you would be in a certain group?

333
00:43:25,170 --> 00:43:29,270
Okay, so any questions on that?

334
00:43:30,320 --> 00:43:33,229
And part of the homework assignment will be like going over this a bit more in depth.

335
00:43:33,230 --> 00:43:38,090
But I think in general, that concept is an important one to grapple with.

336
00:43:43,140 --> 00:43:51,870
So these are three different studies and how the propensity scores can look like in a there is complete separation.

337
00:43:52,770 --> 00:43:59,800
So basically that means that we can use our confounders to predict who would be a copy or coffee drinker.

338
00:43:59,850 --> 00:44:09,630
Not because, you know, there's there's separation here. So the control group has very low probability of being a coffee drinker.

339
00:44:11,490 --> 00:44:15,120
The treatment group, given their covariates, has very high probability of being a coffee drinker.

340
00:44:15,870 --> 00:44:19,260
So, again, complete separation. We do not like this.

341
00:44:19,270 --> 00:44:24,419
So I'm looking at this. We would say like we just can't do a marginal structural model.

342
00:44:24,420 --> 00:44:29,840
There's just complete separation of our points. B is what we do.

343
00:44:30,180 --> 00:44:40,200
So B is there's overlap. So basically what this is showing is that through our covariates we can manipulate our populations to look similar.

344
00:44:42,620 --> 00:44:46,999
And if you think about it, this is what a randomized controlled trial does, because in a randomized controlled trial,

345
00:44:47,000 --> 00:44:51,200
like if we were to randomized people, it would be like, you drink coffee, you don't drink coffee, you drink coffee, you don't drink coffee.

346
00:44:51,890 --> 00:44:57,440
All of the other confounders would be evenly distributed across those groups, right?

347
00:44:59,240 --> 00:45:01,219
So in like a propensity score approach,

348
00:45:01,220 --> 00:45:10,250
what we're doing is we're just like putting all of the confounders into a model and we're hoping that the propensity score that is,

349
00:45:11,090 --> 00:45:15,560
is output from this model will show that there's overlap between these two different groups.

350
00:45:18,190 --> 00:45:25,450
We could offer an example of C, which is sort of like a bit of a hybrid where there's part of the population,

351
00:45:25,450 --> 00:45:33,250
there is overlap, but then there is separation at some points. So we're not really going to get into C too much, but like A you definitely can't do.

352
00:45:33,280 --> 00:45:37,780
B is a great example. We would proceed with our analysis with C,

353
00:45:38,140 --> 00:45:46,960
we might have to cut out these people and then in our discussion we would talk that there's maybe a limitation to our approach.

354
00:45:47,330 --> 00:45:52,210
We just aren't able to generalize to the people with either really low or really high frequencies because.

355
00:45:56,660 --> 00:46:02,480
Okay. So what do we do with this propensity score?

356
00:46:03,170 --> 00:46:09,540
So again, the propensity score is somewhere between zero and one. It's it's like this this number here.

357
00:46:10,310 --> 00:46:12,650
At first, people would just put a propensity score in the model.

358
00:46:12,650 --> 00:46:16,010
So basically they take out all the confounders instead of having like five confounders.

359
00:46:16,340 --> 00:46:22,249
You'd have like one propensity score as your like third variable.

360
00:46:22,250 --> 00:46:29,180
You'd have your exposure or you'd have your outcome that's equal to your mean independent variable, the propensity scores.

361
00:46:30,200 --> 00:46:33,740
But now what people like doing is actually to use a weighting.

362
00:46:34,130 --> 00:46:41,300
So they incorporate this as a points and maybe in your on a balance between those six or two,

363
00:46:41,510 --> 00:46:47,240
you might have gone over three methods a bit and talked about using weights in your analysis.

364
00:46:47,720 --> 00:46:52,640
So basically what people more recently have been saying is don't put it in the model,

365
00:46:53,240 --> 00:47:01,580
but put it as weights and use the inverse of the score, the one over your propensity score, and put that in your model as a weight.

366
00:47:02,810 --> 00:47:06,620
And that is formally called a marginal structural model.

367
00:47:06,920 --> 00:47:08,749
So that's what we mean by marginal structural model.

368
00:47:08,750 --> 00:47:14,500
That means that we develop these propensity scores and the inverse of the propensity score is put into a model forward.

369
00:47:17,530 --> 00:47:25,930
Okay. So I will go over a brief example from the reading for today.

370
00:47:28,540 --> 00:47:29,650
Um, again,

371
00:47:29,830 --> 00:47:38,500
what were the purpose of creating this way is we're trying to come up with like an ideal target population of those with and without the exposure.

372
00:47:38,530 --> 00:47:43,770
So for the homework assignment for seniors, those with or without social support systems,

373
00:47:43,780 --> 00:47:50,470
and 10.1 people think of people who do or do not drink coffee, like whatever your exposure is.

374
00:47:51,070 --> 00:47:59,860
And you want those people to be exactly alike in everything else except for their that treatment variable.

375
00:48:00,580 --> 00:48:03,800
So we want people to be exactly the same except for social support,

376
00:48:04,240 --> 00:48:08,470
or we want people to be exactly the same except for whether they are a coffee drinker.

377
00:48:12,370 --> 00:48:23,080
So what do we do? So I will see that marginal structure models, the benefit of them is you can kind of like multiply these weights together.

378
00:48:23,440 --> 00:48:27,190
And so that's what they did in this in the reading for today.

379
00:48:27,580 --> 00:48:34,630
So let's zoom in at Timepoint one. So you have their support at 10.1, you have depression at 10.1, then you have confounders.

380
00:48:36,340 --> 00:48:40,750
And I believe the confounders are being in this example.

381
00:48:42,640 --> 00:48:55,540
So what you do is you set up a logistic regression model where the outcome is so supported 3.1 And these confounders are the independent variables.

382
00:48:55,540 --> 00:49:01,690
Basically, we're narrowing in on this arrow right here and remodeling that.

383
00:49:02,890 --> 00:49:12,000
So we do not include the outcome in this. We only include like what's relevant for this variable and that.

384
00:49:12,010 --> 00:49:17,860
Like if you remember back to any of your past statistics courses, the inverse of this, the one over.

385
00:49:17,980 --> 00:49:23,920
So this denominator right here is just like a fancy way of abbreviating that equation.

386
00:49:25,960 --> 00:49:31,090
You might be wondering what this is here. This is just the mean value of social support in the population.

387
00:49:31,420 --> 00:49:37,030
And we put this here to stabilize the weights. It I don't think there's really like a theoretical reason why we do it.

388
00:49:37,030 --> 00:49:42,370
It just makes our analysis more stabilized.

389
00:49:42,580 --> 00:49:50,229
I think there's there's tighter confidence circles. But that's not that's not a huge point.

390
00:49:50,230 --> 00:49:55,809
I think the more important thing is like representative denominators and then you can multiply those together.

391
00:49:55,810 --> 00:50:02,290
So again, that's the first part you can go to the second part in the second part is

392
00:50:02,290 --> 00:50:08,349
looking at this relationship between self support at time too and depression.

393
00:50:08,350 --> 00:50:18,160
So our in like our propensity score, this number down here, we're trying to look at this outcome of search the point at time point you.

394
00:50:21,080 --> 00:50:27,590
But in this example, social support at Timepoint one is a confounder.

395
00:50:27,590 --> 00:50:37,010
So we add that I think the L one is confounders for this relationship, and this one is an additional confounder for this relationship right here.

396
00:50:39,970 --> 00:50:44,770
So the takeaway from this is really, again, go back to your dad.

397
00:50:45,340 --> 00:50:48,910
Find out what your exposure is, what your outcome is, what your confounders are.

398
00:50:49,180 --> 00:50:56,110
And then focus on this area right here. If you have multiple time points, basically just for each time point, break it out.

399
00:50:56,110 --> 00:51:04,120
So you have this simple tag, come up with your inverse probability, and then you can just multiply all of those together across your.

400
00:51:05,260 --> 00:51:13,460
The number of leads that you have. Repercussions.

401
00:51:14,990 --> 00:51:24,770
This is this is like a confusing thing. So, yeah, you're not supposed to be proud of it.

402
00:51:25,520 --> 00:51:28,760
Yeah. So, um, and I meant to make sure that I got this girl.

403
00:51:28,940 --> 00:51:36,080
I think V is a confounder across all time points. I think l one is the confounder for 10.2.

404
00:51:37,610 --> 00:51:42,410
Yeah. So there might be different. So there could be different times. There could be different confounders at different points of time.

405
00:51:53,940 --> 00:51:58,630
Okay. Zooming in on instrumental variables, again, we have our example here.

406
00:51:58,680 --> 00:52:03,270
The thing with marginal structural models is that you need to measure these confounders.

407
00:52:04,710 --> 00:52:10,290
You can't deal with unmeasured confounders in a marginal structural model like everything needs to be measured

408
00:52:10,740 --> 00:52:19,010
because you put those confounders in your your model to develop the propensity scores for instrumental variables,

409
00:52:19,040 --> 00:52:23,040
we can deal with unmeasured contracts. So how do we do that?

410
00:52:23,820 --> 00:52:25,290
We have our instrumental variable.

411
00:52:25,620 --> 00:52:33,330
So an instrumental variable is just like some other thing in the universe which is somewhat related to the exposure,

412
00:52:34,770 --> 00:52:38,100
causally relates to exposure, but it is not related to the outcome.

413
00:52:40,190 --> 00:52:44,030
So basically the instrumental variable is something which is tightly linked with the exposure.

414
00:52:44,630 --> 00:52:48,810
But again, there can't be a relationship through the outcome except through the exposure.

415
00:52:48,830 --> 00:52:57,140
Obviously, if you looked at these, there might be some statistical association, but that could only be because of exposure acts.

416
00:53:01,880 --> 00:53:09,380
The better instrumental variables will have a tighter connection, but you can use instrumental variables with more of a weak connection.

417
00:53:10,580 --> 00:53:16,640
But again, the the stronger the relationship here, the easier your analysis will end up being.

418
00:53:20,720 --> 00:53:29,000
Mendelian randomization is a type of instrumental variable analysis and probably the most commonly used where the instrumental variable is the gene.

419
00:53:29,360 --> 00:53:47,330
And the idea here is that the genotype is like what a Leo we get is randomized through meiosis and so we that's where this randomization comes in.

420
00:53:49,190 --> 00:53:55,310
But again, so the instrumental variable here is gene and the specific appeals for it.

421
00:53:56,750 --> 00:54:00,979
So we just need to find something which is related to the exposure but is not related to the outcome.

422
00:54:00,980 --> 00:54:06,050
And I think in any instrument through variable analysis, that's the key part is that you need this arrow here,

423
00:54:06,320 --> 00:54:11,990
but there cannot be an arrow here and like proving something doesn't exist can be kind of hard.

424
00:54:12,320 --> 00:54:16,240
So I'm always like a bit wary when I look at instrumental variable analysis and I think,

425
00:54:16,250 --> 00:54:19,400
you know, how do they know that this instrumental variable isn't related to the outcome?

426
00:54:22,500 --> 00:54:29,610
So an example of this say that we were doing a study looking at the relationship between cholesterol levels and cancer.

427
00:54:30,300 --> 00:54:37,620
The problem is there's a lot of confounders here. There are, you know, smoking your diet tumors.

428
00:54:37,620 --> 00:54:40,620
And, you know, like not all tumors will be clinically apparent.

429
00:54:41,880 --> 00:54:45,730
There could be a host of factors which confound this relationship.

430
00:54:45,750 --> 00:54:49,390
Some of them we might be able to measure. Some of them we might not be able to measure.

431
00:54:49,410 --> 00:54:54,690
Some of them we could only measure when it's too late. So then, like, everything gets really funky thinking about the temporal with this.

432
00:54:56,790 --> 00:55:04,320
So some really intelligent people realized that there was this gene which had a relationship with cholesterol levels in the body,

433
00:55:04,560 --> 00:55:06,450
but it was not related to cancer.

434
00:55:07,650 --> 00:55:14,010
And again, that's up to them to make the justification that there is no relationship between the Apple Imaging and cancer.

435
00:55:14,280 --> 00:55:19,020
But we're just going to take them at their word that there's not that relationship there. But there is this relationship here.

436
00:55:21,360 --> 00:55:26,910
So it's actually kind of like a very similar approach for Mendelian randomization.

437
00:55:27,120 --> 00:55:33,330
Is that what you would do is you would create a regression model of these, so you'd specify this,

438
00:55:33,720 --> 00:55:37,830
and then you would come up with a probability that somebody has low cholesterol.

439
00:55:37,830 --> 00:55:42,660
So you'd come up with a propensity score for low cholesterol given their what meal they have.

440
00:55:43,020 --> 00:55:46,170
And then you put that propensity score in your final model.

441
00:55:47,130 --> 00:55:52,740
So that's an instrumental variable analysis or a Mendelian randomization would would work.

442
00:55:59,890 --> 00:56:03,850
Okay. At this time, I'm going to give you a ten minute break.

443
00:56:04,090 --> 00:56:09,150
The other thing I will do is right now I will put up the attendance on poll everywhere.

444
00:56:09,160 --> 00:56:20,730
So please do go back to that in like 10 seconds so that you can you can put in your attendance.

445
00:56:23,960 --> 00:58:17,740
I think you should be good to. I don't believe that.

446
00:58:23,780 --> 00:58:30,860
Really? Yeah, something like that. But yeah, that can be used to make sure, like, if there's anything you.

447
00:58:34,950 --> 00:58:46,780
So I. That's to. When I first heard my father and I became.

448
00:58:52,870 --> 00:58:56,710
And I this one.

449
00:58:57,370 --> 00:59:01,290
Yeah, I think we're going to have to call two one mean.

450
00:59:02,260 --> 00:59:12,770
I don't mind at all I think it might be the. So there you are.

451
00:59:14,870 --> 00:59:22,100
Yep, yep. Yep. And. And then it's like you.

452
01:00:24,230 --> 01:00:36,060
I was just. Yeah.

453
01:00:36,310 --> 01:00:41,350
So how do you know so much about this?

454
01:00:41,400 --> 01:00:45,910
But also. You. It's like.

455
01:00:50,030 --> 01:00:55,360
I know it is significant, but as a result of this,

456
01:00:55,430 --> 01:00:59,509
I don't really reflect and realize all the different studies I have and I think

457
01:00:59,510 --> 01:01:03,210
we've had an advantage because it's easier when I have like a whiteboard.

458
01:01:08,780 --> 01:01:13,380
Exactly what happened at the top of the fourth.

459
01:01:31,220 --> 01:02:02,090
Yeah. The other.

460
01:02:16,770 --> 01:02:24,050
Don't hide from the fact that this.

461
01:03:24,980 --> 01:03:42,800
But. It's hard.

462
01:03:52,150 --> 01:04:11,280
I. Okay.

463
01:04:12,060 --> 01:04:21,410
Let's reconvene. Yeah.

464
01:04:25,060 --> 01:04:30,940
So that. Oh. So.

465
01:04:33,950 --> 01:04:38,420
Let's talk with let's go back to randomized controlled trials.

466
01:04:39,050 --> 01:04:48,560
Yeah, I think 600. You're learning about these. Does anyone remember the difference between a per protocol and an intent to treat analysis?

467
01:04:49,730 --> 01:04:55,610
So you want to explain one or the other of those? Yeah.

468
01:05:06,540 --> 01:05:14,580
Yeah. So intent. Exactly. So intent to create is, you know, we randomized people to get the COVID 19 vaccine or not.

469
01:05:14,970 --> 01:05:21,960
And then we like that randomization event, like the statistician on their computer clicking randomization.

470
01:05:22,590 --> 01:05:28,290
That is the treatment that that is like the exposure.

471
01:05:28,290 --> 01:05:33,810
That's what we are going to be analyzing regardless of what happens with those two groups of individuals.

472
01:05:35,850 --> 01:05:39,839
So that's typically what is done up for protocol approach is where we look through and see like, oh,

473
01:05:39,840 --> 01:05:45,830
at the end of the study, maybe there are a few people who were mistakenly shifted from one group to another.

474
01:05:45,840 --> 01:05:53,850
Maybe there are some people who got the vaccine anyway. So the per protocol is looking to see what their actual behaviors are.

475
01:05:54,180 --> 01:05:59,190
The intent to treat is looking at who did what did we say they were going to do at the beginning of the study,

476
01:06:00,210 --> 01:06:02,910
if you remember back from AP 600 or whatever class,

477
01:06:03,270 --> 01:06:09,000
the reason why we make intent to treat analysis for randomized controlled trials is that it removes all the confounding.

478
01:06:09,540 --> 01:06:11,760
Once we are doing a per protocol approach,

479
01:06:12,240 --> 01:06:18,420
then we could introduce some confounding into it because maybe some of the people who got the COVID 19 vaccine,

480
01:06:18,420 --> 01:06:26,820
even though they were supposed to from this or from the intent to treat approach, maybe they are biased to be a certain type of group.

481
01:06:27,630 --> 01:06:30,780
Maybe they have higher health behaviors, maybe they're wealthier.

482
01:06:31,080 --> 01:06:35,700
You know, there could be all sorts of reasons why their actual behaviors are not in line

483
01:06:35,700 --> 01:06:40,470
with what we thought was going to go on from the from their randomization event.

484
01:06:41,400 --> 01:06:50,370
Does that make sense? So a quick and dirty way of thinking about this is for randomized controlled trials.

485
01:06:50,370 --> 01:06:53,220
Generally, we will report intent to treat of these,

486
01:06:53,400 --> 01:07:00,120
but maybe a per protocol analysis will be like something supplementary or like an additional thing that's put in like an appendix or something.

487
01:07:01,620 --> 01:07:10,199
But for causal inference models, we do upper protocol approach and I think that kind of makes sense because in a causal inference model,

488
01:07:10,200 --> 01:07:17,909
we're not we don't actually have like a moment where we are randomizing people, but that's all to say.

489
01:07:17,910 --> 01:07:23,100
Like these two could have different, different they can have different results.

490
01:07:24,660 --> 01:07:32,790
So let's think about this conceptually. So I have another pull for you to click on.

491
01:07:32,790 --> 01:07:40,440
You should already be activated. So I want you to put your in in for this one.

492
01:07:40,680 --> 01:07:45,959
You will be going back and changing your answer multiple times. Again, I'm not going to link it to you for your grade or whatever.

493
01:07:45,960 --> 01:07:49,080
This is just an example for the class.

494
01:07:50,100 --> 01:07:55,409
Okay. Um, say you are faced in a difficult circumstances.

495
01:07:55,410 --> 01:07:58,380
You just. I know you had stage four brain cancer.

496
01:07:58,980 --> 01:08:06,320
Your neurosurgeon comes to you and says, you know, we have two treatment options which are available for you in treatment.

497
01:08:06,330 --> 01:08:13,620
A, there is a 20% chance of survival over the next five years for treatment.

498
01:08:13,620 --> 01:08:17,790
B, it is 40%. Which one would you want to proceed with?

499
01:08:18,270 --> 01:08:22,200
So with this given information, which treatment would you want?

500
01:08:38,370 --> 01:08:47,700
All right. So people like treatment B and, you know, given the set of information, that is what I would think of as well.

501
01:08:50,340 --> 01:08:55,499
Say the doctor then comes out and says, oh, you know, I just realized you are this criminologist.

502
01:08:55,500 --> 01:08:59,490
You understand these things. I was giving you some incorrect comparisons.

503
01:09:00,030 --> 01:09:07,830
So actually like treatment aid with permanent tend to treat analysis treatment B with proper protocol.

504
01:09:08,430 --> 01:09:17,640
So actually if we like standardize and we look at the analysis for treatment B in the intent to treat approach, the survival is only 15%.

505
01:09:18,660 --> 01:09:19,410
So which one?

506
01:09:19,410 --> 01:09:29,880
Given the set of information where the survival is 20% for treatment, a 15% for treatment B, and this is under the intent to treat analysis.

507
01:09:37,890 --> 01:09:46,320
Give you a couple more seconds to change your answer. Okay.

508
01:09:46,330 --> 01:09:49,800
So now people are like, Oh, doing going around. You're like, I want treatment.

509
01:09:50,160 --> 01:09:57,300
You know, when we're actually doing an apples to apples comparison, it makes sense that we should do treatment.

510
01:09:57,690 --> 01:10:04,380
Okay. Okay. Neurosurgeon comes back and says, you know, I'm so sorry.

511
01:10:04,420 --> 01:10:14,140
This must be the hardest time of your life, but more information for you. If you want to look at the per protocol outcomes, this is what they are.

512
01:10:14,380 --> 01:10:17,500
It is 25% for treatment and 40% for treating.

513
01:10:17,710 --> 01:10:26,350
Again, this 40% is from the initial thing that he said and the 25, 25%, 25% is for treatment.

514
01:10:26,860 --> 01:10:31,720
So which one with the totality of this evidence, what would you want to do?

515
01:10:52,740 --> 01:10:58,200
Okay. So it seems like a bit more even.

516
01:10:58,650 --> 01:11:03,000
Maybe people are still kind of like a bit more on treatment.

517
01:11:03,000 --> 01:11:08,910
Be, you know, you're you're chasing that 40%, which seems like a lot better than all the other options.

518
01:11:10,020 --> 01:11:11,250
So how should you think through this?

519
01:11:11,610 --> 01:11:17,820
So, you know, there isn't like one exact approach to this, and I would probably want to delve a bit more into the details of this.

520
01:11:18,570 --> 01:11:26,490
But the story I see here is that, you know, there's a substantial difference for these, especially for treatment, B, for treatment.

521
01:11:26,610 --> 01:11:35,490
These are, you know, relatively the same. And oftentimes you will see that the per protocol benefits are higher than the intent to treat.

522
01:11:36,240 --> 01:11:42,110
But this is saying to me, though, is that there is substantial drop off for treatment B compared to treatment aid.

523
01:11:42,750 --> 01:11:46,590
So maybe there's also side effects for treatment B, so maybe, you know,

524
01:11:46,590 --> 01:11:59,640
the reason why the film is so there's so much difference is that people are not able to, you know, adhere to the regimen of treatment B as much.

525
01:12:00,030 --> 01:12:07,980
So looking at this though, like I would hang on to the treatment B, I would hang on to the per protocol analysis because in my decision making,

526
01:12:08,190 --> 01:12:15,510
what's happening is that like other patients who are out there just are unable to continue on with the treatment.

527
01:12:16,050 --> 01:12:19,170
So for me, you know, like somebody would want to survive.

528
01:12:19,710 --> 01:12:25,320
This would be like extra motivation for me to like continue on with the, the treatment.

529
01:12:25,320 --> 01:12:32,549
B And just to realize, to be aware that there potentially are these like side effects associated with it.

530
01:12:32,550 --> 01:12:37,920
There are these reasons why people are, you know, not adhering to the protocol.

531
01:12:40,700 --> 01:12:43,939
That's how I would view it. You know, maybe you'd want to do, like, a bit more research on that.

532
01:12:43,940 --> 01:12:47,089
And but that that's my decision making.

533
01:12:47,090 --> 01:12:50,680
Again, this isn't like there's no there really isn't a correct answer here.

534
01:12:50,690 --> 01:12:57,379
Like maybe in your mind, it really would be important to kind of go back to this and tend to treat maybe there's some confounding

535
01:12:57,380 --> 01:13:03,050
which you still are not able to completely trust how they analyze that for the protocol approach.

536
01:13:03,050 --> 01:13:10,130
But that's how I would put many thoughts from the audience or conclusions.

537
01:13:12,950 --> 01:13:17,989
Okay. But this is all to say.

538
01:13:17,990 --> 01:13:33,379
There was this one paper which kind of did this, this systematic review, and they looked at, I think this was treatment of HIV regardless.

539
01:13:33,380 --> 01:13:37,340
What they were trying to do is compare two different sets of studies,

540
01:13:37,340 --> 01:13:46,100
one which looked at causal inference models and one which looked at randomized controlled trials, and they found some diversity.

541
01:13:47,420 --> 01:13:51,830
So like some people, you know, essentially everything was within the confidence interval,

542
01:13:52,340 --> 01:13:57,260
but for some there was like seemingly more of a positive benefit using the randomized controlled trial,

543
01:13:58,730 --> 01:14:06,230
whereas there's more of a negative benefit for the causal inference. So there is, you know, different ways of thinking through this.

544
01:14:06,230 --> 01:14:16,100
But a lot of epidemiologists would look at this and say, this just doesn't make sense because the causal inference models are a per protocol approach,

545
01:14:16,100 --> 01:14:18,290
whereas randomized controlled trials are intent to treat.

546
01:14:18,590 --> 01:14:25,760
So it's like, again, not only is it like apples and oranges with looking at experiments versus observational studies,

547
01:14:25,760 --> 01:14:31,580
but just like the way they're analyzing stuff from like an intent to treat versus a protocol approach is like different.

548
01:14:31,940 --> 01:14:36,200
So it's sort of like apples and asparagus or something, you know, it's it's just very different.

549
01:14:40,060 --> 01:14:47,560
But you know, if there's any intuition that I could put in your mind, it's that the per protocol approach probably makes more sense just because then,

550
01:14:47,560 --> 01:14:53,920
like, people are actually doing what we said they would be doing as part of the the protocol.

551
01:14:54,040 --> 01:14:58,570
But if there is a difference between the protocol intent, you kind of want to delve into like why that is the case.

552
01:14:59,470 --> 01:15:03,340
And then that could be like an interesting side piece of your analysis.

553
01:15:06,100 --> 01:15:11,620
Okay. So for the last third of class today, I want you to think about counterfactuals.

554
01:15:13,300 --> 01:15:17,020
Okay. So I will be putting you into groups,

555
01:15:17,620 --> 01:15:27,310
and I will want you to think about how you would do how you would do randomized controlled trials for these different studies.

556
01:15:28,510 --> 01:15:34,240
Some of these, like, might be, like, uncomfortable to think through. Some of these might be unethical designs.

557
01:15:35,320 --> 01:15:39,850
Some of them just might not be possible at all. Those are the things that I want you to delve into.

558
01:15:40,210 --> 01:15:48,250
So this isn't necessarily like easy answers for these, but I want you to start thinking about like, what is the counterfactual?

559
01:15:51,130 --> 01:15:56,650
So just let me back up to easy example and then we will be doing these hard ones for you in your groups.

560
01:15:58,420 --> 01:16:01,569
Me I do office studies on vaccine effectiveness,

561
01:16:01,570 --> 01:16:07,660
so I have this I've been working with that reporting on this analysis of of influenza vaccine effectiveness in Nicaragua.

562
01:16:08,020 --> 01:16:16,089
And so what I do is I look at the children who have had an influenza vaccine, those who do not have an influenza vaccine.

563
01:16:16,090 --> 01:16:21,490
And I look at their health outcomes afterwards. A randomized controlled trial would just be in that same group.

564
01:16:21,970 --> 01:16:26,980
I would randomize it. So half get the vaccine impacted, if that makes sense.

565
01:16:26,980 --> 01:16:34,530
And like for a vaccine, it's very easy. It's just like that means half the people we, you know, find them.

566
01:16:34,540 --> 01:16:42,730
We make sure that they get the jab and then half do not. Okay, so let's make this more complicated.

567
01:16:43,930 --> 01:16:49,090
What does it mean to look at sugar and diabetes?

568
01:16:53,460 --> 01:16:59,420
And to maybe make this, like, a bit more consistent.

569
01:16:59,580 --> 01:17:06,450
So I'll, I'll, I'll introduce all three of these, but I'll maybe we'll we'll go one at a time for them.

570
01:17:06,600 --> 01:17:11,550
So what's the relationship between sugar and diabetes? How would you how would you do randomized controlled trials?

571
01:17:11,850 --> 01:17:15,210
You're emperor of the world. You have no ethical standards.

572
01:17:15,810 --> 01:17:23,490
How do you figure this out? Then how do we do something where we look at the effect of weight and diabetes?

573
01:17:24,090 --> 01:17:27,930
Because we'll often say, oh, you know, if you weigh more, you have a higher risk of diabetes.

574
01:17:28,650 --> 01:17:34,590
How would you design a randomized controlled trial where we look at the relationship between these factors and then,

575
01:17:35,570 --> 01:17:42,570
you know, we often will say, oh, people of different racial or ethnic backgrounds will have a higher risk of diabetes.

576
01:17:42,630 --> 01:17:48,300
How would you do a randomized controlled trial to look at the relationship between different studies?

577
01:17:49,050 --> 01:17:55,080
So I will I want these to be fruitful discussions and maybe give you about 10 minutes and then we'll come back to the.

578
01:18:01,840 --> 01:18:15,620
Yeah. So.

579
01:18:19,440 --> 01:18:44,810
For the past five years. So I.

580
01:18:50,910 --> 01:19:03,110
It's kind of like natural killer.

581
01:19:12,180 --> 01:19:19,680
What? Did I say what they said?

582
01:19:19,820 --> 01:19:31,100
Yeah. Okay.

583
01:19:34,750 --> 01:19:42,330
Have. Even though after.

584
01:19:53,800 --> 01:20:12,720
Yeah. Dial in number 375.

585
01:20:13,090 --> 01:20:23,660
I don't even think. So.

586
01:20:28,730 --> 01:20:41,800
Investigators are saying that to be able to begin to trigger.

587
01:20:49,630 --> 01:20:58,480
I think that would be. And you might even think about it as wanting to be like, Oh, this people get this Nicobar and this, people just not.

588
01:20:59,350 --> 01:21:04,480
But like, if you really want to be on a boat, you could have like somebody follow them. Like, every time they be.

589
01:21:04,730 --> 01:21:08,040
Like, the controller is gone from the secret. Yeah.

590
01:21:09,280 --> 01:21:12,920
Yeah. But then, like, conversely, other times, the other group, you feel like you've.

591
01:21:15,310 --> 01:21:18,490
So that there's different layers for that.

592
01:21:18,730 --> 01:21:22,590
Because like the other thing, if you just ran, I think it's going to be like.

593
01:21:25,960 --> 01:21:34,660
How do you respond to the shooter from that attack? And because they're innocent, millions of people are going to make it.

594
01:21:38,820 --> 01:21:54,410
Yeah. During the recent of the first.

595
01:22:09,160 --> 01:22:16,700
Oh, yeah.

596
01:22:21,540 --> 01:22:28,190
That would seem to me to be just like.

597
01:22:31,420 --> 01:22:34,980
As far as.

598
01:22:38,560 --> 01:22:44,190
What? Kind of like they're like, well.

599
01:22:48,010 --> 01:22:51,580
There's an actual damage here.

600
01:22:52,260 --> 01:23:03,450
So I. As long as.

601
01:23:06,500 --> 01:23:28,560
Half an hour and a half. I feel like they're not able to piece together a representative.

602
01:23:39,830 --> 01:23:50,490
Yeah. Oh, yeah. That's.

603
01:23:56,690 --> 01:24:00,100
The 20. Yeah.

604
01:24:00,940 --> 01:24:16,940
You know, I think that's a pretty good idea. That's very.

605
01:24:30,320 --> 01:24:35,000
There's there that.

606
01:24:42,610 --> 01:24:49,350
Few seconds. But as I said before.

607
01:24:52,770 --> 01:24:58,300
Yeah, yeah, yeah. You go through the same process.

608
01:25:00,130 --> 01:25:18,850
But they. Okay.

609
01:25:19,510 --> 01:25:27,370
Let us think through this. Okay. Let me begin with the.

610
01:25:29,680 --> 01:25:31,700
Two of the Type two errors.

611
01:25:32,150 --> 01:25:39,810
What is your example of doing a randomized controlled trial or looking at the relationship between a sugar diet and diabetes?

612
01:25:39,830 --> 01:25:44,450
Again, we're now we're even at this. You have an infinite lot of money to do this.

613
01:25:44,750 --> 01:25:48,200
How do you do this? Not to be bothered by.

614
01:25:49,550 --> 01:26:00,360
But my father died at. Yeah.

615
01:26:00,450 --> 01:26:05,070
I mean, that that sounds like a traditional randomized controlled trial design.

616
01:26:06,000 --> 01:26:12,090
I would advocate even like pushing further the ethics envelope because, you know,

617
01:26:12,270 --> 01:26:16,349
we're just like, oh, this group should you should have a high sugar diet.

618
01:26:16,350 --> 01:26:23,219
This group. We should not have a high sugar diet. Um, maybe what you're randomizing there is just like information like this.

619
01:26:23,220 --> 01:26:28,230
One group gets information due to high sugar. This one group gets the information, not high sugar.

620
01:26:28,950 --> 01:26:37,950
Like what you could do in your example is you could have like one thing we could do is just require them to eat food made by our study.

621
01:26:38,780 --> 01:26:43,890
And so they're like shipped their blue apron, but like one group has of high sugar in it.

622
01:26:44,220 --> 01:26:52,530
One group does not. Um, you can even think about it as you could, like, assign somebody to, like, healthy people in their life.

623
01:26:52,890 --> 01:26:58,680
And every time they, like, reach for a candy bar, if they're in the low sugar diet, like that person floated away,

624
01:26:59,820 --> 01:27:03,840
you know, or it in any other group, the people are trying to eat like a vegetable or something.

625
01:27:03,840 --> 01:27:11,219
Again, they're like, whatever. So you could like but then you also even with that,

626
01:27:11,220 --> 01:27:19,290
there's complications because like then are we disentangling like the sugar versus like what is the nutritional content of the other, other group?

627
01:27:19,830 --> 01:27:23,580
Um, maybe the simplest thing would be they have the same blue apron meals,

628
01:27:23,850 --> 01:27:29,370
but one group has like a cup of sugar added every day throughout their meal.

629
01:27:29,370 --> 01:27:33,120
And the other group sounds delicious in the event.

630
01:27:34,370 --> 01:27:38,070
Oh, yeah. Does that make sense? Okay.

631
01:27:38,670 --> 01:27:44,249
Um. Pieces of card. What would you say for weight on diabetes?

632
01:27:44,250 --> 01:27:52,410
How would you do that? Um, they have no choice but to do so.

633
01:27:52,500 --> 01:27:57,060
Yeah, but. All right, well, that's ten more years, and it's not that.

634
01:27:57,990 --> 01:28:02,459
And then we'll, like, give full randomized.

635
01:28:02,460 --> 01:28:09,370
You know, half the people will, like, get medically induced weight gain, you know, some sort of medicine.

636
01:28:11,110 --> 01:28:15,290
Protagonist in the other. Just happened to be there.

637
01:28:16,170 --> 01:28:19,900
And, you know, you don't do much of anything other than sleep and exercise.

638
01:28:20,620 --> 01:28:25,320
Uh huh. Yeah. And then you get the same thing.

639
01:28:25,330 --> 01:28:29,950
I mean, they already get the same things. Their first choice.

640
01:28:30,470 --> 01:28:34,320
Uh, and then after some time. You know, standing.

641
01:28:35,020 --> 01:28:39,710
There's. So I like her going in a number of different ways.

642
01:28:39,730 --> 01:28:46,810
One is the prison because there's no like the oversight is probably like a bit different and maybe a bit less there.

643
01:28:48,520 --> 01:28:53,490
I like also what you were saying, which was that these groups, actually, they would have the same dialogs.

644
01:28:53,500 --> 01:28:57,280
And I think in prison you can, you know, make it so that things are pretty even.

645
01:28:57,790 --> 01:29:03,310
But your difference is that there's one group which and I think you were kind of a bit vague about this,

646
01:29:03,820 --> 01:29:13,090
has some sort of like medically induced weight gain if they like a leptin antagonist would be something.

647
01:29:13,300 --> 01:29:22,140
I mean, are there ways of doing like. Like a reverse tummy tuck or something, I don't know.

648
01:29:22,560 --> 01:29:28,530
But it's all there, you know, and it's like know exercise routine.

649
01:29:28,980 --> 01:29:34,620
That's true. The problem with that then I feel like the two things is your first example.

650
01:29:34,620 --> 01:29:42,420
Where you're actually randomizing is whether the leptin antagonist is the thing which is causing diabetes or not.

651
01:29:42,690 --> 01:29:45,930
And maybe your argument would be that like that's predominately through drinking,

652
01:29:46,170 --> 01:29:54,230
but maybe there's all sorts of other things with that this, this drug does with like insulin and like glucose uptake in the body.

653
01:29:54,240 --> 01:29:59,100
I don't know if you put somebody in solitary again,

654
01:29:59,100 --> 01:30:05,790
maybe there's just like all sorts of psychological trauma from that, like there could be other pathways and all that.

655
01:30:08,730 --> 01:30:15,630
But this is all to say, like it is really so like I think it is very difficult to think about the risk of weight.

656
01:30:16,380 --> 01:30:23,260
So whenever there's like something which is out there on, you know,

657
01:30:23,370 --> 01:30:30,779
what is the relationship between like being obese or overweight and different health outcomes?

658
01:30:30,780 --> 01:30:34,500
And then it's often like you'll be really high risk if you are a certain group.

659
01:30:34,860 --> 01:30:38,820
It's just hard because like there's no randomized controlled trial that you can actually do.

660
01:30:39,120 --> 01:30:46,830
I think that that is like what my answer would be here, because there is, you know,

661
01:30:48,000 --> 01:30:52,710
even something like a reverse liposuction or whatever, like that would have like so much trauma on the body.

662
01:30:53,010 --> 01:30:56,430
It probably just like wouldn't work. Like, what does that even mean?

663
01:30:56,970 --> 01:31:00,840
If you give somebody a drug, then you're evaluating the impact of the drug motivated self.

664
01:31:02,490 --> 01:31:06,540
So there's, there's all these things which just like don't it doesn't make sense.

665
01:31:06,540 --> 01:31:10,200
And certainly weight could be a mediator. And I think that's maybe the thing to put in your mind.

666
01:31:10,260 --> 01:31:18,630
Like maybe there is something out there. Um, and this isn't to say that we can't study the relationship between weight and other things.

667
01:31:18,900 --> 01:31:21,960
I would just be careful about, like the causal language associated with it.

668
01:31:22,590 --> 01:31:23,670
No, that is the thing.

669
01:31:24,810 --> 01:31:32,639
There might be people out there who are a bit more confident in how they are doing their studies and how they're analyzing these things.

670
01:31:32,640 --> 01:31:40,799
So there could be people out there who say, like, we can have a reasonable causal approximation of the effect of weight on diabetes.

671
01:31:40,800 --> 01:31:44,670
So I'm not trying to like dismiss entire fields just in my mind.

672
01:31:44,670 --> 01:31:49,979
Like my initial reaction to this is like talking a bit more observationally

673
01:31:49,980 --> 01:31:58,050
about this and using words like association might be like a bit more useful.

674
01:32:00,810 --> 01:32:06,570
Okay. Now let's look at the effect of race on diabetes.

675
01:32:06,930 --> 01:32:12,180
What would a counterfactual change mean? Let's move to the AP test here.

676
01:32:12,210 --> 01:32:21,720
What are your thoughts on. So like the.

677
01:32:23,530 --> 01:32:27,040
For examining that divide.

678
01:32:27,490 --> 01:32:35,520
We could say that. So. Time I figured I sound like.

679
01:32:39,410 --> 01:32:47,890
So we didn't want to have an non-Hispanic white resistance in those who had.

680
01:32:51,620 --> 01:32:57,860
So I mean, that is like a typical observational way that we would look at this is we would see like in this population,

681
01:32:58,130 --> 01:33:01,160
what's it like for people who are, you know,

682
01:33:01,190 --> 01:33:05,419
non-Hispanic black, who are Hispanic or maybe, you know, non-Hispanic, black, Hispanic,

683
01:33:05,420 --> 01:33:08,900
black, non-Hispanic, white, Hispanic, white, you know, other groups as well.

684
01:33:09,800 --> 01:33:14,680
And we see, you know, what do they do developing?

685
01:33:15,320 --> 01:33:22,610
What is their rate of developing diabetes? So that's information like observational information that we have like a pretty good feel for.

686
01:33:23,270 --> 01:33:30,380
The randomized controlled trial trick is to be like, is there any way to randomized race on an individual level?

687
01:33:30,770 --> 01:33:37,550
Because like we could think of randomizing sugar on an individual level, there might be highly unethical ways of doing that for weight.

688
01:33:37,580 --> 01:33:41,239
It's like hard to because, you know, do you give black people medication, whatever?

689
01:33:41,240 --> 01:33:44,390
What is that? But for race, I think it becomes even harder.

690
01:33:45,260 --> 01:33:52,910
But I think it's important for us to think about. And again, I think a lot of this might come off as like very uncomfortable to think about.

691
01:33:53,150 --> 01:33:59,900
But I think then a lot of what we say about there being like higher risk of disease in certain

692
01:33:59,900 --> 01:34:05,630
groups or not also have like some sort of uncomfortable analysis associated with it as well.

693
01:34:06,560 --> 01:34:12,590
I guess it's like my own inclination that I don't think that you can randomized race on an individual

694
01:34:13,460 --> 01:34:20,630
because like even I think that the scientific understanding is that there's not a genetic basis to race.

695
01:34:21,410 --> 01:34:25,340
The race is a social construct and of course it has like a lot of important implications.

696
01:34:27,080 --> 01:34:33,379
But even if it were if there was like a gene, then are we doing like a genetic?

697
01:34:33,380 --> 01:34:39,560
Could we take somebody in a black family and like switch their child to being weighed against?

698
01:34:39,560 --> 01:34:45,050
This seems like really awkward to think about. Could you take a white family and switch their child to be black?

699
01:34:45,920 --> 01:34:48,229
And I think that many people would say, well,

700
01:34:48,230 --> 01:34:56,300
that's that's not like the only experience of being black or white is it's not only what you look, it's like, what is your social environment?

701
01:34:56,660 --> 01:35:00,979
What is the, you know, like the intergenerational relationships.

702
01:35:00,980 --> 01:35:06,530
Like, it's not as easy as pinning down to a certain team, even if, like there was one gene.

703
01:35:06,530 --> 01:35:09,980
And I don't think there is. I don't even think there's like a set of genes there.

704
01:35:12,080 --> 01:35:23,030
So for me, like because race is so tightly linked with like social issues, with your environment,

705
01:35:23,030 --> 01:35:30,859
with where you were born, with what your family environment is like, you can't just like randomize all of these things.

706
01:35:30,860 --> 01:35:34,970
I think that would be like very improbable to do.

707
01:35:35,960 --> 01:35:48,360
So that is my reaction to this. I will say that there are people, though, who do study like race ethnicity in a causal inference approach.

708
01:35:48,360 --> 01:35:51,439
So I'm not trying to be dismissive of that and I'm also not trying to say that,

709
01:35:51,440 --> 01:35:56,299
like we need to like throw this under the rug and never do any kind of analysis like this again,

710
01:35:56,300 --> 01:35:59,600
or that we should just be like ignoring race, ethnicity, because I do think it's really important.

711
01:36:00,140 --> 01:36:05,960
What I'm saying is that we should be very careful with our words that we use, because I think these like causal words,

712
01:36:06,380 --> 01:36:11,060
when we say these causal words, you're going back to can we do a randomized controlled trial or not?

713
01:36:11,330 --> 01:36:18,739
And like in my mind, we just yeah, again, there are some people who might argue against that and there are many people,

714
01:36:18,740 --> 01:36:23,330
again, who work with, um, with race, ethnicity in more of like a causal way.

715
01:36:24,110 --> 01:36:30,730
I just think it's, it's very, very difficult. Any questions about?

716
01:36:35,830 --> 01:36:39,130
So to sum up, why do we like Puzzle and Prince methods?

717
01:36:40,330 --> 01:36:44,860
Let's go back to these propensity scores again. This is something you might have to sit with a bit.

718
01:36:45,360 --> 01:36:52,240
But propensity score is kind of give you a visualization of how you deal with confounding or not.

719
01:36:52,510 --> 01:36:55,300
In a situation like this, you can do with confounding. And these ones.

720
01:36:55,600 --> 01:37:01,520
There's like complete separation, even including those confounders in your multivariable model.

721
01:37:01,540 --> 01:37:09,280
They won't do anything. So it's actually nice. Maybe even if you even still do a multivariable model approach and don't use marginal structure models,

722
01:37:09,580 --> 01:37:13,659
just seeing that the propensity scores are similar can kind of give you that feeling of,

723
01:37:13,660 --> 01:37:21,969
Oh, this is like a nice, a nice thing, which makes sense for a marginal social model so you can do your time during confounders.

724
01:37:21,970 --> 01:37:28,240
That's really nice for an instrumental variable analysis, you can deal with unmeasured confounders.

725
01:37:28,840 --> 01:37:33,520
But I think like a bigger point to this, because I realize in five years,

726
01:37:33,520 --> 01:37:39,700
if you are not a researcher who is doing like marginal structure models on the regular,

727
01:37:39,970 --> 01:37:42,580
you're probably going to forget a lot of over in fact about today.

728
01:37:43,150 --> 01:37:48,640
But one thing that I would like you to keep in mind is this idea of like, can you do a randomized controlled trial or not?

729
01:37:49,180 --> 01:37:53,259
And maybe it's a very unethical thing, but like I would say, for sugar diets,

730
01:37:53,260 --> 01:38:01,480
we can kind of approach more of a randomized controlled trial where you're thinking about things that we can use more causal language.

731
01:38:01,960 --> 01:38:07,930
When we start talking about things a bit more nebulous, like things like weight gain,

732
01:38:08,590 --> 01:38:15,129
things like race, ethnicity, those things that it's just like really hard to think about.

733
01:38:15,130 --> 01:38:18,270
What would a randomized controlled trial mean? Mm hmm.

734
01:38:19,210 --> 01:38:23,620
So we will I'll be hammering this into your head over the next few weeks, because I do think this is an important point.

735
01:38:25,240 --> 01:38:28,420
But for now, I will leave you with that.

736
01:38:29,410 --> 01:38:35,780
Here's the study guide for the rest lecture. But I'll say here, if you have any other questions otherwise, thanks for coming first and I'll see you.

737
01:38:35,800 --> 01:38:36,160
I think.

