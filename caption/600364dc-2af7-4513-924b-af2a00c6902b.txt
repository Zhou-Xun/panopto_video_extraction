1
00:00:06,870 --> 00:00:07,769
All right. As usual,

2
00:00:07,770 --> 00:00:15,089
I want to just quickly go over administrative things about where we are in the course and and things I want to make sure you're all aware of.

3
00:00:15,090 --> 00:00:26,430
So we are here October ten and I'm hoping to both finish hand out now and get as close to finishing hand out ten as possible because of course,

4
00:00:26,430 --> 00:00:29,490
we have a long period of time before we.

5
00:00:30,460 --> 00:00:36,370
Come back to handouts. We're going to have a review of quiz materials on Wednesday.

6
00:00:38,180 --> 00:00:45,750
So take a look at your notes, try to get questions in place, because the handout I have for Wednesday is I have time.

7
00:00:45,760 --> 00:00:49,249
I will talk and do everything myself if you don't come with questions.

8
00:00:49,250 --> 00:00:54,709
But it's meant to be a framework for you to say, Oh, yeah, I have a question on this part.

9
00:00:54,710 --> 00:00:59,210
Can we you know, it's meant for you to use the time for yourself to study.

10
00:01:00,220 --> 00:01:05,620
So you could passively listen to what my review is, but I come with questions.

11
00:01:05,620 --> 00:01:16,890
That'll be a good use of your time if you have some. And then on, you know, the following Monday, it's study break time so we won't have class.

12
00:01:16,900 --> 00:01:20,290
I will continue to have office hours.

13
00:01:21,380 --> 00:01:32,750
Uh oh. And I should probably mention I'm traveling this Friday again, so I will either cancel or move my Friday office hours.

14
00:01:32,750 --> 00:01:40,400
I haven't really looked at my calendar to see if there's a good time to move them, but I will have office hours again on Monday.

15
00:01:40,790 --> 00:01:46,880
So if you have a habit of coming to my office hours because you like the way I do office hours,

16
00:01:47,300 --> 00:01:58,490
just know Fridays are not going to work out because I'll be somewhere in northern Michigan and with maybe with no internet.

17
00:01:59,870 --> 00:02:05,510
So try to get your homework questions handled earlier in the week if you want to talk to me.

18
00:02:07,710 --> 00:02:13,920
And then, of course, on the 19th, we have a quiz and it's going to be the online canvas quiz.

19
00:02:13,920 --> 00:02:17,850
So it will appear in your your quizzes section.

20
00:02:18,990 --> 00:02:26,730
And at this point, I'm assuming you've done a lot of online quizzes, but there are practice quizzes posted.

21
00:02:28,680 --> 00:02:39,839
Practice quiz one is available in your. This is the student view, what you would see if you click on that quiz link and the practice quizzes there,

22
00:02:39,840 --> 00:02:49,530
just to give you a sense of the level of the material and to give you you know, I have the format is going to be very similar to what I put up.

23
00:02:49,530 --> 00:02:55,020
So there will be a handout that is for SAS users with code coding output.

24
00:02:55,380 --> 00:02:59,730
There's going to be a handout for our users with code output and you can get used to kind of

25
00:03:00,390 --> 00:03:04,980
organizing your screen and make good use of that material as you're going through the questions.

26
00:03:06,110 --> 00:03:14,750
So it's just magic to orient you. The questions will be entirely different from the practice questions, and the material covered will still be from,

27
00:03:15,810 --> 00:03:20,540
you know, the same handouts, but it might not cover the same topics.

28
00:03:20,540 --> 00:03:28,680
Right? Okay. And because we have that quiz coming up.

29
00:03:31,420 --> 00:03:35,410
And this homework three is due the Sunday before the quiz.

30
00:03:35,890 --> 00:03:39,960
I sometimes have flexibility to give extensions when things come up for students.

31
00:03:39,970 --> 00:03:48,460
I don't have that flexibility this time. I need to post homework solutions very soon after the deadline so people will have those to study from.

32
00:03:49,270 --> 00:03:54,970
And so please, please get your homework done earlier.

33
00:03:55,150 --> 00:04:04,030
So last minute issues that do happen in real life like computer connectivity issues on the, you know,

34
00:04:04,600 --> 00:04:09,580
whatever windstorm throws everything out of whack on Sunday, just as you're about to submit.

35
00:04:10,060 --> 00:04:18,700
You know, just try to get this done early because I have very I don't have the possibility, really, of giving extensions this time.

36
00:04:18,700 --> 00:04:24,370
And I have a little bit of a. A reminder here to plan accordingly.

37
00:04:25,480 --> 00:04:32,379
So just take note. I do drop the lowest homework grades.

38
00:04:32,380 --> 00:04:40,300
So if you really get stuck, it's not going to hurt. It's not going to ruin your grade necessarily unless you've already used to that trick.

39
00:04:43,030 --> 00:04:47,030
But enough said. I think I've said enough. Don't be late for this homework.

40
00:04:47,050 --> 00:04:52,030
I can't really do much about it to help you other than have it be your dropped homework.

41
00:04:56,020 --> 00:05:01,149
And it's probably a good time to remind you that during the quiz you're allowed to have a page,

42
00:05:01,150 --> 00:05:03,980
front and back of notes that can have whatever you want.

43
00:05:04,030 --> 00:05:10,690
So as you were studying, don't forget to work on that page of notes front and back that you're allowed to have with you.

44
00:05:11,830 --> 00:05:17,590
It's a good time to kind of get that rolling. All right.

45
00:05:17,590 --> 00:05:25,830
Are there any questions about administrative stuff? Okay.

46
00:05:26,100 --> 00:05:30,450
CNN, I'm going to go ahead and get to work on the handouts.

47
00:05:30,450 --> 00:05:38,849
So we were learning survival analysis and. So it's a fun topic, but there's a lot of notation involved.

48
00:05:38,850 --> 00:05:48,840
And so just because it's been a while since Wednesday when we were working this, let me just give you a little bit of a review of the notation.

49
00:05:49,440 --> 00:05:56,390
Let's go to a good notation slide. This is kind of a summary.

50
00:06:01,900 --> 00:06:06,040
All right. Let me double check. I did hit the recording. Good.

51
00:06:10,670 --> 00:06:20,690
All right. So so remember that we're looking at times two and we're going to be using subscripts for individual in the dataset.

52
00:06:21,110 --> 00:06:29,960
So all the random variables that are involved with this handout, the subscripts, tend to tell us which individual we're talking about.

53
00:06:30,350 --> 00:06:34,339
We also have this little T that is not a random variable.

54
00:06:34,340 --> 00:06:40,040
It's just the time on study or the time from the beginning of follow up.

55
00:06:40,160 --> 00:06:50,120
And so just to kind of mark a concrete time to think about, I've always been using ten time equals ten in this handout for little T.

56
00:06:51,940 --> 00:06:57,180
The random variables. I tend to use capital letters for all the rate of variables, just like your other professors probably did.

57
00:06:57,760 --> 00:07:03,370
And so the random variable we'd like to evaluate is capital T for a little subject.

58
00:07:03,370 --> 00:07:06,670
I the time from the beginning,

59
00:07:06,680 --> 00:07:13,000
a follow up to the event time you're interested in for that individual but you don't always get to see t we

60
00:07:13,150 --> 00:07:20,650
we have another random variable that's also kind of in the works here and we call that the censoring time.

61
00:07:20,650 --> 00:07:30,530
So it's a random variable to capital c i. And you can think of it as the potential follow up time that you would be able to watch someone.

62
00:07:32,330 --> 00:07:38,090
But we don't always see it either. If the person has the event, we we might stop watching them.

63
00:07:38,090 --> 00:07:41,059
And so we don't know what the value of CII is.

64
00:07:41,060 --> 00:07:50,810
So they kind of you only get to see the minimum of either the event time you're interested in or this follow up time.

65
00:07:51,980 --> 00:07:56,900
And so even in the data set, you'll have a number x AI.

66
00:07:57,410 --> 00:08:01,220
That is basically the time you observe them.

67
00:08:02,610 --> 00:08:08,700
Whether you observed them until their death or you observed them until their last follow up time.

68
00:08:09,330 --> 00:08:13,260
And then Delta Eye is kind of the reason you stopped watching.

69
00:08:13,290 --> 00:08:20,520
So if Tia is less than C.I., that means they had the event before you lost.

70
00:08:20,860 --> 00:08:30,000
You stop watching them. And so if Delta is one, it means that this length of observation time stopped because they had a death.

71
00:08:30,010 --> 00:08:34,050
And if a delta is equal to zero, the length you stop watching them.

72
00:08:35,510 --> 00:08:39,410
But they were still alive. When you stop watching. And the just the follow up just ended for them.

73
00:08:40,710 --> 00:08:45,480
So the outcomes require you to look at both of these random variables.

74
00:08:47,380 --> 00:08:52,220
We're eventually going to have handouts. We'll start having handouts for group in the, you know,

75
00:08:52,270 --> 00:08:58,660
following this and then we'll get to a regression situation with many covariates in the handout even after that.

76
00:08:59,320 --> 00:09:07,120
And so an assumption that I keep coming back to is that for the methods to be valid, when you've got this kind of censored data,

77
00:09:07,840 --> 00:09:12,819
the random variable for the time to event you're interested in has to be independent

78
00:09:12,820 --> 00:09:17,950
of this censoring random variable given the covariates that you have in your model.

79
00:09:18,730 --> 00:09:25,180
And so for this handout, we don't really have any covariates, but in the next handout we will and and we'll keep adding more.

80
00:09:27,010 --> 00:09:30,219
All right. So I'm not going to go through all the notation examples.

81
00:09:30,220 --> 00:09:36,550
That was just a mental memory jog. And we had talked about.

82
00:09:38,660 --> 00:09:44,260
KAPLAN-MEIER estimates. And.

83
00:09:46,950 --> 00:09:50,310
Well, maybe I do want to just say a little bit about Kaplan-Meier estimates.

84
00:09:55,910 --> 00:09:58,910
Because we had all this notation involved in that, too.

85
00:09:59,630 --> 00:10:03,530
So let's let's review this slide as well. And so.

86
00:10:04,760 --> 00:10:14,090
The notation you need to do as to, you know, to understand what's in the sounds or output are involves first of all.

87
00:10:15,060 --> 00:10:20,780
Collecting all of the event times and I'm going to call them deaths or failures.

88
00:10:20,790 --> 00:10:25,200
I sometimes just slip right into calling them deaths, even if there's some other type of event.

89
00:10:26,170 --> 00:10:31,650
And you take all the unique ordered. Death times or failure times.

90
00:10:31,860 --> 00:10:35,790
So this is NDI is the number of unique times at which you have the events.

91
00:10:35,820 --> 00:10:43,200
See, I already slipped into death here, observed. And then from that you do some bookkeeping.

92
00:10:43,200 --> 00:10:47,840
So we called YJ the number at risk at one of these t j.

93
00:10:47,850 --> 00:10:54,990
So as j changes, you're talking about a different event time and how many people possibly could have had the event at that time?

94
00:10:56,100 --> 00:11:05,639
And then GE was the number of failures at that time. So we looked at the Kaplan-Meier estimate here in terms of this formula.

95
00:11:05,640 --> 00:11:06,600
I remember this guy.

96
00:11:06,610 --> 00:11:16,530
This is a product symbol and you're looking at all the observed failure times up until the time t that you're looking at on the kaplan-meier curve.

97
00:11:17,220 --> 00:11:20,340
So think of t as little as ten if that helps you,

98
00:11:20,820 --> 00:11:31,350
and then multiplying one minus j over y j for all of those observed failure times less than or equal to that little time t or ten.

99
00:11:32,310 --> 00:11:42,210
And so for our data set, we only had we had a mini data set where we only had two observed event times less than ten to an eight.

100
00:11:43,450 --> 00:11:53,559
And so the Kaplan-Meier estimate here was basically multiplying at the at those observed event times

101
00:11:53,560 --> 00:12:02,320
this one minus ever YJ for the T equals two and the one minus t j over y j for the t j equals eight.

102
00:12:02,320 --> 00:12:07,150
And these two product together gave us our kaplan-meier estimate here at time eight.

103
00:12:08,290 --> 00:12:15,519
And we also sort of just as a mental jog, these kaplan-meier estimates kind of look like flat, flat, flat.

104
00:12:15,520 --> 00:12:20,560
And then at an event time it drops and then it's flat, flat, flat again and then it drops.

105
00:12:20,980 --> 00:12:31,780
So it only drops that event time. So any time but after eight and before 15, this same kaplan-meier estimate is going to be the estimate.

106
00:12:31,780 --> 00:12:35,320
It'll stay at .78 until the next failure at 15.

107
00:12:35,950 --> 00:12:39,940
So if we're interested in the kaplan-meier time, little T equals ten.

108
00:12:40,240 --> 00:12:49,480
We're really looking to the row before ten and pulling those numbers off the table and and pulling those numbers out of the output.

109
00:12:50,650 --> 00:12:54,000
All right. Okay.

110
00:12:54,020 --> 00:12:56,030
So that's kind of more or less where we were.

111
00:12:56,030 --> 00:13:05,770
We hit some of the highlights, you know, and now let's get to, you know, how do you how do you understand the variability of this estimate?

112
00:13:05,780 --> 00:13:12,360
So remember, you have this plot. Here's actually the.

113
00:13:14,140 --> 00:13:18,670
The plot of our mini data set where it drops every time there's an observed event.

114
00:13:18,760 --> 00:13:27,820
So there's a drop at two, there's a drop at eight and at the other event times and it says or does this to I believe if you want it to,

115
00:13:28,060 --> 00:13:32,020
it'll have hash marks for the sensor time. So that's what the hash marks are.

116
00:13:32,260 --> 00:13:35,649
This curve goes all the way down to zero, which is a common feature.

117
00:13:35,650 --> 00:13:40,719
The largest event time is an observed death rather than a sensor value.

118
00:13:40,720 --> 00:13:47,830
It'll always drop to zero if the largest event time corresponds to a delta equals one to death or failure.

119
00:13:49,240 --> 00:13:54,460
And so wherever you are in this car, on this curve, this kaplan-meier curve,

120
00:13:54,880 --> 00:13:59,560
there's some estimate of variability that goes along with that height on the curve.

121
00:13:59,560 --> 00:14:07,720
So if we have this estimate for the kaplan-meier survival probability, I mean, that is an estimate and there's some variability around it.

122
00:14:09,040 --> 00:14:17,469
So you could actually think of making little at each point on the curve, little confidence intervals all the way through this curve.

123
00:14:17,470 --> 00:14:21,190
I mean, this is a sequence of estimates, right? It's not just one estimate.

124
00:14:21,190 --> 00:14:24,730
It's a sequence of estimates that's giving us the heights on those curves.

125
00:14:25,720 --> 00:14:29,170
And so the variability changes over time.

126
00:14:29,170 --> 00:14:33,460
It's not the same variability for this height versus the one out here,

127
00:14:34,300 --> 00:14:40,120
mainly because this estimate is based on a lot more people who were still in the data set compared to out here.

128
00:14:40,600 --> 00:14:45,759
So the variability tends to get bigger as you go from having a lot of people still

129
00:14:45,760 --> 00:14:49,360
at risk in your data set to having very few people at risk in your data set.

130
00:14:50,050 --> 00:14:54,790
And the formula that people use for getting that variability is called Greenwood's formula.

131
00:14:55,300 --> 00:15:02,860
And that's where we left off last time, was understanding Greenwood's formula and confidence intervals and that sort of stuff.

132
00:15:04,770 --> 00:15:07,890
So I think we're going we're finally ready to go back to slide 30.

133
00:15:11,140 --> 00:15:21,870
Where we're looking at. The variance of the Kaplan-Meier estimate or so it depends on little time to see how this little t is here.

134
00:15:22,380 --> 00:15:29,930
So depending on where t is on that curve, this formula changes and it's it's being affected in two places.

135
00:15:29,940 --> 00:15:33,300
There's a little T here and there's a little T here.

136
00:15:34,590 --> 00:15:43,740
So for the people who learn well by calculating things by hand, you know, I'm going to have examples of you doing it by hand.

137
00:15:44,220 --> 00:15:51,090
I'm never going to ask you to do this formula by hand. It's only for that maybe third of students who learn well by saying, Oh,

138
00:15:51,090 --> 00:15:56,010
this is what is in the output, and they do it by hand once to sort of see how it works.

139
00:15:57,000 --> 00:16:04,200
But my my main goal is to sort of give you the intuition that this does depend on time and that

140
00:16:04,200 --> 00:16:09,270
as you get bigger and bigger time points where there's fewer people at risk in the data set,

141
00:16:09,660 --> 00:16:20,100
this number will get bigger. And it's it's mainly because you're you are adding more and more of these terms to your variance as you go through.

142
00:16:23,810 --> 00:16:29,930
And so there are several confidence interval approaches and they do involve this Greenwood's formula,

143
00:16:29,930 --> 00:16:34,910
but they don't typically use this this confidence interval style that you've gotten used to.

144
00:16:35,540 --> 00:16:42,440
So if you were just coming straight from your interest start class and you wanted or if any, any class,

145
00:16:44,000 --> 00:16:51,500
you might just think of doing estimate plus or -1.96 times this square root of the variance.

146
00:16:51,710 --> 00:16:59,000
So this is the standard error based on agree to its formula and it's not a bad confidence interval.

147
00:16:59,000 --> 00:17:08,960
Honestly, it has all the properties you want, except that the bounds of the confidence limits don't necessarily stay between zero and one.

148
00:17:09,140 --> 00:17:16,040
So depending on how big the standard error is and how close your kaplan-meier is to zero or one,

149
00:17:16,460 --> 00:17:22,400
this adding and subtracting could pop the confidence limits either really below zero or really above one.

150
00:17:22,730 --> 00:17:28,640
And people just don't like that because survival probabilities have to be between zero and one to make sense.

151
00:17:29,510 --> 00:17:39,200
And for that reason alone and no other, they have this log minus log version.

152
00:17:40,210 --> 00:17:47,050
Of a confidence interval. This is what you'll see in South and I'll show you how to get it in as well.

153
00:17:47,350 --> 00:17:51,129
So when I ask for a confidence interval from, you know,

154
00:17:51,130 --> 00:17:58,750
I'm expecting you to find that the South confidence interval that is based on this or the R confidence interval that's based on this,

155
00:17:59,620 --> 00:18:01,870
although the other one really isn't that bad.

156
00:18:01,870 --> 00:18:09,700
I mean, if you if you know that the survival can't go outside of zero and one and you do that old fashioned confidence limit,

157
00:18:10,180 --> 00:18:14,950
you could actually just you know, if you get a negative lower limit, you could just change it to zero.

158
00:18:14,950 --> 00:18:19,149
And if you get a upper limit greater than one, you could just change it to one.

159
00:18:19,150 --> 00:18:24,550
It wouldn't be a bad confidence interval, but people just kind of come to expect this version.

160
00:18:26,430 --> 00:18:28,270
So why do they use this function?

161
00:18:28,390 --> 00:18:38,430
They they use this log minus log of survival to base their confidence intervals on because it can be anywhere from minus infinity to infinity.

162
00:18:38,760 --> 00:18:47,190
And then when you try to do the algebra to get it back to the confidence limit for the survival function, it will be between zero and one.

163
00:18:47,520 --> 00:18:56,670
So it's a weird function, but it has this property of once you get the limits for this guy and do the algebra to get back to the limits for survival,

164
00:18:56,970 --> 00:19:00,810
that confidence limit won't go outside of the range from 0 to 1.

165
00:19:02,710 --> 00:19:06,910
So that's the strategy. Build the confidence interval on this scale,

166
00:19:06,910 --> 00:19:13,360
then transform back to the scale of the survival function which will be naturally constrained to be between zero and one.

167
00:19:15,200 --> 00:19:18,950
And again, I don't expect you to own this algebra.

168
00:19:19,340 --> 00:19:23,270
This is for the third of people who like seeing where this stuff comes from by hand.

169
00:19:23,870 --> 00:19:31,669
And so if you were to, you know, do what South does or are can do behind the scenes,

170
00:19:31,670 --> 00:19:36,950
and you found the confidence interval for log minus log survival and got to numbers L

171
00:19:36,950 --> 00:19:41,719
for the lower limit and you for the upper limit then the confidence interval on the

172
00:19:41,720 --> 00:19:48,620
survival scale you would take that you number from this top limit and go e to the minus

173
00:19:48,620 --> 00:19:53,900
e times that you and that will actually end up being the lower of the two numbers,

174
00:19:54,080 --> 00:19:55,760
sort of by the strange algebra.

175
00:19:56,030 --> 00:20:02,120
And you take the lower limit from up here and you go E to the minus e lower limit and that'll be the upper limit of your

176
00:20:02,120 --> 00:20:11,020
confidence interval for T So it's actually not a typo that I've switched the order from low upper to upper lower in the algebra.

177
00:20:11,030 --> 00:20:12,230
That's just the way it works.

178
00:20:12,500 --> 00:20:18,170
If you try it out, you'll see, you know, that this will be the lower limit and this will be a higher number for the upper limit.

179
00:20:20,950 --> 00:20:27,069
And the 95% confidence interval kind of for this log man is like survival.

180
00:20:27,070 --> 00:20:31,270
If you ever want to do this by hand in the privacy of your own room, you can.

181
00:20:31,270 --> 00:20:36,040
It's just the log minus log for whatever that kaplan-meier estimate is that little time

182
00:20:36,040 --> 00:20:42,670
t plus or -1.96 times the standard error of that log minus log kaplan-meier estimate.

183
00:20:43,150 --> 00:20:54,070
And that standard error piece looks like this. It looks like the standard error of the kaplan-meier at time t based on Greenwood's formula.

184
00:20:54,310 --> 00:20:59,980
But then you divide it by the kaplan-meier estimate at little time t times the natural log of the

185
00:20:59,980 --> 00:21:05,440
kaplan-meier estimate a little time t And so you could actually do this by hand if you wanted to.

186
00:21:06,990 --> 00:21:10,860
Those would be all the pieces for getting the confidence limits for this guy.

187
00:21:11,190 --> 00:21:16,300
And then, of course, you'd have to, you know, do this algebra to get it to be the confidence limits veracity.

188
00:21:17,090 --> 00:21:24,250
So. Just copying from the past page if you want the standard error.

189
00:21:25,350 --> 00:21:34,049
For this log minus log thing. Remember, there's this little detail here that I kind of leaped right past.

190
00:21:34,050 --> 00:21:41,250
But if you're doing the standard error rather than the variance, you need to take the absolute value here so that you get a positive standard error.

191
00:21:43,250 --> 00:21:52,160
And here's the confidence interval on the the log minus log scale, just copied from the last slide as well.

192
00:21:53,090 --> 00:21:56,390
And here's the confidence limits for the survival.

193
00:21:58,860 --> 00:22:04,020
Or you could use your shortcut and just do the similar algebra.

194
00:22:04,950 --> 00:22:13,440
I won't show you all the steps, but the similar algebra would be the kaplan-meier estimate or raised to this formula for safety.

195
00:22:15,900 --> 00:22:22,850
So that's all in the power term and. So I'm going to show you how this works by hand.

196
00:22:23,660 --> 00:22:30,880
If it gives you good intuition, good. I'm not going to be asking you to do these by hand on your homework or anything else.

197
00:22:30,890 --> 00:22:40,670
We will always rely on software. But it may very well help you kind of process all of what's going on in the software to see this.

198
00:22:40,700 --> 00:22:43,220
So here we have our mini data set again.

199
00:22:44,150 --> 00:22:52,070
And if we wanted to find a 95% confidence interval for the survival function, the survival probability at ten, little ten.

200
00:22:53,590 --> 00:23:02,360
So tiny little tennis somewhere between here between the observed value at the event ID at eight.

201
00:23:02,410 --> 00:23:05,790
You know, there was a bend time in eight and then there was a censored value it at 12.

202
00:23:05,790 --> 00:23:07,510
So sometime between there.

203
00:23:08,020 --> 00:23:14,020
So we already figured out the kaplan-meier estimate at time ten was point seven, eight, eight, and that is in Greenwood's formula.

204
00:23:14,020 --> 00:23:22,629
It happens right here. If I want the variance of the kaplan-meier at time ten the kaplan-meier self at time ten is there and squared.

205
00:23:22,630 --> 00:23:33,010
So that's one piece we've already got done. So we just had to figure out this second piece and we already have YJ and J from earlier in the handout.

206
00:23:33,010 --> 00:23:38,230
So this is like the number at risk at t j so at two.

207
00:23:40,720 --> 00:23:45,540
Nobody's had the event yet, so all ten people are at risk and one of them had the event.

208
00:23:45,550 --> 00:23:48,940
So deejays one and at time eight,

209
00:23:50,140 --> 00:23:57,520
these two guys are no longer at risk at time eight because they were removed from the data set either by having an event or being censored.

210
00:23:57,520 --> 00:24:00,999
So there's eight people at risk for having the event at time.

211
00:24:01,000 --> 00:24:06,460
Eight, but none of them. Oh yeah. And we did have a death at times the deejays one.

212
00:24:06,880 --> 00:24:17,470
And so this part of Greenwood's formula over here is saying some over all of the event times that were less than or equal to ten.

213
00:24:18,510 --> 00:24:22,140
At this term. And so we're basically just summing up these two numbers.

214
00:24:23,500 --> 00:24:30,820
So this is deja vu. YJ Times YJ minus t j for the t j equals two and t j was eight.

215
00:24:31,810 --> 00:24:40,960
And so the finally, the Greenwood's formula at time ten is taking the kaplan-meier squaring it, and then summing up these two numbers.

216
00:24:43,140 --> 00:24:48,490
And so, you know, you can sort of say this is just for that one number at time ten.

217
00:24:48,510 --> 00:24:56,610
So if you were looking at the variability for a kaplan-meier curve much further out, you would have a lot more of these terms to some.

218
00:24:57,670 --> 00:25:02,290
And you would have to switch whatever your kaplan-meier estimate at that time was as well.

219
00:25:07,940 --> 00:25:12,260
So just kind of this is going to be the number for the standard error.

220
00:25:12,290 --> 00:25:18,170
You take the square root. This is the standard error. Greenwood's formula that shows up in some of the formulas as well.

221
00:25:20,080 --> 00:25:28,629
And SAS and ah well I'm going to show you how to get it from ah south will automatically give you these green with standard errors at each time,

222
00:25:28,630 --> 00:25:32,590
point t as part of its points the output. So now you'll know where that comes from.

223
00:25:35,480 --> 00:25:39,860
And so if you want to do this by hand, you can. This is how it would go.

224
00:25:39,980 --> 00:25:46,190
The standard error for the log minus is log kaplan-meier estimate at time ten.

225
00:25:46,520 --> 00:25:49,890
This is the formula just putting in the little ts equals ten.

226
00:25:50,150 --> 00:25:53,690
And we've gotten all of these values now.

227
00:25:53,700 --> 00:25:56,880
So from the last page, we got Greenwood's formula at time ten.

228
00:25:56,900 --> 00:26:05,600
This was the Kaplan-Meier estimate, time ten. And so we need to do the natural log n at time of the time ten as well.

229
00:26:05,600 --> 00:26:10,370
And so you would get this number for the standard error of log minus like kaplan-meier.

230
00:26:10,940 --> 00:26:17,300
And so if you were to do the 95% confidence interval by hand for long minus log survival.

231
00:26:18,240 --> 00:26:24,030
This is now what you would do. So this isn't hard algebra once you've got the ingredients in there.

232
00:26:24,510 --> 00:26:29,820
And then if you wanted to get the 95% confidence interval for the survival curve,

233
00:26:30,840 --> 00:26:37,169
taking the the bigger number and doing it minus E to that number here, taking the smaller number.

234
00:26:37,170 --> 00:26:40,560
And then during each the minus E there, you get your confidence limits.

235
00:26:41,890 --> 00:26:49,830
For survival. And or using my little shortcut where I have the Kaplan-Meier estimate here and then all this stuff in here,

236
00:26:49,980 --> 00:26:54,390
I think this is a little bit shorter, but you're going to get the same number either way.

237
00:26:56,320 --> 00:27:01,630
So I did do that really fast for that third that wants to check and do these things by hand.

238
00:27:03,340 --> 00:27:06,430
Again, you will get this from software. All right.

239
00:27:06,430 --> 00:27:12,790
So now is the perk up moment, because this this next slide talking about the hazard function is something that's going

240
00:27:12,790 --> 00:27:19,560
to stay with us for a couple more handouts as a as the big main idea that we use a lot.

241
00:27:19,570 --> 00:27:27,370
So and I mentioned this a little bit in the last time and so the hazard of an event at a little time t like ten.

242
00:27:28,890 --> 00:27:32,040
We're going to call Lamda a little tea.

243
00:27:32,610 --> 00:27:39,030
And they're just unpacking this because there's a lot of stuff here on the right hand side.

244
00:27:39,570 --> 00:27:42,660
This probability part is given.

245
00:27:42,660 --> 00:27:53,190
You were at risk for the event at time. T You know, so we've sort of been thinking the number of people who are at risk is like a y j kind of term.

246
00:27:53,460 --> 00:28:00,510
So given you were at risk for the event at time, t hadn't been censored, hadn't failed quite yet before.

247
00:28:00,510 --> 00:28:05,820
T The probability that you have the event in that next tiny little window of time.

248
00:28:06,540 --> 00:28:13,970
T plus delta t is this first term and then they have some weird stuff over here.

249
00:28:13,980 --> 00:28:17,430
So they this part, this probability part is pretty.

250
00:28:18,450 --> 00:28:22,559
It's not great, but it's pretty easy to interpret because we know about probabilities.

251
00:28:22,560 --> 00:28:31,260
And I think I used this example from Princess Bride last time to tell you, I always think if this have the guy of a senior who, you know,

252
00:28:31,260 --> 00:28:39,630
thinks he's won this poison contest and, you know, he's laughing his head off and then some like so he was at risk right up into the time.

253
00:28:39,720 --> 00:28:45,720
The whole time he was laughing, he was still at risk and then had his event like in this little tiny snap window.

254
00:28:45,840 --> 00:28:50,219
So that's how small these windows are supposed to be thought of.

255
00:28:50,220 --> 00:28:57,120
Really, really tiny. Because we've got this limit here in the front where that tiny window width is going to zero.

256
00:28:59,840 --> 00:29:00,230
All right.

257
00:29:00,650 --> 00:29:14,870
So this is a bit of a theoretical concept, but I you know, there are some situations where it's easier to interpret than other, for example, if that.

258
00:29:16,100 --> 00:29:21,210
That this number here is pretty constant all the way through time.

259
00:29:21,950 --> 00:29:27,259
You know, then you're trying to estimate the same number all the way through time.

260
00:29:27,260 --> 00:29:34,819
That is the same probability of having something bad happen in the next little snap divided by some unit time.

261
00:29:34,820 --> 00:29:36,440
That's very, very tiny.

262
00:29:36,710 --> 00:29:45,680
You're kind of trying to get at the same thing all the way through time, even as your windows moving around from T to T plus Delta, you know.

263
00:29:45,680 --> 00:29:54,870
And in that case, you can sort of think of it as, you know, the rate of events per unit time that it's trying to estimate.

264
00:29:55,640 --> 00:30:01,100
But there's no restriction on that being a constant over time.

265
00:30:01,400 --> 00:30:05,389
And as you know from your epidemiology courses, you know,

266
00:30:05,390 --> 00:30:12,620
you often have these situations where they just think about mortality over a person's lifetime.

267
00:30:13,190 --> 00:30:19,760
You know, there's often higher infant mortality. So the hazard would be up high in those periods.

268
00:30:20,060 --> 00:30:23,180
And then it kind of stabilizes as you kind of get to a certain age.

269
00:30:23,180 --> 00:30:26,479
And then as you get older, mortality goes up again, right?

270
00:30:26,480 --> 00:30:29,930
So there's not a constant hazard or even over your own lifetime.

271
00:30:30,980 --> 00:30:32,959
And even during a course of a clinical trial,

272
00:30:32,960 --> 00:30:39,860
it may not be there might be some initial risk to surgery if you're watching people who've had surgery and then,

273
00:30:39,950 --> 00:30:47,230
you know, as the it during post-op things kind of stabilize, you know, and maybe there's a recurrence of the disease that happens later.

274
00:30:47,240 --> 00:30:50,960
So you don't necessarily have constant hazards over time.

275
00:30:52,650 --> 00:31:05,850
And because we have this division here, this is not going to be a value that's between zero and one has its can get super super large and so.

276
00:31:07,760 --> 00:31:11,990
The really the only restraint we have is that the hazard is positive.

277
00:31:13,300 --> 00:31:18,700
You know, it's we're greater than greater than or equal to zero. So.

278
00:31:20,700 --> 00:31:23,850
So people sometimes struggle with the interpretation of the hazard.

279
00:31:24,300 --> 00:31:27,780
But the general idea of if it's high, it's bad.

280
00:31:28,140 --> 00:31:33,870
That's going to be good intuition. And if it's close to zero, then not much is going on.

281
00:31:34,560 --> 00:31:38,430
But how high is is high can be a struggle.

282
00:31:39,350 --> 00:31:40,520
To understand,

283
00:31:40,520 --> 00:31:50,270
unless you're in this situation where it's a constant hazard over time and in that one special case where it's a constant hazard over time,

284
00:31:50,270 --> 00:31:58,010
it really is trying to estimate kind of like that, you know, this this probability per unit time,

285
00:31:58,520 --> 00:32:02,630
this probability of having the event in a small window per unit time.

286
00:32:03,970 --> 00:32:10,510
Per that same unit time. And when we estimate the from the data, it does kind of look like that.

287
00:32:10,520 --> 00:32:18,729
So the way the hazard is estimated from the data is we look at values of T between event times.

288
00:32:18,730 --> 00:32:27,790
So if we're looking at the hazard at time T and that time T is somewhere in this range from t j to t j plus one,

289
00:32:28,480 --> 00:32:31,630
we estimate that hazard at little time t.

290
00:32:33,260 --> 00:32:41,480
As the this is like DJ over YJ so this is the number of events at t j divided by the number at risk.

291
00:32:42,700 --> 00:32:46,960
At that t.j. And then we divide by this unit time.

292
00:32:48,350 --> 00:32:55,729
Between, you know, t j plus one minus t j. So this hazard is some kind of probability.

293
00:32:55,730 --> 00:33:01,590
This is like the probability the event is happening at TJ given they were at risk at t.j.

294
00:33:01,610 --> 00:33:05,150
That's kind of like this probability that's being estimated here.

295
00:33:06,690 --> 00:33:12,679
And this is like the the unit time. That we wish was smaller.

296
00:33:12,680 --> 00:33:19,490
But, you know, this is this was the unit time we were able to serve in the data set between events.

297
00:33:21,910 --> 00:33:27,490
All right. So that takes the probability of it in the whole interval between t j t t j j plus one.

298
00:33:27,880 --> 00:33:34,600
And it derives that probability of an event per unit time. And there is actually a convention.

299
00:33:34,930 --> 00:33:38,710
Uh, you know that if there's no.

300
00:33:40,360 --> 00:33:45,489
So I sort of write this hazard as if it's constant over that whole interval.

301
00:33:45,490 --> 00:33:52,450
But there's your you'll sometimes see that if there's no event at all at a certain time, they'll just sort of say, this is zero.

302
00:33:53,500 --> 00:34:01,030
So at times ten itself, if there is no event right at time ten, there's no deaths at time ten.

303
00:34:01,040 --> 00:34:04,200
So this would be like a zero and the whole hazard would be zero.

304
00:34:04,210 --> 00:34:08,920
So the hazard actually, you know, you're.

305
00:34:09,900 --> 00:34:13,620
This deejay is only one at the event times themselves.

306
00:34:20,400 --> 00:34:25,799
So it's more common for people to look at the kindling cumulative hazard.

307
00:34:25,800 --> 00:34:29,070
And so that's something that shows up in the output.

308
00:34:29,490 --> 00:34:34,470
Those hazards accumulate over time and that kind of looks like this.

309
00:34:34,620 --> 00:34:40,410
So the cumulative hazard is summing up for all the event times.

310
00:34:41,480 --> 00:34:44,809
In the data set that are less than tiny little t.

311
00:34:44,810 --> 00:34:52,580
So think if little t is ten again you take the hazard and then times that where that's kind of like the cumulative hazard.

312
00:34:57,280 --> 00:35:01,900
And there is a little you know, if you actually write out what these formulas are.

313
00:35:03,170 --> 00:35:09,140
Then it'll end up looking like the sum of the hazards over time.

314
00:35:12,420 --> 00:35:20,100
There's some cancelation that happens when you write out the hazard function and that's known as the Nelson Allen cumulative hazard estimate for.

315
00:35:23,330 --> 00:35:30,290
All right. So just looking at the hazard and the cumulative hazard at time ten.

316
00:35:33,140 --> 00:35:36,950
Here's our little mini data set, here's our order times.

317
00:35:36,950 --> 00:35:40,700
And I've added just a row for ten so we can see it.

318
00:35:41,450 --> 00:35:53,179
And at the y j the number at risk at each of these times, the DGA, the number of events at these times, the j over y j and the hazard.

319
00:35:53,180 --> 00:35:56,210
So the hazard is, did you ever y j over that width.

320
00:35:57,140 --> 00:36:05,030
And so at time t j equals to the dga of or y j is like one over ten.

321
00:36:05,030 --> 00:36:12,140
And then we are looking at t j plus one minus t j.

322
00:36:12,170 --> 00:36:15,470
Did I write out these steps here? No.

323
00:36:15,970 --> 00:36:19,210
And so T.J., plus one minus T.J. is like this.

324
00:36:21,790 --> 00:36:27,250
Eight minus two. So that should give us our .0167.

325
00:36:30,400 --> 00:36:37,910
And. Here at time eight, the DJ average is going to be one over eight.

326
00:36:38,270 --> 00:36:40,010
So that's this part over here.

327
00:36:40,910 --> 00:36:51,410
And then the hazard is going to be this thing divided by t j plus one minus t j and so t j plus one is actually time 15.

328
00:36:51,920 --> 00:36:55,910
So that whole thing would be divided by seven. And this is where you get this number.

329
00:36:59,360 --> 00:37:07,040
And since ten is between those two and it's not at event time, if you were to actually ask what the hazard was exactly at time,

330
00:37:07,040 --> 00:37:14,930
ten people would expect you to say that the hazard at that exact time is is zero.

331
00:37:18,440 --> 00:37:26,629
The cumulative hazard, however, would add up all of the hazards all the way up to the time you're interested in.

332
00:37:26,630 --> 00:37:39,300
And you would add these two numbers. Why am I not adding up these two numbers?

333
00:37:41,130 --> 00:37:45,060
Oh, it was because it. Because. Remember how we canceled?

334
00:37:46,140 --> 00:37:52,500
I just told you a minute ago. When you add up the hazards times the delta t this part cancel.

335
00:37:52,510 --> 00:37:56,970
So it actually is the sum of these two guys 4.225.

336
00:38:01,340 --> 00:38:07,810
Okay. So again, software will provide these things for you.

337
00:38:07,840 --> 00:38:12,760
So hazards are going to right now, I don't expect you to be jumping up and down about how interesting hazards are,

338
00:38:12,760 --> 00:38:16,720
but they are going to become more interesting as you move into regression modeling,

339
00:38:17,380 --> 00:38:25,840
where you look at hazard ratios over time, just like you looked at odds ratios when you were looking at logistic regression,

340
00:38:26,410 --> 00:38:32,050
you're going to be looking at hazard ratios when you're looking at sensor survival data.

341
00:38:32,740 --> 00:38:37,000
And so you're going to be looking at hazard ratios for different combinations of covariates

342
00:38:37,000 --> 00:38:41,560
to understand what's going on in your in your search with your survival data set.

343
00:38:42,580 --> 00:38:49,899
Later on, and there's a special case that comes up over and over again.

344
00:38:49,900 --> 00:38:57,860
And that is. Well, actually, I was thinking about going right into proportional hazards, but I think I need to step first.

345
00:38:57,860 --> 00:39:02,719
The hazards in survival are algebraically linked 100% of the time.

346
00:39:02,720 --> 00:39:07,330
So there's some algebra. That is the survival function.

347
00:39:07,370 --> 00:39:13,639
The true survival is always no matter what. E to the minus cumulative hazard at that same point in time.

348
00:39:13,640 --> 00:39:18,760
T So this is like a fact, a theorem that is always true.

349
00:39:20,520 --> 00:39:24,839
We're asked to choose if I will probably at time t and Lamberti is the true

350
00:39:24,840 --> 00:39:30,840
cumulative hazard function at time t and that algebra is not being derived for you.

351
00:39:31,170 --> 00:39:38,489
But it is like a theorem. This is always true. So when we model hazards later,

352
00:39:38,490 --> 00:39:42,899
it's also going to help us understand survival because the relationships between the

353
00:39:42,900 --> 00:39:47,610
hazards are going to influence how we understand relationships between survival.

354
00:39:50,720 --> 00:39:59,060
And so I'm going to take you through some of the software now to see some of these terms and find them in your output and use them in a practical way.

355
00:39:59,090 --> 00:40:06,680
So this is my little mini data set again, and I'm just inputting the data into South Printing.

356
00:40:08,230 --> 00:40:15,430
And it's it's just a toy data set with just the deltas and the excise that we've been using all along.

357
00:40:17,250 --> 00:40:23,970
And in fact, the main product that you're using for both this hand out in the next cannabis product life test.

358
00:40:25,140 --> 00:40:27,300
So we're not going to do the test in this handout.

359
00:40:27,360 --> 00:40:35,580
We're mainly going to do Kaplan-Meier estimation, but eventually it'll be used for two sample tests for survival data as well.

360
00:40:38,350 --> 00:40:45,070
And so, you know, this is kind of the way the syntax goes proc life tests your set.

361
00:40:45,100 --> 00:40:52,900
We call that example and if you want plots like the kaplan-meier, you kind of use this.

362
00:40:53,060 --> 00:40:57,670
Plots equals survival. And the S.l stands for confidence limits.

363
00:40:58,360 --> 00:41:04,210
So that will request the Kaplan-Meier survival plot with 95% point and with confidence intervals.

364
00:41:04,570 --> 00:41:12,880
So behind the scenes, all of the agreements formula and the log minus log survival, weird confidence limits that I showed you.

365
00:41:13,150 --> 00:41:16,990
You know, they're they're basing all their limits on behind the scenes.

366
00:41:17,470 --> 00:41:24,040
It's all happening for you without your active participation, and it will produce those point wise confidence limits.

367
00:41:25,280 --> 00:41:28,389
Uh, here is just asking for the green with standard error,

368
00:41:28,390 --> 00:41:38,050
the cumulative hazard with this Nelson command and also saving a data set into something called camped out.

369
00:41:39,540 --> 00:41:40,890
So those are just all the options.

370
00:41:41,250 --> 00:41:50,640
The main point of code here is this time statement and this is where you're putting in your random variable for EXI,

371
00:41:51,360 --> 00:41:55,140
which actually called EXI, and then this Asterix.

372
00:41:56,140 --> 00:42:03,070
Is always there and then you're variable for the censoring indicator, which I actually called Delta Eye here.

373
00:42:03,520 --> 00:42:09,010
And then there's always a parentheses with the number and the number is supposed to be what it means,

374
00:42:09,100 --> 00:42:12,100
you know, which number is being used for a censored value.

375
00:42:12,520 --> 00:42:15,940
So almost always there should be a zero here.

376
00:42:15,940 --> 00:42:19,389
Almost everybody uses zero to indicate it censored value.

377
00:42:19,390 --> 00:42:27,850
But this is sort of a carryover and there actually are a few times when switching this value to something else can be helpful,

378
00:42:28,660 --> 00:42:31,390
but I'm not talking really about that today.

379
00:42:31,450 --> 00:42:40,299
So almost always for you, this is going to be a zero unless you have somebody creating your dataset that doesn't that for whatever reason,

380
00:42:40,300 --> 00:42:49,570
use a different number for censoring. All right. And so here and I'm going to print the camped out data so you can see what that looks like as well.

381
00:42:51,410 --> 00:42:56,100
Oh, little kind of. Animations here,

382
00:42:56,110 --> 00:43:02,919
just kind of labeling out what all this stuff is that I said earlier and then save data with the confidence limits into

383
00:43:02,920 --> 00:43:10,930
camped out so saving data into the camped out can be handy because if you need to actually report a confidence interval,

384
00:43:11,290 --> 00:43:18,460
it's hard to get that just from the plot. So you want to have something printed out where you can find the confidence limits you want.

385
00:43:20,730 --> 00:43:25,500
All right, so keyword time followed by the time variable by the event indicator.

386
00:43:26,010 --> 00:43:29,130
This is all stuff that I just sort of said earlier. All right.

387
00:43:29,610 --> 00:43:33,460
So here is the output from SAS.

388
00:43:33,780 --> 00:43:41,620
Our output. I've kind of programed it to give you something similar, but there's a little there's a few quirks here that I just want to show you.

389
00:43:41,640 --> 00:43:47,520
So you know how to interpret this stuff. So first off, this is this column is the Kaplan-Meier survival estimate.

390
00:43:48,510 --> 00:43:57,390
And this number that we found by hand for the Kaplan-Meier estimate at time ten is right here.

391
00:43:57,840 --> 00:44:01,620
That's the number that we've been using the whole time.

392
00:44:02,100 --> 00:44:07,340
And the time ten is is not shown in the output here.

393
00:44:07,350 --> 00:44:12,300
Really, it's only showing you the values that all the X values in your data set.

394
00:44:12,330 --> 00:44:16,319
So if you are trying to find out what the survival estimate is at time,

395
00:44:16,320 --> 00:44:25,230
ten SAS is expecting you to know you're supposed to look at the row that that would be just above time ten left, you know.

396
00:44:25,560 --> 00:44:31,080
So whatever the next lower value is from ten, it expects you to know to look for that row.

397
00:44:32,760 --> 00:44:39,860
All right. And so I it if you want to,

398
00:44:40,520 --> 00:44:45,290
instead of looking at a couple more survival estimates or if you want to look at the probability of having

399
00:44:45,290 --> 00:44:50,870
an event before time ten like one minus the survival estimate how some people like to plot that instead.

400
00:44:51,260 --> 00:44:57,149
That's here in this column. And here is this column.

401
00:44:57,150 --> 00:45:07,230
Here is the Greenwood Standard Error. So those we looked at that earlier in the handout we actually solved for this one, this .1340.

402
00:45:08,700 --> 00:45:17,130
And this here is the Nelson Alan Kimmel hazard we actually solve for this 1.22504 times ten again.

403
00:45:19,130 --> 00:45:24,650
Uh. Oops. And over here, this is just this number left.

404
00:45:24,650 --> 00:45:31,750
That's like the Y. AJ Except we only really paid attention to why J at the event times themselves.

405
00:45:32,780 --> 00:45:36,050
And this is like a cumulative number failed.

406
00:45:36,080 --> 00:45:40,610
So it's not quite it's not quite, T.J. It's a cumulative number that have failed.

407
00:45:41,510 --> 00:45:48,070
So there's something else here to notice. So it says note the marked survival times or censored observations.

408
00:45:48,080 --> 00:45:51,690
So it means these little asterisks.

409
00:45:51,710 --> 00:45:58,330
So if there's it doesn't have a column title here, but at time five, it was a censored value.

410
00:45:58,340 --> 00:46:03,820
So it'll just have an asterisk here. And it just puts dot, dot, dot.

411
00:46:04,000 --> 00:46:08,680
You know, for all these estimates so that this is a quirky thing about sales,

412
00:46:08,680 --> 00:46:13,170
it expects you to know that the KAPLAN-MEIER estimates flatten until the next event.

413
00:46:13,180 --> 00:46:15,610
It's depending on you to know that.

414
00:46:16,120 --> 00:46:23,620
And so think of these dots as little kind of like what do you call them, little quarry, you know, marks for repeat.

415
00:46:25,060 --> 00:46:33,190
So really, if you wanted to know the numbers that apply at time five, you're supposed to know to look at to the row right above it.

416
00:46:33,790 --> 00:46:42,009
So at time five, the survival estimate is 0.9 and the failure, the probability of failing before time t is 0.1.

417
00:46:42,010 --> 00:46:44,830
You're supposed to know to just use all the numbers from the row above.

418
00:46:45,340 --> 00:46:54,170
So the survival standard error time five is is this guy, you know, the cumulative hazard at time five is this point one and so on.

419
00:46:54,190 --> 00:46:58,360
The only thing that they're updating for you is the number of at risk.

420
00:46:58,960 --> 00:47:01,600
That's the only bookkeeping that they bother updating for you.

421
00:47:01,930 --> 00:47:10,810
They are expecting you to know as a as an educated survival analysis person that at time five, you're to look to the row above.

422
00:47:12,260 --> 00:47:18,470
And so that's true for all the censored values. So there's a censored value at 12 and they don't give you any numbers here.

423
00:47:20,270 --> 00:47:27,889
They expect you to know, to look at at time eight to know that at time 12, this is still point seven, eight, seven,

424
00:47:27,890 --> 00:47:34,070
five for your survival probability that it hasn't dropped yet because you know, it only drops at the next event time at 15.

425
00:47:37,110 --> 00:47:42,689
So it's a little quirky and I'm still not sure why they would do that, except that they're like,

426
00:47:42,690 --> 00:47:46,620
it's a test or something because it would be very easy to just write those numbers again.

427
00:47:46,620 --> 00:47:51,630
And actually it does do that. It'll it won't change. It'll just write the numbers for you.

428
00:47:52,080 --> 00:47:56,100
It doesn't assume that it doesn't have that little. I don't know.

429
00:47:56,100 --> 00:48:00,360
It's a little mean write to not just write the numbers out. It costs them nothing.

430
00:48:00,360 --> 00:48:04,530
And so they they just they want you to think harder or something like that.

431
00:48:04,530 --> 00:48:06,090
I don't know. I don't know what's going on there.

432
00:48:09,200 --> 00:48:18,130
And this is the output that we asked for with this serve with that OutServe thing that we did over here.

433
00:48:18,380 --> 00:48:23,170
These are camped out. That we the out servicos came out.

434
00:48:23,170 --> 00:48:31,120
So this is the camped out data and what it looks like. And so it's giving us the survival probabilities.

435
00:48:32,240 --> 00:48:37,630
The Greenwood Standard airs, but it's also during those lower and upper confidence limits.

436
00:48:38,170 --> 00:48:41,040
Usually, and this is based on log minus log survivals.

437
00:48:41,040 --> 00:48:48,880
So we actually I kind of in the handout on had shown these to you by hand but this is where you would get them from the output.

438
00:48:49,650 --> 00:48:57,610
All right. And so here is the same little kaplan-meier estimate.

439
00:48:58,660 --> 00:49:02,860
From Seth. And the point was confidence intervals are kind of shaded.

440
00:49:02,860 --> 00:49:16,210
And so it it really is hard to see this, but they're a little bit narrower at the beginning and they get fatter and they it's really hard to tell.

441
00:49:19,330 --> 00:49:27,570
If this is a little narrow, I wants to say that this is a little narrower again, but it definitely is getting bigger from this first confidence limit.

442
00:49:27,580 --> 00:49:29,770
There's some something changing over time.

443
00:49:30,820 --> 00:49:35,680
And I'm not sure if this is an artifact or my I really is saying that that got narrower at the very last moment.

444
00:49:38,300 --> 00:49:44,150
All right. So, ah, so for ah, we're using the survival package for all this stuff.

445
00:49:44,150 --> 00:49:52,100
Here's reading in the data with a nice little jazz table here and giving the elements of the table names.

446
00:49:53,060 --> 00:50:00,500
And then for the Kaplan-Meier and the Greenwood Variance kind of stuff, we're using formula as again,

447
00:50:00,800 --> 00:50:05,360
the formula for survival data is a little bit weird because we've got two outcomes.

448
00:50:05,360 --> 00:50:09,500
So this is what a formula looks like when you've got survival data.

449
00:50:09,920 --> 00:50:18,080
This part is always there capital s, u, r, v, lowercase, u, rb that's always there with a parentheses and an end parentheses.

450
00:50:18,410 --> 00:50:23,090
And you're putting in your X-Y variable whatever your follow up time variable is,

451
00:50:23,630 --> 00:50:30,530
and the indicator variable for whether the end of follow up was due to a death or just they were still alive.

452
00:50:31,280 --> 00:50:36,710
So these are the variable names that change data, set the data, set this path always there.

453
00:50:37,040 --> 00:50:47,060
And then this little tilde is like an equals again. And if you're just doing kaplan-meier, it's like you just have a one here.

454
00:50:48,080 --> 00:50:53,719
As we get more sophisticated with regression, you might have covariates over here on this side,

455
00:50:53,720 --> 00:51:02,240
but for just doing the kaplan-meier estimate, the formula will have a tilde one here and the function that we're using is serve fit.

456
00:51:02,870 --> 00:51:07,700
I'm going to save everything into the served at a one data set but the function is serve fit.

457
00:51:08,030 --> 00:51:17,389
I'm putting my formula in here from above, telling it the data set saying I want a kaplan-meier estimate for the survival with Greenwoods

458
00:51:17,390 --> 00:51:22,290
error and the confidence type is the log minus log that will match the SAS output.

459
00:51:22,310 --> 00:51:29,000
There are other options, but for this course to sort of stick to the, the, the SAS SAS is favorite.

460
00:51:30,860 --> 00:51:36,820
And then this is code that I just put together to give you output that looks a little bit more like SAS.

461
00:51:37,370 --> 00:51:43,639
So I'm taking elements from that that were saved in a survey data one and sort of printing them out.

462
00:51:43,640 --> 00:51:47,450
So I want a time variable. I want this kaplan-meier survival curve.

463
00:51:47,960 --> 00:51:52,500
I want Greenwood standard error. I want Nelson Allen's cumulative hazard.

464
00:51:52,520 --> 00:52:00,680
I want the standard error of the cumulative hazard and the the number of events and the number of at risk just like South had.

465
00:52:01,040 --> 00:52:09,890
And I'm actually also rounding it. So only four significant digits will be used because otherwise this stuff doesn't fit on on a slide easily.

466
00:52:13,660 --> 00:52:19,030
And then for the other table with confidence limits, I'm rounding it again to four significant digits.

467
00:52:19,450 --> 00:52:27,460
But I went the kaplan-meier estimate, uh, that sorry, the time the kaplan-meier estimate and the lower in the upper confidence limits.

468
00:52:29,330 --> 00:52:32,450
So here is the first table with the cap.

469
00:52:32,570 --> 00:52:36,070
All of this stuff is the same except that it's censored values.

470
00:52:36,070 --> 00:52:44,510
So first off, it doesn't tell you that time five is censored, but it does at least write out the values from the row above for you.

471
00:52:44,510 --> 00:52:47,720
So you don't have to know that that's what you're supposed to do.

472
00:52:48,350 --> 00:52:50,950
You still have to do it four times that don't appear in the data set.

473
00:52:50,990 --> 00:52:57,110
So four times ten you still have to know to look to the row above it to get your values.

474
00:52:57,110 --> 00:53:03,860
Four time little t equals ten. But for censored values, it just tells you, you know, it just puts in the numbers for you.

475
00:53:03,860 --> 00:53:09,920
Yeah, they didn't change if you don't see this blank line with dots that you're supposed to fill in yourself.

476
00:53:11,630 --> 00:53:16,710
All the other numbers are the same as you saw on Saturday, except for that little quirky feature.

477
00:53:16,730 --> 00:53:26,070
The dots is not in our output. And then here is the lower in the upper confidence point was limits.

478
00:53:26,820 --> 00:53:33,030
These showed up as little shaded values in the plot that South gave you so you actually could subtract.

479
00:53:33,040 --> 00:53:36,510
So you don't have to depend on eyeballs to know what those widths worth.

480
00:53:36,930 --> 00:53:49,810
If you're curious about that, for whatever reason. Now R is really great for plots and I actually prefer vastly just ask because one.

481
00:53:49,910 --> 00:53:56,890
Well because I know or better. Honestly, if you're really great at programing stats, you'll know how to make your plots really fancy.

482
00:53:57,700 --> 00:54:01,060
I grew up doing plots and ah, so that's just a bias of mine.

483
00:54:01,540 --> 00:54:06,290
So you put in your survey data into the plot.

484
00:54:06,580 --> 00:54:15,400
I'm saying I do want confidence limits. So t for true I'm putting in labels for the horizontal axis, the vertical axis.

485
00:54:16,540 --> 00:54:26,079
I specifically say where you know what the limits I want to be for my horizontal axis between zero and 30 for this legend locator.

486
00:54:26,080 --> 00:54:34,240
One thing used to be a lot more useful than it was today, and I think it depends on your version of R if this works well or not.

487
00:54:34,250 --> 00:54:39,660
But before class I was playing with it on my laptop and it was kind of a disaster.

488
00:54:39,670 --> 00:54:47,200
It used to be that it would wait for you to click somewhere on the plot and you could choose where to put what where your legend was.

489
00:54:47,590 --> 00:54:51,159
But it was when I clicked. You would puts it somewhere wacky.

490
00:54:51,160 --> 00:54:56,200
So. If you have a good version of our play with this,

491
00:54:56,200 --> 00:55:01,629
see if it works really well for you because it can be handy just to click where you want the legend on your plot to be.

492
00:55:01,630 --> 00:55:10,720
If it works. If it doesn't work, it's going to be much better to use specific coordinates of the X and the Y value where you want your legend to be.

493
00:55:10,720 --> 00:55:16,630
And I think this selects one of the sources where you want one of the corners of your legend to be.

494
00:55:16,660 --> 00:55:23,070
We'll see in a minute which corner it picked. I forget. I think it's the top left, but I'm not I can't remember for sure.

495
00:55:23,430 --> 00:55:27,389
And then the legend is kind of here.

496
00:55:27,390 --> 00:55:31,290
The group worn kaplan-meier estimate with confidence limits.

497
00:55:31,560 --> 00:55:39,150
This six is the size of the characters that you're printing out in your legend.

498
00:55:39,160 --> 00:55:44,040
So see, x equals one is the default. I want them a little bit smaller so they'll fit better.

499
00:55:44,900 --> 00:55:51,860
And then likewise the line types. So the line type one is usually it's just a solid line.

500
00:55:51,860 --> 00:55:57,559
That's what the Kaplan-Meier estimate is going to look like. And line type two is kind of like a dash line.

501
00:55:57,560 --> 00:56:03,290
That's what the confidence limits are going to look like. So this code here, in this code here are the same.

502
00:56:03,290 --> 00:56:08,209
Except here I put the actual coordinates rather than depending on this locator.

503
00:56:08,210 --> 00:56:13,000
One thing. And this is what the plot looks like.

504
00:56:13,030 --> 00:56:17,560
So instead of shading, you know, you have these dashed lines for your confidence limits.

505
00:56:19,890 --> 00:56:25,500
And so what corner did it pick here? So we said the X was what was our.

506
00:56:25,530 --> 00:56:30,990
Let me just look. So we said the X was one and the Y was point two.

507
00:56:31,990 --> 00:56:35,650
So it's trying to it's trying to use this top left corner.

508
00:56:36,960 --> 00:56:41,130
And it places the legend based on this coordinate at the top left corner that you give it.

509
00:56:44,060 --> 00:56:47,180
Okay. So that is the end of this handout. And.

510
00:56:49,480 --> 00:56:54,580
I've been talking a while, so it's probably a good time to take a break. Yeah, it's 859, so, you know, 909.

511
00:56:54,580 --> 00:57:21,580
Let's get back to work on the next handout. I was looking for such.

512
00:57:32,680 --> 00:57:41,760
Well, I guess. Time to look at.

513
00:59:13,480 --> 00:59:24,210
I was. Yeah.

514
00:59:31,200 --> 01:00:18,900
The. We're.

515
01:00:59,240 --> 01:02:44,570
Like. Really good.

516
01:05:20,170 --> 01:06:35,780
So research. Okay, let's get back to work.

517
01:06:36,650 --> 01:06:44,240
So I was checking my email during the break and just randomly forgive the pun if there is a pun there.

518
01:06:45,740 --> 01:06:57,469
I saw an announcement for an RSA short course on survival models for spatial data, which is not a topic I teach and you are not quite there yet.

519
01:06:57,470 --> 01:07:00,379
You're so close to being able to understand this,

520
01:07:00,380 --> 01:07:09,600
but some of you being epidemiologists might want to know how to look at survival patterns based on spatial data.

521
01:07:09,620 --> 01:07:13,399
You know, there's lots of interesting applications for looking at that.

522
01:07:13,400 --> 01:07:20,750
And, you know, I don't know if you'll be able to attend this and follow it this early.

523
01:07:20,840 --> 01:07:22,159
Learning about survival data.

524
01:07:22,160 --> 01:07:30,920
But just know that there are short courses occasionally out there for you to learn some special topics and be be aware, you know, take a look.

525
01:07:30,920 --> 01:07:34,470
There's some really cool stuff out there. Okay.

526
01:07:34,560 --> 01:07:40,800
So I'm actually curious. I don't know if I'm available to watch that myself, but I think that would be kind of fun.

527
01:07:42,240 --> 01:07:48,000
But I'm a geek, so what do I know? All right. So let me go ahead and get back to work with the handout.

528
01:07:53,440 --> 01:08:02,650
All right. So we figured out, you know, how to talk about random variables in this setting with the two outcomes, X and Delta.

529
01:08:03,220 --> 01:08:08,380
And now we need to kind of learn how to do all the things that you might want to do for inference.

530
01:08:08,470 --> 01:08:15,040
So the simplest next step is to look at two sample tests for this kind of data and.

531
01:08:16,250 --> 01:08:22,130
So the the notation is similar to what we just learned when there are no covers.

532
01:08:22,160 --> 01:08:25,430
So actually I think we can skip by this since we just reviewed this.

533
01:08:26,960 --> 01:08:30,590
So the this is when you have just a single event time.

534
01:08:30,590 --> 01:08:34,309
And so what we're going to do is extend this to have another subscript that

535
01:08:34,310 --> 01:08:39,410
helps us keep track of which group you're in since we're comparing two groups.

536
01:08:40,070 --> 01:08:48,440
So the observed data now is going to still use these X's for the follow up time and the deltas for the reason you stopped following them.

537
01:08:48,440 --> 01:08:52,070
One for the event. Zero if they were still alive when you start following them.

538
01:08:53,430 --> 01:09:00,479
But we're now going to have a subscript four group. So the first subscript is going to be four groups of group one.

539
01:09:00,480 --> 01:09:05,370
All of these have the first steps, group one. The second subscript is for individual.

540
01:09:05,670 --> 01:09:08,399
And we are looking at kind of these, you know,

541
01:09:08,400 --> 01:09:17,100
pairs of data for each person for outcomes in group one and these pairs of outcomes for each person and group two.

542
01:09:18,720 --> 01:09:24,690
And just like with any two simple tasks that you do, you don't have to have the same number in each group.

543
01:09:24,690 --> 01:09:27,930
So. And one doesn't need to equal and to.

544
01:09:30,220 --> 01:09:40,190
We still have to kind of lean on this notation for the observed event times and now there are going to be the observed event.

545
01:09:40,190 --> 01:09:43,659
Death times kind of collapsed across the whole data set.

546
01:09:43,660 --> 01:09:49,720
So whether there is an event from group one or group two, it's going to be collected in this overall list,

547
01:09:49,900 --> 01:09:55,780
130 and it's still going to be like for the whole data set, not specific to each group.

548
01:09:58,030 --> 01:10:03,400
So unique event times pooled across the two groups being compared is what we're collecting here.

549
01:10:06,240 --> 01:10:13,950
And the notation for risk sets at events that we used earlier is going to be similar, but with that second subscript four.

550
01:10:14,340 --> 01:10:23,340
So for instance, the number at risk for group one or sample one, I call it here at time t j we're going to call y one.

551
01:10:23,340 --> 01:10:34,050
J There'll be something similar for group to Y to J and d1j will be from group one or sample one.

552
01:10:34,740 --> 01:10:40,090
The number of events at time j. At time, T.J.

553
01:10:42,490 --> 01:10:46,510
So in this handout, we now have a covariate group. Right.

554
01:10:46,540 --> 01:10:53,170
We didn't have that in the last handout. So now the assumption for the methods in this handout is that t j is independent.

555
01:10:53,560 --> 01:10:58,060
Sorry. T is independent of c i for all individuals.

556
01:10:58,060 --> 01:11:05,750
I conditional on which group they're in. So group could actually be very influential.

557
01:11:05,760 --> 01:11:10,970
But as long as you're conditioning on, group, Ty and C.I. need to be independent.

558
01:11:13,380 --> 01:11:23,520
And the goal for hypothesis tests is not to just think of one number like a mean versus another number mean.

559
01:11:23,520 --> 01:11:29,340
We actually want to know if the survival curve for group one and Group two are comparable.

560
01:11:30,030 --> 01:11:36,540
And so we we actually have little time t in here, but we're thinking about, you know,

561
01:11:36,540 --> 01:11:43,170
the null hypothesis being that the survival curves for all times T or comparable that's the null hypothesis.

562
01:11:44,670 --> 01:11:50,370
Versus they're not. And so we're thinking of the shapes of these curves over time.

563
01:11:50,370 --> 01:11:57,680
Are they the same survival curve or are they different? That's what we kind of want to do with these two simple tests.

564
01:11:59,150 --> 01:12:04,400
And I'm going to take you through a few different tests that people talk about and kind of give you

565
01:12:04,400 --> 01:12:08,870
some intuition about how to use them and how to write about them when you've got results from them.

566
01:12:09,440 --> 01:12:12,140
The most popular test is the log test.

567
01:12:12,890 --> 01:12:18,160
And I'm going to take you through the logic of how that test is put together, and it's going to seem a little familiar.

568
01:12:18,170 --> 01:12:19,700
The notation is going to be different,

569
01:12:19,700 --> 01:12:26,600
but it's going to seem a little familiar because the log ranked test is actually based on the mental Hansell test for two by two tables.

570
01:12:27,230 --> 01:12:37,160
So I'll show you how that works. But it's based it's basically a mental Hansell test applied to two by two tables that all the T.J. event times.

571
01:12:39,630 --> 01:12:42,570
There's some trivia. I have more trivia in the footnote,

572
01:12:42,570 --> 01:12:52,590
but the name Lore Great comes from the tests dependent on the ranks of the observed failure times rather than the actual failure times themselves.

573
01:12:52,590 --> 01:13:03,180
And so as long as the ranks like which is the first, second, third event time, as long as the ordering of the failure times is not changing,

574
01:13:03,540 --> 01:13:09,719
you can stretch and compress them however you want and even move them around slightly.

575
01:13:09,720 --> 01:13:14,790
As long as you're not changing the ranks and you'll get the same results for your tests.

576
01:13:19,310 --> 01:13:28,430
A paper by Peter and Peter reframes the test in terms of the random variable log of the survival times instead of the survival times themselves.

577
01:13:28,430 --> 01:13:34,250
And so that's where the log kind of snuck into it, that it was a log rank that was kind of being.

578
01:13:35,720 --> 01:13:48,740
Figure it out with this statistic. So the law grant test was actually written about by Mantle of Mantle and Hansell, you know,

579
01:13:48,740 --> 01:13:53,570
and he and Mantle first described the test as a special case of the Mantle Hansell statistic,

580
01:13:54,170 --> 01:14:01,520
where two by two tables are constructed for each unique event or death time t 130 and for the two groups.

581
01:14:01,670 --> 01:14:05,540
And so here is an example.

582
01:14:05,540 --> 01:14:11,180
Table for the T js event time.

583
01:14:12,410 --> 01:14:17,030
And so the margins, it's very easy to unpack this by looking at the margins first.

584
01:14:17,060 --> 01:14:27,230
So the total in this table is the number of at risk across everybody in the dataset of who could possibly have had an event at that event time.

585
01:14:28,580 --> 01:14:32,810
And the you know, the groups are the rose.

586
01:14:32,900 --> 01:14:39,800
So the number at risk from group one, the number at risk from group two at that TGT event time.

587
01:14:40,520 --> 01:14:47,460
So these numbers will change if you're looking at a different time. In the day to set a definite bedtime of the data set.

588
01:14:47,850 --> 01:14:51,800
And then the columns are the death that T.J. So these are usually pretty small numbers.

589
01:14:51,810 --> 01:14:57,209
So just the total number of deaths that T.J. often T.J. is a one.

590
01:14:57,210 --> 01:15:03,960
And so you're figuring out which of these have is the one and then this is the number of at risk minus T.J.

591
01:15:03,960 --> 01:15:12,130
So all this is kind of a subtraction. So once you get past the notation, everything is kind of like mental Hansel again.

592
01:15:12,140 --> 01:15:16,880
So they, they focus on a cell which is now the d1g cell.

593
01:15:18,250 --> 01:15:22,240
And they expected value for this cell.

594
01:15:22,360 --> 01:15:28,510
We know quite well from contingency table staff that you look at the the row total times.

595
01:15:29,020 --> 01:15:31,420
The column total over the table total.

596
01:15:32,580 --> 01:15:38,940
And so for fixed margins in the table above assuming no association between group and vital status at that time,

597
01:15:38,940 --> 01:15:50,220
t j this d1j follows a hyper geometric distribution just like we had before, with two by two tables where the mean is the expected cell count.

598
01:15:50,940 --> 01:15:54,570
You know, the row total times. The column total. Over the table total.

599
01:15:56,540 --> 01:16:02,840
Or you could think of it as the D.J. Death Times, the proportionate risk in Group One.

600
01:16:04,420 --> 01:16:08,920
And the variance. This is just variance that's coming from the hyper geometric distribution again.

601
01:16:11,960 --> 01:16:17,820
All right. So it's possible to have zeros for both of these, right?

602
01:16:17,830 --> 01:16:25,660
So if there's nobody at risk in group one at T j, then the expected cell count will be zero.

603
01:16:25,680 --> 01:16:31,980
You wouldn't expect any deaths from from group one if you don't have anybody at risk in group one.

604
01:16:33,030 --> 01:16:42,209
And it will naturally make the V.J. turn also into a zero because you have a zero if there's nobody at risk in group one, actually you have a zero.

605
01:16:42,210 --> 01:16:50,610
If there's nobody at risk in group two as well. So we're going to see that in the data set.

606
01:16:50,610 --> 01:16:56,190
We'll see some edges and veggies that are zero because of that, having nobody at risk in Group one.

607
01:16:59,270 --> 01:17:05,839
And so here's the log rank test. It's just like the mental Hansell test, except this is not in a cell.

608
01:17:05,840 --> 01:17:08,110
It's we were calling it d1j minus.

609
01:17:08,110 --> 01:17:18,140
This expected value summed across all the tables for all the unique event times, summed up and squared, and then dividing by this this variability.

610
01:17:20,530 --> 01:17:26,560
So you have indeed to buy two tables for the number of unique event times across both groups.

611
01:17:28,390 --> 01:17:36,790
And if the null hypothesis is true, if there really is for all time points t or t.j,

612
01:17:36,790 --> 01:17:44,530
if you like, a comparable survival probability and if you have a large number of tables and D,

613
01:17:44,590 --> 01:17:49,600
the log rank statistic follows a chi squared one distribution just like the manual Hansel would.

614
01:17:52,080 --> 01:17:57,610
And I have a little mini data set to help orient you as usual.

615
01:17:57,630 --> 01:18:06,540
You don't have to do this by hand. But for the three of you who learn well by seeing this done by hand, it's going to be here in the handout.

616
01:18:06,930 --> 01:18:10,350
And then we'll show you how to deal with the software later.

617
01:18:11,310 --> 01:18:15,629
So for the ordered death, times are event times.

618
01:18:15,630 --> 01:18:21,440
You're looking for all the deltas that are one and you're trying to order the X's.

619
01:18:21,510 --> 01:18:26,670
So the smallest with a delta of one is this four.

620
01:18:27,600 --> 01:18:31,110
And then just skimming through the next one looks like a seven.

621
01:18:31,950 --> 01:18:35,420
And then these are the only two left. So then it's going to be 15 and 22.

622
01:18:35,430 --> 01:18:42,420
So the order event times are going to be t one of 42 of 73 or 15 t four of 22.

623
01:18:43,370 --> 01:18:48,780
All right. You're not paying attention to any of the sensor values here when you're making this list.

624
01:18:49,640 --> 01:18:55,100
So there's going to be four, two by two tables involved in the log rank test at these times.

625
01:18:55,790 --> 01:19:04,460
So if we were to construct this for the first table for my little mini dataset where there's four people in each group.

626
01:19:05,780 --> 01:19:09,580
Then filling in the margins first is easiest.

627
01:19:09,590 --> 01:19:18,649
So you have all eight people at risk at the first, T.J., time of four and there were four in each group because no one's had their event yet.

628
01:19:18,650 --> 01:19:23,959
They're all still at risk. And then there's only one person who had the event at time four.

629
01:19:23,960 --> 01:19:28,970
So and they were four group from group one. So do you want one is going to be one?

630
01:19:29,360 --> 01:19:37,959
They were in group one and they did have the observed event. The total number of events for the two groups at that time was also one.

631
01:19:37,960 --> 01:19:43,450
So by subtraction we get the zero and actually all this other stuff I can just get from subtraction now.

632
01:19:47,470 --> 01:19:56,350
And so for this table, the parts that are going to contribute to the log rank test are the observed d11 minus the expected value,

633
01:19:56,350 --> 01:20:03,040
which nicely comes out to a half. So, you know, row total times column total over the table total.

634
01:20:03,220 --> 01:20:07,990
It's actually going to be like a half that makes sense under the null hypothesis that it would you know,

635
01:20:07,990 --> 01:20:13,120
you'd have a half chance of having that depth be in group one versus group two and then

636
01:20:13,120 --> 01:20:17,410
the variability step up to sort of put in all the numbers from the table down there.

637
01:20:19,400 --> 01:20:28,670
And so across all the four tables for the top part, if we're looking at the observed d1j minus the expected value here,

638
01:20:28,700 --> 01:20:31,850
this was the one for the table of the event at time four.

639
01:20:32,750 --> 01:20:38,059
And then at the other event times, I just sort of filled out the numbers here.

640
01:20:38,060 --> 01:20:48,780
You can check in the previous your own room. And in this very last table, you end up having no contribution at all.

641
01:20:48,800 --> 01:20:54,560
There were zero events from Group one at that last event time, and there was.

642
01:20:55,730 --> 01:21:00,800
I think zero people at risk from group one at that last event time to that's where that's coming from.

643
01:21:00,830 --> 01:21:08,390
Let's just double check that. So at the last event, time would have been 22 and there was nobody left at risk in group one.

644
01:21:09,470 --> 01:21:18,920
So you end up with your your E one's also your E one for that event time also being zero.

645
01:21:22,680 --> 01:21:27,569
And similarly you get kind of a zero for there's nobody at risk in group one.

646
01:21:27,570 --> 01:21:31,950
So that zeros out the variance for that two by two table as well.

647
01:21:32,930 --> 01:21:36,110
And here's just sort of the the voice filled in from those tables.

648
01:21:36,110 --> 01:21:40,700
If you want to fill those in the privacy of your own room, just to see where things come from.

649
01:21:41,780 --> 01:21:52,099
And when you put together the log rank test, you're going to get a .438 with a P value that's .51 based on a chi squared one distribution.

650
01:21:52,100 --> 01:21:56,870
And I just made up this data for the example, so we weren't expecting a six and an exciting,

651
01:21:57,440 --> 01:22:00,760
significant story from four people in two groups right now.

652
01:22:00,770 --> 01:22:06,770
It's a very teeny, tiny data set. So that's kind of how it goes.

653
01:22:06,770 --> 01:22:09,860
Will We'll see real data sets in the software soon.

654
01:22:10,490 --> 01:22:21,979
But there's something about hazards that is interesting with this test that you should know about, and and that is this idea of proportional hazards.

655
01:22:21,980 --> 01:22:31,820
So it turns out that the law grant test is particularly well-suited if your hazards over time are proportional to one another.

656
01:22:32,450 --> 01:22:37,950
So it's a common way to describe survival differences between groups over time using a single parameter.

657
01:22:37,970 --> 01:22:42,530
So you're, you know, in this special case that the HADS is proportional.

658
01:22:42,560 --> 01:22:46,370
What you're saying is the hazard from group one.

659
01:22:46,370 --> 01:22:53,839
If you divide it by the hazard from group two is some constancy that's just writing it in symbols where this C doesn't depend on time.

660
01:22:53,840 --> 01:23:00,500
I don't have a little T on this side. So even if your hazard took going up over time or they're going down over time,

661
01:23:00,770 --> 01:23:04,010
they're supposed to be doing it in a comparable way in the other group.

662
01:23:04,340 --> 01:23:09,170
So that over time, whatever that hazard is is the same number.

663
01:23:11,770 --> 01:23:17,980
So we've looked at ratios like with odds before, but odds don't have this time thing in there.

664
01:23:18,700 --> 01:23:26,290
So that's the part that's slightly more complicated that you're saying this whole function over time, if you take the ratio, it's the same number.

665
01:23:26,680 --> 01:23:34,240
So it's kind of a strong assumption and we're going to have to know whether that assumption is true or not when we're looking at our analysis.

666
01:23:34,480 --> 01:23:38,950
So log rate test is actually the most powerful test when this assumption is true.

667
01:23:39,340 --> 01:23:43,780
But there's going to be other to sample tests that are better when this assumption is not true.

668
01:23:44,350 --> 01:23:47,890
So we actually have to care about this assumption and whether it's true or not.

669
01:23:49,360 --> 01:23:55,870
So just to understand this a little bit better, there's another way to think about proportional hazards.

670
01:23:57,040 --> 01:24:00,579
It turns out that it also affects how survival curves relate to one another.

671
01:24:00,580 --> 01:24:06,460
And I haven't shown you how this algebra in this algebra are are saying the same things.

672
01:24:06,490 --> 01:24:15,010
I've skipped that. But it turns out that if you have proportional hazards, then one survival curve is always the other.

673
01:24:15,010 --> 01:24:21,910
Race to some power. And so what does that mean?

674
01:24:21,940 --> 01:24:30,940
So if C is greater than one, right, then this s one is as to raise to some C that's greater than one.

675
01:24:32,080 --> 01:24:36,040
And this this is always kind of a brain twister, right?

676
01:24:36,040 --> 01:24:43,210
Because we're not used to taking powers of things between zero and one, but this is two of it's between zero and one.

677
01:24:43,690 --> 01:24:50,080
And so if you raise it to a higher power. That squared number is actually getting smaller.

678
01:24:50,600 --> 01:24:58,040
Just like if you square .5.5 times, 0.5 is 0.25, which is smaller than point five.

679
01:24:58,910 --> 01:25:03,620
So whenever you square a number between zero and one, the number is actually getting smaller.

680
01:25:03,950 --> 01:25:10,190
So C is greater than one then as one is smaller than S S2.

681
01:25:11,680 --> 01:25:15,999
Just like 0.5 times 0.5 equals 0.25.

682
01:25:16,000 --> 01:25:19,330
And .25 is less than 0.5.

683
01:25:19,660 --> 01:25:24,700
Right. And it's true for all times.

684
01:25:24,700 --> 01:25:29,200
Tea. So it seems greater than one then.

685
01:25:29,860 --> 01:25:33,440
You know, these are telling the same story.

686
01:25:33,760 --> 01:25:38,319
Greater than one means that it's more group one has a higher hazard, high heights.

687
01:25:38,320 --> 01:25:41,440
Its are bad, right? And it sees greater than one.

688
01:25:41,440 --> 01:25:46,650
Over here the survival curve is smaller and that's also bad.

689
01:25:46,690 --> 01:25:51,070
So that makes that checks out intuitively.

690
01:25:51,100 --> 01:25:55,690
Higher hazard bad. Lower survival probability bad.

691
01:25:55,810 --> 01:26:05,610
And those two go together. And if see is less than one then the inequality switches.

692
01:26:05,610 --> 01:26:11,960
So see, less than one is, you know, some number between zero one being raised to a number less than zero one.

693
01:26:12,410 --> 01:26:15,530
And that's actually making this number over here bigger.

694
01:26:17,510 --> 01:26:20,930
So the inequality switches.

695
01:26:21,050 --> 01:26:28,280
So that makes sense too, because if C is less than one, it's saying that this has it on top is smaller than the hazard on bottom.

696
01:26:28,280 --> 01:26:31,740
So hazard in group one is less.

697
01:26:31,760 --> 01:26:34,950
So being in group one is better, you know, in terms of survival.

698
01:26:35,270 --> 01:26:40,070
And it checks out here to that group. One survival probability over time is is better.

699
01:26:43,060 --> 01:26:50,020
And one of the key implications just from looking at this is that the ordering remains all through time.

700
01:26:50,020 --> 01:26:53,980
And so it's one of two units that two of us don't cross.

701
01:26:55,080 --> 01:27:03,180
They might get you know, they might have kind of weird patterns over time, but they're not going to ever touch.

702
01:27:05,430 --> 01:27:10,169
So when I'm looking for proportional hazards and if I plot kaplan-meier plots,

703
01:27:10,170 --> 01:27:16,830
one really quick way I decide is if they do cross, I know that they can't have proportional hazards.

704
01:27:18,120 --> 01:27:23,250
The ratio must have been, you know, bigger and then small or small and then big.

705
01:27:25,030 --> 01:27:30,670
If they are going to cross. So if they touch.

706
01:27:31,790 --> 01:27:37,129
You don't have proportional hazards when you when you look at your cap of Meyers and the log

707
01:27:37,130 --> 01:27:41,840
rank test is the most powerful to sample test for detecting proportional hazards alternatives.

708
01:27:42,200 --> 01:27:47,840
So if those curves really do obey proportional hazards, the log rank test is the best one to use.

709
01:27:49,910 --> 01:27:54,930
Generally. There's actually a little bit of an assumption that.

710
01:27:56,180 --> 01:28:00,590
I'll come back to you later about that. It has to do with health.

711
01:28:00,860 --> 01:28:08,820
The length of follow up you're allowed to use for the test. So there's a diagnostic for assessing personal hazards as well.

712
01:28:08,840 --> 01:28:12,890
That's a little bit different from my, you know, looking to see if the kaplan-meier curves touched.

713
01:28:13,340 --> 01:28:17,959
And it's based on this log minus log survival kind of function.

714
01:28:17,960 --> 01:28:24,110
Again, this is the same thing we used when we were trying to figure out confidence limits for the survival function.

715
01:28:24,110 --> 01:28:29,510
But it turns out this turns out the same function can help assess proportional hazards.

716
01:28:31,040 --> 01:28:38,610
And so it's going to be part of our insights output to look at these plots, to diagnose whether we have proportional hazards or not.

717
01:28:39,290 --> 01:28:42,410
So they should be parallel if hazards are proportional.

718
01:28:42,860 --> 01:28:48,720
And I've got algebraic justification for this for those who are curious.

719
01:28:48,740 --> 01:28:53,870
I don't expect you to reproduce this kind of algebra or prove to me ever that this is the case.

720
01:28:54,350 --> 01:28:59,150
But just so you know, you know, when you have this proportional hazards relationship.

721
01:29:00,130 --> 01:29:03,340
A few steps of algebra down here.

722
01:29:03,340 --> 01:29:08,530
This bottom row sort of says if you have if you plot log minus log of the first

723
01:29:08,530 --> 01:29:13,840
kaplan-meier curve versus the log minus log of the second kaplan-meier curve,

724
01:29:14,440 --> 01:29:18,879
then there's a difference here that is essentially a constant.

725
01:29:18,880 --> 01:29:24,790
So whatever this number turns out to be, the log of it is also going to be just a number that doesn't depend on time.

726
01:29:25,720 --> 01:29:30,430
And so these two curves should be separated by something that doesn't happen on time.

727
01:29:30,430 --> 01:29:34,150
Some constant. Okay.

728
01:29:34,500 --> 01:29:40,290
So when people do these plots, they're looking to see if the difference between these two are constant.

729
01:29:40,290 --> 01:29:45,900
And if they are, then you can safely say proportional hazards is true.

730
01:29:46,260 --> 01:29:52,740
And that means that the log rank test is going to be a great test and actually later for the regression setting.

731
01:29:53,580 --> 01:29:58,410
It's also a diagnostic people use to see if the proportional hazards model is a good model.

732
01:29:59,220 --> 01:30:11,120
So it comes this plot comes up again and again. And so some packages like South produce plots with log minus log of survival on the vertical axis.

733
01:30:11,120 --> 01:30:20,870
But instead of just time t they for whatever reason, I'm not even sure why they put log t on the horizontal axis and it's the same diagnostic.

734
01:30:20,870 --> 01:30:26,780
It doesn't matter whether you're looking at T or log t, the same algebras is saying there should be a constant apart.

735
01:30:31,510 --> 01:30:40,350
But again, for me, I find it very after some experience, I can usually look at the curves and kind of come up with a good guess.

736
01:30:40,360 --> 01:30:44,079
It has is a proportional based on whether the Kurds curves cross.

737
01:30:44,080 --> 01:30:52,360
That's a definite no. Or if they start off separate, but then they get closer together like they're about to cross, then that could be a no as well.

738
01:30:52,360 --> 01:30:58,900
So if they rejoin or they're about to rejoin, I can usually tell that's a problem with the proportional hazards.

739
01:31:01,420 --> 01:31:06,820
So if you don't have proportional hazards, there are more powerful tests.

740
01:31:07,390 --> 01:31:18,610
And one of them that used to be the most popular when I was a grad student and is still highly featured in software and output is the wilcoxon test.

741
01:31:19,060 --> 01:31:28,410
And SAS uses this version of the test, the wilcoxon test, or actually gives a slightly different version called the gain wilcoxon test.

742
01:31:28,420 --> 01:31:31,240
But they're essentially trying to get at the same thing.

743
01:31:32,350 --> 01:31:43,090
And the idea is that this test is more powerful when the early differences between groups are prominent.

744
01:31:43,360 --> 01:31:48,040
So if your curves start off really far apart and then they start to join back

745
01:31:48,040 --> 01:31:52,839
together or cross the log point test is really going to have a hard time with that.

746
01:31:52,840 --> 01:31:56,380
But the Wilcoxon test is focusing on those early differences.

747
01:31:56,920 --> 01:32:05,739
And I've got this YJ in yellow because that is basically the number at risk at time t j and you can kind of see everything is the same as the log

748
01:32:05,740 --> 01:32:14,260
rank test except for this YJ and it's weighting more heavily those early times when there are a lot of people at risk and very small weight.

749
01:32:14,260 --> 01:32:17,410
So as you have fewer and fewer people at risk over time.

750
01:32:18,070 --> 01:32:19,840
So that's how it's accomplishing this.

751
01:32:20,260 --> 01:32:28,569
I think the K and Wilcoxon test is doing something similar, but instead of y j, it might be using the kaplan-meier estimate itself at time.

752
01:32:28,570 --> 01:32:35,320
T j. I don't remember exactly what little deviation is, but they're both trying to give higher weights early on.

753
01:32:38,360 --> 01:32:46,309
And so in homework, I'm going to give you either option because you'll be reporting whatever test that software gives you easily.

754
01:32:46,310 --> 01:32:49,690
So in Seth, I'll be asking for the Wilcoxon test in R,

755
01:32:49,700 --> 01:32:54,080
it'll be like the game wilcoxon and you'll just know based on your software package which one to give me.

756
01:32:55,650 --> 01:33:02,879
So the test gives more weight to early survival times. And in cases where survival curves cross or rejoin towards the end of life,

757
01:33:02,880 --> 01:33:09,660
there's little influence on this test because the weights are so small towards the end of the the follow up time.

758
01:33:09,660 --> 01:33:21,350
But the Lagrange test loses a lot of power. And there's another to simple test that has is really become more powerful in my opinion.

759
01:33:21,350 --> 01:33:27,480
It's kind of replace the Wilcoxon test as the go to test when you have crossing hazards.

760
01:33:27,500 --> 01:33:35,080
And so I might teach it to you too. And it is comparing restricted means calculated from the center survival data.

761
01:33:35,090 --> 01:33:44,560
So I'm going to take you through that as well. So what do I mean when I talk about restricted means?

762
01:33:44,570 --> 01:33:53,310
So I want to kind of give you. A quick tutorial on means survival and how restricted means survival.

763
01:33:54,470 --> 01:33:59,570
And it's easy to kind of tell you what's going on in the case where there's no censoring because,

764
01:34:00,320 --> 01:34:03,460
you know, censoring adds that extra ingredient we have to deal with.

765
01:34:03,470 --> 01:34:07,700
So if there's no censoring, you know how to do mean survival times.

766
01:34:08,030 --> 01:34:13,509
You just add up the number of times to event and you divide by end.

767
01:34:13,510 --> 01:34:17,750
And if there's no censoring, you get to observe all these ties and there's no problem.

768
01:34:18,170 --> 01:34:25,310
We only get into problems when we have excise and delta eyes because then we we're not sure exactly what to average,

769
01:34:26,030 --> 01:34:36,710
but when there's no censoring, it's super easy. And it turns out that this tea bar kind of looking creature is exactly the same

770
01:34:37,220 --> 01:34:42,200
algebraically as calculating the area under the corresponding kaplan-meier curve.

771
01:34:43,010 --> 01:34:51,500
So this is a perk up moment because this kind of relationship is what's going to save us when we do have censoring that.

772
01:34:52,070 --> 01:35:02,090
Your t bar estimate is algebraically the same as estimating the kaplan-meier curve all the way through time and adding up the area under the curve.

773
01:35:03,200 --> 01:35:13,639
They're algebraically the same a tao restricted mean only averages events during a restricted follow up period of length.

774
01:35:13,640 --> 01:35:20,780
Tao and you can sort of see how that's going to come up for us and survival because we only get to watch people for certain periods of time.

775
01:35:21,320 --> 01:35:26,540
So we might not get all of the ties. We might have a grant that's five years.

776
01:35:26,540 --> 01:35:31,399
And so we can only add up the events up to five years.

777
01:35:31,400 --> 01:35:37,360
We don't know what happens afterwards. So suppose Tao is ten years.

778
01:35:37,370 --> 01:35:43,300
I'm going back to my ten years for the Tao research it means survival times larger than Tao.

779
01:35:43,550 --> 01:35:48,020
Ten years are just truncated at ten or if you like, restricted at ten.

780
01:35:48,020 --> 01:35:49,520
And the averaging process.

781
01:35:50,830 --> 01:36:02,080
And it turns out that this algebra, when you don't have sensory, is exactly the same as calculating the area under the kaplan-meier curve up to tell.

782
01:36:02,110 --> 01:36:04,540
So if I just look at the area under the curve up to ten,

783
01:36:04,990 --> 01:36:13,360
that's going to be the same as doing this restricted mean by hand based on the restricted numbers that are capped at ten.

784
01:36:14,470 --> 01:36:17,880
So what does this look like? So I've got this data set. Might start to look familiar.

785
01:36:17,890 --> 01:36:22,030
This is our little toy data set. Except I assume there's no censoring this time.

786
01:36:23,050 --> 01:36:29,410
And so if I want the mean of those death times, I'm just doing t bar here and getting 17.8 years.

787
01:36:29,410 --> 01:36:42,870
So that's their average lifetime in this group. And if I have a ten year restricted mean, then for everybody that has, you know,

788
01:36:43,590 --> 01:36:47,930
event times less than ten, I'm just putting those down here to five and eight.

789
01:36:47,940 --> 01:36:53,760
But for people who lived longer than ten, I'm restricting their contribution to the mean to be just ten,

790
01:36:54,360 --> 01:37:05,249
as if we could only watch them ten years and they lived all ten of those and so we get for the ten year restricted mean we get 8.5

791
01:37:05,250 --> 01:37:13,860
years and you can interpret that with something like this and it's like that is during the first Tao equals ten years of follow up,

792
01:37:13,860 --> 01:37:19,719
a person lived an average of 8.5 years. All right.

793
01:37:19,720 --> 01:37:25,720
So that's mean and restricted mean. When there's no censoring.

794
01:37:27,080 --> 01:37:37,130
And so my claim is that I could get these same numbers of 17.8 and 8.5 if I just look at the area under kaplan-meier curves.

795
01:37:38,100 --> 01:37:45,360
And for kaplan-meier curves. The area under those curves is just going to be there's all these little rectangles that are height times.

796
01:37:45,360 --> 01:37:52,290
With every time the kaplan-meier height changes, you get kind of a new rectangle to get the area under that.

797
01:37:53,070 --> 01:38:00,250
And so I've got kind of like a quick. Table here to help us get areas under the curve.

798
01:38:00,880 --> 01:38:07,240
So I've got the height of the kaplan-meier across time and I've got and I put

799
01:38:07,240 --> 01:38:11,979
in this extra time ten here to help us out because I use the number ten a lot.

800
01:38:11,980 --> 01:38:20,110
So the height didn't change because there's no event there. But I have the width of the time interval to help us get the areas up those rectangles.

801
01:38:20,110 --> 01:38:29,740
So between zero and two I have a two year width between two and five, I have a three year with between five and eight.

802
01:38:30,010 --> 01:38:37,330
I also have a three year width and between eight and ten I have a two year width and then you can,

803
01:38:37,840 --> 01:38:40,980
in the privacy of your own room, you can fill out the rest of these widths.

804
01:38:41,500 --> 01:38:49,090
And so the area for each of these little rectangles everywhere are the Kaplan-Meier Heights kind of the same.

805
01:38:49,450 --> 01:38:55,660
We just get by doing height times with so the height times width for the rectangle between zero and two.

806
01:38:56,740 --> 01:39:07,310
For the area under that part of the Kaplan-Meier curve is just to. And the area for the part of the rectangle between two and five is that height

807
01:39:08,180 --> 01:39:12,950
of point nine times the number of years it stayed at point nine of three.

808
01:39:13,370 --> 01:39:17,680
So 2.7 is the area of that rectangle. And so it keeps on going.

809
01:39:17,690 --> 01:39:25,580
So between five and eight height times with this 2.4 and between eight and ten height times with this 1.4.

810
01:39:25,910 --> 01:39:33,290
And so if we so we're going to look at the area under the whole Kaplan-Meier curve, but we're also going to look at the area up to time.

811
01:39:33,290 --> 01:39:36,760
Ten. And get the same numbers we saw before.

812
01:39:36,780 --> 01:39:44,670
So under the whole curve, if we add up all the height times with all the way to the end, we get our margin of 17.8 as claimed.

813
01:39:45,360 --> 01:39:52,050
And if we just add up the area under the curve up to ten years, we get the 8.5 years as claimed.

814
01:39:53,100 --> 01:40:02,550
So when we don't have any missing data area under the curve up to that time point is the same as the restricted mean up to that time point.

815
01:40:03,900 --> 01:40:10,740
And this is very useful because we know how to estimate survival curves even when there's sensor data.

816
01:40:12,310 --> 01:40:18,700
We use the kaplan-meier again, right? And we can get areas under the curve very easily.

817
01:40:18,730 --> 01:40:24,310
So we can't do the T bar based on data we don't have in our hand, we don't have the T bars,

818
01:40:24,640 --> 01:40:30,549
but we can estimate area under a kaplan-meier curve to get at these same restricted means.

819
01:40:30,550 --> 01:40:36,500
So that's what we do in the sensor data case. So with census data,

820
01:40:36,500 --> 01:40:40,670
we estimate how restricted means by calculating the area under the Kappa marker

821
01:40:40,670 --> 01:40:45,600
during a follow up period of interest where that curve is validly defined.

822
01:40:45,620 --> 01:40:52,710
So I'm going to have to give you intuition about where is that curve validly defined.

823
01:40:52,730 --> 01:40:58,010
We're going to need some practice with that because we don't get curves all the way out to the end of the data set.

824
01:40:58,010 --> 01:41:03,560
With censoring, there's going to be some upper limit where we can't go beyond and have a real valid estimate.

825
01:41:04,460 --> 01:41:13,370
So if the last event in the data set is an observed failure time like a death, then the kaplan-meier curve drops to zero at that last event time.

826
01:41:13,910 --> 01:41:22,879
We've seen that happen in our toy data set. Right? And if it drops down to zero, it's going to be zero for all times after.

827
01:41:22,880 --> 01:41:26,330
It's not going to go back up. It never goes back up. These curves just go down.

828
01:41:26,930 --> 01:41:34,999
So if the last event time is a failure, then the kaplan-meier curve is validly defined all the way throughout.

829
01:41:35,000 --> 01:41:43,719
Follow up. And you can estimate the mine's arrival time because you have a you know, you have a whole curve that goes down to zero.

830
01:41:43,720 --> 01:41:50,170
You can get the area under it. You could actually get the mean survival time just by doing the area under the whole curve.

831
01:41:50,470 --> 01:41:56,680
Or if you want, you can choose a follow up time table and estimate the time restricted mean.

832
01:41:58,350 --> 01:42:05,360
Like ten. If you want to. So that that's an important case.

833
01:42:05,370 --> 01:42:10,340
If it turns out that the last event in the data sets a censored value,

834
01:42:10,350 --> 01:42:19,440
then it's a little trickier because the kaplan-meier curve drops down into that less censored value and then just sort of hangs in place.

835
01:42:19,860 --> 01:42:24,090
And we don't have any valid estimate beyond that last censored value.

836
01:42:24,360 --> 01:42:30,120
We don't know it was about to drop to zero or if it was going to stay at that same survival probability.

837
01:42:30,120 --> 01:42:36,899
We just don't have the data to say so. We only know what the kaplan-meier curve is up to that censored value.

838
01:42:36,900 --> 01:42:38,640
That was the last one in the data set.

839
01:42:40,870 --> 01:42:49,300
So we may only report towel restricted means for values of Tao less than that less censored follow at time in that case.

840
01:42:53,580 --> 01:43:02,940
So when you're doing two simple tests comparing to restricted means you're trying to look at these this Tao researcher mean test it's you're trying to

841
01:43:02,940 --> 01:43:08,160
compare the area between kaplan-meier curves over 12 follow up years you're

842
01:43:08,400 --> 01:43:12,960
looking at the area between the survival curves up to some Tao follow up years.

843
01:43:14,540 --> 01:43:18,499
And it's going to be interpreted as the the time units,

844
01:43:18,500 --> 01:43:23,960
I say years of life service whatever the time units of life saved during those those tell

845
01:43:24,530 --> 01:43:28,610
follow up units a follow up for the group with the higher versus the lower restricted mean.

846
01:43:31,350 --> 01:43:35,850
And for each comparison group, the kaplan-meier curve must be valid from 0 to 10.

847
01:43:36,030 --> 01:43:42,780
So you can't compare areas between the two survival curves if there is not a valid estimate all the way

848
01:43:42,780 --> 01:43:47,580
between zero and Tao for the two curves are looking at otherwise the choice of towelettes flexible.

849
01:43:48,760 --> 01:43:57,040
So if you're interested in the one year difference in the years of life saved between the two groups and your time unit is being measured in years.

850
01:43:57,040 --> 01:44:04,749
Then you could look at tower equals one and that would be a valid thing to estimate and do two simple tests on.

851
01:44:04,750 --> 01:44:09,550
As long as both groups have valid Kaplan-Meier estimates through TAO equals one.

852
01:44:11,220 --> 01:44:16,410
Or if you want to look at the five year differences in years of life, say, between two groups,

853
01:44:16,410 --> 01:44:22,170
and you had data with kaplan-meier curves estimated in both groups at time five.

854
01:44:22,170 --> 01:44:23,490
You could do that.

855
01:44:25,450 --> 01:44:35,409
And so often that the two simple test that's published is the difference in your life saved over the maximum valid follow up period that's available.

856
01:44:35,410 --> 01:44:36,580
That's the most common.

857
01:44:37,810 --> 01:44:45,670
And so I want to get a little bit of practice with defining what is that largest possible towel that you can use in these tests.

858
01:44:46,120 --> 01:44:52,780
And you have to know how to do this yourself because you're going to have to construct these tests with help from software.

859
01:44:53,080 --> 01:44:58,240
But you'll be having to put in the towels often in the in these situations.

860
01:44:58,720 --> 01:45:02,990
So for example, one. Group one's largest event.

861
01:45:02,990 --> 01:45:08,450
Time is an observed failure. 15 years and Group Two's largest event.

862
01:45:08,450 --> 01:45:12,050
Time is a censored value at 12 years.

863
01:45:13,100 --> 01:45:17,480
All right. So here's a perk up moment. We're going to be learning this live, you know?

864
01:45:18,080 --> 01:45:24,380
So what is the largest possible tool that you can use to compare these two groups fairly?

865
01:45:28,740 --> 01:45:36,030
So let's think about this. So Group one, the largest event time is the observed failure at 15 years.

866
01:45:36,030 --> 01:45:42,300
And I've said that whenever the largest event time is a failure, the curve drops to zero and stays there forever.

867
01:45:43,110 --> 01:45:48,509
So there's actually no there's no time restriction for talking about this curve.

868
01:45:48,510 --> 01:45:56,070
We have a valid kaplan-meier curve even past 15 years because it's going to be zero after 15 years for this case.

869
01:45:56,760 --> 01:46:03,400
But what about group two? So Group Two's largest event time as a censored value at 12 years.

870
01:46:03,420 --> 01:46:06,570
So does that curve drop down to zero at 12 years?

871
01:46:07,230 --> 01:46:10,530
No, it does not. It's some value above zero.

872
01:46:10,800 --> 01:46:16,180
And I don't know what happens after 12 years. For group two, it's just some number greater than zero.

873
01:46:16,920 --> 01:46:20,700
And so I can only fairly compare these up to 12 years.

874
01:46:21,850 --> 01:46:25,540
And so the largest possible tab for this comparison is going to be 12 years.

875
01:46:27,600 --> 01:46:34,879
All right, let's do another one. So example two is group one's largest event time as an observed failure at

876
01:46:34,880 --> 01:46:41,120
15 years and Group Two's largest event time is a censored value at 18 years.

877
01:46:41,120 --> 01:46:45,620
So what's the using similar logic? What's the largest possible tao here?

878
01:46:50,280 --> 01:46:54,050
So. Let's start with group two this time.

879
01:46:54,740 --> 01:46:59,299
So Group two has a censored value add 18 years as their largest event time.

880
01:46:59,300 --> 01:47:04,040
So at time 18, is that curve going down to zero or not?

881
01:47:06,800 --> 01:47:10,190
No. The last person didn't have their event at 18.

882
01:47:10,910 --> 01:47:17,989
So the survival probably is going to be something greater than zero at 18 when this largest event time is censored.

883
01:47:17,990 --> 01:47:23,960
So we can't look at a capital mark. We don't know what the KAPLAN-MEIER estimate is once you hit 18 in group two.

884
01:47:24,830 --> 01:47:29,229
What about group one? At that largest event time.

885
01:47:29,230 --> 01:47:37,670
Is the curve going down to zero or not? The largest event time was an observed failure this time.

886
01:47:39,730 --> 01:47:44,890
So it is, yes. If the last event time is an observed failure, it will go down to zero.

887
01:47:44,920 --> 01:47:52,630
This curve goes down to zero. And so we know, even at time 18 that survival probability in group one is zero.

888
01:47:53,320 --> 01:47:59,830
So we have a valid estimate all the way through time. So it's group two that's limiting the follow up time period.

889
01:47:59,830 --> 01:48:06,400
We can fairly compare the curves. And so the largest possible tower in this case is going to be 18 for that reason.

890
01:48:08,330 --> 01:48:14,600
And we are out of time. And so, you know what?

891
01:48:15,080 --> 01:48:18,620
This is going to be so long. Before.

892
01:48:18,620 --> 01:48:23,960
The next time we see this hand out, we'll probably review a little bit and then come back to this and we'll play this game some more next time.

893
01:48:23,970 --> 01:48:29,390
So we ended on Slide 19. So next time I'll do a review session.

894
01:48:29,390 --> 01:48:36,230
Please start preparing your sheet. Please work on your homework and remember that I won't have office hours this Friday.

895
01:48:36,230 --> 01:48:44,420
I might announce an extra office hour earlier than Friday, but I can't promise because I don't know what my calendar looks like yet.

896
01:48:44,420 --> 01:48:52,459
I haven't looked. All right, take care. I have office hours today and tomorrow for sure, so you can find me.

897
01:48:52,460 --> 01:49:08,140
Them or the guys have lots to. It's pretty.

