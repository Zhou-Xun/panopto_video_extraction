1
00:00:08,530 --> 00:00:15,969
So we've previously talked about confounding and how it occurs because we live in this highly multifactorial world where

2
00:00:15,970 --> 00:00:22,810
each individual has a multitude of characteristics that both increase and reduce their susceptibility to disease.

3
00:00:23,650 --> 00:00:28,000
Previously, we've mostly focused on one or two confounders at a time.

4
00:00:28,300 --> 00:00:35,920
For example, exercise or sleep, acting as a confounder on the impacts of healthy eating on cardiovascular disease.

5
00:00:36,640 --> 00:00:45,100
However, the reality is that all of the characteristics of a person act on them all at once, and those factors are rarely independent of one another.

6
00:00:45,700 --> 00:00:53,560
As a result, we need more complex methods to understand the larger framework of all of the risk factors that impact an outcome.

7
00:00:53,920 --> 00:00:57,940
In order to isolate the causal impacts of one particular exposure.

8
00:00:59,120 --> 00:01:06,140
In today's video, we're going to discuss a causal diagraming tool known as directed acyclic graphs or dogs.

9
00:01:06,770 --> 00:01:10,969
Dogs were first introduced in the artificial intelligence community as a way to

10
00:01:10,970 --> 00:01:15,740
identify associations between variables by following a set of graphical rules.

11
00:01:16,340 --> 00:01:21,860
This graphical approach was later adopted by economists in their research, and then in the early 2000,

12
00:01:22,130 --> 00:01:28,430
these concepts were brought to the field of epidemiology by Sander, Greenland, Judea Pearl and Tom Robbins.

13
00:01:29,030 --> 00:01:36,950
Since that time, dogs have become a very popular and powerful tool that are used by modern epidemiologists geographically illustrate,

14
00:01:36,950 --> 00:01:44,209
hypothesized or known causal relationships as a way to help elucidate causal and non causal associations.

15
00:01:44,210 --> 00:01:45,350
Among the variables,

16
00:01:45,890 --> 00:01:53,960
dogs are especially useful for detecting the presence of confounding and identifying adjustment sets of variables to account for confounding.

17
00:01:54,920 --> 00:02:01,040
Therefore, in this particular video, I'm going to define for you the roles of directed acyclic graphs.

18
00:02:01,370 --> 00:02:09,410
I'm going to construct these dags for a selected problem, and then I'm going to discriminate between what's an open and a closed path.

19
00:02:09,440 --> 00:02:12,500
I'll tell you more about what that means throughout the video.

20
00:02:12,920 --> 00:02:19,790
And then we're going to use these open and closed paths to identify causal and confounded associations.

21
00:02:22,140 --> 00:02:27,000
So let's start at the very top. What is a directed exec like Graff or a dog?

22
00:02:27,480 --> 00:02:32,400
A dog is a diagram that uses arrows to illustrate causal relationships between

23
00:02:32,400 --> 00:02:37,080
different risk factors and outcomes relevant to a specific research question.

24
00:02:37,860 --> 00:02:43,169
Here, I'm showing you an example from the social epidemiology literature that illustrates

25
00:02:43,170 --> 00:02:48,120
how scientists perceive neighborhood violence to causally impact incident,

26
00:02:48,120 --> 00:02:57,930
cardiovascular disease or CVD, and the important roles that income, race, ethnicity and physical activity interact with their association of interest.

27
00:02:59,710 --> 00:03:05,080
In a dog. Arrows are used to denote a causal relationship between two variables.

28
00:03:05,560 --> 00:03:11,590
The arrows are unit directional, meaning that they start at a cause and they point towards that effect.

29
00:03:12,190 --> 00:03:16,630
Importantly, there are no undirected arrows allowed in dogs.

30
00:03:17,260 --> 00:03:25,810
Similarly, dogs are acyclic, meaning that no arrows are allowed to return from an effect back to its cause.

31
00:03:26,350 --> 00:03:36,100
This hard codes the fact that causes must come before an effect, and it also implies that bi directional arrows are not to be included in a dag.

32
00:03:37,280 --> 00:03:40,399
As you might imagine, in order to be informative,

33
00:03:40,400 --> 00:03:49,880
dogs really need to contain all variables with causal effects on two or more variables of interest, even if they're unmeasured.

34
00:03:50,330 --> 00:03:59,000
In other words, DAX are conceptual in nature and really intended to outline how we perceive causal relationships to act in the world.

35
00:03:59,720 --> 00:04:05,360
Looking at the DAG that I'm showing you here, we understand that the authors have identified income,

36
00:04:05,660 --> 00:04:11,750
physical activity and race ethnicity as the only other critically important variables to consider

37
00:04:11,990 --> 00:04:17,840
when trying to understand the association between neighborhood violence and cardiovascular disease.

38
00:04:18,900 --> 00:04:23,790
In this graphic, we can see that violence is a direct cause of CVD.

39
00:04:24,300 --> 00:04:30,750
You can see this by the use of a directed arrow between these two variables from violence to CVD.

40
00:04:31,530 --> 00:04:38,820
It also has a causal impact through physical activity, as a violent neighborhood often prevents people from doing physical activity,

41
00:04:39,060 --> 00:04:42,330
which in turn then changes the risk of cardiovascular disease.

42
00:04:43,300 --> 00:04:46,750
Many other causal relationships are illustrated in the stag.

43
00:04:47,110 --> 00:04:53,560
For example, you can see that income is shown to cause changes in physical activity, but not the reverse.

44
00:04:53,890 --> 00:04:57,520
Since the arrowhead is pointed towards physical activity alone.

45
00:04:58,270 --> 00:05:02,980
Similarly, we see that race ethnicity is shown to causally impact income.

46
00:05:03,520 --> 00:05:07,510
But because there are no other arrows into income,

47
00:05:07,780 --> 00:05:15,220
we understand that this is assumed to be the only causal factor that is important to study this question about income.

48
00:05:16,890 --> 00:05:22,110
Clearly this highlights that a dog is constructed based on the investigators assumptions.

49
00:05:22,470 --> 00:05:27,360
So it will only be as good as all of the factors that are specified by the investigators.

50
00:05:28,330 --> 00:05:39,850
Importantly, dogs are not only used to illustrate causal associations, but they're also used to identify when non causal associations will occur.

51
00:05:40,330 --> 00:05:44,740
As you can imagine, this makes them extremely informative for confounder detection.

52
00:05:45,710 --> 00:05:49,640
To highlight how these non causal associations are depicted in a DAG.

53
00:05:50,000 --> 00:05:53,090
Let's take for a moment this simplified version of the dog.

54
00:05:53,420 --> 00:06:00,440
And imagine that the only relationships of importance are those between income, neighborhood violence and incident CVD.

55
00:06:01,680 --> 00:06:10,829
Even if we were to ignore the direct impact of neighborhood violence as a cause of incident, CVD as shown here by the directed Arrow.

56
00:06:10,830 --> 00:06:21,000
From violence to CVD, we actually would still see a crude association between violence of CVD due to the shared common cause of income.

57
00:06:22,110 --> 00:06:25,590
This is the traditional depiction of confounding that you're accustomed to.

58
00:06:25,980 --> 00:06:33,930
We have the confounder and come, which is associated both with our exposure and is also an independent risk factor for outcome CVD.

59
00:06:34,990 --> 00:06:43,060
Using our dog rules, we can predict that there will be an association between violence and incident CVD due to an open path

60
00:06:43,360 --> 00:06:50,830
between the variables that travel along the arrows connecting incident CVD to income and then to violence.

61
00:06:52,520 --> 00:06:55,700
So what am I talking about with this phrase open path?

62
00:06:55,710 --> 00:06:57,920
You probably have not heard that before.

63
00:06:58,610 --> 00:07:06,410
Well, an open path is simply a constant connection between two variables of interest that travels along arrows and a dag.

64
00:07:06,920 --> 00:07:09,390
You can imagine it as a highway of sorts.

65
00:07:10,010 --> 00:07:17,510
This connection can go in the direction of arrows or against arrows, so long as there is this connection between adjacent variables.

66
00:07:18,230 --> 00:07:24,680
Open paths are critically important to know and understand because when there is an open path,

67
00:07:24,980 --> 00:07:29,330
an association will be observed between variables along that path.

68
00:07:30,420 --> 00:07:36,420
In the slide, I show three very different open paths that can occur between X and Z.

69
00:07:36,990 --> 00:07:43,290
All of the past are open in spite of the different directionality of those arrows in the first.

70
00:07:43,290 --> 00:07:51,540
In the second scenario, the association that will be observed between Action Z are causal since there are directed arrows.

71
00:07:52,080 --> 00:07:58,020
In other words, in the first case we see that X is a cause of y which then causes Z.

72
00:07:58,680 --> 00:08:06,959
In the second case, there is causality present, but it's a scenario of reverse causation where Z is causing Y,

73
00:08:06,960 --> 00:08:11,280
which is then causing X only in the final example.

74
00:08:11,550 --> 00:08:18,840
What the association observed not be causal because the two variables are not related in a directed way,

75
00:08:19,230 --> 00:08:22,860
but rather they're functioning through a shared common cause.

76
00:08:23,550 --> 00:08:29,880
This type of open path is non causal and is called a back door path.

77
00:08:30,390 --> 00:08:36,690
It represents the classic example of confounding that you have likely seen many times before.

78
00:08:38,510 --> 00:08:45,060
What path that I didn't show you on the last slide is the one that's shown here in this situation.

79
00:08:45,080 --> 00:08:50,540
A path enters and exits a variable through arrows that are colliding on that variable.

80
00:08:51,290 --> 00:09:00,620
In this situation, we call See a collider because those arrowheads on the path are both pointing inwards towards see or colliding on it.

81
00:09:01,280 --> 00:09:05,510
Colliders are very important because if we do not conditioned on them,

82
00:09:05,870 --> 00:09:12,980
then the path between X and Y is closed and there will be no crude association between these variables.

83
00:09:14,530 --> 00:09:21,940
What's the insight behind this? A collider is a common effect of two otherwise unrelated exposures.

84
00:09:22,420 --> 00:09:29,410
For example, let's imagine a scenario where we set our sprinkler to run every Tuesday and Friday at 9:00 at night.

85
00:09:30,070 --> 00:09:33,670
At that time, on those days, our grass will become wet.

86
00:09:34,360 --> 00:09:38,170
In contrast, our grass can also become wet if it rains.

87
00:09:38,530 --> 00:09:48,010
But rain occurs randomly across days and times, while both rain and sprinklers cause wet grass across all days and times,

88
00:09:48,250 --> 00:09:51,700
we would expect to see no crude association between the two.

89
00:09:53,500 --> 00:09:58,630
And that's it. You now know the simple rules to identify crude associations with an attack.

90
00:09:59,170 --> 00:10:04,000
Just to summarize, we have arrows that indicate causal relationships.

91
00:10:04,450 --> 00:10:13,630
We've learned that open paths between two variables results in crude associations between these variables directed pass or causal,

92
00:10:13,660 --> 00:10:18,400
whereas non directed pass are non causal and they're called backdoor paths.

93
00:10:18,820 --> 00:10:22,330
Colliders close pass between two variables.

