1
00:00:01,380 --> 00:00:05,360
I was freaking out for a second when I checked the syllabus and I was like, Sorry, I've lost 3%.

2
00:00:07,720 --> 00:00:12,140
Yeah, I mean, like a lot stuff. I thought, Wow.

3
00:00:12,400 --> 00:00:17,310
I was like, Oh, that's what you do after that?

4
00:00:17,470 --> 00:00:21,390
Yeah. So he can tell you the truth.

5
00:00:21,660 --> 00:00:29,640
We'll have to tell you the truth. Like the waiting room and the other.

6
00:00:30,000 --> 00:00:33,240
Yeah, she's a little while later. I know.

7
00:00:34,020 --> 00:00:41,770
I don't mind. So it's like if I don't think I was arguing, but like, if he's going to tell you this, great.

8
00:00:42,420 --> 00:00:45,560
I see it all the time.

9
00:00:49,550 --> 00:00:56,870
Right? Yeah. How was your function? I look like a funny cause.

10
00:00:56,930 --> 00:01:02,350
I mean, I think that it covered.

11
00:01:02,370 --> 00:01:09,360
I think maybe one of her, like, having to write her own interest and support every single.

12
00:01:09,960 --> 00:01:11,060
Because if you just like,

13
00:01:11,310 --> 00:01:28,170
you write the perfect one with two other times I was coming around one the core like parallel with like a linear model for this stuff.

14
00:01:28,800 --> 00:01:33,209
And I, I see this touch. I was the side that took us.

15
00:01:33,210 --> 00:01:37,170
Yeah. So, like, I did it, it was terrible.

16
00:01:37,200 --> 00:01:44,559
I was like, I would like to have, you know, like the cursor cap, like moving.

17
00:01:44,560 --> 00:01:57,940
You're being to say, you know, in the sense that when you play for free, but you got to put it into a super basic.

18
00:01:58,650 --> 00:02:11,300
Yes. And then we just like rather than say, oh, it's obviously other people just like, I'm not going to those, you know, which honestly it is.

19
00:02:11,520 --> 00:02:17,070
I think it would be easier that way.

20
00:02:17,760 --> 00:02:25,910
You could try to do that. When he was saying to like what the problem was, you know, we're hands we here.

21
00:02:27,950 --> 00:02:31,520
Like, it's not like I didn't.

22
00:02:34,920 --> 00:02:39,390
All right, guys, let's get started. This is slightly better than writing for break.

23
00:02:39,510 --> 00:02:44,550
I guess so. Thank you for coming back. I hope you all had a great break.

24
00:02:45,570 --> 00:02:54,930
We have today's lecture about meta analyzes and then Thursday, I'm going to do a very quick high level overview of what the projects are about.

25
00:02:54,930 --> 00:03:01,380
So like smart designs, web design, trial design, very quick overview.

26
00:03:01,410 --> 00:03:05,970
Your projects are due uploaded by Thursday and it looks like several of you.

27
00:03:06,000 --> 00:03:18,600
I just I haven't checked it actually, but there's one, two, three, four, five, five projects up here already out of the 16, which is quite amazing.

28
00:03:18,600 --> 00:03:23,040
We got some great non procrastinators in here.

29
00:03:23,970 --> 00:03:28,670
So if you are remembering go look here you have the peer reviewed two projects.

30
00:03:28,670 --> 00:03:31,920
So your project, they are peer reviewing. One of them might already be up and you could do it.

31
00:03:32,310 --> 00:03:38,490
Otherwise you should be completing your video and uploading it. There are no bonus points for ending early, it's just that you got it out of the way.

32
00:03:38,850 --> 00:03:39,509
So don't worry.

33
00:03:39,510 --> 00:03:53,490
It just needs to be uploaded by Thursday and then from Thursday through Tuesday, I hope that I'm going to put on canvas on the home page here.

34
00:03:54,090 --> 00:03:59,669
I'm going to select four, 12, six, some like half the projects that we're going to go through.

35
00:03:59,670 --> 00:04:07,620
So project number is, I don't know, maybe once before I might switch it up and then the other project numbers will go through on the eighth.

36
00:04:08,340 --> 00:04:13,950
And so I hope that a lot of you have watched the videos or if you're peer reviewing one of those projects, you've done that.

37
00:04:13,950 --> 00:04:16,139
So we can have a really interesting discussion.

38
00:04:16,140 --> 00:04:25,469
And I would expect that if you're in one of the projects that we talk about on the sixth, you're here in person to contribute to the conversation.

39
00:04:25,470 --> 00:04:29,129
And similarly, if you're in one of the projects that we discuss on the eighth,

40
00:04:29,130 --> 00:04:32,520
that you're here in person to answer questions and contribute to that discussion,

41
00:04:34,920 --> 00:04:40,680
I know that it's the end of the semester and there's not necessarily one new material that I'm lecturing about,

42
00:04:40,680 --> 00:04:48,240
but this is new material that's still very much a part of the course that would help you in your general knowledge about clinical trials.

43
00:04:49,080 --> 00:04:53,610
So I do hope that you continue to come to class and engage and perhaps there will

44
00:04:53,610 --> 00:04:58,140
be extra credit for those who continue to come through the end of the semester.

45
00:04:58,260 --> 00:05:08,159
So next. Week, sixth and eighth, we'll have a little attendance ticking so that there could be if you're there, there could be some extra credit.

46
00:05:08,160 --> 00:05:14,520
But please don't call if you're sick. Don't don't worry. Please don't call me if you have COVID or the flu or something.

47
00:05:15,420 --> 00:05:19,630
Okay. I have your tests. They're graded. You should be able to see your grades.

48
00:05:19,650 --> 00:05:24,390
I will give them out. Or you can come get them at the end of class. The answers have been posted online.

49
00:05:24,630 --> 00:05:29,490
Many of you will see that you got near. Well, some of you will see you got perfect or near perfect scores.

50
00:05:29,910 --> 00:05:34,260
There was extra credit. So it's not that you necessarily actually got 100%.

51
00:05:34,260 --> 00:05:39,390
So please do review it and look at the answers so that you see what you did get wrong.

52
00:05:39,930 --> 00:05:45,330
I did appreciate reading your credit. Thank you for letting me know what you enjoyed.

53
00:05:45,690 --> 00:05:49,260
And I also like seeing what people are grateful for.

54
00:05:49,290 --> 00:05:55,049
It was pretty cute that some people did decide to say something about 619 their variable.

55
00:05:55,050 --> 00:05:59,879
For a lot of people though, it was very nice to think about what they were doing or break or who they were seeing.

56
00:05:59,880 --> 00:06:04,890
And so anyway. Any questions before we move on for today?

57
00:06:06,260 --> 00:06:12,830
But the project, your homeworks are over. Once your projects are done and you've done the peer reviews, you can kind of breathe with this class.

58
00:06:12,950 --> 00:06:16,370
And you hope again that you keep showing up and learning.

59
00:06:16,400 --> 00:06:19,700
Yeah. Just curious, did you cap the grades for the test at 100%?

60
00:06:20,450 --> 00:06:28,010
I did not. There was one, but there were a few individuals who got over 100%.

61
00:06:29,420 --> 00:06:34,000
But that with actually. Okay.

62
00:06:34,010 --> 00:06:36,430
So today we were talking about meta analysis,

63
00:06:36,430 --> 00:06:44,500
which is not a type of clinical trial but is incredibly important and has a lot of similarities to a really good clinical trial.

64
00:06:45,610 --> 00:06:48,759
And we are doing this one lecture on this.

65
00:06:48,760 --> 00:06:53,230
I actually took a whole course a full semester on that analysis when I was a grad student.

66
00:06:53,740 --> 00:06:58,000
It was offered by the State Department at Pitt, where I went to grad school,

67
00:06:58,210 --> 00:07:04,480
and the guy who taught it was like his advisor was Ingram Hogan, who is well known for meta analysis.

68
00:07:04,480 --> 00:07:11,260
And he actually made very derogatory comments the whole semester about how bias was inferior to statistics.

69
00:07:11,590 --> 00:07:20,800
And yet I had the best grade in that class. I was just like, Ha! You know, I was so annoyed the entire semester about his like putting bios down.

70
00:07:21,970 --> 00:07:28,180
Anyway, this is a, this is a a topic that could clearly take an entire semester.

71
00:07:28,750 --> 00:07:35,079
We don't have a full class on it and I don't know of any other class that talks about meta analyzes, but this is very important.

72
00:07:35,080 --> 00:07:47,920
It's used across many different disciplines and it's the ultimate evidence for any subjects or any result that you would want to have.

73
00:07:48,550 --> 00:07:54,400
So we're going to try to understand the role of it and how it is at the top of the hierarchy of evidence.

74
00:07:54,790 --> 00:08:01,029
We want to illustrate the steps of a meta analysis. We're going to kind of go through a very simple example well,

75
00:08:01,030 --> 00:08:06,790
understand how and why we test for homogeneity and what that is, and how to handle non homogeneity.

76
00:08:06,790 --> 00:08:11,859
Well, identify the strengths and weaknesses of meta analyzes and try to understand how you show the result,

77
00:08:11,860 --> 00:08:19,150
how the results are shown in figures in particular for this plot, and then talk about biases and with a funnel plot.

78
00:08:21,220 --> 00:08:30,340
So when you hear sometimes these term systematic review, meta analysis and matter regression are all used as if they're synonyms, but they're not.

79
00:08:30,730 --> 00:08:35,950
So systematic review is the actual process of collating this empirical evidence.

80
00:08:35,950 --> 00:08:41,679
So you say you want to look at the effects of X and then you try to find all of the studies that

81
00:08:41,680 --> 00:08:47,380
have been done on the effect of X or you want to in this population and you bring it all together.

82
00:08:48,880 --> 00:08:52,330
You're so you're trying to answer a specific research question and you have to

83
00:08:52,660 --> 00:08:57,460
very carefully delineate your eligibility criteria of what you're interested in.

84
00:08:58,600 --> 00:09:04,390
You want to use explicit, systematic methods. So this is not some sort of random or haphazard gathering.

85
00:09:05,110 --> 00:09:09,880
You're really trying to minimize bias and to provide more reliable findings.

86
00:09:10,360 --> 00:09:15,370
There is a full handbook for systematic reviews, the Cochrane Handbook,

87
00:09:15,370 --> 00:09:19,900
and I'm not sure if this is the latest version, but this was the latest version as of a year or two ago.

88
00:09:20,740 --> 00:09:30,220
And so again, this is a very big topic in and of itself that has a lot of literature that goes around systematic reviews and meta analyzes.

89
00:09:30,910 --> 00:09:37,090
So a systematic review clearly states the objectives they have predefined eligibility criteria.

90
00:09:37,390 --> 00:09:44,320
We're looking for explicit, reproducible methodologies so that we can write it down, kind of like in a protocol and somebody else could follow it.

91
00:09:45,460 --> 00:09:50,650
We want to try to identify all of the studies that meet this eligibility criteria.

92
00:09:50,650 --> 00:09:58,450
So we're scouring online and all of the databases and potentially otherwise to figure out what exists there.

93
00:09:58,870 --> 00:10:03,189
We want to assess the validity of the findings of all the studies were including and

94
00:10:03,190 --> 00:10:10,510
then try to synthesize all of this and present some sort of systematic presentation.

95
00:10:10,960 --> 00:10:16,960
Now the systematic review isn't necessarily quantitative or using lots of statistics.

96
00:10:16,960 --> 00:10:24,040
That's where the meta analysis comes in. Once we start doing the actual math on this summary or synthesis, that's the meta analysis.

97
00:10:24,430 --> 00:10:35,380
The systematic review could be more qualitative or more very summary driven and not not really focusing on the hard numbers.

98
00:10:36,640 --> 00:10:42,100
The meta analysis is going to use the statistical methods that you all learn in all of your courses to actually

99
00:10:42,310 --> 00:10:49,390
bring those results together in a quantitative fashion and summarize the results of these independent studies.

100
00:10:50,410 --> 00:10:54,219
So most a lot of systematic reviews include a meta analysis.

101
00:10:54,220 --> 00:11:02,620
It seems quite obvious that if you're interested in summarizing studies that you would do this sort of quantitative analysis or meta analysis on it,

102
00:11:03,280 --> 00:11:06,729
but not all the time. So it could be more qualitative.

103
00:11:06,730 --> 00:11:12,370
As I said, you could have standalone, systematic reviews that don't use statistics.

104
00:11:15,310 --> 00:11:17,530
Also, you might try to use a meta analysis.

105
00:11:17,530 --> 00:11:21,909
And as we're going to talk about these studies that you're trying to combine or synthesize might be so different,

106
00:11:21,910 --> 00:11:24,879
you can't actually apply the methods.

107
00:11:24,880 --> 00:11:32,800
And so therefore you end up sort of just doing this more qualitative assessment or systematic review and the goal here.

108
00:11:32,860 --> 00:11:37,659
Or is to combine or synthesize all of the information and to provide even more

109
00:11:37,660 --> 00:11:42,070
robust evidence or more precise estimates than the individual studies themselves.

110
00:11:43,640 --> 00:11:47,240
So what we want to do is arrive at conclusions about the body of research.

111
00:11:47,510 --> 00:11:54,200
Maybe some of the best known meta analysis that you might be familiar with are the ones that we're trying to show that autism,

112
00:11:54,230 --> 00:11:56,540
that vaccines do not cause autism.

113
00:11:56,550 --> 00:12:04,780
If you remember right a while ago, there was that article in The Lancet that was a very small basically case report,

114
00:12:04,940 --> 00:12:10,579
case study, where this physician said, oh, and now vaccines cause autism.

115
00:12:10,580 --> 00:12:13,520
And there are lots of errors in this in this study.

116
00:12:13,880 --> 00:12:23,480
And so people pointed out the errors that that manuscript was retracted, but yet people still held onto this belief that vaccines cause autism.

117
00:12:23,720 --> 00:12:27,470
And so to try to disprove it, scientists said, well, let's do mat analyzes.

118
00:12:27,470 --> 00:12:34,610
So they got all the studies together that looked at vaccines and its effects, particularly its relationship to autism,

119
00:12:35,030 --> 00:12:43,250
and proved with millions of individuals across multiple studies that there was no association between vaccines and autism.

120
00:12:44,970 --> 00:12:50,250
But there have been many, many other meta analyzes. So we're going to try to combine similar studies.

121
00:12:51,300 --> 00:12:54,440
And when we say similar, we want to have a similar population.

122
00:12:54,450 --> 00:12:58,830
We want to have similar treatments, and we want to be measuring the same kind of outcomes.

123
00:12:59,130 --> 00:13:02,880
Right? Otherwise, it's going to be very difficult to actually put these things together.

124
00:13:03,960 --> 00:13:09,660
And the goal here is that we can have an increase in power. Right. Our numbers grow substantially now that we're gathering all the data together.

125
00:13:10,830 --> 00:13:19,140
We're improving precision. We can potentially answer questions that each individual study couldn't answer because their sample size was too small.

126
00:13:19,920 --> 00:13:24,030
Now we can actually more data we have we can potentially answer more questions.

127
00:13:24,780 --> 00:13:28,679
And then if some studies said, well, we found this and other studies said no,

128
00:13:28,680 --> 00:13:32,660
we found that we can try to put all that together and see, well, is it this or that?

129
00:13:32,670 --> 00:13:38,100
You know, what? What do we actually see in this full body of research looking across all these studies?

130
00:13:39,930 --> 00:13:42,810
However, bless you better analyzes.

131
00:13:43,470 --> 00:13:51,510
While they can be incredibly powerful, they can also be very difficult because if you're familiar with the big data world and data integration,

132
00:13:51,810 --> 00:13:56,190
it can actually be very difficult to integrate datasets or integrate these different

133
00:13:56,190 --> 00:14:01,379
studies to actually say that you have robust results because you have similar population,

134
00:14:01,380 --> 00:14:04,470
you have similar outcomes, you have similar treatments.

135
00:14:04,470 --> 00:14:12,180
These might not all be that similar. And so there's a lot of potential biases, variation across the studies.

136
00:14:12,810 --> 00:14:17,910
There could be studies that aren't reported or published because they weren't positive and you don't have them.

137
00:14:18,360 --> 00:14:22,620
And so you might still have a biased result at the end.

138
00:14:24,120 --> 00:14:33,350
Okay. So it used to be that we could look at this nice triangle or pyramid of the evidence hierarchy.

139
00:14:33,360 --> 00:14:43,319
The more you now go in and try to search for the evidence hierarchy, you'll see all of these like at least three D, if not like new shapes.

140
00:14:43,320 --> 00:14:44,700
They don't quite look as nice.

141
00:14:45,630 --> 00:14:54,270
But anyway, this is attempting to say that like at the very bottom, right would be case studies, case reports, somebody's own opinion.

142
00:14:55,110 --> 00:15:06,600
And then once we start doing more controlled studies, prospective studies, we get we have more evidence or we believe more in the results.

143
00:15:06,690 --> 00:15:10,320
Right. And so we have case control studies, cohort studies.

144
00:15:11,250 --> 00:15:16,469
Often we think of randomized controlled trials as like the gold standard or the best evidence we could get.

145
00:15:16,470 --> 00:15:23,129
But that's not actually at the top, right? It's actually the combination of these studies, the different trials,

146
00:15:23,130 --> 00:15:28,980
plus observed data, etc., that comes together in systematic reviews or meta analyzes.

147
00:15:28,980 --> 00:15:31,260
That's at the very top of the evidence hierarchy.

148
00:15:34,980 --> 00:15:45,270
It is possible that a really big well run asked a randomized controlled trial could give us better data about this topic because

149
00:15:45,270 --> 00:15:51,270
where we have lots of studies that actually aren't all that similar and so the meta analysis isn't all that convincing.

150
00:15:51,270 --> 00:15:59,159
But mostly we find that meta analyzes with the ability to combine these very different studies are not very different,

151
00:15:59,160 --> 00:16:02,310
but different studies that are hopefully similar.

152
00:16:03,000 --> 00:16:08,130
Allowing us to get more precision can be more powerful. Okay.

153
00:16:08,160 --> 00:16:19,690
So the meta analysis is the actual quantitative review. Yes. So you're saying that they're for similar outcomes and and life after treatment?

154
00:16:19,740 --> 00:16:25,049
Yeah. But is also for similar populations like the U.S. population?

155
00:16:25,050 --> 00:16:28,920
Or would they if you combine like say that there was only a certain amount of

156
00:16:28,920 --> 00:16:33,459
studies that were looking at specific relationship with different countries.

157
00:16:33,460 --> 00:16:39,600
So that's a good question. You may or may not want to combine those depending upon your understanding of the way that like.

158
00:16:42,800 --> 00:16:51,110
I don't know, environment might impact, right? Because I think saying genetics is probably not a good reason not to combine them.

159
00:16:51,110 --> 00:16:54,570
Right, because we're almost all exactly the same regardless of where we live.

160
00:16:55,250 --> 00:16:59,690
But there could be very different environmental exposures that could play a role.

161
00:17:00,110 --> 00:17:04,759
So it's possible that they would want to keep to like the same country.

162
00:17:04,760 --> 00:17:08,660
But it is also possible that they'd say, Well, let's combine these and see if they agree.

163
00:17:08,990 --> 00:17:12,319
And then what you'll see is that we can test for how similar these studies are.

164
00:17:12,320 --> 00:17:16,880
And if they start to see that they're not similar, they might say, well, it's because they're from different countries.

165
00:17:17,870 --> 00:17:23,960
A lot of really big trials are often international, but they don't necessarily have to be.

166
00:17:25,280 --> 00:17:28,370
So it just sort of depends on what how they feel.

167
00:17:29,060 --> 00:17:35,840
The setting would play a role in the outcome or the treatment that.

168
00:17:37,480 --> 00:17:46,120
So the meta analysis is a quantitative or statistical review of the synthesis of the materials or of the systematic review.

169
00:17:47,230 --> 00:17:52,130
Usually there is an informal narrative synthesis that's that systematic review.

170
00:17:53,230 --> 00:17:59,559
But we don't want to leave it there. Right. Most of the time, if we have the data, because that can be very subjective,

171
00:17:59,560 --> 00:18:08,530
it can be biased by the author and where they're coming from, where they where they see what's happening.

172
00:18:08,950 --> 00:18:14,749
The statistics can provide us some better data. Okay.

173
00:18:14,750 --> 00:18:22,450
So ideally a really good meta analysis is just like a randomized multicenter clinical trial.

174
00:18:22,460 --> 00:18:27,410
So this is where we see this parallel and part of the reason why we're talking about it in this class.

175
00:18:27,860 --> 00:18:35,030
So a multicenter, clinical controlled trial would be that we are having, you know,

176
00:18:35,030 --> 00:18:41,359
we have like treatment A versus treatment B and maybe treatment B is some control standard of care or placebo.

177
00:18:41,360 --> 00:18:42,919
We're really interested in the effect of treatment.

178
00:18:42,920 --> 00:18:51,499
A and we've done this at Michigan, we've done it in University of Michigan, we've done at Detroit Mercy.

179
00:18:51,500 --> 00:18:55,700
We've done it in Minnesota, we've done it in Ohio, we've done in Pennsylvania.

180
00:18:55,700 --> 00:19:00,800
We have all these different sites and they all were running the same trial, the exact same protocol,

181
00:19:01,010 --> 00:19:07,640
the exact same treatment, A and B that they had, the same inclusion exclusion criteria, so the same types of subjects.

182
00:19:07,850 --> 00:19:11,990
And we were interested in that primary outcome, which was the same because it was all in the trial.

183
00:19:12,020 --> 00:19:17,480
Right. This would be the ideal, ideal analysis, basically, that each center is its own trial,

184
00:19:17,960 --> 00:19:25,670
but we're combining them across all of the centers to get the ultimate goal of that large clinical trial.

185
00:19:26,150 --> 00:19:28,700
Now, as you can imagine, right,

186
00:19:28,700 --> 00:19:36,290
this is not what's going to happen when we're actually trying to collect studies to answer a certain answer or a certain question.

187
00:19:36,540 --> 00:19:43,550
There is going to be differences. And so we have to figure out how different are they and how much do we believe that we can combine them.

188
00:19:45,830 --> 00:19:50,330
So the first step in a meta analysis is that you have to do a very thorough literature research.

189
00:19:50,600 --> 00:19:54,670
So you go online, you Google Parent Club Med, you go to Harvard.

190
00:19:54,800 --> 00:19:59,209
I think I have a list of lots of it. ClinicalTrials.gov, Cochrane Library.

191
00:19:59,210 --> 00:20:04,910
What the science. These are just a few of these databases that you'd start to go in, you'd put in these keywords.

192
00:20:04,910 --> 00:20:14,150
You have to be very careful about your keywords and you're going to get most likely lots and lots and lots of results.

193
00:20:14,420 --> 00:20:21,230
You could get thousands, hundreds of thousands of results. So meta analysis are not quick studies.

194
00:20:21,650 --> 00:20:28,370
Meta analysis most often take years, and they take a team of individuals working together where often,

195
00:20:28,370 --> 00:20:33,920
like graduate students might be involved in searching,

196
00:20:34,040 --> 00:20:40,700
looking at the the literature search and trying to weed out what actually is relevant versus what isn't.

197
00:20:41,060 --> 00:20:44,480
Because you just collected a lot of data from keywords and then it might not actually be that,

198
00:20:44,480 --> 00:20:49,460
that that study's useful or that was just a that abstract was repeated in this paper.

199
00:20:49,730 --> 00:20:52,850
ET cetera. But you're looking for all existing studies.

200
00:20:52,850 --> 00:21:02,690
So all of the studies that exist out there, all of those that were published, you also want to try to figure out if it wasn't published like it might.

201
00:21:02,690 --> 00:21:05,680
There might be stuff in an abstract, but then there wasn't a manuscript.

202
00:21:05,700 --> 00:21:12,890
You want to try to get that information you're trying to look for for studies that were well done.

203
00:21:13,610 --> 00:21:19,580
But you also want to get studies that exist on this topic that maybe were not all that well done.

204
00:21:20,660 --> 00:21:28,570
And you want to be able to document that, and you just want to be able to say that you did this very thorough research and you found every

205
00:21:28,580 --> 00:21:33,050
you know as much as you possibly could in this area that could answer this question of interest.

206
00:21:39,080 --> 00:21:41,750
By combining all of these things, right.

207
00:21:41,750 --> 00:21:49,870
We're going to have instead of just like one trial with a sample size of 1000 or 500, we're going to have lots of trials and studies together.

208
00:21:49,910 --> 00:21:56,930
We could have sample size and like the millions or hundreds of thousands, or if it's a rare disease, maybe it's in the thousands.

209
00:21:56,930 --> 00:22:05,660
And so it can just provide us with a lot more data, more precise treatment estimates, and the ability to find smaller treatment effects.

210
00:22:06,660 --> 00:22:10,940
Now often we actually have a summary data. We don't actually get all the data right.

211
00:22:10,980 --> 00:22:16,260
It's not like we usually you can actually do a mat analysis where you try to collect all the data sets,

212
00:22:16,260 --> 00:22:24,930
but that's quite difficult because often people don't want to share their data or it was from a long time ago and the data no longer exists somewhere.

213
00:22:25,230 --> 00:22:35,969
So usually we're pulling summary metrics out of of publications and then once we figure out, so for example,

214
00:22:35,970 --> 00:22:45,750
say they're really interested in like the effect of this particular chemotherapy in breast cancer.

215
00:22:45,780 --> 00:22:48,870
Right. We're going to look we're going to try to search on that chemotherapy.

216
00:22:48,870 --> 00:22:52,650
We're going to search breast cancer. We're going to look for trials or studies.

217
00:22:53,460 --> 00:22:59,460
And then we're going to get all that information and we're going to try to look in the publication and see, okay, what was the treatment effect?

218
00:22:59,460 --> 00:23:03,120
Right. It might be a hazard ratio. It might be a odds ratio.

219
00:23:03,120 --> 00:23:07,620
It might be a mean something.

220
00:23:08,460 --> 00:23:14,460
It could be a proportion. Right. We want to try to take that information and collect it.

221
00:23:15,300 --> 00:23:21,240
What we're going to focus on today in our examples is thinking about a binary outcome.

222
00:23:21,240 --> 00:23:29,160
So like response and nonresponse and thinking about we, you know, we could have like a series of two by two table.

223
00:23:29,160 --> 00:23:32,760
So these patients responded on treatment or they didn't respond on treatment.

224
00:23:32,760 --> 00:23:38,040
They these patients responded on treatment. B Or they didn't respond or like A versus control, right?

225
00:23:38,040 --> 00:23:42,960
We can think about trying to pluck that information out of each study and then how do we combine it?

226
00:23:44,700 --> 00:23:50,159
So that'll be our example. But you can do this with any kind of sorry statistic, right?

227
00:23:50,160 --> 00:23:55,800
It can be a proportion. It can be a mean, it could be a regression coefficient, a beta, it can be a correlation coefficient.

228
00:23:56,130 --> 00:24:02,040
There are all types of methods for how you combine these different summary measures together across studies.

229
00:24:04,600 --> 00:24:11,460
Okay. So one of the the summary statistic that I'm talking about usually is what word we're describing.

230
00:24:11,470 --> 00:24:18,549
We're really interested in the effect size and this effect size we talked about when we talked about sample size, right?

231
00:24:18,550 --> 00:24:24,760
An effect size is what is what you're ultimately interested in is a comparison between the treatment groups.

232
00:24:27,250 --> 00:24:32,230
And what we want in this effect size is that we wanted to standardize the findings

233
00:24:32,530 --> 00:24:37,959
in that trial and in the other trial so that we can directly put them together.

234
00:24:37,960 --> 00:24:43,240
Right. We don't want something in this trial, on this scale and something on this trial and this other scale.

235
00:24:43,480 --> 00:24:45,820
We want them to be standardized so we can combine them.

236
00:24:48,700 --> 00:24:57,850
When we talked about sample size, often we're using a standardized effect size or a standardized mean difference.

237
00:24:58,150 --> 00:25:02,470
That's the difference in the means between the two groups divided by the variance.

238
00:25:02,920 --> 00:25:10,690
And so it's often on this 0 to 2 one scale it could be higher, but it's usually in the 0 to 1 scale for continuous variables.

239
00:25:10,690 --> 00:25:19,389
It's called a Cohen's D and we think when it's small, right, like 02.3, it's a small effect size, a small Cinryze effect size.

240
00:25:19,390 --> 00:25:22,540
And the larger it gets, the bigger effect size.

241
00:25:22,540 --> 00:25:28,330
So anything over point seven is a large effect size or a large standardized mean difference between groups.

242
00:25:30,490 --> 00:25:35,740
It could also be if it's a binary variable, it might be that we're looking we're interested in the odds ratio.

243
00:25:37,810 --> 00:25:42,670
So like the odds of success, right? And the treatment over the odds of success in treatment.

244
00:25:43,720 --> 00:25:50,080
And we want to try to get this effect size such that we can now compare it across studies.

245
00:25:50,470 --> 00:25:54,250
We're interested in both how big the effect size is and also the direction.

246
00:25:54,250 --> 00:25:59,049
So which treatment is actually better and its effect size is standardized.

247
00:25:59,050 --> 00:26:05,170
So now it's independent of the sample size, right? We don't want the effect size to be connected to the sample size,

248
00:26:05,170 --> 00:26:10,299
such that the larger the sample size, the smaller the effect or the larger the effect.

249
00:26:10,300 --> 00:26:14,350
Right. We want to, again, people to just standardize this and combine them.

250
00:26:16,420 --> 00:26:19,569
So we'll focus on the odds ratio.

251
00:26:19,570 --> 00:26:23,950
But we could have been using Delta as like we talked about in the sample size calculations,

252
00:26:23,950 --> 00:26:28,990
those standardized mean differences or correlation coefficients or some other measure.

253
00:26:32,690 --> 00:26:38,870
So we've gathered all this data. We're plucking out the standardized effect size from each of these studies.

254
00:26:39,140 --> 00:26:45,470
And now we've got to figure out, like, can we actually combine them or how good of how good is our meta analysis going to be?

255
00:26:45,770 --> 00:26:50,660
And again, the goal of this class is for you to start thinking about, well, where are the possible biases?

256
00:26:51,080 --> 00:27:00,830
And so if this was like a clinical trial, right, where or, you know, just this combination of studies, where can we get bias?

257
00:27:00,830 --> 00:27:04,610
Where can it pop up? Well, we're combining different trials.

258
00:27:04,610 --> 00:27:07,060
Perhaps the treatment regimens aren't exactly the same.

259
00:27:07,070 --> 00:27:13,910
Like maybe they're all studying the same chemotherapy, but maybe they actually give them chemotherapy at different doses or different schedules,

260
00:27:14,690 --> 00:27:19,280
maybe for difference amount of times like this one, six weeks. That study did it for eight weeks.

261
00:27:20,360 --> 00:27:23,870
This study used a combination of that chemotherapy plus something else.

262
00:27:23,870 --> 00:27:27,530
This one used it alone, right? The treatment regimens could be a little bit different.

263
00:27:27,890 --> 00:27:34,700
The control groups could also differ. Perhaps in some trials, it's placebo in other trials at some other standard of care.

264
00:27:37,310 --> 00:27:43,040
It could the patient eligibility could be different. So maybe in this study, it's women with early phase breast cancer.

265
00:27:43,040 --> 00:27:51,349
And in this study, it's women with late phase breast cancer. It may be in this other study, it included both women and men with breast cancer.

266
00:27:51,350 --> 00:27:57,889
Right. So that eligibility could be different. There could be different quality control of these studies.

267
00:27:57,890 --> 00:28:02,540
So there might be really well done. Randomized controlled trials and there could be.

268
00:28:05,500 --> 00:28:15,219
Like semi experimental designs or just like not as well-run trials that perhaps didn't collect all the

269
00:28:15,220 --> 00:28:21,940
follow up or didn't didn't really do a great job in terms of the study conduct or quality control.

270
00:28:23,080 --> 00:28:27,390
There could also be very much differences in terms of how long we followed up the the participants.

271
00:28:27,400 --> 00:28:33,219
So maybe this study was a huge one and it was a five year follow up and the study was an early phase study.

272
00:28:33,220 --> 00:28:35,170
And so it was only a two year follow up. Right.

273
00:28:35,170 --> 00:28:43,300
So you can imagine now we're trying to combine these studies and there are all these points at which bias can come in and these studies can differ.

274
00:28:43,540 --> 00:28:53,409
And yet we want to try to combine the information. One way in which we can try to deal with this is through subgroup analysis.

275
00:28:53,410 --> 00:29:00,760
So saying, well, if this group of studies was different because they had this characteristic, these were all the shorter term follow ups, right?

276
00:29:01,030 --> 00:29:04,809
We could do an analysis on just that subgroup and compare it to one.

277
00:29:04,810 --> 00:29:09,970
We do combine all of the studies or compare it to if we have all the longer term studies.

278
00:29:13,100 --> 00:29:17,090
Okay. So let's look at a very simple example.

279
00:29:17,960 --> 00:29:18,530
Again,

280
00:29:18,530 --> 00:29:28,250
we're looking at a binary or dichotomous outcome and we're thinking about the fact that think it's like response or non-response success or failure.

281
00:29:28,550 --> 00:29:33,200
And we have some treatment, some new treatment that we're interested in and some control here.

282
00:29:33,200 --> 00:29:34,700
We'll call it drug and placebo.

283
00:29:35,150 --> 00:29:41,210
And so we have a series of two by two tables, one for each study, and we can somehow get this information from the study.

284
00:29:42,770 --> 00:29:54,490
So. We have, right? These are the frequencies of the number of responses for the drug group, the number of non responses for the drug group,

285
00:29:54,700 --> 00:30:00,670
etc. So we can calculate the probability of response given the drug and the probability of response given the placebo.

286
00:30:03,010 --> 00:30:08,260
If we want to calculate the odds of response, right, this is P over one minus P.

287
00:30:08,590 --> 00:30:16,390
And so we can also calculate from this the odds of response for the drug and for the placebo.

288
00:30:17,140 --> 00:30:22,000
Ultimately, what we're really interested in is the odds ratio, which uses these two odds.

289
00:30:22,150 --> 00:30:27,160
Right? But here, let's look at a numerical example.

290
00:30:27,160 --> 00:30:36,040
This is data from two studies that was interested in investigating epithelial ovarian cancer risk for women who had zero term pregnancies.

291
00:30:38,260 --> 00:30:44,680
So it was if you've had a term pregnancy, it is one which goes over 20 weeks of gestation.

292
00:30:45,010 --> 00:30:50,320
And so zero term pregnancy is mean that you are never pregnant or you never had a pregnancy go beyond 20 weeks.

293
00:30:50,980 --> 00:30:58,960
So they are comparing the number of women who had cancer,

294
00:30:59,320 --> 00:31:06,490
if they had if they were never pregnant or had no term pregnancies versus if they had one or more term pregnancies.

295
00:31:06,910 --> 00:31:13,990
And so you can see that the odds of cancer with zero term pregnancies in study one was 0.33.

296
00:31:14,320 --> 00:31:22,270
Right. We're just taking this P over one minus P in those who have no term pregnancies.

297
00:31:22,630 --> 00:31:27,490
And the odds of cancer with zero term pregnancies is 0.53 in study two.

298
00:31:27,910 --> 00:31:36,069
Now, it's really hard to try to link these studies or to combine these studies if we're just looking at, well, what was it in zero term pregnancies?

299
00:31:36,070 --> 00:31:38,530
We also want to compare it to like the control group.

300
00:31:38,950 --> 00:31:43,179
And so we don't want to just think about the odds, we actually want to think about the odds ratio.

301
00:31:43,180 --> 00:31:45,540
So we'll see the odds ratio soon.

302
00:31:46,120 --> 00:31:52,959
But essentially, right, we're thinking about, okay, we are these two studies, they looked at similar they had a similar question.

303
00:31:52,960 --> 00:31:56,560
They looked at similar populations that a similar outcome.

304
00:31:56,560 --> 00:31:59,650
And we want to see if we combine these studies, what would we actually get?

305
00:32:01,430 --> 00:32:08,310
So hopefully all of you have remember the odds ratio 2651.

306
00:32:08,330 --> 00:32:15,570
You talk about it, right? So right in the odds ratio we now want to compare.

307
00:32:15,590 --> 00:32:24,770
Well, what are the odds of the outcome for the treatment group compared to the odds of the outcome for the control group?

308
00:32:25,170 --> 00:32:29,660
So you get the estimated odds within each group and we just divide by them.

309
00:32:31,160 --> 00:32:37,580
If the odds ratio is greater than one in this example, it means the drug was observed to be protective.

310
00:32:37,910 --> 00:32:41,380
If the odds ratio is less than one, then the drug was observed to be harmful.

311
00:32:41,390 --> 00:32:49,360
It's actually usually the other way around. And if the odds ratio is one right, we're not actually comparing the mean to zero.

312
00:32:49,420 --> 00:32:52,719
We don't have a mean, we have an odds ratio. So we're comparing the odds ratio to one.

313
00:32:52,720 --> 00:32:55,930
And if it's one, that means there's no difference in the treatments. Okay.

314
00:32:55,930 --> 00:33:01,060
So let's go back to our example, the odds ratio in study one.

315
00:33:01,360 --> 00:33:08,649
So the odds of cancer comparing the zero term pregnancies to the one or more term pregnancy is is 1.58 in study one.

316
00:33:08,650 --> 00:33:14,020
And the odds ratio for cancer in study two is 1.64.

317
00:33:14,020 --> 00:33:21,849
So they're really similar right now. We actually see that these studies, even though the odds if we go back here looks quite different,

318
00:33:21,850 --> 00:33:25,720
the odds of cancer looks quite different, .33 versus .53.

319
00:33:26,080 --> 00:33:30,819
The odds ratios are very similar. Okay.

320
00:33:30,820 --> 00:33:35,710
So if we want to try to now combine this information across the two studies,

321
00:33:36,010 --> 00:33:40,540
we have to ask ourselves for a few questions and go through several steps.

322
00:33:40,930 --> 00:33:44,020
So the first one is like, well, are they really measuring the same thing?

323
00:33:44,050 --> 00:33:47,080
Are these studies similar enough that we can combine them?

324
00:33:48,040 --> 00:33:55,900
And then if we feel like they are similar enough that we can combine them, then is this actually a significant treatment effect?

325
00:33:56,260 --> 00:34:03,520
Right. So how different is this odds ratio from one that would be like in any study?

326
00:34:03,570 --> 00:34:09,560
Right. That would be a question of interest. And so both in a meta analysis before we go there, we have to say, can we actually do this?

327
00:34:09,580 --> 00:34:10,930
Can we put these things together?

328
00:34:11,890 --> 00:34:18,060
And then the last question is, did we actually get all the studies or do we feel like there is some publication bias?

329
00:34:18,070 --> 00:34:22,990
Is there this other level of bias where only specific studies were published?

330
00:34:23,020 --> 00:34:29,640
Most often significant studies write studies that find a difference in treatments are published and those that don't find a difference,

331
00:34:29,650 --> 00:34:37,090
there's what's called the file drawer problem. The investigator says Nobody's going to publish my study because it wasn't significant.

332
00:34:37,090 --> 00:34:45,010
P wasn't less important. I'll just throw it away. Now we try not to have that in part of the clinicaltrials.gov, right?

333
00:34:45,010 --> 00:34:51,040
That registry was to get a you can't get away with that publication bias, but yet people still do.

334
00:34:53,150 --> 00:35:01,219
Okay. So we have all this data, we're trying to calculate the odds ratios for each study.

335
00:35:01,220 --> 00:35:06,290
This is probably already done right. Hopefully that was the finding and that's what we're trying to figure out.

336
00:35:06,560 --> 00:35:09,710
That was likely the finding in the paper and has already been done.

337
00:35:10,190 --> 00:35:13,879
What might be different? Some studies might have an unadjusted odds ratio.

338
00:35:13,880 --> 00:35:17,930
Some might have an adjusted odds ratio. Right. We have to consider how we're combining those.

339
00:35:20,600 --> 00:35:30,440
And then once we have all of these, the summary data across the studies, we'll try to combine them into an overall odds ratio and we can do that.

340
00:35:30,440 --> 00:35:34,700
I'm going to present two methods. One is called the Wolfe Method, what's called a mental pencil method.

341
00:35:34,700 --> 00:35:41,000
These are specific for binary data, right? Or data from a two by two or K by K table.

342
00:35:41,240 --> 00:35:49,850
If you're doing this for other outcomes, they might be similar, you know, like similar idea, but different specific methods that you use.

343
00:35:51,350 --> 00:35:55,639
Then we can perform this test of homogeneity of the odds ratio,

344
00:35:55,640 --> 00:36:00,740
like where are these odds ratios from all these studies actually similar or where they different?

345
00:36:01,940 --> 00:36:09,469
And then given that we have this overall odds ratio and if it's similar enough, we feel confident in it, was there actually a treatment effect?

346
00:36:09,470 --> 00:36:15,680
Is that odds ratio different from one? And then again, we'll check for publication bias using funnel plots.

347
00:36:18,280 --> 00:36:27,459
Okay. So this is showing you if we actually had like the cell data, right, how we could get the the odds ratio.

348
00:36:27,460 --> 00:36:30,970
And what we're going to do to combine it is we're actually going to work in the log scale.

349
00:36:31,720 --> 00:36:36,160
It's a natural log. Even though I'm saying log, I might actually change to Ellen later.

350
00:36:37,480 --> 00:36:46,420
But if we log the odds ratios and call this our our effective interest, this Tao hat, right for each study,

351
00:36:46,420 --> 00:36:53,080
the log odds ratio is actually if we consider all of them is normally distributed

352
00:36:54,160 --> 00:37:00,570
where the means are the log odds ratio and the variance is right for each study.

353
00:37:00,580 --> 00:37:04,030
The mean as log odds ratio in the variance is one over.

354
00:37:04,090 --> 00:37:08,350
It's a sum of one over each cell frequency.

355
00:37:08,770 --> 00:37:11,770
Right. These are just properties of.

356
00:37:13,000 --> 00:37:20,110
Binary binomial data essentially is where we're getting this information and the transfer transformation of it.

357
00:37:21,140 --> 00:37:25,030
Okay. So if I have my log odds ratios, I know it's normally distributed,

358
00:37:25,030 --> 00:37:33,519
so therefore I can use my usual kind of confidence interval calculation for a normally

359
00:37:33,520 --> 00:37:39,219
distributed data plus or -1.96 the mean plus or -1.96 times the standard deviation,

360
00:37:39,220 --> 00:37:44,230
and I can get those. Given that this is normally distributed, okay.

361
00:37:44,560 --> 00:37:52,180
Now in order to use this normal approximation, right, just like how can you use a chi square test versus a fisher's exact test?

362
00:37:52,480 --> 00:37:58,390
You have to have relatively large, but not not super large cell frequencies.

363
00:37:58,840 --> 00:38:08,320
Right. And it's not just the number in there. It's actually like the row columns in the call row and the column totals.

364
00:38:08,800 --> 00:38:12,760
Right. When you multiply them and divide, it have to be greater than five.

365
00:38:12,760 --> 00:38:19,089
So these this is essentially is like very similar to when you can use a chi square test versus a Fisher's exact test.

366
00:38:19,090 --> 00:38:25,810
It's just saying, like, right, we have to have enough data to use this like a normal approximation for binomial data.

367
00:38:26,670 --> 00:38:29,760
Okay. So here we are back to our two studies. Right?

368
00:38:29,780 --> 00:38:34,290
We actually have all the data, but again, we wouldn't necessarily have that from our meta analysis,

369
00:38:35,730 --> 00:38:44,040
but we can calculate the log odds ratio that's 0.457 and we can calculate the.

370
00:38:46,370 --> 00:38:53,870
That's a variance that goes along with it. Right. So for this one study, the second study, again, we can see here that these are quite similar.

371
00:38:54,290 --> 00:38:59,270
The odds ratios were very close to the log odds ratio and their variances are very close.

372
00:38:59,660 --> 00:39:02,150
And from this, we can calculate the confidence intervals.

373
00:39:02,780 --> 00:39:12,380
And so we can do that for the log odds and then we can exponentially get it right, put it back in the odds scale.

374
00:39:12,620 --> 00:39:16,390
So for the first study, right, this is just barely not significant.

375
00:39:16,400 --> 00:39:24,620
That odds ratio, remember, it was like one point something, 1.58 and the confidence interval spans one.

376
00:39:24,980 --> 00:39:29,180
So this one study found that the odds ratio was greater than one.

377
00:39:29,180 --> 00:39:36,739
However, the confidence interval includes one. This other study had a similar odds ratio, but the confidence interval was higher than one.

378
00:39:36,740 --> 00:39:41,270
Right. So they showed some significant difference here. Okay.

379
00:39:41,270 --> 00:39:48,110
So if we want to combine these two odds ratios, like I said, there are these two most commonly used methods.

380
00:39:48,110 --> 00:39:53,770
This Wolfe method is a weighted average of the study specific odds ratios.

381
00:39:53,780 --> 00:39:57,890
So you might just say, well, let's just take the two odds ratios and together divide by two.

382
00:39:57,920 --> 00:39:58,880
Right? We're good.

383
00:39:59,330 --> 00:40:07,790
But the reason why we don't want to say, well, let's just count them equal, right, is the fact well, this study had more people than this study.

384
00:40:08,030 --> 00:40:11,839
Right. And so therefore, we probably believe that estimate it was more precise.

385
00:40:11,840 --> 00:40:15,440
Right. There was something about that study that was likely.

386
00:40:17,250 --> 00:40:21,329
It was. It was. Not necessarily better done,

387
00:40:21,330 --> 00:40:30,600
but there are more people that give us that data and so likely we should wait that more than the other study and or not necessarily just on the size,

388
00:40:30,600 --> 00:40:34,530
but actually based on the variability. So the more variable the study rate,

389
00:40:34,530 --> 00:40:39,719
the less we want to wait it because the less we believe in those results and less variable or more precise the study,

390
00:40:39,720 --> 00:40:42,540
the more we want to wait that study, the more we believe in those results.

391
00:40:43,020 --> 00:40:51,990
So this Wolfe method, inversely, it's going to use that inverse weight related to the variability of the trial specific estimates.

392
00:40:52,350 --> 00:40:59,459
So for example, we'll use this W as a weight. So if any of you have heard of like inverse probability treatment weighting,

393
00:40:59,460 --> 00:41:03,570
this is related or propensity score weighting, this is related to that, right?

394
00:41:03,570 --> 00:41:09,030
Where we're going to say, well, here's the odds ratio and I'm going to put this much weight on it.

395
00:41:09,030 --> 00:41:14,100
In fact, I'm going to weight it inversely proportional to the amount of variability in the study.

396
00:41:14,250 --> 00:41:17,610
So the weight is the inverse of the variance in that study.

397
00:41:19,020 --> 00:41:21,200
And so I take the weight, I multiply it by the logs,

398
00:41:21,210 --> 00:41:27,330
log odds ratio divide by and some of those up over the studies and divide by the sum of the weights.

399
00:41:29,000 --> 00:41:41,660
And then I can figure out the 95% confidence interval for the common log odds ratio, and then I can transform that back to the odds ratio scale.

400
00:41:43,820 --> 00:41:54,440
Okay. So for example here. Right. I have my log odds ratios and I have my variance and I'm inverting it to get the weight variable.

401
00:41:54,440 --> 00:41:59,089
And so I just take my weight times the log odds ratio and study one plus my weight from

402
00:41:59,090 --> 00:42:03,700
study two times the log odds ratio and study two and divide it by the sum of the weights.

403
00:42:03,710 --> 00:42:11,120
And so now I have this weighted average that depends upon the variability in the studies and you can see

404
00:42:11,120 --> 00:42:17,060
that it's pretty close to just an average of these because these were the variances were so similar.

405
00:42:17,480 --> 00:42:26,450
But if these studies actually had quite different variability, right, you would see it go more toward whichever study was less variable.

406
00:42:27,200 --> 00:42:34,400
Okay. So overall, I get this overall weighted average of my odds ratio, which is 1.61.

407
00:42:34,910 --> 00:42:40,640
Remember, if I looked back at my individual studies, I'd 1.58 and 1.64.

408
00:42:41,030 --> 00:42:48,620
And so I find that my odds ratio combined across the two studies is 1.61.

409
00:42:49,610 --> 00:42:55,850
Right. This looks simple one, because these are only two studies and two because these studies had very similar findings.

410
00:42:56,420 --> 00:42:59,870
But you can imagine how much more complex this gets with a lot more studies

411
00:42:59,870 --> 00:43:04,220
and a lot of studies that could be quite different or have different results.

412
00:43:04,700 --> 00:43:09,410
Okay, here is my confidence interval. It's 1.17 and 2.22.

413
00:43:09,770 --> 00:43:17,870
So I have a tighter confidence interval around this overall estimate than I had for each individual study.

414
00:43:18,440 --> 00:43:22,310
And it's in this case, right? It's showing that it's significant.

415
00:43:22,320 --> 00:43:26,899
So it's saying, well, while this one study didn't see the significant effect, when I combine them,

416
00:43:26,900 --> 00:43:31,760
I have enough evidence to see that there is an effect such that women who never had a

417
00:43:31,760 --> 00:43:37,520
term pregnancy have odds of developing ovarian cancer that are between 1.17 and 2.2,

418
00:43:37,520 --> 00:43:42,500
two times greater than the odds of women who had greater than or equal to one term pregnancy.

419
00:43:43,340 --> 00:43:52,310
And again, this is a significant increase in terms of the odds of ovarian cancer for those women who haven't had term pregnancies.

420
00:43:53,900 --> 00:43:57,290
Okay. Now, I could also use this other method, the mental HENZL method.

421
00:43:57,620 --> 00:44:04,970
It's relatively similar. This is a slightly more stable estimate here for the common odds ratio, and so it's often used.

422
00:44:06,560 --> 00:44:09,260
When I say stable, it means that it remains valid.

423
00:44:09,260 --> 00:44:17,569
Even when the number of strata so late, the number of studies is large and you don't have to satisfy those like table counts,

424
00:44:17,570 --> 00:44:26,330
the common row totals being greater than five. So you could have some studies that have, you know, some rare events or smaller counts.

425
00:44:26,330 --> 00:44:30,290
And you can combine them and you can do this across many strata.

426
00:44:31,460 --> 00:44:37,880
The wolf estimate is based on asymptotic, right? So we require it's not quite as stable as sample size can be small.

427
00:44:37,880 --> 00:44:43,010
And some studies of mental hassle style confidence intervals are not simple to do by hand.

428
00:44:43,010 --> 00:44:46,700
So you do software with either of these methods.

429
00:44:46,700 --> 00:44:52,870
With more than two studies, you use software anyway. Even with two studies, I'm sure you do software anyway.

430
00:44:52,880 --> 00:44:56,780
Here is the example of using the mental HENZL estimator.

431
00:44:57,740 --> 00:45:03,700
So it's. Combining. If we look here, right.

432
00:45:03,700 --> 00:45:06,700
It's this t is like a total.

433
00:45:08,030 --> 00:45:11,710
So it's looking at the totals. Oops.

434
00:45:14,480 --> 00:45:21,430
I got books. Here we go. Okay. So you can see it's this a one times candy one, right?

435
00:45:21,440 --> 00:45:25,000
This is a, B, C, D divided by this total.

436
00:45:25,010 --> 00:45:34,130
So for the mental console, you may need to have actual count information and or have to.

437
00:45:35,300 --> 00:45:39,230
That calculate the account data in order to to use this and combine.

438
00:45:39,570 --> 00:45:42,680
Anyway, in this example, it actually made no difference what method you used.

439
00:45:42,680 --> 00:45:46,520
Again, because it's two studies and the estimates were so similar to begin with.

440
00:45:47,570 --> 00:45:52,550
But if we had more than two studies and some studies had small sample size,

441
00:45:53,210 --> 00:45:57,070
the mental Hansell estimate, it could have been somewhat different than the wolf estimate.

442
00:45:58,970 --> 00:46:02,840
Okay, so we got an overall odds ratio and confidence interval.

443
00:46:02,840 --> 00:46:07,850
And now I'd say like, was that a good idea to combine those things and get this odds ratio?

444
00:46:08,120 --> 00:46:11,540
Or are these are these studies not very similar?

445
00:46:12,350 --> 00:46:17,209
So if you remember in linear regression, right.

446
00:46:17,210 --> 00:46:24,070
One of the assumptions is. Homogeneity of the residuals, right?

447
00:46:24,730 --> 00:46:28,090
Here we have homogeneity of these studies.

448
00:46:28,240 --> 00:46:34,719
So we're trying to say that across the studies, we want to see that the estimates were relatively similar,

449
00:46:34,720 --> 00:46:40,000
that we are and this is some sort of like surrogate for saying that we were measuring the same thing.

450
00:46:40,330 --> 00:46:44,380
We have similar similar populations, etc.

451
00:46:45,610 --> 00:46:51,180
So we're we're trying to see are all of these different odds ratios from the studies were combining the same or are they not?

452
00:46:51,190 --> 00:46:54,310
And you can imagine. Right, we don't expected them to be exactly the same.

453
00:46:54,670 --> 00:46:59,770
It might be a little bit of different, but if they're wildly different, then we're probably don't have homogeneous studies.

454
00:47:01,060 --> 00:47:06,219
So this test of homogeneity, the test statistic is some again,

455
00:47:06,220 --> 00:47:14,770
kind of like weighted difference between each study's log odds ratio and the overall mean log odds ratio.

456
00:47:14,810 --> 00:47:18,969
So the reason why we had to calculate that overall mean log odds ratio first is

457
00:47:18,970 --> 00:47:25,810
because we have to use that in this calculation of our test statistic of homogeneity.

458
00:47:26,170 --> 00:47:34,900
So, right, it's kind of like the residual of how different is each study from the average odds ratio across all of the studies?

459
00:47:38,340 --> 00:47:41,520
If we reject this null hypothesis that these are the same,

460
00:47:41,940 --> 00:47:47,849
then the meta analysis is trying to combine interventions or outcomes or populations that are just too different.

461
00:47:47,850 --> 00:47:51,870
And so we don't feel very like we have very robust results.

462
00:47:54,690 --> 00:47:58,020
This test is not one of is this odds ratio significant?

463
00:47:58,050 --> 00:48:01,440
Right. This test is. Are these studies similar?

464
00:48:02,980 --> 00:48:08,430
Okay. So we have these two studies. We can use this test of homogeneity, right.

465
00:48:08,450 --> 00:48:16,500
It's this weighted average of essentially the residual or how different each study is from that overall mean odds ratio or long odds ratio.

466
00:48:17,730 --> 00:48:22,740
We find that we have a P value that's greater than point nine.

467
00:48:23,130 --> 00:48:28,500
Right? So we don't rejects our our null, meaning that we feel like these studies are quite similar.

468
00:48:28,980 --> 00:48:33,570
I think from eyeballing the studies and the odds ratios, we probably would have said this right.

469
00:48:33,570 --> 00:48:42,110
But this gives us the statistic to do that. I have reviewed many a met analysis for journals and almost always there is

470
00:48:42,110 --> 00:48:48,290
significant heterogeneity between studies because studies are often not very similar,

471
00:48:48,290 --> 00:48:50,540
even though we're trying to look at the same question.

472
00:48:50,930 --> 00:49:01,159
So a lot of times they they do this test of homogeneity and they also try to summarize the heterogeneity with this I squared statistic,

473
00:49:01,160 --> 00:49:02,330
which we'll talk about in a second.

474
00:49:04,220 --> 00:49:13,940
And they try to give reasons for why there might be there might be heterogeneity between the studies, but then they still try to make claims.

475
00:49:14,240 --> 00:49:22,270
Right. And we have to be very careful about any claims or results made if we actually see this heterogeneity across the studies.

476
00:49:22,730 --> 00:49:26,000
And this is where often they'll say, oh, there is heterogeneity.

477
00:49:26,000 --> 00:49:30,140
So now actually we're going to look at this subgroup of studies. This subgroup of studies seem more similar.

478
00:49:30,530 --> 00:49:34,759
And actually, their test for homogeneity is non-significant.

479
00:49:34,760 --> 00:49:39,809
And so we feel better in this estimate. And then, oh, look, this estimate is actually similar to the overall estimate.

480
00:49:39,810 --> 00:49:43,820
And so now we feel that that's often what's done in these publications.

481
00:49:44,180 --> 00:49:51,320
But on average, I would say that if you look across all the studies, there is often high heterogeneity.

482
00:49:52,250 --> 00:49:59,930
And so that's why we often see these subgroup analyzes other results having two different estimations.

483
00:50:00,440 --> 00:50:06,290
So is it common to have two different estimations? And then how do you decide your test statistic?

484
00:50:06,290 --> 00:50:13,820
Because it relies on yours. So we did we had two different methods to find tlhat, as we talked about the Wolfe method in the mental HENZL.

485
00:50:14,240 --> 00:50:17,370
In our case, they both agreed you would have just chosen one.

486
00:50:17,390 --> 00:50:22,459
Like when you do this study, you would say, I'm using the middle pencil because it's most stable,

487
00:50:22,460 --> 00:50:26,390
or I'm using the Wolfe method because I don't have any problems with small sample size.

488
00:50:28,550 --> 00:50:33,770
If that's the case, they almost always work out to the same, unless there's issues with the sample size.

489
00:50:34,010 --> 00:50:37,520
So you just use one of those methods and then you'd move forward.

490
00:50:37,550 --> 00:50:43,370
Use that tell. Okay.

491
00:50:43,790 --> 00:50:52,940
So. So this is this this test of homogeneity.

492
00:50:54,170 --> 00:50:57,380
This is not a test of is the odds ratio significant?

493
00:50:57,410 --> 00:51:03,440
It's also not a test of a specific treatment by trial interaction test.

494
00:51:03,530 --> 00:51:07,700
Like does the treatment effect differ for each trial?

495
00:51:08,170 --> 00:51:11,480
Right. That's not this test. That's a different test that requires more power.

496
00:51:14,180 --> 00:51:19,480
It's a generic test that says there's something different across these studies.

497
00:51:19,490 --> 00:51:23,450
It's not necessarily saying it's the treatment.

498
00:51:23,450 --> 00:51:26,840
Right. It could be the population. It could be the follow up. It could be whatever.

499
00:51:27,170 --> 00:51:30,860
Right. And so it's it's not the specific interaction test.

500
00:51:33,130 --> 00:51:42,060
Okay. So once if we find or even if we don't find if we find homogeneity or if we find heterogeneity, we usually don't want to quantify.

501
00:51:42,070 --> 00:51:47,229
Well, like, how different are they? Not just they're significantly different or significantly not different.

502
00:51:47,230 --> 00:51:50,860
We want to say, well, what's some sort of quantitative measure of the difference?

503
00:51:51,220 --> 00:52:03,580
And so we use this often this I squared is used, it's called the homogeneity statistic and it's this weighted effect size right here.

504
00:52:03,580 --> 00:52:12,160
Our effect size was that how you can just replace effect size, but it would be Taos where a weighted sum weighted effect size,

505
00:52:13,300 --> 00:52:18,340
it depends on the degrees of freedom, which is like how many studies there were.

506
00:52:18,820 --> 00:52:23,800
Right. And it gives us this percentage quantitative value.

507
00:52:24,040 --> 00:52:34,570
And so it's basically kind of trying to tell us, well, how much variation is there across studies beyond what we would expect just by chance.

508
00:52:34,600 --> 00:52:40,540
So if this I squared is less than 40%, we might say, well, yeah, they're different, but it's not that different.

509
00:52:40,780 --> 00:52:50,560
You know, it's not too bad if it's somewhere 40 to 60 or 30 to 60, we might say, okay, they are different, but it's just moderate heterogeneity.

510
00:52:51,370 --> 00:52:56,949
50 to 90 is substantial heterogeneity and maybe 75, you can see these all overlap, right.

511
00:52:56,950 --> 00:53:01,570
But because there are sort of different opinions, but obviously the higher you get to 100,

512
00:53:01,900 --> 00:53:07,180
the wildly more different these studies are and the closer you are to zero, the more similar other studies are.

513
00:53:07,450 --> 00:53:11,739
And so people might say, right, we rejected the test statistic that these studies are different.

514
00:53:11,740 --> 00:53:19,300
However, our I squared is 30%. So I'm not that worried about we can still feel okay in our results and often that happens to.

515
00:53:27,400 --> 00:53:30,910
So if we do reject that test of homogeneity,

516
00:53:31,150 --> 00:53:40,570
then we're saying that that common effect size estimate that Towle is not a good summary across all of our studies.

517
00:53:40,720 --> 00:53:44,500
It's not a good descriptor of the distribution of these effects sizes.

518
00:53:46,180 --> 00:53:51,430
So what we can do is we can fit a model.

519
00:53:52,030 --> 00:53:56,130
We can try to model that between study differences. What is it that differs between these studies?

520
00:53:56,140 --> 00:54:00,160
Is it the population? Is it the specific treatment?

521
00:54:00,160 --> 00:54:06,130
Is it the follow up? Is it the setting, the geographic setting of these these studies?

522
00:54:06,160 --> 00:54:13,510
Is it the year the studies were published? Or we consider random effects model that we expect these treatment effects to

523
00:54:13,510 --> 00:54:18,130
be somewhat different and try to account for it in the random effects model.

524
00:54:19,570 --> 00:54:29,410
So. 653 Right. Talks about random effects modeling and whenever if you read a meta analysis paper,

525
00:54:29,830 --> 00:54:38,680
most often they will use a random effects analysis of the to summarize the results.

526
00:54:38,680 --> 00:54:44,200
So often they're not going to use this like simple mental HENZL method of the summary.

527
00:54:44,200 --> 00:54:46,920
They're going to do actually a random effects model.

528
00:54:46,930 --> 00:54:54,970
So if you remember or to remind you a fix effects model assumes that all the variability between effect sizes is due to sampling error.

529
00:54:55,930 --> 00:54:59,460
And so it weights each study by the inverse of the sampling variance.

530
00:54:59,470 --> 00:55:03,610
That's like just doing the simple mental head soul or wolf test, right?

531
00:55:04,120 --> 00:55:08,709
But a random effects model assumes that the variability between effect sizes is due to the sampling error,

532
00:55:08,710 --> 00:55:12,600
plus some variability of this population of effects.

533
00:55:12,850 --> 00:55:18,550
There are unique differences. We expect there to be some heterogeneity because these are different studies.

534
00:55:19,630 --> 00:55:29,700
And so we want to try to be able to. So my lab with the random effects we allow for that other type of variability in studies.

535
00:55:30,060 --> 00:55:34,379
And so the weights in each study are not only the inverse of the sampling variance,

536
00:55:34,380 --> 00:55:39,930
but also a constant that represents the variance, the variability across the main components.

537
00:55:41,040 --> 00:55:48,150
So we have not just this sampling variance, but we also allow for this other type of variability.

538
00:55:49,590 --> 00:55:53,249
And in a meta analysis it says this variance component is based on.

539
00:55:53,250 --> 00:55:58,250
Q if you remember. Q Is this like weighted sum of the effect sizes?

540
00:56:00,990 --> 00:56:05,700
So again, another way to think about fixed effects is that it says that all studies are

541
00:56:05,700 --> 00:56:11,640
estimating the same true effect and the variability is only because we've differed.

542
00:56:12,030 --> 00:56:17,159
We've sampled different people from each study. The real effects is going to say, well,

543
00:56:17,160 --> 00:56:23,280
each study could actually have a somewhat different underlying true effects because these have different studies.

544
00:56:23,790 --> 00:56:34,110
Right? But we can try to quantify that variation across the studies so we can have variation between the studies and within the studies.

545
00:56:36,000 --> 00:56:38,069
The biggest difference in terms of the findings,

546
00:56:38,070 --> 00:56:44,790
if you have a fixed effects model versus a random effects model, you usually have the same effect size, right?

547
00:56:44,790 --> 00:56:50,189
The same effect point estimate, but you get different confidence intervals and the confidence intervals are going

548
00:56:50,190 --> 00:56:53,850
to be larger in random effects because you're allowing for more variability.

549
00:56:56,820 --> 00:57:01,049
So we call because the confidence intervals are going to be larger, right?

550
00:57:01,050 --> 00:57:07,080
The Grano effects model is more conservative because it might be that you find something significant in a fixed effects model.

551
00:57:07,470 --> 00:57:13,590
So smaller confidence intervals, but you wouldn't find that in a random effects model because we've accounted for this actual level of variability.

552
00:57:14,670 --> 00:57:22,150
In almost any meta analysis that you read today, you will find that they use a random effects model.

553
00:57:22,530 --> 00:57:29,340
It'd be very rare to see a fixed effects model used if it was done well.

554
00:57:30,120 --> 00:57:33,209
And usually these will have very similar findings.

555
00:57:33,210 --> 00:57:37,080
Right. But the the random effects will be more conservative.

556
00:57:38,820 --> 00:57:46,890
My SO my mother in law is actually a professor of psychology and did a lot of work in meta analysis.

557
00:57:46,920 --> 00:57:56,370
She's really interested in the lack thereof of gender differences in stem cell stem subjects, particularly math.

558
00:57:57,330 --> 00:58:04,260
So she did a lot of meta analyzes, looking at studies, looking at differences between boys and girls in elementary school, I think in middle school.

559
00:58:04,770 --> 00:58:13,679
Anyway, she when I talk to her about meta analysis, you know, dinner, dinner conversations with my mother was always very educational.

560
00:58:13,680 --> 00:58:20,669
She, like, unable to stop talking about work. Oh, I'm like, my parents were like, Oh, are you the vice dean of your school?

561
00:58:20,670 --> 00:58:24,250
If I can tell you something, no idea what academia's.

562
00:58:24,360 --> 00:58:31,200
Anyway, she she would always use fixed effects models because she was doing this like 30, 40 years ago.

563
00:58:31,200 --> 00:58:37,319
And it's sort of like the norm and what people thought was okay.

564
00:58:37,320 --> 00:58:41,820
And they didn't really they weren't using random effects models at this point.

565
00:58:41,820 --> 00:58:49,830
Everybody's using random effects models. It'd be quite rare to see to read a methodology is that use fixed effects models unless it's like in some.

566
00:58:52,470 --> 00:58:57,750
Not high quality journal. So if you ever do read them an analysis and you see a fixed effects model,

567
00:58:57,750 --> 00:59:02,160
just be somewhat wary and see why they, you know, think about why did they use that.

568
00:59:03,630 --> 00:59:07,370
Again, we're almost always going to see rain with models. Okay.

569
00:59:08,670 --> 00:59:15,770
So again, these are all the different excuse me, places for bias to come in.

570
00:59:15,780 --> 00:59:21,840
So it could be different study designs. Maybe one's a two hour, maybe one's a three arm, maybe one's a crossover, one's a parallel design.

571
00:59:23,430 --> 00:59:26,639
There could be different incidence rates among exposed frontlines.

572
00:59:26,640 --> 00:59:30,000
A follow up. Different distributions of effect modifiers.

573
00:59:30,000 --> 00:59:37,340
Right. So these are the other variables that are potential confounding variables or interacting variables of the treatment effect.

574
00:59:39,390 --> 00:59:47,010
There can be different statistical methods or models use. Like I said, maybe some some publications will present the unadjusted results.

575
00:59:47,010 --> 00:59:52,260
Some will present unadjusted and adjusted will only prevent present adjusted.

576
00:59:53,430 --> 01:00:00,900
And so there's lots of different sources of bias. That could be why these studies are different.

577
01:00:01,560 --> 01:00:05,190
Also, there's can be very much difference in study quality.

578
01:00:05,850 --> 01:00:09,450
And there's actually, again now if you read good meta analyzes,

579
01:00:09,720 --> 01:00:13,800
they go through there's always a table that shows like all of the studies that they used.

580
01:00:14,250 --> 01:00:22,380
And most often now there's been a fair amount of work on how you can go across each study and rates it's study quality.

581
01:00:22,830 --> 01:00:31,960
And so there then these other tables that they will have on each study where they'll go through these different like you know, was it randomize?

582
01:00:33,030 --> 01:00:38,579
Did they control for variable? There are all these different topics and they usually either have a color coded.

583
01:00:38,580 --> 01:00:44,340
So it's like green is good and red is bad or they have like smiley faces or sad faces

584
01:00:45,180 --> 01:00:48,690
or they might put a number on it and they try to rate the quality of these studies.

585
01:00:48,690 --> 01:00:52,650
And so you can get this visual sort of like heatmap of the quality of the studies.

586
01:00:53,640 --> 01:00:59,820
And you obviously want to see higher quality studies and the lower quality study is across the bed analysis.

587
01:01:01,590 --> 01:01:10,920
Okay. So if we've decided that these studies are homogeneous and or they're heterogeneous but have a low level of heterogeneity,

588
01:01:12,900 --> 01:01:16,520
then we can go ahead and test for an overall treatment effect.

589
01:01:16,530 --> 01:01:17,520
So we have our towel.

590
01:01:17,880 --> 01:01:29,250
It's either from a fixed effects model or like a simple combination method and or most likely we've gotten a towel from our random effects model.

591
01:01:29,610 --> 01:01:34,800
And now we can say, well, is this odds ratio significantly different from one across these studies?

592
01:01:35,130 --> 01:01:40,680
We already know this answer right from our example because we calculated the confidence interval.

593
01:01:43,110 --> 01:01:47,740
Like for at least from the simple methods.

594
01:01:47,760 --> 01:01:48,100
Right.

595
01:01:48,120 --> 01:01:54,930
We already see that obviously this is going to be significantly different from one because we have our confidence level that does not include one.

596
01:01:56,900 --> 01:02:01,460
But we can do this. This test statistic, it's a chi squared test statistic.

597
01:02:02,870 --> 01:02:11,329
And we can find here that we have a very small p value such that our confidence interval.

598
01:02:11,330 --> 01:02:16,040
Right was not was telling us the right thing to think before I had this is 1.17.

599
01:02:17,420 --> 01:02:23,030
But after adjusting for the study, women who had never had a term pregnancy have significantly higher odds of developing

600
01:02:23,030 --> 01:02:28,010
epithelial ovarian cancer than women who have had one or more term pregnancies.

601
01:02:30,530 --> 01:02:33,580
Right. And ultimately, from this just goes through the calculations.

602
01:02:33,590 --> 01:02:36,919
Ultimately, this was the goal of the meta analysis, right.

603
01:02:36,920 --> 01:02:42,950
To say, well, this study found this and the study found this and we put it together and we have this better robust evidence.

604
01:02:43,070 --> 01:02:52,730
Right. This more robust evidence of this is the actual effect of having pregnancies on cancer for not having pregnancy, lung cancer.

605
01:02:53,150 --> 01:02:59,780
Okay. Now, this is a sad for us plot because we only had two studies, but usually these forest plots look much bigger, much more exciting.

606
01:03:00,080 --> 01:03:05,510
And what they do is that it's a visual representation of all of the study effects and then the summary.

607
01:03:05,810 --> 01:03:11,210
And so it's you'd always see like a line or a dot on the point estimate,

608
01:03:11,480 --> 01:03:15,350
and then you'll see bars around the confidence interval of the treatment effect for each study.

609
01:03:15,650 --> 01:03:19,700
And at the very bottom, you're going to see the total or the summary effect.

610
01:03:19,700 --> 01:03:22,850
And so. Right, you can see that the first study spanned one.

611
01:03:22,970 --> 01:03:26,060
The second study did not. And a meta analysis.

612
01:03:26,060 --> 01:03:32,810
Right. Brought it together, provided a more precise estimate of the treatment effects, because we had both studies together.

613
01:03:35,750 --> 01:03:42,120
If you're interested in doing meta analysis or you find yourself in the position looking at met analyzes in the future.

614
01:03:42,140 --> 01:03:45,410
There are two main packages and are the two main analyzes.

615
01:03:45,410 --> 01:03:52,210
The first is meta and the other is metaphor, and you just want to look at the documentation for why you might use one over the other.

616
01:03:52,220 --> 01:03:56,300
There's a lot of overlapping abilities of these of these packages.

617
01:03:58,610 --> 01:04:05,650
So this would be there would be so much work before you get to this point of being able to actually use the are packages, right?

618
01:04:05,780 --> 01:04:12,820
Like all of the the processing of that data, it takes a very long time before you can actually start to put it in.

619
01:04:12,830 --> 01:04:14,330
Ah but for example.

620
01:04:14,570 --> 01:04:25,340
Right this would be how we can, how we could get that using this meta package if we had the cell specific data from these two studies.

621
01:04:29,170 --> 01:04:34,810
And so this in the metal package, we use this metal bin because it's a binomial outcome.

622
01:04:35,740 --> 01:04:41,560
And so we have our events, we have the data set, we're telling it to use the mental pencil method,

623
01:04:42,430 --> 01:04:47,130
and we say that we want an odds ratio and so we can get the outputs.

624
01:04:47,530 --> 01:04:53,440
So this is for each study. And then it shows us if we use a fixed effects model or a random effects model.

625
01:04:53,830 --> 01:04:58,600
Here is our output. Again, here, it's these for the fixed effects and the random effects.

626
01:04:58,600 --> 01:05:01,870
They're very similar at the exact same point estimate.

627
01:05:01,870 --> 01:05:05,890
And then it's actually almost the same confidence level because we only have two studies.

628
01:05:06,430 --> 01:05:10,719
So there's not a lot of random effects that we were able to capture. Right. So that's why we're not seeing a difference here.

629
01:05:10,720 --> 01:05:18,160
But if we had lots more studies, you could see a difference between these models and you generally want to choose the random effects model.

630
01:05:18,520 --> 01:05:27,100
It'll also output the tests of heterogeneity. And so here again, it tells us, okay, these are not significantly heterogeneous studies.

631
01:05:32,700 --> 01:05:41,999
There's also this metaphor package. This has this arm a m H.

632
01:05:42,000 --> 01:05:50,130
So it's going to use the mental pencil method and it's actually just like putting in all of the different study data.

633
01:05:50,480 --> 01:05:54,120
And this is actually all the different cells from the two studies.

634
01:05:58,650 --> 01:06:05,660
And this will also this outputs the fixed effects model, the test for heterogeneity.

635
01:06:05,670 --> 01:06:09,030
It outputs your model results on the odds ratio scale.

636
01:06:09,030 --> 01:06:19,169
And then it will give you actually a different test for heterogeneity the Tyrone's test for heterogeneity and it gives you the the is

637
01:06:19,170 --> 01:06:29,399
the odds ratio is significantly different from one test as well but it's basically similar kind of output from the other package.

638
01:06:29,400 --> 01:06:37,380
Just you put it in a different way and you see it in a different you see it differently in this package.

639
01:06:39,040 --> 01:06:42,460
Okay. The metaphor package can also make these forest plots.

640
01:06:43,810 --> 01:06:49,180
This is like a really sad plot. I'm sure you can use juju plot to make a really much nicer looking forest plot,

641
01:06:49,180 --> 01:06:54,610
but and maybe, maybe this metaphor package uses it makes better looking plots these days.

642
01:06:55,090 --> 01:07:01,270
But when I did this, I don't know, three years ago, this is the the little forest plot from the metaphor package.

643
01:07:02,140 --> 01:07:08,290
And this is like you would almost always see these forest plots in the publication, the meta analysis publication.

644
01:07:09,700 --> 01:07:13,610
And that's where the reader is going to go to like find the results at the end.

645
01:07:14,050 --> 01:07:17,380
This one, the fact that this is again destroyed in the sex effects model,

646
01:07:17,860 --> 01:07:22,860
I'm sure you could ask it to do the random effects model instead of showing like

647
01:07:22,870 --> 01:07:28,360
one dot in the line is showing a diamond that goes across the confidence interval.

648
01:07:28,720 --> 01:07:31,750
And here you can see that this is also usual the.

649
01:07:33,200 --> 01:07:37,010
Whatever this point is, you're right here. It's a square, right? Or it could be a circle, whatever.

650
01:07:37,310 --> 01:07:41,690
The larger it is, it means the larger size of the study.

651
01:07:43,160 --> 01:07:46,850
So often the point, estimate, picture or figure.

652
01:07:46,850 --> 01:07:52,490
We'll show you something about the size of the study or like the weight of the study.

653
01:07:55,140 --> 01:08:00,360
Okay. So we were doing that based on odds ratios.

654
01:08:01,020 --> 01:08:01,829
But like I said,

655
01:08:01,830 --> 01:08:12,270
if you if you have like standardized mean differences or differences in sample proportions or regression coefficients or or correlations,

656
01:08:12,540 --> 01:08:18,720
you can also use similar type methods. They just differ a little bit based on the distribution of the data or the type of the data.

657
01:08:20,610 --> 01:08:27,450
Okay. So then the next part is, do we have a good representation of the studies that actually exist out there?

658
01:08:27,810 --> 01:08:33,200
Is there a publication bias? And so, again, in a meta analysis, you'd always want to see them.

659
01:08:33,360 --> 01:08:41,730
The presenters talk about this if there's evidence for publication bias or not.

660
01:08:42,810 --> 01:08:47,720
So this the funnel plot assumes that the largest studies will be near the average, right.

661
01:08:47,730 --> 01:08:56,310
It will be near that towel that you estimated and the small studies will be spread equally spread on either side of that overall estimate.

662
01:08:58,080 --> 01:09:06,660
And if you don't see this spread, then you can kind of you feel like you might have publication bias.

663
01:09:07,830 --> 01:09:12,080
So essentially, it's called a funnel plot because it's supposed to be a symmetric, inverted funnel.

664
01:09:12,090 --> 01:09:18,749
It's essentially a triangle that you want to see where your tail right is at the top.

665
01:09:18,750 --> 01:09:27,030
And your other studies I'll show you a picture are all across the bottom like well or across the triangle, across both sides.

666
01:09:27,660 --> 01:09:34,559
If you have an asymmetrical funnel, then you potentially have publication bias or systematic differences between

667
01:09:34,560 --> 01:09:39,360
smaller and larger studies or potentially an inappropriate effect measure.

668
01:09:39,660 --> 01:09:44,580
And so if you find after all of this that there appears to be publication bias,

669
01:09:44,580 --> 01:09:52,290
then this again leads to doubts over the appropriate of a simple meta analysis and says you should look more into why you're finding this.

670
01:09:55,120 --> 01:09:58,870
The funnel plot can have a variety of choices for measuring the study's size.

671
01:09:58,870 --> 01:10:05,889
Writing could actually just be the total sample size. It could also be related to the variability, the standard error, or the variance.

672
01:10:05,890 --> 01:10:09,700
It could be based on the actual variance or inverse of the variance.

673
01:10:10,000 --> 01:10:15,819
This weight in most people said that we shouldn't just use the total sample size.

674
01:10:15,820 --> 01:10:25,630
Right, because it could be a large study, but it could somehow still have a higher variability because of heterogeneity between subjects or something.

675
01:10:26,230 --> 01:10:30,040
And so they say we should use the standard error. Okay.

676
01:10:30,040 --> 01:10:35,230
So here's an example of a funnel plot and this is a relatively good funnel plot.

677
01:10:35,710 --> 01:10:45,790
So again, is saying that like up here is where we expect the right this line is showing where our overall effect, our tail is.

678
01:10:46,180 --> 01:10:51,190
And in all of the studies that were combined into that tower are by these dots.

679
01:10:51,610 --> 01:11:04,210
And we want to see them relatively scattered across these this this line here and within this triangle or this funnel.

680
01:11:04,390 --> 01:11:10,330
So there's one on the outside here, and there maybe are more on this side, right on the left than on the right.

681
01:11:11,320 --> 01:11:15,460
But it's not it's not too terrible.

682
01:11:15,760 --> 01:11:21,190
This doesn't look overall that bad. Here's one that does look bad.

683
01:11:21,280 --> 01:11:27,280
So here's an example where this was the overall finding or no, sorry, this was the action.

684
01:11:27,280 --> 01:11:31,160
This was a finding from one very large randomized clinical trial.

685
01:11:31,840 --> 01:11:35,200
And you can see that that was mainly driving the overall estimate.

686
01:11:35,440 --> 01:11:40,510
It's very close to what the overall estimate of the odds ratio was.

687
01:11:40,780 --> 01:11:44,230
And these were all the studies. And you can see that almost all of them are to the.

688
01:11:45,300 --> 01:11:49,530
For you. Yeah. Still a left, right or to the left of that line.

689
01:11:50,160 --> 01:11:54,860
All of these studies are down here and these are all smaller than this large study.

690
01:11:54,870 --> 01:12:01,800
So it's showing that there is some really large systematic difference between the large study and the small smaller studies.

691
01:12:04,410 --> 01:12:06,910
So after this one, we would say, right,

692
01:12:06,930 --> 01:12:17,159
there is evidence of publication bias and it could be that people weren't publishing their results if they found something over here or something.

693
01:12:17,160 --> 01:12:22,710
Non-Significant Right. Or it could be for other reasons that we talked about here.

694
01:12:26,020 --> 01:12:31,479
So if we find evidence for heterogeneity and or publication bias, what can we do?

695
01:12:31,480 --> 01:12:39,040
And also just what are general sensitivity analyzes so we can think about stratification or these subgroup analysis.

696
01:12:39,060 --> 01:12:43,209
So we might say, oh, well, these, these studies were particularly heterogeneous,

697
01:12:43,210 --> 01:12:47,830
so let's leave them out and look at the effects just in these specific studies.

698
01:12:49,350 --> 01:12:59,570
We can also account for covariates. So the main meta analysis, right, the word meta analysis does not account for confounding by covariates.

699
01:12:59,580 --> 01:13:02,580
It's just combining the main effect of interests.

700
01:13:02,880 --> 01:13:09,360
Once we start trying to come to control for some of the variability with these covariates, it's now called meta regression.

701
01:13:09,750 --> 01:13:17,129
So now we're going to use a random effects model where we actually control for different variables or what we believe are differences in studies.

702
01:13:17,130 --> 01:13:21,720
So we'd have to capture like what's the proportion of females in the study?

703
01:13:21,720 --> 01:13:25,560
What are the proportion of individuals who are current smokers in each study?

704
01:13:25,590 --> 01:13:28,979
What are the proportion of individuals who are past smokers in every study?

705
01:13:28,980 --> 01:13:35,879
Right. So we'd have to have not just the effect size but this different like use probably baseline characteristics

706
01:13:35,880 --> 01:13:42,810
of individuals and or characteristics of the trials across time that we can now control for.

707
01:13:44,790 --> 01:13:52,600
Okay. So meta analyzes have a fair amount of strengths and that's why they're considered the top of that evidence hierarchy.

708
01:13:52,620 --> 01:13:55,860
They impose a discipline on the process of synthesizing results.

709
01:13:56,280 --> 01:14:01,650
They're capable of finding relationships across studies that can be obscured in other approaches.

710
01:14:02,130 --> 01:14:06,300
It protects against overinterpreting differences across studies because we're now combining

711
01:14:06,300 --> 01:14:11,670
them and we can see how similar or not they are and we can handle a large number of studies.

712
01:14:12,210 --> 01:14:15,840
The weaknesses are that it can take a really long time and a whole lot of effort.

713
01:14:16,950 --> 01:14:24,930
Often studies are quite different. So you might be doing this like apples to apples, to apples to apples is really apples to oranges comparison.

714
01:14:26,430 --> 01:14:32,370
Most meta analyzes are going to include perfect and not perfect studies or blemish studies.

715
01:14:32,670 --> 01:14:38,040
And so we can have various, you know, that can lead to more heterogeneity between studies.

716
01:14:39,030 --> 01:14:41,489
And then we always have a potential selection bias,

717
01:14:41,490 --> 01:14:51,630
either publication bias or other types of biases where we weren't able to actually find all the relevant studies to include in our analysis.

718
01:14:53,470 --> 01:15:00,070
Okay. So many analogies are replicable and defensible method of synthesizing findings that can be quite powerful.

719
01:15:01,000 --> 01:15:06,190
It often points out gaps in research literature, so a lot of times they'll try to say, we'll hear the results,

720
01:15:06,190 --> 01:15:11,710
but then they'll find like, you know, these studies left out this or these groups of people weren't involved in this.

721
01:15:12,250 --> 01:15:23,470
And so it can provide a really great. It can provide a result in one area and also hypothesis generating for next steps for next studies.

722
01:15:24,610 --> 01:15:32,860
It illustrates the importance of replication. So there's been a lot of talk about reproducibility and replication in the last decade or more,

723
01:15:33,130 --> 01:15:38,270
and most people feel like they're incentivized to do things differently.

724
01:15:38,290 --> 01:15:47,800
Right. Well, I need to make a big change in a different area to be celebrated in manuscripts and in my scientific area.

725
01:15:48,160 --> 01:15:55,470
But actually, it's really important to like one study alone doesn't necessarily mean that that is really good evidence, right?

726
01:15:55,480 --> 01:16:01,150
There could be lots of problems with bias or study quality. And also there's always type one and type two errors.

727
01:16:01,150 --> 01:16:06,190
Right. It could be a chance finding. And so replicability is very important.

728
01:16:06,190 --> 01:16:12,670
And then combining it across lots of studies and meta analysis is very important to have the best information we can on studies.

729
01:16:14,530 --> 01:16:18,730
All right. So just about right on time.

730
01:16:19,750 --> 01:16:27,460
Like I said Thursday, I'm going to give a short tutorial on all the different topics in the projects,

731
01:16:27,820 --> 01:16:35,110
so the different types of designs, which is going to kind of get you in the right headspace for our discussion next week.

732
01:16:35,560 --> 01:16:47,980
And I will be thinking about taking attendance next week for extra credit points, everybody who's not here so that you might want to come in,

733
01:16:47,980 --> 01:16:52,950
but again, not if you're sitting, otherwise you can come up and get your test out all the time.

734
01:16:52,960 --> 01:16:57,940
So I also have the first test. If somehow you never received it and you haven't.

735
01:17:09,950 --> 01:17:13,120
And later. And now.

736
01:17:17,660 --> 01:17:27,050
Are you? Are you excited?

737
01:17:27,080 --> 01:17:32,180
Every month.

738
01:17:33,020 --> 01:17:41,620
How is your watch? My wife is great. Yeah, I'm glad I didn't do it.

739
01:17:46,320 --> 01:17:50,450
No, I think we should play it. Yeah.

740
01:17:52,660 --> 01:17:55,850
I mean, do you like what to do with 625 like you do?

741
01:17:56,150 --> 01:18:00,110
I don't know. So you just put it on. Okay.

742
01:18:00,590 --> 01:18:07,280
With me inside, it is like it says. I have to say, oh, this is what I said.

743
01:18:07,670 --> 01:18:12,230
Like, I've never done something like this.

744
01:18:18,320 --> 01:18:24,740
I'm sure that I did something.

745
01:18:24,740 --> 01:18:28,280
Collectors, I guess I like the idea.

746
01:18:28,970 --> 01:18:39,140
I mean, I don't know everything, but I don't worry about it.

747
01:18:40,400 --> 01:18:48,030
I don't know. But I mean, I don't know what that's about.

748
01:18:48,890 --> 01:18:55,090
Yes, my sons are doing. So I focus on like, oh, [INAUDIBLE] get everybody.

749
01:18:55,390 --> 01:18:59,450
Oh, no, you're right.

750
01:18:59,600 --> 01:19:14,690
Thank you. I think it's too toxic. Has even though know I told you, I have no idea what is the price.

751
01:19:21,440 --> 01:19:25,090
So I was like, I think.

752
01:19:26,420 --> 01:19:33,110
Right, I wouldn't stop.

753
01:19:33,200 --> 01:19:36,460
Oh, okay. It's not that I'm fine.

754
01:19:36,640 --> 01:19:39,800
Yeah. No one wants action. I don't.

755
01:19:40,710 --> 01:19:43,910
I don't want to see it.

756
01:19:45,440 --> 01:19:51,410
I think I will drink. Yes, I will. Oh, my God.

757
01:19:59,370 --> 01:20:11,940
Yeah, we have these left over 20 000, really?

758
01:20:12,080 --> 01:20:16,390
And this is short. It's what? I don't know.

759
01:20:16,460 --> 01:20:39,980
It's like, you know, and also, like, for you, if you want to find out, how do they even know how this happened to you?

760
01:20:43,080 --> 01:20:48,770
You're you're basically to up one of the above.

761
01:20:50,480 --> 01:20:56,030
How old are you? Very pretty.

762
01:20:56,090 --> 01:21:01,100
Did you say you just, like, hated the class? I don't like.

763
01:21:01,610 --> 01:21:04,700
I thought I just came all the way.

764
01:21:05,000 --> 01:21:13,850
It was the case, you know, like as the capital intensive British.

765
01:21:13,850 --> 01:21:18,080
And Sarah wasn't the person is. Yeah.

766
01:21:18,380 --> 01:21:25,910
Yeah. So usually they're like, bad, like this first one.

767
01:21:25,910 --> 01:21:35,100
That doesn't necessarily mean the best score, though. I mean, like, you really want to say, yeah, yeah, yeah.

768
01:21:37,340 --> 01:21:45,950
My first one down the investor like, you know, there's a lot of that high school, but really I got my back that were like,

769
01:21:47,810 --> 01:21:58,220
yeah, so you, you know, you have very specific settings, slightly different terms, like he's really doing it.

770
01:21:58,860 --> 01:22:03,170
Yeah, yeah, yeah, yeah. That's right.

771
01:22:04,070 --> 01:22:08,390
Yeah, I like that too.

772
01:22:09,560 --> 01:22:14,490
I got burnt for when she wasn't. She might have been very impressive.

773
01:22:14,810 --> 01:22:18,770
But the other thing is like. He gets butchered somehow.

774
01:22:19,220 --> 01:22:23,420
I run it. I mean, I read about it now.

775
01:22:23,570 --> 01:22:27,140
Yeah, that's kind of a period of 20 minutes, guys.

776
01:22:27,370 --> 01:22:42,110
It's like, even though I'm just for the time, I was like, you know, you had your girlfriend.

777
01:22:46,070 --> 01:22:56,300
He's doing like that while traveling.

778
01:22:56,310 --> 01:23:09,680
And I knew I had to go as far as I knew.

779
01:23:35,520 --> 01:23:42,550
So when you. What? Of course.

