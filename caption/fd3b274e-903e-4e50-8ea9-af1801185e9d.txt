1
00:00:01,610 --> 00:00:22,380
Let me be clear. Oh, no, I knew it. There.

2
00:00:22,840 --> 00:01:08,240
That's wonderful. You start it up.

3
00:01:09,470 --> 00:01:18,950
So I show you the this comet smoother produced from this part of theta.

4
00:01:18,950 --> 00:01:29,930
Right. Remember that we have this, you know, all of the infection data from United States during the 14 year period,

5
00:01:30,410 --> 00:01:42,410
starting the year 1970 and now in the early 1970s, there was a national wide anti-polio sort of intervention policy then.

6
00:01:42,950 --> 00:01:53,450
The overall goal here is really trying to see whether or not there is a decrease of decrease in turning over time to after the,

7
00:01:53,450 --> 00:01:59,120
you know, the implementation of this national wide anti-polio intervention policy.

8
00:01:59,150 --> 00:02:09,020
So so the statistical model is like this because we have low counts, then we want to model the number of counts.

9
00:02:09,500 --> 00:02:14,890
The countries accounts by this are possible like states be smaller.

10
00:02:15,930 --> 00:02:26,329
Right. Yeah. So, so you have your y t that is number of counts in number of is like monthly of number

11
00:02:26,330 --> 00:02:34,819
of polio and they say under nine lt's sort of infection process going on over

12
00:02:34,820 --> 00:02:41,870
time and then you have to times three here so that we want to use to group to

13
00:02:41,870 --> 00:02:47,000
come to understand what's going on here and particularly you want to evaluate,

14
00:02:47,510 --> 00:02:52,570
you know, the effectiveness of this intervention policy which essentially is the covered

15
00:02:52,670 --> 00:02:59,600
t that you like to sort of figure out whether or not this is significant.

16
00:02:59,600 --> 00:03:10,160
So on the way to analyze them all is through this, you know, what is B, small arms structure as I already introduced.

17
00:03:10,670 --> 00:03:17,090
And one of the approach that we're trying to estimate the model parameters as well as to

18
00:03:17,090 --> 00:03:26,870
predict the underlying pseudo t right is using this best in the inner bias prediction block,

19
00:03:27,290 --> 00:03:34,340
namely the common filter coming students. So I gave you all this.

20
00:03:36,230 --> 00:03:43,250
The common sponsor would look like that if you actually implement this common filter into it.

21
00:03:43,910 --> 00:03:48,889
So I wanna spend a little bit of time because in the next homework I want you to program

22
00:03:48,890 --> 00:03:54,380
a little bit this common smooths or just to get used to how this process works.

23
00:03:55,400 --> 00:04:02,690
So I like to do a little bit derivation how this argument can be done, right?

24
00:04:02,690 --> 00:04:12,200
So I already give you a general formula to round the common filter and comes moves right as I sitcom that come and smooth.

25
00:04:12,210 --> 00:04:23,810
Ah but the based on this sort of person these are unbiased prediction theory and then you have this recursive formula that you you know,

26
00:04:24,260 --> 00:04:28,879
follow and to create your comment filters, this is become a future.

27
00:04:28,880 --> 00:04:33,230
Where do you do the forward prediction? Using the historical data?

28
00:04:33,620 --> 00:04:38,240
It looks a little bit evolved. You really need to go a bit of derivation.

29
00:04:38,270 --> 00:04:44,660
You work to understand how we applied it. The application is true for, but it's a little bit tedious how you can apply for it.

30
00:04:44,750 --> 00:04:52,549
Yeah. So after you get to the common filter, you do backward prediction is in this block, right?

31
00:04:52,550 --> 00:04:57,980
So, so you reach the end of the two time series then you just do backward calculation

32
00:04:57,980 --> 00:05:04,640
is also a recursive calculation so that the actually calculation is very fast.

33
00:05:04,670 --> 00:05:13,910
You need your complicity level. So I want to do a little bit derivation to see how this can be done.

34
00:05:13,970 --> 00:05:17,900
Okay. So here I showed that this emissions.

35
00:05:21,240 --> 00:05:27,180
His follow up fossil distribution that is be according to this follow that.

36
00:05:27,180 --> 00:05:36,570
Right. So it's 83. So given the current underlying infectious situation,

37
00:05:36,960 --> 00:05:45,530
how likely would observe the number of pollen comes from a surveillance system, maybe operated boxes.

38
00:05:46,170 --> 00:05:49,240
So the. Okay.

39
00:05:50,060 --> 00:05:54,240
So in fascist. A.

40
00:05:55,210 --> 00:06:00,070
Either put to the seasonality. So we do sit our seasonal thing and also.

41
00:06:02,110 --> 00:06:12,470
If you want to really figure it out, is whether or not there's a decrease in between, which will be out of of your cover of TIME.

42
00:06:12,820 --> 00:06:15,399
So it's a four plus on distribution.

43
00:06:15,400 --> 00:06:23,380
So basically I'll talk to you about how you simulate data front latent process to observe as smooth as possible distribution,

44
00:06:23,470 --> 00:06:29,650
the marginal process of distribution. Given this current time, they see that how it can be generated.

45
00:06:30,270 --> 00:06:35,200
There's a this is my ball. Why should that be generated from awesome distribution?

46
00:06:38,180 --> 00:06:43,160
They show X. Ah, right.

47
00:06:43,610 --> 00:06:47,280
So that's the place where you can introduce some coherence, like, listen,

48
00:06:47,280 --> 00:06:57,380
more for a coffee data that you can introduce vaccination or you know, actually the age groups who can.

49
00:06:58,480 --> 00:07:04,890
We have different set of variants.

50
00:07:05,460 --> 00:07:07,170
You can also put that there.

51
00:07:07,290 --> 00:07:15,780
And so so those are the covers you can add, which whatever you think is appropriate for your data to explain the potential variability of your data.

52
00:07:16,590 --> 00:07:21,180
So where all the from delayed the process to observe process?

53
00:07:21,360 --> 00:07:31,500
This part is from the process of distribution. It's a remarkable more model based on the baseline, which you cannot explain by a variable.

54
00:07:32,700 --> 00:07:37,830
You know, corvids something beyond what you can explain by the and.

55
00:07:44,150 --> 00:07:55,490
It also means you have to make sure that the timing are right so personally exponential sure that the page is not negative.

56
00:07:55,580 --> 00:08:07,920
Right. So if your model is affected by a traditional airline, it doesn't work because you have to guarantee that say no one is positive.

57
00:08:08,430 --> 00:08:13,710
So one thing you could do here. Well.

58
00:08:16,270 --> 00:08:19,900
By the traditional box. JENKINS You know, one process.

59
00:08:20,980 --> 00:08:32,020
Oh, then this say that you will follow a log novel distribution and so that that could be one choice and people actually do that in the literature.

60
00:08:32,440 --> 00:08:36,730
So I want to do this a little bit more straightforward in the.

61
00:08:40,320 --> 00:08:47,610
Know. Then it was always like this following the tradition of We are one of this box.

62
00:08:47,610 --> 00:08:50,910
Jenkins one so you can do that attention to work.

63
00:08:52,420 --> 00:08:57,120
The the process so that you know, basically the.

64
00:08:58,140 --> 00:09:03,060
We are kind of exponential into all of the at once.

65
00:09:03,240 --> 00:09:10,479
So you can do that up. I'm a theater.

66
00:09:10,480 --> 00:09:14,920
It's just director. Should not say that he follows a gamma distribution.

67
00:09:15,150 --> 00:09:25,660
Okay, so we know that. So if a random variable follows dollar distribution, these parameter K and and the state of copper,

68
00:09:26,560 --> 00:09:33,070
we'll see that it has the following sort of the short on the right hand side.

69
00:09:35,290 --> 00:09:39,920
Obama distribution. Right so so the density function you know that is.

70
00:09:45,970 --> 00:09:57,800
You know, to the. And then you have X minus one X over nine, right?

71
00:10:03,440 --> 00:10:10,520
I. Oh, I don't know.

72
00:10:11,110 --> 00:10:14,600
It's okay. We'll see. Anyway, this is the standard.

73
00:10:14,960 --> 00:10:20,660
This is a function of your cop. And.

74
00:10:22,370 --> 00:10:25,790
And this is the distribution you have.

75
00:10:25,790 --> 00:10:33,710
And now I work a little bit a different sort of distribution parameterization system.

76
00:10:34,070 --> 00:10:40,330
So this is your. This is your secret for.

77
00:10:42,520 --> 00:10:54,639
601. And so now what does gamma gamma distribution rank gamma distributions is a spatial is spatial case of your dram, right?

78
00:10:54,640 --> 00:10:57,760
So this one as the jitter.

79
00:10:57,760 --> 00:11:09,220
And so it generates neural representation essentially that there are something called gamma ray burst, right.

80
00:11:09,340 --> 00:11:23,910
So that can be used in practice. People theta stuff has the positive continuous valid where some random variable that common people have done and.

81
00:11:26,520 --> 00:11:29,800
If they do cheer and function, you are also.

82
00:11:35,920 --> 00:11:45,190
But. It is there and the same as you are by normal regression or possible repression.

83
00:11:45,360 --> 00:11:56,320
Okay, so because of this connection to your M for people who are working on their for something like six over one or something like that.

84
00:11:58,540 --> 00:12:01,740
So people want to do some repair of HSC. Okay.

85
00:12:01,780 --> 00:12:08,170
So this is sort of the generic location people use.

86
00:12:09,220 --> 00:12:14,650
You have your thumb, they say, come and see that.

87
00:12:16,000 --> 00:12:19,450
And then random variable follows comma distribution.

88
00:12:19,450 --> 00:12:24,880
Then the expectation of gamma is how times theta, right?

89
00:12:25,030 --> 00:12:33,310
You know that. And the variance of x is I think is.

90
00:12:34,960 --> 00:12:40,510
One more. Status where over half.

91
00:12:50,840 --> 00:12:58,940
I. The. In Peter's Square chafing.

92
00:13:08,560 --> 00:13:11,770
So what is the fairness? Where is.

93
00:13:15,120 --> 00:13:18,360
Opposite a square. Is that right? I think so, yes.

94
00:13:18,630 --> 00:13:21,940
I think so, yeah. Okay. Oh.

95
00:13:23,680 --> 00:13:26,770
So if you put in all this meal. Right, that's me.

96
00:13:27,040 --> 00:13:30,750
So you introduced. As the meaning of this.

97
00:13:31,410 --> 00:13:38,580
And you'll get parasites that come out of distribution according to me and the the school parameter.

98
00:13:39,870 --> 00:13:46,200
So. So what they are trying to do here is that you can add.

99
00:13:46,680 --> 00:13:51,810
You can create it as square and divide by its.

100
00:13:54,090 --> 00:14:00,120
But oftentimes it is your mule. So they becomes one over hopper and views.

101
00:14:02,190 --> 00:14:07,440
You will know that in the g r m there's a famous relationship between being an advertisement.

102
00:14:10,290 --> 00:14:15,239
As a factor. Times Square, just like normal kids.

103
00:14:15,240 --> 00:14:19,139
Right? So this place called Weird Function this year pulled out.

104
00:14:19,140 --> 00:14:25,360
This is called variance function we usually use. So.

105
00:14:25,790 --> 00:14:30,259
So same thing here. I mean, just for the convenience of the cream to talk to you about regression,

106
00:14:30,260 --> 00:14:37,550
all you can refer a wide swath of distribution in the way that you write this as something new squared.

107
00:14:37,640 --> 00:14:42,170
The variance is a factor in Times Square of your mean.

108
00:14:42,710 --> 00:14:46,100
So if you have portion distribution.

109
00:14:49,240 --> 00:14:56,500
Normal function is a constant right for the binomial.

110
00:14:57,850 --> 00:15:06,700
Right. You know that the various functions is new times to y minus you feel connected by an all new variance.

111
00:15:06,700 --> 00:15:08,680
Publishing is New York Times one.

112
00:15:10,410 --> 00:15:23,040
And for parcel distribution, then smoking is your meal and for your karma distribution karma there's funky smooth squared,

113
00:15:24,270 --> 00:15:28,740
a universe, Gaussian distribution various function as you get to the cute.

114
00:15:29,750 --> 00:15:34,910
It still darts at the simplest distribution, which I'm going to introduce next.

115
00:15:35,480 --> 00:15:42,020
The various function is one minus muse, two bit times one minus.

116
00:15:44,870 --> 00:15:55,000
Cost family of jr m follows that that actually same form of wears is a parameter times the other parents can.

117
00:15:55,010 --> 00:15:59,420
So because of this and usually with you know the bears.

118
00:16:02,400 --> 00:16:10,379
Random variable from grim random variable could be normal can be negative binomial could be binomial could be possible.

119
00:16:10,380 --> 00:16:15,240
Anything from a family distributions. That's what you learn from 61.

120
00:16:15,240 --> 00:16:20,940
Could you reason as a dispersion parameter or sigma squared minus variance punishment view?

121
00:16:22,860 --> 00:16:26,040
This is very, very famous. Thank you.

122
00:16:26,220 --> 00:16:32,910
So the reason I mentioned here is that you can rewrite this one rewrite.

123
00:16:34,710 --> 00:16:43,600
Well. Now you introduce a comfort function.

124
00:16:55,710 --> 00:17:01,460
Oh, you know that this sycamore square or your chin probably this.

125
00:17:01,590 --> 00:17:09,020
You know that's. Period.

126
00:17:09,290 --> 00:17:16,120
But typically people use five as the location for disbursement parameter in general.

127
00:17:16,140 --> 00:17:23,400
But I like to use Income Square because this is the parameter in a normal case is the variance parameter anyway.

128
00:17:23,720 --> 00:17:36,940
So so you can use Sigma Square asset, but. Well, okay, so that's something related.

129
00:17:38,600 --> 00:17:46,730
Over Sycamore Square Oak. The full presentation here.

130
00:17:47,150 --> 00:17:50,450
Essentially, I changed. I used notation lambda.

131
00:17:52,980 --> 00:18:01,590
Well. So instead of use. So I use lambda here and lambda is really just your shift parameter in the

132
00:18:01,590 --> 00:18:05,909
gamma distribution is I mean it's just different people use different voltage.

133
00:18:05,910 --> 00:18:11,650
I tell you what the notation is about. Okay. Oh.

134
00:18:12,430 --> 00:18:15,760
So if you use this notation.

135
00:18:20,880 --> 00:18:28,170
Well, if you use the new primer is a new primer. Since I'm now changing gamma distribution, I'm just changing the way of primer cases.

136
00:18:28,830 --> 00:18:36,600
So now I have images done to it and new, which is the mean and lambda.

137
00:18:37,740 --> 00:18:48,240
Okay. So this is your mean? Practice is. Well, this is no use referring here.

138
00:18:50,730 --> 00:18:55,330
So. Because you want you to be.

139
00:19:03,450 --> 00:19:07,920
Yeah. So in the, um, the model specification.

140
00:19:09,960 --> 00:19:13,710
So I have a interceptor here in the model specification.

141
00:19:14,250 --> 00:19:20,220
So in order to guarantee the model of the identified bullet, you have to make this.

142
00:19:23,710 --> 00:19:28,030
To. You have to enforce this one.

143
00:19:28,480 --> 00:19:36,650
This. But able to identify the parameter, the intercept parameter.

144
00:19:36,980 --> 00:19:42,049
Okay. So this is really you are you let this to be arbitrary.

145
00:19:42,050 --> 00:19:49,790
Mean where you put the mean. This field practice just like Cox regression.

146
00:19:49,790 --> 00:19:55,849
Right. You don't have intercept you'd like to do intercept turn to be absorbed in the baseline has a

147
00:19:55,850 --> 00:20:02,750
function here I wasn't able to intercept terms so that I need to force the meaning of this from.

148
00:20:03,910 --> 00:20:07,900
Will that identify the intercept parameter in estimation?

149
00:20:10,440 --> 00:20:14,190
Now of back to you.

150
00:20:14,190 --> 00:20:20,339
That's the model. So now I create a better description.

151
00:20:20,340 --> 00:20:25,950
I directly specify this is no my work is people have done in the.

152
00:20:29,970 --> 00:20:33,950
So you write that here are one process, this Markov process.

153
00:20:34,060 --> 00:20:43,510
Do you know? Infectious status through this Mark.

154
00:20:43,630 --> 00:20:50,460
Mark passes this Garma district. Of this.

155
00:20:51,890 --> 00:20:54,950
So karma is one choice you can use. Order distribution.

156
00:20:54,950 --> 00:20:58,130
But here, since we have this error one.

157
00:21:00,290 --> 00:21:04,040
First of all, ramaco passes this market karma. You can write in this.

158
00:21:06,750 --> 00:21:10,080
So here it follows. Also comma distribution. It's meet one.

159
00:21:10,320 --> 00:21:14,190
That's what you want. Lama That is your ship right here.

160
00:21:14,880 --> 00:21:18,550
And. Uh, decomposition.

161
00:21:19,120 --> 00:21:28,900
Right. So in order to maintain this tissue. Then you need to make the right hand side.

162
00:21:28,930 --> 00:21:33,110
Also follow second one from the right.

163
00:21:33,160 --> 00:21:42,320
So you have to, you know. Oh.

164
00:21:45,120 --> 00:21:49,830
On the distribution with the same margin distribution I before the start.

165
00:21:50,960 --> 00:21:57,300
Is to be. So let's say that he is a station or a process that has deserved distribution.

166
00:21:57,900 --> 00:22:06,840
This is your talking about Fisher policies, but no. Policies that can generate the ultimate regressive correlation.

167
00:22:07,680 --> 00:22:15,030
Okay. So how are you going to do that? Well, that's a lot of people finding out that this in order to achieve that.

168
00:22:21,580 --> 00:22:27,270
This role. Well.

169
00:22:31,200 --> 00:22:49,770
But. More independent than this.

170
00:22:50,040 --> 00:22:56,280
We calculate that the distribution of this product, this will follow.

171
00:22:59,590 --> 00:23:03,520
This row row at.

172
00:23:09,770 --> 00:23:13,430
Wanda, will you. Will this follow this? It can prove that people were.

173
00:23:23,580 --> 00:23:29,050
It's real far and. This one minus the wrong.

174
00:23:31,490 --> 00:23:35,060
Apparently you have to make they love the role to be positive.

175
00:23:35,480 --> 00:23:39,530
So this process doesn't allow you to have negative correlation.

176
00:23:40,240 --> 00:23:45,850
Okay. Well, they are fun.

177
00:23:46,090 --> 00:23:51,670
They should be positive. And so this process, I'm just right out here,

178
00:23:52,240 --> 00:24:04,560
the air one process become a margin only allow you to have positive dependance over time anyway so you can prove that under this clear.

179
00:24:06,330 --> 00:24:11,080
And this product falls a gamma distribution, this mineral.

180
00:24:15,030 --> 00:24:31,389
In my book, I call index printer. I my future advisor, I mean, but I just use it because that's the term that people use in here.

181
00:24:31,390 --> 00:24:43,280
ED Anyway. So by convolution. So if you have truly independent global distribution, then you can and I have this in ship parameter you have.

182
00:24:43,280 --> 00:24:48,310
Colucci. Add them together according to Convolutional.

183
00:24:49,960 --> 00:24:58,630
Then the some will follow. Well, because you can add to the role and.

184
00:25:01,640 --> 00:25:07,400
This. Okay. So that's the way that you guarantee that versus this narrative.

185
00:25:07,580 --> 00:25:10,820
Okay. So this is construction. Okay.

186
00:25:11,630 --> 00:25:17,530
Well, you construct this process using the margins, almost margins.

187
00:25:18,310 --> 00:25:28,920
Okay. That's the part I wanted to.

188
00:25:28,920 --> 00:25:42,000
Elizabeth, the revision. See how we're going to derive the common future.

189
00:25:42,300 --> 00:25:51,910
Okay. Okay. I should mention that as a result here, the auto quotation function from this construction of the back end is wrong.

190
00:25:51,990 --> 00:25:56,910
The role here is the Alpha Corporation proper.

191
00:25:57,090 --> 00:26:04,160
That is what? Ultimate dependance between the process.

192
00:26:05,420 --> 00:26:11,059
So in order to derive common filter that we back to the original formula, right,

193
00:26:11,060 --> 00:26:18,560
because this common film that are smooth are all based on the the Niemeyer system we define and verb beginning right here.

194
00:26:19,700 --> 00:26:28,729
Okay so here I did you since you work out why t state a t that's first thing I need.

195
00:26:28,730 --> 00:26:34,130
I work b to work out why t condition words.

196
00:26:35,030 --> 00:26:39,030
Okay, this is the first line of the two things I need to space.

197
00:26:39,050 --> 00:26:47,540
I need to figure out what is the capital a what is the little a, what is the w t what is wt0 if I can't feel goggles terms right.

198
00:26:47,900 --> 00:26:55,420
But I can use the standard of common filters. Moves are formula or to make a prediction because I really want to figure out what is the main process.

199
00:27:04,200 --> 00:27:11,850
I really once want to know what's going on in the population about something beyond the obvious.

200
00:27:12,460 --> 00:27:15,540
Right. So. So you need to work on this.

201
00:27:15,990 --> 00:27:22,920
And also you work out the conditions, the expectation of data and see that minus one.

202
00:27:23,250 --> 00:27:30,000
What is the average sort of situation of the transition from past to today?

203
00:27:31,100 --> 00:27:40,250
Parents of see that he is in a team. That's something I need to identify the cop to be a little bit covered.

204
00:27:41,000 --> 00:27:45,770
And Tabitha. Okay, so let me just clean up a little bit here.

205
00:27:45,770 --> 00:27:49,640
I already talk about how to construct this. Well, that.

206
00:27:50,840 --> 00:27:59,380
Right. Well, I do have this out for a walk to the mall and the specific place.

207
00:28:01,370 --> 00:28:06,530
So the first layer is how this lit and process generate generated observed process.

208
00:28:06,560 --> 00:28:12,710
Number of pilot incidents. Right. At the same time, I specify the legal process as Markov.

209
00:28:15,330 --> 00:28:24,799
Thelma. Margie, that's basically them. In notation.

210
00:28:24,800 --> 00:28:33,440
It's terrible. I would just tell them that this is. Redistributed is not the same beat of this.

211
00:28:33,450 --> 00:28:36,560
I don't know. But I will tell you, they are different.

212
00:28:36,580 --> 00:28:40,640
But this is bar one.

213
00:28:41,670 --> 00:28:44,940
Okay. So this is not the same as this.

214
00:28:45,770 --> 00:28:49,200
Okay. This 80. It's now same as this.

215
00:28:50,280 --> 00:29:02,440
Okay. So so I'm sorry. I just messed up quite a bit, you know, but I tell you, those are more generic sort of a notation used in this new notice.

216
00:29:02,460 --> 00:29:10,200
And those notations are used to very specifically in very particular application their time in simpler times.

217
00:29:10,920 --> 00:29:16,100
But when I wrote in my book, I should make everything. Do that.

218
00:29:16,550 --> 00:29:21,950
But create some confusion. But anyway, if you want to use much smoother,

219
00:29:21,950 --> 00:29:31,450
I need to really derive those term because all the computers smoother you can see in the formula really deep on this,

220
00:29:32,510 --> 00:29:37,560
those all the terms that we see in this an inner system.

221
00:29:37,580 --> 00:29:44,240
Okay, let me derive this out for you. And you see that you can do it yourself.

222
00:29:48,440 --> 00:29:52,010
How can you calculate the computer expectation? Like you can see that?

223
00:29:52,070 --> 00:29:57,080
Yes. Because why do you give and say that he follows a portion distribution.

224
00:29:58,540 --> 00:30:07,250
So. Y t given that t all the possible distribution, this is a harmless life is observed.

225
00:30:07,970 --> 00:30:12,370
So if y t condition don't see that you y you follow all of this.

226
00:30:12,990 --> 00:30:17,650
What's the meaning of that? If you can't see the teeth.

227
00:30:17,710 --> 00:30:21,280
Right, because that's the being a problem distribution.

228
00:30:22,180 --> 00:30:30,220
So if a random variable follows that Palsson distribution, this means you get less expectation of quality distribution you.

229
00:30:36,600 --> 00:30:40,470
Parcel distribution is the only distribution that has being advanced equal.

230
00:30:41,040 --> 00:30:46,260
If there's not of over dispersion, you know that. Right? So it's so easy to memorize.

231
00:30:47,390 --> 00:30:51,470
000 80 it.

232
00:30:54,090 --> 00:31:01,970
Of course, I shall not mention. I think that's why if you follow personal disputes and this eight is that exponential

233
00:31:01,980 --> 00:31:12,620
X transpose oh ah from here I can identify my capital 80 is in fact a little bit.

234
00:31:14,500 --> 00:31:18,600
Right. You know, it is the one I define both there and here I have.

235
00:31:19,640 --> 00:31:25,160
This 88 little territory differently.

236
00:31:25,640 --> 00:31:30,170
So this 80 okay. This 80 is zero.

237
00:31:32,250 --> 00:31:37,560
They also identify that from from this expression now going to the various parts.

238
00:31:41,960 --> 00:31:49,130
It also ATC that he. Then I can identify this guy.

239
00:31:50,090 --> 00:31:57,970
Okay. What if opportunity stayed up there? So this whole thing is my w t c.

240
00:32:00,380 --> 00:32:04,200
It's just a minor function of the team and us.

241
00:32:04,250 --> 00:32:08,030
Zero. So this one is my wt0.

242
00:32:10,510 --> 00:32:19,800
Will you identify probability zero as zero because you don't have this sort of offset or additional term here.

243
00:32:20,120 --> 00:32:23,240
You finish up this, you identify this.

244
00:32:24,610 --> 00:32:34,799
That's this. And Colin Fielder smoother.

245
00:32:34,800 --> 00:32:38,520
You need to work on additional thing. That's the expectation of that.

246
00:32:39,000 --> 00:32:47,100
So from here we work out w t bar. Which is expectation of w t is.

247
00:32:50,150 --> 00:32:54,200
And see that you is 80 to 90.

248
00:32:56,450 --> 00:33:02,280
But it is not random. It's just, you know, this covers something like.

249
00:33:02,840 --> 00:33:09,650
So you have 80 and the expectation of be that was the expectancy that you won.

250
00:33:10,010 --> 00:33:14,010
Because I want I force the latent process to have me one right.

251
00:33:14,030 --> 00:33:25,640
So this is equal to one because that's how I set up a model for this one will equal 58 is exponential X transpose RFA.

252
00:33:25,820 --> 00:33:29,570
Okay. That's that's the one I figured out here. Okay.

253
00:33:30,740 --> 00:33:36,760
Oh. From this derivation, I captured 80 people to copy.

254
00:33:40,800 --> 00:33:49,050
Is zero. And then I have my baby to see that he is 80.

255
00:33:49,810 --> 00:33:54,960
See that T and wt0 is zero plus.

256
00:34:00,220 --> 00:34:09,860
Yeah. I thought the key bar is. That's the first part coming from this emission probably.

257
00:34:10,040 --> 00:34:13,640
Right. So I did the calculation of that.

258
00:34:13,820 --> 00:34:18,480
No problem. Now for the circle there, I need to do this right.

259
00:34:18,500 --> 00:34:27,760
So. So how are we going to do that? So I'll say that I am working on this first order of Markov process, this karma margin.

260
00:34:28,100 --> 00:34:34,729
Okay. That I have this small right.

261
00:34:34,730 --> 00:34:47,150
This is my ex. Now I know that it is random variable that I should say this is different from this, but not this is transition matrix.

262
00:34:50,280 --> 00:34:54,520
But A anyway, you understand the difference here. Well, it's very confusing.

263
00:34:54,970 --> 00:35:01,900
But anyway, so this is t c t minus one plus some t given t minus one.

264
00:35:03,770 --> 00:35:06,920
This process is defined as the, first of all, remarkable systems.

265
00:35:07,580 --> 00:35:18,290
And this error term, this random shock, is so the number of the random new random numbers of portal cases,

266
00:35:18,290 --> 00:35:22,340
as in the system we move from yesterday to today.

267
00:35:22,700 --> 00:35:29,180
So if some people have some kind of random new numbers of cases, could be zero, could be something different.

268
00:35:30,860 --> 00:35:35,620
Your movie. Is this a common process?

269
00:35:35,650 --> 00:35:42,430
I'm sorry. It could be a very low number. Okay, so they are independent.

270
00:35:42,940 --> 00:35:53,950
This is. No rent noise adding to this system so that you have yet to see that.

271
00:35:54,990 --> 00:35:58,650
You can see that t minus one plus some.

272
00:35:58,960 --> 00:36:07,530
But this is zero. This is independent of that, the conditional expectation, what you call margin expectation,

273
00:36:07,530 --> 00:36:13,080
because state of T minus one does not affect your random noise.

274
00:36:15,790 --> 00:36:19,480
Of this common distribution this fall.

275
00:36:22,070 --> 00:36:26,590
If. Did Prime for London.

276
00:36:27,010 --> 00:36:30,580
Sold out. This is your real bar.

277
00:36:32,960 --> 00:36:40,340
This is an advantage of using this gamma ray parameterization because now you can immediately the mean of that, right?

278
00:36:40,760 --> 00:36:46,000
So how about this term? Because the beta random variable is independent of theta process.

279
00:36:46,010 --> 00:36:54,460
This is. So that you can write this as the FT because this they are independent and.

280
00:37:04,680 --> 00:37:12,569
Oh so you can factories that see that human is one because this is the condition expectation and you condition that they

281
00:37:12,570 --> 00:37:21,520
are to my point in fact to resolve this and I get the marginal beat of that is this doesn't depend on the strength of.

282
00:37:24,110 --> 00:37:31,370
But what is this, the middle of this being a random variable? The patent random variable has often been.

283
00:37:31,400 --> 00:37:40,310
What's the meaning of that? I should be. But it.

284
00:37:49,270 --> 00:37:54,970
I'll be wrong. Okay. So what you have here is.

285
00:38:01,130 --> 00:38:07,890
Well, then if I. This. Okay. Mean. A little bit patient to derive the whole thing.

286
00:38:08,730 --> 00:38:13,170
Although Common Fielder, I suppose, are happy to derive like 70 years ago.

287
00:38:13,180 --> 00:38:18,630
But you need to work out the details based on whatever the model system is based on.

288
00:38:18,850 --> 00:38:21,990
So here you have to be 50. I'm talking about this guy.

289
00:38:22,020 --> 00:38:28,620
Okay? Right here is your rule, which is out of condition, proficient.

290
00:38:28,920 --> 00:38:35,120
Right. And then you are below. But he will be one over.

291
00:38:36,330 --> 00:38:40,370
Okay. That's a little bit. Is your rule bar?

292
00:38:40,370 --> 00:38:44,230
Rule bar is this one minus four. Okay.

293
00:38:46,340 --> 00:38:55,550
Moving to next term from here you'll need to drive this various variants of this that you do the.

294
00:38:59,850 --> 00:39:09,569
In the calculation the various. Earned from conviction expertize and complete conviction.

295
00:39:09,570 --> 00:39:16,860
I mean, you learned XO 0602 or six or one anyway.

296
00:39:18,180 --> 00:39:21,960
Okay, so the parents disarm some of the two random variables.

297
00:39:23,930 --> 00:39:28,740
So independence this will be your experience of BTC that you.

298
00:39:30,930 --> 00:39:39,750
Parents conditions before. Argument that this will be best for.

299
00:39:41,060 --> 00:39:44,210
We're business. Okay. Sanctions.

300
00:39:45,860 --> 00:39:54,950
There's two terms. If something is independent of the process, both of these are random variables and they're the process itself.

301
00:39:55,730 --> 00:39:59,330
So that you can split this. Okay. Right.

302
00:39:59,690 --> 00:40:02,270
If this two terms operate that you have covers,

303
00:40:02,900 --> 00:40:10,130
but they are not under assumption that this is a complete random external noise coming through the system that.

304
00:40:11,300 --> 00:40:17,200
Right. That's the. They start the conversation on their independent assumption.

305
00:40:17,950 --> 00:40:25,160
Let's first of all, call this. So this follows a gamma distribution with the.

306
00:40:27,420 --> 00:40:32,730
Well, it has to be a new square. I said, I mean, there's really.

307
00:40:35,220 --> 00:40:46,640
This one word on the. Oh that to me is so this is the mean the dispersion from.

308
00:40:47,760 --> 00:40:52,140
Forward. Okay. So that you have.

309
00:40:52,950 --> 00:41:04,669
Oh, you're all square overall. And that's the term that you have, right, Erin, by the the gamma distribution,

310
00:41:04,670 --> 00:41:10,010
the variance of gamma random variable will be dispersion parameter times mean square.

311
00:41:16,480 --> 00:41:28,450
Okay. Well, this one, then? Mm hmm. When they say that he is independent, then, uh, let me see how we are going to calculate this.

312
00:41:31,540 --> 00:41:35,600
Although, I mean, there are multiple ways to calculate that. Let me see what I did here.

313
00:41:46,360 --> 00:41:49,780
And. Oh, I did.

314
00:41:50,680 --> 00:41:53,790
I did not a calculator. Do you diet or. No, I.

315
00:42:04,410 --> 00:42:12,500
Roe squared. There are type, there should be a robust square.

316
00:42:12,510 --> 00:42:15,870
Right. And.

317
00:42:21,900 --> 00:42:27,740
Oh. So this will be a.

318
00:42:36,580 --> 00:42:40,290
See that. Of course, that's a function of that. What?

319
00:42:53,520 --> 00:42:58,990
That is to say that. So now you have.

320
00:43:03,300 --> 00:43:06,900
There's a typo here, sir. Should be a T minus one.

321
00:43:07,510 --> 00:43:11,370
But what? I want to see that this is your own.

322
00:43:12,450 --> 00:43:16,990
And you know. Whatever it is.

323
00:43:17,980 --> 00:43:22,900
It happens to be like that. There should be a square according to the ballot distribution.

324
00:43:22,900 --> 00:43:34,930
This is the mean. And while lambda is your dispersion, that should be having a square here.

325
00:43:35,500 --> 00:43:40,460
And then. Yeah.

326
00:43:42,960 --> 00:43:46,140
What I need to work on is really the expectation.

327
00:43:49,110 --> 00:43:52,990
Oh. I need to work on the expectation of. Okay.

328
00:43:53,780 --> 00:43:57,210
I need to. I don't. I don't care about actual function.

329
00:43:58,110 --> 00:44:01,650
What I really care is expectation of function.

330
00:44:03,280 --> 00:44:10,510
Essentially it is expectation of variants of the t c that t minus one given c that.

331
00:44:12,500 --> 00:44:18,700
But. If I want to calculate that, I can do.

332
00:44:30,330 --> 00:44:46,180
Like. So is the expectation of years of BTC a t minus one given C that minus is equal to the marginal variance.

333
00:44:46,750 --> 00:44:59,120
The. Minus the appearance of expectation of in a T minus one.

334
00:44:59,120 --> 00:45:04,290
Given that you might have. Will this formula write?

335
00:45:04,650 --> 00:45:09,820
The margin of variance of X is equal to variance of condition.

336
00:45:12,220 --> 00:45:16,840
Plus expectations of parents of students.

337
00:45:18,720 --> 00:45:26,150
Well, this is a famous thing. Well, now you work to calculate this.

338
00:45:26,300 --> 00:45:32,730
I need to just calculate this. Right. Before this one because they are independent.

339
00:45:35,370 --> 00:45:45,300
You can easily work out this verse of The Independent to remember this is the second moment.

340
00:45:47,630 --> 00:45:56,870
This will be your second moment. Minus the the first moment.

341
00:46:02,710 --> 00:46:09,050
Right. So when you have the this, then you can do the calculation, right?

342
00:46:09,470 --> 00:46:14,150
So you're talking about six moments, but they are independent.

343
00:46:14,930 --> 00:46:18,820
So you have. But.

344
00:46:21,960 --> 00:46:31,460
Most are independent, right? And the expectation of the second moment of the.

345
00:46:43,000 --> 00:46:51,470
This one will be equal to the role. In the expectation.

346
00:46:51,500 --> 00:46:58,000
The expectation is from a single theater, T minus Y is one.

347
00:46:58,360 --> 00:47:01,720
I was stationary too, so this one didn't make those work.

348
00:47:02,290 --> 00:47:07,900
And then I need to rip out the.

349
00:47:11,140 --> 00:47:25,120
The second moment of the tedious calculation that basically enables what I have here is the wrong, wrong, wrong.

350
00:47:25,570 --> 00:47:31,050
I think this this this this is. So this cancelation going on.

351
00:47:31,440 --> 00:47:34,590
So finally, that you get this this thing. Okay.

352
00:47:34,980 --> 00:47:40,470
So so that's out of expectation of it's real ruler over long.

353
00:47:40,930 --> 00:47:51,570
Okay so maybe cancelation from there going on you can complete that provision and so that if so but what?

354
00:47:53,050 --> 00:47:58,830
The time to really work out all the sort of revisions under all the conditional expectation.

355
00:47:59,750 --> 00:48:03,350
What I have. And here is the.

356
00:48:05,740 --> 00:48:14,440
That's right. And then I'm ready to U-turn smoother to read the right things.

357
00:48:14,980 --> 00:48:20,920
Before I do that, I can tell you a bit about the.

358
00:48:27,700 --> 00:48:37,870
Relation here listed in this slides. This is a very sort of high level slides that involves a lot of actually calculation behind the phone number.

359
00:48:38,230 --> 00:48:45,610
For example, I want to derive I just give you an example. T So how many are calculated?

360
00:48:46,770 --> 00:48:55,090
The covariance of y if you see the t like this can be written as covariance of see the t, you can see that.

361
00:49:02,760 --> 00:49:07,050
And expectations of the team. It's.

362
00:49:09,750 --> 00:49:15,510
Okay. This is the you know, the marginal covers can be written as the song submit two conditional variance.

363
00:49:15,510 --> 00:49:19,070
Right. Opportunities.

364
00:49:19,080 --> 00:49:22,250
Given that this you can treat this as a constant condition.

365
00:49:22,260 --> 00:49:28,410
I'll say that this term is constant. So the covers of anything with a constant is zero.

366
00:49:30,960 --> 00:49:41,480
When see that he's given this term is. All the condition of virtually zero because anything with a constant would have zero coverage.

367
00:49:41,490 --> 00:49:45,390
So this term it's zero because this time is fixed.

368
00:49:45,630 --> 00:49:50,490
Condition of the. Now this is your problem.

369
00:49:50,490 --> 00:49:58,140
Although this is your age, you see that this condition exploitation expectation is given by impossible.

370
00:49:58,710 --> 00:50:02,490
And this one. So you have to give and see what is to receive that he.

371
00:50:04,970 --> 00:50:08,820
Right. Okay.

372
00:50:09,040 --> 00:50:13,400
So this is the property of conditional expectation.

373
00:50:13,850 --> 00:50:16,940
So what do you have here? Is enough with the.

374
00:50:19,450 --> 00:50:25,700
It. Spirits of theta t and C that you which is the appearance of you.

375
00:50:28,980 --> 00:50:32,720
So. So what is the appearance of Sidoti?

376
00:50:38,800 --> 00:50:43,920
Right this this is a see that he follows that GAMA distribution so dispersion

377
00:50:43,920 --> 00:50:49,050
parameter which is your one where lambda times the mean but the meaning of see that.

378
00:50:53,430 --> 00:51:01,169
Because you required the meaning of Sidoti to be one you work to satisfy, identify, ability, condition and one over lambda.

379
00:51:01,170 --> 00:51:04,890
Lambda is your index parameter, which is one over dispersion.

380
00:51:05,400 --> 00:51:11,610
Okay. So that's why you could 80 divided by lambda as a result.

381
00:51:12,060 --> 00:51:16,740
Okay. You can do similar calculation using the same inner argument.

382
00:51:16,980 --> 00:51:22,670
And basically this is the formula marginal you have.

383
00:51:22,900 --> 00:51:32,780
Right? Well to derive this margin poverty.

384
00:51:33,230 --> 00:51:36,430
You just use this sort of the properties. I just.

385
00:51:37,450 --> 00:51:41,970
He's very into the various condition on something where you do the coverage.

386
00:51:42,840 --> 00:51:47,610
Okay. So after he did that, then you just come back to work on the computer.

387
00:51:51,060 --> 00:52:00,240
Want to do one, one thing and then you you can go home and check it and oops, this was.

388
00:52:07,980 --> 00:52:12,480
Let's see. Okay, let's do common filter first.

389
00:52:12,480 --> 00:52:16,469
Do first thing. Right. Of course.

390
00:52:16,470 --> 00:52:23,620
My filter is a the recursive formula where you is.

391
00:52:23,640 --> 00:52:29,610
Suppose you've already completed the calculation in the previous step that you're moving down to the next step.

392
00:52:30,150 --> 00:52:57,110
So how are going to do that? Let me put it up here. So so I have my common future calculated and I'm given the data up to yesterday t till today.

393
00:52:58,580 --> 00:53:04,100
Given that the number of the pollen incidence up to yesterday,

394
00:53:05,060 --> 00:53:14,860
this is already good my common computer point prediction bluff and this prediction arrow C matrix.

395
00:53:22,140 --> 00:53:25,400
And what I wanted to update my prediction.

396
00:53:26,060 --> 00:53:32,000
Okay. Become of what is actually latent value.

397
00:53:32,090 --> 00:53:34,630
Let's say that that's something they want to achieve.

398
00:53:34,980 --> 00:53:42,770
And so in order to do that, at first, I need to do a one step forward prediction that is this okay?

399
00:53:44,440 --> 00:53:49,600
That's one or two key words that shoot this I need to do. See that t minus one given why.

400
00:53:50,980 --> 00:53:56,860
So suppose I have the day that now today. And I want to predict something of sorry story.

401
00:53:57,550 --> 00:54:01,780
Bear. Suppose. To get.

402
00:54:04,690 --> 00:54:09,390
But the. And that's the movies.

403
00:54:09,410 --> 00:54:14,280
And the one is Q. Q TV.

404
00:54:18,850 --> 00:54:23,300
Well, how do you calculate the equity? Here is the formula.

405
00:54:23,320 --> 00:54:27,670
Right. The f t is b, t b.

406
00:54:27,670 --> 00:54:32,010
What is my beat? My beat is wrong, right?

407
00:54:32,140 --> 00:54:37,620
My beat is wrong here. And he made one.

408
00:54:41,740 --> 00:54:44,920
Liberty Rule Bar. Right.

409
00:54:44,950 --> 00:54:52,309
You remember that rule book? Oh, I figure out the first term.

410
00:54:52,310 --> 00:54:56,810
I need to do that. And now I. Oh, this is.

411
00:54:57,440 --> 00:55:04,410
Sorry. Messed up. This is not good.

412
00:55:17,570 --> 00:55:20,630
Do the calculation of Einstein's.

413
00:55:23,200 --> 00:55:31,320
Well. Bar duty bar is what d t bar is.

414
00:55:36,350 --> 00:56:01,430
Well. I'm sure all the right terms.

415
00:56:10,100 --> 00:56:14,660
Is this term. That's my ethic coming from.

416
00:56:16,280 --> 00:56:21,500
Depart is my f theta t. Which is the OC.

417
00:56:22,640 --> 00:56:31,670
Which is the. It's is my efforts.

418
00:56:45,270 --> 00:56:52,710
Cooper is my. Look. Okay.

419
00:56:56,830 --> 00:57:00,250
In the location a bit here. Okay.

420
00:57:00,870 --> 00:57:21,179
Then I need to plugin. To plug in my next H.

421
00:57:21,180 --> 00:57:33,830
Right. I need to be te that's my role so that I have.

422
00:57:39,500 --> 00:57:47,210
So the beach party is my role to c t minus one is the one I get from the previous update.

423
00:57:47,960 --> 00:57:51,170
So now transport is also big role.

424
00:57:52,040 --> 00:57:58,250
So that's the one I would calculate based on what I have based on this particular ball.

425
00:57:59,000 --> 00:58:02,550
Okay. So finally that you can check my formula.

426
00:58:02,570 --> 00:58:05,150
Maybe I did wrong, but hopefully I did. All right.

427
00:58:07,130 --> 00:58:16,880
So this is actually the the formula that suppose you have done this step, then you do this one step prediction, all sample prediction.

428
00:58:17,570 --> 00:58:19,660
Here is the prediction value.

429
00:58:19,670 --> 00:58:28,400
This is essentially like, you know, applying the formula or I just talk about that you can get to the common filter, which is quite simple.

430
00:58:28,790 --> 00:58:38,510
The role is the role is the the autocorrelation function and empty minus one is the common future obtained from previous step.

431
00:58:39,080 --> 00:58:47,629
The Mule. The Rover is the meaning of the arrow term c t is the prediction accuracy and the y.

432
00:58:47,630 --> 00:58:54,950
T is the actual observed value. F t is the predictive value from here, which is calculated by the formula.

433
00:58:54,950 --> 00:59:00,079
I talk about the general formula and CTE can be calculated at you.

434
00:59:00,080 --> 00:59:10,190
T minus one, you two minus one is given like this. Okay, so after you get all the formula here and including commas smoother, you can check it.

435
00:59:10,760 --> 00:59:17,690
I think I did. Hopefully I did. Right. So then you can essentially just prevent this.

436
00:59:17,900 --> 00:59:21,229
Okay. After you you do this iteration.

437
00:59:21,230 --> 00:59:24,500
And so what is this good for?

438
00:59:24,500 --> 00:59:27,200
The commerce motor starting value is the one,

439
00:59:27,200 --> 00:59:37,939
you know at the last the position of your comma future because when comma fielder runs to the end right so Sarah and given C y and that's

440
00:59:37,940 --> 00:59:46,730
basically the the result and the last step obtained by Common Future that would be the starting value for your common smoother moving backward.

441
00:59:47,150 --> 00:59:51,440
Okay. Now for the common filter, you do need the starting value.

442
00:59:52,760 --> 00:59:58,830
So what are the what is starting valid. The starting value would be like your M zero, right?

443
00:59:59,030 --> 01:00:09,079
Or M, what is the first prediction like you when you have one data point, what of what to be, you know, prediction of your state at zero.

444
01:00:09,080 --> 01:00:17,090
So typically people put to the the initial value as like M zero asked as one that's the mean of your

445
01:00:18,050 --> 01:00:23,060
so so the initial value to start is coming forward or it would be the meaning of your process.

446
01:00:23,060 --> 01:00:30,230
In this case the mean is one. So that could be the initial value you start from this process.

447
01:00:30,680 --> 01:00:35,840
Okay, so you have 88 is essentially give you a possible model.

448
01:00:36,140 --> 01:00:46,490
Suppose our four values are known so that you can always calculate your M's and MX zero is you know, is is one.

449
01:00:46,490 --> 01:00:58,760
Then that's the meaning of your process. Then you can always get this and when you have the mean, then the first prediction accuracy could be zero.

450
01:00:59,000 --> 01:01:02,400
Okay? So that 80 zero will be zero. Okay?

451
01:01:02,750 --> 01:01:09,800
So that you can start with this whole process and you know that move forward then backward.

452
01:01:10,070 --> 01:01:21,350
Okay. So you can start with M zero as one, which is the mean process is zero as zero because you don't have any variability for the first guess value.

453
01:01:22,370 --> 01:01:34,370
So that you can start with this and you can program this to essentially get this predictive value that's quite straightforward to do this.

454
01:01:36,640 --> 01:01:40,680
So in a partial regression.

455
01:01:40,690 --> 01:01:47,140
Right. So you could replace this by a common.

456
01:02:01,060 --> 01:02:09,190
Well, suppose I give you give you the data of the number of deaths in Michigan in the past six months.

457
01:02:09,880 --> 01:02:13,560
So you can use this model to figure out the.

458
01:02:18,490 --> 01:02:27,310
Oh. On a smoother as I if you program that as I get that this will be your offset right.

459
01:02:28,990 --> 01:02:32,080
This will be offset by your possible regression.

460
01:02:33,040 --> 01:02:38,300
Okay. Function to estimate the regression query.

461
01:02:39,470 --> 01:02:43,630
Offset. Okay.

462
01:02:44,330 --> 01:02:49,590
Look, the. Oh can be done using the existing software to do that.

463
01:02:49,870 --> 01:02:54,390
But the only thing you need to figure out here is how do you figure out that?

464
01:02:55,200 --> 01:02:58,640
Right. So that the way to figure out. Oh.

465
01:03:00,810 --> 01:03:06,060
So you gave me an initial value after I can do a few there or come a smoother

466
01:03:06,480 --> 01:03:12,860
that I treated they see that right block as my offset I run possum regression.

467
01:03:12,870 --> 01:03:16,409
I come back to important to regression.

468
01:03:16,410 --> 01:03:19,410
I opted my RFA after I get to my update,

469
01:03:19,410 --> 01:03:25,080
after I can come back to take the limited carbon filter and come on smoother and then

470
01:03:25,080 --> 01:03:31,210
treat as offset time in my to your question regression that I do estimation of our body.

471
01:03:31,640 --> 01:03:37,860
So I do this information until we get convergence so that simply can be done without

472
01:03:37,860 --> 01:03:44,550
using any sophisticated process where the ability can be treated as offset term.

473
01:03:45,510 --> 01:03:51,780
I like possible regression, but the key here is that you need to pull one is to figure out if.

474
01:03:53,420 --> 01:04:09,530
So suppose that we look at a time window from before and after the occurrence of only out of various war where we can look at the.

475
01:04:10,910 --> 01:04:18,800
Availability and see how that change this number of infections or number of death rate

476
01:04:19,370 --> 01:04:27,540
using this kind of distaste from this model system to understand the data on the.

477
01:04:32,070 --> 01:04:35,340
Well, the treatment, in fact, was some intervention.

478
01:04:37,650 --> 01:04:41,970
So that's certainly doable after you figure out this method.

479
01:04:42,000 --> 01:04:49,260
That's why I want you to learn this programing part and so that, you know,

480
01:04:50,370 --> 01:05:03,270
you know how you're going to use this model to estimate the parameters of interest in this study of, in fact, it is using a time search model.

481
01:05:06,070 --> 01:05:12,310
I did that using more sophisticated method. Right. But I will introduce this message later.

482
01:05:12,370 --> 01:05:22,540
But after we are able to get to the common future becoming smoother and then, you know, you basically impute missing value.

483
01:05:22,540 --> 01:05:30,010
If you think about that as a missing variable, missing data of your process, your computer using block.

484
01:05:34,870 --> 01:05:36,249
I think if you were incantation,

485
01:05:36,250 --> 01:05:44,110
you'd like a sort of IBM update that your imputation will update your offer because the common fields are well related,

486
01:05:44,590 --> 01:05:51,520
your common smoother is fields are related to the little party, which is functionally your offer,

487
01:05:51,640 --> 01:05:56,230
but you can't regularly update that until it reached to a stability.

488
01:05:56,450 --> 01:06:02,880
Okay, that's the time of convergence of the algorithm so that you can, you know.

489
01:06:07,540 --> 01:06:13,080
I will produce this three massive leader, this more systematic way to do that.

490
01:06:14,350 --> 01:06:27,010
But overall, it's the much simpler process after you figure out how to do common filter or this imputation method.

491
01:06:28,880 --> 01:06:33,480
I want to make a I couldn't I was deliberately rushing from his lecture.

492
01:06:33,480 --> 01:06:43,680
I want to finish those slides. Then I really like this kind of sort of state space model.

493
01:06:43,680 --> 01:06:54,719
And of course the here we talk about time as I said that you know, you can think about those T, but the T here could in a position in chromosome.

494
01:06:54,720 --> 01:07:02,070
Right it's not necessary at time you can think about this t is a location on a cross or

495
01:07:02,070 --> 01:07:10,160
something that the another system where you you have some kind of index variable to index the.

496
01:07:14,660 --> 01:07:26,100
Of. Aziz So you can use this kind of dynamic modeling here at the modeling to have something beyond its temporal time source data.

497
01:07:26,240 --> 01:07:37,770
Okay. Anyway, so I what when the here is that I want to mention that in the original part of data analysis we had very.

498
01:07:39,420 --> 01:07:43,350
Then full time trading. That's what now was the model set up by some observers.

499
01:07:44,700 --> 01:07:52,950
But I feel that that model can be a little bit more interpretable if you use this sort of broken

500
01:07:52,950 --> 01:08:01,290
speak model where you introduce some kind of ninja line to to to the modeling of Detroit.

501
01:08:01,470 --> 01:08:05,490
Right, right. Right. Now, using just is for twins.

502
01:08:05,670 --> 01:08:13,300
Right. So the broken speaking model. The.

503
01:08:17,370 --> 01:08:21,450
It's. Want to do it.

504
01:08:21,600 --> 01:08:25,270
Okay. Suppose that before I use just a minor train.

505
01:08:25,290 --> 01:08:34,740
So this is like this is 40 years of data of total sort of incidences, monthly incidents over 14 years.

506
01:08:35,170 --> 01:08:42,280
There's only one time between. Right. Which. Um, you know, a.

507
01:08:44,080 --> 01:08:50,379
But boy interpret boys that you have some a pretty good point I I to zero and 91.

508
01:08:50,380 --> 01:08:57,400
You know that is a time when the government stays shoulder to shoulder or you know,

509
01:08:57,760 --> 01:09:01,240
social distancing was announced and people start to have attention.

510
01:09:01,630 --> 01:09:06,150
So this is a point and this maybe is a time that vaccine is introduced.

511
01:09:06,610 --> 01:09:11,250
It change basically the way of immunity that community.

512
01:09:11,260 --> 01:09:26,490
Right so. In it.

513
01:09:26,820 --> 01:09:31,590
In this time source analysis you can broadcasting all.

514
01:09:31,730 --> 01:09:34,010
So before you have a continuous model,

515
01:09:34,080 --> 01:09:43,380
then you can put this into different pieces because they believe that in different time regions then you are small, are different.

516
01:09:44,610 --> 01:09:48,900
So that's the the point here. You can incorporate that broken city model.

517
01:09:56,940 --> 01:10:03,840
They observe the number of incidents or either weekly or daily incidents that you do.

518
01:10:04,230 --> 01:10:13,400
You know, the different time regions over which you have different policies or different dynamics of the organization.

519
01:10:13,560 --> 01:10:15,480
Well, this province devotes very useful,

520
01:10:15,480 --> 01:10:24,900
particularly to looking at the very complex system there involving different sorts of interventions imposed on government or by public health.

521
01:10:26,190 --> 01:10:31,150
So, so, so this is useful to the in the statistic.

522
01:10:31,160 --> 01:10:40,580
It is also called continuous supply. Basically use piece wise near.

523
01:10:44,620 --> 01:10:54,520
This is the the slope, right. Of sort of the action of sort certain pop.

524
01:10:55,240 --> 01:11:01,500
This is when this siege policies introduced that the twin camps and then the tents

525
01:11:01,510 --> 01:11:05,800
when there's an additional different policy from the regime was introduced.

526
01:11:06,190 --> 01:11:17,309
So you can capture those critical points. Snoop in different forms though.

527
01:11:17,310 --> 01:11:24,030
So this can be introduced a piecewise linear supply slider to to work on that.

528
01:11:25,620 --> 01:11:36,749
So this is something I, I like it because you can enjoy some better interpretation of what you find out rather than having a one hour slow parameter.

529
01:11:36,750 --> 01:11:42,840
We can have more different types according to different time of Windows.

530
01:11:44,250 --> 01:11:46,020
And now this is R1 process.

531
01:11:46,020 --> 01:11:54,690
Then we basically look at the discrete ization and I said that the underlying impulse is supposed to be a continuous time process.

532
01:11:54,690 --> 01:11:56,100
It changes every second.

533
01:11:56,520 --> 01:12:03,989
Right if you think about investors is particularly that Kobe but now when you do this error one process you still use the time unit

534
01:12:03,990 --> 01:12:13,410
one is not really continuous time thing but you could think about this is discrete size the version of the continuous process.

535
01:12:13,860 --> 01:12:18,000
So to some extent I think that it's very.

536
01:12:20,340 --> 01:12:30,210
This can be replaced by SDR or SDR or whatever emoticon part of the ball that we introduced.

537
01:12:31,050 --> 01:12:36,390
While you work on this mode command module, you have the plot using this de sol software.

538
01:12:36,750 --> 01:12:44,530
It is basically the. We're going to talk about Epsilon later.

539
01:12:44,740 --> 01:12:49,990
It's a evolution process of the cartoonist typefaces.

540
01:12:50,260 --> 01:12:53,770
But here is the screen time version of that.

541
01:12:53,770 --> 01:12:54,880
Based on the time you.

542
01:12:55,390 --> 01:13:01,540
Depending on how you define what what could be one minute it could be one day it could be one week in this part of the situation.

543
01:13:01,750 --> 01:13:09,130
What means one month? So you you can imagine that there's a continuous situation going on.

544
01:13:09,880 --> 01:13:17,150
But. If you looked at later dynamics based on the time you did the math.

545
01:13:17,590 --> 01:13:24,600
Okay. So so I think Malcolm Harmon could be very useful.

546
01:13:24,610 --> 01:13:32,440
I don't think anybody has ever worked out the common and smoother for this, you know, the continuous dynamic involved.

547
01:13:32,800 --> 01:13:47,200
But in principle, the common field or a common smoother does not require really a of at some term because you'll see that it's based on this.

548
01:13:52,210 --> 01:13:59,690
On this Nina system. Right. So.

549
01:14:00,590 --> 01:14:10,340
Not a minor system because the way you see this near. So so basically so is that how this operating system drives the observed system.

550
01:14:10,690 --> 01:14:16,880
Okay. It being here, there's no actually a specific specification of epsilon.

551
01:14:18,230 --> 01:14:21,350
Okay. So if you believe that, believe that t follows.

552
01:14:21,770 --> 01:14:27,290
It's our model S model uncontrolled on time basis you can work through.

553
01:14:27,410 --> 01:14:34,590
If you were willing to specify this kind of transition or hierarchy model, you can still work out what you.

554
01:14:36,390 --> 01:14:41,940
I'm a fielder and columnist, Monsieur. Do not directly require Epsom terms.

555
01:14:42,780 --> 01:14:49,740
If you have if some terms like this model to have some terms, you can work out to those first and second moment.

556
01:14:50,220 --> 01:14:54,320
Then you consider. But come I feel very smoother it themselves.

557
01:14:54,320 --> 01:15:01,850
So do not require is this. Specification of transition structure.

558
01:15:01,940 --> 01:15:15,670
Okay. So in the as I am almost confirm all of you that that that says that motor company modifies some countries transition so how to do that I,

559
01:15:16,160 --> 01:15:23,600
I think that would be big and advancing in the field if if someone can figure out how to do that to figure out this

560
01:15:23,600 --> 01:15:33,440
latent process in sort of connection to the common filter scooter so you can understand the lead impulses for.

561
01:15:35,580 --> 01:15:48,239
But I'm minutes so I won't talk about exciting things that I really like it as a toy example and sort of the Star Wars the Japanese statistician

562
01:15:48,240 --> 01:16:02,670
kid got it it took a our so we published paper 9787 and basically want to use this sort of the state space model to detect the change points.

563
01:16:02,880 --> 01:16:11,880
Okay he quickly this toy example is like for one, two or three change points basically from change from 0 to -1.

564
01:16:11,880 --> 01:16:20,400
This is abrupt change rather than smooth change and jump from 1 to 1 from one jump down to zero.

565
01:16:20,760 --> 01:16:29,910
So he's basically this to simulate simulated data and he wants to basically use this space model to estimate those jump.

566
01:16:31,300 --> 01:16:36,070
It's a one solution pass. You can simulate many different solution pass rate.

567
01:16:36,070 --> 01:16:48,380
So this is one real lazy realized you know parts of this like stochastic process he gener well he model this here it said that you know why

568
01:16:48,410 --> 01:16:59,260
he is almost say that he accepts some kind of the random noise right so so you have an underlying process theta t that's a random walk.

569
01:16:59,260 --> 01:17:03,430
It's almost the current values, almost same as previous value.

570
01:17:03,880 --> 01:17:07,330
You can just like a constant. Okay. Unless.

571
01:17:08,640 --> 01:17:16,110
Also somehow appreciate that there is a going up, immediate going up or down.

572
01:17:16,440 --> 01:17:24,270
So see that he is follows this Pearson description of Pearson system and how that has this distribution.

573
01:17:24,940 --> 01:17:28,050
Okay. This distribution is more like a continuous.

574
01:17:29,490 --> 01:17:33,150
Right. So it has a verb or.

575
01:17:37,350 --> 01:17:43,980
But now, same as description. It's like you have this very sharp.

576
01:17:48,940 --> 01:17:52,120
But computers were version of delta function.

577
01:17:52,420 --> 01:17:57,909
So why design? Is this a distribution given this risk given by this?

578
01:17:57,910 --> 01:18:04,130
In this, we say that the system will be very, very stable in that kind of equilibrium.

579
01:18:04,570 --> 01:18:14,680
So physician, unless the system immediate fills up or something different, then you give very big energy to change the system.

580
01:18:14,680 --> 01:18:18,130
This is like this virtual density.

581
01:18:18,550 --> 01:18:22,780
If this, you know, this system feels that there's a big chance on that.

582
01:18:22,780 --> 01:18:33,430
Right. So but this is a hierarchal model given see how you can generalize to this, some noise, normal fear of noise.

583
01:18:33,820 --> 01:18:41,290
And the second reason process is a random walk.

584
01:18:41,680 --> 01:18:50,500
This app dropped. You know, these are two shooters, according to Pearson, has a very, very short change.

585
01:18:51,940 --> 01:19:00,820
Now you can do this whole calculation and do common filter smoother and then you can figure out all those check point.

586
01:19:01,950 --> 01:19:07,050
And that can very easily identify rule change points with high precision.

587
01:19:07,600 --> 01:19:18,749
Okay. This is common schmoozer at this plant. And so very interesting part that I noticed from this is that, you know,

588
01:19:18,750 --> 01:19:24,570
this very famous coal coach combination signifying test the racism in genetics.

589
01:19:25,140 --> 01:19:32,640
Everybody use coach test to combine and correlate the P values to have a so.

590
01:19:35,470 --> 01:19:49,510
A test of covering more variables. So I was thinking that because the Q distribution has almost identical wholesale property as close distribution.

591
01:19:50,080 --> 01:19:51,910
I don't know. I've never studied this.

592
01:19:52,510 --> 01:20:02,560
Whether or not the Q distribution would be better than distribution or B less than I mean, worse than CO two, I don't know,

593
01:20:02,640 --> 01:20:12,340
like, but can figure out something and maybe we can have a Q Pearson combined test rather than consumer tests.

594
01:20:12,340 --> 01:20:17,940
But anyway, so, so what I show you is the basically the power of states.

595
01:20:17,950 --> 01:20:27,069
This model is not only working for a continuous dynamic system, but also work for some, you know, abstracted structural change.

596
01:20:27,070 --> 01:20:30,729
The system is very powerful, too. And of course,

597
01:20:30,730 --> 01:20:41,830
the question here is to what extent this system can be expanded to give with the data at the level of complexity where we face for the COVID 19.

598
01:20:41,830 --> 01:20:49,389
But I think this is very powerful system of modeling sort of structures.

599
01:20:49,390 --> 01:20:53,290
And so and we have come a few to a smoother in place.

600
01:20:53,500 --> 01:21:00,880
And later I introduce M.S. I also have very fast the sampling algorithm for states to be smaller.

601
01:21:01,060 --> 01:21:08,650
Those are the two boxes available for us either to go for and test inference where you go brace inference.

602
01:21:09,040 --> 01:21:16,730
So in the frontend is domain you have computers common field are a smoother in a Bayesian context you have in some C there.

603
01:21:16,750 --> 01:21:21,730
So this system is quite a I mean this kind of system model is quite useful.

604
01:21:22,210 --> 01:21:28,870
So that's why I spend a couple of time and today you derive those details for you and you can finish all

605
01:21:28,870 --> 01:21:36,820
the remaining derivations and homework to I want to practice programing and we'll see how that works.

606
01:21:37,990 --> 01:21:38,770
Thank you very much.

