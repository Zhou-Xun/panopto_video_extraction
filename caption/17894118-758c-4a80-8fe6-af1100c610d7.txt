1
00:00:03,410 --> 00:00:07,550
You know how you feel.

2
00:00:07,920 --> 00:00:17,310
Okay. So you'll is nothing for me at the start of this material now.

3
00:00:18,630 --> 00:00:29,340
Last time we actually went into recovery. No, we were talking about the universe or we have all this happening before for the revival of The Fader.

4
00:00:29,940 --> 00:00:34,200
Better. What's next for Vision for X?

5
00:00:37,080 --> 00:00:41,860
Now that we mentioned this, three methods. Any longer.

6
00:00:42,090 --> 00:00:45,419
Before we move on to a while, we're going to take a look at one example.

7
00:00:45,420 --> 00:00:47,790
So hopefully I've got an example to illustrate.

8
00:00:48,090 --> 00:00:58,080
I mean, here, as we mentioned, we do not have to introduce the very rude words, defamation of all these different concepts like that.

9
00:00:59,240 --> 00:01:02,910
What we're trying to do with that written is exactly what to evaluate.

10
00:01:02,910 --> 00:01:13,020
Mathematical P is defined, but I was trying to explain the idea of the intuition behind all these concepts.

11
00:01:13,290 --> 00:01:19,890
And then in the fall, in the next 20 minutes or so, we are going to take a look at one example, one big example.

12
00:01:20,340 --> 00:01:25,950
Hopefully that example illustrates all the essential ideas behind all these different concepts.

13
00:01:25,980 --> 00:01:30,510
But before we do that, any questions about these two things?

14
00:01:36,170 --> 00:01:45,100
Okay. So, yeah, let's do a few slides, maybe a few slides before that example, a couple of times before that example.

15
00:01:45,110 --> 00:01:53,569
But take a look at the way that hopefully the demo shows exactly how, what ideas are and how to apply those ideas.

16
00:01:53,570 --> 00:02:04,010
Let me rephrase now before we do that here, one thing is that now we were talking about testing, you know, beta one keyhole tool.

17
00:02:14,660 --> 00:02:20,310
I got you in last night. We were talking about casting a better one, equal to something to equal to some small b one.

18
00:02:21,280 --> 00:02:27,790
And the reason that we focused on testing be one thing I want you to be one is that they know what if you

19
00:02:27,790 --> 00:02:39,220
recall the regression model beta one is actually the recognition that quantified the effect of X on Y.

20
00:02:39,760 --> 00:02:46,540
So that is zero or the increased the measures and the big interest and beta zero testing beta zero here.

21
00:02:46,900 --> 00:02:54,340
This is sometimes people do those, but really not because I see this better often.

22
00:02:55,420 --> 00:03:03,489
But a bit here. I mean, this lies just fine is what this line is trying to say so that we can we definitely can't past beta zero equal to,

23
00:03:03,490 --> 00:03:11,080
you know, some given value be zero. And it's actually tested in almost exactly the same way as testing beta one.

24
00:03:11,080 --> 00:03:17,320
You got to be one so we can cast beta zero equal to zero by using the same statistic.

25
00:03:17,410 --> 00:03:22,600
Of course here we are using beta zero, have minus p zero,

26
00:03:22,600 --> 00:03:31,480
which is given by the null hypothesis 11 divided by the center error where the square root of the variance of beta zero hat.

27
00:03:31,720 --> 00:03:36,730
And this again follows a t this fusion with an minus to the your freedom.

28
00:03:37,060 --> 00:03:43,360
This is under under do not have a this is assuming that now because this is true this

29
00:03:43,360 --> 00:03:50,410
t sort of has to follow this t distribution and it does similarly to on to testing.

30
00:03:51,730 --> 00:03:58,840
I've been a Y equal to be one. We are able to apply the p value or construct a confidence interval or look out

31
00:03:58,840 --> 00:04:04,690
of the rejection region to decide whether to reject this null hypothesis or not.

32
00:04:05,440 --> 00:04:16,980
So what this largely is trying to say is that if you want to test for beta zero, then the test is almost exactly the same as past the test or balance.

33
00:04:21,070 --> 00:04:29,200
Yeah. As we mentioned, typically the interest is 3 to 1 because again, it quantifies the association between Y and X,

34
00:04:29,200 --> 00:04:34,529
that is the Ms. And if the question on the interest, however, I mean,

35
00:04:34,530 --> 00:04:39,339
even if you carry out some test, if you're testing whether Peter Beta zero,

36
00:04:39,340 --> 00:04:46,780
whether the indirect is equal to zero or not and you carry out this test, let's say that you fail to reject this test.

37
00:04:48,010 --> 00:04:53,560
So in other words, the data does not provide sufficient evidence.

38
00:04:53,920 --> 00:04:57,190
So you matched this is significantly different from zero.

39
00:04:57,520 --> 00:05:04,510
So the data says that, okay, so this might be zero or at least we do not have enough evidence to say that this is not equal to zero.

40
00:05:05,590 --> 00:05:10,900
So if we fail to reject this number, well, this is then the question is, okay,

41
00:05:11,290 --> 00:05:16,689
so do we fit this model weirdly fit this model because now the thing is as there

42
00:05:16,690 --> 00:05:24,220
is in the Peter well there is not enough evidence to file against this result.

43
00:05:26,950 --> 00:05:32,020
And in this case we will still like to set this model, still include of interest.

44
00:05:32,620 --> 00:05:37,029
We do not remove the reason.

45
00:05:37,030 --> 00:05:45,249
Well, there are two reasons. One is that removing this intercept will change the estimate of Peter for sure.

46
00:05:45,250 --> 00:05:49,800
So which is a better one for sure. And it will change the magnitude of the.

47
00:05:49,890 --> 00:05:52,240
And that's one reason.

48
00:05:52,240 --> 00:06:04,630
Another reason is that typically speaking, the model that we think should not be dictated by the data, we should not add to the model.

49
00:06:04,900 --> 00:06:11,830
And this is not absolute. So sometimes sometimes people do this, sometimes people I mean,

50
00:06:13,690 --> 00:06:18,940
so one thing oftentimes we will try to avoid is that now you have collected data

51
00:06:19,480 --> 00:06:28,209
and based on this single data that actually you you you refine your model,

52
00:06:28,210 --> 00:06:34,420
you, for example, you you think that about a zero by casting aa0 equal to zero you reject you your to

53
00:06:34,610 --> 00:06:38,950
and then you remove the interest happen from the from the from the from the model.

54
00:06:38,950 --> 00:06:47,790
And you only fit a model which mean a lot. So oftentimes you go try to avoid doing that because, because then your value,

55
00:06:47,810 --> 00:06:53,500
then you have the risk of overfitting the model using the current data because the current data as well.

56
00:06:53,500 --> 00:07:00,850
If you refine the model to model the current data, then you will have a you will have a model in the end that fits all the data better.

57
00:07:00,850 --> 00:07:03,730
Well, but then once you are given a new dataset,

58
00:07:03,970 --> 00:07:10,210
your model probably does not fare well to apply to a new dataset because the dataset remember each dataset you collect,

59
00:07:10,240 --> 00:07:13,270
there is randomness in the data, there is noise in your data.

60
00:07:13,720 --> 00:07:20,380
So if you refine our model too much based on this particular dataset, then it means you're trying to model the noise in this data.

61
00:07:21,070 --> 00:07:26,650
You probably you run that, you run the risk of overfitting, and then another dataset of clumps.

62
00:07:27,060 --> 00:07:31,630
Now there is a new noise arising from some extra noise in the other dataset.

63
00:07:31,900 --> 00:07:35,290
Very model probably does not apply well for the new dataset.

64
00:07:36,370 --> 00:07:41,979
So that's, that's one thing that we need to pay attention to, however, I mean,

65
00:07:41,980 --> 00:07:46,930
so I don't want to make this like, you know, this is the absolute rule that we have to follow.

66
00:07:47,110 --> 00:07:55,599
However, sometimes people do improve their model to do, you know, to do all types of model diagnostics based on this particular dataset,

67
00:07:55,600 --> 00:08:02,440
to do models of action, to do better, most likely to do things like that and a better fit a model on the same dataset.

68
00:08:03,160 --> 00:08:09,640
This is generally speaking, this is not like in terms of mathematics.

69
00:08:09,670 --> 00:08:14,710
This may not be absolutely correct in terms of mathematics, but if people do that.

70
00:08:15,100 --> 00:08:22,960
So over here, I think the point I'm trying to make is that now if we fail to reject this null hypothesis,

71
00:08:23,350 --> 00:08:27,370
well, typically speaking, we would still include of intercept in our model.

72
00:08:27,370 --> 00:08:35,970
We do not want to remove it completely. Okay.

73
00:08:36,420 --> 00:08:43,380
So this is. About has been the intercept.

74
00:08:43,410 --> 00:08:53,490
Now, let's take a look at one example. Hopefully this example shows all the essential ideas behind the public setting we talk about.

75
00:08:53,730 --> 00:09:01,790
So this is a study to examine the relationship between the child age and amount of sleep at night.

76
00:09:03,510 --> 00:09:07,560
Age is our X and amount of time asleep.

77
00:09:07,590 --> 00:09:14,000
That's that's our that's our one in the sample sizes and in group 15.

78
00:09:14,860 --> 00:09:25,919
So now after feeding the linear regression model, let's, let's forget about it how the model was a theory.

79
00:09:25,920 --> 00:09:30,430
I know we had an example, one from our model from model B, right.

80
00:09:30,450 --> 00:09:34,460
So that shows us how to film linear regression model using art.

81
00:09:34,470 --> 00:09:45,660
And later maybe today, maybe on the next year, the world can look at another example how to create model using R.

82
00:09:45,900 --> 00:09:52,560
But here now let's let's just say that it's after feeding in the model while we got all these results.

83
00:09:52,680 --> 00:09:58,049
And so the beta one had is estimated by one of the parents of beta.

84
00:09:58,050 --> 00:10:00,330
I had an this test made about 1 to 5.

85
00:10:01,860 --> 00:10:14,970
And again while we are given this three mouse present house of latest version that's only what this means is that suppose we have a tedious fusion.

86
00:10:17,250 --> 00:10:20,400
This is tedious fusion with 13 degrees of freedom.

87
00:10:20,970 --> 00:10:27,750
We consider this PD zero with 13 D freedom because the sample size is is 50.

88
00:10:28,050 --> 00:10:34,530
Remember that no other. Well, this has been a one thing we can look at.

89
00:10:39,040 --> 00:10:43,120
This is because you should call that.

90
00:10:44,560 --> 00:10:49,690
So under the null hypothesis right under not with it is that the the the test the

91
00:10:49,690 --> 00:10:53,830
senators the as follow team disclosure usually have understood your freedom.

92
00:10:54,040 --> 00:10:59,889
So that's why I'm here. In this particular example, we focus on this T distribution with certainty.

93
00:10:59,890 --> 00:11:05,680
We're afraid and because the sum size is equal to 15, so we keep minus two, let's add 13.

94
00:11:06,490 --> 00:11:19,720
And then what these somehow means is that okay, so, so here, if I look at this now 2.16 and -2.16,

95
00:11:20,800 --> 00:11:28,990
what this means is that the area to the right of two plus six, that's .025 here.

96
00:11:28,990 --> 00:11:39,460
The area here is one or two or five. So that's not not a 7.5 power of transmission, kind of this 95% how it is viewed.

97
00:11:39,470 --> 00:11:44,290
And that's 1.2771. That's that's somewhere we're working at 1.771.

98
00:11:44,530 --> 00:11:50,670
What this means is that the whole area to the right of one for the south central one, you know,

99
00:11:50,770 --> 00:12:01,480
this whole area here, that's that's .05 and that's .05, less than 95% out of Titan's division.

100
00:12:01,930 --> 00:12:06,280
And then there's 90% now that's 1.2 31.35.

101
00:12:06,280 --> 00:12:14,950
That's actually somewhere over here at 1.35. What this means is that an area two one to the right of 1.35, that's point five.

102
00:12:16,640 --> 00:12:22,250
That's the 3% power that we need to choose to use for the fall.

103
00:12:22,250 --> 00:12:28,940
And so we are assuming now the alpha level plus significance level is .05.05.

104
00:12:29,060 --> 00:12:35,280
So. Again, the significance level.

105
00:12:35,280 --> 00:12:47,930
What it means is that. I think we have to.

106
00:12:47,940 --> 00:12:56,219
I mean, while in order to explain this Alpha, we have to really tell about a type I never saw before.

107
00:12:56,220 --> 00:12:59,879
Both coverage of type on error will be in sync.

108
00:12:59,880 --> 00:13:07,470
So true. But Taiwan. Caribbean is the error that you make when they not have others is is true.

109
00:13:09,000 --> 00:13:12,000
But if you reject it, result will not happen. This is is true.

110
00:13:13,390 --> 00:13:18,900
You are supposed to not react. But if you unless they were redacted, then you are making an error.

111
00:13:19,290 --> 00:13:25,140
And that error is called a type one error. And we will ask that ever to be as low as possible.

112
00:13:25,890 --> 00:13:30,470
And so this alpha but sadly missed that this alpha is actually a typo.

113
00:13:30,480 --> 00:13:35,040
Never read it well under control and by setting alpha equals 2.05.

114
00:13:35,430 --> 00:13:43,890
We actually are saying that the pilot error is at a maximum point of five, so it's not going to exceed qualifier for this.

115
00:13:45,120 --> 00:13:51,259
So that's the reason why typically we said, oh, that would be a very low level 9.0, 5.1.

116
00:13:51,260 --> 00:13:56,579
What you probably see people do that for no one. But here, I mean, this works.

117
00:13:56,580 --> 00:14:05,110
We will need to focus on using Formula five. So now let's take a look at this part of this example.

118
00:14:06,250 --> 00:14:12,250
So it says that a rather skeptical condition claims that a study was a waste of time

119
00:14:12,560 --> 00:14:18,100
with the money because it's clear that sleeping patterns are not associated with age.

120
00:14:19,210 --> 00:14:25,510
So this is the claim of this condition. So there's no association between sleeping time and age.

121
00:14:26,470 --> 00:14:36,580
So in this case, the null hypothesis, the null hypothesis where testing is always.

122
00:14:37,950 --> 00:14:42,360
Oh, sorry. So. So then what is the is in this.

123
00:14:42,570 --> 00:14:46,860
In this case? Anybody would like to.

124
00:14:50,460 --> 00:14:54,890
Beside what are the novelists that they don't want to zero in on?

125
00:14:54,900 --> 00:14:56,010
What is it? Right.

126
00:14:56,370 --> 00:15:06,029
The reason is because if they don't want is actually zero of the slope in our model is actually zero, then x doesn't actually contribute to more Y.

127
00:15:06,030 --> 00:15:15,150
Yeah. Yeah, exactly. So in this case, a novel, this is a speed of one equal to zero because being a one quantified the association between Y and X.

128
00:15:15,480 --> 00:15:21,330
Right. And this clinician claims that A there is no association between one X.

129
00:15:21,600 --> 00:15:27,270
So in this case, but not in your passing, is that where the beta one is equal to zero?

130
00:15:27,420 --> 00:15:31,110
We're not. That's one where it hasn't. What about all the alternative?

131
00:15:35,130 --> 00:15:38,700
This one's not equal to zero beta one, not equal to zero.

132
00:15:38,910 --> 00:15:44,460
So why? What's the reason behind? Because we're saying that there's no association.

133
00:15:46,590 --> 00:15:50,219
Yeah, but that it actually is part of this, not a ballistics result.

134
00:15:50,220 --> 00:15:53,880
When we say there is no association, we're casting that model as a race.

135
00:15:53,910 --> 00:15:57,660
So I'm saying that there's no there is.

136
00:15:57,840 --> 00:16:03,810
We found that there is not an effect or there is an effect, which is that there will not be equal to zero.

137
00:16:04,260 --> 00:16:07,260
Yeah, that's right. So yeah, I, I got it.

138
00:16:07,260 --> 00:16:14,190
So here, the reason that the alternative might be that well is not even is there because there is no,

139
00:16:14,190 --> 00:16:20,819
I mean there is no prior information provided by this clinician while saying that if this is not equal to zero,

140
00:16:20,820 --> 00:16:25,750
whether it should the pope, ultimately this would be positively associated or negatively associated.

141
00:16:25,780 --> 00:16:31,290
Right. So there is no no. So to claim the claim is simply that there is no association.

142
00:16:31,290 --> 00:16:34,890
So the alternative is that there is some association.

143
00:16:35,160 --> 00:16:41,940
But whether that's positive or negative result, then the alternative is that, you know, one is not equal to zero.

144
00:16:42,810 --> 00:16:49,800
So they're testing even zero against not equal to zero and of a past a statistic.

145
00:16:50,900 --> 00:16:57,280
That's that's the key statistic. We talk about that.

146
00:16:59,370 --> 00:17:09,340
But you know what had made us? One divided by the estimated to be out of beta one out.

147
00:17:11,650 --> 00:17:18,730
Now of course in this case under the map of this is beta one is equal to zero because we hypothesized been away from zero.

148
00:17:19,750 --> 00:17:28,980
Beta Y is equal to zero. So it's beta 100 divided by square root of well this is as with centripetal beta had

149
00:17:29,560 --> 00:17:36,760
not been a one have is a given so that's given that's one divided by square root of.

150
00:17:37,930 --> 00:17:44,990
There is a 700 miles, so that's one over 2.5.

151
00:17:45,020 --> 00:17:48,940
That's two. That's one of the past two years.

152
00:17:53,000 --> 00:17:54,830
So that's the that's has the statistic.

153
00:17:55,310 --> 00:18:02,750
So the reference to this fusion now that is actually what this fusion this passes statistic you should follow on

154
00:18:02,750 --> 00:18:09,890
reliable is this as we mentioned impulsivity this fusion right here is usually within a minus two degree freedom.

155
00:18:10,370 --> 00:18:18,080
So the reference device usually in this case is tied is fusion with minus two, your freedom ten is 15.

156
00:18:18,140 --> 00:18:22,370
So it should be transferred into 13 C where freedom?

157
00:18:26,230 --> 00:18:29,860
Okay. So then let's construct the rejection region.

158
00:18:30,970 --> 00:18:35,350
So what do we mean by a rejection region? The rejection region, naturally.

159
00:18:36,210 --> 00:18:43,720
Well, the causes of the extreme matters. In this case, we are using.

160
00:18:46,010 --> 00:18:57,430
Now because. It's easier because this is Tito's version with 13 degrees of freedom.

161
00:18:57,820 --> 00:19:02,050
And so it's symmetric around about a zero is symmetric.

162
00:19:03,310 --> 00:19:08,650
Now we are looking at a two sided house. Right. So there is no like this is two sided.

163
00:19:08,650 --> 00:19:14,500
It doesn't say whether beta is positive or negative. So we simply say that beta one is it's not equal to zero.

164
00:19:14,920 --> 00:19:21,130
In this case, when we look at the rejection region, we need to look at both tests, the both counts.

165
00:19:21,820 --> 00:19:30,160
These are the relation regional tables. And then we want to keep Elva, remember, we want to queue 11 to be equal, to put all five.

166
00:19:30,910 --> 00:19:37,570
Now we are looking at foothills. So that means once we have Edith Hill it will be .025.05.

167
00:19:39,340 --> 00:19:47,350
And that means we are actually looking at this compound of this personality, this version that's 2.16.

168
00:19:49,510 --> 00:19:53,020
That's true. 116 here is -2.16.

169
00:19:54,890 --> 00:20:02,600
So that, you know, the area, the area to the right of this desert pole and all of that is is alpha in alpha or two.

170
00:20:02,810 --> 00:20:04,400
And because we are looking at the foothills.

171
00:20:06,450 --> 00:20:18,510
And then the rejection region are actually the region to the right on 2.16 and of a region to the left of 19 2.16.

172
00:20:18,960 --> 00:20:22,110
Because these are going to be extreme models.

173
00:20:22,590 --> 00:20:33,220
But in extreme matters, what we mean is that the values that falls to the right of 2.164 falls to the left of Nadia to 2.8.

174
00:20:33,240 --> 00:20:36,480
We can see there, we consider that to be extreme matters.

175
00:20:36,810 --> 00:20:40,560
They are very unlikely to be observed under this chief exposure.

176
00:20:41,040 --> 00:20:47,519
I think the more likely values are the values in the middle is a lot more likely

177
00:20:47,520 --> 00:20:54,720
to observe a value around 000 rather than observing a value less a thousand.

178
00:20:55,320 --> 00:21:01,530
But this far to the right to the right here, so is more likely to observe value in the middle of this distortion.

179
00:21:02,070 --> 00:21:09,330
So in this case, now for the values that are above 2.16 or below -2.6,

180
00:21:09,540 --> 00:21:15,870
we only have in total we only have, you know, point all five probability of observing those values.

181
00:21:16,620 --> 00:21:22,560
So we consider them to be extreme, less likely to be like very, very unlikely to be observed.

182
00:21:23,370 --> 00:21:32,009
So in this case, that all of the values that fall either to the arrival principle and tell, we'll consider them to be extreme values.

183
00:21:32,010 --> 00:21:35,430
And these values are the values in the rejection region.

184
00:21:35,910 --> 00:21:41,460
So in this case, the region and region causes of all the values to the left of.

185
00:21:44,220 --> 00:21:48,480
2516116. Or to the right of.

186
00:21:49,830 --> 00:21:53,580
Supervisor. This is not rejection region.

187
00:21:54,510 --> 00:22:02,829
So if in other words, if the observer team flows into this revival region, then we're seeing this observe.

188
00:22:02,830 --> 00:22:06,510
Value is extreme. It's very unlikely to be observed.

189
00:22:07,110 --> 00:22:10,200
If are anomalies, this is true. Right.

190
00:22:10,470 --> 00:22:13,530
So in this case we see that t t is equal to two.

191
00:22:14,130 --> 00:22:17,370
True. AJ, it does not fall into this region.

192
00:22:17,390 --> 00:22:22,080
Region. Right. So let's say these are two D.

193
00:22:22,080 --> 00:22:27,780
No, there's a rejection region. Then we see that he does not belong to this rejection region.

194
00:22:29,430 --> 00:22:32,690
And he's is somewhere. Somewhere.

195
00:22:32,700 --> 00:22:40,500
He had a team of two somewhere here. So in this case, what it means is that this value is not that extreme.

196
00:22:41,130 --> 00:22:45,110
So it's not that unlikely to be observed even.

197
00:22:45,340 --> 00:22:54,360
But this is. So, in other words, our data does not provide, you know, very strong evidence against this,

198
00:22:54,360 --> 00:23:01,850
not how well is this, because under this not well, this x well is not of that, in my view to observe t through.

199
00:23:03,220 --> 00:23:11,370
And so when we fail to reject reliable is because our data does not provide absolute evidence against monopolists.

200
00:23:12,630 --> 00:23:17,520
So in this case, the decision is that we fail to reject.

201
00:23:21,500 --> 00:23:30,489
Not a business. Okay.

202
00:23:30,490 --> 00:23:33,640
So can everybody follow us on example?

203
00:23:36,510 --> 00:23:42,150
Okay. Now let's take a look at another example.

204
00:23:42,160 --> 00:23:51,700
So now, Faisal. First of all, you see there is a pediatrician believes that the sleep time increases with age, increases with age.

205
00:23:53,350 --> 00:24:01,900
So now we want to pass this rule to family that has formulated this this belief as well as housing problem.

206
00:24:02,620 --> 00:24:11,020
So then what is the of this? What should we not have with us in this case?

207
00:24:16,730 --> 00:24:20,660
Income 00000.

208
00:24:20,720 --> 00:24:26,810
Right. So, yeah, in this case, the novelist is still the same as the last example.

209
00:24:27,590 --> 00:24:32,050
So the still work passive, there is no association between what and X.

210
00:24:33,320 --> 00:24:38,840
And so the novel is and still says there is no association. However, what changes is the alternative?

211
00:24:40,880 --> 00:24:42,860
Alternative? So what should be an alternative?

212
00:24:43,790 --> 00:24:54,950
Now get 00000 because as that's the number of these inefficient buildings, rather believes the sleep time increases with age.

213
00:24:55,640 --> 00:25:00,440
So alternative is not a better one.

214
00:25:00,440 --> 00:25:06,709
Learn as you were testing of either beta one while either there is no association

215
00:25:06,710 --> 00:25:10,430
or even tell you association that the association should be positive.

216
00:25:11,840 --> 00:25:15,499
So that's what we are testing. Okay.

217
00:25:15,500 --> 00:25:19,910
So for this for testing this idea, we're still using the same test as a statistic.

218
00:25:20,480 --> 00:25:26,450
So still is still equal to beta one hedge minus beta one divided by.

219
00:25:38,240 --> 00:25:40,250
We have already calculated it would.

220
00:25:45,360 --> 00:25:54,870
Not only is this not I mean this is we are still comparing in true institution with 32 of freedom 32 more freedom.

221
00:25:56,160 --> 00:26:06,270
But now what it changes from this example is as in the rejection region, because we are passing against this alternative.

222
00:26:08,340 --> 00:26:15,030
So again, if we make a block, it makes things a little bit different.

223
00:26:15,810 --> 00:26:20,480
So this is the tedious fusion with the party, but you are afraid.

224
00:26:24,930 --> 00:26:28,620
In this case, the rejection region should again,

225
00:26:28,620 --> 00:26:36,780
while the relaxed region always consists of fabulous and I would consider extreme in consider that on a likely to to be not you know to be that

226
00:26:37,170 --> 00:26:45,450
observations are not but this is but in this case the association is either while there is no association or I mean if there is an association,

227
00:26:45,460 --> 00:26:48,210
the association needs to be positive or it has to be positive.

228
00:26:48,660 --> 00:26:55,080
So in this case that you see now, those would be the values, you know, very positive, like positive, extreme.

229
00:26:55,290 --> 00:26:59,700
So in this case, the extreme values would be the battles with a right hand.

230
00:27:00,890 --> 00:27:07,270
Right. So the lieutenant, they they wouldn't be extreme matters because we are testing whether beta one is equal to zero.

231
00:27:07,760 --> 00:27:14,840
Yes. Bill on. So we are we're making an assessment of which one is more likely to be the case.

232
00:27:16,130 --> 00:27:26,180
So in this case now, if we still want to keep the high whenever 11.05, that means we want to make this area.

233
00:27:27,850 --> 00:27:34,100
The right held to be impossible. Five. So we're not looking at the foothills, but we're looking at just one hill.

234
00:27:34,520 --> 00:27:38,419
That's actually again, that's determined by the direction of this.

235
00:27:38,420 --> 00:27:50,840
I'll turn to hypothesis. And then while this tedious fusion, you know, actually this principle about the five principle, that's 1.2771.

236
00:27:54,290 --> 00:28:02,029
Right. So in this case, then the rejection region or the values to the right of one common sense and one those values,

237
00:28:02,030 --> 00:28:09,950
we consider them to be extreme matters that are unlikely to occur or are likely to to be observed under this t description.

238
00:28:10,250 --> 00:28:17,000
So the right region are the values to the right of for the site some one.

239
00:28:17,480 --> 00:28:24,770
And again, this rejection region is determined by this alternative hypothesis.

240
00:28:24,770 --> 00:28:31,010
Right. The reason that now we are only looking out of the right, the tail that is determined by this direction of this alternative.

241
00:28:33,980 --> 00:28:44,809
Okay. So now if we again, if we use R to denote this reaction region, then we can see that we see that of this piece that has its value is to it.

242
00:28:44,810 --> 00:28:48,290
It does fall inside the reaction.

243
00:28:50,480 --> 00:29:00,530
So what this means is that now this value achieved indeed in this case is considered an extreme out of its files.

244
00:29:00,570 --> 00:29:03,100
Inside of the reactor is a symbol.

245
00:29:03,890 --> 00:29:15,490
So what it means is that under of a lot of others, this and even a lot of others this is true, which is very unlikely to observe this t equal to.

246
00:29:16,960 --> 00:29:22,000
Because the value of observing this titular tool is less than 45.

247
00:29:22,000 --> 00:29:26,380
I believe the team too is somewhere in somewhere here.

248
00:29:27,310 --> 00:29:33,520
Somewhere here. Observing, observing, probably observing this team tool that's very small.

249
00:29:34,270 --> 00:29:39,660
So that's very unlikely to be observed. And if this novel is this is true.

250
00:29:40,700 --> 00:29:46,220
But as we observe this to you and to. RADDATZ, That's what what a value we've got based on our data.

251
00:29:47,750 --> 00:29:56,150
So this is absolutely sensible. Then what it means is that the data has really provided evidence against this novel disease.

252
00:29:57,770 --> 00:30:03,360
And because because if not. Well, this is true. It's very likely to observe to you to do.

253
00:30:03,560 --> 00:30:12,010
But actually, we did observe to you. So which means that it's not a so I like so that it is more than not.

254
00:30:12,140 --> 00:30:16,070
Well, this is what is unlikely to halt. Right.

255
00:30:16,310 --> 00:30:20,630
So this means that data provides evidence against the liabilities.

256
00:30:21,590 --> 00:30:25,950
So what it means is. Well, in this case, we reject this.

257
00:30:31,020 --> 00:30:40,360
We reject this. Word.

258
00:30:40,830 --> 00:30:42,660
I mean, you know, you could actually.

259
00:30:43,050 --> 00:30:50,030
I mean, there are there are different ways of saying there's of course, the simpler way of saying this is that what we do down a lot with us is right.

260
00:30:50,040 --> 00:30:57,360
But you could also say, you know, the data provides.

261
00:31:01,530 --> 00:31:09,220
To deliver that evidence against the analysis.

262
00:31:14,910 --> 00:31:22,750
That's that's the reason why we reject it, because our data shows that there are strong evidence against anomalies.

263
00:31:31,600 --> 00:31:36,190
Okay. CNET's this example or in terms I mean,

264
00:31:36,190 --> 00:31:46,240
in terms of the the belief of this pediatrician that what it means that while indeed he believes is kind of reasonable.

265
00:31:46,450 --> 00:31:55,510
So there is that you know, there is there is strong evidence actually supporting supporting his claim that an association is positive.

266
00:32:02,160 --> 00:32:07,170
Any questions about this part? Can can another hypothesis be.

267
00:32:07,720 --> 00:32:17,760
Peter, one last. What are you going to zero accused of? Because my my my question is why do we reject the null hypothesis in this case?

268
00:32:17,760 --> 00:32:22,560
But we reject the hypothesis, but we fail to reach the null hypothesis in the last case.

269
00:32:25,150 --> 00:32:30,620
Of. Well while here we reject you that because.

270
00:32:30,710 --> 00:32:40,790
Well, I guess the most straight answer is straightforward is that because this tea falls inside of that rejection region.

271
00:32:41,070 --> 00:32:46,820
Yeah, but but in the last in the last example, it does not fall inside the reaction region.

272
00:32:48,440 --> 00:32:53,780
So that that's actually based on our calculation. So here I repeat what he does not change.

273
00:32:53,960 --> 00:33:04,160
He is able to, in both examples for the region and region has to change because because of the difference in the specification of the alternative.

274
00:33:05,840 --> 00:33:09,440
So in this case, well, we look at of this two side, two sided alternative.

275
00:33:09,770 --> 00:33:17,300
The relation region is skewed by this. In the second example, the Latin region is given is given by this.

276
00:33:18,350 --> 00:33:24,020
So can the null hypothesis. Hypothesis B be the one last equal to zero in this case.

277
00:33:24,500 --> 00:33:35,690
Oh, okay. No, no, that's the now. But this is is always is always maybe the one equal to something.

278
00:33:37,600 --> 00:33:45,010
So the problem is this is never going to be you know, they don't want less than or equal to be one or larger and we're going to be one.

279
00:33:45,160 --> 00:33:49,420
So there there's there shouldn't be any inequality in that in a novelist.

280
00:33:49,690 --> 00:33:58,570
But novelist, as he's always been a one equal, was something of a something could be zero or could be some other numbers will be one example later.

281
00:33:59,680 --> 00:34:04,740
Now, the reason is that, I mean, there are there are different ideas.

282
00:34:04,760 --> 00:34:09,790
There are different ways of different explanations.

283
00:34:10,270 --> 00:34:18,260
So one explanation is that, well, we have to in order to make the past, we have to read it.

284
00:34:18,430 --> 00:34:21,730
What we have to find about the dispute of this team.

285
00:34:22,520 --> 00:34:27,400
Well, for example, in this case, the reference for his TV on TV is 30 word freedom.

286
00:34:28,420 --> 00:34:35,680
So we are able to find out this fusion only. I mean, when the beta value is given is a specific amount.

287
00:34:36,520 --> 00:34:41,590
So there is no way that we are able to find about the description of this test statistic.

288
00:34:42,070 --> 00:34:46,870
If this if that, if we are simply assuming beta one is less than less,

289
00:34:46,870 --> 00:34:56,290
might be one because we actually go because then depending on what what beta one really is, this is going to be a very real change.

290
00:34:57,400 --> 00:35:04,270
So that's one explanation. Another explanation is more more of a scientific explanation.

291
00:35:04,270 --> 00:35:13,150
So the null hypothesis, as the name suggests, all URI it is specified as beta one equal to zero.

292
00:35:15,460 --> 00:35:19,160
What it means is that there is no effect. There's zero effect.

293
00:35:19,670 --> 00:35:27,170
There's the for especially Vertex hasn't for example, this year it hasn't the association between between age and asleep.

294
00:35:28,040 --> 00:35:36,019
So a novel this is always says there's no association, there's no effect, there's there's no relation to this and abide by this.

295
00:35:36,020 --> 00:35:42,800
No relationship, there's no effect. That simply means that the data is equal to zero in passing that being revealed.

296
00:35:43,940 --> 00:35:52,069
So this is another explanation why the novelist is always in a form of beta one equal to something,

297
00:35:52,070 --> 00:35:56,960
that something could be zero or could be something else, which we will take a look at.

298
00:35:57,410 --> 00:36:01,190
One example for a novelist does not involve inequality.

299
00:36:03,110 --> 00:36:08,960
Yeah, but actually in the first case, we accept beta one.

300
00:36:11,870 --> 00:36:15,100
It goes to zero. But in the second case, we accept.

301
00:36:15,500 --> 00:36:20,270
Beta one is bigger than zero. Is that not terribly contradictory?

302
00:36:20,900 --> 00:36:27,430
Because we have the same we assume the same arc. Although there is no correlation.

303
00:36:27,820 --> 00:36:36,670
The reason is that the reason is that okay, so the alternative we specify we are different.

304
00:36:37,180 --> 00:36:44,079
So in other words, the, the amount of information provided by the description of the problem is different.

305
00:36:44,080 --> 00:36:47,890
So in this description in part A in this description,

306
00:36:48,370 --> 00:36:57,970
it is simply says that while the this and this clinician believe that there is no association, that's why his belief there is no association.

307
00:37:00,640 --> 00:37:07,930
So in other words, now, if there is an association, this clinician didn't tell whether it's positive or negative.

308
00:37:08,480 --> 00:37:15,250
Right. So so the community has no knowledge, has has no information about whether the association should be positive or negative.

309
00:37:15,790 --> 00:37:21,729
And that's that's what what information we have from heartache for the parent.

310
00:37:21,730 --> 00:37:29,280
B, you can see that of this pediatrician and believe that while the speech should increase with action.

311
00:37:30,160 --> 00:37:40,840
So now, of course, there's there's no way that so what we're having is that either this is zero or if it is not zero, then it has to be positive.

312
00:37:41,950 --> 00:37:47,910
So it's actually this difference that makes the results look different because the

313
00:37:47,980 --> 00:37:52,300
amount of information we have based on the description of the problem is different.

314
00:37:53,010 --> 00:37:59,890
And so the same in a second example, hearing a silent part here, the information is very specific.

315
00:38:00,520 --> 00:38:05,800
So either there is no association or if there is a association or association, it needs to be multi.

316
00:38:13,770 --> 00:38:19,280
Any other questions? Because.

317
00:38:21,890 --> 00:38:24,140
Yes, indeed. This is of these two parts.

318
00:38:24,140 --> 00:38:30,920
They are very good examples showing that your conclusion might change depending on the way you specify the hypotheses.

319
00:38:30,920 --> 00:38:38,540
Right, even further. That's very reasonable because I mean, even for the same data that if we are casting different hypotheses and there is,

320
00:38:38,540 --> 00:38:42,290
there is no reason to believe that you always reached the same conclusion.

321
00:38:42,320 --> 00:38:46,790
And I fear and neither we are casting to develop alternative hypotheses.

322
00:38:47,000 --> 00:38:52,280
One is that a better one simply not able to zero one is not better.

323
00:38:52,280 --> 00:38:56,840
One is going to reach different conclusions and.

324
00:39:00,430 --> 00:39:04,580
Okay. Another part of the example. So now we're testing.

325
00:39:06,490 --> 00:39:15,160
Well, a previous study found that the average amount of sleep per night increased by 1.5 minutes per year of age.

326
00:39:17,420 --> 00:39:23,940
Okay. So then. What should be the null hypothesis in this case?

327
00:39:30,060 --> 00:39:36,150
The walk through the testing. Should there be favor?

328
00:39:36,160 --> 00:39:41,350
One is equal to 1.5. Yeah. Beta one. You do 1.5, right.

329
00:39:41,830 --> 00:39:44,940
Yeah. Because that's how it was given. So the previous,

330
00:39:44,950 --> 00:39:52,569
this previous study found that well the previous study found that a beta one equal 2.1 of 1.5 and now

331
00:39:52,570 --> 00:39:59,770
with our data will test whether indeed that's the case and that's what we are passing on an alternative.

332
00:40:01,740 --> 00:40:07,350
Well, because there is no other information provided to us. So that alternative is simply not better.

333
00:40:07,380 --> 00:40:10,710
One is not equal to 1.5.

334
00:40:11,310 --> 00:40:15,270
So that is what we are casting. This particular example.

335
00:40:17,160 --> 00:40:20,340
And the test statistic is still the same as before.

336
00:40:20,610 --> 00:40:32,390
That one had a minus beta one divided by. On Beta one now is not equal to zero anymore.

337
00:40:32,660 --> 00:40:35,870
One is equal to 1.5. So here we have Peter.

338
00:40:35,870 --> 00:40:42,350
One hat is equal to one -1.5 divided by square root of 2.25.

339
00:40:42,830 --> 00:40:46,460
That's 19.5 divided by 25.

340
00:40:47,720 --> 00:40:55,830
Now, he was. And this is the observer value.

341
00:40:55,840 --> 00:41:05,310
It has the statistic. Now, the distribution is still a tedious filter with searching for you are still inline as to your freedom.

342
00:41:06,760 --> 00:41:19,530
Another rejection region in this case. Is the same as the first example because we are having this two sided test.

343
00:41:21,160 --> 00:41:28,300
We're having this two sided test. So the values on both tables, we consider that to be Israel.

344
00:41:29,290 --> 00:41:35,690
And so in this case, the rejection region would still be valid to the right of one.

345
00:41:35,860 --> 00:41:39,880
This is one of the five most important cultural five.

346
00:41:41,080 --> 00:41:44,110
This is negative woman six.

347
00:41:44,290 --> 00:41:49,870
Positive two. Bonds excluding. So the region is still.

348
00:41:56,430 --> 00:42:00,990
The students are the same as in the first example.

349
00:42:02,090 --> 00:42:12,270
Simply because, again, we are testing to say you have this alternative hypothesis and then we see that if this negative one,

350
00:42:12,820 --> 00:42:17,960
it does not fall inside of the rejection region. It does not.

351
00:42:19,020 --> 00:42:26,550
So what it means is that it's not a street map, so it's not a library to observe this negative one.

352
00:42:27,090 --> 00:42:34,710
If the problem is this is true and so are the words, our data does not provide strong evidence against novelists.

353
00:42:37,080 --> 00:42:48,720
He does not belong to the reactionary. And so we fail to reject the novelists.

354
00:43:00,070 --> 00:43:09,040
So we filter it. So in other words, now we have our data does not provide strong evidence against novel disease.

355
00:43:11,480 --> 00:43:20,390
We're in other words, internists are doing like this anyway, a problem that means are they to actually fall like this 1.5 risk parallel with our data.

356
00:43:20,660 --> 00:43:25,730
So our data has a support that hasn't been at one people from 1.5.

357
00:43:32,780 --> 00:43:41,420
Look at this example and then let's take a look at of the following parts.

358
00:43:41,870 --> 00:43:48,450
So based on the model, we want to estimate the difference between five and nine year old average.

359
00:43:48,650 --> 00:43:49,310
So we got.

360
00:43:51,150 --> 00:44:08,550
You'll recall that the model they're building is expanded on Y given x equal to zero plus beta one times x, and that's the model we are building for.

361
00:44:09,240 --> 00:44:14,850
If you recall the model, that's. Plus Amazon.

362
00:44:15,360 --> 00:44:19,010
But nobody thinks about internationalizing music.

363
00:44:19,130 --> 00:44:22,610
So that's why I hear and in a way is equal to this.

364
00:44:23,250 --> 00:44:30,620
But if you want to find the average difference between like children with five years old and nine years old,

365
00:44:31,040 --> 00:44:37,340
then what we want is why give an X equal to say, nine years old minus?

366
00:44:38,060 --> 00:44:44,520
Why do you ask people who five years old? Right.

367
00:44:44,920 --> 00:44:51,729
And what else is equal to nine? That's better. Zero plus nine times x nine times better.

368
00:44:51,730 --> 00:44:57,200
What? Minus the bone.

369
00:44:57,340 --> 00:45:02,050
And so you have to find this fun time speed of one. So that's out of the four has been a one.

370
00:45:07,780 --> 00:45:12,730
Right. That's what happens, Bill. So this is actually this work has been a one.

371
00:45:13,750 --> 00:45:20,170
It represents the difference between between five years old and nine years old.

372
00:45:20,380 --> 00:45:26,830
What is that average sleep? That's the policy that we are interested in in this case.

373
00:45:28,840 --> 00:45:35,860
And then we want to ask me this quantity, right? Because this what this first of all, we know that this represents the quality we are looking for.

374
00:45:35,890 --> 00:45:41,770
We are searching for it. So that's the difference in sleep between five years old and nine years old.

375
00:45:42,170 --> 00:45:47,500
But now we need to estimate it and we want to construct 95% of common this year.

376
00:45:50,560 --> 00:45:56,860
And the estimated estimate is really easy to look at it because we have already got a low

377
00:45:56,860 --> 00:46:02,190
estimate of for beta one friends in beta one and so far more than four times beta one half.

378
00:46:02,260 --> 00:46:07,430
That's actually equal to four times one. That's. Just before.

379
00:46:09,460 --> 00:46:16,150
And that's that's actually the average the estimated difference in average sleep time

380
00:46:16,930 --> 00:46:21,430
between between five years old in children and five year old and children that yes,

381
00:46:21,940 --> 00:46:26,680
that's the estimated value and 95% confidence interval.

382
00:46:28,140 --> 00:46:36,180
Well, recall that we talk about that, but there is a formula for 95% confidence interval that's given here.

383
00:46:37,110 --> 00:46:42,030
That's the estimate now, 95% confidence interval for beta one.

384
00:46:42,340 --> 00:46:57,840
That's the estimated value. Then minus four plus, you know, this t this present power, this 97.5% of times the center error, times the center error.

385
00:46:58,490 --> 00:47:01,980
And so in our case, well, let's make it.

386
00:47:03,210 --> 00:47:12,210
But this is actually the center error. So in our case, because we are estimating for Hans-Peter.

387
00:47:14,090 --> 00:47:25,700
For it happens. And so that's so the 95% confidence interval that's going to be four times beat on one hand plus minus the key.

388
00:47:26,960 --> 00:47:31,880
13 You were afraid on point nine, seven, five this person now because we are looking at foothills.

389
00:47:32,090 --> 00:47:42,920
So that's why we are using this environment for fibers and how times the center error of four they one of this aspect.

390
00:47:46,030 --> 00:47:49,570
And that should give us that 95% confidence interval.

391
00:47:52,370 --> 00:47:56,930
And then now we have the values for every quality here that we can calculate.

392
00:47:57,320 --> 00:48:06,620
So this is four times beta one half of that's for we have already calculated that plus minus t 30 do our freedom.

393
00:48:07,460 --> 00:48:11,030
The 97.5 for the tab is 2.16.

394
00:48:13,590 --> 00:48:24,640
The standard error. The standard error of four has been a one that's square root of the variance of four times greater one.

395
00:48:32,730 --> 00:48:42,040
But the bearers of wartime speed are what those if you apply the property of the IRS, that's going to be out of the closet before it can be moved out.

396
00:48:42,040 --> 00:48:46,200
And that's 16 times. The V.A., as we know, better run.

397
00:48:49,450 --> 00:48:56,670
I. And better. That's that's four square a square four times the difference.

398
00:48:57,210 --> 00:49:01,410
And again that's the variances point to five.

399
00:49:01,410 --> 00:49:04,980
So it's 16 times 1 to 5. That's equal to four.

400
00:49:09,020 --> 00:49:15,969
Right. Even before. So this whole thing that's four plus minus two .2.1.

401
00:49:15,970 --> 00:49:19,600
Six times the square root of four. The square root of.

402
00:49:28,140 --> 00:49:39,840
Now that he has, he got a lot of this confidence in our ways. And that's how we calculated the 95% of those.

403
00:49:39,940 --> 00:49:45,540
Or maybe let's let's do a calculation because I think, well, you're going to need to use it.

404
00:49:45,900 --> 00:49:54,480
So this is equal to this, and then this is equal to the four plus -2.1, six times times two.

405
00:49:56,690 --> 00:50:02,130
And that's after the four plus minus. This is all this is four point.

406
00:50:22,100 --> 00:50:29,840
Amanda. I'm not giving this crap, honestly.

407
00:50:29,850 --> 00:50:50,210
So we got to spell check. So it's.

408
00:50:58,690 --> 00:51:04,329
This is not happening. This is normal. This is the 95% of countries.

409
00:51:04,330 --> 00:51:22,220
Not. Okay.

410
00:51:27,200 --> 00:51:41,400
So. Okay, so any questions about this compilation? Okay.

411
00:51:41,500 --> 00:51:49,030
If not, then we are going to stop here. We are going to have a two minute break and then we'll come back to you and.

412
00:52:04,028 --> 00:52:09,908
let's take a look at the amount of work that is to a computer interval estimated of four feet away.

413
00:52:10,208 --> 00:52:22,508
Well, this is sort of comparison, partly because in part of you, we have already calculated the funeral plans for about a month, a year.

414
00:52:22,508 --> 00:52:33,788
Now, let's let's do this or not for based on what the 95% confidence interval of, you know, one that is paid on one hand plus minus.

415
00:52:33,998 --> 00:52:42,548
As I mentioned, the the key 13 you are on 1975 on center error of later life.

416
00:52:45,958 --> 00:52:49,738
And, you know, one, the asthma is actually equal to two.

417
00:52:49,738 --> 00:53:05,668
One is only given and volunteer the plus minus this person powerless or 2.2 was times the center error that that is that is 1 to 5 less than this one.

418
00:53:09,688 --> 00:53:15,418
Oh, sorry. The secretary is actually screwed up for the fact because there is this point in time.

419
00:53:16,408 --> 00:53:20,608
So 2.1. Six times four in five.

420
00:53:21,948 --> 00:53:30,138
Now that's one plus minus. This is 1.8, right?

421
00:53:30,168 --> 00:53:36,828
That's so that's that's 19.8 to 2.8.

422
00:53:37,158 --> 00:53:40,548
So that's not in our for beta one.

423
00:53:46,928 --> 00:53:52,868
Right. In our last meeting, verbatim. Any questions about this combination?

424
00:53:58,888 --> 00:54:03,318
Okay. And then. Let's win this.

425
00:54:03,348 --> 00:54:05,058
You know what? Let's look at another part.

426
00:54:05,088 --> 00:54:13,458
So a previous study found that an average amount of sleep per night increases by 1.6 minutes per year of age.

427
00:54:14,328 --> 00:54:19,248
That's the first study. So a seven the study estimated this to be 2.0.

428
00:54:20,028 --> 00:54:25,548
And again, a third study, well, estimated this to be two out of five or there are three different studies.

429
00:54:26,208 --> 00:54:32,438
Now, each study has an estimate is different. This is actually very, very natural in practice.

430
00:54:32,448 --> 00:54:36,898
If you read the papers, I mean, for the same part of the study, the same disease,

431
00:54:36,918 --> 00:54:43,278
slightly fact of the same covariate, let's say, for example, that the effect of gender on, you know,

432
00:54:43,638 --> 00:54:48,048
the risk of having a certain type of disease, certain type of cancer,

433
00:54:49,278 --> 00:54:56,088
you'll see that different studies give you sometimes because you got two different estimates, different of different aspects of the effect.

434
00:54:56,148 --> 00:54:58,188
But sometimes the estimate may be close,

435
00:54:58,188 --> 00:55:05,657
but sometimes they think this this difference is sometimes they can be explained by the randomness in the data,

436
00:55:05,658 --> 00:55:10,398
because you're inclined to be for data that's in different studies. And sometimes they cannot be.

437
00:55:10,698 --> 00:55:13,368
There, there there is other explanations behind the difference.

438
00:55:13,788 --> 00:55:19,628
But here we have three different studies and each study gives us a different estimate verbatim.

439
00:55:21,258 --> 00:55:30,497
And no, we want to test to determine if this results, if the results of the current study are consistent with the previous study,

440
00:55:30,498 --> 00:55:37,698
because our current study estimated this to be 1.0 lies separate from all these previous studies.

441
00:55:37,878 --> 00:55:44,298
So we want to see whether this previous findings are consistent with that, with our estimate.

442
00:55:45,118 --> 00:55:52,248
Now, here of the way to look at this is, again, we look at the hypothesis.

443
00:55:52,938 --> 00:55:57,408
The bottom of this is for the three different studies corresponding to the three different studies.

444
00:56:03,198 --> 00:56:07,518
Our given by the corresponding to this three of studies.

445
00:56:08,658 --> 00:56:15,137
And we will like to see whether using our data whether we would reject this or not.

446
00:56:15,138 --> 00:56:18,548
But this is where we would filter rejection does not exist.

447
00:56:19,868 --> 00:56:27,408
And in this case, because we have already constructed the 95% confidence in our for theta one using our data.

448
00:56:28,708 --> 00:56:33,248
That makes testing those possibilities quite easy because we just need to look

449
00:56:33,248 --> 00:56:39,328
at whether those three values fall inside of this 95% confidence interval.

450
00:56:41,768 --> 00:56:50,268
And if we can take a look at one by one, so the first one, 1.6, while it is false, is small.

451
00:56:51,518 --> 00:56:58,248
So 1.26 if inside of this interval. But 2.0.

452
00:56:58,368 --> 00:57:03,108
That again falls inside the signal, but a 2.5.

453
00:57:04,008 --> 00:57:12,197
It does not fall inside the signal. So then what that means is look here.

454
00:57:12,198 --> 00:57:15,348
Let's say if I simply.

455
00:57:17,378 --> 00:57:25,298
This one falls inside. The 95% of the homes in this one also fall inside the constitution.

456
00:57:25,628 --> 00:57:27,898
And this law does not fall into.

457
00:57:33,108 --> 00:57:43,818
So this based on these observations, while we would fail to reject the first novel, this is still true regardless and novelists.

458
00:57:44,388 --> 00:57:50,668
But if we don't reject the third novel, this is like at Alpha Equal 2.5 now.

459
00:57:51,228 --> 00:57:55,518
So what this means is that the stories from the first two,

460
00:57:55,518 --> 00:58:01,728
while the result from the first two studies, they actually agree with our findings using our data.

461
00:58:02,808 --> 00:58:06,918
Or at least there is no significant disagreement.

462
00:58:07,338 --> 00:58:12,858
So there's one kind of group, but the third, the result from a third.

463
00:58:13,068 --> 00:58:17,118
The third study, an estimated beta 1 to 2.5.

464
00:58:18,138 --> 00:58:22,158
That's actually significantly different from our estimate.

465
00:58:22,278 --> 00:58:30,648
The data is equal to one because when we construct but 95% of common undesirables, there's 2.5 fold outside of communism.

466
00:58:34,738 --> 00:58:39,988
So. This means.

467
00:58:42,498 --> 00:58:52,608
Studies one and two are inconsistent with our finding.

468
00:59:04,748 --> 00:59:16,858
Three does not. And that's this example.

469
00:59:21,438 --> 00:59:39,178
Okay. Any questions about this art? Okay, then if there's no question, then we will we will need to move on to the next topic.

470
00:59:40,588 --> 00:59:44,578
That's a nova table of small squares, a little bit table.

471
00:59:46,768 --> 00:59:51,208
A nova is actually a name or is a shirt problem.

472
00:59:51,508 --> 00:59:56,697
It stands for analysis of various analysis.

473
00:59:56,698 --> 01:00:04,408
There is actually a type of methods that that is that is for that is what that is categorical.

474
01:00:04,588 --> 01:00:10,828
So when you have a response, why that is continuous, what are your you have a categorical white categorical X.

475
01:00:11,038 --> 01:00:18,298
So for example X, maybe gender, maybe, maybe state, maybe your level or maybe different age groups.

476
01:00:18,508 --> 01:00:27,298
And so you have different categories. ANOVA is a method that compares the group specific means.

477
01:00:28,858 --> 01:00:35,547
Now nowadays ANOVA used to be like, you know, the dominant tool in Boston as well.

478
01:00:35,548 --> 01:00:43,197
The dominant was in Boston. We used to have I mean, if you look at the curriculum like 20, 30 years ago,

479
01:00:43,198 --> 01:00:51,098
you you'll see that no rules were made with hobby most Boston we would probably be part of the offer of single work.

480
01:00:51,098 --> 01:00:54,568
But you got worse. No, it was normal classes of veterans.

481
01:00:55,258 --> 01:01:02,318
But nowadays, with the development of linear regression model, now the emphasis on ANOVA now becomes less and less.

482
01:01:02,818 --> 01:01:09,328
The reason is that so linear regression can take care of like without ANOVA does linear regression can can do the same thing.

483
01:01:09,898 --> 01:01:17,428
So it can be considered as a special case of linear regression by simply considering X to be a categorical covariates.

484
01:01:17,908 --> 01:01:31,558
So then again, there is we do not need to really there's is not a necessary to like data to specifically consider ANOVA as a separate topic.

485
01:01:31,948 --> 01:01:41,908
But still, I mean, nobody is it's commonly seen especially on a normal table, it's commonly seen in the output of beef and software.

486
01:01:42,388 --> 01:01:47,818
So it's totally worthless. And also the fundamental ideas are are quite important.

487
01:01:48,928 --> 01:01:58,308
So now let's take a look at a stat a few minutes, take a look at a normal table, what goes wrong.

488
01:02:00,208 --> 01:02:08,668
So the ANOVA, the fundamental idea behind ANOVA analysis of variance is that the composition of the so called sum of squares,

489
01:02:10,978 --> 01:02:16,318
so there are 3 of sum squares as shown on this slide.

490
01:02:16,648 --> 01:02:24,058
So there is was the so-called total sum of squares, the total sum of squares in actually,

491
01:02:24,538 --> 01:02:33,988
you can think of this as the total variation in the response y you observe or denote that you have different individuals.

492
01:02:33,988 --> 01:02:36,688
For different individuals, you have different y values.

493
01:02:37,588 --> 01:02:44,487
And then there is variation of course across this differ individual across and then this total sum of squares.

494
01:02:44,488 --> 01:02:49,438
This is the total variation in the response you see in the data delivery.

495
01:02:51,528 --> 01:02:53,087
And then there is regression.

496
01:02:53,088 --> 01:03:03,978
Some of the squares, the regression, some almost squares is actually the variation that can be explained by the regression model.

497
01:03:08,428 --> 01:03:17,608
And you feel a linear regression model, right? And the early iteration model is capable of explaining part of the variation in the response Y.

498
01:03:18,478 --> 01:03:22,648
But I think Anthony probably does not fully explain account of all of the information.

499
01:03:22,648 --> 01:03:31,918
But I can what is part of the variation in the response Y and that is part explained by your regression model that's called the Russian seven squares.

500
01:03:33,448 --> 01:03:43,468
And then what it is now is there is some stress that is the variation in the response that cannot be explained by your regression model.

501
01:03:44,968 --> 01:03:53,038
So here, this, this is a graphical illustration and it shows this idea very clearly.

502
01:03:53,728 --> 01:04:06,028
So suppose well here this thought this is an actual observed Y for this individual, the actual observed like that and this this horizontal line here,

503
01:04:07,018 --> 01:04:13,288
this represents the Y bar, the Y bar, the average of Y, your data set the average about it.

504
01:04:15,248 --> 01:04:27,328
And then the total variation is actually the square difference between the actual observed value and above the average in the data set.

505
01:04:27,598 --> 01:04:30,717
So that's, that's the total sum of squared you see here.

506
01:04:30,718 --> 01:04:36,628
So that's the Y fine minus the overall average, obviously the square and taking a sum.

507
01:04:37,018 --> 01:04:43,648
So this is total square and this is actual total variation in what you see in the dataset.

508
01:04:47,698 --> 01:04:51,958
And then we can decompose this difference into this difference.

509
01:04:51,988 --> 01:04:58,018
So first is the difference between. Now this line is the finish line.

510
01:04:58,378 --> 01:05:03,478
So the point is on this line, that's what hat and that's the estimated what?

511
01:05:05,328 --> 01:05:10,728
And then the difference between the actual Y and the 52 Y if you take.

512
01:05:13,438 --> 01:05:27,928
Sorry, sorry from here. So if you look at the difference between the estimate of the wine and the wine bar, the overall average, this part,

513
01:05:28,228 --> 01:05:37,198
if you skirted this part, is that the variation in the variation and why that is explained by your regression model.

514
01:05:38,228 --> 01:05:44,828
Just because you're looking at how far away you are estimated value is to the overall average.

515
01:05:45,608 --> 01:05:50,768
So. So this. We can't afford a variation that is explained by a regression model.

516
01:05:51,638 --> 01:05:55,658
And then what is left is actually this part of the variation.

517
01:05:56,168 --> 01:06:00,098
This part of variation that is y minus y hat.

518
01:06:01,508 --> 01:06:07,148
That is the part that cannot be explained by the regression model.

519
01:06:09,628 --> 01:06:20,378
So that is the call that this is actually the absolute absolute that this this is Epsom Hovis, Epsom High and square.

520
01:06:21,268 --> 01:06:25,378
This is the residual some of the square or areas of square.

521
01:06:25,648 --> 01:06:30,268
So this is the variation why that cannot be explained by your regression model.

522
01:06:31,558 --> 01:06:36,118
So if you think of this, if we forget about the mathematical time for a minute,

523
01:06:36,328 --> 01:06:44,668
this simply says that now the total variation what you observe in your data is that that is that is in fact,

524
01:06:46,198 --> 01:06:49,678
you can actually decompose that total variation into two parts.

525
01:06:50,128 --> 01:06:56,448
One part that can be explained by a regression model, and they are the part that cannot be explained by a regression line.

526
01:06:57,598 --> 01:07:04,228
So intuitively, of course, if you want to have a good regression model, you will have to explain the most part of the variation.

527
01:07:04,468 --> 01:07:15,968
And of course, the more you can explain, the better. It turns out that while mathematically this is also true so the total variation who knows how

528
01:07:15,968 --> 01:07:23,308
much is equal to the regression sample squared plus the error somewhat squared mathematically.

529
01:07:23,328 --> 01:07:28,787
This is true. So we're not going to conclude that this is based on based on something out of the way.

530
01:07:28,788 --> 01:07:36,618
You're you can see that there is nothing but algebra two to show this, but we're not going to do that in our lecture.

531
01:07:37,098 --> 01:07:43,518
But but this is a fact. So the pool variation is just equal to the variation explained by linear regression model.

532
01:07:43,518 --> 01:07:49,098
Plus the variation cannot be explained by the regression model.

533
01:07:50,868 --> 01:07:55,878
And then for these different sum of squares, we have different, bigger freedom.

534
01:07:56,358 --> 01:08:00,918
So for the total sum of square, the degree of freedom is minus one.

535
01:08:01,278 --> 01:08:10,848
The reason is that when we calculated the total, if we're offering a total variation while we use all the data to estimate this y bar.

536
01:08:11,388 --> 01:08:17,568
So we lose money for freedom. So that's why here we have minus one for freedom for total sum, the square.

537
01:08:19,298 --> 01:08:28,268
And for regression some of the square here we actually use all the data points to estimate y

538
01:08:28,288 --> 01:08:33,488
i recall that a Y while we're estimating why we need to estimate better zero and beta one.

539
01:08:33,998 --> 01:08:37,868
So we estimate 200 so that's why we didn't to study our freedom.

540
01:08:38,318 --> 01:08:47,498
We must lie here that you work within these models to. That's four errors almost where we have seen this.

541
01:08:47,778 --> 01:08:51,908
So we have seen the degree of freedom and minus one saw from before.

542
01:08:53,858 --> 01:09:01,848
And then that means that the degree of freedom before the regression sum of squares is simply equal to one.

543
01:09:01,868 --> 01:09:05,198
That's the difference between minus one and this.

544
01:09:10,118 --> 01:09:18,158
Okay. So this is the thing actually the fundamental idea behind ANOVA.

545
01:09:18,728 --> 01:09:22,538
So that's a decomposition of the sum of squares.

546
01:09:22,868 --> 01:09:30,398
So the total sum of squares can be decomposed as some squares that it can be can be explained by a regression model.

547
01:09:30,608 --> 01:09:33,968
And then another part of that cannot be explained by regression model.

548
01:09:35,198 --> 01:09:39,968
Now in the homework, you will actually show these two facts.

549
01:09:41,588 --> 01:09:48,487
Here we are not going to talk too much about this. These these two facts can be shown.

550
01:09:48,488 --> 01:09:55,368
That can be shown by simply recall that, well, we estimated they had zero meaning of what we use.

551
01:09:55,388 --> 01:10:03,188
We minimized the sum of squares, and that's just least square as we minimize the residual sum of squares.

552
01:10:03,338 --> 01:10:04,388
Were errors almost worse?

553
01:10:04,838 --> 01:10:10,898
And when you minimize that, you take the derivative, you take the first part of your T on the side of the root of equal to zero.

554
01:10:11,438 --> 01:10:16,838
And it turns out that these two equations are just these two equations over here.

555
01:10:19,618 --> 01:10:25,308
This is. You showed us in homework part of the problem, but.

556
01:10:28,288 --> 01:10:32,218
Okay. So that's total sum of squares, the sum of squares.

557
01:10:32,698 --> 01:10:42,388
And then based on several squares in the word freedom, we can define the so-called means square.

558
01:10:43,638 --> 01:10:53,448
The music with the music where is defined by dividing the sum of squares by its corresponding degree of freedom.

559
01:10:54,978 --> 01:11:06,038
Your freedom. A leader working to see why we why we introduce these calls that this is we need this new square.

560
01:11:06,248 --> 01:11:11,858
Well, we calculated the so-called F statistic for testing for how well, this testing.

561
01:11:12,968 --> 01:11:18,848
But for now, let's just let's just focus on the definition itself.

562
01:11:19,208 --> 01:11:28,448
And so the definition is that the mean squares are the ratios between the the sample squares and is corresponding to our freedom.

563
01:11:29,078 --> 01:11:36,907
So in this case, then the mean square error, that's the error sample squares divided by the degree of freedom,

564
01:11:36,908 --> 01:11:41,798
of the error of some squares, and thus has an SC divided by m minus two.

565
01:11:46,198 --> 01:12:00,088
Because as we mentioned, the errors on the squares, it has become a Sunni warfare and that residual sorry the regression somewhat squares.

566
01:12:00,538 --> 01:12:03,828
That's that's actually SSR divided by one.

567
01:12:03,838 --> 01:12:08,848
Right. Because maybe we're afraid of providing sample squares in this equal to one.

568
01:12:13,168 --> 01:12:18,058
And previously we have well, we have seen we have already seen this new squared error.

569
01:12:18,388 --> 01:12:34,508
We actually use the mean square to estimate sequence square in which we say it's a box, as we have seen since it is on box as a matter of seamlessly.

570
01:12:37,148 --> 01:12:43,658
So this is the piece where. And this is the so-called Adobe.

571
01:12:43,988 --> 01:12:52,628
Yes. What is the new and the new way we're looking at this as a ratio relates to how we were looking at it as an expected value.

572
01:12:54,418 --> 01:12:56,968
So I can't say that again, the way we're looking at it now.

573
01:12:56,998 --> 01:13:03,808
We went around, we went through the regression model to define it for the like the mean squared error.

574
01:13:04,798 --> 01:13:12,208
But before we were looking at it as an expected value of the similarly before we look at

575
01:13:12,598 --> 01:13:19,977
as the way we define mean square or ones like that as an expected value and then like we,

576
01:13:19,978 --> 01:13:21,928
we showed through like expected values.

577
01:13:23,248 --> 01:13:30,688
Maybe it does make sense what I'm asking of how the two ways we got to mean squared error relate to each other.

578
01:13:31,708 --> 01:13:41,367
Oh, actually, yeah, I think I understand the question, but let me just try to answer that to see if if that answers your question.

579
01:13:41,368 --> 01:13:50,848
So let me see the, the the formula they are they're going to say, well, it's the same formula as before.

580
01:13:50,848 --> 01:14:03,328
So the MLC. Yeah.

581
01:14:03,438 --> 01:14:07,758
The MSI is actually equal to the SC divided by minus.

582
01:14:08,808 --> 01:14:14,288
Right. So involved, as I say, is a given by then and so is it minus two divided by the sum.

583
01:14:14,848 --> 01:14:18,648
Why? I might as well. I had a square.

584
01:14:19,668 --> 01:14:28,108
Right. So it's actually. Right.

585
01:14:28,148 --> 01:14:36,008
It's it's I think it's the same expression as before. And so, you know, we're looking at the same exact man as a mother.

586
01:14:36,018 --> 01:14:40,998
I must see the definition. How I see it does not change. We just found two ways to get the same.

587
01:14:41,268 --> 01:14:46,748
Get to the same problem. Yeah. You guys are going to do exactly the same thing again and ends.

588
01:14:47,778 --> 01:14:50,418
And also here, that's why we actually say, well,

589
01:14:50,718 --> 01:15:01,568
here you've got msi's equal Sigma Square is I was asking you to because previously we had to use this MSI as an estimate or similar square.

590
01:15:01,578 --> 01:15:05,078
We call it a sigma how to square it.

591
01:15:05,128 --> 01:15:12,348
It's not. Yeah. So previously we, we talked about why we divided the by a lot of freedom so that,

592
01:15:12,768 --> 01:15:18,158
you know that information is equal to see my square so that you know this becomes a unbiased as me of

593
01:15:18,168 --> 01:15:23,958
seamlessly and that's why we're saying it summarizes the errors in the model when we look at the ratio now.

594
01:15:26,978 --> 01:15:30,908
Of which in this region.

595
01:15:31,388 --> 01:15:38,218
Yeah. The difference between looking at it as just a ratio versus how we were looking at it before with our Epsilon house.

596
01:15:41,908 --> 01:15:45,058
We can move on. Make my question bigger. My sense of.

597
01:15:50,328 --> 01:15:56,157
Well, yeah, so let me just. Well, just maybe just add one more sentence.

598
01:15:56,158 --> 01:16:05,568
So. So previously we. I mean, when we talk about MSE, we still divide dividing by the sum of squares by this commonness.

599
01:16:06,358 --> 01:16:09,738
Right. So we always have this amount of what we define as.

600
01:16:09,958 --> 01:16:15,348
So here is the same. So the definition or the expression for message is never and never checked.

601
01:16:17,458 --> 01:16:20,708
If you have more questions, maybe we can talk after.

602
01:16:25,498 --> 01:16:28,768
Okay. So now the ANOVA table.

603
01:16:29,218 --> 01:16:33,238
This is something about a you will them to see later.

604
01:16:33,238 --> 01:16:42,928
We are going to see one example. This is a table that is typical output, part of the output from our resource for the other software.

605
01:16:44,128 --> 01:16:48,408
So you will see it has such a structure. So it has a source.

606
01:16:48,418 --> 01:16:51,808
The source means where does the sample square come from?

607
01:16:52,228 --> 01:16:57,448
So you have the some square for the model for regression model, some square for the error and total sum of squares.

608
01:16:58,128 --> 01:17:03,538
Rizzo Total sum of squares is s. S one that's equal to.

609
01:17:13,908 --> 01:17:19,818
They're still totally harmless square. And again the error sum of square that's actually.

610
01:17:27,218 --> 01:17:33,688
That's this some square. And then the models on the square, we use SLR for the square.

611
01:17:34,188 --> 01:17:41,778
That's mention why. The motto somewhere.

612
01:17:46,808 --> 01:17:50,528
And then the degree of freedom. So here for Moto, I was there.

613
01:17:50,528 --> 01:17:58,058
We know the degree from freedom is equal to one in four areas almost, whereas minus two and a four follows how a square is minus one.

614
01:18:02,658 --> 01:18:05,958
And with a mean square. That's just the.

615
01:18:09,408 --> 01:18:13,698
The seas as an equal assessed divided by in the corresponding you are freedom.

616
01:18:14,028 --> 01:18:28,398
So here as art divided by one as as divided minus two in the test as y divided in the score, we will see this different numbers and this f statistic.

617
01:18:30,798 --> 01:18:38,258
This F statistic. This is the achievement as far divided by film as.

618
01:18:39,018 --> 01:18:47,348
And we are going to talk more about this later. So for now, let's just just just give the expression here.

619
01:18:47,838 --> 01:18:53,568
So it's SSR divided by the profit of one divided by, as I see, divided by in minus two.

620
01:18:54,378 --> 01:19:02,328
That's what this F value is. And then this P value, this P value.

621
01:19:02,328 --> 01:19:10,968
That's actually the probability that, you know, f distribution with once equal freedom and minus to your freedom,

622
01:19:11,538 --> 01:19:18,228
larger than what you observe in the F value are not the losses.

623
01:19:18,558 --> 01:19:24,708
So for the last column, we are going to talk more about these, plus to call them later or for now.

624
01:19:25,818 --> 01:19:31,068
And we can focus on the first three columns. Right. These are one of the first three columns are.

625
01:19:31,428 --> 01:19:36,228
So we have the sum of the squares, depending on which source it come from.

626
01:19:36,498 --> 01:19:42,407
So there is model sample squared. This error result squared then is total sum of squares and then we have the

627
01:19:42,408 --> 01:19:47,508
corresponding to our freedom and then we have the new square and those are New Square.

628
01:19:49,648 --> 01:20:00,358
That's the sample square divided by the of all of you. So let me show you very quickly what the table would look like in a conflict while our output.

629
01:20:01,108 --> 01:20:06,518
Later we'll little more about that. But a here a fruit juice.

630
01:20:13,408 --> 01:20:22,408
Yeah. So here, for example, this is analysis of various table and all the tables.

631
01:20:22,588 --> 01:20:27,448
So you have well, of course, the stronger the concrete is stronger, maybe slightly different from our slight.

632
01:20:27,628 --> 01:20:33,478
But here we have the D we're afraid of column. We have some in the square column and we have the mean square in column.

633
01:20:34,318 --> 01:20:45,028
And also we have the whole column and we have the p value column and the and all these numbers are calculated.

634
01:20:47,088 --> 01:20:59,468
As we show here. And I saw here in this in this table, this is what a typical a normal table looks like and how those numbers are calculated.

635
01:21:01,848 --> 01:21:11,118
And eventually what we are looking for in this table is actually this p value because the p value summarize all the evidence in the well.

636
01:21:11,418 --> 01:21:18,948
But we are going to talk more about this F episode and the last two columns in a few minutes.

637
01:21:20,388 --> 01:21:24,118
So that's what the table is. It's.

638
01:21:28,388 --> 01:21:32,678
And then another very important quality is a so so-called R square.

639
01:21:34,868 --> 01:21:38,738
First. Where is the coefficient of determination?

640
01:21:40,388 --> 01:21:46,148
So our square is a quantity that we can use to measure the strength of linear

641
01:21:46,148 --> 01:21:54,248
association between Y and hence if there is no linear association at all.

642
01:21:54,248 --> 01:22:03,188
If Y on an x, there's no linear association, then what we would do is because because there is no linear association.

643
01:22:03,608 --> 01:22:09,127
So when we try to make a prediction of what of course the best prediction Y is simply

644
01:22:09,128 --> 01:22:13,028
we use and assemble everything to predict what if you want to predict an individual,

645
01:22:13,508 --> 01:22:18,128
then the best we can do is probably just to use the central average as a prediction.

646
01:22:18,548 --> 01:22:21,368
So that's one there is no no association.

647
01:22:22,148 --> 01:22:33,608
But if there is meaning no associations, not by modeling this dependance of Y on x, if indeed y depends on x, then the stronger association.

648
01:22:34,118 --> 01:22:44,548
Then by the model of this association now we can have a better prediction of Y because our model actually account for a certain variation in one.

649
01:22:46,898 --> 01:22:56,708
So the question is how do we quantify like how much our linear regression model can account for the variation in one?

650
01:22:57,458 --> 01:23:02,048
And it turns out this R Square is a such qualification, is such a quantity.

651
01:23:02,918 --> 01:23:08,048
So our square is defined as the ratio of these two.

652
01:23:08,918 --> 01:23:16,108
So SS are the sum of square of the regression divided by the total sum of square, the sum of square of one.

653
01:23:18,758 --> 01:23:22,988
So because of a definition, then the interpretation of R is very straightforward,

654
01:23:23,018 --> 01:23:29,047
is actually the proportion of the variation in what the total variation why that's this this

655
01:23:29,048 --> 01:23:36,248
quantity and of the variation that is explained by the regression model that is this quantity.

656
01:23:37,058 --> 01:23:46,808
So if you look at the ratio, then the ratio is simply how much variation you y that it can be explained by a regression model.

657
01:23:48,638 --> 01:23:51,878
Regression model. So that's what this R square means.

658
01:23:54,008 --> 01:23:59,588
So it's a very straightforward quantity, and the provision of R squared is always between zero and one.

659
01:24:00,518 --> 01:24:07,318
So when R squared is zero, that means there is no variation in Y that it can be explained by a regression model.

660
01:24:07,498 --> 01:24:09,278
Regression model is useless.

661
01:24:09,428 --> 01:24:18,878
It does not explain any variation why when our square is equal to one, what it means while your y and x we have a perfect linear association.

662
01:24:19,328 --> 01:24:26,828
So in other words, if you look at X and Y, you have to fall on a perfectly straight line.

663
01:24:27,338 --> 01:24:32,178
If we fall perfectly on a straight line that our square is equal to one.

664
01:24:32,198 --> 01:24:35,678
So any variation in one can be explained by your regression model.

665
01:24:37,028 --> 01:24:41,618
And typically our square is somewhere in between, right between zero and one.

666
01:24:41,828 --> 01:24:47,348
So the right model can explain some variation in wide, but not all the variation.

667
01:24:48,218 --> 01:24:49,838
So of course, when we build a regression model,

668
01:24:49,838 --> 01:24:56,378
we will like the square to be as high as possible so that we can explain as much Generation Y as possible.

669
01:24:57,758 --> 01:25:02,318
But that's that's what our square is, where the so-called coefficient of determination.

670
01:25:03,178 --> 01:25:11,587
So again, it's simply to represent the variation.

671
01:25:11,588 --> 01:25:16,238
Oh, sorry, the proportion of generation Y that it can be explained by your regression model.

672
01:25:17,938 --> 01:25:23,338
Of course, when we fit a rock model, we would want to use our regression model to explain why.

673
01:25:27,158 --> 01:25:33,638
Then they ask Where is actually a computer?

674
01:25:33,968 --> 01:25:38,138
While the feet of two different models ask where is an overall statistic?

675
01:25:39,128 --> 01:25:40,898
So while first we look at.

676
01:25:46,138 --> 01:26:02,418
If you look at this model of this model, while it not included, include the coverage and in this case, the abstract is equal to y y minus one I had.

677
01:26:04,918 --> 01:26:08,278
Right. And then the CAC is in control.

678
01:26:11,198 --> 01:26:14,588
That's why I am one is why I had to square.

679
01:26:18,758 --> 01:26:24,818
That's the sum of square areas. If we fit this model, the model that does include of the covariate.

680
01:26:28,868 --> 01:26:33,638
Now, if we consider another model, the model does not include a covariate at all.

681
01:26:36,288 --> 01:26:42,348
Simply include the intercept. There is no covariate, including in this model, and that in this case the answering.

682
01:26:46,298 --> 01:26:53,588
For the best prediction. The best prediction is actually just use the wine bar to predict the whites or the residual for each individual.

683
01:26:53,588 --> 01:27:01,138
That's simply one minus y bar. And then the samples.

684
01:27:02,798 --> 01:27:08,378
A square for the area that's in this case. That's why I minus one bar square.

685
01:27:11,408 --> 01:27:19,778
Let's see for this particular moment. But this is also, you know, as as said, if you recall the governor.

686
01:27:19,898 --> 01:27:28,717
Yes. That's why this is also and that's why. So the point here they're trying to make is that as why as as why indeed is a

687
01:27:28,718 --> 01:27:35,798
special type of SC for this particular model without including any covariate,

688
01:27:36,728 --> 01:27:49,508
just include the intercept in the model and then the R square, because our square by definition, our square by definition is equal to this.

689
01:27:51,998 --> 01:27:56,738
So our square by definition is equal to s as are divided by S as Y.

690
01:27:57,518 --> 01:28:05,858
And we know that. SSR is equal to.

691
01:28:15,498 --> 01:28:26,608
That's. As I see that because of the composition of the sample squares, we know that SSR is equal to as.

692
01:28:26,668 --> 01:28:36,348
As y by. But as as why can you be considered as a special type of SC for this particular morning.

693
01:28:38,538 --> 01:28:43,158
We've got a model. Now, what this means is that our square?

694
01:28:43,768 --> 01:28:53,298
Now, another way to interpret our square is, well, we are actually comparing the scene for these two different models,

695
01:28:54,168 --> 01:28:59,058
looking while we are looking at the error of some squares for this, two different models.

696
01:29:01,998 --> 01:29:04,938
That's just another way of interpreting our square.

697
01:29:05,208 --> 01:29:12,108
And this this is less and less straightforward than the first interpretation and the first I, I like this one a lot better.

698
01:29:12,678 --> 01:29:19,188
It's simply the proportion of the variation why that isn't explained by the regression model.

699
01:29:23,268 --> 01:29:30,828
And typically that's how people interpret it. So if you ask people what our score means and typically the people who simply give you this answer,

700
01:29:31,248 --> 01:29:34,488
but I hear this line just try to show that they know it's not their side.

701
01:29:34,998 --> 01:29:40,818
Another way of interpreting our square and this is less and less, less straightforward,

702
01:29:43,388 --> 01:29:51,438
but because of the connection between us as why and as I see it, as Azhar in particular also has this interpretation.

703
01:29:51,978 --> 01:29:56,648
And of course, the ss y is always larger than C, right?

704
01:29:58,788 --> 01:30:03,018
So that's another way of interpreting our square.

705
01:30:09,258 --> 01:30:13,037
Oh. Then, of course.

706
01:30:13,038 --> 01:30:23,478
Then what? Minus R squared. This is the proportion of in the proportion of the variability in Y that cannot be explained by a regression model.

707
01:30:26,928 --> 01:30:31,848
If there is a perfect linear association between one and X, if there is no error,

708
01:30:31,848 --> 01:30:37,338
if everything about this condition is perfect linear, then our square is just equal to one.

709
01:30:38,148 --> 01:30:46,668
But typically having two. While we do not have a perfect linear association, then our score typically is somewhat smaller than one.

710
01:30:49,438 --> 01:30:58,917
And also the hour square. Okay.

711
01:30:58,918 --> 01:31:04,978
So the r squared here, it's a quantity.

712
01:31:05,308 --> 01:31:10,828
For the strength of linear association. This is very important for the linear association.

713
01:31:11,818 --> 01:31:18,458
So, for example, let's say now we have this model Y depends on X square as a cognitive function.

714
01:31:18,478 --> 01:31:24,688
Now we can conceptually, I think maybe we can make such an illustration.

715
01:31:24,688 --> 01:31:27,808
So let's say this is a data points we observe.

716
01:31:28,238 --> 01:31:36,688
Right. So there is already a association. Now with Sorrentino's adds, if you fit linear regression models.

717
01:31:38,368 --> 01:31:43,918
Likely. I mean, you have you in the end, you have a set of straight lines and a flat line.

718
01:31:43,948 --> 01:31:48,958
Because because there is no overall I mean, there is no increasing trend or decreasing trend.

719
01:31:49,348 --> 01:31:50,798
Overcrowding is hidden. I mean,

720
01:31:50,818 --> 01:32:00,058
you simply consider it any other function than the travel will be shifted because the true the true dependency is actually the squared dependance.

721
01:32:00,868 --> 01:32:08,838
In this case, the R square will be very small if you count that our square is probably very close to zero travel,

722
01:32:09,238 --> 01:32:17,758
but it does mean that there is no association, no dependance of Y on X as a live event, as is almost perfectly a square.

723
01:32:20,338 --> 01:32:27,178
So our square is actually the quantity that 25, the median association between Y and apps.

724
01:32:29,008 --> 01:32:35,398
So if you see a very small R square, that to me is while the linear association is very weak.

725
01:32:36,178 --> 01:32:39,768
But it doesn't mean that, you know, but there is probably other association.

726
01:32:39,948 --> 01:32:52,958
Other type of association I would have. And it's. Of.

727
01:32:55,098 --> 01:33:09,518
And also our square is equal to. While our square is as an equal to the coronation, the square of the correlation between Y and a y hat.

728
01:33:11,438 --> 01:33:16,388
So if you recall the correlation, the correlation between two other variables, X and Y,

729
01:33:16,988 --> 01:33:26,518
that's defined as as X, Y, divided by the square root of SARS, X and a square root of Y.

730
01:33:26,798 --> 01:33:35,888
So this is sort of the covariance between the X and Y, divided by the square root of the variance of X and the square root of the variance of life.

731
01:33:35,978 --> 01:33:47,218
So that's the correlation between the X and Y. Now it turns out that our square is equal to the square of the correlation between one and y hat.

732
01:33:48,158 --> 01:33:51,598
So this also explains itself.

733
01:33:51,908 --> 01:33:55,458
Again, this is just another interpretation of our square.

734
01:33:55,478 --> 01:34:03,338
You can see that the difference lies and started from starting from here interpreting our square.

735
01:34:03,578 --> 01:34:09,008
Now we are just trying to interpret our square in different ways so that we have a better understanding of our square.

736
01:34:09,608 --> 01:34:14,168
So our square is the square of a correlation between those two, and that's true.

737
01:34:15,248 --> 01:34:24,758
Now, this is the actual observed Y and this white hat is the predicted y value based on your linear regression model.

738
01:34:26,128 --> 01:34:36,498
Right. Then, of course, the stronger the linear relationship between one and X, then the larger the correlation is then the larger the passwords.

739
01:34:37,888 --> 01:34:45,508
So again, based on this connection, we can again see that R squared quantifies the linear association, the strength,

740
01:34:45,508 --> 01:34:53,307
a linear association between Y and X ie how well your model explains why if your model x, y, y,

741
01:34:53,308 --> 01:34:59,518
very well then you have a strong, stronger association between the predicted white and natural zero.

742
01:35:00,448 --> 01:35:04,408
Otherwise the association will be weak and our square would be small.

743
01:35:09,988 --> 01:35:16,498
And another thing is that the R-squared and the beta one half, they actually matter.

744
01:35:18,418 --> 01:35:27,538
I mean, they are connected, but they are are better than different things that are a square can be interpreted as the overall fit of the.

745
01:35:28,048 --> 01:35:31,228
And now here we are looking at a simple linear regression model.

746
01:35:31,498 --> 01:35:40,168
So we only have one clue about an X, but later we're going to talk about a multiple linear regression model where you have multiple X,

747
01:35:40,318 --> 01:35:47,248
you have for example, your model could include age, gender, race and education level and all of these different variables.

748
01:35:48,808 --> 01:35:52,287
And a four for four for multiple linear regression model.

749
01:35:52,288 --> 01:35:57,298
We can still talk about R-squared and R-squared, so you have the same interpretation,

750
01:35:57,598 --> 01:36:05,188
the proportion of the proportion of variation Y explained by the model, but it is equality for the overall fit of the model.

751
01:36:05,878 --> 01:36:10,078
How well overall you are a lot of fiscal data, you really don't run a lot physical.

752
01:36:11,368 --> 01:36:15,148
So the higher R squared is the better fit your model is.

753
01:36:15,868 --> 01:36:22,078
So that's the R squared. So these are all in a represents an overall fit of the model data.

754
01:36:22,978 --> 01:36:33,268
So it reflects the pre the well, I would say overall fit overall fit but a bigger one this is.

755
01:36:35,638 --> 01:36:45,028
As a matter of fact all of particular coverage and it's all been a one quantifies but in fact of X or Y.

756
01:36:45,648 --> 01:36:48,748
Now in this case, of course, we only have one X, so.

757
01:36:49,438 --> 01:36:52,638
So we only have one one later on.

758
01:36:53,128 --> 01:37:01,118
And you could have. Like a beta one that is small in absolute value.

759
01:37:01,138 --> 01:37:04,938
Let's say beta one, the true value of beta one is equal to one.

760
01:37:05,888 --> 01:37:09,388
Right. So you could you could have why it would apply.

761
01:37:09,388 --> 01:37:15,838
And that's a in our surveys quantify that by speed a one times X plus epsilon.

762
01:37:16,588 --> 01:37:20,308
Now let's say for this model, for this model, the value of beta one.

763
01:37:21,028 --> 01:37:25,588
Beta one is actually small in absolute magnitude.

764
01:37:25,888 --> 01:37:36,358
It's part of one. However, this does mean that in the end you will have a small R squared because this is a linear regression model.

765
01:37:36,358 --> 01:37:40,108
So when you come to the R squared, you can still have a relative height R squared.

766
01:37:40,558 --> 01:37:48,328
If you said it was beta measurement. So in other words, this R square, the value of R squared, the value of one hat.

767
01:37:48,748 --> 01:37:57,928
There is no necessary connection, which is true because the R Square reflects the overall fit of the model.

768
01:37:58,168 --> 01:38:08,848
But a beta one hat reflects the the absolute magnitude of the association between x and Y.

769
01:38:10,108 --> 01:38:13,318
So the absolute magnitude can be can be small.

770
01:38:13,708 --> 01:38:19,288
Now you have a small magnitude, but if the clearing, the initiative is quite as strong,

771
01:38:19,678 --> 01:38:23,578
is quite strong, then you will still have R squared and a lot of Times Square.

772
01:38:26,088 --> 01:38:32,668
That's okay.

773
01:38:33,508 --> 01:38:43,318
You almost have any questions because there's no question them.

774
01:38:43,738 --> 01:38:46,878
We'll stop here. I'll see you next Tuesday.

