1
00:00:00,300 --> 00:00:16,410
Oh, my goodness. And.

2
00:01:00,230 --> 00:01:09,090
I didn't like.

3
00:01:24,270 --> 00:01:27,570
Good morning, everyone. Let's get started.

4
00:01:27,990 --> 00:01:32,080
There are a few announcements I would like to make.

5
00:01:32,100 --> 00:01:36,720
The first one is not related to the course, but related to the program.

6
00:01:37,260 --> 00:01:47,220
I was asked to remind you that on November 18th there is the annual poster session.

7
00:01:48,300 --> 00:01:54,540
This is the first time post-pandemic that's happening again in person.

8
00:01:54,750 --> 00:02:04,560
So it's a it's an important academic event where the current second year masters students showcase their work.

9
00:02:05,010 --> 00:02:10,570
Usually the internship research or the advances in it.

10
00:02:12,960 --> 00:02:16,530
So please register for it.

11
00:02:19,300 --> 00:02:28,440
And. There will be refreshments and snacks to sweeten the pot.

12
00:02:29,520 --> 00:02:33,600
A quote and it's required so.

13
00:02:36,580 --> 00:02:49,720
All right. This second announcement is that today's office hours in case anybody signed up with ten, will be actually covered by Hayley.

14
00:02:52,110 --> 00:02:57,180
They will be in the same room originally and was holding them.

15
00:02:57,510 --> 00:03:03,600
She's feeling unwell, unfortunately, but. But luckily Hayley will be covering them.

16
00:03:03,810 --> 00:03:20,940
Okay. And the third one is that, as you may have seen, I posted the first homework and it's due Monday right before the lab.

17
00:03:21,210 --> 00:03:32,640
But then I figured, well, actually at the lab you will be practicing some of the stuff that's that pertains to the homework.

18
00:03:32,640 --> 00:03:40,799
So it might be perhaps nicer if you actually submit the homework after the lab so that,

19
00:03:40,800 --> 00:03:45,720
you know, you can use the practice at the lab to perform better in your homework.

20
00:03:45,720 --> 00:03:52,560
So I'm going to give you until end of business on Tuesday to submit the homework.

21
00:03:54,030 --> 00:03:57,509
Still, I strongly recommend.

22
00:03:57,510 --> 00:04:03,030
Well, let's, let's make it midnight, uh, midnight on Tuesday.

23
00:04:03,030 --> 00:04:06,690
How about that? That's that's sort of more adventurous.

24
00:04:09,060 --> 00:04:17,610
It still, I very strongly suggest that you start working on the homework as soon as possible.

25
00:04:18,180 --> 00:04:26,010
In other words, avoid. Waiting till after the lab to start working on the homework because you might still scramble.

26
00:04:26,790 --> 00:04:34,890
Okay. So, you know, I don't like to ruin anybody's weekends, but, hey, when I always do it, they never stop.

27
00:04:35,820 --> 00:04:40,980
So we can just plan ahead accordingly.

28
00:04:42,420 --> 00:04:46,050
All right, so I will post an announcement about that.

29
00:04:46,230 --> 00:04:51,990
The the homework extension today.

30
00:04:52,590 --> 00:04:59,870
Okay. So let me start with a recap on what we have been up to.

31
00:05:01,100 --> 00:05:08,390
Well, we went through the fact that the widely prevalent understanding of causality currently in epidemiology is based.

32
00:05:10,890 --> 00:05:22,860
Yes. Let me. I should be. Yes.

33
00:05:24,000 --> 00:05:29,450
The owl is lit. So that means these things.

34
00:05:30,450 --> 00:05:41,590
So our widely prevalent understanding of causation in epidemiology nowadays is based on the potential outcomes or counterfactual paradigm.

35
00:05:42,360 --> 00:05:45,240
There have been many other paradigms before.

36
00:05:46,680 --> 00:05:54,450
We briefly mentioned the Bread for Health criterion, for example, but they are really no longer prevailing.

37
00:05:55,500 --> 00:06:03,059
There is another paradigm set forth by Ken Rothman in 1975 based on on the sufficient

38
00:06:03,060 --> 00:06:09,510
component cost model that you may have read in the introduction of your little Rothman book.

39
00:06:10,710 --> 00:06:14,130
And again, that one is still helpful but is not quantifiable.

40
00:06:14,950 --> 00:06:19,410
Therefore, it's it's also not not used a lot.

41
00:06:20,910 --> 00:06:30,210
So this is the one, right? And it's based on posing a causal question as this one.

42
00:06:30,240 --> 00:06:34,110
What would an outcome way have been following Friedman?

43
00:06:34,110 --> 00:06:43,470
A If instead of being exposed a equals one, someone would have been unexposed equal to zero or vice versa, right?

44
00:06:44,730 --> 00:06:51,660
Everything else being equal. So this is how we post causal questions according to this part.

45
00:06:55,810 --> 00:07:04,460
Now the outcomes, the potential outcomes after different levels of exposure are called counterfactuals.

46
00:07:05,050 --> 00:07:15,950
Right. Counterfactual. A causal effect of an exposure or a on an outcome way is present for a person,

47
00:07:16,310 --> 00:07:27,480
i.e. when the counterfactual outcome at level of exposure equals one differs from the controversial outcome at level of exposure equals zero.

48
00:07:28,610 --> 00:07:32,840
And when that is the case, we are in the face of an individual causality.

49
00:07:34,010 --> 00:07:38,149
However, because we only live once and in real life,

50
00:07:38,150 --> 00:07:45,260
people can only experience one level of exposure either a equals one or a equals zero, but not both.

51
00:07:47,270 --> 00:07:56,720
While everything else is equal, only one counterfactual outcome can be observed for us, for a given person or a given individual.

52
00:07:57,110 --> 00:08:05,000
Right. And the sad conclusion of this, well, is that causal effects on individual people can generally not be identified.

53
00:08:08,700 --> 00:08:17,280
However, there is hope because we can still identify causal effects using a different approach that requires fewer assumptions.

54
00:08:18,620 --> 00:08:22,010
Specifically no time machine, which is a big plus.

55
00:08:24,110 --> 00:08:32,810
And that approach is instead of concentrate, individuals estimating average causal effects in populations.

56
00:08:34,220 --> 00:08:46,350
And that's what we do in a ology in real life. The causal question to estimate average causal effect is very similar to the causal

57
00:08:46,350 --> 00:08:50,610
question for individual because elephant except now we are dealing with groups.

58
00:08:51,450 --> 00:08:58,709
So everything else being equal, what would this summary outcome variable measure have been for a population?

59
00:08:58,710 --> 00:09:08,190
If instead of the whole population being exposed to equals when the whole population had been exposed to a equals zero or vice versa.

60
00:09:09,120 --> 00:09:16,319
Right? Now the exposure a would have an average causal effect on the outcome.

61
00:09:16,320 --> 00:09:22,950
If the summary measure of the counterfactual outcome at one level of exposure differs

62
00:09:23,130 --> 00:09:27,450
from the summary measure of the counterfactual outcome at the other level of exposure.

63
00:09:27,640 --> 00:09:36,850
Right. So if you're. Uh, outcome is on the dichotomous scale.

64
00:09:36,850 --> 00:09:46,300
It's a binary variable you the deal with probabilities. If it was a continuous variable, you deal with expected values or arithmetic means, right?

65
00:09:47,350 --> 00:09:55,120
So far so good. Just a recap. And we had started working with this example.

66
00:09:55,150 --> 00:10:01,810
It's a we are interested in the effect of marijuana smoking on the risk of Alzheimer's disease.

67
00:10:03,160 --> 00:10:07,690
So how would we go about estimating an average causal effect?

68
00:10:07,720 --> 00:10:15,430
Well, we would first identify a group of people with daily marijuana smoking, meaning who are exposed.

69
00:10:16,970 --> 00:10:26,250
And then we would figure out after some time, let's say, 30 years, how many of them developed Alzheimer's now?

70
00:10:26,510 --> 00:10:30,950
And let's say this is a binary. Either people develop it, yes or no.

71
00:10:31,040 --> 00:10:36,450
So one zero. But there is a problem, right?

72
00:10:36,870 --> 00:10:41,890
Which is that all people in the group we observe were exposed to daily marijuana smoking.

73
00:10:42,150 --> 00:10:48,240
And again, we are short of voting machines so we can never directly observe what would have happened

74
00:10:48,240 --> 00:10:53,310
to them in the absence of exposure if they had not been exposed to marijuana smoke.

75
00:10:55,230 --> 00:10:56,160
So what's the solution?

76
00:10:56,700 --> 00:11:07,890
Well, we need to identify a group of unexposed people that's presumed to be identical to the exposed group in all other respects.

77
00:11:09,820 --> 00:11:17,920
In other words, we need to find a group of unexposed people who can represent the counterfactual experience of the exposed.

78
00:11:21,180 --> 00:11:31,550
If we were able to find that group. We would say that the two groups, the exposed and the unexposed, are exchangeable with each other.

79
00:11:33,300 --> 00:11:40,380
Exchangeable with respect to everything else other than the exposure.

80
00:11:41,640 --> 00:11:47,440
All right. Now. There is one way to do it in real life.

81
00:11:51,490 --> 00:11:57,220
And that is to conduct what's called an ideal randomized experiment.

82
00:11:59,580 --> 00:12:13,010
So that consists. Of us the largest to take charge of the allocation of exposure.

83
00:12:14,410 --> 00:12:27,140
So to. Become empowered to decide who is going to be exposed and who is going to be unexposed.

84
00:12:28,200 --> 00:12:35,890
And this happens in real life. You know, we are going through it right now with, let's say, the COVID vaccine experiments.

85
00:12:37,120 --> 00:12:51,559
Right. So that's exactly what this is about. So one way to reach exchange ability would be for us to randomly allocate each person in

86
00:12:51,560 --> 00:12:59,540
our example to either smoking marijuana or not smoking marijuana for a given time period.

87
00:13:03,130 --> 00:13:10,150
To make things simple since how many levels of exposure do we have here for the marijuana example?

88
00:13:11,520 --> 00:13:22,930
It's all right. So to make things simple, let's say we are going to give every person an equal chance of being assigned to either.

89
00:13:23,080 --> 00:13:26,260
So would be a 50% chance. Right.

90
00:13:27,070 --> 00:13:30,160
And we can use something as simple as a.

91
00:13:33,270 --> 00:13:36,870
Fair coin. Unbiased coin.

92
00:13:37,170 --> 00:13:43,260
Like a coin toss. So we have 20 people here.

93
00:13:43,260 --> 00:13:49,140
And let me expand this number to represent millions.

94
00:13:49,380 --> 00:13:54,360
So each circle here will represent a million people.

95
00:13:55,170 --> 00:14:04,650
And that's because we want to, for now, do away with the possibly confusing concept called random variability and sample size.

96
00:14:04,670 --> 00:14:07,980
And, you know, I don't want you to worry about sample size.

97
00:14:08,670 --> 00:14:17,230
So assume these are millions, 20 million people. Okay. I may go back to say bergson's, but that's just me.

98
00:14:18,510 --> 00:14:21,870
But always back in the back of your mind that we are talking about millions.

99
00:14:22,110 --> 00:14:25,170
So no small samples, huge sample size. Right.

100
00:14:28,350 --> 00:14:34,920
So for each of these potential participants in our experiment, we are going to do a coin toss.

101
00:14:35,550 --> 00:14:44,520
And if fit that, if the coin folds heads, then it's going to be allocated to being exposed, smoking marijuana.

102
00:14:45,210 --> 00:14:52,680
If the coin lands on tails, it's going to mean the person gets assigned to.

103
00:14:53,850 --> 00:14:57,330
The unexposed are not smoking marijuana. Right.

104
00:14:57,840 --> 00:15:01,430
You ready? Okay, you go. This is the first dose.

105
00:15:02,310 --> 00:15:06,470
So half. So the first person got.

106
00:15:07,170 --> 00:15:11,700
It's a tails. So go to assign to being unexposed.

107
00:15:11,970 --> 00:15:15,330
Not smoking marijuana. All right. By chance.

108
00:15:15,840 --> 00:15:20,250
By chance. Because it does. Going is a chance event.

109
00:15:22,140 --> 00:15:26,220
How about this second? Again, unexposed.

110
00:15:26,580 --> 00:15:31,030
The third one. Exposed this one good designed to.

111
00:15:32,560 --> 00:15:38,770
Smoking marijuana, the next one exposed next to an exposed and so on.

112
00:15:40,480 --> 00:15:45,010
I just summarized the procedure here for today in the interest of time.

113
00:15:45,280 --> 00:15:48,339
All right. So we got our assignment.

114
00:15:48,340 --> 00:15:50,740
And because, you know, we have a large sample size.

115
00:15:50,770 --> 00:15:58,960
Turns out, you know, half of the people were assigned to being exposed and the other half were assigned to being unexposed.

116
00:15:59,050 --> 00:16:09,190
All right. So I'm just going to organize the people here and there, each one of the groups used for the for ease of of visualization.

117
00:16:10,990 --> 00:16:14,410
And then I have a question for you. Okay. You ready?

118
00:16:15,310 --> 00:16:22,900
So at the time of random allocation of exposure, if the groups are large enough,

119
00:16:23,740 --> 00:16:36,520
would you expect there to be any differences in characteristics such as age, sex, socioeconomic status, health status within each group?

120
00:16:40,560 --> 00:16:44,890
Why would. I'm sorry.

121
00:16:45,550 --> 00:16:52,000
Let's forget about sample size. That's why I went through the effort of saying, let's multiply this by millions.

122
00:16:52,030 --> 00:16:55,720
No sample size. We have millions. So sample size is not an issue.

123
00:16:57,240 --> 00:17:05,140
Forget. You say there wouldn't be within each group?

124
00:17:05,200 --> 00:17:13,350
That means. Let me rephrase the question. Among people who were assigned to smoking marijuana exposed.

125
00:17:14,010 --> 00:17:21,420
Do you expect that there will be a distribution, let's say, of gender?

126
00:17:22,840 --> 00:17:28,110
Yes. Yes. We are not randomizing only people of one gender.

127
00:17:28,420 --> 00:17:31,920
There there will be. Do you think there will be a distribution of age?

128
00:17:32,640 --> 00:17:35,970
Yes. Not everybody will be the same age within that group.

129
00:17:36,120 --> 00:17:44,620
Right. So the answer to this question is, yes, there will be differences within each of these distribution, right.

130
00:17:46,170 --> 00:17:56,010
Okay. Now, well, I had done a poll, but again, you are always outsmarting the poll, which is great because I hadn't even signed up.

131
00:17:57,060 --> 00:18:05,700
I mean, that's probably not a big issue. Right. But here is another question, which is perhaps more interesting.

132
00:18:06,600 --> 00:18:13,320
Would you expect any differences in the averages of those characteristics age six years?

133
00:18:14,750 --> 00:18:19,610
Between the two groups. No, you would not.

134
00:18:20,950 --> 00:18:34,780
So if your allocation of exposure was really done at random, you should expect that the proportion of, let's say, gender A will be the same.

135
00:18:36,160 --> 00:18:43,510
In the group allocated to be exposed and the group allocated and exposed that the average age of people

136
00:18:43,510 --> 00:18:49,870
allocated to be exposed will be the same as the average age of people allocated to being unexposed.

137
00:18:50,470 --> 00:18:55,000
That's the magic of randomization, right?

138
00:18:58,120 --> 00:19:01,600
So if the explosive is randomly allocated to large enough groups yet.

139
00:19:01,610 --> 00:19:05,140
What? I mean you have a point. What you name? I'm sorry.

140
00:19:06,300 --> 00:19:11,310
No. Yes. But during.

141
00:19:13,290 --> 00:19:16,530
But your name is Sarah, so you have a point.

142
00:19:17,250 --> 00:19:22,830
I mean, the groups have to be large enough that, let's say I had mentioned, okay,

143
00:19:23,700 --> 00:19:29,760
so if you do that in large enough groups, the only difference between the groups will be the exposure.

144
00:19:31,470 --> 00:19:43,050
Right. And when that's the case, we can say confidently that the exposed and unexposed are exchangeable with each other.

145
00:19:43,590 --> 00:19:46,860
With respect to everything else.

146
00:19:46,920 --> 00:19:51,840
On average, the only difference between them is going to be, again, exposure status.

147
00:19:52,620 --> 00:19:59,760
So in other words, randomization. Helps us reach exchangeable.

148
00:20:01,570 --> 00:20:13,170
Okay. So after 30 years, we finally find out how many develop Alzheimer's in each.

149
00:20:15,040 --> 00:20:19,540
I'm going to show you the results. So.

150
00:20:21,720 --> 00:20:28,780
Five people or 5 million. Developed Alzheimer's in the exposed.

151
00:20:28,780 --> 00:20:31,990
One, two, three, four, five. They are represented by the close circle.

152
00:20:32,830 --> 00:20:37,210
And three developed Alzheimer's in the unexposed.

153
00:20:38,120 --> 00:20:43,700
Okay. How can we summarize? This outcome measure.

154
00:20:50,600 --> 00:20:53,810
Remember to summarize the measure. You have to.

155
00:20:55,070 --> 00:20:58,760
Think of the scale in which that variable is.

156
00:20:59,180 --> 00:21:02,840
What the scale of this variable. Of the outcome variable outside.

157
00:21:12,940 --> 00:21:17,970
Yeah. How do you summarize this outcome quantitatively compared to the exposure?

158
00:21:20,330 --> 00:21:28,010
What is next for your business? Right.

159
00:21:28,940 --> 00:21:32,060
But what do you use in sort of arithmetic terms?

160
00:21:33,650 --> 00:21:40,220
So it's a binary variable. And how do you summarize binary variables as a probability?

161
00:21:40,700 --> 00:21:47,030
I said probably. So we yeah, we are going to estimate the probability for each expulsion.

162
00:21:48,020 --> 00:21:56,220
Okay. So the probability of the outcome in the Expos is five divided by ten.

163
00:21:56,520 --> 00:22:00,860
Right. And in the unexposed is three divided by ten.

164
00:22:04,820 --> 00:22:16,640
Aha. What type of probability is this? And, uh, to refresh your memory, I brought a visual aid, which is that graphs table,

165
00:22:16,670 --> 00:22:22,370
you know, we talked about last time where we described all the types of probability.

166
00:22:23,060 --> 00:22:26,380
So. There are.

167
00:22:26,780 --> 00:22:31,940
Let me. Um.

168
00:22:32,630 --> 00:22:38,130
Let me try to sign up. Uh. Run the ball.

169
00:22:38,340 --> 00:23:00,010
Yeah. Okay. In the meantime, think about the question, of course.

170
00:23:20,200 --> 00:23:30,330
Um. Uh huh.

171
00:23:31,790 --> 00:23:37,470
Yeah. All right.

172
00:23:38,890 --> 00:23:50,950
So, um, go to, uh, to this website for evey dot coms, latch at 600 and go ahead and, and cast your vote.

173
00:23:57,970 --> 00:24:01,870
Oh, no. This is not the best time to contact me about this.

174
00:24:02,260 --> 00:24:05,890
I'm so sorry about that. Okay, we will fix it.

175
00:24:06,910 --> 00:24:27,660
Haley, you preregistered maybe from last year. How about now?

176
00:24:32,500 --> 00:24:38,750
The response is not accept. I know, but somebody responded.

177
00:24:40,690 --> 00:24:48,580
But who's the lucky you? I think it's.

178
00:24:51,710 --> 00:25:02,210
They three people have to go into my email and drag.

179
00:25:04,040 --> 00:25:08,690
Oh. Okay. Thank you. Log in with your email address.

180
00:25:12,390 --> 00:25:19,800
Oh, no. Have you tried this in any other course?

181
00:25:25,270 --> 00:25:28,940
For. Seriously.

182
00:25:35,420 --> 00:25:41,059
Okay. Well, it's a bit of a mystery and I apologize and I'll try to sort it out for next time.

183
00:25:41,060 --> 00:25:44,490
But, you know, in the interest of time again, let's, uh.

184
00:25:44,540 --> 00:25:47,950
Okay. Fight, huh? No, not now.

185
00:25:48,070 --> 00:25:56,870
I'm intrigued. Okay.

186
00:25:58,240 --> 00:26:04,510
Um hmm. Oops. Uh.

187
00:26:09,980 --> 00:26:13,280
So the five of you, we just.

188
00:26:15,590 --> 00:26:19,460
Well, got it. Absolutely right. One graduations.

189
00:26:20,750 --> 00:26:25,160
It's a conditional probability. So why is it conditional?

190
00:26:25,790 --> 00:26:35,060
It's conditional because is a probability of outcome in subsets of the total population.

191
00:26:35,090 --> 00:26:40,700
This one is the probability of the outcome in the subset of people who fulfill the condition of being exposed.

192
00:26:40,820 --> 00:26:49,010
Right. Whereas this is the probability of the outcome in the subset of people who fulfill the condition of being unexposed.

193
00:26:52,790 --> 00:26:54,480
Get permission.

194
00:26:55,370 --> 00:27:07,190
It's conditional because it's a probability not in the total population of 20 or 20 million, but in populations that fulfill conditions.

195
00:27:08,090 --> 00:27:16,640
So these probability. Is conditional on being exposed is among the people who were assigned to being exposed.

196
00:27:16,940 --> 00:27:20,300
And this probability is among the people who are assigned to be unexposed.

197
00:27:22,400 --> 00:27:26,520
Yep. Clear. Okay.

198
00:27:29,980 --> 00:27:33,760
No. In this particular experiment.

199
00:27:34,830 --> 00:27:43,390
The probabilities differ. Right. Obviously the probability of the outcome is higher in the ex post than it is in the unexposed.

200
00:27:47,860 --> 00:27:52,240
So this has a proper name, this difference.

201
00:27:52,630 --> 00:27:55,980
And it's one of the most important names we have learned so far.

202
00:27:58,090 --> 00:28:11,020
This difference in observed outcomes between actually experienced exposure groups, you know, in real life is called an association.

203
00:28:12,160 --> 00:28:23,840
An association. A difference in your summary outcome measure between levels of exposure or exposure groups is called an association.

204
00:28:24,780 --> 00:28:29,310
So if you happen to be asked, I don't know, maybe homework or something.

205
00:28:30,950 --> 00:28:42,200
Are these two things associated? Well, but you have to do now, you know, is to compare this summary outcome measures between exposure groups.

206
00:28:42,320 --> 00:28:50,720
And if they differ, then there is an association and association between exposure and outcome, between variables A and what?

207
00:28:57,680 --> 00:29:02,360
When the summary outcome measures are equal between the groups,

208
00:29:02,900 --> 00:29:14,060
then there is no association a and I are not associated and when they are not associated you can also see that they are independent of each other.

209
00:29:15,110 --> 00:29:24,680
So this is the notation we have been dealing with and there is also a summary notation for independence.

210
00:29:24,860 --> 00:29:32,150
These are these say which is a thing is, is the Greek letter pi upside down?

211
00:29:32,300 --> 00:29:35,780
Is it true? Maria Yes.

212
00:29:35,780 --> 00:29:39,709
B okay. In Greek is pronounced.

213
00:29:39,710 --> 00:29:43,800
P In English they say pipe. Yeah. Um.

214
00:29:45,130 --> 00:29:54,880
So this means a is in the independent of way and because there is reciprocity in independence, then why is also independence of a.

215
00:30:02,990 --> 00:30:06,350
No, no, we are getting there. It's only association.

216
00:30:06,780 --> 00:30:10,520
We really cannot tell about causation yet. It's only.

217
00:30:11,020 --> 00:30:11,630
Good question.

218
00:30:14,780 --> 00:30:28,700
When variables a and why are not independent meaning when there is an association, you may find a cross line over day upside down pilot.

219
00:30:30,290 --> 00:30:33,650
So she is correct. You are.

220
00:30:35,760 --> 00:30:44,640
All right. Now let's do another exercise on intuition.

221
00:30:45,120 --> 00:30:49,320
Right, which I guess is a fun way to. They're very empowering.

222
00:30:50,520 --> 00:30:52,950
There's really find the allocation of exposure.

223
00:30:53,790 --> 00:31:02,250
Let's assume that those allocated to being exposed have instead been allocated to unexposed or vice versa.

224
00:31:03,030 --> 00:31:06,360
You know, the mechanism could be silly, as happened.

225
00:31:06,840 --> 00:31:09,870
Let's say somebody made a mistake and instead of assigning.

226
00:31:11,300 --> 00:31:17,990
Had to be exposed. The assassin had to be unexposed right at the time of randomization.

227
00:31:19,940 --> 00:31:28,040
In other words, the people who they are now or where originally assigned to expose were actually assigned to an expose.

228
00:31:28,040 --> 00:31:32,270
And the people who were originally assigned to an expose were actually designed to expose.

229
00:31:32,900 --> 00:31:37,920
So I'm going to switch them. But.

230
00:31:40,430 --> 00:31:47,270
Question. After 30 years, how many would you expect to have developed Alzheimer's in each group?

231
00:31:47,270 --> 00:31:51,570
About the exposed. Five.

232
00:31:52,020 --> 00:31:55,110
How about the unexposed? Three? Wonderful.

233
00:31:56,730 --> 00:31:59,910
You know what we just did? We exchanged them.

234
00:32:01,560 --> 00:32:08,760
At the time of randomization there exchange. She doesn't really matter who ends up in which group.

235
00:32:10,010 --> 00:32:18,560
If they're exchangeable. And the only difference in on average between them is the exposure, then you are going to see the same outcomes.

236
00:32:19,850 --> 00:32:24,410
Isn't that a nice sort of exercise in intuition?

237
00:32:25,640 --> 00:32:30,400
All right. Um. They had another use for this.

238
00:32:31,960 --> 00:32:35,930
Serving. Okay.

239
00:32:39,410 --> 00:32:48,050
So in this case, again, because they are exchangeable, we can say that one group represents the counterfactual experience of the other.

240
00:32:48,770 --> 00:32:52,520
Thanks to randomization. Random allocation of exposure.

241
00:32:55,040 --> 00:33:08,360
No. This is another little exercise on on intuition that will lead us to understanding, exchange ability from a more mathematical point of view.

242
00:33:08,450 --> 00:33:18,350
You ready? Suppose we pass the people who are saying in real life to be exposed right through the tape machine experiment twice.

243
00:33:21,510 --> 00:33:25,840
Each time under a different exposure level. So.

244
00:33:28,040 --> 00:33:35,090
The first time we passed them. Exposed to marijuana smoking.

245
00:33:35,900 --> 00:33:44,420
We observed the outcome the second time we passed them and exposed to marijuana smoking we observed.

246
00:33:44,900 --> 00:33:59,680
I'm going to go down. Right. If we were to summarize the outcome after each pass through the time machine, what type of probabilities would we use?

247
00:34:01,600 --> 00:34:05,650
Those would be counterfactual outcomes, right? Because we are using the time machine.

248
00:34:06,730 --> 00:34:09,790
So what type of probabilities would we be using?

249
00:34:09,820 --> 00:34:13,720
Of the three we have learned marginal join and condition.

250
00:34:18,080 --> 00:34:24,150
Well. What's the denominator?

251
00:34:25,660 --> 00:34:37,570
Of these summary probability. Well, yes, that then was the denominator for a marginal probability.

252
00:34:39,870 --> 00:34:44,810
In their population. The ten are not the entire population. They fulfill a condition.

253
00:34:46,040 --> 00:34:50,440
Therefore, it is a conditional. Conditional probably.

254
00:34:50,750 --> 00:35:00,030
Right. The only issue is that in the numerator we would have the counterfactual right because

255
00:35:00,030 --> 00:35:04,710
we are using the time machine experiment and in the denominator we will have.

256
00:35:06,000 --> 00:35:18,540
These ten who fulfill the condition of being exposed in real life because these guys were the ones who we assign to being exposed to randomization.

257
00:35:23,260 --> 00:35:26,470
Now let's do the same with the unexposed.

258
00:35:29,600 --> 00:35:36,589
We pass the unexposed to the time machine twice once and there actually being exposed.

259
00:35:36,590 --> 00:35:40,700
Smoking marijuana the second time under not being exposed.

260
00:35:41,930 --> 00:35:46,290
So. When we summarize their outcome.

261
00:35:46,290 --> 00:35:50,010
What needs to change in the annotation from the prior experiment?

262
00:35:52,210 --> 00:35:57,610
You're the denominator, right? So instead of a equals one, there equals zero.

263
00:35:58,480 --> 00:36:05,110
And obviously, you know, the first notation here is for the result of the counterfactual experiment.

264
00:36:05,110 --> 00:36:10,030
When we pass this group at level of exposure equals one.

265
00:36:10,630 --> 00:36:15,910
The second is the result. When we pass this group at the level of exposure equals zero.

266
00:36:17,850 --> 00:36:26,190
Now suppose we combine both groups into the original one and pass it through the time machine experiment.

267
00:36:30,690 --> 00:36:37,680
What type of probabilities would we be using to summarize the effect under each level of exposure?

268
00:36:37,770 --> 00:36:40,980
This is year's marginal. Marginal?

269
00:36:41,010 --> 00:36:46,160
Yes, because we are passing the whole population through the time machine.

270
00:36:47,340 --> 00:36:53,400
Okay. And this would be the notation for each pass.

271
00:36:54,750 --> 00:36:59,639
The counter-factual probability of the outcome in the whole population at level of

272
00:36:59,640 --> 00:37:05,460
exposure equals one counterfactual probability of the outcome for the whole population.

273
00:37:05,840 --> 00:37:10,310
That level of exposure equals zero. Right.

274
00:37:12,240 --> 00:37:19,380
Now using the results from the actual real life experiment you these ones.

275
00:37:20,640 --> 00:37:24,720
We can anticipate the results of these counterfactual experiments.

276
00:37:25,290 --> 00:37:32,550
You know why? Because they are exchangeable.

277
00:37:32,910 --> 00:37:43,940
So it. So what would be the counter-factual probability of the outcome?

278
00:37:45,080 --> 00:37:50,360
That level of exposure equals one among those who ended up being exposed in real life.

279
00:37:54,740 --> 00:38:03,110
Five divided by ten. Exactly. This. Right. Because this is simply the counter-factual probability of what happens when people are exposed.

280
00:38:03,110 --> 00:38:06,200
And these are the guys who represent that. Okay.

281
00:38:07,760 --> 00:38:16,580
How about the controversial probability of the outcome and there no exposure among people who are exposed.

282
00:38:17,630 --> 00:38:28,030
What is your best guess? Well, he's actually there this right.

283
00:38:30,010 --> 00:38:33,700
You can use these. No problem. Because they are exchangeable with these guys.

284
00:38:34,390 --> 00:38:41,230
So if you pass these guys through the technology to experiment and there are no exposure, you should get these result right.

285
00:38:42,380 --> 00:38:46,770
Because in real life they were exchangeable. Okay.

286
00:38:46,770 --> 00:38:53,700
So it's plenty. This group is going to be the same actually as the exposed because again,

287
00:38:54,000 --> 00:38:58,560
even though here the group you're passing through the time machine experiment is the unexposed.

288
00:38:59,370 --> 00:39:05,219
If you said the switch of the time machine to exposed the result you can anticipate is what you

289
00:39:05,220 --> 00:39:10,940
actually observed for the exposed because they were exchangeable with the unexposed in real.

290
00:39:11,750 --> 00:39:15,440
Okay, so be it. Between five and again for this group.

291
00:39:16,880 --> 00:39:20,030
Um. This would be a point three. This thing.

292
00:39:23,650 --> 00:39:27,760
And. Yes, so this part of the denominator is.

293
00:39:29,780 --> 00:39:35,140
Were you for the real experiment? Yes, exactly.

294
00:39:35,150 --> 00:39:38,930
Is there is there the real life experience?

295
00:39:40,340 --> 00:39:48,520
Assignment group, except. But here is the the $1 million question, as they say.

296
00:39:49,990 --> 00:39:54,940
What are the counterfactual marginal probabilities of the outcome?

297
00:39:55,930 --> 00:40:03,950
For each. Level of exposure. Can we still.

298
00:40:06,100 --> 00:40:12,700
The same. If we pass the whole groups, the whole group of these two together to the time machine experiment,

299
00:40:13,510 --> 00:40:22,990
we should find that out of the 2010 should develop the outcome that's between five and six should develop the outcome under no exposure.

300
00:40:23,380 --> 00:40:29,030
Right. Point three. So in that exchange ability,

301
00:40:29,540 --> 00:40:38,029
the marginal counterfactual probabilities for the whole group should be the same as the conditional probabilities

302
00:40:38,030 --> 00:40:46,010
under the same level of exposure for the subgroups actually assigned to given exposure levels in real life.

303
00:40:50,850 --> 00:41:01,510
What? What numerator? One very good question.

304
00:41:01,960 --> 00:41:07,100
Why is the numerator ten here? We didn't know that, actually.

305
00:41:08,410 --> 00:41:13,960
We had. This is our best guess from that probability.

306
00:41:14,950 --> 00:41:20,950
So what we take this is an excellent question because what we actually take from these experiments of.

307
00:41:22,650 --> 00:41:26,940
This of groups actually exposed is the probabilities.

308
00:41:28,040 --> 00:41:36,320
And then we applied the probabilities to the denominators, which we knew because we just counted everybody and then get the numerator.

309
00:41:37,040 --> 00:41:44,279
There's a really good question is so good because you know, actually this has real life applications.

310
00:41:44,280 --> 00:41:48,890
Then after the second time of the term,

311
00:41:49,340 --> 00:41:57,690
we will be using exactly this procedure to estimate numbers and to estimate something called pseudo populations.

312
00:41:59,080 --> 00:42:04,120
So we use the probabilities to anticipate the sizes of numerators.

313
00:42:04,180 --> 00:42:09,220
Excellent. What's your name? Okay, honey.

314
00:42:10,300 --> 00:42:13,900
Thank you. Hey, good question. All right.

315
00:42:15,340 --> 00:42:19,270
Now, here is a bit of a of of summary notation.

316
00:42:20,980 --> 00:42:27,940
So this may look scary, but it really is not so hard based on what we have just gone through.

317
00:42:28,540 --> 00:42:37,929
Right. So under exchange ability, we just saw that the marginal counterfactual probability at one given level of exposure,

318
00:42:37,930 --> 00:42:42,910
let's say a equals one, is equal to a conditional.

319
00:42:44,050 --> 00:42:50,560
Um, counter-factual probability under the same level of exposure among people who in real

320
00:42:50,560 --> 00:42:56,710
life were exposed or among people who in real life were unexposed should be the same.

321
00:42:57,490 --> 00:43:03,270
Right? And they're exchangeable. On the other hand.

322
00:43:05,000 --> 00:43:10,460
The marginal counterfactual probability of the outcome at the level of exposure

323
00:43:10,970 --> 00:43:17,030
equals zero and exposed should be equal to the conditional probability,

324
00:43:17,720 --> 00:43:24,140
counterfactual probability of the outcome at the same level of exposure among those who were exposed in real life,

325
00:43:24,140 --> 00:43:30,130
or among those who were unexposed in real. Right, is exactly what we just went.

326
00:43:32,740 --> 00:43:36,400
Um, so in other words, the probability,

327
00:43:37,120 --> 00:43:45,370
the marginal counterfactual parity of an outcome at a given level of exposure is going to be equal to the

328
00:43:45,370 --> 00:43:57,550
conditional probability of the outcome counterfactual at any given of real life experience exposure.

329
00:43:59,430 --> 00:44:03,570
So the marginal counterfactual probabilities independent.

330
00:44:04,980 --> 00:44:09,650
Of the real life exposure experience.

331
00:44:14,880 --> 00:44:15,780
And that's, again,

332
00:44:16,200 --> 00:44:27,330
summarized in this expression The counter-factual probability at any given level of exposure should be independent of the real life experience.

333
00:44:27,350 --> 00:44:36,420
Level of exposure for all levels of exposure. That's under exchange ability.

334
00:44:36,690 --> 00:44:45,630
Now, remember, consistency. Remember, consistency led most consistency intuitively.

335
00:44:48,630 --> 00:44:57,970
The. Or.

336
00:45:04,610 --> 00:45:17,300
Yes, actually. Or you can anticipate what the counterfactual outcome would be at a given level of exposure.

337
00:45:17,740 --> 00:45:23,150
When you see in real life the outcome at that level of exposure, right?

338
00:45:27,340 --> 00:45:30,680
Yeah. It's fine to be skeptical.

339
00:45:31,270 --> 00:45:42,650
That's good. Let's see if the notation helps. So he has these these first first part of of the annotation is real life.

340
00:45:44,340 --> 00:45:57,720
Stuff, right? This is just a probability of a real life outcome among people who in real life experienced exposure to a equals one.

341
00:45:58,290 --> 00:46:01,320
Right. So this is what you observed.

342
00:46:02,070 --> 00:46:07,410
Let's say that could be the probability of Alzheimer's in people assigned to marijuana smoking.

343
00:46:07,770 --> 00:46:13,040
Right. In real. Can you?

344
00:46:14,050 --> 00:46:18,460
Anticipate or guess the counterfactual outcome.

345
00:46:20,710 --> 00:46:24,010
For those people under that same level of exposure.

346
00:46:25,980 --> 00:46:29,770
Yes. Because of consistency. It should be the same. Right.

347
00:46:31,380 --> 00:46:40,470
So this is what it says. If you pass these people through the time machine experiment at level of exposure equals one,

348
00:46:40,740 --> 00:46:45,450
you should get the same counterfactual outcome as what you observed in real life.

349
00:46:46,810 --> 00:46:50,700
A level of exposure equals one. So this is what this is.

350
00:46:51,610 --> 00:46:56,950
And the same for the people who were unexposed in real life.

351
00:46:57,700 --> 00:47:05,470
Their counterfactual outcome at level of exposure not exposed and exposed should be the same as the one you got to observe.

352
00:47:06,880 --> 00:47:12,220
Right. So in conceit to to evaluate consistency.

353
00:47:14,830 --> 00:47:25,330
Or to assume consistency. You always have to compare a real life probability with a counterfactual probability at the same level of exposure.

354
00:47:31,240 --> 00:47:35,170
This is the summary notation for consistency.

355
00:47:36,540 --> 00:47:43,859
The probability of the outcome in real life at any given level of exposure is equal to the counterfactual

356
00:47:43,860 --> 00:47:51,030
probability of the outcome at the same level of exposure among the people who were exposed to that,

357
00:47:51,390 --> 00:47:57,970
among the people who actually looked at this. So in other words, if what you're saying is someone who runs an experiment.

358
00:48:04,300 --> 00:48:11,620
That when they work with lending. Out of that population, half of them end up with lung cancer.

359
00:48:11,620 --> 00:48:18,250
And they say based on that, we're suggesting that a population that works, about half of everyone can get one.

360
00:48:19,060 --> 00:48:22,300
Get one. Based on a study.

361
00:48:23,420 --> 00:48:29,990
That's how your. That's what this is. Well if some results now apply to entire population people.

362
00:48:32,610 --> 00:48:36,810
Under exchange ability and consistency. Yes. Yes.

363
00:48:40,390 --> 00:48:51,190
Okay. Now, what's the implication of those of the notation for those two concepts exchange, ability and consistency?

364
00:48:52,270 --> 00:48:58,960
Again, this is the summary notation for exchange ability, node for exchange ability.

365
00:48:58,970 --> 00:49:04,180
You are comparing a marginal counterfactual probability in the whole population versus.

366
00:49:05,930 --> 00:49:17,840
The conditional counterfactual probabilities in real life exposure groups for consistency, you are comparing a real life probability of an outcome.

367
00:49:19,200 --> 00:49:30,120
Conditional on exposure to. The counter-factual probability under the same exposure group for the same people exposed in real life to that level.

368
00:49:32,280 --> 00:49:35,820
There is a common expression here so you can substitute.

369
00:49:37,500 --> 00:49:44,659
And. The marginal probability of the counterfactual outcome at that level of exposure is equal

370
00:49:44,660 --> 00:49:53,720
to the observed probability of the outcome in real life at the same level of exposure.

371
00:49:55,210 --> 00:50:08,660
In other words. And their exchange ability and consistency in an ideal randomized trial and association.

372
00:50:09,940 --> 00:50:19,630
Is causation. Because what you see in the real world.

373
00:50:20,890 --> 00:50:24,030
Can be expanded to the counterfactual world.

374
00:50:28,230 --> 00:50:34,050
When they counter-factual probabilities are the same as the real life probabilities.

375
00:50:36,660 --> 00:50:41,210
For the same level of exposure. You have causation.

376
00:50:49,380 --> 00:50:52,920
Okay. So we will get to practice these.

377
00:50:54,310 --> 00:51:03,610
Quite a bit more at last. And for the homework and also late in the concert, it should become much clearer.

378
00:51:06,660 --> 00:51:11,730
The important thing again, really the important concept here is that.

379
00:51:13,540 --> 00:51:25,360
For an association to be causal, to have a causal interpretation, you need to have exchange ability between exposure groups.

380
00:51:27,310 --> 00:51:35,110
And one way to reach exchange ability in the real world is by randomly assigning exposure.

381
00:51:36,910 --> 00:51:39,970
So when you randomly assign exposure.

382
00:51:41,110 --> 00:51:52,460
Whatever you see in the real world. Has a closer interpretation under the conditions offer of an ideal randomized experiment,

383
00:51:52,910 --> 00:51:59,120
which I did, there are actually more conditions, so it's ideal because of the follow.

384
00:52:01,680 --> 00:52:06,460
Everybody is followed for the development of Alzheimer's for 30 years.

385
00:52:06,480 --> 00:52:12,180
So you have to assume that nobody dies, nobody is lost.

386
00:52:12,390 --> 00:52:15,720
You follow everybody for 30 years.

387
00:52:15,870 --> 00:52:19,590
That's one of the things that makes the experiment ideal.

388
00:52:21,660 --> 00:52:31,200
Everybody assigned to marijuana is smoking smokes exactly the same amount every day of the same time with the same intensity of inhalation, etc.

389
00:52:31,560 --> 00:52:34,590
So this goes back to the well-defined interventions.

390
00:52:37,030 --> 00:52:42,310
Your exposure has to be very, very well defined for everybody in your population.

391
00:52:44,650 --> 00:52:49,720
All assigned to know marijuana. Smoking never actually smoked.

392
00:52:52,070 --> 00:52:59,720
Unless one is smoking. Never quit. So everybody has to comply with their.

393
00:53:01,080 --> 00:53:04,800
Random assign randomly assigned exposure.

394
00:53:06,810 --> 00:53:13,620
And finally, neither the researchers nor the participants know the treatment each one is receiving.

395
00:53:13,890 --> 00:53:20,400
I mean, in the case of marijuana smoking, this might be difficult to implement, but.

396
00:53:21,500 --> 00:53:25,370
The idea is that there are. There might be.

397
00:53:27,680 --> 00:53:36,360
Subconscious effects of knowing. What type of exposure you are getting that might actually affect the response.

398
00:53:37,850 --> 00:53:45,150
So have you heard of the placebo effect? This refers to the about the civil effect.

399
00:53:47,540 --> 00:53:52,340
That's actually more interesting sending. Nocebo effect.

400
00:53:53,830 --> 00:53:57,280
Did you ask them? Yeah. No, they're not civil.

401
00:53:58,620 --> 00:54:06,190
Yeah, we. In the context.

402
00:54:09,040 --> 00:54:13,240
Yeah, it's yeah, there are there are many, many versions in the.

403
00:54:14,400 --> 00:54:19,860
In some some cultures, you know, there's some, for example,

404
00:54:19,860 --> 00:54:30,180
like priests or something or I don't know who tell a person you are going to die and the person actually might get sick and die.

405
00:54:31,110 --> 00:54:39,660
It is a fascinating question, really. Anyway, so, so much for subconscious effects.

406
00:54:40,680 --> 00:54:44,069
Now, this is a side note, really.

407
00:54:44,070 --> 00:54:51,030
There was a bit of a notation similarity of two things that I want to avoid confusion about.

408
00:54:51,780 --> 00:55:01,710
You remember, in exchange ability, we saw the independence sign between a controversial outcome and the actually experienced level of exposure.

409
00:55:03,150 --> 00:55:10,950
That's exchange ability. Independence refers to a real life outcome and real life experience levels of exposure.

410
00:55:11,010 --> 00:55:15,840
So avoid getting that too mixed up.

411
00:55:17,150 --> 00:55:23,190
Um, this is a summary of, of, you know, the definition of causation versus association.

412
00:55:23,910 --> 00:55:26,880
So in causation.

413
00:55:28,160 --> 00:55:41,630
Um, you're interested in whether there are differences in counter-factual outcomes in the same whole population under two different exposure values.

414
00:55:42,170 --> 00:55:50,420
You always have to think in terms of time machine four for causation, whereas in association real world,

415
00:55:51,140 --> 00:55:58,940
the difference in outcomes is evaluated in these joint subsets and they are disjoint by their level of exposure.

416
00:55:58,940 --> 00:56:05,660
One is exposed, the other is unexposed. And these are real life exposure levels.

417
00:56:05,690 --> 00:56:14,089
Right. In causation, the outcome in other subjects of the population is evaluated.

418
00:56:14,090 --> 00:56:20,720
Had they been contractually exposed to each level of exposure in association in real life,

419
00:56:20,750 --> 00:56:27,290
the outcome is evaluated in subjects that meet the condition, having actually received one level of exposure or not.

420
00:56:27,590 --> 00:56:38,899
Right. So to summarize outcomes in causation, you have to use marginal or unconditional probabilities because again,

421
00:56:38,900 --> 00:56:44,450
you have to get in the mindset of passing the whole population to a diminishing experiment.

422
00:56:45,050 --> 00:56:50,840
Whereas in association you have to use conditional probabilities of the outcome, conditional on what?

423
00:56:51,230 --> 00:56:54,710
One being actually exposed to one thing or another.

424
00:56:54,950 --> 00:57:02,110
Okay. Now what if the exposure is not randomize, which, by the way,

425
00:57:02,590 --> 00:57:09,820
is our case in like 90 plus percent of the questions we evaluate in epidemiology because,

426
00:57:11,800 --> 00:57:20,170
you know, in this example, assigning people to smoking marijuana not may be ethically questionable, to say the least.

427
00:57:21,340 --> 00:57:29,200
So, in fact, in real life, we are stuck because we cannot do these experiments for everything.

428
00:57:29,380 --> 00:57:34,210
As they say, it's rarely when we can actually do these types of experiments.

429
00:57:36,540 --> 00:57:39,600
So are there any solutions to this?

430
00:57:40,900 --> 00:57:45,370
Yes. That's good news. And here is one proposal.

431
00:57:46,360 --> 00:57:50,590
How about finding a naturally unexposed group?

432
00:57:51,700 --> 00:57:57,280
Observe the occurrence of the outcome you need and compare it with the exposed group.

433
00:57:58,390 --> 00:58:09,650
You know that that's one option. And here I bring a possible example of what that comparison group for the marijuana smokers could be.

434
00:58:11,550 --> 00:58:24,360
How about Seventh Day Adventists? So these these persons are affiliated to a religion and because of that, have a series of of norms for for life.

435
00:58:26,130 --> 00:58:28,170
And one of them is not smoking marijuana.

436
00:58:29,980 --> 00:58:38,710
So, you know, this this group would be suitable in that they are by themselves not exposed to marijuana smoke.

437
00:58:41,800 --> 00:58:48,520
Now. Are there any problems with this comparison group in terms of how do you.

438
00:58:49,630 --> 00:58:53,390
Formulate the causal question. But.

439
00:59:02,280 --> 00:59:06,630
So this could be. Right.

440
00:59:06,920 --> 00:59:20,280
Let's say they may be different from the marijuana smokers in characteristics other than marijuana smoking, other than the exposure.

441
00:59:20,790 --> 00:59:30,430
Right. So. Are they likely to be exchangeable with marijuana smokers?

442
00:59:30,790 --> 00:59:36,390
No. How would we formulate the causal question?

443
00:59:38,070 --> 00:59:49,350
In light of this potential choice of a comparison, is the counterfactual outcome way in the Seventh Day Adventists likely to be the same as the

444
00:59:49,350 --> 00:59:57,480
counterfactual outcome of the marijuana smokers had the seventh day Adventists not being.

445
00:59:59,550 --> 01:00:06,090
Excuse me, like not being unexposed or have had the marijuana smokers not been exposed.

446
01:00:12,950 --> 01:00:18,980
Probably not. Probably not. So this is one way to pose.

447
01:00:20,290 --> 01:00:26,500
The coastal question with mutation is the controversial outcome of the Seventh Day Adventists.

448
01:00:27,490 --> 01:00:36,510
And their exposure. The same as it would be in the Seventh Day Adventists had they been exposed to marijuana smoke.

449
01:00:37,170 --> 01:00:47,150
And the answer is most likely not. So quite likely this probability is different.

450
01:00:49,540 --> 01:00:57,040
The counter-factual outcomes would not be the same because if the group actually exposed to marijuana had not been exposed,

451
01:00:57,040 --> 01:01:02,330
they may still have other predictors of Alzheimer's like tobacco smoking speech,

452
01:01:02,380 --> 01:01:07,660
and that would have made the risk of Alzheimer's different from the Seventh Day Adventists.

453
01:01:07,970 --> 01:01:11,180
Right. And.

454
01:01:12,210 --> 01:01:15,180
You can pose the question the same way.

455
01:01:18,530 --> 01:01:26,419
But what but in terms of the marijuana smokers in real life, their counterfactual outcome and their marijuana smoking,

456
01:01:26,420 --> 01:01:29,780
are they not smoked marijuana likely would still be different.

457
01:01:31,340 --> 01:01:37,400
Because they have other characteristics that may influence the risk of Alzheimer's.

458
01:01:38,480 --> 01:01:52,790
Other than the exposure. So when those counter-factual probabilities differ, we are in the presence of no exchange,

459
01:01:52,790 --> 01:02:00,710
ability or lack of exchange ability in marijuana smokers and the Seventh Day Adventists are not exchangeable with each other.

460
01:02:03,660 --> 01:02:10,780
On average. Other characteristics different from marijuana smoking are not the same.

461
01:02:12,350 --> 01:02:25,330
Between those two groups. So when the exposure is not nonrandomized such as in this case we just chose two groups to compare.

462
01:02:27,250 --> 01:02:30,730
The outcome among those may be different from that among the unexposed.

463
01:02:31,060 --> 01:02:34,120
Even if neither group had been exposed. Or vice versa.

464
01:02:35,260 --> 01:02:39,700
And as. Hennigan said.

465
01:02:40,720 --> 01:02:42,790
This is called confounding in opinions.

466
01:02:42,910 --> 01:02:51,850
And this is just, you know, a very preliminary preview because we will devote lots of time to this later in the course.

467
01:02:53,050 --> 01:03:02,510
But the whole idea here is that. When you have this situation in which the exposure is not randomized.

468
01:03:03,530 --> 01:03:09,859
The Association, you may find the difference in observe probabilities and the actually experienced

469
01:03:09,860 --> 01:03:15,710
exposure levels between two exposure groups may not have a causal interpretation.

470
01:03:19,880 --> 01:03:36,720
In this regard. Very good question, if I may rephrase it.

471
01:03:37,810 --> 01:03:44,380
If if you find that the characteristics between the two groups are on average the same.

472
01:03:45,810 --> 01:03:52,900
Can you still? Prevent the problem of not making consulting firms.

473
01:03:54,710 --> 01:03:59,820
Well, that that that's actually what we do every day in epidemiology.

474
01:03:59,870 --> 01:04:04,050
Think about that. Except. One thing, though, is.

475
01:04:06,600 --> 01:04:10,980
There will not be confounding by those characteristics.

476
01:04:12,530 --> 01:04:16,890
Because. It will be balanced between the two groups.

477
01:04:17,670 --> 01:04:21,230
The problem is. You never really know.

478
01:04:23,020 --> 01:04:31,820
Which are. The predictors of outcome or their causes of outcome that may be in balance between the two groups.

479
01:04:32,950 --> 01:04:35,140
You can never measure everything.

480
01:04:36,340 --> 01:04:43,870
And some things may be different between the groups that you don't even you haven't even thought could affect the outcome.

481
01:04:45,520 --> 01:04:49,450
Like genes and many, many other things.

482
01:04:51,320 --> 01:05:01,010
So I guess to to, you know, make the long story short or the short answer, you will really never know.

483
01:05:02,180 --> 01:05:08,450
The fact that some characteristics are the same between the groups does not guarantee that you can make yourself

484
01:05:08,540 --> 01:05:19,730
friends because there might always be other characteristics you have not measured that might still confound the beauty.

485
01:05:19,850 --> 01:05:25,060
And it's also a great question because it allows me to make a point which I will reiterate later in the course.

486
01:05:25,080 --> 01:05:37,070
But the beauty of randomization. Is that it guarantees both measured and measured characteristics, both known or unknown.

487
01:05:39,690 --> 01:05:45,440
Causes of the outcome are equally balanced. So the effects cancel out.

488
01:05:46,600 --> 01:05:53,500
And the only difference you see is purely due to the expulsion and the randomization.

489
01:05:58,420 --> 01:06:04,240
Now again, you know, these this whole thing doesn't mean that that you cannot,

490
01:06:05,920 --> 01:06:10,030
you know, write your best that estimating causal effects from nonrandomized tests.

491
01:06:10,540 --> 01:06:16,390
You still can. And as they said, this is what we do in epidemiology 90% of our time.

492
01:06:20,480 --> 01:06:27,300
But you have to just think in terms of. Making a compromise.

493
01:06:28,980 --> 01:06:33,030
If you can do a randomized trial.

494
01:06:34,450 --> 01:06:38,200
Then you can make causal inference without assumptions.

495
01:06:39,640 --> 01:06:47,110
If you go through all the steps of an ideal randomized trial and it's well-conducted, you don't have to make assumptions.

496
01:06:47,110 --> 01:06:51,370
And the results you see from the randomized trial represents causation.

497
01:06:52,900 --> 01:06:57,490
This is how the COVID vaccine trials have been done.

498
01:06:59,150 --> 01:07:04,430
Randomization people assigned to the vaccine versus those assigned to the placebo on average

499
01:07:04,790 --> 01:07:09,440
are the same with respect to everything else that can influence their response to COVID.

500
01:07:09,470 --> 01:07:13,600
Therefore, the difference between the two groups.

501
01:07:13,610 --> 01:07:17,130
That's the answer. That's because. Okay.

502
01:07:20,590 --> 01:07:30,510
But then. If you cannot randomize the exposure, then you are in the world of observational epidemiology.

503
01:07:31,690 --> 01:07:34,870
You are conducting observation in which you observe.

504
01:07:36,420 --> 01:07:42,020
Different groups and compared them. But you are not manipulating actively the exposure level.

505
01:07:43,560 --> 01:07:52,530
And because that's the case, you have to make many assumptions to make those elements such as what Sarah was asking about.

506
01:07:53,250 --> 01:07:59,160
You have to make sure that there are no differences in everything that can influence the outcome on average.

507
01:07:59,820 --> 01:08:03,780
And you have to make sure that you know everything that can influence the outcome.

508
01:08:04,520 --> 01:08:09,420
So if you are willing to make that compromise and assume that, well, you know, you have to leave it.

509
01:08:13,380 --> 01:08:23,280
So, you know, this is interesting after sort of all the formulas and notations on the essence of really these these

510
01:08:23,280 --> 01:08:31,500
discussion is that observed associations in real life do not necessarily represent causal events.

511
01:08:34,130 --> 01:08:43,130
So correlation is not causation. So people would say, okay, are there any questions?

512
01:08:52,640 --> 01:08:56,780
Okay. All right. So here is your progress report.

513
01:08:58,710 --> 01:09:02,900
By now, you should be able to state the goal of closely influencing a prevailing.

514
01:09:04,960 --> 01:09:11,290
You know, whether an exposure causes and I'll differentiate exposure from outcome.

515
01:09:11,980 --> 01:09:17,350
So if you are given a scenario you should be able to identify was exposure and was deal.

516
01:09:20,100 --> 01:09:24,300
Define individual causal effect using counterfactual notation.

517
01:09:24,870 --> 01:09:35,890
So recall when the counterfactual outcome for a person subindex or whatever at superscript level of exposures

518
01:09:36,090 --> 01:09:42,180
equals one differs from the counterfactual outcome for the individual at the other level of exposure.

519
01:09:42,210 --> 01:09:46,870
Superscript. There is an individual, but.

520
01:09:48,760 --> 01:09:56,470
Name one key problem with individual causal effects. What is the problem?

521
01:10:06,550 --> 01:10:10,600
We only live once, so they cannot be identified for the most part.

522
01:10:13,220 --> 01:10:16,620
Define average causal effect using counterfactuals.

523
01:10:18,550 --> 01:10:27,970
So the marginal probability of the outcome at a given superscript level of exposure differs from the

524
01:10:27,970 --> 01:10:33,490
marginal probability of the outcome at the superscript level of exposure other than the first one,

525
01:10:33,640 --> 01:10:41,410
right? That machine experiment. The difference in notation with the individual is that now you are using summary measures of the.

526
01:10:44,110 --> 01:10:47,680
Formulate a counterfactual question to identify a causal effect.

527
01:10:49,330 --> 01:10:55,900
So you have to compare a counterfactual probability marginal to.

528
01:10:58,120 --> 01:11:05,560
Conditional probability, also counterfactual, but conditional on actual assigned levels of exposure.

529
01:11:08,380 --> 01:11:11,950
Describe one way to estimate average cost effects without assumptions.

530
01:11:16,420 --> 01:11:22,860
Yes. Random idea. Idea. Define exchange ability.

531
01:11:24,010 --> 01:11:31,060
Intuitively. You know.

532
01:11:33,540 --> 01:11:44,790
Right. Or. Two exposure groups on average are exactly the same in every regard except the level of exposure.

533
01:11:46,290 --> 01:11:50,220
So there are a number of intuitive ways to define exchangeable.

534
01:11:50,250 --> 01:11:54,420
You don't have to do necessarily the notation, but is the concept.

535
01:11:58,030 --> 01:12:04,870
Describe a situation of non exchange ability. Well as opposed to our intuitive definition when.

536
01:12:12,190 --> 01:12:17,230
Right? Right. And I would add, although, again, we will come back to this,

537
01:12:17,500 --> 01:12:23,950
I would that and whichever characteristic or characteristics differ between expedition groups.

538
01:12:25,000 --> 01:12:34,570
Affect the outcome. That's important. If those characteristics that differ between exposure groups don't really affect the outcome.

539
01:12:36,570 --> 01:12:46,450
You may look out. And finally state the consequence of not exchange ability.

540
01:12:52,850 --> 01:12:56,420
Yes, that's a great question.

541
01:12:56,990 --> 01:13:00,770
How do you know if it is so?

542
01:13:00,920 --> 01:13:07,070
It's very it's a very profound question because. Have you heard of something called Bayesian methods?

543
01:13:07,310 --> 01:13:11,510
Bayesian based based on patient statistics.

544
01:13:12,290 --> 01:13:20,870
So we don't know really. I mean, for sure. Right. And the best we can we can do is exactly what you suggested.

545
01:13:21,770 --> 01:13:27,830
We based a lot of causal inference on, let's call, prior causal knowledge.

546
01:13:28,880 --> 01:13:42,020
So you do have to go back always to the literature and apply your best judgment with these tools as to what that has been reported may be causal.

547
01:13:42,830 --> 01:13:48,110
So from your knowledge and experience and so on.

548
01:13:50,090 --> 01:14:00,560
You have to decide. From previous studies, which could be characteristics that may affect the outcome.

549
01:14:01,840 --> 01:14:05,050
This is kind of a circular argument, you you may say. Right.

550
01:14:05,770 --> 01:14:08,930
But but in reality, it's not in, you know.

551
01:14:08,950 --> 01:14:12,400
And philosophers of science devote a lot of time to these still.

552
01:14:13,000 --> 01:14:17,130
But this is how it's done. Any. Um.

553
01:14:18,790 --> 01:14:21,790
Prospective consulting firms. You may.

554
01:14:23,090 --> 01:14:29,930
Has to be based on prior causal knowledge. There is always there is always something that is already known.

555
01:14:32,160 --> 01:14:44,430
So that's exactly how you and I might question a reference to Bayesian methods, is that Bayesian actually assigned a numerical probability.

556
01:14:45,350 --> 01:14:53,100
To the certainty. Of how causal, for example, that prior knowledge may be.

557
01:14:54,030 --> 01:14:59,640
And they used those numbers in their estimation, of new causal effects.

558
01:15:01,890 --> 01:15:05,100
So it's it's a fantastic question. And thank you. That's asking.

559
01:15:10,060 --> 01:15:13,860
Except is there quantitatively weighting a brainwash?

560
01:15:14,140 --> 01:15:17,800
Right. You know, in a nutshell, obviously, it's more complicated than that, but.

561
01:15:18,140 --> 01:15:23,460
But that's more or less what it is numerical. Prior causal knowledge.

562
01:15:23,550 --> 01:15:33,040
Prior knowledge of. So instead the consequence of not exchange ability was the consequence of non exchange ability.

563
01:15:35,680 --> 01:15:43,420
Confounding. Yeah. And in practical terms, what happens with your causal inference from observed associations can.

564
01:15:44,710 --> 01:15:50,290
Except you can't determine causation. There is no way you could say you might have a correlation or something.

565
01:15:50,710 --> 01:16:02,260
Yeah. Yeah. Correlation is synonym of association, but sometimes referred specifically to two continuous variables in the continuous scales existing.

566
01:16:04,530 --> 01:16:07,980
Okay. Any other great questions?

567
01:16:11,390 --> 01:16:14,420
All right. So then I will see you Monday.

568
01:16:15,890 --> 01:16:39,290
And have a good weekend. No, I am not ruling anything out.

569
01:16:40,320 --> 01:16:51,600
Thank you so much for your patience and. Is starting an association.

570
01:16:52,640 --> 01:17:18,240
Oh. We need another dedicated homeless strategist.

571
01:17:18,600 --> 01:17:21,800
You know I can't answer that.

572
01:17:22,500 --> 01:17:28,750
Yeah. Oh, okay.

573
01:17:31,540 --> 01:17:38,360
Maybe I should look forward.

574
01:17:38,970 --> 01:17:43,250
I think it is good for the cause. Okay.

575
01:17:48,680 --> 01:17:52,110
Okay. What do you think? Yes.

576
01:17:52,930 --> 01:17:56,880
Yes, that is correct. Yes.

577
01:17:57,720 --> 01:18:06,330
And that's why I said that.

578
01:18:06,920 --> 01:18:11,610
I wanted to say that I always.

579
01:18:14,950 --> 01:18:21,600
That's my whole life, he said.

580
01:18:22,630 --> 01:18:29,420
Why don't you give me some other instruction?

581
01:18:30,520 --> 01:18:38,860
So that's what my series is, and you'll see how that works.

582
01:18:39,820 --> 01:18:44,000
I help others write them, and they certainly keep.

583
01:18:44,500 --> 01:18:48,670
And if there is something that is required for me, they will reach out to me.

584
01:18:50,430 --> 01:18:53,499
I was like, Oh my God, can you do this thing?

585
01:18:53,500 --> 01:19:06,100
Because I think you can kill any question is a better question, but specifically but changeability.

586
01:19:07,250 --> 01:19:14,830
I was wondering if these kind of families and expressions were individual in each other,

587
01:19:14,840 --> 01:19:23,380
because I believe the credibility of this explosion and this condition of that ability and this explosion.

588
01:19:24,520 --> 01:19:31,630
And I was wondering if they were mainly individual until you kind of put them together to do this execution?

589
01:19:32,030 --> 01:19:36,009
Yeah, this is because I had it from there. Okay.

590
01:19:36,010 --> 01:19:40,810
You have this common term and this is equal to this and this is equal to the same thing.

591
01:19:41,110 --> 01:19:45,070
This must be like this. Okay, so you just pretty much combine them.

592
01:19:45,670 --> 01:19:47,890
Yep. Okay. Yes, I think.

