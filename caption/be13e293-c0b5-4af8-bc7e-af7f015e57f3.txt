1
00:00:00,330 --> 00:00:04,590
Don't forget the people in the back with the main cards.

2
00:00:23,940 --> 00:00:27,570
Okay. So it's 1:00, so I'll just start talking, hopefully.

3
00:00:28,290 --> 00:00:33,540
Um, so I think I'll say right off the top that we're recording every class.

4
00:00:33,540 --> 00:00:35,159
So that's like a thing to know.

5
00:00:35,160 --> 00:00:41,940
If for some reason you don't want to be on the recording, please let me know as soon as possible and we'll try to work something out.

6
00:00:42,470 --> 00:00:48,930
Um, if I set everything up correctly, the recordings should just automatically be on canvas at the end of class.

7
00:00:48,930 --> 00:00:54,389
So that will be available. And I think the fact that the owl has lit up means that it's recording.

8
00:00:54,390 --> 00:00:58,580
So I'm hoping that that works. Okay.

9
00:00:58,590 --> 00:01:02,160
Welcome to 881 Causal Inference.

10
00:01:03,210 --> 00:01:09,180
I'm very excited about this semester and I hope that you are all so excited about this semester.

11
00:01:10,050 --> 00:01:18,390
Today's order of business is I will do so introduction of the class and of myself and how it's going to work.

12
00:01:18,630 --> 00:01:27,180
And then we are going to jump straight into our like fundamental principles that we're going to need for causal inference.

13
00:01:27,180 --> 00:01:30,660
So we're just going to, going to get right going.

14
00:01:32,790 --> 00:01:44,670
So I think we'll start with this course orientation slide and I will show you how to get to this as part of the course orientation.

15
00:01:46,590 --> 00:01:53,940
Okay. So basics for the class. So you've all gotten to the correct room at the correct time.

16
00:01:53,940 --> 00:01:57,420
So this first bullet is a little bit unnecessary.

17
00:01:57,660 --> 00:02:00,720
Is this the projector is like hard to see, right?

18
00:02:01,440 --> 00:02:06,510
And I don't know. I think there's something I can do with the lights that might help me.

19
00:02:07,470 --> 00:02:10,950
Is that too sleepy in the front? Okay.

20
00:02:11,160 --> 00:02:16,740
It's probably easier to see. Okay.

21
00:02:16,750 --> 00:02:24,430
So that book that we will use this semester is called What If I May Go Home and James Robbins and it is free online.

22
00:02:24,640 --> 00:02:29,350
There is a link to it here. There's also a link to it in the syllabus and a link to it on canvas.

23
00:02:30,890 --> 00:02:38,350
I'll go and show you. So you can download the PDF of this book here.

24
00:02:40,450 --> 00:02:43,659
So there will be other readings that I will distribute by canvas.

25
00:02:43,660 --> 00:02:49,780
So you don't have to buy any books. I might recommend a textbook or two, but you don't have to have any of them.

26
00:02:50,590 --> 00:02:54,250
I'm currently planning to have office hours from 3 to 4 on Fridays.

27
00:02:55,930 --> 00:03:00,370
If it turns out that that is a bad time that people can't get to, I will shift them around.

28
00:03:00,370 --> 00:03:05,400
But that's the current plan. If you want to join office hours by Zoom, you can.

29
00:03:05,410 --> 00:03:11,650
But you have to tell me in advance. I won't run Zoom otherwise and there's no zoom option for the class.

30
00:03:11,660 --> 00:03:18,340
There is recording that you can do later. But no, I have no ability to join synchronously.

31
00:03:20,380 --> 00:03:25,180
If you want to see me besides that office hours, just send me an email and I will make time to see you.

32
00:03:26,140 --> 00:03:34,090
Okay, so now we're going to go look at the canvas site. So on the Canada site you'll see this.

33
00:03:34,090 --> 00:03:38,080
So there's this little link to an introductory form that was also in the announcement.

34
00:03:38,320 --> 00:03:41,500
That link will go away after this week because it's only relevant to this week.

35
00:03:41,500 --> 00:03:44,890
It asks you your name and your pronouns and whether you can come to office hours.

36
00:03:46,450 --> 00:03:54,790
The main link that you're going to use is the schedule link, and my aim is that everything that you need should be on schedule.

37
00:03:55,240 --> 00:04:00,280
So currently there are links to the slides for today's lecture.

38
00:04:01,270 --> 00:04:10,380
This. We'll become a link when the homework one is assigned, so you should be able to get to everything you need from the schedule page.

39
00:04:10,390 --> 00:04:14,260
So this tells you what we're going to do on each day, approximately in class.

40
00:04:14,500 --> 00:04:22,209
It's. I'll say likely that some of this stuff will move around that we won't see exactly

41
00:04:22,210 --> 00:04:26,350
on schedule or I will move around orders of lectures or something will change.

42
00:04:28,180 --> 00:04:32,440
I'll try not to change things too close to the moment that they happen.

43
00:04:33,400 --> 00:04:39,400
And so you should see this in class column tells you what's going to happen in class.

44
00:04:39,460 --> 00:04:43,550
Mostly lecture reading is reading that goes with the lectures.

45
00:04:43,550 --> 00:04:51,820
So R is Hernan and Robbins and then any other reading will be in this related reading tab.

46
00:04:52,150 --> 00:04:55,690
I tend to put. More in this tab.

47
00:04:55,710 --> 00:04:59,810
More on this page then I think that you will read or could read for the lecture.

48
00:04:59,820 --> 00:05:07,020
So don't think of this as required reading. Think of this as like supplemental reading that you might look at if you come back to the

49
00:05:07,030 --> 00:05:11,610
stuff later or if you're particularly interested in something or you want clarification,

50
00:05:12,270 --> 00:05:21,569
go check these out. Or if you just have if you're a fast reader and you like to read additional resources, they are here.

51
00:05:21,570 --> 00:05:27,780
And I try to put little notes about like what is in each paper so you don't have to fish around to get what you want.

52
00:05:29,850 --> 00:05:41,040
Okay. So then these two columns, homework and project are the two sort of like types of work that you will do for this class.

53
00:05:41,040 --> 00:05:44,879
So homework. I'm currently planning to have three problem sets.

54
00:05:44,880 --> 00:05:50,610
They're all in the beginning of the semester. So your last problems that will be due right before spring break.

55
00:05:53,840 --> 00:05:57,760
In the second half of the semester, most of what you will do will be the project.

56
00:05:57,770 --> 00:06:09,410
So the project is you should start working on the project soon will have a whole like introduction to what the project is the week after next.

57
00:06:10,250 --> 00:06:15,170
But the project is going to be each person will do this individually and you basically get to work

58
00:06:15,170 --> 00:06:24,379
on any kind of data analysis project that uses the tools of causal inference to accomplish it.

59
00:06:24,380 --> 00:06:30,950
So this can be a wide range of things. People did a really wide range of things last year and it was really fun to see everything that people did.

60
00:06:31,730 --> 00:06:40,280
You can analyze a public dataset, you can analyze data that you already have access to through your GSA or through your thesis work.

61
00:06:41,990 --> 00:06:46,520
Any of those things are fine if you need data.

62
00:06:47,420 --> 00:06:56,329
There will also be I have links to public data on the canvas site and we'll

63
00:06:56,330 --> 00:07:00,320
go through that in more depth when we go through that project in more depth.

64
00:07:02,770 --> 00:07:06,300
Some like scheduling structural things about the project.

65
00:07:06,310 --> 00:07:10,990
So the project starts with it happens in like a few phases.

66
00:07:10,990 --> 00:07:18,160
So the first phase is that you come up with an idea and you have a short meeting with me where you say, This is my idea.

67
00:07:18,160 --> 00:07:23,940
And I say, That sounds like a great idea. Go do it. So that's like your first step in the project.

68
00:07:23,950 --> 00:07:28,890
The second step is that you will write a proposal like a formal semi formal proposal.

69
00:07:28,900 --> 00:07:36,670
It's one page where you write out what you're going to do. And the second, third, maybe we're in the third step.

70
00:07:36,670 --> 00:07:41,170
Now, the third step is that you have a progress report.

71
00:07:41,170 --> 00:07:43,510
And so that's like 3 to 4 pages at this point.

72
00:07:43,510 --> 00:07:51,669
You've been working on your project for something like a month and you have 3 to 4 pages of what you've done so far,

73
00:07:51,670 --> 00:07:55,540
what you've learned, what you're worried about, maybe some interim results.

74
00:07:56,290 --> 00:08:05,290
You'll write that up in 3 to 4 pages. And then this is sort of the important structural thing that I wanted to get to.

75
00:08:05,620 --> 00:08:12,339
Gosh, this project is terrible. I apologize. There's more lines on this than you can.

76
00:08:12,340 --> 00:08:23,079
You can see. So my current plan is that on March 28th, we're going to have an in-class peer review of progress report.

77
00:08:23,080 --> 00:08:27,670
So what's going to happen is your hand in your progress report. I currently have it is 313.

78
00:08:28,060 --> 00:08:31,180
I will then distribute those reports to your group members.

79
00:08:31,180 --> 00:08:35,979
So you'll have three or four group members ideally who are working on something maybe related to you.

80
00:08:35,980 --> 00:08:40,540
I try to put you in groups of like people who I think will be interested in each other's projects.

81
00:08:41,950 --> 00:08:47,350
You'll read the projects, progress review reports for the people in your project groups.

82
00:08:47,710 --> 00:08:54,430
And then on 320, we meet in our groups and talk about the reviews that we've done.

83
00:08:54,430 --> 00:08:59,050
So that's a it's a great way to get feedback on what you're doing and also to get

84
00:08:59,200 --> 00:09:02,950
exposure to what other people are doing and to like push yourself to think about like,

85
00:09:03,670 --> 00:09:08,230
well, how do I think that they maybe should do this? Or How would I solve this problem that they're having?

86
00:09:08,860 --> 00:09:13,810
And you can give each other feedback. This was like, this is my favorite part of the class.

87
00:09:14,350 --> 00:09:19,720
Last year was this day. It was super fun. It was at least super fun for me.

88
00:09:19,930 --> 00:09:30,970
I think the students also had fun. So this is sort of the one day that like, it's pretty important to be here on this day.

89
00:09:31,240 --> 00:09:37,360
So the intro survey asks if you can be here, if there's enough people that can't be here on that day,

90
00:09:37,360 --> 00:09:45,790
I will try to move this around so that we can get something where everyone or close to everyone can be here.

91
00:09:46,960 --> 00:09:55,150
After the progress review, you finish up your report and you hand it in by April 12th.

92
00:09:55,750 --> 00:10:03,460
I have this option if you want a revision opportunity, if you finish your report early and you email it to me by 329,

93
00:10:03,850 --> 00:10:07,180
I will read it and give you comments and you can revise it.

94
00:10:09,610 --> 00:10:14,560
So that's an option. You don't have to do that if you're not ready to hand in a report at that time.

95
00:10:14,680 --> 00:10:18,399
So that's the structure of the project and the structure of the class.

96
00:10:18,400 --> 00:10:24,520
So mostly it will be lecture in class, we'll have some activities in class,

97
00:10:25,630 --> 00:10:31,030
there'll be homework at the beginning and then the second half of the semester will be projects.

98
00:10:31,030 --> 00:10:38,440
That's sort of structurally what will be happening. Topically, we basically go through.

99
00:10:39,540 --> 00:10:45,480
We go through the contents more or less of what ifs over the first.

100
00:10:47,210 --> 00:10:57,550
Chunk of class until about. A bit after spring break, depending on variables.

101
00:10:57,760 --> 00:11:02,320
And then after that, we do. But I'll call special topics.

102
00:11:02,350 --> 00:11:05,800
So I've sort of divided the class into three chunks, four chunks.

103
00:11:06,100 --> 00:11:10,410
The first chunk is like introduction to concepts.

104
00:11:10,420 --> 00:11:16,500
The second chunk is How do I actually estimate stuff? The third chunk is this time varying treatments chunk.

105
00:11:16,510 --> 00:11:21,460
And then the fourth chunk is. Special topics.

106
00:11:21,550 --> 00:11:25,810
So we'll have currently we're planning to have two guest lectures in special topics.

107
00:11:27,040 --> 00:11:35,050
What we cover in some of these later lectures we might sort of put to you guys of would you rather hear this topic or that topic?

108
00:11:36,910 --> 00:11:45,549
I'm excited about the our guest lecture as we have one from sushi to talk about negative controls and one from Walter to talk about interference.

109
00:11:45,550 --> 00:11:47,130
I think both of those are going to be really good.

110
00:11:47,140 --> 00:11:54,350
So those are also sort of things that it would be good to to come both to be welcoming to our guests soon,

111
00:11:54,370 --> 00:11:59,220
because I think those will be good, good content days.

112
00:12:00,910 --> 00:12:05,870
Let's see. We should look at the syllabus. I'll talk briefly about grading. Yeah.

113
00:12:06,350 --> 00:12:15,170
On the schedule when we try opening up HTML slides, is there a way to save them as a PDF to print them or so you can.

114
00:12:15,560 --> 00:12:24,560
I have yeah. I've I've worked on this a little bit so you can in your browser do print and then print it as a PDF.

115
00:12:24,770 --> 00:12:34,219
It's a little bit. I have some fancy widgets and some later slides that might not print.

116
00:12:34,220 --> 00:12:39,140
Right. But besides that, the printer pdf should work.

117
00:12:42,840 --> 00:12:48,880
Okay. Syllabus. Are there any other questions about schedule staff before I talk about syllabus?

118
00:12:52,150 --> 00:12:55,600
Did that name tag stack end up somewhere?

119
00:12:57,820 --> 00:13:01,570
Let's bring it up around here. So many names up here.

120
00:13:03,520 --> 00:13:10,570
All right. So a lot of what's in the syllabus I've already covered. I think the only thing I didn't cover is grading.

121
00:13:12,730 --> 00:13:22,180
So I've laid it out as 30% homework, 50% project with the breakdown here, and then 20% in class activities.

122
00:13:22,510 --> 00:13:27,790
So the in-class activities, the plan is pretty much every lecture except for today.

123
00:13:27,910 --> 00:13:33,070
There will be some small activity that probably you'll do in like pairs, maybe threes.

124
00:13:34,390 --> 00:13:39,310
Write your name on it, hand it back to me. They're graded on completed, not completed.

125
00:13:42,010 --> 00:13:47,410
And I have it scaled so that to get this full 20%,

126
00:13:47,800 --> 00:13:55,090
you only need to complete 80% of the in-class activities so that if you're sick or are gone for a while,

127
00:13:55,090 --> 00:13:58,600
it's not it shouldn't it shouldn't impact you too much.

128
00:13:58,600 --> 00:14:01,950
And if something comes up where you really can't ever be in class for some reason,

129
00:14:01,960 --> 00:14:14,350
to me an email I think I right although right we have this grading scale and I try to use grades in point feedback to communicate,

130
00:14:15,040 --> 00:14:22,690
you know, how well does it seem like you've understood the things that I've hopefully tried to communicate with you?

131
00:14:23,620 --> 00:14:29,439
The goal is not I would be delighted if everyone got an A in this class and it's completely plausible.

132
00:14:29,440 --> 00:14:34,959
So the goal is not to be like. Hard in terms of grading.

133
00:14:34,960 --> 00:14:38,350
So I would view you the points as feedback and not as.

134
00:14:44,690 --> 00:14:50,480
Don't. Don't be worried about your grade, I guess, is what I'm saying. So I think that's.

135
00:14:51,910 --> 00:14:56,590
That is the content of canvas. The recording should show up in Panopto recordings.

136
00:14:56,650 --> 00:15:01,600
Hopefully I've not used Panopto before. I'm really hoping it works the way I think it works.

137
00:15:03,160 --> 00:15:07,420
Are there questions about canvas site material schedule stuff?

138
00:15:11,140 --> 00:15:13,719
Okay, so if there's not questions,

139
00:15:13,720 --> 00:15:19,720
I'll do a little intro about what causal inferences and then we're going to do a bigger intro about what causal inferences.

140
00:15:19,750 --> 00:15:29,050
So I like to describe causal inference as the art and science of using statistical tools to answer causal questions.

141
00:15:29,060 --> 00:15:34,930
So we have a causal question why does this happen? And we're going to translate that into a math problem, right?

142
00:15:34,930 --> 00:15:42,820
That's sort of the whole game of causal inference. How can I take this science question and turn it into a math problem that I can solve?

143
00:15:44,050 --> 00:15:48,520
So interpreting statistical parameters as causal parameters requires two things.

144
00:15:48,520 --> 00:15:54,399
We need basically philosophy. We need to understand what does causality mean.

145
00:15:54,400 --> 00:15:56,650
So that can't come from our mathematical models.

146
00:15:56,650 --> 00:16:04,600
It's sort of a philosophical idea, and we're going to have to take that philosophy and turn it into a math, right?

147
00:16:04,600 --> 00:16:08,770
So we have to turn that into a model for the system that we're studying.

148
00:16:08,950 --> 00:16:12,730
And then from that, we can get a causal interpretation of a statistical parameter.

149
00:16:13,900 --> 00:16:20,170
So what we're going to learn in this class, sort of the big overview, we're going to do languages of causality.

150
00:16:20,170 --> 00:16:26,910
So there's sort of three. Equivalent ish ways to talk about causality.

151
00:16:26,930 --> 00:16:30,650
One of them is called counterfactuals and potential outcomes, and that's where we're going to start.

152
00:16:31,010 --> 00:16:34,340
Here's a graphical models version of how to talk about causality,

153
00:16:34,520 --> 00:16:40,579
and then there's structural equations and we're going to talk about identify ability, condition.

154
00:16:40,580 --> 00:16:46,450
So this is the question of when would I even have any hope of estimating a parameter, right?

155
00:16:46,470 --> 00:16:52,490
I have a parameter. Hypothetically, if I had infinite data, could I estimate that parameter?

156
00:16:53,180 --> 00:16:58,280
And then we're going to talk about estimation strategies. So what do I actually do when I actually have data?

157
00:16:58,280 --> 00:17:07,729
How do I estimate these parameters? So we'll go through a range of these strategies a little bit about me.

158
00:17:07,730 --> 00:17:12,590
So imagine you can call me Jean if you want to call me Dr. Maurice, and you can.

159
00:17:13,490 --> 00:17:22,280
Jean is my preference. My pronouns are they them? My research interests are mostly in the statistical genetics realm.

160
00:17:22,300 --> 00:17:27,709
So I do a lot of work with instrumental variable analysis for genetics.

161
00:17:27,710 --> 00:17:33,700
So we'll have several days of lecture on instrumental variable analysis because I think it's really cool.

162
00:17:33,700 --> 00:17:36,790
And so we're going to talk about it for a while. That'll be at the end of the semester.

163
00:17:37,870 --> 00:17:43,149
I work with high dimensional biological data and I'm interested in data integration, right?

164
00:17:43,150 --> 00:17:46,270
So how do I combine multiple high dimensional data sources?

165
00:17:46,270 --> 00:17:49,210
So not all of this is causal, some of it has causal connections.

166
00:17:50,530 --> 00:17:54,430
And then I'm interested in applications of machine learning tools and causal inference.

167
00:17:55,510 --> 00:18:02,590
My fun facts are I like animals and outdoor activities, hiking and kayaking and that sort of thing.

168
00:18:03,410 --> 00:18:13,209
But my sort of like ethos about this class is that my goal is that we have sort of an active, collaborative learning environment.

169
00:18:13,210 --> 00:18:18,600
So that's why when we do the in-class activities, I put you in groups and make you talk to each other.

170
00:18:18,610 --> 00:18:22,479
It's why we're going to do the peer review and you all interact with each other's work.

171
00:18:22,480 --> 00:18:30,820
The idea is really that you can work together to help each other learn and, you know, help me learn.

172
00:18:30,820 --> 00:18:37,719
I always learn when I teach this class, basic ground rules, be engaged, ask questions when you have them,

173
00:18:37,720 --> 00:18:42,190
contribute to discussions, try to complete the assignments on schedule.

174
00:18:42,190 --> 00:18:46,540
I'll talk a little bit more about ness in a few slides.

175
00:18:47,530 --> 00:18:55,150
Be respectful of your classmates and me and then please contact me if something's not working for you.

176
00:18:55,300 --> 00:19:01,270
If you're struggling, if you have any kind of issue that's class related, please reach out.

177
00:19:01,570 --> 00:19:06,160
Sooner is always better than later. Okay.

178
00:19:06,190 --> 00:19:10,210
I think I've covered this content. No zoom option.

179
00:19:12,220 --> 00:19:23,709
All the classes will be recorded. It's basically to contact their OC late policy, so the due dates in the schedule are intended to keep you on track,

180
00:19:23,710 --> 00:19:28,960
keep you up with the class, make it so you don't have a huge amount of stuff to do at the end of the semester.

181
00:19:30,670 --> 00:19:34,170
I don't. Feel particularly strongly that like.

182
00:19:35,100 --> 00:19:41,999
Due dates are like a fundamental good. So if something comes up and you need to be later, just let me know and we'll work it out.

183
00:19:42,000 --> 00:19:49,020
So send me an email. I've asked that you email me two days in advance if something comes up that that's not possible.

184
00:19:49,500 --> 00:19:56,500
You know, we'll work it out. But this is my request for like my personal time management and mental health.

185
00:19:56,520 --> 00:20:00,860
So if something's due on Monday, email me by Thursday and say I need to turn it in on Tuesday.

186
00:20:00,870 --> 00:20:05,199
Something like that. I don't have late penalties.

187
00:20:05,200 --> 00:20:08,880
So if we agree to a time that you can hand something in late, that's fine.

188
00:20:10,500 --> 00:20:18,090
Keep in mind like I can't post homework keys until you hand in your assignment, so try to be respectful of your classmates in that.

189
00:20:19,380 --> 00:20:25,200
And then there's three exceptions to my general flexibility on deadlines.

190
00:20:25,200 --> 00:20:33,360
So one of them is the progress report has to get handed in at the time that it's due because it's going to get dispersed to your classmates.

191
00:20:33,600 --> 00:20:37,589
So if you hand it in late, your classmates won't have enough time to read it.

192
00:20:37,590 --> 00:20:40,950
And I can't be in that position of mediating that.

193
00:20:40,950 --> 00:20:47,640
So prioritize getting the progress report in on time and then things that happen in-person.

194
00:20:48,120 --> 00:20:58,859
I can't postpone. So the progress report review day happens when it happens that there are final presentations on the last two days of class.

195
00:20:58,860 --> 00:21:04,980
I think I didn't quite get there. Right. So the last two meeting days, one is our last day of class and one is our scheduled final exam.

196
00:21:05,190 --> 00:21:11,880
You'll give a presentation about the project that you did. So obviously those are not postpone together the last days of class.

197
00:21:12,390 --> 00:21:18,090
And then the project report really should ideally get handed in on 412 because

198
00:21:18,090 --> 00:21:21,330
I have to grade them all and get your grades in by a certain amount of time.

199
00:21:22,110 --> 00:21:24,659
There's like a little bit of flexibility there, right?

200
00:21:24,660 --> 00:21:31,260
If you're like, I need to hand it in on 413 and not everyone does that, but I can probably say yes, but there's less flexibility there.

201
00:21:32,010 --> 00:21:36,500
Are there questions about that stuff? Okay.

202
00:21:38,680 --> 00:21:43,510
Interest survey I've now advertised several times. So I go take that.

203
00:21:44,050 --> 00:21:49,600
And I think that is. The end of our introductory slide.

204
00:21:49,870 --> 00:21:53,710
This is my cats here to welcome us to causal inference.

205
00:21:55,840 --> 00:22:00,080
All right, so I will start our introduction, I suppose.

206
00:22:00,130 --> 00:22:04,340
Any comments? All right.

207
00:22:04,540 --> 00:22:10,430
So. Introduction.

208
00:22:12,130 --> 00:22:15,130
All right. So this lecture has sort of three sections.

209
00:22:15,370 --> 00:22:19,090
I don't think that we're going to get through all of them today.

210
00:22:19,120 --> 00:22:22,390
I think we go till 230.

211
00:22:22,780 --> 00:22:28,720
I should know this. Yes, we go till 230. So I don't think that we'll get through all three sections today, which is fine.

212
00:22:29,110 --> 00:22:31,390
So we're going to start with motivation and instruction,

213
00:22:32,260 --> 00:22:40,750
and then we're going to learn what counterfactuals are and what a causal effect is under this particular sort of logical framework.

214
00:22:41,110 --> 00:22:45,040
And then we're going to talk about how to identify a causal effect. So jumping right in.

215
00:22:48,290 --> 00:22:51,320
Okay. So motivation. So I think the.

216
00:22:52,870 --> 00:22:59,160
The idea that being able to ask a Y question is interesting is like intrinsically motivating, right?

217
00:22:59,170 --> 00:23:04,030
That's why we're all here. That's why we're all scientists. Right? You want to know why do people get sick?

218
00:23:04,030 --> 00:23:09,639
Why do people get in car accidents? Why do some people survive longer than other people?

219
00:23:09,640 --> 00:23:15,490
Right. These are all kinds of questions. The answer to this, anything that starts with because.

220
00:23:15,490 --> 00:23:26,890
Right. That's a causal statement. So causal inference is this framework of formalizing what it means to answer why.

221
00:23:27,580 --> 00:23:32,770
And so the idea that we need to formalize this question or formalize our answer to

222
00:23:32,770 --> 00:23:37,629
this question is a much newer idea than the idea that asking why is interesting,

223
00:23:37,630 --> 00:23:43,720
which sort of I would think is maybe like a fundamental human, human state.

224
00:23:45,700 --> 00:23:49,359
So I will I'll say that causal inference has three elements.

225
00:23:49,360 --> 00:23:56,290
I think you can break this down in different ways. So one of them is a formal language model of causality.

226
00:23:56,290 --> 00:24:01,210
So this is the philosophy slash logics, part of causal inference.

227
00:24:01,840 --> 00:24:05,770
The second part is a connection between the language and mathematical models.

228
00:24:07,060 --> 00:24:13,650
And then the third part is, once I have the mathematical model, how do I estimate the parameters?

229
00:24:13,660 --> 00:24:21,880
So that's just statistics. So this last part, the statistics part is going to look moderately like all of the statistics that you've learned before.

230
00:24:21,890 --> 00:24:29,260
Right? You can bring all of the tools that you've learned so far to this to this problem.

231
00:24:29,530 --> 00:24:34,810
The unique part is connecting those back to causal parameters using the first two parts.

232
00:24:34,990 --> 00:24:40,930
And there are some types of analysis that you will not see outside of causal inference.

233
00:24:40,930 --> 00:24:48,490
So things that you may or may not run into outside of the context of trying to answer a causal question.

234
00:24:49,480 --> 00:24:53,800
All right. So we'll do this sort of motivating example, which is smoking and lung cancer.

235
00:24:54,010 --> 00:24:59,230
I think this is sort of a well-known example, so you might know some of this stuff.

236
00:24:59,260 --> 00:25:03,240
So in 1900, lung cancer was extremely rare.

237
00:25:03,250 --> 00:25:11,260
So there was one report that in 1900, all of medical literature had seen 140 total cases of lung cancer.

238
00:25:11,290 --> 00:25:15,520
So probably they missed some, but it was still something that was considered very rare.

239
00:25:17,530 --> 00:25:22,840
By the 1920s, incidence of lung cancer had increased dramatically.

240
00:25:23,950 --> 00:25:27,700
And one of the hypothesized causes of this was smoking, right?

241
00:25:28,000 --> 00:25:31,990
Smoking had picked up dramatically between 1919 20.

242
00:25:32,260 --> 00:25:37,150
As early as 1912, there was a guess. Maybe smoking is causing lung cancer.

243
00:25:37,220 --> 00:25:40,420
Right. That's very early, considering what we know about that story.

244
00:25:41,230 --> 00:25:45,820
However, a lot of other things changed between 1919 20.

245
00:25:46,120 --> 00:25:49,569
And so there were a lot of other hypothesized causes of lung cancer, right.

246
00:25:49,570 --> 00:25:54,010
So there was air pollution, asphalt, dust from newly constructed roads.

247
00:25:54,760 --> 00:25:58,900
World War One had happened. There was the influenza pandemic.

248
00:26:01,830 --> 00:26:07,210
Demographics of the population were changing. And then there was also increased diagnosis of lung cancer.

249
00:26:07,390 --> 00:26:14,549
So as it was seen more, it was diagnosed more with maybe like misdiagnosed as tuberculosis previous to 1900.

250
00:26:14,550 --> 00:26:17,970
And now we were more familiar with what it looked like.

251
00:26:18,810 --> 00:26:25,830
All right. So the case against smoking was sort of built between 1920 and 19, let's say 60.

252
00:26:26,280 --> 00:26:30,959
And this was sort of founded on a few arms. So one of them was observational studies.

253
00:26:30,960 --> 00:26:35,400
So observational studies show a strong association between smoking and lung cancer.

254
00:26:36,510 --> 00:26:46,620
The earliest studies were case control studies. There were also prospective studies that followed healthy smokers and nonsmokers over time.

255
00:26:47,010 --> 00:26:51,180
And then all of these studies observe a very strong association.

256
00:26:51,180 --> 00:27:00,870
So one of the estimates from 1954 is a 40 times increased odds ratio at 40 times increased odds of lung cancer for smokers.

257
00:27:00,870 --> 00:27:07,260
So that's huge rate and odds ratio of 40 is like really quite outside of the norm.

258
00:27:09,180 --> 00:27:19,050
There's also supporting evidence from animal studies. So they do these studies where they expose, I think it was mostly rabbits to tobacco products.

259
00:27:20,130 --> 00:27:26,880
They're a little bit like smear the tobacco product on the rabbit and then they see if the rabbit gets cancer where you put the tobacco product.

260
00:27:26,880 --> 00:27:33,150
And the answer is it does, and then there's some mechanistic evidence.

261
00:27:33,150 --> 00:27:43,740
So there was a chemical analysis of cigaret smoke to identify that it does in fact contain chemicals that are known to cause cancer.

262
00:27:43,740 --> 00:27:50,190
So a lot of that work, the chemical composition work, was done by tobacco companies themselves.

263
00:27:50,640 --> 00:27:57,390
So by the 1950s, tobacco companies internally believe that tobacco causes cancer.

264
00:27:59,900 --> 00:28:03,140
Okay. So what are the challenges to the smoking lung cancer link?

265
00:28:03,980 --> 00:28:09,140
So tobacco companies invest a lot of money in challenging the link between smoking and lung cancer.

266
00:28:09,950 --> 00:28:15,410
Only about a third of doctors in 1960 believe that smoking is a major cause of lung cancer.

267
00:28:16,100 --> 00:28:18,320
And then I think there's this really interesting story.

268
00:28:18,330 --> 00:28:28,430
So Ari Fleischer, one of our like statistical forefathers, was a famous challenger of this smoking lung cancer hypothesis.

269
00:28:28,430 --> 00:28:35,210
So he suggested that there could be this genetic factor that's linked to both smoking and lung cancer,

270
00:28:36,080 --> 00:28:42,260
and that this genetic factor is explaining the association between smoking and lung cancer in the population.

271
00:28:44,810 --> 00:28:49,040
So in picture form, this is Fisher's argument.

272
00:28:49,790 --> 00:28:57,350
We have a genetic variant that causes smoking, it causes lung cancer, and that's causing the association between smoking and lung cancer.

273
00:28:57,590 --> 00:29:00,830
There's no link between smoking and lung cancer in this model.

274
00:29:02,010 --> 00:29:09,590
Okay. So Fisher is right that this model would explain an association between smoking and lung cancer.

275
00:29:10,130 --> 00:29:18,890
So what's wrong with Fisher's argument? So one of the things that's wrong with it is that the effect size of that genetic variant on smoking

276
00:29:18,890 --> 00:29:26,180
and on lung cancer would have to be extremely large to explain the association that we're seeing.

277
00:29:26,810 --> 00:29:30,560
So. Kornfeld at all. So this is this paper is interesting.

278
00:29:30,560 --> 00:29:37,630
It's one of. I think it's viewed as one of the first sensitivity analyzes in statistics.

279
00:29:38,560 --> 00:29:46,870
So they do this analysis where they say, how big would these effects have to be to explain this fourfold increase in odds of lung cancer?

280
00:29:47,170 --> 00:29:48,760
And they say, well,

281
00:29:48,760 --> 00:29:57,129
this genetic variant would have to be nine times more prevalent in smokers than nonsmokers and 60 times more prevalent in two pack a day smokers,

282
00:29:57,130 --> 00:30:02,020
which is like not consistent with patterns that we'd see in the population if that was true.

283
00:30:02,140 --> 00:30:06,160
We would expect smoking to be very concentrated in families, which we know that it's not.

284
00:30:08,320 --> 00:30:14,950
The genetic hypothesis is also inconsistent with the fact that lung cancer rates have increased over time.

285
00:30:15,050 --> 00:30:15,240
Right.

286
00:30:15,290 --> 00:30:25,390
If it's a genetic variant, we would expect it to be causing lung cancer throughout all of human history and not just beginning in the 20th century.

287
00:30:28,090 --> 00:30:34,190
The common cause argument arguments so generally. So even if you say, well, it's not a genetic factor, it's something else,

288
00:30:34,190 --> 00:30:37,690
there's some other common cause you're never going to be able to disprove that.

289
00:30:37,850 --> 00:30:42,309
You can never say there's no common cause of smoking and lung cancer that's leading

290
00:30:42,310 --> 00:30:46,480
to this association because we just don't know everything about how the world works.

291
00:30:46,990 --> 00:30:50,860
But we have all of these other lines of evidence, right? We have the animal studies.

292
00:30:51,520 --> 00:30:58,020
We have the longitudinal studies. And we have some concrete, mechanistic hypotheses, right?

293
00:30:58,020 --> 00:31:02,660
We can point to the chemicals that we say we think this chemical causes cancer.

294
00:31:02,670 --> 00:31:06,750
So all of these together form a story that make us think.

295
00:31:07,990 --> 00:31:11,200
The causal model is probably the right model.

296
00:31:12,730 --> 00:31:21,440
So there's a couple lessons to learn from this study. One of them is that we can't do causal inference only with statistical procedure.

297
00:31:21,460 --> 00:31:29,680
So if we just had people who smoke or don't smoke and people who have lung cancer or don't do lung cancer, there would be no way for me to tell you.

298
00:31:30,810 --> 00:31:34,080
Whether smoking causes lung cancer. Does lung cancer cause smoking?

299
00:31:34,440 --> 00:31:38,009
Do they have some common cause? I have to have a model.

300
00:31:38,010 --> 00:31:41,460
I have to have some kind of assumptions external to the study.

301
00:31:43,080 --> 00:31:48,000
So all causal analysis in observational data requires unprovable assumptions.

302
00:31:49,390 --> 00:31:54,549
And then there's this quote, which is from Judea Pearl that I think expresses this idea really nicely.

303
00:31:54,550 --> 00:32:00,130
It's Judea. Pearl writes, He does not hold himself back from one word.

304
00:32:00,160 --> 00:32:08,500
So it's a little bit floral. But he says behind every causal claim there must lie some causal assumption that is not

305
00:32:08,500 --> 00:32:13,210
discernible from the joint distribution and hence not testable in observational studies.

306
00:32:13,540 --> 00:32:17,650
Such assumptions are usually provided by humans resting on expert judgment.

307
00:32:18,070 --> 00:32:24,430
Thus, the way humans organize and communicate experience, experiential knowledge becomes an integral part of the study.

308
00:32:24,610 --> 00:32:28,870
For it determines the veracity of the judgments experts are requested to articulate.

309
00:32:29,770 --> 00:32:38,280
So what he's saying is. You can't do causal inference without putting in human knowledge about how you think the world works, right?

310
00:32:38,300 --> 00:32:41,510
What you think the reasonable space of models is.

311
00:32:42,500 --> 00:32:51,650
And you have to be able to encode that knowledge in some way that then you can turn into a model with something you can estimate.

312
00:32:53,180 --> 00:32:58,430
So this this quote, by the way, is from this dude. April's book is very good.

313
00:32:59,210 --> 00:33:03,200
It is written more or less in the style of this paragraph. So it's not the most like.

314
00:33:04,470 --> 00:33:14,190
Easy reading. But it is a good reference and I put a link here and I wrote, I think we'll have some PDFs from, from that book this semester.

315
00:33:15,960 --> 00:33:23,250
Okay. So the second lesson. So the second lesson here is that we had to do this analysis in observational data.

316
00:33:23,250 --> 00:33:30,260
So we. Solidified that we think that there is an effect of smoking on lung cancer in humans.

317
00:33:31,380 --> 00:33:38,580
Essentially entirely through observational studies. These were supplemented with interventional experiments done on animals, right?

318
00:33:38,580 --> 00:33:47,159
The rabbit studies, but very arguably the rabbit study doesn't show you that it's causing lung cancer in humans.

319
00:33:47,160 --> 00:33:52,799
Right. Humans aren't rabbits and humans consume tobacco in a very different way than the rabbits were consuming the tobacco,

320
00:33:52,800 --> 00:34:02,070
which was have it applied to them. So we couldn't do a randomized trial of smoking for multiple reasons.

321
00:34:02,070 --> 00:34:07,890
So one of them is that it's unethical to do a randomized trial on smoking if you think that smoking causes lung cancer,

322
00:34:08,640 --> 00:34:13,770
but maybe even more problematic, right? If you're going to argue smoking doesn't cause lung cancer.

323
00:34:13,890 --> 00:34:19,080
Therefore, we could do a trial. It's completely impractical to do a trial of smoking.

324
00:34:19,080 --> 00:34:26,310
Right? You would have to randomize people to you smoke a pack a day every day for the next 40 years, whether you want to or not.

325
00:34:26,610 --> 00:34:31,110
No one's going to do that, right? No one is going to sign up for this study.

326
00:34:31,380 --> 00:34:34,410
And if people did sign up for the study, the adherence would be terrible.

327
00:34:34,470 --> 00:34:44,190
Right. So we can't do a trial of smoking. And there's many types of exposure that fall into this category that we could never do a trial of either,

328
00:34:44,190 --> 00:34:47,550
because we don't have an intervention really that does the thing that we want to do,

329
00:34:47,910 --> 00:34:52,410
or because the intervention is impractical, it takes too long or it's unethical.

330
00:34:53,360 --> 00:35:00,350
So being able to get causal inference out of observational studies is really critical despite this problem.

331
00:35:01,070 --> 00:35:08,690
You might view it as a problem, might not that we had from the previous lesson that we have to add information.

332
00:35:08,720 --> 00:35:15,170
You might view this as bad or you might say, well, if I have to add information, I just don't want to do causal inference in observational data.

333
00:35:16,250 --> 00:35:20,000
Well, if you refuse to do causal inference in observational data,

334
00:35:20,180 --> 00:35:26,120
you're missing out on a whole host of scientific information that we could be getting.

335
00:35:26,120 --> 00:35:33,919
Right. So there's sort of this argument that, you know, in maybe the last several decades of statistics,

336
00:35:33,920 --> 00:35:39,799
statisticians have been maybe rightly very shy about saying that they think that they have a causal effect.

337
00:35:39,800 --> 00:35:43,730
Right. We're really trained to say like this is an association between X and Y.

338
00:35:44,720 --> 00:35:47,930
If I can't justify that, it's causal. I'll just say that it's an association.

339
00:35:48,740 --> 00:35:51,260
There's one argument that, well,

340
00:35:51,260 --> 00:35:59,780
if we've really done careful analysis and we believe more or less the judgments that we put in to build our model and we did it correctly,

341
00:35:59,990 --> 00:36:05,780
then we do have causal inference and we should say that we think that we've measured a causal effect to the best of our ability.

342
00:36:06,020 --> 00:36:12,550
So folks like. Hernan and Robbins and others have made this argument that.

343
00:36:13,710 --> 00:36:17,730
While it's good to be cautious about declaring something to be causal when you

344
00:36:17,730 --> 00:36:21,540
really are measuring an association or really are doing descriptive analysis.

345
00:36:22,860 --> 00:36:29,730
If you do causal inference correctly, you can try to make causal claims and we shouldn't.

346
00:36:31,160 --> 00:36:35,030
Be afraid to do that because it required some judgments and we might be wrong.

347
00:36:37,090 --> 00:36:42,649
Okay. So this brings us to sort of the history of causal inference,

348
00:36:42,650 --> 00:36:51,500
which kind of happened in the same time period that this smoking saga was happening, although they weren't immediately related.

349
00:36:51,530 --> 00:36:55,700
So the first thing that we need is notation, right?

350
00:36:55,700 --> 00:37:01,790
So naming in 1923 formalizes some potential outcomes, notation.

351
00:37:02,960 --> 00:37:12,230
Fisher In 1925, another appearance from Fisher in just 13 slides is sort of called the father of randomization, right?

352
00:37:12,230 --> 00:37:19,280
So Fisher tries to really popularize this idea that you should do these randomized experiments.

353
00:37:21,080 --> 00:37:26,690
And then on the graphical model side, these are sort of first developed by right beginning in 1921.

354
00:37:27,890 --> 00:37:32,360
All right. So languages of causality, we mentioned this a little bit in the introduction.

355
00:37:32,930 --> 00:37:38,000
So one of the languages that we're going to start with is potential outcomes or counterfactuals.

356
00:37:38,240 --> 00:37:42,309
I will use those. Terms more or less interchangeably.

357
00:37:42,310 --> 00:37:47,470
I think the difference is that potential outcomes might happen in the future and counterfactuals could have happened in the past.

358
00:37:47,480 --> 00:37:51,370
So it depends where you are in time relative to what you're talking about.

359
00:37:53,580 --> 00:38:05,580
So the language of counterfactuals begins in 1923 and then it is really deeply developed by Donald Rubin and many others beginning in 1974.

360
00:38:05,820 --> 00:38:08,940
This is often referred to as the Rubin causal causal model.

361
00:38:09,070 --> 00:38:13,920
If you come across that, you'll know approximately what they're talking about.

362
00:38:16,230 --> 00:38:19,260
Kind of a complimentary idea is this graphical approach.

363
00:38:19,500 --> 00:38:25,630
So we already saw one graph, right? We saw that graph of smoking, the genetic factor and lung cancer.

364
00:38:25,680 --> 00:38:31,650
So that's one way of encoding this information that we think there's a common cause potentially of lung cancer in smoking.

365
00:38:32,880 --> 00:38:40,620
So using graphs like that is developed first by right in 1921, our current.

366
00:38:42,970 --> 00:38:46,540
The biggest contributors currently to that field. Judea Pearl is a big one.

367
00:38:46,540 --> 00:38:55,090
So you will sometimes hear maybe arguments between like the Pearl Castle camp and the Rubin Castle camp.

368
00:38:57,260 --> 00:39:01,280
I will if you're interested. I'll post some blog posts where you can see them arguing with each other.

369
00:39:03,660 --> 00:39:07,889
And then I would I would say also complimentary to both of these structural equations.

370
00:39:07,890 --> 00:39:14,370
So you can structural equations are an equation way of representing causal effects.

371
00:39:14,370 --> 00:39:21,510
You can turn both potential outcomes and these graphical representations into structural equation models.

372
00:39:21,900 --> 00:39:29,220
So under some conditions, depending on what assumptions you make, etc., all three of these languages are.

373
00:39:30,400 --> 00:39:35,290
If not exactly equivalent, mutually compatible. So I would say don't get.

374
00:39:37,600 --> 00:39:40,360
If this is a controversy you've seen before,

375
00:39:40,510 --> 00:39:46,570
don't get too hung up on whether you're going to be in the Pearl camp or the Rubin camp or some other camp.

376
00:39:47,650 --> 00:39:50,920
They they all have ideas that are useful. They all go together.

377
00:39:50,920 --> 00:40:01,870
So we're going to use all of them in this class. Don't don't worry too much about the whatever like factionalism you may have seen in this world.

378
00:40:03,280 --> 00:40:08,649
Okay. So this brings us to section two of this lecture, counterfactuals and treatment effects.

379
00:40:08,650 --> 00:40:13,930
Are there any questions or comments about what we saw in Section one?

380
00:40:17,970 --> 00:40:21,750
Okay. All right.

381
00:40:22,110 --> 00:40:26,669
Counterfactuals slash potential outcomes. So we need to be able to formalize.

382
00:40:26,670 --> 00:40:31,470
What does it mean to say that a causes Y? So A is our treatment. Why is an outcome that we care about?

383
00:40:32,560 --> 00:40:34,370
So we're going to introduce some new notation.

384
00:40:34,380 --> 00:40:48,660
So a counterfactual value y of big a i equals little a is the value of y that the individuals in the unit in our sample would have had if

385
00:40:48,660 --> 00:40:58,530
a had been intervened on to set to a so counterfactual is this grammatical term and it refers to this grammar of would have had right so.

386
00:41:00,530 --> 00:41:07,100
If I had skipped breakfast this morning, I would be tired. If I had not skipped breakfast this morning, I would have more energy.

387
00:41:10,130 --> 00:41:15,160
So there's several equivalent notations for this this idea.

388
00:41:15,170 --> 00:41:20,780
So the parentheses Y of A is the one that I will primarily use in the slides.

389
00:41:21,530 --> 00:41:25,050
There's also the superscript notation y, superscript a.

390
00:41:25,070 --> 00:41:29,540
So this is the notation used in the Hernan and Robbins book.

391
00:41:31,340 --> 00:41:40,670
I think this can occasionally get confusing if you also have super scripts in your in your formulas, which is why I use the parentheses.

392
00:41:40,940 --> 00:41:45,500
The parentheses can also be confusing if you have functions, right? So they're kind of like functions, right?

393
00:41:45,740 --> 00:41:49,879
Y of a is a function of the intervention.

394
00:41:49,880 --> 00:41:56,150
A But it's different from just a function, right? It contains this idea of an intervention.

395
00:41:56,540 --> 00:42:00,800
And then there's the do operator. So this is due to Apple's notation.

396
00:42:01,040 --> 00:42:10,190
You say Y given do a equals A, which you're supposed to really read as y given that I intervened and set a equals two.

397
00:42:10,400 --> 00:42:15,620
So this is sort of emphasizing the action, the action of an intervention.

398
00:42:16,520 --> 00:42:22,970
There are very subtle differences between the do operator and counterfactuals.

399
00:42:24,890 --> 00:42:31,530
Those differences can be. That resolved by making some assumptions.

400
00:42:33,660 --> 00:42:39,570
So a counterfactual is fundamentally about a hypothetical intervention or treatment.

401
00:42:40,650 --> 00:42:45,459
There is some. There's some philosophizing to do about.

402
00:42:45,460 --> 00:42:48,970
Can I talk about a counterfactual of something that.

403
00:42:50,540 --> 00:42:54,980
We don't have an intervention on. So there's sort of two questions, right?

404
00:42:54,990 --> 00:42:58,630
Can I talk about a counterfactual of something that there is no intervention on?

405
00:42:58,640 --> 00:43:04,400
So if you were six feet tall instead of five foot seven.

406
00:43:04,400 --> 00:43:09,500
Right, there's no intervention where I can intervene and make you six feet tall.

407
00:43:09,710 --> 00:43:16,220
Does that mean that I can't talk about the causal effect of height on some trait that I care about?

408
00:43:16,520 --> 00:43:26,059
And then there's also questions of there are traits where intervening on that trait without changing anything else doesn't make any sense.

409
00:43:26,060 --> 00:43:31,910
Right? So the common examples here are like race and gender, right?

410
00:43:31,910 --> 00:43:39,809
If. If somebody was a different gender, that would also change lots and lots of things about their life.

411
00:43:39,810 --> 00:43:46,110
Right. So does it make sense for me to talk about an intervention where I change someone's gender and don't change anything else in their life?

412
00:43:46,110 --> 00:43:51,300
And those are sort of two different philosophical questions. We'll come back to them.

413
00:43:51,600 --> 00:43:55,030
I think there are. Yeah. We'll come back to them.

414
00:43:55,040 --> 00:43:59,660
I have some positions on them and we can have a little discussion about what that means.

415
00:44:02,180 --> 00:44:06,050
All right, so we'll do this example. So this is an example where we do have an intervention.

416
00:44:06,410 --> 00:44:10,670
So we have patients with some condition and they need surgery.

417
00:44:10,670 --> 00:44:15,080
So there are some standard surgery that is usually used for this condition.

418
00:44:15,770 --> 00:44:19,070
And then we have an alternative new surgical procedure proposed.

419
00:44:19,430 --> 00:44:22,730
And so we want to know, is this new procedure?

420
00:44:24,080 --> 00:44:29,180
Does this new procedure lead to better outcomes than the standard procedure so we can have our treatment?

421
00:44:29,180 --> 00:44:32,840
A high is zero if the patient got the standard procedure.

422
00:44:33,010 --> 00:44:36,080
They'll be one if they got the new procedure.

423
00:44:36,260 --> 00:44:41,870
And then our outcome wise is going to be whether or not they survive for 30 days after the procedure.

424
00:44:41,900 --> 00:44:47,340
So y equals zero. If they did survive, y equals one as they died.

425
00:44:47,380 --> 00:44:51,830
So larger values of y are bad in this model.

426
00:44:53,550 --> 00:44:57,680
Okay. So here we have a full counterfactual table for eight patients.

427
00:44:57,690 --> 00:45:00,420
So we never actually get to observe this in the real world.

428
00:45:01,020 --> 00:45:07,499
Here I have eight patients and I can see what would have happened to them if they got each of the surgeries.

429
00:45:07,500 --> 00:45:11,490
So this first this first patient would have died no matter what, right?

430
00:45:11,580 --> 00:45:16,140
If they'd gotten either surgery, they would have died the second patient.

431
00:45:17,350 --> 00:45:22,990
Would have died if they got the standard surgery, but they would have survived if they got the new surgery.

432
00:45:23,020 --> 00:45:30,440
So for this second patient, we could say. If they got the new surgery, that surgery saved their life, right?

433
00:45:30,470 --> 00:45:33,050
Or that surgery caused them to survive.

434
00:45:33,320 --> 00:45:39,980
That's what we mean by a causal effect, is that there's a difference between why of a equals zero or why of a equals one.

435
00:45:40,580 --> 00:45:43,100
But we also have this patient down here, number five.

436
00:45:43,790 --> 00:45:48,890
So number five would have survived if they got the original procedure, but they would have died if they got the new procedure.

437
00:45:49,190 --> 00:45:53,180
So for that patient, we could say, well, if they got the new procedure, that procedure.

438
00:45:54,190 --> 00:46:01,120
Killed them or did them harm. Right. So that procedure caused them to die, maybe cause number two, to survive.

439
00:46:04,960 --> 00:46:08,200
So there are two concepts of causality.

440
00:46:08,620 --> 00:46:17,109
There's more than that, but there's two on this side. So the sharp causal no says there's no effect of the treatment on any individual.

441
00:46:17,110 --> 00:46:22,330
So that means for every individual Y of equals, zero is the same as Y of a equals one.

442
00:46:23,890 --> 00:46:28,390
The average causal no says the average causal effect is equal to zero.

443
00:46:28,400 --> 00:46:33,760
So the average Y of equals zero is equal to the average Y of equals one.

444
00:46:34,210 --> 00:46:42,490
So usually we're interested in the average and all this is because usually we apply our new treatment to a population.

445
00:46:42,880 --> 00:46:46,800
Always we apply our new treatment to a population. It might be a sub population.

446
00:46:46,810 --> 00:46:55,709
You might try to say, well, this has. A better causal effect in people who have these features than people who have those features.

447
00:46:55,710 --> 00:47:01,440
I mean, apply it in these people. But as always, a population, it might just be a specific population.

448
00:47:03,030 --> 00:47:08,250
So we're always almost always interested in this average causal effect.

449
00:47:09,390 --> 00:47:15,630
So in this example, this little eight person example, the shark null is not true.

450
00:47:15,780 --> 00:47:23,790
Right? Because we have people who have differences in Y of equals zero and one of equals one, but the average no is true.

451
00:47:23,790 --> 00:47:27,909
And so the way that we see that is we just count. And the left hand column.

452
00:47:27,910 --> 00:47:30,910
What would have happened if everybody got the original surgery?

453
00:47:31,330 --> 00:47:36,520
Four people would have died. What happened if everybody got the new surgery?

454
00:47:36,820 --> 00:47:40,900
Also, four people would have died. So the average causal no is true in this.

455
00:47:42,790 --> 00:47:46,830
In this case. All right. So how do we measure treatment effects?

456
00:47:46,850 --> 00:47:50,360
We know now we have counterfactuals, we have average counterfactual.

457
00:47:50,870 --> 00:47:56,420
How do I measure a treatment effect? One way to do it is to look at the difference between the average counterfactual.

458
00:47:56,430 --> 00:48:04,760
So this is called the average treatment effect. The 80 is probably the most famous or commonly used measure of treatment effect.

459
00:48:06,330 --> 00:48:13,190
There's also a risk ratio and odds ratio. So these are our are our friends that we've seen before rates a risk ratio,

460
00:48:13,190 --> 00:48:20,210
which is the ratio of the averages odds ratio is the odds ratio of the average treatment effects.

461
00:48:20,900 --> 00:48:25,100
One thing to note, right, is that for for the 80,

462
00:48:25,100 --> 00:48:30,440
we have this nice decomposition that the difference of the averages is the average of the difference.

463
00:48:31,220 --> 00:48:34,250
This is not true for the risk ratio and the odds ratio.

464
00:48:34,490 --> 00:48:38,840
So it's good to good to keep in mind when you're interpreting your results.

465
00:48:43,510 --> 00:48:54,340
Okay. So this counterfactual framework takes our our idea of what would have happened and it turns it into a missing data problem.

466
00:48:54,530 --> 00:48:58,839
So in that table, we only ever get to observe one entry for each person.

467
00:48:58,840 --> 00:49:02,229
So no matter what, half of our data is missing.

468
00:49:02,230 --> 00:49:05,230
Right? I only get to see one of the counterfactuals for each person.

469
00:49:06,640 --> 00:49:12,310
So this is this means that there will be fundamental uncertainty in my estimate of the causal effect,

470
00:49:12,520 --> 00:49:15,579
even if I measure every single person in the population.

471
00:49:15,580 --> 00:49:18,459
So usually in statistics, right,

472
00:49:18,460 --> 00:49:26,320
if I want to estimate the average height of a population and if I measure the height of every single person in that population,

473
00:49:26,530 --> 00:49:30,670
I'm not estimating anymore. Right? I can just directly measure the average height.

474
00:49:30,940 --> 00:49:35,440
There's no uncertainty. I know what the average height of my population is for causal effects.

475
00:49:35,440 --> 00:49:41,920
That's not the case, right? If I observe every single person in my population, I observe there, I observe their Y.

476
00:49:42,190 --> 00:49:48,250
I still have to estimate a causal effect because I only saw one counterfactual for each person.

477
00:49:48,610 --> 00:49:52,180
So this is like a fundamental uncertainty in causal inference.

478
00:49:52,330 --> 00:49:56,500
It's been called the fundamental problem of causal inference that we have this missing data problem.

479
00:49:59,380 --> 00:50:04,920
Are there questions here? Okay.

480
00:50:06,510 --> 00:50:10,170
All right. Two ideas of treatment effect.

481
00:50:10,560 --> 00:50:16,830
So the simple average treatment effect. This is just the average average treatment effect in the sample is what it sounds like.

482
00:50:16,830 --> 00:50:21,360
Right? Take the white ones and average them up. Take the why of zero's and average them up again.

483
00:50:21,360 --> 00:50:28,860
We never get to observe the sample average treatment effect. We're always estimating the S.A.T. and then the population average treatment effect.

484
00:50:29,130 --> 00:50:33,540
This is sort of the same thing, but the expectation over some super population,

485
00:50:35,440 --> 00:50:40,380
almost always we want the population average treatment effect for some population, right?

486
00:50:40,800 --> 00:50:44,220
Very rarely have we measured everyone in the population.

487
00:50:44,430 --> 00:50:49,110
Occasionally we're very interested in a particular sample and we want to measure the S.A.T.

488
00:50:50,970 --> 00:50:57,209
Identifying the population is a scientific rather than a statistical task.

489
00:50:57,210 --> 00:51:01,350
Right? So identifying the population requires you to.

490
00:51:02,620 --> 00:51:04,809
Know what population you're looking at, right?

491
00:51:04,810 --> 00:51:10,150
So if I just sample let's say I sample a bunch of people who come through University of Michigan Medical Center.

492
00:51:10,720 --> 00:51:17,890
What population do I care about? Do I care about the population of patients who might come to Michigan medicine?

493
00:51:18,130 --> 00:51:23,200
Do I care about the whole population of Michigan? Do I care about the population of the United States?

494
00:51:23,200 --> 00:51:28,860
What population am I talking about? And that. Might change how I analyze my data.

495
00:51:29,070 --> 00:51:34,900
I might weight my data differently if I think I've overrepresented some group in my sample versus not.

496
00:51:37,330 --> 00:51:42,159
Okay, so causation versus association, this is, I think, a very famous concept.

497
00:51:42,160 --> 00:51:50,410
It's sort of now like almost a meme of people yelling at each other on the Internet that causation is not association.

498
00:51:50,410 --> 00:51:56,860
So this is a figure from the Herrmann and Robbins book that I think illustrates causation versus association.

499
00:51:56,860 --> 00:52:04,480
Well, so we have at the top, this is our like pie of the pie, the whole population.

500
00:52:04,720 --> 00:52:09,850
We have the untreated as the shaded on the right and the treated is the unshaded on the left.

501
00:52:10,420 --> 00:52:15,879
The association measure is just that I compare the untreated to the treated right.

502
00:52:15,880 --> 00:52:18,850
So I take that gray chunk and that white chunk.

503
00:52:19,480 --> 00:52:27,250
The causal effect is if I take what would have happened if everyone was treated versus what would have happened if no one was treated.

504
00:52:27,580 --> 00:52:32,690
That's the causal effect, the association. It's just what I get to observe.

505
00:52:34,400 --> 00:52:39,889
Your questions here. All right.

506
00:52:39,890 --> 00:52:47,360
So. Somehow my state is in trouble.

507
00:52:47,390 --> 00:52:58,800
That's okay. Well, look at the next one. So in this surgery example, so now I've generated 20 samples and for each sample I gave them an A,

508
00:52:58,820 --> 00:53:02,480
so I gave them whether they were got the original surgery or the new surgery.

509
00:53:02,690 --> 00:53:07,820
So I bolded in the table the value that we observe.

510
00:53:08,030 --> 00:53:11,450
So for this first person and I separated them by equals zero and equals one.

511
00:53:11,450 --> 00:53:16,730
So this first person received the original surgery and they survived.

512
00:53:17,000 --> 00:53:21,620
We did not get to see that if they had received the new surgery, they would have also survived.

513
00:53:23,450 --> 00:53:29,629
So if I want to compute the average causal effect, I can just use my full table.

514
00:53:29,630 --> 00:53:35,590
I have this omniscience right where I get to see in in this data, I get to see everything.

515
00:53:35,600 --> 00:53:41,360
So I can see that the average causal effect going down, the A equals one, the Y of equals one line.

516
00:53:41,570 --> 00:53:45,620
I count up how many ones there are and I get hopefully the right numbers.

517
00:53:45,620 --> 00:53:50,060
So there's one, two, three, four, five.

518
00:53:50,330 --> 00:53:54,470
So if everyone had gotten the new surgery, a quarter of the patients would have died.

519
00:53:55,280 --> 00:54:03,410
If everyone had gotten the old surgery. One, two, three, four, five, six, seven.

520
00:54:04,500 --> 00:54:10,080
Should the eight one, two, three, four, five, six, seven, eight.

521
00:54:10,350 --> 00:54:15,330
Yeah. So under the old surgery, eight out of 20 or 40% of the patients would have died.

522
00:54:15,600 --> 00:54:19,229
So my average causal effect is -0.15.

523
00:54:19,230 --> 00:54:23,490
Right? So the new surgery is saving lives in this scenario.

524
00:54:24,620 --> 00:54:26,180
If I look at the association,

525
00:54:26,510 --> 00:54:33,170
this is where I just calculate what is the average survival in the treated group versus the average survival in the control group.

526
00:54:33,590 --> 00:54:37,310
So in the treated group, this one to.

527
00:54:38,630 --> 00:54:43,780
Three four out of my ten. Four out of ten people died.

528
00:54:43,970 --> 00:54:51,820
Right. So the I guess death rate here is 40% in the control group when two people died.

529
00:54:52,120 --> 00:54:55,240
So my estimation, my association here is point two.

530
00:54:55,630 --> 00:55:02,800
So what I observe is that more people in the treated group died than in the control group.

531
00:55:03,280 --> 00:55:09,579
So I might just say, well, it seems like the surgery is bad then, but if I'd been able to see all of these counterfactuals,

532
00:55:09,580 --> 00:55:15,970
I would know that actually using, if I had used that new surgery for everyone, I would have saved.

533
00:55:18,520 --> 00:55:21,520
Eight minus five. Three lives, right?

534
00:55:26,960 --> 00:55:31,940
Okay. So how do I know if I can identify a causal effect for my data?

535
00:55:34,820 --> 00:55:38,380
So the first thing to know is what does identification mean?

536
00:55:38,390 --> 00:55:42,920
So this is this something that came up in six or something?

537
00:55:43,070 --> 00:55:49,000
Three guys. Have you seen the idea of identification before? Maybe.

538
00:55:49,750 --> 00:55:52,570
Maybe it seems sort of familiar. Okay. So we'll we'll go through it.

539
00:55:52,580 --> 00:55:58,390
So a parameter is identifiable if it is theoretically estimable with a large enough sample.

540
00:55:59,020 --> 00:56:06,580
So formally if let's say curly P is the set of distributions that's parameterized by a parameter theta.

541
00:56:06,910 --> 00:56:12,370
So theta is identifiable if unique values of theta correspond to unique distributions.

542
00:56:12,430 --> 00:56:19,750
Sorry about the typo. So that means that p theta does not equal p theta prime if theta does not equal theta prime.

543
00:56:19,780 --> 00:56:24,630
So basically we have a unique mapping of theta two distributions.

544
00:56:25,090 --> 00:56:29,200
If two different values of theta, give me the same probability distribution.

545
00:56:29,440 --> 00:56:35,020
I have no hope of ever telling theta from data prime because I only get to see the probability distribution.

546
00:56:35,050 --> 00:56:43,710
Right. So identify ability is a property of a model and not a particular estimation procedure or a data set.

547
00:56:44,800 --> 00:56:48,550
It might be a property of the kind of data that you're going to observe.

548
00:56:48,550 --> 00:56:52,030
Right? If you say I only observe. And why is this identifiable?

549
00:56:52,240 --> 00:56:58,960
That's different from saying in this specific data set where I've observed and why is data identifiable?

550
00:56:59,500 --> 00:57:05,140
And so knowing that data is theoretically identifiable doesn't mean that we can estimate it well in the data.

551
00:57:05,380 --> 00:57:13,690
It might be identifiable, but when I estimate it, the confidence intervals are so huge that I don't get much out of my estimate.

552
00:57:14,590 --> 00:57:19,690
And it also doesn't mean that I have I know what estimation procedure will give me an estimate.

553
00:57:20,980 --> 00:57:24,370
So just like a quick example is coloniality.

554
00:57:24,370 --> 00:57:36,040
Right? So in this linear model, I have a variable Y in two variables x1x2 and I have this linear model Y is beta zero plus beta 1x1 plus beta 2x2.

555
00:57:37,480 --> 00:57:40,390
And I also have this property of the x one an x two right are identical.

556
00:57:40,840 --> 00:57:46,030
So in this model I think we know intuitively I have no hope of estimating beta one and beta two

557
00:57:46,330 --> 00:57:51,220
because there's lots of combinations of beta one and beta two that would be consistent with my data.

558
00:57:51,490 --> 00:57:56,620
In order for me to estimate beta one and two, I need x one and x two to be different.

559
00:57:56,630 --> 00:58:00,940
Right? I can't have perfect colony parity and still get that estimate out.

560
00:58:02,370 --> 00:58:07,969
That makes sense to folks. Can not.

561
00:58:07,970 --> 00:58:12,110
If it does or if it doesn't, you could ask the question.

562
00:58:15,290 --> 00:58:24,320
Okay. All right. So how do we know if the 18 is identifiable or if a wide array is identifiable?

563
00:58:24,500 --> 00:58:28,340
This is like. A big crux of causal inference, right?

564
00:58:28,340 --> 00:58:32,660
When is is why are they identifiable? So.

565
00:58:34,710 --> 00:58:38,280
Right. All right. So conditions. So let's say we only observe.

566
00:58:38,280 --> 00:58:42,770
And why? When can we identify if we're a.

567
00:58:44,150 --> 00:58:53,570
So if I've only observed A and Y, I have to be able to get e y away from something about the joint distribution of a and y.

568
00:58:53,840 --> 00:58:58,940
So this means that if Y of A has to be equal to E of y given a right,

569
00:58:58,940 --> 00:59:03,290
if the causal effect and the association are different and I don't have any more data.

570
00:59:04,700 --> 00:59:08,670
Then I'm out of luck. So when does this condition hold?

571
00:59:08,680 --> 00:59:13,899
We need three. Conditions that we're going to come back to these like multiple times.

572
00:59:13,900 --> 00:59:22,510
So these are important. So one of them is consistency. One of them is the poorly named stable unit treatment value assumption or setback.

573
00:59:23,710 --> 00:59:30,430
And then one of them is called exchange ability, which if you are an economist, you might have seen called exaggerated.

574
00:59:33,190 --> 00:59:36,280
All right. So we'll go through each of these conditions.

575
00:59:36,280 --> 00:59:39,790
So consistency. How are we on time? We're doing good. Okay.

576
00:59:40,210 --> 00:59:51,280
So consistency. Consistency says that the observed value of Y is the same as the counterfactual value of Y under the treatment I so formally.

577
00:59:52,580 --> 01:00:01,460
We can say y i equals y of air equals little air where air is the treatment that somebody actually observed.

578
01:00:01,700 --> 01:00:10,069
So this condition is saying in that table where I had one of the one of the rows bolded it, saying, I did get to observe one of those.

579
01:00:10,070 --> 01:00:14,570
Right? The thing I observed wasn't different from one of those one of those values.

580
01:00:14,990 --> 01:00:25,430
And the other way to think about this is to say that intervening to set air equals to a is not different than just it happening.

581
01:00:25,430 --> 01:00:33,790
That air equals a. So in the surgery example, I think consistency sort of seems.

582
01:00:35,020 --> 01:00:42,879
Likely or obvious, right, that like a person's chance of surviving if they got the original surgery is the same

583
01:00:42,880 --> 01:00:47,800
as the person's chance of surviving would have been if an intervention occurred,

584
01:00:47,800 --> 01:00:52,330
causing them to get the original surgery right or forcing them to get the original surgery right.

585
01:00:52,330 --> 01:00:57,130
That the action of the intervention has not changed anything about their outcome.

586
01:01:00,230 --> 01:01:05,930
Whether or not consistency holds might depend on the nature of the hypothesized intervention.

587
01:01:06,770 --> 01:01:10,460
So we have. One.

588
01:01:11,670 --> 01:01:15,800
Sort of common example of this is social interventions, right?

589
01:01:15,810 --> 01:01:21,890
If I intervene and I, I require people to wear their seatbelts.

590
01:01:22,580 --> 01:01:26,569
Does that somehow do something different than if they wear their seatbelt on their own?

591
01:01:26,570 --> 01:01:32,899
Or if I require if I require that you go for a walk every day, does that have a different effect,

592
01:01:32,900 --> 01:01:35,959
let's say, on your mental health than if you go for a walk of your own accord?

593
01:01:35,960 --> 01:01:43,130
Right. It may. Maybe the act of choosing to go for the walk is what helped your mental health and not being intervened on and going for the walk.

594
01:01:45,860 --> 01:01:50,450
All right. Stable unit treatment value assumption. So there's two parts to that.

595
01:01:50,780 --> 01:01:55,520
One of them says there's no different versions of treatment available to a single individual.

596
01:01:56,240 --> 01:02:01,940
So if somebody gets treatment little A, there's only one option for that.

597
01:02:01,940 --> 01:02:12,040
I think the example of this is like, let's say you have treatment of take Tylenol versus not take Tylenol in the take Tylenol arm.

598
01:02:12,490 --> 01:02:15,979
But you could take one Tylenol, you can take two Tylenol or you can take three Tylenol, right?

599
01:02:15,980 --> 01:02:22,010
So for each person, they need to have only one available version of that treatment.

600
01:02:22,190 --> 01:02:25,610
We don't require that each person has the same available version.

601
01:02:25,610 --> 01:02:32,750
Right. So it might be that. If Dan is assigned to take Tylenol, Dan can only take two.

602
01:02:33,110 --> 01:02:38,060
And if that lithium. Yeah, everything takes Tylenol.

603
01:02:38,390 --> 01:02:41,840
She can only take one, right? That's okay. We haven't violated that.

604
01:02:42,110 --> 01:02:47,360
But if Lillian can choose to take one or two. Now we have a problem with different versions of treatment.

605
01:02:48,860 --> 01:02:52,459
And then the second part of setback is no interference.

606
01:02:52,460 --> 01:03:00,620
So interference means that the assigned treatment assignments that other units got influences your outcome.

607
01:03:02,720 --> 01:03:06,950
So this as there's like a formal definition of interference,

608
01:03:06,950 --> 01:03:11,389
write interference says if I look at the whole vector of treatments that everyone in the study

609
01:03:11,390 --> 01:03:18,920
got and I look at now one individual's counterfactual given that entire vector of treatments.

610
01:03:19,340 --> 01:03:25,070
All I needed to know was actually what treatment they got, right? So the things their friends are doing doesn't affect their outcome.

611
01:03:25,220 --> 01:03:30,680
The things the people around them are doing doesn't affect the outcome. Just the treatment that they got affects their outcome.

612
01:03:31,970 --> 01:03:36,980
Okay. So we're going to take a little break and do a paired discussion.

613
01:03:36,980 --> 01:03:45,890
So talk with the person sitting next to you. Ideally, if there's maybe Andy and Chris can talk together or you can turn around,

614
01:03:46,070 --> 01:03:55,070
I think there will be one group of three, it looks like, which is fine, so Yazan can pick which side to go to.

615
01:03:55,850 --> 01:04:01,390
So there's two questions. So one is, can you think of a time that no different versions of treatment might not hold?

616
01:04:01,400 --> 01:04:07,940
Don't use the Tylenol example because I just use that one. And then can you think of a time that no interference might not hold?

617
01:04:08,180 --> 01:04:11,840
And we'll talk for like for like four or 5 minutes, something like that.

618
01:04:13,390 --> 01:04:26,500
And then we'll come back and we'll talk about our interest and and you can intervene to reduce causing others.

619
01:04:27,140 --> 01:04:32,750
So I think the second example, like you say you can,

620
01:04:32,750 --> 01:04:41,630
where if that wasn't the type of thing about like when you're in that kind of some surveillance might be better than a choice.

621
01:04:42,480 --> 01:05:00,860
So again, you're, you know each other and things like things.

622
01:05:01,230 --> 01:05:08,050
So it's basically saying let's just like, okay, right.

623
01:05:09,110 --> 01:05:20,670
Like they're not to on and don't think so.

624
01:05:20,690 --> 01:05:33,590
You have to understand, I want you to make clear of prescribed medication, you know,

625
01:05:35,060 --> 01:05:47,910
like that again, because we're not going to go to the hospital, so why not the other two?

626
01:05:47,960 --> 01:05:52,670
But you had to give him some influence on prevention.

627
01:05:52,910 --> 01:05:56,090
And I'm not sure. I mean, I.

628
01:05:56,770 --> 01:06:06,890
I'm not sure there's no reason, you know, what difference does it make?

629
01:06:08,750 --> 01:06:12,270
Any conclusion? Is that right?

630
01:06:12,420 --> 01:06:24,320
Versus ideally, I would consider the same no matter how many times that he was like,

631
01:06:24,530 --> 01:06:33,460
I can't start smoking because like smoking, you know what it was, you know, that's not okay.

632
01:06:33,680 --> 01:06:48,940
I don't know how to you know, I think that's an excuse for my friends.

633
01:06:48,980 --> 01:06:59,120
I actually think that that would come into view on whether it was driven by anything and sex.

634
01:06:59,120 --> 01:07:10,940
And I mean, I could see that as I think what you're saying is more of an inquiry.

635
01:07:10,990 --> 01:07:16,809
But I would say that, you know what?

636
01:07:16,810 --> 01:07:36,720
I would say that this is really the key thing I just wanted to write about.

637
01:07:38,420 --> 01:07:47,720
And then, you know, how, you know, it wasn't always that way is.

638
01:07:50,100 --> 01:08:08,580
It was only three years ago when I was in my life and I was like, What is going on here?

639
01:08:09,120 --> 01:08:33,800
You don't have to identify with those areas or you can read about it in the way you see that human being is or is not an area of life in prison.

640
01:08:35,010 --> 01:08:52,170
I didn't know what I was doing and I didn't say anything like that.

641
01:08:53,400 --> 01:09:02,900
But what you will hear is that you find a way to motivate me.

642
01:09:06,630 --> 01:09:20,200
So I guess my point was, you know, you get a random feel like you've gotten through those questions or you want.

643
01:09:20,220 --> 01:09:25,260
I have a couple more bands. For example, you like one more minute.

644
01:09:28,620 --> 01:09:36,659
Yeah, I heard that. He also says that hear that?

645
01:09:36,660 --> 01:09:49,140
I was going to try and make some of them for any cavalry regiment.

646
01:09:49,950 --> 01:10:07,620
I can't believe I have ever been inspired by, again, everything that happened in my life.

647
01:10:11,160 --> 01:10:20,850
Right. Seems like most people have sort of wrapped up. So let's let's come back together and maybe we'll just pop around and hear a couple ideas.

648
01:10:21,480 --> 01:10:26,010
Does anyone have. Just to start, does anyone have ideas that they are interested to share?

649
01:10:26,040 --> 01:10:30,240
They want to share will encourage you to want to share.

650
01:10:30,270 --> 01:10:35,190
It's a safe place to be wrong or to enjoy being right.

651
01:10:35,940 --> 01:10:43,709
Dan So we were discussing how like no different versions of treatment may not hold like a seatbelt

652
01:10:43,710 --> 01:10:50,750
example or any safety implement very might not all be made equally or even like a helmet on a bike.

653
01:10:50,760 --> 01:10:59,040
They're not all the same. So just the fact you have a seatbelt doesn't imply you got the same treatment as other people who got the seatbelt.

654
01:10:59,220 --> 01:11:02,300
Yeah, I think I think that's on the right track.

655
01:11:02,310 --> 01:11:06,260
It may be I think that might fall into this category of.

656
01:11:07,810 --> 01:11:12,890
People might have different treatments available to them, but each person might only have one.

657
01:11:12,910 --> 01:11:17,710
At least if everyone only has one car. They can't swap out the seatbelt.

658
01:11:17,830 --> 01:11:21,010
So that might be this case. Where.

659
01:11:22,050 --> 01:11:27,060
The treatment looks different for different people, but you can only have it or not have it.

660
01:11:27,750 --> 01:11:33,950
Is there another one? I.

661
01:11:35,190 --> 01:11:38,540
About you. Your group, too.

662
01:11:39,270 --> 01:11:49,980
Do you have an idea for this one? Yeah, I thinking about the enterprise case, but we are not sure whether it is suitable here.

663
01:11:50,220 --> 01:11:53,549
Yeah. Good. Let's hear the no interference thing about this.

664
01:11:53,550 --> 01:12:07,260
The civil case. If the taxi driver is wearing a seatbelt, whether or not you are like, I could lower the risk of the other passengers in the car.

665
01:12:07,740 --> 01:12:11,760
Whether or not they choose to wear well, it sounds like,

666
01:12:12,120 --> 01:12:16,409
is the idea that maybe the driver will drive differently if they're if they're wearing a seatbelt,

667
01:12:16,410 --> 01:12:19,720
regardless of whether everyone else in the car is wearing a seatbelt. Right.

668
01:12:19,740 --> 01:12:23,170
Or. Yeah, I think I think that makes sense.

669
01:12:23,180 --> 01:12:27,590
I think that's a good one. Are there other ones for either either question?

670
01:12:27,980 --> 01:12:33,049
Um, yeah. So we we are thinking about no interference.

671
01:12:33,050 --> 01:12:39,860
A good exam. That'd be long. And the only time we get the longest killed.

672
01:12:40,070 --> 01:12:47,870
Really? Mm hmm. And then help ability improve and it have spillover effect on a nearby.

673
01:12:48,830 --> 01:13:00,350
Yeah. So I think the like communicable disease rate generally like vaccines and communicable diseases like the big example of no interference.

674
01:13:00,350 --> 01:13:05,149
Right. If if I don't get the flu shot but all of you got the flu shot.

675
01:13:05,150 --> 01:13:11,120
I'm protected. So without that, your treatment right.

676
01:13:11,130 --> 01:13:20,730
Can affect my my outcome. Chris, I had a question about a possible example for the different versions.

677
01:13:21,180 --> 01:13:29,190
So if you were doing like a cancer or cancer looking at cancer patients across different

678
01:13:29,820 --> 01:13:37,360
locations and each location has like one or two or three different levels of radiation exposure,

679
01:13:37,380 --> 01:13:40,440
would that be an example of the different version?

680
01:13:40,560 --> 01:13:47,880
Right. So if you if you assign, let's say, radiation, no radiation, but within radiation,

681
01:13:48,090 --> 01:13:55,260
a person could get low radiation or high radiation depending on what their doctor says or what they pick.

682
01:13:56,400 --> 01:14:00,389
Right. Then that would be a violation of no different versions of treatment.

683
01:14:00,390 --> 01:14:07,120
Right. So I can often you can get around no different versions of treatment by being more specific in what you say.

684
01:14:07,140 --> 01:14:15,209
Right. You can say I'm going to assign you to a specific level of radiation or even.

685
01:14:15,210 --> 01:14:21,240
Right, even that specific level could be determined based on some function of that person's cancer.

686
01:14:21,240 --> 01:14:25,890
Right. That would be okay as long as they have a specific treatment that they're getting.

687
01:14:28,050 --> 01:14:32,910
Any other. Yeah. So for the first question and nothing makes sense.

688
01:14:33,660 --> 01:14:36,010
I mean, it would make sense actually.

689
01:14:36,030 --> 01:14:45,410
And you realize that always we have a lot of people who are in some treatments, but just to make our job easier, we say some difference is negligible.

690
01:14:45,420 --> 01:14:51,580
Are we seeing there they wouldn't make a difference to make a difference to any kind of sexual car.

691
01:14:51,610 --> 01:14:55,290
So we're saying that to people, take two of them pills.

692
01:14:55,290 --> 01:14:59,640
But actually there might be very small difference between your willingness to refuse.

693
01:14:59,850 --> 01:15:09,150
Right? I think very practically so in we're currently still living in this like theoretical world where there are specific levels of a right.

694
01:15:09,330 --> 01:15:16,799
But you're right that like if I take two Tylenol pills out of the bottle, they might differ in weight by some like Nandigram or something.

695
01:15:16,800 --> 01:15:24,180
Right. And we'll say like at some point there's some non perceptual perceptible differences in like,

696
01:15:24,600 --> 01:15:27,660
you know, numbers of molecules that I'm going to write off.

697
01:15:28,440 --> 01:15:33,059
But yeah, in this in this theoretical world, we want everything to be the same.

698
01:15:33,060 --> 01:15:44,110
In the real world, we do our best rate. And I think the other part of that is that when the when the differences are non perceptible.

699
01:15:45,900 --> 01:15:47,760
That kind of gets us back to this.

700
01:15:48,450 --> 01:15:55,680
Not everyone's getting exactly the same treatment, but we're not able to choose between perceptibly different treatments right than I might.

701
01:15:56,280 --> 01:16:02,760
Part of my difference might be that I happenstance only got the Tylenol pill that has four more nanograms in the Tylenol pill that somebody else got.

702
01:16:04,260 --> 01:16:11,399
So that's kind of a subtle difference. This like does a single unit have choices or is there differences between units?

703
01:16:11,400 --> 01:16:15,690
So we can have differences in the nature of the treatment that each unit is getting.

704
01:16:15,690 --> 01:16:18,840
Right. But we can't have choices for each unit.

705
01:16:21,970 --> 01:16:27,570
So I think another example is. Let's say smoking, right?

706
01:16:27,580 --> 01:16:32,120
If I assign you to reduce your smoking level. You might.

707
01:16:33,690 --> 01:16:39,710
You know, reduce down to one a day or someone else might reduce by almost nothing.

708
01:16:39,720 --> 01:16:42,660
Right. And they say they both say while I reduce smoking level.

709
01:16:45,380 --> 01:16:50,630
If I assign everyone to quit, they're all kind of getting different treatments because they started at different levels, right?

710
01:16:50,690 --> 01:16:53,930
One person is smoking a pack a day, one person smoking half a pack a day.

711
01:16:54,320 --> 01:16:59,020
They got different treatments, but they all had one choice for how to follow that.

712
01:16:59,570 --> 01:17:05,020
The assignment, which was quit smoking. All right.

713
01:17:05,020 --> 01:17:11,260
So we had that was we've been through consistency and setback.

714
01:17:11,980 --> 01:17:15,640
And now we're going to do our third assumption, which is exchange ability.

715
01:17:16,090 --> 01:17:21,790
So exchange ability says that their value y array is independent of the treatment that you actually got.

716
01:17:21,820 --> 01:17:26,410
So Y of a is independent of AA. So this is an important difference.

717
01:17:26,420 --> 01:17:32,730
How is the statement Y of a independent of a different from the statement y is independent of a.

718
01:17:33,820 --> 01:17:43,420
What makes those different? So first of all, it's kind of back to Ofcom, but second wise there was a different outcome.

719
01:17:43,430 --> 01:17:49,340
So yes, you're right, the second one doesn't hold while we can make the confession economy dependent on state treatment.

720
01:17:49,610 --> 01:17:53,689
Yes, exactly right. So if it does cause Y, I would generally expect that.

721
01:17:53,690 --> 01:17:55,010
Why is not independent of it.

722
01:17:55,670 --> 01:18:03,180
But if let's say I've done a randomization, everyone gets a random treatment, I might be able to argue that Y of A is independent of eight.

723
01:18:04,850 --> 01:18:08,899
So there's a couple different variations of this exchange ability idea.

724
01:18:08,900 --> 01:18:18,440
So mean exchange ability says e of y of a given a equals a prime is equal to e of y of a given a equals a prime prime.

725
01:18:18,590 --> 01:18:18,889
Right.

726
01:18:18,890 --> 01:18:30,920
So people who got two different values of treatment have the same average of e, of y of a where a might be different from a prime or a prime prime.

727
01:18:32,630 --> 01:18:37,910
So mean exchange ability is sufficient for us to get our identification result, which is where we were going.

728
01:18:37,910 --> 01:18:45,629
Right. For a dichotomous outcome mean exchange ability and exchange ability.

729
01:18:45,630 --> 01:18:49,830
Generally, this independent statement are the same.

730
01:18:51,090 --> 01:18:54,570
What about nine dichotomies? Why are they the same or not the same?

731
01:19:12,960 --> 01:19:17,680
Raise your hand if you think those same. Raise your hand if you think they're not the same.

732
01:19:20,780 --> 01:19:25,160
Okay. We got about half the class is not the same and half the class doesn't want to vote.

733
01:19:26,420 --> 01:19:34,970
Okay, so they're not the same. And the reason they're not the same is because we can have non-independent on something other than the mean.

734
01:19:35,000 --> 01:19:42,920
Right. You can have the variance depends on a or this is sort of I think they're like the toy example as you plot like a circle, right?

735
01:19:43,010 --> 01:19:49,639
Two dots around a circle and you say, well, why an x or uncorrelated but y an x are not independent, right?

736
01:19:49,640 --> 01:19:55,300
Given your x, I know pretty close to what your y is. Okay.

737
01:19:55,640 --> 01:19:58,820
So does exchange ability hold in this.

