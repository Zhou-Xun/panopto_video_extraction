1
00:02:23,880 --> 00:02:31,130
G Yeah, if you make. Very healthy drink.

2
00:03:12,580 --> 00:03:17,320
It's one club. Uh, just come back from Pittsburgh.

3
00:03:17,530 --> 00:03:21,880
This is a University of Pittsburgh Department statistics that give the seminaries to.

4
00:03:21,890 --> 00:03:27,310
There tends to be the tennis hall to be the first in-person seminary in the past three years.

5
00:03:27,640 --> 00:03:31,810
But they had those virtual sort of seminars in the past couple of years.

6
00:03:31,810 --> 00:03:36,250
But I was the first one yesterday.

7
00:03:36,370 --> 00:03:39,579
That in-person seminar was quite interesting.

8
00:03:39,580 --> 00:03:49,330
We have at the BI seminar several Michigan alumni working there from Boston's universities came over to my talk,

9
00:03:50,830 --> 00:03:55,500
but there's no faculty member in that department that was mentioned by the chair of the department.

10
00:03:55,520 --> 00:04:01,239
And do you have any students who are graduating this year or would they have two positions?

11
00:04:01,240 --> 00:04:08,040
And anyway, I just say that we haven't had any graduates working that diploma statistic yet.

12
00:04:08,050 --> 00:04:21,100
So it's very interesting, sort of opposite, but quite nice to see that we sort of back to normalcy and talented people.

13
00:04:21,130 --> 00:04:33,010
And yeah, there were a lot of intriguing sort of conversations and the one faculty member who worked with we won back

14
00:04:33,010 --> 00:04:43,209
to Standard and I had quite a bit of discussion about instrumental variable and Markov methods and so on.

15
00:04:43,210 --> 00:04:52,360
So yeah, I thought a conversation would come up a couple of ideas how we can do this sort of confounding control use, you know,

16
00:04:52,360 --> 00:05:02,709
this sort of hyper racing hybrid fashion of knockoff and and then in randomization

17
00:05:02,710 --> 00:05:08,710
were instrumental variable and based on this generative model in machine learning,

18
00:05:08,710 --> 00:05:09,940
I mean deep learning.

19
00:05:09,940 --> 00:05:20,970
AI So there was quite a bit, you know what I'm saying is that really having this in-person conversation is very, very intriguing.

20
00:05:20,980 --> 00:05:25,000
And so I enjoyed those interaction accounts of these conversations.

21
00:05:25,000 --> 00:05:35,020
And for certainly, general, a lot of your ways of thinking with different methods that are developed to, to control confounding factors,

22
00:05:35,020 --> 00:05:37,510
which is a big deal in observational study,

23
00:05:38,530 --> 00:05:46,050
which is also really the course because all the data we have here are surveillance data, basically observational data, right?

24
00:05:46,110 --> 00:05:55,150
There's no design whatsoever unless you talk about the the vaccine trial data, which we don't have it.

25
00:05:55,690 --> 00:06:05,430
But anyway, so of one thing I should mention that on my flight to Pittsburgh, I was thinking that this arrow may not be really a valid one.

26
00:06:05,440 --> 00:06:14,440
So someone developed the antibody and this person should back to the susceptible compartment and rather than being direct,

27
00:06:14,440 --> 00:06:18,520
could in fact move into infectious compartment.

28
00:06:18,820 --> 00:06:23,140
This person who developed the antibody should go through this procedure again.

29
00:06:23,170 --> 00:06:30,670
Right. So so, you know, this person can get the second shot or the third shot or even booster shot or something like that.

30
00:06:30,670 --> 00:06:35,829
So, so, so I was thinking that I should cross this earlier, revise this.

31
00:06:35,830 --> 00:06:47,170
I think this is a little bit too much. I think someone had antibody developed either from vaccine or from this recovery.

32
00:06:47,170 --> 00:06:53,979
And we'll put you back to be a susceptible and goes through recycling and wrote adaptive directed factor so

33
00:06:53,980 --> 00:07:04,000
this arrow that's no wonder I do not put any letter out of absolutely this I will revised this graphic so.

34
00:07:13,400 --> 00:07:18,750
Rick Perry. Is there a re from h o res?

35
00:07:18,800 --> 00:07:22,670
That should be our gamma ray.

36
00:07:23,870 --> 00:07:27,320
This is gamma r. R is used in the item r.

37
00:07:30,250 --> 00:07:34,750
Oh. Call my H.R.

38
00:07:41,300 --> 00:07:45,740
Ahmad Plus. But let's see. What do you want to name is?

39
00:07:46,040 --> 00:07:52,850
I think should be golf. I.

40
00:07:56,080 --> 00:08:00,670
Or one. There's a comma or two because they're all coming to.

41
00:08:01,830 --> 00:08:08,370
The recovery compartment was through different parts, this through a hospital, this directly just at home.

42
00:08:08,370 --> 00:08:14,400
In fact, it did recover and hold without going through the compartment of hospital.

43
00:08:16,970 --> 00:08:22,970
Okay. Like Gamma R1 Gamma. This is our one, this star two.

44
00:08:23,270 --> 00:08:37,630
Let's put down and delete this one. Let's formally talk about the.

45
00:08:41,280 --> 00:08:45,890
Okay. So there's this big deal in in fashion this morning.

46
00:08:45,970 --> 00:08:58,250
All right. So. Maybe I should open using.

47
00:09:09,040 --> 00:09:34,760
It's the right thing. You can sign with your Google account, actually.

48
00:09:35,800 --> 00:09:40,000
University. The Adobe software. Uh huh. Yeah.

49
00:09:40,180 --> 00:09:49,200
Nasty saying, just saying. No, I guess. Hoping that something open with the.

50
00:09:53,930 --> 00:09:58,160
Yeah. Also, which one built this one? Was this one?

51
00:09:58,490 --> 00:10:04,600
And then this. Oh, this one.

52
00:10:05,050 --> 00:10:08,520
I don't have anything to do.

53
00:10:09,370 --> 00:10:19,090
Okay, then open with some of we need to eat. Good. The material doesn't work.

54
00:10:19,120 --> 00:10:23,950
Let's. Let's just do this. Really?

55
00:10:25,370 --> 00:10:29,030
Okay. Oh, you missed some information. You can ask your classmate.

56
00:10:29,060 --> 00:10:32,900
Okay. Okay. So. Okay, let me just pause.

57
00:10:34,190 --> 00:10:47,809
So I'll formally talk about the prediction. So this is the best unbiased predictor as being a very important topic in sort of classical statistics,

58
00:10:47,810 --> 00:10:58,070
where people develop a lot of basic sort of the methodologies and theory around this block.

59
00:10:58,400 --> 00:11:09,320
Okay. But nowadays, because of we have the same scene, we have more powerful computing facility and this method becomes a little bit out of date,

60
00:11:09,320 --> 00:11:25,970
but it gives you a very sensual sort of framework of action and the way of, of, uh, way of presenting the sort of the prediction.

61
00:11:26,010 --> 00:11:33,469
So, so, so it's basically we do through this block, we can understand how prediction is formulated,

62
00:11:33,470 --> 00:11:38,900
how this is actually developed in both method and theory.

63
00:11:38,940 --> 00:11:40,969
Okay. So that's something I want to talk about.

64
00:11:40,970 --> 00:11:47,810
Just give you a little bit of flavor of what people are talking about in general about optimal prediction.

65
00:11:48,140 --> 00:11:58,070
Okay. This is very simple. One that being used widely in engineering and other field, medical field.

66
00:11:58,430 --> 00:12:10,670
When people want to talk about prediction. So, so I want to give an introduction and later I will talk about Common Future, a snoozer.

67
00:12:11,480 --> 00:12:17,630
Those are the essential sort of predictors that we need to solve this space based model.

68
00:12:17,690 --> 00:12:20,780
I will give you an introduction. What is this space model is Bob.

69
00:12:21,290 --> 00:12:29,209
Then we will talk about the help Bob is used to to create a future oncoming

70
00:12:29,210 --> 00:12:36,830
schmoozer to deal with the temples of dynamics in the fiction part of the process.

71
00:12:36,950 --> 00:12:48,290
Okay. So of course, that this is a sort of type of prediction in the the frequentist perspective.

72
00:12:48,950 --> 00:12:57,140
So I did my talk yesterday and the universe people are somewhat asked me because I talk about this hyperdimensional inference.

73
00:12:57,770 --> 00:13:04,790
And then one question from audience was why not do start this inference using base basin method?

74
00:13:04,820 --> 00:13:08,480
Why do you need to do the biasing and go through this sort of.

75
00:13:10,130 --> 00:13:13,700
So from could this the way of high definition inference.

76
00:13:14,240 --> 00:13:19,370
So when we talk about from quantum perspective the first thing used probably could

77
00:13:21,020 --> 00:13:27,240
argue for the advantage of this frequentist perspective is statistical estimation.

78
00:13:27,260 --> 00:13:35,060
Word prediction is it's really about bias in Bayesian statistics.

79
00:13:35,660 --> 00:13:46,460
Okay. Bias is the law because prior you since you're introduced some shrinkage and so that the base estimates in general are biased estimate.

80
00:13:46,910 --> 00:13:55,040
But your friend you from from quantum point of view right when you talk about frequentist statistic or broken school frequentist argument,

81
00:13:55,430 --> 00:14:01,490
we always think about unbiased like you learn six so to right so this you have video.

82
00:14:02,360 --> 00:14:07,489
The first thing you need to work out is how do we create own biases?

83
00:14:07,490 --> 00:14:16,580
The matter that you see among all the unbiased estimate are what is the the one with the smallest variance or most efficient estimate, the result.

84
00:14:16,900 --> 00:14:19,940
So that's very sort of a layman.

85
00:14:19,940 --> 00:14:33,140
Pearson type of thinking of frequentist statistic where the bias can be a priority to somehow you know of.

86
00:14:35,140 --> 00:14:39,860
That's the basis for for the of the property of the massive growth.

87
00:14:40,210 --> 00:14:43,660
So this affects this thinking of prediction as well.

88
00:14:43,770 --> 00:14:50,830
And when we think about the frequentist perspective, about prediction, we're always thinking about making an unbiased predict.

89
00:14:50,950 --> 00:14:54,070
Right. So people are always going to use base method for prediction.

90
00:14:54,100 --> 00:15:02,140
That's fine. You can do that. But that prediction may not be maybe biased in the sort of perspective.

91
00:15:02,770 --> 00:15:08,130
I mean, I from I always argue that biased bias is not a bad thing.

92
00:15:08,140 --> 00:15:16,960
It's. But, I mean, so you always think that you can sacrifice some kind of bias in order to gain your efficiency.

93
00:15:17,200 --> 00:15:21,549
Okay. You have the. So the bias variance treat that always.

94
00:15:21,550 --> 00:15:26,200
If you want to reduce the bias, then you of course, you have to increase the variance.

95
00:15:26,200 --> 00:15:30,249
If you sacrifice the bias a little bit, you can improve your variance.

96
00:15:30,250 --> 00:15:35,860
There's always a by experience sort of a tradeoff.

97
00:15:36,490 --> 00:15:44,560
So that's why in general, people want to work more square error rather than working on bias or variance separately.

98
00:15:44,860 --> 00:15:48,459
But we're talking about very, very firm, this type of thinking.

99
00:15:48,460 --> 00:15:56,680
So or school of statistics where we think that bias is something we'd like to control first.

100
00:15:56,920 --> 00:16:02,620
Okay. Ideally, we want to make something on bias level prediction.

101
00:16:03,370 --> 00:16:11,170
So if a block like best and neither unbiased condition or predictor does not require any distribution assumption.

102
00:16:11,230 --> 00:16:17,530
Of course, if you to some say my mouth trembled collar you have to have a fully premature mode in order to

103
00:16:17,530 --> 00:16:23,220
do sampling because you need to sample from posterior distribution to make a prediction there.

104
00:16:23,230 --> 00:16:31,600
You have to have some kind of distribution in the in the algorithm in order to sample random samples from there.

105
00:16:31,780 --> 00:16:37,510
But of course, this is the old fashion of deep learning, right?

106
00:16:37,810 --> 00:16:49,480
We have deep learning worlds. What the those computer scientists are doing nowadays as really just simulate draw samples with no distribution.

107
00:16:49,840 --> 00:16:52,720
Okay. So that's as I say,

108
00:16:53,590 --> 00:17:05,280
I can't say is a big threat to the systems in massive in MSM say we always think about prior and posterior or then we sample from posterior.

109
00:17:05,290 --> 00:17:11,470
But now those computer scientist start to says sample data result using distributions.

110
00:17:11,980 --> 00:17:19,870
So that's a a big sort of punch to the of the current sort of the same important method.

111
00:17:19,870 --> 00:17:23,950
We are very proud of that. Basingstoke guys are very proud of it.

112
00:17:23,980 --> 00:17:28,150
So, so, so I think that in the next ten years, okay,

113
00:17:29,710 --> 00:17:38,250
the Muslims may well disappear because everybody will use deep learning generated models to to sample rather than using the posterior to sample.

114
00:17:38,260 --> 00:17:48,040
Because how do you know that if this model specification is correct, people always feel that the less assumptions,

115
00:17:48,040 --> 00:17:56,080
the more flexible right to to actually do the detail on my work to do they are machine learning

116
00:17:56,260 --> 00:18:04,240
and now in deploying people to develop the method to simulate data without showing distributions.

117
00:18:04,280 --> 00:18:10,749
So, so this is sort of like binary, but here we consider a much simpler setting.

118
00:18:10,750 --> 00:18:16,450
We're not sampling data. We're working on moments of the data distributions.

119
00:18:16,540 --> 00:18:19,659
Okay? We do not assume a specific distribution.

120
00:18:19,660 --> 00:18:25,300
We think that the distribution exist, but we do not want to fully specify distribution.

121
00:18:25,600 --> 00:18:36,940
We only work out the the first two moments of distribution in the prediction, but only of such required of first two moments of random vector.

122
00:18:37,180 --> 00:18:41,810
Okay. And then because every types of random bird,

123
00:18:42,100 --> 00:18:49,089
random vector counts or proportion will continue categorical random vectors, they all have that, you know,

124
00:18:49,090 --> 00:19:01,000
this final first two moments so that we can easily pull out some kind of moment based prediction, a paradigm using this best unbiased prediction.

125
00:19:01,220 --> 00:19:04,690
Okay, so this is something we want to achieve at the very minimal level.

126
00:19:05,500 --> 00:19:15,130
This is a very classical thing. We learn that we go forward to more sophisticated sort of framework to make a prediction.

127
00:19:15,700 --> 00:19:22,750
So here, back to the very classical ones and old time computer power was very limited.

128
00:19:23,080 --> 00:19:30,190
People want to restrict the type of the prediction you can consider here, particularly or focus on the Nino predictor.

129
00:19:31,000 --> 00:19:34,660
And so we can easily quantify the prediction.

130
00:19:34,900 --> 00:19:47,110
Uncertainty. And then this is very essential when you do this are sort of the future in fashion case prediction.

131
00:19:47,890 --> 00:19:51,160
And of course, this is different from the applied mathematics.

132
00:19:51,170 --> 00:19:57,310
So from her program, Applied Mathematics, I started the work on the dynamics.

133
00:19:58,000 --> 00:20:01,710
Right. This function in time.

134
00:20:01,760 --> 00:20:12,340
It does. It's not that is stochastic process. So there are they cannot really evaluate the the uncertainty the random uncertainty.

135
00:20:12,760 --> 00:20:20,059
But in our contacts will work on this quantify this prediction uncertainty because

136
00:20:20,060 --> 00:20:23,680
so we think that data are simulated from certain underlying distribution,

137
00:20:24,280 --> 00:20:32,830
although we do not directly specify the distributions of data, but we assumed distributions exist.

138
00:20:33,130 --> 00:20:40,480
Okay. We only work on the moments of the distribution as part of the construction of predictors.

139
00:20:42,400 --> 00:20:47,710
So moving forward in this topic, I want to of you know,

140
00:20:49,240 --> 00:20:56,740
derive the common filter and sur where we can use that that can be used to really work out this

141
00:20:56,740 --> 00:21:02,680
sort of time source data and we can work out this time source of time source proportions.

142
00:21:03,160 --> 00:21:07,059
That's basically the the type of data we face.

143
00:21:07,060 --> 00:21:10,170
Right? We can look at number of confirmed cases.

144
00:21:10,180 --> 00:21:14,010
We can look at proportion of infection.

145
00:21:14,020 --> 00:21:22,000
Right. So so those are the sort of now normal data we we look at and we need something a little more

146
00:21:22,720 --> 00:21:30,790
general to deal with those count count types times use of console conscious proportions.

147
00:21:32,190 --> 00:21:37,360
Oh. So check my book, chapter nine.

148
00:21:37,410 --> 00:21:42,270
Sorry. Okay. So has this sort of all the details.

149
00:21:43,140 --> 00:21:46,780
So I basically pick up that chapter from my book.

150
00:21:46,830 --> 00:21:53,040
If you want to have a few chapters of that, there is a PDF file.

151
00:21:53,070 --> 00:21:57,090
Now they basically sell my book by chapters.

152
00:21:57,720 --> 00:22:01,020
Those popular short I Springer is smart. They write that.

153
00:22:01,020 --> 00:22:04,580
So people like they say, Oh, the whole book is too expensive, right?

154
00:22:05,250 --> 00:22:07,310
So I don't know what's the price of now.

155
00:22:07,320 --> 00:22:18,990
So but anyway, they sell by chapters, but I think I put the chapter nine as a PDF Free for your guys to download from offline form of course web page.

156
00:22:19,800 --> 00:22:25,060
So. Well, let me just talk about this, the basic framework.

157
00:22:25,100 --> 00:22:28,580
So we consider two random vectors X, the Y.

158
00:22:28,910 --> 00:22:32,240
Okay, the final and first and second moments.

159
00:22:32,390 --> 00:22:35,660
Okay, that's so we have two vectors you can think about this.

160
00:22:36,620 --> 00:22:49,030
X is a time series of, you know, confirmed cases and the Y is of this week or Y is time series of confirmed cases of last week.

161
00:22:49,050 --> 00:22:54,050
So you can imagine that that you have to back two random vectors, X and Y.

162
00:22:54,710 --> 00:22:58,670
They're not necessarily to be the same direction. Oh, same dimension.

163
00:22:58,940 --> 00:23:02,179
I'm sorry. I got up at 415 this morning.

164
00:23:02,180 --> 00:23:08,299
So my my words are kind of not well-controlled.

165
00:23:08,300 --> 00:23:12,709
Anyway, let's be real me. So the objective here is clear.

166
00:23:12,710 --> 00:23:20,400
So suppose that the Y has been observed because could be your historical data in the past months.

167
00:23:20,400 --> 00:23:29,510
For past three months now you want to predict x x would be like the number of confirmed cases next week.

168
00:23:29,870 --> 00:23:37,250
Okay, so you have a vector of seven elements or a seven dimensional vector you want to predict.

169
00:23:37,520 --> 00:23:46,120
Okay. And why is the vector of like 99 degrees Y contains 90 elements.

170
00:23:46,130 --> 00:23:53,050
That's the date I collect the parts. We want to make predictions.

171
00:23:54,250 --> 00:23:58,480
So the question here is what's there? What is a good predictor under conditions?

172
00:23:58,630 --> 00:24:06,480
Find out first two moments. Okay. So now we're coming from this sort of this firm point, this sort of argument.

173
00:24:06,850 --> 00:24:10,880
You can change the criteria. Right here, I'm talking by criterion.

174
00:24:10,900 --> 00:24:14,470
People adopt in their school of frequentist statistic.

175
00:24:15,160 --> 00:24:18,790
So a good predictor should be unbiased.

176
00:24:19,260 --> 00:24:22,540
See? Here it comes. Okay. Unbiased.

177
00:24:22,840 --> 00:24:30,960
And have the minimal total mean square arrow or the minimal error to norm or expect the square loss.

178
00:24:31,150 --> 00:24:37,510
Okay. So what do you want to do here? First, you wanted the predictor to be unbiased.

179
00:24:39,370 --> 00:24:47,139
You wanted the the prediction has the sort of optimality in the sense of minimal error.

180
00:24:47,140 --> 00:24:52,000
Two squared loss were expected. Square loss would be minimal.

181
00:24:52,270 --> 00:24:56,140
So that means square these refers to something.

182
00:24:56,800 --> 00:25:01,780
But anyway, here's the definition of a business.

183
00:25:02,150 --> 00:25:09,910
Okay. So here is your X. You want to it's something you want to predict is the factor that Y is.

184
00:25:09,910 --> 00:25:17,450
The data you observe is a vector x and y could have different dimensionality.

185
00:25:17,470 --> 00:25:26,710
Okay. Now you create a function theta to manufacture y or to summarize you y in some way.

186
00:25:27,430 --> 00:25:32,000
So this set on y will be used to predict x.

187
00:25:32,710 --> 00:25:37,990
So what you want here is that the difference is equal to zero.

188
00:25:38,380 --> 00:25:47,320
Okay. So if this equality falls, that means the theta y is unbiased predictor of x.

189
00:25:48,130 --> 00:25:52,690
So some some of you may ask why are you but.

190
00:25:57,900 --> 00:26:05,240
But. The. So I have a statistic.

191
00:26:10,710 --> 00:26:16,210
To estimate you can now write. Why?

192
00:26:16,730 --> 00:26:26,970
Because of the left hand side. There's. So a congressman can argue call to.

193
00:26:27,950 --> 00:26:33,870
On the Random Canal. You go to random. You have to you have to write this to define business.

194
00:26:34,570 --> 00:26:39,710
So. So the original definition. I'll buy this for estimate or it doesn't apply here.

195
00:26:39,730 --> 00:26:43,660
Okay. You can now write in this way because life inside is not random.

196
00:26:46,690 --> 00:26:52,420
So so this this is definition of a unbiased predictor, basically.

197
00:26:52,930 --> 00:26:58,750
So, you know, y will be used to predict the expected prediction error.

198
00:26:59,140 --> 00:27:02,890
This can be regarded as a prediction error X minus set of Y.

199
00:27:02,950 --> 00:27:12,760
The predictor tells you how much error this predictor incurs when they see that Y is used to predict x.

200
00:27:13,090 --> 00:27:17,020
So you have on average, this prediction error.

201
00:27:17,860 --> 00:27:21,520
Predicting error is zero. So this is definition unbiased.

202
00:27:22,120 --> 00:27:32,950
Okay. First, do you want to save up? You want theta to satisfy this property so that you define an unbiased predictor.

203
00:27:33,910 --> 00:27:41,680
Secondly, you'll want the state and Y to have the the minimal of mean square error.

204
00:27:42,670 --> 00:27:50,440
So basically you'll you want this mean square URL to be minimize among all possible theta functions.

205
00:27:52,210 --> 00:27:59,480
Okay. Okay. This. This first equation holds.

206
00:27:59,810 --> 00:28:04,480
So you want first state away to be unbiased? I'm an old, unbiased predictors.

207
00:28:04,850 --> 00:28:11,810
You'll want to pick up that thing that's such that this means square is minimized so that you achieve the optimal.

208
00:28:12,080 --> 00:28:15,170
Okay. So. So. So.

209
00:28:15,170 --> 00:28:21,050
That's basically purpose you want to achieve. Okay. It sounds like a very difficult problem to solve.

210
00:28:21,440 --> 00:28:27,440
But you'll know this role will, right? The famous CIA, all the Indians.

211
00:28:28,890 --> 00:28:35,310
Blackwell is a proficient bricklayer. Oh. So they had this famous Rob Blackwell theory.

212
00:28:35,820 --> 00:28:40,850
So essentially, what is this data function? They prove that, right?

213
00:28:40,920 --> 00:28:46,670
This theory of functioning is actually very simple. So two, two satisfies.

214
00:28:46,710 --> 00:28:52,160
There's two things on business and minimum is we arrow the theory of why.

215
00:28:52,170 --> 00:28:56,780
Actually, it's pretty it's pretty simple to specify.

216
00:28:56,940 --> 00:29:06,060
So what is that? According to the role Black Blackwell theory the theory of why the optimal one.

217
00:29:06,360 --> 00:29:12,630
Do you know by M1 m why the m is the optimal theta function?

218
00:29:12,850 --> 00:29:18,870
Okay. M why this m is spatial notation not the most optimal theta?

219
00:29:19,710 --> 00:29:23,520
What is that? This is conditioned expectation of x given y.

220
00:29:24,330 --> 00:29:28,350
So you know that this conditional expectation is a function of Y, right?

221
00:29:28,950 --> 00:29:35,879
From your sixth one, you know. So this is actually a O in sum.

222
00:29:35,880 --> 00:29:40,650
I mean, the people also give the name.

223
00:29:40,660 --> 00:29:46,680
This is basically it's projection. You project X to the space generated about Y.

224
00:29:46,900 --> 00:29:52,680
Okay. So this is the optimal solution.

225
00:29:52,860 --> 00:29:57,750
Very, very simple. It's basically conditional expectation of X given one.

226
00:29:57,990 --> 00:30:02,070
So here is the proof. Okay. First, before the proof is very straightforward.

227
00:30:02,940 --> 00:30:07,590
You have learned this already. You now to I think.

228
00:30:07,950 --> 00:30:20,190
So basically you you improve is simple simply the way that you just insert the this term directly into the expression.

229
00:30:22,810 --> 00:30:33,250
I'll prove that they prove this twice before. Here, you put a condition of expectation of X, given Y and minus plus.

230
00:30:33,460 --> 00:30:36,940
So you you just insert that term the right thing here.

231
00:30:37,630 --> 00:30:41,950
One positive sign, one negative sign. Then you read them.

232
00:30:42,450 --> 00:30:54,780
Okay. So then the class product becomes zero because your, your, your, the term user is condition expectation of x y.

233
00:30:55,170 --> 00:31:07,580
So you've become a two squared term. And then the square term will be the sum of the two, like quadratic term or the positive term will be minimized.

234
00:31:08,910 --> 00:31:21,520
One term is if X is. So, so sorry if this condition of expectation is that this second term will be minimized.

235
00:31:22,150 --> 00:31:25,960
Oh, so that this is how this is proved.

236
00:31:26,120 --> 00:31:32,290
Okay. That's quite amazing. Simple prove, but has been widely used in many places.

237
00:31:32,870 --> 00:31:40,480
Hmm. So in other words, the computer conditional expectation is the optimal prediction with the minimal error to loss.

238
00:31:40,640 --> 00:31:50,930
Okay. Okay. So of course, then this conditional expectation of X giving life would be a nonlinear function of Y.

239
00:31:51,100 --> 00:31:55,890
Right. Well, this this this shows use m sorry.

240
00:31:56,100 --> 00:32:04,739
Here is just m but you know, this optimal solution is only a conditional expectation of X and Y.

241
00:32:04,740 --> 00:32:08,600
And this function y is not necessary to be near.

242
00:32:08,610 --> 00:32:12,360
Right. Because this could be it could be a very complex form.

243
00:32:13,200 --> 00:32:17,340
So people say, well, this is too complex what we can do here.

244
00:32:17,850 --> 00:32:28,980
To simplify it, it is to make this function, force the function to be a linear function, and we force the function to another function.

245
00:32:28,990 --> 00:32:33,390
This is not global optimal solution.

246
00:32:33,990 --> 00:32:40,710
This is no longer global, but it's a optimal, we think, a minor function of that.

247
00:32:40,770 --> 00:32:53,010
Okay. So your force, you theta function to be another function of y and basically reduce your function of space from non-linear functions

248
00:32:53,370 --> 00:33:01,679
into a subspace of minor function of why y you want to do this because in old time you don't have much computing power,

249
00:33:01,680 --> 00:33:05,670
you cannot do anything. So you cannot do this generated model.

250
00:33:05,790 --> 00:33:10,080
Just you don't have that computing power to do very specific function of form.

251
00:33:10,530 --> 00:33:16,200
So what people can do is really do something, you know, simple or you can think about this.

252
00:33:16,200 --> 00:33:19,830
It's just the first order, tighter expansion of your nonlinear function.

253
00:33:20,310 --> 00:33:24,230
So you know that this function itself is very complex, could be done.

254
00:33:24,990 --> 00:33:34,350
I mean, it's very hard to figure it out. I mean, now we know that you can do this by, you know, all other methods or machines,

255
00:33:34,860 --> 00:33:39,929
but old time you don't have that so that people say, well, I cannot have the nonlinear function.

256
00:33:39,930 --> 00:33:48,630
Maybe I just think about the linear function which can be regarded at the first order to the expansion of my nonlinear function.

257
00:33:48,820 --> 00:33:57,070
Okay. So restriction, then you can solve this one very easily analytically.

258
00:33:57,190 --> 00:34:06,040
Okay. So here, of course, a would be a vector of the same dimension of X and b will be a make.

259
00:34:06,550 --> 00:34:14,620
It's not a square matrix. B is not a square matrix where the number of rows is equal to the vector of the

260
00:34:14,620 --> 00:34:21,910
dimension of x number of columns that will be having the same dimension of your Y.

261
00:34:22,150 --> 00:34:27,400
Okay. So. So that you know that you introduce this.

262
00:34:27,430 --> 00:34:38,860
Okay. Then you can calculate the of the meaning of this X, and then you do the optimization based on this inner function.

263
00:34:41,380 --> 00:34:48,300
So. So what do you have here? Well, after you do this sort of stuff, that's okay.

264
00:34:48,850 --> 00:34:53,680
Why do you want to find out? Here is what is your aim? What is your beef?

265
00:34:53,890 --> 00:35:02,140
Right. If you want to create what is the optimal a when you falsely say that life pseudo life function to be a function of why,

266
00:35:02,530 --> 00:35:05,799
what do you need to figure out is what is you a what is optimal?

267
00:35:05,800 --> 00:35:09,310
A what is optimal be. Okay.

268
00:35:10,270 --> 00:35:21,240
If you can find the optimal and be first the this to a unknown no quantities should satisfy.

269
00:35:21,250 --> 00:35:24,670
First of all, they say to what is a wisest predictor.

270
00:35:24,760 --> 00:35:29,410
Secondly, that this will minimize the error to expect the error to loss.

271
00:35:30,340 --> 00:35:43,450
So if you do this simple calculation, the eight is this one they mean of X and B times mu y the expectation of what or why

272
00:35:44,110 --> 00:35:52,060
and what is the b b is actually this form which is very similar to the least square.

273
00:35:52,090 --> 00:35:58,660
Right. So the covariance of extra y and the inverse variance y.

274
00:35:59,340 --> 00:36:04,840
Okay. So this is very, very similar to the discourse solution, but this is population level solution.

275
00:36:05,290 --> 00:36:16,570
So quarters of X. The inverse of that is basically the optimal solution is very, very similar to this discourse solution.

276
00:36:16,600 --> 00:36:21,430
But this is, first of all, working in the paradigm of prediction.

277
00:36:21,760 --> 00:36:25,150
Secondly, we're working on the population level rather than the data level.

278
00:36:26,830 --> 00:36:35,080
So when you have this optimal and we obtenus, you can plug in back to the linear form.

279
00:36:35,710 --> 00:36:46,570
So where you can simplify this in such a form, where is is a near function y.

280
00:36:46,690 --> 00:36:50,470
Okay. So some new x. This is the meaning of your x.

281
00:36:50,980 --> 00:36:54,880
And this is coming from the B matrix minus additional term.

282
00:36:55,420 --> 00:36:59,560
Okay. This additional term is coming from this this part for you.

283
00:36:59,620 --> 00:37:07,970
A new work to achieve on business. So if you you're plotting A and B together, you can use to write in this form.

284
00:37:07,990 --> 00:37:12,880
This will be your best your unbiased predictor.

285
00:37:13,150 --> 00:37:25,260
Best meaning means that it has a minimal achieves the minimal expected error to loss or minimal mean square you.

286
00:37:26,440 --> 00:37:30,010
Because this is a minor predictor and it is unbiased.

287
00:37:30,040 --> 00:37:35,350
This is something we want and is a predictor. A predictor for your X.

288
00:37:36,400 --> 00:37:39,760
Okay. So, so so that's why I call this block.

289
00:37:39,940 --> 00:37:44,110
Okay. Best unbiased predictor, which takes this form.

290
00:37:44,810 --> 00:37:54,070
Okay. So now in the following in my book that I write this into M X give of Y so that it's

291
00:37:55,600 --> 00:38:03,340
clear that you want to predict X using data of Y that will be expressed as this.

292
00:38:03,500 --> 00:38:07,870
Okay. Now, after this, you can calculate this prediction error.

293
00:38:08,860 --> 00:38:19,240
Well, this basically is this means square arrow, right? If I use this graph as my theta y, what is this?

294
00:38:19,900 --> 00:38:23,530
You know, this error matrix.

295
00:38:23,680 --> 00:38:26,919
So this is a vector business. This column vector.

296
00:38:26,920 --> 00:38:35,890
This is rollback. Well, this will be a if X has some dimension that this will be seven by seven square matrix.

297
00:38:36,420 --> 00:38:45,130
Okay. This this one is the matrix that measures the, you know, the prediction error.

298
00:38:45,940 --> 00:38:49,930
So that can be easily calculated, can prove it.

299
00:38:50,110 --> 00:39:04,070
Okay. So so essentially what I'm saying here is that when Y and X a correlated so that the covariance matrix x Y is non-zero.

300
00:39:04,420 --> 00:39:17,110
Okay. And this sycamore matrix Y is always positive in inverse, uh, inverse of the covariance matrix of y is also positive definite.

301
00:39:17,680 --> 00:39:22,749
And you multiply this covariance matrix X and Y either the positive.

302
00:39:22,750 --> 00:39:30,100
Corey a negative core, but you have this quadratic form. So this second term, it's always positive or zero, right?

303
00:39:30,700 --> 00:39:34,960
If X know why I'm calling this, then this term is always positive.

304
00:39:35,260 --> 00:39:45,790
That means the covariance matrix of x will be reduced because you're adding more information.

305
00:39:46,600 --> 00:39:53,590
Right? So if you don't have any information to predict x, then you actually covariance matrix.

306
00:39:55,290 --> 00:40:05,580
Now, if you use a theta Y that's called the X, if you do it in this way, then this virus will be reduced.

307
00:40:06,690 --> 00:40:10,230
So that's you have a smaller bounce that. Okay.

308
00:40:10,770 --> 00:40:23,700
So, uh, you know, people use this to, in order of context to reduce the, sort of the appearance of you are a predictor.

309
00:40:23,940 --> 00:40:32,710
Okay. If you bring something relevant to your original variable, it can always help you to reduce that.

310
00:40:33,030 --> 00:40:40,380
Okay. If X and y are uncorrelated, then this term is zero going back to the variability effects.

311
00:40:40,750 --> 00:40:46,800
Okay. You can reduce the variability of x by putting the prediction there.

312
00:40:47,130 --> 00:40:51,600
Okay. Information, data. And you know, this fact that.

313
00:40:54,190 --> 00:40:59,580
This has very important property, right?

314
00:40:59,590 --> 00:41:05,530
So x transpose x, this is in their product of this vector.

315
00:41:05,980 --> 00:41:09,920
So access vector in their product of x, right?

316
00:41:09,940 --> 00:41:14,720
So they have the role times the column load, times the column getters.

317
00:41:15,100 --> 00:41:25,680
Number two, this is a number. So the trace of this is equal to this is self because trace is actually the disarm of that.

318
00:41:26,230 --> 00:41:30,310
This X transpose x is just a one way matrix.

319
00:41:30,940 --> 00:41:35,320
Right? But the trees has the property that you can swab the water.

320
00:41:39,530 --> 00:41:43,160
They have the same choice. This is property of trees.

321
00:41:43,220 --> 00:41:49,700
Right. So that's why I'm saying that expectation of this is equal to expansion of trees of this.

322
00:41:50,390 --> 00:41:58,970
And the expectation is an operator because x.

323
00:42:00,600 --> 00:42:07,550
X x plus x two is equal to expectation x plus.

324
00:42:07,560 --> 00:42:12,840
This is an x two but. Seems that Nina operated her.

325
00:42:13,320 --> 00:42:19,820
So we have a Nina. It's an expectation of some.

326
00:42:21,050 --> 00:42:26,390
Okay. So Treece is also having this property.

327
00:42:26,660 --> 00:42:33,350
Okay. So so trace is a sum of diagram if the sum of diagrams.

328
00:42:33,650 --> 00:42:37,410
So you can swim the tweez, the patient.

329
00:42:37,430 --> 00:42:45,950
That's exactly what you have. And then for this part, exactly what you have here.

330
00:42:46,580 --> 00:42:52,440
So that's that's the trick when people want to evaluate the total variability of prediction.

331
00:42:53,360 --> 00:42:59,660
So they first calculate this covariance matrix. I use this argument to argue the.

332
00:43:00,140 --> 00:43:05,120
For example, you can think about this as a something like this.

333
00:43:05,660 --> 00:43:12,830
Okay. So that you can calculate this using this argument, get the overall prediction error.

334
00:43:13,230 --> 00:43:30,320
Yeah. So just to do a little bit notation to go folder now I use x conditional y use this to the es doesn't this does not mean this is distribution.

335
00:43:31,310 --> 00:43:34,490
This means that I have the first moment and a second moment.

336
00:43:34,880 --> 00:43:43,970
Okay. I use this notation to t know that this one the conditional expectation of x give the y and the c x given y.

337
00:43:44,360 --> 00:43:51,600
This notation denotes the chorus of x given y conditional for that's okay.

338
00:43:51,890 --> 00:43:57,410
Well that's ask something i, i, i did note.

339
00:43:57,530 --> 00:44:01,009
And then based on this block okay,

340
00:44:01,010 --> 00:44:10,130
we know that the best of all the prediction predictor of X given Y is this form

341
00:44:10,700 --> 00:44:17,090
and this corresponding coordinates structure is the one I just calculated.

342
00:44:18,170 --> 00:44:25,460
Right? Just calculated. Right. This is basically covariance of your prediction.

343
00:44:26,480 --> 00:44:35,500
Okay. So so that's the notation. Or I already said this part that's to skip this.

344
00:44:36,180 --> 00:44:40,540
And moving forward, what are the basic properties of this block?

345
00:44:40,840 --> 00:44:49,600
Okay. So the first thing I already explained that this is actually the inverse of X, given Y.

346
00:44:50,020 --> 00:44:54,220
Okay. So we know this property in the.

347
00:45:04,960 --> 00:45:09,710
Of X equal to its.

348
00:45:12,230 --> 00:45:15,350
If a one plus variant of.

349
00:45:19,660 --> 00:45:27,360
That's that's the well-known. And so that's exactly what same.

350
00:45:27,990 --> 00:45:33,810
And in this case, that we have a margin of X, which is sigma x.

351
00:45:36,310 --> 00:45:42,940
It's basically the same most. I'll put it through notation.

352
00:45:43,780 --> 00:45:52,870
And so what is the my of the of variance of expectation of e of what?

353
00:45:54,270 --> 00:45:58,680
This part. I know that, right? This is the one I recalculated it.

354
00:45:58,920 --> 00:46:02,730
This one this is.

355
00:46:11,890 --> 00:46:17,860
But I just did. And now what is this term that he basically canceled out of this?

356
00:46:19,840 --> 00:46:23,170
But exactly what this conclusion spell. Okay.

357
00:46:23,560 --> 00:46:33,670
So the viewers of the best on the block, the appearance of the of the block is equipped with this.

358
00:46:36,580 --> 00:46:52,120
That's that's what you have. And for the Sugar Bowl matrix, the skills are often better if you have a off line transformation or inner transformation.

359
00:46:55,400 --> 00:46:58,640
You still have offline transformation of your.

360
00:47:00,330 --> 00:47:04,620
And your predictor can have a similar sort of form.

361
00:47:05,190 --> 00:47:11,360
So the mean times for me to dinner times situation on the block and that is of

362
00:47:12,210 --> 00:47:17,040
prediction error is also still about half an hour from transpose which is the property.

363
00:47:17,040 --> 00:47:21,510
You know that in the Matrix you can prove that that's quite straightforward.

364
00:47:24,760 --> 00:47:33,610
The answer is very important. Okay. So Taapsee says that the production arrow and the Y.

365
00:47:37,130 --> 00:47:41,780
So if you use this Amex given, why is the blob right.

366
00:47:42,380 --> 00:47:54,170
If you calculate whether or not this the this arrows contains some information why this property says no.

367
00:47:54,590 --> 00:48:05,959
You know your ex has a orthogonal projection to the exa has a following the projection to the space span by Y so that this you can call this residual

368
00:48:05,960 --> 00:48:17,780
or prediction error contains no information Y is orthogonal to the Y there is no relationship between Y and this prediction residual prediction error.

369
00:48:18,140 --> 00:48:25,610
You can prove that very easily. Well, that's why this is very, very successful prediction.

370
00:48:26,000 --> 00:48:30,350
You do this prediction and the prediction error has nothing to do that.

371
00:48:30,830 --> 00:48:39,260
There's basically the this this prediction for predictor takes all the information Y in consideration.

372
00:48:39,260 --> 00:48:51,530
The error has no residual information related to Y, so that's very poor property to prove this predictor of the blob to be a good predictor.

373
00:48:51,830 --> 00:48:57,860
Okay. So if you have X, Y and Z are uncorrelated.

374
00:48:58,280 --> 00:49:10,939
Okay, basically, you know, you have, you know, two factors correlated and then you can basically predict X by Y and predict X,

375
00:49:10,940 --> 00:49:16,310
Y, Z, then you could write them in a same way.

376
00:49:16,340 --> 00:49:23,900
Okay. So the the X can predict by this Y and Z in this form.

377
00:49:24,440 --> 00:49:29,960
So the first part is coming from this, uh, uh, obviously the, uh,

378
00:49:30,380 --> 00:49:37,190
the best sort of the prediction based on y the second part is the best the prediction from Z.

379
00:49:37,610 --> 00:49:44,900
Okay. This is more like a good run, a linear regression, doing the least square estimation using to predict to covariance.

380
00:49:45,290 --> 00:49:54,350
Right. So basically you want pretty x, y, y and z, but Y and Z are independent, are uncorrelated in this case.

381
00:49:54,830 --> 00:50:02,059
So essentially, are you running a linear regression with two covariance and this two quarters are

382
00:50:02,060 --> 00:50:11,000
uncorrelated so that you basically have this kind of a sort of additive for all of this,

383
00:50:11,690 --> 00:50:17,179
a predictor. And also the variance part is very similar, right?

384
00:50:17,180 --> 00:50:21,140
So this is the origin of variance of your X.

385
00:50:21,620 --> 00:50:28,460
Then after use the information from Y, you reduce this original variance by this amount,

386
00:50:28,970 --> 00:50:40,760
but now you have additional variable Z that can continue to reduce the appearance of x, but x and y are oh.

387
00:50:40,760 --> 00:50:44,839
There is no cross project term between y, y and Z.

388
00:50:44,840 --> 00:50:49,190
So that's very easy to prove. If you want proof, that's quite easy.

389
00:50:51,740 --> 00:50:57,830
And that's an property. Now those are our basic properties and the vast property is like this.

390
00:50:59,030 --> 00:51:02,790
So X and Y Z are random vectors.

391
00:51:02,810 --> 00:51:06,950
These are finite sycamore. Then we have the following result.

392
00:51:07,290 --> 00:51:16,340
Okay, so first of all, the joint predictive of the Y and Z give y, x and y duties is given as follows.

393
00:51:17,030 --> 00:51:24,229
So here you have two x vectors, the Y and Z, a white x and y.

394
00:51:24,230 --> 00:51:31,850
You want to use the Z to predict x, the y, and here x and y are not necessarily uncorrelated.

395
00:51:31,990 --> 00:51:35,600
Okay. X and y could be correlated. Could be right.

396
00:51:36,080 --> 00:51:40,650
There's no requirement that this x, y or z of solving them.

397
00:51:40,670 --> 00:51:52,069
Really. So that you have this one where the this is the flop of x predicted by Z.

398
00:51:52,070 --> 00:51:58,280
This is blob of Y again projected by Z as a statement here.

399
00:51:58,670 --> 00:52:03,580
Now you have this sort of the producing error part.

400
00:52:04,040 --> 00:52:17,600
So this is the prediction error of X when Z is used in the prediction and this one is the to one that of prediction, your Y given Z.

401
00:52:18,170 --> 00:52:22,880
And this is if X and Y are orthogonal, of course this.

402
00:52:26,440 --> 00:52:33,910
I will be. I'm sorry. So this is quite bizarre prediction of that.

403
00:52:34,480 --> 00:52:38,140
If X and Y are uncorrelated, then this is zero.

404
00:52:38,350 --> 00:52:43,210
And this is not necessary. Zero. Unnecessary zero.

405
00:52:43,600 --> 00:52:50,110
But this one is zero. So this term is non zero because the second term is not zero.

406
00:52:50,770 --> 00:52:55,240
Unless you have some field or something about caught, then that's.

407
00:52:57,720 --> 00:53:01,010
The Y uncorrelated only guarantee that this time is zero.

408
00:53:01,110 --> 00:53:04,410
It doesn't imply this okay to be.

409
00:53:07,130 --> 00:53:10,230
A joint predictor of external link efficiency.

410
00:53:12,710 --> 00:53:20,330
Yup. Is is is cognition wise given seeing an X, Y and Z that the two of the twins are in the same.

411
00:53:21,200 --> 00:53:29,690
They're the same. They are seen. Good. Okay, so this is X, Y and Z of X.

412
00:53:31,250 --> 00:53:41,660
You should flip it over. Right? Always too, because that's the virus from your data is easy to use.

413
00:53:42,200 --> 00:53:47,000
But this one could be easy. And. I think we should it.

414
00:53:47,180 --> 00:53:51,620
I think so. Here, if you.

415
00:53:52,400 --> 00:53:56,750
If I have a different dimension, these two terms should be flipped around.

416
00:53:56,900 --> 00:54:01,940
I'll just. Just. It is transport, correct?

417
00:54:02,120 --> 00:54:06,260
It's transport. Yeah. Yeah. This. This metric has to be symmetric.

418
00:54:06,530 --> 00:54:11,490
It. Is that supposed to be symmetrical matrix?

419
00:54:12,510 --> 00:54:25,170
Okay, from here you can derive a case where you use y and Z to predict X and y and z are not necessarily core uncorrelated.

420
00:54:25,470 --> 00:54:30,010
Before we have a formula that you can see, we're right here.

421
00:54:30,030 --> 00:54:33,089
Right. So when Y and Z, I'm correct.

422
00:54:33,090 --> 00:54:36,120
That's the form you get. Right. That's for. Is it not?

423
00:54:36,990 --> 00:54:41,370
Next to the property we see here. Is this why it is?

424
00:54:41,430 --> 00:54:46,620
They are not necessarily uncorrelated. It's more general than the previous case.

425
00:54:47,060 --> 00:54:53,280
Okay. Well, if you use this to predict X, so that's the formula for you.

426
00:54:56,780 --> 00:54:59,930
And so.

427
00:55:04,170 --> 00:55:07,680
I'll just say that this is the flop of X given Z.

428
00:55:08,160 --> 00:55:18,840
Of course, this there's a symmetric situation that you can see X given you can exchange zero one.

429
00:55:19,860 --> 00:55:21,900
You can change Z in the Y,

430
00:55:22,110 --> 00:55:36,690
but here in this property is derived from the previous property where the first appears as the sum of the one in the conditional part first.

431
00:55:36,810 --> 00:55:42,690
So this formula is derived from here so that you have y could see if you wanted.

432
00:55:42,720 --> 00:55:53,490
You know re derive this formula one that starting with ex z give a wide range of basic and switch the y and z expression.

433
00:55:53,790 --> 00:56:00,449
But since I write this one in the way that Z appears,

434
00:56:00,450 --> 00:56:10,739
the first as the conditional sort of information I have to predict when X and Y, then when I move the Y to the condition of part.

435
00:56:10,740 --> 00:56:19,620
This is the resulting formula I have, but you can do a slightly different way if you want to switch Y and Z.

436
00:56:23,790 --> 00:56:30,410
David Z. At the. I'm your wife.

437
00:56:30,770 --> 00:56:35,630
Okay. We will use Y to predict the exact opposite.

438
00:56:35,900 --> 00:56:40,160
You said y minus this block y defensive.

439
00:56:40,460 --> 00:56:44,720
Okay, so that that's basically the formula that you have.

440
00:56:48,810 --> 00:56:59,420
So. So if you have an iPhone.

441
00:57:00,480 --> 00:57:03,970
Exactly. Be the case I talk about then.

442
00:57:04,890 --> 00:57:08,460
So when X and Y are conditionally uncorrelated given.

443
00:57:09,830 --> 00:57:20,870
And if this to our conditional expectation are minor, is either this one is Nina or this is her.

444
00:57:21,950 --> 00:57:25,990
Then this prediction will be equal to.

445
00:57:26,490 --> 00:57:36,590
Oh, same as this one. Okay. So this is essentially what you know that in this.

446
00:57:37,040 --> 00:57:42,200
So the meeting table literature. Right. So basically, you can imagine that.

447
00:57:43,220 --> 00:57:51,320
You can imagine that the this is your total covariance.

448
00:57:51,980 --> 00:57:56,000
And why is your recent data indicator right.

449
00:57:56,030 --> 00:57:57,370
Why do I'll do that. Some reason.

450
00:57:58,130 --> 00:58:09,110
So conditions on covariance your reason data mechanism which is you know that y is independent actually your observed the data then

451
00:58:09,110 --> 00:58:21,500
basically the the recent data park will not affect the way the meeting data arrives with no fact actually be the result of your x given z.

452
00:58:22,790 --> 00:58:27,400
Let me just repeat it. So this probably has been seen in the recent literature.

453
00:58:27,800 --> 00:58:29,090
So let's see,

454
00:58:29,390 --> 00:58:39,920
x denotes actually did they observe y denotes actually that recent data indicate or reason that the mechanism they g knows covers in the meeting.

455
00:58:40,310 --> 00:58:47,870
Meeting in a random literature. A situation. Right. Confusion on Corvids.

456
00:58:48,030 --> 00:58:51,860
Well, this also used to be causal events, so conditional covers.

457
00:58:55,330 --> 00:59:01,690
Is independent of how they are leasing so that basically if you.

458
00:59:03,000 --> 00:59:08,700
The relationship between X and Z. Under this condition, the independent assumption.

459
00:59:08,850 --> 00:59:19,110
And you do not need to worry about. For context conditions indirectly of.

460
00:59:25,130 --> 00:59:30,900
A condition of independent condition. It's phenomenal.

461
00:59:31,360 --> 00:59:39,329
Okay. So this is, again, a random sort of assumption here.

462
00:59:39,330 --> 00:59:46,500
You can prove it in the prediction as well. Okay. If X the Y condition conditionally unquote.

463
00:59:47,970 --> 00:59:52,740
And you few. Right. You can use these to predict X.

464
00:59:54,370 --> 01:00:07,560
And right. So this is even more complicated situation.

465
01:00:07,820 --> 01:00:11,010
Okay, so I have this.

466
01:00:11,010 --> 01:00:28,030
It's more like the property of two. So basically, you know that x10 y and z.

467
01:00:32,130 --> 01:00:35,640
As X, Y and Z.

468
01:00:40,420 --> 01:00:49,150
So the joint distribution company think. So here in the moment comes.

469
01:00:49,160 --> 01:00:56,240
You should. It's sort of a similar situation if you have a free.

470
01:01:00,610 --> 01:01:05,080
So you have this prediction for Y given Z.

471
01:01:05,880 --> 01:01:09,240
That's the flop. This is the prediction error.

472
01:01:11,170 --> 01:01:16,650
Her piece, X, Y and Z. And this is the one where the.

473
01:01:23,410 --> 01:01:34,750
Well, this is a neither predictor. The. So if you have this to do to this issue, should you have this two spatial forms?

474
01:01:35,680 --> 01:01:42,250
Then you can have the following result. First of all. You want to mark the conditional distribution.

475
01:01:42,520 --> 01:01:55,830
Or if you have. What is this?

476
01:01:56,530 --> 01:02:01,680
So what is this? Basically the same question I ask.

477
01:02:01,920 --> 01:02:05,790
If I have this predictor, I have this predictor.

478
01:02:06,000 --> 01:02:11,760
Okay. And what is the predictor of doing predictor of X to likability?

479
01:02:12,030 --> 01:02:15,600
So here is actually the formula in calculating.

480
01:02:18,510 --> 01:02:29,980
Oh. So I probably if you if you're interested in improving progress giving in the chapter so that you have this minor part.

481
01:02:30,000 --> 01:02:34,590
Basically you replace you'll replace that one here.

482
01:02:35,310 --> 01:02:42,150
You place the Y here by the plot of like fancy and the Z.

483
01:02:42,150 --> 01:02:51,160
It doesn't change, right? So that. And why a prediction of this prediction?

484
01:02:54,090 --> 01:02:59,250
This sort of prediction error in our way given.

485
01:03:04,350 --> 01:03:14,340
Well, now you can marginalize this. This is one thing people may ask if they have 20 distribution of this.

486
01:03:17,250 --> 01:03:20,880
When distribution or that kind work out conditions of distribution and.

487
01:03:25,290 --> 01:03:31,800
Well if I integrate our Z. The.

488
01:03:34,040 --> 01:03:37,910
I didn't agree that. Why here? But what is the need in protection?

489
01:03:39,480 --> 01:03:45,240
You. That's the form you get.

490
01:03:51,840 --> 01:03:57,300
Oh, so the. The.

491
01:04:01,820 --> 01:04:07,200
If you're interested. Okay, you can. To look at the detail of the proof.

492
01:04:07,220 --> 01:04:10,550
There are just some warming calculations.

493
01:04:10,700 --> 01:04:16,790
Okay. Should be. If you think it's too difficult for you to prove, it's fine.

494
01:04:16,790 --> 01:04:23,380
Just you can walk. Okay. Deprived of that you can.

495
01:04:32,780 --> 01:04:34,570
Popular France loser. Oh,

496
01:04:34,760 --> 01:04:42,350
this is actually the motivating example because I already mentioned a few of our guest moves are are motivated from engineering

497
01:04:42,350 --> 01:04:55,020
project right so in the 1980 1950s so you have this aerospace special aerospace space eral engineering space engineering.

498
01:04:55,970 --> 01:05:00,920
So here you'll have those computers NASA has and are on the ground.

499
01:05:01,730 --> 01:05:12,350
Okay. So now you launch whatever this flying object to this space, either a rocket or a space shuttle or something like that.

500
01:05:13,400 --> 01:05:17,570
So, of course, that offered this flies to some distant.

501
01:05:17,570 --> 01:05:22,190
It's all of your eye inspection, right?

502
01:05:22,190 --> 01:05:25,640
So you cannot see this visual anymore.

503
01:05:26,330 --> 01:05:33,080
So this flying object will send signal back to the loop nozzle.

504
01:05:33,890 --> 01:05:40,500
All the computers that we received, the signal sent back from this space shuttle because they flies,

505
01:05:40,770 --> 01:05:44,060
we are more and more away from you cannot see this. Okay.

506
01:05:44,540 --> 01:05:55,280
So so here is essentially what they formulates to monitor the trajectory that this flying object moves in this space.

507
01:05:56,060 --> 01:06:03,240
Okay. So this is basically the. What a computer comments.

508
01:06:13,160 --> 01:06:19,030
To leave because this is really, really. Oh, well, see that he.

509
01:06:23,170 --> 01:06:32,350
Of this flying rock. You don't see it. Leighton. Though.

510
01:06:32,460 --> 01:06:35,730
So it's always a latent position.

511
01:06:38,860 --> 01:06:47,280
Now at this position. Just one object like the space shuttle sent a signal.

512
01:06:47,550 --> 01:06:51,060
Which is why team which can.

513
01:06:52,670 --> 01:06:58,310
And the computers in the whatever station outside muscle station.

514
01:06:58,760 --> 01:07:08,540
So that's why these observe the scene back from this one back to the outside your computer is that why is the data captured by.

515
01:07:11,860 --> 01:07:15,980
The. I object to will move the position from.

516
01:07:18,840 --> 01:07:36,730
Plus 100 and. Well, this object flies in computer time somewhere in the 20 kilometers of wave on the space.

517
01:07:37,240 --> 01:07:43,510
We don't see it, but they send signals every 2 minutes, every 5 minutes back to Earth.

518
01:07:56,530 --> 01:08:02,590
So why are the data captured the computer and you have this latent of.

519
01:08:03,930 --> 01:08:23,350
For. I think. Or you want to figure out will see that observe using these signals sent back by the flying object.

520
01:08:23,770 --> 01:08:29,200
So what are trying to do? Why the historical data?

521
01:08:29,950 --> 01:08:36,070
Why one or why zero? The opposition in this launch.

522
01:08:39,890 --> 01:08:43,090
But the consequences. Do you have data?

523
01:08:43,580 --> 01:09:01,660
So you have this vector. Why? Position of the rocket or this space shuttle gave him all.

524
01:09:03,940 --> 01:09:08,710
Okay. That's the engineer problem. It's not a problem I'm talking about here.

525
01:09:09,610 --> 01:09:27,560
Okay. It's this link.

526
01:09:27,890 --> 01:09:35,420
So why is all the signals received up to this time, like 2 hours later?

527
01:09:35,450 --> 01:09:38,450
You know, the shuttles, the space shuttle is flying somewhere.

528
01:09:38,460 --> 01:09:41,720
You'll receive like minimum amount with signals back from it.

529
01:09:42,290 --> 01:09:46,100
Now, you wonder, what's the current position of this space shuttle?

530
01:09:46,340 --> 01:09:53,290
You want make that prediction. Okay. At that time, 1950s, you do not have very power over computer.

531
01:09:53,300 --> 01:10:02,350
We do have computer now, very powerful. So what you're trying to do is to block in the on base.

532
01:10:03,910 --> 01:10:10,270
In engineer you call common fuel. This engineer proposed this.

533
01:10:10,680 --> 01:10:20,730
I will tell you, why wouldn't this block why we named this give a special name called Common Filter possibly find a very fast way to compute this.

534
01:10:24,090 --> 01:10:28,850
Using a recursive formula. Okay.

535
01:10:28,850 --> 01:10:35,710
So so this is this this piece animal is well motivated by this event here, Bob.

536
01:10:36,590 --> 01:10:42,730
No. It's essential, very, very similar to what we're thinking about infectious disease.

537
01:10:45,510 --> 01:11:00,830
If. Current number of the infected of effective individual infected individuals are being infectious.

538
01:11:01,400 --> 01:11:06,230
So this is something to observe. This is underlying infection situation.

539
01:11:06,650 --> 01:11:09,830
But you have surveillance data. You can.

540
01:11:15,780 --> 01:11:19,610
From Civilian system hospital testing center.

541
01:11:19,620 --> 01:11:30,170
That's the way. Diseases that's not directly observable.

542
01:11:31,600 --> 01:11:40,890
Well so this why is this small is useful is because this model structure very similar to what we're thinking about infectious diseases you.

543
01:11:46,450 --> 01:11:55,030
What you have is captured some snapshots of this whole in factories and they were trying to figure out what is going on.

544
01:11:55,520 --> 01:11:58,780
The two. The truly fascist situation.

545
01:12:01,900 --> 01:12:17,410
Very similar problem. Okay. You don't. 5 seconds or every ten six you receive the signal back, sent back by the base, this flying object.

546
01:12:17,410 --> 01:12:26,670
And you tend to find the current position that you don't. That's exactly something called a stage space ball.

547
01:12:27,810 --> 01:12:32,550
It is not a spatial model is a temporal model.

548
01:12:32,850 --> 01:12:38,670
So sometimes become confused why it is called space. Space based model is it's not something.

549
01:12:38,700 --> 01:12:40,260
It's nothing to do with space.

550
01:12:41,010 --> 01:12:48,000
But because of that engineering, things like people are trying to find out the position of the flying object in the space,

551
01:12:48,420 --> 01:12:52,920
the state space or the state in this space.

552
01:12:53,190 --> 01:13:02,400
Okay. And they call this space space small, but actually is a time so small is smaller for the temporal process, not the spatial process.

553
01:13:04,020 --> 01:13:07,300
The structure is very, very straightforward.

554
01:13:07,320 --> 01:13:12,940
You have Markov. The moved according to sort of a rule.

555
01:13:13,300 --> 01:13:21,940
I mean an infectious disease will follow this SRM all the CIA model S air removal and so so this there's a rule.

556
01:13:22,360 --> 01:13:29,589
Okay so you know there's a rule to how these positions will move in the infectious disease.

557
01:13:29,590 --> 01:13:36,790
Of course, this theta will be evolving according to the differential equation in this space,

558
01:13:36,790 --> 01:13:41,020
space like space shuttle, this will be move according to certain orbit.

559
01:13:41,080 --> 01:13:52,900
Right? This gravity theory and all everything, there's a law behind this sort of evolution, of evolution of the stick positions.

560
01:13:53,260 --> 01:13:57,430
Okay. In effect, effectively, this is described by differential equations.

561
01:13:58,000 --> 01:14:02,489
But anyway, this is not the right observed. This is something you are interested in.

562
01:14:02,490 --> 01:14:10,450
You like to figure out. But from some discrete time, you capture some information, send from this system, the underlying little system.

563
01:14:10,810 --> 01:14:14,530
There are some information out from here which you can capture.

564
01:14:15,690 --> 01:14:22,080
This is observed data. Okay. This is your latent process of interest.

565
01:14:22,800 --> 01:14:26,190
And then the whole purpose is very straightforward.

566
01:14:26,200 --> 01:14:30,840
You want to use observed data to estimate process.

567
01:14:31,820 --> 01:14:43,200
Okay, that's first. So we have observe process like daily case loads of from CBC that you have in process the current status of COVID infection.

568
01:14:45,570 --> 01:14:49,070
Now I call this calm structure is more like calm, right?

569
01:14:51,100 --> 01:14:56,020
But, you know, it's a very straightforward, intuitive.

570
01:14:56,530 --> 01:15:02,140
But now how do you do that? Okay. So we use our block to do this.

571
01:15:02,620 --> 01:15:12,040
And common this engineer, if we call them very fast and we compute blowup using recursive formula, that formula is called a few common future.

572
01:15:12,460 --> 01:15:21,850
Okay. I will give you a little bit more details when we come to that stage to say how our block can be formulated in a very,

573
01:15:22,900 --> 01:15:26,260
very special, specific way that will can do fast computing.

574
01:15:27,730 --> 01:15:34,420
Okay. So just give a little bit form a different definition of a space based model.

575
01:15:34,710 --> 01:15:39,730
Okay. Some that are two factor value is stochastic process, which he had.

576
01:15:39,730 --> 01:15:47,320
This is a sequence of data observed, you know, and then you have your latent process.

577
01:15:47,690 --> 01:15:54,400
Okay, then what's special about this comp structure, this process?

578
01:15:55,600 --> 01:15:59,200
First of all, the impulses is a markov chip. Okay.

579
01:15:59,650 --> 01:16:03,160
So this is a temporal process follows a markov process.

580
01:16:03,450 --> 01:16:10,220
Okay. This is first condition. The second condition is very, very important condition is the conditional independence.

581
01:16:10,960 --> 01:16:17,290
So if you covered this, if you covered this, not this square, then why?

582
01:16:17,290 --> 01:16:23,829
It is completely disconnected for the entire structure of the upper part of this structure.

583
01:16:23,830 --> 01:16:34,729
Right. All the. Of y t minus along with the plus one or C the T minus one.

584
01:16:34,730 --> 01:16:38,390
C the people wouldn't say that she's given. The.

585
01:16:41,010 --> 01:16:44,280
It's essential. That's why need all the preparation.

586
01:16:44,280 --> 01:16:49,470
You can see it. Many places. I tried to bring in some condition of independence.

587
01:16:49,800 --> 01:16:53,820
Conditionally uncorrelated in the derivation of the blocks.

588
01:16:53,880 --> 01:17:00,150
Right. So that's exactly the place where you want to use that property into this calculation.

589
01:17:00,360 --> 01:17:07,010
BLOCK Population. So when theta t is fixed, we'll see that is covered or condition.

590
01:17:07,020 --> 01:17:12,270
I'll see that light is independent of all the other variables in this system.

591
01:17:12,550 --> 01:17:15,600
Okay. So that's two properties.

592
01:17:19,040 --> 01:17:25,579
According to this rest of the whiteys, this is condition of independence in this state.

593
01:17:25,580 --> 01:17:30,800
To be small, you have to have that in order to have very fast calculation of that.

594
01:17:31,070 --> 01:17:34,880
Okay. So this is actually acceptable situation, right?

595
01:17:34,940 --> 01:17:44,040
So you believe that the current caseload of infection that will be really related to current sort of the infection status?

596
01:17:44,060 --> 01:17:50,840
If this is fixed, then why it should be unrelated to other of course you believe.

597
01:17:52,940 --> 01:18:00,120
This could be something more complicated. So say that he is first order marked for process.

598
01:18:00,430 --> 01:18:02,310
Okay. This is also a assumption.

599
01:18:02,700 --> 01:18:16,740
But they say while this sounds like a very restricted sort of still this is the assumption which basically is not really that restricted.

600
01:18:17,940 --> 01:18:21,630
I will tell you why. That's later on.

601
01:18:21,750 --> 01:18:26,310
That's full time. He just assumed that that he is first order for process.

602
01:18:27,030 --> 01:18:32,220
So basically these see that he only.

603
01:18:33,850 --> 01:18:37,000
They're not the only people who say that he might have one, right?

604
01:18:37,870 --> 01:18:44,920
It doesn't say that he wants to. Well, this is, first of all, a remarkable process.

605
01:18:45,310 --> 01:18:49,210
Basically, the variable today only depends on variable.

606
01:18:49,630 --> 01:18:55,880
It doesn't depend on any variable order from yesterday.

607
01:18:55,900 --> 01:18:58,960
So that's the first of all remark or process.

608
01:18:58,990 --> 01:19:02,740
I will say that this actually is not a very restricted condition.

609
01:19:02,770 --> 01:19:11,500
I will tell you why later on. Okay. And here I do not make any special distributions option for this process.

610
01:19:11,920 --> 01:19:19,360
I only tell you, first of all, that uncorrelated this is a condition based on the second moment covariance matrix.

611
01:19:19,840 --> 01:19:24,850
The first order Markov process is basically a property of dependance.

612
01:19:25,060 --> 01:19:30,020
There is no specific case of any distributions in the states.

613
01:19:30,040 --> 01:19:33,880
Be small. Okay.

614
01:19:34,960 --> 01:19:41,900
Move forward about this space based model. Um.

615
01:19:43,030 --> 01:19:53,040
This is actually more. We work on a common future smoother according to the following structure.

616
01:19:53,430 --> 01:19:56,700
First of all, the conditional mean. Okay.

617
01:19:57,060 --> 01:20:01,830
So basically we're talking about how wide he's generated.

618
01:20:03,300 --> 01:20:12,870
We're working on this. We are working out this vertical thing.

619
01:20:13,170 --> 01:20:16,430
How? See that he generates black tea.

620
01:20:16,740 --> 01:20:23,340
That that's something I need to make a model to explain that data generation mechanism.

621
01:20:23,730 --> 01:20:30,260
So here is the model. Okay. So again, I'm not satisfied specifying any distributions.

622
01:20:30,270 --> 01:20:33,899
I'm just working on moments. Okay, first moment. What?

623
01:20:33,900 --> 01:20:42,030
How this c that the general white T at this first moment level is a function of see

624
01:20:42,030 --> 01:20:51,450
that T and H is the the matrix right or y depends on a light t if there's a skull,

625
01:20:51,460 --> 01:20:57,710
a one dimensional random variable, then this will be a just a vector, you know.

626
01:20:57,730 --> 01:21:05,520
But anyway, h could be a matrix. It could be a vector, could be a, you know, just a scholar.

627
01:21:05,760 --> 01:21:12,360
If you if you think about see that T and white t are both one dimensional plus intersect.

628
01:21:12,510 --> 01:21:16,140
Right. This is more like a mean or more. And how about appearance?

629
01:21:17,400 --> 01:21:28,140
So there's a sycamore. So how? The variability in the data generation looks like it basically could be announcing their function.

630
01:21:29,740 --> 01:21:35,730
W t could be any nonlinear function of time and it's not necessary either.

631
01:21:35,800 --> 01:21:42,940
And there's a intercept there. But W ts theoretically could be a non meaningful.

632
01:21:43,630 --> 01:21:47,170
So this how this friend generated.

633
01:21:52,740 --> 01:21:56,160
Oh, this. I see that he might as one move into.

634
01:21:58,740 --> 01:22:02,850
So the first order market process is shown to be like this.

635
01:22:03,330 --> 01:22:06,640
The conditional expectation of Sidoti. Okay.

636
01:22:07,770 --> 01:22:09,660
Given the historical data,

637
01:22:10,320 --> 01:22:17,330
because what we consider first order market processed only depends on see that he minus one is also another function of state achievements.

638
01:22:18,600 --> 01:22:22,550
Yeah. Like this.

639
01:22:23,360 --> 01:22:30,020
Okay, so here we show them clearly the first moment is near, but second moment may not be here.

640
01:22:30,710 --> 01:22:33,770
And in the second moment of all, stop AT&T.

641
01:22:36,700 --> 01:22:47,330
And two wise and the consent terms wt0 and double tdt0 are constant terms doesn't depend on c t.

642
01:22:48,200 --> 01:22:58,260
So also we calculate this expectation of this two matrices denoted by w t bar, bar and.

643
01:22:59,360 --> 01:23:09,490
Okay. Well, now, what I want to do here is really something I already explained to you.

644
01:23:10,150 --> 01:23:15,120
Use white. Instead of first two factors basic.

645
01:23:15,120 --> 01:23:18,730
This is historical data I collect up to time.

646
01:23:19,830 --> 01:23:32,940
So this is first position I added which I received the data this the position on time t when I received my data, I have a t data points to keep this.

647
01:23:32,940 --> 01:23:36,329
This could be a large vacuum. Could be one solid.

648
01:23:36,330 --> 01:23:39,390
It could be one. It depends on what time you're looking at.

649
01:23:39,690 --> 01:23:43,079
But anyway, you have a collection of historical data up to time.

650
01:23:43,080 --> 01:23:48,120
T use this okay. This to t know that big factor.

651
01:23:49,470 --> 01:23:59,340
So what is common filter consider is define up best predictor of C the T given the source of data.

652
01:24:00,930 --> 01:24:09,450
So the engineers want to do is try to find it predict the current position using the historical data.

653
01:24:10,530 --> 01:24:21,630
Okay and the commerce moves are is the block of city given all the additional data so not only what kind of historic.

654
01:24:24,780 --> 01:24:29,820
That I want to predict. Well, in that case is called common snoozer.

655
01:24:30,990 --> 01:24:34,740
Okay. So the innovation fold is coming.

656
01:24:34,740 --> 01:24:44,850
Fields are coming smoother. Listen to this recursive procedure for recursive recursion, which basically makes this very fast.

657
01:24:44,850 --> 01:24:51,750
And I believe common future income will be the first version of online learning online,

658
01:24:51,960 --> 01:25:04,380
because it basically says that your prediction for Cedar Key, whatever you have done, you don't need to reprocess the entire data anymore.

659
01:25:04,380 --> 01:25:10,180
You just use whatever, predict. How to predict the next one.

660
01:25:11,660 --> 01:25:21,420
This is all the original data. So. This is essentially a online idea.

661
01:25:21,660 --> 01:25:34,410
So you just use whatever or they predict predictive us to predict their position next time rather than processing entire data from beginning.

662
01:25:34,860 --> 01:25:42,480
So this is very brain idea for their sort of prediction and very fast.

663
01:25:43,830 --> 01:25:52,469
So I don't have time to talk more, but I we show you how this recursive formula looks like and then we would do the

664
01:25:52,470 --> 01:25:57,570
infectious disease and make a prediction of the infection status using this system.

665
01:25:57,760 --> 01:25:57,940
Okay.

