1
00:04:32,000 --> 00:04:49,000
Okay, Good morning. Everyone let's get started we first we'll go over the all on quiz you just turning today. I have to say it is a pretty tricky challenging quiz and some of the pleasure might be controversial so let's pick some time

2
00:04:49,000 --> 00:04:55,000
to go over them

3
00:04:55,000 --> 00:05:08,000
First question is talking about statement of regarding key value. and then no, we trying to think is this actually up sort of a multiple choice.

4
00:05:08,000 --> 00:05:14,000
So that make you hard so let's go over one by one

5
00:05:14,000 --> 00:05:20,000
So one a pivotal point 5, the scientific importance of study.

6
00:05:20,000 --> 00:05:32,000
Are there any bottom here trying to? take on this one

7
00:05:32,000 --> 00:05:39,000
I just said true. because I remember it being in your notes.

8
00:05:39,000 --> 00:05:47,000
Okay, Any. Thank you. Thank you. Any Any other comments from other people?

9
00:05:47,000 --> 00:06:04,000
Yeah, I think I feel it's forced because of we withdraw from statistics. So it quantifies the statistic importance of a study rather than the scientific importance.

10
00:06:04,000 --> 00:06:10,000
Yeah, thank you both for answer. Really appreciate that I think you know let's take it slow.

11
00:06:10,000 --> 00:06:17,000
Let's go to the lectures review a little bit

12
00:06:17,000 --> 00:06:32,000
Right here.

13
00:06:32,000 --> 00:06:36,000
So the statistical second is imply scientific on public health importance.

14
00:06:36,000 --> 00:06:41,000
Yeah. answer is no. So this is gonna be very distinct from practical clinical importance.

15
00:06:41,000 --> 00:06:58,000
So that goes back to this question right? the P. by It is not the same thing as a scientific importance on the scientific importance is quantify by You could say that in fact, of the quality being measured or the signal no matter

16
00:06:58,000 --> 00:07:03,000
how to call it that's not the p by the pivot only measures this.

17
00:07:03,000 --> 00:07:09,000
The this is the which is the likelihood of study, will hold against repeated experiment.

18
00:07:09,000 --> 00:07:18,000
So the answer here is no yeah that's that's Why, we put a you know, exercise here, because you know, some of them might be quite confusing or challenging.

19
00:07:18,000 --> 00:07:22,000
But that's the whole point we are learning this and the second one.

20
00:07:22,000 --> 00:07:44,000
The P. value is a probability that the No have had this true anyone

21
00:07:44,000 --> 00:07:55,000
Is it false? cause? you you never say if the null is true, you say, you fail to reject it, or you reject it.

22
00:07:55,000 --> 00:08:02,000
With the P. value data that you have. Thank you.

23
00:08:02,000 --> 00:08:20,000
Yeah, you're you're absolutely right, you know we I I think the a better argument here is is that the hypothesis is either true, or not true, although we don't know it by any thought it is not random

24
00:08:20,000 --> 00:08:28,000
though the hypothesis is not random, we can only talk about probability with random stuff, and the only thing that is random here is our data.

25
00:08:28,000 --> 00:08:32,000
So the pivot is exactly the probability of our data.

26
00:08:32,000 --> 00:08:37,000
Bye observing something as a extreme or more extreme, not a while with zoom in the data.

27
00:08:37,000 --> 00:08:46,000
Assuming that now is true. So it's not about the law So let's go back here right okay.

28
00:08:46,000 --> 00:08:50,000
The pivot is the probability that h naught is true that's no right.

29
00:08:50,000 --> 00:08:55,000
We can only talk about probability with observe the data because the data is random.

30
00:08:55,000 --> 00:08:58,000
So that's going back to this policy therefore it is false.

31
00:08:58,000 --> 00:09:04,000
Thank you, Barbara, for answering. Yeah. if you do have questions, please let me know right.

32
00:09:04,000 --> 00:09:10,000
This is the chance we review our concepts. You like I.

33
00:09:10,000 --> 00:09:14,000
I always had a paid between, whether I should tell you more, or I should just make it simple.

34
00:09:14,000 --> 00:09:18,000
But I make it simple, probably easy, if I understand but there's always comication.

35
00:09:18,000 --> 00:09:23,000
For example, everything we talk about in this class, and you, Many of the class we we learn here are so called.

36
00:09:23,000 --> 00:09:29,000
The frequencies approach, and within that framework everything I I told you is true.

37
00:09:29,000 --> 00:09:33,000
But there's a parallel framework called the bayesian framework in the basin framework.

38
00:09:33,000 --> 00:09:39,000
People really put probability on hypothesis. So once you go outside this framework philosophically.

39
00:09:39,000 --> 00:09:49,000
This told it a different argument there. so so so that's why i'm struggling what I should mention things all there, cause you know, sometimes you might run into another sanitation.

40
00:09:49,000 --> 00:09:56,000
They say, Oh, we people probably look about this they were crazy and there's a frequency versus baying argument for for that case, right?

41
00:09:56,000 --> 00:09:59,000
There's there's this is never gonna happen but within it's frequent. this framework.

42
00:09:59,000 --> 00:10:05,000
You can only assign property to random measurements which is data.

43
00:10:05,000 --> 00:10:10,000
So I hope that's a that's efficient yeah for the purpose?

44
00:10:10,000 --> 00:10:22,000
Is that the wanna you know, count out too much out of the the realm of this course I'm third i'll keep out is a probability that may are that my or more conclusive results would occur when the law and how is

45
00:10:22,000 --> 00:10:39,000
true. anyone will take on. Take on this one

46
00:10:39,000 --> 00:10:45,000
I think this one is correct. the way state is that Yeah.

47
00:10:45,000 --> 00:10:50,000
Yeah, and you measure like an effect and you measuring the problem.

48
00:10:50,000 --> 00:10:59,000
To get these results, or more extreme results.

49
00:10:59,000 --> 00:11:04,000
Given the, or assuming that the null hypothesis is true or not.

50
00:11:04,000 --> 00:11:07,000
Thank you very much. You're absolutely correct this is the definition of key value, right?

51
00:11:07,000 --> 00:11:13,000
Although you can change a little bit of wording but essentially this is that an Api value it's the poverty of observer.

52
00:11:13,000 --> 00:11:20,000
The data under the all those women are always true. So this is correct, and the last one, the Peabody 0 point, 0 6 means my result.

53
00:11:20,000 --> 00:11:27,000
Have 6% chance of being published. Oh, this is like a Joel Crap. right?

54
00:11:27,000 --> 00:11:34,000
So is the probability of the data. Now, the probably anything else, although you know in in the reality I have place there.

55
00:11:34,000 --> 00:11:38,000
There's some degree of you know truth in in real world right?

56
00:11:38,000 --> 00:11:42,000
Miko paper more likely to be published.

57
00:11:42,000 --> 00:11:49,000
But by no means that's the definition of key value Okay, So so it turns out only the third wise the crack answer.

58
00:11:49,000 --> 00:11:55,000
Although this is a multiple choice that might, you know, make make it more harder than than it should have been.

59
00:11:55,000 --> 00:12:00,000
But nevertheless, actually, most of you got it why so i'm very glad to see that.

60
00:12:00,000 --> 00:12:07,000
Okay, question Number 2. You are told that the population, the Us.

61
00:12:07,000 --> 00:12:14,000
Man has have an average way to £195 cool assass the truth of this statement.

62
00:12:14,000 --> 00:12:25,000
You measure someone 100 us men which could do the sample mean of £180. And this is a you have to look at very carefully the wording right with a corresponding standard error of the mean equals 5

63
00:12:25,000 --> 00:12:28,000
100. So this is not a corresponding standard deviation.

64
00:12:28,000 --> 00:12:31,000
This is an arrow to me, so they are told you.

65
00:12:31,000 --> 00:12:39,000
The standard arrow. Let me think about what that means. So the following pivot given, which would be the closer pivotal tapping, whether or not your account will come from a population.

66
00:12:39,000 --> 00:12:42,000
So these are the one sample t types where the null value is this.

67
00:12:42,000 --> 00:12:45,000
And then I give you data, ask you what the pivotal would be.

68
00:12:45,000 --> 00:12:52,000
You could do a sort of a back of envelope calculation, or, to use the spreadsheet we we we use.

69
00:12:52,000 --> 00:12:55,000
The goal is not together exactly value. The exact key value is not.

70
00:12:55,000 --> 00:13:00,000
Is none of this right? The goal is for you to ask me what the people

71
00:13:00,000 --> 00:13:30,000
So anyone want to take on this one

72
00:13:38,000 --> 00:13:59,000
Sure I could do it So So yesterday we learned the the T statistic equation, and so, jeez I remember the the exact T statistic number that I got and but I remember it was

73
00:13:59,000 --> 00:14:13,000
statistically significant. So the only one here that corresponds the same here value A P would be P equals at the third 1 0 point 0 one. Thank you.

74
00:14:13,000 --> 00:14:16,000
Thank you. but I have to point out. both of these are point 0.

75
00:14:16,000 --> 00:14:22,000
2 5 is also significant, and 0 point 0 5 is on the marginal being significant.

76
00:14:22,000 --> 00:14:28,000
You're, you're not not enough of value.

77
00:14:28,000 --> 00:14:40,000
Yeah, I heard I I I think the the t value was like it's negative 3. I I wanna say, I don't remember the Zack, but it was very, very, it was definitely not within the negative 2 and 2.

78
00:14:40,000 --> 00:14:44,000
Range. Okay, Yeah, that's very helpful thank you yeah. i'll go over. hope.

79
00:14:44,000 --> 00:14:54,000
I think you're absolutely right right so the key statistic is the it's a sample mean minus the reference value of the meal under the null divided by the standard error.

80
00:14:54,000 --> 00:15:01,000
So the crucial thing crucial thing here is is I don't know a number to use right so 180 is your sample mean X bar.

81
00:15:01,000 --> 00:15:03,000
You have to subtract 195, which is the new, you not?

82
00:15:03,000 --> 00:15:09,000
Now you got a negative 15, and the the denominator should be the standard error.

83
00:15:09,000 --> 00:15:15,000
So in the in the formula for the key statistics, the standard arrow is standard deviation, divided by square root of add.

84
00:15:15,000 --> 00:15:25,000
However, you have to be careful here, I already give you the standard arrow I'm. not giving the standard deviation, so you should directly take the file instead of divide divided by square root of 100 that's.

85
00:15:25,000 --> 00:15:29,000
Not necessary, because this is already calculated. So you what you got the piece?

86
00:15:29,000 --> 00:15:33,000
That is it would be negative 15 divided by 5 that's 93.

87
00:15:33,000 --> 00:15:39,000
So you're absolutely right negative 3 so now is the is the time to think about what negative 3 means right?

88
00:15:39,000 --> 00:15:47,000
So basically, you think about under the standard normal curve and then things go beyond negative 3, and the positive 3 that's not left.

89
00:15:47,000 --> 00:15:52,000
Not 1% of the population mass. Basically, you know the area.

90
00:15:52,000 --> 00:15:56,000
Another normal car between 3 standard deviations, almost 100%.

91
00:15:56,000 --> 00:16:00,000
So outside is less than 1%. So the pivot is actually last time. point.

92
00:16:00,000 --> 00:16:05,000
Oh one, so that's why the point of why is the closest? answer.

93
00:16:05,000 --> 00:16:09,000
The the 2 give out is actually a point all o 3, or something like that.

94
00:16:09,000 --> 00:16:12,000
But nevertheless, here I just want you to get a rough guess.

95
00:16:12,000 --> 00:16:15,000
So this one is the correct answer. thank you for answering.

96
00:16:15,000 --> 00:16:20,000
And majority of you actually got it this live for the rest of you you know.

97
00:16:20,000 --> 00:16:26,000
Please make sure you You you understand what we just did so so.

98
00:16:26,000 --> 00:16:38,000
No, you know how to solve this problem. Any question

99
00:16:38,000 --> 00:16:45,000
Yeah, I appreciate any volunteer to answer questions. you might Get it wrong in my head. Right? That's not the point right?

100
00:16:45,000 --> 00:17:00,000
The point is that you try it, and the you know the cost. cannot really proceed well without volunteers, and you know it will be very boring if i'm the only one talking about 3 h Thanks. so much. question 3 I I think questions 3

101
00:17:00,000 --> 00:17:05,000
is really tricky it's even controversial so i'll take some time and go over it hopefully.

102
00:17:05,000 --> 00:17:08,000
I'll give you some satisfactory explanation.

103
00:17:08,000 --> 00:17:16,000
But I I will, you know I welcome different opinion and I I respect if i'm kidding, Okay, sometimes things are a little more philosophical.

104
00:17:16,000 --> 00:17:23,000
So question 3. right. After analyzing my data, I produce a pivotal P equals 0 point 0 5 1.

105
00:17:23,000 --> 00:17:29,000
All that in some sense it's on a function that right if you're hoping something left on point.

106
00:17:29,000 --> 00:17:35,000
Oh, fine, but not in the last. This happens right. so give it this result right?

107
00:17:35,000 --> 00:17:42,000
Which of the following is true. I think this is very practical, because in practice you might actually run into this kind of situation right? it's good to know what to do.

108
00:17:42,000 --> 00:17:45,000
And this question I think it is a very excellent question.

109
00:17:45,000 --> 00:17:48,000
Put out by my my. My colleague would part of this course right?

110
00:17:48,000 --> 00:17:51,000
Who prepare the material. I I think he had a very good point.

111
00:17:51,000 --> 00:17:56,000
Making this question. Nevertheless, this is a kind of tricky and even controversial question.

112
00:17:56,000 --> 00:18:03,000
So i'll go with them so again. this is this is a you know, single choice.

113
00:18:03,000 --> 00:18:10,000
Nevertheless, I kind of tricky so i'll go over 1, one by one, number one, since my key value is greater than 0 point 0 5.

114
00:18:10,000 --> 00:18:16,000
I have proved now is true. another one. welcome up. Take on this one.

115
00:18:16,000 --> 00:18:30,000
Do you think it's right or wrong? and why well I can go with this one, so we we can never use p value to prove to get a found shoes?

116
00:18:30,000 --> 00:18:35,000
Yeah. So So at best, we can that's the poster all refuel kids.

117
00:18:35,000 --> 00:18:40,000
So the proof that now hypothesis is true is a boundary.

118
00:18:40,000 --> 00:18:45,000
So it's not 5 here, thank you you're absolutely right.

119
00:18:45,000 --> 00:18:49,000
So. so I think there are 2 faults right one is that we only rejected.

120
00:18:49,000 --> 00:18:57,000
Now we don't group to know the now is assumed right we only, you know, with enough evidence rejected now, or without enough evidence to return.

121
00:18:57,000 --> 00:19:04,000
Now we do not already pro for now. so this is 1 s is that we never really proved anything right, because we just make probabilistic argument.

122
00:19:04,000 --> 00:19:12,000
But we think there is not evidence, but we still could be wrong it's not a proof I would say right.

123
00:19:12,000 --> 00:19:17,000
We think the now should be wrong, because of which one which but we never know for sure.

124
00:19:17,000 --> 00:19:21,000
So there. There are several layer of argument, but nevertheless, this as well. Thank you so much.

125
00:19:21,000 --> 00:19:25,000
And the second one, since my key value is close to point 0 5.

126
00:19:25,000 --> 00:19:29,000
I should collect more data and then allies the data again.

127
00:19:29,000 --> 00:19:35,000
Okay, but there's a smaller key value so I find this one to be very tricky.

128
00:19:35,000 --> 00:19:40,000
So, anyway. I won't pay on this I I welcome any comment. you know. don't don't be afraid or wrong.

129
00:19:40,000 --> 00:19:47,000
I I think it's more important we have a thought process about I

130
00:19:47,000 --> 00:19:54,000
I originally picked this one of my first attempt cause it It just like sound incorrect, cause like

131
00:19:54,000 --> 00:20:09,000
It's always better to call it more tada but the more I thought about it at least a reasoning for my second and tenth widened pick is other than it's wrong like the data you might

132
00:20:09,000 --> 00:20:17,000
get might not exactly make it statistically significant, like the data you collect.

133
00:20:17,000 --> 00:20:25,000
My actually be an outlier make it less statistically significant.

134
00:20:25,000 --> 00:20:30,000
So. Alice, I was my understanding of why it was wrong.

135
00:20:30,000 --> 00:20:34,000
The first time a out of a way. Thank you. Thank you very much.

136
00:20:34,000 --> 00:20:40,000
Yeah, I mean, you are right. that clinging more data, I actually might give you a larger queue battery and stuff that could happen.

137
00:20:40,000 --> 00:20:44,000
Anything could happen. but there are. There are more through this argument.

138
00:20:44,000 --> 00:20:55,000
I like to hear from other people. Maybe anyone that's different already. So after I answered, the some came to the right answer.

139
00:20:55,000 --> 00:21:07,000
I looked back at the question, and I had overlooked the wording which one of the following is the best conclusion to reach, and I thought it easier to work with the data set that you have.

140
00:21:07,000 --> 00:21:11,000
Possibly, you know, given the right answer to work with that rather than a team.

141
00:21:11,000 --> 00:21:19,000
More data, one that one, the analysis and system Again: Thank you.

142
00:21:19,000 --> 00:21:23,000
Yeah, I I think. thank you both. I think you both make very good argument.

143
00:21:23,000 --> 00:21:33,000
I think it's a little not fair to you because I I never really talk about this you know in class.

144
00:21:33,000 --> 00:21:37,000
Actually, you know, The main point here, you know, will make this statement wrong.

145
00:21:37,000 --> 00:21:43,000
Is that he, bio is a is a random stock of all your data, right?

146
00:21:43,000 --> 00:21:47,000
So if you clap data and you analyze your data, you calculate P.

147
00:21:47,000 --> 00:21:50,000
Value, and look at it, whether it's smaller than 4.5 or not.

148
00:21:50,000 --> 00:21:53,000
That's the hubble passing framework this framework.

149
00:21:53,000 --> 00:22:03,000
The whole point of this framework is to guarantee that type on there is less than 5%, and it works. if you follow the framework, which means that you collect the data, analyze it.

150
00:22:03,000 --> 00:22:08,000
However, you can think about it if you repeat your analysis right, you type another data.

151
00:22:08,000 --> 00:22:22,000
Analyze again that you eventually occupy will be left on top 5%, even if the No is true, because even if the know is true, you have 5% chance of having a few battle less than 5% that's just the nation right if you do

152
00:22:22,000 --> 00:22:29,000
it again. that's you know no longer 5% that's almost like a 10%. if you do it 3 times, that's 15%. right?

153
00:22:29,000 --> 00:22:41,000
So if you do it 100 times it's almost sure you're gonna have some one of them have come up to you on a less than 5%, then that I lost the whole point of controlling the I know it's

154
00:22:41,000 --> 00:22:46,000
not totally fair, because I'm not really mentioned this thing costs actually trying to measure the next lecture.

155
00:22:46,000 --> 00:22:49,000
But this is one of the main thing. People argue is the drawback of pivot.

156
00:22:49,000 --> 00:22:55,000
Yesterday. there's a steel and you Know send me a article about the drawback of key value, and there are many articles like that.

157
00:22:55,000 --> 00:23:01,000
There are still that ask me to discuss it in class actually i'll talk about it right after this I was planning to talk about it actually in the Max vector by.

158
00:23:01,000 --> 00:23:12,000
I think it's more important. so the crucial thing is that if you run a whole passing once, then the pivotal framework works really well, excellently, optimally, perfectly.

159
00:23:12,000 --> 00:23:17,000
If you do it again again again again, problem happens. and this thing is just like that, right?

160
00:23:17,000 --> 00:23:21,000
You you click data, you'll analyze this you know it is not significant.

161
00:23:21,000 --> 00:23:25,000
You'll collect more data analyze again and click more than analyze again.

162
00:23:25,000 --> 00:23:31,000
Eventually you're gonna have some key value left on point of fine. but at that time you'll type on error rate is no longer.

163
00:23:31,000 --> 00:23:38,000
5% is way larger than 5%. Well, that that that new place. i'll have a highway by repeated testing.

164
00:23:38,000 --> 00:23:46,000
So that's the main problem. This approach Okay, I i'm going to talk about this again again again.

165
00:23:46,000 --> 00:23:59,000
So this is this is not a best practice, any any any questions, so far

166
00:23:59,000 --> 00:24:10,000
So basically, you know, repeatedly, analyze your data or clock more data, you know repeatedly twice, until you have a peer give out a less than 5% that defeats the main purpose of using the Peabody in the first

167
00:24:10,000 --> 00:24:17,000
place, so that's that's the best though you know the Major Downside of this.

168
00:24:17,000 --> 00:24:23,000
Yeah. Oh, give you some time to to you know Have this talk to thinking.

169
00:24:23,000 --> 00:24:44,000
Then i'll repeat it again right so that hopefully after this course. you gotta remember that. so the form one I should look for unusual data at could be omitted and hopefully produce a smaller key right I selected this from like for

170
00:24:44,000 --> 00:24:49,000
the first attempt, because I thought that maybe an outlier may may be affecting my p-value. But

171
00:24:49,000 --> 00:24:55,000
But it was wrong, so so what I so so then I selected the the last option.

172
00:24:55,000 --> 00:25:02,000
But I still don't know why I still don't know why the this was incorrect.

173
00:25:02,000 --> 00:25:18,000
Very any anybody else. So the reason that I excluded the third option was just because you don't want to manipulate your data in any way, because if you're manipulating it, then you are inserting bias

174
00:25:18,000 --> 00:25:26,000
into your overall analysis. Yes, thank you both. You know I think this is tricky, because, I can.

175
00:25:26,000 --> 00:25:34,000
I can think about both sides of arguments right? th this whole send? That sounds like manipulative data, which doesn't sounds good right, but, on the other hand, all layers do happen.

176
00:25:34,000 --> 00:25:41,000
So how do we do with them? So I I I guess you all have this kind of a a dog that's why, you know, make this chat strategy.

177
00:25:41,000 --> 00:25:48,000
I'll try to call it. but you know there's always a problem that the more I say the moral I could sometimes make it more complicated.

178
00:25:48,000 --> 00:25:57,000
Okay. So if you think about the P value framework again, the P value framework of the whole point is control type, one error, and that works perfectly.

179
00:25:57,000 --> 00:26:07,000
If you follow the framework perfectly, which means that you need to prescribe you analysis plan, Basically, you need prepared this is the way i'm going to analyze the data after I collect them.

180
00:26:07,000 --> 00:26:11,000
And then this is 0 point 0. 5 is the type of error i'm going to use right.

181
00:26:11,000 --> 00:26:17,000
You have to prescribe your analysis plan, and then stick to it and analyze the data wise.

182
00:26:17,000 --> 00:26:30,000
I'm only once that's the way to protect the hypothesis, and that's exactly what happens, you know, when when Fda is running a trial actually the pharmaceutical company should some in the report particularly before the even clamp data.

183
00:26:30,000 --> 00:26:35,000
saying this is the way we're gonna type data this way we're gonna analyze it right and in the report.

184
00:26:35,000 --> 00:26:38,000
You have to talk about. We expect the data to be this right.

185
00:26:38,000 --> 00:26:40,000
But this is the way we deal with. you know, missing data.

186
00:26:40,000 --> 00:26:47,000
We deal with all our basically, everything should be pretty strong. Once you do that, then have arrows is good.

187
00:26:47,000 --> 00:26:54,000
The problem of this particular sentence here is that once you collect that they like. I don't know if they've got a P.

188
00:26:54,000 --> 00:26:58,000
Vio, which is, they point 0 point 0 6 and you you're not happy about it.

189
00:26:58,000 --> 00:27:02,000
Now you try to change your data, change your analysis. You tried many times.

190
00:27:02,000 --> 00:27:08,000
Then you get a few by the last 1.5. This is just the same as the previous, You know Toys right.

191
00:27:08,000 --> 00:27:15,000
You are either my meeting with the data or change your analysis plan or try a many different time.

192
00:27:15,000 --> 00:27:19,000
And Julia may, you know, find one which is less than 5.

193
00:27:19,000 --> 00:27:23,000
If you keep trying right, you eventually find a way to have a p that that's 1 point of 5.

194
00:27:23,000 --> 00:27:28,000
But that that lost the whole point of how about packing a problem?

195
00:27:28,000 --> 00:27:31,000
Because at that time your type of error is already in place.

196
00:27:31,000 --> 00:27:41,000
I guess some of you might heard about this sense and say if you quarter the data, you know the data will tell you what you want to hear. right?

197
00:27:41,000 --> 00:27:46,000
So that's basically what's saying this right if you keep trying until you've got a P. by the last small amount.

198
00:27:46,000 --> 00:27:52,000
But the at that point you know things are for of a marquee on blur. Well, you actually don't know what you're talking about.

199
00:27:52,000 --> 00:28:00,000
Our way is, so that's about practice, but you might also say what if you know I just got a outlier adding packet?

200
00:28:00,000 --> 00:28:06,000
How should I do with it right that's the that's the gray zone that make it tricky right?

201
00:28:06,000 --> 00:28:16,000
So. So there are different different answer to that so in the most rigorous place where control type of error is is number one priority, like a clinical trial.

202
00:28:16,000 --> 00:28:19,000
Right if you're a statistician you know from a typical company.

203
00:28:19,000 --> 00:28:30,000
You're right up into a trial. and you'll find something weird after you kind of data you need to fire a reporter, Fda saying we find it's weird we want the modified analysis and after you had to approve

204
00:28:30,000 --> 00:28:34,000
that before you can change your now that one so it's very rigor, right?

205
00:28:34,000 --> 00:28:39,000
You have to do it. Now i'm saying changing the plan will not need for the panel, or this much, and so on and so forth. Right?

206
00:28:39,000 --> 00:28:44,000
So that's that's the most rigorous way. but most of us are not running files.

207
00:28:44,000 --> 00:28:50,000
You know everything explore 3 data analysis, most of time. Actually, we do not really prescribe all analysis.

208
00:28:50,000 --> 00:28:59,000
Right. We have some rough idea, but we don't really ride on. We don't really write a like a 50 page report before we collect the data, saying this is the way we're going to deal with it right?

209
00:28:59,000 --> 00:29:03,000
So how do we do with that right? If if there is a obvious object which you know doesn't make sense?

210
00:29:03,000 --> 00:29:12,000
I think we should remove them. i'll keep you mind that not, mind you, Please don't pipe on there.

211
00:29:12,000 --> 00:29:18,000
Okay, So there, there's a fine line between you know you're trying to do a battle analysis versus.

212
00:29:18,000 --> 00:29:20,000
You just want to torture your data until you'll see a P. model.

213
00:29:20,000 --> 00:29:30,000
That's 1.5. So that got your paper published although the the you know the the the action that you're doing both seem like the same on the rash.

214
00:29:30,000 --> 00:29:40,000
Now it's totally different right that's the gray zone there that that make it this tricky So that's what i'm trying to say here.

215
00:29:40,000 --> 00:29:52,000
But I I welcome any questions or comment

216
00:29:52,000 --> 00:29:57,000
So basically in in real practice, unless you are running a clinical file and your replexes.

217
00:29:57,000 --> 00:30:04,000
Typically, this happens right? you have to ask yourself, Are you trying different analysis just to get a small key value?

218
00:30:04,000 --> 00:30:11,000
Oh, are you, you know, very analysis, so that you think is a better analysis for quit another day than you've got alright.

219
00:30:11,000 --> 00:30:19,000
So I have to answer a question at the center is putting here apartment.

220
00:30:19,000 --> 00:30:26,000
You know It's the first case right you are trying to look for a user that can be removed hopefully to produce a smaller key as the go here to get a smaller keyboard.

221
00:30:26,000 --> 00:30:36,000
Then that's wrong, right, but only yourself knows why you do that

222
00:30:36,000 --> 00:30:43,000
Yeah, I think this is something like, you know, when you review a paper where you do anything right? they ask you, Do you have a complex interest? right?

223
00:30:43,000 --> 00:30:55,000
Sometimes clearly you have a common interest sometimes clear. you don't but there's a male zone right say you you know the person, but you don't know them so well. Now, whether you have a common interest size, you ask yourself right if you think you

224
00:30:55,000 --> 00:30:59,000
have the you have right same thing here, if you think you're manipulating the data.

225
00:30:59,000 --> 00:31:06,000
Then you're yeah hold I ask the question here. So I have to say, This is kind of tricky right.

226
00:31:06,000 --> 00:31:10,000
So that make this question both hard and tricky, but also a very excellent question.

227
00:31:10,000 --> 00:31:29,000
I like this question. Okay, Any comments

228
00:31:29,000 --> 00:31:38,000
Yeah. So so Some's the decision they hate this whole practice right because they they won't make the whole the whole process as a robotic.

229
00:31:38,000 --> 00:31:44,000
That's really not possible, but you know when people apply things there's always a degree of subjectiveness.

230
00:31:44,000 --> 00:31:48,000
You know this question and that make like the whole thing.

231
00:31:48,000 --> 00:31:52,000
You know more of a market i'll talk about some argument later on.

232
00:31:52,000 --> 00:31:56,000
So number 4, I should conclude that I have modest evidence that alternative. How is true?

233
00:31:56,000 --> 00:32:05,000
This is really tricky and controversial. Anyone will take on this

234
00:32:05,000 --> 00:32:20,000
I can go. So did this so like I I selected this for my second attempt, and I thought that maybe if, for example, if with me value is 0 point 0.

235
00:32:20,000 --> 00:32:24,000
7. So that means that the alpha I mean the the the type.

236
00:32:24,000 --> 00:32:38,000
One error. the amount of like one error is increased but that doesn't but that doesn't mean that all that we don't have that our alternate hypothesis is as you know as far as false so that's why I

237
00:32:38,000 --> 00:32:46,000
selected a piece equal to 0 point 0 5 like the last one, because this just means that our that the alpha may be e.

238
00:32:46,000 --> 00:32:51,000
This just means that, we have evidence that the alternative hypothesis is true.

239
00:32:51,000 --> 00:32:56,000
If the if like, if, for example, for p value is 0 point 0 6.

240
00:32:56,000 --> 00:32:59,000
So. if that was the case, then you would have seen listed this right?

241
00:32:59,000 --> 00:33:05,000
We would have thought that the alternative that we had enough evidence to say that our alternative hypothesis is true.

242
00:33:05,000 --> 00:33:14,000
But then, in that case our alpha would have been a little bit more. So that's that, was my rational Yeah, I like it.

243
00:33:14,000 --> 00:33:17,000
I like your argument, so i'll say a little bit I I I agree.

244
00:33:17,000 --> 00:33:22,000
If you feel like this one is you know, is this wrong?

245
00:33:22,000 --> 00:33:35,000
I totally understand you So I i'll basically talk about the rationale that my product for this question here So when we got to keep out of 0 point 0, 5 one of the part is it's: unfortunate right If you follow the

246
00:33:35,000 --> 00:33:39,000
textbook remarkable. How about happening you should failed to reject them all?

247
00:33:39,000 --> 00:33:48,000
That's what you should do. right. But failed to reject the now, especially if this is your main conclusion of the paper might make your paper a little hard to publish.

248
00:33:48,000 --> 00:33:52,000
I mean that's just the reality like the face that it in principle, right?

249
00:33:52,000 --> 00:33:58,000
The journal should never really look at just a look at key value and decide whether the paper should be accepted or not.

250
00:33:58,000 --> 00:34:04,000
But in practice, right. there are more paper That month will be published than can be published on the Journal, so they have to make a call off.

251
00:34:04,000 --> 00:34:13,000
This is more like, you know. If you are a office, 30 at a university, you wanna miss you this, and then in the end you have to draw a cut off a Gpa. Great!

252
00:34:13,000 --> 00:34:21,000
How to 3.7 You know you're gonna think about the mission below that you don't think about the mission, And then I i'll ask you a question: What about the 2 kids?

253
00:34:21,000 --> 00:34:25,000
Why this Gpa: 3.7, one, the otherwise 3, point 6, 9.

254
00:34:25,000 --> 00:34:29,000
They actually are not i'm up different right just a random flu.

255
00:34:29,000 --> 00:34:34,000
But unfortunately you have to call the line Broadway phone. But same thing here.

256
00:34:34,000 --> 00:34:40,000
So you have to draw the line at 0 point 0 5, therefore 0 point 0, 5, one will fail to reject it.

257
00:34:40,000 --> 00:34:49,000
Now 0 point 0 4, 9 will reject enough right so that's just the conclusion you have to follow up receiver to protect the type.

258
00:34:49,000 --> 00:34:59,000
One error rate. Nevertheless, I think we talk about in class that the P value is a continuous measurement of the amount of evidence.

259
00:34:59,000 --> 00:35:08,000
In that sense. 0 point 0 5, one and 0 point 0 4 9 aren't really that different in terms of amount of evidence you just by chance.

260
00:35:08,000 --> 00:35:13,000
They form both sides of the cut-up, for the amount of evidence embedded in the pivotal like.

261
00:35:13,000 --> 00:35:19,000
This is another thing. If you think 0 point 0 4 9 has more mod moderate evidence.

262
00:35:19,000 --> 00:35:23,000
Okay, and you know, play with the autonomy. Then 0 point 0 5.

263
00:35:23,000 --> 00:35:27,000
One you don't have the same amount of evidence nearly the same amount.

264
00:35:27,000 --> 00:35:32,000
So in terms of in terms of a conclusion is that you pay to reject them.

265
00:35:32,000 --> 00:35:36,000
Now on the amount of avenue against the novel favourite.

266
00:35:36,000 --> 00:35:45,000
It's about the same So I think my colleague put this question here is that you in real life? If you've got a pivotal like this?

267
00:35:45,000 --> 00:35:51,000
What you should do is just to write it up in your paper, saying, I gotta keep out of 0 point 0 5 1.

268
00:35:51,000 --> 00:35:55,000
I think that's decent amount of evidence right although I failed Rejected?

269
00:35:55,000 --> 00:36:02,000
Not. But I think that's the amount of evidence that's my paper is that you go back, You know, re-analize your data, kick calls on you.

270
00:36:02,000 --> 00:36:13,000
Think that all I is trying to get is down to 0 point 0, 4, 9, and then write a paper that's a bad practice, although you know you can understand it.

271
00:36:13,000 --> 00:36:21,000
I mean in reality right When people really do the bad practice they wouldn't say what they did, they just say Oh, this is our data, and we did this.

272
00:36:21,000 --> 00:36:26,000
We got 0 point 0 4 9. They never really say we actually got 0 point 0 5 one, you know, in the beginning.

273
00:36:26,000 --> 00:36:30,000
Then we did a lot of things, and finally go down to 0 point 0 5.

274
00:36:30,000 --> 00:36:39,000
They don't buy that and that's not honest right So the question here is to tell you when you really got this kind of key value.

275
00:36:39,000 --> 00:36:44,000
The honest, and the right approach is just right. right it as it say we got this key value.

276
00:36:44,000 --> 00:36:52,000
We think there's decent amount of evidence and that's our paper, and that's the crack. Answer just a quick question.

277
00:36:52,000 --> 00:37:04,000
In terms of I know we're using it point 0 5 is like the absolute cutoff. but I mean, statistically speaking, you can use a 90% confidence interval, and that still gives you information on the data and So that's

278
00:37:04,000 --> 00:37:08,000
where of how I thought about it as even though you're just about 0 point 5.

279
00:37:08,000 --> 00:37:13,000
It's not that you can't say anything about your data it's that you can't say it to that degree of confidence.

280
00:37:13,000 --> 00:37:17,000
So you can say it with like a lower degree of confidence of what your data is saying.

281
00:37:17,000 --> 00:37:26,000
Appropriate way to think about it yeah you're absolutely right of 0 point 0 5 was arbitrary to begin with, right with us.

282
00:37:26,000 --> 00:37:31,000
If, say we, we use a 0 point, one cut off, then this will be rejected now.

283
00:37:31,000 --> 00:37:43,000
But the whole point is that this whole approach will work. Oh, maybe if you put a 0 point 0 5 out of there before you'll see your data right if you decide on the call of after your key.

284
00:37:43,000 --> 00:37:51,000
Value. Then you can always reach out to now, right? So usually before you see our data, people put a 0 point 0 5.

285
00:37:51,000 --> 00:37:54,000
There Now you'll see a 0 point 0. 5 one that's too late to change your 0 point.

286
00:37:54,000 --> 00:38:05,000
0 5. You cannot change it. Happy, you'll see the pile so so that wouldn't help you unless you prescribe right? You say, Okay, I you know I I decided to use 0 point 0 one to begin with you saw I

287
00:38:05,000 --> 00:38:10,000
did I before I see there. Then people trust you. otherwise no one trust you because they might think you.

288
00:38:10,000 --> 00:38:13,000
You know you change the caught up later on so that's a very good point.

289
00:38:13,000 --> 00:38:20,000
But that's not the argument here. so so in Yeah, you just report a key value, and you'll write it off.

290
00:38:20,000 --> 00:38:23,000
So actually, there are. There are a lot more to do to this right.

291
00:38:23,000 --> 00:38:34,000
Some people, some even statistician They think you shouldn't even you shouldn't even report a key by because you know the 0 point 0 5 is failed to reject the number of data rejected not period that They should just say

292
00:38:34,000 --> 00:38:38,000
say, you pay the rejection Now that some people don't agree with that.

293
00:38:38,000 --> 00:38:42,000
Even within the school of sanitation. They have different opinion so I don't want to go to Dp.

294
00:38:42,000 --> 00:38:49,000
Do now, Right So if your only goal is to control type on error, then you should actually just reject.

295
00:38:49,000 --> 00:38:52,000
I don't think you might report a pivotal that's it right.

296
00:38:52,000 --> 00:38:57,000
But a lot of times people I mean they're only going not just rejected 900, right?

297
00:38:57,000 --> 00:39:01,000
You do about things more than that. So I think the keyword is a piece of evidence showing things.

298
00:39:01,000 --> 00:39:05,000
So I think it's helpful I like the conclusion here.

299
00:39:05,000 --> 00:39:08,000
Just go ahead, write it off right? so you still fail to return.

300
00:39:08,000 --> 00:39:15,000
Now period. Well, you can still say this is a you know decent piece of of evidence which make the paper interesting.

301
00:39:15,000 --> 00:39:22,000
I think that's good enough and that's my personal pay but at this moment there's there's some great degree of subjectiveness.

302
00:39:22,000 --> 00:39:27,000
There. So I I like this question not dislike it because you know some of you may not be happy.

303
00:39:27,000 --> 00:39:32,000
You know what your answer is, but I have to say what score you got out of this.

304
00:39:32,000 --> 00:39:37,000
Is not that important? and many of you are not taking him and taking more credit even for those who take a credit.

305
00:39:37,000 --> 00:39:42,000
You feel like a 100 question here You know just got one wrong wouldn't really change your final grade pretty much at all.

306
00:39:42,000 --> 00:39:59,000
I think the key is here to keep some thinking any, any path

307
00:39:59,000 --> 00:40:09,000
Okay, moving on to question 4 before collecting a sample of data, you decide to use a pivot cut out of 0 point or one.

308
00:40:09,000 --> 00:40:14,000
This is what I talk about. If you want to use something different from 0 point 0 5 totally fine.

309
00:40:14,000 --> 00:40:20,000
But you need to do that before you see your data. and then, instead of 0 point 0 5, right?

310
00:40:20,000 --> 00:40:26,000
Which of the following would be an impact of the decision, My truth, every response to the multiple choice one.

311
00:40:26,000 --> 00:40:31,000
You are less likely to make type on arrow anybody quickly.

312
00:40:31,000 --> 00:40:40,000
This should be very straightforward.

313
00:40:40,000 --> 00:40:47,000
That's true that's the definition Thank you Yeah, exactly This is the definition of the type of error.

314
00:40:47,000 --> 00:40:51,000
You're going to get if you follow the procedure so using port 0 point 0 1.

315
00:40:51,000 --> 00:40:55,000
You've got to get a type of error no no one of 0 point 0.

316
00:40:55,000 --> 00:41:01,000
1 s, your results are less likely to be published. Well, I I will take on this one.

317
00:41:01,000 --> 00:41:06,000
This may be true, but this is not a point right I I think when we do this we don't.

318
00:41:06,000 --> 00:41:12,000
We don't do our analysis just to make it you know more likely to be published, because that's going to more, you know.

319
00:41:12,000 --> 00:41:25,000
Not so ideal, behavior right? don't Think about that right? So so this is your relevant third in general usual, consider collecting a larger sample, and that is always correct, right?

320
00:41:25,000 --> 00:41:33,000
Everything else Equal order is notable, always helpful, right, especially if you have a you know higher.

321
00:41:33,000 --> 00:41:39,000
If you have a you know more strange and hyper error rate, which means your power will be lower.

322
00:41:39,000 --> 00:41:47,000
Remember power, go with half a hour rate. In this case you even need a more large sample to to keep your power the same level.

323
00:41:47,000 --> 00:41:51,000
Right. So in any case, you need to have a order sample.

324
00:41:51,000 --> 00:42:03,000
Finally, you are less likely to make type 2 out. Anyone off the pig on the last one

325
00:42:03,000 --> 00:42:14,000
I said that was incorrect cause if you're less likely to make type One error, then you're more likely to make type 2, because they're inversely related. absolutely correct.

326
00:42:14,000 --> 00:42:18,000
Thank you so much. Yeah, Pipeline and type. 2 are go against each other.

327
00:42:18,000 --> 00:42:22,000
You cannot hold both done until you, you know, unless you increase your sample.

328
00:42:22,000 --> 00:42:25,000
Size will do other things right. Everything else equal is to go against each other.

329
00:42:25,000 --> 00:42:30,000
Thank you very much. Any further question or comment regarding this quiz.

330
00:42:30,000 --> 00:42:44,000
I know it is kind of tricky but I I think that's the whole point of doing them any question

331
00:42:44,000 --> 00:42:50,000
Okay.

332
00:42:50,000 --> 00:42:56,000
I I you know I promised the students will send me the paper to talk about briefly in cost.

333
00:42:56,000 --> 00:43:04,000
I will talk about it briefly. Cost

334
00:43:04,000 --> 00:43:12,000
So in the past 10 years or more after more than 10 years.

335
00:43:12,000 --> 00:43:20,000
There's a whole bunch of argument within the scientific and statistical community regarding the use of pivot.

336
00:43:20,000 --> 00:43:24,000
Apparently. This is a one of the paper I didn't read it, you know it's very long.

337
00:43:24,000 --> 00:43:27,000
I just got the paper yesterday. I look at it abstract.

338
00:43:27,000 --> 00:43:30,000
Essentially this paper, you know, talk about the problem with P. value.

339
00:43:30,000 --> 00:43:34,000
And the, you know, argue that some other approach, like companies, level.

340
00:43:34,000 --> 00:43:43,000
Other things going more more sophisticated statistical tools might, you know.

341
00:43:43,000 --> 00:43:46,000
May pop the the the drawback of key value, right?

342
00:43:46,000 --> 00:43:51,000
So there are many paper like this. and they they they!

343
00:43:51,000 --> 00:43:54,000
They argue about it has some problems. they argue, you know.

344
00:43:54,000 --> 00:43:59,000
Some of the some of the journal like that one of the journalists psychology. I think they even ban the pivotal.

345
00:43:59,000 --> 00:44:03,000
They say any papers coming here cannot use people at all that's like fine stream, right?

346
00:44:03,000 --> 00:44:07,000
But there are many, many advocates of a different variation of p value.

347
00:44:07,000 --> 00:44:27,000
If you look at there is a as a statement key value, and you know a Ia is the America's, the Physical Association, and they discussed, you know, with all the statisticians they discussed the the pros on account of using key value and in the end they

348
00:44:27,000 --> 00:44:32,000
give this recommendation. They say you can still use p value. But you need to watch on 1, 2, 3, 4, 5 many things right?

349
00:44:32,000 --> 00:44:43,000
I mean you're very interesting. and look we can read this so i'll i'll talk very briefly about this whole argument, you know, last one decades. so the pio was you know now you're familiar with the whole

350
00:44:43,000 --> 00:44:51,000
framework right? The people out of was proposed back to more than 50 years ago, almost like a 100 years ago, right by by, by, by, by a whole bunch of us.

351
00:44:51,000 --> 00:44:59,000
The decision made it already efficient, Right? The whole point at that time is that yeah, you can claim anything you can find anything, but because your data is random.

352
00:44:59,000 --> 00:45:05,000
How do I know that this finding is not from randomness, rather from some to reality, right?

353
00:45:05,000 --> 00:45:12,000
The whole key value. Primer was put all there Cool guard you from making pipeline arrow.

354
00:45:12,000 --> 00:45:19,000
Basically, you know, if you follow the pivotal framework, you are less likely to make false positive claims.

355
00:45:19,000 --> 00:45:29,000
So from that point of view, it works right I mean it's it's a simple, I mean, I would say it's kind of a twisted argument, because it's a it's a proof by contradiction but

356
00:45:29,000 --> 00:45:33,000
still is a simple argument relatively i'm probably some of the other tools out there nowadays, right?

357
00:45:33,000 --> 00:45:39,000
It's a simple argument that's Why it take off right Kevin is used. Why did he insign today's studies right?

358
00:45:39,000 --> 00:45:45,000
He in biology and clinical automatically all different studies people use provided because it's a simple framework, and it actually works.

359
00:45:45,000 --> 00:45:56,000
Look guard against type on narrowly. So everything seems so fun, However, at the at the beginning of using pivot there there are much less researcher, much less study. Right?

360
00:45:56,000 --> 00:46:06,000
So so that's fine I mean they use it but in in the past few decades there are just so many researchers, so many, paper, all there.

361
00:46:06,000 --> 00:46:10,000
And then there's so much data on it right in the past your design experiment.

362
00:46:10,000 --> 00:46:13,000
You collect data and you analyze the data value you publish.

363
00:46:13,000 --> 00:46:17,000
Everything seems smart. But now, what's happening is that you you design a study.

364
00:46:17,000 --> 00:46:30,000
You clack data, then you run like a 100 different tasks, and some people just pick the one which are significant to to publish, and also because there are some many papers, so many research out there, so many journals right in the end it's

365
00:46:30,000 --> 00:46:37,000
hard to say which, which study is more work well than the other, and the you know.

366
00:46:37,000 --> 00:46:43,000
Somehow some journal hit the easy evolve they look at the pivotal right you've ever staggered finding.

367
00:46:43,000 --> 00:46:49,000
Then your paper is more likely to be published you kind of start this community.

368
00:46:49,000 --> 00:46:56,000
Well, that costs a disaster, you know, for the whole community in the sense that if you look at publications, there's a P.

369
00:46:56,000 --> 00:47:01,000
Value sort of a bias right. Most of the published paper has a key value of that 1 point, 0 5.

370
00:47:01,000 --> 00:47:14,000
And because the publication pressure in academics right now, we we have the pressure to publish something with last pivot. So you know, Key hacking, you know, pay them and information all those packing, because how long?

371
00:47:14,000 --> 00:47:17,000
And once those become a norm, then you look at the the publishers right.

372
00:47:17,000 --> 00:47:24,000
All of them have small pivot, but but they are less likely to be reproduced because they were not reliable.

373
00:47:24,000 --> 00:47:36,000
In the first place. so the problem is out there that you know the many finding cannot be reproduced because they rely on key value to get published. and the people itself is sort of a bias, because this whole approach right so

374
00:47:36,000 --> 00:47:42,000
chicken. That problem is very hard to solve, so the pivot itself mathematically is still correct.

375
00:47:42,000 --> 00:47:46,000
Still a good approach, but being abused by the whole community.

376
00:47:46,000 --> 00:47:51,000
Alright, I will everything go in the video contribute a little bit.

377
00:47:51,000 --> 00:47:55,000
I'll give you the p vio approaches back I wish with so only the way.

378
00:47:55,000 --> 00:48:01,000
So that's why some journal goes to extreme saying you cannot use key via, you know paper if you use Pv.

379
00:48:01,000 --> 00:48:11,000
And we will never consider it right that's why extreme but there are other arguments saying, Yeah, you should use most sophisticated matters, like basic matter, like you know, common in the world like many things.

380
00:48:11,000 --> 00:48:16,000
This paper argues, which by myself, I think, makes sense but but the P.

381
00:48:16,000 --> 00:48:19,000
Value was there because it's simple right use more sophisticated matters.

382
00:48:19,000 --> 00:48:28,000
First of all that make it harder to use and then make it harder to understand. and I personally, I don't really think those men, those matter is not subject to the same problem.

383
00:48:28,000 --> 00:48:35,000
Right. Even you use the common interval, and then you you do a I mean common interval, and keep out of their equivalent to some degree right.

384
00:48:35,000 --> 00:48:46,000
I saw some point. If you do a 100 tests and pick the one to publish, then that bias is not going away just because he was probably the And there are other people in the middle, they say you just need to be careful.

385
00:48:46,000 --> 00:48:50,000
Right if you run many paths right you'll count for that use a smaller cut off.

386
00:48:50,000 --> 00:48:55,000
So you run 10 pass. You need a 0 use 0 point 0 0 5 instead of 0 point 0 5 right?

387
00:48:55,000 --> 00:49:03,000
That's that's another school and also people you know some people argue you should just use 0 point 0 to 5 for every pass, not 0 point 0 5.

388
00:49:03,000 --> 00:49:07,000
So the whole community should down the the you know the the cut out by tenth.

389
00:49:07,000 --> 00:49:10,000
Oh, who caught on the pipe number of pipeline?

390
00:49:10,000 --> 00:49:15,000
So it is a big argument out there and in the foreseeable future.

391
00:49:15,000 --> 00:49:19,000
I don't really see the keyword is going away because it just has being there for that case right?

392
00:49:19,000 --> 00:49:23,000
People are used to it that's Why, i'm still teaching you how to use P.

393
00:49:23,000 --> 00:49:31,000
Buyer, because it's not going away in any time soon. right? because you know all this problem people out of they raise up.

394
00:49:31,000 --> 00:49:38,000
I totally agree, but nobody can agree on a solution. yeah so that's the status right?

395
00:49:38,000 --> 00:49:45,000
That's why i'm gonna talk. about this paper because you know, some of you pointed out. i'll go back to this a little later to show the problem of this right?

396
00:49:45,000 --> 00:49:47,000
But just so, you know this is the current status. right?

397
00:49:47,000 --> 00:49:53,000
Be aware of the problem. pivot who use it you know carefully right.

398
00:49:53,000 --> 00:49:56,000
Don't abuse it on the other hand be aware of the paper.

399
00:49:56,000 --> 00:49:59,000
All there right, just because you'll see up here at the left I'm.

400
00:49:59,000 --> 00:50:13,000
Point to find in a paper that's on mute that not much reliable, because you don't know what's what's happening before they got up like, Okay, Yeah, I know it's brief I want to save some time on this

401
00:50:13,000 --> 00:50:27,000
any questions.

402
00:50:27,000 --> 00:50:33,000
Yeah, this is a good paper to read, you know. If you I will send you a link in here.

403
00:50:33,000 --> 00:50:42,000
In the chat. and then this is also, you know, I think, a very nice statement on Kevin.

404
00:50:42,000 --> 00:50:45,000
If you are interested, take a look right. See what people say about there.

405
00:50:45,000 --> 00:51:07,000
I see different kind of arguments. Okay, for the sake of time I move on to lecture 7

406
00:51:07,000 --> 00:51:18,000
So so far we are done with the fundamental right. We talk about everything of the whole cycle to do analysis, not, we use, the one sample mean as an example to show.

407
00:51:18,000 --> 00:51:21,000
How do we do? conflict, interval, and hypothetical testing?

408
00:51:21,000 --> 00:51:25,000
And you know the one time home in the example is kind of single.

409
00:51:25,000 --> 00:51:31,000
That's why we use as the first example and useful in some sense. Why not?

410
00:51:31,000 --> 00:51:41,000
The most useful thing. So from now we're gonna go into some more specialized 2 for different kind of study design, which are more useful in practice.

411
00:51:41,000 --> 00:51:46,000
Are they not like? They still follow The same principle so underlying the fundamental principle is crucial.

412
00:51:46,000 --> 00:51:50,000
But now we are going deep into different specialized tools, Right?

413
00:51:50,000 --> 00:51:53,000
So that you can really use to analyze your real study.

414
00:51:53,000 --> 00:51:56,000
So the first one is, how about tapping with continuous outcomes in particular?

415
00:51:56,000 --> 00:52:03,000
Here. we talk about tool sample tests. Okay, So the one we just show to Tamil 3.

416
00:52:03,000 --> 00:52:08,000
How about testing was so for the one sample. P. that where are you assume a no value for the population? mean?

417
00:52:08,000 --> 00:52:14,000
I use our data to go against it, but which is good to illustrate the cost of our passing.

418
00:52:14,000 --> 00:52:29,000
But personally I find it. The one simultaneous is I would say not widely used, because it's always hard to come out with a no value right unless you have a good idea how to know value is otherwise it's hard to set

419
00:52:29,000 --> 00:52:35,000
it so is that from now on we talk about It so have 2 sample tasks which is widely used.

420
00:52:35,000 --> 00:52:38,000
You've got to use it. this is the most widely used passing statistic.

421
00:52:38,000 --> 00:52:43,000
You're gonna use pretty much you know everything will study so all the paper we wrap.

422
00:52:43,000 --> 00:52:51,000
Probably did some version of the 2 sample time. So in the in the one sample path, remember, we have a null value which is our No, And then a turning value.

423
00:52:51,000 --> 00:52:56,000
Computer statistic can be the P value and go and go first Right from now on.

424
00:52:56,000 --> 00:53:02,000
We gonna do a different kind of path. Therefore the setup, the null have office, and the alternative will be different.

425
00:53:02,000 --> 00:53:06,000
The statistic will be different, sometimes slightly different, sometimes totally different.

426
00:53:06,000 --> 00:53:10,000
But the whole framework is the same. You still set up a null, and the target have office.

427
00:53:10,000 --> 00:53:15,000
You still compute the statistic, You still got a few hours to make the decision.

428
00:53:15,000 --> 00:53:23,000
So all those pipeline error rate have to every power, all of those calls that they will find just a particular statistic gonna change depend on the type of question.

429
00:53:23,000 --> 00:53:39,000
Okay, I'll, i'll quick. question. because is one sample to sample. the same word for like one sided, 2 sided. what they use in the those are totally different different.

430
00:53:39,000 --> 00:53:45,000
Okay, Okay, not yeah. So one example is that you should have one population.

431
00:53:45,000 --> 00:53:50,000
Collect one sample from one population. right? 2 something. you have to have. 2 different populations.

432
00:53:50,000 --> 00:53:59,000
Have a category, so that's totally different study design. the one tail, 2 tail, or one side a two-sided, is to do with the the way you have a key by right.

433
00:53:59,000 --> 00:54:03,000
If you'll have part of it your alternative is is you know doesn't have a direction.

434
00:54:03,000 --> 00:54:15,000
Then you are doing 2 sided paths if your alternative have a direct and say, mu greater than 28,000. Then you compute the pivot only by looking at the one tail of the normal so you've got half your

435
00:54:15,000 --> 00:54:21,000
pivotal the one you got from 2 side so that's a totally different thing that way. I didn't touch too much, because that just confused things.

436
00:54:21,000 --> 00:54:25,000
But but things you ask so that's a total of different policy.

437
00:54:25,000 --> 00:54:32,000
You can do it. 2 sided to sample test that's what I'm saying, those 2 go i'll talk about 3.

438
00:54:32,000 --> 00:54:39,000
Okay, Now let's go back to yeah thanks for having questions here. So let's let's not move on to 2 sample about happening right?

439
00:54:39,000 --> 00:54:47,000
So. this this one population is is rarity Now say it's not so often encountering study.

440
00:54:47,000 --> 00:54:53,000
Most often we have 2 population, which is the case. For example, you have a draw.

441
00:54:53,000 --> 00:54:57,000
You have a Trip Trip treatment group. They have a control group.

442
00:54:57,000 --> 00:55:00,000
Now I have 2 population right. You want to see the treatment.

443
00:55:00,000 --> 00:55:05,000
You know this sounds different. right then, that's why you have a placebo group of control group, right?

444
00:55:05,000 --> 00:55:11,000
So, having a control group is always very helpful. You manage study design without a control group.

445
00:55:11,000 --> 00:55:19,000
You need to come up with a no no service as sort of a control, but it's always hard to come up with a null null value. Right?

446
00:55:19,000 --> 00:55:22,000
So that's why the 2 samples would more use more Let's see.

447
00:55:22,000 --> 00:55:28,000
Suppose we want to compare 2 population means so here. we still study population meet, because that's a quantitative interest.

448
00:55:28,000 --> 00:55:32,000
But instead of 100 now we have 2 population. right? so Mu.

449
00:55:32,000 --> 00:55:46,000
One will be the mean birthway the baby born to will, living in poverty. But instead of assuming a low value for this, now we have a mu 2, which the mean verse, weight of baby board of women not living in poverty so

450
00:55:46,000 --> 00:55:55,000
In this case these are 2 separate population, right? Each individual can only come from one population not most simultaneously.

451
00:55:55,000 --> 00:56:00,000
And then. Now we compare new one versus new, too.

452
00:56:00,000 --> 00:56:05,000
If mu one is not equal to mu 2. Now I know I mean birthway.

453
00:56:05,000 --> 00:56:18,000
The baby born in this 2 population are different. which means Poverty is is associated with the birthday associated right, not call positive, but associated right?

454
00:56:18,000 --> 00:56:27,000
So that's the argument. so in fact we use a find the revival to to define the the membership of the group right here.

455
00:56:27,000 --> 00:56:33,000
Poverty or not is a binary variable, and then we measure a continuous variable all over the population.

456
00:56:33,000 --> 00:56:44,000
Here the birthday. So you Saturday. These 2 sample he has is trying to establish the association between a binary variable, which is a group indicated variable versus a continued outcome.

457
00:56:44,000 --> 00:56:48,000
Right. So we have a configuration where all comments continue to sell.

458
00:56:48,000 --> 00:56:53,000
Exposure is binary, you can always think about using to sample.

459
00:56:53,000 --> 00:56:55,000
So here the exposure is: Is mother living in poverty.

460
00:56:55,000 --> 00:57:04,000
The outcome is birth, weight of the baby

461
00:57:04,000 --> 00:57:11,000
Question.

462
00:57:11,000 --> 00:57:16,000
So, because mu one is the mean birthday in one group, mute with the members, with the in the other.

463
00:57:16,000 --> 00:57:25,000
If poverty it's not a social with birth weight that with the then eventually this 2 group are the same right.

464
00:57:25,000 --> 00:57:30,000
The 2 population have this are the same. If they are the same, then they have the same mean.

465
00:57:30,000 --> 00:57:35,000
Therefore, under the null, which means the birth. weight is not associated with poverty under the null.

466
00:57:35,000 --> 00:57:42,000
Me y equals no 2, because they have the They are the same population for me, one minus mu, 2 equals 0.

467
00:57:42,000 --> 00:57:58,000
That's the same statement mathematically and then the alternative will be the opposite of this. But I mean what not equal to mute to a new one, not equal to 0. So this look very much the same but it's different from the one sample key that's because you want some

468
00:57:58,000 --> 00:58:04,000
of we say mu equals mu not under the knot where mule now is the value you assume.

469
00:58:04,000 --> 00:58:07,000
You have to come out with the new mouth which in practice always tricky right here.

470
00:58:07,000 --> 00:58:11,000
You don't have those zoom annual now you just assume they are equal.

471
00:58:11,000 --> 00:58:19,000
You do have to assume their value, not give it back, making more natural and more widely used.

472
00:58:19,000 --> 00:58:25,000
So I know this kind of hypothesis. We are doing the so. how to sample.

473
00:58:25,000 --> 00:58:31,000
He had. So the crucial thing, what was on the computer is, Think about what are your 2 population right?

474
00:58:31,000 --> 00:58:38,000
The 2 population should be not overlapping. Everything individual should come from one population only.

475
00:58:38,000 --> 00:58:42,000
And they basically think about indicator variable, defining the appropriate.

476
00:58:42,000 --> 00:58:48,000
Usually the exposure value treatment versus control, then the treatment variable is the indicative animal right.

477
00:58:48,000 --> 00:58:56,000
Poverty versus not poverty. Then the poverty meeting right well, man versus woman by the the gender is not indicated so well.

478
00:58:56,000 --> 00:59:01,000
Now we have 2 popularity. Think about what what indicate a variable, what binary are we defined in the 2 population?

479
00:59:01,000 --> 00:59:08,000
Once we figure that out, right, things are clear.

480
00:59:08,000 --> 00:59:13,000
Because we have 2 population now, instead of calculating 3 numbers from a population.

481
00:59:13,000 --> 00:59:22,000
Now we capital 6 numbers from each from each population We haven't published a mean, obviously a standard division, and the sample size.

482
00:59:22,000 --> 00:59:31,000
We we have a different sample size. We could also have the same Center Assembly as proposed population.

483
00:59:31,000 --> 00:59:38,000
So we have 3 numbers for each population, so the the quality of each is now mu one minus mu, 2.

484
00:59:38,000 --> 00:59:41,000
So me y is a parameter. Mutual is a parameter.

485
00:59:41,000 --> 00:59:46,000
So mu one minus mu 2 is the quality of interest, which is a fixed, unknown quality.

486
00:59:46,000 --> 00:59:51,000
We want to know whether this difference in me is 0 or not.

487
00:59:51,000 --> 00:59:59,000
The best quantity To estimate this difference is x one bar minus x, 2 bar right, because x one bar map approximately one as 2 barbar possible new.

488
00:59:59,000 --> 01:00:04,000
To then the difference of X, one R and X. 2 bar a product V, one minus mu, 2.

489
01:00:04,000 --> 01:00:11,000
Sorry for the difference between X one bar and x bar of big We think there's more evidence by saying this to a different versus.

490
01:00:11,000 --> 01:00:17,000
You know little evidence, but we need to take into consideration of the variability right?

491
01:00:17,000 --> 01:00:21,000
This x one bar minus x 2 bar is random they're always at some point.

492
01:00:21,000 --> 01:00:24,000
If you do, and therefore it has a variability, we need to qualify that.

493
01:00:24,000 --> 01:00:30,000
So now see the quality we're dealing with is different but the framework is the same right?

494
01:00:30,000 --> 01:00:43,000
You'll find a signal you're qualified annoying you find a statistics only by the uncertainty

495
01:00:43,000 --> 01:01:13,000
Alright. This slides is very easy, so let's take a 10 min break, and then continue after 10 min

496
01:11:36,000 --> 01:11:45,000
Okay, let's continue so thanks who are on this topic you know it occurs to me. it's worthwhile mentioning this particular news.

497
01:11:45,000 --> 01:11:51,000
If you you know where of this this drop I know how to pronounce it.

498
01:11:51,000 --> 01:11:56,000
But it's an alzheimer's drug the followed by my biology.

499
01:11:56,000 --> 01:12:05,000
I was a quite a news last year it was approved by having a If you read what's going on is that, you know the original analysis right there.

500
01:12:05,000 --> 01:12:10,000
The real result, for the drug in a clinical trial was not really secondary time.

501
01:12:10,000 --> 01:12:21,000
However, they, they they did the different analysis. I I forgot the details of, you know, the adding more data, or they, you know they have some of the population.

502
01:12:21,000 --> 01:12:27,000
Things like those. Eventually they find analysis, you know, which seems like the drug is effective. not a lot.

503
01:12:27,000 --> 01:12:31,000
There are still some controversy around, because, you know, I think they run 2 different trial, different population.

504
01:12:31,000 --> 01:12:34,000
One, and them is still not significant in the end, you know.

505
01:12:34,000 --> 01:12:39,000
Have a lot of discussion, I think Fda approved the drug.

506
01:12:39,000 --> 01:12:48,000
Last year, but because the the flossing analysis right, as I mentioned earlier on, you really need to prescribe your analysis and get a clear pivot less than 0 point 0 5.

507
01:12:48,000 --> 01:12:54,000
Look at it path, by the way, that did it draw a lot of controversy.

508
01:12:54,000 --> 01:12:59,000
Therefore a lot of criticism to both the drug company also mainly the Fda.

509
01:12:59,000 --> 01:13:06,000
And then, for example, 3 otherwise i'd be able to resign, you know, so on, so forth.

510
01:13:06,000 --> 01:13:12,000
You can imagine in reality this. you know you. You see both sides of argument, for example, from the draw company.

511
01:13:12,000 --> 01:13:17,000
Of course they want to push it out. They spent decades, if not longer, to develop a drop, you know.

512
01:13:17,000 --> 01:13:21,000
To run a trial cost them like hundreds of millions. not billions of dollars. Right?

513
01:13:21,000 --> 01:13:24,000
Of course they want to to get a proof, so they can sell the drug.

514
01:13:24,000 --> 01:13:30,000
On the other hand, the Fba is also under a lot of pressure, because, you know, there's is no effective drop to Alzheimer a long time. right?

515
01:13:30,000 --> 01:13:37,000
So any drop out there could we hope to patience right so But, on the other hand, there's a scientific rigor.

516
01:13:37,000 --> 01:13:41,000
Do you want to compromise that this right in the past If we support another drop?

517
01:13:41,000 --> 01:13:48,000
There's a probably the case they will not approve it but given the card situation of the particular, you know disease, they approve it right?

518
01:13:48,000 --> 01:13:55,000
So that draw a lot of controversy. So so in practice is always tricky to do with this kind of problem.

519
01:13:55,000 --> 01:14:01,000
So So I just wanna be praying because it's a link to the thing we just talk about on p value.

520
01:14:01,000 --> 01:14:06,000
And you know the rigor. Any comment on this?

521
01:14:06,000 --> 01:14:23,000
Anybody know more about this story than I just talk about

522
01:14:23,000 --> 01:14:32,000
Yeah, I think I think this is a a good example of what you were talking about earlier, with the question of statistical significance versus clinical significance.

523
01:14:32,000 --> 01:14:44,000
Right. You have a disease process that is devastating that we have no good treatment, for they've done a study which just based on the reading here.

524
01:14:44,000 --> 01:14:55,000
Maybe the last statistical significance, but could have shown clinical significance in a population that has no other option, you know.

525
01:14:55,000 --> 01:15:04,000
So I I think it's a good example of that yeah I think it's, you know, giving people a clinical chance is a positive in in my mind.

526
01:15:04,000 --> 01:15:08,000
Anyway. Assuming that there are no significant side effects, let me say that.

527
01:15:08,000 --> 01:15:15,000
Yup: Yeah. But you see, you know, people resigned because of the right people are different, different.

528
01:15:15,000 --> 01:15:17,000
Take on usuals like this so it's it's not a clear answer.

529
01:15:17,000 --> 01:15:27,000
Appreciate your comments. Okay. yeah let's go back So

530
01:15:27,000 --> 01:15:34,000
We talk about this, right? So we use x one bar minus x, 2 bar as the signal, and then we try to quantify the uncertainty around it.

531
01:15:34,000 --> 01:15:37,000
So basically we need to figure out the standard error of this type.

532
01:15:37,000 --> 01:15:40,000
So basically any quantity you compute from a data is random.

533
01:15:40,000 --> 01:15:46,000
Therefore it has a summary distribution that you need. If you can calculate the

534
01:15:46,000 --> 01:15:51,000
Standard. Db: from that sampling distribution. No, we call the standard error of the quantity right?

535
01:15:51,000 --> 01:15:57,000
So this X one bar minus X 2 bar is the statistic therefore it's Random Air Force has a standard arrow.

536
01:15:57,000 --> 01:16:02,000
So it turns out that this particular quantity 25% error.

537
01:16:02,000 --> 01:16:04,000
So this go a little deeper to the math. But X.

538
01:16:04,000 --> 01:16:11,000
One bar will be normal Distributed X. 2 bar will be normal disability, but both based on centralism theorem.

539
01:16:11,000 --> 01:16:20,000
Therefore there are different. We'll also be normal distributed I bet you have this particular standing error right?

540
01:16:20,000 --> 01:16:27,000
Essentially it's a standard arrow squared, for x one bar, and the standard arrow square for X 2 bar the added the mob, and then takes square root.

541
01:16:27,000 --> 01:16:30,000
The total of this bomber you know mathematically can prove it I'm.

542
01:16:30,000 --> 01:16:42,000
Then once you'll figure out the standard error for this quality, then just like before you can now get a t statistic, which is the 6 statistic you are measuring X one more minus I toolbar divided by his own standard error.

543
01:16:42,000 --> 01:16:52,000
It turns out this thing again. we'll follow normal 0 One distribution with the sample size in both are large I'm saying, both group have more than 20 subjects.

544
01:16:52,000 --> 01:16:59,000
This is a fairly good, normal standard, normal distribution so you might ask why don't you just add up the 2 standard arrow?

545
01:16:59,000 --> 01:17:04,000
You just turn all the mathematics that doesn't work. only you have to add up the square and the take square root.

546
01:17:04,000 --> 01:17:15,000
That works. Okay. But now for the last now, i've got this t statistic, which also follow normal 0 one and everything go back to the same framework right?

547
01:17:15,000 --> 01:17:27,000
You can get a p value by looking at the pale i'm. Now i'm making a decision right? So the only thing change from before. from what sample do to sample is the way we collect data and then we come here

548
01:17:27,000 --> 01:17:38,000
statistic, everything else be the same question

549
01:17:38,000 --> 01:17:42,000
Yeah, Probably these kind of formula you don't have to remember them.

550
01:17:42,000 --> 01:17:51,000
And you. You probably will forget, anyway, right after a year. but a key things Now, to understand what we are doing here right? We use the father to get a statistic so that we can get a P.

551
01:17:51,000 --> 01:17:56,000
By, and now we just do it together right with the key value.

552
01:17:56,000 --> 01:17:59,000
We can run a high all pass, but, for example, the same thing.

553
01:17:59,000 --> 01:18:03,000
Piece statistic was absolute value locked on key or unlocked on 2.

554
01:18:03,000 --> 01:18:06,000
We at least will keep out of left 1 point o 5 just like the ones.

555
01:18:06,000 --> 01:18:13,000
I'm: Okay, and then we can say we reject them all Okay, to reject them all.

556
01:18:13,000 --> 01:18:18,000
Similarly we could construct a confident in the vote this time for the quality of mu, one minus v.

557
01:18:18,000 --> 01:18:26,000
2, so the quality of interest is mu one minus mu 2. so the common interval will also for this difference.

558
01:18:26,000 --> 01:18:29,000
So we can construct a confidence interval for anything of interest. right here.

559
01:18:29,000 --> 01:18:43,000
The interest is the difference between mu one and mu 2, so the company level for this will be similarly via point estimate, which is x one minus x 2 bar plus minus 2 times the standard arrow of the party.

560
01:18:43,000 --> 01:18:51,000
So this equation look long by the same format right point at the main plus minus 2 times the standard error.

561
01:18:51,000 --> 01:18:54,000
So if we have the data, we can compute all of this.

562
01:18:54,000 --> 01:18:59,000
We can get a common interval, so the combinable will also be equivalent to the P.

563
01:18:59,000 --> 01:19:07,000
To the pivot right? If this common interval contain 0, because 0 is the novel is the value of the difference under the North.

564
01:19:07,000 --> 01:19:13,000
And then. Now this no one might be 2 equals 0, because they are equal. Right?

565
01:19:13,000 --> 01:19:19,000
If this common interval contains 0, then the key value will be left will be larger than 0 point, 0 5, and the vice versa.

566
01:19:19,000 --> 01:19:23,000
The polynomial does not contain 0. then the P value will be last time.

567
01:19:23,000 --> 01:19:28,000
Point of 0 point 0 5 that's when we talk about a common, though interval, 95% versus.

568
01:19:28,000 --> 01:19:35,000
The key value cut out of 0 Point 0 5, right? If you use the color of 0 point 0 one, then you should look at a common mode of 99%.

569
01:19:35,000 --> 01:19:41,000
They all go alongside in China. I know i'm talking a lot.

570
01:19:41,000 --> 01:19:47,000
So you know I I i'll take a 30 s break, you know for the same thing.

571
01:19:47,000 --> 01:20:01,000
Let me know you have any questions

572
01:20:01,000 --> 01:20:09,000
The the value is a t greater than 2. In this case it cannot be less than 2.

573
01:20:09,000 --> 01:20:25,000
No, this is key absolute value. Oh, the Absolutely. Okay. Okay,

574
01:20:25,000 --> 01:20:42,000
Okay, So now let's do a real hello So here we we consider studying alzheimer's and dementia, and then we studied whether the consumption of sugar evaporate in a dose is

575
01:20:42,000 --> 01:20:49,000
associated with level of chloric intake so I just want to ask you when you're studying things like this. right?

576
01:20:49,000 --> 01:20:56,000
So what is the exposure of What it's all call and the what are the 2 groups?

577
01:20:56,000 --> 01:21:08,000
Anyone

578
01:21:08,000 --> 01:21:22,000
So the exposure appears to be the number of beverages. they're consuming in the day should reverages. And then the population are us adults consuming beverages. Yup perfect yeah, usually it's the

579
01:21:22,000 --> 01:21:25,000
outcome and interest outcome right. The The exposure is the intake.

580
01:21:25,000 --> 01:21:28,000
So the tricky thing here is that you can think about the sugary beverage.

581
01:21:28,000 --> 01:21:34,000
Intake probably is continuous, but to use 2 sample Key has with need to dive, optimize it.

582
01:21:34,000 --> 01:21:37,000
How do I spend? I'll do it right you have to use other means.

583
01:21:37,000 --> 01:21:41,000
So to that, optimize the exposure. we we do this by population.

584
01:21:41,000 --> 01:21:47,000
One. Are those us adults for report consumption consuming last, not one value today, and then the rest.

585
01:21:47,000 --> 01:21:50,000
You know, actually one to 2 barrier today are apart.

586
01:21:50,000 --> 01:21:57,000
Our our the second population. So we are ignoring those consuming one and 2.

587
01:21:57,000 --> 01:22:01,000
So the 200 here are one or one to 2 that's not one.

588
01:22:01,000 --> 01:22:18,000
I wanted to do. the once you have the 2 population design, we can define the the parameter interest. by meal wise, the mean of calories per day in the first group a mutual is the mean of the second group right Here is

589
01:22:18,000 --> 01:22:25,000
the mean on calories, not amino average, because the average is only defined at Google.

590
01:22:25,000 --> 01:22:31,000
The management is on the calorie. now, you know. once you define everything.

591
01:22:31,000 --> 01:22:38,000
Now go to the math part right? The how it is always mu y equals mu 2 at no mu one not equal to mutualized alternative.

592
01:22:38,000 --> 01:22:45,000
Now collect the data. We have X Bar S and N. one x, you know X, 2 bar as 2 and N.

593
01:22:45,000 --> 01:22:49,000
2. So this you can collect from your data. So now we have 2 group.

594
01:22:49,000 --> 01:23:00,000
Both are positive. The phone of subjects right you don't have to you don't have to have equal sample size, although in your in your study design equal sample size is is convenient, and also has the highest power right

595
01:23:00,000 --> 01:23:05,000
if you fix the total sample size not equal sample size in both group has a high power relatively.

596
01:23:05,000 --> 01:23:11,000
So that's Why, that's something you want to consider but nevertheless, you don't have to do it.

597
01:23:11,000 --> 01:23:14,000
That's why, I suppose samples are large you have to power so?

598
01:23:14,000 --> 01:23:21,000
No, the numbers right. The T statistic is x, one bar minus x, 2 bar.

599
01:23:21,000 --> 01:23:29,000
Here. You know this number minus this number divided by the square root of this you have to plug in the numbers, So the key is to find a number to plug in.

600
01:23:29,000 --> 01:23:35,000
What's your problem There's a copulator you've got to keep the ticket Now, the piece that this is negative 10.4.

601
01:23:35,000 --> 01:23:39,000
Just by looking at this we don't have to really gather pivot.

602
01:23:39,000 --> 01:23:49,000
We know it will be. How do you take a good time because it's last night, minus 2, and way less than minus 2

603
01:23:49,000 --> 01:23:55,000
So it turns out you know we're not there yet we'll we'll we'll show the key value later, but the key value will be highest in every time.

604
01:23:55,000 --> 01:24:03,000
Now let's go look at a common interval I will already answer this question.

605
01:24:03,000 --> 01:24:09,000
So the combinable will be X one bar minus x 2 bar plus minus 2 times the standard deviation.

606
01:24:09,000 --> 01:24:17,000
Again. You'll probably the quality you got this commonlyable So this is a confident interval for the difference of the mean. In 2 groups.

607
01:24:17,000 --> 01:24:36,000
You see, the company involved does not include 0, which means 0 is unlikely to be the difference of the 2 group, which means the 2 group are unlikely to have equal, mean which means the null is unlikely to be true right there's

608
01:24:36,000 --> 01:24:51,000
a lot of implications right in the argument But That's how we, how we get the conclusion. Therefore, based on this data, would you be supportive price release dating that the consumption sugar vary leads to higher

609
01:24:51,000 --> 01:24:54,000
Clara intake, be prepared to define your answer.

610
01:24:54,000 --> 01:25:11,000
Anyone will take on this question

611
01:25:11,000 --> 01:25:18,000
I would say Yes, because the you are the values that you have does not cross 0.

612
01:25:18,000 --> 01:25:26,000
So that means you have a pretty significant you that your alternative hypothesis is annull as well.

613
01:25:26,000 --> 01:25:32,000
Yup, Thank you. Anybody else

614
01:25:32,000 --> 01:25:39,000
Can you also think that because the values in the confidence interval are negative, it shows us that X.

615
01:25:39,000 --> 01:25:49,000
One is less, then x 2, and x. one is when you have less than one beverage day versus one to 2 beverages per day or x.

616
01:25:49,000 --> 01:26:01,000
2. Thank you. I thank both of you. However, I want you to so so i'm not so sure myself.

617
01:26:01,000 --> 01:26:05,000
I think I think the statement is a little strong for what the data is saying.

618
01:26:05,000 --> 01:26:11,000
Right, in my opinion, I think, I think the data is saying that there's a difference between their color intakes.

619
01:26:11,000 --> 01:26:16,000
I don't know that the sugary beverages are the difference in the chloric intakes right?

620
01:26:16,000 --> 01:26:23,000
So I don't know that the second group has higher color intake because of the shouldn't re beverages.

621
01:26:23,000 --> 01:26:26,000
Not sure that I I think I think with the data saying that There's a difference in the core.

622
01:26:26,000 --> 01:26:32,000
Can take between the 2 groups I don't know that it's necessary to surely beverages This cost. Thank you.

623
01:26:32,000 --> 01:26:36,000
Yeah, you're you're absolutely right so the the keyword here is lease right.

624
01:26:36,000 --> 01:26:40,000
If you say something, lists to something you are making causal conclusion.

625
01:26:40,000 --> 01:26:46,000
Up here. We are establishing association right? so there could be confounding going on.

626
01:26:46,000 --> 01:26:52,000
Maybe those people who take more sugary beverage also, if candy at the same time right?

627
01:26:52,000 --> 01:27:00,000
So we never know. So you we are not ruling out I mean sugar, Babe doesn't have more you know, calories in them.

628
01:27:00,000 --> 01:27:05,000
But maybe those people who drink more sugarary average also eat more right. There's a there could be confounding.

629
01:27:05,000 --> 01:27:08,000
So you Saturday again. this is observational study.

630
01:27:08,000 --> 01:27:15,000
We can say they are associated, saying one least, for the the other is a stronger statement, stronger than the data.

631
01:27:15,000 --> 01:27:26,000
Of course, so that that's the crucial thing here right? So I I think you'll absolutely right you understand the company, you know the pivot or the you know what what they infer statistically, But when when you make a

632
01:27:26,000 --> 01:27:32,000
press release right. The wording is crucial but you know honestly, I don't really think most of the layers.

633
01:27:32,000 --> 01:27:38,000
I understand the difference between association and the calls already association and the non causal association. Right?

634
01:27:38,000 --> 01:27:43,000
So not only we should try to avoid using this worth, I think right.

635
01:27:43,000 --> 01:27:49,000
They should also measure that in the press release right? as basically saying, the association may not be causal, right.

636
01:27:49,000 --> 01:28:00,000
There could be other things going on but I don't know when most of the passengers are saying that right Now this, you know, Press trying to make up make make the title catchy right?

637
01:28:00,000 --> 01:28:11,000
So that they got more clicks, and obviously many people will just use the word but but by doing that they are actually, you know, exaggerating the finding right?

638
01:28:11,000 --> 01:28:17,000
So so unfortunately that's the reality but we should all be careful when we read, when we read the press release like this right, you know.

639
01:28:17,000 --> 01:28:30,000
Think about 2. their study really support this conclusion? Any comment

640
01:28:30,000 --> 01:28:40,000
Yeah, I think as long as they add up word like a could lead to not making much better right, cause it, it definitely could lead to.

641
01:28:40,000 --> 01:28:44,000
But by the day that does support the the definitive answer.

642
01:28:44,000 --> 01:28:49,000
A conclusion like this

643
01:28:49,000 --> 01:28:59,000
Okay. Now let's use some visual try to find out what's going on, because you know the the equations are some somewhat complicated right?

644
01:28:59,000 --> 01:29:06,000
Involved. So essentially, the 2 sample key test is trying to see whether the 2 population are the same or not right.

645
01:29:06,000 --> 01:29:15,000
If the 2 population are the same, which means the indicator variable, the exposure has nothing to do with the outcome.

646
01:29:15,000 --> 01:29:20,000
So Now let's think about let's say let's use different caller right?

647
01:29:20,000 --> 01:29:32,000
We because we're in u m use maze and blue. right let's use maze to denote the baby born to women living in poverty and their weight and blue means the other population let's assume the poverty

648
01:29:32,000 --> 01:29:38,000
population has a mean new one and a standard deviation. Sigma one and the other probably has new Trans. Sigma.

649
01:29:38,000 --> 01:29:43,000
2. So if we assume both population are sort of a normal I'm.

650
01:29:43,000 --> 01:29:47,000
Now we can draw them using some some other circle, right?

651
01:29:47,000 --> 01:29:52,000
So we gotta use square to start the center Radius to stand a standard deviation and so on.

652
01:29:52,000 --> 01:29:59,000
Let's go down through the figures right so these are the population right?

653
01:29:59,000 --> 01:30:05,000
So the the main one, or the poverty population the blue one or the nonprofit population.

654
01:30:05,000 --> 01:30:09,000
They're a mean and their standard deviation so you see the 2 publishers.

655
01:30:09,000 --> 01:30:14,000
Actually they overlap quite a bit. right. So there are.

656
01:30:14,000 --> 01:30:20,000
There are babies born with a poverty Population have higher weight. that baby is born to the nonprofit population.

657
01:30:20,000 --> 01:30:26,000
So. But look at individuals. you can't really say much because there's a huge vulnerability within each population.

658
01:30:26,000 --> 01:30:36,000
Now, how about the last? One population Long lies on right side of the other, which means overall the blue population has higher birth rate than the maze population right?

659
01:30:36,000 --> 01:30:48,000
The population mean is higher. So in this case no let's think about what? if the 2 population are separated more by this case?

660
01:30:48,000 --> 01:30:53,000
The difference in the mean are are bigger which means we have a stronger effect File.

661
01:30:53,000 --> 01:31:08,000
They're overlapping small, right therefore we have higher power. Therefore the piece statistic given the same sample size, should be more significant

662
01:31:08,000 --> 01:31:18,000
Right. So these are the We try to vary one variable the time, because the power depend on all the evidence depend on many things like sample, size, defect, size, like variability, right?

663
01:31:18,000 --> 01:31:26,000
So once we fix everything, we just increase the effect. size, increase the difference, though we have higher power.

664
01:31:26,000 --> 01:31:32,000
Even we have the same sample size. this is the case.

665
01:31:32,000 --> 01:31:36,000
We have even higher impact size. therefore the power is even higher by the P.

666
01:31:36,000 --> 01:31:42,000
Value from this case will be super small

667
01:31:42,000 --> 01:31:46,000
And this is the case where we keep one group, but make the other group larger.

668
01:31:46,000 --> 01:31:55,000
So basically larger variability. You see that everything else equal, we will increase our ability, Right?

669
01:31:55,000 --> 01:31:59,000
You increase the amount of overlap between the 2 population.

670
01:31:59,000 --> 01:32:07,000
Therefore you have a lower power

671
01:32:07,000 --> 01:32:13,000
And then this is the case where you decrease variability different, the standard deviation of one group you have smaller overlap.

672
01:32:13,000 --> 01:32:24,000
There you have a higher power, right? So the power, the increase with the defect size basically difference between the 2 mean and also increase, also decrease with the variability, right?

673
01:32:24,000 --> 01:32:35,000
Unless you have a higher value of a lower power, so that this kind of visual help, you understand, you know, where the power come from.

674
01:32:35,000 --> 01:32:56,000
So any questions, so far.

675
01:32:56,000 --> 01:33:00,000
Okay, So this is the 2 sample key hats. right?

676
01:33:00,000 --> 01:33:03,000
So you compare 2 population later on you might think about what if we have more than 2 public?

677
01:33:03,000 --> 01:33:10,000
What do you have? 3 population right now? We do something harder, Annova, one way or no one.

678
01:33:10,000 --> 01:33:16,000
Specifically. we're going to talk about that after we did a exercise, for, for example, keypass.

679
01:33:16,000 --> 01:33:22,000
Maybe tomorrow, maybe later the day depend on the time okay so Let's take a you know.

680
01:33:22,000 --> 01:33:29,000
Pause on the lecture we'll continue later on now let's switch gear to the exercise.

681
01:33:29,000 --> 01:33:34,000
So this particular lecture is long. therefore we have 4 exercise 7, 8, and 7.

682
01:33:34,000 --> 01:33:43,000
B is about to sample t that 7 c and 70 is about both who Samuel Pizza and nova, which will do after we finish the whole lecture.

683
01:33:43,000 --> 01:33:48,000
So today we're gonna do 7 a and a 7 D. and in particular with only 77 A.

684
01:33:48,000 --> 01:33:54,000
So tomorrow we do 70 and 70 70 70 and also to quit we'll do after we finish the whole act.

685
01:33:54,000 --> 01:33:58,000
So the quiz will also submit tomorrow. So so tomorrow morning only 7 days.

686
01:33:58,000 --> 01:34:06,000
Do just make up may talk to you so let's now move on to the the project.

687
01:34:06,000 --> 01:34:13,000
7, 8, cause that's the wine venus on it and after 7, 8 we couldn't do something about 70.

688
01:34:13,000 --> 01:34:28,000
Let me share 7 A. The 7 A is talking about this paper on a football players. the paper is given on cameras and also a spreadsheet.

689
01:34:28,000 --> 01:34:31,000
How are you? i'll kind of use something is this relatively short.

690
01:34:31,000 --> 01:34:40,000
Now let me go back. So the paper is here, and pay price here.

691
01:34:40,000 --> 01:34:44,000
Okay, and then we might be used this to sample key test spreadsheet.

692
01:34:44,000 --> 01:34:57,000
Let me turn it up, open and up.

693
01:34:57,000 --> 01:35:02,000
So just like before. Right, however, this case, the data at 2 column.

694
01:35:02,000 --> 01:35:07,000
Now this is the outcome. This is the group indicator.

695
01:35:07,000 --> 01:35:13,000
So here we are mixing 2 group together. right we are not putting the outcome for the first group has the first call in the second group.

696
01:35:13,000 --> 01:35:21,000
That's signed on problem. Rather we mix them together? so the outcome from both Google Are you the second column and the first call on top of the group indicator. right?

697
01:35:21,000 --> 01:35:29,000
So it's actually here means the individual from the second group. This one is the individual from the first group, and so forth.

698
01:35:29,000 --> 01:35:33,000
Now, why are these 2 column have the same left it's pretty long years?

699
01:35:33,000 --> 01:35:36,000
So i'm not in the show but they have the same left That's where the data looks like.

700
01:35:36,000 --> 01:35:44,000
So you can copy pay for data here. once you put the data here, everything else will be converted automatically, just like before.

701
01:35:44,000 --> 01:35:48,000
Sometimes you may need to change the label here right now.

702
01:35:48,000 --> 01:35:52,000
The label is one and 2. So you put a label at one and 2 later on, maybe A and B.

703
01:35:52,000 --> 01:36:02,000
So you have to change the label. Well, once you you fix this, the mean, the standard deviation sample side for both group will be covering automatically, and then the 2 sample tasks.

704
01:36:02,000 --> 01:36:08,000
This is the x one minus x, 2 bar. This is the standard arrow of that have poverty.

705
01:36:08,000 --> 01:36:16,000
This is the key statistic, which in the Ratio of this 2 first 2 quality, i'm in the p value, and then the 95% commonable for the difference.

706
01:36:16,000 --> 01:36:23,000
Mu, one minus mu 2. Everything will be covered automatically. similarly if you don't have the data, but you have this numbers.

707
01:36:23,000 --> 01:36:27,000
You can just building the number manual right, but then you will break the formula.

708
01:36:27,000 --> 01:36:30,000
So you have to start a new spreadsheet for the next data site.

709
01:36:30,000 --> 01:36:40,000
You want to analyze. So any question regarding this Brashi

710
01:36:40,000 --> 01:36:48,000
Alright. so let's now work on 7 and 8 Oh, again for 25 min, like I mentioned yesterday.

711
01:36:48,000 --> 01:36:52,000
Right. I gotta go with groups for those of you who want to do it yourself.

712
01:36:52,000 --> 01:36:56,000
Feel free to stay in the main room, and I will set it up so that you can have your job group.

713
01:36:56,000 --> 01:37:02,000
So you turn out. you will turn out you're the only one in your group, you know you can find another group to join.

714
01:37:02,000 --> 01:37:11,000
No! I will set up breakout rooms for 25 min.

715
01:37:11,000 --> 01:37:23,000
And and you can come back to the main session anytime you like. And then you've got all the basically a jump between groups different rooms or the main main session, as you like.

716
01:37:23,000 --> 01:37:53,000
Any question again. it will be 25 min, and the see you in a bit

717
02:03:41,000 --> 02:03:47,000
Okay, welcome back, everyone. Thanks. Many of the groups are, you know.

718
02:03:47,000 --> 02:03:53,000
Some of the groups are empty, so I would not be calling on group numbers all these exercise anymore.

719
02:03:53,000 --> 02:03:58,000
I've just you know. welcome having volunteers So this is about his football data, right?

720
02:03:58,000 --> 02:04:03,000
I hope you got a chance to read a little bit about the article for interesting.

721
02:04:03,000 --> 02:04:09,000
So number one question describe how the patients were selected to participate in the study.

722
02:04:09,000 --> 02:04:15,000
And would you characterize the subjects at random? sample and explain your answer?

723
02:04:15,000 --> 02:04:22,000
Anyone wants to. You wish shop

724
02:04:22,000 --> 02:04:35,000
Sure I can speak on behalf of group 2 So it's interesting, because in the study, if you look at the methods there's 3 groups there's just the .

725
02:04:35,000 --> 02:04:41,000
It's not clear where the control boot came from but there's 25 control people subjects.

726
02:04:41,000 --> 02:04:48,000
And then there's, 25 people who have a history of concussion, and there's 25 people who had some type of head trauma.

727
02:04:48,000 --> 02:04:56,000
But don't have a history of concussion and so I guess, to to answer this question.

728
02:04:56,000 --> 02:05:08,000
Where the subjects was this to the random sample we we thought in terms of the the people with you have head trauma on playing football.

729
02:05:08,000 --> 02:05:16,000
I think that part was was pretty random, because the patients had the choice of whether they wanted to practice pay in the study or not.

730
02:05:16,000 --> 02:05:34,000
But we weren't sure if the controls were randomly selected, because the paper makes no documentation of how, where the controls came from, whether they came from the same institute, or if they were from different database thank you Thank

731
02:05:34,000 --> 02:05:55,000
you very much anyone wants to add on anything so I also thought it wasn't random, because they did consecutive cases as you have on the screen. so consecutive cases out of this out of one of the

732
02:05:55,000 --> 02:06:04,000
football divisions division One football and took the consecutive patients are consecutive people as opposed to some randomized like.

733
02:06:04,000 --> 02:06:09,000
I don't know every third or fourth person in the list right so they just took the first.

734
02:06:09,000 --> 02:06:23,000
I guess I guess that first 25 i'm not exactly. sure. but I didn't think that was that was randomization, or they could have been more randomized when you say that thank you Yeah, So

735
02:06:23,000 --> 02:06:26,000
Yeah, I guess you know, think concepts on on the textbook is always easy for this time.

736
02:06:26,000 --> 02:06:32,000
I want, you know. in reality it's always sort of a kind of tricky, right?

737
02:06:32,000 --> 02:06:37,000
So here in the abstract they mean they set is cross-sectional study.

738
02:06:37,000 --> 02:06:42,000
So by. you know, cross-sectional is different from prospective versus retrospective cross-sectional.

739
02:06:42,000 --> 02:06:45,000
Basically, you take a snapshot in time and collect things information.

740
02:06:45,000 --> 02:06:53,000
And you know, subject There they did a cross-sectional study conducted between this time and then analyze 3 groups right?

741
02:06:53,000 --> 02:07:04,000
And then in the study participants previous conservative cases on second case. It's probably they, you know, most likely just mean they take everybody they could, because they don't have a big sample side right?

742
02:07:04,000 --> 02:07:11,000
All of us are football apple is now we can imagine there won't be Oh, madam, and the who are also waiting to participate. right?

743
02:07:11,000 --> 02:07:19,000
So they basically pretty much took everything everything they could. And so did this talk about randomization.

744
02:07:19,000 --> 02:07:26,000
You talk about random random? Yeah. no, this is rather than selective volumes.

745
02:07:26,000 --> 02:07:32,000
So they they did a randomly, so random sample, you know, by definition we only learn single random sample.

746
02:07:32,000 --> 02:07:38,000
Simple, random sample will be everybody in the population has equal probability being included in the study. Right?

747
02:07:38,000 --> 02:07:42,000
That's a very ideal situation, usually that do not happen perfectly.

748
02:07:42,000 --> 02:07:50,000
But at least you try to manage that right here. They are not really flipping a coin or do any random draw on their population.

749
02:07:50,000 --> 02:07:55,000
But they basically take everything, everybody possible from the population who is waiting for participating.

750
02:07:55,000 --> 02:08:05,000
So is this far from a random sample I will say and also a lot of times, you know, you just cannot involve need enroll people. right?

751
02:08:05,000 --> 02:08:13,000
I'll randomly cause you have to have their consent and people who consent that we include our no longer random right?

752
02:08:13,000 --> 02:08:18,000
So in this case it's hard to have a random sample but you might ask, what about random randomized trial? right?

753
02:08:18,000 --> 02:08:21,000
They also need to consent. Yeah, you're right in the random trial.

754
02:08:21,000 --> 02:08:28,000
They also need 10%. So people involved in the random trial are typically not random sample from the population.

755
02:08:28,000 --> 02:08:32,000
Right, However, the randomness there is on the 2 to arm. you know, random trial.

756
02:08:32,000 --> 02:08:40,000
They really have to live a point to give you placebo or treatment right? So that's it in that sense that the in the randomized trial.

757
02:08:40,000 --> 02:08:54,000
The difference between the placebo and the treatment is really randomized So that's good, although, look, things learn from a randomized trial do not really fully generalize the population, because they do not represent everybody in the population.

758
02:08:54,000 --> 02:09:01,000
So there's different. 2 different level randomness you have to think about in this case it's not a rather much trial.

759
02:09:01,000 --> 02:09:06,000
It's a visual study also they do not run on that. So I will say it's not a randomized study.

760
02:09:06,000 --> 02:09:12,000
That's how they include people and also they talk about like education.

761
02:09:12,000 --> 02:09:19,000
And you know, non-player, non, football, playing age, sex, education, Max, How they control right?

762
02:09:19,000 --> 02:09:24,000
So the football players they don't have a big population so they take a you know, convenience staff.

763
02:09:24,000 --> 02:09:29,000
I will say right. However, they can gather, yeah of a non football playing. You know.

764
02:09:29,000 --> 02:09:33,000
Population is huge, right? So what they did is that they did a H sex education.

765
02:09:33,000 --> 02:09:40,000
Max and control. So by match usually they mean, you know they they you could match how individual.

766
02:09:40,000 --> 02:09:50,000
So for every subject in your you Okay, right? or you on your treatment group, or whatever group you find, someone with similar age, same sacks, similar education level.

767
02:09:50,000 --> 02:09:55,000
You call it a match right? Oh, you could just match on the distribution.

768
02:09:55,000 --> 02:09:59,000
You look at the distribution in your football player group, then imagine that they should be in the control group.

769
02:09:59,000 --> 02:10:08,000
They didn't say exactly how they match but because they pick the person to match. It's also unlikely to be randomized.

770
02:10:08,000 --> 02:10:16,000
Okay. yeah. that's about any comment any question on this question.

771
02:10:16,000 --> 02:10:27,000
Yeah, I think the key point here is after the think about how representative this population is, but it's because they take a consecutive, convenient sample from what they have during that period.

772
02:10:27,000 --> 02:10:32,000
Right. So you may not be very representative of the overall population.

773
02:10:32,000 --> 02:10:38,000
I think the overall population is football players overall right?

774
02:10:38,000 --> 02:10:51,000
Okay, cool question here. sure. So could it be that initially, when they were designing the study, they wanted to randomize it some way, but then realize they their sample population that they were going to was very small.

775
02:10:51,000 --> 02:10:58,000
And so yeah, So, sample Yeah, yeah I mean that's that's usually how we end up with community sample.

776
02:10:58,000 --> 02:11:05,000
Right. Say, Yeah, Say, you will design a study. you want to samples out the 100 to reach a certain power.

777
02:11:05,000 --> 02:11:12,000
But you realize you don't really have that many people at hand. Anyway, by just take, however, money you could get bye you.

778
02:11:12,000 --> 02:11:15,000
Basically, you don't have the luxury to pick from a large population.

779
02:11:15,000 --> 02:11:25,000
You study brass cancer You are in the local hospital you don't have access to outside, and you don't really got over a 100% of patients a year, anyway.

780
02:11:25,000 --> 02:11:31,000
Right. So what can you do? You just pick everybody right, everybody calling me up, you know, Clinic.

781
02:11:31,000 --> 02:11:35,000
You ask them whether you are interested in the study so take Whoever is interested.

782
02:11:35,000 --> 02:11:46,000
That's a come in your sample, thanks for asking Okay, second question.

783
02:11:46,000 --> 02:11:52,000
We do this from the study as pro prospective and the retrospective, and explain anyone?

784
02:11:52,000 --> 02:12:09,000
Question. So I said that this is a retrospective study, because the information that is being collected is about their past specific, one of them being their concussion cool.

785
02:12:09,000 --> 02:12:30,000
Thank you. any other any other Akini. Oh, answers Yeah. I agree in part, because it also prospective the sense that the cognitive testing and Mri were performed after conclusion today.

786
02:12:30,000 --> 02:12:38,000
Study. Thank you. Yeah, I mean, I I think both of your both of you may good point.

787
02:12:38,000 --> 02:12:48,000
So you know there's no clear answer things like this i'll explain a little bit what I think. So first of all this study called itself a cross-sectional study.

788
02:12:48,000 --> 02:12:50,000
Yeah, I just meant a cross-sectional study it's not something.

789
02:12:50,000 --> 02:12:59,000
I toggling cloth somewhere in between. So essentially, you 1 million prospective, you just follow up in time to collect the outcome right in retrospective.

790
02:12:59,000 --> 02:13:04,000
You go back in history to dig out the record right cross-sectional is like you.

791
02:13:04,000 --> 02:13:08,000
You take whatever you have for the moment and measure him that's it right?

792
02:13:08,000 --> 02:13:12,000
So that's where the car the cross section of which means it's not. Probably not, you know, in their minds.

793
02:13:12,000 --> 02:13:18,000
Not fast back. People retros back here, right but let's Look at it, answer any questions.

794
02:13:18,000 --> 02:13:22,000
So the first student was right up, you know. They they go back in time and look at.

795
02:13:22,000 --> 02:13:26,000
Look at their medical records, see whether that they have a concussion or not.

796
02:13:26,000 --> 02:13:31,000
Right, but by, if you look at it in this study, right?

797
02:13:31,000 --> 02:13:40,000
The concussion was what's the exposure right the how come on things, you know the manager after the composure.

798
02:13:40,000 --> 02:13:50,000
So they use the concurrently defined the so the concoction. Yes, happen in the past, but they use that to define a group, so that's even for prospects study you need to define a group and

799
02:13:50,000 --> 02:13:55,000
that's something having in the past right like smoking or things like those. right?

800
02:13:55,000 --> 02:13:56,000
So in prospect. study you only watch the outcome in the future.

801
02:13:56,000 --> 02:14:00,000
So let's now look at outcome what are the outcome of this study.

802
02:14:00,000 --> 02:14:05,000
Right. Especially they study like people, camel volume and the cognitive outcome.

803
02:14:05,000 --> 02:14:08,000
This kind of thing and this kind of thing they measure, use.

804
02:14:08,000 --> 02:14:12,000
Either, you know Mri or other things, or either some path right?

805
02:14:12,000 --> 02:14:17,000
And the second student was definitely right. They run the past after they got us got the players.

806
02:14:17,000 --> 02:14:27,000
So the past happening sort of a society in the future, after they got a players, although very quickly, they do not really wait for 30 years, but all comes to happen.

807
02:14:27,000 --> 02:14:37,000
They manage it pretty quickly, so that's why, they call it a cross-section, though. but it's also okay to kind of prospective, probably because they haven't you know, after they enrolled enrolled the the

808
02:14:37,000 --> 02:14:42,000
subject. But now, you know, there's another layer of argument right things like people.

809
02:14:42,000 --> 02:14:50,000
Camel volume, although You'll measure it use Mri but those volume probably already exist before you're given your role. right.

810
02:14:50,000 --> 02:15:03,000
You are measuring something already there. it's not like you're waiting time for it to have to change right So in that sense it's also sort of a either retrospective or cross-sectional right you're basically

811
02:15:03,000 --> 02:15:06,000
you use Mri to measure things already happen versus you.

812
02:15:06,000 --> 02:15:13,000
Look at medical history. What's the difference I think is is sort of a grave on here.

813
02:15:13,000 --> 02:15:22,000
Whatever study you're calling, so the end of the day is that whatever it doesn't really matter that much the difference between prospective and retrospective is that there's a recall bias in

814
02:15:22,000 --> 02:15:34,000
retrospective study right if you Ask people what you did in the past. they might forgot about it right. but for process study, because you can control how you observe the outcome.

815
02:15:34,000 --> 02:15:38,000
It's more reliable in this case because you measure the people camera volume.

816
02:15:38,000 --> 02:15:45,000
You measure the cognitive outcome by yourself. you have control of the of the you know accuracy of the outcome.

817
02:15:45,000 --> 02:15:48,000
That's why it's more like a prospective study.

818
02:15:48,000 --> 02:15:55,000
Okay, So So you know, think about the rational way we put into the different basket because we want to see where the bias.

819
02:15:55,000 --> 02:16:01,000
My line is right for retrospective study always ask yourself. i'll i'll come really reliable right there.

820
02:16:01,000 --> 02:16:08,000
Recall bias, you know, entertainment bias, that kind of thing, and also in like a survey.

821
02:16:08,000 --> 02:16:15,000
Right when you ask a survey, that's sort of like a cross-sectional, or retrospective, because you ask, Things happen in the past.

822
02:16:15,000 --> 02:16:19,000
Are you in the survey you know there. are recall lines right people don't remember what they did.

823
02:16:19,000 --> 02:16:22,000
Oh, you believe they remember they don't want to tell you the truth. Right?

824
02:16:22,000 --> 02:16:26,000
That's the downside of retrospective or a service study right?

825
02:16:26,000 --> 02:16:30,000
For this study is not subject to that. because you run a mri yourself. right?

826
02:16:30,000 --> 02:16:34,000
There's no way i'm our machines give you something biased.

827
02:16:34,000 --> 02:16:41,000
So you know, in that sense. it's more like a perspective but you know it's. this is this is the way i'll say more publicly than necessary.

828
02:16:41,000 --> 02:16:46,000
So I I think you know, as long as you know what the question is up to is fine.

829
02:16:46,000 --> 02:16:53,000
The label doesn't really matter that much how much here

830
02:16:53,000 --> 02:17:06,000
Alright. So now that's now go to the table on the page 80, 86, and then, as for the null and alternative, So it's right here paid in 80.

831
02:17:06,000 --> 02:17:12,000
6 this table right? So there is a box here, a box here, so that, you know.

832
02:17:12,000 --> 02:17:21,000
Ask you to read the box. right? So the question is about state and all, and alternative hypothesis cause one of the information in the box and fully defined.

833
02:17:21,000 --> 02:17:25,000
You know any ladder and subscript right let's go back. Let's look at the first box.

834
02:17:25,000 --> 02:17:54,000
Right. So anyone wants to tell us how you read the box and you know what, for example, there's a key statistic, or there's a p value right? what exactly they're captain in the red box anyone

835
02:17:54,000 --> 02:18:05,000
I can go. We We talked about this one as the null hypothesis being that they would be sorry they're looking at years of football experience.

836
02:18:05,000 --> 02:18:20,000
Between the the 2 groups of interest. So the null hypothesis here would be that the mean number of years playing football would be the same between athletes with all the history of concussion and athletes

837
02:18:20,000 --> 02:18:24,000
with the history of congestion

838
02:18:24,000 --> 02:18:32,000
Thank you. Yeah. So this is the mean in the concussion group, or without concurrent group. it's a standard deviation.

839
02:18:32,000 --> 02:18:37,000
This is the common interval for the mean on the similar for the concussion group.

840
02:18:37,000 --> 02:18:44,000
And then you got a piece statistic you know the 46 is the degree of freedom and a piece of history.

841
02:18:44,000 --> 02:18:54,000
I I won't talk too much of all that yeah you know this. This is very much like a normal Did you do it just side a different and then I mean difference is the mean difference.

842
02:18:54,000 --> 02:18:58,000
And I gather with the common level, so on, so forth.

843
02:18:58,000 --> 02:19:02,000
Yeah, I apologize. I I do the watch on time so it's already passed on a great time.

844
02:19:02,000 --> 02:19:32,000
Let's take a 10 min break and then i'll continue afterwards

845
02:28:19,000 --> 02:28:26,000
Alright welcome back let's continue so there's no no!

846
02:28:26,000 --> 02:28:29,000
Have all the same, the 2 group have the same mean and no charge let's type it right.

847
02:28:29,000 --> 02:28:42,000
Only once the H. naught muse you want, because you who cares new ones with new to age, is u one not equal to a new 2.

848
02:28:42,000 --> 02:28:54,000
I have to define you one. Yes, the mean I mean, particularly depending on the particular point right?

849
02:28:54,000 --> 02:28:57,000
And this is the football experience. But here is reactant time.

850
02:28:57,000 --> 02:29:05,000
Here is people, cameras size, right volume. So depend on the management like I say the mean in group.

851
02:29:05,000 --> 02:29:10,000
One. right? New 2 is the in group 2, whatever group you are managing.

852
02:29:10,000 --> 02:29:23,000
So you have to spell all the sentence. Right? for example, mean a year experience in long time.

853
02:29:23,000 --> 02:29:30,000
Awesome, cool, correct, so you have to stop spelling off but that's the way you line it.

854
02:29:30,000 --> 02:29:33,000
I don't know how to go through because these are quite straightforward Right?

855
02:29:33,000 --> 02:29:36,000
The name of the Scheduler Pass used to determine the key value in the final follow.

856
02:29:36,000 --> 02:29:42,000
Here here. the Pv. will come from this p statistic right?

857
02:29:42,000 --> 02:29:49,000
So that that is exactly 2. Sam. Hold Keith. Okay, by default.

858
02:29:49,000 --> 02:29:54,000
Is this 2 sided you you can't write it but you don't have to buy it.

859
02:29:54,000 --> 02:30:00,000
That's by default. But if you do one side you have to write it on

860
02:30:00,000 --> 02:30:07,000
So usually you don't have to live and then finally right because we got a in this particular case.

861
02:30:07,000 --> 02:30:13,000
We got a piece statistic larger than 2 We got a configurable that does not include 0.

862
02:30:13,000 --> 02:30:18,000
We've got a P. by the left. 1 point o 5 so all of them are consistently saying, Right?

863
02:30:18,000 --> 02:30:25,000
So the computer will be key last time. point or 5.

864
02:30:25,000 --> 02:30:33,000
Reject the null right right I actually it's quite small.

865
02:30:33,000 --> 02:30:54,000
I, the strong half of this following good friends in the 2 means right along the full population or 2 groups, I would say strong evidence showing concussion is associated with you know, years of Pop.

866
02:30:54,000 --> 02:31:01,000
You know, years of football experience, and so on, so forth.

867
02:31:01,000 --> 02:31:05,000
So I am not going to do the other toolbox, but it just different.

868
02:31:05,000 --> 02:31:14,000
Variable, right different number. So one thing I could do

869
02:31:14,000 --> 02:31:27,000
It's. actually the us Okay, that's why i'm going to do But i'm saying I only do one of them.

870
02:31:27,000 --> 02:31:31,000
So this is the simulated data is right here so what's happening?

871
02:31:31,000 --> 02:31:42,000
Is that you know it happened many times. hold on a second

872
02:31:42,000 --> 02:31:48,000
Great

873
02:31:48,000 --> 02:31:53,000
When you have a paper like this right i'm in New York Taylor.

874
02:31:53,000 --> 02:31:58,000
It's not big right, just a 75 subject with a bunch of measurements.

875
02:31:58,000 --> 02:32:11,000
Right, and they post a lot of paper on the figure, but what happened is that when you want to get their wallet data and analyze yourself, it's often very hard to get if they don't provide as a part of the paper Oh.

876
02:32:11,000 --> 02:32:17,000
is a supplementary table. but when you ask the author often you don't get a response in time or gather response, you want.

877
02:32:17,000 --> 02:32:21,000
So what happens are my my colleagues, who want used to do the exercise.

878
02:32:21,000 --> 02:32:26,000
He just put and get the data from the authors. So he simulated the data himself.

879
02:32:26,000 --> 02:32:31,000
So that will happen. So this was the day like it simulated I'm.

880
02:32:31,000 --> 02:32:38,000
Then let's just use the similar data to run the analysis, right? We just do one which is a year synchronous.

881
02:32:38,000 --> 02:32:41,000
So we just copy these 2 column see congratulations is no one is.

882
02:32:41,000 --> 02:32:50,000
Yes, so that's 2 group and sorry poly here bye, Yeah, no.

883
02:32:50,000 --> 02:32:55,000
Because the 0 is is group one. And why is group 2? So I have to change the label here.

884
02:32:55,000 --> 02:32:59,000
Now the number all work call right, you see the 2 mean 2 standard deviation.

885
02:32:59,000 --> 02:33:01,000
The sample size is 30 24 instead of 25 here.

886
02:33:01,000 --> 02:33:09,000
I don't know why that's the case but nevertheless you got a difference that arrow he's the to say key value and call minimal right?

887
02:33:09,000 --> 02:33:15,000
Just like those who got in the table and because it's a similar data, right?

888
02:33:15,000 --> 02:33:17,000
So the the number from the data are sort of a similar to the buying.

889
02:33:17,000 --> 02:33:23,000
The payable, but not exactly the same or qualitatively, is the same.

890
02:33:23,000 --> 02:33:29,000
So you got this you should get these numbers right? So the keyword is still highly significant.

891
02:33:29,000 --> 02:33:37,000
Right the statistic is second any time. the common level doesn't include one. but this number are not the same as those in the paper, because it's not a rip.

892
02:33:37,000 --> 02:33:43,000
This is not a raw data, or you should nevertheless get these numbers in your analysis.

893
02:33:43,000 --> 02:33:57,000
Any question.

894
02:33:57,000 --> 02:34:02,000
So, as I said, I'm not going to do the other 2 column, but that's the same thing.

895
02:34:02,000 --> 02:34:09,000
Just copy paste in the payload. You got the number similar to the number show up here, but not the same.

896
02:34:09,000 --> 02:34:15,000
Okay, but the difference is that here you have a second number of key value.

897
02:34:15,000 --> 02:34:18,000
Here you have a new, significant key value, and here you have a second my P. value.

898
02:34:18,000 --> 02:34:23,000
Again, and that corresponds to here. The key statistic is last time, too.

899
02:34:23,000 --> 02:34:28,000
Here is again greater than 2, right, and here the common level you include 0 here.

900
02:34:28,000 --> 02:34:33,000
The company mode does not equals 0. So all these 3 should align right.

901
02:34:33,000 --> 02:34:36,000
We have a key larger than 2 or less than minus 2.

902
02:34:36,000 --> 02:34:39,000
The commonly number of does not include 0, and the pivot is less than 0 point 0.

903
02:34:39,000 --> 02:34:45,000
5, without what you should observe.

904
02:34:45,000 --> 02:34:51,000
Okay.

905
02:34:51,000 --> 02:35:00,000
Any question.

906
02:35:00,000 --> 02:35:04,000
I'm not going to do all 3 but they're all sort of the same right.

907
02:35:04,000 --> 02:35:10,000
Let's not move on to a question number 4 one of the function of the method.

908
02:35:10,000 --> 02:35:14,000
We use the Threeb: So basically to sample key tasks. Right?

909
02:35:14,000 --> 02:35:25,000
Remember the 2 sample, t has but zoom right basically what's saying that if the public are really normal, then the 2 sample here is the exact right.

910
02:35:25,000 --> 02:35:30,000
And now the question is that, based on February one?

911
02:35:30,000 --> 02:35:37,000
Do you think the normal assumption is? not it or not.

912
02:35:37,000 --> 02:35:50,000
Figure. one is here. so peak this is figure 2 i'm sorry

913
02:35:50,000 --> 02:35:54,000
Oh, this is Okay, Look at a bigger one.

914
02:35:54,000 --> 02:36:05,000
Right. You have to know how to read the figure. I think that circle and the squares are individual data points, because they only have, you know, 25 individual in school.

915
02:36:05,000 --> 02:36:10,000
They can afford to plot them right. and then these are the box blocks.

916
02:36:10,000 --> 02:36:13,000
They all in the box box. They show both of me on the media.

917
02:36:13,000 --> 02:36:18,000
I think the the follow line, the medium and the desktop are the mean.

918
02:36:18,000 --> 02:36:27,000
So? So what's Okay, do you think this distributions are normal or not, and why?

919
02:36:27,000 --> 02:36:36,000
I think the distribution for each group is a prop, 6 monthly normal.

920
02:36:36,000 --> 02:36:42,000
Because the we know that it cannot be exactly normal about the maintenance.

921
02:36:42,000 --> 02:36:49,000
We can have the same similar, very close foundation in the Box plot.

922
02:36:49,000 --> 02:37:07,000
And for each box parties I think I feel like it's kind of a metric along the y access and if you're looking to the symbols on the left side of each group spot a box, plot they feel they are more

923
02:37:07,000 --> 02:37:12,000
concentrated beside the me on near the me and the mice.

924
02:37:12,000 --> 02:37:20,000
Lisa is rebuilded in the tail around health, which is like the distribution of normal.

925
02:37:20,000 --> 02:37:26,000
Oh, distribution. Yeah, that's at my gas thank you very much Yeah, you're right, you know.

926
02:37:26,000 --> 02:37:38,000
There's no perfect normal, but in most of this you are they are not hugely skewed in this one skewed to the to the to the rifle through the pop, but the skewing is is not you know

927
02:37:38,000 --> 02:37:42,000
crazy that some of them are rather symmetric, right?

928
02:37:42,000 --> 02:37:50,000
I mean. Yeah. And then there are no crazy allliers right there. There's all letters, only sliding above the bar.

929
02:37:50,000 --> 02:37:56,000
Not too far, so part of it to be. I think these are, you know, reasonably normal.

930
02:37:56,000 --> 02:38:03,000
Yeah. And So the question is that they're originally normal. second thing is that this whole key hat no matter is one sample.

931
02:38:03,000 --> 02:38:09,000
All of it, although they depend on normal right the normality is not so crucial.

932
02:38:09,000 --> 02:38:13,000
That's not the sample size is large is not too small.

933
02:38:13,000 --> 02:38:20,000
And that's because i'm having a theorem right in this case the sample size in each group is 25 is above the 20.

934
02:38:20,000 --> 02:38:24,000
I talk about. Hold on. Quote large sample, right? I mean some people have different caught up.

935
02:38:24,000 --> 02:38:29,000
But personally, I think above 20 the key guys are pretty robust.

936
02:38:29,000 --> 02:38:33,000
Who nominal monopoly So We We don't really worry about the the normal assumption.

937
02:38:33,000 --> 02:38:46,000
If the sample size is less than 20 they can each rule the the the message is that, the central limit theorem do not apply those phase, but you cannot read, not really tell from 10 data points.

938
02:38:46,000 --> 02:38:49,000
Whether it's normal enough, unless there's a queue dollar right?

939
02:38:49,000 --> 02:38:56,000
So so. Nevertheless, you know he has his wide use for this purpose.

940
02:38:56,000 --> 02:39:13,000
Any questions, no comment.

941
02:39:13,000 --> 02:39:20,000
Okay, So I think that's all we have for the 7 A.

942
02:39:20,000 --> 02:39:26,000
Remember to turn in today, and the save 7 c and the quiz for tomorrow.

943
02:39:26,000 --> 02:39:33,000
And now let's quickly do 7 b together it's just a little more exercise.

944
02:39:33,000 --> 02:39:51,000
We do it together. So, guys, how do you quick

945
02:39:51,000 --> 02:39:57,000
Okay, So 7 B. is again about a medical expenditure data.

946
02:39:57,000 --> 02:40:00,000
And then ask us to look at the overall gender difference.

947
02:40:00,000 --> 02:40:04,000
In the mean of the full outcome and then state the hypothesis.

948
02:40:04,000 --> 02:40:22,000
That's that's the first part open i'm eps data

949
02:40:22,000 --> 02:40:30,000
But we are already familiar with this data. Well, nevertheless, there are 9,000 subjects, and then, with many variable measured on them.

950
02:40:30,000 --> 02:40:34,000
Right. So here to do 2 sample tasks. we need to have 2 variable.

951
02:40:34,000 --> 02:40:41,000
One is exposure which is supposed to be binary, defined to group the otherwise continuous.

952
02:40:41,000 --> 02:40:46,000
A call call right. So just like we're showing this test we need to talk.

953
02:40:46,000 --> 02:40:50,000
How? why is binary wise continue so in this particular case?

954
02:40:50,000 --> 02:40:59,000
Alright ask how to compare the general or gender difference in the mean of the falling outcome. So the exposure is gender.

955
02:40:59,000 --> 02:41:02,000
The all come out of the pool How's care expenditure.

956
02:41:02,000 --> 02:41:08,000
Hold on more biddy pmi and a that's new personal kind of health care expenditure.

957
02:41:08,000 --> 02:41:31,000
Right. So what we need is first take the render group which is sax Here there's no gender, so we take the sex, and let's call here paste in the group column and then go back to take house

958
02:41:31,000 --> 02:41:42,000
expenditure. whole house expenditure hello house expanded. I think it's this column total expenditure.

959
02:41:42,000 --> 02:41:49,000
We go back and pays the second one. Now the tool group has 0 and one we see.

960
02:41:49,000 --> 02:41:55,000
The sample size is 5,000 first group, second, 4,000, s group, all together. 9,000. right?

961
02:41:55,000 --> 02:41:59,000
So the 0 one are not clear. so we need to look at what they mean.

962
02:41:59,000 --> 02:42:04,000
Right through in target. we go back. We look at a dictionary.

963
02:42:04,000 --> 02:42:12,000
So 0 is female. One is male so let's go back on, put them here.

964
02:42:12,000 --> 02:42:19,000
So 0. this is a female. one is male. So now we see this is in thousands of dollars, right?

965
02:42:19,000 --> 02:42:28,000
So female has slightly higher total health care, expenditure in the mail, and then standard deviation and sample size.

966
02:42:28,000 --> 02:42:33,000
So with this large of sample size, this sample that is in 1,000, right, really large.

967
02:42:33,000 --> 02:42:37,000
So you should have very, very high power to detect a very little difference.

968
02:42:37,000 --> 02:42:45,000
So that's the that's the place I want to re-adphasize right statistical secondly, because versus practical significance.

969
02:42:45,000 --> 02:42:52,000
Oh, scientific, With such high power we have there with such high sample size, we have very high power.

970
02:42:52,000 --> 02:43:06,000
So we should be able to pick up a little difference so the difference you have to look at here right So here means that email average spend more that $1,000 per year on house carries Spanish or maybe by year.

971
02:43:06,000 --> 02:43:11,000
I don't know Well, this is a lifetime let's look at it.

972
02:43:11,000 --> 02:43:16,000
I mean. Why, why, here is a little too hard. Right I was Carrie Sandy.

973
02:43:16,000 --> 02:43:24,000
True how's the house guy spanish or in 1,000 I don't know what I see year or lifetime.

974
02:43:24,000 --> 02:43:28,000
Let's take it as a I I think annually it seems too hard.

975
02:43:28,000 --> 02:43:36,000
But nevertheless, let's assume it's annually the female average spend more than $1,000 in total healthcare manager.

976
02:43:36,000 --> 02:43:44,000
Practically, I find that will be significant, Right? so that determined the practical significance.

977
02:43:44,000 --> 02:43:49,000
And then, apparently the P value is significant because we have very high power.

978
02:43:49,000 --> 02:43:56,000
Right. So the confidence in the vote right? So So the 1,000 is A is a point Sd.

979
02:43:56,000 --> 02:44:00,000
Made by really range from $370 to $1,900.

980
02:44:00,000 --> 02:44:07,000
So that's a that's thanks okay I invitation here any question.

981
02:44:07,000 --> 02:44:15,000
So Now you have to think about right whether it's 1,000 actually $1,150 difference between 2 gender group.

982
02:44:15,000 --> 02:44:20,000
First of all is that significant? Is that important right to know it.

983
02:44:20,000 --> 02:44:28,000
Second is that is that real right? Basically, that goes back to whether the study is is is biased.

984
02:44:28,000 --> 02:44:36,000
I have to think about how people collect this data where the sample came from, right.

985
02:44:36,000 --> 02:44:43,000
If a random sample, then if it's a rough thing or random sample, then the difference is real.

986
02:44:43,000 --> 02:44:55,000
Otherwise there could be some some other bias right, for example, if they only, you know, collect people who are older.

987
02:44:55,000 --> 02:44:58,000
I know we know female has a higher level of expectancy.

988
02:44:58,000 --> 02:45:03,000
Then there are more elderly, you know women that men in the study.

989
02:45:03,000 --> 02:45:15,000
Now, of course, when you're older, you spend more right on health care, so really the interpretation is wild, you have to think about how the study, how the subject are recruited to interpret the difference. But statistically.

990
02:45:15,000 --> 02:45:28,000
The difference is significant, both statistically also, you know scientifically, any question

991
02:45:28,000 --> 02:45:35,000
Alright. Now let's move on to the second the second ask for about total mobility.

992
02:45:35,000 --> 02:45:52,000
I I I find it more fine or more, I would say, The form of it is that before you even look at the data, you form your own hypothesis, you know, Matt or women which one has higher in for example, total mobility.

993
02:45:52,000 --> 02:45:58,000
or bi or age, right? and once you have you know halfway you look at the data.

994
02:45:58,000 --> 02:46:05,000
That's much better than just now usually look at the data, because if you look at the data, whatever difference you find you will find explanation to it.

995
02:46:05,000 --> 02:46:11,000
People I mean humor are very good at making explanations after you see the result.

996
02:46:11,000 --> 02:46:15,000
I'll read if you form a habit beforehand that's more trustworthy.

997
02:46:15,000 --> 02:46:20,000
So let's look at the data let's now look at the total morebedded.

998
02:46:20,000 --> 02:46:29,000
He hold on Monday this column, and then come back here.

999
02:46:29,000 --> 02:46:36,000
We just call we just type the second column because the gender doesn't change, but the polar ho of morbidity is like a integer.

1000
02:46:36,000 --> 02:46:49,000
How many mobility right? And now we see the difference. So this time, again, email has more call morbidity.

1001
02:46:49,000 --> 02:46:58,000
But the difference is 0 point 1. so I find that although it's again highly significant, even more significant than the previous one right because of some of that is huge.

1002
02:46:58,000 --> 02:47:05,000
But personally I find this difference last imported or less significant than the previous one.

1003
02:47:05,000 --> 02:47:12,000
Right, because point one more ability. it's hardily in carbon right?

1004
02:47:12,000 --> 02:47:19,000
Not, you know, $1,000 a year. So, although there is a difference.

1005
02:47:19,000 --> 02:47:26,000
But the difference, or last, I would say, scientifically or clinically or practically significant than the previous part.

1006
02:47:26,000 --> 02:47:36,000
That's my personal head any comma on this

1007
02:47:36,000 --> 02:47:42,000
And now let's look at the the third Wise vm on that's go.

1008
02:47:42,000 --> 02:47:51,000
Look at Bmi again. you can form your hypothesis and then use the data to back out of time.

1009
02:47:51,000 --> 02:47:57,000
You pay post to be? Am I here? You see? Okay, women have slightly higher.

1010
02:47:57,000 --> 02:48:03,000
Vmi. But the difference is 0 point 5 5 again it's highly significant.

1011
02:48:03,000 --> 02:48:09,000
But I personally don't find this to be very important.

1012
02:48:09,000 --> 02:48:18,000
So the Dmi offline 0 point 5. Honestly, my, my, my my own Bmi can vary point 5 from month to month. Right?

1013
02:48:18,000 --> 02:48:28,000
So is, is a small fluctuation. So if if one group has 5 higher mean, and the other group doesn't remain in too much.

1014
02:48:28,000 --> 02:48:34,000
For in the video. So personally, I don't find it to be very second name.

1015
02:48:34,000 --> 02:48:41,000
Yes, the Peabody is highly secondly right so that's probably mostly driven by the large sample size.

1016
02:48:41,000 --> 02:48:51,000
Okay, remind yourself right. A very tiny pivotal do not really mean the higher scientific significance.

1017
02:48:51,000 --> 02:48:57,000
Finally, we take 8

1018
02:48:57,000 --> 02:49:07,000
Okay, So the female are on average one year older than male in the study.

1019
02:49:07,000 --> 02:49:12,000
It's on the margin for myself to call it sorry to give you a second right.

1020
02:49:12,000 --> 02:49:25,000
However, one you're older so it's not as much as I would expected, you know the difference we know in life expected expectancy between men and women, or maybe 5 years right or around that right?

1021
02:49:25,000 --> 02:49:32,000
So one. Your difference in the this 2 population is not that significant compared with the 5 year difference in life expectancy?

1022
02:49:32,000 --> 02:49:38,000
But still this is more than I would say negligible right it's why.

1023
02:49:38,000 --> 02:49:53,000
Here 1.6, 1.6 years and again. it's high, probably because sample size, too, big for my conclusion is that we are a little bit overpower for this study.

1024
02:49:53,000 --> 02:49:58,000
So our power in this study is probably very, very hard, 99% higher.

1025
02:49:58,000 --> 02:50:02,000
So we are going to pick up just a tiny difference in the 2 population.

1026
02:50:02,000 --> 02:50:11,000
So one of the tiny difference is significant. Now, in practice is up for you to to charge based on knowledge.

1027
02:50:11,000 --> 02:50:17,000
Okay, Any questions here.

1028
02:50:17,000 --> 02:50:21,000
I have a question just theoretical you mentioned before that.

1029
02:50:21,000 --> 02:50:28,000
There's I know that the we won't go into this necessarily but there are ways to calculate before your when you're designing a study.

1030
02:50:28,000 --> 02:50:34,000
How much? What kind of sample size you need to detect a certain to give it a certain power.

1031
02:50:34,000 --> 02:50:39,000
So a sort of a minimum sample size, can you also determine ahead of time.

1032
02:50:39,000 --> 02:50:44,000
If you're gonna be overpowered like this Yeah, Yeah, absolutely.

1033
02:50:44,000 --> 02:50:49,000
I mean, you need a for 4 quantity to compute the power right? the type on error rate, which is 0 point 0, 5. Yeah.

1034
02:50:49,000 --> 02:50:55,000
And then the sample sites, which are the normal right? and then the variability you pretty much can guess. Right?

1035
02:50:55,000 --> 02:50:59,000
Finally the impact 5. the difference between men and women. You can guess.

1036
02:50:59,000 --> 02:51:02,000
Once you guess all the phone number. you can cover the power right.

1037
02:51:02,000 --> 02:51:10,000
If I have your power power, you've got like a 99.9% power, you will know almost for sure you've got a few out of very tiny.

1038
02:51:10,000 --> 02:51:19,000
So at that moment it's not very useful to capital pivot at all right, because they're overpowered.

1039
02:51:19,000 --> 02:51:27,000
Thank you, and if you're overpowered what would you recommend using instead of the p-value, I know that it's controversial.

1040
02:51:27,000 --> 02:51:33,000
No you you can still do the pivot it's just not very important, because before him do it you know the keyword will gonna be small.

1041
02:51:33,000 --> 02:51:38,000
Right, so it doesn't provide any additional information rather if you're overpowered.

1042
02:51:38,000 --> 02:51:42,000
I think the difference is self right it's it's more interesting.

1043
02:51:42,000 --> 02:51:47,000
I look at the difference whether to see you. stuff is, you know, interesting or not.

1044
02:51:47,000 --> 02:51:51,000
Right. Say, why your difference? I mean, I personally find this 1 1.

1045
02:51:51,000 --> 02:51:56,000
Your difference in this study is is not interesting, because, first of all, the study may be biased.

1046
02:51:56,000 --> 02:51:59,000
But this may not, may have nothing to do with the population, but rather the way in sample.

1047
02:51:59,000 --> 02:52:13,000
The second is that even if it's a population difference one your difference in age? Is not that striking for me? So it's basically it's not a news had or something right? So i'll i'll look for something more

1048
02:52:13,000 --> 02:52:21,000
interesting. Scientifically, I think the the total expenditure is the most interesting to me.

1049
02:52:21,000 --> 02:52:29,000
Say, if you, if you claim that the women spend on average $1,000 per year on healthcare, I find it out to be a little impressive.

1050
02:52:29,000 --> 02:52:36,000
You but you have to make sure that doesn't come from the sampling wise.

1051
02:52:36,000 --> 02:52:45,000
Thank you, you're welcome, I have a question it's not exactly related to the 2 sample ttas.

1052
02:52:45,000 --> 02:52:56,000
But here we're looking at differences between sex right men and women And so i'm wondering if we're gonna learn it all about statistical test that we can use that maybe are more inclusive of just

1053
02:52:56,000 --> 02:53:00,000
different genders. So I know, obviously here we have to have a group, A and a group B.

1054
02:53:00,000 --> 02:53:11,000
But folks identify outside of female or male, so will we learn about like how to possibly look at gender differences outside of a binary perfect question.

1055
02:53:11,000 --> 02:53:19,000
Actually, the second top of this lecture is about one way and Nova, which is exactly trying to compare more than 2 groups.

1056
02:53:19,000 --> 02:53:28,000
Cool. So gonna do the actual I I don't think we're going to do the access on different gender, because we don't have the data here. But we're gonna do different access on different regions and different ways.

1057
02:53:28,000 --> 02:53:38,000
So that's the same up concept. but i'll have to point out one thing. there's a of course we try to be inclusive, right, but everything has a price.

1058
02:53:38,000 --> 02:53:43,000
So the thing is that we'll compare different you know different gender or different racial groups right?

1059
02:53:43,000 --> 02:53:50,000
It's hard to have equal sample sign I can see that you may have a lot of men and women behind very little other.

1060
02:53:50,000 --> 02:53:54,000
You know gender groups we have little samples that you have a queue of variability.

1061
02:53:54,000 --> 02:53:58,000
There, So you'll pay a price there I I will talk about later.

1062
02:53:58,000 --> 02:54:01,000
So you know, practically we want to compare all of them.

1063
02:54:01,000 --> 02:54:08,000
But you know, mathematically sometimes it's hard because we don't have large samples on.

1064
02:54:08,000 --> 02:54:21,000
But that's a very good point. thank you for asking any other questions regarding this, So far

1065
02:54:21,000 --> 02:54:27,000
Okay, so that's you know I i'm not gonna write down.

1066
02:54:27,000 --> 02:54:31,000
But for each of the tasks there's a no I have there's a turtle here right now.

1067
02:54:31,000 --> 02:54:33,000
That is still time of tasks, and so on, so forth.

1068
02:54:33,000 --> 02:54:36,000
You know how the riding and on the mobile.

1069
02:54:36,000 --> 02:54:43,000
The next question. Right? so repeat the question one but now you're interesting.

1070
02:54:43,000 --> 02:54:50,000
Every difference between people with and without moderate or vigorous physical exercise, at least 5 times a week.

1071
02:54:50,000 --> 02:54:54,000
This is going to show you how the way convert a variable into binary.

1072
02:54:54,000 --> 02:55:04,000
So we still compare the 4 different outcome but Now the 2 groups are people with at least 5 times of a exercise versus which have not.

1073
02:55:04,000 --> 02:55:09,000
So let's see how do we do that so there's a group here.

1074
02:55:09,000 --> 02:55:29,000
It's called physical exercise, right and let's see no, not not physical health, physical access size.

1075
02:55:29,000 --> 02:55:34,000
Where is it?

1076
02:55:34,000 --> 02:55:39,000
Yeah, here, please don't have to stop right he's more than 5 or less than 5.

1077
02:55:39,000 --> 02:55:53,000
So we take the noise. know yourself that's not fine Yes, it's more than 5, so we take column G.

1078
02:55:53,000 --> 02:56:03,000
Right here. So we go to the spreadsheet

1079
02:56:03,000 --> 02:56:08,000
Before we are asking the difference between men and women, what will be fine?

1080
02:56:08,000 --> 02:56:15,000
I personally don't find it to be very striking also I suspect it's due to the company by now for physical access.

1081
02:56:15,000 --> 02:56:26,000
Things are becoming more interesting because you all of us, probably have a strong assumption of other association between physical exercise and some of the outcome.

1082
02:56:26,000 --> 02:56:35,000
Here, right. let's see what our assumption will hold so let's go to physical exercise.

1083
02:56:35,000 --> 02:56:44,000
We paste it here. So now this means no exercise. This means you exercise right.

1084
02:56:44,000 --> 02:56:51,000
This is on total health expenditure. So with that, no group has a lot higher.

1085
02:56:51,000 --> 02:57:02,000
Owner, health care, expenditure, $3,000 a more, which is a much bigger difference between gender and also, although there still could be some money.

1086
02:57:02,000 --> 02:57:06,000
Wise. Personally, I I think it makes a lot more sense.

1087
02:57:06,000 --> 02:57:23,000
Right. People exercise a lot. Tend to be healthier and will spend less right? So that's why i'm saying forming on how part of this before you see your data. give you a lot more confidence, because I want to come from your background

1088
02:57:23,000 --> 02:57:31,000
knowledge that already give you some prior knowledge. And then, if the data conforms, we all have all this that just give you more evidence.

1089
02:57:31,000 --> 02:57:35,000
So the pea is always significant, so that's not interesting or rather the difference here and now.

1090
02:57:35,000 --> 02:57:44,000
Let's do it very quickly. second one is the let's just do it, Bmi.

1091
02:57:44,000 --> 02:57:54,000
You can imagine what the outcome would be right. So the exercise group has 1.5 days lower in Dmi.

1092
02:57:54,000 --> 02:58:02,000
This is a a smaller difference, as I would Imagine Well, now with the last it's on the same direction.

1093
02:58:02,000 --> 02:58:10,000
Right exercise group has lower Vmi, and then the

1094
02:58:10,000 --> 02:58:16,000
The total morbidity you again can imagine while i'm your mighty fact.

1095
02:58:16,000 --> 02:58:21,000
Now the exercise group has the lower mobility. The difference is about Point 5.

1096
02:58:21,000 --> 02:58:25,000
Remember the previous gender difference of point one now it's point 5.

1097
02:58:25,000 --> 02:58:28,000
Right. it's still harder to interpret the other outcome.

1098
02:58:28,000 --> 02:58:35,000
I personally find it to be much bigger than the thender difference.

1099
02:58:35,000 --> 02:58:39,000
And finally you look at H. Now the ages interesting, right?

1100
02:58:39,000 --> 02:58:48,000
Because they have a page on age difference it's very tricky let's look at age.

1101
02:58:48,000 --> 02:58:55,000
Put it back here. so before you see it, you might imagine which group has higher rate or lower head.

1102
02:58:55,000 --> 02:58:59,000
It turned all that the exercise group is younger by 3 years old.

1103
02:58:59,000 --> 02:59:05,000
Again. This is much larger than a one year old agenda.

1104
02:59:05,000 --> 02:59:11,000
So now the interpretation will become tricky. So of course, you know there is a difference right?

1105
02:59:11,000 --> 02:59:15,000
Putting the sampling bias aside, there is a difference between the 2 group.

1106
02:59:15,000 --> 02:59:22,000
Yeah, this association, so we can only say, call, but by no means exercise anymore.

1107
02:59:22,000 --> 02:59:30,000
Make you younger. That cannot possibly be the pay right because age doesn't depend on other thing. it's a intrinsic poverty.

1108
02:59:30,000 --> 02:59:37,000
So one possible, you know. possibility is that younger people tend to exercise more.

1109
02:59:37,000 --> 02:59:43,000
So I think that might make sense right. So the association might be able to in interpret.

1110
02:59:43,000 --> 02:59:49,000
As you know, younger people access more rather access to make you younger but that's number one.

1111
02:59:49,000 --> 03:00:02,000
Number 2 is that we know there's a difference you know in in Bmi, in housecare, expanding in homogeneity between the exercise and not accessible.

1112
03:00:02,000 --> 03:00:16,000
But now we also notice the exercise group are younger. So now the tricky thing is that do you really believe the difference in in the other 3 outcome might be caused by the exercise or not?

1113
03:00:16,000 --> 03:00:20,000
It could be, could be exercise make you healthier, so do you spend less?

1114
03:00:20,000 --> 03:00:25,000
Your you know less lower, Bmi. but it also be maybe just because this group is younger.

1115
03:00:25,000 --> 03:00:36,000
So they are healthier anyway. they spend, last anyway. right maybe maybe the access doesn't do anything at all, just because this group is younger.

1116
03:00:36,000 --> 03:00:46,000
Therefore they they are lower ubi they are healthier right So that's that's the hardness of that's the difficulty of using a visual study to draw any causal thing you can make a 1,000 different

1117
03:00:46,000 --> 03:00:53,000
hypotheses. But in yeah, you cannot rule out confounding

1118
03:00:53,000 --> 03:01:01,000
Right before we see the age, We might be very happy with our data. con conform to our hypothesis.

1119
03:01:01,000 --> 03:01:08,000
That exercise make you healthy? Oh, absolutely see the age we started out, or something, or or how about it? really?

1120
03:01:08,000 --> 03:01:15,000
Hold. But what if it's not cost by 8 what they've caused by founding out you have a thought about right?

1121
03:01:15,000 --> 03:01:19,000
So the the typical is not confounders that you can imagine, but rather confound us.

1122
03:01:19,000 --> 03:01:24,000
You didn't even expect which is called a unmeasured component. right?

1123
03:01:24,000 --> 03:01:30,000
So this study in this study they collect like a 20 of 30 different variable.

1124
03:01:30,000 --> 03:01:34,000
They collect every single variable they can see. that could be a component.

1125
03:01:34,000 --> 03:01:40,000
Oh, but there are like a meeting different how variable out there What if there's a compiler you never thought about.

1126
03:01:40,000 --> 03:01:49,000
You cannot really rule all things. you have a download right so i'm, measured, a confounder is a crucial issue for any causal inference, you know.

1127
03:01:49,000 --> 03:01:58,000
Observational study. So you know, although observational study can cannot generally in for a puzzle, but people do it widely.

1128
03:01:58,000 --> 03:02:01,000
So people try to steal you for something causal from a dimensional study.

1129
03:02:01,000 --> 03:02:05,000
There are many matters how there is active research view you, many of the framework.

1130
03:02:05,000 --> 03:02:14,000
They make further assumptions right? One of the why did he make made assumption is that there is no on manager to confound them right.

1131
03:02:14,000 --> 03:02:18,000
If you assume all the confounded already included in the data.

1132
03:02:18,000 --> 03:02:20,000
Now you can do something, but that the phone is hard to check right.

1133
03:02:20,000 --> 03:02:24,000
How do you know you don't have a major compounder

1134
03:02:24,000 --> 03:02:27,000
You can think very hard to include every possible compiler, you know.

1135
03:02:27,000 --> 03:02:32,000
Variable measurement. I cannot rule out. There are things you never thought about right.

1136
03:02:32,000 --> 03:02:37,000
So those assumptions are sort of a shaky that's why, you know, always be cautious.

1137
03:02:37,000 --> 03:02:45,000
I'm not saying he's it's impossible but always be cautious about drawing any causal conclusion using of the original study.

1138
03:02:45,000 --> 03:02:50,000
There's always caveats

1139
03:02:50,000 --> 03:03:08,000
Any question here

1140
03:03:08,000 --> 03:03:16,000
Yeah, next week we will talk about if we consider age and also automatically.

1141
03:03:16,000 --> 03:03:28,000
I'll come we will try to model them all the guy that will see whether the age difference can composed explain the difference in other outcome, or only partially, that way.

1142
03:03:28,000 --> 03:03:35,000
We can adjust for age. and look at the difference in physical exercise right now require a more sophisticated model.

1143
03:03:35,000 --> 03:03:41,000
We call the multiple in your regression which we'll talk about next week up here I just give you a head right? Why, we need that.

1144
03:03:41,000 --> 03:03:58,000
Because when you have more than we have more than 2 variables, who some of he has is not sufficient; who analyze them all together.

1145
03:03:58,000 --> 03:04:02,000
Alright, i'll give you 30 s to thinking and ask me any questions.

1146
03:04:02,000 --> 03:04:31,000
If you have

1147
03:04:31,000 --> 03:04:39,000
Okay.

1148
03:04:39,000 --> 03:04:44,000
So for the rest of the time today, we'll start a little bit on the rest of the lecture, 7 because it's a second half.

1149
03:04:44,000 --> 03:04:50,000
It's a little longer. so we save some time for tomorrow we just talk about 2 sample key guys where we have 2 population.

1150
03:04:50,000 --> 03:04:57,000
We compare the difference in the need? Now, the question is that what if you have 3 or 5 populations?

1151
03:04:57,000 --> 03:05:03,000
What do you do? and that's still called analysis of errands or anoma?

1152
03:05:03,000 --> 03:05:09,000
Don't be fooled by the name right it's called analysis of errors, but really is comparing the mean.

1153
03:05:09,000 --> 03:05:16,000
I'll explain a little bit why they do that they actually use variance to analyze on a meeting.

1154
03:05:16,000 --> 03:05:23,000
Okay, this is one way a nova When you don't say one way is one way or no one means that one variable.

1155
03:05:23,000 --> 03:05:27,000
Do you find the mean? Define the different groups, give it 2 variables.

1156
03:05:27,000 --> 03:05:30,000
Then you have a sort of a you know, 2 way design.

1157
03:05:30,000 --> 03:05:32,000
Then you have 2 way on Nova, which is, Be honest.

1158
03:05:32,000 --> 03:05:37,000
Call here for simplicity. We have one way of normal.

1159
03:05:37,000 --> 03:05:39,000
So one way to know is the extension of 2 samples.

1160
03:05:39,000 --> 03:05:46,000
He has. You have more than 2 groups

1161
03:05:46,000 --> 03:05:51,000
So now, suppose we have a total of K population so typically K.

1162
03:05:51,000 --> 03:05:58,000
Larger than 2 by the 2 sample it has is a special case of one we are know what, Therefore, even if K.

1163
03:05:58,000 --> 03:06:07,000
If 2 you have still applied by way of no one, it just. in that case you should get the same conclusion as the 2 sample guidance.

1164
03:06:07,000 --> 03:06:18,000
So we have K. population. You know this thing from each other, which means one subject can only come from one and only one population.

1165
03:06:18,000 --> 03:06:22,000
And then you want to compare whether they are the same or not.

1166
03:06:22,000 --> 03:06:26,000
So each population you could have a mean call, the new one mute out to me.

1167
03:06:26,000 --> 03:06:36,000
Okay, that's the mean measurement in that population. so we form one. No hypothesis of this test under the No, I have this.

1168
03:06:36,000 --> 03:06:43,000
All the public have the same meeting which I mean they're all equal.

1169
03:06:43,000 --> 03:06:50,000
So remember the the opposite of the null is the alternative.

1170
03:06:50,000 --> 03:06:55,000
So. the alternative here is that at least 2 of them are not equal. Be careful.

1171
03:06:55,000 --> 03:07:00,000
This is not the same thing. All of them are not equal, so you could have.

1172
03:07:00,000 --> 03:07:05,000
So you have you have like 10 group right you could have 9 of them have equal mean.

1173
03:07:05,000 --> 03:07:11,000
But the last one the pen doesn't have an equal meeting that's still alternative

1174
03:07:11,000 --> 03:07:20,000
No 2 or more of the population. Means are unequal, so at least 2 population have on you call me so.

1175
03:07:20,000 --> 03:07:26,000
This is a very general authority. Anything other than no is the alternative.

1176
03:07:26,000 --> 03:07:39,000
So, even if you can conclude the alternative it's true you still don't know which one is not equal, from which one you can do something called a post office to figure that out i'll talk about later.

1177
03:07:39,000 --> 03:07:43,000
But at least, for now we only care about whether all the group are equal or not.

1178
03:07:43,000 --> 03:07:46,000
This is the case where you have one variable, you find the group indicator.

1179
03:07:46,000 --> 03:07:51,000
The other variables outcome. You are plotting the association between these 2, variable.

1180
03:07:51,000 --> 03:07:59,000
So essentially the association between the categorical, variable, and a continuous variable right at the guard of our define the group and the continue variable.

1181
03:07:59,000 --> 03:08:10,000
Defined the outcome

1182
03:08:10,000 --> 03:08:16,000
That's go back to the all that and you know sugary beverage. sort of a study, remember?

1183
03:08:16,000 --> 03:08:22,000
Previously we have only 2 population. The first population is those with less than one divergence.

1184
03:08:22,000 --> 03:08:26,000
Day. second is one to 2. We were missing people with more than 2 batteries.

1185
03:08:26,000 --> 03:08:40,000
Now we make it up, how we define 3 population by the third holiday of being those taking more than 2 average a day with this 3 population, we have new one meal commute free.

1186
03:08:40,000 --> 03:08:56,000
So if drinking sugary beverage has nothing to do with your cap, then the 3 population will be the same, then new one, so that you call the bye, if any of the 2 are not equal then the 2

1187
03:08:56,000 --> 03:09:06,000
variable, are associated.

1188
03:09:06,000 --> 03:09:15,000
So now, how about this? Here is H. Naught is new wine, cause mu 2 equals mu 3, and a alternative is that at least 2 of them are not equal at least 2.

1189
03:09:15,000 --> 03:09:19,000
It could be all 3 on our ego but we don't need that.

1190
03:09:19,000 --> 03:09:29,000
Another way of writing It is it's new white not equal to mu 2 and or or me one not equal to mu 3 or me one not equal to mu 3.

1191
03:09:29,000 --> 03:09:33,000
Apparently You don't want to write this when you have like a 4 or 5 up, you know 6 schools right.

1192
03:09:33,000 --> 03:09:37,000
That'll be too long. so that's why we say at least 2 of them are equal.

1193
03:09:37,000 --> 03:09:46,000
But this is the right way of writing it. nevertheless.

1194
03:09:46,000 --> 03:09:51,000
So this this way of running a target give you a hint.

1195
03:09:51,000 --> 03:09:58,000
Now you could actually do this whole thing by doing 3 to some of you have right.

1196
03:09:58,000 --> 03:10:02,000
You could pass new wires of mute to then has new hours of the meal.

1197
03:10:02,000 --> 03:10:07,000
3 then, has muttered to the mule 3, and see if any of them in reject it.

1198
03:10:07,000 --> 03:10:13,000
You could do that. that's 4 of them equivalent to what I do here.

1199
03:10:13,000 --> 03:10:24,000
But I will talk about Actually, there are some subtle different i'll talk about the difference later on the The main difference is that if you test a Nova like this, you are running one path.

1200
03:10:24,000 --> 03:10:27,000
If you do 3 tests like this, you are running 3 times.

1201
03:10:27,000 --> 03:10:33,000
You have a multiple testing issue which go back to the thing I talk about already on where you do model what's happening.

1202
03:10:33,000 --> 03:10:39,000
You are risking, implating a high one. Era

1203
03:10:39,000 --> 03:10:46,000
So that's why we do you know one path using this a Nova framework.

1204
03:10:46,000 --> 03:10:54,000
Any question.

1205
03:10:54,000 --> 03:11:02,000
I have a question. So how is it that multiple pairways comparisons will increase the type?

1206
03:11:02,000 --> 03:11:08,000
One error. Yeah. perfect question. I I will talk more about this in the later.

1207
03:11:08,000 --> 03:11:13,000
But the idea is that the whole framework of our passing is to control type.

1208
03:11:13,000 --> 03:11:28,000
One error at 0 point 0 5 and this whole process will work only when you just run it once, because you're running the ones you have 5% who pipe on our way.

1209
03:11:28,000 --> 03:11:34,000
If you do it twice the chance you will make one type of error Among 2 trials we'll almost double.

1210
03:11:34,000 --> 03:11:43,000
You have, like a 10% type of error rate, near 10%, because we have to consider a case, you know, both of and so on, is complicated.

1211
03:11:43,000 --> 03:11:46,000
But nearly 10%. You just have more time for making a type of arrow.

1212
03:11:46,000 --> 03:11:55,000
If you do it again again again. So that's the basic concept i'll have some some more explanation later.

1213
03:11:55,000 --> 03:12:11,000
Okay. Good. Thank you. Any other question

1214
03:12:11,000 --> 03:12:22,000
Okay, So let's see how do we do one more now. it's a little technical, but you know again go always going back to the phone the month. right?

1215
03:12:22,000 --> 03:12:27,000
We're trying to quantify a signal no require trying to quantify a noise.

1216
03:12:27,000 --> 03:12:32,000
We're trying to qualify the youpac measuring the thing We're part of the artifact.

1217
03:12:32,000 --> 03:12:37,000
Now we measure the variability

1218
03:12:37,000 --> 03:12:45,000
Okay, So just like the 2, some of you guys, Now we collect data from the 3 population.

1219
03:12:45,000 --> 03:12:48,000
So for each population we can calculate the someomium.

1220
03:12:48,000 --> 03:12:54,000
Why we would draw a sample we got covered with sample mean Samuel Standard deviation.

1221
03:12:54,000 --> 03:12:57,000
This is the mean is the standard deviation, and then we have a different sample size.

1222
03:12:57,000 --> 03:13:05,000
Alright. So again, I have 3 numbers for each group, but we have 9 out 9, number but total 3 groups.

1223
03:13:05,000 --> 03:13:13,000
So the goal is to use this 9 numbers to figure out the way to calculate some statistic.

1224
03:13:13,000 --> 03:13:19,000
You have this thing about the way of generalizing p statistics but turns out that's hard.

1225
03:13:19,000 --> 03:13:27,000
So now we're going to jump into a sort of a seemingly totally different statistics.

1226
03:13:27,000 --> 03:13:33,000
But the basic idea is still the same

1227
03:13:33,000 --> 03:13:40,000
So let's think about it. The way we do it is that we always come here.

1228
03:13:40,000 --> 03:13:47,000
This statistic right We week we compute the Peabody under the no head of this.

1229
03:13:47,000 --> 03:13:51,000
So I don't know how did all the 3 group population should have the same mean.

1230
03:13:51,000 --> 03:13:58,000
So only reason we see 3 different meaning in some ways because some of our available.

1231
03:13:58,000 --> 03:14:03,000
You're not that under the null hypothesis the 3 published have the same mean.

1232
03:14:03,000 --> 03:14:13,000
Now, what's the best estimate of this overall common mean or kind of grand mean of the 3 population under the know would be that would just pull the data together right?

1233
03:14:13,000 --> 03:14:16,000
Because I don't know now they are the same we put them together.

1234
03:14:16,000 --> 03:14:24,000
Ask me the grandmother, So the way that I have, you know, moderate a mean by the sample size and divided by the sample size.

1235
03:14:24,000 --> 03:14:30,000
So here, you know the sum of size was too. big in thousands it's hard for us to do the calculation.

1236
03:14:30,000 --> 03:14:34,000
So here, with simplified, we assume the sample has a much smaller.

1237
03:14:34,000 --> 03:14:37,000
Oh, they are the same story bye. we keep the me understand Dvd.

1238
03:14:37,000 --> 03:14:40,000
That's assumed the sample size were much smaller to begin with.

1239
03:14:40,000 --> 03:14:47,000
So the way we do that Grammy is basically modeled by the mean, by the sample size and the atom up divided by the total sample size.

1240
03:14:47,000 --> 03:14:51,000
This way we got a brand name of about 1,900.

1241
03:14:51,000 --> 03:14:58,000
You see it on a 99 or light in the middle of these 3 numbers, so that's the mean you would estimate when you assume that now have other.

1242
03:14:58,000 --> 03:15:03,000
This is true

1243
03:15:03,000 --> 03:15:08,000
So that there is that if the know about it is true, which means all the probably have the same mean. then this is our best thing.

1244
03:15:08,000 --> 03:15:22,000
I have best estimate for the overall mean, which means the only reason we see the graduation is because companies are available

1245
03:15:22,000 --> 03:15:30,000
So the 100 just for convenience 100, whereas 100 the the reduce the sample size.

1246
03:15:30,000 --> 03:15:39,000
Oh, bio back to 100. Yeah. I mean, this is the point we are consuming. The data are really poor, Right? In reality.

1247
03:15:39,000 --> 03:15:44,000
You have to modify by this time there, but you pretty much got similar numbers here.

1248
03:15:44,000 --> 03:15:50,000
Now

1249
03:15:50,000 --> 03:15:58,000
Yeah, i'm saying you're in real analysis you don't do this here just for your illustration

1250
03:15:58,000 --> 03:16:07,000
And this is a bigger right So this yeah that's the line is the grammar.

1251
03:16:07,000 --> 03:16:11,000
And the blue bars are the 3 group mean for the 3 groups right?

1252
03:16:11,000 --> 03:16:15,000
The 3 columns are the group. Right left are one true grade, average, one to 2, one and 2.

1253
03:16:15,000 --> 03:16:27,000
So these are 3 group. Each one has its mean and they of course, they will be different from the grand name, because there's a sampling variability

1254
03:16:27,000 --> 03:16:31,000
So the idea is that even know how hard is it true?

1255
03:16:31,000 --> 03:16:36,000
Then this 3 group means shouldn't be too far away from the grandmother.

1256
03:16:36,000 --> 03:16:41,000
Well, they shouldn't be far away this should be cool if they are far away.

1257
03:16:41,000 --> 03:16:51,000
That's more evidence against no so overall difference between this blue bars with a yellow bar.

1258
03:16:51,000 --> 03:16:58,000
He's our manager of the effect for measure of the signal, no matter how to call you so.

1259
03:16:58,000 --> 03:17:03,000
We just need to summarize this 3 difference into one number to measure that you can.

1260
03:17:03,000 --> 03:17:08,000
So how do we summarize again? You see, there are negative number.

1261
03:17:08,000 --> 03:17:11,000
There are positive number. Also, you need to take into consideration.

1262
03:17:11,000 --> 03:17:14,000
Different group have different sides. right? you need to. do. You want to wait?

1263
03:17:14,000 --> 03:17:19,000
The group with more larger size, higher, because that provide more information.

1264
03:17:19,000 --> 03:17:36,000
So how do you take all the group? 5 and all the directions into consideration and summarizing the one number? It turns out people use this formula. So you take the difference between a group mean and the overall grandmother.

1265
03:17:36,000 --> 03:17:43,000
I'm then you'll square it because negative become hard the way to square it just like a way of computer standard deviation.

1266
03:17:43,000 --> 03:17:54,000
And then you multiply by the sample size, because large sample have higher weight than insane, and that you add them together.

1267
03:17:54,000 --> 03:18:01,000
They would divide by the number of groups. Here is number of group minus one, just like this time a division turned out.

1268
03:18:01,000 --> 03:18:09,000
This thing has better mathematical, probably, but essentially you average out number of groups.

1269
03:18:09,000 --> 03:18:18,000
This quantity is always positive, and measures the so call deviation of the group mean from the grand mean.

1270
03:18:18,000 --> 03:18:28,000
So the larger this quality, the larger evidence against the null

1271
03:18:28,000 --> 03:18:39,000
Okay, and we are not done yet, because this quantity has a variability we're all to quantify the variability on uncertainty of this quantity.

1272
03:18:39,000 --> 03:18:46,000
Basically the standard error, right? You want to figure out the sample distribution of this quantity and figure out the standard error.

1273
03:18:46,000 --> 03:18:56,000
So the very reason we pick this particular formula is because mathematically we can figure out the standard error of authority around this point.

1274
03:18:56,000 --> 03:19:03,000
So again, the particular formula is not the important, the importance understanding what we are trying to do right.

1275
03:19:03,000 --> 03:19:15,000
The sauce. The digital software always do the calculator for you

1276
03:19:15,000 --> 03:19:25,000
So here. This is the formula with with you know real numbers and sample size and 3 group right, we're on the more general formula for more than 3 groups.

1277
03:19:25,000 --> 03:19:28,000
So suppose we have K group. Then a formula looks like this: right?

1278
03:19:28,000 --> 03:19:42,000
So some over sample size in the case group modify the difference between the Kth group mean. subtract the overall graming the square sum over everything divided by the number of groups minus one.

1279
03:19:42,000 --> 03:19:48,000
So once you compute this you got a number one single number It's called Msp.

1280
03:19:48,000 --> 03:19:56,000
Was that for mean sum of square, the between groups

1281
03:19:56,000 --> 03:20:02,000
So this is a measure of the signal. So in this particular data side, once we plug in, you know all the real numbers right?

1282
03:20:02,000 --> 03:20:08,000
This is the group mean for the first group. This is the grandmother, me for the second group running and sample size, and so on.

1283
03:20:08,000 --> 03:20:17,000
And of course we do everything you got a very huge number Don't be food by the you know, Magnitude the number.

1284
03:20:17,000 --> 03:20:23,000
This number will go is the unit you are measuring. Okay, for example, this means right.

1285
03:20:23,000 --> 03:20:26,000
This mean right now is measuring calorie, even measuring thousands of calories.

1286
03:20:26,000 --> 03:20:36,000
Then this number will be a 1,000 times smaller it doesn't matter that the magnitude of this number, particularly because the variability also go with the same unit right? a larger number.

1287
03:20:36,000 --> 03:20:49,000
We have a larger variability, anyway.

1288
03:20:49,000 --> 03:20:53,000
So I will visualize this, and then stop for the day.

1289
03:20:53,000 --> 03:21:01,000
After this file was down. So you essentially the reason I scale down the size of the data because I want to visualize it.

1290
03:21:01,000 --> 03:21:04,000
Right now we have 3 groups. First group have samples out of 24.

1291
03:21:04,000 --> 03:21:09,000
So I plot all the data points of 24, s brewer, you know.

1292
03:21:09,000 --> 03:21:15,000
12 third will have 6. So with such small group sides I can plot the data point.

1293
03:21:15,000 --> 03:21:21,000
Each dot is the data point, and then the bar out of group meet

1294
03:21:21,000 --> 03:21:31,000
So the key thing is that even though 3 group have different mean now, you should see the 3 bar or Sub separated, you know, far from each other.

1295
03:21:31,000 --> 03:21:35,000
In this case the 3 bar, and also separate from each other right?

1296
03:21:35,000 --> 03:21:46,000
Which means just by looking at data I wouldn't say there's a strong evidence against them all of course we need to make it more positive.

1297
03:21:46,000 --> 03:22:03,000
But just visually it seemed like the 3 group are largely similar to each other rather than be similar

1298
03:22:03,000 --> 03:22:13,000
Okay, Any question.

1299
03:22:13,000 --> 03:22:19,000
Or I will continue tomorrow to finish the lecture that's called an end for the day.

1300
03:22:19,000 --> 03:22:22,000
So remember the turning in 7 8 that's it we'll do 7, c.

1301
03:22:22,000 --> 03:22:27,000
And a lecture tomorrow, and I have a office over right after this.

1302
03:22:27,000 --> 03:22:31,000
So feel free to stay and ask me questions otherwise i'll see you tomorrow.

1303
03:22:31,000 --> 03:22:36,544
Have a good day. Thank you.

