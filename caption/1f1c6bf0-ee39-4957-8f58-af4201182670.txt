1
00:00:17,430 --> 00:00:20,800
Time. Five.

2
00:00:22,730 --> 00:00:31,340
Know it's boring. I didn't want him introduced on the debate to listen that they would say because already we were having.

3
00:00:42,010 --> 00:00:46,860
It's the same. Yeah. Right now.

4
00:00:46,860 --> 00:00:53,730
It is going to be one of. That's not that.

5
00:01:14,060 --> 00:02:35,070
Let's wait for a few minutes. Okay.

6
00:02:35,070 --> 00:02:40,340
Good afternoon and a few things to talk about before I start lecture.

7
00:02:40,870 --> 00:02:48,310
And first of all, I'd like to talk about a timeline.

8
00:02:49,100 --> 00:02:51,690
So let me just make it bigger.

9
00:02:55,440 --> 00:03:05,309
So some of you are sort of like to know what is sort of the overall sort of timeline of the the remaining work for this course.

10
00:03:05,310 --> 00:03:10,560
And, you know, today is November 3rd.

11
00:03:10,770 --> 00:03:18,750
And so we still have about, you know, five weeks or six weeks or so, you know, to finish up this.

12
00:03:20,520 --> 00:03:25,169
So I already posted the homework.

13
00:03:25,170 --> 00:03:30,629
Number three is due on Thursday, November 17th.

14
00:03:30,630 --> 00:03:40,020
But I think you have all the knowledge to solve the problems there and I don't think need to wait and you can start work on that.

15
00:03:42,040 --> 00:03:46,109
Um, yeah, I, I will open up the homework number three in a few minutes.

16
00:03:46,110 --> 00:03:55,560
So it's quite straightforward homework and I just want you to think about this computer is smoother.

17
00:03:55,950 --> 00:04:02,879
As I already mentioned, that's something related to reinforcement learning and something you need to practice a lot.

18
00:04:02,880 --> 00:04:07,080
But I think that's something also related to the prediction part.

19
00:04:08,220 --> 00:04:11,549
So you have about two weeks for this homework.

20
00:04:11,550 --> 00:04:15,990
Both of you need really that much time. And it's that you have all the knowledge.

21
00:04:15,990 --> 00:04:19,980
You don't need to wait for my new lectures anymore to do this homework.

22
00:04:22,050 --> 00:04:23,370
So for your project,

23
00:04:23,370 --> 00:04:34,350
I said that this is designed for the nontraditional data capture and some simple exploratory data analysis should be first offered.

24
00:04:35,550 --> 00:04:45,930
And do you have out, you know, a mouse to do that, you know, this good project and maybe you don't need to spend that much time.

25
00:04:45,930 --> 00:04:53,670
I will tell you where to find the data. And most of the the data captures should be very straightforward.

26
00:04:54,270 --> 00:05:00,120
And but you need a lot of time to organize the data and and something like that.

27
00:05:01,560 --> 00:05:05,770
And so author finish, that's the default.

28
00:05:06,020 --> 00:05:14,669
You know, the following date you just skip a ball 10 minutes presentation to share your dataset with the class, at least your peers.

29
00:05:14,670 --> 00:05:24,840
And so if they want to use their data, then, you know, think of what are the key features of the data should be quite to for presentation.

30
00:05:25,290 --> 00:05:29,309
If you really do have some experience of all your, you know,

31
00:05:29,310 --> 00:05:34,500
the group project and and we have homework for I want to you practice the number

32
00:05:34,500 --> 00:05:41,460
spatial data analysis because I am going to cover spatial data analysis next week,

33
00:05:41,580 --> 00:05:48,479
maybe next Thursday. I need to finish up a MSI so that I move to spatial data analysis.

34
00:05:48,480 --> 00:06:00,900
I want you to practice a little bit straight ahead of this. That's homework for I can make the homework fall short like two problems or something.

35
00:06:00,900 --> 00:06:05,460
Very short question. But I want to practice a little bit the spatial theorem tasks,

36
00:06:06,630 --> 00:06:11,880
the final projects is scheduled December 19th, but you can give me any time before that.

37
00:06:12,180 --> 00:06:22,860
You can give me like December ten. So if you want, if you want to finish everything before December 10th, I don't want to wait up to December 19.

38
00:06:22,860 --> 00:06:27,179
Difference like December 19 is the last day of this final exam.

39
00:06:27,180 --> 00:06:30,690
I don't know. What is that when the due date is feasible?

40
00:06:30,690 --> 00:06:33,899
I mean, it's good duty for you. I set up this as a daylight.

41
00:06:33,900 --> 00:06:39,480
Doesn't mean that you have to submit the the final project on December 14th, 19th.

42
00:06:39,520 --> 00:06:44,670
You can you can send me today, December 10th, and you finish up this course if you want.

43
00:06:45,040 --> 00:06:51,570
So it's up to you how to manage your time. Yeah. Any questions?

44
00:06:51,660 --> 00:06:57,150
Concerns. Question for the project.

45
00:06:57,640 --> 00:07:00,860
Oh. Turned into something. Portman. Yeah.

46
00:07:01,070 --> 00:07:06,100
I wanna talk about that. Yeah. Yeah. So.

47
00:07:06,440 --> 00:07:18,300
Okay. Yeah. I think some of our concern is just given the timeline in this summer, from the first to the 19th, the homework is long.

48
00:07:18,710 --> 00:07:22,840
It's going to take up a lot of our time right here. And homework for should be short.

49
00:07:23,030 --> 00:07:28,519
Okay. If it's short at this one is just a like 10 minutes presentation it's not as something

50
00:07:28,520 --> 00:07:35,270
versus potentially just come here to talk about this is really just talk about your day.

51
00:07:35,480 --> 00:07:44,390
Right. So share something you like to share or I should not be taking too much time for preparation.

52
00:07:44,780 --> 00:07:48,410
Essentially, I would say like couple of hours would be sufficient.

53
00:07:48,980 --> 00:07:55,340
Like if you have group three beneficial contribution to slides that be sufficient

54
00:07:55,340 --> 00:07:59,320
or three slides just sufficient to give the presentation of 10 minutes,

55
00:07:59,330 --> 00:08:08,120
right? Mm hmm. So it won't take much time. Right. So essentially what you need to do in December is to think short homework for and the final project,

56
00:08:08,780 --> 00:08:15,320
whichever, you know, this time, like where pace you want to take to finish up that that's that's fine.

57
00:08:18,690 --> 00:08:21,930
Okay. Okay.

58
00:08:24,780 --> 00:08:27,809
So let's talk about the project first.

59
00:08:27,810 --> 00:08:41,430
And so I share the I feel that it might be even more interesting to capture more data than just mortality and vaccination.

60
00:08:43,750 --> 00:08:46,840
Okay. Police, open fire for me. Okay, here we go.

61
00:08:47,800 --> 00:08:52,850
And I sort of tried to find the data sources.

62
00:08:52,870 --> 00:08:56,400
According to your homework one. Okay, so.

63
00:08:56,530 --> 00:09:03,700
So I have t one and T and intel and job.

64
00:09:04,430 --> 00:09:15,049
They they talk about a little bit the social determinants in their homework of them, you know, interesting.

65
00:09:15,050 --> 00:09:25,210
The social determinant. And then you can find this sort of 2020 census data, I mean, from the Census Bureau of the United States,

66
00:09:25,630 --> 00:09:35,740
where you can really capture the socio distribution, you know, the distribution from from from different at the county level.

67
00:09:36,100 --> 00:09:46,750
Right. So that that's the data that can be easily captured from the Census Bureau and then organization.

68
00:09:46,930 --> 00:09:48,700
And we know that before.

69
00:09:49,330 --> 00:10:03,960
So so so you know that the the the African-Americans have a disproportionately higher risk for COVID, particularly in the early stage of that.

70
00:10:03,970 --> 00:10:09,940
So I think the social distribution should be a important risk factor or so should

71
00:10:09,940 --> 00:10:19,290
determine to determine the risk of the of the COVID vaccination and mortality.

72
00:10:19,300 --> 00:10:26,230
I think that's important for confounding factors to study some of the study, this sort of urbanization,

73
00:10:26,230 --> 00:10:36,990
where people who are live in the city have to take cover to commute using public transportation system where they are more likely to be infected,

74
00:10:37,570 --> 00:10:45,760
that the people living in the rural area where people just probably have their own cars or will have that limited sort of,

75
00:10:46,990 --> 00:10:50,920
you know, mobility to our contract covers.

76
00:10:51,460 --> 00:11:04,550
So the there's a census from Agriculture Department in the United States 2017 that basically have this survey data on the county level to now,

77
00:11:05,320 --> 00:11:09,370
you know, measure the urbanization at different counties.

78
00:11:09,580 --> 00:11:13,900
There are some multiple factors that determine the urban urbanization.

79
00:11:14,560 --> 00:11:18,879
That's also a data that you can do that is there.

80
00:11:18,880 --> 00:11:23,980
It just needs to download and organize a little bit to find a time range where, you know,

81
00:11:23,980 --> 00:11:33,820
you can find the evil, the, uh, the factors that are relevant to that to, to our project.

82
00:11:34,750 --> 00:11:45,850
And another group think about the temperature data could be a risk factor because you see the seasonality in your heart rate in Michigan,

83
00:11:46,210 --> 00:11:48,580
mortality that you see for pigs. Right.

84
00:11:49,090 --> 00:11:56,620
So there are something going on and people like to see learning out of these metrological variables like temperature,

85
00:11:56,950 --> 00:12:06,240
humidity, that something like that could be related to, you know, the rise and up and downs of the infections or mentalities.

86
00:12:06,250 --> 00:12:09,640
And we do have EPA data, okay.

87
00:12:10,030 --> 00:12:15,910
From EPA or CDC. They both have this kind of comprehensive out of control variables.

88
00:12:16,780 --> 00:12:24,819
I don't know if people have ever studied PM 2.5 because those fine particles may be related to respiratory diseases,

89
00:12:24,820 --> 00:12:30,550
as I stated, for asthma, bronchitis, those are respiratory diseases.

90
00:12:31,030 --> 00:12:37,629
But you can if for this group, you can capture the the pollution data as well,

91
00:12:37,630 --> 00:12:44,709
just in case people want to do some analysis to see whether an association between mortality and

92
00:12:44,710 --> 00:12:54,820
PM 2.5 if you have a concentration level and I mean of course that this is economic variables.

93
00:12:55,630 --> 00:13:00,820
So, so then and then you and that and how you read that.

94
00:13:01,300 --> 00:13:04,900
So from the Bureau of Labor Statistics,

95
00:13:04,900 --> 00:13:12,880
you can find the county level unemployment rate and income average median income for each county where you can.

96
00:13:14,680 --> 00:13:20,140
So define sort of their socio economic history personalities across different counties in the country.

97
00:13:20,150 --> 00:13:24,290
Yes. So with these variables, I'm looking across all groups, right?

98
00:13:24,310 --> 00:13:29,650
Things like meteorological variables that can that that has like a temporal change.

99
00:13:29,740 --> 00:13:39,580
And I could see the epi data be monthly or weekly or something, but something like unemployment and income is usually only reported per year.

100
00:13:39,580 --> 00:13:47,830
So. No, no, you have this I. I think this this one, this unemployment income is mostly monthly.

101
00:13:47,830 --> 00:13:51,819
But I'm saying it's not necessarily. It's not necessarily weekly.

102
00:13:51,820 --> 00:13:59,290
But you can believe that unemployment is pretty much constant across two weeks for that.

103
00:13:59,860 --> 00:14:03,670
I also this is more important, more kind of spatial across countries.

104
00:14:04,060 --> 00:14:10,210
Right. So, I mean, for the mutual variable as well, it's not only just temporary.

105
00:14:10,270 --> 00:14:16,720
Right. But also you look at that, they are across different counties in the United States.

106
00:14:16,990 --> 00:14:20,500
Right. Or were you think ABC State of Michigan. Right.

107
00:14:20,860 --> 00:14:23,560
So you have the different unemployment rate.

108
00:14:23,710 --> 00:14:32,350
And another so certainly is the place that has higher employment rate and higher median income, higher housing price.

109
00:14:32,800 --> 00:14:37,180
So some sometimes it'd be you can see that people are having higher income,

110
00:14:37,180 --> 00:14:47,360
that they're more likely to have a detached house or some better living conditions that have less likely to share some common space so that have less,

111
00:14:47,750 --> 00:14:53,050
less risk to contract coverage. So so that's something related to that.

112
00:14:53,180 --> 00:14:57,190
Right. So these variables are collecting just in the grand scheme of things.

113
00:14:57,370 --> 00:15:05,229
Some groups may be collecting weekly variable, some monthly variables, and we just have to find a way to correlate that.

114
00:15:05,230 --> 00:15:08,740
Yeah. To harmonize them. All right. Data harmonization.

115
00:15:09,100 --> 00:15:14,889
So if you want to use that. Yes, if you want, you say I'm not interested, that you know, that you're fine.

116
00:15:14,890 --> 00:15:18,330
But if you can't use some of them, don't you?

117
00:15:18,370 --> 00:15:23,499
Will you? I, I don't remember exactly the level of resolution, but when you give your presentation,

118
00:15:23,500 --> 00:15:32,440
you tell people the level of resolution and what the possible strategy you can impute mostly to the weekly data where,

119
00:15:32,680 --> 00:15:37,479
you know, yes, some aggregation is if you have daily data, I think the temperature must be data,

120
00:15:37,480 --> 00:15:43,180
data where you say you can take your average, you will take a maximum of some some kind of variable.

121
00:15:43,180 --> 00:15:49,540
So you prepare for the team and we can stored there and to be used at full up for our people.

122
00:15:49,690 --> 00:15:52,740
If people want to use our data for research.

123
00:15:52,750 --> 00:16:01,059
Right, it's very comprehensive. This a collective effort from this class that may be useful for our research because we're collecting all

124
00:16:01,060 --> 00:16:09,640
sorts of social investment determinants and many domains that will be very useful for other questions.

125
00:16:12,280 --> 00:16:19,810
Okay. So there's a viewer of transportation where you can look at the county level of

126
00:16:20,200 --> 00:16:30,159
mobility and how how many highways connected and how what's the distance to the county,

127
00:16:30,160 --> 00:16:38,190
to major airports. And so that you can see the the volume of customers and the particular airport like

128
00:16:38,200 --> 00:16:42,879
we have a lot of small airports and they have different agrees like you have this

129
00:16:42,880 --> 00:16:48,610
international airport like this DPW that it's really a place where you can see a lot of

130
00:16:50,200 --> 00:16:57,700
international travelers coming and they may likely to spread disease from other countries.

131
00:16:57,820 --> 00:17:05,170
Maybe that's the early places that can, you know, start all the sort of disease outbreak.

132
00:17:06,190 --> 00:17:13,090
So I can see there is a like a monthly transportation start is typically the cities this looks like it's like a month later like first.

133
00:17:13,570 --> 00:17:24,350
Yeah yeah. I don't remember the level of resolution, but you can also calculate the the distance of like a neighbor, how the you know,

134
00:17:24,560 --> 00:17:32,230
you know, you have a big airport, like what's the distance from and number to the location, the airport,

135
00:17:32,240 --> 00:17:41,680
right to you Pfizer index that can be used to sort of characterize the the

136
00:17:42,370 --> 00:17:48,279
possibility or the potential chance of being running to a person international

137
00:17:48,280 --> 00:17:58,650
traveler who carries there were some that right so because this is a global pandemic that so you have to think about the mobility of the you know,

138
00:17:58,660 --> 00:18:02,320
the people living in the country. Yeah.

139
00:18:02,590 --> 00:18:10,450
I don't know how to use them. I mean I mean, there's so many variables I really want to have some good but hypothesis behind.

140
00:18:10,450 --> 00:18:15,459
And now the last one is the 2020 election.

141
00:18:15,460 --> 00:18:26,980
They are like president election data where there are county level sort of segregation of political sort of map in the

142
00:18:26,980 --> 00:18:40,510
country where people can say that the blue regions are more likely to accept the vaccination compared to regions and so on.

143
00:18:40,750 --> 00:18:55,340
So there are some kind of. You know it's something that his originality that can drive the different love of his sense of of of accepting,

144
00:18:55,340 --> 00:19:08,419
of rejecting vaccination or something like that. So. So that's another variables that we can use in the analysis to understand how the blue state,

145
00:19:08,420 --> 00:19:14,450
a group of a blue county versus the red county or and the proportion of that they swing

146
00:19:14,450 --> 00:19:21,610
counties and how that could be related to vaccination or even fatality or infections.

147
00:19:21,770 --> 00:19:29,959
So stuff like that. So those are the data that will be, you know, be available by your effort.

148
00:19:29,960 --> 00:19:37,130
And of that small study, you can learn from each other how those data are being collected and how you're

149
00:19:37,130 --> 00:19:45,400
going to use those data to analyze the COVID 19 pandemic and right to fund project.

150
00:19:45,740 --> 00:19:53,000
You're gonna need to use all of them. Anything that you feel, it's something interesting you want to explore, not your choice.

151
00:19:54,230 --> 00:19:57,440
Clear question.

152
00:19:57,480 --> 00:20:09,800
That's also very interesting, like in the kind of like the policy that involves the policy related to like party affiliation.

153
00:20:10,160 --> 00:20:18,410
So I was wondering like if they'll keep everybody happy with it's like that should be our focus, the whole project for Noodled,

154
00:20:18,410 --> 00:20:31,390
although I mean this is just the focus of your data capture, but you can write the, uh, your final project using any variables you like now.

155
00:20:31,760 --> 00:20:35,500
So you're assigned to do this just for your project, right?

156
00:20:35,570 --> 00:20:41,580
So this group project the end of this month, but the data will be available for everyone, right?

157
00:20:41,600 --> 00:20:49,220
So that you can look at the this 2020 presidential election data to understand this policy were

158
00:20:49,640 --> 00:20:56,750
segregation of the political map in this country and to say how that would be a risk factor for,

159
00:20:57,230 --> 00:21:02,390
you know, vaccination acceptance or rejection or hesitation and whatever.

160
00:21:02,420 --> 00:21:13,399
So so that can be a factor of mortality of some type that you can choose to analyze this for the entire for the final project.

161
00:21:13,400 --> 00:21:18,260
I'm not talking about this one. This one has to be nationwide data mission, the white data.

162
00:21:18,260 --> 00:21:21,980
I mean, the full, final project you can focus on only on state of Michigan.

163
00:21:22,250 --> 00:21:35,810
We have 33 counties or you analyzed for the national level for all the 400, about 20 county for 4200 roughly the counties in the country.

164
00:21:38,510 --> 00:21:42,900
Okay. More questions. That's clear. Okay.

165
00:21:44,730 --> 00:21:54,030
I'm just get this. I won't take much time, and I just need to find a rate.

166
00:21:54,120 --> 00:22:00,090
Look, let me know if you need. Have a meeting with me to complete this state capture.

167
00:22:02,730 --> 00:22:06,480
Let me just move on to the project.

168
00:22:07,230 --> 00:22:11,910
Okay. Just talk a little. That prompted. So, so.

169
00:22:12,810 --> 00:22:21,299
So primary aim of this project is to capture some important social, social and environmental determinants.

170
00:22:21,300 --> 00:22:26,010
Useful to understand fact the mentality of the COVID 19 pandemic.

171
00:22:26,010 --> 00:22:32,940
Unless these so so that's basically what we like to do and I have defined clearly some of the domains of the

172
00:22:34,110 --> 00:22:40,769
confounding of risk factors that I would like to capture using various states and data bases and hypothesis one.

173
00:22:40,770 --> 00:22:48,630
And so risk of COVID 19 infection mortality and prevention such as vaccination,

174
00:22:52,290 --> 00:22:58,529
is highly heterogeneous and potentially social, with many social and environmental determinants.

175
00:22:58,530 --> 00:23:04,109
Understanding this determines can help you for preventive measures and interventional

176
00:23:04,110 --> 00:23:11,599
programs to contain infection and reduce mortality rate for the general US population.

177
00:23:11,600 --> 00:23:13,650
Population. Okay, so that's hypothesis one.

178
00:23:14,010 --> 00:23:23,730
A positive is indeed in addition to tempo pattern you have invested, we have learned so far I will start to teach a spatial data analysis next week.

179
00:23:24,120 --> 00:23:34,290
There are strong spatial heterogeneity is cross regions of nation understanding spatial pattern can identify hotspots require more resources,

180
00:23:34,440 --> 00:23:40,090
medical resources and better effective policies.

181
00:23:40,320 --> 00:23:49,060
Okay, so those are the something we like to explore and to go deeper in our analysis.

182
00:23:52,320 --> 00:24:00,210
So we like to study the above two hypotheses by collecting various national wide data on social and environmental stressors.

183
00:24:00,750 --> 00:24:09,420
After the data collection, we will apply some exploratory data analysis to to visualize the data temporary and spatially,

184
00:24:09,420 --> 00:24:16,649
and to generate some preliminary data results. So this is for this group project.

185
00:24:16,650 --> 00:24:23,010
Now for the five project, the five project, I will expect to use some models to generate some key values.

186
00:24:23,010 --> 00:24:30,749
But this is just display the the data using this EDA approach and we see how data looks like.

187
00:24:30,750 --> 00:24:42,470
And so long this project will be concluded with at least one major study aim that will be explored thoroughly in the final two party.

188
00:24:42,540 --> 00:24:47,520
Are you not? So, so. So after we do this and you should come call your own party.

189
00:24:48,030 --> 00:24:51,089
So please on this I we plan to do this.

190
00:24:51,090 --> 00:24:54,690
That is this unifying project. It will need to do that.

191
00:24:55,020 --> 00:25:06,060
Exactly. You can modify your study aim in your final project but and we that you should do this because this is more like grant writing, right?

192
00:25:06,540 --> 00:25:11,909
So if you write a NIH grant or a massive grant or any grant to US Summit organization,

193
00:25:11,910 --> 00:25:20,219
first thing you need to do is using some preliminary data analysis, generate some of the preliminary result.

194
00:25:20,220 --> 00:25:32,730
And you see here is what I say. I hypothesize this, that I need more money, need more manpower, better, you know, resources to go forward to answer.

195
00:25:32,730 --> 00:25:36,330
Do you provide, prove or disprove that persistence?

196
00:25:36,330 --> 00:25:42,990
Right. So it's pretty much like the group project is pretty much like setting up a stage of,

197
00:25:43,230 --> 00:25:51,360
you know, preliminary data analysis that can lead to a big study project later on.

198
00:25:51,450 --> 00:25:58,379
Okay. But you need to go step by step, right? So so that's what I want you to do, an end of the project.

199
00:25:58,380 --> 00:26:07,150
You conclude your project with that one. And this the one major study aims that you have hypothetically you like to explore thoroughly.

200
00:26:07,150 --> 00:26:16,920
You can find a project, you can stick out this major study or you can modify it to do to include other variables in this study.

201
00:26:19,370 --> 00:26:25,880
So this project, good project serves as a preparation for the final project with strong promise.

202
00:26:25,910 --> 00:26:34,069
You have to think what what kind of the, you know, relevant literature or some hypothesis around.

203
00:26:34,070 --> 00:26:46,010
And so I'm going to, you know, display data to provide some kind of supporting evidence from, you know, preliminary data analysis to, you know,

204
00:26:46,580 --> 00:26:53,059
help you conclude or draw a major study in that you like to study further to

205
00:26:53,060 --> 00:26:57,020
motivate next comprehensive data analysis to be conducted the final project.

206
00:26:57,980 --> 00:27:01,170
This is more like a some kind of pre numerical analysis.

207
00:27:01,190 --> 00:27:06,050
You like to write in your grand proposal for a major study leader.

208
00:27:08,090 --> 00:27:18,260
The task is organized follows part one concerns the on data collection, data QC, quality control, data preparation and data sharing.

209
00:27:19,520 --> 00:27:26,450
You are expected to deliver a data set in Excel format, if you can give me our form that that's also okay.

210
00:27:26,510 --> 00:27:30,250
Okay. But Excel form format is essential.

211
00:27:30,260 --> 00:27:42,500
So is kind of universe. So I need this from every team and so I can put on the in the canvas so everybody can use lidar and a data dictionary.

212
00:27:42,590 --> 00:27:47,780
You just don't get just give me your data and you want a variable, you capture it, just describe it.

213
00:27:48,230 --> 00:27:52,970
Did you ever transformative variable from overall data?

214
00:27:53,450 --> 00:27:56,269
What's the unit of this variable? And so on.

215
00:27:56,270 --> 00:28:06,380
Just give the definition of any variables that have been involved in the dataset, you know, in the X Excel format.

216
00:28:06,800 --> 00:28:13,310
Okay that's something captured data to QC and prepare it and you know and you

217
00:28:13,310 --> 00:28:21,020
know prepare data dictionary part to focus on some exploratory data analysis

218
00:28:21,020 --> 00:28:31,490
to visualize some temporal spatial patterns as well as some like for nimble data analysis leading to at least one major study in for defined project.

219
00:28:31,500 --> 00:28:35,750
So here are the some of the items you can touch.

220
00:28:36,410 --> 00:28:40,160
So you referred to the Google should I just showed that to you.

221
00:28:40,460 --> 00:28:43,670
That has defined the data capture task for each team.

222
00:28:43,670 --> 00:28:50,210
Focus the domain of social and environmental determinants that's so so variables have defined.

223
00:28:51,770 --> 00:28:58,040
Assign you to do that according to what I read from your point, clean up the cop today.

224
00:28:58,130 --> 00:29:03,160
Maybe you don't need to do any clean. The data has been clean already, right?

225
00:29:03,170 --> 00:29:07,190
So is already clean. So you don't need to. I'm not sure I.

226
00:29:07,730 --> 00:29:17,510
I think that's something you need to really make sure that you, you will carry out data quality control and, and normalized data if needed.

227
00:29:17,810 --> 00:29:30,260
Okay. And then clean the data will be CV an Excel file to to be sure with the clause I'll send a copy to me and will upload it to campus.

228
00:29:30,920 --> 00:29:33,530
Okay. And prepare data dictionary.

229
00:29:33,590 --> 00:29:41,420
Maybe you only have one variable that's easy if you have three variables is define and and I can send that to me now,

230
00:29:41,510 --> 00:29:48,750
I can combine them to at one file, then you can I can share that this the cost expert,

231
00:29:48,770 --> 00:29:49,729
the analysis,

232
00:29:49,730 --> 00:30:00,660
data visualization and next analysis basically display data temporally and come on core time course data patterns in connection to COVID 19 pandemic.

233
00:30:00,850 --> 00:30:03,740
Okay so so you display whichever,

234
00:30:04,250 --> 00:30:16,670
you know the way that you want to show and some time search plot and to to indicate some major episodes or some kind of interesting patterns that be,

235
00:30:17,570 --> 00:30:27,200
you know, related to the pandemic the course of pandemic and then you display the data are spatially

236
00:30:27,500 --> 00:30:35,590
okay you can do the spatial plotting and and and come on the spatial course hit.

237
00:30:35,600 --> 00:30:39,319
It's not across different areas of the United States.

238
00:30:39,320 --> 00:30:45,440
And most of these are actually at at the resolution of countries.

239
00:30:45,650 --> 00:30:48,380
Okay. So that's okay. We can see that.

240
00:30:49,850 --> 00:31:01,930
So you can calculate some simple summary statistic mean median and standard deviation, and you can compare if you want, you know, the for,

241
00:31:02,120 --> 00:31:13,790
for example, for, for if you have the data of the, the political segregation in the country, you define blue country versus gray country.

242
00:31:14,120 --> 00:31:17,720
Then you can really make a simple comparison of the.

243
00:31:18,210 --> 00:31:29,160
Vaccination rates. Right. You can applaud the vaccination compliance like full dose vaccination for these

244
00:31:29,190 --> 00:31:35,170
all the the average one from all the blue counties versus all of the red counties.

245
00:31:35,190 --> 00:31:38,190
You can see whether or not there's a gap or no gap.

246
00:31:38,280 --> 00:31:38,420
Right.

247
00:31:38,440 --> 00:31:47,640
So where you look at this also states or you're looking at different regions, you know, is Coles or New England's, you know, regions versus unreason.

248
00:31:47,790 --> 00:32:01,170
But you define something that may be maybe able to, you know, show some kind of interesting pattern, some soap that whichever you want to do.

249
00:32:01,360 --> 00:32:08,180
Okay. But ultimately, I want to your group part of this, too,

250
00:32:08,190 --> 00:32:14,160
to have a conclusion with at least one major study that could be explored through in the final project.

251
00:32:14,700 --> 00:32:18,900
It's not clear what you need to do. Okay.

252
00:32:21,760 --> 00:32:23,080
So that's something you need.

253
00:32:23,200 --> 00:32:32,950
If you end of this month and let me know if you need to set up a meeting to go over this and if you have some difficulty or some further discussions,

254
00:32:33,010 --> 00:32:39,520
I'd be happy to talk through all and set up meetings with you.

255
00:32:41,590 --> 00:32:51,790
So for each group, you provide the list of variables and like you named bureaus, or we might be able to find that unemployment rate.

256
00:32:52,690 --> 00:33:01,690
Yes. So quickly, I think, look, there's many bureaus that have done that, not necessarily labels, labor statistics.

257
00:33:01,690 --> 00:33:06,649
Do we have to go from that bureau list or can we get it from a different source and be provided?

258
00:33:06,650 --> 00:33:12,670
That's about it. Okay. It doesn't matter where you get it. Just tell us the source of where you get it.

259
00:33:13,510 --> 00:33:19,270
And of course, you would need to find a source that's reliable sources should be profitable.

260
00:33:19,270 --> 00:33:22,570
Yeah, reputable and reliable. And I mean,

261
00:33:22,600 --> 00:33:28,870
if someone I worked for very data about you can go start you know but anyway I think that's a

262
00:33:29,920 --> 00:33:35,350
collective effort that we need to do together to make this analysis a little bit more interesting.

263
00:33:35,350 --> 00:33:38,660
Yeah. Okay.

264
00:33:39,800 --> 00:33:48,110
So the homework three, that's the one that is due in two weeks.

265
00:33:51,030 --> 00:33:58,109
Okay. Here it. Here it goes. And so problem y is quite straightforward because I would improve this in my

266
00:33:58,110 --> 00:34:04,470
lecture knows for a fielder I already identify all the terms in the lecture notes.

267
00:34:05,130 --> 00:34:09,510
Okay, let me just bring up. Okay. So that's lecture nine, I think.

268
00:34:22,120 --> 00:34:25,280
Okay. You see that here? I already have proof of fear.

269
00:34:25,420 --> 00:34:33,280
I do not approve commerce move, sir. So all the items have been identified and you just need to go over it.

270
00:34:33,280 --> 00:34:39,370
And here I had all the derivation and now I did not do smoother.

271
00:34:39,370 --> 00:34:44,859
It's just a a a little bit actual work.

272
00:34:44,860 --> 00:34:55,300
It's it's not a big deal, right? So it's I just once you get used to this sort of system where how this prediction can be defined,

273
00:34:55,450 --> 00:35:00,939
I don't have don't already feel that's the most time consuming Pakistani to

274
00:35:00,940 --> 00:35:05,820
identify all the terms here you were to apply the standard formula to do that.

275
00:35:05,830 --> 00:35:11,260
Okay so I just asked you to do the smoother and the prediction arrow for smoother.

276
00:35:11,590 --> 00:35:17,440
Okay. That's should be done a half hour, 30 minutes.

277
00:35:17,440 --> 00:35:24,489
Because if you if you know this system well all of if you want to review it maybe takes 2 hours notice.

278
00:35:24,490 --> 00:35:29,210
Nine, eight. Oh, she would surely just probably want to take the North nine.

279
00:35:31,850 --> 00:35:35,320
Oh yeah, yeah, yeah, yeah. No. Okay, cool.

280
00:35:37,690 --> 00:35:40,900
Yeah. Okay. So the butcher knife.

281
00:35:41,260 --> 00:35:46,659
How come I could not serve lecture ten G. Okay, so that.

282
00:35:46,660 --> 00:35:51,010
Sorry. So the other thing is, we have this problem.

283
00:35:51,010 --> 00:35:55,080
Gamma stays very small. I want you do simulation, right?

284
00:35:55,090 --> 00:36:00,490
You have this comm structure so you you can simulate this system, right?

285
00:36:00,910 --> 00:36:06,320
This should not be very difficult to do a write an article to simulate from this data.

286
00:36:06,340 --> 00:36:11,440
70 y t and see that t plus one.

287
00:36:13,120 --> 00:36:22,989
See the t minus the post. You want to do a, you know, a simulation study you need to simulate from this system.

288
00:36:22,990 --> 00:36:26,950
Right? So first, use your simulated late in process.

289
00:36:27,610 --> 00:36:30,939
So little process can be simulated by this. Right.

290
00:36:30,940 --> 00:36:42,580
So you have your theta one set of zero, which is one and you have BTB two was independent, simulated like by a by a beta distribution.

291
00:36:42,730 --> 00:36:47,440
Here's the parameter. A beta distribution is simply a random number from beta distribution.

292
00:36:47,710 --> 00:36:53,380
Multiply one which is the t00 and then you simulate this one.

293
00:36:54,220 --> 00:36:59,620
So you figure out what the distribution this will be. And I, I mean, there should be a gamma distribution, right?

294
00:36:59,630 --> 00:37:06,340
So anyway, so that attempt to get rid of your signal one day you just say simulate after you get this 15 out there,

295
00:37:06,350 --> 00:37:11,170
use probability distribution symbol, you want to get free easy.

296
00:37:11,290 --> 00:37:18,610
Right. So. I want to give a distribution, though, if some T you might not know.

297
00:37:18,970 --> 00:37:23,570
So you can find that. Okay. So.

298
00:37:23,770 --> 00:37:34,000
So you'd write an article to stimulate data. Then I want to give you the r cold for a filter and come a smoother you the article.

299
00:37:34,030 --> 00:37:37,130
You just copy paste that part. Okay.

300
00:37:37,600 --> 00:37:42,340
To create a function to offer good simulated data.

301
00:37:43,450 --> 00:37:46,720
Okay. There's no underestimation. You get the white t.

302
00:37:47,380 --> 00:37:53,080
Okay. Simulated. Then you use the formula of the our code on in order to give to you a pick up that

303
00:37:53,080 --> 00:38:01,120
piece out and just calculate recursively how many filters are using the two.

304
00:38:01,410 --> 00:38:08,500
My old friend of ours. You don't need to estimate. You just calculate computer filter smoothing using the true parameter values.

305
00:38:09,280 --> 00:38:10,990
Okay. No estimation. Okay.

306
00:38:12,010 --> 00:38:22,890
Now you plot common filter and smoother generate from part B and with your simulator you see that because common filter and common smooths are blob,

307
00:38:22,930 --> 00:38:27,940
right? Best in either unbiased prediction of your latent process and see for this late

308
00:38:27,940 --> 00:38:33,670
in process you simulate it right and then you'll get the common filter smoother.

309
00:38:34,180 --> 00:38:42,160
And how close there are supposed to be close because you know, this is the best in the unbiased prediction of your rating process.

310
00:38:42,250 --> 00:38:50,690
Right. And then if you want the replicas, because there are some randomness in this simulation, you feel.

311
00:38:50,980 --> 00:38:59,500
You can you can replicate this ten times to see how to calculate average of, you know, discrepancy or distance of the prediction.

312
00:38:59,980 --> 00:39:04,960
But but the prediction error is given by s c t or c t star.

313
00:39:04,990 --> 00:39:11,050
Right. But here I do not ask you calculate that. I just see just numerically to see that.

314
00:39:11,520 --> 00:39:17,100
Okay. Should be very straightforward. You don't need to write this common filter and smoother.

315
00:39:17,110 --> 00:39:22,720
Our code. I already gave you the you need to identify that part from my code and right.

316
00:39:23,050 --> 00:39:28,420
And I think you can do it. So now back to the problem three.

317
00:39:29,750 --> 00:39:40,820
So, you know, in the homework number two problem for B is to remember that we are looking at how vaccination and age interaction term.

318
00:39:40,840 --> 00:39:49,170
Right. If you did your homework to know you still remember that problem where I asked you to do the interaction, the age.

319
00:39:49,180 --> 00:39:56,620
Right. So you so used negative binomial there to estimate the parameters and report.

320
00:39:56,860 --> 00:39:59,469
So. So that's exactly. You just repeat the analysis.

321
00:39:59,470 --> 00:40:08,890
But now I just want to remove the the age group 0 to 70 because this mortality counts is very low.

322
00:40:08,920 --> 00:40:17,499
Most of us zero. There's occasionally one one where something like that, there's probably only five six ones over all the year.

323
00:40:17,500 --> 00:40:26,180
So I don't think that's very good sort of of times to follow possible distribution.

324
00:40:26,200 --> 00:40:35,229
I just won't take this group out. So you remove this age group using this attribute 18 to 64 and 65 or older.

325
00:40:35,230 --> 00:40:41,950
You call this. Okay. Then you refit your model using this up to level two group age.

326
00:40:42,310 --> 00:40:47,800
You can do this. You have done this for three eight. So you get it now.

327
00:40:48,220 --> 00:40:54,850
So after you estimate the use this negative binomial regression to get the regression coefficient estimate,

328
00:40:55,570 --> 00:41:03,250
then you can apply this common filter smoother from from the pa.

329
00:41:03,250 --> 00:41:07,660
Be okay here. You worried about this in the problem too.

330
00:41:08,320 --> 00:41:17,950
And I want to you just use that to, you know, calculate the common filter smoother.

331
00:41:19,060 --> 00:41:22,660
But here you use simulated data. Here you use estimate data.

332
00:41:22,660 --> 00:41:24,130
There's no difference. Right?

333
00:41:24,190 --> 00:41:32,610
Is just that the here you use similarly to the true parameters here you estimate the parameter from negative binomial regression.

334
00:41:32,980 --> 00:41:42,100
Right. Then you do this. Then after you you get your of your estimate, then you can predict.

335
00:41:42,100 --> 00:41:45,670
Right. Suppose this to see the end. Right.

336
00:41:45,680 --> 00:41:49,780
So this is I don't know which days is the last day of your data.

337
00:41:49,780 --> 00:41:54,280
Is September or something already sometime.

338
00:41:54,280 --> 00:42:03,340
Very quickly to then ask you to predict this for for for weekly values, for weekly basis.

339
00:42:04,360 --> 00:42:07,509
So because you have a unit weekly data, right?

340
00:42:07,510 --> 00:42:10,809
So I want you to predict for release.

341
00:42:10,810 --> 00:42:13,810
We have. So you basically need to generate this.

342
00:42:15,300 --> 00:42:18,370
And. Right.

343
00:42:18,520 --> 00:42:23,559
Generate impulse wasn't up to impulse for how do you generate you already did this

344
00:42:23,560 --> 00:42:29,890
you come up to a problem to in the problem to you already know how to generate this.

345
00:42:30,280 --> 00:42:36,820
So you just need to get to see generally this form will then generate your way.

346
00:42:37,630 --> 00:42:48,730
So you're. So if you could have done what we did is it's time we did go in that we get like that like that.

347
00:42:48,730 --> 00:42:52,940
These are not they're just degenerated. I mean, one for was in problem.

348
00:42:53,080 --> 00:42:59,950
Second problem. Yeah so here we like the we start the thing with he does not as they say immediately different from the model.

349
00:43:00,040 --> 00:43:11,160
Yeah and for beat for beat with student as his tuition is be done 0.6.7 for no here you'll have curfew.

350
00:43:11,680 --> 00:43:19,509
So in a negative binomial distribution you you obtain the dispersion parameter estimate that's called

351
00:43:19,510 --> 00:43:26,000
theta right through that parameter in the that our output is exactly word lambda you make notation.

352
00:43:26,590 --> 00:43:34,300
So lambda is exactly the notation you may in my lambda is exactly theta in the negative binomial regression.

353
00:43:34,780 --> 00:43:40,719
So the other out of the has to be estimated because when you have this kind of answer watch model like you'll

354
00:43:40,720 --> 00:43:47,710
order pooled in the former right and custom gamma distribution is equivalent to negative binomial right.

355
00:43:47,990 --> 00:43:53,530
Do you have already proved and in that situation you running the binomial you do not care about the

356
00:43:53,530 --> 00:43:59,710
rhythm process so you can get the over dispersion parameter estimate theta which is exactly lambda.

357
00:44:00,400 --> 00:44:05,350
And here I don't want to estimate the role, I just suppose that the role is 0.5.

358
00:44:06,430 --> 00:44:10,149
So how are you going to predict that? No estimation, right?

359
00:44:10,150 --> 00:44:12,040
Nice to me. You just do simulation.

360
00:44:12,520 --> 00:44:19,240
So you have this whole thing simulate up to some day, then continue to simulate a few days that you can get to a white.

361
00:44:19,810 --> 00:44:24,940
Got it. Is it is a question the format.

362
00:44:26,200 --> 00:44:34,060
But I want you to see my future because I have displayed in this like so.

363
00:44:45,880 --> 00:44:50,890
So like keep this planning, this solid curve here so you can make a similar plot.

364
00:44:51,310 --> 00:44:54,400
The dots are similarly the Y, right?

365
00:44:54,400 --> 00:44:57,960
So this will be your common small.

366
00:44:58,120 --> 00:45:06,730
It will come a few days. See if they're able to capture this kind of peaks or some patterns from there from your data.

367
00:45:06,790 --> 00:45:10,910
Right. So so you're seeing any data you can you can make similar product, right?

368
00:45:11,110 --> 00:45:18,000
Not only just from a filter, but you can make it some background plot of your observed way to see if this

369
00:45:18,010 --> 00:45:22,990
meeting process will help you to capture some residual variability of this,

370
00:45:23,320 --> 00:45:26,470
the random system. Right? So, okay, wait, so,

371
00:45:27,790 --> 00:45:33,879
so that's something I want you to really experience because this is probably something

372
00:45:33,880 --> 00:45:43,300
that I've never done in other courses and so it's good experience type of questions clear.

373
00:45:44,080 --> 00:45:48,130
It doesn't take much time I think probably. Yeah.

374
00:45:48,610 --> 00:45:51,660
So no air.

375
00:45:52,030 --> 00:45:59,650
You need a bit time, but no substantial technical hurdles that you need to overcome.

376
00:46:02,740 --> 00:46:08,130
I think that's all I need. Okay. Let me just bring up the hour so that you know where to find.

377
00:46:09,580 --> 00:46:13,240
Okay. So here.

378
00:46:13,750 --> 00:46:16,780
Oops. Let me just make a bigger kind.

379
00:46:19,010 --> 00:46:25,510
Control room. Not so here.

380
00:46:27,080 --> 00:46:34,700
So this is a prediction. This is my you. This m m and m a is the common future here.

381
00:46:35,300 --> 00:46:39,340
M is a common future. Words.

382
00:46:39,440 --> 00:46:47,230
My name's. I am m s because you do it.

383
00:46:47,680 --> 00:46:53,750
So the computer is going for prediction for pop masses.

384
00:46:54,040 --> 00:46:57,759
And stars come a few come a smoother doing backwards.

385
00:46:57,760 --> 00:47:02,200
So after a give come a field already pumped to next stage of comers to Mozart.

386
00:47:02,200 --> 00:47:06,580
So M. S is the common future outcome is smoother.

387
00:47:06,820 --> 00:47:12,040
You calculate it. So we pick up this chunk and this chunk that that's really how you.

388
00:47:12,430 --> 00:47:20,890
So all the parameters have been given. You don't need to do loops to estimate parameters, just produce the result from the call.

389
00:47:21,010 --> 00:47:25,760
Some some statements are not relevant like the speed of this.

390
00:47:25,760 --> 00:47:37,430
This is the quantity I used to calculate is standard errors or sandwich estimate or like s you only need to use you and massive success.

391
00:47:37,430 --> 00:47:46,600
This prediction errors if you want you can help with that but other part this DMB to ask are those are the things that are used to calculate,

392
00:47:46,620 --> 00:47:51,459
sandwich estimate or full day of inference. For this one you will have you for this problem.

393
00:47:51,460 --> 00:47:58,290
You do need to include the output. That's quite straight forward to do it.

394
00:47:58,410 --> 00:48:07,780
Okay. Given that people are programed. If you're going to try the ARB, if you can write a R package,

395
00:48:07,780 --> 00:48:15,580
you will be nice because this can be a very nice package to share with people who are going to take this course in future.

396
00:48:15,970 --> 00:48:19,360
I don't know if I'm going to teach the course again, but, you know,

397
00:48:19,360 --> 00:48:27,100
so it's it's nice to to have a piece of things that the software called that you can share with our people.

398
00:48:28,750 --> 00:48:32,739
Yeah. Okay.

399
00:48:32,740 --> 00:48:37,360
So back to the lectures.

400
00:48:39,040 --> 00:48:47,590
That covers everything. I've been talking about the mask of Monte Carlo.

401
00:48:48,250 --> 00:48:52,690
That's a very powerful ways of doing statistical analysis.

402
00:48:53,140 --> 00:49:02,469
The idea of doing that is really using simulation based inference procedure to overcome some of

403
00:49:02,470 --> 00:49:10,360
the analytical difficulties in the derivation of your MLC or duration of your standard error.

404
00:49:10,450 --> 00:49:24,549
That basically is simulate data. So the key thing here you need to do is really simulate data from this posterior beta is generic notation

405
00:49:24,550 --> 00:49:31,960
to denote the of the set of parameters of interest that you like to estimate and make inference for.

406
00:49:32,440 --> 00:49:37,120
And the Y is your data. Okay. So now what you want to do here is of course,

407
00:49:37,120 --> 00:49:42,639
to maximize this sort of posterior to get the posterior molar where we can get the

408
00:49:42,640 --> 00:49:48,910
meaning of this particular distribution as the base estimate for the parameter beta.

409
00:49:50,110 --> 00:49:55,569
So, you know, if, if you, if you have super power, you can calculate as analytically.

410
00:49:55,570 --> 00:50:01,990
But a lot of time this posterior function is very, very complex.

411
00:50:02,020 --> 00:50:11,260
What we can do here is really just trying to simulate beta from this posterior distribution that we use empirical

412
00:50:11,650 --> 00:50:20,890
distributions or empirical statistics to estimate the movements of this distribution or to estimate the distribution,

413
00:50:20,950 --> 00:50:24,930
marginal distribution of this posture distribution.

414
00:50:26,710 --> 00:50:30,320
The typically that this is the high dimensional case.

415
00:50:30,340 --> 00:50:38,980
For example, if you think that the theta dilated process is theta letham process is also part of the parameter,

416
00:50:39,550 --> 00:50:47,110
then you would have a lot of parameters. The dimension of the parameter would be higher than the the sample size.

417
00:50:47,680 --> 00:50:54,489
So you have the model parameter like R4 and you offer dispersion parameter out

418
00:50:54,490 --> 00:51:01,060
operation parameter plus the latent variables there can be treat a missing data or of,

419
00:51:01,210 --> 00:51:07,240
you know, the unknown quantities your system and then you only have one,

420
00:51:07,240 --> 00:51:16,090
two y and those you know, number of the best comes you're suddenly in the situation.

421
00:51:16,090 --> 00:51:19,090
Number of the parameters is bigger number, sample size.

422
00:51:19,570 --> 00:51:26,980
So that you have quite a difficulty to do traditional statistical inference or estimation.

423
00:51:28,600 --> 00:51:37,510
So when you faced this high dimensional, complex posterior distribution, you need to really sample this from, you know, this distribution.

424
00:51:37,700 --> 00:51:41,620
Okay. So how do you do that? Can we use this same method?

425
00:51:41,620 --> 00:51:44,920
I already talked about the prior distribution.

426
00:51:45,370 --> 00:51:48,580
The first thing people proposed is, of course, skip sampling.

427
00:51:48,820 --> 00:51:59,650
Okay. So as I said, I mean, Cobble is used to for numerical validation, which requires to store samples from the joint distribution.

428
00:52:00,160 --> 00:52:06,940
Okay. This sampling can be very difficult because of complicity of like a function of high dimensionality.

429
00:52:07,030 --> 00:52:22,350
Okay. So this difficulty can be overcome by using keeps keep sampling keeps the is a is a statistician is I show that is a physicist.

430
00:52:22,360 --> 00:52:26,080
He's working on statistic physics and something like that.

431
00:52:26,230 --> 00:52:34,360
Okay. The key idea of Gibbs sampling is the fact that it is the sampling from high dimension

432
00:52:34,360 --> 00:52:39,580
distribution can be carried out through low dimensional condition of distribution.

433
00:52:40,150 --> 00:52:45,460
So suddenly it's very hard to sample from a one solid dimension of distribution,

434
00:52:45,910 --> 00:52:51,190
but is possible to sample from a one dimensional conditional distribution.

435
00:52:51,520 --> 00:52:56,050
Okay, conditional on variables. So that's basically the idea of this gap sampling.

436
00:52:57,870 --> 00:53:06,540
So in the literature, very, very simply, schemes have been developed for lower dimension, for example, one dimension densities.

437
00:53:06,690 --> 00:53:11,820
As a result, keep sampling. Make sampling for us feasible and easy to implement.

438
00:53:13,950 --> 00:53:20,340
However, this approach is usually very computationally intensive, and so we need to really have,

439
00:53:20,580 --> 00:53:25,220
you know, modern computing facility in order to implement to occur out.

440
00:53:25,230 --> 00:53:28,290
This method is not possible.

441
00:53:28,860 --> 00:53:34,440
In the early time, we just do not have enough RAM or CPU to handle this.

442
00:53:35,070 --> 00:53:45,060
But nowadays it becomes more and more feasible because we have quite a bit of improvement in our the capacity of computing power.

443
00:53:45,540 --> 00:53:55,409
So this becomes possible and this idea is like simulation based method is quite popular nowadays,

444
00:53:55,410 --> 00:54:00,780
even like deep learning all the time in this machine learning framework,

445
00:54:00,780 --> 00:54:06,120
people say I do now need to figure out exactly the, you know, the statistic model.

446
00:54:06,120 --> 00:54:10,550
I just like the data to simulate the data, to use my observed data,

447
00:54:10,570 --> 00:54:21,209
simulate some data in order to make a sort of inference here it it's a similar idea like m same C so you I have observed data,

448
00:54:21,210 --> 00:54:24,600
I show a certain maldini structure.

449
00:54:25,080 --> 00:54:32,940
I do not need to estimate model structure. I just like the data to simulate data from a given sort of modeling structure.

450
00:54:32,940 --> 00:54:36,270
That's something you could consider.

451
00:54:36,360 --> 00:54:44,920
Okay. So let me just say to introduce briefly how this sampling works.

452
00:54:45,070 --> 00:54:49,410
Okay. So let's consider a very simple case, a try case.

453
00:54:49,410 --> 00:54:54,930
You v w I mean, this is just some provide version, just illustrate the idea.

454
00:54:54,930 --> 00:55:00,930
And in reality, of course, you're working much hard to make a problem than this three dimensional problem.

455
00:55:01,770 --> 00:55:08,549
So denoted by this square bracket you'll be w this know that it's basically doing distribution of you've

456
00:55:08,550 --> 00:55:15,090
adopted okay I don't care about this good distribution of joint distribution because it's so complex,

457
00:55:15,090 --> 00:55:20,489
I'm not able to even figure out what it is like.

458
00:55:20,490 --> 00:55:24,330
Suppose it's very difficult to sample directly from two in distribution.

459
00:55:24,330 --> 00:55:29,940
I don't even know what the joint operation looks like. You know this, but exist of course.

460
00:55:30,810 --> 00:55:36,150
But now suppose that we are able to sample from each of the conditional

461
00:55:36,150 --> 00:55:40,530
distribution because the conditional distribution is one dimension of distribution.

462
00:55:40,530 --> 00:55:47,519
You you fix you in a V and then you fix V and W.

463
00:55:47,520 --> 00:55:58,349
This is the distribution of you given V and suppose that you are able to have this conditional distribution you given VW V,

464
00:55:58,350 --> 00:56:07,140
given you W and W give you v, v. Okay, so you have this three one dimensional condition of distribution.

465
00:56:08,100 --> 00:56:16,890
Suppose this three pieces available and you are able to sample from okay, so how the give sampling would proceed.

466
00:56:17,070 --> 00:56:22,330
Okay, it's doing that like this. So you start from arbitrary values, okay?

467
00:56:22,410 --> 00:56:30,750
You specify arbitrary values as long as they're in there to map the distribution, which is easy to specify,

468
00:56:31,320 --> 00:56:41,250
then you sample where you draw a random number from this competition distribution where the you and a V are fixed at initial value.

469
00:56:41,850 --> 00:56:48,990
Okay, this becomes a one dimension of distribution. You draw a random variable, random number from this universe.

470
00:56:49,140 --> 00:56:53,580
I mean, this conditional distribution condition and the initial values.

471
00:56:53,850 --> 00:56:59,840
Okay, one does one to measure distribution you no value one then after you finish your one,

472
00:56:59,850 --> 00:57:15,660
now you plot you you draw V from the condition of this v giving you and v now you fix the you at the most recent sort of draw from the previous step.

473
00:57:16,170 --> 00:57:20,649
Okay, so you have your one and W zero because W has not been updated yet.

474
00:57:20,650 --> 00:57:27,690
So. Right. So you still have old W zero, but you have new you one which has been dropped in the previous stuff.

475
00:57:28,170 --> 00:57:33,840
So now you given this, then you can draw a random number from this condition of distribution.

476
00:57:34,440 --> 00:57:41,399
V Okay, so big of. V one Now you keep doing this, then after you get you, you, you want to be one,

477
00:57:41,400 --> 00:57:49,680
you can draw a w a random number from the conditions of this condition on the most recent update of values you want,

478
00:57:49,800 --> 00:57:56,310
namely you want the one you draw down. Okay, you're finished one cycle because after you finish this cycle.

479
00:57:56,730 --> 00:58:03,600
You 000 have been updated into u r v1 W1.

480
00:58:04,170 --> 00:58:08,130
Then you move to next step. You're doing the same thing.

481
00:58:08,350 --> 00:58:19,320
Okay, so you you draw your updating you you by drawing a random number from the convenient distribution of your given v1 your and you get you two.

482
00:58:19,890 --> 00:58:27,840
Then you fix the you two and w one and draw a new number v2 and so on and so forth.

483
00:58:27,930 --> 00:58:37,319
You just keep doing this very simple, right? The obvious thing is build up to the assumption that you are able to figure out this three marginal ah,

484
00:58:37,320 --> 00:58:43,530
three one dimensional condition of distribution. Okay. So you are able to know how to draw them.

485
00:58:43,980 --> 00:58:47,700
Then you just keep doing this one many times, for example.

486
00:58:47,910 --> 00:58:56,260
Okay. So the question here is really the the fundamental solution.

487
00:58:56,480 --> 00:59:09,200
Okay. So after a large number of trivializing one sort of like 100,000, for example, or one, okay, you attend this number.

488
00:59:09,530 --> 00:59:13,060
Okay. So you keep running this. So,

489
00:59:13,070 --> 00:59:23,150
so the question for you is what are now the draw samples are relevant because remember that your original question

490
00:59:23,240 --> 00:59:31,760
you were originally what you want here is to draw you we w from this joint distribution but you cannot do it.

491
00:59:32,210 --> 00:59:34,640
So you take a compromise and you say, okay,

492
00:59:34,640 --> 00:59:41,330
I'm not going to I'm not able to do all this joint distribution because I don't even know how this joint division looks like.

493
00:59:41,900 --> 00:59:48,080
But I'm able to somehow figure out how to do the conditional distribution.

494
00:59:48,080 --> 00:59:55,910
This is lower dimensional much, much, much lower dimensional distribution, which I'm able to work out and and draw doing the numbers.

495
00:59:56,570 --> 01:00:08,660
But so after you do this, the question here is by this kind of the sequential drawing scheme, what are not the the numbers draw from this?

496
01:00:09,230 --> 01:00:16,010
So the conditional distribution actually are the samples drawn from this joint distribution?

497
01:00:16,550 --> 01:00:22,460
There's a gap, right? Therefore, can you draw out conditionally, sequentially going for easy, easy, easy, you can draw.

498
01:00:22,880 --> 01:00:32,150
But whether or not the draw samples are actually the random numbers draw from joint distribution, that's something you like to it to have.

499
01:00:32,630 --> 01:00:41,060
Right? That's the final question. Okay. No, not always, too, but 98 for gaming.

500
01:00:41,060 --> 01:00:56,299
Gaming. Okay. The show mathematically using, you know, probability theory under some like regulatory condition,

501
01:00:56,300 --> 01:01:00,200
like everybody's exponential or Evolve City or something like that.

502
01:01:00,200 --> 01:01:05,360
Right. So the empirical distribution of the sample.

503
01:01:06,440 --> 01:01:09,589
Okay. Based on this, remember,

504
01:01:09,590 --> 01:01:17,930
those are the samples thrown by this conditional keep sampling the conditional based on conditional draws based on conditional distribution.

505
01:01:18,680 --> 01:01:24,310
Then you put them together. This will converge not not of course not freelance.

506
01:01:24,320 --> 01:01:28,580
You have to have this regularity condition like exponential.

507
01:01:28,580 --> 01:01:36,230
Eric Odyssey okay. To converge to underline stationary distribution, I shall say joint distribution.

508
01:01:36,530 --> 01:01:39,830
Okay. In a exponential rate.

509
01:01:40,070 --> 01:01:52,070
So what this theory says is that even though you do all this conditional sort of keep sampling scheme and end of day if you keep doing this,

510
01:01:53,150 --> 01:01:56,780
then eventually this took place.

511
01:01:57,650 --> 01:02:05,270
Okay, well, be actually a random sort of vector draw from the joint distribution.

512
01:02:06,140 --> 01:02:09,740
You can improve this. Okay, so,

513
01:02:09,920 --> 01:02:17,270
so so because of the theory when you this will happen when be spawn the B has to be

514
01:02:17,270 --> 01:02:24,350
large enough so that the this true becomes actually a draw from the joint distribution.

515
01:02:24,890 --> 01:02:32,150
When that happens we give the name called BR you burying is the the magic point

516
01:02:32,600 --> 01:02:39,410
okay when this triplet becomes actually the draw from your joint distribution.

517
01:02:40,490 --> 01:02:42,470
Okay so this called burn.

518
01:02:43,940 --> 01:02:55,520
Okay, so anything draw after the burning, then you basically have this repeat measurements, you have this random draws from joint distribution.

519
01:02:55,910 --> 01:03:00,799
So if you continue to draw after the time of burning B supposed to be burning

520
01:03:00,800 --> 01:03:08,750
time is the iteration that burning happens denoted by B anything after that,

521
01:03:09,590 --> 01:03:16,460
the sample will be useful because they're all regarded as their draws from the joint distribution.

522
01:03:16,850 --> 01:03:28,670
Suppose you continue to draw and many after that critical point then you can use this draws to calculate the empirical distribution.

523
01:03:29,300 --> 01:03:32,570
So that's the theoretical basis of missing C okay.

524
01:03:33,410 --> 01:03:41,090
So loosely split comparing refers to A, such a B or your time point or iteration,

525
01:03:41,390 --> 01:03:49,190
after which Markov process has no memory about the initial state that the process starts is for getting initial.

526
01:03:49,390 --> 01:03:59,530
It is not trivial. Some sample passes to draw from a mark of ten sensitive samples are drawn from Markov chain while the condition of distribution.

527
01:03:59,880 --> 01:04:14,980
Okay so so you know but under some conditions that this process could can forget about the initial position so that you reach the the time of burden.

528
01:04:15,150 --> 01:04:26,900
Okay. So. So know M.S. applications of this algorithm.

529
01:04:26,900 --> 01:04:35,870
We really need to find that type of beef because that's a very critical point for this situation.

530
01:04:35,990 --> 01:04:42,450
Right. So here is the MATTHEWS point B, there's this.

531
01:04:42,510 --> 01:04:49,229
This is not unique. Right. So so. So, anyway, here is the first iteration you start with, right?

532
01:04:49,230 --> 01:04:55,590
You draw this, you know, conditional distribution is more like you write it from Markov trend because you're doing this condition to throw.

533
01:04:56,200 --> 01:05:06,510
And at this point it's called bringing the anything observe this the samples about example fun to a distribution.

534
01:05:06,900 --> 01:05:09,960
The question here is how do you find this guy? Right.

535
01:05:10,560 --> 01:05:15,000
So this is become in theory is when this goes to infinity,

536
01:05:15,480 --> 01:05:24,370
the samples will be you be available samples both quick where the question here is practically one day how large this should be, right?

537
01:05:24,450 --> 01:05:35,820
So on. So that's basically the the issue people need to use different diagnostic diagnostics and different ways to judge that.

538
01:05:36,860 --> 01:05:41,820
Okay, so let me talk about theming and ah, over relaxation.

539
01:05:44,460 --> 01:05:51,150
So so if multiple sample pass or generate and initial state from Markov process,

540
01:05:52,020 --> 01:06:02,550
the and point of this burning be this sample pass would come together and produce similar sample pass afterwards under such

541
01:06:02,760 --> 01:06:11,730
circumstances circumstances the the triplets you draw after that will be approximate regardless sample from the joint distribution.

542
01:06:12,120 --> 01:06:17,400
This is basically suggested by many authors in practice.

543
01:06:17,760 --> 01:06:22,550
So so what you're trying to do here instead of we started is the y initial value.

544
01:06:22,860 --> 01:06:30,840
The default is that you start with three initial values, three, you can use five, we use six whatever, but the default is three.

545
01:06:32,760 --> 01:06:37,110
I put zero here. So you randomly pick out three initial values.

546
01:06:37,410 --> 01:06:42,470
So I'm talking about use. There are these zero and w okay,

547
01:06:42,810 --> 01:06:47,969
so this is the initial value choose random which was three different lives because you draw

548
01:06:47,970 --> 01:06:53,460
this according to container distribution of course that the previous values were fact,

549
01:06:53,700 --> 01:06:56,790
the two draws. So you have this pass.

550
01:06:57,510 --> 01:07:00,540
Right, get this.

551
01:07:01,080 --> 01:07:10,150
Okay then this is back to of course, I'm just making this a little bit too abstract, but I'm telling conceptual what happens.

552
01:07:10,350 --> 01:07:16,860
Of course, this is back to our three elements. So this is three values starting right now.

553
01:07:16,860 --> 01:07:24,360
You have not, right? Of course, value depends on your initial value, but eventually, somehow this becomes the greater of it.

554
01:07:24,360 --> 01:07:35,610
If this process satisfy this very condition, at certain point they will converge because afterwards that's when burning happens.

555
01:07:35,610 --> 01:07:46,560
The process, you'll figure this out in this photo. So this one could go, you know, very widely, but somehow get together afterwards.

556
01:07:47,220 --> 01:07:53,700
So the initial value would have that effect in like first 10,000 of 50 solid iterations.

557
01:07:54,300 --> 01:07:59,550
But after a certain time, they start to make it a big error.

558
01:07:59,610 --> 01:08:05,280
Right. So this is the burning. You can monitor this, right?

559
01:08:05,280 --> 01:08:08,549
So given three randomly choose initial values,

560
01:08:08,550 --> 01:08:16,470
then try to see the pass solution passed and our of states that this free prop the pass

561
01:08:16,980 --> 01:08:25,680
become all together mixed together and then they basically are sort of tangles together.

562
01:08:26,160 --> 01:08:34,810
That basically means that the this things have no influence from image but they forgot about all the about.

563
01:08:35,190 --> 01:08:40,260
So this time typically is choosing tasks that for the time that people like to see.

564
01:08:40,360 --> 01:08:54,990
Okay so so however after that burning time, if you are able to identify, you need to generate additional m values from this Markov chain to,

565
01:08:55,860 --> 01:09:01,260
you know, to use them as the source of the values from the joint distribution.

566
01:09:01,270 --> 01:09:10,890
Okay. So, so here you continue to draw any sort of graphs right after this.

567
01:09:11,250 --> 01:09:22,010
So for each one, of course, they are according to this game and game theory, they are all like B plus period, right?

568
01:09:22,560 --> 01:09:27,560
The B plus two and w b plus K, right.

569
01:09:27,570 --> 01:09:36,150
They're all coming from this joint draw from going distributions and you could a draw from the joint distribution.

570
01:09:38,020 --> 01:09:41,760
Listen would right now.

571
01:09:41,850 --> 01:09:46,530
The question here is how you're going to use the invalid straw from outside burning

572
01:09:46,530 --> 01:09:54,660
to make statistical analysis because there are all from the conditional distribution.

573
01:09:55,050 --> 01:10:06,180
There is the issue of dependance. Okay. So that's why I there are generally calling because there is due to all from the conditional distribution.

574
01:10:06,780 --> 01:10:13,770
This would make the direct use of the cluster theory a law of law of large number of central nervous theory.

575
01:10:13,860 --> 01:10:22,349
Question for because in your sixth or two or you or six y, you will learn central new material to go off large number.

576
01:10:22,350 --> 01:10:26,280
You always assume independent sample. Here you do not have any.

577
01:10:27,540 --> 01:10:30,810
You have called this out because there are 200, including two conditions.

578
01:10:33,270 --> 01:10:41,430
So. So you'll have a sample and each triplet there are genera from joint distribution, but they are calling it.

579
01:10:42,060 --> 01:10:45,060
So how do you deal with that correlation? Okay.

580
01:10:45,070 --> 01:10:53,610
Mincemeat. There are many people who discuss the strategy to deal with the simplest way is doing this something called thing.

581
01:10:54,270 --> 01:10:58,920
Okay. What they are doing here is they trying to thing the chain, right?

582
01:10:59,430 --> 01:11:08,729
So instead of you record every octave here, instead of you record every update, you record as a 50 update,

583
01:11:08,730 --> 01:11:13,680
for example, or 100 update because this cheap right is everything is general by your computer.

584
01:11:14,610 --> 01:11:19,510
So here is the burning. So you just ignore the generate until.

585
01:11:20,580 --> 01:11:31,410
So here's you get and be positive. You do not record B plus one because to say next recorded that it is out of you 15.

586
01:11:31,650 --> 01:11:36,810
So you get a jump. Okay, so that's called a thing.

587
01:11:37,050 --> 01:11:43,860
Okay. So your thing, the chain, and then you can reduce the out of correlation from this.

588
01:11:44,340 --> 01:11:51,419
So 50 is the default and people always use it, but you can increase or decrease.

589
01:11:51,420 --> 01:12:00,570
It depends on your situation. You look at the operation plot two to see how strong the correlation would be.

590
01:12:00,990 --> 01:12:10,400
And this corporation of correlation is a concern in the use of the classical like symbol theory.

591
01:12:10,840 --> 01:12:14,130
So you need the thing to chime by record. Every one fifties,

592
01:12:14,490 --> 01:12:22,830
some iterations or even every 100 to it's not a way to reduce all the correlation is

593
01:12:22,830 --> 01:12:28,950
Niels this is professor from universe Toronto News over a relaxation method that

594
01:12:28,950 --> 01:12:34,379
generates multiple random variance from a conditional distribution and each iteration

595
01:12:34,380 --> 01:12:39,780
and selects the one that is most negative is according to this lack of samples.

596
01:12:40,350 --> 01:12:46,860
This is a similar idea to the antithetical algorithm in random number generation.

597
01:12:47,280 --> 01:12:56,579
So here it says is that the general theory is what number to contain multiple numbers and trying to find choose one

598
01:12:56,580 --> 01:13:03,420
to generate by 5050 random numbers and one iteration and people of one has the smallest correlation is the proof.

599
01:13:04,590 --> 01:13:10,530
Okay. That would be sort of anti correlation sort of exercise.

600
01:13:11,130 --> 01:13:16,230
If you combine this two method over relaxation and thing at the very end,

601
01:13:16,230 --> 01:13:24,150
you can really get something that are relatively independent, obviously very little, a core of all the creation.

602
01:13:24,160 --> 01:13:28,710
So you can use the sample to make inference. So how are going to make an inference?

603
01:13:28,740 --> 01:13:36,090
Well, first, before it, if you have idea sample, then you can do a lot of things you like, right?

604
01:13:36,720 --> 01:13:40,250
So after the completion of sample generation, this all be done.

605
01:13:40,260 --> 01:13:45,930
The computer like our jacks the what I'm going to talk about they already implement this.

606
01:13:46,140 --> 01:13:50,910
You give me the gamma pulse model that can generate this at one meeting.

607
01:13:50,910 --> 01:13:54,700
Data points for you you can easily generate from your computer.

608
01:13:54,720 --> 01:14:00,240
Okay. B After the completion of sample generation,

609
01:14:00,240 --> 01:14:06,750
an important issue is to estimate margin distribution of each variable that each variable here means a parameter, right?

610
01:14:06,750 --> 01:14:09,840
You don't need estimate margin distribution. Why?

611
01:14:09,990 --> 01:14:15,720
You need to estimate the margin, the conditional distribution of your parameter giving of data.

612
01:14:15,900 --> 01:14:16,100
Okay.

613
01:14:17,010 --> 01:14:30,060
So what you can do here to post this is you allow the alpha value, for example, association between the mortality and what the vaccine vaccination.

614
01:14:30,300 --> 01:14:34,350
Okay. So you is a parameter. So this is posterior distribution.

615
01:14:34,980 --> 01:14:44,880
You look at this because you already have. This conditional distribution that allows you to, you know, generate this random number,

616
01:14:45,090 --> 01:14:51,600
you know, from this conditional distribution of distribution, then you have m value, okay?

617
01:14:51,790 --> 01:15:00,420
Generate either use thinking operation or you use this over a relaxation or combine of this two, you get like,

618
01:15:01,470 --> 01:15:07,830
like consolidated of this extra values that you can take the average because there's a low of last number.

619
01:15:08,040 --> 01:15:16,440
Okay, then you can, you know, get a consistent estimate of your of your something or you can copy the meaning of this distribution.

620
01:15:16,530 --> 01:15:24,480
You can do a lot of things now when you have a sample from distribution, which turns out to be more efficient, actually,

621
01:15:24,660 --> 01:15:31,110
to use the density function than the estimate based on empirical distribution such as curve is measured using the samples.

622
01:15:31,290 --> 01:15:34,370
If you cannot figure out that the conditional distribution of you,

623
01:15:34,380 --> 01:15:41,100
if you are now waiting to use the conditional distribution to estimate the the marginal distribution,

624
01:15:41,700 --> 01:15:51,890
you can just use the, you know, this non parametric estimate, you can draw histogram and then you use a col smooth for on top of that.

625
01:15:52,320 --> 01:15:58,140
And the method that you use in the classical statistic can be used to summarize your data.

626
01:15:58,200 --> 01:16:05,250
Okay. So if you, if you are waiting to use this conditional distribution, that's as good.

627
01:16:05,580 --> 01:16:12,000
If you don't have that distribution where that distribution looks very popular, you don't want to use it,

628
01:16:12,000 --> 01:16:18,329
then you can use simply the curve method to estimate that after you get this distribution,

629
01:16:18,330 --> 01:16:28,320
of course you can calculate the mean median quintiles and so on as your statistic to make estimation and inference for your parameters.

630
01:16:28,470 --> 01:16:37,920
Okay, so, so that's what people do in M same C and as I said that these the available samples we

631
01:16:37,920 --> 01:16:44,610
can obtain basic statistic as the mean standard version meaning so you have m samples.

632
01:16:47,070 --> 01:16:50,790
So here you have M samples, right? So I did samples.

633
01:16:52,020 --> 01:16:57,210
So those samples are samples of your parameter. You can make order statistics, right?

634
01:16:57,270 --> 01:17:09,510
You rank from lowest and highest. You can calculate the corresponding .2.025 quantile and 0.975 quantile from the order statistics.

635
01:17:09,660 --> 01:17:20,160
Right among the m values you generate from m same c grip sampler and then you can construct a 95 credible interval is the first inference.

636
01:17:20,520 --> 01:17:26,190
Okay. And you know, you can also do the mode estimation.

637
01:17:26,190 --> 01:17:38,280
This will be equivalent to memory. So you find the, you know, the, the kernel this estimation and you can estimate the mold of this kernel.

638
01:17:38,280 --> 01:17:47,220
This is the estimation. So so there are many, many things you can do after we get sort of approximately i d data from CMC.

639
01:17:49,460 --> 01:17:55,160
So of course, then the issue comes to really, how do you determine this?

640
01:17:55,160 --> 01:17:59,240
Very, very, very, very, very critical quantity.

641
01:17:59,270 --> 01:18:09,740
Bernie, how do you know that after this, the everything will be okay or, you know, this is the gate of heaven, very big how you get there.

642
01:18:10,010 --> 01:18:13,790
So this is very, very critical thing that you need to figure out.

643
01:18:14,360 --> 01:18:19,250
And so so based on inference volumes, then see,

644
01:18:20,000 --> 01:18:27,530
practically you've all said good diagnostic work in which converges and also is must since is critical.

645
01:18:28,070 --> 01:18:39,050
To make a valid inference you have to really make a diagnosis that what or not a certain choice a b is a right choice of this, Bernie.

646
01:18:39,230 --> 01:18:45,800
Okay. So this is really a very subjective choice in practice, people.

647
01:18:46,280 --> 01:18:52,600
I'm going to choose B 50,000, of course, that that you can imagine that if it happens here, right?

648
01:18:53,040 --> 01:19:01,430
It happens here. Anything after that, it will be safe. The problem here is just you cannot choose a B before this point.

649
01:19:01,610 --> 01:19:09,050
Right. So you need to really make a diagnosis to make sure that the the choice is is right choice.

650
01:19:09,350 --> 01:19:15,829
If the bearing happens to solid, if you choose in practice, 50 solid, it's safe choice.

651
01:19:15,830 --> 01:19:22,010
No problem. All right. So but the question here is that you cannot choose tensile is is not the right thing,

652
01:19:22,010 --> 01:19:31,040
because at that time, this process OC this through process have not forgotten the initial got it.

653
01:19:31,070 --> 01:19:41,540
So it's an issue about a dependent and this becomes very a subtle issue you need to really deal with in practice nowadays if you look at literature,

654
01:19:41,540 --> 01:19:43,579
people just take this for granted.

655
01:19:43,580 --> 01:19:55,830
I mean, people don't really go through this rigorous attack in the sticks procedure, in this sort of comfortable conforming the choice of burden.

656
01:19:56,240 --> 01:20:01,190
And my time, anybody who's M.S. will be sort of monitored.

657
01:20:01,760 --> 01:20:04,969
How do you determine this burning?

658
01:20:04,970 --> 01:20:08,270
How do you determine this number? There is nobody asked this question.

659
01:20:08,270 --> 01:20:10,970
Maybe this question had been asked too many times.

660
01:20:11,510 --> 01:20:21,799
And as becomes a trivial question to ask, if someone gave a presentation using MSNBC, a very natural question here is how do you determine brain?

661
01:20:21,800 --> 01:20:26,840
How do you choose the you know, how to verify the convergence of the algorithm?

662
01:20:27,110 --> 01:20:35,610
This has been asked many, many times the my time with this MSM as it becomes available, you know how you know,

663
01:20:35,660 --> 01:20:42,649
people always ask this question and there always sort of some subjectivity involved in the choices.

664
01:20:42,650 --> 01:20:49,070
But nowadays people somehow stop asking the question because I don't know, maybe it's a very boring question,

665
01:20:49,070 --> 01:20:52,880
but it is a good question, is a relevant question, is a very pointed question.

666
01:20:53,180 --> 01:20:56,900
But it just probably becomes a very boring question to us.

667
01:20:57,260 --> 01:21:04,490
I mean, people think is a question this low intelligence, but it is a very, very creative questions to ask.

668
01:21:04,820 --> 01:21:08,420
So what I'm saying is that you, in fact, is not free lunch.

669
01:21:09,230 --> 01:21:11,870
You have to really deal with this.

670
01:21:12,860 --> 01:21:21,620
And there are some procedures that people have built up in the literature and see for a practical use of this algorithm.

671
01:21:22,990 --> 01:21:32,480
And so I will talk about a few. I don't have time today, but there's something that there's a package called CODA.

672
01:21:33,680 --> 01:21:47,830
This is the one that. And it's widely used for the diagnosis of the convergence of M.S. That's something people use,

673
01:21:48,010 --> 01:21:59,830
even though our jacks they also connect to this called out that you can use to to more diagnosis mostly force can't be be this the burning point.

674
01:21:59,980 --> 01:22:04,240
Okay that's something I like to talk about next week.

675
01:22:05,680 --> 01:22:07,960
Okay. That's it for today.

