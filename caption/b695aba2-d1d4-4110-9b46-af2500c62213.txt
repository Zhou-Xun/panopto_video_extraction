1
00:00:01,470 --> 00:00:06,810
All right. So we have some administrative stuff that we need to talk about.

2
00:00:07,500 --> 00:00:19,960
So let's start there. So. This morning, I'm going to just go to the last slide of the handout on the zero and.

3
00:00:20,490 --> 00:00:25,240
Yes, question. Oh. Thank you.

4
00:00:35,120 --> 00:00:40,430
Okay. Thank you for letting me know. All right.

5
00:00:42,370 --> 00:00:49,420
So we will be looking shortly at the last slide of handout eight.

6
00:00:49,600 --> 00:00:50,889
We kind of rushed through it.

7
00:00:50,890 --> 00:01:02,020
And I just wanted to say a few final words about that, because we have this quiz coming up, and that is the last handout that's covered on the quiz.

8
00:01:02,110 --> 00:01:08,629
So let's just sort of. Get our notes in order about what's happening with the quiz.

9
00:01:08,630 --> 00:01:14,650
So. We're going to be starting new material today.

10
00:01:14,680 --> 00:01:20,020
Handout nine. But that material, I won't have homeworks assigned.

11
00:01:21,200 --> 00:01:23,210
For you to study with before the quiz.

12
00:01:23,220 --> 00:01:31,580
So I'll be assigning a homework after the quiz related to survival analysis material that I'll be speaking about today.

13
00:01:31,940 --> 00:01:41,450
So for the quiz, I've posted some review materials, so this should be available to you,

14
00:01:41,870 --> 00:01:46,820
but I will cover it in, you know, in a few lectures as it gets closer to the quiz.

15
00:01:47,390 --> 00:01:50,870
And oh, that's really strange how that showed up like this.

16
00:01:52,820 --> 00:02:02,059
My idea of a quiz review is to summarize information for you, but the main purpose is so that you have chances to ask questions,

17
00:02:02,060 --> 00:02:05,930
and it just gives context for how I'm going to review the material.

18
00:02:06,380 --> 00:02:17,720
So I am hoping that you are working very diligently on your current homework, which will be the last homework covered on the exam,

19
00:02:18,560 --> 00:02:27,770
and that you'll be thinking about preparing your one page of notes front and back that you're allowed during the exam,

20
00:02:28,670 --> 00:02:33,710
and that you'll show up to the review with questions ready to go.

21
00:02:33,740 --> 00:02:41,090
You know that you're done. You've done a little bit of legwork so that you'll have good questions and make a good use of that time.

22
00:02:41,360 --> 00:02:46,370
But if you are too busy and you just don't get to it out the review handout.

23
00:02:46,370 --> 00:02:51,770
I will go through the review handout and you know, your classmates might have some things that you can benefit from.

24
00:02:52,580 --> 00:02:59,660
So no pressure there. So again, you get a page front and back, four of notes for the quiz.

25
00:02:59,990 --> 00:03:06,549
The quiz is going to be online. So you don't need to come in to this room that day.

26
00:03:06,550 --> 00:03:12,880
You can do it at home. And I plan to open up the quiz at 6 a.m. and close at 11 a.m.

27
00:03:13,540 --> 00:03:21,549
It's only a two hour quiz, but I like to cover my bases in case there's a technical difficulty and you have to restart it or

28
00:03:21,550 --> 00:03:30,340
something that you have time to have that happen to you and and not have your quiz experience ruined.

29
00:03:30,910 --> 00:03:37,300
All right. During the quiz, I will be with my cell phone.

30
00:03:37,930 --> 00:03:41,590
If you have any problems during the exam or any questions, you can text me.

31
00:03:42,280 --> 00:03:46,880
I will end with a number where I can reach you. I'll call you back. All right.

32
00:03:49,510 --> 00:03:54,750
So. Uh, I guess the last administrative thing.

33
00:03:56,060 --> 00:04:01,160
Is that I plan to put up a practice quiz.

34
00:04:03,110 --> 00:04:09,170
Soon. I think that the only problem with the practice quizzes, a few little are things I'm sorting out.

35
00:04:10,820 --> 00:04:15,770
But that will be available to you probably. I'll make I'll open it up sometime this weekend.

36
00:04:17,220 --> 00:04:24,390
And it's not meant to be a practice quiz that covers the exact same types of questions or anything like that.

37
00:04:24,390 --> 00:04:29,550
It's just to give you an idea of the level and how my mind works when I'm writing questions.

38
00:04:30,000 --> 00:04:34,319
So it'll give you a little bit more insight on what to expect.

39
00:04:34,320 --> 00:04:37,530
But the questions won't be necessarily.

40
00:04:38,710 --> 00:04:43,320
Similar. I mean, they'll just be the level will be similar. Okay.

41
00:04:45,890 --> 00:04:49,670
All right. Remember, it's only worth 15%, one five.

42
00:04:50,420 --> 00:04:56,180
So the timing of the course is that you get a chance to really study this material just

43
00:04:56,180 --> 00:04:59,720
as we're getting into a bunch of new stuff that you haven't seen in previous courses.

44
00:04:59,990 --> 00:05:07,340
So we get that foundation super firm because we're going to be learning models very quickly from this point on,

45
00:05:08,030 --> 00:05:11,060
leaning on all these this background, this solid foundation.

46
00:05:12,780 --> 00:05:16,200
Any questions about the administrative part of the exam?

47
00:05:17,210 --> 00:05:20,980
Quiz. Okay.

48
00:05:22,060 --> 00:05:26,469
All right. So. Good.

49
00:05:26,470 --> 00:05:31,540
We're recording, too. Good to double check that. All right, so the final slide.

50
00:05:32,440 --> 00:05:43,700
Oh, wait, there's one more thing. So Homework three is due October 16th, just before midnight.

51
00:05:44,210 --> 00:05:51,440
But I wanted to give you a heads up that because of that homework deadlines, proximity to quiz one.

52
00:05:51,890 --> 00:05:55,270
I won't be. It won't be possible to give extensions.

53
00:05:55,340 --> 00:05:58,670
I will need to post solutions for students to study with.

54
00:05:59,510 --> 00:06:08,390
So I often have more flexibility to grant extensions because I can delay posting solutions for a bit.

55
00:06:08,900 --> 00:06:14,900
But I won't have that flexibility for homework. Three. So you need to prepare accordingly.

56
00:06:14,930 --> 00:06:19,540
Don't put it off. Good. Get it done. All right.

57
00:06:19,550 --> 00:06:25,410
So just a little bit of warning there. All right, now let's go back to hand out a.

58
00:06:27,610 --> 00:06:38,730
This last slide. And so we've done two handouts relating to modeling count outcomes.

59
00:06:38,820 --> 00:06:45,450
And Poisson regression is the most commonly known model,

60
00:06:46,200 --> 00:06:56,670
but it has kind of been replaced to some extent with the three other modeling paradigms that came out that were developed after Poisson regression.

61
00:06:57,210 --> 00:07:04,380
So person regression is still a very good method, but be aware that it underperformance in two situations.

62
00:07:04,770 --> 00:07:11,940
One is if there is over dispersion in the data, so there's more variability in that counts than the position distribution will allow.

63
00:07:12,450 --> 00:07:24,089
And the advice is, if that's your issue, to try negative binomial regression and you can see whether negative binomial is preferred to the Poisson,

64
00:07:24,090 --> 00:07:30,239
be a good ratio test when you have count data that can be tabulated.

65
00:07:30,240 --> 00:07:35,880
So there's not really continuous coverage. Everything could be put into tables if you wanted to.

66
00:07:36,510 --> 00:07:45,630
Then looking at the deviance value divided by the degrees of freedom shown in your output can be useful to spot a potential over dispersion problem.

67
00:07:46,290 --> 00:07:52,050
But you know, you can go straight to the likelihood ratio test as well to to look at that.

68
00:07:52,290 --> 00:07:59,549
We had also actually just graphed or Dunkirk means to look at the mean counts related

69
00:07:59,550 --> 00:08:07,170
to the variability of those counts in the data when we looked at our Poisson dataset.

70
00:08:08,610 --> 00:08:13,770
So the other problem that Poisson regression by itself doesn't handle well as excess zeros in the data.

71
00:08:14,160 --> 00:08:22,530
And so this comes up a fair amount when the events you're trying to count are really.

72
00:08:23,660 --> 00:08:28,850
Only. Only a certain subject in your sorry backup.

73
00:08:29,210 --> 00:08:34,550
Only a subset of your cohort is really susceptible to having those counts.

74
00:08:35,390 --> 00:08:41,660
And so are the zero inflated models that we learned in in this particular handout are very,

75
00:08:41,660 --> 00:08:49,250
very good for separating out the probability of having an excess zero because those kinds

76
00:08:49,250 --> 00:08:53,900
of people weren't susceptible to the having their about events that are being counted.

77
00:08:54,810 --> 00:09:02,940
Versus the people who are susceptible to the events you're counting and what is influencing those counts coming up.

78
00:09:03,630 --> 00:09:10,200
And so zero inflated model. I actually live in the zero inflated model landscape because in pulmonary medicine,

79
00:09:10,200 --> 00:09:16,650
it's very hard to tell when patients get to the point where they're susceptible to exacerbations on a regular basis.

80
00:09:17,580 --> 00:09:20,730
And so I usually have to use those models. So.

81
00:09:22,620 --> 00:09:28,590
Just this is the kind of overall summary of how I think about which model to choose.

82
00:09:29,430 --> 00:09:35,040
So be aware that the Poisson model expects to see zero counts with probability each of the minus me

83
00:09:35,880 --> 00:09:41,220
where this new one I think of it is what you are actually putting into your ex model for the counts.

84
00:09:41,220 --> 00:09:51,540
You know it's the mean according to the risk profile that are in the covariates and so.

85
00:09:52,780 --> 00:10:01,780
If you are seeing a lot of events larger than than the mean that are zeros, then you need to kind of use these zero inflated models.

86
00:10:03,230 --> 00:10:08,720
Both Das and Aachen estimate these models, and there is a have one macro that can help you assess.

87
00:10:09,920 --> 00:10:15,410
Whether this more complex modeling strategy is needed, it's an imperfect method.

88
00:10:15,800 --> 00:10:22,580
And so additional methods that you can look at, are there any covariates that come up as significant in your zero model?

89
00:10:23,240 --> 00:10:23,750
You know,

90
00:10:24,080 --> 00:10:34,730
and just being aware of of what the Poisson model is assuming for the number of zeros it expects to perform well as it as a modeling paradigm for you.

91
00:10:36,560 --> 00:10:45,520
Okay. So. This is the end of what I'm going to be teaching you about count outcomes for a little while.

92
00:10:45,540 --> 00:10:51,780
We're going to come back to it later in the course when we will be modeling dependent outcomes.

93
00:10:51,810 --> 00:10:55,680
So count maybe if there's dependent counts that are measured within the same person.

94
00:10:56,010 --> 00:11:00,810
We'll have a whole different handout to address that situation.

95
00:11:05,700 --> 00:11:09,030
Okay. So now we're going to start.

96
00:11:11,140 --> 00:11:17,140
Yet another brand new modeling situation.

97
00:11:22,000 --> 00:11:28,530
So the field is called survival analysis. But really what we're modeling is just time to any type of event.

98
00:11:28,540 --> 00:11:33,550
So you're following people up and you want to know the time it takes to have a certain event of interest.

99
00:11:34,060 --> 00:11:41,650
So survival analysis kind of became the terminology because, you know, time of death was such a common thing to want to model.

100
00:11:42,130 --> 00:11:45,430
But there are lots of different times to event that you might model.

101
00:11:45,510 --> 00:11:56,370
So we'll look at some of those shortly. So just a quick start of it is, you know, that there's an end point of interest.

102
00:11:56,400 --> 00:12:00,720
You want a model that involves time until a life changing event.

103
00:12:03,090 --> 00:12:09,380
And you want to describe the probability of that occurring or not occurring by some time.

104
00:12:09,450 --> 00:12:17,310
So there's going to be some estimation of the probability of sort of surviving event free over some period of time.

105
00:12:18,750 --> 00:12:29,790
And there's also interest in detecting associations between risk factors and whatever that failure time outcome is adjusted for confounders.

106
00:12:30,060 --> 00:12:38,670
And so we're going to be learning lifetime regression methods to do testing and describe inference about what covariates,

107
00:12:38,670 --> 00:12:44,790
what risk profiles are associated with people having these events more frequently or quicker.

108
00:12:46,430 --> 00:12:50,960
And so survival analysis methods are of great interest to researchers all over the map.

109
00:12:50,970 --> 00:12:55,400
So in medicine, biology, epidemiology, genetics, demography, health, economics,

110
00:12:55,700 --> 00:13:00,110
environmental, health sciences, health policy, engineering, many other specialties.

111
00:13:01,400 --> 00:13:05,930
I often think because I work in survival analysis, it's my research area.

112
00:13:05,930 --> 00:13:10,880
I often think, you know, whatever statistical method you've learned so far.

113
00:13:11,750 --> 00:13:15,890
Survival analysis, there's a form of if it were, survival analysis can be applied.

114
00:13:19,870 --> 00:13:28,600
So there's going to be a lot of new notation and jargon in this handout, and it's going to take time for you to unpack it all.

115
00:13:29,050 --> 00:13:32,860
So we may not get through the end of this handout.

116
00:13:33,430 --> 00:13:36,560
Historic. I mean, we may. It's possible,

117
00:13:36,890 --> 00:13:46,160
but historically there have been many questions about the notation from from the students who are hearing this material for the first time.

118
00:13:46,700 --> 00:13:50,190
And I expect those questions. This is going to be very, very new.

119
00:13:50,840 --> 00:13:56,840
So we might move through this handout a bit slow just because of it's not that the ideas are hard,

120
00:13:56,840 --> 00:14:00,770
it's just the language takes a little bit of time to pick up, clearly.

121
00:14:01,460 --> 00:14:08,420
So this is going to sound like I'm really starting off fairly simple, but the notation builds up very quickly.

122
00:14:09,500 --> 00:14:18,799
So in survival analysis, we tend to use TES for the failure timer survival time that we're collecting.

123
00:14:18,800 --> 00:14:28,460
So it t one would be the time to event, maybe time to death for the first participant to time to death for the second participant and so on.

124
00:14:28,790 --> 00:14:35,180
So if there are no people in the data set, then there's the survival time or the failure time.

125
00:14:36,050 --> 00:14:41,810
Two different ways of saying the same thing, really. It's like a cup, half full cup, half empty kind of a lingo.

126
00:14:42,440 --> 00:14:52,319
I'll be calling those TS. And although the time of interest may literal be literally be a survival time, it need not be.

127
00:14:52,320 --> 00:15:00,959
So we could be looking at time to death or we could be looking at time to hospitalization or time.

128
00:15:00,960 --> 00:15:05,100
It could be a good time to event like a time to defeat due to disease remission.

129
00:15:06,630 --> 00:15:10,230
It could be time to transplant failure, time to vision loss.

130
00:15:10,950 --> 00:15:19,499
I've seen datasets that are looking to time to re incarceration, time to infection, time to tumor shrinkage.

131
00:15:19,500 --> 00:15:30,569
It could be anything that is interesting that takes time to observe and particularly where the follow up time you might be able to watch.

132
00:15:30,570 --> 00:15:46,510
People can vary person to person. So suppose you have data where you're looking at the time from when a person got a transplant to death.

133
00:15:46,510 --> 00:15:56,260
Post-transplant. Select your beginning time. A follow up is when they got a transplant and I'm counting the months that they lived after transplant.

134
00:15:56,650 --> 00:16:04,210
And so t one equal to ten is just notation that person one died ten months post-transplant.

135
00:16:05,180 --> 00:16:11,300
Tier two equals 24 is notation that just means person two died 24 months post-transplant.

136
00:16:13,190 --> 00:16:18,050
And so on. And so everybody gets a transplant at different calendar times.

137
00:16:18,290 --> 00:16:23,569
And so the period of time you're following people is different depending on when they got their transplant.

138
00:16:23,570 --> 00:16:28,670
Right. And so the kind of time you start watching.

139
00:16:29,830 --> 00:16:33,129
The the people and the data set will vary from person to person,

140
00:16:33,130 --> 00:16:38,650
and that will kind of play a role in how we think about follow up time for each person.

141
00:16:43,780 --> 00:16:47,139
All right. So this is just to give you some context.

142
00:16:47,140 --> 00:16:53,740
I mean, these curves are very, very popular in papers. And you might have seen these types of curves already in your reading.

143
00:16:53,740 --> 00:17:02,020
But these are this is an example of kaplan-meier curves that are estimating survival probabilities over time.

144
00:17:02,170 --> 00:17:05,770
So the horizontal axis here is years.

145
00:17:06,870 --> 00:17:12,390
And the vertical axis axis is the probability that they.

146
00:17:14,720 --> 00:17:21,000
Literally survived. At least that time point in on.

147
00:17:21,390 --> 00:17:29,550
So everybody survi has a probability of one of surviving past time zero because that's when we're we start following the patients.

148
00:17:29,970 --> 00:17:34,709
Each individual patient has their maybe own time zero. But that's when we start following them.

149
00:17:34,710 --> 00:17:38,220
And no one is dying right at the moment. We collect them before our data set.

150
00:17:39,320 --> 00:17:46,790
At least not in this plot. The top line and the bottom line are two different groups.

151
00:17:46,790 --> 00:17:55,940
And in that we're we're following and comparing. And in this particular situation, this is interstitial pulmonary fibrosis patients.

152
00:17:57,290 --> 00:18:04,340
This came from an older figure where they were calling it usual interstitial pneumonia as a subset of IPF patients.

153
00:18:04,880 --> 00:18:13,970
And the predictor was to saturation during a six minute walk test that exceeded some dangerous level.

154
00:18:14,000 --> 00:18:16,190
So it's very well known now.

155
00:18:16,190 --> 00:18:24,740
But this was the paper that first described that if you have a person that's doing a six minute walk test and their oxygen,

156
00:18:24,860 --> 00:18:31,280
oxygen saturation drops below 88%, I mean, you see this little finger, you color the hospital had those things put on your finger.

157
00:18:31,280 --> 00:18:43,099
Right. So if you if you go below 88% while you're doing your six minute walk test, it's very, very indicative of how unhealthy you are.

158
00:18:43,100 --> 00:18:52,550
And for this particular diagnosis, the people who desaturated below 88% that this this lower curve is their survival probability over time.

159
00:18:53,390 --> 00:19:02,840
And the upper curve is the survival probability over time for those who didn't drop below that level of oxygen, have oxygen saturation in their blood.

160
00:19:04,860 --> 00:19:07,860
And so I'm going to just be kind of.

161
00:19:08,820 --> 00:19:14,040
Asking some obvious questions here. So the curve that you would prefer to be on is which one?

162
00:19:16,490 --> 00:19:19,700
The top one, right. Because they have a probability of surviving over time.

163
00:19:19,700 --> 00:19:22,850
That's higher all the way through than the bottom one. Yeah.

164
00:19:23,360 --> 00:19:32,839
Okay, good. And we're going to talk a little bit during this handout about why this plot is so herky jerky.

165
00:19:32,840 --> 00:19:36,230
So it's only it's drops and then it's flat drops and then it's flat.

166
00:19:36,650 --> 00:19:43,040
We're going to explain why that happens. And we're also going to kind of explain what these little hash marks are.

167
00:19:44,020 --> 00:19:48,580
Throughout the plot as well. So we're keen to see where those come from.

168
00:19:49,570 --> 00:19:52,990
And in this handout, by the end, you'll be able to do these plots.

169
00:19:53,560 --> 00:19:57,580
And in the next follow up out, you'll be able to do statistical tests.

170
00:19:57,580 --> 00:20:01,480
Comparing Are these groups significantly different from one another?

171
00:20:02,430 --> 00:20:06,570
You know. Or is this just random noise, how these plots came about?

172
00:20:07,050 --> 00:20:14,010
And then after that, we're going to start doing regression models where one covariate would be which group they're in,

173
00:20:14,010 --> 00:20:22,230
but we might want to adjust for other things like age or other measures of their risk profile that we want to and capturing, perform and adjust for.

174
00:20:23,020 --> 00:20:34,610
All right. So that's the goal. And again, I'm kind of easing you into this because of all the notation and jargon.

175
00:20:34,630 --> 00:20:39,150
So I wanted to just briefly review what you already know.

176
00:20:39,150 --> 00:20:50,190
You already know a lot of techniques, and if you knew everybody's time to invent completely, then you know how to analyze this data already.

177
00:20:50,850 --> 00:20:55,170
So. So consider times to event that are observed for every person.

178
00:20:56,420 --> 00:20:59,479
Sir. Excuse me. Excuse me.

179
00:20:59,480 --> 00:21:07,490
Excuse me. So if you knew. If you knew everybody's time to have been completely, you could watch them forever to see when they had it.

180
00:21:07,910 --> 00:21:14,720
Then you could estimate the probability of being alone a live longer than, say, two years very easily.

181
00:21:14,900 --> 00:21:18,830
And throughout the handout I start using little t equals ten.

182
00:21:18,830 --> 00:21:26,210
Just so that we don't have we have one less piece of notation. So if you want to estimate the probably being alive longer than ten years,

183
00:21:26,600 --> 00:21:30,829
you would just count how many people in the dataset live longer than ten years.

184
00:21:30,830 --> 00:21:35,300
You would have have it completely observed. You'd know it, and you divide it by the total sample size.

185
00:21:37,050 --> 00:21:41,690
So we could use sample proportions for unadjusted analysis.

186
00:21:42,000 --> 00:21:45,470
You know, just a little remark about that on the next slide as well.

187
00:21:45,480 --> 00:21:51,870
You know how to do sample proportions. You've maybe already done stuff like this for data sets where you had all these event times.

188
00:21:53,540 --> 00:21:58,159
And for a just analysis, you use logistic regression where you're modeling.

189
00:21:58,160 --> 00:22:04,190
Yes. No, they lived longer than ten years.

190
00:22:04,550 --> 00:22:06,710
And what impact impacted that?

191
00:22:09,030 --> 00:22:17,489
So that only captures survival probably at one point in time, and it does ignore the timing of events before and after time T or ten.

192
00:22:17,490 --> 00:22:23,129
So it's not really the full story if you have these event times and some people are having your events very,

193
00:22:23,130 --> 00:22:29,580
very quickly, like around five years, and some people are having their events very, very long away like 30 years.

194
00:22:30,270 --> 00:22:36,690
Reducing it to whether they were alive at ten years or not is kind of throwing away a lot of information in the data.

195
00:22:37,470 --> 00:22:45,510
So we're going to definitely want to use all the information and the data that we can, not just whether they were alive or not at one point in time.

196
00:22:47,400 --> 00:22:53,160
And so if you were estimating survival probabilities where all the event times are known.

197
00:22:55,020 --> 00:22:58,649
You would just use the sample proportion and you know how to do that.

198
00:22:58,650 --> 00:23:05,070
But what I want to do on this slide is get your feet wet on the notation again, so we start getting comfortable.

199
00:23:05,400 --> 00:23:13,049
So I've got an example data set where the following failure times are observed for ten patients and for convenience,

200
00:23:13,050 --> 00:23:21,930
the failure times have been ordered. So here are the event times that we're calling the event times t and the first person to have their event at two.

201
00:23:21,960 --> 00:23:26,820
Second person had their event at five. Their person had their event at eight and so on.

202
00:23:26,820 --> 00:23:33,959
Here are the ten event times. And we've used indicator variables before when we're describing covariates.

203
00:23:33,960 --> 00:23:42,240
But now I want to use this to describe whether or not person I had their event time greater than ten.

204
00:23:43,640 --> 00:23:48,830
All right. So we know with indicator functions it's either one or zero and it's a one.

205
00:23:49,370 --> 00:23:53,870
If the inside of the parentheses is the true thing and it's zero otherwise.

206
00:23:54,350 --> 00:24:00,650
So if we look through each of these individuals, we can actually say what that indicator function is.

207
00:24:00,680 --> 00:24:07,520
So for the first person within event time at two, that event time was not greater than ten, right?

208
00:24:07,550 --> 00:24:13,840
Two is not greater than ten. So we get a zero. Second person there have been time was five.

209
00:24:13,840 --> 00:24:17,260
Five is not greater than ten. So this indicator function is the zero.

210
00:24:18,390 --> 00:24:23,700
And same for eight and nobody had we don't have a ten in our dataset.

211
00:24:24,330 --> 00:24:29,090
But every value after ten. Their tie is greater than ten.

212
00:24:29,090 --> 00:24:30,500
So all of these get ones.

213
00:24:31,530 --> 00:24:39,240
And this is just a way to do bookkeeping, but we're going to be having a lot of weird notation and symbols with bookkeeping in this way.

214
00:24:39,840 --> 00:24:45,420
So this is step one of what that bookkeeping starts to look like.

215
00:24:47,940 --> 00:24:53,730
And so how do we estimate survival of, you know, the probability for that?

216
00:24:53,850 --> 00:24:58,139
We're going to be calling the survival probably a little time t that notation,

217
00:24:58,140 --> 00:25:02,760
Big S is going to stand for the probability that they're event times greater than little T.

218
00:25:03,000 --> 00:25:06,360
And again, just for context, I'm going to keep on saying little T is ten.

219
00:25:06,360 --> 00:25:14,850
So we're going to be looking at what's the probability that they that the people in this cohort survived greater than ten years.

220
00:25:15,240 --> 00:25:24,740
So. And all we would do is we would, you know, do the the p hat for that and we can use this notation to do that.

221
00:25:25,670 --> 00:25:29,659
The number of columns is ten. All right. The number of people in the dataset is ten.

222
00:25:29,660 --> 00:25:33,500
That's the denominator. And we count the number of ones.

223
00:25:33,500 --> 00:25:39,270
Those are the people who live beyond ten. And we don't need this notation to do that calculation.

224
00:25:39,660 --> 00:25:45,290
But, boy, it's going to help us get used to this kind of note notation that comes up again and again in the handout.

225
00:25:45,300 --> 00:25:56,910
So the probability of this people in this cohort living beyond ten years is one over the, you know, the number of people in this table, one over ten.

226
00:25:57,330 --> 00:26:02,100
And we're summing up actually all of these indicator variables.

227
00:26:02,250 --> 00:26:08,040
So this is equivalent to just counting the ones that the zeros don't contribute to the numerator.

228
00:26:08,340 --> 00:26:14,310
And so we can just look over all ten of those indicator functions and get the answer for P that we would usually have.

229
00:26:17,610 --> 00:26:24,929
All right. So we're still in the situation where we know all the event times and we could actually look at the average of the event

230
00:26:24,930 --> 00:26:30,960
times to to do an analysis that would be a little bit better than just looking at this of our probability at time.

231
00:26:31,530 --> 00:26:34,770
A little T or the probability of surviving.

232
00:26:35,040 --> 00:26:40,980
Greater than ten. Because we would be using information of how long people took to get to the event.

233
00:26:42,450 --> 00:26:53,130
And so if we were looking at the two to the the two two saturation groups from the Kaplan-Meier plot, we thought earlier,

234
00:26:53,940 --> 00:26:59,370
if we knew all the event times we could just take the average and the people who desaturated

235
00:26:59,370 --> 00:27:02,940
in the average and the people who didn't do saturate during their six minute walk test.

236
00:27:03,300 --> 00:27:08,910
And that's how we could analyze the difference between them using two simple T tests.

237
00:27:10,380 --> 00:27:12,330
And as long as the sample size is large,

238
00:27:12,780 --> 00:27:20,190
that two sample t test would look approximately normal and it would be what we would do for our unadjusted analysis.

239
00:27:21,760 --> 00:27:29,680
And for adjusted analyzes, you know, standard multivariable linear regression methods would be our go to analysis.

240
00:27:30,190 --> 00:27:37,360
Now, this isn't ideal for survival data. Uh, survival data has also this feature.

241
00:27:37,360 --> 00:27:46,450
All the event times are positive, right? You live zero or longer and the mean and the variance tend to be linked.

242
00:27:46,540 --> 00:27:50,020
Like the longer you live, the more variability the that times have.

243
00:27:50,410 --> 00:27:58,690
So it's not perfect, but it could be done. So the issue that I'm struck just summarize is that failure times are usually not normally distributed.

244
00:27:58,690 --> 00:28:02,590
They're right skewed perhaps even heavily with some people who live a long, long time.

245
00:28:03,690 --> 00:28:08,640
And predicted survival times must be positive. So standard linear regression.

246
00:28:09,390 --> 00:28:15,300
We might have to do something like take a log and model the log at the event times to kind of keep on top of that.

247
00:28:15,970 --> 00:28:23,940
And but you could you could take log of the survival times and you could model that.

248
00:28:23,940 --> 00:28:38,620
And, you know, it might be okay. So why do we need special survival analysis tools in this handout?

249
00:28:38,640 --> 00:28:45,040
I mean, I've just described a bunch of different methods that you already know for analyzing the data.

250
00:28:45,060 --> 00:28:49,950
When you see everybody's event times completely, every person's event time.

251
00:28:50,580 --> 00:28:58,060
So the problem is that most often you observe individuals for a fixed study duration and you know,

252
00:28:58,230 --> 00:29:02,450
so for instance, grants tend to be funded for four or five years.

253
00:29:02,460 --> 00:29:09,780
And after that point, you can't watch people. And there are lots of people who are still event free after the grant has ended.

254
00:29:10,380 --> 00:29:16,740
So a follow up time is exerted long before all individuals have experienced the event of interest.

255
00:29:19,280 --> 00:29:23,840
And so that limits our ability to apply standard analysis techniques that require complete data.

256
00:29:24,440 --> 00:29:29,480
And just going back to the plot. That we showed earlier.

257
00:29:31,350 --> 00:29:35,400
This is precisely what happened in this situation.

258
00:29:36,640 --> 00:29:40,110
The follow up actually ended.

259
00:29:40,680 --> 00:29:44,579
You know, the longest follow up they had was just past five years.

260
00:29:44,580 --> 00:29:48,900
And there was no other information in the data about what happened to the people who were still alive.

261
00:29:49,650 --> 00:29:53,160
And if you look at the survival probability at the LAT for this top group,

262
00:29:53,760 --> 00:30:03,059
the last time they had information on anybody from that top group, the probability of surviving past that time was still pretty high.

263
00:30:03,060 --> 00:30:08,340
It was well, it's all relative, but it was just under 60%.

264
00:30:08,340 --> 00:30:12,420
So there were a large number of people who hadn't had the event yet.

265
00:30:14,370 --> 00:30:18,149
You know, you can tell really easily because if the curve drops all the way down to zero,

266
00:30:18,150 --> 00:30:22,130
you know, you've pretty much observed what you need to observe.

267
00:30:22,140 --> 00:30:25,440
But there were a lot of people who were event free.

268
00:30:26,560 --> 00:30:29,650
When they were when they ran out of time to watch people.

269
00:30:30,040 --> 00:30:36,089
And that was actually true in both groups. Although this poor group of two saturates, as they were, a lot of them were having their events.

270
00:30:36,090 --> 00:30:41,690
So you had it was roughly a 20% chance of being event free.

271
00:30:41,740 --> 00:30:45,250
The last time they saw someone at risk in this in this group.

272
00:30:47,910 --> 00:30:49,260
So we have to deal with that.

273
00:30:49,270 --> 00:30:57,270
We don't have event times past the end of follow up, and so we need to have methods that will let us deal with that problem.

274
00:30:57,570 --> 00:31:12,100
It's a big problem. Okay.

275
00:31:12,100 --> 00:31:16,749
So here's where I have a perk up moment for Jergens.

276
00:31:16,750 --> 00:31:23,560
So individuals who are observed throughout the study period without experiencing the event are said to be censored.

277
00:31:25,740 --> 00:31:32,250
So the last time we saw them, they were alive without the event having had it happen to them.

278
00:31:32,790 --> 00:31:37,340
And so we have zero knowledge past that point.

279
00:31:37,350 --> 00:31:44,700
So their information is censored. You can kind of think about it as, you know, just information is not available to us past that last follow up time.

280
00:31:45,300 --> 00:31:52,200
So it's partially missing that. It's not completely missing because we know they were event free all the way through the part.

281
00:31:52,200 --> 00:31:58,890
We follow them so we know something about their event time that it was bigger than at least the period we watched them,

282
00:32:00,120 --> 00:32:04,170
but we don't know exactly when it would have happened in the future. So it's partially missing.

283
00:32:05,190 --> 00:32:08,849
And so censored or censoring, that's can be jargon I use a lot.

284
00:32:08,850 --> 00:32:13,920
It's like the one of the biggest pieces of of jargon that this field has.

285
00:32:14,640 --> 00:32:18,780
And in practice, the fraction of event times that are censored can be quite substantial.

286
00:32:19,230 --> 00:32:25,920
So, you know, they even 90% of the people in your dataset might be censored.

287
00:32:25,920 --> 00:32:33,210
And in fact, I once worked with the data set four times, had severe vision loss in diabetic retinopathy patients,

288
00:32:33,570 --> 00:32:40,469
and after nine years, it's still about 95% had not experienced severe vision loss.

289
00:32:40,470 --> 00:32:46,230
So 95% were censored for their time to severe vision loss.

290
00:32:50,090 --> 00:32:57,200
So survival analysis estimation methods are built around extracting information from all the subjects, censored or not.

291
00:32:57,500 --> 00:33:02,299
So that we're using whatever partial information we have on on the people who were

292
00:33:02,300 --> 00:33:06,530
censored about how long they remained event free at the time we lost track of them.

293
00:33:09,920 --> 00:33:18,140
So here's just a little bit of a picture of what we're seeing from the point of view of a patient who enters the study here.

294
00:33:18,920 --> 00:33:24,760
So an event time is right, censored at time. Little T if it's only known to be greater than T.

295
00:33:24,770 --> 00:33:29,420
So suppose a person enters here. This is the first time we start following them.

296
00:33:29,780 --> 00:33:38,239
And so we think of this like time little t equals zero and the end of the study happens some sometime here.

297
00:33:38,240 --> 00:33:44,150
This is when we can't follow them anymore because of administrative reasons that we we don't have any more funding to watch people.

298
00:33:44,930 --> 00:33:49,370
And then sometime in the future we don't see any of this information.

299
00:33:49,370 --> 00:33:56,690
But just for the purposes of understanding, I suppose the patient would have had the event out here if we could have kept watching them.

300
00:33:57,890 --> 00:34:06,360
So. The observed time on study is this amount of time and we know they were event free this long.

301
00:34:09,350 --> 00:34:15,500
And this is the length of time we wanted to measure and we've been calling that t i for person.

302
00:34:16,790 --> 00:34:23,630
So even though we can't see it, we want to to know t i when they would have had their event.

303
00:34:26,920 --> 00:34:35,110
And so we need some more notation. We don't have a piece of notation yet that covers this observed time on study.

304
00:34:36,010 --> 00:34:39,760
So we can't just rely on tie. We don't necessarily see tie.

305
00:34:39,970 --> 00:34:43,930
We need more notation to kind of reflect, you know, what we did see.

306
00:34:44,620 --> 00:34:51,609
So that's coming now. So I'm going to call a little I individual.

307
00:34:51,610 --> 00:34:55,180
It's going to be a subscript most of the time on a random variables.

308
00:34:55,840 --> 00:35:01,510
T is the the time on study the time on follow up that we're talking about.

309
00:35:01,510 --> 00:35:07,629
So what we've been using ten like what's the probability that you would have the event free at time?

310
00:35:07,630 --> 00:35:10,660
Ten That's been our context for little T. It's not random.

311
00:35:10,660 --> 00:35:21,130
It's just a time that we're interested in. And we've also kind of had this notation Capital T for the time of the event for Individual I,

312
00:35:21,940 --> 00:35:27,999
and we've sort of seen, based on the last graphic that we don't always observe why we have partial information.

313
00:35:28,000 --> 00:35:36,550
So I'm going to introduce a couple of pieces notation to deal with that in the handout, the first of which is the CI, the censoring time.

314
00:35:37,450 --> 00:35:47,440
And so this is how you know, this is the potential the potential follow up time that we would have watched them.

315
00:35:49,180 --> 00:35:53,680
But we only get to see either T.I. or C.I.

316
00:35:54,430 --> 00:36:02,620
So if the event if the person has the event at time. T.I., we're not really certain how long we would have watched them if they'd stayed alive.

317
00:36:02,650 --> 00:36:05,290
So C.I., we just know C.I. is greater than T.I.

318
00:36:06,550 --> 00:36:14,890
And if they were censored, we really don't know how much longer we would have had to wait to see T.I. We just know that they lived past CII.

319
00:36:15,460 --> 00:36:20,740
So the observed data that we have, we're going to call XY.

320
00:36:22,140 --> 00:36:31,860
And it's the minimum of these two random variables, the time we're interested in and the censoring time and so excise the observed time.

321
00:36:33,490 --> 00:36:41,650
And we need a second piece of notation to tell us whether XY reflects an event time or a censoring time.

322
00:36:42,280 --> 00:36:46,510
So Delta Eye is the indicator that tis less than C.I.

323
00:36:46,540 --> 00:36:51,340
So Delta equals one means this inside was true.

324
00:36:51,730 --> 00:36:55,150
And we saw the event before they were censored.

325
00:36:56,480 --> 00:37:00,740
And Delta equals zero is going to mean this inside is false.

326
00:37:01,160 --> 00:37:09,470
And so the C.I. was less than T.I. We observed a censored event for the event we really wanted to watch.

327
00:37:12,150 --> 00:37:19,170
And so let's take a moment to kind of to perk up and recognize that you in this scenario,

328
00:37:19,170 --> 00:37:25,830
for the first time, a single outcome is described with two random variables for your data set.

329
00:37:26,910 --> 00:37:33,120
So both of these random variables are going to be in your model statement describing your outcome.

330
00:37:35,460 --> 00:37:37,110
Eventually we're going to have covariates.

331
00:37:37,110 --> 00:37:42,540
I just sort of had a picture with a cover that was binary about their saturation status in that earlier graphic,

332
00:37:42,990 --> 00:37:46,500
but we're going to be building in covariates pretty soon.

333
00:37:47,620 --> 00:37:54,600
Not in this handout, but in the next couple. And then there's a key assumption that we need.

334
00:37:55,500 --> 00:37:58,560
This is an assumption that I use throughout this course,

335
00:37:59,250 --> 00:38:07,799
that conditional on the covariates you're using in your modeling or your tests, we need the true event.

336
00:38:07,800 --> 00:38:13,500
Time to be independent, statistically independent of the true censoring time.

337
00:38:13,830 --> 00:38:25,800
And so I need to teach you intuition for that assumption, but it's quite important because, you know, all of the methods that I'm focusing on.

338
00:38:27,090 --> 00:38:33,120
In this course really require this to be true or your your analysis is incorrect.

339
00:38:38,680 --> 00:38:44,469
All right. So I'm going to give you a little bit more time to get used to this notation through examples.

340
00:38:44,470 --> 00:38:48,250
So here's individual ones notation examples.

341
00:38:48,250 --> 00:38:55,600
So they start they enter the study here that's like their that person's time, little T equals zero for that person.

342
00:38:56,170 --> 00:38:59,710
And let's suppose they have the event at two years.

343
00:39:01,410 --> 00:39:05,319
And. You know, I kind of have this dot, dot, dot.

344
00:39:05,320 --> 00:39:11,470
This is part the part that we never, never see because we don't know if they would have had some drop out or withdrawal or anything else.

345
00:39:12,040 --> 00:39:13,179
But for this particular person,

346
00:39:13,180 --> 00:39:20,050
I'm making up this idea that they would have been available for follow up all the way through the end of the study at five years.

347
00:39:20,140 --> 00:39:25,450
So I've made up as an example that if we could have watched them,

348
00:39:25,450 --> 00:39:29,530
if they hadn't actually died, we would have been able to watch them the entire five years.

349
00:39:30,940 --> 00:39:35,589
So my event time random variable is t one for this person.

350
00:39:35,590 --> 00:39:38,649
Individual one. It happened at two years.

351
00:39:38,650 --> 00:39:45,010
T one equals two. And for this situation, I've kind of told you what C one is.

352
00:39:45,010 --> 00:39:48,070
That is C one is equal to five. That's a censoring random variable.

353
00:39:48,790 --> 00:39:53,080
But in real life, we only get to see whichever one happens first.

354
00:39:54,130 --> 00:40:00,250
So the observed time on study. We now have notation for R and.

355
00:40:01,260 --> 00:40:06,630
We say X one equals two is two, x one is the minimum of T one and C one.

356
00:40:06,990 --> 00:40:10,820
So the minimum of that is two. And we observed the events.

357
00:40:10,830 --> 00:40:20,770
The delta is equal to one. So the Delta was the indicator that the the tie was less than CII, and that's certainly true.

358
00:40:20,800 --> 00:40:33,510
So this Delta is a one here. And so to give you practice thinking about this, because I know, you know, it's already probably a lot.

359
00:40:34,170 --> 00:40:38,610
I want you to help me translate this notation and what it means in words.

360
00:40:39,630 --> 00:40:46,440
So the indicator that X one is greater than four is equal to zero.

361
00:40:47,160 --> 00:40:51,570
What, in words, you need to help me out here. Now, this is like participation time.

362
00:40:51,870 --> 00:41:00,420
What does that mean in words? Yes.

363
00:41:06,860 --> 00:41:12,379
Okay. So the answer from the crowd was it's the indicator.

364
00:41:12,380 --> 00:41:15,620
The observation time was not greater than four.

365
00:41:16,710 --> 00:41:19,980
And for that individual.

366
00:41:20,670 --> 00:41:31,890
That is correct. Yes. And I kind of want to say it in a bunch of different ways because, you know.

367
00:41:32,430 --> 00:41:39,900
But that's correct. The observation time that we were, you know, watching them was not greater than four.

368
00:41:40,950 --> 00:41:48,170
That's what this zero is. So what are other ways to translate this?

369
00:41:48,200 --> 00:41:51,320
So X1 also is kind of.

370
00:41:53,230 --> 00:41:57,920
The observed time on study of the observed time on study greater than four is equal to zero.

371
00:41:57,940 --> 00:42:00,940
That means that they were not event free at time for.

372
00:42:00,970 --> 00:42:10,330
That's another way to describe it. And I think that's all we can say.

373
00:42:10,360 --> 00:42:20,430
Right. Because we don't know. We don't have anything in this question that talks about whether they were a death or centered person.

374
00:42:21,250 --> 00:42:27,520
We just know that their follow up time was not greater than four and they were not event free.

375
00:42:28,960 --> 00:42:34,240
Meaning, you know, that they had something happen, either a tea or a C at time for.

376
00:42:38,280 --> 00:42:43,889
Okay. Let's do another one. So this time we have indicator x one greater than one is equal to one.

377
00:42:43,890 --> 00:42:52,760
So what does this mean in words? Yes.

378
00:43:00,570 --> 00:43:05,280
So the response was that the event we're interested happened after one.

379
00:43:06,240 --> 00:43:09,660
And we know that.

380
00:43:10,850 --> 00:43:14,329
Either T.I. or C.I. was greater than one.

381
00:43:14,330 --> 00:43:27,670
We don't know if it was the event or not. We just know that. You know, at the time of one, they hadn't either been censored or had their event.

382
00:43:28,760 --> 00:43:32,300
So strictly speaking, x one is the minimum of ty and.

383
00:43:32,780 --> 00:43:38,479
So we can just look at the T that the ty and the CY and say, well, they're both greater than one.

384
00:43:38,480 --> 00:43:47,540
So that's a real easy technical check. But basically at time one, they are still at risk for either Ty or C.I. We just don't know which one.

385
00:43:53,340 --> 00:43:57,030
All right. So here's another notation example for this person here.

386
00:43:57,030 --> 00:44:04,810
They enter this study, and then five years after they enter the study, they have still not had the event.

387
00:44:04,830 --> 00:44:08,490
So they're censoring same time. C two is five.

388
00:44:08,880 --> 00:44:11,160
They're still event free as of five years.

389
00:44:11,400 --> 00:44:19,170
If we could have watched them, I've kind of made up this two equals seven, so we don't actually get to see any of this dash time.

390
00:44:19,800 --> 00:44:23,640
But just for the purposes of this example, I'm saying t two is seven.

391
00:44:24,600 --> 00:44:32,940
So here for this person, this was their observed time on study and we captured that data with two random variables.

392
00:44:32,940 --> 00:44:45,659
The first is x two is equal to five, so the minimum of C two in T two is five and at five when we start following them, they were still alive.

393
00:44:45,660 --> 00:44:49,710
So Delta two equals zero is saying they were a censored person at time five.

394
00:44:56,860 --> 00:45:05,890
Okay. And so when you get an actual data set and I don't have any covariates here yet, I just have the outcomes when you get an actual data set.

395
00:45:06,310 --> 00:45:09,420
This is the form of the data that you'll have.

396
00:45:09,430 --> 00:45:14,960
You'll have the individual. I may be with the subject idea from the study there in.

397
00:45:15,910 --> 00:45:20,740
You'll have their EXI, which is basically the last time you saw them.

398
00:45:23,450 --> 00:45:27,470
And you'll have their status. The last time you saw them.

399
00:45:28,010 --> 00:45:34,490
So this delta, I sometimes people call it their status alive or not at the last time you saw them.

400
00:45:34,880 --> 00:45:40,420
So person to person one sorry, died at two.

401
00:45:40,430 --> 00:45:47,080
So they get a delta equals one. Person two was censored at five.

402
00:45:47,090 --> 00:45:53,630
So there Delta is zero and you kind of just go through and you can really quickly see based on Delta, where were the observed deaths?

403
00:45:53,780 --> 00:45:58,580
They all have ones. And where were the people that were alive the last time you saw them?

404
00:45:59,480 --> 00:46:09,290
And those are the zeros. And you get to observe T.I. or C.I., but aren't those.

405
00:46:09,510 --> 00:46:17,870
So this is the extent of the data you have on these people. Good.

406
00:46:20,160 --> 00:46:27,540
All right. So I'm going to use a trick that textbooks use to summarize data sets in a smaller space.

407
00:46:27,540 --> 00:46:32,430
So textbooks, textbooks sometimes summarize short data sets like this.

408
00:46:33,210 --> 00:46:40,170
So this is the same data set as you would, you know, that I have represented here with the data you would use with your software.

409
00:46:41,830 --> 00:46:47,050
The first row is the individual eye and the second row is their EXI.

410
00:46:47,710 --> 00:46:53,920
And so what has happened here with the Delta is the Delta II has been changed to plus versus no plus.

411
00:46:54,370 --> 00:46:58,329
So they have a plus that is like a delta equals zero.

412
00:46:58,330 --> 00:47:02,770
So they live to five plus, you know, time units.

413
00:47:04,150 --> 00:47:07,210
Whereas if they don't have a plus, that was their event time.

414
00:47:08,290 --> 00:47:12,940
So this is just easier to show in a small form.

415
00:47:13,960 --> 00:47:17,110
Um, software packages don't do.

416
00:47:17,110 --> 00:47:25,900
Plus they only use the delta and it's going to be convenient as I'm showing you kind of the bookkeeping that goes on with these data sets.

417
00:47:27,250 --> 00:47:34,710
So here's the same data set where I've got the pluses for the censored people, and I want to kind of build intuition about the sensor data.

418
00:47:34,720 --> 00:47:42,610
So you have this data set and you know, your linear regression, you know your P hats,

419
00:47:42,610 --> 00:47:47,670
you know, so can you squeeze those methods and impose them on this data set?

420
00:47:47,680 --> 00:47:56,380
That's the kind of intuition I'm trying to build here. So can we just exclude the observations that are censored?

421
00:47:56,500 --> 00:48:02,440
You know, because we don't know. There have been times. Can we just exclude them and then use standard analyzes without introducing bias?

422
00:48:06,860 --> 00:48:11,000
And the answer to all these little test questions is going to be no, that's a horrible idea.

423
00:48:13,370 --> 00:48:17,810
So you know why? Why? So why is this a horrible idea?

424
00:48:18,560 --> 00:48:27,410
All right. So often the censored individuals are the ones who are the most likely to live beyond the follow up window.

425
00:48:27,440 --> 00:48:32,120
They're the people who are doing the best at avoiding these events.

426
00:48:32,780 --> 00:48:37,400
So excluding those would underestimate survival probabilities.

427
00:48:38,320 --> 00:48:44,590
And so, for instance, if you took away if you just dropped all these people who were alive when they were censored,

428
00:48:45,070 --> 00:48:47,260
you'd have all observed death at times.

429
00:48:47,260 --> 00:48:55,540
Your curve would go down to zero at at 34, and you wouldn't have any allowance for the fact that this person might have lived past 34.

430
00:48:57,100 --> 00:49:00,670
You know, so you you can't do that.

431
00:49:00,680 --> 00:49:11,020
You will absolutely be showing nonsense curves where people will look like they all died with the survival probability at zero at the last event time.

432
00:49:11,020 --> 00:49:13,750
If you drop out these people who were your survivors?

433
00:49:19,660 --> 00:49:28,180
And so let's just look at person four and we've been talking about little T of ten, like what's the problem where they live past ten years?

434
00:49:28,540 --> 00:49:33,180
So let's look at person four. You know, person four did live past ten.

435
00:49:33,400 --> 00:49:37,150
They were censored at 12. So what if you drop them?

436
00:49:37,150 --> 00:49:42,820
You're tossing out information that's actually pretty solid about survival at ten years.

437
00:49:43,000 --> 00:49:47,200
We know that this person did survive that long and you'd be, you know,

438
00:49:48,010 --> 00:49:53,320
tossing out not just this person, but two other people that we know lived past ten.

439
00:49:54,190 --> 00:49:57,729
So do we really believe that Individual four contains no information on survival?

440
00:49:57,730 --> 00:50:04,900
Probability at times equals ten? No. All these censored people but one there solid information.

441
00:50:04,900 --> 00:50:09,970
They live beyond ten years and if you drop them, you're really going to be getting the incorrect analysis.

442
00:50:11,580 --> 00:50:14,670
You're dropping people who are more likely to have lived a long time.

443
00:50:18,320 --> 00:50:23,550
All right. So what's another thing that we might be tempted to do but is a terrible idea?

444
00:50:23,570 --> 00:50:27,110
So this question, again, it's going to be a big no. We can't do it.

445
00:50:27,410 --> 00:50:30,890
But why? We're trying to build that muscle for intuition.

446
00:50:30,900 --> 00:50:38,660
So can we assume that censored patients would live beyond the time t we're interested in and use standard methods?

447
00:50:38,900 --> 00:50:46,370
So can I just assume that say I'm still interested in time little t equals ten?

448
00:50:46,850 --> 00:50:55,520
Can I just assume that this person who was sensitive at five would have lived all the way to ten and use my p hats and and that kind of stuff.

449
00:50:56,720 --> 00:51:00,620
So the answer, of course, is no. But we want to stretch that intuition of why is that a no?

450
00:51:01,970 --> 00:51:10,430
So if you do that, it would overestimate survival for those centered individuals who could potentially die before time ten.

451
00:51:11,820 --> 00:51:20,309
Oops. Sorry. So for that little if you're looking at time ten and you assume that this person would have lived five more years,

452
00:51:20,310 --> 00:51:26,310
well, that's really kind of optimistic about that person. Anything could have happened to them over a five year period, including an event.

453
00:51:26,880 --> 00:51:33,750
So you would be overestimating survival. If you at time ten, if you just assume that this person,

454
00:51:33,750 --> 00:51:38,370
you gave him a free pass and said that they would have lived five more years if you just watched them.

455
00:51:39,490 --> 00:51:45,490
So you can't do that either. So so all of your known methods are a fail.

456
00:51:46,300 --> 00:51:55,210
So we need to come up with new methods. So I'm going to kind of teach you the mechanics of how this is done correctly.

457
00:51:56,140 --> 00:52:01,330
I want you to keep in mind this whole time you will be using software for this.

458
00:52:01,930 --> 00:52:05,320
I'm going to be showing you lots of formulas and trying to build your intuition.

459
00:52:05,770 --> 00:52:10,060
I am. It is not my intent that you become formula person.

460
00:52:10,660 --> 00:52:18,610
You know, I am using this particularly for the third of students typically that learn by doing stuff

461
00:52:18,610 --> 00:52:23,620
by hand and understanding all the ins and outs of what the computer package is doing.

462
00:52:24,430 --> 00:52:27,540
So I don't want you to get too bogged down by the formulas.

463
00:52:27,550 --> 00:52:34,150
Use them to learn intuition about what the packages are going to produce and how you can get into trouble with bias.

464
00:52:34,750 --> 00:52:46,520
That's my intent here. All right. So with that in mind, I want to show you how to estimate probabilities of surviving over time,

465
00:52:46,520 --> 00:52:52,760
just like in that first plot without stumbling into these errors we mentioned in the previous two slides.

466
00:52:53,270 --> 00:53:01,130
And so the approach is the Kaplan-Meier method, and it builds the correct survival estimate by a couple of steps here.

467
00:53:01,160 --> 00:53:09,200
So it breaks up the time interval that we have here in the data into non-overlapping intervals that don't have censoring within.

468
00:53:09,920 --> 00:53:16,180
So for instance, up until this first event time, there's no censoring that happened in the data set.

469
00:53:16,190 --> 00:53:22,520
So we have an interval between zero and two where we don't have any censoring and then between two and five.

470
00:53:23,910 --> 00:53:30,870
We don't have any censoring. There's a censored value of five, but between times two and five, we're still cool.

471
00:53:31,110 --> 00:53:35,460
And then just after five. Between five and eight, we don't have any censoring.

472
00:53:35,880 --> 00:53:45,630
So you sort of we're kind of looking at the intervals based on the observed data cutting, slicing up the time access.

473
00:53:46,780 --> 00:53:47,130
Right.

474
00:53:47,140 --> 00:53:54,160
And each one of these we have information about what happens in this little interval that doesn't actually have a censored person in that interval.

475
00:53:55,530 --> 00:53:57,780
Is that step one and then step two.

476
00:53:59,350 --> 00:54:06,670
Is looking at conditional probabilities of surviving to the end of a sub interval given survival at the beginning of that interval.

477
00:54:06,940 --> 00:54:11,770
And don't worry that I know that's a lot of jargon. We're going to look at an example of what I mean by that.

478
00:54:12,790 --> 00:54:16,570
So this is just a summary of step two. We'll see. And it's in a little more detail in a minute.

479
00:54:18,410 --> 00:54:26,959
And we're going to see something that when we have an interval like this interval between

480
00:54:26,960 --> 00:54:30,740
five and eight where there's a censored person at the beginning of the interval,

481
00:54:31,160 --> 00:54:38,540
we're going to exclude the censored individual from probabilities we calculate in that interval,

482
00:54:39,380 --> 00:54:41,840
since they contribute no information for that interval.

483
00:54:41,850 --> 00:54:49,969
So even though there's a person who was censored at time five that person because we don't witness even a minute pass time.

484
00:54:49,970 --> 00:54:57,680
Five That doesn't help us understand the probability of having an event in this interval, given they made it that far.

485
00:55:01,190 --> 00:55:09,230
And so this trick, these steps are going to allow us to talk about conditional probabilities for each interval.

486
00:55:09,560 --> 00:55:10,250
Like, what's the problem?

487
00:55:10,280 --> 00:55:16,210
If something bad happens in an interval, given they made it to the beginning, that's going to be based on fully observed data.

488
00:55:16,230 --> 00:55:20,780
We'll be able to avoid all the sensor observations by focusing on these tiny

489
00:55:20,780 --> 00:55:24,740
intervals and then putting those pieces of statistical information together.

490
00:55:25,850 --> 00:55:31,280
And so the last step is going to be multiplying appropriate conditional probabilities to estimate the survival function.

491
00:55:32,950 --> 00:55:40,900
And this is a good time to take a break so that you can perk up for the notational part.

492
00:55:41,590 --> 00:55:45,790
All right. So let's meet at 907 again and get back to work.

493
00:56:11,440 --> 00:56:23,480
Get. Yeah.

494
00:56:24,200 --> 00:56:28,050
Yeah. So this.

495
00:56:33,830 --> 00:57:32,780
It would. So.

496
00:57:37,670 --> 00:57:59,670
I don't know why. You know.

497
00:58:14,760 --> 00:58:41,520
I. So you are given a sensory.

498
00:58:46,190 --> 00:58:54,730
We're just noticing. You know, we're asking about is the name of those creating or is maybe sort of spreading.

499
00:58:57,840 --> 00:59:03,430
So this first. The first thing being zero.

500
00:59:04,570 --> 00:59:11,300
I had either an event or three values. There's nothing about this experiment.

501
00:59:13,660 --> 00:59:18,020
So I just know that there are many. I see your observation.

502
00:59:21,200 --> 00:59:25,850
It's not. So this is not. Complete information.

503
00:59:27,150 --> 00:59:41,950
Research shows that the formula is. You know, it seems to me that this person I know to.

504
00:59:50,100 --> 00:59:58,500
So it means that the inside. So it's a little bit more.

505
01:00:00,880 --> 01:00:06,560
When my mother. For.

506
01:00:07,250 --> 01:00:27,200
They have their I. Well, it means that.

507
01:00:29,160 --> 01:00:32,330
Because both of them right to excellent is the minimum.

508
01:00:40,650 --> 01:00:43,690
We don't know. You see, it's great.

509
01:00:45,600 --> 01:00:49,020
You just know that. This person has not.

510
01:00:49,780 --> 01:00:59,540
Have there. Okay.

511
01:01:55,520 --> 01:03:28,730
Yeah. Right next to you.

512
01:03:28,910 --> 01:03:38,880
If, like. You know, I don't know.

513
01:04:31,860 --> 01:04:50,300
I read. Progress. Yeah.

514
01:04:53,930 --> 01:04:57,700
Here comes another example for you.

515
01:05:05,670 --> 01:05:09,750
I was thinking you. On the cover.

516
01:05:38,430 --> 01:05:52,490
Yeah. You're driving me. But it's really quite.

517
01:06:04,860 --> 01:06:10,980
Okay. Are you guys are you guys feeling like you can get going again?

518
01:06:11,430 --> 01:06:15,600
All right, let's get back to work, everybody. So.

519
01:06:19,200 --> 01:06:24,659
So here is an example. Now we kind of talked about the steps that the Kaplan-Meier approach uses.

520
01:06:24,660 --> 01:06:30,240
And here's an example now of kind of the the note keeping in the mechanics that's happening with this estimation.

521
01:06:31,020 --> 01:06:36,329
So we've already sort of seen tables that started this process.

522
01:06:36,330 --> 01:06:41,430
So the top row is telling you which individual in the dataset we're talking about.

523
01:06:41,940 --> 01:06:51,480
The next row is their XY value. And I'm using that kind of shorthand where if there's a plus, then it's a censored person with x equals zero.

524
01:06:51,660 --> 01:06:56,190
Sorry, delta equals zero. And if there's no plus there, delta is equal to one.

525
01:06:56,910 --> 01:07:00,540
All right. So that's our dataset, kind of in short.

526
01:07:00,900 --> 01:07:07,139
And now I've got all of these indicator functions that are going to help us with the bookkeeping of the Kaplan-Meier estimate.

527
01:07:07,140 --> 01:07:11,680
So this first one. It's very similar to what we saw earlier.

528
01:07:12,070 --> 01:07:16,510
It's just an indicator function with tie greater than two.

529
01:07:16,840 --> 01:07:22,710
And so remember indicator functions. If the inside is true, it becomes a one.

530
01:07:22,720 --> 01:07:30,850
If the inside is false. It's a zero. So the indicator that times greater than two is going to be a zero for this person.

531
01:07:30,970 --> 01:07:33,970
You know, person one because they actually had their event at two.

532
01:07:34,000 --> 01:07:38,530
So t one was not greater than two, t one was equal to two.

533
01:07:38,560 --> 01:07:42,310
So they get a zero there. But everybody else in the dataset.

534
01:07:43,150 --> 01:07:46,570
Their ties had to have been greater than two.

535
01:07:46,600 --> 01:07:56,920
So even though we don't see the tie, we know that the minimum of tie and psi was greater than five, so tie had to be greater than two.

536
01:07:57,370 --> 01:08:02,810
So everybody, whether they had their event time or was censored, all of those times happened after two.

537
01:08:02,860 --> 01:08:07,210
So they all get once the event time was greater than two for all of those people.

538
01:08:09,480 --> 01:08:13,670
Right. And then this is known completely right for everybody in that row.

539
01:08:13,680 --> 01:08:18,930
We know the data completely. So what is this next row doing?

540
01:08:19,320 --> 01:08:25,290
So you remember from your intro classes, when you use that vertical pipe, it's like a conditional thing, right?

541
01:08:25,320 --> 01:08:34,200
So I'm using the pipe in the same way. So the inside of this indicator function is saying given tis greater than two.

542
01:08:35,510 --> 01:08:40,900
So we're in the next interval. Between two and five.

543
01:08:40,930 --> 01:08:46,900
Given Thai is greater than two, is T.I. greater than five?

544
01:08:47,980 --> 01:08:53,380
So the conditional part means that this first person doesn't contribute to the conditional probability.

545
01:08:53,680 --> 01:09:01,200
They didn't have T.I. greater than two than zero here. So they don't they are not involved in this in this calculation.

546
01:09:01,210 --> 01:09:06,570
They didn't make this conditioning criteria. But for everybody else, I can say what happened.

547
01:09:06,580 --> 01:09:10,750
So every single other T.I. had to have been greater than five.

548
01:09:10,750 --> 01:09:18,610
This is the closest thing. And this the fact that this x I was five means that.

549
01:09:19,900 --> 01:09:25,330
And that we know that they were censored means they at least were event free as of five.

550
01:09:26,290 --> 01:09:31,659
So that T.I. was also greater than five. The minimum of T.I. and C.I. was greater than five.

551
01:09:31,660 --> 01:09:37,670
So T.I. had to be greater than five as well. So you're going to this is going to be a common pattern.

552
01:09:37,670 --> 01:09:47,870
You see, if there's a censoring time here and we're looking at everybody who's at risk for a row, it's going to end up being a row of all ones.

553
01:09:49,580 --> 01:09:55,610
So let's look at this, this last row just to kind of affirm at that point, oh, I've just done a censoring time.

554
01:09:55,640 --> 01:10:00,230
Never mind. Let's go to row three and figure out row three. So row three is given.

555
01:10:00,560 --> 01:10:04,639
T was greater than five. Yes.

556
01:10:04,640 --> 01:10:08,360
No. Were they also there was their tie greater than eight.

557
01:10:09,560 --> 01:10:14,750
And so this is the one rule that's a bit tricky. So tie greater than five.

558
01:10:15,200 --> 01:10:19,400
You know, all of these people had tie greater than five, including person two.

559
01:10:20,980 --> 01:10:30,230
But in the event that we're trying to talk about. Whether they were also greater than eight this person to contribute zero information about

560
01:10:30,230 --> 01:10:36,770
that we stopped watching them exactly at five and so we have no information from that person.

561
01:10:36,770 --> 01:10:40,820
On whether Ty was greater than eight zero follow up time contributed.

562
01:10:41,450 --> 01:10:51,110
So we just put a dash here. Even though they were greater than five, we put a dash here because they don't help us with this with this information.

563
01:10:52,570 --> 01:10:56,890
And so they get a dash and then we look at the next person aid.

564
01:10:57,220 --> 01:11:05,180
Well. What did they have of an event time greater than a given that they were greater than five?

565
01:11:05,480 --> 01:11:10,100
Well, this person had the event at exactly eight, so their tie was equal to eight, not greater than eight.

566
01:11:10,100 --> 01:11:16,970
So they could have zero, but all the rest were we had X greater than eight.

567
01:11:17,210 --> 01:11:20,560
We get one. Okay.

568
01:11:20,650 --> 01:11:25,450
And then in this last row, I have our ten here. That's the little tee that we've been looking at.

569
01:11:25,450 --> 01:11:30,459
There's no one in the dataset who had an event or, you know, a tie or ECI equal to ten.

570
01:11:30,460 --> 01:11:34,420
But I just wanted in here to show you what that bookkeeping looks like.

571
01:11:35,720 --> 01:11:40,940
So ten is somewhere between eight and 12. Right. So given tis greater than eight.

572
01:11:41,960 --> 01:11:47,000
That's going to be looking at all of these people. And these don't contribute.

573
01:11:47,690 --> 01:11:50,960
They didn't make the conditioning event. What is.

574
01:11:51,470 --> 01:11:58,760
You know. Is it true? Yes. No, that tie is greater than ten. And every single person who's left who contributes is a one.

575
01:12:00,920 --> 01:12:04,820
And so by breaking it up into these intervals, we have an answer.

576
01:12:04,970 --> 01:12:08,930
What yes or no about all these indicator functions?

577
01:12:09,620 --> 01:12:13,330
It's based on complete information within each of these intervals.

578
01:12:13,340 --> 01:12:15,950
And so this is the bookkeeping.

579
01:12:15,950 --> 01:12:25,280
And now if we want to estimate the probability that someone is event free at time ten so the probability of their tie is greater than ten,

580
01:12:26,330 --> 01:12:31,580
we can use each of these rows of information and conditional probability to get to our answer.

581
01:12:32,390 --> 01:12:42,170
So this here may be a stretch because it's been a long time since you had your first probability class.

582
01:12:42,710 --> 01:12:47,660
But you had this rule that you learn in your first probability class that the

583
01:12:47,660 --> 01:12:57,040
probability of A and B was the probability of A given B times the probability of B.

584
01:12:57,260 --> 01:13:06,150
That's how we learned conditional probability in your first class. And so the probability that someone survives greater than ten.

585
01:13:07,440 --> 01:13:16,290
We can make that a whole union of events actually intersection of events that they lived greater than

586
01:13:16,290 --> 01:13:21,150
ten and they lived greater than two and they lived greater than five and they lived greater than eight.

587
01:13:21,300 --> 01:13:25,380
Right. Living greater than ten contains all those other events.

588
01:13:25,920 --> 01:13:29,150
And so this. Condition.

589
01:13:29,190 --> 01:13:35,580
The string of conditional probabilities is basically using that rule from your entire probability class over and over and over again.

590
01:13:37,470 --> 01:13:44,459
So the probability that T is greater than ten is the probability is greater than ten.

591
01:13:44,460 --> 01:13:48,480
Given T is greater than eight times the probability is greater than eight.

592
01:13:49,470 --> 01:13:54,510
And then all the rest of this stuff is what happened with the probabilities greater than eight.

593
01:13:55,140 --> 01:13:58,980
It got kind of recursively used over and over again. So.

594
01:14:01,390 --> 01:14:06,920
When you are estimate each of the estimating each of these probabilities, you're using one of these rows.

595
01:14:06,940 --> 01:14:10,330
So when we're looking at the probabilities greater than two,

596
01:14:10,780 --> 01:14:16,810
we're basically taking this first row of data and saying in our dataset, what's the probabilities greater than two?

597
01:14:17,170 --> 01:14:24,040
And we have complete information on all ten people for that. And that's going to be nine out of ten of them that tie greater than two.

598
01:14:26,400 --> 01:14:30,870
The sum of those nine over the some of all of them. Being ten.

599
01:14:31,380 --> 01:14:34,709
And then for the next one, the probability is greater than five.

600
01:14:34,710 --> 01:14:39,000
Giving two is greater than two. We can calculate that from the second row.

601
01:14:40,630 --> 01:14:48,270
And so that probability. Is 100% because everybody look, everybody had a one here.

602
01:14:50,660 --> 01:14:57,380
And so out of the nine people who gave us information about that interview, all nine of them had the tie greater than five.

603
01:14:59,170 --> 01:15:04,480
And then the this third one, this probability is greater than eight given Thai is greater than five.

604
01:15:05,020 --> 01:15:07,480
All the information is coming from this third row.

605
01:15:07,750 --> 01:15:20,620
And so for everybody, we had eight people who contributed information about that time interval, and of them, seven lived longer than eight.

606
01:15:22,260 --> 01:15:30,270
So that became seven over eight. And then for this last row, the probability is greater than ten given t is greater than eight.

607
01:15:30,870 --> 01:15:35,009
Every single person who contributed information to that interval survives.

608
01:15:35,010 --> 01:15:41,430
That was seven over seven. That had the tie greater than ten among those that had tie greater than eight.

609
01:15:42,120 --> 01:15:45,430
And so we didn't make any weirdo assumptions about censored values.

610
01:15:45,450 --> 01:15:52,680
Each one of these fractions is estimated in a sensible way, assuming nothing about censored people die or living or anything weird.

611
01:15:53,190 --> 01:16:01,140
And so this number in the end is going to be the estimate, the kaplan-meier estimate for the probability they live longer than ten.

612
01:16:02,550 --> 01:16:08,480
In the data set. Okay.

613
01:16:10,660 --> 01:16:15,010
And so it's always going to look something like this behind the scenes,

614
01:16:15,010 --> 01:16:20,620
when you're getting your kaplan-meier estimates, it's always going to be, you know, in each of these time intervals.

615
01:16:20,770 --> 01:16:26,140
So the time intervals came from these event times themselves, you know,

616
01:16:26,290 --> 01:16:30,670
breaking it up to look at what happens up to two and then between two and five and then between five and eight,

617
01:16:30,990 --> 01:16:35,889
eight and 12 and so on that for whatever data set they're using,

618
01:16:35,890 --> 01:16:42,820
those same kind of intervals carved up by these event times and then looking

619
01:16:42,820 --> 01:16:47,889
at these conditional probabilities of making it to the end of one interval,

620
01:16:47,890 --> 01:16:53,920
given you made it to the start of that interval and multiplying all those together.

621
01:16:57,560 --> 01:17:04,100
So something to notice when you're looking at these intervals where no one died.

622
01:17:04,910 --> 01:17:11,300
Right. This this interval, no one died because the start of the interval was a censored value, not an event time.

623
01:17:11,960 --> 01:17:22,850
And that ended up being nine over nine. And this last interval, no one died because we were looking at given tis greater than eight,

624
01:17:23,420 --> 01:17:30,100
you know, which happens after this point in time how many people live greater than ten?

625
01:17:30,110 --> 01:17:34,490
So nobody and nobody contributed at a death to that interval either.

626
01:17:34,880 --> 01:17:37,910
So those two rival probabilities always end up being one.

627
01:17:38,270 --> 01:17:39,980
If no one died in that interval.

628
01:17:41,450 --> 01:17:50,360
And so this finally now answers that question of why when you're looking at a kaplan-meier curve, it only drops when there's an event time.

629
01:17:50,840 --> 01:17:54,320
If no one has a death in an interval, the curve won't drop.

630
01:17:54,770 --> 01:17:59,209
It only drops when there's an event time. Like in this interval.

631
01:17:59,210 --> 01:18:06,100
In this interval. So that's kind of the example for T little T equals ten.

632
01:18:06,110 --> 01:18:15,380
So what's the general formula that you'll see? So there's observed that the Kappa marker only changes when you have an observed

633
01:18:15,950 --> 01:18:19,400
like failure time in observed death if you're looking at time to death.

634
01:18:19,940 --> 01:18:24,170
And so the notation for those times when there is actually a death.

635
01:18:25,220 --> 01:18:34,160
Or whatever your event of interest is are going to be t 1 to 30 and for like number and these like for number of deaths in this handout.

636
01:18:36,080 --> 01:18:42,590
And we're ordering them. It's going to be convenient to have these in an ordered way when we're looking at the formulas.

637
01:18:43,810 --> 01:18:48,750
And there is some you know, there could have been some type death times.

638
01:18:48,760 --> 01:18:56,980
And we're just looking at the number of unique times. So if there's a tie, then that would just be captured by one of these little tees.

639
01:19:00,420 --> 01:19:05,909
So here's notation for some of the stuff that we've been doing on the previous slides a little more formally.

640
01:19:05,910 --> 01:19:13,320
So y j is the number of people who are at risk for the event at t equals t j.

641
01:19:13,890 --> 01:19:17,910
So when our little bookkeeping table, if they had either a zero or one,

642
01:19:17,910 --> 01:19:23,490
they were at risk for the event at t j so y js kind of counting the denominator of those fractions.

643
01:19:25,590 --> 01:19:29,760
And E.J., is the number of failures that happened at that time.

644
01:19:29,760 --> 01:19:39,040
Two equals T.J. And here is the formula and it's a it's a bit much so we're going to unpack it in a second.

645
01:19:39,040 --> 01:19:47,350
But here's the formula for the Kaplan-Meier estimate. For s of t the probability that t is greater than little t.

646
01:19:47,860 --> 01:19:54,550
And so this is my notation for the kaplan-meier estimate with the hat on top at little time t.

647
01:19:56,880 --> 01:20:00,450
And so what's the rest of this stuff? So let's start over here.

648
01:20:01,740 --> 01:20:09,120
So here is D.J. over YJ. So this is the number of deaths at Little T j.

649
01:20:09,120 --> 01:20:11,820
So t j is going to be one of these observed failure times.

650
01:20:12,390 --> 01:20:19,469
So that's the number of deaths that one of these failure times in our little data set, it was either a zero or a one.

651
01:20:19,470 --> 01:20:25,320
But since we're all looking at it at observed Death Times, it's always going to be in our little mini dataset.

652
01:20:25,410 --> 01:20:34,400
So it was always a one. If you're just looking at the places where there was a death and then why is the number of at risk at that same time?

653
01:20:36,190 --> 01:20:46,170
So what does this part mean? So this is maybe your first time seeing this, but this is like a product symbol.

654
01:20:46,180 --> 01:20:50,230
We've seen summation symbols where you go from I equals one to something.

655
01:20:50,590 --> 01:20:57,820
This is not a summation. It's a product. But otherwise it's working kind of in a similar way.

656
01:20:58,000 --> 01:21:04,040
You take this bottom part is saying, look at all of these observed failure times.

657
01:21:04,430 --> 01:21:11,550
Those are the t js. Where you have those times less than little tea.

658
01:21:12,730 --> 01:21:18,280
So we're thinking about little T equals ten. We're looking at all the unique.

659
01:21:19,370 --> 01:21:25,980
Absurd failure times. The T Jays that are less than ten.

660
01:21:29,090 --> 01:21:39,050
All right. And then you're multiplying one minus J over YJ for t one as long as t one's less than ten,

661
01:21:39,470 --> 01:21:45,620
and then times the next one, one minus D.J. over y j as long as that t j is less than ten.

662
01:21:46,250 --> 01:21:52,730
So you're multiplying until you get to a point where the t j is no longer less than or equal to the time t you're interested in.

663
01:21:56,330 --> 01:22:01,100
So because of this product that's involved, it's also known as a product limit estimate.

664
01:22:01,100 --> 01:22:06,469
Or you'll see some people call it kaplan-meier estimate. Some people will call it product limit estimate.

665
01:22:06,470 --> 01:22:11,210
Or I prefer Kaplan-Meier just because I like giving credit to the people who came up with stuff.

666
01:22:11,450 --> 01:22:15,020
But this is this is fair. You'll see that in literature as well.

667
01:22:17,180 --> 01:22:23,180
All right. So we're going to go back to our toy data set just so you can sort of see this and how it plays out.

668
01:22:23,690 --> 01:22:29,990
This is the kind of bookkeeping you'll actually be able to see this kind of output in SAS r.

669
01:22:31,130 --> 01:22:35,210
So when you go and you use and you want to come up with your your kaplan-meier estimate here,

670
01:22:35,480 --> 01:22:39,770
it will show you bookkeeping in this format if you ask for it.

671
01:22:40,400 --> 01:22:50,150
And so here's the same data set. And so what's happening here? So if I added in this first row with T.J. equals zero, depending on the package,

672
01:22:50,150 --> 01:22:55,549
they might just skip this row because at time zero, everybody's at risk.

673
01:22:55,550 --> 01:22:59,010
No one's died generally, and so do you.

674
01:22:59,030 --> 01:23:02,509
J Over y j is going to be zero one minus T.J. Everybody is going to be one.

675
01:23:02,510 --> 01:23:10,610
And the survival estimates one. So packages will just sort of skip this row because they assume, you know, no one died right then.

676
01:23:11,690 --> 01:23:16,490
And so I put it in just so we have it and we know what it is because we're learning.

677
01:23:17,000 --> 01:23:22,069
I'm putting it in so that you don't have to think hard about what happens at T equals zero for this data set.

678
01:23:22,070 --> 01:23:26,399
That's what it would look like. And then for this.

679
01:23:26,400 --> 01:23:30,210
So this column is the ordered. Observed.

680
01:23:30,630 --> 01:23:35,380
Failure times. So what are those?

681
01:23:35,400 --> 01:23:38,580
Well, two was an observed failure time.

682
01:23:39,030 --> 01:23:44,610
Five was censored. We didn't observers. So five is not going to appear in this data.

683
01:23:44,850 --> 01:23:51,540
Summary eight was an observed event time so it was 15.

684
01:23:51,540 --> 01:23:55,500
So it was 25. So it's 29. So it's 34. So T.J.

685
01:23:56,930 --> 01:24:04,070
Four, Jake was one, two, three, four, five, six. Those are the ordered event times that we're referring to down here.

686
01:24:05,330 --> 01:24:11,720
Under this product thing and then depending on what time little T you're interested in.

687
01:24:13,210 --> 01:24:16,330
You are multiplying this part.

688
01:24:16,330 --> 01:24:20,110
So the rest of the line here is helping you get this term.

689
01:24:20,770 --> 01:24:28,870
So at T.J. equals two, everybody is still at risk because this person hasn't quite had their event yet.

690
01:24:28,870 --> 01:24:35,860
So out of that, there are ten people that contribute to figuring out this one minus J over y j.

691
01:24:36,370 --> 01:24:40,450
There was one event at exactly two. That's what this is.

692
01:24:40,810 --> 01:24:44,350
And then this is just saying what j over y j is.

693
01:24:44,350 --> 01:24:48,550
So is one over ten one minus two over YJ is nine out of ten.

694
01:24:49,030 --> 01:24:56,230
And so the kaplan-meier at time two drops from one to point nine.

695
01:24:58,330 --> 01:25:04,660
So at times zero where no one had had the event, it was a 100% chance of surviving past time zero.

696
01:25:04,960 --> 01:25:11,650
And it will stay at 100% until the first death time in the data set, and then it will be 90%.

697
01:25:13,330 --> 01:25:19,780
And the next time that curve will drop is all the way out at eight because there's no events,

698
01:25:20,230 --> 01:25:23,920
there's no failures or death that was there between two and eight.

699
01:25:24,640 --> 01:25:30,790
There's a censored person. But we've seen that the curves don't drop when you have a sensor person, right?

700
01:25:30,790 --> 01:25:38,500
When we're doing conditional probabilities. And so the next time it's going to drop is that this next t j that was an observed event time.

701
01:25:39,100 --> 01:25:47,259
And so this is kind of at that time, if we look at the row of, of of our bookkeeping data set,

702
01:25:47,260 --> 01:25:56,350
there were eight people at risk for the event at time eight it was persons three through ten who were at risk for having the event at time.

703
01:25:56,350 --> 01:26:01,419
Eight Person five isn't at risk. They're, we're not following them anymore as of time eight.

704
01:26:01,420 --> 01:26:08,230
So we're only looking at these eight people and of them there was one death, so it's one.

705
01:26:08,590 --> 01:26:11,379
And then this is just kind of going through the bookkeeping of what they were.

706
01:26:11,380 --> 01:26:22,450
Y j is one minus t j were y j and then the kaplan-meier estimate is now the product of 0.9 times 0.875.

707
01:26:22,450 --> 01:26:32,260
So that's where that 0.788 comes from. And so this this this last column is always going to be.

708
01:26:33,910 --> 01:26:39,970
Whatever the previous number was times the number over here, that one extra one minus J over YJ.

709
01:26:40,660 --> 01:26:44,200
So if I want to know where this 0.656 came from.

710
01:26:44,770 --> 01:26:50,080
It's 0.78, eight times this last one minus J over YJ.

711
01:26:50,950 --> 01:26:55,150
So 0.788 times 0.833 is 0.656.

712
01:26:56,490 --> 01:27:09,090
And if I want to see what the next one is, this .492 well that's .65, six times that last one minus t j over YJ of 0.75.

713
01:27:09,090 --> 01:27:12,900
So 0.656 times 0.75 is is 0.492.

714
01:27:13,530 --> 01:27:21,300
And it just kind of goes on from there. So 0.330 comes from 0.492 times 0.67.

715
01:27:22,690 --> 01:27:28,720
And this it drops to zero because this last term has 0.33 times zero equals zero.

716
01:27:32,270 --> 01:27:36,229
And so that is how you get this all through time.

717
01:27:36,230 --> 01:27:39,890
So now finally we have a curve, right?

718
01:27:40,190 --> 01:27:50,210
We have time's T for the horizontal axis and what the survival time was at each of those times T which is this column of numbers.

719
01:27:52,220 --> 01:27:59,810
So here is the plot. Of what we just did the survival probability over time.

720
01:28:00,590 --> 01:28:04,070
All the way through the end of our follow up where we had our last event time.

721
01:28:04,730 --> 01:28:10,310
And so the curve drops at the observed events, too.

722
01:28:10,670 --> 01:28:12,260
And then it dropped again at eight.

723
01:28:12,860 --> 01:28:20,420
And all these other drops are at observed event times, and the censored values are showing up in this plot and these little hash marks.

724
01:28:20,450 --> 01:28:23,780
So here is the censored person at five.

725
01:28:24,470 --> 01:28:26,150
They're just represented by little hash.

726
01:28:26,150 --> 01:28:31,910
So you know that there is a person who was lost to follow up or censored administratively or something at that time.

727
01:28:33,460 --> 01:28:43,350
That's what those are coming from. And because the last person in the data set wasn't observed like a death or event, the curve drops to zero.

728
01:28:43,360 --> 01:28:47,550
We saw on the table have dropped to zero. That's going to be a common thing that we see.

729
01:28:47,560 --> 01:28:54,550
If the person with the longest follow up in the data set is an observed death or event,

730
01:28:55,000 --> 01:28:59,350
the curve will drop to zero at that time that it's a general principle that will happen.

731
01:29:00,190 --> 01:29:08,349
If the last person in the data set is censored, the survival curve will just kind of end at that value and it won't have dropped down to zero yet.

732
01:29:08,350 --> 01:29:16,809
It will be whatever that value was. Okay.

733
01:29:16,810 --> 01:29:21,610
So we've accomplished getting our feet wet in a lot of the jargon.

734
01:29:22,420 --> 01:29:26,829
What the data set looks like and what the how to calculate a kaplan-meier curve.

735
01:29:26,830 --> 01:29:32,379
That's actually a big, big jump, no pun intended, because I've been talking about jumps a lot,

736
01:29:32,380 --> 01:29:38,170
but that's actually a big, big step forward in in learning this material.

737
01:29:38,950 --> 01:29:44,649
So I'm going to keep on orienting you about these curves and how to interpret them.

738
01:29:44,650 --> 01:29:48,550
So one thing that we often want to know about is the median survival time.

739
01:29:49,630 --> 01:29:56,170
So I'm going to be calling the median survival time this jargon T with a subscript of point five.

740
01:29:56,530 --> 01:29:59,530
And this point five is kind of standing in for 50%.

741
01:29:59,530 --> 01:30:01,899
So the median survival time, whatever that is,

742
01:30:01,900 --> 01:30:08,740
this number is the time point beyond which 50% of the individuals in the population are expected to survive.

743
01:30:10,440 --> 01:30:16,620
Or cup half empty, the time at which 50% of the population is expected to been dead.

744
01:30:16,800 --> 01:30:20,790
You know, cup up for cup. There's always a cup of FA cup, FMG with survival.

745
01:30:21,420 --> 01:30:31,800
So in kind of jargon, this number satisfies as the survival probability that T is greater than this number is point five.

746
01:30:34,490 --> 01:30:39,770
And so we can get that median survival term from the KAPLAN-MEIER estimate,

747
01:30:39,770 --> 01:30:46,040
or it's going to be the time point when that Kaplan-Meier estimate for first cross is 0.5.

748
01:30:46,040 --> 01:30:49,730
So we're going to see that in a sec here. So here's our little toy data set.

749
01:30:50,360 --> 01:30:57,829
And so when we're looking for median survival time visually from a plot, we find point five here on the vertical axis.

750
01:30:57,830 --> 01:31:06,830
That's when the probability of surviving past a certain time point is 0.5, and we run it along until we hit the kaplan-meier curve and drop down.

751
01:31:06,830 --> 01:31:10,040
And that's going to be the time point. That's our median survival time.

752
01:31:11,240 --> 01:31:19,640
So in this data set, the first time the KAPLAN-MEIER drops below time, drops below 0.5 is at 25.

753
01:31:25,860 --> 01:31:29,579
That went really fast. So we're going to be able to do it from plots.

754
01:31:29,580 --> 01:31:33,920
But you know, when we get to homeworks and things, it's kind of hard to eyeball a plot.

755
01:31:33,930 --> 01:31:41,639
So we get to homeworks and things. We're going to look at the output and find the the column for the Kaplan-Meier estimate

756
01:31:41,640 --> 01:31:46,230
and figure out when it first drops below point five and then find that that time point.

757
01:31:51,310 --> 01:31:55,000
All right. Other little facts. We talked about some of these before.

758
01:31:55,030 --> 01:32:01,180
So the Kaplan-Meier survival estimate is constant between death times or event time.

759
01:32:01,200 --> 01:32:07,330
So it just lays flat until the next observed event in the data set where there was a death.

760
01:32:07,450 --> 01:32:19,079
And then it drops. And so it only drops at these absurd event times that there is information from the censored people

761
01:32:19,080 --> 01:32:24,570
involved because the information from the censored observations contributes to the size of the drops.

762
01:32:25,260 --> 01:32:29,340
So you'll see. Let's just go back here real quick.

763
01:32:29,760 --> 01:32:36,060
The size of the drops is different depending on how many censored people were in the dataset.

764
01:32:36,070 --> 01:32:42,420
So they're not always the same drop the sense that's where the information from the censored people is coming.

765
01:32:42,820 --> 01:32:48,030
It's involved in all these denominators of the things you're multiplying together.

766
01:32:48,300 --> 01:32:54,750
And so these the size of the drops varies depending on how many censored people were contributing to your data.

767
01:32:59,190 --> 01:33:09,000
And another thing to notice is we had a really small data set, but as the number of subjects and death times increases,

768
01:33:09,150 --> 01:33:11,550
so you get a larger and larger data set with a lot of events,

769
01:33:12,210 --> 01:33:17,520
this kaplan-meier curve will become more and more smooth looking, even though they're still tiny.

770
01:33:17,520 --> 01:33:23,230
Drops at the death times are still happening. It's very hard to see them as like a big drop.

771
01:33:23,250 --> 01:33:28,680
So when our first plot with that desaturated is, it looked a lot smoother, didn't it?

772
01:33:28,710 --> 01:33:33,350
I mean, then our toy data set did because they had a lot more events going on.

773
01:33:33,360 --> 01:33:44,989
So let's just look real quickly back at that again. And you know, you can sort of just tell now that, you know, a little bit more about it.

774
01:33:44,990 --> 01:33:51,100
You can tell that this bottom group of people who are desaturated during their six minute walk test,

775
01:33:51,110 --> 01:34:00,600
there were a lot of them that had events here because the drops are happening very often and it's looking more smooth.

776
01:34:00,620 --> 01:34:08,150
There was a lot of data that went into this lower plot and in the top plot there were very few actual events.

777
01:34:08,420 --> 01:34:16,220
There's some, but there's a lot more people who just sort of were censored or lost to follow up or something with the hash marks coming in.

778
01:34:23,800 --> 01:34:35,190
All right. Yeah.

779
01:34:35,460 --> 01:34:39,280
Okay, so we're here. Okay. So.

780
01:34:41,970 --> 01:34:47,040
I want to. Also, I'm getting your feet wet on a lot of different things today.

781
01:34:47,060 --> 01:34:56,000
It's a lot. But I also want to kind of draw your attention in a perk up moment to that last observation in the data set.

782
01:34:56,420 --> 01:35:05,870
The last follow up time. And if that last follow up time, the largest follow up time is a censored individual at time t star.

783
01:35:06,530 --> 01:35:17,000
Then the the estimate for the survival probability at time t is unknown undefined for all values a t greater than that last observed time.

784
01:35:18,470 --> 01:35:23,840
So we can't extrapolate information past the last follow up time in the data set.

785
01:35:24,800 --> 01:35:28,940
And we don't know. That is if it last person censored it.

786
01:35:28,940 --> 01:35:35,030
This curve is not going to go down to zero. It's going to kind of hang somewhere above zero.

787
01:35:35,240 --> 01:35:38,960
And we don't know anything about survival probably past that time.

788
01:35:43,080 --> 01:35:51,270
But if the largest observation, the largest follow up time in the data set is an observed event at that time.

789
01:35:51,570 --> 01:36:00,850
So it was it corresponds to a person who died. Then the curve drops to zero and it will be zero for all points.

790
01:36:00,870 --> 01:36:09,430
Time passed that last death. So it's okay for us to extrapolate our estimate if it went all the way down to zero.

791
01:36:09,450 --> 01:36:14,820
It's not going to go any lower than zero. So we call it zero all the way through.

792
01:36:16,610 --> 01:36:24,920
So this is going to have an implication for us as we get to testing and regression methods that in some data sets,

793
01:36:25,250 --> 01:36:30,319
we can't really talk about what happens after the last observed event time if it's censored.

794
01:36:30,320 --> 01:36:37,160
We just don't have good information past that time and we're going to have to kind of be aware of this or

795
01:36:37,160 --> 01:36:43,640
come back to this idea again and again that in some situations we don't have a very well-defined estimate.

796
01:36:45,180 --> 01:36:49,440
Past the last observed event time glass observed follow up time or other.

797
01:36:53,110 --> 01:36:57,070
Okay. A few more notes about survival analysis.

798
01:36:58,060 --> 01:37:04,660
I've talked about Time Zero being the time that we started following the patients.

799
01:37:04,810 --> 01:37:10,930
So each person has their own time that they started getting followed in calendar time, whether it was April or May.

800
01:37:11,290 --> 01:37:17,170
But the moment we start watching them, we call them time zero on our scale here when we're calculating these probabilities.

801
01:37:18,100 --> 01:37:21,220
That time margin needs to be well-defined for all subjects.

802
01:37:22,750 --> 01:37:26,410
And in in all the methods that I'm teaching you in 523.

803
01:37:28,700 --> 01:37:33,379
And an important assumption is that a time that a patient's able to be followed,

804
01:37:33,380 --> 01:37:38,900
that whole follow up duration should not convey any information about their true failure time.

805
01:37:40,370 --> 01:37:43,670
That's the essential independent censoring assumption.

806
01:37:44,330 --> 01:37:51,350
So just because I am able to watch you for two years versus I'm able to watch you for four years,

807
01:37:51,350 --> 01:37:56,270
depending on my funding mechanism, it should have no influence on how long you were going to live.

808
01:37:58,190 --> 01:38:00,589
That's the essential independent censoring assumption.

809
01:38:00,590 --> 01:38:06,920
And if that assumption fails, consistent estimation is considerably more complicated and may not even be possible.

810
01:38:07,250 --> 01:38:13,579
But those methods are going to go kind of beyond what we do in 523 and May at the very end of the class.

811
01:38:13,580 --> 01:38:23,959
If I have time, squeeze in a little bit of something there. And it's important to understand this assumption because there's different sources of

812
01:38:23,960 --> 01:38:28,730
censoring that you can have that that can interfere with your interpretation of the data.

813
01:38:29,270 --> 01:38:31,520
So subjects can be censored for different reasons.

814
01:38:32,290 --> 01:38:41,150
So an easy thing that isn't going to interfere with our interpretations is if just the study ends before the event has occurred.

815
01:38:41,420 --> 01:38:45,170
And we would have happily watched them forever. But the study just ended.

816
01:38:45,450 --> 01:38:49,489
There's nothing about the study ending that would have any information about

817
01:38:49,490 --> 01:38:53,360
whether person one person to person three would have died or not afterwards.

818
01:38:54,370 --> 01:38:58,000
And so we there's a special name for that called administrative censoring.

819
01:38:58,660 --> 01:39:06,700
It has no information about when these people would have lied. It would have had their events lived, died beyond when the study ended.

820
01:39:08,350 --> 01:39:13,540
So usually that's independent of the failure process and it causes no problems.

821
01:39:14,980 --> 01:39:18,520
Last. A follow up is potentially a problem.

822
01:39:18,840 --> 01:39:27,670
So subject might be lots of follow up if they can't be traced or they're no wonder longer under observation for the event of interest.

823
01:39:28,180 --> 01:39:34,190
And the potential problem is that there might be a reason why they were lost to follow up.

824
01:39:34,210 --> 01:39:41,620
So for instance, suppose the individual leaves the country and therefore can no longer be followed through the national mortality database.

825
01:39:41,620 --> 01:39:50,409
We don't know anything about them. So there's a concern about whether such censoring is related indirectly to the failure time.

826
01:39:50,410 --> 01:39:57,040
So if they left the country because they were feeling great and they didn't feel desperately linked to their doctors or caregivers,

827
01:39:57,490 --> 01:40:01,719
maybe that meant that they were feeling great and they would have had a longer

828
01:40:01,720 --> 01:40:04,570
event time than the people that were still in the study that you're following.

829
01:40:05,530 --> 01:40:12,820
Or, you know, suppose they leave the country because they're so desperate and there's a specialist that they want to live closer to.

830
01:40:12,850 --> 01:40:16,749
Well, they might have a higher risk of death than the people who stuck around.

831
01:40:16,750 --> 01:40:24,310
So the reason for Lost a follow up might introduce bias if it's if it's related to their health status.

832
01:40:26,670 --> 01:40:31,590
And then another common source of censoring is the active withdrawal from the study.

833
01:40:31,600 --> 01:40:37,560
So it's not that you've lost track of them, it's that they actively withdrew from the study.

834
01:40:38,310 --> 01:40:42,450
And this is also another common source of censoring and potential concern.

835
01:40:42,840 --> 01:40:49,220
So for instance, suppose the patient drops out of a clinical trial because that person is too sick to participate.

836
01:40:49,230 --> 01:40:54,090
They might have a higher chance of death than the people who remained in the study.

837
01:40:56,150 --> 01:41:00,300
Or the patient doesn't want to be in the study anymore.

838
01:41:00,320 --> 01:41:07,820
They discontinue participation in the trial because her symptoms have subsided and they don't feel like they're getting any benefit.

839
01:41:07,820 --> 01:41:10,309
But they're spending a lot of time on the trial, so they want to walk away.

840
01:41:10,310 --> 01:41:16,910
But if that's the reason they withdrew, they probably have a smaller chance of dying than the people who remain in the dataset.

841
01:41:17,240 --> 01:41:24,230
So both of these kind of things being related to why you're still able to watch them or not, they can bias your analysis potentially.

842
01:41:26,130 --> 01:41:31,440
So often dependent censoring or informative drop out will be a concern in these cases.

843
01:41:31,440 --> 01:41:35,910
So you'll see both of these phrases thrown around dependent censoring or informative drop out,

844
01:41:36,210 --> 01:41:40,230
both of which mean that your standard methods for survival analysis won't be valid.

845
01:41:41,010 --> 01:41:48,390
And you'll have to bring in an expert that can maybe help you figure out if there's anything that can be done.

846
01:41:52,610 --> 01:41:58,520
All right. So here's just some graphics to help solidify some of these ideas.

847
01:41:58,570 --> 01:42:08,270
So suppose the person enters the study here, that's that individual person's time, zero, and then they drop out some time over here.

848
01:42:08,930 --> 01:42:13,430
We really were hoping to follow them all the way through in this study. But they they dropped out.

849
01:42:14,120 --> 01:42:17,840
And we don't see anything after the dropout. Right.

850
01:42:18,740 --> 01:42:21,020
We don't see any of that information about what happened to them.

851
01:42:21,230 --> 01:42:27,830
I've made up a pretend event time like the death time out here, but we wouldn't be able to see this event.

852
01:42:28,340 --> 01:42:36,200
And so the observed data on study, the II is going to end up being this tiny length of time here before they dropped out.

853
01:42:37,330 --> 01:42:40,909
So the length of this time is going to be zero. And then what's their delta?

854
01:42:40,910 --> 01:42:44,260
I'm going to be. Is it going to Delta?

855
01:42:44,260 --> 01:42:55,650
Is it going to be won or is it going to be zero? Okay.

856
01:42:55,800 --> 01:43:01,590
So just as a reminder, there's so much notation here. Delta II is a one.

857
01:43:02,840 --> 01:43:07,400
If tis less than I. So where's the tie in this graphic?

858
01:43:09,910 --> 01:43:14,300
Tea is way out here. Right.

859
01:43:14,840 --> 01:43:21,659
And we didn't get to see it. Where is their C.I.? The sky is over here.

860
01:43:21,660 --> 01:43:25,440
This is the last time we saw them alive. This is their C.I.

861
01:43:27,110 --> 01:43:33,169
So the IXI is the minimum of those two. This is the EXI and this was the last time we saw them.

862
01:43:33,170 --> 01:43:37,070
And the reason we don't see them anymore is they dropped out. They didn't die, they dropped out.

863
01:43:37,080 --> 01:43:41,270
So their exercise going to be a zero. I'm sorry, Delta IV is going to be a zero.

864
01:43:43,470 --> 01:43:51,150
All right. So the observed time on study this exercise here we really wanted to measure this whole time tie but we couldn't.

865
01:43:54,880 --> 01:44:04,000
So I keep on emphasizing this, that the KAPLAN-MEIER requires the censoring mechanism to be independent of the failure mechanism.

866
01:44:04,900 --> 01:44:14,160
And so just saying that another way, if we see an X side Delta I where the Xs five in the delta is zero.

867
01:44:14,170 --> 01:44:17,950
So the last time we saw them alive was at time five and they were alive.

868
01:44:18,640 --> 01:44:24,880
That should imply nothing about their real death time other than it was greater than five.

869
01:44:25,840 --> 01:44:29,560
They should be similar to everybody in the data set who was also alive at five.

870
01:44:32,710 --> 01:44:41,380
So stated another way yet again, an individual censored at five has the same risk of death as individuals alive and uncensored at time five.

871
01:44:42,010 --> 01:44:45,970
That's all we should know if we see this data point for that person in the dataset.

872
01:44:46,750 --> 01:44:50,710
These are all different ways of describing the independent censoring assumption.

873
01:44:53,940 --> 01:45:01,200
So, for example, independent censoring assumption would be violated of patients who regain their health are more likely to withdraw from the study.

874
01:45:05,660 --> 01:45:10,340
All right. So there are a lot more formulas going on here.

875
01:45:10,340 --> 01:45:14,419
I know that we're kind of wrapping up for today pretty soon.

876
01:45:14,420 --> 01:45:20,780
So we'll probably kind of stop before we get too bogged down in a new topics.

877
01:45:20,780 --> 01:45:25,909
Let's just see if there's some mental things to chew on that are easy to chew on in this handout.

878
01:45:25,910 --> 01:45:33,560
So we're going to have some variability that depends on time t for these kaplan-meier estimates.

879
01:45:34,160 --> 01:45:39,920
So each one of these points on the survival curve is an estimate, and it's going to have its own variability that depends on time.

880
01:45:39,920 --> 01:45:45,950
T That's kind of a bit of a stretch, right? We have to stretch our minds around the fact that when we look at a kaplan-meier

881
01:45:46,370 --> 01:45:51,529
function and we think about 95% confidence intervals for each of those time points,

882
01:45:51,530 --> 01:45:58,820
they're going to have different confidence interval widths. And these numbers tend to get bigger as the little T gets bigger.

883
01:45:59,660 --> 01:46:03,110
You know, think about you have fewer people at risk in the data set far out.

884
01:46:03,470 --> 01:46:08,270
So your estimates of survival probably far out in time are going to be more wiggly.

885
01:46:09,250 --> 01:46:12,280
So that's going to be a thing to look forward to next time.

886
01:46:13,900 --> 01:46:17,530
I'll talk about confidence intervals. Next time, that's a bit of a detail. Let's see if there's anything else.

887
01:46:17,530 --> 01:46:22,419
I want to just give you a big picture that will help. All right.

888
01:46:22,420 --> 01:46:26,409
I will give you a little teaser as well that we'll spend more time on next time.

889
01:46:26,410 --> 01:46:35,590
And is that that is that we're going to introduce a new function to our our knowledge base called the hazard function.

890
01:46:36,280 --> 01:46:40,720
And so we already have, we've been talking about survival functions.

891
01:46:40,770 --> 01:46:52,360
SFD So we're going to have this new thing called Lambda of T the hazard and it's this is going to be a bit tricky to to own some seed you now.

892
01:46:52,900 --> 01:46:58,000
So the next time we talk about this you'll have that seed that's already started germinating in your in your back brain.

893
01:46:58,960 --> 01:47:05,260
And so the hazard is a function where just look at this probability part first.

894
01:47:05,260 --> 01:47:08,260
Given their event, time hadn't happened yet.

895
01:47:08,260 --> 01:47:16,420
They're still at risk for having something terrible happen. What's the probability that it happens in that next snap of a moment?

896
01:47:18,190 --> 01:47:24,280
And that's the probability part. They had this weird one over Delta thing, and the limit is Delta goes to zero.

897
01:47:24,280 --> 01:47:31,480
So this is what makes it really tricky to interpret. So there's some rescaling here that happens.

898
01:47:31,990 --> 01:47:37,810
So for now, just think about a very, very teeny tiny delta like a snap.

899
01:47:38,260 --> 01:47:43,120
That's pretty much like Delta being close to zero.

900
01:47:43,570 --> 01:47:49,390
You're having one divided by something very small. So this could be this is not going to be something that's between zero and one.

901
01:47:49,690 --> 01:47:53,830
This is going to be a probability that's divided by a small number.

902
01:47:54,100 --> 01:47:58,329
And so it's going to tend to be it could be anything greater than zero.

903
01:47:58,330 --> 01:48:01,870
It could definitely go beyond one. So it's not quite a probability.

904
01:48:01,870 --> 01:48:04,360
It's kind of a probability per unit time.

905
01:48:05,350 --> 01:48:15,190
And for me, when I'm trying to get intuition about this, I always go back to The Princess Bride, which I think is still a popular movie.

906
01:48:15,190 --> 01:48:22,720
It's become kind of a staple. And there's the guy who just one day, is that his name?

907
01:48:23,020 --> 01:48:32,560
There's the guy who is trying to do a contest with intellect, with Wesley, the main character.

908
01:48:32,830 --> 01:48:37,209
And he's he puts poison or Wesley puts poison in caps,

909
01:48:37,210 --> 01:48:44,140
and then he's supposed to have some mental fight about, you know, which cup has the poison and the present.

910
01:48:44,140 --> 01:48:49,600
He thinks he's one, you know, because he did some trickery. He thinks he's one and that Wesley's had the poison and he's laughing.

911
01:48:49,600 --> 01:48:52,180
He's going, Oh, ha ha ha ha. You know?

912
01:48:52,270 --> 01:49:05,320
And then, you know, so that that that's this T plus delta T, it's like, you know, he was fine right up until then and then boom.

913
01:49:05,530 --> 01:49:12,040
And so this is kind of like this, that that's the moment I think of when I think of about a hazard function.

914
01:49:12,580 --> 01:49:21,940
And so it's a it's a little strange creature, but it's going to be something that we use a lot when we're modeling regression models.

915
01:49:21,940 --> 01:49:28,630
The most popular regression model is modeling that kind of hazard over time.

916
01:49:28,960 --> 01:49:32,320
So when we learn regression, this is going to be a big player.

917
01:49:32,320 --> 01:49:35,379
So I want to give you some intuition about that, but we'll get to it next time.

918
01:49:35,380 --> 01:49:42,280
But in the meantime, I just want you to chew on this hazard so that you recognize the jargon and you think of the.

919
01:49:44,530 --> 01:49:49,170
What is it? What is his name again? The Senior.

920
01:49:49,380 --> 01:49:53,580
I can't remember anymore. You know, my memory is not so great. I have. I haven't watched that since I have little kids.

921
01:49:53,580 --> 01:49:58,770
So. So that's it for today. We'll pick up on slide 30 next time.

922
01:50:00,210 --> 01:50:00,540
Right.

