1
00:00:05,050 --> 00:00:09,877
Hello, this lecture will cover hypothesis
testing with continuous outcomes when you

2
00:00:09,877 --> 00:00:13,591
have three or more populations
you're interested in comparing.

3
00:00:13,591 --> 00:00:19,730
The statistical method in this case
is known as analysis of variance.

4
00:00:19,730 --> 00:00:23,530
We call that t-test were used when
we wanted to compare the means of

5
00:00:23,530 --> 00:00:29,170
two populations when we had a continuous
outcome measured in two samples.

6
00:00:29,170 --> 00:00:32,220
We now discuss the extension
of two sample t-test

7
00:00:32,220 --> 00:00:35,390
when we want to compare
more than two groups.

8
00:00:35,390 --> 00:00:40,317
The method used to compare the means
of these groups is known as analysis of

9
00:00:40,317 --> 00:00:41,121
variance.

10
00:00:41,121 --> 00:00:45,628
Now do not be fooled by the name,
analysis of variance also known by

11
00:00:45,628 --> 00:00:49,182
the acronym ANOVA is not
a comparison of variances.

12
00:00:49,182 --> 00:00:52,710
It is still a comparison of means.

13
00:00:52,710 --> 00:00:56,685
Now, the ANOVA method we will
learn in this lecture is known as

14
00:00:56,685 --> 00:00:58,831
one-way analysis of variance.

15
00:00:58,831 --> 00:01:02,623
You may be familiar with
two-way analysis of variance.

16
00:01:02,623 --> 00:01:08,261
However, we will not cover
that method in this lecture.

17
00:01:08,261 --> 00:01:12,192
Now, imagine you have a set
of capital K populations.

18
00:01:12,192 --> 00:01:17,138
Each of those populations has a mean
represented by the Greek letter mu and

19
00:01:17,138 --> 00:01:19,931
subscript to denote the population, and

20
00:01:19,931 --> 00:01:24,505
we wish to compare the means of all
these populations to each other.

21
00:01:24,505 --> 00:01:29,153
The null hypothesis would again,
be that all the populations have

22
00:01:29,153 --> 00:01:33,728
the same mean that mu 1 is equal to mu 2,
etc., up to in the mu K.

23
00:01:33,728 --> 00:01:39,204
The alternative is more complexed than
it was with the two sample t-test.

24
00:01:39,204 --> 00:01:43,933
In this case, our alternative
hypothesis is that at least two of

25
00:01:43,933 --> 00:01:47,883
the population means
are different from each other.

26
00:01:47,883 --> 00:01:54,750
We don't know which to means and it may
be more than a single pair of means.

27
00:01:54,750 --> 00:01:58,780
We say that two or
more of the population means are unequal.

28
00:01:58,780 --> 00:02:02,320
Because there are a vast number of
differences in this alternative,

29
00:02:02,320 --> 00:02:05,450
ANOVA is also known as an omnibus test.

30
00:02:05,450 --> 00:02:10,701
We return to our example published
by Pase et al in 2017 looking at

31
00:02:10,701 --> 00:02:17,236
the association of sugary beverage intake
and the level of caloric intake in adults.

32
00:02:17,236 --> 00:02:19,791
We now have three populations
we wish to compare.

33
00:02:19,791 --> 00:02:24,292
The first population are the US adults
who are consuming less than one sugary

34
00:02:24,292 --> 00:02:25,424
beverage per day.

35
00:02:25,424 --> 00:02:29,970
The second population consumes one
to two sugary beverages per day and

36
00:02:29,970 --> 00:02:34,602
the third population consumes more
than two sugary beverages per day.

37
00:02:34,602 --> 00:02:39,577
Each of those populations has a mean
number of calories consumed per day, mu 1,

38
00:02:39,577 --> 00:02:40,540
mu 2 and mu 3.

39
00:02:40,540 --> 00:02:47,340
Our null hypothesis is that all three
populations have the same mean.

40
00:02:47,340 --> 00:02:52,223
And therefore, sugary beverage
consumption does not have an association

41
00:02:52,223 --> 00:02:54,631
with total average caloric intake.

42
00:02:54,631 --> 00:02:59,168
Again, the alternative is that at least
two of these means are different from each

43
00:02:59,168 --> 00:03:02,210
other and there are three
ways by which this can occur.

44
00:03:02,210 --> 00:03:06,023
It could mean that the first two
population means differ and, or

45
00:03:06,023 --> 00:03:09,343
the first and
the third population means differ and, or

46
00:03:09,343 --> 00:03:13,800
the second and third population
means differ from each other.

47
00:03:13,800 --> 00:03:19,845
So there are three comparisons of means
that are in the alternative hypothesis and

48
00:03:19,845 --> 00:03:25,373
we will return to these multiple
pairwise comparisons in a later lecture.

49
00:03:25,373 --> 00:03:29,858
Now, Pase et al report the following
summary information from the data they

50
00:03:29,858 --> 00:03:31,481
collected in their study.

51
00:03:31,481 --> 00:03:35,293
Again, we have three populations
defined by the level of sugary beverage

52
00:03:35,293 --> 00:03:37,030
consumption.

53
00:03:37,030 --> 00:03:39,480
Each of the groups has a sample mean and

54
00:03:39,480 --> 00:03:42,260
a sample standard deviation
of calories per day.

55
00:03:42,260 --> 00:03:45,430
And I also listen the final column,
the number of subjects and

56
00:03:45,430 --> 00:03:47,860
each of the three samples.

57
00:03:47,860 --> 00:03:51,210
Now from this data,
we can see that the group with the lowest

58
00:03:51,210 --> 00:03:55,850
sugary beverage consumption has the lowest
caloric intake and the group with

59
00:03:55,850 --> 00:03:59,530
the highest beverage consumption
has the highest calorie intake.

60
00:03:59,530 --> 00:04:04,620
So these data do suggest through their
sample means that there is a difference

61
00:04:04,620 --> 00:04:08,130
in the mean caloric consumption
between these three groups.

62
00:04:08,130 --> 00:04:12,472
However, we need to formally access
this relative to the amount of

63
00:04:12,472 --> 00:04:17,372
noise quantified by the sample standard
deviations and the sample sizes.

64
00:04:17,372 --> 00:04:18,650
Now for exposition,

65
00:04:18,650 --> 00:04:22,629
I'm going to reduce each of
the sample sizes by a factor of 100.

66
00:04:22,629 --> 00:04:27,289
So let us pretend that the first sample
only had 24 observations, the second

67
00:04:27,289 --> 00:04:33,980
sample only had 12 observations and
the third sample only had 6 observations.

68
00:04:33,980 --> 00:04:38,258
The first thing we do is compute
the mean of all of the 42 observations,

69
00:04:38,258 --> 00:04:40,022
which we call the grand mean.

70
00:04:40,022 --> 00:04:44,417
So that is simply 24 times the first
sample mean plus 12 times the second

71
00:04:44,417 --> 00:04:49,253
sample mean plus 6 times the third sample
mean divided by all 42 observations.

72
00:04:49,253 --> 00:04:56,113
That gives me an overall
sample mean of 1,898 calories.

73
00:04:56,113 --> 00:05:00,464
If the null hypothesis that the three
populations have the same mean,

74
00:05:00,464 --> 00:05:06,060
then the sample means should all
be close to this grand mean.

75
00:05:06,060 --> 00:05:09,340
We can visually represent
this in this figure.

76
00:05:09,340 --> 00:05:12,957
The x-axis denotes the three
group memberships and

77
00:05:12,957 --> 00:05:16,501
the y-axis represents
the mean calories per day.

78
00:05:16,501 --> 00:05:20,667
The horizontal blue lines represent
the means of the three samples and

79
00:05:20,667 --> 00:05:23,986
the horizontal dashed yellow
line represents the grand

80
00:05:23,986 --> 00:05:25,691
mean that we just computed.

81
00:05:25,691 --> 00:05:29,573
The arrows represent how far each
of the sample group means is

82
00:05:29,573 --> 00:05:31,103
from the overall mean.

83
00:05:31,103 --> 00:05:35,250
We can see that if these three horizontal
blue lines move closer to the yellow line,

84
00:05:35,250 --> 00:05:37,512
they'll become more similar to each other.

85
00:05:37,512 --> 00:05:40,464
And as I move these blue lines
further from the yellow line,

86
00:05:40,464 --> 00:05:43,313
I show that the groups are more
distinct from each other.

87
00:05:43,313 --> 00:05:47,773
So the size of these three
black arrows is helping me to

88
00:05:47,773 --> 00:05:50,557
figure out the signal in my data.

89
00:05:50,557 --> 00:05:53,806
So the signal, however is
the average of the square distances.

90
00:05:53,806 --> 00:05:57,972
Again, I am not interested on whether or
not is a groups mean is above or

91
00:05:57,972 --> 00:05:59,327
below the grand mean.

92
00:05:59,327 --> 00:06:04,557
I simply wanna know how far
each is from the grand mean.

93
00:06:04,557 --> 00:06:08,680
Each group also receives a weight
equal to that group's sample size.

94
00:06:08,680 --> 00:06:13,570
A group that has more observations in it
should contribute more information to me

95
00:06:13,570 --> 00:06:17,230
than a sample that has
only a few observations.

96
00:06:17,230 --> 00:06:22,781
So if I have a grand mean of x bar dot and
I have three sample means of x bar 1,

97
00:06:22,781 --> 00:06:27,082
x bar 2 and x bar 3,
then the signal is simply the squared

98
00:06:27,082 --> 00:06:30,668
difference between each
of the sample means and

99
00:06:30,668 --> 00:06:35,982
the grand mean weighted by the number
of observations in each sample.

100
00:06:35,982 --> 00:06:37,801
I add up those three numbers and
I take an average.

101
00:06:37,801 --> 00:06:39,924
However, I do not divided by 3.

102
00:06:39,924 --> 00:06:47,652
I divided by 3-1 for mathematical
reasons that I will not discuss further.

103
00:06:47,652 --> 00:06:48,390
In general,

104
00:06:48,390 --> 00:06:52,832
we have an equation here that if we are
comparing the means of capital K groups.

105
00:06:52,832 --> 00:06:56,737
Again, we simply take the squared
deviation of each group's mean from

106
00:06:56,737 --> 00:07:00,440
the overall mean weighted by
the number of observations.

107
00:07:00,440 --> 00:07:05,102
Add those up and
divided by the number of groups minus 1.

108
00:07:05,102 --> 00:07:09,592
This signal is known as the means
sum of squares between the groups.

109
00:07:09,592 --> 00:07:13,310
It tells me how much the groups
differ from each other.

110
00:07:13,310 --> 00:07:15,732
Again, I would like this number to be big.

111
00:07:15,732 --> 00:07:19,908
This tells me that if my samples
differ from each other quite a bit,

112
00:07:19,908 --> 00:07:24,773
then I'm likely to conclude that
the populations have a difference as well.

113
00:07:24,773 --> 00:07:30,575
In my example, if I plug in the numbers
that Pase gathered from their study,

114
00:07:30,575 --> 00:07:36,027
I find that I get a mean sum of
square between groups of 412,833.

115
00:07:36,027 --> 00:07:38,823
The magnitude of this
number is irrelevant and

116
00:07:38,823 --> 00:07:44,240
uninterpretable until we compare it to the
level of noise that is seen in the data.

117
00:07:44,240 --> 00:07:49,580
Again, I'm looking for a signal to
noise ratio, this is the signal.

118
00:07:49,580 --> 00:07:54,025
The noise is computed by looking at
each of the individual observations

119
00:07:54,025 --> 00:07:56,331
relative to the mean of their group.

120
00:07:56,331 --> 00:08:00,712
So in this figure, again, on the x-axis,
I denote the group membership.

121
00:08:00,712 --> 00:08:03,195
The number of sugary
beverages consumed and

122
00:08:03,195 --> 00:08:07,390
the y-axis is highlighting
the number of calories consumed.

123
00:08:07,390 --> 00:08:11,640
Each of the dots is an individual in
that group and the three colors, again,

124
00:08:11,640 --> 00:08:16,240
help to make the three groups
visually distinct from each other.

125
00:08:16,240 --> 00:08:21,500
The horizontal lines represent
the mean in each of the groups.

126
00:08:21,500 --> 00:08:25,600
I now want to measure how much
each observation deviates from

127
00:08:25,600 --> 00:08:29,420
their group's mean highlighted
by the black arrows.

128
00:08:29,420 --> 00:08:35,170
These deviations quantify how
noisy the data are themselves.

129
00:08:35,170 --> 00:08:37,150
So I look at the first group.

130
00:08:37,150 --> 00:08:41,140
I take each of these deviations, the
difference between an observed person's

131
00:08:41,140 --> 00:08:43,232
caloric intake and their group's mean.

132
00:08:43,232 --> 00:08:47,697
Again, I am not interested in whether or
not someone's observation is above or

133
00:08:47,697 --> 00:08:48,641
below the mean.

134
00:08:48,641 --> 00:08:52,630
So I square those differences and
I add them up.

135
00:08:52,630 --> 00:08:57,502
I do the same thing for the second group,
I have 12 observations in this group.

136
00:08:57,502 --> 00:08:59,392
And in the third group,
I have six observations.

137
00:08:59,392 --> 00:09:04,142
I add up these six squared
deviations from the group mean.

138
00:09:04,142 --> 00:09:08,938
The overall noise in the data is simply
the sum of these three sums of squared

139
00:09:08,938 --> 00:09:11,904
deviations.

140
00:09:11,904 --> 00:09:14,090
So I add up this information.

141
00:09:14,090 --> 00:09:19,442
And again, I would like an average across
all of the 42 observations in my data set.

142
00:09:19,442 --> 00:09:22,984
However, I do not take the sum and
divided by 42.

143
00:09:22,984 --> 00:09:25,188
Again, I make a slight change and

144
00:09:25,188 --> 00:09:29,212
I divided by the sample size
reduced by the number of groups.

145
00:09:29,212 --> 00:09:33,107
Again, for mathematical reasons
that are not important for

146
00:09:33,107 --> 00:09:36,694
the content in this course and
the final equation on this

147
00:09:36,694 --> 00:09:40,533
slide represents a general equation for
capital K groups.

148
00:09:40,533 --> 00:09:43,411
Again, there are two sums,
the two sigma's.

149
00:09:43,411 --> 00:09:48,945
The sum on the inside is adding up
deviations within each group and

150
00:09:48,945 --> 00:09:55,190
then I add up those sums across all
the K groups, and than take an average.

151
00:09:55,190 --> 00:09:57,630
This is called the mean squared error.

152
00:09:57,630 --> 00:09:58,434
And again,

153
00:09:58,434 --> 00:10:03,512
represents the amount of noise relative
to the amount of signal in my data.

154
00:10:03,512 --> 00:10:09,925
From the data and Pase,
we get a mean squared error of 393,870.

155
00:10:09,925 --> 00:10:14,089
Again, the magnitude of this number is
irrelevant until I compare it to the mean

156
00:10:14,089 --> 00:10:17,090
square between.

157
00:10:17,090 --> 00:10:22,442
So like the t-test, we again have a signal
to to noise ratio for our test statistic.

158
00:10:22,442 --> 00:10:27,123
For ANOVA, we use the letter F to
represent the test statistic instead of t.

159
00:10:27,123 --> 00:10:32,721
And again, it is the ration of the mean
sum of squared between groups and

160
00:10:32,721 --> 00:10:34,692
the mean squared error.

161
00:10:34,692 --> 00:10:38,401
The statistic will then be
converted to a p-value.

162
00:10:38,401 --> 00:10:43,389
Now the sampling distribution, the
variability of the F-statistic from sample

163
00:10:43,389 --> 00:10:46,432
to sample can be quantified
by a different model.

164
00:10:46,432 --> 00:10:48,692
It is not a normal distribution in ANOVA.

165
00:10:48,692 --> 00:10:51,800
It is what is known as an F-distribution.

166
00:10:51,800 --> 00:10:55,414
The F-distribution will change with
the number of observations and

167
00:10:55,414 --> 00:10:57,000
the number of groups.

168
00:10:57,000 --> 00:11:00,975
Therefore, unlike a t-test,
we do not have a standard value of two

169
00:11:00,975 --> 00:11:04,063
to decide whether or
not we will find significance.

170
00:11:04,063 --> 00:11:08,326
That number will vary again, depending
upon the number of observations and

171
00:11:08,326 --> 00:11:09,602
the number of groups.

172
00:11:09,602 --> 00:11:14,772
Therefore, you would need a computer
to get the actual value you need.

173
00:11:14,772 --> 00:11:15,785
But generally,

174
00:11:15,785 --> 00:11:20,716
we want large values of our F-statistic
in other to reject the null hypothesis.

175
00:11:20,716 --> 00:11:24,415
If the F-statistics is large,
that means that there is a distinct

176
00:11:24,415 --> 00:11:29,100
difference between the sample means
relative to the noise in the data.

177
00:11:29,100 --> 00:11:31,990
The observations tend to cluster
in their groups rather than

178
00:11:31,990 --> 00:11:34,130
overlap with each other.

179
00:11:34,130 --> 00:11:37,060
In our example,
the ratio of the mean square between and

180
00:11:37,060 --> 00:11:41,350
the mean squared error gives
us an F-statistic of 1.05.

181
00:11:41,350 --> 00:11:45,686
Again, to determine whether or not this
is enough information to reject the null

182
00:11:45,686 --> 00:11:49,082
hypothesis, we'll require
a computer to give us a p-value.

183
00:11:49,082 --> 00:11:53,945
The p-value for
this situation ends up to be 0.36.

184
00:11:53,945 --> 00:11:56,826
So again,
if we assume a type 1 error rate of 5%,

185
00:11:56,826 --> 00:12:02,140
meaning that our p-value must be less
than 0.5 to reject the null hypothesis.

186
00:12:02,140 --> 00:12:05,750
We would fail to reject the null
hypothesis from this data and

187
00:12:05,750 --> 00:12:09,650
conclude that we lack evidence that there
is an association between sugary drink

188
00:12:09,650 --> 00:12:13,580
consumption, and
daily caloric consumption.

189
00:12:13,580 --> 00:12:14,430
However, again,

190
00:12:14,430 --> 00:12:19,680
I wanna emphasize that failing to reject
the null does not mean the null was true.

191
00:12:19,680 --> 00:12:23,662
We do not conclude that there is no
association between sugar drinking

192
00:12:23,662 --> 00:12:26,302
consumption and daily calorie consumption.

193
00:12:26,302 --> 00:12:31,020
Again, we were unlikely to find
significance simply because we only had 42

194
00:12:31,020 --> 00:12:34,663
observations in this data set
divided among three groups.

195
00:12:34,663 --> 00:12:38,102
The p-value is also driven by
the amount of data you have.

196
00:12:38,102 --> 00:12:38,879
So in this case,

197
00:12:38,879 --> 00:12:42,162
it would've been difficult to find
significance in the first place.

198
00:12:42,162 --> 00:12:48,230
There may be a difference, we just
cannot find it with this small sample.

199
00:12:48,230 --> 00:12:48,930
Now often,

200
00:12:48,930 --> 00:12:52,993
the results of ANOVA are displayed
in what is known as an ANOVA table.

201
00:12:52,993 --> 00:12:55,703
It typically has five
columns of information.

202
00:12:55,703 --> 00:12:59,292
Our interest,
again is on the last three columns.

203
00:12:59,292 --> 00:13:02,424
We have discussed the mean square
between and the mean square error,

204
00:13:02,424 --> 00:13:05,800
which also known as the mean
square within groups.

205
00:13:05,800 --> 00:13:08,970
The mean square between
groups is our signal and

206
00:13:08,970 --> 00:13:12,520
the mean square within groups or
error is the noise.

207
00:13:12,520 --> 00:13:15,790
We take the ratio of those two
numbers to get my F-statistic and

208
00:13:15,790 --> 00:13:21,801
that F-statistic is then found in
an F-distribution that produces a p-value

