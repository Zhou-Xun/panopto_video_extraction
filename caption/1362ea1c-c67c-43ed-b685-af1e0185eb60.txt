1
00:00:09,570 --> 00:00:17,520
In this unit. We've been talking through several different examples of how we can introduce selection bias into our studies,

2
00:00:17,760 --> 00:00:22,110
either by the selection or retention of participants into our studies.

3
00:00:22,800 --> 00:00:26,790
So far, we've looked at intuitive ways of understanding selection bias.

4
00:00:27,030 --> 00:00:33,089
We've also looked at mathematical ways by looking at two by two tables in this video here,

5
00:00:33,090 --> 00:00:40,200
what I want to show you is how we conceptualize selection bias using causal diagrams or dacs.

6
00:00:41,160 --> 00:00:48,840
So in this particular video, you should expect to learn how to define selection bias structurally in dogs.

7
00:00:49,320 --> 00:00:57,480
Well, then look at how to identify selection bias in several different dogs using examples that you've seen already.

8
00:00:59,220 --> 00:01:11,910
The way in which we conceptualize selection bias in a DAG is having selection B a common effect of both exposure and our outcome, as you recall,

9
00:01:11,910 --> 00:01:22,560
by conditioning on selection, which is a collider here, what we do is we artificially induce association between our exposure and our outcome.

10
00:01:24,230 --> 00:01:31,879
The reason that conditioning on a collider causes an open backdoor path between two unrelated

11
00:01:31,880 --> 00:01:39,530
causes of that collider is because by looking only within one value of that collider,

12
00:01:39,950 --> 00:01:43,460
we now have information about the two causes.

13
00:01:43,610 --> 00:01:52,040
There is no association between whether or not a sprinklers on wetting our grass or whether or not there's rain that results in wet grass.

14
00:01:52,610 --> 00:02:01,760
But once they know that wet grass has happened, I now have information on whether or not the sprinkler went off based on whether or not it rained.

15
00:02:03,660 --> 00:02:12,840
So let's apply the rules of a DAG here to our example of looking at alcohol use as a risk factor for diabetic shock.

16
00:02:13,770 --> 00:02:21,419
As you recall, we talked about a scenario where we had a case control study where people presenting to the hospital with

17
00:02:21,420 --> 00:02:28,650
diabetic shock were cases and the controls were other people who presented to the hospitals with injuries.

18
00:02:30,330 --> 00:02:31,830
Let's draw our DAG.

19
00:02:32,100 --> 00:02:40,290
So here what you can see is that I've drawn an arrow between drinking and car crashes and car crashes to going to the emergency room.

20
00:02:40,290 --> 00:02:45,299
Right. Because drinking puts you at greater risk of being in a crash and then crashes.

21
00:02:45,300 --> 00:02:53,460
If you're injured, you're going to show up at the E.R. Similarly, I have diabetic shock going to the E.R. because you need to be treated.

22
00:02:53,940 --> 00:02:59,640
However, you'll notice that there's an absence of an arrow between drinking and diabetic shock,

23
00:03:00,000 --> 00:03:08,160
meaning that there is no true causal association between diabetic shock and alcohol use in the general population.

24
00:03:09,760 --> 00:03:15,160
The trip comes in when we subset our population to the emergency room.

25
00:03:15,310 --> 00:03:23,060
In other words, we're taking our cases and controls from the emergency room itself by doing that.

26
00:03:23,080 --> 00:03:31,420
What we've done is we've conditioned on a collider and we've induced an association between drinking and diabetic shock.

27
00:03:32,740 --> 00:03:39,910
In other words, if we had a scenario where the only two reasons that you would go to the emergency room were either because you had

28
00:03:39,910 --> 00:03:48,850
diabetic shock or because you had gotten in a car crash by somebody walking in the door and not having a diabetic shock.

29
00:03:48,850 --> 00:03:55,870
We now know that they were in a car crash. What this does is that it makes it so that in the study population,

30
00:03:56,170 --> 00:04:03,880
we would observe associations between drinking and diabetic shock when there was none in the true population.

31
00:04:05,570 --> 00:04:08,570
Let's next look at a lost a follow up example.

32
00:04:09,200 --> 00:04:15,710
This was a scenario where we were interested in whether or not new medications change people's risk of death.

33
00:04:17,430 --> 00:04:26,610
Drawing your dog for this particular scenario. What we have here is that treatment leads to people being lost, otherwise known as censoring,

34
00:04:26,880 --> 00:04:34,470
because they're more likely to have side effects and side effects make people feel gross and therefore they stop participating in the study.

35
00:04:34,620 --> 00:04:43,470
We also had that underlying disease was also a cause for people having more side effects and therefore disappearing from the study population.

36
00:04:44,340 --> 00:04:53,430
Again, in this particular DAG, you can see that there is no open backdoor path between mortality and medication use

37
00:04:53,700 --> 00:05:00,870
because censoring is a collider and no crewed association can pass through that collider.

38
00:05:01,200 --> 00:05:08,730
If we sample on everyone, however, by nature of people not coming back to our study,

39
00:05:09,000 --> 00:05:18,450
we are inherently conditioning on whether or not somebody is censored effectively, only taking data from people who do come back in our study.

40
00:05:18,990 --> 00:05:24,360
And therefore what we have done is we have induced associations between death

41
00:05:24,360 --> 00:05:29,139
and our treatment simply because of the way in which people are being lost.

42
00:05:29,140 --> 00:05:37,560
To follow up. Now let's take the final example of selection bias that can occur due to participation bias.

43
00:05:38,040 --> 00:05:47,970
Here the scenario was looking at pesticide use and prostate cancer and thinking about how family history can play a role in this association.

44
00:05:49,650 --> 00:05:56,650
So here what I have drawn is an association between pesticide use and participation, right?

45
00:05:56,670 --> 00:06:01,440
So people who are worried about their exposures are more likely to participate in the study.

46
00:06:02,010 --> 00:06:06,570
Similarly, I have an era between family history and participation,

47
00:06:06,870 --> 00:06:15,780
because people who've lost somebody due to prostate cancer in their family are more likely to participate in a study on that outcome of interest.

48
00:06:16,680 --> 00:06:26,670
Similarly, family history is a risk factor for prostate cancer in the general population because participation is a collider.

49
00:06:27,000 --> 00:06:33,150
There is no crude association between pesticide use and prostate cancer in this DAG.

50
00:06:34,810 --> 00:06:41,230
However, if I condition on participation so people self-selecting into my study,

51
00:06:41,680 --> 00:06:48,700
what I will do is I will induce an association between prostate cancer and pesticide use.

52
00:06:49,060 --> 00:06:55,180
What this means is I would observe an association among this population of my study participants

53
00:06:55,480 --> 00:07:01,090
that was different than what I would get in the overall population if I could sample everyone.

54
00:07:02,810 --> 00:07:08,090
So the key takeaways from this particular video are to think about the structural definition

55
00:07:08,090 --> 00:07:15,530
of selection bias as selection being a common effect of both your exposure and your outcome.

56
00:07:16,460 --> 00:07:26,510
If we condition on selection, then we will be conditioning on a collider and therefore introduce bias between our exposure and outcome and

57
00:07:26,510 --> 00:07:32,300
get a different association to my study population than what I would expect to see in the general population.

