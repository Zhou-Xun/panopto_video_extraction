1
00:00:01,510 --> 00:00:05,829
Hi. Hi, everyone. Welcome.

2
00:00:05,830 --> 00:00:09,489
One more time to my thoughts.

3
00:00:09,490 --> 00:00:18,910
523 laughs Today we're going to finish covering surviving the minorities in our audience thoughts.

4
00:00:20,110 --> 00:00:28,240
These will be hopefully very helpful for homework number four, which is due on November 9th, I think.

5
00:00:29,710 --> 00:00:33,610
Okay. So this is the outline for today.

6
00:00:33,610 --> 00:00:42,310
We're going to review the data that we're working on, what survival analysis, and then we're going to mainly focus on two sample testing.

7
00:00:44,380 --> 00:00:48,760
Okay. So remember that this is our notation for survival.

8
00:00:48,760 --> 00:01:01,180
So E corresponds to like one, two and the finding of the number of individuals that I have the find the so I is the time of event going to be my c,

9
00:01:01,180 --> 00:01:07,540
i is the sensing time and it's r is the minimum between these two x is what I observe.

10
00:01:07,690 --> 00:01:13,480
Okay. So for every individual I either have B or C, but I kind of have both.

11
00:01:14,680 --> 00:01:20,079
And the variable that is telling me which one do I have is this delta I which takes value

12
00:01:20,080 --> 00:01:26,799
of one is what happened for that individual is the event and zero that happened for if what

13
00:01:26,800 --> 00:01:35,110
happened to that individual is the center in time okay we review it is assigned this is just

14
00:01:35,120 --> 00:01:40,690
like basically how we estimate survival probabilities based on the couple of mer estimated.

15
00:01:41,260 --> 00:01:48,330
Okay so the dataset we're working with, it has 25 patients diagnosed with me alone.

16
00:01:48,340 --> 00:01:53,229
My process, which is that type of cancer, we have four variables.

17
00:01:53,230 --> 00:01:56,640
The first variable is the duration, which is our x i.

18
00:01:56,980 --> 00:02:02,950
That's the time we observe for each individual. We have a status which is our delta eye,

19
00:02:03,250 --> 00:02:07,479
which is the binary variable that is indicates like it's the indicator variable

20
00:02:07,480 --> 00:02:14,140
that is telling us whether that individual has event or a sensory type tree,

21
00:02:14,260 --> 00:02:19,120
which is, let's say I randomize these 25 people into two groups.

22
00:02:19,120 --> 00:02:25,040
One is from Job A and the other one is for drug B and then you bring out this [INAUDIBLE]

23
00:02:25,270 --> 00:02:29,260
call variable that it's like telling me the renal function for that patient.

24
00:02:30,730 --> 00:02:35,080
Okay, so this is how our dataset looks like.

25
00:02:35,740 --> 00:02:45,520
This is the third time. The second column is the indicator variable, the, the treatment assignment, and then finally the renal function.

26
00:02:45,520 --> 00:02:56,860
So for example, for individual number five, I followed that person for 52 units of time and of the 50 like 52 units of time,

27
00:02:56,860 --> 00:03:04,630
I know that person die either was assigned to the treatment one and it had one in there like renal function.

28
00:03:06,310 --> 00:03:14,020
But I could also like have this individual here. So individual number four, I follow them for 852 units of time.

29
00:03:14,350 --> 00:03:20,440
And in that time the event didn't happen. So like that, so centering time, it's a zero in this indicator variable.

30
00:03:20,830 --> 00:03:25,300
It was assigned to treatment. One on the renal function is zero for that individual.

31
00:03:26,440 --> 00:03:29,680
Do we have questions about how to read this data set?

32
00:03:31,600 --> 00:03:46,040
Okay. So last time we looked at these code which in our is telling us how to like plug our survival curve so we use for up lifespans here.

33
00:03:46,140 --> 00:03:50,740
Data corresponds to the data that I just showed you on here in floods.

34
00:03:52,210 --> 00:03:57,820
I have two parts in this plot. So the first one that is called survival.

35
00:03:59,210 --> 00:04:02,170
This is what is going to give us this survival glut.

36
00:04:02,390 --> 00:04:07,970
So, like, if you look survival in the bottom, this is off, the lot is immediately going to give you the survival.

37
00:04:08,510 --> 00:04:14,120
And important as you see, the survival is c l which is going to give us the confidence intervals.

38
00:04:14,630 --> 00:04:24,560
And then there is another one that is that is called log logs that we will use after to check for the proportional hazard assumption.

39
00:04:24,560 --> 00:04:32,600
Okay. So the first one is going to give you the survival log. The second one is going to give you a upload that is useful to check for the.

40
00:04:34,520 --> 00:04:38,930
Proportional hazard assumption. And then here we have.

41
00:04:42,870 --> 00:04:50,940
Oh. So we have the Istanbul areas, we have our normal characteristic model in this case is time.

42
00:04:51,450 --> 00:04:58,379
It's estimate the survival function of like my observed time, which is called duration in my data.

43
00:04:58,380 --> 00:05:09,390
So it finds this practice which is my then thigh, my indicator variable, and then give me one survival function for each treatment.

44
00:05:09,930 --> 00:05:13,560
I'm sorry for each level in the tree. So that's what I'm doing here.

45
00:05:13,590 --> 00:05:20,510
It's not that tree. What is going to happen is going to estimate one survival function is when I estimate two survival functions.

46
00:05:20,520 --> 00:05:24,030
I'm sorry, one for the people with three equal one.

47
00:05:24,030 --> 00:05:29,219
So they were assigned to treatment one and another one for people with three equals zero,

48
00:05:29,220 --> 00:05:32,430
which is like they were assigned to the other treatment group.

49
00:05:33,450 --> 00:05:37,529
Any questions so far? Okay.

50
00:05:37,530 --> 00:05:40,709
But the output looks something like this.

51
00:05:40,710 --> 00:05:44,190
So the first look we have is the survival club.

52
00:05:44,910 --> 00:05:48,750
As you can see, we have two lines, one for each treatment group.

53
00:05:49,080 --> 00:05:57,090
And then these like colon areas represent the confidence intervals for each of their lines.

54
00:05:57,600 --> 00:06:08,580
As you can see, if we just like analyze, explored ideology always, I would say that without doing any tests,

55
00:06:08,730 --> 00:06:12,630
which is something that we'll do later today without doing any tests.

56
00:06:13,350 --> 00:06:18,090
It seems like treatment one, it has the better survival probabilities.

57
00:06:18,120 --> 00:06:23,160
Right. Because it's like, well, more to the one and more to the zero.

58
00:06:23,460 --> 00:06:29,850
So that's why like between Jimmy, why don't you try to. It seems like treatment one is giving the best survival probabilities.

59
00:06:30,120 --> 00:06:33,660
But I have to test. Okay.

60
00:06:36,360 --> 00:06:43,920
This flood may be a little hard to understand because of this shade of lions shaded areas which are the confidence intervals.

61
00:06:45,210 --> 00:06:48,900
If you run the same code, this is exactly the same as before.

62
00:06:48,900 --> 00:06:52,590
The only difference is the PR instead of survival.

63
00:06:52,980 --> 00:06:58,410
Find this parentheses c l. I just have an s is going to give you a base.

64
00:06:58,650 --> 00:07:03,180
Read out the confidence intervals, but by timing is exactly the same plot.

65
00:07:04,170 --> 00:07:11,910
Okay. So we base what we are interested now is in the following test.

66
00:07:13,350 --> 00:07:18,420
So we want to know whether those survival curves are the same.

67
00:07:18,750 --> 00:07:24,420
So I want to test whether that survival probability for the first three group is the

68
00:07:24,420 --> 00:07:29,040
same as the survival probability for the second treatment group versus the alternative.

69
00:07:29,040 --> 00:07:30,300
That is like they are different.

70
00:07:30,870 --> 00:07:42,120
There are a couple of ways to test for this hypothesis that first tells us we're going to do is the log rank test on this test.

71
00:07:42,600 --> 00:07:46,410
Like in terms of the code in SAS, you don't have to do anything.

72
00:07:47,560 --> 00:07:56,230
If you include a strata tier like if you include these 90 years call it is going to automatically give you the test.

73
00:07:56,400 --> 00:08:02,320
Okay. So in terms of code, you don't have to do anything else. No road test.

74
00:08:03,280 --> 00:08:09,570
It's a powerful test. Eve, the proportional hazard assumption is met.

75
00:08:09,870 --> 00:08:17,280
Okay, so before assessing, like whether I should use this test or another test,

76
00:08:17,580 --> 00:08:22,350
the first thing that we should actually do do is to check for the proportional cause.

77
00:08:22,360 --> 00:08:28,440
Our best option. If the proportional hazard assumption is like make use that log run best.

78
00:08:28,740 --> 00:08:33,240
If it is not, use the wilcoxon test which is like Little League below.

79
00:08:33,510 --> 00:08:40,660
Okay. Does that make sense? So these first their first test is very powerful.

80
00:08:41,230 --> 00:08:49,930
If we have proportional hazards, if we do not have proportional hazard, but we still want to test for that, the news there will come of.

81
00:08:51,080 --> 00:08:59,110
Okay. How do we read this test? So if I do the rank test, this is the test.

82
00:08:59,250 --> 00:09:07,090
So this thing, this is the degrees of freedom. And then this is the bottom because we see your point 25, which is greater than 5%.

83
00:09:07,360 --> 00:09:11,290
So I fail to fail to reject the null hypothesis.

84
00:09:11,590 --> 00:09:17,620
If I fail to reject its means that for me the null hypothesis is quote unquote true.

85
00:09:18,820 --> 00:09:22,660
So I still believe that those two survival.

86
00:09:25,150 --> 00:09:29,860
Curves are the same, okay. Even though they don't seem like they are the same.

87
00:09:30,460 --> 00:09:31,070
Look at that.

88
00:09:31,150 --> 00:09:40,660
Like they don't seem like very much the same, especially like here, because clearly this is like close to 0.5 and these other line is decreasing.

89
00:09:41,080 --> 00:09:47,520
If we notice, like if we look at the beginning of the plot, they behave very similarly.

90
00:09:49,500 --> 00:09:52,920
Okay. So that's what's happened. What is happening here?

91
00:09:53,340 --> 00:10:00,059
However, to like be confident about these result because I'm using the log run test,

92
00:10:00,060 --> 00:10:09,240
I should test for the proportion of class work assumption because if the assumption is not met that I then I should use the wilcoxon fest.

93
00:10:09,810 --> 00:10:14,580
So next is really check for the proportional hazards assumption.

94
00:10:15,120 --> 00:10:20,520
So to check for the proportional hazard assumption, we are going to use something that is called the log log.

95
00:10:22,680 --> 00:10:30,480
Log of logs plot. Okay.

96
00:10:30,780 --> 00:10:37,230
So first of all, insert the second argument in the prop life past.

97
00:10:37,440 --> 00:10:42,720
L l s is going to give you the love of logs.

98
00:10:44,400 --> 00:10:53,230
Clear that we need. So three Ferraris, very similar to the other one.

99
00:10:53,240 --> 00:11:01,250
It's like normally in survival. Our x axis is tiny and our y axis is survival probability.

100
00:11:01,610 --> 00:11:10,910
Here I'm using log of the time in in the x axis and in the y axis I'm using log of negative log of the survival.

101
00:11:11,000 --> 00:11:23,840
So it's just a transformation of the data. E the and we get like when we plot this, we again we got two lines, one line for each treatment group.

102
00:11:24,610 --> 00:11:27,500
If the proportional hazard assumption is met.

103
00:11:27,860 --> 00:11:35,480
Then I would expect to have two lines that are not crossing between each other and they are mainly parallel.

104
00:11:35,810 --> 00:11:48,740
Okay, if the proportional hazard assumption is met, I would expect to have two lines that are not crossing and they are like kind of parallel here.

105
00:11:49,190 --> 00:11:52,340
There are many crossings. So for example.

106
00:11:53,920 --> 00:12:02,229
Oops. So for example,

107
00:12:02,230 --> 00:12:11,530
there's crossing here and crossing here and crossing here because there are many crossings and clearly these two lines are not parallel,

108
00:12:11,890 --> 00:12:21,190
then that's enough evidence for me to say, okay, maybe these data does not have proportional to our assumption,

109
00:12:21,190 --> 00:12:25,780
okay, the hazards are not like proportional to each other between the two groups.

110
00:12:26,320 --> 00:12:34,299
In that case, if I still want to test whether the survival of one, it's equal to the survival.

111
00:12:34,300 --> 00:12:44,980
Two. So if I want to test for D and the proportional hazards function is not met, then I should use the wilcoxon test.

112
00:12:45,410 --> 00:12:49,360
Okay. There is no insults.

113
00:12:49,360 --> 00:12:56,530
There is no additional code. Again, if you use a straight up in the life test is immediately going to give you this.

114
00:12:57,790 --> 00:13:05,830
The second row is there will come some tests. I have the highest point to test the spacing, the degrees of freedom and the p value.

115
00:13:06,250 --> 00:13:09,580
Here we can see that the p value is the same greater than 5%.

116
00:13:09,880 --> 00:13:15,490
So my conclusion is like there is like I failed to reject the null hypothesis.

117
00:13:16,150 --> 00:13:24,430
Patient receive drug drug one were observed to have is likely greater but not a statistically significant survival over time.

118
00:13:25,930 --> 00:13:33,760
So if again, if I go to my survival plus the history of my life, they seem to have a higher survival plot.

119
00:13:33,970 --> 00:13:37,210
However, this difference is not a statistically significant.

120
00:13:37,610 --> 00:13:43,900
Okay, that's what is happening here. Any question so far?

121
00:13:47,160 --> 00:13:57,210
Okay. Good. Lastly, we are going to review something that is called the comparison of restricted means.

122
00:13:58,500 --> 00:14:05,250
So far restricted me only average events during our restricted follow off period of time passed.

123
00:14:05,610 --> 00:14:10,770
So what we want to do is here I'm going to go back to my survival plot.

124
00:14:11,850 --> 00:14:12,180
Okay.

125
00:14:19,670 --> 00:14:32,030
So what I want to do with the title restricted means graphically is I'm going to oops here, I'm going to define a power which is like a time frame.

126
00:14:32,330 --> 00:14:36,230
Let's say I'm one of the five these to be 500.

127
00:14:36,500 --> 00:14:41,930
So this is like user defined. I define. And what I want to do is.

128
00:14:43,010 --> 00:14:49,580
Cross a line in this 500, like a vertical line, which is like my power value.

129
00:14:50,660 --> 00:14:57,139
And then what I want to do is between the start of the follow up period, which is Time zero and Michael,

130
00:14:57,140 --> 00:15:04,910
which is in this case, I chose the value to be 500 because randomly I chose W, but it can be anything.

131
00:15:05,720 --> 00:15:18,950
So between the follow up period and the Taliban view, I want to assess the mean survival time for each group, the mean survival time for each group.

132
00:15:19,400 --> 00:15:28,090
So for example, and you already know that the mean survival time between these two is like the area.

133
00:15:28,820 --> 00:15:32,330
I'm like, it's the same, but what does that mean?

134
00:15:32,570 --> 00:15:43,160
Like in general, for example, let's say I pick group number two and I estimate that the power restricted mean power restricted mean is.

135
00:15:45,640 --> 00:15:51,220
I don't know, 380 so I estimated this value I'm is 380.

136
00:15:51,550 --> 00:16:07,360
What this value means is that in the first 500 units of my follow up period for the second group, the expected survival time is 380 units of time.

137
00:16:08,230 --> 00:16:16,690
Okay. So that's what it means in the first all units of time, in the first 500 units of a little time for group number two,

138
00:16:16,690 --> 00:16:20,130
because I'll do that for each of the groups, for people in the group.

139
00:16:20,140 --> 00:16:24,580
Number two, I would expect them to survive 380 units of time.

140
00:16:25,150 --> 00:16:30,410
Does that make sense? That is the. That's what we want to estimate.

141
00:16:31,460 --> 00:16:34,130
Okay. So how do we estimate that?

142
00:16:34,640 --> 00:16:44,790
So to estimate that there are like two caveats that we just need to take care of it, then that's the by the data set.

143
00:16:44,840 --> 00:16:49,250
It is an absurd, absurd failure time.

144
00:16:49,730 --> 00:16:58,340
Then the Kaplan-Meier score jumps to zero at the last event time and valuably they find out all follow up times.

145
00:16:58,760 --> 00:17:06,700
So what that means is that if the last event that I have in my dataset is a failure time, we are good to go.

146
00:17:06,920 --> 00:17:11,660
We don't have to care about anything else. That problem comes in the other way around.

147
00:17:12,410 --> 00:17:24,380
If the last event in the dataset is a censored value, then the estimate, the kaplan-meier estimate does not have a valid estimate beyond this value.

148
00:17:25,040 --> 00:17:32,270
So when I'm picking the Tao because remember that to restrict that means you have to pick up Tao number.

149
00:17:32,970 --> 00:17:36,380
That's the user. That's for the user to decide.

150
00:17:36,920 --> 00:17:39,200
So I just have to be careful of this.

151
00:17:39,650 --> 00:17:47,000
If the last event in my data set its unobserved failure, then I don't have any restriction about what that Tao can be.

152
00:17:47,300 --> 00:17:58,370
But it can be. Just let it be. However, if the last event in the dataset is a censored value, then that Tao can be maximal these like last time event.

153
00:17:59,360 --> 00:18:03,650
The style can be maximum these less time a bit. So just be careful of that.

154
00:18:03,810 --> 00:18:06,410
That's like a little caveat to define like Tao.

155
00:18:06,650 --> 00:18:12,410
But other than that, you can define it like it can be any number, but it's meaningful in your research.

156
00:18:14,720 --> 00:18:18,040
Okay. So that's the first thing we should.

157
00:18:18,140 --> 00:18:24,290
Yes. The second one if it it's uncertain and it can only be the facts of that time.

158
00:18:24,300 --> 00:18:27,920
Events, you can go underneath that. Yes, exactly.

159
00:18:28,770 --> 00:18:32,000
It does it does not have to be. The math is just like the maximum values.

160
00:18:32,300 --> 00:18:36,650
It can be anything like below the number. Okay.

161
00:18:36,890 --> 00:18:42,080
So the first thing I should check is what's the maximum value that I can choose for the two groups?

162
00:18:42,620 --> 00:18:51,860
So my first is going to give you the life test tables and you should check for each of the treatment groups, which is like the top.

163
00:18:52,400 --> 00:18:55,610
So in this case, I go to the last observation.

164
00:18:55,610 --> 00:19:25,830
The last observation is here. Okay.

165
00:19:26,250 --> 00:19:32,270
I'm sorry. So. For the first.

166
00:19:33,440 --> 00:19:39,710
So you get these still tables. If you do a start up in life, in a different life space, you will get this two tables.

167
00:19:40,490 --> 00:19:47,060
So what to dig for this one in the first table? And this is going to indicate like maximum value 220.

168
00:19:47,510 --> 00:19:52,100
And if you go for the second one, this is going to indicate the maximum value to you.

169
00:19:52,340 --> 00:20:00,260
Okay. You can check these actually using these tables or just basically like looking at your dataset.

170
00:20:06,580 --> 00:20:12,350
Okay. Hmm. All right.

171
00:20:12,470 --> 00:20:16,120
So now how do we, like, estimate those values in SOPs?

172
00:20:16,580 --> 00:20:24,800
In SaaS, the code is mainly the same, so I'm still using profit from life that I have import.

173
00:20:25,250 --> 00:20:31,910
That's the first thing. So imports notice that instead of using survival, I'm using error esp s.

174
00:20:32,390 --> 00:20:41,240
So it's not only as these errors are MSDS And then here I'm setting that restrictive,

175
00:20:41,240 --> 00:20:45,740
mean, restrictive mean of survival times thought to be equal to 200.

176
00:20:46,160 --> 00:20:50,240
So like in this case, I just chose out to be 200.

177
00:20:51,620 --> 00:20:54,229
What does that change in the code? First of all,

178
00:20:54,230 --> 00:21:04,610
I need to use like these restricted mean survival time before the survival flood here and then here is specify what that value means to me.

179
00:21:04,610 --> 00:21:08,569
In this case is just 200. The code is exactly the same.

180
00:21:08,570 --> 00:21:19,460
So I have my time is duration versus a status. I have a strata and then here I'm including beef equal all because I want to test for it.

181
00:21:20,420 --> 00:21:30,559
This is to give me a test. So what I want to do is let's say, like what is happening here is like if I go back to my like example,

182
00:21:30,560 --> 00:21:42,170
I have my call 500 I can estimate the restrictive mean for one you take from one group it's 380 and I can also estimate it for the second,

183
00:21:42,470 --> 00:21:45,560
let's say for the second group is 403.

184
00:21:46,700 --> 00:21:51,890
Now, what I want to do is just to test whether these two values are like equal or not.

185
00:21:51,950 --> 00:21:58,999
Taking into account that since this is an estimate, I have an estimate and understand that error around that estimate.

186
00:21:59,000 --> 00:22:03,140
So a lot of times for like whether those two values are the same makes sense.

187
00:22:04,470 --> 00:22:20,150
Okay. So if I include in source backslash, they've equal all, then I'm computing those those things now I'm computing the rest of.

188
00:22:21,310 --> 00:22:28,000
So the first thing is like when you run the code code, you're going to have a table that looks something like this.

189
00:22:29,020 --> 00:22:36,160
So this is the restricted mean survival time estimates you have for each of your groups,

190
00:22:36,550 --> 00:22:42,400
the estimate of the average survival time and the standard error.

191
00:22:42,700 --> 00:22:47,230
So for example, again, how do I interpret this 132?

192
00:22:47,530 --> 00:22:53,139
That means that for the first 200 units of time of my follow up,

193
00:22:53,140 --> 00:23:00,550
because remember that I set out to be 200 the mean survival time for the people in the treatment group.

194
00:23:00,570 --> 00:23:03,730
One, it's 132. Okay.

195
00:23:04,270 --> 00:23:11,140
That's how I interpret that. But I want to check for and of course, you interpret the second value.

196
00:23:11,670 --> 00:23:17,500
Similarly, I want to test for this hypothesis whether those two numbers are the same or not.

197
00:23:18,010 --> 00:23:25,160
And the test statistic is based one so is in this case is the estimate for the first mean.

198
00:23:25,210 --> 00:23:30,520
So these value will be, for example, the 132.8.

199
00:23:32,980 --> 00:23:36,910
These will be 136.5.

200
00:23:40,670 --> 00:23:46,640
The standard error. So this one's come from the first table.

201
00:23:46,910 --> 00:23:56,390
So the first the standard error is 23.4 and I need to elevate.

202
00:23:56,780 --> 00:24:07,729
And then here I also do 21.7 and then a square and then I take the, I guess the number.

203
00:24:07,730 --> 00:24:10,880
So I take the road and I do nominative versus denominator.

204
00:24:12,230 --> 00:24:17,840
Importantly, the vs estimate respects this, but we think follows our normal distribution,

205
00:24:18,170 --> 00:24:24,559
which means that you can compare it with the 95% first and follow the normal distribution.

206
00:24:24,560 --> 00:24:35,450
Actually, you can compute the 95.90 7.5% and then compare or that's one way to do it.

207
00:24:35,840 --> 00:24:48,600
Or the second way is item. I don't know if you guys remember, but if I take this square of a normal distribution, I get a chi square random variable.

208
00:24:48,990 --> 00:24:55,260
So the other way that you can do it like this is the first one, just like do this and then compare it to a normal distribution.

209
00:24:55,590 --> 00:25:02,430
Or the second way to do it is like elevate these value a square and then instead of

210
00:25:02,430 --> 00:25:07,380
comparing to a normal distribution compared to a square with one degree of freedom,

211
00:25:07,770 --> 00:25:10,620
it should be the same. It should be exactly the same.

212
00:25:10,620 --> 00:25:15,240
The conclusion, I mean, the values are clearly not going to be the same, but the conclusion should be this.

213
00:25:16,860 --> 00:25:19,530
Okay. Any questions so far?

214
00:25:25,180 --> 00:25:38,860
So I would recommend that you try these by hand and then confirm your value here with the table sources giving you the Chi Square bar level.

215
00:25:38,860 --> 00:25:43,129
So it's doing this one here. So it's giving you the test.

216
00:25:43,130 --> 00:25:49,420
It's 46 and then the p value. And then based on the value, you can conclude whether those means are the same or.

217
00:25:51,530 --> 00:25:59,040
Any questions so far? Oh, great.

218
00:26:00,660 --> 00:26:10,229
That's on us. So if I go back way back to our date.

219
00:26:10,230 --> 00:26:21,000
Yes. So until now, I have been able to compare the survival curve for the we using the treatment variable.

220
00:26:21,000 --> 00:26:29,670
Right. So I have one survival for the people in one treatment group and then another survival with the people in the second treatment group.

221
00:26:29,670 --> 00:26:34,860
And I want to compare them. But I could also do that using the renal function.

222
00:26:34,860 --> 00:26:42,900
Right? Or if I had another covariate, I could also do the same with the values of another covariate, let's say, for example, sex.

223
00:26:43,980 --> 00:26:51,900
So what I want to do is like you in that case, let's say I tell you, okay, repeat the analysis.

224
00:26:51,900 --> 00:26:55,770
But instead of like using three, use the random variable.

225
00:26:56,100 --> 00:26:58,200
So you have to like run everything again.

226
00:26:58,770 --> 00:27:08,100
So in SAS and this is only for SAS users if you are not a user, this probably is not going to be a interest in for you.

227
00:27:08,670 --> 00:27:13,140
You can create a SAS macro. I saw as my brain, my head is like a loop.

228
00:27:13,380 --> 00:27:20,220
So you are going to tell SAS to like run a chunk of code a couple of times.

229
00:27:20,570 --> 00:27:25,620
That's what is doing. So my chunk of code is the following.

230
00:27:26,040 --> 00:27:34,770
I have my proc life test. I want to change the variables that goes in the structure to like any other variable in my dataset.

231
00:27:35,130 --> 00:27:42,300
So what I'm going to do is like instead of having here the name of my variable, I am going to use first these.

232
00:27:45,480 --> 00:27:52,980
Symbol and then use any name. In my case, I'm using predictor name and that's going to be like my variables do change.

233
00:27:53,370 --> 00:27:56,639
And this those macro words like you have the marker here.

234
00:27:56,640 --> 00:28:00,540
So you start a macro, you want the macro here in between the lines,

235
00:28:00,540 --> 00:28:07,440
you have the code and then in parentheses you have the variable that you want to change on how those works.

236
00:28:07,530 --> 00:28:10,950
So my macro, like the name of my letter is called text.

237
00:28:11,340 --> 00:28:20,879
So I can call my macro. The way I'm calling my macro is always like using the percentage because that's how macros work itself super fast.

238
00:28:20,880 --> 00:28:24,720
It's calling my macro that is called text and if I give it,

239
00:28:25,110 --> 00:28:31,409
then what is going to happen is that is when I run these code on here is going out like instead of

240
00:28:31,410 --> 00:28:36,270
having predictor name is going to have st so he's just going to do the analysis for the three variable.

241
00:28:36,630 --> 00:28:40,620
But if I have the following, so for example,

242
00:28:40,770 --> 00:28:49,140
test I'm in front of this is I have right now so it's going to run this code is the same prop like this but here I have the rental variable.

243
00:28:51,210 --> 00:28:56,100
I know this may not be like useful if we only have two covariance.

244
00:28:56,400 --> 00:29:00,840
I understand that, but you probably will have more than two covariance.

245
00:29:00,840 --> 00:29:08,220
Like in a real data set you have like 57 B many, many, many variables,

246
00:29:08,700 --> 00:29:13,830
especially like you are interested like in genetics, they use a lot of a lot, a lot of covariance.

247
00:29:14,130 --> 00:29:20,040
So they may be handy in that case. Okay. And that's it.

248
00:29:20,490 --> 00:29:24,630
That's everything you need to know about survival.

249
00:29:25,970 --> 00:29:32,480
Yes. About like this type of survival, because then you're going to review Cox models for the homework.

250
00:29:33,410 --> 00:29:42,340
Now, the big question is, if I'm using art, how do I do it? Okay, so remember now that first you need to install the survival.

251
00:29:45,350 --> 00:29:49,040
Everything we're doing, it's using the package. Survival.

252
00:29:50,090 --> 00:29:55,930
So installed package is survival. And then call library survival into your own script.

253
00:29:56,690 --> 00:30:00,530
Okay, so how do I feed our survival function?

254
00:30:00,980 --> 00:30:04,540
I'm going to use the of feed function. Okay.

255
00:30:05,300 --> 00:30:13,380
And the formula that I'm using. Is the same one as we use less class.

256
00:30:20,110 --> 00:30:27,210
So I have here my survival formula is estimate the survival probability given the times.

257
00:30:27,230 --> 00:30:33,490
These are the observed times. Unlike the indicator variable, whether like an individual is there or censoring time.

258
00:30:34,000 --> 00:30:38,200
I do that for all of the levels of my three viable.

259
00:30:39,290 --> 00:30:49,790
Okay. And then tied Coplan may estimate the extent that errors that we are using are the Greenwood and then just data, the data frame,

260
00:30:49,790 --> 00:30:55,640
but you improve your data and then the summary of your survival object here,

261
00:30:56,060 --> 00:31:04,970
summary of your survival data here that this summary of the data is going to look something like this.

262
00:31:05,330 --> 00:31:10,159
So you have like a table for each of the levels of the treatment variable.

263
00:31:10,160 --> 00:31:18,950
So at table for treatment one on a table for treatment two, on the table we read the table in the same way we did have less class.

264
00:31:19,250 --> 00:31:32,900
So for example, for people in the first three can group the survival, the probability of surviving, surviving after time 52 is 0.75.

265
00:31:33,290 --> 00:31:39,170
Okay, so, so on and so forth. These tables are not very interesting.

266
00:31:41,210 --> 00:31:44,210
They are just like easier to understand if we plug them.

267
00:31:44,780 --> 00:31:54,200
So to plug them just use become like the function lot from base r so function plot and then your

268
00:31:54,200 --> 00:32:02,810
survival function that you feed it like in the last page which is full survival data to look here.

269
00:32:05,510 --> 00:32:08,540
And that's it. These things.

270
00:32:09,380 --> 00:32:14,480
So these showed the censoring data. So like you see here we have some vertical bars.

271
00:32:14,900 --> 00:32:21,080
Those vertical bars up here because they help mark the time equal to if you don't want them, just take that out.

272
00:32:21,920 --> 00:32:27,080
You can change the levels. So you can change where it goes, here or here.

273
00:32:27,470 --> 00:32:33,290
And then these things are changing, like the types of lines that you displayed for each of the groups.

274
00:32:33,680 --> 00:32:37,910
Okay. And we can also change the legend.

275
00:32:38,960 --> 00:32:52,760
So to appear here, maybe here or maybe here, like this part is just like literally displaying what goes in the legend.

276
00:32:53,240 --> 00:33:00,880
And then you can just change the text size, the text size on the different line types.

277
00:33:02,050 --> 00:33:06,100
Okay. That's one way to do it.

278
00:33:06,790 --> 00:33:09,370
There is another way to do, like, love the same thing.

279
00:33:09,550 --> 00:33:20,080
There's actually many, many packages built in art to plot survival curves with the base one, which is like just using base R.

280
00:33:20,500 --> 00:33:26,830
And there's another one that you can use that is like very commonly used but is sort of minor.

281
00:33:27,400 --> 00:33:31,150
Okay. You don't have to do this if you don't want to.

282
00:33:32,440 --> 00:33:42,670
This is just optional. So libraries of minor like please remember that before that you need to install the package.

283
00:33:43,120 --> 00:33:50,200
So then my version is so minor and then here I can use that function.

284
00:33:50,530 --> 00:33:54,040
Did you serve plot? And then this is my survival object.

285
00:33:54,490 --> 00:33:58,510
And if I do that, then I have the following graph.

286
00:34:00,070 --> 00:34:03,820
Does it does anything the event that the banks are? No, it does not.

287
00:34:03,820 --> 00:34:08,740
Is the same plot is just like a manner of like how do you like your plots?

288
00:34:09,010 --> 00:34:13,570
Like this is very colorful and you don't have like the grid on the background, the other one you have.

289
00:34:13,900 --> 00:34:17,530
But like, I mean, it's completely up to you. Which one do you want to use?

290
00:34:19,360 --> 00:34:22,600
Okay, so now the very important thing.

291
00:34:22,990 --> 00:34:26,710
So I want to test whether those survival cards are the same or not.

292
00:34:27,070 --> 00:34:34,750
I can use the low growth F if they are proportional has our assumption is met or there will come some test in the other case.

293
00:34:35,230 --> 00:34:39,480
So for the long run test we are going to use the function.

294
00:34:39,490 --> 00:34:46,150
So the if this is the same formula that we have been using so far.

295
00:34:46,510 --> 00:34:50,299
So I want to estimate the survival given this time.

296
00:34:50,300 --> 00:34:57,580
So I mean, this indicator functions using the trick variable and then this is my data frame.

297
00:34:57,580 --> 00:35:03,340
And lastly, but the most important part is to specify raw equals zero.

298
00:35:03,670 --> 00:35:11,740
So Roy equals zero is going to give you the look run test and then 3.1 is going to give you the wilcoxon test.

299
00:35:12,040 --> 00:35:15,660
Same code just change 0 to 1. Okay.

300
00:35:15,880 --> 00:35:26,230
So buzzer log run test. If I run the code, the output looks something like this and here you have the your case quite like the test,

301
00:35:26,230 --> 00:35:31,240
the stuff you see value, the degrees of freedom and the p value and that's it.

302
00:35:31,900 --> 00:35:35,410
You can see you have these values exactly the same as the one that we had in SAS.

303
00:35:35,830 --> 00:35:42,250
Okay, just remember that equal equals zero will give you the log from test.

304
00:35:44,890 --> 00:35:51,850
We discussed that this test I can use it and is more powerful if the proportional partner assumption is met.

305
00:35:52,150 --> 00:35:54,580
So next we are going to check for that assumption.

306
00:35:54,940 --> 00:36:06,730
To check for that assumption, we use the log of log plot to plot that in our use the same code plot that we used before.

307
00:36:07,090 --> 00:36:11,110
The only difference is that you're going to add the following line line.

308
00:36:12,070 --> 00:36:21,220
So you're going to on phone, which stands for function equals two, and then you put that in quotes, see, log off.

309
00:36:21,490 --> 00:36:25,750
Okay. So it's going to give you the log of logs and that's it.

310
00:36:26,110 --> 00:36:31,130
If you do that and you run the code, the rest is like basically the same IP.

311
00:36:31,190 --> 00:36:34,540
I'm just changing the labels, but the rest is basically the same.

312
00:36:34,870 --> 00:36:41,380
You get the same load that we had in SAS. Okay, so just make sure to like odd font equals, you know, here.

313
00:36:43,050 --> 00:36:48,240
Okay. We can see that there are many crossings and the lines are not parallel.

314
00:36:48,240 --> 00:36:56,040
So. So we kind of use the log rank first.

315
00:36:56,060 --> 00:37:07,670
We should use that with content best. So oh, before that, if you're using this sort of minor library, you can also get the log of logs.

316
00:37:08,420 --> 00:37:12,110
Same code, just odd fun. You can see Lola.

317
00:37:12,320 --> 00:37:18,190
Okay. You can use. You can use until this sort of minor welcome.

318
00:37:18,320 --> 00:37:22,340
So that is the same code as the log run test.

319
00:37:22,490 --> 00:37:25,760
The only difference is here, row equals one.

320
00:37:26,240 --> 00:37:33,230
Okay. Everything else stays the same. Just make sure you have really quick one that will give you the week on some test.

321
00:37:33,590 --> 00:37:37,489
Okay. The output log.

322
00:37:37,490 --> 00:37:41,540
Something else you have your test is that basic figures of freedom value.

323
00:37:43,520 --> 00:37:50,780
Yeah. P value. Here the p values is like the difference that we observe in SAS.

324
00:37:51,440 --> 00:37:59,780
Just be aware that if like you're comparing answers with your classmates, that's possibly due to how R is coded.

325
00:38:01,400 --> 00:38:06,920
But I'm not sure. Like if I have to bet is because of how are you?

326
00:38:07,910 --> 00:38:12,060
But I mean. Even though they are different, they are not that different.

327
00:38:12,300 --> 00:38:18,140
You are failing to reject them both in either source or.

328
00:38:19,440 --> 00:38:24,420
Okay. And then finally, how do we compute the restricted means?

329
00:38:25,980 --> 00:38:29,000
So here. Couple of things.

330
00:38:31,010 --> 00:38:36,830
First one, we are using the brain function is going to race here.

331
00:38:37,190 --> 00:38:44,870
This is this sort of data. So this is the estimated survival probability object that we did at the very, very beginning.

332
00:38:45,740 --> 00:38:50,120
And then I'm just going to add brained that rested the mini courtroom.

333
00:38:50,720 --> 00:38:52,040
So I have the restricted means.

334
00:38:52,370 --> 00:39:01,910
And after that establish you talk about you are mean equal to hundred in this case meaning that my Talabani we still have it and then they.

335
00:39:02,300 --> 00:39:05,300
This is just to give me like how many pages do I have?

336
00:39:05,300 --> 00:39:08,390
Like awesome notation thing.

337
00:39:09,890 --> 00:39:12,260
The output looks something like this.

338
00:39:12,890 --> 00:39:19,850
So you have the power restricted that mean for each of the groups you have the standard error and more importantly.

339
00:39:28,290 --> 00:39:33,420
Oh, yeah. And then you have your standard error, and then you can calculate the paste by hand.

340
00:39:34,290 --> 00:39:46,980
So for 132, -106 divided by the root of 23.4 squared close to you, 1.7 a square.

341
00:39:47,280 --> 00:39:50,460
And this is your normal C0 one.

342
00:39:50,700 --> 00:40:01,080
Or I could square these value, which is going to be -0.1 12 for square and then compare that to a square on the file.

343
00:40:01,260 --> 00:40:05,040
Okay. You can do either of them. Okay.

344
00:40:06,900 --> 00:40:10,049
There is some way to load me.

345
00:40:10,050 --> 00:40:19,800
And this is only for our users. There is some way to perform the power restricted means like test in R, but you have to install the new library.

346
00:40:21,600 --> 00:40:25,860
So like in D using the brain function.

347
00:40:25,980 --> 00:40:33,600
As you can see, you have to calculate the. Like it's a static and the p value by hand.

348
00:40:33,660 --> 00:40:41,580
There's no other way to do it. If you want like ah to do it like a function that building r to do it you can use

349
00:40:41,580 --> 00:40:47,310
the following package so you can use sort of r m to like restrict that mean to.

350
00:40:47,790 --> 00:40:52,470
And then the only thing you need to be aware of is that in this function.

351
00:40:57,790 --> 00:41:07,330
In this function. My variable that is like dividing people into two groups can only be binary, meaning that can only be zero or one.

352
00:41:07,630 --> 00:41:12,040
It cannot be four or five or three or two or one or two.

353
00:41:12,070 --> 00:41:16,450
It can only be your one. But the only caveat here.

354
00:41:16,900 --> 00:41:21,210
So what I'm doing here is if we go very, very box.

355
00:41:24,140 --> 00:41:27,740
If we go back, Friedman is quoted one and two.

356
00:41:28,100 --> 00:41:33,410
So I need to record that because right now that function is going to give me an error.

357
00:41:33,710 --> 00:41:37,640
So what I'm going to do is I'm going to change all of this, those to be zero.

358
00:41:38,030 --> 00:41:49,520
That's what I'm doing. My way is going to be like zero one. So that's what I'm doing here.

359
00:41:50,080 --> 00:41:54,710
Oops. So that's what I'm doing here.

360
00:41:54,840 --> 00:42:06,380
I'm not recording that variable, so it's 0 to 1 and then I can use the function r r m sd to restrict the means are able to remain so I have.

361
00:42:07,700 --> 00:42:16,460
This is my observe time. This is my indicator variable like delta for either censoring or death.

362
00:42:16,940 --> 00:42:21,230
And this isn't like my new coded for treatment.

363
00:42:21,500 --> 00:42:24,860
And then I specify the tell and that's it.

364
00:42:25,190 --> 00:42:28,910
If you run these line, you will get something like this.

365
00:42:30,470 --> 00:42:36,290
So you will get these two tables. These are going to give you the estimates on based on the errors.

366
00:42:36,290 --> 00:42:46,309
And then you also have the estimated value for the difference.

367
00:42:46,310 --> 00:42:50,730
And then. Yeah. Estimated value for the defense.

368
00:42:51,060 --> 00:42:56,070
The 95% confidence interval of the difference on the very important value.

369
00:42:58,010 --> 00:43:03,350
The P value for the best these people is very, very similar to the one that we had in.

370
00:43:04,610 --> 00:43:09,660
All right. And that's it. Any questions so far?

371
00:43:12,650 --> 00:43:15,800
All right. Well, that's everything you need to know about survival.

372
00:43:15,830 --> 00:43:17,510
Until now, find out what.

