1
00:00:01,320 --> 00:00:07,399
So my next. Okay.

2
00:00:07,400 --> 00:00:22,969
Maybe you get started. So the final time for me to sit sit rather than stand in front and yeah, I have a, you know, in all the data set in the canvas.

3
00:00:22,970 --> 00:00:30,200
And we will listen to some of the details from all six teams and in today and Tuesday next week.

4
00:00:30,200 --> 00:00:34,969
And so we can understand better about the data, the social and environmental determinants,

5
00:00:34,970 --> 00:00:40,940
and so we can see what or not we are going to use some of these variables you collected to,

6
00:00:41,270 --> 00:00:49,790
you know, to look folder analysis of the, you know, either mortality or the vaccination situations in the country.

7
00:00:50,540 --> 00:00:53,989
So we have three teams to present.

8
00:00:53,990 --> 00:01:05,090
And also I include some of guidelines for the final project last night that you can take a look and and like to set up a meeting this year next

9
00:01:05,090 --> 00:01:16,600
week and email me what you want to to do for your phone project that I can listen and give you some suggestions and then before you start to work,

10
00:01:16,670 --> 00:01:18,110
how you'll find the project. Right?

11
00:01:18,110 --> 00:01:34,150
So and the presentation will be evaluated by four categories and slight quality of clarity at time management and, and handling questions.

12
00:01:34,160 --> 00:01:40,760
Right. So each team is given 20 minutes and 5 minutes Q&A.

13
00:01:40,760 --> 00:01:47,240
And so if you finish early, you don't use 20.8 minutes, then we can ask more questions.

14
00:01:47,720 --> 00:01:52,340
And so but each team would have 25 minutes for that.

15
00:01:52,460 --> 00:02:05,440
So so the first. Okay. So I presume chocolates and for the teams that face the presentation will receive white back of the chocolate.

16
00:02:05,800 --> 00:02:13,260
So anyway that's the gift that you besides the the other knowledge you receive.

17
00:02:13,270 --> 00:02:17,810
And so anyway first team is team two.

18
00:02:17,830 --> 00:02:22,980
Is that right? Okay. I'm fine.

19
00:02:23,400 --> 00:02:26,700
I can see it clearer. And there are lots of questions.

20
00:02:39,050 --> 00:02:43,260
Well, you know, you just here. Sorry.

21
00:02:52,500 --> 00:02:57,570
So who is going to time their presentation? I'm sorry.

22
00:02:57,720 --> 00:03:01,730
I know you don't have over. I hope so.

23
00:03:02,070 --> 00:03:05,550
So presentation might some time for the minutes then.

24
00:03:05,790 --> 00:03:11,060
5 minutes question. So you are the person who is going to all this reference.

25
00:03:11,400 --> 00:03:14,640
Okay. Are you going to forgive? Yeah. Well, we can time ourselves. Yeah.

26
00:03:14,700 --> 00:03:17,759
Yeah. Okay. Sure, I can. Time your presentation.

27
00:03:17,760 --> 00:03:23,560
Okay? Sure. So, yeah. Go ahead. Start her body today.

28
00:03:23,610 --> 00:03:30,180
You draw me and I will give a presentation about the capture of Florida Agricultural Organization data.

29
00:03:30,540 --> 00:03:34,260
So, first of all, I will give a brief introduction about the data source.

30
00:03:34,560 --> 00:03:41,880
So data was collected from the Rural Analysis of Economic Research Service by the US Department of Agriculture.

31
00:03:42,660 --> 00:03:47,460
The USDA is, as research was originally established in 1962,

32
00:03:47,760 --> 00:03:54,329
and the goal is to anticipate trends and emerging issues in agricultural food in our online reality,

33
00:03:54,330 --> 00:04:01,739
and to conduct some enormous research that could be used to help decision makers and researchers.

34
00:04:01,740 --> 00:04:07,890
We have the private or public decision making, and the main topics include agricultural economy,

35
00:04:08,130 --> 00:04:15,030
food and nutrition for safety, global markets and trade resources and empowerment and rural economy.

36
00:04:16,440 --> 00:04:22,739
So rural analysis is pretty important part of us and has resources and data to determine

37
00:04:22,740 --> 00:04:27,990
worldwide urban status and describe the socioeconomic conditions by county level.

38
00:04:28,650 --> 00:04:37,350
So in our data, we use the rural urban continuum codes as the index for describing the rural urbanization.

39
00:04:37,620 --> 00:04:47,279
So it was originally developed in 1974 and up there is ten years in 1983, 1993, 23 and 2013.

40
00:04:47,280 --> 00:04:50,820
So it should be updated for next year, 2023.

41
00:04:51,180 --> 00:05:00,839
But until now, the latest data we could find is 2013 data and also the data prior to year 2000 cannot be compared.

42
00:05:00,840 --> 00:05:11,280
Luckily after the year 2000, because new methodology was applied to define the status of Metro or non-natural place.

43
00:05:11,580 --> 00:05:16,440
So in our analysis, we only include data from 20 all throughout 2013.

44
00:05:16,650 --> 00:05:19,350
And then you focus on the latest one 2013.

45
00:05:19,770 --> 00:05:27,659
So the court could distinguish the metropolitan counties by the population size of the metro area and the module pilot in counties.

46
00:05:27,660 --> 00:05:31,350
By the degree of urbanization and adjacency to metro areas,

47
00:05:31,770 --> 00:05:37,980
it could help us to work with county data by a more detailed classification instead of only the binary ones,

48
00:05:38,340 --> 00:05:43,050
and for the analysis of trends related to the way of morality and metro proximity.

49
00:05:45,000 --> 00:05:53,790
So to construct the rural, urban, rural continuum codes, all glass counties and county equivalents were first grouped according to the official.

50
00:05:53,790 --> 00:06:03,000
Metro were not Metro status as defined by the Office of Management and Budget, and the specific code description is in this list.

51
00:06:03,240 --> 00:06:06,840
They contain a three match rule and six no metro groupings.

52
00:06:08,310 --> 00:06:15,209
So the table shows that detailed categories for the the meaning of each category.

53
00:06:15,210 --> 00:06:21,510
So it's a categorical variable and containing 1299 worth in total.

54
00:06:22,180 --> 00:06:25,970
And the first of all is the are three match.

55
00:06:25,980 --> 00:06:31,110
We'll call these categories the metro column these were all classic were divided by the

56
00:06:31,110 --> 00:06:36,780
total population of the metro areas where the county belonged to by the population

57
00:06:36,780 --> 00:06:44,819
of 1 million population one more fewer than 200,000 population were between and the now

58
00:06:44,820 --> 00:06:51,470
metro counties have to stop the first that is to classify by the urban population.

59
00:06:52,080 --> 00:07:02,040
20,000 will more or less than 2532 and then further classify it by whether the county is agent to a metro area.

60
00:07:02,430 --> 00:07:13,739
So if a county is a physically or to to metro areas and have at least 2% of the labor force will commuting to the central metro areas,

61
00:07:13,740 --> 00:07:20,160
they will be classified as adjacent to the metro area where it will be classified as not adjacent.

62
00:07:20,820 --> 00:07:26,070
So they are total for tonight the six categories for the now metro counties.

63
00:07:27,120 --> 00:07:33,989
Yeah. So the table two shows a summary statistics for the rural urban calls our data

64
00:07:33,990 --> 00:07:44,700
including 3230 for county or county equivalents in total and by the year 23 and 2013.

65
00:07:45,120 --> 00:08:00,800
So the 2030 overall. Urban Continuum coat included 1236 or 38% metro counties and 1,996% non-metro counties were 62%,

66
00:08:01,160 --> 00:08:09,830
and for 23 data included 36% metro counties and 64% metro causes.

67
00:08:10,280 --> 00:08:14,030
So as we can see, as the time goes by, for ten years,

68
00:08:14,030 --> 00:08:20,210
the number of metro counties increase a little bit and the corresponding non-metro counties decrease a little bit.

69
00:08:21,680 --> 00:08:31,700
And also it contains a few just annoyance categories because this kind of districts are not classified.

70
00:08:32,660 --> 00:08:36,980
Just like some island like grows at an hour. Not the islands.

71
00:08:37,190 --> 00:08:48,920
More a municipality. So in order to get a better sense of the spatial distribution of the overall cognitive index across the United States,

72
00:08:49,700 --> 00:08:52,610
made a huge map of the the index.

73
00:08:53,090 --> 00:09:04,310
So in the plot, the rather area like this ball park represents counties with lower carbon platinum index and higher urbanization level.

74
00:09:04,340 --> 00:09:15,110
And in the plot, like in the green circle, the wider area represents higher index and lower urbanization level.

75
00:09:15,680 --> 00:09:25,880
So since our focus of our analysis is on the state of Michigan, so I made another heat map of the overall index of the state of Michigan.

76
00:09:26,300 --> 00:09:32,600
So Phil, alterations from the and what is that like in the area?

77
00:09:32,840 --> 00:09:41,420
The counties with higher level in the gray area, the counties with higher quality are more likely to have neighbors.

78
00:09:41,840 --> 00:09:51,260
And like in the blue area counties with higher urbanization, they're more likely to be adjacent to counties with a similar urbanization level.

79
00:09:51,620 --> 00:09:59,900
So we have our guess that there are probably some spatial correlation in terms of the distribution of the organization.

80
00:10:03,690 --> 00:10:05,520
So as we have seen in the heat map,

81
00:10:05,610 --> 00:10:13,740
there seems to be some kind of spatial autocorrelation regarding to the sea level in the state of Michigan and also across the nation.

82
00:10:14,160 --> 00:10:22,180
So we further move on to see whether they do if this kind of observation has a correlation with the more it's basically

83
00:10:22,260 --> 00:10:31,080
more as if half measures the individual by the value of individual county and its correlation of its neighboring counties.

84
00:10:31,470 --> 00:10:36,030
And our main focus here is in the state of Michigan instead of the whole nation,

85
00:10:37,020 --> 00:10:44,030
by saying the neighboring counties, we use the Queen method to define a neighboring county involving masses.

86
00:10:44,460 --> 00:10:47,430
The counties showing common points are always fairly common.

87
00:10:47,430 --> 00:10:54,180
Vertex are considered to be neighbors, and for each neighboring polygons they will be assigned equal weight.

88
00:10:54,540 --> 00:10:57,660
So, for example, a county, if you have four neighbors,

89
00:10:57,930 --> 00:11:03,719
then each of its neighboring counties will be assigned a weight of points 25 And then we

90
00:11:03,720 --> 00:11:09,420
also show the Morenci coefficient in calculating the formula here and in our case here,

91
00:11:09,420 --> 00:11:18,870
why I represent the American ten index of the Ice Polygon and y bar represents the global mean always kind of say the weighted average of of in

92
00:11:18,870 --> 00:11:27,180
rural county meeting that is calculated by the neighboring counties and it's represented three spacial ways of a link between five and I MJ.

93
00:11:28,630 --> 00:11:41,550
And the result shows that our Morenci coefficient is a positive point 68, which means p value way smaller 7.01.

94
00:11:41,850 --> 00:11:47,339
So we reject a the off random allocation of urban rural continuing mass and

95
00:11:47,340 --> 00:11:52,049
we conclude that there exist positive correlation between the county and we

96
00:11:52,050 --> 00:12:02,100
have seen notice that positive more and coefficient represents some clustering of the correlation Y that to represent south our dispersion pattern.

97
00:12:02,490 --> 00:12:12,209
So we conclude that the urbanization level is clustered in spatial in Michigan and we also blockade of Gaza.

98
00:12:12,210 --> 00:12:20,480
Part of the urbanization code against is spatially like the counterpart and here spatial counterpart

99
00:12:20,490 --> 00:12:27,480
represents the weighted average value calculated by the neighboring counties of specific county.

100
00:12:27,840 --> 00:12:33,600
So we have free from this gap now do exist some kind of off of the correlation in the map.

101
00:12:35,160 --> 00:12:42,300
And by now we have connected to the inadequate approach to task, the null hypothesis.

102
00:12:42,570 --> 00:12:50,310
But we also notice that there are some assumptions to be required in the inadequate approach.

103
00:12:50,490 --> 00:12:59,730
And if we realize these assumptions, we can conduct a multi kind of simulation to test the not hypothesis using any more test.

104
00:13:00,180 --> 00:13:10,320
And here this plot displays the expected distribution that the extended density distribution and then the null hypothesis.

105
00:13:10,590 --> 00:13:17,850
And here this vertical line represents the our observed more eye coefficient

106
00:13:18,090 --> 00:13:22,830
and we can see this vertical line four way to the right of this distribution.

107
00:13:23,190 --> 00:13:30,750
So again, we are using the result of the both the inadequate approach and the multicolor simulation.

108
00:13:31,020 --> 00:13:37,680
We conclude that the urbanization level in Michigan is clustered in some spatial pattern

109
00:13:38,340 --> 00:13:44,040
because of the relatively small P value and also the positive observed moral ice coefficient.

110
00:13:44,520 --> 00:13:52,110
And we think that this result makes perfect sense because in the first place our primary variable,

111
00:13:52,110 --> 00:13:57,510
which is the rural urban continuum, always has a urbanization level, is defined spatially.

112
00:13:57,780 --> 00:14:00,990
If we were to call how we define be this Faribault,

113
00:14:01,890 --> 00:14:09,090
the metropolitan counties define it based on the population size of the metro area by the metropolitan counties.

114
00:14:09,330 --> 00:14:14,280
If I look face out the degree of annihilation and the adjacency to metro areas.

115
00:14:14,670 --> 00:14:21,479
So it is defined in the first place spatially. So we think he makes a reasonable, reasonable result.

116
00:14:21,480 --> 00:14:34,950
You see, the urbanization level is clustered in Michigan and after doing these for I think data analysis for the data our group has captured,

117
00:14:35,220 --> 00:14:43,709
where you also move out, you see the like found associating between these socioeconomic factors such as

118
00:14:43,710 --> 00:14:48,570
the urbanization level associated with the food vaccination rate in Michigan.

119
00:14:48,990 --> 00:14:58,650
So again, we may therefore keep his mask and the data we use in the November nice 2022, which is the most intriguing phase in the dataset.

120
00:14:59,100 --> 00:15:09,390
And again, we can never be more. I hear the coefficients is a positive 129 which gives a p value with more than 1.01.

121
00:15:09,630 --> 00:15:10,140
So again,

122
00:15:10,140 --> 00:15:19,260
we conclude that we reject the null hypothesis and we conclude that events clustering state spatial patterns for full maps of human race in Michigan.

123
00:15:20,100 --> 00:15:29,970
So by now we have seen that the clustering pattern in for both full vaccination rates and for the urbanization level in the state of Michigan.

124
00:15:30,330 --> 00:15:40,950
And we now want to see to do some preliminary analysis on the association between vaccination rates and urbanization level in our state.

125
00:15:41,490 --> 00:15:45,840
And in this analysis,

126
00:15:45,840 --> 00:15:57,570
we also account for the spatial correlation in Michigan by using the our function ESP also and here our comparative urbanization level system.

127
00:15:57,660 --> 00:16:03,610
We have say we have nine categories in VS now about both in this preliminary analysis.

128
00:16:03,630 --> 00:16:09,900
We obviously like divide them into two groups based on whether it's a natural or a low natural.

129
00:16:10,590 --> 00:16:14,190
If it is if it has a cold ranging from 1 to 3,

130
00:16:14,190 --> 00:16:20,120
we would have a metro and the polluted one and otherwise it would be a long line and we call it half is zero.

131
00:16:20,670 --> 00:16:24,510
And our response scenario is the full vaccination rate.

132
00:16:24,930 --> 00:16:34,770
And we also created a model, especially for the age groups, and they're 18 years old, I think, to 64 years old and above 65 years old, respectively.

133
00:16:35,010 --> 00:16:45,060
And also, we can better, we think, a model for considering this age group and we hope of aging as a entirety.

134
00:16:45,480 --> 00:16:47,910
And here from these results table,

135
00:16:47,910 --> 00:16:58,860
we can see that the P-value of Lambda is significant and point of significance allowable for RV for all these models.

136
00:16:59,160 --> 00:17:05,700
And also the coefficients are all positive and the p value of the combinations

137
00:17:06,030 --> 00:17:12,600
is significant and the .05 significance and if everybody above 65 years out.

138
00:17:13,080 --> 00:17:21,450
So our really preliminary conclusion is that by, for example,

139
00:17:21,690 --> 00:17:28,520
for the age group under 1818 to 64 years old and also for the whole population has

140
00:17:28,710 --> 00:17:33,630
significant had a positive association between urbanization level and vaccination rate.

141
00:17:33,930 --> 00:17:45,060
To be more specific, more urbanized, more organized up, more over a nice country, if the full vaccination rate will be higher in that county.

142
00:17:45,600 --> 00:17:49,290
And we move on to do some further analysis.

143
00:17:52,330 --> 00:17:59,680
So after that preliminary analysis, we did a modeling to investigate the effect of urbanization on full vaccination rates.

144
00:18:00,310 --> 00:18:08,490
So our outcome of interest is the control level for vaccination rates from the safety of our sites as December 13th, 2020.

145
00:18:08,950 --> 00:18:13,450
And our focus is on, say, how much there is any three colleagues in total.

146
00:18:13,450 --> 00:18:18,309
And the the range of the full vaccination rates is 0 to 1.

147
00:18:18,310 --> 00:18:27,879
So in all of this as a proportion data using the better distribution and why issue is the better distribution modeling is that it cannot be always

148
00:18:27,880 --> 00:18:33,970
exactly around the was so based on a paper on better distributed dependent

149
00:18:33,970 --> 00:18:38,380
variable with either transformation to deal with the exact error on one side.

150
00:18:38,390 --> 00:18:41,920
And the transformation is based on this formula.

151
00:18:42,880 --> 00:18:51,280
And the corollary is the way in quality that your model is are the rural continuum index from 1 to 9 the week,

152
00:18:51,280 --> 00:18:55,090
which is the ten variable ranging from 51 to 50.

153
00:18:55,570 --> 00:19:06,220
And we kind of was the age into three groups, just us showing the previous slide and workers of the early after we have access to their data.

154
00:19:09,580 --> 00:19:18,550
The model used is a mixed model of the I.D., part of the specific first model and the conditional progressive model.

155
00:19:19,030 --> 00:19:26,379
And you can see the model specification that we model the vaccination rates by a beta distribution.

156
00:19:26,380 --> 00:19:34,750
And the coefficients of the method used to be able to validate how were represented using the mean and the subversion parameter

157
00:19:35,110 --> 00:19:43,360
and the mean of the vaccination rates were modeled using the fixed effects and to run the effects on the first round.

158
00:19:43,360 --> 00:19:47,620
The results accounts for the polygon specific effects.

159
00:19:47,620 --> 00:19:57,459
And the second random in fact is the car model to account for the structural spatial heterogeneity and the method that we use to fit this model.

160
00:19:57,460 --> 00:20:03,340
In some sense, the algorithm and we used it to conduct statistical estimation inference.

161
00:20:04,840 --> 00:20:11,200
So similarly the model was the win box and we did three chains for each.

162
00:20:11,500 --> 00:20:19,180
We included 10,000 revisions, whereas the first 6000 were kind of considered boring parties and discarded.

163
00:20:19,510 --> 00:20:31,420
So and the thing in Rachel was two. So the following table shows the posterior point estimate and that's why we're going to add the our hat.

164
00:20:31,900 --> 00:20:38,720
So the our hire was from the former Google Docs now think to check the cord injury.

165
00:20:39,670 --> 00:20:47,110
Um so we can see from the nice represented quite a rainfall about the four being

166
00:20:47,120 --> 00:20:53,499
the one between the three though 95% across the audience were all in toward zero.

167
00:20:53,500 --> 00:21:01,990
So they were we can conclude that the corresponding variable was significantly associated with the four vaccination rates.

168
00:21:02,860 --> 00:21:13,360
But the sum problem is our model is that like a zero and for three the AHA was pretty high greater than 1.01.

169
00:21:13,360 --> 00:21:21,590
So it kind of indicates that our model has some convergence issue which we need to to improve in the future.

170
00:21:24,280 --> 00:21:29,259
So several implication of the model results. First of all, people in all our age group,

171
00:21:29,260 --> 00:21:36,970
how severely we can be a higher for vaccination rates and obviously for vaccination rates increased over time.

172
00:21:37,450 --> 00:21:45,550
And our interest is that colleagues with higher organization level have a significantly higher rates of full vaccination.

173
00:21:46,240 --> 00:21:52,270
So as I mentioned that the first issue is the current model is that some of the parameters didn't work.

174
00:21:52,810 --> 00:22:01,270
So we either can fix them a little bit or run more iterations to see if you can solve the problem.

175
00:22:01,720 --> 00:22:10,120
And the second issue is that we didn't call for the time trial Croatia the full vaccination rates which is our next focus for the final report.

176
00:22:11,350 --> 00:22:15,610
Um, so back to the study aims of the project.

177
00:22:16,060 --> 00:22:17,110
So in this future,

178
00:22:17,110 --> 00:22:28,840
we intended to investigate the potential social or human mental factors associated with the COVID 19 vaccination by both spatial and temporal model.

179
00:22:29,290 --> 00:22:31,990
And our study focus is on the state of Michigan.

180
00:22:32,560 --> 00:22:42,010
And as we just examined that, our first aim is to determine the association between the full vaccination rates and the organization level.

181
00:22:42,520 --> 00:22:53,470
And the second aim when they continue to investigate is to, uh, to investigate the association between task force and organization level.

182
00:22:53,890 --> 00:22:59,410
And we, of course, will include other variables from other groups.

183
00:23:01,090 --> 00:23:07,390
Uh, thanks for listening. Time management.

184
00:23:08,080 --> 00:23:12,760
17 seconds. Oh, great.

185
00:23:13,150 --> 00:23:20,710
So I open for questions. So here's an opportunity for you to ask questions.

186
00:23:20,740 --> 00:23:24,130
Otherwise, they will ask your questions because they finish their presentation.

187
00:23:26,560 --> 00:23:29,680
So let me start with one simple question.

188
00:23:29,710 --> 00:23:34,060
Your analysis results, you have something a p value of lambda.

189
00:23:34,060 --> 00:23:40,990
What is lambda? I think it has to be like this spatial operation of the home.

190
00:23:41,740 --> 00:23:45,160
It is not taking is in the regression. Yeah.

191
00:23:45,310 --> 00:23:48,340
Yeah. Here results from B.

192
00:23:51,070 --> 00:23:54,790
So you run a linear regression. These older age groups.

193
00:23:54,820 --> 00:24:00,910
Yeah. Yeah. But using also take into account in the spatial autocorrelation.

194
00:24:02,500 --> 00:24:06,280
So lambda is a parameter for ultimate correlation.

195
00:24:06,290 --> 00:24:11,349
Spatial. Yeah. Okay. So here is not the association.

196
00:24:11,350 --> 00:24:14,640
This is a ultima correlation. Yeah.

197
00:24:15,880 --> 00:24:21,220
I don't think it has anything to do with the associative between these two variables here.

198
00:24:22,250 --> 00:24:36,280
Okay. I think from the result, the coefficient means the regression coefficient for the for for the for vaccination rate by each age group.

199
00:24:36,850 --> 00:24:40,089
And the p value for lambda is for the spatial all the correlation.

200
00:24:40,090 --> 00:24:47,319
So the lambda. So our null hypothesis is alumnus equal to zero is that means the low spatial.

201
00:24:47,320 --> 00:24:52,780
The spatial is independence, but that is a the p value is further our t test.

202
00:24:53,890 --> 00:25:00,250
Just if your title is really association between full vaccination rate and organization level.

203
00:25:00,550 --> 00:25:06,370
So basically this result does not answer that question in terms of association between these two variables.

204
00:25:06,820 --> 00:25:20,260
If P-value coefficients represents the kind of association between these two variables and this is the spatial of the correlation, I see.

205
00:25:20,740 --> 00:25:26,920
I say so, so, so I say probably is more clear than you run around the rate of beta.

206
00:25:26,920 --> 00:25:31,300
You have some p value for spatial autocorrelation because nobody knows.

207
00:25:31,480 --> 00:25:36,700
You don't give any formula, any lambda whatsoever. Nobody knows what that means.

208
00:25:36,700 --> 00:25:43,680
Right? Rather than writing a mathematical term which you have no formula or whatever to explain

209
00:25:43,760 --> 00:25:50,550
the c p value for spatial operation and p value for the significance of association.

210
00:25:50,740 --> 00:25:54,380
That could be better. So that's my question.

211
00:25:54,410 --> 00:26:01,830
Pair questions. Yeah. So on this slide, so you fit four models basically.

212
00:26:01,870 --> 00:26:07,390
So this is in the covariant age, it's just fitting it with the stratified stratified.

213
00:26:09,980 --> 00:26:15,020
So how difficult if you extend this work to the entire country with 50 states?

214
00:26:16,040 --> 00:26:24,200
I think this is very interesting to see the social beat between the urbanization and the vaccination coverage rate.

215
00:26:24,530 --> 00:26:29,450
I think there are probably more heterogeneity these across different states because this is

216
00:26:29,450 --> 00:26:37,520
very heterogeneous countries in terms of compliance or a certain attitude towards vaccination.

217
00:26:38,210 --> 00:26:45,560
Have you ever considered that you would like to extend this to the entire country with force through solid in some countries?

218
00:26:46,910 --> 00:26:50,149
Yeah, we we can do that forever.

219
00:26:50,150 --> 00:26:58,040
We chose to limit our analysis in the state of Michigan because yeah, I understand that you did this previous analysis for Michigan,

220
00:26:58,040 --> 00:27:10,470
but in your final project, do you plan to work on the data from the entire country or do you want to continue focusing on Michigan State?

221
00:27:13,790 --> 00:27:24,080
So by now what we do commitment though we don't sit in front of people and that's your plan.

222
00:27:24,950 --> 00:27:29,809
So I think our plan by now is like focusing on the state of Michigan.

223
00:27:29,810 --> 00:27:41,120
But I think you have just made a very good point that's out heterogeneity across the country with our considering is that it's a big

224
00:27:41,120 --> 00:27:55,189
dataset and also if like involving the car model is well you will take longer or yeah longer time to compute write a report over time.

225
00:27:55,190 --> 00:28:04,309
Longer time for computing. For computing. But I can give you a cluster account so you can run and consider if you think

226
00:28:04,310 --> 00:28:09,980
that as the computing power you need or you don't think that's the option.

227
00:28:14,120 --> 00:28:17,170
Never discussed this issue or. Okay. Okay.

228
00:28:18,530 --> 00:28:28,249
And, um, yeah, I think that my comment here is if you want somehow target on publishing this result,

229
00:28:28,250 --> 00:28:33,889
I think it's quite interesting resolve to submit for a publication and the end of this.

230
00:28:33,890 --> 00:28:42,160
I think the national data problem is more sort of appealing for you to have a better chance to publish the results.

231
00:28:43,350 --> 00:28:48,300
It's up to you. Right. So I see that you have learned quite a bit.

232
00:28:48,510 --> 00:28:52,940
You know, this whole specialty and analysis and car mold and everything.

233
00:28:52,950 --> 00:28:56,609
So that's not our thing.

234
00:28:56,610 --> 00:29:01,350
It's not that you use speed or distribution. Right. And that was the simplest distribution as a surprise.

235
00:29:01,710 --> 00:29:06,600
And so you could try simpler distribution.

236
00:29:06,960 --> 00:29:10,890
We have these supplies rack our package someplace.

237
00:29:10,890 --> 00:29:18,390
Right. Right. So you can model the proportion of data directly using simplex right.

238
00:29:18,390 --> 00:29:28,050
On beta distribution. And that could be a little bit more innovative because not many people know simplex distribution.

239
00:29:30,680 --> 00:29:36,559
And just as a suggestion and maybe it's half a question here, you used the vaccine as a proportion.

240
00:29:36,560 --> 00:29:43,129
You could also possibly think of it as the number of people per 100,000 individuals that got it.

241
00:29:43,130 --> 00:29:45,410
And then it would be like a count based variable.

242
00:29:45,470 --> 00:29:54,830
So then you could maybe it's a negative binomial as well or assign you make it more appropriate to the set about proportion that just happened.

243
00:29:55,140 --> 00:30:03,740
And then, you know, so instead of using the proportion, we can also use the account number, cancel them for example.

244
00:30:04,010 --> 00:30:06,410
You know, I think we have also think about that,

245
00:30:06,800 --> 00:30:15,680
but our primary concern is that our comparative the of of an additional level we should also if I if based on the population number.

246
00:30:15,980 --> 00:30:21,890
So we are not sure whether it's like a valid thing to do here.

247
00:30:22,490 --> 00:30:32,840
Yeah. I can clearly see that you do. There's sort of a natural non-metro sort of aggregation of this because you only have 33 countries.

248
00:30:33,100 --> 00:30:37,190
Yeah. If you really want to do high resolution like using 1 to 9,

249
00:30:37,550 --> 00:30:49,460
probably have to go for more countries in analysis so that can generate enough sample size with the each category to get sort of your analysis power.

250
00:30:49,820 --> 00:30:53,410
So I can see that why you do that collapsing.

251
00:30:53,420 --> 00:31:00,950
And so anyway, I'll be happy to schedule meeting with you team and your team and we'll see if we

252
00:31:00,950 --> 00:31:08,670
can move forward and how to write a good project but already see some significant.

253
00:31:08,690 --> 00:31:11,700
These are based on 30 in three counties.

254
00:31:11,720 --> 00:31:17,260
That's nice to see. Thank you very much for talking with us.

255
00:31:20,780 --> 00:31:24,500
Thank you so much. Appreciate the presentation.

256
00:31:25,040 --> 00:31:31,180
Oh, yeah. Okay. Next time I.

257
00:31:35,250 --> 00:31:56,510
Six. Shall we start?

258
00:31:57,380 --> 00:32:00,620
Sure. Please. Yeah. So. So again, down here. Yeah.

259
00:32:01,010 --> 00:32:07,340
Oh. Is that how you time? Yes, of course. I mean, I don't want to treat their time to kill on time.

260
00:32:08,960 --> 00:32:12,080
No worries. Okay, start. Yeah.

261
00:32:12,080 --> 00:32:17,690
So we're doing three and today we're doing a CAPTCHA task is to collect meteorological data.

262
00:32:18,860 --> 00:32:24,380
So when you break into three parts, I'll first introduce the collection of air quality data,

263
00:32:24,680 --> 00:32:28,730
then labs, and we'll talk about the collection of weather data at temperature and wind speed.

264
00:32:29,220 --> 00:32:37,790
And then we'll talk about a small case study of a meteorology data and COVID 19 incidences in Southwest California.

265
00:32:39,200 --> 00:32:44,150
So what I tell you when I say air quality, we're talking about different air pollutants.

266
00:32:45,290 --> 00:32:50,360
So this table was taken from one of the AQI technical documents provided by EPA.

267
00:32:50,940 --> 00:32:56,960
And we have six variables each hour ozone, 24 hour PM, 3.5.

268
00:32:57,500 --> 00:33:05,000
And of our PM10, it's our carbon monoxide when our sulfur dioxide and when are over nitrogen oxide.

269
00:33:05,810 --> 00:33:15,050
So this table for each of these these pollutants and their raw values can be projected onto a linear scale called AQI,

270
00:33:15,590 --> 00:33:19,730
which is which is air quality index. And it's essentially a spline.

271
00:33:19,790 --> 00:33:21,499
Let's just look at first example.

272
00:33:21,500 --> 00:33:34,820
If if it's ozone, then the value range of 0202.05 flora will be projected onto the scale of 0 to 50 and of correspondingly the values in this range,

273
00:33:35,470 --> 00:33:40,340
in this range will be projected onto a 51 to a to 100.

274
00:33:41,540 --> 00:33:48,260
So I just want to mention that so every pollutant has an AQI value every day as long as it's collected.

275
00:33:48,710 --> 00:33:57,470
And the value here, for example, if you're hearing the weather report like today's AQI value is 125, mainly driven by PM 2.5.

276
00:33:57,800 --> 00:34:03,920
It carries two information. First is that the AQI value of PM 2.5 is 125.

277
00:34:04,460 --> 00:34:11,870
And the second thing is that PM 2.5 has the highest AQI value, all the pollutants collected that day in that area.

278
00:34:13,160 --> 00:34:18,110
So yeah, that's the main information I want to convey on this page.

279
00:34:18,890 --> 00:34:31,280
So in our data collection, we collected the data, all the data we can find from year 2019 to 2022, and both the raw values and the AQI value,

280
00:34:31,280 --> 00:34:38,179
which is essentially a scaled value for all the six pollutants are provided and the values are provided on stations.

281
00:34:38,180 --> 00:34:41,270
But given that most of the other data you capture on county level,

282
00:34:41,930 --> 00:34:50,510
so I group the data on to county level and let's say there are multiple stations collecting PM 2.5 in one county on the specific day,

283
00:34:50,540 --> 00:34:55,400
I'll take the average as the head count. I think that's a reasonable and easy solution.

284
00:34:56,150 --> 00:35:05,000
And another thing I want to mention is that most counties every day they only have a subset of air pollutants.

285
00:35:05,000 --> 00:35:08,540
You know, there are a total of six of them, but sometimes they only have two or three.

286
00:35:08,840 --> 00:35:12,200
Most of the time they only have two or three pollutants collected every day.

287
00:35:13,170 --> 00:35:16,460
Depends on the technical ability of the different stations.

288
00:35:18,070 --> 00:35:25,900
So just so I just want to show you. So I just had mentioned that we collected four years of data.

289
00:35:26,980 --> 00:35:32,500
The plot on the left is showing the number of stations that collect each pollutant

290
00:35:34,330 --> 00:35:40,810
ozone and PM 2.5 has more stations collecting them than other pollutants.

291
00:35:41,440 --> 00:35:54,250
Ozone has about 1200 stations across the nations, collecting them, at least in the year of about 1200 stations in about 770 counties.

292
00:35:54,520 --> 00:36:00,550
And other pollutants have fewer stations and fewer counties collected throughout the recent four years.

293
00:36:01,600 --> 00:36:11,410
So we can we can show the locations of these stations. For example, this is a visualization of the locations in mainland U.S. that collected PM 2.5.

294
00:36:11,980 --> 00:36:16,780
And I want to mention one thing that is a per EPA requirement.

295
00:36:17,470 --> 00:36:25,810
Large metropolitan statistical areas with population larger than 350,000 are required to report AQI every day.

296
00:36:26,530 --> 00:36:32,650
So you you will notice that most of these points but these points are in large metropolitan areas.

297
00:36:33,310 --> 00:36:36,730
So these they are required for other regions. It's voluntary.

298
00:36:36,850 --> 00:36:42,580
So it's not surprising to see a lot of stations located in populous areas like California.

299
00:36:42,940 --> 00:36:49,930
And this is a northeast kind of New England area. The pattern is universal for all the pollutants.

300
00:36:50,350 --> 00:36:59,140
It's a bit it's a bit too small for you to see. But this is a and all the six pollutants in 2021 the stations that collect them.

301
00:37:00,250 --> 00:37:07,150
As I've mentioned this ozone and the PM 2.5, they have the most number of stations collecting them.

302
00:37:08,410 --> 00:37:12,070
Other pollutants have fewer stations, but the trend is the same.

303
00:37:12,610 --> 00:37:18,820
Most stations we collect which collect pollution data, are in the metropolitan areas with a large population.

304
00:37:20,350 --> 00:37:28,629
And also we can show it on the country level. This is also so this is an area showing the counties that contain the stations.

305
00:37:28,630 --> 00:37:32,290
I just show on the previous previous pages.

306
00:37:32,830 --> 00:37:37,690
Most of the countries are in California and Northeast for PM 2.5.

307
00:37:39,040 --> 00:37:47,380
And the same thing can be said as well for all the all the other air pollutants as well.

308
00:37:50,800 --> 00:37:57,370
So what I heard that not every country has the data where every country has some kind of pollution data.

309
00:37:57,970 --> 00:38:03,220
These are the countries that have data. All the other white areas, they don't have been able to do that.

310
00:38:03,650 --> 00:38:10,240
Okay. So there are missing data essentially. Yeah, yeah, yeah. Most counties don't have air pollution data.

311
00:38:10,810 --> 00:38:15,430
Those who have they usually only have maybe two or three out of the six pollutants.

312
00:38:15,700 --> 00:38:19,910
So you need to do critically especially. Yeah.

313
00:38:22,820 --> 00:38:28,970
Okay. We talk. So next up, I will talk about the weather data that I have collected.

314
00:38:29,210 --> 00:38:37,880
So just to be in a consistent manner. So I collected the data over the years of 2019 to 2022 as well.

315
00:38:38,240 --> 00:38:43,100
And so there were fall weather, how variables that were collected.

316
00:38:43,340 --> 00:38:49,610
So the temp that which means the outdoor temperature and the unit as in degree Fahrenheit.

317
00:38:49,970 --> 00:38:56,990
And then there's the press, which is the volumetric pressure, the units unit in mini bars.

318
00:38:57,350 --> 00:39:05,149
And then there's the are the P as the relative relative humidity or of the of the point.

319
00:39:05,150 --> 00:39:08,900
But I think to avoid confusion, we just call that relative humidity.

320
00:39:09,200 --> 00:39:18,050
And there's a number that represent represented in a unit of a percent relative humidity, so which basically 0 to 100.

321
00:39:18,440 --> 00:39:23,360
And finally, the wind as the resulting wind speed,

322
00:39:23,570 --> 00:39:29,840
which is in the unit of not so press and when there would be like some kind of like positive number,

323
00:39:29,880 --> 00:39:37,160
a positive continuous number and for weather like for the temperature, you know, the Fahrenheit very well.

324
00:39:37,580 --> 00:39:42,049
And so, again, so each of these are variables.

325
00:39:42,050 --> 00:39:48,470
They are connected in the station and there might be multiple stations within the same county.

326
00:39:49,460 --> 00:39:54,290
So we just aggregate the data and it take average just as much as that.

327
00:39:54,590 --> 00:40:03,079
But however, there's another issue here is that these variables are already sample at a given station.

328
00:40:03,080 --> 00:40:10,760
So. So. So, for example, you can have up to 24 hour, 24 hour measurements within a day.

329
00:40:10,940 --> 00:40:19,700
So we still we would take the average of these already samples if they were recorded across all the records.

330
00:40:20,120 --> 00:40:30,380
So this is how we processed this about whether data and similar kind of plots can be generated that for the weather data.

331
00:40:30,650 --> 00:40:37,850
And we can see that. So so for weather data, there is no requirement for the.

332
00:40:38,420 --> 00:40:45,590
So you don't have to it's not compulsory for the metro parking areas to to generate these data.

333
00:40:45,590 --> 00:40:50,059
So I think this might be the reason why you have a relatively fewer number of

334
00:40:50,060 --> 00:40:56,220
station that are collecting it relative to the data that catch just a curator.

335
00:40:57,110 --> 00:41:03,409
So but we can see here for wind and temperature data in general,

336
00:41:03,410 --> 00:41:11,420
it has more number of stations compared to the relative humidity and also with the metric pressure.

337
00:41:11,870 --> 00:41:15,060
And same thing can be said about the county.

338
00:41:15,080 --> 00:41:20,750
So in general, yeah, there are more available data simply for wind and temperature.

339
00:41:22,100 --> 00:41:30,020
And again, this kind of distribution can also be seen even though it's not like compulsory.

340
00:41:30,020 --> 00:41:34,400
But we are also see like to us these are metropolitan areas.

341
00:41:34,940 --> 00:41:39,860
We have tend to have more measurements of these weather data.

342
00:41:42,020 --> 00:41:47,390
And this is another perspective from the county perspective.

343
00:41:47,750 --> 00:41:53,960
Same thing can be said. Yeah. So next up our path to it's it's fun to talk about the case study.

344
00:41:54,200 --> 00:42:03,049
So in the following case study, we are going to look at the association between environment variables such as air

345
00:42:03,050 --> 00:42:12,350
pollutants and weather variables and COVID incidence using data from California.

346
00:42:12,620 --> 00:42:22,000
And we will only focus on these five counties since they have the most complete records, all variable of interest.

347
00:42:22,010 --> 00:42:28,910
So these five counties include Kern, Ventura, Los Angeles, Orange and San Bernardino County.

348
00:42:30,920 --> 00:42:39,440
And COVID 19 is in this data. Only five counties are obtained from California Health and Human Services Open Data Portal.

349
00:42:40,070 --> 00:42:47,570
And the time period of this data range from February 2020 to November 2022.

350
00:42:48,080 --> 00:42:57,260
And COVID 19 is and this is calculated by dividing the total number of confirmed cases,

351
00:42:58,070 --> 00:43:04,550
by the total number of population size, and then applying it by 100,000.

352
00:43:04,910 --> 00:43:14,030
And throughout this time period, there are about 4.8, covering 4.8 million confirmed cases in these five counties.

353
00:43:16,160 --> 00:43:21,860
And to study the temporal patterns of our data, we pass the time through as part of each variables.

354
00:43:22,390 --> 00:43:33,100
So you can see on each panel represent those variables and each coalesce represent our county.

355
00:43:33,550 --> 00:43:43,000
So from this part we can see that C0 and two shows similar patterns to cover the incidence rate.

356
00:43:43,480 --> 00:43:54,340
You can see that they all increase their levels in winter months and then decrease after and remain relatively low during the summer and fall period.

357
00:43:55,210 --> 00:44:02,890
And you can also see some inverse patterns in also P.

358
00:44:04,240 --> 00:44:15,760
So you can see that both RPO have increased in late summer and fall and then remain relatively slow during the winter period.

359
00:44:16,330 --> 00:44:20,510
So this is the inverse pattern where the incidence range.

360
00:44:23,550 --> 00:44:39,450
And we have a plot. If we look at the 12 of the variables, you can see that temperature and wind three has a similar pattern with two also.

361
00:44:40,590 --> 00:44:50,070
So you can see that these values are much higher in some up here and fall up here and much lower during the winter up here.

362
00:44:52,560 --> 00:45:00,390
So every firm will gather by varied correlations between these variables, we can find some interesting results.

363
00:45:00,870 --> 00:45:04,920
So we calculate the Pearson correlations between variables.

364
00:45:05,490 --> 00:45:19,440
So here the correlation plot with darker red indicating stronger positive correlation and darker blue indicating strong this negative correlation.

365
00:45:19,470 --> 00:45:26,130
So from this call we can see that COVID 19 incidence shows positive association

366
00:45:26,580 --> 00:45:32,760
between C0 and an all too and negative association between also inverse or two.

367
00:45:33,150 --> 00:45:39,450
And we also found that all other variables show negative association where the COVID 19 incidence.

368
00:45:43,080 --> 00:45:53,580
So we also calculate our annual average of each variable and then visualize these values using the spatial heatmap.

369
00:45:54,090 --> 00:46:02,129
And from this how you can see that it's dispatcher San Bernardino counties have higher COVID

370
00:46:02,130 --> 00:46:08,790
incidence values and higher and no to and awesome concentration than the other counties.

371
00:46:09,300 --> 00:46:16,800
So we have some spatial spatial geometry in our data.

372
00:46:17,190 --> 00:46:24,540
So in conclusion, we define some correlation between our variable interest and COVID incidence.

373
00:46:25,230 --> 00:46:31,860
So COVID 19 is and it's very correlated with some of the other pollutants variables and weather variables.

374
00:46:32,250 --> 00:46:38,880
And we did find some spatial heterogeneity in our data across counties.

375
00:46:40,530 --> 00:46:47,920
And this is all in our. Time for questions.

376
00:46:51,810 --> 00:47:02,820
So I think that's interesting to see this sort of negative correlation and you've presented this negative sort of temporal patterns.

377
00:47:03,120 --> 00:47:16,410
You know, the my question here is really, what is the premise to what is the possible reason behind this negative of.

378
00:47:17,400 --> 00:47:27,260
Have you ever thought about it that we will do something better revealed than previous study shows?

379
00:47:30,740 --> 00:47:37,070
Now some of air pollutants variables are influenced by weather variables.

380
00:47:37,880 --> 00:47:49,760
So you can see that, for example, your CO2, CO2 is negative correlated with wind speed.

381
00:47:50,090 --> 00:47:59,420
So during winters or during winter, the fear is much lower than other seasons.

382
00:48:01,130 --> 00:48:15,680
But why do you believe that when temperature is higher than the chance of survival for various is low so that they may be less infectious?

383
00:48:16,130 --> 00:48:22,490
But if you have in the air or is not around because wind speed and meaning temperature

384
00:48:22,490 --> 00:48:29,800
will determine the determines how long these air pollutants will stay in.

385
00:48:31,730 --> 00:48:34,910
Yeah. Staying in the environment. Yeah.

386
00:48:35,030 --> 00:48:38,209
So, you know, so yeah. So I yeah.

387
00:48:38,210 --> 00:48:45,260
We will also notice this pattern of when we did this exploratory analysis and we are.

388
00:48:45,640 --> 00:48:53,180
So, so yeah, we wondered why. And so as I just mentioned, we did some kind of literature review.

389
00:48:53,480 --> 00:49:00,379
And so I am suspecting there's some kind of like so because these are air pollutants.

390
00:49:00,380 --> 00:49:06,560
So I am hypothesizing like other, like there's some kind of like causal pathway going from,

391
00:49:06,860 --> 00:49:12,740
say, the temperature that they caused a change and these air pollutants level.

392
00:49:12,950 --> 00:49:17,540
And by then it's like increase or decrease and then that's subsequently maybe because

393
00:49:17,960 --> 00:49:24,380
affect our prosperity system and so that maybe it will lower our immunity so

394
00:49:24,680 --> 00:49:31,100
and so that's a good kind of like causal pathway potentially where being responsible

395
00:49:31,100 --> 00:49:38,630
for like how this changes the COVID 19 like and so the incidence or death.

396
00:49:38,840 --> 00:49:47,420
So yeah, I think one way, one thing we could potentially do is like to apply what like Sumac has developed in our group.

397
00:49:47,570 --> 00:49:48,920
So to use the,

398
00:49:48,920 --> 00:50:01,430
this method of like directionality testing to see like what like we should as that barista not like whether as like are the air pollutants

399
00:50:01,430 --> 00:50:09,800
like causing the temperature or temperature causing the output of it so that this kind of like things could be potentially done.

400
00:50:10,100 --> 00:50:13,130
Yeah, in my view. Okay, here's my hypothesis.

401
00:50:13,520 --> 00:50:24,589
Well, air pollution is high. People tend to reduce their outdoor activities so that it's more likely to reduce the interaction among people,

402
00:50:24,590 --> 00:50:27,590
too, so that you have zero chance to get infected.

403
00:50:28,300 --> 00:50:34,490
I understand this fancy statistic I'm talking about more like what would be the possible

404
00:50:34,490 --> 00:50:39,620
scientific hypothesis behind this so that you can see this negative correlation,

405
00:50:39,630 --> 00:50:43,310
high pollution that but you have lowered that strange word,

406
00:50:43,310 --> 00:50:51,830
you know, because people when air pollution is high and then people want to reduce their outdoor activities or,

407
00:50:51,830 --> 00:50:58,309
you know, that that probably that will reduce the chance of contracting the virus.

408
00:50:58,310 --> 00:51:05,450
So that yeah, I just I just to add that, you know, it's it's you some kind of scientific climate might have a reason.

409
00:51:05,450 --> 00:51:09,560
I mean, the trend you see in California might not be the same in Massachusetts.

410
00:51:09,740 --> 00:51:14,020
So climate could be. And that's my second question. Can you reproduce this result?

411
00:51:14,240 --> 00:51:19,520
The weather like similar to I mean you've started to high scientific hypotheses like what I said.

412
00:51:19,880 --> 00:51:27,950
So let's say higher temperature, higher level of air pollution would reduce the outdoor activities so that people like to stay inside,

413
00:51:27,950 --> 00:51:33,680
to reduce sort of the chance of interaction and then, you know,

414
00:51:34,160 --> 00:51:40,069
reduce the risk of being a, you know, attracting bears can reproduce this in the weather.

415
00:51:40,070 --> 00:51:50,660
Like for, for example, you choose a county that has somewhat similar a setting that you may be reproduce the result you observe here.

416
00:51:51,590 --> 00:51:55,070
I know that you choose this because you have more data and so on,

417
00:51:55,460 --> 00:52:01,890
but I think that it's very important to have a component of validation where reproducibility and

418
00:52:02,390 --> 00:52:09,860
and this whole thing has to be based on kind of a very clear and well-defined sort of hypothesis.

419
00:52:09,860 --> 00:52:16,910
You like to verify, you have something, you've heard observations of what's going on that were motivated to think why would be

420
00:52:17,270 --> 00:52:24,649
impossible reasonably and the hypothesis like to to you know guide you to do the done analyzes.

421
00:52:24,650 --> 00:52:33,650
But at the end of the day I think that you should have another Sunday or a couple of more data or locations in the States to verify to,

422
00:52:33,650 --> 00:52:37,130
to to reproduce this as a confirmation.

423
00:52:37,940 --> 00:52:41,810
Yeah. On questions.

424
00:52:42,710 --> 00:52:51,440
And if I just am curious on the slide with the like time series of the.

425
00:52:53,900 --> 00:52:58,830
I'm just curious if you looked into what those jumps, that big jump is.

426
00:52:59,330 --> 00:53:04,670
Yeah, we don't actually know what you mean. The last second was when the sulfur dioxide.

427
00:53:04,670 --> 00:53:08,510
Yeah, that's kind of. We didn't even picture that. Maybe artificial things.

428
00:53:08,550 --> 00:53:18,370
Yeah. Yeah, that's artifacts. Yeah. I think now what you observe here,

429
00:53:18,370 --> 00:53:25,059
I think is important because you know that China is at the age of really relaxing

430
00:53:25,060 --> 00:53:31,810
all the policies of these sort of strict quarantines is really happening now.

431
00:53:32,380 --> 00:53:40,540
But there are some places, you know, think like Hong Kong where Gwangju or some places have very similar weather.

432
00:53:40,690 --> 00:53:42,219
I don't know if the US did.

433
00:53:42,220 --> 00:53:51,280
I can give them some guidelines that how the policy implications that which areas should be more open, which we should be less open.

434
00:53:51,350 --> 00:54:00,009
And here is the you know this all countries from a way that probably reflect or

435
00:54:00,010 --> 00:54:05,050
can mirror some of the places in China in terms of weather and the situation that,

436
00:54:05,620 --> 00:54:11,980
you know, that could be some kind of policy implication to other countries to use to make a decision.

437
00:54:16,830 --> 00:54:25,620
I think the important thing to observe this kind of concordance or discordance in all of the situations and of course,

438
00:54:25,620 --> 00:54:28,469
this will be also related to the vaccination rate.

439
00:54:28,470 --> 00:54:37,870
That's when you go further, you look a little bit more countries and part of reproducibility and I often read questions there.

440
00:54:38,250 --> 00:54:42,130
So on the correlation heatmap found that.

441
00:54:42,210 --> 00:54:50,130
So just before I asked the question here, he took all time points in all counties and looked at the correlation between the variables.

442
00:54:50,130 --> 00:54:53,940
Correct. There are some using data in some.

443
00:54:54,390 --> 00:55:00,150
Okay. So it might be interesting if you took like one specific month and I know you don't have any counties,

444
00:55:00,450 --> 00:55:10,710
let's say you only took one specific month's values or one specific time points values because then you could say in May 2020,

445
00:55:10,920 --> 00:55:17,309
a higher PM 2.5 is correlated with a higher monthly incidence because you're now you're

446
00:55:17,310 --> 00:55:25,300
saying over time as a counties e like PM two increases their monthly incidence increases.

447
00:55:25,320 --> 00:55:28,380
So it may just be a different dimension to look at that.

448
00:55:28,410 --> 00:55:30,530
Yeah, that makes sense. Yeah.

449
00:55:30,540 --> 00:55:40,679
So compound varying coefficients right here you have the overall average coefficient and then you'll see how this variance coefficients.

450
00:55:40,680 --> 00:55:48,690
I think some coefficient will change, some will stay stable and then you'll see why they're stable and why they're changing.

451
00:55:48,840 --> 00:55:58,440
Could they be related to the more coverage of vaccination or, you know, other reasons that change over time.

452
00:55:58,440 --> 00:56:02,040
So so yeah. So something can be invested more.

453
00:56:02,040 --> 00:56:08,639
I think this is a very complex system and so that some of clearer insight or

454
00:56:08,640 --> 00:56:14,219
direction or hypothesis like if you like to sort of verify using your data,

455
00:56:14,220 --> 00:56:24,120
it's useful to being happy to set up a meeting with you and always give the more detail and think for your presentation.

456
00:56:27,470 --> 00:56:36,600
Thank you for. Everyone gets one back, so we move out the next thing.

457
00:56:39,730 --> 00:56:45,940
So I just won't confirm that you have the f i p s code for each country to write in your dataset.

458
00:56:46,000 --> 00:56:50,200
That's the code that you need to link all the data together.

459
00:56:52,690 --> 00:56:56,200
Do you have this five piece code in your dataset?

460
00:56:58,090 --> 00:57:01,270
If you don't, please. I had intuited so that when we do.

461
00:57:01,420 --> 00:57:06,190
Yeah, we will. Yeah. We added. Our initial one that she sent us didn't we actually.

462
00:57:06,430 --> 00:57:13,090
Okay. Cool. Because we don't, we don't have it yet added because that's the code that people can use to encode data.

463
00:57:13,180 --> 00:57:19,720
Right. Sure. Yeah. Sorry. It's a hard distribution.

464
00:57:20,290 --> 00:57:27,189
Oh, it's like the first. You are from there.

465
00:57:27,190 --> 00:57:31,690
So it might be time to add the F amps. Oh, you can combine that.

466
00:57:31,810 --> 00:57:34,820
Oh, there's a map, too. So I'm pulling.

467
00:57:35,010 --> 00:57:45,760
Yeah, if. Yeah.

468
00:57:46,690 --> 00:57:54,700
Thank you. The third team, based on the data that you related to your homework.

469
00:57:54,880 --> 00:57:58,100
Before we do, we need to know to. To you.

470
00:58:08,220 --> 00:58:13,070
Fine with that. Yeah, that's okay.

471
00:58:13,080 --> 00:58:19,240
Why did it not lock me in one? I've seen Joanne continue to time.

472
00:58:19,580 --> 00:58:23,350
Yeah, yeah, yeah. Okay.

473
00:58:39,640 --> 00:58:45,610
And of course, I wonder who might be interesting to help me to build up a database that can be

474
00:58:45,610 --> 00:58:51,490
like put all the data together in somewhere that other researchers can access.

475
00:58:51,580 --> 00:58:54,730
We have good documentation and all the data there.

476
00:58:55,930 --> 00:59:04,670
I think that could help other researchers to access and use our data because I don't want to waste your time effort because this data is precious.

477
00:59:04,690 --> 00:59:15,429
You can see that for all the different type of variables, information coming together, and those are all related to probably COVID pandemic.

478
00:59:15,430 --> 00:59:20,469
And if we have a database in the name of from Boston University, Michigan,

479
00:59:20,470 --> 00:59:28,540
that people can access and make acknowledgments using the data, that would be something that we contribute to this field of research.

480
00:59:29,110 --> 00:59:33,730
Let me know. Okay. Whenever you're ready.

481
00:59:33,790 --> 00:59:38,390
Okay. Okay. Go. Oh, okay.

482
00:59:38,520 --> 00:59:41,260
Hi, everyone. I'm Annie and this is Ben.

483
00:59:41,500 --> 00:59:53,740
And we will be presenting on our data capture, which focuses on vaccine hesitancy and political affiliation at a county level across the U.S.

484
00:59:55,810 --> 00:59:59,350
So first, I'm going to go over the data sources that we use.

485
00:59:59,860 --> 01:00:05,110
So the first sources from the CDC, the vaccine hesitancy data,

486
01:00:05,770 --> 01:00:13,300
which takes data from the U.S. Census Bureau's Household Pulse survey from April of 2021,

487
01:00:13,450 --> 01:00:18,880
which is about four or five months after the initial vaccine rollout.

488
01:00:19,600 --> 01:00:23,110
And we were interested in a few different county level variables.

489
01:00:23,560 --> 01:00:31,690
The main one that we focus on is the percent hesitant, which is categorized as people who respond,

490
01:00:31,690 --> 01:00:39,610
probably not or definitely not, to the question of whether they will get the COVID 19 vaccine once it becomes available to them.

491
01:00:40,480 --> 01:00:47,080
And then the other variables of interest were the Social Vulnerability Index, SDI,

492
01:00:47,080 --> 01:00:54,399
which is based on 16 other variables to determine how much assistance a specific county will

493
01:00:54,400 --> 01:01:01,900
need in case of a hazardous event like a global pandemic or a natural disaster or something.

494
01:01:02,920 --> 01:01:10,239
And then other county level variables that we took from this data were racial demographic variables which we plan on,

495
01:01:10,240 --> 01:01:25,050
including later on in our analysis. The second data source is the 2020 election data from Kaggle, which was sent to us by Dr. Sun,

496
01:01:26,370 --> 01:01:31,380
and there were a bunch of sources in this little repository.

497
01:01:31,950 --> 01:01:34,950
We chose these two data files.

498
01:01:36,060 --> 01:01:43,710
The President County gave us the total number of votes for each county across all of the candidates,

499
01:01:45,090 --> 01:01:55,590
and we think that this was updated daily during the election period so that people could track the proportion of votes that were reported.

500
01:01:56,850 --> 01:02:01,590
But for us, we use this to determine the voting participation per county.

501
01:02:02,580 --> 01:02:05,310
By the end of the election period.

502
01:02:06,510 --> 01:02:17,520
So we combined this these total number of votes with the population data on the next slide to determine the county level voting participation rates.

503
01:02:18,630 --> 01:02:21,209
And then the next data file.

504
01:02:21,210 --> 01:02:30,060
The president county candidate specifies the specific candidates that were voted for and there were a bunch of candidates.

505
01:02:30,210 --> 01:02:33,660
We chose to focus only on Biden and Trump.

506
01:02:34,620 --> 01:02:42,660
Calculating the voting proportions with the denominator being the total number of votes between those two candidates only.

507
01:02:43,560 --> 01:02:49,230
And an important thing to note here is that there weren't any FIPS codes in these data files,

508
01:02:49,680 --> 01:02:55,380
so we had to merge the data with a list of FIPS codes by state and county name,

509
01:02:55,530 --> 01:03:02,610
which then we'll talk about presented some issues for us in the data cleaning process.

510
01:03:06,260 --> 01:03:18,049
And then, as I mentioned before, we use the U.S. Census Bureau 2020 population data to get the the eligible voters.

511
01:03:18,050 --> 01:03:27,350
So we able to pull the estimated population of people in each county that were over 18, 18 or older.

512
01:03:28,610 --> 01:03:33,230
And then we use this as the denominator in our voting participation rates.

513
01:03:36,230 --> 01:03:36,500
All right.

514
01:03:36,500 --> 01:03:43,670
So I'm going to talk about the data cleaning a little bit because as you know, especially in the FIPS codes, there was quite a bit of this to do.

515
01:03:44,450 --> 01:03:48,890
So there were really two main goals that we wanted to accomplish in our data cleaning.

516
01:03:49,640 --> 01:03:53,690
The first of which was reformatting our President County CSV file.

517
01:03:55,070 --> 01:04:01,850
So the main issue with this was that first of all, it contained quite a few more candidates than we needed to look at.

518
01:04:02,720 --> 01:04:06,730
There were a total of 37 distinct candidates as well as writers.

519
01:04:08,840 --> 01:04:13,819
And so this was all contained in one. This was all one variable that was just candidates.

520
01:04:13,820 --> 01:04:17,510
And then there was another variable called votes, which is how many each candidate received.

521
01:04:18,440 --> 01:04:24,110
So the data was presented to us in a long format, which wasn't quite what we wanted.

522
01:04:24,110 --> 01:04:30,620
So we wanted to transform this as well as only include the two major parties.

523
01:04:35,660 --> 01:04:46,760
Yeah. So we wanted to only include the two major parties and as well as get it into a wider format so that we could make the maps correctly.

524
01:04:47,970 --> 01:04:56,180
Yeah. Okay. And so as we discussed before, so there were quite a few issues with because we had three different data sets.

525
01:04:56,180 --> 01:04:59,270
We wanted to merge it on the lack of FIPS codes.

526
01:04:59,690 --> 01:05:13,250
I did present some some significance roadblocks to getting these, so we had to figure out how to get those FIPS codes in there.

527
01:05:14,330 --> 01:05:18,350
Additionally, in the President's County CSV file,

528
01:05:19,100 --> 01:05:26,150
there was an issue with single counties being split into several subsections in some cases that were nearly like

529
01:05:26,390 --> 01:05:34,100
eight or ten odd rows for a single county where I just subdivided a single county into several differently.

530
01:05:34,100 --> 01:05:37,220
Still call them counties, but they weren't really.

531
01:05:38,900 --> 01:05:46,430
So for example, I included you had Hancock County and Hancock County townships where they were still both included in the same county.

532
01:05:47,090 --> 01:05:53,660
But the townships oftentimes had somewhere on the order of like 10 to 20 people in the entire county.

533
01:05:54,740 --> 01:05:59,270
So we wanted to deal with that as well, to get it to sort of one zero per county.

534
01:06:00,860 --> 01:06:11,149
And then of course we have the inconsistencies in naming convention, which presented the problem when we were talking about the FIPS code.

535
01:06:11,150 --> 01:06:16,580
So the FIPS codes would have been preferable as they were a standardized method.

536
01:06:17,420 --> 01:06:21,710
But we did have to work with the state and county names instead.

537
01:06:23,990 --> 01:06:30,410
All right. So going through each issue. So the FIPS code was probably the easy one.

538
01:06:30,410 --> 01:06:35,280
So our problem is we don't have the codes, the solution merchants and FIPS codes.

539
01:06:36,050 --> 01:06:39,680
So this was done. It was I believe it was the part.

540
01:06:39,710 --> 01:06:44,150
It was tidy. Census has the data sets as tidy tidy census.

541
01:06:44,150 --> 01:06:50,840
Sorry, the R package. And then that included a data set with state county names and the PIPS codes.

542
01:06:51,500 --> 01:06:53,210
So that was how we included that.

543
01:06:53,990 --> 01:07:02,900
So we were able to merge that into President counties as indeed this did of course have still have the issue with the inconsistent naming,

544
01:07:03,680 --> 01:07:09,139
but the names here were more consistent than between the other two data sets that we wanted to merge.

545
01:07:09,140 --> 01:07:12,860
So this was the preferable solution. All right.

546
01:07:13,430 --> 01:07:16,460
The second one, so single counties being split into districts.

547
01:07:17,240 --> 01:07:23,510
I believe it was maybe I think there's like 3100 counties in the United States.

548
01:07:23,810 --> 01:07:27,020
Our data set started with about 4600 counties.

549
01:07:27,980 --> 01:07:31,310
That's more than there are counties in the United States.

550
01:07:32,300 --> 01:07:36,020
So that was an issue where, again, I discussed this,

551
01:07:36,350 --> 01:07:44,630
I believe it was District of Columbia was split in two around ten different areas within the District of Columbia itself.

552
01:07:45,650 --> 01:07:54,680
So we wanted to include these all as a single county. So we did have to go through and aggregate these data into a single county.

553
01:07:55,190 --> 01:07:58,270
You did didn't manually to do that. I know.

554
01:07:58,280 --> 01:08:03,310
So they were still so for District of Columbia, especially with all.

555
01:08:03,530 --> 01:08:06,800
Because it considered the state as District of Columbia.

556
01:08:07,070 --> 01:08:12,410
And then it then then it subdivided it from there so we could just collapse all of District of Columbia into one.

557
01:08:12,560 --> 01:08:17,540
Okay. Yeah. So we did it mostly by that.

558
01:08:18,500 --> 01:08:25,790
There were a couple like so we went through at the end just to make sure that our thing worked and we weren't able to get them all.

559
01:08:25,790 --> 01:08:30,770
So we did do a few manually at the end just to make sure that we got everything.

560
01:08:31,580 --> 01:08:35,120
Great things. Yeah. Okay.

561
01:08:35,130 --> 01:08:38,300
So inconsistency this one. I had to do a lot of manual stuff.

562
01:08:40,580 --> 01:08:46,040
Yeah. Looking back, this is this is something where we probably should have just gotten another data set with the FIPS code.

563
01:08:46,700 --> 01:08:50,950
Um, I don't know, but yeah.

564
01:08:50,960 --> 01:08:56,060
So this is the data that we were provided. So this is the data set we wanted to use, so we did.

565
01:08:56,090 --> 01:09:00,770
This was the probably the single largest issue was the lack of FIPS codes.

566
01:09:02,120 --> 01:09:08,150
So there were just a lot of inconsistent naming conventions between the data sets.

567
01:09:08,870 --> 01:09:12,140
The number one was inconsistent use of counting after the name.

568
01:09:12,710 --> 01:09:17,430
So again, going back to Hancock County as our example, one data set would say Hancock County,

569
01:09:17,450 --> 01:09:22,120
the other one would say Hancock that it would not merge on those variables.

570
01:09:22,130 --> 01:09:27,470
So a lot of what was done was doing, you know, just a naive merge,

571
01:09:27,470 --> 01:09:33,470
just trying to get them on initially and then going in see which ones didn't get matched.

572
01:09:33,590 --> 01:09:39,320
Why didn't they get matched? Go and fix that and then see what other issues came up.

573
01:09:40,070 --> 01:09:49,900
Another common one was difference in abbreviation. So if a county was called St something county on one data set would abbreviated as period one or

574
01:09:49,910 --> 01:09:56,790
the abbreviated SD and then also inconsistent spacing where the county name had multiple words.

575
01:09:56,810 --> 01:10:04,639
So you have something like DeKalb versus DeKalb, I hope I'm pronouncing that correctly, but and so yeah,

576
01:10:04,640 --> 01:10:10,160
this was really an iterative process where we go through just kind of make sure that it's getting everything.

577
01:10:10,700 --> 01:10:19,040
We do not want any missing data at the end more troublesome to create this kind of data, more valid data,

578
01:10:19,190 --> 01:10:28,009
because no people even bothered to downplay this because they take so much time and detail state to do this data leakage.

579
01:10:28,010 --> 01:10:38,360
And I think that we can popularly publish this data somewhere and this will be downloaded probably without high volume because people are lazy.

580
01:10:38,810 --> 01:10:45,380
They don't want do this. Yeah, I'm so happy that you guys spend time cleaning up and make all the right linkages.

581
01:10:46,130 --> 01:10:49,730
Oh, much as possible. Yeah. Oh, great. Messy data builds character.

582
01:10:49,730 --> 01:10:53,690
What can I say? All right.

583
01:10:54,710 --> 01:11:00,230
Yeah. So a lot of that was about recognizing patterns in the data and the inconsistencies.

584
01:11:00,230 --> 01:11:06,950
So then we were able to do some temporal data visualization because of the structure of our data.

585
01:11:07,340 --> 01:11:21,290
The most important part was the spatial data analysis, but we did use temporal data visualization kind of just as an exploratory data analysis.

586
01:11:22,190 --> 01:11:35,209
So here in this plot, you can see along the x axis this time the y axis is the vaccination rate in as a cumulative rate and it's split up.

587
01:11:35,210 --> 01:11:38,790
The colors are Biden in blue. Sorry, that's a little small area.

588
01:11:39,310 --> 01:11:49,670
Um, biden. So, so those are the counties where the majority of the county voted for Biden over Trump.

589
01:11:50,240 --> 01:11:56,930
And that red is the opposite. Um, and as you can see, this is supposed to be cumulative.

590
01:11:56,960 --> 01:12:02,660
There are a few dips which indicates some sort of data inconsistency there.

591
01:12:03,200 --> 01:12:06,560
Um, there's dips in both Biden and Trump kind of near the middle.

592
01:12:06,920 --> 01:12:19,610
And then, um, some overall trends that you can see are just the general higher vaccination rate among Biden majority counties.

593
01:12:19,610 --> 01:12:28,880
And then also after about June of 2021, the rates are pretty parallel to each other.

594
01:12:29,510 --> 01:12:37,510
So it kind of seems as though it's the initial vaccination rates, the slope of these vaccination rates.

595
01:12:37,530 --> 01:12:47,200
That is where the bulk of the differences in vaccination are happening so early on.

596
01:12:47,210 --> 01:12:52,340
And why this is very interesting because you look at the original arrangement

597
01:12:52,340 --> 01:12:57,470
of vaccination that basically allows senior people to get vaccination first.

598
01:12:57,860 --> 01:13:02,540
You can see that regardless of party affiliation, senior people tend to be.

599
01:13:03,340 --> 01:13:07,270
They are somehow more willing to take a vaccination.

600
01:13:07,270 --> 01:13:09,540
You can see that they're a very close third.

601
01:13:09,910 --> 01:13:18,820
When that you open to a general public population, that you just start to see the bifocal sort of pattern of younger people,

602
01:13:19,900 --> 01:13:25,690
you know, tend to be have a very different attitude towards the vaccination.

603
01:13:26,260 --> 01:13:29,490
So, yeah, very interesting to see that. Yeah. Yeah.

604
01:13:29,500 --> 01:13:36,639
Maybe for younger people, it's a little bit less of an immediate health issue and more of a it becomes

605
01:13:36,640 --> 01:13:41,170
more of a decision about which takes into account your political point of view.

606
01:13:42,790 --> 01:13:49,059
So, yeah. All right. It's map time. Okay. So given that, given the spatial nature of the data,

607
01:13:49,060 --> 01:14:00,040
we did create the maps for our primary outcome being the estimated hesitancy on that one's on the left, as well as the proportion of Biden voters.

608
01:14:00,040 --> 01:14:05,950
So more blue is more of that county proportionally voted for Biden, more red is more proportionally voted for Trump.

609
01:14:07,930 --> 01:14:14,290
And so I'll start with the the election map, because that's probably the most familiar with most people.

610
01:14:15,310 --> 01:14:18,340
And so we do see some general trends.

611
01:14:18,880 --> 01:14:24,640
A lot of the blue votes are concentrated near the coasts and in the I'm sorry,

612
01:14:24,640 --> 01:14:32,620
it's not very big and they are concentrated near the coasts and near big cities, which is typically what we expect from an election map.

613
01:14:33,490 --> 01:14:39,190
Just knowing demographics on a large majority of the middle of the country is very red.

614
01:14:40,540 --> 01:14:47,800
And then I think the spatial trend in the vaccination hesitancy is incredibly interesting.

615
01:14:47,950 --> 01:14:54,819
I remember I was showing this and someone commented on my use of a state level map for that and said and I said,

616
01:14:54,820 --> 01:15:01,360
no, that's county level because it is very heavily clustered within the states themselves.

617
01:15:01,720 --> 01:15:08,610
So you can see like hard boundaries. So there is a lot of clustering within the state I guess.

618
01:15:08,720 --> 01:15:12,640
So just use different color because in the election day that right.

619
01:15:12,970 --> 01:15:16,270
Yeah, it has red is just party now. Yeah.

620
01:15:16,640 --> 01:15:20,320
Intensity is you know this color person party.

621
01:15:20,560 --> 01:15:25,480
But on the left hand side of the counter, you know, directness represents the intensity.

622
01:15:26,200 --> 01:15:30,370
It probably is a little bit confused that I think that you use different colors that would be better.

623
01:15:31,180 --> 01:15:38,870
You can keep that blue and red. You can use other colors, green or some dark greenish things.

624
01:15:39,010 --> 01:15:42,820
Yeah, but that now probably a better. Yeah, I do agree and thank you for that.

625
01:15:42,850 --> 01:15:46,450
So we'll make sure to look into that for the final project.

626
01:15:47,440 --> 01:15:54,820
And then just to confirm what we saw with both of these, we did do a more eye test and it does confirm spatial dependance.

627
01:15:55,090 --> 01:15:59,350
So it rejected the null hypothesis of no spatial dependency in both.

628
01:16:03,610 --> 01:16:09,610
Okay. And we are winding down. We just did some very preliminary just looking at our two variables.

629
01:16:09,970 --> 01:16:16,420
Again, we did have some other variables that we're interested in and we're we're saving a lot of that analysis for the final project.

630
01:16:16,420 --> 01:16:24,160
So we did just want to get a very basic sort of interpretation of how our variables are related to each other.

631
01:16:24,670 --> 01:16:31,540
So this is just a simple scatterplot with the linear fits on the x axis, there is the proportion of Biden voters in the county,

632
01:16:31,960 --> 01:16:38,710
and our y axis is the hesitancy, and it's colored by who won the county.

633
01:16:38,710 --> 01:16:42,730
So it's just a split at the 0.54 proportion of Biden.

634
01:16:43,900 --> 01:16:46,450
And so we did fit the two models.

635
01:16:46,450 --> 01:16:53,020
So the linear regression you see here, that is the naive model where it doesn't take into account the spatial dependency.

636
01:16:53,470 --> 01:16:57,970
And you can see those the variable estimates in the top table on the right.

637
01:16:58,900 --> 01:17:05,680
And then we also fit our spatial model just to compare the results.

638
01:17:05,680 --> 01:17:08,259
And we do see both of them are significant.

639
01:17:08,260 --> 01:17:21,430
And so it does suggest that the proportion of Biden voters in the county is correlated with excuse me is correlated with lower vaccine hesitancy.

640
01:17:21,670 --> 01:17:27,639
And we do see that the effect size shrinks when we incorporate it into the spatial model,

641
01:17:27,640 --> 01:17:33,100
indicating that some of that dependency is in the spatial relationship.

642
01:17:33,100 --> 01:17:36,999
Well, this is anomalous, I think the outlier. So I'm more interested in general trends.

643
01:17:37,000 --> 01:17:47,850
Yeah, but yeah, I'm following some companies that are somehow against your theory or because it's very blue, but the president's very high.

644
01:17:47,860 --> 01:17:54,069
That's the Colby problem. We will see what's going on. Those all our companies probably more interesting this.

645
01:17:54,070 --> 01:17:57,250
Fine, I'll just use some random facts. Yeah.

646
01:17:57,250 --> 01:18:02,020
So there are yeah there are a couple. So you see like just a band all the way at the top there.

647
01:18:02,020 --> 01:18:05,720
Right. And you see the. It is primarily read, but once you do get over to that side,

648
01:18:05,760 --> 01:18:10,380
you do see a couple of blue counties that are right up there near the top for most hesitant.

649
01:18:10,710 --> 01:18:14,640
So that would be something that I, I wonder like, which are those?

650
01:18:15,360 --> 01:18:23,820
So counties that are stand out as a whole. WATERS Again, somehow, because the general point is very clear that you do have a winning margin.

651
01:18:24,180 --> 01:18:29,640
And so, yes, so the winning margin is just zero point win counties, right?

652
01:18:30,090 --> 01:18:33,480
Okay. So yeah, the proportional one. Yeah. Like so purple thing, right.

653
01:18:33,720 --> 01:18:40,080
Yeah. So the purple companies and. Okay.

654
01:18:41,670 --> 01:18:51,180
Yeah. So in summary, our goals for the final project, the first aim, which we've covered a lot of in this presentation,

655
01:18:51,300 --> 01:18:55,590
is to explore the spatial and temporal heterogeneity of our data,

656
01:18:55,620 --> 01:19:03,570
mainly through exploratory data analysis, looking at trends from the maps and other temporal trends.

657
01:19:04,290 --> 01:19:14,610
And then the second aim is to understand how the 2020 election results are associated with vaccine hesitancy using spatial modeling techniques.

658
01:19:14,880 --> 01:19:26,240
We also started this work as presented here and planning to continue and look into some of the other variables that we have in our data set.

659
01:19:28,200 --> 01:19:31,680
And then the third aim is taking things a step further.

660
01:19:32,220 --> 01:19:39,940
If we do find an association between election results and vaccine hesitancy, which we think we have.

661
01:19:40,870 --> 01:19:46,170
So so there are premiumness that were used for kind of like common questions.

662
01:19:47,550 --> 01:19:51,870
I always love them. This is yeah, this is the final final slide.

663
01:19:51,870 --> 01:20:00,090
Final sentence. Yeah. Because we think there is an association between the election results and vaccine hesitancy.

664
01:20:00,750 --> 01:20:06,600
And we know that vaccine hesitancy usually makes people not get the vaccine.

665
01:20:06,600 --> 01:20:11,730
And not getting a vaccine is associated with excess mortality due to COVID.

666
01:20:12,420 --> 01:20:25,140
We want to see if we can identify an association between election results and COVID excess mortality and quantify this association.

667
01:20:25,440 --> 01:20:30,720
So these are our aims for the final project and these are data sources.

668
01:20:31,590 --> 01:20:37,020
Thank you. I ask too many questions.

669
01:20:37,590 --> 01:20:44,770
Give the chance for you guys to ask questions. I just thought think was a good version of the plot.

670
01:20:44,830 --> 01:20:51,960
The first plot that you have, this one. They give you some over each one of you.

671
01:20:52,350 --> 01:21:02,309
Yeah. So. So we have the, we have the count of the vaccinations per county and then yeah, so we split by Biden, Trump.

672
01:21:02,310 --> 01:21:09,100
And then within those it was a total of all two vaccinated or total of all population within the county.

673
01:21:09,120 --> 01:21:12,270
So it's, it's kind of an overall vaccination rate.

674
01:21:12,600 --> 01:21:16,140
And in your data set, you only have Biden and Trump.

675
01:21:16,230 --> 01:21:20,580
You don't have other opportunities now.

676
01:21:20,670 --> 01:21:28,830
Correct. So again, so the the original data set did include up to I believe it was 37 plus, right in, so a total of 38.

677
01:21:29,730 --> 01:21:35,459
But the reality is that let's say one county, it was like 50,000 Biden, 50,000 Trump.

678
01:21:35,460 --> 01:21:38,700
And then the next highest would be like 1000, right? So.

679
01:21:38,950 --> 01:21:42,580
So your total is still the so like the total number of votes is.

680
01:21:42,600 --> 01:21:47,120
Did you change that? Do we. Yeah, yeah.

681
01:21:47,730 --> 01:21:53,370
I think I wish you all the special because I saw a lot of red dirt and very little blue.

682
01:21:54,000 --> 01:21:58,320
But yes, this one and blue is my favorite.

683
01:21:59,220 --> 01:22:02,730
But Biden won I yeah.

684
01:22:02,730 --> 01:22:06,720
So so that is so that is because it's so it's proportion of the county.

685
01:22:07,260 --> 01:22:12,329
But then if you have, let's say, like the county with New York City went 60%.

686
01:22:12,330 --> 01:22:16,110
Biden that's just a lot more votes. So it is proportional.

687
01:22:16,110 --> 01:22:20,790
So it is just proportional. This isn't like a, you know, votes per yeah,

688
01:22:20,790 --> 01:22:29,380
you would know it when you write the citizen test race because you don't wanna so you have sort of highlighted a problem.

689
01:22:30,150 --> 01:22:33,450
Yeah. Whereas the like so yeah.

690
01:22:34,110 --> 01:22:39,000
Yeah. So it is yeah. This is pretty. Oh yeah.

691
01:22:39,270 --> 01:22:45,690
But yeah, something like this is pretty typical where the blue counties tend to just be large cities and so they are.

692
01:22:45,910 --> 01:22:53,700
So those counties are just worth a lot more votes, right. As opposed to the middle of the country, which has like four people.

693
01:22:53,940 --> 01:23:03,840
Right. So but I think what is suggesting is useful looking at the purple countries where like if you have like plus minus two.

694
01:23:04,170 --> 01:23:08,399
So the difference is in the maybe it's by random chance or I don't know,

695
01:23:08,400 --> 01:23:15,760
like swing counties that could be live of a higher resolution than the current one that the food.

696
01:23:16,290 --> 01:23:19,439
Yeah thank you so much. We don't have time for extra questions,

697
01:23:19,440 --> 01:23:30,660
but I'm so impressed that it worked so far and I did do set up a meeting with me and to go over to pilot project and appreciation.

698
01:23:30,750 --> 01:23:34,600
Oh, thank you. Yeah, yeah. Okay. Oh, very good.

699
01:23:34,710 --> 01:23:38,460
Very good to go on email.

700
01:23:39,210 --> 01:23:48,940
So I did just one. I might like to know if it all happened.

701
01:23:50,820 --> 01:23:55,560
So for this model. So now, so basically from our reporting.

702
01:23:55,860 --> 01:24:04,250
Oh so in the one that I. So you have.

703
01:24:05,650 --> 01:24:12,820
No. You put it into Arabic if you haven't read a dictionary.

704
01:24:13,120 --> 01:24:17,650
Did you try not to do everything?

705
01:24:18,110 --> 01:24:23,170
He didn't say 45 I so I can say that I did.

706
01:24:23,500 --> 01:24:26,820
Now you're finding that you're okay. Okay.

707
01:24:26,860 --> 01:24:29,890
I just want to make sure, you know, you got the same version.

708
01:24:31,360 --> 01:24:40,219
I said, no. I mean, there's nothing in this.

709
01:24:40,220 --> 01:24:46,120
And I'm like, Oh, I know that too.

710
01:24:46,650 --> 01:24:56,280
Yeah, I got a little confused, but anyway, I gave you a card.

711
01:24:56,440 --> 01:24:59,760
Thank you. We are still meeting tomorrow.

712
01:24:59,860 --> 01:25:11,420
Yes. Thank you so much. Yeah, yeah. Oh, yeah, yeah, yeah, yeah, yeah, yeah, yeah.

713
01:25:12,050 --> 01:25:18,480
We're looking at it because I thought about what you just said.

714
01:25:18,550 --> 01:25:22,690
I don't want to. Yeah, I think I did when I was younger.

715
01:25:22,690 --> 01:25:25,809
I don't know how something will happen to Cambodia. I don't know.

716
01:25:25,810 --> 01:25:31,540
I think you got away from it next. Yeah.

717
01:25:31,540 --> 01:25:40,430
So, yeah, you first of all I would just like to know the address is will come snow maybe that day.

718
01:25:40,930 --> 01:25:52,690
Yeah. So you understand we we have I do think that there can be rights abuses when any worry.

719
01:25:52,900 --> 01:25:58,990
Oh I'd be happy to you know culture that I am one of you is this Burma.

720
01:25:58,990 --> 01:26:04,120
I think you are the school students you want to work.

721
01:26:04,660 --> 01:26:11,240
So I get that. So what the board said. Yeah, yeah, yeah.

722
01:26:11,560 --> 01:26:16,060
Oh, no problem. What you're doing here is on.

723
01:26:16,060 --> 01:26:20,820
Yeah. Very important. You know, you do a lot.

724
01:26:21,160 --> 01:26:32,080
But you think let me just turn off this wonderful recording and an excellent.

