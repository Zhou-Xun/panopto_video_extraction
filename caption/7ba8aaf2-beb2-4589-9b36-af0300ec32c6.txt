1
00:00:00,570 --> 00:00:12,560
Nothing really is like that or less just about you.

2
00:00:13,920 --> 00:00:17,819
Like I'm in the clinics.

3
00:00:17,820 --> 00:00:44,100
As soon as we had to spend so much time, I was going to continue doing this for a long time.

4
00:00:46,500 --> 00:00:58,319
Okay, very welcome. So. Yeah, so happy to have given them do some minutes after class 10 to 1030.

5
00:00:58,320 --> 00:01:05,219
[INAUDIBLE] be in my office and say hello or receive this talk or anything like that.

6
00:01:05,220 --> 00:01:11,910
Just come in the fourth floor and somebody will open up.

7
00:01:11,910 --> 00:01:16,770
He just posted it on the online camera side.

8
00:01:17,970 --> 00:01:23,220
Two things. Something.

9
00:01:28,970 --> 00:01:32,500
Cindy's home page. This is a way to introduce.

10
00:01:32,920 --> 00:01:39,490
I can find it simple. Just one line of text.

11
00:01:39,500 --> 00:01:48,460
It shouldn't be. I mean, that takes, you know, 12 seconds to localize sex services.

12
00:01:48,820 --> 00:01:52,780
I didn't mean for that to be my introduction. Well, I. That's fine. That's just the spirit here.

13
00:01:52,790 --> 00:02:02,139
So anyway, it's a mike is a statistician informatics person who used to be at Memorial Sloan-Kettering in New York City.

14
00:02:02,140 --> 00:02:08,799
And two years ago he moved to Cleveland and is the head of the Division of Health Sciences,

15
00:02:08,800 --> 00:02:12,640
I think, which is something like biostatistics epidemiology at the Cleveland Clinic.

16
00:02:12,640 --> 00:02:20,680
And they have been doing this spacecraft patients get seen, a lot of patients get seem to have a sort of a research portfolio going on there, too.

17
00:02:20,680 --> 00:02:27,969
And Mike's head of statistics thing and just sort of some sort of cute things that I thought were in this Web page.

18
00:02:27,970 --> 00:02:31,260
So book is out. Yeah. Yeah, please.

19
00:02:31,270 --> 00:02:35,130
So maybe, you know, you can get a discount today and a signed copy.

20
00:02:35,230 --> 00:02:44,590
That's true. And and then cute little things like number ten down here may not a statistic is bogus.

21
00:02:44,650 --> 00:02:57,950
Yeah and what is a real mammogram anyway so anyway anyways you want if you want to explore these sort of little provocative statements, it is.

22
00:02:58,180 --> 00:03:03,040
All right. So let's. You're mad if you want, and I'm fine.

23
00:03:03,040 --> 00:03:06,520
Just actually talk the talk so you can practice.

24
00:03:10,250 --> 00:03:14,360
Okay. So over to you. I think the right download technologically better.

25
00:03:40,650 --> 00:03:47,550
The sign up sheet is going around. Was there a customer who did the same things that happened here?

26
00:03:48,420 --> 00:03:57,150
A few minutes on right where I work, who lives for one of the sciences we loved?

27
00:03:57,330 --> 00:04:01,350
I mean, she can read right away. We're always hiring.

28
00:04:01,650 --> 00:04:07,500
We're a fairly big group. See, we have that here in 11.

29
00:04:08,520 --> 00:04:11,700
We have a lot of so this is like our store.

30
00:04:11,700 --> 00:04:16,430
We have weird staff meetings where, you know, you'll figure.

31
00:04:16,800 --> 00:04:20,760
So we have about 25 faculty, so we got about 31.

32
00:04:21,000 --> 00:04:26,490
These are basically master's level first decisions. We have 20 odd statistical programmers.

33
00:04:26,790 --> 00:04:34,229
We're a research group. We teach it to the medical school, so we teach bio stats and AP and stuff like that to medical students.

34
00:04:34,230 --> 00:04:41,370
But that's the extent of our teaching. Everything else is just research group, but we, we collaborate.

35
00:04:41,370 --> 00:04:47,430
It's a very, very collaborative environment. Notice sweatshop that's that's going too far.

36
00:04:47,850 --> 00:04:52,799
But it is very collaborative and collaboration. It is expected as well as rewarded.

37
00:04:52,800 --> 00:04:58,080
So we we're pretty good about that. But we work with clinicians writing papers.

38
00:04:58,260 --> 00:05:03,030
We get some protected time too. So it's just a it's a research group, but three hour drive, it's a very easy drive.

39
00:05:03,480 --> 00:05:07,340
So you should check us out some time now.

40
00:05:07,560 --> 00:05:11,280
That's it for the commercial. We get back to the show.

41
00:05:13,620 --> 00:05:16,950
So I've been doing statistical prediction models for one time.

42
00:05:17,340 --> 00:05:21,930
So I met Jeremy. I like Jeremy. Luckily, he has a very dry sense of humor, very dry.

43
00:05:22,470 --> 00:05:27,300
I, like a lot, goes a long way. So we've been in the same circles for a long time.

44
00:05:28,020 --> 00:05:31,380
I didn't choose to do this. It chose me.

45
00:05:31,980 --> 00:05:35,190
So this is day after Christmas, 1989.

46
00:05:35,190 --> 00:05:38,850
I'm in the middle. My two best friends at that time, that's Ricky and that's Matt.

47
00:05:39,300 --> 00:05:40,830
We're playing best after Christmas.

48
00:05:41,580 --> 00:05:52,050
The last picture I have of myself before I go to my family doctor, who at this point is my mother complaining of sweating at night, night sweats.

49
00:05:52,230 --> 00:05:56,820
Did you know there's a term called night sweats? Not just like. Hi.

50
00:05:57,200 --> 00:06:00,440
Like waking up at two in the morning and just soaked.

51
00:06:00,740 --> 00:06:06,080
Like it looks like I went for a run for an hour and got back under the covers and it was, like, drenched.

52
00:06:06,620 --> 00:06:12,110
And this goes on for, like, a week. And then Tylenol and aspirins and nothing shakes it.

53
00:06:12,710 --> 00:06:16,380
I call my mom and say, I'm sweating. You're living in Houston.

54
00:06:17,390 --> 00:06:22,970
You didn't. I told you not to go. No, no, I'm sweating at night with the air conditioner on under the covers.

55
00:06:23,600 --> 00:06:28,010
And she said she didn't go to college, she got medical school injury, anything at all like that?

56
00:06:28,100 --> 00:06:32,360
She has the Merck Manual Home Edition here now. Highly recommended.

57
00:06:32,360 --> 00:06:33,410
So she gets I got this.

58
00:06:33,970 --> 00:06:42,020
She she calls out the Merck manual home edition and she goes, yeah, it's got night sweats and I need you to check your armpits.

59
00:06:43,100 --> 00:06:51,589
Okay. And so I'm fortunate because the only thing I have no same day Mark that he gave me and she goes, okay, that's great.

60
00:06:51,590 --> 00:06:57,320
That's great. I feel your neck is the only thing in your neck. And I go, Well, on one side, there's a rock.

61
00:06:57,620 --> 00:07:01,669
There's a rock sticking out of my neck right at the edge of my collarbone.

62
00:07:01,670 --> 00:07:05,210
It's kind of a big rock. And I have it on one side. I don't have it on the other side.

63
00:07:05,920 --> 00:07:10,670
There's a rock in my neck. She goes, okay, well, I have to refer you now to a different doctor.

64
00:07:12,350 --> 00:07:16,940
Long story short, it turns out to be Hodgkin's disease, cancer of the lymph nodes.

65
00:07:16,940 --> 00:07:21,200
One of them, which is is going to get cancer.

66
00:07:21,200 --> 00:07:26,180
That's a hard try. I do this myself. Kaplan-Meier curves very poorly.

67
00:07:26,420 --> 00:07:30,590
That is the example of how not to do it. It's really students.

68
00:07:30,980 --> 00:07:40,400
But the interesting thing was I the stage for like nothing hurt I didn't wasn't really even tired is playing basketball

69
00:07:40,400 --> 00:07:46,730
I was working out everything was good I was in graduate school in a Ph.D. and I wake up with these night sweats,

70
00:07:46,910 --> 00:07:50,870
it turns out. So it was going okay in the testing until I have a spot on a rib.

71
00:07:52,190 --> 00:07:57,110
Phones can't close because it's over here, which now everything changes.

72
00:07:57,320 --> 00:08:01,970
So all the debates about we take your spleen out, I want to know is does Barrymore read one?

73
00:08:02,450 --> 00:08:08,839
You're now looking at a year of chemo just under and as every other week and we can get rid of it.

74
00:08:08,840 --> 00:08:13,160
But this is going to be a path it is before we had the fancy anti-nausea drugs and all that stuff.

75
00:08:13,700 --> 00:08:19,729
Now it's a little better, but there will be radiation after that anyway. This experience, though, this is 89,

76
00:08:19,730 --> 00:08:26,540
98 treated Indians opened my eyes to how poor of a job we typically do in

77
00:08:27,290 --> 00:08:32,330
counseling patients and giving prognosis and just answering what are your chances?

78
00:08:32,480 --> 00:08:37,100
And it gets very frustrating because we're over here doing all the modeling and all this stuff.

79
00:08:37,100 --> 00:08:44,509
And I was actually in the business school, which is super duper modeling in the sense of predictive accuracy.

80
00:08:44,510 --> 00:08:48,229
And all that is very high priority because that's how you make money over there.

81
00:08:48,230 --> 00:08:50,110
So they really care about you don't care.

82
00:08:50,180 --> 00:09:01,580
Explainability All I want to do in that world is that predict you jerks damage in a holding if I can predict you is all for is flat out prediction.

83
00:09:01,850 --> 00:09:07,850
If I can now predict to you this much and hold it, I'm going to drive you into the ground and take all your money.

84
00:09:08,120 --> 00:09:12,700
And that's my goal. That's the name of the game over there. You come over here as well?

85
00:09:12,710 --> 00:09:17,960
Yes. Disease and over here and it is all loosey goosey, whether you're in the stage bucket.

86
00:09:18,500 --> 00:09:27,469
And it just was driving me nuts and it sent me Rampage when I was sitting with in the waiting room because I asked my doctor,

87
00:09:27,470 --> 00:09:31,270
is this something I need to be worried about because everyone's getting freaking out here?

88
00:09:32,000 --> 00:09:37,730
And he said, Well, you do with if your mom has this agent has the revenue,

89
00:09:37,850 --> 00:09:42,050
that's how that go and find out your stage and you look it out and that's the picture of this thing.

90
00:09:42,950 --> 00:09:53,930
And I'm in the waiting room with a much older lady because I'm 24 and I'm in, whereas she's she's 90, she's confined to a wheelchair,

91
00:09:54,230 --> 00:10:03,020
his oxygen tank in the back, it comes up and it's raining or she's not playing basketball, I think because she's stage four diabetic or whatever.

92
00:10:03,020 --> 00:10:08,600
It's very next. The same way Ruby, the same oncologist. I go in there full guns blazing.

93
00:10:08,960 --> 00:10:21,470
Hey, doctor. I was at the stage for the when she says dark dockworker my chances you give it the same graph just try to understand it.

94
00:10:21,470 --> 00:10:28,910
You said, look at that. So you tell me to look at that. Yeah. Well, back then, like, we're not adjusting for all the co-morbidities.

95
00:10:28,910 --> 00:10:35,850
And and you know what? Basically you're I'm going to treat you as a for which you not a classic for whatever

96
00:10:35,970 --> 00:10:40,390
it basically you don't have it below your diaphragm and it's not in your blood level.

97
00:10:41,000 --> 00:10:44,360
You said this once, but I don't even know if that spot on the rib is real. Let's just.

98
00:10:45,020 --> 00:10:48,230
You're bad, too. That's what you are. You're a bad too. You're not real.

99
00:10:49,610 --> 00:10:57,900
But there's no curve for back to and this is just getting worse and and so you'll see where this is going to go and Matt not taking name.

100
00:10:59,340 --> 00:11:04,350
Because it's not tailored to me. It's like adjusting for things I think should be adjusted for.

101
00:11:04,590 --> 00:11:11,729
These categories are very bright. You can barely be a stage four, apparently, or you can be a slam dunk stage for the council.

102
00:11:11,730 --> 00:11:15,390
Exactly the same treatise, more or less exactly the same.

103
00:11:16,020 --> 00:11:20,160
It's all a bucket. You go in and depending on what bucket you're in, here's what you do.

104
00:11:20,640 --> 00:11:24,570
You don't like to be tailored, right? Take everything you know about me.

105
00:11:25,560 --> 00:11:29,100
Into account and then decide what's best for me. What are the pros and cons?

106
00:11:29,430 --> 00:11:33,860
What if I do this? What if I do that? What do you think's going to happen? And let's look at side by side like we do at Amazon.

107
00:11:34,300 --> 00:11:38,430
And now buying a camera will see a side by side. I can't do that here.

108
00:11:38,820 --> 00:11:43,680
I should be able to. This is a much bigger deal than buying a camera because I only get to buy the camera once.

109
00:11:44,640 --> 00:11:50,280
If I can't return it, I call Jeff Bezos and say, Camera back, no, you get one camera.

110
00:11:50,910 --> 00:11:55,440
So this better be the right. You not have to. We do it all the time.

111
00:11:56,170 --> 00:11:59,390
There's all sorts of prescription going all across medicine.

112
00:12:00,260 --> 00:12:00,960
We've always done it.

113
00:12:01,410 --> 00:12:11,330
And, you know, it's not anyone's fault as to the doctors to pass the test, have to memorize these things, and you got to be able to regurgitate them.

114
00:12:11,350 --> 00:12:15,290
What's a high risk patient with so-and-so? Oh, it's one who has this and then progressed.

115
00:12:15,450 --> 00:12:20,160
You're your real doctor now or whatever, how that works. So it's it's it's ingrained.

116
00:12:20,230 --> 00:12:25,680
And and it, you know, just aging systems do that.

117
00:12:26,040 --> 00:12:29,069
But if we just did more modeling, the world would be a better place.

118
00:12:29,070 --> 00:12:33,270
And I'll trying to convince you that that's something we can all help fix.

119
00:12:34,200 --> 00:12:37,710
By the way, ask questions. Anytime you're at me, any time.

120
00:12:38,580 --> 00:12:44,130
I'm still trying to figure this stuff out. So the question that was shouted.

121
00:12:47,430 --> 00:12:51,510
Well. So what do we do here, folks? Well, start out.

122
00:12:52,020 --> 00:12:56,460
Figure out the most accurate model we can. Whatever that is, it is machine learning.

123
00:12:56,470 --> 00:13:02,960
Maybe it's not major regression. So I'm pretty agnostic about that, but predict as accurately as you can.

124
00:13:02,970 --> 00:13:06,240
So it's not goodness if it really.

125
00:13:06,960 --> 00:13:15,420
And its predictive accuracy is a little different. So concentrate on measures that really reflect that as best we can and figure out what the best.

126
00:13:16,520 --> 00:13:21,230
Mousetrap is that you can build. Now, we got to take that into the clinic.

127
00:13:21,500 --> 00:13:27,890
So it's kind of two steps there because ultimately it's the clinician's decision to use your tool.

128
00:13:29,070 --> 00:13:33,260
Before it actually gets done. We can't force anything up on the clinician, the doctor.

129
00:13:33,270 --> 00:13:36,120
We can't say you have to use Jeremy's model.

130
00:13:37,500 --> 00:13:44,220
The clinician has to opt to use that because it's the clinician that's on the hook or the responsible party for making the right,

131
00:13:44,580 --> 00:13:50,250
helping the patient make the right decision. That's the person who gets sued or what not if things don't turn out right.

132
00:13:50,500 --> 00:13:54,330
And so it's it's right. That person's call to use the model or not.

133
00:13:54,630 --> 00:14:01,410
We can educate and advise and do all we want, but it's really end of the day, that person has to want to use your tool.

134
00:14:02,730 --> 00:14:06,900
So we have to make it convenient. I'll show you some numbers. Grams I love milligrams of cool.

135
00:14:07,380 --> 00:14:10,470
But better is really standalone software.

136
00:14:11,700 --> 00:14:21,090
A website is pretty fun. But ideally, just just build it into the electronic health record so that it's no extra step doctor taking care of patients,

137
00:14:21,540 --> 00:14:24,900
entering things and results in pulling in lab tests and all that stuff.

138
00:14:25,470 --> 00:14:31,410
Predictions automated up there in the corner. You can as a clinician, you can look at them or not.

139
00:14:31,440 --> 00:14:35,640
You can act on them or not. But there is no extra work to get them.

140
00:14:36,090 --> 00:14:39,420
That's the ideal scenario. It's doable.

141
00:14:39,630 --> 00:14:45,300
It's a little harder. I'll show you an example of that. But so I'm pretty.

142
00:14:45,300 --> 00:14:49,560
I get a little sloppy. I think this is our preferred terminology here.

143
00:14:49,560 --> 00:14:55,320
We all get a data set. We build some sort of a model that produces an equation.

144
00:14:55,510 --> 00:14:59,610
We display it maybe as a number and we'll put it into an online risk calculator.

145
00:14:59,610 --> 00:15:02,790
And I call that statistical collection models. But.

146
00:15:03,600 --> 00:15:11,410
I apologies when I saw some of those. So you have been doing this since 19.

147
00:15:12,980 --> 00:15:13,790
Three or so.

148
00:15:14,600 --> 00:15:26,059
I was much more of a machine learning fan in that I'm around the early nineties, so I was heavy into like kart and artificial neural networks.

149
00:15:26,060 --> 00:15:28,490
Those were all the race to doing a lot of that stuff.

150
00:15:29,210 --> 00:15:38,240
Um, and I could, I could simulate data where that worked, where that made a difference, like it would do better.

151
00:15:38,270 --> 00:15:45,580
Like if I, if I simulate data and had an interaction and input the interaction in the regression model and then I compared it against cart.

152
00:15:47,070 --> 00:15:50,220
You know, the decision tree software. It'll find that interaction pretty well.

153
00:15:50,580 --> 00:15:54,200
It didn't predict my regression model where I didn't know it was there and didn't at the end.

154
00:15:54,210 --> 00:16:00,270
So that kind of makes some sense. So we don't know where all the interactions are and it'll, it'll find them very well there I think.

155
00:16:00,960 --> 00:16:04,560
Or if there is a, if I simulate data with a cool nonlinear relationship,

156
00:16:05,370 --> 00:16:09,389
the neural network would find that if I didn't and if I don't put in the regression model,

157
00:16:09,390 --> 00:16:13,260
the neural net win so I could simulate data where it would do what I thought it should do.

158
00:16:13,680 --> 00:16:18,610
But then every time I kept getting data from Mother Nature. It wasn't panning out.

159
00:16:19,210 --> 00:16:23,100
So the regression model would just would be at least tied if not win.

160
00:16:23,320 --> 00:16:26,410
It's very frustrating because I can cut that out quick.

161
00:16:26,920 --> 00:16:35,380
But the other stuff I had weigh in on use and do all sorts of fiddling around only to get something that was not quite as accurate typically.

162
00:16:36,130 --> 00:16:42,190
And my whole soapbox was accuracy, since I will have to justify the cool thing less accurate.

163
00:16:42,250 --> 00:16:48,400
That's a tough sell. So to fill that favor, I'm kind of keeping my toe back in the water.

164
00:16:48,760 --> 00:16:57,579
The point is, over here, we try to do things where we're trying to promote predictive accuracy.

165
00:16:57,580 --> 00:17:03,790
So you're making a regression model, a similar logistic path to keep your continuous variables continuous.

166
00:17:04,860 --> 00:17:08,970
I don't. What's a good rule? Why would you chop them up or dichotomous them?

167
00:17:09,060 --> 00:17:13,230
If your goal is accuracy again, that's we're shooting for he's going to lose information.

168
00:17:13,440 --> 00:17:20,879
I don't see how that can be a good thing very often, but don't necessarily force it to have a linear relationship.

169
00:17:20,880 --> 00:17:27,870
So you can relax that a little bit, like with lines and things like that. So if you need some non-linear effects, put them in like that.

170
00:17:28,230 --> 00:17:32,680
It's a little harder to explain. What's the hazard ratio?

171
00:17:32,700 --> 00:17:36,900
Well, there's not just one anymore. You get to pick your two points you want to compare.

172
00:17:37,870 --> 00:17:44,490
That's kind of a hassle. But again, predictive accuracy is the goal. I do know variable selection that, you know.

173
00:17:45,150 --> 00:17:45,450
You know,

174
00:17:45,450 --> 00:17:52,349
what's what's significant by itself and what does go to the multivariable but nope don't do that don't do any variable selection in the multi verbal.

175
00:17:52,350 --> 00:18:00,570
Now business is business. I go I make the clinician who is a subject matter expert tell me how the world works.

176
00:18:00,870 --> 00:18:09,150
So I try to get that guy or gal to basically draw me a little gag, you know, a direct in a segment graph, just a crude one.

177
00:18:09,510 --> 00:18:13,530
But I think this I think a bigger tamer is worse because I can't it's harder to get it out.

178
00:18:13,740 --> 00:18:17,790
All right. Tumor size again. And then you start listening for other things.

179
00:18:18,000 --> 00:18:21,090
And then finally the clinician says, oh, yeah, go for your shoe size.

180
00:18:21,210 --> 00:18:25,290
Get the shoe size. Oh, okay, cool. What's the deal?

181
00:18:25,630 --> 00:18:30,660
I think the dangerous or protective, how's the foot thing work?

182
00:18:31,380 --> 00:18:37,800
Oh, I have no idea. But the Germans found that shoe size was important, so we need to put that in our mouths and we'll go back up.

183
00:18:38,340 --> 00:18:47,970
So you can't put it on your whole thing. Here is no rationale for shoe size here, but some other study, you know, how are they doing it?

184
00:18:48,180 --> 00:18:51,190
Lord knows claims shoe size. We don't need them.

185
00:18:51,390 --> 00:19:00,840
And it's not in your in your day. The thing is out and try to get them to specify a direction of effect because they say, oh, we need to print,

186
00:19:02,250 --> 00:19:11,400
we need to put some continuous let's go make excuses easy because she says, okay, which way to go?

187
00:19:12,670 --> 00:19:16,889
Well, I don't know. I don't know the direction of effect. So I don't know if it's bigger.

188
00:19:16,890 --> 00:19:24,150
I know it's important. It's very important. But I don't know if it's if if a larger shoe fit is is is good or bad.

189
00:19:24,540 --> 00:19:28,559
All right. Well, if you can't articulate the direction of effect, probably shouldn't be in there either.

190
00:19:28,560 --> 00:19:37,550
So I'm cutting those out, too. So you start listening to little tricks like that and listen to them specify an interaction without using those words.

191
00:19:37,560 --> 00:19:40,620
You know this matters, but only if that's their hug.

192
00:19:40,740 --> 00:19:46,860
That's very helpful. Anyway, so not only devalues and there's this and it's all about predictive accuracy,

193
00:19:46,860 --> 00:19:52,080
I think that gets you to the most accurate model on patients in the future.

194
00:19:52,200 --> 00:19:58,169
That's the rationale kind of hypothesis, but that's the argument for making some of these decisions.

195
00:19:58,170 --> 00:20:02,820
Even though your data might be suggesting to go a slightly different path,

196
00:20:03,540 --> 00:20:09,360
the goal is predictive accuracy theoretically on the future patients, not per say on the patients that we have here.

197
00:20:09,370 --> 00:20:17,110
So there's a little philosophy in there, but it's very well mentioned in Frank Carroll's book Regression Modeling Strategies.

198
00:20:17,130 --> 00:20:20,500
Got to get you to have that, but you haven't got to get it.

199
00:20:21,990 --> 00:20:30,660
So we need some measure of performance. I think the classic one is still is this concordance index.

200
00:20:30,900 --> 00:20:37,380
So most of the things that we did, I find the clinicians want to predict are kind of time to an end.

201
00:20:37,590 --> 00:20:46,050
It is most and not all, but most of our stuff. That boy, if I could predict who's going to relapse, recur, die, you know,

202
00:20:46,140 --> 00:20:50,550
some of this future stuff that if I could do that, that would really help me right now.

203
00:20:50,880 --> 00:20:56,490
Is it? Because that's kind of what I'm hinting. My thing, that's kind of it usually the most interesting thing,

204
00:20:56,820 --> 00:21:01,350
but it's also kind of the hardest to think about in terms of a protective factors measure.

205
00:21:02,130 --> 00:21:03,950
But the concordance index is still out there.

206
00:21:03,960 --> 00:21:14,490
It kind of stood the test of time so that so that one's pretty easy though if in the sense of I have a test set,

207
00:21:15,240 --> 00:21:23,820
I test I have predictions for my prediction model, I have the outcome I have did the person recover or not?

208
00:21:24,180 --> 00:21:30,740
And the follow up time, that's what I'm working with. So the concordance index just says take all possible pairs of patients.

209
00:21:30,750 --> 00:21:34,530
Parent one with two, one with three, two with four, which is all possible.

210
00:21:34,530 --> 00:21:41,070
Pairs only keep the pairs where the patient with the shorter follow up time has the advantage.

211
00:21:41,980 --> 00:21:49,660
So. So you exclude the ones where the patient with the shorter follow up time got since is in that payer.

212
00:21:49,660 --> 00:21:54,640
I don't know exactly who did better, but information with the shorter follow up time has the event.

213
00:21:55,150 --> 00:22:01,059
I know that person did worse. If it's a bad outcome, then the patient with a longer follow up time,

214
00:22:01,060 --> 00:22:07,840
whether that person had the event or not because they they out lived or whatnot, the first patient in the pair.

215
00:22:07,840 --> 00:22:13,000
So in that pair of patients, I should predict a worse outcome for that first patient.

216
00:22:13,330 --> 00:22:15,610
So it's just a proportion of payers where I got the ranking right.

217
00:22:15,850 --> 00:22:20,709
That's why it's 4.5 to 1 because I'll get half of them right coin because it's all possible payers.

218
00:22:20,710 --> 00:22:25,450
I have a clear winner in each pair. It's 5050 with no nothing.

219
00:22:25,920 --> 00:22:30,900
Perfect for me to be one. So that that's out there, I use it a lot.

220
00:22:31,470 --> 00:22:35,790
The catch is it's just discrimination. It's a big catch, the first catch.

221
00:22:36,240 --> 00:22:40,080
So it doesn't really get at my calibration. So when I say 20%, am I really right?

222
00:22:41,010 --> 00:22:46,980
It doesn't matter. This game is predicting worse for the patient who did worse, so we've got to do some more work after this.

223
00:22:47,550 --> 00:22:51,810
Second thing is the time point does not come into play here.

224
00:22:52,200 --> 00:23:01,220
So and this is kind of meant for if a cock smile or something where where you have the proportional hazards assumption.

225
00:23:01,260 --> 00:23:05,130
So if you're predicted at risk at any point, you're always predicted to be higher.

226
00:23:05,580 --> 00:23:10,590
So the two year prediction accuracy measure could be different from the five year accuracy prediction,

227
00:23:10,590 --> 00:23:16,620
but that doesn't come into play here, simply time to event. So it's not specific to a prediction horizon.

228
00:23:16,620 --> 00:23:19,470
It's kind of a drawback too, but it's out there.

229
00:23:21,750 --> 00:23:32,280
But the cool thing is when you start looking at these comparisons of groups with with grouping systems,

230
00:23:32,280 --> 00:23:34,980
with individual systems and some cool things are.

231
00:23:35,640 --> 00:23:45,060
So here's a study where we made a Cox model and predicted recurrence in men treated with prostatectomy.

232
00:23:45,960 --> 00:23:54,210
And so there's, there's a continuous prediction tool there called the gnome gram up against the staging

233
00:23:54,210 --> 00:23:59,430
system or of sorts is more of a risk grouping system that puts the patient into low,

234
00:23:59,430 --> 00:24:05,370
intermediate or high. This one in particular is the Tamiko grouping system.

235
00:24:05,820 --> 00:24:10,380
So you can you can place the patient into a low risk group and intermediate risk group in a high risk group.

236
00:24:10,770 --> 00:24:14,730
So now I have a test if I got the group lowering me, huh?

237
00:24:15,420 --> 00:24:19,820
I got the prediction from the Cox model for five years.

238
00:24:19,830 --> 00:24:26,350
So it's a 0 to 1 for five year risk and I have event and follow.

239
00:24:26,820 --> 00:24:36,570
So I can do a concordance index on each of those. Head to head to tell me which ones at least discriminating better in that category by that metric.

240
00:24:39,030 --> 00:24:40,319
This was kind of a no contest.

241
00:24:40,320 --> 00:24:50,790
The continuous one out predicts the categorical one, probably in part because a tie hurts you as a categorical predictor.

242
00:24:52,030 --> 00:24:59,320
But I think that's fair because if you're lumping and you shouldn't be lumping, you should be penalized for that.

243
00:24:59,890 --> 00:25:03,850
If you can split, you should split. That's a kind of a four way of explaining it.

244
00:25:03,860 --> 00:25:13,270
But if you so if I predict the same, if I if you're in the same risk group by the concordance index, you get 2.5 for that.

245
00:25:13,540 --> 00:25:20,200
So that's not so good. But should you be grouping if you don't need to group?

246
00:25:20,650 --> 00:25:28,030
It's kind of theoretical. Anyway, I just by concordance index you've got to go continuous in that sense.

247
00:25:28,360 --> 00:25:31,840
But this is the implication for doing that. So the X is a patient.

248
00:25:32,590 --> 00:25:37,340
So here is. It's not so bad over here.

249
00:25:37,610 --> 00:25:44,330
Everybody in the low risk group has a just has a relatively good prediction of freedom from recurrence.

250
00:25:45,480 --> 00:25:51,030
But in the intermediate risk group, there's a lot of heterogeneity there and it's even worse in the high risk group.

251
00:25:51,840 --> 00:25:56,130
So this poor guy, Joe, he's in a high risk group.

252
00:25:57,120 --> 00:26:01,440
Made a continuous smile. His chances of making it five years without recurrence are about 1%.

253
00:26:02,430 --> 00:26:10,110
It's not good. But Pete, here he is in a high risk group, but by a continuous model which discriminates better.

254
00:26:11,070 --> 00:26:13,340
I don't know if he's got a 95% chance of making it five.

255
00:26:14,830 --> 00:26:22,480
But if you're using this Tamiko grouping system to treat your patients, these guys are getting the same treatment.

256
00:26:22,960 --> 00:26:30,250
I like that. If this is real, I would prefer to treat based on risk and not the group that you fall into.

257
00:26:30,610 --> 00:26:34,000
But this is going on all the time. It's going on right now. We know it.

258
00:26:34,030 --> 00:26:38,590
I mean, so I'm not a fan of that.

259
00:26:38,830 --> 00:26:43,930
And I think if we can get rid of some of that, will treat patients better and make better decisions.

260
00:26:44,620 --> 00:26:53,470
So let's pick out an example. Here's a guy from our shop. So he has a PSA of six.

261
00:26:53,710 --> 00:27:01,140
It's pretty good. The rest of this stuff, if you're not a prostate cancer person, is is not so good since PSA is pretty good.

262
00:27:01,230 --> 00:27:08,510
That's a good number. The rest of these features not so great, but when he's he's trying to make a decision.

263
00:27:08,520 --> 00:27:13,980
He is he is recently diagnosed with an early stage prostate cancer.

264
00:27:14,010 --> 00:27:22,530
Should he get surgery? Should he get radiation? So you just watch. It is a very hard decision, very confusing and lots of arguments about that.

265
00:27:23,080 --> 00:27:27,760
But one thing is, you certainly have to get multiple opinions, pretty decent data.

266
00:27:27,830 --> 00:27:32,330
If you go straight to the radiation oncologist, you get radiation, you get a surgeon, you get surgery.

267
00:27:32,340 --> 00:27:37,710
There's bias in that issue. So check them all out is the first thing to do.

268
00:27:38,100 --> 00:27:42,300
So he's on. He's going to go do that. He goes first to radiation.

269
00:27:42,360 --> 00:27:47,620
Check it out. The radiation, they pull out paper from JAMA.

270
00:27:48,740 --> 00:27:52,240
I don't think you're on it. Jeremy I'm not either, but it was there.

271
00:27:52,550 --> 00:27:55,810
That 2,000% is new is in JAMA Internal Medicine.

272
00:27:56,300 --> 00:28:05,590
It was a cool Carter treated, uh, the, the early machine learning stuff and they built a nice little tree in there.

273
00:28:05,600 --> 00:28:09,209
And the radiation oncologist pulls out the jam paper.

274
00:28:09,210 --> 00:28:13,640
It says, Here you go, Bill. You good news. You're in a favorable category.

275
00:28:13,880 --> 00:28:19,000
When I drop you down the country, the first tree, the current split is PSA.

276
00:28:19,340 --> 00:28:24,260
And it's it's it's like 6.1. So he just gets under the window.

277
00:28:24,980 --> 00:28:29,959
Boom, he's over there. That's all you need to know for favorited. He doesn't do any splits of that part of the tree over there.

278
00:28:29,960 --> 00:28:33,110
So he's home free because he came in just under the 6.1.

279
00:28:33,140 --> 00:28:37,630
Nothing else matters. So he's in the favorable risk group by machine learning and jammer.

280
00:28:38,060 --> 00:28:45,230
What do you how are you going to beat that? So he says, what if radiation oncologist says, when do you want to start?

281
00:28:45,710 --> 00:28:52,190
Because it could last a month or so, start tomorrow. The patient wisely says, No, I'm going to get a second opinion.

282
00:28:52,240 --> 00:28:55,280
I go talk to the urologist and see what surgery is like for me.

283
00:28:55,730 --> 00:28:58,879
So he says, I go do that goes overseas.

284
00:28:58,880 --> 00:29:06,350
A surgeon, surgeon, she pulls out a no, runs it through that, taking other everything into account, says,

285
00:29:06,350 --> 00:29:12,380
Well, if I treat you a surgery, you have about a 68% chance of freedom from recurrence in five years.

286
00:29:13,280 --> 00:29:17,540
When would you like to get your surgery? I can do tomorrow, she says.

287
00:29:17,540 --> 00:29:26,190
Now. Amount of radiation. Patients knocked down and the surgeon says, why would you do that?

288
00:29:27,420 --> 00:29:31,469
68% chance of freedom from recurrence. Oh, no, it just came from radiation.

289
00:29:31,470 --> 00:29:42,360
And they said it's 81 at five years, if I get radiated and they use a machine learning tree and jam, so that's where I'm going.

290
00:29:43,310 --> 00:29:47,180
Yeah. The this is a good this is a very smart urologist, by the way.

291
00:29:47,180 --> 00:29:52,520
She. She says, well, did they compare that treat with a regression model?

292
00:29:54,170 --> 00:30:00,630
You don't don't doesn't that person know the head to head the regression model out discriminated the tree.

293
00:30:02,060 --> 00:30:03,790
Patient says, I don't think they knew that.

294
00:30:03,810 --> 00:30:10,820
And she says, You mind if I run you through the progression model for radiation, which is got a better concordance index than the tree does?

295
00:30:11,450 --> 00:30:16,189
Patient says, I'll be my guest. I'm sure it's going to be 79 or 82.

296
00:30:16,190 --> 00:30:19,700
It's going to be close to the 81. It's not is 24.

297
00:30:21,030 --> 00:30:26,030
Wait. So the 81 is wrong. I don't know, man.

298
00:30:26,810 --> 00:30:33,170
I got two models compared head to head. The third one is got a better concordance index in the first one.

299
00:30:33,590 --> 00:30:38,260
I got to give you one number. My number four use 24.

300
00:30:38,560 --> 00:30:42,610
It's not. Forget the 81. Right.

301
00:30:42,970 --> 00:30:47,560
Let's see what you got to do. Oh. So. So now it's 24 versus 68.

302
00:30:47,560 --> 00:30:51,910
It's not 81 versus 68. Doctor declared prostate.

303
00:30:53,280 --> 00:30:57,420
I since this is all kind of made for TV. Numbers are real, though.

304
00:30:58,660 --> 00:31:02,680
I see the confusion. Who goes to this level of complexity?

305
00:31:03,730 --> 00:31:09,490
We're lucky if the patient gets any one of these numbers, let alone go through the process of competing models.

306
00:31:09,490 --> 00:31:13,970
Which model predicts better? Which one do I trust more? What are my chances?

307
00:31:13,990 --> 00:31:18,940
I just. But I would like to see these things head to head if I'm the one making the choice, because this is a big deal.

308
00:31:19,050 --> 00:31:25,450
Again, you only get one, one bite at this table. So I really want to do the best you can.

309
00:31:25,670 --> 00:31:29,970
Quick example. This stomach cancer staging system.

310
00:31:29,980 --> 00:31:33,520
Okay. Typical. A progression model.

311
00:31:35,120 --> 00:31:39,379
Same issue, but now I'm taking everything a little better.

312
00:31:39,380 --> 00:31:43,340
Concordance index on the original dataset.

313
00:31:44,780 --> 00:31:50,870
Didn't we publish this paper? And then these Dutch folks saw that we got a validation set for you.

314
00:31:50,880 --> 00:31:53,860
Okay. It's kind of a similar story.

315
00:31:54,250 --> 00:32:02,620
But once again, that kind of weird little take home message is this heterogeneity issue that folks like you and me can help resolve.

316
00:32:03,340 --> 00:32:07,600
So if you're a one, a stomach cancer patient, things are pretty good.

317
00:32:08,530 --> 00:32:11,140
I think you're probably safe with whatever they're going to do.

318
00:32:11,740 --> 00:32:20,230
The catch is, if you're at two because because the treatment guidelines, they draw the cuts horizontally,

319
00:32:20,800 --> 00:32:27,700
like you get the so-and-so treatment if you're above two where the such and such treatment if you're up three.

320
00:32:27,880 --> 00:32:32,320
So they're drawing the lines this way. So you might if you're in two.

321
00:32:33,370 --> 00:32:38,110
You're going to get treated like all these other folks. And you could be this to.

322
00:32:39,180 --> 00:32:43,680
Or you could be that, too. But you're two Jews get the same treatment.

323
00:32:43,830 --> 00:32:49,590
That's no good. What we need to do is push the field to draw a line this way.

324
00:32:50,220 --> 00:32:53,700
That's what we got to do. Predict risk as accurately as you can.

325
00:32:54,120 --> 00:33:00,360
Now draw the line on the risk scale. The other things Kathy Griffin never went.

326
00:33:01,050 --> 00:33:04,740
The calculate risk is that you can't then decide what risk is worth.

327
00:33:04,750 --> 00:33:10,829
What is if I'm if I'm very low risk, I don't want to take too much gamble on the side effects of the treatment.

328
00:33:10,830 --> 00:33:19,650
I'm going to play my cards pretty safe. If I'm ultra high risk by the conventional therapy, it's probably time to roll some dice and see how it goes.

329
00:33:19,980 --> 00:33:25,680
But if you're lucky, you're not. Now, we've done this 20 times.

330
00:33:25,680 --> 00:33:31,020
These are prostate cancer examples. Concordance Index. AMA's Model C is categorical.

331
00:33:32,580 --> 00:33:36,170
You can get a little or a lot of improvement on concordance index by a model base.

332
00:33:36,180 --> 00:33:43,470
There's a prostate. These are non prostate. It's pretty rare when this doesn't help you concordance index.

333
00:33:43,920 --> 00:33:46,980
But again it's it's it's discrimination. It's not calibration.

334
00:33:47,400 --> 00:33:53,790
So we need kind of a calibration curve to understand is it is it accurate or not across the spectrum?

335
00:33:54,420 --> 00:33:58,200
So I think the best way to do that is kind of visualize it.

336
00:34:00,180 --> 00:34:04,440
So so this is the predicted risk here. This is observed.

337
00:34:05,490 --> 00:34:08,520
This is now just a binary outcome here.

338
00:34:08,660 --> 00:34:18,690
This is hospital mortality. So that zeros and ones is an outcome just running a smoother nature across the zeros and ones.

339
00:34:18,690 --> 00:34:22,500
And I'm getting that. I'm hoping for the 45 degree line.

340
00:34:24,020 --> 00:34:28,460
So I get things going well here and then something goes haywire over there.

341
00:34:28,870 --> 00:34:33,800
The question is, do I care? This is an area where context kind of matters.

342
00:34:35,870 --> 00:34:42,320
The first question when you see that line, I think is, well, what's the distribution of the predictions?

343
00:34:42,530 --> 00:34:49,160
Are there is anyone even getting predictions out here? So that's kind of nice to add something like this to show you.

344
00:34:50,270 --> 00:34:54,860
Most of the predictions are down here. This is where all the predictions are coming from.

345
00:34:54,860 --> 00:35:01,710
And then there's some scattered stuff out there. But then the other question is, Will, what's the decision point here anyway?

346
00:35:02,070 --> 00:35:08,920
Like, if it's over 10%, you do something else, something like that.

347
00:35:08,940 --> 00:35:09,209
Well,

348
00:35:09,210 --> 00:35:15,570
then that's probably doesn't really matter much that you're mis calibrated after 10% because you're already going to do whatever you're going to do.

349
00:35:15,570 --> 00:35:19,530
And the fact that it's off over there, you just get off.

350
00:35:19,530 --> 00:35:23,010
But you're already high enough risk that you're going to get whatever you're going to do there.

351
00:35:23,580 --> 00:35:28,770
So you kind of have to dig a little bit on that. But it's something I'm working on here.

352
00:35:29,940 --> 00:35:32,429
Is it still hard to work with this?

353
00:35:32,430 --> 00:35:41,339
I think because you kind of have to in your mind, overlay the distribution part with the calibration curve and think about that is one stuff,

354
00:35:41,340 --> 00:35:50,580
some goofing around with rescaling the X proportional to the distribution of X.

355
00:35:51,030 --> 00:35:54,120
So in other words, I give more here.

356
00:35:54,120 --> 00:36:01,139
I just give and give the same amount of space and irrelevant to the distribution of the predictions here.

357
00:36:01,140 --> 00:36:08,580
And I'm allocating more space in the visual of X based on where the distribution actually lies.

358
00:36:08,600 --> 00:36:14,160
So the bulk of my predictions are here between 1% and 15%.

359
00:36:14,370 --> 00:36:20,940
So that's dwarfing the whole view. And then I only allocate a little bit to where the predictions actually lie.

360
00:36:21,720 --> 00:36:25,140
I think this is cool. I'm not sure it's work in progress. This is bad.

361
00:36:25,140 --> 00:36:31,380
I don't use it yet to be honest. Purchasing anyway.

362
00:36:32,640 --> 00:36:37,020
Where are we headed? We still we're doing great. Here's the deal.

363
00:36:38,160 --> 00:36:41,640
It's always about alternatives. So one thing that holds.

364
00:36:41,970 --> 00:36:46,440
So if you like the risk prediction models and you think this is kind of fun and you can make a difference, you can make a tool.

365
00:36:46,650 --> 00:36:50,970
The doctor will use to counsel a patient. And that's neat.

366
00:36:51,400 --> 00:36:58,049
I think it's fun. Pretty easy way to get applied. It's also very easy to be scared of doing that because.

367
00:36:58,050 --> 00:37:05,280
Well, what's my tools, Rod? You know, that's what I feel bad if I do it and everybody's using it.

368
00:37:06,450 --> 00:37:11,760
Well, it is wrong. How about that? It's guaranteed to be wrong.

369
00:37:11,910 --> 00:37:17,640
I've got a page I'll show you in a second. With hundreds of wrong ones that are getting lots of hits every day.

370
00:37:18,360 --> 00:37:25,450
You're all wrong. How do you sleep at night? Like you're pumping out hundreds of miles that are not and you admit to different.

371
00:37:26,140 --> 00:37:29,380
I know. I'd like them not to be wrong. I don't know what to do here.

372
00:37:30,190 --> 00:37:35,559
But the key is its alternatives. What else are you going to do if you don't use the model?

373
00:37:35,560 --> 00:37:41,110
That's not perfect. What's the what's the alternative?

374
00:37:41,590 --> 00:37:45,910
So in my experience, this is the collection.

375
00:37:46,330 --> 00:37:50,560
So if I go to the doctor and say doctor to my chances and I'm looking for a number of.

376
00:37:51,420 --> 00:37:58,470
And I have now I have some cancer, whatever it is. And I'm asking, you know, what's my what's my five year survival prediction?

377
00:37:59,580 --> 00:38:03,650
And I'm looking is it the doc is going to do one of these things?

378
00:38:04,500 --> 00:38:08,040
Some dogs actually, before we get to that one, we're going to do this.

379
00:38:09,380 --> 00:38:13,460
Know, the ability to predict at the individual patient level, which is lengthy.

380
00:38:13,640 --> 00:38:18,470
Jeremy It talks about the argument against this, but it goes on like this.

381
00:38:18,950 --> 00:38:24,740
You know, the dog dog or my chances dog says to me, Mike, you're a patient, not a statistic.

382
00:38:26,000 --> 00:38:29,660
You're either going to live or die. So what does this statistic mean to you anyway?

383
00:38:32,160 --> 00:38:35,970
Take all of the statistics you've ever read. Throw them in the trash. Right.

384
00:38:36,000 --> 00:38:40,620
Nobody knows whether you're going to make it or not. So forget about it.

385
00:38:41,280 --> 00:38:54,200
Quit asking them questions. Well, so they come back to that is is a game that's really falling out of favor these days.

386
00:38:54,210 --> 00:38:58,710
But it's a game called Russian Roulette. It's not played very often anymore.

387
00:38:58,740 --> 00:39:06,480
But even if you have two pistols today, they each hold potentially six bullets.

388
00:39:07,350 --> 00:39:10,270
That little revolver things you can spin them and shoot things.

389
00:39:10,860 --> 00:39:15,570
These potentially hold six bullets and put five bullets in this gun and one bullet in that gun.

390
00:39:15,660 --> 00:39:16,830
We're going to play Russian roulette.

391
00:39:16,830 --> 00:39:23,520
And again, you know, it's a dumb game, really, but you just shoot yourself in the head and you hope you don't hit yourself.

392
00:39:24,300 --> 00:39:28,650
So anyway, we're going to play Russian roulette. Are you indifferent with respect to which gun you want to play?

393
00:39:30,990 --> 00:39:35,590
I don't think you are. Are you? Would you rather have the gun with one bullet or the gunman?

394
00:39:35,610 --> 00:39:39,040
Five bullets? I strongly, strongly prefer the gun with one bullet.

395
00:39:39,120 --> 00:39:46,710
I. This doctor I just talked to apparently is indifferent because, I mean, you're going to shoot myself in the head or not, so.

396
00:39:47,330 --> 00:39:54,100
So what does it matter? I think knowing that it was one at six versus five out of six, even though I can't tell you zero or one, which is his point,

397
00:39:54,730 --> 00:40:03,160
is extraordinarily helpful to you as a decision maker that you come back to the old school doctor who uses this and, you know, it's a way of life.

398
00:40:03,460 --> 00:40:06,550
Financial advisors, you can't predict perfectly.

399
00:40:07,180 --> 00:40:10,360
Did you bring your umbrella today? I didn't.

400
00:40:10,370 --> 00:40:16,309
I looked at the forecast. It looks pretty good, but there's no forecast I know of.

401
00:40:16,310 --> 00:40:20,630
It's based on zeros and ones I still use. I still look at the weather report.

402
00:40:21,230 --> 00:40:24,950
All right, so why can't I do that? Whoo! Anyway.

403
00:40:25,980 --> 00:40:29,670
Yep. That's that. So that's hard to get past that.

404
00:40:30,990 --> 00:40:37,470
I've already pushed it down here, I think. But now you get past this fatigue based on knowledge and experience.

405
00:40:38,280 --> 00:40:50,400
Doctor fires back and says, I know more about you than your model does. So I'm going to make my prediction based on my using my noggin.

406
00:40:51,540 --> 00:41:01,920
I've done multiple studies where I've given doctors information and compared them with regression models and gone head to head with real doctors.

407
00:41:02,970 --> 00:41:07,560
The doctors will incorporate extra information to do a worse job than have they just stuck with them.

408
00:41:08,190 --> 00:41:13,379
Here's what the model says. Here's a bunch of extra information. Do give me your prediction now that you've seen all that.

409
00:41:13,380 --> 00:41:16,380
Okay. My revised prediction is so-and-so. Congrats.

410
00:41:16,380 --> 00:41:20,040
Your revised prediction is worse than the model that you thought was flawed anyway.

411
00:41:20,460 --> 00:41:24,010
Here's that example. Here's another example. Prostate cancer.

412
00:41:25,560 --> 00:41:29,670
I hand this out at a fancy lunch sponsored by drug companies.

413
00:41:29,670 --> 00:41:35,040
It's like a who's who of medical oncologist, and they have to fill this format to get their lunch to me.

414
00:41:35,790 --> 00:41:39,909
And they say this is some sort of nomogram. Thank you, Tom. And I said, Well, yeah, kind of.

415
00:41:39,910 --> 00:41:44,190
And we don't need another name for this. So you don't.

416
00:41:44,310 --> 00:41:48,720
I said there's one out there and I didn't even make I'm not on this. I didn't make this a Sloan-Kettering, but I didn't make it.

417
00:41:49,980 --> 00:41:55,470
Len Heller And I said, Yeah, but I don't have a dog in the fight and we're just messing around here, guys.

418
00:41:55,680 --> 00:41:59,830
Okay, well, we don't need a game and we're going because we have to stop that.

419
00:41:59,850 --> 00:42:07,710
So the clinicians also, this one guy says, stop, then everybody else is look around and go, Yeah, if you don't make, it's stopped.

420
00:42:08,010 --> 00:42:11,640
We have to stop. Yeah, I don't know what a good start is. It's powerful stuff.

421
00:42:12,150 --> 00:42:15,560
They all have this clinical stuff. You can't buy this stuff, by the way.

422
00:42:15,570 --> 00:42:22,220
You don't Amazon. You can't buy. But it's terrible, this Gustavo thing.

423
00:42:22,440 --> 00:42:26,429
So this is patient number one. Everybody looked at patient number one.

424
00:42:26,430 --> 00:42:29,309
This is Dr. U, Dr. U, the patient number one and said,

425
00:42:29,310 --> 00:42:33,810
if I have a hundred patients just like this one five, we're going to develop meds within a year.

426
00:42:34,350 --> 00:42:41,750
Dr. Veit looked at the same patient. His or her guest says no, 99, 99 or developments in one year.

427
00:42:42,440 --> 00:42:49,670
So all these results can't be right. And I think the red is what this program is predicting that I didn't make.

428
00:42:51,110 --> 00:42:54,140
Read Concordance Index doctor. Doctor. Dr. Dr. Dr.

429
00:42:54,290 --> 00:42:56,770
Only one doctor. Worse than a coin toss. That's pretty good.

430
00:42:56,810 --> 00:43:02,960
Usually it's more than one doctor worse at doing this stuff that's getting a little heavy.

431
00:43:03,410 --> 00:43:06,590
Isn't a doctor romance a human being from I can't do it either.

432
00:43:06,590 --> 00:43:11,430
I don't think you can. It's calculating probabilities in our heads.

433
00:43:12,290 --> 00:43:15,890
Incorporating all these things is pretty hard to do. It's hard to expect them to do that well.

434
00:43:16,400 --> 00:43:20,480
So because all these bases get in the way, you can't remember all your patients equally well,

435
00:43:21,050 --> 00:43:26,959
you overweight things and didn't respond to what you recently saw versus, you know, all that stuff.

436
00:43:26,960 --> 00:43:31,940
And it messes up all your mates up here and it really goes to be a disaster.

437
00:43:32,420 --> 00:43:38,209
So that's where we're shooting for. I want to see treatment options head to head, good and bad outcomes side by side.

438
00:43:38,210 --> 00:43:44,690
And then I can look at this if you fill it in and say, Well, I want treatment one and that's fine, but I get I get all my information there.

439
00:43:44,690 --> 00:43:48,530
I see my risks and pros and cons. That's perfect. Here you go.

440
00:43:48,530 --> 00:43:53,560
We'll do it for my diabetes. This poor fellow in this is all made up.

441
00:43:53,570 --> 00:43:56,870
This is from James when he left Cleveland the first time.

442
00:43:56,870 --> 00:44:00,200
If he had diabetes, I don't think he did as upset.

443
00:44:01,040 --> 00:44:08,509
And I like his features in still a little upset if the guy hits if you and

444
00:44:08,510 --> 00:44:13,520
LeBron has four drug options classes at that point like one eyes and so forth,

445
00:44:14,090 --> 00:44:17,210
there's about 127 outcomes at stake.

446
00:44:18,350 --> 00:44:22,730
There is predicted probabilities for each outcome for each combo.

447
00:44:23,480 --> 00:44:29,150
Across my grid, I color coded green is the best in a row.

448
00:44:29,150 --> 00:44:31,130
Red is the worst in a row.

449
00:44:31,820 --> 00:44:40,820
So if you had solid green almost, if you did have solid green and you believed in this system and these are the only outcomes you care about,

450
00:44:41,300 --> 00:44:43,910
you'd actually you would have a no brainer situation on your hands.

451
00:44:44,330 --> 00:44:48,920
So if this were solid green and and good models or you're happy with them and this is the only outcomes

452
00:44:48,920 --> 00:44:54,500
care about you get back one experience and there's no rational reason not to choose that one.

453
00:44:54,650 --> 00:44:56,090
So he got pretty close with this.

454
00:44:56,090 --> 00:45:02,329
He's trading a little risk here of coronary artery disease, but not enough to dwarf mortality or something like that.

455
00:45:02,330 --> 00:45:06,559
Think so? You should break it down. So make these tools all the time.

456
00:45:06,560 --> 00:45:10,160
Some doctors love them, some breakfast hates them, but we're putting them out there.

457
00:45:10,160 --> 00:45:18,020
That one we just did with Epic Data, you know, it's a bunch of models under each one of those cells as a model.

458
00:45:18,860 --> 00:45:25,280
Yeah, I think each one of the cells as models for the day and run all the models at once, return it all at one kind of the visual.

459
00:45:26,780 --> 00:45:31,550
That's not important. Go here sometime, goof around our counter code or risk calculator.

460
00:45:32,300 --> 00:45:35,930
Library is a whole bunch of things in there. They all live up there.

461
00:45:35,930 --> 00:45:39,110
They're all free. Sleep one cool when.

462
00:45:40,080 --> 00:45:45,610
Let's do that. A missing data issue.

463
00:45:47,050 --> 00:45:50,780
All right. This guy has prostate cancer. Who is he?

464
00:45:50,800 --> 00:45:56,870
He's African-American. He is 60 is PSA six.

465
00:45:57,370 --> 00:46:01,300
He has a normal diary and he has an.

466
00:46:03,930 --> 00:46:08,460
So he does not have this fancy, expensive out of pocket stress test.

467
00:46:08,760 --> 00:46:13,410
He also does not have a fancy expensive potentially out of pocket MRI.

468
00:46:13,530 --> 00:46:16,770
Does he need them? Right now, this risk of high grade is 43%.

469
00:46:17,680 --> 00:46:21,490
Because you really need to figure out what his risk is. Does he need to order one of these things?

470
00:46:22,030 --> 00:46:27,100
Well, he got another thing that would change the 43 down to 14 or up to 64 potentially, if he got it.

471
00:46:28,100 --> 00:46:31,180
A-ha. What about the MRI? MRI scan? Interesting.

472
00:46:31,270 --> 00:46:34,510
All right. Doc says, okay, go get an MRI. All right.

473
00:46:34,960 --> 00:46:39,670
Would it show I got a Pirates three? All right, we do that now.

474
00:46:39,670 --> 00:46:42,790
You're 24. Should I go get the Xs on real quick?

475
00:46:43,120 --> 00:46:51,250
It would lower five or options for potentially 5 to 4. That's kind of cool to see the value of ordering a new test on a friendlier scale.

476
00:46:51,370 --> 00:46:55,870
Well, what might my prediction be if I do that? So you can do all sorts of cool things.

477
00:46:55,870 --> 00:47:00,130
That risk calculator is a shiny site. Anything you can do in our we can do it there.

478
00:47:00,360 --> 00:47:03,579
It's kind of fun. So, yeah, go play with that. That's cool.

479
00:47:03,580 --> 00:47:07,090
Tools. This one is navigation at our place.

480
00:47:07,540 --> 00:47:11,770
Readmission risk assessment. So everybody gets a readmission risk.

481
00:47:12,310 --> 00:47:15,459
When you're discharged, that model runs up there live.

482
00:47:15,460 --> 00:47:19,300
You act on it. You don't act on it, whatever. But is there no extra work?

483
00:47:20,230 --> 00:47:26,709
And we let the individual places within the Cleveland Clinic decide what would they want to do?

484
00:47:26,710 --> 00:47:32,710
In other words, the heart team has a certain threshold for what they'll do. Neurological team, different threshold for what they want to do.

485
00:47:33,490 --> 00:47:38,920
But that's kind of all you can do. So when we have a few minutes of questions.

486
00:47:39,990 --> 00:47:43,530
Predictions are fun. They help with informing decision making.

487
00:47:43,530 --> 00:47:48,269
They're better than large clinical judgment. Risk counting, risk factors.

488
00:47:48,270 --> 00:47:52,080
Oh, yeah. Three risk factors. That's not good. What's that mean?

489
00:47:52,830 --> 00:47:58,090
Well, it's worse than if you only had two. I don't care about the Saddam Hussein.

490
00:47:58,140 --> 00:48:01,620
I care about the hypothetical of some other person who has to.

491
00:48:02,040 --> 00:48:05,870
It's not very useful to me. Oh, well, they got you in for again.

492
00:48:07,170 --> 00:48:11,010
It's not my question asked. Treatment aid.

493
00:48:11,640 --> 00:48:16,560
Here's your average treatment effect for treatment aid. Yeah, but what about me?

494
00:48:18,560 --> 00:48:32,430
It's not there. It would have questions again. Which easy, no question to dumb.

495
00:48:37,800 --> 00:48:41,490
Question. All right. So I can. Someone else can just sit in between.

496
00:48:41,790 --> 00:48:49,110
So my first question is sort of like I think one big challenge in this kind of work is that you find in the data that we're using to build the models.

497
00:48:49,530 --> 00:48:55,860
Do you have any insight as to how to sort of do that actually in your vision and sort of randomize data that can then be used to build a model?

498
00:48:56,630 --> 00:49:03,180
Yeah. Yeah. So that's a that's a tricky one. I, I probably ignore that problem more than should.

499
00:49:03,210 --> 00:49:08,220
So I get a lot of data like a single cohort or single treatment cohort.

500
00:49:09,300 --> 00:49:18,570
So the diabetes calculator I showed you, for example, this treatment and that's a clearly a not a random.

501
00:49:19,010 --> 00:49:24,209
So when I have in my project things I've got, the table is not randomly assigned.

502
00:49:24,210 --> 00:49:30,180
All these patients get surgery. I just adjust for as much as I can and hope for the best.

503
00:49:30,720 --> 00:49:40,020
I don't know of a better way to get around that. I very rarely get randomized data to work, so I don't know what else to do.

504
00:49:41,040 --> 00:49:46,810
But just to. Just. You earned it.

505
00:49:47,110 --> 00:49:50,979
Yes, sir. One thing that's like a second sort of building I've got. Have you done any work?

506
00:49:50,980 --> 00:49:56,440
I thought you'd be curious to see in which patients are randomized to be to be treated with one of these models,

507
00:49:56,440 --> 00:49:58,600
like sort of viewing the model itself. I don't know if you.

