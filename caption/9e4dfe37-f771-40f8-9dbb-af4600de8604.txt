1
00:00:12,950 --> 00:00:17,840
Okay. Let's get started. So.

2
00:00:24,420 --> 00:00:33,570
So we started from the basics of M algorithm.

3
00:00:35,000 --> 00:00:48,230
So, uh. I just wanted to be crystal clear about the part that.

4
00:00:49,600 --> 00:00:56,900
So most relevant for the. Midterm exam and finals and that the part that our.

5
00:00:58,300 --> 00:01:03,250
Not, I'd say sure, but I think it's very, very helpful for the for the future.

6
00:01:03,260 --> 00:01:12,940
So for the region, I'm just wanted to make sure that you understand this, the image that you need to derive the minimum wage and for the midterm.

7
00:01:13,300 --> 00:01:15,640
So you need to know how to do that.

8
00:01:18,610 --> 00:01:29,440
So if you have have I'm willing to take as much as time as in need as because it's important to understand how this works.

9
00:01:32,190 --> 00:01:43,319
And next lecture is going to be talking about very practical stuff, which I didn't promise to teach you, like our language stuff,

10
00:01:43,320 --> 00:01:56,070
but I think it's important for you to know how to use ICP and how to how to make an art package just to accomplish the final project.

11
00:01:56,340 --> 00:02:02,970
So those are Lecture 15. So those are two lectures I would consider very important just to be.

12
00:02:05,530 --> 00:02:10,839
Yeah. Just, just just to be in the same page.

13
00:02:10,840 --> 00:02:15,970
On the same page, to come to be ready for during the finals and midterm.

14
00:02:17,790 --> 00:02:24,050
And the rest of the part. I'm going to talk about other things.

15
00:02:24,070 --> 00:02:27,330
We're going to cover the rest of the randomized or Williams stuff.

16
00:02:28,160 --> 00:02:39,180
We're going to cover some something called the dynamic programing and the hidden Markov model, which I think is very, very useful.

17
00:02:39,830 --> 00:02:49,170
And but and so that's why I do really want you to listen to and focus on that.

18
00:02:49,190 --> 00:02:55,270
Those are there's no real hallmarks or associated with it because we don't have final exam.

19
00:02:55,280 --> 00:03:01,280
So, yeah, just I wanted to be clear about what to expect.

20
00:03:01,910 --> 00:03:09,319
I do have the last lecture. I do have a part to possibly teach about the first fully transform,

21
00:03:09,320 --> 00:03:15,860
which I think it's really great to learn about, but I doubt that we have time to do it.

22
00:03:15,860 --> 00:03:20,140
But if we do, you know, those are those are the topic I would like to cover ideally.

23
00:03:21,430 --> 00:03:24,940
Okay. So that's that's upcoming lectures. Okay.

24
00:03:26,020 --> 00:03:30,220
But let's finish the part we started.

25
00:03:30,280 --> 00:03:33,680
So EMR algorithm, so EMR wisdom, what is it?

26
00:03:33,700 --> 00:03:37,660
So basically my algorithm, the essence of it is this.

27
00:03:38,230 --> 00:03:44,140
So you have. Uh. You're doing the iterative step.

28
00:03:44,440 --> 00:03:49,190
Okay. Do.

29
00:03:49,230 --> 00:03:56,780
Yeah. So you're doing a 3D step between between to the first step is that there is

30
00:03:56,780 --> 00:04:03,560
a observe that observed values Z there is a unobserved values missing data.

31
00:04:04,460 --> 00:04:09,890
Z So X and Z are complete data and X is observed data.

32
00:04:10,400 --> 00:04:19,100
And the evaluation basically ask you calculate the distribution of.

33
00:04:20,030 --> 00:04:26,839
Uh, well, that that you need to calculate a Z given X eventually.

34
00:04:26,840 --> 00:04:36,920
But what's required here is that you need to calculate the expected log likelihood in this form.

35
00:04:37,520 --> 00:04:42,890
So this form is hard to understand, but basically this part is a function of theta.

36
00:04:43,580 --> 00:04:50,870
But you are calculating this as a function of theta, assuming the true theta two parameter is theta t.

37
00:04:51,270 --> 00:05:00,050
Okay, so that's the. So you, you assume that your parameters are fixed, but you're modeling your likelihood as a function of your parameter.

38
00:05:00,060 --> 00:05:03,139
So that that's a little strange.

39
00:05:03,140 --> 00:05:06,470
But that's that's what that's what exactly it's asking for.

40
00:05:07,130 --> 00:05:14,780
And once you have these expected log likelihood, you can try to maximize these parameters,

41
00:05:15,830 --> 00:05:22,580
assuming that you you pretend to be, you know, the the parameters and you maximize it.

42
00:05:23,120 --> 00:05:31,010
If you keep doing this, you're going to have some guaranteed problem, guaranteed properties that I'm going to explain.

43
00:05:32,660 --> 00:05:41,600
So, uh, the conceptually I went through this picture before, and this, uh, blue ones are.

44
00:05:43,560 --> 00:05:48,120
Actual actual look like the function, which usually looks very complicated.

45
00:05:48,720 --> 00:05:51,840
So it's hard to maximize the actual look like the function.

46
00:05:52,650 --> 00:05:57,630
But expect a logo like a function has a property that of.

47
00:05:57,690 --> 00:06:09,720
If I started with a specific parameter at that point, my expected log likelihood function is exactly the same as actual logo like create value.

48
00:06:10,020 --> 00:06:13,770
So you are guaranteed to cross this particular point.

49
00:06:14,400 --> 00:06:22,050
But everything else is smaller than the logo libraries that that's the second property.

50
00:06:22,470 --> 00:06:31,230
And usually this expect a logo like function is a sort of convex functions so it's easy to maximize so that that's another property.

51
00:06:31,230 --> 00:06:38,640
So you're trying to maximize this function then obviously because you're not maximizing only origin of function, it's not doing the right thing.

52
00:06:39,180 --> 00:06:47,970
But if it goes a closer to the intended maximum and you try this parameter again and they'll calculate,

53
00:06:48,120 --> 00:06:55,620
calculate the step and then step again and try to so try to maximize the parameter again, you will reach here.

54
00:06:56,130 --> 00:07:05,100
So it's very close to the maximum now. And your if you keep doing this now, theta is updated very slightly, but it keeps climbing up.

55
00:07:05,790 --> 00:07:10,470
So that's the illustration of how immigration works.

56
00:07:13,230 --> 00:07:27,260
So. And we we basically this equation is basically showing that how immigration actually works.

57
00:07:27,290 --> 00:07:37,320
So I stopped here. So. So basically you have this property where my local likely to function.

58
00:07:37,620 --> 00:07:41,160
Exactly. Expect a logo. Like a function.

59
00:07:41,190 --> 00:07:44,909
Exactly. Crosses my actual logo.

60
00:07:44,910 --> 00:07:51,270
Like function with theta equal date of t and we can use each Jensen's inequality

61
00:07:51,570 --> 00:07:56,280
where you just a change of the expectation law to the law of expectation.

62
00:07:56,790 --> 00:08:05,520
Then because a low is concave function you're expecting a lower likely will be equal or smaller than this value.

63
00:08:06,060 --> 00:08:13,020
And this value is exactly the log like in function.

64
00:08:13,140 --> 00:08:17,050
Right. So. So. So.

65
00:08:17,980 --> 00:08:24,670
So that's why your expected lower likelihood is always smaller than actual over likely the function.

66
00:08:24,700 --> 00:08:27,730
So these low volatility this is a blue line.

67
00:08:27,750 --> 00:08:34,720
This is a green line basically. So it's showing that this green line is always equal or smaller than this blue line.

68
00:08:35,110 --> 00:08:42,090
Okay. Then your M step is basically you're trying to maximize this function.

69
00:08:42,630 --> 00:08:52,590
So maximizing this this function is the what step is doing and it's important that you know

70
00:08:52,590 --> 00:08:59,010
what you are maximizing if you developed and I would imagine for specific distribution.

71
00:08:59,850 --> 00:09:07,920
So that's why I wanted to you to try this good in a problem that is not well known Gaussian problem.

72
00:09:08,220 --> 00:09:11,940
Okay. So.

73
00:09:13,050 --> 00:09:21,629
Yeah. So. Uh, when you do this, I'll, I'll show the example,

74
00:09:21,630 --> 00:09:28,050
but the optimization can be easily separated by DC parameters because you have a complete data likelihoods.

75
00:09:28,050 --> 00:09:35,040
So it's not as bad as you think in terms of finding the misstep close form.

76
00:09:35,040 --> 00:09:44,310
So you have a closer form of the misstep and those are those are usually doable if the problem is well-suited for EMI going.

77
00:09:46,320 --> 00:09:50,770
I just wanted to make sure that this is being recorded. Okay.

78
00:09:50,980 --> 00:09:56,440
It is. Okay. So, uh.

79
00:09:59,450 --> 00:10:05,280
So let's look at this. These examples.

80
00:10:05,870 --> 00:10:09,260
Okay. So just.

81
00:10:10,230 --> 00:10:17,650
We do this. So. This function, as I mentioned, this is just a function that.

82
00:10:19,230 --> 00:10:24,270
Simulates. Data from EMR.

83
00:10:24,630 --> 00:10:28,110
So from from the sorry. The normal mixture.

84
00:10:29,260 --> 00:10:32,530
So you have a series of new vector, sigma vector and py vector.

85
00:10:32,710 --> 00:10:35,710
So this is a, this is a setup.

86
00:10:36,370 --> 00:10:41,000
You start. So you need to have a data you need to start with.

87
00:10:41,050 --> 00:10:47,700
So this is not the part of the EMR wisdom. And.

88
00:10:49,440 --> 00:10:56,009
This is though this is a one way to implement the embargo. You really don't have to use a class necessarily.

89
00:10:56,010 --> 00:11:04,469
This is just one way to do it. So if you maintain a class, you can then keep all these different field here.

90
00:11:04,470 --> 00:11:11,610
So you have this and you can initially what I'm going to do is I'm going to initialize the parameter.

91
00:11:11,610 --> 00:11:23,370
So note that we are using the global assignment to because you want to keep this value, you have the function finishes.

92
00:11:23,850 --> 00:11:35,880
So this is data vector, a number of component and NN is the the length of data and you initialize the parameter pi and mu and sigma.

93
00:11:36,540 --> 00:11:41,590
So here with the. So initializing PI.

94
00:11:41,600 --> 00:11:48,370
We're doing the uniform distribution and I explained this last lecture so new we

95
00:11:48,370 --> 00:11:54,069
want to make them equally spaced across the quantile so that I think that's a very,

96
00:11:54,070 --> 00:12:02,860
very useful design to do it. So how you want to set up MU and the pi and sigma can be very different.

97
00:12:02,860 --> 00:12:11,079
So you can start with a totally random value. You could do that too, but if you start the random value, where should I choose?

98
00:12:11,080 --> 00:12:19,270
So that's probably some point you need to decide on the strategy to select this move.

99
00:12:19,780 --> 00:12:22,479
MU is that you just select the random three point.

100
00:12:22,480 --> 00:12:29,920
I work random k points from the observe data and those tend to be different and those are good starting points too.

101
00:12:29,950 --> 00:12:36,639
You just need to start from slightly different views and sigma is the variance.

102
00:12:36,640 --> 00:12:45,910
It doesn't matter much so because once you start between different means, even if a variance is same, likely they will be quite different.

103
00:12:45,910 --> 00:12:49,300
So you know, Sigma, we're just using the pool of variance.

104
00:12:50,200 --> 00:12:56,589
And so obviously this initialization strategy, there's no single way, right way to do it.

105
00:12:56,590 --> 00:13:05,740
You can you can try a different strategy. And where we're initializing this probably probability matrix which will store this value.

106
00:13:06,010 --> 00:13:09,210
Okay. And the lower likelihood here. Okay.

107
00:13:11,060 --> 00:13:16,860
So. After that. What we want to do is.

108
00:13:19,080 --> 00:13:29,219
So calculating these value to each step is basically is a culture leading these low like expected low like any other.

109
00:13:29,220 --> 00:13:35,620
Right. So Q. Q of data, given data and key.

110
00:13:36,310 --> 00:13:51,760
So this looks complicated, but if you if you remember it says data equity and logo and so the given.

111
00:13:53,420 --> 00:13:56,580
Even data. Right. So to.

112
00:13:59,030 --> 00:14:04,159
It's important to know that there's a P here and there's no T here.

113
00:14:04,160 --> 00:14:07,440
It's just data. Okay. Okay.

114
00:14:08,720 --> 00:14:12,020
So what does this mean? So I'm going to.

115
00:14:12,050 --> 00:14:17,550
So this one I have. I know X is given x.

116
00:14:17,620 --> 00:14:21,650
X is known. So what? What's unknown is z.

117
00:14:21,980 --> 00:14:26,600
Right. So but I'm going to assume that Z is known.

118
00:14:27,350 --> 00:14:33,500
Okay. How, how, how? Then assuming I'm going to assume the Z distribution, I know this value.

119
00:14:34,010 --> 00:14:37,910
Exactly. Okay. That's that's the trick you're going to have.

120
00:14:38,330 --> 00:14:43,340
Okay. Now, if I know Z, this is just a complete data pleasure.

121
00:14:43,640 --> 00:14:47,150
So this is complete data. Like this should be easy to calculate.

122
00:14:47,390 --> 00:14:50,510
Okay, so that's the that's the idea.

123
00:14:50,780 --> 00:14:59,510
Okay. The key idea is that, oh, when you have some data, like, oh, I have a distribution of.

124
00:15:00,550 --> 00:15:06,200
You know, it's a visceral fight with the male and females. And I don't know, you know, which one is which.

125
00:15:06,250 --> 00:15:13,270
I say then once you have some observation, this is your X or Y, and I say, okay.

126
00:15:13,840 --> 00:15:19,480
And I don't know what the missing components are, but I'm going to assume that oh,

127
00:15:20,110 --> 00:15:25,510
I'm going to assume that this is my you know, this is my B is, you know.

128
00:15:26,760 --> 00:15:33,000
I am going to assume that this is a new one and I shouldn't assume that I know the sigma one.

129
00:15:33,640 --> 00:15:45,620
I'm going to assume that I know the new two and Sigma two square and you can model this particular value as a mixture of these two unknown components.

130
00:15:46,500 --> 00:15:58,049
Then you will have a probability. Okay, probability of x of I even knew one and a sigma was carried probability of x.

131
00:15:58,050 --> 00:16:01,920
So I've given you two and sigma squared to.

132
00:16:02,070 --> 00:16:07,200
Right. Okay. And you have to provide probability.

133
00:16:07,210 --> 00:16:10,970
I'm going to just say that, oh, I say this probability.

134
00:16:10,990 --> 00:16:16,930
I'm just making up. Making it up. Okay, let's these probabilities like 0.0.2.

135
00:16:17,170 --> 00:16:22,749
Well, this is not a probability. This is a this is active probability density.

136
00:16:22,750 --> 00:16:31,330
So I'm going to use an arbitrary number. So the density, let's say this is point or to let's say this is a point of eight or something like this.

137
00:16:31,510 --> 00:16:42,100
Okay. Then you have two components here. This means that actually I should change it because this doesn't make sense between an eight point or two.

138
00:16:43,180 --> 00:16:49,060
Then this means that. This is a desert.

139
00:16:49,150 --> 00:16:53,200
This is lovely. Four times more likely to happen than this, right?

140
00:16:53,690 --> 00:17:06,120
Okay, so you can you can you can have these clusters of I don't know whether this belongs to a component to one or a component to.

141
00:17:06,380 --> 00:17:10,390
Okay. So. Well, this is a roughly 4 to 1.

142
00:17:10,570 --> 00:17:14,590
80%. 20%. So I'm gonna I'm going to count.

143
00:17:14,600 --> 00:17:20,530
This is a 0.8 fraction or can't even point eight as a first component.

144
00:17:21,280 --> 00:17:28,240
And, you know, point to observation from second components. That's what this is Tab is trying to do.

145
00:17:28,990 --> 00:17:43,899
So instead of assigning Z of V of eight to specific cluster, your proportional assigning based on the, uh, based on the fraction.

146
00:17:43,900 --> 00:17:49,030
Well, the relative likelihood, relative probability between those.

147
00:17:51,310 --> 00:17:52,030
Those clusters.

148
00:17:52,030 --> 00:18:01,540
So it is somewhat similar to the base or base method if you are familiar with the base theory, but you are not actually using base there specifically.

149
00:18:02,750 --> 00:18:07,700
Okay. So so this.

150
00:18:07,940 --> 00:18:15,790
So you're basically this this is all possible class assignment or all possible class assignment compared.

151
00:18:16,010 --> 00:18:24,739
You know, when you when you normalize by that value, how likely is it for for my particular assignment?

152
00:18:24,740 --> 00:18:31,700
And here, just to simplify it, I assume that those two probably are equally priors and equally probable.

153
00:18:31,700 --> 00:18:35,420
But if you have a different mixing proportion,

154
00:18:36,110 --> 00:18:45,319
these this may not be exactly 4 to 1 if it once you multiply this pi over j so so this is the this is a probability.

155
00:18:45,320 --> 00:18:51,290
So if you assume that I know what the true probably true parameters are,

156
00:18:51,680 --> 00:18:58,490
you should be able to calculate this value pretty easily in this Gaussian mixture case.

157
00:19:00,000 --> 00:19:05,409
Okay. So let's try to think about how to implement this.

158
00:19:05,410 --> 00:19:09,330
So what we need to do is we need to keep this value, okay?

159
00:19:09,570 --> 00:19:14,880
We need to keep this value. How do we keep this value? This value is.

160
00:19:15,870 --> 00:19:20,880
Uh, you need to store for each of the observations separately.

161
00:19:20,910 --> 00:19:24,090
Right. So. And different observation for all.

162
00:19:24,090 --> 00:19:28,110
And different observation you need to store. It is better. You need to calculate these value.

163
00:19:29,430 --> 00:19:33,270
And there are two different classes, right?

164
00:19:33,900 --> 00:19:40,170
So I need to calculate this value for all an observation across k components.

165
00:19:40,500 --> 00:19:44,610
So this is a this can be stored in N by key metrics.

166
00:19:45,500 --> 00:19:52,670
So that's why when I when I had this this is a may represent there's a matrix with the end by key metrics.

167
00:19:53,020 --> 00:19:57,410
Now now initially was filled as an A I need to fill this value.

168
00:19:57,830 --> 00:20:02,010
Okay. So when you feel this value.

169
00:20:02,270 --> 00:20:08,130
Okay. So I'm going to I'm going to talk about.

170
00:20:09,370 --> 00:20:15,200
Uh. Well, I'm going to talk about the.

171
00:20:16,260 --> 00:20:22,500
Your way in. So let me let me just make a code to to make it make it simpler.

172
00:20:22,560 --> 00:20:32,270
All right. So. If you think about naively about how how you calculate this value, this is what I would do.

173
00:20:32,360 --> 00:20:37,730
So well, what I need to do is I need to calculate this value, right?

174
00:20:38,150 --> 00:20:42,670
So what you can do is maybe by probably matrix.

175
00:20:42,830 --> 00:20:47,930
Okay. And I'm going to make it really, really simple.

176
00:20:47,930 --> 00:20:53,990
Simple. So that let's say I, I'm, I'm just doing this for each of each of these parameters separately.

177
00:20:54,200 --> 00:20:56,210
This is definitely not efficient way.

178
00:20:56,840 --> 00:21:11,870
But just to make that make you understand better I comma j then this is basically I have a mixing proportion high vector of g times the norm.

179
00:21:12,290 --> 00:21:22,220
Okay, and new. So this is the normal with the vector of I.

180
00:21:22,610 --> 00:21:32,660
Okay. And then my mean is a new vector of G, my SD is a sigma vector of K.

181
00:21:32,840 --> 00:21:38,990
Okay, so, so this, this is what you could do, right?

182
00:21:41,900 --> 00:21:51,440
And after, let's say you finish the field, everything, then what you can do is that, oh, I'm going to normalize by some of these value.

183
00:21:51,610 --> 00:21:56,980
Okay, so let's say you. So I'm going to just pour I equal one, one, two.

184
00:21:57,020 --> 00:22:04,069
And so this is a this is a conceptual you can try to implement it then this is definitely

185
00:22:04,070 --> 00:22:07,969
very inefficient if you're doing the double look that we're doing the very bad thing now.

186
00:22:07,970 --> 00:22:13,370
But you know, we can try to conceptually implement it for now and try to improve it.

187
00:22:14,150 --> 00:22:19,310
So this one is if you do that, we're just calculating the upper part of it, right.

188
00:22:19,880 --> 00:22:23,270
Okay. So you need to normalize it. How do you normalize it?

189
00:22:23,270 --> 00:22:27,980
I'm going to calculate the low sums of probability matrix.

190
00:22:28,370 --> 00:22:31,609
Right. And I'm going to divide divide by this.

191
00:22:31,610 --> 00:22:35,649
So this means that I'm going to make a matrix of rows and rows.

192
00:22:35,650 --> 00:22:39,350
Some becomes a column vector, just double the sides of end.

193
00:22:40,070 --> 00:22:48,630
So I'm going to make the duplicate these rows by just making out and and complicated and if you make make a matrix these by default it just

194
00:22:48,650 --> 00:22:57,560
you predicate the column or if you if you're worried about this thing you can also always say by false vertices is this is not necessary.

195
00:22:58,310 --> 00:23:05,930
So basically if you say that oh my probably the matrix is a probability matrix divided by this matrix,

196
00:23:06,530 --> 00:23:10,320
then that's a normalized that now I calculated the value.

197
00:23:10,640 --> 00:23:15,380
This is a horrible implementation in terms of the efficiency, but this should work.

198
00:23:17,640 --> 00:23:23,230
Okay. So but this implementation is very different.

199
00:23:23,230 --> 00:23:27,310
So I'm going to get from from this one to this one.

200
00:23:27,460 --> 00:23:31,960
Okay. So, first of all, I want to get rid of all this loop.

201
00:23:32,310 --> 00:23:36,610
Okay. This is definitely not useful. So how can you do it?

202
00:23:37,360 --> 00:23:40,660
Okay. So one one thing.

203
00:23:40,690 --> 00:23:49,120
One way you can do it is that the one you can get rid of this if instead of saying, Oh, instead of doing this.

204
00:23:49,420 --> 00:23:55,300
J If you say, Oh, I'm going to just use everything here.

205
00:23:56,920 --> 00:24:09,910
And instead of saying instead of provide instead or calculate each element, you can actually assume that, oh, my view is sigma are both vectors.

206
00:24:11,350 --> 00:24:17,880
I'm still calculating this for each of the elements separately, but at least I'm doing okay.

207
00:24:17,950 --> 00:24:21,070
Different components in the at the same time.

208
00:24:21,260 --> 00:24:26,190
This is a double. This is single loop. Then double loop. Single loop is much better than double.

209
00:24:26,350 --> 00:24:29,970
So this is hopefully better. Okay.

210
00:24:31,510 --> 00:24:37,870
So. And you can also do a similar thing.

211
00:24:38,230 --> 00:24:42,879
Okay. So. So you can you can do this.

212
00:24:42,880 --> 00:24:46,390
But another way to do it is to. Oh, I don't want to do this.

213
00:24:46,390 --> 00:24:53,500
I want to do my column because you have you have only a few few rows, few elements per column.

214
00:24:53,500 --> 00:24:59,649
So instead of doing this while you can do this, I want to do by each of the column what then.

215
00:24:59,650 --> 00:25:08,730
What, what, what you mean is that you want to do something like this before instead doing this for I in one, one plus one.

216
00:25:08,980 --> 00:25:22,600
Well J in 1.2 K and what, what I, what I think it's better is that oh I want to fill all the column using the PIO vector.

217
00:25:25,090 --> 00:25:28,480
In this case I need to specify just one one column.

218
00:25:29,320 --> 00:25:35,590
In this case, a data vector. I, I'm, I'm doing the all in different elements together.

219
00:25:36,250 --> 00:25:43,810
But my mean is this okay and a my SD is this.

220
00:25:44,200 --> 00:25:47,320
Okay. So maybe this is better than this.

221
00:25:49,220 --> 00:25:54,310
A great. Okay. Because then you need to go through this different time.

222
00:25:54,310 --> 00:26:01,450
So this is the more this is faster and this is probably good enough if you want to use oh, I don't want to use loop.

223
00:26:01,810 --> 00:26:06,910
If you want to use it as supply you could do that by.

224
00:26:07,180 --> 00:26:12,520
Oh. So you can use if you, if you know how to use a supply.

225
00:26:13,300 --> 00:26:24,820
This is, this is not going to make us speed up much, but you can actually make a function of g uh, and you, you can, you can make it return this.

226
00:26:24,970 --> 00:26:31,000
It's going to be basically the same if you make it as probability matrix like this.

227
00:26:31,390 --> 00:26:39,459
So this is an equivalent from the previous version, which is, you know, not, not, you know, super important.

228
00:26:39,460 --> 00:26:46,990
But this, this is this is good enough. So you can implement this algorithm in these two lines, basically.

229
00:26:47,880 --> 00:26:54,770
Okay. If what you want into is what you want to do is just to calculate this value.

230
00:26:56,030 --> 00:26:59,090
But actual implementation is more complicated.

231
00:26:59,570 --> 00:27:04,450
Let's see why. Okay. So what did.

232
00:27:04,480 --> 00:27:07,750
What what kind of problem do you think if you if you do this?

233
00:27:10,860 --> 00:27:18,030
Sometimes you have observation like here. And this probably could have been could it become zero?

234
00:27:18,830 --> 00:27:22,260
Numerically, it's not zero, but it could become zero.

235
00:27:23,040 --> 00:27:26,910
Okay. So that's what we want to always be careful about.

236
00:27:27,180 --> 00:27:33,209
Okay. You make it so that if you have a zero here, this is not going to work.

237
00:27:33,210 --> 00:27:37,170
So everything will fail because you are going to calculate the low at the end.

238
00:27:37,290 --> 00:27:42,540
So you really need to have some way to avoid this problem.

239
00:27:43,140 --> 00:27:48,299
So one way to do it is that, oh, if I assume zero, I'm going to make Johan the way the argument that,

240
00:27:48,300 --> 00:27:51,450
oh, I'm going to set everything is like some minimum value.

241
00:27:51,870 --> 00:27:56,310
But that's probably not going to work very well because that's not mathematically correct way to do it.

242
00:27:57,300 --> 00:28:03,940
So. So how do you avoid this kind of problem when you have a very, very small probability?

243
00:28:04,600 --> 00:28:07,750
How do I avoid this? You know as much as I can.

244
00:28:08,080 --> 00:28:17,620
Okay. So. Well, instead of instead of doing this way, I want to calculate log of these value.

245
00:28:18,610 --> 00:28:24,589
Okay. That means that, you know, instead of putting this, I'm cloaking the logo of these values.

246
00:28:24,590 --> 00:28:28,520
So that's what that's what we discuss. Okay.

247
00:28:29,730 --> 00:28:44,850
So here, when I quoted the norm, if you called more people true than you calculate the log of the density and add this log of a pie vector and.

248
00:28:45,870 --> 00:28:52,120
And. Yeah. So logo py that got that logo.

249
00:28:52,270 --> 00:29:01,270
So this pie, could it be you know sometimes if it's really, really anything goes really bad so I'm adding a small probability really.

250
00:29:01,270 --> 00:29:06,239
Small properties tend to the most hundred. So adding them.

251
00:29:06,240 --> 00:29:14,610
But other than that, this is a very, you know, as close as it as accurate as you can get in terms of the value.

252
00:29:16,620 --> 00:29:22,620
But the problem is that well I. I did calculate log but what is the point.

253
00:29:22,980 --> 00:29:26,420
If I anyway have to do the addition. Okay.

254
00:29:26,910 --> 00:29:32,970
So you need to do the addition. So the point here is that this is a very useful skill.

255
00:29:33,210 --> 00:29:41,670
Once you calculate the log, okay, you if you take the exponent exponential function of it, it's basically, you know, log is useless.

256
00:29:42,090 --> 00:29:49,080
But before doing this, what you can do is that you can quite so you have a bunch of different values here.

257
00:29:49,170 --> 00:29:54,930
So you're calculating a value here. Okay, let's say what value is that?

258
00:29:55,860 --> 00:30:01,110
Let's say you have three values. Okay. Let's say you have your value, outlier value or something like this.

259
00:30:01,920 --> 00:30:11,220
And you could have a ten to the minus, you know, 310 to the -320, 10 to -330 or something like this.

260
00:30:11,760 --> 00:30:14,850
Can the value could it be like this.

261
00:30:15,960 --> 00:30:19,050
If you stored as a log.

262
00:30:20,070 --> 00:30:24,170
Well it's a this one is starting is low, low, natural.

263
00:30:24,180 --> 00:30:30,660
But let's say we're showing a log of ten that it'll store in the ten, 20, 20, 23.

264
00:30:30,670 --> 00:30:35,510
Right. Okay. Then after that. What you can do is that.

265
00:30:35,510 --> 00:30:41,140
Well, I can, because I only want to normalize this value.

266
00:30:41,770 --> 00:30:46,810
So what you can do is that, oh, I know that this is the maximum value.

267
00:30:47,020 --> 00:30:54,650
So I'm going to I'm going to say, well, this is negative, negative, -370 320, 330.

268
00:30:54,900 --> 00:30:59,110
Okay. So oh, this is this is my maximum value.

269
00:30:59,560 --> 00:31:04,450
So I'm going to I'm going to subtract the maximum value that this is zero.

270
00:31:04,450 --> 00:31:13,319
Negative ten. -20. Okay. The then then take, take, take the exponent.

271
00:31:13,320 --> 00:31:19,740
In this case, ten to the zero, ten to the negative ten, ten to the 20 and normalize this value.

272
00:31:20,760 --> 00:31:26,489
Then you are going to properly have this value close to zero, close to ten times ten to the -20.

273
00:31:26,490 --> 00:31:30,600
So this value becomes numerically stable.

274
00:31:31,290 --> 00:31:37,109
But if you start with this and try to normalize from here, this value will become zero.

275
00:31:37,110 --> 00:31:40,200
So you're not going to have the right value.

276
00:31:40,200 --> 00:31:46,530
So if you expect of it, expect to have a very, very small value across the board.

277
00:31:47,130 --> 00:31:50,890
What you can do is you can calculate the maximum value here. Okay.

278
00:31:52,080 --> 00:31:56,970
And in the subtract the maximum value for each for each of them.

279
00:31:57,480 --> 00:32:02,430
Okay. And so then then you're going to rescale it, right?

280
00:32:03,090 --> 00:32:11,219
And after that I already take the exponential function of it, so and calculate some of it.

281
00:32:11,220 --> 00:32:13,140
So this is basically to calculate the raw sum.

282
00:32:13,530 --> 00:32:27,090
Okay then and if you normalize this, this is going to be slightly more stable than the version that didn't subtract the maximum maximum value.

283
00:32:27,450 --> 00:32:30,480
So this is a more stable way to calculate.

284
00:32:32,000 --> 00:32:40,489
The probability. So. So maybe if you just use this equation in this part of some particular example,

285
00:32:40,490 --> 00:32:47,090
this may just work, especially because we simulate the data from from the random distribution.

286
00:32:47,540 --> 00:32:54,020
These are which may work. I haven't tried, but I'm pretty sure that is prob this is a good chance to just work out of the box.

287
00:32:54,650 --> 00:33:00,020
But this is more robust against outliers and very small values.

288
00:33:00,770 --> 00:33:08,060
Okay. So if you have to handle very small value, you're going to have a similar kind of problem, basically.

289
00:33:10,570 --> 00:33:20,650
And after that, you can also calculate the local acreage value in this case, because we calculate we calculate the maximum value here.

290
00:33:20,660 --> 00:33:24,910
You need to add them because if we subtract them, but when you calculate the local likelihood,

291
00:33:25,390 --> 00:33:29,470
you need to add them, add them back to properly calculate the likelihood.

292
00:33:31,130 --> 00:33:35,550
Okay. So. So that's that.

293
00:33:35,550 --> 00:33:38,700
Is that. Okay. Any questions so far?

294
00:33:43,460 --> 00:33:46,930
Okay. So now let's talk about the M0. Okay.

295
00:33:47,510 --> 00:33:53,600
So M step is basically the part that you currently update the parameters.

296
00:33:53,780 --> 00:33:57,230
Okay. Now I am expecting an overnight and a function.

297
00:33:57,830 --> 00:34:06,860
So the nice thing is that a lot of cases you can have a closed on solution to update your primers.

298
00:34:08,060 --> 00:34:15,230
Okay. So. So.

299
00:34:17,680 --> 00:34:21,280
So you update the mixture upon your to maximize the likely data.

300
00:34:21,470 --> 00:34:27,720
Okay. And a. So that update becomes simple.

301
00:34:28,080 --> 00:34:33,990
If you assume that, you know, the old lady, the collector, the class assignment value,

302
00:34:34,350 --> 00:34:45,200
which is this value in this case probability of z given team of I include j Cuban X of I and all these pioneers, right?

303
00:34:47,500 --> 00:34:53,990
So. Uh. And when you do the M step,

304
00:34:54,140 --> 00:35:03,920
the one one thing that is important is that if you are if you try to maximize your local likelihood to expect a lower likelihood,

305
00:35:04,310 --> 00:35:08,000
it's it will never decrease your actual local acreage.

306
00:35:08,000 --> 00:35:17,329
So it'll always go to always go up in terms of optimization perspective, increase the likelihood.

307
00:35:17,330 --> 00:35:23,810
So you will reach to the maximum likelihood or at least the local maximum likelihood from your starting point.

308
00:35:24,020 --> 00:35:27,470
So that's the very good feature that is guaranteed.

309
00:35:27,740 --> 00:35:31,330
Okay. So how do you. How do you do it?

310
00:35:31,360 --> 00:35:36,610
So. So maybe we should go from here.

311
00:35:36,880 --> 00:35:47,020
Okay. So this is the function, not the expected function here.

312
00:35:47,860 --> 00:36:01,060
So you're you're doing the I want to end j commander k and calculate the role of pi over j plus.

313
00:36:02,410 --> 00:36:08,950
Longbow. This is a this is just an N is a normal distribution, the normal density.

314
00:36:09,850 --> 00:36:13,050
So I am sorry.

315
00:36:13,120 --> 00:36:30,430
X of x of. And probably courtesy of high frequency human x i.

316
00:36:33,030 --> 00:36:38,600
80. So this looks like really hard 40 miles.

317
00:36:38,810 --> 00:36:42,890
If you look at the first one, it became the first look.

318
00:36:43,760 --> 00:36:49,300
But one thing here is that now this one is known.

319
00:36:49,340 --> 00:36:53,090
This is a computer. This is not a function of data. Okay.

320
00:36:53,690 --> 00:36:57,720
So this is basically we already calculate this value in the east.

321
00:36:58,730 --> 00:37:03,320
So this value can be just represented, some non-verbal, let's say.

322
00:37:03,770 --> 00:37:09,350
This is very typical, easy of idea. I say let's say I already calculate this value.

323
00:37:09,860 --> 00:37:15,200
So this is just non value. Okay. What you need to know is how to optimize this value.

324
00:37:15,470 --> 00:37:18,860
And. This this what was this value?

325
00:37:18,890 --> 00:37:23,540
This is basically well to so.

326
00:37:25,320 --> 00:37:30,480
So one of our team go to high single mom ice.

327
00:37:30,930 --> 00:37:43,120
Its parent plus. Minus two sigma secured against the minus new and superior.

328
00:37:43,160 --> 00:37:47,060
So. So this. This part can be rewritten in this way.

329
00:37:47,440 --> 00:37:50,750
Right. So. So keep in mind.

330
00:37:53,020 --> 00:38:00,190
So and what we want to do is that we want to find them in the parameter that maximize this value of this value.

331
00:38:00,190 --> 00:38:03,470
This value. So.

332
00:38:04,400 --> 00:38:07,880
Well, let's. Let's try the pie first.

333
00:38:08,300 --> 00:38:11,630
Okay. This part is. There's nothing.

334
00:38:11,900 --> 00:38:14,990
Nothing that is relevant here. So how do we maximize our pie?

335
00:38:15,470 --> 00:38:25,130
So if you do, let's say pie, okay, signal IQ data to theta t, then you're going to have.

336
00:38:25,850 --> 00:38:32,000
Okay. Hi, AJ.

337
00:38:32,480 --> 00:38:35,930
Okay. And this is this. This is one.

338
00:38:36,680 --> 00:38:41,110
And. Deep. You are the oldest part that will be gone.

339
00:38:41,110 --> 00:38:44,860
And this will be the agency of aging. Right.

340
00:38:47,320 --> 00:38:51,910
So this is this is what you're going to have, right? So.

341
00:38:53,540 --> 00:38:59,600
Then. Then this will be equally high of J.

342
00:39:01,630 --> 00:39:05,320
Uh. To the.

343
00:39:07,970 --> 00:39:14,630
Okay. So, so this is a pie of J in summation of G of AJ, right?

344
00:39:16,270 --> 00:39:26,510
So, uh. So this this part can be a little bit confusing because, well, there's no way this can be zero.

345
00:39:26,650 --> 00:39:37,450
Right. The region is that pi I kind of a constraint that this actually is do this actually add up to one.

346
00:39:38,020 --> 00:39:49,210
Right. So that part is not here. So because this is a discrete process, so I'm going to assume that there is just a two to PI just to make it easier.

347
00:39:49,510 --> 00:39:53,079
Okay. You can definitely make it make a generalized.

348
00:39:53,080 --> 00:39:59,720
Generalized. So in this case, let's say you have just pi and one minus pi.

349
00:40:00,400 --> 00:40:05,469
And so the way how this works is that pi. So I said this pi.

350
00:40:05,470 --> 00:40:11,560
Welcome. Just let's say just pi to g of a one.

351
00:40:12,840 --> 00:40:16,040
And the one my one MySpace you to.

352
00:40:16,740 --> 00:40:22,930
And you need to have have this become zero. Right. So then, uh.

353
00:40:24,620 --> 00:40:34,790
What you have. If you multiply this for the both of them, one minus you of a one plus page of a two equals zero.

354
00:40:35,930 --> 00:40:39,620
So what you have at the end is.

355
00:40:46,410 --> 00:40:53,320
So. Geo geo a want a one minus pi?

356
00:40:56,070 --> 00:41:04,730
Uh. Uh.

357
00:41:05,060 --> 00:41:10,530
I'm sorry. I think I made a mistake because this is one man's pie.

358
00:41:10,550 --> 00:41:14,240
This has been minus. Right. This has to be minus.

359
00:41:14,640 --> 00:41:20,810
Okay. So. So this is g of a one minus pi.

360
00:41:21,110 --> 00:41:28,380
You, I, I want you to think because, you know, we're basically doing the summation.

361
00:41:28,640 --> 00:41:32,930
Summation everywhere from. I want to end. I want to end.

362
00:41:33,410 --> 00:41:37,910
I want to end. Right. So we're doing the summation.

363
00:41:38,570 --> 00:41:47,780
So what you what you have at the end is a pi is so g one plus g two is always one because this is a conditional probability.

364
00:41:48,410 --> 00:41:55,760
So pi becomes and and over summation of g of i1, i go one through ten.

365
00:41:56,130 --> 00:42:03,760
Okay. So this becomes pi. Okay. So so that's how you how you threw the first.

366
00:42:07,520 --> 00:42:14,390
So first equation, this one. Okay, so, so in each of them, you basically prove this way.

367
00:42:14,540 --> 00:42:22,400
So PI's a little tricky because you have a hidden constraint into some of the pi equal pay of equal one.

368
00:42:23,000 --> 00:42:28,700
And you can use a librarian or you can use just an equation like this to prove that.

369
00:42:29,690 --> 00:42:33,670
But. The rest of the part should be actually more straightforward.

370
00:42:33,680 --> 00:42:37,100
So if you want to take a picture of it, you can take a picture of it.

371
00:42:37,790 --> 00:42:48,290
And I'm going to move on to just a meal and we're going to be going for the move, too, to make sure that we understand this part.

372
00:42:49,420 --> 00:43:00,629
And. So if you want to optimize for them, you know that this part doesn't matter.

373
00:43:00,630 --> 00:43:06,150
This part doesn't matter. Only this part matters. Right? So you can you can do the same thing.

374
00:43:06,550 --> 00:43:11,670
Well, I'm going to add this thing.

375
00:43:12,210 --> 00:43:17,730
And here my testing them out here to maximize new.

376
00:43:17,910 --> 00:43:22,980
I think. And. Oh, oh, sorry.

377
00:43:24,730 --> 00:43:27,850
Thanks. And I'm not Kelvin.

378
00:43:30,150 --> 00:43:38,060
Location. So this is a new chain and single by chain server that is similar chain.

379
00:43:39,740 --> 00:43:45,090
So. It is a deal.

380
00:43:45,140 --> 00:43:48,960
I. G of ideas, too.

381
00:43:49,050 --> 00:43:52,110
Still here. All right, so this is what you have.

382
00:43:52,890 --> 00:43:57,310
And you just need to make this to be zero. Right.

383
00:43:58,410 --> 00:44:01,760
So. To to. To maximize it.

384
00:44:03,040 --> 00:44:13,049
So. Well and then and it's pretty obvious that to to cancel a job and these parties are always equal, almost equals.

385
00:44:13,050 --> 00:44:16,770
So you're going to have you're going to have this situation, basically.

386
00:44:17,130 --> 00:44:25,470
How do you get there? So because we're not dealing with a thing about component, okay, this part doesn't matter.

387
00:44:25,470 --> 00:44:29,100
So you just have one single submission as a summation here.

388
00:44:30,460 --> 00:44:35,370
And what you have is a Sigma Square x.

389
00:44:35,370 --> 00:44:41,670
So I see. I. J. Should be equal to.

390
00:44:44,190 --> 00:44:47,790
Equal to view of J.

391
00:44:48,870 --> 00:44:55,800
C Kumar G Square. Uh, summation of i j.

392
00:44:56,570 --> 00:45:00,860
Xo idea of hygiene. Right. So you have this.

393
00:45:01,010 --> 00:45:04,480
Oh, sorry. No, no, x. And.

394
00:45:05,870 --> 00:45:18,140
So in. And stigma. Sigma J Square is is just canceled out here.

395
00:45:19,510 --> 00:45:25,840
And this part is basically the same as end times pi.

396
00:45:25,870 --> 00:45:29,720
Okay. So this is a neat new packaging.

397
00:45:30,040 --> 00:45:33,850
Right. So you don't have to do this. You can just use this.

398
00:45:33,860 --> 00:45:50,900
So either you can write it down. Your Jane is now summation of A.J to AJ summation of X so idea of AJ you can write it this way or.

399
00:45:51,310 --> 00:45:58,480
Oh, I know that. I already know that PMG had in the in my in t plus one.

400
00:45:58,480 --> 00:46:07,000
So new view of the updated long history of i j p by my end summation of Jehovah by 18 divided by ten.

401
00:46:07,570 --> 00:46:18,370
So I because I already concatenated I don't need to recalculate this summation so you can write this as a and times pi had.

402
00:46:19,550 --> 00:46:27,010
JT plus one and you can write it this week, so this is totally fine.

403
00:46:27,940 --> 00:46:37,380
Okay. But the point is this point is this part and this part.

404
00:46:38,170 --> 00:46:42,190
So and, you know, getting the right equation is important.

405
00:46:42,190 --> 00:46:46,270
But let's try to understand why this why you have this equation.

406
00:46:47,290 --> 00:46:52,970
What does this mean? Z was a fraction of count.

407
00:46:53,800 --> 00:47:00,370
Right. So you are basically you have different observations, but you don't know which observation.

408
00:47:01,790 --> 00:47:08,060
Came from component one which which observation it came from component component to write.

409
00:47:09,310 --> 00:47:13,800
So. This equation basically tells that.

410
00:47:14,340 --> 00:47:16,049
So let's say you have a complete data.

411
00:47:16,050 --> 00:47:24,700
You actually know that which one lost from component one was in, which comes from component two or which one is male or female.

412
00:47:25,470 --> 00:47:34,710
The way how you would do it is that, Oh, I'm going to take all males and copulate the men, I'm going to take them all females and calculate the mean.

413
00:47:34,740 --> 00:47:42,210
That's what would you do? What would you do? That's that's that's what you do if you have a complete data.

414
00:47:42,960 --> 00:47:45,240
But because we have we don't have a complete data.

415
00:47:45,750 --> 00:47:52,650
What we are doing is that we're fractional where we're treating each observation fractionally for each of the components.

416
00:47:52,660 --> 00:47:57,600
So in this case, oh, my observation is observation.

417
00:47:57,600 --> 00:48:05,759
It says 80% male, 20% female. Then I'm going to calculate point eight, give up, keep away to 0.8.

418
00:48:05,760 --> 00:48:12,150
When I look at the mean of mean for male, I'm going to give a point to weight when I call get amino female.

419
00:48:12,160 --> 00:48:19,800
So. So some, some value in the middle basically have almost equal probability between the two components.

420
00:48:20,310 --> 00:48:26,480
So my in the very extreme, it's more likely to be treated as a as a one component versus the other.

421
00:48:26,490 --> 00:48:32,030
So that's how this works. Okay. So.

422
00:48:32,630 --> 00:48:38,990
So that's how you calculate the mean. So you must create a little, little more difficult.

423
00:48:38,990 --> 00:48:47,150
But I, I think you actually know how to calculate the maximum likely estimate for Gaussian.

424
00:48:47,170 --> 00:48:53,690
So. So it's the same thing. Just it's not a mixture anymore because you, you pretend we know that.

425
00:48:53,690 --> 00:48:57,889
We pretend to know that. What's the edge of the pretend?

426
00:48:57,890 --> 00:49:04,610
No, that is the matrix. So you can just try to calculate the, you know,

427
00:49:04,610 --> 00:49:14,050
take the derivative for C component J and you are going to have this probability and I'm sure that you can prove this.

428
00:49:14,060 --> 00:49:19,910
I'm not going to do it again because if you it's pretty clear to how you do this to this calculation.

429
00:49:21,200 --> 00:49:27,290
Okay. So you can do pretty much the same thing for the common problem,

430
00:49:27,290 --> 00:49:35,480
except that this is a multidimensional things that you need to be more careful about the handling very low probabilities and you know,

431
00:49:35,500 --> 00:49:43,970
you're dealing with a multiplication of a lot of numbers, but I think that algorithm is actually easier to derive than this one.

432
00:49:46,540 --> 00:49:55,260
Okay. So you need to write the derivation derivation in that in that in your in your answer not just the you know, through the code of oldest.

433
00:49:57,650 --> 00:50:01,580
Okay. Any questions about the mixture so far?

434
00:50:03,650 --> 00:50:15,350
Okay. So let's just try to go through the implementation of this step in.

435
00:50:18,550 --> 00:50:25,220
So. Well, this one just had the actual conclusion of this.

436
00:50:27,620 --> 00:50:34,489
Uh, Mr. UPDATE But I have to know that knowing this update roll, it's a pretty simple oh, I have this.

437
00:50:34,490 --> 00:50:39,520
So. Which means that I already have the GST matrix here.

438
00:50:39,740 --> 00:50:44,540
Right. So. Calculate just the mean for each of them.

439
00:50:44,630 --> 00:50:47,810
So apply. You can do this. Or you can.

440
00:50:47,870 --> 00:50:51,380
You can just. Well, you can do comments if you.

441
00:50:51,760 --> 00:50:57,350
I you know, if you don't like using apply function. So both of them should be good.

442
00:50:57,800 --> 00:51:04,930
Okay. And the view is you have this education basically.

443
00:51:05,710 --> 00:51:10,450
So which means that you're going to have.

444
00:51:15,070 --> 00:51:23,570
So. The denominator is just pi vector and the way that some to add some policy value because you don't want to divide by zero.

445
00:51:25,890 --> 00:51:35,340
The numerator is basically you're multiplying x theta vector and with the probability matrix and take take the mean.

446
00:51:36,330 --> 00:51:37,590
So yeah,

447
00:51:37,620 --> 00:51:45,540
I actually don't like this kind of formulation because you are multiplying vector by matrix or you're implicitly making the vector as a matrix.

448
00:51:45,540 --> 00:51:53,430
So the the way how usually I would usually like it is that I'm going to explicitly make this as a.

449
00:51:54,390 --> 00:51:58,380
Vector like this. So I would. I would write it this way.

450
00:51:58,560 --> 00:52:02,890
I. Yeah. I think I got this.

451
00:52:03,210 --> 00:52:09,480
I, I, I was just reusing this code from the previous class, so.

452
00:52:09,490 --> 00:52:16,120
Yeah, but this is how I would do it, just to make sure that are doing the matrix by matrix to avoid any confusion.

453
00:52:16,360 --> 00:52:20,600
But both of them works. Okay. So.

454
00:52:21,230 --> 00:52:25,210
And if you don't like doing to apply me you can also do the column means.

455
00:52:25,280 --> 00:52:28,460
As I said, this will be the same basically.

456
00:52:28,550 --> 00:52:31,630
Right. So, uh. Yeah.

457
00:52:31,850 --> 00:52:39,290
So. So both of them should work. And, uh, let's just do it.

458
00:52:40,630 --> 00:52:44,900
On. I'm just.

459
00:52:46,220 --> 00:52:50,330
I don't need to do this. Sorry. Because this is a better code.

460
00:52:51,390 --> 00:52:55,850
Okay. Let me go. And updating sigma is basically the same.

461
00:52:55,860 --> 00:53:01,200
You're just calculating this square value and apply and multiply this matrix.

462
00:53:01,200 --> 00:53:09,150
But there's nothing special here. You're just using all the function here to calculate this square, but you don't have to use all of the function.

463
00:53:09,330 --> 00:53:15,819
Okay. So this one is just that, you know, calculating, making,

464
00:53:15,820 --> 00:53:25,000
making a matrix of X minus MU and taking care of it so that that's, that's a good use of using out a function.

465
00:53:26,610 --> 00:53:33,900
Okay. So now we're almost close to putting everything together.

466
00:53:34,110 --> 00:53:40,770
Okay, so. So how do we do it?

467
00:53:40,860 --> 00:53:42,510
So how do we put things together?

468
00:53:42,740 --> 00:53:54,620
Okay, so first step is to use you guesstimate the start starting parameters and we're using this Bayes theorem basically just which is a,

469
00:53:55,650 --> 00:54:01,650
which is just using this a Z of I.J to calculate the assignment values and the

470
00:54:01,650 --> 00:54:07,220
user m step to use the calculate estimated assignment and keep doing this.

471
00:54:07,230 --> 00:54:10,380
So step by step. Step by step until.

472
00:54:11,270 --> 00:54:14,970
Until the likelihood is stable. But what what does that mean?

473
00:54:14,990 --> 00:54:24,350
So once you keep doing this, it'll it'll help you evaluate the likelihood every time the likelihood of keeping encouraging.

474
00:54:24,350 --> 00:54:29,570
But once you reach to the maximum likely, it becomes almost the same.

475
00:54:30,170 --> 00:54:34,790
So you keep track of the likely value and oh my, in likelihood not increasing any more.

476
00:54:34,790 --> 00:54:44,060
Then you can decide to stop it. Okay. So those are the the stopping criterion you can implement.

477
00:54:44,780 --> 00:54:54,440
So here what what this does is that it's checking the tolerance between the two likely to compare and if the relatively.

478
00:54:54,830 --> 00:54:58,080
So this is after all, is the relative tolerance for of tolerance.

479
00:54:58,420 --> 00:55:07,100
If I if it didn't improve ten times five here, the proportion of improvement is not less than ten times five.

480
00:55:07,100 --> 00:55:11,330
Then I'm going to say this is saturated.

481
00:55:11,640 --> 00:55:15,870
Okay. So that's what it does. Okay. So.

482
00:55:17,660 --> 00:55:25,490
So because this is implement there's a class it's called is kind of clean so say when you run this

483
00:55:25,670 --> 00:55:32,210
excel function is run in here so I'm going to run VM function but the initialize parameters.

484
00:55:33,500 --> 00:55:41,450
And in the local library there was zero one and initially and I'm updating the probability

485
00:55:41,450 --> 00:55:47,620
is this is the updating the probability matrix this tab is updating the py vector,

486
00:55:47,630 --> 00:55:55,880
mu vector, sigma vector. And after that, you know, update the local likelihood and you know, return all the value.

487
00:55:55,880 --> 00:56:01,610
So this is my way that basically. So now.

488
00:56:02,630 --> 00:56:07,459
We have a step to evaluate the algorithm.

489
00:56:07,460 --> 00:56:12,680
So what I'm going to do is I'm going to simulate a.

490
00:56:14,090 --> 00:56:24,120
Similarly, three components here the one minus five and zero and five and sigma vectors are one one, two.

491
00:56:24,140 --> 00:56:29,360
So one is the value two. Two of them has a variance of one second.

492
00:56:29,360 --> 00:56:35,840
One has a larger variance and a mixing proportion is a 24%, 50% and 30%.

493
00:56:36,940 --> 00:56:44,350
And the way how you call this thing is that you make a make a class called M.

494
00:56:44,920 --> 00:56:55,980
So when you create a class make class name and new that that'll that'll create a new class with these initial parameters you actually defined here.

495
00:56:55,990 --> 00:57:02,410
So you if you don't remember this is a initialize function will be run if you do new.

496
00:57:02,590 --> 00:57:09,790
Okay. So make a new class with three components and run this with them.

497
00:57:10,540 --> 00:57:15,220
And this will just the plot. How the likelihood will.

498
00:57:16,910 --> 00:57:17,750
They look like.

499
00:57:17,900 --> 00:57:29,540
So this is converted and it went through 36 iterations and of the of the desktop and these are the three new vectors that was estimated.

500
00:57:29,540 --> 00:57:31,040
These are the sigma parameters.

501
00:57:31,040 --> 00:57:41,020
So this is pretty close to what is estimated even though we simulate only 20,000 observations and the low likelihood looks like this.

502
00:57:41,060 --> 00:57:49,580
Okay. So at each step, more likely the increase is quite a lot and at the end it became very flat here.

503
00:57:49,670 --> 00:57:55,040
So and these are actual value of the likelihood in each of the step.

504
00:57:55,580 --> 00:57:59,500
Okay. So these are the works.

505
00:57:59,500 --> 00:58:09,190
And you obviously the nice thing about this CoLab kind of setting is that you can actually try to do something like,

506
00:58:09,190 --> 00:58:18,910
oh, you know what happens if I do like zero? And so if I do two components each problem I say, so this is a major problem.

507
00:58:19,600 --> 00:58:24,520
And how does this works? Oh, this this actually went through four iterations.

508
00:58:24,520 --> 00:58:29,320
But, you know, it does it does improve quite a lot.

509
00:58:29,950 --> 00:58:35,439
You can make the problem much harder. You can make like a mixture of ten component.

510
00:58:35,440 --> 00:58:38,500
Does it really work? You can try multiple different things.

511
00:58:38,980 --> 00:58:46,390
You have algorithm is not magic. So you're going to find a case that this doesn't work and you can figure out why this doesn't work.

512
00:58:47,050 --> 00:59:02,200
In which cases do you think it doesn't work? Which case?

513
00:59:08,490 --> 00:59:14,180
So. If you have a fast reading capability, it says here.

514
00:59:14,490 --> 00:59:22,010
Okay. So this is a these are going is trying to find a local optimum actually.

515
00:59:22,070 --> 00:59:25,190
So there is no guarantee that it will find a global optimum.

516
00:59:25,760 --> 00:59:30,710
Let's say you started from here. Okay. You may get trapped here.

517
00:59:32,260 --> 00:59:38,680
Okay. So it is possible that you can get trapped here and it may never improve again.

518
00:59:39,310 --> 00:59:42,790
And those cases rarely happens.

519
00:59:44,160 --> 00:59:56,010
If you're you're talking on the smaller dimensional space, but if you work in a very large dimensional space, this could happen quite a lot.

520
00:59:56,360 --> 00:59:57,959
Okay, so you make it.

521
00:59:57,960 --> 01:00:10,350
You may have to check the convexity of the problem or you can do the multiple random restart to make sure that you're getting to the right parameters.

522
01:00:10,500 --> 01:00:15,090
And so that that that's that's the one challenge you might see.

523
01:00:15,720 --> 01:00:22,350
But yeah, my algorithm actually does pretty well compared to other algorithm in terms of converging to the global optimum.

524
01:00:22,350 --> 01:00:29,160
So I do think that this is a really good algorithm unless you're dealing with like thousands of parameters.

525
01:00:30,810 --> 01:00:34,530
Okay. So. Yeah.

526
01:00:34,630 --> 01:00:36,550
And just to recap,

527
01:00:37,180 --> 01:00:46,810
Jim is the army Jim that does repeatedly do the estab and then step to update the condition of probability of thinking of parameters given,

528
01:00:47,770 --> 01:00:51,310
given observed data and the current parameters.

529
01:00:51,790 --> 01:00:58,090
And, and after that it throws up data parameters using the conditional probabilities.

530
01:00:58,480 --> 01:01:06,049
And that's very so that that has a guarantee that this always increases the likelihood.

531
01:01:06,050 --> 01:01:12,280
So it will eventually converge into the convention to the local maximum.

532
01:01:12,730 --> 01:01:18,520
So that that's that's very good property in terms of optimization perspective.

533
01:01:18,520 --> 01:01:21,670
So a lot of people are using and in many different cases.

534
01:01:22,920 --> 01:01:32,880
So it it is the iteration does not decrease L.A. function this is great and you don't have to worry about like if the parameters are inside,

535
01:01:33,210 --> 01:01:41,240
inside the valid parameter and so on. So if you do the LP, if you have LP approaches B or some other algorithm,

536
01:01:41,250 --> 01:01:45,510
you may have to check the boundary condition, but in this case you don't have to.

537
01:01:46,260 --> 01:01:57,210
So that's equity and. So. Maybe one funny experience experiment you may want to do, which is not required here.

538
01:01:57,300 --> 01:02:02,330
I in in the in the class but which I really, you know,

539
01:02:02,880 --> 01:02:12,960
encourage you to play with is that try to implement the my wisdom and try to implement something equivalent in the LDA for

540
01:02:12,980 --> 01:02:21,870
GSP algorithm or some gradient disintegration because you can also write the likelihood in just a it's not a closed form,

541
01:02:21,870 --> 01:02:28,290
but you can calculate the likelihood and you can numerically optimize it and try to see which algorithm works better.

542
01:02:28,530 --> 01:02:37,319
How does the ambiguity works better or does, you know, does in another me or some other algorithm work better?

543
01:02:37,320 --> 01:02:45,210
So this is a good problem. You can compare different algorithms and this mixture normal is very, very good problem.

544
01:02:46,270 --> 01:02:48,760
And if you have more capacity,

545
01:02:49,600 --> 01:02:58,149
we we all need help to talk about the mixture on normally the one dimension if you talk about the mixture minimum are into two dimension,

546
01:02:58,150 --> 01:03:02,200
three dimension, there's a lot more parameters you need to estimate.

547
01:03:02,200 --> 01:03:10,660
There's a correlation between them. So coming up with the right algorithm to to implement the M is a little more challenging.

548
01:03:10,990 --> 01:03:15,460
So that's that's worth trying, trying to think about as well.

549
01:03:15,790 --> 01:03:23,660
Okay. You know, you can usually simulate such data. And two dimensional data is a little more fun to visualize.

550
01:03:24,290 --> 01:03:28,010
If you wanted to make it as a project. Okay.

551
01:03:29,170 --> 01:03:35,550
So another question. Okay.

552
01:03:35,600 --> 01:03:48,720
So now let's move on to the new topic. Okay.

553
01:03:50,420 --> 01:03:54,610
So this topic in this topic, we are going to just you know.

554
01:03:55,820 --> 01:04:05,000
Talk about something that is technical, that is not a part of what I you know, what I promise to cover.

555
01:04:05,000 --> 01:04:11,329
But this is something that I think you are going to learn from some other classes, like.

556
01:04:11,330 --> 01:04:17,870
625 I could teach some of these and maybe six some of the six or seven may teach this, but.

557
01:04:20,030 --> 01:04:24,170
Because you need to make a package for the project.

558
01:04:24,470 --> 01:04:33,020
You just you. I think it's better to just teach about it and everyone and have everyone on the same page.

559
01:04:34,500 --> 01:04:41,720
Um. I'm not gonna teach you how to put your clothing to keep tabs.

560
01:04:43,320 --> 01:04:47,640
But once you make a package, it's very straightforward.

561
01:04:47,790 --> 01:04:52,310
There's tools like dev tools, other things you can easily find out.

562
01:04:52,320 --> 01:04:57,090
Find out how to do it if you have a problem with it. I'm happy to help you in.

563
01:04:58,110 --> 01:05:06,390
In the office. I like setting. Okay. So probably today we're going to not going to go through the pesky stuff.

564
01:05:06,400 --> 01:05:12,480
We're going to probably talk about the ACP, but before going to ACP, let's talk about benchmarking a little bit.

565
01:05:14,050 --> 01:05:23,890
So there is a package called the micro benchmark, which helps you to evaluate the performance of different things.

566
01:05:24,020 --> 01:05:31,990
Okay. So, so the nice thing about the micro benchmark is that it provides very precise, precise timings.

567
01:05:33,040 --> 01:05:36,610
And it makes it possible to compare operations between.

568
01:05:40,060 --> 01:05:44,340
Between different things and it repeats the same thing over and over.

569
01:05:44,350 --> 01:05:51,100
So if you have something that takes a really long time, maybe Microsoft Benchmark will run you like a thousand times or something.

570
01:05:51,100 --> 01:05:59,980
So it may not be a best way to do it, but if you have a have a specific routine you wanted to see, oh, which one is more?

571
01:06:01,170 --> 01:06:05,760
Uh, which one is more efficient or which one is not than this?

572
01:06:05,790 --> 01:06:08,910
This is a good thing to try.

573
01:06:09,240 --> 01:06:17,770
So. Micro benchmark is a kitchen stove that RCP in call lab takes about 30 seconds to install, so it'll take a little bit of time.

574
01:06:17,770 --> 01:06:23,560
So I'm going to just go through this. Okay, so this example.

575
01:06:25,320 --> 01:06:30,479
In this case, this is how you use it. So you generate a thousand random variables.

576
01:06:30,480 --> 01:06:36,210
And what it does is that you do the micro benchmark or calculating the square root of x.

577
01:06:37,170 --> 01:06:41,420
And X 2.5 and the XP and log.

578
01:06:41,430 --> 01:06:49,470
So these are calculating the same thing. But each of them takes the different amount of computational time.

579
01:06:49,760 --> 01:06:53,840
Okay, so you may not have it.

580
01:06:53,850 --> 01:06:56,910
You may just use this X to the 2.5.

581
01:06:57,330 --> 01:07:01,590
But this power calculation is a lot heavier.

582
01:07:01,860 --> 01:07:10,170
So it's actually it's even more expensive than doing the, the two operations that we expect exponential function and low function.

583
01:07:10,440 --> 01:07:20,770
So and calculating the security is a lot cheaper and you probably have not aware of these differences.

584
01:07:20,790 --> 01:07:26,009
So it's a good to know to compare these things.

585
01:07:26,010 --> 01:07:32,610
So these are in this case, it's a microsecond unit and it gives a mean a median and mix in mean.

586
01:07:32,610 --> 01:07:39,390
So it gives a pretty comprehensive overview of how each matter compared to each other in a similar setting.

587
01:07:42,340 --> 01:07:46,360
Okay. So. Okay.

588
01:07:46,570 --> 01:07:50,070
So. This is what I was talking about.

589
01:07:50,310 --> 01:07:56,190
Okay. So this gives almost a similar answer to the slide we just saw.

590
01:07:58,620 --> 01:08:04,620
Okay. So. Oh, sorry, I.

591
01:08:04,620 --> 01:08:07,630
I have one more thing to do here. Okay.

592
01:08:09,150 --> 01:08:12,390
So we can just go to 1 to 2. Yeah. So.

593
01:08:14,820 --> 01:08:18,660
We're going to go through this and the second item and they'll come back.

594
01:08:19,740 --> 01:08:26,640
So the next item we want to evaluate is this calculating, this cumulative sum.

595
01:08:27,240 --> 01:08:33,540
Okay, what is a cumulative sum? So do you have you use a com sum function in an in art?

596
01:08:33,540 --> 01:08:44,540
So it calculates the cumulative sum. So it it s so if you have ten elements, it calculates first elements just just store the same value.

597
01:08:44,550 --> 01:08:48,390
Second element stores are some of the first and second element.

598
01:08:48,720 --> 01:08:53,129
Third elements becomes first, first, second, third, and so on.

599
01:08:53,130 --> 01:09:05,130
So the way you can implement easily the same thing in R is basically you calculate the X of I as a addition of the previous number.

600
01:09:05,760 --> 01:09:09,530
Then this is not P manage number because it's a it accumulates, right?

601
01:09:09,540 --> 01:09:13,709
So accumulates everything from the summation from the left.

602
01:09:13,710 --> 01:09:20,780
So it basically by, by using these, it calculates the summation from, uh.

603
01:09:22,170 --> 01:09:31,010
From from the all the previous elements, from the first elements, first elements to the to up to that particle index.

604
01:09:31,350 --> 01:09:40,420
Okay. So for example, if you haven't used them, let me just show you X is like let's say I don't know.

605
01:09:42,690 --> 01:09:52,490
Let's say you wanted 1 to 10 and if you do if you did the pump some and it's it's going to basically evaluate this, discriminate something like this.

606
01:09:52,500 --> 01:09:57,210
So that's how it works. Okay. Then I hope that this is clear what it does.

607
01:09:58,450 --> 01:10:01,450
Okay. So you can you can do this using loop.

608
01:10:01,960 --> 01:10:08,590
Okay. But I think using loop is uncool.

609
01:10:08,980 --> 01:10:12,880
Okay. So why don't I use air supply?

610
01:10:13,180 --> 01:10:27,610
Okay. But if I use air supply, I know that I need to you know, if I use air supply, then when I assign this without global assignments,

611
01:10:27,610 --> 01:10:36,459
if I just do this, this is not going to update my, you know, X of the function finishes.

612
01:10:36,460 --> 01:10:44,610
So I need to do the global assignment. So, well, this is not pretty, but this probably cooler than this.

613
01:10:44,620 --> 01:10:51,790
Why don't I do this? Okay. So the second version of Air Supply basically does the same thing eventually.

614
01:10:51,790 --> 01:10:55,210
But using global assignment and above is a loop.

615
01:10:55,390 --> 01:10:59,080
At least. Right. Although air supply is also loop.

616
01:10:59,410 --> 01:11:10,040
Okay. Which one do you think is better? The cooler weather should be faster, isn't it?

617
01:11:11,390 --> 01:11:19,760
So let's see. So if you compare the first one, the second one, second one is much slower.

618
01:11:21,170 --> 01:11:27,680
Okay. The reason why is the economy is slow is that it's using a global assignment and global summit is very heavy operation.

619
01:11:28,160 --> 01:11:32,480
So you need to make sure that, you know, everything is, you know.

620
01:11:33,590 --> 01:11:38,510
So it tried a lot of value first and updated and it's just very slow.

621
01:11:38,600 --> 01:11:50,629
Okay. So in fact, just using loop operation is much faster under the one microsecond versus 937 seconds, ten times faster.

622
01:11:50,630 --> 01:11:54,500
So the loop is not so bad, especially as supply is a loop.

623
01:11:54,500 --> 01:12:03,620
So I don't really recommend to use a l apply with a supply over loop because I don't think that the performance will be very different.

624
01:12:04,510 --> 01:12:10,370
But what striking thing you might see is that this come some function is actually built in comes and function.

625
01:12:10,390 --> 01:12:13,630
If you compare this, this is 100 times faster.

626
01:12:14,560 --> 01:12:17,650
Wow. Okay. So that's embarrassing. So it's.

627
01:12:18,730 --> 01:12:23,890
You know it. This is a still this is okay, but still a hundred times slower than what it could be.

628
01:12:23,980 --> 01:12:27,540
Right. So that's one lesson.

629
01:12:28,290 --> 01:12:37,110
Okay. The second part is sort of unrelated, but I just wanted to briefly go through it.

630
01:12:37,620 --> 01:12:43,470
So what this does is just to connect the supply, apply and apply.

631
01:12:43,590 --> 01:12:47,520
Okay. So let's make a you have a matrix.

632
01:12:48,060 --> 01:12:55,200
If I have a matrix, then my my goal is to calculate accumulated sum for each of the rows separately.

633
01:12:55,410 --> 01:12:59,790
Okay. Then what?

634
01:13:00,030 --> 01:13:04,110
Each of each of a column separately. Sorry. Then what?

635
01:13:04,500 --> 01:13:09,890
What should I do? Okay. So one way to do it is using supply.

636
01:13:11,130 --> 01:13:16,290
And supply is basically taking a taking a vector and using that as a as a index.

637
01:13:16,290 --> 01:13:20,279
And you can either last apply this function.

638
01:13:20,280 --> 01:13:27,690
So it's is pretty similar to just using loop, but it's a, it's a using just a single line, right.

639
01:13:28,560 --> 01:13:35,010
So and apply function if you use apply function, it's more clear because if you have a matrix,

640
01:13:35,010 --> 01:13:40,590
apply function allows to apply some function in the either in the root by drawing by column.

641
01:13:41,400 --> 01:13:45,990
So you can in this case, I apply one to apply this function by column.

642
01:13:45,990 --> 01:13:49,380
So it should work. The loop com.

643
01:13:49,390 --> 01:13:55,410
Some function is a function that basically.

644
01:13:57,590 --> 01:14:03,229
Have these matrix and, you know, fill, fill, fill each of the columns.

645
01:14:03,230 --> 01:14:07,070
So this is not doing the double loop because double loop is going to be slow.

646
01:14:07,520 --> 01:14:14,870
And I know the some function is much easier, much faster. So I'm going to use a column, some function, but for each of the column, just using loop.

647
01:14:15,930 --> 01:14:19,350
So I'm going to use a micro benchmark to compare the performance.

648
01:14:19,410 --> 01:14:29,690
Which one do you think is faster? Which one is slower? Raise your hand if you think air supply will win.

649
01:14:32,660 --> 01:14:36,050
And if you think apply plywood we. Okay.

650
01:14:36,860 --> 01:14:47,660
How about Lupe? Oh, some people many people think look cool when I thought I always fond of applied functions, so I thought this will be better.

651
01:14:49,040 --> 01:14:54,469
Uh. But. When I tried to run it said this.

652
01:14:54,470 --> 01:15:02,120
This takes the train because it's running running quite of quite of the quite a bit of code.

653
01:15:03,520 --> 01:15:06,600
Repeatedly. Okay. So then.

654
01:15:06,610 --> 01:15:11,440
Yeah, so this one actually it is true the loop version is faster.

655
01:15:11,680 --> 01:15:14,749
So I was surprised actually. And apply function.

656
01:15:14,750 --> 01:15:18,520
It was slower. I still don't understand why.

657
01:15:18,730 --> 01:15:25,040
Okay. But this difference is a bit, you know, trivial though, right?

658
01:15:25,060 --> 01:15:32,650
So my my point I wanted to make using this example is that sometimes you think loop is bad, but loop is not so bad,

659
01:15:32,650 --> 01:15:40,390
especially if so if you use double loop is bad but single loop is not so bad and using as supply l apply.

660
01:15:40,810 --> 01:15:44,770
You think that those magically make it faster so those are not true.

661
01:15:44,770 --> 01:15:47,860
So I'm just trying to make that point.

662
01:15:48,130 --> 01:15:57,070
Okay. So now we have 5 minutes and I am glad that we have 5 minutes because I want to talk about this.

663
01:15:57,350 --> 01:16:08,440
Okay. So now so we're making a new version of a come some function, but we know that our implementation is really slow.

664
01:16:09,190 --> 01:16:14,710
So can we do some other implementation using C++ language?

665
01:16:15,280 --> 01:16:20,020
There is where our CIP function RCP package is coming from.

666
01:16:20,380 --> 01:16:24,490
Okay. So RTP well.

667
01:16:25,960 --> 01:16:31,150
I'll keep let me. Because this is supposed to start with a good temper.

668
01:16:31,360 --> 01:16:35,500
Let me just start with the example. I know. I know you don't know what this is about.

669
01:16:35,800 --> 01:16:41,170
You don't? I'm going to teach all this. The sentence in the syntax is later.

670
01:16:41,170 --> 01:16:51,030
But this is a function basically that is a written in C++ taking a vector and to do pretty much a similar thing with a common function.

671
01:16:51,040 --> 01:16:56,770
The in our version is a still loop, but it's just written in C++.

672
01:16:57,220 --> 01:17:03,940
So and you can compare this function using the SIP, using something called SIP function.

673
01:17:04,310 --> 01:17:08,230
Now once you compare this function, you can run this function and you can compare it.

674
01:17:08,770 --> 01:17:12,810
Okay. Then this is what it looks like.

675
01:17:13,150 --> 01:17:23,830
Okay. Uh, so these two still takes the hundred microsecond, nine, and their microsecond now comes and three takes a seven microseconds.

676
01:17:23,830 --> 01:17:29,950
This one takes about three microseconds. Right. So and median value is about twice slower.

677
01:17:29,950 --> 01:17:32,830
So twice slower. So it's still bad still.

678
01:17:33,220 --> 01:17:45,100
So worse than the implementation in the, in the very fast implementation that are implemented in Fortran usually in R but it it's pretty close now.

679
01:17:45,100 --> 01:17:48,320
It's an order of magnitude. Order of magnitude is is similar.

680
01:17:49,030 --> 01:17:53,829
So and if you look at this code, even if you can't read exactly.

681
01:17:53,830 --> 01:17:58,930
C++ code, this looks awfully similar to the loop version we just had.

682
01:17:59,530 --> 01:18:05,380
Right. So almost same code. It's just a different language, but it's much faster.

683
01:18:05,920 --> 01:18:09,430
Okay, so why is that? Why?

684
01:18:09,850 --> 01:18:21,190
You know what? So what we're going to learn is that sometimes RCP makes really, really things really, really fast, like orders of magnitude fast.

685
01:18:21,190 --> 01:18:24,430
Sometimes it doesn't make things faster. Okay.

686
01:18:25,090 --> 01:18:31,450
So you just need to know which cases you will expect a very huge improvement.

687
01:18:32,350 --> 01:18:39,150
And when you do your project or some other things, you can decide, oh, should I use our ship, okay or not?

688
01:18:39,160 --> 01:18:42,970
So that that's what you need to figure out.

689
01:18:43,240 --> 01:18:47,680
And that's what we will cover in the next few lectures.

690
01:18:48,250 --> 01:18:53,350
Okay. Next lecture, I guess in in in Wednesday.

691
01:18:54,910 --> 01:19:06,160
So this is a really good tool. RCP package is a really, really good tool to be able to use C++ in R, so I don't have much time.

692
01:19:06,160 --> 01:19:17,350
But you know, I had used the C++ before RCP and because I know that using C++ is much faster in R,

693
01:19:18,370 --> 01:19:23,409
it's enormous amount of coding if you didn't use RCP and it's very hard to debug.

694
01:19:23,410 --> 01:19:31,090
So when I, when I saw RCP came out like I felt like I wished all my time to implement in the

695
01:19:31,090 --> 01:19:36,190
package I had before because it was a lot more work just to do that at the time.

696
01:19:36,190 --> 01:19:39,399
So it's really a revolutionary. I really like it.

697
01:19:39,400 --> 01:19:42,130
So that's all I can say now.

698
01:19:42,700 --> 01:19:54,670
And we have one minute for question if you have any questions and we have yeah, we have office hour after that C C on Wednesday.

