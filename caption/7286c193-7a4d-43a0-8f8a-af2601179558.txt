1
00:01:16,740 --> 00:02:10,060
Like. Okay, let's start.

2
00:02:10,070 --> 00:02:20,990
So at the beginning, do you have some question about data like you want to discuss about the vaccination data or the mortality data?

3
00:02:21,230 --> 00:02:27,290
You are okay with the data. I'm saying we should just estimate the.

4
00:02:27,770 --> 00:02:35,390
So basically, let's say we find that in the age group, 8 to 64 in the US, there are 24 deaths.

5
00:02:35,490 --> 00:02:38,510
I'm 65, there's ten and below 18 is one.

6
00:02:38,710 --> 00:02:45,960
Right. And in Michigan, the number of deaths in that week was can we just scale that total to ten?

7
00:02:45,980 --> 00:02:49,969
So now you have four? Yeah, I exactly.

8
00:02:49,970 --> 00:02:54,010
I asked CDC to send me the Michigan data by age group.

9
00:02:54,020 --> 00:02:58,429
I don't know how. So they can give you data. I don't know if they are able to send me the data,

10
00:02:58,430 --> 00:03:06,559
but we can use the national ratio of the group distribution to actually like average sort

11
00:03:06,560 --> 00:03:13,430
of population average distribution and apply to Michigan data to get that group level.

12
00:03:13,940 --> 00:03:20,030
And what should we do with the should be like sample from like a low number?

13
00:03:20,030 --> 00:03:23,750
Because the problem is if you do a direct average, you're going to get like 0.5 to people,

14
00:03:24,080 --> 00:03:30,740
which you could do can't do that with a negative binomial model because it assumes cons of mixed responses.

15
00:03:33,330 --> 00:03:36,570
That's the problem. Just take a quick. Just round and round.

16
00:03:36,650 --> 00:03:39,840
Round two integer. You don't have 2.5 people.

17
00:03:42,710 --> 00:03:46,340
Yeah, yeah. Yeah. Perfect. Okay.

18
00:03:46,350 --> 00:03:49,780
So good. So. Oh, I don't know.

19
00:03:49,800 --> 00:03:55,170
How much do you know about the possible process? Maybe I can give you a quick review, hopefully, of all that.

20
00:03:56,460 --> 00:04:01,620
But for role is have you ever heard or you got give a quick review.

21
00:04:02,220 --> 00:04:05,310
I don't know how much I can memorize it, but let me give it a try.

22
00:04:09,960 --> 00:04:18,660
Oh, okay. Awesome process. Now, first of all, we're talking about common process, right?

23
00:04:18,660 --> 00:04:28,979
So copying a common process, I will see that you have this.

24
00:04:28,980 --> 00:04:36,550
And it's a process is a process that to, you know, cut to number of events.

25
00:04:37,050 --> 00:04:40,980
So you have zero time, you have time axis.

26
00:04:42,450 --> 00:04:54,359
Then you have the event of interest to occur at randomly all the time or in fact of the individual or a phone call or, you know,

27
00:04:54,360 --> 00:05:02,189
somebody randomly walk in the room, whatever, you know, I mean, maybe there are more occurs than there is couple of them.

28
00:05:02,190 --> 00:05:07,160
But basically are talking about the occurrences of a event.

29
00:05:07,170 --> 00:05:13,800
Right. Or something interesting like infection in this case or where the event of interest is recovery.

30
00:05:14,190 --> 00:05:23,610
So this is or time access. You have an event of interest recovery, infection or or somebody you know, there's something happening over time.

31
00:05:23,880 --> 00:05:27,570
Then, of course, that you given the time t you cut.

32
00:05:28,500 --> 00:05:32,280
Okay, how many basically you just cut, right?

33
00:05:32,670 --> 00:05:36,570
This is the first thing with history. We can do this, right?

34
00:05:36,810 --> 00:05:44,740
In this case, you can't do that. The five right is of course, five is random variable as a realization.

35
00:05:44,740 --> 00:05:49,559
What process. Right. So so okay. That's basically in process.

36
00:05:49,560 --> 00:05:56,460
Right. So you can see that this is very much like what we're talking about at the T and R, t.

37
00:05:57,030 --> 00:06:02,070
And first of all, this process, of course, is not negative.

38
00:06:03,190 --> 00:06:12,209
You are not doing a negative value. Right. And secondly, that this process is decreasing.

39
00:06:12,210 --> 00:06:26,660
And it's if you're talking about a time in future time versus the you know, the the past time that you always have more event and that same event.

40
00:06:26,670 --> 00:06:33,690
Right. Okay. And this is third one is integer.

41
00:06:36,630 --> 00:06:49,020
Got it. Okay. And of course, I will talk about one dimension that you can have more dimension, but this is essential, what you know you do.

42
00:06:49,020 --> 00:06:54,570
And based on this three things you are you are not able to figure out the probability law.

43
00:06:55,140 --> 00:06:58,170
Then you need a couple of conditions.

44
00:06:58,770 --> 00:07:05,510
So the most famous condition in the study of this kind of the process is memory loss property.

45
00:07:05,520 --> 00:07:09,140
Right. You know, this memory.

46
00:07:10,770 --> 00:07:15,210
Let's see this.

47
00:07:15,720 --> 00:07:20,640
I always you I say this assumption. Okay. How is this assumption right about the process?

48
00:07:21,180 --> 00:07:26,550
Sometimes people call independent increments. Independent.

49
00:07:34,000 --> 00:07:54,700
So what this says is that if you look at the time interval of crime to okay you look at the time interval here as and.

50
00:07:56,100 --> 00:08:06,570
She's right. And you look at how many events occur in this interval, which is this difference, right?

51
00:08:07,680 --> 00:08:12,710
And then this will be a the number.

52
00:08:12,990 --> 00:08:18,330
This is the enough number. Incremental number of events.

53
00:08:18,810 --> 00:08:23,270
This one will be independent to an order of time interval.

54
00:08:23,730 --> 00:08:26,940
So so we have independent increments.

55
00:08:27,180 --> 00:08:38,489
So if you look at another time interval here, which is exclusive or have no overlap with this interval,

56
00:08:38,490 --> 00:08:49,100
then in the red and law of that, how many events you observe here is independent of how many you then ran in all reserve here?

57
00:08:49,860 --> 00:08:55,650
Or you can say what happens within the window has nothing to do with anything else outside of window.

58
00:08:56,040 --> 00:09:00,719
That's basically the memory is proper to work in different increments.

59
00:09:00,720 --> 00:09:11,070
So the probability law of this increments has nothing to do with what happens outside.

60
00:09:11,250 --> 00:09:17,190
Okay. In terms of, you know, the the.

61
00:09:18,720 --> 00:09:29,490
What happens happens happens right outside the window, the interval.

62
00:09:32,130 --> 00:09:34,890
That's essentially the assumption of that.

63
00:09:35,750 --> 00:09:45,840
And so this is very important property this called intimately independent increments is essentially a sense that, you know,

64
00:09:46,200 --> 00:09:52,529
you have this sort of of independent chance of things coming, of course,

65
00:09:52,530 --> 00:10:00,390
that as long as you don't have overlap over time, then you just have the independent.

66
00:10:01,050 --> 00:10:04,170
So this is very important assumption or the one.

67
00:10:05,430 --> 00:10:16,730
Another important assumption here is that the the probably law of this doesn't depend on which time specific time you're talking about.

68
00:10:16,740 --> 00:10:29,070
It only depends on the length of time. So the probability law of office implement on paper mental clocks.

69
00:10:30,060 --> 00:10:40,260
Right. Only depends on the length of time coming before.

70
00:10:42,570 --> 00:10:54,810
Okay. It does not and does not depend on the physically time space.

71
00:10:57,790 --> 00:11:01,070
It doesn't mean that it doesn't matter.

72
00:11:02,250 --> 00:11:11,790
This event you're talking about, this time interval today, like from 1 to 2, or you talk about this interval estimate, they have the same law.

73
00:11:11,910 --> 00:11:20,700
It doesn't matter which time in which space and time you talk about it only depends on the length of the time interval we're talking about.

74
00:11:20,700 --> 00:11:26,280
This is two important assumptions that to build a concrete process.

75
00:11:26,640 --> 00:11:33,970
Now what is a present process? So you understand that then we need the process.

76
00:11:34,010 --> 00:11:38,130
Process. Okay.

77
00:11:44,840 --> 00:11:48,710
So property prices, of course, a special case of counting process.

78
00:11:49,040 --> 00:11:54,830
Okay, so so how this process the process is spatial.

79
00:11:55,430 --> 00:12:06,620
It is when you talk about this a two and then the N, T minus and s,

80
00:12:08,810 --> 00:12:23,450
this probably law follows the possible distribution of labor and the length of time which is T minus s.

81
00:12:26,650 --> 00:12:42,720
Okay. So many times Lambda Times the letter and the meaning of from it goes processing but serial moodiness by the homogeneous properties.

82
00:12:43,180 --> 00:12:50,350
Yeah. So I'm talking about simple case. Okay. Yeah, we're talking about homogeneous problem process.

83
00:12:50,440 --> 00:12:58,300
Okay. So that's why I have a two there. Mm hmm. So basically what to do if you want.

84
00:13:03,370 --> 00:13:10,420
So homogeneous problem process is sometimes when we say the process of wimp meant homogeneous problem process.

85
00:13:10,780 --> 00:13:15,780
If we are talking about in a homogeneous well, we talk about being homogeneous.

86
00:13:15,780 --> 00:13:19,810
The problem with that like adjective specific to.

87
00:13:20,110 --> 00:13:24,600
Oh, but anyway that that's essentially this this.

88
00:13:26,290 --> 00:13:35,380
So you can see that the the probability law to generate this incremental count will be from this partial distribution.

89
00:13:35,400 --> 00:13:46,780
This is the average incidence rate and time you look at the length of the time doesn't depend on actually the specified time.

90
00:13:46,780 --> 00:13:51,910
It only depends on the the length of the interval to generate the process.

91
00:13:52,300 --> 00:13:55,420
Okay. So that's a process and process now.

92
00:14:03,970 --> 00:14:18,060
So if you have like let's let's put a simple way that here is my, you know, the I'd say recovery, okay?

93
00:14:18,140 --> 00:14:23,770
Our team is the number of the remove the case.

94
00:14:23,770 --> 00:14:29,650
That's the number of removed cases.

95
00:14:31,100 --> 00:14:36,640
Okay. So picking example are is a remove cases.

96
00:14:36,650 --> 00:14:45,250
Of course, this is a interannual time source into the body type of process and this is increasing, right.

97
00:14:45,250 --> 00:14:51,610
Because you have more and more people moving to the our compartment that our to you will be a non

98
00:14:51,610 --> 00:15:00,070
decreasing counting process that if you are waiting to show a partial distribution as we did here,

99
00:15:00,670 --> 00:15:03,920
then you can turn this to a production process. Okay.

100
00:15:05,980 --> 00:15:22,740
And so also that that the so what you have here is like if you have like t one, which is 24 hours, then you have T two, which is second day.

101
00:15:22,760 --> 00:15:32,890
So if you look at daily data or you look at the weekly data, whichever way you want to sort of formulate your likelihood.

102
00:15:33,490 --> 00:15:39,520
So the number events here are t one, okay.

103
00:15:39,520 --> 00:15:51,070
That's basically the, the number of recovered removed cases from zero to T one and then you have rt2 minus rt1.

104
00:15:52,240 --> 00:16:00,940
This is number of remove the cases to happen in the next time unit time interval either next week or next date.

105
00:16:01,570 --> 00:16:10,510
They are independent according to the are sort of incremental of sort of independent increment assumption.

106
00:16:11,200 --> 00:16:19,810
So this is and each one will follow a partial distribution with a common parameter so that you're able to write the likelihood out.

107
00:16:20,620 --> 00:16:29,880
Okay. So that's basically the idea of generalizing this s are using the possible bosses so you can do email,

108
00:16:29,950 --> 00:16:38,740
you can you can imagine that that you can extend to this T and more and more to you create

109
00:16:39,810 --> 00:16:48,310
an overlap time intervals and count number of the removed cases within each intervals.

110
00:16:49,090 --> 00:16:53,139
Each of the intervals that are are not overlap each other.

111
00:16:53,140 --> 00:16:59,110
So you have this independent increment and then you can write the likelihood as the product

112
00:16:59,110 --> 00:17:06,040
of the you'll know the likelihood of individual observations happening at each interval.

113
00:17:06,040 --> 00:17:09,160
So, so that's basically idea of formula to make it good.

114
00:17:11,740 --> 00:17:19,120
So maybe I take opportunity to talk a little bit about exponential distribution, if I can remember it.

115
00:17:21,710 --> 00:17:28,240
This is a part of quantified that for me. So, so let's let's talk about this.

116
00:17:28,450 --> 00:17:34,120
We can time, right? We can talk because we're talking about I want to give you the idea of how this

117
00:17:34,690 --> 00:17:39,370
sort of the transmission rate is under exponential distribution will follow it.

118
00:17:39,410 --> 00:17:46,389
You know, something to to really. So the lambda is more like a transmission, right?

119
00:17:46,390 --> 00:17:51,070
So anyway, so let's talk about waiting time, I think is a very related part.

120
00:17:54,900 --> 00:17:59,170
All right. So what you're trying to do here is.

121
00:17:59,860 --> 00:18:05,080
So here's time. Access can see if we're not in or it can move there.

122
00:18:06,430 --> 00:18:09,759
Okay, so you have your zero time there.

123
00:18:09,760 --> 00:18:26,070
You look at the of the time that at time t you are, you wonder, oh, what what is so I have.

124
00:18:26,920 --> 00:18:43,150
All right, so let me see. You want or that what will be the time of your first time of first.

125
00:18:45,140 --> 00:18:48,230
Occurrence of the events.

126
00:18:51,890 --> 00:18:58,820
Okay. So you see how long I have to wait from zero time to observe the first occurrence of the event.

127
00:18:59,150 --> 00:19:03,920
How long? I have to wait to see my first removed case.

128
00:19:04,070 --> 00:19:12,920
Okay, so how what's the distribution of the time? Okay, so basically you calculate the this probability.

129
00:19:13,250 --> 00:19:19,130
Okay. So how long you have to wait to the to see the first occurrence?

130
00:19:19,820 --> 00:19:30,860
Well, because the time is bigger than t that means you do not observe any event happens between zero and T because the

131
00:19:30,860 --> 00:19:37,969
capital T is the time you see first occurrence of the event of the first occurrence when t is bigger than little t,

132
00:19:37,970 --> 00:19:41,270
that means from zero to little t, you don't see anything.

133
00:19:42,920 --> 00:19:49,220
You agree because T is the time of first occurrence of the event.

134
00:19:49,790 --> 00:19:54,240
But you're looking at what's the time? The first of those five figured out.

135
00:19:54,680 --> 00:19:58,910
That means from zero to little T, there's no event. Right.

136
00:19:59,270 --> 00:20:04,819
So this is equivalent to that from T to zero.

137
00:20:04,820 --> 00:20:12,290
You have zero count. Right. You have come prices at the zero time at a time.

138
00:20:12,710 --> 00:20:16,340
You don't see anything, right? Zero. Nothing happened.

139
00:20:16,340 --> 00:20:19,610
B to zero. T your grid.

140
00:20:19,910 --> 00:20:24,350
Okay, then this is this from zero to cheap.

141
00:20:25,430 --> 00:20:28,970
This this energy follows a partial distribution.

142
00:20:30,280 --> 00:20:33,410
If we are talking problem process this lambda t right.

143
00:20:36,890 --> 00:20:47,930
This is the basic problem process. So when this equal to zero, you could have lost the key to the k e to the -0 and zero factorial.

144
00:20:49,850 --> 00:20:54,770
This problem distribution. Right. Right.

145
00:20:54,980 --> 00:20:58,640
Parcel distribution. When you go to zero.

146
00:20:59,150 --> 00:21:06,260
Right. So that's basically probably. So that would use you will need to list minus Q lambda.

147
00:21:08,380 --> 00:21:11,440
Right. I hope I pass the qualified exam.

148
00:21:11,440 --> 00:21:20,680
At least get the 60% of my score. So what is this, the viral distribution of your exponential distribution?

149
00:21:21,880 --> 00:21:24,980
Right. This is survival distribution of going to school.

150
00:21:25,990 --> 00:21:27,850
That means. Okay.

151
00:21:28,270 --> 00:21:38,040
The key, if you define this as the first occurrence of that event, this falls exclusive distribution, which is primarily about what is lambda lambda?

152
00:21:38,050 --> 00:21:41,530
Is this lambda in the processing process of the transmission rate?

153
00:21:43,150 --> 00:21:47,820
Okay, so now what's the name of the team? Well, over long.

154
00:21:50,090 --> 00:21:56,150
Right. So what's the average time you have to wait in order to see that first occurrence.

155
00:21:57,530 --> 00:22:06,439
Okay. So that's basically why we use one over the transmission route as the full downstream interpretation

156
00:22:06,440 --> 00:22:13,550
of what we have been used to or a time to do the homework one one to figure out the the mean.

157
00:22:13,730 --> 00:22:21,780
Okay. So that's the first how well, the second one. Okay.

158
00:22:22,530 --> 00:22:26,370
So so let's see how we're going to look at a second occurrence of time.

159
00:22:29,540 --> 00:22:37,460
So. Okay.

160
00:22:39,590 --> 00:22:43,070
Now I try to get another 30% of this question.

161
00:22:44,150 --> 00:22:47,930
Okay, so suppose this is my key. Okay.

162
00:22:48,170 --> 00:22:52,700
There must be a time you've never talk about seeking the occurrence of this event.

163
00:22:52,970 --> 00:22:57,010
There must have time at which you observed first events.

164
00:22:57,680 --> 00:23:02,720
Let's say that's the time one.

165
00:23:02,960 --> 00:23:06,140
This is the one you observe in the first event. Right.

166
00:23:06,710 --> 00:23:12,410
So this is the time. First.

167
00:23:14,890 --> 00:23:24,280
Of course. And that supposed that conditional of one equal faith.

168
00:23:24,890 --> 00:23:34,750
S So what is the chance that the time that you will see something after the t?

169
00:23:36,260 --> 00:23:42,400
Okay, so you calculate this distribution. No.

170
00:23:42,690 --> 00:23:46,770
So what is that? So basically you use the same argument.

171
00:23:47,520 --> 00:23:53,700
Right. So you basically say that I would have if the first occurs is right.

172
00:23:53,700 --> 00:24:01,350
And s then you are looking at the the interval here from T.

173
00:24:02,960 --> 00:24:09,560
Right. There should be no way that you or your will be observe the secret event, the cigar curse,

174
00:24:10,190 --> 00:24:18,590
because the first occurs condition, unless the first occurs, is right here, right at a commission, all that.

175
00:24:18,920 --> 00:24:28,730
So if I take this looking at that, this is equivalent to see that from he who.

176
00:24:30,510 --> 00:24:38,210
S and and there's no direct connection this.

177
00:24:41,600 --> 00:24:45,600
Right. Right.

178
00:24:45,900 --> 00:24:54,900
Because you basically if you believe that and and then first you have the intervention that increments because this event

179
00:24:55,380 --> 00:25:07,440
that happens between s02s the first one happens between 02s has nothing to do with what's going to happen right here.

180
00:25:08,070 --> 00:25:18,500
So this becomes a operational. Because independent memories.

181
00:25:18,920 --> 00:25:22,490
What happens outside this interval, that's nothing to this.

182
00:25:23,090 --> 00:25:26,510
Anything else? Okay. So here is what you get.

183
00:25:26,960 --> 00:25:31,430
So this follows a possible distribution, then?

184
00:25:31,430 --> 00:25:35,410
This will only depend on the key to the long.

185
00:25:38,230 --> 00:25:55,460
My. Right. So if you formulate this a little bit like the way that s t right is basically if it appears to the first time I, I do now, right.

186
00:25:55,480 --> 00:26:07,670
The, you know, the way that you do as you think about you have a tee time after a s property is the the short time after it s right so that you

187
00:26:07,670 --> 00:26:24,920
can do a little bit like this and t plus s t plus s and then here you only have on the t a t becomes what the length of interval.

188
00:26:28,520 --> 00:26:34,700
So this exactly same distribution as the first one derived, the first one derived is from zero to t.

189
00:26:35,090 --> 00:26:40,300
Now if you look at another interval with the same length from the first time,

190
00:26:40,310 --> 00:26:47,390
first time of current occurs the time of first course t time interval, you would have exactly the same distribution.

191
00:26:51,530 --> 00:26:59,899
Okay. So what I'm saying here is that, no, you can't you can't you can't extend this to any other time interval.

192
00:26:59,900 --> 00:27:07,190
As long as the interval length is t value would have explained to distribution with the repressed or lumpen.

193
00:27:08,820 --> 00:27:19,740
This is derived from the production process as part of the understanding of the art of this process.

194
00:27:20,150 --> 00:27:27,059
Okay. I think I can get 90% of the score for this question, the quality exam.

195
00:27:27,060 --> 00:27:33,840
But anyway, just quicker reveal what has happened for this company process of processing.

196
00:27:33,840 --> 00:27:36,930
Processing. And that's all I remember.

197
00:27:37,590 --> 00:27:46,310
Unfortunately, yeah. So but hopefully you'll get the taste of that composition process works and how that thing that we can do.

198
00:27:46,320 --> 00:27:50,020
Okay. Now back to this morning. Okay.

199
00:27:51,650 --> 00:27:54,870
So turn off and talk about the morning now.

200
00:27:55,020 --> 00:28:02,580
Okay. So what we're trying to do here is really looking at the Delta T time.

201
00:28:02,640 --> 00:28:07,170
So the time interval is very, very a short time interval.

202
00:28:07,830 --> 00:28:21,530
And look at how many sort of cases has been lost from the susceptible compartment and we assume a portion process that follows this fact.

203
00:28:22,290 --> 00:28:33,270
So one thing I should point out that this is is really not the same as we just considered before.

204
00:28:33,720 --> 00:28:44,970
So this is a little bit more complicated because here it not only depend on the time interval, but also depend on the T s and I t.

205
00:28:45,870 --> 00:28:53,240
Okay. So this is a inhomogeneous fossil process is a little bit more complicated than what you think.

206
00:28:53,260 --> 00:28:58,470
Right before we say that you only have beta if that's a homogeneous process of process.

207
00:28:58,890 --> 00:29:04,170
Now you have this one that can depend on the the previous position.

208
00:29:04,830 --> 00:29:09,360
Okay, this is inhomogeneous, but you get to the flavor.

209
00:29:09,360 --> 00:29:12,540
If you want to go live before or down to that, you can do that.

210
00:29:13,130 --> 00:29:22,980
Okay. But anyway, so this essentially is increment in this problem process that gives you this.

211
00:29:22,980 --> 00:29:30,570
And for the the recovery, the removed compartment, you have this process.

212
00:29:30,570 --> 00:29:41,850
And as I mentioned before, that you cannot define a from which process and process for it because it is not a monotonic quantum process

213
00:29:41,870 --> 00:29:50,240
counting process required monotonously your i.t to now have that multiplicity so you cannot directly define t.

214
00:29:51,090 --> 00:30:03,590
So in some sense that of putting the Poisson process as a stochastic model to extend the ACR is kind of limited.

215
00:30:03,600 --> 00:30:05,370
If you have more compartments,

216
00:30:06,090 --> 00:30:16,739
maybe you have more of like the process that do not satisfy Montano City so that you cannot really possibly process tomorrow,

217
00:30:16,740 --> 00:30:23,610
but suddenly you need something more channel. Then enrollment is problem process to really model what's going on.

218
00:30:24,270 --> 00:30:32,520
So space with space based model we talk about would not depend on that kind of model positive.

219
00:30:32,850 --> 00:30:36,959
So that's why it's more general than what what you see here.

220
00:30:36,960 --> 00:30:45,180
But anyway, this it's a very classical way, people to bring in the Castle City into the S.R. model through the process and process.

221
00:30:45,210 --> 00:30:55,140
You mentioned the present process. Then they're trying to get this sort of employees, set it up and test them on the parameter.

222
00:30:55,260 --> 00:30:58,270
Okay. Clear. Okay.

223
00:30:59,540 --> 00:31:08,249
Okay. So, uh, so now we have the, the understanding this is our model.

224
00:31:08,250 --> 00:31:18,050
And based on what, you know, we talk about sometimes people call this time interval the length of time you talk about in survival analysis, right?

225
00:31:18,090 --> 00:31:26,360
They call the gap time. But that time is time between the first occurrence and second occurrence.

226
00:31:26,370 --> 00:31:32,940
Right? So the gap time the gap time means that during the two weeks that you do not do anything, it's called gap time.

227
00:31:33,690 --> 00:31:41,220
Okay. So that so that's really something that we're trying to do here.

228
00:31:42,630 --> 00:31:46,380
Hmm. Okay. So let me move on.

229
00:31:46,500 --> 00:31:50,500
Mm hmm. So let's talk about the family.

230
00:31:50,520 --> 00:31:56,730
Okay. So we first, for example, we take a D to be a date as a time a unit.

231
00:31:56,850 --> 00:32:07,980
Okay. So based on sampling from the public civilians database and we can this quote ties this stochastic aside to model over days.

232
00:32:08,010 --> 00:32:19,770
Basically we take the D to be on the one day in this kind of discrete decision and then we have this increment.

233
00:32:20,700 --> 00:32:24,600
So this gives you a non-negative integer value,

234
00:32:25,320 --> 00:32:32,490
basically telling you how many individual you will lose the next day from the specific susceptible compartment

235
00:32:33,030 --> 00:32:41,940
and the delta r t is the increment count for the number of people who moved to the removable compartment.

236
00:32:42,030 --> 00:32:52,050
Okay. So both are demand conic counting process that we assume the possible process and we can model them.

237
00:32:52,230 --> 00:32:56,520
Okay. So what you're trying to do now, first I look at that.

238
00:32:56,520 --> 00:33:04,890
The second problem process contains only the remove parameter comma in this case,

239
00:33:06,120 --> 00:33:10,319
so we can formulate a likelihood based on the inhomogeneous partial process.

240
00:33:10,320 --> 00:33:13,830
So you have the theta. The theta are increments.

241
00:33:14,040 --> 00:33:19,170
How many people are moving into the our compartment on a daily basis.

242
00:33:19,200 --> 00:33:28,860
Okay. So this is the data you observe. Suppose you can capture this increment count from the civilian system out of the end days.

243
00:33:28,980 --> 00:33:32,790
Okay. So that gamma is the parameter of interest.

244
00:33:32,790 --> 00:33:43,680
And then because of the independence, given the the r t and given it conditional i t number of infections.

245
00:33:44,100 --> 00:33:56,580
This is cumulative thing. And then you you can write this problem process according to the independent increments assumption.

246
00:33:57,060 --> 00:34:01,560
You can write them into a system of log likelihood.

247
00:34:01,770 --> 00:34:10,100
Right. So this essentially the F is a proton distribution with parameter gamma.

248
00:34:10,110 --> 00:34:14,849
It is one. Okay. So here are gamma.

249
00:34:14,850 --> 00:34:20,110
It would be the property plus on parameter.

250
00:34:20,140 --> 00:34:27,110
And that's that is would be the the approximate distribution and log portion

251
00:34:27,130 --> 00:34:33,720
distribution is like this with the parameter of gamma to here you conditional it.

252
00:34:34,080 --> 00:34:47,040
Of course it is the information that you collect at time t and you're looking at next next time you know next date.

253
00:34:47,040 --> 00:34:54,910
What's the the increased number of the removals from the of yeah in the our compartment.

254
00:34:55,260 --> 00:35:02,120
And you have independent observations due to the independent increment assumption from possible process.

255
00:35:02,130 --> 00:35:07,460
Then you can write this. Okay. So this f k lambda is proton.

256
00:35:08,310 --> 00:35:20,160
Uh, probably mass function of variable k k is basically the number of cases where in this case a removed cases the same mean parameter lambda,

257
00:35:20,160 --> 00:35:30,299
which everybody knows how to write. So you have the initial value here to start with the, the thing.

258
00:35:30,300 --> 00:35:40,460
And you know, so basically what you're trying to do here is to formulate a likely function instead of using this the square method.

259
00:35:40,580 --> 00:35:40,710
Right.

260
00:35:40,860 --> 00:35:54,689
We consider method moments where we do, you know, this sort of moment conditions or to the calibration type of way to estimate lambda parameter.

261
00:35:54,690 --> 00:35:59,790
Now we can formulate likelihood under problem process assumption.

262
00:36:00,150 --> 00:36:10,260
Then we can estimate the domino. So after you get this, then you can run like a minor regression that's quite straightforward to to do that.

263
00:36:11,910 --> 00:36:22,820
Yeah. So. So are the I.T. is something observed, right?

264
00:36:22,850 --> 00:36:31,640
Because that's basically number of infections, individual in the population.

265
00:36:32,630 --> 00:36:42,020
So your your fossil me is I lambda I look for common I t right.

266
00:36:42,350 --> 00:36:47,060
So if you do look the new normal, that's what do we do, right?

267
00:36:47,510 --> 00:37:09,720
So we become us longer. Okay.

268
00:37:10,320 --> 00:37:15,720
So so if you look at this and simply this, you can regard this as an intercept.

269
00:37:15,810 --> 00:37:20,370
This is a parameter that you don't know what I'm writing here.

270
00:37:20,370 --> 00:37:24,959
Sorry, comma. You don't know. Of course Gamma is the parameter.

271
00:37:24,960 --> 00:37:32,820
Also log gamma is your parameter, right? And so this is something you want estimate that can be regarded as an intercept.

272
00:37:33,930 --> 00:37:37,370
Right? And what is this you observe i.t.

273
00:37:37,440 --> 00:37:42,479
Which is the data you can get from the surveillance system.

274
00:37:42,480 --> 00:37:46,200
This is like an indoor model. There's a spatial term for this term.

275
00:37:46,470 --> 00:37:50,580
Spatial name for this term. You'll remember it is called offset, right?

276
00:37:55,080 --> 00:38:02,520
So in the year end function, you can allow to put the lock a term offset to run the regression.

277
00:38:02,730 --> 00:38:07,980
Right. So, so now from this point of view,

278
00:38:08,460 --> 00:38:19,180
because what you are trying to do here in for the for the maximization of this right maximize the log likelihood is really essentially wrong.

279
00:38:19,200 --> 00:38:28,800
A log in in a regression model is the data of delta IP with this offset term log IP and comma.

280
00:38:29,100 --> 00:38:34,710
So you can map all of this comma, you can put a log.

281
00:38:36,390 --> 00:38:40,890
Yakama This is recovery rate. What do you think?

282
00:38:41,070 --> 00:38:46,560
What were the covers you think it will be affecting this recovery or removal rate?

283
00:38:47,250 --> 00:38:52,740
You know, you can put some X transpose RFA, right?

284
00:38:53,280 --> 00:39:00,629
So Corvids, you can put some corvids into this to explain the you know, this maybe you know,

285
00:39:00,630 --> 00:39:13,100
this will be pax what the track or you have more possible bats we have I don't know what what that

286
00:39:13,170 --> 00:39:22,290
where you want to put into this and also you could say well maybe I can do a time grading recovery.

287
00:39:22,980 --> 00:39:34,140
Right. So, so I could make comma to be time varying then I can make so this can be your say the parameter.

288
00:39:35,190 --> 00:39:40,710
Okay, define I say the parameter then you can put this whole thing in the can.

289
00:39:43,370 --> 00:39:50,910
They are fun to scan to ask them this non parametric function as part of this because you have data here,

290
00:39:51,480 --> 00:39:58,740
then you have the the model here your model is modeled on comma i t is your offset.

291
00:39:59,310 --> 00:40:03,150
Okay? So you can do lot of things that under this framework,

292
00:40:04,170 --> 00:40:14,430
namely these locking in their possible regression framework and that you can do of bringing some covers into this modeling and so on.

293
00:40:14,670 --> 00:40:19,200
Okay. If you don't want to do any modeling, then here's this solution.

294
00:40:21,250 --> 00:40:29,790
Okay. So. So here is this solution. You can you can get you can easily solve this maximize this from percent distribution.

295
00:40:31,360 --> 00:40:34,780
That's the really the solution you can get. Okay.

296
00:40:36,460 --> 00:40:48,490
So I don't I don't know if this solution is really something important because it's too simple, but you could, you know, maximize that.

297
00:40:49,210 --> 00:40:55,540
More interesting situation is that you bring some of covers into that that can solve the problem.

298
00:40:56,050 --> 00:41:02,800
So we should try our best to impose a afl.com.au friends or to understand what are

299
00:41:02,800 --> 00:41:08,230
the driving factor behind the recovery rate rather than just a recovery rate.

300
00:41:09,640 --> 00:41:16,360
So, you know, you can put a gamma parameter as a function over time or seasonality.

301
00:41:16,930 --> 00:41:25,659
Do you believe that people are more likely to recover in the summer time because they can stay outside a higher temperature?

302
00:41:25,660 --> 00:41:30,550
Or if the people are living in some, you know,

303
00:41:31,450 --> 00:41:38,769
regions where they have better access to hospitals and or some house cares that they can have faster recovery.

304
00:41:38,770 --> 00:41:48,489
And you know, that Paxil. So it certainly is a drug that is approved by for emergency use by FDA to treat COVID disease.

305
00:41:48,490 --> 00:41:52,980
But, you know, you have a lot of choices there and then to to feed this.

306
00:41:53,080 --> 00:41:56,080
Okay. So that's something you could consider.

307
00:41:58,740 --> 00:42:06,360
So in this context, because you bring this stochastic city into this model and formula likelihood in principle,

308
00:42:06,810 --> 00:42:11,520
you can really obtained a standard error of the estimate.

309
00:42:11,580 --> 00:42:14,950
Right. In the error you provides.

310
00:42:15,630 --> 00:42:25,140
But here the MLP certainly as formulated under these strong assumptions of this independent increments and

311
00:42:25,140 --> 00:42:34,620
then the standard arrows are subject to the are the risk of violation of that independent assumption.

312
00:42:34,740 --> 00:42:40,740
But your your our package you allow the neither model either parametric regression a non

313
00:42:40,740 --> 00:42:46,860
parametric regression can give you the standard as standard arrow of your MLA estimates.

314
00:42:47,100 --> 00:42:51,590
Okay. What are all those estimates are reasonable or not.

315
00:42:51,600 --> 00:43:04,440
It's really depends on how you make assumption of your data and your model that that's certainly easy to carry out under these tier M framework.

316
00:43:05,820 --> 00:43:10,500
So now moving on to the beta, right? So beta is a little more involved.

317
00:43:12,930 --> 00:43:18,209
So can be obtained by the first part of the process of stochastic sire.

318
00:43:18,210 --> 00:43:25,600
And in this case you need the increment and the cumulative sort of the is the,

319
00:43:25,990 --> 00:43:34,260
the size of the susceptible compartment and cumulative size of your eye compartment.

320
00:43:34,260 --> 00:43:39,629
And then you can look at the department process.

321
00:43:39,630 --> 00:43:45,480
Here is the assumption. Oops, oops, it's too fast.

322
00:43:53,540 --> 00:44:00,320
This is not process and process anymore. Oh, okay.

323
00:44:00,950 --> 00:44:07,030
Finally, we reach equilibrium. Just a left hook.

324
00:44:07,840 --> 00:44:15,820
Okay. I'm glad that I'm neither fly, no pilot.

325
00:44:15,830 --> 00:44:22,820
I'll be very panicked this moment. Okay, here we go.

326
00:44:23,750 --> 00:44:29,570
Okay, so here is the process. Process where you see the parameter of beta, right?

327
00:44:29,630 --> 00:44:37,730
So. So that this is the data you are going to get and in order to formulate your likelihood estimate of parameter beta.

328
00:44:38,150 --> 00:44:48,440
So here you can easily see that this whole thing will be, you know, this ratio will be your after term and very similar way of doing that.

329
00:44:51,730 --> 00:44:59,440
So that's that's basically precisely we can write the likelihood based on increments

330
00:44:59,440 --> 00:45:07,330
of the the susceptible compartment and then you have your possible distribution.

331
00:45:09,010 --> 00:45:16,000
Why people don't have an active binomial process. Have you ever see the like negative binomial process rather than present process?

332
00:45:16,570 --> 00:45:23,770
Because this one has certainly the issue of over dispersion can now hand over to some or maybe generalize possum distribution.

333
00:45:23,770 --> 00:45:27,610
You have some generalized approximate distribution when all the we have to

334
00:45:28,330 --> 00:45:33,309
generalize across some process or some time down rather than possum process.

335
00:45:33,310 --> 00:45:34,629
And but anyway,

336
00:45:34,630 --> 00:45:46,090
I'm just having a random thought here why we stick on possum process because of familiarity or is because this is really good model we can use.

337
00:45:46,090 --> 00:45:57,459
But certainly you can work on generalized processing to to rewrite the likelihood based on generalized possibly feel like but in a way that I don't

338
00:45:57,460 --> 00:46:09,160
think people have ever done that for generalized processing process anyway so here is your like your functioning F is your problem mass probability,

339
00:46:09,160 --> 00:46:16,090
mass function. This is your offset and you can model this by a model beta.

340
00:46:16,090 --> 00:46:24,579
This transmission rate transmission route can be time, train could be seasonality, it could be some of the control measures.

341
00:46:24,580 --> 00:46:30,150
For example, you know, vaccination vaccination should come to the debater.

342
00:46:30,610 --> 00:46:34,810
So vaccination certainly will change the transmission rate in the population.

343
00:46:35,500 --> 00:46:42,489
That could be a very important parameter to inter this model to understand the the fact

344
00:46:42,490 --> 00:46:51,430
that vaccination to estimate transmission rate and after to get the beta and gamma,

345
00:46:51,430 --> 00:47:03,700
then you can take the ratio to get your basic reproduction number or you can get your estimated effective reproduction number.

346
00:47:04,310 --> 00:47:14,410
Are you t now people say you can also join the estimate beta gamma in this case that you can use both

347
00:47:14,410 --> 00:47:23,110
the incremental data from your specific compartment and the incremental comes from your R compartment,

348
00:47:23,590 --> 00:47:31,780
conditional SE and T the cumulative size of your compartment, s compartment, that cumulative size of your eye compartment.

349
00:47:31,780 --> 00:47:38,079
And then you can write them under the in independence here.

350
00:47:38,080 --> 00:47:45,520
You believe that there is assumption here that Delta T and Delta R t are independent.

351
00:47:45,520 --> 00:47:52,660
So then you can write them into the separate. Do they really are are they really independent?

352
00:47:52,810 --> 00:47:56,190
I don't think so. But that's why people write in their legislature.

353
00:47:56,260 --> 00:48:01,930
Okay. I will tell you why this is a legal way to write it.

354
00:48:02,440 --> 00:48:02,830
Okay.

355
00:48:03,220 --> 00:48:16,570
So do you believe the these change in the S compartment and the incremental change in the recovery, the remove compartment, statistically independent?

356
00:48:16,720 --> 00:48:23,980
I don't think so, because they're coming from consistent. How many people move out from s there should be somehow related?

357
00:48:24,010 --> 00:48:34,090
How many people move into the s r right. So so there are not new per se, but under this formulation, the the human the independence.

358
00:48:34,960 --> 00:48:43,240
Now why they want to estimate them jointly because they want to have more data.

359
00:48:43,690 --> 00:48:48,460
Okay. To to estimate the parameter 202 in this marginal estimation.

360
00:48:48,820 --> 00:48:58,550
So you only have margin likelihood for s you estimate better you have margin likelihood for your R, you ask them of Gamma.

361
00:48:58,790 --> 00:49:02,589
So you want to put them together. But at the end of day,

362
00:49:02,590 --> 00:49:10,180
they are to now really use the sort of the dependance between the incremental count of s incremental

363
00:49:10,180 --> 00:49:16,480
R in estimation and this pretty much equivalent to what you do for the market analysis.

364
00:49:16,870 --> 00:49:20,920
Okay. Unless that debate are the gamma share the same parameter.

365
00:49:21,400 --> 00:49:32,740
For example, if you want to model better and the gamma using the same covariates, that could be a situation where, you know,

366
00:49:32,770 --> 00:49:44,200
you can borrow some strains of that because you have a common cognitive that all of this parameters, in this case beta and gamma are state alone.

367
00:49:44,200 --> 00:49:49,630
Parameters have nothing to do with other process than this.

368
00:49:50,290 --> 00:49:54,340
You know, independent assumption is equivalent to rounding separately.

369
00:49:55,780 --> 00:50:00,010
So. So what? What if you write this?

370
00:50:00,280 --> 00:50:03,700
What's the right way to calculate the standard errors?

371
00:50:03,970 --> 00:50:09,520
Okay. So that that's really a technical issue.

372
00:50:09,760 --> 00:50:15,460
Okay. You know that Delta s and Delta are not independent.

373
00:50:16,300 --> 00:50:24,200
But you force them to write in a way that is independent under your assumption of independence, which is not true.

374
00:50:24,220 --> 00:50:28,810
I mean, we know that nurse, cause no point estimations are fine.

375
00:50:28,870 --> 00:50:33,910
If you ignore several dependance or cross component dependance, you.

376
00:50:33,930 --> 00:50:37,060
You can audit dependance. You still can get a consistent estimate.

377
00:50:37,090 --> 00:50:40,420
I saw as your marginal models are correct as specified.

378
00:50:41,230 --> 00:50:46,540
But you have a dependance between the delta s and delta that dependance.

379
00:50:46,570 --> 00:50:50,290
Well, in fact, your calculations stand here. What is the right.

380
00:50:50,290 --> 00:50:58,599
And we do cargo standard error after you write this sort of in the generalized container you have taken.

381
00:50:58,600 --> 00:51:02,980
653 Right. There is something called that working independence option.

382
00:51:03,430 --> 00:51:10,800
So here is very much similar idea that you impose a working dependance assumption in the formulation

383
00:51:10,810 --> 00:51:16,719
of the likelihood that working independence assumption is not due to dependent structure,

384
00:51:16,720 --> 00:51:21,120
but you impose in the way of doing all MLG.

385
00:51:22,270 --> 00:51:31,749
So clear there's a you know of caveats, you know about this dependance.

386
00:51:31,750 --> 00:51:34,840
How do you convert that into this inference?

387
00:51:35,470 --> 00:51:45,910
And because of this working dependance assumption between Delta and Delta, are you the standard URLs are typically underestimated.

388
00:51:46,150 --> 00:51:52,150
So that's something we we need to really correct that finding way not our thing

389
00:51:52,150 --> 00:52:00,420
is that the whole thing is conditional on s t and i t as the way to right up.

390
00:52:00,430 --> 00:52:11,290
And so the the the directly plock in the city and into the model as something you observed.

391
00:52:11,710 --> 00:52:18,550
Because the way you calculate ofsome offset term, you just use the i.t whatever you know,

392
00:52:18,590 --> 00:52:24,210
its report in the cdc 17 states, then you plug in that number into the model.

393
00:52:25,020 --> 00:52:35,110
There's no potential like sampling error or some reporting error in this estimation that could fact it would result

394
00:52:35,110 --> 00:52:44,010
because you directly assume that s t and are measure accurately so that you can take a log of them as to offset time.

395
00:52:44,350 --> 00:52:48,790
So there are some of the caveats that when we use this system to do that.

396
00:52:51,970 --> 00:52:57,900
So so let me talk about a little bit the the inference in this context.

397
00:52:57,940 --> 00:53:02,440
And there are two types of inference theory considered.

398
00:53:03,610 --> 00:53:08,860
One is called infill of some party theory, and the other one is outrageous and part theory.

399
00:53:09,760 --> 00:53:21,880
Okay. So this is a very different sort of in a stochastic sort of field for inference for the Cassatt stochastic process data.

400
00:53:22,330 --> 00:53:29,030
There are some different types of statistical for theory, which is called if you are sometimes not alone called all,

401
00:53:29,060 --> 00:53:34,240
which are some parties that only just explain each of them.

402
00:53:34,630 --> 00:53:39,760
The infill of some products concerned a sampling point increase within a fixed time window.

403
00:53:41,560 --> 00:53:50,030
While the outreach of some parties pertains to practical relevance or where the time window data collected infinity.

404
00:53:50,170 --> 00:53:54,880
Okay. So this may be easier to understand the.

405
00:54:01,250 --> 00:54:08,480
So if you are some colleagues or some party is always about the limit in p, b, here we'll increase sample size.

406
00:54:08,900 --> 00:54:13,550
Okay. So you feel awesome tonics like i have zero to t.

407
00:54:14,900 --> 00:54:19,240
This is the time when the over which I'm going to claim data.

408
00:54:20,150 --> 00:54:24,440
So. So what what is the the way to increase data.

409
00:54:24,830 --> 00:54:28,220
Okay. So you have the answer and based on your theta.

410
00:54:31,030 --> 00:54:37,270
So in you have this you have the boundaries of the of your time interval.

411
00:54:37,780 --> 00:54:43,060
When you increase sample size, you're not going to get more time or whatever.

412
00:54:43,300 --> 00:54:48,820
You have a fixed time window. You have you feel you more and more sampling points into time.

413
00:54:51,370 --> 00:54:55,899
So you take that points are getting into in certain days the denser this is you feel some

414
00:54:55,900 --> 00:55:02,860
colleagues when you have more and more data points filled inside this fixed time interval,

415
00:55:03,420 --> 00:55:10,800
the dependance between adjacent data points are increasing all the time.

416
00:55:10,810 --> 00:55:14,770
So your data becomes more and more dependent when you.

417
00:55:14,770 --> 00:55:18,310
Jean-Christophe. So this is a very, very difficult problem.

418
00:55:18,400 --> 00:55:23,440
When you talk about some topics, so you feel sometimes people are I don't see this problem.

419
00:55:23,680 --> 00:55:29,020
Yes. You see in imaging data, the this size, our brain is fixed.

420
00:55:29,740 --> 00:55:34,610
You cannot enlarge my size up. You know, this, this.

421
00:55:34,630 --> 00:55:41,380
But yeah. So so that you get are taking data inside the brain imaging.

422
00:55:42,250 --> 00:55:45,850
Okay so a lot of time when you talk about brain injured data,

423
00:55:45,850 --> 00:55:52,030
you have you feel some time because this size of the brain fixed you have more dense our data,

424
00:55:52,030 --> 00:55:57,069
no data to get to higher higher resolution of your imaging data.

425
00:55:57,070 --> 00:56:02,860
You already have it. You feel sometimes we have more and more data point the feeling into the fixed space.

426
00:56:03,970 --> 00:56:07,360
You have higher and higher dependance among the data points.

427
00:56:08,050 --> 00:56:13,870
But that's very hard because all a lot of life numbers and you name it, they are based on independent data.

428
00:56:14,590 --> 00:56:25,000
So so it's so that's why all people go to Bayesian statistic in imaging data rather than frequentist approach, because you have no it's very,

429
00:56:25,020 --> 00:56:30,549
very hard to prove that some products using this existing theory of war I've lost number I

430
00:56:30,550 --> 00:56:35,750
sent in theory you have to really just two days in that get forget about some politics.

431
00:56:37,410 --> 00:56:44,319
So there is no way that so of course that you can imagine that in the actually, you know,

432
00:56:44,320 --> 00:56:52,720
the spatial statistic like so so so so maybe you can have outreach, for example, you add more and more counties.

433
00:56:53,440 --> 00:56:53,769
Okay?

434
00:56:53,770 --> 00:57:03,010
So now you start this Michigan data, then you can add more counties from Ohio, more counties from Tennessee, you are more counties from Pennsylvania.

435
00:57:03,250 --> 00:57:07,389
So you have all rich, right? So you have no bottom fixed boundary.

436
00:57:07,390 --> 00:57:12,790
So you adding this tracks your space or time in a sampling.

437
00:57:12,790 --> 00:57:16,810
So, so basically that you have daily data.

438
00:57:18,130 --> 00:57:26,170
Okay, so you stretch this to future time and in the spatial context you add a more and more counties or,

439
00:57:26,200 --> 00:57:30,390
you know, some spatial locations, your data, your outreach there.

440
00:57:30,400 --> 00:57:36,130
So this outreach, some politics, this is the one that we are able to work on.

441
00:57:36,140 --> 00:57:46,720
Asymptotic theory, right? Ascension theory and law lots number you feel it's very hard that because the the dependance.

442
00:57:47,000 --> 00:57:57,880
Okay okay so so a lot of time when people looking at infill are sometimes people switch to functional data analysis.

443
00:58:02,750 --> 00:58:10,530
Because you have things here. And then there are data that collected running and looking at the they at the

444
00:58:11,060 --> 00:58:17,450
you know the data individually they look at as a functional okay so that is

445
00:58:17,450 --> 00:58:23,390
a not really to overcome the problem that so people try to work on something

446
00:58:24,170 --> 00:58:30,850
slightly different to overcome some of the technical difficulties and so on.

447
00:58:31,040 --> 00:58:32,839
So I'll reach a large sample three or four.

448
00:58:32,840 --> 00:58:39,440
Emily with discrete time series provides the two symphonies to the most of their universities is application.

449
00:58:40,250 --> 00:58:46,820
So that's something we look at. And in the frequentist a method.

450
00:58:46,850 --> 00:58:51,890
Of course, if you do base in that you don't have this issue, you know everything based on posterior distribution,

451
00:58:52,310 --> 00:58:58,910
which is the finite sample of machinery that you can do an estimation or estimate,

452
00:58:59,210 --> 00:59:08,060
you have different octave function to optimize your on your, you know, your goal or objective of study.

453
00:59:09,950 --> 00:59:25,580
So what here you see here is that it's little bit the sort of the conditional composer like so essentially that you have a lot of ops to see.

454
00:59:26,810 --> 00:59:33,500
Essentially you have a lot of increments, right? So theta increments of S, delta s T and delta r t.

455
00:59:34,190 --> 00:59:39,530
The first simplification you take here is that you assume that those increments are independent.

456
00:59:41,300 --> 00:59:46,190
What would would that be really to the number of the, you know,

457
00:59:46,190 --> 00:59:54,350
increment count today is independent of increment count tomorrow in an infectious disease setting, maybe now two.

458
00:59:55,250 --> 00:59:59,690
So what happens to deal is independent of what's going to happen tomorrow.

459
01:00:00,200 --> 01:00:07,189
I mean, term is number of a number of, you know, additional effective cases.

460
01:00:07,190 --> 01:00:14,389
I mean, it's very hard assumptions that you believe is true, but you want to formulate likelihood.

461
01:00:14,390 --> 01:00:18,640
You assume this independent increment, which may not be true.

462
01:00:18,800 --> 01:00:23,330
Right. So but you assume this or that, you can write this and this likelihood.

463
01:00:25,250 --> 01:00:35,390
And also you assume that independence between Delta s T and Delta R t, which also may not be true when you do this kind of like in the first.

464
01:00:35,960 --> 01:00:47,600
Now people say that this is a simplification, a more complex model that involves a lot of different types of correlation.

465
01:00:48,140 --> 01:00:57,170
But here when you formulate like you who has a role here is much simpler, ignores most of the dependance here among all the data.

466
01:00:57,170 --> 01:00:59,840
So so if you take this point of view,

467
01:00:59,840 --> 01:01:08,180
you can really think that this is the point of view of compulsive likelihood or conditional compulsive likelihood.

468
01:01:10,190 --> 01:01:16,370
So so in the next few slides I will want I want to introduce what is composer like here.

469
01:01:16,730 --> 01:01:23,180
And, and so the standard theory compulsive like to estimate implies that some particle matrix of estimate is

470
01:01:23,180 --> 01:01:30,670
given by inverse full damage information matrix or the sandwich estimate or you have seen this in the T,

471
01:01:30,720 --> 01:01:39,530
right? 653 You know, that can be estimate by bootstrap and to get the values that inference.

472
01:01:39,590 --> 01:01:50,510
Okay. So, so basically the question here is how do we understand the legitimacy or of of ignoring dependance?

473
01:01:51,260 --> 01:01:58,880
Okay. So that's essentially what happens in the previous slides to where we formulate likelihood by

474
01:01:59,450 --> 01:02:06,860
putting a lot of assumptions that essentially try to avoid modeling for you considering dependance.

475
01:02:06,980 --> 01:02:13,860
Okay. So let me just introduce the the concept, a compulsive likelihood.

476
01:02:14,570 --> 01:02:24,080
I work quite a bit on this because there are a lot of complex dependance if we need to deal with sometimes we are just lazy,

477
01:02:24,080 --> 01:02:29,210
we don't want to deal with that. So and we want to do something simpler.

478
01:02:29,420 --> 01:02:36,080
Okay, that then really deal with the very complex dependance structure.

479
01:02:36,650 --> 01:02:40,370
Can we just figure out what are the basic dependance structure?

480
01:02:40,370 --> 01:02:49,700
We need to count and sacrifice some of the complex high order dependance that we don't know when we we do not have data to figure it out.

481
01:02:50,630 --> 01:02:54,170
So basically, this is Bruce Lindsey.

482
01:02:55,310 --> 01:02:58,670
He he's trying to figure out and some other people like.

483
01:02:59,550 --> 01:03:06,920
So trying to work in this field and Jen Kong and I had a paper and we used this method to work on some of the spatial data.

484
01:03:08,240 --> 01:03:11,650
But anyway, so I work quite a bit on the composer like you.

485
01:03:11,660 --> 01:03:19,040
I just wanna give you a brief introduction that may help you to give you another toolbox to

486
01:03:19,040 --> 01:03:26,940
do this complex sort of process of complex data analysis of this complex dependent structure.

487
01:03:27,720 --> 01:03:37,160
Okay. So this is working on proposed by Bruce Lindsey in a mathematical journal, I believe, and it's not in such a journal.

488
01:03:37,160 --> 01:03:47,030
And later on summarized by a review paper by Cristiano of varying and diverse and nice to read 2011,

489
01:03:47,030 --> 01:03:56,390
which is a very excited paper and so composer like is really something powerful for us to, you know, simplify our problem.

490
01:03:56,620 --> 01:04:03,550
Okay. So essentially compose like there is a pseudo likelihood method for dimension reduction in word to handle high dimension like it.

491
01:04:03,950 --> 01:04:10,670
If you think about the complex dependance, then your end of with a very complex likelihood.

492
01:04:10,820 --> 01:04:17,780
What you're trying to do here is like you want to really do a simple of simplification, okay?

493
01:04:18,290 --> 01:04:23,300
By, you know, throwing out a lot of complex high level structure.

494
01:04:23,810 --> 01:04:28,549
So composed likelihood helps you to do that, to achieve this dimension.

495
01:04:28,550 --> 01:04:39,620
Reduction in likelihood. Mm hmm. So your general rigor being better than method moments, and it relies on some low dimension.

496
01:04:39,620 --> 01:04:49,040
Like who? Objective with the a well-defined object function for estimation, goodness, feat, assessment and model selection.

497
01:04:49,790 --> 01:05:00,580
Okay, so, so most important appealing feature of composer likelihood is it allows us to overcome computational burden.

498
01:05:00,800 --> 01:05:08,360
That is the problem is a complex like it and the complex likelihood will be result from a complex dependent

499
01:05:08,360 --> 01:05:15,380
structure like the data we have thus is appearing to deal with spatially and temporally copy of data.

500
01:05:15,870 --> 01:05:22,640
Lot of people or common composer like in this view, particularly in this spatial data analysis.

501
01:05:23,140 --> 01:05:27,980
Hmm. Okay. Let let me just introduce this very briefly.

502
01:05:29,390 --> 01:05:35,300
So we begin with a high dimension of parametric model that involves substantial computation burdens.

503
01:05:36,890 --> 01:05:41,570
So how do you deal with that? So suppose I have a parametric model.

504
01:05:42,590 --> 01:05:46,850
I, I have a p dimensional perpetual model.

505
01:05:47,300 --> 01:05:49,280
P is very high dimensional.

506
01:05:49,750 --> 01:05:59,090
This likelihood is very complex and I have a parameter of interest that I like to use my data to make estimation and inference.

507
01:06:00,020 --> 01:06:04,909
Suppose that this joint likelihood is very hard to compute because if the

508
01:06:04,910 --> 01:06:10,880
methodology and some of the structures are really hard to estimate in practice.

509
01:06:12,800 --> 01:06:20,640
So Bruce Lindesay said, Why do you want to work on this all p dimensional likelihood directly?

510
01:06:20,660 --> 01:06:25,430
You can consider something simpler. So what he propose here is right.

511
01:06:26,000 --> 01:06:29,600
Let's consider all pairwise favored margins.

512
01:06:30,740 --> 01:06:38,780
Okay. So. So, of course, I if I'm able to look at all pairwise by varied likelihood.

513
01:06:39,500 --> 01:06:46,040
So now you only have a two dimensional sort of likelihood function, but you have many, many, many.

514
01:06:46,140 --> 01:06:52,120
Right. Right. So. But. But each piece is only two dimensional rather than p dimension.

515
01:06:52,130 --> 01:06:57,740
You cannot compute t p dimension of the joint likelihood p dimension to make it.

516
01:06:58,040 --> 01:07:02,330
But if you are able to compute the is two dimensional thing, right?

517
01:07:03,050 --> 01:07:10,670
So of course that we do this dimension reduction, then the parameters that you have with the bivariate margins,

518
01:07:11,120 --> 01:07:15,050
not the same set of parameters from the marginals.

519
01:07:15,490 --> 01:07:15,750
Okay.

520
01:07:15,770 --> 01:07:28,780
So there is some kind of attrition of your parameters in this dimension reduction, but you captured the most important parameters in this dimension.

521
01:07:28,790 --> 01:07:32,300
So the reduction from P dimension to two dimension like.

522
01:07:33,500 --> 01:07:40,190
So you you had the mean because the marginal distribution will be involved.

523
01:07:40,520 --> 01:07:43,920
Okay. You also capture all the pairwise dependance.

524
01:07:44,900 --> 01:07:52,160
So when you have theta here in the p dimension, likelihood this data will evolve out of higher order dependent structure.

525
01:07:54,230 --> 01:07:59,210
But when you do this reduction to only consider pairwise two dimension likelihood.

526
01:07:59,590 --> 01:08:05,230
You will sacrifice some, you know, high order structures,

527
01:08:05,410 --> 01:08:12,010
only able to capture the basic first human like this the most important parameters

528
01:08:12,010 --> 01:08:18,160
that that is the mean parameters and also the pairwise dependance parameter.

529
01:08:18,940 --> 01:08:28,000
This pairwise by word margins will not for you to get to this higher order dependent structure.

530
01:08:28,570 --> 01:08:39,420
So then after a good all the margins, then by very margins you you just make this product okay weighted by pull the weight okay

531
01:08:39,490 --> 01:08:46,510
typical IT people put to the weight as one but you put them together and this formulate

532
01:08:46,510 --> 01:09:00,010
a composite like okay so so then you can maximize this new pseudo likelihood with respect

533
01:09:00,010 --> 01:09:08,140
to beta and this beta hat is called compulsive compulsive as the measure of your beta.

534
01:09:08,800 --> 01:09:14,500
Okay so that's basically idea right around working on P the mission like you, you're working on many, many,

535
01:09:14,500 --> 01:09:22,750
many to the measure of IQ and to form the as the way of likelihood so that you can estimate some of program.

536
01:09:24,310 --> 01:09:35,230
So here, here is example. Okay, so if I have a three dimensional normal, okay, so I have, you know, mean parameter is three dimensional.

537
01:09:35,230 --> 01:09:38,410
A sigma covariance matrix is three by three matrix.

538
01:09:38,410 --> 01:09:49,660
Right. So that I, you know, instead of a working on three dimensional normal distribution, I can only work three by very margins.

539
01:09:49,660 --> 01:09:55,240
So for I have three such situations one, two, one, three and two, three.

540
01:09:55,330 --> 01:10:03,129
Right. So those are these three possible bivariate distributions and you can see that from one to margin,

541
01:10:03,130 --> 01:10:05,770
those you don't have parameter or sigma three, three, right.

542
01:10:06,160 --> 01:10:14,710
You don't have parameter related to Y three because you have marginal distribution of one two rather than joint of 1 to 3.

543
01:10:15,490 --> 01:10:19,870
But you can say that they have, you know, the prioritization here.

544
01:10:20,230 --> 01:10:27,430
For example, mule one, this is type two, okay?

545
01:10:27,430 --> 01:10:37,059
This is type of okay. So here should be once read the here is mule one meal three and one one, one, three, three, three and one, three.

546
01:10:37,060 --> 01:10:40,780
Okay, this is table. Okay, let me repeat this.

547
01:10:40,780 --> 01:10:46,780
This would be beta. One, two mule, one, mules, three, one, one, three, three and one, three.

548
01:10:46,960 --> 01:10:54,160
Okay, so here that you have the, the marginal distribution, the bivariate margins,

549
01:10:54,160 --> 01:11:03,220
then you just put them multiply this three by very, distribute likelihood together and then you maximize the parameters.

550
01:11:03,580 --> 01:11:08,920
In this case, you have all the parameter involved here. You do not have any attrition in parameter.

551
01:11:09,430 --> 01:11:12,729
Okay? So there are overlap parameters and three margins.

552
01:11:12,730 --> 01:11:17,920
So they need to be jointly estimate from all the parameter.

553
01:11:19,920 --> 01:11:26,550
So in general now all the parameter will pile up here in the boundary margins.

554
01:11:26,730 --> 01:11:30,840
In principle, the same procedure may be applied to one.

555
01:11:31,040 --> 01:11:35,920
WEISS Like you say, why I need to do the sort of pairwise.

556
01:11:35,940 --> 01:11:39,190
Can I just use one marginal. You can do that. Okay.

557
01:11:39,330 --> 01:11:43,319
You can use a table twice the size you use.

558
01:11:43,320 --> 01:11:46,530
Three dimensional, like. It depends on the.

559
01:11:46,650 --> 01:11:55,470
The problem is problem dependent to decide what the likelihood margin distribution dimension of lower dimension might be that you want to work on.

560
01:11:56,100 --> 01:12:00,270
So pairwise is is the most popular one.

561
01:12:00,870 --> 01:12:11,240
Okay. So and people establish all this estimation and some party theory for for this compulsive likelihood.

562
01:12:11,250 --> 01:12:21,120
So. Okay. So just give you a quick example about this high order structure, because in a normal case, you don't see what is high order structure.

563
01:12:21,330 --> 01:12:25,050
What what is the effect of treating when we do this projection?

564
01:12:25,080 --> 01:12:30,690
Okay. So in statistics, there is a very famous multivariate by normal distribution.

565
01:12:31,930 --> 01:12:39,000
Ah, we call this bar outdoor representation. This is essentially a p dimension of binomial pronounced distribution.

566
01:12:39,180 --> 01:12:46,500
Okay. And this distribution made a profound impact in statistics and in machine learning.

567
01:12:46,530 --> 01:12:53,040
Okay. I think I like ice model by nature of data analysis, people humanness distribution as well.

568
01:12:53,200 --> 01:12:59,520
Let me just explain to you. So you so you have C one dimensional Bernoulli distribution, right?

569
01:13:00,030 --> 01:13:04,469
X is four. No, the distribution is two point distribution.

570
01:13:04,470 --> 01:13:11,970
But if you have p a vector, a p elements, each one takes zero one value and they're calling it.

571
01:13:12,540 --> 01:13:17,180
Then you have a p dimensional Bernoulli distribution. What is the distribution look like?

572
01:13:17,190 --> 01:13:21,540
And Baldur gave you a specification like this.

573
01:13:21,830 --> 01:13:25,560
Okay, this is the distribution. Baldur it representation. Very famous.

574
01:13:27,150 --> 01:13:39,720
So here you can see you have a parameter theta j is the marginal parameter that's related to your marginal meaning of your single element.

575
01:13:40,230 --> 01:13:51,570
Then you have the C that you want you to. This is the parameter related to pairwise dependance between a pair of the binary random variables.

576
01:13:52,530 --> 01:13:59,150
It's not finished yet. You can have next term. That will be a parameter C that you want you to choose.

577
01:13:59,160 --> 01:14:08,790
Three that will be used to describe the three way dependance a next term that you have not a parameter.

578
01:14:08,790 --> 01:14:13,080
What do y describe the forward correlation?

579
01:14:13,380 --> 01:14:16,530
And now you have another parameter theater one to p.

580
01:14:16,530 --> 01:14:24,270
This is a single parameter that describes the dependance of all p p variables.

581
01:14:24,630 --> 01:14:31,380
So you can see that there is a structure after this in data that the this describes the

582
01:14:31,470 --> 01:14:36,630
high order structure three way fully up to peewee dependance structure in the moment.

583
01:14:36,990 --> 01:14:39,600
Okay. This is more complex and normal distribution.

584
01:14:41,010 --> 01:14:48,270
So of course that you know that there are a lot of parameters to P minus one parameter and c c that is normalized

585
01:14:48,270 --> 01:14:56,070
constant in order to make this representation be a density function for P division or burden distribution or binary.

586
01:14:57,300 --> 01:15:00,960
And then there are two P cell probabilities, of course.

587
01:15:01,020 --> 01:15:04,620
Okay. So first there is a science paper.

588
01:15:05,010 --> 01:15:08,130
Many years ago, foresman machine. Okay.

589
01:15:08,160 --> 01:15:16,560
So Boseman machine is very famous of machine in the machine learning field in constructing a neural network.

590
01:15:16,920 --> 01:15:21,059
Because zero one is essentially determines connectivity.

591
01:15:21,060 --> 01:15:25,320
Right. You have many notes, but p you can think about that.

592
01:15:25,320 --> 01:15:29,430
P is p nodes or P individuals in the population.

593
01:15:30,300 --> 01:15:37,830
So the of the the say that you want you to would describe the dependance between two nodes.

594
01:15:38,430 --> 01:15:43,860
Okay. If say that you want you to is zero, that means there's no dependance between two nodes.

595
01:15:44,310 --> 01:15:46,980
So basically this model is called Boltzmann machine.

596
01:15:46,980 --> 01:15:58,260
That's the first fundamental machine in the theory of neural network where people describe the the connectivity in a stochastic model.

597
01:15:58,650 --> 01:16:11,130
Okay. So, so what Boltzmann machine attached is really they just put everything after this term they the second two way dependance zero.

598
01:16:11,580 --> 01:16:17,940
So they truncated this distribution part where said that you need the high order structure but.

599
01:16:18,250 --> 01:16:22,479
My machine basically truncated all the higher terms.

600
01:16:22,480 --> 01:16:27,010
Three we four we are up to the p wave that put them at zero.

601
01:16:27,340 --> 01:16:30,010
Okay. What you need to do here is of course,

602
01:16:30,010 --> 01:16:37,870
you need to readjust or normalize in constant your to make it this to be a stochastic model is quite stochastic that neural network.

603
01:16:38,620 --> 01:16:47,140
So this is it's a science paper many years ago of of type Boseman is very famous Boseman mission.

604
01:16:48,190 --> 01:16:56,170
Okay so later on people do this, including like our colleagues from Stanford, the prime of the call ice model.

605
01:16:56,290 --> 01:17:05,199
Okay this is when you truncate this of through the for re up to play dependance okay then

606
01:17:05,200 --> 01:17:11,200
you have only the the marginal and paralyzed structure in this balloon the distribution.

607
01:17:12,010 --> 01:17:20,319
This is also used in the graphical multicolored icing model in the longitudinal data analysis of Ross

608
01:17:20,320 --> 01:17:30,280
Prentice put this also the truncation in the modeling of binary longitudinal data dependance data.

609
01:17:30,550 --> 01:17:41,410
Okay, so all this famous work star is the body representation and got this right chunk this put all the Hollywood term to be zero.

610
01:17:42,040 --> 01:17:50,560
So if you use the point of view of the compulsive likelihood, what they're trying to do here is just doing the projection.

611
01:17:53,780 --> 01:18:02,900
Whether they are trying to do here is nothing special is trying to remove the higher order dependent structure and only look at pairwise dependance.

612
01:18:03,320 --> 01:18:09,860
Okay. So essentially what they're having here by removing all the horror term is really

613
01:18:09,860 --> 01:18:14,810
tourneys for p dimension likelihood in a two dimensional composer likelihood.

614
01:18:15,050 --> 01:18:17,300
Okay. That's essentially what they're doing.

615
01:18:17,750 --> 01:18:28,850
But of course, that they do not realize that this strategy they use is really creating a composer like a could and they are studying their own theory,

616
01:18:29,030 --> 01:18:39,620
everything there. But the theory actually, if you look at all the papers together, they share the same foundation of statistical estimation inference,

617
01:18:40,070 --> 01:18:44,720
which is essentially the theory of inference and compulsive activity.

618
01:18:45,830 --> 01:18:49,399
So they become the inference estimation composer.

619
01:18:49,400 --> 01:18:57,650
Like it's very, very straightforward. You just take a log of this compulsive likelihood and then, you know, you're just trying to maximize this.

620
01:18:57,680 --> 01:19:05,150
You take this score of this and you get this respect, parameter of interest, and you get to the compulsive score.

621
01:19:05,630 --> 01:19:12,290
Then you, you know, you find the solution from the score equation.

622
01:19:12,290 --> 01:19:15,380
And typically the it will be selected as one.

623
01:19:15,920 --> 01:19:21,530
And in some spatial data analysis is spatial spatial data analysis.

624
01:19:21,530 --> 01:19:27,040
Then people to put this we'd ask some distance between the two locations and so on.

625
01:19:27,050 --> 01:19:30,800
So there are different ways how people choose to wind up.

626
01:19:31,010 --> 01:19:37,309
But fundamentally, this is the compulsive like in method and of both.

627
01:19:37,310 --> 01:19:43,550
Here's goal I puppy as a paper and jossa to how to do by sea.

628
01:19:44,090 --> 01:19:52,219
So how to how to do a see in the context of the compulsive I called that paper was oh okay sided

629
01:19:52,220 --> 01:20:01,790
reasonable sided but so so you have the model selection and and how to use the kill distance couple

630
01:20:02,090 --> 01:20:10,129
looper distance to define a you know sort of the model difference and to do model section particularly

631
01:20:10,130 --> 01:20:17,930
for the I say prove them selection consistency of under in the context of compulsive likelihood.

632
01:20:19,330 --> 01:20:22,090
Okay. So let me finish out this couple of remarks.

633
01:20:22,100 --> 01:20:31,910
And first of all, I mentioned that that the optimal weight you you can work it out, but it involves high order dimension distributions.

634
01:20:33,740 --> 01:20:38,360
So so this is not very good choice. In the original paper by Bruce Lindsey.

635
01:20:38,930 --> 01:20:43,520
He said that if you want to work out the optimal weight, you need no force water.

636
01:20:43,910 --> 01:20:54,440
So which certainly is not the idea is contradicting to the original idea because the original idea is you want to avoid high water structure, right?

637
01:20:54,770 --> 01:20:59,610
You only need to want the low dimensional structure to be used in the estimation.

638
01:21:00,170 --> 01:21:07,700
If you go back to force of Mormon structure, then you sort of lose your original motivation why you want to do this?

639
01:21:07,700 --> 01:21:12,649
Likelihood protection. So in practice, people try to make it.

640
01:21:12,650 --> 01:21:18,530
It's simple solution of choice, either one or zero based on the practical consideration.

641
01:21:18,530 --> 01:21:22,310
What using the nearest neighbor who in the spatial data models.

642
01:21:22,460 --> 01:21:30,320
So there are all sorts of ad hoc ways of dealing with that that's not very is is

643
01:21:30,350 --> 01:21:34,850
important but now the you know really important thing and also now we can work

644
01:21:34,850 --> 01:21:41,959
out all are some topics there and and some people in the spatial data analysis

645
01:21:41,960 --> 01:21:50,420
are a field trying to work on this using nested bootstrap method and so on.

646
01:21:50,420 --> 01:21:56,800
So there are a lot of quite a nice way to do inference in the compulsive active contexts.

647
01:21:59,150 --> 01:22:06,530
What's his name? Besides I think is D, g c?

648
01:22:06,650 --> 01:22:10,760
Last I met him one time in University of Bath, UK.

649
01:22:11,420 --> 01:22:16,580
He was a professor at the University of Washington, Seattle.

650
01:22:17,240 --> 01:22:26,809
And he I, he got this sort of in the stage renal disease he needed to and and then he moved back to you

651
01:22:26,810 --> 01:22:35,150
can join universe a bath or later on after it so they he got kidney transplant in the UK.

652
01:22:36,380 --> 01:22:44,209
I met him oh many years ago and he works on this a lot of this very influential his idea about

653
01:22:44,210 --> 01:22:52,020
using computer like spatial to houses and Michael stem from Chicago I think you know is in Duke.

654
01:22:52,690 --> 01:22:56,020
And he's also talking about working on this compulsive likelihood.

655
01:22:57,250 --> 01:22:59,500
So hypothesis testing is very straightforward.

656
01:23:00,700 --> 01:23:10,000
And one thing I should mention that is, of course, that when you do this to measure reduction, you lose the efficiency to some extent.

657
01:23:10,570 --> 01:23:20,670
But all the numerical result I've seen so far surprising means such efficiency loss is found to be marginal in many private cases.

658
01:23:20,680 --> 01:23:28,959
And that's read from University of Toronto. While her students did a lot of simulation on an all different many,

659
01:23:28,960 --> 01:23:37,570
many scenarios and found that the loss efficiency loss was very minimal and it wasn't quite surprising that.

660
01:23:38,290 --> 01:23:47,769
So we had a one conference in Warwick University of Warwick UK to discuss all the

661
01:23:47,770 --> 01:23:52,470
controls I get because David first was there as a professor who organized the workshop,

662
01:23:52,480 --> 01:23:56,260
and many are, so we just come together.

663
01:23:56,260 --> 01:24:02,649
So here, in a way, we share the same similar finding that the efficiency loss is very minimal.

664
01:24:02,650 --> 01:24:06,800
Even you do the prediction from high dimension to low dimension.

665
01:24:06,830 --> 01:24:12,450
And and it's quite a, you know, mysterious property of why that's true.

666
01:24:13,840 --> 01:24:21,960
So, of course, the most important game here is computational physics, because you're no longer trying to work on a hibernation.

667
01:24:21,970 --> 01:24:27,190
Or like you, you don't need to take the inverse of puberty matrix or covariance matrix.

668
01:24:27,190 --> 01:24:30,520
We only take the inverse of two way two by two matrix.

669
01:24:30,850 --> 01:24:40,270
So this even have closed from the expression for the calculation of inverse matrix over because also the computationally is very simple and fast.

670
01:24:40,360 --> 01:24:44,170
So people are trying to use composer like you in many fields.

671
01:24:44,620 --> 01:24:50,890
I think that what I just presented about this sort of the data,

672
01:24:51,430 --> 01:24:55,410
the likelihood form is really the composer like but I don't think it is like you

673
01:24:55,450 --> 01:25:00,520
good because they did this big assumptions that the really is just trying to

674
01:25:00,970 --> 01:25:07,360
predict the high dimensional complex process into a very low dimensional case to

675
01:25:07,390 --> 01:25:14,379
formulate the binary anyway so nobody really seriously invested but but anyway,

676
01:25:14,380 --> 01:25:20,310
so that's very important point of view to understand that my people think that's how I want to see today.

677
01:25:20,320 --> 01:25:20,770
Thank you.

