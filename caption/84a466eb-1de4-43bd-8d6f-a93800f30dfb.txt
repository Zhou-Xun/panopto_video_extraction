1
00:00:05,480 --> 00:00:09,646
Hello, this lecture will cover measures
of association and regression,

2
00:00:09,646 --> 00:00:16,541
specifically estimation of something known
as Pearson's correlation coefficient.

3
00:00:16,541 --> 00:00:19,426
Now, this topic will focus
on quantifying whether or

4
00:00:19,426 --> 00:00:23,560
not two continuous values
are related to each other.

5
00:00:23,560 --> 00:00:27,290
We can visually assess the association
between two continuous measures

6
00:00:27,290 --> 00:00:29,470
with a scatter plot.

7
00:00:29,470 --> 00:00:33,490
A positive association between these two
values would indicate that both variables

8
00:00:33,490 --> 00:00:35,870
change in the same direction.

9
00:00:35,870 --> 00:00:38,910
While a negative association
would mean that both variables

10
00:00:38,910 --> 00:00:40,930
are changing in opposite directions.

11
00:00:40,930 --> 00:00:44,200
One would get large,
when one would get small.

12
00:00:44,200 --> 00:00:48,420
Here we have an example in which each dot
in the scatter plot represents one U.S.

13
00:00:48,420 --> 00:00:56,120
state and its corresponding rate of gun
ownership and firearm suicide rate in men.

14
00:00:56,120 --> 00:00:58,750
We can talk about one state here.

15
00:00:58,750 --> 00:01:01,550
Again, the scatter plot
tells me the values

16
00:01:01,550 --> 00:01:06,650
of firearm suicides is
about 15 on the y axis.

17
00:01:06,650 --> 00:01:09,459
And the rate of gun ownership is 61%.

18
00:01:09,459 --> 00:01:15,674
And likewise, another state has
25 firearms suicides per 100,000.

19
00:01:15,674 --> 00:01:19,766
And the rate of gun ownership of 39%.

20
00:01:19,766 --> 00:01:23,610
A corresponding interpretation occurs for
all 50 states.

21
00:01:23,610 --> 00:01:28,020
A visual examination of the dots seems to
suggest that states with higher levels of

22
00:01:28,020 --> 00:01:34,060
gun ownership also have higher levels
of firearms suicide rates in men.

23
00:01:34,060 --> 00:01:38,140
How might we quantify
this pattern numerically?

24
00:01:38,140 --> 00:01:40,900
In order to do so,
we first need some notation.

25
00:01:40,900 --> 00:01:45,580
We let x sub i denote the rate of
gun ownership in state i, so again,

26
00:01:45,580 --> 00:01:47,180
i will go from 1 to 50.

27
00:01:47,180 --> 00:01:53,580
Y sub i will denote the rate of
male firearm suicide in that state.

28
00:01:53,580 --> 00:01:57,340
We then take the average of all
50 x values and compute X bar,

29
00:01:57,340 --> 00:02:00,710
the sample mean of gun
ownership in all of the states.

30
00:02:00,710 --> 00:02:04,730
And correspondingly, we take all 50
y values, take their sample mean, and

31
00:02:04,730 --> 00:02:10,530
this is the sample mean for firearms
suicide rate in all of the states.

32
00:02:10,530 --> 00:02:18,040
For each state now, we can compute
the deviations, xi-xbar, and yi-ybar.

33
00:02:18,040 --> 00:02:22,510
Again, how far is each state
from the middle of the data?

34
00:02:22,510 --> 00:02:26,410
And how far is their Y value
from the middle of the data?

35
00:02:26,410 --> 00:02:30,670
If Xi and Yi,
if the rate of gun ownership and

36
00:02:30,670 --> 00:02:36,450
the rate of male firearm suicide move
away from means in the same direction,

37
00:02:36,450 --> 00:02:41,430
then both differences must be negative or
both differences must be positive.

38
00:02:41,430 --> 00:02:44,990
They both are above the mean or
they both are below the mean and

39
00:02:44,990 --> 00:02:49,020
therefore their product
will always be positive.

40
00:02:49,020 --> 00:02:51,980
Correspondingly, we might
have a state in which Xi and

41
00:02:51,980 --> 00:02:56,220
Yi move away from the means
in opposite directions.

42
00:02:56,220 --> 00:03:00,280
If this is the case,
then one difference must be positive and

43
00:03:00,280 --> 00:03:02,150
one difference must be negative.

44
00:03:02,150 --> 00:03:07,810
If x is above the sample mean then y must
be low the sample mean or vice versa.

45
00:03:07,810 --> 00:03:11,620
Their product therefore
will always be negative.

46
00:03:11,620 --> 00:03:15,310
So for all 50 states, we will have
a number that is either positive or

47
00:03:15,310 --> 00:03:17,030
negative.

48
00:03:17,030 --> 00:03:20,600
We can then add up all these numbers and
take an average.

49
00:03:20,600 --> 00:03:25,663
And again we do not divide by n to compute
the area which we divided by n minus 1.

50
00:03:25,663 --> 00:03:30,750
This quantity is known as
the sample co-variance of x and y.

51
00:03:30,750 --> 00:03:35,443
This number by itself is meaningless
until we compare how x and

52
00:03:35,443 --> 00:03:40,145
y move together relative to
the noise in x and y individually.

53
00:03:40,145 --> 00:03:45,470
Therefore, we scale the sample co-variance
by each of the sample standard deviations.

54
00:03:45,470 --> 00:03:48,080
This number is given the letter r and

55
00:03:48,080 --> 00:03:51,250
is known as Pearson's
correlation coefficient.

56
00:03:51,250 --> 00:03:55,013
This number is bounded by -1 and 1.

57
00:03:55,013 --> 00:03:56,875
Here are some visual examples of data and

58
00:03:56,875 --> 00:04:00,078
the corresponding values of
the Pearson correlation coefficient.

59
00:04:00,078 --> 00:04:03,900
In the upper-left hand corner
we have an example of some data

60
00:04:03,900 --> 00:04:06,550
that have strong positive association.

61
00:04:06,550 --> 00:04:09,800
Large values of x correspond
to large values of y, and

62
00:04:09,800 --> 00:04:13,640
small values of x correspond
to small values of y.

63
00:04:13,640 --> 00:04:17,421
The actual correlation
coefficient is 0.78, and

64
00:04:17,421 --> 00:04:23,770
0.78 is summarized by the clustering
of the points around their pattern.

65
00:04:23,770 --> 00:04:28,530
Likewise, in the row in the middle we
have a correlation coefficient of -0.7,

66
00:04:28,530 --> 00:04:33,730
showing a strong negative pattern
where large x values have small y and

67
00:04:33,730 --> 00:04:36,340
large y have small x values.

68
00:04:36,340 --> 00:04:40,870
Again the magnitude of the correlation
is visually shown by how clustered

69
00:04:40,870 --> 00:04:44,640
the points are to each other.

70
00:04:44,640 --> 00:04:50,189
In the top row on the right, we have an
example of a Pearson's correlation of 0.4.

71
00:04:50,189 --> 00:04:55,146
Here again visually we see somewhat of
a positive association where larger x

72
00:04:55,146 --> 00:04:57,600
values have larger y values.

73
00:04:57,600 --> 00:04:58,720
However, in this plot,

74
00:04:58,720 --> 00:05:03,960
the points are more scattered than they
are on the first plot in the top row.

75
00:05:03,960 --> 00:05:07,310
And therefore the magnitude
of the correlation is less.

76
00:05:07,310 --> 00:05:11,430
In the bottom row, we see a corresponding
correlation coefficient of -0.4

77
00:05:11,430 --> 00:05:15,930
in which we see a pattern in which
large x have small y and vice versa.

78
00:05:15,930 --> 00:05:21,016
And there is more scatter in the dots than
there was when the correlation was -0.7.

79
00:05:21,016 --> 00:05:24,660
In the bottom row in the middle,
we see correlation

80
00:05:24,660 --> 00:05:29,530
of 0.18 We see now that the points are
scattered more than they were before and

81
00:05:29,530 --> 00:05:35,110
it is hard for us to visually discern if
there is a pattern that goes up or down.

82
00:05:35,110 --> 00:05:38,180
And the final plot in the bottom
right-hand corner is an example in which

83
00:05:38,180 --> 00:05:42,067
we have X an Y values that essentially
have no correlation with each other.

84
00:05:42,067 --> 00:05:44,910
Our eyes cannot see any
pattern in the data.

85
00:05:44,910 --> 00:05:50,000
And it's a simply random scatter of
points spread very part from each other.

86
00:05:50,000 --> 00:05:52,380
There are properties of
Pearson's Correlation Coefficient that

87
00:05:52,380 --> 00:05:54,080
are important to know.

88
00:05:54,080 --> 00:05:57,390
First, the correlation
coefficient is an estimate.

89
00:05:57,390 --> 00:06:02,630
It is an estimate of row, this is the
population correlation of X and Y, that we

90
00:06:02,630 --> 00:06:08,190
cannot observe, but we estimate with our
Pearson's correlation and coefficient R.

91
00:06:08,190 --> 00:06:11,060
Pearson's correlation
coefficient is unit-less, so

92
00:06:11,060 --> 00:06:15,360
that it is not affected by
changes of location or scale.

93
00:06:15,360 --> 00:06:19,530
I can talk about correlation in
centimeters, in meters, in inches,

94
00:06:19,530 --> 00:06:20,630
in yards.

95
00:06:20,630 --> 00:06:21,850
The unit is irrelevant.

96
00:06:21,850 --> 00:06:24,910
The correlation would remain the same.

97
00:06:24,910 --> 00:06:29,200
If X and Y are truly independent
of each other, then the population

98
00:06:29,200 --> 00:06:33,590
correlation coefficient row is exactly
zero, and your data would suggest

99
00:06:33,590 --> 00:06:38,570
that with a Pearson's Correlation
that would also be close to zero.

100
00:06:38,570 --> 00:06:43,280
If my Pearson's correlation coefficient is
exactly 1 or -1, which are the limits of

101
00:06:43,280 --> 00:06:47,070
the values for the Pearson's correlation
coefficient, then a plot of x and

102
00:06:47,070 --> 00:06:50,520
y will produce points that are perfectly
fitting on a straight line.

103
00:06:50,520 --> 00:06:54,960
There will be no scatter or
spread in the points.

104
00:06:54,960 --> 00:07:00,400
And Pearson's correlation coefficient,
r, and the population coefficient

105
00:07:00,400 --> 00:07:05,852
row are measures of linear association or
how well x and y fit on a straight line.

106
00:07:05,852 --> 00:07:09,305
It is important to realize that
correlation quantifies the degree to which

107
00:07:09,305 --> 00:07:11,580
x and y fit in a straight line.

108
00:07:11,580 --> 00:07:14,740
Therefore, it is necessary to
always make a scatter plot of x and

109
00:07:14,740 --> 00:07:18,600
y before you compute a Pearson's
correlation coefficient.

110
00:07:18,600 --> 00:07:22,360
And we will see in another lecture that
x and y can actually have an estimated

111
00:07:22,360 --> 00:07:27,710
correlation very close to zero, yet
be very strongly related to each other.

112
00:07:27,710 --> 00:07:30,926
For our example,
we find that rates of gun ownership and

113
00:07:30,926 --> 00:07:34,966
rates of firearm suicides in men
Have a correlation coefficient of r,

114
00:07:34,966 --> 00:07:38,129
Pearson's Correlation Coefficient,
of 0.84.

115
00:07:38,129 --> 00:07:42,818
It therefore states with gun ownership
rates that are above the US average rate

116
00:07:42,818 --> 00:07:47,410
tend also to have higher rates of
firearm related suicide in men.

117
00:07:47,410 --> 00:07:50,940
So now that we have an estimate of
the population correlation coefficient,

118
00:07:50,940 --> 00:07:55,080
we want to know if our evidence is large
enough to suggest that we know that rates

119
00:07:55,080 --> 00:07:56,090
of gun ownership and

120
00:07:56,090 --> 00:08:01,470
rates of male suicides using firearms
are truly associated in the population.

121
00:08:01,470 --> 00:08:04,600
And to do this we need a hypothesis test.

122
00:08:04,600 --> 00:08:06,600
For a hypothesis test of correlation,

123
00:08:06,600 --> 00:08:11,300
the null hypothesis is that there is
no correlation rows equal to zero.

124
00:08:11,300 --> 00:08:14,643
And the alternative is that
there is correlation or

125
00:08:14,643 --> 00:08:16,596
that row is not equal to zero.

126
00:08:16,596 --> 00:08:21,000
Again, the observed magnitude of
the correlation is our signal.

127
00:08:21,000 --> 00:08:24,390
We are not interested in whether the
correlation is negative or positive, so

128
00:08:24,390 --> 00:08:29,960
we take the absolute value,
denoted by the vertical bars around r.

129
00:08:29,960 --> 00:08:34,350
The noise is quantified by this
formula presented on my slide.

130
00:08:34,350 --> 00:08:37,770
Again, it lacks some intuition and I will
not describe how it is computed, but

131
00:08:37,770 --> 00:08:41,020
it is the measurement of
the noise from sample to sample

132
00:08:41,020 --> 00:08:45,240
that we would expect in Pearson's
correlation coefficients.

133
00:08:45,240 --> 00:08:49,950
Like our previous examples of hypothesis
tests, the signal to noise ratio

134
00:08:49,950 --> 00:08:54,340
would be the observed correlation
coefficient divided by it's noise.

135
00:08:54,340 --> 00:08:58,940
In our example,
we get a signal to noise ratio of 10.82.

136
00:08:58,940 --> 00:09:04,390
Again, we would look to see where 10.82
lies on a standard normal distribution.

137
00:09:04,390 --> 00:09:09,330
And therefore, we get a p value that is
extremely small, because a critical value

138
00:09:09,330 --> 00:09:13,150
of 2 is much smaller than our
observed statistic of 10.82.

139
00:09:13,150 --> 00:09:16,890
We have strong evidence that we
should reject the null hypothesis and

140
00:09:16,890 --> 00:09:21,380
conclude that these two quantities
are correlated with each other.

141
00:09:21,380 --> 00:09:26,210
We can also compute a 95% confidence
interval by simply taking our observed

142
00:09:26,210 --> 00:09:31,980
correlation coefficient of 0.84 and adding
and subtracting two times the noise.

143
00:09:31,980 --> 00:09:36,880
That produces a confidence
interval of 0.69 to 0.99.

144
00:09:36,880 --> 00:09:40,750
Again, the null value of zero is
not in this confidence interval so

145
00:09:40,750 --> 00:09:45,200
we should expect to see a p value
that is much smaller than 0.05.

146
00:09:45,200 --> 00:09:47,690
One thing to note,
is that the upper bound for

147
00:09:47,690 --> 00:09:53,430
the above confidence interval could have
gone above 1 if the noise had been larger.

148
00:09:53,430 --> 00:09:58,360
It is nonsensical because correlation
cannot be more than 1, and likewise,

149
00:09:58,360 --> 00:10:01,920
we could have situations in which
the lower bound of the confidence interval

150
00:10:01,920 --> 00:10:06,930
would go below negative one
which no biologically plausible.

151
00:10:06,930 --> 00:10:10,340
So there are approaches that are beyond
the scope of this course to compute

152
00:10:10,340 --> 00:10:17,020
the confidence interval in a way so that
the bounds will always be within -1 and 1.

153
00:10:17,020 --> 00:10:21,422
For our example,
my statistical program gave me a 95%

154
00:10:21,422 --> 00:10:26,572
confidence interval using
this process of 0.74 to 0.91.

155
00:10:26,572 --> 00:10:31,002
One important fact of this confidence
interval and you will see it,

156
00:10:31,002 --> 00:10:35,502
that it is not symmetric around
our observed correlation of 0.84

