1
00:00:03,460 --> 00:00:13,260
Oh, well. Okay.

2
00:00:13,800 --> 00:00:16,940
So hopefully that's getting captured again.

3
00:00:19,800 --> 00:00:23,000
Sorry about that. Okay.

4
00:00:26,590 --> 00:00:34,809
Where was I? Yeah. So going back, I had actually no hope until, you know,

5
00:00:34,810 --> 00:00:40,480
for a good few years until this whole class of agents came in called the immunotherapy agents in cancer,

6
00:00:40,930 --> 00:00:45,610
which basically activate the body's own immune system to act against cancerous cells.

7
00:00:46,150 --> 00:00:51,430
And, you know, so that has changed the whole paradigm of cancer research and treatment as well

8
00:00:51,670 --> 00:00:55,480
in multiple cancers and showing very remarkable benefits in certain cancers.

9
00:00:55,900 --> 00:00:58,570
So here it's basically two immuno activating agents.

10
00:00:58,570 --> 00:01:05,630
So they basically stimulate your immune system and to induce what is called cell differentiation implicitly,

11
00:01:06,120 --> 00:01:12,370
you know, killing the cell basically, you know, and this could be a tumor cell, for example.

12
00:01:12,880 --> 00:01:16,840
And you count the number of cells and you see how these two agents do that.

13
00:01:16,840 --> 00:01:20,829
And you want to see what is that an effect or not or is that a joint effect?

14
00:01:20,830 --> 00:01:28,989
Is that what sort of an effect is it? So the the scale of the responses here is the number of cells and and they have to be dealt with.

15
00:01:28,990 --> 00:01:36,040
So a typical table would be like this. So maybe there was a pathologist who sat down, looked at the slide and said,

16
00:01:36,040 --> 00:01:40,600
there are 11 cells and those that are 18 cells and those that are 20 cells in this particular slide.

17
00:01:41,110 --> 00:01:44,950
And then depending on what those well, these two agents were given.

18
00:01:45,340 --> 00:01:48,460
So the idea is these two are the covariates. We want to see the assessment.

19
00:01:49,170 --> 00:01:59,170
Right. So so the response clearly is a count variable here and the distribution one uses because the poison on distribution,

20
00:01:59,410 --> 00:02:03,970
it's a natural distribution because the realizations of the poison distributions are counts.

21
00:02:04,540 --> 00:02:09,820
And the regression model essentially becomes a version of the what is called the poison model.

22
00:02:10,360 --> 00:02:13,599
Right? So then you have just plain count data.

23
00:02:13,600 --> 00:02:18,909
You can just take these responses by themselves, you can treat them as continuous and do a regression.

24
00:02:18,910 --> 00:02:21,970
What you could do it. Nobody's stopping, you know.

25
00:02:22,510 --> 00:02:28,940
But if you use linear regression, know what issues could arise and and.

26
00:02:31,980 --> 00:02:36,870
Actually, I may pose that as a question. So if you if you used a standard linear regression, right.

27
00:02:37,020 --> 00:02:45,230
Did this work then as would you put this as a Y, put this as an X matrix and pipe it through an element function in our right.

28
00:02:46,680 --> 00:02:52,620
You could do it. There's nothing stopping you from doing what what what might what type of issue is going to arise.

29
00:02:54,680 --> 00:02:58,040
Yes. The songs and characters, football is great.

30
00:02:58,880 --> 00:03:03,650
That's a good point. You could get negative predictions, right?

31
00:03:04,320 --> 00:03:08,150
You're not built, so you're not maintaining fidelity to the scale of the data.

32
00:03:08,330 --> 00:03:15,530
Right. So it's sort of a imprecise model or as statisticians call it, a mis specified model.

33
00:03:16,250 --> 00:03:20,270
So it'll be a mis specified model that goes that's not fidelity to the scale of the data here.

34
00:03:20,960 --> 00:03:24,230
Right. And that's where the poison distribution is used.

35
00:03:24,740 --> 00:03:32,090
So that's one of the issues. And we're coming to other issues now when we get to Western regression.

36
00:03:32,600 --> 00:03:45,130
Yes, sure. So in 650, we used a lot of data from a dataset where the outcome variable was depression and it had it was like scores on a questionnaire.

37
00:03:45,140 --> 00:03:52,550
So it was kind of like count, but kind of like categorical or like pseudo continuous data.

38
00:03:52,940 --> 00:03:57,410
Is that something where you could use another like alternate strategy to deal with it?

39
00:03:57,470 --> 00:04:03,080
Absolutely. Again, what would be like the best type for those questionnaires that give you kind of a score?

40
00:04:03,110 --> 00:04:11,090
So it depends on. So usually questionnaires are on a scale of 1 to 5 or.

41
00:04:11,240 --> 00:04:22,160
Right. And or they're -2 to -2 to do you have this and sometimes you just, you know, if you're averaging across multiple questions,

42
00:04:22,520 --> 00:04:29,179
right, and you just say that I'm going to do it, take an average response, you could be answering questions.

43
00:04:29,180 --> 00:04:35,050
Right. And 20 of them at a certain set and you just average out and just treat it as a continuous looking.

44
00:04:36,020 --> 00:04:42,530
But a true model would obey saying that you have multi-category you're going from -2 to 2.

45
00:04:42,860 --> 00:04:53,420
That would be. So it's basically it becomes an ordinary regression that will be the the the most appropriate model to factor that data.

46
00:04:54,140 --> 00:05:00,290
But folks do sometimes just an average on just pretending could work.

47
00:05:03,860 --> 00:05:11,360
It can give you some answers. But then again, you have to see how well then it goes into the goodness of fit issues, right?

48
00:05:11,390 --> 00:05:17,900
Like how well does the model fit the data? And then on the other hand, what you learn in this course is on your regression.

49
00:05:17,900 --> 00:05:21,200
You fit both the models and then blend and compare them.

50
00:05:21,530 --> 00:05:24,740
There are subject objective metrics where you can compare.

51
00:05:27,840 --> 00:05:40,380
But answer your question. No. So so the way now we'll get more into the notational aspects of this so here why would be the event count.

52
00:05:40,390 --> 00:05:47,830
So in our case this would be the this would be the cells, right?

53
00:05:47,830 --> 00:05:53,920
And they and they they can take values from zero one, two, three, the number of cells.

54
00:05:54,460 --> 00:06:01,600
Right. And, and we'll have what it will define what is called an event rate, which will be basically from a zone distribution.

55
00:06:01,600 --> 00:06:07,150
It has a very natural interpretation as an event trait, which would be the mean of the poison.

56
00:06:07,390 --> 00:06:09,160
So I'm just giving a very high level.

57
00:06:09,160 --> 00:06:17,530
You don't have to, you know, understand this as yet to accept the fact that the bottom line here being that when that is gone,

58
00:06:17,530 --> 00:06:25,660
data will be modeling what is called an event rate, and that will be the mean of the response and the mean will be related to the covariance.

59
00:06:26,490 --> 00:06:32,800
No through some magic will come to the magic in the next few classes.

60
00:06:33,840 --> 00:06:41,860
All right. So why would this constitute as an appropriate more mock modeling strategy?

61
00:06:42,070 --> 00:06:48,250
Right. So the first is the violation of normality, because, you know, why is discrete and non negative.

62
00:06:49,120 --> 00:06:53,080
And that would be the first as you just answered no.

63
00:06:53,350 --> 00:07:03,100
And and also the event rate, which which has a nice interpretation is the has an event rate that should be positive.

64
00:07:03,310 --> 00:07:08,980
And that's not modeled using the using the normal distribution.

65
00:07:09,760 --> 00:07:17,020
The other one will get more into this is when Y is account data and the variance of.

66
00:07:18,430 --> 00:07:28,120
In Poisson distribution. The variance of responses and the mean of the responses are related basically death the same.

67
00:07:28,120 --> 00:07:38,500
So the variance is increased through the mean. So the key assumption in linear regression, which is the constant variance that's that's violated.

68
00:07:39,580 --> 00:07:43,600
Right. And there's a very simple way to intuitively think about this.

69
00:07:43,870 --> 00:07:49,659
So think about prevalence, right? Think about the number of, let's say, cancer deaths.

70
00:07:49,660 --> 00:07:54,070
Right. We want the number of cancer deaths in U.S. and compare it to, say, Mexico.

71
00:07:54,610 --> 00:08:02,020
Let's say compared to Costa Rica. Now, if you expect the prevalence will be much higher in U.S. because it's a higher population.

72
00:08:02,260 --> 00:08:06,700
Right. And Mexico and Costa Rica or Japan, for that matter.

73
00:08:07,260 --> 00:08:17,750
There are many factors. And as you see, higher number of incidences like the variance of that number of incidences over the U.S. will also be higher.

74
00:08:18,820 --> 00:08:22,390
Right. As you compare us versus a smaller country.

75
00:08:23,020 --> 00:08:26,640
So that means the mean changes with the variance. Right.

76
00:08:26,650 --> 00:08:31,660
So the variance changes would be greater because it's a number of incidences.

77
00:08:32,140 --> 00:08:39,670
So that's why the constant variances that this is this is not an appropriate model to consider because the more number you observe,

78
00:08:40,690 --> 00:08:45,280
then you would be observing a higher variability in that number. That's a simple, intuitive.

79
00:08:45,670 --> 00:08:51,610
What it means by the constant variance. Variance is not a reliable assumption for computer models.

80
00:08:54,280 --> 00:09:03,610
Okay. And then and in the poison setup or we'll be doing is just relating to the mean of the Poisson distribution to the covariate.

81
00:09:03,630 --> 00:09:13,300
So instead of modeling the the Y is directly and this would be the linear regression and instead model the,

82
00:09:13,700 --> 00:09:18,550
the lambda I, which is the mean of the Poisson that would be related to the variance.

83
00:09:19,300 --> 00:09:26,830
No. And actually, let me take that back.

84
00:09:27,190 --> 00:09:33,550
We'll be relating it, but we will not be relating it directly because the event rate, this is a positive number.

85
00:09:33,730 --> 00:09:36,730
So Lambda is always positive. Right.

86
00:09:37,150 --> 00:09:42,670
And then what do we be doing is using the link function and as we did in the logistic,

87
00:09:43,240 --> 00:09:48,670
instead of modeling directly as a function of lambda, we'd be just taking the law above lambda.

88
00:09:49,390 --> 00:09:56,980
So now we have covered the whole range. So the lambda is from zero to infinity, theoretically.

89
00:09:57,310 --> 00:10:03,670
So lambda I. But the log lambda ranges from minus in unity to 39.

90
00:10:04,270 --> 00:10:07,420
And so this would be a model that would be fitting.

91
00:10:09,550 --> 00:10:16,090
Okay. So our observed data is why I write an essay.

92
00:10:16,870 --> 00:10:22,870
So instead of connecting them directly, we go through Lambda I and we connecting them to the.

93
00:10:29,220 --> 00:10:35,140
Right. And this is the model that does it right.

94
00:10:36,000 --> 00:10:41,940
Again, I'm just calling it to take away right now is the infusion of growth that the

95
00:10:41,940 --> 00:10:45,920
fact being that these two are not modeled directly as in linear regression,

96
00:10:46,350 --> 00:10:50,850
but they go to another mean but the mean of another distribution.

97
00:10:51,840 --> 00:11:00,720
For example, it was the binary Bernoulli or the binary distribution of the Bernoulli or the binomial distribution for binary data.

98
00:11:00,960 --> 00:11:06,180
In the data, it would be logistic, it would be Poisson distribution, right?

99
00:11:07,020 --> 00:11:09,970
And we were fitting this model through maximum likelihood in a spin, but.

100
00:11:15,180 --> 00:11:18,710
So the mechanics are not important at this point, just intuition that we're doing that.

101
00:11:20,610 --> 00:11:32,409
Right. So. So the so the the summary here focus for the interest of slides is giving these examples is linear regression

102
00:11:32,410 --> 00:11:38,640
regressions inappropriate in each of these examples is not the need for a more general regression framework.

103
00:11:41,220 --> 00:11:46,470
Accounting for the responses that have a different variety of measurement skills.

104
00:11:47,520 --> 00:11:50,880
Binary Multi-category incumbent right.

105
00:11:53,580 --> 00:11:59,580
And they learn about all the methods for module fitting. So we have specified a model and as soon as you come to a distribution,

106
00:11:59,580 --> 00:12:03,210
as soon as you think about distribution for data, you're thinking about a model, right?

107
00:12:03,960 --> 00:12:08,550
And that's what you're learning from as you've learned in six or one, right?

108
00:12:08,550 --> 00:12:11,940
You have a dataset. As soon as you say it's coming from a distribution,

109
00:12:12,360 --> 00:12:19,860
you're positing as you're saying that the scientific process that generated the data comes from this distribution.

110
00:12:21,780 --> 00:12:29,460
And that's a model. And then once you ask the model, mean your variances, you have other other summaries of the distribution one can.

111
00:12:30,150 --> 00:12:34,110
And that's where the inference comes in your estimate of the parameters.

112
00:12:35,250 --> 00:12:43,800
And ideally, as we'll see some elements of regression, linear regression, everything that you've learned in 650 should carry over.

113
00:12:45,600 --> 00:12:48,670
And they do. And you'll see they're better.

114
00:12:48,720 --> 00:12:55,590
All they do and sometimes don't. And especially the interpretation of the covariates, the do it remains the same.

115
00:12:56,160 --> 00:13:07,500
And so the aspects of least squares and squares, estimation and generalizations, you see, that has a direct relationship with any user question.

116
00:13:11,720 --> 00:13:18,680
And as I said, that's alluded to before. One can see generalizations to more complex settings.

117
00:13:18,830 --> 00:13:30,930
And as you lose 651 some of you might go on to take 653 or might be taking with 695 categorical and might be taking in parallel in this,

118
00:13:30,950 --> 00:13:39,080
which is categorical data analysis which just focus this and there's no regression component to it as purely analysis of categorical data.

119
00:13:39,740 --> 00:13:47,230
And let's see a much more complex setting where you have correlated data and you have sensor observations, which is another type of response.

120
00:13:47,780 --> 00:13:53,750
When you get to 675, it's a survival data, the elements that you will learn here.

121
00:13:53,990 --> 00:14:00,710
So this building blocks for those. Questions.

122
00:14:07,610 --> 00:14:10,970
So with me so far and this deck of slides.

123
00:14:12,020 --> 00:14:21,420
Right. That's right.

124
00:14:22,740 --> 00:14:29,310
So for the rest of the at least today's class, I'll cover a little bit of linear regression review.

125
00:14:29,580 --> 00:14:37,260
And as I said again before, some of these aspects that you might already know most of it.

126
00:14:37,260 --> 00:14:49,080
But just for completeness, I want to do a review here and also introduce the concept of weighted squares to do two allegedly squares in 650.

127
00:14:50,530 --> 00:14:56,799
It was mentioned it was mentioned as a solution for violations of memory assumptions.

128
00:14:56,800 --> 00:15:01,250
But we didn't go into any details. No details could be right.

129
00:15:02,320 --> 00:15:11,140
So thank you. So that's all I'll mention the concept of it at least squares, because that's sort of an important one to learn about.

130
00:15:12,680 --> 00:15:16,930
Then we get into the estimation of re-introduce class comes in.

131
00:15:17,660 --> 00:15:25,600
Right. So, so setting up the standard notation here for linear regression, you have the response why I.

132
00:15:30,660 --> 00:15:33,690
I so think about this as just one response right now.

133
00:15:38,080 --> 00:15:41,230
What I mean by one response is just so skilled. Right.

134
00:15:43,300 --> 00:15:53,410
And then you have a covariates of covariates. So this is B dimensional and I indexes the subjects.

135
00:15:53,410 --> 00:15:56,980
I guess we want to end by the standard notation.

136
00:15:59,500 --> 00:16:02,890
So this is the representation that an individual level.

137
00:16:03,560 --> 00:16:14,170
All right. And then the model that you're positing is you're linearly relating the predictors to the mean response.

138
00:16:14,680 --> 00:16:21,820
And the key assumption here and NGL is both for linear models in GLM is that X is deterministic.

139
00:16:25,000 --> 00:16:30,280
You know, anybody willing to answer, what does that mean, x is deterministic?

140
00:16:31,000 --> 00:16:34,630
Yes. Basically no measurement using these measurements of.

141
00:16:35,200 --> 00:16:39,040
Exactly. Correct. Predicted. Yep. So.

142
00:16:39,670 --> 00:16:45,730
Well, what would be the implication of that? Correct.

143
00:16:46,340 --> 00:16:49,910
So X is not a random variable. Right.

144
00:16:51,530 --> 00:17:00,320
And that's a really, really important to understand in regression because everything is done Y conditional on X.

145
00:17:01,730 --> 00:17:05,470
I. X.

146
00:17:05,770 --> 00:17:13,200
There's no model on X or other distribution because as soon as you see a random variable, you say distribution and there's a distribution model.

147
00:17:16,010 --> 00:17:23,960
And the reason one does that is, first of all, it's simplicity, because once you once you model start modeling exist also,

148
00:17:24,560 --> 00:17:30,860
then you're modeling and you're specifying many, many models because each of these will have a model in itself.

149
00:17:31,490 --> 00:17:37,160
So in this case, there's only one model. The only model is this way.

150
00:17:38,740 --> 00:17:41,740
Is is. You know, this whole thing.

151
00:17:42,070 --> 00:17:47,290
That's the mean. Plus some random error. And that random error is a normal distribution.

152
00:17:47,620 --> 00:17:58,000
So you just specifying it goes in distribution on a single Y and and assuming X is deterministic, you don't need to specify a distribution.

153
00:17:59,290 --> 00:18:03,430
So that's what it's been is conditional on Y given X.

154
00:18:05,700 --> 00:18:15,360
I guess it would depend on model in response variables, things, weights and incentive variables like exposure and.

155
00:18:21,590 --> 00:18:28,040
I like very much by improving your models, which is we are trying to find them in real life is true.

156
00:18:29,060 --> 00:18:39,500
Are we just simplifying it by that assumption? So in some sense, the that goes into what is the delineation of the response versus the covariant.

157
00:18:40,370 --> 00:18:45,530
And the assumption, as you said, is that it's observed with no error.

158
00:18:45,590 --> 00:18:49,430
So if you observe rate when a given study now goes are observed.

159
00:18:49,430 --> 00:18:53,600
Exactly. There's no there's no matter around that. But it's true.

160
00:18:53,600 --> 00:19:01,850
There could be an outcome, though, right? For example, there is survey instruments, this one.

161
00:19:02,090 --> 00:19:08,690
Right. And so, again, self-reported right to somebody could do a self-reported reads and those could be all right.

162
00:19:08,930 --> 00:19:14,810
And how would you ascertain? So there has to be some true weight and you observe the the reported rate with some error.

163
00:19:15,440 --> 00:19:24,890
Right. And that's why there's this another class of models called measurement error models where you could bring in the the error in the axis as well.

164
00:19:26,510 --> 00:19:30,570
So that's called measurement error models. Yeah.

165
00:19:30,590 --> 00:19:42,950
So I would just put aside no directive. Give X is measured with error, but that's part of measurement.

166
00:19:52,270 --> 00:19:56,420
Yeah. Hmm.

167
00:19:58,050 --> 00:20:07,100
Yeah. And that goes into positing a model where, you know, y given X, and then there's there's a model on X by itself.

168
00:20:08,450 --> 00:20:14,690
Right. So you specify a joint model, but your your model, I give an X as this and then X follows some distribution.

169
00:20:15,350 --> 00:20:25,150
Some distribution. And then basically this record, if you go through the problems of conditional probability, you know,

170
00:20:25,220 --> 00:20:29,600
one joint, you know, the marginal, you can get the joint distribution of both Y and X together.

171
00:20:32,540 --> 00:20:37,820
That's the joint distribution that will not go there. But, you know, that's a that's sort of the intuition.

172
00:20:39,130 --> 00:20:42,530
Now, this is complex enough to answer your question.

173
00:20:43,070 --> 00:20:53,050
Yeah. Yeah. No. Or the other way to do that is in a natural way is to do it in a Bayesian setting where everything is a random variable.

174
00:20:53,560 --> 00:20:54,610
Then you create the whole thing.

175
00:20:54,610 --> 00:21:02,410
It's not expensive by a judge on distribution on this, and you tease out the conditional distribution that goes back into regression.

176
00:21:03,910 --> 00:21:09,480
And let's, let's stick to this formalism right here, which is actually very useful enough.

177
00:21:09,880 --> 00:21:15,010
So we're just saying that one of the assumptions we are making, one needs to be about the assumptions we are making.

178
00:21:15,790 --> 00:21:25,899
Right. So the. So essentially what that boils down to is that, you know, there's a mean of the distribution and then the variance of the distribution.

179
00:21:25,900 --> 00:21:32,020
And the variance is a constant variance, which is a normal zero Sigma Square.

180
00:21:32,860 --> 00:21:36,010
Right. Now, that's that's the right.

181
00:21:36,340 --> 00:21:41,770
And and this is the matrix form that you might have seen countless number of times,

182
00:21:43,270 --> 00:21:47,020
you know, collate all the wise together, stack them up in a vector.

183
00:21:47,500 --> 00:21:51,220
Similarly, your stack all the axes together for all the individuals,

184
00:21:51,790 --> 00:21:58,600
put them in a vector and that becomes the canonical representation of a linear regression model.

185
00:21:59,300 --> 00:22:21,600
This is your multivariate normal distribution. But the whole multivariate normal would be centered around x beta, right?

186
00:22:22,210 --> 00:22:27,430
And then with with a spread which is scaled by Sigma Square.

187
00:22:32,840 --> 00:22:34,790
Across the observations.

188
00:22:39,620 --> 00:22:59,180
So essentially you have to do a set of parameters to estimate here, which is a P plus one dimensional parameter and that's the variance, right?

189
00:23:06,770 --> 00:23:13,000
And the people as one just come to an intercept and the B does.

190
00:23:13,100 --> 00:23:20,530
Yep. So that's the intercept. And then each of the regression coefficients corresponding to the right.

191
00:23:25,790 --> 00:23:35,650
All right. So you might have covered this in security as well, but you put it in the in a more formal setting.

192
00:23:37,870 --> 00:23:44,350
Jill says, well, the assumptions can be succinctly stated as there is a systematic component.

193
00:23:47,880 --> 00:23:53,040
Night at the the predicted effect is through linear regression on the mean.

194
00:23:54,040 --> 00:24:05,570
Right. And this is another. Important observation to note we are regressing only the mean of the responses on the comments.

195
00:24:07,750 --> 00:24:16,420
Right. It's the conditional mean why given X the conditional meaning of life is related, you know.

196
00:24:20,480 --> 00:24:24,530
You could think like that's just modeling the center of the distribution.

197
00:24:24,920 --> 00:24:32,240
We're not modeling anything about the spread of the distribution or anything that's just assumed that comes under the random component.

198
00:24:32,720 --> 00:24:36,280
Right at each level of the predicted, the variation is constant.

199
00:24:36,380 --> 00:24:39,380
So this is what is called homeless cadastre.

200
00:24:47,770 --> 00:25:01,540
Spell that right. See if I can do that. Speaking of that, I got that right.

201
00:25:03,160 --> 00:25:08,380
It's a spelling bee word. It's called homeless Gadsden City.

202
00:25:09,390 --> 00:25:12,970
What it means is that the variance does not change, right?

203
00:25:15,750 --> 00:25:20,430
So two things are happening here. One, we are modeling only the mean.

204
00:25:20,790 --> 00:25:24,890
Right. One could ask, why don't we model the variance? Why don't we model that one tails?

205
00:25:24,900 --> 00:25:31,160
Why don't we model something else, right? That could be more flexible models when done in principle, do that.

206
00:25:31,170 --> 00:25:35,130
That goes into more what is called model regression models and such.

207
00:25:35,670 --> 00:25:39,300
So so the key here is we are doing mean regression.

208
00:25:46,810 --> 00:25:50,260
Okay. I mean, regression. Right.

209
00:25:50,890 --> 00:25:54,820
Both in 650 and 653 will always be doing mean regression.

210
00:25:55,510 --> 00:26:03,250
We won't go beyond mean regression, but that could be more complex settings, as I said, which is called quantile regression.

211
00:26:06,960 --> 00:26:13,320
Which can model other aspects of the distribution, such as variants given as doses.

212
00:26:14,430 --> 00:26:19,020
You can think about how they change, which could be important in some context.

213
00:26:19,560 --> 00:26:32,960
But main regression is actually pretty. It's a pretty ubiquitous model and lots of most, most data centric questions like.

214
00:26:36,800 --> 00:26:41,720
That's one. The other is, if you see this, there's only one parameter sigma squared.

215
00:26:41,720 --> 00:26:47,060
That's that that's not changing across the responses and it's also not changing across.

216
00:26:47,150 --> 00:27:09,890
So that means homozygous team is sigma squared does not change or does not change with my basically sample to sample.

217
00:27:10,280 --> 00:27:13,850
So it does not change. And I'll put it here.

218
00:27:14,100 --> 00:27:24,590
And the Greeks, which is the covariance, they aren't modeling because that would induce additional complexity.

219
00:27:27,670 --> 00:27:33,319
So it seems that the expectation of what arguments excite and excited.

220
00:27:33,320 --> 00:27:43,780
You know, right now collectively they are the same. So that again expectation of why not even excite equals to the expectation.

221
00:27:43,790 --> 00:27:48,040
Why is that true? In terms of mathematics, no.

222
00:27:48,360 --> 00:27:58,689
No more. Because the expectation of why by itself would be centered anywhere but moment you put in why I

223
00:27:58,690 --> 00:28:03,000
give an excited the distribution is centered around this this is the mean of that distribution.

224
00:28:05,990 --> 00:28:11,510
So if I don't consider the excise the marginal way, I could be basically just zero.

225
00:28:13,570 --> 00:28:17,110
Like the better zero. That would be the exact right.

226
00:28:17,320 --> 00:28:21,030
That's the. Right.

227
00:28:22,300 --> 00:28:29,650
So it's the conditional distribution we are talking about. And that's what I wrote somebody here right there modeling this Y given X.

228
00:28:30,730 --> 00:28:41,710
That's the whole whole basically the the the bottom line of regression, of sufficient statistics, as you know, not talking usually a sufficient fixed.

229
00:28:43,000 --> 00:28:56,050
It's always Y given X. But that the key is that this conditioning on exercise is that X is deterministic like this.

230
00:28:56,080 --> 00:29:02,680
No, no. His conditioning, this is not in the sense that what no one means.

231
00:29:02,680 --> 00:29:08,500
You clearly know what the variants are. What is the responses? There's no there's no ambiguity in that.

232
00:29:10,460 --> 00:29:15,510
Know. I mean, if one of the characters jumps over, that becomes a response.

233
00:29:15,510 --> 00:29:24,380
That's a different model. Good clarifying question.

234
00:29:24,960 --> 00:29:30,920
Yeah. But let me postpone questions.

235
00:29:31,070 --> 00:29:38,360
Yes, I think there is expectation of why is like the population explosion if I get the sample.

236
00:29:39,230 --> 00:29:44,000
Yes. You can think about it like that, correct? Yeah, that's a really good point.

237
00:29:45,770 --> 00:29:51,600
So if you think about this one, right. So you took the example, right, of Wade.

238
00:29:51,990 --> 00:29:56,030
Do you think Wade was his age or something?

239
00:29:56,040 --> 00:30:01,920
Right. So for each specific age, you can get a prediction of the weight, right?

240
00:30:02,640 --> 00:30:11,670
Because individuals will vary across. You know, somebody is £70, somebody £60, somebody is £40 or kilos and then calibrated kilos.

241
00:30:12,450 --> 00:30:15,570
Yeah. Right.

242
00:30:15,690 --> 00:30:22,540
So the idea. So the mean is individual specific. But these baiters right here, they are population level.

243
00:30:25,810 --> 00:30:32,350
So no. So if you had rate and you had I smoking status you know the gender.

244
00:30:32,740 --> 00:30:38,800
So you it's a linear combination of all three but the rates that we assign to each this are specific across the population.

245
00:30:39,940 --> 00:30:49,040
But these betas are population specific. You can't estimate those from because you have you need samples to estimate those.

246
00:30:49,450 --> 00:30:53,300
You need replication. And that replication comes from individuals.

247
00:30:53,510 --> 00:30:57,950
So that's actually that's a perfect segue to my third bullet here, which is independence.

248
00:30:58,460 --> 00:31:05,540
So we assuming that you've taken a random sample and there's no systematic dependance within the samples.

249
00:31:08,150 --> 00:31:12,260
So that's what it's called independence. Independence between subjects.

250
00:31:12,770 --> 00:31:20,089
This is a critical assumption because you need this is the true replicates and you need the true replicates to estimate these two sets of parameters,

251
00:31:20,090 --> 00:31:31,930
beta and sigma squared. And if there is any dependance right then it then it basically what it means is you don't get to replicate.

252
00:31:32,270 --> 00:31:34,250
Right. Think about time. Right.

253
00:31:35,420 --> 00:31:43,730
I mean, I could get somebody could use the measurements on me taking it for like 20 years and say and collated data set right.

254
00:31:44,150 --> 00:31:48,350
And say, I have a 20 dimensional data set about my baby, have everything about me and do a regression.

255
00:31:49,100 --> 00:31:54,770
Right. But it's not a to replicate it. Just coming from me at the time is a replica, but it's all dependent.

256
00:31:54,770 --> 00:31:59,570
If I start off at whatever, one £40, you know, I'll go higher and go low.

257
00:31:59,990 --> 00:32:05,840
Right. That's a dependance. That's an interdependence. So those are not true replicates.

258
00:32:06,470 --> 00:32:15,980
The true replicates is a random sample that each of them I think it's funny how many times I have to report this to a scientist.

259
00:32:16,670 --> 00:32:23,540
When they do biological experiments, they collect 25 measurements on a rat and say, Oh yeah, I have a large dataset.

260
00:32:23,540 --> 00:32:29,030
I said, No, it's only when you add one, right? You've got to do multiple rats.

261
00:32:29,400 --> 00:32:37,040
And so I am very clear on the assumptions.

262
00:32:38,480 --> 00:32:45,650
So the key is like that aspect I just wanted to mention is the nominal feature we're using here, the systematic and the random component.

263
00:32:45,650 --> 00:32:46,879
This will come up again and again.

264
00:32:46,880 --> 00:32:53,950
And this, okay, the systematic component is the mean, the expectation and the random component is the distribution on the errors.

265
00:32:55,130 --> 00:33:04,400
Right. So in terms of interpretation, again, for a single covariate.

266
00:33:05,660 --> 00:33:09,140
No, no. You should know this by the back of your hand.

267
00:33:09,170 --> 00:33:14,690
What is the interpretation of beta one? It's one unit increase, right?

268
00:33:14,690 --> 00:33:19,670
If it puts in three units at one unit increase, either categorical or continuous.

269
00:33:20,090 --> 00:33:24,260
And how it changes. And that's the change in the mean response.

270
00:33:24,830 --> 00:33:29,220
What is the mean response, body or net increase in the code?

271
00:33:30,770 --> 00:33:33,860
I, I really good with that. Right.

272
00:33:34,610 --> 00:33:38,870
And in multiple regression, all you need to do is conditional on the fact that you've observed.

273
00:33:39,900 --> 00:33:50,280
Another variable. Right. So you're going to condition one keeping the the the second covariate in this case I do next to as constant and

274
00:33:50,280 --> 00:33:55,860
then you'll see in a one unit increase what is the change and August two you can write it down for later to.

275
00:33:57,940 --> 00:34:01,450
Right. It's a clear interpretation.

276
00:34:02,850 --> 00:34:07,410
No. And the parameter estimation and I can proceed in multiple ways.

277
00:34:07,680 --> 00:34:10,890
Right. One is least squares estimation.

278
00:34:11,430 --> 00:34:14,870
Right. Which is basically just minimizing the distance. Right.

279
00:34:14,880 --> 00:34:21,520
It's just basically a distance based. Right.

280
00:34:22,720 --> 00:34:28,140
So you're saying I've got mine through, I've got my responses, I've got my ex badass.

281
00:34:28,150 --> 00:34:31,990
I'm going to find the data that minimizes the distance, the squared distance between the two.

282
00:34:32,620 --> 00:34:40,659
No, it's one of the most often used distance metric use the Euclidean distance and the

283
00:34:40,660 --> 00:34:46,120
mathematics works out nicely that that you have an enclosed form of the estimate.

284
00:34:47,080 --> 00:34:54,120
She's really nice. Know that you can estimate the beta and you can also get the variance estimate.

285
00:34:54,600 --> 00:34:58,700
So this does not assume any distribution, right? This is just these squares.

286
00:34:58,710 --> 00:35:02,010
This is one can use any optimization to do this.

287
00:35:02,970 --> 00:35:10,090
It's only when you come to the likelihood that this is. Likelihood, as in discussing distribution right here.

288
00:35:14,580 --> 00:35:18,220
Right. And that's when you can compute the variance and other things.

289
00:35:18,550 --> 00:35:24,100
Right. And and some key properties is it's an unbiased estimate.

290
00:35:24,460 --> 00:35:33,220
Which is really nice. Right. In another sense, you can get the variance of the estimates again, which is useful for uncertainty.

291
00:35:33,220 --> 00:35:38,230
Quantification and testing can also get the estimate of sigma x square.

292
00:35:39,640 --> 00:35:44,290
All right. And then from this, you can get the estimate of the recipients as well.

293
00:35:44,920 --> 00:35:53,530
This goes into model fit and other aspects. I'm going to this fast, but everybody with me so far, just 650 territory.

294
00:35:54,700 --> 00:36:00,670
No. So I want to go alone too much, except to set up the annotations here so that, you know, on the same page.

295
00:36:03,490 --> 00:36:14,170
The linear regression also has a nice interpretation as an analysis of variance and know what interpretation of how do you break the total variation.

296
00:36:14,680 --> 00:36:21,669
Right. Of the of all the responses and how much is explained by the regression, which is by the covariance,

297
00:36:21,670 --> 00:36:27,430
and how much is unexplained, which is a useful metric to understand variation in the responses.

298
00:36:28,090 --> 00:36:35,800
Right. And this is the total variation which is given by this by the mean and how much is explained by the regression.

299
00:36:36,700 --> 00:36:45,100
And I quite like this geometric way of interpreting this.

300
00:36:45,370 --> 00:36:52,809
Right. So this is my mean, this is my single point why I write and this is my mean.

301
00:36:52,810 --> 00:36:57,940
I'm looking at this distance right here, which is my total variation that's defined as the total variation.

302
00:36:58,780 --> 00:37:03,069
I find the best fitting line and I see how far is this point away from this?

303
00:37:03,070 --> 00:37:06,320
Right. At this point.

304
00:37:06,320 --> 00:37:09,680
Right? Sorry, this and that becomes the sum of squares.

305
00:37:09,680 --> 00:37:12,920
And what's not explained by this line becomes the error.

306
00:37:13,250 --> 00:37:16,820
And my and the whole game is how do I minimize this, right?

307
00:37:17,700 --> 00:37:24,859
I when they might this right find the best fitting line which would minimize that might exist.

308
00:37:24,860 --> 00:37:33,380
It might not exist. That depends on the quality of the game. And so.

309
00:37:35,280 --> 00:37:43,740
And having an innovative presentation, one can put into a standard an audible that comes out of usual R and SAS and MATLAB or Python,

310
00:37:44,250 --> 00:37:50,940
whatever language you can use, right? And then getting the sums of squares that explained.

311
00:37:50,940 --> 00:37:56,490
So R squared is a nice metric to see how well it's one of the goodness of fit criteria.

312
00:37:56,520 --> 00:38:01,469
How good is the model? Right. And we'll see variations of our squared flagellum as well.

313
00:38:01,470 --> 00:38:08,070
It's not very straightforward to get our squares for glimpse, but we see variations of our square that one can assess goodness of fit.

314
00:38:08,820 --> 00:38:15,350
All right. So can everybody hear me okay at the back?

315
00:38:16,070 --> 00:38:19,610
Am I good? Okay. The other class.

316
00:38:19,610 --> 00:38:32,960
I needed a mike because a little bit away. Let's take a very simple example to just go through these concepts and just reinforce them.

317
00:38:33,620 --> 00:38:50,810
So here's an example where the response is the weight of a giant squid, and it's looking at the effect of prenatal smoke like the mom smoked or not,

318
00:38:51,410 --> 00:38:58,879
and also the age between child's weight, age and exposure to prenatal smoke.

319
00:38:58,880 --> 00:39:04,960
So this will be the age and this would be the status of the if they have been exposed to prenatal smoke or not.

320
00:39:05,360 --> 00:39:12,290
And if there is a difference, I would add that why I would be the weight of the child.

321
00:39:13,100 --> 00:39:19,970
All right. So this is a simple model where you have a main effect for the age, for the smoking, and that you put in an interaction effect.

322
00:39:20,770 --> 00:39:27,829
And and as Kevin mentioned, like the interaction is basically do is that interaction with the slope, right?

323
00:39:27,830 --> 00:39:32,450
Is it a constant slope? This is a different slope. That's the interaction term.

324
00:39:33,710 --> 00:39:39,020
And once you have this once you have this model, you can answer a lot of questions.

325
00:39:39,770 --> 00:39:43,520
So questions as soon as you say a question, it's a testing.

326
00:39:43,670 --> 00:39:48,970
So that's why the hypothesis tests come in and you will learn more this in six or do right.

327
00:39:48,980 --> 00:39:52,670
It's it's a decision. Right. Does age affect this or not?

328
00:39:53,780 --> 00:39:59,780
It does have an effect. Does exposure to prenatal smoke have an effect to they interact with it together?

329
00:40:00,440 --> 00:40:06,400
And as soon as you ask, asking the question, you're making a decision. Then you can make it through hypothesis tests.

330
00:40:06,570 --> 00:40:11,750
Right. So beta one is equal to zero. The same does age of an effect.

331
00:40:11,750 --> 00:40:27,960
Right. Beta one equal to beta to equal to zero.

332
00:40:27,990 --> 00:40:47,490
Is there any is there any effect? Is there. New Age or smoking affected status, or there could be controls as well.

333
00:40:48,090 --> 00:40:49,470
There can be specific questions.

334
00:40:50,460 --> 00:40:59,100
So the nice thing is in a linear regression setting, you can you can consume all of these different hypotheses into a contrast matrix.

335
00:40:59,490 --> 00:41:08,730
Right, and do a testing. So based on this and you can come up with a statistic that's an F distribution, right?

336
00:41:09,030 --> 00:41:16,760
The result of this. Right. Contrast matrices and like and how to construct them and why they are distributed.

337
00:41:18,270 --> 00:41:21,990
Yes. Okay. Good to see some heads.

338
00:41:35,690 --> 00:41:42,920
So in terms of hypothesis test, this is the most general way of positing that this statistic,

339
00:41:45,620 --> 00:41:52,159
this by this general fund, if you're testing for only one parameter, he produces to a detest.

340
00:41:52,160 --> 00:42:09,090
Right to only one parameter. Only one parameter which reduces to a T test, which is a specific case of an F test.

341
00:42:09,630 --> 00:42:15,660
But if you're testing multiple parameters, then it's basically a good enough test out on a test.

342
00:42:16,830 --> 00:42:20,670
So this is testing it all. All of them are equal to zero or not.

343
00:42:21,390 --> 00:42:24,480
Like all of the competitors. Is that any effect of any of them?

344
00:42:24,480 --> 00:42:27,600
So one thing that hypothesis does based on that.

345
00:42:30,360 --> 00:42:40,080
And that goes back to the you know, what they do here is if you test the you know, this is testing, is that a regression effect?

346
00:42:42,840 --> 00:42:45,900
But one can test, as I said, the other regression model.

347
00:42:46,680 --> 00:42:55,250
You can test one covariate at a time to coverage at a time or combinations of covariates at a time.

348
00:42:55,770 --> 00:43:00,960
And this this this does statistics allows you to do that now.

349
00:43:01,360 --> 00:43:08,579
So makes it very nice because you can test for anything you want and you can also test for inequalities.

350
00:43:08,580 --> 00:43:11,010
Actually, you know, I don't know if you've done that.

351
00:43:12,270 --> 00:43:17,880
Is that a differential effect of the covariates is an increasing trend in the community, those kind of things as well.

352
00:43:20,250 --> 00:43:30,090
Okay. Which is really nice because they're all close from formulas for this, which really easy to compute, know and then obtain inferences.

353
00:43:30,750 --> 00:43:39,240
Right. But then it makes, as I said again, it makes assumptions and systematic assumptions.

354
00:43:39,480 --> 00:43:43,590
So one is, of course, linearity. It assumes that everything is linear.

355
00:43:43,920 --> 00:43:49,050
The expectation of Y is a linear function of the covariates.

356
00:43:49,890 --> 00:43:56,120
Of course, it assumes Gaussian distribution that assumes normality, assumes equal variance.

357
00:43:56,130 --> 00:43:59,250
If you said homeless Cadastre City, it assumes independence.

358
00:44:01,020 --> 00:44:13,679
Yes. The general systems commitments. So this one right here, let's say you're explaining you have like five like coefficients and zero.

359
00:44:13,680 --> 00:44:18,110
They want to integrate, but you're generalizing your hypothesis or we test two of them.

360
00:44:18,120 --> 00:44:24,510
So if they one beta two with your contrast me with your contrast matrix, just include a bunch of zeros at the end.

361
00:44:24,780 --> 00:44:28,679
Correct. And the degrees of freedom would not be that changed.

362
00:44:28,680 --> 00:44:34,370
It would still be at minus five because beta one v0.

363
00:44:35,220 --> 00:44:39,480
Yes. It'll be a to cross five matrix because you're testing two at a time.

364
00:44:40,240 --> 00:44:43,890
Right. So it would be a matrix at one go vector anymore. Okay.

365
00:44:45,140 --> 00:44:52,470
Yes. And you're setting. So yet your true model always has five covariance in it.

366
00:44:53,460 --> 00:44:59,400
So you won't be changing when you're testing combinations of a subset of them at a time.

367
00:45:00,210 --> 00:45:06,990
Okay. Yeah. Any other questions?

368
00:45:10,390 --> 00:45:17,550
So so these are, as I said, systematic assumptions in linear regression that's been posited.

369
00:45:18,120 --> 00:45:21,900
Right. But of course, as you make assumptions, there could be violations of these assumptions.

370
00:45:22,470 --> 00:45:32,070
And how does one check. Right. So one can do different kind of diagnostics to to see the violations of these assumptions.

371
00:45:32,640 --> 00:45:35,340
Some are objective. Some are more subjective. Right.

372
00:45:36,300 --> 00:45:43,610
So if that is linearity right, one can do check one can do a partial regression plot or residual plot.

373
00:45:44,370 --> 00:45:49,650
And that is the next class out now.

374
00:45:50,290 --> 00:45:56,760
Yeah. Okay. So let me finish this slide and stop today.

375
00:45:56,770 --> 00:46:00,720
I'll do the way. Done this class. I wanna rush through it. But let me finish this slide.

376
00:46:02,310 --> 00:46:06,300
One can do of a partial regression plot or a residual plot.

377
00:46:06,600 --> 00:46:11,340
Right. And then you can see there is a systematic change.

378
00:46:11,970 --> 00:46:19,830
Right. In the residual plot. Is that a trend? And one can do a transformation, usually a box course transformation or other transformations?

379
00:46:20,640 --> 00:46:29,520
No. Or one can do another, you know, go into more adding another regress such as the cubic or a quadratic term.

380
00:46:30,420 --> 00:46:43,710
And that goes into what is more non non parametric like some of you might have heard about airplanes.

381
00:46:44,220 --> 00:46:47,660
Right. Planes. One can use planes.

382
00:46:47,660 --> 00:46:57,180
So now actually, I should say even neural networks as part of the deep learning and other things one can add if you suspect this moment in reality,

383
00:46:58,320 --> 00:47:06,920
which is most no. It goes back into what's interpretable, right.

384
00:47:07,430 --> 00:47:12,379
And sometimes linear models are really interpretable to see as a constant effect.

385
00:47:12,380 --> 00:47:19,700
Right. That's what you said. If there's a slope to the normality, one can do desperate, right?

386
00:47:20,340 --> 00:47:29,210
Normal quantile plot. One can do a bit over the test to do a normality test, and that goes in distribution is not a good fit to the model.

387
00:47:29,240 --> 00:47:32,690
One can again do a transformation like in ghost transformations.

388
00:47:33,200 --> 00:47:38,360
Alternatively, one can look at globes where you can bring in more other forms of distribution.

389
00:47:38,930 --> 00:47:44,440
Like I said, even for continuous data grabs could be useful because that could be what is called gamma distribution.

390
00:47:44,450 --> 00:47:56,030
If you see a skew which doesn't go away after even after transformation, equal variance, again, the check is through a residual plot.

391
00:47:56,420 --> 00:48:04,879
Now, if you see a constant scatter so thing or if you see a funnel shaped aspects like one can do a transformation or you can do it in these squares,

392
00:48:04,880 --> 00:48:14,660
as we'll get to in the next lecture, the independence is basically done, done by intuition as in sense how was the data collected, right?

393
00:48:14,660 --> 00:48:23,330
If that is repeated measures, as I give an example, you know, that is a dependance and that's where you should consider more complex models,

394
00:48:23,330 --> 00:48:26,840
not complex models, but appropriate models live longitudinal time series.

395
00:48:27,260 --> 00:48:29,809
And again, retinal blocks are useful.

396
00:48:29,810 --> 00:48:36,650
So these are some of these are objective like quantile plots and statistical tests, but some are more personal plots.

397
00:48:36,650 --> 00:48:40,730
You have to do standard them. And so don't make a decision. Right. This becomes a bit more subjective.

398
00:48:41,900 --> 00:48:51,590
All right. So and we'll see how the equal variance assumption, at least we can we can somehow remedy through the rate at least squares.

399
00:48:52,010 --> 00:48:57,980
And let's stop there for to this class and think of this.

400
00:48:58,250 --> 00:49:01,730
So next class again will be on Zoom because Dr. Hay is traveling now.

401
00:49:02,360 --> 00:49:10,600
So we'll do a Zoom lecture which will cover greater squares and then but from after that, we hope everything will be inclusive.

402
00:49:10,970 --> 00:49:15,330
So right. This Wednesday results.

403
00:49:15,720 --> 00:49:22,100
Yes. Because after this the other sections do and I can't decide on this the global world.

