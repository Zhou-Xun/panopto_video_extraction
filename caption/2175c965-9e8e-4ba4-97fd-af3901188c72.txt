1
00:00:00,720 --> 00:00:09,750
That's so much more. Ah, that thing of it. Oh, I have no idea.

2
00:00:14,250 --> 00:00:21,930
Yeah, this might just be something that.

3
00:00:22,980 --> 00:00:42,430
You didn't say that. Yeah. And I was going to tell you.

4
00:00:42,670 --> 00:00:46,710
And so are you.

5
00:00:47,100 --> 00:00:53,310
Okay. Good afternoon. And. Well, this is very skewed.

6
00:00:53,850 --> 00:01:07,020
Setting it up in a way, we need the propensity scores to suggest that this selection bias applies.

7
00:01:08,010 --> 00:01:15,990
Yes. As promised, I'd like to touch base a little bit the midterm coming on this Thursday.

8
00:01:16,590 --> 00:01:21,390
So due to my engaged panel, I cannot teach on Thursday.

9
00:01:21,420 --> 00:01:24,730
And we have a town hall as well.

10
00:01:24,750 --> 00:01:38,940
Maybe I'm not able to attend either, so. Oh, this is very big commitment for two days of online meeting for all the proposals.

11
00:01:39,570 --> 00:01:51,000
Anyway, so I will distribute the midterm at noon on Thursday 27.

12
00:01:53,460 --> 00:02:01,300
Right. So I think I made a time break to just make sure that our is okay.

13
00:02:06,080 --> 00:02:13,660
Yes. So I will release the I mean, distribute the midterm questions, sharpen the.

14
00:02:15,230 --> 00:02:26,629
I'm sort of thirsty in canvas so that you are supposed to return by 1159 Friday next day.

15
00:02:26,630 --> 00:02:33,650
So technically you have 24 hours, one minute short, but 1/2 short,

16
00:02:33,950 --> 00:02:43,130
whatever the 24 hours and it you just submitted to your PDF file to canvas you know that.

17
00:02:43,140 --> 00:02:46,720
So you have been doing this all the time in the past couple of years.

18
00:02:46,760 --> 00:02:51,440
And yeah, so I will come to print the hard copies.

19
00:02:51,890 --> 00:02:56,930
If you want to send me a PDF file to my personal email account, that's fine,

20
00:02:59,090 --> 00:03:11,540
but I prefer that you send to the canvas drop box and there I can click all the your your returns there and grade it.

21
00:03:13,580 --> 00:03:20,030
I started look how your home work number two but I couldn't have finished grading I mostly look at

22
00:03:20,030 --> 00:03:29,470
your data and and data analysis part and just to see if you are capable of analyzing the data.

23
00:03:30,050 --> 00:03:39,350
So far for all the second homeworks I look at, I think you are fine and you are able to produce things.

24
00:03:39,350 --> 00:03:51,800
I, I say there are some kind of similarities in a sense, the data processing and the model used.

25
00:03:52,400 --> 00:03:55,640
And but I think the explanations are somewhat different.

26
00:03:56,660 --> 00:04:05,360
So I feel that there must be some group of teamwork somewhere in the data processing, which is fine.

27
00:04:05,990 --> 00:04:10,879
I think that it's very tedious sort of work to process data.

28
00:04:10,880 --> 00:04:18,620
Now I have a clean the dataset and I will use that data for midterm.

29
00:04:19,910 --> 00:04:23,930
So there are three questions in the midterm.

30
00:04:24,020 --> 00:04:36,200
So of mimicking what you have been doing the homework. So one question above modeling like so if you have this COVID infectious disease data, I mean,

31
00:04:36,200 --> 00:04:42,829
we are talking about some of models, but there are other models are possible that, you know, to model the data.

32
00:04:42,830 --> 00:04:48,049
So I want to you do a little bit to model the modeling question,

33
00:04:48,050 --> 00:04:54,890
like the question you did for the first question of whom up to that type of question were like,

34
00:04:54,890 --> 00:05:01,860
how do you look at over dispersion because of the tempo dependance when you aggregate data together, right?

35
00:05:02,840 --> 00:05:12,770
Working on weekly data routers and data data you this data aggregation naturally produce over dispersion and then

36
00:05:12,770 --> 00:05:21,800
that you need to understand how over dispersion can be mauled by some more like in our case is GABA plus or model.

37
00:05:21,830 --> 00:05:24,649
Right. So that's how the is the scenario.

38
00:05:24,650 --> 00:05:34,190
Well gamma brings the gamma distribution pretty some over dispersion sort of option in the model in the modeling part.

39
00:05:34,190 --> 00:05:39,019
So so that's type of question. And second question is about simulation.

40
00:05:39,020 --> 00:05:47,840
You know, you learn that these solve in the homework one you know this how to produce serum all the serum all and so on so forth.

41
00:05:48,410 --> 00:05:53,000
And I think did terrifically for that question then, you know, so,

42
00:05:53,000 --> 00:06:04,579
so there will be some kind of our coding for that type of very ideal, you know, this, you know, most comparable model.

43
00:06:04,580 --> 00:06:07,700
And there's one question about simulation there.

44
00:06:07,850 --> 00:06:17,390
And so the simulation question or simulation study question or something like that are mostly based on this motor compartment model.

45
00:06:18,200 --> 00:06:30,109
And then we come up with this. So the the data analysis question that very similar to your, you know, the question you did for homework too,

46
00:06:30,110 --> 00:06:34,700
because that you you spend a lot of time working on data preparation,

47
00:06:34,700 --> 00:06:40,950
data plotting and and then analysis and the thorough additional question you can ask there.

48
00:06:41,000 --> 00:06:54,139
And so so those are the things like I want sort of ask in the midterm because I know that some of you work as a group is in some

49
00:06:54,140 --> 00:07:05,180
form and I want you sort of show me that individually how you would answers questions and so that that's the structure of the beta.

50
00:07:05,930 --> 00:07:13,080
So so my estimation, the first question will be take like 30 minutes, 40 minutes or so to derive it.

51
00:07:13,350 --> 00:07:17,950
So second one project. $1 and 31 may take another one hour.

52
00:07:18,160 --> 00:07:22,150
Given that you already process data, data, is there a you don't need to really process data.

53
00:07:22,390 --> 00:07:31,180
You only need to read data into hour and maybe you take a little bit more time and like double the time.

54
00:07:31,210 --> 00:07:34,810
I totally will not accept like three or 4 hours.

55
00:07:36,830 --> 00:07:46,600
Okay. So anyway, you can arrange a time any time of the 24 hours period that to to to do that.

56
00:07:48,310 --> 00:07:56,020
You can give me the written sort of form of the solutions or if you want to give us, you know, to most of you.

57
00:07:56,440 --> 00:07:58,000
Tidy latex is beautiful.

58
00:07:58,060 --> 00:08:07,780
That's very nice to see that you are able to produce the solution is latex, but if handwritten solutions are fine as long as they're legible.

59
00:08:09,040 --> 00:08:20,050
And if you have, you know, the tables and figures and some are cold, you should put that into supplemental material.

60
00:08:20,560 --> 00:08:27,630
You just give me the solution and the key figures that you use or tables that used to provide us solutions.

61
00:08:27,640 --> 00:08:40,550
If you have some additional things like articles or or some, you know, supplementary material you want me to see and just put in appendix, okay?

62
00:08:41,110 --> 00:08:47,919
And so that I can grade your homework with your main findings.

63
00:08:47,920 --> 00:08:54,010
And if necessary, I will go back to make a reference to your supplement to mature, to understand more details.

64
00:09:00,280 --> 00:09:08,470
As I said, that you need a little time to work on the both simulation study question and the data analysis question,

65
00:09:08,770 --> 00:09:12,669
but not very time consuming, given that you have to do your homework.

66
00:09:12,670 --> 00:09:19,140
Number two. Yeah. So which lecture members because of midterm?

67
00:09:20,580 --> 00:09:24,200
Oh, I should say that there's no comma filter in the question. Yeah.

68
00:09:24,750 --> 00:09:27,780
So. And the others would be fine. They're all included.

69
00:09:31,680 --> 00:09:34,760
I think those questions will not surprise anyone.

70
00:09:34,770 --> 00:09:40,320
Still, as a little extension from your home or a little bit to the variant from your home.

71
00:09:40,500 --> 00:09:43,829
I cannot give you exactly one word questions for you solving the midterm.

72
00:09:43,830 --> 00:09:51,360
But and the purpose of this is really want to see if you work on those problems independently.

73
00:09:51,750 --> 00:10:04,170
How you identify your sort of places that you may have some of the, you know,

74
00:10:04,530 --> 00:10:12,240
the understanding or some some place that you feel that you need a little bit more review or study.

75
00:10:12,390 --> 00:10:16,740
It's open book exam. You can Google everything you use.

76
00:10:16,740 --> 00:10:22,870
And so but of course, Googling will take time to complete.

77
00:10:23,130 --> 00:10:28,920
QUESTION But you can you can use whatever whatever resources are available for you to do it,

78
00:10:30,060 --> 00:10:37,470
but not just do not talk to each other because they're supposed to be complete, be done independently, write individually.

79
00:10:38,430 --> 00:10:41,790
Any other questions as is that clear that what you're going to face.

80
00:10:42,570 --> 00:10:46,979
And and I don't think you need more than 24 hours.

81
00:10:46,980 --> 00:10:50,480
So if you do need it then I mean though, right. So I don't think so.

82
00:10:50,490 --> 00:10:56,850
Yeah. But is that is the process dead on canvas or is it going to be released with the exam together?

83
00:10:56,970 --> 00:11:01,080
Oh, it will be on campus. I will put the data in canvas.

84
00:11:01,080 --> 00:11:07,530
I haven't done that. You know, you will see the data at the time when I give you the questions.

85
00:11:07,730 --> 00:11:10,590
Okay. Yeah, yeah, yeah. Not now. Yeah.

86
00:11:12,390 --> 00:11:22,170
So so basically, I can give you the, you know, Excel type of data and also the our data if if you use the hard to do that.

87
00:11:25,820 --> 00:11:29,300
Clear. Any other questions? Okay.

88
00:11:35,090 --> 00:11:42,110
So that's the only exam for this. Later on uni we will do a final project for the course.

89
00:11:45,320 --> 00:11:50,600
Okay, so let me just finish up lecture nine and then.

90
00:11:52,760 --> 00:11:58,490
So then. But we'll start something new next week.

91
00:12:00,380 --> 00:12:18,140
So I'm asking about this comment, asking the question and this is the the estimation method that we discussed to deal with this sort of Hakamada.

92
00:12:18,140 --> 00:12:23,530
And the reason I already explained that the reason I want to spend, you know,

93
00:12:23,540 --> 00:12:33,259
very significant amount of time explaining this sort of dynamic model or space based model is that this is very related to reinforcement learning

94
00:12:33,260 --> 00:12:47,570
and this unlearning sort of paradigm where people try to build up some Markov process that can help people to make a sequential decisions,

95
00:12:48,290 --> 00:12:54,139
you know, so, so so that's the model structure we're looking at, right?

96
00:12:54,140 --> 00:13:08,060
So y people could be number of, you know, in fashion cases at a week t where I have data and then you have a late in process, right?

97
00:13:08,330 --> 00:13:12,559
So that is underlined in fascist status.

98
00:13:12,560 --> 00:13:26,120
And there's the Markov classes that this latent infectious situation evolves over time from past the future.

99
00:13:26,930 --> 00:13:40,280
Okay. Now you have the surveillance system is able to capture some of the, you know, the data from from the population.

100
00:13:40,280 --> 00:13:43,370
But so you have one model we use.

101
00:13:43,370 --> 00:13:53,860
They force them all to describe how this observed problems have been related to late in evolution of different places.

102
00:13:53,870 --> 00:14:03,499
And we also use the the Markov so model to model the development process.

103
00:14:03,500 --> 00:14:11,299
And in this slides I use this stationary article of two ultra ultra regressive order,

104
00:14:11,300 --> 00:14:16,910
one process, but you can extend that to use other type of Markov process.

105
00:14:16,910 --> 00:14:23,720
Basically, if Markov improper, did you see how this process can be related to the past?

106
00:14:23,910 --> 00:14:32,299
Right. So, so because we have a temporal process that the current value of the process,

107
00:14:32,300 --> 00:14:41,990
you can depend on some of the historical values and the Markov the property, the policies will allow you to formulate that dependance.

108
00:14:42,530 --> 00:14:49,579
So that's basically modeling structure and particularly we're interested to figure out at length in process.

109
00:14:49,580 --> 00:14:54,380
We're not just, you know, treat the delayed impulses as nuisance.

110
00:14:55,360 --> 00:15:00,290
We are not treating latent process as nuisance structure over time.

111
00:15:00,290 --> 00:15:05,090
All we really want to figure out what is the process, right?

112
00:15:05,480 --> 00:15:15,290
So as I say, that method, I'll treat the live in process and uses prior usage process as a sort of process that produced the

113
00:15:16,070 --> 00:15:24,760
temple dependance and the G itself does not really provide a way to estimate the aging process,

114
00:15:25,020 --> 00:15:27,559
the way to, you know, figure out the details.

115
00:15:27,560 --> 00:15:36,950
But if we really want to understand what's going on in the live in process in evolution of the in fashions of.

116
00:15:38,030 --> 00:15:41,300
Disease. Beyond what the cold beers can describe,

117
00:15:41,780 --> 00:15:50,590
then then you really can figure all this out of the way to figure out this estimation of the lethal processes to form a filter and kind of smoother.

118
00:15:50,600 --> 00:15:57,660
Right. So if you want to make a prediction of this, right.

119
00:15:57,690 --> 00:16:03,019
So that's that's denoted by name key in indication of common filter.

120
00:16:03,020 --> 00:16:07,220
You can estimate this five block. That's the new year on life.

121
00:16:09,920 --> 00:16:16,430
And if you want to use entire data to make a prediction this that we have and you start right.

122
00:16:16,880 --> 00:16:23,600
So which is also a block prediction, but now use conditional on the entire data.

123
00:16:23,870 --> 00:16:26,120
So this call comes, right?

124
00:16:29,960 --> 00:16:41,000
So you in addition to filter all deleted process, also we want to evaluate the factor of covariates like vaccination and time training.

125
00:16:41,000 --> 00:16:53,420
You have done that and h right. So so you feel after observe the cover process like AIDS and vaccination and also you

126
00:16:53,420 --> 00:17:01,370
make time to read later on your project and maybe you you even put the political stance,

127
00:17:01,370 --> 00:17:04,939
right. So the, you know, some of the you know,

128
00:17:04,940 --> 00:17:19,100
the the other things that you can put into the profile model to explain the observed of the the time source of deaths or mortality or infection cases.

129
00:17:19,430 --> 00:17:30,200
So here, when you put this, you also need or model using either not negative binomial or possible sort of regression model,

130
00:17:30,620 --> 00:17:35,320
you need to estimate the regression coefficients. I you know this by RFA, right?

131
00:17:35,690 --> 00:17:39,380
So that's the parameter that we need to figure it out right here.

132
00:17:39,980 --> 00:17:45,620
So how do you estimate this? You need to build out some estimation procedure to do that, right?

133
00:17:46,160 --> 00:17:52,549
So you have basically two sets of parameters that you need to estimate.

134
00:17:52,550 --> 00:18:02,940
Why is the RFA that's basically a set of association parameters that you need to estimate that are why you estimate the length of process, right?

135
00:18:03,020 --> 00:18:06,110
So that that's this thing you want to issue.

136
00:18:06,410 --> 00:18:12,950
But how do you do that? Well, I have introduced several of them.

137
00:18:13,760 --> 00:18:27,320
Now I just want to continue this argument. So one thing to do with estimation of this is that you can treat to be this data process, right?

138
00:18:27,380 --> 00:18:38,240
This live data process, because you observe them. You believe the existence of this Markov process that drives this fascist disease,

139
00:18:38,930 --> 00:18:46,820
which can now be directly observed but exist in a is is changing and evolving over time.

140
00:18:47,300 --> 00:18:52,370
So the way to to handle estimation, of course, is treating this as decent data.

141
00:18:56,500 --> 00:19:05,469
Right. This recent data may be, you know, collected if you're able to if you have them, oh, maybe Powerade or something.

142
00:19:05,470 --> 00:19:09,600
But we are human. We can observe and observing is required.

143
00:19:11,590 --> 00:19:18,399
So if you were to put this everything into one systematic formulation to do

144
00:19:18,400 --> 00:19:23,290
parameters diminishing according to poverty law suggests that by this model,

145
00:19:23,470 --> 00:19:32,110
this hierarchy model, we could, you know, take the point of view of treating these process as recent data.

146
00:19:32,590 --> 00:19:37,120
So in that case, that we need to come back to work on this argument.

147
00:19:37,120 --> 00:19:44,020
Likelihood, right? The ultimate likelihood base exists that you have your observed data, which is, you know,

148
00:19:44,830 --> 00:19:51,610
the the the wise right the whitest actually contact your evaluation of any system.

149
00:19:52,150 --> 00:19:55,830
And then you also have your C t which is not observed.

150
00:19:55,860 --> 00:20:01,330
You know, recent data like this together gives you augmented data.

151
00:20:01,900 --> 00:20:09,430
I think you all commit your data, but part of data is missing.

152
00:20:09,430 --> 00:20:16,809
But this is if you have this superpower like Spider-Man or somebody like that,

153
00:20:16,810 --> 00:20:21,640
that the Article two captures data, but we're not like that, so we are not able to do it.

154
00:20:22,060 --> 00:20:30,040
But if you have this something very powerful or like superpower, then you are able to attack.

155
00:20:30,190 --> 00:20:38,680
So basically you you build up your likelihood based on this augmented data, including the missing data as part of it.

156
00:20:40,360 --> 00:20:47,650
And then you know that then you write that your likelihood based on augmented data, then this is called ultimate likelihood, right?

157
00:20:48,490 --> 00:20:52,510
So this ultimate likelihood can be easily written as this form.

158
00:20:52,630 --> 00:21:02,350
And then so you have your joint distribution of Y as could this week end,

159
00:21:03,220 --> 00:21:14,140
which you can be decompose as your Y and give data end and times the joint distribution of data and right so that's that's something you can do.

160
00:21:14,140 --> 00:21:20,680
This is your argument data. So then this is the conditional data and this is your, you know,

161
00:21:21,310 --> 00:21:26,980
the joint distribution model late in process but give given is com structured and this kind of you written

162
00:21:26,980 --> 00:21:42,610
this product of quite t given see that he looks t from one to n okay because of the the the independence.

163
00:21:42,610 --> 00:21:48,999
So y and only depends on y it only depends on signal t it doesn't depend, obviously,

164
00:21:49,000 --> 00:21:58,329
that t and they have this condition independent for the second piece you have t from one to N seeded T and C,

165
00:21:58,330 --> 00:22:03,250
the T minus one because you have the first order Markov process.

166
00:22:04,180 --> 00:22:08,860
Right. So so this is due to the first of all, remarkable forces.

167
00:22:08,860 --> 00:22:14,290
This is due to condition independence. So if you take a lock, right?

168
00:22:14,300 --> 00:22:17,770
If you take a log, then you got the log.

169
00:22:19,810 --> 00:22:25,760
You know, the argument likelihood and this all the products become sum, right?

170
00:22:25,870 --> 00:22:34,269
So that essentially you get this sum of the log of likelihood of why if you give this data to RFA,

171
00:22:34,270 --> 00:22:44,320
this is coming from your possum model right you because you assume that like you follows a positive distribution condition thi that t

172
00:22:44,410 --> 00:22:55,330
right and here this you have log likelihood from your conditional karma distribution because you assumed 30 follows ar1 comma process.

173
00:22:56,650 --> 00:23:06,910
And this is actually initial position which is the starting power, you know, the randomness of this whole of mechanism.

174
00:23:07,030 --> 00:23:13,390
Right. So, so this is, you know, the initial position of your you facility, this model,

175
00:23:13,870 --> 00:23:19,480
whichever happens to be there and that you don't have data collected with this initial position.

176
00:23:19,840 --> 00:23:26,050
But this initial position initiates the whole of dynamics.

177
00:23:26,200 --> 00:23:36,190
You want them all. Okay, so, so of course the, the method that we deal with this recent data is the M algorithm.

178
00:23:36,700 --> 00:23:44,769
Now, of course, but you know, our LAN Kim there was he appeared early this summer and he had this Mm.

179
00:23:44,770 --> 00:23:50,230
Algorithm, right. So my algorithm is more powerful GM algorithm.

180
00:23:50,770 --> 00:23:55,750
And so, but, but anyway.

181
00:23:56,090 --> 00:24:00,310
So in this context, suppose you are going to use em outward then?

182
00:24:00,850 --> 00:24:04,989
And then the omega outward involves two steps, the right,

183
00:24:04,990 --> 00:24:14,350
the easy step and the cute step in the is that basically you need to calculate expectation word that would average off your log likelihood.

184
00:24:14,780 --> 00:24:24,609
Okay, so I'll reintroduce that. So so what you're trying to do here is in each step to basically marginalize your

185
00:24:24,610 --> 00:24:31,300
recent data part to get the like object function that only evolves what you observe.

186
00:24:31,570 --> 00:24:38,470
Okay, so that's this Q function in the EMI awkwardness, basically, you know,

187
00:24:38,600 --> 00:24:45,880
it's expectation under conditional distribution of your posterior distribution of missing data,

188
00:24:48,220 --> 00:24:58,210
conditional on your observed data and the currently known values of your model parameters.

189
00:24:58,690 --> 00:25:02,349
So those are for Prime and Zeta prime.

190
00:25:02,350 --> 00:25:08,770
Are values updated from previous iteration because you have iterative algorithm to solve it.

191
00:25:09,280 --> 00:25:18,490
So our for prime Zeta Zeta Prime are the the parameters of the updates from the previous iteration.

192
00:25:18,820 --> 00:25:23,740
So our PHA is the value from your program model Zeta is the parameter, right?

193
00:25:23,740 --> 00:25:26,760
And from your common native process. Okay.

194
00:25:26,920 --> 00:25:32,290
So you basically take the average where you integrate out the missing details,

195
00:25:32,290 --> 00:25:44,140
see the end from your augment likelihood with using this, you know, the best posterior you can have and the current iteration.

196
00:25:44,440 --> 00:25:47,139
Okay, so you create a new object function.

197
00:25:47,140 --> 00:25:58,960
Now you try to maximize this with respect to the parameter alpha and Zeta so that you can create a new update, okay?

198
00:25:59,080 --> 00:26:08,409
Of your parameter. And you just continue to do this using algorithm and then eventually you will get converges hopefully.

199
00:26:08,410 --> 00:26:16,750
Right? So, so next step after a good Q function are calculated that you want to maximize this object

200
00:26:16,750 --> 00:26:24,250
function with respect to parameters so that you can obtain new updates of in the M step.

201
00:26:24,880 --> 00:26:32,260
So then you can see that there's a parameter also carnality in this log likelihood,

202
00:26:32,560 --> 00:26:39,580
namely the parameters in the RFA in the first piece on the Evolve parameter RFA.

203
00:26:39,580 --> 00:26:54,610
And the second piece of this likelihood involves only a parameter of your game of and process that that's data so that when you want to, you know,

204
00:26:54,610 --> 00:27:04,230
maximize this likely function using, you know, the typical method of find root from the score equation, that's basically what you want do.

205
00:27:04,360 --> 00:27:16,620
This is the the one that you tried to deal with in the M step to M step is set up to maximize the Q function or right.

206
00:27:16,630 --> 00:27:23,410
But if you do little bit calculation, maximizing the Q function is equivalent to find the root.

207
00:27:23,980 --> 00:27:30,430
Okay, do the root of the parameters from this to score equations.

208
00:27:30,640 --> 00:27:38,950
Okay. So the first one is the one that takes the the way that you all for with from generalize in

209
00:27:38,950 --> 00:27:46,750
your model where you know this expect this expectation here is carried over to you know,

210
00:27:47,440 --> 00:27:51,120
marginalize your latent variable theta t okay.

211
00:27:51,610 --> 00:28:01,770
So in the grammar, you don't have this extra term expectation because you don't have the latent process now in the, in the model here.

212
00:28:01,780 --> 00:28:13,120
Right. So we do have a latent process set of T because you believe that the observed Y is driven by some unobserved latent process.

213
00:28:13,630 --> 00:28:25,290
So in this possible regression, right, you need to really deal with this offset term that will be calculated,

214
00:28:25,300 --> 00:28:31,000
will be replaced by its conditional expectation or this, you know.

215
00:28:36,600 --> 00:28:40,980
This the so of the blowup, right?

216
00:28:41,080 --> 00:28:45,399
So this the you know, this conditional expectation.

217
00:28:45,400 --> 00:28:55,230
You see that he you see that he's not observed. Right. So what do you want to do here is you replace this unobserved city by its expectation.

218
00:28:55,620 --> 00:28:59,010
This expectation is derived from this.

219
00:28:59,310 --> 00:29:08,100
Um, I with them, it's not something you made up because the algorithm requires the calculation of expectation.

220
00:29:08,700 --> 00:29:14,280
So this expectation actually comes with this expectation in the east, right?

221
00:29:14,880 --> 00:29:24,390
So this expectation carries over to the cup, to the estimation to M step in estimation.

222
00:29:24,390 --> 00:29:32,879
So here you need to do that. And then if you look how the scored equation for the Z top parameter in the latent process,

223
00:29:32,880 --> 00:29:40,709
it looks a very complex so that and I try to implement this is numerically

224
00:29:40,710 --> 00:29:50,550
very unstable so I just give up the you know the ML yesterday and and you know

225
00:29:50,760 --> 00:29:58,680
try to solve this Z to parameter in the slightly easier way using some consistent

226
00:29:58,680 --> 00:30:04,320
estimate you know rather than using this may lead directly from this.

227
00:30:04,620 --> 00:30:11,020
Okay so, so I did two modifications from it.

228
00:30:11,220 --> 00:30:18,090
Yeah. My algorithm first of all, I do not calculate the expectation of theta t c,

229
00:30:18,570 --> 00:30:25,380
I calculate a best near unbiased prediction of the C that t given what I have.

230
00:30:25,830 --> 00:30:28,830
Okay. So, so that's the one approximation.

231
00:30:28,830 --> 00:30:38,610
I took a comma estimate function. So in principle I need to calculate expectation of theta t given, you know,

232
00:30:39,120 --> 00:30:48,150
the data and updated values which requires to work out the conditional distribution of this, which can be complex.

233
00:30:49,050 --> 00:30:53,430
So you need to work out this distribution in order to calculate expectation.

234
00:30:54,030 --> 00:31:06,090
So what I did here is in the comma estimate function is that if I can be a lazy person, which we can be lazy optimally, right.

235
00:31:06,240 --> 00:31:13,200
I don't want, I don't want to calculate that. So basically I don't want to calculate is joint distribution is so complex.

236
00:31:13,980 --> 00:31:20,460
So can I take a step back to do something reasonable but computationally fast?

237
00:31:21,270 --> 00:31:26,010
Okay, that's my so so in principle I need to calculate expectation of this.

238
00:31:26,400 --> 00:31:28,170
I know that this calculation,

239
00:31:28,170 --> 00:31:37,260
first of all analytically very difficult to derive because you have to derive your joint distribution of the latent process or your recent data.

240
00:31:38,520 --> 00:31:44,940
So what I want to do here is why not? It takes something simple but fast, but good enough.

241
00:31:45,180 --> 00:31:51,629
Okay, so what I did here is that's a little creation, creativity.

242
00:31:51,630 --> 00:31:58,770
My work is that when are you replaced there by a best Nina unbiased predictor.

243
00:31:59,280 --> 00:32:04,980
So what I want to approximate this exact patient is among all the linear functions.

244
00:32:05,880 --> 00:32:09,000
What is the best one I could use? Okay.

245
00:32:09,000 --> 00:32:20,520
When I think about this approximation in the context of the best in Nina on bias prediction that Amy

246
00:32:20,520 --> 00:32:27,989
the sense this common filter come smoother that can help me to do this recursive optics which is very,

247
00:32:27,990 --> 00:32:31,230
very fast computation complexity is just linear.

248
00:32:31,800 --> 00:32:42,330
So you can finish this less than 1/2 to minute calculation so that this that I your original proposal and secondly

249
00:32:42,330 --> 00:32:50,130
this one looks at me and I don't think it works both you can go to work out all the details but numerically unstable.

250
00:32:51,570 --> 00:33:00,090
So that's not the way the one I really want have something numerically very stable, you know.

251
00:33:00,090 --> 00:33:03,389
So why people like linear regression model?

252
00:33:03,390 --> 00:33:06,600
Because the regression model is very stable, right?

253
00:33:06,600 --> 00:33:16,590
So so you really want to create a statistical method that are numerically very stable, reproducible.

254
00:33:17,670 --> 00:33:22,110
There are a lot of fast and massive like every year maybe the Hungary's a Hungary's that this

255
00:33:22,110 --> 00:33:29,820
massive proposed literature and only those methods that are numerically very stable survive.

256
00:33:29,830 --> 00:33:34,970
And if you look at all the you know sort of the of the.

257
00:33:35,360 --> 00:33:39,139
You'll learn so far they're all very numerically stable.

258
00:33:39,140 --> 00:33:47,990
So, you know, you can propose fast method, but end of day your message will not be used in practice because is lacked.

259
00:33:48,290 --> 00:33:55,699
It lacks reproducibility. If you change data then a little bit have little bit perturbation in the data,

260
00:33:55,700 --> 00:34:01,100
then the results are very different that then you know that this is not good method.

261
00:34:01,700 --> 00:34:05,179
So so what I want I try this, I do simulation.

262
00:34:05,180 --> 00:34:19,249
I found that this solving this is non numerical stable so that I take a step back and try to, you know, find something simple but numerically stable.

263
00:34:19,250 --> 00:34:29,990
I will lose a little bit of estimation efficiency, meaning I need to liberate larger sample size in order to reach the same statistic power.

264
00:34:30,530 --> 00:34:42,410
But I think that sacrifice is needed because numerical stability is seems to be a my priority in this complex data analysis so that I,

265
00:34:42,950 --> 00:34:45,079
I did the experiment first.

266
00:34:45,080 --> 00:34:56,450
I replace this by common field or I found that if you replace this by common snoozer, it's actually better than common filter.

267
00:34:56,690 --> 00:35:02,750
So that's why I proposed the comma estimate function.

268
00:35:03,200 --> 00:35:14,700
It's not something I provide. The final solution in one shot, I tried a different ways and, you know, evaluate their stiffness,

269
00:35:14,930 --> 00:35:20,479
efficiency, numerical stability and so on, so forth, and make a compromises here, there.

270
00:35:20,480 --> 00:35:25,460
And finally, I found that this is the most desirable solution.

271
00:35:25,760 --> 00:35:30,800
Namely, I solve my RFA use and commas smoother.

272
00:35:31,250 --> 00:35:34,670
Okay. And that gives me quite a stable result.

273
00:35:34,670 --> 00:35:43,670
In the same time, I estimate the Zeta parameters from lead process using a method moment estimate rather than likelihood estimate.

274
00:35:44,450 --> 00:35:57,290
So this moment estimate no no guarantee this would inconsistency so that still not that not you know, you still have the convergence rate,

275
00:35:57,290 --> 00:36:04,249
but the efficiency is a little bit lower than MLG, but the precision is a bit lower than MLG,

276
00:36:04,250 --> 00:36:08,750
but it still gives you the same, you know, a convergence rate.

277
00:36:08,750 --> 00:36:16,100
So that's something I think is quite a you lose something, but not really a disaster, basically.

278
00:36:16,100 --> 00:36:21,840
So that in a literature, people have been doing this all time, this idea it.

279
00:36:21,900 --> 00:36:32,360
Yes, well, as a matter of fact. So you treat this role and lambda as nuisance parameter estimated by the method

280
00:36:32,360 --> 00:36:37,759
moments and plucking and you use your common smoother to solve this question.

281
00:36:37,760 --> 00:36:44,990
And when you solve this one, it becomes very straightforward because this is just a Poisson regression process.

282
00:36:45,000 --> 00:36:50,329
Allow the NINA regression with an offset term calculated by coming smoother.

283
00:36:50,330 --> 00:36:58,760
So this can be use dear and you know our package to solve it quickly with this offset term.

284
00:36:59,150 --> 00:37:02,720
So the estimation Marlborough becomes very numerically stable.

285
00:37:03,290 --> 00:37:12,529
Now since I did a couple of the approximation or simplification from my likelihood,

286
00:37:12,530 --> 00:37:24,950
I start with a standard likelihood formulation using M algorithm, but I just did some approximation or some simplification for this in the M step.

287
00:37:26,240 --> 00:37:34,370
And so I lose actually the the actual the exact likelihood formulation because I

288
00:37:34,370 --> 00:37:40,909
replaced this supposed to be expectation of theta t conditional the all distribution.

289
00:37:40,910 --> 00:37:53,270
But now I just approximate comments smoother, which is not exactly same as this is only the best approximation about all the minor predictor.

290
00:37:53,600 --> 00:38:00,620
So that's the compromise I took in order to reach numerical efficiency.

291
00:38:01,310 --> 00:38:06,770
So. So, technically speaking, I no longer have a score equation here.

292
00:38:07,070 --> 00:38:14,389
I only have a estimating equation so that there are some efficient asymptotic covariance matrix.

293
00:38:14,390 --> 00:38:18,620
It's no longer fishery information because you know that only efficient information,

294
00:38:19,430 --> 00:38:25,909
only likelihood method will give you the efficient information as asymptotic covariance matrix.

295
00:38:25,910 --> 00:38:34,970
Now I chance this score equation score a function into a estimating function due to the use of a smooth.

296
00:38:35,030 --> 00:38:38,840
Ah, so that I would have to damage information.

297
00:38:39,890 --> 00:38:45,980
So essentially this one hour for hat by a key comma estimate,

298
00:38:45,980 --> 00:38:58,000
the function I create here will converge at some product we to a goals and distribution with this sandwich form that's called you

299
00:38:58,010 --> 00:39:09,350
know go and the information where this is this the expectation of mine is of the first order derivative of your estimated function.

300
00:39:09,350 --> 00:39:20,659
You and V is the variability matrix which takes the expectation the outer product of this score function of this estimate function.

301
00:39:20,660 --> 00:39:26,880
So, so this is the goddamn information, right? You have seen this in G g.

302
00:39:26,920 --> 00:39:34,159
You is also estimating functioning. It does not give you a visual information.

303
00:39:34,160 --> 00:39:43,670
And for this some part covariance you will have a sandwich form when this you the you is a score function.

304
00:39:43,820 --> 00:39:48,050
We know that the of identity that s and V are the same.

305
00:39:48,560 --> 00:39:53,240
So this is proving there are six l to your six or two, right?

306
00:39:53,540 --> 00:39:57,350
If you is a score function, right.

307
00:39:57,410 --> 00:40:03,020
First of all, the derivative, the log likelihood, then this sensitivity and variability are the same.

308
00:40:03,710 --> 00:40:08,090
That's proving you are six or two. Okay, this is called Bartlett identity.

309
00:40:08,690 --> 00:40:15,680
So if you is likely to function, then this s and a v can be canceled because there are the same.

310
00:40:16,280 --> 00:40:22,520
So that you only have this fisher information right as you are some type of covariance matrix.

311
00:40:22,820 --> 00:40:36,200
But in general, in general you this s and we are now the same when you move away from the likelihood method so that you have this sandwich form.

312
00:40:36,380 --> 00:40:53,420
And this is this was a found 1956 by Indian statistician GL Denby and so many people call this form like sandwich covariance matrix.

313
00:40:53,990 --> 00:41:01,129
But I told all my students that we should name this goddamn information to recognize his contribution.

314
00:41:01,130 --> 00:41:05,870
He's the first person he was a student. Fisher And he did his speech to this.

315
00:41:05,870 --> 00:41:13,939
FISHER Artificial. And he was the first person who discovered this sandwich form in 1956 paper.

316
00:41:13,940 --> 00:41:19,460
So I always respect his contribution. Call this goddamn information matter in sandwich form.

317
00:41:20,450 --> 00:41:24,950
And anyway, so that that's called any information.

318
00:41:24,950 --> 00:41:32,840
And I don't, I can now require other people to call this, but if you want to be my student, you have to use this name.

319
00:41:33,260 --> 00:41:40,370
So this basically a requirement to graduate from good and the information.

320
00:41:40,370 --> 00:41:43,970
Yeah okay just to make jokes who can remember it.

321
00:41:45,860 --> 00:41:51,739
So you in the my implementation I already give you are packaged right so you can actually

322
00:41:51,740 --> 00:41:57,710
calculate this goddamn information recursively according to common filter against smoother.

323
00:41:58,190 --> 00:42:01,640
So that's the how I did in my work.

324
00:42:01,760 --> 00:42:10,940
I also want to calculate this, you know, the sandwich form, the essence of the matrix and a very bit of matrix using you know,

325
00:42:11,000 --> 00:42:19,729
the, you know the recursive relationship are created by computer a smoother okay I work very hard.

326
00:42:19,730 --> 00:42:22,040
Lots of you can say a lot of things,

327
00:42:22,040 --> 00:42:31,189
but I don't think that's very important because I feel that it's a little bit tedious but has to be up in my our packet, our function.

328
00:42:31,190 --> 00:42:38,179
I already passed to you. Right. But I think that I mean, okay, let's let's skip that.

329
00:42:38,180 --> 00:42:44,180
Okay. So how do you estimate the the the the parameters from your latent process?

330
00:42:44,180 --> 00:42:53,120
You can use method moments, right? So and so that's what I did there for the.

331
00:42:54,080 --> 00:42:55,540
So first of all, your estimate,

332
00:42:55,540 --> 00:43:05,500
this dispersion parameter lambda using the method moments and then you estimate the autocorrelation function using the method moment.

333
00:43:05,900 --> 00:43:17,360
Okay, so just use moment, right. And like you it so that that's what I did and, and I implement it.

334
00:43:19,490 --> 00:43:26,240
So how do you, how do you prove this thing? Okay, Row, there's this course.

335
00:43:28,100 --> 00:43:31,280
So I probably did too little approve of this.

336
00:43:31,820 --> 00:43:35,420
This is harder one to get. Well, let's prove it.

337
00:43:37,760 --> 00:43:42,140
Okay. Just for fun, because this is a level course.

338
00:43:42,150 --> 00:43:55,910
Maybe you will see the problem in the qualifying exam. Who knows? Okay.

339
00:43:56,240 --> 00:44:00,420
So I want to estimate the alt operation parameter.

340
00:44:00,420 --> 00:44:09,079
This, you know, the condition parameters of the operation parameter from the latent process in the lithium process using method moment.

341
00:44:09,080 --> 00:44:23,300
So I need to create in a moment. Okay. So first that I calculated covariance of common field or m t minus one and then t right and so that.

342
00:44:23,750 --> 00:44:28,040
So let me just look at, I have a recursive form, right?

343
00:44:28,040 --> 00:44:33,950
So let me just look at what's the expressed expression of my common future.

344
00:44:34,370 --> 00:44:43,370
So right here, equation one, right? So equation one here is the expression of my common future.

345
00:44:44,810 --> 00:44:52,820
So I have my what role am t minus one plus low bar, which is one -0.

346
00:44:52,820 --> 00:44:57,290
Right? And plus c, t and the y t.

347
00:44:57,440 --> 00:45:00,470
This is innovation. All prediction, right?

348
00:45:01,130 --> 00:45:05,150
And empty minus one.

349
00:45:07,280 --> 00:45:17,120
Okay, so. So I have rho covariance of empty minus one, m t minus one.

350
00:45:18,230 --> 00:45:24,340
And then I have the second term constant and empty minus one to call my filter from previous

351
00:45:24,350 --> 00:45:28,700
that this should be zero because covariance of constant any random variable is a zero.

352
00:45:29,870 --> 00:45:33,650
And then I have c t coherence.

353
00:45:33,800 --> 00:45:41,760
CTE is the of the variance of your common filter empty covariance of and.

354
00:45:43,740 --> 00:45:51,420
Y t minus one y, t, minus f, t and MP minus one.

355
00:45:52,710 --> 00:46:00,570
Right. So this role is the variance of an T minus one.

356
00:46:02,400 --> 00:46:07,890
Right. So that's basically the equation I have here.

357
00:46:08,220 --> 00:46:14,480
Right. So what is the variance of this?

358
00:46:14,560 --> 00:46:18,650
Okay, let me just. Just copy this stuff. So what is this?

359
00:46:18,680 --> 00:46:25,070
This will be a row stage minus one, because, you know, safety is defined.

360
00:46:25,580 --> 00:46:30,650
Safety is defined as the deviance of your common future.

361
00:46:31,040 --> 00:46:34,550
So variance of command influence should be rule seats minus.

362
00:46:34,880 --> 00:46:38,960
Okay. So look at the second term here.

363
00:46:40,310 --> 00:46:44,120
And this should be zero. Why?

364
00:46:44,120 --> 00:46:57,600
This is zero? Well. Why this is zero coherence of innovation, prediction error and m t minus one equals zero.

365
00:46:58,230 --> 00:47:11,880
Why? Well, this is because that on your back to this the you know the argument right here.

366
00:47:12,390 --> 00:47:18,180
So you can write this as expectation for coherence of life.

367
00:47:20,760 --> 00:47:25,710
This beautiful formula gave them Y and minus.

368
00:47:28,460 --> 00:47:31,580
I think what I have. Yeah. T minus one.

369
00:47:32,030 --> 00:47:35,089
So I created a soccer match bra, you know.

370
00:47:35,090 --> 00:47:51,200
And of course that causes of expectation of y team has given me back to her from y1yyt minus one and expectation of and two minus one give.

371
00:47:54,530 --> 00:47:59,120
Okay. And the first part is zero.

372
00:47:59,810 --> 00:48:07,970
Why this is zero? Well, because Whitey is given two minus one is given that this way is a constant.

373
00:48:11,100 --> 00:48:17,669
So so you continue on covariance is zero because y to give it your common field or at the end time t minus

374
00:48:17,670 --> 00:48:25,950
one is completely determined by the information of your y t minus one if y t minus if the data from y y to y,

375
00:48:25,950 --> 00:48:29,610
t minus one time are fixed, right?

376
00:48:29,730 --> 00:48:32,760
Any function of that data should be fixed.

377
00:48:33,090 --> 00:48:41,310
So this is a constant when your data are fixed, so constant of any random variable should be zero.

378
00:48:41,700 --> 00:48:46,500
So that's zero. And for this one, right.

379
00:48:47,490 --> 00:48:55,080
So why this is zero and and that's because the the expression of your f t minus one.

380
00:48:57,450 --> 00:49:08,930
So let's back to this. So.

381
00:49:16,500 --> 00:49:31,440
So the f t what is the f t so so the f t here right after is the prediction of your white t give them all the data at y t minus one.

382
00:49:33,870 --> 00:49:47,189
Right? So if you write this out like this should be zero y, this is zero because that's how f ts defined f t is the prediction of your y team.

383
00:49:47,190 --> 00:49:53,040
Give them t minus one. Okay, so that's a zero because that's all I have left.

384
00:49:53,370 --> 00:50:00,480
The prediction is given. So you're trying to use the data up to T minus one to predict a white t.

385
00:50:01,350 --> 00:50:10,640
That's how you this this. Right.

386
00:50:10,940 --> 00:50:16,130
So. So this can be a briefing that's like given, like, t minus one.

387
00:50:16,580 --> 00:50:20,540
And I. This is how you define.

388
00:50:22,580 --> 00:50:29,030
Right. So this is zero. Okay. So that's why you. You calculate this zero and.

389
00:50:37,280 --> 00:50:42,970
And then you can you can derive this whole thing like roads.

390
00:50:43,370 --> 00:50:55,220
So finally you have this formula, the this covariance of MTM to isolate equal to row of c t minus one.

391
00:50:56,340 --> 00:51:00,620
Okay. So that's basically the, the moment condition that used to estimate a row.

392
00:51:00,710 --> 00:51:14,690
Okay, that's easy. So now so I during the implementation that is is possible that adopted value of Sigma Square for the two.

393
00:51:14,960 --> 00:51:20,540
I mean the the lambda parameter, this index parameter or the this one over lambda is used.

394
00:51:21,500 --> 00:51:32,010
Dispersion parameter can fall outside of their admissible values because we use method moment method so that the estimate.

395
00:51:32,030 --> 00:51:35,060
So the dispersion parameter has to be positive.

396
00:51:35,360 --> 00:51:38,000
The row parameter has to be from 0 to 1.

397
00:51:38,510 --> 00:51:48,590
Sometimes that, you know, because of the, you know, small sample size word instability of instability of your numerical procedure,

398
00:51:49,010 --> 00:51:56,870
you know, you use duration to search the result that may not give you a legitimate value in updates.

399
00:51:58,100 --> 00:52:07,250
So when those adaptive values for outside of their admissible values, then this whole, you know, algorithm stops.

400
00:52:07,580 --> 00:52:13,160
Okay? So you will get a sort of warning message out of this updating and.

401
00:52:13,700 --> 00:52:22,640
Well, so you were to overcome this issue, you could make a transformation like in the estimation.

402
00:52:22,760 --> 00:52:26,329
Okay. That's basically the idea of G here. M Right.

403
00:52:26,330 --> 00:52:29,540
So in the gram, you do not directly model new.

404
00:52:29,870 --> 00:52:34,720
You model a transformation like in the logistic regression, right?

405
00:52:34,730 --> 00:52:39,260
You do not really direct to model the probability as a function of covariance.

406
00:52:39,770 --> 00:52:48,970
You use logit of your probability as a function of covariance that's essentially trying to relax.

407
00:52:49,670 --> 00:53:04,499
Right? So in the U.S. around the fundamental idea of using link function is just transform your parameter to a region of minus infinite paths,

408
00:53:04,500 --> 00:53:10,860
an infinity that you never into the situation of outside of admissible range of parameter, right?

409
00:53:11,580 --> 00:53:15,629
So you have probability P which of course is between zero one.

410
00:53:15,630 --> 00:53:21,360
That's just like a my role here. Rho is the correlation parameter between zero one.

411
00:53:21,480 --> 00:53:30,150
Okay. So you do not directly model the probability of success in logistic regression model as function cause because corvids age,

412
00:53:30,630 --> 00:53:33,690
everything else could be really very everywhere.

413
00:53:33,690 --> 00:53:36,970
You cannot control those variables to be certain.

414
00:53:36,990 --> 00:53:48,420
Rich Right. So you work to do that. You use no major transformation of the probability that you you create a to the your regression.

415
00:53:49,170 --> 00:53:53,360
When you look at this low, your transformation, this becomes minus infinity,

416
00:53:53,370 --> 00:53:59,820
thousand infinity so that you overcome the problem you'd never run into problem anymore.

417
00:54:00,570 --> 00:54:06,510
Okay, so you can use automated you function but at the league function but going to function is most popular one.

418
00:54:06,840 --> 00:54:16,920
Why you want to do that technically speaking is that do you really want the estimate of parameter to be free of constraints?

419
00:54:17,940 --> 00:54:21,870
You can use operating function. Nobody will say you have to use 40 allowed.

420
00:54:21,870 --> 00:54:30,120
You has a a interpretation right based on access, but you can use other ways to do that.

421
00:54:30,420 --> 00:54:40,860
So here are similar idea. You know that the parameters are confined with certain space other instead of doing this constraint optimization

422
00:54:41,130 --> 00:54:46,500
to make sure that the estimate of values so arbitrary values are falling to the legitimate regions,

423
00:54:46,920 --> 00:54:53,400
you just make transformation. So for the sigma you can do log transformation.

424
00:54:53,670 --> 00:55:00,340
And you know, you, you, you, you re promoted Sigma Square by Log Sigma Square.

425
00:55:00,360 --> 00:55:12,209
Okay. So this course, if you use E to ask Log Sigma squared, then you estimate ITR, it's fine.

426
00:55:12,210 --> 00:55:21,330
It can vary from minus to infinity. So after you get your ether estimated sigma square of exponential it high, right?

427
00:55:21,330 --> 00:55:27,600
So you just do reprioritization in the estimation another one for the road that you can use.

428
00:55:27,600 --> 00:55:37,800
This features z transformation in this form that well guarantee that you have no constraint anymore in estimation, basic constraint free.

429
00:55:38,290 --> 00:55:50,880
Okay, so that's what I did and in the implementation and calculating called and information is not attractive,

430
00:55:51,210 --> 00:55:59,190
which would be replaced by bootstrap method using this random weighting bootstrap method of this.

431
00:55:59,190 --> 00:56:03,690
I never explored this before, but I always want to do this.

432
00:56:05,760 --> 00:56:10,530
And the my time that the computing power was very limited.

433
00:56:10,920 --> 00:56:20,750
So bootstrap takes a lot of computing power. We at my time there was just beginning of peril computing not as popular as now because

434
00:56:20,760 --> 00:56:25,470
now we have so much computing resource that you can use to do paralyzed computing.

435
00:56:26,040 --> 00:56:31,170
When I was on my time that we only have very, you know,

436
00:56:32,250 --> 00:56:40,020
crap computer and that's one to the yeah we have to live with those limit the computing resource and so that's

437
00:56:40,020 --> 00:56:45,660
why you force us to do a lot of analytic war trying to get a closed form expression as possible nowadays.

438
00:56:46,260 --> 00:56:53,790
I mean, people can rely on the, you know, to supply a powerful computing resource and power to,

439
00:56:54,570 --> 00:56:59,400
you know, do a lot of more a computational based driven method.

440
00:56:59,700 --> 00:57:10,500
So bootstrap is one way to do this. Now, here it faces a little issue in the bootstrap because you only have a single time series.

441
00:57:10,800 --> 00:57:13,800
So how do you do sample resampling?

442
00:57:13,820 --> 00:57:16,980
This replacement is not that easy.

443
00:57:17,520 --> 00:57:29,250
So what idea to do bootstrap estimation is, you know, using this so called random wave in bootstrap, which is different way to do bootstrap.

444
00:57:29,340 --> 00:57:36,960
Let me just give you a little bit introduction and tell you why this is relevant to the method that we can implement this.

445
00:57:37,110 --> 00:57:42,780
I've never done this before. Okay, so if you want to try, you can try that maybe in homework three.

446
00:57:42,780 --> 00:57:54,090
We can try it. Okay. It's a pretty easy to implement so is very tedious to derive and computer code can be information to sandwich covariance you.

447
00:57:54,170 --> 00:57:57,680
Or to obtain standard errors. Estimation part is very easy.

448
00:57:57,830 --> 00:58:02,570
Com a smoother is easy. As a leaner, meaner complexity.

449
00:58:02,600 --> 00:58:08,239
It's very fast to get it result and after you get come a smoother you just do the law

450
00:58:08,240 --> 00:58:13,490
continue regression using gear and function with the offset term very fast to get estimate.

451
00:58:14,090 --> 00:58:19,969
So point estimation is not that issue. Very stable, very fast, very robust, very reproducible.

452
00:58:19,970 --> 00:58:27,320
This key is so it is these so-called and information sandwich form as you see that is very evolved.

453
00:58:27,650 --> 00:58:36,140
You need to pull I quite a bit. But if you use bootstrap, you just repeatedly produced an estimate using different samples,

454
00:58:36,440 --> 00:58:45,440
then you can use you can utilize the computing power to bypass the analogy to replacing of a sandwich form for that information.

455
00:58:45,440 --> 00:58:51,980
Okay. So as I said, it is hard to implement the bootstrap resampling method.

456
00:58:52,010 --> 00:59:01,160
Full time source data typically is done by using block wise bootstrap method in order to preserve the zero dependance in resample data.

457
00:59:01,550 --> 00:59:11,090
So in times of data, one key feature that you need to preserve in the resampling is the of the temporal dependance, right?

458
00:59:11,090 --> 00:59:15,140
So, so data are independent, are not independent.

459
00:59:15,380 --> 00:59:18,380
So you cannot randomly randomly shuffle the data.

460
00:59:18,680 --> 00:59:22,730
You have to preserve that temporal pattern.

461
00:59:23,060 --> 00:59:33,980
We do re sampling. So it seems to me that the random wading bootstrap seems to be a straightforward and appealing to implement the key.

462
00:59:34,520 --> 00:59:37,820
Okay, so this idea is originally from Dan Rubin.

463
00:59:38,090 --> 00:59:44,149
Okay, but I don't know why this idea is largely ignored in the literature.

464
00:59:44,150 --> 01:00:00,350
I feel this is quite a nice method, particularly when you have a hard time to directly sample from subjects in in the post trap method.

465
01:00:00,930 --> 01:00:12,079
Okay. I don't know if the random weighting recently means that works in the context of estimate inference this time.

466
01:00:12,080 --> 01:00:15,440
Sorry, data. Nobody has ever investigated this.

467
01:00:15,500 --> 01:00:18,770
I think maybe I'm not aware of that literature.

468
01:00:20,540 --> 01:00:28,880
But anyway, I. I introduced you to this and then then you can see it makes sense for now.

469
01:00:28,970 --> 01:00:33,470
Okay. So how do you implement the random wave in bootstrap in the key?

470
01:00:34,310 --> 01:00:39,020
So. So here. Here is my result.

471
01:00:39,020 --> 01:00:42,799
A w t this is really just the comma, the function, right?

472
01:00:42,800 --> 01:00:49,820
So you do this log in in a regression with the offsets defined by your commas smoother, right?

473
01:00:50,090 --> 01:00:56,240
So that you solve this equation to, to often ask the point estimate.

474
01:00:56,690 --> 01:01:02,360
That's all you need to do. Now, what I need to do here is to find different types of solution.

475
01:01:02,410 --> 01:01:09,050
However, by shuffling my data points, that's a basically the reference bootstrap idea.

476
01:01:09,530 --> 01:01:17,600
Okay, so here just for simplicity, I fix this comma, zipper, add hat and.

477
01:01:18,720 --> 01:01:29,180
Hmm. Okay. And notice that when w t is one, there is no perturbation for this piece.

478
01:01:29,720 --> 01:01:34,520
Then. Then you basically have the key.

479
01:01:34,940 --> 01:01:43,219
Okay, so I'm set. Now, what I want to do here is I generate multiple different types of estimate of

480
01:01:43,220 --> 01:01:47,930
my RF commingled to generate multiple different types of estimate alpha I,

481
01:01:48,890 --> 01:01:52,540
you know, randomly seemingly weedy. It's more like proportions.

482
01:01:52,640 --> 01:01:59,150
I mean, I think Don Rubin has worked quite a bit, this propensity score and waiting, waiting, waiting, waiting, waiting.

483
01:01:59,180 --> 01:02:02,329
I mean, imputation method is also a weeding method.

484
01:02:02,330 --> 01:02:16,760
Right? So anyway, so you, you instead of putting this as one, you are, you know, preserve this weight w t according to certain distribution.

485
01:02:17,630 --> 01:02:22,250
So you let this w1wn okay.

486
01:02:23,060 --> 01:02:35,299
So to be I have the random samples from a distribution is mean and variance one suppose like you randomly simulate w one to w and from exponential

487
01:02:35,300 --> 01:02:39,380
distribution this parameter one which guarantees that you have a random

488
01:02:39,980 --> 01:02:43,430
distribution of a distribution of random variable this meaning and variance one.

489
01:02:43,580 --> 01:02:52,590
Okay, that's a requirement then for each of this a b this n dimensional value you you can solve full.

490
01:02:54,000 --> 01:03:04,290
Then you recycle. I mean, this idea. Repeat this procedure, then generate another set of w1w and from exponential distribution, this params are one.

491
01:03:04,590 --> 01:03:08,390
You'll get another set of w t. There you estimate again.

492
01:03:09,390 --> 01:03:16,230
Okay, now you do the same thing. Okay. So every time that you have a different B you can repeat.

493
01:03:16,380 --> 01:03:20,970
Had to be many times. Okay. Get it.

494
01:03:21,780 --> 01:03:30,180
Okay. So then for every fix a little B, you can obtain a corresponding estimate of our fun.

495
01:03:31,770 --> 01:03:43,830
Okay. Then you repeat this, and then what you need to do is really after generated B are many estimate that are fun.

496
01:03:44,280 --> 01:03:53,810
Then you can use the, you know, bootstrap standard deviation or 2.5% power, 97.5%.

497
01:03:53,820 --> 01:03:57,900
How to construct your 95 confidence interval. Okay.

498
01:03:58,830 --> 01:04:02,040
And that's basically where you do inference. No question.

499
01:04:02,040 --> 01:04:08,969
Here is why this works. This is different from the way of re sampling from data points here.

500
01:04:08,970 --> 01:04:13,850
What they are trying to do here is you are not here.

501
01:04:13,860 --> 01:04:17,620
You are not doing any resampling on the data point.

502
01:04:17,640 --> 01:04:23,760
What they're trying to do here is really just, you know, a chance to read.

503
01:04:25,110 --> 01:04:31,230
But Donald said that this is equivalent to do the ransom recycling or data point.

504
01:04:31,710 --> 01:04:41,490
Okay. Why is that? Let me just give you an explanation. So this formulation is exactly the same as the way that you sample you are from data points.

505
01:04:41,490 --> 01:04:46,800
Resample your data points. Restrict this risk pace with replacement.

506
01:04:47,580 --> 01:04:52,170
Bootstrap idea. But what? You don't need to directly sample your data points.

507
01:04:52,650 --> 01:04:55,890
You just change the weight in such a way. Just this way.

508
01:04:56,250 --> 01:04:59,280
Okay. Let's just consider a very simple case.

509
01:05:00,870 --> 01:05:04,450
Suppose that you want to estimate your prime term. You are.

510
01:05:04,470 --> 01:05:08,340
Which is know that from the root of this equation. Is your sample mean?

511
01:05:08,820 --> 01:05:13,590
Right. So. So this is basically the beginning of what?

512
01:05:13,590 --> 01:05:16,860
How Gol's Gulf. Think about the estimation.

513
01:05:17,250 --> 01:05:23,190
All right. You want your do your your Ariel, your estimation, your observation.

514
01:05:23,190 --> 01:05:30,360
Parallel is zero. Okay. So this is basically the formula that Gauss meeting much 300 years ago.

515
01:05:31,170 --> 01:05:34,890
So he believes that sometimes the over measure the the new.

516
01:05:35,250 --> 01:05:39,390
Sometimes you'll know the lengths of your linguistic right.

517
01:05:39,510 --> 01:05:42,570
Sometimes the overestimate. Some of the other measure.

518
01:05:42,660 --> 01:05:47,940
Some of the over measure, sometimes mystery lengths of a will stick.

519
01:05:48,210 --> 01:05:55,620
So you hope that in the long run that this error it becomes zero zero because they cancel out.

520
01:05:56,400 --> 01:06:03,420
Okay. So this is what gauss set up. And so what's the the best the gas of the new sample average.

521
01:06:04,110 --> 01:06:12,330
That's the root. Right. You can easily solve so that later on the mean becomes the central statistic because

522
01:06:13,590 --> 01:06:18,570
people have talk about all the time of their the importance of sample use statistics.

523
01:06:18,700 --> 01:06:29,770
Okay. This will not. Hmm. Now, we also know that the sample variance, the gas of the sample medias sigma squared over time.

524
01:06:30,230 --> 01:06:36,780
Okay. So now suppose that I want to estimate this whole thing.

525
01:06:37,830 --> 01:06:42,630
I don't know. Sigma squared. I want to estimate the the. The variance of my estimation.

526
01:06:42,900 --> 01:06:49,740
So how you could do that? Okay. So one thing we do here is, you know.

527
01:07:02,060 --> 01:07:16,280
We can use bootstrap, right? The bootstrap this that I have my the estimate or write the estimate or as a function of my data.

528
01:07:16,280 --> 01:07:19,610
It must be a random variable. I should have a distribution. Right?

529
01:07:20,780 --> 01:07:25,640
So this is my distribution of export. We'll call sampling distribution 61.

530
01:07:26,420 --> 01:07:35,180
So what I want to figure out here is the variance, the virus of this stuff, that's something I try to figure out.

531
01:07:35,570 --> 01:07:44,020
So suppose this is the distribution of X bar. Now what I need to figure out is what is the variance of this distribution, right?

532
01:07:44,190 --> 01:07:51,920
Because if this is distribution of Max Bar, then I really want to figure out what is the the variance of this distribution.

533
01:07:56,760 --> 01:08:00,490
Okay. So what's the variance of of the distribution X bar?

534
01:08:00,520 --> 01:08:04,020
Right. That's equivalent to ask the question of variance of X bar.

535
01:08:04,560 --> 01:08:09,670
So how many good do you have the formula right. But suppose I don't know the formula.

536
01:08:09,700 --> 01:08:13,920
How am I going to do that? I want to draw samples from the distribution.

537
01:08:14,610 --> 01:08:18,510
Right. I draw random samples.

538
01:08:22,840 --> 01:08:31,340
From this distribution. Okay.

539
01:08:31,820 --> 01:08:37,219
So I want to draw samples from this distribution. It's not from the origin, the population distribution.

540
01:08:37,220 --> 01:08:41,750
This is sampling. I want to draw this data point from this distribution.

541
01:08:42,800 --> 01:08:49,200
So if I'm able to do that, then sample variance will be the solution.

542
01:08:49,220 --> 01:09:00,740
Right. Sample variance will be the estimate of my what I want.

543
01:09:00,860 --> 01:09:03,560
Right. I want to estimate the variance of my x bar.

544
01:09:04,100 --> 01:09:13,940
If I'm able to draw random samples from a distribution rights bar, the sample variance will be my estimate of variance of x bar career.

545
01:09:16,340 --> 01:09:20,060
Okay. Now, how do you how do you do all this?

546
01:09:20,090 --> 01:09:24,900
This is this big contribution from. How do you do all this?

547
01:09:25,580 --> 01:09:29,720
This. Well, he said that you can do the following thing, right?

548
01:09:37,380 --> 01:09:44,420
No. So how do you how do you do all that?

549
01:09:44,750 --> 01:09:51,380
Well, he says that if you are able to have this impersonal CPF.

550
01:09:52,670 --> 01:09:56,090
Right. So it the you know, you jump.

551
01:09:59,550 --> 01:10:02,700
All right, so I can go over one.

552
01:10:04,860 --> 01:10:08,730
So this is empirical CDF for data.

553
01:10:09,060 --> 01:10:12,120
Like you have data point that you can have empirical CDF.

554
01:10:12,540 --> 01:10:18,330
Let's use a step function which is approximation of the truth into the function of the see function.

555
01:10:18,480 --> 01:10:23,740
Of course you're gonna know that the underlying to community.

556
01:10:24,420 --> 01:10:28,200
But you please the data you can create this you purpose CDF.

557
01:10:28,650 --> 01:10:32,850
Okay. Now you can draw from this distribution your data.

558
01:10:33,840 --> 01:10:40,230
How you do all that? Well, you don't have any chance to draw anything here in between.

559
01:10:41,200 --> 01:10:46,290
That's the problem. You don't have probably much right to draw anything.

560
01:10:46,290 --> 01:10:56,940
I have zero chance to observe the data point between. You'll only have a chance to observe the data pointed right on the jumps.

561
01:10:59,130 --> 01:11:03,300
Right. So you are able to draw this data point x one.

562
01:11:03,900 --> 01:11:09,040
The older statistic, the first one you are able to draw that because is has about zero value the bulbs

563
01:11:09,040 --> 01:11:15,509
or if you want to draw data the data from a our distribution of data distribution,

564
01:11:15,510 --> 01:11:20,640
this certainly in parentheses is the best distribution to have.

565
01:11:21,030 --> 01:11:24,390
And if you want to draw their point are going to draw.

566
01:11:24,660 --> 01:11:31,790
And based on this empirical statement, you are only able to draw a data point from this set of observed.

567
01:11:31,920 --> 01:11:45,059
That is that they basically fundamental argument of bootstrap why you can draw data points from your data that your existing data is replacement.

568
01:11:45,060 --> 01:11:53,490
That's really exactly what happens. Okay. So then that's basically idea of bootstrap.

569
01:11:53,490 --> 01:11:59,010
You use your imperative side, you have to draw this and then out through all B samples,

570
01:11:59,010 --> 01:12:06,050
then you estimate the view B times and then you use the R the variance of this.

571
01:12:06,090 --> 01:12:13,850
Okay. The B.M. hat, the, the IMU, the mutual B is actually the sample draw from this distribution.

572
01:12:15,240 --> 01:12:22,649
You have B is this data point, those samples you draw from this distribution, if you have capital P, many of them,

573
01:12:22,650 --> 01:12:32,730
then you can calculate variance of the based on new heart beat, which gives you the estimate of the appearance of X bar, if you like.

574
01:12:33,330 --> 01:12:40,770
Okay, this is basically every idea. Okay, let's move on to Don Rubin's idea.

575
01:12:40,890 --> 01:12:46,380
Okay. Here here is the very simple example to show why this can be simply done,

576
01:12:46,830 --> 01:12:52,320
not from this empirical CDF, but just varied of the weights in the in the equation.

577
01:12:52,410 --> 01:12:56,160
Okay. So let's consider a data set of five data points.

578
01:12:56,490 --> 01:12:59,760
Okay. See y one way to why three why?

579
01:13:00,000 --> 01:13:09,930
There are distinctive data observations. Let's see, we are going to draw out for bootstrap samples of face replacement.

580
01:13:10,110 --> 01:13:17,160
Okay. I say the first one would be, you know, the.

581
01:13:18,090 --> 01:13:26,970
So I put a star, right? Because you do this data point resampling you use in replacement.

582
01:13:27,510 --> 01:13:38,370
So, so this is the first data point that you draw from the empirical CDF y11 star and the superscript

583
01:13:38,610 --> 01:13:44,550
indicated which bootstrap data you're talking about and the Y here it means that have the first,

584
01:13:45,300 --> 01:13:49,350
you know, bootstrap sample data. Okay. Now I do all five data points.

585
01:13:49,500 --> 01:13:53,160
Okay. The actual realization is like this. Okay.

586
01:13:53,820 --> 01:13:58,470
The first one I draw one. The second one I draw y one because I load replacement.

587
01:13:58,620 --> 01:14:02,160
Replacement, right. So yy2, y four and five.

588
01:14:02,200 --> 01:14:08,760
This is actually realized five data point I draw for this first data so you can so on, so forth.

589
01:14:08,900 --> 01:14:20,990
Okay. So now if you formulate the first data point of the first year, your estimate function here is the estimate function of this,

590
01:14:21,030 --> 01:14:25,150
this estimate the function you are applying to solve this equal to zero.

591
01:14:25,160 --> 01:14:29,479
Now you want to solve this again for your bootstrap sample.

592
01:14:29,480 --> 01:14:36,710
So why a star is the one that you're going to plug into this to solve your parameter, right?

593
01:14:37,220 --> 01:14:43,670
So now here I have my first data point draw from bootstrap and now I plug into that.

594
01:14:43,670 --> 01:14:51,920
What I have here is I have five out of two here, should be so here should be a one over five.

595
01:14:51,950 --> 01:14:55,220
Doesn't matter because that's a constant doesn't affect your route.

596
01:14:55,730 --> 01:14:59,209
But just for the simplicity, easy for interpretation.

597
01:14:59,210 --> 01:15:06,680
I did now write one out of five. I should take a while over five because that that doesn't matter because the right hand side is zero.

598
01:15:06,680 --> 01:15:11,090
That time to cancel, right? But one or five is easier to understand.

599
01:15:11,450 --> 01:15:24,290
So you have two data performing, one here and two there of one data point of two here and zero for y three and two from Y four and zero from y five.

600
01:15:24,830 --> 01:15:30,050
Okay, this is coming from this. So what you see here is what you want.

601
01:15:31,160 --> 01:15:36,830
Those are the random numbers draw from multi normal distribution.

602
01:15:38,120 --> 01:15:40,099
You have five categories.

603
01:15:40,100 --> 01:15:53,240
They have equal chance to be observed and then by chance you'll observe y12 times you observe y21 time and you observe y for two times you have zero.

604
01:15:53,540 --> 01:15:58,690
You do not see anything observed for y three and this is the coefficients.

605
01:15:58,700 --> 01:16:05,570
Here is what it is actually generated in numbers independent from your multi normal distribution.

606
01:16:06,710 --> 01:16:10,880
Great observation from O down Rubin.

607
01:16:11,150 --> 01:16:14,180
He said, well you don't need to. I mean, you give me the data,

608
01:16:14,180 --> 01:16:20,839
that's fine you can process with but what you actually the way you solve to to pass the data it's

609
01:16:20,840 --> 01:16:27,710
really just putting this coefficients in the origin equation is that by no on your distribution.

610
01:16:28,880 --> 01:16:34,760
Okay, that's very good. So this is published in the analysis.

611
01:16:34,850 --> 01:16:38,690
It is the paper 1981. So Operation cannot wait.

612
01:16:40,520 --> 01:16:45,200
So later on people say, oh, there are some challenges.

613
01:16:45,200 --> 01:16:48,470
And okay, so once you have this insight,

614
01:16:48,500 --> 01:16:54,920
you basically open the new door for a lot of else you can do so then moving propose a smooth

615
01:16:54,920 --> 01:17:00,950
multi normal weight by continuous with like generally from due to regular distribution.

616
01:17:01,580 --> 01:17:13,850
And then Jean Sal from Wisconsin and too from Queens University in Canada, they basically say you don't need to directly distribution.

617
01:17:14,150 --> 01:17:23,870
What you need is really any distribution that has a mean one and variance one can do the same thing so you don't need the originally.

618
01:17:23,870 --> 01:17:28,430
So basically they said you only need to generate something from exponential distribution.

619
01:17:29,120 --> 01:17:34,790
This parameter one the original is very complex like would you very final data distribution.

620
01:17:35,060 --> 01:17:43,910
But this is you don't need to do fancy stuff really just you only need to do this for a distribution from being 1/1 one.

621
01:17:44,690 --> 01:17:48,889
Okay. So, so that's basically idea.

622
01:17:48,890 --> 01:17:58,490
So what are we going to do here is right instead of the is that it resample your my data point I can just add a way to sample the

623
01:17:58,490 --> 01:18:08,150
sample direct independent of from exclude the distribution risk meaning the variance one following this random here so I can

624
01:18:08,600 --> 01:18:19,920
implement this without resampling my data I just shuffling my make some perturbation oh my equation so that I will generate more

625
01:18:20,000 --> 01:18:29,360
different types of RFA so that I can use that as my samples from a distribution from estimate or to get all the estimation.

626
01:18:30,470 --> 01:18:40,400
So so that's basically the way that I never try this, but I saw this is square idea that people can implement and see what happens.

627
01:18:40,850 --> 01:18:43,430
Okay, back to this table.

628
01:18:44,000 --> 01:18:58,340
And I was able to use key to process this the data and which is very similar to the result given by this ICM algorithm using Monte Carlo M algorithm.

629
01:18:59,090 --> 01:18:59,780
And,

630
01:18:59,780 --> 01:19:09,800
and so you have seen this already that an advantage of this key is that you are able to obtain to come a smoother common filter than you can really.

631
01:19:10,020 --> 01:19:20,819
What this an observed leatham process as part of the estimation procedure which I like a lot because you are trying to

632
01:19:20,820 --> 01:19:30,470
see what are the additional evolution of the disease that can now be simply explained by Time Train and your coverage.

633
01:19:30,780 --> 01:19:33,780
So that's something we can see in the system.

634
01:19:34,380 --> 01:19:37,920
Okay. Well, that's all I want to talk about today.

635
01:19:39,420 --> 01:19:39,780
Thank you.

