1
00:00:05,182 --> 00:00:10,463
Hello, this lecture will cover measures
of association and regression.

2
00:00:10,463 --> 00:00:13,960
In previous lectures we talked about
Pearson's correlation coefficient.

3
00:00:13,960 --> 00:00:21,073
Today, we will talk about a corresponding
value, the slope from linear regression.

4
00:00:21,073 --> 00:00:25,744
Pearson's correlation coefficient was
a general description of the degree and

5
00:00:25,744 --> 00:00:29,257
the direction of association
of two continuous variables.

6
00:00:29,257 --> 00:00:33,710
We will now explore another approach
to quantify the association.

7
00:00:33,710 --> 00:00:36,820
Our question might be that if
we change one of the variables,

8
00:00:36,820 --> 00:00:41,520
how much on average would
the other variable change?

9
00:00:41,520 --> 00:00:45,897
So recall our original scatterplot
of gun ownership rates and

10
00:00:45,897 --> 00:00:48,180
firearm suicide rate in males.

11
00:00:48,180 --> 00:00:51,989
Hopefully, you can see that
the correlation is related to how strongly

12
00:00:51,989 --> 00:00:55,120
the points cluster around a line.

13
00:00:55,120 --> 00:01:00,180
However, how do we know where that
line should exactly be drawn?

14
00:01:00,180 --> 00:01:04,937
We can use statistical methods to
determine what line would best

15
00:01:04,937 --> 00:01:06,002
fit the data.

16
00:01:06,002 --> 00:01:11,345
Recall that a line is expressed
with an intercept and a slope so

17
00:01:11,345 --> 00:01:17,760
that each Y value is a function
of its corresponding X value.

18
00:01:17,760 --> 00:01:22,131
In the population, the intercept is
denoted by the greek letter alpha.

19
00:01:22,131 --> 00:01:27,593
And in the population, the slope is
denoted by the greek letter beta.

20
00:01:27,593 --> 00:01:31,900
Now, we don't expect the line to fit
perfectly around all of the points.

21
00:01:31,900 --> 00:01:35,350
There will be some movement of
the points around the line.

22
00:01:35,350 --> 00:01:39,720
We quantify this movement through
a letter e denoting the error.

23
00:01:39,720 --> 00:01:43,920
So that our Y value will not be perfectly
on the line, but might be above or

24
00:01:43,920 --> 00:01:45,520
below it a little bit.

25
00:01:45,520 --> 00:01:49,983
This value of a little bit above or
below is quantified by the error.

26
00:01:49,983 --> 00:01:54,742
We again gather a sample of data from
the population, we plot the X and

27
00:01:54,742 --> 00:01:58,202
Y values, and
we fit a line through those points.

28
00:01:58,202 --> 00:02:00,825
I have shown an example
here of some points and

29
00:02:00,825 --> 00:02:04,490
one of the lines I have chosen
to draw through the points.

30
00:02:04,490 --> 00:02:07,670
So that I have a line, y = a + bx.

31
00:02:07,670 --> 00:02:12,070
a is the sample estimate for
the population intercept alpha.

32
00:02:12,070 --> 00:02:16,420
And b is the sample estimate
of the population slope beta.

33
00:02:16,420 --> 00:02:21,425
The line, therefore, for every x value
predicts what the y value should be.

34
00:02:21,425 --> 00:02:24,486
I have shown an example
here of an x value and

35
00:02:24,486 --> 00:02:29,100
its corresponding predicted
value y i hat from the line.

36
00:02:29,100 --> 00:02:34,330
Note that this value is different
from the actual y for that point.

37
00:02:34,330 --> 00:02:37,060
The difference between
the actual y value and

38
00:02:37,060 --> 00:02:41,440
the y value predicted by
the line is called a residual.

39
00:02:41,440 --> 00:02:48,660
And it measures for this point how well or
how poorly my line fits that point.

40
00:02:48,660 --> 00:02:52,030
We only care about the magnitude
of each of these residuals.

41
00:02:52,030 --> 00:02:53,725
And like in other methods,

42
00:02:53,725 --> 00:02:57,862
we will now square these residuals
to get rid of the negative sign.

43
00:02:57,862 --> 00:03:01,820
Again, the magnitude is the only
thing of interest to us.

44
00:03:01,820 --> 00:03:07,200
If the line I have chosen there in blue
fits all of the points well enough,

45
00:03:07,200 --> 00:03:11,120
then I would like all of these
squared errors to be small.

46
00:03:11,120 --> 00:03:15,898
Of course, it is impossible to make
all of them small at the same time.

47
00:03:15,898 --> 00:03:21,460
And so therefore, I will simply add up all
the squared errors and take an average.

48
00:03:21,460 --> 00:03:22,940
However, like in other methods,

49
00:03:22,940 --> 00:03:28,090
I don't take an average by dividing by n,
I now divide by n- 2.

50
00:03:28,090 --> 00:03:32,480
We call this quantity the mean
squared error, or MSE.

51
00:03:32,480 --> 00:03:36,240
It is essentially quantifying
how much noise is left over

52
00:03:36,240 --> 00:03:39,190
after I have drawn my
line through the points.

53
00:03:39,190 --> 00:03:43,770
I would like my MSE to
be as small as possible.

54
00:03:43,770 --> 00:03:49,602
Again, I could draw a different line
through the data, come up with a different

55
00:03:49,602 --> 00:03:54,842
MSE, and the smallest value of MSE
would give me the best fitting line.

56
00:03:54,842 --> 00:03:59,320
This is also known as least-squares
regression because I am trying to find

57
00:03:59,320 --> 00:04:01,888
the line that has the least squared error.

58
00:04:01,888 --> 00:04:06,609
Through a method called least-squares,
we can find the specific values of

59
00:04:06,609 --> 00:04:11,130
the sample estimates of the intercept and
the slope.

60
00:04:11,130 --> 00:04:14,510
The slope is the ratio of
the sample covariants of X and

61
00:04:14,510 --> 00:04:18,250
Y to the variants of X.

62
00:04:18,250 --> 00:04:22,610
And the intercept is computed by
the assumption that the line must exactly

63
00:04:22,610 --> 00:04:26,510
intersect with the sample mean X bar and
Y bar.

64
00:04:26,510 --> 00:04:30,725
Therefore the line will always fit
through the average of the data.

65
00:04:30,725 --> 00:04:34,675
And then all of the points
will scatter around that line.

66
00:04:34,675 --> 00:04:39,830
For our example, the line that I showed
you earlier is the least-squares line.

67
00:04:39,830 --> 00:04:44,782
It has an intercept of 1 and
a slope of 0.32.

68
00:04:44,782 --> 00:04:50,934
Therefore, for example, if I have a state
with 40% of residents who own firearms,

69
00:04:50,934 --> 00:04:55,907
our model, this fitted line,
expects that the number of suicides per

70
00:04:55,907 --> 00:04:59,278
100,000 would be 1 + 0.32 x 40,

71
00:04:59,278 --> 00:05:05,530
producing a value of 13.8
suicides per 100,000.

72
00:05:05,530 --> 00:05:08,766
So the question now is,
is this a good estimate for

73
00:05:08,766 --> 00:05:12,627
a state with a rate of 40% of
residents owning firearms?

74
00:05:12,627 --> 00:05:16,788
And hopefully you can see that if
I draw a vertical line from 40 all

75
00:05:16,788 --> 00:05:21,469
the way through the points, we see that
not all states with a firearm rate of

76
00:05:21,469 --> 00:05:24,621
about 40% all have suicide rates of 13.8.

77
00:05:24,621 --> 00:05:27,690
Some are higher, some are lower.

78
00:05:27,690 --> 00:05:32,510
So our regression line Y =
1.00 + 0.32X is not a perfect

79
00:05:32,510 --> 00:05:38,080
explanation of how firearm ownership and
suicide are related to each other.

80
00:05:38,080 --> 00:05:41,060
There are certainly some
points on the line, so

81
00:05:41,060 --> 00:05:44,010
that our model predicts very well.

82
00:05:44,010 --> 00:05:47,020
There are some points that
are well below the line, so

83
00:05:47,020 --> 00:05:51,490
that our line is actually
overestimating the rate of suicide.

84
00:05:51,490 --> 00:05:53,930
And there are points that
are above the line, so

85
00:05:53,930 --> 00:05:58,280
that our line underestimates the actual
suicide rates in those states.

86
00:05:58,280 --> 00:06:02,691
So we see that there is no guarantee
that our line will fit well to

87
00:06:02,691 --> 00:06:04,030
a specific point.

88
00:06:04,030 --> 00:06:09,710
Instead, the regression line is designed
to fit well to the average of many points,

89
00:06:09,710 --> 00:06:12,806
all of which have the same x value

