1
00:00:00,240 --> 00:00:05,370
Hi everyone. Welcome. Thanks so much for being here on like one of the last classes before the break.

2
00:00:05,700 --> 00:00:09,470
My name is Lindsey Kobayashi, I think I don't know most of you.

3
00:00:09,480 --> 00:00:17,210
I'm an assistant professor in the Department of Epidemiology. I'm a faculty member in CCF, the Center for Social Epidemiology and Population Health.

4
00:00:18,060 --> 00:00:23,129
And Katie and I were just talking this is like one of the first in-person lectures I've done since COVID.

5
00:00:23,130 --> 00:00:29,760
I was out on maternity leave and kind of not around, but I'm back, so it's nice to be back teaching in-person.

6
00:00:30,030 --> 00:00:41,040
This lecture I am really excited about. I taught F at 601 back in fall 2020 during the pandemic, so I'm intricately familiar with this class.

7
00:00:41,040 --> 00:00:46,829
I know it's a really it's a hard class. And let me tell you, it was even harder back in fall 2020.

8
00:00:46,830 --> 00:00:50,729
So I hope I just hope it's going okay for you all this year.

9
00:00:50,730 --> 00:00:54,360
I think this is one of the more fun topics of the class.

10
00:00:55,050 --> 00:00:56,879
I think not everyone would agree with me.

11
00:00:56,880 --> 00:01:04,350
Systematic review and meta analysis can be hard, but I think it's a really, really good skill for any epidemiologist to have.

12
00:01:04,350 --> 00:01:09,120
I think it's something that everybody should do at least once in their career and just sort of interest.

13
00:01:09,120 --> 00:01:12,660
Has anybody in the room done a systematic review and meta analysis before?

14
00:01:13,290 --> 00:01:16,829
Cara Great. So let's, let's get right into it.

15
00:01:16,830 --> 00:01:20,459
There's a lot of material and feel free to interrupt me at any point if you have

16
00:01:20,460 --> 00:01:26,610
any questions so that I kind of live for learning objectives for this topic.

17
00:01:26,940 --> 00:01:34,050
So at the end of this two hour session, I want everyone to be able to articulate the scientific need for a systematic review and meta analysis.

18
00:01:34,470 --> 00:01:38,220
Understanding the difference between the two from methodological point of view,

19
00:01:38,670 --> 00:01:43,770
be able to articulate the concept of publication bias and how best to look for it in the literature.

20
00:01:43,770 --> 00:01:50,160
When you're doing a review and then know at least two of the best practices for conducting systematic reviews and meta analyzes,

21
00:01:50,490 --> 00:01:59,010
and hopefully we'll all feel inspired to do a systematic review and meta analysis of your own after today's session reviews and general

22
00:01:59,850 --> 00:02:06,959
a review as any attempt to systematically synthesize the results and conclusions of two or more publications on a given topic.

23
00:02:06,960 --> 00:02:10,200
Right. I feel like we are all kind of like generally familiar with the concept.

24
00:02:10,200 --> 00:02:14,370
I'm sure many of you have come across review articles before or read them.

25
00:02:15,180 --> 00:02:18,270
Technically, a review is of two or more papers.

26
00:02:18,270 --> 00:02:22,440
If you can find a published review of two papers, I would be shocked.

27
00:02:23,160 --> 00:02:29,580
I would say that's not that's bad practice. In practice, it'll be usually anywhere from 10 to 100 papers included.

28
00:02:29,580 --> 00:02:33,630
I've seen very, very small reviews that include maybe like ten, 15, 20 papers.

29
00:02:34,380 --> 00:02:39,720
100 papers would be a lot to include in a review that's starting to get a little bit unwieldy in terms of synthesis.

30
00:02:40,380 --> 00:02:44,460
But that's kind of like the general ballpark of what you might see in the published literature.

31
00:02:45,630 --> 00:02:49,560
Reviews are most useful when there is a substantive research question.

32
00:02:49,950 --> 00:02:55,799
Several papers have been published on that question, but there still remains uncertainty about results.

33
00:02:55,800 --> 00:03:00,240
So they can help us to synthesize this current state of knowledge on a given topic.

34
00:03:02,680 --> 00:03:05,930
All reviews have a research question. And this is really, really important.

35
00:03:05,950 --> 00:03:11,170
This is the kind of the whole foundation, if you're going to actually go out and do your own systematic review and meta analysis.

36
00:03:11,470 --> 00:03:13,630
You need to start with a substantive question.

37
00:03:14,200 --> 00:03:18,820
An example would be what is the association between social support and cognitive function in later life?

38
00:03:18,940 --> 00:03:24,460
Or what is the association between incident cancer and subsequent lifetime risk of Alzheimer's disease?

39
00:03:25,420 --> 00:03:28,480
So when you have your question and you're trying to conduct a review,

40
00:03:28,780 --> 00:03:33,609
rather than go out and collect primary data or else download a data set and conduct a

41
00:03:33,610 --> 00:03:38,410
secondary analysis of that data set where the level of the observation is the individual.

42
00:03:38,650 --> 00:03:44,890
When you conduct a review, you would gather the available evidence and then synthesize them to come up with a general result.

43
00:03:44,900 --> 00:03:52,300
And so now the level of analysis is like a paper that has been published, which has some form of effect estimate that you're trying to synthesize.

44
00:03:52,960 --> 00:03:59,740
And we have systematic review and meta analysis are two different types of reviews that we're going to get into over the course of the session.

45
00:04:01,150 --> 00:04:07,870
A systematic review aims to comprehensively identify and synthesize published literature on a given topic.

46
00:04:08,740 --> 00:04:14,350
Rigorous, systematic literature searching methods are developed and used for a systematic review.

47
00:04:14,680 --> 00:04:19,900
And again, like I said, it has a specific research question that's the most important or one of the most important things here.

48
00:04:20,230 --> 00:04:26,440
So typically in epidemiology, a systematic review should be focused on a specific exposure outcome relationship.

49
00:04:27,220 --> 00:04:34,360
If a review does not clearly state whether and how all relevant results are identified and synthesized,

50
00:04:34,360 --> 00:04:42,850
if it does not have some kind of systematic methodological approach for searching the literature, that it is not a systematic review.

51
00:04:43,420 --> 00:04:45,940
You might see other types of reviews published like,

52
00:04:45,940 --> 00:04:50,980
and they could be called things like a narrative review or a scoping review or an umbrella review.

53
00:04:51,250 --> 00:04:56,680
Those are not systematic reviews. Those are very, very different. I don't really like those phrases.

54
00:04:56,680 --> 00:05:00,340
I don't know how like I don't even know what Umbrella Review is like. I've seen those.

55
00:05:00,640 --> 00:05:05,650
I don't know if you've seen those before. I think they're just kind of very ad hoc, like in my mind,

56
00:05:05,650 --> 00:05:10,660
a sort of like we did a Google Scholar search and we also had some ideas about important papers on the topic,

57
00:05:10,660 --> 00:05:17,410
and we're just going to like talk about them. You might also see like commentary pieces that are sort of framed as a review.

58
00:05:17,410 --> 00:05:24,190
I've even published something like this before. That's kind of very targeted, and it's not meant to be a systematic review, and that's fine.

59
00:05:25,240 --> 00:05:32,680
There's a little bit of a gray area when it comes to reviews that are not systematic reviews and like what their quality is and what their purpose is.

60
00:05:33,340 --> 00:05:41,050
But I would say systematic reviews have a specific set of systematic methods for searching and synthesizing literature, and we'll talk about those.

61
00:05:42,430 --> 00:05:49,870
A meta analysis, on the other hand, is when measures of association or effect estimates such as a beta coefficient from linear

62
00:05:49,870 --> 00:05:54,760
regression model and odds ratio from a logistic regression hazards ratio from a survival analysis.

63
00:05:55,150 --> 00:06:01,059
When these effect estimates for an individual study that are identified through a systematic

64
00:06:01,060 --> 00:06:05,170
review are pooled using statistical techniques to generate a combined effect estimate.

65
00:06:05,530 --> 00:06:11,140
And there are specific conditions for when this can validly be done. And we'll talk about those later in the session.

66
00:06:11,440 --> 00:06:13,879
So the point I want to make here is that all meta analysis,

67
00:06:13,880 --> 00:06:18,490
these are also systematic reviews because we use systematic reviewing methods to identify papers

68
00:06:18,790 --> 00:06:24,670
that we then pull effect estimates out of and combine quantitatively using meta analysis methods.

69
00:06:25,000 --> 00:06:31,600
But not all systematic reviews are meta analyzes. So meta analysis is kind of one step further than a systematic review.

70
00:06:32,800 --> 00:06:36,400
This is a paper that I wanted to flag. We're not going to get into it in any depth.

71
00:06:36,640 --> 00:06:42,700
I think it should be there in the readings. Is that right? And there's also a link to the article in the slides.

72
00:06:43,240 --> 00:06:45,550
This is a really great, kind of straightforward,

73
00:06:45,880 --> 00:06:51,820
easy to understand paper that gives a really kind of practical and introductory guide of how to get started with systematic reviews.

74
00:06:51,820 --> 00:06:55,959
And I remember I did I did my first systematic review when I was a Ph.D. student.

75
00:06:55,960 --> 00:07:00,640
It was one of the papers that was in my Ph.D., and I felt like it was a really, really good learning experience for me.

76
00:07:01,540 --> 00:07:07,629
And I remember reading this paper when I was kind of getting started with, you know, learning how to do a review.

77
00:07:07,630 --> 00:07:11,110
And it was helpful to me. So I'm putting it here as a resource for you guys.

78
00:07:12,880 --> 00:07:21,790
And now this is an example of systematic review and meta analysis that I published with a trainee and other collaborators a couple of years ago now.

79
00:07:21,790 --> 00:07:31,299
It came out in late 2020, and I'm going to talk about the methods for conducting a systematic review using this paper as an applied example.

80
00:07:31,300 --> 00:07:38,560
So I'm going to walk you through step by step in terms of how we kind of going behind the published results that you see how

81
00:07:38,560 --> 00:07:45,070
we came up step by step with all the different parts of the process of conducting this systematic review and meta analysis.

82
00:07:45,700 --> 00:07:49,389
So the first thing that we needed to do so this is a T, I'll actually back up a little bit.

83
00:07:49,390 --> 00:07:51,910
This is a team that I've been working with for a couple of years,

84
00:07:52,540 --> 00:08:01,120
doing work to try to understand the sort of paradoxical inverse association between Alzheimer's disease and cancer that we'd observed in the late.

85
00:08:01,170 --> 00:08:08,700
Erica. So we went into this review with a little bit of kind of background and content area knowledge as dementia epidemiologist,

86
00:08:08,700 --> 00:08:17,610
noting that there are a lot of epidemiological studies that paradoxically show that there's an inverse association between cancer and dementia.

87
00:08:17,700 --> 00:08:27,359
What I mean by that is that in epidemiologic studies, using cohort data or medical record data, people who have a lifetime history,

88
00:08:27,360 --> 00:08:32,430
who have a history of cancer seem to have a lower subsequent lifetime risk of Alzheimer's disease,

89
00:08:32,820 --> 00:08:38,549
which we think of as a paradox, because these are both these are two chronic conditions that are associated with age.

90
00:08:38,550 --> 00:08:43,500
And age is the strongest risk factor for both dementia or Alzheimer's disease and cancer.

91
00:08:43,740 --> 00:08:51,930
And they share several other common risk factors. And we were doing a series of kind of primary studies to try to better disentangle this association.

92
00:08:51,930 --> 00:08:55,979
But we thought we need to do a real systematic literature review and meta analysis

93
00:08:55,980 --> 00:09:00,299
to try to understand the state of the literature and understand characteristics of

94
00:09:00,300 --> 00:09:04,379
studies that are showing this inverse finding because we strongly suspected it was

95
00:09:04,380 --> 00:09:12,020
potentially due to study bias rather than a real kind of substantive etiologic result.

96
00:09:12,810 --> 00:09:16,380
So that was kind of the level of background knowledge that we had going into this review.

97
00:09:16,650 --> 00:09:18,240
And I think if you do a review of your own,

98
00:09:18,240 --> 00:09:23,610
you'll probably find yourself kind of at a similar state where you're researching a general topic and you're like, Okay,

99
00:09:23,610 --> 00:09:27,540
I actually need to sit down and do some systematic synthesis of the results,

100
00:09:27,540 --> 00:09:32,100
just so I understand the lay of the land in terms of like what is the current state of scientific evidence?

101
00:09:32,790 --> 00:09:36,989
And I'll say doing systematic review is a great way if you ever want to feel like you know everything there

102
00:09:36,990 --> 00:09:41,370
is to know about what has been published on a given topic and doing a systematic review will really,

103
00:09:41,370 --> 00:09:45,360
really, really make you feel like you're on top of the literature, and it's kind of an exciting feeling.

104
00:09:46,290 --> 00:09:49,949
So the research question was the first thing that we had to come up with once we had an

105
00:09:49,950 --> 00:09:54,540
idea that we wanted to do a systematic review and meta analysis on this particular topic.

106
00:09:54,930 --> 00:09:58,200
So we spent some time crafting our question, you know, through discussions,

107
00:09:58,560 --> 00:10:03,900
and we ended up coming up with the question Does an association exist between cancer and subsequent AIDS?

108
00:10:04,110 --> 00:10:06,680
I'm going to use the acronym e.g. for Alzheimer's disease here.

109
00:10:07,170 --> 00:10:13,620
And how likely is it that such a finding is associated with methodological bias rather than a true common etiology?

110
00:10:14,430 --> 00:10:17,260
So this latter part. So there's really two questions here, right?

111
00:10:17,280 --> 00:10:23,129
And this latter part of the question was informed by our prior hypotheses, given kind of what we already know,

112
00:10:23,130 --> 00:10:29,940
news or a background knowledge of the topic, not all systematic reviews need to kind of specifically get into questions around bias.

113
00:10:29,940 --> 00:10:34,290
But as you'll learn, bias assessment is a really important part of office and high tech reviews.

114
00:10:34,710 --> 00:10:38,130
So we wanted to do this in a very formal way for this particular research question,

115
00:10:38,130 --> 00:10:41,990
because we really thought the association we were interested in was due to bias.

116
00:10:42,000 --> 00:10:51,770
So this was a particular situation here. So when developing the research question, my first piece of advice would be identify the need for a view.

117
00:10:51,790 --> 00:10:57,969
So I kind of just talked about this might be getting to a point in researching whatever question or topic you're looking at where you're like,

118
00:10:57,970 --> 00:11:01,690
okay, I need to sit down and do a systematic review because there's some studies.

119
00:11:01,690 --> 00:11:05,830
The results are all over the place. I need to have a better understanding of what is the current state of literature.

120
00:11:06,760 --> 00:11:13,400
First, I would recommend in order to kind of identify like whether you're at that point, do read, read, read, just read a lot.

121
00:11:13,420 --> 00:11:14,830
Do a scoping literature search.

122
00:11:14,950 --> 00:11:22,990
Know use PubMed, Google Scholar Search, reference lists of papers you've identified just to get an initial feel for the lay of the land.

123
00:11:23,260 --> 00:11:26,050
With respect to what has been published on your topic of interest,

124
00:11:26,980 --> 00:11:32,750
are there any existing systematic reviews or meta analyzes of the same topic that you want to do your review on?

125
00:11:32,770 --> 00:11:33,790
That's really important.

126
00:11:34,390 --> 00:11:40,570
If it's already been done and been done well recently, then you probably can just like use that existing review and not do one of your own.

127
00:11:40,900 --> 00:11:45,390
No need to reinvent the wheel. Right. But if there is an existing review, is an update warranted?

128
00:11:45,400 --> 00:11:50,060
Was the review done a long time ago? Was it done badly? Is it time to update it?

129
00:11:50,080 --> 00:11:56,090
So those are good questions to ask yourself. So here's an example.

130
00:11:56,100 --> 00:11:59,000
So the paper that I was working on with my colleagues,

131
00:11:59,180 --> 00:12:06,200
we were interested in the relationship between incident cancer and subsequent lifetime risk of Alzheimer's disease.

132
00:12:07,190 --> 00:12:08,929
So yeah, I guess I kind of already said this.

133
00:12:08,930 --> 00:12:15,020
So we were really concerned that this inverse association was due to bias and maybe not due to a real etiology.

134
00:12:15,020 --> 00:12:21,200
And no, there actually were some systematic reviews on this topic previously that confirmed that,

135
00:12:21,200 --> 00:12:25,700
you know, across epidemiologic studies that have been published, there is this paradoxical inverse relationship,

136
00:12:26,060 --> 00:12:31,280
but we wanted to both update that review and then take things a step further by doing a formal bias analysis.

137
00:12:34,650 --> 00:12:39,450
The second piece of advice I'll give is to define and refine your research question.

138
00:12:39,870 --> 00:12:45,300
Focus. Specific questions usually tend to result in higher quality, more useful reviews.

139
00:12:45,630 --> 00:12:50,580
And this is analogous to if you're doing a study using primary data.

140
00:12:51,030 --> 00:12:57,989
You might have experienced in your work so far that having a focused and specific question usually like gives you a

141
00:12:57,990 --> 00:13:03,959
better roadmap to doing your statistical analysis and gives you better quality results than trying to ask a vague,

142
00:13:03,960 --> 00:13:07,860
undefined question. Does that kind of is this like ringing true for people?

143
00:13:08,220 --> 00:13:12,210
Like asking vague questions gives you vague answers or they're not answerable.

144
00:13:12,510 --> 00:13:19,500
So just as when you're trying to do your own study using primary data on end at the individual level,

145
00:13:19,770 --> 00:13:27,390
when you're asking a question of a systematic review, a specific and focused question is going to make actually conducting the review tractable.

146
00:13:28,770 --> 00:13:35,610
So your search is easier, it's more likely to be complete. It helps you come up with very clear inclusion and exclusion criteria.

147
00:13:35,610 --> 00:13:43,679
If you are very specific about what is the research question that you're trying to ask and it makes your data extraction decisions easier to make.

148
00:13:43,680 --> 00:13:50,879
Like if you go into if you go into any database of any literature database and try to

149
00:13:50,880 --> 00:13:54,060
do your search and you don't have your exposure and your outcome clearly defined,

150
00:13:54,360 --> 00:13:57,930
you're going to get thousands of abstracts back. You're not going to know what to do with them.

151
00:13:58,230 --> 00:14:01,680
So having a very clear and focused question from the start is going to help you

152
00:14:02,310 --> 00:14:05,280
so much in actually doing your review and getting something useful out of it.

153
00:14:07,170 --> 00:14:15,569
So we refined this question at this point to try to make it more specific and tractable when we were searching the literature.

154
00:14:15,570 --> 00:14:19,680
So we refined this to say, Does an association exist between cancer?

155
00:14:19,680 --> 00:14:24,270
And that was it. We needed refining. It couldn't just be like cancer broadly, right?

156
00:14:24,270 --> 00:14:26,070
Like cancer. There's many different types of cancers.

157
00:14:26,400 --> 00:14:34,910
So we specified breast, prostate, lung, colorectal, non-melanoma skin or any cancer versus no cancer history and subsequent aids.

158
00:14:34,920 --> 00:14:41,610
So we kind of imposed a temporal order. And here we wanted to look at longitudinal studies that measured age after cancer diagnosis.

159
00:14:42,510 --> 00:14:48,270
And the reason why we chose these cancers breast, prostate, lung and colorectal are four of the most common cancers.

160
00:14:48,540 --> 00:14:56,549
And then we also wanted to include non-melanoma skin cancer, because non-melanoma skin cancer has a very, very high survival rate.

161
00:14:56,550 --> 00:15:02,190
And we were actually quite concerned that this the particular type of bias that could be resulting

162
00:15:02,190 --> 00:15:07,469
in this inverse cancer association was something like a selective survival bias or competing risks,

163
00:15:07,470 --> 00:15:10,590
right? Like the most like the healthiest people, all else being equal,

164
00:15:10,590 --> 00:15:17,190
the healthiest people are more likely to survive years after their cancer and live long enough to develop an 80 diagnosis.

165
00:15:17,940 --> 00:15:20,969
So that would be a fraction of all people who are diagnosed with breast,

166
00:15:20,970 --> 00:15:25,380
prostate along and colorectal, especially lung, for example, which has the highest mortality rates.

167
00:15:26,010 --> 00:15:29,550
But essentially all people diagnosed with non-melanoma skin cancer are going to survive.

168
00:15:29,910 --> 00:15:35,160
So if we see an association between non-melanoma skin cancer and a risk that gives us

169
00:15:35,160 --> 00:15:38,370
an indication that there's something else going on that isn't selective survival bias.

170
00:15:38,730 --> 00:15:45,330
Does that make sense? Great. I also like I also really like teaching this review because I feel like it's a really good overview of

171
00:15:45,330 --> 00:15:50,370
a lot of the forms of bias or threats to internal validity that you're also learning in this class.

172
00:15:50,580 --> 00:15:55,620
So I hope you're picking up on that as I go through. Let's go through an example.

173
00:15:56,040 --> 00:16:00,090
Here is a vague question Does obesity cause influenza?

174
00:16:00,480 --> 00:16:06,360
And now can anyone give me any suggestions of, like, what you could do to refine this question to make it more specific and tractable?

175
00:16:10,970 --> 00:16:14,120
You get restricted to a certain population?

176
00:16:14,570 --> 00:16:21,100
Yeah, exactly. So like among children, for example, or adults over 18 or whatever population interested.

177
00:16:22,890 --> 00:16:28,860
What else? Yeah, we'd like to look more broadly at whether because she's increased.

178
00:16:31,660 --> 00:16:34,810
Yeah. Specifically, just describe what you're dealing with.

179
00:16:36,370 --> 00:16:40,089
Yeah, exactly. Exactly. So put a direction of association.

180
00:16:40,090 --> 00:16:46,810
So like increased incidence and maybe like incidents implies we're going to use longitudinal studies for new cases of influenza.

181
00:16:47,530 --> 00:16:56,140
Yeah, great. Anything else? He's fine with her infection, but symptomatic.

182
00:16:56,780 --> 00:17:01,390
Mm hmm. Exactly. It seems like, um.

183
00:17:01,440 --> 00:17:05,260
Chris. Yes. Yes, exactly.

184
00:17:06,130 --> 00:17:12,250
Yes. So, being more specific about our exposure measure and outcome measure, what do we mean when we say obesity?

185
00:17:12,310 --> 00:17:18,580
Like what operationalization of obesity in research studies are we going to use to define obesity?

186
00:17:18,580 --> 00:17:21,730
And same thing with influenza. How do we catch the cases?

187
00:17:22,090 --> 00:17:27,580
Is it going to be symptomatic cases? Is it going to be, you know, diagnosed cases of people who go in for testing?

188
00:17:28,060 --> 00:17:31,780
Probably going to be all kinds of testing biases around that. So that's really important.

189
00:17:32,280 --> 00:17:39,890
And so this probably everything everybody just said is valid. Here's an example of what I gave here is in adults.

190
00:17:39,940 --> 00:17:45,610
So narrowing the population or being specific about the population, does obesity measured using body mass index?

191
00:17:45,880 --> 00:17:51,250
So you could definitely get more specific. So just like you said, define what you like, what measure of obesity you going to use.

192
00:17:52,090 --> 00:17:56,610
And I didn't say, you know, like those categories that have been identified as cutoffs using BMI,

193
00:17:56,620 --> 00:18:02,410
that would be one example lead to increase susceptibility to influenza virus infection, household settings.

194
00:18:02,680 --> 00:18:09,100
So getting more specific about that influence, like what we mean by influenza and I'm sure there are probably better or different

195
00:18:09,100 --> 00:18:13,719
ways that we could equally validly define what is an influenza virus infection.

196
00:18:13,720 --> 00:18:18,070
And then here identified household settings to help narrow things a little bit more.

197
00:18:18,490 --> 00:18:24,640
So if you were going to go into like PubMed or Medline or any kind of database

198
00:18:25,030 --> 00:18:29,530
and try it like the first the first question like obesity and influenza,

199
00:18:29,530 --> 00:18:33,430
you are going to get a complete mess and it's going to be really, really hard to like.

200
00:18:33,610 --> 00:18:36,759
It's just going to be an unwieldy volume of abstracts that you're going to get back.

201
00:18:36,760 --> 00:18:38,080
A lot of them won't be relevant,

202
00:18:38,980 --> 00:18:44,470
and it's going to be really hard to filter through and then choose which ones are relevant to answering this question.

203
00:18:44,830 --> 00:18:50,200
Whereas if you develop a set of search terms based on the language that's used in this question,

204
00:18:50,500 --> 00:18:52,719
and you've been really specific about what it is you're looking for,

205
00:18:52,720 --> 00:18:56,920
it's really going to help you to have a tractable search where you can easily read

206
00:18:56,920 --> 00:19:00,580
through abstracts and decide what is and isn't eligible for inclusion in your review.

207
00:19:02,080 --> 00:19:10,030
And I would recommend using the PICO Framework to frame your question for the systematic review as anyone familiar with the PICO Framework.

208
00:19:12,410 --> 00:19:18,709
That's totally fine. So I will say that the PICO framework was sort of developed with a clinical setting in mind where as many

209
00:19:18,710 --> 00:19:23,090
of us in epidemiology are going to be doing work in population settings rather than clinical settings.

210
00:19:23,690 --> 00:19:27,320
But essentially, PICO stands for patient intervention, comparison and outcome.

211
00:19:27,800 --> 00:19:34,460
So after you have developed your research question, take a look at it again with through the lens of the PICO Framework.

212
00:19:34,820 --> 00:19:38,750
So ask yourself, does it address does it identify each of these four things?

213
00:19:39,110 --> 00:19:45,680
So patient I would say in an epidemiologic context we're thinking about the population rather than the patient.

214
00:19:45,920 --> 00:19:52,970
So what is the population that we're studying? So in the previous example, it was adults over age 18, the intervention,

215
00:19:53,870 --> 00:19:58,760
this would be the therapeutic, diagnostic, other intervention or exposure under investigation.

216
00:19:59,120 --> 00:20:05,329
So obviously intervention would apply to like an RC, T or other type of interventional design.

217
00:20:05,330 --> 00:20:11,050
Whereas for observational research, I would rephrase this to be the exposure that we're interested in.

218
00:20:11,060 --> 00:20:14,810
So what is the exposure of interest in our research?

219
00:20:14,810 --> 00:20:18,920
Question In the previous example, that was obesity in the cancer of you,

220
00:20:18,920 --> 00:20:22,880
it is like cancer and it was one of four specific cancer types that we were interested in.

221
00:20:23,660 --> 00:20:28,100
The comparison would be the alternative intervention or such as a control or placebo group,

222
00:20:28,850 --> 00:20:32,180
or it would be the unexposed group in an observational setting.

223
00:20:33,170 --> 00:20:39,020
So define like what? What are you counting as exposed versus unexposed for your exposure, measure or variable?

224
00:20:39,410 --> 00:20:41,230
And then finally, the outcome of interest.

225
00:20:41,240 --> 00:20:49,070
So in the cancer review, that is incident Alzheimer's disease, in the previous question I showed you, it was susceptibility to influenza infection.

226
00:20:50,120 --> 00:20:56,309
Any questions at this point? Well, good. And then here is kind of a framework.

227
00:20:56,310 --> 00:20:58,770
I'm not going to talk this through, but you can return to this.

228
00:20:59,010 --> 00:21:04,829
This is a framework of different types of questions and how we might apply the PICO framework in those types of questions.

229
00:21:04,830 --> 00:21:09,629
So regarding research questions around therapeutics, prevention, diagnosis,

230
00:21:09,630 --> 00:21:17,340
prognosis in etiology and sort of etiology is that world of observational studies that we most commonly do in epidemiology.

231
00:21:19,440 --> 00:21:25,620
So after you have defined and refined your question, the next step is to develop and refine your protocol for your systematic review.

232
00:21:26,580 --> 00:21:29,600
The protocol is used. It's essentially like your methods, right?

233
00:21:29,630 --> 00:21:36,090
Like it's it's the method section that's going to ultimately be in your paper and you're going to follow this protocol when you do your literature,

234
00:21:36,090 --> 00:21:39,030
search, data extraction and synthesis of results.

235
00:21:39,030 --> 00:21:44,310
The protocol is really, really important because it ensures that you have standardized methods across multiple reviewers.

236
00:21:44,640 --> 00:21:48,300
And it's always a good idea to have multiple reviewers, and I'll talk a bit more about that.

237
00:21:49,170 --> 00:21:53,549
It ensures that you have a transparent and reproducible process of conducting the review.

238
00:21:53,550 --> 00:21:56,550
And this is important for science overall, not just for reviews.

239
00:21:57,330 --> 00:22:03,240
And the other thing to keep in mind is that systematic reviews are iterative and your protocol is probably going to change over time.

240
00:22:03,570 --> 00:22:08,309
And every time I have done a systematic review, the protocol definitely changes with time.

241
00:22:08,310 --> 00:22:13,020
Like you think you've come up with a good question and you think you've come up with good search terms for searching a database,

242
00:22:13,380 --> 00:22:19,020
and then you're finding that like you were returning studies where maybe like there's important characteristics or

243
00:22:19,020 --> 00:22:24,479
details of those studies that you didn't think about and you need to edit add information to your data extraction form,

244
00:22:24,480 --> 00:22:27,930
or you need to go back and refine something in your question to be more specific.

245
00:22:28,260 --> 00:22:36,989
And I think that's very, very common. But I will say the more clear you are in your question and the more detailed you are in your protocol and all

246
00:22:36,990 --> 00:22:42,809
of this is form is better supported by having good background knowledge and kind of going into the review,

247
00:22:42,810 --> 00:22:52,320
having a solid foundation of knowledge in the area that you're studying is going to help you go back less often to revise the protocol.

248
00:22:52,320 --> 00:22:57,060
But I do think it is normal to go back and revise it iteratively, at least a couple of times.

249
00:22:58,770 --> 00:23:04,110
So the components of the protocol one, like I said, the research question two is the search strategy.

250
00:23:04,110 --> 00:23:10,080
So are you going to identify databases in which you will be searching for published literature on your research question of interest?

251
00:23:10,470 --> 00:23:15,510
These are the most common ones that people in epidemiology use are Medline and Basic and there's others.

252
00:23:15,990 --> 00:23:21,750
You also have search terms for your database and then you will set inclusion criteria for the types of studies that

253
00:23:21,750 --> 00:23:28,120
you are going to include in your review after you find them using the search terms you've applied in your database.

254
00:23:28,130 --> 00:23:33,230
So these would be things like language of publication. What study population are you interested in?

255
00:23:33,240 --> 00:23:39,900
Is it children? Is it adults? Is it women? Are there any kind of restrictions of the population of interest?

256
00:23:40,620 --> 00:23:47,880
What designs of studies do you want to include? Do you want to include any type of study design or even for RC, TS, longitudinal cohorts, etc.?

257
00:23:48,450 --> 00:23:51,450
The time frame does it matter when the studies were conducted?

258
00:23:52,140 --> 00:23:58,050
Do you want to go through know all studies that have been published all through time or only after a certain year?

259
00:23:59,040 --> 00:24:02,040
Demographic restrictions. Types of measures of association.

260
00:24:02,700 --> 00:24:09,810
Things like that. Aside from databases, can anyone think of other data sources that might be useful to search in a systematic review?

261
00:24:17,320 --> 00:24:27,630
There's lots. When you're looking for literature on a topic, where do you look?

262
00:24:28,740 --> 00:24:32,880
If your topic is very specific, you might search for journals. Yeah, yeah.

263
00:24:32,940 --> 00:24:39,080
That's a really, really good idea. Anything else you're referencing?

264
00:24:39,530 --> 00:24:45,860
Yes, that's a huge resource. So after you've identified some papers that are useful, you're going to include in your review,

265
00:24:46,130 --> 00:24:52,970
search their reference lists to help you sort of snowball to find other relevant papers that you might have missed in your initial search.

266
00:24:55,290 --> 00:25:00,120
Another way to find literature or another place to go to find literature.

267
00:25:07,620 --> 00:25:18,400
Can I ask you in the middle? Sorry. I was going to say, it's just like looking at there are a lot of just databases.

268
00:25:19,840 --> 00:25:24,719
Mm hmm. I'm not sure. Well, yeah, no.

269
00:25:24,720 --> 00:25:31,200
I think searching the reference lists of papers you've found, even, like, Google Scholar has, like, pretty decent algorithms.

270
00:25:31,590 --> 00:25:34,990
The other things that I would say are, um,

271
00:25:35,340 --> 00:25:45,420
the Cochrane Library is really useful to find if there is an existing systematic review on the topic that you're studying the great literature.

272
00:25:45,450 --> 00:25:51,170
So like this is considered like anything that's not a peer reviewed publication in a journal.

273
00:25:51,180 --> 00:25:56,489
So like theses reports, I put non peer reviewed journals here.

274
00:25:56,490 --> 00:25:58,410
To be honest, I wouldn't really go for that.

275
00:25:59,040 --> 00:26:04,679
But a really good source of the great literature are people's like Ph.D. dissertations, and some like, they're really good.

276
00:26:04,680 --> 00:26:09,390
Quite a lot of them are very, very good quality and go very in-depth on specific topics.

277
00:26:09,750 --> 00:26:16,049
And you may or may not wish to, if you can find it like you, and you would usually go to like a university.

278
00:26:16,050 --> 00:26:21,450
So first of all, if a university has published publishes dissertations online,

279
00:26:21,540 --> 00:26:26,590
you can find them through Google Scholar like I know my Ph.D. is like they are on Google Scholar PDF.

280
00:26:26,610 --> 00:26:31,290
Like, I don't want anyone to read it ever, but it's there. So you can find dissertations on Google Scholar.

281
00:26:31,650 --> 00:26:37,020
You can also, if there's a certain person you know who has done their dissertation, you can email them,

282
00:26:37,020 --> 00:26:41,490
you can go to the university's website and search through a database and find them.

283
00:26:41,850 --> 00:26:47,640
And those are just helpful for gaining background knowledge and also for like searching reference lists.

284
00:26:47,650 --> 00:26:51,600
And then if that author has published any papers out of it, that can be helpful.

285
00:26:52,580 --> 00:26:55,709
Um, a couple other things I put here.

286
00:26:55,710 --> 00:26:59,580
Unpublished sources known to experts in the field seek my personal communication.

287
00:26:59,580 --> 00:27:02,549
You can email someone just to sort of pick their brain about a topic.

288
00:27:02,550 --> 00:27:08,820
I would say this may or may not be helpful in terms of actually finding papers, like published papers that you can include in your review.

289
00:27:08,820 --> 00:27:15,880
But it's really helpful for expanding your content knowledge. And people will generally talk to you like I have Ph.D. students and like, you know,

290
00:27:15,900 --> 00:27:20,460
more kind of junior people emailing me about systematic reviews and I will talk to them.

291
00:27:20,820 --> 00:27:27,560
And I think when you tell someone that you're conducting a systematic review and meta analysis, especially if they say to you like,

292
00:27:28,020 --> 00:27:32,759
especially if you say like, you know, you publish on this topic I'm interested in including one of your papers.

293
00:27:32,760 --> 00:27:38,969
People are generally really receptive and helpful. Um, raw data from published studies.

294
00:27:38,970 --> 00:27:45,299
This is usually for meta analysis. So if someone has published a study and they have an effect estimate that isn't of the type you need,

295
00:27:45,300 --> 00:27:50,280
let's say like, I don't know, you need an incidence rate and they report and odds ratio.

296
00:27:50,280 --> 00:27:53,999
You can ask them very nicely if they will like give you their data,

297
00:27:54,000 --> 00:27:59,390
if it's publicly available or like reanalyze the data to give you an effect estimate in the form that you need.

298
00:27:59,400 --> 00:28:02,459
When I was doing my Ph.D. and I was doing a meta analysis,

299
00:28:02,460 --> 00:28:07,830
I did that and I emailed a bunch of people and they really helped me like reanalyzed data so I could include things.

300
00:28:07,830 --> 00:28:09,899
And like I said, people are generally really receptive.

301
00:28:09,900 --> 00:28:13,920
If you reach out to them and tell them you're doing a review or meta analysis and ask for help.

302
00:28:17,210 --> 00:28:20,660
This is a table from that paper that I cited a few slides back that was sort

303
00:28:20,660 --> 00:28:23,900
of the introductory guide for starting a systematic review and meta analysis.

304
00:28:24,290 --> 00:28:29,090
This gives you a kind of a useful guide to how to come up with search terms.

305
00:28:30,080 --> 00:28:36,680
I'm sure you've all heard of like mesh subject headings. Certain databases like Medline use the mesh subject headings.

306
00:28:37,040 --> 00:28:40,580
There you can find them online at that link the NIH website.

307
00:28:40,910 --> 00:28:46,340
These are really, really helpful to put together and that they're kind of just a standardized set of search terms.

308
00:28:46,340 --> 00:28:54,350
And then you can also use free text terms. And then there are certain rules around like for example, like mesh terms have a slash after them.

309
00:28:54,680 --> 00:28:58,790
You can also put I don't know if it's kind of here, you're going to put a star at the end.

310
00:28:58,790 --> 00:29:07,070
You can have like fall with a star and the star can be any suffix like falls falling, things like that, so it can help you explode out your terms.

311
00:29:07,850 --> 00:29:14,600
I would recommend to talk to a university librarian when you are like developing your search term.

312
00:29:14,600 --> 00:29:20,929
And I think that's actually good practice for any any time you do a systematic review, whether if you do it at all in your career,

313
00:29:20,930 --> 00:29:25,489
whether you do it here at Michigan or somewhere else in the future, talk to your university librarian.

314
00:29:25,490 --> 00:29:29,230
The Tobin Health Sciences Library offers consultations with librarians.

315
00:29:29,240 --> 00:29:34,430
They also have free courses and workshops on systematic reviews, and I put a link here on the slide.

316
00:29:34,730 --> 00:29:41,510
So if you're thinking about doing a review like we have resources here at Michigan, I definitely recommend taking those up.

317
00:29:41,510 --> 00:29:44,749
And you might even see like in papers, people will say,

318
00:29:44,750 --> 00:29:50,110
we developed our search terms and search protocol in collaboration with librarian at the University of Michigan.

319
00:29:50,120 --> 00:29:54,649
It's really common and people put it in their methods and it's just kind of like nice.

320
00:29:54,650 --> 00:29:59,600
Like we we didn't just come up with this out of nowhere. We consulted an expert who really knows how to search literature really well.

321
00:30:02,690 --> 00:30:07,370
Okay. We're going to do a brief in-class activity and we might take even a little bit

322
00:30:07,370 --> 00:30:11,329
less than 10 minutes just so like you guys can all get out of here in good time.

323
00:30:11,330 --> 00:30:14,239
At the end, we're going to come up with mesh terms.

324
00:30:14,240 --> 00:30:19,430
So imagine you're conducting a systematic review of the research question What is the relationship between social

325
00:30:19,430 --> 00:30:27,680
network size and risk of all cause mortality among adults and what mesh and free text terms would you use?

326
00:30:27,680 --> 00:30:33,530
And then also ask yourself, would you add specificity to any areas of the research question in order to give yourself more targeted search terms?

327
00:30:34,220 --> 00:30:38,959
If you have laptops, I think it'll help you a little bit to look through.

328
00:30:38,960 --> 00:30:45,470
For example, the database which is there and you can just like have a word document and like list them out.

329
00:30:46,250 --> 00:30:51,560
You could even try a search term if you want and we'll regroup, let's say maybe like 7 minutes.

330
00:30:58,740 --> 00:31:02,190
Do you want to use my laptop to, like, look at search terms? Is it helpful? Yeah.

331
00:31:02,190 --> 00:31:07,140
I didn't bring any technology. Yeah. No, no worries at all. Okay.

332
00:31:08,100 --> 00:31:20,340
Oh. Disenfranchized and crazy.

333
00:31:23,960 --> 00:32:27,220
I'm like agonizing over think two weeks. Mm hmm.

334
00:32:28,740 --> 00:32:33,160
But, yeah, I guess I see the problem. Well.

335
00:32:35,930 --> 00:32:40,670
It's like a study of this index.

336
00:32:41,100 --> 00:32:54,490
What is this term? It's about. If you're only doing it more likely that maybe something like Instagram, like TikTok.

337
00:32:55,310 --> 00:32:58,790
I feel like that might be good somewhere else. Yeah.

338
00:32:58,790 --> 00:33:02,300
It's just like. It seems more like.

339
00:33:08,580 --> 00:33:17,370
So how would you like to be the. I guess they removed all the.

340
00:33:22,660 --> 00:33:29,670
We want. So I'm saying.

341
00:33:29,680 --> 00:33:34,770
What do you mean? More like. Like a network thing, like. Like your social network.

342
00:33:35,910 --> 00:33:42,750
Your friends are all over. Okay.

343
00:33:55,340 --> 00:33:58,720
The same thing is more.

344
00:34:02,330 --> 00:34:05,710
What is more like most of each other.

345
00:34:16,370 --> 00:34:19,700
And then the speakers text and I'm so tired.

346
00:34:22,190 --> 00:34:26,389
Yeah. That's why I didn't like the topic.

347
00:34:26,390 --> 00:34:30,840
And not just like. The word. Does that make sense?

348
00:34:32,130 --> 00:34:39,980
Because the social network is like. Right.

349
00:34:41,220 --> 00:34:48,160
You just. Could be the same thing.

350
00:34:48,730 --> 00:34:56,070
I think it was more like. What they would categorize or conversely, what you're saying.

351
00:34:57,250 --> 00:35:01,060
Celebrities start talking about just like. Instagram.

352
00:35:01,100 --> 00:35:05,469
And I think that's what you're talking about. You know, actually use the word social network.

353
00:35:05,470 --> 00:35:09,310
They might tag it as social networking because it's like that's what it's like.

354
00:35:10,210 --> 00:35:14,920
Under. Right.

355
00:35:16,360 --> 00:35:17,979
Am I thinking about that right? I didn't hear, he said.

356
00:35:17,980 --> 00:35:25,450
So I kind of like if you're doing like research, people are like just like using Instagram and TikTok and some real, like you say, social networking.

357
00:35:25,450 --> 00:35:29,290
And we're in it like the term that they would tag the social network.

358
00:35:29,290 --> 00:35:33,090
But even as a like yeah, it's times are like good question.

359
00:35:33,100 --> 00:35:40,540
I think if I was going to if I was interested in like social networks as measured using social media, I would,

360
00:35:41,140 --> 00:35:48,610
I would put in a lot of free text terms, like I would literally put Instagram right into social media, but it would be different from that.

361
00:35:48,910 --> 00:35:52,030
That would be different from that. Like this. Like tagging is like social network.

362
00:35:52,090 --> 00:35:57,340
Yeah, I would think so. But I don't know. I don't know if mesh terms really kind of like took it out like, okay,

363
00:35:57,340 --> 00:35:59,820
one's like what they're going to categorize and then once it what you're actually like,

364
00:36:00,280 --> 00:36:05,830
yeah, like I don't think, I don't think mesh terms like account for things like that.

365
00:36:07,090 --> 00:36:15,719
But I would put it in. And see what comes up because you can combine you combine search terms like and and or operators, right?

366
00:36:15,720 --> 00:36:25,620
So I do not think I removed all that really because you would put like all you would come up with your search terms that refer to your exposure.

367
00:36:25,800 --> 00:36:29,840
So anything related to like social network size with or link them with or.

368
00:36:29,940 --> 00:36:34,980
Right. And then all the terms related to your outcome, link those with or and then link those two buckets.

369
00:36:34,990 --> 00:36:38,490
Yeah. Right. Oh sorry.

370
00:36:38,760 --> 00:36:44,450
I feel like I said I remember like I think like I was like I think a lot of research awhile ago I contacted like the librarians and stuff.

371
00:36:44,470 --> 00:36:52,830
They were like, Oh, we actually don't use like when they call the operators our Boolean operators, they and they weren't, they don't use them anymore.

372
00:36:53,310 --> 00:36:56,670
Like I know was like that's how I learned to do search things. But like if you want.

373
00:36:56,910 --> 00:37:04,360
So then how do you call you. Well yeah, in PubMed like if you do a complex search like in the program.

374
00:37:05,050 --> 00:37:08,850
Yeah, great. So it makes it better. It's like Ovid.

375
00:37:09,270 --> 00:37:15,190
It's like, yeah, yeah. I use I've used all that in the past, not funded what was on.

376
00:37:15,720 --> 00:37:21,000
So I was like, I've stopped doing like, really complicated. Yeah. Like where you pull off asterisk at the end.

377
00:37:21,330 --> 00:37:26,450
Yeah. Yeah. I've actually never done that in PubMed whenever.

378
00:37:26,460 --> 00:37:29,460
So I've done two systematic reviews and I used Ovid for both.

379
00:37:29,610 --> 00:37:33,780
And that's what librarians recommended. And you can definitely use the and or operators.

380
00:37:33,780 --> 00:37:36,960
Yeah, yeah, yeah. This way. But this. Yeah.

381
00:37:37,260 --> 00:37:41,040
I don't see that like this stuff like this.

382
00:37:41,040 --> 00:37:52,700
I think like we would get better if you have all this social studies, how's everyone doing?

383
00:37:54,690 --> 00:38:03,200
Katie, can I ask you, how can I change? I want to show on like a web browser or how can I change the screen share, you know?

384
00:38:03,390 --> 00:38:11,070
And so. Sure.

385
00:38:24,320 --> 00:38:28,820
Oh, yeah.

386
00:38:29,720 --> 00:38:33,050
Oh, is that okay?

387
00:38:38,030 --> 00:38:43,700
Yeah. Oh, great. Oh, God.

388
00:38:43,960 --> 00:38:51,890
Oh, no. I can't go back there. I don't know how we went out there, and then it was great.

389
00:38:52,490 --> 00:39:00,230
Okay, so that's the place there. Oh, my God. That's happening. That's right.

390
00:39:00,710 --> 00:39:06,320
Oh, my God. You keep going. Okay, so general impressions.

391
00:39:06,320 --> 00:39:09,470
How did that go? Not good.

392
00:39:10,730 --> 00:39:17,270
Not at all. Why was it not good? I'm not really familiar with that, so I don't really know.

393
00:39:17,960 --> 00:39:23,390
Yeah. So in some sense, this is this is, like, kind of not fair because you're doing this for the very first time and it's kind of hard.

394
00:39:24,700 --> 00:39:25,640
Any other impressions?

395
00:39:32,110 --> 00:39:42,430
I think like it's interesting about MASH was that there's this group of guys out there that are kind of like creating the framework for like,

396
00:39:42,940 --> 00:39:52,420
what is there? Yeah. Like what you can search for. Like, with my interest in this ability, like, it's not work that's so it's really hard to search.

397
00:39:52,600 --> 00:39:58,140
Yeah, like, there's one term. Wow. Or, like, one group.

398
00:39:58,540 --> 00:40:02,230
That's really surprising. Oh, yeah. And so, um.

399
00:40:02,770 --> 00:40:09,759
Or are we order like otherwise like very much about clinical diagnoses, the things I'm interested in.

400
00:40:09,760 --> 00:40:12,800
So it's like. Yeah, it's kind of interesting.

401
00:40:12,820 --> 00:40:16,240
Yeah, that's interesting. I will say, in my experience, I will.

402
00:40:17,470 --> 00:40:19,360
I don't use MASH that much.

403
00:40:19,360 --> 00:40:24,970
I will throw in a few MASH terms that I think are relevant, but I've relied a lot more on open text, like free text search terms.

404
00:40:25,510 --> 00:40:28,690
So did anyone come up with any free text search terms?

405
00:40:28,690 --> 00:40:31,930
Like if you were just going to go to a database and search, like what would you put in?

406
00:40:35,840 --> 00:40:40,250
I mean, you could literally put social network size. That's like a perfectly valid search term.

407
00:40:41,950 --> 00:40:50,950
Like, I think a good way to kind of brainstorm or come up with search terms is think about like what in this overall topic, what are like.

408
00:40:51,280 --> 00:40:54,999
Like, you know, when you put keywords, when you publish a paper, there's keywords, right?

409
00:40:55,000 --> 00:41:00,400
You come up with 5 to 8 keywords that go at the end of the abstract. Or when you read a paper, you only see keywords.

410
00:41:00,820 --> 00:41:07,000
That is how stuff gets indexed. It's those keywords and it's also words that are in the title of the paper.

411
00:41:07,390 --> 00:41:10,720
So like what are kind of keywords? I mean, sorry, I've said this before.

412
00:41:10,750 --> 00:41:19,120
Like what are keywords associated with a topic? It might not be just social network size, it could be like social relationships or social engagement.

413
00:41:19,130 --> 00:41:23,770
Like you kind of want to, at least at the start, cast a wide net.

414
00:41:24,160 --> 00:41:24,850
Does that make sense?

415
00:41:26,110 --> 00:41:33,280
So and the general way that it's done is and if you sit down with a librarian, they'll teach you all of this like you'll come up if you.

416
00:41:33,280 --> 00:41:37,030
So you have your research question, assuming it's an observational question,

417
00:41:37,030 --> 00:41:42,069
you have your exposure and your outcome come up with a ton of keywords which are search terms like that,

418
00:41:42,070 --> 00:41:48,070
relate to your exposure, come up with a bunch that relate to your outcome, and then when you enter them in the database, you will link.

419
00:41:48,070 --> 00:41:51,969
Let's see, you've come up with like ten different search terms for your exposure.

420
00:41:51,970 --> 00:41:54,070
A couple of those might be official mesh terms.

421
00:41:54,070 --> 00:41:58,630
Some of them might just be free text that you're entering you would link them with, or so you might have.

422
00:41:58,930 --> 00:42:06,759
You're searching for a social network size or social relationships or social engagement or social support and like that would

423
00:42:06,760 --> 00:42:12,069
go into the database and then you would come up with all of your search terms for your outcomes of the outcomes mortality.

424
00:42:12,070 --> 00:42:16,300
You might have mortality or survival or death, right?

425
00:42:16,420 --> 00:42:20,770
And you would link those with or and then you have these two buckets of terms that refer

426
00:42:20,770 --> 00:42:24,490
to your exposure outcome and you would link those two buckets with an and operator.

427
00:42:24,910 --> 00:42:29,620
So when you have search terms, you can link your search terms with and or operators like filter,

428
00:42:29,620 --> 00:42:33,010
you know, like it's a Venn diagram or like you filter things in or out as needed.

429
00:42:33,760 --> 00:42:35,709
And then I just wanted to show an example.

430
00:42:35,710 --> 00:42:44,440
This is a systematic review that I like to see that is sort of on the same topic that I based that question on.

431
00:42:44,680 --> 00:42:50,190
So this has been published and this is how I get stuck over there.

432
00:42:50,360 --> 00:42:56,049
Okay, okay, okay. Okay. So this is a published systematic review and meta analysis.

433
00:42:56,050 --> 00:42:59,440
It's really highly cited. This paper has 4100 citations.

434
00:42:59,890 --> 00:43:02,590
It's about social relationships and mortality risk.

435
00:43:03,250 --> 00:43:11,400
And it's got to me, this is kind of vague, like this is the meta analysis of social relationships and mortality.

436
00:43:11,410 --> 00:43:19,360
Let's look at it a little bit in a little bit more depth here. So this is kind of contextualizing stuff, introduction, etc., etc.

437
00:43:19,360 --> 00:43:27,040
Let's look at their research question What is the overall magnitude of association of the

438
00:43:27,070 --> 00:43:30,770
of the association between social relationships and mortality across research studies?

439
00:43:30,770 --> 00:43:35,290
That's really vague, right? Like what aren't like how do you define social relationships?

440
00:43:35,290 --> 00:43:42,460
But then they get a bit more specific. Do structural versus functional aspects of social relationships differentially impact this for mortality?

441
00:43:42,820 --> 00:43:46,750
So here we're to get a bit more specifics about what we mean when we say social relationships,

442
00:43:46,750 --> 00:43:51,220
and then they're also interested in moderation by different factors.

443
00:43:51,520 --> 00:43:58,700
And then whether it's a gradient or a threshold effect, which is like, okay, I guess it depends how you measure it, you know.

444
00:43:58,960 --> 00:44:05,140
But anyway, I would recommend like maybe I'll give you this link and it can go as another example.

445
00:44:05,140 --> 00:44:11,530
This one I thought was really interesting here. They give a little bit more context on how they define social relationships.

446
00:44:11,530 --> 00:44:16,870
And you see they're very broad. They consider social relationships as well as social support.

447
00:44:18,670 --> 00:44:23,260
And it just seem to be really vague in terms of how they define social relationships.

448
00:44:23,710 --> 00:44:30,760
They talk about their data abstraction, but they really just kind of only talk about there is some specificity, but throughout the rest of the paper,

449
00:44:30,760 --> 00:44:35,290
they just talk about social relationships like high levels of social relationships and low social relationships.

450
00:44:35,290 --> 00:44:38,889
Is that like having a greater number of social relationships?

451
00:44:38,890 --> 00:44:44,320
Is it having like a tight, dense network with like a high frequency of interaction with social relations?

452
00:44:44,320 --> 00:44:51,820
Is it like family, friends? Is it support, like tangible things you get from social relationships like financial support or emotional support?

453
00:44:51,820 --> 00:44:56,800
It's really vague. So to me I would say like, don't do this.

454
00:44:57,310 --> 00:45:05,469
But again, I don't know, it was really splashy and got a lot of citations and you can even look a little bit more specifically and look,

455
00:45:05,470 --> 00:45:08,170
we're not going to do this, but they reviewed the like.

456
00:45:08,170 --> 00:45:13,420
These are the characteristics of the studies included, and maybe they give some specifics on the measures of social relationships,

457
00:45:13,870 --> 00:45:24,250
but somehow they decided that they could combine everything and come up with kind of like one single pooled odds ratio for mortality.

458
00:45:24,670 --> 00:45:33,190
I don't know. Anyway, I would say like, if you're going to do a review, don't be this vague, but it's a great topic.

459
00:45:33,370 --> 00:45:38,410
If you want like kind of a big splashy study, look at my lifestyle.

460
00:45:39,040 --> 00:45:43,529
I go write a lot. Michael Rowland okay, I got it.

461
00:45:43,530 --> 00:45:49,230
I got it. Okay then how do we sit back, sort of couch that point and just leave it?

462
00:45:50,280 --> 00:45:58,350
Okay, great. Okay, so we're going to come back. So for the Cancer and Alzheimer's Disease Review, let's come back to the supplied example.

463
00:45:58,800 --> 00:46:08,670
We for our protocol, we decided to include any articles published in any language in PubMed and based on psych info up to September 2nd, 2020.

464
00:46:09,000 --> 00:46:14,340
I would also commend there's kind of like a parent database called Ovid and I would recommend

465
00:46:14,340 --> 00:46:19,980
searching an Ovid and then you can select other kind of sub databases within Ovid to search.

466
00:46:21,060 --> 00:46:26,340
It's generally best to not restrict the language of the study just to not, you know,

467
00:46:26,370 --> 00:46:31,240
just to be as inclusive as possible and not restricted to English language.

468
00:46:31,260 --> 00:46:37,860
It reduces chances of publication bias. In any case, you probably won't find many studies that are published not in English, but if there are,

469
00:46:38,550 --> 00:46:45,060
try to try to get them and try to translate them so that you can read them if you can't read whatever language they're published in.

470
00:46:46,890 --> 00:46:56,100
Our search terms were really, really simple. We used neoplasia or cancer or malignancy to refer to our exposure and cognitive function or

471
00:46:56,100 --> 00:47:01,770
cognitive impairment or Alzheimer with the star to allow any suffix to refer to the outcome.

472
00:47:02,040 --> 00:47:08,850
And then we had and epidemiologic study or cohort or case control or longitudinal to refer to the study design that we wanted.

473
00:47:09,540 --> 00:47:15,329
And then and adult or middle age or elder. These were these age search terms are used.

474
00:47:15,330 --> 00:47:19,500
I'm not sure if these are technically mesh terms. Maybe they are, but that's what the database used.

475
00:47:20,130 --> 00:47:26,640
So that's how we came up with it and that's how we combined the end or filters to restrict to exposure, outcome, study, design, a population.

476
00:47:27,450 --> 00:47:33,120
And we did this with the librarian at UCSF, the inclusion criteria.

477
00:47:33,120 --> 00:47:36,059
So these are really important to come up with. And having setting good exclusion,

478
00:47:36,060 --> 00:47:42,270
inclusion exclusion criteria will make it kind of tractable for you to know which studies to include

479
00:47:42,270 --> 00:47:47,640
and which to exclude when you get to the point of actually returning papers from your database search.

480
00:47:48,480 --> 00:47:53,670
So we, we wanted longitudinal cohort or case control study designs cross-sectional were not of good

481
00:47:53,670 --> 00:47:57,840
enough quality because they don't establish that temporality between exposure and outcome.

482
00:47:58,440 --> 00:48:02,370
And we didn't want study designs that required mortality data to ascertain the outcome.

483
00:48:02,370 --> 00:48:07,829
This one was specific to cancer and a D is an outcome because sometimes there's like

484
00:48:07,830 --> 00:48:12,270
autopsy based studies where they determine at pathology from like brains postmortem.

485
00:48:12,270 --> 00:48:17,700
And we didn't want people who had died. We wanted living people to capture like kind of incident surviving cases.

486
00:48:18,360 --> 00:48:20,610
The exposure was a history of cancer at baseline.

487
00:48:20,910 --> 00:48:28,350
And we split these into like prevalent cancers and incident cancers that occurred over the follow up in longitudinal studies.

488
00:48:28,350 --> 00:48:33,149
Right? So here we're kind of trying to get at or avoid prevalent incidence bias, right.

489
00:48:33,150 --> 00:48:38,910
We wanted to separate out these two because we were worried about studies that used prevalent ongoing cases.

490
00:48:39,650 --> 00:48:43,979
Um, the comparison group were people with no cancer history at baseline or no cancer

491
00:48:43,980 --> 00:48:48,180
history at each longitudinal follow up for the time varying incident cancer.

492
00:48:48,570 --> 00:48:51,600
The outcome, like I said, was incident, Alzheimer's disease or dementia,

493
00:48:51,900 --> 00:48:56,670
and studies were included if they had a measure of association with a 95% confidence interval.

494
00:48:56,670 --> 00:49:00,900
So this was a risk ratio, hazard ratio, incidence density ratio or an odds ratio.

495
00:49:02,640 --> 00:49:09,060
So after you set your search strategy, including your search terms and your inclusion criteria, then you make your data extraction.

496
00:49:09,720 --> 00:49:13,740
So like, what information are you interested in getting out of all these studies that you are returning?

497
00:49:14,070 --> 00:49:19,020
And I recommend making a table of like all the variables that you want to extract.

498
00:49:19,020 --> 00:49:24,700
And this is going to be essentially like the data that you analyze in both a systematic review and meta analysis.

499
00:49:24,720 --> 00:49:29,340
So you might have columns for all the variables you want to pull and rows for each individual study.

500
00:49:29,700 --> 00:49:34,829
So you might have a column that is like refers to like the study ID that you set

501
00:49:34,830 --> 00:49:39,569
or the name of the paper column for the year in which the study was conducted.

502
00:49:39,570 --> 00:49:44,940
A column for the sample size, a column for like what their exposure variable was, a column for what their outcome variable was and so on.

503
00:49:46,620 --> 00:49:50,520
Yeah. And then this table is kind of going to form the basis of a descriptive results table.

504
00:49:50,790 --> 00:49:54,089
And your systematic review, I'm sure if you read a systematic review,

505
00:49:54,090 --> 00:49:58,320
you've seen that massive table where they summarize characteristics of the included studies.

506
00:49:59,220 --> 00:50:04,260
And to kind of give you an example of what this looks like when you're kind of in the day to day process of doing it.

507
00:50:04,590 --> 00:50:10,620
This is the table that Monica she was the study first author that Monica and I used for our systematic review.

508
00:50:11,040 --> 00:50:14,370
So we set we assigned the studies of studied.

509
00:50:15,060 --> 00:50:19,920
So this is after the process when we downloaded the abstract screen,

510
00:50:19,920 --> 00:50:24,600
then we had already identified which full text articles we were going to include in our review.

511
00:50:25,290 --> 00:50:28,320
So we renamed them to get to give kind of a random study ID.

512
00:50:28,830 --> 00:50:36,479
And then we had all of these variables that we created that we pulled information or extracted the information out from each study.

513
00:50:36,480 --> 00:50:38,520
So we had, you know, like the age,

514
00:50:38,520 --> 00:50:44,729
the measure of like very variability in the age some studies gave an IQ are some gave us standard deviation, some gave a range.

515
00:50:44,730 --> 00:50:48,780
So it was like a mess, right? When you do this, it will be a mess. That's all I can say.

516
00:50:48,780 --> 00:50:53,280
It's a nice thing to go back and refine in order to capture the things you think you

517
00:50:53,280 --> 00:51:00,149
need to capture the mail and then see things and read our notes that I was dropping,

518
00:51:00,150 --> 00:51:06,870
right? Or notes that I was giving. Or I think this is where we identified problems in terms of like we couldn't get the

519
00:51:06,870 --> 00:51:11,190
information that we wanted to get as defined in the variable that we had put forward.

520
00:51:11,460 --> 00:51:15,960
Or Monica and I had some kind of disagreement about what that piece of information was.

521
00:51:15,960 --> 00:51:19,350
And as you can see, there's a lot of read, right like here.

522
00:51:19,350 --> 00:51:24,420
I think this is the risk ratio. So this column W is like the measure that the study gave.

523
00:51:24,810 --> 00:51:30,390
And then I think we actually gave the measure here like point estimate and then the lower and the upper confidence interval.

524
00:51:30,400 --> 00:51:33,240
You can see why. That's right. Oh, shoot. I'm sorry. I thought you could see my ass.

525
00:51:33,930 --> 00:51:41,639
So, like, column double W is the measure of association called X is the point estimate for that association.

526
00:51:41,640 --> 00:51:45,360
And then Y and Z are the lower and upper limits of that confidence interval.

527
00:51:46,020 --> 00:51:52,920
So you can see there's like a lot of disagreement. And what I would recommend doing is having two different reviewers at this stage.

528
00:51:53,160 --> 00:51:58,380
So have two people fill out this data extraction table. And it doesn't even need to be like every single study.

529
00:51:58,680 --> 00:52:04,379
You might each do like ten together and then go and check the concordance of your ten.

530
00:52:04,380 --> 00:52:09,510
And if they're looking good and you're generally agreeing just for efficiency, one reviewer can take over.

531
00:52:09,510 --> 00:52:13,530
If you want to be extremely thorough, you can do them all or a greater number,

532
00:52:14,070 --> 00:52:18,680
but it's generally good to have two different people and then just check and kind of resolve through discussion.

533
00:52:19,500 --> 00:52:24,180
So we worked with this like really, really messy Excel table for quite a long time.

534
00:52:24,180 --> 00:52:26,790
We had to go back and revise and reiterate,

535
00:52:26,790 --> 00:52:33,270
but this messy Excel table ultimately became this really nice table that was like in our final published paper.

536
00:52:34,200 --> 00:52:39,720
So having a good data extraction table is kind of like fundamental and ultimately becomes part of your results.

537
00:52:41,180 --> 00:52:46,440
Assess risk of bias. So as we all know, like reviews are garbage in, garbage out.

538
00:52:46,460 --> 00:52:53,090
I'm sure like many of you have heard that phrase, if not like one of the rules of systematic reviews is it's garbage in, garbage out.

539
00:52:53,090 --> 00:52:57,169
If you're reviewing, if you've like, you know, you've done everything that I've just talked about.

540
00:52:57,170 --> 00:53:00,200
Come up with your question, copy, search terms, etc.

541
00:53:00,230 --> 00:53:05,059
You've pulled maybe, let's say like 20 relevant studies from the literature if they're all bad,

542
00:53:05,060 --> 00:53:08,780
like really bad quality studies, like you're not going to conclude much.

543
00:53:08,780 --> 00:53:15,380
That's really helpful, right? Like the results of your review are only as good as the results of the individual studies that you're reviewing.

544
00:53:16,290 --> 00:53:23,299
Um, and we all know from this course so far, like what these kinds of, like sources of poor quality can be, right?

545
00:53:23,300 --> 00:53:27,680
Usually it's threats to internal validity, like confounding selection bias and so on.

546
00:53:29,060 --> 00:53:33,830
So but it's really important in a systematic review to do a formal risk of bias assessment.

547
00:53:34,380 --> 00:53:40,160
Um, and there are criteria such as checklists that have been published that you can use to kind of weight or group

548
00:53:40,160 --> 00:53:45,830
studies according to risk of bias to common tools of the Newcastle Ottawa scale and the Cochrane Risk of bias scale.

549
00:53:45,830 --> 00:53:51,320
I've linked to these in the slides so you can kind of go through and look at them on your own later if you want.

550
00:53:52,040 --> 00:53:57,529
The Newcastle Ottawa scale is used in case control and cohort studies and it basically just assigns a score

551
00:53:57,530 --> 00:54:03,770
which is a number or a number of stars to each study included in the review based on quality criteria.

552
00:54:04,160 --> 00:54:09,440
So you'll have to go through and read each study to ascertain like the, um, you know,

553
00:54:09,440 --> 00:54:14,719
the number of stars you would give it according to each criteria in the Newcastle Ottawa scale.

554
00:54:14,720 --> 00:54:19,730
So things like, you know, non differential responses in cases and controls, having the controls,

555
00:54:19,730 --> 00:54:22,910
being representative of the underlying population in case control studies.

556
00:54:23,210 --> 00:54:27,380
So this is kind of where your core F skills are going to come in in assessing the risk of bias,

557
00:54:27,410 --> 00:54:31,670
um, which is considered the same thing as quality in a systematic review.

558
00:54:32,750 --> 00:54:38,420
Cochrane Risk of bias tool. This is kind of cool. You can find it at risk of bias info.

559
00:54:38,420 --> 00:54:42,790
And then there's also an R app called Rob Vibes, which I've never used.

560
00:54:42,820 --> 00:54:47,809
It looks quite interesting and it kind of gives this kind of visual kind of heatmap of risk of bias

561
00:54:47,810 --> 00:54:53,360
across domains where you have maybe like five different domains where you're worried about risk of bias.

562
00:54:53,360 --> 00:54:59,120
These are defined out here. So bias due to randomization, bias due to missing data, etc., that's called three.

563
00:54:59,420 --> 00:55:03,530
And then you have each of your studies in the rows. So I recommend taking a look at this.

564
00:55:03,530 --> 00:55:11,750
If you do a systematic review for our cancer and a D review because we were so concerned about bias,

565
00:55:12,080 --> 00:55:18,830
we really thought this inverse cancer association was due to bias and not a real common cause.

566
00:55:18,830 --> 00:55:28,340
We wanted to like take it a step further than just applying like a generic checklist and do a really kind of thoughtful bias assessment.

567
00:55:28,340 --> 00:55:31,940
So we use dags you've all learned to ex at this point, I'm assuming, right?

568
00:55:32,360 --> 00:55:45,770
So we used Dags to kind of think about biases or data structures that could give rise to the Inverse Cancer Aid Association in a serious way.

569
00:55:46,070 --> 00:55:53,030
And in the systematic review we illustrated it using two eggs as shown in this figure, which I copied from the review itself.

570
00:55:54,110 --> 00:55:59,599
So I'm going to talk these through and actually I hope this will be kind of like a useful DAG review in addition

571
00:55:59,600 --> 00:56:04,790
to just like this is a neat method you could try and a systematic review when you're concerned about bias.

572
00:56:05,630 --> 00:56:11,750
So Dag ay in the upper left corner is kind of like the causal no bias scenario, right?

573
00:56:11,750 --> 00:56:18,590
Like cancer causes reduced risk of ad that's kind of like that's the no bias scenario which is helpful to draw out.

574
00:56:18,590 --> 00:56:25,549
This is a reference dag b shows confounding bias by an unknown common cause.

575
00:56:25,550 --> 00:56:29,390
You write just the standard confounding DAG. That's also a possibility.

576
00:56:29,810 --> 00:56:33,610
In this case, we don't actually think cancer causes AIDS.

577
00:56:33,950 --> 00:56:37,249
If there is a true inverse etiological association,

578
00:56:37,250 --> 00:56:45,799
maybe there's something like a genetic or biological factor that is kind of linking carcinogenesis and neurodegeneration.

579
00:56:45,800 --> 00:56:52,520
So maybe there's some kind of inverse genetic regulation of the carcinogenesis and neurodegeneration processes.

580
00:56:52,820 --> 00:57:00,900
We kind of don't think that's the case. But if it's causal, this is maybe why it's not actually like cancer itself causing AIDS.

581
00:57:01,010 --> 00:57:06,889
We think there's actually like a common cause that's acting in inverse directions with these two conditions.

582
00:57:06,890 --> 00:57:09,740
So we felt like that DAG was important to draw out.

583
00:57:10,040 --> 00:57:17,930
And in the text we specify that scenario B is actually not necessarily bias, but there's some other common factor that we haven't identified.

584
00:57:18,200 --> 00:57:22,340
So studies that show that Inverse Cancer AIDS Association, it's not necessarily causal,

585
00:57:22,670 --> 00:57:28,100
but there's some unknown common cause that hasn't been identified yet that we would want to identify if it's real.

586
00:57:28,100 --> 00:57:32,270
Right. Scenario B is confounding by known confounders.

587
00:57:32,270 --> 00:57:36,080
So these would be things like you didn't adjust your model for education.

588
00:57:36,410 --> 00:57:39,590
And education we know is associated with reduced risk of ad.

589
00:57:39,740 --> 00:57:44,510
But it's. Actually associated with increased risk of some cancer types due to kind of

590
00:57:44,510 --> 00:57:48,800
like socioeconomic differences in various exposures and access to diagnosis.

591
00:57:49,580 --> 00:57:53,810
And some people really don't do this, I will say. So we wanted to recognize that.

592
00:57:53,960 --> 00:58:00,110
Now the dogs get more complicated. D Here is adjustment for factors influenced by cancer.

593
00:58:00,120 --> 00:58:06,529
So you adjusted for something like cancer treatment, which is downstream a factor or downstream of cancer.

594
00:58:06,530 --> 00:58:09,829
There's some other you variable that's associated with that downstream factor.

595
00:58:09,830 --> 00:58:15,570
And can someone tell me what this biases? Like What is this general form of bias?

596
00:58:23,550 --> 00:58:28,570
I hope you learn this because you definitely need to know what I'm looking for when I go more in-depth and exemplary.

597
00:58:28,620 --> 00:58:32,280
Okay. Okay then maybe you haven't learned any of the collider stratification bias.

598
00:58:32,640 --> 00:58:40,620
Right. You have arrows going into the cancer treatment or other downstream factor which has a box around it because we're conditioning on it.

599
00:58:40,620 --> 00:58:47,220
So we've conditioned on the collider, which is opening up a spurious path between cancer and which you can see if you just follow,

600
00:58:47,460 --> 00:58:52,920
ignoring the directionality of the arrows, you can see through the arrows opening up.

601
00:58:53,310 --> 00:58:58,020
I guess you'll learn more of a closed application bias. Super important. It's a it's a basic form of selection bias.

602
00:58:58,620 --> 00:59:04,679
But here, this kind of over adjustment by something that's on the causal pathway potentially or a downstream

603
00:59:04,680 --> 00:59:13,979
of your exposure can also open up a quite a stratification bias is diagnostic bias of status have

604
00:59:13,980 --> 00:59:18,910
you in your information bias yet okay yeah so we want what have you and what information bias is dags

605
00:59:20,040 --> 00:59:25,170
seen it okay yeah so here this is essentially information bias or like misclassification right.

606
00:59:25,530 --> 00:59:30,130
So we have a cancer diagnosis might lead to a diagnostic error in a DD,

607
00:59:30,360 --> 00:59:38,429
whereas actual diagnosis diagnosed is a function of someone's actual true status as well as diagnostic error in their diagnosis.

608
00:59:38,430 --> 00:59:47,040
So you could imagine plausibly people like older people like this is a study of older people or older people who have a history of cancer maybe.

609
00:59:47,040 --> 00:59:54,119
And I actually think this could go in both directions, maybe like more or less likely to be diagnosed with dementia.

610
00:59:54,120 --> 01:00:01,319
First, someone having cognitive problems who's being seen in the clinic for follow up after cancer treatment, their physician might miss it.

611
01:00:01,320 --> 01:00:06,750
Attribute symptoms of dementia as cognitive problems being induced by cancer therapy

612
01:00:07,440 --> 01:00:12,140
that could actually happen and that could lead to diagnosis in people who have cancer.

613
01:00:12,150 --> 01:00:19,709
On the flip side, we think it's also possible that increased medical surveillance due to going in for cancer follow up could actually work

614
01:00:19,710 --> 01:00:25,430
the opposite direction and increase the likelihood of diagnosis of dementia due to increased medical surveillance.

615
01:00:25,430 --> 01:00:26,340
So we really don't know.

616
01:00:26,700 --> 01:00:30,989
It could go either way and we're doing some of the follow up studies on the topic, but that's like a conversation for another day.

617
01:00:30,990 --> 01:00:35,190
So diagnostic bias and that is how you would draw a diagnostic bias on a DAG,

618
01:00:35,430 --> 01:00:45,000
where a diagnosis of a condition is a function of the true condition someone has as well as diagnostic error in the condition competing risks bias.

619
01:00:45,420 --> 01:00:49,890
I'm sure you will learn about this if you haven't already. So essentially I think this sounds really straightforward.

620
01:00:50,190 --> 01:00:54,390
Cancer reduces lifetime life expectancy, which lowers the lifetime risk of 80.

621
01:00:54,690 --> 01:00:58,550
So just simply dying is a competing risk to 80. Right.

622
01:00:59,070 --> 01:01:04,020
And then finally, selective survival bias. This is another collider stratification dag on the bottom, right?

623
01:01:04,320 --> 01:01:07,320
So having cancer reduces the likelihood of survival.

624
01:01:07,320 --> 01:01:13,260
And if there are other unmeasured factors that are also related to survival and protective of aid,

625
01:01:13,860 --> 01:01:19,679
then we have a collider stratification bias known here as a selective survival bias, right?

626
01:01:19,680 --> 01:01:26,669
So kind of all else being equal, someone who survives longer after a cancer diagnosis might have other unmeasured health promoting

627
01:01:26,670 --> 01:01:31,860
factors that are like enhancing their chances of survival and also reducing their risk.

628
01:01:32,190 --> 01:01:37,350
And that was the one we were actually kind of most concerned about here. Any questions about the death?

629
01:01:39,790 --> 01:01:40,680
Okay. Move up.

630
01:01:42,900 --> 01:01:50,820
And yeah, I would say like so using these the checklists I showed you in the last few slides are really not the only way to assess risk of bias.

631
01:01:50,820 --> 01:01:57,120
You can come up with your own criteria. Absolutely. And you'll see people do this a lot of the time because you might have a research question

632
01:01:57,120 --> 01:02:01,350
that doesn't fit neatly into kind of the boxes that are available on existing checklists.

633
01:02:01,680 --> 01:02:05,040
Sometimes you have to get creative about the specific types of bias that might

634
01:02:05,040 --> 01:02:09,180
apply to your research question at hand and the ways that it's been studied.

635
01:02:09,450 --> 01:02:15,179
So that's what we aim to do here. So we've kind of gone back and forth.

636
01:02:15,180 --> 01:02:20,909
But of course, the next step after you've come up with your inclusion research question and inclusion criteria,

637
01:02:20,910 --> 01:02:24,410
search terms make a template of your data extraction form.

638
01:02:24,510 --> 01:02:27,390
Think about how you're going to assess risk of bias, do your search.

639
01:02:27,810 --> 01:02:33,120
So everything that we just talk about leading up to this step is going to go into your protocol and then you do your search.

640
01:02:34,170 --> 01:02:37,020
And the search consists of two steps as a first screen.

641
01:02:37,020 --> 01:02:41,729
In a second screen, the first screen is when you just you literally you put in your search terms into the database,

642
01:02:41,730 --> 01:02:45,000
you get back abstracts, you're going to get titles and abstracts.

643
01:02:45,000 --> 01:02:52,950
You're not going to get full papers yet. I mean, you can click and download them, but you'll go through those abstracts first and not the full papers.

644
01:02:53,250 --> 01:02:57,480
And this is just to help manage the volume of results that you're going to get back.

645
01:02:57,510 --> 01:03:03,690
You're going to get a few hundred to several thousand abstracts back, and you're going to have to screen through all of them.

646
01:03:04,080 --> 01:03:10,110
If you get a totally unwieldy number of abstracts back, maybe you need to go back and refine your search terms to be a bit more focused,

647
01:03:10,110 --> 01:03:14,780
like you shouldn't be getting 10,000 abstracts back, you should maybe be getting like up to a thousand.

648
01:03:15,030 --> 01:03:23,640
I would say. Um, and it's really good to have like two reviewers do this independently, both at the abstract screen as well as the full text screen.

649
01:03:23,970 --> 01:03:29,700
So maybe you take like a certain number of abstracts you screen through them and then you compare and check your results.

650
01:03:30,360 --> 01:03:36,389
And this is just to ensure kind of integrative reliability and you record how many abstracts you're including

651
01:03:36,390 --> 01:03:41,250
and how many you're excluding the second screen for all those abstracts that you decide to include,

652
01:03:42,000 --> 01:03:49,860
you're going to download the texts in like full PDF form and read through them to further confirm that they meet your eligibility criteria.

653
01:03:49,860 --> 01:03:56,040
So the abstract is usually going to give you enough information to say, Oh, this study is definitely not eligible, I'm excluding it.

654
01:03:56,370 --> 01:03:58,979
But if you read an abstract and you think, okay,

655
01:03:58,980 --> 01:04:03,450
this article is potentially includable like I can't exclude it on the basis of the abstract you include

656
01:04:03,720 --> 01:04:08,820
and then you read the full paper and again you should have two people doing this and check results.

657
01:04:09,780 --> 01:04:15,299
This is a Prisma flow diagram. If you've read a systematic review or done one in the past,

658
01:04:15,300 --> 01:04:22,140
you've seen one of these and this is just kind of good practice is to do a Prisma flow diagram.

659
01:04:23,010 --> 01:04:27,570
The Prisma is the preferred reporting items for systematic reviews and meta analysis.

660
01:04:27,900 --> 01:04:32,820
It's kind of like if you've seen research checklists, like there's the strobe statement for observational studies,

661
01:04:34,110 --> 01:04:41,160
a lot of journals will kind of require you to do your systematic review according to the PRISMA statement or guidelines like the strobe,

662
01:04:42,090 --> 01:04:43,620
like the strobe checklist, for example.

663
01:04:43,950 --> 01:04:51,450
And I would definitely recommend when you are coming up with your protocol, have the Prisma checklist on hand and it's linked here so you'll,

664
01:04:51,450 --> 01:04:55,809
you know, record the number of records identified through your database searching the number of records.

665
01:04:55,810 --> 01:04:57,240
Oh yeah. Always remove duplicates.

666
01:04:57,240 --> 01:05:03,000
And that's like an easy automated step you can do in databases and then record the number of records screened and excluded.

667
01:05:03,000 --> 01:05:08,639
This is at the abstract stage here, and then record the number of full texts that you read in full for eligibility,

668
01:05:08,640 --> 01:05:16,620
the number excluded and why you excluded them, and then the number of studies included in the qualitative synthesis, and that's the systematic review.

669
01:05:16,890 --> 01:05:21,150
And then the number of studies included in the quantitative synthesis, which is the Met analysis.

670
01:05:21,600 --> 01:05:25,230
And then here you can also see the number of records identified through other sources.

671
01:05:25,560 --> 01:05:27,600
So that would be things like searching reference lists,

672
01:05:27,600 --> 01:05:33,840
maybe like just you knew this paper existed and so you included it because you've done your own kind of background reading on the topic.

673
01:05:34,470 --> 01:05:37,890
You found a paper through like personal communication with an expert in the field.

674
01:05:37,890 --> 01:05:41,160
You found something through searching great literature or theses, for example.

675
01:05:43,600 --> 01:05:47,160
And this is what our Prisma flow diagram looked like for the cancer aid review.

676
01:05:47,440 --> 01:05:56,440
So we found 3000, just over 3000 records through database searching, and we had 2764 after removing duplicates.

677
01:05:56,830 --> 01:06:02,020
We read all 2764 of these abstracts, and we excluded most of them.

678
01:06:02,020 --> 01:06:05,650
As you can see, like 2500 or just not relevant.

679
01:06:05,710 --> 01:06:12,220
And maybe there are things we could have done to further refine our search terms to avoid having so many irrelevant abstracts.

680
01:06:12,310 --> 01:06:15,060
It took a lot of time to read through them just to throw them out, right?

681
01:06:17,050 --> 01:06:25,210
We ended up with 51 full text articles that we downloaded and assessed for eligibility and we of those we excluded 29 And here are the reasons why.

682
01:06:25,840 --> 01:06:33,669
A really common reason was someone had or a paper had cognitive impairment prevalent ad rather than incident ID or cancer as the outcome was,

683
01:06:33,670 --> 01:06:38,890
we want to cancel the exposure. Six were commentaries or review articles, not original research articles.

684
01:06:39,250 --> 01:06:42,400
One was the same like two papers on the same study population.

685
01:06:43,300 --> 01:06:48,760
And then a couple more reasons. And then we included 22 studies in our narrative synthesis.

686
01:06:48,760 --> 01:06:50,020
So a systematic review.

687
01:06:53,440 --> 01:06:59,910
And then the next step is to report your results so you can base the reporting of your results around your data extraction form.

688
01:06:59,920 --> 01:07:02,629
So once you kind of have your number of studies,

689
01:07:02,630 --> 01:07:10,209
you're going to include like get all of the relevant information into your data extraction form and then write it up as a narrative.

690
01:07:10,210 --> 01:07:13,540
And this is kind of like I can't really tell you exactly how to do it.

691
01:07:13,540 --> 01:07:16,480
It's just a narrative discussion of the results and different research questions

692
01:07:16,780 --> 01:07:19,210
are going to have different things that you're going to want to emphasize.

693
01:07:19,540 --> 01:07:24,190
Like in this study, for example, we talked about how the exposures and outcomes are measured.

694
01:07:24,460 --> 01:07:29,560
We talked about results according to the different cancer types we're interested in and kind of general things.

695
01:07:29,560 --> 01:07:32,440
You want to comment on kind of your overall impressions of the literature,

696
01:07:33,100 --> 01:07:37,750
notable or unique studies that were identified that help give a little bit more insight into the topic.

697
01:07:38,080 --> 01:07:41,200
And this is where it gets a bit subjective of like how you want to kind of structure your narrative.

698
01:07:42,040 --> 01:07:49,120
And my, my advice would be just like read a few systematic reviews to get familiar with kind of what this narrative synthesis looks like.

699
01:07:51,710 --> 01:07:54,830
Yeah, I said I mentioned Fresno before, and like I said,

700
01:07:54,830 --> 01:07:59,750
Prisma is the gold standard criteria of items that should be reported in a systematic review and meta analysis.

701
01:07:59,750 --> 01:08:06,500
So if you end up doing one download the Prisma checklist and make sure you're following it when you're doing your review,

702
01:08:06,500 --> 01:08:11,210
and then you kind of know that you're kind of checking all the boxes of things you need to do to ensure a good quality review.

703
01:08:13,350 --> 01:08:20,390
Okay. Moving on to meta analysis. Um, I would say normally like let's take a break at this point,

704
01:08:20,410 --> 01:08:26,460
but how do people feel like feel if we kind of just go through and then we can leave a little bit early as opposed to having a break?

705
01:08:28,080 --> 01:08:35,010
There's only, like 15 slides left. Good. Um, and if anyone needs to, like, get up and go to the washroom, like, that's fine.

706
01:08:36,400 --> 01:08:43,110
A meta analysis is a statistical method for pooling measures of association or effect estimates from studies that are included in a

707
01:08:43,110 --> 01:08:51,750
systematic review such that you generate a single pooled effect estimate from your kind of body of studies you've returned in your review.

708
01:08:52,500 --> 01:08:57,840
Most meta analytic methods are variations on a weighted average of an effect estimate from included studies.

709
01:08:57,870 --> 01:09:02,280
That's it. Like you're just taking. So now your your unit of observation is the study.

710
01:09:02,610 --> 01:09:08,249
You're going to pull the main effect estimate from that study whether it's and you've said what you're interested in earlier,

711
01:09:08,250 --> 01:09:09,390
whether it's an odds ratio,

712
01:09:09,690 --> 01:09:14,970
hazard ratio, risk ratio, you're going to pull those point estimates and you're essentially going to come up with a weighted average.

713
01:09:15,300 --> 01:09:18,209
And it's typically weighted based on the sample size of the study. Right.

714
01:09:18,210 --> 01:09:25,530
Like we give higher weight to larger studies because they're estimated more precisely, meta analysis can give you an extremely,

715
01:09:25,530 --> 01:09:30,540
extremely precise pooled estimate because it's coming from so many individuals, wonderful together.

716
01:09:31,830 --> 01:09:37,440
But like I said, it's garbage in, garbage out. And there are a lot of bad meta analyzes out there.

717
01:09:37,440 --> 01:09:45,239
There are a lot of bad meta analyzes that combine estimates like you could take ten random studies that present a hazard ratio and combine them.

718
01:09:45,240 --> 01:09:51,480
But like, what does it mean? Right? You have to make sure that you are combining like you're comparing apples with apples, right?

719
01:09:51,510 --> 01:09:57,270
You don't want to be you want to be combining effect estimates from studies that are studying the same thing that you can validly

720
01:09:57,270 --> 01:10:04,710
compare and say this is a meaningful like pooled hazard ratio for the association between social network size and mortality.

721
01:10:04,980 --> 01:10:10,560
And all of these studies measured social network size like using the same measure or definition of social network size.

722
01:10:10,680 --> 01:10:19,710
Yeah. Like if you have two studies in your review that say he's engaged, so they're the same people, oh, can you do a measure now?

723
01:10:20,310 --> 01:10:25,140
That's a good question. I see. I think that's kind of a subjective decision.

724
01:10:25,830 --> 01:10:30,870
My feeling like in this study we ended up we had a similar situation.

725
01:10:30,870 --> 01:10:32,909
It wasn't anything I don't remember what cohort it was,

726
01:10:32,910 --> 01:10:39,420
but there were two studies published on the same population and we excluded one because we didn't want to double count people in different ways.

727
01:10:39,840 --> 01:10:43,540
Yeah. Private therapy.

728
01:10:43,560 --> 01:10:51,740
That's determined by the. Like the study. Yeah. Yeah, I would I would look really closely.

729
01:10:51,740 --> 01:10:56,840
I wouldn't I would hesitate if it meant double counting the same people in a panel.

730
01:10:57,770 --> 01:11:06,979
But maybe if like I don't know if it's taken, if it's a longitudinal study and they've taken measures from different years, like cohorts,

731
01:11:06,980 --> 01:11:13,730
like for example, I work with aging studies and a lot of aging studies will add refresher cohorts at the bottom end of the age range.

732
01:11:14,120 --> 01:11:20,330
If they if the two papers have done analysis in such a way that they represent different segments of the cohort over time, then maybe.

733
01:11:20,930 --> 01:11:24,770
But I would just avoid double counting people would be my advice. So yeah.

734
01:11:26,730 --> 01:11:29,969
Um, meta analysis can only be conducted in certain conditions.

735
01:11:29,970 --> 01:11:35,220
And what I mean by that is studies must be homogenous enough to be pulled together in a meta analysis.

736
01:11:35,640 --> 01:11:39,000
So one numerical estimates of the same type are required.

737
01:11:39,000 --> 01:11:43,130
You cannot combine an odds ratio with the hazard ratio or incidence density ratio, right?

738
01:11:43,140 --> 01:11:49,890
Like that should be intuitive. Like they're just mathematically different. And then there are these qualitative or theoretical considerations.

739
01:11:49,890 --> 01:11:52,830
That's what I mean when I say compare apples to apples,

740
01:11:53,190 --> 01:12:00,870
like are your exposure and outcome measures like the variables that are in the study, are they measuring the same thing?

741
01:12:01,290 --> 01:12:04,510
Are you combining a measure of social support?

742
01:12:04,530 --> 01:12:09,209
Let's say like you have some measure of social support from an interview study and like you

743
01:12:09,210 --> 01:12:13,350
ask people like are your family and friends socially supportive or something like that?

744
01:12:13,350 --> 01:12:17,010
And you've got a scale, let's say, new dichotomies of high versus low social support.

745
01:12:17,460 --> 01:12:24,570
And then another study looks at social network size and then defines like you have ten friends and you have 20 friends, you have 30 friends.

746
01:12:24,840 --> 01:12:32,790
And then they call it a mises two high, low social network size. If you want to meta analyze those two things, is that high, low measure, the high,

747
01:12:32,790 --> 01:12:37,470
low measure of social support in the high, low measure of social network size? Sure, you can combine them.

748
01:12:37,920 --> 01:12:42,930
It's a dichotomous variable and maybe both looked at mortality. But what does it mean when you pull them together?

749
01:12:42,930 --> 01:12:46,469
Those are two different concepts and now you're pulling them. Right.

750
01:12:46,470 --> 01:12:52,470
And I think that's what that meta analysis that I just showed you, the social relationships and mortality risk, that's, I think what they did.

751
01:12:53,460 --> 01:13:01,020
So like theoretically, does that make sense? And then other things like aside from what variables are measured, who are you pooling together?

752
01:13:01,320 --> 01:13:10,200
Is it valid to combine your populations depending on your research question, it may or may not be valid to combine estimates in men and women.

753
01:13:10,200 --> 01:13:15,359
For example, there might be effect modification or across populations from different geographic areas,

754
01:13:15,360 --> 01:13:18,689
from different cultural backgrounds who might, you know, just like it might.

755
01:13:18,690 --> 01:13:21,179
Like the associations you're studying might operate differently.

756
01:13:21,180 --> 01:13:26,460
There might be different distributions of effect modifiers across populations or across demographic factors.

757
01:13:26,850 --> 01:13:34,739
So if like differences in the populations that you're combining might lead to effect modification of the association you're studying,

758
01:13:34,740 --> 01:13:39,510
then your meta analysis result is not going to be valid, but there's no way to kind of empirically test for that.

759
01:13:39,510 --> 01:13:46,709
You kind of have to use your best available knowledge and theory to make the most appropriate decision on

760
01:13:46,710 --> 01:13:53,640
like which estimates from which studies you can validly meta analyze on the basis of like just simple math.

761
01:13:53,640 --> 01:13:59,760
Can these measures be combined? How were things measured? Are they measuring the same thing and are these populations comparable?

762
01:14:00,420 --> 01:14:06,060
Does that make sense? Does the point of what effect modification make sense or is it confusing?

763
01:14:06,080 --> 01:14:13,370
I can explain more. And in your table one from the paper you were presenting before, there were multiple countries.

764
01:14:13,370 --> 01:14:16,700
So did you pull us together? We did. We did.

765
01:14:16,730 --> 01:14:23,180
And our feeling was that the cancer association, if it's real,

766
01:14:23,180 --> 01:14:28,549
we really think maybe there's some kind of like inverse genetic regulation between carcinogenesis and

767
01:14:28,550 --> 01:14:33,860
neurodegeneration that we think there's no reason to think that wouldn't be like constant across populations.

768
01:14:34,220 --> 01:14:39,620
Like, maybe it's different, but we think that probably is kind of the same across populations.

769
01:14:40,010 --> 01:14:44,390
And then the types of bias that we're concerned about, like the selective survival bias,

770
01:14:44,870 --> 01:14:48,949
that we also kind of felt like that is potentially what's driving associations.

771
01:14:48,950 --> 01:14:56,000
And we felt like that would be more of a function of like study design and administration as opposed to like the underlying population.

772
01:14:56,540 --> 01:15:00,109
But I do think so we weren't too concerned about it, but I think actually,

773
01:15:00,110 --> 01:15:04,490
like we could have we could have definitely had done that and seen if there was a fact modification.

774
01:15:04,910 --> 01:15:10,790
Would you be concerned about like across country or whether there are like diagnostic differences or.

775
01:15:10,790 --> 01:15:13,369
Yeah. Yeah. Yeah.

776
01:15:13,370 --> 01:15:21,770
I think that would be the biggest reason for any kind of cross country differences like access to health care for sure, and kind of like yeah.

777
01:15:22,400 --> 01:15:26,870
So I think, yeah, we could have looked at it. We didn't. That's a good question.

778
01:15:26,990 --> 01:15:31,219
Yeah, I feel like it's often kind of like when you're worried about the effect modification.

779
01:15:31,220 --> 01:15:36,650
It would be like more related to social factors. But here, access to health like health care is important because looking at diagnoses.

780
01:15:37,280 --> 01:15:46,120
Yeah. Okay. Um, and then if you determine the study's included in your systematic review are homogenous enough to synthesize.

781
01:15:46,120 --> 01:15:48,910
And that's a subjective decision like you have to make as the expert,

782
01:15:49,600 --> 01:15:54,549
then you can do your meta analysis and so you can pull the published mean effect estimate from the studies.

783
01:15:54,550 --> 01:15:55,570
Or like I said earlier,

784
01:15:55,570 --> 01:16:02,380
you can contact the study author to get the type of effect estimate that you need if they haven't done it and they might help you out.

785
01:16:04,090 --> 01:16:07,120
So the steps in the meta analysis decide which studies will be included.

786
01:16:07,330 --> 01:16:11,260
This is based on your kind of expert knowledge and then run your meta analysis,

787
01:16:11,260 --> 01:16:16,270
which is a weighted average of study estimates during a forest plot and evaluate heterogeneity.

788
01:16:17,020 --> 01:16:20,130
Have people seen a forest plot before? Yeah.

789
01:16:20,160 --> 01:16:23,880
The kind of like I'll show you in a bit. Like, I guess it looks like trees in a forest.

790
01:16:24,450 --> 01:16:30,629
You're basically just going to plot like the individual effect estimates from the studies and then put your pooled effect estimate at the bottom.

791
01:16:30,630 --> 01:16:36,090
And it provides like a really nice visualization. You can do sensitivity analysis and measure regressions.

792
01:16:36,390 --> 01:16:39,930
I'll talk about those later. And then subgroup analysis, if appropriate.

793
01:16:39,930 --> 01:16:45,600
So that's kind of like what Cara was talking about. Like you might want to look in subgroups defined by like country of their studies

794
01:16:45,600 --> 01:16:48,780
in different countries or like other kind of social or demographic factors.

795
01:16:49,110 --> 01:16:54,180
And then evaluate publication bias to actually run the meta analysis itself.

796
01:16:54,480 --> 01:17:01,620
It's surprisingly easy, and I think this is why we see so many bad analysis that are published is like almost anyone can do it.

797
01:17:01,890 --> 01:17:04,140
You can just like download the papers you're interested in,

798
01:17:04,590 --> 01:17:10,440
grab the main effect estimate and run it through software without kind of really thinking too critically.

799
01:17:11,130 --> 01:17:17,760
So again, like garbage in, garbage out. So like there are really straightforward meta analysis, package add ins data.

800
01:17:17,760 --> 01:17:22,559
You can do it in Excel even. I've heard the metaphor package and R is really, really good.

801
01:17:22,560 --> 01:17:28,889
I've never used it, but if you're an art user, take a look at that and you can even use a free software provided by the

802
01:17:28,890 --> 01:17:32,910
Cochrane Collaboration called Redmond and it produces really nice forest plots.

803
01:17:33,540 --> 01:17:36,959
So doing them in office itself is actually super easy.

804
01:17:36,960 --> 01:17:45,150
You just run it through your program. Um, and like you, yeah, like I said, the meta analysis is a pooled effect estimate.

805
01:17:45,150 --> 01:17:49,049
So all you really need is like the main point estimate from each study and the sample size.

806
01:17:49,050 --> 01:17:53,130
And I'll give you the weighted average of those effect estimates weighted according to sample size.

807
01:17:53,940 --> 01:17:59,370
And you can do this overall or you can do it broken down into subgroups if you want to do the subgroup analyzes.

808
01:18:02,060 --> 01:18:04,910
Heterogeneity is really important though, so you can assess.

809
01:18:04,910 --> 01:18:11,750
So like everything else kind of talking about, you can, you know, are you measuring the same thing or you study populations the same.

810
01:18:11,750 --> 01:18:17,720
You can quantitatively assess this using tests for like statistical tests for heterogeneity.

811
01:18:18,110 --> 01:18:22,100
And you should definitely always do this and it'll tell you how appropriate your pooled estimate is.

812
01:18:22,880 --> 01:18:29,030
Heterogeneity can be due to true population differences, so that would be things that lead to effect modification where there's a true

813
01:18:29,030 --> 01:18:34,159
difference in the point estimate across two populations or differences in study,

814
01:18:34,160 --> 01:18:38,209
methodology, bias or error that lead to differences in the effect estimate.

815
01:18:38,210 --> 01:18:44,900
Like maybe they measured variables differently and that leads to different observed effect estimates, even though in truth they're really the same.

816
01:18:45,860 --> 01:18:49,219
So there's two tests that can be used to evaluate heterogeneity.

817
01:18:49,220 --> 01:18:56,030
There's Cochran's. Q Which tests whether individual effect estimates are further away from the combine than would be expected by chance.

818
01:18:56,600 --> 01:18:58,309
And then the I squared statistic,

819
01:18:58,310 --> 01:19:04,460
which is very commonly used and it describes the percentage of variation across studies that's due to heterogeneity rather than due to chance.

820
01:19:05,560 --> 01:19:11,810
Um, the i squared statistic is honestly, it's generated automatically in most meta analysis softwares.

821
01:19:12,140 --> 01:19:20,600
This is the formula for calculating the R-squared statistic, and there are kind of like like any kind of statistic like this.

822
01:19:20,780 --> 01:19:27,230
There's rules of thumb in terms of like what is a lot of heterogeneity versus like a little or like non or ignorable heterogeneity.

823
01:19:27,710 --> 01:19:31,040
Um, so this is, I don't even remember where I pulled this from.

824
01:19:31,040 --> 01:19:33,800
I think it's published somewhere. This is the general rules of thumb,

825
01:19:33,830 --> 01:19:41,900
like how much heterogeneity we consider is important and the importance of the observed i squared values so it ranges from 0 to 100%.

826
01:19:42,350 --> 01:19:47,900
Depends on like the magnitude and direction of effects as well as the strength of evidence for heterogeneity.

827
01:19:48,710 --> 01:20:00,770
So kind of the. Like the like the stronger the effects are, the more important heterogeneity is going to be in influencing the results.

828
01:20:01,850 --> 01:20:10,190
So what happens if we do our meta analysis and we find there's really high heterogeneity, like we had nice word statistic of like 80% or something.

829
01:20:10,910 --> 01:20:18,110
This way we can handle this. So there are fixed effects, regression or meta analysis and random effects meta analysis.

830
01:20:18,530 --> 01:20:21,770
If we observe no heterogeneity based on that is great statistic.

831
01:20:21,800 --> 01:20:27,200
You can do a fixed effects meta analysis and the fixed effects meta analysis basically assumes that the

832
01:20:27,200 --> 01:20:32,870
true point estimate is the same or equivalent across all of the studies that you are meta analyzing.

833
01:20:33,110 --> 01:20:38,059
But if you observe a lot of heterogeneity, you can do what's called a random effects meta analysis.

834
01:20:38,060 --> 01:20:41,750
And basically this allows the true effect estimate to vary across studies.

835
01:20:41,750 --> 01:20:46,700
So it accounts for heterogeneity accounts for things like effect modification between populations.

836
01:20:46,700 --> 01:20:51,979
I give you two differences in the point estimate and in the random effects meta analysis.

837
01:20:51,980 --> 01:20:56,209
The meta analytic model that's giving you that weighted average is going to include an

838
01:20:56,210 --> 01:21:02,030
additional error error term that represents unexplained sources of between study variation.

839
01:21:02,030 --> 01:21:06,470
So it allows mathematically for that between study variation in the weighted average.

840
01:21:07,400 --> 01:21:11,570
It assumes both heterogeneity and bias are randomly distributed across or across studies.

841
01:21:11,570 --> 01:21:19,070
And you just sort of have to assume that. And the random effects model, in my experience, won't really meaningfully change your point estimates,

842
01:21:19,070 --> 01:21:24,110
but it will introduce more variance into your pooled effect estimate.

843
01:21:24,890 --> 01:21:28,760
And in practice, what I have typically done is run the fixed effects meta analysis,

844
01:21:28,760 --> 01:21:33,500
generated the experts statistic and then run the meta, the random effects meta analysis.

845
01:21:33,950 --> 01:21:39,349
I will say, like in practice, you're often going to see heterogeneity just because of like how the world works.

846
01:21:39,350 --> 01:21:43,070
Like there is heterogeneity and true point estimates across studies and across populations.

847
01:21:43,490 --> 01:21:49,670
So like you're safest bet usually. Like even if your I squared statistic is like somewhere here, if it's like 30%,

848
01:21:49,670 --> 01:21:56,660
I would still run the random effects meta analysis to be conservative and allow that true point estimate to vary across studies.

849
01:21:58,280 --> 01:22:04,170
Here's an example of the forest plot. So this is from the study, the cancer ad review that I published.

850
01:22:04,190 --> 01:22:08,170
I'm going to zoom in a little bit more. So the x axis is kind of hard to see.

851
01:22:08,180 --> 01:22:14,330
I might actually go ahead. One slide. So the x axis of shoot to the bottom is cut off.

852
01:22:14,720 --> 01:22:18,570
So this is the scale for the point estimate.

853
01:22:18,590 --> 01:22:25,790
So we have one is the null. So we have both odds ratios and hazards ratios shown here.

854
01:22:26,510 --> 01:22:32,210
So obviously above below one, we're on a logarithmic scale and above one were to sound like a regular scale.

855
01:22:32,630 --> 01:22:36,410
And then we have I think it's kind of hard to see. We have case control studies at the top.

856
01:22:36,410 --> 01:22:39,170
If you recall, we included both case control and cohort studies.

857
01:22:39,530 --> 01:22:45,380
There were only, I think, three case control studies, and we went out and got that point estimate.

858
01:22:45,710 --> 01:22:49,640
And then these are all the cohort studies here. And then we grouped them according to cancer type.

859
01:22:49,640 --> 01:22:57,400
So we did subgroup analyzes here the we highlight we gave some details on this figure to make it easier to interpret.

860
01:22:57,410 --> 01:23:05,270
So effect estimates below one indicate a reduced risk of ad and effect estimates above one indicates an increased risk of 80.

861
01:23:05,270 --> 01:23:14,420
Right. Like we've learned that in this class. And then the weight on the right hand column is the is proportionate to the sample size.

862
01:23:14,840 --> 01:23:20,540
And then the boxes indicate the magnitude of the effect estimate for a given study.

863
01:23:20,540 --> 01:23:28,970
And then the bars, the horizontal lines are the 95% confidence intervals for each individual study, which are all described here in the rows.

864
01:23:31,940 --> 01:23:38,440
And then let's zoom in a little bit more. So this is the case control studies and the cohort studies of all cancer types.

865
01:23:38,450 --> 01:23:42,080
I haven't shown the subgroups for the individual cancer types.

866
01:23:42,230 --> 01:23:46,220
So what do people generally interpret or take away from this figure?

867
01:23:46,250 --> 01:24:07,120
Like when you look at it. I think this confusion lives for a person because it's smaller and studies have a larger effect.

868
01:24:08,580 --> 01:24:18,330
Yeah. Yeah. We're seeing those more like wider confidence intervals with like the more extreme point estimates.

869
01:24:18,660 --> 01:24:22,660
Yeah. What else?

870
01:24:29,440 --> 01:24:37,480
What was the overall kind of conclusion we made? Like generally we're seeing point estimate below one right in the predictive direction.

871
01:24:37,810 --> 01:24:47,200
So we're seeing generally in existing studies a decreased risk of Alzheimer's disease with cancer in the case control studies.

872
01:24:47,320 --> 01:24:55,360
Um, so this diamond, you see the diamond. So the widest part of the diamond indicates the pooled point estimate.

873
01:24:55,360 --> 01:25:03,040
And then the points of the diamond are the upper and lower bounds of the 95% confidence interval for the pooled point estimate.

874
01:25:03,040 --> 01:25:06,669
So for the case control studies, there is actually no heterogeneity.

875
01:25:06,670 --> 01:25:11,889
You can see that I scored 3 to 6 zero and the pooled odds ratio was .75.

876
01:25:11,890 --> 01:25:16,690
And then for the cohort studies, lot of heterogeneity, the R-squared statistic was 93.8.

877
01:25:16,690 --> 01:25:22,420
So we used a random effects meta analysis here and the pooled hazard ratio was 0.81.

878
01:25:25,690 --> 01:25:30,310
When there is heterogeneity, it's usually important to identify sources of significant heterogeneity.

879
01:25:30,700 --> 01:25:37,480
Lots of ways to do this, and it's kind of like subjective and like the way in which you look for heterogeneity kind of depends on your own.

880
01:25:37,900 --> 01:25:41,560
Like, again, expert knowledge of like where do you think the heterogeneity is coming from?

881
01:25:42,010 --> 01:25:44,919
So you can do graphical methods like the first plot is helpful.

882
01:25:44,920 --> 01:25:51,400
So what you identified that's like right on point, you can plot the results by risk of bias as well.

883
01:25:52,120 --> 01:25:54,909
You can do subgroup analysis. So like men versus women,

884
01:25:54,910 --> 01:26:01,629
if you think there's differences like real gender or sex based differences in your results here we stratified by cancer type.

885
01:26:01,630 --> 01:26:06,010
In our review. You can do various sensitivity analysis or you can do a regression either.

886
01:26:06,010 --> 01:26:13,059
Really, really cool. So a meta regression is just a straightforward regression model where the study is the unit

887
01:26:13,060 --> 01:26:19,720
of analysis and you can investigate attributes of studies that might impact the results.

888
01:26:20,140 --> 01:26:24,280
So like going back to that data extraction table, you can take characteristics.

889
01:26:24,280 --> 01:26:30,820
Those are all variables measured at the study level, like the percent women in the study or like the country where the study was done.

890
01:26:31,210 --> 01:26:36,580
And you can put those variables into a model to see how much differences in those things across.

891
01:26:36,580 --> 01:26:40,240
Studies are like pushing the point estimate in a certain direction or not.

892
01:26:40,930 --> 01:26:45,100
So that's what we did in this cancer paper for our formal bias analysis.

893
01:26:45,100 --> 01:26:48,850
So recall I showed you all of those tags for the different sources of bias.

894
01:26:49,240 --> 01:26:52,570
I didn't get into it here, but in the paper, based on those tags,

895
01:26:52,870 --> 01:26:58,720
we came up with a list of criteria that would make a study susceptible to each form of bias shown in the Dags.

896
01:26:59,170 --> 01:27:05,770
So, for example, something that would make a study susceptible to these selective survival bias would be if they

897
01:27:05,770 --> 01:27:11,739
used prevalent cancer at baseline rather than incident cancer cases over the follow up rate.

898
01:27:11,740 --> 01:27:17,139
Because prevalent cancer cases at baseline in a longitudinal study would reflect people who

899
01:27:17,140 --> 01:27:21,700
have been diagnosed at some unknown point of time in the past before the study started.

900
01:27:22,060 --> 01:27:26,560
And then they had to survive for a certain amount of time before study baseline.

901
01:27:26,560 --> 01:27:33,730
So they are by definition survivors, right? Like people who were diagnosed with cancer and died before the study started would be

902
01:27:33,730 --> 01:27:37,059
kind of like the most aggressive cases of cancer who didn't make it into the study.

903
01:27:37,060 --> 01:27:41,590
So prevalent cancer cases are a fraction of all incident cancer cases.

904
01:27:42,040 --> 01:27:47,680
Right. And that's like a core if it's one concept, like the prevalence incidence bias, same thing, a selective survival bias.

905
01:27:48,370 --> 01:27:51,820
So we've separated out studies who looked at prevalent versus incident cancers.

906
01:27:52,450 --> 01:27:58,179
That's one example we did others as well. And then we group studies into like yes, no.

907
01:27:58,180 --> 01:28:01,210
Were you at risk of this form of bias based on these criteria?

908
01:28:01,510 --> 01:28:03,220
And then we put it in a regression.

909
01:28:04,330 --> 01:28:11,739
So here we have in the columns the types of methodological biases and these all map onto the dags that I showed you.

910
01:28:11,740 --> 01:28:14,559
So we had missing adjustment for age, sex or education.

911
01:28:14,560 --> 01:28:21,760
This was confounding by unmeasured but known confounders adjusted for factors influenced by cancer.

912
01:28:21,760 --> 01:28:28,540
That's that factors associate adjusted that are downstream of cancer that's the over adjustment bias that led to the colitis stratification,

913
01:28:29,080 --> 01:28:37,090
diagnostic bias, cognitively impaired individuals not being excluded at baseline and then situations where cancer status might influence ad diagnosis.

914
01:28:37,090 --> 01:28:44,829
These would be studies that ascertained age from medical records so susceptible to that diagnostic bias we were talking about

915
01:28:44,830 --> 01:28:51,130
rather than using it in study research assessment of of dementia status that was identified during the research process,

916
01:28:52,210 --> 01:28:57,850
competing risks. So basically these were studies that look at used Cox proportional hazards models but didn't

917
01:28:57,850 --> 01:29:03,579
account for mortality as a competing risk to ad and then the survival or related biases.

918
01:29:03,580 --> 01:29:07,989
So here like I said, the prevalent cancer is not being separated out from incident cancers leading to

919
01:29:07,990 --> 01:29:12,700
a prevalence incidence bias cancer types that raise subsequent mortality risk.

920
01:29:12,700 --> 01:29:16,750
So just by definition, like people like lung cancer,

921
01:29:16,930 --> 01:29:23,110
the Association for Lung Cancer is just inherently more likely to be susceptible to bias than the Association for non-melanoma skin cancer,

922
01:29:23,770 --> 01:29:28,660
for example, high percentage of missing data, and then restrictive inclusion and exclusion criteria.

923
01:29:29,290 --> 01:29:33,370
So here in the top row, we showed a number of studies that were subject to that bias.

924
01:29:33,820 --> 01:29:40,180
So like a lot of them, surprisingly 12 were didn't adjust for age, sex or education.

925
01:29:40,180 --> 01:29:46,360
Right. Those are really core things you'd want to adjust for men's medical records that might be subject to diagnostic bias.

926
01:29:46,360 --> 01:29:51,850
And then many looked at cancer types that raised subsequent mortality risk.

927
01:29:52,510 --> 01:29:55,960
So I'll walk you through interpreting this table. So these are the meta regression estimates.

928
01:29:56,500 --> 01:30:03,760
So the first row shows the pooled line of the hazard ratios in studies without the bias.

929
01:30:04,180 --> 01:30:10,180
So that first row of estimates shows you the villain hazard ratio is kind of hard to interpret.

930
01:30:10,900 --> 01:30:13,299
So don't worry about the absolute interpretation too much,

931
01:30:13,300 --> 01:30:20,020
but that's basically the lawn hazards ratio pooled from the middle regression and studies that were not subject to the bias.

932
01:30:20,800 --> 01:30:23,950
That's essentially the model intercept in the middle regression model and.

933
01:30:24,000 --> 01:30:27,540
In the second row shows the difference in the lawn hazards ratio.

934
01:30:27,840 --> 01:30:32,360
For studies that did have that bias and if if the difference.

935
01:30:32,370 --> 01:30:43,160
So like if you summing the two together pushes that pooled hazard ratio in the more inverse direction, then that's indicative of bias accounts,

936
01:30:43,230 --> 01:30:48,180
that particular form of bias potentially accounting for the inverse association that's been observed in the literature.

937
01:30:48,670 --> 01:30:54,940
So that makes sense. So like going through them, first of all, we see kind of like negative.

938
01:30:55,450 --> 01:31:02,080
Of course, we see we're seeing negative line hazard ratios because the actual hazard ratios, pool hazard ratios were inverse, right?

939
01:31:02,470 --> 01:31:06,370
Like for a number below one, the line of that number is going to be negative.

940
01:31:06,970 --> 01:31:12,340
And we're looking for differences in the law hazard ratio that are even more negative.

941
01:31:12,340 --> 01:31:16,240
So pushing that hazard ratio away from the null.

942
01:31:16,600 --> 01:31:21,969
So the missing adjustment for known confounders factors influenced by cancer, those aren't really coming through.

943
01:31:21,970 --> 01:31:28,360
Same thing with cancer, maybe affecting ad status, but the ones that really came through was this one.

944
01:31:28,360 --> 01:31:31,120
The prevalent cancer is not being separated from cancers.

945
01:31:31,360 --> 01:31:39,159
Those have very negative lot hazard ratios translating to very inverse hazard ratios on the hazard ratio scale.

946
01:31:39,160 --> 01:31:46,240
So below one. So we kind of concluded that selective we couldn't rule out bias.

947
01:31:46,570 --> 01:31:53,170
That's kind of hard to do definitively, but we were really concerned about selective survival bias kind of contributing

948
01:31:53,170 --> 01:31:56,950
in a large way to this observed inverse association between cancer and AD.

949
01:31:57,070 --> 01:32:02,290
So we made recommendations in the systematic review about how investigators can avoid this in the future.

950
01:32:02,830 --> 01:32:08,200
I will also say, even though diagnostic bias didn't come out, I'm actually becoming increasingly convinced that that is playing a role.

951
01:32:08,200 --> 01:32:11,440
And we've done some follow up studies using primary data on that.

952
01:32:12,970 --> 01:32:16,900
And then finally, funnel plots. So far, a positive screening test for publication bias.

953
01:32:17,440 --> 01:32:20,950
What is publication bias? How do you talk about that in this class or other classes at all?

954
01:32:23,550 --> 01:32:27,840
Yeah. Yeah. I think when I taught this class, John Smith came in and talked about it.

955
01:32:27,850 --> 01:32:33,010
I don't know if you should have read her like recorded lecture or. Okay, we talked about it a little bit.

956
01:32:33,490 --> 01:32:39,630
Can someone tell us what publication bias is really quickly? Yeah.

957
01:32:40,730 --> 01:32:45,230
It's kind of the general bias towards publishing.

958
01:32:45,380 --> 01:32:50,030
Like significant or results that are high.

959
01:32:50,900 --> 01:32:54,380
Like either they're very protective or bad.

960
01:32:54,560 --> 01:32:58,970
Yeah. So basically for publishing results that show really strong results.

961
01:32:59,030 --> 01:33:02,030
Exactly. For like authorship and like one.

962
01:33:02,930 --> 01:33:06,980
Exactly. Yeah. So it's it's hard to publish null results. Like, it just it is.

963
01:33:07,430 --> 01:33:10,000
And we also call it like a file door problem. Right?

964
01:33:10,010 --> 01:33:16,069
So we just want to make sure that what we're seeing here is it being influenced by a publication bias where like splashy,

965
01:33:16,070 --> 01:33:21,860
like strong results in either direction are getting differentially published over null results.

966
01:33:22,700 --> 01:33:30,859
So if Jen Smith gave you her lecture, you might have seen of like a funnel plot already, but basically you would plot the effect estimate,

967
01:33:30,860 --> 01:33:35,960
usually a log transformed effect estimate, like what I showed you previously against a measure of standard error.

968
01:33:36,260 --> 01:33:41,030
And there shouldn't be an association between the effect estimate and standard error.

969
01:33:41,940 --> 01:33:44,480
Right. Yeah.

970
01:33:44,510 --> 01:33:53,569
Like that's the indication that we use that there's some kind of publication bias going on because like imprecise results when they're null,

971
01:33:53,570 --> 01:33:56,420
that's really hard to include, like to conclude anything but like very,

972
01:33:56,420 --> 01:34:02,840
very strong study findings, if they're imprecise, can still be statistically significant.

973
01:34:02,840 --> 01:34:06,020
So we shouldn't see a systematic association between statistical significance,

974
01:34:06,590 --> 01:34:10,580
which is related to precision and the magnitude of the effect to estimate.

975
01:34:12,380 --> 01:34:15,620
So yeah, so what's often going on is like small negative or small?

976
01:34:15,620 --> 01:34:23,330
No, like precise null studies are often missing. And this is the final thought from the cancer aid review.

977
01:34:23,600 --> 01:34:28,850
So we have the standard error in the individual studies that were observed or included on the Y axis.

978
01:34:29,150 --> 01:34:32,930
And we have the line of the hazards ratio on the X axis.

979
01:34:33,620 --> 01:34:37,309
So this this funnel plot is actually like not too bad.

980
01:34:37,310 --> 01:34:45,709
I would say there's a few outliers here that have a high standard error and a really strongly negative line hazard ratio.

981
01:34:45,710 --> 01:34:49,580
So very strongly inverse association and we're not seeing anything kind of on the other side.

982
01:34:49,910 --> 01:34:55,610
But up here, among the more precise studies with smaller standard errors that are kind of floating,

983
01:34:55,610 --> 01:34:59,989
we're kind of seeing an equal distribution around the null for the hazard ratio.

984
01:34:59,990 --> 01:35:01,070
So it's actually not too bad.

985
01:35:01,640 --> 01:35:07,670
But these are generally generally like a really useful kind of just quick visual inspection that can give you an idea of publication bias.

986
01:35:08,690 --> 01:35:12,620
And again, your systematic review software is going to like spit this out for you.

987
01:35:12,920 --> 01:35:20,389
You don't need to, like, plot yourself. And I just want to say really briefly the difference between a meta analysis and a pooled analysis.

988
01:35:20,390 --> 01:35:26,180
You might hear these phrases used interchangeably, and I talked about the pooled effect estimate in a meta analysis.

989
01:35:26,570 --> 01:35:31,790
So a meta analysis gives you a pooled estimate where you're pulling an estimate from

990
01:35:31,790 --> 01:35:34,699
an individual study that's been published and then you're getting a weighted average,

991
01:35:34,700 --> 01:35:38,000
like I said, using either fixed effects or random effects meta analysis model.

992
01:35:38,300 --> 01:35:40,820
This is different from pooled analysis in a pool analysis.

993
01:35:41,120 --> 01:35:47,749
You're going to go to the primary data sources and get records like get the data at the individual level where like the rows in your dataset

994
01:35:47,750 --> 01:35:53,990
are going to be the individual person and you're going to be pooling those across multiple different like samples or studies or cohorts,

995
01:35:54,320 --> 01:36:00,020
whereas in the meta analysis you're going to have like that level of observation as the study of the person.

996
01:36:00,080 --> 01:36:00,710
Does that make sense?

997
01:36:01,610 --> 01:36:08,900
Pooled analyzes, bring a set, bring their own whole set of challenges where like you need to harmonize variables across different data sets.

998
01:36:10,960 --> 01:36:16,060
So to recap, we're done. Systematic review and meta analysis can be highly rigorous.

999
01:36:16,060 --> 01:36:21,219
They can be highly useful. They can be used to greatly improve the precision of individual studies.

1000
01:36:21,220 --> 01:36:26,770
And you can use tools such as metal regressions and forest plots to get an idea of where sources of

1001
01:36:26,770 --> 01:36:32,229
bias or heterogeneity are coming in the literature and whether it's publication bias as a doing one.

1002
01:36:32,230 --> 01:36:38,050
As an investigator, it'll really help you kind of sharpen your expertise and knowledge on a given topic,

1003
01:36:38,380 --> 01:36:41,320
and it's just a really exciting and gratifying feeling.

1004
01:36:41,500 --> 01:36:46,600
Once you've conducted a systematic review and meta analysis and you feel like it's good quality and is like really contributing something.

1005
01:36:47,110 --> 01:36:49,239
But having said that, it's really garbage in, garbage out.

1006
01:36:49,240 --> 01:36:54,729
Like you really, really have to be rigorous and thoughtful at every step of your systematic review and meta analysis.

1007
01:36:54,730 --> 01:37:01,600
Otherwise I would say it's not a good use of your time. And yeah, as a epidemiological tool, they're just really,

1008
01:37:01,600 --> 01:37:07,809
really useful for reaching consensus in a body of literature and moving forward with interventions and policy recommendations.

1009
01:37:07,810 --> 01:37:12,400
Like there are organizations that do systematic reviews and meta analysis and base policy on them,

1010
01:37:12,400 --> 01:37:19,120
like the Cochrane Collaboration, for example, we have an in class activity, so it's 1138.

1011
01:37:19,120 --> 01:37:26,169
I know you typically end 1150, is that right? So there's 12 minutes and we're going to kind of just combine this activity with the

1012
01:37:26,170 --> 01:37:31,780
end of class and it's a critical appraisal of a systematic review and meta analysis.

1013
01:37:31,780 --> 01:37:38,650
So on canvas I've put a systematic review and meta analysis about endometriosis and cancer risk,

1014
01:37:39,040 --> 01:37:46,119
and you will apply the PRISMA checklist to that article as a quality assessment of the review itself.

1015
01:37:46,120 --> 01:37:51,729
So I've talked a lot about, like you as the investigator who is conducting a systematic review and meta analysis,

1016
01:37:51,730 --> 01:37:53,980
you're going to do quality assessments of the individual studies,

1017
01:37:54,460 --> 01:38:00,970
but that doesn't you can also assess the quality of a meta analysis itself because some are garbage, some are really good.

1018
01:38:01,630 --> 01:38:04,390
So the Prisma checklist I think should be on canvas.

1019
01:38:04,390 --> 01:38:10,900
And you just take that and read through the review to find kind of the key aspects that should be reported.

1020
01:38:10,900 --> 01:38:16,570
And I think it's a useful exercise because it helps you to familiarize yourself with the types of

1021
01:38:16,570 --> 01:38:20,980
attributes that should be included in a systematic review and meta analysis for it to be good quality.

1022
01:38:22,570 --> 01:38:27,310
And that's all I have, I guess before we move on to the activity, are there any questions?

1023
01:38:33,430 --> 01:38:36,760
Okay. Was that helpful? Do people feel like you kind of learned something?

1024
01:38:36,850 --> 01:38:41,020
And I hope I hope some of you do a systematic review and meta analysis after this class,

1025
01:38:41,020 --> 01:38:43,569
because I think they're really, really hard, but they're also a lot of fun.

1026
01:38:43,570 --> 01:38:47,860
And like I said, I think it's something everyone should do at least once in their career.

1027
01:38:48,100 --> 01:38:50,970
I'm going to be here during the duration of the activity, but if you like,

1028
01:38:50,980 --> 01:38:55,570
do feel free to come and talk to me at any time, whether now or else after the class.

1029
01:38:55,570 --> 01:38:59,260
And I think my my contact details can be put on campus. Okay.

1030
01:39:00,130 --> 01:39:02,110
Okay. Thanks, everyone. Thank you.

