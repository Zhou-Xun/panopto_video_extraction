1
00:00:01,860 --> 00:00:09,930
All right. So I just want to just say something about, you know, generally staying healthy, school public health wise.

2
00:00:11,730 --> 00:00:15,450
And I just verified this as well,

3
00:00:15,480 --> 00:00:24,090
that the chief health officer at the University of Michigan is kind of he sent an email out yesterday sort of saying,

4
00:00:24,540 --> 00:00:29,609
you know, there's a lot of respiratory stuff going around, not just COVID, but flu and other stuff, too.

5
00:00:29,610 --> 00:00:36,659
And so if you want to stay healthy, he's kind of making a push but not forcing it.

6
00:00:36,660 --> 00:00:40,170
But he's kind of making a push to encourage mask wearing.

7
00:00:40,200 --> 00:00:47,009
So COVID in the community level is still a medium which, you know,

8
00:00:47,010 --> 00:00:53,340
our guidance is you do what you feel is best for you except when you're on public transportation and stuff.

9
00:00:53,940 --> 00:01:02,190
But I'm going to have my mask ready if anybody comes up to protect you.

10
00:01:02,760 --> 00:01:11,820
And frankly, with the calendar coming up to Thanksgiving where I'm visiting old people, I don't know what you guys are doing.

11
00:01:12,210 --> 00:01:17,790
I might be more conscious of this for that reason as well, because I'm going to be visiting all people soon.

12
00:01:19,530 --> 00:01:26,420
So we're in that kind of range. And just keep your eye on the metrics.

13
00:01:26,670 --> 00:01:32,690
I after election day, you don't know people were outstanding in lines and impulse.

14
00:01:32,900 --> 00:01:38,960
We could get a little tick up. So just be aware, you know, to keep your eye on this stuff.

15
00:01:41,140 --> 00:01:50,680
All right. In terms of the course, we're doing well, we are almost, almost close to done.

16
00:01:53,170 --> 00:02:00,260
With the material that's being covered on the final. So we're going to finish up hand out 15 today.

17
00:02:00,280 --> 00:02:04,569
Start hand out 16. And I might post two homework,

18
00:02:04,570 --> 00:02:10,450
the last homework I might put it this weekend so that you can at least start working on one of

19
00:02:10,450 --> 00:02:15,610
the last homework problems if you want to kind of work with the pace of the material in class.

20
00:02:16,480 --> 00:02:21,250
All right. So you already have all you need to do homework. Five, please get started.

21
00:02:21,250 --> 00:02:30,760
It's long, but you're starting. You're almost ready to do a problem on homework six And so I might just get that out so you can play with homework.

22
00:02:30,760 --> 00:02:37,330
Six Wouldn't it be nice if you finish all of your homework, you know, before the holidays?

23
00:02:37,420 --> 00:02:44,440
Wouldn't that be nice? We won't quite have all of the material covered for the last homework, so there might be a problem left.

24
00:02:44,440 --> 00:02:47,830
But wouldn't it be cool if you if you did that? There's always hope, right?

25
00:02:50,170 --> 00:02:56,620
So when you come back from break, I'm going to be covering topics that aren't covered in the exam I'm in.

26
00:02:57,010 --> 00:03:00,819
There's it's possible I'll still be working at the end of handout 18.

27
00:03:00,820 --> 00:03:10,270
I'm not sure exactly where we're going to end up on that day before break, but the material on hand out 1920 is not covered on the final quiz.

28
00:03:10,270 --> 00:03:16,330
And of course, the final quizzes has been down here this whole time, invisible to us, unless we scroll to the bottom.

29
00:03:16,810 --> 00:03:22,120
But it's going to be a canvas quiz and it's going to be run in a similar way to the first quiz.

30
00:03:22,450 --> 00:03:28,599
The only difference is that now you can have two sheets of paper front and back as your cheat sheets,

31
00:03:28,600 --> 00:03:34,240
and whether you decide to reuse the one from quiz one and make one for quiz two, that's up to you.

32
00:03:34,840 --> 00:03:42,639
The focus of the material is going to be on things after the first quiz, but materials cumulative.

33
00:03:42,640 --> 00:03:50,260
So it might feel sometimes as if material from the first quiz is coming up because we learned a lot about formulas back then.

34
00:03:50,260 --> 00:03:55,330
Right. But the focus on modeling is going to be post quiz two methods.

35
00:03:56,760 --> 00:04:04,900
Okay. So let's go ahead and remind ourselves where we were in the handout.

36
00:04:05,200 --> 00:04:13,050
15. So we ended up. On Slide 14.

37
00:04:13,080 --> 00:04:21,420
Yes. So we were just about to talk about crossover trials in the more general case we had.

38
00:04:21,540 --> 00:04:29,069
So right prior to that, what had we done? Sorry.

39
00:04:29,070 --> 00:04:39,809
Louder. Oh, we didn't really do what Sly did we.

40
00:04:39,810 --> 00:04:42,880
I had some for some reason. Put that down. We're.

41
00:04:42,890 --> 00:04:49,690
Where did we start then. At the very beginning. Was like.

42
00:04:51,880 --> 00:04:58,420
Yeah. Okay, that's fair. So just, uh, I think the early part.

43
00:05:01,000 --> 00:05:09,340
Of the hand out was just kind of looking at the pancreatic enzyme study.

44
00:05:09,670 --> 00:05:14,409
So this whole hand is just getting us warmed up with more models for dependent outcomes.

45
00:05:14,410 --> 00:05:23,870
And so we've looked at the pancreatic enzyme crossover study and looking at, you know,

46
00:05:23,950 --> 00:05:31,360
assumptions to use for that analysis and running it with an assumption of compound symmetry.

47
00:05:32,630 --> 00:05:35,810
Or exchangeable is another way to talk about that,

48
00:05:35,870 --> 00:05:44,780
that assumption where you have all of the off diagonal call correlation estimates trying to get at the same estimate of row.

49
00:05:48,030 --> 00:05:56,580
And so I think that's where we ended up. And didn't we get to this point where we saw.

50
00:05:57,590 --> 00:06:01,430
The if you try to assume an unstructured.

51
00:06:03,230 --> 00:06:06,500
Covariance matrix that SAS and are both broke.

52
00:06:06,740 --> 00:06:10,230
Did we get to that part last time? You don't remember this?

53
00:06:10,260 --> 00:06:18,800
Okay, let's go. So let's. Let's just go back then. So here is the format of the data that we're going to be looking at.

54
00:06:19,310 --> 00:06:26,000
And so everybody is in. All the outcomes are in one big column.

55
00:06:26,240 --> 00:06:38,140
So this is long format for outcomes per person where we have either, you know, the placebo kind of pill type, it says none here.

56
00:06:38,150 --> 00:06:42,950
So that actually might have been none in this study. I don't recall if they had a placebo mark up or not.

57
00:06:44,450 --> 00:06:48,020
Probably not because there's all these other choices. The formulation.

58
00:06:48,030 --> 00:06:53,500
So tablet capsule encoded. And fecal fat.

59
00:06:54,880 --> 00:07:02,530
Being high is, you know, not great for your absorption of nutrients and things.

60
00:07:02,530 --> 00:07:11,470
So they measure problems with, you know, your pancreatic enzymes by measuring fecal fat.

61
00:07:13,500 --> 00:07:17,060
And so. Let's see here.

62
00:07:17,090 --> 00:07:21,950
So pill type is coded zero 1 to 3 as a categorical variable.

63
00:07:21,960 --> 00:07:34,490
So it's going to be like a class statement. So you can compare physical fat by formulation and the ID being the same within person.

64
00:07:34,730 --> 00:07:41,180
One, two, three, four. This variable ID is keeping track of which outcomes are correlated in the analysis.

65
00:07:42,110 --> 00:07:45,709
Okay. All right.

66
00:07:45,710 --> 00:07:51,560
So I do remember we talked about this part that, you know, when you're looking at the variability terms.

67
00:07:52,970 --> 00:08:02,690
We have the choices we've talked about in the past. Independence, probably not because the same person is being measured for all four formulations.

68
00:08:03,230 --> 00:08:09,200
And just looking at the first couple of people, you can sort of see that there's correlation with in person.

69
00:08:09,200 --> 00:08:13,200
This person started off high and they tend to be high.

70
00:08:13,220 --> 00:08:16,940
This this person, too. And I think earlier in the data set.

71
00:08:18,330 --> 00:08:25,150
Where we see the whole data set. It's the same way have six people.

72
00:08:25,720 --> 00:08:28,990
And so if they start off low, they're going to be low most of the time.

73
00:08:28,990 --> 00:08:33,760
So that's a big indicator of correlation that wherever, you know, wherever they are,

74
00:08:34,060 --> 00:08:41,470
their measures tend to be closer to themselves under these different form formulations compared to what is happening to other people in the data set.

75
00:08:42,850 --> 00:08:46,660
So independence is probably not going to be okay to assume.

76
00:08:47,170 --> 00:08:53,680
Most likely it's going to be a compound symmetry structure just because there's no reason

77
00:08:53,680 --> 00:08:59,050
to think that any pair of outcomes would be more correlated to any other pair of outcomes.

78
00:08:59,530 --> 00:09:04,030
If we had the timing in the data set of when they took which type of formulation,

79
00:09:04,030 --> 00:09:12,669
we might be able to stretch that assumption a bit to assume that measures further apart would be less correlated, but that data is not provided.

80
00:09:12,670 --> 00:09:18,670
All we know is the fecal fat under the different formulations and we have and it could have been given in random order.

81
00:09:18,670 --> 00:09:24,970
So we don't have a specific time variable to base correlation weakening over time.

82
00:09:26,220 --> 00:09:32,340
So we don't have enough data to really model an auto regressive correlation structure

83
00:09:32,340 --> 00:09:35,460
where the correlation weakens depending on how far apart the measures are.

84
00:09:36,420 --> 00:09:43,530
There is this unstructured option that doesn't make any assumptions at all about the correlation between pairs.

85
00:09:43,920 --> 00:09:49,980
Every possible pair of outcomes. Those two columns can have their own correlation when you choose unstructured.

86
00:09:50,640 --> 00:09:57,370
And one of the things that we're going to see with this particular data set is it's only six people with four measures.

87
00:09:57,370 --> 00:10:00,660
So 24 total outcomes to model.

88
00:10:01,170 --> 00:10:11,490
And if you have an unstructured covariance matrix that requires one, two, three, four, six.

89
00:10:12,850 --> 00:10:18,910
Covariance terms. And that's a lot to model.

90
00:10:19,670 --> 00:10:21,910
And the data only has 24 measures.

91
00:10:22,420 --> 00:10:32,200
So it's going to be we're going to see South and are both kind of break when they try to measure when they try to estimate all of those terms.

92
00:10:34,440 --> 00:10:37,829
On top of the scientific stuff that you're interested in. All right.

93
00:10:37,830 --> 00:10:42,809
So let's pick up here. All right.

94
00:10:42,810 --> 00:10:51,870
So this is just putting in the data and this is how you would use proc gen mod when you have these correlated measures.

95
00:10:51,870 --> 00:10:55,440
So we're putting ideas always in the class statement.

96
00:10:55,920 --> 00:11:02,309
The thing that says which measures are correlated, that's always in the class statement, choosing the reference group for the PIL type.

97
00:11:02,310 --> 00:11:09,540
So this is the group and then the model is the fecal fat outcome equals to PIL type.

98
00:11:09,550 --> 00:11:16,530
So this will be, you know, basically three parameters will be estimated for the various PIL types.

99
00:11:16,530 --> 00:11:17,940
Aside from the non group,

100
00:11:18,480 --> 00:11:29,280
we're assuming that the outcome is normally distributed and this repeated line is allowing us to assume that the outcomes are dependent within person.

101
00:11:29,700 --> 00:11:33,959
So subject equals ID. This is a common thing here.

102
00:11:33,960 --> 00:11:37,410
This ID variable here corresponds to what you called it here.

103
00:11:38,220 --> 00:11:45,570
So subject equals is part of the code and the parameter estimate kind of label and ID is your variable for the group.

104
00:11:46,290 --> 00:11:52,799
The grouped outcomes type is saying which type of correlation structure you want to assume.

105
00:11:52,800 --> 00:12:01,200
C stands for compound symmetry, where you're assuming all pairs of outcomes are equally correlated, that there's no different correlation.

106
00:12:01,200 --> 00:12:06,780
When you look at PIL type one versus two or Pil type one versus three, they're all the same parameter.

107
00:12:07,110 --> 00:12:13,379
So you're estimating one correlation term one, you assume this, that's common to all the pairs of outcomes.

108
00:12:13,380 --> 00:12:18,870
So that's kind of efficient if that assumption is true that you only have to estimate one.

109
00:12:20,080 --> 00:12:31,430
Of those. And this here is the same model, but trying to get it to fit the unstructured correlation matrix.

110
00:12:31,910 --> 00:12:41,870
And so they're this because there's four outcomes that's a four by four storage system with 16 entries, right?

111
00:12:42,740 --> 00:12:50,420
The diagonal entries are all one for the correlation, just because those are the correlations with each random variable in itself.

112
00:12:50,990 --> 00:12:58,729
And so the six terms that are above the diagonal are all being assumed to be possibly different.

113
00:12:58,730 --> 00:13:03,020
And they're. And then you've kind of already covered that for the below.

114
00:13:03,320 --> 00:13:11,330
There's only six unique correlation things being estimated since these matrices are symmetric around the center diagonal of ones.

115
00:13:12,530 --> 00:13:17,530
So this is assuming you need to only estimate one term to get your correlation going.

116
00:13:18,050 --> 00:13:24,200
This is assuming you need to estimate six terms to get your correlation going.

117
00:13:30,260 --> 00:13:41,030
All right. The Mod Model C is saying that you want to show both model both model based and robust standard error estimates.

118
00:13:41,240 --> 00:13:48,020
Model based is kind of the results from compound symmetry or unstructured assumptions.

119
00:13:48,290 --> 00:13:55,459
And those when you have a small sample size, those are the ones that you want to use to report your P values.

120
00:13:55,460 --> 00:13:58,850
They behave much better for small data sets.

121
00:13:58,850 --> 00:14:08,630
As long as this assumption is true for large datasets, you would report what's called the robust standard estimate related P values,

122
00:14:09,140 --> 00:14:13,100
and that will be repeated in various slides many times.

123
00:14:13,100 --> 00:14:18,680
But that's the general idea. Small data sets. Try to get your model based estimates correct.

124
00:14:19,250 --> 00:14:24,079
Large datasets are you want to use these robust standard or estimates,

125
00:14:24,080 --> 00:14:30,900
sometimes called sandwich estimates for the variability and the P values that go along with that table are.

126
00:14:30,950 --> 00:14:39,179
And then here are the results. So. The correlation matrix.

127
00:14:39,180 --> 00:14:44,450
When you assumed compound symmetry, the correlation is actually quite high in this dataset.

128
00:14:44,460 --> 00:14:52,110
So this even just looking at this, it's pretty clear you shouldn't try to assume independence between the outcomes.

129
00:14:52,110 --> 00:14:59,460
The correlation between any pair of outcomes in these six people is that estimate is pretty high.

130
00:15:01,120 --> 00:15:05,580
Okay. And here are the two sets of results.

131
00:15:05,590 --> 00:15:12,909
So let's unpack this. So I didn't cover this last time because I must have like in my head had that I already talked about this,

132
00:15:12,910 --> 00:15:20,110
but that's because I looked at the slides before class. So here this is the empirical standard error estimate.

133
00:15:20,110 --> 00:15:25,120
So that's the robust results or the sandwich variance estimate results.

134
00:15:25,120 --> 00:15:29,079
This is the table that you use with large sample sizes.

135
00:15:29,080 --> 00:15:39,490
It's kind of giving you extra protection if your correlation really didn't follow whatever assumption you had in your type equals part,

136
00:15:40,870 --> 00:15:47,410
the standard error is still usually pretty good. As long as your sample size is large for small sample sizes,

137
00:15:47,830 --> 00:15:54,100
you're supposed to get the model based result as correct as you can and use the P values over here.

138
00:15:54,610 --> 00:16:03,460
So the estimate column there is a nice thing to notice and that is that the parameter estimates the basis for your model,

139
00:16:04,180 --> 00:16:12,220
which here are just about, you know, the different, you know, the various parameters for the types of pancreatic, pancreatic enzyme formulations.

140
00:16:12,610 --> 00:16:21,429
Those don't change between these two. So the only thing that changes is the standard error and everything else.

141
00:16:21,430 --> 00:16:28,270
That depends on standard error, of course. So this is I've got boxed in red because that's kind of what you're looking at

142
00:16:29,320 --> 00:16:33,100
for the difference between these two tables and it does affect the other stuff.

143
00:16:33,100 --> 00:16:37,240
So confidence limits depend on standard error. These statistics depend on standard error.

144
00:16:37,540 --> 00:16:47,829
P values depend on standard error. So I like to look at those two results for the follow with the following goal in mind.

145
00:16:47,830 --> 00:16:53,200
I want to see, you know, based on what I assumed for my working correlation matrix,

146
00:16:54,100 --> 00:17:02,650
do I have vastly different patterns of numbers here than I do with the robust version of the same?

147
00:17:03,670 --> 00:17:08,409
And these are the ones that I want to use P values for, for small sample sizes.

148
00:17:08,410 --> 00:17:16,930
So I want to see if there's huge deviations and what the robust things look like compared to the model based.

149
00:17:18,220 --> 00:17:27,130
And I right now the model based is is forced to assume that these standard errors for the various pill types are the same.

150
00:17:28,030 --> 00:17:34,120
And so I'm looking up here and seeing by I, you know, is that a reasonable assumption?

151
00:17:34,450 --> 00:17:38,680
And I think probably it is a reasonable assumption.

152
00:17:40,520 --> 00:17:46,069
So the model based center errors are comparable to the robust standard error estimates,

153
00:17:46,070 --> 00:17:55,130
and it's close enough that I can trust that I picked a pretty good model based assumption with this compound symmetry slash exchangeable thing.

154
00:17:55,640 --> 00:18:03,710
And so if I'd seen big differences, I might have tried a different model base standard error and use p values that corresponded to that choice.

155
00:18:08,110 --> 00:18:12,669
So here's what happens when we try to use an unstructured correlation matrix.

156
00:18:12,670 --> 00:18:17,350
And I have to confess, you know, there's a large sample size to work with.

157
00:18:17,350 --> 00:18:20,350
And I know I have plenty of data to be able to estimate these parameters.

158
00:18:20,920 --> 00:18:30,090
I might just start with unstructured, you know, because that doesn't require me to actually know what correlation pattern is being used.

159
00:18:30,190 --> 00:18:37,630
And as a student, there's, you know, you just want to use the this will work in every case kind of a choice.

160
00:18:38,320 --> 00:18:45,670
So I want this to be a cautionary tale that it isn't always the right the right choice in small data set.

161
00:18:45,670 --> 00:18:47,240
So let's see what happens here.

162
00:18:47,290 --> 00:19:01,119
So here's the the parameter estimates when you assume unstructured covariance and here is the same kind of robust sandwich empirical standard error.

163
00:19:01,120 --> 00:19:07,569
There's three different names for what people will call this in the literature, and here is the model base standard error.

164
00:19:07,570 --> 00:19:14,170
And again, I boxed it in red. That's the main part that changes between these the robust and the model based.

165
00:19:14,740 --> 00:19:26,110
And when I look at the variability over here in SAS, this is super, super tiny for these standard errors and the robust.

166
00:19:27,260 --> 00:19:35,600
Is it looks quite different. Right. So something is off here with this model based assumption.

167
00:19:35,600 --> 00:19:42,530
It's not working well. And so model based standard errors are very different from robust standard errors.

168
00:19:42,890 --> 00:19:52,760
And SAS doesn't give you any other symptom. But this to tell you that this is not a good correlation assumption to make for this dataset.

169
00:19:52,760 --> 00:19:58,430
Even though it seems like it's the most general case, this is the only symptom SAS will give you that something is off.

170
00:19:58,430 --> 00:20:06,379
The model isn't fitting this well are will actually tell you don't do this.

171
00:20:06,380 --> 00:20:13,940
This is awful. I mean doesn't say those words, but it'll give you results with not a number and you'll know you don't have a real model fit.

172
00:20:14,840 --> 00:20:16,970
But you have to not look for this and sas.

173
00:20:18,740 --> 00:20:25,640
So the unstructured assumption seems to have destabilized the model, fit the data sets to small with just 24 outcomes.

174
00:20:25,790 --> 00:20:28,819
You know, for four for each of six subjects,

175
00:20:28,820 --> 00:20:34,760
it's too small to support the extra number of parameters that are required to fit this correlation matrix.

176
00:20:34,760 --> 00:20:43,020
And that's why it kind of destabilized. So I'll show our results in a moment here.

177
00:20:43,020 --> 00:20:48,839
But so this is, again, the the comparison of parameters and comments relative to that.

178
00:20:48,840 --> 00:20:58,919
So the estimates the same regardless that for how you model the variability here, this is the sandwich or robust or empirical standard error.

179
00:20:58,920 --> 00:21:05,940
You know, the one that we in four small datasets we're looking at this mainly to see which of the model based is the closest to that.

180
00:21:07,440 --> 00:21:17,580
And you know in this case for the unstructured Bayes standard error, it's helping us rule out this as a reasonable choice for the data.

181
00:21:18,660 --> 00:21:24,950
So the advantage of the method is that you have the availability of this robust empirical standard,

182
00:21:24,960 --> 00:21:29,010
which I'm just using all three times support every time standard error estimate.

183
00:21:29,010 --> 00:21:34,979
That's something that, you know, is there to protect you in large data sets because this should be reasonable,

184
00:21:34,980 --> 00:21:39,540
even if your model based standard errors are wrong when you have large data.

185
00:21:39,540 --> 00:21:43,140
I just use this result to get my P values.

186
00:21:43,680 --> 00:21:47,339
So for the large samples, robust empirical standard errors are reasonable.

187
00:21:47,340 --> 00:21:54,750
Even if you got your type equals part of your model wrong, it fixes it up.

188
00:21:55,830 --> 00:22:05,760
For small samples. People have done simulation studies and seen that that the same column here the empirical standard errors are

189
00:22:05,760 --> 00:22:12,540
more variable than correctly specified model based in our ears and tend to underestimate the true standard error.

190
00:22:13,950 --> 00:22:21,000
So, you know, if they underestimate the true standard error, that means you're you're type one error for these is too high.

191
00:22:21,390 --> 00:22:26,160
If you are trying to use the these kinds of standard errors.

192
00:22:28,810 --> 00:22:36,549
To analyze a small data set. And so for pill type three, you're kind of seeing that this estimate is kind of on the low end.

193
00:22:36,550 --> 00:22:40,060
That might be something that's corresponding to what they saw in simulation.

194
00:22:40,060 --> 00:22:46,180
It's kind of hard to tell for any one particular data set if that happened, but that is what they saw in simulation.

195
00:22:47,560 --> 00:22:54,070
And so you can use this column, though, to help rule out poor model based choices.

196
00:22:54,070 --> 00:22:59,920
So the empirical centers above validate this compound symmetry or exchangeable assumption,

197
00:23:00,430 --> 00:23:10,059
since both the robust and the compound symmetry based standard estimates are reasonably close and all three of these values are kind of in this range,

198
00:23:10,060 --> 00:23:20,220
more or less, except for this guy. It completely rules out using the unstructured based estimates.

199
00:23:20,580 --> 00:23:26,880
So you have to look at this every time you use one of these generalized estimating equation models.

200
00:23:27,330 --> 00:23:34,020
You have to know to look for these differences between your choice and the robust choice.

201
00:23:35,290 --> 00:23:41,050
Because if you just used the results without thinking, you could be very wrong.

202
00:23:42,730 --> 00:23:46,250
All right. And Seth doesn't explicitly tell you this.

203
00:23:46,270 --> 00:23:49,889
You have to know. More comments.

204
00:23:49,890 --> 00:23:55,020
This is just the same stuff copied up here. The estimated predictor coefficients.

205
00:23:55,020 --> 00:23:59,489
These estimates aren't affected by the standard estimation procedure.

206
00:23:59,490 --> 00:24:06,870
So people say these estimates are always valid, you know, but the P values depend on the standard errors chosen for the analysis.

207
00:24:06,870 --> 00:24:13,710
So the part that you rely on to protect yourself from public embarrassment by having something no one else can validate,

208
00:24:14,000 --> 00:24:18,810
you know, that part is affected by which standard error you choose.

209
00:24:20,190 --> 00:24:26,729
There's very limited model fit information. When you're using these G models, there's no likelihood ratio tests.

210
00:24:26,730 --> 00:24:30,210
They're avoiding the use of the likelihood in this approach.

211
00:24:30,990 --> 00:24:40,380
So there's no liquid ratio test, there's no AIC that you can use to choose between these things for the general assessment equation kind of methods.

212
00:24:40,890 --> 00:24:46,680
So there's very little guidance on choosing the correct correlation structure other than this eyeball thing.

213
00:24:49,780 --> 00:24:54,129
It's possible that a wrong correlation structure using a small number of parameters will fit

214
00:24:54,130 --> 00:24:59,980
better than a correctly specified correlation structure using many parameters in these small.

215
00:25:01,280 --> 00:25:02,329
Data sets, right?

216
00:25:02,330 --> 00:25:10,610
Because it's not like the unstructured correlation assumption can be wrong because it's supposed to be very broad and cover all things,

217
00:25:10,940 --> 00:25:13,220
but it can't be estimated by the data.

218
00:25:13,700 --> 00:25:23,060
So it's you know, in this case, even if we can't say unstructured is an incorrect assumption, it can't be fed by the data.

219
00:25:23,630 --> 00:25:34,180
So it can't be used. So in this case, the compound cemetery based tenders are probably the best choice to use in the manuscript summary of the data.

220
00:25:34,570 --> 00:25:40,510
And so this is kind of the way you would structure that kind of sentence that both the tablet and capsule

221
00:25:40,510 --> 00:25:47,950
formulations demonstrated similar significant decreases in grams per day of fecal fat when compared to placebo.

222
00:25:48,310 --> 00:25:55,600
And so I have here like tablet colon was 21.6 less grams per day.

223
00:25:55,600 --> 00:26:00,970
So I kind of just instead of more per day, I just said less per day and switched my sign there.

224
00:26:01,510 --> 00:26:08,260
And then I have the 95% confidence interval where again I switched the sign because I said less instead of more and my p value

225
00:26:08,260 --> 00:26:17,650
over here and then for capsule I have again I just phrased it so that it I could say positive numbers in my sentence here.

226
00:26:18,130 --> 00:26:31,900
And so for the the the capsule, I have 20.7 less grams per day, 95% confidence interval, 8.1 to 33.2 grams less per day.

227
00:26:31,900 --> 00:26:38,280
And then the p value. Right. And then for the last Pell type,

228
00:26:38,910 --> 00:26:48,230
the sentence might be the much smaller decrease observed for the capsule formulation was not statistically significant when compared to placebo.

229
00:26:48,240 --> 00:26:58,440
And I'm guessing I don't know what they did here. I just it could be none in this study, seven less grams per day.

230
00:26:58,470 --> 00:27:00,360
So I switched design here again,

231
00:27:01,170 --> 00:27:13,290
95% confidence comes from here and here I to be careful because it's 19.6 grams less for this 1 to 5.5 grams per day, more with the p value.

232
00:27:15,700 --> 00:27:20,110
Okay. So things to notice here.

233
00:27:20,780 --> 00:27:28,970
This table. I mean, we know it came from doing an analysis where you took into account the dependance between measures,

234
00:27:29,360 --> 00:27:33,890
but the table itself looks like any other table you interpret.

235
00:27:34,490 --> 00:27:37,819
And here it's we assumed a normally distributed outcome.

236
00:27:37,820 --> 00:27:43,490
There was no link like logit or log or anything to take into account.

237
00:27:43,850 --> 00:27:48,470
So all of these interpretations are similar to what you would do with just regular linear regression.

238
00:27:49,810 --> 00:27:55,990
So it's so I don't know if if this was completely clear when I read the manuscript were these sentences,

239
00:27:56,350 --> 00:28:00,670
but that means that the parameters are for a one unit increase.

240
00:28:01,180 --> 00:28:07,149
You know, one unit increase in the parameter corresponds to that change on the outcome.

241
00:28:07,150 --> 00:28:20,230
So a one unit increase when you look at pill type is going from 0 to 1, the placebo to this first pill type, the tablet.

242
00:28:22,520 --> 00:28:31,520
Right. So if I had wanted to write this manuscript using these signs, I would have said that fecal fat was lower.

243
00:28:33,170 --> 00:28:36,649
I'll wait. That is what I did. That. Yeah.

244
00:28:36,650 --> 00:28:39,760
The difficult that was. Actually, I.

245
00:28:39,790 --> 00:28:48,320
Yeah. I guess this is it is tricky as I thought actually, right,

246
00:28:48,330 --> 00:28:57,060
that this is the vehicle that is this much lower when you're comparing the placebo versus the.

247
00:28:58,090 --> 00:29:03,320
The Pill type one. And here.

248
00:29:04,610 --> 00:29:09,440
Going from one higher 2 to 1 less.

249
00:29:09,980 --> 00:29:14,780
Again, this is just like you would interpret for linear regression, right?

250
00:29:15,820 --> 00:29:20,110
So whatever skills you learned to do this for linear regression, we're using them again.

251
00:29:20,320 --> 00:29:27,670
We just fixed up the P values and this is going to be a common theme when we're using these generalized estimating

252
00:29:27,670 --> 00:29:34,120
equations where you're using proc gen mod and and taking into account what the repeated statement things are correlated,

253
00:29:34,300 --> 00:29:37,630
whatever model you have used in the past.

254
00:29:38,110 --> 00:29:42,670
Taking into account the correlation, we'll give you a table that's that similar to interpret.

255
00:29:43,030 --> 00:29:48,610
So right now we're focusing on normally distributed outcomes, but eventually we'll come back to this after the break.

256
00:29:48,940 --> 00:29:52,420
Well, model dependent. Yes. No outcomes with a logit link.

257
00:29:53,390 --> 00:29:59,920
One model dependent count or rate outcomes with the log link and the tables will

258
00:29:59,920 --> 00:30:04,810
always look comparable to what you saw for the models without dependent outcomes.

259
00:30:05,260 --> 00:30:11,469
So writing these sentences is going to be a breeze. It's modeling the standard error correctly.

260
00:30:11,470 --> 00:30:12,730
That's going to be the challenge.

261
00:30:18,390 --> 00:30:29,160
So when you look at our code for G.E., it's a little disappointing because at least four small data sets for large data sets are works great.

262
00:30:29,160 --> 00:30:34,560
But for small data sets, it's very hard to locate the models standard errors.

263
00:30:35,130 --> 00:30:42,690
And you can actually find them. But then it it's a lot harder to use them.

264
00:30:43,140 --> 00:30:47,670
And it'll it'll just give the robust results.

265
00:30:47,670 --> 00:30:53,220
And otherwise you have to dig and do extra programing to figure out what the model based stuff was doing.

266
00:30:53,790 --> 00:30:56,939
And you don't the algorithm is a little bit different.

267
00:30:56,940 --> 00:31:03,479
The results don't match SAS for the model based standard errors and they're further away from the robust ones.

268
00:31:03,480 --> 00:31:07,830
So they just gave a very unsatisfying feel for this particular data set.

269
00:31:08,160 --> 00:31:10,380
It's hard to generalize based on one data set,

270
00:31:10,770 --> 00:31:18,870
but the inconvenience of not producing a separate table for your model based results is is kind of a problem,

271
00:31:18,870 --> 00:31:24,690
I think, for our if you have a small data set, so there will be big data sets and then you can be fine.

272
00:31:25,020 --> 00:31:33,810
So to get into the are a loaded some packages here the main package is G pack.

273
00:31:34,380 --> 00:31:37,680
I'm sure I did this because I talked about guys. Right.

274
00:31:38,840 --> 00:31:42,730
I had to have done this. I did get this far, guys. You you.

275
00:31:42,740 --> 00:31:45,860
I did. I remember making the geese comment.

276
00:31:45,860 --> 00:31:49,980
Right that I always want to say geese. Is this just in my head?

277
00:31:50,030 --> 00:31:53,819
Did you. Okay.

278
00:31:53,820 --> 00:31:57,420
I might have jumped around. All right, so. All right, that's fair.

279
00:31:58,050 --> 00:32:04,230
I trust your memory more than me because I'm old. So. So you use this GSC formula with the.

280
00:32:05,010 --> 00:32:11,100
This is the same kind of formula you would have done in linear regression where you have as a factor to do categorical pill type.

281
00:32:12,290 --> 00:32:20,690
Here. The correlation structure is specified over here and the compound symmetry is called exchangeable and ah, they use the other term for it.

282
00:32:22,280 --> 00:32:31,610
And you can also request the robust daily standard error by looking using this command.

283
00:32:32,000 --> 00:32:39,620
And you can actually get to the model base standard error using this command this be made in naive.

284
00:32:42,970 --> 00:32:46,510
And so here is the output you get.

285
00:32:46,510 --> 00:32:48,879
It always reports the empirical,

286
00:32:48,880 --> 00:32:56,080
robust or sandwich standard or so here they're calling it the starting to the right sandwich standard error which is the same thing.

287
00:32:56,080 --> 00:32:59,110
That is the empirical thing in the sass output.

288
00:33:00,340 --> 00:33:04,570
And so that's pretty similar to SAS and you get similar results.

289
00:33:09,770 --> 00:33:17,250
Here is the the model based standard error and this does not match SAS.

290
00:33:17,270 --> 00:33:25,580
It's a little bit further away from the robust sandwich standard error than SAS was.

291
00:33:27,110 --> 00:33:31,730
So what was the moral basis of smaller than SATs, further away from their standard era than stars?

292
00:33:32,270 --> 00:33:37,450
So whatever algorithm they're using, at least for this small data set, it just didn't work really well.

293
00:33:39,770 --> 00:33:46,879
So are things better suited for use with larger data sets where the above standard errors behave very well and you would use them for your P values,

294
00:33:46,880 --> 00:33:54,110
but for small samples, the model based editors are more reliable than the empirical standard errors,

295
00:33:54,110 --> 00:33:59,270
and so artisans seem to have a good model based indoor air available for smaller data sets.

296
00:33:59,720 --> 00:34:02,570
And so even though we could peak at the model base standard errors,

297
00:34:02,570 --> 00:34:08,299
it doesn't give you that test statistic p value stuff automatically you would have to do those by

298
00:34:08,300 --> 00:34:14,810
hand if you really wanted to try to use RS model base standard errors and I and for this example,

299
00:34:14,810 --> 00:34:21,110
at least they didn't. They were worse than Sassa's and in that they were further away from the robust version.

300
00:34:23,390 --> 00:34:29,310
Our does do something that's nice though when we try to fit the unstructured model.

301
00:34:29,780 --> 00:34:37,850
I just kind of copied and pasted what it did and it gives a pretty clear problem, a symptom that it can't do it.

302
00:34:39,080 --> 00:34:48,020
So unstructured model by senator looks like this where it says any ends produce and that's a pretty clear symptom that it couldn't.

303
00:34:48,230 --> 00:34:53,120
It's like choking on it. I'm like doing the choking pantomime here that couldn't do it.

304
00:34:53,420 --> 00:34:58,310
So at least are it's better at signaling a problem.

305
00:34:58,700 --> 00:35:06,380
But you know, still, I think SAS might win for small data sets pretty clearly for this model.

306
00:35:07,130 --> 00:35:14,210
Okay, now in my memory, this is somehow a recap, but I think I just touched on it by skipping, like you said.

307
00:35:14,220 --> 00:35:22,550
So most crossover trials have much more information than the pancreatic enzyme data set that was in your textbook.

308
00:35:22,570 --> 00:35:35,450
So most crossover trials give you the order in which the various treatments were given, you know,

309
00:35:35,450 --> 00:35:42,319
the period calendar wise where the outcomes were measured, they give you a lot more information just then.

310
00:35:42,320 --> 00:35:44,990
Here is the outcome under the different types of settings.

311
00:35:45,650 --> 00:35:53,180
So this is just with two treatments, what it looks like when you have a crossover trial that's got more standard set up.

312
00:35:53,510 --> 00:36:00,850
So you randomize and the randomizing is the order with under which you get your treatment sequence.

313
00:36:00,860 --> 00:36:10,280
So the first arm here, everyone's randomized to get treatment A first, then B and in this arm people are randomized,

314
00:36:10,310 --> 00:36:17,060
get treatment B first, then A and that randomization helps, you know, balance.

315
00:36:19,260 --> 00:36:26,220
Potential biases that can come in if one is always given first like calendar time ballot biases can be helped by that.

316
00:36:28,250 --> 00:36:35,780
And so this study design is really only appropriate when you have a fair competition.

317
00:36:37,170 --> 00:36:43,770
For the treatments in the two periods you give the drugs, so you take treatment a first in this arm,

318
00:36:44,130 --> 00:36:48,030
you get your follow up data and then you have a washout period.

319
00:36:48,030 --> 00:36:56,220
And the purpose of the washout period is to have all of the benefits or harms from treatment A in that first period disappear.

320
00:36:56,640 --> 00:37:02,040
And when you are at your new baseline before treatment B,

321
00:37:02,070 --> 00:37:10,410
you're the hope is that the baseline is very comparable here before treatment versus here before treatment.

322
00:37:10,410 --> 00:37:14,399
B If there is baseline data measured, you're hoping it's comparable.

323
00:37:14,400 --> 00:37:18,570
Not all studies have the baseline measure done. They don't have a pretest done.

324
00:37:19,170 --> 00:37:24,990
So a lot of times you're working with an assumption that hasn't been verified with actual data.

325
00:37:26,280 --> 00:37:33,270
Okay. And if they aren't if it isn't like a reset to back to the same health status,

326
00:37:34,110 --> 00:37:42,780
then there's some problem with the study design because you need to have the second period treatment,

327
00:37:42,780 --> 00:37:49,650
have the same fair shot at treating the patient under the same conditions as the first treatment period.

328
00:37:50,640 --> 00:37:53,700
And so I'm going to talk more about some of the data issues.

329
00:37:53,700 --> 00:38:01,090
But if this if this baseline period isn't comparable on average, you know,

330
00:38:01,110 --> 00:38:06,720
before these two different treatments that are given to the same person, your study design has messed up seriously.

331
00:38:09,600 --> 00:38:19,320
Okay. So pretest data can be quite important since it gives us information about whether outcomes return to baseline values after the washout period.

332
00:38:19,860 --> 00:38:24,780
But you don't always get to the pretest data.

333
00:38:25,080 --> 00:38:31,760
And the example that I'm going to show to you is, is very much in this scenario.

334
00:38:31,770 --> 00:38:34,980
So it's very instructive.

335
00:38:35,700 --> 00:38:41,969
So some general comments are that caution must be taken when comparing treatments given to the same person.

336
00:38:41,970 --> 00:38:46,920
Over time, there must be no carryover effects or period by treatment interactions.

337
00:38:47,250 --> 00:38:50,040
This is jargon. I'm going to explain to you what this means.

338
00:38:50,040 --> 00:38:57,480
But it all has to do with things not being fairly reset to baseline before period to treatment begins.

339
00:38:59,470 --> 00:39:04,209
So treatment carryover effects is what it sounds like it subtle lingering physiological effects

340
00:39:04,210 --> 00:39:09,400
in the second treatment period which can usually be prevented with a suitable washout period.

341
00:39:10,690 --> 00:39:16,060
As long as you haven't done anything silly, like totally cure the patient, you cure the patient.

342
00:39:16,300 --> 00:39:20,830
You can't give that patient treatment B or whatever the second period treatment is.

343
00:39:24,110 --> 00:39:32,810
Period by treatment interactions happened when the underlying patient condition changes independently of the treatments understudy.

344
00:39:33,020 --> 00:39:36,829
So you want to study treatment versus treatment.

345
00:39:36,830 --> 00:39:42,410
B But meantime, somehow between the first and the second treatment period,

346
00:39:42,500 --> 00:39:50,810
some concomitant therapy is come up that everybody is taking suddenly or there's better auxiliary cure generally.

347
00:39:51,290 --> 00:40:01,550
Or it could be that the disease has progressed substantially in, you know, before they get to try the second period treatment.

348
00:40:02,550 --> 00:40:04,800
Could be lifestyle changes, temporal trends,

349
00:40:04,800 --> 00:40:13,020
all kinds of things that change between period one when they're taking that treatment and period two where they're taking the other treatment.

350
00:40:13,410 --> 00:40:19,360
Things that change over time between those, that's a period by treatment interaction.

351
00:40:19,380 --> 00:40:23,520
And so if you have a long study over calendar time,

352
00:40:23,520 --> 00:40:28,320
this is a real danger because you're not the only study in town probably trying to help these people.

353
00:40:28,770 --> 00:40:37,260
And things can happen. If you have a long will and crossover trial, things can happen to affect your period to data.

354
00:40:39,800 --> 00:40:42,920
Think of all the crossover trials that were going on when COVID started.

355
00:40:44,270 --> 00:40:53,840
Right. They probably were wringing their hands because that was a big period by treatment interaction issue that would have been introduced then.

356
00:40:57,180 --> 00:41:01,860
There's also something called treatment by treatment sequence interaction that can occur.

357
00:41:01,890 --> 00:41:12,660
So for instance, if the order of the treatment you're given really changes the perceived benefit of treatment, that's a problem, right?

358
00:41:12,680 --> 00:41:14,130
If that's the only thing that shifted.

359
00:41:14,520 --> 00:41:22,049
So for instance, if it's easier to unblind a treatment group when given in a certain order that could and that affects patient outcomes,

360
00:41:22,050 --> 00:41:28,040
you could be in trouble. This is something that I suspect is happening with our example I'm going to show you.

361
00:41:28,050 --> 00:41:33,959
So the example I'm going to show you is children who had bedwetting issues and they were

362
00:41:33,960 --> 00:41:38,700
studied for the number of dry days out of 14 when they had little different treatment,

363
00:41:39,150 --> 00:41:49,020
either a placebo or a drug. And it if you can tell that you're an act of treatment and you perceive yourself as doing better,

364
00:41:49,530 --> 00:41:57,090
and then you perceive that treatment being taken away, then you might like.

365
00:41:58,030 --> 00:42:07,780
Act out, sabotage your results, you know. So we're going to see something that might have been the case with this treatment

366
00:42:07,780 --> 00:42:12,700
by treatment sequence where if you got placebo first and then treatment,

367
00:42:12,700 --> 00:42:14,709
it didn't look like treatment was so great.

368
00:42:14,710 --> 00:42:23,950
But if you got treatment first and then placebo stuff was so bad in the second period that it kind of looked like there was a treatment effect.

369
00:42:23,950 --> 00:42:28,959
But it might have been just kids acting out when they had that sequence of treatment followed by placebo.

370
00:42:28,960 --> 00:42:35,510
So we'll see that soon. So when any of these are present, the treatment benefit of the second period is misrepresented.

371
00:42:35,510 --> 00:42:41,510
And there's so many ways this stuff can happen that I try to avoid cross over trials.

372
00:42:43,010 --> 00:42:52,730
I actively try to avoid crossover trials when someone suggests them because you could end up having one of these things happen quite easily.

373
00:42:53,180 --> 00:43:03,410
Even even that said, sometimes you're forced into this design by practical situations and I could tell you stories about this, but in some situations.

374
00:43:05,680 --> 00:43:12,130
Pragmatically, this is the only design that can happen, and so you use it in those cases.

375
00:43:13,810 --> 00:43:16,390
Maybe you can get me to gossip later about stuff like that.

376
00:43:18,390 --> 00:43:24,290
So in the pancreatic enzyme crossover trial, we didn't have any data on treatment periods or sequences in the textbook.

377
00:43:24,300 --> 00:43:26,100
It didn't tell us anything about that stuff.

378
00:43:26,100 --> 00:43:32,219
So we couldn't investigate carryover effects or period effects or treatment by treatment sequence effects.

379
00:43:32,220 --> 00:43:40,590
We just didn't have the ability to do that. So without that information, it's hard to assess whether those problems existed in the data set.

380
00:43:40,590 --> 00:43:46,170
And our interpretation are usually this information is collected and scrutinized carefully.

381
00:43:46,470 --> 00:43:52,020
And so we're going to consider the following dependent outcomes data from a crossover trial with two study periods.

382
00:43:52,020 --> 00:43:58,140
So here is our and your ace examples and your ACS is the bedwetting diagnosis.

383
00:43:58,620 --> 00:44:10,260
So patients children were randomized to 14 days of drug either preceded by or followed by 14 days of placebo for the treatment of aneurysms.

384
00:44:10,620 --> 00:44:16,020
And the outcome measured was the number of dry days or dry nights rather out of 14.

385
00:44:17,930 --> 00:44:25,450
And I'm going to show you the date on the next slide. This is the it's an old article that this data comes from.

386
00:44:25,460 --> 00:44:28,490
And so here's what the data set looks like.

387
00:44:28,490 --> 00:44:36,050
And it's a bit it's a simple dataset in some sense, but there's a lot of variable details you need to know.

388
00:44:37,790 --> 00:44:41,440
So what are the potential outcome variables and what are later?

389
00:44:41,450 --> 00:44:46,100
What are the covariates that you might want to look at? So here is group one.

390
00:44:46,490 --> 00:44:50,899
Group one was randomized to get the drug first, followed by placebo.

391
00:44:50,900 --> 00:44:57,410
So what I've done here to help us keep track is I put subscripts for the period.

392
00:44:58,070 --> 00:45:07,400
So drug subscript one means they got the drug in period one P with a two means they got placebo in period two.

393
00:45:07,610 --> 00:45:12,590
And so group two was randomized to get placebo first and drug second.

394
00:45:13,040 --> 00:45:19,729
So the Substance Group again is helping us keep track of the period that goes along with that.

395
00:45:19,730 --> 00:45:25,940
So period one they got placebo, period, two, they got drugs as were the subscripts.

396
00:45:26,720 --> 00:45:30,110
And so here is the data for sequence one.

397
00:45:30,110 --> 00:45:34,040
These are the number of dry days out of 14 when they were given the drug here,

398
00:45:34,040 --> 00:45:37,850
the number of dry days and 14 when they were given placebo in Group one.

399
00:45:38,830 --> 00:45:42,760
And the delta I want you to pay attention to.

400
00:45:42,790 --> 00:45:49,270
For the Delta, I'm always taking the number of dry days on drug and subtracting the number of dry days on placebo.

401
00:45:49,510 --> 00:45:55,480
And I do that for both arms. So here drug is first.

402
00:45:55,510 --> 00:45:58,790
You know, I'm subtracting placebo dry days from drug dry days.

403
00:45:58,790 --> 00:46:07,420
So that delta is always looking at the number of days dry that you gained when you moved to the drug.

404
00:46:09,740 --> 00:46:18,140
So if you have positive numbers here, it means that you had more dry days when you were on the drug.

405
00:46:18,770 --> 00:46:23,030
Right. So positive numbers are good for how that worked.

406
00:46:24,310 --> 00:46:28,490
And so and there's some summary statistics down here as well.

407
00:46:28,510 --> 00:46:32,260
Means standard deviation, standard errors.

408
00:46:32,680 --> 00:46:37,100
And we're going to look at plots of this right away.

409
00:46:37,120 --> 00:46:40,870
The mean of the dry days that you gain on drug.

410
00:46:41,920 --> 00:46:45,610
Our higher for this group then this group.

411
00:46:46,180 --> 00:46:51,220
Right. And I know that the scale is kind of small here.

412
00:46:51,730 --> 00:46:58,780
It's like roughly two extra dry days in group one than gains in group two.

413
00:46:58,900 --> 00:47:02,770
Right. And so this is the group where they were given the drug first.

414
00:47:04,350 --> 00:47:09,280
And this is the group where they were given a.

415
00:47:11,090 --> 00:47:16,400
The drug second. Mm hmm.

416
00:47:17,180 --> 00:47:31,750
Is that right? That is right.

417
00:47:32,020 --> 00:47:35,410
Okay. I just went mental far.

418
00:47:35,440 --> 00:47:48,479
But this is. This is right. So the the data set in terms of notation, we're going to have 29 paired dry days when we get together.

419
00:47:48,480 --> 00:47:53,400
All of the individuals, there's 29 total their ID numbers over here.

420
00:47:54,930 --> 00:47:58,229
And we're going to have two different ways to analyze the data.

421
00:47:58,230 --> 00:48:08,459
So if we analyze outcomes as dependent outcomes using, say, the way we've been doing the G, we're going to look at that approach.

422
00:48:08,460 --> 00:48:11,610
And there are a lot of advantages to that approach that we're going to go over today.

423
00:48:14,000 --> 00:48:19,340
But there's a second where you can analyze the data using your existing linear regression.

424
00:48:19,370 --> 00:48:26,449
So one thing that you could do with this data is analyze this column of differences, right?

425
00:48:26,450 --> 00:48:30,650
So this is a single number and there's only one for each kid.

426
00:48:31,570 --> 00:48:35,740
The difference is the number of days you gained when you were on drug versus placebo.

427
00:48:36,250 --> 00:48:41,740
And, you know, there's this temptation and honestly, it's it's not unreasonable.

428
00:48:42,340 --> 00:48:49,450
There's a to analyze just this single one dimensional outcome that is independent across individuals.

429
00:48:49,450 --> 00:48:52,840
So we're going to look at that independent outcomes approach.

430
00:48:53,320 --> 00:48:58,750
And if all goes well, this is a fine approach.

431
00:48:58,750 --> 00:49:04,640
If everything went well with your design, this is a fine approach to use and it doesn't require you to use GP.

432
00:49:04,690 --> 00:49:11,319
So people really like to use the simpler model, but we're going to see the advantages of using the G approach as well.

433
00:49:11,320 --> 00:49:16,570
It's it helps us more with diagnostics for the study than this other approach.

434
00:49:17,740 --> 00:49:23,710
There's a few different advantages, actually. So what are the potential predictors that we have?

435
00:49:23,780 --> 00:49:31,750
We have the outcomes now and we have the predictors we can actually evaluate are different depending on the outcome choice we make.

436
00:49:33,640 --> 00:49:35,350
So here's the column.

437
00:49:35,350 --> 00:49:45,159
If we look at DRI de pairs and we use E, we can look at covariates like, you know, does the outcome correspond to drug or placebo?

438
00:49:45,160 --> 00:49:49,390
You know, we'll have a cover for each of those outcomes in the pair.

439
00:49:50,590 --> 00:49:58,420
Can we look at the sequence? You know, we'll have whether the drug was given first, the placebo was given first to this patient.

440
00:49:58,480 --> 00:50:04,360
So that's a patient level. Covariate What's the period of outcome?

441
00:50:04,360 --> 00:50:07,840
Was this outcome from period one or period two?

442
00:50:09,420 --> 00:50:14,730
And what were the number of days that were dry when they were taking placebo?

443
00:50:14,760 --> 00:50:19,650
We can use that as a covariate and we can look at interactions between any of these things.

444
00:50:20,490 --> 00:50:28,139
Okay. So that's the keywords we can look at when we look at the use the GP approach to this, when we look at the dry day difference,

445
00:50:28,140 --> 00:50:36,150
will be modeling the number of dry days you gain from drug with and there'll be one of those per person.

446
00:50:37,270 --> 00:50:44,560
And ah, because we kind of took the difference between the drug and the placebo in the outcome.

447
00:50:45,910 --> 00:50:49,780
We don't have a single parameter that's going to tell us the effect of drug.

448
00:50:50,820 --> 00:50:55,110
Right. It's already in the way we created the outcome. The outcome is the difference.

449
00:50:55,140 --> 00:51:01,650
Both drugs are kind of being used in the outcome. So there's not like a cover for drug that will have come out in the model.

450
00:51:02,190 --> 00:51:12,180
So when you model these differences, you can look at the effect on the dry days gained for different covariate profiles.

451
00:51:13,360 --> 00:51:23,850
But if you want to get the overall effective drug adjusted for staff, you kind of have to average across all the covariate profiles in your data.

452
00:51:23,860 --> 00:51:27,640
You know, what was the average gain?

453
00:51:30,120 --> 00:51:35,699
In dry days for all the choices, but a parameter isn't available to you.

454
00:51:35,700 --> 00:51:41,790
So when we use G, we have one parameter that we can put our manuscript where the sentence, when we use this,

455
00:51:42,600 --> 00:51:52,049
if if we're adjusting for other things that are study design issues, we kind of have to average the gains across those profiles in the data set.

456
00:51:52,050 --> 00:51:55,500
We don't have a parameter we can use for that, so we'll see that in a minute.

457
00:51:56,620 --> 00:52:04,600
So it's a little bit more work in that sense. So we can still talk about the sequence, even though this is a difference, that person,

458
00:52:04,990 --> 00:52:08,500
that person level covariate of whether they were given drug first or placebo first,

459
00:52:08,500 --> 00:52:12,400
we can still look at so we can look at that potential bias and adjust for it.

460
00:52:13,720 --> 00:52:20,860
We don't have a way to look at period one versus period two because both periods were used in defining the outcome.

461
00:52:20,870 --> 00:52:25,540
It's not like there's one period that applies to a person in their difference.

462
00:52:25,570 --> 00:52:32,080
Both of those were used in defining the outcome, so we can't get a parameter for adjusting for four period.

463
00:52:33,220 --> 00:52:37,060
We can use the number of dry days on placebo and we can look for interactions.

464
00:52:37,600 --> 00:52:45,190
So the G will let us have a nice, easy treatment parameter for our manuscript for this sentence,

465
00:52:45,190 --> 00:52:48,640
and it will let us look at period effects and adjust for them a little bit better.

466
00:52:51,980 --> 00:52:56,240
So in this study, unfortunately, there's no pretest information collected.

467
00:52:56,270 --> 00:53:05,000
So, for instance, there's no number of dry days before the drug was given for this first sequence.

468
00:53:05,300 --> 00:53:11,780
And there's and there's no dry days right after the wash up period when there are nothing and there's no you know,

469
00:53:11,780 --> 00:53:19,879
they don't they just don't have the pretest. It would have increased the overall length of the study by 14 days for each pretest.

470
00:53:19,880 --> 00:53:21,350
And it just wasn't practical.

471
00:53:21,380 --> 00:53:29,270
I mean, these are kids and parents might have been going crazy by the end of the study if it was extended by all these extra pretest.

472
00:53:29,270 --> 00:53:37,150
So pragmatically, they didn't do it. So the closest thing we have to pretest is the number of dry placebo days.

473
00:53:37,160 --> 00:53:40,820
It's almost like that's an adjustment factor for how they did when they weren't on drug.

474
00:53:41,870 --> 00:53:48,829
If the study design affects this number of dry placebo days, then that game is not successful.

475
00:53:48,830 --> 00:53:57,560
But it is the closest thing they have to adjusting for non-drug outcomes that they have.

476
00:53:59,240 --> 00:54:11,000
So here is creating the two versions of the data set, the Geelong dataset version and the paired outcomes dataset version.

477
00:54:11,000 --> 00:54:17,060
And in the interest of time and probably in, you know, just overall boredom,

478
00:54:18,020 --> 00:54:21,620
I'm not going to go through too much of the details of how the data sets are made.

479
00:54:21,920 --> 00:54:29,209
But I do want you to note the variables are so for the data I.D. is the kid drug is one.

480
00:54:29,210 --> 00:54:37,220
If they got the drug for that outcome row, the number of dry days is the outcome.

481
00:54:38,090 --> 00:54:47,450
Right. So the you know, so this is going to be the number of dry days when they were on drug because drug is one.

482
00:54:47,450 --> 00:54:51,439
And here's the number of dry days when they were on placebo, when drug was zero.

483
00:54:51,440 --> 00:54:53,600
So that's what it looks like for patient one.

484
00:54:54,020 --> 00:54:59,299
And placebo dry days is always going to be the number of dry days and placebo in this person that was five.

485
00:54:59,300 --> 00:55:04,900
So they have that covariate for their outcomes for the differences data set.

486
00:55:04,910 --> 00:55:10,040
If you really love code and you want to just see how I made that data set, go for it.

487
00:55:10,040 --> 00:55:20,330
But I mainly want to point out the differences is the number of dry days you gain out of 14 when you take the drug.

488
00:55:21,310 --> 00:55:24,969
And Drug one and drug two.

489
00:55:24,970 --> 00:55:32,620
I created to be the treatments that they were given for the first and the second period, respectively.

490
00:55:32,980 --> 00:55:37,870
So this is like for me, it's almost like a subscript thing.

491
00:55:38,320 --> 00:55:44,210
So this is the drug one or zero for period one.

492
00:55:44,230 --> 00:55:50,190
So if it's a one, then they got the drug first.

493
00:55:50,200 --> 00:55:55,750
Otherwise it's a zero or sorry if they if it's a one, they got the drug in that period.

494
00:55:55,930 --> 00:56:03,390
Otherwise, it's a zero. And dry days are the corresponding outcomes.

495
00:56:04,500 --> 00:56:17,760
For those first and second periods. So here's what the data set looks like for the G.E. kind of model and for the dry day differences kind of model.

496
00:56:22,100 --> 00:56:27,290
Here's the same kind of exercise of creating the two data sets and are and again,

497
00:56:27,290 --> 00:56:30,499
I'm going to just it's here if you want to look through it and figure out how I did it.

498
00:56:30,500 --> 00:56:34,610
But otherwise I'm just going to skip it. And the data but the data sets look the same.

499
00:56:36,780 --> 00:56:45,270
And here is a nice plot of the data just looking at it with proc t test and so product t test.

500
00:56:45,330 --> 00:56:48,790
This is looking at the differences, the difference data set.

501
00:56:49,410 --> 00:56:52,680
We'll do that analysis first and then we'll go to the G analysis.

502
00:56:53,070 --> 00:56:58,440
And I do like this plot a lot. So there's an option in T test to do a crossover study.

503
00:56:59,160 --> 00:57:02,340
And so this is how you kind of get that code done.

504
00:57:02,340 --> 00:57:07,709
Crossover equals drug one, drug two. And this is the plot that you see.

505
00:57:07,710 --> 00:57:13,740
So the different colors of the lines have to do with the treatment sequence.

506
00:57:13,740 --> 00:57:19,800
So if it's a red line, it means that they got placebo first and then drug.

507
00:57:20,790 --> 00:57:25,140
If it's a blue line, they got drug first and then placebo.

508
00:57:25,650 --> 00:57:30,240
And so the treatment sequence, you know, is is based on the color.

509
00:57:30,630 --> 00:57:35,820
And the overall estimates are the thick lines for those two scenarios.

510
00:57:36,420 --> 00:57:44,250
The plot there does this clever thing where all of the values of dry days on drug are on the left, regardless of whether they got a first or second.

511
00:57:45,090 --> 00:57:54,090
So everybody in this plot, their drug, another result of dry days is over here and everybody on the right is their dry days under placebo,

512
00:57:54,090 --> 00:58:00,300
regardless of whether they got it first or second. So this crossover, you know, figures that out for you.

513
00:58:01,840 --> 00:58:09,430
And orders them so that you can kind of look at, you know, what happens in dry days when you're on drug versus placebo.

514
00:58:11,630 --> 00:58:15,050
So there's a couple of things I want you to notice about this.

515
00:58:15,200 --> 00:58:23,070
One is, you know, there's a big difference in the placebo draw days depending on the sequence that you got.

516
00:58:23,090 --> 00:58:27,390
So if so, what's this? What's this blue line?

517
00:58:27,410 --> 00:58:32,270
The blue line means they got drug, then placebo. So these people got drug first.

518
00:58:32,660 --> 00:58:38,090
If the kids knew if there was some side effects that made them know, oh, this is the drug.

519
00:58:38,120 --> 00:58:42,469
If they got unblinded, then, you know, it's possible.

520
00:58:42,470 --> 00:58:45,710
They felt like they were getting help with their bedwetting.

521
00:58:46,370 --> 00:58:53,629
And if they could tell when the that treatment was removed and they felt like that help was removed, you know,

522
00:58:53,630 --> 00:59:00,080
this number of less dry days on average that happen, that could be because they were frustrated kids.

523
00:59:00,680 --> 00:59:05,870
And so, I mean, some of these people were doing okay and then they just went, boom, down here.

524
00:59:07,140 --> 00:59:10,890
Again, this blue line is if they had drug first, then placebo.

525
00:59:12,340 --> 00:59:20,080
And so if an unblinding issue, that's what is driving this lower placebo value, it could have broken the design.

526
00:59:21,170 --> 00:59:24,950
There's not going to be a lot of statistical power to firmly say this is what happened.

527
00:59:26,970 --> 00:59:34,470
But I want you to pay attention also to the effect of the drug.

528
00:59:34,650 --> 00:59:39,330
Ah, it's very close over here. You know, the number of dry days on drug.

529
00:59:39,780 --> 00:59:46,370
And if you look at the blue line over here. This blue line is almost the same as this.

530
00:59:47,520 --> 00:59:54,690
Number of dry days on placebo for the other sequence. So it's it's.

531
00:59:57,680 --> 01:00:02,810
Kind of a little worrisome that you only see this huge change.

532
01:00:04,720 --> 01:00:10,810
When they got the drug first and not much change at all for the red line overall.

533
01:00:14,680 --> 01:00:21,580
So I think that these little boxes are just the comments on thicker lines or the means and the placebo seems lower when given in the second period.

534
01:00:23,050 --> 01:00:27,550
And actually that was the biggest difference from any of the other three points that were estimated.

535
01:00:28,870 --> 01:00:34,840
Okay. So we want to look at the analysis of treatment effect adjusted for sequence using the differences data set.

536
01:00:35,080 --> 01:00:38,260
And so if the study was conducted well in the patients remaining blinded,

537
01:00:38,260 --> 01:00:43,149
then the order of the randomized treatment should not matter in assessing the treatment differences.

538
01:00:43,150 --> 01:00:47,680
So we can test that with SAS by modeling the difference.

539
01:00:47,830 --> 01:00:57,550
And instead of just having an intercept where we're looking at the overall benefit, adjust for this, you know, which drug they were given first.

540
01:00:59,410 --> 01:01:06,129
And this is like even though we're count, you know, this is just like a regular linear regression where we only have one outcome per person,

541
01:01:06,130 --> 01:01:09,790
even though we're doing it through production where we don't have a repeated statement here.

542
01:01:10,510 --> 01:01:13,300
So it's just one outcome per person, assumed independent.

543
01:01:13,780 --> 01:01:24,350
And so we've got two parameters beta non beta one, and we can estimate the gain in dry days for all the patient profiles.

544
01:01:24,370 --> 01:01:27,910
So one of the patient profiles is when placebo was given first.

545
01:01:28,270 --> 01:01:34,480
So that's drug first equals zero. And that gain is just going to be the intercept.

546
01:01:36,260 --> 01:01:43,030
Right, because drug first is when placebos first. And then we can look at the gain for the other type of patient profile here.

547
01:01:43,030 --> 01:01:44,920
And that's when the drug was given first.

548
01:01:46,140 --> 01:01:55,500
And so when drug is given, first we have the intercept, but we also have drug first because, you know, that's a one when drug was given first.

549
01:01:57,170 --> 01:02:04,819
Yeah. So these two estimate statements are kind of the treatment benefit under two different scenarios.

550
01:02:04,820 --> 01:02:14,510
So to get the overall treatment benefit adjusted for this, we kind of have to over, you know, estimate the average gain across these two profiles.

551
01:02:14,510 --> 01:02:18,350
So we'll have to eventually do another estimate statement to get at that.

552
01:02:18,350 --> 01:02:22,010
But let's look at this first one. Here's our code that's comparable.

553
01:02:23,960 --> 01:02:28,020
You know, you've done this you've done this kind of thing with with contracts.

554
01:02:28,020 --> 01:02:33,020
It's doing the same stuff. And so here are the results.

555
01:02:33,380 --> 01:02:39,690
Here's the model with the intercept drug. First, an average gain in dry days on drug.

556
01:02:39,710 --> 01:02:48,740
Here's the way we would write that linear regression in the average gain when placebo was given first was 1.25 days.

557
01:02:48,740 --> 01:02:52,100
Right? Because drug first is zero when placebo is given first.

558
01:02:53,060 --> 01:03:02,450
And we don't even need an estimate statement to get that confidence interval because it is right here with the intercept and the P values right here.

559
01:03:02,750 --> 01:03:12,680
But we're going to have estimate statements too. So the gain when drug is first, we need to add 1.25 plus 1.57.

560
01:03:13,730 --> 01:03:23,690
So that's going to be 2.82 and you get your confidence limits and P value here so if drug I just just notice if if placebo is

561
01:03:23,690 --> 01:03:32,030
given first there's no significant benefit checked it but if drug is given first the significance suddenly pops to like extreme.

562
01:03:35,000 --> 01:03:40,970
So the average gain when the drug is given first 2.82 days with confidence, interval and p value.

563
01:03:45,450 --> 01:03:50,700
So overall, we didn't see a significant effect of the sequence.

564
01:03:52,010 --> 01:03:58,309
A of which was given first the sequence an active drug with given did not show a statistically significant effect on the dry day.

565
01:03:58,310 --> 01:04:02,720
So we don't get a significant signal that this was a problem.

566
01:04:04,040 --> 01:04:07,549
But that significant treatment effect only appears when the drug's given first.

567
01:04:07,550 --> 01:04:14,570
And that's somewhat concerning. And the power to detect these kinds of things is low in this dataset with 29 people.

568
01:04:16,610 --> 01:04:23,769
Same output for our. And let's take a ten minute to a five minute break.

569
01:04:23,770 --> 01:04:28,750
I feel like I want to get there more material. So 5 minutes, 910 will come back.

570
01:04:44,420 --> 01:04:58,190
Everybody can hit refresh on their election stuff. I.

571
01:04:59,540 --> 01:05:08,320
Yeah. Okay.

572
01:05:09,880 --> 01:05:13,320
It's like, okay. You.

573
01:10:23,880 --> 01:10:27,290
Oh. Okay.

574
01:10:28,540 --> 01:10:38,109
Let's get back to work, everybody. Okay. So we're still analyzing the the differences as outcomes and trying to account for the

575
01:10:38,110 --> 01:10:42,129
difference in the outcomes by reducing the dimension from two outcomes to one blocking.

576
01:10:42,130 --> 01:10:51,730
And the difference when we want to report the average treatment effect, though, we have to average across every patient profile that our model allows.

577
01:10:51,730 --> 01:10:56,500
So in this case, where we have the sequence order, this is how it would go.

578
01:10:58,250 --> 01:11:03,560
So used the model adjusting for sequence and take an average of the gains seen across the two sequences.

579
01:11:04,460 --> 01:11:14,780
So the algebra is going to look like this it's it's the beat or not was what we saw for the game for you know the.

580
01:11:17,480 --> 01:11:22,670
The placebo first, right? And Peter not pleased.

581
01:11:22,670 --> 01:11:27,590
Peter. One is the game we saw when the drug was given first.

582
01:11:27,710 --> 01:11:32,390
Correct. And so we're averaging those two by multiplying by half.

583
01:11:32,990 --> 01:11:38,730
And so this is the formula that we need to use for our control statements.

584
01:11:38,750 --> 01:11:47,450
So we need to figure out how many beta knots to have in our formula, how many betas for the drug first variable we need our formula.

585
01:11:48,110 --> 01:11:52,219
So when we plug in numbers, this is the beta, not this is the beta one really.

586
01:11:52,220 --> 01:11:56,270
We have to we have to pay two nights in here. So when we multiply.

587
01:11:59,260 --> 01:12:03,339
Oh, sorry. I think that I plugged it. Not in here, but then I put it plugged in.

588
01:12:03,340 --> 01:12:06,340
Better not place drug first over here just from the previous page.

589
01:12:06,820 --> 01:12:13,030
So on average you get 2.04 dry days on the drug when you average across the two sequences.

590
01:12:13,600 --> 01:12:16,270
We do need to figure out how to do this with an estimate statement.

591
01:12:16,270 --> 01:12:23,290
So when we're trying to figure out what number to put for beta or not, it's really a half times better not plus a half times or not.

592
01:12:23,620 --> 01:12:27,760
So we'll need one beta not term for our formula. Ordering a contrast.

593
01:12:28,390 --> 01:12:36,820
We only have a half times the beta for drug first, so there's going to be a point five for our formula with the drug first variable.

594
01:12:38,140 --> 01:12:42,400
So to estimate the average gain, that's where these numbers came from.

595
01:12:42,400 --> 01:12:50,590
The one and the point five came from looking at the multiple times beta not here and the multiple times drug

596
01:12:50,590 --> 01:12:57,280
first here and so that we get the same dry days on drug but now we have confidence limits and p values.

597
01:13:01,070 --> 01:13:06,320
So on average, the drug converts 2.0 for additional dry days,

598
01:13:06,320 --> 01:13:12,320
adjusting for the sequence of drug administration over 14 days of follow up and the confidence interval on the P values.

599
01:13:12,320 --> 01:13:19,640
So this analysis would suggest that there's a huge benefit to the drug adjusting for the the sequence.

600
01:13:21,470 --> 01:13:26,180
And here's the R code. It has very close, but not quite identical results.

601
01:13:26,180 --> 01:13:29,990
But you're putting in the one for the baby, not 2.5 for the beta one again.

602
01:13:32,230 --> 01:13:35,560
So the analysis should also adjust for baseline values if possible.

603
01:13:35,590 --> 01:13:39,940
This is the closest thing we have to pretest, like what they would be if they didn't have drug.

604
01:13:40,300 --> 01:13:45,450
So if we can adjust for that, that would be great. So we didn't have baseline dry days.

605
01:13:45,460 --> 01:13:51,040
We're just going to look at the dry days when placebo is a surrogate for where they were at baseline.

606
01:13:52,590 --> 01:13:56,670
And so in sense, when you're analyzing the differences as an outcome,

607
01:13:56,940 --> 01:14:00,810
we're putting in this covariate for the number of dry days when they were on placebo.

608
01:14:00,840 --> 01:14:12,270
That's the only thing that's changed. And I, I think you can ignore this estimate, David, because no, I think it's a carryover from the previous code.

609
01:14:12,390 --> 01:14:16,500
It doesn't really do anything because I've added in placebo dry days.

610
01:14:16,500 --> 01:14:19,580
So it's not really a useful formula.

611
01:14:19,590 --> 01:14:27,630
I guess it's if you had zero placebo dry days, what the average gain is, but that's not really within the range of the data, I don't think.

612
01:14:28,530 --> 01:14:36,360
And here's the same thing with our and again, I just kept the the contrast in there, but I don't think I'm using it at all.

613
01:14:36,990 --> 01:14:40,400
So here's the output from the model.

614
01:14:40,410 --> 01:14:51,450
So here's the effect of placebo dry days. And we're going to need to think about this for a minute to understand the same results when you have ah,

615
01:14:51,480 --> 01:14:55,650
you know, they're not algebraically identical, but they're close.

616
01:14:56,310 --> 01:15:00,930
And so this placebo dry day varies variable is very significant.

617
01:15:00,940 --> 01:15:04,649
And so what is this negative meaning that means that, you know,

618
01:15:04,650 --> 01:15:16,590
for every dry day hire you have when you're on placebo, the gain in dry days on drug is going down by a half.

619
01:15:17,790 --> 01:15:29,770
And so why on earth would that be? So the more dry days and placebo, the smaller the gains in dry days that you can have those negative coefficient.

620
01:15:30,780 --> 01:15:33,899
So we need to think about this as a concern.

621
01:15:33,900 --> 01:15:38,610
One is that the dry days are out of 14.

622
01:15:38,760 --> 01:15:47,850
So if you start out with a higher number of dry days on placebo, then there's not much you can gain.

623
01:15:48,910 --> 01:15:57,340
When you changed to drug. Right. So if you start off with 14 dry days when you're taking placebo, that difference can't increase.

624
01:15:58,060 --> 01:16:04,230
When you're on drug, you can only do a tie and have 14 days of dryness on drug, too.

625
01:16:04,720 --> 01:16:11,220
So there's a ceiling effect here that's affecting the results when we analyze the data this way.

626
01:16:11,440 --> 01:16:15,819
This is the weird part that's showing up because of that ceiling effect.

627
01:16:15,820 --> 01:16:24,070
When you look at the possible days gained, you have zero possible days gained if you started off with 14 dry days on placebo.

628
01:16:25,140 --> 01:16:35,220
There's just no possible way to gain. Concern, too, if there are crossover trial issues affecting the number of dry days measured on placebo.

629
01:16:35,550 --> 01:16:44,040
The number of dry placebo days may not be comparable across the two periods, and this plot did suggest that could potentially be a problem.

630
01:16:46,150 --> 01:16:52,540
And we just don't have high power to get it. Now, I want to move now to the approach for analyzing this data.

631
01:16:52,540 --> 01:16:59,919
So this is the approach where you have two outcomes for each kid and you have covariates that go along with each of those outcomes.

632
01:16:59,920 --> 01:17:05,430
So drug is now you know what drug corresponds to that outcome.

633
01:17:05,440 --> 01:17:15,280
There's two of them. And drug first is an individual level predictor that says whether they had the drug first yes or no.

634
01:17:17,230 --> 01:17:22,600
So this is the adjusting for the sequence of the drug assignment when we use the method.

635
01:17:22,600 --> 01:17:29,259
But I also have a whole bunch of other stuff here. Some of this, you know, we couldn't do with the differences.

636
01:17:29,260 --> 01:17:29,919
So for instance,

637
01:17:29,920 --> 01:17:37,870
adjusting for period is one of the analyzes that we couldn't do with the differences because we didn't have a real period covariate in that case.

638
01:17:38,740 --> 01:17:42,300
But we can certainly look for results adjusting for period using G.

639
01:17:42,310 --> 01:17:48,700
So we'll look at that adjusting for the sequence of drug assignment and period.

640
01:17:48,700 --> 01:17:58,509
We can do as well as that. Again, this is an analysis we couldn't do before because we didn't have a period variable looking at interactions.

641
01:17:58,510 --> 01:18:05,050
So we'll look at the interactions between the treatment and whether they got the drug first or not.

642
01:18:05,920 --> 01:18:14,710
And we'll look at interactions between the drug and which period they got in, which period the outcome came from.

643
01:18:15,490 --> 01:18:20,710
And then finally we'll look at adjusting for period drug first and placebo dry days.

644
01:18:21,100 --> 01:18:28,870
So this is comparable to one of the analysis it did, but adjusting for an additional variable of period in which the outcome was measured.

645
01:18:30,160 --> 01:18:34,870
So we have more possible results to use here to diagnose whether there's issues.

646
01:18:37,070 --> 01:18:41,900
So first first model adjusting for treatment sequence.

647
01:18:43,770 --> 01:18:47,070
So this is the variable that says whether they got drug first or not.

648
01:18:47,640 --> 01:18:55,710
And it's not statistically significant. This is a model that we could evaluate also with the differences analysis.

649
01:18:55,730 --> 01:19:00,780
So this is very similar to the analysis, except, look, we've got a parameter here.

650
01:19:01,470 --> 01:19:08,730
Here's our model that the dry days is the is an intercept plus this parameter times the

651
01:19:08,730 --> 01:19:14,700
drug that they were on minus this parameter time is whether they got drug first or not.

652
01:19:15,120 --> 01:19:25,170
And we have this lovely line here corresponding to drug that's going to tell us what's the impact of going from placebo to drug.

653
01:19:26,170 --> 01:19:31,870
In the data. And so we had to do some averaging of the day's gain.

654
01:19:31,960 --> 01:19:39,510
If you got placebo, first days gained, if you got drug first, we had to do this half contrast statement.

655
01:19:39,520 --> 01:19:46,479
It was extra work to get this thing. But with the G model, we just have to look at this row and that's where we put the information for.

656
01:19:46,480 --> 01:19:49,720
I mean, that's where we get the information for a manuscript or sentence.

657
01:19:50,170 --> 01:19:55,720
So that's advantage number one that there's the way the model is set up, lets you get this a little bit more easily.

658
01:19:56,620 --> 01:20:04,870
And so we can use just the regular way we think about contrast statements that is the the model for dry days.

659
01:20:04,870 --> 01:20:10,600
If we look in the case where they took the drug, we have the drug parameter popping up in the formula.

660
01:20:10,960 --> 01:20:19,720
If we look at the the number of dry days when they're not on drug, then there's a zero for that parameter.

661
01:20:19,720 --> 01:20:24,190
And so we get that nice manuscript where these sentence information all in one go.

662
01:20:27,740 --> 01:20:33,410
The sequence in which the active drug was given did not show a statistically significant effect on the dry days.

663
01:20:33,440 --> 01:20:38,990
Again, this is very low powered, but that's similar to what we saw with the difference outcome analysis.

664
01:20:39,470 --> 01:20:45,620
So both analyzes in the GEC setting are nicely tied together in one G model.

665
01:20:46,580 --> 01:20:52,490
So arguably you could get these results more quickly if you are familiar with this using how to use this model.

666
01:20:53,990 --> 01:21:02,810
We can also adjust for periods. So here is just putting in the the period variable whether the outcome came from period one or period two.

667
01:21:03,320 --> 01:21:09,140
And so it's a net. That analysis gives us a natural way to adjust or test for period differences.

668
01:21:10,220 --> 01:21:13,700
No, no statistical period differences were seen.

669
01:21:15,860 --> 01:21:21,319
The analysis with the single difference outcome can't distinguish between period effects and treatment sequence effects.

670
01:21:21,320 --> 01:21:30,620
They're algebraically linked. There's just no a covariate that is able to handle the period in the model.

671
01:21:32,450 --> 01:21:37,009
So your manuscript for this sentence you get very quickly average gain and droids on drug

672
01:21:37,010 --> 01:21:43,280
versus placebo was 2.4 from this line of the 14 follow up dates adjusted for treatment period.

673
01:21:43,700 --> 01:21:47,750
And here's the confidence that are all just from here and the p value.

674
01:21:50,570 --> 01:21:54,200
And here's just playing around now with what this model can do.

675
01:21:54,210 --> 01:22:01,880
So here's results adjusting for both period and the sequence of the drug and how the results change.

676
01:22:01,880 --> 01:22:12,230
And if we were doing the differences model, we couldn't adjust for period, you know, and now we get this one line that corresponds to the drug row,

677
01:22:12,230 --> 01:22:17,990
the output that we can use for our manuscript or the sentence where we've adjusted for period and the randomized period,

678
01:22:18,530 --> 01:22:22,130
randomized treatment sequence is easy.

679
01:22:22,640 --> 01:22:33,740
Here's looking at interactions with the treatment and whether you were given the drug first or not, and it's not significant.

680
01:22:34,490 --> 01:22:41,870
Here is an interaction looking at the treatment effect and whether it's the first period or not.

681
01:22:42,230 --> 01:22:43,460
Again, not significant.

682
01:22:43,910 --> 01:22:50,720
So it's very natural to test for treatment by treatment sequence interactions and treatment by period interactions when using the model framework.

683
01:22:51,280 --> 01:22:57,470
And at least one of these you just can't do when you analyze the differences as an outcome.

684
01:22:58,490 --> 01:23:03,350
So unfortunately, I keep coming back to this. The power for detecting these interactions is notoriously low.

685
01:23:03,680 --> 01:23:16,850
And when I ran a power analysis for detecting r say this this level of a parameter, there was only 27% power to detect that.

686
01:23:17,420 --> 01:23:27,380
And I want you to just appreciate the fact that this is a very big estimate of, you know, how it affects the number of dry days.

687
01:23:27,620 --> 01:23:34,970
The overall treatment benefit was two extra dry days, like 2.0 for something extra dry days on treatment.

688
01:23:36,600 --> 01:23:43,270
But this interaction. That effect size is on the order of three dry days.

689
01:23:43,270 --> 01:23:46,480
I mean, that's a big effect size and we can't detect it.

690
01:23:48,350 --> 01:23:52,580
We only have 27% power. So it's.

691
01:23:53,680 --> 01:24:03,580
Very difficult to come up with these evaluations and have statistics guide you on whether your study design for a crossover trial was solid or not.

692
01:24:07,550 --> 01:24:10,580
Okay. So how does the adjustment for dry days on placebo effect the story?

693
01:24:10,610 --> 01:24:18,830
Here it is. And the effect of the number of dry days on placebo is very significant.

694
01:24:19,520 --> 01:24:25,610
So an increase in dry days is seen with treatment when accounting for the number of dry days observed on placebo period and treatment sequence.

695
01:24:26,000 --> 01:24:29,209
Here are the new updated numbers for the treatment effect.

696
01:24:29,210 --> 01:24:37,700
Adjusting for all of these things. The number of dry days on placebo groups, the number of dry days on placebo is very significant,

697
01:24:37,700 --> 01:24:50,600
but we usually have to be very careful not to place too much emphasis on, you know, covariates that are tightly linked to the outcome.

698
01:24:50,840 --> 01:24:56,210
And we do have the number of dry days on placebo as one of the outcomes in our two.

699
01:24:56,870 --> 01:25:03,469
So I usually would not remark on this row of the data in and of itself,

700
01:25:03,470 --> 01:25:08,570
this p value is a little bit I mean, it's predicting one of the outcome is exactly right.

701
01:25:08,570 --> 01:25:17,930
So I don't want to really report this P value, but it does allow us to adjust for the number of placebo dry days when we're looking at this over here.

702
01:25:22,090 --> 01:25:28,930
And our code doing all the same stuff are only results from the robust standard.

703
01:25:28,930 --> 01:25:35,950
Errors are easily accessible, so those are the only results that you can get with smaller datasets like this one.

704
01:25:35,950 --> 01:25:39,550
Model based estimates are preferred, but our package doesn't do that particularly well.

705
01:25:40,030 --> 01:25:45,009
There is there are results here that you can get from RC.

706
01:25:45,010 --> 01:25:48,190
These are all the robust sandwich empirical standard errors.

707
01:25:49,960 --> 01:25:58,150
And here is the the smaller model base standard errors that you see smaller than reported by SAS.

708
01:25:58,150 --> 01:26:00,700
So I don't know how trustworthy worthy these are.

709
01:26:01,060 --> 01:26:06,520
So for small data sets, again, that's only had 29 people for smaller data sets, I recommend using SAS.

710
01:26:08,720 --> 01:26:18,140
So the the final thought for this here note that I want to leave you with is that if you have questions about the study design.

711
01:26:18,380 --> 01:26:22,820
If you look at this data and you worry that the significant treatment benefit is

712
01:26:22,820 --> 01:26:27,770
entirely driven by this weird placebo behavior when they were given drug first,

713
01:26:28,370 --> 01:26:33,350
then the proper course of action really is to analyze period one data,

714
01:26:33,890 --> 01:26:43,100
because period one data doesn't have those potential unblinding flaws and sort of compare the

715
01:26:43,100 --> 01:26:47,740
two treatments as if they've been collected in a parallel design with just period one data.

716
01:26:48,380 --> 01:26:54,260
You lose a ton of power that way because you don't get to use any of the additional data in period two.

717
01:26:55,040 --> 01:27:03,139
So this is a this is a desperation of trying to recover some story from your trial based on the part of your trial that,

718
01:27:03,140 --> 01:27:08,720
you know, wasn't contaminated by carryover effects or all these other little issues that can happen.

719
01:27:09,800 --> 01:27:11,330
So it's a very low powered test.

720
01:27:11,330 --> 01:27:19,130
Since you're testing differences between independent groups and not taking advantage of the extra outcomes you get from this paired crossover design.

721
01:27:20,550 --> 01:27:29,220
In the end, your easiest study, if you just look at period one data, there's nothing statistically persuasive about benefit of the drug.

722
01:27:29,220 --> 01:27:33,180
It's very you. I mean the P values 0.73. It's not even remotely close.

723
01:27:33,780 --> 01:27:39,599
Um. And a skeptic might use this follow up analysis to raise questions about the appropriateness

724
01:27:39,600 --> 01:27:44,430
of the cross-over analysis because you get a significant result from the crossover analysis.

725
01:27:47,110 --> 01:27:50,160
But it's all driven by this weird placebo behavior.

726
01:27:50,980 --> 01:27:54,580
When you were given the drug first. All right.

727
01:27:54,580 --> 01:28:00,430
So I'm going to go right ahead and start on the next handout.

728
01:28:00,700 --> 01:28:13,279
And I think. This is going to be the the final handout of how to analyze normally distributed outcomes over time.

729
01:28:13,280 --> 01:28:17,560
And I will have to tell you, I use this. Handout method.

730
01:28:18,950 --> 01:28:24,530
Constantly. I mean, this is one of my go to methods. So this is going to be a perk up hand-out.

731
01:28:24,710 --> 01:28:31,490
It's not just a perk up moment. It's a perk up hand-out because of the usefulness of this method and understanding data.

732
01:28:37,980 --> 01:28:42,700
So the dataset that I'm using is from the AIDS Clinical Trials Group,

733
01:28:42,700 --> 01:28:52,470
a CTG study 193a and this is a randomized clinical trial of 1309 HIV infected patients,

734
01:28:53,010 --> 01:28:59,729
and the study was randomizing to two treatment arms, one a triple therapy treatment,

735
01:28:59,730 --> 01:29:08,100
which I'll I'll name the drugs in a moment, compared to dual therapies with two of the three being used.

736
01:29:09,060 --> 01:29:13,440
And the outcome was something called City for Count.

737
01:29:13,440 --> 01:29:21,719
And the higher they are, the better your your price, your immunity is is handling things, you know, not a doc.

738
01:29:21,720 --> 01:29:25,560
So this is my understanding that's about as good as it gets for city for high as good.

739
01:29:27,160 --> 01:29:32,770
And so the idea is they want to look at these three or four counts over time and see,

740
01:29:32,770 --> 01:29:36,760
you know, one treatment or the other keeps them higher or maintains them.

741
01:29:37,120 --> 01:29:42,160
Right. And what's the difference between the two treatment arms on those trajectories over time?

742
01:29:43,120 --> 01:29:48,579
So in this study, because it was the main outcomes, three, four counts were scheduled to be collected regularly.

743
01:29:48,580 --> 01:29:57,490
So at baseline and then every eight weeks during follow up that and they were because they wanted to see those trajectories over time.

744
01:29:57,700 --> 01:30:00,220
In the two groups, that was their main primary endpoint.

745
01:30:01,890 --> 01:30:09,680
But 34 counts are unbalanced in the sense that they couldn't quite perfectly get everybody in every eight weeks.

746
01:30:09,720 --> 01:30:13,680
Scheduling issues either on the hospital side or the patient side.

747
01:30:13,680 --> 01:30:17,280
It made it difficult since they were sometimes skipped visits.

748
01:30:17,280 --> 01:30:21,150
There were sometimes drop or sometimes that just weren't quite on their eight week mark.

749
01:30:23,210 --> 01:30:29,360
So the number of measurements of city forecasts during the first 40 weeks of follow up varied from 1 to 9 with a median of four.

750
01:30:31,400 --> 01:30:38,570
And I'll show you a little bit of what the data look like, like the dataset itself, just to sort of peek at that.

751
01:30:38,870 --> 01:30:42,530
So the outcome was log three, four, that's the variable name.

752
01:30:42,740 --> 01:30:46,610
It's actually the log, the natural log of the C for count plus one.

753
01:30:46,610 --> 01:30:53,569
And this plus one is just to prevent you from potentially having a log of zero, although I don't think that was really an issue in the data set.

754
01:30:53,570 --> 01:31:00,350
They set that up from the beginning. The outcomes are dependent because they're measured within the same person at various weeks.

755
01:31:00,860 --> 01:31:02,030
See all the outcomes.

756
01:31:02,030 --> 01:31:11,059
If you had a low city for count starting off, you would expect those outcomes to be in the absence of anything else remaining low, right?

757
01:31:11,060 --> 01:31:15,560
So correlated with an individual. If you had a high C for outcome, you would expect them to sort of,

758
01:31:15,560 --> 01:31:20,300
in the absence of anything else going on, stay high right within with measurement variability.

759
01:31:20,780 --> 01:31:24,170
So definitely we're thinking dependent outcomes.

760
01:31:24,890 --> 01:31:29,780
And here are the covariates that we're going to play with the treatment variables called triple.

761
01:31:30,650 --> 01:31:34,190
Here are the drugs that they were had in the triple therapy.

762
01:31:34,700 --> 01:31:44,809
And if Triple A zero meant that they had some version of a dual therapy, two of the three, but not including the Europeans.

763
01:31:44,810 --> 01:31:50,299
So the Zidovudine and the Dyddiad and Z.

764
01:31:50,300 --> 01:31:54,560
Now when you get a text in to pronounce medical terms, you always have a mixed bag of what you get.

765
01:31:54,560 --> 01:32:01,850
So that might or might not be how these are pronounced. But so I'm going to call them triple versus dual, and then I don't have to have that problem.

766
01:32:02,270 --> 01:32:07,370
Age is a covariate measured in years. Male is the covariate one of males, you're a female.

767
01:32:07,370 --> 01:32:10,700
And then the week corresponds to the timing of outcome measures.

768
01:32:10,940 --> 01:32:15,830
This is the only time dependent covariate, but it's going to be the most important cover in in some ways,

769
01:32:16,250 --> 01:32:22,910
because to see anything interesting over time, we have to look at weak and we have to look at interactions with week.

770
01:32:23,600 --> 01:32:30,260
So if we think the trajectory of C four counts is different over time, we're going to need to look at interactions with treatment a week.

771
01:32:31,070 --> 01:32:40,040
This is from now on here on in, you need to be solid on your interactions because everything interesting is with an interaction over time.

772
01:32:40,610 --> 01:32:48,860
All right. Age similarly if you think the trajectory of how your city for changes over time patterns on age you have to look at an

773
01:32:48,860 --> 01:32:55,490
interaction with age in a week if you think that the trajectory of what your C forecast doing over time depends on your gender.

774
01:32:55,880 --> 01:33:00,170
So maybe, you know, men decrease faster than women over time.

775
01:33:00,170 --> 01:33:04,940
In the absence of anything else, you would have to look at an interaction between male and weak.

776
01:33:05,330 --> 01:33:11,780
So are a lot more model building. When you have this longitudinal data, we're only going to be peaking at a little corner of it.

777
01:33:12,320 --> 01:33:18,050
We're not going to look at all possible interactions, but we are going to focus on weekend treatment.

778
01:33:21,390 --> 01:33:26,860
Here's what the data set looks like. And so here is the first person.

779
01:33:26,860 --> 01:33:31,120
The idea is indicating what outcomes are correlated with one another.

780
01:33:31,570 --> 01:33:44,530
Second person, third person, fourth person. And so all of the age and gender kind of and the treatment over there are the same within an individual.

781
01:33:44,530 --> 01:33:53,020
They don't change with the time. So this is the time variable and here's the outcome variable that we're modeling.

782
01:33:53,860 --> 01:33:58,930
And so you can kind of see, you know, person one, they were pretty close to their eight week marks.

783
01:34:00,070 --> 01:34:03,490
Person two did okay. Is well,

784
01:34:03,490 --> 01:34:11,110
there's a little bit of a wiggle here for some of them are not perfectly on the eight week marks person three we only had their baseline measure.

785
01:34:11,230 --> 01:34:15,910
That's all we got, but we're going to use it. Person four They clearly missed some visit.

786
01:34:16,540 --> 01:34:23,110
They didn't quite make it on 24. So there was a little bit of variability and wiggle room here on when these were taken.

787
01:34:23,890 --> 01:34:32,470
If you if they were pretty close to their eight, eight week measures, we could use G to analyze this data.

788
01:34:32,740 --> 01:34:42,960
We would have to massage the dataset a bit because at least person four here, they need to have a placeholder for that missing value to use G.

789
01:34:43,210 --> 01:34:47,290
So the correlation numbers will line up with their right column of week.

790
01:34:48,220 --> 01:34:54,690
The data set. As you see it here, does not have a missing value for week 24,

791
01:34:55,200 --> 01:35:00,900
but you would need that to make sure that they no for person for they didn't have that measure.

792
01:35:01,260 --> 01:35:07,470
If you don't have it, they'll assume this measure was that week 24 when they're doing the correlation matrix.

793
01:35:07,920 --> 01:35:10,470
Even though you have weak here is a covariance.

794
01:35:10,860 --> 01:35:17,070
The correlation matrix is just using the ordering of the numbers within the person to figure out what column of the correlation they're doing.

795
01:35:18,880 --> 01:35:27,820
So one nice thing about the random effects model is that it doesn't really care exactly when the measures are happening.

796
01:35:27,820 --> 01:35:35,379
They're thinking of each person as a trajectory. And you can I'm trying to like hold the microphone and point to my arm.

797
01:35:35,380 --> 01:35:40,240
Let's try this. I think I can do this. This is a little I'm not a physically talented person.

798
01:35:41,320 --> 01:35:44,500
So you can measure the same person as many times as you want.

799
01:35:44,500 --> 01:35:48,910
If you're thinking about their pattern as being a trajectory and they don't care when it happened.

800
01:35:49,600 --> 01:35:54,549
Whereas in G you have to have everybody measured at the same time so that you can group numbers in a

801
01:35:54,550 --> 01:36:00,730
column and figure out what the correlation between measures that zero and measures that we get are.

802
01:36:02,140 --> 01:36:08,830
But mixed models don't care. They're thinking of the trajectory as a line and they're going to talk about variability of the line either

803
01:36:08,830 --> 01:36:14,410
being higher or lower than the population average or tilting differently than the two population average.

804
01:36:14,830 --> 01:36:22,240
But so it gives you a little bit more flexibility and and forgiveness for missing measures or taking extra measures and stuff like that.

805
01:36:26,300 --> 01:36:29,620
So I just want to take a quick look at the data.

806
01:36:29,630 --> 01:36:38,240
It's always a good idea to plot your data. And so we've been looking at plots called spaghetti plots to look at these trends.

807
01:36:38,510 --> 01:36:46,489
This is a huge study, so we're a little bit limited in what we can look at with this many lines for all these different people.

808
01:36:46,490 --> 01:36:57,290
So this is our first path of plotting the trajectories over time in these people, we have the outcome.

809
01:36:57,620 --> 01:37:01,880
The outcome is a log three, four, and the predictor is weak group equals ID.

810
01:37:02,150 --> 01:37:06,170
That's how we're getting the lines. And I'm not going to spend too much time on the code.

811
01:37:06,170 --> 01:37:12,739
I just want to show you the plot and it you know, I've had a student tell me before, this just looks like a furball.

812
01:37:12,740 --> 01:37:16,910
You know, you can't really see what's going on in the middle at all.

813
01:37:17,540 --> 01:37:22,849
This is red line is the lowest line that's trying to predict the average trajectory,

814
01:37:22,850 --> 01:37:27,140
kind of nonpayment metrically by moving the window and averaging through a window.

815
01:37:28,310 --> 01:37:30,709
And you might see a little bit of curvature here,

816
01:37:30,710 --> 01:37:36,830
but it's very hard to see what's going on with the individual trajectories over time when you have a data set that large.

817
01:37:37,730 --> 01:37:44,090
So I tried to make it a little bit better by looking at one panel for the triple therapy,

818
01:37:44,090 --> 01:37:49,670
one panel for the dual therapy, so at least less people in each of the panels.

819
01:37:50,270 --> 01:37:52,759
And that ends up looking kind of like this.

820
01:37:52,760 --> 01:38:02,210
And it's still a furball, but you're starting to be able to see at least I can see some dots of light between the inside the furball a little bit,

821
01:38:02,600 --> 01:38:04,670
but the low line is still helpful.

822
01:38:04,760 --> 01:38:10,610
This is we need to kind of model over time what we think is happening in treatment group one and treatment group two.

823
01:38:10,610 --> 01:38:14,090
Right. This is the dual therapy. This is the triple therapy.

824
01:38:14,510 --> 01:38:23,089
And the lowest line suggests that there's a little bit of a lift, you know, in the four counts early on when they're on treatment.

825
01:38:23,090 --> 01:38:32,150
Maybe it's really subtle. So but we might want to look at, you guessed it, a spline term that allows the line to bend.

826
01:38:33,110 --> 01:38:38,600
So some slight curvature and it might be worth testing for linear blind term, near 16 weeks or some other function of time.

827
01:38:39,920 --> 01:38:45,770
It would also be worth looking at Lydia's plans every eight weeks, kind of mimicking when the plan protocol was supposed to be measured.

828
01:38:45,770 --> 01:38:50,660
But in the interest of time, I'm just going to show you the 16 to explain to kind of show you how it goes.

829
01:38:50,990 --> 01:38:54,020
You're going to be playing with this. Did that more during your homework.

830
01:38:54,020 --> 01:38:58,670
I'm going to have this data set. You're going to analyze it a bit more during your last homework.

831
01:38:58,940 --> 01:39:01,580
So you're going to be very familiar with this data set by the end.

832
01:39:04,290 --> 01:39:11,819
This is our code for the same kind of spaghetti plots our has this you know nice kind of I don't know it looks kind of messy,

833
01:39:11,820 --> 01:39:18,420
but it has different colors for the lines. And so you maybe can see a little bit better than the furball from South.

834
01:39:18,420 --> 01:39:23,280
But the the smoothing algorithm is similar, not exactly the same.

835
01:39:23,280 --> 01:39:30,419
This is the overall plot. And then when you look at by treatment groups, there's again, there's maybe in the triple therapy,

836
01:39:30,420 --> 01:39:34,979
there's maybe some bend or a quadratic or some kind of shape here to model them.

837
01:39:34,980 --> 01:39:41,640
And in this handout, we're going to just be modeling it as a line that bends around 16 weeks.

838
01:39:45,400 --> 01:39:50,770
So the covariance matrix approach versus the random effects approach,

839
01:39:50,770 --> 01:39:55,259
I've kind of already said this out loud, but I want to make sure it's in your notes.

840
01:39:55,260 --> 01:40:02,740
So the covariance matrix approach works well when the data are successfully measured at fixed time points,

841
01:40:03,070 --> 01:40:10,300
baseline, eight weeks, 16 weeks, etc., so that we can adequacy model within subject correlation.

842
01:40:10,310 --> 01:40:18,070
So we need a number of columns to be corresponding to baseline and a different number column of numbers that correspond to eight week values.

843
01:40:18,130 --> 01:40:21,580
Somehow we have to make that decision of what numbers go there for our outcome,

844
01:40:22,090 --> 01:40:25,900
and that's the program is figuring out correlation between those columns.

845
01:40:26,440 --> 01:40:34,580
If we can't define columns, we can't use the the covariance matrix approach to analyzing this data with a G.

846
01:40:36,960 --> 01:40:42,030
So many of these patients had all measures on schedule, but a substantial number were off by several weeks.

847
01:40:42,030 --> 01:40:48,660
And it makes it hard to decide, you know, do they go with the measure that was the four weeks earlier or the four weeks later?

848
01:40:49,170 --> 01:40:52,500
Classifying the numbers becomes a little uncertain,

849
01:40:53,160 --> 01:40:56,309
so it's more convenient to model the variability in a way that allows these

850
01:40:56,310 --> 01:41:00,990
irregularities and timing that's more forgiving when schedules don't match.

851
01:41:00,990 --> 01:41:06,000
The protocol does at times. Exactly. So that's where the random effects model helps us.

852
01:41:07,690 --> 01:41:17,319
And so I think in this last 10 minutes or so, I'm going to be able to give you a good start and get you thinking about random effects models.

853
01:41:17,320 --> 01:41:22,150
And so there's two types of regression models that are included in the mixed effects model.

854
01:41:23,920 --> 01:41:29,380
The one that's going to seem the most familiar is what we call the population mien model.

855
01:41:29,950 --> 01:41:35,110
This is like the line or spline connected line that we're trying to fit through the furball.

856
01:41:36,180 --> 01:41:40,110
All right. And we're going to interpret that and have p values about that in our paper.

857
01:41:41,040 --> 01:41:49,019
So this is what is the average trend or trajectory that's in the overall population for different coverage profiles?

858
01:41:49,020 --> 01:41:53,460
All of our main script where these sentences are going to be related to this population mean model.

859
01:41:55,160 --> 01:42:00,150
So it's the usual model you're used to thinking with standard linear regression parameters and covariates.

860
01:42:00,530 --> 01:42:06,349
I've I'm kind of submerging all the substrate scripts for individual and measure

861
01:42:06,350 --> 01:42:10,190
within individual here so that you don't get bogged too much down with notation.

862
01:42:10,970 --> 01:42:15,950
I think I can get the idea across by like not having all these objects everywhere.

863
01:42:17,960 --> 01:42:24,620
So the average outcome is going to look something like what you would have seen with linear regression.

864
01:42:24,620 --> 01:42:29,810
And the output is going to look similar to what you would have seen in linear regression.

865
01:42:31,190 --> 01:42:36,079
One, one or more of these variables might be weak in interactions with weak.

866
01:42:36,080 --> 01:42:40,219
So the complexity is going to come in that you have to be able to interpret

867
01:42:40,220 --> 01:42:44,930
and set up contrast statements involving these interactions with with time.

868
01:42:46,030 --> 01:42:52,990
So you've exercised that muscle and this is why this is the marathon people.

869
01:42:53,770 --> 01:42:59,380
You know, you're going to be dealing with interactions over time, including spleen terms and all kinds of stuff.

870
01:42:59,890 --> 01:43:05,320
So this is this is the Olympics. This is you've been working and practicing making your way towards this.

871
01:43:05,590 --> 01:43:08,590
This is the time people need to pull it all out.

872
01:43:10,120 --> 01:43:14,740
All right, I'll stop. That's. I'm a mom. I tell mom type jokes, you have to forgive me.

873
01:43:14,920 --> 01:43:18,050
But, you know, my teenagers have tried to discourage me. It's.

874
01:43:18,070 --> 01:43:24,740
But it's impossible. All right. So covariates are allowed to change for different dependent outcome measures.

875
01:43:24,890 --> 01:43:33,600
In our example, the only one that changes over time is a week. So this part of the model includes what are called fixed effects.

876
01:43:33,600 --> 01:43:42,080
And so people call it the fixed effects part of the model. And that's absolutely jargon that is going to need to be familiar.

877
01:43:42,110 --> 01:43:45,470
The fixed affects part of the model. That's about the population mean.

878
01:43:47,660 --> 01:43:56,510
Or the population. I mean, trajectory. Right. Random effects are all about the variability that you see individual to individual.

879
01:43:57,500 --> 01:44:05,930
And so there is an individual level, I mean, model, what the average trend or trajectory is for person I.

880
01:44:06,470 --> 01:44:08,300
And so this is something that's new.

881
01:44:08,310 --> 01:44:17,070
So this is a perk up moment because this is the part that you need to kind of have your brain start working on over the weekend, you know.

882
01:44:18,050 --> 01:44:20,720
So in addition to the population me model,

883
01:44:20,720 --> 01:44:28,730
they're terms that just affect person I enter describing how person I differs from the population mean trajectory.

884
01:44:29,820 --> 01:44:39,180
So we've seen the random intercept term. In our earlier handout, I think the very first hand out, we had an example of, you know,

885
01:44:39,270 --> 01:44:47,610
a random intercept that made the height of the values that are paired t testing those lines start higher or lower.

886
01:44:47,610 --> 01:44:53,550
We kind of just barely scratched the surface there, but it can be much more complex.

887
01:44:54,180 --> 01:45:02,459
So the general way you write the model of a person is going to be the population mean part where I'm using all betas that look familiar and happy.

888
01:45:02,460 --> 01:45:08,220
You know that model plus the model that is individual,

889
01:45:08,220 --> 01:45:15,630
the person I and so I have put subscripts here for person I because these are different for every person in your dataset.

890
01:45:17,150 --> 01:45:22,700
They're their own values and they're basically describing, you know, the intercept is like,

891
01:45:22,700 --> 01:45:26,210
what's the height of the trajectory and how is it changing for the person?

892
01:45:26,630 --> 01:45:35,280
But you could have a cover for a week here, and that would be sort of like, is there a changing trajectory that changes with week?

893
01:45:35,300 --> 01:45:38,240
So not only different heights but different weeks.

894
01:45:38,690 --> 01:45:49,190
We could even think about putting in spline terms like I want to move like an Egyptian, you know, or my, my bend can be different by person as well.

895
01:45:49,910 --> 01:45:57,649
Okay, so this can be quite complicated. We're just going to barely play with this and we're going to start from the baby model first.

896
01:45:57,650 --> 01:46:07,070
So you get an intuition for how these models work. So these kind of gamma parameters are typically different for every person.

897
01:46:07,070 --> 01:46:11,150
They try to capture that person's individual trend or trajectory.

898
01:46:12,590 --> 01:46:16,400
In relationship to what the overall population trajectory is.

899
01:46:16,820 --> 01:46:22,060
So it's usually some kind of thing that's shifting from the population trajectory.

900
01:46:22,070 --> 01:46:35,000
What's going on with these people? And the covariates in the random effects part of the model were have the cover it's a called disease

901
01:46:35,000 --> 01:46:42,139
here are often but not always functions of time or interactions with time so that can be used.

902
01:46:42,140 --> 01:46:50,540
You could have the same variables show up in both the population model and in the random effects model, and that's usually it's time related stuff.

903
01:46:50,540 --> 01:46:54,399
But it can. Different data sets, different situations.

904
01:46:54,400 --> 01:47:04,230
There could be other things in here as well. And the this new thing is that these gamma terms,

905
01:47:04,380 --> 01:47:12,330
these random effects all have their own assumed distribution to them, that they're assumed to have a multivariate.

906
01:47:12,450 --> 01:47:17,759
So think these like if you had two of these, think of that bell shape that we drew earlier.

907
01:47:17,760 --> 01:47:25,590
You know, they have a distribution of of their own that's multivariate means zero.

908
01:47:26,130 --> 01:47:33,060
And it's describing the variability in the data. So the most common case is to have a random intercept and a random slope for each

909
01:47:33,060 --> 01:47:37,980
person so their heights can be different and they can have a different overall slope.

910
01:47:38,400 --> 01:47:43,380
And so the distribution of an individual's height and slope,

911
01:47:43,620 --> 01:47:50,459
those two numbers have their own kind of bell shaped curve distribution that's assumed for them,

912
01:47:50,460 --> 01:47:55,980
like the three D barrels that we showed earlier on in this material.

913
01:47:58,340 --> 01:48:04,880
So those parts of the model called the random effects. And the whole point is to get the variability of the data accurately described.

914
01:48:05,830 --> 01:48:09,520
And you can kind of get a sense by looking at maybe not the furball,

915
01:48:09,760 --> 01:48:16,660
but generally if you can plot the data and look at trajectories, you can get a little bit of a sense of what might be needed.

916
01:48:18,380 --> 01:48:25,460
Fortunately for this model, you do have some model building tools that are going to be available like I see.

917
01:48:25,790 --> 01:48:29,120
So in the random effects model, when we're assuming normally distributed outcomes,

918
01:48:29,360 --> 01:48:33,710
we can lean on things like AIC to help us with our modeling choices.

919
01:48:35,080 --> 01:48:38,700
So. I think.

920
01:48:42,070 --> 01:48:46,420
I wish I had five more minutes to give you something to chew on with your brain over the weekend.

921
01:48:46,900 --> 01:48:57,520
But all I can do is stop here and say, we're going to tackle this Monday and we'll finish it up on Monday.

922
01:48:58,150 --> 01:49:04,810
Next week's lab is also about these models. And like I said, I'm going to post the homework six over the weekend.

923
01:49:05,140 --> 01:49:10,030
You won't be able to start it yet, but next week, as you go through lab,

924
01:49:10,030 --> 01:49:16,510
I think it'll be helpful to know what the homework questions look like because it's going to that lab may help you with one of the problems.

925
01:49:18,400 --> 01:49:25,030
All right. Good luck, everybody, in getting the rest of the homework for this class tackled.

926
01:49:25,420 --> 01:49:30,220
There is a lot of work left in the very short time. It seems like we're almost over.

927
01:49:31,090 --> 01:49:35,649
But there's two long homeworks left for the course that you have to finish in this remaining time.

928
01:49:35,650 --> 01:49:39,340
So I know you just turned one in. It hurts.

929
01:49:39,340 --> 01:49:45,129
It hurts, but you just gotta go. This is you're going to be using these models for the rest of your careers.

930
01:49:45,130 --> 01:49:50,440
And now is the constant time when you have someone around helping you learn them.

931
01:49:50,440 --> 01:49:54,350
So let's use that time. Well. I everybody.

