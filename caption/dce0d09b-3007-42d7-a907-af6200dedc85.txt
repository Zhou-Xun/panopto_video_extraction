1
00:00:00,060 --> 00:00:03,420
Setting. Okay, we're going to use. Okay.

2
00:00:04,080 --> 00:00:09,129
And please get started. Hi, everyone.

3
00:00:09,130 --> 00:00:12,910
We were group seven. So our final project is about you.

4
00:00:12,910 --> 00:00:21,280
Do you implementation of the mix principle component analysis in thank you in our studio.

5
00:00:21,610 --> 00:00:34,240
So this technique is so this technique is applied to neuro analysis uh, especially for some neuro has a mixture of motto and sensory function.

6
00:00:34,540 --> 00:00:45,100
So say is introduced into like captured the dependency of neuro about parameters such as stimulus time or decision.

7
00:00:45,610 --> 00:00:53,560
So, uh, and also you can like preserve the principle component of the from the original data.

8
00:00:55,670 --> 00:01:00,970
So let's do a simple illustrate illustration for this idea.

9
00:01:02,960 --> 00:01:13,520
We can see that on the figure A and B, we're using A using a linear discriminant analysis method to separate this free stimulus.

10
00:01:14,210 --> 00:01:22,580
And however, it doesn't for like preserve the geometry of this stimulus because they're equally spaced from each other.

11
00:01:23,720 --> 00:01:34,570
However, in the conventional principle, component analysis from the figures C and D, we can see that this really stimulus are conserved the geometry.

12
00:01:34,640 --> 00:01:38,060
However, the time dependency are not like.

13
00:01:39,440 --> 00:01:52,220
Separated. So that's why we introduced the deep saying that this you could both favor both go by like separating the decoder and decoder access.

14
00:01:52,730 --> 00:02:03,300
So you need to figure out each. So from the figure eight each, we can see that we become we compressing the data using a linear math.

15
00:02:04,340 --> 00:02:09,170
And we're like d decompressing. It's using the neighbor to near my age.

16
00:02:09,780 --> 00:02:17,980
Uh, I've never met the. So on the right hand side is, uh, lost function for PSA.

17
00:02:17,990 --> 00:02:32,000
And it's a method. Um, we can see that ABS is our data matrix and F is our encoder Colter matrix and B is our decoder matrix.

18
00:02:32,480 --> 00:02:41,240
So in the L in the DC method where the reconstructing target is the matrix of the stimulus average,

19
00:02:41,480 --> 00:02:52,100
which has the same size of the matrix X and is replaced, that every data point in the matrix l x with the stimulus averaged.

20
00:02:54,100 --> 00:03:02,720
Yeah. Then when you to implement that, you'd be able to implement it.

21
00:03:02,840 --> 00:03:07,520
We first need to be compulsive, furious, factoring data into a set of calories.

22
00:03:08,090 --> 00:03:14,270
Here we can see that X bar is simply the already found range of our neural activity.

23
00:03:14,270 --> 00:03:20,930
Bar is the average time bar we found to rate once the overall mean has been subtracted, etc.

24
00:03:21,540 --> 00:03:24,029
For the purpose of the meeting, you wrote signals.

25
00:03:24,030 --> 00:03:33,870
In the context of our data sets, we can bind some terms together and we can record rates of our almost by summation of the last four years,

26
00:03:34,470 --> 00:03:38,700
which is the function and so of law. Right next please.

27
00:03:39,450 --> 00:03:48,330
And here is the translation of marginal organization we can only fancy over data can be the composing issue averaging fairly rate

28
00:03:48,720 --> 00:03:58,740
condition independent average stimulus independent average decision independent average and stimulus distribution in reaction average.

29
00:03:59,250 --> 00:04:02,520
This fits with decomposition from on the firm of light.

30
00:04:04,820 --> 00:04:08,000
And our main purpose of our inmates.

31
00:04:08,000 --> 00:04:13,610
PC first door is to find the decoder and in order to capture our first Q Co-winners.

32
00:04:13,790 --> 00:04:24,589
According to our marginalized lost function, we first flatten our one dimensional array into two dimensional map trees to make it more

33
00:04:24,590 --> 00:04:31,610
easier for us to manipulate and after flattened in our metro has become not implantable.

34
00:04:31,940 --> 00:04:42,019
So we introduced two packages to calculate your generalizing numbers and we also found that is to have are much faster than simply using CVD

35
00:04:42,020 --> 00:04:52,670
function or randomize to really function and to a captured of his companies and also satisfy that our decoder requires to be orthogonal metrics.

36
00:04:52,910 --> 00:04:57,950
We try to do composition as we do and randomize as we the composition.

37
00:04:58,220 --> 00:05:08,720
And at this time we find that the randomized activity of happy is much faster when our AQ is much smaller than the total number of our variable.

38
00:05:09,230 --> 00:05:19,400
And to avoid overfitting we also introduce the reader randomization so we simulated data to test our function.

39
00:05:19,790 --> 00:05:25,910
So over time and stimulus as test parameter and each data point are drawn from a Gaussian distribution

40
00:05:26,180 --> 00:05:32,780
and we generate in sample trials for total numbers of X and to calculate the average among all of X.

41
00:05:33,140 --> 00:05:44,120
So for this case, the dimension should be a T by s by in any order in your lead data, we're adding a decision as a technical parameter as well.

42
00:05:44,120 --> 00:05:47,180
So the final data side, well we have four dimension of.

43
00:05:47,210 --> 00:05:51,050
Yeah, yes. And this is our newest outcome.

44
00:05:51,260 --> 00:05:57,080
We can see in this poor showing that all our data is projected out on our first decision coming in.

45
00:05:57,440 --> 00:06:05,629
And here we have three different colors to represent, three different stimulus and two different life names of the caller,

46
00:06:05,630 --> 00:06:10,430
the darker and the lighter one to represent your two different decision.

47
00:06:11,030 --> 00:06:18,409
Yes. And we will further try to find a way to optimize the regular order of my higher dimensional

48
00:06:18,410 --> 00:06:24,730
areas of experimental data and write our final report and upload our package to IHOP.

49
00:06:25,160 --> 00:06:32,910
That's all. Thank you. Oh, so we well, within 7 minutes.

50
00:06:32,910 --> 00:06:38,970
So we have a time for one or two questions. We're putting two questions in one with the questions.

51
00:06:42,870 --> 00:06:51,660
Okay. So you said that the randomizer that we need was faster, so it didn't affect the accuracy or what?

52
00:06:51,690 --> 00:06:55,080
What what do you think the trade off between those two are going to mean?

53
00:06:55,230 --> 00:06:59,770
Can you describe a little bit more than what the random items that we need are?

54
00:07:02,030 --> 00:07:12,860
The randomizer activity is to continue iteration to find the approximation of their single deposit.

55
00:07:13,250 --> 00:07:17,870
The. So if we said we only need like a few companies.

56
00:07:18,140 --> 00:07:27,590
So there are, the iteration will be much smaller then if we try to like using the whole size

57
00:07:27,590 --> 00:07:34,120
of the data to calculate a like general inverse like the SBT function do.

58
00:07:34,820 --> 00:07:39,140
Yeah. Okay. Any any other questions?

59
00:07:42,570 --> 00:07:46,050
Okay. Very good Samaritan. I don't care at all.

60
00:07:47,660 --> 00:07:52,560
The opening quarter, I question whether you're familiar with auto insurance.

61
00:07:53,130 --> 00:07:59,310
Other automakers are one quarter auto or is an individual an employer?

62
00:07:59,520 --> 00:08:04,080
All right. I question that. Oh, we didn't use all the encoder.

63
00:08:04,110 --> 00:08:12,240
We. I know. I was curious about the comparison with auto comparisons that the similar idea I mean, this like encoding decoding.

64
00:08:12,740 --> 00:08:16,350
Yeah. But if you're not familiar with them in the backwards.

65
00:08:17,010 --> 00:08:22,379
Oh sorry. We didn't use the auto encoder in like machine learning area.

66
00:08:22,380 --> 00:08:29,660
We just use the RANDOMIZE or SVT function to to find the organism actually to decode and included it.

67
00:08:30,150 --> 00:08:35,660
I think it's a it's interesting comment. I think it is a comment that there is a connection there.

68
00:08:35,670 --> 00:08:39,690
So try to submit research for it and if you want it, including in your binary.

69
00:08:40,140 --> 00:08:44,070
Okay. Thank you. And I think we can go on to the next group.

70
00:08:46,080 --> 00:08:49,770
So next group is next quarter.

71
00:08:49,770 --> 00:08:55,040
We could have four. Uh.

72
00:08:55,460 --> 00:09:10,220
Okay. Hi, everyone.

73
00:09:10,460 --> 00:09:14,410
I'm Annie. This is Brady and John Faye.

74
00:09:15,310 --> 00:09:22,010
We'll be presenting on the cure algorithm, which is a clustering algorithm using representative points.

75
00:09:24,710 --> 00:09:33,560
The motivation for care clustering is to deal with non-uniform cluster sizes and non hyper spherical shapes.

76
00:09:33,560 --> 00:09:40,390
So on the left here you can see these clusters are pretty non-uniform sizes.

77
00:09:40,640 --> 00:09:46,610
And then on the right you see clusters that are not traditional spherical shapes.

78
00:09:49,350 --> 00:09:54,659
So clustering algo algorithms like K means do not always capture these types of

79
00:09:54,660 --> 00:10:00,450
clusters and the cure algorithm can perform better than algorithms like K means,

80
00:10:00,450 --> 00:10:09,660
which are centroid based because this algorithm is robust to outliers and can identify non spherical clusters.

81
00:10:10,350 --> 00:10:19,980
However, there's currently no implementation in R for cure, only in C++ and Python through high clustering.

82
00:10:21,420 --> 00:10:31,290
So our goal is to implement cure in our here's a general workflow of it's a really simplified version of the algorithm.

83
00:10:32,580 --> 00:10:43,530
The input there's four inputs. The first is and two dimensional points to be clustered so the actual algorithm can take up to K dimensions.

84
00:10:43,530 --> 00:10:50,460
But for simplicity we focus on two dimensions and we'll talk about that later in the further work section.

85
00:10:51,540 --> 00:11:01,259
The second input is the number of clusters and then the maximum number of representative points and the shrinkage parameter,

86
00:11:01,260 --> 00:11:04,980
which is the amount that the representative points are shrunk.

87
00:11:05,160 --> 00:11:09,510
And then the output is the data with the corresponding cluster IDs.

88
00:11:12,220 --> 00:11:17,250
And so some of the intuition for cure suffered classic hierarchical clustering methods.

89
00:11:17,260 --> 00:11:22,089
These include things such as centroid based methods which merge clusters you centroid the

90
00:11:22,090 --> 00:11:26,740
closest or all points approaches that merge clusters that have the closest pair points.

91
00:11:26,830 --> 00:11:32,380
So looking out over all the points that are in the clusters and some of the issues with these methods for centroid based,

92
00:11:32,740 --> 00:11:36,549
the centroid is assumed representative of the cluster and so it doesn't do so well when there

93
00:11:36,550 --> 00:11:41,590
is non spherical data and for all points approaches we assume all points to be representative.

94
00:11:41,590 --> 00:11:47,739
And so it's quite sensitive to outliers. So Kira tries to find some middle ground here and uses SI well,

95
00:11:47,740 --> 00:11:53,950
scattered points to be representative and then we shrink these towards the centroid to account for outliers.

96
00:11:53,950 --> 00:11:58,269
And I'll describe what well scattered is in a second, but as an outline for the algorithm,

97
00:11:58,270 --> 00:12:03,909
we start with each input point being a single cluster and then at each step merging the closest pair of clusters.

98
00:12:03,910 --> 00:12:08,319
And so this is the distance function that we use to compute the distance between two clusters.

99
00:12:08,320 --> 00:12:11,530
So we have a cluster, you and B we look over its representative points,

100
00:12:11,680 --> 00:12:17,620
calculate the Euclidean distance between each pair and then take the minimum of those to merge the clusters.

101
00:12:17,620 --> 00:12:22,120
We just take the union of the points in the merge clusters, fancy new well scattered points,

102
00:12:22,120 --> 00:12:25,150
shrink them towards the centroid and then get the new representative points.

103
00:12:26,160 --> 00:12:31,469
To the find the well scattered points we first find the first point to be the farthest from the centroid and in

104
00:12:31,470 --> 00:12:36,350
Italy we find the case well scattered point to be the point that's farthest from the other K minus one points.

105
00:12:37,140 --> 00:12:43,799
We then shrink the well scatter points towards the centroid. So we have a cluster W and a centroid W by then for a well scatter point P,

106
00:12:43,800 --> 00:12:48,540
this is how we would get the representative point and of course we stop when the number of clusters is equal to K.

107
00:12:50,280 --> 00:12:57,710
To compare it to a well-known clustering of the room. We compared it to gaming, so it's similar to cure in that we're determining the closest cluster.

108
00:12:57,720 --> 00:13:02,190
But this and we're using based on the distance from the centroid, we're still merging the closest clusters,

109
00:13:02,190 --> 00:13:07,590
recalculating the cluster centroid and repeating these steps until there's no change from one iteration to the next.

110
00:13:08,220 --> 00:13:14,490
So this tends to produce equal sized clusters and Kira is K means is actually a special case of cure when our shrink is parameter set to one.

111
00:13:17,620 --> 00:13:22,390
So these are the three pictures so far, you know, clusters.

112
00:13:22,810 --> 00:13:30,220
So on the left, too, you can see they are now uniformly sized and spherical and all the rightmost.

113
00:13:30,820 --> 00:13:35,740
You have three clusters which are uniformly size and convex.

114
00:13:37,450 --> 00:13:48,760
So this is the results from our accurate clustering, which is five represented points and the shrinkage parameter to be 0.75.

115
00:13:49,030 --> 00:13:53,380
And we will compare it with results of K means and we will come back to this one.

116
00:13:55,450 --> 00:14:01,689
So the results from K means, like you can see from the leftmost picture,

117
00:14:01,690 --> 00:14:14,260
like it was really bad at identifying the bottom huge circle and on the second picture it was bad at identifying the other half of the bottom spiral.

118
00:14:14,590 --> 00:14:20,260
But as for the common sense and uniform case, it didn't perfect.

119
00:14:22,860 --> 00:14:33,950
And as you can see, like The Cure did reasonably well on the first two and it also didn't perfect on the third one.

120
00:14:33,960 --> 00:14:38,040
So while this contrast with Canning's is also expected.

121
00:14:41,310 --> 00:14:48,150
So one thing we could introduce in our future work is heat data structures, especially minimum heat.

122
00:14:48,360 --> 00:14:53,099
So which is the pictures on the bottom left? So, I mean, if you please,

123
00:14:53,100 --> 00:15:00,179
I actually like the other structures which stores the minimum value on the note and I'm not the property is the

124
00:15:00,180 --> 00:15:11,370
key of the parent of is always less of and the key of is children notes and so like finding minimum keys or one

125
00:15:11,370 --> 00:15:21,239
complexity and that is easy because you only need to pop out on the blue note to get the minimum key and inserting

126
00:15:21,240 --> 00:15:28,580
on key by the pair is all up in complexity because you only need to go through the other half of the children.

127
00:15:28,590 --> 00:15:32,730
Those are to insert the pair instead of going through every one of them.

128
00:15:38,210 --> 00:15:43,580
So then the rest of the future work and our next steps include running a more

129
00:15:43,580 --> 00:15:48,590
comprehensive sensitivity analysis where we change the shrinkage value alpha,

130
00:15:49,040 --> 00:15:55,639
which is the shrinkage towards the represented or towards the of the representative points towards the centroid and then

131
00:15:55,640 --> 00:16:02,000
also changing around the number of representative points within each cluster and seeing how this changes the accuracy.

132
00:16:02,720 --> 00:16:13,070
And then also applying our algorithm to new data sets and expanding the function to support key dimensions instead of just two,

133
00:16:14,540 --> 00:16:22,670
which is pretty easy for us to implement, but might cost more in terms of time efficiency.

134
00:16:22,880 --> 00:16:31,370
So some of the ideas we have for improving time efficiency are using heaps for storage and extraction of clusters and their nearest cluster.

135
00:16:32,060 --> 00:16:39,440
Also finding the representative points of merged clusters from old representative points instead of from all of the points,

136
00:16:39,920 --> 00:16:43,880
and then computing distances in parallel to reduce looping.

137
00:16:46,200 --> 00:16:49,960
These are our references and thank you for listening.

138
00:16:49,980 --> 00:17:01,560
Does anyone have any questions? What if I was going to ask you about how you should see this interview?

139
00:17:02,650 --> 00:17:07,080
So I have another question. So can you go back to your work? Do something like that?

140
00:17:08,420 --> 00:17:13,080
Yeah. So intuitively, why would you cure that?

141
00:17:20,040 --> 00:17:25,640
Part is, if you're using Euclidean distance, I'm like you.

142
00:17:29,650 --> 00:17:34,800
Yeah, there is. Well, at least for Caymans, you're using the centroid.

143
00:17:34,840 --> 00:17:42,520
So you're always, like, looking at, uh, some kind of sphere for this case because you're working and picking the, well, scattered points.

144
00:17:42,670 --> 00:17:47,709
Um, so you're able to, um, you know,

145
00:17:47,710 --> 00:17:54,220
kind of maybe if some of the well scattered points to be up here like where the on the left part of this parabola.

146
00:17:54,820 --> 00:18:00,240
And then you kind of shrink them in so you can almost like think of getting like different spheres to come around.

147
00:18:00,730 --> 00:18:05,530
So you still calculate Euclidean distance, which is spherical, but you have these points that are all over the place.

148
00:18:06,310 --> 00:18:14,560
And so it's almost like, I think having like tiny spheres that are kind of approximating the shape of the clusters in terms of just the one.

149
00:18:14,560 --> 00:18:15,460
So you just have the ones.

150
00:18:16,000 --> 00:18:23,710
So for coming up with, I think we now have the one in that this is a I have a specific comment for your group, so I'll give you that later.

151
00:18:23,810 --> 00:18:31,820
Perfect. Yeah. Next group.

152
00:18:32,790 --> 00:18:35,810
Uh, 11.

153
00:18:40,250 --> 00:18:49,110
So. But why not do this full screen?

154
00:18:53,740 --> 00:18:58,090
Got it. Hi, everyone. We are going 11 today.

155
00:18:58,330 --> 00:19:06,910
We are going to introduce a new develop algorithm named Bipartite ties, virtual clustering, which is used to identify clustering to a species.

156
00:19:08,410 --> 00:19:13,630
So first of all, we will give a very general overview of the this algorithm and the methodology.

157
00:19:14,230 --> 00:19:23,680
So the basic functionality of this algorithm is used to identify the cluster based on the also salary information, a gene expression data.

158
00:19:24,400 --> 00:19:30,850
The main reason that we are choosing this algorithm because it outperforms the previously similar algorithm by three neighbors.

159
00:19:32,050 --> 00:19:39,720
First of all, it addresses a problem where some genes have individual individualized functions because it allows for in cluster,

160
00:19:39,750 --> 00:19:42,560
either in cluster genes. And secondly,

161
00:19:42,580 --> 00:19:51,430
it is able to consider all the genes in two species which address the problems that some genes does not have a psychologist in any other species.

162
00:19:51,850 --> 00:19:59,330
And certainly it does not rely on any distribution and distribution assumptions, which makes a very robust it.

163
00:20:00,850 --> 00:20:09,340
So we are going to emit we have three input and the first two refers to the the covariance matrix of the two species

164
00:20:09,730 --> 00:20:17,709
and for an area for the it has to call one that refers to the name of the genes and has the other domain names,

165
00:20:17,710 --> 00:20:22,720
refers to the name of genes and the caller names refers to the name of the features.

166
00:20:22,990 --> 00:20:27,970
And then there's also another a matrix matrix, which is a binary value matrix.

167
00:20:28,480 --> 00:20:37,600
We can use one to represent the edge between the nose being a two species, and then our expert output is a type, no clusters.

168
00:20:38,980 --> 00:20:43,540
So we can see from this figure it is mainly to demonstrate how it works.

169
00:20:43,930 --> 00:20:52,210
And also we will have some tuning parameters we are using in SUBSAMPLE and also to address issue we probably will encounter.

170
00:20:53,560 --> 00:20:57,810
So our main step is first we want to we will build up training.

171
00:20:57,820 --> 00:21:06,510
HANSMAN Where we will address a problem where for that input you have a relatively sparse structure and disconnect now.

172
00:21:07,000 --> 00:21:16,659
So we were just calculated in quality and distance for the nodes and then generate you symmetric for endometriosis and then we will use this to

173
00:21:16,660 --> 00:21:27,430
metro metro's is to address a problem where for the matrix a we will re weigh the weights by incorporating pairwise similarity on these nodes.

174
00:21:31,450 --> 00:21:39,370
And yeah, our second step is to apply the bipartisan spectral clustering to the sample data in a single iteration,

175
00:21:39,910 --> 00:21:49,390
and we'll build up a block matrix W with P and B transpose on the off diagonal position and compute the normalized succession of W and denoted as L.

176
00:21:49,690 --> 00:21:58,989
And then we'll start the iterations. And first we'll subsample M to two and into the data from Species one and species two.

177
00:21:58,990 --> 00:22:12,550
And we will match, match it and match it with the L matrix and find a L tutor by matching the aging ID and we'll, uh,

178
00:22:12,820 --> 00:22:22,840
and then we will based on the L to two and calculate the icon for the largest K0 eigenvalues and find the corresponding eigenvectors.

179
00:22:22,840 --> 00:22:31,360
And after standardization as well, uh, apply k means to the eigenvectors and find the K0 clusters.

180
00:22:33,130 --> 00:22:39,130
And then we will. And then since we have entered it and into the sample data,

181
00:22:39,130 --> 00:22:46,540
there's still an minus into two and a minus into and sample data from species one and switches to.

182
00:22:46,540 --> 00:22:52,179
We can zero to also arrange these data into the clusters.

183
00:22:52,180 --> 00:22:59,830
So first we can calculate the mean covariance vector obtained from the simple nose and the we assign the

184
00:23:00,010 --> 00:23:06,430
sample notes until the clusters who have the smallest Euclidean distance to them mean covariance vectors.

185
00:23:06,760 --> 00:23:11,590
And then we will combine the two clusters of sample and simple those.

186
00:23:14,160 --> 00:23:18,070
And then we will construct a commemorative matrix.

187
00:23:18,690 --> 00:23:26,729
And and it is a binary matrix and it equals one of the genes or in the same cluster and equal zero otherwise.

188
00:23:26,730 --> 00:23:36,510
And after the H iterations we will obtain the average A and matrix and denote it as a consensus matrix.

189
00:23:36,510 --> 00:23:45,780
And then finally we will apply the hierarchical clustering to y minus and bar with complete linkage and the threshold y minus alpha.

190
00:23:46,050 --> 00:23:54,540
And it could indicate that, uh, the pairwise distance between the notes are no greater than one minus over in each, uh, cluster.

191
00:23:56,340 --> 00:24:06,899
And next is a cluster, the application. So we use the real data, uh, given by the original paper and x1 and x2 are the uh, all right.

192
00:24:06,900 --> 00:24:14,760
Ah ah, they are expression levels with their, uh, with uh, multiple covariance as shown in the plots.

193
00:24:14,760 --> 00:24:24,300
And uh, do you want, uh, g one is from the flight, uh, flight species and Jean two is from the worm species.

194
00:24:24,630 --> 00:24:30,330
And also the adjacency matrix is given to show the connections of the two things.

195
00:24:31,230 --> 00:24:35,190
And so after, after a point,

196
00:24:35,430 --> 00:24:42,870
but we also may have this output or as a consensus matrix member and it is the average matrix

197
00:24:42,870 --> 00:24:49,350
formed H ROMs and it represents for the frequencies of the connection between things.

198
00:24:50,100 --> 00:25:01,650
And we have shown the distribution of M using this CDF graph on the right and we also visualized our M bars using the datagram visualization.

199
00:25:01,950 --> 00:25:11,610
So me, I'm using a woman's RFID codes 2.8 on misrepresenting that on the jeans are connected or at least 80% of the time.

200
00:25:11,850 --> 00:25:19,560
And this is just a on sub cluster of the overall class and we can see that on

201
00:25:19,860 --> 00:25:26,540
the doing on in the red box represent are from the VI species and the gene.

202
00:25:26,550 --> 00:25:31,910
The blue dogs are from the worm species. On.

203
00:25:32,050 --> 00:25:35,170
And besides, are these most mental details.

204
00:25:35,170 --> 00:25:47,260
We also optimized our rhythm a little bit. So first we optimize it by avoiding to repair the calculation by on the first or by calculator x one,

205
00:25:47,260 --> 00:25:53,140
x two and a matrix as well as in on the pleasure matrix l before us during any sub simply.

206
00:25:53,590 --> 00:26:00,760
And we also replace on the matrix medication based on all instead of using the loop om.

207
00:26:01,030 --> 00:26:09,820
We also overcome the runtime bottlenecks by applying a faster and more efficient way to find down values and eigenvectors.

208
00:26:10,480 --> 00:26:19,840
So after applying all these optimization steps, we have improved our run time from 3 hours to 448 seconds.

209
00:26:20,440 --> 00:26:28,460
In our preliminary case based row equals 2.5 and number of integration close to ten on that bit,

210
00:26:28,570 --> 00:26:34,750
beating the larger case of rollercoaster point eight and number of iterations in close to 100.

211
00:26:35,050 --> 00:26:38,650
The wrong time is still a little bit too long. It's about 10 hours.

212
00:26:40,180 --> 00:26:45,200
So in the future we are trying to improve and improve our time efficiency.

213
00:26:46,030 --> 00:26:53,650
We have currently identified the bottlenecks as the distance matrix and the and the construction of the consensus matrix.

214
00:26:54,160 --> 00:27:02,440
We were trying to solve this problems by applying, by reconstructing in the our CVP or using parallel rooms.

215
00:27:03,160 --> 00:27:14,470
We also we, we also tend to develop an algorithm to choose the best case zero, which is the number of the cluster by using the research or AUC.

216
00:27:15,220 --> 00:27:22,570
And finally, we will compare Neocam here. Our result is the absence of resolving puzzle and create it in our package.

217
00:27:23,580 --> 00:27:27,820
Okay, so we have 30 seconds. Anyone want to ask questions?

218
00:27:29,530 --> 00:27:34,150
Okay. So, yeah, I think so.

219
00:27:34,840 --> 00:27:41,440
That's great. And I think, you know, so I wanted to just pause a little bit then at some point.

220
00:27:41,440 --> 00:27:49,690
So if you can join the for the people who presented today, if you're joining the office now, I'll give a specific feedback.

221
00:27:50,290 --> 00:27:59,739
And if you haven't seen yet, there is a, uh, if you go to canvas there is a link to, to evaluate here, so here,

222
00:27:59,740 --> 00:28:07,899
so you can use this evaluation form to provide the feedback for each of the group which will be used as a reference for the grading,

223
00:28:07,900 --> 00:28:12,639
not really for actually use for the grading itself at grading.

224
00:28:12,640 --> 00:28:16,090
So okay. So thank you. Okay, let's move on.

225
00:28:20,780 --> 00:28:27,260
Okay. So. So 12 is all right.

226
00:28:27,630 --> 00:28:37,640
Okay. All right.

227
00:28:38,070 --> 00:28:44,120
Hello, everybody. We are a group called and today we'll be presenting our final project on line detection of labeled image data with the hope,

228
00:28:44,130 --> 00:28:50,070
transformation and novel argumentation. So we'll start with our problem statement of feature extraction.

229
00:28:50,550 --> 00:28:55,550
And the purpose of the feature extraction is essentially to derive informative features from our datasets.

230
00:28:56,160 --> 00:29:01,260
And an example from image analysis is wanting to identify informative features from

231
00:29:01,590 --> 00:29:07,590
MRI images to detect features about tumors in order to increase tumor diagnostics.

232
00:29:07,980 --> 00:29:15,000
These features can include things like tumor intensity, texture and shape. For an example from DNA sequence analysis,

233
00:29:15,000 --> 00:29:19,590
we might want to identify Kramers of like six in order to identify different

234
00:29:19,590 --> 00:29:24,870
motifs in DNA that are associated with certain features such as protein binding.

235
00:29:26,190 --> 00:29:32,010
So the problem statement for our project is basically can we extract features from sets of data points?

236
00:29:33,150 --> 00:29:40,770
And one question you might ask is if you have a set of data points, why not just use linear regression to identify a pattern among these data points?

237
00:29:41,190 --> 00:29:47,730
And essentially what we linear regression will return you a single line attempting to explain the pattern among your points.

238
00:29:48,000 --> 00:29:54,239
But we're attempting to identify multiple underlying factors among our points to insert a single line explaining a pattern.

239
00:29:54,240 --> 00:30:00,930
There might be a couple of different factors that are influencing what our dataset looks like.

240
00:30:02,760 --> 00:30:04,950
So now some background on the health transformation.

241
00:30:06,660 --> 00:30:14,130
The transform is used in processing of noisy digital images in order to perform shape detection, such as our example on the left,

242
00:30:14,400 --> 00:30:19,020
where we take a noisy image and we've detected straight lines going through this image,

243
00:30:19,890 --> 00:30:24,930
we notice that there's currently no implementation of a hub transform that takes lots of points as input.

244
00:30:24,930 --> 00:30:29,400
They all generally take images rather than data points.

245
00:30:31,020 --> 00:30:39,990
So our approach to solving this problem of detecting lines in sets of points using the hub transform.

246
00:30:40,290 --> 00:30:46,260
This is a general overview of the Transform procedure, which we'll go into in a little bit through toy example.

247
00:30:46,920 --> 00:30:53,130
But essentially what you want to do is obtain your data points, project those points into the parameter space,

248
00:30:53,910 --> 00:30:59,610
run those points through a half accumulator to identify sort of lines in the half

249
00:30:59,610 --> 00:31:04,110
space and then extract those lines and project them back onto your original data set.

250
00:31:05,700 --> 00:31:12,599
So for a short toy example, we have the set of points here and you might be able to pick out a couple of

251
00:31:12,600 --> 00:31:17,850
different potential lines such as this line here and maybe this line here.

252
00:31:19,140 --> 00:31:26,100
So just for example, we'll focus on this point and for this point and for each point in our dataset,

253
00:31:26,430 --> 00:31:31,050
we're going to identify all the possible lines that could be going through this point.

254
00:31:32,010 --> 00:31:35,790
There are an infinite number of possible lines that could be going through this point.

255
00:31:35,790 --> 00:31:42,810
So we'll restrict it to maybe 180 lines to represent a multitude of possible slopes going through this point.

256
00:31:44,100 --> 00:31:51,900
So we'll choose, say, this line going through this point to continue in our example and for each line going through this point out,

257
00:31:51,950 --> 00:31:57,359
we'll take the line that hits that line orthogonal eight and we'll take row,

258
00:31:57,360 --> 00:32:03,030
which is the length of that line and theta, which is the angle that that line forms the x axis,

259
00:32:03,030 --> 00:32:07,410
essentially converting this line into polar coordinates.

260
00:32:08,520 --> 00:32:11,760
And so for each line that we've found going through this point,

261
00:32:12,030 --> 00:32:19,800
we'll end up with a set of 180 row theta pairs representing these lines in polar coordinates.

262
00:32:20,190 --> 00:32:26,309
And one really nice feature of this is that this also allows us to represent the vertical line going through this point,

263
00:32:26,310 --> 00:32:31,740
which you wouldn't be able to do in a Y equals of X format, because that would be a undefined line.

264
00:32:33,570 --> 00:32:41,010
And next, we're going to take all of these row theta pairs and project them into the half space.

265
00:32:41,670 --> 00:32:47,159
And when we do this, we essentially end up with a sinusoidal curve just like this.

266
00:32:47,160 --> 00:32:55,469
So we've taken all of our row theta pairs for a single point and we've projected those

267
00:32:55,470 --> 00:32:59,490
pairs into the half space and we end up with a curve that looks something like this.

268
00:33:00,330 --> 00:33:06,630
And we'll do that for every point in our dataset, and we'll end up with four sinusoidal curves that look like that.

269
00:33:08,250 --> 00:33:15,270
You can see that we have these intersections here and these basically represent what will be our line features in our data.

270
00:33:16,110 --> 00:33:20,759
And so we're trying to identify where these intersections are and to identify these intersections,

271
00:33:20,760 --> 00:33:30,840
we basically split our space into a lot of equally sized bins, and we're going to count the number of points that are in each of these bins.

272
00:33:31,140 --> 00:33:35,460
The underlying logic being the bend with the most points will be the bean.

273
00:33:35,830 --> 00:33:42,340
With the intersection, for example, we're counting the number of lines intersect, the bend, but more complicated in the actual code.

274
00:33:42,640 --> 00:33:45,880
So for that bean, for example, there's one line intersecting it.

275
00:33:45,890 --> 00:33:53,590
So I'll get a one. That bean has two lines going through it, so it'll get a value of two, and we'll do that for all of our beans.

276
00:33:53,920 --> 00:33:57,700
And once we do that, we can identify the beans with the most intersections.

277
00:33:57,880 --> 00:34:01,060
There's a few beans in this example that have a value of two.

278
00:34:01,750 --> 00:34:06,219
In a real example, we'd be using them number of points in each bean, not the number of lines.

279
00:34:06,220 --> 00:34:11,350
So the value of the counts in the beans would be a lot larger and a lot more spread out.

280
00:34:11,560 --> 00:34:21,070
But for this example, this like three and that two are our largest values also happen to coincide with our intersection beans.

281
00:34:22,240 --> 00:34:26,320
And so once we have these once we've identified our intersections,

282
00:34:26,560 --> 00:34:32,950
we can take the center of those intersections and convert them from the half space back into Cartesian coordinates

283
00:34:33,400 --> 00:34:40,900
and end up with these lines which happened to represent these line features going through our points.

284
00:34:41,860 --> 00:34:49,210
So now we can go through sort of what this looks like in a simulation. For our results, we generated these points.

285
00:34:49,990 --> 00:34:56,530
We can see there might be some line features in there. We project these points into hub space.

286
00:34:56,530 --> 00:34:59,620
We can see those sinusoidal curves that we were talking about earlier.

287
00:35:00,760 --> 00:35:10,620
We use an accumulator to identify, you know, our intersections within the half space using our accumulator function we project.

288
00:35:11,440 --> 00:35:20,499
And so yeah, we accumulate where we think these intersections might be and then we can project the lines from those intersection

289
00:35:20,500 --> 00:35:27,700
points back into our original space and we can see that it has created these line features which seem to line up with,

290
00:35:27,850 --> 00:35:31,090
you know, the lines in our original data.

291
00:35:32,920 --> 00:35:39,280
Some challenges that we face in this project. We had a little trouble implementing our accumulator setup efficient, efficiently,

292
00:35:39,280 --> 00:35:42,900
but we were able to get over this sort of bottleneck using a restaurant.

293
00:35:43,960 --> 00:35:49,690
Yeah. So a lot of the implementations that we saw the half transform online used a triple for loop.

294
00:35:49,690 --> 00:35:57,159
So our way of turning our account into a restaurant image kind of follows that kind of implementation of the origins of efficiency.

295
00:35:57,160 --> 00:36:06,740
So that was one of our first hurdles. Yeah. See the big hurdle in implementing the accumulator, but we managed to overcome that.

296
00:36:06,980 --> 00:36:10,639
There was brief confusion about when to represent things in polar versus Cartesian.

297
00:36:10,640 --> 00:36:18,510
Coordinates also got through that. Our results are it's not perfect.

298
00:36:18,620 --> 00:36:21,170
It's also kind of zoomed in precision.

299
00:36:21,320 --> 00:36:26,180
But you can see that our intercepts are a little off, even though we do tend to identify the slopes pretty well.

300
00:36:26,540 --> 00:36:34,430
Exactly. And so we'd like to we would try and work on this more before implementing it as our package for everyone else to use.

301
00:36:35,420 --> 00:36:39,020
And we'll now take questions. Thank you. Good question.

302
00:36:40,610 --> 00:36:48,740
It's a great idea. So I think it was a really good time for how we are looking at things.

303
00:36:48,890 --> 00:36:51,980
Do you 87? Or is it more like.

304
00:36:52,970 --> 00:36:56,720
Yeah. So that's the initial assumption. Something else.

305
00:36:56,960 --> 00:37:01,840
I'm just oh, just out of curiosity is a possibility to talk about like moving forward stuff.

306
00:37:01,880 --> 00:37:07,250
Is it possible to use other sources for hours?

307
00:37:07,820 --> 00:37:10,910
It's just lines but other like implementation. Yeah.

308
00:37:11,210 --> 00:37:15,530
Half as half transform is designed to detect like lines, curves, shapes.

309
00:37:16,220 --> 00:37:21,050
But it's been developed over time, since its initial inception to identify other shapes like circles.

310
00:37:21,320 --> 00:37:27,670
Yeah, exactly. I think we were prepared for one more question.

311
00:37:31,410 --> 00:37:36,150
Okay. So, uh, so I don't understand.

312
00:37:36,180 --> 00:37:39,950
What's the point of use if you extend this line?

313
00:37:39,960 --> 00:37:52,770
What what's the what's the next step? How how does it work? All systems, users and US citizens.

314
00:37:53,260 --> 00:38:02,670
So we will try to. Implement these to see the infrastructures and connect them with you.

315
00:38:02,870 --> 00:38:07,280
Do something. Do something to see if it may be helpful.

316
00:38:07,700 --> 00:38:10,760
Okay. So I probably would need to follow up with you, but thank you.

317
00:38:10,810 --> 00:38:18,230
Okay, let's move on. The next group is group two.

318
00:38:18,470 --> 00:38:31,580
Thank you. Big you, but.

319
00:38:35,570 --> 00:38:44,270
Hi, everyone. So we're grouped you are Marley our that's Llewyn and our project is recorded is for elastic not regularized generalized linear models.

320
00:38:45,200 --> 00:38:49,400
So this is the general outline of our presentation and I'll get us sort of a some background and vote.

321
00:38:50,420 --> 00:38:56,209
So what exactly are we trying to implement? So we are trying to implement algorithms that were developed in a paper from

322
00:38:56,210 --> 00:39:01,490
Stanford about globes essentially using elastic net penalty and quartet dissent.

323
00:39:01,820 --> 00:39:06,410
So this paper served as the basis for the delimit function and package in art.

324
00:39:06,830 --> 00:39:13,010
So our goal is to implement these algorithms and make our own function that's competitive with that in art.

325
00:39:13,580 --> 00:39:20,090
So why are we using globes? Well, you know, I'm essentially just like an extension of ordinary linear regression.

326
00:39:20,480 --> 00:39:23,510
So we can use multiple different models within the same function.

327
00:39:23,510 --> 00:39:28,700
So we'll be focusing on linear regression, logistic regression and multi logistic regression.

328
00:39:29,240 --> 00:39:31,070
So that's what we're focusing on for our function.

329
00:39:31,430 --> 00:39:37,760
Um, and our ultimate goal is for this function to be competitive with film that in terms of precision and timing.

330
00:39:38,330 --> 00:39:42,380
So that's our ultimate goal. Some background elastic net is kind of a review.

331
00:39:42,770 --> 00:39:53,690
So elastic that is kind of a mixture between L1 L2 penalties using Alpha as kind of the the value that tells us how mixed they are going to be.

332
00:39:53,690 --> 00:39:58,940
So in all, 5.5 would give us a pretty equal mix between an L one and two penalty.

333
00:39:59,540 --> 00:40:03,410
So that's used in our algorithm as well as of the Sun.

334
00:40:03,620 --> 00:40:07,699
So quite a design is like an optimization method, right?

335
00:40:07,700 --> 00:40:14,800
So for corner design, instead of using all of our elements through every iteration,

336
00:40:14,810 --> 00:40:21,110
it cycle actually uses one at a time to each cycle to find optimization of our objective function.

337
00:40:21,110 --> 00:40:26,570
So the minimization of that function, and this is much simpler and faster than other optimization methods,

338
00:40:26,900 --> 00:40:30,050
which is why we're using it for our algorithm today.

339
00:40:30,140 --> 00:40:41,350
So. Is this because coordinator sound is already accomplished?

340
00:40:42,120 --> 00:40:46,550
It is hard for us to improve the performance by adjusting the algorithm.

341
00:40:47,070 --> 00:40:51,060
However, in the realization part, we find there is still something we can change.

342
00:40:51,900 --> 00:40:55,700
Can you put that question and either reference parts?

343
00:40:55,700 --> 00:41:04,680
The step of updating elements involving calculating current residuals, which contains the matrix and vector multiplication x times theta.

344
00:41:05,280 --> 00:41:11,400
However, by looking to the residuals, we can find that the update of it only involves one column, the X,

345
00:41:11,940 --> 00:41:18,300
and this simplifies the calculation from intensity to peak, which helps a lot in improving the speed.

346
00:41:19,560 --> 00:41:26,910
Either coding part we use our to implement all the functions, including the data standardization and the algorithm.

347
00:41:30,240 --> 00:41:30,809
For now,

348
00:41:30,810 --> 00:41:41,930
we have done some experiments on the simulated data we generate and the numbers of s paper and noise and use the first equation to calculate Y.

349
00:41:41,950 --> 00:41:45,050
Then we use x and Y as equals to estimate data.

350
00:41:45,870 --> 00:41:51,589
We will evaluate the performance based on precision and calculate timing for the precision.

351
00:41:51,590 --> 00:41:59,399
When we use the absolute relative metric and relative deviance as the metric, and we set number equals to zero.

352
00:41:59,400 --> 00:42:04,860
So there will be no penalty on data and the close operator had to beat her the better it is.

353
00:42:05,430 --> 00:42:14,190
The experiment results shows that our implementation has more elements closer to the true, greater and maximum of the absolute reality.

354
00:42:14,430 --> 00:42:24,509
Deviance is smaller than that of Joan. And here we also compare the calculating time with Joan that found.

355
00:42:24,510 --> 00:42:30,780
Here is the summary table of the different sizes data set in milliseconds.

356
00:42:31,230 --> 00:42:36,420
We can find that while the while our function is slower than Joan,

357
00:42:36,420 --> 00:42:42,360
that when P is greater than for the case that is greater than p l a function, it's faster.

358
00:42:42,840 --> 00:42:48,749
We think the reason might be that Joan adjusts the algorithm for the first cases because

359
00:42:48,750 --> 00:42:55,620
we can find that for the data set with the same number of elements like the third row.

360
00:42:55,650 --> 00:42:58,350
Our function, like the timing is similar about Joan.

361
00:42:58,350 --> 00:43:08,520
That itself is faster, but for in the future we will try to improve the performance for the cases and for the logistic regression.

362
00:43:08,520 --> 00:43:15,870
We just use the similar methods to implement and we the necessary to do mentioning the paper and we also tested we simulated data,

363
00:43:15,870 --> 00:43:23,429
but we didn't show the result here because the function didn't give a best performance now and the output of our functions do have some barriers,

364
00:43:23,430 --> 00:43:33,300
but we think it is acceptable. However, the run time is too long, so we will improve the efficiency efficiency of this regression function.

365
00:43:33,660 --> 00:43:37,830
And actually we have already found some possible ways to improve this path speed

366
00:43:38,370 --> 00:43:43,259
from some details of the paper of the jump nets and is here and for the first ones

367
00:43:43,260 --> 00:43:48,809
above the nested do nets to as I mentioned earlier and for the next we look at the

368
00:43:48,810 --> 00:43:53,040
jump that usually is because the just inputs a list of lambda for different values.

369
00:43:53,640 --> 00:44:02,670
So for each that we have to you know ratner loop to get outputs but for our function we only need one lambda y2.

370
00:44:02,670 --> 00:44:09,600
So maybe we have a white nested loop to just make our function because of the red button and for another

371
00:44:09,600 --> 00:44:17,040
way to avoid the coefficients that we're doing to the order to a few of the properties of the Y,

372
00:44:17,040 --> 00:44:21,840
which means if the, if we get the probability near well there all we can do actually set the wrong one.

373
00:44:22,440 --> 00:44:31,409
And the last one maybe could be important point we think is that about the unity of the beta vectors actually not our function.

374
00:44:31,410 --> 00:44:34,170
If just initialize the beta vectors as all the zeros,

375
00:44:34,170 --> 00:44:41,940
but in the paper we can see that the used a closed form expression as a starting solution and we think this this make the error coordinate

376
00:44:42,730 --> 00:44:52,410
with the ref buzzer and we plan to use these methods our function and we hope we can just improve our function and make them read faster.

377
00:44:52,620 --> 00:44:55,919
And that's part of our future. Yeah.

378
00:44:55,920 --> 00:44:57,659
So we have some future work in mind,

379
00:44:57,660 --> 00:45:03,450
so we have some concrete plans for our final project once it's submitted and some ideas if we have extra time to work on.

380
00:45:03,780 --> 00:45:07,350
So the first is we just talked about is to improve our logistic regression implementation

381
00:45:07,950 --> 00:45:12,059
and the second is to complete the implementation of our multi regression aspect.

382
00:45:12,060 --> 00:45:15,510
So we have successfully implemented logistic and linear regression.

383
00:45:15,510 --> 00:45:19,139
And multi normal regression is kind of just an extension of logistic regression, right?

384
00:45:19,140 --> 00:45:23,070
This has more classes and just two, but because of that,

385
00:45:23,070 --> 00:45:27,690
the algorithm is a bit more complicated and thus the implementation has been a little more complicated as well.

386
00:45:28,380 --> 00:45:34,230
So we hope to have that completed by the time we finish our project as well as we hope to do experiments on real data.

387
00:45:34,230 --> 00:45:38,740
So in the paper. Referencing, they looked at their performance on simulated data and real data.

388
00:45:39,070 --> 00:45:42,760
So we've already done that with simulated data, and we hope to do that on real data as well.

389
00:45:43,390 --> 00:45:47,260
And if we have time, we could implement a weighted update for our functions,

390
00:45:47,260 --> 00:45:53,080
because right now we're using unweighted algorithms, but the weighted algorithms also exist so we can implement those as well.

391
00:45:53,710 --> 00:45:58,810
And additionally, we are currently looking at normal distributions in terms of like testing our functions and everything.

392
00:45:59,170 --> 00:46:03,980
So we could also implement other distributions like quiz on for example, to look at.

393
00:46:04,000 --> 00:46:07,430
So yeah, so that's our project. Thank you for your time.

394
00:46:07,490 --> 00:46:11,740
Does anyone have any questions in queue? Questions?

395
00:46:13,480 --> 00:46:23,650
I'm sorry. I'm definitely one of those people who are like, jump in and you're like, do you have any of your ideas?

396
00:46:23,650 --> 00:46:28,900
And because of that, it's kind of like it's like you have.

397
00:46:32,400 --> 00:46:37,340
Don't you go back that. Yeah.

398
00:46:37,540 --> 00:46:45,510
Oh, so now we don't know the reason, but because, like, um, for the 36 year old, they have the same number of elements.

399
00:46:46,020 --> 00:46:50,940
But German as itself is faster than the cases of earnings greater than P.

400
00:46:51,420 --> 00:46:56,520
So these things are going to be only starting because in the original algorithm in that paper,

401
00:46:57,000 --> 00:47:02,910
it's evaluating convergence only one only uh, at the end of each cycle.

402
00:47:03,540 --> 00:47:08,880
That's for because one P is greater than the interoperability, much longer.

403
00:47:09,390 --> 00:47:15,030
So there might be a. The question.

404
00:47:16,580 --> 00:47:24,520
But if there's an episode of intuition I recorded DEC performed faster than my guy reading the update.

405
00:47:25,010 --> 00:47:31,460
I just because it seems like reading is when you get an update on your parameters simultaneously, where's that cycle through?

406
00:47:32,150 --> 00:47:38,210
Um, because like for gradient descent, this function, uh, this question doesn't have a close for.

407
00:47:38,660 --> 00:47:42,830
However, like for instance, it's only optimized for each element.

408
00:47:43,310 --> 00:47:51,090
So it's like for each element in each cycle it's completely off to the offset instead of like taking a step towards optimum.

409
00:47:51,140 --> 00:47:55,690
It just calculates the optimization. Thank you.

410
00:47:56,710 --> 00:48:01,390
Thank you. Okay. Two in a.

411
00:48:04,310 --> 00:48:08,790
We we have three more groups to go.

412
00:48:09,300 --> 00:48:13,350
Just to make sure that you you put it a little closer.

413
00:48:13,680 --> 00:48:20,400
Otherwise, it's hard to hear. So, uh, and group three is next.

414
00:48:21,340 --> 00:48:28,120
Uh, you know, please. Come on. I'll go first.

415
00:48:32,530 --> 00:48:36,280
Oh. Oh, sorry. There you go. Hello.

416
00:48:36,280 --> 00:48:42,370
We're group three. We are in the section and our project is on implementing a Bayesian particle filter.

417
00:48:42,370 --> 00:48:46,569
And ah, so we start with what exactly is a particle filter.

418
00:48:46,570 --> 00:48:49,740
So particle filters are also called sequential Monte Carlo methods.

419
00:48:50,230 --> 00:48:54,670
This originated from different problems, especially in signal processing.

420
00:48:54,880 --> 00:49:01,900
So a really classic example would be tracking the movement of an object through time or tracking the change of a signal.

421
00:49:02,170 --> 00:49:06,340
So here is the very naive like diagram of what it looks like.

422
00:49:06,340 --> 00:49:11,290
We have a bunch of axes, the light travels through time, so we have x two minus one, x, t, blah, blah.

423
00:49:11,710 --> 00:49:16,450
These are the true signals or the measurements, but they're actually never observed in real life.

424
00:49:16,450 --> 00:49:21,400
Instead, we have on the bottom the wise. They're also tracked through time, but they are very noisy.

425
00:49:22,480 --> 00:49:28,450
The very important two parameters in our model is on the top side we have the transition matrix.

426
00:49:28,450 --> 00:49:32,829
The probability of observing s t you give an x, t minus one.

427
00:49:32,830 --> 00:49:40,030
So this is a very classic idea in Monte Carlo methods. And additionally we have this probability of white t given x t,

428
00:49:40,090 --> 00:49:45,010
so the connection between the X's and wise, and this is what we call the observation probability.

429
00:49:45,400 --> 00:49:50,500
So the whole purpose of having a particle filter like this is to deduce our observed.

430
00:49:50,500 --> 00:49:56,170
So our y signals this could give us an estimate, a better estimate on the true signal value.

431
00:49:56,350 --> 00:50:00,940
And also with this better estimate, we can perform a more accurate trend analysis.

432
00:50:02,340 --> 00:50:04,320
So how does this actually work?

433
00:50:05,040 --> 00:50:12,360
As a particle filter, we simulate a bunch of particles that we have in particles too up to represent the hidden state x t.

434
00:50:13,290 --> 00:50:17,190
And the first step is to incorporate the idea of the transition matrix.

435
00:50:17,460 --> 00:50:20,970
So we sample the next state, as you can see on the on the second row, on the right side,

436
00:50:20,970 --> 00:50:26,550
we have a bunch of samples for the next state for each particle according to the transition matrix.

437
00:50:27,000 --> 00:50:33,510
And then we incorporate the evidence which we basically weigh the samples using this observation probability.

438
00:50:33,660 --> 00:50:40,990
So you can see all the third row that the two samples with a bigger observation probably is going to get a bigger weight.

439
00:50:41,490 --> 00:50:49,590
Lastly, we re sample the the next state using these particles and then we rinse and repeat so that we get each state following.

440
00:50:51,390 --> 00:50:59,250
So this is what the algorithm roughly looks like in pseudocode. One thing I would like to highlight is that the number of MN in our input,

441
00:50:59,250 --> 00:51:04,200
basically representing the number of fruit and the number of particles we're using in this filter.

442
00:51:04,650 --> 00:51:08,790
The more we have, we would get a better estimate. So this is on the right side.

443
00:51:08,790 --> 00:51:12,540
This is a MATLAB interim implementation of this particle filter.

444
00:51:12,780 --> 00:51:21,090
As you can see with more particles, we have a closer and more accurate and also smoother estimation of our actual position of the particle.

445
00:51:24,800 --> 00:51:33,080
So to perform the political theater, we created labor and then the bass piece which were done delivered by this command.

446
00:51:34,550 --> 00:51:42,250
So our base pdf library contains three functions, which are some data particle turbine conference.

447
00:51:42,260 --> 00:51:49,160
I predict this in Iraq with a simulated set of similar data that includes the high

448
00:51:49,440 --> 00:51:55,370
of X and the observation why we could be passed to all our political theater.

449
00:51:55,880 --> 00:52:04,850
And the political theater function uses a symbol of how to go for turn to the lowest observation Y based on X and of a particle.

450
00:52:04,850 --> 00:52:11,660
Further predict and predict that the observation Y based on the time of analysis.

451
00:52:12,260 --> 00:52:21,320
So more of health and so simply examples can be found out the I needs of our library.

452
00:52:21,770 --> 00:52:30,590
So that's we were able to solve simple example to prove basically the functionality and the performance of all our functions.

453
00:52:33,080 --> 00:52:39,500
So here are the simple example of particle theater results.

454
00:52:39,860 --> 00:52:52,010
So very first we run the light from randomly generated saw data of 5000 Y and then we pass this data to our political affairs and function.

455
00:52:52,430 --> 00:52:56,180
And then finally the we got the results.

456
00:52:56,660 --> 00:53:11,420
So the blue line here is the original data and the right line here is the unites the results after our political filter of the here is the evaluation.

457
00:53:11,960 --> 00:53:21,290
So there is a political filter function in our in the library pump and then the P filter and

458
00:53:21,290 --> 00:53:27,320
the way you compare the our political affiliation to the P filter function in public library.

459
00:53:27,980 --> 00:53:39,920
And we can see here the blue line is the original data, the red lines, the results from our public affairs and the product lines.

460
00:53:39,920 --> 00:53:51,680
The results from P filter we can see both right and the black line in the results and they are overlapped are the most of time.

461
00:53:52,460 --> 00:54:03,140
And next we compare the running time performance between the political filter and filter from forever.

462
00:54:03,590 --> 00:54:11,390
And then we can see our function political filter have shorter running time,

463
00:54:11,990 --> 00:54:28,940
which means it's more efficient when project equals to 1000 or you want people on the here is the simple example of particle filter predict function.

464
00:54:29,570 --> 00:54:42,110
So on the left the go shows the results of prediction without the political field and the real figure is the our results.

465
00:54:42,110 --> 00:54:52,400
After we use in the political future, we can see the prediction after the political theater usually have better precision,

466
00:54:52,670 --> 00:55:00,410
more precision, more precise our confidence interval and the due to a lower virus.

467
00:55:04,140 --> 00:55:14,730
In conclusion. So we created a package of particle physics with basic functionality of particle filter based on API's distribution,

468
00:55:16,320 --> 00:55:23,430
and we compared the power of our function to the field, for example, ivory.

469
00:55:24,030 --> 00:55:30,090
And we have a similar accuracy. And I've interviewed about her performance on speed.

470
00:55:30,900 --> 00:55:40,740
And in the future, if we have time, we will try to implement a higher dimension particle theater.

471
00:55:41,730 --> 00:55:47,850
You are and there is no package of the international particle field here in New York.

472
00:55:49,170 --> 00:55:53,850
Here is the figure shows how how do you measure traffic?

473
00:55:54,740 --> 00:56:09,570
How how do you measure particle filters data looks like so each x is corresponding to a y and the x are relevant to our next and the.

474
00:56:09,790 --> 00:56:13,860
So here are the references. Thank you. Thank you.

475
00:56:15,080 --> 00:56:20,729
So so the the work goes one one member in a team.

476
00:56:20,730 --> 00:56:23,950
That system has a special circumstance. It couldn't compete.

477
00:56:24,040 --> 00:56:29,220
It's not that the person wasn't participating so yeah.

478
00:56:29,550 --> 00:56:38,540
Just and if any questions. So it just came in that read more of what what how your answer is going.

479
00:56:39,830 --> 00:56:47,210
We don't see any signs of life conditions like.

480
00:56:51,740 --> 00:57:00,040
So this is a very so kind of going back to our idea of like Time series, this is actually very applicable to like a lot of the time series.

481
00:57:00,050 --> 00:57:03,890
You know, actually something we ran into originally we're thinking about using a microarray.

482
00:57:03,890 --> 00:57:08,540
So that would be something similar to this high dimensional idea, except that there is no current implementation.

483
00:57:08,540 --> 00:57:11,300
And we figured out it was extremely difficult to do the implementation.

484
00:57:11,570 --> 00:57:17,450
So it's something we're still working on, but anything that is tracked through time with like few samples,

485
00:57:17,450 --> 00:57:22,790
that's something that so very wide application. Something else about why would work better than pump.

486
00:57:22,790 --> 00:57:27,590
Pump is well developed ish time series package.

487
00:57:27,830 --> 00:57:33,920
So they import and implement a lot more features, which is why they probably work slower than ours.

488
00:57:33,920 --> 00:57:38,149
They also take in very specific type of dataframe into their approval function,

489
00:57:38,150 --> 00:57:43,610
and ours is a more fundamental, very naive implementation like you guys are.

490
00:57:45,390 --> 00:57:48,620
I don't know. Thank you.

491
00:57:48,930 --> 00:57:58,980
Thank you. So next is group 13.

492
00:58:20,670 --> 00:58:25,280
So hello everyone. We are going to talk about our packages single album,

493
00:58:25,740 --> 00:58:34,440
which is a single year DNA sequencing data simulations with varying level of room and hour and I think, oh, contamination.

494
00:58:34,440 --> 00:58:37,710
I'm Rosita. I'm Andre. And I'm Mike.

495
00:58:40,350 --> 00:58:45,370
So this is our agenda for today and some background for sure.

496
00:58:45,420 --> 00:58:52,379
First of all, what is all single nuclei? DNA sequencing is a method to isolate nuclei instead of the whole.

497
00:58:52,380 --> 00:58:58,260
So for gene expression, profiling, however, is much more difficult than it sounds like.

498
00:58:58,770 --> 00:59:08,399
So then why are we doing this? Is because we can achieve comparable gene detection to single cell DNA or RNA sequencing exposure

499
00:59:08,400 --> 00:59:14,220
in cases where isolating nuclear is more feasible compared to isolating the whole cells.

500
00:59:15,060 --> 00:59:21,600
This happened in frozen biopsy for tissue structure that do not offer readily encapsulate encapsulated

501
00:59:22,380 --> 00:59:30,300
single cell structures such as mole can utilize some skeleton muscle but a few other size.

502
00:59:30,300 --> 00:59:41,790
And also and Olivia's cells. So we will accomplish it by the technique called droplet microfluidic tissue technology.

503
00:59:41,820 --> 00:59:49,680
First of all, we will obtain the tissue and then we will pass the droplet through lipid to encapsulate them.

504
00:59:50,100 --> 00:59:58,770
And then we can identify the cell type and state by clustering and the challenges we have here is the single nuclei.

505
00:59:58,770 --> 01:00:06,560
RNA sequencing contamination will happen during the nucleus encapsulation in the droplet.

506
01:00:06,570 --> 01:00:11,340
So in a very ideal case, a job that will have one peer nuclei in it.

507
01:00:12,030 --> 01:00:23,910
But a lot of how is contaminated with ambient RNA cans and have cell structure to contain RNA and nuclei and also organelles.

508
01:00:25,590 --> 01:00:36,149
So diving deeper into ambient RNA are they are RNA molecules that leak into the nucleus suspensions from other nuclei due to APR,

509
01:00:36,150 --> 01:00:41,570
ptosis, stress or other reason they get incorporated into the job that the barcoded.

510
01:00:41,640 --> 01:00:47,190
They got barcoded and also they were amplified along with a new cat, native RNA.

511
01:00:48,690 --> 01:00:58,770
Just a little bit mentions extend or packages the can X can estimate the ambient related contaminations and on the image here,

512
01:01:00,660 --> 01:01:07,620
the bottom right will be the expression distributions and also the contamination distribution.

513
01:01:07,620 --> 01:01:13,829
In the table here you can see and diving into the first population on the or the first cluster,

514
01:01:13,830 --> 01:01:20,490
you can see in the observe, can they contain both the native cans and also the contaminate count?

515
01:01:23,010 --> 01:01:31,190
Okay. So the motivation for our project is that in analysis of single nucleotide RNA sequencing data, you can use either real or simulated data.

516
01:01:31,220 --> 01:01:40,070
However, for real datasets, you lack ground truth as to the native and contain real native and contamination RNA counts.

517
01:01:40,340 --> 01:01:47,450
And then when you use simulated data, it can suffer from irreversibility and dissimilarity from real data.

518
01:01:48,110 --> 01:01:55,280
So our goal was develop in our package to obtain simulated ground truth sonar and RNA SEQ data

519
01:01:55,280 --> 01:02:02,930
from contamination in order for researchers to validate and test their own analysis methods.

520
01:02:03,230 --> 01:02:09,950
And they'd be able to simulate different levels of RNA contamination to kind of show how well their methods work.

521
01:02:10,520 --> 01:02:13,669
Um, so here's just a overview of our package.

522
01:02:13,670 --> 01:02:19,190
So first we basically just called the contacts to separate ambient and native counts.

523
01:02:19,610 --> 01:02:30,740
And then we assign individual nuclei to metadata groups which are just based off of permutations of provided information such as sex or cell type.

524
01:02:31,100 --> 01:02:37,340
And then for these metadata group, we would estimate the mean and standard deviation for each group.

525
01:02:37,610 --> 01:02:47,209
We would estimate the mean sanitation native counts, and then we would simulate the native counts using negative binomial with me mu and the

526
01:02:47,210 --> 01:02:52,460
biological coefficient of variation which is this same deviation over mean squared.

527
01:02:52,910 --> 01:02:56,530
And then we'll scale these back scale.

528
01:02:56,570 --> 01:03:05,299
Then it accounts back to a more real environment because they're relatively margins after using the negative binomial.

529
01:03:05,300 --> 01:03:14,900
So we want to mimic real data as best we can and then we estimate parameters and simulate ambient counts in the same manner as steps three and four.

530
01:03:15,260 --> 01:03:23,390
So we'll get the mean standby deviation of those and scale of the back based on information we get from the contacts.

531
01:03:23,840 --> 01:03:29,130
And then after adjusting the scaling factor to the desired level, um,

532
01:03:29,720 --> 01:03:39,770
you can just combine the native and ambient distributions and basically play with how much contamination you want to include in your simulated data.

533
01:03:42,230 --> 01:03:45,710
So I'll be talking more about the actual our package implementation and some of our results.

534
01:03:46,610 --> 01:03:54,019
So our our package has one part input, which is a single nucleus gene expression matrix that you have in rows which represent your genes and

535
01:03:54,020 --> 01:03:58,760
you have columns which represent you can play the bunch of optional inputs that you could put in,

536
01:03:58,970 --> 01:04:04,400
add to the package for more accuracy or specificity, including different levels of contamination.

537
01:04:04,400 --> 01:04:07,100
If you want, try to promote the contamination.

538
01:04:07,490 --> 01:04:13,940
And then our output is a sparse matrix or a list of stress matrices if you want different levels of contamination.

539
01:04:15,260 --> 01:04:19,340
And those matrices will be and VK, which is the same size as your input matrix.

540
01:04:20,690 --> 01:04:24,470
Um, so our package has a main function to be lambda,

541
01:04:24,600 --> 01:04:31,430
which causes the helper functions that are listed below that pretty much do exactly what Andre said in the methods overview.

542
01:04:32,900 --> 01:04:35,990
You decontaminate, you estimate parameters, and then you simulate counts.

543
01:04:36,410 --> 01:04:41,210
And due to the nature of our data, we use heavy use this past winter season.

544
01:04:41,240 --> 01:04:45,290
Ah, so here's some preliminary results.

545
01:04:45,710 --> 01:04:51,860
I have some new map plots here. Your map is a dimension reduction method that reduces high dimensional data to two dimensions.

546
01:04:52,730 --> 01:05:02,000
So on the top you have the real data from a skeletal muscle single nucleus, from the fusion study on the left,

547
01:05:02,000 --> 01:05:08,150
you have positive contacts on the right and then on the bottom you have results from our simulations.

548
01:05:10,610 --> 01:05:15,050
So two good things that we see is that when we see really clean data, which is the bottom left one,

549
01:05:15,800 --> 01:05:20,660
we see relatively clean clusters that look similar to the real data, plus the contacts.

550
01:05:21,110 --> 01:05:23,450
And also, as you move right across the bottom row,

551
01:05:23,570 --> 01:05:30,980
how you're increasing that you're interested in increasing the contamination and also the chaos, the clustering seems to increase as well.

552
01:05:31,070 --> 01:05:35,149
And I would expect one thing I would say, though, is that when we increase our contamination,

553
01:05:35,150 --> 01:05:41,810
we never really our clusters don't maybe a little more distinct than what we would like to see in the real data,

554
01:05:44,120 --> 01:05:48,169
which is a contribution to our potential future work. So our model, along with the architecture,

555
01:05:48,170 --> 01:05:52,700
currently assumes that gene expression is relatively marginal within a metadata group such as among cell types.

556
01:05:53,540 --> 01:05:59,030
This may not be too realistic and probably why our high contamination model still have relatively distinct clusters.

557
01:05:59,690 --> 01:06:05,660
So we're thinking how we can improve on this. And also our current method is generalizable to single cell RNA SEQ data.

558
01:06:06,470 --> 01:06:10,400
We can make the method more specific to a single nucleus by introducing a new decontamination model

559
01:06:10,400 --> 01:06:15,080
that handles both ambient contamination and contamination of also larger cellular structures.

560
01:06:15,620 --> 01:06:21,500
After their protocols of doing this, the probing you'll be more specific to single employees in the can be.

561
01:06:21,910 --> 01:06:29,500
The generalizability single self. So we're still thinking about that and then come to the references and that's it.

562
01:06:29,500 --> 01:06:33,430
Thank you. Q So the question.

563
01:06:40,650 --> 01:06:44,820
So, you know, this is an area I'm a little familiar with.

564
01:06:44,820 --> 01:06:53,580
And uh, so, uh, you're basically assuming the, that it follows an entity going nowhere, which may not be true.

565
01:06:53,610 --> 01:07:01,919
So have you thought about not using the special assumption in those use actually because ambient or should be like in a

566
01:07:01,920 --> 01:07:11,040
mixture of you can make like a ball chlorinated within that and you can use use the data driven sort of contaminate data.

567
01:07:11,040 --> 01:07:17,370
Is it for a different approach you know using any. If this was an assumption.

568
01:07:18,120 --> 01:07:21,660
That doesn't make sense. Have you thought about the temperature?

569
01:07:22,750 --> 01:07:27,710
Um, we thought about, um, like increasing, like, uh, multi vary model,

570
01:07:27,840 --> 01:07:31,950
more so focusing on the nucleus where you want to like simulate ambient contamination.

571
01:07:31,950 --> 01:07:35,250
Also larger cellular contamination. Would you have to distribution to that?

572
01:07:37,500 --> 01:07:49,159
Yeah. Okay. But do you can you think about any you know, this mission with an operative it doesn't require it's a strong descriptive assumption.

573
01:07:49,160 --> 01:07:54,470
I suspect that that that might be why your result is not matching the world better.

574
01:07:56,420 --> 01:08:02,420
Okay. If I don't, I don't have a specific idea, but that's something you might want to consider.

575
01:08:02,930 --> 01:08:08,330
Okay, so we have a week. So we have one, one last group to do.

576
01:08:09,140 --> 01:08:17,060
Okay, so ten last, but not the least.

577
01:08:21,230 --> 01:08:32,130
Okay. All right.

578
01:08:32,490 --> 01:08:40,020
Hi, I'm Robert. And some thought our project was random glimmer, a random generalized linear mix effect regression model.

579
01:08:41,820 --> 01:08:45,150
First, we're going to talk a little bit about random forest models.

580
01:08:45,480 --> 01:08:50,370
They are a tree based and symbol method that gives us a wood strip in polyrhythm.

581
01:08:50,760 --> 01:08:54,870
So it fits one model there that they are really fast.

582
01:08:55,300 --> 01:08:57,780
You can extract variable importance measures,

583
01:08:58,230 --> 01:09:05,580
but they are not always the best because interpretations are very different from other regression methods.

584
01:09:08,470 --> 01:09:16,450
Why? Why is James better? The black box protection, for example, protectors are hard to come by.

585
01:09:16,930 --> 01:09:23,920
On the other hand, the Germans are much more interpretable into my models and the selection methods.

586
01:09:24,370 --> 01:09:33,009
We also can extract variable importance from linear models and linked function provides more flexibility than

587
01:09:33,010 --> 01:09:44,620
tree the mass at random j and use the method which shows the advantage of random fact with those of your hands.

588
01:09:44,920 --> 01:09:59,550
And he uses the tagging of DNA through a generalized linear model whose features are selected using for regression according to I see it,

589
01:09:59,710 --> 01:10:04,300
but it only supports fixed effects of giants.

590
01:10:04,300 --> 01:10:07,810
And these functions are not equations. Yeah.

591
01:10:08,020 --> 01:10:10,659
Random glimmer is our up our implementation.

592
01:10:10,660 --> 01:10:19,090
So we implemented both an optimized implementation of the original random Geelong package using speed Geelong.

593
01:10:19,690 --> 01:10:27,850
We also implemented additional model selection techniques such as lasso and we ensure that these are parallel visible over the bags.

594
01:10:28,450 --> 01:10:35,589
Additionally, we implemented a mixed effects version, specifically forcing a random intercept into the model which allows our interpretation of

595
01:10:35,590 --> 01:10:40,810
variable importance to become more interpretable when random effects modeling is appropriate.

596
01:10:43,310 --> 01:10:52,070
This is our modern workflow. The first step is to hold training data and so we like to both trap samples.

597
01:10:52,520 --> 01:10:56,480
Next, we take a random subset of features for each bag,

598
01:10:56,900 --> 01:11:06,920
and then we select the candidate to covariance based on correlation fairly within model use appropriate method.

599
01:11:10,200 --> 01:11:16,250
Just as an overview. So mixed effects models, specifically with a random intercept, are a common statistical technique.

600
01:11:16,260 --> 01:11:20,520
You can see it in RNA, c g was used for batch correction, spatial analysis.

601
01:11:20,520 --> 01:11:26,099
There are lots of applications, not a regression class, so we're not going to dive into it,

602
01:11:26,100 --> 01:11:29,850
but it's essentially adjusting for variation within each group.

603
01:11:30,060 --> 01:11:33,210
You can think of it like test score to sample from the same school.

604
01:11:33,540 --> 01:11:38,340
When you have lots of different schools or cells from the same individual, when you have lots of individuals in your sample,

605
01:11:39,690 --> 01:11:45,630
the random intercept model, like we said, you force a random intercept into them, into each of the bag models.

606
01:11:46,050 --> 01:11:49,200
So we then apply appropriate model selection criteria.

607
01:11:49,200 --> 01:11:54,120
Here we're applying HCC, which seems to be more appropriate for the mixed effects models.

608
01:11:54,120 --> 01:11:58,080
We're fitting. Then we use forward selection to select the fixed effects.

609
01:11:58,500 --> 01:12:04,139
So we're not selecting random effects, but we tested on simulations and real data here.

610
01:12:04,140 --> 01:12:05,670
We're just going to present the simulations.

611
01:12:05,910 --> 01:12:14,010
What you'll see is that on the I guess your left, the random glimmer or random deal on model and the random linear model,

612
01:12:14,010 --> 01:12:24,209
they both re recover the original important covariates about the same, but the false positives are a lot lower in the random glimmer model.

613
01:12:24,210 --> 01:12:33,780
So it's saying things are important incorrectly fewer times than the random GLM model on the same simulation datasets.

614
01:12:34,140 --> 01:12:38,130
So that's a benefit of being able to run this model.

615
01:12:39,900 --> 01:12:47,910
For our optimized implementations. We first use this feature which provides an efficient way of implementing and for

616
01:12:47,970 --> 01:12:53,790
big data is directly so the normal equations without the use of Q R optimization,

617
01:12:53,790 --> 01:12:57,330
which is more efficient and gives in the regular general function.

618
01:12:57,690 --> 01:13:04,500
Additionally, we use that as a regression with a G and a package which is a common which is common for

619
01:13:04,530 --> 01:13:10,860
models selection because it allows some coefficients go to zero those we only need to do one.

620
01:13:14,250 --> 01:13:21,630
We you see some of the tape and real daytime drama call for you to a paramedic on set.

621
01:13:22,020 --> 01:13:31,380
And now we are 9% real data, which has six in through observation and about 2300 are called events.

622
01:13:34,690 --> 01:13:40,770
In the blood. On the left, we can observe an appearance with the most burned form almost identically.

623
01:13:41,440 --> 01:13:49,010
The next blood shows that as the number of covariance increases, Greenland's runtime increases faster than the other two.

624
01:13:49,750 --> 01:13:56,910
And The Last Blood shows that with the full text size and number of covariance, our methods consistently outperform due.

625
01:13:59,200 --> 01:14:04,650
Here. We'll start on the top part. That's the number of cores on the x axis in our runtime.

626
01:14:04,660 --> 01:14:09,010
So we see that while all of them benefit from an increased number of cores and parallel zation,

627
01:14:09,460 --> 01:14:15,880
we still maintain advantage even all the way up to 16 cause and that has to do with how the runtime is optimized.

628
01:14:17,500 --> 01:14:24,530
When we look at the bottom left plot, you can see on average for a fixed configuration that we picked, we had speed,

629
01:14:24,550 --> 01:14:30,970
we'll have a 2.1 times speed up about from Jhelum and the jail and lasso or jail on

630
01:14:30,970 --> 01:14:37,180
that lasso implementation have about a 2.6 times speed up from the G1 implementation.

631
01:14:37,870 --> 01:14:44,350
Finally, the bottom left shows the speed up calculation over the number of cores across the number of cores on the x axis.

632
01:14:44,620 --> 01:14:50,739
And this is again showing we maintain a speed up advantage even as you paralyze over a large number.

633
01:14:50,740 --> 01:15:00,260
Of course. We can conclude that we have optimized implementation, we obtain significant runtime advantages,

634
01:15:00,260 --> 01:15:03,940
we maintain accuracy and we benefit from feeding models.

635
01:15:03,950 --> 01:15:10,820
In parallel are mixed effects models. Our early results suggest we have a misinterpretation.

636
01:15:11,540 --> 01:15:21,589
They recover the same features, we have fewer false positives and therefore they are more applicable to random intercept problems in the future.

637
01:15:21,590 --> 01:15:28,159
We'd like to re-implement the optimized version with the addition of soft lasso that doesn't take all coefficients to go to zero,

638
01:15:28,160 --> 01:15:36,980
but rather like a threshold. And then we also want to test non formula based implementations of the model selection.

639
01:15:37,250 --> 01:15:41,120
We'd also like to try alternative criteria like P values and adjusted R squared

640
01:15:41,120 --> 01:15:46,489
because AIC is not the only metric to use for the mixed effects models.

641
01:15:46,490 --> 01:15:50,629
We'd like to include a more complicated random effect because certain modeling strategies

642
01:15:50,630 --> 01:15:55,700
require that we'd like to test on larger data with bigger and more complex simulations.

643
01:15:55,940 --> 01:16:01,669
We'd like to compare our results to more real datasets. Yeah, citations.

644
01:16:01,670 --> 01:16:07,520
And we are open to any questions. Thank you for questions.

645
01:16:11,750 --> 01:16:14,809
So just let me ask a question for you.

646
01:16:14,810 --> 01:16:21,060
You may need to implement ensure that you can plug in just a Glenn or speed Glenn or gentlemen.

647
01:16:21,200 --> 01:16:24,889
So that's that's how you implement it. We didn't implement it.

648
01:16:24,890 --> 01:16:30,590
So you can provide your own implementation because certain implementations don't provide the model selection criteria that you need.

649
01:16:31,070 --> 01:16:38,090
Rather, we hardcoded which implementations you were allowed to use because that seemed a whole lot safer from a developer standpoint.

650
01:16:38,600 --> 01:16:44,270
So you have a three, four and three different functions for four different functions because of the mixed effects models.

651
01:16:44,450 --> 01:16:47,660
Okay. So mixed effects model doesn't use, though.

652
01:16:48,230 --> 01:16:51,550
Is this speeds jarring on other things? No, it does not.

653
01:16:51,560 --> 01:16:54,890
But it does use the parallel ization over the number, of course.

654
01:16:55,130 --> 01:17:02,740
Okay. So then then your recommendation is to use a mix effect model to for better extra variable.

655
01:17:02,780 --> 01:17:05,780
It'll come with a slower it'll it'll take a little bit longer.

656
01:17:05,800 --> 01:17:15,200
But we, what we were saying is you reduce your false positives and that seems like a good drawback if you're a good advantage if you're doing model

657
01:17:15,200 --> 01:17:21,310
selection because you're going to be including fewer covariates and reducing the probability of multiple linearity or something like that.

658
01:17:21,680 --> 01:17:31,520
I mean, the comparison was interesting and in the other question, we have one more minute, but so yeah, if you don't have any questions,

659
01:17:31,760 --> 01:17:40,129
please make sure that you complete the evaluation that are in the canvas said it's a and that that'll be useful for me to

660
01:17:40,130 --> 01:17:49,310
understand how you are that you received this presentation how how much you understood and how much you felt felt the process.

661
01:17:49,790 --> 01:17:52,820
Those are those are important to therefore for me to know.

662
01:17:53,720 --> 01:18:04,580
Yeah. And I'm I'm really glad that the opportunity here because made it was really really great and also looks like you put a lot of effort.

663
01:18:04,600 --> 01:18:13,160
I know that this has been a lot of work, but I'm really proud of you guys and I thank you for doing a good work that in the Wednesday, too.

664
01:18:13,340 --> 01:18:22,139
Thank you very much. Okay. So then we'll see you on Wednesday.

665
01:18:22,140 --> 01:18:23,730
And I feel I had a.

