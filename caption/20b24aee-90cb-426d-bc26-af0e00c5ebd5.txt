1
00:00:01,840 --> 00:00:06,110
Okay. So welcome back.

2
00:00:06,470 --> 00:00:11,030
We're at a high community level right now.

3
00:00:11,150 --> 00:00:23,450
I think the update is on Thursday. But yeah, we have, we've, it's masking up, uh, indicator here for this week at least till Thursday.

4
00:00:31,880 --> 00:00:39,150
All right. So. Just checking in on where we are in the course today.

5
00:00:39,160 --> 00:00:47,170
We are going to be. Looking at handout three basic inference for diagnostic tests and disease screening.

6
00:00:47,860 --> 00:00:50,620
And this is this week we have lab.

7
00:00:50,630 --> 00:00:59,200
So your very first lab and they're going to be covering meta analysis, which is very related to the homework that's coming up.

8
00:01:00,310 --> 00:01:05,940
And if you recall. Under the assignment tab.

9
00:01:09,410 --> 00:01:14,310
The first homework. Is due September the 18th.

10
00:01:14,320 --> 00:01:21,790
So you have time to do it, but it's a lot of work. So as you go through this week, you definitely want to get started.

11
00:01:21,850 --> 00:01:24,999
You have enough to do the main analysis based on my course notes.

12
00:01:25,000 --> 00:01:28,900
But of course the the lab this week will be additionally helpful.

13
00:01:29,620 --> 00:01:38,200
So if you have a chance to take a stab at it before your lab and have really good questions for the exams during lab, that might be a good strategy.

14
00:01:46,150 --> 00:01:51,730
And there's quite a bit of a lag here today. In the in the Internet.

15
00:01:51,740 --> 00:01:58,140
So we'll deal with that as it comes. Okay.

16
00:02:05,430 --> 00:02:10,350
All right. So some of you may have seen a little bit of this material before in an introductory class,

17
00:02:10,770 --> 00:02:16,290
at least when I was teaching Intrastat quite some time ago.

18
00:02:16,800 --> 00:02:21,840
You learned the term sensitivity, specificity, positive predictive value,

19
00:02:21,840 --> 00:02:28,350
negative predictive value, and you learned Bayes theorem for going between sensitivity.

20
00:02:28,350 --> 00:02:33,749
Specificity. Versus positive predictive value, negative predictive value.

21
00:02:33,750 --> 00:02:36,059
So there's going to be a review of that material here,

22
00:02:36,060 --> 00:02:41,610
but I'm going to go into it much more deeply than you would have had time to do in an intro class.

23
00:02:42,180 --> 00:02:46,770
And in particular, I'm going to put some focus on the receiver operating characteristic curves,

24
00:02:47,400 --> 00:02:54,720
which is a method that would be very useful in assessing how a continuous biomarker might help you predict.

25
00:02:55,890 --> 00:03:08,070
Either a diagnosis or progression or some yes or no outcome and how people go about setting up tests for screening disease and so on.

26
00:03:08,250 --> 00:03:10,650
All right. So that's kind of my goal here.

27
00:03:12,450 --> 00:03:21,809
And so just to get us on the same page, diagnostic testing or screening is usually based on either continuous measures,

28
00:03:21,810 --> 00:03:30,240
so it could be based on serum blood, protein expression or metabolomics, any, any omics data that you can think of.

29
00:03:31,200 --> 00:03:42,000
It's based on some kind of continuous measure or could be some ordinal measure and where these are linked to disease outcomes in some way.

30
00:03:42,660 --> 00:03:49,860
And so we don't necessarily talk to patients about whether they have a condition or not based on a continuous measure.

31
00:03:50,280 --> 00:03:55,050
You come up with a diagnosis, right? You say yes or no, you have the diagnosis.

32
00:03:55,530 --> 00:04:05,550
So the threshold for the diagnostic measure is typically used to separate diagnoses into two groups that the subject has or does not have the disease.

33
00:04:06,300 --> 00:04:11,310
And so for the purposes of this handout, I've just kind of arbitrarily said,

34
00:04:12,150 --> 00:04:16,260
you know, for any biomarker, high values would be more likely to have disease.

35
00:04:16,260 --> 00:04:20,010
Low values would be less likely. It doesn't have to be that direction.

36
00:04:20,010 --> 00:04:27,000
But I've just been consistent throughout the handout where, you know, high is bad, low is good.

37
00:04:31,050 --> 00:04:37,920
So how do we choose that threshold for a particular diagnostic measure if we've got a new biomarker that no one's played around with before?

38
00:04:37,950 --> 00:04:45,720
How do we choose such a threshold for a diagnostic measure, and how well are patients classified based on that threshold?

39
00:04:47,600 --> 00:04:55,790
So sensitivity is what proportion of deceased subjects will get a positive test result.

40
00:04:56,000 --> 00:05:01,430
So you start off with people that, you know have the condition.

41
00:05:02,780 --> 00:05:09,200
So you have to be sure they had the condition and then you estimate the probability that they come back with a positive test,

42
00:05:09,200 --> 00:05:12,430
given they have the condition. All right.

43
00:05:12,700 --> 00:05:19,510
And some people call that the true positive fractions. So you might see that in the Hindu in various places.

44
00:05:20,080 --> 00:05:31,200
But I will tend to almost always use sensitivity. And specificity is what proportion of non-disney subjects will get a negative test result.

45
00:05:32,040 --> 00:05:38,010
So in this case, you have to have a population you start off with, you know, they don't have the condition.

46
00:05:38,280 --> 00:05:43,679
So there's some gold standard with which you can determine they do not have the condition.

47
00:05:43,680 --> 00:05:47,710
And then among those people, how many? Test negative.

48
00:05:49,150 --> 00:05:57,160
So in each case you're trying to come up with, you know, how often is the test giving you a useful result?

49
00:05:59,690 --> 00:06:09,500
So specificity and sometimes looked at, you know, in the complement one mile specificity where that's the false positive fraction.

50
00:06:09,510 --> 00:06:12,889
So if you get a diagnostic test, you can have false positives.

51
00:06:12,890 --> 00:06:17,990
And the false positive fraction is the probability that you get that false positive.

52
00:06:21,040 --> 00:06:24,910
Positive predictive value or PPV.

53
00:06:26,060 --> 00:06:31,730
Is usually you use Bayes Theorem or something like that to figure out what this is.

54
00:06:32,690 --> 00:06:40,470
I'm so used to it. And so this is what proportion of subjects with a positive test really have the disease.

55
00:06:40,490 --> 00:06:43,670
So this is, of course, what everybody really wants to know.

56
00:06:44,390 --> 00:06:48,560
So given I'm looking at a positive test, what's the probability? I have the condition.

57
00:06:52,230 --> 00:06:58,320
And negative predictive value is what proportion of subjects with negative test results truly have no disease.

58
00:06:58,350 --> 00:07:03,270
So again, if I'm looking at a negative test, what's the probability that I don't have the disease?

59
00:07:06,680 --> 00:07:15,110
All right. So I'm going to get some notation together so we can talk about data and random variables that are involved in these definitions.

60
00:07:15,830 --> 00:07:20,240
And so D is going to be like D for disease.

61
00:07:20,300 --> 00:07:26,060
So it's a one or a zero. It's one if they actually truly have the disease and zero if not.

62
00:07:27,500 --> 00:07:32,450
And I'm going to use t t for test where it's a one.

63
00:07:32,450 --> 00:07:35,960
If it's a positive test, result in zero. If it's a negative test result.

64
00:07:36,290 --> 00:07:42,679
And throughout the hair that I've tried to color code, unfortunately, my choice of orange and yellow, it's hard to see the difference.

65
00:07:42,680 --> 00:07:48,110
But I have used so many colors and felt that just bear with me.

66
00:07:48,110 --> 00:07:51,200
I'm d d orange and yellow.

67
00:07:51,200 --> 00:07:58,750
You'll see elsewhere in the handout. So sensitivity in terms of these random variables is given.

68
00:07:59,050 --> 00:08:03,010
You really have the disease that equals one. What's the probability?

69
00:08:03,010 --> 00:08:08,020
My test comes back positive. Specificity is given.

70
00:08:08,020 --> 00:08:12,720
I don't have the disease. What's the probability? My test comes back negative.

71
00:08:12,730 --> 00:08:17,140
So both of these are what you would hope happens with a good test.

72
00:08:19,860 --> 00:08:22,660
And the fourth positive fraction is one minus specificity.

73
00:08:22,680 --> 00:08:28,260
So given this person really doesn't have the disease, what's the probability they come back with a positive test?

74
00:08:31,680 --> 00:08:35,640
So positive predictive value. The thing you conditioned on is switched.

75
00:08:36,900 --> 00:08:46,380
So for positive and negative predictive value, you're basing the the probability on what their test result was to begin with.

76
00:08:46,410 --> 00:08:52,440
So if they tested positive, what's the probably they really have the disease that's positive predictive value.

77
00:08:53,550 --> 00:08:57,240
And negative predictive value is given. They tested negative.

78
00:08:57,240 --> 00:09:01,170
What's the probability that they really don't have the condition, the disease?

79
00:09:05,080 --> 00:09:09,480
So a quick review of Bayes Theorem and positive predictive value.

80
00:09:09,490 --> 00:09:14,530
A negative predictive value is going to be on the next couple of slides here.

81
00:09:14,830 --> 00:09:21,520
And so if I know the sensitivity and specificity of a disease, do I know the positive predictive value and the negative predictive value?

82
00:09:23,080 --> 00:09:27,880
And the answer. Comes from Bayes Theorem.

83
00:09:28,450 --> 00:09:32,310
So this is probably your nightmare from your first interest class.

84
00:09:32,320 --> 00:09:35,500
This is the least favorite topic in Intrastat, if I recall.

85
00:09:36,470 --> 00:09:40,390
But it's not too bad once you know the terminology.

86
00:09:40,390 --> 00:09:42,850
So it looks like a lot of gobbledygook down here.

87
00:09:42,850 --> 00:09:49,570
But the positive predictive value is, again, if you have a positive test with the probability, they really have the disease.

88
00:09:50,110 --> 00:09:58,240
And so Bayes Theorem says that you can write that as the probability is that they have the the disease.

89
00:09:59,580 --> 00:10:03,440
Times the probability that they test positive given they have a disease.

90
00:10:03,450 --> 00:10:07,830
So this is like the prevalence here and this is.

91
00:10:09,010 --> 00:10:13,900
The sensitivity. Right. You probably they really have the disease the probably test positive.

92
00:10:14,590 --> 00:10:19,120
And then that same part from the top is copied down. That's the same as this first part in the denominator.

93
00:10:19,600 --> 00:10:23,770
And the second part of the denominator has a term that's one minus the prevalence.

94
00:10:24,690 --> 00:10:33,390
And here. Given they don't have the disease, what's the probability they test positive that as the false positive fraction or one minus specificity.

95
00:10:34,110 --> 00:10:41,610
So in terms of the in terms of the words and the jargon rather than the symbols, this is what that formula is saying.

96
00:10:42,090 --> 00:10:46,079
And an important feature of this formula is that, you know,

97
00:10:46,080 --> 00:10:52,470
even if you know the sensitivity and the false positive fraction or this or one minus the specificity,

98
00:10:52,980 --> 00:10:59,790
you need to know the prevalence of the condition in the population to know the positive predictive value.

99
00:11:00,210 --> 00:11:06,410
So, for instance, everybody is very pandemic knowledgeable now.

100
00:11:06,420 --> 00:11:13,050
Right. And you had a period of time when you were probably taking or are still taking COVID tests all the time.

101
00:11:13,830 --> 00:11:20,910
So if you test someone who is symptomatic, you know that the prevalence of COVID and symptomatic.

102
00:11:22,000 --> 00:11:26,920
Populations is higher. Your positive predictive value will be higher.

103
00:11:26,950 --> 00:11:29,740
So if you're testing only symptomatic people,

104
00:11:30,550 --> 00:11:38,320
then the probability that given that they have a positive test that they really have COVID is going to be higher.

105
00:11:38,530 --> 00:11:41,620
So if you're testing a more prevalent population.

106
00:11:42,690 --> 00:11:46,020
But if you're testing a population that isn't symptomatic.

107
00:11:47,520 --> 00:11:53,669
There's still going to be some false positives in that mix and that the fact that your

108
00:11:53,670 --> 00:11:58,200
prevalence is very low in that group will make your positive predictive value much lower.

109
00:11:59,250 --> 00:12:04,170
So the sensitivity and specificity are the same probabilities.

110
00:12:04,170 --> 00:12:09,390
They're not affected by prevalence, but positive and negative predictive values.

111
00:12:10,680 --> 00:12:18,930
Are highly affected by prevalence. And so you can sort of see how when they're doing screening for anything, not just COVID,

112
00:12:18,930 --> 00:12:27,330
but screening for prostate cancer or you name your condition, they they are trying to, you know, screen.

113
00:12:28,350 --> 00:12:37,320
Very high risk populations to the extent they can figure that out so that their positive and negative predictive values are high.

114
00:12:39,300 --> 00:12:45,930
So negative predictive value has a similar. Bunch of math here with Bayes Theorem.

115
00:12:46,710 --> 00:12:51,330
And so the probability that you test negative giving, you don't have it.

116
00:12:51,410 --> 00:12:57,479
Sorry, the probability you don't have the condition giving you test negative is again

117
00:12:57,480 --> 00:13:02,220
very tied to prevalence with one month per month showing up in the formula.

118
00:13:02,550 --> 00:13:10,890
A lot here. And so it's not I don't want you to necessarily get too bogged down by the formula.

119
00:13:10,900 --> 00:13:12,840
This isn't your first intrastat class.

120
00:13:13,500 --> 00:13:23,790
But I would I do want you to own for this course is this idea that positive and negative predictive value change with prevalence.

121
00:13:24,570 --> 00:13:27,090
So that dependance I want you to be familiar with.

122
00:13:30,760 --> 00:13:39,670
So PBB and IPV require knowledge of the prevalence of the disease in the population and prevalence can only be estimated from a cohort study.

123
00:13:44,700 --> 00:13:49,180
So just an example to give you a little bit of ah, you know, reference here.

124
00:13:49,200 --> 00:13:59,280
So suppose we have a diagnostic test with sensitivity and specificity each of 0.95 for a disease that's prevalent in 1% of the population.

125
00:14:01,960 --> 00:14:07,660
So using the previous formula, we have a positive predictive value of 16%.

126
00:14:09,350 --> 00:14:15,020
So is that a good value of positive predictive value? So positive value.

127
00:14:15,020 --> 00:14:19,550
Remember, that's what's the probability. I really have the condition given I have a positive test.

128
00:14:21,140 --> 00:14:29,510
So before you had the test, there was a 1% chance, just as being a member of the general population, that you had this condition.

129
00:14:30,760 --> 00:14:34,390
And after a positive test, that probability goes up to 16%.

130
00:14:35,020 --> 00:14:41,860
And this is for a very good test. I mean, sensitivity and specificity of 95% is very good for each of those metrics.

131
00:14:43,820 --> 00:14:50,420
Now. So given a person has a disease, they test positive 95% of the time, that's really good.

132
00:14:51,320 --> 00:14:55,190
And given they don't have the disease, they test -95% of the time.

133
00:14:55,190 --> 00:15:02,440
That's also very good. And yet if you are applying the test in a population where 1%.

134
00:15:03,440 --> 00:15:08,080
I have the condition. The positive predictive value isn't isn't great.

135
00:15:11,220 --> 00:15:20,190
So large sensitivity does not guarantee large values of positive predictive value because rare diseases are difficult to screen.

136
00:15:21,760 --> 00:15:24,550
So prior to being screened, the probability of disease is 1%.

137
00:15:25,300 --> 00:15:31,180
And with the added information of a positive test result, that probability of disease is 16%, which is helpful.

138
00:15:31,990 --> 00:15:41,500
But there's there's still further testing that if you really need to have a very clear diagnosis, further testing is needed.

139
00:15:44,440 --> 00:15:50,920
Okay. So that's a little bit of the background. And so now, you know, estimation with data.

140
00:15:51,160 --> 00:15:59,920
So suppose we have a two by two table where the columns are, whether you really have the condition or not, positive or negative.

141
00:15:59,920 --> 00:16:09,180
And the rows are the test results, positive or negative. And we have our usual ABCD notation for the entries.

142
00:16:09,180 --> 00:16:17,110
So if you are looking at this table. The sensitivity or the true positive fraction is the number positive.

143
00:16:18,400 --> 00:16:22,120
This a small number of people who truly have the disease.

144
00:16:22,810 --> 00:16:27,639
Given you had a positive test. Sorry.

145
00:16:27,640 --> 00:16:30,850
Giving you sorry giving you have you truly have the disease.

146
00:16:31,570 --> 00:16:37,660
So a over A-plus D so remember sensitivity is given you really do have the disease.

147
00:16:38,770 --> 00:16:44,860
What's the probability you test positive say a policy and specificity or one minus the false positive fraction.

148
00:16:46,100 --> 00:16:50,540
Is giving you really don't have the disease. That's B plus D people.

149
00:16:51,050 --> 00:16:54,800
What's the probability you test negative. So D over B plus D.

150
00:16:56,260 --> 00:17:03,010
And. I double check all the time with two by two tables and do these calculations just to

151
00:17:03,010 --> 00:17:07,270
make sure that whatever software package I'm using is doing what I think it's doing.

152
00:17:09,320 --> 00:17:14,450
If you have a cohort study and only if you have a cohort study,

153
00:17:14,450 --> 00:17:21,110
you can use this two by two table to get positive predictive value, a negative predictive value, even without using Bayes theorem.

154
00:17:21,890 --> 00:17:29,420
So if you have a cohort study, then the prevalence in this table is is going to be accurate.

155
00:17:30,790 --> 00:17:37,149
And so in that case, the positive predictive value is going to be A, over A-plus.

156
00:17:37,150 --> 00:17:42,520
B So given they have a positive test result, so that's a plus.

157
00:17:42,520 --> 00:17:48,340
People, what's the probability that they really have the disease that's a over A-plus B.

158
00:17:49,920 --> 00:17:59,129
And similarly negative predictive value, given they don't have a given, they don't have a positive test, the negative test, what's the probable?

159
00:17:59,130 --> 00:18:01,770
They really don't have the disease. That's D over C plus D.

160
00:18:03,060 --> 00:18:12,550
So this truly depends on having true disease status measured in a way that's capturing prevalence.

161
00:18:12,560 --> 00:18:19,140
So A-plus C over n better B the prevalence in the population you're trying to talk about.

162
00:18:19,590 --> 00:18:24,590
So if you have a case control study where you over sample cases, you know,

163
00:18:24,720 --> 00:18:28,650
you might have half of the people in this column and half of the people in this column.

164
00:18:29,160 --> 00:18:31,950
You can't use this shortcut formula.

165
00:18:31,950 --> 00:18:38,160
You have to go to Bayes theorem and insert the true prevalence of the population and get those numbers with Bayes theorem.

166
00:18:43,100 --> 00:18:47,329
So we're trying to figure out how to put together diagnostic tests.

167
00:18:47,330 --> 00:18:53,660
And so when you're trying to look at different diagnostic tests and compare them,

168
00:18:54,260 --> 00:19:01,400
you want to have some kind of, you know, single dimension number to compare them with.

169
00:19:01,410 --> 00:19:05,870
So we know that they're going to have sensitivity and specificity, specificity values,

170
00:19:06,620 --> 00:19:09,620
but it's just convenient to have a single value to compare them with.

171
00:19:10,400 --> 00:19:20,600
And so one possibility is to use some kind of a weight that places more or less importance on sensitivity versus specificity.

172
00:19:21,020 --> 00:19:33,349
And a special case is called Newtons Index, where you have a way to behave for sensitivity and a way to behave for specificity.

173
00:19:33,350 --> 00:19:37,560
So you're kind of weighting them equally. And you're trying to figure out.

174
00:19:40,830 --> 00:19:45,330
You know, what's the best what's the best diagnostic test if I weight them equally?

175
00:19:48,630 --> 00:19:55,770
So the way it quantifies the relative importance of sensitivity and specificity to each other and depending on your situation,

176
00:19:55,770 --> 00:19:58,590
one might be much, much more important than the other.

177
00:19:59,550 --> 00:20:07,680
So in a pandemic, you really want to have high sensitivity so that you can isolate people who can continue to spread,

178
00:20:08,580 --> 00:20:13,080
and you don't want to miss people that are unknowingly going to go out and spread.

179
00:20:13,110 --> 00:20:19,710
So in a pandemic, that's really, really important. If there is a very invasive next step.

180
00:20:21,030 --> 00:20:25,350
Then specificity is going to be very important.

181
00:20:27,180 --> 00:20:33,780
And so you certainly want sensitivity, but you don't want to put a patient through something very invasive.

182
00:20:34,980 --> 00:20:44,880
Nessus Necessarily, so you want high specificity. So in any situation and how you wait, sensitivity or specificity can be quite different.

183
00:20:45,150 --> 00:20:51,270
And even in the same situation for different individuals, they might have a different weight for sensitivity or specificity.

184
00:20:55,990 --> 00:21:01,540
So the relative importance is based on the impact of incorrect diagnoses, and it can be somewhat subjective.

185
00:21:03,520 --> 00:21:12,399
If the way to have than sensitivity and specificity are deemed equally important and if no one clinically is is looking at it,

186
00:21:12,400 --> 00:21:17,680
this is something a mathematician might automatically seek to maximize, you know,

187
00:21:19,240 --> 00:21:25,600
for any diagnostic test trying to find the one that has the best score or the way to have.

188
00:21:28,180 --> 00:21:33,249
So even more popular than this kind of weighted average is something called

189
00:21:33,250 --> 00:21:37,180
the area under the receiver operating characteristic curve or the RC curve.

190
00:21:37,180 --> 00:21:42,670
And this might be a term you've heard of. How many of you have heard of RC curves before?

191
00:21:43,540 --> 00:21:45,999
All right. So you may have been seeing plots of these.

192
00:21:46,000 --> 00:21:54,070
So that's a very popular summary measure when you're considering continuous biomarkers and how to make a threshold for a diagnostic test.

193
00:21:59,160 --> 00:22:04,739
Okay. So we mentioned this earlier. So the test result is typically reported as a binary outcome.

194
00:22:04,740 --> 00:22:10,560
Either they have or do not have the disease. It's usually based on a continuous measurement behind the scenes.

195
00:22:11,520 --> 00:22:16,530
So for instance, subjects with a very high value of a continuous biomarker might be diagnosed with the disease.

196
00:22:16,890 --> 00:22:22,560
And it can certainly be the reverse as well, that if you have a very low value of a biomarker, that could be disease.

197
00:22:22,860 --> 00:22:27,629
But throughout this handout, just so that we always are thinking in the same direction and not confusing ourselves,

198
00:22:27,630 --> 00:22:34,220
I'm going to assume that my plots and figures and things are relate to a biomarker were high as bad.

199
00:22:37,570 --> 00:22:43,030
And so how do we choose that diagnostic threshold for the disease based on that continuous biomarker?

200
00:22:43,060 --> 00:22:48,190
Well, you can consider all possible diagnostic thresholds.

201
00:22:49,190 --> 00:22:52,040
From a data set with known cases and controls.

202
00:22:52,700 --> 00:22:58,220
So you have people, you know, don't have the disease, you know, people who do for sure have the disease.

203
00:22:59,240 --> 00:23:04,430
And then you look at all the thresholds that are possible for that biomarker and see how well it classifies them.

204
00:23:05,300 --> 00:23:12,680
So again, for this handout, subjects with values below the threshold are going to be classified as testing negative for the disease,

205
00:23:13,940 --> 00:23:18,140
and subjects with values above the threshold are going to be labeled a positive test result.

206
00:23:19,950 --> 00:23:26,820
And so each of these possible thresholds which are going to basically be the values in your dataset that people had for the biomarker,

207
00:23:27,240 --> 00:23:33,540
there is going to be a corresponding sensitivity and specificity for how it classifies people in your dataset,

208
00:23:34,170 --> 00:23:38,820
and you choose a threshold with the sensitivity or specificity profile that looks attractive.

209
00:23:42,300 --> 00:23:50,100
So I want to kind of give you a sense of how this is working in terms of random variables and thresholds.

210
00:23:50,100 --> 00:23:56,190
So imagine a setting and where you have a continuous value of a diagnostic test.

211
00:23:57,150 --> 00:24:00,510
And I'm just showing you two possible thresholds here.

212
00:24:02,660 --> 00:24:06,860
And under this kind of salmon color.

213
00:24:08,010 --> 00:24:13,170
Is the distribution of values of this biomarker if you actually truly have the disease.

214
00:24:14,870 --> 00:24:22,160
So you could have any this is kind of your distribution of values you might have in the population among people who truly have the disease.

215
00:24:22,700 --> 00:24:29,630
And if you truly don't have the disease, then this is the distribution of values for this biomarker you might have.

216
00:24:31,890 --> 00:24:39,000
So it's imperfect declassifying disease. There's a lot of overlap between the two curves and there's no real clear

217
00:24:39,000 --> 00:24:43,230
threshold that you could pick where you get 100% sensitivity and specificity.

218
00:24:44,160 --> 00:24:48,870
The only way that can happen is if there's no overlap at all on these and these distributions.

219
00:24:50,870 --> 00:24:56,600
So most of the time we have overlap. So let's look at threshold 1/1, this big green line.

220
00:24:58,250 --> 00:25:03,170
And so we said in this handout, Haier is going to be test positive, lower.

221
00:25:03,170 --> 00:25:07,850
It's going to be test negative. So for threshold one.

222
00:25:10,720 --> 00:25:15,610
I'm going to test positive every time I'm on this side of that green line.

223
00:25:15,610 --> 00:25:22,180
Right. So I'm going to catch a lot of these disease people with that threshold one.

224
00:25:23,400 --> 00:25:26,910
So given I'm just, you know, I'm looking at a disease person,

225
00:25:26,910 --> 00:25:32,520
the probability that they test positive or have a biomarker greater than the threshold one is pretty high.

226
00:25:33,330 --> 00:25:37,020
So my sensitivity is going to be great for threshold one.

227
00:25:40,830 --> 00:25:43,920
All right. What about? Specificity.

228
00:25:43,920 --> 00:25:58,120
So if I'm in a non disease group. And my probability of testing negative, as I would hope to, is looking like 50%.

229
00:25:58,150 --> 00:26:02,380
And I can tell it's 50% because it's right in the middle of the symmetric distribution.

230
00:26:04,060 --> 00:26:13,130
So my specificity doesn't look great. And you know, that means my fourth party production is going to also be 50%.

231
00:26:13,150 --> 00:26:19,510
That's this group. So all these people on this side of the green line under the kind of blue curve,

232
00:26:20,440 --> 00:26:24,040
they really didn't have the disease, but their biomarker was above that threshold.

233
00:26:24,040 --> 00:26:29,770
So 50% of the time, a positive test is going to happen.

234
00:26:30,780 --> 00:26:33,870
Even though they don't have the condition. So that's threshold one.

235
00:26:34,530 --> 00:26:40,290
So this is one possible value of the biomarker that you could use to call someone disease, not disease.

236
00:26:40,950 --> 00:26:45,300
Great sensitivity, not great specificity. And so let's look at threshold two.

237
00:26:47,360 --> 00:26:55,730
So for threshold two, given you have the disease, the chance you test positive is less than 50%.

238
00:26:57,360 --> 00:27:02,780
Right, because it's less than the halfway point in that film and curve.

239
00:27:04,130 --> 00:27:08,150
So your sensitivity is not great. You're missing a lot of people who have disease.

240
00:27:08,150 --> 00:27:15,980
If if a score greater than that yellow line over here is what you use to classify people as disease.

241
00:27:16,550 --> 00:27:19,580
And I look at the specificity.

242
00:27:19,590 --> 00:27:25,190
So if I'm really not disease, the chance of me getting your value above that threshold is pretty small.

243
00:27:25,910 --> 00:27:33,260
So my specificity is going to be great. And so I've only looked at two thresholds here.

244
00:27:36,010 --> 00:27:43,450
But I could have moved the threshold anywhere from left to right, and I would have had a different value of sensitivity and specificity.

245
00:27:43,450 --> 00:27:48,820
So just sort of gaining intuition about this that I'm big on intuition, right?

246
00:27:49,750 --> 00:27:58,900
Formulas. I'm not really necessarily set on you memorizing, but intuition I want you to own so.

247
00:28:01,970 --> 00:28:10,190
So higher test values are more indicative of presence of the disease, but there's a lot of overlap in these distributions.

248
00:28:10,760 --> 00:28:15,590
And if I move the threshold to I have this on the next page.

249
00:28:21,640 --> 00:28:29,860
I'm going to do it here. He comes up later in the. So if I move the threshold more and more to the right over here.

250
00:28:32,630 --> 00:28:38,480
Is sensitivity increasing or decreasing as I increase the threshold for diagnosing.

251
00:28:42,040 --> 00:28:47,650
It's going to be going up or is it going down? If I'm moving this threshold more and more to the right.

252
00:28:52,030 --> 00:28:56,710
So let's throw. All three is right here. Is my sensitivity.

253
00:28:57,700 --> 00:29:02,169
Better or worse than the other two? Yeah,

254
00:29:02,170 --> 00:29:08,260
it's a lot worse because I would only be catching a very small proportion of the people

255
00:29:08,260 --> 00:29:12,550
who truly have the disease if I require them to have a biomarker that's above this value.

256
00:29:14,700 --> 00:29:20,370
Right. And but my specificity is going to be terrific.

257
00:29:20,370 --> 00:29:25,320
I got nearly everybody. If you're just if you're don't have the condition.

258
00:29:26,510 --> 00:29:34,220
Your probability of testing negative is great. You know, it doesn't even show up as a line going out that far in this little plot.

259
00:29:38,240 --> 00:29:43,500
So moving. So the general pattern is if you move the threshold, one of them is going to get better.

260
00:29:43,520 --> 00:29:50,700
One of them is getting a get worse. All right.

261
00:29:50,700 --> 00:29:56,279
So each Tercel has a false positive fraction for one minus specificity and a sensitivity.

262
00:29:56,280 --> 00:30:05,820
So these are just for the same thresholds in the plot. These are the actual numerical values of the sensitivity and one minus the specificity here.

263
00:30:07,430 --> 00:30:15,259
And we can tabulate the false positive fraction and the sensitivity of all threshold values, not just the two that we focused on.

264
00:30:15,260 --> 00:30:22,940
In that last slide, we can look at all possible thresholds from left to right and plot them.

265
00:30:23,570 --> 00:30:27,470
And that plot is the receiver operating characteristic curve.

266
00:30:29,490 --> 00:30:35,850
So here is just, you know, this is not actually a real RC curve.

267
00:30:35,850 --> 00:30:42,420
I just made a curve where I've got two values that correspond to the sensitivity specificity I had earlier.

268
00:30:42,930 --> 00:30:47,010
So this is threshold one's spot on the ROIC curve.

269
00:30:48,600 --> 00:30:58,079
Right. So they had very good sensitivity, but it specificity was in the middle so far as false positive fraction was also right at 50%.

270
00:30:58,080 --> 00:31:02,610
So that's threshold one. So it only gives you one dot on this plot.

271
00:31:04,510 --> 00:31:14,500
And this is the one that goes with threshold to the threshold to had much poorer sensitivity, but its specificity was much better.

272
00:31:14,530 --> 00:31:18,700
So one minus the specificity is what's on this horizontal axis.

273
00:31:18,700 --> 00:31:22,750
So it's kind of on the low end for false positive reactions.

274
00:31:24,740 --> 00:31:32,750
So. And so. Any possible threshold, we'll give you one dot on this plot.

275
00:31:34,390 --> 00:31:43,299
And there's not really an indication on the plot itself of what was the biomarker value that gave you these.

276
00:31:43,300 --> 00:31:51,180
That's like a third dimension you don't see in the plot directly. So you know that each one of the dots is a threshold, but which threshold?

277
00:31:51,180 --> 00:31:55,280
You have to. Go back to the data and some way to look at that.

278
00:31:57,020 --> 00:32:06,170
So the best possible scenario is that you have a 100% sensitivity and 100% specificity.

279
00:32:06,950 --> 00:32:13,160
That would be like if there's no overlap in those distribution curves at all, you could totally separate them with a threshold through the middle.

280
00:32:14,180 --> 00:32:20,240
And that would put you right here. The best is this upper left corner.

281
00:32:20,660 --> 00:32:29,690
And so for a biomarker, if you're trying to compare different biomarkers for classification, you want the the biomarker that gives you the curve.

282
00:32:29,690 --> 00:32:34,850
That's her closest to that top left corner.

283
00:32:36,410 --> 00:32:41,870
Or alternatively, you know, has the highest area under the curve.

284
00:32:43,670 --> 00:32:51,020
All right. So as both of them are proving in the curves, kind of getting pushed over towards the perfect system,

285
00:32:51,770 --> 00:32:56,929
the area under the ROIC curve is going to be one like the height times.

286
00:32:56,930 --> 00:33:05,360
The width will be one. And there's also this kind of dash line that appears on every RC curves, the dash line.

287
00:33:06,680 --> 00:33:11,090
As you know, what would happen if your test was completely useless?

288
00:33:12,370 --> 00:33:20,170
You know, so it doesn't help you discriminate between people who are, you know, truly have the disease and don't at all.

289
00:33:21,040 --> 00:33:25,450
And so that's like area under the curve of point five.

290
00:33:27,220 --> 00:33:34,299
And I've actually seen situations where you can get a data set that will give you area under the curve less than point five.

291
00:33:34,300 --> 00:33:40,030
You know, like you're you're actually doing worse than rates. I'm a little bit with that diagnostic measure,

292
00:33:41,470 --> 00:33:52,480
but most of the data that you'll see will be somewhere between this dashed line and the perfect kind of boxy area equals one.

293
00:33:57,520 --> 00:34:02,409
So the performance of the continuous diagnostic test is summarized by this area under the RNC curve.

294
00:34:02,410 --> 00:34:10,030
And so if you're looking at a lot of different biomarkers and trying to figure out how to classify a new condition or an existing condition,

295
00:34:10,630 --> 00:34:17,170
you know, one quick way to see how well the biomarker is going to do generally is to look at this area under the curve.

296
00:34:17,170 --> 00:34:24,970
And then once you figure out which biomarker you want to focus on, then you come up with your threshold that you're going to use to diagnose people.

297
00:34:28,400 --> 00:34:38,299
So most of the time, end of the curve between point five and 1.5 being kind of a useless test and one being a perfect test.

298
00:34:38,300 --> 00:34:44,120
We don't actually datasets can give you value slightly under this just randomly doing worse.

299
00:34:45,300 --> 00:34:51,990
But you know, if you're doing worse than 0.5, you're going to basically say never do this biomarker for this condition.

300
00:34:52,530 --> 00:34:54,929
So that's that area under the curve, 8.5,

301
00:34:54,930 --> 00:35:02,460
as demonstrated by that diagonal dotted line and indicates useless tests of disease and non disease are not effectively separated into groups.

302
00:35:03,800 --> 00:35:10,010
An area under curve of one is a perfect test where sensitivity is one for all values of specificity.

303
00:35:11,980 --> 00:35:15,250
So in this latter case with area undercover one,

304
00:35:15,250 --> 00:35:20,260
there's a threshold with both perfect sensitivity and specificity that completely separates deceased

305
00:35:20,260 --> 00:35:25,989
individuals on one side of the threshold and non disease individuals on the other side of the threshold.

306
00:35:25,990 --> 00:35:32,920
So again, just looking at the distribution of the biomarkers in disease and not disease, there's there's no overlap in that situation.

307
00:35:36,670 --> 00:35:46,959
So the previous example that I was using to teach you about your end of the curve, how to earn the curve a point nine, which is very good, actually.

308
00:35:46,960 --> 00:35:51,190
I'd be very excited if I had a biomarker that gave me that area under the curve.

309
00:35:54,980 --> 00:36:01,820
All right. So all of this these previous slides have kind of presume that you have a gold standard, that, you know,

310
00:36:01,820 --> 00:36:07,010
the true diagnose you're able to determine true diagnosis so that you can say,

311
00:36:07,010 --> 00:36:11,989
given they have that true diagnosis, what's the probability they test positive or negative.

312
00:36:11,990 --> 00:36:17,720
Right. And so you definitely need that that gold standard.

313
00:36:18,290 --> 00:36:24,710
And then, you know, you might have several different diagnostic tests that are in use,

314
00:36:24,980 --> 00:36:28,520
some that are for screening and some that are more invasive, that are more deterministic.

315
00:36:29,150 --> 00:36:32,270
So what are some of the reasons to develop a new diagnostic test?

316
00:36:32,270 --> 00:36:38,719
So you might want to replace a very accurate but very expensive test with a cheaper

317
00:36:38,720 --> 00:36:42,800
test that's nearly as accurate so that you can put it in a more common use.

318
00:36:44,190 --> 00:36:51,180
Or you might want to replace a very accurate yet very invasive tests with a less invasive alternative that's nearly as accurate.

319
00:36:52,820 --> 00:36:59,060
Or you want to replace the currently accepted your imperfect test with a hopefully more accurate test.

320
00:36:59,960 --> 00:37:08,000
And in this last case, you you know, how do you know for certain that a subject disease at the currently accepted test is imperfect?

321
00:37:08,000 --> 00:37:12,319
So that's kind of, you know, area under the curve,

322
00:37:12,320 --> 00:37:18,260
kind of rosy curve analysis are only as good as the the truth of the diagnosis

323
00:37:18,260 --> 00:37:22,280
is known if you if it's very fuzzy about whether they had the condition or not.

324
00:37:22,700 --> 00:37:27,830
RC Curves are going to be a little bit problematic to define well, right?

325
00:37:31,300 --> 00:37:33,190
And actually, I should have a fourth dot here.

326
00:37:35,260 --> 00:37:40,210
Remember, early in the pandemic, when if you got a test, you had to wait for two weeks to get the result.

327
00:37:41,230 --> 00:37:50,540
Yeah. So you might want to have a test that comes back with a result in 2 hours or 15 minutes instead of two weeks.

328
00:37:50,540 --> 00:37:54,280
So that would be another reason to come up with a new diagnostic test.

329
00:37:58,550 --> 00:38:04,010
So it's hard to determine the sensitivity and specificity of a new diagnostic test if no gold standard exists.

330
00:38:04,760 --> 00:38:09,229
And I, I work very often with pulmonologists.

331
00:38:09,230 --> 00:38:14,630
And one of the diseases that they have a hard time defining clearly is interstitial pulmonary fibrosis.

332
00:38:15,470 --> 00:38:23,220
And so to come up with a gold standard for that diagnosis, they will pull experts from around the country into a room.

333
00:38:23,930 --> 00:38:30,739
They'll look at all the information for cases, and they will come to a consensus for each person individually.

334
00:38:30,740 --> 00:38:37,010
And that will be the gold standard that is used for that data set.

335
00:38:37,640 --> 00:38:46,070
So there can be disagreement between experts and they just have to, you know, wrestle down a common diagnosis.

336
00:38:51,510 --> 00:38:58,590
All right. So everything we've done so far. I hope that this is not really going to be a recording issue.

337
00:39:00,720 --> 00:39:04,800
So everything we've done so far is kind of comparing sensitivity, specificity.

338
00:39:05,550 --> 00:39:14,190
But what we haven't really asked is, you know, for these different diagnostics, you know, suppose they had the same sensitivity and specificity.

339
00:39:14,760 --> 00:39:18,120
Are they really catching are they really calling the same people?

340
00:39:19,030 --> 00:39:25,840
Disease or non disease. So you can have the same sensitivity and specificity characteristics of tests.

341
00:39:26,230 --> 00:39:31,870
But do those two tests classify the same people into disease or not disease?

342
00:39:31,870 --> 00:39:39,580
And so it's a bit of a related but not quite the same statistical question.

343
00:39:39,910 --> 00:39:49,090
Right. And so we need to have statistics that show us how to measure agreement between two different diagnostic tests.

344
00:39:49,660 --> 00:39:55,420
So instead of determining if one test is better than another, looking at our A C curve, an area under the curve,

345
00:39:55,720 --> 00:40:01,690
we may just wish to determine how similar the two tests are or how often they agree with one another.

346
00:40:02,410 --> 00:40:07,270
And that can be very useful when comparing to reviewers of the same test result.

347
00:40:07,330 --> 00:40:13,209
So again, in this interstitial pulmonary fibrosis example, you know, what's the agreement?

348
00:40:13,210 --> 00:40:21,490
If a radiologist is looking at the data as opposed to a pathologist, you know, how often do they agree on their diagnosis?

349
00:40:21,490 --> 00:40:30,300
This is kind of useful information to have. Or a medical resident who's just starting off versus a radiologist or human eye versus computer.

350
00:40:30,340 --> 00:40:43,410
So for all the sexy new articles that are coming out about machine learning, you know, how often does that diagnosis agree with the standard?

351
00:40:45,470 --> 00:40:50,540
So I have the same notation for you about your table, but the rows in the columns of this table have changed.

352
00:40:50,550 --> 00:40:55,010
So the columns are one method of doing the diagnosis.

353
00:40:55,430 --> 00:41:01,999
The rows are another method of doing the diagnosis, and they can either both say they have the disease,

354
00:41:02,000 --> 00:41:07,670
so that's the positive ASO or they can both say they don't have the disease.

355
00:41:07,670 --> 00:41:14,180
That's the de cell or they can disagree. The B and C cells is where they had some disagreement with one another.

356
00:41:17,870 --> 00:41:24,920
So if I want to have the probability that the the two raters are agreeing with one another,

357
00:41:25,340 --> 00:41:30,320
I'm summing up the A and the D cells over the total number of people that were looked at.

358
00:41:31,460 --> 00:41:33,260
This is like the probability of agreement.

359
00:41:36,380 --> 00:41:42,500
So I mean, and in any situation, any type of like this, there's some random chance that they're going to agree.

360
00:41:43,370 --> 00:41:48,439
You know, so how do we know if that probability of agreement that using your data is better than

361
00:41:48,440 --> 00:41:53,690
what would have been seen if we had two methods that had no relation to one another?

362
00:41:56,590 --> 00:42:05,020
So if the two methods were completely unrelated, then what we would expect to find is a table that looks like this.

363
00:42:05,140 --> 00:42:08,680
So there's no associate. If there's no association between rows and columns,

364
00:42:09,610 --> 00:42:14,770
then we've been training ourselves to look at expected cell counts, kind of under the null hypothesis.

365
00:42:15,430 --> 00:42:22,750
So each one of these entries is the row total times, the column total over that, the table total of the number of people you're looking at.

366
00:42:25,670 --> 00:42:37,720
And so this p e is looking at the percentage of subjects where the the two raters would agree by chance alone.

367
00:42:37,730 --> 00:42:41,720
So this SL expected value plus this.

368
00:42:44,500 --> 00:42:51,970
De sell expected value and then dividing by end so we get a probability instead of account.

369
00:42:54,630 --> 00:42:58,080
So please think of the E subscript as like expected.

370
00:43:00,650 --> 00:43:07,700
And so the amount of agreement between the two methods is summarized with something called Cohen's cap and statistic.

371
00:43:08,270 --> 00:43:15,800
And it's basically the difference between the observed probability of agreement minus the amount of agreement that you would expect by chance.

372
00:43:16,700 --> 00:43:24,680
And then it's scaled by that one minus that same probability that you would expect to have seen the agreement by chance.

373
00:43:25,400 --> 00:43:30,290
And so this is if you've seen campus statistics and journal articles, this is where that comes from.

374
00:43:32,100 --> 00:43:40,150
And there's an article that sort of suggests wording for how much agreement it means when you see these numbers.

375
00:43:40,170 --> 00:43:46,640
So. You can see cap in statistics, you can actually even see negative statistics.

376
00:43:46,650 --> 00:43:56,720
This is where there is extremely poor agreement to the fact to the point where it's more disagreement than you will expect by chance.

377
00:43:57,470 --> 00:44:03,440
So whenever I see a negative cap, I mean, I actually think of my in-laws.

378
00:44:04,220 --> 00:44:07,549
If one of them says black, the other will say white.

379
00:44:07,550 --> 00:44:13,850
If one of them says yes, the other will say no. So negative cap is almost like disagreeing on purpose.

380
00:44:15,000 --> 00:44:22,650
But otherwise, these adjectives are kind of telling you how much agreement there is between the two different raters.

381
00:44:23,280 --> 00:44:29,370
And just in my practice, I've almost never seen this cap a .821 that's very rare.

382
00:44:29,730 --> 00:44:34,590
Usually see somewhere in the fair market moderate or substantial agreement range.

383
00:44:35,400 --> 00:44:39,690
And I think later on we'll have an example where we will see moderate agreement.

384
00:44:43,480 --> 00:44:47,170
So there are hypothesis tests that go along with Cohen's Kappa statistic.

385
00:44:47,950 --> 00:44:52,000
And we'll get that from software. Their details here if you want to do it by hand.

386
00:44:52,810 --> 00:45:01,840
But we'll be getting these from software. And so the null hypothesis is that the cap statistic is zero versus not zero.

387
00:45:04,280 --> 00:45:08,120
And just so you have an example that you can match to that to later.

388
00:45:08,810 --> 00:45:12,290
Suppose you have two radiologists that are reading the same 100 mammograms,

389
00:45:13,130 --> 00:45:18,830
and each radiologists records the presence or absence of breast cancer in each mammogram, giving the following results.

390
00:45:18,830 --> 00:45:23,270
So here's radiologist. One would know that there was breast cancer.

391
00:45:23,270 --> 00:45:26,419
Yes. No. Radiologist two Breast cancer?

392
00:45:26,420 --> 00:45:34,640
Yes. No. So they both agreed that there was cancer in 25 cases of 25% of the time.

393
00:45:35,270 --> 00:45:41,510
They both agreed cancer was absent 60% of the time, but they disagreed 15% of the time.

394
00:45:46,020 --> 00:45:54,660
So using the formula from the previous slides, the observed Kappa statistic was 85%, so they mostly agreed.

395
00:45:55,500 --> 00:45:58,710
That was what we called like almost perfect, a perfect agreement.

396
00:46:00,170 --> 00:46:03,920
Uh, the. Oh, sorry. That's just the observe the Kappa statistics, actually.

397
00:46:03,920 --> 00:46:11,420
Moderate 0.66. That's where the adjectives were coming in, interpreting this statistics that this was actually moderate agreement.

398
00:46:12,750 --> 00:46:19,110
So observed agreement 85% of the time. But just by chance, they should have been agreeing 56% of the time.

399
00:46:19,410 --> 00:46:27,230
That's where the moderate came from. When you calculated the Kappa and it was statistically significantly different from zero.

400
00:46:27,240 --> 00:46:38,010
So I don't know how useful this P value is in general because it's very easy to reject a campus statistic being different from zero.

401
00:46:38,460 --> 00:46:43,050
So actually most of the time looking at the statistic and confidence interval is going to be

402
00:46:43,050 --> 00:46:49,260
more useful than a p value just to see how precise this estimate of moderate agreement is.

403
00:46:51,610 --> 00:47:01,320
So I'm going to get into the soft wear now. And because that's kind of a mental break between concept and programing,

404
00:47:01,330 --> 00:47:08,020
the rest of the handout is going to be a lot of a lot of programing, some concept, too, but more programing.

405
00:47:08,020 --> 00:47:13,240
So let's take a ten minute break and we'll meet at 857.

406
00:47:45,700 --> 00:48:00,240
Okay. Oh, that's.

407
00:49:46,460 --> 00:50:01,590
The reason why. Yes.

408
00:51:08,110 --> 00:52:13,430
Know. But.

409
00:52:17,720 --> 00:53:12,110
Now. I was.

410
00:53:32,720 --> 00:53:37,980
But. Every single.

411
00:53:40,490 --> 00:54:42,080
How? Okay.

412
00:54:42,090 --> 00:54:47,420
Are you ready to get back to work? All right.

413
00:54:48,740 --> 00:54:52,520
Ready or not? Okay.

414
00:54:52,520 --> 00:54:57,420
So again, in this course, I'm trying to show you both sass and our code.

415
00:54:57,440 --> 00:55:05,780
It doesn't matter to me which one you use, and I'll try to show my preference one way or another, depending on the situation.

416
00:55:05,810 --> 00:55:11,270
They both have good features. So in this case, for Kappa statistics, here's the SAS code.

417
00:55:11,270 --> 00:55:18,440
It's part of Freq. And so in this first bit of code here, I'm just putting the data in.

418
00:55:18,440 --> 00:55:24,080
So this is for the mammogram dataset where there were 100 cases that were being looked at by two radiologists.

419
00:55:24,530 --> 00:55:34,070
So red one and red two, both character vectors here with the results of whether they said the cancer was present or absent and the counts.

420
00:55:35,180 --> 00:55:46,820
And so to get the Kappa statistic you use proc freq and with the weight statement giving the cell counts, that's the fruit here.

421
00:55:47,600 --> 00:55:53,210
Tables command is something with radiologist for one versus radiologist two they're putting in.

422
00:55:53,600 --> 00:55:59,210
And the cap statistic is really kind of coming from this agreed syntax here.

423
00:56:01,460 --> 00:56:08,000
So that'll give you the statistics. And then to get the P values, you would use the option to test Kappa.

424
00:56:10,240 --> 00:56:13,470
And so what comes? Out of that.

425
00:56:13,530 --> 00:56:18,920
So, yeah, so this is giving you the 95% confidence interval as well as the estimated CapEx statistic.

426
00:56:21,070 --> 00:56:24,220
Versus just getting the P value for the Kappa statistic.

427
00:56:24,230 --> 00:56:30,400
So what comes out of this is, you know, first showing you the data that you put into double check that it is what you think it is.

428
00:56:32,820 --> 00:56:37,230
And then Coen's cap. A statistic is here.

429
00:56:37,650 --> 00:56:40,710
This is exactly what we got by the hand calculations.

430
00:56:41,280 --> 00:56:46,079
And here are also 95% confidence intervals. So that's going to be the most useful.

431
00:56:46,080 --> 00:56:54,680
You can sort of see that, you know, there's decent information here, but there's still quite a bit of a spread in the confidence limits here.

432
00:56:54,690 --> 00:56:59,339
So, you know, and then the p value is down here.

433
00:56:59,340 --> 00:57:03,660
So it's very highly, significantly different from a Kappa of zero.

434
00:57:04,260 --> 00:57:09,690
But I will say that's a very easy hurdle to reach statistical significance for.

435
00:57:09,690 --> 00:57:16,290
So this is probably the best way to present the data to include the confidence interval, not only the p value.

436
00:57:18,470 --> 00:57:23,150
We'll have an example with sentences later for manuscript worthy sentences.

437
00:57:23,630 --> 00:57:31,490
So here is a snapshot of our studio and how the code goes.

438
00:57:31,500 --> 00:57:37,280
There's actually in the footnote for this very slide. There's our code that you can copy and paste if you're going to edit it yourself.

439
00:57:38,000 --> 00:57:41,090
But this is just a snapshot so you can see everything at once in our studio.

440
00:57:41,510 --> 00:57:44,780
And so here is reading in the same matrix of data.

441
00:57:45,560 --> 00:57:50,060
And so again, the C notation here is just saying,

442
00:57:50,060 --> 00:58:00,770
I have a list of numbers coming and then this number of columns to in biro equals true is saying that I'm putting it in as a, b, c, d.

443
00:58:03,520 --> 00:58:11,009
And Jim name is giving labels. So I'm calling radiologist one is C present absence.

444
00:58:11,010 --> 00:58:14,670
So I'm just giving labels of president absent for radiologist one and two.

445
00:58:14,710 --> 00:58:22,840
So it'll look just like what we had in earlier in the slides and ad margins does what it sounds like.

446
00:58:22,840 --> 00:58:27,610
It's basically putting the row totals and column totals all there for you.

447
00:58:27,610 --> 00:58:34,870
So when you look at the data, after you do the add margins, this is the dataset that we've been looking at this whole time.

448
00:58:34,870 --> 00:58:40,509
So I haven't done anything with Kappa statistic. This is just putting in the data in R with labels.

449
00:58:40,510 --> 00:58:50,649
All nice. And so to get the cap a statistic, we're going to install this package called CYC, which it's very big.

450
00:58:50,650 --> 00:58:58,780
So it takes a long time to be patient. Once you install, once loading it with this library command is much quicker.

451
00:58:59,200 --> 00:59:01,300
You only need to install at one time.

452
00:59:03,700 --> 00:59:11,080
And then the command for getting the Kappa statistic is coincident Kappa with that same data set that we already put in,

453
00:59:12,700 --> 00:59:17,200
and the output will look like this. So output always shows up.

454
00:59:18,040 --> 00:59:25,540
You know, you kind of type in the R code into this window up top and then you kind of run the our code.

455
00:59:27,100 --> 00:59:31,180
Uh. In in lab there.

456
00:59:31,450 --> 00:59:36,040
Give you examples of how to do that live rather than on a snapshot.

457
00:59:36,670 --> 00:59:42,820
And then the output will show up down here. So it'll kind of repeat the command that you ran and the output right underneath.

458
00:59:45,010 --> 00:59:50,659
And so here is where Cohen's Kappa statistic and the estimated 95% confidence interval is.

459
00:59:50,660 --> 00:59:56,230
So the estimate is kind of in the middle and it gives the lower in the upper confidence limits here.

460
00:59:59,790 --> 01:00:04,050
And to get the p value you need yet another package installed.

461
01:00:04,080 --> 01:00:14,240
So I'm installing this package called epi tools and loading it with the library command and then looking at just, you know,

462
01:00:14,370 --> 01:00:26,070
whatever this this table is, I needed to actually expand it with expand a table to have it be useful in the Cappa command that comes down later.

463
01:00:26,610 --> 01:00:34,050
So expand table I if I recall correctly, it's making one row per patient from the two by two table.

464
01:00:35,710 --> 01:00:38,110
And then installing and load this package.

465
01:00:38,180 --> 01:00:46,300
Ah, and then finally you can use this capture to statistic and all of that work mainly just to give you this p value.

466
01:00:53,710 --> 01:00:59,230
All right. So we had earlier talked about Rosie curves.

467
01:00:59,230 --> 01:01:05,080
And so I wanted to give you some code, an example for the RC curves as well in SAS.

468
01:01:05,080 --> 01:01:10,180
And and so I'm going to be looking at this data set on melanoma for that.

469
01:01:10,180 --> 01:01:21,930
So it's a data set from an article in Biometrics from 1996, and there are 72 patients with their melanoma status confirmed via biopsies.

470
01:01:21,940 --> 01:01:31,570
That's the gold standard of whether they really had it or not. And in that data set, 21 actually had melanoma and 51 did not.

471
01:01:34,750 --> 01:01:39,970
And each patient had a suspicious lesion scored with one of two methods.

472
01:01:40,900 --> 01:01:49,180
Actually, two methods. Both methods. So the first was a visual assessment and the second was an assessment that was done with the derma scope.

473
01:01:51,260 --> 01:01:56,420
And so each of these is going to give you a continuous random variable.

474
01:01:56,510 --> 01:02:02,059
So there's two continuous kind of biomarkers here that we're we're looking at and we

475
01:02:02,060 --> 01:02:07,700
want to assess the utility of each of these separately in terms of diagnosing melanoma.

476
01:02:09,880 --> 01:02:14,620
So Fritz says it looks like this.

477
01:02:15,250 --> 01:02:19,630
So this is as reading in the data. And.

478
01:02:20,640 --> 01:02:24,180
The data lines are on canvas.

479
01:02:24,570 --> 01:02:31,260
This is kind of with data emitted just to put on the slide, but the code with all of the data is on canvas.

480
01:02:32,080 --> 01:02:35,110
And so we have ideas.

481
01:02:35,110 --> 01:02:38,770
The first column, their true disease status.

482
01:02:39,280 --> 01:02:43,510
I called these so it's one if they had melanoma and zero.

483
01:02:43,540 --> 01:02:49,180
Otherwise by the biopsy and then t one continuous and t two continuous.

484
01:02:50,200 --> 01:02:53,259
I chose t one and t two to be like, test one.

485
01:02:53,260 --> 01:02:57,100
Test two. And their values are here.

486
01:02:58,810 --> 01:03:07,510
So I think I wish I could remember clearly, but I think t one continuous was visual and t to continuous was derma scope.

487
01:03:08,380 --> 01:03:12,040
So those are the ones that we're going to be comparing.

488
01:03:12,520 --> 01:03:16,570
And then just to give us one threshold to consider.

489
01:03:17,780 --> 01:03:22,250
I looked at positive values for each of those biomarkers.

490
01:03:22,290 --> 01:03:26,630
You can see they have either, you know, sometimes it's negative, sometimes it's positive.

491
01:03:27,530 --> 01:03:36,860
So I just made up a10 variable. So t one is going to be a one whenever the inside of this parentheses is true and zero otherwise.

492
01:03:37,550 --> 01:03:44,660
So it'll be a one whenever the t one biomarker, the visual score was was positive and zero otherwise.

493
01:03:44,960 --> 01:03:48,070
I don't know if you've seen this syntax before, but that's basically what's happening here.

494
01:03:48,080 --> 01:03:56,750
Whenever you have parentheses and some logical expression in the middle, it's kind of like a shortcut for if then and creating this binary variable.

495
01:03:57,530 --> 01:04:00,590
So it'll be one whenever the inside is true and zero otherwise.

496
01:04:01,040 --> 01:04:11,030
So t two is also going to be a10 variable where it's one when the T to the dermis go score was positive and zero otherwise.

497
01:04:11,930 --> 01:04:16,310
So it's just nice shorthand for creating these binary variables, which is something we do a lot.

498
01:04:18,200 --> 01:04:23,570
So this is just getting the data in and one possible kind of arbitrary threshold that I just picked because,

499
01:04:23,780 --> 01:04:27,170
you know, zero seemed like an easy choice to look at.

500
01:04:30,430 --> 01:04:37,090
All right. And then I'm going to look at the first ten observations just to, you know, make sure I know what that.

501
01:04:38,390 --> 01:04:44,470
Looks like. And so this is what that looks like.

502
01:04:44,480 --> 01:04:48,020
So here are just these are the continuous measures, right?

503
01:04:48,020 --> 01:04:57,080
The visual and the dermis go and my version of what the diagnosis is going to be using my threshold of zero.

504
01:04:58,030 --> 01:05:01,150
So no melanoma and they agree. No melanoma.

505
01:05:01,160 --> 01:05:05,540
They agree. They both say melanoma here, but sometimes they don't agree.

506
01:05:05,560 --> 01:05:10,480
So here's person eight. The visual said no.

507
01:05:10,480 --> 01:05:28,280
The dermal scope said yes. Melanoma. And so if I wanted to look at sensitivity and specificity for a single threshold value, it's proc freak again.

508
01:05:28,280 --> 01:05:35,479
So we're looking at the melanoma data that we've read in and we're doing tables by the true disease status.

509
01:05:35,480 --> 01:05:39,380
So this is shorthand for t one by the T two by the.

510
01:05:41,030 --> 01:05:47,059
And there's this option for sensitivity and specificity that staff has here.

511
01:05:47,060 --> 01:05:56,450
And I just want to caution you, I'm showing you the results. Just so you know, if you're curious what this does, but I want to caution you on its use,

512
01:05:56,450 --> 01:06:04,370
because it doesn't really know if you're really using high values to call something a disease or low values.

513
01:06:04,370 --> 01:06:13,909
So it'll pick labels of sensitivity and specificity, but it might be switched from what it would be if you really calculated it by hand yourself.

514
01:06:13,910 --> 01:06:19,930
So I want to just warn you about that. All right.

515
01:06:21,930 --> 01:06:26,920
And so this is just looking at the tables again to make sure we got them right.

516
01:06:26,940 --> 01:06:39,749
So for my threshold of positive values being called melanoma test positive and negative values being not, this is what the tables look like.

517
01:06:39,750 --> 01:06:44,700
So we should be able to calculate sensitivity and specificity from these tables ourselves,

518
01:06:45,430 --> 01:06:50,790
you know, because we know sensitivity is given, they really have the condition.

519
01:06:51,600 --> 01:06:55,230
So that's this column here where D is equal to one. There's 21 of them.

520
01:06:56,250 --> 01:06:59,520
How many people actually had the positive test?

521
01:06:59,530 --> 01:07:04,679
So 13 out of 21 is the 61.9.

522
01:07:04,680 --> 01:07:12,120
So the sensitivity for this first biomarker, the visual is 61.9%.

523
01:07:13,290 --> 01:07:18,299
And for the dermis group, they classified 14.

524
01:07:18,300 --> 01:07:26,820
So one more person as having melanoma, I think we even saw them on our listing right out of the 21.

525
01:07:26,820 --> 01:07:30,060
So their sensitivity. 66.7.

526
01:07:34,380 --> 01:07:38,510
Right. So there's sensitivity for test one.

527
01:07:38,600 --> 01:07:40,070
Sensitivity for test two.

528
01:07:42,390 --> 01:07:50,129
And then if we do the specificity, we know that we for specificity, we need to look at everybody who truly doesn't have the conditions.

529
01:07:50,130 --> 01:08:01,709
That's 51 people. And for test 146 were correctly classified as not having the condition based on the visual assessment.

530
01:08:01,710 --> 01:08:05,960
So that's 90.2% specificity.

531
01:08:07,850 --> 01:08:14,790
And actually the same number. For the dermis, the dermis cope.

532
01:08:14,790 --> 01:08:22,020
So they both for this arbitrary threshold that I just made up of zero, their specificity was 90.2%.

533
01:08:24,460 --> 01:08:31,870
So here's what the census best output looks like. This is for the visual score and the term scope with those thresholds of zero.

534
01:08:32,740 --> 01:08:36,310
And so first off.

535
01:08:38,580 --> 01:08:43,920
We need to ignore everything about positive and negative predictive value that that is put out here.

536
01:08:43,950 --> 01:08:52,979
This is assuming that you have a cohort study and in our situation they oversampled melanoma cases.

537
01:08:52,980 --> 01:08:58,710
There's certainly not that higher percentage of melanoma cases in the general public.

538
01:08:58,720 --> 01:09:05,520
So there's positive and negative predictive value that those ROEs need to be ignored completely.

539
01:09:06,330 --> 01:09:09,659
But also I want you to look at these sensitivity and specificity.

540
01:09:09,660 --> 01:09:23,760
ROSE So they actually switched them and it's actually the specificity that we saw that was 90.2% and the sensitivity was 0.619.

541
01:09:24,330 --> 01:09:27,989
So the confidence limits are fine, but the labels are entirely switched.

542
01:09:27,990 --> 01:09:35,750
So if you are. Trying to just read this table that says spits out you need to know.

543
01:09:35,750 --> 01:09:43,630
It doesn't really know which row is sensitivity and specificity, but it will act like it knows which row is sensitivity and specificity.

544
01:09:43,640 --> 01:09:48,290
So you have to know from context and looking at the two by two table.

545
01:09:49,130 --> 01:09:52,610
You know what what the sensitivity and specificity are.

546
01:09:52,790 --> 01:09:57,470
Once you've figured that out, there is going to be a confidence limit that you can use.

547
01:09:58,190 --> 01:10:02,360
But you have to know which row is which SAS doesn't know.

548
01:10:04,140 --> 01:10:07,140
They'll tell me. It's a little irritating to acts like it does.

549
01:10:08,770 --> 01:10:15,370
And again, just don't look at positive or negative predictive values unless your data set really does come from a cohort study.

550
01:10:16,360 --> 01:10:32,700
And that was not the case here. So just because it's sitting right there, the campus statistic for the, you know, the two methods.

551
01:10:33,030 --> 01:10:38,189
It would be measuring agreement. So they both have kind of similar sensitivity and specificity.

552
01:10:38,190 --> 01:10:43,739
So the question here is, you know, are they diagnosing this the same people most of the time.

553
01:10:43,740 --> 01:10:56,350
So what's the agreement? And when you look at ah the, the t one by two data they don't.

554
01:10:57,630 --> 01:11:03,150
Necessarily read all the time. So they both say it's not melanoma.

555
01:11:04,200 --> 01:11:07,379
47 times out of 72 and they say it is melanoma.

556
01:11:07,380 --> 01:11:12,510
12 times that is 72. But they have a fair amount here of disagreement as well.

557
01:11:12,990 --> 01:11:17,370
So the cap a statistic is .5273.

558
01:11:17,520 --> 01:11:19,040
So even though they have similar,

559
01:11:19,050 --> 01:11:26,790
both of these diagnostics have similar sensitivity and specificity to one another really for that threshold that we looked at.

560
01:11:27,360 --> 01:11:31,520
But the cap, a statistic for that threshold is just still on the moderate side.

561
01:11:33,240 --> 01:11:35,950
Again, the P-value is almost always significant for these.

562
01:11:36,540 --> 01:11:43,470
If there's any utility to the biomarker at all, here's the captain's statistic with the confidence limits, the pretty big.

563
01:11:48,640 --> 01:11:57,690
The P value. So, you know, is there a better threshold than the zero that I just pull out of a hat because it was zero?

564
01:11:58,530 --> 01:12:01,410
And so this is where we're going to use RC curves.

565
01:12:04,710 --> 01:12:11,550
So SAS will automatically calculate sensitivity and specificity for all possible thresholds and they do this through proc logistic.

566
01:12:11,550 --> 01:12:14,220
So we haven't reviewed logistic regression yet.

567
01:12:14,220 --> 01:12:22,590
We will very quickly do it in the upcoming handout, but you're going to have to use it for doing RC curves.

568
01:12:22,590 --> 01:12:24,630
So we're going to have a little bit of that code today.

569
01:12:28,300 --> 01:12:36,160
So I'm reading in the melanoma data again, I'm asking for it to do some some plots or rosy plots.

570
01:12:36,970 --> 01:12:39,960
And here is my model statement for proc logistic.

571
01:12:39,970 --> 01:12:47,500
I have both of the measures in there, the continuous biomarker for the visual score and the derma scope score.

572
01:12:47,920 --> 01:12:53,409
But I'm not really interested in looking at the model output.

573
01:12:53,410 --> 01:12:56,770
I'm not really interested in the literature, the odds ratios or anything for this.

574
01:12:57,190 --> 01:13:02,950
I'm mainly doing this for the RC curve analysis. So that's where this no fit is coming from.

575
01:13:03,610 --> 01:13:07,010
And I'm going to save the results into this data.

576
01:13:07,060 --> 01:13:11,150
Set our RC. So.

577
01:13:12,910 --> 01:13:16,210
So don't fit this logistic regression model. That's what the no fit is.

578
01:13:16,930 --> 01:13:21,909
And instead I want to model the outcome within separate RC statements.

579
01:13:21,910 --> 01:13:28,450
So here's going to be an RC curve. Just using t one here is going to be an RC curve just using t two.

580
01:13:29,660 --> 01:13:38,700
And. Saving all the results for all the possible thresholds in the dataset called ROIC.

581
01:13:41,730 --> 01:13:45,840
And I think I mentioned this earlier. So the plots come in and requests are O.C. curves.

582
01:13:46,260 --> 01:13:53,190
And the part of the parenthesis says that, you know, for the fruit of the points on the plot,

583
01:13:53,640 --> 01:14:01,080
give the observation number in the melanoma data so that I can look at the threshold used to generate that sensitivity and specificity.

584
01:14:04,850 --> 01:14:11,990
And then this RC contrast line is requesting a test statistic and a P, you know, with the p value.

585
01:14:13,100 --> 01:14:18,430
To test whether the area under the curve is equivalent for each of those to measure.

586
01:14:18,450 --> 01:14:24,410
So this is actually comparing area under the growth curve for visual score versus the derma scope.

587
01:14:27,310 --> 01:14:32,410
And I'm going to look at the data for the first five observations so that we can look

588
01:14:33,040 --> 01:14:38,590
at the sensitivity specificity results just to see what that data set ROIC looks like.

589
01:14:41,440 --> 01:14:46,540
So here is the arrows, the curve for the visual score.

590
01:14:46,540 --> 01:14:50,770
And the area under the curve is 0.9057.

591
01:14:50,780 --> 01:14:55,750
That's pretty good. I mean, this really looks pretty good, right? It's very close to this perfect one.

592
01:14:58,100 --> 01:15:03,320
And so, you know, there are various thresholds here that they're showing.

593
01:15:03,440 --> 01:15:06,499
We only looked at one of them. We looked at the threshold of zero.

594
01:15:06,500 --> 01:15:13,220
So, you know, there's no place where I can obviously see where the threshold of zero is.

595
01:15:14,280 --> 01:15:20,160
Right. Because only shows me that the vertical axis, the sensitivity, the the horizontal axis is one minus specificity.

596
01:15:20,940 --> 01:15:24,810
There's no threshold shown directly.

597
01:15:25,380 --> 01:15:32,430
But since we already did the calculations for sensitivity and specificity for that zero threshold, we can find it just by.

598
01:15:33,360 --> 01:15:35,969
Knowing what it was for that threshold.

599
01:15:35,970 --> 01:15:45,890
So when we used the diagnosis based on a visual assessment score greater than zero, we had 61.9% sensitivity, 90.2 specificity.

600
01:15:45,900 --> 01:15:50,459
So somewhere around here is the threshold of zero.

601
01:15:50,460 --> 01:15:58,380
But we only know that because we know what the corresponding sensitivity and specificity for zero was that we calculated elsewhere.

602
01:16:01,160 --> 01:16:07,100
And there's no individual with an exact score of zero.

603
01:16:07,100 --> 01:16:12,230
So the closest was observation 12. That's marked here.

604
01:16:12,530 --> 01:16:22,100
So the 12th row in the date, in the original data set, they had a pretty close to zero not quite zero score.

605
01:16:26,410 --> 01:16:29,850
So let's test your this is going be a perk up moment.

606
01:16:29,860 --> 01:16:37,239
I know that I need perk up it's it at 917 and so would moving the threshold for

607
01:16:37,240 --> 01:16:42,520
a positive diagnosis of melanoma to a lower visual assessment score increase,

608
01:16:42,520 --> 01:16:51,540
decrease or leave sensitivity unchanged. This is the kind of intuition that I would be like testing you on a quiz or something.

609
01:16:51,550 --> 01:16:57,190
So if I move the threshold from zero, which was somewhere around here.

610
01:16:59,720 --> 01:17:02,900
To a lower visual assessment score.

611
01:17:04,700 --> 01:17:08,480
So a more negative score as my threshold.

612
01:17:09,680 --> 01:17:13,700
Would that increase, decrease or leave sensitivity unchanged?

613
01:17:17,460 --> 01:17:22,860
Who says it increases sensitivity? Who says it decreases sensitivity?

614
01:17:24,930 --> 01:17:34,410
Okay. So remember, we're if the threshold is more negative, then fewer people are going to be called melanoma.

615
01:17:34,770 --> 01:17:39,600
It'll be harder. Right. So that means sensitivity is going to go down.

616
01:17:43,020 --> 01:17:54,419
And so reset with moving the threshold for a more positive for positive diagnosis of melanoma to a lower visual assessment score increase,

617
01:17:54,420 --> 01:17:58,050
decrease or leave specificity unchanged.

618
01:18:01,220 --> 01:18:09,410
So is specificity going to go? So again, I'm using a more negative value as my threshold.

619
01:18:11,840 --> 01:18:18,230
So is that going to make specificity go up? Go down.

620
01:18:19,750 --> 01:18:32,700
Or leave it unchanged. Oh.

621
01:18:32,700 --> 01:18:37,620
Did I get this backwards on sensitivity? Let's see.

622
01:18:37,620 --> 01:18:41,310
This is an example of your instructor having to rethink your.

623
01:18:41,640 --> 01:18:47,550
So if I'm making it easier to call someone melanoma by lowering the score, making more negatively.

624
01:18:49,470 --> 01:18:55,880
That should have increased my sense of calling more people positive. That should increase the sensitivity of Papworth before I think I did.

625
01:18:56,850 --> 01:19:03,690
Yeah. And so same if I'm lowering the score to be more negative, I'm calling fewer people positive.

626
01:19:07,290 --> 01:19:12,210
No. I'm playing more people. That's what I keep on. Going backwards and going more people positive when I lower that score.

627
01:19:14,120 --> 01:19:17,870
So if I'm calling more people positive, my specificity is going to go down.

628
01:19:23,100 --> 01:19:26,600
All right. We got it. Do we all have it?

629
01:19:29,220 --> 01:19:35,480
I had to double check myself there. Okay.

630
01:19:38,840 --> 01:19:49,370
So that the test command is looking at the two RC curves overlaid and looking for statistical significance between them.

631
01:19:49,400 --> 01:19:54,230
So both of these diagnostics have a similar area under the curve.

632
01:19:55,070 --> 01:20:01,160
The the visual was 0.905, the dermis gap was 0.900.

633
01:20:01,730 --> 01:20:06,920
They really look like whichever one you use, they have a similar area under the curve.

634
01:20:11,430 --> 01:20:14,850
And so remember, the higher the area under the curve is, the better.

635
01:20:14,880 --> 01:20:19,200
So just the visual is just slightly better.

636
01:20:19,950 --> 01:20:29,890
The confidence limits are very comparable as well. And they're not significantly different from one another using this rosy comparison.

637
01:20:29,920 --> 01:20:37,450
So the area under the curve for the visual versus the dermis scope is not significantly different from one another.

638
01:20:41,360 --> 01:20:49,460
So what are the conclusions? Based on what we saw for the melanoma data in SAS.

639
01:20:49,780 --> 01:20:58,359
So we we'll do the R code in a moment. So using SAS the to Paul RC area under the curve test failed to detect differences

640
01:20:58,360 --> 01:21:02,380
in the sensitivity and specificity profile of the two measures with this p value.

641
01:21:03,840 --> 01:21:06,900
There's a reference for the test. I think I have that on canvas.

642
01:21:09,510 --> 01:21:18,450
The melanoma diagnosis based on a visual assessment score that was positive, gave 90.2% specificity and 61.9% sensitivity.

643
01:21:19,760 --> 01:21:23,839
And the same threshold applied to the derma scope criteria had somewhat higher

644
01:21:23,840 --> 01:21:29,360
sensitivity and equivalent specificity with moderate agreement and diagnosis,

645
01:21:29,360 --> 01:21:34,040
regardless of the use of visual or dermal scope assessment with this threshold.

646
01:21:34,040 --> 01:21:38,570
And here's where I would put in the sentence about Kappa statistic and the confidence interval.

647
01:21:41,710 --> 01:21:49,810
And to achieve higher sensitivity at the cost of lower specificity, the threshold for a positive diagnosis should be lowered.

648
01:21:49,820 --> 01:21:53,660
That's the one that we had to think about really hard.

649
01:21:53,680 --> 01:22:01,060
So if you call more people positive, you're going to increase your sensitivity and decrease your specificity.

650
01:22:04,460 --> 01:22:13,100
All right. So, again, looking at different thresholds, trying to figure out if there's a better threshold than the zero.

651
01:22:14,830 --> 01:22:22,569
Here is what the ROIC curve. We save data in this ROIC dataset and this is what that dataset looks like.

652
01:22:22,570 --> 01:22:26,320
So it has a column here for sensitivity.

653
01:22:26,860 --> 01:22:30,100
It has a column for one minus the specificity.

654
01:22:31,900 --> 01:22:35,200
And it won't. It has for every observation and data set.

655
01:22:35,200 --> 01:22:44,360
It does this. And it can be a little bit confusing that in this little RC data set it has obs one, two, three, four, five.

656
01:22:44,370 --> 01:22:53,250
But ah, I. The observation number that shows up in the ROIC curve using PROC logistic.

657
01:22:54,150 --> 01:23:01,080
I, I don't know if that's the observation from this mini data set that it does or the original data set.

658
01:23:01,080 --> 01:23:04,409
And I think it's the original data set. So you can double check me on that.

659
01:23:04,410 --> 01:23:07,170
But just be aware that there's.

660
01:23:08,510 --> 01:23:15,140
It might be this did is it or it might be the original ordering to double check when you're trying to pull off something from this data set.

661
01:23:17,360 --> 01:23:24,560
So one thing that it does not show you is the actual threshold that generated the sensitivity and the one minus specificity.

662
01:23:25,190 --> 01:23:30,049
What it does show you is the probability of having melanoma.

663
01:23:30,050 --> 01:23:31,100
That's this column.

664
01:23:31,820 --> 01:23:42,380
So for logistic regression, this is, you know, the E to the model over one plus either the model the P hat that you get from logistic regression.

665
01:23:44,590 --> 01:23:48,400
And so you. It's a bit inconvenient.

666
01:23:48,670 --> 01:23:53,970
This is the one thing about South for this handout that is unattractive.

667
01:23:53,980 --> 01:23:59,920
You actually have to do math to figure out the threshold based on this p hat.

668
01:24:00,850 --> 01:24:07,150
And so I'm going to show you that Algebra R does not make you do hand calculations, but sass will.

669
01:24:11,150 --> 01:24:16,070
All right. And so all the rows of output are going to be down here.

670
01:24:16,070 --> 01:24:24,470
So this is all the first part of the data set of RC is going to be using just t one the second.

671
01:24:25,640 --> 01:24:30,860
If you go down lower and lower in the RC data set, it will eventually say Source T two.

672
01:24:31,720 --> 01:24:34,960
For the dermis cope values.

673
01:24:35,620 --> 01:24:42,370
So that's why again, I think that when you see OBS and RC curves those numbers, I think they correspond to the original dataset.

674
01:24:44,510 --> 01:24:51,420
And not this RC data set. Okay.

675
01:24:52,410 --> 01:24:56,760
So the result is sorted by the setting values of the probabilities.

676
01:24:56,760 --> 01:25:05,160
So you know, you're going to see sensitivity going up, specificity going down that there is that is a nice feature.

677
01:25:06,450 --> 01:25:11,340
That particular ordering can be a nice feature because you can find, you know,

678
01:25:12,060 --> 01:25:16,140
rows that you'd like to use for sensitivity, specificity quickly that way.

679
01:25:19,830 --> 01:25:28,620
So to get the thresholds for the visual or the dermis values, you have to remember logistic regression, algebra.

680
01:25:28,920 --> 01:25:33,870
And I'm just doing the minimum amount here for this handout. We'll do the full review later.

681
01:25:34,440 --> 01:25:39,240
So this is if you just had the T two continuous predictor in the model,

682
01:25:39,510 --> 01:25:44,430
this is the the table of parameter estimates for that particular logistic regression.

683
01:25:44,430 --> 01:25:48,990
And it's a little weird that it has a slope of one, but that's just what happened with the data set.

684
01:25:49,800 --> 01:25:58,440
And so the, the probability of having melanoma using this t two continuous predictor comes from this model.

685
01:25:58,710 --> 01:26:08,100
So thresholds, the only you know covariate here we call it T to continuous but I want to solve for the threshold in terms of p hat.

686
01:26:09,430 --> 01:26:13,060
So p hat is like prob in that output.

687
01:26:13,570 --> 01:26:21,220
So I have all these values of prob in my RC data set and I want to solve for the threshold that gave those probes.

688
01:26:22,190 --> 01:26:28,190
So. So for this particular table of output, this algebra is going to work.

689
01:26:28,190 --> 01:26:34,240
And you just have to remember to substitute the intercept in the slope here for your data set.

690
01:26:34,250 --> 01:26:46,610
So in this particular output, I'm subtracting off the beta not that ends up being a plus .0000 or two and I'm dividing by the slope 42 continues,

691
01:26:46,610 --> 01:26:51,320
which just weirdly is one to several significant digits.

692
01:26:52,230 --> 01:26:57,190
And so I. You know I can always solve.

693
01:26:59,090 --> 01:27:03,020
For the threshold in terms of that probe Roe here.

694
01:27:05,460 --> 01:27:17,340
So suppose we desire the threshold for a positive melanoma test that's based on the derma scope score to give specificity no lower than 75%.

695
01:27:17,350 --> 01:27:21,600
So we want to make sure our specificity is always 75% or more.

696
01:27:23,720 --> 01:27:30,650
And based on the data, what's the threshold for the dermal scope score that gives the highest sensitivity under that restriction?

697
01:27:32,820 --> 01:27:37,890
So that's going to be your goal. And then what's the corresponding sensitivity when using this threshold?

698
01:27:38,940 --> 01:27:48,620
So this is kind of like almost like a homework problem. I might even write a homework problem similar to this for homework too.

699
01:27:48,980 --> 01:27:57,770
So first we want to identify the output that corresponds to, you know, the criteria above and find that probe value.

700
01:27:59,000 --> 01:28:02,510
And we use that logistic regression algebra to find the threshold.

701
01:28:04,950 --> 01:28:11,540
So here is. I've skipped some rows here just so that we don't have a big, messy slide.

702
01:28:12,890 --> 01:28:17,590
Here are the p hats. Here's the sensitivity and specificity.

703
01:28:17,860 --> 01:28:21,879
And remember, I didn't want to have specificity go below 75%.

704
01:28:21,880 --> 01:28:27,900
So any of the rose up until this last row would have been okay for that.

705
01:28:27,910 --> 01:28:32,740
This last row has one minus specificity of 0.254.

706
01:28:33,370 --> 01:28:36,849
So that would mean specificity fell below 75%.

707
01:28:36,850 --> 01:28:41,950
So any of the other rows and it would have been okay for the specificity requirement.

708
01:28:42,610 --> 01:28:47,929
And then we kind of want to pick the best threshold in terms of sense of tivity that we can.

709
01:28:47,930 --> 01:28:56,129
So that's this column. And so normally we'd go to the highest to the row with the highest sensitivity.

710
01:28:56,130 --> 01:29:04,290
But we have ties here. So we have three p hats for the probability of melanoma.

711
01:29:05,590 --> 01:29:15,040
Then at the same sensitivity and meet our threshold of specificity also being, you know, 75% or more.

712
01:29:15,940 --> 01:29:24,220
So if you're picking a threshold here for diagnosing melanoma, which of these three rows would you want to pick?

713
01:29:26,710 --> 01:29:31,900
If you're just choosing between these three rows of 29, 30 and 31.

714
01:29:33,480 --> 01:29:36,990
Which threshold would you pick for diagnosing melanoma?

715
01:29:45,970 --> 01:29:59,110
I guess this is the perk up moment. All right.

716
01:29:59,110 --> 01:30:03,990
So. They all have the same sensitivity. So I can't judge based on this column.

717
01:30:04,000 --> 01:30:11,680
So I'm going to want to look at this column. So this is this is the one minus the specificity.

718
01:30:11,680 --> 01:30:15,790
So what does that mean? That's like the false positive rate, given that they don't have.

719
01:30:17,330 --> 01:30:21,440
Melanoma. This is like the probability I say they do.

720
01:30:22,620 --> 01:30:27,390
So I want this number to be smaller. Smaller is good for this column.

721
01:30:28,140 --> 01:30:31,320
So if I'm choosing between 29, 30 and 31.

722
01:30:33,250 --> 01:30:36,760
I want to pick the one with the smallest false positive rate.

723
01:30:37,420 --> 01:30:40,990
So I'd pick this row 29. That's the one we're going to focus on.

724
01:30:43,100 --> 01:30:46,220
So they'll all have the same very high.

725
01:30:47,310 --> 01:30:56,550
Sensitivity. But I got a little bit more specificity than if I'd gone to this last row 31.

726
01:30:59,170 --> 01:31:05,930
So I want to figure out what threshold corresponds to having these operating characteristics for sensitivity and specificity.

727
01:31:05,950 --> 01:31:15,560
So I've got the p hat. Of 0.3 164a so I can use p hat to figure out what the actual threshold is with that algebra.

728
01:31:16,370 --> 01:31:26,250
So I'm going to focus on that row. And we want that threshold corresponding to .31648 since this gave a specificity

729
01:31:27,960 --> 01:31:33,900
of 80.392% and it gave us the highest sensitivity under the restriction,

730
01:31:33,900 --> 01:31:37,170
that specificity must be greater than 75%.

731
01:31:38,680 --> 01:31:43,650
So for the algebra, remember we're always putting in the beta not here, you know,

732
01:31:43,750 --> 01:31:51,190
minus beta not and then dividing by beta one for whatever the logistic regression was that gave us these columns.

733
01:31:51,790 --> 01:31:59,740
And so once you and this is the natural log, whenever I say log in this class, it's always going to stand for l m on your calculator.

734
01:32:00,430 --> 01:32:06,580
And so the threshold that they recommend then is -0.77.

735
01:32:06,610 --> 01:32:09,429
So we had just sort of used zero before in our examples.

736
01:32:09,430 --> 01:32:16,060
But they're saying if you want these operating characteristics, you should be using a threshold of -0.77.

737
01:32:20,820 --> 01:32:31,680
And I always double check by creating a two by two table using that new diagnostic just to make sure, you know, everything checks out.

738
01:32:32,490 --> 01:32:39,180
And so I'm going to, you know, set this new data,

739
01:32:39,180 --> 01:32:45,570
set melanoma to the RC data set where my specificity I'm just going to make it easier to read that variable.

740
01:32:47,650 --> 01:32:51,010
One minus whatever they call their their false positive rate.

741
01:32:51,370 --> 01:32:56,840
And then I'm going to create a01 variable called test a were.

742
01:32:57,040 --> 01:33:00,220
Remember, this is shorthand for creating a01 variable.

743
01:33:00,550 --> 01:33:07,090
So everything on the inside of the parentheses, if it's true test underscore a will be a one.

744
01:33:07,090 --> 01:33:12,070
Otherwise a zero. So I'm going to call it a one.

745
01:33:12,550 --> 01:33:17,170
If my dermis value is greater than or equal to minus .77.

746
01:33:21,640 --> 01:33:29,830
So this is actually, you know, checking our logic from earlier to we just made that we lowered the score that we would call positive.

747
01:33:29,830 --> 01:33:37,120
So that should increase our sensitivity and it should decrease our our specificity.

748
01:33:37,120 --> 01:33:41,380
But we you know, we know that we just calculated from the previous slide what our sensitivity and specificity was.

749
01:33:42,670 --> 01:33:51,520
All right. Now I'm going to look at proc freq and I'll even use that option since spath just to show you what it will give you.

750
01:33:51,520 --> 01:34:00,190
But again, I'm saying I really don't like that command because it labels things incorrectly.

751
01:34:03,540 --> 01:34:11,369
Okay. So here's our two by two table. And so the specificity specificities check that first.

752
01:34:11,370 --> 01:34:14,640
The specificity is given. I don't have the condition.

753
01:34:15,480 --> 01:34:20,780
What's the chance that I say I don't have the condition with the test?

754
01:34:20,790 --> 01:34:27,030
That's 80.39. So that works out. We wanted that to be above 75%.

755
01:34:27,180 --> 01:34:31,049
It is. And that's what the threshold is giving us.

756
01:34:31,050 --> 01:34:39,209
And then for sensitivity that's in this column, given they really do have melanoma, what's the chance?

757
01:34:39,210 --> 01:34:42,810
We're calling them a positive test, and that's the 90 point for it.

758
01:34:43,440 --> 01:34:48,599
So we have our high sensitivity. We have our specificity that's above 75%.

759
01:34:48,600 --> 01:34:51,840
So we got that right and.

760
01:34:54,740 --> 01:35:02,530
Let's see what this gave us. So how did it get the labels incorrect or correct?

761
01:35:05,360 --> 01:35:13,080
So it's saying specificity is point nine. Is that is that the specificity.

762
01:35:14,250 --> 01:35:22,219
Yeah. It's switched him again. So we know from our calculation, just looking at this table, given they have the disease,

763
01:35:22,220 --> 01:35:29,930
the chance that as posit as 90.48 that's this row and it show it should read sensitivity therefore our situation.

764
01:35:31,160 --> 01:35:38,660
So it switched the labels again and again. We just ignore the positive and negative predictive value unless we have a cohort study.

765
01:35:44,370 --> 01:35:50,160
Okay. So actually the confidence limits you can use as long as you switch the labels here.

766
01:35:54,760 --> 01:36:04,360
All right. So the hour code, I'm going to have to tell you, you know, I really like the hour code better for this RC curve stuff.

767
01:36:05,200 --> 01:36:10,120
It just is so much nicer. You don't have to use any hand calculations to find your threshold.

768
01:36:10,120 --> 01:36:18,700
It just has a lot of nice features. So if you're kind of on the fence about learning horror, this is one scenario where you might like to learn it.

769
01:36:20,590 --> 01:36:27,950
Okay. So. Here's his reading in the melanoma data and looking at what that data looks like.

770
01:36:27,950 --> 01:36:33,019
It looks just like what we saw in Seth. I had a file on the side called Melanoma.

771
01:36:33,020 --> 01:36:37,280
That text that I read in from my desktop that looked kind of like this.

772
01:36:39,210 --> 01:36:46,020
If you're using our studio and you have an Excel file, it's really quick to load in a data set and the G has size.

773
01:36:46,410 --> 01:36:52,230
During lab, they can probably show you a few tricks beyond what I'm showing here with a snapshot.

774
01:36:53,820 --> 01:36:57,590
So here's creating the rosy curves. Isn't this beautiful looking?

775
01:36:57,600 --> 01:37:07,229
So we install a couple of packages, Gigi plot two and WB plots and then there's this ROIC plot function that you can use.

776
01:37:07,230 --> 01:37:13,469
Here's our data set. We're putting in our predictor for T one continuous.

777
01:37:13,470 --> 01:37:21,629
So that was the visual scope score. We tell it what the variable is that says the true disease status.

778
01:37:21,630 --> 01:37:24,120
So that's we call that D.

779
01:37:26,470 --> 01:37:35,410
And the truth target is saying the value of one indeed is the true disease, that it has both the truth fair and what the label for the disease is.

780
01:37:36,910 --> 01:37:40,660
And so it gives you this. I really think this is a much nicer plot than Seth.

781
01:37:40,660 --> 01:37:52,960
And it gives you the area under the curve which substance to. This is the part that I think ah ah does more nicely than stars.

782
01:37:52,990 --> 01:38:01,330
So this is a threshold plot. And so this top part here is showing you sensitivity.

783
01:38:01,870 --> 01:38:05,710
And the bottom part here is showing you specificity.

784
01:38:06,160 --> 01:38:13,600
So this is actually the, you know, the label markings for sensitivity, for the top plot and specificity for this plot.

785
01:38:13,990 --> 01:38:19,180
And then the horizontal axis is the threshold. And I've just put a line at zero.

786
01:38:19,180 --> 01:38:23,020
That was the first little kind of pretend threshold we played with in the handout.

787
01:38:23,650 --> 01:38:31,120
And so you can see very quickly that if I use a threshold of zero, this is my sensitivity and this is my specificity.

788
01:38:31,120 --> 01:38:37,270
And it's just easier to see, you know, instead of just the one curve where you really don't know where the thresholds are.

789
01:38:38,050 --> 01:38:43,240
These are the same things that go into an RC curve, but it shows you where all the thresholds are.

790
01:38:43,870 --> 01:38:48,520
So if I want really high sensitivity, I'm going to want thresholds that are lower.

791
01:38:49,090 --> 01:38:57,639
That's helping our intuition again. Right. And but when I whenever I lower that threshold, it's going to increase my sensitivity.

792
01:38:57,640 --> 01:39:03,190
But, boy, it's going to lower my specificity. So it just gives you that intuition much more quickly.

793
01:39:03,910 --> 01:39:07,330
And so you don't have to, you know, confuse yourself like I did earlier.

794
01:39:11,500 --> 01:39:20,710
All right. And you can also, if you just add this dollar sign data to the end, it'll actually give you the thresholds.

795
01:39:22,380 --> 01:39:32,220
Without any algebra. So here are each one of these rows, or you have to kind of read them in pairs.

796
01:39:33,150 --> 01:39:39,780
So for each possible threshold, the first entry is for sensitivity and the second entry is for specificity.

797
01:39:40,770 --> 01:39:51,810
So you kind of go down these thresholds and we looked there's not a real person with zero for their test result for this visual score.

798
01:39:51,840 --> 01:39:58,290
The closest is -0.5. And but on the other side of 0.18.

799
01:39:58,290 --> 01:40:02,580
So what? One thing that you have to do in order is double check.

800
01:40:03,480 --> 01:40:08,700
If there's not a real threshold there, it's going to be one of these sets of rows.

801
01:40:09,780 --> 01:40:13,110
But you have to double check for a threshold of zero. Which one is that?

802
01:40:13,120 --> 01:40:17,790
Is it this one or is it this one? So checking by hand is still important to do.

803
01:40:20,650 --> 01:40:30,340
And so when we did it by hand, we got the sixth, we got 61.9% sensitivity and 90.2 specificity.

804
01:40:30,340 --> 01:40:37,240
And so that's actually. The second set.

805
01:40:40,070 --> 01:40:44,750
Of entry. So even though the minus .05 was closer to zero,

806
01:40:45,200 --> 01:40:53,360
the sensitivity and specificity results for zero are really kind of set up in this row where the threshold of 0.183.

807
01:40:56,200 --> 01:41:00,220
But you'll be in the right, you know, you'll be right in the right area without algebra.

808
01:41:00,880 --> 01:41:06,550
And then you just have to double check. So to double check with your own two by two table,

809
01:41:07,480 --> 01:41:18,670
I'm going to load this package g models and use this cross table command or I'm looking at, you know, the threshold of zero.

810
01:41:21,210 --> 01:41:26,940
Here's the disease variable. And you'll see your two by two table here.

811
01:41:28,350 --> 01:41:31,980
And so this is for the threshold of being greater than or equal to zero.

812
01:41:32,520 --> 01:41:36,450
And again, this is. So you get your sensitivity and specificity from this table.

813
01:41:41,020 --> 01:41:48,820
So there's your sensitivity. 13 over 21 from that column where they truly have the disease.

814
01:41:48,830 --> 01:41:55,240
So you're always sampling based on the column and the specificity of 0.902 from this column.

815
01:42:00,290 --> 01:42:03,470
So if you need confidence intervals for your area under the curve,

816
01:42:05,240 --> 01:42:09,110
R does require you to load a lot of different packages to get the functions to work.

817
01:42:09,110 --> 01:42:17,270
So you're loading this process package and putting in the true response, the disease variable and the predictor.

818
01:42:17,900 --> 01:42:26,479
And second confidence overall equals true. And then that call will give you this output over here.

819
01:42:26,480 --> 01:42:31,850
So here is the area under the curve and here is the 95% confidence interval that you wanted.

820
01:42:36,420 --> 01:42:43,140
And to get the comparison. Remember, Seth did this all kind of within proc logistics to get the comparison.

821
01:42:44,750 --> 01:42:46,580
Between the two RC curves.

822
01:42:47,900 --> 01:42:58,549
We do this RC plot pair command and we're putting in the data set and then the two different biomarkers with the truth fair and truth targets.

823
01:42:58,550 --> 01:43:03,320
The truth is the variable that is saying what the disease is, yes or no.

824
01:43:03,320 --> 01:43:06,440
And the truth target says that the value of one is disease.

825
01:43:08,030 --> 01:43:11,600
Put a little title here and estimate significance is true.

826
01:43:12,560 --> 01:43:15,650
So before we didn't have the p value. So this is all to get the p value.

827
01:43:15,650 --> 01:43:19,969
So it's just a different plot from the receipt curve. Maybe you like this better.

828
01:43:19,970 --> 01:43:28,580
I don't know. I'll show you the area under the curve for the two different metrics visual dermis scope and then for the P value.

829
01:43:28,970 --> 01:43:38,900
It's a little strange, isn't it? It says we're testing that the area under a curve for the first variable over here.

830
01:43:38,900 --> 01:43:42,440
So t one continuous is greater than the second one.

831
01:43:43,880 --> 01:43:48,440
So it's like a one sided test. I'm not sure why they did it that way.

832
01:43:48,950 --> 01:43:53,240
And so this p value, is it really the two sided p value?

833
01:43:53,250 --> 01:43:58,160
It's the one sided p value set the p value. You need to multiply this by two.

834
01:44:00,180 --> 01:44:02,159
So it's a one sided p value.

835
01:44:02,160 --> 01:44:07,800
And so you multiply by two for the two sided test comparing the area under the curves and then you'll get the same things that gave you.

836
01:44:12,270 --> 01:44:22,860
And that was the end of the handout. We have time for questions if you want to ask questions or, you know, we can end as well.

837
01:44:22,870 --> 01:44:31,740
So. What anyone like to ask a question or just.

838
01:44:32,650 --> 01:44:38,870
About whatever. Okay.

839
01:44:38,990 --> 01:44:43,280
So I completely displayed my awkwardness successfully.

840
01:44:43,930 --> 01:44:46,700
I'm going to go ahead and stop the recording if you want to ask me a question.

