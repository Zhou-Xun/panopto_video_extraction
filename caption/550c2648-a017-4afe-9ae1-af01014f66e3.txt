1
00:00:02,100 --> 00:00:17,370
Who knows? You don't have to identify it yet.

2
00:00:21,300 --> 00:00:26,820
So let's take one look at it and get out of.

3
00:00:30,600 --> 00:00:35,530
It's my job to care of.

4
00:00:35,610 --> 00:01:05,490
You know, what it's like to feel like it is the little things down there.

5
00:01:06,040 --> 00:01:13,480
And I think he's right.

6
00:01:21,330 --> 00:01:48,450
About 38, it was a guy that started to talk about the central limit theorem, another very important number.

7
00:01:49,570 --> 00:01:55,150
Yeah. Last time when we talked about long hours, number one, they focused on a sample.

8
00:01:56,170 --> 00:02:08,830
We provide a limit for the the sample, meaning when the sample size puts the infinity cell focus on this sample mean that we can ask another question.

9
00:02:08,890 --> 00:02:16,000
That is, if we are interested not only in the limit, the probability limit, but also in the distribution,

10
00:02:16,000 --> 00:02:23,590
all this means that what you say about if you think about the results that we have last time,

11
00:02:23,650 --> 00:02:27,190
either we fill out a large number or no loss last summer.

12
00:02:27,490 --> 00:02:38,080
It just tells us if sample size goes to infinity, this statistic, this sample means statistic will converge to a number to generate distribution.

13
00:02:38,920 --> 00:02:47,850
I. So you already got a limited distribution of this sample, meaning we need to magnify it to some extent.

14
00:02:48,720 --> 00:02:57,300
That's to look down the sample itself doesn't help because it will convert to a degenerate distribution that is only a single number.

15
00:02:58,170 --> 00:03:03,750
So we need to magnify some samples in order to find the proper magnifier.

16
00:03:04,740 --> 00:03:13,170
Let's think about this. Sample me a second. It's what I mean.

17
00:03:14,130 --> 00:03:18,790
You embarrass. Think I'm a square.

18
00:03:19,270 --> 00:03:23,250
What do we know about the meaning of this? Why? It inspired me.

19
00:03:26,220 --> 00:03:32,480
What is the expectation of the sample mean the same meal?

20
00:03:32,490 --> 00:03:39,210
Right. And we also know that the variance of the sample means what?

21
00:03:41,540 --> 00:03:48,560
The veterans of the 70. Stigma Square grant.

22
00:03:50,490 --> 00:03:54,890
And that tells us why this weight loss of a large number works.

23
00:03:54,900 --> 00:03:59,280
Because the virus, if you look at the virus, sample size goes to infinity.

24
00:03:59,790 --> 00:04:06,480
The virus of the sample on me has shrunk to zero. So that's why it compares to a degenerate distribution.

25
00:04:07,290 --> 00:04:10,410
The virus goes to zero just means that there is no variability.

26
00:04:11,340 --> 00:04:19,050
So these random variable will converge to its there is no variability.

27
00:04:21,180 --> 00:04:29,250
So in order to take care of this strength in advance, we need a magnifier.

28
00:04:29,810 --> 00:04:36,270
And if you multiply this sample mean by that's the end.

29
00:04:39,180 --> 00:04:44,450
And then you look at this quantity. The memories of end times.

30
00:04:44,660 --> 00:04:59,440
The sample means whether we know. And that's going to be and squared variants of sample mean there's going to be times sigma squared.

31
00:05:00,890 --> 00:05:05,360
Right. And this guy will explode as an infinity.

32
00:05:06,380 --> 00:05:13,070
The other words, if you use a magnifier that is end times, the sample mean that might expire.

33
00:05:13,190 --> 00:05:21,650
Maybe just to break the barriers up ten times, the sample means going to explode as goes infinity.

34
00:05:23,810 --> 00:05:34,790
So that tells us that directly low count is summation of why or in other words and times the sample mean may not give us what we want,

35
00:05:34,820 --> 00:05:38,450
may not help us the limiting distribution on the samples.

36
00:05:38,960 --> 00:05:48,050
On the other hand, the original sample mean, as we have argued, without any proper magnifier, the bar has shrunk a zero, so it's too small.

37
00:05:49,010 --> 00:05:55,819
So we need to strike a balance between the two. So that's why we need a magic magnifier.

38
00:05:55,820 --> 00:06:01,740
That is the square root of that, which is a proper glass back to give us the limited distribution of the samples.

39
00:06:02,540 --> 00:06:06,980
So if we multiply a square root out to this sample, mean,

40
00:06:07,910 --> 00:06:14,810
what do we know about the variance that's going to be and times the variance of the sample

41
00:06:15,440 --> 00:06:26,090
right end times sigma square over and you can multiply this proper scaling parameter.

42
00:06:26,450 --> 00:06:31,040
Square root of and square root of the sample size. Then the balance is going to be stabilized.

43
00:06:32,090 --> 00:06:37,550
It won't change with your sample size, which is a good starting point.

44
00:06:38,480 --> 00:06:45,110
So that is it. Classes that are used to help us find the limited distribution of the sample mean.

45
00:06:45,710 --> 00:06:50,240
And that's also the starting point of the central limit theorem.

46
00:06:52,160 --> 00:07:00,540
Okay. So the standard version of the central limit theorem is called the US Central Limit Theorem and States as follows.

47
00:07:00,970 --> 00:07:03,840
That's why I buy a sequence of random samples.

48
00:07:06,180 --> 00:07:16,170
Namely, they are the samples from the same distribution and independent with finite mean being new and finite barrels being Sigma Square.

49
00:07:16,620 --> 00:07:19,980
Then we always have the following relation.

50
00:07:22,590 --> 00:07:29,810
So if we take the sample mean of this random sample, subtract the population mean,

51
00:07:30,090 --> 00:07:37,020
and then divide by this by the standard deviation of the sample mean, which is sigma over spirit.

52
00:07:37,200 --> 00:07:41,730
And we have a new sequence of random arrivals.

53
00:07:43,970 --> 00:07:45,650
And we know that by design.

54
00:07:45,950 --> 00:07:56,960
Now, what do we know is that Z will always converge to the standard novel distribution in distribution or distribution to the standard novel.

55
00:07:59,810 --> 00:08:14,030
So what that means is it can clear the limit as these poverty for any Z since the standard normal distribution is a continuous distribution.

56
00:08:14,300 --> 00:08:18,980
So every single point out of the real space is a continuity point.

57
00:08:19,670 --> 00:08:26,270
So what that means, what this convergence improper distribution means is for anything we always have.

58
00:08:26,660 --> 00:08:33,700
The probability of Zion allows already called a Z, which is essentially the Z of Z, right?

59
00:08:33,720 --> 00:08:36,830
The city, our functional position evaluated as Z.

60
00:08:37,100 --> 00:08:47,030
If you take the limit that angles infinity, that will be the area for the standard novel evaluated as Z.

61
00:08:48,080 --> 00:08:54,470
And so we use this find to represent the city, our function or the of normal distribution.

62
00:08:54,920 --> 00:08:59,660
It's not just the translation of what it means by convergence and distribution.

63
00:09:03,460 --> 00:09:07,360
So that's the standard central limit theorem.

64
00:09:09,130 --> 00:09:11,020
So again, what it tells us is that.

65
00:09:13,250 --> 00:09:23,950
Well, any random samples, if you take the sample, mean you can construct a new synchronous RNA variables by subtracting its population,

66
00:09:23,950 --> 00:09:27,190
meaning divided by its, you know, standard deviation.

67
00:09:27,490 --> 00:09:32,590
Then that new sequence will always converge. Distribution to the standard normal distribution.

68
00:09:35,400 --> 00:09:40,709
Yes. How or how did we construct this?

69
00:09:40,710 --> 00:09:46,260
Zsa Zsa Zsa Zsa Zsa Zsa is constructed exactly in this way.

70
00:09:46,380 --> 00:09:50,070
Well, yeah. But I'm wondering, why are we using it like that?

71
00:09:52,770 --> 00:09:56,400
Very good question. So why it takes this particular point, right?

72
00:09:56,940 --> 00:10:02,470
Yeah. So, like, like, is there. Like, what does that tell me?

73
00:10:02,680 --> 00:10:10,300
Mm hmm. So, first of all, when we talk about Central Limit Theorem again, we need to be clear of what is the.

74
00:10:10,930 --> 00:10:15,540
The. What is our objective here?

75
00:10:15,600 --> 00:10:18,630
We're trying to find a limiting distribution for the sample mean.

76
00:10:18,900 --> 00:10:22,410
Right. So that's why Ambar is our target.

77
00:10:22,710 --> 00:10:28,770
We want to find a limited distribution for this wine bar, but the wine bar itself does not have a limiting distribution.

78
00:10:31,050 --> 00:10:39,720
So instead of directly finding the limiting distribution for a wine bar, think about what the mean is what are the variances?

79
00:10:39,810 --> 00:10:42,990
Or wine bar? Right. So the mean is mu.

80
00:10:42,990 --> 00:10:54,000
The variance is Sigma Square over. So in order to have a limiting distribution, we need to be to make sure that two things at least.

81
00:10:54,300 --> 00:10:59,070
First, the mean has to be stabilized. Second, the balance has to be stabilized.

82
00:10:59,820 --> 00:11:07,320
Right? If the mean explode, if the mean goes to infinity as an infinity, there's no way that I can find a limited distribution.

83
00:11:08,550 --> 00:11:15,690
On the other hand, if the virus is either goes to zero or goes into infinity, you cannot find a limiting distribution either.

84
00:11:16,560 --> 00:11:21,570
If it goes to zero, then essentially the limiting distribution will be degenerate.

85
00:11:21,960 --> 00:11:26,350
It's just going to be a point. That's that's what I would have said last time. All right.

86
00:11:26,770 --> 00:11:32,510
If you just look at this sample mean as angles in infinity, it does converge.

87
00:11:32,530 --> 00:11:36,310
It converts to a constant. That's a degenerate case, which is not trivial.

88
00:11:36,430 --> 00:11:44,230
It's not very interesting. What do we want to do here is we want to find a distribution for this sample mean.

89
00:11:44,980 --> 00:11:55,270
So we need to properly inflate this sample me a little bit so that it has a stabilizing or stabilized barrens as a constipated.

90
00:11:56,890 --> 00:11:59,980
So that's what we have seen from the previous slide.

91
00:12:00,250 --> 00:12:08,990
If you if you are scanning parameters to a large, say, end times this sample mean, then that's not just going to explode.

92
00:12:09,450 --> 00:12:15,340
This variance goes to infinity. The variance times y and bar that are being infinity.

93
00:12:15,880 --> 00:12:18,910
But that doesn't work. The only parameters are works.

94
00:12:19,210 --> 00:12:25,780
If the square root of it, if you multiply square root of n to this sample means that will stabilize your parents.

95
00:12:26,530 --> 00:12:33,910
So that's why we have this square root of that. So now putting everything together we can have.

96
00:12:35,590 --> 00:12:41,470
So starting from this mean a variance of the sample mean from the previous argument.

97
00:12:41,710 --> 00:12:45,340
We know we need a square root and stabilize the balance.

98
00:12:45,720 --> 00:12:53,290
Right? But if you directly multiply the square root of two, why are that?

99
00:12:53,290 --> 00:12:58,150
Indeed, stabilize the variance but going to sabotage the mean.

100
00:12:59,890 --> 00:13:07,360
So we need to do something about the MI as well. So the simplest way is to minus the mean from the original lion bar.

101
00:13:07,750 --> 00:13:17,440
So that is stand at zero. No matter what it constant, you multiply to that cause to that one bar minus mu the mean it's always going to stay as zero.

102
00:13:18,580 --> 00:13:21,890
So now give us. This new sequence.

103
00:13:22,040 --> 00:13:25,670
SCARBOROUGH And times when bar minus me.

104
00:13:26,570 --> 00:13:32,750
Now, let's take a look at this guy. The expectation of this guy is what?

105
00:13:35,630 --> 00:13:41,850
What's the expectation of this guy? That's a zero.

106
00:13:42,100 --> 00:13:45,569
Right. Which is stable and then goes to infinity.

107
00:13:45,570 --> 00:13:48,840
That stays at zero. So it's good. What about the merits?

108
00:13:53,220 --> 00:13:59,330
The. Same argument as before, right?

109
00:13:59,670 --> 00:14:03,820
You can take the square root of an outside of this vase, make it become NW.

110
00:14:04,360 --> 00:14:11,090
And then the variance of this Y and bar minus nil is the same at the bar as a wine bar, which is a sigma square over it.

111
00:14:11,770 --> 00:14:14,800
If you do the calculation, that's going to be a Sigma Square.

112
00:14:14,810 --> 00:14:22,270
So it's also stabilized. Engels name vanity the man and the variance of this guy doesn't change.

113
00:14:22,810 --> 00:14:32,680
But that's why we have these term this square root of it and times Y and fa minus means stabilize the mean and appearance.

114
00:14:34,270 --> 00:14:41,740
All right. Now, now, of course, you can divide it up by another constant sigma that doesn't change that again.

115
00:14:41,920 --> 00:14:46,660
That only change the variance instead of adding the bounds B and Sigma Square.

116
00:14:46,690 --> 00:14:49,780
Now, you have apparently quite a one, right?

117
00:14:50,080 --> 00:14:53,470
So that's not true about change and it doesn't change the mean.

118
00:14:54,370 --> 00:15:02,410
So now if you look at this time that we have, what do we know that Isaiah this guy has always have me on a zero.

119
00:15:02,550 --> 00:15:04,930
Varanasi quarter one no matter what.

120
00:15:04,930 --> 00:15:13,870
And it's so what this central limit theorem tells us is that it not only have how mean always being equal to zero variance,

121
00:15:13,870 --> 00:15:19,300
always being equal to one, but also as an angles infinity.

122
00:15:20,290 --> 00:15:28,540
This sequence we all converge all converging distribution to this that a normal was mean zero unbalanced one.

123
00:15:31,250 --> 00:15:36,950
So that is not a trivial result. Okay. So the mean you've got a zero virus equal to one, that's Trevor Ariza.

124
00:15:37,250 --> 00:15:47,300
You can theoretically calculate that from how from the random sample property by the convergence and distribution is not a trivial result.

125
00:15:48,500 --> 00:15:49,760
So it's a very strong result.

126
00:15:49,760 --> 00:16:01,250
In fact, it tells us that if you construct a sequence this way, then that angles to infinity will always converge to this particular distribution.

127
00:16:01,640 --> 00:16:05,560
That is the standard normal distribution. Right.

128
00:16:06,040 --> 00:16:09,240
So that's the central element there.

129
00:16:10,180 --> 00:16:15,010
And the proof of the central theorem is also quite complicated.

130
00:16:15,880 --> 00:16:18,900
It involves many sophisticated techniques.

131
00:16:18,910 --> 00:16:24,820
So I won't talk about the actual proof of the Central Theorem, but I hope you can.

132
00:16:27,140 --> 00:16:32,660
Are these and our stand why we have this particular form, this melody,

133
00:16:32,660 --> 00:16:39,200
because we want to first of all, our target, our tactic is to find a distribution for this Y and bar.

134
00:16:40,160 --> 00:16:41,270
Any order to do that,

135
00:16:41,270 --> 00:16:51,200
at least the necessary condition is we have to stabilize the mean on the balance and this constant scorer and would help us achieve that.

136
00:16:52,930 --> 00:16:57,480
Okay. So we need this particular form to stabilize the mean and the balance.

137
00:16:58,870 --> 00:17:02,319
And then with more sophisticated proof,

138
00:17:02,320 --> 00:17:10,430
we can show that this new sequence of rhinovirus was constructed this way over to converge in distribution to the standard.

139
00:17:10,430 --> 00:17:14,860
No. That makes sense.

140
00:17:18,180 --> 00:17:25,260
So one beauty about this central myth that it applies to any.

141
00:17:26,800 --> 00:17:31,540
But any distribution as long as you have a random sample.

142
00:17:32,170 --> 00:17:40,330
You always have this result. So we don't have any constraint on what distribution is, why it needs to come from.

143
00:17:41,860 --> 00:17:45,160
The Y can be a discrete a variable.

144
00:17:45,490 --> 00:17:48,520
It could be a continuous random variable, could be a mix of both,

145
00:17:49,210 --> 00:17:57,160
could be bounded with bounded support, or could be on a unbothered support no matter what this y is.

146
00:17:57,670 --> 00:18:02,530
As long as you have a I'd sample the sample mean after this transformation.

147
00:18:03,110 --> 00:18:08,830
Always have this limit distribution.

148
00:18:09,460 --> 00:18:22,260
So that's the the. That's why this central limit theorem is so important, because it's very flexible, applies to all distributions.

149
00:18:24,420 --> 00:18:30,360
All we need is ivy samples and find out me in a finite balance.

150
00:18:32,150 --> 00:18:36,680
As long as we have all three, we always have this convergence and distribution.

151
00:18:38,470 --> 00:18:47,770
So that's a very strong result. And there are also other variants of this central limit theorem which relax the different conditions.

152
00:18:48,430 --> 00:18:55,210
We do not necessarily need either condition, as we have seen previously in the last number.

153
00:18:55,300 --> 00:18:58,450
It can be both relaxed. But this is a classic one.

154
00:18:58,570 --> 00:19:02,500
And this is consider the past. Central limits are.

155
00:19:05,570 --> 00:19:16,879
So this version was first proved in the 17,004 special case where why I actually call up are normally

156
00:19:16,880 --> 00:19:24,350
random animals but later all the parts showed that can be true or more general distributions.

157
00:19:25,940 --> 00:19:33,430
It doesn't have to be. You know why it doesn't have to follow a particular distribution, doesn't have to be a paranoid rant about anything.

158
00:19:34,370 --> 00:19:42,350
You'll always have this convergence. And in 19,000, the family provided.

159
00:19:47,710 --> 00:19:51,400
So that's the central limit there. The basic form of central limit theorem.

160
00:19:52,750 --> 00:19:57,310
And that's I want to talk about when you use this central limit theorem.

161
00:19:59,040 --> 00:20:04,560
How should you write about it? How should you talk about this limited distribution?

162
00:20:05,460 --> 00:20:09,750
So one thing to make sure that you, um.

163
00:20:10,600 --> 00:20:16,200
Um. One mistake that I should avoid is that writing something like this.

164
00:20:16,420 --> 00:20:26,670
Okay, so this is wrong. We can now say on the y and bar this sample mean converge in distribution to

165
00:20:26,670 --> 00:20:31,290
a normal distribution with a minimal and a balanced sigma square over that.

166
00:20:32,880 --> 00:20:38,820
But this is a around statement. And why is that?

167
00:20:40,170 --> 00:20:47,710
Yes. This is an upgrade. Are good. It's the same reason as we talk about when we talk about the law of large numbers.

168
00:20:48,150 --> 00:20:53,780
You cannot have unopposed side if you have this convergence either in distribution or

169
00:20:53,800 --> 00:21:00,600
the probability of a surely convergence you have to fix was on the right hand side.

170
00:21:01,380 --> 00:21:05,459
The right hand side does not have an if.

171
00:21:05,460 --> 00:21:11,580
You also have N on the right hand side. That's not convergence because the right hand side is a moving target.

172
00:21:12,780 --> 00:21:18,630
In order to have convergence, that's your right hand side at the limit has to be fixed.

173
00:21:19,770 --> 00:21:25,290
Either it's Carlson or it's distribution. It has to be fixed and not have a unit.

174
00:21:25,710 --> 00:21:30,480
So in this statement, the right hand side definitely has a right.

175
00:21:30,820 --> 00:21:36,540
And in that in the variance, which is not correct.

176
00:21:37,800 --> 00:21:42,390
So instead, what are you can say one way to say this.

177
00:21:45,750 --> 00:21:57,450
One way to see the result of this central limit theorem is to say square root of n times y at far minus mu divided by sigma.

178
00:21:59,460 --> 00:22:03,270
Convert in distribution to a standard normal distribution.

179
00:22:04,350 --> 00:22:12,950
So this is the correct way to say it. So that's one way to say it, because if you look at this statement, the right hand side does not depend.

180
00:22:13,080 --> 00:22:20,900
That is a fixed distribution. The left hand side is a sequence of random variables.

181
00:22:21,560 --> 00:22:26,900
Right. So that's a valid and a correct statement of the central limit.

182
00:22:29,080 --> 00:22:34,000
Okay. So that's one way to say it. Alternatively, you can also say.

183
00:22:37,050 --> 00:22:47,050
Why Ga. Is approximately.

184
00:22:49,180 --> 00:22:55,430
Distributed as a normal distribution with MU and Sigma Square open.

185
00:22:58,110 --> 00:23:06,180
So technically it's not perfect, but it's a it's a correct way to say this central limit theorem as well.

186
00:23:06,750 --> 00:23:14,670
You can say Y and bar approximately distributed from this normal distribution with minimal and a variance Sigma square over it.

187
00:23:16,410 --> 00:23:19,500
But it's large. That's also a correct way to say it.

188
00:23:20,340 --> 00:23:28,820
But what you cannot say is that y and bar converging distribution to that normal distribution and

189
00:23:29,600 --> 00:23:36,980
say approximately that's the same distribution with the normal with me mu and sigma square over.

190
00:23:38,130 --> 00:23:43,260
Or you can say with sequence converging distribution to the standard normal distribution.

191
00:23:43,500 --> 00:23:59,960
So both are correct. Any questions?

192
00:24:19,200 --> 00:24:23,370
Any distribution can be drawn from any distribution.

193
00:24:23,820 --> 00:24:28,020
Why? I come from any distribution. As long as you have fun on me.

194
00:24:30,150 --> 00:24:37,890
So pretty much all of distribution. Now we have learned so far satisfy not except for the kosher distribution,

195
00:24:38,070 --> 00:24:44,790
which doesn't have meat, which doesn't have a B and balance other than the culture distribution.

196
00:24:45,240 --> 00:24:51,000
All the distribution that we have so far have fun on me and but it varies.

197
00:24:51,000 --> 00:24:56,000
So if you take the sample mean and do this transformation and always conversion

198
00:24:56,400 --> 00:25:01,320
in distribution just I don't know that's the beauty of this central limit.

199
00:25:01,320 --> 00:25:04,410
They're so universally applicable.

200
00:25:05,850 --> 00:25:12,000
All right. So any distribution as far as specify that me in front embarrassed to satisfy this.

201
00:25:15,200 --> 00:25:24,800
So in practice, you you also see that when sometimes people would say the asymptotic distribution.

202
00:25:25,400 --> 00:25:33,980
So that essentially means the same thing. So when people say asymptotic, they mean with proper scaling.

203
00:25:34,880 --> 00:25:44,870
So most likely is square root of N and the proper center of this run environment and converge in distribution to the center at all.

204
00:25:45,830 --> 00:25:48,680
So that's what people mean by asymptotic distribution.

205
00:25:49,360 --> 00:25:58,910
C Other words, you can express this result as the asymptotic distribution for Y and bar is instead of normal.

206
00:25:59,630 --> 00:26:08,630
So it's a separate distribution which implicitly implies we need to have proper scaling, right?

207
00:26:09,350 --> 00:26:15,490
Which means square root of m divided by sigma and the proper center,

208
00:26:15,710 --> 00:26:25,480
which is minus the population mean with that center and scaling the distribution converge into

209
00:26:26,010 --> 00:26:31,370
the run of our whole converging distribution for the standard novel that is called Asymptotic.

210
00:26:32,790 --> 00:26:40,489
Okay, let's say either say this sequence is a topic that a novel or the center of distribution of y of

211
00:26:40,490 --> 00:26:51,620
bar is setting off the system time that typically implies you have already down the scaling,

212
00:26:51,620 --> 00:27:03,110
proper scaling and proper centering around interval. So when you were asked about the asymptotic distribution of a random variable, no way or ask.

213
00:27:03,510 --> 00:27:10,610
Okay. So basically it asks you to find the properties center of hard mean and the proper

214
00:27:10,610 --> 00:27:18,020
scaling of the random variable so that as angles infinity that transform the version.

215
00:27:18,260 --> 00:27:23,990
Well converging distribution that are normal. So that's why I said chaotic means that.

216
00:27:31,150 --> 00:27:36,790
So as I just said, that distribution we have said so far almost all how it find out the barriers.

217
00:27:36,790 --> 00:27:53,570
So the central limit there applies. And the normal approximation again although you know in some asymptotic sense these holes.

218
00:27:54,120 --> 00:28:05,310
So this convergence host definitive but in practice we rarely have a case where an infinity they always have finite sample size.

219
00:28:06,700 --> 00:28:10,320
Whenever you know practical problems that they are looking at.

220
00:28:10,860 --> 00:28:15,570
So then the natural question is what does that mean?

221
00:28:16,080 --> 00:28:22,590
As NGOs, infinity, the ways that break it up and so that this approximation is good.

222
00:28:24,630 --> 00:28:29,520
And the answer to that question is it depends. It depends on many factors.

223
00:28:30,150 --> 00:28:36,830
And. Generally speaking, it depends on the distribution of your life.

224
00:28:37,940 --> 00:28:46,700
If your original Y is symmetrically distributed and has a, you know, sort of a balance type,

225
00:28:47,000 --> 00:28:54,320
then you don't have to have a very large and to have this good approximation probably ain't quite at 2030.

226
00:28:54,710 --> 00:28:58,580
You've already got very good approximation from this normal distribution.

227
00:29:00,130 --> 00:29:11,020
But if your original distribution for Y is extremely skewed or, you know, binary or some other weird [INAUDIBLE] distributions,

228
00:29:11,260 --> 00:29:18,790
they don't have to have a much larger order to use this central limit theorem and probably

229
00:29:18,790 --> 00:29:26,590
have to be a few hundred to have a radically good approximation from the normal distribution.

230
00:29:26,950 --> 00:29:33,280
So we're going to say a couple of examples in a second. So in practice, there is no general rule.

231
00:29:33,820 --> 00:29:41,410
How large is large enough or there's some other. It depends on the true distribution of the data.

232
00:29:42,890 --> 00:29:54,100
And so, um, with more experience, you was, you got a better understanding of how large these are different of distributions.

233
00:29:54,910 --> 00:30:02,260
But again, the rule of thumb is if you are original distribution, why is irregular is,

234
00:30:02,530 --> 00:30:12,460
you know, is a relationship then and doesn't have to be to make 30, 40, 50 probability.

235
00:30:13,390 --> 00:30:22,980
And if you are original has taken a very strange ship, then you have to have a much larger.

236
00:30:26,200 --> 00:30:33,670
All right. First example, let's consider the case where Y is from a Bernoulli distribution.

237
00:30:34,090 --> 00:30:41,860
So actually, this is the the original example, what people first consider the central limit theorem.

238
00:30:42,280 --> 00:30:46,210
So Y follows a Bernoulli distribution was parameter theta.

239
00:30:46,720 --> 00:30:55,870
In other words, why is binary probability theta being one and probability one minus theta IBM zero.

240
00:30:57,100 --> 00:31:06,760
Then we can calculate the sample mean and essentially that sample mean as we collect and samples from this Bernoulli distribution.

241
00:31:07,210 --> 00:31:12,730
The sample mean is a good estimate of this success rate.

242
00:31:14,090 --> 00:31:18,500
I consider this why I being one.

243
00:31:18,620 --> 00:31:24,950
If you throw a coin, right? If you've got a hat, you know that buy one got a tailored zero.

244
00:31:25,370 --> 00:31:28,519
Then you throw the coin many, many times.

245
00:31:28,520 --> 00:31:32,569
And how many hats you do have if you take the average.

246
00:31:32,570 --> 00:31:40,960
I've that. The probability or the sample mean that I got is a good estimate of the probability.

247
00:31:42,360 --> 00:31:51,100
And we do know that as Y and borrower alternative like we use this peer had since it's really also an estimate of this.

248
00:31:51,680 --> 00:31:58,490
This P theta hat is a good estimate of Zeta.

249
00:31:59,060 --> 00:32:04,040
Then following this central limit theorem, we have the following result.

250
00:32:04,460 --> 00:32:13,730
That is the sample mean minus the population mean divided by the population.

251
00:32:13,730 --> 00:32:17,510
Standard deviation divided by the square root of ten.

252
00:32:17,960 --> 00:32:28,110
What a converge in distribution to the sample. So again, this wine bar, we know the men.

253
00:32:28,350 --> 00:32:39,020
It's what? Wine over here. Some speculation on why I might want to add one over here.

254
00:32:39,470 --> 00:32:43,100
What is the expectation of each white theta?

255
00:32:44,600 --> 00:32:51,980
So expectations. William Barr, as Peter said, a very tough one.

256
00:32:53,520 --> 00:32:58,970
Is one over. Parents apply.

257
00:32:59,370 --> 00:33:07,290
These are random samples. This is one over a variance.

258
00:33:08,220 --> 00:33:12,680
Why is theta moment theta?

259
00:33:13,570 --> 00:33:19,370
Okay. Okay. So this is from. A Bernoulli distribution.

260
00:33:19,760 --> 00:33:26,760
I know that. Me in a virus or this y and bar. Now with the Myanmar barracks we can directly use.

261
00:33:26,760 --> 00:33:36,569
The resulting problem is the central limit zero. Recall that this is the format that we need to use to somehow be minus the population,

262
00:33:36,570 --> 00:33:46,250
meaning for each individual random variable in the random sample divided by the standard deviation of each random sample divided by square root.

263
00:33:46,580 --> 00:33:50,040
And they put all the terms together.

264
00:33:50,310 --> 00:33:56,790
We have this hat which is our population mean minus theta,

265
00:33:57,540 --> 00:34:02,129
which is the sorry p and how it is the sample mean theta is the population

266
00:34:02,130 --> 00:34:07,170
mean the square root of theta times y minus theta is the standard deviation.

267
00:34:09,270 --> 00:34:18,150
And finally this square root of and is the constant that we need to properly scale this sequence over unobservable.

268
00:34:18,750 --> 00:34:29,729
And then from the central limit theorem, we know this this guy converting distribution to send it off and turn it.

269
00:34:29,730 --> 00:34:39,360
That way we can also write it this way. A pen had the sequence of population of sample mean approximately follows a normal

270
00:34:39,360 --> 00:34:48,310
distribution with the me being theta and a variance being theta times one minus theta over.

271
00:34:50,640 --> 00:34:57,580
Okay. So these are equivalent expressions. So this is what the original version of the central limit theorem.

272
00:34:58,450 --> 00:35:02,260
But no. Any question about this form?

273
00:35:08,150 --> 00:35:12,320
So this is the first time that I've ever come across this central limit theorem.

274
00:35:13,220 --> 00:35:20,480
It's probably easier if you go from this expression. Okay, so it's on the left hand side.

275
00:35:20,570 --> 00:35:24,830
That's our sample means. On the right hand side.

276
00:35:25,550 --> 00:35:31,430
You know, the limiting distribution is always not all you need to decide is what is the mean?

277
00:35:31,700 --> 00:35:34,880
What is the balance? A normal distribution. Right.

278
00:35:35,690 --> 00:35:39,620
And to go to me in the balance, just calculate the minimum balance for this population,

279
00:35:40,280 --> 00:35:48,920
for this sample mean right about the sample mean the position of the sample mean it's

280
00:35:48,980 --> 00:35:55,700
going to be put here and the balance of the sample mean it's going to be put here.

281
00:35:57,590 --> 00:36:02,910
So that's the central limit there. And once you have this form.

282
00:36:04,710 --> 00:36:08,610
You can do the linear transformation on both sides of this approximation.

283
00:36:09,670 --> 00:36:13,300
You are to make the right hand side set a novel you made first.

284
00:36:13,390 --> 00:36:17,440
Subtract the myth from that novel. But then you have.

285
00:36:19,790 --> 00:36:31,490
He had a theta approximately fall and now always means zero and embarrassed theta minus theta over.

286
00:36:32,600 --> 00:36:39,130
Right. And then from there, you can further scale both sides of this equation.

287
00:36:39,640 --> 00:36:44,620
Both sides of this approximation, the order to make the right hand side instead of normal,

288
00:36:44,620 --> 00:36:54,040
you need to divide that novel by the standard deviation, which is the square root of this theta five as one minor theta overhead.

289
00:36:54,670 --> 00:37:08,470
So you have to divide the same concept on the left hand side, which gave you we had one theta over square root theta, minus theta over it.

290
00:37:08,960 --> 00:37:12,430
And so that's what you have a left hand side on the right hand side.

291
00:37:12,820 --> 00:37:15,880
After that scaling, you have a set of normal distribution.

292
00:37:16,660 --> 00:37:16,960
Right?

293
00:37:17,440 --> 00:37:30,490
And now instead of approximating y, you can still write approximating, but it can have a more rigorous notation, which is a convergence distribution.

294
00:37:31,750 --> 00:37:36,580
So if I give you the equivalent expression of this central limit theorem, right,

295
00:37:37,330 --> 00:37:45,850
this is the first time I want to recommend you start from that second expression because that's more intuitive, easier to memorize.

296
00:37:46,930 --> 00:37:52,930
So the simple mean, just follow approximate all of this normal distribution with proper median and balance.

297
00:37:53,510 --> 00:38:02,140
I think I'm working backwards to get the original form to figure out what to put on the left hand side.

298
00:38:03,250 --> 00:38:06,520
And then you have a convergence distribution to the stack.

299
00:38:07,290 --> 00:38:16,659
Okay. Does that make sense? All right.

300
00:38:16,660 --> 00:38:22,989
So now we can evaluate the accuracy of this normal approximation, because in this case,

301
00:38:22,990 --> 00:38:31,510
we actually do know the exact distribution for the summation of this random variables, this Bernoulli Right.

302
00:38:32,680 --> 00:38:42,190
Because if a sum of an independent identically distributed Bernoulli random variable as we know it, it follows a binomial random variable.

303
00:38:42,520 --> 00:38:45,540
It follows the dynamic distribution, right?

304
00:38:46,300 --> 00:38:52,690
That's how binomial is defined. The summation of this individual Bernoulli random variable.

305
00:38:53,050 --> 00:39:02,560
So if you want to be the summation of y, I know we have you one follows the binomial distribution with parameter and theta.

306
00:39:06,670 --> 00:39:20,040
So what the previous central limit theorem tells us is that if you think about a unit divided by and last how exactly our sample mean right?

307
00:39:21,370 --> 00:39:27,880
The U.N. is a summation of y if you further divided by ten that's our sample mean so the previous

308
00:39:28,780 --> 00:39:34,450
central limit theorem just tells us what the approximate distribution is for this Y on bar.

309
00:39:36,100 --> 00:39:42,759
Now we know the exact distribution for you and we should also know the exact distribution policy one divided by.

310
00:39:42,760 --> 00:39:47,280
In the other words, we know the exact distribution of this Y and box.

311
00:39:48,250 --> 00:39:58,930
Now we can compare the two. On the one hand, we have that distribution of wine bar from binomial divided by, divided by and.

312
00:40:01,120 --> 00:40:08,680
On the right hand side, we have the approximate distribution of this Y and bar with proper center and the scaling.

313
00:40:08,980 --> 00:40:12,700
We know it follows the normal distribution. All right.

314
00:40:13,120 --> 00:40:16,599
So to put everything together from the central limit theorem,

315
00:40:16,600 --> 00:40:23,889
we know this result you'll approximately follow this normal distribution with a mean being.

316
00:40:23,890 --> 00:40:32,470
GN Time theta varies between theta times one minus theta, and on the other hand, we know the exact distribution of you in which is binomial.

317
00:40:34,250 --> 00:40:40,500
Okay. So now we can compare this to this and say how good?

318
00:40:42,880 --> 00:40:56,200
This approximation is from the Central Limit theorem. Not clear is that is it clear or what we are trying to do here?

319
00:40:57,670 --> 00:41:05,200
Whereas in this particular example, we do not know the exact attribution of a U.N. and around a central limit there.

320
00:41:05,290 --> 00:41:11,680
We also know the approximate distribution of the U.N. what is binomial wise and all for both.

321
00:41:11,980 --> 00:41:15,650
We know there are explicit PDF functions in function.

322
00:41:16,270 --> 00:41:25,570
Right now we can evaluate the city at different cutoffs to see how well that approximation is or different sample size.

323
00:41:27,620 --> 00:41:35,709
Recall that frankly, we have a open question of how large and do we have a good approximation.

324
00:41:35,710 --> 00:41:40,960
Now it takes place on anybody with that problem for this particular distribution, right?

325
00:41:40,980 --> 00:41:43,900
So that's what we are trying to do here. So.

326
00:41:47,610 --> 00:41:58,800
The exact distribution of yuan is binomial so we can evaluate the key function that any x using the key our function for a binomial.

327
00:41:59,250 --> 00:42:05,280
So this is essentially the summation of this of this formula.

328
00:42:05,550 --> 00:42:16,500
And so entries k theta to the k y mark theta to and minus k work from zero all the way to the constant glow all the way to the integer below.

329
00:42:16,500 --> 00:42:23,520
This cut of x or any x last are equal to smallest.

330
00:42:24,360 --> 00:42:32,460
And on the right hand side we have this approximated value which is the CDF function of our normal evaluate at this quantity.

331
00:42:36,140 --> 00:42:40,550
Conceptually, what we should have is a clear function for this particular in our normal distribution.

332
00:42:41,240 --> 00:42:51,380
Everybody that acts that is equivalent to this guy, the CDF function for this standard novel evaluate that this quantity.

333
00:42:53,490 --> 00:43:05,340
And about why that's the case. Okay. So conceptually that should be approximately equal to the CDF function of this normal and theta,

334
00:43:05,790 --> 00:43:21,360
and it might not be to evaluate x and that is equal to this function of this, be it the CDF for.

335
00:43:23,970 --> 00:43:28,100
I don't know. But this is not a hard exercise.

336
00:43:28,120 --> 00:43:38,550
I'll leave it to you to say why the city function for this normal scenario and the data with various end times that

337
00:43:38,820 --> 00:43:48,220
what minor theta evaluate on acts equal to this phi phi evaluated have x minus and theta over and theta time.

338
00:43:48,240 --> 00:43:52,410
So what might happen? Okay, so assume that's true.

339
00:43:52,530 --> 00:43:55,140
Which is which is the case.

340
00:43:56,010 --> 00:44:05,760
All we need to do is for fixed and we would try a bunch of acts and see how well this quantity is approximating this quantity.

341
00:44:06,450 --> 00:44:10,510
How well this two quantities are approximating one another and.

342
00:44:12,490 --> 00:44:19,570
So here is a few numbers that we can we can we can plug in and try.

343
00:44:19,960 --> 00:44:22,060
In particular, if theta,

344
00:44:22,510 --> 00:44:32,680
this success rate for this Bernoulli trial is based to be on quantified and it is fixed to be point five with different sample size,

345
00:44:32,950 --> 00:44:35,260
we can try different X values.

346
00:44:35,830 --> 00:44:44,470
In fact, with each sample size you can try a bunch of x models and see how well approximate the two quantities approximate of one another.

347
00:44:45,550 --> 00:44:56,110
So once again, there we can see that if sample size is small, let's say an equal to a 10th sample size, it's only ten the approximation.

348
00:44:58,000 --> 00:45:11,560
Is not that good. All right. So in particular or perhaps equal to sex, you say the exact probability of you are less than six is point 83.

349
00:45:13,060 --> 00:45:21,040
The approximate probability from the normal approximation is point 74 is close, but not that close.

350
00:45:21,730 --> 00:45:29,140
But as you increase your sample size to 100, for example, that approximation is much closer.

351
00:45:32,020 --> 00:45:38,530
That just tells us as we increase our sample size, that normal approximation becomes better and better.

352
00:45:40,980 --> 00:45:44,550
All right. So that's one observation from this.

353
00:45:44,570 --> 00:45:49,710
I forgot about the last column. Now, let's just focus on the first two columns.

354
00:45:51,150 --> 00:45:57,560
So that's why observation. Okay. So as and becomes larger, approximation becomes fatter and fatter.

355
00:46:00,000 --> 00:46:07,980
That agrees with the central limit theorem because the central limit theorem just tells us as angles to invalidate the normal approximation,

356
00:46:08,760 --> 00:46:12,660
the segments are unavailable. Sure to converge in distribution to that now.

357
00:46:12,930 --> 00:46:16,080
So this makes sense. This agrees with the central.

358
00:46:17,100 --> 00:46:26,640
Okay. So that's why observation. On the other hand, we we can also set the theta to be a different body.

359
00:46:26,850 --> 00:46:36,870
Right. And look at a different distribution with that theta to be point one, which means you have 10% off chance of getting a had a 90% of a chance.

360
00:46:37,140 --> 00:46:40,260
Got an tail. You can repeat this process.

361
00:46:40,470 --> 00:46:47,370
Right. And what do we find there is that if the sample size is ten, again, the approximation is not good.

362
00:46:48,950 --> 00:46:54,350
If the sample size increases, the approximation becomes better, but still not that good.

363
00:46:56,960 --> 00:47:04,960
If your sample size becomes 100, the true probability should be point eight and the approximate probability is point 74.

364
00:47:06,020 --> 00:47:12,740
It's getting better, but not as good as what we have seen for the previous example and why that's the case.

365
00:47:15,740 --> 00:47:19,470
If you compare the baseline. With this line.

366
00:47:25,950 --> 00:47:29,340
Why the second approximation I it out the first line.

367
00:47:33,020 --> 00:47:39,200
Yes. Because with the smaller but the smaller number of other examples.

368
00:47:40,520 --> 00:47:44,810
Are you good? Recall what I mentioned previously.

369
00:47:45,320 --> 00:47:51,350
You ought have a good approximation. Although NGOs invented it, everything's going to be perfect.

370
00:47:52,410 --> 00:48:01,250
Always find out. The more regular the shape of the original distribution is the similar.

371
00:48:01,250 --> 00:48:04,850
And it requires this approximation to be good.

372
00:48:06,860 --> 00:48:12,050
So the top example, we have 3.5, so it's a fair toss.

373
00:48:14,060 --> 00:48:27,650
So if you think about this CDO function, it has 2.8120121 and those have probabilities on five, which is a symmetric distribution.

374
00:48:29,420 --> 00:48:34,460
But course the bottom distribution is quite skewed.

375
00:48:35,330 --> 00:48:41,660
So zero has a probability 0.9 and a one have a probability of point of one.

376
00:48:44,030 --> 00:48:50,240
So compared to the previous one, which is symmetric this way, it definitely last symmetric.

377
00:48:51,540 --> 00:48:56,400
Okay. If it's far from the cemetery, far from a regular distribution.

378
00:48:56,730 --> 00:49:00,300
Then the approximation requires much larger sample size.

379
00:49:01,290 --> 00:49:08,910
So with equal sample size of 100, the approximation of the top one is much better than the lower one.

380
00:49:09,300 --> 00:49:15,690
So that's one. All right. So you can try it after class using you know,

381
00:49:15,690 --> 00:49:24,059
you can you can generate this random numbers from the software like R and try this different

382
00:49:24,060 --> 00:49:32,520
approximations and get a sense of how that n theta and args play a role in this approximation.

383
00:49:33,120 --> 00:49:36,960
You will say that the more symmetric, the more regular distribution is,

384
00:49:37,290 --> 00:49:46,860
the smaller and the smaller n you have to have in order to have a good approximation and vice versa.

385
00:49:47,220 --> 00:49:54,240
If your distribution is high skewed, how do you rather then you have to have a large end in order to have a good approximation.

386
00:49:55,410 --> 00:49:59,250
Right. So this is just a quick example to show that.

387
00:50:01,280 --> 00:50:11,629
The last column is also an approximation because previously we also know that also the distribution is a good approximation of the binomial.

388
00:50:11,630 --> 00:50:16,250
If you are binomial have increasing hand and decreasing p.

389
00:50:17,210 --> 00:50:24,050
In this case we do have that right. So you are you will and has increasing.

390
00:50:24,050 --> 00:50:29,090
That right is increasing and the probability theta is fixed.

391
00:50:29,570 --> 00:50:37,010
But as theta, if you divide the UN by and then that probability is going to shrink.

392
00:50:37,760 --> 00:50:42,920
So anyway, so you can approximate the binomial distribution with a plus arm.

393
00:50:43,220 --> 00:50:46,340
So these numbers are from the fossil distribution.

394
00:50:47,180 --> 00:50:54,410
So you can compare this approximation. And he will say not as Engels eliminated the normal approximation dominance.

395
00:50:54,710 --> 00:51:03,460
Okay, to be closer to the true distribution, which means central limit there really holds.

396
00:51:04,770 --> 00:51:21,380
Okay. All right. So let's take a break and continue to assess the situation.

397
00:51:22,280 --> 00:51:34,970
If you do side effects, maybe, oh, you know your feeling about this.

398
00:51:35,610 --> 00:51:49,950
So that's one of the most difficult things to think about how you can do this.

399
00:51:52,950 --> 00:52:23,990
And you see, I think the last time you can be sure that you feel you can do just three this size,

400
00:52:24,080 --> 00:52:46,040
it still is obviously during the height of the digital age 1000000 what did you do that?

401
00:52:47,380 --> 00:53:07,250
So I think it's reasonable, actually.

402
00:53:12,050 --> 00:53:51,090
And in fact, I was always really happy to do some of these things, but I thought, well, the mission said, What do you want to give me?

403
00:53:51,110 --> 00:54:06,950
A story isn't going to work as a single person.

404
00:54:06,950 --> 00:54:29,330
I'm sure you'll probably see this sort of social events like, you know, the show she was there.

405
00:54:30,260 --> 00:54:58,190
Jordan She was just like, oh, you're not going to you can't take.

406
00:55:01,310 --> 00:55:16,440
So we have a problem with the question in real time.

407
00:55:16,450 --> 00:55:53,980
That was about the place where you have an obligation to do something for the future of creative work.

408
00:55:58,510 --> 00:56:41,350
For those of you who see something like this are true stories of criminals who will no longer be able to office until they want to use the tax code.

409
00:56:43,050 --> 00:56:53,390
I'm sure that sounds good to me,

410
00:56:54,520 --> 00:57:56,140
but I told the commission and I'm telling you that what you should get your children in Russia will tell you all about it.

411
00:57:58,480 --> 00:58:11,210
But how would you say to somebody who needs to work with you?

412
00:58:12,070 --> 00:58:29,010
You could drop something that wasn't true?

413
00:58:31,680 --> 00:58:33,740
You know,

414
00:58:40,690 --> 00:59:27,580
it's something that we always talk about how so many times that's how you account for a lot of things when you have to negotiate a few exceptions.

415
00:59:41,230 --> 00:59:57,480
I mean, on the one hand, nitrogen service, you know, what was a how?

416
01:00:08,370 --> 01:00:59,000
Although Google had a hard time getting up, looks forward to the fact that we've got the Department of Human Services to help us.

417
01:01:07,400 --> 01:01:22,340
We don't want to go into detail because it's so easy to look at this.

418
01:01:22,730 --> 01:01:36,740
Another example of sort of having the similar disclaimer we've been trying to explore to try to verify how close this information is.

419
01:01:36,740 --> 01:01:40,580
Not approximation is in the limited case.

420
01:01:42,940 --> 01:01:51,150
Okay. So in this case, consider why is Amanda Myhrvold from the by now active binomial distribution with parameter and status.

421
01:01:53,300 --> 01:01:55,670
Recall that when we talk about lack of a binomial,

422
01:01:56,300 --> 01:02:04,280
we see that an active binomial actually can be considered as a summation of independent geometric distribution.

423
01:02:05,990 --> 01:02:12,860
That's something that I always have a laptop out of. When we first learned about this that this discard distributions.

424
01:02:18,690 --> 01:02:25,830
Continuous distribution. So why has it why and follows an active binomial distribution with prime theta?

425
01:02:27,150 --> 01:02:33,690
By definition it can be considered as arising from the sum of an independent joe matched with random barrels.

426
01:02:35,470 --> 01:02:47,250
Now the words you can assume there exist x one, two x and then our i the geometric means parameter theta.

427
01:02:48,960 --> 01:02:58,860
Right that's the case. Then your y hand can be written as the summation of I was at one.

428
01:02:59,340 --> 01:03:03,240
You had a backstop. All right.

429
01:03:05,740 --> 01:03:11,050
So because of the relationships that define me on that metric, you can directly rewrite.

430
01:03:11,470 --> 01:03:17,200
It's why I run a variable as a summation of I the geometric rhinovirus.

431
01:03:19,180 --> 01:03:32,520
And I you to have that. This summation of either you run a samples to remind us of, first of all, the large number and second central limit.

432
01:03:33,490 --> 01:03:37,960
Because if you divide this summation by and that's the sum of me.

433
01:03:38,990 --> 01:03:48,040
All right. Once you have a sample, mean always think about whether you can apply this a large number and the central limit theorem.

434
01:03:50,620 --> 01:04:01,480
So that is to say, if you divide a1in you know, that converge in probability to something by this large, large number.

435
01:04:01,790 --> 01:04:09,860
All right. So what is a converge to? Why on earth from negative binomial?

436
01:04:10,070 --> 01:04:13,120
If you divide a y on by end, what does that converge to?

437
01:04:13,120 --> 01:04:29,050
Conversion probability. We should converse to this guy.

438
01:04:29,860 --> 01:04:36,620
The expectation of X, Y or Z is that you might as well get this revision from receipt.

439
01:04:36,790 --> 01:04:48,670
So we know base taxation is one over theta and the variance is one minus theta over the square.

440
01:04:49,810 --> 01:04:59,260
The other words this y and over as converging probability to one over theta and.

441
01:05:00,930 --> 01:05:07,590
So you can prove this using two approaches. One is try to grab the.

442
01:05:11,210 --> 01:05:20,210
A PDF for this a wire overhead from the distribution of narrative binomial and then follow the

443
01:05:20,210 --> 01:05:27,170
definition of convergence in probability to find one that limiting distribution or limiting value.

444
01:05:28,850 --> 01:05:29,989
Or alternatively,

445
01:05:29,990 --> 01:05:39,230
you can use this much simpler approach by noticing that a negative binomial is essentially a summation of a dependency on magic run of animals.

446
01:05:39,650 --> 01:05:46,140
And then you can directly use this week law of large number to get this convergence right.

447
01:05:47,150 --> 01:05:56,810
Once you notice that only the summation of that site Y and over and it just the X and bar the sample mean of this geometric runabouts.

448
01:05:57,530 --> 01:06:04,970
Then I recall the last number. We know that actually converting probability to the population mean of a geometric distribution.

449
01:06:05,660 --> 01:06:10,610
This is one over theta. Okay, so that's the convergence in probability.

450
01:06:10,850 --> 01:06:16,220
We can also think about the convergence and distribution of property normalized.

451
01:06:17,990 --> 01:06:24,830
Sequence. If you multiply square root of y and over n minus this one over theta.

452
01:06:26,630 --> 01:06:30,590
Issue. Commercial and distribution. Normal zero.

453
01:06:31,220 --> 01:06:35,420
An embarrassing one over sata one minus sata over sata square.

454
01:06:36,240 --> 01:06:43,460
Okay. So this is from. Central limit, John.

455
01:06:46,760 --> 01:06:53,330
Yet again. This while overhead is a sample mean for the giant mattress.

456
01:06:54,530 --> 01:07:04,610
If you subtract the population from that central, you multiply it by square root up and that converging distribution to this normal distribution.

457
01:07:07,470 --> 01:07:12,440
So there are different and different ways that you can write about this central myth.

458
01:07:12,810 --> 01:07:19,260
So this is one way. Alternatively, you can put this standard deviation or balance to the left hand side.

459
01:07:19,770 --> 01:07:25,350
So you divide one square root of one minus theta over a square on both side.

460
01:07:25,680 --> 01:07:34,690
Now, on the right hand side, you would have sun. On the left hand side, you would have a, you know, a different formula or alternatively,

461
01:07:34,740 --> 01:07:47,370
a right while or an approximate of a normal distribution with mean being one over and theta and various being one minus theta over a square.

462
01:07:48,420 --> 01:07:52,230
So sorry, one minus theta over and theta square.

463
01:07:54,330 --> 01:08:02,840
So those are all incremental expressions. So anyway, we have this result from the Central Limit theorem and.

464
01:08:07,210 --> 01:08:09,100
I feel like that's the expression.

465
01:08:12,050 --> 01:08:21,470
Why an approximate for this normal distribution was meme B and over theta and various B end times one minus theta over theta square.

466
01:08:22,520 --> 01:08:33,170
Alternatively, we know that why has this explicit distribution has this exact distribution which is not a binomial then similar as before?

467
01:08:33,410 --> 01:08:43,220
We can compare this exact distribution with this approximate distribution and see how well this normal approximate and active binomial.

468
01:08:47,030 --> 01:08:57,230
And in particular if I said to be 100 and theta to be point one, then we have this quantity.

469
01:08:57,950 --> 01:09:00,080
Of course we can evaluate different quantities.

470
01:09:00,380 --> 01:09:12,890
But if you evaluate at 950, then we know the exact exact probability of Y and last hour equal to nine 56.3 or nine and the approximate about it.

471
01:09:13,230 --> 01:09:17,750
Normally it's 1299, which is very close.

472
01:09:18,860 --> 01:09:27,499
All right. And you can try different numbers. So again, the story should be the same as n goes to infinity.

473
01:09:27,500 --> 01:09:31,580
As M becomes larger, this approximation is going to become better and better.

474
01:09:32,480 --> 01:09:36,470
So that's the general story. And this is another example.

475
01:09:40,490 --> 01:09:55,300
Any questions? So in this example, there's a very important technique that you can you can take advantage of your future work.

476
01:09:55,690 --> 01:10:02,110
Is that many of the distribution that I have learned, actually they have equivalent expressions.

477
01:10:02,770 --> 01:10:11,980
The negative binomial can be considered a summation of independent geometric, a chi square, you know, a chi square was ending.

478
01:10:12,010 --> 01:10:20,709
Our freedom can be considered as the summation of an independent chi square with one degree of freedom and so on, so forth.

479
01:10:20,710 --> 01:10:30,760
There are many relations between this different distributions and sometimes noticing that a relation and a race crossing this random variable in

480
01:10:30,760 --> 01:10:41,080
terms of the summation or some calculation of all the right of our host of other I'd run the variables would greatly simplify our calculation.

481
01:10:41,740 --> 01:10:47,110
In this case, if we replace this negative binomial by the summation of a geometry,

482
01:10:48,580 --> 01:10:57,010
we can directly use this large number and simulation theorem to get an approximation or approximate a distribution for this negative binomial.

483
01:10:59,380 --> 01:11:06,070
So this is a very useful technique to keep in mind.

484
01:11:15,230 --> 01:11:20,060
The next one was on distribution, so it was on distribution.

485
01:11:21,140 --> 01:11:28,790
It's why I've followed up on distribution with the lambda that we have this result.

486
01:11:29,280 --> 01:11:40,580
And so the sample mean across all of this pulls out random variables minus this population mean divided by the standard deviation,

487
01:11:41,270 --> 01:11:47,540
and multiply this square root out and will approximate a standard of distribution.

488
01:11:48,420 --> 01:11:51,440
Right? So that's throughout the use of central limit theorem.

489
01:11:52,550 --> 01:11:57,410
Again, the equivalent expression is that Y and bar approximate follow.

490
01:11:57,710 --> 01:12:06,260
This distribution is normal distribution with mean being makes the expectation of the y about which is lambda and the variance

491
01:12:06,260 --> 01:12:17,180
being equal to the variance of why of bar which is lambda over the basis the right to use this sharp central limit theorem.

492
01:12:22,750 --> 01:12:32,530
Now for the binomial case, we can evaluate the approximation of a binomial with some transformation to the normal approximation.

493
01:12:33,040 --> 01:12:35,260
We can also get the.

494
01:12:37,940 --> 01:12:47,300
We know that exact attribution of this, a U.N. in this case, in this person case, because the summation of independent science also doesn't.

495
01:12:47,600 --> 01:12:50,330
That's a property of the distribution. Right.

496
01:12:50,570 --> 01:12:58,280
So similar to the binomial case, we can also evaluate the exact distribution of this summation applied in this case,

497
01:12:59,960 --> 01:13:05,390
the submission to the U.N. We know that exact distribution is a proposal was from the end times.

498
01:13:06,560 --> 01:13:14,900
So as a result from this, we know y and bar approximate all the scenarios and times why and bar which is the

499
01:13:14,900 --> 01:13:23,350
U.N. approximately followed in normal with a mean a lambda and Americans love that.

500
01:13:24,310 --> 01:13:30,100
Okay and here this is it's approximate distribution problem central limit there.

501
01:13:30,740 --> 01:13:33,770
And we also know the exact distribution, which is a plus on.

502
01:13:34,160 --> 01:13:45,080
So again, we can do the same practice to compare how well this normal distribution was approximate I suppose on distribution was probably.

503
01:13:47,250 --> 01:13:54,930
So here is a result. With Space lambda different and different acts.

504
01:13:55,470 --> 01:14:01,890
Here is the approximation from that distribution and from the normal approximation.

505
01:14:02,400 --> 01:14:13,530
Again, as you can say, with increase in sample size, this approximation starts from a pretty bad approximation and increases.

506
01:14:13,770 --> 01:14:18,780
The approximation gets much, much better and goes to 1000.

507
01:14:19,920 --> 01:14:24,820
The approximation is very close. I'll increase.

508
01:14:24,920 --> 01:14:27,940
I keep improving as your end goes to infinity.

509
01:14:29,770 --> 01:14:40,190
And. And in this case, it does require a pretty large end for that approximation to work,

510
01:14:40,190 --> 01:14:47,360
because if you think about the proposal in Korea, it's not very symmetric, right?

511
01:14:47,660 --> 01:14:54,319
Especially if you have small laptop, it's not as high risk.

512
01:14:54,320 --> 01:14:59,670
You'd recall that. What's PD out?

513
01:15:00,850 --> 01:15:07,570
Looks like this. Right.

514
01:15:08,230 --> 01:15:13,030
Only takes one must have this sticky non-active integers.

515
01:15:13,510 --> 01:15:18,790
If your alumni is small, it means the mode and the means is shift to the left.

516
01:15:19,270 --> 01:15:31,110
In this case, if you set alumna to be on one that's extremely skewed with very high probability of zero one and very low probability,

517
01:15:31,110 --> 01:15:37,180
yes, at this run, a variable taking large values is highest scale.

518
01:15:37,960 --> 01:15:43,000
So no honor. You need a pretty large end to have to achieve good approximation.

519
01:15:44,200 --> 01:15:54,219
If you have a rather large lambda, you do not need such a beat and to have a good approximation, probably an equal to 100, but all right.

520
01:15:54,220 --> 01:15:57,580
Give you pretty good approximation if can try that one.

521
01:16:02,560 --> 01:16:12,690
I sense it's because of the relationship between Pakistan and I know there are relationships between the two nations, said I.

522
01:16:12,750 --> 01:16:17,980
Gonzales talked about the relationship between the two.

523
01:16:19,180 --> 01:16:24,280
Does that have any relation to your approximations of the normal distribution?

524
01:16:25,120 --> 01:16:31,460
Not necessarily so. The of the approximations of the normal really comes from the the central limit there.

525
01:16:31,810 --> 01:16:38,800
So it doesn't really matter whether, you know, the original two distributions are approximate the same or not.

526
01:16:39,340 --> 01:16:45,160
An end of the day, as long as you are calculating the limited distribution while the sample mean it always goes to normal.

527
01:16:46,630 --> 01:16:52,300
So in this case it happens to be that person and a binomial have this approximation.

528
01:16:52,690 --> 01:16:56,770
So then you can have a three way approximation. It doesn't have to be.

529
01:16:57,930 --> 01:17:01,750
So if you look at each individually, like, what do we do here?

530
01:17:02,020 --> 01:17:09,500
You still have this approximation. This approximating approximation of the normal has nothing to do with the,

531
01:17:09,730 --> 01:17:15,010
you know, the relation between the plus on the binomial and other questions.

532
01:17:16,570 --> 01:17:30,070
Right. So as I mentioned, central limit, there is a most standard and most basic form of central limit theorem.

533
01:17:30,820 --> 01:17:41,240
And in practice it's not that commonly used, mainly because it's not the condition for the central limits.

534
01:17:41,240 --> 01:17:46,810
There is very restricted recall that we do need either random samples.

535
01:17:47,050 --> 01:17:50,980
Right. And we also need find on me finding a balance.

536
01:17:52,090 --> 01:18:00,440
So if you are, you know, doing some research, then most often you will say that your case would not satisfy both.

537
01:18:00,830 --> 01:18:06,220
If it satisfy both. Most likely it's a trivial case that has already been well-studied.

538
01:18:06,910 --> 01:18:16,390
So that's why people have come up with different balance to relax those conditions similar to the of large number in particular.

539
01:18:17,620 --> 01:18:25,450
There are a lot of variations that are elasticity condition because it is really a very strong requirement in practice.

540
01:18:27,460 --> 01:18:30,480
Most likely you do not have to have independent.

541
01:18:30,680 --> 01:18:34,410
You do not have to have identical distribution,

542
01:18:34,840 --> 01:18:40,810
but rather you can have independent and different means at variance as long as that converges in some sense.

543
01:18:41,590 --> 01:18:52,450
And otherwise, you can also drop this independence assumption by replacing it with some on correlation or even weaker conditions.

544
01:18:54,080 --> 01:19:00,250
Okay. So for example, here is a is an example of a variant of the central limit theorem.

545
01:19:00,580 --> 01:19:06,550
So that's why one two can be independent, random variable, always different meaning a different balance.

546
01:19:07,120 --> 01:19:11,350
So as long as the y i's, they are uniformly bounded.

547
01:19:11,560 --> 01:19:19,960
Namely the absolute value last time has probably one, and the summation of the variance is infinite.

548
01:19:20,920 --> 01:19:29,350
The summation of the variance is infinity. Then we still have this form, this convergence in the situation.

549
01:19:29,920 --> 01:19:42,820
Okay. If you take a look at this form, this essentially the central limit there on form to us is influenced by.

550
01:19:48,350 --> 01:19:53,090
So this as is defined as the summation of Y.

551
01:19:53,570 --> 01:20:03,470
Okay. So that varies the expectation of U.N. in the summation of state of mind and the variance of you.

552
01:20:03,490 --> 01:20:07,160
What is the summation of Sigma Squared?

553
01:20:09,410 --> 01:20:15,580
Okay. I goes from one to. Then you still have this convergence in distribution.

554
01:20:20,950 --> 01:20:26,470
So this is one variant of this central limit which does not require common than a common barriers.

555
01:20:27,370 --> 01:20:31,540
And and that's just why I you do not have to memorize that form.

556
01:20:32,170 --> 01:20:41,650
So long story short, there are different versions sometimes that you can further look into if you are interested.

557
01:20:41,950 --> 01:20:49,540
But it's a it's a it's a very large way to return to explore or this course we only focus on this but that our version,

558
01:20:49,630 --> 01:20:55,090
this passage won't matter as long as you've got to get a handle on this for the class central limit theorem.

559
01:20:55,420 --> 01:21:05,030
That's good enough. So any question before I move on to the next important theorem?

560
01:21:08,560 --> 01:21:15,770
All right. So the main takeaway from this central limit theorem, again, first of all,

561
01:21:15,800 --> 01:21:24,290
we need to be clear what the central government is doing is really trying to find a limiting distribution of a sample.

562
01:21:25,190 --> 01:21:35,120
All right. So that's the fundamental. And in Canada, I think the limiting distribution, even if you do not do proper scaling,

563
01:21:35,900 --> 01:21:42,530
we have seen that if you do not a scaling the sample mean going to converging probability to a constant.

564
01:21:43,520 --> 01:21:47,060
The population mean which is a de jour case.

565
01:21:47,870 --> 01:21:53,860
So we need to properly scale it and the magic scaling parameter is square root of it.

566
01:21:55,100 --> 01:21:59,360
And with that you can think about what? Not normal distribution.

567
01:21:59,540 --> 01:22:02,120
What is the man? What is the balance for the normal distribution?

568
01:22:03,390 --> 01:22:11,840
To me, in the balance that really comes from the ME and Paris for the population and that's what the central nervous system is about.

569
01:22:12,650 --> 01:22:16,310
And again, central limit there applies to any distribution.

570
01:22:17,060 --> 01:22:24,740
And as far as you are concerned, any distribution that is that has foreign media and finite variants.

571
01:22:25,340 --> 01:22:30,440
So the random sample of sample mean has this normal approximation.

572
01:22:31,280 --> 01:22:37,470
So that's central limit there. All right.

573
01:22:37,480 --> 01:22:41,290
So the next one is called the Case Theorem.

574
01:22:42,580 --> 01:22:48,700
The McCluskey theorem does not have to be tied to the central limit theorem,

575
01:22:48,970 --> 01:22:55,450
so it states as follows If I have to a sequence of random parallels one sequence converging distribution,

576
01:22:55,780 --> 01:22:59,110
the other sequence converting probability to a constant.

577
01:22:59,920 --> 01:23:01,600
Then we always have.

578
01:23:02,410 --> 01:23:11,710
The linear transformation of these two sequences will also converge in distribution to the linear transformation of the original element.

579
01:23:13,480 --> 01:23:21,310
Okay, so that's what this is about or any other words to combine in this.

580
01:23:24,610 --> 01:23:32,020
In this most generous form. If you have three sequences and converge in distribution to acts, which is a distribution.

581
01:23:32,740 --> 01:23:42,040
Y and Z are both approximate or converge into any probability to a constant the degenerate case.

582
01:23:42,760 --> 01:24:00,340
Then we have Y and Z times access to this new sequence converge in distribution to this a plus B times x, the same operation as the left hand side.

583
01:24:00,430 --> 01:24:10,640
The same operation for the limit. And so that's what this is film is about, and we will not go over the proof.

584
01:24:10,990 --> 01:24:16,000
But I just want to give you some equivalent expression of the Stauskas theorem.

585
01:24:16,420 --> 01:24:20,470
So what that means is if you have to sequence.

586
01:24:28,290 --> 01:24:39,150
It's it's an converging distribution to x y and converge improbability or equivalent in distribution to this constant C then.

587
01:24:41,470 --> 01:24:51,460
We have the following three results. First, if you do the multiplication of the two, what does that come out to?

588
01:24:57,700 --> 01:25:09,520
The same calculation, the same operation of the limit, which is a constant times this amount of our blacks or the C times the distribution of X.

589
01:25:10,650 --> 01:25:14,840
Okay. So that's first result. Second, if you do, Bernard.

590
01:25:16,240 --> 01:25:27,960
Oh, sorry. If you do addition. It also converting distribution to the addition of this content and the run of our box.

591
01:25:29,010 --> 01:25:37,790
And finally, if you do. I send over wire.

592
01:25:38,010 --> 01:25:43,040
It's the same, but. It's going to be acts overseas.

593
01:25:43,750 --> 01:25:48,190
All right. Or C not point zero.

594
01:25:51,560 --> 01:26:01,380
What is the swastikas serum tells us is that if you have convergence and distribution and convergence improbability for Coulson,

595
01:26:01,650 --> 01:26:07,080
then you can do a leaner transformation on both sides and you still have this convergence and distribution.

596
01:26:08,370 --> 01:26:10,980
So that's what Starsky Serum is about.

597
01:26:13,650 --> 01:26:24,280
And it's often used together with a central limit theorem because in central limit theorem we do have a convergence in distribution, right?

598
01:26:24,720 --> 01:26:34,620
So we naturally have this guy. If you think about the Central Limit theorem, we do have a convergence in distribution.

599
01:26:34,950 --> 01:26:42,390
That is the sample mean with proper scaling and a salary, a convergent distribution to a normal distribution, right?

600
01:26:43,440 --> 01:26:51,819
If at the same time you have another sequence that converting probability to a constant, then you can do the computation.

601
01:26:51,820 --> 01:26:56,520
Now you can combine both to get more flexible result.

602
01:26:57,180 --> 01:27:03,360
So that's why classic is there is often used to gather with the central limit narrow.

603
01:27:06,070 --> 01:27:12,610
Okay. The thoughts of your serum alone does not depend on the Central Theorem.

604
01:27:12,880 --> 01:27:17,770
We always have these results or any sequence of random variables.

605
01:27:19,540 --> 01:27:21,880
Any question about this classical theorem statement.

606
01:27:22,060 --> 01:27:32,650
Before I move on to examples now, let's take a look at an example of how to use this to help us generalize the results from the central limit below.

607
01:27:34,090 --> 01:27:39,910
So assume as I write samples from any distribution with finite me independent variance.

608
01:27:41,440 --> 01:27:49,510
So this I I be from any distribution with a meme you balance sigma squares.

609
01:27:50,740 --> 01:27:54,130
And so from this Laplace central limit theorem,

610
01:27:54,460 --> 01:28:03,100
we directly have this result the square root of and times that and bar the sample mean minus mu

611
01:28:03,340 --> 01:28:11,350
populations divided by sigma population standard deviation well converge in distribution to start at all.

612
01:28:12,700 --> 01:28:18,490
So that's the standard form of central limit theorem or equivalently.

613
01:28:22,350 --> 01:28:28,230
Are approximately follow it normal distribution.

614
01:28:28,590 --> 01:28:31,680
With me being what? What's that mean?

615
01:28:32,880 --> 01:28:39,990
What should I put here? Mehl and Baron's.

616
01:28:44,000 --> 01:28:48,510
All right. Go ahead. Segment squirrel rat. So these are equivalent expressions.

617
01:28:49,530 --> 01:28:55,080
All right. So this is Central American standard vanilla version of a central limit theorem.

618
01:28:56,130 --> 01:29:00,690
Now, the question is, most often we do not know sigma in practice.

619
01:29:02,220 --> 01:29:08,550
All right. So if they want to say, for example, we can collect samples to get this sample mean that's fine.

620
01:29:08,760 --> 01:29:15,540
And we know this n because any seminal size if we want to get a that's a comment about

621
01:29:15,540 --> 01:29:21,540
all this meal the population mean we do need sigma inorder to use this approximation.

622
01:29:21,990 --> 01:29:24,899
But in practice, most of the time we do not know.

623
01:29:24,900 --> 01:29:33,780
Sigma So one thing that people typically do is to replace Sigma by this sample variance or a sample standard deviation.

624
01:29:34,470 --> 01:29:46,110
Recall that as a is as a square is device defined as one over n minus one summation of psi, but R squared.

625
01:29:48,510 --> 01:29:57,420
Okay. So that's the sample variance. But what said, what people typically do is to replace the sigma by this as a.

626
01:29:58,470 --> 01:30:05,430
Then the question is, do we still have this approximation, this a limited distribution?

627
01:30:06,600 --> 01:30:14,040
If they do, then that's great because we can use this to construct comment as intervals of this mean if it does not, then we have a problem.

628
01:30:15,810 --> 01:30:21,000
Then the question is, what is the sentence submission for this new guy?

629
01:30:22,170 --> 01:30:42,830
Okay. What does this new formula the square root of an accent bar by this new format and compares to in distribution.

630
01:30:44,450 --> 01:30:50,450
I'll give you a couple of minutes. Think about how we can take advantage of what we have learned so far.

631
01:30:51,390 --> 01:30:57,350
You got a limiting distribution for this kind. Last time.

632
01:30:57,800 --> 01:31:02,070
We actually do know what this as R-squared compares to its probability.

633
01:31:02,560 --> 01:31:05,660
I see how true that towards the end of the course.

634
01:31:07,210 --> 01:31:12,620
So how do we combine both the limited distribution of new content?

635
01:33:56,130 --> 01:34:12,190
It's. Right.

636
01:34:12,610 --> 01:34:17,050
Got it. Okay. So let's look at this together.

637
01:34:17,560 --> 01:34:26,050
So from last time, we know that this symbol, Barrons converge, improbability is sigma squared.

638
01:34:26,880 --> 01:34:30,160
All right, we have proved that during the week long.

639
01:34:30,160 --> 01:34:34,960
Last number. Think about how it proved that I do it first.

640
01:34:34,970 --> 01:34:41,530
Go from this as a star square with one over and one over and minus one.

641
01:34:42,790 --> 01:34:50,950
And we break down this square term to create a new sequence or a square.

642
01:34:51,800 --> 01:34:55,200
Right. Then we apply this a lot.

643
01:34:55,360 --> 01:35:05,380
Last number five, all square. This new idea roundabout rebels to kind of a limit for that new sequence.

644
01:35:06,520 --> 01:35:19,900
And we use this continuous mapping theorem to get the final result that is as a square as a limiting converging probability to sigma squared.

645
01:35:20,560 --> 01:35:23,100
So that's one we know from last time. Okay.

646
01:35:23,710 --> 01:35:33,100
So now if you think about what we are asking here and compare it to what we already have, there's only a one minor difference.

647
01:35:33,370 --> 01:35:43,350
Right? This guy you can.

648
01:35:47,060 --> 01:35:52,040
Rewrite it first with this term.

649
01:35:52,250 --> 01:36:05,059
That is a sequence that has a limit in distribution times is sigma over asset right and if all the first

650
01:36:05,060 --> 01:36:10,700
part we know that by converting distribution to a standard novel from the central limit there are.

651
01:36:12,110 --> 01:36:18,940
And what do we know about the second one. Does that converge to something?

652
01:36:22,250 --> 01:36:26,990
Yes. Very good converging probability wise.

653
01:36:28,760 --> 01:36:36,310
What they're doing is. The continuous mapping.

654
01:36:36,420 --> 01:36:42,340
Very good continuous mapping. Right, because as a square converging probability, two sigma squared.

655
01:36:42,700 --> 01:36:48,580
Then here it's just a continuous function of our sense of square.

656
01:36:49,630 --> 01:36:54,360
Just take the square root and then take sigma over that square root.

657
01:36:55,330 --> 01:37:00,010
So it's a continuous function and you can apply the same continuous function to the limit.

658
01:37:01,140 --> 01:37:04,900
That's exactly continuous mapping. Right. So it's a continuous mapping theorem.

659
01:37:05,200 --> 01:37:09,400
We know the second line converge improbability two one.

660
01:37:10,300 --> 01:37:16,510
Now we have two sequences one converging distribution, the other commerce in probability to a constant.

661
01:37:17,650 --> 01:37:20,950
And now we can use this loss with zero, right?

662
01:37:21,100 --> 01:37:27,190
If you have two sequences one, converging distribution, the other converging probability,

663
01:37:27,580 --> 01:37:34,060
then we know if you multiply both and also converge in distribution to.

664
01:37:36,280 --> 01:37:41,260
The distribution times. The constant. So that just give us.

665
01:37:42,640 --> 01:37:45,070
The same standard of normal distribution.

666
01:37:46,120 --> 01:37:57,880
So what that tells us is that even if we replace this true standard deviation sigma by the sample standard deviation and we still have the same limit.

667
01:37:59,020 --> 01:38:02,080
Right. Is still converging distribution to Staten Island.

668
01:38:03,850 --> 01:38:09,700
So that's a that's a very good result meaning that we can replace sigma by as N and use

669
01:38:09,700 --> 01:38:15,760
this to construct comment incentives or some topic confidence intervals are a population.

670
01:38:16,930 --> 01:38:23,800
So everything is known except this means all of these can be obtained from your sample.

671
01:38:24,310 --> 01:38:30,910
So you can construct asymptotic come in and you wish you all or more about in that semester.

672
01:38:31,310 --> 01:38:39,130
Okay. Now all we need to know is that this guy still converting distribution to the same limit, which is instead of normal.

673
01:38:40,210 --> 01:38:48,520
So that's how you can use Lasky Theorem to generalize the results from the central questions.

674
01:38:51,440 --> 01:38:57,500
All right. So let's take a look at a couple more examples of how to use the Central Limit Theorem, Anastas Theorem.

675
01:38:58,790 --> 01:39:07,460
So the next example, consider you have a high sample, Assad that followed standard normal distribution.

676
01:39:08,560 --> 01:39:22,790
Okay. So first question, what is the limiting distribution where some content distribution blocks are just directly apply the central limit there?

677
01:39:23,450 --> 01:39:33,140
And so. Write down this result, see whether you can directly write out what that limit in distribution is or as far.

678
01:39:34,890 --> 01:39:39,870
Okay. What is this Saturday? Distribution is my spot.

679
01:39:40,230 --> 01:39:50,520
When I say I started distribution again, what I mean is that you have to properly center and the scale this X bar the final limited distribution.

680
01:39:51,630 --> 01:40:02,730
Whenever you hear the word asymptotic, it means you have to properly center and scale the parameter to garner the limited distribution.

681
01:40:25,160 --> 01:40:33,410
So directly applying the central limit there on what should we. How should we shift this parameter?

682
01:40:35,790 --> 01:40:43,040
I need to ship that from her. We need to shift this amount of arable.

683
01:40:45,240 --> 01:40:49,350
But you're not right, because the means zero population means zero.

684
01:40:49,360 --> 01:40:52,960
So if you want to shift your shift by zero, which means you do not shift.

685
01:40:53,860 --> 01:40:58,030
And what about the the scary and the square root of it?

686
01:40:58,570 --> 01:41:02,770
That's the magic number, right? And with that, you come right out.

687
01:41:04,730 --> 01:41:13,460
The limit is going to be an zero one because here one is the population barons.

688
01:41:14,820 --> 01:41:24,380
In this case is a special case which tells us that the square root of an entire tax bar converting distribution to stand alone.

689
01:41:25,160 --> 01:41:40,670
In fact, it's not even convergence because any, you know, square and transaction bar always exactly follow.

690
01:41:41,510 --> 01:41:46,040
It's not a normal distribution. Why is that?

691
01:42:00,850 --> 01:42:04,720
As our eyes eye our idea from the standpoint of.

692
01:42:05,970 --> 01:42:11,580
So we have talk about any linear transformation of now is still not right.

693
01:42:12,090 --> 01:42:17,790
So this X bar is a linear transformation is a linear transformation.

694
01:42:19,440 --> 01:42:25,110
So the bar should follow a normal distribution. Then alternating side is what is the mean?

695
01:42:25,110 --> 01:42:33,900
What is the variance? Right. And if I as far as I are, start a normal, then the ME $0 is one.

696
01:42:35,810 --> 01:42:42,470
So that's why the square root of and contacts are always following a standard normal distributions.

697
01:42:44,330 --> 01:42:49,790
And of course, it goes to infinity itself out all standard, all the distribution.

698
01:42:51,380 --> 01:43:00,140
So this is a special case. And to some extent, it also tells us the relation between the sample size and the approximation.

699
01:43:00,440 --> 01:43:13,590
If you are original run, the original distribution is a standard novel that does not need to be large actually, and can be anything.

700
01:43:13,610 --> 01:43:17,360
You always have this perfect approximation, right?

701
01:43:18,830 --> 01:43:23,450
But if your original distribution deviates from the normal distribution,

702
01:43:24,110 --> 01:43:31,100
the more it deviates from the normal distribution, the larger an error will require to get a good approximation.

703
01:43:32,390 --> 01:43:37,550
So this is a extreme case where the original distribution is a normal distribution.

704
01:43:37,970 --> 01:43:44,120
So this approximation of the exact okay with any hand is always exact.

705
01:43:45,920 --> 01:43:50,240
Okay. So that's question number one. Question number two.

706
01:43:51,980 --> 01:43:59,030
If we denote one to be one over summation of I Square.

707
01:44:00,080 --> 01:46:15,840
What is that? Syntactic distribution? Paul Wyatt. That's how to tackle this problem.

708
01:46:20,520 --> 01:46:32,830
But yes. Are you asking to for the entire distribution center, not a distribution compliance school.

709
01:46:33,350 --> 01:46:41,930
Again, when we say asymptotic distribution, we mean you have to properly center and scale this way to find a limited distribution.

710
01:46:44,430 --> 01:46:49,860
How do I do it? She chooses an exquisite short skirt.

711
01:46:51,660 --> 01:46:56,190
Very good. So we are to use some totty so far.

712
01:46:57,060 --> 01:47:00,120
Basically, there is only one technique that we have learned so far.

713
01:47:00,130 --> 01:47:07,530
That is the central element there. Right. If we can create a central image theorem, it's going to create a some omen.

714
01:47:07,980 --> 01:47:13,320
Then we know the magic scaling parameter. How about the kind of limiting distribution?

715
01:47:14,040 --> 01:47:19,590
That's pretty much the only technique that we have learned to give us the limited distribution.

716
01:47:20,440 --> 01:47:32,850
Right. So then if you look at this form, it should remind us of you can create a sample from the sample.

717
01:47:34,020 --> 01:47:42,180
If you treat this eye square as a new round marble, then this is essentially the sample mean that you run a marble, right?

718
01:47:42,720 --> 01:47:55,140
So for example, if you know this Z is quite as high square by Z, this one is exactly Z and bar the sample mean for Z.

719
01:47:56,400 --> 01:48:09,060
Then from the sample from the central limit theorem, we know we can have square it up and z bar minus three is about Hazen or Z.

720
01:48:09,570 --> 01:48:15,840
I divide by the square root and Alvarez of the Z i.

721
01:48:19,030 --> 01:48:24,460
Welcome to normal distribution. In distribution, right.

722
01:48:24,640 --> 01:48:32,340
This is directly from the central limit zone. But then all we need to do is figure out what is this CCI?

723
01:48:32,710 --> 01:48:44,830
What is the variance of the. At least we know we can get a limited distribution for this wire use in the Central Park.

724
01:48:45,410 --> 01:48:50,970
So always the first step is trying to construct the sample mean so that we can take advantage of central.

725
01:48:52,900 --> 01:48:59,140
Now the problem is, what is the ECI? And apparently now we have to look at how the eye is defined.

726
01:49:01,450 --> 01:49:05,820
So the eye is defined as eyes square. If I say, is that a novel?

727
01:49:05,830 --> 01:49:08,920
What do we know about Oxide Square? Yes.

728
01:49:09,520 --> 01:49:14,170
Square is Chi Square, right? Is Chi Square was developed by number one.

729
01:49:15,040 --> 01:49:20,110
So a z eye is from Chi Square with the growth rate of one.

730
01:49:20,740 --> 01:49:24,460
And if you look at the distribution, you know, the variance.

731
01:49:24,520 --> 01:49:30,970
The man is equal to the degree of freedom and the variance is two.

732
01:49:32,290 --> 01:49:36,100
So that's something that I can calculate from this Chi Square distribution.

733
01:49:36,430 --> 01:49:42,280
Now I can plug it into these quantities to go to their sanitary distribution point.

734
01:49:43,420 --> 01:49:51,280
So again, always first up. Think about how you can construct a simple mean so that it can use the central limit there.

735
01:49:52,450 --> 01:49:59,560
All right. So one more question I can leave for you to think about.

