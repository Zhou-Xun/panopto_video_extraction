1
00:00:00,610 --> 00:00:11,770
It looks like the government is pulling in so much grant money.

2
00:00:12,090 --> 00:00:28,950
People are very, very interested in trying to make sure we have not just enough people that we're not going to help in your population right now.

3
00:00:31,110 --> 00:00:36,600
Not a lot is known about it. All right. Why don't we get started since we have a glass half full of the glass here?

4
00:00:37,280 --> 00:00:43,680
I know it's going to take coming back and not even.

5
00:00:43,980 --> 00:00:53,910
All right. Good afternoon. And those of you that are here, a lot of the rest of you who are not going to know office hours next week.

6
00:00:55,170 --> 00:01:00,270
Well, great is Monday. Tuesday, my office hours are Monday, Tuesday.

7
00:01:00,990 --> 00:01:04,740
I'm basically booked the rest of the week. I've tried really hard.

8
00:01:05,610 --> 00:01:12,090
If that's going to be a problem for someone, if you really wanted to ask some questions next week, please email me.

9
00:01:12,810 --> 00:01:16,440
You can beg for a time to see me in person. I'm happy to do that.

10
00:01:17,310 --> 00:01:20,520
But formally, that we won't have someone sitting waiting for people to come.

11
00:01:20,550 --> 00:01:29,740
All right. So no office hours because of fall breaks. Some Hung's proposal was next Thursday.

12
00:01:29,750 --> 00:01:35,180
If you want to wish him some luck on his proposal, you'd appreciate that, I'm sure.

13
00:01:35,870 --> 00:01:42,830
Homework number three. And it's still the Wednesday after break.

14
00:01:47,800 --> 00:01:53,140
Everybody's cool with that. You have basically everything you need to do the homework already.

15
00:01:53,680 --> 00:01:57,370
I hope you don't say that until the Wednesday after break to get it done.

16
00:02:01,150 --> 00:02:07,210
Four is due two weeks from today.

17
00:02:09,700 --> 00:02:16,180
I'm going to put a big question mark on that, that they're not sure that's going to be possible either.

18
00:02:16,300 --> 00:02:23,350
So we'll keep that in check. I'll get back to you as depending on how we do today and Friday.

19
00:02:29,440 --> 00:02:35,350
All right. Questions about wintry mix models that I'm still not making clear to folks.

20
00:02:37,300 --> 00:02:49,360
Piece of cake. You got a question in my office hours that I think is a valuable one for everybody to think about.

21
00:02:51,040 --> 00:03:02,930
The question is, is why am I fitting? I.

22
00:03:03,240 --> 00:03:14,490
I'm sitting close to data and sitting for the next model.

23
00:03:14,700 --> 00:03:21,490
Linear. Next model. So the same day that.

24
00:03:23,440 --> 00:03:27,910
What's the point in having me spend all that time on Homer too, and then use the same Homer?

25
00:03:27,970 --> 00:03:36,030
Same. Get it over again with three. So implicitly I was hoping that you would one compare.

26
00:03:39,330 --> 00:03:44,850
Or at least think. The results.

27
00:03:44,850 --> 00:03:55,000
You got results. Two over three.

28
00:03:56,640 --> 00:04:02,980
Informally ask you in all of our assignments you go back and compare and contrast coefficient estimates or result.

29
00:04:04,630 --> 00:04:07,850
But. You know.

30
00:04:09,210 --> 00:04:15,340
If you used. If you used Chelsea.

31
00:04:19,140 --> 00:04:24,140
With compound symmetry. Correlation.

32
00:04:28,490 --> 00:04:38,830
But should you probably get it from number three? What model are you going to probably come up with in homework?

33
00:04:38,830 --> 00:04:42,190
Number three. Random intercept.

34
00:04:43,390 --> 00:04:48,930
You probably. If a random intercept model.

35
00:04:56,910 --> 00:05:01,950
What's. Number three.

36
00:05:03,390 --> 00:05:06,390
Can was normal data. Only with normal data.

37
00:05:07,760 --> 00:05:12,530
Compound symmetry with glass is the same as a random inner set model and a linear, mixed model approach.

38
00:05:16,070 --> 00:05:24,290
And if you came up with maybe using an air one structure. INGLIS Then you'll probably.

39
00:05:30,410 --> 00:05:33,740
I remember slope intercept model.

40
00:05:40,250 --> 00:05:44,630
Number three and homeworks number two.

41
00:05:46,690 --> 00:05:55,629
Random slopes also produce correlation among the data that it decreases over time, not strictly speaking, the same way as glass.

42
00:05:55,630 --> 00:05:59,110
What they are, one does, but they have the same sort of flavor.

43
00:06:00,130 --> 00:06:06,190
It doesn't mean you have to use a random intercept model in homework three if you use common symmetry,

44
00:06:06,550 --> 00:06:12,610
but you should see the connections between the two. If you chose a random intercept model in homework.

45
00:06:12,610 --> 00:06:17,799
Number two, you should get sorry if you chose compound symmetry in homework.

46
00:06:17,800 --> 00:06:22,750
Number two, you should get exactly the same results with a random intercept model.

47
00:06:24,130 --> 00:06:29,680
I'm not talking about approximately. They should give you the exact same coefficient estimates, the same standard errors.

48
00:06:30,790 --> 00:06:38,379
If you said they are one in over of number two, you will get similar answers with with a random intercept and slope in homework.

49
00:06:38,380 --> 00:06:41,410
Three You won't necessarily get a you shouldn't get exactly the same answers.

50
00:06:42,900 --> 00:06:49,620
But they are related to each other. So implicitly you might want to check that out on your own free time.

51
00:06:50,910 --> 00:07:05,630
But there is more to it than just playing around with data. So let's talk about Shalit's generalizes square says that a vector of outcomes.

52
00:07:07,740 --> 00:07:13,469
Is equal to a fixed heart. The mean structure calmness is a designed matrix.

53
00:07:13,470 --> 00:07:19,770
This is a vector plus aluminum. Except my ease and my epsilon from my notes.

54
00:07:22,960 --> 00:07:25,810
There's a mean component and then there's noise around that.

55
00:07:27,190 --> 00:07:41,590
And we believe that this noise within person has some correlation structure attached to it, whereas in a linear, mixed model.

56
00:07:43,390 --> 00:07:49,900
You say that that factor of outcomes again, I should put here, I want to put data, she tells us.

57
00:07:51,570 --> 00:07:57,760
And there's a Veda Elmer here. Plus we see eye to eye.

58
00:07:57,780 --> 00:08:01,409
This is a matrix. This is a vector. Vector.

59
00:08:01,410 --> 00:08:06,190
That's a matrix. Plus a factor of noise.

60
00:08:06,610 --> 00:08:16,180
So now we've taken the variability and we've broken it up into two pieces between subject variability and then what's left over within a person.

61
00:08:17,200 --> 00:08:24,410
And so, again, this could have any structure you want. But we typically say it's kind of swim independence at this point.

62
00:08:24,440 --> 00:08:28,580
So it's sigma squared times an identity matrix for the variance of the errors.

63
00:08:29,300 --> 00:08:36,560
And then of course, we have assumed that this again is mean zero, the matrix and this vector we call the DE.

64
00:08:41,350 --> 00:08:54,430
These are two different approaches to modeling the same data. What's the interpretation?

65
00:08:54,610 --> 00:09:01,490
Interpretation? She was.

66
00:09:08,520 --> 00:09:12,930
What's the interpretation of the coefficient on x in the glass model?

67
00:09:17,080 --> 00:09:20,370
For Sanchez this time. What's the interpretation of better?

68
00:09:20,370 --> 00:09:29,770
Is there somebody? Anybody.

69
00:09:31,500 --> 00:09:34,889
How do I interpret data in a regression model? Average change.

70
00:09:34,890 --> 00:09:39,150
Log on to time. Okay. So when you do change in X effects this time.

71
00:09:41,230 --> 00:09:47,820
So thanks this time. And data is the change.

72
00:09:51,810 --> 00:10:02,640
An average white. Home to one unit change units that it got bad fast.

73
00:10:06,610 --> 00:10:14,070
Due to. She?

74
00:10:18,360 --> 00:10:23,879
So every time it's changes on average why changes by X James Spader procedures

75
00:10:23,880 --> 00:10:29,250
they did excuse me when you the change in X can you just read that and zero.

76
00:10:29,250 --> 00:10:35,940
No, no I'm sorry you can't read any of that. Even so, that word is change.

77
00:10:36,870 --> 00:10:48,270
So I said X is time. So if expert time, x is time and data is the change in average y due to a one unit change in time.

78
00:10:56,700 --> 00:11:01,880
What's the interpretation? Of data.

79
00:11:02,590 --> 00:11:09,200
How am. There's a one unit change in X.

80
00:11:11,330 --> 00:11:18,520
What's the interpretation of data, L.M.? Is it the same as bitter glass?

81
00:11:23,980 --> 00:11:28,540
I'm going to Professor is a question like that. Usually the answer is no.

82
00:11:30,190 --> 00:11:35,450
So the interpretation of the LMM is not the same as it is for the patent.

83
00:11:35,450 --> 00:11:39,349
She let's. The interpretation here of data.

84
00:11:39,350 --> 00:11:42,500
Remember, whenever you have a multiple linear regression model,

85
00:11:42,740 --> 00:11:46,639
the interpretation of that beta coefficient is conditional on everything else in the model.

86
00:11:46,640 --> 00:11:50,000
Right after adjusting for this is what X does.

87
00:11:51,770 --> 00:11:54,830
So it's conditional on someone's I.

88
00:11:56,180 --> 00:12:00,110
This is a one unit change due to time.

89
00:12:01,540 --> 00:12:05,740
Right. For a person with those random effects.

90
00:12:06,730 --> 00:12:19,960
So it's a subject, specific interpretation. So again, if X is time and beta, that is the change.

91
00:12:22,210 --> 00:12:26,320
An average. Why?

92
00:12:27,900 --> 00:12:33,390
2 to 1. So it looks the same. Change in time.

93
00:12:38,000 --> 00:12:44,050
Among those same people.

94
00:12:44,630 --> 00:12:47,750
And that's usually one person. Right. That person. Has there be a.

95
00:12:50,460 --> 00:12:55,050
So it has what we call a conditional interpretation or a subject specific interpretation.

96
00:12:56,430 --> 00:13:03,570
It's not one a unit change across the population. It's a one unit change after you hold control for that person.

97
00:13:04,020 --> 00:13:13,460
So that person's random effects. So there is a difference between girls and glamor.

98
00:13:13,510 --> 00:13:20,140
Besides just thinking about random effects or correlation structures, the interpretation of the regression parameters is different.

99
00:13:20,530 --> 00:13:28,390
Do you want to talk about a one unit change across the entire population, or do you want to talk about a one unit change within a person?

100
00:13:28,930 --> 00:13:35,620
Right. Now one of the beauties of normally distributed data.

101
00:13:41,860 --> 00:13:58,410
Do this. I want. See, there are things again in 601 that are that we actually do teach you because they have some utility.

102
00:14:01,610 --> 00:14:06,900
The expected value, the marginal expectation of something is the expectation of a contingent conditional expectation.

103
00:14:08,820 --> 00:14:12,510
So as I told you on an element, we expect value of y given by.

104
00:14:14,390 --> 00:14:18,080
Is that the same data as CIPA?

105
00:14:19,010 --> 00:14:22,550
It's error. So he.

106
00:14:27,520 --> 00:14:29,350
Because the everything else doesn't mean zero.

107
00:14:29,830 --> 00:14:36,610
But you're left with this next time because everything is linear here, and I should be more explicit here.

108
00:14:37,360 --> 00:14:44,770
This is element. I'm talking about the element. The expected value of Y is the expected value of its conditional mean.

109
00:14:45,310 --> 00:14:49,500
Which is that. Which is this?

110
00:14:51,380 --> 00:14:57,800
Because everything's legit. Yes, go ahead. Go ahead. We talked about beta being like a population level effect.

111
00:14:58,310 --> 00:15:03,430
So why is it specifically conditional on the rate?

112
00:15:03,690 --> 00:15:07,250
The interpretation is conditional on this isn't a population.

113
00:15:08,270 --> 00:15:17,210
Okay. Oh, it's yes. For the it's if you want to interpret it my way I changes by one unit for X after this is in there.

114
00:15:17,570 --> 00:15:27,480
Okay. Now things get a little fuzzy in normally distributed data, and I think her question is a good one.

115
00:15:28,590 --> 00:15:32,070
I just told you that beta element has this conditional interpretation.

116
00:15:32,100 --> 00:15:36,300
But notice what happens in the expectation. The marginal mean of why I.

117
00:15:37,880 --> 00:15:40,900
Is also a one unit change in its time space to help out.

118
00:15:42,020 --> 00:15:47,240
So with normal data, the coefficients are the same, they have the same interpretation.

119
00:15:48,170 --> 00:15:51,620
And this is only because everything is linear and everything is normal.

120
00:15:51,980 --> 00:15:56,510
I mean, we could go through some intervals here and everything comes out of the interval sort of sort of idea.

121
00:15:57,680 --> 00:16:03,020
So we have a population average model, a marginal model, three goals.

122
00:16:03,230 --> 00:16:09,770
We have this subject specific type of approach with with linear mixed models for normal data.

123
00:16:10,370 --> 00:16:12,440
That distinction is really irrelevant.

124
00:16:12,740 --> 00:16:23,120
The beta hat from GLS and the beta half from LMM have the same interpretation because of this fact the linearity and everything comes out nicely.

125
00:16:24,350 --> 00:16:27,860
This will not happen with non normal data.

126
00:16:28,970 --> 00:16:37,000
Whether you want to fit a glass model with binomial data or you want to find a linear mixed model that two things no longer become the same.

127
00:16:37,010 --> 00:16:41,720
There is a population interpretation and there is a subject specific interpretation

128
00:16:42,170 --> 00:16:50,430
and the data hats from the two approaches will be slightly different. Yeah, but besides is an interesting case too.

129
00:16:50,550 --> 00:16:57,800
But so theoretically you have to ask yourself do I want a population average estimate?

130
00:16:57,920 --> 00:17:02,240
Or do I want sort of this estimate after controlling for subjects specific factors?

131
00:17:03,500 --> 00:17:07,700
And with normal data, that question doesn't come up because it all washes out in the integration.

132
00:17:08,840 --> 00:17:13,220
All right. So why do I have to fight these two?

133
00:17:14,390 --> 00:17:18,530
Because eventually it's going to become important. It isn't necessarily so important right now.

134
00:17:19,400 --> 00:17:24,230
The two things do the same thing. However, there are unique features to each of these approaches.

135
00:17:26,000 --> 00:17:37,180
So with Jill's challenge. If you want an estimate of the within person correlation.

136
00:17:40,630 --> 00:17:48,480
This will give it to you very nicely. You can derive it from a linear, mixed model, but it takes some algebra.

137
00:17:51,670 --> 00:17:59,770
I don't think there's any other benefit to glass. You're going to see this maybe today in our code.

138
00:18:00,310 --> 00:18:07,260
No, it's going to be Friday. It's an algorithm that's more likely to converge.

139
00:18:08,040 --> 00:18:14,500
And this is going to become especially important with now normal data. It's really hard to estimate variance components,

140
00:18:15,070 --> 00:18:19,660
random intercept variance and a random slope variance and a covariance between the random intercept the random stuff.

141
00:18:22,090 --> 00:18:26,440
I have very rarely ever run into non convergence with a glass model.

142
00:18:27,790 --> 00:18:31,630
Because it's just estimating a correlation parameter, and that's usually pretty straightforward.

143
00:18:32,290 --> 00:18:40,480
So I typically go to a gloss model simply because I know that things are likely to converge and I'm not going to run into problems.

144
00:18:40,720 --> 00:18:45,900
That's not a reason to do it, although that's the reason I do it right.

145
00:18:46,060 --> 00:18:47,290
Think about the model you're setting.

146
00:18:47,950 --> 00:18:57,850
But again, in practice, in practical terms, do I care about beating out from a linear mixed model or a mixed model approach versus a less approach?

147
00:18:58,630 --> 00:19:02,350
Probably not in practical terms, but in theoretic terms.

148
00:19:03,160 --> 00:19:05,590
These two are doing slightly different things with non-trivial data,

149
00:19:05,590 --> 00:19:10,960
but g less gives me a nice quick way to spin correlation without having to deal with random effects.

150
00:19:13,900 --> 00:19:21,670
Linear, mixed models. A love for computation.

151
00:19:25,180 --> 00:19:31,290
Predictions. Of random intercepts.

152
00:19:37,180 --> 00:19:45,040
And so should you want them if you can get them. So if you want to figure out again, what do people's random intercepts look like?

153
00:19:46,960 --> 00:19:50,530
Elements allow you to do that? Glass does not it doesn't give you that.

154
00:19:50,920 --> 00:20:01,590
It's a different really different approach. When I say nice because that's a really scientific term.

155
00:20:02,640 --> 00:20:12,110
Nice interpretation. A random intercept slope model just says there's a population average line and

156
00:20:12,110 --> 00:20:16,010
then everybody has a line that is above or below it and may shift in slope.

157
00:20:16,880 --> 00:20:20,540
Everybody gets their own intercept and slope. I think a lot of folks like that.

158
00:20:20,690 --> 00:20:27,530
A lot of noncitizen folks like that idea to give us that it all seems more abstract.

159
00:20:27,530 --> 00:20:32,749
But anyways, so there is not a better model.

160
00:20:32,750 --> 00:20:37,790
I don't want to say that one of these is better than the other. They're different. They have different approaches and different goals.

161
00:20:39,650 --> 00:20:44,750
In the end, if you're looking for a group effect or a time effect, either approach is going to probably give you the same conclusion.

162
00:20:45,530 --> 00:20:50,510
If there's a strong signal, so please don't think that one is better than the other,

163
00:20:51,260 --> 00:20:56,930
but they both are different approaches and they have different aspects to them depending on what you want to do.

164
00:20:58,580 --> 00:21:02,120
And you may just say that. Kyra.

165
00:21:02,370 --> 00:21:09,930
I don't care. All right. I'll correlate correlation. So they're going to ask me to say that I want to thank them all.

166
00:21:10,140 --> 00:21:15,310
You may say that. All correlation. Is nuisance.

167
00:21:20,200 --> 00:21:23,320
I don't want him out all the time. I don't want a correlation coefficient.

168
00:21:23,350 --> 00:21:27,610
I don't care about people's random intercepts. I just want to get a good standard error.

169
00:21:30,460 --> 00:21:33,940
So then you just go, Oh, well, this plus the sandwich.

170
00:21:37,660 --> 00:21:41,940
Standard error. Sandwich Theater.

171
00:21:44,080 --> 00:21:51,940
Any one of those approaches again. They're all going to lead to consistent, unbiased estimation of the mean effects.

172
00:21:53,320 --> 00:21:58,630
And they're all changing how we're dealing with the covariance structure of the data or the errors or however you want to think about it.

173
00:22:00,160 --> 00:22:08,810
So any one of those is feasible. But remember again that there is a weight matrix.

174
00:22:08,820 --> 00:22:15,480
These two methods are putting a weight matrix and they remember it's Exide transpose sigma inverse x.

175
00:22:16,590 --> 00:22:23,040
So these two are coming in one estimation through natively squares and all else is not.

176
00:22:24,090 --> 00:22:30,540
So they're not going to give the same answer, but across right across many, many samples, they're both consistent.

177
00:22:33,000 --> 00:22:37,170
So just think about what you want to do here. All three are feasible.

178
00:22:39,210 --> 00:22:46,430
I don't know what you have in 699 if you do get it usually is repeated measures project and 699.

179
00:22:48,750 --> 00:22:51,900
No. Think the professors would be thrilled with the last approach.

180
00:22:53,370 --> 00:22:56,250
They think they want to see that you know how to use these methods.

181
00:22:57,870 --> 00:23:04,409
So anyway, go forth and six out of nine, if there's a, if there's a correlated data project, I suppose,

182
00:23:04,410 --> 00:23:07,709
I don't know if you said I'm going to some independence and you which senators because Dr. Brown

183
00:23:07,710 --> 00:23:14,310
said so I think gentlemen reprimand me and say well no I think you should fit a linear mix model,

184
00:23:15,170 --> 00:23:23,970
give it a try. But when you're out in practice, remember that all of these are at your disposal and they all are suitable approaches.

185
00:23:24,780 --> 00:23:31,139
All right. So that was my attempt to kind of talk about the why here and why it's two different approaches.

186
00:23:31,140 --> 00:23:32,840
They kind of overlap and that kind of don't.

187
00:23:34,020 --> 00:23:41,730
And they have different features that are useful to the study and hopefully it'll become more apparent as we start talking about G next week.

188
00:23:43,410 --> 00:23:53,640
Can I ask one? You may serious. If your research question is asking about like population average, you don't really care about the individuals.

189
00:23:54,120 --> 00:23:57,810
Mm hmm. Do you care about estimating the correlation?

190
00:23:58,800 --> 00:24:04,050
Is there any other reason you might choose to do an alum, or is it as simple as that?

191
00:24:04,050 --> 00:24:08,280
Like you should just do deals? I think it's just as simple as that's my goal.

192
00:24:08,280 --> 00:24:17,680
And Jealous answers that goal for me. But remember that if if you fit a random intercept model, you can compute the correlation, right?

193
00:24:17,700 --> 00:24:22,380
It's the ratio of one variance component divided by the sum of the two, just like that.

194
00:24:24,210 --> 00:24:27,030
But it doesn't just come out in the wash right away. Right. So.

195
00:24:32,220 --> 00:24:39,840
So what we're going to see shortly is that so GE If you haven't seen before and you probably shouldn't have, why would you be here?

196
00:24:41,220 --> 00:24:44,490
Generalize estimating equations is a marginal modeling approach.

197
00:24:44,640 --> 00:24:48,810
It's goals for not normal data, but of course we had to give it a new name.

198
00:24:49,890 --> 00:24:56,190
So it's called G. G. LMM is a random effects approach with no normal there.

199
00:24:56,700 --> 00:25:04,140
So you have logistic regression with goals added to it or logistic regression with very random effects put into the model.

200
00:25:07,100 --> 00:25:15,980
What I what I often see in published research is someone will say, we found G was a random intercept.

201
00:25:17,820 --> 00:25:21,030
No, you didn't. G is a marginal approach.

202
00:25:21,030 --> 00:25:23,120
There aren't all random effects in G,

203
00:25:23,880 --> 00:25:30,900
but what they're what they're thinking about is that a random intercept model looks like exchangeable correlation.

204
00:25:31,620 --> 00:25:39,809
And so it starts to get mixed up in people's heads anyway. So just keep that in mind as we start moving forward here with normal data.

205
00:25:39,810 --> 00:25:43,950
All these things are sort of fuzzy because they all lead to the same path, really.

206
00:25:45,150 --> 00:25:53,510
One to finish lectures, three and four today. That's 4330 302.

207
00:25:56,190 --> 00:26:00,400
All right, let's just freeze pretty short. Three seats are really pretty short.

208
00:26:01,260 --> 00:26:06,719
But it's one of those classic things that if you haven't thought about it already,

209
00:26:06,720 --> 00:26:11,420
what I've just been talking about these subject specific intercepts and slopes of the problem.

210
00:26:13,080 --> 00:26:16,550
It's this idea of shrinkage. Shrinkage estimates.

211
00:26:17,730 --> 00:26:23,010
So back to the labor pin data that I've been talking about all semester.

212
00:26:23,610 --> 00:26:29,160
We're going to fit a woman's pain on time continuous.

213
00:26:29,430 --> 00:26:33,270
There is a group indicator for placebo versus treatment. There's an interaction of time.

214
00:26:33,960 --> 00:26:39,300
We have a random intercept and a random slope and we have the errors I should have in my rotations.

215
00:26:39,300 --> 00:26:47,160
That would be the two. These are but again, by not is the is the random intercept and b i1 is the random slope for each person.

216
00:26:47,730 --> 00:26:51,230
And the errors are independent of the two random acts.

217
00:26:53,100 --> 00:26:54,630
So that's a linear, mixed model.

218
00:26:55,710 --> 00:27:01,980
So for a woman with my pain scores again here, if you're struggling with all the matrix notation that's being thrown at you,

219
00:27:02,310 --> 00:27:11,190
we write the lyrics model as a vector of y observations is a linear combination of X's and times beta plus the random effects times,

220
00:27:11,190 --> 00:27:16,229
the design matrix and plus the errors. So again, for vector of outcomes,

221
00:27:16,230 --> 00:27:24,510
the design matrix is one row for every timepoint and then the covariance go down each column time is changing, right?

222
00:27:24,510 --> 00:27:26,910
So the second column and x, those are the time points.

223
00:27:27,300 --> 00:27:34,710
The g indicator is the same for every person because they're either in one group or the other group the entire time or coefficients.

224
00:27:35,490 --> 00:27:41,790
I'm only fitting a random intercept and a random slope against the z and the x matrices do not have to be the same.

225
00:27:42,940 --> 00:27:46,840
You could you could have put in a random group effects and a random interaction effect.

226
00:27:47,710 --> 00:27:51,300
But again, think about that. You've now got four columns there.

227
00:27:51,310 --> 00:27:57,670
So you have a variance component for every one of those and then a possible covariance parameter for every pair of those.

228
00:27:58,330 --> 00:28:01,450
It's probably impossible to fit with most data unless they're enormous.

229
00:28:01,990 --> 00:28:03,840
We have two random effects.

230
00:28:03,850 --> 00:28:13,550
There should be a comma there between the Einat and the B01 and again they have variances along diagonal of D and they have a possible covariance.

231
00:28:14,800 --> 00:28:18,490
And then there's a vector of residuals. And often we assume that they're independent.

232
00:28:19,840 --> 00:28:21,490
So that's everything from that model.

233
00:28:24,010 --> 00:28:30,910
So each woman in the control arm, after we fit our linear mix model, each woman in the control arm has their own predicted intercept.

234
00:28:31,570 --> 00:28:38,950
It's the data, not for everybody. Tilda perhaps use me plus their own deviation up or down from The Intercept.

235
00:28:39,340 --> 00:28:44,360
And they each have their own slope. And each woman in the treatment arm as predictions.

236
00:28:44,380 --> 00:28:50,420
Again, there's a beta or not plus a beta two for the difference in the intercepts and then their own individual deviation.

237
00:28:51,040 --> 00:28:55,420
And then there's a slope for this group overall. And then a population.

238
00:28:55,720 --> 00:28:57,760
Ah, excuse me. A subject specific deviation.

239
00:29:00,270 --> 00:29:08,460
Well, if we want a person's specific intercept and slope, then why don't we just fit all else to every woman individually?

240
00:29:09,000 --> 00:29:17,010
Why don't I just take the first woman's data set and interceptors ready, you know, a basic model through their data and get intercepted a slope.

241
00:29:17,490 --> 00:29:21,540
Why do I have to learn all this complicated stuff, right?

242
00:29:21,750 --> 00:29:31,020
What is the benefit of taking everyone's data to predict each individual rather than just using a person's own data to predict that individual?

243
00:29:31,740 --> 00:29:38,790
And again, it's this idea of shrinkage that the prediction for a person is based not only upon their data,

244
00:29:39,270 --> 00:29:46,080
but some sort of combination of their data with everybody else. So if someone is really high from the population line.

245
00:29:47,150 --> 00:29:51,380
They remain high in their predictions, but they're pulled a little bit toward the population.

246
00:29:51,860 --> 00:29:59,000
And likewise, someone who's below, they might get pulled up a little bit based upon how much variability there is between and within individuals.

247
00:30:00,920 --> 00:30:02,450
So explicitly for each woman,

248
00:30:02,450 --> 00:30:12,080
I'm going to do this and are I'm going to fit an ordinarily squares model to each woman individually so their own intercept, their own slope.

249
00:30:12,300 --> 00:30:21,080
Again, this is not linear mixed models. This is 650. This is just a simple linear regression model with time as a covariate versus doing the

250
00:30:21,080 --> 00:30:25,400
linear mixed model with random intercepts and random slopes that I showed you earlier.

251
00:30:26,720 --> 00:30:29,310
And so I set that and we'll look at that in our shortly.

252
00:30:29,960 --> 00:30:40,580
Then what I'm going to do is look at the Alpha I not at each woman's intercept from this model and I'm going to compare it to

253
00:30:41,000 --> 00:30:48,650
beta not have to plus data I not tilde the predicted intercept and the same thing for the slopes and which group they're in.

254
00:30:49,850 --> 00:30:51,770
And if I plot on the x axis,

255
00:30:52,070 --> 00:31:02,120
everybody's intercept from the way of modeling everybody individually with all less versus the linear mixed model approach with their own intercepts.

256
00:31:02,600 --> 00:31:08,420
You see something like this. The diagonal is when the intercepts for both approaches would be equal.

257
00:31:10,040 --> 00:31:16,880
And again, I thought this was really cool when I created it a while ago, and then when I start to look at it, it's not so cool.

258
00:31:17,600 --> 00:31:21,560
But again, it shows you what I want to show you that you're not not sure in the best way.

259
00:31:21,980 --> 00:31:29,570
But for example, here is a woman who if I just analyze her data, she has an intercept of something over 70.

260
00:31:31,150 --> 00:31:38,110
But when I analyze her and everyone's together, her intercept is somewhere below 50 get pulled down.

261
00:31:39,250 --> 00:31:45,340
So that's what we're trying to show here, is that intercepts that are really large tend to get pulled back a little bit.

262
00:31:46,700 --> 00:31:52,930
Likewise, intercepts that are really negative, they tend to get pulled in the other direction back toward the population.

263
00:31:53,440 --> 00:31:56,890
So we sort of see this teeter totter. These are below. These are above.

264
00:31:57,910 --> 00:32:02,650
And so what we're doing is we're taking those intercepts that we would have gotten with just one person's data,

265
00:32:03,070 --> 00:32:06,250
and they're sort of pulled in a little bit based upon what everybody else is doing.

266
00:32:09,520 --> 00:32:16,719
And likewise for the slopes, how about I can see sort of this observations above getting pulled and observations this way,

267
00:32:16,720 --> 00:32:17,590
getting pulled the other way.

268
00:32:18,100 --> 00:32:26,200
So again, it's the idea of shrinkage that I don't just believe that one person's data is the best way to to think about what's going on for them.

269
00:32:26,920 --> 00:32:30,850
They've got to use everybody. And that is done through random effects.

270
00:32:31,510 --> 00:32:34,780
So. Again these two approaches.

271
00:32:34,900 --> 00:32:41,200
If I want to subject specific intercept and slope, I don't just fit a line to their data and it aligns to everyone's data.

272
00:32:41,380 --> 00:32:44,800
And there are other nuances. I don't mean to say that that's the only reason.

273
00:32:46,570 --> 00:32:54,250
Remember, when you fit a different lines, every woman's data, you get a different sigma squared hat for the errors.

274
00:32:55,780 --> 00:32:59,860
So you're, you're assuming non constant variance across all the women as well.

275
00:33:00,340 --> 00:33:07,980
Whereas when we fit. This model here, all the errors have a Sigma Squared Epsilon.

276
00:33:08,370 --> 00:33:11,280
So there are some other important nuances. But for the most part,

277
00:33:12,120 --> 00:33:22,440
we are trying to not just use one woman's data to think about what might be going on with her because everybody has variability and odd measurements.

278
00:33:22,620 --> 00:33:29,190
So we sort of smooths each person based upon all of the data, not just their individual data.

279
00:33:30,870 --> 00:33:34,410
So again, I want to show you a slightly different view that I think is.

280
00:33:35,610 --> 00:33:40,740
Next year, I would have in my lectures, I'm going to go into our studio.

281
00:33:42,090 --> 00:33:44,970
That is not our studio. That is.

282
00:33:46,840 --> 00:33:53,690
So I've discovered that I've used up my hours of free hour, studio time, reward me, that I'm already there, and it took me off.

283
00:33:53,710 --> 00:33:59,970
So I think we're still good today. What is the message says other sex.

284
00:33:59,980 --> 00:34:08,930
Whenever you see an exclamation point. Right. So they're going to cut me off at noon on Friday and then let me start again at 2:00.

285
00:34:10,250 --> 00:34:14,600
So there's a two hour blackout, so I think I'm okay.

286
00:34:15,620 --> 00:34:18,679
We'll see what happens on Friday when we get ready to wait a bit, too.

287
00:34:18,680 --> 00:34:24,890
If we start to. I can't until 211. Okay. All right.

288
00:34:25,460 --> 00:34:27,680
So let me create recreate those spots for you.

289
00:34:27,680 --> 00:34:35,060
Just again, you can do this, but you have these code, these lines of code if you ever want to track yourself from the top.

290
00:34:36,290 --> 00:34:39,860
All right. So I'm going to load the labor pin data there.

291
00:34:40,130 --> 00:34:44,030
It's in our studio cloud. So if I have a question.

292
00:34:44,030 --> 00:34:51,140
Oh, yeah, I'm sorry. Go ahead. You can look up if you look at the like the standard error estimates on the betas themselves.

293
00:34:51,560 --> 00:35:01,580
Standard error on the from the individually fit lines, you have those differ than the standard errors on the people specific lines and all of them.

294
00:35:03,900 --> 00:35:04,340
People.

295
00:35:04,690 --> 00:35:13,710
So you're talking about the still the standard error of the data that's from the individual lines versus the variability of the ones from Elemental.

296
00:35:15,640 --> 00:35:20,230
Keep me posted. I'll have to think about that.

297
00:35:20,350 --> 00:35:25,420
What are you thinking? I don't know. I was asking because if you're using Jessica, they're going to shrink.

298
00:35:25,590 --> 00:35:29,440
I think they're going to be less variable with Yellow Mountain than they will be with the individual.

299
00:35:29,560 --> 00:35:36,340
Yeah, there's probably a formula, but I think that's probably because they shrink towards zero.

300
00:35:36,340 --> 00:35:42,700
There must be less variability. Well, that's the whole point of this, right, is if someone's really wildly out there in the stratosphere,

301
00:35:42,700 --> 00:35:46,390
we're going to pull them back down toward the population to reduce some of the noise.

302
00:35:46,570 --> 00:35:49,960
Yeah, good question. All right.

303
00:35:50,620 --> 00:35:55,690
I went to the data set. I said, give me every unique idea because I'm going to do it one I.D. at a time.

304
00:35:56,680 --> 00:36:03,280
And then I know the unique treatment assignment of each woman. And so the first thing I did was, I need to do this.

305
00:36:05,330 --> 00:36:10,160
Oops. There we go. So I had this thing called coefficient individual.

306
00:36:10,190 --> 00:36:13,670
I'm going to do an individual regression line for each woman on the top.

307
00:36:14,540 --> 00:36:19,610
And again, it's really straightforward. I just pulled out the lines of data that apply to each woman.

308
00:36:20,660 --> 00:36:24,890
And then I set an alarm, just a standard alarm of their Kane score entitled.

309
00:36:26,720 --> 00:36:32,630
I'm not sure why I hate you need to know their your treatment assignment comes up later.

310
00:36:33,060 --> 00:36:35,360
Right. So that's the individual regression line approach.

311
00:36:36,020 --> 00:36:46,620
And then again, just fitting it with a linear mixed model is again, the the model the library is LME for and the line of code is LME hour.

312
00:36:47,210 --> 00:36:59,150
I'm 17 on time. Why don't I have a grip on it?

313
00:36:59,670 --> 00:37:07,010
It's so funny. I've done this for years. I used this code for years and there's another group effect right now.

314
00:37:07,020 --> 00:37:13,080
But anyway, I have been a linear, mixed model in which pain is modeled as a function of time, and then there's a random intercept.

315
00:37:13,290 --> 00:37:20,430
So I guess I have assumed no group effect in all of this to get the subject's specific intercepts and slopes.

316
00:37:21,600 --> 00:37:23,430
The command is complex.

317
00:37:23,670 --> 00:37:28,470
So this is one of the challenges of this function as your take, maybe you've discovered as you're doing this homework assignment.

318
00:37:29,800 --> 00:37:40,510
If you see this with linear mixed models in glass, if you say C, o f of a model fit, you get the fixed effect parameters.

319
00:37:41,920 --> 00:37:47,380
If you say a series of a linear mixed model in our you don't get those.

320
00:37:47,920 --> 00:37:51,190
You get the subjects specific ones with the blobs added to them.

321
00:37:52,030 --> 00:38:01,020
So anyway, just be aware of that. So these are now the fitted population estimates plus the deviations, the random, their substance numbers.

322
00:38:01,780 --> 00:38:09,370
So again, all I did was make a plot of, of the coefficients from the individual and the coefficients from the shrinkage model,

323
00:38:09,370 --> 00:38:15,400
the linear mixed model, and that is down here.

324
00:38:16,660 --> 00:38:21,130
So I just showed you that plot of the intercepts from the linear regression versus the limerick's model.

325
00:38:21,800 --> 00:38:27,760
So again, you guys are all Gigi plot experts. You're probably thinking this code is nonsense, but this is again, this is how I think.

326
00:38:29,500 --> 00:38:34,000
And then there is also a plan for for the random slots that I just showed you in the slides.

327
00:38:36,660 --> 00:38:46,260
Let's see. I guess I'm going to need some of this for a sec in a second. Oh, but I'm not going to do that right now.

328
00:38:51,170 --> 00:38:56,150
Oh, very simply, look around them again. All right.

329
00:38:56,960 --> 00:39:03,340
So again, you see the same plots. So there we go.

330
00:39:03,460 --> 00:39:08,310
So that was the Intercept's plot of the Intercept plot that I showed you in show,

331
00:39:08,320 --> 00:39:12,100
in the election slides, and there's a corresponding one for the slopes.

332
00:39:13,180 --> 00:39:17,410
And how do they get out of there?

333
00:39:17,470 --> 00:39:20,980
There we go. Want to show you a different approach to looking at all of this.

334
00:39:23,440 --> 00:39:28,120
And that is this block of code making this.

335
00:39:28,120 --> 00:39:32,410
But here again, it's my old man, planet code.

336
00:39:32,740 --> 00:39:39,910
But before we get there. Troops during it.

337
00:39:40,180 --> 00:39:49,720
All right. And those mug shots from the slides there, it slides three.

338
00:39:51,260 --> 00:40:04,380
Three be. Three B prediction of random effects.

339
00:40:08,700 --> 00:40:13,670
So there we go. This letter right here. I'm current.

340
00:40:16,760 --> 00:40:28,490
It said that an individual level prediction is a weighted average of what we have for everybody and a weight applied to their very own data.

341
00:40:29,210 --> 00:40:33,350
So I'm going to use our code now. I'm going to set the linear, mixed model intercepts and slopes.

342
00:40:34,280 --> 00:40:39,139
I have to pull out this our matrix again. This is sigma squared times an identity matrix.

343
00:40:39,140 --> 00:40:41,630
I have to get sigma squared for the errors.

344
00:40:43,520 --> 00:40:52,760
And this sigma again remember is all of this here it's the error variance plus the variance between individuals.

345
00:40:53,150 --> 00:40:55,610
So I have to pull out the D matrix that's estimated.

346
00:40:55,880 --> 00:41:02,240
So the D matrix has a variance for the intercepts, the variance for the slopes, and of course variance term between the two of them.

347
00:41:02,390 --> 00:41:09,560
So a two by two matrix, this is just the intercept column of ones and the column of the time variables.

348
00:41:10,130 --> 00:41:13,370
And again, this is just some sigma squared times, an identity matrix.

349
00:41:14,240 --> 00:41:19,430
And then I have to invert it. Right? So there's a weight matrix for all of the individuals.

350
00:41:19,430 --> 00:41:25,430
So everybody has this weight. And then there's a different weight applied to their individual data.

351
00:41:26,180 --> 00:41:31,220
And that gives me their individual level predictions. So in our studio.

352
00:41:34,140 --> 00:41:39,030
This is what I did. So again, for each individual person.

353
00:41:39,450 --> 00:41:43,469
The first thing, it would be nice if everybody had the same number of observations.

354
00:41:43,470 --> 00:41:49,800
So I know how big these matrices are for each person. But in these data, there are different number of observations per each woman.

355
00:41:50,310 --> 00:41:53,730
So I had to first figure out how many observations does each woman have?

356
00:41:54,270 --> 00:41:57,720
So I just said some of the number of times the idea shows up.

357
00:41:58,620 --> 00:42:06,120
I pulled out her pain scores. I got the regression fixed effect parameters from the linear mixed model.

358
00:42:08,190 --> 00:42:11,760
I then created a design matrix. So let's let it go.

359
00:42:11,940 --> 00:42:15,400
Let's. Talking realities here.

360
00:42:16,060 --> 00:42:20,620
So this person wants you to already know that they are, you know, open to her as everything.

361
00:42:22,430 --> 00:42:27,950
So decisions. All right.

362
00:42:28,280 --> 00:42:35,050
So am I. Oh, really? Oh.

363
00:42:35,290 --> 00:42:39,970
So they said it's too. What did I do wrong? There we go.

364
00:42:41,290 --> 00:42:45,250
Okay. So, woman number two has seven observation. She had a pant skirt each time.

365
00:42:46,000 --> 00:42:52,149
What are those being? Skirts where they are. So she was showing no pain.

366
00:42:52,150 --> 00:42:57,490
This was a very unusual person. No pain. And then again, she was on a treatment.

367
00:42:57,700 --> 00:43:02,860
And then it started to escalate. The pain medication was wearing off.

368
00:43:04,870 --> 00:43:11,500
There are regression coefficients to intercept that group effect at the time of sex and the interaction.

369
00:43:15,480 --> 00:43:19,920
That's right. It's took the group effect in this thing. All right. Intercept and slow.

370
00:43:20,100 --> 00:43:23,400
Just a time effect. There's two regression parameters.

371
00:43:24,720 --> 00:43:27,720
So then there is a design matrix for those fixed effects.

372
00:43:36,480 --> 00:43:39,800
Common ones for the innocents. And then each of her time plates.

373
00:43:39,990 --> 00:43:44,310
Time, time again. There is no group effect in this model.

374
00:43:45,090 --> 00:43:57,390
I'm not sure why I decided to in order to do all this. To which they then took extra time to speed up.

375
00:44:00,470 --> 00:44:10,630
You call that new? I am not sure I call them. I think it's because there's a different vector so that sexy data should be going up linearly.

376
00:44:11,500 --> 00:44:17,590
That was our model. So that's what we have for every person who has seven observations.

377
00:44:17,590 --> 00:44:23,470
For every woman who has seven observations. This is excited data and that's the same across individuals.

378
00:44:24,820 --> 00:44:27,080
But then we have the subject specific components.

379
00:44:27,100 --> 00:44:34,810
So again, we have our eye that is the variance covariance matrix of the errors and that's a diagonal matrix.

380
00:44:39,280 --> 00:44:47,650
So again today, if you're not familiar with a lot of our code dying egg of a number is just an identity matrix of that dimension.

381
00:44:47,890 --> 00:44:52,360
So diagram is just a seven by seven matrix with one ones along the diagonal.

382
00:44:53,110 --> 00:44:56,200
And then I multiply that by sigma squared and this is how you get that estimate.

383
00:44:57,280 --> 00:45:00,790
So you get the summary of it. It gives you the standard deviation you have to square.

384
00:45:05,080 --> 00:45:12,010
Right. So that's a seven by seven matrix with the same variance across the diagonal string,

385
00:45:12,550 --> 00:45:18,700
which in this case the design matrix for the random effects is the same as it is for the fixed effects.

386
00:45:18,700 --> 00:45:31,600
It's just a column of ones and time. And then again, the overall variance of Y has two pieces the variability between which is Z,

387
00:45:31,870 --> 00:45:36,550
D, Z, transpose, and what's within which is the right component.

388
00:45:37,780 --> 00:45:46,840
So where do I pull up to stay within a half?

389
00:45:48,370 --> 00:45:51,430
Everybody has the same deal. So I didn't do it in the fall of.

390
00:45:53,580 --> 00:45:59,690
There's a variance of the random intercepts, there's a variance of the random slopes and there's a covariance.

391
00:45:59,690 --> 00:46:07,090
And that's the same for everybody. And so there again, if you ever want to get that matrix, here's the code.

392
00:46:10,460 --> 00:46:15,680
So there's the variance of the random intercepts, the variance of the random slopes and the covariance terms.

393
00:46:18,080 --> 00:46:26,630
Now we should be able to do it. So now I'm going to create this overall variance of y sigma i that should be a seven by seven matrix.

394
00:46:28,760 --> 00:46:37,610
There it is. Still want to what I want to show you something I want to show you is, again,

395
00:46:37,790 --> 00:46:45,100
the prediction for an individual is a weighted average of what is going on for everyone versus another wave,

396
00:46:45,110 --> 00:46:49,280
one minus, essentially one minus that eight times their actual data.

397
00:46:49,820 --> 00:47:00,890
So I'm taking the W matrix times, essentially the population values and one minus that identity, minus that times the observed data.

398
00:47:02,210 --> 00:47:07,550
So what does the CW Matrix look like? This is not so much fun to look at.

399
00:47:14,890 --> 00:47:18,550
Okay. Good. So there's the weight matrix. It's a weight matrix.

400
00:47:19,520 --> 00:47:23,120
So everybody's prediction. So at time one.

401
00:47:25,180 --> 00:47:33,970
A woman's prediction is a weighted average of all her values and all of the population timepoint values.

402
00:47:34,330 --> 00:47:39,040
It's not just a weighted average between those two things at that time point. It's a combination of everything.

403
00:47:40,420 --> 00:47:46,690
So someone's prediction at time, one is 60% of the population average.

404
00:47:48,040 --> 00:47:51,070
But then you have these adjustments for all the other time points.

405
00:47:51,580 --> 00:47:56,710
So again, with a weight matrix, it's not so easy to think of of of what things are going to come out here.

406
00:47:57,250 --> 00:48:05,350
But essentially, I'm taking linear combinations of all of the population values to create a prediction at each time like.

407
00:48:07,280 --> 00:48:09,140
And that's what I do here in this line right here.

408
00:48:10,530 --> 00:48:16,220
Let's take a weighted average of everybody's fit, plus an individual wait times for their individual data.

409
00:48:18,530 --> 00:48:24,650
And then I will plot I'm going to plot three things for you relative to time.

410
00:48:25,550 --> 00:48:35,060
I'm going to show you the woman's observed gain scores, the population average and that woman's predicted plot.

411
00:48:36,940 --> 00:48:41,980
To show you what we're trying to do here. And that's what all this is. So I'm going to do this for each woman.

412
00:48:44,160 --> 00:48:49,630
And they're going to save it in a PDF. And I hope it works and it still works.

413
00:48:50,180 --> 00:48:56,350
Right. And I have this plant right here, and I did it individually for each woman.

414
00:48:57,700 --> 00:49:04,020
Right. So here's the very first person she only observed the first part time points in every plot.

415
00:49:04,170 --> 00:49:08,610
You're going to see this light blue line. This is the population average.

416
00:49:08,610 --> 00:49:13,650
It's at the same point for every individual. We then have the observed values in blue.

417
00:49:13,680 --> 00:49:17,910
Look at that blue and mays. You think of that and the subject specific.

418
00:49:18,270 --> 00:49:22,230
So the blue, the dark blue and the light blue.

419
00:49:23,760 --> 00:49:27,720
There's a weighted average of all of those that leads to the yellow line.

420
00:49:30,610 --> 00:49:33,790
And so you see here, this woman was remaining fairly flat.

421
00:49:35,880 --> 00:49:45,240
But the population in general was going up. There also appears to be a lot of variability between individuals.

422
00:49:46,320 --> 00:49:50,280
So that I believe her data more than I do of the population.

423
00:49:50,640 --> 00:50:00,060
Her prediction is pretty close to her observed values. It's pulled off a little bit because the population, but not a lot of folks can't do this.

424
00:50:00,060 --> 00:50:05,970
Should I'd like to just pass between each one. Here's the one that we just did in class a second ago.

425
00:50:05,980 --> 00:50:09,240
Person number two, again, there's the population line.

426
00:50:09,690 --> 00:50:14,040
The blue is what is happening with this person and then the prediction is in yellow.

427
00:50:15,060 --> 00:50:21,630
And so, again, I don't mean to imply that the prediction is always somewhere in between those two values,

428
00:50:22,080 --> 00:50:32,730
because remember, it's a weighted average of everything. So even though that woman even though this woman was staying fairly flat,

429
00:50:33,030 --> 00:50:36,600
her prediction was going up by just a little bit because overall, everybody else was.

430
00:50:37,380 --> 00:50:41,790
And then she got pulled up quite a bit here because she was going up and the population was going up.

431
00:50:43,500 --> 00:50:49,020
Here's an interesting one. Person number three. So this is a woman who had a lot of pain and then it just vanished.

432
00:50:50,220 --> 00:50:53,490
Again, if we believe that things are increasing over time.

433
00:50:56,230 --> 00:51:02,559
This figures out that while this one really was going down on her pain score and she's going down linearly,

434
00:51:02,560 --> 00:51:07,450
there's a weighted average of this line and the observed data that gives me this prediction in yellow.

435
00:51:11,680 --> 00:51:22,180
Again, this person right here, although she jumped up a lot in her pain and then vanished, which is what a good pain treatment would do for you.

436
00:51:23,800 --> 00:51:31,780
Her prediction is a little a little slower because, again, it's a weighted average of what is going on in her versus what's going on in everybody.

437
00:51:32,590 --> 00:51:40,690
And again, if you want to run this code and you can just see how each individual's predictions are some weird combination.

438
00:51:41,780 --> 00:51:45,040
Again, here's a person whose pain score just kept going up and up and up.

439
00:51:45,790 --> 00:51:54,100
And although the population tended to not increase as fast, we came up with this prediction that tried to make a marriage of these two things.

440
00:51:54,110 --> 00:51:58,810
But in all of these floods, again, this is a good exam question.

441
00:52:01,510 --> 00:52:06,850
What's greater in these data, the between subject variability or the within subject variability?

442
00:52:10,120 --> 00:52:14,560
Between subjects. Right. I said that earlier, but she emphasized this.

443
00:52:14,980 --> 00:52:23,800
The individuals are varying so much from each other that if I'm going to predict, I'm going to rely more upon their data than everybody.

444
00:52:25,550 --> 00:52:26,620
That's how these weights work.

445
00:52:27,040 --> 00:52:33,700
If there wasn't much variability between the individuals, then everybody's prediction might be closer to the population.

446
00:52:34,900 --> 00:52:42,540
It's again, a complex function of both of those features, and it's also a feature of the number of observations a woman gets right.

447
00:52:42,630 --> 00:52:47,130
The more data that each woman has, the more I can trust her data versus the population.

448
00:52:47,820 --> 00:52:51,550
So that's all in there. 5 to 4.

449
00:52:53,900 --> 00:52:59,570
So again, a nice way to look at this through our. There is one more aspect of linear rates models.

450
00:53:00,930 --> 00:53:04,290
And it's one I think you're implicitly doing.

451
00:53:05,010 --> 00:53:09,060
And what I'm going to show you at the end is something I'm not going to ask you to do.

452
00:53:10,590 --> 00:53:14,310
But it's an important fact to know with linear, mixed models.

453
00:53:16,360 --> 00:53:21,399
That's again, you might ask yourself, I got a variance of 24.

454
00:53:21,400 --> 00:53:30,490
The random intercepts, the computer spit out a value of 20. Is that significantly different from zero if a variance component has a variance zero.

455
00:53:32,030 --> 00:53:36,980
And it's zero for everybody. No variance means zero. And that means you don't need a random intercept.

456
00:53:37,820 --> 00:53:41,299
So if you want to think about Should I have a random intercept in my model, you might say,

457
00:53:41,300 --> 00:53:44,900
Well, I'll just do a hypothesis test on whether the variance is zero or not.

458
00:53:46,700 --> 00:53:49,790
And that isn't as simple with random effects as it is with fixed effects.

459
00:53:50,930 --> 00:53:56,060
So in Shields, if you think about nested models, you compare two models.

460
00:53:56,690 --> 00:54:02,730
Again, model one has a set of covariates and parameters and there's an error set of errors with the cover

461
00:54:02,750 --> 00:54:08,300
correlation structure sigma one And there's a second model with covariates in regression parameters.

462
00:54:08,960 --> 00:54:14,960
And again, we say things are nested if all of the covariates in model one are included in model two.

463
00:54:14,960 --> 00:54:18,350
Right, sorry. All the covariates of model two are in model one.

464
00:54:18,560 --> 00:54:27,140
We're trying to reduce the model. And so, again, if things are nested in terms of the coefficients, but the error structure is the same, right?

465
00:54:27,380 --> 00:54:31,040
So if you fit two models and again, if you want to see if the interaction terms should be in the model,

466
00:54:31,520 --> 00:54:39,380
you'll see the model with and without the the interactions. But she'll use the same correlation structure compound symmetric, for example.

467
00:54:39,770 --> 00:54:47,749
Right. And if you do that, then there are F tests and all tests and all the things that you do to to figure out which model is better.

468
00:54:47,750 --> 00:54:48,920
Nothing is affected there.

469
00:54:50,590 --> 00:55:00,220
They're not vested if right, if the first model didn't have age and the second model had some term that wasn't wasn't in the model.

470
00:55:00,460 --> 00:55:08,470
If the if the nesting in the coefficients isn't there or if you change the correlation structure and as we talked about earlier,

471
00:55:08,530 --> 00:55:14,049
you could have two models with the same mean structure and you want to compare two different correlation matrices,

472
00:55:14,050 --> 00:55:20,920
maybe compound symmetric versus auto regressive. And as we said, there was no hypothesis testing framework for that.

473
00:55:20,950 --> 00:55:27,070
You had to use AIC for Vasey or even pseudo R squared value, right?

474
00:55:27,730 --> 00:55:32,290
So in those situations, there is no P value.

475
00:55:32,350 --> 00:55:35,200
There is no P value to choose correlation structures and goals.

476
00:55:36,940 --> 00:55:41,110
And so you try to think about this, but you've been doing this if you've been doing homework.

477
00:55:41,110 --> 00:55:46,120
Number three is do all these ideas hold with linear makes models and we just have to think about

478
00:55:46,120 --> 00:55:51,970
nesting because we can talk about nesting with respect to random effects and what we can do there.

479
00:55:52,420 --> 00:55:57,190
So now think of two linear mixed models. Again, there's a fixed component exhibitor.

480
00:55:57,850 --> 00:56:01,870
There's a set of random effects, and then there's error terms that we assume are independent.

481
00:56:02,740 --> 00:56:07,960
The first model has a set of random effects, and the second model has a set of random effects.

482
00:56:08,380 --> 00:56:13,390
And again, we can change the variance covariance matrix of the of the random effects.

483
00:56:14,290 --> 00:56:18,699
So again, if the mean parameters, if you're just trying to reduce the number of variables in the,

484
00:56:18,700 --> 00:56:25,210
in the mean model, right, then we talk about nesting as long as you don't change the random effect structure.

485
00:56:25,330 --> 00:56:32,080
So as long as the random effect structure is the same and you change the mean structure so that it's nested in those coefficients,

486
00:56:32,470 --> 00:56:36,870
then there's nothing new here. Okay. Like the ratio tests, right?

487
00:56:36,920 --> 00:56:41,889
You look at the difference in the number of parameters that is a Chi square distribution and so forth.

488
00:56:41,890 --> 00:56:48,770
All that holds just fine. For assessing the need for random effects.

489
00:56:49,250 --> 00:56:55,100
Again, I sent a random intercept model in homework. Number three, I set a random intercept swap model.

490
00:56:56,470 --> 00:57:01,270
Which model is better? Is there a way to do that through hypothesis testing?

491
00:57:02,080 --> 00:57:06,610
So we have two different possibilities for nested models, for the random effects.

492
00:57:06,640 --> 00:57:12,200
We're going to keep the fixed effects the same. So I've got group time and the interaction in both models.

493
00:57:12,760 --> 00:57:15,910
And now I want to figure out about different kinds of random effect structures.

494
00:57:16,750 --> 00:57:24,340
So, for instance, I could have a random intercept for B I1 and a random intercept slope for B i2.

495
00:57:26,290 --> 00:57:33,430
Or I could have a situation in which I have the same number of variance components.

496
00:57:34,610 --> 00:57:38,180
But the correlation structure between those various components changes.

497
00:57:39,240 --> 00:57:45,840
And specifically, you can think of the question of a random intercepts intercept and slope model.

498
00:57:45,900 --> 00:57:48,630
And I showed you this how to fit this in our last week.

499
00:57:49,410 --> 00:57:57,750
If you want to fit a random intercept and slope model, you might ask yourself some, like David Byrne if anyone else are tracking heads so down.

500
00:57:59,670 --> 00:58:02,970
I could have independence between the random intercept and the random slope.

501
00:58:03,750 --> 00:58:08,780
I could have a covariance terminus. So this model has two variants components.

502
00:58:09,500 --> 00:58:15,950
This model has three variants components. And what I want to know is if I can reduce that by one parameter.

503
00:58:20,080 --> 00:58:26,709
So as I said, for nested mean models, you are always forcing some of the parameters to be zero, right?

504
00:58:26,710 --> 00:58:31,720
You reducing the model. And so you just said, well, how many parameters are in the first model?

505
00:58:31,750 --> 00:58:33,790
How many parameters are in the second model?

506
00:58:34,060 --> 00:58:40,930
You took the difference between those two numbers and you said you test test that had a chi square with two degrees of freedom.

507
00:58:40,930 --> 00:58:45,790
Three whatever the difference, the number of parameters was for nested random effects.

508
00:58:46,660 --> 00:58:50,980
Again, if the reduced model is simply the full model with some parameters for it to be zero,

509
00:58:51,670 --> 00:58:55,120
then you might think of a difference in the number of variance components.

510
00:58:55,780 --> 00:58:59,230
So again, in this example here.

511
00:59:00,340 --> 00:59:04,390
I had two variance components and here I have three. So the difference is one.

512
00:59:08,390 --> 00:59:16,400
The normal distribution. So again, whatever test statistic you want, like a wild set, a wild test takes a coefficient estimate,

513
00:59:16,790 --> 00:59:21,680
subtracts off a no value zero divided by some sort of standard error we've even talked about.

514
00:59:21,860 --> 00:59:29,030
How do you get the standard error for the variance estimate our gives you that if we look at some of your output,

515
00:59:29,030 --> 00:59:34,010
I will give you a standard error for the variance of the random effects.

516
00:59:36,680 --> 00:59:38,270
The no distribution, however,

517
00:59:38,270 --> 00:59:45,350
is not is not a chi square distribution with degrees of freedom equal to the number of reduction and variance parameters.

518
00:59:46,550 --> 00:59:56,040
Right. It's actually a mixture of chi squares. And there's some excellent research by Gibran Gucciardo, who I think is still at Johns Hopkins.

519
00:59:57,390 --> 01:00:04,590
This is a really hard problem. The mixture of proportions and degrees of freedom are really complicated in most settings.

520
01:00:05,350 --> 01:00:13,830
If I have five random effects versus three, what's the null distribution for determining whether I can reduce the number of variance parameters?

521
01:00:14,340 --> 01:00:20,430
It's really hard, except in this simple example here, and it's a question that you should be asking yourself now.

522
01:00:22,790 --> 01:00:28,970
If you have a fixed effects model. Oh, well, yes. Is there any benefit to adding a random intercept?

523
01:00:30,970 --> 01:00:37,570
So the reduced model has no variance parameters and the full model has as either.

524
01:00:40,020 --> 01:00:43,830
Full model has a q equals one variance parameter. Either is.

525
01:00:46,830 --> 01:00:52,200
Hmm. I think that should be done just as one for comparing these two models.

526
01:00:52,200 --> 01:00:59,040
The test artistic has a null distribution that's a 5050 mixture of a chi square with zero degrees of freedom and a chi square

527
01:00:59,040 --> 01:01:06,240
with zero degrees of freedom as just a spike in zero and a square with one degree of freedom for typical chi square one.

528
01:01:06,870 --> 01:01:12,419
So it looks like a big spike at zero on top of a chi square with one degree of freedom.

529
01:01:12,420 --> 01:01:15,960
And it's a 5050 mixture. And again, what's the big deal there, Tom?

530
01:01:16,140 --> 01:01:23,160
Well, it's because, remember, you're trying to get a p value. Um, the chi square was a chi square.

531
01:01:23,160 --> 01:01:32,940
One is different than half of a chi square and a spike at zero at 50% of that distribution is already at zero, 50% of a case where one is not at zero.

532
01:01:33,870 --> 01:01:44,850
So it's going to mess up the tails of the distribution. And your inference why what happens here and without going into great detail,

533
01:01:44,910 --> 01:01:49,380
this would be a great topic for you don't want to go to I don't know if it's covered any more,

534
01:01:49,650 --> 01:01:53,370
but why does the no distribution change when testing for random effects?

535
01:01:54,150 --> 01:01:59,220
Now, if the random intercept has variance sigma not squared, then you have a hypothesis test here.

536
01:01:59,820 --> 01:02:05,309
You want to know whether or not that thing is zero or not. If it's zero, that means it doesn't.

537
01:02:05,310 --> 01:02:13,800
It doesn't include in the model. This is one of these things you learn in 602 about the parameter space and hitting the boundary.

538
01:02:14,700 --> 01:02:19,890
So the null value of zero here is on the boundary of the parameter space because variance parameters can't be negative.

539
01:02:20,340 --> 01:02:24,480
Unlike regression parameters, they can be anywhere from negative infinity to infinity.

540
01:02:25,230 --> 01:02:30,870
And this violates the regularity condition that you need for all the likelihood theory that you learn in your classes.

541
01:02:31,740 --> 01:02:37,170
And so that's a very quick summary of all the theory that underlies what goes on, what goes on here.

542
01:02:37,560 --> 01:02:41,070
Right? So what is the point of telling you all this?

543
01:02:41,070 --> 01:02:44,730
I'm not going to ask you to do inference and variance components themselves.

544
01:02:46,440 --> 01:02:49,620
Inference random effects is not programed in right now.

545
01:02:50,130 --> 01:02:55,950
It used to be programed in SAS back when we shot this class with SAS and those wonderful programmers

546
01:02:55,950 --> 01:03:03,090
at SAS decided to do the hypothesis test and give a p value with the wrong like the theory right.

547
01:03:03,840 --> 01:03:07,980
So we used to have to warn folks, don't look at those p values so our doesn't have them.

548
01:03:09,300 --> 01:03:12,900
And again, we rarely, rarely do we do inference just using I, c and D.

549
01:03:12,900 --> 01:03:19,050
I see if you want to compare between a random intercept and a random slope intercept slope, just look at the CBC.

550
01:03:19,260 --> 01:03:23,280
Don't worry about a p value. It's really done.

551
01:03:23,640 --> 01:03:28,260
So what time I get to actually plug my own research I'll in a class that I teach.

552
01:03:28,260 --> 01:03:32,040
So I had a student, Oliver Lee, who graduated a while ago.

553
01:03:32,040 --> 01:03:39,960
Now I can't believe it, but permutation tests if you want to learn about permutation tests, I'm teaching in 65 next semester,

554
01:03:41,160 --> 01:03:46,320
but we use permutation test as a way to get the null distribution without worrying about likelihood theory anyways.

555
01:03:46,680 --> 01:03:50,190
So don't do inference with random effects.

556
01:03:51,000 --> 01:03:59,790
You can do inference for the fixed effects in an all of them do not do inference for the variance components unless you really, really want to.

557
01:04:00,360 --> 01:04:04,649
And then you should talk to somebody or read up on literature because it's a very

558
01:04:04,650 --> 01:04:08,910
complicated problem and hence why we're not going to do any further in this class.

559
01:04:13,200 --> 01:04:16,230
All right. So I had one other thing.

560
01:04:16,410 --> 01:04:22,520
I got through all of it. All right? Everybody's looking exhausted because they need a fall break.

561
01:04:22,530 --> 01:04:25,870
Right? Let's.

562
01:04:26,370 --> 01:04:31,380
Where did my Internet come from?

563
01:04:31,650 --> 01:04:39,030
Then I close it. I want to ask you guys a couple of questions about your picks. So let's pull out one more pull everywhere.

564
01:04:39,210 --> 01:04:43,830
This is going to be as deep as my other ones, but I want to get a feel for it.

565
01:04:43,830 --> 01:04:47,760
So here I hear how important fall breaks are for everybody, and I totally get it.

566
01:04:47,910 --> 01:04:51,030
So poetry phones. The first thing I want to know.

567
01:04:59,340 --> 01:05:02,999
So let's get rid of that message again. It's two, two, three, three, three.

568
01:05:03,000 --> 01:05:07,740
And it's brand six, eight, six, two, three, four. During fall break, I plan to leave Ann Arbor.

569
01:05:21,390 --> 01:05:27,300
I think we go buy a sample here of dedicated buyers if you want to stay here.

570
01:05:30,870 --> 01:05:36,270
They're going to get more than 20 of them. All right.

571
01:05:38,810 --> 01:05:45,240
This. Lisa.

572
01:05:47,810 --> 01:05:59,420
Okay, so they're just going to feel for what? What's going on? I think the myth is that everybody gets a chance to go away and do something.

573
01:05:59,450 --> 01:06:03,170
I think that's a myth of what the freshmen might do.

574
01:06:04,400 --> 01:06:09,950
But by the time you're in grad school, maybe it's less, you know, less opportunity to travel or you don't have the funds.

575
01:06:10,700 --> 01:06:17,000
But again, in terms of the center break, you're right after the break.

576
01:06:17,030 --> 01:06:20,720
There are just so many midterms and homework right after break.

577
01:06:21,050 --> 01:06:25,520
So I'm planning to leave for the weekend after. Yeah. People are also increasing the study.

578
01:06:25,780 --> 01:06:29,810
So the point of having this weekend, where were you?

579
01:06:30,050 --> 01:06:34,970
So let's go to what's the midterm, you know, one six 8681 there, the stochastic one.

580
01:06:37,670 --> 01:06:45,709
Okay. So that might answer my next question, that those are still leaving anyway.

581
01:06:45,710 --> 01:06:53,870
But I mean, here's this question. When you spoke primarily to Rob's instructions.

582
01:06:54,410 --> 01:07:01,370
If you take a break, are you gonna teach?

583
01:07:15,850 --> 01:07:22,440
And again, there's nothing wrong with getting a break to get better.

584
01:07:23,070 --> 01:07:24,300
That certainly is helpful.

585
01:07:25,680 --> 01:07:33,120
I do hope everybody who said they're going to get caught up in their coursework has an opportunity to lead over into the other group for a little bit.

586
01:07:35,340 --> 01:07:42,480
And I get it. I really do get it. But I do hope you have a chance to do more than think about biostatistics.

587
01:07:44,160 --> 01:07:52,540
But like I said, it is it is wonderful to have two days where you can get caught up on stuff and nothing new is coming at you.

588
01:07:53,800 --> 01:07:57,330
Professor feel that way too. Although, why do we do it to you?

589
01:07:57,870 --> 01:08:07,170
I don't know. I guess because our professors did it to us. Anyway, on Friday we're going to do a little bit more simulating.

590
01:08:08,100 --> 01:08:13,800
Again, I think simulation is a really nice way to understand why we model data the way we do.

591
01:08:13,990 --> 01:08:21,600
I can show you how to simulate with random effects. Do you want to start talking about GDP so that we can get going and normal data,

592
01:08:21,600 --> 01:08:25,680
but we're basically done with normal data for this way through the semester.

593
01:08:26,510 --> 01:08:30,510
So go forth. I don't save homework.

594
01:08:30,510 --> 01:08:36,690
Three till the end. Please don't get it done before full break so you can study for your midterms, right?

595
01:08:37,980 --> 01:08:40,680
All right. See you on Friday.

