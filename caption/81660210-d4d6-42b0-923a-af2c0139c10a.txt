1
00:00:03,570 --> 00:00:09,480
Okay. Good afternoon, everybody. So let's continue talking about and talk about the mixed effects model.

2
00:00:09,630 --> 00:00:14,280
So let me actually open the mic. Hey, guys, hear me in the back?

3
00:00:14,310 --> 00:00:18,750
In the back. Okay, uh, let me start from the top.

4
00:00:19,500 --> 00:00:23,160
We are going to continue discussing the mix effects model.

5
00:00:23,370 --> 00:00:28,740
It is based on hand now solvency. I'm glad that you can make make to the class in person.

6
00:00:28,770 --> 00:00:33,650
For those of you, if you haven't watched the video for oh seven, eight, seven B,

7
00:00:33,660 --> 00:00:38,100
please do so as quickly as possible because this will be part of the midterm.

8
00:00:40,050 --> 00:00:47,190
So for this particular handout, this is going to be a reason, a relatively more technical panel.

9
00:00:49,220 --> 00:01:03,340
And my goal. My goal is to walk you through the major reasons why we need to bother ourselves with lots of mathematical notation.

10
00:01:03,640 --> 00:01:08,050
And my hope is that you will see there's a reason for going through them,

11
00:01:08,590 --> 00:01:12,040
so hopefully that can motivate you a little bit in terms of reviewing these materials.

12
00:01:12,580 --> 00:01:22,840
And number two, and it is about kind of the importance of unit mix model and what why do people use them?

13
00:01:23,530 --> 00:01:27,250
And they have a lot to do with the theoretical aspect of it.

14
00:01:27,760 --> 00:01:32,530
So today we have a few learning objectives.

15
00:01:32,980 --> 00:01:38,980
The first one is to derive the general variance and structure for the inner mix effects models.

16
00:01:39,370 --> 00:01:45,910
In the past lecture on Monday, we have seen one example with random intercepts and random slopes.

17
00:01:46,390 --> 00:01:53,379
And our major conclusion there is the mix effects models are pretty flexible of letting

18
00:01:53,380 --> 00:01:58,570
the marginal variance and the marginal covariance to depend on time in the smooth way.

19
00:01:59,170 --> 00:02:03,820
And we will see that again today. But in more generic and general notation.

20
00:02:04,570 --> 00:02:17,290
Number two, it is about hypothesis testing, for example, in a model that has random intercepts and random slopes.

21
00:02:17,770 --> 00:02:25,020
Can we ask the question? Is the model with only random intercepts good enough?

22
00:02:25,170 --> 00:02:30,020
Right. So that is essentially a hypothesis testing question.

23
00:02:30,030 --> 00:02:35,430
So we will study that and there we will be introducing the weird null distribution.

24
00:02:36,180 --> 00:02:46,650
Number three, we will be talking about the most beautiful part of the universe model, which is the prediction of random effects.

25
00:02:47,070 --> 00:02:55,680
So as you can see for fixed effects, we call estimation, but for random effects,

26
00:02:55,680 --> 00:02:59,850
we call it prediction because it's not a quantity that's fixed a priori.

27
00:03:00,000 --> 00:03:03,990
Rather, it is individual specific. And we're talking about prediction there.

28
00:03:05,550 --> 00:03:11,850
We will write down the form of the prediction and then do some interpretation in the relatively intuitive way.

29
00:03:12,600 --> 00:03:16,140
And there we will introduce a word called a shrinkage.

30
00:03:17,940 --> 00:03:21,300
And the idea is trying to balance that data,

31
00:03:22,020 --> 00:03:28,140
balance the information in the population and information for the subject that you're trying to make prediction for.

32
00:03:28,770 --> 00:03:35,800
Have you guys heard about this word shrinkage before? In what context, if you don't mind me asking.

33
00:03:36,980 --> 00:03:40,160
Bayesian shrinkage. Massive method. Okay, good.

34
00:03:40,340 --> 00:03:47,260
Other contexts. Have you guys heard about this?

35
00:03:47,590 --> 00:03:54,050
This is you can totally say no because this is just a theoretical concept.

36
00:03:54,070 --> 00:04:01,890
So have you guys heard about Stein's paradox? No.

37
00:04:02,700 --> 00:04:08,820
So this one has a YouTube video link. If you interested, I can send them to you.

38
00:04:09,240 --> 00:04:19,520
But essentially, this is the. I think this is a 1950s paper that basically revolutionized the way of doing estimation,

39
00:04:20,480 --> 00:04:28,220
and that's where people motivate the Bayesian shrinkage and motivate this kind of shrinkage thing.

40
00:04:28,790 --> 00:04:32,750
And in general, if you have heard of have you guys heard about rich regression?

41
00:04:35,020 --> 00:04:41,020
Read regression. Now.

42
00:04:44,290 --> 00:04:48,190
Have you guys have a lasso? Yes or no.

43
00:04:48,650 --> 00:04:54,430
Okay. It's so interesting. You guys hear about last but not rich. Um, so what is the.

44
00:04:55,330 --> 00:05:01,510
The first? Ask me, what's the full name of lasso? Come on.

45
00:05:11,140 --> 00:05:14,920
Yeah. So there's. Yeah. Okay.

46
00:05:14,950 --> 00:05:26,900
Let me do a Google. So it's quite a list, absolute shrinkage and selection.

47
00:05:26,930 --> 00:05:30,200
Operator Okay. So you guys know Lasso and.

48
00:05:35,130 --> 00:05:38,670
And there's a nest representation. The idea is that you must have heard about.

49
00:05:38,790 --> 00:05:43,140
You must have probably used it, but you did not realize there's a thing called shrinkage.

50
00:05:43,530 --> 00:05:52,110
So the third objective today is trying to introduce to you what we meant by shrinkage in the context of predicting rhythm effects model.

51
00:05:52,610 --> 00:05:56,550
Anyway, I'm happy that you learned something new today.

52
00:05:57,210 --> 00:06:00,330
So three objectives. Let's start with the first one.

53
00:06:01,260 --> 00:06:06,350
We will talk about the general covariance structure. So this is the model.

54
00:06:06,360 --> 00:06:13,950
Just to recap, in the first bullet, there are three components the fixed effects, the random effects and the residuals.

55
00:06:14,490 --> 00:06:19,410
And I would ask you to pay attention to notation here.

56
00:06:19,650 --> 00:06:27,390
So I although it is written as a single letter, but it is representing a vector of outcome measurements from subject.

57
00:06:28,260 --> 00:06:31,889
And at side, does that matrix is a matrix.

58
00:06:31,890 --> 00:06:43,710
Yeah, it is of any dimension by P and this is and I by one and this is in I by Q and E is in I by what by one?

59
00:06:44,220 --> 00:06:49,350
Theta is P by one by is Q by one. I'll give you 30 seconds to take this on in.

60
00:07:09,850 --> 00:07:13,910
So we also made some additional assumptions.

61
00:07:13,990 --> 00:07:21,200
The first one is that we often assume by is independent that that side is independent.

62
00:07:21,350 --> 00:07:28,719
If you and I and we assume that the covariance of by is in Matrix G and the covariance

63
00:07:28,720 --> 00:07:37,140
of option I is a matrix code are often often we set them to be Sigma Square and I yeah,

64
00:07:37,240 --> 00:07:45,610
I now equals identity, but in the most generic form we can just treat this as a positive, definite matrix.

65
00:07:45,910 --> 00:07:50,379
And G here is basically representing the variance.

66
00:07:50,380 --> 00:07:55,900
Covariance is for the random effects and how many random if that's cute of them.

67
00:07:55,900 --> 00:08:03,730
So it will be, uh, Q by Q and there we have G one to up to g1q and so on and so forth.

68
00:08:04,450 --> 00:08:08,200
And these are the primary assumptions.

69
00:08:08,200 --> 00:08:13,930
And I think we also assume that the expectation of by is zero.

70
00:08:14,350 --> 00:08:19,030
Yeah. And also the expectation of July is zero two.

71
00:08:20,020 --> 00:08:24,550
So I need to add these a little tilde on the bar on the bottom, not the exact size of matrix.

72
00:08:29,930 --> 00:08:34,130
All right. So there we have the primary set of assumptions now.

73
00:08:36,150 --> 00:08:42,590
The currencies are basically measuring, um, you know, variability and variability.

74
00:08:42,970 --> 00:08:49,010
Right. So with the conditional mean we have this form.

75
00:08:49,020 --> 00:08:57,510
Yeah. So this is averaging out all the random measures we can do, the further marginalization,

76
00:08:57,630 --> 00:09:07,620
which is to say we can average over the individual specific random effects because this is a sum and expectation over buy is in any operation.

77
00:09:07,980 --> 00:09:11,640
And we know that expectation of buy has is zero.

78
00:09:12,300 --> 00:09:18,780
So this second bullet suggests that the marginal expectation is just outside time speed up.

79
00:09:19,430 --> 00:09:23,640
Now we need to move on to talk about the currencies, which is of this form.

80
00:09:24,300 --> 00:09:29,610
So covariance of y can be calculated like this. So this is a result I want to show you first.

81
00:09:30,240 --> 00:09:36,160
It essentially it is comprised of two components. The first one, as you can see, it involves a Z.

82
00:09:36,210 --> 00:09:39,940
I remember Z is it is a matrix of random effects.

83
00:09:39,960 --> 00:09:43,090
G is the variance covariance matrix for the idea.

84
00:09:43,620 --> 00:09:49,240
So this is basically the. How to say within subject variability.

85
00:09:55,360 --> 00:10:11,710
The ME. Okay.

86
00:10:12,160 --> 00:10:20,049
All right. On the other hand, is representing the variance covariance matrix for the measurement errors after you

87
00:10:20,050 --> 00:10:26,950
have modeled the population mean the exposure and also the random effects that zip by.

88
00:10:27,340 --> 00:10:31,170
So this one is uh oh.

89
00:10:31,330 --> 00:10:35,649
Actually, I made a mistake here. I apologize. So you have to start with me.

90
00:10:35,650 --> 00:10:41,860
So this is ah, between subject. If you have wondered about this good job between subject.

91
00:10:42,790 --> 00:10:54,960
And this is within subject. Okay.

92
00:10:56,370 --> 00:11:03,480
So it is a sum of two components. The reason why our eye is called within subject variability is because it's measuring it's measurement error.

93
00:11:03,690 --> 00:11:13,160
So going from one point to another, all the variation comes from, you know, the error made by the measuring instrument upon that person.

94
00:11:13,170 --> 00:11:17,340
So this, this is capturing the variability in the measurement.

95
00:11:17,700 --> 00:11:25,740
While for the zei gonzi prime there it is characterizing variability regarding the random effects.

96
00:11:26,460 --> 00:11:30,420
And random effects also are introduced to characterize how different people are.

97
00:11:32,690 --> 00:11:37,700
We will work out this particular formula first and then introduce some special cases.

98
00:11:39,570 --> 00:11:43,969
So. Covariance of what I look at here.

99
00:11:43,970 --> 00:11:50,780
We do not have any conditioning upon by, so we need to just write everything inside.

100
00:11:54,010 --> 00:12:01,020
The coverage appeared to be like this. All right.

101
00:12:01,170 --> 00:12:04,650
So we have this now. We need to call a few facts.

102
00:12:04,680 --> 00:12:12,420
The first one is that we treat EXI and beta as fixed quantities in this particular set of slides.

103
00:12:12,810 --> 00:12:16,830
So covariance of exhibitor is just zero.

104
00:12:18,040 --> 00:12:24,610
Plus the covariance of z I'd buy plus option I now.

105
00:12:31,260 --> 00:12:39,360
Now, to simplify further, we got to use the fact that VA is independent of the measurement errors.

106
00:12:40,020 --> 00:12:50,130
So that means we can write the entire thing into two separate currencies like this because they are not correlated.

107
00:12:53,070 --> 00:12:59,310
Then it is basically some simple application of calculated variance.

108
00:12:59,610 --> 00:13:04,920
So whenever you see something, multiply it by a random vector.

109
00:13:05,520 --> 00:13:08,610
What do you do? You basically copy this.

110
00:13:09,420 --> 00:13:12,870
You transpose this and then put a convergence API inside.

111
00:13:14,970 --> 00:13:19,400
You may ask him to ensure this calculation is correct. Muscle memory.

112
00:13:21,170 --> 00:13:31,100
So. You can check if you want, but am hesitant to derive that entirely element by element.

113
00:13:31,760 --> 00:13:36,650
But this is the kind of the generic form and I have taught you how to.

114
00:13:38,100 --> 00:13:41,190
Factor these out. Okay.

115
00:13:41,640 --> 00:13:45,450
Now, the second thing is how what's the cover and submission I hear?

116
00:13:45,840 --> 00:13:49,590
Well, what's the provenance of the measurement error? It is.

117
00:13:49,680 --> 00:13:54,780
Right, right. We have designated AI to represent that.

118
00:13:56,580 --> 00:14:02,730
So a few remarks. The first one is, hey, why is our index by here?

119
00:14:05,650 --> 00:14:13,550
I would want some of your thoughts here. Remember all rise the measurement error variance covariance matrix for subject i.

120
00:14:15,120 --> 00:14:22,840
Why is it possibly different across people? What are the things that may be different across people?

121
00:14:25,420 --> 00:14:29,739
People necessarily have the same set of measurement timings. No.

122
00:14:29,740 --> 00:14:39,130
Right. If the if the measurement error depends on time, then it's possible that the measurement error structure may be different.

123
00:14:42,240 --> 00:14:58,370
Number two is. We have said earlier that often we do this, but there are situations where we want to set this area structure to be auto regressive.

124
00:14:59,760 --> 00:15:04,110
So in those cases. You know, or exponentially sorry.

125
00:15:04,410 --> 00:15:05,190
In those cases,

126
00:15:05,190 --> 00:15:13,440
you will have to take into account of the gaps between measurement times and those gaps between measurement times can be different across people.

127
00:15:14,250 --> 00:15:21,149
So this is just a very generic notation in most of the discussion we're going to in most of the data application,

128
00:15:21,150 --> 00:15:28,250
you encountering the homework, whatever usually we do, I equals R equals Sigma Square and I.

129
00:15:28,260 --> 00:15:33,180
But this, uh, this is often true.

130
00:15:33,180 --> 00:15:41,000
But when I ask you to do some more complicated measurement error, covariance structure, I will tell you that.

131
00:15:41,010 --> 00:15:48,360
So I would say if you do not see any instructions about what to use for, I, I would say just go for the simple as possible.

132
00:15:50,580 --> 00:16:00,090
Number two, what would happen if we only have one single random intercept, which means that we have z equals one one, one two, one.

133
00:16:00,090 --> 00:16:03,960
So it's in a by one matrix. What would happen here?

134
00:16:07,540 --> 00:16:11,290
Oh, actually this one should be G, right. We define this term to be G.

135
00:16:13,330 --> 00:16:19,660
So when you have this kind of design matrix representing the random intercept.

136
00:16:25,440 --> 00:16:30,600
Right. So what you get is that it's. Let's consider three occasions.

137
00:16:30,850 --> 00:16:40,560
Right. So it will be 111 times G times 111 plus.

138
00:16:40,890 --> 00:16:44,250
If we use a simple one here, sigma squared.

139
00:16:45,250 --> 00:16:48,850
111000000.

140
00:16:48,880 --> 00:17:00,290
Right. So what do we get? Okay.

141
00:17:00,380 --> 00:17:06,920
Let's calculate this. So let's just plug in the G here.

142
00:17:14,300 --> 00:17:17,420
Oh. Choo choo choo choo choo. Three, three.

143
00:17:18,520 --> 00:17:21,660
G one, 2g1, three, g two, three.

144
00:17:21,670 --> 00:17:27,610
Here. So let's do the this part first.

145
00:17:27,760 --> 00:17:38,250
So this. This is what this is. This should be a three by three matrix, right?

146
00:17:39,180 --> 00:17:45,560
Oh. Is this right? Let me see. Okay.

147
00:17:45,980 --> 00:17:50,600
This is correct. I made a mistake here if you're wondering this.

148
00:17:51,170 --> 00:17:58,550
So, uh. G in this case is just simply sigma squared.

149
00:18:00,910 --> 00:18:09,610
What a should I say be here. Right. Because if we are only considering one random intercept, then we only have one.

150
00:18:11,540 --> 00:18:14,560
Random effect. Per person. Right.

151
00:18:14,570 --> 00:18:24,300
So you can only talk about variances. You just we want you don't have any conveniences.

152
00:18:24,660 --> 00:18:27,690
So, Jean, this case is basically one by one, a number.

153
00:18:28,260 --> 00:18:36,419
When you have this, essentially, you can calculate the first two turn to be singular squared, B sigma squared, B Sigma Square.

154
00:18:36,420 --> 00:18:39,480
B, Right. And then you do one, one, one.

155
00:18:40,890 --> 00:18:45,470
All right. So this would give you what? Give you.

156
00:18:47,640 --> 00:18:53,540
Sigma be squared. Sigma B squared sigma b squared.

157
00:18:54,620 --> 00:19:03,290
Sigma B squared. Sigma B squared. Right.

158
00:19:03,310 --> 00:19:07,630
So you have this. Now, if you add these two things together.

159
00:19:13,520 --> 00:19:17,060
What do you get? What you get will be just.

160
00:19:20,300 --> 00:19:23,840
Sigma squared plus sigma squared. Sigma B squared.

161
00:19:24,590 --> 00:19:37,910
Some B squared. All right.

162
00:19:38,420 --> 00:19:43,790
So you got this one again. This, as you can see, is fully determined by two parameters.

163
00:19:44,240 --> 00:19:47,450
And all the numbers on the diagonals are the same.

164
00:19:47,450 --> 00:19:53,720
All the numbers of diagonals are the same. So this is a compound symmetry, total covariance, covariance structure.

165
00:19:54,670 --> 00:20:04,420
So hopefully this demonstrates to you that under a special case of no equals three and a single random intercept, this formula works.

166
00:20:05,080 --> 00:20:09,580
And this one has been derived before. So I was just reviewing this with you.

167
00:20:12,750 --> 00:20:17,729
Okay. So going back to this formula, before we leave this particular slide,

168
00:20:17,730 --> 00:20:25,469
I want to say that what we have calculated is a marginal covariance of y and terminology wise,

169
00:20:25,470 --> 00:20:33,990
because this term is a sum of two parts, the between subject variability and the within subject variability.

170
00:20:34,020 --> 00:20:40,290
Right. We usually call this total variability or total variance covariance.

171
00:20:48,690 --> 00:20:51,780
Okay. So this is not a name that I would use.

172
00:20:56,470 --> 00:21:01,330
And also about the terminology of between and within. So let's look at the first term, right?

173
00:21:01,330 --> 00:21:05,560
So the first term was this one.

174
00:21:09,570 --> 00:21:17,130
So every. Element in that matrix is about the variability of the random intercept.

175
00:21:18,820 --> 00:21:22,060
And the variability of the random intercept is about.

176
00:21:23,790 --> 00:21:27,090
Between subject variability. So that's the reason for that name.

177
00:21:28,030 --> 00:21:37,840
And for the second part, clearly you can see. All the variability are due to the measurement error.

178
00:21:37,850 --> 00:21:41,000
So that's why we call that within subject variability.

179
00:21:42,200 --> 00:21:45,920
All right. Any questions before we move on to next slide?

180
00:21:59,030 --> 00:22:02,600
Okay. So just to provide you some summary.

181
00:22:03,410 --> 00:22:11,300
Um, essentially we have decomposed the marginal or the total variance covariance into two parts,

182
00:22:11,840 --> 00:22:17,930
the part that's for between subject variability, the part that's for within subject variability.

183
00:22:18,800 --> 00:22:24,680
And what are the key implications based on this calculation?

184
00:22:26,300 --> 00:22:30,420
Uh. So.

185
00:22:31,610 --> 00:22:38,500
The. Basically it is quite clear that a mixed model.

186
00:22:39,400 --> 00:22:43,390
Is partitioning the total variability into two parts.

187
00:22:43,750 --> 00:22:54,280
And when your scientific goal is trying to characterize the two sources of probability, you probably would consider the mix effects model first.

188
00:22:55,590 --> 00:23:02,700
Okay. Instead of doing the general model where only a total variance covariance matrix would need to be specified.

189
00:23:07,800 --> 00:23:16,920
And. A subtle point which we will fully address when we learn the general generalized mixed model.

190
00:23:17,520 --> 00:23:20,520
It is about the form of this marginal mean.

191
00:23:21,690 --> 00:23:28,350
So in that calculation we showed earlier, when you average over individual specific random effects.

192
00:23:29,730 --> 00:23:37,080
Which we have called marginal. Sometimes people call that population averaged because you average over the pie in the population.

193
00:23:37,590 --> 00:23:50,550
What you get is to say that excessive beta, the beta has a indentation not only at the individual level but also at the population level.

194
00:23:51,870 --> 00:23:58,770
So what do I mean by that? Well, in this formula, you do not see any conditioning upon by.

195
00:23:58,800 --> 00:24:04,420
Right. So you are talking about a population. Relationship between the culvert and the outcome.

196
00:24:05,200 --> 00:24:10,750
So Peter here has the interpretation of population averaged association strength.

197
00:24:12,090 --> 00:24:22,620
If I scroll back to this particular slide, if you look at bullet one bullet one says that first you see a beta there, right?

198
00:24:23,160 --> 00:24:27,299
But the equation in bullet one is conditioning.

199
00:24:27,300 --> 00:24:34,260
A pump by it is to say, let's not talk about, you know, genetics, whatever.

200
00:24:34,440 --> 00:24:41,770
Let's just talk about one person and that person only by conditioning upon his or her by right.

201
00:24:41,850 --> 00:24:46,650
And then we ask, what's the relationship between the coverage and the outcome?

202
00:24:48,030 --> 00:24:52,710
Right. Peter, there again is that association parameter.

203
00:24:53,580 --> 00:25:00,240
So I call in that so I can interpret beta as a individual specific.

204
00:25:02,790 --> 00:25:13,950
I would say for every unit change in covered x beta characterized the corresponding change in the average outcome given by right.

205
00:25:14,490 --> 00:25:26,040
So you can see beta appearing in this formula and also in this formula which suggests that beta has two ways to be interpreted.

206
00:25:26,370 --> 00:25:32,820
One, in the population, as is is shown here, the other as individual level covariate affects.

207
00:25:34,940 --> 00:25:41,690
This. However, is not true when you're dealing with binary, you know.

208
00:25:43,190 --> 00:25:45,650
Count with other categorical outcomes.

209
00:25:46,790 --> 00:25:58,370
The reason will be technical, but the implication is huge, which means that depending on your research interest, you would want to use unit mix model.

210
00:25:59,430 --> 00:26:02,970
Which has individual specific effect. Interpretation or.

211
00:26:03,210 --> 00:26:10,590
You want to directly start with this model specification completely forgoing the need to

212
00:26:10,590 --> 00:26:16,350
specify random effects so we will touch them primarily in the generalized and mixed model.

213
00:26:18,100 --> 00:26:21,140
But this should be a prelude. All right.

214
00:26:23,740 --> 00:26:32,290
So that completes part one. Now, we have looked at the mathematical form of all these.

215
00:26:34,380 --> 00:26:38,700
Nina makes a model. How do we choose among them?

216
00:26:40,050 --> 00:26:44,400
So this is a this is a Costco problem.

217
00:26:44,790 --> 00:26:50,429
Instead, Cisco inference, whenever you need to consider a parsimonious model,

218
00:26:50,430 --> 00:26:54,390
but you're not sure whether that's that's going to be not fit in there too well.

219
00:27:00,290 --> 00:27:05,640
So just a quick recap of the notations here in the first bullet.

220
00:27:05,660 --> 00:27:14,210
What you see here is a marginal response or what we have called population average mean response in the marginal variance or the total variance.

221
00:27:14,510 --> 00:27:20,930
We have derived that it looks like this and this is obtained by setting r i equals sigma i.

222
00:27:20,930 --> 00:27:24,440
Sigma squared times i and I here.

223
00:27:26,070 --> 00:27:29,790
How many various coverage parameters are there? Well.

224
00:27:31,550 --> 00:27:37,650
We got to count the number of. Various parameters in G.

225
00:27:39,400 --> 00:27:48,130
And also the number, the single parameter here sigma squared g is a variance covariance matrix for KU random effects.

226
00:28:01,470 --> 00:28:05,130
So it is not surprising that G is Q back q.

227
00:28:09,420 --> 00:28:15,450
Okay. And how many parameters do we need for a completely unrestricted Q by Q covariance matrix.

228
00:28:16,500 --> 00:28:24,400
So it is. All the numbers on the diagonals or the numbers on the off diagonals.

229
00:28:24,430 --> 00:28:29,350
Again, it is Q times Q plus one divided by two, hence this number.

230
00:28:29,890 --> 00:28:45,490
Okay. And I want to say that this calculation does not care the actual timings of the measurements out.

231
00:28:46,670 --> 00:28:54,260
Because it is going to produce a very parsimonious characterization of the variance covariance matrix.

232
00:28:56,660 --> 00:29:03,140
Okay. Oh, I think some people would often ask, hey, in general, in this context,

233
00:29:03,740 --> 00:29:10,190
do we have the option to constrain the diagonal elements of G to be identical?

234
00:29:10,790 --> 00:29:17,600
Often we do not because for example, in the model with random intercepts, random slopes,

235
00:29:18,410 --> 00:29:25,880
you would never know a priori that the variability of the random intercepts and the variability of the random slopes are the same.

236
00:29:27,200 --> 00:29:30,980
You will you would need very strong evidence to to suggest they are the same.

237
00:29:31,430 --> 00:29:40,400
So generically, in most of these dynamics model software, we just leave G as complete unstructured.

238
00:29:58,310 --> 00:30:02,330
Because it's hard to gather information a priori about the probability their.

239
00:30:04,630 --> 00:30:13,090
And as you will see, although this notation is general in most applications, random intercept at random slopes would suffice.

240
00:30:13,600 --> 00:30:24,700
So in my experience, I have never used a model that's more complicated than random random slopes that could be in for some complicated models.

241
00:30:24,700 --> 00:30:32,080
But that's quite rare. Now the question is how do we choose?

242
00:30:32,100 --> 00:30:39,150
Q Which is to say how many? How do we choose the number of random effects we would need in a particular data application?

243
00:30:39,870 --> 00:30:44,160
So this is the problem set up.

244
00:30:44,700 --> 00:30:51,820
The larger model has Q plus one correlated random effects can be positively correlated or can be negatively correlated.

245
00:30:51,870 --> 00:30:59,400
I'm just going to use correlated as a generic capitalization that random effects may have some correlation.

246
00:30:59,670 --> 00:31:06,000
The small model is one. It contains one less, one fewer random effect.

247
00:31:07,520 --> 00:31:17,090
Okay. So the first question is how many additional parameters do we have for the bigger model?

248
00:31:22,710 --> 00:31:26,550
The answer is there, but clearly you need to figure out why. That's Q plus one.

249
00:31:28,970 --> 00:31:35,540
So shown at the bottom. It is a variance covariance matrix for the random effects.

250
00:31:35,600 --> 00:31:45,709
Right. So my question is how many and which elements in this matrix needs to be set to

251
00:31:45,710 --> 00:31:51,170
particular values so that you recover the model with just two random effects?

252
00:32:10,450 --> 00:32:17,140
Suppose our. Suppose the random effect in question is has index of Q plus one.

253
00:32:17,440 --> 00:32:25,150
Suppose we have our decided it is the Q plus one random effect that we want to consider whether it's needed or not.

254
00:32:28,900 --> 00:32:32,830
So first, how do we get rid of a random effect?

255
00:32:33,730 --> 00:32:37,300
How do we. How do we. Force that.

256
00:32:47,960 --> 00:32:56,730
So let's look at this one. So. Let's just consider how to eliminate random random intercepts.

257
00:32:57,210 --> 00:33:03,240
Suppose this is the data for a day and this is the data for subject B.

258
00:33:04,950 --> 00:33:11,520
And this is. This is B.B. and we have some variants of beer era.

259
00:33:14,500 --> 00:33:22,030
So how do you force the main trajectory for R&B to be collapsed with the population line in the center?

260
00:34:02,440 --> 00:34:06,910
So do we want to have like larger variability of babies or smaller?

261
00:34:10,790 --> 00:34:14,189
Smaller, right? And what's the extreme case?

262
00:34:14,190 --> 00:34:18,329
That there's no viability. When is the Coke Zero?

263
00:34:18,330 --> 00:34:22,530
Right. So when this equals zero, everybody has to be here.

264
00:34:23,570 --> 00:34:31,430
Or here? I don't know. Here. So this gives you one example where to eliminate the random.

265
00:34:32,830 --> 00:34:38,680
Intercepts. You just said the variability of the random intercepts should be zero.

266
00:34:39,580 --> 00:34:47,830
Now, returning to the Matrix, we we show here on the slide if our goal is to eliminate the Q plus one random effect.

267
00:34:48,820 --> 00:35:05,230
What should we do? So it must have something to do with B.

268
00:35:07,680 --> 00:35:13,630
Q plus one IRA. Should we set this to zero?

269
00:35:14,910 --> 00:35:20,650
Okay. So which which quantity represents this very, very variance.

270
00:35:20,680 --> 00:35:31,330
Sorry. Which term? So it is this one, right?

271
00:35:31,480 --> 00:35:37,790
So we've got to set this 1 to 0. How about are there other terms that should be zero?

272
00:35:42,990 --> 00:35:47,610
Suppose I am a quantity with a variable below zero. What's your correlation with me?

273
00:35:50,480 --> 00:35:57,710
Zero, right? So because all these elements are representing the covariance between.

274
00:35:58,860 --> 00:36:02,630
Any other random effects with a Q plus ones?

275
00:36:02,640 --> 00:36:08,060
Random effects. If I set. Variants of B, Q plus one.

276
00:36:09,420 --> 00:36:14,820
Com i20. All the other covariance is also zero.

277
00:36:15,690 --> 00:36:19,710
So to do. To reduce the model.

278
00:36:22,010 --> 00:36:26,780
With Q plus one random effects to a model with two random effects.

279
00:36:27,760 --> 00:36:34,210
With that. We just said the one column to be completely to be all zeros.

280
00:36:34,660 --> 00:36:40,360
Right. And here because the random effect under consideration is a Q plus one.

281
00:36:40,420 --> 00:36:46,640
So we just said the final column to be zero. Does this sound logical to you guys?

282
00:36:49,030 --> 00:36:53,350
Okay. I need the affirmative answer there. This is the bottom.

283
00:36:55,840 --> 00:37:03,550
Yeah. Because of this matrix symmetric. So you must have that. Yeah.

284
00:37:10,180 --> 00:37:19,830
Okay. So, yeah. So essentially what you did here, what we did here is that we have gone from the.

285
00:37:20,840 --> 00:37:33,070
Model with. I'm not going to observe a number of parameters, but we have removed Q plus one parameters, right?

286
00:37:33,100 --> 00:37:37,840
Because the final column contains. Q Plus one parameters.

287
00:37:41,730 --> 00:37:45,690
So now this is setting the stage for what?

288
00:37:45,960 --> 00:37:51,930
For like a racial test because we are setting certain parameters to specific values.

289
00:37:52,530 --> 00:37:58,740
So the smaller model with Q random, if X is nested within the bigger model with Q plus one random effects.

290
00:38:00,330 --> 00:38:11,130
However, as we have alluded to probably two or three lectures ago, we are dealing with a situation where the theory of maximum likelihood,

291
00:38:13,050 --> 00:38:20,010
the regularity conditions that we often do not do not read that condition actually felt right.

292
00:38:20,760 --> 00:38:29,220
Can anybody recall what's that condition for MLT to be consistent then asymptotically normal?

293
00:38:35,490 --> 00:38:39,590
I can. I can. I can try to refresh your memory little bit.

294
00:38:40,550 --> 00:38:48,500
Essentially the novel. Hypotheses needs to be represented by a parameter value that's in the.

295
00:38:49,810 --> 00:38:54,150
How to say that's. An inner point of the parameter space.

296
00:38:55,230 --> 00:39:04,260
So if you have a parameter space where you are theta can take value from the MLP regularity condition.

297
00:39:04,680 --> 00:39:08,580
What do you require that the truth? Is.

298
00:39:09,940 --> 00:39:14,710
Interior points, which means that you can draw a ball.

299
00:39:15,280 --> 00:39:18,670
Well, it's 2D here, so I can only draw a circle. But imagine it's a.

300
00:39:20,680 --> 00:39:29,830
Possibly higher than two D So you can draw a ball around the truth and everything so the ball will be contained within that parameter space.

301
00:39:31,360 --> 00:39:36,100
Is this true? In our setting here. It is not.

302
00:39:36,340 --> 00:39:40,250
Why? Well. GQ Press one.

303
00:39:40,910 --> 00:39:45,440
Q Plus one is always going to be positive or zero.

304
00:39:46,250 --> 00:39:51,710
And what we just said is that to be able to reduce the larger model to the smaller model.

305
00:39:52,700 --> 00:39:59,599
We got a set GQ plus one Q plus one to be zero, which unfortunately is on the boundary.

306
00:39:59,600 --> 00:40:03,050
And you cannot do such a thing that you draw a ball around it. Right.

307
00:40:03,200 --> 00:40:10,380
Well. This is the parameter space and this, unfortunately, is a truth.

308
00:40:10,620 --> 00:40:20,710
So it is the the null. Where you're hoping to investigate and you cannot draw a ball around it.

309
00:40:23,230 --> 00:40:30,490
So that entire thing is contained within that parameter space that's greater than than zero or equal to zero.

310
00:40:31,300 --> 00:40:35,890
So this is a situation where the regularity condition.

311
00:40:41,110 --> 00:40:46,060
For Emily, and indirectly, the lack of racial test would fail.

312
00:40:47,760 --> 00:40:53,530
Oh. Fails. Okay.

313
00:40:55,580 --> 00:40:58,700
So have people noticed this?

314
00:40:59,300 --> 00:41:03,650
Yes, people have. Have people provided solutions? Yes, people have.

315
00:41:04,430 --> 00:41:14,020
Now, let's talk about the solution. So the problem can be generally summarized like this.

316
00:41:15,130 --> 00:41:18,640
The usual. Now, distribution for the recreational test is no longer valid.

317
00:41:19,120 --> 00:41:23,350
In particular, the comparison. The random effects model for the covariance is such a nonstandard problem.

318
00:41:23,880 --> 00:41:29,020
All right, so we have hit the jackpot that we have to deal with something new.

319
00:41:29,770 --> 00:41:34,390
So quick question. What's the usual null distribution for like racial test?

320
00:41:36,840 --> 00:41:44,320
What? Distribution. A C word. How do you pronounce this?

321
00:41:47,130 --> 00:41:51,330
Chi squared chi square distribution. So.

322
00:41:54,030 --> 00:42:02,730
This slide basically is providing you a kind of summary of what people discover to be the right now distribution.

323
00:42:03,390 --> 00:42:08,730
So in general, when you are testing all hypotheses, that's on the boundary of parameter space,

324
00:42:08,730 --> 00:42:12,690
like when the null is the variance of the random effect being zero,

325
00:42:13,200 --> 00:42:19,050
the usual the usual null distribution for the arguments official test is no longer a chi squared distribution

326
00:42:19,410 --> 00:42:24,900
with a degree of freedom equal to the difference between number of parameters in the for and the reduced model.

327
00:42:26,130 --> 00:42:32,220
Instead, the null distribution is a mixture of chi square distributions.

328
00:42:32,400 --> 00:42:38,250
Okay. So this is something new you probably have not heard about before, but this is the important piece,

329
00:42:38,580 --> 00:42:42,930
which means that compared to what you have been doing in like original tests

330
00:42:42,930 --> 00:42:48,329
where you just count the difference in the parameters between the two models,

331
00:42:48,330 --> 00:42:49,740
the larger one, the smaller one,

332
00:42:50,130 --> 00:42:56,970
and you assume that theory will magically work and the null distributions chi squared with that particular degree of freedom.

333
00:42:57,360 --> 00:43:01,470
That theory is not going to hold because the null is on the boundary.

334
00:43:02,310 --> 00:43:11,520
So now the question becomes then how do we decide the mixture now mixed the mixture form of the null distribution.

335
00:43:13,020 --> 00:43:19,230
So interestingly, the null is now just 2.5 times Chi Square.

336
00:43:19,320 --> 00:43:22,350
Q Plus 0.5 Chi Square.

337
00:43:22,500 --> 00:43:27,860
Q Plus one. Okay. Now, I'm going to ask you to do some quick.

338
00:43:31,030 --> 00:43:34,060
Just to figure out what this means. Let's put this in action.

339
00:43:37,860 --> 00:43:44,380
So. In our case, we were interested in testing.

340
00:43:45,640 --> 00:43:54,650
GQ plus one. Q Plus one equals zero versus alternative hypotheses that G.

341
00:43:55,120 --> 00:44:00,220
Q Plus one. Q Plus one is greater than zero, right?

342
00:44:02,640 --> 00:44:12,210
So the claim here is that like a racial test should be following a normal distribution as a mixture.

343
00:44:12,720 --> 00:44:33,600
So I'm just going to rewrite this whole thing. I'm going to use another color to represent what you would do if you have not learned that.

344
00:44:33,600 --> 00:44:38,530
This lecture. You would naively.

345
00:44:41,210 --> 00:44:46,610
And incorrectly. Assume that this like a racial test would follow.

346
00:44:46,610 --> 00:44:52,040
What? Chi Square.

347
00:44:52,160 --> 00:45:00,620
With how many degrees of freedom? Well, how many parameters we set to zero in the previous model.

348
00:45:02,290 --> 00:45:07,420
Q Plus one. So that would be your naive all distribution.

349
00:45:09,160 --> 00:45:15,970
Okay. I want to make sure we're all on the same page here regarding the possible mistake we can make.

350
00:45:16,600 --> 00:45:23,260
The reason why we need to know this is because you need to communicate, at least not only to me or to the exam, whatever.

351
00:45:23,950 --> 00:45:29,860
That's you have to communicate to other people who care about this inference why there can be consequences.

352
00:45:30,880 --> 00:45:34,750
So I'm going to draw to draw one big figure.

353
00:45:37,730 --> 00:45:41,300
This is a bit complicated. I want to go slow, but I see you guys can follow it.

354
00:45:46,290 --> 00:45:53,099
So first I want to say that I want to ask the first question for Chi Square distribution with

355
00:45:53,100 --> 00:45:58,080
two degrees of freedom versus a Chi Square distribution with two plus one degrees of freedom.

356
00:45:58,620 --> 00:46:01,800
Which one is going to be stochastic with smaller?

357
00:46:06,810 --> 00:46:11,820
I if I have to dance this one for Chi Square, cue the other for Chi Square two plus one.

358
00:46:11,820 --> 00:46:16,210
Which one is going to be? The two left and which one is going to be to the right?

359
00:46:17,300 --> 00:46:28,580
Well, what's the meaning of the Chi Square distribution? I think you guys should know this.

360
00:46:31,130 --> 00:46:38,360
It's a very simple answer. It's just quite.

361
00:46:41,970 --> 00:46:47,210
Chi square k with ming of K.

362
00:46:48,620 --> 00:46:51,640
Right. So you have girls in verbal. You take the square.

363
00:46:51,670 --> 00:46:55,220
What's that mean? One.

364
00:46:55,320 --> 00:47:00,080
Right. Because Chi Square distribution with one degree of freedom just square of the stand normal.

365
00:47:01,520 --> 00:47:06,379
Now, if you look at this figure. Right. I'm very good at using weekend teaching.

366
00:47:06,380 --> 00:47:08,690
So if you are looking at these colors.

367
00:47:08,900 --> 00:47:19,220
So this one is representing the chi square distribution with K equals nine degree freedom of six cables for 3 to 1.

368
00:47:19,880 --> 00:47:26,810
So in General Chi Square, distributions with larger degrees of freedom is going to put more mass towards larger values.

369
00:47:27,740 --> 00:47:35,980
Right. I can call that. Chi Square distribution with larger degrees of freedom are in general stochastic, way larger.

370
00:47:37,490 --> 00:47:42,360
I. So that's the first fact. Now returning to the slide.

371
00:47:43,290 --> 00:47:48,810
Is it okay that I draw something like this? What's the color I'm having?

372
00:47:50,630 --> 00:47:57,410
It's a blue. Okay, that's good. So let's say this is Chi Square distribution with Q plus one.

373
00:47:59,440 --> 00:48:03,480
Okay. The naive one now returning to the rat color.

374
00:48:04,030 --> 00:48:08,210
If we're going to draw the mixture. And the mixture.

375
00:48:08,960 --> 00:48:13,310
I do think it'll be towards the left or the right of the blue curve.

376
00:48:15,860 --> 00:48:22,280
What's the effect that the Chi Square distribution with Q degree freedom is going to exert on the mixture?

377
00:48:29,480 --> 00:48:37,300
It's a simple answer. You got to help me. All right. Or you can not look at me and not say the answer.

378
00:48:37,840 --> 00:48:42,080
Otherwise you will avoid eye contact. Left to right.

379
00:48:43,910 --> 00:48:48,110
Left. Yes. So in general, you can envision that it will be roughly here.

380
00:48:50,050 --> 00:48:59,420
Okay. So this is the mixture. Okay.

381
00:49:05,640 --> 00:49:08,650
So. Here.

382
00:49:08,650 --> 00:49:13,720
I'm showing you that's the no distribution that the correct distribution is in general.

383
00:49:13,960 --> 00:49:19,660
To the left. Okay. Now I am going to.

384
00:49:23,380 --> 00:49:32,190
Ask. One question. Is it harder?

385
00:49:33,920 --> 00:49:37,640
Or easier to reject in the null.

386
00:49:44,850 --> 00:49:49,710
Under the blue approach, you use blue color.

387
00:49:50,860 --> 00:49:57,700
The blue. And which means incorrect.

388
00:50:07,810 --> 00:50:12,930
Now. Distribution. Of.

389
00:50:14,150 --> 00:50:23,550
Chi square ku plus one. So your task is to figure out whether it's harder or easier to reject or not.

390
00:50:24,000 --> 00:50:31,739
If you have not sit in this class and learn that there is a shift to the left for the true not so we

391
00:50:31,740 --> 00:50:36,960
all going to come back at 357 and hopefully this can be some good time for you to think about it.

392
00:52:22,870 --> 00:53:09,120
You know. This is talking about.

393
00:53:11,480 --> 00:56:37,070
But. All right.

394
00:56:37,640 --> 00:56:40,310
Let's get back to work and try to resolve this question.

395
00:56:40,670 --> 00:56:50,720
So the question was posed as, is it harder or easier to inject it all under the incorrect, naive, blue shaped blue color and all distribution?

396
00:56:51,650 --> 00:56:55,070
And if people want to give it a try and the reasoning behind it.

397
00:57:00,310 --> 00:57:09,060
Harder. Harder, yes. Any reason? Because there's there's more probability maps do that to the right.

398
00:57:09,070 --> 00:57:12,340
And so you have to go farther. Yes, exactly.

399
00:57:12,550 --> 00:57:21,520
So to ask some questions. So. Thanks for helping out. So essentially, suppose you are going to decide the regional rejection.

400
00:57:21,520 --> 00:57:29,409
Yeah. So for the red one, you got to have these areas and that sum up 2.5% for the blue one.

401
00:57:29,410 --> 00:57:35,860
You got to have what you got to have like here, which is going to be .05.

402
00:57:35,860 --> 00:57:39,280
Right. So. To be able.

403
00:57:40,350 --> 00:57:47,310
To reject the novel. Under the incorrect novel, you got to have to be to the right of the blue vertical line.

404
00:57:48,210 --> 00:57:55,710
So that's a bigger number. But under the true north, it has way past the vertical red line.

405
00:57:56,130 --> 00:58:06,360
So if you observe a statistic, say r t statistic in this region, then you would do what?

406
00:58:07,110 --> 00:58:11,490
If you use the blue shapes now, you could not reject the null.

407
00:58:12,590 --> 00:58:19,760
However, you would have already read them. Now, if you use a true now on a past 30 seconds because this probably is still a bit.

408
00:58:20,790 --> 00:58:24,780
Yeah. Complicated, but the answer is it's harder to reject the.

409
00:58:25,530 --> 00:58:31,950
Which means that if you pose a parsimonious model, although the data may have perfect signal to detect it,

410
00:58:32,490 --> 00:58:38,550
if you are not aware of this particular node distribution issue, you will be stuck with the.

411
00:58:39,560 --> 00:58:43,180
Parsimonious model, which you probably should. Shouldn't. Yeah. Okay.

412
00:58:43,950 --> 00:59:17,370
30 seconds for you to digest a little bit. All right with that, let's still let's do one exercise.

413
00:59:17,380 --> 00:59:26,150
So if you are trying to compare two models, one model is what one model is on.

414
00:59:28,800 --> 00:59:32,480
Y equals xy theta plus i.

415
00:59:32,550 --> 00:59:37,459
The other one is XY. Sorry.

416
00:59:37,460 --> 00:59:55,300
Let me let me try to rewrite this. Y i j equals b0i plus beta zero plus beta 1tj plus epsilon i j.

417
00:59:55,870 --> 01:00:00,640
So this is a larger model. The smaller model is y j.

418
01:00:02,990 --> 01:00:10,930
Equals zero zero plus beta one thai j plus Egyptian yj oc.

419
01:00:13,940 --> 01:00:28,330
So what is Q? And this will be a midterm exam question.

420
01:00:28,600 --> 01:00:54,320
So try it now. So what do we set to zero.

421
01:00:54,980 --> 01:01:02,460
In the larger model. It is the variance of. The first random, etc.

422
01:01:03,120 --> 01:01:07,499
So Q plus one is the if random.

423
01:01:07,500 --> 01:01:13,260
In fact, we're going to set a zero and this is the for the larger model, we only have one.

424
01:01:13,260 --> 01:01:20,960
So that's the first random. So Cooper. Cooper zero, right. So in this particular case, we just choose quick zero.

425
01:01:21,440 --> 01:01:24,680
And then if you're just talking about if you're working with.

426
01:01:27,400 --> 01:01:31,990
The one i. T. I. J. And you want to keep the.

427
01:01:35,950 --> 01:01:39,040
Keep the random intercept. Then what do you do?

428
01:01:39,070 --> 01:01:42,610
Well, you just use a Q equals.

429
01:01:45,350 --> 01:01:52,130
Why in that case? Because you have one random intercept and the second one is the one you want to get rid of.

430
01:01:52,700 --> 01:02:01,069
So you close one and essentially you will refer to this table where it basically lists,

431
01:02:01,070 --> 01:02:09,860
enumerates all the shoes you would need to use and also all the critical values for particular significance levels.

432
01:02:10,760 --> 01:02:14,300
So for example, for cubicle zero.

433
01:02:15,690 --> 01:02:23,830
Then the critical value is 2.71 for Q equals one, the critical value is 5.1 for something like that.

434
01:02:24,630 --> 01:02:28,530
So in the following example.

435
01:02:30,700 --> 01:02:34,690
Uh, let me see what we have here. So we have.

436
01:02:36,010 --> 01:02:45,910
A random intercept and random slope. And we want to ask between these two models whether this model model is good enough to fit the data.

437
01:02:46,480 --> 01:02:53,410
So in this case, we have two equals one. Now, if we have a statistic.

438
01:02:56,580 --> 01:02:59,690
That's, say, five point. Three.

439
01:03:00,410 --> 01:03:06,920
Right. Then this statistic will be between the two critical values.

440
01:03:07,340 --> 01:03:12,950
So what are the two critical values? So if a critical is one, we go back to this table.

441
01:03:13,580 --> 01:03:26,100
This is what this is 5.14. And this corresponds to, you know, the critical value of actually the quantile.

442
01:03:27,770 --> 01:03:33,010
The 95% quantile for. Chi Square distribution.

443
01:03:34,070 --> 01:03:39,580
Actually mixture of Chi Square one plus point five, chi square two.

444
01:03:39,860 --> 01:03:45,790
Right. So this is the, um, the critical value.

445
01:03:46,670 --> 01:03:52,280
And for this one, if you are using the naive or incorrect and all distribution,

446
01:03:52,310 --> 01:03:57,590
this would be the quantile 95% quantile of Chi Square distribution with two degrees of freedom.

447
01:03:58,220 --> 01:04:00,320
And I have calculated that here using our code.

448
01:04:01,370 --> 01:04:11,330
So if you have a like a ratio statistic that's 5.3, then you would not be able to reject the null under the incorrect.

449
01:04:13,060 --> 01:04:17,500
No distribution. But you would be able to jump now if you use the table here.

450
01:04:18,460 --> 01:04:21,640
So this is how we use this table.

451
01:04:22,090 --> 01:04:28,270
And this demonstrates the consequence of not using the correct, not correct and all distribution.

452
01:04:30,960 --> 01:04:41,470
Okay. And on the other hand, so I want to say that using the wrong distribution may lead to larger P values.

453
01:04:42,150 --> 01:04:48,670
Right. How do we interpret that? Again, let's return to this particular visualization here.

454
01:04:49,450 --> 01:04:52,630
So how do you calculate p value? What's the definition of P value?

455
01:04:53,440 --> 01:05:00,090
Yeah, actually, what's the definition of p value here? Is that probability of the now being true?

456
01:05:04,920 --> 01:05:08,850
So P-value is not the probability of not being true,

457
01:05:08,850 --> 01:05:16,140
but rather it is a probability of you observing this statistic as a more extreme than the one you're observing that you observed.

458
01:05:16,890 --> 01:05:24,450
So to calculate P value using the wrong null distribution, what's the area under the distribution you would calculate?

459
01:05:25,230 --> 01:05:33,210
So I'm going to use the color. Like. So so these will be the this will be the p value, right?

460
01:05:33,730 --> 01:05:39,020
The blue one under the incorrect, incorrect null distribution.

461
01:05:39,020 --> 01:05:50,950
Right. So is that area of the yellow is the yellow area greater than .05 or less than 105?

462
01:05:53,720 --> 01:05:58,730
Greater than point of firearm. How about.

463
01:06:00,010 --> 01:06:08,470
If you calculate p value using the red, the correct distribution is that pivot is going to be less than 0.05 or bigger than .05.

464
01:06:11,070 --> 01:06:16,140
Hopefully you can see it's just this area. Right. And it's going to be less than .05.

465
01:06:17,250 --> 01:06:23,340
This means that when you know that from the Valley perspective in general,

466
01:06:23,340 --> 01:06:28,430
you will get too big P values and too big P values makes it harder to reject it all.

467
01:06:28,950 --> 01:06:34,200
So you can see they're consistent anyway. So that's the final comment here.

468
01:06:34,920 --> 01:06:38,700
Using the wrong distribution may lead to larger P values, make it harder to reject the null,

469
01:06:39,450 --> 01:06:49,020
and if it's hard to reject at all, then you know you may incorrectly keep the parsimonious model.

470
01:06:52,910 --> 01:06:56,810
So a final comment on this part.

471
01:06:59,190 --> 01:07:07,830
Is that? What do we do if we are testing the importance of not only one but k random effects?

472
01:07:08,190 --> 01:07:13,380
So in the discussion we just had, there are just one.

473
01:07:13,410 --> 01:07:17,310
There is just a one random effect in question. Right? Do we keep that or not?

474
01:07:17,610 --> 01:07:22,080
But what if we ask for k random effects at the same time?

475
01:07:22,330 --> 01:07:29,190
And we said all of them to zero or not. So in general, the null distribution is harder to derive.

476
01:07:29,220 --> 01:07:32,910
Maybe there are one recent lecture that talks about what's in our distribution.

477
01:07:33,450 --> 01:07:40,710
But it is simply not a mixture of chi square with CU degrees of freedom and the chi square with Q plus one degrees of freedom.

478
01:07:41,640 --> 01:07:45,510
So in those cases, what people do is a very ad hoc approach.

479
01:07:45,510 --> 01:07:54,420
And I think that's OC for the purpose of this class that you just use the incorrect and all and then use a.

480
01:07:55,950 --> 01:07:58,020
How to say higher significance level.

481
01:07:59,300 --> 01:08:06,020
Because, you know, you're using a incorrect distribution and you know, that will cause it hard to reject and all.

482
01:08:06,050 --> 01:08:11,810
Then you just make it easier to reject. So just raise alpha four point or 5.1.

483
01:08:12,380 --> 01:08:19,430
And so that's what people often do. Is it mathematically accurate for this approximation?

484
01:08:21,140 --> 01:08:23,240
It's way out of the scope of this class.

485
01:08:23,870 --> 01:08:34,100
But I think to me is of interest to sort of discuss what is a normal distribution if you are setting K random effects to zero at the same time.

486
01:08:35,060 --> 01:08:41,750
But that's too theoretical for this class. So again, this is ad hoc solution, and you can use this in your projects if you want.

487
01:08:44,010 --> 01:09:33,410
Any questions before we move on to the final part? Okay.

488
01:09:35,150 --> 01:09:40,660
The final part, the prediction of random effects. So this is where we were going to talk about the shrinkage estimation.

489
01:09:51,060 --> 01:09:59,730
What is the scope of this discussion? We have been focused on at least in 650 or 651, the estimation of these Greek letters betas,

490
01:10:00,270 --> 01:10:05,850
and we want to interpret them as changing them in response to a time per unit change in covariates.

491
01:10:06,060 --> 01:10:07,110
We're very familiar with that.

492
01:10:07,740 --> 01:10:13,980
But in many studies, especially in longitudinal studies, we want to predict what's going to happen for a person in the future.

493
01:10:14,370 --> 01:10:18,600
So it is about predicting the reasonable trajectory of the outcome for that person only.

494
01:10:18,990 --> 01:10:22,980
So this will lead us to predicting the random effects for that person.

495
01:10:23,340 --> 01:10:27,530
For example, in studies of growth. We may want to power plot trajectories.

496
01:10:29,670 --> 01:10:35,100
And also, by plotting out these trajectories, you will be able to tell the individual,

497
01:10:35,100 --> 01:10:38,580
hey, you know, your outcome seems to be in direction that's not desirable.

498
01:10:38,820 --> 01:10:47,240
Right? So that kind of. A task will be very important to perform in the framework of the mixed effects model.

499
01:10:48,200 --> 01:10:54,649
So what do we mean by predicting trajectories? It simply means that in the framework of the mixed model,

500
01:10:54,650 --> 01:11:04,880
we just predicted this quantity x eight times better the population trajectory and individual specific deviation from that population trajectory.

501
01:11:05,300 --> 01:11:11,060
Because we know better how we will estimate beat ahead and we will predict by.

502
01:11:11,420 --> 01:11:21,590
So essentially if we plugged in xy beta out, if we plug in beta hat and z by hat, this will be the prediction.

503
01:11:24,670 --> 01:11:51,700
Of subject eyes trajectory. As I have alluded to, we often do not call this estimation.

504
01:11:51,710 --> 01:11:57,560
We call this prediction because there is no ground truth for at least in this framework for BII.

505
01:11:57,740 --> 01:12:05,780
So it's a random quantity we want to predict. What's the predictor?

506
01:12:06,260 --> 01:12:11,060
Is up. You have heard of something about blue based on bias estimation.

507
01:12:11,780 --> 01:12:15,859
And we just replace the estimation with. Prediction.

508
01:12:15,860 --> 01:12:19,280
So that's cobble up. And there is a paper with a name.

509
01:12:20,800 --> 01:12:31,570
Blood is a good thing. And with the help of Google, let's make sure that's not a joke.

510
01:12:35,430 --> 01:12:42,150
Love is a good thing. It's actually a good paper if you're interested, especially, especially your future research involves.

511
01:12:43,640 --> 01:12:46,730
Involves this. Look, it's a real thing.

512
01:12:47,240 --> 01:12:50,510
The blob is a good thing. Oh, it's actually called estimation.

513
01:12:50,540 --> 01:12:56,500
Look at that. I don't think that's a good idea.

514
01:12:56,520 --> 01:12:59,550
But anyway, club the P is for prediction.

515
01:13:00,330 --> 01:13:03,630
So this paper as being quite high is highly cited. It's pretty readable.

516
01:13:04,380 --> 01:13:10,740
Again, if you're doing longitudinal data analysis, really the thesis research or projects, I highly recommend this paper.

517
01:13:10,950 --> 01:13:14,090
Well, very well written. We're turning back the slide.

518
01:13:14,840 --> 01:13:19,790
What is the form of the blob? So this is the formula here.

519
01:13:20,210 --> 01:13:25,220
I want to make some explanations. First, this form.

520
01:13:27,360 --> 01:13:34,530
Almost came from nowhere. But to be able to explain how to derive this, I wanted to invoke a lot of technical details,

521
01:13:34,920 --> 01:13:38,580
but I really want to make sure that I get the overall idea through.

522
01:13:39,450 --> 01:13:44,760
So in this formula, what we get here are our two components.

523
01:13:44,850 --> 01:13:51,540
The first one is the residual. Sorry. The, the part that's not modeled by the population.

524
01:13:51,550 --> 01:13:53,980
Me. Right. So that's why am I better hat?

525
01:13:54,340 --> 01:14:00,760
So better hat has been produced by a certain estimation procedure and you can plug in like Remo estimates, for example.

526
01:14:02,290 --> 01:14:08,590
A second component. The second component is the term you saw in front of it.

527
01:14:09,400 --> 01:14:15,880
What are these? What are what are the components of this part?

528
01:14:16,510 --> 01:14:20,140
G. That's the variance covariance matrix for the random effects.

529
01:14:20,650 --> 01:14:24,010
Z. You know them because you specify the model.

530
01:14:25,170 --> 01:14:32,500
Sigma I. Well, Sigma II is a total variance covariance we have derived earlier.

531
01:14:32,520 --> 01:14:37,680
Sigma equals zero times g and z transpose plus i.

532
01:14:39,270 --> 01:14:43,980
How did I remember this? Again, muscle memory. But this is what we will always use.

533
01:14:44,320 --> 01:14:49,880
Always use. And by the way, I have already derived this in the past, so no excuses.

534
01:14:49,920 --> 01:14:52,920
All right? This is a total variance. Okay.

535
01:14:53,580 --> 01:15:01,680
So in this particular blood equation or formula, you can see that it's still not a real predictor.

536
01:15:02,190 --> 01:15:05,820
Why? Well, you don't know G. You don't know Sigma.

537
01:15:07,430 --> 01:15:10,520
You do know why? Because the data you do knows z do not exi.

538
01:15:10,550 --> 01:15:14,900
These are data you use in the model. So this is a theoretical quantity.

539
01:15:16,040 --> 01:15:20,750
Okay. So what do people do then? They just plug in the estimate.

540
01:15:21,380 --> 01:15:26,270
How surprising? Plug in the G estimate. Plug in the sigma estimate.

541
01:15:26,990 --> 01:15:34,550
Because we know that Sigma II is z, i, g, z I transpose plus i.

542
01:15:35,390 --> 01:15:40,040
What we do is to put hairs on the unknowns. Here. Here and here.

543
01:15:43,200 --> 01:15:47,939
So once you have these estimates for G and R, you will be able to plugging all these.

544
01:15:47,940 --> 01:15:55,320
And this whole thing is called empirical. So the word empirical is trying to suggest that relative to the formula we saw before.

545
01:15:55,350 --> 01:15:58,650
That depends on the unknown G and sigma.

546
01:15:59,340 --> 01:16:03,130
This formula in bullet four is completely dependent upon theta.

547
01:16:03,180 --> 01:16:13,200
So actually you can calculate a number based on this formula. An immediate question you may have is, Hey, Jim, how did you estimate G and R?

548
01:16:14,010 --> 01:16:19,470
Well, you have learned that, Remo. Yeah. It turns out that Remo can do the job because.

549
01:16:20,630 --> 01:16:30,470
If you write down the entire thing, the motto is was x equals x, i beta and the variance covariance matrix of why is this thing?

550
01:16:31,780 --> 01:16:38,450
You know, if you remove the head part. You can use water seals, right?

551
01:16:38,540 --> 01:16:41,930
You can use that function to do this. You specify the main structure.

552
01:16:42,230 --> 01:16:45,320
You specify the various command structure.

553
01:16:45,710 --> 01:16:47,780
Although it will be a bit complicated, but you can do that.

554
01:16:48,830 --> 01:16:56,270
So it brings us back to the remote discussion where you're just trying to estimate this whole thing.

555
01:16:56,700 --> 01:17:07,630
Right. So. The takeaway message is that we do can use remote technique to estimate the G and R

556
01:17:07,840 --> 01:17:12,280
and we can plug them back in to produce the imperial blob and the hand-waving here,

557
01:17:12,880 --> 01:17:22,450
primarily because of that. In this class, we will now require you to derive the Remo s or G and R because those will be done by

558
01:17:22,450 --> 01:17:27,310
the function in our packages and they are working really hard to get you those answers.

559
01:17:27,640 --> 01:17:32,230
Don't get me wrong, but I think it's important that you conceptual understand that they are coming from Remo.

560
01:17:34,970 --> 01:17:41,510
So now you're ready to produce a prediction because we just talking about how do we predict by and

561
01:17:41,510 --> 01:17:46,850
you plug that in and you also plug in the beta hats and you will be able to get this prediction.

562
01:17:49,600 --> 01:17:51,009
I realize I'm out of time.

563
01:17:51,010 --> 01:18:00,190
So what I'm going to do is just to go give you one slide, quick prelude to what we are going to discuss regarding the shrinkage.

564
01:18:01,240 --> 01:18:06,280
So this is a beautiful and simple form of prediction, but it turns out that we can rewrite this.

565
01:18:10,000 --> 01:18:16,880
Into a shrinkage form. Okay.

566
01:18:18,070 --> 01:18:23,060
It'll be something like. Ex I Peter Hart.

567
01:18:23,780 --> 01:18:28,090
Plus why I. With a weight.

568
01:18:33,230 --> 01:18:44,270
I'm intentionally being a little bit vague here. My goal is to have you seen that this thing can be written as this convex combination in some sense.

569
01:18:45,680 --> 01:18:50,180
It is between what? Between the population level linear model?

570
01:18:50,210 --> 01:18:55,640
Well, this beta is about population level association between the cover and outcome.

571
01:18:56,540 --> 01:19:06,080
So if you plug in, one person is covered, it will give you the best possible prediction in the population, although not necessarily for that person.

572
01:19:07,130 --> 01:19:11,990
So this is relying on the data in the entire population to produce a prediction.

573
01:19:12,920 --> 01:19:16,880
This part is subsidized data. This is to say that.

574
01:19:18,780 --> 01:19:24,000
You know, I am myself. I don't want to trust anything, not from myself.

575
01:19:24,010 --> 01:19:29,250
So this why is representing a prediction purely based on this person's data?

576
01:19:30,510 --> 01:19:38,350
And this combination is to show that. This formula on the left is a compromise.

577
01:19:39,300 --> 01:19:49,690
Between. The prediction using the population level information that you can learn using not only subsidized data but other people's data.

578
01:19:51,600 --> 01:19:56,340
End subject data. Subject is data alone. So let me say that again.

579
01:19:56,820 --> 01:20:04,590
It turns out that we can rewrite that prediction into a combination two terms while representing production by the population quantity.

580
01:20:04,710 --> 01:20:10,950
The other representing production only by subject is data. So that is why we call that shrinkage, right?

581
01:20:11,400 --> 01:20:15,870
It's shrinking between the individual population and end up somewhere in between.

582
01:20:16,470 --> 01:20:21,480
So this is formula we are going to be diving into.

583
01:20:21,780 --> 01:20:24,719
I will start in the next lecture by deriving this.

584
01:20:24,720 --> 01:20:37,920
It's pretty quick and now you can see that it's if it can be represented that way, it will be a combination representing a shrinkage.

585
01:20:38,310 --> 01:20:41,790
All right. I'll see you next Monday.

586
01:20:43,360 --> 01:20:45,370
Have a have a good day.

