1
00:02:44,870 --> 00:02:54,680
Hey. Good afternoon. And we have a job candidate coming to give a seminar this afternoon from 3 to 5.

2
00:02:55,130 --> 00:02:59,320
So I have to turn to form.

3
00:02:59,330 --> 00:03:09,319
I have to go there. So the person who will be ultimately responsible for job offer if this candidate is going to be in command of for job offer.

4
00:03:09,320 --> 00:03:14,250
So I feel that you feel obligated to listen to his seminar.

5
00:03:14,280 --> 00:03:18,799
So I have to cancel the officer this afternoon.

6
00:03:18,800 --> 00:03:28,610
If you have any question about your overall code lectures, just email me that we can schedule time to talk about your questions.

7
00:03:32,100 --> 00:03:42,450
But what we're trying to do here is working on this desperate smile, as I said, that this is very sort of natural architecture,

8
00:03:42,450 --> 00:03:53,610
that we can build up this underlying infectious disease spread or mechanism in the population, which you cannot directly capture.

9
00:03:54,420 --> 00:04:07,410
And on the top of this underlying infectious sort of evolution or dynamics, we want, you know,

10
00:04:07,890 --> 00:04:15,510
understand how data is generated from this population so that we can sort of sit back,

11
00:04:15,870 --> 00:04:26,370
sort of doing this sort of estimation, using the data captured by this surveillance system to understand what's going on in the actual population.

12
00:04:26,400 --> 00:04:30,270
So the strategy is what?

13
00:04:33,350 --> 00:04:41,920
Something like a constantly evolving dynamics of this infectious disease could be.

14
00:04:45,330 --> 00:04:52,740
Represent the proportion of infected individuals over an individual's proportion.

15
00:04:54,440 --> 00:04:58,750
But this could be back to or could be a one dimensional solution.

16
00:05:03,400 --> 00:05:17,870
For you. Families are severely. It's a like temple, the evolving system from current time like today and moving to the future.

17
00:05:17,900 --> 00:05:23,600
Right. So then this is a process that evolves.

18
00:05:29,950 --> 00:05:36,090
I'll direct and observe this process, but we want to figure out how this system looks like.

19
00:05:37,290 --> 00:05:46,000
The data is actually generated. You can think about the that is a continuous time.

20
00:05:49,020 --> 00:05:53,940
But the data we capture is on a daily basis, weekly basis, a monthly fee.

21
00:05:54,450 --> 00:06:02,970
So that's the data we can capture. But we can't of temper from some, you know, public house database of civilians.

22
00:06:03,630 --> 00:06:10,280
Okay. So you can imagine that maybe there are some additional fee that would be 2021.

23
00:06:10,290 --> 00:06:16,920
There are some additional data popping up and is evolving a confused.

24
00:06:18,050 --> 00:06:21,170
But you know after given the time t that we.

25
00:06:24,780 --> 00:06:28,650
Oh. So this is some kind of.

26
00:06:35,690 --> 00:06:52,960
Observe. So the question here is very for not how we're going to use the observed data to figure out what's going on about that meeting process.

27
00:06:53,590 --> 00:06:56,800
Actually, the fascists situation in the population.

28
00:07:05,220 --> 00:07:08,940
On this of using some kind of framework.

29
00:07:08,940 --> 00:07:12,120
You are memory. This is sort of like a base for me.

30
00:07:12,470 --> 00:07:17,100
Right. So you you have your base y and theta.

31
00:07:17,790 --> 00:07:23,970
This is how data generated. But what you will try to figure out is like selecting the Y.

32
00:07:26,680 --> 00:07:31,210
Oh. Why is generally under some kind of leave in process data.

33
00:07:31,570 --> 00:07:40,390
But what you want here. I think this is. I'll do it.

34
00:07:41,410 --> 00:07:47,740
But what you want here is the reverse of this.

35
00:07:49,060 --> 00:08:05,850
I. Oh. Oh, of course, this problem is more complicated because you have a stochastic process, nonlinear structure.

36
00:08:06,450 --> 00:08:15,660
We do need some assumption in this framework to work out from this data generative mechanism to figure out who will be.

37
00:08:19,090 --> 00:08:23,230
These are bosses and this sort of.

38
00:08:27,050 --> 00:08:32,300
The computational fact that you can sort of.

39
00:08:34,580 --> 00:08:42,640
But. So the choice from here to here is.

40
00:08:50,530 --> 00:08:58,740
But on a smoother. So so you can use unheard of techniques or to.

41
00:08:59,870 --> 00:09:02,930
But choose to work on the common field where it comes.

42
00:09:06,810 --> 00:09:23,180
The. But we need just to look.

43
00:09:27,310 --> 00:09:39,290
The. Well, the. That we'll hear from the current the process to the White House all weekend long.

44
00:09:41,450 --> 00:09:45,150
It. It's.

45
00:09:47,910 --> 00:09:59,280
Uh oh. Data generated mechanism, how many people will be observed as number of, you know, in fact, their cases or deaths.

46
00:09:59,600 --> 00:10:05,160
So so we have some poor records not necessarily available in the engineer.

47
00:10:06,330 --> 00:10:15,620
But I. Just thinking that how we can add those covers like age.

48
00:10:15,630 --> 00:10:20,370
We know that elderly people are more. I.

49
00:10:23,470 --> 00:10:30,200
The. Also to our system is a little bit more complex because we needed to incorporate

50
00:10:30,200 --> 00:10:35,960
a lot of covers into the system but that the fundamental questions versus.

51
00:10:40,160 --> 00:10:45,520
I'm switched back to the data. Oh.

52
00:10:45,950 --> 00:10:55,810
So the whole preparation that I'm trying to do. This is a prediction or prediction.

53
00:10:58,450 --> 00:11:10,610
Mathematical preparation for statistical. That's not the way that gives us too fast computing.

54
00:11:11,000 --> 00:11:15,780
You worked. This. You can call full service if you.

55
00:11:15,800 --> 00:11:24,070
But I need an update because that is. All of this data.

56
00:11:24,640 --> 00:11:37,130
So instead of doing this. So that's how I think the objective here is clear.

57
00:11:37,820 --> 00:11:46,650
The question here is, how could we do that? And actually, this problem has been solved many years ago.

58
00:11:46,670 --> 00:11:53,570
We just need to see how we could, you know, proceed this in our context.

59
00:11:53,660 --> 00:11:53,930
Right.

60
00:11:54,410 --> 00:12:07,250
So we need some assumption, first of all, of course, that we need to this conditional independence essentially is this that you fly like a A320,

61
00:12:07,250 --> 00:12:11,440
then this light is independent of all the other variables there.

62
00:12:11,540 --> 00:12:15,680
So you should independence and this. Hi.

63
00:12:16,070 --> 00:12:23,060
This comm structure. Okay, this is the way we are in the development of a field for us.

64
00:12:23,570 --> 00:12:26,690
That's the first condition. Second condition?

65
00:12:26,690 --> 00:12:32,390
That. So this led to the temporary offices, Michael.

66
00:12:34,320 --> 00:12:41,100
That's we do not make any specific assumptions for the distributions of the two process.

67
00:12:41,730 --> 00:12:48,420
As you know, if you wanted to work on MSNBC, then you need to dispute.

68
00:12:48,900 --> 00:12:53,730
But for a future smoother. And this moment we do not need to work out.

69
00:12:54,450 --> 00:12:58,200
We need to not need to show any distributions for process.

70
00:12:58,650 --> 00:13:05,310
But later on, if you want to work in CMC and you need to distribution, we will cover to that stage later.

71
00:13:07,640 --> 00:13:16,410
Well, in addition to the the the decomp structure, this sort of graphic model that you just saw.

72
00:13:16,430 --> 00:13:24,380
And we need to also put some kind of structure for the equation the first and second moment.

73
00:13:24,560 --> 00:13:39,980
Okay. So the first equation, it says that how the Y t is is related to your theta t because we have this o arrow from 72 whitey.

74
00:13:39,980 --> 00:13:45,050
Basically how the white observed captured.

75
00:13:46,600 --> 00:13:57,140
Oh. Number of deaths captured by a civilian system condition at the current time of infection.

76
00:13:57,930 --> 00:14:06,710
Or how many people have been infected at. So the idea that you could have a distribution from.

77
00:14:10,060 --> 00:14:22,690
But and this moment that we're just looking at the the simple structure that on the first moment conditional expectation.

78
00:14:22,690 --> 00:14:29,440
So on average this basically say on average what's the observed number of the confirmed

79
00:14:29,440 --> 00:14:37,180
cases related to number of the actually infection in fact that people in the population.

80
00:14:37,460 --> 00:14:47,560
Okay. And then this one, of course, is related to the certain coefficients, right?

81
00:14:47,610 --> 00:14:51,120
They see that 88 could be known, could be unknown.

82
00:14:51,130 --> 00:14:54,130
Right. So that's something up to you to specify.

83
00:14:54,550 --> 00:14:59,110
And of all the sycamore and what is the volatility of this process.

84
00:14:59,590 --> 00:15:09,700
So how how white varies over time condition on your C that T okay so that's the so the variability of your process.

85
00:15:10,540 --> 00:15:10,820
Okay.

86
00:15:10,840 --> 00:15:22,870
That's the basically the the first layer of modeling that's basically talking about the, you know, how the these impulses would affect our process.

87
00:15:23,200 --> 00:15:35,890
This is hierarchy model. That is the first layer of the model I saw in the first moment describing how the live in process of facts observe process.

88
00:15:36,100 --> 00:15:40,120
And then second is for Markov process.

89
00:15:40,600 --> 00:15:53,139
So how the transition on is is operates right under this first order of Markov process here we instead of specifying distribution once again,

90
00:15:53,140 --> 00:16:05,450
we only work on the first to moment. So given the of the number of in fact the cases yesterday, what is the average number of affected cases today?

91
00:16:05,750 --> 00:16:15,010
Okay. So transition, right. And you can imagine that this this is the number of, in fact, cases yesterday.

92
00:16:15,400 --> 00:16:21,520
And then you have a factor of two a fact how many people was staying in this system.

93
00:16:21,520 --> 00:16:24,400
And plus, some new cases come in today.

94
00:16:24,700 --> 00:16:38,260
So VCE could be regarded as some of your cases coming today, and this is proportion of the cases yesterday stay in this sort of compartment of fabric.

95
00:16:38,680 --> 00:16:42,130
So so some of maybe move out some or you know.

96
00:16:44,020 --> 00:16:48,760
You know, disappear from this infection. And so there's a variability.

97
00:16:49,240 --> 00:16:59,010
Okay. So now based on this, then you can work out the very, very basic sort of prediction.

98
00:17:00,160 --> 00:17:04,670
Okay. Well, this is coming under this kind of system, right.

99
00:17:04,690 --> 00:17:08,510
So I just described. And now you can work on the common filter.

100
00:17:08,530 --> 00:17:20,670
Occam's Razor. Okay, so first of all, I know this white t the superscript t to be a set of the first t o ys.

101
00:17:20,830 --> 00:17:25,450
Okay, so, so this is your historical, historical data up to time.

102
00:17:25,450 --> 00:17:37,030
T Okay. So you observe the, for example, number of in fact cases up to today's if t denotes today, right?

103
00:17:38,290 --> 00:17:48,159
Well, what is common field or common filter is trying to really see how I'm going to figure out my C that T given the historical data up to time.

104
00:17:48,160 --> 00:17:52,480
T It's not it's not only one data point like.

105
00:17:52,540 --> 00:17:55,860
T To figure out severity, you use the data.

106
00:17:58,150 --> 00:18:02,080
All the past data make it to all the historical data up to 19.

107
00:18:05,320 --> 00:18:11,920
This is information collected up to time to you. This whole chunk of variables were data.

108
00:18:12,040 --> 00:18:18,320
Right? To predict or to to estimate the number of.

109
00:18:19,540 --> 00:18:23,450
Basically make the best of the unbiased prediction of your.

110
00:18:24,980 --> 00:18:34,160
Well that's come a few to get what has come a smoother come a small surface that suppose I have the entire data collected.

111
00:18:35,000 --> 00:18:39,399
Okay. Up to the end of time.

112
00:18:39,400 --> 00:18:48,880
So false that you have in your your study period involves ten days and days and so little ends.

113
00:18:49,030 --> 00:18:51,880
The total number of days. Okay.

114
00:18:52,300 --> 00:19:01,300
So given that the total number of the data you observe that you what's the best immediate, unbiased prediction, obviously.

115
00:19:01,990 --> 00:19:05,110
Okay. So so this there's a difference, right?

116
00:19:05,380 --> 00:19:10,420
If you look at the state of t come of uteruses and it.

117
00:19:11,360 --> 00:19:16,820
What? But.

118
00:19:25,190 --> 00:19:33,710
Over a period of days. Just.

119
00:19:37,790 --> 00:19:46,310
That's smooth. Okay. So both predictions were calculated via this sort of recursive procedures,

120
00:19:46,580 --> 00:19:57,889
because in 1950s when people trying to figure out the positions of the rocket and in the fly on the bow according to certain orbits right there,

121
00:19:57,890 --> 00:20:06,050
they need to really do something fast because at that time, computer is not that powerful.

122
00:20:06,080 --> 00:20:13,860
You really need to have a very elegant sort of algorithm in order to calculate.

123
00:20:13,880 --> 00:20:21,260
And the way to speed up this calculation is really this recursive procedure that I'm going to introduce.

124
00:20:21,410 --> 00:20:28,520
Okay. Before I do that, I just let me to talk about a few late in process.

125
00:20:28,670 --> 00:20:43,700
Okay. And so so this is the very famous of sort of auto regressive, ultra regressive, ultra regressive model.

126
00:20:44,150 --> 00:20:54,469
So A stands for A and the R stands for are so often very small, these short ignored by one,

127
00:20:54,470 --> 00:21:01,160
basically an ultra rigorous model or one which is a stationary continuous value.

128
00:21:01,520 --> 00:21:06,470
Markov neither passes over the one specified by this equation.

129
00:21:06,830 --> 00:21:14,719
Okay. So basically it says that if you think about the, the C that denotes the number of the in fact,

130
00:21:14,720 --> 00:21:22,430
in fact that cases or number of infectious individuals in population which you can observe directly.

131
00:21:23,060 --> 00:21:27,890
So you look at how this number of the cases will evolve over time.

132
00:21:28,370 --> 00:21:33,890
So our size, the proportion of the is constant over times number.

133
00:21:34,280 --> 00:21:43,159
Well, what's the percentage of individual stay in the system of being a fascist and anti is the random shock.

134
00:21:43,160 --> 00:21:50,090
What's the increment. The number of cases that can add into the system and I sort of like that.

135
00:21:50,090 --> 00:21:55,520
Right. You can think of al-Qaeda t as a percentage or something like that.

136
00:21:55,790 --> 00:22:10,160
But anyway, this is. They are one process that's a stationary process where this RFA is the coefficient termed as autocorrelation coefficient.

137
00:22:11,600 --> 00:22:14,930
Basically it tells you how strongly that be.

138
00:22:16,970 --> 00:22:20,100
The value of yesterday will be related to value today.

139
00:22:20,890 --> 00:22:29,810
Alpha describes the distress if RFI zero basically that say that t today you know has nothing to do with the SD.

140
00:22:30,440 --> 00:22:38,720
If the RFI is equal to one. That's something called random walk.

141
00:22:38,810 --> 00:22:49,190
Right. So this is a random walk. So the value today is very, very similar to yesterday, subject to a random shock or random noise.

142
00:22:49,550 --> 00:22:54,110
So this is the case of random walk that we know.

143
00:22:54,500 --> 00:22:58,220
And this is not stretching our process. I could talk about.

144
00:22:58,430 --> 00:23:04,370
So that's why when we talk about stationary impulse, once this process that we required,

145
00:23:04,370 --> 00:23:18,110
our foot coefficient of varies between mice one and one not taking value on the boundary either minus one or plus one.

146
00:23:18,740 --> 00:23:28,220
So only this AAFA is only allowed to take a value between minus one so that you can guarantee this station narrative of the process.

147
00:23:28,430 --> 00:23:35,160
Yes. So if something is called white noise, there are.

148
00:23:41,790 --> 00:23:52,589
Otis. I'm calling it This is type. Well are basically like you can think about independent noise or I'm sorry, noise, which means zero.

149
00:23:52,590 --> 00:24:00,220
And then the second square. Well, this is also well known as Box Jenkins Airline model.

150
00:24:00,490 --> 00:24:05,800
Okay. So just a side story about this.

151
00:24:06,280 --> 00:24:15,340
So this model originally studied by Box and Jenkins.

152
00:24:15,850 --> 00:24:20,950
So so there are see now extension.

153
00:24:20,950 --> 00:24:28,480
There's a kind of parallel development of this kind of model in economics, right?

154
00:24:28,490 --> 00:24:32,470
So it's called volatility ball,

155
00:24:32,590 --> 00:24:38,050
a garden mall or something like that where basically we use the same model structure started the

156
00:24:38,460 --> 00:24:50,100
inverse of the process because in in finance or even sometimes risk management is quite essential.

157
00:24:50,110 --> 00:24:57,730
Risk is essentially defined by the second moment of the process, more variability you more risk investment.

158
00:24:58,330 --> 00:25:10,000
So like Robert Engel writes this, this economic vision, he basically used this structure to Maldivians,

159
00:25:10,000 --> 00:25:13,290
the second moment of the process, rather than the meaning of the process.

160
00:25:13,300 --> 00:25:16,330
This is basically model the first moment of the process.

161
00:25:16,720 --> 00:25:27,000
But Robert Engel, he model used this kind of the the R1 structure to model the second moment, the process take a moment, as I said,

162
00:25:27,010 --> 00:25:35,380
is variability that is related to volatility or risk of the you know that so that he call

163
00:25:35,390 --> 00:25:46,330
this Gotch model work and that model you know received the Nobel Prize in economics 20 2021.

164
00:25:48,850 --> 00:25:56,799
Sure to solve the oh eight or to some but anyway so at a time is very controversial because the original idea

165
00:25:56,800 --> 00:26:04,060
is now from Robert Engle is actually from box of Jenkins box died but Jenkins was still alive at that time.

166
00:26:04,060 --> 00:26:10,240
So people think that Nobel Prize should be also given to a an Australian statistician.

167
00:26:11,590 --> 00:26:21,460
JENKINS But but that he the original contribution is from box JENKINS and by doing that that's a size doesn't matter.

168
00:26:23,320 --> 00:26:27,610
So we're talking about this sort of this this narrative in a weak sense.

169
00:26:28,150 --> 00:26:33,010
First of all, basically we're talking about timing isn't mean that this here you can see that

170
00:26:33,580 --> 00:26:42,520
the meaning of this one is are constant and and the variance is also constant.

171
00:26:42,820 --> 00:26:47,090
Okay. And. All the coalition function.

172
00:26:49,820 --> 00:27:03,860
Oh. Operation functioning of this process, basically looking at the correlation of the the data today and the data at each.

173
00:27:04,730 --> 00:27:16,550
You know in past what lacked okay this lack age but that does not depend on specificity but only depend on time lack age.

174
00:27:16,880 --> 00:27:26,540
Okay. So this is the the definition of this dictionary to you sort of weeks and

175
00:27:26,540 --> 00:27:31,669
sometimes we call weeks this narrative essentially the mean and the variance.

176
00:27:31,670 --> 00:27:40,070
The first two moments are time invariant. It doesn't change over time and then the autocorrelation function.

177
00:27:40,850 --> 00:27:45,380
Okay. Which is the the function of in the lack h.

178
00:27:45,620 --> 00:27:52,399
Okay. Does it depend the actually time doesn't depend on which day you're looking at this correlation of

179
00:27:52,400 --> 00:27:59,540
the process only depend on how many days apart or how many time unit apart you are talking about.

180
00:27:59,840 --> 00:28:05,479
So if you calculated this as a function for this error one process,

181
00:28:05,480 --> 00:28:13,700
you can easily find out that this alteration function is our fault, which is the coefficient of the process to the power of h.

182
00:28:14,150 --> 00:28:17,230
So this all the core function doesn't depend on space date.

183
00:28:17,240 --> 00:28:24,110
It only depends on how many days are part of this, of the two variables you're looking at.

184
00:28:24,380 --> 00:28:30,140
Okay. So the age is called lag in this sort of multicore function.

185
00:28:30,530 --> 00:28:35,960
Of course, that this correlation decays to zero as age increases.

186
00:28:36,200 --> 00:28:45,080
Okay. Basically the correlation will be you will become weaker ranker when you look at more and more part of the the data in the process.

187
00:28:47,900 --> 00:28:55,370
So you can calculate this conditional expectation based on the equation, which is our fallacy two T minus one,

188
00:28:55,370 --> 00:29:06,320
because expectation of if some T is the is zero so that you feel back to the original formula.

189
00:29:06,830 --> 00:29:15,890
Okay. So here we have the general model set up.

190
00:29:17,150 --> 00:29:20,730
So here I have. Right.

191
00:29:20,780 --> 00:29:29,180
You see that C that you're talking about, one dimensional process first e t t minus one plus b t, right.

192
00:29:29,350 --> 00:29:33,470
So that's the conditional meaning. Conditional variance.

193
00:29:34,460 --> 00:29:39,200
It's a t c the t minus one plus what?

194
00:29:39,320 --> 00:29:50,930
D t c. Well, that's the balance between okay, in general.

195
00:29:50,930 --> 00:29:58,670
And now we're looking at this error one process, right? So we can easily calculate that expectation of theta t.

196
00:29:59,270 --> 00:30:09,200
Here's the thing. A T minus one is half of C that you might want because you just use the the property of conditional expectation.

197
00:30:09,230 --> 00:30:19,070
Right. And the if something is independent of the C that you can see, that random noise is independent of this theta process.

198
00:30:19,070 --> 00:30:29,050
So and meaning is zero. So this term is zero. So in this case, you can use any Phi Phi is your b t them.

199
00:30:30,470 --> 00:30:33,590
And the, you know, beta is.

200
00:30:35,190 --> 00:30:39,900
So you back to the original specification. Now you're looking to the variants.

201
00:30:42,940 --> 00:30:47,470
I'll see that she didn't see that t minus one for this one process.

202
00:30:48,190 --> 00:30:55,660
Okay. So, so when they say that he is fixed, right.

203
00:30:56,140 --> 00:31:02,310
The, the, the, where the variance is zero, the condition variance is zero.

204
00:31:02,320 --> 00:31:08,440
Right. And, and the variance of this if some t is.

205
00:31:08,980 --> 00:31:15,180
Well. Because when I say that he is fixed condition,

206
00:31:15,180 --> 00:31:20,850
I'll say that you see that he is given his cost and there's no variability is the fixed

207
00:31:22,170 --> 00:31:27,180
theta t minus one in the calculation of condition variance when signal to my point is fixed.

208
00:31:27,780 --> 00:31:32,520
Then this whole thing is constant, right? So that over this one.

209
00:31:33,210 --> 00:31:39,820
So that's easy to compute so that you can identify this to be zero and this one to be your sigma squared.

210
00:31:41,070 --> 00:31:44,880
Oh, so you can do the simple calculation like this.

211
00:31:46,650 --> 00:31:50,190
So which are the preparation later on for common filters.

212
00:31:50,290 --> 00:31:56,880
Also because common filters, most are based on this general formulation or general model structure.

213
00:31:57,210 --> 00:32:01,650
So when you actually do the data, then you could specify this error,

214
00:32:01,650 --> 00:32:09,660
one structure or something like that, so that you can, you know, put some covers into it and so on.

215
00:32:09,660 --> 00:32:15,210
So for start something you were trying to do, you were to apply your common filter,

216
00:32:16,410 --> 00:32:20,489
but you need to sort of identify those term in the model specification.

217
00:32:20,490 --> 00:32:25,680
Okay. Now talking about this error P process, right?

218
00:32:25,680 --> 00:32:39,990
So, so for the error P process, so this thing that will depend on the data in the past few days, the peak would be seven.

219
00:32:40,230 --> 00:32:51,540
So the data today, the infectious disease in fascist status could depend on the status in the past seven days.

220
00:32:51,810 --> 00:33:00,810
Okay. So you believe that what, what, what is the current status as a function of the infection situation in the past seven days.

221
00:33:00,810 --> 00:33:11,060
Could be p could be 30 days. It depends on entire months or different it's really depends on how you believe the the current process of

222
00:33:11,460 --> 00:33:17,940
would be depends on you know the past situation okay really depends on what kind of transition you believe.

223
00:33:18,480 --> 00:33:33,030
Oh there. Right. So, so this could be a little CRP is number of the variable in the past that be related to the the variable today of because you

224
00:33:33,030 --> 00:33:41,910
have one process and you really looking at how the process is translate depend on how many past days in the transition.

225
00:33:42,300 --> 00:33:45,600
So this is called error P ultra regressive of order.

226
00:33:45,600 --> 00:33:54,540
P and it's on t is also a white noise which is uncorrelated were independent noise and are you

227
00:33:55,710 --> 00:34:00,750
independent of this process through the T itself is just random shock from external system.

228
00:34:00,870 --> 00:34:08,490
Okay it's not part of system but this external random shocks are coming to this system.

229
00:34:08,580 --> 00:34:12,890
Okay, so now you have the coefficient after.

230
00:34:12,930 --> 00:34:16,020
Well I repeat you work to make this process to be stationary.

231
00:34:16,020 --> 00:34:22,050
People prove that you have to make this error to more of our face, more equal to one.

232
00:34:23,760 --> 00:34:28,350
So this is pretty much like the situation of.

233
00:34:31,480 --> 00:34:37,500
So if you have to fall like you have to situation, right?

234
00:34:37,900 --> 00:34:46,100
So you have to criticize this new war. Our our fight here is a factor of two to me.

235
00:34:46,900 --> 00:34:50,850
This why it's more than one. It's really.

236
00:34:54,340 --> 00:34:57,640
Four years of voting equal and equal. Okay.

237
00:34:58,090 --> 00:35:02,110
So so so for this one is basically since the day.

238
00:35:08,150 --> 00:35:14,570
Well, what is the the situation of this equal to one that is basic is a unique circle, right.

239
00:35:15,750 --> 00:35:19,760
Is this an arbitrary equal to one?

240
00:35:19,760 --> 00:35:28,580
This is you. So if this equal to one, the trajectory is just a a circle with, you know, the unit circle you.

241
00:35:32,030 --> 00:35:37,010
Exactly. So this is your hour from one hour to.

242
00:35:43,310 --> 00:35:48,220
Goes through the middle. Well, this is our to.

243
00:35:51,890 --> 00:35:57,350
So this is one. And this is also what these have, you know, circle.

244
00:35:58,380 --> 00:36:05,670
So when this smaller town was essentially the interior of this circle, we saw cone in the boundary.

245
00:36:06,210 --> 00:36:14,030
So you feel any value inside here? Nicole will give you a stationary process.

246
00:36:14,060 --> 00:36:21,139
Now you think about a ball. P dimensional ball will a dynamic or one that you need.

247
00:36:21,140 --> 00:36:24,710
All the values are inside that ball. Okay.

248
00:36:25,010 --> 00:36:29,820
You want to give a solution or process any value outside.

249
00:36:29,840 --> 00:36:32,890
This will generate down station or process.

250
00:36:33,350 --> 00:36:39,709
Okay, so that's the proof. But anyway, so this is our process.

251
00:36:39,710 --> 00:36:44,270
And originally we. Paul.

252
00:36:45,780 --> 00:36:53,490
Like order one mark of process, but now you have older P and essentially you can write this.

253
00:36:54,030 --> 00:36:58,920
They are processed into a sort of older one mark.

254
00:37:01,540 --> 00:37:04,449
But we can rewrite about this as a vector value.

255
00:37:04,450 --> 00:37:12,460
The latent Markov process order one, whether you are trying to do here, is you put a T and see the T minus P plus one.

256
00:37:13,300 --> 00:37:17,230
Okay? You create the chunk of this and then.

257
00:37:21,050 --> 00:37:24,560
That's a T minus one up to T minus P. Okay.

258
00:37:25,400 --> 00:37:31,620
So you can denote this whole thing. This notation is bad, but you understand what I mean.

259
00:37:31,640 --> 00:37:36,500
I should use a boldface or some different notation, but you understand what I'm talking about.

260
00:37:36,770 --> 00:37:45,080
So you treat this entire vector of p elements, p element, vector isolate impulses here.

261
00:37:45,680 --> 00:37:49,459
That's the part that the loop impulses has to be.

262
00:37:49,460 --> 00:37:53,930
One dimension could be a p dimensional vector.

263
00:37:54,920 --> 00:37:59,790
Okay, you write this equation in this way so you can see that.

264
00:37:59,840 --> 00:38:07,190
See that T dependency? That T minus one goes up to see that P and t minus y is really interesting and t minus one.

265
00:38:07,190 --> 00:38:12,649
So you have this matrix, this, this define this matrix.

266
00:38:12,650 --> 00:38:17,570
This is p by p matrix. This define your v t matrix.

267
00:38:19,150 --> 00:38:29,370
It's defying gravity. And if our term is really defined by this, where you have the P one minus one elements equal to zero.

268
00:38:29,850 --> 00:38:33,030
So this takes a very adhoc expression.

269
00:38:33,660 --> 00:38:36,810
You actually create our p process.

270
00:38:37,020 --> 00:38:48,120
So what I'm saying here is that our P process is actually a special case of this, you know, this state space model expression.

271
00:38:48,570 --> 00:38:56,000
So you don't need to show this process to Markov P particle processing repeat.

272
00:38:56,250 --> 00:39:05,550
You can use this trick to put everything into a vector to maintain this sort of Markov order of one process.

273
00:39:08,690 --> 00:39:13,850
So I have already had a couple random walk where they are equal to one.

274
00:39:14,270 --> 00:39:17,300
There's also a very popular method people use.

275
00:39:18,010 --> 00:39:29,630
Eventually, you know, this infectious disease modeling, people will see that, you know, the infectious disease situation.

276
00:39:31,400 --> 00:39:41,100
Well. Constant. So the infection status today is very much similar to the status of yesterday.

277
00:39:41,430 --> 00:39:49,110
Okay. So there is there is no external noise or random shot from outside of system.

278
00:39:49,560 --> 00:39:56,990
The process will remain the same. Essentially, like this, you don't have a fraction of our you.

279
00:39:57,690 --> 00:39:58,230
It just.

280
00:39:58,620 --> 00:40:09,210
But in reality, of course, you have some random shots outside of maybe some, in fact, people moving into the city or some some people, you know.

281
00:40:11,420 --> 00:40:16,440
There's some randomness, right, coming to change the system even random way.

282
00:40:16,460 --> 00:40:22,760
This is white noise so that if you believe this is due process in a short period of time,

283
00:40:22,760 --> 00:40:31,850
that the underlying fact is this situation is almost the same except some external random noise.

284
00:40:32,330 --> 00:40:42,800
Then this is essentially a announced issue of process because the variance of this process is three times Sigma Square,

285
00:40:43,280 --> 00:40:51,709
so that this C component of the process depend on time, which violates the definition of this narrative.

286
00:40:51,710 --> 00:40:57,770
As I say, that this minority requires that both the mean and awareness are time invariant.

287
00:40:58,250 --> 00:41:04,370
Okay, being advanced. The first two moments are stationary constant over time.

288
00:41:04,610 --> 00:41:08,750
But for this particular process that the variance is a function of time.

289
00:41:09,020 --> 00:41:15,979
Essentially the variance gets exposed, it increases, you move a fourfold.

290
00:41:15,980 --> 00:41:20,059
Are we in the future time? Then the variance will become larger and larger.

291
00:41:20,060 --> 00:41:28,040
That's not a stationary situation. The process is now stationary because the variance of changes increases over time.

292
00:41:28,490 --> 00:41:37,350
And this destroyed this narrative. But a lot of people use this in the stage space model because you can work out this easily.

293
00:41:37,350 --> 00:41:43,579
The conditional meaning is minus one and the the condition variance is Sigma Square.

294
00:41:43,580 --> 00:41:54,080
So this is still fit into the the moment is sort of the assumption of moments condition of humans that would work on even this is announced

295
00:41:54,090 --> 00:42:03,410
this is a process so you can say that stage space model is very flexible to accommodate the both stationary pulses and stationary.

296
00:42:06,600 --> 00:42:25,230
It's one. And this is also something people use very frankly in literature, where this they say that is not a continuous value process.

297
00:42:25,770 --> 00:42:29,670
So this is a count integer value process.

298
00:42:30,280 --> 00:42:39,690
Okay. So if you have our thought leader, a T minus one positive.

299
00:42:41,280 --> 00:42:44,550
Right. So this is the error. One process.

300
00:42:45,900 --> 00:42:53,380
So one thing that is integer value, then this equation does not hold anymore.

301
00:42:53,400 --> 00:43:01,700
So the order was one process does not work for the case of of of integer value.

302
00:43:01,720 --> 00:43:05,160
The time source. Why? Because our fi's fractional.

303
00:43:06,000 --> 00:43:15,890
So. So. So sometimes people want to model the integer process, for example, number of that number of.

304
00:43:16,640 --> 00:43:20,310
That's the non negative integer about it.

305
00:43:20,910 --> 00:43:29,910
Okay. So people want to directly model the integer value to process for just for the physical interpretation.

306
00:43:29,910 --> 00:43:36,030
For example, you consider late in process, ask number of incidences, number of something,

307
00:43:36,630 --> 00:43:45,900
but the traditional error one box chicken small doesn't work for that case because our face with value between zero one times integer would be.

308
00:43:47,090 --> 00:43:53,690
And this one will be fractional. So so so it doesn't make sense to write them all anymore like this.

309
00:43:54,440 --> 00:44:05,990
So people are trying to overcome this issue that if I have a time series of columns, how I'm going to create the auto regressive model of order one.

310
00:44:06,240 --> 00:44:17,940
Okay. Okay. So I got sort of interested in this problem when I was a student and trying to figure out how how this can be solved.

311
00:44:17,940 --> 00:44:22,090
And of course, people already have some solution. To this problem.

312
00:44:22,100 --> 00:44:26,370
But I spent a lot of time, too, working on this because I work on space.

313
00:44:26,370 --> 00:44:34,280
Space more when the incident is interaction in the US is how I'm going to generate

314
00:44:34,310 --> 00:44:40,370
this space model to to have more flexibility to model some university this situation.

315
00:44:40,640 --> 00:44:47,030
Okay. So the solution here is coming from this something called thinking process.

316
00:44:47,360 --> 00:44:51,110
Let's take an example. Let's see that he follows a on distribution.

317
00:44:51,260 --> 00:44:58,040
Of course, the process of this this is random variable that, you know, follows a problem distribution.

318
00:44:58,040 --> 00:45:02,360
So that statement is a in search of a random variable.

319
00:45:02,660 --> 00:45:10,930
Okay. Oh. So you want to specify something similar to the airline process.

320
00:45:10,940 --> 00:45:13,690
So here you have a special operation here.

321
00:45:14,150 --> 00:45:23,479
This operation, just the regular multiplication right in the air on process, 25 times something that gives you something, right?

322
00:45:23,480 --> 00:45:27,110
So but here, this can now be a regular multiplication.

323
00:45:27,440 --> 00:45:30,920
It has to be operated in a special fashion.

324
00:45:31,310 --> 00:45:39,830
To create a integer value. You have to make sure that after this operation you still get an integer about it.

325
00:45:39,980 --> 00:45:43,920
So how to achieve that? That's essentially people call it.

326
00:45:45,950 --> 00:45:54,560
Stochastic optical beta, also named at a seeming operator defined as follows How do you define this operation?

327
00:45:55,370 --> 00:46:05,790
So, so after circle time x given x equal to a little x little x of course is integer value okay?

328
00:46:06,230 --> 00:46:19,219
Is an integer that positive integer value? Or now we think the internet is defined as the sum of big from B1 to be Zoraida Bernoulli Random variable.

329
00:46:19,220 --> 00:46:25,490
So BGA is either zero one. Okay, so you have some zero hands on one.

330
00:46:25,820 --> 00:46:31,230
How many terms you need in this sample x? Oh.

331
00:46:31,770 --> 00:46:34,770
So this senior operator is defined in such a way.

332
00:46:35,460 --> 00:46:46,590
Our full circle time x condition of x equal to a little x is defined as the sum of independent baloney random variables.

333
00:46:46,740 --> 00:46:51,240
This is definition of operation. So as a result,

334
00:46:51,240 --> 00:46:57,059
you can prove that the marginal distribution this is defined as conditional

335
00:46:57,060 --> 00:47:08,810
distribution and and the margin distribute distribution x is also lambda after a.

336
00:47:10,280 --> 00:47:15,190
To Middle X is a by a novelties.

337
00:47:16,210 --> 00:47:19,610
Of our foot. Okay.

338
00:47:20,180 --> 00:47:28,040
So you you have this marketing distribution platform because you want the process to be stationary,

339
00:47:28,040 --> 00:47:34,880
that the process itself has to be a possible distribution all the time conditional on this.

340
00:47:35,240 --> 00:47:40,940
Right. It's binomial distribution and then you can easily approve it from 601.

341
00:47:42,380 --> 00:47:47,780
The modular distribution of this is partial distribution of our full time slot.

342
00:47:48,230 --> 00:47:57,720
Okay. So. Together at this fall's awesome distribution of.

343
00:47:59,400 --> 00:48:05,490
So in order to make this competition, I only required that ipsum some t term.

344
00:48:05,490 --> 00:48:13,710
Also inter divided brandon noise files a plus one minus are full of lambda so by the convolutional problem.

345
00:48:13,830 --> 00:48:23,610
Right. You know that if x with 1x2 follows problem distribution oh this fall awesome distribution 1.1.

346
00:48:24,690 --> 00:48:28,500
This follows possible distribution lambda to their independent.

347
00:48:29,400 --> 00:48:35,040
Then the sum of their will be the sum of their mean this a convolution or partial distribution.

348
00:48:35,820 --> 00:48:37,049
So people are smart, right?

349
00:48:37,050 --> 00:48:46,950
So you say, okay, I want to see that in the same Poisson distribution with the constant lambda that's required by this narrative.

350
00:48:47,310 --> 00:48:52,320
I don't want to distribute watching this version to change over time. Otherwise this is a violation of station therapy.

351
00:48:53,160 --> 00:48:57,950
So I want the student to always follow the same distribution problem of alpha.

352
00:48:58,530 --> 00:49:03,809
Well then see that t minus one also false positive distribution responder and I to the

353
00:49:03,810 --> 00:49:08,480
seeming operator that the margin distribution of this according to this different

354
00:49:08,580 --> 00:49:13,770
definition false also distribution is missing of our full time small order and this guy

355
00:49:13,770 --> 00:49:21,000
follows this one minus half of when you add this to the conclusion of this distribution,

356
00:49:21,450 --> 00:49:24,570
you'll get this possible one. Okay.

357
00:49:24,960 --> 00:49:31,260
That's the deconstruction of Integer Out of time series or time series of accounts.

358
00:49:31,560 --> 00:49:40,540
Okay. Look. The alt functioning out of this construction is our fault to the p0h.

359
00:49:46,790 --> 00:49:50,840
Well, this is h. I so I change this. Okay, this.

360
00:49:50,870 --> 00:49:54,169
This is h. Okay, so this is like, right.

361
00:49:54,170 --> 00:49:57,860
So you have exactly the same operating function as you.

362
00:49:58,400 --> 00:50:02,480
You have sending out the regular error one process.

363
00:50:03,020 --> 00:50:10,099
Okay. So my, I mean, due to my pitch, the I was very fascinated with kind of this argument.

364
00:50:10,100 --> 00:50:16,600
I try to extend this to almo p q So how to work on a model?

365
00:50:16,610 --> 00:50:24,940
I'm a model. And, and anyway, so, so that's some, some work I've done due to my views.

366
00:50:26,680 --> 00:50:38,919
Yeah. So so senior operator can be extended to many non normal marginal distributions such as dollar distribution.

367
00:50:38,920 --> 00:50:46,900
So I have example you may book, you can look at if so you that follows and comma distribution.

368
00:50:47,350 --> 00:50:55,080
How do you create the stemming operator so that you have sort of marginally closed comma distribute

369
00:50:55,320 --> 00:51:04,870
distribute the process and can be created binomial that's the work I did of is not to this was from ISI.

370
00:51:05,500 --> 00:51:13,930
He came to visit me in Toronto a couple of times. We work out how to do that or binomial distribution and we would the fact parodies.

371
00:51:18,210 --> 00:51:24,140
And I did the very general class of the, you know, especially my my distribution, my,

372
00:51:24,150 --> 00:51:32,219
my supervisor and I published a paper in and of probability just see how broadly this problem,

373
00:51:32,220 --> 00:51:41,130
this all senior operator can be defined to define different type of non normal distributed statistical process.

374
00:51:41,790 --> 00:51:50,220
Okay. So all these things they trying to prepare the common field are smoother to really understand this infectious disease situation.

375
00:51:50,370 --> 00:51:50,530
Okay.

376
00:51:50,640 --> 00:52:02,610
So that I can build up different type of model for different problems to apply common future and instabilities or to solve the times or small problem.

377
00:52:02,960 --> 00:52:13,540
Okay. So one thing I should mention that common future is actually the idea of online learning, because it really is, you know,

378
00:52:13,540 --> 00:52:23,530
this recursive sort of opt in procedure is essentially it's online learning, which is very popular now in the.

379
00:52:30,260 --> 00:52:41,670
The how this looks like I. Okay.

380
00:52:41,680 --> 00:52:48,420
So what I want to achieve here is really a recursive relationship to predict.

381
00:52:48,970 --> 00:53:00,160
So this is my goal. So I want to use the historical data up to time t to calculate the block best buy's prediction of my city.

382
00:53:00,400 --> 00:53:06,730
That's my goal. Okay. But I want to achieve this goal by a recursive updating procedure.

383
00:53:06,880 --> 00:53:10,390
Okay. Okay. How am I to do that? Suppose.

384
00:53:10,660 --> 00:53:16,840
Suppose I'm already able to do this. So adds previous time t minus one.

385
00:53:17,230 --> 00:53:21,220
I've already done this. Okay. So T minus one.

386
00:53:21,610 --> 00:53:25,230
I suppose I have done this.

387
00:53:25,240 --> 00:53:34,870
I'm able to use something to predict C that t minus one that gives me my M minus one, which is the the block,

388
00:53:35,140 --> 00:53:42,760
the best in the bias prediction and the deviance of that prediction error or prediction accuracy.

389
00:53:43,120 --> 00:53:47,560
So I have the mean I have the variance, all of that prediction.

390
00:53:48,130 --> 00:53:56,890
So this is suppose I can do this. Now the question here is how am I going to move this from T minus one to T?

391
00:53:57,700 --> 00:54:03,210
Okay. If I am able to calculate this up to here, then I finished my job.

392
00:54:03,220 --> 00:54:06,520
This is actually what a column of your common future does. Okay.

393
00:54:06,550 --> 00:54:10,590
How do you do that? You first do this.

394
00:54:10,760 --> 00:54:15,410
Okay, step one, your computer. What is the noise?

395
00:54:15,420 --> 00:54:23,040
Really? Prediction. Using the historical data up to T minus one to predict the time to predict

396
00:54:23,050 --> 00:54:28,950
the latent variable at time t so you have sort of one time ahead prediction,

397
00:54:28,980 --> 00:54:33,750
right? I have a data from time up to yesterday.

398
00:54:33,750 --> 00:54:35,970
I'm trying to let's a t is today.

399
00:54:36,360 --> 00:54:47,399
I used data up from all the data up to yesterday and trying to predict my latent variable infected disease today using it.

400
00:54:47,400 --> 00:54:50,830
So this is actual prediction. This is our sample prediction.

401
00:54:59,650 --> 00:55:05,110
So you predict something out of your sample, so you'll only have through the sample up to t minus one.

402
00:55:05,110 --> 00:55:09,550
You're one step ahead outside of the sampling window.

403
00:55:09,910 --> 00:55:14,530
So this is all sample prediction. So how do you do that?

404
00:55:14,560 --> 00:55:20,380
Well, this can be proved, right? So if you wanted to kind of prove that, that's easy.

405
00:55:20,620 --> 00:55:31,270
But I bet you, you know, the BDA, I have calculated many more and many times now before you have to see that minus one.

406
00:55:31,780 --> 00:55:39,939
Now you replace your theta T minus one by blow up and you might as well empty.

407
00:55:39,940 --> 00:55:44,230
Minus one has to be calculated. Okay. Plus BD and h t.

408
00:55:45,610 --> 00:55:48,920
So h t is given like this. Okay. Okay.

409
00:55:49,070 --> 00:55:53,470
This is one thing you need to do at the same time you do the prediction for y.

410
00:55:54,370 --> 00:56:03,070
So you get the data up to two months. Of course you observe Whitey, but you want to have also the prediction for Whitey.

411
00:56:03,100 --> 00:56:12,010
This is also a wholesome prediction. So you predict what's the of the observed out of life.

412
00:56:12,070 --> 00:56:17,080
This is the black and Q is predicting covariance matrix.

413
00:56:17,290 --> 00:56:22,210
It's given like this. Okay. So you can program this very easily.

414
00:56:22,900 --> 00:56:28,690
Those are all the linear operation. Okay. Nixon They're all DNA operations, right?

415
00:56:29,320 --> 00:56:35,650
So now after we get this awesome prediction, now you do insightful prediction.

416
00:56:43,420 --> 00:56:49,000
So example prediction is like I have the data up to t and predicted this.

417
00:56:49,840 --> 00:56:53,990
They see that within the sampling window. Sampling time window.

418
00:56:54,040 --> 00:56:57,430
So this is in sample protection. Okay.

419
00:56:57,440 --> 00:57:03,940
How are you going to do that? So suppose this empty are the quantity I want to compute.

420
00:57:04,210 --> 00:57:08,020
Okay. And then this one will be given like this.

421
00:57:08,290 --> 00:57:15,880
Okay. So this is the prediction from the this part from step one.

422
00:57:16,390 --> 00:57:21,460
This is all sample prediction. This part. This part is your all time prediction.

423
00:57:22,000 --> 00:57:28,880
This is the prediction error. If you use FTT to predict y t what is the prediction error?

424
00:57:29,500 --> 00:57:33,790
We call in times which we call innovation. So is the combination.

425
00:57:33,940 --> 00:57:37,350
How do you predict? You sample prediction empty.

426
00:57:37,360 --> 00:57:41,980
You have all the time prediction and all sample prediction error.

427
00:57:43,060 --> 00:57:52,080
This is also predict. Some prediction error for the for whitey.

428
00:57:52,350 --> 00:58:03,880
So that gives like this looks complicated but they're all computable and this is the the actually updated the prediction performance metrics.

429
00:58:05,390 --> 00:58:14,050
So the key point here, the key point here is that you can see that the calculation of the AMT, which is your common future,

430
00:58:14,060 --> 00:58:27,530
I think the one that you really want to get for the blood only depends on all the quantities obtained in, you know, recursively.

431
00:58:28,070 --> 00:58:32,060
Okay. It doesn't it does not depend on anything and beginning.

432
00:58:32,720 --> 00:58:39,710
So everything everything here depends on what you have in the previous step.

433
00:58:40,160 --> 00:58:43,250
For example, right. You look at empty minus one.

434
00:58:43,250 --> 00:58:48,360
Empty minus one is the one you just opted from previous that your milk had left.

435
00:58:49,070 --> 00:58:53,570
Empty also depend on empty minus one only depend on the value.

436
00:58:53,570 --> 00:59:04,160
From this step. You look at your q t, which is this one perfectly all independent of the city, minus one which is of tendered from this step.

437
00:59:04,610 --> 00:59:10,340
So you look out of the h t only depends on say T minus one, which is obtained from previous step.

438
00:59:10,730 --> 00:59:14,540
So. So you have a perfect recursion.

439
00:59:15,200 --> 00:59:18,560
Okay. You don't need that. You. The.

440
00:59:22,010 --> 00:59:27,580
Lot of sort of calculation down in the time t minus one.

441
00:59:28,000 --> 00:59:32,590
Then you need only use model structure those quantities eight eight team.

442
00:59:33,710 --> 00:59:43,190
DADT. Those are the components from your mall, plus the values from your previous update.

443
00:59:43,670 --> 00:59:49,880
And then you can finish up this recursion. This is very fast because this is all in operation.

444
00:59:50,600 --> 01:00:00,920
So you don't need to essentially reprocess data from time ty1 to y t minus one and everything is based on this opting for updating.

445
01:00:03,810 --> 01:00:06,840
Thought then comments. Moser then.

446
01:00:07,320 --> 01:00:19,379
So this is a forward okay I'm a filter is for updating you'll start a form so you don't see the 1y1 then you used the first one the first time.

447
01:00:19,380 --> 01:00:24,890
Then you do the forwards updating recursively, which is very easily.

448
01:00:24,910 --> 01:00:38,220
Then after you get all common filter completed to the time and then you do backward the backward sort of updating.

449
01:00:39,240 --> 01:00:45,330
So, so because you know, comments Moser requires that you need to do plot using all data.

450
01:00:46,110 --> 01:00:50,220
So that's also can be done by these returns relationship.

451
01:00:50,850 --> 01:01:01,470
So suppose that you already have the the block calculated at a T plus one given all the data now you're moving backward to.

452
01:01:04,880 --> 01:01:08,390
Then you can update this whole thing using this.

453
01:01:10,390 --> 01:01:16,250
But that those artful maneuvers that happened to be like that and some some people said apply mathematics.

454
01:01:16,420 --> 01:01:23,390
I was that is sometimes it very oddly but that's the way it looks like all the way, because it looks complicated.

455
01:01:23,450 --> 01:01:27,450
But that's how this looks. The term computing.

456
01:01:27,450 --> 01:01:30,740
That's pretty fast. So, so, so for commerce.

457
01:01:30,750 --> 01:01:34,730
Moser What do you do here is that you do backwards.

458
01:01:35,070 --> 01:01:40,520
Okay, after. Or to reach the end of the time window.

459
01:01:40,700 --> 01:01:46,759
And then you go backward to do of updating up here.

460
01:01:46,760 --> 01:01:52,070
This y t is always fixed up to white and then you do the calculation.

461
01:01:54,660 --> 01:01:57,840
Okay. So here is the application of this.

462
01:02:00,410 --> 01:02:16,850
So I did this for. So after I do this, then I just try to apply this to certain data analysis.

463
01:02:16,860 --> 01:02:26,460
So this is very famous data analyzed by scholars who with tinkers and the visor and just.

464
01:02:30,110 --> 01:02:33,800
In a department. You got to analyze this data using cheat.

465
01:02:34,610 --> 01:02:38,660
So what I did here is to analyze this by computer.

466
01:02:39,770 --> 01:02:49,760
And so. So this is a single time series of monthly Apollo incidences in the USA from 1972 to 1983.

467
01:02:50,090 --> 01:02:55,580
So you have monthly data, a number of the positive incidences in the country.

468
01:02:56,430 --> 01:03:00,590
Yeah. So we want to analyze data.

469
01:03:00,650 --> 01:03:06,590
There is a clear scientific question behind this data analysis.

470
01:03:07,640 --> 01:03:11,630
So let me just show the data. Okay. So here is the data.

471
01:03:14,110 --> 01:03:21,170
So part of disappearing the country and it has some incidents, monthly incidences.

472
01:03:21,750 --> 01:03:25,580
You're captured by CDC. Right. So this is a single time series.

473
01:03:25,970 --> 01:03:29,030
Number of incidences, new cases. Okay.

474
01:03:33,180 --> 01:03:48,300
At somewhere. It's a little bit seasonal because you can say there's really is a high O counts in the day for the fall season for some reason.

475
01:03:48,720 --> 01:03:54,360
Okay. And relative lull comes in the spring and winter time.

476
01:03:55,620 --> 01:03:58,750
There is a little bit seasonal pattern here.

477
01:03:59,080 --> 01:04:02,950
But the question once a scientific question, design questions like this.

478
01:04:04,670 --> 01:04:11,020
Oh. To whether or not the data provide evidence of decreasing trend.

479
01:04:15,670 --> 01:04:26,600
After the country implement a national wide. I did so in that night early 1970s.

480
01:04:26,600 --> 01:04:39,200
The country implement the Andy Paul campaign vaccine a policy and then really want to see whether or not this vaccine is really help to reduce the,

481
01:04:39,890 --> 01:04:43,460
you know, the rate of US infection over time.

482
01:04:44,410 --> 01:04:48,440
Okay. And to not study like.

483
01:04:50,510 --> 01:04:59,360
So I was involved in the one project, even with the Collaborative Fund, the Center of Transportation Research.

484
01:04:59,930 --> 01:05:05,899
You know that in the U.S. there's something called the GDR graduate driver's license license.

485
01:05:05,900 --> 01:05:11,240
So you need to do the oh, some kind of a tweet.

486
01:05:12,160 --> 01:05:15,190
Or something that you were to get a full driver's license.

487
01:05:15,790 --> 01:05:19,390
So then, of course, at a certain time, this girl.

488
01:05:21,550 --> 01:05:28,360
Was introduced in different state like Michigan. 1994, 99.

489
01:05:28,700 --> 01:05:32,170
It's not that far.

490
01:05:32,470 --> 01:05:37,810
Like, a lot longer. It's just like 30 years or so.

491
01:05:38,710 --> 01:05:50,290
So you have a certain policy that you will see after the introduction of what are not the number of car accidents involving dual drivers would be.

492
01:06:00,510 --> 01:06:03,960
Let her know that the number of the incidents will reduce.

493
01:06:04,930 --> 01:06:13,680
Introduced GDL this restrictive driver license sort of a policy with the number of car accident involving new drivers.

494
01:06:13,680 --> 01:06:18,000
So she knew that drivers would do it. Right. So so some similar question.

495
01:06:18,180 --> 01:06:22,020
Okay. So you can use the car if you are from schools or to really studies.

496
01:06:24,410 --> 01:06:26,780
Well, Time, says Paul, reveals the system and pattern.

497
01:06:26,780 --> 01:06:33,260
As I said, that higher incidence during October, December, in addition a potential decrease in turning over time.

498
01:06:33,590 --> 01:06:42,680
So you can't say that between somehow this is the hire and reduce the somehow backstop and then reduce over this 40 years

499
01:06:43,160 --> 01:06:51,770
there's sort of a higher here and reduced probable vaccine and there's a bounce back problem with this due to seasonality.

500
01:06:52,490 --> 01:07:01,250
Okay. Then all that. So you want to answer this scientific question in general, if there there's a decrease in twin of that, right.

501
01:07:02,840 --> 01:07:10,680
Because of the low counts, the data cannot be properly analyzed by, you know, traditional this normal data.

502
01:07:10,700 --> 01:07:16,610
Right. So you have a lot of zero a lot of ones and all tools.

503
01:07:16,640 --> 01:07:19,670
And so this is low can't data.

504
01:07:19,670 --> 01:07:22,069
You cannot assume data to be normally distributed.

505
01:07:22,070 --> 01:07:32,930
You cannot use traditional like this all traditional time search model to analyze it, plus that you really want to model the covers into this.

506
01:07:33,390 --> 01:07:37,460
Okay. Well, here is the model. How do we specify?

507
01:07:40,010 --> 01:07:51,890
So you introduce this pricing model, you really want to work out why t is a monthly number of incidents observed for the parties of this.

508
01:07:52,700 --> 01:08:00,470
Now, you believe that there is a underlying infection, infection severity in the country.

509
01:08:00,500 --> 01:08:12,090
This is why it's something the number of. But there is underlying of the Apollo infection going on in the population.

510
01:08:12,100 --> 01:08:16,000
You don't have direct observation from that. Okay.

511
01:08:16,430 --> 01:08:24,220
The start of the plasma distribution will be like this given this current the status of this infection.

512
01:08:24,700 --> 01:08:28,330
You are able to observe it.

513
01:08:28,510 --> 01:08:31,419
That's exactly the space based model I'm talking about.

514
01:08:31,420 --> 01:08:39,040
Using plasma is why you need to present this vision, because you have locals, you have lots of zeros and ones and too.

515
01:08:39,040 --> 01:08:44,080
So this doesn't offer distribution. So you're suppressing while in uniform to do this.

516
01:08:44,560 --> 01:08:52,750
And also that this number of the is part of the incidence to be affected by some seasonal things.

517
01:08:52,750 --> 01:08:58,150
Right. For some time. And you're in the.

518
01:09:01,960 --> 01:09:07,780
Vaccination the time varying coverage of vaccination in population.

519
01:09:10,080 --> 01:09:20,610
Demonstrably in number of weekly work, daily number of deaths in the state of Michigan you can look at the 80 could be

520
01:09:21,000 --> 01:09:27,270
a factor that you put some of the vaccination or some intervention policies.

521
01:09:27,270 --> 01:09:32,860
But any of you here, I do not have any specific individual level data.

522
01:09:32,880 --> 01:09:41,530
This is aggregate data. In terms of number of the incident parties.

523
01:09:41,550 --> 01:09:48,110
This here according to suggestion by Karl Ziegler that we have this season of you have

524
01:09:48,110 --> 01:09:55,610
this half year this is mostly the season though this is sort of half year cycle,

525
01:09:56,110 --> 01:10:06,170
seasonal, seasonal anyway. So we have some seasonality and there you have this t is the exact thing that you are really interested, right?

526
01:10:06,620 --> 01:10:19,580
Because T is the time when you're really a bit over time this the average incidence incidents will decrease over time.

527
01:10:19,640 --> 01:10:31,320
So that this t is the key covered you want added to the process so that you can really answer the question whether or not the department,

528
01:10:31,560 --> 01:10:37,520
the anti-polio pilot vaccination policy is effective over time.

529
01:10:37,550 --> 01:10:45,290
Right. That's something you're trying to estimate. So we have the latent base sort of baseline.

530
01:10:49,540 --> 01:10:51,740
Now you have some couriers that you.

531
01:10:53,070 --> 01:11:00,540
What's it like hot small to read cosmology you have based on has a time something cool beers that can explain the observed hazard

532
01:11:01,890 --> 01:11:09,120
okay delirium fossils describe the baseline evolution of pollen incidence beyond what the time varying covers may capture.

533
01:11:09,930 --> 01:11:17,160
Okay, so you want to observe the number of participants can be explained by some.

534
01:11:18,510 --> 01:11:23,280
There are very, very limited. You only have some seasonal one and plus a twin.

535
01:11:23,370 --> 01:11:28,170
But maybe there are additional things, but we don't have covers to capture that.

536
01:11:29,370 --> 01:11:40,469
For example, here I do not really look at the spatial sort of future where actually the estimates occur in Michigan or in Virginia or outer place.

537
01:11:40,470 --> 01:11:48,170
I don't have that kind of spatial where you don't have temperature, for example, wise in the fall, you have high relative,

538
01:11:48,180 --> 01:11:55,799
a higher number of the participants because in the winter time you have more cases occur

539
01:11:55,800 --> 01:11:59,730
in the northern part of country because the bird offs already in this autumn part,

540
01:11:59,810 --> 01:12:05,640
you don't know. I mean, there are other things that may be evolving to explain the variability of why.

541
01:12:06,330 --> 01:12:15,210
But here I only have this covers and see that you will be something that the young what the other covers can explain.

542
01:12:17,510 --> 01:12:30,000
So if you want to include. The expression I mean, similar to cops model, then you have to make sure that they see that he has me one.

543
01:12:30,280 --> 01:12:36,450
Okay. So. So that's the identify ability problem.

544
01:12:36,450 --> 01:12:39,210
Otherwise the intercept term is not identifiable.

545
01:12:40,080 --> 01:12:48,360
So because they see that he enters this possum model as the mean, they see that he has to be one part of it.

546
01:12:48,840 --> 01:12:52,290
You cannot have an activity that t because you the possum.

547
01:12:52,590 --> 01:12:56,930
This is part of the pass ball. They see that he has to be positive.

548
01:12:57,030 --> 01:13:11,350
Right. So what I did here in the paper is that I created this solution or Iran process, this the common market,

549
01:13:11,410 --> 01:13:15,610
because we know that if you follows karma distribution, that has to be positive.

550
01:13:16,600 --> 01:13:24,190
That's basically motivation. Why I look at something beyond the traditional box Jenkins error.

551
01:13:24,250 --> 01:13:34,690
One process here we see that you follow a karma distribution so that guarantee that see that he is positive and the

552
01:13:34,690 --> 01:13:43,360
thing operator here is that the debate the distribution and karma distribution will give you a karma distribution.

553
01:13:43,780 --> 01:13:46,900
So that's basically the same operator. Okay.

554
01:13:47,410 --> 01:13:51,309
Well, here is the detail you can look at.

555
01:13:51,310 --> 01:14:03,730
And and if you that there's a little bit more detail on this from this example or that condition on that,

556
01:14:03,730 --> 01:14:06,940
we can calculate a lot of the moment conditions.

557
01:14:09,010 --> 01:14:13,540
So there's a little bit involved and you know, it's calculable,

558
01:14:13,540 --> 01:14:21,060
it's just calculating the conditional me and condition variance and just figure out all the terms of that.

559
01:14:21,070 --> 01:14:26,950
Then you can do common future. Okay. So after we get all this moment conditions, right?

560
01:14:27,820 --> 01:14:33,580
So here is the model. You have your polynomial, we have the error one karma process.

561
01:14:33,580 --> 01:14:35,260
You can work out this model.

562
01:14:35,680 --> 01:14:45,610
Those are the the terms that you need in order to figure out what is the capital club be little and capital B, B or something like that.

563
01:14:46,150 --> 01:14:51,760
Then you can, you know, apply that to the the comma filtering smoother.

564
01:14:51,850 --> 01:15:04,200
Okay. So as a result, then you can get your karma filter out of that calculation, which is given right here.

565
01:15:05,370 --> 01:15:11,780
And come on, come on. This is a common future. You can calculate that.

566
01:15:11,790 --> 01:15:15,600
And and this is common, smoother.

567
01:15:16,530 --> 01:15:20,310
You can you can do that. This is kind of smoother and.

568
01:15:21,970 --> 01:15:29,490
That they are all recursive calculation based on the first two moments of principle.

569
01:15:32,490 --> 01:15:37,350
Oh. So this will do.

570
01:15:37,350 --> 01:15:46,410
And I think this one is this this this little one's it's your blood is your estimated the

571
01:15:46,590 --> 01:15:52,950
well this economist most of you this this y is the estimate that we're your empty star.

572
01:15:53,970 --> 01:15:59,310
Okay, so so this this is calculated by this formula.

573
01:16:05,170 --> 01:16:12,180
Empty star. I told you first to become a filter until you to the end of the time search.

574
01:16:12,200 --> 01:16:20,050
Then you do backward calculation, then you get empty star, which is displayed right here.

575
01:16:21,010 --> 01:16:27,610
This solid curve is your empty star. This is the best Anita advise prediction using the entire time series.

576
01:16:28,180 --> 01:16:36,580
This data points here are actually observed. The ones is the estimation result.

577
01:16:36,670 --> 01:16:40,780
Okay, so this is the one g method.

578
01:16:41,050 --> 01:16:45,550
This is published in the Parametric 1988 by Scott Ziegler.

579
01:16:46,150 --> 01:16:50,560
And you can look at this. He's I mean, he has this estimation.

580
01:16:51,940 --> 01:16:53,330
You can you can look at that.

581
01:16:53,350 --> 01:17:01,120
The ratio of this to of course, that you can see that there's a decrease in time between there's a decrease in time between.

582
01:17:01,360 --> 01:17:05,680
But if you look at the ratio, this is not statistically difficult.

583
01:17:06,310 --> 01:17:18,640
This ratio is more than two. No.

584
01:17:19,600 --> 01:17:29,740
And. And again, I think they did this sort of mce.

585
01:17:30,190 --> 01:17:35,830
So Monte Carlo, yes, we are being treated leaving process as euphemism data.

586
01:17:36,190 --> 01:17:43,540
They implement something estimating position using the oh I'm algorithm.

587
01:17:44,140 --> 01:17:47,830
Then they did the same calculation estimation.

588
01:17:48,310 --> 01:17:52,990
Okay, then the metric.

589
01:17:52,990 --> 01:17:56,950
And then I think this is published 1994 dossa.

590
01:17:57,280 --> 01:18:02,650
Anyway, so they did this then. So significant fact.

591
01:18:03,400 --> 01:18:14,140
Okay. So the ratio of this is bigger. So from the NCM algorithm, they did find that there's an increase in between.

592
01:18:16,140 --> 01:18:20,670
I come to the hour. My methods. I'll take this common estimate.

593
01:18:20,670 --> 01:18:23,910
The equation where I do this common funeral and snoozer.

594
01:18:29,570 --> 01:18:36,370
There's you decrease in. Well, this is the a significant point.

595
01:18:38,290 --> 01:18:45,040
The. So the way I did this years I used to come in smoother to.

596
01:18:47,460 --> 01:19:08,210
Latent infection. Two of a posse of two off to really model the dynamics of this we are to the M algorithm also you want.

597
01:19:10,370 --> 01:19:17,170
Yeah. Every year. So the data brought an end to this kind of marginalization.

598
01:19:17,740 --> 01:19:26,170
Like that's both Emsellem and the comment estimate increasing I proposed in my dissertation.

599
01:19:26,350 --> 01:19:31,210
We all work out of making process in some fashion.

600
01:19:31,570 --> 01:19:35,740
In my way, I work hard to deal with believe in process use.

601
01:19:36,550 --> 01:19:44,140
BLOCK Best of the prediction, but you seem to work on the posterior in the Kufa.

602
01:19:45,150 --> 01:19:49,860
To to estimate that they are the parameter of interest in mall.

603
01:19:50,100 --> 01:19:57,870
By the way, there's two methods that have calculated the delays in process in some fashion.

604
01:19:59,330 --> 01:20:09,680
In office with a higher power to get a significant contribution and the significant conclusion about the country.

605
01:20:09,950 --> 01:20:17,420
Okay. So I will cover this. How do you move from the common future to estimate those problems next?

606
01:20:17,430 --> 01:20:22,400
Because we need to that to model our COVID data as well.

607
01:20:23,480 --> 01:20:35,600
Okay. So just a couple of. A couple of remarks.

608
01:20:43,330 --> 01:20:52,630
So one thing I think that's particularly useful for the community modeling here is that instead of doing this ninja trend,

609
01:20:54,010 --> 01:20:57,400
you can use the so-called broken stick model or PS1.

610
01:20:58,730 --> 01:21:05,850
What I mean by that. Yeah.

611
01:21:07,030 --> 01:21:10,860
Right. I just assumed that this for two years.

612
01:21:10,870 --> 01:21:17,170
Right. I have this sort of a simple twin.

613
01:21:17,510 --> 01:21:21,050
So here I have my baby. Yeah.

614
01:21:23,670 --> 01:21:29,040
That's an estimate. So do you have nine or ten? But for the COVID data, right.

615
01:21:32,490 --> 01:21:43,710
You have a December 20, 20 lots of time to impact vaccinations and to introduce maybe what you have here.

616
01:21:47,310 --> 01:21:53,020
Well. My. To this time, then you start to see decreasing.

617
01:21:53,650 --> 01:21:58,630
So the breather is not necessarily a constant over entire period.

618
01:21:58,900 --> 01:22:11,860
And maybe you can have other periods here with the Delta variant up here is that the vaccination efficacy drops and we've all become.

619
01:22:15,050 --> 01:22:18,210
Yours then the. The. The effect. The.

620
01:22:20,120 --> 01:22:24,830
If you look at the total number of the deaths were in fact cases.

621
01:22:25,460 --> 01:22:29,710
The the twin is not necessarily to be a [INAUDIBLE].

622
01:22:30,110 --> 01:22:34,950
So this but you know it can be time dependent at the peace lights near.

623
01:22:35,240 --> 01:22:42,740
So the twins can be, you know, maybe going up or going slowly and then finally bounce back.

624
01:22:43,220 --> 01:22:46,760
Possible. Right. So you know those tiempos.

625
01:22:46,760 --> 01:22:53,750
I wish that the vaccine, if you want to look at the policy effectiveness of vaccination policy,

626
01:22:54,260 --> 01:23:03,379
this policy may be not really a constant for the entire three years we experience that that there is a up and down

627
01:23:03,380 --> 01:23:10,130
during this period depends on what variant you look at how many what what the level of the coverage in population.

628
01:23:10,200 --> 01:23:16,309
All right. So so what I'm seeing here, that is, you know, the your data analysis, I have a cast of data.

629
01:23:16,310 --> 01:23:22,700
But in the reality we mall the infectious disease debate are can be time varying and piecewise.

630
01:23:23,600 --> 01:23:33,110
Okay where you can identify those critical point to really adjust your claim that that's something that you can consider.

631
01:23:34,970 --> 01:23:40,280
And finally and I just just for fun.

632
01:23:40,280 --> 01:23:50,840
Right. So so this is another point that people use this see this model to ah, to detect the check points.

633
01:23:50,930 --> 01:23:56,020
For example, you have observed process that has this switch in point,

634
01:23:56,030 --> 01:24:04,260
this abrupt change point from 0 to -1 drop and from one from minus one to increase to one zero.

635
01:24:04,280 --> 01:24:11,030
So this is one simulated process that means lumps and bumps, breath and drops.

636
01:24:11,190 --> 01:24:17,900
Okay, so this is actual observed times. You can use this based model to model that.

637
01:24:18,140 --> 01:24:25,730
So here is the Pearson distribution. Pearson distribution has very, very sharp distribution.

638
01:24:26,210 --> 01:24:37,600
And then when you use this. You can actually estimate the density of the at this what is this is common future comments.

639
01:24:38,130 --> 01:24:43,560
The comments also can capture the charge point very nicely.

640
01:24:44,640 --> 01:24:47,820
Okay. And then if you come,

641
01:24:48,600 --> 01:24:58,470
we can see that those are quantum lines and you can see clearly the charge points can be easily detected using the state space model.

642
01:24:58,980 --> 01:25:08,280
So why is that? Okay, so sometimes if you look at a chromosome, right, you have copy of the copy number of variation, you have normal copy number.

643
01:25:08,490 --> 01:25:14,370
And so now you have deletion and some they have amplification, you have more copies than necessary.

644
01:25:14,580 --> 01:25:22,140
So those ten points are on chromosome are important to be detected to see where the copy number occurs.

645
01:25:22,350 --> 01:25:31,740
So this is this model is very, very popular, is not only is multifaceted, but also allow us to model some change points easily.

646
01:25:31,950 --> 01:25:37,709
I will tell you how this happens on Thursday, but not our example.

647
01:25:37,710 --> 01:25:43,560
We export how to how to do this use common future to smooth or to do something beyond what we.

648
01:25:44,130 --> 01:25:48,240
Well you know typical think about this like change point.

649
01:25:50,990 --> 01:26:00,300
Yeah. How should we turn it? And step on campus and you turning in person to person.

650
01:26:00,490 --> 01:26:04,080
Right. Right. Okay. Right here? Yeah, in class.

651
01:26:04,830 --> 01:26:09,090
Just want to make sure we had to. Right. Right. Because we have an in-person class or so.

652
01:26:09,270 --> 01:26:15,770
Yeah. If you cannot, then. I mean, so you can arrange a different way to of them.

653
01:26:17,960 --> 01:26:31,870
Okay. That's a. Yup.

654
01:26:34,210 --> 01:26:37,840
That's from USC, but they have a data agreement that needs a signature.

655
01:26:38,110 --> 01:26:41,760
You signed USC? Yes, I did.

656
01:26:43,270 --> 01:26:49,060
So why you for this course? Yeah. So what is this for?

657
01:26:49,240 --> 01:26:53,530
Yeah. There's civilians in the survey for us, I think.

658
01:26:54,310 --> 01:26:57,640
I don't. I don't know how many. I don't have access to the data. We have some people.

659
01:26:57,700 --> 01:27:00,910
They're interviewing their mental status throughout.

660
01:27:00,940 --> 01:27:04,530
Oh, my God. That's great. That's great. Okay, we're on song.

661
01:27:05,120 --> 01:27:09,460
Um, I think we need to look at data room.

662
01:27:10,900 --> 01:27:21,620
Okay. 83%.

663
01:27:21,930 --> 01:27:25,770
Oh, I see. Just. Yeah.

664
01:27:25,930 --> 01:27:30,280
I can't see. Right.

665
01:27:33,210 --> 01:27:40,090
Uh oh. Oh. Number we used to do frequency.

666
01:27:41,910 --> 01:27:45,630
Your basic reproduction number, given the problem.

667
01:27:45,810 --> 01:27:51,690
Is that right? That's. Oh, you do.

668
01:27:52,200 --> 01:27:55,590
All three. Oh, oh, all three. Yeah, of course. All three.

669
01:27:55,600 --> 01:28:01,290
Yeah, yeah, yeah, yeah. You know, so we went to school, so for the first three.

670
01:28:02,450 --> 01:28:11,450
Yeah. But if have some things in London or.

