1
00:00:00,180 --> 00:00:07,620
I apologize. I got a note that for some reason that the audio wasn't very loud for Thursday's recording if anybody was trying to dodge it.

2
00:00:07,620 --> 00:00:15,120
So I think this should work. But if there was a question about whether I have a similar recording or something,

3
00:00:15,120 --> 00:00:22,350
I probably can dig something up or have it go through any the questions that folks have.

4
00:00:23,250 --> 00:00:33,510
What's going on here? All right.

5
00:00:37,910 --> 00:00:41,280
Huh? So you might have seen minor.

6
00:00:41,520 --> 00:00:45,180
I know. Just a a small pivot, I think, here this week.

7
00:00:45,600 --> 00:00:52,770
I thought maybe that I would try to skip this part of the class that traditionally pulled in with a no event cover.

8
00:00:55,080 --> 00:00:59,489
It just it's a nice kind of half step between what I think folks probably learned in

9
00:00:59,490 --> 00:01:04,050
five over three and where we're going to move into next week with multiple regression.

10
00:01:04,650 --> 00:01:09,900
So it's I think it'll dovetail the two and the bivariate stuff we did last week.

11
00:01:10,560 --> 00:01:14,370
Allow us to look at kind of an alternative framing to regression.

12
00:01:14,370 --> 00:01:19,050
So by sure hands folks familiar with ANOVA, did you come across it in earlier classes?

13
00:01:19,410 --> 00:01:26,910
It was an entire different discipline. So back in the day, ANOVA was started as a kind of experimental design that could be utilized.

14
00:01:27,120 --> 00:01:30,419
It was something separate than regression, so the purposes were different.

15
00:01:30,420 --> 00:01:35,340
If you were talking about intervention, if you were talking about testing the effect of your program, you're doing overs,

16
00:01:35,640 --> 00:01:40,709
you're not messing around in the regression analysis back back in the 50 seconds,

17
00:01:40,710 --> 00:01:44,840
whatever it might be, it turns out that mathematically they're identical and right.

18
00:01:44,850 --> 00:01:51,090
And at this point in, I guess, methodological practice, the two are pretty much fully integrated.

19
00:01:51,450 --> 00:01:57,420
That said, there are slightly different commands that we can use to run an ANOVA versus running a regression,

20
00:01:57,630 --> 00:02:00,150
and they're going to get us to the same point at the same place.

21
00:02:00,600 --> 00:02:07,200
So depending on how you want to think about these and if you really want to get into the experimental framework that, you know,

22
00:02:07,260 --> 00:02:13,680
psychologists are using in medicine, might be using that stuff a couple of times we'll use when they run our cities and stuff like that.

23
00:02:14,070 --> 00:02:20,639
You can use a ANOVA, more flexibility to think about things like priority comparisons and postdocs and stuff like that.

24
00:02:20,640 --> 00:02:27,030
So it's not a battle tool to have in our tool kit, but hopefully over the course of the day and as we move into next week,

25
00:02:27,330 --> 00:02:30,450
you're going to see how the underlying principles are the exact same.

26
00:02:30,840 --> 00:02:39,050
And it's also going to give us an opportunity to learn new sort of tests, and we're going to be connecting to the code, the tenant,

27
00:02:39,060 --> 00:02:45,389
the correlation test that we did last week, and the T test that you've done in the past and the Chi Square test that we're going to do in the future.

28
00:02:45,390 --> 00:02:50,580
Right. So following that same logic of trying to find a test statistic that we compared to a known

29
00:02:50,580 --> 00:02:56,400
distribution in order to make our determination whether something is significant or not.

30
00:02:56,970 --> 00:03:03,300
All right. So far today. Homework one, this is your data management homework is going to be due on Thursday.

31
00:03:03,570 --> 00:03:06,990
Our folks at least looking at the assignment, feeling okay about it.

32
00:03:07,320 --> 00:03:11,100
If you've not done an hour before, you're welcome to reach out with questions.

33
00:03:11,100 --> 00:03:12,419
We're kind of following those guidelines.

34
00:03:12,420 --> 00:03:18,989
And again, this is just so that when you're digging around with our you start to understand a little bit about how we can subset data,

35
00:03:18,990 --> 00:03:20,639
how we can call specific cells,

36
00:03:20,640 --> 00:03:26,700
how we do some of the little minutia that we often need to do when we're doing our data data management moving forward,

37
00:03:26,700 --> 00:03:31,019
we'll get into more trying to answer a specific question in using the appropriate tests and all that.

38
00:03:31,020 --> 00:03:35,070
So but this one is just kind of a warmup exercise to make you more familiar.

39
00:03:36,840 --> 00:03:41,720
Is there what format they wanted it like as a simple word document?

40
00:03:41,880 --> 00:03:46,250
Yeah. Word documents, right? Yeah. I do like to have at least your script file.

41
00:03:46,560 --> 00:03:51,540
So that allows me to kind of file if there is an error. This one probably not going to find out without moving forward.

42
00:03:51,540 --> 00:03:53,579
Oftentimes if there's a mistake somewhere.

43
00:03:53,580 --> 00:04:02,580
If I had your script file too, so what I would do is upload the word back and then you're asking if I was going to ask other questions.

44
00:04:03,420 --> 00:04:11,819
Yeah, there was two variables and then it didn't work, so it created four variables instead of having the two.

45
00:04:11,820 --> 00:04:17,400
I didn't find any of the answers or anything, but it just wouldn't be worth the data.

46
00:04:18,950 --> 00:04:23,240
You've got to expand to extra.

47
00:04:23,840 --> 00:04:37,610
Yes, it was now. Yes, it was. So I guess the prestige data had to use to the values for the same census number

48
00:04:38,180 --> 00:04:44,960
so that when you merged profession data ended up having to slaughter one.

49
00:04:45,470 --> 00:04:48,890
Yeah, because it was a slaughter and it didn't line up with it.

50
00:04:48,890 --> 00:04:53,060
So it, it like swapped them when they merged into a creative force.

51
00:04:53,060 --> 00:04:57,020
And keeping that from slaughter is one, slaughter is two measures.

52
00:04:57,620 --> 00:05:03,019
So go ahead, submit it. I mean, these are not great in the sense that, you know, this is the only way to do it.

53
00:05:03,020 --> 00:05:09,620
So I don't want to see it shouldn't there shouldn't be acts as I just I can think of it as should want it very well,

54
00:05:09,620 --> 00:05:17,030
but it could just be a command and part of the merge file or it could just be a default that I would try to pick out.

55
00:05:17,060 --> 00:05:20,959
But as long as it is looking okay, that's what we're kind of going for.

56
00:05:20,960 --> 00:05:24,790
But it's good to flag those things. So practice makes perfect here.

57
00:05:24,800 --> 00:05:29,850
So we're noticing that there's an irregularity, something that you try to correct and we try to delete those rules.

58
00:05:29,850 --> 00:05:32,450
You want to figure out what's going on. So go ahead, submit the right.

59
00:05:32,450 --> 00:05:37,370
If it's not exactly what my introduces, but I'll investigate somebody else's name from.

60
00:05:37,790 --> 00:05:44,720
Yeah, it just was giving us all the possible combinations, like Christine just slaughtered 100.

61
00:05:45,590 --> 00:05:50,520
And I think we take you. Yeah.

62
00:05:50,540 --> 00:05:54,290
I'm trying to think. I haven't with my eyes are getting a bit of. Oh yeah.

63
00:05:54,720 --> 00:05:58,100
Also Securency would find a lot of questions about homework.

64
00:06:01,990 --> 00:06:07,030
Early in the semester, somebody asked me, like about markdown files. We haven't had a chance to look at those data on this specimen.

65
00:06:07,030 --> 00:06:12,120
It was a mark on file, which is just a logic editor within our studio to make things look pretty.

66
00:06:12,130 --> 00:06:17,110
Sometimes they can include some of the text and I can introduce those files a little

67
00:06:17,110 --> 00:06:21,009
bit later if folks want to know more about the nice way to submit an assignment,

68
00:06:21,010 --> 00:06:27,760
if you're if you're interested. But it takes a little bit of coding. So today we're going to look into ANOVA and coma.

69
00:06:28,330 --> 00:06:33,100
The difference is a C, but that C is pretty important because now we're introducing I mean,

70
00:06:33,100 --> 00:06:36,220
we are becoming from butter analysis to multivariate analysis.

71
00:06:36,550 --> 00:06:46,240
So we'll go through the defining features of what an ANOVA is, the defining features of what you cover are, and then we'll kind of do it together.

72
00:06:46,660 --> 00:06:54,490
So I wanted to start, though, with any questions that you might have had from the readings or the and the lecture video.

73
00:06:54,790 --> 00:06:56,679
So we can at least I have them up here,

74
00:06:56,680 --> 00:07:01,960
I'll even write them on the board so I make sure that we touch on it at some point over the next few minutes here.

75
00:07:02,440 --> 00:07:12,219
So any questions anywhere we're talking about comparing to groups T tests, which I'm assuming folks did in five over three.

76
00:07:12,220 --> 00:07:16,510
We're talking about multiple comparisons when we have more than two groups and some of the

77
00:07:16,510 --> 00:07:20,830
challenges of having multiple comparisons and why we can't just run a whole bunch of tests,

78
00:07:21,700 --> 00:07:26,220
questions that just popped out. And so they're not just at the top.

79
00:07:26,230 --> 00:07:32,980
Just one of the discussion questions is what is a dummy variable and how does it really turn over?

80
00:07:32,980 --> 00:07:37,030
And I think it's really good. And again, it's something that everybody gets up front.

81
00:07:38,350 --> 00:07:42,340
Okay. And folks cover DNA coding in the past.

82
00:07:44,830 --> 00:07:48,730
Okay. All right. We'll walk through that. Other questions.

83
00:07:54,140 --> 00:07:58,270
All right, well, let's just see about that, then. Um hmm.

84
00:07:58,790 --> 00:08:02,779
All right. Who can, uh, just read into this result as a little bit last week?

85
00:08:02,780 --> 00:08:07,010
I'm pretty sure you saw this in five, four, three, two. This was our regression model, right?

86
00:08:07,100 --> 00:08:16,330
We have linear model formula. This is what we saw. Who wants to interpret this line forming religiosity or outcome variable over here?

87
00:08:18,010 --> 00:08:27,820
Who's got these guys? Yeah.

88
00:08:28,840 --> 00:08:35,300
Um, so as that invitee increases, religiosity decreases by.

89
00:08:37,560 --> 00:08:45,480
That's their us. In a way, yes.

90
00:08:46,500 --> 00:08:54,840
But at least the way that we set this up in a learning model, not only is it for when death anxiety is zero.

91
00:08:55,080 --> 00:09:00,220
No. You've got the relationship.

92
00:09:00,610 --> 00:09:07,000
But we are using now kind of that predictive framework in that our independent variable religiosity as it changes.

93
00:09:07,420 --> 00:09:11,230
This is what's happening to debt anxiety, not the other way around.

94
00:09:11,410 --> 00:09:19,570
Right. So as our religiosity score increases by one, we expect our predicted debt anxiety values to be about one and a half points lower.

95
00:09:19,960 --> 00:09:22,750
Part of that's kind of manifest in excess.

96
00:09:23,630 --> 00:09:29,380
And then eventually we're going to start adding all kinds of mumbo jumbo down here with additional variables that we need to think about.

97
00:09:30,040 --> 00:09:33,339
But then our outcome variables are always going to be this one on the left hand side. Right.

98
00:09:33,340 --> 00:09:35,710
That's what we are using for prediction.

99
00:09:35,950 --> 00:09:40,929
Now, when we have two variables that we collect at the exact same time, cross-sectional data, that's not a big deal.

100
00:09:40,930 --> 00:09:45,160
If we flip them around, you can just as easily have death, anxiety, predicting, religiosity.

101
00:09:45,490 --> 00:09:50,590
However, if I collected this in 2020 and I collected this in 2022,

102
00:09:50,860 --> 00:09:56,830
it would make absolutely no sense for me to be talking about how deep anxiety is predicting religiosity to happen two years ago.

103
00:09:57,230 --> 00:10:03,459
Right. So temporality is one of those big things that we can do from a design perspective to make sure that our prediction

104
00:10:03,460 --> 00:10:12,640
model is and is valid in terms of something that preceded our often variable is in some way associated with it.

105
00:10:14,600 --> 00:10:23,870
Okay. What about all this stuff? Jenny was kind enough to give us the point estimate when all this stuff.

106
00:10:34,140 --> 00:10:37,860
Really? I think you set a standard here. That is.

107
00:10:40,580 --> 00:10:46,490
How far we are off from the estimated war from the civil the civil population.

108
00:10:48,000 --> 00:10:51,020
So now what immunity does the sampling distribution?

109
00:10:51,020 --> 00:10:56,000
Right. So under a null hypothesis, which is generally speaking that these things are not related,

110
00:10:56,300 --> 00:11:01,879
we would expect a point estimate of about 1.5 to be about oh,

111
00:11:01,880 --> 00:11:07,130
about oh, actually about three and a half standard deviations away from our mean, how do I know?

112
00:11:07,140 --> 00:11:11,270
That's because our standard error is 0.45 0.4496.

113
00:11:11,780 --> 00:11:16,310
Right? So this becomes the standard deviation of our sampling distribution,

114
00:11:16,640 --> 00:11:22,040
which gives us a sense of where on that distribution a value like this belongs or wouldn't land.

115
00:11:22,640 --> 00:11:30,950
What this is telling us is that a value of an observed value of 1.057 is so extreme,

116
00:11:31,700 --> 00:11:37,880
so extreme that it's very, very unlikely we'd observe this by chance.

117
00:11:39,410 --> 00:11:45,820
What if, in fact. We settle the population?

118
00:11:45,950 --> 00:11:52,939
No, no. If in fact, he the null hypothesis, we're sure if in fact,

119
00:11:52,940 --> 00:11:58,940
there wasn't there was no association between religiosity and and death anxiety throughout in

120
00:11:59,060 --> 00:12:05,000
the population to be able to pull out a sample of 60 people and observe a correlation this size,

121
00:12:05,690 --> 00:12:10,970
this point estimate, the size is very, very, very, very unlikely.

122
00:12:11,720 --> 00:12:15,709
That's why we say it's significant and that's what's important.

123
00:12:15,710 --> 00:12:23,740
As you start to develop your own studies and making your own hypotheses, you're always comparing you're always trying to reject the null hypothesis.

124
00:12:23,750 --> 00:12:27,680
We're not proven anything. I know you've heard this before, but it bears repeating.

125
00:12:28,160 --> 00:12:34,190
It's not saying that this association actually, absolutely, positively exists, and this is what it's going to be.

126
00:12:34,760 --> 00:12:37,640
You know, we can make this a correlation instead of a regression coefficient.

127
00:12:38,330 --> 00:12:44,300
What we're saying is that we thought that there wasn't anything associated between the two, no associations whatsoever.

128
00:12:44,780 --> 00:12:46,730
But then we collected 15 observations.

129
00:12:47,180 --> 00:12:54,990
And darn it, if it doesn't seem like there is a strong negative association between the two, that's our world, right?

130
00:12:55,580 --> 00:13:03,380
And we want to correct folks when we fall into that trap of saying we have proved that religiosity is related to death anxiety.

131
00:13:03,920 --> 00:13:04,219
All right.

132
00:13:04,220 --> 00:13:13,070
So this is minor corrections, just mild nudges away from some of that language to talk about what we're really seeing when we see an output like this.

133
00:13:14,120 --> 00:13:17,360
Good. Excellent. How about this one? How about this one?

134
00:13:18,260 --> 00:13:26,120
Oh, you know what we are getting down to, you know, this fully standardized model.

135
00:13:26,570 --> 00:13:30,410
And I covered it really quickly last week, but it was awesome in the video.

136
00:13:30,480 --> 00:13:37,740
You get no excuses. You get. That's the difference having my children in this now.

137
00:13:53,890 --> 00:13:58,690
I'll look at your computer. Something, look at this thing and tell me what you know.

138
00:13:59,110 --> 00:14:03,479
Well, I tell you what. I think the same interpretation.

139
00:14:03,480 --> 00:14:06,580
The number is small. Hmm. Well, no.

140
00:14:08,100 --> 00:14:13,780
In some instances, yes. It's going to be the same interpretation. And yes, the numbers are smaller, but it's there.

141
00:14:13,800 --> 00:14:18,240
There's an important little nuance here. Okay.

142
00:14:18,240 --> 00:14:22,020
We chatted about this. We chat about this memory.

143
00:14:22,020 --> 00:14:25,980
You came up and asked me, but I. With the z-score. Yes, yes.

144
00:14:26,160 --> 00:14:32,550
Yes. Z scores at this point is more or less we've converted both the things that a lot of you are said it is these scores,

145
00:14:32,880 --> 00:14:38,810
which means which means that everything is like on this page.

146
00:14:38,820 --> 00:14:41,850
All the units are taken out. So there's standardized. Absolutely.

147
00:14:42,060 --> 00:14:45,900
Which means this interpretation will not change. It's just a little bit.

148
00:14:46,140 --> 00:14:50,550
Now you're up. You said same interpretation in the sense that a unit change here.

149
00:14:50,970 --> 00:14:59,100
Yes, correct. A unit change and religiosity is associated with a -0.69.

150
00:14:59,100 --> 00:15:07,590
That's what this -0 one means. All right. Just move the decimal place over one -2.69 decrease in your death anxiety score.

151
00:15:08,190 --> 00:15:09,270
Now, here's the rub.

152
00:15:09,690 --> 00:15:17,309
As soon as we standardize things, we are no longer, as Jenny mentioned, talking about the original units, which didn't make a ton of sense anyways.

153
00:15:17,310 --> 00:15:20,640
They were just some random scale. We're going to talk a lot about scale in this class.

154
00:15:21,390 --> 00:15:26,940
Now we are in standardized units, which means we have a mean of zero and a standard deviation of one.

155
00:15:28,110 --> 00:15:32,999
A unit change here means a standard deviation.

156
00:15:33,000 --> 00:15:44,430
Change in our predictor variable is associated with a -0.69 standard deviation change in our outcome variable.

157
00:15:45,630 --> 00:15:51,200
That's a subtle but really important difference. We're not talking about being another scale that might go from 1 to 60.

158
00:15:51,270 --> 00:15:57,900
We go up one one step. We're now talking about whatever a standard deviation means for that scale.

159
00:15:58,320 --> 00:16:02,160
Recall that the standard deviation is just the square root of the variance.

160
00:16:03,330 --> 00:16:07,620
Good. Remember that. Right. And it's just a measure of spread.

161
00:16:08,310 --> 00:16:12,270
The standard deviation for any given scale or item or whatever we collect is

162
00:16:12,270 --> 00:16:16,710
going to change just based on how we've collected or how we've set that scale.

163
00:16:17,640 --> 00:16:21,690
But it's unlikely to be just a one unit or one step up.

164
00:16:21,700 --> 00:16:25,830
It's unlikely if we're in a 1 to 5 liquid scale, going from a 3 to 4,

165
00:16:26,430 --> 00:16:32,880
chances are now it's going to be something like 2.2 because that's how much a standard deviation is on a literal scale.

166
00:16:33,120 --> 00:16:36,630
For example, just making that up, that makes sense to folks.

167
00:16:37,500 --> 00:16:45,690
The reason why this is not great is because folks have a hard time understanding and thinking about standard deviations, right?

168
00:16:45,690 --> 00:16:48,989
It just is not intuitive for us to say, oh, okay, yeah, standard deviation,

169
00:16:48,990 --> 00:16:56,399
religious or a standard deviation in settings they usually are stealing has some kind of more

170
00:16:56,400 --> 00:17:04,920
communicable driver language or or a number of systems that is as is easier or more intuitive for folks.

171
00:17:06,060 --> 00:17:10,470
This is helpful because now we are on units that can be compared across studies.

172
00:17:11,100 --> 00:17:14,129
So if you recall, this is the correlation coefficient.

173
00:17:14,130 --> 00:17:19,980
When we have two variables in a regression model that's fully standardized, it reduces down to the Pearson correlation coefficient.

174
00:17:20,370 --> 00:17:25,650
If I ran a correlation between these two items, instead of running a regression, I would get minus .69.

175
00:17:26,160 --> 00:17:29,230
And we did that last week. Right. So that's the,

176
00:17:29,250 --> 00:17:34,110
that's what that regression coefficient is and we have a similar test we should get something on

177
00:17:34,170 --> 00:17:41,190
very similar to whatever we get from the correlation test directly comparable no matter what.

178
00:17:41,220 --> 00:17:43,590
Two variables we decide to correlate together.

179
00:17:44,100 --> 00:17:52,440
If I run a Pearson correlation between any two variables, a point or a minus .69 would be smaller than a minus .75.

180
00:17:52,440 --> 00:17:55,800
It would be larger than a -0.55. Right.

181
00:17:56,280 --> 00:17:59,520
That's why we like standardization for interpretability sake.

182
00:18:00,000 --> 00:18:01,620
More likely to go in and standardize model.

183
00:18:04,700 --> 00:18:10,880
It's good to get this stuff down because it's only going to get a little bit more complex when we start adding new variables or

184
00:18:10,910 --> 00:18:17,150
when we start changing a little prior little parts of our process as we move into more and more complex regression analysis.

185
00:18:17,870 --> 00:18:22,250
But the great news is it's going to look like this a lot of this semester.

186
00:18:22,910 --> 00:18:26,390
You're going to see an output that, by and large looks just like this.

187
00:18:27,470 --> 00:18:33,920
Okay. So I challenge you this week. Find a slide if you want to talk to your friend about that.

188
00:18:33,920 --> 00:18:37,550
Jeremy, really like you burn? I don't care, Dr. Drew. You're pregnant.

189
00:18:39,020 --> 00:18:45,079
So good, right? Just tell us, what's the difference between these two and why is it important to think about maybe a

190
00:18:45,080 --> 00:18:48,800
scale that has some value to you and something that's very meaningful and terrible for you?

191
00:18:49,160 --> 00:18:56,530
And then what that would mean if we suddenly standardize things and why that becomes all of us to the doctors questions about this.

192
00:18:56,550 --> 00:19:08,840
Yeah. Yeah. So would you interpret, could you say, the interpretation for the 96 women and would that be the same as above or one standard deviation?

193
00:19:08,840 --> 00:19:18,350
Change in religiosity is associated with a minus points 0.69 decrease in your predictor or standard deviation, decrease in your death anxiety.

194
00:19:21,020 --> 00:19:24,469
So what we could do if you wanted to, you could take that scale of death anxiety.

195
00:19:24,470 --> 00:19:30,410
We brought that IQ data and we could just calculate what's the study members that want to see function of death anxiety.

196
00:19:30,650 --> 00:19:33,740
And it would tell us that it's maybe 5.1.

197
00:19:34,220 --> 00:19:40,910
So really what it would be saying is that about some fraction of 5.1,

198
00:19:40,910 --> 00:19:45,920
because that's how many now deviations or that's how many points a standard deviation is for death anxiety.

199
00:19:46,310 --> 00:19:58,040
That would be like the true decrease if you think as was the one that IQ in theory has a median of 100 and elimination of ten.

200
00:20:01,370 --> 00:20:07,280
Right? In theory. Right.

201
00:20:08,660 --> 00:20:13,340
So, A, if this was if this work was whatever.

202
00:20:14,150 --> 00:20:24,650
Q Instead of religiosity. A standard deviation change in religiosity would be a change would be associated with a 4.69 decrease.

203
00:20:25,560 --> 00:20:36,380
2.69 decrease. In your predicted outcome variable because it's going down by standard deviations.

204
00:20:40,420 --> 00:20:49,980
Therefore, for. All right. We'll do this quite a bit, because, again, if I have four predictor variables and they're all on different metrics,

205
00:20:50,280 --> 00:20:53,640
I'm not going to be able to compare these regression coefficients to one another.

206
00:20:53,970 --> 00:20:55,710
They're all going to be dependent on the scaling.

207
00:20:56,190 --> 00:21:03,150
But as soon as I standardize my model, then if I have variables from all different backgrounds and shapes and and scaling,

208
00:21:03,630 --> 00:21:07,290
they're going to be all they're going to be transformed on to the same metric.

209
00:21:07,710 --> 00:21:11,040
And as this nifty little command is going to allow us to do this better.

210
00:21:11,490 --> 00:21:18,450
So let's do that. Because you mentioned earlier now that people didn't interpret the standard model, solidly standardized model.

211
00:21:19,440 --> 00:21:24,870
So it's just that instance, if we had three predictors here, this was religiosity.

212
00:21:25,620 --> 00:21:27,780
The next variable could be your income.

213
00:21:28,800 --> 00:21:34,980
Next very variable could be your your health insurance status, all of which might be related to death and anxiety.

214
00:21:35,130 --> 00:21:44,010
I have no idea. And mega income is going to be on a scale of like zero to however many hundred thousand dollars right now in terms of religiosity,

215
00:21:44,010 --> 00:21:47,900
if you saw the original metric like range from 1 to 30 or 1 to 60, I can't remember.

216
00:21:47,960 --> 00:21:56,270
Now, do you have health insurance is just a01 vary because of that different scale and you might talk about a scaling it's two factors.

217
00:21:56,280 --> 00:22:00,870
One variable is 1 to 60, the other is 0 to 1000000, the other is 0 to 1.

218
00:22:01,470 --> 00:22:11,129
Because of that, scaling, the way that we calculate this coefficient is going to be much smaller for anything that has a huge range, right?

219
00:22:11,130 --> 00:22:20,490
Because when you think about a regression equation, we have our outcome variable equals our intercept,

220
00:22:21,150 --> 00:22:28,080
whereas our predictor variable times the x value plus the predictor variable time, the other x value kind of etc.

221
00:22:28,110 --> 00:22:34,110
Right. This coefficient goes right here.

222
00:22:39,100 --> 00:22:46,210
Right and get multiplied by whatever value of religiosity that we have when we actually do run our prediction model.

223
00:22:46,630 --> 00:22:51,100
Right. So this X value is going to range between 1 to 60.

224
00:22:52,540 --> 00:22:57,970
Now, this outcome, variable death anxiety, I think also ranges between like 1 to 60.

225
00:22:59,920 --> 00:23:00,650
What do you have to do?

226
00:23:00,670 --> 00:23:10,450
We have to construct this model so that when we plug in people's values and all kinds of and all kind of adds up to some value between 1 to 60.

227
00:23:11,590 --> 00:23:16,690
Right. It's got to be within the range of our plausible outcome variable or at least have some plausible range.

228
00:23:17,740 --> 00:23:33,300
If I. And this was income. And I know that I'm going to have X values that are going to be thousands, thousands, let's just say 50,000.

229
00:23:37,430 --> 00:23:40,880
Is this data coefficient going to be big or small?

230
00:23:45,200 --> 00:23:48,660
Why the small settlement? Because you really.

231
00:23:50,050 --> 00:23:55,750
That's why. Exactly.

232
00:23:56,370 --> 00:23:58,830
Chris. Chris. Yeah, exactly.

233
00:23:59,050 --> 00:24:09,670
If we know that this value has to be between one and 16, this beta coefficient that we're going to multiply by 50,000 has to be something pretty tiny,

234
00:24:10,570 --> 00:24:15,310
probably something like, you know, .00003 or something like that.

235
00:24:16,900 --> 00:24:24,910
Right. Now comparing these beta coefficients directly, we have -1.5 7.0003.

236
00:24:25,570 --> 00:24:33,670
Which one's better? Which ones though? If we had to choose between these two variables, which of these coefficients do you think we should prefer?

237
00:24:34,480 --> 00:24:37,870
Which has a stronger influence on our outcome variable?

238
00:24:41,860 --> 00:24:46,480
Spoiler. You can't answer that question. Looking at the answer standardizes the coefficients.

239
00:24:47,290 --> 00:24:48,369
Now, it's interesting.

240
00:24:48,370 --> 00:25:01,960
As soon as we standardize these things and we put these both on the same scale, we're going to find that this one standardized is going to be -8.6.

241
00:25:02,540 --> 00:25:10,720
Now, this one standardized mechanism is going to be positive .53.

242
00:25:11,620 --> 00:25:15,579
Which of these value or which of these variables has a stronger effect on the outcome?

243
00:25:15,580 --> 00:25:22,840
Variable minus .69.

244
00:25:24,940 --> 00:25:30,919
Positive .53. They're fully standardized models.

245
00:25:30,920 --> 00:25:38,130
You can think of these as correlations. Success is not the absolute value.

246
00:25:38,250 --> 00:25:43,650
Absolutely. Is it any larger here than this variable?

247
00:25:44,190 --> 00:25:51,480
Making all this up, obviously, this variable at minus point 39 is a larger effect size than .53.

248
00:25:51,960 --> 00:26:01,950
We would say that religiosity explains more variability in our outcome variable than someone's income.

249
00:26:03,990 --> 00:26:11,520
Right. That is why we want to move from a original metric model to something like this.

250
00:26:12,390 --> 00:26:18,420
Now, again, the challenge becomes if this is in fact my standardized recognized standardize equation,

251
00:26:18,870 --> 00:26:29,700
I have to say something like a unit change and an income or a standard deviation change in income is associated with blah blah blah.

252
00:26:30,300 --> 00:26:36,750
Right. Whereas here the change in income could be like $1,000, like a one year change.

253
00:26:36,750 --> 00:26:43,770
They can't just be equated to $1,000 or even a dollar, depending on how we set up this account that for most people.

254
00:26:44,070 --> 00:26:49,650
All right. So when my income goes up by $1,000, I know that my anxiety is going to go up by 20 003.

255
00:26:49,980 --> 00:26:56,130
That's usually a little bit more accessible than if I change my mindset of my income by a standard deviation.

256
00:26:56,640 --> 00:27:01,020
Then my anxiety is going to go up by going from five three standard deviations.

257
00:27:01,650 --> 00:27:05,430
That's the that's that's the that's the loss of interpretability.

258
00:27:07,020 --> 00:27:12,210
So we lose that comparing apples to apples, but we gain some interpretability.

259
00:27:13,920 --> 00:27:21,850
How I feel about this, because, I mean, we're we're presenting all sorts of stuff that we're going to be doing next week.

260
00:27:22,000 --> 00:27:25,379
So this is good. I'm glad we're spending time doing our time. Just a question.

261
00:27:25,380 --> 00:27:32,610
Is this least I know you know my reviews, that I'm happy for them up there if you want.

262
00:27:32,920 --> 00:27:42,220
But yeah, yeah, I'll do that after class, especially if someone says, you know, I lessons the review and it's good.

263
00:27:42,240 --> 00:27:47,100
I love this. This will be a pretty free review here. Get well we get.

264
00:27:49,970 --> 00:27:56,970
Q I saw no, no, no. You said that there no play, no checks swings in this class here you're going.

265
00:27:57,510 --> 00:28:09,990
Oh, I think so. The more tests you do, the like the group becomes or the less impactful they will.

266
00:28:11,220 --> 00:28:24,140
Not quite, quite something. No.

267
00:28:24,700 --> 00:28:30,610
Yes. Any words? He says they used the higher likelihood of making time for her.

268
00:28:30,850 --> 00:28:33,999
Right. Right. So this idea of family wise here,

269
00:28:34,000 --> 00:28:39,459
when we are making multiple comparisons and each one of them has a point or five

270
00:28:39,460 --> 00:28:42,820
or whatever a threshold is going to be a likelihood of making a type one error.

271
00:28:43,150 --> 00:28:49,750
If we do if we do several of those in a row, we're thinking that they're not truly kind of independent test.

272
00:28:50,110 --> 00:28:57,940
Right? The ability to be a true chance of making that type one error skyrockets.

273
00:28:58,300 --> 00:29:06,110
So the more that we do, the more test of you run kind of consecutively, the more likely that happens, which is why we don't forget.

274
00:29:06,130 --> 00:29:12,490
Example, if we have a group of a grouping variable with four values and some sort of continuous outcome variable,

275
00:29:12,820 --> 00:29:21,070
we don't just run four separate teachers, we run a turnover, which is going to correct for the fact that we're making multiple comparisons,

276
00:29:21,910 --> 00:29:24,460
then analyze error, multiple comparisons, the same exact thing.

277
00:29:25,000 --> 00:29:30,940
This, you know, some of these things that we talk about, some assumption violations and stuff in class, not a big deal.

278
00:29:31,150 --> 00:29:32,800
This is a big deal.

279
00:29:33,190 --> 00:29:40,030
Like if you're just running, you know, a half dozen T tests and finding something that's not good if you are doing double checking with your number.

280
00:29:40,300 --> 00:29:44,350
Right. Excellent. Uh huh.

281
00:29:45,250 --> 00:29:48,640
All right. We can only assume that there's another democratic.

282
00:29:49,090 --> 00:29:56,180
Democratic? So the way that this kind of works,

283
00:29:56,420 --> 00:30:00,440
it's a way for regression or really any kind of software to take a grouping

284
00:30:00,440 --> 00:30:05,990
variable and code it in such a way it allows for comparisons across groups.

285
00:30:06,440 --> 00:30:08,240
So when we talk about a grouping variable,

286
00:30:08,540 --> 00:30:16,280
it's something that has we assume that they're kind of mutually exclusive and exhaustive categories or exhaustive categories, right?

287
00:30:16,520 --> 00:30:20,960
You belong to one group, whatever that is, within that variable, and that's it.

288
00:30:21,020 --> 00:30:28,100
You don't spread, you know, spread across groups. So what we're going to be doing is making comparisons through those across those different groups.

289
00:30:28,460 --> 00:30:34,970
And whatever outcome variable might be are another software are going to be able to do this by themselves.

290
00:30:35,480 --> 00:30:43,040
So what the what's ultimately going to happen is you're going to take the number of levels in a groovy variable.

291
00:30:43,130 --> 00:30:52,790
That's just another way of saying the number of groups. And you're going to create the number of levels minus one dummy variables down.

292
00:30:52,790 --> 00:30:58,280
The variables are just an indicator variable that is zero versus one not present versus present.

293
00:30:59,450 --> 00:31:07,220
And so if we have a grouping variable that has three groups. It's on ABC.

294
00:31:11,270 --> 00:31:17,330
We are going to create a set of dummy variables that will be an hour.

295
00:31:17,400 --> 00:31:21,260
A number of groups, three minus one. So we're going to have two dummy variables.

296
00:31:21,960 --> 00:31:36,890
They're going to become zero once. So the current way that the coding works out, if you are in group, we say, I'm going to do this in the thing.

297
00:31:41,250 --> 00:31:55,930
We say. Which is easier if you're a groupie.

298
00:31:57,340 --> 00:32:05,270
Oh, that's right. I was.

299
00:32:05,690 --> 00:32:07,250
I'm boss. Thank you.

300
00:32:08,620 --> 00:32:20,930
Essentially, we're right where we want to get to a point where we're indicating that if if you are if you say something or try to visualize this,

301
00:32:24,440 --> 00:32:28,830
we want to get to a point where if you are in an.

302
00:32:34,200 --> 00:32:40,190
I got I got it. So because the dress we're going to create two birds, we're going to call them variable one.

303
00:32:43,230 --> 00:32:46,530
One variable, one variable two.

304
00:32:47,820 --> 00:32:52,230
If you are a person and group A were arbitrarily going to call the reference variable,

305
00:32:52,650 --> 00:32:57,360
we're going to give you either a zero value to say not present or a one value to say present.

306
00:32:58,110 --> 00:33:03,750
If you're the reference category, you're going to say not present for both of these variables.

307
00:33:04,590 --> 00:33:09,750
If you are in group B, we are going to say present here, not present here.

308
00:33:10,580 --> 00:33:14,910
If you are a group C, we're going to say not present here and present here.

309
00:33:15,990 --> 00:33:23,940
The little trick behind this is if there there is a third variable that you could think of as this group A,

310
00:33:24,390 --> 00:33:28,890
but we don't have to explicitly include that in our regression analysis because

311
00:33:28,890 --> 00:33:33,600
it's going to know if you say no to variable one and you say no to variable two,

312
00:33:34,260 --> 00:33:38,280
that in effect means you are in group B, right?

313
00:33:38,640 --> 00:33:43,080
If there is a fourth category, we would have a third variable.

314
00:33:43,830 --> 00:33:48,900
We're still going to have a reference category, but now it's going to look something like this.

315
00:33:50,220 --> 00:33:55,620
All right. It kind of keeps going up depending on how you respond to one of these three variables.

316
00:33:57,000 --> 00:34:01,830
I'm going to know what group you belong to. And this is how we're going to tell, ah, what group you belong to.

317
00:34:02,430 --> 00:34:07,560
If I'm group D, I say, no, no, no, yes.

318
00:34:08,250 --> 00:34:11,520
If I'm Group C, I say, no, no, yes, no.

319
00:34:12,120 --> 00:34:18,620
And again, A, if you say no to all three of these things, by default, you're categorized as group B.

320
00:34:19,740 --> 00:34:23,040
The good news. The good news, I was going to do it this.

321
00:34:24,630 --> 00:34:28,380
You just need to know what's kind of happening behind the scenes.

322
00:34:28,740 --> 00:34:37,650
The reason why is because when we look at this variable and interpret it, this is going to have a scaling of zero or one.

323
00:34:37,860 --> 00:34:41,280
You can either say zero to this value or a one to this value.

324
00:34:42,660 --> 00:34:48,000
A unit change. A unit change.

325
00:34:49,860 --> 00:34:57,840
A unit change means going from our referent category to the group.

326
00:34:59,160 --> 00:35:08,340
So from here, the variable interpretation is going to be as I go from group A to B, this is the change.

327
00:35:09,630 --> 00:35:18,660
This one, as I go from group A to C, this is the change, right?

328
00:35:19,440 --> 00:35:26,790
That's what is going to allow us to do. It's going to create new variables that let us make comparisons or distinguish

329
00:35:26,790 --> 00:35:33,840
between levels for other groups that we can change based on our own decisions.

330
00:35:34,110 --> 00:35:38,700
Who is the referent category? Right. We don't want be to be the reference category.

331
00:35:39,000 --> 00:35:43,730
We can change this country. So let's see who is the group.

332
00:35:43,740 --> 00:35:49,920
And this is kind of me. If I want a group D to be my referent category, how would my little matrix here change?

333
00:35:51,890 --> 00:36:00,940
Here we observe a quiet. Well, what would be L0 variables?

334
00:36:00,950 --> 00:36:15,350
One, two, three, four, eight. I mean, which was which were so invariable and very different and negative.

335
00:36:15,370 --> 00:36:19,310
I love girls. Very bad coming up here. How does this change?

336
00:36:19,790 --> 00:36:23,000
If we want group D to be our referent category.

337
00:36:27,630 --> 00:36:31,330
Look at this. Okay. All right. Give on on bus here.

338
00:36:33,110 --> 00:36:42,800
Well, I think that's absolutely right. So now we are we are we are using this variable to indicate that group a relative to our referent category.

339
00:36:44,330 --> 00:36:50,870
Absolutely. So that would be happening behind the scenes where you need to know who your reference category is.

340
00:36:51,080 --> 00:36:55,160
Are is going to help you do that, too. But that decision is up to us.

341
00:36:55,340 --> 00:37:00,590
Right? So var from like A to B is like -1.5.

342
00:37:01,040 --> 00:37:05,540
But if we go to eight, a C is the negative five. It's not like two units over or anything like that.

343
00:37:05,540 --> 00:37:11,180
So in dummy three we are not able to compare to Non-Reference.

344
00:37:11,660 --> 00:37:14,090
We actually have to change the referent category.

345
00:37:14,090 --> 00:37:20,450
If we wanted to compare at least in this setup C to A, we'd have to make see the record category, for example.

346
00:37:20,720 --> 00:37:25,610
So we would change this coding again to make this is zero and this is the one I guess.

347
00:37:25,910 --> 00:37:32,150
And then we would look at the coefficient between A and C. That's kind of the bummer biochemically.

348
00:37:35,310 --> 00:37:38,889
This is important here because when we get into a no vote, we're going to have by default,

349
00:37:38,890 --> 00:37:43,530
we're going to have a grouping variable with our many levels and we're going to continue this outcome vertical.

350
00:37:44,520 --> 00:37:49,020
We can do this in an overt and it's a lot more explicit. We have the grouping variable with the outcome variable.

351
00:37:49,500 --> 00:37:55,420
We can also do the same thing in regression, but when we do that, it's going to default to this kind of dummy code.

352
00:37:56,490 --> 00:37:58,890
All right. So we know that you would only need the two variables.

353
00:37:59,310 --> 00:38:08,100
If we did this in regression, we'd end up with three variables just because of this dummy code.

354
00:38:08,920 --> 00:38:16,140
Okay. So it's the way that regression and ANOVA get to the same answer, even though they look a little bit different.

355
00:38:17,760 --> 00:38:23,550
Right. And coding like this is actually somewhat common.

356
00:38:23,850 --> 00:38:29,969
It's very common to know that because we will often do like freak out first a very comparisons if we want to,

357
00:38:29,970 --> 00:38:33,570
if we know that in advance, we think certain groups are going to be different from each other.

358
00:38:33,900 --> 00:38:39,570
We can set some of those things up before we ever run our experiment and then see how they kind of manifest.

359
00:38:39,870 --> 00:38:45,750
And oftentimes it starts with some sort of effect coding like this where enough folks have done a little bit as well,

360
00:38:46,350 --> 00:38:49,620
but we're kind of assuming some of the values before we ever actually run the model.

361
00:38:53,540 --> 00:38:59,000
Questions about this before. Oh.

362
00:39:06,880 --> 00:39:10,240
Usually not heard from small community yet.

363
00:39:10,260 --> 00:39:25,870
Yes. So the when the advancement is zero, very close to zero, it means that the between group variation is small and within group three, Asian is low.

364
00:39:30,120 --> 00:39:41,690
You're. But when you're on the bottom line.

365
00:39:41,970 --> 00:39:50,940
Right. Yeah. So the trigger variation relative to a variation when this number in the numerator is very small, this is very, very large.

366
00:39:51,150 --> 00:39:56,639
We don't have statistics that's equal to zero in layperson's terms.

367
00:39:56,640 --> 00:40:02,310
What would that what does that mean? Marijuana.

368
00:40:03,210 --> 00:40:07,800
There's no indication that being part of that group really matters.

369
00:40:08,130 --> 00:40:08,680
Right.

370
00:40:08,700 --> 00:40:16,649
That's basically saying that if we have very small between group variability with the variability of groups of this group relative to other groups,

371
00:40:16,650 --> 00:40:23,950
relative to another group versus the folks that comprise the different groups, when that's when that that ratio is quite small.

372
00:40:23,970 --> 00:40:27,270
It basically means our group of variables doesn't matter in terms of prediction.

373
00:40:27,360 --> 00:40:37,110
So we would accept the null hypothesis, which is not we would retain our yes or not for a version of this,

374
00:40:39,770 --> 00:40:49,680
which by the way, if this value is close to zero, this becomes the reason we set this up relative to some critical value.

375
00:40:50,010 --> 00:40:53,090
And the decision is based on the value of our statistic.

376
00:40:53,880 --> 00:40:56,850
Is this very unlikely to be observed relative to chance?

377
00:40:57,540 --> 00:41:08,580
If it's not, then we would retain our null hypothesis, which is usually there are no differences between those that like when the statistic is small,

378
00:41:09,780 --> 00:41:17,610
that means that there is such wide variability between the groups that that is a matter what group you belong to is a deficit.

379
00:41:18,750 --> 00:41:25,379
Well, it's it's a good question. So I will say, yes, it doesn't matter what group you belong to, the divide, variability between groups.

380
00:41:25,380 --> 00:41:29,370
That is a nice little segway into this last piece.

381
00:41:29,640 --> 00:41:34,080
I think it's a nice one here. And folks see this. This is kind of hard for the yellow.

382
00:41:34,200 --> 00:41:41,820
You see this A relative to B, can you see the difference spread between the two?

383
00:41:42,810 --> 00:41:46,230
All right. Now, this is directly related to Jenny's question.

384
00:42:09,650 --> 00:42:17,570
Folks think. You'd be the.

385
00:42:17,980 --> 00:42:22,810
My true statistic, because think the more spread out there, the smaller.

386
00:42:23,860 --> 00:42:28,749
So we would be like this smaller statistic, closer to zero.

387
00:42:28,750 --> 00:42:32,190
Or is it complete? Yeah, we've got but recoveries.

388
00:42:32,210 --> 00:42:45,610
Yeah. Yeah, absolutely. Right. So what we're seeing here is not a lot of within group variability, but a fair amount of between group variability.

389
00:42:46,600 --> 00:42:54,250
Right here we see about the same amount of between group variability like the means are still different.

390
00:42:54,970 --> 00:43:02,020
But look at the within group variability, the spread of these points around, that's where all that overlap.

391
00:43:02,290 --> 00:43:08,290
That's why we'd be less likely to see a large F statistic relative to this.

392
00:43:08,770 --> 00:43:12,700
Now here's some here's a practical application for the work that you might want to do.

393
00:43:13,570 --> 00:43:23,110
If your grouping variables are relatively homogenous in any way that you want to think about, that you're more likely to end up here.

394
00:43:23,470 --> 00:43:25,900
Like in a situation where there are true, there are big differences.

395
00:43:26,320 --> 00:43:33,880
Now, ideally, it's something that we are doing, for example, a program that creates this homogeneity.

396
00:43:34,150 --> 00:43:39,309
Right. It's because you've had very similar exposures to our different form of treatment or a different

397
00:43:39,310 --> 00:43:44,290
form of engagement that people are responding in ways that are tightly clustered together,

398
00:43:45,190 --> 00:43:50,620
not a lot of within person variability. It doesn't matter who we're talking about, it matters what group they belong to.

399
00:43:51,280 --> 00:43:56,800
Whereas here there's some clear dependance on who belongs in your group.

400
00:43:57,340 --> 00:44:04,330
And that can be done through sampling. That could be done through a treatment effect that could be done just by relative chance.

401
00:44:04,600 --> 00:44:10,810
Those are the types of questions that we're trying to answer with, for example, that I know and when we're doing some of these group comparisons.

402
00:44:11,650 --> 00:44:20,040
But I think this is good to just try to wrap your head around so that we're not kind of defaulting through just saying it's just between versus group.

403
00:44:20,040 --> 00:44:27,609
They're right. We're trying to understand what the data look like kind of in practice and what we're doing from an intervention

404
00:44:27,610 --> 00:44:34,750
standpoint that would make our F statistic most likely to kind of manifest and allow us to reject our null hypothesis.

405
00:44:35,430 --> 00:44:38,260
Okay. Sometimes we think of the groups as the criers.

406
00:44:38,650 --> 00:44:43,990
Oftentimes what we're trying to do is create the change within groups, sorry for the within part of it,

407
00:44:44,350 --> 00:44:52,989
for groups that we do start to see some of these differences quite so far and there's only use with already defined groups.

408
00:44:52,990 --> 00:44:54,160
Or could you use it a little bit?

409
00:44:54,280 --> 00:45:02,470
Has this fancy grandmotherly class analysis and created groups who use a number to test this fit that those groups are you for sure.

410
00:45:02,590 --> 00:45:07,180
Yeah. Yeah. And we can actually kind of just take a peek at lean class analysis for folks that don't

411
00:45:07,180 --> 00:45:11,950
know any class analysis is another form kind of factor analysis or dimension reduction.

412
00:45:12,280 --> 00:45:17,590
So if you take like 20 different items, you can look for different patterns for the way that people respond to those items.

413
00:45:18,430 --> 00:45:23,620
It's a recall, a person centered technique that's actually pretty, I think, really interesting.

414
00:45:24,400 --> 00:45:30,700
So and then once you've constructed those groups that's kind of constructing groups artificially or with our are empirically,

415
00:45:31,150 --> 00:45:38,080
we can make comparisons just like those between the groups that we've created and we don't even know we can do cross sectional everything.

416
00:45:38,080 --> 00:45:43,890
I wanted to do an area I can show you some examples. How folks feel about this.

417
00:45:46,800 --> 00:45:55,530
This is very conceptual. I think it's good. Now, let's move right into some of the code right now for a practical example.

418
00:45:56,760 --> 00:46:01,829
I think there is this I use that this use it or not.

419
00:46:01,830 --> 00:46:09,510
I, for example, will need update at some point. But so I am interested in explosion violence in this case.

420
00:46:09,510 --> 00:46:19,910
The question for the interest here is whether being exposed to school violence or neighborhood violence is is different across some pretty variable.

421
00:46:19,920 --> 00:46:26,639
So why negative variables that we'll use fairly frequently here and this class was the self identified race variable.

422
00:46:26,640 --> 00:46:33,260
This was collected in 1994. So these are the response options that participants provide.

423
00:46:33,270 --> 00:46:39,430
There was originally a list of eight and the three that through focus groups that the students provide.

424
00:46:39,720 --> 00:46:43,980
These are the four largest high schools in Flint, Michigan. And it's kind of cool the sample.

425
00:46:43,980 --> 00:46:47,070
They followed them from 1994 all the way through 2016.

426
00:46:47,490 --> 00:46:52,319
So they are now they're all about my age. They all have kids and we've got five similar kids.

427
00:46:52,320 --> 00:46:56,040
It was really interesting. So Mark was there started to step way back in 1994.

428
00:46:56,340 --> 00:46:59,460
It's this big dataset of about 850 people.

429
00:47:00,600 --> 00:47:04,830
And I just I just I think it's kind of cool, especially if we get into like on a longitudinal analysis.

430
00:47:05,100 --> 00:47:12,570
It's also freely available to everybody, which is great. So we have a grouping variable we have are dependent variables here.

431
00:47:13,170 --> 00:47:20,340
My neighborhood is a safe place to be. Do I feel safe at school and am I afraid of violence in my neighborhood?

432
00:47:21,030 --> 00:47:27,750
These are the variable names I need folks to start to get familiar with, especially if we're going to use these for kind of assignments and stuff.

433
00:47:28,200 --> 00:47:35,159
So I will show you the code in just 1/2. And then if we get to and cover, we also have a third variable.

434
00:47:35,160 --> 00:47:41,190
That theory could affect our response to something like my fear.

435
00:47:41,610 --> 00:47:49,079
So essentially we're saying the question that we're trying to ask is whether the group you belong to has some sort of influence in a way.

436
00:47:49,080 --> 00:47:53,850
And in terms of the way that you respond to your exposure to violence, does a group matter?

437
00:47:54,840 --> 00:48:01,799
Right. That's what we're asking. This will be a control variable so that we can say something like and this is what a

438
00:48:01,800 --> 00:48:07,860
code language would be accounting for whether or not someone has ever threatened you.

439
00:48:09,030 --> 00:48:19,900
Does groups still matter in terms of your, you know, kind of feelings of feelings of stay with me.

440
00:48:20,860 --> 00:48:24,870
All right. And if you want to ask one, go ahead, pull up the script.

441
00:48:24,870 --> 00:48:28,320
But you can do this all together. All right.

442
00:48:28,920 --> 00:48:33,900
I do want to call your attention to a couple of things when you're starting a question like this.

443
00:48:34,710 --> 00:48:38,310
I've been working with this data for ten years. I know like the back of my hand.

444
00:48:39,060 --> 00:48:41,460
You've been looking at this data for 10 seconds.

445
00:48:42,060 --> 00:48:46,710
You got to get a little bit into what we're doing before we can really start writing some of these analysis.

446
00:48:47,310 --> 00:48:50,370
I promise you, we're going to spend the next 48 minutes going through this,

447
00:48:50,610 --> 00:48:54,690
but the ANOVA that you're going to run is going to take point 8 seconds, right?

448
00:48:54,990 --> 00:49:00,960
That's the current part about this work. The actual analysis part is the quickest and easiest.

449
00:49:01,380 --> 00:49:06,420
All the preamble is what takes our time and what takes our mental energy, right?

450
00:49:06,780 --> 00:49:09,900
So in this case, I would also encourage you to call it the codebook.

451
00:49:10,380 --> 00:49:17,340
The code book is going to be documentation of all the different variables in response within the dataset.

452
00:49:18,120 --> 00:49:25,860
Why is this important? Because if we're reading a data that we've never seen before, we better make sure that we've done it correctly.

453
00:49:26,550 --> 00:49:31,560
Go ahead, Clara. You told me that that Prestige Dinner had a girlfriend. Something went wrong with that merge.

454
00:49:31,830 --> 00:49:34,830
We got to investigate why that's right or why that's wrong.

455
00:49:35,190 --> 00:49:40,530
One of the two same thing here. We can look at her codebook and we can see a couple of these items.

456
00:49:40,950 --> 00:49:44,460
How much do I agree? I feel safe at school. This tells me a lot of information.

457
00:49:44,700 --> 00:49:48,389
One, how is this scale strongly agreed to?

458
00:49:48,390 --> 00:49:53,940
Strongly agree? We can see the distribution of scores about how people respond to these things, right?

459
00:49:54,450 --> 00:49:59,970
We can now think about what a unit change means thinking in anticipation of the analysis that we're going to run.

460
00:50:00,450 --> 00:50:06,450
Right. So unit change means going from agree to strongly agree. For example, it's going to tell us if anybody didn't respond to these things.

461
00:50:07,020 --> 00:50:14,400
So in this case, a negative nine in the actual code or text was was indicating that somebody didn't respond to that.

462
00:50:15,060 --> 00:50:18,090
We're going to have to start thinking a lot about missing data in this class.

463
00:50:18,350 --> 00:50:21,360
This is one of the reasons why I liked this idea, this data set, because it's not perfect.

464
00:50:21,600 --> 00:50:30,000
And prestige data set is cheats. It cheats because it's all data is perfect for illustration because everything always works out.

465
00:50:30,420 --> 00:50:33,810
And I hate using examples like that because they never work out in practice.

466
00:50:34,170 --> 00:50:38,250
But now in this class, every time you see prestige, you know that you're going to have some nice data.

467
00:50:38,700 --> 00:50:44,360
It's going to be just, Oh, yeah, okay, this. Very, very similar to what we might expect in practice.

468
00:50:44,420 --> 00:50:48,440
Things get messy. Same thing down here. Lots of good information for us.

469
00:50:48,800 --> 00:50:54,770
What I would suggest that we do, and you'll see this in your script file is begin to run little commands like this.

470
00:50:55,250 --> 00:51:02,990
The table commands is just a way for us to use a categorical or grouping variable.

471
00:51:03,350 --> 00:51:06,510
You kind of understand. Now, you might ask yourself already.

472
00:51:06,560 --> 00:51:10,730
Wait a minute. Is this a categorical variable or a numerical variable?

473
00:51:11,090 --> 00:51:14,630
Is this one of four or is this category? That's a great question.

474
00:51:14,640 --> 00:51:20,090
I'm glad you're thinking about that. We'll get to that in just a second. But right now, I'm just going to use what's called the table command.

475
00:51:20,100 --> 00:51:24,830
You can think about just give me some descriptive information. I'm going to use that variable name recalled.

476
00:51:24,840 --> 00:51:30,330
I'm going to index it by by naming the data frame and the dollar sign.

477
00:51:30,680 --> 00:51:37,220
What I'm not going through right now is in the beginning of that file, I installed six packages and loaded them from my library.

478
00:51:37,670 --> 00:51:46,160
I read in the data called S whatever, and then a subset added my data frame to just five items or six items.

479
00:51:47,150 --> 00:51:50,420
I won't go to that right now because we're a little pressed for time to folks have questions.

480
00:51:50,720 --> 00:52:03,270
I also pasted a or sir, I included a when I call our our instant replay or something stupid.

481
00:52:04,340 --> 00:52:11,150
The idea being if you want to see me go through the are studio steps in-depth very slowly and methodically.

482
00:52:11,570 --> 00:52:16,070
Those will be a nice resource for folks to get to this point.

483
00:52:17,450 --> 00:52:25,430
I like to subset data because to me, having all 400 and something variables for every S is overwhelming.

484
00:52:26,450 --> 00:52:33,470
I know in my analysis that I'm might use one, two, three, four, ultimately five variables.

485
00:52:34,160 --> 00:52:38,420
A smaller data frame with just five variables to me feels very manageable.

486
00:52:39,140 --> 00:52:45,770
It also means that I can manipulate that smaller data frame and not have to worry about making up my original dataset.

487
00:52:47,170 --> 00:52:54,300
Okay. So if you see some of that code at the beginning of the file, is this too hard for me to talk about it conceptually and not have it up here?

488
00:52:54,310 --> 00:53:01,410
Do you want to the thank you for. Do I pull it up?

489
00:53:04,320 --> 00:53:07,750
Sir. Good. Good.

490
00:53:08,100 --> 00:53:13,410
Here we go. Restarting our plan so folks can see this.

491
00:53:14,970 --> 00:53:18,990
All right, so here. Lines three through four, and everybody see that?

492
00:53:20,430 --> 00:53:25,410
Three through four. I'm just installing packages. How do I know which packages to install?

493
00:53:26,070 --> 00:53:30,450
We're starting to get familiar with foreign swagger in SPSS Data File Snake.

494
00:53:30,510 --> 00:53:36,300
Has that described function in a few others that I like so much? This is a companion to applied regression.

495
00:53:36,990 --> 00:53:43,230
It just has some nice features that we can use to, um, to view data differently.

496
00:53:43,260 --> 00:53:47,670
I can call it some like a it's, there's like an effects package that it brings in.

497
00:53:47,940 --> 00:53:53,790
So help me think about things after I've run some analysis. I'll introduce all of these things in good time.

498
00:53:54,840 --> 00:54:03,210
Paste This is just some additional dependencies that the car package is going to need.

499
00:54:03,750 --> 00:54:11,700
So I had to do this because when I downloaded this or installed this, it said that this didn't come in multiple comparisons.

500
00:54:11,850 --> 00:54:19,410
So help us with our turnovers once we're done. If you've done this, once installed all these packages, you don't have to do it again.

501
00:54:20,760 --> 00:54:32,160
Nine and ten are just reading things into my library, which I'm not going to do because my RS 15 is where I read in my data frame.

502
00:54:33,320 --> 00:54:39,600
Right. We did this last week. I need to use one of the three or four different ways that we can rename data.

503
00:54:40,170 --> 00:54:44,010
I'm using the read SPSS command because this is a CV file.

504
00:54:45,000 --> 00:54:57,120
I'm specifying the path name value labels equals false is going to read the data in those numbers instead of categorical means.

505
00:54:57,540 --> 00:55:05,660
So when you look at the data that you read in, if I included value labels equals true,

506
00:55:06,360 --> 00:55:10,110
you're going to get things like strongly disagree, disagree, agree.

507
00:55:11,010 --> 00:55:17,160
When I read it in as false, you're going to get numbers like one, two, three and four.

508
00:55:18,630 --> 00:55:25,380
If you read it in as true and you got these in these names, it still keeps that information.

509
00:55:25,380 --> 00:55:29,310
One, two, three, four in the background. It's just not as obvious.

510
00:55:31,760 --> 00:55:35,510
If I have my handy code book, I don't need this stuff.

511
00:55:36,200 --> 00:55:42,140
The labels don't make much of a difference to me, and in fact, I want to make all of these things numeric variables pretty soon.

512
00:55:43,310 --> 00:55:46,880
So I'm content to just read them in as one, two, three, four,

513
00:55:47,360 --> 00:55:55,340
because I know if somebody responds to this question one or whatever as a two, that means they disagree with this statement.

514
00:55:58,860 --> 00:56:03,090
Okay. The line 16 is a.

515
00:56:05,260 --> 00:56:09,580
Did you read them together by 15 and 16 or right around seven?

516
00:56:11,860 --> 00:56:14,980
At first it was written separately and then I tried to find them together.

517
00:56:16,270 --> 00:56:20,200
You should be reading this. This small economy is that it should default to the next line.

518
00:56:20,230 --> 00:56:26,040
This is all 50 and 16 hour one coming together. Great.

519
00:56:26,450 --> 00:56:34,370
I got an error message that said the indicated value was different from an internal value for at least one of the three system values.

520
00:56:34,400 --> 00:56:41,960
Yeah, that's going to happen. And those are just the way that R is looking for the cutoff for like very, very small, very large numbers.

521
00:56:43,530 --> 00:56:46,900
So that one is one of the safety normals and there should be like 34 of them.

522
00:56:48,710 --> 00:56:52,340
There's a number of layers. Okay.

523
00:56:55,110 --> 00:57:01,980
We'll do more of this on Thursday. So don't worry if you're if you're struggling, this piece here is the subsidy piece.

524
00:57:03,570 --> 00:57:08,790
Hopefully you saw a little bit of this in your homework. What I'm doing is creating a new data frame,

525
00:57:09,990 --> 00:57:14,130
and I want to take my big one that I just hopefully read in 34 hours and I'll

526
00:57:15,570 --> 00:57:19,860
take this data friend that I had and just give me a specific set of columns.

527
00:57:21,030 --> 00:57:26,240
You don't have to do this. You could run this whole entire exercise with the big data frame defined.

528
00:57:27,240 --> 00:57:32,400
I like to have a smaller set of variables. It's just for me, it's easier to see.

529
00:57:32,730 --> 00:57:38,520
It's easier for me to work with. So what I'm doing here is I'm saying from the set, this data set.

530
00:57:39,720 --> 00:57:45,510
Give me. That's what these brackets mean. What is that?

531
00:57:45,840 --> 00:57:56,130
Nothing before this column. Bears repeating what some are shouting out of all the rose brackets.

532
00:57:56,370 --> 00:58:02,060
Rose comes, rose comes. Give me all the rose.

533
00:58:02,070 --> 00:58:10,290
Which in our dataframe means what? Every single, every single variable.

534
00:58:11,140 --> 00:58:16,200
Now, every single I know there's one person.

535
00:58:16,530 --> 00:58:18,660
Right. In every row as a person.

536
00:58:19,470 --> 00:58:24,780
Every row is an actual kid up in Flint, Michigan, who is responding to these different variables that create outcomes.

537
00:58:25,300 --> 00:58:28,560
Right. We want everybody. We don't want any of those voices to get lost. Right.

538
00:58:29,220 --> 00:58:34,350
Every single row. Now variables are called names.

539
00:58:35,250 --> 00:58:39,480
And so we have to indicate which of the variables we want to select.

540
00:58:40,140 --> 00:58:46,620
If it's ever more than one. We need to use this combine function or concatenate function, see.

541
00:58:47,160 --> 00:58:54,360
And then within the parentheses we have to name the digits variables individually with quotation marks.

542
00:58:55,880 --> 00:58:59,210
San Francisco. This is good stuff.

543
00:59:00,410 --> 00:59:03,600
I was trying to run some of these things. I'm getting a whole bunch of different.

544
00:59:04,690 --> 00:59:07,990
Just. All right. Let's. I'll see you in about 10 minutes.

545
00:59:08,000 --> 00:59:12,470
We're going to stay five, and I can walk around and help some folks or maybe even start over.

546
00:59:13,430 --> 00:59:16,220
All right. But understand, more than anything,

547
00:59:16,490 --> 00:59:22,760
all I'm doing here is a little bit of free work to get to a nice small data frame that's just going to have

548
00:59:23,030 --> 00:59:29,780
five variables so that I can mess around with it without having to worry about changing my original dataset.

549
00:59:30,860 --> 00:59:34,870
Okay. That's all I'm doing here from 1 to 25.

550
00:59:34,880 --> 00:59:45,860
Just try to get the data. Once I have the data, I want to check to make sure I have the right data.

551
00:59:47,210 --> 00:59:51,260
So I'm using a little too small commands.

552
00:59:51,260 --> 00:59:57,710
I could just I can look at it visually and I can compare what I like to use, for example, this table command.

553
00:59:58,010 --> 01:00:02,060
It's just going to give me the frequencies of this specific variable.

554
01:00:02,120 --> 01:00:05,840
It's ETV now because I created my little small subset.

555
01:00:06,200 --> 01:00:11,390
Right. SH. That's why this is ETV.

556
01:00:12,680 --> 01:00:16,280
This is one of the variables in their right index.

557
01:00:16,280 --> 01:00:23,660
But a little data side. I do want to ask and this is a special command in R and A is not available.

558
01:00:24,200 --> 01:00:31,180
It's the word are uses for missing. If I ran this without this, I would get these values.

559
01:00:31,190 --> 01:00:37,220
One, two, three, four, which is fine. But I do happen to know, based on my code book that there's some missing values.

560
01:00:37,940 --> 01:00:42,170
So I'm going to use this. It's a logical statement here.

561
01:00:42,500 --> 01:00:45,920
Use any equals quotation marks, if any.

562
01:00:46,190 --> 01:00:50,660
If there are any missing values, include them in my output.

563
01:00:51,980 --> 01:01:01,220
All right. And so anyways, or one of those tricky ones and ah, most functions have a command about what to do with missing values.

564
01:01:02,000 --> 01:01:07,700
And the best I can tell you is you kind of have to memorize them or have examples of when you've used them in the past.

565
01:01:08,990 --> 01:01:14,510
But we'll we'll be very conscientious about saying, what do we want to do with missing values?

566
01:01:15,770 --> 01:01:20,000
If this looks like this, I'm good.

567
01:01:20,660 --> 01:01:29,420
I'm happy. I'm ready to move on. If this doesn't look like this, then I got problems and I need to rectify them before I start going off running and.

568
01:01:30,590 --> 01:01:31,430
That's what I got to do.

569
01:01:32,510 --> 01:01:39,980
Now, you thought this was going to be an easy question where we say, give me a grouping variable, I got an outcome variable that's continuous.

570
01:01:40,250 --> 01:01:43,460
Let's find out if there are differences in safety feelings based on someone's race.

571
01:01:43,730 --> 01:01:48,310
In Flint, Michigan, 1984, you thought there was going to be days, but it's not.

572
01:01:48,350 --> 01:01:54,110
All work is difficult. What? But that's what they're going for.

573
01:01:54,110 --> 01:01:58,640
But what about this? Remember my question.

574
01:01:59,870 --> 01:02:05,040
What's wrong here? Do you read those? What's wrong here?

575
01:02:10,360 --> 01:02:14,740
It's my question to strongly agree, man, for each one of these statements.

576
01:02:18,550 --> 01:02:28,960
So if you're strong, right, we're going to ultimately combine these three pieces of information into our dependent variable.

577
01:02:29,560 --> 01:02:36,790
This is the outcome variable. We think group is going to explain variability in our outcome variable to create this.

578
01:02:37,210 --> 01:02:40,240
We want to combine these three items. I'll show you how to do that.

579
01:02:40,870 --> 01:02:46,060
But before we do that, we want to make sure they are all coded in the same direction.

580
01:02:47,200 --> 01:02:51,720
If I say my neighborhood is a very safe place to be, it is a scary.

581
01:02:52,090 --> 01:02:58,210
Yes, very safe. Strongly agree. But then I turn around and say, I'm afraid of violence in my neighborhood.

582
01:02:58,450 --> 01:03:01,689
That is. Yes, strongly to you. Just that I strongly agree.

583
01:03:01,690 --> 01:03:04,900
Excuse me. That's very different. That's conflicting information.

584
01:03:05,620 --> 01:03:10,360
So before we create our scale, we're going to have to think about what to do with this reverse coded idea.

585
01:03:11,080 --> 01:03:15,610
We're going to change the scale or change the coding so they're all moving in the same direction.

586
01:03:16,150 --> 01:03:21,460
Otherwise, we could wash out some of the true variant variability in these responses,

587
01:03:21,910 --> 01:03:27,190
because chances are someone who says yes to this item and yes to this item probably says no to this item.

588
01:03:27,880 --> 01:03:33,850
That's what we want to watch out for. Or just as we start to really get into our own kind of data management,

589
01:03:34,390 --> 01:03:42,310
if you're seeing that people kind of across the board are saying, yes, yes, yes, chances are people aren't reading this item very well.

590
01:03:42,910 --> 01:03:46,090
Right. They're just five, five, five, five. Yeah, I got it.

591
01:03:46,490 --> 01:03:56,620
All right. So we're looking out for even some of those potential things here, too. But we can run some some analyzes to see if that's true.

592
01:03:58,060 --> 01:04:03,720
Let's just try to get the values in three different ways.

593
01:04:03,740 --> 01:04:10,000
Reading the data. This is my happier, smaller data frame.

594
01:04:10,510 --> 01:04:14,530
This is all trying to get to six variables, not four and 50.

595
01:04:15,370 --> 01:04:23,290
The reason why these are numbers and not strongly agree disagree member this means strongly disagree, strongly disagree.

596
01:04:23,800 --> 01:04:28,060
This is where again, knowing your data is great, having a code book is essential.

597
01:04:28,570 --> 01:04:34,090
If you forget all this information, then you don't want to just ignore the values.

598
01:04:34,840 --> 01:04:38,140
But it's going to save us a couple of steps if we can just read this in as numbers.

599
01:04:39,220 --> 01:04:46,840
But here's my data. This is a correlation matrix between the six items in my dataset.

600
01:04:47,680 --> 01:04:53,500
I did that by using this core command that we saw last week, the name of my data frame.

601
01:04:54,820 --> 01:04:59,080
I had to use this command right here that says complete observations.

602
01:04:59,920 --> 01:05:05,260
The reason why, if I tried to run a correlation on a data frame that has six items,

603
01:05:05,440 --> 01:05:08,710
but some of those items have missing values, it's going to return here.

604
01:05:09,490 --> 01:05:15,370
So this is our second instance already where we have to be conscious of our missing values.

605
01:05:16,180 --> 01:05:21,220
So what I'm telling it to do is to use complete observations when in doubt.

606
01:05:21,850 --> 01:05:25,690
Just throw this into your code for the correlation command.

607
01:05:26,320 --> 01:05:29,320
So if you're not sure if there are any missing values, which ideally you should worry about,

608
01:05:29,860 --> 01:05:34,780
but if you're not sure or if you get an error, it's very likely because there are some missing values.

609
01:05:36,220 --> 01:05:40,810
Our does not know how to create a secondary correlation coefficient if there's no value there.

610
01:05:44,020 --> 01:05:47,140
So I do this and it returns something like this.

611
01:05:48,040 --> 01:05:54,700
I want you to ignore this race variable because this is just kind of a practical thing that we have to deal with.

612
01:05:55,120 --> 01:06:00,130
This is supposed to be a grouping variable that's mutually exclusive and exhaustive.

613
01:06:01,780 --> 01:06:06,490
But this was read in as one, two and three.

614
01:06:07,900 --> 01:06:11,590
Just like these were supposed to be strongly disagree, agree, blah, blah, blah.

615
01:06:12,370 --> 01:06:18,610
But they were read in as one, two, three, four. I'm sorry, this seems pedantic, but this is important stuff.

616
01:06:19,660 --> 01:06:24,910
It makes no sense. Decoder race variable as one, two, three and four.

617
01:06:26,510 --> 01:06:31,060
Right. Any grouping variable that's just buckets with no one here in order.

618
01:06:31,570 --> 01:06:36,190
We don't want to fall in the trap of treating like a continuous or numerical variable.

619
01:06:37,630 --> 01:06:44,200
So even though this is part of the correlation cofactor output, this is nonsense.

620
01:06:44,470 --> 01:06:49,090
Ignore this count. And if you didn't know that or if somebody looks at it,

621
01:06:49,540 --> 01:06:57,490
we want to make sure that people aren't inferring incorrectly that as you go up the race values, this is what the correlation would be, right?

622
01:06:58,240 --> 01:07:04,930
Small things that we're doing before we ever get to that, unless we actually run the analysis for this promise will take you point 8 seconds.

623
01:07:05,920 --> 01:07:16,750
But these are important values. This is the correlation between item and item beat item.

624
01:07:16,780 --> 01:07:28,089
And I see de item A and C, similarly, B and a, B and D, B and C,

625
01:07:28,090 --> 01:07:35,049
I don't know why it is right or a correlation matrix is always going to have a diagonal.

626
01:07:35,050 --> 01:07:39,340
That's one because these are the variables, correlations with himself perfectly correlated.

627
01:07:40,570 --> 01:07:48,160
They are inverted in the sense that this is just the mirror image on top of the top diagonal and bottom diagonal m your energies.

628
01:07:48,790 --> 01:07:53,739
So all of this information above the ones is redundant with the information below the ones.

629
01:07:53,740 --> 01:08:04,390
If you haven't seen this before with me, these are all kind of Pearson correlations, so a zero essentially means no correlation.

630
01:08:05,470 --> 01:08:12,130
Now point to small correlation point four, bigger point seven, large structure, right?

631
01:08:12,550 --> 01:08:18,250
So we can see that most of these are relatively small item D,

632
01:08:18,250 --> 01:08:26,740
which was our problem item is noticeably negatively correlated with the other two items in our scale, right?

633
01:08:27,610 --> 01:08:32,020
B and A and C for that matter are all positively correlated.

634
01:08:33,280 --> 01:08:39,610
D, on the other hand, is negative, negatively correlated, or there's this one that's no correlation whatsoever.

635
01:08:41,080 --> 01:08:44,590
Jane, what's the 64 item on your flip?

636
01:08:44,590 --> 01:08:53,030
In the answer, the question of getting that random when you think about it, I mean, there's no there's not there's five things that.

637
01:08:56,480 --> 01:09:04,160
So then as I think about this as a small do forgive for me but it doesn't make a difference what

638
01:09:04,160 --> 01:09:08,750
this six was because I thought this is actually a five skip so far as we could change this to five.

639
01:09:09,530 --> 01:09:13,400
But on your typical five item scale.

640
01:09:20,080 --> 01:09:25,330
One, two, three, four, five. Strongly disagree.

641
01:09:25,810 --> 01:09:33,040
Disagree. Neutral. Read my great works of art.

642
01:09:34,150 --> 01:09:42,940
So now we know that even though this is what those people responded to or some version of this in the background are coded these as a one,

643
01:09:42,940 --> 01:09:46,720
two, three, four or five. We saw one in the output.

644
01:09:47,020 --> 01:09:52,660
It means this is not for the output. It means this. If I want a reverse code something.

645
01:09:52,960 --> 01:09:56,380
There are multiple ways to do that. There's a recode function.

646
01:09:56,890 --> 01:10:05,740
You can go in and manually change all those values, or you can transform the variable with sometimes just a small calculation.

647
01:10:07,270 --> 01:10:16,180
A little trickier as you can take each variable and put a six in front of it.

648
01:10:21,550 --> 01:10:31,100
And subtract the values. Six miners, one six months to six months.

649
01:10:31,110 --> 01:10:37,840
Three months for six miners. Five. Nice for.

650
01:10:41,890 --> 01:10:45,310
This is a quick and easy way to recode a variable.

651
01:10:46,420 --> 01:10:57,460
So what I've done here, and I'm taking my data frame and indexing it with, in this case, the same variable name.

652
01:10:58,090 --> 01:11:06,610
If you wanted to be more careful, you can use the same variable name and it will ignore R to indicate that you are recording the variable.

653
01:11:07,870 --> 01:11:12,190
I am saving everything to the right as this variable name.

654
01:11:13,330 --> 01:11:24,340
What I'm saving is the value six minus the scores for this specific value or very I'm essentially doing this.

655
01:11:25,810 --> 01:11:33,930
Now there are only actually four variables, so that shouldn't matter because it's one, two, three, four as strongly as is.

656
01:11:33,940 --> 01:11:37,660
Strongly disagree. Disagree, agree. Strongly agree.

657
01:11:38,110 --> 01:11:43,120
Still taking. I would probably do this as a five now because there are only four categories.

658
01:11:43,540 --> 01:11:51,560
But this is a linear transformation. It doesn't matter. So if you haven't done this already, I would change this six to a five.

659
01:11:52,840 --> 01:11:56,440
But you can use pretty much any number that's larger than it was.

660
01:11:57,610 --> 01:12:00,780
So this is important to understand.

661
01:12:02,290 --> 01:12:07,720
Let's go ahead and do five, five, five, five, five.

662
01:12:08,410 --> 01:12:14,030
So what it's going to do is it's going to take the value five and subtract the response that people gave.

663
01:12:15,070 --> 01:12:25,300
And you're going to end up with just like up here for 3 to 1, you are recoding that variable.

664
01:12:26,440 --> 01:12:31,240
You are changing nothing inherently about the variable except for the scale.

665
01:12:32,590 --> 01:12:39,820
So all that happens here. And even when I made my goof, all that happened here, as all these signs flipped,

666
01:12:40,510 --> 01:12:44,780
instead of being negatively correlated, they're now all positively correlated.

667
01:12:45,390 --> 01:12:53,380
They're all moving in the same direction. We are now ready to create or scale questions about this.

668
01:12:54,370 --> 01:12:59,960
I do apologize for this move. I don't want you to correct her right now.

669
01:13:01,970 --> 01:13:08,209
Yeah. So just to confirm, it can be any number instead of six as long as it's larger, essentially.

670
01:13:08,210 --> 01:13:12,050
Yeah, because it's it's just flipping the it's flipping the number.

671
01:13:12,050 --> 01:13:20,790
So if this was six, this would have been what? Five, four, three, two.

672
01:13:21,500 --> 01:13:25,800
Okay. And still, that's what I did. You should get the same exact thing.

673
01:13:27,000 --> 01:13:30,750
All you're doing is flipping the scale and we are recoding the variable.

674
01:13:31,260 --> 01:13:34,169
And again, that's an easy way to do it, a fast way to do it.

675
01:13:34,170 --> 01:13:41,310
But you could also use that as a Recode function or actually a couple of different kind of recode like functions in are that we can use.

676
01:13:46,520 --> 01:13:51,050
So here it this again. That's what this line is here. It's telling us now that everything's going in the right direction.

677
01:13:52,440 --> 01:13:55,729
You're still not quite there yet. Just a quick question.

678
01:13:55,730 --> 01:14:02,490
Is this more about the class in general? If we do our homework and we maybe run into an issue and we, you know,

679
01:14:02,510 --> 01:14:07,280
are you too in the air is informed through whatever are you find that that's using different types of code?

680
01:14:07,310 --> 01:14:10,879
Yes. Variations in class. Okay, fine.

681
01:14:10,880 --> 01:14:12,260
Fine. One speaks to you. Serious?

682
01:14:12,810 --> 01:14:18,500
That's I mean, that's one great thing about kind of open source is we're looking for the kinds of codes, packages that you like.

683
01:14:18,890 --> 01:14:22,190
And if you do it in a way that gets to, that'd be a pretty appropriate answer.

684
01:14:22,430 --> 01:14:26,950
I don't care about the packaging. Right.

685
01:14:29,000 --> 01:14:38,870
Case in point here. This is just the most expedient way to create a meme, or at least it's a very familiar way for us to create a mean.

686
01:14:39,380 --> 01:14:41,750
This would not be the way that we'll do this moving forward.

687
01:14:41,750 --> 01:14:46,280
We can use like a Ramirez command or a couple of other different coding things that we can do.

688
01:14:46,640 --> 01:14:52,190
But if we wanted to just create a mean, we could add these three values up and divide by three.

689
01:14:53,390 --> 01:14:59,060
We're ignoring for just this one time the fact that there could be missing values that we might have to contend for,

690
01:15:00,650 --> 01:15:04,820
but for just operational purposes,

691
01:15:05,050 --> 01:15:12,770
know that we can use simple arithmetic or we can use normal mathematic functions within our to create a brand new variable.

692
01:15:13,640 --> 01:15:19,190
And that's what's going on. I'm using my dataframe. I'm saying give me a new variable name.

693
01:15:19,400 --> 01:15:27,480
This is what I'm calling it. We want exposure, violence. And that's going to be comprised of these three values, including the new recoded values.

694
01:15:29,450 --> 01:15:33,650
I feel safe in my neighborhood. I feel safe at school. I'm afraid of violence in my neighborhood.

695
01:15:35,300 --> 01:15:40,910
All right. We're combining that information together to give us a sense of kind of your general levels of exposure.

696
01:15:41,960 --> 01:15:47,150
I can use the same commands we did last week to give me a sense of what's the distribution of my variable,

697
01:15:48,170 --> 01:15:51,440
what are this is going to be kind of written or similar information. What's it mean?

698
01:15:51,440 --> 01:15:57,950
What's a standard deviation with its range? What's what's the skewness and keratosis of that variable?

699
01:15:58,220 --> 01:16:01,250
This is my opener is what we care about.

700
01:16:03,350 --> 01:16:08,870
Now grouping variable again is going to be their race variable. Here's the problem with the way that we write in that data.

701
01:16:09,650 --> 01:16:19,610
If you run the structure command of our little object, you're going to see that this race variable was included as a numeric variable.

702
01:16:20,030 --> 01:16:28,490
It has levels one, two and three in order to use some of the dummy coding that we're going to want to do and to better represent reality,

703
01:16:29,080 --> 01:16:33,830
you need to change this variable to what are calls a factor variable.

704
01:16:34,340 --> 01:16:40,120
That's a categorical variable. This is all right.

705
01:16:42,350 --> 01:16:51,470
This is going to be important for us knowing what variables are categories and what variables are numbers to make this a factor variable.

706
01:16:52,490 --> 01:17:01,460
We can use the as period factor function and we just say what variable it is we want it to convert to effect.

707
01:17:02,630 --> 01:17:12,040
So instead of thinking of race as a 1 to 3 numeric variable, we are going to think of race as a 1 to 3 category variable with no order.

708
01:17:13,640 --> 01:17:18,980
If we want to know what, who's a one and who's a to lose a three, we go back in our codebook.

709
01:17:19,520 --> 01:17:23,330
In this case, it's black, African-American, white, Caucasian, mixed race.

710
01:17:24,320 --> 01:17:33,850
And. We can confirm after we've run this as factor command that this is, in fact, effective.

711
01:17:34,360 --> 01:17:38,830
This is a logical question. We say this factor. I interpret this as question.

712
01:17:38,920 --> 01:17:42,130
It's going to give you a true or false. This will give you a true.

713
01:17:42,940 --> 01:17:46,080
And now we're ready to go. We table this. This is just frequencies.

714
01:17:46,090 --> 01:17:51,540
It's going to tell us there's 681 black, African-American, 100 and something white, Caucasian.

715
01:17:51,580 --> 01:18:05,590
I forget how many measurements, all this work, all this work, all of this work describing this is our dataframe.

716
01:18:06,730 --> 01:18:11,530
This is a handy little thing from the safe package. This might be our first step towards an ANOVA.

717
01:18:12,190 --> 01:18:19,150
It's the described by a command. Give me my outcome variable by race.

718
01:18:20,560 --> 01:18:24,250
What that's going to do is take the same information that we get from the described command,

719
01:18:24,640 --> 01:18:28,840
but it's going to break it down for us by group, one group to three.

720
01:18:29,500 --> 01:18:36,520
African-American, black, white, Caucasian. Mixed race. Same exact information as you see up here.

721
01:18:36,850 --> 01:18:45,880
But now we're taking the sub samples so that using the mean the mean of the three.

722
01:18:47,260 --> 01:18:53,319
Yeah. Because that's what we created. Right. This variable, this one right here, we want ETV, we want exposure.

723
01:18:53,320 --> 01:18:57,280
Bias. That's the one we create. Here it is up here.

724
01:18:57,610 --> 01:19:01,450
Brand new variable. Never existed before. This room never existed.

725
01:19:02,080 --> 01:19:07,420
And now it does tells us a little bit of something about what some folks were exposed to in 1994,

726
01:19:09,220 --> 01:19:15,070
and specifically which groups had the highest levels of exposure. We have the safest or at least safe in our neighborhoods.

727
01:19:16,180 --> 01:19:19,630
And so you go back to our code looking to see what our items are, kind of indicating,

728
01:19:19,900 --> 01:19:26,050
which is a green statement versus a disagree statement need for these sorts of responses.

729
01:19:26,140 --> 01:19:31,600
And I know we're reckless folks. I know we're real close. But I just had do we had to get here?

730
01:19:31,990 --> 01:19:46,070
We absolutely had to get here. All that work all that work for a very simple line of code, which is the best part and this is the fun part is random.

731
01:19:46,090 --> 01:20:00,920
And whatever you want to call an arbitrary name for object A, O B stands for another analysis, it earns it.

732
01:20:01,150 --> 01:20:10,630
And so we have our outcome variable, we have our grouping variable, don't mix them up.

733
01:20:11,380 --> 01:20:15,910
It's not going to like it. It's expecting a factor here.

734
01:20:16,480 --> 01:20:25,840
What is the factor? That's not categorical because this is not a categorical variable.

735
01:20:25,900 --> 01:20:31,900
You will get an error or a T test and you don't have a category of categorical variable ruby variable generic.

736
01:20:33,610 --> 01:20:39,430
This is going to be a continuous variable categorical variable data set that we want to use.

737
01:20:41,530 --> 01:20:47,640
If you run this line, you're not going to get anything except for just a reprint of it all.

738
01:20:47,650 --> 01:20:56,560
That information is saved in this object. Column number one, we are finally, finally ready to run this summary of this information.

739
01:20:57,130 --> 01:21:02,260
And we're going to get something like this. This is what you spend 45 minutes watching over the weekend.

740
01:21:02,620 --> 01:21:12,340
All right. This is that question about grouping a group of variables and between group variability relative to an indoor variable.

741
01:21:12,760 --> 01:21:20,440
At the end of the day, at the end of the day, those groups matter in terms of feeling safe in your school neighborhoods.

742
01:21:20,800 --> 01:21:26,310
You can tell me whether or not group matters. Cory.

743
01:21:26,350 --> 01:21:30,370
I love. I love this. Oh, somebody else? No, you just.

744
01:21:30,370 --> 01:21:35,240
You got a little bit. Well, tell me.

745
01:21:35,460 --> 01:21:43,300
It's happening more and more than this. So the pressure are the p values.

746
01:21:44,590 --> 01:21:47,830
Everyone loves this is that. I'm just going to suppress the feedback.

747
01:21:48,150 --> 01:21:56,170
Yeah. For you, I guess the question I have, I know that if it's not close to zero, right, you can get the.

748
01:21:56,170 --> 01:22:02,930
No, but how far away does and how large does that a value need to be to shelter?

749
01:22:04,030 --> 01:22:07,690
I love that question. I think you and I appreciate that question.

750
01:22:08,020 --> 01:22:12,129
That is a so 1950s like that. Check the back of the textbook question.

751
01:22:12,130 --> 01:22:16,340
But yes, this is the comparison to some critical value table, right.

752
01:22:16,360 --> 01:22:22,239
You remember those from textbooks and I want you look in the back you CDF table and you got to go find the degrees of freedom,

753
01:22:22,240 --> 01:22:28,030
you got to find residuals, right? And then you compare whether or not this is good or this is to be or whatever.

754
01:22:28,450 --> 01:22:35,170
Ah, takes all the fun out of that. It takes all the, the thinking out of it, which is in some ways a very wonderful thing.

755
01:22:35,410 --> 01:22:45,310
But you're absolutely right, this f value is so large relative to what we would expect from a distribution of s of F,

756
01:22:45,940 --> 01:22:51,220
from an F table with degrees of freedom to an 842.

757
01:22:52,060 --> 01:22:56,920
I got this because we had three groups. So it's the number of groups minus one.

758
01:22:58,240 --> 01:23:02,410
And then this is going to be the number of observations minus the number of groups.

759
01:23:02,860 --> 01:23:08,350
So we probably have the missing values here, minus three to get to even 40 to this,

760
01:23:09,160 --> 01:23:13,149
there is a specific F distribution that has two degrees of freedom from the grouping

761
01:23:13,150 --> 01:23:19,090
perspective and probably more than like 500 from a residual or within person perspective.

762
01:23:20,050 --> 01:23:26,290
We compare the F value that we observe relative to the values that we might

763
01:23:26,290 --> 01:23:30,310
expect under the null hypothesis that there are no differences between groups.

764
01:23:31,240 --> 01:23:40,540
Right? We got this f value by comparing that between group variability relative to the within group variability.

765
01:23:41,980 --> 01:23:48,880
This ratio is quite large relative to the within group variability.

766
01:23:49,300 --> 01:23:53,230
If you can visualize those graphs, this was hey,

767
01:23:54,070 --> 01:24:01,060
all the points were clustered around the group means they were not flying out for a whole bunch of overlap.

768
01:24:01,750 --> 01:24:10,930
What this tells me is it would be very helpful for me to know what race you are if I'm going to understand your exposure to violence,

769
01:24:11,680 --> 01:24:17,560
it's a predictor of exposure to violence. Now, which races matter?

770
01:24:18,400 --> 01:24:21,640
We'll talk about the numbers. But we may, folks.

771
01:24:21,730 --> 01:24:26,560
We may we may let this wash over you.

772
01:24:26,770 --> 01:24:30,820
Thank you for asking these questions. I know it feels like we're moving right on.

773
01:24:30,860 --> 01:24:34,810
What kind of getting we're getting? They're getting they're getting there. We will take off, I promise.

774
01:24:35,350 --> 01:24:38,950
But this is such good, foundational stuff. What's wrong?

775
01:24:39,580 --> 01:24:44,110
What's wrong? Okay, this is such good foundational stuff.

776
01:24:45,100 --> 01:24:49,540
We will do this as a group on Thursday. So I want to make sure that, you know, somebody cares.

777
01:24:49,540 --> 01:24:57,400
If only for 5 minutes, we'll spend the first half getting you to this stage and then we'll use the second half to go through like the first talks.

778
01:24:57,410 --> 01:25:04,570
And I know, but I want to make sure everybody gets to take to where I was explaining that, yes, in fact, group does matter.

779
01:25:05,040 --> 01:25:09,640
And now all of them will kind of delve into in what ways do they matter and working on those.

780
01:25:09,880 --> 01:25:19,120
Thank you, everybody. I hope you have a great day. We will see you on Thursday night.

781
01:25:19,630 --> 01:25:23,160
Hi, Richard Sutton.

782
01:25:24,040 --> 01:25:30,190
Yes. Yeah, I'm sorry I didn't get your email. So, yeah, I'm here. And then also this this review slide that you went over.

783
01:25:30,460 --> 01:25:33,850
If it's not the one, is it, that wasn't posted. Oh yeah. The firm just today.

784
01:25:34,000 --> 01:25:38,050
Yeah. Yeah, I'll post it, I call my in-class slide so I'll put in different values.

785
01:25:38,290 --> 01:25:42,700
Okay. We're going to show you this because I think we figured it out. Yeah, it happened here.

786
01:25:42,820 --> 01:25:47,080
Let me just. So we don't record you fumbling through.

