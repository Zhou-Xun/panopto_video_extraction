1
00:00:00,900 --> 00:00:06,660
So as I mentioned, this homework was, uh, it's supposed to be a warm up.

2
00:00:06,660 --> 00:00:14,460
And the data that you have started looking at, I mean, we will actually that stroke data,

3
00:00:15,180 --> 00:00:23,220
we will use it throughout the semester, you know, sort of different parts of the dataset.

4
00:00:24,450 --> 00:00:32,249
So that's why this is more of a warm up for you to get familiar with the data, with the variables.

5
00:00:32,250 --> 00:00:38,520
And we gave you the codes because at this point I know there are maybe, you know,

6
00:00:40,260 --> 00:00:52,500
one or two people in the class who are sort of in terms of either like any coding in any any statistical software,

7
00:00:53,160 --> 00:00:56,760
you are sort of kind of learning or catching up.

8
00:00:57,150 --> 00:01:05,070
So that's why we gave you the codes. But what your task will be is even if you've seen several, you know,

9
00:01:05,520 --> 00:01:20,620
she has given us kind of everything but your job to sort of run the codes, get the results and answer the questions in the home bar.

10
00:01:20,700 --> 00:01:24,149
So like pleasing to create your answers, for example, you know,

11
00:01:24,150 --> 00:01:36,750
there are lots of ways that I ve asked you like what what is like give us the size of the concrete case data.

12
00:01:36,780 --> 00:01:40,950
So like, you know, how many observations, how many participants have they done?

13
00:01:40,950 --> 00:01:48,330
All the modules. Then we ask you to sort of record some of the variables into categories.

14
00:01:49,380 --> 00:01:56,730
Then there is one thought where we are asking you to do like some basic tasks like, you know,

15
00:01:57,300 --> 00:02:06,780
sort of quantity means and the things that you have done already or you know, sort of like all or like we have even the course.

16
00:02:07,800 --> 00:02:17,520
As I said, this is more like a warm up to familiarize you with this particular data set that we've been used to over the semester.

17
00:02:18,240 --> 00:02:26,850
And you can replicate the boards, you can use the same boards, or you can build your own board.

18
00:02:26,850 --> 00:02:35,700
And the main goal of this homework is to sort of what if the boy would answer the simple set of questions and then,

19
00:02:36,750 --> 00:02:45,239
you know, we ask you to kind of give us some, you know, some tables with descriptive statistics.

20
00:02:45,240 --> 00:02:55,740
So so all of this is it doesn't really use anything that we have talked about and new stuff in this class yet.

21
00:02:56,250 --> 00:03:01,500
And the homework that you are going to get today will actually start asking

22
00:03:01,500 --> 00:03:06,809
you about things that you know you have been learning for the past two weeks.

23
00:03:06,810 --> 00:03:15,570
So like we asked. To do stuff on your own using this, you know, data set.

24
00:03:16,410 --> 00:03:22,100
But based on what we have learned, I think this is so far so.

25
00:03:24,120 --> 00:03:33,720
So I just wanted to mention, because I think two or three people wrote to me saying that Apple doesn't work at home and.

26
00:03:35,100 --> 00:03:39,889
And I think some of that I just wanted to clarify.

27
00:03:39,890 --> 00:03:46,420
I mean, I know I mentioned this last week that this is the warm up homework and I didn't

28
00:03:46,420 --> 00:03:54,440
want to elaborate on this much other than saying that you wouldn't get used to it.

29
00:03:54,890 --> 00:03:58,070
So. So any other questions?

30
00:04:02,230 --> 00:04:08,470
Okay. So any questions related to the materials we talked about last week?

31
00:04:16,180 --> 00:04:19,770
So what is the piece? Fine. No, I don't.

32
00:04:19,960 --> 00:04:23,690
And I kind of. Bonehead,

33
00:04:23,860 --> 00:04:32,050
I say something too fast and slow because we are now slowly getting into dating

34
00:04:32,230 --> 00:04:40,630
that we get into kind of build on the things that we have already talked about.

35
00:04:41,320 --> 00:04:45,640
And the piece may be done faster.

36
00:04:45,670 --> 00:04:50,140
So I just want to make sure that you all are sort of.

37
00:04:52,050 --> 00:05:02,130
Okay. One one thing that so we are going to talk about a hypothesis testing in simple linear regression today.

38
00:05:02,700 --> 00:05:13,439
But before that, I thought what I would do is like I'm going to take maybe five, at least 10 minutes, not 10 minutes,

39
00:05:13,440 --> 00:05:27,090
5 to 7 minutes to mention a couple other points about the example we did on Thursday with the with the sleep study.

40
00:05:27,930 --> 00:05:38,640
Remember, this is the this is a very small example that we did at the end of a module B last Thursday.

41
00:05:39,510 --> 00:05:45,419
And so I wanted to mention a couple things.

42
00:05:45,420 --> 00:05:49,460
I know that we didn't have a whole lot of time.

43
00:05:49,470 --> 00:05:55,230
So although this is, like I said, this is a very, very small dataset,

44
00:05:55,230 --> 00:06:06,480
a very simple example, but just two points that I am going to mention and reiterate.

45
00:06:06,810 --> 00:06:14,940
So this is the study that was used to examine the relationship between a child's age and the amount the child sleeps per night.

46
00:06:15,300 --> 00:06:21,300
And the it's a small dataset size of 13.

47
00:06:22,150 --> 00:06:30,570
Mm hmm. The Y variable is the average number of minutes slept per night over a one week period.

48
00:06:31,650 --> 00:06:35,220
And the X variable, the.

49
00:06:35,490 --> 00:06:41,880
This is a simple linear regression. So there is only one called the x and that is age.

50
00:06:42,090 --> 00:06:47,520
So no. And obedient and the x variable is it.

51
00:06:47,880 --> 00:07:06,930
Okay. So what we did is we first ran a bunch of descriptive univariate statistics and I told you that basically, you know, I pointed out these.

52
00:07:11,020 --> 00:07:20,259
These descriptive statistics. This is the sample size and these are the mean so mean age of the 13 children in the

53
00:07:20,260 --> 00:07:31,000
study was 9.5 months and the standard deviation was 2177 and the minimum was 4.4.

54
00:07:31,840 --> 00:07:44,559
The average sleep, the mean of the, you know, average sleep bum in minutes over a one week period was 519.3,

55
00:07:44,560 --> 00:07:49,900
the standard deviation was 40.9 and the minimum was four 61.75.

56
00:07:49,900 --> 00:07:53,890
So we talked about how did we get this 2.7 and seven.

57
00:07:54,340 --> 00:08:03,550
This is the sample standard deviation of eight. So 2.77 and I think we talked about this, but I'm just going delighted one more time.

58
00:08:03,880 --> 00:08:13,660
The 2.77 where is that is is X over and minus one and is 13.

59
00:08:13,660 --> 00:08:19,090
So 13 minus one. So this is the sample standard deviation of eight.

60
00:08:19,420 --> 00:08:27,770
And what is this is X is a six is basically you will take all the age values X over video.

61
00:08:27,880 --> 00:08:34,060
This is it. So you take the each of the 13 age values.

62
00:08:34,090 --> 00:08:46,450
So extract the mean age which is 9.25 and square and sum these up.

63
00:08:46,450 --> 00:08:50,260
So that says the six them. So that's by definition.

64
00:08:52,300 --> 00:08:57,580
So that's the sample for 2.77 is the sample standard deviation.

65
00:08:59,530 --> 00:09:10,269
Now then and this is, you know, kind of the next slide shows the hand calculation in hard to compute this and

66
00:09:10,270 --> 00:09:16,690
similarly you can calculate assess why it is the well then why is the average sleep

67
00:09:16,690 --> 00:09:23,950
in minutes what I want to be the period and then and divide this is why you buy

68
00:09:23,950 --> 00:09:33,580
in minus one a to get the sample standard deviation for average sleep time.

69
00:09:34,630 --> 00:09:38,110
Then we have a scatter plot of age versus sleep. And you know,

70
00:09:38,110 --> 00:09:45,429
we kind of looked at what what is the sort of the association between the sleep

71
00:09:45,430 --> 00:09:51,219
demand it and from this scatterplot it is like the only thing that that at this

72
00:09:51,220 --> 00:09:57,040
point I'm not keeping the best line but it is quite apparent that the Reserve Bank

73
00:09:57,040 --> 00:10:02,500
for negative association light as it increases the average sleep then decreases.

74
00:10:02,740 --> 00:10:07,060
So let's kind of settle on that at this point.

75
00:10:07,660 --> 00:10:16,840
And then we sort of fitted a simple linear regression model using the non alien

76
00:10:16,840 --> 00:10:25,629
function in R and this is where I see sort of I probably talked too fast,

77
00:10:25,630 --> 00:10:29,560
so I just wanted to mention this one more time.

78
00:10:30,010 --> 00:10:36,190
So I pointed out the the coefficients.

79
00:10:36,190 --> 00:10:43,899
But let me just write out the one thing that we did not talk about is the interpretation,

80
00:10:43,900 --> 00:10:54,190
and I would kind of solicit it from the class, but let's first sort of circle this.

81
00:10:54,790 --> 00:10:58,800
So this 646 148 is might be done on.

82
00:10:59,320 --> 00:11:06,640
So this is the least squares estimate to estimate here because now we have plugged in values and we get a numerical value.

83
00:11:07,000 --> 00:11:10,900
So this is the least squares estimate for the intercept.

84
00:11:11,830 --> 00:11:18,430
Okay. And -14.24 is the least squares estimate for the slope.

85
00:11:20,380 --> 00:11:23,920
Okay. What about the standard error.

86
00:11:29,390 --> 00:11:36,920
So the standard errors are basically the square root of the variance of beta, not per.

87
00:11:40,640 --> 00:11:44,840
And the square root of the variance of beta one had.

88
00:11:49,220 --> 00:11:54,200
Okay. So we are in module B like in B,

89
00:11:54,200 --> 00:12:06,650
actually calculated variants of beta knockout and variants of beta one have right variants of beta one had was sigma squared divided by six.

90
00:12:08,780 --> 00:12:15,930
Yes. And variance of beta not had was sigma squared nine swan over n plus x y squared over six.

91
00:12:17,000 --> 00:12:23,800
And then we said, well, you know, those are the variance formula for these estimate, but they,

92
00:12:24,190 --> 00:12:34,790
they involve a C plus square, which is the error variance of variance of the unknown random error.

93
00:12:34,910 --> 00:12:39,470
The Epsilon and sigma squared is typically unknown.

94
00:12:40,370 --> 00:12:49,970
So we talked about in the estimate that Four Sigma squared using the residuals and we estimated it using MSP.

95
00:12:50,780 --> 00:13:05,509
So once we plugging the MSE into that, those variance expressions and then big the square root, we get this numerical estimates of the standard error.

96
00:13:05,510 --> 00:13:17,900
So 12.91 is the standard error for be done on pad or in other words the square root of the estimated variance of beta not part.

97
00:13:18,290 --> 00:13:27,980
And 1.36 is the standard beta of beta one hat or in other words, the square root of the estimated gradients will be the one hat.

98
00:13:28,370 --> 00:13:39,490
So this column I forgot to sort of highlight on Thursday at this point and we will not worry about the p value and the last column,

99
00:13:39,500 --> 00:13:42,320
which is the P value because we haven't to about that to date.

100
00:13:43,370 --> 00:13:58,159
And then the other thing that I had highlighted is this residual standard error 13.15 on the on the 11 degrees of freedom.

101
00:13:58,160 --> 00:14:04,610
And what is this? This is basically my estimate of sigma squared.

102
00:14:04,970 --> 00:14:07,580
And where do I get that 11 degrees of freedom?

103
00:14:07,580 --> 00:14:17,720
Remember, we talked about the degrees of freedom and I said be formally it using distribution theory, but at this point resorting to heuristics.

104
00:14:18,440 --> 00:14:23,780
Basically the degrees of freedom mean the total number of observations minus

105
00:14:23,780 --> 00:14:28,280
the number of parameters we have estimated in that linear regression model,

106
00:14:28,490 --> 00:14:33,770
which is two corresponding to beta not and beta one the intercept that slope.

107
00:14:34,070 --> 00:14:45,140
So a minus two or 13 minus two gives me that 11 degrees of freedom and that's my estimated time.

108
00:14:45,740 --> 00:14:50,090
So that's one estimate off of the of the signal squared.

109
00:14:50,240 --> 00:14:59,150
So what I wanted to I mean, you know, we kind of talked about this, but I forgot to highlight the standard error last time.

110
00:14:59,450 --> 00:15:03,200
One other thing, and this is where I would solicit your input.

111
00:15:03,200 --> 00:15:14,740
Yes. Is that there is no difference between the extended release and the squared variances.

112
00:15:14,750 --> 00:15:17,900
The squared of the standard in this is the.

113
00:15:20,010 --> 00:15:24,930
Estimate. Exactly. Yeah. So.

114
00:15:26,400 --> 00:15:29,920
So now I would like to solicit your.

115
00:15:30,690 --> 00:15:40,080
Oh, and then the other two rows, the multiple R-squared statistic once again, you know, we've talked about those today.

116
00:15:40,110 --> 00:15:44,190
I'm not today on Thursday. So for now, just ignore those.

117
00:15:44,820 --> 00:15:49,110
So now I'm going to solicit from the class.

118
00:15:50,700 --> 00:16:00,680
Can you looks like can you tell me in English what does that let's let's talk about the slope estimate.

119
00:16:00,690 --> 00:16:11,550
So the -14.04 can you give me an interpretation of that -14.04 and please remember to specify your units.

120
00:16:12,810 --> 00:16:25,980
So who wants to? So remember the scatterplot that we have shown.

121
00:16:27,960 --> 00:16:31,560
I had said that, you know, like at this point eyeballed the scatterplot.

122
00:16:31,560 --> 00:16:37,469
And what we can see is that there's a negative association ad that increases the

123
00:16:37,470 --> 00:16:45,810
average time decreases because visually and this is kind of what is going on,

124
00:16:46,770 --> 00:16:54,450
but we haven't estimated the best feed line using these squared.

125
00:16:54,450 --> 00:17:03,660
So that that's what we did in the next slide. So we now we have an equation for the best fixed line using the method of this quest.

126
00:17:03,930 --> 00:17:08,910
So now tell me, what does that -14.4, what is the interpretation of that?

127
00:17:10,780 --> 00:17:18,520
At the age of the topic, this is one of the reasons that this is worse because.

128
00:17:20,270 --> 00:17:29,720
Exactly. So everybody heard that. So it's remember, it's the it's the estimate of slope.

129
00:17:31,190 --> 00:17:38,570
So basically what it's saying, children who are one year old are on an average.

130
00:17:39,530 --> 00:17:42,530
They sleep on an average.

131
00:17:42,590 --> 00:17:49,300
They sleep 14 minutes less swimming, which was three weeks ago.

132
00:17:50,830 --> 00:17:57,090
Police want to ask if you can say that a one year increase in age is associated with a 42.

133
00:17:58,320 --> 00:18:01,979
We need to increase the sleep, time says. Yes.

134
00:18:01,980 --> 00:18:07,080
Associated? Not. No. There's no causal interpretation.

135
00:18:07,470 --> 00:18:11,340
Associated. That's what who we're talking about. Linear association.

136
00:18:11,610 --> 00:18:21,120
Okay. So one year increase and to be to be to be absolutely kosher, strictly speaking.

137
00:18:21,420 --> 00:18:36,450
And because I mentioned this point and I don't know if you were hinting at that, at the interpretation of how the design of the study drives it.

138
00:18:36,450 --> 00:18:40,650
I don't know if you were hinting there, but let me just say this clearly.

139
00:18:41,310 --> 00:19:02,640
So maybe even like a strictly for her way of saying this would be children who are one year old daughter on an average sleep 14 minutes less.

140
00:19:04,970 --> 00:19:16,850
So do you know this how I phrased it? So instead of saying with one year increasing its children, sleep 14 minutes sleep less.

141
00:19:17,360 --> 00:19:25,159
As soon as I see the quantity increase in any type of teen said like a change for a single child.

142
00:19:25,160 --> 00:19:30,870
And as I said if I did not more than five if my be ignored.

143
00:19:30,920 --> 00:19:38,840
If it's an observational study which is you know like inherent undecided of a cohort

144
00:19:38,840 --> 00:19:44,450
study if it's an observational study that's inherently cross-sectional there.

145
00:19:44,840 --> 00:19:49,880
I really cannot talk about increasing it.

146
00:19:50,060 --> 00:19:57,530
What I can say is that children who are I want to be both cross-sectional interpretation,

147
00:19:57,540 --> 00:20:03,170
children's world one year or they're on an average sleep 40 minutes less.

148
00:20:04,880 --> 00:20:13,110
But this would that's the interpretation. And so so, you know, kind of this is going to really like, you know,

149
00:20:13,150 --> 00:20:23,270
be the sort of being absolutely, absolutely strictly correct and paying attention to the design.

150
00:20:23,510 --> 00:20:28,639
But that's the interpretation of the 14 point negative, 14.44.

151
00:20:28,640 --> 00:20:35,320
And the negative sign basically says that there's a negative association between sleep time mph.

152
00:20:35,330 --> 00:20:39,020
What about the The Intercept?

153
00:20:40,550 --> 00:20:54,210
Who wants to say about The Intercept? What's the interpretation of intercept in this model?

154
00:21:03,380 --> 00:21:08,200
Who are your. Okay.

155
00:21:08,380 --> 00:21:17,920
So children who are literally awful like newborn on an average sleep 646 point 5 minutes per night.

156
00:21:18,430 --> 00:21:26,950
So that's the interpretation of the intercept estimate, the beta mark.

157
00:21:27,280 --> 00:21:32,920
What's prob what's problematic with this interpretation is.

158
00:21:35,110 --> 00:21:41,440
Is he doing the range of the data? No to zero.

159
00:21:41,590 --> 00:21:45,610
So this is an extrapolation. Zero is outside the range of the data.

160
00:21:45,610 --> 00:21:51,730
The minimum age in this study was 14.4.

161
00:21:52,840 --> 00:21:57,100
I'm sorry. What am I saying was let's look it up.

162
00:21:57,100 --> 00:22:03,040
I think 4.4 because the minimum age was 4.4 years.

163
00:22:04,060 --> 00:22:08,050
Did I some months at some point. Sorry? No. The unit deserves.

164
00:22:09,730 --> 00:22:15,400
So the minimum age was 4.4 years.

165
00:22:15,410 --> 00:22:23,170
So this is the zero eight or the newborn is basically an extrapolation.

166
00:22:23,180 --> 00:22:28,780
So once again, the dangers of extrapolation we have looked at or we have seen.

167
00:22:29,200 --> 00:22:34,059
And so, you know, one has to be very careful in making such an interpretation.

168
00:22:34,060 --> 00:22:39,219
And we talked about how we could say that the data said very mean.

169
00:22:39,220 --> 00:22:54,130
Slide 86, subtract the sample mean and then the reputation of be done on pat becomes it's the average sleep time for a child of average age.

170
00:22:56,080 --> 00:23:15,720
Good. So I just wanted to kind of take you back to this example briefly and comment on the interpretation of the intercept and slope estimates.

171
00:23:15,840 --> 00:23:22,590
So that's it. And then, you know, the next few slides, as I mentioned, that really 42.

172
00:23:23,610 --> 00:23:27,900
So this is the best featured line superimposed on the scatterplot.

173
00:23:28,170 --> 00:23:37,390
But the next few slides are essentially convincing you to hand calculation that those least squares estimate terms

174
00:23:37,440 --> 00:23:45,630
that we got using the l m function in are would coincide if you actually did the brute force calculations by hand.

175
00:23:45,960 --> 00:23:53,820
The S6 as this x minus y and calculated and plugged in the formula for B,

176
00:23:53,820 --> 00:24:01,410
the not pattern with the one hat and the corresponding standard orders to get these these values.

177
00:24:02,070 --> 00:24:05,550
Okay. So that's all.

178
00:24:06,780 --> 00:24:10,680
Any. Any questions?

179
00:24:21,270 --> 00:24:32,590
Okay. No questions. Okay then let's go through some what you'll see.

180
00:24:34,720 --> 00:24:45,550
So what you'll see is, um, basically we will talk about hypothesis testing, so be it regression.

181
00:24:50,010 --> 00:24:57,660
This is sort of the last due. It's a it's a pretty sort of dense module,

182
00:25:01,920 --> 00:25:18,870
but this is the last module of where we are when we use algebra of innovative algebra to sort of establish the inference and simply immigration.

183
00:25:19,530 --> 00:25:27,690
After this module, we actually go into matrix algebra and then we sort of introduce the multiple in any direction.

184
00:25:28,260 --> 00:25:34,530
We basically switch from scalar vector.

185
00:25:37,140 --> 00:25:40,230
So. Um. Hmm. Okay.

186
00:25:40,620 --> 00:25:48,330
So just a review and review.

187
00:25:50,100 --> 00:25:53,940
We have all we have seen all of these last week.

188
00:25:54,660 --> 00:26:05,700
So we have a sample of size n and we have pairs of these eight side by side from one to n bear.

189
00:26:06,780 --> 00:26:12,540
Just to remind you, the Xs are assumed fixed.

190
00:26:14,410 --> 00:26:22,030
There's nothing random about taxes. And if there is measurement error in this course, we are ignoring that.

191
00:26:23,410 --> 00:26:29,560
And the Y values are randomly selected from a population at a given x value.

192
00:26:30,310 --> 00:26:35,890
So be assumed a linear relationship y equal to bitterness plus beta one except as Epsilon II.

193
00:26:36,310 --> 00:26:44,620
And basically this beta not plus beta on its side bar is what we had called the systematic or the deterministic part of the model.

194
00:26:45,010 --> 00:26:49,270
And the Epsilon II is the random part of the model. This is the noise.

195
00:26:49,270 --> 00:26:54,850
Random noise. And for the least squares estimation,

196
00:26:55,150 --> 00:27:05,530
all we assume is that these errors epsilon I have a mean zero come from a distribution that could mean zero and

197
00:27:05,530 --> 00:27:14,859
constant billion sigma squared and the error so epsilon I select g for I want equal today are at least uncorrelated.

198
00:27:14,860 --> 00:27:26,769
So this is all that I'm something that we need to obtain the least squares estimates and given the data we have shown how to estimate beta

199
00:27:26,770 --> 00:27:35,829
open beta one the intercept and slope from this model using the method of this person here are the least squares estimates for beta,

200
00:27:35,830 --> 00:27:43,120
not only beta one which involved the observations like, you know, the kind of the beta.

201
00:27:43,600 --> 00:27:46,870
And as you can see from here,

202
00:27:46,870 --> 00:27:53,829
from the expressions of the least worse estimates and that because the values of beta not

203
00:27:53,830 --> 00:27:58,690
having beta one had depend on the particular random sample drawn from the population.

204
00:28:00,310 --> 00:28:03,730
These quantities are considered random variables.

205
00:28:04,240 --> 00:28:16,209
These are what we call or estimated hours and these are random variables y because all the x bodies is fixed and ssx is fixed, but y bar is just x y.

206
00:28:16,210 --> 00:28:22,150
These are random quantities because they involve y, y, z and a believer.

207
00:28:23,350 --> 00:28:27,100
So beta not Hampden. Beta one had the random variables.

208
00:28:27,370 --> 00:28:32,589
Once the plugin observed the data, then we have estimates.

209
00:28:32,590 --> 00:28:39,100
We have the numerical values of the alpha, beta, anarchic and beta one had.

210
00:28:39,730 --> 00:28:48,850
A typical goal of the analysis is to infer or drawing sentence on the values of B Dunant and beta one that are supported by the data.

211
00:28:49,360 --> 00:28:55,960
And the point estimates are the numerical single values that we got based on data.

212
00:28:56,800 --> 00:29:07,540
And we are we also can talk about the interval estimates or confidence intervals that give us a plausible range of the intercept and slope values,

213
00:29:08,350 --> 00:29:14,200
as well as decide if particular values of beta not in beta want a consistent with the beta.

214
00:29:14,200 --> 00:29:22,570
This is the the premise of hypothesis testing where we set our null hypothesis

215
00:29:23,020 --> 00:29:30,999
and we test if the data observed that is consistent with the null hypothesis.

216
00:29:31,000 --> 00:29:42,040
So this model is going to focus on inference in particular hypothesis testing and confidence

217
00:29:42,040 --> 00:29:50,540
interval for the slope and intercept from the linear regression simple regression model group.

218
00:29:50,620 --> 00:30:02,140
To draw inference, we need to know the sampling distribution of the estimates because as we had shown that the beta not happen beta one had addendum.

219
00:30:02,920 --> 00:30:11,889
So the kind of questions that we are or we looked at what are the average values of beta not pad and beta

220
00:30:11,890 --> 00:30:18,730
one had if we were able to take repeated samples of the population and we derived the average values,

221
00:30:18,730 --> 00:30:30,640
we actually algebraically derived the expressions for the mean of beta, one hat and beta,

222
00:30:30,910 --> 00:30:39,370
but not her, or in other words, the expectation of the sampling distribution.

223
00:30:39,390 --> 00:30:45,040
So beta blocker could be the one that we also derived.

224
00:30:45,760 --> 00:30:52,899
How much variation there is in the values of beta, not having made that one hat from one random sample to another.

225
00:30:52,900 --> 00:31:00,820
So we derive the expressions for the for the variance also of variances also for both.

226
00:31:00,820 --> 00:31:06,400
We've done what happened beta one hat. So these are things that we have already done.

227
00:31:08,530 --> 00:31:12,460
But just to set the stage, I'm kind of reminding you.

228
00:31:13,570 --> 00:31:27,760
Since the expectation of bidders not hacked and the expectation of beta one had are the respective barometers they do not need to one.

229
00:31:28,150 --> 00:31:39,440
So we know that the sampling distribution of these estimates will be centered around those barometer values and the like.

230
00:31:40,140 --> 00:31:50,350
So let's look at the histogram on the left. This is the histogram corresponding to the sampling distribution of the estimate for a data like that.

231
00:31:50,860 --> 00:31:58,750
So if we could do repeated sampling and we talked about this, if I send each of you in the class and draw a random sample from the same population,

232
00:31:59,470 --> 00:32:06,130
and based on your sample, you calculate if you are beta one, have a similar number.

233
00:32:06,640 --> 00:32:14,800
So I have printed that in the video on her from this class, you know, from the point of view.

234
00:32:15,580 --> 00:32:17,770
And then I draw a histogram,

235
00:32:19,000 --> 00:32:29,350
basically the histogram of these observe that is a bigger one had the frequency maybe we did samples from the population would

236
00:32:29,350 --> 00:32:44,440
actually start as s the number of I mean if the class as being this the class size from 20 5000 it would become start looking more and

237
00:32:44,440 --> 00:32:54,519
more like the normal distribution the same curve and now that true unknown because what seems to be happening on particular data

238
00:32:54,520 --> 00:33:09,900
said we can only observe one value but we showed using the map that the expectation and these quantities are equal to that one.

239
00:33:10,180 --> 00:33:15,040
And so on an average we will be doing fine.

240
00:33:15,130 --> 00:33:26,300
Or in other words, we have these estimates are what people are unbiased estimates of the population intercept and still the

241
00:33:26,380 --> 00:33:33,750
and you also divide the formulas for the variance which tells us how much spread is in this histogram.

242
00:33:34,810 --> 00:33:40,970
And we talked about that these formulas involve Cinemascope typically on all that everybody.

243
00:33:42,130 --> 00:33:46,780
So we need to further estimate the Sigma Square before drawing any inference.

244
00:33:47,230 --> 00:33:54,880
And we do this by plugging in an estimate of Sigma squared, which at this point I told you,

245
00:33:55,390 --> 00:34:00,940
hold on, I will prove this in module f distribution theory.

246
00:34:01,150 --> 00:34:09,580
But Sigma had squared or an estimated for the error variance is given by MSE means square error,

247
00:34:09,940 --> 00:34:19,630
which is basically the sum of the squared residuals divided by n minus two.

248
00:34:21,070 --> 00:34:26,889
And once again the n minus two is coming from the degrees of freedom, total number of observations,

249
00:34:26,890 --> 00:34:36,940
minus the two degrees of freedom that we use to estimate between not only the one, we haven't proved this result.

250
00:34:36,940 --> 00:34:45,579
We have proved all the others. We have proved the expectation and the variance results for both we cannot have can be the one hat this one.

251
00:34:45,580 --> 00:34:48,670
We have improved it, but we will prove it later.

252
00:34:48,670 --> 00:35:01,780
For now, just you have to trust me that the MSE is n is actually it's an unbiased estimate for sigma squared.

253
00:35:02,230 --> 00:35:13,120
So what we do is we plugging sigma the MSE in the place of sigma squared for in the variance expressions for b do not happen.

254
00:35:13,120 --> 00:35:16,900
We don't want that and we are done. Okay.

255
00:35:17,380 --> 00:35:27,910
So now note that all of these above estimate, those are derived without assuming independence are normality.

256
00:35:29,620 --> 00:35:41,290
So not only did we not to require independence, independence and normality to get the least squares estimate there for beta not and beta one.

257
00:35:41,680 --> 00:35:50,320
We also did not need the independence and normality assumptions to establish the expectation and variance of these estimate.

258
00:35:50,440 --> 00:35:58,809
So in other words, to establish the sampling distribution of these estimates, we did not need normality either.

259
00:35:58,810 --> 00:36:06,400
Did we need independence? All we needed was that the errors come from a distribution with mean zero constant variance in most of it,

260
00:36:06,400 --> 00:36:13,210
and actually still unclear because one additional piece of thing.

261
00:36:13,330 --> 00:36:20,980
One mission that is needed now for us to carry out the conference.

262
00:36:21,160 --> 00:36:27,670
Establish the framework for hypothesis testing is the shape of the histogram.

263
00:36:31,530 --> 00:36:36,280
In other words, the distribution of beta one had and beta not had.

264
00:36:38,190 --> 00:36:41,970
Okay. You will see why we need the distribution.

265
00:36:43,530 --> 00:36:49,109
For now. Please note here that what is beta?

266
00:36:49,110 --> 00:36:52,290
One has to be given herpes cystic fibrosis six.

267
00:36:53,430 --> 00:37:01,260
I know what is a six. Why is I you know is this x y has different can be written in different forms.

268
00:37:02,910 --> 00:37:10,670
And I am writing will write it in the original form, which is summation if say minus X part times y.

269
00:37:10,740 --> 00:37:18,930
So it's a cross product of the X and Y centered around the mean.

270
00:37:19,380 --> 00:37:22,770
Okay. And I have a six on the bottom.

271
00:37:24,150 --> 00:37:36,150
So this barcode that is this X bar and it's IXI minus X blah part, I'm going to sort of write it together as w one.

272
00:37:37,830 --> 00:37:43,100
So w1i is ixi minus x one divided by six.

273
00:37:43,800 --> 00:37:49,920
So now I can write veto on hat as summation w1iy.

274
00:37:50,910 --> 00:38:06,670
It's like kind of. Linear combinations of wireless where the weights are given by W1.

275
00:38:06,670 --> 00:38:12,700
I guess I can write better on her as a linear combination of Y.

276
00:38:14,620 --> 00:38:20,919
And what about her? We do not have we do not have these y bi miners.

277
00:38:20,920 --> 00:38:25,270
We don't have a spot. What is why by why is the sample mean?

278
00:38:25,270 --> 00:38:37,270
Soit ap does one over n summation one over n y and then beta one had a plugging this expression involving the w and the wise.

279
00:38:37,810 --> 00:38:42,070
And so I took the submission w1iy and I have expired.

280
00:38:43,750 --> 00:38:48,850
So now rearranging the terms a little bit, what do I have?

281
00:38:48,850 --> 00:38:59,169
I have B does not have I can write does sum over I from one to n one over in minus w y nags bah.

282
00:38:59,170 --> 00:39:06,670
So this is the part that is fixed that only involves x, nothing else.

283
00:39:08,770 --> 00:39:12,820
Good times y. So this part here.

284
00:39:17,360 --> 00:39:25,130
This park here is basically, you know, only one six.

285
00:39:33,210 --> 00:39:34,140
So it's fixed.

286
00:39:35,340 --> 00:39:54,910
And similarly, this part also the w1i so now I can replace that expression one over n minus wiw1i aespa as w0i so give it a new sort of notation.

287
00:39:56,610 --> 00:40:03,419
So now I have also shown that we've done not had can be written as a linear combination

288
00:40:03,420 --> 00:40:11,490
of the Y that no variables where the coefficients or the weights are w zero i's.

289
00:40:14,090 --> 00:40:17,390
Okay. So what have I shown here?

290
00:40:17,750 --> 00:40:23,180
That bored Beethoven happened. We do not had a linear combinations of why.

291
00:40:24,170 --> 00:40:30,110
Okay, everybody sort of would be that. Where am I going?

292
00:40:30,800 --> 00:40:38,370
Why did I need to show this? Was something before about the summer budget.

293
00:40:39,180 --> 00:40:48,530
Exactly. Okay. Now, this is where I need the normality of the errors, because now I like to remember.

294
00:40:48,930 --> 00:40:56,009
My goal is to get the shape of this histogram or the distribution of the median off.

295
00:40:56,010 --> 00:41:01,350
That would be the one that so far I did not achieve in distribution about the errors.

296
00:41:03,600 --> 00:41:09,540
I would show them that what we do not have and we don't have the linear combinations of the wise.

297
00:41:09,840 --> 00:41:18,450
So if I can establish that Y is normal, then basically linear combinations of nine months are normal.

298
00:41:19,590 --> 00:41:26,760
So then I am. Then I get a shape of the beaten track and we don't want to have to keep up the distribution.

299
00:41:26,970 --> 00:41:34,470
So this is where I start meeting the normality assumption you remember all along so far I said, I don't need normality.

300
00:41:35,040 --> 00:41:38,790
I don't need independence. But now I do.

301
00:41:39,570 --> 00:41:47,660
Okay. So. Under the line.

302
00:41:47,840 --> 00:41:52,700
Remember, the line is unfair. Linearity, independence, normality.

303
00:41:53,900 --> 00:41:59,380
And. Equal obedience.

304
00:42:00,270 --> 00:42:03,630
The E stands for the equal variance, the constant sigma squared.

305
00:42:03,960 --> 00:42:14,160
So under the line assumptions, under the fluid set of line assumptions all for linearity, independence, normality.

306
00:42:14,160 --> 00:42:27,510
Constant variance. The y i's are independent and identically distributed from a normal random variable from a

307
00:42:27,510 --> 00:42:34,260
normal distribution with mean B to not plus beta one exi and constant vivien's sigma squared.

308
00:42:35,130 --> 00:42:42,450
So everybody has to be absolutely certain that you be.

309
00:42:42,450 --> 00:42:49,169
This is what the line assumption the full set of assumptions give us why?

310
00:42:49,170 --> 00:42:54,989
Because under the line now I'm adding the normality and the independence assumption.

311
00:42:54,990 --> 00:42:57,390
So I'm assuming that the epsilon nines,

312
00:42:58,230 --> 00:43:11,460
the random arrows are independent and identically distributed from a normal with mean zero and constant radians sigma squared.

313
00:43:13,740 --> 00:43:17,300
And I know that y equal to beta,

314
00:43:17,310 --> 00:43:35,760
not blood beta one it's one plus epsilon i under using the linearity or the assumption of line and it says the x values are fixed.

315
00:43:36,360 --> 00:43:43,499
So if if in this model line, again we cannot plug the size, the deterministic part,

316
00:43:43,500 --> 00:43:48,809
the only that intermediation is being contributed by the noise variable epsilon

317
00:43:48,810 --> 00:43:57,420
nine which now has by assumption is coming from I the normal zero sigma squared.

318
00:43:58,290 --> 00:44:11,400
So then under line y eyes are normal with the I do normal with mean beaten up does become unexercised and B and C must square.

319
00:44:11,820 --> 00:44:26,010
So I'm going to pause for 5 seconds and let you sort of observe that then only I will go far that everybody absolutely did.

320
00:44:26,010 --> 00:44:29,100
Sure. That this is this is what we get.

321
00:44:33,330 --> 00:44:43,150
Yes. Yes. There should be no inkling of doubt that this is where we stand.

322
00:44:45,580 --> 00:44:49,930
Okay. So then if that's the case, then why is that IAB?

323
00:44:50,230 --> 00:44:54,040
And in the previous slide, I showed that based on what happened,

324
00:44:54,040 --> 00:45:02,400
we have on track a linear formulations of the way ice, which consists of normal linear combinations of normals.

325
00:45:03,700 --> 00:45:09,400
Is that a normal? We kind of prove that or he showed that in one unit.

326
00:45:09,940 --> 00:45:14,139
So using that piece of data not happened.

327
00:45:14,140 --> 00:45:24,430
We don't want to have that all so normal. They're also normally distributed random variables and I have already derived

328
00:45:24,430 --> 00:45:29,890
the expected value and the variance of be done on pattern with video on hat.

329
00:45:30,550 --> 00:45:34,270
So now what am I getting extra?

330
00:45:35,200 --> 00:45:38,320
I am getting the shape of these distributions.

331
00:45:38,650 --> 00:45:41,740
So beta one hat is normal.

332
00:45:41,740 --> 00:45:50,730
We mean that I want invariant sigma squared over physics and beta are not hat is normal with

333
00:45:50,770 --> 00:45:59,710
mean beta zero and bd variance sigma squared times one over n plus exasperated by is a six.

334
00:46:00,520 --> 00:46:05,500
Yes. So that's what we get.

335
00:46:07,820 --> 00:46:18,100
Now take this first key part of this result, which is which talks about the sampling distribution of the slope estimate there.

336
00:46:20,660 --> 00:46:30,290
I can standardize it with respect to its meaning variance and get a standard normative.

337
00:46:30,300 --> 00:46:31,560
That's exactly what I do.

338
00:46:31,600 --> 00:46:45,890
So beta one minus beta one divided by C must progressive six gives me a normal zero one like this is like a z random variable standard normals.

339
00:46:47,390 --> 00:47:01,350
Yes. However, sigma squared is unknown and we have estimated it using MSE sigma had square.

340
00:47:02,220 --> 00:47:10,260
Therefore, if we instead of this sigma squared, which is unknown, if we plan B mse in the denominator,

341
00:47:12,150 --> 00:47:17,520
then we need to understand what will be the distribution of this beast.

342
00:47:18,270 --> 00:47:24,540
Beta one hack minus beta one divided by square root of MSU versus six.

343
00:47:25,710 --> 00:47:30,840
We need to understand what this how this guy will behave.

344
00:47:37,380 --> 00:47:41,370
So that is going to be the premise of my.

345
00:47:45,980 --> 00:48:00,380
Of my inference. So remember, the first thing that we are going to do is we are going to establish the distribution of this quantity.

346
00:48:03,800 --> 00:48:09,200
So coming up, we are going to. So I've set the stage name.

347
00:48:09,570 --> 00:48:17,480
Now, the things that we are going to specifically talk about are hypothesis testing, testing the slope and intercept parameters.

348
00:48:19,310 --> 00:48:29,480
And then we are going to introduce some softwares, the concept of some softwares and the analysis of gradient stable for simple linear regression.

349
00:48:29,480 --> 00:48:35,330
And finish up with some examples from the textbook con.

350
00:48:36,170 --> 00:48:45,860
Good sections that I know are relevant to this module are sections two, one, three, six and seven.

351
00:48:46,070 --> 00:48:51,500
So you know that the extent of your reading material.

352
00:48:51,920 --> 00:48:56,870
So with that, let's go for a break and let's come back at nine to.

353
00:49:31,043 --> 00:49:38,203
So you can float mo back.

354
00:49:38,273 --> 00:49:49,683
So. Okay. So we have set the stage and here are the properties of the estimates.

355
00:49:49,703 --> 00:50:01,403
All that we talked about in one slide summarized, we have all of these results here.

356
00:50:04,133 --> 00:50:16,493
We have established that MSI is an unbiased or we have mentioned that this is an unbiased estimation of sigma squared,

357
00:50:16,943 --> 00:50:20,422
the unknown error virions, but we haven't proved that.

358
00:50:20,423 --> 00:50:29,663
So this needs to be approved and so this one we will prove later.

359
00:50:33,633 --> 00:50:50,793
And but we have also moved that based on you know, the results above that underline they do not happen be the one that are also normally distributed.

360
00:50:54,033 --> 00:50:57,483
So so that's the that's the summary.

361
00:50:58,443 --> 00:51:10,533
And as I said, we will prove the results about Sigma Square later in module F and we sure that it follows the syllabus.

362
00:51:10,533 --> 00:51:14,423
You must follow the price by distribution. Okay.

363
00:51:17,133 --> 00:51:19,233
Precision of slope estimate or recall.

364
00:51:19,233 --> 00:51:31,623
The variance of beta one half is sigma squared over SFX and because Sigma Square is unknown B and we use MSE to estimate that.

365
00:51:32,103 --> 00:51:37,923
So once we plug in MSE in that expression,

366
00:51:38,403 --> 00:51:50,403
we get the estimated variance of beta one hat variance of an estimate there tells us about the precision of the estimate.

367
00:51:51,393 --> 00:51:59,543
So here is a question. For example, I have two studies and I'm just going to kind of draw a hypothetical scatterplot.

368
00:51:59,553 --> 00:52:11,403
So here is why and here is X and suppose it's W, one is like this.

369
00:52:11,973 --> 00:52:27,423
Okay, here is the scatterplot. And I have a second study where the scatterplot looks something like this.

370
00:52:32,863 --> 00:52:43,183
So the difference like the, you know, just visually looking at the two is in the left hand side scatterplot.

371
00:52:43,633 --> 00:52:59,233
The spread of X is much larger. In the 19 side scatter block, the spread of x is much smaller, more formally in this left hand side,

372
00:52:59,503 --> 00:53:08,533
scatterplot six is quite large and in the right hand side scatter ssx is quite small.

373
00:53:08,953 --> 00:53:11,953
I mean, another sort of hypothetical scatterplot would be.

374
00:53:16,133 --> 00:53:23,333
Maybe another one that I'm going to blog here is something like this.

375
00:53:24,283 --> 00:53:30,593
Oh, just a color. Something like this.

376
00:53:32,043 --> 00:53:37,863
So this is another. Okay.

377
00:53:38,193 --> 00:53:41,013
So in in the right hand side.

378
00:53:42,063 --> 00:53:54,903
But each of those two sectors are basically scenarios where the range of X is much smaller than what we have in the in the left hand side.

379
00:53:56,193 --> 00:54:00,723
So which one for redistributing beta one five would be more precise?

380
00:54:00,963 --> 00:54:07,983
I mean, of course, it's very easy to sort of argue formally based on the variance expression,

381
00:54:07,983 --> 00:54:14,193
but it's also easy to see from these visual from this feedback results.

382
00:54:14,943 --> 00:54:28,653
So the study for which the SS X is larger because this is X is is in the denominator of the variance between had the expression.

383
00:54:28,983 --> 00:54:35,733
So as is this x increases if it's a six is larger than the variance will be smaller.

384
00:54:36,483 --> 00:54:39,903
So the precision of beta one had will be larger.

385
00:54:40,383 --> 00:54:51,503
So the precision of of of the slope estimated would increase as variability in the pool width space in the X for really it increases.

386
00:54:51,543 --> 00:55:05,283
So why do I mean simplistically put wider the range of the x values, the higher position you will have in the slope estimate?

387
00:55:07,233 --> 00:55:14,703
Okay. Now let's talk about hypothesis testing.

388
00:55:17,753 --> 00:55:21,123
So this is review from module, the distribution.

389
00:55:21,983 --> 00:55:30,983
If I have a standard normal Z and I have an F Chi square squared and z and squared an independent,

390
00:55:30,983 --> 00:55:40,343
then z, the ratio of z and the square root of a squared divided by by the by degrees of freedom.

391
00:55:40,433 --> 00:55:47,873
Follow the p distribution with degrees of freedom v f and we sort of review

392
00:55:47,873 --> 00:55:53,702
this distribution in module E We see that this is a symmetric distribution,

393
00:55:53,703 --> 00:55:59,393
that the center that zero and adds the degrees of freedom increases the distribution

394
00:55:59,423 --> 00:56:05,123
starts looking more and more like a normal but not the standard normal distribution.

395
00:56:05,153 --> 00:56:09,443
So there are some properties. So now let's bring it in.

396
00:56:10,313 --> 00:56:13,373
In the context of SLR simply regression model,

397
00:56:13,823 --> 00:56:25,093
we have shown that beta one hat has a normal distribution with Mean B, Bitterman and BD and Sigma squared over six.

398
00:56:25,103 --> 00:56:33,472
So we have shown this and we have we know that SCC divided by sigma swell had

399
00:56:33,473 --> 00:56:38,153
the Chi square within minus two degrees of freedom under the line assumption.

400
00:56:38,453 --> 00:56:42,773
So what why am I saying we know as I mentioned, we haven't proved this yet.

401
00:56:43,043 --> 00:56:48,473
But I have told you at this point, trust me, so that we will prove it later.

402
00:56:50,213 --> 00:56:54,713
So now I can get the standard normal based on beta one,

403
00:56:54,923 --> 00:57:00,443
the sampling distribution of video on hats or video on hat minus beta one divided by square root of

404
00:57:00,443 --> 00:57:11,963
sigma theta versus x is a standard normal the and let me call it Square Associates Sigma Square.

405
00:57:13,013 --> 00:57:20,752
This has a Chi Square distribution with n minus two degrees of freedom and let me denote

406
00:57:20,753 --> 00:57:28,072
a squared divided by n minus two as so what is a squared divided by in minus two.

407
00:57:28,073 --> 00:57:33,743
It's a C divided by n minus two times sigma squared.

408
00:57:40,243 --> 00:57:43,393
This far is the MSCI.

409
00:57:45,823 --> 00:57:55,933
Correct. So the MSCI World Sigma squared is I'm denoting it by this is spread over in minus two.

410
00:57:56,413 --> 00:58:04,813
So now I have the standard normal and I have Amy Silver Sigma Square has a square distribution.

411
00:58:05,053 --> 00:58:11,233
One other thing that I should have would be here, which, again, I haven't proved.

412
00:58:11,233 --> 00:58:22,813
But you will have to trust me on this for now. We will prove it in a in a later module that this V and a square are independent.

413
00:58:30,183 --> 00:58:34,203
But the Z is this. And that is clear.

414
00:58:34,353 --> 00:58:41,593
Is this. These two statistics are independent.

415
00:58:43,423 --> 00:58:49,573
So now we've zero squared independent using the result in the previous bullet point.

416
00:58:50,233 --> 00:59:00,312
I can say that that issue of the Z and A squared divided by and square root of x squared divided by two degrees of freedom.

417
00:59:00,313 --> 00:59:15,103
If I plug it in then I get beta one had minus beta divided by square root of MSE over a six and applying the result in this bullet,

418
00:59:16,573 --> 00:59:20,713
this quantity has a distribution with degrees of freedom and minus two.

419
00:59:20,893 --> 00:59:24,462
Yes. How does that square sky or anything?

420
00:59:24,463 --> 00:59:33,013
Or are we just we are just know doing it. We are just using it as a notation and basically mimicking the the quantity,

421
00:59:33,013 --> 00:59:37,693
the location of the quantity in the previous bullet just to make it easier.

422
00:59:39,373 --> 00:59:45,343
Okay. So everybody with me on this, did you have a question?

423
00:59:46,693 --> 00:59:50,233
Question, no. Okay.

424
00:59:50,383 --> 00:59:55,213
So basically there are certain thoughts here that I have improved.

425
00:59:55,933 --> 01:00:00,463
And but I also said that I have proof.

426
01:00:01,933 --> 01:00:09,132
So at this point, the two things that I have, the things that I have proved are the distributions of beta.

427
01:00:09,133 --> 01:00:10,513
One happen. We do not crack.

428
01:00:11,023 --> 01:00:21,403
I haven't proved the distribution of a square and I also have the independence of the Z and square, so you'd have to trust me on those two items.

429
01:00:21,913 --> 01:00:31,323
But once we have that, then the ratio of the Z and the square root of a square divided by the freedom that I'm applying that further.

430
01:00:31,993 --> 01:00:40,453
So this has a distribution with N minus two degrees of freedom, and this is what we call basically make up the,

431
01:00:40,903 --> 01:00:46,933
you know, foundation for hypothesis testing for a slope in simple linear.

432
01:00:48,913 --> 01:00:56,623
So before we go directly to testing the slope of, of, you know,

433
01:00:58,723 --> 01:01:08,863
testing for the slope of a linear in a simple linear regression here is just to remind me of the general framework for hypothesis testing.

434
01:01:08,863 --> 01:01:15,763
So and the general procedure is because the null hypothesis is not.

435
01:01:16,393 --> 01:01:25,093
And the alternate hypothesis and this again, this should be familiar to you, the general framework.

436
01:01:25,573 --> 01:01:29,713
I'm just sort of going through this as a as a very quick review.

437
01:01:30,103 --> 01:01:37,273
So the first step is to set up an appropriate test statistic, which involves the parameter of interest,

438
01:01:37,273 --> 01:01:48,822
but which also allow which also basically allows you to compute a numerical value of the test statistic based on the data.

439
01:01:48,823 --> 01:01:59,353
So the parameter of interest like in SLR I might be done would be the one computable using our observed data are might be square

440
01:01:59,353 --> 01:02:09,463
centimeters beta on pad and beta one pad and the tractable sampling distribution is are my sampling distributions of the test statistic.

441
01:02:10,723 --> 01:02:32,453
So. At this point I have established normality of the sampling distributions off between our pad and the beta one hat.

442
01:02:34,373 --> 01:02:50,663
Okay, so now I have to the test statistic is, you know, is based off of beta not happen beta one test.

443
01:02:53,033 --> 01:03:02,722
So I compute the state test to stick using observe beta then based on a pre-specified significance

444
01:03:02,723 --> 01:03:09,023
level alpha which is often which is also referred to as the type one error of the test,

445
01:03:09,413 --> 01:03:15,383
often set to 5% level of significance.

446
01:03:15,743 --> 01:03:24,833
I will determine I can determine sort of the conclusions of the test using any of these three methods.

447
01:03:25,043 --> 01:03:28,583
And we are going to talk about all these three methods in the context of SLR.

448
01:03:28,943 --> 01:03:37,312
The first is the rejection region method, where we basically see if the test statistic falls in the rejection region and

449
01:03:37,313 --> 01:03:42,293
the rejection region is defined by the sampling distribution of the statistic.

450
01:03:42,743 --> 01:03:54,953
The second approach is the P value a method, and we check whether the P value is less than the pre-specified level of significance alpha.

451
01:03:55,433 --> 01:03:58,793
And finally, the confidence interval method.

452
01:03:59,603 --> 01:04:05,303
Although this the confidence interval method only applies in the context of a two sided alternative.

453
01:04:06,923 --> 01:04:12,233
And in this method, we check if the null value falls in the confidence interval.

454
01:04:12,243 --> 01:04:17,703
So these are the three ways in which we could carry out the test of hypothesis.

455
01:04:18,173 --> 01:04:28,013
And in either the conclusion as to whether we have enough evidence to reject the null hypothesis or not to reject the null hypothesis.

456
01:04:29,093 --> 01:04:37,763
Once we have arrived at the conclusion, then we translate the answer back to the language in which the question was originally posed.

457
01:04:37,763 --> 01:04:45,743
So this is sort of the, you know, sequence in which we conduct any hypothesis test in general.

458
01:04:46,943 --> 01:04:59,783
Now, in the context of testing about the slope parameter in, in the SLR model,

459
01:05:00,443 --> 01:05:07,283
suppose we are interested in testing the null hypothesis that beta one is equal to zero.

460
01:05:08,693 --> 01:05:17,213
Before I talk about the test statistic, what does beta one equal to zero signify or what does it mean?

461
01:05:17,633 --> 01:05:20,993
Beta one equal to zero. The null hypothesis is beta one.

462
01:05:20,993 --> 01:05:24,143
Equal to zero means what in English?

463
01:05:26,833 --> 01:05:31,283
Sitting in. The model is not significant.

464
01:05:31,293 --> 01:05:35,823
I want to develop simpler, even simpler answers.

465
01:05:36,573 --> 01:05:40,783
We think there is. And there's no.

466
01:05:41,723 --> 01:05:45,123
Exactly. There's no linear association between Y.A.

467
01:05:45,473 --> 01:05:50,813
Right. And so the model is not significant either.

468
01:05:50,813 --> 01:05:58,552
I didn't, sir, but it's it's a much more kind of play at this point, a more convoluted.

469
01:05:58,553 --> 01:06:05,473
And so there's a more debate because the model for it was a lot of other things right now.

470
01:06:05,693 --> 01:06:09,322
So what we are seeing is an eight between equal to zero.

471
01:06:09,323 --> 01:06:14,903
That null hypothesis signifies that there's no linear association between X and Y.

472
01:06:15,863 --> 01:06:27,203
So for that, we are we will use this following test statistic, the test statistic D and capital T.

473
01:06:27,683 --> 01:06:35,583
This is what we compute using data. So this is a number.

474
01:06:39,633 --> 01:06:47,643
And it's given by beta one hecht minus the non hypothesized value in general.

475
01:06:47,943 --> 01:06:51,393
What is the null hypothesis? Value hypothesis better than zero.

476
01:06:51,963 --> 01:06:56,943
So on the in the numerator I have beta one half -0 divided by.

477
01:06:56,943 --> 01:07:05,252
In the denominator I have the standard error of beta one here, which is basically the square root of the estimated variance of beta one.

478
01:07:05,253 --> 01:07:14,492
Heck yes. Yes, everybody. So standard error is standard error of an estimate.

479
01:07:14,493 --> 01:07:26,243
There is. Is the square root.

480
01:07:30,253 --> 01:07:43,703
Of the estimated variance of the estimate there.

481
01:07:43,713 --> 01:07:47,103
So lots of like kind of.

482
01:07:50,503 --> 01:07:55,123
Lots of terms estimate Davidians, estimate ingredients and so on.

483
01:07:55,363 --> 01:08:06,402
But basically it's the babies that of beta one have estimated y estimated because the medians of beta

484
01:08:06,403 --> 01:08:11,983
one have had that sigma squared lacking in there and I had to plug in an estimated four signal squared.

485
01:08:12,373 --> 01:08:20,083
So that's why it's the estimate that I have a hack on top of variance also of beta one hack and then spread of that one.

486
01:08:20,233 --> 01:08:25,833
So that gives me the test statistic. And when I plug in the expressions,

487
01:08:25,843 --> 01:08:37,632
I have beta one hack divided by the square root of MSE over six from the previous slide that has a p distribution with degrees of freedom n minus.

488
01:08:37,633 --> 01:08:42,343
Do I want to point out the the sort of the notation here.

489
01:08:42,553 --> 01:08:46,843
So capital P is what I compute using data.

490
01:08:47,143 --> 01:08:48,583
So this is my test statistic.

491
01:08:48,583 --> 01:09:00,643
This will be a single number and the small D with subscript in minus two denotes the theoretical p distribution with degrees of freedom in minus two.

492
01:09:01,693 --> 01:09:07,753
Everybody with me. Okay, why do I need this distribution?

493
01:09:08,203 --> 01:09:15,163
Because now I have a reference for my observed value of the test statistic.

494
01:09:15,403 --> 01:09:21,163
I mean, let's say you have a data set, you have a sample, you calculate the D.

495
01:09:21,523 --> 01:09:23,623
Now how? How do I know?

496
01:09:23,623 --> 01:09:35,803
Is that so large or small or does it fall in the spectrum or the difficulty distribution if the null hypothesis indeed were true?

497
01:09:37,783 --> 01:09:42,552
Okay. So that's why I need a reference distribution.

498
01:09:42,553 --> 01:09:47,833
And my reference distribution is this little D with subscript N minus two.

499
01:09:47,833 --> 01:09:51,703
It signifies a theoretical T distribution within minus two degrees of freedom.

500
01:09:52,033 --> 01:09:56,892
And there are three ways to conduct this hypothesis. Test Objection.

501
01:09:56,893 --> 01:10:00,913
Region method development value and a confidence interval method.

502
01:10:01,813 --> 01:10:05,052
Okay, so let's go through each of these one by one.

503
01:10:05,053 --> 01:10:11,053
The deduction reading method we call the null hypothesis is that would be equal to zero.

504
01:10:11,593 --> 01:10:22,423
So we are assuming like universe where the null hypothesis is true and we look at how unlikely or how extreme the observed data look like.

505
01:10:23,203 --> 01:10:28,333
So once again, remember, P is the computed value of the statistic.

506
01:10:28,753 --> 01:10:41,413
So it's a number, single number but is later D with subsidy f is the purity it'll be distributed random variable with degrees of freedom equal to def,

507
01:10:42,673 --> 01:10:57,463
and by this notation the little d subscript DFA comma q I will denote the Q percentile for the T distribution with degrees of freedom def.

508
01:10:59,443 --> 01:11:08,142
So what the dictionary did, method says, is go to the theoretical distribution with degrees of freedom.

509
01:11:08,143 --> 01:11:16,633
Give find a cubic percentile and the q percentile will depend what the prespecified significance level alpha is.

510
01:11:17,263 --> 01:11:27,492
So find the got point b, d, f, former q where q is one minus alpha or one minus alpha over two,

511
01:11:27,493 --> 01:11:38,443
depending on whether the alternative hypothesis h one is one sided or two sided because remember P distribution is symmetric.

512
01:11:39,583 --> 01:11:55,823
So the so the sum of the area above a critical point in the writing will be equal to the area below negative of that critical point in the left,

513
01:11:55,843 --> 01:12:05,113
then by symmetry of the T. So depending on whether the alternative hypothesis is one sided or two sided,

514
01:12:06,193 --> 01:12:13,183
B will determine the cut point from the theoretical T distribution and the prediction region method says that

515
01:12:13,363 --> 01:12:22,553
we did the null hypothesis when the computed value of the tested distinctive capital D is beyond the 2.0.

516
01:12:22,603 --> 01:12:31,333
In other words, the observed data is beyond what we would expect to see if there were truly no association.

517
01:12:32,983 --> 01:12:38,983
So here is sort of four different kinds of alternative.

518
01:12:38,983 --> 01:12:43,363
Here is how you would specify the appropriate prediction needed.

519
01:12:43,663 --> 01:12:48,223
So the first example is where the so the null hypothesis is.

520
01:12:48,863 --> 01:12:58,163
Same for all three be equal to zero, or in other words, that there is no linear association between VNS.

521
01:12:58,493 --> 01:13:08,033
And suppose we are assuming that the pre-specified alpha or the level of significance of the test is .25 a younger.

522
01:13:11,433 --> 01:13:20,583
Okay. So now the first alternative, H-1, is a one sided alternative, better one greater than zero.

523
01:13:22,173 --> 01:13:25,563
Here is how the rejection region will look like.

524
01:13:26,193 --> 01:13:29,223
So let's see. I have computed capital T right.

525
01:13:29,703 --> 01:13:34,113
And I have. Here is how my theoretical T looks like.

526
01:13:35,793 --> 01:13:42,273
So what I am going to find is the alternative is beta one greater than zero.

527
01:13:42,283 --> 01:13:47,192
So I'm going to go to that theoretical t distribution with degrees of freedom in minus

528
01:13:47,193 --> 01:13:58,803
two and find the value such that the area to the right of that is point or five.

529
01:14:07,093 --> 01:14:25,223
This value here. This value here is my B in -2.95.

530
01:14:26,003 --> 01:14:39,082
So the .95 signifies that all the 80 up to that point under the decode was .95 and the area to the right of it right there is 1 to 5.

531
01:14:39,083 --> 01:14:45,923
So that would be the rejection region for the one sided alternative between greater than zero.

532
01:14:47,033 --> 01:14:55,163
Okay. So now what about if the alternative is one sided but B the one less than zero?

533
01:14:55,733 --> 01:14:59,093
So by symmetry, what would they have?

534
01:15:01,163 --> 01:15:07,402
What would be my critical that. So I would in the previous case I would I have a number for the capital D.

535
01:15:07,403 --> 01:15:17,623
Right. And I'm going to take that number and see if that is greater than this blue point, like blue red D and -2.95 B.

536
01:15:17,873 --> 01:15:21,833
If it is, then I would reject the hypothesis. Okay.

537
01:15:22,373 --> 01:15:26,182
If the alternative is better one less than zero, then what do I do?

538
01:15:26,183 --> 01:15:30,383
So here is my D again, like the theoretical t.

539
01:15:32,603 --> 01:15:38,303
Okay. And I have the numerical statistic computed.

540
01:15:38,843 --> 01:15:50,183
So now I'm going to look at the left side of the left thing and I'm going to find the critical value that sorts that.

541
01:15:50,183 --> 01:15:56,002
The area to the left of it is going to five body symmetry of the T distribution.

542
01:15:56,003 --> 01:16:03,923
What would this be? This would be just minus this value, the the blue value.

543
01:16:03,923 --> 01:16:08,543
So it would be minus B in -2.95.

544
01:16:15,023 --> 01:16:29,133
Yes. So again, I would see if the capital t numerical value is less than negative B and -2.95, then I would reject.

545
01:16:29,883 --> 01:16:36,333
Now the third is the two sided alternative beta one not equal to zero.

546
01:16:36,363 --> 01:16:50,283
So now I have to distribute that alpha 5% in the two PS, both the right and the left it and once again by symmetry I have to distribute it equally.

547
01:16:51,543 --> 01:16:56,433
So what would be the block off point?

548
01:16:56,553 --> 01:17:02,583
So here is again my theoretical distribution.

549
01:17:03,783 --> 01:17:06,903
I have computed capital t the test statistic.

550
01:17:07,113 --> 01:17:23,043
But now I'm going to look at. These two players thought that the idea to the right and the idea to the left is

551
01:17:23,813 --> 01:17:32,003
point or 5.5 to the right and don't go too far to the left and then up together.

552
01:17:32,013 --> 01:17:35,943
I get the point of five. So what what am I finding out?

553
01:17:35,943 --> 01:17:43,803
I'm finding out the be in minus two, the 97.5% dying or.

554
01:17:46,483 --> 01:17:56,563
Here is my critical value of the deal. And similarly by symmetry, this point is negative B and -2.975.

555
01:17:57,823 --> 01:18:08,913
If so, I would reject the null hypothesis. If the capital B is either bigger than the N -2.975 or it is smaller than negative and -2.975.

556
01:18:09,013 --> 01:18:19,113
Yes, the reduction. And those who were not included here are similar to those of.

557
01:18:21,083 --> 01:18:24,903
I'm not sure I understood it if they did it for me to say.

558
01:18:24,933 --> 01:18:29,823
I'm just asking. Yes.

559
01:18:30,093 --> 01:18:38,333
Yes. Oh, so if the if the third hypothesis that you sided offer of an alternative means that if that is a more point of this and that is.

560
01:18:38,393 --> 01:18:41,243
Yes. Because he I mean,

561
01:18:41,663 --> 01:18:52,073
normally you would do an cited alternative because you do not have a priori knowledge about which direction that association would be.

562
01:18:52,223 --> 01:19:02,303
So unless there is, you know, scientific evidence established that says that that allows you to use that prime information,

563
01:19:02,513 --> 01:19:05,813
you would normally be left inside the desk.

564
01:19:06,353 --> 01:19:09,763
What can I do as a group on that? We that process.

565
01:19:11,583 --> 01:19:17,843
Okay. So that's the that's the premise of the rejection reading method.

566
01:19:18,503 --> 01:19:26,632
Know the p value method. So the beginning I put any other question.

567
01:19:26,633 --> 01:19:30,343
Sorry. Okay.

568
01:19:30,643 --> 01:19:33,222
So what what does the P value method do?

569
01:19:33,223 --> 01:19:42,733
So once again, we call that the null hypothesis is that between equal to zero we are assuming the universe with the null hypothesis is true.

570
01:19:43,423 --> 01:19:46,483
Now what is the p value?

571
01:19:48,643 --> 01:19:59,863
The p value by definition is the probability of getting results at least as extreme or more extreme as the

572
01:19:59,863 --> 01:20:08,022
ones who have observed under the null people does not tell you the probability that the knowledge spawned.

573
01:20:08,023 --> 01:20:11,203
So this is a very, very important concept.

574
01:20:13,633 --> 01:20:17,293
So once again, let's try to make sure that we all are on the same page.

575
01:20:17,923 --> 01:20:32,263
So if the null hypothesis is true, what is the chance that you could get a result that is as extreme or more extreme than what you bought?

576
01:20:32,563 --> 01:20:42,223
That's the interpretation of feedback. So one more time, if the null hypothesis were to be true,

577
01:20:42,703 --> 01:20:51,883
the p value tells you the probability of getting a result that is as extreme or more extreme than what you bought.

578
01:20:53,293 --> 01:20:56,583
So this conceptually tell eco friendly small.

579
01:20:56,593 --> 01:21:03,823
What does that mean and what would you what would you do in the dictionary or would you not predict that other.

580
01:21:05,683 --> 01:21:09,042
Huh? You said read it. Why do they did the beginning?

581
01:21:09,043 --> 01:21:12,103
Just tell me. Like, give me a plain English answer.

582
01:21:12,413 --> 01:21:16,633
You observe. They are very extreme. Probably not very likely.

583
01:21:17,353 --> 01:21:24,762
Exactly. So if the people are small, then basically what we are seeing is a probability of getting a result.

584
01:21:24,763 --> 01:21:31,753
Like what you bore is very, very small, very extreme, really very small.

585
01:21:32,053 --> 01:21:35,533
If the null hypothesis indeed would be true.

586
01:21:36,223 --> 01:21:40,783
So that means you are casting a serious doubt on the null hypothesis.

587
01:21:41,413 --> 01:21:45,733
So you would reject the null hypothesis? That's the interpretation.

588
01:21:46,063 --> 01:21:56,443
He does not believe that the probability that it does not tell you anything about the probability that the knowledge is false.

589
01:21:56,683 --> 01:22:06,743
It tells you that the evidence points to the direction that something to get and is that something like this is really very low chance.

590
01:22:07,423 --> 01:22:14,503
If the null hypothesis was true. So the the method is you will first compute the p value.

591
01:22:15,013 --> 01:22:20,983
And again, depending on the whether it's one sided or a full sided alternative,

592
01:22:21,373 --> 01:22:32,532
you would basically compute the p value again using the theoretical distribution and the computed value of the test statistic.

593
01:22:32,533 --> 01:22:42,173
So formally. If if if I have to define if the alternative is Bateman greater than zero, then what is the p value?

594
01:22:42,493 --> 01:22:53,263
So this is capital D, remember is your number numerical the statistic that you computed and you are saying how like you know,

595
01:22:53,503 --> 01:23:03,553
what is the area to the right of that numerical value for a p distribution within minus two degrees of freedom?

596
01:23:04,843 --> 01:23:08,363
Because that's the reference distribution for the slope estimate.

597
01:23:09,163 --> 01:23:15,673
And similarly for the left sided alternative and for the two sided alternative,

598
01:23:15,823 --> 01:23:21,642
what what will you do is you will actually remember the p value is two times.

599
01:23:21,643 --> 01:23:27,193
So it would be either in the United or the left and it would be extreme either way.

600
01:23:27,853 --> 01:23:47,623
So you are going to compute the probability that the theoretical D is either greater than capital T the observed value or it's less than negative T,

601
01:23:47,773 --> 01:23:53,893
capital D or in other words, the political P in absolute value is greater.

602
01:23:54,283 --> 01:23:59,053
The difficulty is greater than the capitals d the absolute value.

603
01:24:01,433 --> 01:24:12,833
Slice of that. Okay. So that's the interpretation of the of the head when you would predict when the P value is is less than alpha.

604
01:24:13,583 --> 01:24:21,892
So once again, just to draw some picture, because it's I'm going to sort of maybe do it for this first alternative.

605
01:24:21,893 --> 01:24:30,983
So here is my theoretical P and for a one sided alternative, beta one rather than zero.

606
01:24:31,043 --> 01:24:34,703
So again, remember, this is a number. Capital is a number.

607
01:24:35,063 --> 01:24:47,873
So let's the capital B is here. So I'm going to look at the area to the right of capital B because that is an example

608
01:24:47,873 --> 01:24:54,832
of something I'm getting a result that is more extreme than what you observe.

609
01:24:54,833 --> 01:25:18,423
And my p value would be this area. Okay so the right deal probability to the right of capital T which is which is a numerical red.

610
01:25:18,813 --> 01:25:28,623
And then for the for this hypothesis you get look at the left then probability to

611
01:25:28,623 --> 01:25:39,053
the left of negative capital T and then finally for the two sided alternative,

612
01:25:39,063 --> 01:25:45,603
you are what you would be better would be the area which is equal to twice this.

613
01:25:58,693 --> 01:26:11,763
This is positive. This is negativity. And by symmetry being eaten I simply calculate this area and multiply by two to get the p value.

614
01:26:14,583 --> 01:26:24,363
Okay. So that's the P value method. And then you are going to reject the null hypothesis if the p value is less than the pre specified alpha.

615
01:26:27,303 --> 01:26:33,183
Okay. Confidence interval method. This method works really well.

616
01:26:33,393 --> 01:26:35,853
A lot of what I said works nearly well.

617
01:26:35,853 --> 01:26:48,393
I mean, like, this method is really quick and like, you know, sort of useful, but it only applies when the alternative hypothesis is two sided.

618
01:26:49,173 --> 01:26:58,382
So each one is better, one not equal to be one in general and this be one we have been testing for no association,

619
01:26:58,383 --> 01:27:04,293
so this one would be zero in the context of testing for nonlinear association.

620
01:27:05,763 --> 01:27:14,402
So the confidence interval method. So you would compute a 95% confidence interval for beta one and the 95% confidence interval for beta

621
01:27:14,403 --> 01:27:25,713
one is given by the point estimate beta one head plus minus the standard error for beta one half.

622
01:27:25,723 --> 01:27:40,653
Remember this guide square root of the estimated variants of beta one had this multiplied by the reference the critical

623
01:27:41,193 --> 01:27:54,033
value of B distribution with n minus two degrees of freedom and the critical value for a 95% confidence interval.

624
01:27:54,813 --> 01:27:59,373
Basically, again, one, two, two, five, one, two, two, five on the two pairs.

625
01:27:59,703 --> 01:28:08,343
So the critical value correspond to the 97 one fifth percentile of a T distribution with N minus two degrees of freedom.

626
01:28:10,143 --> 01:28:15,453
So that's the formula for a confidence interval.

627
01:28:15,663 --> 01:28:21,093
What is the standard error of beta one hat? We know how to estimate that.

628
01:28:21,543 --> 01:28:24,663
So that's MSE. What is the sixth square root of the whole thing?

629
01:28:25,713 --> 01:28:35,343
So I just plug that in and I get the confidence interval for for beta one.

630
01:28:35,703 --> 01:28:42,813
So what is the interpretation of the confidence interval? The confidence interval tells you that these are values you can't rule out.

631
01:28:43,623 --> 01:28:49,563
And if we could repeat it, if we could do repeated sampling a lot of time.

632
01:28:51,183 --> 01:28:54,363
So once again, these are two the repeated sampling idea.

633
01:28:54,363 --> 01:28:57,842
Everybody in the class bought the data from the same population.

634
01:28:57,843 --> 01:29:01,533
Blood starts up. You know, it's a simple thing.

635
01:29:01,533 --> 01:29:10,143
Any person gets a point, estimate and see if we do this 100 times is repeated sampling.

636
01:29:10,563 --> 01:29:27,153
So 95% of the time or out of those hundred 95 of those of the intervals will contain the true unknown value.

637
01:29:28,323 --> 01:29:34,173
It may not necessarily be your your current confidence interval,

638
01:29:35,133 --> 01:29:48,363
but we go back into the large sample for the repeated sampling idea of the 195 of the confidence intervals will capture the true unknown barometer.

639
01:29:49,293 --> 01:29:58,243
Again, you must have sort of heard this analogy from your basic statistics, cause it's like, suppose there's a kind of 400 banded.

640
01:29:58,293 --> 01:29:59,223
It is like a butterfly.

641
01:29:59,433 --> 01:30:09,993
Butterfly, but they know, you know, at this point, let's pretend that, you know, it's it's static and you are trying to you don't know where it is.

642
01:30:10,713 --> 01:30:16,053
And each of you are trying to sort of you have a net and you trying to catch the butterfly in the map.

643
01:30:18,003 --> 01:30:26,833
So you feel it is sort of. Fight or, you know, kind of try and get it a hundred times.

644
01:30:27,683 --> 01:30:34,913
The net, the confidence interval is like 95 of the 100 times actually.

645
01:30:35,483 --> 01:30:40,973
You will be able to get of what I say, but there are 35 times who you would miss it.

646
01:30:42,143 --> 01:30:52,013
Okay, so your particular confidence interval may not have the true meaning, but these are all values that we can pull out.

647
01:30:52,463 --> 01:31:02,723
And the way we conduct the hypothesis test here is you would reject the null hypothesis if B1 is outside the confidence interval in this case,

648
01:31:02,723 --> 01:31:05,993
if it's a 95% confidence interval.

649
01:31:06,323 --> 01:31:10,013
So what at what level of significance would it not be detected?

650
01:31:10,373 --> 01:31:13,793
5% I would fight will lose 5% to that.

651
01:31:14,183 --> 01:31:17,513
So the idea is this once again, a picture is worth a thousand words.

652
01:31:18,053 --> 01:31:29,663
So here is let's say this is my confidence interval and suppose my, you know,

653
01:31:29,663 --> 01:31:36,653
B1, the hypothesized null value is here, it falls in the confidence interval.

654
01:31:38,303 --> 01:31:46,783
Then I am going to see that, you know, I cannot predict the null hypothesis because B1 is up.

655
01:31:46,943 --> 01:32:04,163
So here is my confidence interval. The true unknown parameter may not be here, but the your hypothesized value like for that particular interval,

656
01:32:04,583 --> 01:32:08,693
but you hypothesized value be one false in that interval.

657
01:32:09,113 --> 01:32:14,003
So I would end up not rejecting the null hypothesis.

658
01:32:14,603 --> 01:32:19,423
So that's the confidence interval method. Questions?

659
01:32:20,123 --> 01:32:28,533
Yes. Yes, the alpha should be pre-specified.

660
01:32:28,983 --> 01:32:34,923
The alpha in the in the general test of hypothesis is always pre-specified.

661
01:32:35,433 --> 01:32:43,443
So you set it at 5% level or 10% level again, depending on the scientific objective of the study.

662
01:32:44,643 --> 01:32:48,063
Usually it's said that, but usually it's at 5%.

663
01:32:51,093 --> 01:32:56,103
So you do not appear to have conducted the test.

664
01:32:56,103 --> 01:32:58,023
You have to specify that period.

665
01:32:59,633 --> 01:33:09,933
If the I mean, there's a sort of a philosophical debate, but generally like 1530, you have done the testing, you start the beginning of the study.

666
01:33:10,143 --> 01:33:21,623
Right. So you have to specify that. And again, as I said, the your scientific objective determines the level of significance.

667
01:33:21,633 --> 01:33:25,383
I mean, can you live with making that 10%?

668
01:33:25,923 --> 01:33:29,823
I pointed out when you are that one of the means to reject the a hypothesis,

669
01:33:29,833 --> 01:33:36,993
then then our hypothesis is still very liquid that in the case, yes, you can say the signal 5.1,

670
01:33:37,023 --> 01:33:47,433
but normally instead of 5% because that basically is the conventionally okay you can live with the 5% making an error of

671
01:33:48,183 --> 01:33:55,863
historic decision of predicting the null and saying that actually there is a significant association with the reason.

672
01:33:56,553 --> 01:34:08,523
So making the 5% error, you can live with it. And that's why you set Alpha normally at 5%, but otherwise there is nothing holy about that 5%.

673
01:34:08,913 --> 01:34:15,753
Okay. So that's that's the confidence interval method.

674
01:34:15,753 --> 01:34:24,603
No interpreting tests of slope. If the null hypothesis between equal to positive is rejected,

675
01:34:24,603 --> 01:34:30,483
then what is our conclusion that the slope of the regression line is significantly different from zero?

676
01:34:31,263 --> 01:34:35,403
So in other words, that X and Y have a significant linear association.

677
01:34:36,423 --> 01:34:42,573
If you fail to reject the null hypothesis in either of using either of these three methods,

678
01:34:43,023 --> 01:34:54,143
then we conclude that we do not have enough evidence to say that the slope of the regression line is zero.

679
01:34:55,673 --> 01:35:05,373
So. So in other words, we are seeing that X and Y exhibit no significant linear association.

680
01:35:07,023 --> 01:35:10,403
However, note that once again, this is a very important one.

681
01:35:10,413 --> 01:35:20,133
But you know, sort of about the general framework of hypothesis testing is that these tests won't tell you if each know that each one is true.

682
01:35:22,063 --> 01:35:30,723
Okay. What do you say? Is that. Do I have enough evidence to believe that it's not cause?

683
01:35:31,383 --> 01:35:36,553
Or each does more cause. They gave you the strength of the evidence.

684
01:35:36,573 --> 01:35:48,483
Significant or not significant. But they don't tell you anything about the truth of H Dark, the null, or the alternative.

685
01:35:50,553 --> 01:35:57,123
On the other hand, although it tells you about the strength of evidence, significant or not significant,

686
01:35:57,513 --> 01:36:08,613
you have to be very cautious about not to overinterpret the significant reason for for beta one.

687
01:36:09,333 --> 01:36:16,383
And I'm talking mostly in the context of the slope now because that is intrinsically of more interest.

688
01:36:16,833 --> 01:36:23,733
Here are two scenarios. This figure is from your textbook on the left hand side.

689
01:36:25,263 --> 01:36:29,613
My scatterplot y versus x looks like this on the right hand side.

690
01:36:30,153 --> 01:36:35,043
My scatterplot Y versus X looks like this in both situations.

691
01:36:35,643 --> 01:36:39,243
I would reject the null hypothesis.

692
01:36:39,393 --> 01:36:46,383
I needed the null hypothesis in both situations. But what is the difference between these two scatter plots here?

693
01:36:47,313 --> 01:36:49,743
Clearly I'm making a correct decision.

694
01:36:49,983 --> 01:37:01,703
Rejecting the null hypothesis means that I'm saying that there is a significant linear association between Y index for the left hand side scatterplot.

695
01:37:01,713 --> 01:37:15,603
I am making a correct decision for the right hand side and I have to be very cautious in not overinterpreting because there is,

696
01:37:16,623 --> 01:37:22,863
you know, this does this scatter plot. I mean, it has a V, the Y versus in X relationship.

697
01:37:23,293 --> 01:37:27,103
It's actually very strong, but it's a quadratic relationship.

698
01:37:28,873 --> 01:37:34,363
That's why I end up rejecting the the null hypothesis.

699
01:37:35,413 --> 01:37:38,773
However, the association is not linear.

700
01:37:39,673 --> 01:37:44,923
It is squadron. So in this case, in for the right hand side scatterplot.

701
01:37:45,253 --> 01:37:50,473
Actually there is no linear association.

702
01:37:55,453 --> 01:37:58,963
But a very strong quadratic association.

703
01:38:05,673 --> 01:38:11,733
So I have to be very careful in not overinterpreting the significance of that.

704
01:38:12,683 --> 01:38:23,043
Desktop beta one equal to zero. Conversely, if you're just about the second or third.

705
01:38:24,673 --> 01:38:37,623
Sadly, the standard errors are bigger than the standard errors are bigger in V than in, if not necessarily not necessary.

706
01:38:39,363 --> 01:38:44,913
The functional form is different. On the left, it's a linear function.

707
01:38:44,923 --> 01:38:52,623
The relationship between Lion X on the right is a strong quadratic relationship we do in X.

708
01:38:54,813 --> 01:39:05,403
What about the converse? Conversely, if we actually fail to reject the null Beethoven equal to zero, then also we must not overinterpret.

709
01:39:05,403 --> 01:39:12,483
That is because Beethoven measures the linear association between y index and the relationship need not be linear.

710
01:39:12,753 --> 01:39:17,793
So here again, another set of figures from your textbook.

711
01:39:18,243 --> 01:39:24,753
So on the left hand side, I have a scatterplot of y votes for sex for which the best line fit.

712
01:39:24,993 --> 01:39:33,093
He's a flat line. If you step away from that scatter, it looks like kind of a random factor note.

713
01:39:33,273 --> 01:39:38,583
So there's no linear association between Y and X.

714
01:39:38,853 --> 01:39:41,613
So here again, I am making a correct decision.

715
01:39:45,483 --> 01:39:54,123
By failing to predict the null hypothesis, I am making a correct decision, whereas for the right hand side scatter.

716
01:39:54,123 --> 01:40:01,143
Also I end up getting the best line as the horizontal line here.

717
01:40:02,073 --> 01:40:13,533
But here I have to be very cautious again because if I look at these two points and maybe I'll just do it with blue here.

718
01:40:18,023 --> 01:40:26,963
Look at these two points here. MARTIN These two data points, which potentially may be outliers.

719
01:40:27,353 --> 01:40:34,043
Again, there's a very strong association between why index, but it's a quadratic association.

720
01:40:34,763 --> 01:40:42,593
So that flat line, which basically for this scatter gives me the best line fare,

721
01:40:43,763 --> 01:40:57,743
best weighted line by B squares and for which I end up feeling to reject the null is actually sort of not telling, giving me the full picture.

722
01:40:58,943 --> 01:41:00,433
So this is the situation.

723
01:41:00,443 --> 01:41:13,913
There is a strong association that is in some way which is but quadratic, but in some way it is maybe masked by these two blue outliers.

724
01:41:14,633 --> 01:41:26,693
And the two settings are on the left and right are very different, although in both cases we end up failing to reject the null.

725
01:41:28,163 --> 01:41:36,893
So once again, you have to be really sort of cautious about not overinterpreting the result either in the case of,

726
01:41:37,313 --> 01:41:41,093
you know, failing to predict the null or not failing to reject the null.

727
01:41:42,113 --> 01:41:50,043
Bottom line, the whole message, a picture is worth a thousand words the minute you see this of note.

728
01:41:50,123 --> 01:41:54,563
That's why you keep on emphasizing. Be forewarned, jump and try to fit the model.

729
01:41:55,133 --> 01:42:00,203
That's that. It's fun. You get lots of good distributions.

730
01:42:00,213 --> 01:42:07,973
Look at histograms because immediately, as soon as you see the scatterplot, it's apparent what's going on.

731
01:42:09,713 --> 01:42:23,243
So I'm going to end here and then we'll pick up from here and talk about hypothesis testing for your test and avoiding for an example.

732
01:42:23,273 --> 01:42:24,023
Thank you.

