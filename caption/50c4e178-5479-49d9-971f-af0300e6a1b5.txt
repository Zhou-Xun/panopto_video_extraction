1
00:00:02,000 --> 00:00:08,900
It's not it's not a 300. Oh, indeed.

2
00:00:08,920 --> 00:00:23,900
The thing we saw on the screen is. Turn this way first.

3
00:00:29,210 --> 00:00:34,150
So. Yes.

4
00:00:37,480 --> 00:00:42,640
I'm not screaming. So we need to switch it, though, right?

5
00:00:42,660 --> 00:00:47,930
Yeah, but sometimes for the moment.

6
00:00:54,770 --> 00:01:01,390
As the. Yeah.

7
00:01:01,830 --> 00:01:09,400
Yeah, but I think we have to. Right.

8
00:01:10,940 --> 00:01:13,999
Such as? Which one's aware of it?

9
00:01:14,000 --> 00:01:27,270
Doesn't down here. Hmm.

10
00:02:07,630 --> 00:02:13,290
At the time. All right.

11
00:02:20,490 --> 00:02:26,800
Phones. All right.

12
00:02:27,180 --> 00:02:32,330
So after that, we're going to get started on the topics of today's My First Call.

13
00:02:33,880 --> 00:02:37,780
So I graduated with my teaching in 2009 from Berkeley.

14
00:02:38,110 --> 00:02:43,059
And at the time that I took a course like this, causality was always a lecture at the very end of the course, right?

15
00:02:43,060 --> 00:02:48,880
So we'd get all the fee and we wouldn't think really about causality until the end of the lecture to work out of bit.

16
00:02:50,710 --> 00:02:57,820
But, you know, it's, it's really important that we think about causality kind of continuously in epidemiology and I'm going to

17
00:02:57,820 --> 00:03:04,660
do my best or I do my best to try to incorporate causality throughout all of the lectures in this class.

18
00:03:05,080 --> 00:03:10,600
And so for that reason, we start off with causality and talking about causality.

19
00:03:11,530 --> 00:03:16,480
All right. But first, I wanted to do a quick revisit of the first lecture.

20
00:03:17,110 --> 00:03:24,610
So one of the things I do always try to do is if I notice a mistake or somebody brings something to my attention or points out that something is old,

21
00:03:25,120 --> 00:03:29,349
I will let you all know was sort of put slides.

22
00:03:29,350 --> 00:03:35,110
I'll exit out and correct it, right, and then make changes to the slides for next year.

23
00:03:36,250 --> 00:03:44,140
So some of the definitions I presented last time pointed out to me what old writing aren't really the definitions used anymore.

24
00:03:44,590 --> 00:03:49,059
And so these slides will be changed online, will actually change the version of the intro slide.

25
00:03:49,060 --> 00:03:53,890
But don't worry about those definitions.

26
00:03:54,130 --> 00:03:59,260
I also, by the way, won't I will ask you to define something usually, right?

27
00:03:59,260 --> 00:04:10,569
I'll ask you to apply the concept. So one definition of health issues is the W.H.O. definition of health, which is a state according to them,

28
00:04:10,570 --> 00:04:16,480
of complete physical, mental and social wellbeing and not merely the absence of disease or infirmity.

29
00:04:16,930 --> 00:04:21,790
One of the big problems with this definition, there's a lot of debate around this definition is the word complete.

30
00:04:21,790 --> 00:04:26,290
It's actually a 1949 definition that we're still working with.

31
00:04:28,510 --> 00:04:34,870
And, you know, so questions and issues that kind of come up with like does being healthy exclude having any disease?

32
00:04:35,380 --> 00:04:39,490
You know, does that mean that somebody who's older, older, adult, many of them have some chronic diseases,

33
00:04:39,490 --> 00:04:49,210
may be very well managed or they not healthy then because they have diabetes, even if it is, you know, very well managed with medication, etc.

34
00:04:49,600 --> 00:04:57,700
And so it is kind of something to think about. There's a lot of debate in the field around what exactly is health and defining healthy.

35
00:04:58,540 --> 00:05:02,199
I think the main thing I wanted to do before when I was talking about health is

36
00:05:02,200 --> 00:05:04,929
just to make sure we're all thinking about kind of all these different dimensions.

37
00:05:04,930 --> 00:05:07,900
You don't need to know this specific model or different models of health.

38
00:05:08,710 --> 00:05:16,480
I just pulled this one out because it brings and it kind of talks about biology, social, social aspects and then psychological aspects.

39
00:05:16,480 --> 00:05:18,400
Right. And these all have an interplay.

40
00:05:18,670 --> 00:05:24,379
When you think about hospitals, you know, infectious disease, people tend to just think about the kind of biological aspects.

41
00:05:24,380 --> 00:05:27,750
It's been very interesting for me as an infectious disease person as of late.

42
00:05:27,760 --> 00:05:34,419
I mean, COVID was a good example of sometimes the infectious disease people not really thinking about all of the aspects,

43
00:05:34,420 --> 00:05:38,380
right for health and and how you communicate health and issues.

44
00:05:39,880 --> 00:05:43,750
But one of the ones that's been really interesting for me lately has been monkeypox.

45
00:05:44,410 --> 00:05:44,709
Right?

46
00:05:44,710 --> 00:05:56,680
Because when monkeypox that the outbreak first started and, you know, a bunch of infectious disease people together, a lot of them older individuals.

47
00:05:57,160 --> 00:06:00,130
Right. You know, very well-established in the field.

48
00:06:00,550 --> 00:06:05,500
And they're like, but I just don't understand, how could it only be on the genitals, you know, like,

49
00:06:05,950 --> 00:06:11,169
you know, and and you're like, okay, well, let's talk about how it's probably transmitting here.

50
00:06:11,170 --> 00:06:14,350
And they're just like, I don't get it. Virus must have changed.

51
00:06:14,350 --> 00:06:17,829
Like behavior could change. There are all kinds of things that can change.

52
00:06:17,830 --> 00:06:20,889
It's not just the biology of things, right?

53
00:06:20,890 --> 00:06:28,360
So anyways, when we think about health in this class, I want you to make sure you think about all of the different aspects that come into health.

54
00:06:28,450 --> 00:06:32,590
So and that was kind of just the point of that first part of the lecture last night.

55
00:06:34,360 --> 00:06:40,960
So lecture outline for today we've got basic concepts, so I'll get some definitions.

56
00:06:41,830 --> 00:06:49,930
We're going to talk about different causal types. Your causal types will come back again later in the lecture later, and then course start.

57
00:06:50,650 --> 00:06:57,130
And then we're going to talk about for causal models, the suppression component model causes model,

58
00:06:57,190 --> 00:07:03,730
the counterfactual model or the potential outcomes model. It's another name for it directed acyclic graphs.

59
00:07:04,270 --> 00:07:09,399
We'll talk about directing acyclic graphs a little bit in this lecture to give you some background.

60
00:07:09,400 --> 00:07:11,440
I'm going to use them in a bunch of lectures.

61
00:07:12,910 --> 00:07:20,110
You'll get a tiny bit of practice with drawing them, but you're not expected to really be able to draw complicated,

62
00:07:20,110 --> 00:07:24,400
directly directed at these good graphs until after the DAG lecture.

63
00:07:24,940 --> 00:07:31,120
Which happens in the third module. So don't get too worried about that if you don't have a lot of background.

64
00:07:32,110 --> 00:07:40,030
But dogs are super useful for demonstrating the structural relationships between things for life when we talk about confounding.

65
00:07:41,230 --> 00:07:45,459
There will be a lot of dacs for that. And so I want to give you some basics of DAX.

66
00:07:45,460 --> 00:07:56,170
At the beginning of the course, you'll see various AP concepts with DAX, and then we'll bring it all together in the DAG lecture,

67
00:07:56,170 --> 00:08:00,670
which is like kind of a more of almost an in-class activity where you get to draw a bunch of things,

68
00:08:00,670 --> 00:08:08,620
answer bunch of questions, and if you like, usually kind of coming out of that lecture or of that class or just having an activity,

69
00:08:08,980 --> 00:08:18,100
people feel really confident about that and a little bit about that. So we'll also talk about hills viewpoints for causality.

70
00:08:18,100 --> 00:08:22,839
So those are four models we're going to be talking about. So as I said, last one causation, right?

71
00:08:22,840 --> 00:08:26,080
It's really the holy grail of technology. It's what we want to show.

72
00:08:26,620 --> 00:08:30,880
We're usually showing associations and there are a lot of caveats to that.

73
00:08:33,130 --> 00:08:38,530
So causality, what is it? Cause there are a lot of different definitions for causality as well.

74
00:08:38,530 --> 00:08:44,230
For what is a causal. I'm sorry, you're not going to help. You don't have to memorize any of these specific definitions.

75
00:08:44,560 --> 00:08:50,020
I'm not going to say, like, what was Rothman's definition? Right. It's more to just understand the concept.

76
00:08:50,030 --> 00:08:59,740
So it's an antecedent event, condition or characteristic that was necessary for the occurrence of the disease at the moment that it occurred,

77
00:08:59,860 --> 00:09:04,719
given that the other characteristics are fixed. And so what are the important parts of this?

78
00:09:04,720 --> 00:09:11,050
Right, the antecedent event that something that kind of nobody can really argue with, right?

79
00:09:11,260 --> 00:09:14,499
Because that's what I'm talking about, essentially.

80
00:09:14,500 --> 00:09:22,329
Right. And then it's something that was necessary for the event to occur, for the outcome to occur.

81
00:09:22,330 --> 00:09:28,060
Right. That is, without this specific thing, the outcome occurred.

82
00:09:29,020 --> 00:09:34,749
And then it has this other line, given that other characteristics are fixed, kind of acknowledging that there may be other causes,

83
00:09:34,750 --> 00:09:40,960
other things that would ensue, somebody developing, for example, a particular disease outcome.

84
00:09:41,200 --> 00:09:44,620
But we're concerned with this one piece of it that we're looking at.

85
00:09:47,230 --> 00:09:56,070
All right. So an older definition of causality from him, 1748, right.

86
00:09:56,080 --> 00:10:03,879
We may define a clause to be an object followed by another, where if the first object had not been the second never had existed.

87
00:10:03,880 --> 00:10:07,360
Right? Same, same concept. In there we have temporality.

88
00:10:07,780 --> 00:10:11,310
And the second thing won't occur without the first one.

89
00:10:15,500 --> 00:10:23,630
So three criteria for causation that were proposed by our larger bill, 1959,

90
00:10:23,660 --> 00:10:30,170
is that a causal relationship between two variables must have a temporal order in which the cause must proceed.

91
00:10:30,170 --> 00:10:41,030
The effect in time, right? If a is a cause of B, a has to come before B, the two variables should be empirically correlated with one another.

92
00:10:41,720 --> 00:10:45,200
And then the observed empirical correlation between the two variables cannot be

93
00:10:45,200 --> 00:10:51,140
explained away as the result of the third variable that causes both A and B.

94
00:10:52,010 --> 00:10:55,850
In other words, the relationship is not spurious and occurs regularly.

95
00:10:57,740 --> 00:11:01,280
Then we'll talk about that a lot more in talking about confounding.

96
00:11:02,510 --> 00:11:07,970
And so when you think about causes, right, you can think about proximate, intermediate and distal causes.

97
00:11:08,690 --> 00:11:17,000
So epidemiology used to really focus on the proximate causes, the kind of causes, particularly those related to biologic markers of risk.

98
00:11:18,890 --> 00:11:28,970
However, the study of more upstream causes is really useful for population level interventions, for example, social determinants of disease.

99
00:11:30,320 --> 00:11:36,920
Can somebody kind of come up with a disease and an example of a proximate and then either an intermediate or distal cause?

100
00:11:39,280 --> 00:11:43,000
Any disease or any outcome that you're interested in.

101
00:11:44,750 --> 00:11:50,520
Is anybody studying anything? They need to.

102
00:11:57,130 --> 00:12:07,000
Yeah. Great. Exactly. But yeah, you could think of a lot of different things where you could say, like, okay, somebody died, somebody died.

103
00:12:07,360 --> 00:12:13,150
Great reason they died is because they had a heart attack. I think that's the kind of immediate the proximate cause.

104
00:12:13,480 --> 00:12:22,480
Right. Why did they have a heart attack? Well, because they were, you know, maybe obese and had a sedentary lifestyle.

105
00:12:22,480 --> 00:12:26,440
And all these factors were why? Why is that? Right. We'll go back further.

106
00:12:26,920 --> 00:12:30,740
Well, you know, they are low risk.

107
00:12:30,760 --> 00:12:37,629
Yes. They live in a neighborhood where you don't have really good access to food, but higher quality food right there.

108
00:12:37,630 --> 00:12:42,000
No sidewalks or people don't walk anywhere, you know, kind of, etc.

109
00:12:42,010 --> 00:12:50,290
So I'm thinking of that whole, um, causal kind of compliment or, or series that comes together.

110
00:12:51,580 --> 00:12:57,520
So the other thing kind of thinking about causality and some people kind of really one one way versus another, right?

111
00:12:57,790 --> 00:13:01,780
You can think of it as deterministic or as kind of a probabilistic.

112
00:13:02,680 --> 00:13:06,250
So deterministic is really whatever it is leads to the outcome.

113
00:13:06,550 --> 00:13:08,620
You got this. You know the outcome. Right.

114
00:13:09,190 --> 00:13:15,970
And you're saying that you can always predict in a way what will happen if you know enough of the pieces of what's going into it,

115
00:13:16,600 --> 00:13:26,139
the other the other ways to think of it and probabilistic. So X leads to a distribution of possible outcomes or some sort of percent probability.

116
00:13:26,140 --> 00:13:28,090
Right. That that outcome will occur.

117
00:13:29,020 --> 00:13:36,700
All four models can be thought of in either way, like whatever your way of thinking as you combine these off or causal models,

118
00:13:37,360 --> 00:13:41,380
I will say that in epidemiology we kind of get that probabilistic approach right.

119
00:13:43,540 --> 00:13:44,619
So in in epidemiology,

120
00:13:44,620 --> 00:13:53,170
we're going to kind of replace the deterministic concept of conservation or supplemented with the problem with probabilistic methods.

121
00:13:54,310 --> 00:13:58,110
So instead of demonstrating causality and individuals, right,

122
00:13:58,120 --> 00:14:07,979
we're looking at a population and we'll make causal inferences about a hypothesized relationship in a particular population.

123
00:14:07,980 --> 00:14:13,330
And so does that mean it's kind of the end of the deterministic world?

124
00:14:13,330 --> 00:14:17,920
You can never fully predict what's going to happen. Not necessarily.

125
00:14:17,920 --> 00:14:24,190
Right. You can kind of still adhere to that view. That disease occurrence is completely determined by a set of factors.

126
00:14:24,580 --> 00:14:32,710
Uh, but we just don't know all the factors, right? And as you'll see in a lot of these models that fit all the models, when we do them right,

127
00:14:33,250 --> 00:14:39,940
you'll have you unmeasured confounding unmeasured factors right that are contributing to the outcome.

128
00:14:42,790 --> 00:14:46,900
So just kind of quickly to touch on risk factors versus prognostic factors.

129
00:14:47,440 --> 00:14:55,929
Risk factors are an exposure that affects positively or negatively the total number of individuals who develop the disease.

130
00:14:55,930 --> 00:14:57,310
So here's an example, right?

131
00:14:57,790 --> 00:15:08,649
Smoking and lung cancer, a prognostic factor is something an exposure that influences the course of disease after it occurs.

132
00:15:08,650 --> 00:15:13,299
You can see in a cold, you know, the same chemical would be another one.

133
00:15:13,300 --> 00:15:19,870
Right. So risk factors versus causes.

134
00:15:20,470 --> 00:15:27,790
Risk factors are defined at the population level, whereas causes are going to be defined at the individual level.

135
00:15:31,660 --> 00:15:38,140
So when we think about causes and causal types, we've got four different types of people in every population.

136
00:15:38,410 --> 00:15:40,059
And some people can argue for certain things.

137
00:15:40,060 --> 00:15:48,760
They don't think one of these types exist, which may be true, but but in theory, all four of these types exist for every exposure.

138
00:15:49,450 --> 00:15:52,570
I'm sure you've got type one. Those are people who are doomed.

139
00:15:52,990 --> 00:16:00,310
It doesn't matter, right? If they have the exposure or not, whatever they do, they're going to get the disease with or without that exposure.

140
00:16:01,210 --> 00:16:04,750
All right. Type two or the effect is positive.

141
00:16:04,750 --> 00:16:10,150
So for those individuals, if they have the exposure, they develop the disease.

142
00:16:10,810 --> 00:16:15,010
Right. But if they didn't have that exposure, they wouldn't get the disease.

143
00:16:16,270 --> 00:16:19,770
Then we've got type three, whereas the effect is preventative.

144
00:16:21,040 --> 00:16:29,290
So the disease occurs if the person isn't exposed but doesn't occur if the person is exposed.

145
00:16:30,220 --> 00:16:39,549
And then type four are the immune individuals, the disease does not occur regardless of whether or not they have the exposure.

146
00:16:39,550 --> 00:16:45,400
And so they have it. They don't they're not going to get the disease or our outcome of interest.

147
00:16:47,770 --> 00:16:54,510
All right. So here are our causal types again. Right? We've got conflict here, right?

148
00:16:54,790 --> 00:16:58,030
We've got our type one. Right. Those are those doomed individuals.

149
00:16:58,750 --> 00:17:01,720
They're going to be a case. They're going to have the outcome of interest.

150
00:17:02,020 --> 00:17:05,290
If they're exposed, they'll have the outcome of interest if they're unexposed.

151
00:17:05,800 --> 00:17:15,910
Type two are affect positive. They will have the outcome of the interest if they're exposed, but they won't have it if they're not exposed.

152
00:17:16,210 --> 00:17:19,570
Type three effect preventative, right.

153
00:17:19,570 --> 00:17:23,080
They won't have the outcome of interest if they're exposed.

154
00:17:23,920 --> 00:17:30,010
But if they're unexposed, if they don't have that exposure, they will. And then take four know of the act or the immune individuals.

155
00:17:30,010 --> 00:17:34,450
They will not have the outcome regardless of whether or not they're exposed.

156
00:17:36,340 --> 00:17:41,410
So in this particular example, right, or when we're thinking about these people in a population,

157
00:17:42,580 --> 00:17:46,780
you actually only really see an effect of the exposure in types two and three.

158
00:17:46,960 --> 00:17:50,860
Right? Because for type one and four, it doesn't matter what their exposure status is.

159
00:17:52,420 --> 00:17:56,979
So the thing about these causal types are we don't actually need to observe

160
00:17:56,980 --> 00:18:02,440
that right for pretty much anything because we don't get to rewind time and,

161
00:18:02,620 --> 00:18:07,330
you know, take somebody and expose them to cigaret smoke for 20 years.

162
00:18:07,660 --> 00:18:14,500
Right. Rewind it, go backwards and then take them and have them not smoke for that 20 year period and see what their outcome would be.

163
00:18:18,830 --> 00:18:22,850
So when we have somebody who's exposed, right.

164
00:18:23,240 --> 00:18:29,390
And becomes a case, we actually don't know if they're type one and type two.

165
00:18:31,050 --> 00:18:40,140
All right. And if we have an unexposed person who becomes a case, right, we don't know if they're type one or type three.

166
00:18:44,610 --> 00:18:49,290
So we're kind of always mixing these different causal types together.

167
00:18:50,220 --> 00:18:53,700
Well, there are certain methods to try to sort them out, but in epidemiology,

168
00:18:53,820 --> 00:18:57,959
if you're doing more basic associations and obviously actually these consultation

169
00:18:57,960 --> 00:19:01,470
together and you can't actually figure out who really belongs to you.

170
00:19:05,880 --> 00:19:11,100
So kind of in thinking about this, right, it means that when we are looking at a population,

171
00:19:11,100 --> 00:19:15,150
right, and we have a certain proportion of people that were exposed in the population.

172
00:19:15,930 --> 00:19:21,840
We can't say how many cases that occur or actually occur because of the disease.

173
00:19:22,470 --> 00:19:25,330
Because of the exposure. Sorry. All right.

174
00:19:26,290 --> 00:19:34,090
So those are all causal types or causal types will also come back, particularly when we're talking about measures of association.

175
00:19:35,740 --> 00:19:38,800
So several ways of looking at causation.

176
00:19:39,070 --> 00:19:42,860
Right. We've got the sufficient component cause model, the counter frame,

177
00:19:42,970 --> 00:19:49,240
factual framework or the potential outcomes model directly, cyclic graphs and those criteria.

178
00:19:49,240 --> 00:19:52,870
And we'll go through those. I should actually change that to those few points.

179
00:19:54,600 --> 00:20:00,990
The slides kind of sneak in there. So why do we have several different models of causation and epidemiology?

180
00:20:03,600 --> 00:20:09,809
It's really because each model has a number of strengths, right? And then they really help us think about different epidemiologic concepts.

181
00:20:09,810 --> 00:20:15,550
And so when you're thinking through problems, like when at writing a grant, for example, putting a study together,

182
00:20:15,570 --> 00:20:21,809
putting an analysis together, you may use components of all of these different causal models.

183
00:20:21,810 --> 00:20:24,150
And I certainly do like I will draw back,

184
00:20:24,630 --> 00:20:33,960
I will kind of think about those pies when I'm thinking about courses and we'll go through those in a moment and thinking about studies.

185
00:20:34,350 --> 00:20:43,260
All right. So this efficient component, cost model, also called Rothmans Pies or Rothmans causal pie model is a mechanistic model.

186
00:20:44,040 --> 00:20:48,330
So that so each pie here. So here we've got three pies, right?

187
00:20:48,810 --> 00:20:53,520
Each pie, it represents kind of a theoretical causal mechanism,

188
00:20:54,060 --> 00:20:58,650
meaning that you could have whatever this factor is and you get the outcome that we're looking at.

189
00:20:59,400 --> 00:21:06,600
You could have this, Judy, or there's some portion of the population that does and they get that outcome or B,

190
00:21:06,600 --> 00:21:10,260
s and G, and they also get that outcome of interest.

191
00:21:10,530 --> 00:21:17,430
So in theory, you should draw all the different pies that you think could lead to your particular outcome of interest.

192
00:21:19,620 --> 00:21:21,719
So there's some definitions that you need to know here.

193
00:21:21,720 --> 00:21:29,640
A sufficient cause is a complete causal mechanism, a minimal set of conditions and events that are sufficient for the outcome to occur.

194
00:21:29,910 --> 00:21:33,390
Right. Each pie is going to be a sufficient cause.

195
00:21:35,040 --> 00:21:39,300
It's important to know that there's almost always more than one pie purposes, right?

196
00:21:39,600 --> 00:21:43,380
If none of the pies occur, then the disease should not occur.

197
00:21:45,300 --> 00:21:49,740
And usually our pies will include unknown causes.

198
00:21:50,190 --> 00:21:56,639
Right? Because not everybody that has all of those components, you know, if you're looking at three things, will develop the disease.

199
00:21:56,640 --> 00:21:59,160
And so that we kind of put an unknown slice in there to say like,

200
00:22:00,000 --> 00:22:05,310
we don't know why some people who have these components do get the disease or don't get the disease.

201
00:22:06,540 --> 00:22:10,349
So here's an example with TV, right?

202
00:22:10,350 --> 00:22:19,739
So some people who are exposed to TB, who have no known risk factors that we all will develop TB But Judy is one of those diseases,

203
00:22:19,740 --> 00:22:27,260
right, that most people who are exposed to that kind of develop disease or maybe they don't develop disease much later.

204
00:22:28,790 --> 00:22:32,830
You know what? So here we go.

205
00:22:32,840 --> 00:22:37,549
We've got these people, that exposure to TV. I know they're young, they're healthy.

206
00:22:37,550 --> 00:22:41,210
They live in the conditions. Right. None of the no risk factors.

207
00:22:42,140 --> 00:22:45,740
Then we have another pod here where they're exposed to TB.

208
00:22:47,180 --> 00:22:54,170
The individual has AIDS, so they're aging positive. It's not being controlled medication, access to those medications.

209
00:22:54,710 --> 00:23:00,980
And they also have poor nutrition. Right. And so these are two well-established risk factors for developing.

210
00:23:03,660 --> 00:23:07,970
We've got another individual here or another group of people. Right, that get exposure to TV.

211
00:23:07,980 --> 00:23:17,520
They are poor nutrition. They live in crowded conditions and poor ventilation that most certainly are risk factors for developing TB.

212
00:23:18,390 --> 00:23:27,000
All right. So some definitions, a component cause is an event or condition that plays a necessary role in the occurrence of some cases of the disease.

213
00:23:27,780 --> 00:23:32,220
So, for example, here, AIDS is a component cause in this time, right?

214
00:23:36,990 --> 00:23:43,710
Unnecessary causes and a mental condition that plays a necessary role in the occurrence of all cases of a given disease.

215
00:23:44,130 --> 00:23:50,310
It must be present for the disease to occur. What is a necessary posture?

216
00:23:51,510 --> 00:23:56,610
Tuberculosis exposure. Exposure to the bacterium when you're not going to develop.

217
00:23:56,610 --> 00:24:03,709
Tuberculosis if you don't have that. And then the causal complement or the other factors.

218
00:24:03,710 --> 00:24:14,730
So if we're looking at AIDS, attrition and exposure to B to TB are the causal complement for for developing TB in this particular high.

219
00:24:16,530 --> 00:24:23,160
So there are the other factors which are necessary and sufficient when you put them all together for a factor to produce disease.

220
00:24:26,960 --> 00:24:32,120
So I used to talk about Grenfell Towers, but I feel like we're a little bit far away from that as well.

221
00:24:32,390 --> 00:24:36,220
Like when I first taught this course, there was, um.

222
00:24:36,470 --> 00:24:42,630
That was right after that we happened to remember Grenfell Towers in the UK.

223
00:24:42,730 --> 00:24:46,280
Um, there weren't enough assets and faulty wiring.

224
00:24:46,700 --> 00:24:50,090
There was some thoughts that perhaps it had to do with the cladding.

225
00:24:50,440 --> 00:24:54,489
On the outside of the building, there was a fire. It was a mass casualty event.

226
00:24:54,490 --> 00:24:58,120
A lot of people died and people were also told to stay strong.

227
00:24:58,510 --> 00:25:01,780
And then I would kind of go through that. But I feel like we're kind of far from that.

228
00:25:04,060 --> 00:25:09,810
So I'm going to talk about a car accident that I had in 2020, kind of.

229
00:25:09,820 --> 00:25:16,690
Right. Actually, there it was during the shutdown, like in May of 2020.

230
00:25:16,690 --> 00:25:20,489
And I hadn't had a car accident in like 20 years.

231
00:25:20,490 --> 00:25:24,729
So knock on wood, I hope I didn't have a car accident again any time in the near future.

232
00:25:24,730 --> 00:25:32,620
Right. So I just met you know, I spent a lot of time kind of thinking about that car accident and what contributed to that car accident,

233
00:25:32,620 --> 00:25:38,920
because the car accident was my fault. The other person was very nice about it and nobody was hurt, thankfully.

234
00:25:40,480 --> 00:25:47,590
But when you think about it and think about one write up and I can make them apply right to think about that car accident.

235
00:25:47,600 --> 00:25:52,149
Is that feeling just like I didn't actually think like what factors would into this?

236
00:25:52,150 --> 00:25:55,870
Like, why did this happen? How can I prevent it from happening again?

237
00:25:56,350 --> 00:25:59,620
So I thought that there was a pretty big issue with the road design.

238
00:26:00,580 --> 00:26:04,660
So this is my way of saying that tire my fault, right?

239
00:26:05,080 --> 00:26:14,260
What happened was I was like driving down Washtenaw and I was going to turn into a set of stores on the left hand side,

240
00:26:14,530 --> 00:26:20,140
and they just changed the bus stop, which is now gone back and changed because there were a lot of accidents at that site.

241
00:26:20,440 --> 00:26:27,069
And the way they changed it, it made it look like it was a turn into the stores, at least for me.

242
00:26:27,070 --> 00:26:31,870
Looking quickly, I was like, Oh, okay, you know, when I go to turn and then I stopped because I was like,

243
00:26:32,020 --> 00:26:36,790
wait, that's not, you know, like that's a bus stop. I can't actually turn in another car.

244
00:26:37,630 --> 00:26:44,410
Right. One of the things that made me feel slightly better about this for a quote also bad was that when the guys don't take them

245
00:26:44,770 --> 00:26:54,580
like a position that you're then she also went to turn into the bus stop then go in the car was like cheat accident,

246
00:26:54,670 --> 00:26:57,100
you know, accident that's occurring at the site. Right.

247
00:26:57,100 --> 00:27:03,790
So she was like, I would just like to turn it in because like I know we just experienced that, right?

248
00:27:04,840 --> 00:27:10,600
So anyways, so there was a road design issue I think, and they had actually changed it because there were enough accidents at that site.

249
00:27:11,710 --> 00:27:12,940
I was also really tired.

250
00:27:13,600 --> 00:27:22,720
I had been working like 80 hours a week on as far as could be too and I'm a single moms and trying to do like virtual schooling at the same time.

251
00:27:23,110 --> 00:27:27,760
So I was like, I'm not getting more sleep, I can get in and this is not practical.

252
00:27:28,270 --> 00:27:33,099
So that probably played into it. Maybe I would have noticed that it was a bus stop, or maybe I would have.

253
00:27:33,100 --> 00:27:39,730
I just paused when I stopped the car. Maybe I would have quickly realized I should get the gas and get out of there before somebody, you know, hit me.

254
00:27:40,690 --> 00:27:48,969
That's looking like I was distracted. I was thinking a lot about life and what I was doing and was I doing enough on the COVID side?

255
00:27:48,970 --> 00:27:52,570
Was I doing enough for my children, you know, etc.?

256
00:27:53,500 --> 00:27:59,709
And it was kind of like it was a morning. It was a little bit foggy. I mean, maybe that played a role and then unknown factor.

257
00:27:59,710 --> 00:28:08,020
So there you go. Me thinking through this pie and and so this is kind of the pie I would maybe draw, but maybe, maybe fog had nothing to do with that.

258
00:28:08,020 --> 00:28:13,330
So, you know, if you think about something and you think, oh, maybe that wasn't really it, maybe it was speed, right?

259
00:28:13,810 --> 00:28:18,280
Like the other guy said, I didn't even see I said, well, maybe he was speeding.

260
00:28:18,280 --> 00:28:21,550
I don't know. Still not was my fault. But let's go back to our model.

261
00:28:22,240 --> 00:28:26,350
Okay. So you can, you know, kind of think about this.

262
00:28:26,350 --> 00:28:31,960
So there are some real utilities on this model, I would say.

263
00:28:32,320 --> 00:28:38,650
Right. It really incorporates and multifactorial causality makes you think about all the factors that came in because

264
00:28:38,980 --> 00:28:45,970
your outcome of interest makes you think about what are necessary and sufficient causes to have that outcome.

265
00:28:46,390 --> 00:28:50,560
And it's also really good for looking at strength of association attributable percentages,

266
00:28:50,560 --> 00:28:54,370
which we'll talk about in the classroom the next few lectures and effect modification.

267
00:28:54,820 --> 00:28:57,580
More to come on that in module two.

268
00:28:59,320 --> 00:29:05,290
So going back to this model, when we think about strength of association, I would say instead of just looking at me,

269
00:29:05,440 --> 00:29:09,550
we're looking at a big population like everybody or everybody in Ann Arbor.

270
00:29:09,700 --> 00:29:20,139
There were a number of accidents and that's up right. And we've got the road design is like but I think it would have been better if I put speed here.

271
00:29:20,140 --> 00:29:23,110
Let's pretend I put speed here instead of five. Right?

272
00:29:23,110 --> 00:29:30,310
So imagine that, you know, that that kind of situation occurs in 5% of the other drivers are speeding.

273
00:29:30,910 --> 00:29:35,320
Okay. Or in an alternate reality, 50% of drivers are speeding.

274
00:29:35,860 --> 00:29:45,850
Where would the strength of association between the road design and traffic accidents be greater where 5% of people are speeding or 50% because you.

275
00:29:51,290 --> 00:29:55,330
Where would an accident be more likely to happen? 50%, right.

276
00:29:55,340 --> 00:29:58,880
So that's how these is going to help you think about the strength of associations.

277
00:29:59,660 --> 00:30:11,290
Okay. So some criticisms of the sufficient component model, cost model, it does not depict sequential mechanisms or direct versus indirect effects.

278
00:30:11,300 --> 00:30:18,830
We're putting them all together in one pie. Right. And it's more useful on a conceptual basis than in kind of real application.

279
00:30:18,830 --> 00:30:24,530
When you start to think about how you're going to do your analysis, you will probably draw back or I certainly hope you will draw that.

280
00:30:28,010 --> 00:30:35,210
All right. So an example here, assume that smoking has only has an effect on birthweight when maternal weight gain is low,

281
00:30:36,470 --> 00:30:42,290
are smoking and weight gain sufficient causes or component causes of the same sufficient cause?

282
00:30:47,870 --> 00:30:51,649
Individuals smoking and weight gain. Are they individually sufficient causes?

283
00:30:51,650 --> 00:30:56,130
Are they component causes of the same sufficient cause? Think.

284
00:31:02,500 --> 00:31:07,060
There are going to be component causes of the same sufficient cause, right?

285
00:31:07,480 --> 00:31:13,030
Because smoking only has an effect on birth weight.

286
00:31:13,030 --> 00:31:17,950
That's our operative interest. When maternal weight gain is low in this particular example.

287
00:31:18,340 --> 00:31:23,010
Right. So they're going to be in one place together. All right.

288
00:31:23,160 --> 00:31:30,780
So component causes of the same sufficient cost. So this is probably a good moment for I believe I'm looking at my GSI now.

289
00:31:31,860 --> 00:31:36,209
We post student slides. Usually the student slides do not have the answers.

290
00:31:36,210 --> 00:31:40,800
And the reason for this is, is because if you see the answer, you go, yeah, I knew that.

291
00:31:41,340 --> 00:31:47,129
Right? Whereas if you don't see the answer, then you think about it and the class time is actually more valuable for you,

292
00:31:47,130 --> 00:31:57,660
and you also realize things that you aren't understanding. But the full lecture slides with the answers will be up and available after class.

293
00:31:58,110 --> 00:32:06,419
Right? So we post the student lecture before class for one after, and occasionally we mess up and put the answers in.

294
00:32:06,420 --> 00:32:09,960
But we do try to make sure that we have those student versions up.

295
00:32:10,830 --> 00:32:19,180
All right. So that was our one cosmology questions, the kind of.

296
00:32:26,700 --> 00:32:31,800
So here, one thing we're missing is we would want to have a bunch of guys.

297
00:32:32,240 --> 00:32:38,660
Right. I have one with me. Maybe the accidents would happen even if it wasn't designed that way.

298
00:32:38,930 --> 00:32:43,280
Right. And so we have to draw it out and then then we can get to work on this.

299
00:32:45,380 --> 00:32:49,090
I haven't really done this kind of out of population. Or you.

300
00:32:50,650 --> 00:32:56,660
Big recollections. Pie charts are terrible for anyone.

301
00:32:58,450 --> 00:33:01,710
Controversy over the Army's controversy over that.

302
00:33:01,720 --> 00:33:06,220
I think it's great to get multiple opinions on these things and then you have to make up your own mind.

303
00:33:07,060 --> 00:33:12,040
I actually think the hybrids are still really helpful in thinking about how all these factors come together.

304
00:33:12,520 --> 00:33:15,700
Right. So I think it's a helpful causal model.

305
00:33:16,570 --> 00:33:19,860
But is this evidence 600 watts?

306
00:33:19,930 --> 00:33:30,520
Yeah. So you'll notice. And when I disagree, they, they tend to be things where there's some disagreement in in the kind of general effort world.

307
00:33:30,550 --> 00:33:36,790
I didn't actually know that. He said the pictures were terrible. I mean, I know that there are things that we disagree on actually point it out.

308
00:33:37,280 --> 00:33:46,240
I will say, like I'm telling you this, like 100, you were told that, you know, and you decide where you fall on the issue.

309
00:33:47,050 --> 00:33:52,660
I think that the critique of pie charts is like if this that this PI was representing

310
00:33:52,660 --> 00:33:57,100
quantitative data and I had it and I'm a human and I'm thinking about what's this APR?

311
00:33:57,670 --> 00:34:02,860
What's the percentage of real design that's really hard to do? So you should do that bar chart, right?

312
00:34:02,870 --> 00:34:09,580
Because it's very easy. But like, conceptual, like just as a conceptual tool, do something.

313
00:34:09,940 --> 00:34:11,620
Yeah. And you can like I,

314
00:34:11,890 --> 00:34:19,550
I don't think people doing these slides ever meant that the exact size resonance and like would if like one this is a bigger factor.

315
00:34:19,580 --> 00:34:22,389
There's a stronger, you know, association,

316
00:34:22,390 --> 00:34:29,620
you can make it a bit bigger and you should be like measure them out and think that that's the contribution or the strength of each one.

317
00:34:29,620 --> 00:34:35,770
Right. So. But yeah, I agree. But this is I mean, this is like you're not going to use it in analysis.

318
00:34:36,820 --> 00:34:42,850
I still do think you will use that when you're writing a book after setting up a study and kind of thinking about everything that you,

319
00:34:43,270 --> 00:34:49,690
you know, hopefully should be collecting data on and looking at, you'll also then really use a DAG for that as well.

320
00:34:52,030 --> 00:34:56,550
But Dacs have their issues to answer so.

321
00:34:59,510 --> 00:35:08,030
So even though some people think you shouldn't use the of, you still need to know what a sufficient causes the causal component of that kind of thing.

322
00:35:08,140 --> 00:35:11,140
So we will ask some questions on that. Um.

323
00:35:13,990 --> 00:35:20,620
Okay. Counterfactual or the potential outcomes model also called the Rubin Causal Model and outcomes model.

324
00:35:21,310 --> 00:35:30,610
Then David Rubin counterfactual framework really asking this question of what to write on the counterfactual, contrary to the fact,

325
00:35:31,030 --> 00:35:39,590
what if we could see what would have happened had this exposure occurred or not occurred, depending on what that kind of effect actually is on?

326
00:35:42,150 --> 00:35:47,300
So the counterfactual framework is this kind of idea of an idea like this idea of experiment, right?

327
00:35:47,310 --> 00:35:52,500
Where you take a person, they're exposed to the exposure of interest,

328
00:35:52,500 --> 00:35:59,370
you're going to roll back time and now they're not going to be exposed and you're going to follow them forward in time and see what happens.

329
00:36:03,170 --> 00:36:06,980
So question exposed here. We roll back the clock.

330
00:36:07,580 --> 00:36:15,889
They're not exposed. So there are a bunch of different examples of counterfactuals in the Middle East when it out to me that these are a little dated,

331
00:36:15,890 --> 00:36:19,970
but then we weren't coming up with exact like perfect. None of these are perfect anyways.

332
00:36:20,750 --> 00:36:23,840
The one I always used to kind of think about was Groundhog Day.

333
00:36:24,440 --> 00:36:27,440
But I'm figuring most of you probably haven't seen Groundhog Day, right?

334
00:36:27,440 --> 00:36:29,989
But that was like Bill Murray and he's a weatherman,

335
00:36:29,990 --> 00:36:36,979
and he wakes up and he repeats the same day and he's aware of the fact that he's repeating the same day, but nobody else's environmental comedy.

336
00:36:36,980 --> 00:36:41,210
So whatever her name is, falls in love with him. Right.

337
00:36:41,930 --> 00:36:46,399
But anyways, these are often examples where people are repeating the same thing.

338
00:36:46,400 --> 00:36:48,860
Right, and trying to change the outcome.

339
00:36:50,030 --> 00:36:59,310
It's another example of sort of is one of the Harry Potter's writers in Harry Potter where her mind can change time.

340
00:37:00,440 --> 00:37:00,770
Yeah.

341
00:37:01,340 --> 00:37:09,110
And then you have one up, several hanging by examples where Avengers endgame for them going back in time and realizing they have the return of stone,

342
00:37:09,110 --> 00:37:14,810
that was like most everything up. And if anyone watched the Atom project on Netflix.

343
00:37:16,780 --> 00:37:23,829
And all of them are kind of probably slightly imperfect because the people know what's going on right and what they're trying to do for all of them.

344
00:37:23,830 --> 00:37:29,590
But anyways, that's kind of examples of counterfactuals and maybe so counterfactual framework.

345
00:37:30,070 --> 00:37:37,870
We've got this observed person. In this particular example, they don't take any aspirin and they get a headache.

346
00:37:38,230 --> 00:37:45,930
Right. And so what you would like to know, what you'd like to see is the counterfactual where that same person at that same time point,

347
00:37:45,940 --> 00:37:53,080
maybe they're waking up in the morning, they take aspirin now and then we see that they get a headache or not.

348
00:37:57,560 --> 00:37:59,150
So for the counterfactual framework,

349
00:38:00,110 --> 00:38:05,870
one of the really great things about the counterfactual framework is it really forces you to think about the exposure,

350
00:38:06,380 --> 00:38:09,740
what is exposed, what is not exposed, right?

351
00:38:09,830 --> 00:38:16,610
We tend to go kind of like smoking this sort of air pollution, how it didn't have it right.

352
00:38:16,620 --> 00:38:25,790
But like, in fact, there are all kinds of levels of these exposures and think very carefully about exactly what the exposure was for my.

353
00:38:28,940 --> 00:38:32,809
Car example from the thing before know there was road design.

354
00:38:32,810 --> 00:38:38,690
Very vague, right? I could see exactly what it was. And what would the counterfactual of that be?

355
00:38:38,960 --> 00:38:47,810
Right. Like what is the other design? Oh, counterfactual framework leads to a number of analytical methods or structural equation modeling,

356
00:38:47,810 --> 00:38:50,810
particularly like non parametric structural equation modeling.

357
00:38:51,030 --> 00:38:55,429
Yeah, marginal structural models are causal mediation analysis.

358
00:38:55,430 --> 00:39:01,220
What I will tell you is we do not go into that in this class. However, each 24.

359
00:39:04,330 --> 00:39:08,630
Students visit A24. Yes, A24.

360
00:39:08,710 --> 00:39:14,230
You will go into those topics in depth if you take that course and there are other courses as well.

361
00:39:14,260 --> 00:39:17,530
Right. And bio starts and across campus from where you can get into them more.

362
00:39:18,730 --> 00:39:22,270
All right. So some notation for the individual causal effect.

363
00:39:22,900 --> 00:39:28,000
If we consider a binary treatment cheat, we've got zero and one.

364
00:39:28,000 --> 00:39:33,310
So no treatment and treatment, right. Each individual has two potential outcomes.

365
00:39:37,870 --> 00:39:43,120
So they're going to have a potential outcome for each treatment value of zero and one, right?

366
00:39:45,250 --> 00:39:50,350
A potential outcome is the one that would be realized if the individual received a specific value of the treatment.

367
00:39:50,710 --> 00:39:59,020
So here we've got y one, right? The particular outcome with treatment equal to one, that is, they get it y zero.

368
00:39:59,020 --> 00:40:06,640
The outcome of the treatment equal to zero. So for each individual, one can generally only observe one of these that either get it or they don't.

369
00:40:06,760 --> 00:40:11,650
Right. The unobserved outcome is called the counterfactual.

370
00:40:14,800 --> 00:40:22,420
So the individual causal effect is going to be the difference between y one and y zero, right?

371
00:40:22,420 --> 00:40:31,720
Y one -5.0, or the difference between that that one individual's to outcomes with and without exposure.

372
00:40:32,860 --> 00:40:40,120
Okay. So if we look at the aspirin example, y zero is the outcome of the person did not take aspirin, right?

373
00:40:40,480 --> 00:40:49,480
Y one is the outcome with aspirin. I really just like this classes, by the way, that's neither here nor there, I suppose.

374
00:40:49,480 --> 00:40:53,889
Think it's also going to make it. I guess it's okay to write on the board.

375
00:40:53,890 --> 00:40:57,580
I usually write on the board a lot, but, uh. Let's see.

376
00:40:58,330 --> 00:41:03,400
So potential outcomes. We've got a few different potential outcomes, right?

377
00:41:03,700 --> 00:41:06,970
So we've got y zero. But don't take aspirin.

378
00:41:07,570 --> 00:41:11,320
You have the same outcome as y one. If they do take aspirin, right.

379
00:41:11,320 --> 00:41:14,530
So y want zero because y one which equals zero.

380
00:41:14,770 --> 00:41:18,970
Right. Was saying that no headache, regardless of aspirin, there's no effect.

381
00:41:19,870 --> 00:41:29,350
Another potential outcome is once again right here, it's the same regardless of whether you do or don't take aspirin, you get a headache.

382
00:41:30,310 --> 00:41:34,670
Right. What causal type would this be doomed doing?

383
00:41:34,720 --> 00:41:37,870
Those are doomed individuals, right? These are immune individuals up here.

384
00:41:38,800 --> 00:41:43,810
And then we have people where if they don't take aspirin, their outcome is one.

385
00:41:43,810 --> 00:41:47,620
So they get that headache. If they do take aspirin, their outcome is zero.

386
00:41:48,010 --> 00:41:52,809
So they don't get the headache. So a headache without aspirin, but no headache with aspirin.

387
00:41:52,810 --> 00:41:56,170
A positive effect. Um, what group is that?

388
00:41:56,620 --> 00:42:01,060
The more causal types, the effect causative right.

389
00:42:01,990 --> 00:42:05,350
Are the exposure positive side type.

390
00:42:06,330 --> 00:42:19,299
To. And then we've got people where if they are not exposed, they won't get a headache.

391
00:42:19,300 --> 00:42:25,000
But if they take aspirin, they will get a headache. Right. So here it is having a negative effect.

392
00:42:25,990 --> 00:42:30,280
Okay. And that's so we're thinking about this right now, like this is for an individual, right?

393
00:42:31,150 --> 00:42:37,420
But we're usually looking at population. So what we're concerned about is the population causal effect, the average causal effect.

394
00:42:39,700 --> 00:42:44,890
So causal effects for individuals are going to be different across different individuals.

395
00:42:46,210 --> 00:42:53,080
So we're going to have effect heterogeneity. Um, meaning if everybody takes aspirin, it doesn't mean everybody's going to have the same outcome.

396
00:42:54,520 --> 00:42:57,270
So the population effect is going to be here.

397
00:42:57,640 --> 00:43:07,810
This is kind of the, the expectation of y zero expectation of y 1/2 about the outcome without aspirin and then the outcome with aspirin.

398
00:43:08,290 --> 00:43:19,960
Right. And then the population causal effect will be this expectation of y one minus expectation y zero or eyy1 -0.

399
00:43:20,320 --> 00:43:23,110
So here we're looking at the difference in the entire population.

400
00:43:23,890 --> 00:43:32,260
If the entire population took aspirin, minus if the entire population did not take aspirin, that's what we're looking at there.

401
00:43:35,420 --> 00:43:35,650
Okay.

402
00:43:36,130 --> 00:43:45,950
One of the fundamental problems of the causal inference that we run into is we don't actually usually get the factual right, but that's what happens.

403
00:43:47,620 --> 00:43:54,970
And so we we don't typically directly observe the causal effects either on an individual or a population level,

404
00:43:56,470 --> 00:44:01,960
because we're never going to observe the outcomes resulting from both exposure or absence of exposure in the same individual.

405
00:44:02,170 --> 00:44:08,750
There are some exposures where, you know, they have a particular time frame, right,

406
00:44:08,770 --> 00:44:12,759
where you'd expect to have the effect and then you actually can look at those.

407
00:44:12,760 --> 00:44:15,850
But this is kind of generally that's not the case.

408
00:44:15,850 --> 00:44:19,209
And even with that, you're still not looking at that exposure in that same moment.

409
00:44:19,210 --> 00:44:21,500
There may be other factors that come into play. Right.

410
00:44:21,520 --> 00:44:27,909
But they're study designed to talk about later in class where you look at an individual, you know, say they had a road accident.

411
00:44:27,910 --> 00:44:35,390
That's one example. Right. And you could look at their cell phone usage right before the the road accident.

412
00:44:35,410 --> 00:44:40,750
You can look at that for a population of people who had traffic accidents from state of Michigan and whatever these,

413
00:44:40,750 --> 00:44:46,660
you know, like in the last six months or injury traffic accidents, for example, in the state of Michigan.

414
00:44:46,960 --> 00:44:50,960
And you could then actually, you know, go back to their cell phone records and, you know,

415
00:44:51,010 --> 00:44:53,799
certain things, you know, where people are, you know, if they're driving or not.

416
00:44:53,800 --> 00:44:59,530
And you could look, you know, on the same day, you know, for the weeks prior or the weeks forward,

417
00:44:59,540 --> 00:45:05,410
kind of like did they use their phone or did they have a, you know, a car accident to complain, to compare them to themselves?

418
00:45:06,070 --> 00:45:08,559
But that's still not that exact same moment. Right.

419
00:45:08,560 --> 00:45:13,450
So we still don't know exactly what would have happened had they not used their cell phone or had they

420
00:45:13,450 --> 00:45:20,230
been using their cell phone if they weren't using it at the time that that car accident occurred.

421
00:45:20,590 --> 00:45:24,520
All right. So for my car accident, my other car accident example,

422
00:45:24,520 --> 00:45:30,310
my own car accident with the accident would have occurred if not for the road design, I think it wouldn't have.

423
00:45:30,490 --> 00:45:37,830
But, you know, so what is the causal effect of the roads, much less top design compared to another design?

424
00:45:38,770 --> 00:45:44,709
So we've got for me, because it's just one person, individual causal, why one might ask why zero?

425
00:45:44,710 --> 00:45:50,350
So the difference between the drive outcome accident or not that the road bus stop design and with a different design.

426
00:45:50,590 --> 00:45:52,520
One of the things that that you know, like I said,

427
00:45:52,550 --> 00:45:59,320
makes you think about the exposure or what is that different design and actually figure out what the counterfactual would be.

428
00:46:00,130 --> 00:46:07,230
It's not any other design. Now I've got to think about what no exposure is here, so maybe I would, you know,

429
00:46:07,300 --> 00:46:15,400
put up a curve and paint it bright yellow or something like that would be one an example individual causal effect for the car accident.

430
00:46:15,400 --> 00:46:21,490
So we know the outcome. Right, had had car accident. We're going to roll back the clock.

431
00:46:21,610 --> 00:46:28,389
We're going to use a different design at that spot. And then we're going to observe the outcome from my same drive.

432
00:46:28,390 --> 00:46:31,570
Right, driving into work and stopping by the store.

433
00:46:32,320 --> 00:46:41,950
All right. So then when we look at population causal effects, right, and we've got our other notation here, expectation of Y one minus y zero,

434
00:46:42,640 --> 00:46:46,930
it's the difference between the outcome when the entire population of road slash response

435
00:46:47,260 --> 00:46:53,440
has one design compared with the entire population of road roads having a difference.

436
00:46:54,130 --> 00:46:58,150
But same rate for all of them. Design, right? That's the counterfactual.

437
00:46:59,470 --> 00:47:03,040
So some of the utilities and strengths and we're going to take a break after

438
00:47:03,040 --> 00:47:08,199
we finish the counterfactuals utility strengths of counterfactual framework.

439
00:47:08,200 --> 00:47:11,460
It requires a specific definition of exposure.

440
00:47:11,480 --> 00:47:14,500
You really need to think about what that exposure is, right?

441
00:47:14,500 --> 00:47:23,260
And what the counterfactual to that exposure is leads to more precise definitions for measure of association, confounder and modification.

442
00:47:24,160 --> 00:47:28,510
And this led to a number of quantitative analytic methods.

443
00:47:28,510 --> 00:47:36,870
So when thinking about the exposure, right, for example, air pollution, you might want to examine the effect of heavy air pollution.

444
00:47:36,880 --> 00:47:40,300
You actually have to think like what is heavier air pollution? How am I going to define that?

445
00:47:40,300 --> 00:47:45,340
Right. And of course, there's probably nobody that has no exposure to air pollution.

446
00:47:45,730 --> 00:47:54,280
So now you have to think of what your counterfactual is going to be for those heavy air pollution people.

447
00:47:54,580 --> 00:48:00,460
Like, what would you compare it to? Was it fair to compare to for a counterfactual situation?

448
00:48:02,550 --> 00:48:10,860
All right. So criticisms and limitations of this model is you cannot portray mechanism mechanics of causal interactions.

449
00:48:11,880 --> 00:48:17,130
There's challenges around some exposures is female the counterfactual for male.

450
00:48:20,960 --> 00:48:31,460
Right. Hard to say on that one, right? Probably not. But you might think it is a purchase thinking from an experimental or intervention lens.

451
00:48:32,480 --> 00:48:36,320
And that may not be useful for all circumstances.

452
00:48:36,320 --> 00:48:45,860
Right? Because we can't we can't change everything. So use of the counterfactual, we can think about top actuals as missing data.

453
00:48:46,280 --> 00:48:51,290
That's kind of nice. We are trying to estimate the effect in this group that we don't have data on.

454
00:48:51,410 --> 00:48:55,760
And so we use a comparison group when comparing groups of exposed and unexposed.

455
00:48:56,600 --> 00:49:01,999
We're going to want the unexposed group to be as close to the counterfactual group as possible.

456
00:49:02,000 --> 00:49:09,980
So we want them to be kind of exactly the same as close as we can get anyways to the group that has the exposure, just minus that exposure.

457
00:49:11,090 --> 00:49:19,160
And with that, we're going to take a ten minute break and start right back on the hour.

458
00:49:30,090 --> 00:49:43,284
Yeah. Although I actually forget the pause because I like the break for me.

459
00:49:52,704 --> 00:49:59,573
All right, so we'll get started back up. So it's changeability, right?

460
00:49:59,574 --> 00:50:03,954
We've got this. So you pointed out. Try it and see.

461
00:50:04,854 --> 00:50:08,154
Right. Formally. Oh, that did not work.

462
00:50:08,724 --> 00:50:18,044
Okay. I'll play with this one when you guys are here sometime so I can figure out how to use the stylus formally, why a is independent of it.

463
00:50:18,054 --> 00:50:27,534
That's where we have extreme ability. So the potential outcomes under exposure A is independent of the exposure actually them.

464
00:50:28,014 --> 00:50:33,234
Right. And because this independence is not conditional in this particular formula.

465
00:50:33,414 --> 00:50:43,554
Right. This is known as marginal exchange ability. Okay.

466
00:50:43,584 --> 00:50:47,784
There we go. So what is exchange ability? Imagine a randomized controlled trial.

467
00:50:48,114 --> 00:50:53,933
Right. And you've got two really large groups of people and there that have been randomized.

468
00:50:53,934 --> 00:51:01,344
Right. One, to take a drug. And one group is not going to be taking that drug or that treatment of interest that you're studying.

469
00:51:01,794 --> 00:51:06,684
But we have this expectation, right, that outside of that exposure that we're controlling.

470
00:51:07,104 --> 00:51:13,764
Right. That the the groups are the same in every other kind of measured and unmeasured factor.

471
00:51:13,884 --> 00:51:19,523
If we have to have a really large groups for this medication, you should still have differences arise by chance.

472
00:51:19,524 --> 00:51:22,524
But like that's this is the idea anyways of exchange ability, right.

473
00:51:22,524 --> 00:51:28,964
So they would be the same gender wise height, hair color, biological sex, etc.

474
00:51:28,974 --> 00:51:33,914
Right. So okay, so what is exchange ability?

475
00:51:37,314 --> 00:51:45,534
It's when the unexposed group or is when if the unexposed group were in the stead assigned as the exposed group,

476
00:51:45,864 --> 00:51:53,124
we would expect them to have a very similar response as the actually exposed group, meaning that you've got group and girl group B, right?

477
00:51:53,124 --> 00:51:56,273
We group give group A our treatment of interest.

478
00:51:56,274 --> 00:52:04,404
We get we don't give it to group B, but if we were to switch it, we would expect to get the same kind of net effect right.

479
00:52:04,734 --> 00:52:08,334
And any differences that occur should just be due to random error.

480
00:52:08,754 --> 00:52:11,484
And that's when you have true exchange ability.

481
00:52:13,644 --> 00:52:22,614
So that is in the unexposed group where counter to the fact exposed, we would expect them to have the same outcome as the actually exposed group.

482
00:52:24,174 --> 00:52:28,374
So exchange ability is a characteristic of the samples.

483
00:52:29,394 --> 00:52:34,584
It is. It's a characteristic of the sample. Right. Of the two groups that you're comparing.

484
00:52:35,694 --> 00:52:42,744
And we are only striving for exchange ability, right, when we are making comparisons in epidemiology.

485
00:52:44,094 --> 00:52:48,954
So before the treatment is given, both the unexposed and the exposed groups are the same.

486
00:52:49,554 --> 00:52:54,294
So regardless of which treatment they're given right, they have the same potential outcomes.

487
00:52:57,544 --> 00:53:05,864
And so that's the kind of counterfactual exchange ability we will go back into exchange ability again in the class there.

488
00:53:06,034 --> 00:53:14,374
Any questions about the counterfactual model? Directed Acyclic graph stagflation.

489
00:53:14,374 --> 00:53:20,884
I looked really like them. So that depicts structural relationships.

490
00:53:21,364 --> 00:53:25,844
Other people are like, I hate guys. I can actually see homes with the faces right now, right?

491
00:53:26,404 --> 00:53:30,664
But by the end of this class, I think you're really like that, or at least I hope you will,

492
00:53:30,664 --> 00:53:34,474
or at least have more of an appreciation for them if you don't appreciate them.

493
00:53:34,474 --> 00:53:43,894
All right. So DAX, are graphical representations of causal relationships in DAX make explicit or assumptions

494
00:53:43,894 --> 00:53:48,484
about causal relationships and the particular problem or issue that we're looking at,

495
00:53:49,444 --> 00:53:53,734
they are directed, right? That means they're going to have an arrow going in one direction.

496
00:53:54,994 --> 00:53:59,644
And they are it is cyclic, meaning we do not see any feedback loops.

497
00:54:01,504 --> 00:54:08,764
There's a way to incorporate feedback into DAX, right? You can make a separate DAG for each time, set times your timeline, etc.

498
00:54:10,444 --> 00:54:13,984
So DAX are the model most commonly used in epidemiology?

499
00:54:14,734 --> 00:54:21,364
When we think about causality and causal relationships, it's an incredibly useful tool in epidemiology.

500
00:54:22,564 --> 00:54:29,464
It's going to be informed by general knowledge, by prior research, and by whatever your hypotheses going be.

501
00:54:30,994 --> 00:54:37,564
And they're going to depict how you were thinking or how we're thinking about a problem and the direction of causation.

502
00:54:38,914 --> 00:54:43,654
So some nomenclature for DAX we've got a is our exposure.

503
00:54:44,434 --> 00:54:51,034
Why is the outcome W is the measured confounder and then you is an unmeasured confounder.

504
00:54:51,044 --> 00:54:55,174
So here. Okay, right. Is our exposure.

505
00:54:55,834 --> 00:54:59,704
In this particular case, we're looking at low income. We have an arrow.

506
00:54:59,704 --> 00:55:06,544
I put a question mark here. You don't usually win, but why is our outcome right in this case?

507
00:55:06,544 --> 00:55:10,234
Diabetes and W is a confounder.

508
00:55:10,724 --> 00:55:12,634
I will talk about why there's lots more on that later.

509
00:55:14,804 --> 00:55:24,863
The other thing is mother has diabetes, and that's because if your mother has diabetes, that might be related to the classic low income.

510
00:55:24,864 --> 00:55:28,683
My health problems can have difficulties working, etc.

511
00:55:28,684 --> 00:55:35,104
Right. And also, if mother has diabetes there, there is a genetic component, I'm sure diabetes.

512
00:55:35,104 --> 00:55:39,063
So that affects the outcome as well. Right. So this is a very simple DAG.

513
00:55:39,064 --> 00:55:46,204
I put the question here because our question is, does low income lead to an increased risk of diabetes?

514
00:55:50,074 --> 00:55:57,544
So arrows and daggers. So if an arrow is present, it means that a directed association could be present.

515
00:55:57,674 --> 00:56:03,574
Doesn't mean it is right. But it could be present if an arrow is not present.

516
00:56:03,604 --> 00:56:07,054
It means that a directed association is definitely.

517
00:56:07,054 --> 00:56:10,074
Or at least we are assuming it to be missing. Right.

518
00:56:10,084 --> 00:56:13,894
There's no relationship between or no direct relationship between those two factors.

519
00:56:16,354 --> 00:56:23,584
So if we're constructing gags for direct effects, direct causal relationships are going to be shown by an arrow right here.

520
00:56:25,234 --> 00:56:31,864
All causal relationships will have a direction. I don't want to see any arrows pointing in both directions.

521
00:56:31,864 --> 00:56:36,063
Right. Certainly people used to be that a lot like what I was taught the drawbacks originally.

522
00:56:36,064 --> 00:56:39,514
We would do that just to show an association with its own. Okay.

523
00:56:40,384 --> 00:56:45,724
And then a variable and a single time point in time cannot be both a cause and an effect, right?

524
00:56:45,724 --> 00:56:57,824
We just got to incorporate time into that. DAG So dogs are useful for depicting both direct and indirect effects.

525
00:56:57,994 --> 00:57:01,954
By the way, there are really nice programs for drawing dogs which we actually get.

526
00:57:01,954 --> 00:57:09,964
One, improve the slides a little bit here, but here we've got smoking leading to low birth weight, leading to increased mortality.

527
00:57:09,964 --> 00:57:14,194
Rates of smoking is having an indirect effect on infant mortality.

528
00:57:14,764 --> 00:57:19,854
Smoking is also having a direct impact on infant mortality in this particular example.

529
00:57:23,094 --> 00:57:30,984
Dogs are going to be temporarily order sort of factors or tempo in order to achieve that outcome at two one.

530
00:57:31,234 --> 00:57:37,584
We are committed to leaving cup of tea for you. You don't want any arrows that go from future factors.

531
00:57:37,584 --> 00:57:42,084
Back to past factors. Rate something that happens with the future or not cause something that happened in the past.

532
00:57:43,974 --> 00:57:51,234
Artie said there is cyclic rate, but I don't want to see anything like this. Poverty leads to infection, which leads to more poverty.

533
00:57:52,314 --> 00:57:56,514
This is actually true. It's going to establish. Right. But that's not how we present that on that.

534
00:57:57,594 --> 00:58:03,894
This is a no right. And so we're going to say poverty and type one leads to infection at time one.

535
00:58:03,894 --> 00:58:09,113
It's also related to poverty at time to right that infection at time.

536
00:58:09,114 --> 00:58:13,124
One could lead to poverty and time to an infection of time to.

537
00:58:13,344 --> 00:58:17,814
Right. And then obviously you could keep going in time for.

538
00:58:21,844 --> 00:58:26,524
All right. So relationships through DAX, we've got our direct causal effects.

539
00:58:26,944 --> 00:58:32,464
A clause is why right here, we've got the absence of a causal effect.

540
00:58:33,004 --> 00:58:43,854
Here's. Here's why there's no arrow between them so as not having an impact on why, at least not directly indirect causal effects.

541
00:58:43,854 --> 00:58:50,284
So here A is having an effect on why trubee there also has a directed causal factor, right?

542
00:58:50,614 --> 00:58:53,943
Or an indirect effect in the absence of a direct causal effect here.

543
00:58:53,944 --> 00:59:04,474
So A affects B which affects which this causal effect on Y, but A itself is only acting on Y for me in this example.

544
00:59:09,324 --> 00:59:23,394
So time flows in dogs. Time usually flows from the top to the bottom and then from the left to the right.

545
00:59:24,084 --> 00:59:26,964
Oh, and you'll see that on some of these slides. Foster boosters.

546
00:59:26,964 --> 00:59:34,554
Foster was an amazing student I worked with, was kind enough to give me a bunch of the stack slides that I incorporated into the lecture.

547
00:59:34,554 --> 00:59:36,924
So particularly in the dog workshop, you'll see quite a few.

548
00:59:37,524 --> 00:59:45,923
Um, so usually dogs are drawn such that factors on the top come before factors on the bottom right,

549
00:59:45,924 --> 00:59:50,753
factors on the left, comfortable factors on the right. And it's just, it makes it easier.

550
00:59:50,754 --> 01:00:00,624
They get messy, particularly when they get really complicated, right? So unmeasured variables that are common causes of your measured factors must

551
01:00:00,624 --> 01:00:05,034
be present on a dog you use that you write to say that's a measured variable.

552
01:00:09,344 --> 01:00:13,364
So how do we decide what we're going to put in bags? You've got subject matter expertize.

553
01:00:14,114 --> 01:00:15,913
You'll talk with collaborators,

554
01:00:15,914 --> 01:00:23,084
talk with your mentor to figure out what to put in the dag useful you want in the end to end up with a useful, correct tag, right?

555
01:00:23,084 --> 01:00:26,324
An incorrect baggage, meaning loss that's going to completely send you astray.

556
01:00:26,754 --> 01:00:29,054
Right. So it's really important to get the dogs correct.

557
01:00:31,844 --> 01:00:38,414
So a dog is a simplified way of diagraming the relevant factors, which means that you should not include extraneous factors.

558
01:00:38,564 --> 01:00:47,534
Here we have an example of a bike crash where there's a bus strike that leads to somebody riding to work leading to the bike crash.

559
01:00:47,534 --> 01:00:50,924
The bus strike also has a direct effect on the bike crash.

560
01:00:51,014 --> 01:00:54,494
Maybe there was a lot more traffic because there were fewer busses, for example.

561
01:00:54,914 --> 01:01:00,284
And it was also raining. Right. And here we go, right into the dark.

562
01:01:00,824 --> 01:01:05,024
But rain is only having an effect on the bike crash.

563
01:01:05,024 --> 01:01:10,364
It's not actually having an effect on any of the other factors. So we don't need it in the stack.

564
01:01:10,634 --> 01:01:18,644
Right. So we should exclude rape from the DAG in this scenario.

565
01:01:20,504 --> 01:01:24,524
And the same thing would occur, for example, if rain impacted the bike.

566
01:01:24,794 --> 01:01:32,994
The bus strike. Right, but did not impact bike crash or any other factors in that.

567
01:01:35,464 --> 01:01:40,094
All right. So there's also some terminology that you should be kind of aware of for dogs.

568
01:01:40,304 --> 01:01:46,134
Parent and child. So a parent just comes forward to the child.

569
01:01:46,184 --> 01:01:51,224
Right. So prenatal care. Here is a parent of smoking.

570
01:01:51,794 --> 01:02:00,734
Right. Smoking in here is a parent of low birth weight. And then child smoking is the child of prenatal care.

571
01:02:03,424 --> 01:02:08,584
There's also ancestors and descendants. So here you going back a little bit further, right?

572
01:02:08,714 --> 01:02:12,034
So this is an ancestor of low birth weight.

573
01:02:12,634 --> 01:02:15,934
And low birth weight is a descendant of X.

574
01:02:18,034 --> 01:02:23,884
And you'll see that terminology coming up in papers when you read them, which is why we go over it.

575
01:02:24,874 --> 01:02:32,524
So some of the utilities and strengths of Dags are they're useful for depicting known or hypothesized causal relationships,

576
01:02:33,334 --> 01:02:36,844
and they're also useful for examining or thinking about confounding and bias.

577
01:02:36,844 --> 01:02:43,894
And we'll talk more about that model, too, and we'll use tags to illustrate and look at confounding and bias.

578
01:02:45,934 --> 01:02:51,214
Some of the limitations or criticisms of Dags are they cannot show effect modification or interaction.

579
01:02:51,694 --> 01:02:58,264
Talk more about that in module two. And the arrows do not provide specific definitions of the effects.

580
01:02:59,074 --> 01:03:06,274
Right. And they can also be extremely complicated. So here's just an example of a dag from one paper, right.

581
01:03:06,274 --> 01:03:12,234
That can get actually a lot more complicated than this. So. Okay.

582
01:03:12,624 --> 01:03:20,874
You want to hand out the in class activity. So we're going to go through give you a little bit of time to work on.

583
01:03:24,504 --> 01:03:28,104
Problem one, we're not a problem to solve for most classes.

584
01:03:28,104 --> 01:03:41,484
We try to take some breaks and work through some problems. Forever and also the ones that have to down.

585
01:03:43,554 --> 01:03:56,954
I've written something for. Oh, yeah.

586
01:03:57,404 --> 01:04:03,154
Yes. So if you do have an extreme that.

587
01:04:04,364 --> 01:04:13,653
That. So you want to add everything in and people will draw all these experience factors in and

588
01:04:13,654 --> 01:04:18,154
you kind of have to draw them in a way to determine that you're trying to draw people back.

589
01:04:18,454 --> 01:04:27,064
But what I do see all the time is they realize that all these things that are just connected to one thing were here.

590
01:04:27,214 --> 01:04:36,964
And it's not a every measure. Right? And you're like, okay, well, and, you know, it's not actually this rationally confounding your relationship.

591
01:04:37,384 --> 01:04:42,454
Right? And so you just want to draw your jack as simple as you can.

592
01:04:42,484 --> 01:04:51,604
Well, actually. So here it is.

593
01:04:53,614 --> 01:05:02,044
Riding your bike to work, for example. Right. Then you want to or.

594
01:05:07,094 --> 01:05:10,724
If it only works if the outcome doesn't need to be in the back.

595
01:05:23,664 --> 01:05:28,314
It's hollow. Yeah. You don't like the.

596
01:05:41,614 --> 01:05:47,014
So you could definitely work together on these. These are better. Usually if do work together and just talk through things.

597
01:05:47,194 --> 01:05:50,314
I know this is not the best classroom for her doing that.

598
01:08:25,664 --> 01:08:31,544
Yeah. Yeah.

599
01:08:46,104 --> 01:08:49,344
Just like old logic. You know, I like.

600
01:08:55,964 --> 01:09:25,254
That was me games game. Clint Eastwood when I was first starting.

601
01:09:29,064 --> 01:09:36,804
I don't see. This.

602
01:09:37,754 --> 01:09:40,914
Yeah. We might not get.

603
01:09:45,844 --> 01:09:51,534
Right. Might just be like that.

604
01:09:54,624 --> 01:10:00,004
Are people generally done? Generally done. Okay.

605
01:10:00,964 --> 01:10:01,534
All right.

606
01:10:01,774 --> 01:10:10,864
So because you're a researcher investigating why morbidity and mortality from COVID 19 is higher in males, you believe that biological sex, right?

607
01:10:10,864 --> 01:10:13,954
This or hopefully is the fetus.

608
01:10:19,394 --> 01:10:22,784
Okay. So you believe that biologic sex affects sex hormones?

609
01:10:31,264 --> 01:10:38,584
Which impacts. Your risk of severe COVID 19.

610
01:10:45,134 --> 01:10:51,474
All right. And you also believe that while I have side effects, risk taking behavior.

611
01:10:54,684 --> 01:11:00,284
Write risks, which impacts your risk of severe COVID 19.

612
01:11:01,724 --> 01:11:05,264
In addition, men have higher rates of comorbid conditions.

613
01:11:06,254 --> 01:11:23,834
So we'll go down here. All right, such as heart problems, which are risk factors for severe COVID 19.

614
01:11:25,464 --> 01:11:34,824
All right. In addition, you believe that men have higher levels of ease to biological sex.

615
01:11:38,004 --> 01:11:44,244
Is to enhance your risk of severe COVID 19.

616
01:11:44,394 --> 01:11:49,374
Yours may look very different than this, but the important thing is how the arrows got right.

617
01:11:51,234 --> 01:11:56,484
You also think that mail success may impact severe COVID 19 risk factors through other pathways.

618
01:11:59,774 --> 01:12:05,204
So we'll drop like that in direct effect sometimes people over to you in the middle that that would be okay too.

619
01:12:05,204 --> 01:12:15,524
But since we didn't say also has a direct effect, it has other pathways right but we don't know what the topics are so dry that right.

620
01:12:15,824 --> 01:12:21,524
So this is roughly what your dad should look like. I realize that may be very different depending on how you set it up.

621
01:12:25,194 --> 01:12:36,624
But we've got a question. Would it be important to kind of make it so like that?

622
01:12:36,674 --> 01:12:41,384
Because males are more likely to be like well being that they're also.

623
01:12:42,414 --> 01:12:46,914
Lead to more co-morbid conditions like that. Like alternative pathways.

624
01:12:46,944 --> 01:12:47,964
Yeah, you could.

625
01:12:47,964 --> 01:12:56,524
Although this one said just assume usually the only because otherwise you start guessing that all these other which is probably in fact that is true.

626
01:12:57,054 --> 01:12:59,873
Right. But you start inserting other things into it.

627
01:12:59,874 --> 01:13:05,354
And so we're oftentimes like, this is it like these are the only relationships you can occur, right?

628
01:13:06,264 --> 01:13:11,534
Yeah, that's true. Yeah, you can have. Then it starts to get messier.

629
01:13:11,544 --> 01:13:15,174
You know, if I. If I was drawing it that way, what I would have done is put risks down here.

630
01:13:15,834 --> 01:13:21,684
Right. And then have risks with an arrow up to comorbid conditions if that were the case.

631
01:13:33,624 --> 01:13:37,433
So he it could be a direct effect, right? Any direct effect on them?

632
01:13:37,434 --> 01:13:44,364
Like maybe there are probably intermediate steps along that direct effect or for a lot of things that we're not necessarily diagraming out.

633
01:13:44,574 --> 01:13:47,904
I have seen people, which is fine if you put.

634
01:13:51,084 --> 01:13:55,194
Like that. Right. Like, I would be okay with that. I would call that wrong.

635
01:13:55,974 --> 01:14:07,394
Right. But. We sometimes don't show intermediate steps, and sometimes it's not related to our questions.

636
01:14:07,394 --> 01:14:13,464
We don't. Okay.

637
01:14:15,764 --> 01:14:22,064
Vaccination protects against severe COVID 19, both by preventing infections and reducing the severity of infections.

638
01:14:22,814 --> 01:14:26,594
Okay. So vaccination.

639
01:14:27,674 --> 01:14:35,954
You do not think that biological sex is associated with vaccine vaccine rates or response should you add vaccination into your DAB?

640
01:14:38,004 --> 01:14:42,083
No. Right. Not based on that. But you would draw probably potentially.

641
01:14:42,084 --> 01:14:44,124
Or you might read it. Okay. We're not going to.

642
01:14:44,274 --> 01:14:52,374
Of course, you can certainly argue that risk taking behaviors are related to whether or not you would get.

643
01:14:53,434 --> 01:14:56,843
I some people draw decks a lot with crossing lines.

644
01:14:56,844 --> 01:15:05,454
I really try not to. In this particular case, I'm going to write, you know, and then we would, you know, leave vaccination.

645
01:15:05,664 --> 01:15:11,484
And you're right, this is an important factor that we should. Take into account.

646
01:15:12,054 --> 01:15:15,384
But in this particular case, we're saying it's not related to that stuff.

647
01:15:15,714 --> 01:15:23,663
Right. So we will not include vaccination for that, even though we know that vaccinations for important people,

648
01:15:23,664 --> 01:15:31,014
whether or not you get to COVID 19, it's just not important for our question with the relationships that we believe exist.

649
01:15:35,824 --> 01:15:41,884
And people get tripped up about that all the time. They're but I know that that is really a big determine, you know, determinant of whatever.

650
01:15:42,154 --> 01:15:45,904
But it's really how it relates to your question.

651
01:15:46,564 --> 01:15:50,534
Right. All right.

652
01:15:51,074 --> 01:15:52,454
So Hills viewpoint.

653
01:15:52,464 --> 01:16:03,524
So I, I still have it as Phil's criteria earlier in the slides, um, kind of, I think, you know, so every textbooks usually call it those criteria.

654
01:16:03,914 --> 01:16:06,554
So I don't know that Boston does, but I'll, but a lot of them do.

655
01:16:07,064 --> 01:16:14,084
Um, but actually when help put them forward originally they were very much say not criteria.

656
01:16:14,084 --> 01:16:19,064
He was not saying that you absolutely had to have all of these. So there you go.

657
01:16:19,694 --> 01:16:20,054
All right.

658
01:16:21,254 --> 01:16:28,244
So hills viewpoints are the things that you should look at or think about when you're thinking about whether something is a causal relationship,

659
01:16:28,994 --> 01:16:41,294
is strength of association, consistency, specificity, temporality, biological, gradient, possibility, coherence, experimental evidence and analogy.

660
01:16:41,894 --> 01:16:53,084
And we will go through these. Um, so for the strength of association is usually measured by volunteers and what we can talk about that later,

661
01:16:54,044 --> 01:16:57,253
the stronger the association, the more likely the relationship is causal.

662
01:16:57,254 --> 01:17:02,234
That's kind of like that idea, right? If you get a relative risk of then you're more likely to think it's causal.

663
01:17:02,244 --> 01:17:03,944
Then if you get something that's 1.2.

664
01:17:04,814 --> 01:17:14,354
However, the observation of a weak relative risk does not negate the possibility of causality, and people can argue about what they think.

665
01:17:14,354 --> 01:17:21,134
A weak part of this case. For those of us in infectious diseases, we usually think there's a weak relative risk,

666
01:17:21,134 --> 01:17:26,593
whereas some chronic disease people will be like weren't from for this huge like that's a really strong risk factor.

667
01:17:26,594 --> 01:17:28,454
So it doesn't really vary by field,

668
01:17:30,704 --> 01:17:38,023
but just because you don't see a really strong association between the two doesn't mean that there isn't a causal association.

669
01:17:38,024 --> 01:17:45,014
One one example of that or counterexample to that is the weak exposure between smoking and cardiovascular disease.

670
01:17:45,614 --> 01:17:51,944
Right. We know smoking does cause cardiovascular disease, even though, you know, the relative risk is relatively low.

671
01:17:52,394 --> 01:17:58,964
Uh, another counterexample would be like birth order and Down syndrome, right?

672
01:17:59,314 --> 01:18:06,074
You know, if your child number six, you're more likely to have Down syndrome than if your child number one.

673
01:18:06,644 --> 01:18:08,743
Right. But it has nothing to do with birth order.

674
01:18:08,744 --> 01:18:16,574
It has to do with the fact that on average, parents are usually older when they have child number six than child number one.

675
01:18:17,054 --> 01:18:24,463
Right. Although, uh, not always.

676
01:18:24,464 --> 01:18:31,603
All right. So consistency is the repeated observation of an association and different populations under different circumstances.

677
01:18:31,604 --> 01:18:36,014
Basically, if you see it different studies on different populations, right,

678
01:18:36,224 --> 01:18:40,003
you start to think like this is probably causal because I'm not just doing it in one place,

679
01:18:40,004 --> 01:18:43,094
in one population, I'm seeing it across multiple different populations.

680
01:18:44,204 --> 01:18:50,204
So consistency of results across epidemiology, epidemiologic studies gets at the heart of inductive reasoning,

681
01:18:50,654 --> 01:18:55,244
and it's really used to infer causality in observational studies we've seen in one cohort.

682
01:18:55,244 --> 01:19:04,604
We want to check in other studies. Right? Got to do this a little bit cautiously because it can just reflect consistency of confounding.

683
01:19:04,844 --> 01:19:11,384
Maybe all the studies have the same problem or bias across these studies.

684
01:19:12,314 --> 01:19:22,934
It could also result from publication bias, right, which is where a positive result is much more likely to get published than a null result.

685
01:19:28,224 --> 01:19:34,013
So and another thing like it's sometimes it's kind of argued that in the literature that

686
01:19:34,014 --> 01:19:38,274
results are inconsistent just because one is statistically significant and another one is not.

687
01:19:39,054 --> 01:19:46,094
I would say that that's not really that's not true. And so you can have the fact that studies that are essentially the same.

688
01:19:46,104 --> 01:19:49,913
Right. But like one is not significant because it crosses one.

689
01:19:49,914 --> 01:19:54,564
And so therefore, in one study, it's a factor in another one that they say there's no association.

690
01:19:55,104 --> 01:19:59,864
Right. But that's really just a difference in standard errors or potential,

691
01:19:59,874 --> 01:20:04,734
at least a difference in standard errors across those studies, and not necessarily that they're inconsistent.

692
01:20:07,604 --> 01:20:11,804
So there are a lot of reasons why causal associations might not appear to be consistent.

693
01:20:12,824 --> 01:20:16,394
There could be differences in the specific circumstances of the exposure.

694
01:20:17,444 --> 01:20:21,334
There can be differences in the timing of the study with regard to the exposures.

695
01:20:22,334 --> 01:20:26,983
And you do that study like you immediately versus five years after the exposure,

696
01:20:26,984 --> 01:20:31,154
a certain procedure that different causal relationships for some diseases,

697
01:20:31,784 --> 01:20:41,443
differences in the design and analytic strategies that are used, differences in the distribution of the causal component.

698
01:20:41,444 --> 01:20:44,634
This is one place where thinking about the pause can be really helpful, right?

699
01:20:44,654 --> 01:20:51,704
Like how these how these populations are different differences in the stage of the natural history underlying the process,

700
01:20:52,634 --> 01:20:55,663
the effect of differences in the effectiveness of different interventions,

701
01:20:55,664 --> 01:20:59,114
that same intervention can have a very different effectiveness in different in

702
01:20:59,114 --> 01:21:03,884
different settings and then differences in the variability of a particular risk factor.

703
01:21:05,534 --> 01:21:07,584
Once again, think about the cost applies here. Right?

704
01:21:07,604 --> 01:21:16,304
That's really helpful for thinking about why you might see inconsistent results even though there is a causal relationship across different settings.

705
01:21:16,904 --> 01:21:19,964
So just one example for the kind of latency period, right?

706
01:21:19,964 --> 01:21:24,974
We've got this minimum incubation period here, but exposure to a causal agent.

707
01:21:25,574 --> 01:21:34,244
So if you look at 48, right, if that's when you conduct your study, you're going to get a very different relationship than if you later on.

708
01:21:34,814 --> 01:21:40,594
Right? Specificity.

709
01:21:41,074 --> 01:21:44,164
So that's another of Hill's viewpoints, right?

710
01:21:44,644 --> 01:21:50,014
Cost leads to a single effect, not multiple effects. This one is invalid.

711
01:21:50,464 --> 01:21:56,043
Right. It's not true. We have we now know that there are lots of exposures, treatments.

712
01:21:56,044 --> 01:22:03,573
Right. That can lead to lots of different effects. This is kind of like very much developed during the time of infectious diseases.

713
01:22:03,574 --> 01:22:08,644
So this kind of idea of one agent leads to one disease, which we even know now that is not true.

714
01:22:08,704 --> 01:22:11,734
Right. But so, for example, think about smoking.

715
01:22:11,944 --> 01:22:19,083
Smoking can lead to a lot of different diseases. Right. Even exposure to, like influenza actually can lead to a lot of different diseases.

716
01:22:19,084 --> 01:22:23,733
Some people can get exposed to flu. They don't get sick. Some people get kind of a traditional illness.

717
01:22:23,734 --> 01:22:24,724
Some people get pneumonia.

718
01:22:25,084 --> 01:22:29,854
Some people seem fine and then have a heart attack two weeks later because they had inflammation of the heart that we didn't see.

719
01:22:29,854 --> 01:22:39,254
Right. So not necessarily all the same disease presentation, even though it's a agent, a virus or worse.

720
01:22:40,354 --> 01:22:44,313
All right. Temporality exposure must perceive the onset of disease.

721
01:22:44,314 --> 01:22:48,354
This one is inarguably definitely present in all causal models.

722
01:22:48,364 --> 01:22:52,414
Right. It can be difficult to establish temporality sometimes, though.

723
01:22:53,914 --> 01:23:03,184
Biological gradient. Right. A dose response got a monotonic relationship between exposure, dose and risk of outcome is regarded as strong evidence,

724
01:23:03,424 --> 01:23:09,514
meaning the more exposure you have, the higher the risk of getting that of having that outcome right.

725
01:23:09,814 --> 01:23:17,164
But it is true that that's possible that you can have other shapes right from these exposure outcome shapes.

726
01:23:17,164 --> 01:23:20,464
So a j-shaped curve, a threshold pattern.

727
01:23:21,244 --> 01:23:28,504
So here's this kind of where you typically think of a dose response rate at an increasing exposure level, you have an increasing risk.

728
01:23:31,054 --> 01:23:36,814
One example of this is, you know, smoking and lung cancer. The more they smoke, the higher your.

729
01:23:39,094 --> 01:23:41,584
But there's other diseases where it's a j-shaped curve.

730
01:23:41,944 --> 01:23:51,654
For example, alcohol or cardiovascular disease or vascular disease, which is another classic to shoot for.

731
01:23:52,114 --> 01:23:58,844
If you are underweight, you are more likely to die. And if you are overweight, right, they're more likely to die early.

732
01:24:01,024 --> 01:24:05,913
And then you can also have an exposure level, which is a threshold pattern.

733
01:24:05,914 --> 01:24:09,244
You just get to a certain point and then the risk goes up quite quickly.

734
01:24:09,994 --> 01:24:14,824
So here we have weight and coronary disease, sudden death as an example.

735
01:24:17,044 --> 01:24:21,454
There are also examples of monotonic relationships where it's not causal.

736
01:24:22,804 --> 01:24:29,434
And so we've already we've already had one of those before, right, that I've mentioned, which is birth order and Down's syndrome.

737
01:24:29,944 --> 01:24:34,684
Right. As you go off birth order, the risk of Down's syndrome is higher, but it's not causal.

738
01:24:35,014 --> 01:24:39,544
Right. It's just that it's confounded by parental age.

739
01:24:41,674 --> 01:24:47,434
All right. Plausibility. For an association to be causal, it must be plausible.

740
01:24:47,914 --> 01:24:54,544
That is consistent with the laws of biology, based on a primary evidence that may not stand the test of time.

741
01:24:55,594 --> 01:25:00,844
And a problem is it may be based on logic or data, but but only on prior beliefs.

742
01:25:00,844 --> 01:25:05,134
So like people, scientists, it's really it's interesting.

743
01:25:05,134 --> 01:25:09,414
You think like, oh, yeah, it's totally been proven right. But often it actually hasn't.

744
01:25:09,424 --> 01:25:18,574
There's just kind of like this belief that has been set up. One example of that, that one I worked on was dengue fever.

745
01:25:18,874 --> 01:25:25,054
Like, there's there's been a belief for a long time, right, that you can only get each type of dengue fever,

746
01:25:25,204 --> 01:25:28,954
like each type of dengue at one time, or there's four serotypes for different viruses.

747
01:25:29,254 --> 01:25:32,884
Once you develop, once you get one virus, you're immune to it for your entire life.

748
01:25:33,294 --> 01:25:38,944
And that actually turns out not to be true. You can get it again. It's just that the disease typically looks much more mild.

749
01:25:39,154 --> 01:25:40,684
It doesn't look quite like typical dengue.

750
01:25:41,164 --> 01:25:50,494
And so historically has been has been in most, although sometimes people probably can get quite sick having in a second time.

751
01:25:55,904 --> 01:25:58,934
Coherence. Coherence implies a cause and effect.

752
01:25:58,934 --> 01:26:04,424
Interpretation for an association does not conflict with what is known of the natural history and the biology of disease.

753
01:26:07,644 --> 01:26:12,173
The there's a fine line really between the criteria for coherence and plausibility.

754
01:26:12,174 --> 01:26:15,024
It's one thing that people kind of struggle with a little bit.

755
01:26:15,294 --> 01:26:23,664
Coherence is the presence or absence of conflicting information where plausible is just like, is it reasonable to believe this?

756
01:26:25,734 --> 01:26:34,004
And here is a quote that somebody pointed out to me last year that is sometimes helpful for students.

757
01:26:34,284 --> 01:26:40,403
Some good looking at the difference. So a subtle difference between coherence and plausibility is about plausibility.

758
01:26:40,404 --> 01:26:44,634
Asked, Could you imagine a mechanism that if it had truly operated,

759
01:26:44,994 --> 01:26:49,314
which could be counterfactual, would have produced results such as those observed in the data?

760
01:26:50,334 --> 01:26:59,784
By contrast, Coherence asks if you assume that established theory is correct, i.e. counterfactual what the observed results fit into that theory,

761
01:27:00,204 --> 01:27:06,144
whereas the consideration of coherence would reject the observed result to be non causal if it is contradicted,

762
01:27:06,444 --> 01:27:12,354
if it contradicted a predominant theory plausibility structure, kind of a little bit more flexibility, right?

763
01:27:13,014 --> 01:27:19,404
A little bit more room regarding which particular piece of substance of knowledge, knowledge to evaluate the results against.

764
01:27:22,524 --> 01:27:29,824
Experimental evidence is the laboratory experiments in animals or humans.

765
01:27:30,194 --> 01:27:35,424
Experimental evidence is regarded really as the gold standard in establishing causality.

766
01:27:36,804 --> 01:27:41,034
However, it's often not ethical possible to randomly assign exposures.

767
01:27:41,724 --> 01:27:45,443
We also like in my field you write, you often will.

768
01:27:45,444 --> 01:27:47,664
You look at, you know, watch a talk, for example.

769
01:27:48,354 --> 01:27:54,444
Most likely just because you see it in an animal model doesn't mean that it actually will be what happens in humans.

770
01:27:54,774 --> 01:27:56,093
I can tell you that for sure,

771
01:27:56,094 --> 01:28:03,774
because we have a lot of universal influenza vaccines that work in mice and we have zero universal influenza vaccines in humans.

772
01:28:03,774 --> 01:28:11,574
Right. So while it's thought of as the gold standard, there could be issues with experimental evidence as well.

773
01:28:12,144 --> 01:28:18,013
And, you know, it may not be ethical after this, not ethical or possible to randomly assign exposures.

774
01:28:18,014 --> 01:28:20,994
Right. You can't randomly assign biological sex, for instance.

775
01:28:23,544 --> 01:28:31,074
So intervention trials are one one way that we do use, you know, experimental evidence in humans.

776
01:28:32,514 --> 01:28:38,724
So those are four treatments where we can ethically assign, you know, people to get the treatment or not to get the treatment.

777
01:28:40,224 --> 01:28:45,284
There are also human challenge studies which are being done kind of increasingly in.

778
01:28:45,724 --> 01:28:54,964
In the infectious disease world once again were very common, relatively common, usually done in unethical ways.

779
01:28:55,774 --> 01:29:00,934
In the 1950s and before had kind of stopped and started back up again.

780
01:29:01,294 --> 01:29:07,414
They're actually doing human challenge studies of SARS-CoV-2 in the U.K. now, but the U.S. has been more.

781
01:29:09,684 --> 01:29:18,414
Cautious about not doing this. So experimental evidence is really testing a causal causal hypothesis.

782
01:29:20,214 --> 01:29:24,534
Okay. And then there's analogy. The effect of similar factors may be considered.

783
01:29:25,904 --> 01:29:30,654
All right. So putting them all together. So you have we have all these different causal models.

784
01:29:31,404 --> 01:29:34,434
How do we use them? Right. We've got our counterfactual model.

785
01:29:34,434 --> 01:29:39,894
The potential outcomes model is used to establish the research question, and that's one way that you can use it.

786
01:29:41,334 --> 01:29:45,413
Dags are going to aid in study design and in determining what covariates you

787
01:29:45,414 --> 01:29:49,224
need to collect and determining what covariates you need to collect data on.

788
01:29:49,614 --> 01:29:53,903
You're also going to be very useful, obviously, in setting up your analysis.

789
01:29:53,904 --> 01:30:02,604
I'm going to look at that data. You do you do want to draw the dark when you're designing the society, not after you've already collected the data.

790
01:30:03,234 --> 01:30:09,264
Although I see so many people actually not try to draw back until they like analyzing the data,

791
01:30:09,264 --> 01:30:17,274
at which point they realize that they've not collected all the data they need or collected it in a manner that is not ideal at the very least, right?

792
01:30:17,274 --> 01:30:28,074
So the sufficient component model AIDS in considering how your exposure fits into the big picture and how it would vary depending on the population.

793
01:30:28,764 --> 01:30:37,014
Right. And then the Bradford Hill viewpoints could be used to assess support for causal relationships where you usually see these coming up.

794
01:30:37,494 --> 01:30:43,164
Does can anybody think of where you see at least those concepts coming up in a paper?

795
01:30:46,904 --> 01:30:53,054
In your discussion section write, you can read a discussion section and just kind of assign what people are doing,

796
01:30:53,114 --> 01:30:57,944
know what or what, what, what points or what viewpoints are being addressed.

797
01:30:58,994 --> 01:31:04,214
And so with that, we have a few minutes.

798
01:31:04,484 --> 01:31:12,764
So there is part two that goes over some of those viewpoints and specifically takes some points from a discussion section.

799
01:31:16,214 --> 01:31:20,834
We want to kind of read through and answer to see what viewpoints you're bringing up.

800
01:31:43,054 --> 01:31:48,414
No, because it's not whether they changed or not. Oh, so I guess part.

801
01:31:51,264 --> 01:31:54,944
Because. Autism spectrum.

802
01:31:55,464 --> 01:31:58,794
Open borders. Therefore.

803
01:32:05,214 --> 01:32:08,334
It kind of predetermined present day. Right.

804
01:32:09,694 --> 01:32:31,874
His name starts to present. Once again, you can you can talk to one another.

