1
00:00:22,080 --> 00:00:27,060
And finally, to authentication through all means.

2
00:00:28,200 --> 00:00:32,790
Thank you. Health, nutrition for Mom. How can I help?

3
00:00:33,220 --> 00:00:40,560
Sure. There are things that are already going on inside.

4
00:00:41,910 --> 00:00:45,770
I know we can try. No, no.

5
00:00:46,170 --> 00:00:49,980
We've got all them and all are good. Yeah, I think so.

6
00:00:50,010 --> 00:00:55,890
Like, maybe the. The one one order of things to bring it home.

7
00:00:58,680 --> 00:01:02,100
Okay, we should get on track, so it's good to see you guys.

8
00:01:03,750 --> 00:01:09,120
There is a sign in sheets, of course. Otherwise, I pester you tremendously.

9
00:01:14,870 --> 00:01:18,310
So I hope the courses give you some feedback on the course.

10
00:01:18,320 --> 00:01:21,950
What do you think so far? You have five folks possibly.

11
00:01:24,310 --> 00:01:28,390
Some have been very rapid and some have been.

12
00:01:28,930 --> 00:01:32,230
Let's just go through all the. Okay.

13
00:01:33,820 --> 00:01:39,290
Okay. And I want to ask you have a discussion or or engaging for other feedback.

14
00:01:39,330 --> 00:01:48,840
That's helpful. Some things are more one, but you can share your share.

15
00:01:54,720 --> 00:02:00,750
One of the comments that I got back from several of the instructors was we didn't review the materials they had asked you to.

16
00:02:01,710 --> 00:02:08,710
So I'm giving you that feedback, that idea that that might be on the Rapid Fire.

17
00:02:09,090 --> 00:02:12,400
It's. Well, feedback.

18
00:02:15,620 --> 00:02:22,129
So can you, if you could give me more feedback either email anonymously on the survey.

19
00:02:22,130 --> 00:02:26,290
That's helpful. Okay. I really appreciate the support.

20
00:02:26,660 --> 00:02:30,400
I mean, it's a requirement, but we do want you to get something out of it. Okay.

21
00:02:30,410 --> 00:02:34,340
Yeah. You know, it is a school requirement to try to educate.

22
00:02:36,320 --> 00:02:43,730
Okay. You know, when I think about rigor and reproducibility, which is the topic for today.

23
00:02:44,060 --> 00:02:48,320
Yeah, you know, I have this sort of structure that if we don't get through material today,

24
00:02:48,320 --> 00:02:55,010
we'll continue next week because it's a very big and important topic for NIH as well.

25
00:02:55,760 --> 00:03:00,049
But when I think about this topic, I want to bring something to it.

26
00:03:00,050 --> 00:03:05,390
And, you know, yesterday at breakfast, I'm thinking, what can I bring to Section one class?

27
00:03:05,720 --> 00:03:10,280
And staring at me in New York Times was this particular story.

28
00:03:10,610 --> 00:03:16,759
And this is a guy who leads a center at UC Davis.

29
00:03:16,760 --> 00:03:25,970
There's this picture and it is the center, which is a leading agricultural research center at UC Davis.

30
00:03:26,420 --> 00:03:31,620
And if you're doing agriculture work, UC Davis is one of the places to be okay.

31
00:03:31,670 --> 00:03:42,500
Top school in that field. And here you can read this, you know, outspoken defender of the meat industry talking about sustainability and so forth.

32
00:03:42,920 --> 00:03:50,270
But a number of international research efforts have looked at climate change.

33
00:03:50,660 --> 00:03:58,100
And yeah, I think The Lancet, if I recall, they said to fight climate change and improve human toll,

34
00:03:58,400 --> 00:04:02,030
the world needs to dramatically cut back on eating meat.

35
00:04:02,540 --> 00:04:06,200
Okay, so what's the deal with meat and climate change?

36
00:04:07,790 --> 00:04:17,490
How are these linked? Yeah.

37
00:04:18,860 --> 00:04:25,070
Yeah. Methane is a huge greenhouse gas. It's increasing by, like 5% a year.

38
00:04:25,490 --> 00:04:34,910
It's already like 2.1 people. And it has radiative forcing, which is like 20 to 40 times CO2.

39
00:04:35,210 --> 00:04:39,170
It's much lower in concentration, but they're forcing factor.

40
00:04:39,170 --> 00:04:42,650
It means it's second important and growing very rapidly.

41
00:04:43,340 --> 00:04:48,700
Where's my. Yeah.

42
00:04:49,210 --> 00:04:52,850
Yeah. Okay. They essentially work alone. Okay.

43
00:04:52,960 --> 00:04:57,910
What else? Human health, climate change.

44
00:04:58,330 --> 00:05:01,990
I have my sustainability. Excellent. Here. Fantastic.

45
00:05:02,830 --> 00:05:10,060
The amount of agriculture that's required to create the feed for the cattle.

46
00:05:10,750 --> 00:05:16,560
And if you're telling the field. Environment that way.

47
00:05:17,040 --> 00:05:22,350
Yeah. I mean, come on. How many people are in the world right now? Yeah, but 8 billion people, right?

48
00:05:22,860 --> 00:05:31,500
That's a lot of people to feed. And, you know, protein conversion from grain to be or whatever, you lose a lot.

49
00:05:32,190 --> 00:05:35,260
So they're arguing this is something else that's going.

50
00:05:39,220 --> 00:05:47,650
The World War. Got any ideas?

51
00:05:48,070 --> 00:05:55,360
Clearing space for actual action. So mean the Amazon is being cut down to change it into grasslands to grow beef.

52
00:05:55,810 --> 00:06:00,340
Okay. And you have all this carbon sequestered there. Okay, everyone.

53
00:06:00,790 --> 00:06:08,470
Okay. So anyway, he says the head of the Center on Radical and Private Agenda.

54
00:06:09,920 --> 00:06:13,860
Right. And, you know, fun. All right.

55
00:06:13,880 --> 00:06:18,890
So anyway, he's this appears to be quite an activist on Twitter.

56
00:06:19,340 --> 00:06:35,030
And this piece of investigative journalism shows that most of his funding comes from the Institute for Feed Education and Research or by Feeder.

57
00:06:35,320 --> 00:06:40,460
And by the way, if you're writing a proposal, you're doing a master's thesis or whatever.

58
00:06:40,790 --> 00:06:52,220
Get a catchy acronym. Okay. Cancer. Anyway, I feeder gets most of their research from this eye feeder.

59
00:06:54,380 --> 00:07:00,170
Association, which is funded by the industry by light livestock industry group like Cargill.

60
00:07:02,080 --> 00:07:13,250
Okay. So what does this tell you? Do you think he's objective in his assessment?

61
00:07:14,300 --> 00:07:20,430
Do you think he has a conflict of interest? Tell me more. What? Because he has a financial.

62
00:07:25,400 --> 00:07:35,210
If that's what's funding his research. UC Davis told me about what is Mr. Davis do to minimize conflicts of interest?

63
00:07:40,150 --> 00:07:44,050
Okay. What did we talk about last week when you were the.

64
00:07:47,370 --> 00:07:51,590
I repeat, I'm sorry. The week before or two weeks before. People remember.

65
00:07:52,310 --> 00:07:59,340
I forgot, I. I can't hear you.

66
00:08:01,590 --> 00:08:03,240
All right. Retention and retention.

67
00:08:03,420 --> 00:08:19,220
So what if you re down further in here is that California law requires researchers and professors to provide disclosure agreements.

68
00:08:19,980 --> 00:08:23,730
That what's a disclosure agreement? It says we're getting my money from.

69
00:08:24,510 --> 00:08:28,380
Okay. What I'm having an influence over. Okay.

70
00:08:28,780 --> 00:08:33,460
If I serve on the board of a company, if I serve on the board of a nonprofit.

71
00:08:34,310 --> 00:08:39,430
Okay. If I advise somebody and get money for consulting, I have to disclose that.

72
00:08:40,270 --> 00:08:43,450
Okay. University of Michigan. What is University of California?

73
00:08:45,070 --> 00:08:51,520
Researchers. Researchers must disclose funding from private donors, but not from nonprofit groups.

74
00:08:53,650 --> 00:08:57,639
The industry, Cargill, etc., is where they set up a theater.

75
00:08:57,640 --> 00:09:02,830
It's a nonprofit, send it to professors and they don't have to disclose it.

76
00:09:03,610 --> 00:09:09,800
What do you think? Big egg.

77
00:09:10,010 --> 00:09:14,110
There you go. Other thoughts?

78
00:09:19,600 --> 00:09:23,140
Is this a conflict of interest for this guy? He's a professor.

79
00:09:23,440 --> 00:09:27,309
Can't he say what he wants to say? Can't right now.

80
00:09:27,310 --> 00:09:31,540
Go to Twitter if I want. I'm not necessarily representing the University of Michigan.

81
00:09:31,550 --> 00:09:39,550
Right. And then there's something about your research and your work done.

82
00:09:45,190 --> 00:09:51,700
The institution, even when you're not in, it's not always crystal clear.

83
00:09:51,700 --> 00:09:59,050
But yeah, if I'm representing the University of Michigan or my research, I'm going to want to be objective and, you know, just for my own reputation.

84
00:09:59,440 --> 00:10:03,880
But, you know, for the university, I mean, they would like to see things displayed in a good way.

85
00:10:04,180 --> 00:10:08,320
And so I but I mean, if I going to say I don't agree with this policy,

86
00:10:08,920 --> 00:10:13,989
nothing stopping me from saying, okay, hopefully it's evidence based and so forth.

87
00:10:13,990 --> 00:10:19,530
But I mean, I certainly have changed my opinions. I imagine ten year tenure.

88
00:10:19,630 --> 00:10:22,540
He's full professor. He's very senior guy. He's well-known.

89
00:10:26,910 --> 00:10:35,160
There is more in this article about where he gets his money from and, you know, pictures of what animal feeding operations look like.

90
00:10:35,700 --> 00:10:42,980
And in fact, we're doing some research on methane right now to keep those confined animal feeding operations,

91
00:10:43,010 --> 00:10:47,280
massive source of distress, methane and other things.

92
00:10:48,600 --> 00:11:03,690
One of his group oh, the the other research was done by International Consortium went to the IPCC, the climate change analysis folks.

93
00:11:04,110 --> 00:11:08,700
You mean here? Yeah. Is work. Oh, my group is doing some research on that.

94
00:11:10,180 --> 00:11:18,670
Yeah. Is there anything the institutions can say that you could contact for free stuff like social media and misinformation,

95
00:11:19,150 --> 00:11:22,780
regardless of if you like, that university?

96
00:11:22,780 --> 00:11:27,700
Is there any legal responsibility that comes with that professorship title?

97
00:11:28,060 --> 00:11:31,330
I mean, I don't think so. But is there anything that can be done with that?

98
00:11:33,020 --> 00:11:34,160
What do you think should be done?

99
00:11:35,630 --> 00:11:41,350
I mean, I guess the most that we have is just what the social media companies have a place in screening for, misinformation, all that stuff.

100
00:11:41,510 --> 00:11:42,980
But like, give me a break.

101
00:11:44,000 --> 00:12:00,110
You know, but I think that's I think that most faculty would want to know their arguments and what they believe is supported by the research,

102
00:12:00,470 --> 00:12:05,150
what is, you know, why they have a reputation that they can have a topic.

103
00:12:07,250 --> 00:12:17,330
On the other hand, lots of folks at this university and overseas have done government service, done research, review panels and so forth.

104
00:12:17,630 --> 00:12:22,130
And they can say this policy is crazy. It makes no sense.

105
00:12:22,490 --> 00:12:25,760
You can see that editorial. You can do Twitter.

106
00:12:25,850 --> 00:12:27,170
Nothing stopping you from doing that.

107
00:12:27,890 --> 00:12:35,540
But if you just do it without a scientific basis as a science professor, that doesn't probably help the reputation.

108
00:12:35,540 --> 00:12:39,230
So this is wrong? Yeah, I would say peer pressure is pretty important.

109
00:12:40,790 --> 00:12:45,410
Misrepresenting university policy, if I'm speaking for that, could be problematic.

110
00:12:45,990 --> 00:12:53,400
But, I mean, I don't think this. At least it hasn't come across my desk very well.

111
00:12:53,940 --> 00:12:57,960
His Twitter handle is GHG, which I imagine it's greenhouse gas.

112
00:12:58,440 --> 00:13:05,459
Absolutely. Thank you. That's amazing. Greenhouse gas. I had a friend at EPA once in his business.

113
00:13:05,460 --> 00:13:10,380
Gorgeous, that expert. But anyway, he also.

114
00:13:13,080 --> 00:13:23,760
Thank you for looking at it. So anyway, Drew is saying this is a use of a method because he's talking about methane gas.

115
00:13:23,760 --> 00:13:29,760
Once it's in the atmosphere, it only last like five years compared to CO2, which was 100 years.

116
00:13:30,060 --> 00:13:33,450
And so he's saying, well, because it's only five years long,

117
00:13:33,450 --> 00:13:39,810
it's a way to justify high emissions as well through it saying this is completely inappropriate.

118
00:13:40,170 --> 00:13:44,440
So there's scientific disagreement here as well. Okay.

119
00:13:44,460 --> 00:13:51,150
And then more scenes of farming and and yeah, this article was published in The Lancet.

120
00:13:51,150 --> 00:13:55,080
Have you heard of The Lancet? The Premier Health Journal.

121
00:13:55,230 --> 00:14:08,330
Okay. And so really the goal the idea here is did industry support influence this guy's opinion?

122
00:14:08,540 --> 00:14:13,280
Whose conclusions? And I think everybody would say yes.

123
00:14:13,440 --> 00:14:22,380
Right. All right. So that brings up the question is it appropriate to take research funding?

124
00:14:25,350 --> 00:14:37,010
To do research industry funding to do research. You should fund research.

125
00:14:41,090 --> 00:14:49,360
Can you trust industry funded research? No, no.

126
00:14:49,740 --> 00:14:56,820
I think I mean, the guy responds. And I think, as we said, all the pieces that you have, the answer is no.

127
00:14:58,350 --> 00:15:04,409
But that may also just be like the examples that you bring up, I don't know, just an industry.

128
00:15:04,410 --> 00:15:08,010
And I think that's what has most of the money in this country.

129
00:15:08,020 --> 00:15:15,270
So I don't know if there would be any way to mandate certain amounts that companies just have to provide to research efforts or if actually,

130
00:15:15,270 --> 00:15:22,230
say, against. And if that would. Those are some good observations.

131
00:15:22,800 --> 00:15:29,160
I don't know if there's a way. Probably not. We don't spend a lot of money on Pew Research anyway.

132
00:15:30,210 --> 00:15:39,180
But who funds most research in the countries in the country? You know, this is private company.

133
00:15:40,980 --> 00:15:46,940
You know, there's all kinds of health related, medical related, drug related, military related, things like this.

134
00:15:46,950 --> 00:15:47,340
Okay.

135
00:15:47,620 --> 00:15:58,550
And, you know, everybody wants to get that next big contract, have the next new iPhone or whatever that is massive effort by industry to do this.

136
00:15:59,760 --> 00:16:08,880
So can you have industry funded research in a way which is helpful and supportive and objective and fair and.

137
00:16:14,870 --> 00:16:21,320
Let me bring it back to this particular thing. Who's interested in doing agricultural research and funding that?

138
00:16:21,350 --> 00:16:27,320
Well, obviously, big ag. But consumers are interested, too, right?

139
00:16:28,370 --> 00:16:38,150
Government should be interested. Is there a way that you could begin to make this industry, your sponsored research, more objective?

140
00:16:39,860 --> 00:16:42,170
Or should we just discounted? Or What do you want to do?

141
00:16:44,680 --> 00:16:53,320
I think it depends on like what stipulations are tied to the funding, like how they're being funded and what they're allowed to do.

142
00:16:54,130 --> 00:17:02,080
Because if they're writing a blank check, that's very different than if they're like, we're only going to fund it if you're looking into this.

143
00:17:04,590 --> 00:17:11,670
So in this case, for this particular center, which is called the clear center.

144
00:17:12,090 --> 00:17:18,210
Okay. Clarity and other clarity leadership for environmental research at UC Davis.

145
00:17:19,020 --> 00:17:25,620
There is a board advisory board. It includes Meat Institute, Cargill Feed Industry and so forth.

146
00:17:25,890 --> 00:17:33,570
And basically, the objective is funding research and advancing the industry into a more sustainable.

147
00:17:35,170 --> 00:17:41,600
Through innovation and research. Sounds fantastic. And it's an advisory board.

148
00:17:41,840 --> 00:17:45,050
It doesn't tell them what to do. It gives advice.

149
00:17:47,180 --> 00:17:55,980
So there is kind of a layer here insulating the advisory board from what this professor and his group does, maybe.

150
00:17:57,440 --> 00:18:03,979
But it didn't seem to work. Right, because it's clearly, you know, it's happening, this sort of research perhaps, you know,

151
00:18:03,980 --> 00:18:10,520
because I think it's interesting to ask this question, though, and it's an industry and it's like community groups.

152
00:18:10,880 --> 00:18:21,920
Like, I know you've done work on air quality control in Detroit and that has been like directed by others because I think we're

153
00:18:21,920 --> 00:18:28,680
much less likely to be mad about the funding and potential conflict of interest when it's like a grassroots organization.

154
00:18:30,050 --> 00:18:35,780
Well, that's also very interesting, too, because communities have their own perspectives and priorities as well.

155
00:18:37,460 --> 00:18:40,570
What do you want that down the road, though?

156
00:18:40,640 --> 00:18:49,219
Because I want to deal with industry first. Okay. I mean, here we're talking about billions of dollars in industry, I guess, and community groups,

157
00:18:49,220 --> 00:18:53,810
I mean, are lucky enough to have anybody, you know, have a meeting room or something like that.

158
00:18:53,810 --> 00:18:58,640
Everyone's just looking at his Twitter, his response to an article he wrote in April,

159
00:18:58,790 --> 00:19:06,200
and he talks about how like it's important to work with the people who, in fact, are really worried.

160
00:19:06,330 --> 00:19:12,290
I feel like he's making it sound like the industry organization is the same as like a community organization.

161
00:19:12,290 --> 00:19:20,090
So this is actually a very interesting way to go. Not what I agree, but I mean, what we talk about is stakeholders, okay.

162
00:19:20,390 --> 00:19:31,820
If we look at what the mission of this group is right here, advancing industries to sustainability, who are the stakeholders?

163
00:19:35,240 --> 00:19:42,590
Well, it's pretty clear to me that he defined the stakeholders as the beef producers, the feed industry and so forth.

164
00:19:43,190 --> 00:19:52,159
He's not looking at the consumer, probably not looking at the health implications, diabetes or whatever it is, you know, McDonald's fast food.

165
00:19:52,160 --> 00:19:55,190
He's probably not looking at the people very narrowly.

166
00:19:57,700 --> 00:20:01,120
It's not even looking at the farmers themselves. Maybe the farm.

167
00:20:02,120 --> 00:20:05,589
But yeah, we're not like small scale. I mean.

168
00:20:05,590 --> 00:20:10,840
Well, small scale gardening involved in, you know, a meat packing industry and growing the grain.

169
00:20:10,840 --> 00:20:13,900
But. Well, you would be surprised.

170
00:20:14,320 --> 00:20:22,720
Let's say, for example, in Michigan, I could have a small farm with an animal to where do I get the animal?

171
00:20:22,900 --> 00:20:30,520
What do you call it? Not butchering, but whatever it is, you have to go to a very large facility because there's only a few of them that do that work.

172
00:20:31,300 --> 00:20:33,610
So it's not it's completely small scale.

173
00:20:35,350 --> 00:20:45,700
Anyway, we're all targeted here, but what I'm after here is discussing with you how can industry do research that's perceived and is credible?

174
00:20:46,720 --> 00:20:52,540
That's the question, because they are doing most of the research in this country and in the world.

175
00:20:54,040 --> 00:21:02,640
Can you think of any ways to make it credible? We want solutions.

176
00:21:07,490 --> 00:21:11,240
Yes. Sure. It's just that.

177
00:21:11,580 --> 00:21:15,780
It's just closure. Absolutely.

178
00:21:16,680 --> 00:21:17,430
Good. What else?

179
00:21:22,370 --> 00:21:32,060
So he can disclose his getting money and then continue to tweet out, write opinion pieces for the state or whatever, you know, interpret things.

180
00:21:33,260 --> 00:21:41,950
What else can we do? I guess you need more sugar.

181
00:21:43,750 --> 00:21:51,700
I think we're broadcasting a variable.

182
00:21:51,730 --> 00:21:55,300
How do we do that here? Oh, perfect.

183
00:21:55,320 --> 00:21:59,410
Yeah, that's very helpful. Peer review. That's super. We're going to talk a lot about that.

184
00:22:00,070 --> 00:22:03,450
Okay. Yes, sir. Another question, I suppose.

185
00:22:05,240 --> 00:22:14,060
But in terms of reproducibility and peer to peer, these things are great and people actually reviewing understand that and look into that.

186
00:22:14,780 --> 00:22:20,210
But it's just kind of like when something's out there, it's out there and it's just.

187
00:22:20,330 --> 00:22:24,620
Do we have any ways that we've thought about your research to engage the public in

188
00:22:24,620 --> 00:22:30,320
looking into this very independent peer review and how we see people looking for it?

189
00:22:30,380 --> 00:22:33,740
Do you have more questions than I have answers for? You know, but absolutely.

190
00:22:33,740 --> 00:22:40,700
Great point. Okay. Just curious, in the public gauge, what's critical or what's a jump?

191
00:22:42,500 --> 00:22:50,540
Right. So this is a real issue. We need to bring the peer review process to some sort of public discourse.

192
00:22:53,580 --> 00:22:54,350
Those are great points.

193
00:22:54,360 --> 00:23:02,190
The other one that I want to add here was there are mechanisms to promote the use of industry funding in a way which is more objective and scientific.

194
00:23:02,750 --> 00:23:08,610
Okay. And for example, I participated in one research institute called the Health Effects Institute,

195
00:23:08,940 --> 00:23:12,420
which takes money from the car manufacturers and from EPA.

196
00:23:12,780 --> 00:23:13,859
They bring it together.

197
00:23:13,860 --> 00:23:23,610
They have a very objective panel, are good scientists, and they make grants to researchers to deal with pollution from vehicles and so forth.

198
00:23:23,970 --> 00:23:30,870
And they are at the top of their game and its structure matches industry and government.

199
00:23:32,340 --> 00:23:44,910
A similar sort of thing is done with the Wildlife Conservancy and Sierra Club and government and industry as well in applications.

200
00:23:45,180 --> 00:23:49,530
So their balancing or pairing these different parties together,

201
00:23:49,830 --> 00:23:55,770
and this is actually one sort of strategy where you bring in your competitors to help you.

202
00:23:56,940 --> 00:24:00,670
Okay. Or the antagonists or the animals, if you think that that's.

203
00:24:03,040 --> 00:24:16,150
Okay. Good. Uh, I wanted to, of course, show you a PowerPoint as my life is lost without PowerPoint university.

204
00:24:16,510 --> 00:24:22,740
And. I have no sense of time.

205
00:24:24,470 --> 00:24:29,420
And especially for a class that begins at 1130. Really confusing.

206
00:24:30,680 --> 00:24:41,660
So I've adapted a couple of things here. And, you know, day one, I want you to look at these readings day to the salt on the canvas side.

207
00:24:42,050 --> 00:24:43,880
There's a lot of topics to talk about.

208
00:24:45,230 --> 00:25:00,560
And there is some amazing text by Renee, who is or was the editor for the New England Journal of Medicine for many, many years and knows his stuff.

209
00:25:01,610 --> 00:25:05,810
And he has this statement which floors me.

210
00:25:06,530 --> 00:25:10,320
Okay, let's read it together here. Scarcely any bars to eventual publication.

211
00:25:10,340 --> 00:25:14,600
So we're talking about peer review as a way to ensure that stuff is credible.

212
00:25:15,920 --> 00:25:22,190
There seems to be no free to bring, no hypothesis to trivial, no literature, citation bias or even.

213
00:25:23,180 --> 00:25:30,880
Design work methodology to the presentation of results to inaccurately obscure or to contradict contradictory no one else itself.

214
00:25:30,900 --> 00:25:38,030
Further two circular no conclusions contrived to adjust the volume effects to or cancel to get published.

215
00:25:40,890 --> 00:25:46,410
So I think he has just thrown the peer review process out the window.

216
00:25:49,170 --> 00:25:53,070
That's amazing. Where was this published?

217
00:25:54,990 --> 00:25:58,500
Major, have you heard of that journal? Yes. Okay.

218
00:25:59,620 --> 00:26:03,410
My gosh. How can somebody say but.

219
00:26:06,150 --> 00:26:16,470
Any thoughts? I mean. Do you think the peer review process is supposed to be very scientific and inaccurate?

220
00:26:18,240 --> 00:26:22,020
Basically he says that anything can be accomplished. That's right.

221
00:26:22,500 --> 00:26:29,889
So peer review has a problem itself. Okay. And Rainie and others go into this and it's been influential, actually.

222
00:26:29,890 --> 00:26:37,560
You and I has done a number of things. And guess what? I'm sitting here talking to you about this because of this.

223
00:26:38,730 --> 00:26:43,200
Okay. This is a problem, and I h recognizes the problem.

224
00:26:44,910 --> 00:26:48,030
Okay. As an aside. Have you heard of predatory drugs?

225
00:26:48,960 --> 00:26:55,990
What are they? You have to pay.

226
00:26:56,040 --> 00:27:00,040
Yeah. What else? These days you have to pay for all the journals because.

227
00:27:00,670 --> 00:27:04,150
Oh, that's just the way it is, especially if you want open access.

228
00:27:04,690 --> 00:27:09,310
So it's like published a paper. $3,000. I have to cough up to get it out there.

229
00:27:12,200 --> 00:27:17,660
What's a predatory journal? They're just going to.

230
00:27:24,780 --> 00:27:31,560
And they just all that they had all the papers in and out.

231
00:27:32,550 --> 00:27:38,380
That's right. And why does that exist? Prone to get published in a piece of.

232
00:27:41,550 --> 00:27:46,800
We'll try to. Other journals that are well established.

233
00:27:46,850 --> 00:27:50,750
Yeah. Yeah, but why? I'm curious. Why? What's the motivation?

234
00:27:52,070 --> 00:28:00,230
Well, I mean, if you're a professor, you're trying to get tenure or you're a researcher trying to get a grant.

235
00:28:01,310 --> 00:28:05,030
It's publisher push. You don't have good research productivity.

236
00:28:05,810 --> 00:28:14,210
Like at least a couple of papers a year or more at the end of your tenure promotion are to say, publication record spotty.

237
00:28:15,050 --> 00:28:18,440
You're not going to make tenure. You're trying to get a grant.

238
00:28:19,310 --> 00:28:24,980
They're going to look at your productivity. They're going to say, Is there existing data that supports your hypothesis?

239
00:28:26,050 --> 00:28:31,250
You don't have anything published. It's not peer reviewed. Your chances are still open.

240
00:28:33,050 --> 00:28:36,290
So. So predatory journals. Yeah.

241
00:28:36,500 --> 00:28:41,810
Is just a way to get published because, you know, it's part of the process these days.

242
00:28:42,950 --> 00:28:47,840
Oh, another aside, this summer, have you heard of Frontiers?

243
00:28:48,050 --> 00:28:54,920
The journal Frontiers is a for profit publisher.

244
00:28:55,280 --> 00:28:59,060
They are the fastest growing scientific publisher in the world.

245
00:28:59,720 --> 00:29:02,780
I think they have 2000 journals right now.

246
00:29:03,200 --> 00:29:04,070
Something like that.

247
00:29:05,120 --> 00:29:13,670
They have an incredible growth model, super automated peer review system with use of artificial intelligence in all kinds of websites.

248
00:29:15,170 --> 00:29:25,130
They were they contacted me because they wanted to set up a new journal, Environmental Health, and they asked me to be the head of it.

249
00:29:26,120 --> 00:29:30,920
Okay. And I'm thinking, wow, that's really cool. I'd like to do that. So I went back into it.

250
00:29:31,190 --> 00:29:39,350
Ten years ago, they were cited for being a predatory journal. They there's no ranking of predatory journals.

251
00:29:39,530 --> 00:29:46,610
There was somebody that was ten years ago. They've said that their model is amazing, that they have solved their problems.

252
00:29:46,970 --> 00:29:51,290
They have a system that the and it's a British company.

253
00:29:52,490 --> 00:29:59,540
They have their offices in several different countries. They would select the CEO, the editor in chief, or whatever.

254
00:29:59,990 --> 00:30:09,920
Iceland, if I were to do this now, I think six different editors first year each on a different theme with the intention to grow to 36.

255
00:30:10,550 --> 00:30:16,580
Okay. Associate editors. Each of those associate editors is to have a team of 30 reviewers.

256
00:30:17,510 --> 00:30:24,110
Each of those teams is supposed to review something like 10 to 30 journal articles each year,

257
00:30:24,590 --> 00:30:30,620
and the goal is to publish on the order of 100 to 1000 articles each year in this new journal.

258
00:30:31,640 --> 00:30:35,990
Okay. Each of those articles has page charges.

259
00:30:36,020 --> 00:30:39,440
Okay. So they have to collect money to support this operation.

260
00:30:40,010 --> 00:30:45,510
They offered me $17,000 to do this. They said, I don't think I have time.

261
00:30:45,890 --> 00:30:51,200
How much work is involved? Oh, you just have to sort of set direction, help a little bit with selection,

262
00:30:51,200 --> 00:30:55,460
write an editorial now and then you can participate in any aspect of the process.

263
00:30:56,450 --> 00:31:00,080
The rest of the pyramid Ponzi scheme roll forward.

264
00:31:00,740 --> 00:31:04,490
Nobody's paid, okay? And the authors have to pay.

265
00:31:08,200 --> 00:31:12,610
And they are growing like a banshee. It's amazing how fast they're going to grow.

266
00:31:16,510 --> 00:31:24,370
And I've published in the Frontier Journals, in fact, the latest ones I've published, it's the most glowing reviews I ever have.

267
00:31:25,390 --> 00:31:31,630
I mean, it's good stuff. So there's maybe a role for this alternative model.

268
00:31:31,960 --> 00:31:37,240
In contrast, nature is quite different.

269
00:31:37,240 --> 00:31:41,410
But many journals or society journals. Okay.

270
00:31:41,410 --> 00:31:47,680
But most of them just to stay competitive, have to automate, have to get bigger and so forth.

271
00:31:47,920 --> 00:31:52,659
But nobody is like Frontier because they just have this.

272
00:31:52,660 --> 00:31:57,430
They want to capture the market. What do you think?

273
00:32:01,260 --> 00:32:08,920
Yeah. I mean, but I mean, I'm sure with no standardized way to research and evaluate.

274
00:32:09,490 --> 00:32:13,420
Peer review technique. I just like looking at this as like an article here.

275
00:32:13,520 --> 00:32:19,510
Is there any developed, like, methodology to see how good a peer review system.

276
00:32:19,690 --> 00:32:24,220
Oh, you are beautiful. You just segway. Yeah, that's wonderful.

277
00:32:24,610 --> 00:32:28,390
Okay, your comments are great. Yeah, exactly.

278
00:32:28,600 --> 00:32:40,700
But so, um. You know, stepping back for a second here, the idea of relying on peer reviewed data, you know,

279
00:32:40,700 --> 00:32:49,100
this statistic, inconsistencies in two thirds of studies, only six out of 53 studies could be replicated.

280
00:32:49,520 --> 00:32:51,680
If we look at some of the cancer literature.

281
00:32:52,490 --> 00:33:03,380
There were systematic problems with respect to blinding, with sample size, with analysis techniques, with replication.

282
00:33:05,750 --> 00:33:12,710
These reproducibility challenges are really significant, and this affects you.

283
00:33:13,970 --> 00:33:21,650
You go to your doctor, prescribes a script for you, and you look at the journals and they say,

284
00:33:21,650 --> 00:33:28,970
oh, yeah, we paid for that research that evaluated that clinical trial.

285
00:33:30,020 --> 00:33:37,900
Yes. Your drug company. And you've heard of publication bias, right?

286
00:33:38,350 --> 00:33:56,940
What's publication bias? If I get a negative result to my study, we're not gonna hurt my chances of wandering.

287
00:33:57,720 --> 00:34:00,150
Right. It doesn't seem to contribute to literature.

288
00:34:00,660 --> 00:34:08,280
On the other hand, if I get a slight indication that a new therapy, a new drug works, or if I go preliminary results, it's depressed.

289
00:34:10,710 --> 00:34:14,430
So there's really a balance here between what's published and what's not published.

290
00:34:19,620 --> 00:34:28,470
That's all peer reviewed. We're going to come back to peer review. But this notion of peer review is important to avoid big data.

291
00:34:29,130 --> 00:34:37,170
Okay. Or misrepresented data. And this is becoming well-recognized in 2009.

292
00:34:37,530 --> 00:34:45,750
The biggest case in the academic setting for fabrication was from Duke University.

293
00:34:46,110 --> 00:34:49,800
And basically a whistle blower claimed fraud.

294
00:34:49,830 --> 00:34:52,080
And this was investigated by age.

295
00:34:52,590 --> 00:35:04,230
And Duke misrepresented a number of research applications by taking data on context, using you to basically falsified data.

296
00:35:04,620 --> 00:35:12,510
And they agreed to pay $112 million as a penalty to the government for the use of this.

297
00:35:13,740 --> 00:35:16,880
Falsified research data. Okay.

298
00:35:17,420 --> 00:35:21,130
Falsified is different from interpretation. Okay.

299
00:35:21,140 --> 00:35:29,630
And, you know, I want to state that. But nonetheless, if the peer review process had worked, they probably wouldn't have gotten away with it as well.

300
00:35:33,430 --> 00:35:49,330
So you ask about peer review. Granny and others have set up a series of workshops and papers and to help reviewers do peer review.

301
00:35:50,840 --> 00:35:55,460
Questions are simple or experiments before. Okay.

302
00:35:55,500 --> 00:36:00,530
Do I know who's the case and who's the control? Were the experiments repeated?

303
00:36:00,530 --> 00:36:04,140
Were all the results presented for positive and negative controls?

304
00:36:04,160 --> 00:36:12,370
You know what I mean by positive or negative controls? In situations where I should see the results, I might do the test my better.

305
00:36:13,460 --> 00:36:17,830
Okay. Maybe. The age validated.

306
00:36:17,830 --> 00:36:30,600
Statistical tests appropriate. So if I go to do a peer review, I need to check off each one of these to ensure at least a minimal level of quality.

307
00:36:30,760 --> 00:36:33,930
Right. It's not rocket science. That's pretty easy to do.

308
00:36:34,830 --> 00:36:42,170
These are just flags. So the journals have started a more rigorous peer review process.

309
00:36:42,890 --> 00:36:49,880
Okay. And maybe this is why, you know, some of the reviews have seem to be getting longer, more iterations of things like this.

310
00:36:51,160 --> 00:36:54,580
But we still fall back on peer review.

311
00:36:57,120 --> 00:37:06,920
And so. Greater transparency in peer review itself is also really important, and the idea is that the system can fix itself.

312
00:37:08,620 --> 00:37:17,680
Do you agree? Do you think this is possible? Yeah.

313
00:37:19,910 --> 00:37:25,270
Some of the. So I do.

314
00:37:27,070 --> 00:37:30,390
And their papers. They don't care.

315
00:37:33,090 --> 00:37:45,090
Explaining all this. They are not transparent finding something out on the news or.

316
00:37:46,790 --> 00:37:53,380
Excluded from the experiment because some recent. Yeah.

317
00:37:53,390 --> 00:38:02,750
That's a really interesting and important point. So if you exclude data that pings, you know, then essentially you're misrepresenting, you know,

318
00:38:02,750 --> 00:38:08,900
one of the amazing things in the last ten or 20 years is nobody prints out hard copies anymore.

319
00:38:09,430 --> 00:38:13,610
At you guys, you don't have a notebook. You just have your laptop. Journals are all electronic.

320
00:38:14,360 --> 00:38:18,290
I push a button, I get the supplementary information or supplementary materials.

321
00:38:18,290 --> 00:38:24,320
You know what I mean? And in many cases, I think the data set that they have.

322
00:38:25,130 --> 00:38:29,250
Okay. Isn't that amazing? I can look at the data that took you years.

323
00:38:29,250 --> 00:38:33,530
Then they maybe a lot of money to collect and do my own analysis.

324
00:38:34,670 --> 00:38:42,590
That's amazing. So that could help. So this is a question peer review plus transparency.

325
00:38:44,040 --> 00:38:48,060
Plus access to the data. Maybe this one.

326
00:38:49,410 --> 00:38:52,530
Of course. I've got to know how to analyze the data.

327
00:38:53,010 --> 00:38:57,060
Probably have to be an expert as well. So it's not easy, but it's possible.

328
00:38:58,800 --> 00:39:10,170
So that's a real positive change. Okay. Still look, people are saying we need retraining, okay?

329
00:39:10,680 --> 00:39:14,790
We need to have you recognize this hypercompetitive environment.

330
00:39:15,480 --> 00:39:23,160
Publish promotion. And. We used to have to deal with publication practices, including publication bias.

331
00:39:23,550 --> 00:39:30,420
Today, we have to publish negative findings and we have to sort of tone down these preliminary studies that may not hold up.

332
00:39:31,860 --> 00:39:37,020
So I'm in here for the day and I'll be talking about how time consuming that is.

333
00:39:37,080 --> 00:39:40,110
And then you were just saying that the editors and peer reviewers who are.

334
00:39:41,270 --> 00:39:44,960
30, whatever paper. We are not being paid to do that.

335
00:39:45,380 --> 00:39:50,630
And I remember all also reaction that like you just peer review out of the goodness of your heart.

336
00:39:52,170 --> 00:40:03,440
So. And analyzing data and so I'm wondering if you wouldn't even catch those data like falsified data, even if depending on how much time you have?

337
00:40:03,980 --> 00:40:07,310
Absolutely. Well, you know, this raises the question, first of all, should you be paid?

338
00:40:08,880 --> 00:40:13,350
And in my case, who pays the cost even more? And the goodness of your heart?

339
00:40:13,360 --> 00:40:20,399
Yeah. So this weekend, you know, wonderful weather. But I stayed to work because I had to review 15 applicants for a Fulbright program.

340
00:40:20,400 --> 00:40:26,160
You know, this is the one that sends people overseas or vice versa. And 15 is quite a lot.

341
00:40:26,700 --> 00:40:32,970
And I did it. And then I read the instructions that are going to be handed out, which I declined anyway.

342
00:40:33,090 --> 00:40:36,300
So it's just not worth giving somebody my Social Security number.

343
00:40:38,280 --> 00:40:44,790
But yeah, I mean, it's a problem. So if you should do it, you need experts.

344
00:40:46,170 --> 00:40:51,870
But you know, in addition to being sort of altruistic because if I do, somebody else is going to have to review.

345
00:40:52,500 --> 00:40:59,430
I do learn about the field, I get the information, I get citations if I want to look at them that they've been using.

346
00:40:59,520 --> 00:41:03,210
I don't plagiarize it because I'm not allowed to do that.

347
00:41:03,630 --> 00:41:07,320
But it does educate. It's a great way to figure out.

348
00:41:10,050 --> 00:41:15,630
I feel that out of the air if you're doing the same thing you're doing.

349
00:41:15,640 --> 00:41:18,030
I didn't even have that. Yeah.

350
00:41:18,030 --> 00:41:29,640
So the air peer review is more like to ask you these kinds of questions and it will go through your review and see if you if you address them,

351
00:41:30,000 --> 00:41:35,130
it will go look at the text, look at the references to see if there's plagiarism.

352
00:41:35,790 --> 00:41:40,920
And it will go beyond that. This isn't plagiarism. So it's not actually doing the review.

353
00:41:41,130 --> 00:41:48,000
It's just augmenting you. And so if you suggest something and you write it in, it's going to maybe push you a little bit.

354
00:41:53,250 --> 00:42:02,129
So the National Institute for Neurological Disorders and Stroke, which is one of the NIH of one of the institutes, could be different.

355
00:42:02,130 --> 00:42:06,810
Workshops have recommendations to improve peer review.

356
00:42:07,170 --> 00:42:13,800
Some of the things are noted or noted here randomization, blinded data, handling other things.

357
00:42:14,280 --> 00:42:18,150
And basically to make this more transparent.

358
00:42:19,230 --> 00:42:22,650
Okay. I want to talk about one more thing in the last 2 minutes.

359
00:42:23,430 --> 00:42:28,610
So we're going to have to do a five hour zoom session for my work. Oh.

360
00:42:30,720 --> 00:42:41,040
That's why I need to sugar. So even with good peer review, even with balance between industry funding and so forth,

361
00:42:41,460 --> 00:42:49,350
I still need to be objective as a scientist and I bring lots of baggage as everyone else does.

362
00:42:49,350 --> 00:42:53,670
I look to put analysis to a fault, to all hypotheses.

363
00:42:54,240 --> 00:42:57,630
And this is one set of fallacies.

364
00:42:58,170 --> 00:43:02,370
It's a fallacy, as we call them, that affects each of us when we approach.

365
00:43:03,510 --> 00:43:07,980
The first one is essentially called a hypothesis.

366
00:43:08,400 --> 00:43:15,840
I am collecting evidence to support a hypothesis, evidence against it, and ignoring other explanations.

367
00:43:16,290 --> 00:43:20,160
So I bring a frame theoretical model.

368
00:43:20,520 --> 00:43:25,020
I'm looking to support that things that don't support it all for it.

369
00:43:25,470 --> 00:43:29,580
That's not right. That's not fair. But that's part of my training.

370
00:43:29,700 --> 00:43:35,080
I need to break free of that. Texas sharpshooter.

371
00:43:35,290 --> 00:43:43,900
I don't know why Texas, but seizing on random patterns in the data and misinterpreting them for actual findings, for interesting findings.

372
00:43:44,800 --> 00:43:48,820
This is always something I do a lot of data analysis.

373
00:43:49,120 --> 00:43:54,640
I expect certain things to behave because I understand how the data were generated, what affects them.

374
00:43:55,360 --> 00:44:03,190
And what I find interesting patterns. I hope they're real, but I recognize they could be random.

375
00:44:03,730 --> 00:44:07,630
So I need to do statistical analysis or other things to make sure that they are real.

376
00:44:11,480 --> 00:44:19,430
Asymmetric attention is quite interesting. Rigorously checking for unexpected results, but giving expected ones a free pass.

377
00:44:21,820 --> 00:44:25,270
So if things fit my model, I feel great.

378
00:44:25,860 --> 00:44:31,120
I write the paper and go on. If things don't, I'm looking for flaws with those data points.

379
00:44:32,390 --> 00:44:36,220
Okay. In fact, there was a video that we asked you to look at today.

380
00:44:36,250 --> 00:44:39,640
Did you happen to see that? Was that one outliers?

381
00:44:42,930 --> 00:44:47,079
I think it was on outliers. Okay. And, you know, they say, oh, these can't be real data points.

382
00:44:47,080 --> 00:44:50,590
So they disagree. But be careful, okay?

383
00:44:51,940 --> 00:44:57,160
Can't sensor data without rules just because they don't fit your model?

384
00:44:57,580 --> 00:45:00,490
It's not a good rule. That's inappropriate.

385
00:45:01,900 --> 00:45:13,120
And then the last fallacy is just storytelling claims, but finding stories after the fact that rationalize whatever the results turn out to be.

386
00:45:15,240 --> 00:45:23,940
So I'm working on, you know, a number of papers and I'm finding some weird things at some places, some locations or whatever.

387
00:45:24,370 --> 00:45:28,200
I understand what might influence them. I can tell some good stories.

388
00:45:29,730 --> 00:45:34,490
I can get that published. But is that fair? Be careful.

389
00:45:34,730 --> 00:45:42,140
Okay. This is where you need to recognize this cognitive fallacy that you're bringing to.

390
00:45:43,130 --> 00:45:51,650
That makes sense. So anyway, there's a little bit more here.

391
00:45:51,950 --> 00:46:00,890
This is really all of these fallacies, these what we call a crisis of confidence about reproducibility.

392
00:46:01,520 --> 00:46:10,760
And it goes well beyond fraud. It really goes to improving your awareness of these issues so that you don't you're not brought down by it.

393
00:46:11,360 --> 00:46:17,600
Okay. So I think that's really, really important. I think we're out of time.

394
00:46:18,350 --> 00:46:25,790
So I guess we'll continue next week. Please look at the readings and let me grab the a few here.

395
00:46:27,350 --> 00:46:38,720
Thanks so much. You guys have a good. Rock call for all the world.

396
00:46:41,620 --> 00:46:48,490
To learn how to study like classical sciences in my undergrad.

397
00:46:50,500 --> 00:46:50,830
I'm.

