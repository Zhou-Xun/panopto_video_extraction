1
00:00:01,642 --> 00:00:07,352
So why don't you know how to use it?

2
00:00:08,772 --> 00:00:13,732
The Republicans in the US from Texas am convinced that assistant professor.

3
00:00:14,842 --> 00:00:18,802
He's been there since 2018. Before that? Post facto so.

4
00:00:19,372 --> 00:00:26,272
At least ostensibly. And then before that was interesting.

5
00:00:27,002 --> 00:00:36,441
I mean, works in graphical models is not the beginning of the applications to this medicine because

6
00:00:36,442 --> 00:00:43,072
the microbiome will be on glucose days becoming a graphical model for discovery regulations.

7
00:00:46,272 --> 00:00:54,292
Thank you very much for the invitation and thank you for the invitation as my great pleasure to be here to talk about my research in front of you.

8
00:00:55,792 --> 00:00:57,862
So I study gene regulation now.

9
00:00:58,882 --> 00:01:07,492
So what are you remember now as a graph that summarizes the regulatory relation among a set of genes and maybe the relationship,

10
00:01:07,492 --> 00:01:11,482
I think the main causal relationship which gene disintegration is.

11
00:01:12,592 --> 00:01:19,682
So this is a textbook well-established pathway that involves a set of genes, including 1853.

12
00:01:20,662 --> 00:01:26,571
The reason why biologists are so obsessed with this is because they typically

13
00:01:26,572 --> 00:01:32,181
underlie important functions such as cell differentiation division roles,

14
00:01:32,182 --> 00:01:38,572
in that their disruption or malfunction often lead to genetic diseases such as cancer.

15
00:01:39,982 --> 00:01:44,152
So how traditionally biologists establish this pathway?

16
00:01:44,602 --> 00:01:48,742
One doesn't give one example. This is very nicely outlined in this paper.

17
00:01:49,602 --> 00:01:54,082
It's about two genes 80 and and and two in this pathway.

18
00:01:54,802 --> 00:02:07,192
So they first have to come up with the hypothesis, you know, how do you describe in the paper, no drugs, no p53, one gene that regulates cell death?

19
00:02:07,562 --> 00:02:14,381
And so that's one prior knowledge. Second, p53 and Mdm2 the fifth.

20
00:02:14,382 --> 00:02:18,352
And that you saw on the right hand side, on the right side of this order.

21
00:02:18,802 --> 00:02:21,862
They form a feedback loop, so they are called in of each other.

22
00:02:22,472 --> 00:02:28,942
It's the field of information they need and they also have it to know it's a process cell back.

23
00:02:29,842 --> 00:02:37,432
And then based on their three sites or knowledge, they hypothesize that 18 they act on.

24
00:02:37,472 --> 00:02:45,172
And then two does not mean that's actually how they wrote in the paper and that's how they actually come up with this hypothesis.

25
00:02:45,352 --> 00:02:52,912
And then later they provide, you know, they perform experimental validation to actually verify this causal relationship is actually true.

26
00:02:55,042 --> 00:03:00,822
Well, this hypothesis generation procedure is required.

27
00:03:01,032 --> 00:03:08,242
A prior knowledge is actually, in this case, use three pieces of orthogonal information regarding the biologic system.

28
00:03:08,842 --> 00:03:16,042
And it is always very subjective because I'm sure it could come up with other pathway that can also explain these three facts.

29
00:03:16,282 --> 00:03:22,761
So how about entity directly? P53 got it through and GM to let it be either positive.

30
00:03:22,762 --> 00:03:27,902
That can also leads you to suspect but is subjective and requires a lot of prior knowledge.

31
00:03:28,792 --> 00:03:32,542
And furthermore, we have tens of thousands of genes, you know, genome.

32
00:03:32,872 --> 00:03:38,872
So if you just look at one parent, genotype is obviously not very scalable for statistical screening.

33
00:03:41,272 --> 00:03:49,492
So what would be nice if we can somehow automate this hypothesis generation procedure based on observations alone,

34
00:03:49,942 --> 00:03:53,302
which are plenty thanks to next generation sequencing technology.

35
00:03:53,442 --> 00:03:57,862
So there are many observational data available online in the public domain.

36
00:03:59,512 --> 00:04:06,952
If I told you, in fact, with an appropriate design for the discovery algorithm, one can indeed lie about a pathway.

37
00:04:07,772 --> 00:04:15,502
Right. This sort of textual pathway within a reasonable modern era without using much prior knowledge at all.

38
00:04:18,812 --> 00:04:21,902
I'm not saying that we should get rid of expecting nothing at all.

39
00:04:21,902 --> 00:04:25,952
We should not use prior biologic knowledge available.

40
00:04:26,492 --> 00:04:35,402
Well, what I am saying is they call a discovery organism, which is probably design and use that they can help you to excavate,

41
00:04:35,612 --> 00:04:45,212
help you and expedite the scientific discovery process by coming up with more focused hypotheses through a data driven, objective way.

42
00:04:46,442 --> 00:04:53,642
Now, you may wonder that this maybe this may sound too good to be true to you, because, after all, we only have observational data.

43
00:04:54,002 --> 00:04:57,842
How can you come up with something right as something that is causal?

44
00:04:58,832 --> 00:05:03,902
Why do we need to make assumptions? Some of the assumption that even hard to verify in practice, for example,

45
00:05:04,352 --> 00:05:11,432
typically people would assume there is a measure on us which can never really be verified in practice because they're not measuring our flour.

46
00:05:11,792 --> 00:05:16,771
All we have to do or we need to do in this case is we need an is it has to be at

47
00:05:16,772 --> 00:05:21,262
least transparent about those assumptions so that practitioners who are using table,

48
00:05:21,602 --> 00:05:27,212
the method you develop of algorithm develop, they can make their own judgment call whether this particular problem is going to be

49
00:05:27,212 --> 00:05:31,322
an algorithm with this particular set of assumptions in their application or not.

50
00:05:31,722 --> 00:05:38,792
And that's the focus of all of this talk and the tool we're going to use to discover causality

51
00:05:38,882 --> 00:05:44,102
or generating causal hypotheses based on observational data is called Product Traffic Model,

52
00:05:45,272 --> 00:05:50,912
which was pioneered by computer scientist, you know, Prof. And a few philosophers from CMU.

53
00:05:51,392 --> 00:05:55,772
Peter spread his problem out in recessions in the eighties, in the 1980s.

54
00:05:56,522 --> 00:06:00,032
So I don't want to get into the details of the model just yet.

55
00:06:00,392 --> 00:06:07,172
But on the high level, because the work model consists of two component, one is a graph, a causal diagram,

56
00:06:07,472 --> 00:06:15,652
which delineates how different variables in a system totally related to each other and a property model that pair with that cause.

57
00:06:15,662 --> 00:06:25,832
A graph tells you how to generate observation from the call and call method model also can tell you what would happen if we somehow,

58
00:06:25,832 --> 00:06:28,442
you know, we intervene some of the variables in a system.

59
00:06:29,042 --> 00:06:37,742
For example, if I need to be variable X to that will generally change the distribution of x 1 to 3 in this case,

60
00:06:38,132 --> 00:06:44,612
because the intervention actually is no longer listen to x1s3 using that language.

61
00:06:45,512 --> 00:06:54,572
So actually the pro invented a whole new kind of a screen will allow you to calculate that intervention, distribution plus the system.

62
00:06:55,592 --> 00:06:59,972
This topic will focus on intervention because we're going to use observational data suffices to

63
00:06:59,972 --> 00:07:05,582
say that observation intervening or conditioning and intervening are generally very different.

64
00:07:05,912 --> 00:07:11,132
And a typical example you see in this context is, I suppose X one is sprinkler,

65
00:07:11,162 --> 00:07:15,512
whether it's also x three as well as being that you're going to be on up.

66
00:07:15,782 --> 00:07:23,912
And let's suppose a sprinkler system is designed so that it's independent of the weather of the day, so they are largely dependent.

67
00:07:23,912 --> 00:07:29,622
The next six to is whether the grass a sprinkler is whether or not right.

68
00:07:29,672 --> 00:07:33,692
So obviously the status of sprinkler and whether it was raining in a day.

69
00:07:34,022 --> 00:07:37,922
Yeah. Cause of whether the grass is going to be wet or not.

70
00:07:39,062 --> 00:07:45,332
So if we observe the grass to be wild that knowing that sprinkler wasn't on during the day,

71
00:07:45,332 --> 00:07:49,952
that will increase the probability of aspin raining because one of water causes has to explain the effect.

72
00:07:50,702 --> 00:07:53,942
Let's say the fact that grass wetness.

73
00:07:55,022 --> 00:08:01,862
So actually conditional on this x two variable x1x3 becomes dependent on our hand.

74
00:08:01,862 --> 00:08:06,692
If I have ecosystem, if I'm just pour a glass of water, a bucket water on the grass,

75
00:08:07,022 --> 00:08:13,022
not knowing the grass as well tells me really nothing about when it was raining, when a reason was not.

76
00:08:13,322 --> 00:08:22,532
Therefore these two variable were remain independent of each other. So in some way observation or conditioning is very different from intervening.

77
00:08:23,882 --> 00:08:24,932
Like I said earlier,

78
00:08:24,992 --> 00:08:30,842
we're going to focus on the first case where we only have observational data that are thought to be generated from some kind of reference.

79
00:08:31,892 --> 00:08:37,711
And the central question we ask in just talk is whether and when and to what condition can.

80
00:08:37,712 --> 00:08:42,242
But true data generated call the graph get identifiable based on observational data.

81
00:08:42,252 --> 00:08:50,431
All right. It's not a completely hopeless problem because this call the graph as well as the

82
00:08:50,432 --> 00:08:56,962
joint distribution on the right side implies a set of dependencies particular well,

83
00:08:56,972 --> 00:09:02,281
this graph here's the three dependencies relation and it's encoded in this graph openness

84
00:09:02,282 --> 00:09:08,282
distributions and for dependance rulership we can in principle giving those sample size,

85
00:09:08,492 --> 00:09:14,572
we can test for observations. They are not. Oh, that's not actually, I'm going to give you one example, a quick example,

86
00:09:14,722 --> 00:09:19,942
where actually we can use this three dependance relationships first, a set of assumptions.

87
00:09:20,182 --> 00:09:24,802
We can indeed identify this because again, if this graph is a true energy generated graph.

88
00:09:26,612 --> 00:09:33,562
All right. So right off the bat, we need to make the first assumption that the graph is basically meaning that there's no feedback loops.

89
00:09:34,342 --> 00:09:41,352
So if you're willing to make this assumption, then we are there are 25 graphs for three nodes X, Y and Z.

90
00:09:42,292 --> 00:09:48,441
So there's three dependencies that you saw on the last night and see whether we can write down the right on the roof.

91
00:09:48,442 --> 00:09:58,122
Again, the first assumption or the first dependencies was x one and x three are marginally independent of each other.

92
00:09:59,522 --> 00:10:06,272
So if you look at this graph, you know the graph with orange background and a common feature of these is x1x3.

93
00:10:06,272 --> 00:10:11,002
Now quarterly revisions you can do x one is a cause of x three or x17.

94
00:10:12,752 --> 00:10:21,382
So if you making those so-called causal physical assumptions which basically says to calculate related variables should not be independent data.

95
00:10:21,812 --> 00:10:23,372
So if you're willing to make this assumption,

96
00:10:23,762 --> 00:10:30,842
then we can safely remove all these graphs with orange background because they are not compatible with the data.

97
00:10:31,352 --> 00:10:40,522
So if you remove those. The second assumption, the second the dependencies we have on the last slide was x1x3x1x2.

98
00:10:40,932 --> 00:10:45,342
They are marginally dependent. Okay. So go look at these three graphs.

99
00:10:46,032 --> 00:10:49,812
Noted that x1x2 are not connected and often regress.

100
00:10:50,712 --> 00:11:02,142
So if we addition some causal markers and a no confounder assumptions for the lack of really just f two variable are not causally related.

101
00:11:02,172 --> 00:11:05,262
They should appear to be dependent. Sorry, this should be up here.

102
00:11:05,382 --> 00:11:13,242
This should be dependent on the essence. If you are making this traditional assumption that this three graph weighs or is back on.

103
00:11:13,632 --> 00:11:18,222
Again, not they are not compatible with a dependance relationship between the two.

104
00:11:18,432 --> 00:11:21,122
Now that we can test for in observation. Yes. Yeah.

105
00:11:21,132 --> 00:11:28,302
So I don't quite understand the location of dependance, but the dependance doesn't mean there's a direction of association.

106
00:11:28,722 --> 00:11:34,332
You have this derived x ray so that from 2 to 1 or something down in the second,

107
00:11:34,332 --> 00:11:40,332
what you are basically trying to check whether or not are dependent, but you are not checking the direction?

108
00:11:40,932 --> 00:11:47,232
Not yet. Not yet. So right now, the dependency, meaning they are dependent, they may be directed at relationship between the two.

109
00:11:47,562 --> 00:11:51,072
Maybe there is just a common confounders, so we don't know yet.

110
00:11:51,222 --> 00:11:57,822
So right now I just assume x1x2 our. We don't know whether there's ax1 called x two or the always around.

111
00:11:58,232 --> 00:12:06,582
So if that's the case that to just use this X rather than the right to collect to to make connections,

112
00:12:07,752 --> 00:12:13,142
you can but this procedure I'm talking about is you study from 25 and the using this condition

113
00:12:13,152 --> 00:12:18,012
to energy independence to eliminate graph that is not compatible with this set of rules.

114
00:12:18,252 --> 00:12:22,932
And again, we will show that for this particular example, we can eliminate all but one graph.

115
00:12:25,172 --> 00:12:29,042
I'm just a little confused by that. Director Edge that that's.

116
00:12:29,262 --> 00:12:34,502
Yeah, but anyway, that's fine. I understand. So far, there's no direct magic that I'm trying to draw on this.

117
00:12:34,502 --> 00:12:38,072
What? I'm trying to eliminate. Any other questions?

118
00:12:40,502 --> 00:12:47,441
All right. So we actually think we're willing to make a lot of assumption and look for assumption that we should be able to

119
00:12:47,442 --> 00:12:52,302
eliminate these three groups because excellent actors should be independent because they are not causally related.

120
00:12:52,302 --> 00:12:58,832
But in your data, they are dependent. So again, not compatible with available anything extra.

121
00:12:58,842 --> 00:13:04,592
The three are dependent and allows us to eliminate these two grass clippings at the same reason because three

122
00:13:04,592 --> 00:13:10,112
are not causal 8 minutes x two but enhanced data they appear to be because they appear to be dependent.

123
00:13:11,722 --> 00:13:20,282
So well if it for once now so let's look at this graph just to excellent next three are not directly causal related three directly

124
00:13:20,522 --> 00:13:28,592
related to the first graph x one is the indirect cause of x three in a second graph x one is an indirect effect of that three,

125
00:13:29,762 --> 00:13:36,362
although they're not directly causal related, there should be some dependencies because there is indirect concentration between

126
00:13:36,692 --> 00:13:41,822
three and once again the data tell us that x1x3 are marginal independent,

127
00:13:41,822 --> 00:13:45,992
therefore these two are not compatible. We can safely move this to as well.

128
00:13:48,032 --> 00:13:58,442
And lastly, let's look at this graph where x two is the common cause of x1x3, although it's 1x3 are not causally related to each other,

129
00:13:58,712 --> 00:14:05,582
but because they share the same common cause again, they should be marginally dependent, but our data says they are not.

130
00:14:05,942 --> 00:14:11,582
The one classic example in this case is by looking at the correlation or dependencies association

131
00:14:11,582 --> 00:14:18,002
between the number of Nobel Prize won by each country versus chocolate consumptions for each country,

132
00:14:18,572 --> 00:14:20,672
they are apparently not causally related,

133
00:14:21,032 --> 00:14:28,742
but they are actually quite strong correlation between these two variable because they most likely share a common cause,

134
00:14:28,802 --> 00:14:34,912
which is the income level of the country. So for that reason, even if there are two variables that's not causally related,

135
00:14:34,922 --> 00:14:39,452
but if they share a common cause, they should be marginally independent.

136
00:14:39,752 --> 00:14:42,932
But our data tells us that it is not independent.

137
00:14:43,262 --> 00:14:50,162
Therefore this graph can be attributed as well. So notice that we now only left with only one graph.

138
00:14:50,432 --> 00:14:55,532
I didn't really use the directions of the dependencies. It's really just by way of eliminations.

139
00:14:55,532 --> 00:15:02,941
So autograph that I emitted and not compatible with this dependance and that's looking and we're end up with

140
00:15:02,942 --> 00:15:10,052
this graph is actually compatible with these three dependencies friendships and of course there is a catch.

141
00:15:10,322 --> 00:15:21,662
And so in addition to you have you have to making those assumptions like I said, some of which can now be checked for official data.

142
00:15:22,432 --> 00:15:33,182
So this is a that with this approach, you may not always be able to identify, you may not always be able to eliminate all but one graph.

143
00:15:33,482 --> 00:15:39,692
In fact, you can partition your graph space. In this case, just 25 runs into so called model equivalent classes,

144
00:15:40,412 --> 00:15:44,792
and within each class all the grass will have the exact same dependencies segmentation.

145
00:15:45,392 --> 00:15:52,412
Therefore, if you just look at dependance on independence, you can already further tell a pop graph that belongs to the same same class.

146
00:15:53,062 --> 00:15:56,702
Some of the happen to have one singleton.

147
00:15:56,972 --> 00:16:04,982
That's the example I gave you. So sometimes you're lucky if the true data generating graph devoted to a class that only has one graph.

148
00:16:05,342 --> 00:16:07,562
So you're not okay if you're willing to make false assumption.

149
00:16:07,862 --> 00:16:15,272
If you can identify the to call the graph by many articles equivalent causes coconut coded in this table.

150
00:16:15,422 --> 00:16:19,442
For example, an afro you have a six kilograms.

151
00:16:19,472 --> 00:16:24,742
They all have very different causal meanings, but unfortunately we have exact the same conditional events,

152
00:16:25,042 --> 00:16:30,452
dependance independence of relationships, namely there's none. So all variables are dependent on that.

153
00:16:30,602 --> 00:16:37,412
Therefore you can really further reduce the size of the element cost to one in which case you will be able to identify.

154
00:16:39,802 --> 00:16:48,352
In fact, the math that I basically illustrate with this toilet example is called constraint based method, which really just a way of elimination.

155
00:16:48,712 --> 00:16:57,202
So observation data imply that settling dependencies and independent dependencies allows you to constrain your model space to equivalent causes.

156
00:16:58,252 --> 00:17:08,092
And one of the most well-known alloys and folding to this cause is called PCL algorithm, invented by Peter Sturgess and ended in 2000.

157
00:17:08,612 --> 00:17:14,662
And because we're only use independent relationships, we can only arrive at equivalent costs.

158
00:17:16,532 --> 00:17:21,111
And while you may wonder that independence dependance is just one statistical properties

159
00:17:21,112 --> 00:17:27,082
19 for fun observation made up of on other property like moments or distribution family.

160
00:17:27,742 --> 00:17:33,812
Indeed, there is another popular classical model called model methods called model based causal

161
00:17:33,832 --> 00:17:39,082
discovery methods where people make assumption a specific assumption on the data,

162
00:17:39,742 --> 00:17:44,722
for example, for continuously that people have been using Gaussian as assumptions for discrete data,

163
00:17:45,292 --> 00:17:49,732
categorical data, categorical distribution of multinational distribution is very common.

164
00:17:50,892 --> 00:17:58,762
And unfortunately you can prove that even with this additional assumption on distribution family, all you can find is still Euclidean causes.

165
00:17:59,092 --> 00:18:04,602
You can not really identify any because of graph in general. So the question is, can we do better?

166
00:18:04,612 --> 00:18:13,132
Can we really identify a unique hologram? Always, no matter what the trajectory of generating graph is, it is of course possible.

167
00:18:14,482 --> 00:18:23,632
In 2006, in the seminal paper Shimizu and all said that they establish that for continuous data.

168
00:18:24,232 --> 00:18:32,152
And as soon as you deviate away from God entity, if you assume all the variables are non Gaussian, then you can.

169
00:18:32,152 --> 00:18:36,652
Indeed, I don't know. Life is unique because it doesn't matter what this kind of graph looks like.

170
00:18:36,652 --> 00:18:40,342
We can always identify a unique call the graph based on observation data.

171
00:18:41,452 --> 00:18:47,862
So if you are looking, you know about the more context, if you are interested in causal, plausible model is really not remarkable paper.

172
00:18:47,872 --> 00:18:50,632
I encourage you to check out in a paper.

173
00:18:51,112 --> 00:18:59,092
You actually first establish that equivalence between independent component analysis model as a model and a causal discovery algorithm.

174
00:18:59,572 --> 00:19:05,152
And if you are familiar with I say, you know, identify ability of ice is very much established.

175
00:19:05,512 --> 00:19:09,232
So if you all have known Gaussian signals, you can identify the source.

176
00:19:10,492 --> 00:19:19,132
And they established equivalence between the two model and used the youth identified base theory for ICI to prove identify beauty of cosmos.

177
00:19:20,252 --> 00:19:23,302
And I think that they assume that this is a meta.

178
00:19:23,482 --> 00:19:30,502
People extend out to secret wealth in ASADA and all in 28 I think 9696296.

179
00:19:30,742 --> 00:19:35,512
Many people came out every year to prove cause, identify, ability and different functions.

180
00:19:38,182 --> 00:19:44,052
Well, despite there's a large literature on closet scarring, there are some limitations of existing approaches.

181
00:19:45,412 --> 00:19:48,321
For this thought, I would like to highlight just one limitation.

182
00:19:48,322 --> 00:19:56,272
One major limitation I feel that we need to address is most of the general purpose culture, discovery hours and publishing machine only conferences.

183
00:19:57,142 --> 00:20:01,252
They're not specific to multiple media, which I'm interested in.

184
00:20:02,332 --> 00:20:11,211
In other words, there are many features. Don't make our own data that are not take into account with existing general outcomes.

185
00:20:11,212 --> 00:20:15,352
And just to name a few, omega data tend to be highly noisy.

186
00:20:16,072 --> 00:20:22,012
They tend to be highly skilled, uh, for single cell data or metagenomics data.

187
00:20:22,282 --> 00:20:27,152
They are zero in for an account. For on it, which I'm really interested in.

188
00:20:27,422 --> 00:20:30,422
And the, you know, the samples tends to be heterogeneous.

189
00:20:31,502 --> 00:20:38,292
What do you reckon that works? There's no knowledge that there may be feedback loops where most of the method assume is basically.

190
00:20:39,002 --> 00:20:47,402
And lastly, a typical assumption of discovery is there's a measurement, father, which may not be true, and it's very difficult to find in practice.

191
00:20:48,422 --> 00:20:54,392
So because of those limitation of his features of Multi-Omics study in the past 2 to 3 years,

192
00:20:54,932 --> 00:21:01,802
we developed a suite of products out of ours that specifically explicitly address those limitations.

193
00:21:03,572 --> 00:21:06,272
Today, I would like to tell you about three simple ideas.

194
00:21:07,232 --> 00:21:11,732
Feel free to stop me any time if you want, because I really don't have to finish all the three ideas.

195
00:21:12,062 --> 00:21:19,502
I'll be really happy if I can explain just one idea and hopefully can convince you to join the effort to the discovery.

196
00:21:20,402 --> 00:21:25,382
All right. Any questions about. Right.

197
00:21:26,372 --> 00:21:30,812
First of all. Well, this is a project on discrete types of discrete data.

198
00:21:30,872 --> 00:21:35,512
So we propose it basically graph directly, second graphs, and we're still looking in the area where there's no feedback loops.

199
00:21:36,002 --> 00:21:40,172
And hopefully towards the end result, we're going to extend that to with no second graph.

200
00:21:41,342 --> 00:21:44,732
So anyway, so we're disorganized.

201
00:21:45,392 --> 00:21:47,221
Well, we're not as quickly as possible,

202
00:21:47,222 --> 00:21:55,021
but I will just go ties genomic data into three levels and just quantify so that we have interpretation of the category of that.

203
00:21:55,022 --> 00:21:59,162
This could have as low expression, average expression to high expression.

204
00:22:00,272 --> 00:22:03,722
So of course you're asking, well, why do we use good times data?

205
00:22:03,812 --> 00:22:10,772
Obviously with information we that it's good to ization what is true, but they are just ghettoization also have some first.

206
00:22:12,182 --> 00:22:19,262
First of all methods based on this witness data are generally more robust to high level voice of the role measurements.

207
00:22:19,712 --> 00:22:26,732
And if you look at, you know, informatics literature or biological literature, you know, discrimination is actually quite common argument.

208
00:22:26,732 --> 00:22:35,582
There is the long measurement you take from, say, next next generation sequencing technology may not be as meaningful.

209
00:22:35,582 --> 00:22:40,412
The role of measurement may not be as meaningful as you might think. Because the very high level of noise.

210
00:22:41,982 --> 00:22:48,372
Second methods based on data democratization also tend to be more robust to skewness.

211
00:22:48,762 --> 00:22:53,682
Like I said earlier, especially for single cell data, you typically have a large mass around small counts,

212
00:22:53,832 --> 00:22:57,962
but for certain cells, that may be extremely large counts as well.

213
00:22:57,972 --> 00:22:59,622
So it's highly skilled data.

214
00:22:59,892 --> 00:23:07,842
If you do not descriptive data, you haven't usually have to design a distribution that can accommodate a lot of zero associated single cell data.

215
00:23:08,052 --> 00:23:11,862
But as well as accounting for any count of single cell as well.

216
00:23:13,242 --> 00:23:18,432
And I think that's the most important point I want to make, is what would you describe as the data?

217
00:23:18,612 --> 00:23:22,692
What does it want with a data, swift or in general discrete data?

218
00:23:23,232 --> 00:23:26,652
We can improve both theoretically and also show it properly.

219
00:23:27,072 --> 00:23:33,762
Another thing we develop is identifying so we can indeed identify unique human observational data.

220
00:23:34,992 --> 00:23:41,282
That's not sort of the motivation of this project. So what has been done in this area?

221
00:23:41,672 --> 00:23:49,472
But it turns out not much. So colleges going for continuous data has been well studied in the last three years or so.

222
00:23:49,752 --> 00:23:56,732
So now that the paper came out every year providing different conditions under which a continuous kind of map is identifiable,

223
00:23:57,362 --> 00:24:03,152
also basically second class. But for some reason, progress for discrete data is very, very limited.

224
00:24:05,142 --> 00:24:10,902
In fact, before all paper came out, the current model, the general belief at that time,

225
00:24:10,902 --> 00:24:19,182
if all we published in that identifying equivalent class is that industry and audience, a part of this talk is the best anyone can hope for.

226
00:24:19,212 --> 00:24:23,862
You cannot really further narrow down or reduce the size of the recruitment accounts to single,

227
00:24:24,492 --> 00:24:28,151
except for a very few peculiar cases that appear in the literature.

228
00:24:28,152 --> 00:24:35,562
For example, if you're willing to assume that that discrete data somehow is additive, then they can show that theoretically it is identifiable.

229
00:24:35,772 --> 00:24:39,222
But to me it's a condition that is very hard to understand for discrete data.

230
00:24:41,642 --> 00:24:52,652
Well, for some mysterious reason. There's actually an important piece of information that has always been overlooked in the statistical statistics,

231
00:24:52,652 --> 00:24:54,902
and the machinery necessary for the purpose,

232
00:24:54,902 --> 00:25:02,761
of course, discovered is that most of this great data, one of the most many discrete data, ought to not, of course,

233
00:25:02,762 --> 00:25:05,972
include that this good type of data, if you just talking about, for example,

234
00:25:06,542 --> 00:25:14,942
some quantile naturally order and order information contained that this take beyond scattered data,

235
00:25:15,092 --> 00:25:20,912
say Soviet questionnaires, a point which is used a lot in social science and psychology.

236
00:25:21,902 --> 00:25:22,472
For example,

237
00:25:22,502 --> 00:25:36,632
the ensemble is use like a scale to do this other questions you know you have to choose among strongly disagree disagree neutral agree strongly agree.

238
00:25:36,992 --> 00:25:40,382
The answer to this bitwise to this question is obviously ordinal.

239
00:25:42,122 --> 00:25:52,862
So before we publish our paper, the common practice in this area is shape discrete data on discrete time data as if normal.

240
00:25:53,162 --> 00:25:56,642
So ignore, discard or do information altogether.

241
00:25:58,132 --> 00:26:03,412
In this neighborhood. We show again theoretically and agree that whatever information,

242
00:26:03,502 --> 00:26:10,552
you should definitely keep that information because it is absolutely crucial in the discovery of this data.

243
00:26:11,272 --> 00:26:15,172
That's the main take home message I would like you to take.

244
00:26:15,802 --> 00:26:20,812
If you don't get anything else in front. All right, so what is our law?

245
00:26:20,962 --> 00:26:24,292
Our model is simple. You find as embarrassingly simple.

246
00:26:24,682 --> 00:26:29,922
If the one that I'll call the perfect model, I can tell you just with one liners.

247
00:26:30,292 --> 00:26:34,982
So I said that with conditional distribution being online or a question that's, you know,

248
00:26:35,082 --> 00:26:39,442
not familiar with the cause of graphic, a lot of situations and it's really just two equations.

249
00:26:40,252 --> 00:26:46,312
So let's say exchange categorical so that we may have potential different number of categories, but that's not super important.

250
00:26:46,972 --> 00:26:50,212
And for call those that we'll call them directly, second method model,

251
00:26:51,082 --> 00:26:59,982
we basically assume that joint distribution can be centralized into product of conditional distributions of each variable, giving us different causes.

252
00:27:00,382 --> 00:27:04,162
That's not something you assign. It is true for any that.

253
00:27:06,232 --> 00:27:10,682
So actually there's only one thing left to decide which is the right hand side of this equation.

254
00:27:10,702 --> 00:27:15,982
So what are you going to use for conditional distribution for each node on each variable, given its direct calls?

255
00:27:16,762 --> 00:27:22,612
So right now, assuming the graph is not going to learn from the data, well,

256
00:27:22,792 --> 00:27:27,952
we have all of the data most natural choices, perhaps on your question for the condition of distribution.

257
00:27:28,282 --> 00:27:31,462
That's exactly what we did. So we specify the conditional distribution,

258
00:27:31,912 --> 00:27:38,031
humanity's cumulative conditional cumulative function of each node as something function

259
00:27:38,032 --> 00:27:44,632
F and with category specific cutoff gamma intercept over and the regression coefficient.

260
00:27:44,662 --> 00:27:51,202
In this case, it's called access the called effects theta. So just vanilla mush onto those questions.

261
00:27:51,622 --> 00:27:59,902
Okay. So yeah, so that's that's our model. So centralization according to that, then each condition distribution is going to request.

262
00:28:00,802 --> 00:28:05,612
Okay. Again, the real question is not holding itself because it's actually pretty easy.

263
00:28:05,722 --> 00:28:13,252
It's simple model. The real question is, well, is this model really has any advantage over any existing methods for it?

264
00:28:13,252 --> 00:28:18,622
Really not in terms of identified ability. I'm not.

265
00:28:18,902 --> 00:28:21,812
No unmeasured confounders. Suppose we measure all the variables interest.

266
00:28:22,172 --> 00:28:27,931
We can indeed directly show that the proposed ordinance that is identifiable in the sense that we

267
00:28:27,932 --> 00:28:34,072
can uniquely identify the true call it was just based on observational observational data at all.

268
00:28:36,272 --> 00:28:41,702
I'm not ready to give another going to give you proof, but rather give you some intuition as to why.

269
00:28:42,212 --> 00:28:46,262
Cause all of that is identifiable with a very simple example.

270
00:28:47,342 --> 00:28:51,722
So let's say I have two variables X and Y. Let's consider the first.

271
00:28:51,722 --> 00:29:01,032
Call them all over X. Call this one. So as I mentioned briefly at the beginning of this talk, the Rifle Club model tells you how to generate data.

272
00:29:01,692 --> 00:29:08,021
So in this case, I would generate calls first access vice model distribution given by this model probably

273
00:29:08,022 --> 00:29:13,091
table and then conditioned on a clause because they asked us to listen to the call,

274
00:29:13,092 --> 00:29:23,202
so to speak, so I can generate the Y, the variable conditional conditioning on X from this conditional property on the left hand side of this table.

275
00:29:25,102 --> 00:29:31,882
Nothing stops us without. We can always guess, you know, that joy distribution of the favorite is just too honorable.

276
00:29:32,182 --> 00:29:35,772
So I get to join probably table on the right side of the slide.

277
00:29:37,482 --> 00:29:42,222
Of course, nothing stops us from satirizing this joint distribution the other way around.

278
00:29:42,582 --> 00:29:49,212
Always factor in the drug distribution into the model of why and the conditional estimate why so-called anti causal model.

279
00:29:50,652 --> 00:29:57,942
So they're all equivalent, obviously. That will actually give you an idea why in general causal wrath is not identifiable

280
00:29:58,002 --> 00:30:01,662
for categories because they won't impact exact the same joint distribution.

281
00:30:02,362 --> 00:30:06,342
So therefore, if you just have observational data that draw from that joint probably table,

282
00:30:06,402 --> 00:30:10,032
there's no way you can identify which ones true, because they will fit that the equivalent.

283
00:30:11,862 --> 00:30:18,011
All right. But when I haven't told you so far, it is only one call.

284
00:30:18,012 --> 00:30:23,922
The graph is compatible with the model I proposed ordinal graph or model of that model.

285
00:30:24,462 --> 00:30:34,032
That's actually the first model. Now, those number you saw on this list in just the two tables under this very particular characterizations.

286
00:30:34,572 --> 00:30:38,342
Neither one being able to do this map of one minus one one.

287
00:30:39,852 --> 00:30:47,772
If you're trying to find a causal, ordinal causal, that model that generates the numbers in this model,

288
00:30:48,012 --> 00:30:54,192
that's kind this model, you can not find any other such order parameters prime in space.

289
00:30:54,462 --> 00:31:01,902
You can not find any combination of download and data on the last dice such that they they are joint distribution on their condition.

290
00:31:01,902 --> 00:31:05,682
Distribution model distribution matches those numbers. Okay.

291
00:31:05,892 --> 00:31:10,752
So for example, if I do some simple simulations, maybe generate data from this joint distribution,

292
00:31:10,752 --> 00:31:19,481
either with any general data from the model, even with 100,000 samples for this relatively small dimension,

293
00:31:19,482 --> 00:31:28,172
two dimensional problem, if you fit the second model and assume the data is ordinal, you will still have a significant bias compared to the truth.

294
00:31:28,182 --> 00:31:32,712
Even with 100,000. And this bias will never go away.

295
00:31:32,892 --> 00:31:39,042
We never go away even and close to infinite because they do not exist an equivalent model

296
00:31:39,792 --> 00:31:45,082
equivalent in model that had the exact same joint distribution as the data generating models.

297
00:31:45,372 --> 00:31:48,882
Do nothing to show why this is true.

298
00:31:49,442 --> 00:31:54,642
And our theorem basic shows this is generally true for any graph, not just my averages.

299
00:31:55,332 --> 00:31:59,322
But of course we make a very strong assumption that there are no unmeasured opponents.

300
00:31:59,502 --> 00:32:03,162
So that's, first of all, we not be true. Second, it's not checkable in practice.

301
00:32:04,152 --> 00:32:10,902
So let's look at how this matters perform in practice where there's almost certainly a measure component.

302
00:32:13,032 --> 00:32:21,432
So this is a classic example in causal discovery, emotional imitation, sex, single cell flow cytometry data.

303
00:32:22,152 --> 00:32:29,802
It is a data that consists of 11 proteins that have relative small durations with 853 samples.

304
00:32:32,072 --> 00:32:39,662
In fact, everyone knows him as a highly skilled and noisy, so highly, so skilled and noisy that in their original paper, in a science paper,

305
00:32:39,662 --> 00:32:43,382
when they published this paper, when it published this dataset,

306
00:32:43,442 --> 00:32:48,992
the authors actually discrete types the data into three categories low, average and high.

307
00:32:50,012 --> 00:32:56,012
The reason this paper on this particular dataset is so popular in the community is, you know, addition to the beat.

308
00:32:56,642 --> 00:33:01,082
The authors the authors actually philologist they provided a consensus,

309
00:33:01,082 --> 00:33:06,002
causing that all of these proteins accepted accepted by the scientific community at that time.

310
00:33:06,482 --> 00:33:09,932
Okay. So in biology, we say there's no real ground truth.

311
00:33:10,052 --> 00:33:18,212
But so this graph shows is not perfect. But all of us, we are going to use this as approaches so that we can compare different methods on this and.

312
00:33:20,992 --> 00:33:25,101
Because like I said, traditionally, when people look at these good times,

313
00:33:25,102 --> 00:33:30,652
they don't they would just throw away the normally ordinary information and treat data as nominal categorical data.

314
00:33:30,952 --> 00:33:38,332
That's exactly what they did in that science paper. They applied a nominal categorical model with all that information discarded.

315
00:33:38,632 --> 00:33:45,132
And with that approach, they can only identify a very few edges from the consensus networks.

316
00:33:45,412 --> 00:33:52,372
Because they ignored what. That's actually how the consensus network looks like.

317
00:33:52,672 --> 00:34:01,822
Okay. So as in other proteins and when you adjust the edges and this is how our method is doing.

318
00:34:02,512 --> 00:34:07,132
So the glue in the case, the true part of this. So we correctly identified six of them.

319
00:34:08,262 --> 00:34:11,752
So that's are the false negative? Yes, we do.

320
00:34:11,782 --> 00:34:22,002
We tend to miss a lot of edges. So they are 12 of them. And there are also two edges we identified, but we get a direction of everything.

321
00:34:22,012 --> 00:34:30,712
While this does seems to be a very powerful process, which I kind of agree, but our specificity element is actually very good.

322
00:34:30,982 --> 00:34:38,392
First of all, it didn't detect any spirits. We didn't detect any existing edges and all of the edges that we detect.

323
00:34:38,662 --> 00:34:43,102
Six out of eight had the correct directions.

324
00:34:43,702 --> 00:34:49,792
So we sort of argue that for this type of analysis of generating causal hypotheses, you know,

325
00:34:49,792 --> 00:34:55,402
exposure and fashion specificity is perhaps more important than a sensitivity because if false,

326
00:34:55,402 --> 00:34:58,372
positive or a lot of false positives can lead to, you know,

327
00:34:58,432 --> 00:35:05,662
wasting money and time because biologists potentially use usage of this tool is for biologists to run this out on their observational data.

328
00:35:05,842 --> 00:35:10,102
Then later they will follow up with, you know, experiments to verify certain things.

329
00:35:10,492 --> 00:35:19,372
So if your method generates a lot of false positives, then of course it can be a huge waste of time and money, you know, from the biologists.

330
00:35:19,582 --> 00:35:23,692
And eventually, ultimately, they will lose confidence in this type of algorithm.

331
00:35:24,052 --> 00:35:30,502
So to me, for this type of exposure analysis, it is more important to have a high specificity compared to sensitivity.

332
00:35:30,742 --> 00:35:36,402
So at least in this particular and I want to say the specificity is good.

333
00:35:38,832 --> 00:35:50,322
Well, we also actually, we do have to reverse directions that we identify, which is sort of indicated by the orange curve.

334
00:35:51,852 --> 00:36:00,762
One explanation of this reverse directions. Even though we prove that all method is identifiable, the reason why they may be reversed?

335
00:36:01,182 --> 00:36:06,552
Well, part of the reason maybe there are actually feedback loops between these two pathogenes,

336
00:36:07,182 --> 00:36:10,302
which had not been discovered at the time of publication of this dataset.

337
00:36:10,692 --> 00:36:16,242
In fact, if you look at the literature carefully enough, there are more recent evidence showing that, for example,

338
00:36:16,242 --> 00:36:25,932
PKC an adjustment in the actually there is a feedback loop between the two and so is the causal relationship between the PRC, Gamma and the PIV two.

339
00:36:26,332 --> 00:36:33,072
Okay, so this may be one explanation of why our algorithm detects or could get a wrong direction from these two passages.

340
00:36:35,862 --> 00:36:41,622
We also apply many other methods. In fact, we apply 15 other state of the art machinery algorithm.

341
00:36:41,892 --> 00:36:51,342
Some of them work on the discrete data. Someone can go to the raw continuous data to apply 15 other competing methods, and we show the top of eight.

342
00:36:51,762 --> 00:36:57,712
And by looking at a structure how many this is basically how many edges you install reversed method in

343
00:36:57,762 --> 00:37:04,872
which we can see the all the discovery is the task compared to other methods for this particular test.

344
00:37:08,002 --> 00:37:12,172
Any questions so far? Okay. Go back to your equation page.

345
00:37:15,652 --> 00:37:21,412
So here. So suppose we wanted to. And you have a plan.

346
00:37:21,802 --> 00:37:29,262
Why would. Because I'm all right. Suppose white contacts on your left hand side of the equation is joint distribution of action.

347
00:37:29,272 --> 00:37:32,542
One of the reasons I would be conditional.

348
00:37:32,662 --> 00:37:35,691
So the divisional will be conditional times marginal.

349
00:37:35,692 --> 00:37:39,472
So why given x times x the left hand side the right.

350
00:37:40,902 --> 00:37:45,222
So when I'm asking the left hand side is joining is going on right now that both sides join this region.

351
00:37:45,432 --> 00:37:52,782
But on the right hand side of your denounce, right in the margin is usually well where the parents when the direct is empty.

352
00:37:53,562 --> 00:38:00,442
So this condition description degenerates to marginal distribution. So if that's another kind of direct causes, then the conditions are visual.

353
00:38:00,702 --> 00:38:04,182
X given empty set is just a modern distribution of acts. Okay.

354
00:38:05,022 --> 00:38:10,632
That is what happened. Yeah, that's my definition. Yeah. That the general systems market distribution network.

355
00:38:11,142 --> 00:38:16,422
So it's not always defines a profitable distribution because of this recursive relationships.

356
00:38:17,052 --> 00:38:21,912
Okay. So for the next page, you have that very nice example maybe in the following major.

357
00:38:21,922 --> 00:38:29,262
Yeah. There is not a test out for categorical data, you know, continuous that I have the same issue here, right.

358
00:38:29,412 --> 00:38:35,922
Yeah. No, no Kaushik make this in Gaussian. You will have exact the same actually, you know second project in history with one example.

359
00:38:36,162 --> 00:38:38,861
So if you do Gaussian for continuous you will have exact the same problem.

360
00:38:38,862 --> 00:38:42,222
You can factor as you know directions you will have equal and likelihood and

361
00:38:42,222 --> 00:38:46,332
there's no way you can identify which one is a truth given observational data.

362
00:38:46,632 --> 00:38:52,542
But as soon as you deviate away from college India, you have the same similar properties and you can affect the.

363
00:38:53,202 --> 00:38:56,192
Therefore, you can identify the true right by the photo.

364
00:38:56,592 --> 00:39:05,412
But you still need to assume that a true graph generator based on the model assumption and the you know, the way the model is correctly specified.

365
00:39:05,592 --> 00:39:08,762
Right. But if the model means easier, find it.

366
00:39:08,772 --> 00:39:14,802
So. Yeah. So still you are able to actually identify which one more close are closer to that.

367
00:39:15,252 --> 00:39:19,722
True. Maybe. Yeah. We haven't had any results. Yeah. Maybe the call says yes.

368
00:39:19,752 --> 00:39:23,712
Maybe it is a model that is closest to mutual model. It's totally possible.

369
00:39:23,892 --> 00:39:28,032
We have studied that can address other questions.

370
00:39:29,042 --> 00:39:36,302
So I have a simple question here. You give me an example for this three level categorical variable.

371
00:39:36,782 --> 00:39:42,302
This is true for the two level categories. They beg the question. So what I did I put here is there's a condition.

372
00:39:42,572 --> 00:39:46,562
So on our has to be larger than two. It goes for binary order.

373
00:39:46,862 --> 00:39:53,162
That's a lot of information. Because I saw I can't find examples that somehow, you know exactly what you.

374
00:39:53,602 --> 00:40:00,092
Yeah. So l has. Yeah. I should put on this Syrian what l has to be not equal to because the binary there, it doesn't matter where.

375
00:40:00,652 --> 00:40:04,322
Right, exactly. Other questions.

376
00:40:05,072 --> 00:40:10,172
Yeah. So maybe this is not the right way to think about it, but, like, I see this and it seems like it.

377
00:40:11,352 --> 00:40:15,732
Like you just have to have enough degrees of freedom, like if you had more parameters.

378
00:40:15,912 --> 00:40:20,412
Yeah, that's very good. That's a one way to think about this called a model. So no power nationally.

379
00:40:20,562 --> 00:40:25,772
So if if you don't put any assumption which in this case equivalent to categorical or nominal categorical variable,

380
00:40:25,782 --> 00:40:31,302
you just specify all the elements in this table. It's is kind of the most comprehensive model for this column.

381
00:40:31,872 --> 00:40:36,912
So in that case, it's not on the table precisely because what you said so you have multiple is freedom.

382
00:40:37,632 --> 00:40:43,211
In some sense all the quotes can be a result of they are identified by the results or relies on this

383
00:40:43,212 --> 00:40:50,802
possible are restricted within freedom so that you cannot satirize not simply people's actions.

384
00:40:51,012 --> 00:40:55,662
So you'll have exactly right. I think my intuition is really not me.

385
00:40:55,692 --> 00:41:01,182
The narrative is the in in case you have linear dependance, which is really symmetric,

386
00:41:01,572 --> 00:41:07,962
but you have this way that you basically need to high order like dependance nonlinear dependance.

387
00:41:08,322 --> 00:41:15,512
That becomes the trick to really help you to identify different sort of the marginal condition

388
00:41:15,582 --> 00:41:20,112
of distribution one way or another that's really done in in order to place the like.

389
00:41:21,162 --> 00:41:24,642
So in your way, you explore the asymmetry between the two? Exactly.

390
00:41:26,442 --> 00:41:29,532
You didn't tell us what your algorithm was. Is that maximum likelihood?

391
00:41:29,892 --> 00:41:33,761
Yeah. So, yeah, I was too embarrassed to stand out even now.

392
00:41:33,762 --> 00:41:40,062
There was like a simple, greedy search algorithm. So I have a basic criteria, the 19% capacity,

393
00:41:41,202 --> 00:41:46,062
and then we just search in the browser space in a sense that every time, every stack, I just order one edge.

394
00:41:46,362 --> 00:41:57,122
He then adds delete, add and remove and. It was a very simple algorithm has no guarantee of conversion to a global search.

395
00:41:57,552 --> 00:42:02,182
But yeah, we're actually now extending this to a sort of basic methods.

396
00:42:02,392 --> 00:42:07,912
We actually try to embed this method in a mixture model so that we don't have to decide where to democratize data.

397
00:42:08,062 --> 00:42:13,192
And I've had this with an MSI sort of have an influence on this paper.

398
00:42:13,522 --> 00:42:16,582
We just did a simple point estimate and it's really such.

399
00:42:19,852 --> 00:42:27,802
We're going to revert to simple. So you said you have every possible wrath in your greedy sense?

400
00:42:27,802 --> 00:42:28,011
Yeah,

401
00:42:28,012 --> 00:42:36,262
we started down some graph and I started adding edges or delete edge or regress edge until the base information criteria cannot be further decreased.

402
00:42:37,882 --> 00:42:45,782
It's guaranteed to convert your local mode, but not necessarily. And.

403
00:42:52,372 --> 00:42:59,002
All right? Yeah. So. All right, so something funny or some enticement?

404
00:42:59,142 --> 00:43:03,802
Sorry. Some halftime commercial. So if you have to.

405
00:43:03,982 --> 00:43:09,172
If you like what you've heard at this concert to join our group funded by.

406
00:43:10,132 --> 00:43:14,212
So we have a project on single cell data, cosmic discovery.

407
00:43:14,412 --> 00:43:17,632
So if you really like what is considered, join our group.

408
00:43:18,202 --> 00:43:22,762
And a second if you kind of like what I said, but you have no idea what we're talking about.

409
00:43:23,392 --> 00:43:24,802
It's my country, my fault.

410
00:43:25,072 --> 00:43:33,982
But in this case, consent to join our conference, which is going to be held in titanium May 15 to May 19 and the organizer of the conference.

411
00:43:34,342 --> 00:43:40,262
We are very glad to be able to invite a top notch colleague,

412
00:43:40,262 --> 00:43:46,552
discovery researchers Dr. Conjunct from the Department of Philosophy at CMU kind of cutting it down.

413
00:43:46,912 --> 00:43:50,272
So we have some limited funds to support students trouble.

414
00:43:50,392 --> 00:43:54,202
Not much because it's NSF funded conference. They don't give us much money.

415
00:43:54,242 --> 00:43:58,112
But if you are interested, please consider to join us to advance.

416
00:43:58,402 --> 00:44:06,802
All right. Let's go back to that is I think I only have 15 minutes lost.

417
00:44:07,042 --> 00:44:12,862
I may very quickly go through this project as, again, this created this good this quick data.

418
00:44:13,132 --> 00:44:19,371
But the motivation was in the previous project, we discretely stayed on for a very obvious reason.

419
00:44:19,372 --> 00:44:23,152
We lose information. So what if we model that they don't distribution directly?

420
00:44:23,152 --> 00:44:31,102
Can we do identify as kind of double divisions and we'll propose something called zero inflated, generalized hypogeum that or zigzag for sure.

421
00:44:33,142 --> 00:44:40,372
Yeah. So I'm going to really run through this size. So we're going to assume the same distribution or at least on a high level Western factorization.

422
00:44:40,412 --> 00:44:51,052
With that, here was an SGA our count. So there is interest potentially how many zeros because they are inflation in single cell data consumption is.

423
00:44:51,262 --> 00:44:56,232
Yeah, very simple model we just going to have to specify this distributions because since

424
00:44:56,312 --> 00:45:01,882
doing for the count why not just using a zero count regression for theoretical reason?

425
00:45:01,972 --> 00:45:08,002
We make it a very general to specify that conditional distribution as using probability generator,

426
00:45:08,092 --> 00:45:17,392
both of which just is if we can find a generalized have a dimensional distribution where on this equation PI is the very inflation probability,

427
00:45:17,392 --> 00:45:20,302
which depends on the cost. That's variable date.

428
00:45:20,722 --> 00:45:27,922
And the last bit, you know, this g function is a figure, the party generating function for generalized type geometric distribution.

429
00:45:28,072 --> 00:45:36,562
So conceptually, again, the model is extremely simple and the reason we use jars of geometric distribution is because it's a very general class model,

430
00:45:36,712 --> 00:45:44,562
includes many well known counter solutions such as binomial Poisson negative binomial type about some geometric geometry and so on.

431
00:45:44,602 --> 00:45:47,482
So for many come distribution for the family.

432
00:45:47,782 --> 00:45:57,142
So we really, we use this really just for theoretical purposes so that we can and what is true, all this distribution is actually inclusive inflation.

433
00:46:00,432 --> 00:46:05,382
Basically the same side of the fold. Assuming no unmeasured confounder, the proposed model is identifiable.

434
00:46:05,832 --> 00:46:09,012
Again, not giving you the proof, but intuition.

435
00:46:09,582 --> 00:46:13,512
So again, just two variable x and y. You can start on the left hand side.

436
00:46:13,542 --> 00:46:18,012
Suppose you somehow use Gaussian, you're assuming the marginal condition of Gaussian.

437
00:46:18,122 --> 00:46:24,542
Therefore the binary of joint is Gaussian. You can summarize it is not the other way round by just normal theory accounts.

438
00:46:24,972 --> 00:46:30,482
So because all Gaussian, if you see the data to that observational data, do have exactly the same effect.

439
00:46:30,702 --> 00:46:37,492
So you can only break frequency, which by the hand if you just count as I use places like example.

440
00:46:37,512 --> 00:46:38,382
But in general, it's true.

441
00:46:38,802 --> 00:46:44,952
So if you start with puts all marginal some regression for conditional you get some distribution that I don't know the name of this now,

442
00:46:45,012 --> 00:46:48,402
but it doesn't really matter in this case. Again, in fact, what is it the other way around?

443
00:46:48,822 --> 00:46:52,601
I don't know the name of the distribution, but I'm pretty sure it's not possible.

444
00:46:52,602 --> 00:46:56,652
For example, if you look at distribution, why it's going to be a natural parasite, which is what we discussed.

445
00:46:56,892 --> 00:47:04,712
So it cannot be precise anymore. So for that reason, if you insist that your model is coming from the sum, and if it is to model that data,

446
00:47:04,992 --> 00:47:12,652
this model to better generate follow up on that, then maybe it's obvious that the first model will have a higher likelihood of second one.

447
00:47:13,182 --> 00:47:19,122
I'll you to show that it doesn't just apply to liposome and apply to any practically

448
00:47:19,122 --> 00:47:23,382
any distribution you can think of that falls into this hyper generalized,

449
00:47:23,382 --> 00:47:30,702
hyper geometrical distribution fabric. You're going to do that. That's what the theoretical contribution of this method.

450
00:47:30,912 --> 00:47:36,312
Overall, you will see that the structure of this project is very, very similar to the project on the first part.

451
00:47:36,912 --> 00:47:38,352
So let's look at some real data.

452
00:47:38,562 --> 00:47:44,442
So these are data that are given to me by our collaborator, which is a real single cell data that collected from a mouse experiment.

453
00:47:44,742 --> 00:47:50,252
And again, we tried to compare that with other methods, but a difficulty is in quality control.

454
00:47:50,302 --> 00:47:57,822
You typically don't have to choose. So we actually go time to spend a little time to search the literature.

455
00:47:58,002 --> 00:48:02,652
Trying to find is any way to have some sort of ground truth that we can compare with.

456
00:48:03,312 --> 00:48:14,621
We end up using this trust in a basis which contains 479 pairs of transcription factors and that target and that we use the logical ground truth.

457
00:48:14,622 --> 00:48:22,362
That transcription factor is a causal property. Again, there's no real ground truth in biology, but that's the closest we can get.

458
00:48:24,492 --> 00:48:27,672
Just very, very, very, very quickly out of this,

459
00:48:27,672 --> 00:48:34,902
479 pairs of transmission back and tongue alone measures correctly identified 304 calls and relationships.

460
00:48:35,172 --> 00:48:42,732
It's obviously not by chance, because if you pivot ten to the minus nine and we connect again many competing methods,

461
00:48:42,882 --> 00:48:50,092
especially for cultivator, most of those that model does not take into account zero inflation.

462
00:48:50,172 --> 00:48:53,132
So for example, there are methods based on negative binomial for some.

463
00:48:53,502 --> 00:48:59,442
So anyway, the best alternative methods can actually only identify 198, which is a less than half the pencil.

464
00:48:59,452 --> 00:49:05,861
It's actually worse than random gas. And part of the reason maybe they don't take into account zero inflation.

465
00:49:05,862 --> 00:49:10,072
But the data is hiding in plain. Sorry.

466
00:49:10,072 --> 00:49:17,422
I'm sort of Russian in between just this part because it's kind of similar to the first part, although it was a different scene at the last part,

467
00:49:17,632 --> 00:49:23,512
which I would say quite, quite different from the first two parts where we tried to get rid of these two assumptions.

468
00:49:23,542 --> 00:49:28,972
One is a second city because the network has feedback loops, so how can we get rid of that consumption?

469
00:49:29,302 --> 00:49:32,572
Second, we also tried to get rid of a measurement on the assumptions.

470
00:49:32,782 --> 00:49:36,562
So we allow the components. But as you will see, we have to make assumptions on this.

471
00:49:38,212 --> 00:49:46,492
All right. So the project was originally motivated by cancer genomic data, where we know that cancer genomic did not a bit genius.

472
00:49:47,872 --> 00:49:51,382
We start off this project as a simple extension of my earlier work.

473
00:49:51,862 --> 00:49:58,042
I tried to, you know, estimate a causal graphical model jointly with some sort of latent state estimation that

474
00:49:58,042 --> 00:50:04,342
regions that may be representing disease progression or in general just some native factors.

475
00:50:05,182 --> 00:50:15,472
But it turns out data heterogeneity is really a nothing disguise for the purpose of discovering why most of the methods

476
00:50:15,622 --> 00:50:21,412
in the literature they will fail miserably when they're not there just because they have seen observational I.D.,

477
00:50:21,712 --> 00:50:26,212
or at least partially an idea which may lead to misleading results.

478
00:50:27,172 --> 00:50:36,982
Our metaphors thrive this way, and actually precisely to heterogeneity that allow us to identify a true cultivars, even in the present cycles.

479
00:50:39,492 --> 00:50:46,042
All right. So I'm going to use a slightly different tool, but actually that's generous to the tool that we used in the past.

480
00:50:46,192 --> 00:50:50,212
If you make any cyclic assumptions. So the tool we use is called structural equation model.

481
00:50:51,682 --> 00:50:58,132
In this case, we're going to assume all the data are continuous. You're giving the object space I'm going to use.

482
00:50:58,792 --> 00:51:06,351
Suppose I said a vanilla and start to create a model where we assume that x equal to some intercept possibly times that notice to appear

483
00:51:06,352 --> 00:51:13,402
on both sides of this equation and cross noise about the point which is normally distributed with some covariance matrix signaling.

484
00:51:14,032 --> 00:51:22,012
A key part of interest here is that the matrix which you don't represent, the direct causal effects from November, are not capable to know.

485
00:51:22,672 --> 00:51:28,402
Okay. We only make one assumption on b that is the diagonal elements of B has to be zero.

486
00:51:28,582 --> 00:51:34,972
So excluding self self-rule is not viable in this model, but we do not make the following assumption.

487
00:51:35,752 --> 00:51:40,602
If you do make this to the following two assumptions just structurally the model will degenerate to about.

488
00:51:41,692 --> 00:51:47,332
So the first assumption is be as low or triangular after some row in the column, simultaneous annotation.

489
00:51:47,512 --> 00:51:52,312
So that's corresponding to the A6 exemption. The second assumption is sigma is diagonal.

490
00:51:52,312 --> 00:51:57,462
So now uncorrelated or independent in this case. So if you make this assumption, that's equivalent to making assumptions,

491
00:51:57,472 --> 00:52:02,961
no measurements on this for this talk, for this model talk, we did not make these two assumptions.

492
00:52:02,962 --> 00:52:09,142
So that does not degenerate your case, but it does show some symmetry of dimensions.

493
00:52:09,502 --> 00:52:14,571
No, that's not so much to offer, but can have different conditions, can be different.

494
00:52:14,572 --> 00:52:20,452
You can even have different zero patterns of very general, just a zero diagonal assumption.

495
00:52:22,492 --> 00:52:26,692
All right. So the longest limitation, these are existing methods. It's not something that I invented.

496
00:52:27,502 --> 00:52:32,482
But using this methods to cancel genomic data has limitations.

497
00:52:32,502 --> 00:52:37,662
One is what? Typically, when people use this method, their assumed observation is one.

498
00:52:37,672 --> 00:52:44,422
Assume that assuming this idea rests on Moises for Case Genomics, this may not hold a second,

499
00:52:44,422 --> 00:52:48,441
which is more important for the purpose of this talk as it is not identifiable,

500
00:52:48,442 --> 00:52:53,982
even in the simple case, meaning that we assume there's a significant graph and there's no measurement of precisely the.

501
00:52:56,902 --> 00:53:04,131
All right. So we just make one simple tweak, one simple modification of the model by introducing a native variable, which is happening.

502
00:53:04,132 --> 00:53:09,922
That's a lie, which is observation specific. And this means a variable can have know physical interpretation.

503
00:53:10,132 --> 00:53:17,781
For example, it may be the case to maintain the disease progression, but in, you know, be it just a little not a little tweak it,

504
00:53:17,782 --> 00:53:22,192
that small trick that we make to the model that we bequeathing this little variable to the intercept,

505
00:53:22,342 --> 00:53:25,492
which is not really important by most important, is the facts.

506
00:53:25,492 --> 00:53:31,332
Now we assume that matrix is a matrix function and the function of the data variables is survival.

507
00:53:32,272 --> 00:53:37,822
We're still going to assume the error noises idea, but now the vision now becomes independent.

508
00:53:38,122 --> 00:53:41,841
Do independent, but they are not identically distributed more because they are closer in size.

509
00:53:41,842 --> 00:53:44,772
Actually depends on the factors.

510
00:53:45,802 --> 00:53:56,182
So if you can, this patient is called, if this becomes so called patient specific because index by patients is very decent variable.

511
00:53:56,192 --> 00:54:04,341
These eyes are just once again, we do not make this following assumption that this guy has to do what triangular or sigma has to be diagonal.

512
00:54:04,342 --> 00:54:05,752
So we do not make these approaches.

513
00:54:07,792 --> 00:54:14,362
As I mentioned earlier in the very beginning, a talk, the assumption we have to make, we just have to be transparent.

514
00:54:14,572 --> 00:54:19,252
So even if those are implicit, so in this case, we do make important implicit assumptions,

515
00:54:19,552 --> 00:54:23,632
which I believe that of all, because we're going to assume there's there may be a confounder.

516
00:54:24,052 --> 00:54:25,342
But you have to make an assumption about it.

517
00:54:25,792 --> 00:54:33,732
So implicitly, we assume the confounding factors are homogeneous because we actually didn't put Z inside a signal.

518
00:54:33,742 --> 00:54:38,992
When a signal is the component that's so and may or may not be true in certain applications,

519
00:54:39,352 --> 00:54:46,162
but it could be true if you believe that that's a measure of although in this case, maybe even some genetic effects from a viewing level.

520
00:54:46,432 --> 00:54:48,622
So if you're comfortable with the following assumption that in fact,

521
00:54:48,622 --> 00:54:54,202
all this unmeasured genetic material expression may not change as disease progresses,

522
00:54:54,442 --> 00:55:03,492
and if comfortable, that that's a function of homogeneous components as well, it should start.

523
00:55:03,552 --> 00:55:04,672
Yeah, I have to remember,

524
00:55:06,472 --> 00:55:14,062
let me skip the intuition so I can talk to you offline what is important in my decision table and then just show you one example.

525
00:55:14,512 --> 00:55:20,602
Again, real data on patients who have breast cancer data sat with over a thousand heterogeneous tissues,

526
00:55:20,602 --> 00:55:25,822
some anomalous amount in tissues, and we focus on a set of genes in this important pathway.

527
00:55:26,032 --> 00:55:29,452
I've just showed two examples. One is the little variable as conditions.

528
00:55:29,752 --> 00:55:32,181
So we reduce the dimension just for visualization.

529
00:55:32,182 --> 00:55:38,692
We reduce the data into two dimension using PCR and we control the task with these little variables.

530
00:55:38,992 --> 00:55:44,292
And notice that we know some addition novel somewhat of tumor, but we didn't use that information once more.

531
00:55:44,722 --> 00:55:49,072
So it looks like all the radicals, which means they have similar written variable,

532
00:55:49,342 --> 00:55:57,862
they tend to be sparse at the moment which sort of validate that all finding we can very well estimation sense and I say very quickly that's

533
00:55:57,862 --> 00:56:07,482
the first part is solved maybe on the first line of all is the truth somewhat true and then this estimate now looks with some truth out there.

534
00:56:07,582 --> 00:56:13,222
So some false positive and some false things. But I would say overall problems very, very well.

535
00:56:13,432 --> 00:56:15,332
For example, the test cycles,

536
00:56:15,652 --> 00:56:24,302
because we test all the pathways and other methods out of just eight possibly we tested almost a second in the first pathway.

537
00:56:24,322 --> 00:56:30,711
But what the rest is the best according to the mathematics correlation coefficient at all time.

538
00:56:30,712 --> 00:56:35,422
So I'll go back the origin of the students and the first two online.

539
00:56:35,422 --> 00:56:40,341
My students work on the projects and also my collaborators by mnemonic and cogent and also

540
00:56:40,342 --> 00:56:46,132
scientific library who provide our single cell data and also make my front pages in a supplement.

541
00:56:46,522 --> 00:56:55,942
And with that, thank you very much. A little bit against of an issue here.

542
00:56:55,942 --> 00:56:58,972
There is a class that comes out in 2 minutes. I'm sorry.

543
00:56:59,022 --> 00:57:02,722
So probably going to have to hold classes, I think.

544
00:57:03,712 --> 00:57:09,162
But you should feel free to reach out to me.

545
00:57:09,202 --> 00:57:14,002
You'll be around for a while. So speak.

546
00:57:15,682 --> 00:57:29,682
And that is because this is a huge function of relationship.

547
00:57:32,062 --> 00:57:33,932
You have to do a lot.

