1
00:00:00,780 --> 00:00:06,609
Literally they just like do it. Joe There is that.

2
00:00:06,610 --> 00:00:19,980
Just like you and me.

3
00:00:20,470 --> 00:00:29,370
Okay. Okay. Okay.

4
00:00:29,760 --> 00:00:33,060
It's amazing. And I like it.

5
00:00:34,830 --> 00:00:53,730
Yeah. But, you know, we're just trying to prove that this was a goddamn thing, this thing.

6
00:00:55,260 --> 00:01:04,130
I think I saw that with my first wife.

7
00:01:09,270 --> 00:01:30,060
Did you? Hey, I think you should try to look it up like their skill set is.

8
00:01:30,120 --> 00:01:34,080
It seems like it's like a couple things.

9
00:01:34,410 --> 00:01:38,540
Yeah, that's. That's what's that word interrupt.

10
00:01:38,670 --> 00:01:45,410
But I think I know what it was like to do.

11
00:01:46,260 --> 00:02:08,780
And, I mean, I tried to like, you know, I can't, you know, and causing a lot of people in the class to do that.

12
00:02:08,970 --> 00:02:12,030
And I can't think of anything. Right.

13
00:02:12,210 --> 00:02:16,730
And so that's the first thing I was going on. This is like on both sides.

14
00:02:16,880 --> 00:02:21,050
You can still try and see if they can switch it. And I was like, Oh, you did?

15
00:02:21,730 --> 00:02:37,170
Yeah, I think I was working the transcript process, which is like, if if it was, I don't really get it, but, you know, you feel like that.

16
00:02:37,200 --> 00:02:47,580
And I was like, I just don't like I like I won't be the reason that I thought I was going to like it was like 35% of the time.

17
00:02:49,230 --> 00:02:53,430
Yeah. Yeah. So, yeah, that's that's good. That's good.

18
00:02:54,450 --> 00:02:54,810
Oh, yeah.

19
00:02:54,930 --> 00:03:05,700
So like under that, like, everyone could at least get a good ask, but it's like they're not I don't really I was like, that's like pass away pass.

20
00:03:06,360 --> 00:03:10,690
And you're like, not because you can't feel how you caught me.

21
00:03:12,510 --> 00:03:28,720
And I was like, I would not want to be like this, but I know, I know how you feel about that.

22
00:03:28,740 --> 00:03:35,530
You know, I've never seen anything like this thing.

23
00:03:35,550 --> 00:03:42,120
And he was like, I don't like I think I don't like him.

24
00:03:42,540 --> 00:03:47,009
Let's find out what it is.

25
00:03:47,010 --> 00:03:49,910
He is so upset about that.

26
00:03:49,930 --> 00:04:04,560
Yeah, I feel like every other class I was like, let's basically like, yeah, we were just like I was like, All right, I hope he's okay.

27
00:04:04,800 --> 00:04:11,760
It's just like, I was like, really?

28
00:04:11,760 --> 00:04:26,730
I'm just saying things like that. But I was like, I don't know if you guys are treated like that, know, feel so nice.

29
00:04:28,560 --> 00:04:35,820
And then you're like, you can smell like you like.

30
00:04:36,510 --> 00:04:54,660
I guess I'm just like, you know, I don't really like you'll be like, are you going to like,

31
00:04:54,870 --> 00:04:58,530
I'm kidding with your friends, you are just like, so that's the last.

32
00:05:02,800 --> 00:05:25,540
I think as much as we like to know how the social policies are by basically to education or anything like that,

33
00:05:26,620 --> 00:05:45,609
and finally realizing that everything but one of those classes of so, you know, just nice goals with the right things and you not going to help here.

34
00:05:45,610 --> 00:05:50,860
But I just flew in last night and so I want to say like, welcome back. I'm the one who was kicked back from that.

35
00:05:53,140 --> 00:06:00,070
I this is weird timing because we're today we're talking about shared decision making.

36
00:06:00,520 --> 00:06:06,310
And I just spent the last five days at a conference immersed in research on shared decision making.

37
00:06:06,760 --> 00:06:19,500
So I'm all really salient to me. But before we dove into that, I want to make sure we pause and check back in about the upcoming assignment.

38
00:06:20,640 --> 00:06:27,900
You've now got, as of today, several additional decision made examples, different types of tables.

39
00:06:28,560 --> 00:06:31,860
Is there anything that's come up as you were doing the reading,

40
00:06:31,860 --> 00:06:37,500
as you've been starting to think about what types of topics you want to do that I can try and clarify and I'd like to do this.

41
00:06:38,250 --> 00:06:45,660
I mean, obviously one on one conversations whenever you want, but I also want to do this more generally because my experience is usually kind of the

42
00:06:45,670 --> 00:06:49,180
questions that come up at the stage are things that everybody's trying to wrestle with.

43
00:06:49,200 --> 00:06:53,340
So do you have any thoughts?

44
00:06:53,370 --> 00:06:58,529
Yeah. I found the clinical trial paper.

45
00:06:58,530 --> 00:07:00,599
Really? To to my topic. My topic.

46
00:07:00,600 --> 00:07:09,510
It's about ventricular assist, a device that comes to assist heart failure patients to pass the blood from the ventricle to the whole body.

47
00:07:09,740 --> 00:07:19,080
So that's the basic idea. So I found like a Lego child about this to compare patients with with the device and without the device.

48
00:07:19,080 --> 00:07:24,840
But the sample is pretty small. Is it okay to use the numbers that provide it?

49
00:07:25,050 --> 00:07:30,240
So this is a great case. Yes.

50
00:07:30,600 --> 00:07:34,650
Notice that I'm giving you a license to make up numbers if you need to.

51
00:07:35,220 --> 00:07:40,560
That also includes rounding, adjusting what you find in a clinical trial,

52
00:07:40,560 --> 00:07:47,160
etc. like I'm doing for the purpose of this assignment is that you practice communicating the information,

53
00:07:47,160 --> 00:07:54,000
not that you necessarily had precisely the correct medical information at the current moment, like we're all going to be doing the best we can.

54
00:07:54,600 --> 00:07:59,280
So what would you do? What should you be doing in this case? And I'm going to use here as a as a case study.

55
00:08:00,240 --> 00:08:04,080
Pay attention to what the outcomes are that they measured in that trial.

56
00:08:04,620 --> 00:08:08,160
So what were the primary outcomes of perception of benefit?

57
00:08:08,400 --> 00:08:12,470
What were the primary risks and complications that they were documented?

58
00:08:12,900 --> 00:08:22,440
That information is super important. Pay attention to, at least in the ballpark, what the size of the risks are,

59
00:08:22,440 --> 00:08:26,460
both in the absolute sense and the size of the differential between the trucks.

60
00:08:29,170 --> 00:08:31,660
Well, I mean, like, I guess I'm not gonna be checking this,

61
00:08:32,050 --> 00:08:38,050
but both of those pieces of information are super important for designing what it is that you want to communicate.

62
00:08:39,070 --> 00:08:42,400
So in that kind of a context, in some sense,

63
00:08:42,550 --> 00:08:47,460
what I want you to pay more attention to is the design of the study and the specific numbers that got out.

64
00:08:50,380 --> 00:08:57,250
And a lot of you are going to run into that, you know, run into situations in which you'll find a study here or a study there.

65
00:08:57,250 --> 00:09:02,020
And maybe they don't measure the same things. You get to make your own judgments about what do you think are the critical things.

66
00:09:02,470 --> 00:09:07,450
Yeah, it's maybe overly specific, but like when you're presenting your numbers,

67
00:09:07,510 --> 00:09:15,550
should you just present it as a single number, even if it was like an error range, like plus or minus a value?

68
00:09:15,700 --> 00:09:21,760
Would that be too confusing? That is a question that every developer has wrestled with.

69
00:09:24,910 --> 00:09:37,120
A short answer is your patience with that variability would be critical for your decision making for complicating things.

70
00:09:38,570 --> 00:09:46,190
There's definitely time for from a scientific standpoint we have variability, but honestly, no patient is really going to care.

71
00:09:46,190 --> 00:09:50,660
Like is it 12% or is it 14% or your chance of having a headache?

72
00:09:50,680 --> 00:09:54,640
Like, I'm not sure that difference is probably going matter that substantially.

73
00:09:55,460 --> 00:10:07,760
Another area, there are other circumstances under which either the range of variability is really wide or the outcomes are so substantial

74
00:10:07,760 --> 00:10:14,210
that you really need to pay attention to really small differences and then maybe you have to start to engage with that.

75
00:10:14,810 --> 00:10:20,090
But you're raising the right questions in the sense that it's going to make things more complicated for people and.

76
00:10:21,540 --> 00:10:26,130
Doing that for ten dimensions can be super complicated.

77
00:10:26,160 --> 00:10:29,680
Doing that for one dimension is not so exciting.

78
00:10:30,660 --> 00:10:34,260
This is part of the point of this assignment is to have you wrestle with these questions.

79
00:10:35,460 --> 00:10:41,580
I mean, we've seen already that volume of information makes it hard.

80
00:10:43,410 --> 00:10:46,490
But you're right, there are times in which uncertainty is really a substantial issue.

81
00:10:46,500 --> 00:10:57,059
Like if we're talking about an experimental treatment where the the estimates of survival benefit vary between 20 and 50 percentage points,

82
00:10:57,060 --> 00:10:58,950
like, yeah, that uncertainty is kind of important.

83
00:10:59,850 --> 00:11:05,880
On the other hand, there are definitely other situations in which you and I have done this for real world trials.

84
00:11:06,360 --> 00:11:10,620
We have looked at that variability and said, You know what? We're just going to take our best shot.

85
00:11:11,670 --> 00:11:18,750
This is all relatively approximate anyway once you apply it from the clinical trial to an actual person or patient in the field.

86
00:11:19,020 --> 00:11:24,149
I mean, we're hand-waving at every step of this, so we do the best we can.

87
00:11:24,150 --> 00:11:31,200
And when you design something where you hope it will give people the right balance of sort of possibility,

88
00:11:31,200 --> 00:11:36,839
knowledge and probability knowledge to help them make a good decision, and that we're going to come back to that today,

89
00:11:36,840 --> 00:11:45,149
too, because the question in the ethics case with this really, really is that same kind of a thing,

90
00:11:45,150 --> 00:11:50,280
like how much precision is actually necessary for somebody to really get what they need to make a good decision?

91
00:11:52,480 --> 00:12:04,860
Other things about the assignment as we look forward. So just a few reminders.

92
00:12:06,850 --> 00:12:17,690
Narrowing scope to do A versus B that can be do or do not that could but but in that do not let's be really clear about what that pathway is.

93
00:12:17,690 --> 00:12:21,040
So like it could be. Surgery or no surgery.

94
00:12:21,040 --> 00:12:24,580
But if it's no surgery, what are all the things that you're going to do as sort of standard care?

95
00:12:27,330 --> 00:12:32,250
It could be two different types of interventions. Whatever. Try to avoid the multiple things.

96
00:12:32,580 --> 00:12:37,740
And you you have my permission to narrow the scope of who this applies to in order to make that happen.

97
00:12:42,470 --> 00:12:46,370
Must have quantitative risk benefit components.

98
00:12:46,460 --> 00:12:55,430
Not all of them, but at least some. And don't separate stuff.

99
00:12:55,550 --> 00:13:03,470
Remember, one of the key parts that we're talking about here is having somebody be able to see a holistically set of information.

100
00:13:04,040 --> 00:13:06,619
So avoid some of the decisions that gave.

101
00:13:06,620 --> 00:13:14,000
You do this a little bit, but try to avoid, especially for this assignment, putting all the risk stuff over here and all the benefit stuff over here.

102
00:13:14,010 --> 00:13:22,820
Like ideally you want these things like in the drug facts box table in the same place so that that trade off becomes as clear as possible.

103
00:13:26,930 --> 00:13:33,920
And I really encourage you if you're thinking about a context and you just want to run it past me like please do.

104
00:13:37,200 --> 00:13:43,940
All right. So. Today I want to really dove into.

105
00:13:46,170 --> 00:13:50,200
What is? The model of interaction.

106
00:13:51,750 --> 00:13:56,520
We want to work for one on one between a health expert.

107
00:13:56,760 --> 00:14:00,749
And we're talking today mostly in a medicine context.

108
00:14:00,750 --> 00:14:09,450
But I don't think that this is exclusive that you and the patient or the lay person or the community representative.

109
00:14:13,250 --> 00:14:18,860
I ask you to watch that video? Not because.

110
00:14:20,040 --> 00:14:21,839
I think it isn't a singular answer,

111
00:14:21,840 --> 00:14:27,090
but because I wanted you to make sure you understood the spectrum of different possible types of relationships that exist.

112
00:14:28,740 --> 00:14:40,380
And when I saw about two weeks ago, I guess, lecture for bunch of clinical scholars over at the medical school, and I asked them to watch that video.

113
00:14:41,730 --> 00:14:45,600
And then when we started the discussion, I started the discussion by saying.

114
00:14:47,020 --> 00:14:53,110
What are situations in which a paternalistic model is right?

115
00:14:55,860 --> 00:15:04,310
Before that you. What are some situations in which the paternalistic model the doctors God make the decision with no patient input.

116
00:15:05,560 --> 00:15:11,020
He's right. Do you think?

117
00:15:13,010 --> 00:15:22,800
She argued that the prosecutor's office waiting in the emergency department of.

118
00:15:25,870 --> 00:15:27,830
So let's let's lay out some examples here.

119
00:15:27,900 --> 00:15:34,180
Like, there are real obvious examples of situations in which there's not going to be a shared decision making,

120
00:15:34,180 --> 00:15:37,750
there's not going to be a deferral to patient decision or perspective.

121
00:15:38,050 --> 00:15:46,260
The doctor's going to do stuff and make those decisions about strokes.

122
00:15:46,930 --> 00:15:55,600
So it's natural to intervene. But after that point, it is your right.

123
00:15:56,080 --> 00:16:01,510
But there's then there's an asterisk to that. Yeah, I think of like interop during surgery.

124
00:16:01,510 --> 00:16:08,530
If the complication arises like that, the doctor, the surgeon would intervene right then instead of having like you're not going to take

125
00:16:08,530 --> 00:16:12,010
somebody out of anesthesia to ask him a question about what you ought to do in the surgery.

126
00:16:12,070 --> 00:16:12,350
Right.

127
00:16:12,940 --> 00:16:24,990
So the inability to provide input, whether that's because of a seizure, whether that's because of the extent of injuries, etc., is an obvious thing.

128
00:16:25,020 --> 00:16:29,080
You do at least what you need to get to that person to the point in which they can engage.

129
00:16:29,860 --> 00:16:34,839
That's one obvious thing. And yeah, I was going to say I read a little bit about it.

130
00:16:34,840 --> 00:16:40,930
It's it's not I could not tell you about a COVID situation, but a situation of something like tuberculosis,

131
00:16:41,290 --> 00:16:45,270
where it's extremely potent and can spread extremely fast.

132
00:16:45,280 --> 00:16:55,070
And maybe it may be okay for public health officials or, you know, clinicians to be paternalistic and say you need to go into isolation immediately.

133
00:16:55,180 --> 00:16:57,430
So the public health, isolation, power,

134
00:16:58,150 --> 00:17:05,799
so the fact that we have that that this field has quarantine power is a reflection of certain types of situations in which the collective

135
00:17:05,800 --> 00:17:13,930
good is so strong that we are willing to limit somebody's individual autonomy and decision making power in order to manage that.

136
00:17:16,050 --> 00:17:25,110
So what we use that rarely and very specific circumstances of another obvious context situations in which.

137
00:17:26,230 --> 00:17:34,490
It is unlikely that the patient. Input is going to have any substantive usefulness.

138
00:17:35,180 --> 00:17:38,610
I can think about situations in which. Yeah.

139
00:17:39,270 --> 00:17:43,920
Somebody needs a medication. They have no knowledge about the medication.

140
00:17:44,110 --> 00:17:47,520
I have no knowledge of the relationship to their particular health conditions, etc.

141
00:17:48,510 --> 00:17:57,059
We can get into this with drug ads like concerns about, well, let's use the obvious one and not the drug,

142
00:17:57,060 --> 00:18:04,260
but the the animal drug that everybody started asking for related to COVID 19 ivermectin.

143
00:18:04,440 --> 00:18:13,469
Yeah, but it isn't ethically appropriate for a clinician to step in at that point and say, no, this is not acceptable.

144
00:18:13,470 --> 00:18:17,580
This is not appropriate treatment. No, we are not having a shared decision making conversation.

145
00:18:17,580 --> 00:18:21,650
I am not prescribing this for you. Surgeons do this all the time.

146
00:18:21,660 --> 00:18:26,820
It's going to say, I am not willing to operate on you. It's not safe, whatever.

147
00:18:28,200 --> 00:18:36,749
We can go on and on here. But my point is spectrum and there are appropriate things in which the right answer is to be paternalistic.

148
00:18:36,750 --> 00:18:38,160
Where there is this is not a should.

149
00:18:39,870 --> 00:18:50,080
Now let's flip it on the opposite side, the opposite extreme situations in which really fundamentally you ought to be the patient's decision people.

150
00:18:52,480 --> 00:18:55,950
Yeah. Like labor and delivery stuff.

151
00:18:56,910 --> 00:19:04,260
Be more specific. There's a lot of so, like, work done, like shared birth partnership documents where you,

152
00:19:04,260 --> 00:19:07,389
like, lay out exactly what you want for your birth and that type of thing.

153
00:19:07,390 --> 00:19:13,710
And it goes into all sorts of like the medications and the music you want to play and like everything.

154
00:19:14,070 --> 00:19:18,900
Now let's note that. I'm going to chip away at that.

155
00:19:20,310 --> 00:19:31,350
And if something occurs, it's the birth process that was unanticipated that may be modifications made to that plan as a result,

156
00:19:32,010 --> 00:19:40,080
ranging everything from a conversation with the mother to an intervention that is then, you know, in some sense imposed.

157
00:19:40,090 --> 00:19:44,640
I mean, if the mother is going into shock, you're going to jump in and you're going to do something about that.

158
00:19:47,090 --> 00:19:54,170
But you know, there's definitely a strong element of this is the kind of of process that I am aiming for.

159
00:19:55,220 --> 00:20:00,800
What are other situations? Learn more explicitly on the patient side.

160
00:20:02,360 --> 00:20:05,480
Genetic testing. That's a really good one, right?

161
00:20:05,980 --> 00:20:09,200
I can get this information. I cannot get this information.

162
00:20:10,460 --> 00:20:14,540
That information obviously may have medical implications for me and my family.

163
00:20:15,890 --> 00:20:23,510
But we have genetic counseling precisely because one can go down either of those pathways.

164
00:20:25,160 --> 00:20:31,820
And thus that question of is that information going to be provided to the patient is grounded in what I want to know.

165
00:20:32,030 --> 00:20:34,009
There's also a shared decision making component.

166
00:20:34,010 --> 00:20:40,190
If I think there's more value like this, there are times in which that's not just purely on the patient's side,

167
00:20:41,360 --> 00:20:47,360
but we certainly have examples that are pretty strong and the few others that come to mind.

168
00:20:51,540 --> 00:20:58,970
And. That's more if you think about sort of elective abortions, things are like, why, why?

169
00:20:59,300 --> 00:21:06,740
I can do this, I can not do this, etc. Like immunotherapy for allergies treatment is.

170
00:21:07,670 --> 00:21:12,559
So why isn't that a shared decision as opposed to the patients? Because those those treatments have benefits and risks.

171
00:21:12,560 --> 00:21:15,920
And it was a trade off.

172
00:21:16,790 --> 00:21:22,520
Why isn't it a shared decision? Yeah. I mean, you're framing it as a patient decision as being from a shared decision.

173
00:21:22,820 --> 00:21:28,550
On my experience, it it's never like mandatory treatment.

174
00:21:29,090 --> 00:21:37,130
It's always kind of like, well, you can live with it or you can do this. So so that language to me is stepping away from pure patient.

175
00:21:37,400 --> 00:21:41,300
That's that language is. Yeah. Be more in that shared space.

176
00:21:41,630 --> 00:21:45,110
Now, remember, there's two pieces in this little set. Right, there's the.

177
00:21:47,440 --> 00:21:53,400
The addition is agent model. So I want I talk with my expert.

178
00:21:54,610 --> 00:21:57,850
We're talking here about health care. So we're talking about doctors and nurses.

179
00:21:58,540 --> 00:22:06,940
But honestly, the same dynamic occurs if you are in finance and you're talking to your investment advisor, for example, what is your risk tolerance?

180
00:22:07,210 --> 00:22:16,270
When do you think you're going to need your money, etc.? The conversation is around goals, around values, about what's okay for you.

181
00:22:16,300 --> 00:22:22,780
What is not okay for you? And then. Expert acting as agent makes that choice.

182
00:22:25,090 --> 00:22:34,590
The shared decision making process requires the patient or the public member to be an active part of that final process.

183
00:22:35,590 --> 00:22:37,480
Is this what I want to do or not?

184
00:22:39,160 --> 00:22:47,980
I think those gradations are important, but I don't want to get hung up on the premise that there is that there's a whole spectrum of stuff.

185
00:22:49,930 --> 00:22:59,380
And I hope I convince you up until now that there really are situations that fall into each one of these.

186
00:23:00,040 --> 00:23:07,270
And the right question is not how do we get everything we should decision making, it's when is shared decision making the right thing.

187
00:23:08,710 --> 00:23:12,820
And so the foreshadow where we're going to go at the end of this class. But the question stands up on the board.

188
00:23:14,260 --> 00:23:20,680
When might shared decision making undermine public health goals?

189
00:23:25,340 --> 00:23:28,100
Because there are times when that's possible.

190
00:23:29,180 --> 00:23:34,460
Plus, you ask, well, feel free to respond to it now, but it will come back to it when we're discussing in small groups.

191
00:23:34,520 --> 00:23:41,000
So what's your first thought when it affects others outside of oneself in ways that aren't just, like, emotional?

192
00:23:41,030 --> 00:23:46,340
Like if you decide not to get a surgery, your kids might be upset about something where it's transmissible.

193
00:23:46,490 --> 00:23:55,340
Yeah. So all of these infection control contexts, the more that we incorporate shared decision making,

194
00:23:55,340 --> 00:24:01,010
the more that there is potential, not necessarily guaranteed outcome, but certainly potential for outcomes.

195
00:24:01,010 --> 00:24:08,210
To be really careful with my words here to result in larger medical.

196
00:24:13,250 --> 00:24:20,630
But that is itself a definition of value that puts aside control and autonomy and other things.

197
00:24:20,780 --> 00:24:24,020
So better here is trying to balance those two things.

198
00:24:26,450 --> 00:24:34,520
So we'll come back to that. So let me start with bringing up a couple examples.

199
00:24:35,360 --> 00:24:42,750
I want to talk to you about something that. Is it an interesting part of show decision making?

200
00:24:43,470 --> 00:24:49,230
I saw a really interesting research presentation while I was in Seattle on this topic and I show you an example of it.

201
00:24:49,620 --> 00:24:53,190
But to start off, Audrey.

202
00:24:56,890 --> 00:25:03,550
In your music, you were really talking about essentially the process of values, clarification manifesting in decisions.

203
00:25:03,570 --> 00:25:06,010
It's a really nice example. So if you would be willing to share.

204
00:25:07,270 --> 00:25:13,330
So I went to the dermatologist to get moles removed because I have like a history of pre-cancerous moles.

205
00:25:13,900 --> 00:25:19,990
And this one, it was on my hip. And I was like, oh, like my doctor was saying, like gossip out of your area.

206
00:25:19,990 --> 00:25:27,490
So it might not like recover as well and might have more scarring with just like the traditional like razor and colorization method.

207
00:25:28,480 --> 00:25:33,790
But my whole thing is I didn't care about, like, scarring. I was like, I just care that it's not, like, pre-cancerous.

208
00:25:34,390 --> 00:25:39,010
So at the end, she's like, Oh, you can do this other method that has a smaller scar.

209
00:25:39,550 --> 00:25:43,840
But once I told her, like, I don't care. She just do like the usual razor.

210
00:25:44,890 --> 00:25:51,920
I remember right from Amusing. One of the things that was true about the alternate method was that you were going to have to come back to the.

211
00:25:51,930 --> 00:25:56,530
The second dimension of the choice was one visit versus two visits.

212
00:25:57,550 --> 00:26:01,240
And that's a valuable piece of your time and effort.

213
00:26:01,350 --> 00:26:10,060
I mean, that's equally part of this decision as a medical outcome. So now we're setting up a 1 to 1 visit versus two versus the scarring dimension.

214
00:26:10,540 --> 00:26:13,779
And you're making a choice. You're saying, okay, I don't care about the scarring.

215
00:26:13,780 --> 00:26:16,840
I do care about the visit that we be here rather than there.

216
00:26:17,740 --> 00:26:26,200
This is a reason why I like this is a simple example is notice that the critical piece here is the articulation of the dimensions.

217
00:26:27,100 --> 00:26:30,969
But you needed to understand that there is a difference in the level of the

218
00:26:30,970 --> 00:26:34,870
scarring and that there was a difference in the how this was going to play out.

219
00:26:35,590 --> 00:26:40,190
And there may be other dimensions, too, but it's that tradeoff that is the key piece of this.

220
00:26:41,020 --> 00:26:44,560
And essentially what the clinician is saying to us from a.

221
00:26:47,190 --> 00:26:49,589
Success standpoint. That's one to me.

222
00:26:49,590 --> 00:26:56,340
And by the way, I didn't even mention the question of are these equally likely to prevent cancer from growing and etc.?

223
00:26:56,490 --> 00:27:02,730
So that's a third dimension of this choice, and maybe it's an equal dimension, but knowing that is really important.

224
00:27:04,650 --> 00:27:09,480
This is the kind of laying out the pieces, laying out the dimensions that's at the heart of this task.

225
00:27:11,660 --> 00:27:14,000
But a key thing there which came up in the video.

226
00:27:14,000 --> 00:27:24,050
But what you're also articulating is you learn like if you had to do this again today, you'd already know that trade off.

227
00:27:24,470 --> 00:27:28,040
So you'd be better able to participate in that decision right from the start.

228
00:27:29,480 --> 00:27:35,840
And part of what's key about the education here is clarifying what that what those dimensions are and what the trade off is.

229
00:27:37,100 --> 00:27:39,139
So that's the that's what we're aiming for here,

230
00:27:39,140 --> 00:27:48,080
is sort of helping people know what are the dimensions of the choice so that when it matters or in your case, it doesn't matter, you can express the.

231
00:27:52,740 --> 00:27:57,470
Sarah. Concussions.

232
00:27:59,160 --> 00:28:03,860
Right. So. So let's talk about concussions.

233
00:28:05,170 --> 00:28:10,000
You wrote about it. And I want to hear sort of like what made you think about this in this context more?

234
00:28:10,930 --> 00:28:16,509
But it's a really interesting and important example of a situation in which we might get into the

235
00:28:16,510 --> 00:28:20,680
question of how much of a shared decision versus how much of an expert decision should this be?

236
00:28:22,470 --> 00:28:29,190
If I remember correctly, I think I wrote about when a concussion would actually terminate the athlete's career.

237
00:28:29,240 --> 00:28:34,770
Yeah. So it does it. I, we attended a guest lecture about.

238
00:28:35,830 --> 00:28:45,100
Um, that conversation between concussions, uh, the number of concussions, the number of traumatic injury actually affects a career.

239
00:28:45,850 --> 00:28:54,120
And ultimately there's not like a clinical list where be like, Oh, you hit X number or terminated from your athlete.

240
00:28:55,840 --> 00:29:03,069
It's actually a conversation between the athlete, the doctor and their family, right, about what that looks like for them.

241
00:29:03,070 --> 00:29:10,630
If they have to terminate the career or what their risk are, if it continues to play and a higher risk.

242
00:29:12,580 --> 00:29:20,920
And notice where the problem is. They both notice why this has to have some degree of shared decision making.

243
00:29:21,130 --> 00:29:29,730
It is the. Athletes here literally will either experience or not experience the negative outcomes of this choice.

244
00:29:31,440 --> 00:29:35,850
And the negative outcomes here needs to be on both sides, like there's the negative outcome of.

245
00:29:37,270 --> 00:29:43,810
Developing brain injury and worsening of their condition and a negative outcome of their.

246
00:29:45,710 --> 00:29:51,650
Athletic career, which may well be their professional career being terminated.

247
00:29:52,360 --> 00:29:55,970
And if you change someone's life, we don't go down that pathway lightly.

248
00:29:58,540 --> 00:30:01,840
That's the problem. But where is that likely to fail?

249
00:30:02,350 --> 00:30:12,350
Probably decision making or risk management standpoint. Okay.

250
00:30:12,470 --> 00:30:18,890
Feeling fine now? So this is not a short term, long term trade offs.

251
00:30:19,400 --> 00:30:33,650
They're fine now. The cost of not continuing the athletic career occurs now, the burden of that, the risks that they might face that occurs later.

252
00:30:34,660 --> 00:30:41,290
And there is clearly emotional and psychological and often financial implications.

253
00:30:42,820 --> 00:30:48,610
Now hard to push back against when you don't know whether it's going to turn out badly.

254
00:30:48,610 --> 00:30:52,900
We just know that it's more likely that something bad will happen if they continue their.

255
00:30:55,290 --> 00:30:59,909
That is a difficult conversation, but there's no simple way to make that call.

256
00:30:59,910 --> 00:31:03,060
And Sarah's point is right, there is no arbitrary formula.

257
00:31:03,090 --> 00:31:06,490
Nobody has said, okay, so that's your third research strike and you're out like.

258
00:31:07,690 --> 00:31:11,770
It's a judgment call. And.

259
00:31:15,240 --> 00:31:19,980
The one part of me is like, yes, that is absolutely where our situation, which our decision making must occur.

260
00:31:20,520 --> 00:31:25,290
And I'm aware that that patient's ability to engage in that is going to be biased.

261
00:31:26,340 --> 00:31:29,970
Yeah. I think that's why they, like, invite their family to come.

262
00:31:30,120 --> 00:31:36,330
Yes. It's like actually that's way of like I think to a non biased decision because they're not.

263
00:31:36,750 --> 00:31:40,440
Well I mean they're trying to unbiased. This is complicated.

264
00:31:40,440 --> 00:31:45,179
So you bring in outside people. So family members but family members have their own associated biases.

265
00:31:45,180 --> 00:31:48,840
They're proud of their family member for doing what they do.

266
00:31:50,610 --> 00:31:53,910
That may help to some degree, but it certainly isn't the only solution.

267
00:31:54,840 --> 00:32:00,600
Another thing that we are not going to talk about in this class I talked about before in my other communication classes,

268
00:32:01,230 --> 00:32:07,650
is how do you solve what is referred to as the affective forecasting error?

269
00:32:08,880 --> 00:32:15,120
So does the person really understand what would happen to them with further concussions?

270
00:32:16,080 --> 00:32:20,100
Well, one way you can solve that is not through me. You can put the numbers up.

271
00:32:20,100 --> 00:32:27,809
How likely is that? But you can also perhaps share narratives of athletes who had continued down this

272
00:32:27,810 --> 00:32:32,940
pathway and the harms that that befell them that they get a better picture of.

273
00:32:33,480 --> 00:32:40,990
This is what might happen to you. But again, it all comes back down.

274
00:32:41,000 --> 00:32:46,200
Even if they get that information, they may still feel. But they want to keep.

275
00:32:47,060 --> 00:32:53,520
It's going. And to some degree, from an ethical standpoint, I can't guarantee that they're wrong.

276
00:32:53,700 --> 00:32:57,780
But it's their life, it's their body, it's their values. They have to make that tradeoff.

277
00:33:00,880 --> 00:33:04,780
But this is where the shared decision after decision making.

278
00:33:05,920 --> 00:33:09,230
Peace gets complicated. Now,

279
00:33:09,310 --> 00:33:14,830
the prototype of shared decision making is the domain in which I gave you the full decision made by

280
00:33:14,830 --> 00:33:19,840
the original domain in which probably the most work has been done is prostate cancer treatment.

281
00:33:19,840 --> 00:33:31,520
Decision making. And the reason for that, aside from the fact that there's lots of money in cancer so you could get stuff funded is for decades.

282
00:33:33,060 --> 00:33:37,740
Early stage prostate cancer has been a coin flip in terms of mortality.

283
00:33:37,740 --> 00:33:44,670
And when you do surgery, whether you do radiation therapy or whether you did active surveillance outcomes,

284
00:33:44,670 --> 00:33:53,730
like for a certain subset of patients with with prostate cancer, you could not see any substantial differences in mortality resulting from it.

285
00:33:54,270 --> 00:34:03,030
But, boy, that was their experience is different. So that the lived experience was radically different, but the mortality outcome was not.

286
00:34:03,570 --> 00:34:07,100
And so very quickly, that field moved towards, look,

287
00:34:07,200 --> 00:34:10,500
we got to have this conversation with the patient because they're the one who's going to deal with.

288
00:34:11,750 --> 00:34:18,290
The impotence or incontinence outcomes for the psychological experience of knowing they have a cancer and growing inside them.

289
00:34:18,830 --> 00:34:23,180
Or the surgery, like each one of those things, had major impacts.

290
00:34:27,740 --> 00:34:35,460
Few things are quite as. Neutral on things like survival.

291
00:34:36,450 --> 00:34:38,399
Usually it's more of a just like, Oh, well,

292
00:34:38,400 --> 00:34:45,660
there's this little difference and then there's these experimental things and how much do we care about this versus how much do we care about that?

293
00:34:46,020 --> 00:34:52,099
It gets complicated. That's why I wanted to give you a prostate cancer decision to look at that.

294
00:34:52,100 --> 00:34:55,550
That's that's the domain in which and that was the one, by the way, is.

295
00:34:57,300 --> 00:35:00,360
And 16 years old, something like that.

296
00:35:00,360 --> 00:35:07,760
It's been around a while. There are clearly much more modern versions of it, but it's a nice basic.

297
00:35:08,120 --> 00:35:11,510
You see all the pieces. You understand how it's going to guide somebody through.

298
00:35:12,750 --> 00:35:15,930
By the way, while we're talking about that, when do people get that?

299
00:35:18,280 --> 00:35:21,760
You've read it. When would you want to get it? In the process.

300
00:35:25,380 --> 00:35:31,230
Okay. So what we kind of see when you say early stage, what do you mean by that?

301
00:35:36,950 --> 00:35:37,490
Fair enough.

302
00:35:38,920 --> 00:35:47,740
This was actually the subject of a major clinical trials like when not just what do you give a patient, but when do you give it to that patient?

303
00:35:48,630 --> 00:35:49,980
So here are some of the options.

304
00:35:54,970 --> 00:36:04,990
The earliest possible time you could give it to somebody is at the time in which they're getting the biopsy done to find out if they have cancer.

305
00:36:05,050 --> 00:36:09,910
So they already been diagnosed yet. You could give it to them to them to sort of warn them.

306
00:36:10,480 --> 00:36:19,260
If you get diagnosed, here's what your choice will be. You could give it to them the moment that they're diagnosed.

307
00:36:21,980 --> 00:36:26,930
We couldn't give it to them after the visit in which they learn that they've been diagnosed with cancer.

308
00:36:27,970 --> 00:36:37,830
To take home and then learn from. You could give it to them even further down the stream, sort of at a pre surgery discussion,

309
00:36:37,830 --> 00:36:41,340
like at the point of which you're starting to think about step down and things of head shape.

310
00:36:41,340 --> 00:36:48,590
But you know, that's probably too late. But did you really want the decision made before you were diagnosed?

311
00:36:48,640 --> 00:36:53,150
I mean, it's it's tricky. Like, when do you want this?

312
00:36:53,900 --> 00:37:02,270
And notice, part of the challenge here is that the decision aid included some information that probably we didn't want before.

313
00:37:02,270 --> 00:37:09,450
You know, you have cancer, like, what is it? And some information that is not actually relevant to you until you have been diagnosed.

314
00:37:10,340 --> 00:37:18,810
What your specific treatment options are might be. This, by the way, is an unresolved question, like there is nothing in general.

315
00:37:19,680 --> 00:37:24,210
What happens is, is you get diagnosed and then you tend to get decision aids because we don't know.

316
00:37:24,480 --> 00:37:31,770
We don't have clean populations to give this to beforehand. But this there's a lot of discussion about sort of like, what do you give people with?

317
00:37:36,400 --> 00:37:45,780
By the way, the outcome of that trial that I referred to was that it was not it didn't make as big of a difference as we thought it probably would,

318
00:37:46,260 --> 00:37:50,160
both in terms of people's emotional state and in terms of what they chose.

319
00:37:51,810 --> 00:37:54,840
But I'm not sure that's true in many other circumstances.

320
00:37:59,390 --> 00:38:07,040
Speaking of which, let's spend a little bit of time talking now about that Mrs. Reid case, the surgery case.

321
00:38:07,970 --> 00:38:10,940
And I want to start that discussion by asking, Annie,

322
00:38:11,840 --> 00:38:20,480
you brought up a very specific distinction about the way in which the vocal cord issue was articulated.

323
00:38:20,490 --> 00:38:29,030
That I want to follow up on what I remember you saying is, look, there's a difference between saying there's a chance of damage to the nerve,

324
00:38:29,030 --> 00:38:33,920
serving a vocal cord versus there's a chance you're going to lose your voice.

325
00:38:35,670 --> 00:38:43,399
Yeah, I thought like reliving the initial story that it was going to be about her using more technical language and not like putting it in real terms.

326
00:38:43,400 --> 00:38:48,710
And then it was very focused on the numbers which made sense given the way the story went on about patients.

327
00:38:49,340 --> 00:38:57,079
But I think more typically, like what I've seen, more than like real life patients not being like,

328
00:38:57,080 --> 00:39:00,230
Oh, I didn't understand what that would mean for me in my life.

329
00:39:01,040 --> 00:39:04,040
And I also think that, like a lot of times, if you say it,

330
00:39:04,460 --> 00:39:11,990
say there's a rare chance of damage to your vocal range or versus chance of you losing your voice, you get more questions from patients.

331
00:39:11,990 --> 00:39:13,910
And so then you have a conversation,

332
00:39:14,330 --> 00:39:20,620
a patients walk away from feeling more like they had a chance to ask questions and they understood what you were talking about.

333
00:39:22,700 --> 00:39:26,810
So, I mean, I think it made sense the context, but that was just something that stood out to me as like.

334
00:39:27,080 --> 00:39:33,860
So the thing that I really want to make sure we pull away from what you raised is there's a difference between

335
00:39:35,120 --> 00:39:43,940
precisely describing a medical condition with a symptom versus describing the outcome in someone's life.

336
00:39:47,300 --> 00:39:54,350
So here it was. Losing your voice versus somebody else.

337
00:39:54,350 --> 00:40:00,810
It might be. Experiencing shortness of breath versus you can't climb a flight of stairs.

338
00:40:06,620 --> 00:40:15,710
There's a translation between the terminology of health and medicine, which does need to be precise from the standpoint of professionals.

339
00:40:16,670 --> 00:40:19,760
To the language at every death.

340
00:40:22,720 --> 00:40:27,190
That is the way most of us really make our decisions.

341
00:40:29,680 --> 00:40:39,640
Even something as at this gets especially true when we talk about conditions that we are not comfortable talking about.

342
00:40:42,220 --> 00:40:46,720
For example, the prostate cancer decision is grounded if.

343
00:40:47,790 --> 00:40:52,530
The two of the most salient side effects is impotence and incontinence.

344
00:40:54,430 --> 00:41:06,080
Euro area bids to be specific. What are we talking about when I use those terms is actually really kind of important here is this.

345
00:41:09,860 --> 00:41:17,700
I'm going to dribble a little bit. And the uncomfortableness here is important for us to own.

346
00:41:17,710 --> 00:41:23,030
Like we don't have to talk about this stuff, but it is precisely because we don't talk about this stuff.

347
00:41:23,040 --> 00:41:32,430
At the very least, the vagueness matters for whether or not a patient is going to be able to describe what it is that they care about.

348
00:41:34,410 --> 00:41:42,630
And when I was talking about scarring, you knew it was bounded by the size of the the location that we talking about.

349
00:41:43,380 --> 00:41:47,980
But if we're talking about a symptom such as in pregnancy. Is this?

350
00:41:49,190 --> 00:41:56,380
Yeah. My dribble a little bit if I'm running hard for you, will be wearing a diaper for the rest of your life.

351
00:42:00,570 --> 00:42:02,820
Or anything in between like that.

352
00:42:02,820 --> 00:42:09,960
Specificity matters in terms of our ability to evaluate how serious that is, how much it would matter to your life, etc.

353
00:42:14,610 --> 00:42:19,440
As you dove into your assignment. Think about this.

354
00:42:20,550 --> 00:42:26,670
You will get clinical trials, you will look at patient education material, read them for precision,

355
00:42:26,670 --> 00:42:31,230
and then read them through the eyes of a patient and say, What is this really mean for my life?

356
00:42:31,830 --> 00:42:37,860
How would you describe it to your grant parent, to somebody who you would meet on the street, etc.?

357
00:42:37,890 --> 00:42:44,990
Like what? What are these things? On the more serious examples of my mind.

358
00:42:45,950 --> 00:42:59,600
I've told this story. I have a professional friend who I saw at the conference whose father passed away from cancer that manifested in their jaw.

359
00:43:01,270 --> 00:43:09,360
Top three years ago, and they had a very difficult round of treatment decisions to make over the last couple of years of their life.

360
00:43:10,990 --> 00:43:13,030
That they were really struggling with.

361
00:43:14,860 --> 00:43:21,120
And going trying to do a shared decision making process, trying to figure out what did this person want that they could decide,

362
00:43:21,120 --> 00:43:24,340
you know, should do not for major surgery here should be chemotherapy.

363
00:43:24,500 --> 00:43:32,460
But there were big differences. And finally, well into this process, after multiple rounds of appointments,

364
00:43:32,850 --> 00:43:38,130
they finally got around to talking about, will you be able to taste your food?

365
00:43:41,240 --> 00:43:47,270
And it was like a light bulb went off in this guy's head. It's like, yes, that I care about.

366
00:43:48,590 --> 00:43:51,830
Survival, maybe not degree of physical dysfunction.

367
00:43:52,610 --> 00:43:58,339
I want to make sure I can continue to taste my food. This is an important thing in my life and all of a sudden,

368
00:43:58,340 --> 00:44:02,899
like half the options dropped off the table and they were able to zoom down to a focus decision that

369
00:44:02,900 --> 00:44:08,570
made sense for that patient because he realized this was the if he wasn't going to live for that long,

370
00:44:09,140 --> 00:44:16,070
he was okay with that. But he was not willing to go down a pathway in which the enjoyment of food was something that was going to be denied.

371
00:44:16,670 --> 00:44:23,090
And it fundamentally changed the way in which that conversation progressed. That's a good shared decision making notice.

372
00:44:23,110 --> 00:44:26,680
It's not necessarily that he's going to be involved in every decision from that point.

373
00:44:26,690 --> 00:44:32,900
This is about to move into the position of agent. He has made it clear this is the dimension that he cares the most about.

374
00:44:33,770 --> 00:44:39,440
And then the physician can say, Well, okay, we're not doing that and we're not doing that because both of those are totally messed up.

375
00:44:39,440 --> 00:44:42,940
Your taste buds. Here's the remaining choice and I'm going to leave you.

376
00:44:42,950 --> 00:44:46,220
But the conversation shifts to a more guided process.

377
00:44:51,890 --> 00:44:56,810
Which I wanted to touch upon. City What you raised in your music at this point because you brought up this

378
00:44:56,810 --> 00:45:00,200
question and I don't remember the details that it probably doesn't matter.

379
00:45:00,200 --> 00:45:04,240
But you brought up the question of, you know. Values clarification.

380
00:45:04,250 --> 00:45:09,079
So I'm not talking a lot in this class about the specific ways in which we do values.

381
00:45:09,080 --> 00:45:16,300
Clarification. Mostly what I want you to take away from this is this idea of can we figure out what things matter the most you like?

382
00:45:16,310 --> 00:45:19,680
I want to be able to taste like it's far even.

383
00:45:19,700 --> 00:45:25,020
And cetera. And you brought the question, well, how much can we do this in appointments?

384
00:45:25,810 --> 00:45:30,510
I it's not a small thing. So just reflect a little bit of what you led to that.

385
00:45:30,540 --> 00:45:37,170
Yeah. So we use like value, like we do a full on values exercise and like every single like wellness coaching session that we do.

386
00:45:37,470 --> 00:45:42,540
And it's, it's fantastic. And people usually really enjoy it and we can like reference it later in the session.

387
00:45:42,540 --> 00:45:48,840
But it takes like 20, 30 minutes depending on how early the student wants to spend on it.

388
00:45:49,240 --> 00:45:56,350
So like this is fantastic. But like in a clinical plan, they can they spend like 20 to 30 minutes exploring that, you know?

389
00:45:56,820 --> 00:46:04,840
So now what? And this is the this is the thing that we have to wrestle with.

390
00:46:04,870 --> 00:46:12,760
So now what? You're not going to do that every time. And there's two answers that basically have come up as to resolve the problem that is raising.

391
00:46:13,870 --> 00:46:20,940
What is the design of the patient decision? It. We really care about this patient.

392
00:46:21,420 --> 00:46:26,700
For the most part are not designed to be used or read in the visit.

393
00:46:27,720 --> 00:46:30,870
They're designed to be used and read before the visit.

394
00:46:32,070 --> 00:46:35,640
Give them so they can take them home. They can read them at their own time.

395
00:46:36,000 --> 00:46:38,280
They can ask questions. They can look up stuff.

396
00:46:39,560 --> 00:46:48,620
The whole point is to move a lot of that work out of the business so that when the person comes and you only have ten or 15 minutes,

397
00:46:48,890 --> 00:46:55,160
they already know what their condition is, what the relevant dimensions are, and have some clue about what they might care about the most.

398
00:46:57,170 --> 00:47:01,220
The idea is that if the if you really want to hear decision making, where you have to have that back and forth,

399
00:47:01,610 --> 00:47:06,530
you focus, you get everything else done so that you can spend the limited time you have on that back.

400
00:47:08,810 --> 00:47:12,350
The other approach, which has a lot of strengths and weaknesses.

401
00:47:13,890 --> 00:47:21,300
Is. This is the expert and I'm going to get on a try and say this is a process that applies more broadly than just clinician.

402
00:47:22,820 --> 00:47:30,480
The expert narrows the focus. To just the few dimensions that are the most critical.

403
00:47:31,690 --> 00:47:38,440
And that is absolutely a, in one sense, shaping of the decision and manipulation of the decision process.

404
00:47:39,570 --> 00:47:44,140
But if we go back to Audrey's example. The.

405
00:47:45,920 --> 00:47:48,829
This is going to be different in terms of scarring,

406
00:47:48,830 --> 00:47:57,350
is an example of that by your clinician oriented view and said here is a dimension that I know is different between these.

407
00:47:57,530 --> 00:48:06,260
What do you think about this? That's not going to take nearly as long as doing a full values glorification of all the different things.

408
00:48:07,840 --> 00:48:11,260
But it is an expression of some to some degree of shared decision making,

409
00:48:11,260 --> 00:48:15,040
because now you get to elicit that person's preferences on those key dimensions.

410
00:48:15,940 --> 00:48:20,310
And honestly, for me, I think about the prostate cancer decision.

411
00:48:20,320 --> 00:48:32,150
You saw a full decision. It. But it boils down for the most part in my mind, too, if you do a surgery plan for having impotence and incontinence,

412
00:48:32,600 --> 00:48:39,740
if you don't do surgery, plan for the psychological experience of knowing you still have cancer in you, and that's pretty much it.

413
00:48:41,000 --> 00:48:46,550
If you can have that conversation at that level, you are well on the way to capturing the trade off.

414
00:48:47,210 --> 00:48:51,170
Not all the details, not the precise livelihoods, but a major chunk of.

415
00:48:52,550 --> 00:49:02,960
So that can be done. And in fact, there are there is now a line of research talking about what is called everyday decision making.

416
00:49:03,560 --> 00:49:09,050
What can you do in 2 minutes but still incorporate two elements of shared decision making?

417
00:49:10,000 --> 00:49:14,170
And it looks like this clinician already knows what the relevant dimensions are.

418
00:49:14,710 --> 00:49:18,040
They come into the deficit and they say, okay, so we have a choice to make.

419
00:49:19,190 --> 00:49:22,790
There's this, there's this. They vary on these dimensions.

420
00:49:23,180 --> 00:49:26,570
And that usually incorporates either an explicit statement of.

421
00:49:27,710 --> 00:49:36,380
A recommendation. So because this value is something that you care about, I think this one might be more valuable to use of that one.

422
00:49:36,380 --> 00:49:42,610
But I'm open to this conversation. Or the clinicians say there really is no medically better choice.

423
00:49:42,620 --> 00:49:44,450
This is a gray zone situation.

424
00:49:45,260 --> 00:49:50,630
I'd like to talk about, you know, which of these things matters, more teams and you can do that in about 2 to 3 minutes.

425
00:49:50,780 --> 00:49:57,430
But it is very much a shaped process. And it's not just simply giving somebody information and letting.

426
00:49:59,680 --> 00:50:08,860
So again, what I'm I'm asking you to make or I think I've said this, but let me reiterate, I want you to make for your next assignment.

427
00:50:09,920 --> 00:50:14,510
The thing you would give somebody at the end of the conversation.

428
00:50:16,840 --> 00:50:24,410
The summary of the information and the trade off of you're not building a full decision decision.

429
00:50:24,520 --> 00:50:29,830
I'm not asking you to do the introduction to all of these, but the risks and the issues, etc.

430
00:50:30,490 --> 00:50:38,830
I'm asking you to give somebody the things they would take away to process the trade off before they made the final decision.

431
00:50:39,250 --> 00:50:49,230
The summary table. That's going to allow you to have your thing be like 2 to 3 pages, not the 50.

432
00:50:51,050 --> 00:50:57,710
But it still requires you to do all of those dimensions there and set up that that table to show the trade off.

433
00:51:04,540 --> 00:51:07,950
So let me show you this now what we're talking about.

434
00:51:08,430 --> 00:51:10,710
Sure. Decision making I want to talk about.

435
00:51:12,810 --> 00:51:26,640
An idea that is I know it through the research of Alan Schwarz, who is a psychologist by training at the University of Illinois, Chicago.

436
00:51:27,960 --> 00:51:38,190
And he and his colleagues have been working on for decades now on the militaries of experts to recognize contextual factors in decision making.

437
00:51:39,000 --> 00:51:47,550
What do I mean by a contextual factor? The example I used when I presented on day to day was this.

438
00:51:48,540 --> 00:51:55,769
Imagine you've got a patient who's on, who has diabetes and who has blood glucose.

439
00:51:55,770 --> 00:52:01,530
Control doesn't seem to be getting better, even though they have been prescribed the medication that should be helping them.

440
00:52:03,950 --> 00:52:07,790
And. The issue is the all knowing.

441
00:52:07,810 --> 00:52:10,070
The issue is not that the medication is wrong.

442
00:52:10,090 --> 00:52:14,380
The issue is that the patient doesn't have the money to buy the medication and so isn't filling the medication.

443
00:52:15,480 --> 00:52:19,690
Here she comes in. What? Test results show it's not under control.

444
00:52:19,870 --> 00:52:25,240
Coalition says half this is obviously isn't working really well and increases the dose of the medication.

445
00:52:26,960 --> 00:52:28,460
That is not going to solve the problem.

446
00:52:30,170 --> 00:52:39,050
And so here's a table and I show you this table from one of their papers that's got examples of the kinds of things that pop up.

447
00:52:41,190 --> 00:52:45,450
So this could be rapid access to care.

448
00:52:45,790 --> 00:52:48,900
It could be issues in terms of people's emotional state.

449
00:52:49,470 --> 00:52:56,970
It could be issues of social support. Getting responsibilities like these are the social factors that are inhibiting.

450
00:52:58,360 --> 00:53:02,800
Otherwise potentially effective medical interventions from addressing the issues.

451
00:53:03,950 --> 00:53:09,650
So shared decision making in true form must address this.

452
00:53:10,920 --> 00:53:18,629
Like if the patient says, I can't afford the medication, treatment through shared decision making says, well, there are other options.

453
00:53:18,630 --> 00:53:20,190
Let's say there's less of you know,

454
00:53:20,620 --> 00:53:27,000
we think about the tradeoff between brand name drugs and generic drugs that may have been around for a while longer and are cheaper.

455
00:53:27,540 --> 00:53:32,100
They may be less effective, but you can have a shared decision making conversation about,

456
00:53:32,970 --> 00:53:38,370
well, if we do this one or you're more able to afford it, are you going to like the outcome?

457
00:53:38,370 --> 00:53:44,250
May in fact be better. And.

458
00:53:46,740 --> 00:53:50,130
What he's researching on is how do you get clinicians to pick up on this stuff?

459
00:53:50,940 --> 00:53:59,969
Like how do you help that conversation to unfold where the clinician can learn that there are social

460
00:53:59,970 --> 00:54:06,240
support issues or financial issues or emotional issues that are just as much part of the trade off.

461
00:54:07,670 --> 00:54:11,330
As the clinical issues or the medication side effects are.

462
00:54:12,240 --> 00:54:16,710
And then engage in the shared decision making about how you go forward from there.

463
00:54:17,680 --> 00:54:21,850
Now. I could probably read this. I could send this to you if you're interested.

464
00:54:22,690 --> 00:54:26,379
But. This. I want to roll in here.

465
00:54:26,380 --> 00:54:32,400
This idea that shared decision making isn't just always about the probabilities of particular outcomes.

466
00:54:32,410 --> 00:54:35,710
It's also how does this fit into somebody's life?

467
00:54:36,340 --> 00:54:38,440
Which is why I actually started with Audrey's example,

468
00:54:38,440 --> 00:54:47,139
because another example of that is if you can't get back to the clinic in two weeks, you have your staples taken out.

469
00:54:47,140 --> 00:54:55,719
Or if you can't come back, if you couldn't do a sequence of coming for radiation therapy every day,

470
00:54:55,720 --> 00:55:01,420
every week for a day over the course of six weeks, then why are we talking about that option?

471
00:55:02,350 --> 00:55:08,820
It is not a viable option for you, even if it is a clinically possible option for that particular version.

472
00:55:10,440 --> 00:55:16,800
So that is an example in which the shift that the decision process has to represent the contextual components of this.

473
00:55:18,910 --> 00:55:27,069
Because risk of non-adherence due to financial or social life is just as much of a risk as risk of headache.

474
00:55:27,070 --> 00:55:41,230
Because the medication cause that. I want to hear that.

475
00:55:41,830 --> 00:55:49,420
Oh, Mark, something you brought up in your music, which I want to spend some time talking about today, was.

476
00:55:51,340 --> 00:55:58,210
How much this process of shared decision making is about written stuff as opposed to talked about.

477
00:55:58,550 --> 00:56:02,410
So share a little bit more about what led you to write about that?

478
00:56:02,440 --> 00:56:11,510
Yes, that just made me think of my grandpa when he was in his nineties and still in a very mentally, you know.

479
00:56:11,530 --> 00:56:14,589
Q And able to follow a lot of these medication lists that he had.

480
00:56:14,590 --> 00:56:16,920
I remember just talking to one morning and, you know,

481
00:56:16,960 --> 00:56:21,550
he had this big pillbox of this kind of joking around about which ones are more important than others and stuff.

482
00:56:21,550 --> 00:56:25,360
Because he had this kind of written list of your like once I got this one,

483
00:56:25,400 --> 00:56:29,650
the other showstopper, like all the other ones, you know, whatever, take up as I will.

484
00:56:30,400 --> 00:56:36,760
And I just imagine, like, how hard that would be if you didn't have any kind of written communication, especially for,

485
00:56:36,760 --> 00:56:42,010
you know, older people, or if he was still pretty cognizant and able to follow this information accurately.

486
00:56:42,980 --> 00:56:46,670
Especially if I was knocked out of your mental state. I didn't have any.

487
00:56:46,670 --> 00:56:51,700
Communication seemed pretty important to me. Otherwise, I don't know how you keep track of, you know, telephone calls to take.

488
00:56:53,000 --> 00:57:00,229
So take the wrong one. It could be really bad. Yeah. I mean, so you're focusing your example here focuses on the complexity problem.

489
00:57:00,230 --> 00:57:04,040
You've got so many different things keeping track of what's important and what's not,

490
00:57:04,040 --> 00:57:08,630
what you take, what etc. and why written material is so valuable there.

491
00:57:09,770 --> 00:57:13,640
The parallel in the decision making space is.

492
00:57:15,660 --> 00:57:20,250
I mean, let's be clear here. And have you brought this up in the in the in the media?

493
00:57:20,600 --> 00:57:27,120
Like the moment we tell somebody that they have a medical decision to make is not generally the moment in which

494
00:57:27,120 --> 00:57:31,800
they have the most cognitive capabilities to deal with all the information we're about to be telling them.

495
00:57:32,750 --> 00:57:38,120
It doesn't matter how educated you are, doesn't matter how numerate or literate you are,

496
00:57:38,300 --> 00:57:41,450
you are not really going to be processing as well as you could.

497
00:57:43,160 --> 00:57:48,380
And you are you're dealing with you've just been provided new news that your life has been changed.

498
00:57:49,130 --> 00:57:58,520
So a key element of the decision support idea is to give people information in a

499
00:57:58,520 --> 00:58:02,910
format that enables them to process it when they have the capability to process.

500
00:58:04,000 --> 00:58:11,229
So historically that's been written a lot now has moved online so that people can go back to the website or the app,

501
00:58:11,230 --> 00:58:15,820
etc., and get that information there. And that's fine. Like we can they adapt through the technology.

502
00:58:16,480 --> 00:58:21,940
But the key idea here is when I say, oh, wait a second,

503
00:58:21,940 --> 00:58:28,120
wasn't there wasn't there something about radiation that was going to bug me or mess up my job?

504
00:58:29,080 --> 00:58:35,680
I got to be able to go back and figure out what it was that God said, because I will totally have forgotten about it.

505
00:58:36,840 --> 00:58:42,240
To be able to that process, whether that's a good idea. Because nobody can keep all of this.

506
00:58:43,470 --> 00:58:52,740
And honestly, that's part of the purpose of decisions, too, which is to create, in some sense, a checklist for the conversation.

507
00:58:53,010 --> 00:58:56,940
Then we talk about that becomes a big key piece of this.

508
00:58:57,540 --> 00:59:02,550
So now I want to talk to you about the optics around this decision to also share with you guys.

509
00:59:04,690 --> 00:59:08,409
That one's a sort of an exception to the rule of this is primarily for the patient.

510
00:59:08,410 --> 00:59:17,110
That one is is both for the patient and the clinician. The flow charts for the clinician, the flip side, the tables for the patient.

511
00:59:19,360 --> 00:59:24,070
The story of that project is basically this is a domain in which.

512
00:59:25,320 --> 00:59:37,000
Prescription of steroids happens routinely. And a large fraction of those prescriptions are not in alignment with the studies.

513
00:59:37,420 --> 00:59:41,740
And this is not new evidence like this is stuff that's been around a while. So there's no.

514
00:59:42,130 --> 00:59:44,440
Oh, I just didn't read that paper kind of excuse.

515
00:59:44,440 --> 00:59:52,360
Like the pattern of of prescribing prescribing steroids is built into the sort of standard practice in most contexts,

516
00:59:52,750 --> 00:59:56,020
and yet there's clear reasons why it's happening too much.

517
00:59:56,560 --> 01:00:00,100
And so this question came to me. I was like, okay, so how do we do this?

518
01:00:00,610 --> 01:00:01,900
How do we break the cycle?

519
01:00:02,320 --> 01:00:08,650
And our conversations basically said, okay, we have to look separately at the problem from the patient side and from the Canadian side.

520
01:00:08,710 --> 01:00:13,210
From the clinician side, it's. Making sure they understand.

521
01:00:13,220 --> 01:00:16,520
If you don't have each of these, there's a couple risk factors.

522
01:00:16,520 --> 01:00:21,500
The ones that would lead you to the side where it said, yes, it was green and it said yes, there may be some benefit here.

523
01:00:22,320 --> 01:00:29,870
You don't have those factors then this is there is no clean medical evidence to argue for prescribing.

524
01:00:31,750 --> 01:00:38,139
It becomes a shared decision making conversation. And then the idea was to support,

525
01:00:38,140 --> 01:00:46,130
give the clinicians a few talking points and that summary table on the back so that in the visit they could like flip it over and yes,

526
01:00:46,150 --> 01:00:50,950
go through the table. This is true and this is true and here's what the trade off is, etc.

527
01:00:51,550 --> 01:01:00,640
So that in that context, at least the goal was at least the patient would walk out the door knowing there was a choice,

528
01:01:01,540 --> 01:01:07,330
knowing that the only benefit was possibly reducing the time.

529
01:01:09,580 --> 01:01:16,299
To me, the main thing that I hope that works on is reducing the number of patients who think

530
01:01:16,300 --> 01:01:19,510
that taking steroids will increase the chance that their vision will come back,

531
01:01:22,060 --> 01:01:26,770
because there's no evidence of any long term benefit in quality.

532
01:01:28,360 --> 01:01:32,350
And that in talk of a mental model, that's a mental model fix.

533
01:01:32,860 --> 01:01:36,370
Why do we give people things? Because it makes us better. Not in this case.

534
01:01:36,370 --> 01:01:40,059
It's only time. It doesn't actually have any evidence.

535
01:01:40,060 --> 01:01:45,330
It makes us better. We'll see how that one plays out.

536
01:01:45,350 --> 01:01:51,649
She literally just put in the ground to test that decision and October submission.

537
01:01:51,650 --> 01:01:59,490
So to be. What else do I want to be here today?

538
01:01:59,850 --> 01:02:03,150
Oh, yeah. Yeah.

539
01:02:05,960 --> 01:02:12,740
Talking about choosing wisely. Yeah.

540
01:02:12,740 --> 01:02:18,830
So choosing wisely is like a campaign in primary care, really.

541
01:02:18,830 --> 01:02:25,340
Just trying to talk about a lot of like big issues or reasons that patients would come in like antibiotic

542
01:02:25,340 --> 01:02:32,360
use and basically helping physicians be able to explain their clinical decision making to patients.

543
01:02:32,960 --> 01:02:39,440
And I kind of like looked at them and I like feel like they're almost made to help, like with shared decision making.

544
01:02:39,440 --> 01:02:48,860
But I also feel like a lot of physicians just kind of give them to patients at the end of visits to kind of like explain why

545
01:02:48,860 --> 01:02:56,840
they did something rather than actually like engage in a conversation with the patient during the actual clinical encounter.

546
01:02:57,200 --> 01:03:00,290
That was kinda like my feelings on it. So I'm intrigued by your framing of.

547
01:03:02,300 --> 01:03:08,300
As it is with the language I just try to use with explain why they did something right.

548
01:03:09,360 --> 01:03:16,850
At the heart of the choosing Wisely campaign is a focus on overutilization of low value services,

549
01:03:16,860 --> 01:03:24,360
things which we test, interventions, etc. that we know don't have significant benefit.

550
01:03:28,240 --> 01:03:33,000
You could have phrase that as explaining why we didn't do something.

551
01:03:34,660 --> 01:03:44,709
And there's an I want to raise that because there's a framing of inaction or choosing the less intervention, not doing the scan, etc.

552
01:03:44,710 --> 01:03:52,480
That is part of the conversation that that piece of it right here is hard to tell somebody, No, we're not going to do this.

553
01:03:54,040 --> 01:03:59,510
And so part of the messaging from the Choosing Wisely campaign has been about sort of basically reframing this.

554
01:04:00,480 --> 01:04:05,850
Decisions not to do something as decisions to do something, to do a different thing.

555
01:04:06,870 --> 01:04:12,600
So instead of it being a versus not a, it becomes A versus B.

556
01:04:15,020 --> 01:04:20,830
In the prostate cancer example. The variant of this is.

557
01:04:22,470 --> 01:04:33,130
Here is why we're not going to do a surgery on you. But it's often reframed here as a choice to pursue active surveillance.

558
01:04:37,440 --> 01:04:43,740
But that language is a key piece of this because you just told me I have cancer.

559
01:04:44,040 --> 01:04:47,730
It's going to be hard for me to say I don't want to do anything about that.

560
01:04:50,000 --> 01:04:57,920
But if it's presented ad there's no benefit in the survival sense of doing surgery.

561
01:04:57,920 --> 01:05:07,910
And there is you can do this thing, this thing being coming back on on a regular basis for further tests until actively what, etc.

562
01:05:08,000 --> 01:05:11,240
However, we define active surveillance that feels different to people.

563
01:05:12,700 --> 01:05:15,910
But the Choosing Wisely initiative is a fascinating example of.

564
01:05:17,860 --> 01:05:22,600
In one sense, it's shared decision making because it's engaging patients.

565
01:05:23,760 --> 01:05:25,470
In another sense, it's persuasive messaging.

566
01:05:27,110 --> 01:05:33,200
And that's part of the reason why it's been used the way you're talking about in terms of like this is their justification.

567
01:05:34,140 --> 01:05:42,670
They're trying to enable patients to feel comfortable with decisions essentially that were already pre-made.

568
01:05:42,710 --> 01:05:47,150
We're not going down this path where we're going to take the less interventionist approach.

569
01:05:48,160 --> 01:05:52,570
And then to encourage patients to feel comfortable with that. And that's the hard part about it.

570
01:05:53,800 --> 01:05:57,590
Usually. At least in this society.

571
01:05:59,560 --> 01:06:03,760
It is way harder to get people to not do stuff than it is to get them to do stuff.

572
01:06:05,360 --> 01:06:09,830
If I say to you, Hey, there's this test, it probably doesn't make a difference.

573
01:06:12,050 --> 01:06:16,870
Not usually. But. But just to be safe will do it. Most people, I'm sure, if.

574
01:06:16,900 --> 01:06:23,780
Fine, you flip that around. Hey, there's this test, but it probably, probably won't help.

575
01:06:23,840 --> 01:06:27,200
And so we're not going to do it. And there's a lot of hesitation.

576
01:06:28,490 --> 01:06:32,870
And that is another domain that we need to talk about for our in the discussion,

577
01:06:32,870 --> 01:06:38,570
which is there may be things which are not necessarily clinically damaging,

578
01:06:39,470 --> 01:06:46,190
but that patients push for that the public pushes for that have societal implications.

579
01:06:47,100 --> 01:06:55,020
And the cost of resources. You spend a lot of money on sort of things that don't give us a lot of benefit because it makes people feel good.

580
01:06:58,160 --> 01:07:06,110
So if shared decision making ends up leading us to have scans for that lower back pain,

581
01:07:06,110 --> 01:07:10,970
surgery is for, you know, cancers that don't actually reduce mortality.

582
01:07:12,050 --> 01:07:17,110
That may be shared decision making, but it may not be good practice of public health or good practice and that.

583
01:07:24,670 --> 01:07:36,010
Any other stuff I want to make sure to touch upon. I guess the last thing I want to mention before having this break apart is this.

584
01:07:39,560 --> 01:07:44,070
Go back to the missus, for example. A number of you wrote about it.

585
01:07:44,100 --> 01:07:53,190
I'm not going to necessarily call out individual people, but I want to pause for a second and think back to that that discussion that the doctor had.

586
01:07:55,390 --> 01:08:02,970
What did they do? What? This was not a.

587
01:08:04,320 --> 01:08:11,610
Your paternalistic interactions. There was a conversation between the doctor and patient.

588
01:08:12,330 --> 01:08:17,220
There were things that were brought up. The doctor did not just bulldoze through and say.

589
01:08:19,730 --> 01:08:23,389
Okay. So this is what we're doing. What did they do?

590
01:08:23,390 --> 01:08:26,960
Right. Because there are right and wrong. And this is why this is an ethics case.

591
01:08:27,680 --> 01:08:34,170
But let's it's it's easier in some sense in the context of this course to pick on the things that we might think they did wrong.

592
01:08:34,190 --> 01:08:37,350
What could be worse? Yeah.

593
01:08:38,560 --> 01:08:45,070
After 2 hours, if she had to stay in the hospital, she, like the doctor, responded.

594
01:08:45,440 --> 01:08:51,250
Explain some of the complications, what they would do to and treat those specific complications.

595
01:08:51,970 --> 01:08:56,170
So the presence of complications was explicitly acknowledged.

596
01:08:59,550 --> 01:09:08,940
And there was some framing of that, maybe imperfectly, but it's certainly some framing of the consequences of those surgical complications.

597
01:09:11,300 --> 01:09:21,440
So certainly I don't think really walked out of that conversation with no understanding of the possibility that the surgery might result in harm.

598
01:09:24,390 --> 01:09:27,450
Hey. What else? What else was good in that context?

599
01:09:28,720 --> 01:09:34,240
Yeah. Suggesting to do it for like a three day weekend, knowing that she doesn't want to miss a lot of work.

600
01:09:34,570 --> 01:09:40,810
So there was an acknowledgment of sort of the short term life implications of this decision.

601
01:09:41,770 --> 01:09:47,200
Warning them of, you know, this is not a you can walk out and go right back to work kind of situation.

602
01:09:47,800 --> 01:09:54,190
And framing, giving them advice to support them, be able to fit this into their life to some degree.

603
01:09:54,460 --> 01:09:59,660
Yes. Yeah. Also, like the surgery is important to have, like it's of like.

604
01:10:00,010 --> 01:10:04,420
Yeah, that's a really important one. Like, they're really important.

605
01:10:05,020 --> 01:10:10,450
One of the key things of supporting your decision making is one, making it clear that there is a choice.

606
01:10:10,990 --> 01:10:16,510
So the idea of one of the choices that is actually available here is time.

607
01:10:16,620 --> 01:10:24,760
We can do this now, we can do this later, etc. There are some decisions like the prostate cancer decision which the time is available,

608
01:10:24,850 --> 01:10:28,830
like no urologist for for certainly for an early stage prostate.

609
01:10:28,840 --> 01:10:31,990
You know, urologists say, okay, so we got to get this done this week.

610
01:10:32,470 --> 01:10:36,340
Oh, you don't know. Nothing's going to happen in one week. That's going to change anything.

611
01:10:36,650 --> 01:10:39,520
Now, by the way, there are other cancers for which that's not true.

612
01:10:40,610 --> 01:10:46,170
And so knowing is this one where we have to decide right now or not becomes a really critical piece.

613
01:10:48,680 --> 01:10:55,150
So yeah, that's another key piece here. What else did the doctor like?

614
01:10:55,720 --> 01:10:59,850
Get some space, like ask questions about this, which makes sense.

615
01:11:00,430 --> 01:11:07,810
So there was definitely an effort to engage the patient in eliciting something.

616
01:11:08,300 --> 01:11:20,400
Maybe not as much as we would like. But a pause for questions and at least some degree of this is a subtle one.

617
01:11:20,400 --> 01:11:25,190
And I want you to think about this. Did. The Coalition.

618
01:11:26,440 --> 01:11:29,780
Place the final yes no in the patient's hand?

619
01:11:30,600 --> 01:11:36,660
Or what is the sort of go? No, go? I'm making this choice.

620
01:11:37,880 --> 01:11:44,550
Are you okay with that? How would you how would you frame sort of that end moment of that conversation?

621
01:11:44,600 --> 01:11:54,309
Whose choice was it? You don't have to look it up like.

622
01:11:54,310 --> 01:12:00,130
What was your memory? I was a patient. Okay. If that's what you were left with, that's an important piece here.

623
01:12:02,850 --> 01:12:09,360
But in the end, what you heard was it was the patient's choice to say, yes, I want to do this.

624
01:12:09,600 --> 01:12:17,220
Yes, I'm okay with that. Now, we may look back at that and say they were insufficiently informed at the moment at which they're making that choice.

625
01:12:18,330 --> 01:12:22,380
And that's a problem. Well, let's also acknowledge.

626
01:12:23,570 --> 01:12:33,970
They felt like they had a choice. And the conversation came to a point in which it was that there are plenty of interactions between

627
01:12:34,300 --> 01:12:39,700
health experts and the public or patients in which the patient never feels like they had a choice.

628
01:12:41,530 --> 01:12:56,650
Never felt like there was a moment in which they could say no. And also like from an example of the doctor kind of frame it like this surgery they've

629
01:12:57,460 --> 01:13:02,740
done it multiple times is kind of easy so as a patient's perspective that she might think,

630
01:13:02,740 --> 01:13:09,430
okay, she's done it multiple times with the minimal risk so I'll pick it up even though I pick the surgery.

631
01:13:10,210 --> 01:13:19,870
Yeah. So there's where we start to get into the bias problem. Like the clinician is framing this as routine risks.

632
01:13:21,190 --> 01:13:29,860
Basically, trust me, acknowledging that things can happen, but conveying an emotional message of these are not likely to happen.

633
01:13:29,860 --> 01:13:37,950
This is, you know, things will turn out well. And what they then that that leads us to potentially believe is that the patient

634
01:13:37,950 --> 01:13:42,050
might hear that emotional tone and make a choice to go ahead with surgery,

635
01:13:42,060 --> 01:13:46,440
not fully understanding the complication risks as it would play out for them.

636
01:13:49,350 --> 01:13:57,060
But I also want to acknowledge. If a clinician meets the standard that we might hold for informed consent.

637
01:13:59,640 --> 01:14:12,940
I mentioned. Risk of something happening to the acknowledged variety of surgical complications may give the person a choice.

638
01:14:14,470 --> 01:14:23,680
So from a legal standpoint, I'd be hard pressed to say that there was a failure of informed consent, even if, from an ethical standpoint,

639
01:14:24,010 --> 01:14:30,640
it became clear as this played out that the person did not fully understand the risks in the way that would matter for their life.

640
01:14:32,810 --> 01:14:37,010
And that's the tension that this ethics case is designed to bring up.

641
01:14:38,370 --> 01:14:44,399
Yeah. That's kind of like a critical part of informed consent, though, that like, you don't just like, wait things out.

642
01:14:44,400 --> 01:14:50,820
The you do that like the patient understands like what you're saying and like the risk.

643
01:14:50,850 --> 01:15:00,870
And that is why I wanted to bring this up is yes, from an ethical standpoint, you hold the belief that true shared decision making,

644
01:15:00,870 --> 01:15:05,940
true participation, true informed consent requires a level of understanding that goes past.

645
01:15:06,240 --> 01:15:09,510
Did you present you with that information? But how did they process it?

646
01:15:11,650 --> 01:15:16,180
And so we can look at this case and say this was a failure of informed consent.

647
01:15:16,190 --> 01:15:20,230
This was not true shared decision making in terms of fully informed.

648
01:15:21,330 --> 01:15:25,320
While still ticking off the various elements that might lead us to say.

649
01:15:27,330 --> 01:15:31,210
A very appropriate behaviors occurred. They just were not enough.

650
01:15:32,450 --> 01:15:37,570
Yeah, that's right. And this is why I want to draw the distinction between the legal requirement and perhaps our ethical.

651
01:15:39,050 --> 01:15:40,820
Let's start on this before we break up. Yeah.

652
01:15:40,820 --> 01:15:46,729
I mean, the patient also comes in and says, well, if I had known there is a 4% chance, I never would have done it.

653
01:15:46,730 --> 01:15:52,010
But that's like you're so biased at that point because you had the 4% outcome that

654
01:15:52,460 --> 01:15:58,280
you she can't say definitively that in the previous moment you wouldn't have said,

655
01:15:58,280 --> 01:16:02,330
oh, 4%. That's not very high. But it's it's this retrospective bias.

656
01:16:02,390 --> 01:16:12,470
I'll be stronger than that. I am totally not convinced that had the doctor said there is a 4% chance of you of this outcome occurring,

657
01:16:12,680 --> 01:16:15,770
that that would have changed all that decision.

658
01:16:15,770 --> 01:16:17,240
Yeah. So like saying that,

659
01:16:17,450 --> 01:16:26,899
not saying that would be not getting informed consent is discounting the fact that like this situation could happen 96 times where

660
01:16:26,900 --> 01:16:36,080
there is no bad outcome in a hundred and the with ever with ever not being told about the 4% chance and it has no issues at all.

661
01:16:36,920 --> 01:16:41,920
To me the real failure here is. A possibility.

662
01:16:41,920 --> 01:16:45,050
Failure more than a probability. It's not the 4%.

663
01:16:45,100 --> 01:16:49,179
It's the. I didn't really process that.

664
01:16:49,180 --> 01:16:57,340
I could lose my voice from this. Whether it was 4% or 1% or 8% is not really the issue here.

665
01:16:58,000 --> 01:17:02,980
She did not process the potential for what was for her a catastrophic outcome.

666
01:17:05,330 --> 01:17:12,700
And the conversation needed to engage in that level of discussion in order for her to really fully understand what she was putting herself on.

667
01:17:14,120 --> 01:17:20,750
So take the last few minutes today. We spent a lot of time talking about sort of individual failures.

668
01:17:21,290 --> 01:17:26,059
I want to go back to the sort of question of the larger failures like here.

669
01:17:26,060 --> 01:17:32,299
Decision making allows people to potentially say, yeah, I don't want to do that or I do want to do that.

670
01:17:32,300 --> 01:17:37,250
I want to have this scan even though it's not going to potentially change my, you know, be appropriate.

671
01:17:37,250 --> 01:17:41,780
I want to have this surgery even though it might put me at greater risk, etc.

672
01:17:42,200 --> 01:17:45,410
And by the way, there was another poster at the conference that I was at that was.

673
01:17:46,370 --> 01:17:48,379
Replicated a result that I've seen before,

674
01:17:48,380 --> 01:17:59,150
which is that there are situations in which people will sign up for screening and do surgeries when it has zero benefit and significant risk to them.

675
01:18:00,590 --> 01:18:04,640
They were told this has no benefit and it has risk. And they said, Yeah, I want it anyway.

676
01:18:07,170 --> 01:18:13,620
Because we have a bias to doing things. So given that involving people,

677
01:18:13,620 --> 01:18:19,310
allowing people to make their own personal choices means that sometimes they're going to choose things that we might not otherwise want.

678
01:18:20,470 --> 01:18:25,110
Then I might have shared decision making. Have some implications for public health.

679
01:18:26,910 --> 01:18:31,489
Think about cancer screening. Think about. Resource utilization.

680
01:18:31,490 --> 01:18:38,120
Think about infectious disease. Think about these different contexts in which the choices that we make might affect others, might affect society.

681
01:18:38,600 --> 01:18:58,170
Okay, chew on that for a little while. It's been a privilege and I would like to share a great like this.

682
01:18:59,090 --> 01:19:14,640
What can I say? I'm very excited about how this plays out.

683
01:19:18,020 --> 01:19:38,030
When I thought of something that you said, it was like things like that were just stupid.

684
01:19:38,090 --> 01:19:58,060
So sounds like right now we're trying to look like maybe it night, obviously.

685
01:19:59,600 --> 01:20:07,480
Or you can tell us why.

686
01:20:10,130 --> 01:20:14,480
I don't think so.

687
01:20:16,350 --> 01:20:22,300
Whatever. Anything.

688
01:20:24,200 --> 01:20:29,150
I feel like there was like it's just like this other guy is just one thing I worry about now.

689
01:20:29,990 --> 01:20:38,389
I'm like, I'm going to write another four years.

690
01:20:38,390 --> 01:20:56,240
I feel like I'm not arguing about like this, but there's never a thing that I'm just asking.

691
01:20:56,790 --> 01:21:01,920
That's what I find out.

692
01:21:03,500 --> 01:21:06,740
It's something that, you know.

693
01:21:07,910 --> 01:21:14,810
But I think it's just in case, and I don't want to have my brother figure it out.

694
01:21:14,870 --> 01:21:27,850
But like also I think that they were like, Yeah, I don't know about this question of trade offs.

695
01:21:29,230 --> 01:21:35,660
That's a class we're gonna really engage with questions of trade offs, especially about public health programs like cancer screening.

696
01:21:36,530 --> 01:21:42,090
I come back to this, this debate, which is all I'm going to hang around for a little bit.

697
01:21:42,100 --> 01:21:49,310
If you have questions about your ideas, remember that a week from today you bring a draft to class.

698
01:21:50,330 --> 01:22:02,250
So we think for that one, mostly, I think, you know, yes, I'm a big fan.

699
01:22:03,010 --> 01:22:07,339
I don't sleep. I don't know.

700
01:22:07,340 --> 01:22:12,170
I guess I really I was tired of waiting for what I'm saying.

701
01:22:12,890 --> 01:22:35,629
I'm not sure how long it's watch the government so I countries like to know and I think also like the I think when

702
01:22:35,630 --> 01:22:44,130
they remember because they definitely prefer yes or no you will not believe it when you are a different topic even.

703
01:22:44,570 --> 01:23:03,100
Yes. I mean, imagine that somewhere this was something that you could take out necessary to point of right now.

704
01:23:03,740 --> 01:23:12,650
Yes, that's an appropriate thing to bring up. We wrestle with a lot, figure out ways to do interventions.

705
01:23:14,420 --> 01:23:25,069
So what I heard somebody go over there like we don't really have an out like I guess surgery or something that brings up the next part,

706
01:23:25,070 --> 01:23:29,400
you know, no longer understood.

707
01:23:29,900 --> 01:23:33,730
Well, more with that as you see it with your mind.

708
01:23:33,840 --> 01:23:43,070
I know for me more than one for takes and other people have found success because they've gotten better.

709
01:23:44,180 --> 01:23:48,460
But at the same time, there can be serious surgical complications.

710
01:23:48,470 --> 01:23:53,720
The recovery can take as long as two years, whereas platelet rich injection is basically a stem cell.

711
01:23:54,560 --> 01:23:58,760
So I guess I'll see you. Have you guys complicated?

712
01:23:58,850 --> 01:24:06,319
But it also doesn't have to be quite effective that Tommy John surgery has less best chance of.

713
01:24:06,320 --> 01:24:13,530
It's going to get really interesting. This meeting last time, I was like, Yep, sure.

714
01:24:14,450 --> 01:24:33,630
But it would be like, you know, in surgery, like, for ago, I it's even going.

715
01:24:40,800 --> 01:24:47,500
Has come after what you said, namely some of the evidence that showed because I was kind of basing it off the examples.

716
01:24:47,510 --> 01:24:54,860
Yeah, that that was interesting because really offering is a process.

