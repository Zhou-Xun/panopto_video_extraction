1
00:00:01,170 --> 00:00:05,220
Okay, Connie. All right.

2
00:00:05,430 --> 00:00:09,360
So we have a lot about today to walk through today.

3
00:00:09,370 --> 00:00:12,760
I'm sorry. Love number four for homework.

4
00:00:12,780 --> 00:00:21,929
Number three, as the past homework will have to elapse covering all the materials for their homework today and next Wednesday.

5
00:00:21,930 --> 00:00:28,170
Okay. And just remember that these homework is due October 15.

6
00:00:29,010 --> 00:00:35,430
That's the sun. The sun of the 16th, I think is the 16th.

7
00:00:36,510 --> 00:00:41,580
Okay, that's fine. Oh, okay.

8
00:00:41,730 --> 00:00:46,290
So. A lot of examples we're going to start with.

9
00:00:46,370 --> 00:00:49,850
Plus, all models are inflated by some models.

10
00:00:50,270 --> 00:00:56,330
Then negative binomial model, negative binomial. We just blinds on inflated negative binomial models.

11
00:00:57,260 --> 00:01:00,379
Okay, so throw the whole lot.

12
00:01:00,380 --> 00:01:02,780
We're going to work with the same dataset.

13
00:01:03,920 --> 00:01:11,930
It's just safe to say that maybe this is not the best approach for that dataset, but is the only data that we have.

14
00:01:11,930 --> 00:01:22,489
So we are going to work with it. Okay. So the dataset involves chronic obstructive pulmonary disease patients who were retrospectively

15
00:01:22,490 --> 00:01:27,020
asked about the number of exacerbations they had experienced in the previous year.

16
00:01:27,500 --> 00:01:37,260
We have a lot of variables and covariates, just those age sex if they are currently smoking or not.

17
00:01:37,970 --> 00:01:48,400
But our main predictor of interest is something that is called Wolfie, which in 17 is variable that measures the thickness in the lungs, I think.

18
00:01:49,910 --> 00:02:00,740
Okay. So based on that, that's the dataset we are interested in predicting the number of exacerbations.

19
00:02:01,100 --> 00:02:04,180
That's the variable exact underscore account.

20
00:02:04,430 --> 00:02:11,239
And we have all of these predictors. And among all of those predictors, the one that we are particularly interested.

21
00:02:11,240 --> 00:02:14,570
It's one thing which is a continuous variable. Okay.

22
00:02:15,140 --> 00:02:25,760
So since the outcome, it's are positive integer meaning zero one, two, three, four until as many as you as much as you want.

23
00:02:27,440 --> 00:02:33,379
Probably the first approach we should explore to try to predict the variable.

24
00:02:33,380 --> 00:02:38,930
It's using a count model, which means either Poisson model or a negative binomial model.

25
00:02:40,330 --> 00:02:48,130
Okay. So we're going to start exploring what some else for this first example, we want to achieve three things.

26
00:02:48,520 --> 00:02:53,140
The first is just simply to feed our regular plus whole model for the data.

27
00:02:53,590 --> 00:02:57,280
The second one is to feed our series fleet of plus home models.

28
00:02:57,610 --> 00:03:02,679
And the third one is called where the student select which one the we think it's

29
00:03:02,680 --> 00:03:08,590
best for that data are be either the regular parcel or this inflated plus or minus.

30
00:03:09,070 --> 00:03:13,780
So to compare these two things, we are going to use something that is called a home test.

31
00:03:14,170 --> 00:03:20,260
That is this is particularly useful for comparing regular person with two inflated personas.

32
00:03:21,250 --> 00:03:25,720
Okay. So first of all, let's feed up what's on model.

33
00:03:26,020 --> 00:03:34,810
So in SAS I. Kind of forget the way you saw her are mainly in this class.

34
00:03:37,210 --> 00:03:40,300
Are you all right? Are you okay?

35
00:03:40,720 --> 00:03:49,570
All right. So what we can do is so focused on the users for the site's users.

36
00:03:49,580 --> 00:03:54,069
Um, Prop G are not the data is the data.

37
00:03:54,070 --> 00:03:58,820
So that we have these first statement, this class is Connor.

38
00:03:58,870 --> 00:04:02,980
A statement is just to serve the reference group in my categorical variable.

39
00:04:03,250 --> 00:04:07,720
In this case, I'm setting the reference group price, which is a categorical variable.

40
00:04:07,990 --> 00:04:11,410
So we things, okay, that's the only thing I'm doing.

41
00:04:11,830 --> 00:04:14,770
And then I'm just going to fit the model with the model statement.

42
00:04:14,770 --> 00:04:22,120
So model my outcome variable here and then to the right hand side, all of the parameters and the most important part,

43
00:04:22,270 --> 00:04:32,800
just backslash after all the parameters dist and then to feed our regular was a model just Poisson for so I'm there and then that's it.

44
00:04:33,520 --> 00:04:39,220
The output looks something like this. We have a big table that it's called analysis of maximum likelihood.

45
00:04:39,430 --> 00:04:42,460
We look with all of the parameters estimates.

46
00:04:42,760 --> 00:04:46,120
Just remember that this thing is giving you zero qualities.

47
00:04:46,120 --> 00:04:50,860
So for example, beta one is 0.5 to 3.

48
00:04:51,250 --> 00:04:59,980
However it seems worth since we are feeding plus our model and if you would like to make some conclusion about the mean,

49
00:05:00,280 --> 00:05:05,980
you would need to do e to the power of that. Correct, as we did in the logistic models.

50
00:05:06,040 --> 00:05:13,660
Just remember that this is a requirement that is, you still need to do it to the power of those things to actually make a conclusion about the mean.

51
00:05:14,710 --> 00:05:19,910
Okay. So we just feed a regular for some photos.

52
00:05:19,990 --> 00:05:23,500
Next, we want to feed a similar inflated Poisson model.

53
00:05:23,500 --> 00:05:28,260
And next, compare those so things. So.

54
00:05:31,630 --> 00:05:35,900
Okay. I guess you don't play it for someone else.

55
00:05:35,920 --> 00:05:41,440
We have to do three things. The first thing is to feed this translated for someone.

56
00:05:42,160 --> 00:05:48,360
As you know, whenever I am feeding series related something either negative binomial or what,

57
00:05:48,370 --> 00:05:52,449
tomorrow I'm feeding two models the in this case, the Poisson model.

58
00:05:52,450 --> 00:05:55,540
And then behind that, it's a logistic regression model. Right?

59
00:05:56,140 --> 00:06:03,480
So that's kind of what we are doing here. So the first thing we're going to do, every time you want to feed us, you inflated something in this case.

60
00:06:03,670 --> 00:06:14,200
So the first thing you're going to do is just feed the model. Then the second step is to look at the logistic regression, not the current model.

61
00:06:14,260 --> 00:06:24,670
Look at the logistic regression, select the significant variables, and then refit the model based on those significant variables.

62
00:06:24,910 --> 00:06:28,000
Okay. So it's like a kind of like a three step way.

63
00:06:29,230 --> 00:06:32,440
First, we feed the models, we feed the model.

64
00:06:32,590 --> 00:06:36,220
Then we look at the logistic regression outcome.

65
00:06:36,580 --> 00:06:42,150
We select the. Significant variables.

66
00:06:42,160 --> 00:06:46,200
I mean, we feed re feed the model based on those significant biases.

67
00:06:46,320 --> 00:06:49,950
Okay. So that's what we're going to do first.

68
00:06:50,790 --> 00:06:55,280
They are steering plate. It was all in general.

69
00:06:55,290 --> 00:07:02,580
It's exactly the same logic is that the only difference is that here in the distribution instead of fossil it's.

70
00:07:04,900 --> 00:07:13,190
The VIP sewer inflated for something, and that's basically under nothing more of a statement.

71
00:07:13,210 --> 00:07:18,400
After that, I have a zero model, the serial model, it's the logistic model behind it.

72
00:07:18,670 --> 00:07:21,940
So this moral statement is the force on moral.

73
00:07:22,330 --> 00:07:29,860
These zero moral is the logistic model that is behind it. And for the logistic model, I only need to at least.

74
00:07:30,850 --> 00:07:34,960
All of the predictors. I don't need to list the outcome. Okay. So look at this here.

75
00:07:35,020 --> 00:07:38,200
When I'm seeing the question, what do I have outcome equal to?

76
00:07:38,380 --> 00:07:42,370
And then all of the predictors here are what I'm seeing, the logistic regression.

77
00:07:42,370 --> 00:07:48,160
I only need to list the predictors, not the outcome. Okay, because the outcome is supposed to be the same for the two items.

78
00:07:49,990 --> 00:07:53,020
Okay. When I feed students something, if one.

79
00:07:53,030 --> 00:07:56,950
So I'm going to have got I'm going to have two big tables as a result.

80
00:07:57,370 --> 00:08:04,600
The first table is the count output, which is the Poisson, and then the second table is the logistic model.

81
00:08:05,110 --> 00:08:09,260
So in the first step, we only feel this zero inflated plus all.

82
00:08:10,100 --> 00:08:19,180
And in the second step, we are going to look at the logistic regression output, the logistic regression output in size.

83
00:08:19,330 --> 00:08:22,870
It's called analysis of maximum likelihood, zero inflation.

84
00:08:23,380 --> 00:08:28,270
Okay. So just make sure you are looking at the correct table.

85
00:08:28,330 --> 00:08:31,690
This is the count output. It doesn't says anything here.

86
00:08:32,020 --> 00:08:40,780
So if it doesn't says anything is the count output. If it says something like zero inflation, it means is the logistic regression behind.

87
00:08:41,800 --> 00:08:44,500
Okay. So now we look at this output.

88
00:08:45,010 --> 00:08:51,790
I look at all of the p values and then I select the p value and then I select the variables that are significance,

89
00:08:52,030 --> 00:08:58,110
meaning that I'm going to choose between all of these people use those that are less than 0.5.

90
00:08:58,120 --> 00:09:03,580
In this case, this is the only variables that is less than 2.5, meaning that for my logistic model,

91
00:09:03,820 --> 00:09:10,300
the only viable that it's a good predictor is this one f every one piece.

92
00:09:10,960 --> 00:09:20,140
Okay. So once I have like the, once I'm done choosing my variables from the logistic model, now I have to feed the model.

93
00:09:20,260 --> 00:09:25,510
So to refit the model is the same assignment as before.

94
00:09:25,870 --> 00:09:30,129
Notice that my Poisson model is exactly the same.

95
00:09:30,130 --> 00:09:33,520
I have my alpha and all of my predictors.

96
00:09:33,850 --> 00:09:41,620
All of them. However, for my Poisson model, I only have the parameter that it's significant.

97
00:09:42,520 --> 00:09:52,180
Okay. So when I chose the significant variables that changes the logistic regression model, that does not change the Poisson models behind.

98
00:09:53,390 --> 00:09:58,630
Does that make sense? Okay.

99
00:09:59,370 --> 00:10:06,629
And then here in this last line. The only thing I'm doing is just saving all of the outputs and doing this just

100
00:10:06,630 --> 00:10:10,700
because we want to compare the Poisson model with this inflated plus one.

101
00:10:11,880 --> 00:10:18,960
Okay. So once I'm done that, this will be my final series slated for tomorrow.

102
00:10:19,290 --> 00:10:27,450
And now I'm ready to compare those things. The regular bustle and the regular bustle and the.

103
00:10:33,590 --> 00:10:37,100
And the Syrian playbook was I think it was like OC OC.

104
00:10:37,730 --> 00:10:46,640
It's particularly for the ones for the people that are using source in the love of files.

105
00:10:46,910 --> 00:10:52,430
There is some macro that runs the warm desk, so make sure you like Dumbo.

106
00:10:52,430 --> 00:10:56,150
The dog thing said the correct spots here.

107
00:10:56,360 --> 00:11:02,290
So like you can read this thing. It's already uploaded, encompassing the love finds.

108
00:11:03,120 --> 00:11:10,880
Okay. And then to run the past, I just run them off the exact same code that I have here.

109
00:11:11,330 --> 00:11:15,470
The only thing you need to be careful about is these nine and seven.

110
00:11:15,860 --> 00:11:23,030
These minus seven correspond to the total number of number of parameters that I have estimated in each of the models.

111
00:11:23,450 --> 00:11:30,490
So for the first four, the model for the regular person would not be zero inflated for the regular person model.

112
00:11:30,950 --> 00:11:37,940
I have seven variables, so in total I need to see I have six variables plus Z and in total I have seven variables.

113
00:11:38,150 --> 00:11:46,590
So this is like the seven bundles here. And then for this, you want to play the person model, which is the other and what I have done.

114
00:11:46,920 --> 00:11:53,420
Two more new variables. Remember that I had one significant variable, plus the intercept of that model.

115
00:11:53,690 --> 00:11:58,160
So that's nine variables in total. Okay, so those are these numbers here.

116
00:11:59,690 --> 00:12:05,960
And then that's it. When you run that for the output, it's going to look something like this.

117
00:12:06,350 --> 00:12:10,700
They know the null hypothesis is models are equally close to the true model.

118
00:12:11,090 --> 00:12:17,510
On the alternative hypotheses is one of them. Model is closer or better to the real models for my data.

119
00:12:18,680 --> 00:12:25,009
We are going to look for the unadjusted statistic is going to give you the simplistic the p value.

120
00:12:25,010 --> 00:12:30,230
And then very importantly in the last column, it's also give you which model do we prefer?

121
00:12:30,500 --> 00:12:33,620
In this case, I prefer the super inflated question.

122
00:12:34,370 --> 00:12:39,350
Okay. Otherwise, it's going to be like poison blossom here in this case, Blossom.

123
00:12:40,760 --> 00:12:44,340
Any questions so far? Okay.

124
00:12:44,790 --> 00:12:55,859
That what if what we have done so far is first we fitted our regular bus model, then we fitted R0 inflated for sometime for the seating plan.

125
00:12:55,860 --> 00:12:59,190
It was a modern we have to do three things, feed them model,

126
00:12:59,610 --> 00:13:06,360
look at the logistic output of the model and select the significant variables and then refine the decisions made.

127
00:13:06,360 --> 00:13:12,750
It was all changing the predictors for the logistic regression to be only the significant variables.

128
00:13:13,110 --> 00:13:17,700
However, the Poisson model is still the same. We have all of the predictors, okay?

129
00:13:18,750 --> 00:13:24,060
And that's it. And then we run this test to compare those two models.

130
00:13:25,170 --> 00:13:28,320
Okay, now how do we do that?

131
00:13:28,410 --> 00:13:35,940
All of that are okay. So in our the first thing I want to create is a formula for my model.

132
00:13:35,940 --> 00:13:44,549
In this case, I have the outcome and then Delta and then all of my variables, all of my predictors and the feedback was to fit regular fossil model.

133
00:13:44,550 --> 00:13:54,180
I'm using the GM in function. Okay. So Jillian, here I have my formula, I have the data set and then I have to run a plus model.

134
00:13:54,180 --> 00:13:59,680
I have to specify the the formula in this deal and function is the plus off finally.

135
00:13:59,970 --> 00:14:04,550
Okay. So it's very similar as what we did last week.

136
00:14:04,560 --> 00:14:10,110
Last week we fitted our logistic regression. So here was a logic this week since we are creating a plus.

137
00:14:10,110 --> 00:14:14,280
So it follows the same logic. The only difference is on here and have a blossom.

138
00:14:14,590 --> 00:14:18,479
Okay, under here.

139
00:14:18,480 --> 00:14:29,370
I'm just creating the table that you guys see here. Okay, exactly as it happens in size, these estimates are the role estimates.

140
00:14:29,640 --> 00:14:36,629
So to make any conclusion about any, you still need to do do you need to do e to the power of those estimates?

141
00:14:36,630 --> 00:14:41,240
Okay. So make sure you do that in case that's me.

142
00:14:42,830 --> 00:14:47,780
Any questions so far? Okay.

143
00:14:49,260 --> 00:14:53,159
That's how we're for someone else. Now let's feed us inflated.

144
00:14:53,160 --> 00:14:57,040
What's our model for that?

145
00:14:57,090 --> 00:15:06,700
I'm going to make this library. So make sure first you install the package B, s, c, l and then call it into the script that you're working out.

146
00:15:07,500 --> 00:15:14,790
So the first thing is that we are going to use a library. Second thing, I'm going to change my formula a little bit.

147
00:15:15,240 --> 00:15:19,290
So here I the formula remains the same for the most part.

148
00:15:19,290 --> 00:15:30,180
I have my output against all of my predictors exactly as I have before and after my last predictor when I have this bar, which is like a vertical bar.

149
00:15:30,690 --> 00:15:36,390
And then after that, he is going to list all of the predictors for the logistic model.

150
00:15:36,840 --> 00:15:45,989
So the way this formula works is something like this first part corresponds to the Poisson.

151
00:15:45,990 --> 00:15:51,660
What else? So whatever is to the left of that vertical bar is the Poisson model,

152
00:15:53,400 --> 00:15:59,640
and then whatever is right to the vertical bar corresponds to the logistic model.

153
00:15:59,700 --> 00:16:07,930
So this bar right here. Corresponds to the predictors I'm going to use for the logistic model.

154
00:16:08,200 --> 00:16:16,540
Okay. And then I also notice that here I'm having a feed this year instead of fossil.

155
00:16:16,930 --> 00:16:21,880
For that, I'm going to use our function that is within that forecast is that it's called sero in.

156
00:16:22,810 --> 00:16:28,410
I and l is going to give me is going to ask me for this formula right here.

157
00:16:29,350 --> 00:16:35,040
Specify the distribution in this case is support. So you check, I'm going to specify the dataset.

158
00:16:35,440 --> 00:16:45,730
Okay. It's very important that we don't use digital information because unfortunately, digital in function does not feed any series of models.

159
00:16:45,970 --> 00:16:52,600
So you need us to make a model. We need to use a different function that it's within that VCO package.

160
00:16:52,690 --> 00:16:59,650
Okay. So make sure you install this package first and then use this inflated formula.

161
00:17:00,670 --> 00:17:07,930
Okay? And then here in this last two lines in this line, I'm just going to print, I'm printing the result.

162
00:17:08,290 --> 00:17:16,749
I mean, the second line, what I'm doing is e to the power of the estimates again, because remember that these estimates are below estimates.

163
00:17:16,750 --> 00:17:21,430
We need to explain see those things to make some conclusion about them.

164
00:17:23,830 --> 00:17:27,190
Okay, so same logic as in source.

165
00:17:27,670 --> 00:17:30,760
The first step is to feed a serum plated model.

166
00:17:31,120 --> 00:17:40,690
Then I look at the logistic output of that you inside inflated model and then I refit the model based on the significant variables.

167
00:17:40,960 --> 00:17:47,700
So. The output of that of this function, if you like, run this.

168
00:17:48,270 --> 00:17:56,790
The output of this function is going to be two tables. The first table is the column table, which will be the estimates for the queso model.

169
00:17:57,180 --> 00:18:01,320
And then the second table, it's going to be something that is close to zero.

170
00:18:01,830 --> 00:18:06,360
This will tell you that is the output of the logistic regression.

171
00:18:06,360 --> 00:18:14,760
Okay, so this thing. This is the output of the logistic regression.

172
00:18:17,820 --> 00:18:25,020
I know from my logistic regression I select the variables that are significant,

173
00:18:25,350 --> 00:18:33,150
meaning that I'm looking through all of these P values on selected ones that are less than 0.05.

174
00:18:33,540 --> 00:18:38,430
In this case, this is the only variable dice. 0.0 less than 2.05.

175
00:18:38,700 --> 00:18:43,110
Okay. Ah. Normally gives us.

176
00:18:46,770 --> 00:18:54,300
Scientific notation. Just remember that, for example, this number here, it's 0.31.

177
00:18:54,930 --> 00:19:01,380
Okay. That's the scientific notation of these number and these number, for example, right here.

178
00:19:03,030 --> 00:19:06,420
Is 0.547.

179
00:19:07,680 --> 00:19:12,120
And then I'm comparing all of these numbers against my alpha value.

180
00:19:12,990 --> 00:19:17,270
That is 0.05.

181
00:19:18,450 --> 00:19:21,540
The only one that is less than 0.05 is this one.

182
00:19:22,230 --> 00:19:29,730
So that's my only significant value. Now what I'm going to do is re feed that zero inflated model.

183
00:19:29,970 --> 00:19:34,080
So notice that the formula model is the same.

184
00:19:34,920 --> 00:19:43,680
Remember that everything that goes before it to the left of that vertical bar stays exactly the same because that's the Poisson model.

185
00:19:44,100 --> 00:19:54,209
And then to the right of that vertical line, I only have my significant variables, which in this case is only one, and then I do the same process.

186
00:19:54,210 --> 00:19:57,690
I changed my formula and received my model and look at the.

187
00:19:59,960 --> 00:20:03,840
All right. Any questions so far?

188
00:20:06,280 --> 00:20:17,650
Oh, good. Lastly, I want to compare my tomatoes so we can use directly the viewing function.

189
00:20:20,150 --> 00:20:25,610
And that if I use that regularly for information, I pass my two models.

190
00:20:26,030 --> 00:20:41,670
So for example here these first argument that I have here feed that so you inflated the voice off is just my pseudo inflated voice on my final sear.

191
00:20:41,720 --> 00:20:48,530
Inflated possum. Okay. And then the second argument that I have here is just a regular plus or minus, not this.

192
00:20:48,530 --> 00:20:53,790
You inflated this regular possum, and then I just put those two arguments.

193
00:20:53,810 --> 00:21:01,540
It can be in any order. And I have exactly the same table as me, as we have, as we got, as we have before.

194
00:21:02,660 --> 00:21:08,540
I am going to look at the row here, the row row.

195
00:21:09,530 --> 00:21:12,890
I have the test, the statistics, and I have the p value.

196
00:21:13,280 --> 00:21:20,060
Okay. I'm here very importantly is telling you in ah, it's telling you which model you prefer.

197
00:21:20,420 --> 00:21:25,400
So in this case he's telling me model one, it's better than model two because it's greater.

198
00:21:25,760 --> 00:21:30,200
It's more than one. It's better than model two. And which one is what I want?

199
00:21:30,560 --> 00:21:33,620
The first one that you pass as performance or so in this case.

200
00:21:33,680 --> 00:21:41,360
Seems my first parameter is the C related plus all it means that R is telling me that the zero three plus is better.

201
00:21:44,260 --> 00:21:48,940
Okay. Questions so far. Great.

202
00:21:50,800 --> 00:21:55,300
Okay. That's everything I had to do for someone else.

203
00:21:55,320 --> 00:21:58,950
Now, let's look at negative binomial models.

204
00:21:59,280 --> 00:22:00,899
So for negative binomial models,

205
00:22:00,900 --> 00:22:12,570
we're going to do three things are normal without anything fancy negative binomial and negative binomial with s lines.

206
00:22:12,930 --> 00:22:18,840
And then I see you inflated negative binomial. Okay. So first, let's just go for the.

207
00:22:20,420 --> 00:22:23,690
Regular nothing fancy negative binomial.

208
00:22:24,080 --> 00:22:33,740
So what we want to do is just to create a negative binomial model on that outcome and look at our primary predictor of interest.

209
00:22:34,190 --> 00:22:38,510
And then the second thing we're going to do is that I would like to fill this table.

210
00:22:38,960 --> 00:22:51,200
What is this table? I want to be able to feed the estimate of E to the power of 0.25 and then the beta for one Cygnus.

211
00:22:51,560 --> 00:22:54,440
And then I also want the confidence interval for this thing.

212
00:22:54,950 --> 00:23:03,470
Notice that in general, this is not a trivial question because whenever I feed a model either in R or in source,

213
00:23:03,830 --> 00:23:10,370
my output is always telling me theta hot is not telling me E to the power of

214
00:23:10,370 --> 00:23:17,540
beta hot or more importantly is not telling me e to a power of 0.25 beta hot.

215
00:23:17,930 --> 00:23:20,240
So in general is not a trivial question.

216
00:23:20,480 --> 00:23:28,460
We want to be able to go from this, which is the output, like the regular output of either source or R to something like this.

217
00:23:28,760 --> 00:23:32,150
Okay. And then the confidence that all of this.

218
00:23:33,560 --> 00:23:38,960
Okay. So source code for negative binomial models.

219
00:23:39,650 --> 00:23:44,540
It looks very, very similar. So we are going to divide this code into two parts.

220
00:23:51,440 --> 00:24:02,900
Oops. The part in red. And then the rest. Okay, so the Espada right here, the upper part looks very similar to what's water?

221
00:24:03,020 --> 00:24:09,979
It's exactly the same, actually. The only difference is that here, when I specify the distribution, just change.

222
00:24:09,980 --> 00:24:13,700
Plus all four in the which is that's when I think, why not you? And that's it.

223
00:24:14,120 --> 00:24:23,390
Just feed it on negative binomial model. I can tell that this is just negative binomial, not c later than negative binomial.

224
00:24:23,780 --> 00:24:31,570
Because if I were to feed a series of negative binomial after my model statement, I would have something like zero one.

225
00:24:32,330 --> 00:24:36,680
So since I don't have that, I just know that I'm feeling negative by now.

226
00:24:37,190 --> 00:24:42,220
So that's the only thing you need to change. Now we are interested.

227
00:24:42,470 --> 00:24:50,270
The very important question here. It's e to the power of 0.25 oops beta.

228
00:24:52,370 --> 00:25:03,800
Beta one thickness. So to be able to estimate those kind of things we are going to use in size, the estimate is state.

229
00:25:04,520 --> 00:25:14,540
So the estimate statement is going to ask me for every parameter in my model to set a number so I know that my model looks something.

230
00:25:14,540 --> 00:25:17,960
Oops, sorry. I know that my model.

231
00:25:20,400 --> 00:25:26,900
Then they just erase this. I know that my model looks something like this.

232
00:25:28,610 --> 00:25:36,740
So I have my output, not the logarithm of my output.

233
00:25:38,200 --> 00:25:44,919
And then here I have my intercept at zero, and then I have Beda.

234
00:25:44,920 --> 00:25:48,940
Well, take this time. You know, we'll take this.

235
00:25:49,510 --> 00:25:51,249
And then I have my other covariates.

236
00:25:51,250 --> 00:26:02,770
So, for example, I have beta h times h, I think I also have like, I don't know, beta six times six, so on and so forth.

237
00:26:03,400 --> 00:26:14,440
Okay. So I want to be able to estimate E to the power of 0.25 beta well thickness.

238
00:26:14,830 --> 00:26:18,340
So we are going to divide this question into two parts.

239
00:26:19,090 --> 00:26:29,140
First, let's just figured out how to do 0.25 times beta, one thickness and then e to a part of that thing.

240
00:26:29,620 --> 00:26:36,700
So what we're going to do to have this 0.25 beta wall thickness is that I'm going to assign to

241
00:26:36,940 --> 00:26:43,270
every beta that I have here and number to make sure I have this same equation that I have here.

242
00:26:43,570 --> 00:26:48,520
So this equation does not involve the intercept. So I'm going to set the intercept to zero.

243
00:26:49,270 --> 00:26:55,630
It involves beta one thickness, but it's multiplied by 0.25.

244
00:26:55,900 --> 00:27:01,059
So this is going to be like a 0.25. It does not involve H.

245
00:27:01,060 --> 00:27:05,620
So this term it needs to be set to zero does not involve sex.

246
00:27:05,620 --> 00:27:09,430
So this don't need to be set to zero, so on and so forth.

247
00:27:09,580 --> 00:27:12,580
Does that make sense? And then that's what I'm doing here.

248
00:27:13,150 --> 00:27:18,520
So for each variable, I'm just setting the value that I need for quality.

249
00:27:18,520 --> 00:27:29,350
And this is 0.25. And for all of my other covariates, it's set to zero age male and my other covariates, including The Intercept.

250
00:27:29,770 --> 00:27:34,059
Make sure you don't forget the intercept, because if you don't set this to zero,

251
00:27:34,060 --> 00:27:40,240
that is going to give you the estimate of bias zero plus 0.25 beta opaqueness.

252
00:27:40,480 --> 00:27:45,380
So make sure you set the intercept to zero and then the exponential,

253
00:27:45,400 --> 00:27:53,950
the thing in size under the backslash e xp which stands for exponents and that's it.

254
00:27:54,160 --> 00:27:59,340
That's home when I get the results. Does that make sense? A little bit.

255
00:28:00,480 --> 00:28:06,300
Okay. The output will look something like this.

256
00:28:06,750 --> 00:28:12,840
So first of all, I have my huge table for my negative binomial model with my row estimates.

257
00:28:14,580 --> 00:28:20,760
And then more importantly, I have one more table that is called the construction trust estimate.

258
00:28:20,970 --> 00:28:27,360
That is going to give me the estimate of that thing that I sit there for each statement.

259
00:28:27,750 --> 00:28:36,330
I'm going to have two rows. The firm, the first one, it's for only 0.25 quality.

260
00:28:36,750 --> 00:28:41,040
And the second one is for each part of that thing, and it's always like that.

261
00:28:41,430 --> 00:28:50,340
So this is just the first one. The first row here is just 0.25, four times beta will pick units.

262
00:28:51,000 --> 00:28:54,860
And then the second one, it's e to the power of that thing.

263
00:28:55,350 --> 00:29:01,020
Okay. So in our case, we're interested in the second one.

264
00:29:01,920 --> 00:29:08,520
And then here is going to give you the estimate of that thing and the confidence interval of that thing,

265
00:29:09,060 --> 00:29:17,370
meaning that e to the power of 0.25 beta wall thickness.

266
00:29:18,210 --> 00:29:26,070
If I try to estimate this thing, the estimate of this thing is 1.18 49.

267
00:29:28,170 --> 00:29:32,490
Okay? And the confidence interval of this thing, it's this.

268
00:29:33,830 --> 00:29:39,880
Okay. Based on the confidence interval.

269
00:29:42,950 --> 00:29:52,270
If I were to test the hypothesis. E to the power of 0.25 beta.

270
00:29:53,230 --> 00:30:01,000
It's equal to one on the alternative hypothesis being the same thing but different than one.

271
00:30:01,630 --> 00:30:09,190
So if I were to test that, no, I'm alternative hypothesis based on the output that you have here.

272
00:30:09,850 --> 00:30:13,300
Can I reject or do I fail to reject?

273
00:30:18,600 --> 00:30:22,580
Just. Yes. I reject the know why.

274
00:30:22,920 --> 00:30:26,430
Remember, the confidence interval can also tell you those things.

275
00:30:26,730 --> 00:30:30,960
So I have the estimate on the confidence interval for this thing.

276
00:30:31,260 --> 00:30:37,890
I look at the confidence interval. The confidence interval goes from 1.0 something to 1.3.

277
00:30:38,220 --> 00:30:44,280
These does not include one, which is the value to the right hand side of my hypothesis.

278
00:30:44,610 --> 00:30:47,690
Meaning that I reject the hypothesis.

279
00:30:47,700 --> 00:31:06,570
Okay. Makes sense. But remember that you can also like reject or not reject, or you have for many different hypotheses.

280
00:31:07,170 --> 00:31:10,680
You can also use the while how you square on the while you're here.

281
00:31:11,040 --> 00:31:18,930
So this wall chi square and the people you are testing, whether these individual betas are equal or not to zero.

282
00:31:20,380 --> 00:31:25,180
Okay. Now, that's the negative binomial and that's our table.

283
00:31:25,360 --> 00:31:29,380
How do we do that? Are very similar.

284
00:31:30,520 --> 00:31:33,700
First, I'm going to need these two packages. I'm sorry.

285
00:31:33,700 --> 00:31:40,630
This one package. I'm going to need this one package and these.

286
00:31:40,740 --> 00:31:44,100
It's not for me. Oh, yeah. And this is for.

287
00:31:44,370 --> 00:31:49,440
This is to be able to feed the negative binomial.

288
00:31:49,710 --> 00:31:56,520
So once I installed this package, I go to my formula, which is the formula we have been working on.

289
00:31:56,820 --> 00:32:06,120
So my output on my outcome on all of my covariance and then st negative binomial and when I use the function

290
00:32:06,610 --> 00:32:16,170
them the end which is funds for down the MRL for and finally I pass my formula on my data set and that's it.

291
00:32:16,830 --> 00:32:21,450
Okay. And then same deal as before.

292
00:32:21,450 --> 00:32:30,809
We have the estimates and then here I use I'm using X of the same just the exponential those estimates because they are in the raw scale,

293
00:32:30,810 --> 00:32:35,010
not in the E to the power level of large scale. Okay.

294
00:32:36,390 --> 00:32:41,010
So that's reading. Then I got the binomial. That's should be pretty doable.

295
00:32:41,350 --> 00:32:48,750
The very interesting, interesting question here is how do we said these e to the power of pseudo point 25 thing.

296
00:32:49,320 --> 00:32:50,700
So what we are doing,

297
00:32:50,730 --> 00:33:00,150
what we are going to do is to create a constructs contrast matrix which follows basically the same logic that we already discussed in SAS.

298
00:33:00,540 --> 00:33:11,280
So the first thing is install this package. Within this package there is a function based called general linear hypothesis,

299
00:33:11,430 --> 00:33:17,100
hypothesis, hypothesis, testing, g, LHC general linear hypothesis testing.

300
00:33:17,490 --> 00:33:21,930
Okay. So the first thing is what I'm going to do is exactly as I did in SAS.

301
00:33:22,290 --> 00:33:25,709
So SAS was asking me for each of the parameters.

302
00:33:25,710 --> 00:33:32,280
Give me a number. I'm doing exactly the same here. So these first value ups.

303
00:33:34,810 --> 00:33:38,110
So this first volume corresponds to beta zero.

304
00:33:38,440 --> 00:33:43,270
Then this is beta one, then this is beta two and so on and so forth.

305
00:33:43,420 --> 00:33:56,979
Okay, so given that the order of my variable here is that I have my output and then beta one is mostly I set that intercept plus zero.

306
00:33:56,980 --> 00:34:02,379
Remember always to set the intercept to zero and then 0.25 beta one because

307
00:34:02,380 --> 00:34:06,310
that's the first variable that I have in my formula and so on and so forth.

308
00:34:06,580 --> 00:34:11,040
All of the other betas are set to zero. Does that make sense?

309
00:34:17,160 --> 00:34:20,520
It's better to set a moose jaw when you're looking at people.

310
00:34:21,000 --> 00:34:24,390
Yes, because we are working with the same equation.

311
00:34:24,660 --> 00:34:37,950
I want I estimate this thing. But for example, if I wanted to estimate e to the power of four times vader h.

312
00:34:39,520 --> 00:34:45,370
What I would need to do is instead of having a 0.25 here, I would have.

313
00:34:46,320 --> 00:34:52,320
I'm just going to do it in another color. So it's clear I would have a zero here.

314
00:34:52,950 --> 00:34:57,540
My age variable. It's my second estimate.

315
00:34:58,260 --> 00:35:04,740
So I intercept. And then two. After that, these, instead of being a zero, would be a four.

316
00:35:05,310 --> 00:35:11,010
Does that make sense? Yes. Andro equals one or two of them.

317
00:35:11,400 --> 00:35:22,260
Oh, okay. So. Basically it's just because I want I only have one hypothesis.

318
00:35:22,590 --> 00:35:30,420
So in this case, my hypothesis is that although I want to test whether all of this is equal to one or not, but I can have multiple hypotheses.

319
00:35:30,540 --> 00:35:35,010
We are going to see that later. In which case I can make changes.

320
00:35:35,430 --> 00:35:38,910
But just because I have one hypothesis, that's all that admits.

321
00:35:39,180 --> 00:35:42,960
I have one hypothesis. Okay. So.

322
00:35:51,530 --> 00:36:01,550
So I set all of my data to my desire value, and then I used generally general linear hypothesis testing.

323
00:36:01,850 --> 00:36:10,310
I pass my model in this case is the negative binomial model that we already see under this contrast equation that I have here.

324
00:36:10,730 --> 00:36:22,200
So I plotted here. And then if I were to do it only I'm still here only until these line.

325
00:36:22,500 --> 00:36:26,760
So everything above these line, everything above this line.

326
00:36:27,840 --> 00:36:36,930
It's only telling me, see your point 25 times better while fitness however I want to the power off.

327
00:36:37,200 --> 00:36:40,890
So what I'm doing here it's exponential shading those things.

328
00:36:41,250 --> 00:36:49,290
Okay so we just e to the power off everything above this line is only 0.25 times veto on Beowulf

329
00:36:49,290 --> 00:36:57,300
thickness on that I exponential over those results to get into a power off does that make sense okay.

330
00:37:00,520 --> 00:37:07,420
So once I run this code, my output is going to look something like this.

331
00:37:07,720 --> 00:37:16,780
So here I have my estimate, the estimate of this whole thing, it's 1.18 and the 95% confidence interval.

332
00:37:18,010 --> 00:37:21,430
Okay. Just a note.

333
00:37:21,550 --> 00:37:30,010
The confidence interval, a little difference between source on art and that just may be because how R is coded,

334
00:37:30,010 --> 00:37:34,700
I actually think that source may have the correct results, but I don't know.

335
00:37:34,720 --> 00:37:38,530
And this is just speculating. This is just me. But they are very similar.

336
00:37:38,530 --> 00:37:44,170
I mean, I think they are always the same until before the maximum place, which is like they are very, very similar.

337
00:37:45,130 --> 00:37:49,420
Okay. So now what those all of these things means.

338
00:37:49,630 --> 00:37:53,800
So I feel feels my table. This is the estimate and this is the confidence.

339
00:37:54,580 --> 00:38:01,870
So how do I how do I interpret this thing? So for every 0.1 thickness, it's measured in millimeters.

340
00:38:02,140 --> 00:38:06,550
So for every 0.25 because of the number I have here.

341
00:38:06,790 --> 00:38:20,529
Mm. Increase in my variable which is one thing this but all of my patients are expected to have and then 1.18 times more outcome and my

342
00:38:20,530 --> 00:38:31,389
outcome is times more reported exacerbations in the past year conditioning or adjusting for the other variables adjusted for age,

343
00:38:31,390 --> 00:38:34,930
adjusted for sex, for smoking and stop loss, and so on and so forth.

344
00:38:37,020 --> 00:38:40,320
Any questions about how to interpret these numbers?

345
00:38:43,890 --> 00:38:49,150
Of. So more.

346
00:38:52,680 --> 00:39:01,100
Let's say a more appropriate way to interpret these results is to include whether those results are significant or not.

347
00:39:01,110 --> 00:39:05,460
And we already did that either with the wall test or with the confidence interval.

348
00:39:05,470 --> 00:39:10,970
So that's what I have here. I included either the wolf test or the confidence interval.

349
00:39:11,010 --> 00:39:16,650
Both are perfectly fine. And then based on that, I have the same sentence.

350
00:39:16,980 --> 00:39:22,620
So for every 0.25 millimeter increase in my wall thickness variables,

351
00:39:22,620 --> 00:39:32,430
I would expect my patients to have an increase in 1.18 times more exacerbations adjusted for the other coverage.

352
00:39:33,840 --> 00:39:40,770
Okay, now that's all about negative binomial.

353
00:39:41,160 --> 00:39:46,690
Now let's go for I'm negative while I'm here with I'm flying per.

354
00:39:46,780 --> 00:39:50,670
I'm doing what each side is flying.

355
00:39:52,980 --> 00:40:01,460
That's all it. You probably should ask, do you think that I'm going to try to explain it as I understand it?

356
00:40:02,240 --> 00:40:10,370
I'm not going to use a negative binomial. I'm going to use simple linear regression example, but it should work in the same way, in the same logic.

357
00:40:10,370 --> 00:40:15,050
But we don't think of this binomial. So let's save.

358
00:40:20,720 --> 00:40:28,670
This is X, for example, this is wall thickness and this is my outcome variable y.

359
00:40:29,180 --> 00:40:34,910
Okay. And let's say I changed its.

360
00:40:40,060 --> 00:40:51,940
And then I just. Okay.

361
00:40:52,800 --> 00:41:00,110
And these are all of my patients. So. Let's say this is a very simple example.

362
00:41:00,560 --> 00:41:09,590
Here I have my outcome. Here I have one variable, for example, wall thickness, and I have a current lot of all of my patients.

363
00:41:10,400 --> 00:41:15,290
This, for example, is the wall thickness, and this, for example, is the outcome, the value of the outcome.

364
00:41:16,520 --> 00:41:19,940
If I were to. If I were to.

365
00:41:21,920 --> 00:41:25,990
Estimate a linear model for this behavior.

366
00:41:26,090 --> 00:41:34,040
Behavior. Let's say if I were to estimate just a simple linear regression model my estimated life,

367
00:41:34,520 --> 00:41:41,330
it will do something like either this or maybe something like either these or even something like this.

368
00:41:42,050 --> 00:41:49,190
However, like if I if I were to estimate a simple linear regression model, even like these type of data,

369
00:41:49,670 --> 00:41:58,490
it will look something like either these or maybe either these or maybe even like something more or less, more or less, more or less constant.

370
00:41:58,880 --> 00:42:05,570
However, something that may feed the data a little bit better would should be something like this.

371
00:42:07,230 --> 00:42:11,490
So these line and then these other line.

372
00:42:15,010 --> 00:42:20,110
This will feed the reader the data even better. Instead of having just one estimate.

373
00:42:20,140 --> 00:42:29,220
So for example, one line that goes like this or just one of those like these, or maybe these mine are going to have something even better.

374
00:42:29,230 --> 00:42:38,470
A combination of those two things. One that goes like based on one button that goes like this, that it's less plain and it's less plain.

375
00:42:38,980 --> 00:42:44,750
Oops. In the. This is Carla's plane.

376
00:42:44,750 --> 00:42:49,970
And then these bodies, you were like that line shifts and changing to another thing.

377
00:42:50,420 --> 00:42:53,960
It's called The Knot. In this case, for example, there's not.

378
00:42:54,440 --> 00:42:57,440
It's 1.4. Let's say here.

379
00:42:57,850 --> 00:43:05,810
As you may see, what we want to do is to feed a fly, seen two knots in 1.4 and 1.7.

380
00:43:06,050 --> 00:43:10,760
Just to simplify the example idea, just read one. So this will be my 1.4.

381
00:43:11,120 --> 00:43:14,720
So I want to be able to do this kind of things. This is called flying.

382
00:43:15,080 --> 00:43:19,969
Those all make sense? A little bit. You should ask my family tomorrow involved.

383
00:43:19,970 --> 00:43:23,030
Something like that. Okay. So that's what we want to do.

384
00:43:25,340 --> 00:43:28,460
So in order to fit this, the first thing I'm going to do.

385
00:43:28,470 --> 00:43:40,190
Okay. Oh, and also before that, notice that here what I'm going to do is I'm going to divide my predictor, my eggs variable into two points.

386
00:43:41,260 --> 00:43:45,600
Right before my lot and that's right after my not okay.

387
00:43:46,000 --> 00:43:51,280
But notice that I'm dividing the predictor. I'm I'm not doing anything with my outcome.

388
00:43:51,280 --> 00:43:53,230
My outcome is the same. I'm not even touching it.

389
00:43:53,530 --> 00:44:02,800
I'm just going to divide my spread into two parts, this little line, which will be like right before my node, which is 1.4 and right after my not.

390
00:44:04,170 --> 00:44:12,990
Okay. So the first thing I'm going to do is that I'm going to divide my outcome into two parts.

391
00:44:15,300 --> 00:44:18,300
Two parts. I'm three parts. Exactly.

392
00:44:18,310 --> 00:44:24,570
Just because here in the example, we would like to have two west planks, two nuts in 1.4 and 1.7.

393
00:44:24,990 --> 00:44:28,110
So the first thing I'm going to do is to create two new environments.

394
00:44:28,410 --> 00:44:34,560
One for the first is fly in 1.4 and then one for the other is nine is in 1.7.

395
00:44:35,160 --> 00:44:44,010
So what I'm doing here is I have my data set that looks something like this while deaconess age sex and many other variables.

396
00:44:44,310 --> 00:44:48,870
And I'm going to add two new columns to that variable to that data set, I'm sorry.

397
00:44:49,260 --> 00:44:55,710
One for the first is flying in 1.4 and then the other one is flying in 1.7.

398
00:44:59,620 --> 00:45:03,970
Okay. So how am I creating these tonia variables?

399
00:45:04,300 --> 00:45:15,520
I'm going to look at my wall thickness variable. So let's say, for example, one individual here has 0.9 and then have 1.5.

400
00:45:16,240 --> 00:45:24,580
And then here. Then have 1.8. Okay.

401
00:45:25,300 --> 00:45:30,280
Let's say my kind of my health data look something like this.

402
00:45:30,490 --> 00:45:35,080
So for the first thing to be, well, my blood thickness variable, it's 0.94.

403
00:45:35,080 --> 00:45:38,230
The second one is 1.5 and for the third one is 1.8.

404
00:45:38,500 --> 00:45:42,070
So the first is blind, it's in 1.4.

405
00:45:42,670 --> 00:45:47,889
So the logic is like this is the variable in this blind.

406
00:45:47,890 --> 00:45:54,550
In this case, one thing, this is if one penis is less than 1.4, then I sign a zero.

407
00:45:55,000 --> 00:46:00,670
If it is greater than 1.4, I find the difference between the real value and 1.4.

408
00:46:01,120 --> 00:46:04,240
Okay. That's the logic. Always without being explained.

409
00:46:04,900 --> 00:46:10,600
If my variable is one thing, this is less than my not 1.4 a sign as zero.

410
00:46:11,260 --> 00:46:17,080
If it is greater than that, I find the difference between the real value and 1.4.

411
00:46:17,440 --> 00:46:24,670
So for example, in my case, this first variable would be oh.

412
00:46:32,170 --> 00:46:36,610
All right. So, for example, for the first subject, they won't be necessary.

413
00:46:36,660 --> 00:46:40,210
A point that 0.9 is less than 1.4. Yes.

414
00:46:40,570 --> 00:46:46,260
So my first variable for him should be zero for the second subject.

415
00:46:46,300 --> 00:46:50,140
My whole thing, this is 1.5 that that's greater than 1.4.

416
00:46:50,410 --> 00:46:54,490
So I should assign the difference in this case, this will be 0.1.

417
00:46:55,730 --> 00:46:59,410
These will be 0.4. Does that make sense?

418
00:47:01,570 --> 00:47:08,080
And then repeat the algorithm for all of this points.

419
00:47:08,170 --> 00:47:14,830
So for the second explosion, which is in 1.7, 0.9 is less than 1.7.

420
00:47:14,860 --> 00:47:20,600
Yes. So assign a zero. 1.5 is less than 1.7.

421
00:47:20,620 --> 00:47:26,229
Yes. So assign us zero. 1.8 is bigger than 1.7.

422
00:47:26,230 --> 00:47:29,650
So assign the difference in this case, 0.1.

423
00:47:31,300 --> 00:47:34,510
Does that make sense? And that's how we are creating this plant.

424
00:47:35,620 --> 00:47:40,580
But it's based in source. So this is the name for my new variable here.

425
00:47:40,600 --> 00:47:45,370
I'm asking whether or not my wall thickness is greater than 1.4.

426
00:47:45,670 --> 00:47:50,080
And if it is assigned a difference and if is not, is going to assign a zero.

427
00:47:50,260 --> 00:47:57,910
Same case here. Wall thickness is greater than 1.7 if it is assigned to the difference, if not assigned as.

428
00:47:59,080 --> 00:48:02,170
Does that make sense? A little bit. Okay.

429
00:48:02,800 --> 00:48:07,600
And then to run a regression, we displays what we know.

430
00:48:07,600 --> 00:48:14,710
It's okay. This looks like a lot. But I promise you, is not that much.

431
00:48:15,550 --> 00:48:22,430
So. Okay.

432
00:48:23,480 --> 00:48:35,540
So before my everything before my yellow line, it was just I'm feeding a negative binomial we just lines so is the same code that we review before.

433
00:48:35,660 --> 00:48:46,550
Notice that the distribution here is N.B. I have my model on all of my variables, and then to free this line, I only need to add my new variables.

434
00:48:46,760 --> 00:48:55,820
So notice that now I have more variables. That's it. To feed this line, I create the new variables on facet as predictors for my model, and that's it.

435
00:48:56,630 --> 00:49:01,550
Does that make sense? Oh, and then what I have here.

436
00:49:02,570 --> 00:49:06,420
So for the other part of.

437
00:49:10,100 --> 00:49:15,170
So what I have here is the same estimates that we were looking at before.

438
00:49:15,710 --> 00:49:25,340
So remember that we are interested in doing e to the power of 0.25 times made our world famous.

439
00:49:26,150 --> 00:49:35,470
However, we just divided wall thickness into three parts before 1.4 between 1.4 and 1.7 and after 1.7.

440
00:49:35,840 --> 00:49:41,450
So you have two. For each part of my division, I have to create an estimate.

441
00:49:41,810 --> 00:49:47,030
Okay. So I want that for when Vader for when world thickness is less than 1.4.

442
00:49:47,360 --> 00:49:51,349
I want that for when what thickness is between 1.4 and 137.

443
00:49:51,350 --> 00:49:58,490
And the same thing for when it's greater than 1.7. So the first one, it's when it's less than 1.4.

444
00:49:58,820 --> 00:50:03,500
So this will be 0.25. And then my thoughts lines are equal to zero.

445
00:50:03,500 --> 00:50:08,900
Because remember that the way we did it is if it is less than the value, not give us zero.

446
00:50:08,960 --> 00:50:13,370
So I have a zero here for between 1.4 and 1.7.

447
00:50:13,820 --> 00:50:20,900
So one thing is issue of point 25 and then the first line is 0.25 because it's supposed to take value there, right?

448
00:50:20,930 --> 00:50:27,500
That's how we created this line. But that second line is equal to zero, because I want to be between 1.4 and 1.7.

449
00:50:28,070 --> 00:50:34,490
And then for the third is why? Same logic. I have the one thing this than the first this line and then the second line.

450
00:50:35,450 --> 00:50:38,570
Does that make sense? Hi, Don.

451
00:50:40,090 --> 00:50:47,530
Maybe. Okay. All right.

452
00:50:48,100 --> 00:50:53,079
And then the outcome looks very familiar. This is a bigger table.

453
00:50:53,080 --> 00:50:58,690
I just trim it because, you know, I only have much space here and it is light.

454
00:50:58,930 --> 00:51:06,070
But we have the table read our estimates, and more importantly, we have our contrast estimate results.

455
00:51:06,340 --> 00:51:09,010
We will have one for each of these statements.

456
00:51:09,400 --> 00:51:18,550
So these, for example, is the estimate for when one thing is less than 1.4, between 1.4 and 137 on after 1.7.

457
00:51:20,360 --> 00:51:27,429
Okay, let's just review how do we do it in our logic is exactly the same.

458
00:51:27,430 --> 00:51:32,380
I need to create the new two new variables and then run the model with the two new variables.

459
00:51:32,920 --> 00:51:37,660
So that's what I'm doing here. I'm creating the two new variables with the same logic.

460
00:51:38,320 --> 00:51:41,860
Each one thickness is greater than 1.4.

461
00:51:42,370 --> 00:51:51,220
Assign the difference. Okay, so here assign the difference the real value -1.4 and the same for 1.7.

462
00:51:51,520 --> 00:51:54,580
If it is greater than 1.7, then assign difference.

463
00:51:55,120 --> 00:52:02,710
And then what I do is in my formula, notice that I added my two new terms and that's it.

464
00:52:04,360 --> 00:52:06,990
And then that's how you run on a negative binomial,

465
00:52:07,360 --> 00:52:15,280
which you explained just when you are able to do your formula and run the same function that you run before.

466
00:52:16,000 --> 00:52:20,920
And then the contrast matrix in this case should be very similar.

467
00:52:20,980 --> 00:52:24,790
So this is for less than 1.4.

468
00:52:25,750 --> 00:52:30,130
This is for between 1.4 and 1.7.

469
00:52:30,940 --> 00:52:36,639
So you have to do this three times and then one for something bigger than 1.7.

470
00:52:36,640 --> 00:52:46,540
So for less than 1.4, only while taking this space value for anything between 1.4 and 1.7 will feeding this on the first display.

471
00:52:47,020 --> 00:52:54,070
And then for anything above 1.7, I have one thing as the first displaying on the second line.

472
00:52:54,550 --> 00:52:59,740
So you have to like run the same code three times, adding those three 2.25 each time.

473
00:53:00,680 --> 00:53:06,250
Does that make sense? I do the question so it's greater than or equal to seven.

474
00:53:06,820 --> 00:53:11,280
Why is it false for a 46 year old?

475
00:53:12,280 --> 00:53:18,290
I think were in the estimates or in the line.

476
00:53:18,520 --> 00:53:22,060
What are you doing here? Yes.

477
00:53:22,130 --> 00:53:29,980
So why is that? The zero for a wall or if it's greater than or equal to 1.7?

478
00:53:29,980 --> 00:53:34,630
Unless maybe I misinterpret it. Well, because you are carrying the effect.

479
00:53:34,840 --> 00:53:39,010
Okay. So here you have to carry these things because these value.

480
00:53:39,160 --> 00:53:47,950
So these things value, it is greater than 1.4. So one, let's say I have 1.9 is it is greater than 1.4.

481
00:53:47,980 --> 00:53:53,440
Yes. So this takes value of 1.5 is greater than 1.7.

482
00:53:53,450 --> 00:53:56,900
So this takes value. You know, you paraphrasing. Okay.

483
00:53:58,840 --> 00:54:08,180
Okay. There is one more thing that we unfortunately don't have time to go over today, but it's very similar.

484
00:54:08,200 --> 00:54:13,450
It's easy reinflate a negative binomial justice steps are very similar.

485
00:54:13,780 --> 00:54:18,430
Your feed ended up being binomial. Then you look at the output of the largest.

486
00:54:18,940 --> 00:54:27,430
I'm sorry you feel as you inflated negative binomial, which in case of source is just adding that zero morally in case of art.

487
00:54:27,460 --> 00:54:37,050
It's also very similar. You only need to change the formula with the bar and then to the right you look at the output, right?

488
00:54:37,060 --> 00:54:41,350
Correct. Use the output of the logistic regression.

489
00:54:41,350 --> 00:54:49,000
You select the variables that are significant and then rephrase the negative binomial based on the significant if it's okay.

490
00:54:49,510 --> 00:54:57,960
In the case here, since we already did that for the Poisson boil, we already know that this is the only significant variable.

491
00:54:58,210 --> 00:55:04,060
So I can just go ahead and fit the model only with the significant bias, and that's it.

492
00:55:04,270 --> 00:55:07,900
So that's the only thing that we are missing by the field. That code is here.

493
00:55:09,930 --> 00:55:10,260
I.

