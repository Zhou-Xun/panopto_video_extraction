1
00:04:34,000 --> 00:04:43,000
Okay, Good morning. Everyone Let's get started so let's continue with the lecture on one way. a Noba.

2
00:04:43,000 --> 00:04:49,000
So. yes, I will talk about while we are over, as an extension of to sample key test.

3
00:04:49,000 --> 00:04:55,000
So the idea is that if we had more than 2 population the formula for 2 sample key test will not work.

4
00:04:55,000 --> 00:04:59,000
So I have to modify the vominer to make it still work.

5
00:04:59,000 --> 00:05:02,000
So that's where the will come from as you would see.

6
00:05:02,000 --> 00:05:06,000
The formula is mostly based on the analysis of of a variation.

7
00:05:06,000 --> 00:05:13,000
So that's why it's kind of analysis of variance. but the the goal is deal to compare different needs.

8
00:05:13,000 --> 00:05:24,000
So hope that's so helpful So the null hypothesis of one way or no Well, is that when you have K population, you assume the mean of all of them?

9
00:05:24,000 --> 00:05:29,000
Are the same. I apologize if you hear some background noise from my kids.

10
00:05:29,000 --> 00:05:38,000
They're not left phone for camp yet so the now how this is a new one was new to up to all the way, because me okay, So all of the population have the same. need.

11
00:05:38,000 --> 00:05:45,000
And then the alternative is the opposite of that which means that at least 2 of them are not equal.

12
00:05:45,000 --> 00:05:55,000
So this is people call the the on the bus test, in the sense that essentially we are considering many different alternative.

13
00:05:55,000 --> 00:05:58,000
Right me one could not equal to mute or so on.

14
00:05:58,000 --> 00:06:04,000
So there are many pairwise comparison, but we belong them all together as one single test.

15
00:06:04,000 --> 00:06:10,000
And the reason is because that way we have the highest power while controlling the type, one that i'll talk about later on.

16
00:06:10,000 --> 00:06:13,000
How do we really deal with individual pair wise tests?

17
00:06:13,000 --> 00:06:16,000
So there are pros and council doing each right doing this.

18
00:06:16,000 --> 00:06:21,000
Single path. We can reject the now with the highest power.

19
00:06:21,000 --> 00:06:25,000
While retaining pipeline error but even if we reject the now.

20
00:06:25,000 --> 00:06:31,000
We don't really know which 2 group has the sort of a significant difference or not.

21
00:06:31,000 --> 00:06:41,000
So. So depend on your purpose. If you really want to figure out which new group has there has segregated difference, you need to do some, you know.

22
00:06:41,000 --> 00:06:55,000
Pair pairwise test, we'll talk about that later. So, using this example, where we have a 3 population which are us adults consuming different amount of sugary babies, less than 1 one to 2 and one and 2 so we

23
00:06:55,000 --> 00:07:01,000
arbitrarily split into 3 groups like this to apply our webinar nova analysis.

24
00:07:01,000 --> 00:07:08,000
And then the This was the the, you know, the mathematical formulation of the No and the attorney.

25
00:07:08,000 --> 00:07:12,000
You have office and then assume these are the data we observe right on each group.

26
00:07:12,000 --> 00:07:23,000
We have a sample mean assembly and the sample size, and because this almost has a 2 bit, so we are reduced them by about the fold of a hundredfold.

27
00:07:23,000 --> 00:07:26,000
And just to 3 D. Lester. Okay, let me know if I have any questions.

28
00:07:26,000 --> 00:07:37,000
Cause. This is just review yesterday's material in case you have You've missed it, and these are the you know, reviews the poor example where the me and a standard division is will be the same.

29
00:07:37,000 --> 00:07:45,000
But the sample side is much smaller. So the number one thing we conduct was to conduct any study of the test is first to find us.

30
00:07:45,000 --> 00:07:58,000
The testing right. In particular. the statistic usually count, you know, come as a racial 2 part, but not always, but most often, the ratio of 2 parts and a numerator is something measured, a signal and a denominator something measure The

31
00:07:58,000 --> 00:08:07,000
noise, right or uncertainty along with the signal. So in this case the signal is basically the amount of evidence against the No.

32
00:08:07,000 --> 00:08:11,000
So under the null hypothesis, all those 3 group have the same mean.

33
00:08:11,000 --> 00:08:15,000
Therefore, we first computer grandm under the Noah apologize.

34
00:08:15,000 --> 00:08:21,000
So basically, by alarming all the data together from 3 population estimated single being across all 3 groups.

35
00:08:21,000 --> 00:08:25,000
That will the group. Now we will be the grandmother of the overall meeting.

36
00:08:25,000 --> 00:08:30,000
Assuming the now is true once we have this grandmother.

37
00:08:30,000 --> 00:08:34,000
So basically we measure this. the amount of evidence against the Novi.

38
00:08:34,000 --> 00:08:39,000
Looking at it, that the the distance between each group mean to the grandmother.

39
00:08:39,000 --> 00:08:46,000
So on the integrate all this piece of information together as one number to measure the evidence against the now.

40
00:08:46,000 --> 00:08:54,000
So the longer the distance here apparently the stronger the evidence against them, not because under the now all 3 need should be close, fairly close to the brand new.

41
00:08:54,000 --> 00:08:59,000
We also need to take you to consideration. A definite group sounds right.

42
00:08:59,000 --> 00:09:06,000
So group with larger size will have more say in the final you know, measure of evidence. So we come up with this formula.

43
00:09:06,000 --> 00:09:10,000
Essentially the distance between the group mean versus the grammy and the square.

44
00:09:10,000 --> 00:09:16,000
The distance to turn everything into positive, and and then do a weighted sum waited by the sample size.

45
00:09:16,000 --> 00:09:22,000
Finally divided by the number of groups there. So but here is group minus one.

46
00:09:22,000 --> 00:09:25,000
For for mathematical reason, which I won't be going into.

47
00:09:25,000 --> 00:09:30,000
So this number, this particular formula, and more general formula given here.

48
00:09:30,000 --> 00:09:35,000
Assuming we have 8 different groups. This formula summarized the amount of evidence against the number.

49
00:09:35,000 --> 00:09:44,000
The larger this number is this is always a positive number If you look at a foreigner, because this is square right sum of squares. so the minimal number of these can take is 0. so.

50
00:09:44,000 --> 00:09:56,000
It's always number the larger this amount here. is the stronger. So this this thing is slightly different from what we do in heat has either one simultaneous or 2 sample pieces.

51
00:09:56,000 --> 00:10:01,000
If you think about int as i'll just go over very briefly, right in.

52
00:10:01,000 --> 00:10:07,000
He has. the numerator is x, one minus x, 2, 4, right, and in one sample T.

53
00:10:07,000 --> 00:10:18,000
That is X X bar minus mu. Now, so either case, the signal or the amount of evidence measuring it has is assigned, you know, quality which can take a negative values, right?

54
00:10:18,000 --> 00:10:24,000
So, which means that both very large, positive, or very small negative values right?

55
00:10:24,000 --> 00:10:28,000
They both contain strong evidence that's why we have so for the one-sided versus 2 side.

56
00:10:28,000 --> 00:10:38,000
It has a keypass manner. What this one way I know about bombing them no longer has not poverty, because the single measurement is always positive.

57
00:10:38,000 --> 00:10:42,000
So a little larger than this quantity, the stronger signal.

58
00:10:42,000 --> 00:10:48,000
We do not have a direction anymore. So in this sense there's no so called.

59
00:10:48,000 --> 00:10:55,000
The 2 sample test animal. We always go one side of that one, and that reason because, you know, we have 2 group.

60
00:10:55,000 --> 00:11:05,000
You can just take the difference of 2 group meet. right we have 3 group the direction doesn't really have a meeting anymore, because the 3 group they always send her on a brand new.

61
00:11:05,000 --> 00:11:10,000
They always go pop and down. So you have just have to long them together as one single number.

62
00:11:10,000 --> 00:11:14,000
The direction doesn't matter anymore. Therefore There's no so-called.

63
00:11:14,000 --> 00:11:19,000
You know one side of a 2 side of test, but one another one is always one sign.

64
00:11:19,000 --> 00:11:24,000
So this whole thing is called a mean sum of square. But when groups Msb.

65
00:11:24,000 --> 00:11:33,000
But plug in our real number. we got this huge number don't be food by the the magnitude of this number, because that depend on the measurement unit here, we use calorie, right?

66
00:11:33,000 --> 00:11:36,000
If you use 1,000 calendar, this will be a 1,000 times smaller.

67
00:11:36,000 --> 00:11:45,000
So the magnitude number doesn't matter we have to look at the magnitude of this number relative to the measure of uncertainty.

68
00:11:45,000 --> 00:11:52,000
Okay, which we'll talk about right after so you can look at this number.

69
00:11:52,000 --> 00:11:57,000
It's actually quite interesting, you know. this this one way a nova is a generalization of 2.

70
00:11:57,000 --> 00:12:02,000
Several key has but to more than 2 groups. Nevertheless, even if K.

71
00:12:02,000 --> 00:12:09,000
Equals 2, you can still calculate it right you're plugging cake, with which is a 2 sample tasks.

72
00:12:09,000 --> 00:12:11,000
You can still capture the grand mean, You can still calculate this.

73
00:12:11,000 --> 00:12:18,000
The third state that's still divided by one minus k minus one Wow, Okay, minus one.

74
00:12:18,000 --> 00:12:30,000
You turn on that phone. It is very much related to the I'll talk a little bit later, so you sent for this one way of Nova is a it's a direct generalization of the Tuesday in that if you really apply this

75
00:12:30,000 --> 00:12:36,000
to the 2 sample data is still works, and you will get exactly the same P.

76
00:12:36,000 --> 00:12:42,000
Value, which is very comforting. right We don't want a 2 different tests to give us 2 different conclusion, which is not a case.

77
00:12:42,000 --> 00:12:47,000
Yeah. So this is the illustration of the kind of data.

78
00:12:47,000 --> 00:12:55,000
We have 3 groups with different sample side here. This sample side is small enough so that we can plot all the data right under the 3 caller.

79
00:12:55,000 --> 00:13:06,000
3 3 bars in the middle shows the root mean what i'm trying to say is that remember the null have all the same, the 3 group have the same mean apparently they don't as a sample because

80
00:13:06,000 --> 00:13:18,000
there's a variety there but just by looking at the data point Personally, I couldn't tell that you know I I I wouldn't believe that there's a strong habit that's against them all so it's

81
00:13:18,000 --> 00:13:26,000
very possible that a 3 group actually have the same mean. We just got this kind of data right due to randomness you can think about if you run them.

82
00:13:26,000 --> 00:13:30,000
Some of these data again. you've got different points this 3 bar will change location, right?

83
00:13:30,000 --> 00:13:35,000
Maybe they're older, They're relative you know order will also change.

84
00:13:35,000 --> 00:13:41,000
So, which is a signal which is a sign that you know maybe there's not enough evidence against the no let's do.

85
00:13:41,000 --> 00:13:44,000
Don't know whether the now is true or false but just from the data.

86
00:13:44,000 --> 00:13:54,000
I don't need to see a lot of evidence we have to do. A qualitative analysis to to see you know what's really there cause our intuition may not always hope so.

87
00:13:54,000 --> 00:14:01,000
What we exactly are doing here is that we measure the difference of every single data points to grant me.

88
00:14:01,000 --> 00:14:08,000
So this the all this distance here measure, how variable the population was right.

89
00:14:08,000 --> 00:14:12,000
So the larger this, you know, there are 24 difference here.

90
00:14:12,000 --> 00:14:17,000
Call 3 or 6 here, right altogether there's 42 right?

91
00:14:17,000 --> 00:14:24,000
The larger overall is 42 numbers in magnitude, the more variable the the population originally was.

92
00:14:24,000 --> 00:14:29,000
We have to pick that into consideration. So what we do is that we, you know.

93
00:14:29,000 --> 00:14:36,000
Now we are trying to calculate the the so called denominator, which is the uncertainty measurement of the numerator.

94
00:14:36,000 --> 00:14:45,000
So first we sum over all the squire business of every single data point, for It's all mean in the first group.

95
00:14:45,000 --> 00:14:56,000
Now we do the second group. Now I do the third rule, and finally we add them all together.

96
00:14:56,000 --> 00:15:00,000
So once we add them all together, this number will measure the overall variability.

97
00:15:00,000 --> 00:15:11,000
We still have to normalize by the sample size. So in the end we take all the sum of square for every single data point with an individual group mean, with divided by the total samples on.

98
00:15:11,000 --> 00:15:15,000
But here again we subtract the number of groups to adjust.

99
00:15:15,000 --> 00:15:22,000
For some, some, you know, some bias in the estimation which is very similar to when we calculate the standard deviation.

100
00:15:22,000 --> 00:15:25,000
Remember we do one over m minus one. here. we do one over M minus K.

101
00:15:25,000 --> 00:15:32,000
Because case, the number of groups right, the real map behind is very sophisticated.

102
00:15:32,000 --> 00:15:35,000
But but you know the intuition is that we use the data.

103
00:15:35,000 --> 00:15:40,000
To estimate 3 group means. So we lose so called hold on called 3 degree of freedom.

104
00:15:40,000 --> 00:15:48,000
So that's why we subtract 3 here and the essentially is taking a average.

105
00:15:48,000 --> 00:15:55,000
Okay for the data points. So this whole thing is similar to the to the to the variance, you know, of a single population up here.

106
00:15:55,000 --> 00:15:59,000
We have K population right? This measure the overall variability of the data.

107
00:15:59,000 --> 00:16:03,000
This would call the Amassi is mean square arrow.

108
00:16:03,000 --> 00:16:07,000
So once we plug in the real data it's also a huge number.

109
00:16:07,000 --> 00:16:10,000
Okay, So now we have 2 quantity Msb. and Ms.

110
00:16:10,000 --> 00:16:19,000
E. So the Msp. Msp. This thing somehow measures the evidence against the norm.

111
00:16:19,000 --> 00:16:21,000
I have a larger this number than we'll have it's again.

112
00:16:21,000 --> 00:16:30,000
No, this I'm: as e Somehow, measures the overall variability of the of the data right? the larger line I see the more variable the data is.

113
00:16:30,000 --> 00:16:36,000
So now, taking this 2 piece of information into consideration, and now we do this.

114
00:16:36,000 --> 00:16:42,000
We take the ratio of this. Remember, this is very much like the t statistic where we take the

115
00:16:42,000 --> 00:16:50,000
You know the difference divided by the standard error. Right here we we measure Msp and Mse, and then we gotta ask statistics.

116
00:16:50,000 --> 00:16:56,000
So the ratio of this 2 is called app statistic, because it follows something like a after distribution.

117
00:16:56,000 --> 00:17:01,000
So this time the is not normal anymore, because both Msp.

118
00:17:01,000 --> 00:17:09,000
And Mce. they are always non-negative. When I take a ratio between 2 night, you know positive number, you always got a positive number.

119
00:17:09,000 --> 00:17:15,000
So this app statistics always positive because it's always positive you can not be a normal distribution, right?

120
00:17:15,000 --> 00:17:28,000
It is actually not normal distribution now, but the last. This ratio is called app statistic, which measures no, the the the amount of you know, evidence against the norm.

121
00:17:28,000 --> 00:17:33,000
So basically, we somehow standardized. Or you can say, normalize the Msv.

122
00:17:33,000 --> 00:17:37,000
By Ms. Both are very large Number right? You see, Ms. B. depend on the unit. Also Ms. E.

123
00:17:37,000 --> 00:17:39,000
Under the unit will take the ratio of the ratio.

124
00:17:39,000 --> 00:17:45,000
This unit. independent. So this app doesn't have a unit anymore.

125
00:17:45,000 --> 00:17:49,000
These are. These are unit list management of the amount of evidence against them all.

126
00:17:49,000 --> 00:17:54,000
So it's hard to just look at the statistic to Figure out what the pivot is.

127
00:17:54,000 --> 00:18:01,000
No longer like like the key page. Right When the samples are large in the key pace, we can just look at whether the piece large on 2 or less than minus 2.

128
00:18:01,000 --> 00:18:09,000
Now we cannot tell anymore, because apps statistic has a distribution which depends on the degree of freedom in both the numerator.

129
00:18:09,000 --> 00:18:14,000
So basically the number of groups, the number of samples data point etc.

130
00:18:14,000 --> 00:18:21,000
So you have to use the software or lookup table in the in all days to turn off statistic into the p vile.

131
00:18:21,000 --> 00:18:24,000
But as soon as you you got a pivot you know what to do.

132
00:18:24,000 --> 00:18:31,000
Now you just look at the key value to see whether you know whether you reject it now or not, and what amount of habit that's the case to know.

133
00:18:31,000 --> 00:18:38,000
But in general the larger the statistic the smaller the p value So that's the that's what's going on.

134
00:18:38,000 --> 00:18:49,000
Okay, let's now you know just like the 2 sample key test case right? Let's now use some visual to see what really is going on, because there's a lot of formula we did right?

135
00:18:49,000 --> 00:18:52,000
But the key is not to remember those bomb in there.

136
00:18:52,000 --> 00:18:56,000
But trying to understand what the problem is trying to capture.

137
00:18:56,000 --> 00:19:03,000
So, Paul: Okay, this is our data. Okay, very, very simple.

138
00:19:03,000 --> 00:19:07,000
Illustration. So the 3 caller shapes right.

139
00:19:07,000 --> 00:19:10,000
The rest are always what is ensemble from one population.

140
00:19:10,000 --> 00:19:19,000
The green stars is a sum up on the second population. The blue triangle is the sample from the third population, and the black square is the overall gravity.

141
00:19:19,000 --> 00:19:27,000
So the idea is that if the 3 population are more separated, the 3 sample was more separated.

142
00:19:27,000 --> 00:19:31,000
We consider we have more evidence against the null, which is saying, the 3 publishers actually have the same mean versus.

143
00:19:31,000 --> 00:19:40,000
If the 3 you know then we have last evidence against the North right?

144
00:19:40,000 --> 00:19:53,000
So in this case we have sort of a moderate we don't know whether the evidence is strong enough because we have to concede that the variability, so let's say, we draw 3 circles to show the 3 so groups right so the

145
00:19:53,000 --> 00:19:59,000
more separate. The 3 circles are so stronger to have it.

146
00:19:59,000 --> 00:20:10,000
So now we draw this these lines right? So the So the blacks are called Black Star, and the Black Client, though, are the group.

147
00:20:10,000 --> 00:20:16,000
Me. We have 3 group, and a black, correct angle in the middle is the overall mean.

148
00:20:16,000 --> 00:20:22,000
So now we might do The distance between the 3 group mean to the overall mean right, which is the the solid black arrow.

149
00:20:22,000 --> 00:20:28,000
The longer these 3 arrows are the more evidence against the law.

150
00:20:28,000 --> 00:20:40,000
And another thing we measure is every single data point, individual group mean, which are the dotted Hello, Arrow, And there are many of them.

151
00:20:40,000 --> 00:20:43,000
We just draw 3, because every single day the point is error.

152
00:20:43,000 --> 00:20:54,000
So the longer the dogged black Arrow are the more vulnerability in the data we have, which means that the small, the last, evidence.

153
00:20:54,000 --> 00:21:01,000
So basically trying to measure the relative length of the Black Arrow versus the relative length of the audit arrow.

154
00:21:01,000 --> 00:21:08,000
And because they are so many of them, we have the summarize them into 1 one number only, right for each.

155
00:21:08,000 --> 00:21:20,000
So in intuitively right. if the black arrow are shorter relative to the dogged arrows, as you can see in this case, the 3 circle they all will have a lot.

156
00:21:20,000 --> 00:21:26,000
In this case we really cannot tell that they come from different population.

157
00:21:26,000 --> 00:21:33,000
Really could be that you know the difference of the 3 circle really just come from random sample variability.

158
00:21:33,000 --> 00:21:37,000
Alright. In this case we have last signal or less evidence against the now.

159
00:21:37,000 --> 00:21:48,000
On the contrary, if the the solid black arrow are longer on average, then the dotted arrow, then we have more evidence.

160
00:21:48,000 --> 00:21:55,000
You see that in this case the 3 groups are more separated it's very unlikely that they actually come from the same population. right?

161
00:21:55,000 --> 00:22:00,000
So really well marrying the the less of the Black Arrow versus the last of the dogged.

162
00:22:00,000 --> 00:22:05,000
You know dogged arrow so that's exactly what we're doing here.

163
00:22:05,000 --> 00:22:12,000
It's happening with a specific mathematical partner so that they have a in the ratio has a mathematical, you know, categorizable distribution that we can turn into p value.

164
00:22:12,000 --> 00:22:21,000
You can come up, come up with many different, you know many other different measurements of the signal and evidence, but unfortunately, most of them.

165
00:22:21,000 --> 00:22:26,000
You, you can really be gone there sampling distribution.

166
00:22:26,000 --> 00:22:28,000
Therefore you don't know how to get a peabody How about they?

167
00:22:28,000 --> 00:22:32,000
There. There are approaches, you know, in modern statistics to deal with those.

168
00:22:32,000 --> 00:22:39,000
But this these things are are more, you know, easy one time, and I think, the you know message of the chat.

169
00:22:39,000 --> 00:22:42,000
This. This illustration was down again by my colleague, Tom Bra.

170
00:22:42,000 --> 00:22:49,000
I really love it. Okay.

171
00:22:49,000 --> 00:22:54,000
Yeah, let me know you've got any questions and in this particular case, right?

172
00:22:54,000 --> 00:23:05,000
We plug in the Msp. and Msc. we just calculated we propose our big number the numbers itself are not of interest because they depend on the unit, you measure, but they're racial.

173
00:23:05,000 --> 00:23:11,000
It's a unit list. only 1 point Oh, 5 the larger this number is the larger, the app statistic.

174
00:23:11,000 --> 00:23:18,000
We have the smaller the p value. In this particular case, the corresponding pivot to this aspect.

175
00:23:18,000 --> 00:23:27,000
Again. You can never get from aspect to the p value by hand There's no same formula to do that the software can do it.

176
00:23:27,000 --> 00:23:35,000
But nevertheless, you got people out of 0 Point 3 6, which is way bigger than 0 Point 0 5. The report we failed to reject.

177
00:23:35,000 --> 00:23:40,000
Now, which is also consistent with our you know. Remember, I volume of this bigger right here.

178
00:23:40,000 --> 00:23:44,000
I talk about. Personally, I don't really see a strong evidence against them.

179
00:23:44,000 --> 00:23:54,000
Now that's sort of a consistent with the p via which is 0 point 3, 6.

180
00:23:54,000 --> 00:24:08,000
So. So you know, visualize your data. To get the impression is very helpful in case you may sound mist mistake or analysis, you know you can. you know, often spot that right and correct before you you move on to the Next

181
00:24:08,000 --> 00:24:14,000
step. So that's a conclusion of this key value we you know we have a lack of evidence that there is association.

182
00:24:14,000 --> 00:24:18,000
So basically, you know, remember here we failed to reject the novel.

183
00:24:18,000 --> 00:24:32,000
And now, says the 3 population has the same mean which is also to say that there's no association between daily sugary drink, consumption, and Dave clara consumption fail to reject now, which means I will

184
00:24:32,000 --> 00:24:39,000
fail, or to show there is association right which is again not the same as proving.

185
00:24:39,000 --> 00:24:46,000
There's no association because we never approved them all right, If we collect more data, I mean association.

186
00:24:46,000 --> 00:24:57,000
You could imagine it's always there it's just how big it is right, you know, in in a so the most rigorous way.

187
00:24:57,000 --> 00:25:01,000
Anything is associated. anything else in the world they just association could be minus you.

188
00:25:01,000 --> 00:25:05,000
So that's not interesting right was was infinite sample size You can reject any now.

189
00:25:05,000 --> 00:25:13,000
Honestly right so, whether there's association or not it's not that useful unless you have a reasonable sample size and power.

190
00:25:13,000 --> 00:25:19,000
If you overpower your show association. But association could be super super time, you know, to not be interesting.

191
00:25:19,000 --> 00:25:23,000
So in the end we do not conclude that no is true.

192
00:25:23,000 --> 00:25:33,000
We just by you to reject them all? So now, once we reject the null, we we come. Who?

193
00:25:33,000 --> 00:25:42,000
The alternative is likely to be true that's hard to say that remember not all 3 groups are equal right at least 2 group have different me.

194
00:25:42,000 --> 00:25:49,000
But that's not enough if you're interested in knowing which the group have differently.

195
00:25:49,000 --> 00:25:55,000
So in this particular case it may not be that interesting, but in cases where, for example, you, you run a clinical trial, right?

196
00:25:55,000 --> 00:26:03,000
You compare 2 different blocks, or or, you know, 2 different main manufacturer of the same drop versus a placebo.

197
00:26:03,000 --> 00:26:08,000
But you really want to know which 2 groups have difference, right or all of them have difference.

198
00:26:08,000 --> 00:26:13,000
You want to know that right? So in that case you need to do something called the post hoc P.

199
00:26:13,000 --> 00:26:19,000
Test. So Let's first summarize what we do as yeah Nova in the table.

200
00:26:19,000 --> 00:26:27,000
This is called a nova table. So every statistical software that can does a one can do a long way or Nova will give you more or less a table like this.

201
00:26:27,000 --> 00:26:30,000
They will give you more information, maybe more than the table provided here.

202
00:26:30,000 --> 00:26:34,000
But this table is always there, so we call that nova table.

203
00:26:34,000 --> 00:26:41,000
I just trying to show you how to read them. I can already go through all of them

204
00:26:41,000 --> 00:26:46,000
So there are 2 roles, actually 3 roles, but the third row is the total is the sum of the first 2 roles.

205
00:26:46,000 --> 00:26:56,000
So basically we measure quantity between groups and within groups right between groups you can think about as a signal within group as the noise. But in the end we take the ratio.

206
00:26:56,000 --> 00:27:00,000
So there's a between group sum of square and within group sum of square.

207
00:27:00,000 --> 00:27:08,000
So this is the sum of square of the you know the the the grand mean between the grooming and the grandmother.

208
00:27:08,000 --> 00:27:12,000
This is the sum of square. of all the data point to the group meet.

209
00:27:12,000 --> 00:27:17,000
This is the way this is where this is just each individuals you know data point.

210
00:27:17,000 --> 00:27:22,000
I mean there's a degree of freedom basically how many variables are evolving calculated this right?

211
00:27:22,000 --> 00:27:27,000
So here we have a 2 degree reference, because we have 3 group, but we lose one degree of freedom.

212
00:27:27,000 --> 00:27:32,000
What, when, as many of grant me here, we have 39, do we free them?

213
00:27:32,000 --> 00:27:36,000
Because we have 42 data points, right? 24 plus or 6.

214
00:27:36,000 --> 00:27:45,000
Well, we lose 3. the degree of freedom by all estimating a 3 group means, So that's 39, and then we divide each sum of square by the degree of freedom.

215
00:27:45,000 --> 00:27:55,000
That's exactly what we did you know obama here. So these are the sum of square, and this is the degree of freedom.

216
00:27:55,000 --> 00:27:59,000
So basically take a racial by normalizing them, And you divide this number by 2.

217
00:27:59,000 --> 00:28:04,000
You got this mean square, you divide this number by 39.

218
00:28:04,000 --> 00:28:06,000
You got this mean square? So this is mean, square between group.

219
00:28:06,000 --> 00:28:12,000
This is mean, square error. I must be I see, and they will take the ratio of these 2 again.

220
00:28:12,000 --> 00:28:25,000
So this one is the ratio of this and this and this one is the ratio of this, and this and this app statistic is the ratio of this, and this there's a lot of racial by indiana we

221
00:28:25,000 --> 00:28:29,000
got the f statistic and the larger this value is i'm more out of this against the null.

222
00:28:29,000 --> 00:28:36,000
So the whole point of confident. this this way is going to make this app statistic unitless.

223
00:28:36,000 --> 00:28:40,000
It doesn't contain it. doesn't care about the unit you measure, and also you can compare the cost experiment.

224
00:28:40,000 --> 00:28:44,000
So you you know, by dividing a degree of freedom here.

225
00:28:44,000 --> 00:28:49,000
And here you are, sort of a standardizing by the number of groups and a number of data points.

226
00:28:49,000 --> 00:28:59,000
All of those are. How so you In the end you have this app statistic, which is a pure measurement of amount of evidence, and then the larger, that statistic, more evidence.

227
00:28:59,000 --> 00:29:06,000
And then you can turn this into a P. value. So I have to say, the statistic P. vio is not one to one correspondent.

228
00:29:06,000 --> 00:29:15,000
It depends on the degree of freedom. So you could have the same as statistic from different experiments, but because they have different number of groups, so they have different number of data. points.

229
00:29:15,000 --> 00:29:32,000
Their corresponding pivot is different. which was actually the same case in the heat has the key test. also has one degree of freedom parameter, except that when that parameter is large enough, we kind of approximate, that user lo that's kind of

230
00:29:32,000 --> 00:29:38,000
normal. I I know all of those you know details so that you don't got distracted too, much.

231
00:29:38,000 --> 00:29:46,000
But nevertheless, that's the typical software will take all these information into consideration.

232
00:29:46,000 --> 00:29:51,000
Finally give you a key value. i've been the total here just the sum of the 2 column here.

233
00:29:51,000 --> 00:29:56,000
So I won't go through

234
00:29:56,000 --> 00:30:05,000
Okay, Now I talk about. Once you run the one way or no value you will know whether you're rejecting now on fail to reject.

235
00:30:05,000 --> 00:30:11,000
Now, right. if you fail to reject the null then you you basically have not.

236
00:30:11,000 --> 00:30:15,000
You don't have enough evidence against them all therefore there's nothing else.

237
00:30:15,000 --> 00:30:20,000
You need to do right, because the nose has the the 3 public group and equal.

238
00:30:20,000 --> 00:30:27,000
You cannot reject it right. but what if you use so successfully reject them now?

239
00:30:27,000 --> 00:30:34,000
You know the groups are not all equal. Well, many of you might be interested in learning which tools are not equal.

240
00:30:34,000 --> 00:30:43,000
Which way? Hi! the welcome to tell you that. So if you reject it now, running the one way or no.

241
00:30:43,000 --> 00:30:47,000
Well, the next thing you could do is so called the post hot test.

242
00:30:47,000 --> 00:30:56,000
Essentially you are doing a pairwise. Then right compare every 2 groups for the problem doing this is that you're running many tests.

243
00:30:56,000 --> 00:31:01,000
If you use 0 point 0 5 as the hyper error rate for each of them.

244
00:31:01,000 --> 00:31:05,000
When when you run menu pass like this you can play don't type on that.

245
00:31:05,000 --> 00:31:16,000
So you have to adjust for that. So this post office, you know, adjusting all of this use some some, you know, advanced techniques. but you know the simplified version would be.

246
00:31:16,000 --> 00:31:25,000
You just run. We have 3 groups right you just run 3 parallel. Who'sabel Keith has and that's here.

247
00:31:25,000 --> 00:31:38,000
So, for example, now we have 3 population you know again for simplicity. Let's say that me understand dvd a number like this which are much simpler, and the sample size are a little bigger like this, so that you know our statistic in

248
00:31:38,000 --> 00:31:42,000
this case are larger, the pivot is smaller, so we reject. not.

249
00:31:42,000 --> 00:31:45,000
All of these are made up. Okay, just to illustrate when you're rejecting.

250
00:31:45,000 --> 00:31:54,000
Now what you do next. So for this data we could reject them up because the samples have the larger right.

251
00:31:54,000 --> 00:31:58,000
Now we reject the null. What we do next, right?

252
00:31:58,000 --> 00:32:03,000
So we can do the so-called Post Hot Pass, which essentially does 3 pair wise keypad.

253
00:32:03,000 --> 00:32:09,000
We come here. The first group versus the second first group versus the third, second versus the third.

254
00:32:09,000 --> 00:32:17,000
Right that's the default you compare every single pairwise you could pick a few pairs with do, but you want to pick the pair essentially before you see the data.

255
00:32:17,000 --> 00:32:21,000
If you you saw your data and then pick the pair that's already biased.

256
00:32:21,000 --> 00:32:26,000
Okay. So usually people just, you know, by deeper conduct all the pairwise.

257
00:32:26,000 --> 00:32:30,000
But this thing will grow quickly right. If you have 3 group, you have 3 pairwise.

258
00:32:30,000 --> 00:32:35,000
You have 4 group of 6 pairwise, the 5 or 10 per one, right that grow quickly.

259
00:32:35,000 --> 00:32:44,000
So now we conduct 3, 2 sample keypads of the 3 pairs, and got 3 t statistic. The 3 P.

260
00:32:44,000 --> 00:32:52,000
V. and we see that 2 of them are not significant, but one of them will compare first group or the third group.

261
00:32:52,000 --> 00:33:08,000
They are significant. Could you conclude that you know there is a significant difference between the first group and the third group, and no significant difference between the first and the second?

262
00:33:08,000 --> 00:33:13,000
There. there is some some caveat there i'll talk about essentially multiple testing, right?

263
00:33:13,000 --> 00:33:20,000
Because you conduct 3, Hey, Our test here. if you again use point 0 5 for each of them.

264
00:33:20,000 --> 00:33:27,000
The overall. The chance you make, the type of error rate will almost be tripled, would be 3 times higher than this.

265
00:33:27,000 --> 00:33:32,000
Almost so you fail to control type of highway that's that's this level.

266
00:33:32,000 --> 00:33:38,000
So a simple way of doing it would be divided by our right

267
00:33:38,000 --> 00:33:42,000
By 3. So if you divide this by 3, or you can modify the P.

268
00:33:42,000 --> 00:33:55,000
By 3 to be the same thing you divide this one by 3. That's 0 point 0 1 6 or something how you compare this with 0 point 0 1 6, and to see which one this way overall you still control half an

269
00:33:55,000 --> 00:34:00,000
hourly. So it turns out that even you point 0 point 0, 1 6.

270
00:34:00,000 --> 00:34:04,000
This one is still significant. So the conclusion doesn't change right still person.

271
00:34:04,000 --> 00:34:09,000
Third group has difference. But you know, first and second second third doesn't have a significant difference.

272
00:34:09,000 --> 00:34:13,000
The conclusion doesn't change or your type of error rate is totally different.

273
00:34:13,000 --> 00:34:19,000
Right. So the way we just divide the type of memory by 3. it's called the bomb for running correction.

274
00:34:19,000 --> 00:34:26,000
Is this one of the why they use the way we have an automobile comparison, and then the interpretation is also tricky.

275
00:34:26,000 --> 00:34:34,000
Right. Now you think about what we learned is that the first and the second group have no different.

276
00:34:34,000 --> 00:34:43,000
The second third has no difference. Right? If you really think about a now being true, then if if A equals B equals C, then a must equal, c.

277
00:34:43,000 --> 00:34:50,000
But nevertheless we show the one first and third group are are different than me, which means which actually go.

278
00:34:50,000 --> 00:34:54,000
There's no contradiction at all because I have to go back to the way we interpret here.

279
00:34:54,000 --> 00:35:02,000
We fail to show, of course, and second group are different it's not the same as saying we show first and second the same.

280
00:35:02,000 --> 00:35:06,000
But if we show dy equals mu 2, we also show mute because of 3.

281
00:35:06,000 --> 00:35:11,000
Then we are showing me one equals mail 3, which will be a contradiction to to here.

282
00:35:11,000 --> 00:35:23,000
You know the only way to you limit that contradiction it's because, you know, we are not really proving new one, because we just fail to show me what not even too.

283
00:35:23,000 --> 00:35:29,000
So everything there's a probabilistic going on there, and also there is a, you know, interpretation which is different.

284
00:35:29,000 --> 00:35:34,000
So that's why i'm making this whole thing consistent.

285
00:35:34,000 --> 00:35:38,000
So that's still called the pulse hot path you know this is very naive.

286
00:35:38,000 --> 00:35:46,000
For some past the reality. There are different versions of the improved version of this give you slightly higher power than doing things notably like this.

287
00:35:46,000 --> 00:35:51,000
But the principle is the same. Okay, if you use the software running with one way and Nova, they have track box asking.

288
00:35:51,000 --> 00:35:55,000
Do you want to run post hot packs after you? reject it?

289
00:35:55,000 --> 00:35:58,000
Now if you click yes, they will ask you what kind of post-pass right.

290
00:35:58,000 --> 00:36:01,000
Who's on one of it's called the 2 key staff they did.

291
00:36:01,000 --> 00:36:03,000
They does something like this, but do it more in a more.

292
00:36:03,000 --> 00:36:11,000
You know, smarter way, so that they have side higher power than of anything than the while. We'll just talk about here, for the principal is the same.

293
00:36:11,000 --> 00:36:15,000
They also adjust mobile testing.

294
00:36:15,000 --> 00:36:19,000
And up here. I just try to show what's really the problem with multiple testing.

295
00:36:19,000 --> 00:36:25,000
I talk about many times right, but I think this cardoon actually shows you the the best.

296
00:36:25,000 --> 00:36:40,000
I really like this. I see xk CD website i'll go over here. You know it's supposed to be a joke, but it's actually very real what's going on here is that you know these girls have generally been caused

297
00:36:40,000 --> 00:36:55,000
acting. This boy says, you know that's run aside scientist investing that's fine start I'm gonna run the first experiment showing we find knowing between javaine and Akin P is greater than 0 point 0

298
00:36:55,000 --> 00:37:09,000
5, the design requirement pack data show loading. Let me think about what if you know that association only applied to a certain Hi You have specific a certain caller of Johnny.

299
00:37:09,000 --> 00:37:14,000
Now the design environmental, you know, to study a different caller reality.

300
00:37:14,000 --> 00:37:18,000
When it study purple javine, they find those significant.

301
00:37:18,000 --> 00:37:23,000
Then they do wrong. No second anytime pain no's a gonna be time.

302
00:37:23,000 --> 00:37:29,000
Hello, keel etc. Each of the experiment they find no significant. and Q.

303
00:37:29,000 --> 00:37:33,000
The right run. run all the way they run green. They gotta keep out of less than point.

304
00:37:33,000 --> 00:37:37,000
Oh, 5. So among all the plenty caller they they, they studied.

305
00:37:37,000 --> 00:37:47,000
They find the green kind of key value that's 1.5. and then they wrote a paper about it, news Green generally being linked to acting 95% permanent.

306
00:37:47,000 --> 00:37:51,000
No key less than point 0, 5, etc. How part of this is it?

307
00:37:51,000 --> 00:38:10,000
Oh, right, No one is gonna study things like this how the problem. It's: actually very real, because what is key is starting for is that even if the know is true, you still have 5% chance of having a P last 9.5 just due to the random

308
00:38:10,000 --> 00:38:21,000
flu, because the data is random. So if you just do this 20 times, and you know, each time, even though there's no associate at all.

309
00:38:21,000 --> 00:38:29,000
On average, you're gonna find one experiment with key left and phone will fine, and when you write it out you've never mentioned it the other night, 19 experiment.

310
00:38:29,000 --> 00:38:40,000
I failed. so people might think people who rather news, or your article might think this is really Are you really part of all your the other 19 environment?

311
00:38:40,000 --> 00:38:50,000
They they will all this but the the fact that you never write the 19 you did if if i'm a reviewer, I know you that you did planning experiment, and find one.

312
00:38:50,000 --> 00:38:53,000
Let's my point or 5 I wouldn't talk this at all.

313
00:38:53,000 --> 00:38:58,000
But no, I really write a paper saying the the failed requirement, they, the attempt.

314
00:38:58,000 --> 00:39:03,000
No one has an issue. Apparently this Janet, in case is this is just a job.

315
00:39:03,000 --> 00:39:11,000
No one will do this, but think about what you're doing in your data analysis, right like in a while we talk about before. right? you design Pharaoh, you'll run it. Your P.

316
00:39:11,000 --> 00:39:14,000
Is not on the point, or why what you do. you clap more data right again for your time.

317
00:39:14,000 --> 00:39:20,000
Different tests. I try and try and try to find something I write it off.

318
00:39:20,000 --> 00:39:28,000
That's essentially the same thing as You are doing here so the So the multiple testing issue is interesting.

319
00:39:28,000 --> 00:39:34,000
And not only that, you know. You might think, Yeah, I could.

320
00:39:34,000 --> 00:39:41,000
I could be careful by not doing this. What this multiple passing at go beyond what you can imagine.

321
00:39:41,000 --> 00:39:47,000
Because you could think about this. 20 experiments may not be done in the same lab.

322
00:39:47,000 --> 00:39:52,000
What if there are done by 20? different, live each lab nda environment?

323
00:39:52,000 --> 00:39:56,000
They? they have no idea what other level do. And now you have failed.

324
00:39:56,000 --> 00:40:05,000
Nothing happened but what the one lab group succeeded by the paper and That's the same thing.

325
00:40:05,000 --> 00:40:17,000
You see only the one lap who's let's say the right. The paper, the other 9 11, never mentioned it's the same thing. So that's why the model has been usually actually go beyond what you are aware because even if

326
00:40:17,000 --> 00:40:25,000
you intentionally control for your module passing, you cannot control for other people doing the same stuff.

327
00:40:25,000 --> 00:40:27,000
So in the end we end up with a publication bias.

328
00:40:27,000 --> 00:40:33,000
So the the you know the P. value framework itself is not wrong.

329
00:40:33,000 --> 00:40:42,000
If I only conduct it once. But the problem is that just so many researchers, so many, you know, study out there and then.

330
00:40:42,000 --> 00:40:53,000
The journal has limited space so more or less. In the end the journal has to be picky, and they tend to pick significant findings, and then this calls the box right only significant.

331
00:40:53,000 --> 00:40:58,000
Finding get published. So at this level in yeah, you can think about that.

332
00:40:58,000 --> 00:41:10,000
Actually one of the reason why it's so many significant findings cannot be reproduced, because there's a selection filtering by the publication layer.

333
00:41:10,000 --> 00:41:22,000
Right. So in principle, if every experiment got published no matter. a piece larger than point o 5 or 11.5, then that's better, because at least you know, we got to see 2 you know every paper we see here that is significant.

334
00:41:22,000 --> 00:41:27,000
We see another 19 paper showing. it is not significant. I give up a complete picture.

335
00:41:27,000 --> 00:41:42,000
Are you reality that that's not the case because you know they're there's not enough space on how journals right and also negative finding is not so excited doesn't make the news headlines so that's you know part

336
00:41:42,000 --> 00:41:47,000
of the so called, you know, P. value crisis, right? People are on our warning right?

337
00:41:47,000 --> 00:41:54,000
This is the Colby I mean. nobody did anything wrong but in the end. What we find is that many finding cannot be reproduced.

338
00:41:54,000 --> 00:41:57,000
So that's why people add the case you know stop using P.

339
00:41:57,000 --> 00:42:03,000
Value, or try something else, or you know blah blah blah, you know I I totally gather arguments right.

340
00:42:03,000 --> 00:42:12,000
But as I mentioned yesterday. we all agree there's a problem. but it doesn't seem to be there is a perfect and everybody but a great solution.

341
00:42:12,000 --> 00:42:16,000
Yeah, So for that reason, you know, in a foreseeable future.

342
00:42:16,000 --> 00:42:21,000
Probably there's not gonna be a perfect solution. but just so you aware of this fact.

343
00:42:21,000 --> 00:42:29,000
The model of having is actually beyond what's your account Imagine so at least the least We can do right apparently.

344
00:42:29,000 --> 00:42:33,000
One lab, you know, 20 lab does different thing and keep, you know.

345
00:42:33,000 --> 00:42:38,000
Line to each other. This thing is very hard to solve but it's one lab.

346
00:42:38,000 --> 00:42:44,000
There's plenty experiment, and the only right of the one that can be avoided right?

347
00:42:44,000 --> 00:42:51,000
This just make things worse. So you should always be careful right when you i'm sorry when you analyze your data.

348
00:42:51,000 --> 00:42:57,000
Many, many, many times you have to think about the fact that you have the inflating of hyper harrow.

349
00:42:57,000 --> 00:43:02,000
The possibility

350
00:43:02,000 --> 00:43:11,000
Okay, Any questions here.

351
00:43:11,000 --> 00:43:15,000
So there are many approach that you know this problem is not new.

352
00:43:15,000 --> 00:43:21,000
People know it for decades. So with multiple testing, you inflate just type of hardware, right?

353
00:43:21,000 --> 00:43:26,000
So there are many approach to counter that one of the is called the Bonfronic Fraction.

354
00:43:26,000 --> 00:43:31,000
Why, just talk about right, Basically, if you run one test, use point 0 point 0, 5 as the color.

355
00:43:31,000 --> 00:43:34,000
If you run 10 tests, you divide the color by hand.

356
00:43:34,000 --> 00:43:41,000
You use 0 point 0 0, 5 if you want 100 this way Each individual passes.

357
00:43:41,000 --> 00:43:45,000
There's lots of people make a mistake Oh, moreo you know the overall.

358
00:43:45,000 --> 00:43:51,000
Have an error rate. we'll still be control that point we'll fine, but you know, Yeah, it also has its downside to reduce your power right?

359
00:43:51,000 --> 00:43:56,000
So there's no free lunch, and there are many you know approaches, develop.

360
00:43:56,000 --> 00:44:00,000
Try to control. What have I also improved power? So also for us.

361
00:44:00,000 --> 00:44:04,000
Well, the problem is there, right? You just find different way to get around it.

362
00:44:04,000 --> 00:44:13,000
But the problem is, you know, still there

363
00:44:13,000 --> 00:44:18,000
Okay, that's go back to our data in this case we compare we run 3 pairwise Tpas.

364
00:44:18,000 --> 00:44:25,000
We got 3 p value One way to you know. there are 2 way to conduct Bonvoroni corruption.

365
00:44:25,000 --> 00:44:35,000
One is to divide the cut off by 3, which means that we will use 0 point, 0, 1 6, a one-seven, as a call out rather than 0.

366
00:44:35,000 --> 00:44:41,000
Point 0 5 the other way. equivalent is to modify the P. by 3 still compared to 0 point 0 5.

367
00:44:41,000 --> 00:44:44,000
That's the same thing. The only thing is that if you multiply the divided by 3.

368
00:44:44,000 --> 00:44:49,000
You might have to keep out larger than one and you've got happens you just copy that one.

369
00:44:49,000 --> 00:44:53,000
It doesn't really matter. So this to our equivalent you can take whatever approach.

370
00:44:53,000 --> 00:45:00,000
Personally, I like to divide a caught up by 3 no you can't take this, you know, multiply by 3 as well in this particular case, right?

371
00:45:00,000 --> 00:45:08,000
It turns out the conclusion doesn't change right here this one is less than point 0 0 point 0. 5 here is still left on 0 point 0 5.

372
00:45:08,000 --> 00:45:13,000
So the conclusion is that you know, we have a significant difference between the first and third group.

373
00:45:13,000 --> 00:45:19,000
But sometimes, after all, running corruption, the conclusion actually will be different.

374
00:45:19,000 --> 00:45:24,000
So this? Ha! Then you might wonder why the way this one is pair by pass in the first place, right?

375
00:45:24,000 --> 00:45:29,000
Why do we go all the way on bothering Run one way and know about it?

376
00:45:29,000 --> 00:45:40,000
But usually that the if you run multiple tests and you got more information out of it, you lose power, because when you conduct 3 tests you have a lower call out by right.

377
00:45:40,000 --> 00:45:48,000
You have lower power for each type and then that's only happy When you have 3 group and you have when you have K group, they're about, you know, 8 times K.

378
00:45:48,000 --> 00:45:54,000
Minus one, divided by 2 comparison. Right? We have 10 groups. You need about 45 comparison each one.

379
00:45:54,000 --> 00:46:00,000
We have a very low, significant value. They have low power for each one.

380
00:46:00,000 --> 00:46:04,000
So the one we are. Noba, gather on this problem by looming all the past as one.

381
00:46:04,000 --> 00:46:07,000
So you have the highest power rejected. All right.

382
00:46:07,000 --> 00:46:10,000
So what people usually do is that you run over and over first.

383
00:46:10,000 --> 00:46:13,000
If you fail to reject now, you just move on. if you reject it.

384
00:46:13,000 --> 00:46:24,000
Now you run this post hot path to identify which 2 group at the different. Oh, this is sort of a solution, or all these problems still doesn't perfectly solve the problem.

385
00:46:24,000 --> 00:46:33,000
But I think that's what people you know stick with for for this type of people set off so.

386
00:46:33,000 --> 00:46:40,000
You're, reality as I said people don't really run this too sample tetrahed by some more sophisticated version, which has slightly higher power.

387
00:46:40,000 --> 00:46:43,000
Like the tool keys, post hot pass at so on, so forth.

388
00:46:43,000 --> 00:47:13,000
Oh, the principal is the same alright that's all the lecture, any questions before we move on to exercise

389
00:47:27,000 --> 00:47:34,000
Okay, Yeah, I know For those of you who heard about this molecule has been the first time, you know.

390
00:47:34,000 --> 00:47:40,000
It might be a shock to you, take Some time from thinking and understand what's the situation right now.

391
00:47:40,000 --> 00:47:46,000
We're facing right among all the hundreds of thousands paper published every year.

392
00:47:46,000 --> 00:47:54,000
There may be 10 times more that study that you didn't have published. so that factory, you know that's your thing.

393
00:47:54,000 --> 00:47:58,000
How reliable the P. by always see in the paper hard.

394
00:47:58,000 --> 00:48:06,000
The pile approach is still widely used, because people find it useful, and also because we don't have a much better alternative, which is also, you know, both single one.

395
00:48:06,000 --> 00:48:11,000
Is that also, you know. Easy to use but not with the last.

396
00:48:11,000 --> 00:48:17,000
The problem is there, right? Okay, and things like this that's not to say all the queue by the left hand point of 0 point.

397
00:48:17,000 --> 00:48:24,000
0 5 is not reliable it's just that you know you can now look at 1 one publication, one key value and fully trust it.

398
00:48:24,000 --> 00:48:34,000
There. There are many things that can Go Well, right but if you see many, many publications repeatedly, validate one finding that give you a lot more confidence right if it's.

399
00:48:34,000 --> 00:48:54,000
If you know if you don't just gotta keep out of less than 0 point 0 5 by a random chess, then it's very unlikely not finding will be repeatedly validated by independent labs or independent studies

400
00:48:54,000 --> 00:49:02,000
Okay, if there's no question let's move on to the projects today, we're gonna focus on 7 C and 70.

401
00:49:02,000 --> 00:49:04,000
And also there's a post-lctor quit so 7 c.

402
00:49:04,000 --> 00:49:16,000
And the quiz will be deal tomorrow morning that's not work on 7 c.

403
00:49:16,000 --> 00:49:28,000
So these are exercise around, both to sample details, and the while we are Nova

404
00:49:28,000 --> 00:49:38,000
So 7 sees here again, is to do is a paper on the neighborhood disadvantage for social conditions and the cardiovascular disease.

405
00:49:38,000 --> 00:49:44,000
And you know they ask you to look at information in table one, and so on.

406
00:49:44,000 --> 00:50:02,000
Answer these questions, and then So to do this exercise, you need to use this one way or nova spreadsheet. I will, illustrate a little bit

407
00:50:02,000 --> 00:50:10,000
So again. the paper is here, and then the data is in the paper.

408
00:50:10,000 --> 00:50:21,000
But the run is one way or Nova Alright, by what you do with that, just like the 2,000, what you have right now you have 2 column, as input.

409
00:50:21,000 --> 00:50:24,000
The second caller is all out the first column of the group indicator.

410
00:50:24,000 --> 00:50:27,000
So remember, in the 2 sample it has. we have 2 groups.

411
00:50:27,000 --> 00:50:33,000
There are 0, 1, one or 2. Now we have as many as we like, So in this case we have 4 group right, 1, 2, 3, 4.

412
00:50:33,000 --> 00:50:37,000
So the spreadsheet 10 can take care of at most.

413
00:50:37,000 --> 00:50:45,000
5 groups. If you only have 4 group you just you know, remove the the group label here, so that i'll analyze more groups right?

414
00:50:45,000 --> 00:50:54,000
And then, once you put the data here, you'll label the book here, you automatically compute the group mean standard deviation sample size, and then give you the Nova table.

415
00:50:54,000 --> 00:50:59,000
Right. This is the sum of square Msp. Ms. Oh, this is Msp. M. S.

416
00:50:59,000 --> 00:51:04,000
E. another statistic p value. It also conducts the pairwise comparison.

417
00:51:04,000 --> 00:51:09,000
Right. This is the 5 group. This is a 5 group against each other, Right?

418
00:51:09,000 --> 00:51:13,000
Each group mean each group standard deviation. This is the P.

419
00:51:13,000 --> 00:51:17,000
Value compared a versus B. hey versus C. a versus B.

420
00:51:17,000 --> 00:51:22,000
And so on, and so forth. These are from 2 sample tasks. therefore these are not after confrontation.

421
00:51:22,000 --> 00:51:26,000
Have to think about. You know. how do you do running corruption here.

422
00:51:26,000 --> 00:51:31,000
I'll be redes. Well, not with the last b sorry scratchy.

423
00:51:31,000 --> 00:51:34,000
You can read this right, You know. also type in the meal instead of division yourself, like people.

424
00:51:34,000 --> 00:51:44,000
Just read this up comments. So this scratchy might be help you for your

425
00:51:44,000 --> 00:52:00,000
Oh, you're analysis so any question before we move on to groups

426
00:52:00,000 --> 00:52:09,000
If you're saying something i'm not hearing you

427
00:52:09,000 --> 00:52:19,000
Not. Is anyone saying anything? Okay? Because we are approaching time to take a break. right?

428
00:52:19,000 --> 00:52:24,000
So I was thinking, Make the break our room time a little longer.

429
00:52:24,000 --> 00:52:27,000
Let's say 35min, all you can take great any time in between.

430
00:52:27,000 --> 00:52:40,000
Now your life, and then, after 35min does come back and so he's basically 25min plus 10min break take the break as you'll need and Then we'll come back to God to discuss it again you

431
00:52:40,000 --> 00:53:01,000
know, feel free to stay in the main main hall. if you like to work by yourself.

432
00:53:01,000 --> 00:53:05,000
The question in the chat is that does the spreadsheet have a problematic production.

433
00:53:05,000 --> 00:53:16,000
You need no it doesn't so you have the thing about how to do the software on your captioning yourself

434
00:53:16,000 --> 00:53:20,000
Okay.

435
00:53:20,000 --> 00:53:35,000
If there's no more question. I will open the great caller rooms, is for 42 35min, including a brain.

436
00:53:35,000 --> 00:54:05,000
See you in the bread

437
01:29:49,000 --> 01:29:54,000
Okay. hello, Everyone Welcome back. I hope you had the enough time to work on.

438
01:29:54,000 --> 01:30:03,000
It also took a break that. Now, get together and review what we did.

439
01:30:03,000 --> 01:30:08,000
So this is about this paper on the neighborhood of the advantage to do with.

440
01:30:08,000 --> 01:30:12,000
How do you ask for the disease? there's a big table there?

441
01:30:12,000 --> 01:30:18,000
The table, while i'm gonna focus on so first question ask about our computer, observe mean difference.

442
01:30:18,000 --> 01:30:25,000
You know, alcohol consumption between the individual with and without Cvd: So that has to be.

443
01:30:25,000 --> 01:30:38,000
Regarding this particular alcohol assumption grams and the mean standard deviation given here and in this with them without Cvd Cvd group.

444
01:30:38,000 --> 01:30:55,000
So You also ask you if you can order p but I'll i'll explain why so anyone wants to answer this question.

445
01:30:55,000 --> 01:31:04,000
Sure. So speaking for group 2 here, so we we just we just took the there between the 2 numbers.

446
01:31:04,000 --> 01:31:10,000
So if you look at the tables, 4 point, 4, and 2.8.

447
01:31:10,000 --> 01:31:13,000
So the answer that we guys 1.6 grams awesome.

448
01:31:13,000 --> 01:31:20,000
So yeah, you're absolutely right. So just find the right number 4.4, minus 2, point, 8, 1.6 gram perfect.

449
01:31:20,000 --> 01:31:27,000
Yeah, that's the mean mean different I don't know what option between the 2 groups just need to identify the live number.

450
01:31:27,000 --> 01:31:33,000
Thank you, and the second one Now we have a hypothet testing h naught.

451
01:31:33,000 --> 01:31:39,000
Is There's no difference between the 2 groups Oh, totally is there is a difference between Google.

452
01:31:39,000 --> 01:31:42,000
What kind of path would you use to analyze this kind of data?

453
01:31:42,000 --> 01:31:50,000
Depend on what you learn in class, and why anyone I said that I would use the 2 sample to test.

454
01:31:50,000 --> 01:31:57,000
Because you have 2 different populations, and comparing their means absolutely awesome.

455
01:31:57,000 --> 01:32:02,000
Thank you. Yeah, exactly. This is the 2 group. Harran, 2 grummy with 2 samples.

456
01:32:02,000 --> 01:32:16,000
He has no Let's move on to question number 3. so perform the passage state into and state whether or not you would reject No, your favorite page, hey?

457
01:32:16,000 --> 01:32:21,000
Awesome. So this is the exact same reason why we ask you that you know the pivot.

458
01:32:21,000 --> 01:32:27,000
Because if you really run, it has based on this number I can i've got a different p vile Now it's my wife.

459
01:32:27,000 --> 01:32:29,000
So now the last we can conduct a 2 sample tasks.

460
01:32:29,000 --> 01:32:33,000
We need to figure out the numbers right, the mean standard deviation for each group.

461
01:32:33,000 --> 01:32:37,000
Also the sample size given here. So we have all this number.

462
01:32:37,000 --> 01:32:42,000
I will go ahead, do it. So you should get the same number.

463
01:32:42,000 --> 01:32:47,000
So in this case we don't have the raw data. So we just purely we take the 2 sample key test brushy.

464
01:32:47,000 --> 01:32:54,000
We just fuel in the 6 numbers what we got is 4.4 here, and 2.8 here. Understand?

465
01:32:54,000 --> 01:33:02,000
The deviation. also in the table, 18 point, 2 and 8.0, hey?

466
01:33:02,000 --> 01:33:07,000
In point 2, 8.0 under the sample size i'll just type it.

467
01:33:07,000 --> 01:33:14,000
Here I distribute 38, 64 for the first group, 2 32 for the second group.

468
01:33:14,000 --> 01:33:24,000
I have a paper my, a different screen. So once you plug in all this number, everything is just spell automatically right. Got the difference. 1.6.

469
01:33:24,000 --> 01:33:36,000
We just that, and the standard error for this difference. key statistic above 2, and the pivot left 1 point o 5, an accommodate level doesn't include 0. right?

470
01:33:36,000 --> 01:33:39,000
So the pivot will be 0 point 0 0 8 so that's what we got.

471
01:33:39,000 --> 01:33:45,000
So back here because the p-value is 0 point 0, 8, last, 10.0, 5.

472
01:33:45,000 --> 01:33:52,000
So the conclusion would be rejection now, right in favor Oh, that's hard.

473
01:33:52,000 --> 01:34:03,000
Okay? and you know, this is reasonably strong evidence, but less than point or why I say reasonably, strong evidence showing there is a difference between the 2 population.

474
01:34:03,000 --> 01:34:08,000
So that should be what you've got if you got a different answer.

475
01:34:08,000 --> 01:34:18,000
You know all your questions. Please let me know So you might wonder why you know the paper have a different Q.

476
01:34:18,000 --> 01:34:24,000
Id actually this key value was insignificant, A lot of the point all fine.

477
01:34:24,000 --> 01:34:35,000
So you had to read carefully under the table or you know Sometimes there's a matter section in the paper, but nevertheless, they say under the table they show that P.

478
01:34:35,000 --> 01:34:41,000
Value determined by Kaisworth. has the categorical, variable, and the crusco wall is test for continuous variable.

479
01:34:41,000 --> 01:34:45,000
So that's their statement of method they used to get a pivot.

480
01:34:45,000 --> 01:34:52,000
So essentially we haven't talked about any of this and we will not actually talk about possible live law as in the course that require a different course.

481
01:34:52,000 --> 01:34:55,000
Are we going to talk about pies work? as well kind of garbage in the next matter?

482
01:34:55,000 --> 01:35:01,000
So essentially we will compare different groups 2 or more. if the outcome is continues.

483
01:35:01,000 --> 01:35:07,000
We learned use to sample key tasks for anova right, but i'll call me categorical binary.

484
01:35:07,000 --> 01:35:11,000
We'll talk about use case for test we're gonna talk about the next sector.

485
01:35:11,000 --> 01:35:14,000
But here the outcome is actually continuous because it's ground.

486
01:35:14,000 --> 01:35:20,000
So we would use key to sample tasks, but they use something for the crystal.

487
01:35:20,000 --> 01:35:24,000
Wallace test, and that is a version of the so called non-power metric test.

488
01:35:24,000 --> 01:35:28,000
So. just very briefly, this is outside of skull.

489
01:35:28,000 --> 01:35:33,000
The key has, and Nova they all based on this normal assumption.

490
01:35:33,000 --> 01:35:41,000
Nevertheless, right, even if the data is not normally still based on normal distribution, is a parametric distribution.

491
01:35:41,000 --> 01:35:48,000
It's characterized by me on the standard deviation so everything is parametric. but because they are parameters of the population.

492
01:35:48,000 --> 01:35:51,000
So everything based on that is called the parametric tab.

493
01:35:51,000 --> 01:36:05,000
So the keyhead is parametric path and parametric past pass has advantage in the sense that they are simple and the more powerful now that sometimes people just they don't prefer parametric pass especially for a small

494
01:36:05,000 --> 01:36:09,000
sample they they they are not willing to us normal distribution.

495
01:36:09,000 --> 01:36:17,000
So they develop different kind of tests. for example, they're so called, non-power magic test, where they they just do not assume any distribution for the data.

496
01:36:17,000 --> 01:36:22,000
You might wonder if You do that some assume any that you're gonna help. You can have the test it turned out, you know they're they could.

497
01:36:22,000 --> 01:36:27,000
They're very smart mathematician or statistician They figure out ways, right?

498
01:36:27,000 --> 01:36:30,000
So essentially they are. this whole family of non parametric path.

499
01:36:30,000 --> 01:36:36,000
I'll be happy. They can do similarly like to Sample multiple sample, and the 2 sample version is called Wilcox and Test.

500
01:36:36,000 --> 01:36:41,000
The module sample version is called the course of water stuff, because they are non parametric.

501
01:36:41,000 --> 01:36:44,000
They don't really care the exact value of the data point they only take the order.

502
01:36:44,000 --> 01:36:50,000
So they run the data point and they take the order. So the idea is, I say, suppose we have 2 group.

503
01:36:50,000 --> 01:36:56,000
I suppose we have difference. I have no difference I don't know regards your distribution.

504
01:36:56,000 --> 01:37:00,000
If the 2 group have no difference, they way you mix the 2 group together.

505
01:37:00,000 --> 01:37:05,000
You should see the order of the membership from the 2 group are sort of uniform, right?

506
01:37:05,000 --> 01:37:15,000
So basically, if you put a 2 group together and the older them, learn from the smallest, the largest, you should see the 2 group, the you know the data point from each group up here sort of randomly, right?

507
01:37:15,000 --> 01:37:21,000
But you'll see most of the see the point from the first group appear in the beginning of the order.

508
01:37:21,000 --> 01:37:25,000
Most data point of the second group here at the end of the order.

509
01:37:25,000 --> 01:37:28,000
Then you should see you should conclude they are different. No, there is a difference. right?

510
01:37:28,000 --> 01:37:32,000
So that kind of concept they actually they can develop path for those.

511
01:37:32,000 --> 01:37:36,000
Although this is beyond the scope of this course, But you know if you take a nonprofit matrix the this day.

512
01:37:36,000 --> 01:37:42,000
Course they'll teach you this, but you know you know you want sometimes right.

513
01:37:42,000 --> 01:37:46,000
This author they conduct. This is non parameter version test for all of this, but all the key value here.

514
01:37:46,000 --> 01:37:52,000
They conduct is a grisical wallace task if there's a continuous They didn't do to have to sample.

515
01:37:52,000 --> 01:37:57,000
You have to So they're a profound cost of mutual calls.

516
01:37:57,000 --> 01:38:02,000
Right. So the to the Tpas will be more powerful if the data really normal.

517
01:38:02,000 --> 01:38:13,000
Yeah, mostly. Okay, the data are not hugely non normal i'm the The cruisco wall is past the non parametric test will be more robust because they don't assume normality.

518
01:38:13,000 --> 01:38:20,000
Or they will be less powerful. Give the data is, you know, close to normal, or something like that.

519
01:38:20,000 --> 01:38:24,000
So the fact that they got enough Instagram P. value.

520
01:38:24,000 --> 01:38:28,000
There are many explanations with that so first of all different tasks.

521
01:38:28,000 --> 01:38:34,000
We'll give you a different answer right sometimes slide it different sometimes very different.

522
01:38:34,000 --> 01:38:37,000
This In this case I would say it's very different which is red.

523
01:38:37,000 --> 01:38:42,000
Most okay, they give you a slightly different answer. so qualitatively still the same in this particular case.

524
01:38:42,000 --> 01:38:54,000
It could be that you know their past you know at the lower power, because they they don't do not use parametric assumption given the large sample size, their power should be very high.

525
01:38:54,000 --> 01:38:59,000
So I I think that explanation may you know is less likely to be true.

526
01:38:59,000 --> 01:39:14,000
It's more likely to be true in my imagination. is that this 2 distribution, indeed defer by the mean, but somehow they they have some some sort of a characteristic.

527
01:39:14,000 --> 01:39:21,000
Now normal, with a skill tail something like that. make them h insignificant when you when they conduct this.

528
01:39:21,000 --> 01:39:25,000
So you could say the non parametric pass our test of the medium.

529
01:39:25,000 --> 01:39:32,000
But that's the sort of a non rigorous way of saying that they test meeting, but they also test the overall shape of the population.

530
01:39:32,000 --> 01:39:36,000
So it's really hard for me. to imagine how can you have 2 distribution different by the mean.

531
01:39:36,000 --> 01:39:41,000
But still they fail, you know, fail to reject the not parametric version.

532
01:39:41,000 --> 01:39:47,000
I still don't know how that happened. but it is what it is, so they failed to reject by the the key test.

533
01:39:47,000 --> 01:39:51,000
We check. So basically take a message that different tasks might give you a different P. value.

534
01:39:51,000 --> 01:39:56,000
That's also a case when for example you'll collect your data, you run the test.

535
01:39:56,000 --> 01:40:07,000
I it's insect I have if you run a whole bunch of other paths with the hope that you find one of them reject, and that's the wrong way of doing it, because you are doing multiple testing to some degree right so.

536
01:40:07,000 --> 01:40:17,000
The best practice would be. Think about the past you're gonna use before you even see your data. Not way. You really guarantee your pipeline already right in this particular paper.

537
01:40:17,000 --> 01:40:21,000
They I I guess the author didn't really care because they conduct so many.

538
01:40:21,000 --> 01:40:26,000
It has right, and most of them are significant. So one of them failed to reject is no big deal to them right.

539
01:40:26,000 --> 01:40:30,000
It's not their like a most important finding of this paper anyway.

540
01:40:30,000 --> 01:40:39,000
Right. but nevertheless, if this is the only finding of their paper, and they got this, and they then they try to keep us, and then they reject them out.

541
01:40:39,000 --> 01:40:47,000
That's not a very good practice i'll see that's a bad practice, because you know you you should not keep manipulating your data.

542
01:40:47,000 --> 01:40:52,000
Try to run past. You want to pick on one and stick to it and that's the best approach.

543
01:40:52,000 --> 01:40:56,000
So that's how I want to say about this proof Crystal Wallace past.

544
01:40:56,000 --> 01:41:07,000
If you want to learn more, you have to take the non parameters, that is, of course, you know they will tell you everything about this kind of thing, but nevertheless, many of the past week. We learned this course actually all the paths we're learning

545
01:41:07,000 --> 01:41:14,000
the parts are parametric right but they're not primary reversion of them which is slightly mortal boss.

546
01:41:14,000 --> 01:41:32,000
Most likely that's powerful honey question regarding this this exercise, so far

547
01:41:32,000 --> 01:41:39,000
Okay, Now let's repeat from one to 3 but replace alcohol consumption with try closer lives. right?

548
01:41:39,000 --> 01:41:46,000
So essentially we move on from this box with this box and this case, you know, you can still stay.

549
01:41:46,000 --> 01:41:51,000
The know about this being, these 2 grouping are equal, the alternative to doing the I equal.

550
01:41:51,000 --> 01:41:56,000
The group difference is this number minus that number, and then standard deviation.

551
01:41:56,000 --> 01:42:01,000
And T. vio here there are a few items, I mean.

552
01:42:01,000 --> 01:42:05,000
If you run through some of details, the occupied will also be highest in the beginning.

553
01:42:05,000 --> 01:42:24,000
Okay, i'll just do it very quickly. so what you do here is, you know, typing the other number 100.2, 1, 1, 4 point, one and the 53.9, and the 61 I think that's it your Q*.

554
01:42:24,000 --> 01:42:32,000
Will also be highly significant. So in this case the 2 2 different tasks give you the same conclusion that's most of the time.

555
01:42:32,000 --> 01:42:43,000
I'm still could I understand Why? they got it you I don't like that, because you know, they have low power samples that should give them high power, and apparently these 2 group have different mean right?

556
01:42:43,000 --> 01:42:46,000
So how come they didn't pick it up this test for people to test it?

557
01:42:46,000 --> 01:42:51,000
And pick it up is still a mystery to me. but with other raw data I cannot really figure it out.

558
01:42:51,000 --> 01:42:56,000
But anyway, in this second test we got the same conversion as the one is showing the table.

559
01:42:56,000 --> 01:43:00,000
So we conclude there's strong evidence right because the Quebec is so significant.

560
01:43:00,000 --> 01:43:07,000
There's strong evidence showing the 2 group have if I mean in their type of classroom level.

561
01:43:07,000 --> 01:43:13,000
Any questions so far

562
01:43:13,000 --> 01:43:20,000
I want to move a little fast, because this is still to sample key test right? The next one will be on one way and over.

563
01:43:20,000 --> 01:43:28,000
Okay, now, let's so again alright p equals 0 point 0 0 1.

564
01:43:28,000 --> 01:43:35,000
So we reject It's not right you have to you know you in the real paper you have to spile off.

565
01:43:35,000 --> 01:43:44,000
What does that mean? Okay? Oh, so now let's move on to Number 5, right?

566
01:43:44,000 --> 01:43:57,000
So now that I know how to say there's no difference. in me alcohol consumption between individually in 3 levels of neighborhood disadvantage versus there is a different So here we are talking about this 3 numbers

567
01:43:57,000 --> 01:44:04,000
because That's the column of the neighborhood disadvantage highest Manchester medium dissolved this landing right?

568
01:44:04,000 --> 01:44:07,000
I'm gonna have 3 group therefore I have 3,000,003 standard Division.

569
01:44:07,000 --> 01:44:13,000
So anyone who wants to tell me what cash should we use here, and why?

570
01:44:13,000 --> 01:44:25,000
Yeah, I could do that question. So the appropriate statistical test to use would be in a Noba test, because we are analyzing the levels of variance within 3 groups.

571
01:44:25,000 --> 01:44:36,000
So since the hypotheses are analyzing the differences between the means for alcohol, consumption between individuals in more than 2 groups for the levels of neighborhood at disadvantage then we can assume that

572
01:44:36,000 --> 01:44:39,000
I know the test would be appropriate to use perfect.

573
01:44:39,000 --> 01:44:50,000
Thank you very much. Yeah. Second person with their thoughts

574
01:44:50,000 --> 01:44:56,000
Okay, yeah, you're absolutely right. because we are comparing group mean of 3 groups.

575
01:44:56,000 --> 01:44:59,000
One will Nova will be, though, you know.

576
01:44:59,000 --> 01:45:05,000
Thing to use. There are many assumptions. One will know why I just you know.

577
01:45:05,000 --> 01:45:11,000
Did I mention, because I don't wanna go into dial level detail, for example, it does zoom each group to be normal.

578
01:45:11,000 --> 01:45:17,000
And furthermore, it's. actually assume each group have the same standard deviation, right? and even a 2 sample pizza.

579
01:45:17,000 --> 01:45:21,000
They're a different version. Some of them assume that each group have the same standard deviation.

580
01:45:21,000 --> 01:45:33,000
Some assume they have different standard deviation so on so forth. I just don't want to spend too much time talking about, because for each of them we've got different formula right If you take up you know more detail and more qualities to do of course.

581
01:45:33,000 --> 01:45:36,000
they're gonna go into the math and you know showing all of those in this course.

582
01:45:36,000 --> 01:45:41,000
I want to focus on the the interpretation so different phone calls.

583
01:45:41,000 --> 01:45:44,000
But that's how you'll know there there are many different version of different tasks.

584
01:45:44,000 --> 01:45:47,000
They have different mathematical, probably the consumption behind

585
01:45:47,000 --> 01:45:52,000
But nevertheless, one way and mobile would be the right one to use here, because we are analyzing 3 group.

586
01:45:52,000 --> 01:46:00,000
Okay, hi there, mean. And now conduct the path. So what we do is we go back to the paper.

587
01:46:00,000 --> 01:46:10,000
Look at the 3 number here means that the deviation for each group, and also the sample size here for the each tool.

588
01:46:10,000 --> 01:46:16,000
So instead we this time we use the one way or nova scratchy, and we type in the 3 group because we don't have the group.

589
01:46:16,000 --> 01:46:20,000
D we just take it out. We type the mean standard deviation samples.

590
01:46:20,000 --> 01:46:29,000
As for each tool, so the number we would get, Let me double try.

591
01:46:29,000 --> 01:46:35,000
Yeah. 4, 5.1, 4. Let me do it, Mara, quickly.

592
01:46:35,000 --> 01:46:44,000
Then 4 first group, 5.1s group 4 third group and That's a division would be 16.4 12.4. Yeah.

593
01:46:44,000 --> 01:46:49,000
If you hide it, you are spreadsheet alongside, you know, double chat and the correct any.

594
01:46:49,000 --> 01:46:56,000
Then you got differently. 1352, 1432, 1312.

595
01:46:56,000 --> 01:47:02,000
So it happened it's 9 numbers everything else will be how hello there, Accordingly, Right.

596
01:47:02,000 --> 01:47:14,000
The important number to see is this P value. You should have 0 point 0, 4, 3, and back in the paper they got 0 point 0 6.

597
01:47:14,000 --> 01:47:18,000
So this is again, is the case. Different tests give you a different conclusion.

598
01:47:18,000 --> 01:47:25,000
You know the pivotal themselves right? 0 point, 0, 6 and 0 point 0, 4, 3 are not that different.

599
01:47:25,000 --> 01:47:26,000
But nevertheless one of them is above 0. Point 0.

600
01:47:26,000 --> 01:47:36,000
5, one of the below. so using one. we don't know about re-rejected now by using their close coal, Wallace path, they failed to reject them all.

601
01:47:36,000 --> 01:47:43,000
So this is again to remind you that we are only making probabilistic conclusions.

602
01:47:43,000 --> 01:47:53,000
Nothing we say, we conclude will be absolutely true, so we do not prove to know.

603
01:47:53,000 --> 01:47:59,000
We do not prove the alternative. we just make the decision based on the data.

604
01:47:59,000 --> 01:48:04,000
Okay, so it's totally normal, not due to the randomness in the data due to the different test.

605
01:48:04,000 --> 01:48:11,000
We we choose, we might get different answers for the same question of interest.

606
01:48:11,000 --> 01:48:15,000
Yes, there is a I I have a question. So

607
01:48:15,000 --> 01:48:19,000
They did the Cisco Wallace test, and we are doing a no one.

608
01:48:19,000 --> 01:48:28,000
So the results are different. So So my question is like what's more relevant, because probably probably I mean in the paper.

609
01:48:28,000 --> 01:48:42,000
They're like the it's not normal and it doesn't fall in it doesn't have similar standard deviate standard deviation, or something. So that's why they dig the Because if we are doing the one we know when our is also

610
01:48:42,000 --> 01:48:50,000
different. So So at the end of the day, which one is more relevant like in a real case scenario, it's really, I mean very good question.

611
01:48:50,000 --> 01:48:53,000
But it's really hard to say because different tests they have different assumption.

612
01:48:53,000 --> 01:48:59,000
They focus on different things. So what we all know about indeed, make this normal assumption.

613
01:48:59,000 --> 01:49:07,000
Nevertheless, was this sort such thing that assumption is irrelevant, because central name theorem always kick you.

614
01:49:07,000 --> 01:49:19,000
So there's a another assumption. Why, we'll all make I didn't mention is that they assume the 3 group have equal me equal standard deviation, so they could have different mean but they always have equal standard deviation, which could

615
01:49:19,000 --> 01:49:35,000
be a issue here, because the 3 group, you know doesn't have the same standard deviation right by usually this do not hurt you in the sense that this will lower your power, but rather instead of the increase of type and error nevertheless, there are tests

616
01:49:35,000 --> 01:49:40,000
you know, which can handle different standard deviation so you could argue?

617
01:49:40,000 --> 01:49:43,000
Why would I know about doesn't it's not very applicable here?

618
01:49:43,000 --> 01:49:49,000
Because you have, you know, this equal standard division assumption is failed Right?

619
01:49:49,000 --> 01:49:53,000
Well, I i'm not fully convinced that I will increase your type of error rate.

620
01:49:53,000 --> 01:49:56,000
The other thing is that it's a cruise covalent past right?

621
01:49:56,000 --> 01:50:03,000
So different tasks, although they both Harvey the same null, have on the same you know.

622
01:50:03,000 --> 01:50:06,000
The 3 group was the same, actually they're all hypothetically different.

623
01:50:06,000 --> 01:50:13,000
The the one we all know i'll assume the 3 mean are the same, while the standard deviation are the are different.

624
01:50:13,000 --> 01:50:18,000
Oh, are the are the same, and all that that you're doing on normal right under these assumptions of 3 distributed already.

625
01:50:18,000 --> 01:50:23,000
This, but they are all normal, and this Prosco Wallace.

626
01:50:23,000 --> 01:50:27,000
They, assuming a 3 distribution, are the same, not necessarily normal.

627
01:50:27,000 --> 01:50:33,000
So they? it seems like they have the same no but they don't really have the same. No, similarly it seems like they have the same alternative.

628
01:50:33,000 --> 01:50:42,000
Actually they don't. the the alternative hobbies while we know our zoom, all 3 distributions are still normal with equal standard deviation, but with possibly different mean.

629
01:50:42,000 --> 01:50:46,000
And this Crystal Wallace has a different attorney.

630
01:50:46,000 --> 01:50:55,000
They basically say they have different distribution. So when you have it will mathematically slide it from me inside the different alternative, you just have different power of detecting different kind of thing. right?

631
01:50:55,000 --> 01:51:02,000
One pass is trying to detect difference in mean the other passes try to detect difference in other things of the distribution.

632
01:51:02,000 --> 01:51:13,000
That's why? they have size different power So it's really hard to say which one right or wrong. that I mean you raise a very good question, but it's very hard to answer those are correct I see nothing wrong with either.

633
01:51:13,000 --> 01:51:27,000
Of them. Okay, So you're, saying like we have to detect we have to select statistical tests on our requirements like if you wanna compare the mean, or if you want to do something else, Yeah, yeah, you're right I mean in

634
01:51:27,000 --> 01:51:35,000
this particular case, because one way I know about assume equal standard deviation, and because the 3 group has send me a different standard deviation.

635
01:51:35,000 --> 01:51:39,000
So I would think Crossover Wallace probably is of the past year.

636
01:51:39,000 --> 01:51:49,000
Same reason. But here But last of them usually because actually there is a version of keypas that actually find those different standard deviation. Yeah, it's it's more technical.

637
01:51:49,000 --> 01:51:57,000
When you talk about, you have to really look at a different test and make your subjective judgment, which one might be slightly better by the at the point.

638
01:51:57,000 --> 01:51:59,000
You're comparing different tests without looking at a p Viole.

639
01:51:59,000 --> 01:52:01,000
It's hard to say, but once you start comparing the past?

640
01:52:01,000 --> 01:52:05,000
Well, what kind of people are you doing? Logical testing? Yeah.

641
01:52:05,000 --> 01:52:10,000
So the better approach would be actually think via path before you'll see your data, and then stick to it.

642
01:52:10,000 --> 01:52:19,000
So in this case there's nothing wrong. that they did what they did, or they did, alternatively, while we analyze, I also see nothing wrong, either.

643
01:52:19,000 --> 01:52:29,000
Oh, yeah, I guess your phone Thank YouI understand this whole course is somewhat sometimes very confusing.

644
01:52:29,000 --> 01:52:40,000
I'm trying my best. but I I I I met it is confusing from time for time, just because the nature of statistics and the complexity you know, of all the tools people put all there right.

645
01:52:40,000 --> 01:52:45,000
So. I I spoke with a student yesterday at the office hour. you know.

646
01:52:45,000 --> 01:52:50,000
I have to admit, You know it took me many years to get used to this physical thinking.

647
01:52:50,000 --> 01:53:04,000
And you know, coming from a math background, many people we will find statistics very hard to understand, and I intuitively, so statistically use all the math.

648
01:53:04,000 --> 01:53:07,000
But the physician i'm at my patient they have different way of thinking right.

649
01:53:07,000 --> 01:53:15,000
You must. Something is either right or wrong. there's no no way arguing. But is that this is something can be both right around at the same time.

650
01:53:15,000 --> 01:53:23,000
So just not, you know, very math flavor. So you should have to take time for you to fully get the the type of thinking.

651
01:53:23,000 --> 01:53:29,000
Okay, So one question in the chat is that would you Apply the bomb from any direction here. that's a good question.

652
01:53:29,000 --> 01:53:39,000
That's where we are going to in the second So here. let's look at all one way, or nova Sorry this is our one way, Nova, based on this Keyby.

653
01:53:39,000 --> 01:53:43,000
We would reject the know which, says the 3 group, are not all equal.

654
01:53:43,000 --> 01:53:50,000
But to really figure out which way they're not equal we need to do the pairwise comparison which is given here to have 3 p values.

655
01:53:50,000 --> 01:53:53,000
This is a pivot. Compare Group, a versus B.

656
01:53:53,000 --> 01:53:57,000
These are pivotal comparison, group, hey, versus c.

657
01:53:57,000 --> 01:53:59,000
And this is the pivotal comparison. Group B.

658
01:53:59,000 --> 01:54:04,000
Versus c.

659
01:54:04,000 --> 01:54:15,000
Let me see whether this yeah I want to look at a formula to see whether the P. value has been corrected for bomb from Ronnie, which I don't.

660
01:54:15,000 --> 01:54:27,000
It doesn't look like from the phone number okay let's Assume this pivotal are not corrective about Ronnie, which means they're these are the raw key value from 2 sample key hat So you See

661
01:54:27,000 --> 01:54:41,000
that these pivotal is significant by cell. This is a party month, and this is signed by itself, However, because these people are not. after adjusting profile for audi corruption, and the fact that we are comparing 3 past

662
01:54:41,000 --> 01:54:47,000
year, so the real color we should use would be equals 0 point 0, 5 divided by 3.

663
01:54:47,000 --> 01:54:54,000
So our our cut off should be 0 point 0, 1 7. Is that so?

664
01:54:54,000 --> 01:54:58,000
Looking at this, cut up, this one is still significant, but this one is no longer significant.

665
01:54:58,000 --> 01:55:17,000
So using this set of post hot path we would conclude that there's a significant difference between group one and 3, but not being one of 2, or 2, 1 3. so that's how we do about for any correction so essentially the conclusion

666
01:55:17,000 --> 01:55:28,000
will be, we find the difference among the 3 group, as specifically, we find a difference between group one and 3, which is not surprising, because group one has the highest amute.

667
01:55:28,000 --> 01:55:43,000
Oh, no sorry. A group, 2 on the 3 group, 2 had a high 3, I mean Group 3 has a law with me, and although you know, group, one also has the lowest mean, He had the larger standard deviation, therefore more randomly so lack of lack of

668
01:55:43,000 --> 01:55:47,000
a Oh, evidence right? But group of 3 had a smaller standard deviation.

669
01:55:47,000 --> 01:55:52,000
Therefore, you know, higher power to detect the difference between 2 and 3.

670
01:55:52,000 --> 01:56:15,000
So any question here

671
01:56:15,000 --> 01:56:22,000
Okay, No, let's come back right so once we did the path. We got a P.

672
01:56:22,000 --> 01:56:36,000
Equals 0 point 0 4 Where was it 3? So reject h not.

673
01:56:36,000 --> 01:56:55,000
And then post poppoth. So one versus 3 group, 3, Okay, Okay, thanks.

674
01:56:55,000 --> 01:57:02,000
So much. I you know I I do the sleep well that That's the excuse.

675
01:57:02,000 --> 01:57:04,000
Okay, First, thank you for that bond for any correction.

676
01:57:04,000 --> 01:57:12,000
You divided the P. value by the number of groups there. So this is I.

677
01:57:12,000 --> 01:57:26,000
This is I divide a 0 point 0 5 alpha level by final number Okay, So a a alternatively or equivalently, you could multiply the P value by 3 and compared to 0 point 0 5 that's

678
01:57:26,000 --> 01:57:32,000
the same thing. But, President, I like to do the the way I did so.

679
01:57:32,000 --> 01:57:40,000
This is 0 point 0, 5 over 3. and there is a question in the chat.

680
01:57:40,000 --> 01:57:44,000
Why isn't there a calculation to compare c.

681
01:57:44,000 --> 01:57:48,000
With, there is right, so you have to look at the row versus column.

682
01:57:48,000 --> 01:57:53,000
So this is hey versus B. This is a versus C.

683
01:57:53,000 --> 01:57:59,000
And this is B versus C. So this one is the comparison between B and C.

684
01:57:59,000 --> 01:58:09,000
I know this is a little hard to read, but because we have so many comparison. so that's the way it is a range

685
01:58:09,000 --> 01:58:26,000
Any other question you do explain the I can't finance it with bonfie harry fraction So you divided by 3, divided by 2, I divided by 3, because i'm conducting 3 tests here.

686
01:58:26,000 --> 01:58:31,000
So the bomb foronic correction is not specific To post hopcasts of the nova.

687
01:58:31,000 --> 01:58:35,000
Apply to any time you conduct multiple tests. So you can conduct multiple test.

688
01:58:35,000 --> 01:58:43,000
You want to control the overall hardware array you just divide 0 point 0 5 by the polynomial test you conduct.

689
01:58:43,000 --> 01:58:50,000
Okay, So it's on by the number of groups we have the number of tests that we have to conduct.

690
01:58:50,000 --> 01:58:52,000
Yeah, it's by the number of tests not a number pool.

691
01:58:52,000 --> 01:58:59,000
Okay, 3. Thank you. Oh, well, no worries. Yeah, it just turns out that we have 3 group and 3 paths.

692
01:58:59,000 --> 01:59:03,000
Right. but you have 4 group we'll have 6 past year so we're gonna divide by 6 rather than pool.

693
01:59:03,000 --> 01:59:13,000
I noticed that Yeah, hello

694
01:59:13,000 --> 01:59:19,000
Right i'm not i'm not touching the p value there you're just searching your your threshold right?

695
01:59:19,000 --> 01:59:33,000
Because where we called in the threshold at point 0 5, so we're dividing that by however, many groups and then, and or the alternative would be then to multiply the between the group one by 3, as compared to the the

696
01:59:33,000 --> 01:59:40,000
key value. Yeah, you can think about if you modify this by 3.

697
01:59:40,000 --> 01:59:43,000
That's still last time 0 point 0 5 right but this will be over.

698
01:59:43,000 --> 01:59:55,000
So the conclusion would be the same regard as what you take they just you know I don't like multiply, because if you want about this, do you have a key value of 3, and that doesn't make sense how only V between 0

699
01:59:55,000 --> 02:00:05,000
and one. So you have the cap it at once so that's one extra step. you do So that's Why, I just divide 0 point 0 5 by 3 and again 3 is the number of tests.

700
02:00:05,000 --> 02:00:22,000
Not the number group here. Okay, Hold on a second

701
02:00:22,000 --> 02:00:28,000
So now number 7 is do the same thing but for alcohol assumption. but replace the alcohol assumption.

702
02:00:28,000 --> 02:00:38,000
With the client So what we do is that we're going to use the data 3 this 3 piece of data, right?

703
02:00:38,000 --> 02:00:42,000
And let me do quickly. But you can. You should be to follow yourself.

704
02:00:42,000 --> 02:00:50,000
We just type in different numbers here, I mean in this this 6 blocks, because the samples I stayed the same.

705
02:00:50,000 --> 02:01:04,000
So this one will be one o 1 point, 7, 1 0, 3.5, 97.

706
02:01:04,000 --> 02:01:17,000
Point 5, and the standard deviation will be 53, point, 6, 57, point, 5 and 51.6.

707
02:01:17,000 --> 02:01:22,000
So assembles. I stay the same I have to drag it a little bit to see the numbers.

708
02:01:22,000 --> 02:01:31,000
But this number really don't really matter too much What you really want to see is the pvil now 0 point 0 1 3 again this last time.

709
02:01:31,000 --> 02:01:34,000
Point Oh, 5. when you look at paper is 0 point 0, 1 4.

710
02:01:34,000 --> 02:01:43,000
These are 2 are very similar, right? so this is a case where 2 different paths give you almost the same key value, which is actually the common pins.

711
02:01:43,000 --> 02:01:49,000
So don't worry too much about things like this this is really weird in over my career.

712
02:01:49,000 --> 02:01:55,000
I have yet to see things like this happen. actually even once I don't know why they got it.

713
02:01:55,000 --> 02:02:00,000
This is what happened most like this, and this usually different tasks, usually with large sample.

714
02:02:00,000 --> 02:02:19,000
Right. I mean you either, you have evidence you don't have evidence So now, having the number, we reject, the now and then going back to the spreadsheet, the 3, you know, again we have 3 paralyzed paths but

715
02:02:19,000 --> 02:02:22,000
comparatively, is the bombaroni corrected, cut off value.

716
02:02:22,000 --> 02:02:27,000
Only one of them is second. Everything is still groupie versus group c.

717
02:02:27,000 --> 02:02:32,000
Right this one. what's the good by itself but when you compare with the balcony? Crack!

718
02:02:32,000 --> 02:02:39,000
If you buy, cut out this. only this group is that negative.

719
02:02:39,000 --> 02:03:00,000
So your conclusion would be no Again, he equals 0 point 0 1, 3 reject takes not. and also for the post hot past.

720
02:03:00,000 --> 02:03:06,000
Again do we find difference in cool I think it's group B.

721
02:03:06,000 --> 02:03:16,000
Versus group, c. whatever you call them right, so that one will be the medium versus log.

722
02:03:16,000 --> 02:03:28,000
This advantage. Cool? Yeah. which personally I don't find to be very meaningful or trustworthy, because it's, if not neighborhood.

723
02:03:28,000 --> 02:03:35,000
The advantage play a role. Then the height is what i'm in the group should have the pies in.

724
02:03:35,000 --> 02:03:42,000
I mean, depend on how they define it. I I I I personally take this as the highest invited group.

725
02:03:42,000 --> 02:03:46,000
This is the lowest. This one is cool i'm thinking this group has the highest.

726
02:03:46,000 --> 02:03:51,000
This advantage, therefore, is the highest impact, right? but not that would lastly, only find difference among the other 2.

727
02:03:51,000 --> 02:03:59,000
That's number one number 2 is that you know personally I don't find this finding to be very convincing is that Yep.

728
02:03:59,000 --> 02:04:05,000
Neighborhood is advantage has association between among you know with alcohol or triggers.

729
02:04:05,000 --> 02:04:14,000
Right that's true. Then you will imagine association to be you have a plan which means going from a high to low.

730
02:04:14,000 --> 02:04:20,000
You know the level should have a tr but now if you see the means it's 4, and then go up to 5.1.

731
02:04:20,000 --> 02:04:28,000
Then go down again to 4, which just hardware in target for me right.

732
02:04:28,000 --> 02:04:35,000
It's hard to think about. Why, the time you know is the lowest in a in a 2 extreme group, or highs in the middle.

733
02:04:35,000 --> 02:04:39,000
Some of the things here right one o one goes, one o 3 go up, then go down right again.

734
02:04:39,000 --> 02:04:45,000
So, even though they got a key value small enough it's hard for me to believe.

735
02:04:45,000 --> 02:04:51,000
You know there is a real associated going on There It's probably going through some other confining thing in the end.

736
02:04:51,000 --> 02:05:00,000
You'll find this association about association is in my opinion very interpretable.

737
02:05:00,000 --> 02:05:14,000
Any thoughts along this side. I noticed that the the highest that the values for highly disadvantaged medium and low, that varies for other categories as well.

738
02:05:14,000 --> 02:05:25,000
Right like in like in college education, in the third in the some college, and then in the third column we have 26 point, 2 and 31.6.

739
02:05:25,000 --> 02:05:31,000
I mean it's similar the medium is similar to low or in the third one.

740
02:05:31,000 --> 02:05:39,000
It's it, you know So I feel like there are some areas where it increases.

741
02:05:39,000 --> 02:05:44,000
And then decreases as well. Yeah, Yeah, I I agree with some of them.

742
02:05:44,000 --> 02:05:52,000
Make sense. For example, 8 right there is a there's a linear plan going down, and the gender there is a linear trial. education.

743
02:05:52,000 --> 02:06:00,000
There is actually in your client Oh, yeah, I mean missing family income.

744
02:06:00,000 --> 02:06:08,000
It goes up in the middle and then goes down. which one the missing family in come the last one.

745
02:06:08,000 --> 02:06:16,000
Yeah, Oh, missing, I I think these are missing value so it doesn't really each income level.

746
02:06:16,000 --> 02:06:20,000
They actually is quite clear. so that's trustworthy I could imagine, You know.

747
02:06:20,000 --> 02:06:25,000
Hi, this is a group. they have more low income, right? So all of this makes sense.

748
02:06:25,000 --> 02:06:36,000
By the time you talk about this alcohol consumption it's not a linear china anymore. So which means even if you got a key value, small, even if you show this association, I don't really believe the association has any

749
02:06:36,000 --> 02:06:48,000
meaning for invitation. So basically, you know that's that's that before you even see your data make a sound for yourself, that will make the finding more sort of a drop rippy right?

750
02:06:48,000 --> 02:06:55,000
If you just okay. So I I have a thing. But yeah, go ahead.

751
02:06:55,000 --> 02:07:05,000
Okay, but it I mean I understand we're looking for association, and that even allows us to to look for a trend or to anticipate the trend. right?

752
02:07:05,000 --> 02:07:10,000
But the question is actually asking about causality, Right? So yeah, right.

753
02:07:10,000 --> 02:07:14,000
I. I figured this study itself is not designed to tell us causality.

754
02:07:14,000 --> 02:07:21,000
To begin with, and then if we were looking for a an association that was significant.

755
02:07:21,000 --> 02:07:28,000
I agree with you that we would hopefully see it trending throughout the different groups. right?

756
02:07:28,000 --> 02:07:31,000
So there would be sort of a linear type relationship between

757
02:07:31,000 --> 02:07:40,000
Whatever we thought the issue was so for example, the triglycerides where we would expect that there would be a sort of linear difference between the high, the middle, and the low.

758
02:07:40,000 --> 02:07:45,000
And we're not really seeing that here, Yeah thank you yeah you're absolutely right.

759
02:07:45,000 --> 02:07:48,000
Causality is the question we're going having to right definitely.

760
02:07:48,000 --> 02:07:52,000
This is a you know, observational data. So there are some usual there.

761
02:07:52,000 --> 02:07:58,000
But here i'm just saying just by the associate itself this 2 association, all those significant.

762
02:07:58,000 --> 02:08:04,000
This is not very clotworthy, cause I would imagine if they are real something going on.

763
02:08:04,000 --> 02:08:07,000
Then this will be the highest this will be the highest but that's not a case.

764
02:08:07,000 --> 02:08:13,000
Yeah, thank you very much. So one thing I want to point out is that you can think about this particular study right?

765
02:08:13,000 --> 02:08:18,000
This group. They design a study, collect a whole lot of data, and then a lot, Because, you know, the data is big, right.

766
02:08:18,000 --> 02:08:30,000
They have thousands of individual and tens of variables so in the end. They they break down the cohort into this 2 group or this 3 group, and they analyze every single variable.

767
02:08:30,000 --> 02:08:35,000
They've got a key value, and the either 2 or not through their supplies.

768
02:08:35,000 --> 02:08:39,000
Many of the pivotal to the significant which is somewhat interesting.

769
02:08:39,000 --> 02:08:48,000
But there's a downside of this kind of a on study in a sense that look at how many times they are conducting right for every variable.

770
02:08:48,000 --> 02:08:53,000
There cannot be 2 tests. so overall this table there are probably 50 different tests.

771
02:08:53,000 --> 02:09:08,000
I leave 30. So, because the fact that they are conducting 30 pass in this table, which means that even if there's nothing going on, they will buy some p vitamin.

772
02:09:08,000 --> 02:09:10,000
Well, they find a lot more, then, you know one or 2.

773
02:09:10,000 --> 02:09:15,000
So, which means that you know the the the significance are probably real.

774
02:09:15,000 --> 02:09:19,000
But but there is a multiple typing issue going on here right you good.

775
02:09:19,000 --> 02:09:36,000
You know that that goes back to some people believe you should have one single hypnotist, and only one for your son. and that way right when you call the day o clock, conduct nothing analysis not always hopefully unbiased, and

776
02:09:36,000 --> 02:09:41,000
you know, control a lot of times. People don't really have a single How about this?

777
02:09:41,000 --> 02:09:52,000
They just assume something's going on and they collect the data, many data and run many tests and this is actually a good case, because they they output the table with many past.

778
02:09:52,000 --> 02:09:57,000
They are, they are not hiding the number that they are doing I mean assuming that's all the path they did.

779
02:09:57,000 --> 02:10:07,000
But sometimes, you know, think about there could be another group, they collect all the data, run many tests, and then just report a one which turned out to be significant.

780
02:10:07,000 --> 02:10:12,000
And that would be more problematic, because not only they conduct many tests, but also they are not telling you.

781
02:10:12,000 --> 02:10:18,000
They are conducting many tests Right? So this is actually a good case where at least they're telling you. there could not be many past right.

782
02:10:18,000 --> 02:10:30,000
They do not have a single habit in their mind but Nevertheless, they you know they did what they can So what I'm trying to say is that don't take one key value too seriously.

783
02:10:30,000 --> 02:10:35,000
Because that number, depending on a lot of things right so what's actually more about.

784
02:10:35,000 --> 02:10:44,000
I think this study is that they collect the data. They analyze the data, but it would be much better if they publish the data so that we can look at it ourselves.

785
02:10:44,000 --> 02:10:49,000
In a way we are interesting

786
02:10:49,000 --> 02:10:56,000
So the end? any question?

787
02:10:56,000 --> 02:11:06,000
Okay. So now again, you know I think someone just adds, right.

788
02:11:06,000 --> 02:11:12,000
So there is association, but it's very suspicious and the not not that part.

789
02:11:12,000 --> 02:11:16,000
And also here is asking, Do you think there is a causal link, right?

790
02:11:16,000 --> 02:11:26,000
And the answer is, You know, this is observational data

791
02:11:26,000 --> 02:11:34,000
Observational data, bye and hello, causal completely.

792
02:11:34,000 --> 02:11:43,000
Go ahead. So and the main issues that there could be confounding So that's what we'll go back here right?

793
02:11:43,000 --> 02:11:54,000
So post we find a association or difference between tribal authorized and alcohol

794
02:11:54,000 --> 02:11:58,000
Well, you know, even ignoring the fact that this association is suspicious.

795
02:11:58,000 --> 02:12:04,000
Suppose there is association. What other confounders might be causing this association?

796
02:12:04,000 --> 02:12:07,000
We call it a spurious association, so the association is not really there.

797
02:12:07,000 --> 02:12:10,000
By It's called by some component so what other factors right?

798
02:12:10,000 --> 02:12:22,000
If you think about it right, just moving to a different neighborhood, will not the rapidly change your what might be associated with both of these animals?

799
02:12:22,000 --> 02:12:32,000
I think one who say something

800
02:12:32,000 --> 02:12:43,000
You may have a link between these variables based on kind of overall things like, Do they have medication available to them to lower their triglycerides?

801
02:12:43,000 --> 02:12:46,000
Do they have medication? compliance what's their overall diet?

802
02:12:46,000 --> 02:12:55,000
What does their diet consist of? What are stress levels that could potentially be causing the treatmentless rights to be higher.

803
02:12:55,000 --> 02:13:01,000
So there's a number of confounding kind of absolutely thank you very much.

804
02:13:01,000 --> 02:13:06,000
Yeah, they're they're like just so many right they're even even their age.

805
02:13:06,000 --> 02:13:15,000
Their you know, gender, their education, their income they're you know smoking, and everything of this on the table probably are associated with everything else, right?

806
02:13:15,000 --> 02:13:24,000
So anything can be a compound. So it's a very sophisticated, you know, although, network, and then that's why they collect so many data, because all of this could be confounders, right?

807
02:13:24,000 --> 02:13:31,000
If you really want to study association between disadvantage and some outcome, you need to collect all the possible compound and address them in your model.

808
02:13:31,000 --> 02:13:36,000
In the particular, for example, multiple regression models gonna talk about next week.

809
02:13:36,000 --> 02:13:40,000
That's why they collect all these data, because one of these I have a visual study like this.

810
02:13:40,000 --> 02:13:43,000
Not only you need to clap data on exposure and outcome and interest.

811
02:13:43,000 --> 02:13:57,000
I don't know whether they they really have a outcoming interest here per se. but you also need to collect all the data on the other potential confounders, which includes pretty much all variable here. right behavior diet, you know, access

812
02:13:57,000 --> 02:14:02,000
to health care all of those things right? any other comments.

813
02:14:02,000 --> 02:14:06,000
I think one of the things we also know about these populations.

814
02:14:06,000 --> 02:14:12,000
Is that there's a different density of certain types of stores right?

815
02:14:12,000 --> 02:14:19,000
So in like low socioeconomic areas there's a higher density of liquor stores than there are like in in suburban areas. right?

816
02:14:19,000 --> 02:14:23,000
And so the availability of alcohol and things that nature are very.

817
02:14:23,000 --> 02:14:28,000
And then the types of grocery stores right inside.

818
02:14:28,000 --> 02:14:33,000
The quality of no use, and things of that nature we know also very depending on the socioeconomics that that area.

819
02:14:33,000 --> 02:14:40,000
So there's so many other things that into this besides what they measured absolutely.

820
02:14:40,000 --> 02:14:47,000
Thanks so much. Yeah, you have access to different kind of tools. And you know, basically the whole neighborhood that the issue of this whole thing is that you know, observational study.

821
02:14:47,000 --> 02:14:51,000
Like I said right could could be subject to confounding.

822
02:14:51,000 --> 02:14:54,000
So one way to gather around that is that you?

823
02:14:54,000 --> 02:14:58,000
You want to collect data on all the confounders and the customer.

824
02:14:58,000 --> 02:15:02,000
But the problem of this particular kind of study is that it's just too complicated.

825
02:15:02,000 --> 02:15:07,000
Right. there are almost even a number of potential components. it's almost impossible to collect all of them.

826
02:15:07,000 --> 02:15:12,000
Right. So in yeah, you you can. You can, you know, collect some confounding and make up a story.

827
02:15:12,000 --> 02:15:16,000
But people always suspect there's some other factor you're missing right?

828
02:15:16,000 --> 02:15:23,000
So this i'm in the game so in the end you know you can do a study like this, you know, hand.

829
02:15:23,000 --> 02:15:30,000
Something might be going on by eventually. you. You need to be smart of designing some sort of a other type of study, right?

830
02:15:30,000 --> 02:15:36,000
Either intervention or other way, to collect an independent piece of evidence for people to really gain confidence.

831
02:15:36,000 --> 02:15:44,000
On the conclusion you find from this. but nevertheless, this is still very, you know, important piece of study, where you know, at least, you can contribute the problem also.

832
02:15:44,000 --> 02:15:51,000
The data side right and the draw some, you know, interest, and also serve as a pilot kind of study which you know.

833
02:15:51,000 --> 02:15:55,000
Raise some high quality for future, larger or other kind of well design.

834
02:15:55,000 --> 02:15:59,000
Study to follow up and validate what you find right so it's by South is still useful.

835
02:15:59,000 --> 02:16:06,000
But it just cannot be the only piece of evidence which to draw the fundamental conclusion.

836
02:16:06,000 --> 02:16:17,000
Any common.

837
02:16:17,000 --> 02:16:21,000
Okay, I know it's time for break let's let's you know.

838
02:16:21,000 --> 02:16:26,000
Look at a very last question and complete that, and then take a break so based on the result showing table one right.

839
02:16:26,000 --> 02:16:32,000
If you were to sponsor a program focused on health promotion, you highly disadvantaged neighborhood.

840
02:16:32,000 --> 02:16:41,000
What factor would you want the program to address? right? I I want you to feel still just to tell me what you want to focus on these.

841
02:16:41,000 --> 02:16:47,000
I think that yeah, go go back. So if you look at the title of this paper, I mean, I take it back.

842
02:16:47,000 --> 02:16:52,000
Yeah, actually have a primary outcome of interest which is cardiovascular, you know.

843
02:16:52,000 --> 02:16:58,000
I think disease right. So if you really want to reduce, how do you invest?

844
02:16:58,000 --> 02:17:02,000
You know. Rest what fact you want to address in this neighborhood.

845
02:17:02,000 --> 02:17:07,000
Anyone

846
02:17:07,000 --> 02:17:19,000
I can share what I thought so obviously systemic issues are totally at play at lower, like disadvantaged neighborhoods. so keeping that in mind.

847
02:17:19,000 --> 02:17:23,000
But just looking at the table, I looked at across neighborhood levels where P.

848
02:17:23,000 --> 02:17:31,000
Was significant, and then also saw that relationship that we talked about where the lower disadvantage was highest than M.

849
02:17:31,000 --> 02:17:34,000
Medium was in the middle and highest was the lowest.

850
02:17:34,000 --> 02:17:41,000
So I looked at diabetes and hypertension status, and I saw that relationship kind of the relationship.

851
02:17:41,000 --> 02:17:47,000
I just described as being a big issue and lower or higher disadvantage groups.

852
02:17:47,000 --> 02:17:53,000
So. I thought I would focus on potentially doing things to target that.

853
02:17:53,000 --> 02:17:59,000
So potentially talking about what are good foods to eat for diabetes?

854
02:17:59,000 --> 02:18:13,000
Right, so like healthy foods, and then also ways to increase, like physical movement, to overall improve health outcomes like cardiovascular disease, since it looks like they have a have a relationship.

855
02:18:13,000 --> 02:18:24,000
Thank you. Thank you very much. Anybody else i'm gonna cheat here a little bit, because I do work in the emergency department in a urban environment.

856
02:18:24,000 --> 02:18:27,000
So. So this question is, this is a good one.

857
02:18:27,000 --> 02:18:41,000
But if you look at the table, the areas in which the there's sort of a significant number difference would be in like the diabetes status and the hypertension set if right, and so oftentimes those

858
02:18:41,000 --> 02:18:52,000
numbers are even underestimated, because the participation in in studies and surveys, and like highly disadvantaged populations, is often very low, Right?

859
02:18:52,000 --> 02:19:05,000
And so if I had money, and and in a program where I was trying to promote it would be aimed at those 2 things right, one actually figuring out if people have diabetes and then giving them access to medications

860
02:19:05,000 --> 02:19:09,000
to manage it. Same with hypertension. All the rest of this.

861
02:19:09,000 --> 02:19:17,000
The cardiovascular disease. is all down there also the downstream effects of poly control diabetes, and for the controlled hypertension, right?

862
02:19:17,000 --> 02:19:35,000
So if we're able to one identify those risk factors in the population, and then to give them access to control them, then we would, by definition, decreased the outcome that they were looking at which was Thank you very much.

863
02:19:35,000 --> 02:19:45,000
I'm married very young. Nice summary yeah I I think the question, you know. promise that single point of view is actually asking, so find out the variable.

864
02:19:45,000 --> 02:19:57,000
We are both strongly associated with cvd remember here, and the strongly associated with neighborhood. So those are the variable might be, you know, playing a role, link the 2, right? and if by chance that variable, is something that work on that'll

865
02:19:57,000 --> 02:20:07,000
be good. right? So it turned out a diabetes. I suppose you point out diabetes strongly associated with cad, also strongly associated with the neighborhood, and also hypertension.

866
02:20:07,000 --> 02:20:14,000
So those are a good place to to work, huh? And also obesity to a lesser extent.

867
02:20:14,000 --> 02:20:18,000
Right. So if you look at things like try to operate, you know, held the out.

868
02:20:18,000 --> 02:20:29,000
There do not seem to be associated right with either with both, so those probably are not a place to call and calorie a little bit, but not much right.

869
02:20:29,000 --> 02:20:36,000
So smoking, Maybe But yeah, smoking by itself is a big issue.

870
02:20:36,000 --> 02:20:48,000
You want to work on the regard as and access to food things like those it's not even list on the table right? so which could be at the actually the original pause of all this will be CD diabetes have a patient

871
02:20:48,000 --> 02:20:52,000
right, but it's 90 on the table and so so there are many things right.

872
02:20:52,000 --> 02:21:12,000
Oh, you have a real call, Any other comments. Hi, dear.

873
02:21:12,000 --> 02:21:21,000
Okay, if that's the case, let's take a 10min break and continue at 11 for for the for the second, though. exercise.

874
02:21:21,000 --> 02:21:51,000
Thank you.

875
02:32:20,000 --> 02:32:26,000
Okay, that's a continue with 7 d nevertheless 7 D.

876
02:32:26,000 --> 02:32:30,000
Is not required to summit. So you saw me 7, c.

877
02:32:30,000 --> 02:32:41,000
And please do the quiz today, still tomorrow. So seventies continue with the exercise on the end medical expenditure data.

878
02:32:41,000 --> 02:32:48,000
Yesterday. we did the you know, analysis on on on sex and today we're gonna do our Us regions.

879
02:32:48,000 --> 02:32:53,000
So that and also regional difference, so that we will.

880
02:32:53,000 --> 02:33:01,000
You know, get used to a nova. So this time we also need to use the one way or no about table.

881
02:33:01,000 --> 02:33:11,000
But if you have used the table and modify the links the equations here, you need to, either, you know, go all the way back to the beginning, or you download the new table.

882
02:33:11,000 --> 02:33:18,000
So that is oh, sorry, Not this one actually this one this one.

883
02:33:18,000 --> 02:33:26,000
You can go all the way from the beginning. for you you' on the new table for this to work.

884
02:33:26,000 --> 02:33:34,000
Okay, so you'd like to do yourself. we have a little time.

885
02:33:34,000 --> 02:33:45,000
You. you like to spend some time doing yourself before we do it together as an exercise

886
02:33:45,000 --> 02:33:51,000
Any preference

887
02:33:51,000 --> 02:33:55,000
Hello over the time to work on our salesforce. We could for me, anyway.

888
02:33:55,000 --> 02:34:01,000
Awesome thanks. Oh, I I will! I will send out a, you know, breakout room for 15min.

889
02:34:01,000 --> 02:34:04,000
Of course people need to do it yourself. I know we've got to you know.

890
02:34:04,000 --> 02:34:10,000
Go over the right. you mean i'll have time to finish but I think you know all the exercise are pretty much similar.

891
02:34:10,000 --> 02:34:40,000
So you got a flavor. so how 50min, and then come back to God during the bit

892
02:49:55,000 --> 02:50:06,000
Okay, hope you get the chance to finish some of it now, let's do it together pretty straightforward.

893
02:50:06,000 --> 02:50:10,000
But you know it's kind of fun so today.

894
02:50:10,000 --> 02:50:21,000
We're gonna analyze this it's medical expenditure data again, the same for outcome as we did yesterday. Whole housecare expenditure to the morbidity.

895
02:50:21,000 --> 02:50:29,000
The mih by different from yesterday. when we analyze 2 group by gender.

896
02:50:29,000 --> 02:50:36,000
Today. we're gonna do it by good area saw those statistical tests.

897
02:50:36,000 --> 02:50:47,000
We're gonna use or parody is one way of Nobel because it's a multiple group compared the mean of different outcomes, bye.

898
02:50:47,000 --> 02:50:56,000
And no, you know. would it be all groups people means right.

899
02:50:56,000 --> 02:51:05,000
Alternative. Is that not our group right it's not the same as

900
02:51:05,000 --> 02:51:13,000
You know, all group have differently. So

901
02:51:13,000 --> 02:51:25,000
Now let's do it right so let's first look at house expenditure versus different us region, so i'll do it, and you can just follow along and the check whether you got the same answer because it's

902
02:51:25,000 --> 02:51:29,000
hard for you for me to ask you to to show me what you did.

903
02:51:29,000 --> 02:51:32,000
So what we did is I would go to the I Meanps data set.

904
02:51:32,000 --> 02:51:42,000
Now we use region as the group indicator. So we just copy this region and go back to the one way or Nova.

905
02:51:42,000 --> 02:51:53,000
We're supposed to hear. Hi! steve here, and then we go back Now we do total healthcare expenditure.

906
02:51:53,000 --> 02:52:04,000
Call them queue control, c. going back. So now we have group 1, 2, 3, 4, perfectly aligned.

907
02:52:04,000 --> 02:52:14,000
Right. So everything is already done quite straightforward so the full group. Let's Let's go back to see what the 4 groups are because that make it more interesting. so again, like a top of the last time.

908
02:52:14,000 --> 02:52:22,000
Right. you'll be more interesting and reliable if you form your hypothesis before you see the result. In this particular case.

909
02:52:22,000 --> 02:52:32,000
The 4 groups are northeast, midwest south and west, north, east, midwest, south in the last let's put it here.

910
02:52:32,000 --> 02:52:38,000
This is north east mute left sales and the left.

911
02:52:38,000 --> 02:52:43,000
So these are the total health care expansion. the mean actually differ quite a bit.

912
02:52:43,000 --> 02:52:51,000
By the highest is 8, the lowest is 4, quite a bit kind of surprising to me, and the northeast has the highest.

913
02:52:51,000 --> 02:52:58,000
Midwest has a second highest, and sauce, and the web has the third and the fourth, and they are, You know the gap is actually quite big.

914
02:52:58,000 --> 02:53:07,000
If you think about these are in thousands of dollars, Right? Is that what you got

915
02:53:07,000 --> 02:53:14,000
Okay, Yes, yeah. I suppose the answer is, yes. So oh, apparently was again with this to our sample size.

916
02:53:14,000 --> 02:53:19,000
We have very hot homework. The P model is highly significant.

917
02:53:19,000 --> 02:53:27,000
This is means less than 0 point 0 0 one It's very high, I think more interesting would be the paralyzed comparison.

918
02:53:27,000 --> 02:53:31,000
But to really look at his number, you need to first look at a poverty correction.

919
02:53:31,000 --> 02:53:38,000
So here the corruption will be you know it's like caught off right pop up.

920
02:53:38,000 --> 02:53:45,000
Value will be 0 point 0, 5, divided by their total number of pass is 6. He does.

921
02:53:45,000 --> 02:53:51,000
Paul Bro: Okay. Sorry. equals 0 point 0 5 divided by 6.

922
02:53:51,000 --> 02:53:57,000
So we should look at 0 point 0 0 8 that's the call out value for the pairwise comparison.

923
02:53:57,000 --> 02:54:02,000
So Everyone's B. which is north east the versus midwest no second, but a difference.

924
02:54:02,000 --> 02:54:06,000
Well, otherwise. there's a gonna make difference here here here here.

925
02:54:06,000 --> 02:54:11,000
I'm here right also, and also gonna make a difference between last and songs.

926
02:54:11,000 --> 02:54:22,000
So you can look at this for you centralized saying north East and the Midwest are the high High School group, and there's no significant difference between the 2 and South South West are the lowest 2, group and there are no

927
02:54:22,000 --> 02:54:26,000
cigarettes among the 2, but between the high group and the logo.

928
02:54:26,000 --> 02:54:40,000
Any pair there's a significant difference. I have to say the difference across different region is kind of a surprising and large to meet, and which I did not expect.

929
02:54:40,000 --> 02:54:47,000
I saw this data every year, but every time I do them I forgot, and I do not expect this, and I have yet to have a part of it.

930
02:54:47,000 --> 02:54:57,000
Why, this would be the case right you couldn't imagine I mean maybe there is that age difference across different regions.

931
02:54:57,000 --> 02:55:01,000
But it's hard to believe that's the case we can double check.

932
02:55:01,000 --> 02:55:20,000
So anyone might have any hypothetical. Why, this might be the case of the regional difference being so high

933
02:55:20,000 --> 02:55:33,000
Okay, So let's combat so the conclusion would be so we got a P.

934
02:55:33,000 --> 02:55:54,000
Last 1.0 0 one right reject it's not also have friends between north the North north, east, right boss, mid west nurses.

935
02:55:54,000 --> 02:56:00,000
What a sauce! awesome, so essentially their difference, you know.

936
02:56:00,000 --> 02:56:07,000
Between this this pairs

937
02:56:07,000 --> 02:56:14,000
Alright. Now the second one is total mobility. Now we can imagine, unless you know the last.

938
02:56:14,000 --> 02:56:21,000
You know this how's he spending time right which area might have the highest of the mobility?

939
02:56:21,000 --> 02:56:29,000
Let's take this column. We only need to simply Hi is the total mobility here.

940
02:56:29,000 --> 02:56:44,000
Now I see Hi Creek again, it's not something I would expect. but northeast midwest and south their mean are relatively high

941
02:56:44,000 --> 02:56:54,000
Are these a number? Now you guys go

942
02:56:54,000 --> 02:57:10,000
Sample size looks off right sample size are all let me see what's going on

943
02:57:10,000 --> 02:57:22,000
Nice. Okay, I I I'm. not very hold on a second. I believe I got this numbers, too. but I don't know if it's correct.

944
02:57:22,000 --> 02:57:28,000
Or no. Yeah, I got the same cool. Thank you. yeah, I think this is a car.

945
02:57:28,000 --> 02:57:47,000
So if you have different number double track? Yeah. So north, East, Midwest, and South are high in Co. comorbility and West are low, just like at least in this data set, people from the West are healthier relatively and

946
02:57:47,000 --> 02:57:53,000
mostly due to the Florida sample size.

947
02:57:53,000 --> 02:58:01,000
Nevertheless, you can you can see the the difference among these 3 group are really small.

948
02:58:01,000 --> 02:58:06,000
They're a lot that's Why, you know when you test them paralyzed all of them.

949
02:58:06,000 --> 02:58:15,000
Are using that again. So the difference really lies on between the West and versus the other 3.

950
02:58:15,000 --> 02:58:24,000
By the last singles out as the most healthy the east region. the other 3 are inequality sort of a

951
02:58:24,000 --> 02:58:29,000
Let me come her. With the last any question anybody got different?

952
02:58:29,000 --> 02:58:35,000
Answer.

953
02:58:35,000 --> 02:58:49,000
Okay, So yeah, this is for how i'm not gonna write on the rest about morbidity.

954
02:58:49,000 --> 02:59:01,000
The same thing right, re Jack, it's not on the west good friend from other regions.

955
02:59:01,000 --> 02:59:05,000
Okay, i'm not going to write on the rest but you know you got the way.

956
02:59:05,000 --> 02:59:11,000
What I meant so that's now move on to Bmi again.

957
02:59:11,000 --> 02:59:21,000
It's fun to actually think about which region. you might think is high, although all this metrics. but sometimes no, it just surprise you.

958
02:59:21,000 --> 02:59:30,000
Let's put it back here. The Bmi this time you know not so much a difference.

959
02:59:30,000 --> 02:59:40,000
But sauce is the highest, only slightly higher, and the west is the lowest.

960
02:59:40,000 --> 02:59:46,000
But I would say the difference among all of our nozzles striking if they compare with the other 2. Right?

961
02:59:46,000 --> 02:59:53,000
And okay, he's always very highest significant right Now look at the difference.

962
02:59:53,000 --> 03:00:01,000
Right. This is between north, east, and midwest although by itself, is less than point. Oh, 5, or you look at, you know Bunker only correctly call off this.

963
03:00:01,000 --> 03:00:10,000
It's not gonna be different. This is not is It not so really the difference are lying north north east versus south.

964
03:00:10,000 --> 03:00:20,000
And me the West versus versus what? and sauce versus last right?

965
03:00:20,000 --> 03:00:36,000
These 3 are second. Look at this call I have to say the difference here. I'm not so cute so personally i'm not so interested in, you know difference here, right, cause you have high power. Do the large sample size

966
03:00:36,000 --> 03:00:42,000
the different South. it's arguably you know meaningful let me help you.

967
03:00:42,000 --> 03:00:45,000
My difference of one, maybe. Still, you know, clinically meaningful.

968
03:00:45,000 --> 03:00:54,000
But thinking about the sampling bias right personally i'm not so convinced that there's a strong regional difference here.

969
03:00:54,000 --> 03:01:01,000
And finally let's go to the age

970
03:01:01,000 --> 03:01:09,000
Oh, you can think about what read, and might have the highest reach age in on average.

971
03:01:09,000 --> 03:01:16,000
So it turned out that the this 3, like north East, is the highest in average age, and the Midwest and the South are similar.

972
03:01:16,000 --> 03:01:28,000
West is the youngest, so that partially explains why the young, the the you know the West is the healthist because they're younger in this data set right.

973
03:01:28,000 --> 03:01:32,000
It could reflect the whole world population. Now the West is younger.

974
03:01:32,000 --> 03:01:40,000
If the data is representative, why did not, then this definitely age is a confounding factor for the call mobility, right?

975
03:01:40,000 --> 03:01:49,000
Because they are younger by 2 2 years right. so a party there they will be healthier on average, and also they're probably gonna spend last a health care expenditure.

976
03:01:49,000 --> 03:01:55,000
But that doesn't explain the the south spend the same again.

977
03:01:55,000 --> 03:02:00,000
Keyvado is always, How is it gonna be done? And the difference?

978
03:02:00,000 --> 03:02:07,000
Now again, live between South West and other 3 regions, right?

979
03:02:07,000 --> 03:02:24,000
Just like mobility. Only this comparison, any question or any comment

980
03:02:24,000 --> 03:02:41,000
So when you take determine the cutoff value sorry for the for the comparison, you're dividing * by the number of different permutations that not just by the total number of group there, are but like the

981
03:02:41,000 --> 03:02:54,000
amount of comparison there are between the 2 loops or the how many, cause like I I think I would have thought I always thought that it was gonna be by the groups which is 4, but here there's 6 different permutations

982
03:02:54,000 --> 03:03:02,000
of the 2 for our income when you compare to so That's why So you'll always divide by them.

983
03:03:02,000 --> 03:03:06,000
It's not the permutator is the number of test you want.

984
03:03:06,000 --> 03:03:12,000
So here we do 6 pairwise tests, by 6.

985
03:03:12,000 --> 03:03:18,000
Okay. And so, alternatively, you would multiply those by 6 if you would go the opposite direction.

986
03:03:18,000 --> 03:03:27,000
Yeah, yeah, Okay, So this this again this pump Ronnie is a general approach to deal with multiple testing.

987
03:03:27,000 --> 03:03:39,000
It's not specific. to Anova so just you know know about the number of past, you know, is is parallelized between group right, But in in the Java being example, because you run 20 different callers of javaine

988
03:03:39,000 --> 03:03:42,000
you really should divide the 0 point 0 5 at one.

989
03:03:42,000 --> 03:03:50,000
There so so essentially always divide by the number of tests you want

990
03:03:50,000 --> 03:04:04,000
Any other questions.

991
03:04:04,000 --> 03:04:10,000
Now very quickly. We, you know, do the raft So the rest.

992
03:04:10,000 --> 03:04:16,000
Here is the same outcome. But now the group are defined based on ratio.

993
03:04:16,000 --> 03:04:26,000
Difference rather than regional difference. So all we need to do is we'll go back to the data. sheet.

994
03:04:26,000 --> 03:04:36,000
This is the regional right now. we take a different column. right right and then paste it here. No, we we can't look at an age first, right we, instead of changing it.

995
03:04:36,000 --> 03:04:42,000
So one thing we need to do first of all there's an age of group of 5 here, so we need to add a 5 here.

996
03:04:42,000 --> 03:04:50,000
Otherwise group is no longer exist. And then, second thing we got we do with how we go back to the dictionary, recognize the colony.

997
03:04:50,000 --> 03:04:55,000
This is the case when you you know, when you do is carry categorical data, you could you?

998
03:04:55,000 --> 03:05:01,000
How the data used to numbers right 1, 2 3 4 5 but that doesn't mean you should analyze them as numbers just here.

999
03:05:01,000 --> 03:05:06,000
1, 2, 3, 4, 5, 5, 5 different races apparently make no sense to all of them. Right?

1000
03:05:06,000 --> 03:05:11,000
This is not the orbital data, nominal data, nominal kind of garbage.

1001
03:05:11,000 --> 03:05:17,000
They make no sense for convertible numbers. So the 1, 2, 3, 4, 5 are just coding, coding scheme.

1002
03:05:17,000 --> 03:05:28,000
Right. So really you should never analyze this kind of data as continuous, even though they were told you, as numbers, always free time at a category call, and also usually interpret that as category right so now we have

1003
03:05:28,000 --> 03:05:37,000
hispanics wide black, patient and others it's not like why black agent others let me go back.

1004
03:05:37,000 --> 03:05:45,000
I have to I don't pay them right it's very wide, black, patient, and audit other all multiple.

1005
03:05:45,000 --> 03:05:59,000
So usually, you know, we like to include just for completeness right when we analyze racial data that's always a group of other or multiple. my personal, I find that you know this barrier to interpret first of all this group always has a

1006
03:05:59,000 --> 03:06:02,000
small sample size. Therefore you have a lack of power, Right?

1007
03:06:02,000 --> 03:06:06,000
So whatever difference you find Europe out of book failed to find here could be load lead to low.

1008
03:06:06,000 --> 03:06:10,000
How that's number one number 2 is that even if you find the difference.

1009
03:06:10,000 --> 03:06:14,000
It's very hard we in target because you know it's other or multiple right?

1010
03:06:14,000 --> 03:06:18,000
How do we interpret that? So you can live in here?

1011
03:06:18,000 --> 03:06:21,000
But just just keep in mind. The invitation is always tricky.

1012
03:06:21,000 --> 03:06:28,000
But nevertheless we see the 4 groups right This in age we see the white group has the highest age.

1013
03:06:28,000 --> 03:06:33,000
Abroad, and this is way higher than, for example, Hispanic.

1014
03:06:33,000 --> 03:06:37,000
I I will leave all the other from now, on because I I personally, don't really know how to analyze them.

1015
03:06:37,000 --> 03:06:42,000
Probably interpret them right. You can still analyze them just already in perfect.

1016
03:06:42,000 --> 03:06:50,000
But the wide population has a much higher average age. There is many population, has the lowest average age, and the black and Asian are in the middle.

1017
03:06:50,000 --> 03:06:58,000
In this data set, and that will serve as a confounder to pretty much everything we talk about, you know, under the hate, right?

1018
03:06:58,000 --> 03:07:01,000
Because ages is a play, a big role in your house.

1019
03:07:01,000 --> 03:07:14,000
Carry Spanish or your your call mobility and the Bmi right? So it's always converted by age. and no, because the large sample size is again how high is the significant!

1020
03:07:14,000 --> 03:07:23,000
What a significant live, you know, along the Abcdp. right if you compare C.

1021
03:07:23,000 --> 03:07:29,000
Versus D. he's not saying anything which means you know black and Asian.

1022
03:07:29,000 --> 03:07:34,000
They are similar in age. and then, when you compare Group Y, some of them are significant.

1023
03:07:34,000 --> 03:07:46,000
But personally I can explain them right because it's other So let's just focus on this right Seems like why is significantly different from hispanic black.

1024
03:07:46,000 --> 03:07:53,000
And Asian on my signature from you know this is a really strange right.

1025
03:07:53,000 --> 03:07:59,000
Oh, why is Groupie okay? Sorry. Why is it a little bit different from all other groups?

1026
03:07:59,000 --> 03:08:04,000
But Hispanic is not a different from others but you know, because these are the 2 youngest.

1027
03:08:04,000 --> 03:08:08,000
But others we don't really know what race they they are like Anyway.

1028
03:08:08,000 --> 03:08:16,000
And the wide is the highest right. And see! on the d black on Asia are not so different here, right so on, so forth.

1029
03:08:16,000 --> 03:08:21,000
I think this just give you some detailed and paralyzed number of the comparison.

1030
03:08:21,000 --> 03:08:24,000
But just by looking at the meeting. you will see what's going on here, right?

1031
03:08:24,000 --> 03:08:34,000
They're very, quite a bit in the data center any questions so far

1032
03:08:34,000 --> 03:08:41,000
Alright. so let's come back and the second one would be last to.

1033
03:08:41,000 --> 03:08:47,000
Now, do go back to health, care, expenditure again.

1034
03:08:47,000 --> 03:08:59,000
Form some hypothesis and see what are the data Follow your gas

1035
03:08:59,000 --> 03:09:08,000
So see the wide population spend the most, and the the black population spend the second height.

1036
03:09:08,000 --> 03:09:15,000
An Asian third spend the least no you know the the others group spend the second half.

1037
03:09:15,000 --> 03:09:19,000
So again. it's hard to interpret cause we don't really know how what they are.

1038
03:09:19,000 --> 03:09:23,000
You know what what race they are, and the this is not so surprising.

1039
03:09:23,000 --> 03:09:28,000
Given the age distribution right? Why is the oldest Hi reef?

1040
03:09:28,000 --> 03:09:33,000
His background is so once you know the 8 you don't really find this difference. that's a problem.

1041
03:09:33,000 --> 03:09:40,000
I think the white probably remember, is on every 50 years old, and his Hispanic population was like 41 years old.

1042
03:09:40,000 --> 03:09:58,000
So the fact i'm 50 years old, spend more it's not as So again, it's highly significant across the group and there are some group wise comparison, some of the group are different any comma no how how

1043
03:09:58,000 --> 03:10:03,000
would you? in terms of the data, you know definitely from what I did or you know.

1044
03:10:03,000 --> 03:10:19,000
Similarly, I want to be a real cost

1045
03:10:19,000 --> 03:10:28,000
Okay, Now, let's quickly go to call mobility, although you know morbidity.

1046
03:10:28,000 --> 03:10:48,000
Copy paste here. see the the white populator has the highest and the black population has the second highest almost as high as one, and this difference cannot be explained by the age.

1047
03:10:48,000 --> 03:10:55,000
I i'm saying No, there's no difference you know I mean the what do I?

1048
03:10:55,000 --> 03:11:07,000
How do I put it? So the high comma value rate of the black populator cannot be explained by their age.

1049
03:11:07,000 --> 03:11:11,000
Only right because their age are similar to the Asian population.

1050
03:11:11,000 --> 03:11:20,000
But the Asian actually has the lowest mobility here almost as as low as his plan. You can see, the Hispanic population has low common ability, because they are younger right?

1051
03:11:20,000 --> 03:11:26,000
But remember that, you know age black and Asian population has a similar high age.

1052
03:11:26,000 --> 03:11:31,000
And why has the highest so there's you know considering this commaability and age?

1053
03:11:31,000 --> 03:11:37,000
There is something same as me. real difference going out here, although they could be explained by other polarities.

1054
03:11:37,000 --> 03:11:49,000
Right now. we only consider age. We haven't considered other things right? I think about. There could be like access to, you know, soldier economics status, you know, family income educational.

1055
03:11:49,000 --> 03:12:04,000
Many things right? sorry. How did How are you How are you inserting age into this? Yeah, i'm just trying to explain things with the contacts of age. And see now, age might be a fun, father you know, remember Yeah, you know

1056
03:12:04,000 --> 03:12:10,000
in the previous one, when we see yeah how's our expenditure, Why is the highest?

1057
03:12:10,000 --> 03:12:21,000
But why is also high in age so so basic association between expanding, trying to raise whole country could be explained by 8, right?

1058
03:12:21,000 --> 03:12:28,000
So we don't really know what's what's driving this could be simply because they are older.

1059
03:12:28,000 --> 03:12:32,000
But when we look at the Comor battery this cannot be.

1060
03:12:32,000 --> 03:12:46,000
This difference cannot be explained by age alone. that's what i'm trying to say right any any comment here

1061
03:12:46,000 --> 03:12:51,000
So there should be some other variable which can which is driving this difference right?

1062
03:12:51,000 --> 03:12:55,000
We don't know what they are but probably not as long right H.

1063
03:12:55,000 --> 03:13:05,000
Could explain some definitely. I'll probably want my all and There's a pivot here, and this you know paralyzed comparison.

1064
03:13:05,000 --> 03:13:13,000
Right. So I was just trying to say you know analyzing data like this is very, very tricky, because it's hard to make any conclusion, right?

1065
03:13:13,000 --> 03:13:22,000
Because there's so many variable play each other you could have a hypothesis that's Why, it's very crucial for you to have a part of this kind of data right?

1066
03:13:22,000 --> 03:13:25,000
If the data support your father, you should definitely buy it all.

1067
03:13:25,000 --> 03:13:31,000
Well always keep in mind there. it could be limitations. You are in your discussion, right? cause there.

1068
03:13:31,000 --> 03:13:38,000
The causal link is too complicated to be simplified, and the same whole, you know Abc.

1069
03:13:38,000 --> 03:13:41,000
A Cosby v. con. see that kind of a graph right?

1070
03:13:41,000 --> 03:13:53,000
There are many, many potential factors driving this difference. Any question

1071
03:13:53,000 --> 03:13:59,000
And finally let's look at bmi

1072
03:13:59,000 --> 03:14:10,000
So we put the bi here. you see, Actually, yeah, black is highest.

1073
03:14:10,000 --> 03:14:18,000
But honestly, this 3 group are sort of similar, right within a range of one, and the h is the lowest.

1074
03:14:18,000 --> 03:14:25,000
So this might explain why patient population here has a lowered call mobility right?

1075
03:14:25,000 --> 03:14:28,000
Together with the age, I gather with the Vmi.

1076
03:14:28,000 --> 03:14:41,000
But you know you are basically trying to make hobbies explanation after you see the result, which is highly unreliable. it's much, much more reliable. If you have a hypothesis beforehand Well.

1077
03:14:41,000 --> 03:14:54,000
That's why? i'm saying you's always helpful for you to form some apology, and expectation beforehand, because after you'll see the result you can always explain them I can always find out this one nature. Oh, this is this is might be

1078
03:14:54,000 --> 03:15:00,000
because of that right? We are very good at making those explanations.

1079
03:15:00,000 --> 03:15:05,000
Oh, how many hours before! see the result? You find you will be surprised very often.

1080
03:15:05,000 --> 03:15:13,000
Not that house

1081
03:15:13,000 --> 03:15:18,000
Okay, So again. it's highly significant right there's some difference.

1082
03:15:18,000 --> 03:15:27,000
I I find those are less interesting looking at the different, you know, looking at a difference among different the population and the put the gather, the different viral we look at right.

1083
03:15:27,000 --> 03:15:35,000
Probably the thing about why, but I think more more interesting to me is that the problem about it beforehand, and see whether the data is supposed to help?

1084
03:15:35,000 --> 03:16:03,000
I mean i'm i'm often very often supply any coma

1085
03:16:03,000 --> 03:16:09,000
Okay, Thank you. Then I I think this you know there's a pop about time for today.

1086
03:16:09,000 --> 03:16:15,000
I'm not going to start a new lecture we don't have them all time left, so please summit 7 C.

1087
03:16:15,000 --> 03:16:24,000
And the perform the a post like a quiz and tomorrow we're gonna stop like lecture 8 and the on the homepage.

1088
03:16:24,000 --> 03:16:30,000
There is a sprash sheet, you know. there are some scratchy here, 2 with our central limit.

1089
03:16:30,000 --> 03:16:34,000
Theorem who is combinable there's one with sample size and one sample.

1090
03:16:34,000 --> 03:16:40,000
T has essentially, you know, show you how how this link together, and also the power, and so on, and so forth.

1091
03:16:40,000 --> 03:16:50,000
If you're interested, I could demonstrate tomorrow, but you know for the sake of time, I will also skip that, so you can also take a look at by yourself.

1092
03:16:50,000 --> 03:16:59,000
You're very interested in if there's no more question you know that's part of the day and see you tomorrow morning.

1093
03:16:59,000 --> 03:17:04,959
Thank you.

