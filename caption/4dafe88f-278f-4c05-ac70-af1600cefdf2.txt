1
00:00:00,360 --> 00:00:04,060
How are your weekends? Yeah.

2
00:00:05,610 --> 00:00:08,840
I'm sorry about the video. I.

3
00:00:09,210 --> 00:00:12,490
I know as long, but there is so much to.

4
00:00:12,550 --> 00:00:17,310
To cover with multiple regression. I'd rather even if you didn't quite make it through the entire thing,

5
00:00:17,880 --> 00:00:22,920
I'd rather folks had the material available to them so that you can return to it if you need to.

6
00:00:23,190 --> 00:00:27,000
But we're also going to spend a lot of time as we go through today, kind of a working example.

7
00:00:27,270 --> 00:00:34,890
I want folks to look at this content and really try to process what we're doing rather than simply just running code,

8
00:00:35,550 --> 00:00:40,800
but understanding why we do the things that we're doing. Which is why, as I'm moving to the example,

9
00:00:40,800 --> 00:00:44,610
I think it would be a great thing if folks feel comfortable to pull that script

10
00:00:44,610 --> 00:00:49,860
down from R or from a canvas and upload it in our studio and walk along with me.

11
00:00:49,860 --> 00:00:55,830
The reason why. There's a few things that I'm not going to talk about here, but you're going to be doing in the background,

12
00:00:56,070 --> 00:01:01,680
including like some of the variable preparation that's going to be necessary in order to run even kind of a simple regression,

13
00:01:01,860 --> 00:01:09,540
just like a week or a week ago or two weeks ago. We'll do a whole lot of front work to finally get to the one analysis that we really want to run,

14
00:01:09,540 --> 00:01:15,329
which is just to equate or to regress one of our variables on a group of predictor variables.

15
00:01:15,330 --> 00:01:20,040
But we're going to get a lot into the output. We're going to think about some of the nuances.

16
00:01:20,040 --> 00:01:23,130
We're going to see how multiple regression is different than bivariate regression.

17
00:01:23,400 --> 00:01:29,280
We're going to hopefully see how that's interesting, a logical extension of ANOVA and even the correlation stuff that we did a couple of weeks ago.

18
00:01:29,460 --> 00:01:32,760
And this is going to be our launching point, our launching point for the rest of the semester.

19
00:01:33,450 --> 00:01:37,109
It's this critical. And I just want to say this.

20
00:01:37,110 --> 00:01:40,499
I just want to make sure that this thing is recording. All right.

21
00:01:40,500 --> 00:01:48,120
I'm sorry. Could be all right. So we'll start today with a five minute recap.

22
00:01:48,120 --> 00:01:52,100
Don't be fooled by this binary regression piece. We're going to introduce some of these kind of stickier,

23
00:01:52,140 --> 00:01:55,770
thornier concepts that maybe folks have not seen in a while or might be the very

24
00:01:55,770 --> 00:02:00,659
first time before we move into multiple regression when on a show of hands,

25
00:02:00,660 --> 00:02:04,110
how many folks made it through at least part way of the video this weekend?

26
00:02:07,400 --> 00:02:15,620
Catfish. Catfish. Okay. So I'm going to go ahead and start then with just general questions that I might not answer right this second,

27
00:02:15,980 --> 00:02:17,990
but I'm going to put them up on the board so I don't forget.

28
00:02:18,830 --> 00:02:26,600
And then I will absolutely either touch on them as we're moving through or at any point anything coming out that folks have some questions about or,

29
00:02:26,620 --> 00:02:31,910
you know, you'd like me to comment on specifically as we're moving through this example together.

30
00:02:35,600 --> 00:02:40,430
Yeah. I think what they are confused about is the role that residuals play some of the.

31
00:02:41,590 --> 00:02:49,990
Okay. Okay.

32
00:02:53,060 --> 00:02:57,200
I can read this summary this.

33
00:02:59,060 --> 00:03:03,889
Maybe this goes along with the. Yes, maybe it is.

34
00:03:03,890 --> 00:03:05,150
Good luck with that question.

35
00:03:05,150 --> 00:03:12,890
But looking at the regression kind of formula and just going through, I guess, just the brief of the variables for a brief overview.

36
00:03:14,150 --> 00:03:19,010
Tell me a little bit more a little more specificity. So I think like some of them, the residuals.

37
00:03:19,010 --> 00:03:24,260
I remember the video and had the statistical model of the regression model

38
00:03:24,260 --> 00:03:28,940
and then have like lines for each part of it describing what each piece was.

39
00:03:29,300 --> 00:03:36,490
And maybe just like going through that really briefly. Is great.

40
00:03:37,670 --> 00:03:45,950
Well, let's. Katie Yeah.

41
00:03:47,290 --> 00:03:51,310
I think it related to the first question or the very least.

42
00:03:52,780 --> 00:03:56,690
Mm. Okay.

43
00:03:57,660 --> 00:04:01,080
And specific. Are you just talking about like the machinery behind?

44
00:04:01,200 --> 00:04:06,590
I think we just don't. I didn't really understand that it was like the much behind.

45
00:04:07,400 --> 00:04:15,910
Some of the reasons that I didn't really understand. Look, if you got that much, you have to understand a lot more about all of us than you realize.

46
00:04:15,910 --> 00:04:26,750
But. Yeah. Okay. Hmm. Any other. That type of drink has boosted that threat.

47
00:04:27,950 --> 00:04:31,910
But this is helpful just so that I know if there are some sticking points that we can absolutely touch on.

48
00:04:32,330 --> 00:04:35,390
All right, um. Let's see here.

49
00:04:35,510 --> 00:04:39,420
Oh. Simpler function.

50
00:04:40,800 --> 00:04:44,820
What do you think? This. What's going on here? What's happening up here?

51
00:04:45,990 --> 00:04:49,500
What are we doing here? Object. Object. We're creating object.

52
00:04:49,510 --> 00:04:54,510
What's going to be in that object? US. Right.

53
00:04:54,510 --> 00:04:58,450
So if you were to type all this jazz and you don't do, everybody remembers anybody.

54
00:04:59,560 --> 00:05:04,480
I do think you're going to put your name up there. All right.

55
00:05:05,700 --> 00:05:10,260
If I if I if I create this object and I don't run this line, what happens?

56
00:05:14,180 --> 00:05:27,180
It's going to return for me. What do you predict? It's going to happen.

57
00:05:33,030 --> 00:05:39,120
Just one student. Yeah. Nine, six year old messages returned.

58
00:05:40,660 --> 00:05:50,880
There we are. Sharon, answer is good. This is a function of a very useful function in our what we think this function does.

59
00:05:56,400 --> 00:06:01,600
Hi. Which one? Family.

60
00:06:05,590 --> 00:06:19,210
I don't know for sure, but this is just. It draws from her object.

61
00:06:22,700 --> 00:06:26,960
Here's a sample, a random sample of size one.

62
00:06:27,680 --> 00:06:32,720
If we ever are not sure about what is what a function does, we can use this arguments command.

63
00:06:35,510 --> 00:06:41,270
It's going to tell us what the function is, what it's looking for, all this kind of information.

64
00:06:41,270 --> 00:06:44,840
So this is gonna be our object. This is the size that we're going to use.

65
00:06:45,170 --> 00:06:49,249
Replace is just going to tell us whether or not once we had once withdrawn something from our object.

66
00:06:49,250 --> 00:06:54,380
If we're going to put it back in and read sample or if it's going to be a sample without replacement.

67
00:06:54,800 --> 00:06:57,620
So if you know you don't want to draw out the same person twice,

68
00:06:58,010 --> 00:07:02,420
we would want to make sure that this is false, which looks like it's going to be the default.

69
00:07:03,320 --> 00:07:09,890
But we can always be explicit, for example, right there, and then you can continue on down.

70
00:07:10,460 --> 00:07:16,760
If you ran to help me out menu, you would see a whole bunch more information about a given particular function.

71
00:07:17,390 --> 00:07:27,320
And so here in line 11, Delaney us, walk us through that on.

72
00:07:27,800 --> 00:07:39,200
Oh, wow. Okay. On the sample. Is it just going to play over the border?

73
00:07:39,590 --> 00:07:43,010
Yeah, sure. It's going to take our object.

74
00:07:43,460 --> 00:07:54,080
All your names is going to draw 13 symbols randomly from those names without replacing your name is only going to come up once.

75
00:07:57,080 --> 00:08:05,510
Memorize this list here. Let's go. There we go. The flight school district superintendent wants to know whether underrepresented students

76
00:08:05,510 --> 00:08:09,860
in the district were exposed to violence at a similar rate of majority students.

77
00:08:10,610 --> 00:08:15,530
How do you what do you see up here? Tell me if you can help answer this question.

78
00:08:29,680 --> 00:08:37,540
Okay. Okay. Can you help her out, Amy? So we have three groups that are being compared.

79
00:08:37,750 --> 00:08:48,100
Mm hmm. And yes, you can use this to insert the question by looking at the names.

80
00:08:48,530 --> 00:08:53,600
Okay. Where are you finding those in the second column?

81
00:08:54,350 --> 00:08:59,000
So descriptive statistics tells us, yes, that we have the means for each one of these groups.

82
00:08:59,960 --> 00:09:06,170
Fabulous. But we don't want to just go by the means.

83
00:09:06,170 --> 00:09:13,220
What do we do next? We want to look at the.

84
00:09:19,000 --> 00:09:31,030
Oh. The next. So go past the descriptive, descriptive statistics and go down to the next line of value.

85
00:09:31,040 --> 00:09:39,640
Then look at the sum of squares and that means pairs in the F value to see if those differences matter.

86
00:09:40,090 --> 00:09:46,030
Okay. So you tell me. Or underrepresented students exposed to violence that had a similar rate as majority students.

87
00:09:54,260 --> 00:09:58,040
There is a difference. There is a difference, Eddie.

88
00:09:58,040 --> 00:10:05,240
Not because of the value.

89
00:10:05,900 --> 00:10:10,420
Fabulous. Oksana.

90
00:10:11,710 --> 00:10:16,040
Where. Where's that difference? You mean, where is the difference?

91
00:10:16,120 --> 00:10:22,720
Cohan said that there is a difference because of this. If yes, but between group air relative to the women, group air is quite large.

92
00:10:24,430 --> 00:10:30,310
So we know that group matters. But in what way does it matter which group or which groups are different from each other?

93
00:10:40,510 --> 00:10:48,280
You know, I'm trying to see if. Can you tell from here?

94
00:10:49,990 --> 00:10:54,040
How do you. Nope.

95
00:10:54,560 --> 00:10:59,630
Only after that. Yeah, that's right.

96
00:11:01,190 --> 00:11:04,909
So when we did before with the omnibus test, the overall ANOVA test.

97
00:11:04,910 --> 00:11:09,010
Right. Just tells us one thing. It tells us there is a difference between groups.

98
00:11:09,020 --> 00:11:12,790
If we really want to know which of these groups, we have a suspicion.

99
00:11:12,800 --> 00:11:16,430
Right. We see that this is the lowest mean followed by this by this.

100
00:11:16,430 --> 00:11:20,760
Right. So we might expect that this group and this group are different or who knows about these two, right.

101
00:11:20,810 --> 00:11:32,780
We don't know until we do some form of post-hoc test, in which case we would look at us output like this right here and decide one who's different.

102
00:11:42,140 --> 00:11:48,920
What? Three. How do you know that? You're right. If you look at the different comparisons of each other.

103
00:11:49,670 --> 00:11:54,190
One. Yeah.

104
00:11:55,790 --> 00:11:59,140
Yeah. And don't be intimidated by these big, long numbers.

105
00:11:59,210 --> 00:12:01,370
All they really are. Are these mean differences.

106
00:12:01,600 --> 00:12:05,860
So that def value is just taking this attack in this day and subtracting this or whatever it's going to be.

107
00:12:06,360 --> 00:12:10,420
Right. And then we have it provides us with kind of a rough confidence interval.

108
00:12:11,800 --> 00:12:15,700
So these were this is our point estimate for the difference between the two groups.

109
00:12:15,970 --> 00:12:19,060
We recognize that there could be some error just because of the way that we've sampled.

110
00:12:19,430 --> 00:12:26,380
Right. So we will use that imprecision through this construct a confident confidence interval around.

111
00:12:26,470 --> 00:12:33,459
We're looking to see if that confidence interval contains what value zero sum PDB to it.

112
00:12:33,460 --> 00:12:40,840
Zero. Yes, absolutely. If it contains zero, it's going to map on to essentially a non data insecure or non-significant p value.

113
00:12:41,200 --> 00:12:42,670
If it does not contain zero,

114
00:12:42,970 --> 00:12:49,150
then we can assume that there is very small chance that the true point estimate actually is zero or no relationship or no difference.

115
00:12:49,900 --> 00:12:57,070
And that's why we see that kind of significant p that how we feel about this question about clear.

116
00:12:57,820 --> 00:13:05,920
Yes. How we record like how we're writing, how to report the the differences that are significant.

117
00:13:06,130 --> 00:13:09,720
And do you put them in parentheses p value? Yeah.

118
00:13:09,730 --> 00:13:14,740
Yeah, that's that's a good question. I think it depends on kind of the audience that you that you are that you're talking to.

119
00:13:14,980 --> 00:13:19,180
If you want to do this in sort of like the APA style format, you might say that.

120
00:13:20,350 --> 00:13:25,870
I mean, I would use kind of a combination of of language as well as the statistics themselves.

121
00:13:26,230 --> 00:13:29,500
You might say, oh, is it one in three? You might say Africa-American or black?

122
00:13:29,680 --> 00:13:36,850
Participants in the sample had a mean exposure to violence of about 2.53, whereas mixed African-American white students reported 2.67.

123
00:13:36,850 --> 00:13:46,210
That mean difference was significant or was significant at a point all five level.

124
00:13:46,420 --> 00:13:57,219
And then I would report in parentheses the difference of .31 and the p value and then probably a semicolon.

125
00:13:57,220 --> 00:14:10,510
I would say above here the F value with 84 or 842 and two degrees of freedom for 42 and 842 equals this.

126
00:14:12,670 --> 00:14:16,600
So, I mean, do most people care about the statistic?

127
00:14:16,600 --> 00:14:22,989
Probably not. But I think and certainly academic reporting, you're going to want to have all that information probably in the text.

128
00:14:22,990 --> 00:14:27,879
And so I personally like to lean towards trying to just explain,

129
00:14:27,880 --> 00:14:33,400
use your verbiage to tell about the actual differences and meaningful changes that we're expecting to see or that we see in the statistics.

130
00:14:33,790 --> 00:14:37,800
And then as almost an afterthought, you're throwing in the numbers now that your numbers.

131
00:14:37,820 --> 00:14:44,770
Now, another option that folks will sometimes use is if you put all this in a table, including like the F statistics,

132
00:14:44,770 --> 00:14:50,770
and you can denote these post hoc tests with like asterisks that tell you which one you can use,

133
00:14:50,770 --> 00:14:53,230
superscript or something to say that these are different from each other.

134
00:14:53,530 --> 00:14:58,510
And then in the actual text you're only saying in words, what are the findings?

135
00:14:58,840 --> 00:15:02,380
So it's these three groups. We know there's a difference. These two groups are different.

136
00:15:02,770 --> 00:15:08,070
Other groups, despite having fairly large differences, denarii statistical significance or something.

137
00:15:11,720 --> 00:15:19,880
My second question was so in the case, working on at home is faced by school as opposed to race.

138
00:15:20,000 --> 00:15:24,890
Mm hmm. The sex and race values. You just want us to report them as demographics.

139
00:15:25,010 --> 00:15:28,610
Like in the beginning or. I was just making sure.

140
00:15:28,890 --> 00:15:32,879
Oh, it's like describing the sample. You don't want to see.

141
00:15:32,880 --> 00:15:36,410
You don't even have to report those for that exercise.

142
00:15:36,500 --> 00:15:42,170
Right. Right. It's more just trying to. Can you talk about that, that couple of variables that are in the that in the analysis.

143
00:15:42,200 --> 00:15:49,260
Okay. It's good practice to do it maybe once or twice, especially if you haven't written that way in the past.

144
00:15:49,280 --> 00:15:51,889
I know the sample very well, so I don't need you to describe.

145
00:15:51,890 --> 00:15:56,900
But in general, I think it is good for folks to say this is huge comprising my sample before we say.

146
00:15:57,140 --> 00:16:01,370
Turns out it was all schools that was related to the differences.

147
00:16:01,610 --> 00:16:09,469
It might be because there are large differences in the populations within the schools or whatever, just to have the distribution of demographics.

148
00:16:09,470 --> 00:16:16,620
Who knows? I'll let people read this for a second.

149
00:16:17,640 --> 00:16:58,270
Well, I also read this thing. Jenny, what's the correlation between technology use and unhappiness?

150
00:17:04,480 --> 00:17:09,850
1%. That's the that's our our square.

151
00:17:17,990 --> 00:17:31,080
I. Mm hmm. You know, it's not like zero one.

152
00:17:31,600 --> 00:17:39,100
No. So if we know that our square is going to equal what kind of 1%.

153
00:17:40,450 --> 00:17:44,060
We can take the square root of.

154
00:17:49,270 --> 00:17:57,349
Right. This reversed the process. So if you take a square, if you take a correlation of point one from square, you're going to get a point.

155
00:17:57,350 --> 00:18:01,520
All right. That's our if the amount of variability shared between those two values.

156
00:18:02,390 --> 00:18:10,820
This sorry. This is an example of our of our of an infectious sense.

157
00:18:11,270 --> 00:18:15,770
We like to communicate this because it tells us a little something that's perhaps a more meaningful than,

158
00:18:16,100 --> 00:18:20,030
you know, a correlation coefficient, which folks kind of have a sense of.

159
00:18:20,030 --> 00:18:23,720
But is point one correlation 2.1 to Pearson, correlation 2.1.

160
00:18:24,110 --> 00:18:26,690
What does that really kind of mean in practical terms?

161
00:18:27,680 --> 00:18:35,959
And so I also like that they did this as well in a sense that they found a similar effect size for something that you might say is not quite as,

162
00:18:35,960 --> 00:18:42,890
you know, a cut rate and in terms of trying to get people excited about the idea.

163
00:18:43,700 --> 00:18:48,650
And it turns out that they might have just as much influence, if not more, on people's reported on happiness.

164
00:18:49,130 --> 00:18:49,400
All right.

165
00:18:49,400 --> 00:18:56,389
So again, being kind of consumers of of information like this is going to be helpful when I see it or if you see examples, please send them to me.

166
00:18:56,390 --> 00:18:59,090
Because sometimes it's funded to think through what do they actually do,

167
00:18:59,450 --> 00:19:06,110
what are they actually talking about within our within the media that we read and notice, you know, statistics here.

168
00:19:06,680 --> 00:19:11,000
There's no there's no there's no test of this correlation to see if it's different from zero.

169
00:19:11,780 --> 00:19:15,610
This is kind of all we have to go on. All right.

170
00:19:18,170 --> 00:19:25,550
Okay. Now move on. Moving on a little bit here, just to give folks some kind of mapping for the rest of the semester.

171
00:19:25,760 --> 00:19:32,150
We're going to spend the next two or three weeks talking a lot about multiple regression and how we can extend

172
00:19:32,360 --> 00:19:39,510
some of these ideas that we talked about today to include some assumption checking that we'll do next week.

173
00:19:40,100 --> 00:19:46,729
We're going to get into some of the the measurement piece. And right now I'm kind of glossing over, but it's really critical in the work that we do.

174
00:19:46,730 --> 00:19:52,550
So again, before we actually run away these regressions, we have to do some of that work as well as some other measurement techniques.

175
00:19:52,880 --> 00:19:55,310
We'll get into non-linear and interaction models,

176
00:19:55,310 --> 00:19:59,690
which are going to be very handy for those of us who are interested in testing models that are, for example,

177
00:19:59,690 --> 00:20:05,750
have protective factors, or if we want to even get into mediation analysis later on this semester,

178
00:20:06,440 --> 00:20:13,190
then we'll move on to some specific scenarios where we use the same principles.

179
00:20:13,460 --> 00:20:18,800
But we start to we start to challenge some of the assumptions that we're in to start talking about next week.

180
00:20:19,250 --> 00:20:25,610
So when we don't have independent observations because people come from the same neighborhoods or attend the same hospital or go to the same schools,

181
00:20:26,510 --> 00:20:31,430
we'll find we'll find situations where people come from the same families that have shared history.

182
00:20:31,430 --> 00:20:34,610
We'll talk about when they're repeated observations and therefore they are going to hear

183
00:20:34,610 --> 00:20:40,100
observations or be more correlated with each other than between person observations.

184
00:20:40,610 --> 00:20:44,690
So same kind of ideas that we talk about today and next week.

185
00:20:45,350 --> 00:20:48,200
Now we're finding new and exciting ways to use them.

186
00:20:48,200 --> 00:20:53,210
We'll talk about when our outcomes are no longer or continue a normally distributed continuous variable.

187
00:20:53,480 --> 00:20:56,150
And instead we have just the yes no present, not present sick.

188
00:20:56,150 --> 00:21:01,390
Well, whatever it's going to be when our open might be a grouping variable that doesn't really have an order.

189
00:21:01,790 --> 00:21:06,200
When we have a count variable and there's no and nothing below zero can only be zero to infinity.

190
00:21:06,590 --> 00:21:13,850
And all of those pieces are going to have an influence on how we approach with our seeing multiple regression techniques,

191
00:21:14,390 --> 00:21:22,430
slightly different calculations, rates are going to move away from our happy friend over the less two different ways of estimating

192
00:21:22,430 --> 00:21:27,050
the effects that will interpret in a very similar way that we'll start talking about right now.

193
00:21:27,140 --> 00:21:34,070
So just to give folks an idea of where we're going to go, but it's another way of saying that this stuff is important.

194
00:21:34,070 --> 00:21:38,730
So if at the end of this week and at the end of next week, you're still feeling uncomfortable, that's a time to reach out.

195
00:21:38,780 --> 00:21:43,520
You can come on to office hours or find a 1037 zoom call or whatever it's going to be.

196
00:21:43,850 --> 00:21:47,030
But you really want to make sure folks get this piece. All right.

197
00:21:47,090 --> 00:21:50,959
I'm not going to spend too much time here because I hope everyone said at least started

198
00:21:50,960 --> 00:21:55,430
to watch the videos and it should be the same kind of information that we covered.

199
00:21:55,850 --> 00:22:03,110
We saw this afternoon when we, too, were trying to use our sample estimates from regression to say something about the population at large.

200
00:22:03,110 --> 00:22:10,460
Right. All this is predicated on the assumption that our sample is, at least in some way, representative of the population that we care about.

201
00:22:10,850 --> 00:22:16,220
That sample, this stuff doesn't mean quite as much right before three people from a sample.

202
00:22:16,640 --> 00:22:23,480
That's probably not going to give you meaningful information even though you might do better on a regression theory's

203
00:22:23,480 --> 00:22:33,020
a little bit dicey but try and it did all right for us the example that we're going to use was us in the video.

204
00:22:33,230 --> 00:22:37,400
We're going to use a pretty well-known scale to get a little data these days, but it's still out there.

205
00:22:37,400 --> 00:22:45,020
Actually, the brief symptom inventory gives us a sense of two constructs that actually it's I think 12 items total.

206
00:22:45,020 --> 00:22:51,200
We're going to use the the five that are related to anxiety.

207
00:22:51,590 --> 00:22:55,580
There's also five or six that are related to depression and you can see those within the current one.

208
00:22:55,940 --> 00:23:06,230
But essentially we're going to say that if you respond in similar ways to these five items, that suggests a certain underlying level of anxiety then.

209
00:23:06,950 --> 00:23:09,830
Somebody respond to these same five items in a very different pattern.

210
00:23:10,160 --> 00:23:16,250
So if you're strongly agreeing to all of these five items, it suggests you have a higher level of anxiety than someone who says,

211
00:23:16,790 --> 00:23:20,080
no, I don't tend to feel this way, but I like the last two weeks or something.

212
00:23:20,090 --> 00:23:24,420
It's going to be much.

213
00:23:29,860 --> 00:23:33,220
So I understand self-acceptance very well, and I think that's pretty clear.

214
00:23:34,060 --> 00:23:39,820
And generally speaking, more of the construct of higher self-acceptance.

215
00:23:41,990 --> 00:23:48,190
All right. So within the code, if anybody is following along with the R code and we're going to have to do a little bit of work.

216
00:23:49,000 --> 00:23:50,740
We already have this part done.

217
00:23:50,900 --> 00:23:58,090
So I'm going to just pull this up so people see if you are going to start working here, now's a good time to get these packages loaded.

218
00:23:58,930 --> 00:24:03,040
I do have another note about masking in here because if you recall from week two,

219
00:24:03,460 --> 00:24:08,200
when you have packages that have the same function, you're going to get an error about masking.

220
00:24:08,530 --> 00:24:13,090
I noticed this last night just because of the order that I'd saved to these library commands,

221
00:24:13,420 --> 00:24:23,560
and this is just another way for us to figure out, you know, what's the order that are sorry that are is going to be looking for my functions.

222
00:24:24,160 --> 00:24:28,180
And so in this case, I think it's the Alpha Command that we're I don't think we're going to use today.

223
00:24:29,230 --> 00:24:32,440
So did you want us to run around the detach?

224
00:24:34,140 --> 00:24:42,390
Oh, no, that's just. Just if you if you really wanted to use that Alpha Command, which you're not going to use today, that's one way to do it.

225
00:24:44,850 --> 00:24:49,860
Think when when it says like a package doesn't exist, how do you how do you go about that?

226
00:24:49,980 --> 00:24:53,340
So you probably have to install it still. So if you haven't.

227
00:24:53,550 --> 00:24:59,820
I think we've used all of these today or all of these up to this point, except for probably number ten.

228
00:25:00,420 --> 00:25:08,580
So if you just do install that packages or if you go to um, one of these installer packages tools, you should go get it.

229
00:25:11,220 --> 00:25:15,630
All right. And then reading in the data. We'll actually get to that in just a second.

230
00:25:16,050 --> 00:25:21,390
All right. So, Dana, a sample. We're going to do all this. We're going to be using the FISA data.

231
00:25:21,400 --> 00:25:27,959
Again, I feel at this point, folks are feeling pretty familiar with home readings data.

232
00:25:27,960 --> 00:25:37,200
And a couple of things to know. When we're reading in our SPSS file, there are a few commands that we're going to pay we're going to pay attention to.

233
00:25:37,590 --> 00:25:40,290
One is this use value labels equals true.

234
00:25:41,160 --> 00:25:49,550
This means that R is going to read in the data as it was originally constructed because I want to have value labels like male,

235
00:25:49,560 --> 00:25:54,420
female, black, white, whatever, right? I want to keep that information.

236
00:25:54,960 --> 00:26:02,370
Essentially. I want to keep the factors. Factors. Now, the problem is when we have some of those strongly agreed to strongly disagree type scales,

237
00:26:02,760 --> 00:26:07,770
it's going to read those in as exactly that, strongly disagree, agree, whatever.

238
00:26:08,430 --> 00:26:17,220
Right. So we're going to have to think about whether R is considering those as numeric variables or factor variables or something different.

239
00:26:26,560 --> 00:26:30,550
How would I know if I was treating the variables that I read in?

240
00:26:33,910 --> 00:26:37,510
As factors or numeric for something else.

241
00:26:37,510 --> 00:26:42,670
What would I do here? Which what would I do here?

242
00:26:43,360 --> 00:26:48,390
The structure, structure, community. So if you remember, you can use SDR.

243
00:26:49,550 --> 00:26:51,970
Yes. I mean, it's going to give you all of the different values here.

244
00:26:51,970 --> 00:26:58,750
If you just want the whole dataframe or you can just do a specific a specific variable like this.

245
00:27:00,070 --> 00:27:03,450
What we're going to find is these variable.

246
00:27:03,470 --> 00:27:07,000
Here's a couple of examples. This is kind of a problem for us.

247
00:27:07,870 --> 00:27:11,379
These values, not true, little true, always true.

248
00:27:11,380 --> 00:27:16,600
They're kind of stuff. We treat them as numeric. We treat them as a kind of a continuum.

249
00:27:17,170 --> 00:27:21,620
Part of the reason why we do that is because there's a logical order. We're going to take four or five of them.

250
00:27:21,640 --> 00:27:23,140
We're going to average them all together.

251
00:27:23,530 --> 00:27:28,840
So we kind of create a continuous construct that we can then manipulate, like we do any sort of other number.

252
00:27:29,200 --> 00:27:37,720
We can get a mean we can multiply by things. If our is considering this to be a factor, we have to make a change.

253
00:27:38,250 --> 00:27:42,380
You change this from a factor variable to a numeric variable. Okay.

254
00:27:43,180 --> 00:27:47,650
So I'm going to think about those values that I want to use in my scale, my brief anxiety scale.

255
00:27:48,430 --> 00:27:51,730
And that's what I'm doing here in lines 29 through 33.

256
00:27:53,080 --> 00:27:59,830
Now, think we've done this together yet? So I'm taking each one of the scale names.

257
00:28:01,810 --> 00:28:04,900
This would be I felt nervous or shaky all the way down.

258
00:28:05,270 --> 00:28:10,090
All right. Those five items that we talked about before. I'm going to take the original data frame.

259
00:28:10,810 --> 00:28:23,980
I'm going to index the actual variable I care about. I'm going to assign the same variable using the function as a period numeric.

260
00:28:24,940 --> 00:28:32,530
So it's going to coerce what are considers right now to be a factor into a numeric variable.

261
00:28:34,310 --> 00:28:40,700
Right. So it's not about R is when it says, you know, strongly agree or disagree, blah, blah.

262
00:28:40,900 --> 00:28:44,740
Behind the scenes, it's still labeling those things as one, two, three, four.

263
00:28:45,700 --> 00:28:51,340
All we're going to do is to say, drop the names right now and give me the one, two, three and four.

264
00:28:52,970 --> 00:28:56,830
Okay. They still mean the same things to us.

265
00:28:57,580 --> 00:29:05,100
If somebody says three to this item. It's the same thing as say, I agree, or it's mostly true.

266
00:29:05,920 --> 00:29:09,250
That's what I want you to remember, is that there's no loss of information.

267
00:29:09,790 --> 00:29:12,790
It's just essentially the labeling that's changed.

268
00:29:15,290 --> 00:29:20,210
Okay. So question with this one. We have like doctors.

269
00:29:21,170 --> 00:29:29,120
It's it doesn't matter what order the number in numerical value is assigning it to them because they're all like a constant on the same level.

270
00:29:29,590 --> 00:29:34,700
That makes sense. Versus like having something where it's like little to no shaping, shaping whatever.

271
00:29:34,700 --> 00:29:38,210
And then you want the numbers to potentially have like increasing.

272
00:29:38,600 --> 00:29:45,229
Yeah. You know, and so it will be consistent across when you when you make the change.

273
00:29:45,230 --> 00:29:49,730
But you do have to keep writing. You have to remember which number means what value.

274
00:29:49,790 --> 00:29:52,670
So then you just like put that in the code. Right. Yeah.

275
00:29:52,700 --> 00:29:58,090
And so that's a it's a bigger deal for potentially kind of grouping variables, but these will stay in order.

276
00:29:58,100 --> 00:30:03,420
So I think there was little to no or to them little, you know, it'll still be one, two, three, four.

277
00:30:03,440 --> 00:30:06,200
It shouldn't get at our. Okay.

278
00:30:07,220 --> 00:30:14,840
So all I'm going to do here for these three are these five lines and running this and I'm converting all of these to numeric variables.

279
00:30:15,440 --> 00:30:22,540
This line 34 is handy. We'll use these. We'll use this is numeric or is factor or or is logical value.

280
00:30:22,550 --> 00:30:26,600
It's a question we're going to ask are and it's basically tell me whether or not

281
00:30:26,600 --> 00:30:31,550
this new value which remember I've saved over is a numeric value and if it is,

282
00:30:31,580 --> 00:30:35,630
it'll step back a true. If you didn't do it correctly, it'll spit back a false.

283
00:30:36,500 --> 00:30:40,670
You see my hand, my side says this is true. Okay.

284
00:30:42,910 --> 00:30:44,559
So that's what's going to happen here.

285
00:30:44,560 --> 00:30:54,130
All the way up through line 39, I'm just converting a bunch of their values that were written as factors into numeric variables.

286
00:30:54,880 --> 00:31:00,220
Now, depending on the kind of data that you work with, this might be something that you don't have to do a lot of.

287
00:31:00,760 --> 00:31:04,749
But that structure commands and imagine run that each time.

288
00:31:04,750 --> 00:31:09,069
Pull in a new data set to make sure what you're expecting to see is what our is reading

289
00:31:09,070 --> 00:31:13,480
it all right so that we don't inadvertently start to work with data that has no,

290
00:31:14,230 --> 00:31:16,590
you know, numerical sense like, you know,

291
00:31:16,630 --> 00:31:22,840
it's an ordered category or something or it's a categorical variable that has behind the scenes like one, two, three, four.

292
00:31:23,020 --> 00:31:33,740
But there's no order to those those four numbers. Anything else, the other end here.

293
00:31:33,750 --> 00:31:37,070
So the reason why that happened was because legal values equals true.

294
00:31:37,370 --> 00:31:41,720
If this is label values equals false, it would have automatically put everything in as a number.

295
00:31:42,380 --> 00:31:48,590
The downside of that is if there were any factors in your data, they would also be read in as a number and not as a factor.

296
00:31:48,600 --> 00:31:52,100
So that race variable that has three numbers would have three numbers.

297
00:31:52,580 --> 00:32:02,240
It would not recognize that as a category. It still means there are there are 681 ones, 177 twos and something like 80 something threes.

298
00:32:02,750 --> 00:32:07,040
But what it doesn't realize is that's like African-American or Caucasian, mixed race, whatever.

299
00:32:07,460 --> 00:32:11,540
Okay, that match value labels infinity.

300
00:32:11,540 --> 00:32:17,990
This is something that just to help with the size of the number cells, you can pretty much ignore it, but there's no way.

301
00:32:18,320 --> 00:32:25,880
And in this part, again, this only happened once or twice last week where we had somebody reading it as a list instead of a dataframe.

302
00:32:26,210 --> 00:32:34,140
We want a dataframe because data frames can handle both text data and numeric data, and you have a few more operations that you can do it.

303
00:32:38,430 --> 00:32:46,590
All right. So the next thing that we're going to do is we just want to make sure that we have the variables that we care about.

304
00:32:46,920 --> 00:32:52,530
So in this case, we are going to try to use self-acceptance to predict anxiety.

305
00:32:54,810 --> 00:33:01,430
Unfortunately, if he does not have a self acceptance variable and an anxiety variable in its dataset,

306
00:33:02,190 --> 00:33:07,710
we are going to use the existing items to create a scale so that we can ask a question like this.

307
00:33:08,400 --> 00:33:13,820
From Synergy. We're going to go beyond what exists in the data and try to create something new in anxiety.

308
00:33:14,340 --> 00:33:19,290
So here, after I read in all of these values is numeric and you will not be able to do

309
00:33:19,290 --> 00:33:23,040
this if you have factor variables because you cannot take a mean of factors.

310
00:33:24,000 --> 00:33:27,870
This is the first time that we are going to jump right into scale creation.

311
00:33:28,740 --> 00:33:33,210
I'm going to introduce a very briefly a new command. It is the means command.

312
00:33:33,240 --> 00:33:35,160
If you haven't seen if you maybe you've seen it before.

313
00:33:35,850 --> 00:33:41,910
But essentially what we're going to do is we're to create a new variable face, dollar sign, name, whatever you want.

314
00:33:42,000 --> 00:33:45,660
I like the way when anxiety, but you can call it whatever you feel like right now.

315
00:33:45,990 --> 00:33:49,049
Okay. We're going to assign to that value that we want.

316
00:33:49,050 --> 00:33:56,460
Exactly. All â‚¬850. The mean of all of these items.

317
00:33:57,570 --> 00:34:06,870
I need to use this what's called column vein command so that I tell our give me these five columns.

318
00:34:07,620 --> 00:34:15,120
Combine them together. And once you've done that, we have kind of like a new, if you can imagine,

319
00:34:15,120 --> 00:34:22,440
kind of like an inner data set or enter dataframe that has five columns and I want you to take every rows me.

320
00:34:23,760 --> 00:34:27,780
So it's going to take all of your values. One, two, three, four, five for each person.

321
00:34:28,200 --> 00:34:32,850
And it's going to take the average not to be ignored at the very end here.

322
00:34:36,600 --> 00:34:48,360
If there are missing data in any of those columns, I need to indicate that with a dot arm which is missing data to remove missing value.

323
00:34:48,390 --> 00:34:55,050
So if somebody doesn't have one of these values, we're just going to drop them from this particular analysis to make our lives a little bit easier.

324
00:34:55,800 --> 00:35:00,200
So this is to remove question true is my answer.

325
00:35:00,210 --> 00:35:09,750
So it's going to drop as is doing. So would this be the point where if one of those one of those factors like would we reverse code here if we needed.

326
00:35:09,770 --> 00:35:15,120
Yep, yep. So before we create this meeting, we want to make sure that all of these five values are going in the same direction.

327
00:35:16,800 --> 00:35:24,800
Good question. Good question. Is there fun stuff behind the scenes that have you?

328
00:35:25,850 --> 00:35:29,270
You know, there's actually six anxiety. There are six books.

329
00:35:29,840 --> 00:35:33,980
How do you count up here? Do you find I'm doing something sneaky here?

330
00:35:34,280 --> 00:35:40,130
All right. I'm taking data. I'm making a decision. And this is going to have a big impact on my ultimate results.

331
00:35:40,610 --> 00:35:44,180
Again, it's objective quantitative analysis, right?

332
00:35:44,210 --> 00:35:47,540
Well, let's just think about this. How are we creating these variables?

333
00:35:47,810 --> 00:35:51,140
What are the decisions that we're making along the way? I'm going to run this value.

334
00:35:51,440 --> 00:35:56,330
I'm going to get a new variable. So you should see that your obs, your variable observations are going up.

335
00:35:56,630 --> 00:36:01,490
Should go up by one. And look at this. We have a brand new value that we can start to play around with.

336
00:36:02,010 --> 00:36:05,750
Right. So it should look like this.

337
00:36:07,920 --> 00:36:14,010
That's what we get in my screen. It all looks good. If you recall, we started with 850.

338
00:36:14,160 --> 00:36:20,130
What does this variable 800 has? 849. That's because somebody was missing one of those cars they regret.

339
00:36:20,550 --> 00:36:23,790
That shouldn't happen to our expectations. If it doesn't. You got a problem.

340
00:36:23,970 --> 00:36:29,790
You got to go back and investigate little checks. I don't even think about it, but I do it right.

341
00:36:30,120 --> 00:36:33,120
And it didn't have a note in my head where we should look at the sample size.

342
00:36:33,390 --> 00:36:35,700
It's just kind of a natural thing that I'm going to take a peek at.

343
00:36:36,030 --> 00:36:42,300
Are there things that you might want to take a peek at here, or is my little oh, man, it's all tucked away.

344
00:36:42,570 --> 00:36:47,130
Shoot all the things that we might want to take a peek at.

345
00:36:48,000 --> 00:36:51,650
Now come. How are you?

346
00:36:51,690 --> 00:36:57,040
What about what you might want to think about? And then you said descriptive statistics, things that might just catch your attention.

347
00:36:59,380 --> 00:37:03,490
It's a brand new skill no one's ever created before was pretend.

348
00:37:05,470 --> 00:37:08,850
That's no one else. Who else?

349
00:37:09,280 --> 00:37:12,430
I know you've got us, but it's there. Well, it's great.

350
00:37:14,140 --> 00:37:20,490
Well, so much you look for here. For, like sales or.

351
00:37:20,810 --> 00:37:29,030
Yeah. Yeah. Yeah.

352
00:37:29,930 --> 00:37:34,670
M.A. That's a great idea. So, you know, all these values range between one and probably one in five.

353
00:37:35,090 --> 00:37:39,890
And if you all of a sudden have a max value that's, you know, seven, that's a problem.

354
00:37:39,920 --> 00:37:43,880
You have an invited point or two. That's a problem. Think about the mean.

355
00:37:44,180 --> 00:37:47,270
These are all off by about one. Read the means by 1.59.

356
00:37:47,540 --> 00:37:51,440
If it's a scale of 1 to 5 five being high and very little one being low anxiety.

357
00:37:52,190 --> 00:38:01,160
What do we think about our sample? Katie G. Could you repeat if we have a mean of 1.59, what does it mean roughly in our sample?

358
00:38:01,820 --> 00:38:06,810
Are students anxious or not so anxious? No.

359
00:38:09,290 --> 00:38:15,380
Oh, not anxious. Not so anxious. Right. I mean, it's just some information that we can start to infer about what we might expect.

360
00:38:15,890 --> 00:38:19,129
We see that those folks are kind of on the left end of the scale. All right.

361
00:38:19,130 --> 00:38:24,050
We can think about this unitary process. We can even map, which is we're going to we're going to do about a second and a half here.

362
00:38:24,350 --> 00:38:31,070
We can take a look at some of these variables that we're creating. And hopefully they will look more or less like kind of a normal bell curve.

363
00:38:31,160 --> 00:38:37,400
Right. That's what we want to have when we're running kind of multiple regression, linear regression, doing the exact same thing for self-acceptance.

364
00:38:37,700 --> 00:38:44,240
It should be within your code or I'm taking them using arrow means command to create a new variable from the to self-acceptance variables.

365
00:38:44,660 --> 00:38:50,480
How about how many people are we missing? 350 million to folks, right.

366
00:38:51,290 --> 00:38:55,760
We might want to check to make sure there's no those two folks weren't excluded. And you've me.

367
00:38:56,390 --> 00:38:58,840
But chances are they just didn't respond to one of those items.

368
00:39:02,240 --> 00:39:07,200
You can do better visualizations and I am still working on my DG for two presentations that we can do.

369
00:39:07,250 --> 00:39:15,920
There's still a more elegantly, but the plot function in R works perfectly well for some of these initial kind of checks.

370
00:39:16,310 --> 00:39:20,270
I like visualizations because they're fast, they're easy.

371
00:39:20,630 --> 00:39:25,160
You don't have to really have to spend a lot of time thinking about what you want to do.

372
00:39:25,370 --> 00:39:31,250
A scatterplot just takes the two values, right? And puts them on a grid together.

373
00:39:31,820 --> 00:39:35,930
Right. So we can see as self-acceptance goes up, what tends to happen to anxiety.

374
00:39:36,080 --> 00:39:43,100
Where's my list? Uh, in self-acceptance goes up.

375
00:39:43,310 --> 00:39:47,630
What's going on with anxiety? Yeah.

376
00:39:48,020 --> 00:39:54,770
So if that's the case, then if we were to run a bivariate regression, we would expect the beta coefficient to be what there's not here a clear.

377
00:40:03,830 --> 00:40:05,100
Thank you for that, too.

378
00:40:05,810 --> 00:40:12,480
So, again, he told us the self-acceptance is going up and that seems to be going down, which means that we were going around by her regression.

379
00:40:12,480 --> 00:40:21,139
The barrier coefficient should be what? That no small check for your books, just small checks.

380
00:40:21,140 --> 00:40:25,460
If you run a regression curve you are regression and somehow you get a positive coefficient.

381
00:40:25,790 --> 00:40:29,870
Something's wrong. Either your scatterplot from a regression from right.

382
00:40:30,380 --> 00:40:35,480
Don't wait until you've generated lines along the lines of code and you're finally getting ready to start.

383
00:40:35,480 --> 00:40:41,060
Write things up to do some of these kind of a chance to make sure that this is kind of mapping on to your experience,

384
00:40:41,390 --> 00:40:45,049
do we see one of these points is way out in the stratosphere, right?

385
00:40:45,050 --> 00:40:51,290
That would be ratios like setting value for this scale. This scale should go from five to 5 to 1.

386
00:40:52,040 --> 00:40:55,040
If it's not if it's our opportunity margin that we got.

387
00:40:55,040 --> 00:40:58,939
Now, what does that outlier for the it is recording mistake to somebody actually

388
00:40:58,940 --> 00:41:03,030
respond to that how are we going to start to do some of these problems applied?

389
00:41:03,140 --> 00:41:06,290
So it's just a this is actually a super powerful command.

390
00:41:06,290 --> 00:41:11,930
I'm just using the most basic way are two values to get this line.

391
00:41:12,200 --> 00:41:17,420
It is literally running the regression line and asking for this anyway.

392
00:41:19,850 --> 00:41:22,130
Yeah. It's all right to do that.

393
00:41:22,490 --> 00:41:28,280
And if you were to use the summary function for this red one, you would see that coefficient that Cody was talking about.

394
00:41:28,280 --> 00:41:39,020
There would in fact, you know, if it's, uh, if it's a, um, if it's in the original matrix, it's going to be some negative beta value.

395
00:41:39,410 --> 00:41:41,120
If it's in standardized matrix,

396
00:41:41,120 --> 00:41:50,329
it's going to be work between something that in and in a acceptance inside the vehicle efficient would be in a standardized regression

397
00:41:50,330 --> 00:42:01,010
equation with just two variables would be equal to the thing we didn't re to correlation coefficient between the two groups.

398
00:42:02,810 --> 00:42:08,660
One more gut check, right? Mm hmm. Or is this stuff is it just happened right here?

399
00:42:08,660 --> 00:42:12,700
And I'm just repeating it and repeating that story. You start to tell me again.

400
00:42:13,010 --> 00:42:18,470
Yeah, I'm just say, okay. Yeah. The line top Typekit. We want to go beyond just the line.

401
00:42:18,930 --> 00:42:26,299
Yeah, it's I'm not sure if you said this, but what is the line that's actually going to graph this line?

402
00:42:26,300 --> 00:42:32,780
Force shift. All right. So it's going to take from our regression object and give us the line.

403
00:42:43,500 --> 00:42:48,239
Um, these maybe maybe helpful, maybe not.

404
00:42:48,240 --> 00:42:52,380
So just different ways to look at the distribution of our variables.

405
00:42:52,710 --> 00:42:56,310
So I think it is a big deal from the very beginning. Folks are thinking about that.

406
00:42:56,310 --> 00:42:59,640
We just took five items, we average them together and were calling that anxiety.

407
00:43:00,090 --> 00:43:07,020
So we want to make sure that our items are or scales are functioning in a way that's appropriate for the kind of analysis that we want to run.

408
00:43:07,680 --> 00:43:10,540
So, for example, the district boxing whisperer,

409
00:43:10,570 --> 00:43:20,520
what's going to give us an idea of whether or not our distribution is smooshed down into one area of the scale?

410
00:43:20,880 --> 00:43:24,480
It gives us an idea of some of the outliers that we see. All right.

411
00:43:24,480 --> 00:43:27,540
So this is going to give us our median is going to give us are quartiles.

412
00:43:27,900 --> 00:43:31,920
All right. Can you tell us where that if you are, the interquartile range is going to be.

413
00:43:32,310 --> 00:43:37,260
And most of these folks are going to be kind of sandwiched down here at the lower end of the anxiety scale.

414
00:43:37,740 --> 00:43:40,860
Similarly, when we look at a density plot, we're seeing the same type of thing.

415
00:43:40,920 --> 00:43:46,110
If I had my druthers, you know, we'd have a median of three and we'd have a nice, pretty dull curve over this.

416
00:43:46,890 --> 00:43:56,340
More or less, we are seeing kind of a rough multimodal distribution with what kind of skewness as part of the skews getting pulled to the right.

417
00:43:57,340 --> 00:44:01,180
Right. So we should see that within the statistics, the descriptive statistics that we saw.

418
00:44:01,770 --> 00:44:09,450
All right. We can see that these kind of give us similar pieces of information which you might be looking out here for, would be by modality.

419
00:44:09,930 --> 00:44:15,870
If, like all of a sudden you had a whole bunch of one and a half and four and a half that might tell us something about the variable that we're using.

420
00:44:16,530 --> 00:44:24,330
Maybe we thought you had a 1 to 5 scale, but really it was one and two and people just saying one or two or maybe we just naturally use one in five.

421
00:44:24,600 --> 00:44:27,780
It was an extreme. I'm either super anxious or not interested at all.

422
00:44:28,370 --> 00:44:32,910
Right. Some of the things we can discern, but let's keep going. Self-acceptance, not as happy.

423
00:44:33,670 --> 00:44:41,520
This makes me a little concerned. We can ultimately see whether or not this could work out within our regression equation.

424
00:44:41,820 --> 00:44:47,309
But we'll spend some time next week thinking about when we get something like this and we say, Well, we're kind of expecting a normally distributed,

425
00:44:47,310 --> 00:44:52,470
continuous variable, particularly if the outcome is the predictor variables that may not be this concern.

426
00:44:53,940 --> 00:45:00,410
But again, $0.05 takes you 5 seconds to do this. Take peak time for when you're asking for a box.

427
00:45:00,450 --> 00:45:20,860
But that's what. But I know it's just one that in the winter anyway I was just trying to figure out why is until the.

428
00:45:22,410 --> 00:45:31,959
Oh, sorry. I'm sorry. I come over here. Uh, I don't know.

429
00:45:31,960 --> 00:45:38,520
I just told her it was just a single variable. I'd have to look at the home.

430
00:45:38,920 --> 00:45:42,700
I thought. It's been a while since they asked me I'd take these code.

431
00:45:42,700 --> 00:45:46,570
I just a copy version of my new files and I just change the variable names.

432
00:45:47,020 --> 00:45:52,850
I'd have to look at what is a tilde? Um. Oh, it's emails and.

433
00:45:53,490 --> 00:45:58,040
If you get a second, you have to help me. You just type in box one to help me.

434
00:45:58,180 --> 00:46:03,700
And she told us why there's a telephone. And might run without it soon?

435
00:46:04,840 --> 00:46:09,580
Probably not, but my guess is it's probably doing something I think was a scaling.

436
00:46:09,820 --> 00:46:14,200
Or maybe it's like indicating the list, the outliers or something. I forget.

437
00:46:15,940 --> 00:46:22,750
Okay. So moving of through these steps of regression analysis car we know are moving an okay pace at least for not.

438
00:46:26,710 --> 00:46:30,730
If we have questions inside ask, we're still not quite here, which is fine.

439
00:46:31,000 --> 00:46:36,850
Right now we're doing still a lot of the prep work. We're getting a couple of variables that we ultimately want to use in a regression analysis.

440
00:46:37,270 --> 00:46:41,200
We already had the the sample collected for us.

441
00:46:42,070 --> 00:46:48,640
The statistical model piece, I don't know if this was kind of the game was what we're talking about.

442
00:46:51,370 --> 00:47:01,479
By and large, the math behind what we do seems a little less important to me than really understanding what we're seeing intentional about the

443
00:47:01,480 --> 00:47:08,050
variables that we include in making sure that we're using the appropriate procedures based on the question that we're trying to answer.

444
00:47:08,920 --> 00:47:12,940
Right. It's very rare where I'm just putting up equations outside,

445
00:47:12,970 --> 00:47:20,740
perhaps like a grant proposal where people really need to see what's kind of underlying the work that we're doing.

446
00:47:20,770 --> 00:47:26,560
That said, I think it can be helpful when we understand, for example, what an intercept means.

447
00:47:27,250 --> 00:47:34,870
And if we can say an intercept is the value when all of the other values in our equation are zero, then intercept, we can make more meaningful.

448
00:47:34,870 --> 00:47:39,519
Wider decisions are made for centering decisions that we make or the grouping variables, dummy variables.

449
00:47:39,520 --> 00:47:45,880
We saw that last week. So having at least a little sense of what this function is ultimately going to look like,

450
00:47:46,480 --> 00:47:52,480
and then when we're comparing some of those effect sizes and why we think some variables are more important than others,

451
00:47:53,020 --> 00:48:00,940
that might be where it's worth trying to understand how this math translates into real world implications or recommendations.

452
00:48:01,930 --> 00:48:07,660
I chose the X value or the data value with the largest effect on the outcome variable

453
00:48:08,140 --> 00:48:15,010
as my center focal area event for some focal variable of future research direction.

454
00:48:15,690 --> 00:48:19,389
Right. That's that's where you're going to maybe want to know this math piece of it.

455
00:48:19,390 --> 00:48:24,880
But I don't think it's the end of the world if it's not completely coding.

456
00:48:26,530 --> 00:48:29,530
Basically, we're going to have this for every single person, right?

457
00:48:29,920 --> 00:48:32,950
Each individual is going to have some sort of predictive score.

458
00:48:33,430 --> 00:48:37,990
We're going to use the values that that they respond to within our survey.

459
00:48:38,440 --> 00:48:43,390
We're going to use from those values. We're going to calculate these two parameters.

460
00:48:44,460 --> 00:48:47,620
Right. Which is going to help us to get to that best fit line.

461
00:48:48,190 --> 00:48:54,400
And the best fit line is that which minimizes the sum of squared deviations.

462
00:48:55,000 --> 00:49:01,720
Okay. All right. Zero on procedure. All this really means is where you're going to find these values.

463
00:49:02,440 --> 00:49:05,920
That makes the best prediction line across that scatterplot.

464
00:49:06,700 --> 00:49:10,360
Okay. The best prediction line is relative.

465
00:49:11,050 --> 00:49:18,040
And then we're ultimately going to really try to figure out, compared to our predicted values, what do people actually say?

466
00:49:18,850 --> 00:49:22,930
And if there is any difference or residual is large.

467
00:49:23,410 --> 00:49:28,150
We're less happy with our prediction, like if those residuals or differences are small.

468
00:49:28,660 --> 00:49:35,160
We're happier with our production line and we're happier with our regression model. We say it's doing a good job of protecting our open door.

469
00:49:36,070 --> 00:49:39,490
If it's not doing a good job, then what is this regression line really doing for us?

470
00:49:40,730 --> 00:49:45,760
Okay. This is our function.

471
00:49:46,360 --> 00:49:50,950
This is our air. This we will expand to include many, many predictors.

472
00:49:51,880 --> 00:49:54,910
Will always have an intercept. We'll always have an outcome variable.

473
00:49:55,630 --> 00:49:58,330
For now and for most of the semester, this can be one outcome variable,

474
00:49:58,750 --> 00:50:04,720
but we can ultimately generalize to multiple outcome variables and we are going to spend a lot of time thinking about these error terms.

475
00:50:05,350 --> 00:50:10,180
How much are we missing people? Are we missing people in predictable ways?

476
00:50:10,840 --> 00:50:18,610
And that's what this means over here, that if we took all of those with all of those distances between what we predicted and what we actually saw,

477
00:50:19,330 --> 00:50:23,080
those should more or less cancel out to a median zero.

478
00:50:23,710 --> 00:50:27,340
And then it was some distribution that we that we can estimate.

479
00:50:29,580 --> 00:50:34,320
We assume that that distribution has roughly a normal shape, like a bell curve shape.

480
00:50:34,890 --> 00:50:44,010
And from there, we can get a sense of, again, how well we feel our line is capturing people or forgetting people's alternate responses.

481
00:50:46,020 --> 00:50:48,900
I will pause and see if there are questions about this.

482
00:50:56,760 --> 00:51:04,380
I use this once a year in my real world, maybe twice when I'm trying to figure out an interaction term.

483
00:51:05,730 --> 00:51:08,280
The math pieces is just not it's not essential.

484
00:51:08,640 --> 00:51:13,110
It's helpful, especially if this is kind of language that speaks and makes things consistent in your head.

485
00:51:13,470 --> 00:51:18,900
Great. Know that everything that we do, all these analyzes will boil down to an equation and ultimately, like.

486
00:51:19,630 --> 00:51:27,230
I mean, we can write it up if we want to. From our sample, we are going to generate these key values.

487
00:51:27,620 --> 00:51:31,969
And this is what we ultimately see in our output. Right? We see our estimates for the intercept.

488
00:51:31,970 --> 00:51:37,010
We see our estimates for the predictor variables. And we also get some sense of the error in our model.

489
00:51:37,460 --> 00:51:41,690
That's where that ANOVA at the bottom of the regression table is going to pop up and.

490
00:51:44,410 --> 00:51:51,990
Shh. All of that is taking the work out of this.

491
00:51:52,980 --> 00:51:56,460
We have a scatterplot of these two, two, two sets of data. Right.

492
00:51:56,670 --> 00:52:02,820
You know, your anxiety or your acceptance, you know, you're saying, what are these lines is the best thing.

493
00:52:03,540 --> 00:52:10,110
Which one of these minimizes the distance from all of these values to what we predict?

494
00:52:10,440 --> 00:52:14,340
Our line is not super sophisticated. We only have one predictor here.

495
00:52:14,970 --> 00:52:19,050
So the only information that we have to predict anxiety is how you feel about yourself.

496
00:52:20,040 --> 00:52:24,180
As we have more predictor variables, we would expect this line to do a little bit better job.

497
00:52:24,900 --> 00:52:29,880
But still, even with just one value, we could pick a whole bunch of different lines.

498
00:52:30,810 --> 00:52:39,450
Ordinarily, Squares is going to essentially use calculus to figure out which of these lines or the millions of other options that we can have.

499
00:52:40,170 --> 00:52:44,190
That's going to minimize the sum of squared deviations.

500
00:52:44,670 --> 00:52:51,300
That's a fancy term. That just means that we have distances from every one of these circles to our prediction line.

501
00:52:52,230 --> 00:52:57,440
We have to square them because we have positive differences and negative differences.

502
00:52:57,450 --> 00:53:05,040
This is on your pretest. If we just added them all up without squaring them, we get zero, right does because this line is so good.

503
00:53:05,490 --> 00:53:11,520
It's going to go right smack dab in the middle. It's going to have the same number above and below are the same distances, I guess.

504
00:53:12,040 --> 00:53:18,149
Right. So we're going to try to minimize that value. But really it comes down to is our blue line.

505
00:53:18,150 --> 00:53:22,340
The best is to whatever color these two and the best in our study that offers.

506
00:53:31,060 --> 00:53:34,690
So this is the method. We won't just use all this in this class.

507
00:53:35,500 --> 00:53:39,459
This is a way, again, that we can is derived using calculus. We don't have to do this.

508
00:53:39,460 --> 00:53:47,110
Our students can do this. As we start to look at different values, we might have to turn from something like oils to an interim algorithm.

509
00:53:47,470 --> 00:53:51,400
So maximum likelihood in some other ways that people aware of be squares, etc.

510
00:53:51,790 --> 00:53:58,240
So there are different ways to calculate these different lines. This is the one who will be using with linear regression.

511
00:53:59,920 --> 00:54:07,749
You really don't need to know so much about the mechanics behind it other than if there were any outcome.

512
00:54:07,750 --> 00:54:14,110
Variables in your assumptions start to change. It's good to understand that we can use this all the time.

513
00:54:15,080 --> 00:54:17,680
Okay, so that's not always the method that we're going to be able to use.

514
00:54:19,900 --> 00:54:25,960
Residuals, as I mentioned before, is simply the difference between what we observe and what we predict.

515
00:54:26,930 --> 00:54:33,700
Okay. We expect that when we start to think about all of these as a group, they'll have a distribution.

516
00:54:34,000 --> 00:54:37,180
Sometimes we miss by a little bit. Sometimes we must buy a lot of it.

517
00:54:37,810 --> 00:54:41,200
By and large, we're going to overshoot. Sometimes we're going to undershoot. Other times.

518
00:54:41,530 --> 00:54:46,540
Most of the time we're hopefully close to the mark, and that's going to look like a nice, normal distribution.

519
00:54:46,570 --> 00:54:55,500
Least that's the assumption. This is just squaring those guys.

520
00:54:56,530 --> 00:55:02,290
So when we start to think about like the total error, the total number of misses for the amount of misses if you want,

521
00:55:02,920 --> 00:55:06,700
that's why we use kind of some squares behind the scenes.

522
00:55:06,700 --> 00:55:11,140
This is what's doing, this is what's going to get us to our beta coefficients.

523
00:55:11,410 --> 00:55:17,920
All I really care about this slide. The only reason why I have this slide in this class and I just want you to see this that what does this mean?

524
00:55:21,320 --> 00:55:27,890
I'll go back over there and look at the list. The measurement of the correlation is the correlation, right?

525
00:55:28,230 --> 00:55:34,920
This is all based on that correlation coefficient. Yes, there's the standard deviation, the two variables.

526
00:55:35,610 --> 00:55:42,330
That's kind of multiplied by the correlation. But underlying all of our regression is our correlation coefficient.

527
00:55:43,110 --> 00:55:46,470
Okay. That association that we expect between the two values.

528
00:55:47,460 --> 00:55:50,480
This is simply just rearranging the terms of an x possibly.

529
00:55:50,490 --> 00:56:00,299
And Y equals and x plus be right. So if we had this and over we have y equals the intercept plus beta times our predictor.

530
00:56:00,300 --> 00:56:06,620
Right. Okay. So we get these values.

531
00:56:06,630 --> 00:56:10,680
You should know that they're out there if you want to look at some of the derivations.

532
00:56:10,950 --> 00:56:16,140
They're not super, super complex. But again, not something that I use on a typical basis.

533
00:56:19,350 --> 00:56:24,060
Standard deviation of error. This is really looking at, again, those the distribution of residuals.

534
00:56:24,090 --> 00:56:26,130
Now we're getting into a little bit more complexity.

535
00:56:26,440 --> 00:56:33,540
And really, what we're trying to think about and I don't know if I have the picture within the slides.

536
00:56:33,870 --> 00:56:37,590
We have our production line. We have our values.

537
00:56:41,730 --> 00:56:47,730
This is talking essentially about our 95% confidence interval right for our air in terms.

538
00:56:50,480 --> 00:56:58,160
We want to see where most of our ear kind of the band around the regression line at most of our major our errors happened.

539
00:56:58,760 --> 00:57:02,060
So when we were calculating these things like the mean square there,

540
00:57:02,090 --> 00:57:08,720
we're just trying to get a sense of in general, if we think about all 850 points around a regression line,

541
00:57:09,230 --> 00:57:19,670
how big or wide or an inner of an interval do we need to more or less kind of calculate to capture a certain percentage, whether it's 60, 80% or 95%?

542
00:57:19,670 --> 00:57:24,650
Right. So we're assuming, again, because of that normal distribution assumption of the residuals,

543
00:57:25,730 --> 00:57:30,860
about 95% of them should fall within two standard deviations.

544
00:57:32,600 --> 00:57:38,300
Sorry about this. Two standard deviations of our regression line right above and below.

545
00:57:40,100 --> 00:57:43,780
We can use these equations to get a sense of kind of distance.

546
00:57:43,790 --> 00:57:48,180
I do think there. I hope I don't.

547
00:57:49,790 --> 00:57:54,290
But if you want to think about this, it's we have our production lines, our best fit.

548
00:57:54,590 --> 00:58:04,040
We know that there are points in above and below. This gives us a sense of how precise our estimate is, how precise our regression line is.

549
00:58:04,580 --> 00:58:09,710
And we see a lots of variability or on a regression line or points kind of clustered tightly around it.

550
00:58:10,430 --> 00:58:19,070
So the smaller this error turn the tide of the fit around our regression line, the better our model will predict the outcomes.

551
00:58:20,330 --> 00:58:24,770
So we'll also talk last week about what our model explains and what it doesn't explain.

552
00:58:25,760 --> 00:58:29,810
This is kind of what our model does not explain, right?

553
00:58:30,470 --> 00:58:35,940
These are errors. These are residual. These are missing. Shh.

554
00:58:36,540 --> 00:58:45,180
All right, back to the coding piece so that we can take that function that I talked about and more or less map it on to our linear regression model.

555
00:58:45,750 --> 00:58:49,590
So we have our outcome variable as regressed on our predictor variable.

556
00:58:49,910 --> 00:58:54,720
Now it's we don't have to think so much about it. The residuals are going to calculate that automatically.

557
00:58:55,230 --> 00:58:58,500
You've seen this now many times. We're going to create an object.

558
00:58:58,710 --> 00:59:01,740
Our assignment operator, our function that we care about.

559
00:59:02,790 --> 00:59:09,809
And this is going to be our equation, right? This is when it starts to get a little bit fun because it's quite easy to try quite a

560
00:59:09,810 --> 00:59:14,880
few different equations once we've done the heavy lifting of constructing our two bags.

561
00:59:15,210 --> 00:59:18,420
We collect our sample. We have a hypothesis. We create error.

562
00:59:18,420 --> 00:59:26,220
Two variables. That's from regression analysis. And the fact is, right there, all that work now are hours worth your time just to get you right here.

563
00:59:28,320 --> 00:59:38,710
Questions about this, do we make it this far in video? Getting there might have been about the time I had the football game and it's a nice day.

564
00:59:39,930 --> 00:59:43,770
Let's do this. All right. We've seen this before.

565
00:59:44,760 --> 00:59:52,380
This is a binary regression. Do I want to do you want to take a chance or take a stab at interpreting this or we can talk about this kind of output?

566
00:59:52,620 --> 01:00:02,159
If you are not, I want you to see devolving into I want to take a shot at explaining what you see up on this slide for me walking by right here,

567
01:00:02,160 --> 01:00:06,150
you know that person out there that they walked into this classroom, what would you tell them about this slide?

568
01:00:09,070 --> 01:00:20,230
What are some of the key pertinent pieces of information? I'm calling you.

569
01:00:28,380 --> 01:00:39,220
Danny. I'm a little unsure about the residuals piece, but the second part was on coefficients that intercept estimate is.

570
01:00:43,240 --> 01:00:54,190
So when? When self-acceptance is zero, anxiety is 2.1.

571
01:00:54,640 --> 01:01:00,480
Wonderful, and this low is -0.155.

572
01:01:00,590 --> 01:01:06,700
And then I think it shows from the P-value that it's a significant relation.

573
01:01:08,520 --> 01:01:19,530
So the higher or so like you can say that the more people are subject to themselves, the little less things like that.

574
01:01:20,460 --> 01:01:26,280
Well, I know for us for getting ahead where we write.

575
01:01:27,120 --> 01:01:30,929
You saw that scatterplot. You're right. Absolutely. We saw the name.

576
01:01:30,930 --> 01:01:35,190
We saw the negative line. We thought of that inverse relationship. Now, there's not a correlation.

577
01:01:35,490 --> 01:01:39,840
But turns out, yes, this manifests. Right. So, again, you would have seen this five slides ago.

578
01:01:39,840 --> 01:01:46,170
You seen a few minutes ago in practice. Double check is there is the coefficient going in the right direction.

579
01:01:46,500 --> 01:01:51,660
Generally speaking, when we start to add variables, then we can run into some confounding and suppression effects.

580
01:01:52,200 --> 01:01:57,270
If you ever start to see these coefficient flipping signs on here, it's a sign that something is wrong.

581
01:01:57,760 --> 01:02:02,700
Right? You're expecting a negative value here and it turns out to be zero or positive.

582
01:02:03,180 --> 01:02:08,100
How common is it because of the combination of predictors? Is it because our hypothesis is wrong?

583
01:02:08,370 --> 01:02:12,540
What's going on? There should be some expectations rather than Hey, ah, tell me what I think I should see.

584
01:02:14,160 --> 01:02:16,380
So, Jenny, thank you. I should mention about the residuals.

585
01:02:16,560 --> 01:02:24,810
We do have more or less a distribution of residuals here, so we would again expect that they're going to be some misses, some positive.

586
01:02:25,150 --> 01:02:26,760
I guess I want to say this one more time.

587
01:02:27,090 --> 01:02:34,410
We think that we're going to have some normal distribution of residuals with a mean of zero and some variance that we estimate.

588
01:02:34,750 --> 01:02:41,379
Right. So that means we're going to have. We're going to have some points where we are.

589
01:02:41,380 --> 01:02:48,290
Our prediction was very close. Right. So we take predicted minus or observed minus predicted.

590
01:02:48,650 --> 01:02:52,070
If we're dead spot on, we're going to get a value, kind of like right. Right here.

591
01:02:52,640 --> 01:02:55,760
Sometimes we're going to be off a little bit. Sometimes we're going to be all the way out.

592
01:02:55,760 --> 01:02:58,010
Sometimes it's going to high, sometimes is going to be too low.

593
01:02:58,460 --> 01:03:06,020
But ultimately, we are expecting something like our good friend, the normal distribution here for our residuals.

594
01:03:06,020 --> 01:03:09,530
If I stack them all next to each other, we started to show them all.

595
01:03:09,530 --> 01:03:16,880
Next of all 850, we should see a distribution and it looks like a big bell curve with a mean of zero.

596
01:03:17,600 --> 01:03:24,410
This gives us a sense of that distribution. It kind of spits it out and we don't quite have a mean zero, but it's close ish.

597
01:03:24,860 --> 01:03:29,510
Could be our first sign that perhaps our regression model isn't as great as we would hoped it would be.

598
01:03:30,440 --> 01:03:35,959
This is close ish to zero, so I'm not super, super worried yet, but there's definitely merits.

599
01:03:35,960 --> 01:03:39,140
Some additional investigation. Hashtag week five.

600
01:03:39,560 --> 01:03:44,120
I will do this and a lot next week. All right, Jenny, great job.

601
01:03:44,420 --> 01:03:57,230
What about this? Who's got this for me? Okay.

602
01:03:57,590 --> 01:04:01,590
We can claim 1.5%.

603
01:04:05,020 --> 01:04:09,400
And they had. You know that. Oh, yeah.

604
01:04:09,540 --> 01:04:15,679
That's good for you. Yes. What else?

605
01:04:15,680 --> 01:04:19,520
What else? That's correct.

606
01:04:19,520 --> 01:04:24,950
So far. What about this should be rings of bells.

607
01:04:25,490 --> 01:04:29,840
Was this very large?

608
01:04:30,080 --> 01:04:34,200
Mm hmm. Also the help with the law.

609
01:04:35,640 --> 01:04:41,360
And. It's a in a sense.

610
01:04:41,360 --> 01:04:45,020
Yeah, that's exactly what's happening. That statistic where we've seen it before.

611
01:04:48,660 --> 01:04:58,860
And homework. Right. Same basic principle where our model last week was outcome variable predicted by group.

612
01:04:59,220 --> 01:05:08,100
We are breaking up. We are partitioning variability in our outcome variable to that which was between groups and winning groups.

613
01:05:09,210 --> 01:05:17,660
Sure. Thumbs up effect. It's okay.

614
01:05:18,020 --> 01:05:24,650
It's okay. The reason why I'm I want to mention that is because we're going to do the exact same thing here.

615
01:05:25,910 --> 01:05:32,030
We're going to take the means sums of squares, regression,

616
01:05:33,260 --> 01:05:42,170
and compare that to our mean squared residuals to tell whether or not this model fits the data.

617
01:05:44,090 --> 01:05:53,540
Okay. Another way to think about both of those statements is we take what our model can explain relative or proportionate

618
01:05:53,540 --> 01:06:01,730
to that which our model does not explain what's predicted by our model versus those residuals that we solved.

619
01:06:02,720 --> 01:06:08,270
When will we predict by our model is large relative to what we can explain?

620
01:06:09,110 --> 01:06:15,800
We get significant statistics of significant to know a significant regression model, right?

621
01:06:16,220 --> 01:06:24,080
When our residuals are when our air is large relative to what our model explains, we get very small statistics.

622
01:06:24,630 --> 01:06:30,740
We see that our model with just one value is not doing a great job of predicting outcome variable.

623
01:06:31,250 --> 01:06:41,510
Katie very helpfully told us that our model explains about a percent and a half of variability R squared versus adjusted R squared.

624
01:06:42,200 --> 01:06:47,090
This is a more conservative estimate that's akin to kind of a population estimate, right?

625
01:06:49,950 --> 01:06:57,479
This is telling us essentially that this value or this value is significant and we

626
01:06:57,480 --> 01:07:06,010
are explaining some meaningful value of variability in our outcome variable anxiety.

627
01:07:07,390 --> 01:07:12,060
Right. So almost like we have a test for this data coefficient, is this different from zero?

628
01:07:12,960 --> 01:07:19,890
You can kind of think of this maybe as an analog of whether this value is significantly different from zero.

629
01:07:21,750 --> 01:07:26,010
Okay. It's giving us a sense of relative to the air in our model.

630
01:07:26,400 --> 01:07:32,100
Does our model actually explain some meaningful proportion of variance in the algorithm?

631
01:07:33,150 --> 01:07:41,520
Knowing knowing someone's self-acceptance tells us something about someone's anxiety.

632
01:07:42,810 --> 01:07:50,130
Does that sound familiar? Knowing something about someone's group tells me something about their exposure to violence.

633
01:07:51,240 --> 01:08:01,130
These are different sorts of set ups, but the same underlying principles since you're trying to break up that.

634
01:08:01,680 --> 01:08:04,770
The outcome variable and all the different ways people report their anxiety.

635
01:08:05,310 --> 01:08:09,430
And can we consistently say that if we know your self-acceptance?

636
01:08:10,050 --> 01:08:17,190
Can we say something about your anxiety? We have three pieces of information here that help us answer that question.

637
01:08:18,660 --> 01:08:21,060
Not only can we say that if we know your self-acceptance,

638
01:08:22,230 --> 01:08:28,170
we understand that there's probably an inverse relationship or maybe a relationship with anxiety,

639
01:08:28,800 --> 01:08:36,000
and that we think that that relationship is significant. But as the model as a whole, we talk about the whole model,

640
01:08:36,210 --> 01:08:40,890
and I know this is the simplest case of a model when we talk about a whole regression model.

641
01:08:41,490 --> 01:08:48,210
Are we doing a good job? Yes, we are doing a decent job.

642
01:08:48,660 --> 01:08:53,040
Decent. We're explaining about 2% and a half a very building outcome.

643
01:08:54,690 --> 01:09:03,570
Is that earth shattering? Probably not. But at least we have something to say that our model is doing better than, for example, chance, right.

644
01:09:04,410 --> 01:09:09,000
That's kind of where we're trying to do it. Now, is that feeling photogenic?

645
01:09:09,540 --> 01:09:15,600
Is there like a certain like how can you tell the multiple equations?

646
01:09:18,300 --> 01:09:21,680
Like a lot or not. Is that just like you look at the feedback, is it significant?

647
01:09:22,350 --> 01:09:26,120
Like a threat isn't straight up affects us, straight up affects us.

648
01:09:26,120 --> 01:09:35,190
So you get to say whether or not you think that's significant. If this is anxiety, I don't know if this is I don't know.

649
01:09:35,790 --> 01:09:39,119
This is a mortality outcome. Maybe a percent and a half means something.

650
01:09:39,120 --> 01:09:42,870
I don't know. So this really does become a little bit more subjective.

651
01:09:43,140 --> 01:09:47,190
We can talk about it like empirically and again, whether we think we're doing better than chance.

652
01:09:47,610 --> 01:09:54,030
But this will ultimately come down to the root to feel that you're in this value will

653
01:09:54,030 --> 01:09:58,860
start to diverge from this one quite a bit more when we have multiple predictors.

654
01:09:59,460 --> 01:10:05,580
So in some ways, it kind of penalizes a model that includes a whole bunch of predictor values that don't do a good job.

655
01:10:06,480 --> 01:10:12,270
So if you have six other variables in here, none of which are related to anxiety,

656
01:10:12,720 --> 01:10:18,370
you'd probably still see this value at this point six, six, one, or maybe a little bit higher.

657
01:10:19,260 --> 01:10:22,530
All things being equal, adding predictors is not going to explain much variance,

658
01:10:23,250 --> 01:10:31,080
but you can think about things like a penalize adjusted R-squared value and would that would change based on the number of new predictors of that.

659
01:10:31,830 --> 01:10:34,440
So that's the just that's how you got to distinguish between those two.

660
01:10:34,820 --> 01:10:41,310
When in doubt, this is probably the more conservative in some cases, this one might be a little bit more accurate.

661
01:10:44,480 --> 01:10:56,460
Hmm. Bonferroni to. Hey, look at this.

662
01:10:57,660 --> 01:11:00,840
Thought you wouldn't have to use these math equations, but there it is, right.

663
01:11:00,870 --> 01:11:05,669
You can do this, right? Take this one to take this.

664
01:11:05,670 --> 01:11:09,540
Plug in it. When we calculate residuals, when we predict the casualty predicted scores.

665
01:11:09,840 --> 01:11:15,540
Which is kind of fun to do. I'm only going to take the survey and we could predict your anxiety based on what your self-acceptance is.

666
01:11:16,500 --> 01:11:17,790
Interesting, huh? All right.

667
01:11:17,790 --> 01:11:24,150
So, you know, plug in Amy's self-acceptance score, and we'd have a sense, at least a predictive value of Amy's anxiety score.

668
01:11:26,360 --> 01:11:29,690
I mean, I knew I had this life.

669
01:11:29,710 --> 01:11:35,060
I knew I had just one. So here's our analysis of variance table.

670
01:11:35,080 --> 01:11:44,340
It should tell us the same kind of information that we're seeing here and trying to do better than I could do here.

671
01:11:45,340 --> 01:11:49,480
We have kind of our prediction line, the error term,

672
01:11:49,490 --> 01:11:59,500
and we're going to use at the bottom this kind of mean squared error rate is going to be what helps us construct this interval around a regression

673
01:11:59,500 --> 01:12:07,720
line in which we hope to have a lots of or a little points clustered quite tight when we have a lot of little points clustered quite tightly.

674
01:12:08,260 --> 01:12:10,240
We have a good fitting regression line.

675
01:12:10,480 --> 01:12:16,750
We're going to have a big statistic when we have points flying all over the place, even though our regression line stays the same.

676
01:12:17,620 --> 01:12:27,790
Our squared error is going to be quite, quite big. This relative to this becomes much larger if value goes down.

677
01:12:29,770 --> 01:12:34,960
We saw the same thing with that graph last week when we were talking about grouping variables, right?

678
01:12:35,230 --> 01:12:40,510
We talked about when people's points were clustered around the groups versus when the groups were really far spread apart.

679
01:12:41,320 --> 01:12:50,070
Right. Similar principle. We get all these things, we take the say, we take our sun.

680
01:12:50,070 --> 01:12:56,490
The square is divided by the degrees of freedom. That's because there are tons and tons of people verses.

681
01:12:56,490 --> 01:12:59,940
In this case, we only have one group to compare it to.

682
01:13:00,360 --> 01:13:04,230
So that's where we get these mean square values that there are a little bit over to the right.

683
01:13:04,950 --> 01:13:10,820
But we're going to compare our sum to square regression relative to our error.

684
01:13:11,640 --> 01:13:23,370
Same here. Is it 5.47 divided by 8.32, which gives us this big old statistic, which kind of leads to our conclusion that again,

685
01:13:23,820 --> 01:13:28,410
relative to here in the model, what we predict seems to be pretty much.

686
01:13:32,360 --> 01:13:35,540
Question about that. This time of the week that apparently has happened.

687
01:13:40,260 --> 01:13:43,680
All right. We're going to do a lot of this next week.

688
01:13:43,690 --> 01:13:51,300
But in order for this to hold water, we need to make sure that some basic assumptions are met.

689
01:13:52,200 --> 01:13:58,529
One, that there's a linear, linear relationship between the two, the constant variance assumption that if we looked at our X value,

690
01:13:58,530 --> 01:14:05,730
we would see the residuals kind of distributed across kind of in a consistent way, normality of the residuals.

691
01:14:06,300 --> 01:14:09,660
Independence just means that the observations are related to each other,

692
01:14:09,960 --> 01:14:13,860
and that's where we get into some of those, like nested assumptions or longitudinal data or whatever.

693
01:14:14,400 --> 01:14:17,520
If this is the first time that you're seeing these, do not worry.

694
01:14:17,790 --> 01:14:23,340
Next week is entirely about this. It's entirely about figuring out what these assumptions are.

695
01:14:23,670 --> 01:14:32,630
And after we run our regression, taking a look back at all of these things to see whether or not they're they're met.

696
01:14:32,640 --> 01:14:36,030
These assumptions are met. We can even do a little bit of this work before we run the analysis.

697
01:14:36,600 --> 01:14:41,550
But once you can do here, your jobs are not done.

698
01:14:42,480 --> 01:14:45,570
You've got to follow up and make sure some of this stuff is there.

699
01:14:45,870 --> 01:14:52,560
Otherwise, the results are not trust. They're not right. Um, and again, we'll do this.

700
01:14:53,430 --> 01:15:03,600
We'll do some of this next week in a lot more depth. Okay, so this is just kind of an illustration.

701
01:15:03,810 --> 01:15:07,230
Next week, we will visualize these four assumptions.

702
01:15:07,410 --> 01:15:10,260
So we can we can pull out graphs of residuals.

703
01:15:10,590 --> 01:15:16,830
We can look at the different observations of people to see if they're more or less kind of randomly dispersed.

704
01:15:17,220 --> 01:15:20,490
We can look at those residuals to see if they change the levels of X value.

705
01:15:20,850 --> 01:15:25,590
I'm not going to go through a lot of that other than to say, in addition to visual representations,

706
01:15:25,920 --> 01:15:31,300
we can use the built in tests to determine, to try to see whether or not those assumptions are.

707
01:15:41,060 --> 01:15:48,020
What's this? This is really delicious. Did this matter to me just for you?

708
01:15:48,980 --> 01:15:54,920
Other than just to tell you one more time from the exact same procedures and t test for the regression coefficients.

709
01:15:54,920 --> 01:16:01,400
The F test for nova for the exact same thing. Compare what you're expecting from a sampling distribution to what we observed.

710
01:16:02,060 --> 01:16:04,100
I'll let you watch the video if you really want to see that.

711
01:16:05,870 --> 01:16:14,060
This is just so that you know, when we get those P values next to those regression coefficients, there is something going on behind, right?

712
01:16:14,120 --> 01:16:17,480
Those standard errors that we're talking about. They do come from somewhere.

713
01:16:18,260 --> 01:16:19,640
This is exactly where they come from.

714
01:16:20,270 --> 01:16:28,610
And because of this, we can make some inferences about whether or not they're significantly different from zero or some other value that we them.

715
01:16:29,480 --> 01:16:36,320
So we're not always just interested in knowing where their coefficient differs from zero, but perhaps from another value as well.

716
01:16:37,040 --> 01:16:50,320
All right. Um. Um.

717
01:16:51,440 --> 01:16:57,840
I think you're done. I think I'm done. I think we will start.

718
01:16:58,230 --> 01:17:03,030
We will have a look at a lot of this on Thursday. The nice thing is what we talked about so far.

719
01:17:03,540 --> 01:17:14,910
It extends logically to multiple regression. If if any of this is still at all, you feel a little uncertain.

720
01:17:15,810 --> 01:17:20,040
This is just the math behind what we've just been misinterpreting in words.

721
01:17:20,730 --> 01:17:26,280
Show me a note or what we'll do is something similar here on Thursday where an interesting question is up.

722
01:17:26,280 --> 01:17:29,740
And then I'll make sure that if we if we want to go through how we, you know,

723
01:17:30,390 --> 01:17:33,780
how we made this meeting, this value a little bit more before we do that.

724
01:17:34,140 --> 01:17:41,070
Okay. All right. Thank you all. I know this is a lot of information, but I promise you it's going to be popping back up again in your lives,

725
01:17:41,550 --> 01:17:47,540
at least while we're in this class together. So take a second and review if you can take a look at the video.

726
01:17:47,560 --> 01:17:52,860
If you have an opportunity for we'll do this and you'll do this on Thursday as prevalent.

727
01:18:12,550 --> 01:18:12,830
Yeah.

