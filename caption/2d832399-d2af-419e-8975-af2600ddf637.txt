1
00:00:01,470 --> 00:00:04,890
And those watching at home. We are just now moving into our missing dad example.

2
00:00:05,460 --> 00:00:10,470
But we're going to start with just two warmup questions. What are the five steps to testing and interaction?

3
00:00:11,850 --> 00:00:16,410
Well, we got folks with number one examined.

4
00:00:16,910 --> 00:00:23,760
And yeah, we're looking at correlations. We're looking to make sure that the variables make sense to include a linear model.

5
00:00:23,760 --> 00:00:30,889
Fantastic. About step two. The name of the manufacturing model.

6
00:00:30,890 --> 00:00:37,670
What is an ASX model. Multiple. It's multiple regression, right, without the interaction term.

7
00:00:38,090 --> 00:00:45,410
So if you have four variables and an outcome variable and you plan to create an interaction term across at least two of those variables,

8
00:00:45,950 --> 00:00:50,870
you first want to run that model with everything kind of on its own, no interaction terms.

9
00:00:51,260 --> 00:00:54,440
That's what we call a mean effect model. And what kind of case?

10
00:00:54,620 --> 00:01:02,330
See how these variables kind of all relate to each other. But those the effects, the data values, these things are all going to be independent.

11
00:01:03,020 --> 00:01:07,870
Okay. Step three, you add in. Step three, adding an interest.

12
00:01:07,880 --> 00:01:13,490
And so this would be step three here. So it's our two predictor variables and our interaction term.

13
00:01:13,820 --> 00:01:20,780
Wonderful step for. DNA diagnostics.

14
00:01:20,790 --> 00:01:27,529
Let's look at those residuals. Let's make sure that we don't have any coding error and let's make sure that there's no matter what residuals,

15
00:01:27,530 --> 00:01:37,099
you know, the form is going to be, whatever it's going to be. And then finally, step five, and we haven't had a lot of time to probe the interaction,

16
00:01:37,100 --> 00:01:43,940
but that would be looking in more specifically in terms of if we know that the association between, for example,

17
00:01:43,940 --> 00:01:51,110
X two and Y varies as a function of X three, we want to break down or sorry, x one,

18
00:01:51,320 --> 00:01:57,470
we want to break down that moderating variable to different levels and look at how the association changes.

19
00:01:58,070 --> 00:02:05,420
So we're going to create kind of a a stratified analysis where we look at how the association

20
00:02:05,420 --> 00:02:10,819
between our focal predictor and the outcome changes at different levels of our monitoring.

21
00:02:10,820 --> 00:02:18,440
For example, from Tuesday was depression.

22
00:02:19,100 --> 00:02:22,220
Depression regressed on social support and.

23
00:02:23,730 --> 00:02:28,970
What? H. Yeah, sure.

24
00:02:30,840 --> 00:02:35,910
So what we would do is we were interested in an association between social support and depression,

25
00:02:36,270 --> 00:02:39,480
and we want to know how it differs at different levels of age.

26
00:02:40,020 --> 00:02:47,910
So what we could do is we could break down our sample between those who are 15 and under those who are between 15 and 16, those who are 16 and over.

27
00:02:48,270 --> 00:02:52,710
And then we're going to examine that that association between parental social support and depression.

28
00:02:52,950 --> 00:02:58,109
For the young kids, the medium age kids and the older kids, that's probing the interaction.

29
00:02:58,110 --> 00:03:01,650
We're looking for the different levels and how they how they change things.

30
00:03:02,340 --> 00:03:04,260
It's also called examining the simple slopes.

31
00:03:04,680 --> 00:03:13,590
So what you find you essentially do is you rerun the multiple regression equation at different levels of the moderator.

32
00:03:14,100 --> 00:03:17,280
So we take the moderating variable, how we stratify our sample.

33
00:03:17,580 --> 00:03:24,000
And then we we're going to do regression analysis. I'm just going to assume that that's a little bit too much to take on just verbally.

34
00:03:24,240 --> 00:03:29,450
And so we can walk through an example of we need to. Five steps to an interaction.

35
00:03:29,840 --> 00:03:33,770
That's what I'm going to expect to see here. That's what I would encourage you to do in practice.

36
00:03:34,130 --> 00:03:42,710
If you are planning to test some sort of moderation and if needed, you will if you stay in this field, about number two that's going on here.

37
00:03:42,830 --> 00:03:59,420
Why is this so important? It tells you this was the slight accent because how we interpreted changes and when we add to our interaction terms.

38
00:04:04,500 --> 00:04:08,610
Sorry. Let me hear your interpretation changes. Once we add our interaction term,

39
00:04:08,610 --> 00:04:15,690
we move away from a one year increase in beta and the next quarter excuse gives me is this much of a change in our outcome variable?

40
00:04:16,350 --> 00:04:38,890
What is this telling us down here? Anybody can have effective x one on the outcome variable varies as a function.

41
00:04:40,960 --> 00:04:44,310
All right. You've heard me say it now a few times now. Quasar additive.

42
00:04:44,830 --> 00:04:51,580
What this is really saying. So if we had a normal multiple regression, we have this right.

43
00:04:52,120 --> 00:05:01,120
We have a one year change. I want you to change your next to write a one in a change.

44
00:05:01,360 --> 00:05:06,010
An x two is associated with about this much change in our predicted outcome variable.

45
00:05:06,640 --> 00:05:18,250
Does anybody have questions about that? Once we've added this interaction term to Quest Point, we no longer talk about x two by itself.

46
00:05:19,180 --> 00:05:20,650
This value.

47
00:05:23,160 --> 00:05:39,420
Now is complemented by this interaction term such that this the effect of X to our outcome variable changes as some function of the interaction term.

48
00:05:40,680 --> 00:05:41,670
The first variable,

49
00:05:43,080 --> 00:05:53,730
it's not I mean it's the data value that changes based on the level or value of the other variable that which is kind of interacting with it.

50
00:05:54,540 --> 00:05:56,280
That's the key to moderation.

51
00:05:57,540 --> 00:06:07,290
We are talking about how one variable and its effect on the outcome variable changes based on the level of this variable.

52
00:06:08,190 --> 00:06:19,380
That's what this term is kind of saying. This is our new fancy way to write and I'm sorry for the math, but that's really what's going on here.

53
00:06:19,800 --> 00:06:28,050
If you can kind of capture that distinction between multiple regression and that which has an interaction term, that's the key to this week.

54
00:06:28,830 --> 00:06:36,990
And it's this really neat part where we take this kind of interaction value variable and how it changes data when it was just by itself.

55
00:06:38,700 --> 00:06:44,250
Now what's great about multiple regression with an interaction term, it's still one you to change,

56
00:06:45,090 --> 00:06:50,640
but at one year to change that leads to this change in the outcome variable depends.

57
00:06:50,790 --> 00:06:55,590
Depends on the level of that or sorry, the level of x one.

58
00:06:56,740 --> 00:07:02,200
All right. The change, the effect that social support has on your depression,

59
00:07:02,650 --> 00:07:13,120
changes based on how old you are or what sex you identify or what school you went to, or how much your money you made or whatever.

60
00:07:13,120 --> 00:07:19,090
Right? Whatever is moderating the association between social support and depression from our end, I want.

61
00:07:21,690 --> 00:07:30,580
That's how important. I got the master spinner because these exponents two variables they and their future.

62
00:07:31,400 --> 00:07:35,430
Yeah. Yeah. You can easily sell. These will all be the same.

63
00:07:35,640 --> 00:07:43,050
We could flip this if you want. It's a nice little thing to do in practice. We could change those to beta one and still then three,

64
00:07:43,650 --> 00:07:49,560
which is kind of neat because we still take the same interaction term so that then these two in front.

65
00:07:53,590 --> 00:07:59,940
Questions before I move on to some data. All right.

66
00:08:02,850 --> 00:08:06,059
Try to understand a little bit what have you noticed?

67
00:08:06,060 --> 00:08:07,680
And I know we did a little bit of math here,

68
00:08:07,950 --> 00:08:14,459
but the big thing is more of a conceptual understanding that what your moderator is really doing and now try to translate that

69
00:08:14,460 --> 00:08:21,660
into the types of questions that you're that you're asking is moderation the appropriate question for the thing I'm trying to do,

70
00:08:21,900 --> 00:08:30,720
do I believe that one association in my world that I care so much about is moderated or changes as a function of another area?

71
00:08:32,100 --> 00:08:44,030
And those are some pretty cool, nuanced questions, or would you just put it that way?

72
00:08:45,000 --> 00:08:53,560
All right. If you want to follow along with this example, you may we are going to go through some missing data.

73
00:08:53,580 --> 00:08:58,020
I am recording this. So if you would just assume continue working on your homework assignment, you may also do that.

74
00:08:59,970 --> 00:09:07,000
Or if you don't, you're open to that, too. But it's really tiny down here at the bottom of of six.

75
00:09:07,380 --> 00:09:16,710
There's the script in the data file that that I'm and I'm probably going to have to change my own.

76
00:09:17,620 --> 00:09:20,940
We'll see how far I've got to read it at some point. All right.

77
00:09:21,900 --> 00:09:28,050
So missing data. It's got a it's a it's a it's own thing within our.

78
00:09:28,350 --> 00:09:30,450
So the missing data is going to be important everywhere.

79
00:09:31,050 --> 00:09:37,860
But our has been impressive is that there are just has some very particular ways of handling missing data that we want to be aware of.

80
00:09:38,610 --> 00:09:44,249
And this is how we run into some of the issues that folks talked about with their homework,

81
00:09:44,250 --> 00:09:47,070
for example, the residuals, not lining up and stuff like that.

82
00:09:48,060 --> 00:09:52,530
So there is a worksheet in my, I think, the resources that kind of goes through this as well.

83
00:09:53,250 --> 00:09:54,930
But just starting from the very beginning,

84
00:09:57,450 --> 00:10:03,689
missing data has a class in and all of its own and that it's logical we can ask true false questions around missing data.

85
00:10:03,690 --> 00:10:07,020
It's kind of funny, but it's the way that our wants to treat it.

86
00:10:07,030 --> 00:10:15,870
So when we create, for example, a small little data set that has four values and we ask what kind of values are,

87
00:10:15,870 --> 00:10:23,820
these is going to say these are all numeric areas which makes sense to us if we run line seven here asking the same question.

88
00:10:24,270 --> 00:10:34,560
Notice what's kind of funny about our is sometimes it overwrites what in our mind is a little bit more obvious.

89
00:10:34,890 --> 00:10:38,520
So we have one character value, we have three numeric values.

90
00:10:38,520 --> 00:10:47,130
But when we ask are what we're dealing with, it's going to coerce this entire set of many set of data and say that these are all character values.

91
00:10:47,640 --> 00:10:53,190
So just one more reminder that we have two to consider when we are working with our data.

92
00:10:53,370 --> 00:10:57,720
Are we dealing with factor variables, married variables, character variables, whatever?

93
00:11:01,570 --> 00:11:04,750
Missing again, because it's its own special thing.

94
00:11:05,440 --> 00:11:13,810
Our knows how to handle this. So when we have a missing value notice in this little data set, it recognizes that I'm going to kind of skip this.

95
00:11:14,830 --> 00:11:19,030
I'm going to skip this and then go to what else is in my little from my little dataframe here.

96
00:11:19,990 --> 00:11:25,540
Okay. So what's really nice and convenient about missing values is rather than having nothing.

97
00:11:25,930 --> 00:11:29,139
Or rather than having some other form of substitute r,

98
00:11:29,140 --> 00:11:36,010
we'll have some well-defined sets of operations that it will follow if it sees an A within the dataset.

99
00:11:36,850 --> 00:11:43,959
Okay. So in this case here, I'm creating a vector, which is just a mini dataset, and I'm going to have,

100
00:11:43,960 --> 00:11:48,550
let's say, one, two, six, seven values, three of which are going to be considered missing.

101
00:11:50,140 --> 00:12:00,430
Okay, easy one, sweetie. We can do some certain operations with our based on our missing values.

102
00:12:00,850 --> 00:12:04,780
This is a nice one for us thinking about, well, does my data have missing us?

103
00:12:05,410 --> 00:12:09,020
So the question simply is, are there missing values?

104
00:12:09,040 --> 00:12:13,150
Is any it's going to ask this across my vector or across my little dataframe.

105
00:12:14,110 --> 00:12:15,040
Are there missing values?

106
00:12:15,040 --> 00:12:22,950
And notice that I said at the very, very beginning, this is a logical it's a it's class logical, which means it's going to assume essentially true.

107
00:12:22,960 --> 00:12:26,620
False. Is this a missing value? Is this a missing value? Is this a missing value?

108
00:12:27,010 --> 00:12:32,320
And so what we see here is when there is an actual value, it adds there's no to that question, is this value missing?

109
00:12:32,770 --> 00:12:36,820
If there is a missing value, it's going to say yes or true to that question.

110
00:12:37,270 --> 00:12:43,390
So it gives us a way to identify if any of the data in our dataframe is missing.

111
00:12:44,350 --> 00:12:49,180
Usually a big print out like this is manageable when we have 20 observations.

112
00:12:49,420 --> 00:12:56,889
If we had 150, not so much. So we can use other features of our other operations to more or less.

113
00:12:56,890 --> 00:13:01,870
Kind of summarize how much missing data do we have? Right.

114
00:13:01,880 --> 00:13:08,170
So instead of asking the first dataset, how many of my 150 or 850 observations of alcohol use are missing?

115
00:13:08,740 --> 00:13:15,040
We can. Or sorry. Instead of actually running it out and trying to count, we could ask very specifically, can you sum this?

116
00:13:15,610 --> 00:13:21,550
And I would use different commands and functions like this when we're doing things like counting residuals or whatever.

117
00:13:21,790 --> 00:13:27,030
You can use little kind of cheats or operators to make some of this stuff easier.

118
00:13:28,180 --> 00:13:29,740
If you're running around,

119
00:13:29,760 --> 00:13:40,210
that is accounting for all of the missing values in a file or just a certain variable or a new set of specific variable to see which how many missing?

120
00:13:40,570 --> 00:13:46,240
It depends. So if if you run the whole entire data set, it's going to catch it's going to start at the column level.

121
00:13:46,570 --> 00:13:48,129
If you want to say specifically,

122
00:13:48,130 --> 00:13:54,040
I have one column or one variable and I want to count that you would want to indicator with the dollar sign or whatever you say.

123
00:13:54,160 --> 00:13:58,510
First dollar sign feature orientation. All right.

124
00:13:58,810 --> 00:14:01,840
So that's just when we're playing around with data on our own.

125
00:14:02,590 --> 00:14:09,280
But more often in practice, we're going to use different sort of functions or arguments related to missing values.

126
00:14:09,760 --> 00:14:16,420
The NRA Remove any grammar command is one that you're going to see throughout this class.

127
00:14:16,780 --> 00:14:23,950
It's built in as an argument to a lot of other functions like the function ILM linear model, right?

128
00:14:23,950 --> 00:14:30,700
So one of the arguments, which means one of the values that we can put in after our equation is what to do with missing data.

129
00:14:31,240 --> 00:14:38,590
So you're going to have to look out for the R and the NAGARJUNA'S across the different functions and packages that you use.

130
00:14:38,920 --> 00:14:43,810
How are we going to tell our to handle our missing data? Now, this is just an example.

131
00:14:44,140 --> 00:14:51,010
If you're following along, you're not going to be perfectly because the very first thing I'm going to do is ask for R to

132
00:14:51,010 --> 00:14:57,220
generate a random sample of 25 values from a normal distribution with a standard deviation of ten.

133
00:14:58,720 --> 00:15:12,570
Just for the sake of explication here. Which I can run basic descriptors on my little creative dataframe.

134
00:15:13,140 --> 00:15:17,670
So this is the smallest value of my normal distribution with standard deviation.

135
00:15:17,680 --> 00:15:21,900
Ten is my largest value. It's going to change based on who you are.

136
00:15:22,350 --> 00:15:26,750
The reason why these are perfect like 19 and 18 is that's what this is, our ashes.

137
00:15:27,210 --> 00:15:28,740
That's what this as integer means.

138
00:15:28,980 --> 00:15:35,220
It's going to round it or it's going to make it specifically integers rather than, you know, 0.24, four, four, four one or something like that.

139
00:15:37,350 --> 00:15:41,320
Right. So my little mini dataframe, it's called Sample 25.

140
00:15:41,340 --> 00:15:44,910
There it is. Now let's add a missing value.

141
00:15:45,960 --> 00:15:47,610
And this is where we have to be careful.

142
00:15:47,910 --> 00:15:57,600
So notice now when I tried to run the same command on a dataframe that has one missing value, one missing value out of 25.

143
00:15:57,990 --> 00:16:03,030
And when I return, the minimum. If I get an in a I get in any back.

144
00:16:03,600 --> 00:16:07,050
The reason why is it doesn't know where to put that any value.

145
00:16:07,740 --> 00:16:11,819
It's looking for the very, very smallest. And it comes to nine -19.

146
00:16:11,820 --> 00:16:15,890
But it also has this and doesn't know what to do with it. So it just returns that.

147
00:16:15,900 --> 00:16:19,650
It's the same thing with Max. Same thing to me.

148
00:16:20,850 --> 00:16:24,320
So you might see this. You might think, okay, clearly an error.

149
00:16:24,600 --> 00:16:29,010
What it means is we're asking R to do something with missing values and it doesn't know what to do.

150
00:16:30,130 --> 00:16:35,940
Right. So here is an example of how we can use this arm command.

151
00:16:36,610 --> 00:16:40,680
We're basically saying we recognize that there is missing data in our sample.

152
00:16:41,280 --> 00:16:47,580
Can you please take it out? So this is an argument for a part of the main command, max command and medium command.

153
00:16:47,820 --> 00:16:52,500
I can't just make it do this. We have to you have to pay attention to what are some of the things.

154
00:16:52,500 --> 00:17:02,160
And we can always go like you can always use this args or looking to help menu to figure out what is it, what is it expecting.

155
00:17:03,300 --> 00:17:07,230
Right. This is the default that it's expecting is that there's no missing values.

156
00:17:08,130 --> 00:17:11,460
We are going to save and in fact. Aha, there is a missing value here.

157
00:17:12,510 --> 00:17:17,760
There it is. So now we have removed or held out the missing value.

158
00:17:17,770 --> 00:17:22,739
It can identify what is is now the smallest numeric value.

159
00:17:22,740 --> 00:17:28,200
We are in a seven example. Yeah. So just a technical question.

160
00:17:28,200 --> 00:17:32,670
Why do we have to put through versus just putting any cover up for interaction?

161
00:17:34,260 --> 00:17:37,680
So I think because normally there are other ways that you can respond to this

162
00:17:37,680 --> 00:17:42,420
question we have to put true because the default is to say that there's no missing.

163
00:17:42,990 --> 00:17:50,460
If the default was true, then we wouldn't have to worry about it. But if you just put in a dot arm equals nothing, it's going to return an error.

164
00:17:50,790 --> 00:17:54,070
All right. Thanks, Sam.

165
00:17:54,640 --> 00:18:01,510
All right, so the next example is going to require that you use the car package.

166
00:18:04,330 --> 00:18:10,960
And lines when 50 through 50 or 53 here are just pulling in data.

167
00:18:11,290 --> 00:18:16,450
So I'm asking for the answer from data. So a lot of these packages have their own kind of built in data sets that always work.

168
00:18:17,380 --> 00:18:21,100
So I don't like them. This one is U.S. public school expenditures.

169
00:18:21,970 --> 00:18:31,090
I'm asking for three columns and I'm or I'm putting three missing values into a column here.

170
00:18:33,720 --> 00:18:38,130
So here's my data clicker here.

171
00:18:38,370 --> 00:18:45,780
These are the three missing values that I inserted into y one just as just to create the missing values.

172
00:18:46,440 --> 00:18:52,320
All right. So I'm going to create an outcome variable that's going to have a predictor variable that has the missing data.

173
00:18:53,040 --> 00:18:58,290
All right. Now, here is the big crux. And this is what I want to get through before folks start to go home.

174
00:18:58,680 --> 00:19:04,350
We're going to run the same old one with any old mitt and one of any exclusion.

175
00:19:04,890 --> 00:19:13,620
So notice now we're not using the any doorframe command which was remove this or kind of hold this aside for the l m function.

176
00:19:14,790 --> 00:19:20,010
We have an argument called an a action. What to do with my nesting values.

177
00:19:20,490 --> 00:19:25,770
Okay. You can look through the any help menu and all that kind of stuff.

178
00:19:25,770 --> 00:19:28,380
And it's going to tell you some information about what to do with missing values.

179
00:19:28,620 --> 00:19:35,360
But the two that we're almost always going to use are either an eight or Mitt or an A exclude.

180
00:19:36,180 --> 00:19:42,180
Now, in practice for the results, it's not going to have any implication.

181
00:19:42,990 --> 00:19:46,890
The reason is we can't run a regression with missing data here.

182
00:19:47,310 --> 00:19:53,220
All right. It doesn't. It's handling in a different way. Won't change the model coefficients.

183
00:19:53,670 --> 00:20:01,770
It won't change your sample size and it won't change your R squared like the fitted values of our or how well the data kind of maps on to the model.

184
00:20:02,460 --> 00:20:10,590
What it's going to do is have an implication for the residuals that we hang on to at the back end of the model.

185
00:20:12,240 --> 00:20:15,600
So when we use the enablement command,

186
00:20:16,170 --> 00:20:21,360
it's going to drop any of those missing data points from the sample in the sense

187
00:20:21,360 --> 00:20:26,220
that it's almost like a list wise deletion folks familiar with that term.

188
00:20:27,780 --> 00:20:37,890
Imagine those three people are now not part of the sample and I exclude is going to more or less use a pairwise deletion.

189
00:20:38,610 --> 00:20:40,890
It's not going to drop them from the sample,

190
00:20:41,460 --> 00:20:47,890
but it still can include them in the analysis because if you have the missing data, you can't be part of the analysis.

191
00:20:47,910 --> 00:20:50,160
We don't know how to do that. We couldn't calculate an effect for it.

192
00:20:51,540 --> 00:21:00,450
So what's important to recognize is that the models, if we did like the summary model or myth summary model exclude, you get the same results.

193
00:21:00,720 --> 00:21:06,840
That's not going to differ. What differs is the objects when we create them and we look at those residuals.

194
00:21:07,110 --> 00:21:10,020
I don't know if I've shown folks, I think I've done kind of on occasion,

195
00:21:10,500 --> 00:21:15,900
but when we look down here in this data environment and we click on a regression object that we've created,

196
00:21:16,440 --> 00:21:23,880
it has all this stuff, all this great stuff that it includes and we don't even think about, right?

197
00:21:23,890 --> 00:21:28,440
So all the model fits, all the beta coefficients.

198
00:21:28,440 --> 00:21:33,569
This is how we are able to run that summary command and get our output so we

199
00:21:33,570 --> 00:21:36,750
can run that now and beta command and get those standardized coefficients.

200
00:21:36,990 --> 00:21:45,300
This is how we can run those residual analysis. All of this is built into that object that we created model exclude right here.

201
00:21:45,310 --> 00:21:51,270
So when we ran this linear, linear model and we set it as an object, that's where all this stuff kind of gets stored.

202
00:21:52,200 --> 00:22:02,220
Now let's take a look at the residuals. Good time, reminder of can somebody tell me what residuals, Jenny?

203
00:22:04,630 --> 00:22:10,360
It's the difference between the value that the model predicts and the actual value.

204
00:22:10,590 --> 00:22:15,730
Yes. A lot. Yes. So we use these variables to predict and we use y one to predict y two.

205
00:22:16,240 --> 00:22:24,310
So we should see based on our our model, these 11 observations, everyone's going to have a predicted value in an observed value.

206
00:22:28,780 --> 00:22:37,300
If you look at this read the residuals for the NHL and the NHL mid commands of 55.

207
00:22:37,960 --> 00:22:49,510
We have 12345678 residuals we have list wise deleted the are these three missing values.

208
00:22:49,630 --> 00:22:52,660
We had 11 we started with 11 but there were three missing values.

209
00:22:53,110 --> 00:22:59,250
So these residuals, we only have eight. Now, tell me, what do you think you're going to see when I run line 60?

210
00:23:00,000 --> 00:23:04,780
Well, we have more or less residuals. More or fewer.

211
00:23:08,220 --> 00:23:11,790
Why? Because it's including the.

212
00:23:13,200 --> 00:23:16,410
The people and the environment is no different.

213
00:23:17,960 --> 00:23:23,460
Sure. We think we're going to have residuals for those three missing values.

214
00:23:29,600 --> 00:23:33,830
We're actually going to have the same number of residuals,

215
00:23:34,610 --> 00:23:40,370
residual being strictly the difference between what the Model Observer predicts and what we observe in practice.

216
00:23:41,210 --> 00:23:50,540
The biggest difference here is that we still have, I call it, kind of holding places in line for the three people who had missing observations.

217
00:23:51,740 --> 00:23:56,240
Now, for those of you that ran into an issue and I'm sorry if it wasn't built in my code,

218
00:23:56,240 --> 00:24:03,290
I should have used exclude in the multiple regression example for those of you that use any or Ms.

219
00:24:03,830 --> 00:24:10,490
If you tried to map the residuals from one model to another where there was some additional missing data,

220
00:24:10,910 --> 00:24:18,860
you're going to run into a problem like this. So here these 11 observations have been reduced down to just eight.

221
00:24:19,400 --> 00:24:26,480
And we cannot do any sort of appending of a frame that has 8 to 8 values to one that has 11.

222
00:24:27,320 --> 00:24:37,309
So I can't save these residuals to my actual dataset, my handscomb dataset, which has 11 values because it doesn't map on.

223
00:24:37,310 --> 00:24:45,650
Right. I would have have to add values one, two and three or figure out some way to delete the three people from the onscreen data.

224
00:24:46,640 --> 00:24:49,040
When I save my residuals like this.

225
00:24:49,640 --> 00:24:58,760
I won't have an issue, even though three of them have in a remembered how horror kind of treats that any a is a very special value.

226
00:24:59,450 --> 00:25:04,970
It allows it to kind of say this person is still here, but not for this analysis.

227
00:25:06,440 --> 00:25:13,280
Kind of funny, but really important for some of the comparisons that we're going to notice here, though,

228
00:25:17,000 --> 00:25:26,750
we get these still the exact same values because our fitted statistics are just what the model is predicting based on the observations that we have.

229
00:25:27,740 --> 00:25:43,490
So same kind of thing in that for our excluded model, we have 11 spaces, eight values, whereas here we only have eight spaces and eight values.

230
00:25:46,410 --> 00:25:53,250
For a residual analysis. It doesn't matter for understanding the coefficients and calculating the coefficients.

231
00:25:53,460 --> 00:25:56,190
It doesn't matter because the data are the exact same.

232
00:25:56,460 --> 00:26:05,160
It's only in a couple of special instances where we're trying to to work with the like the whole data frame,

233
00:26:05,160 --> 00:26:08,010
if you will, that this is going to make a big difference to your life.

234
00:26:08,970 --> 00:26:16,410
So when in doubt, I would probably use exclude rather than omit, but it's really kind of a preference thing.

235
00:26:16,710 --> 00:26:21,990
What I just need you to know is what are some of the similarities and what are some of the differences?

236
00:26:22,380 --> 00:26:24,990
And what that's going to do is help you diagnose some of the challenges.

237
00:26:24,990 --> 00:26:30,240
If we're like kind of mapping on data's, you know, just different data frames and stuff,

238
00:26:30,240 --> 00:26:33,900
we do a fair amount of merging between datasets and data frames and whatnot.

239
00:26:34,290 --> 00:26:42,390
This would be an example. So if you ever plan to create like a residual variable or a cook's distance variable, this is going to be pretty important.

240
00:26:43,380 --> 00:26:48,600
So we couldn't use the cook's distances from this model and directly append them.

241
00:26:48,900 --> 00:26:58,620
To our answer comes data and our original dataset. We could here I can attend or add a column called Fitted to my Hanscom data.

242
00:26:59,160 --> 00:27:04,760
I cannot add this column called Fit. Questions.

243
00:27:06,620 --> 00:27:14,120
Yeah. So when we ran the original regression, I did use the.

244
00:27:16,280 --> 00:27:20,030
Okay. Still below. Got that. An error with the enrollment.

245
00:27:20,030 --> 00:27:26,600
So where do you tell it that. So that could be if the models actually have truly different people.

246
00:27:27,200 --> 00:27:36,890
So this is trying to take what I guess AIDS observations and map them on to 11 observations.

247
00:27:37,290 --> 00:27:44,989
Yours, you might actually have different numbers of people in the models, so you might have like 820 versus 835, which is a different sort of problem.

248
00:27:44,990 --> 00:27:48,020
So I can help troubleshoot that afterwards.

249
00:27:48,410 --> 00:28:00,110
So we are at a time, but I do want you to if you want, you can spend just a couple of minutes and you can see how this matters for some of the data.

250
00:28:00,110 --> 00:28:05,750
So this is the self acceptance and anxiety example and you can see how this will make a difference,

251
00:28:06,140 --> 00:28:11,120
mostly for taking your residuals and creating that variable.

252
00:28:11,360 --> 00:28:22,310
And the other reason why, again, you'd want that is, I don't know, maybe for some of your diagnostics, maybe for, I don't know, plotting.

253
00:28:24,200 --> 00:28:37,460
It's just good to know that, uh, so this last one is just kind of a funny, uh, goof that I recognize a year or two ago.

254
00:28:38,150 --> 00:28:44,420
This is more about skill creation. And so I might pose this to a challenge to folks on next week.

255
00:28:44,960 --> 00:28:48,290
All right. So next week, here's what I'm thinking, folks.

256
00:28:49,310 --> 00:28:55,670
I want to spend a little bit more time on Tuesday.

257
00:28:59,540 --> 00:29:01,760
I want to give you another crack in multiple regression.

258
00:29:02,900 --> 00:29:09,570
So what I'm going to ask is that you try to do your best to watch the videos this weekend that are going to be on scale analysis,

259
00:29:09,570 --> 00:29:14,690
factor analysis, that type of thing. We're going to spend Tuesday with an example.

260
00:29:15,740 --> 00:29:22,760
We're going to do a clean example. I have one in mind that's going to work out pretty well, so there won't be quite so much coding involved.

261
00:29:23,000 --> 00:29:27,380
It's going to be run my regression model, interpret the results.

262
00:29:27,980 --> 00:29:31,730
All the plots are going to work out really well. All the residual analysis are going to work out well.

263
00:29:31,910 --> 00:29:35,390
I'm going to ask you to to to work maybe in partners or teams or something like that.

264
00:29:35,630 --> 00:29:37,280
And we can run through those things together.

265
00:29:37,550 --> 00:29:41,870
And then if folks are feeling good with that, then I'm a lot more comfortable with moving forward with the midterm.

266
00:29:42,290 --> 00:29:45,590
I've got to figure out how this is going to work. But again, don't or any fall breaks.

267
00:29:46,490 --> 00:29:50,090
But I also just based on some of the questions and where folks are.

268
00:29:50,330 --> 00:29:57,110
I'm not going to rush the midterm and just have folks kind of rush through that piece of it and maybe not fully internalize any information.

269
00:29:57,560 --> 00:30:02,150
So we'll spend a good chunk of time on Tuesday going through an example.

270
00:30:02,780 --> 00:30:05,270
I'd like you to work on teams first or if you want to write yourself.

271
00:30:05,960 --> 00:30:12,440
And then once we're good with the multiple regression pieces, we can move on a little bit to the scale analysis.

272
00:30:12,700 --> 00:30:17,659
We won't forget about the interaction part of it, although this is pretty advanced stuff.

273
00:30:17,660 --> 00:30:23,750
This is like second semester Ph.D. program level stuff, as is a lot of the rest of the class.

274
00:30:24,050 --> 00:30:27,090
So we'll return to this now.

275
00:30:27,110 --> 00:30:33,290
It's a good thing. It's a good thing you read your friends. So we will return to this at different times.

276
00:30:33,950 --> 00:30:38,540
And I will I'll try to keep it kind of front and center in your in your your mind.

277
00:30:38,780 --> 00:30:40,260
But well, let's get to this.

278
00:30:40,280 --> 00:30:46,340
Make sure that we're feeling real, real good about the multiple regression stuff, the diagnostic stuff, before we move too much further forward.

279
00:30:46,460 --> 00:30:51,440
Okay. So plan on that. But it's going to take a little bit of a commitment from you to please watch that video.

280
00:30:52,050 --> 00:30:57,260
The like the scale analysis video because we going to spend as much time on Tuesday going through some of those scale analysis.

281
00:30:57,470 --> 00:31:00,160
And I will apologize one more time. I'm going to do it this afternoon.

282
00:31:00,170 --> 00:31:05,389
I will make my own video of me walking through my homework assignment three And because I've been so delinquent,

283
00:31:05,390 --> 00:31:09,740
I will also walk through the residual part of it too. That's not due until this evening.

284
00:31:10,340 --> 00:31:14,110
And if it's not going to get done this evening, tomorrow's fine set is fine.

285
00:31:14,120 --> 00:31:20,269
I'd rather you give it the time and attention. Then just again, trying to submit something, because I know everybody's schedules are crazy right now.

286
00:31:20,270 --> 00:31:23,960
So thank you all very much. I appreciate your question. Appreciate your attention.

287
00:31:24,110 --> 00:31:27,320
I will see you on Tuesday. Have a great weekend. Thank you.

288
00:31:31,450 --> 00:31:34,690
I don't mean that. That's just.

