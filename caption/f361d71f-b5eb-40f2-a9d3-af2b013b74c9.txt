1
00:00:00,930 --> 00:00:06,060
Well, hello, everyone. I hope that you had an enjoyable time outside with the fire alarm.

2
00:00:07,080 --> 00:00:11,730
So I'm going to finish up talking about hacking.

3
00:00:12,300 --> 00:00:15,930
You know, I do want to spend too much time on this. It's the end of our half term.

4
00:00:15,930 --> 00:00:23,709
But I do think some of this is really important. Okay.

5
00:00:23,710 --> 00:00:27,000
So. We.

6
00:00:27,000 --> 00:00:36,780
What I was saying is that we should be able to prevent p hacking through some standards that we have in science, in epidemiology.

7
00:00:37,230 --> 00:00:44,010
And I think that our culture of epidemiology should be one where we are thinking thoughtfully about the study that we want to do.

8
00:00:44,100 --> 00:00:54,270
And we have this consistent thread in the science from the process of getting funding to do our study to actually and is placing the results.

9
00:00:55,200 --> 00:01:02,189
But you know, what happens a lot is we, you know, if we're doing secondary data analysis,

10
00:01:02,190 --> 00:01:08,520
we are presented with options for doing all sorts of of crazy analysis which which could be good and could be really helpful.

11
00:01:08,910 --> 00:01:14,880
So that paradigm that I'm talking about is maybe a bit idealistic.

12
00:01:16,050 --> 00:01:23,160
Parking can be exacerbated by the ongoing culture of like what's going on in in journals.

13
00:01:23,280 --> 00:01:28,589
So this is something from Lancet. It's from Lancet global health.

14
00:01:28,590 --> 00:01:32,880
But they have similar policies across the whole suite of Lancet journals.

15
00:01:33,660 --> 00:01:40,830
And they mentioned that they prioritize original research that is likely to change clinical practice or thinking.

16
00:01:41,490 --> 00:01:49,530
And they are looking for a strong or unexpected beneficial or adverse response or a novel mechanism of action.

17
00:01:50,160 --> 00:01:54,630
To me, this screams we want a p value under .05.

18
00:01:54,720 --> 00:01:57,960
Like they want weird results. They want new things.

19
00:01:58,290 --> 00:02:00,210
They don't want the status quo.

20
00:02:00,780 --> 00:02:08,760
And what that means in practice, I think, is they're going to be publishing a lot of stuff which might, in the long term, not be shown to be true.

21
00:02:09,150 --> 00:02:18,389
And if you recall, Lancet was also the Journal in 1998, which published the study by Asher Wakefield,

22
00:02:18,390 --> 00:02:28,830
which purported that there was this relationship between vaccination and autism through this this weird gastrointestinal disease mechanism.

23
00:02:29,070 --> 00:02:34,920
But that's all to say that I think if this journalists like, we want to see results, we want new things.

24
00:02:34,920 --> 00:02:37,950
We want p values that are point of five.

25
00:02:37,950 --> 00:02:43,950
They're more likely to be publishing these types of articles which come out of left field and which could say actually

26
00:02:43,950 --> 00:02:50,880
some pretty harmful things than they are to just kind of be consistent with the existing evidence being added on.

27
00:02:51,030 --> 00:03:00,450
You know, these these aren't going to be journals which look at which published studies which would show the default status quo.

28
00:03:05,230 --> 00:03:12,850
Okay. So again, I think what we want to do is we want to first develop a hypothesis, do a sample size calculation, or we could do a power analysis.

29
00:03:12,850 --> 00:03:17,860
If we're doing a secondary data analysis, if we have primary data collection,

30
00:03:17,860 --> 00:03:21,550
obviously you can collect your own data and then we analyze data for that one association.

31
00:03:22,030 --> 00:03:36,160
I think what happens in reality, you know, there could be something which is called pricking, which is hypothesis after results.

32
00:03:36,550 --> 00:03:47,220
No. And the idea of harking is that people start here, they're analyzing data, they analyze lots of different,

33
00:03:49,800 --> 00:03:54,810
different, different associations, and then they go back and develop their hypothesis.

34
00:03:54,810 --> 00:04:00,990
And that's not that's not correct. And that can lead to I mean, I would say that probably leads to packing in in all situations.

35
00:04:03,630 --> 00:04:11,190
So what should we do? You know, I mentioned multiple testing earlier use on Bonferroni or if there's other or other ways that you can do that.

36
00:04:11,700 --> 00:04:18,930
We could think about using smaller alpha levels. You know, there are always those to just change the standard alpha level 2.05 and of .05.

37
00:04:19,200 --> 00:04:23,700
And that just makes it a harder to pack because they're that much more sample size.

38
00:04:25,170 --> 00:04:29,159
But I think a big thing is just to distinguish between exploratory and confirmatory research.

39
00:04:29,160 --> 00:04:34,530
So exploratory research could be you going out there saying, I have a secondary data set that I'm looking at.

40
00:04:34,530 --> 00:04:39,389
I didn't collect the data, but I'm just looking to see what our predictors of diabetes are like,

41
00:04:39,390 --> 00:04:46,660
what nutritional factors are related to cancer in an exploratory fashion.

42
00:04:46,680 --> 00:04:52,799
You could you could run a lot of different things and just report them and just say that you don't have a hypothesis guiding this.

43
00:04:52,800 --> 00:05:00,180
This is just exploratory. But then people who read that, no, and then it wouldn't necessarily be counted as packing because you're not,

44
00:05:00,180 --> 00:05:03,810
you know, first developing your hypothesis and then going forward from there.

45
00:05:05,870 --> 00:05:09,259
And then I think people would also potentially quite a bit less weight on your

46
00:05:09,260 --> 00:05:14,989
results just because we we think that maybe this is not not confirmatory,

47
00:05:14,990 --> 00:05:20,330
because confirmatory would be where from what we know, the research or in the existing literature base,

48
00:05:20,330 --> 00:05:25,400
we like know what the direction of the association should be or we have a strong feeling for it.

49
00:05:25,730 --> 00:05:35,180
And our study can try to be in accordance with what we already know from theory and what we know from past research.

50
00:05:36,740 --> 00:05:43,100
So I would love it if we could move to a model where journal submissions are

51
00:05:43,100 --> 00:05:47,299
based on the quality from the instruction methods and not based on results.

52
00:05:47,300 --> 00:05:50,930
Like I would love that, but I don't know of any journal which is like that.

53
00:05:51,200 --> 00:05:58,819
I would say the big distinction in journals nowadays is that there are some journals which will just they will publish a study if it is high quality.

54
00:05:58,820 --> 00:06:03,230
And there could be, you know, distinctions across studies of what's high quality or not.

55
00:06:03,650 --> 00:06:13,129
But then there are selective journals which publish if they think that your results are interesting or like what we saw from that Lancet journal,

56
00:06:13,130 --> 00:06:17,330
if it's likely to change clinical decision making or something like that.

57
00:06:19,540 --> 00:06:22,480
But again, I think those journalists are just going to be encouraging packing.

58
00:06:23,770 --> 00:06:29,650
If possible, you should be adding your data to a publicly accessible database.

59
00:06:29,950 --> 00:06:34,719
And there's there's many out there. And actually, it's going to be a condition of many cities funding in the future through the

60
00:06:34,720 --> 00:06:39,730
NIH that researchers need to upload their data to some public repository.

61
00:06:43,960 --> 00:06:50,410
So when we're thinking of multiple studies that are being conducted on the same topic,

62
00:06:51,400 --> 00:06:56,920
we can think of a few different ways to compare consistency in these results.

63
00:06:57,230 --> 00:07:00,970
So there's repeatability, replicability and reproducibility.

64
00:07:01,630 --> 00:07:07,780
Repeatable means that the same analyst is looking at the same basically like you have your illy project.

65
00:07:08,080 --> 00:07:13,950
You did it. If you were to run it from scratch from start to finish, would you get the same results?

66
00:07:13,960 --> 00:07:18,130
And ideally that should be the basis for, you know, good research.

67
00:07:18,130 --> 00:07:22,720
And I think we get problems in this if if your code is messy,

68
00:07:22,720 --> 00:07:27,400
if you have things in different orders or if you're if you don't actually have the full code.

69
00:07:28,300 --> 00:07:31,480
So repeatability is, you know, like a baseline for what we want.

70
00:07:32,200 --> 00:07:39,950
Replicability means that. You take your data set and you basically give people your methods,

71
00:07:39,960 --> 00:07:45,770
you give another researcher your methods, and then you see if they can replicate what you did.

72
00:07:46,250 --> 00:07:51,320
So could somebody else in your lab, like a team mate or a group mate, if they knew,

73
00:07:52,430 --> 00:07:56,870
if they had that same dataset that you had and if they knew what you want, would they end up in the same place?

74
00:07:57,170 --> 00:08:02,780
And you should write your method section of your elite so that somebody else would be able to do that.

75
00:08:04,870 --> 00:08:08,590
Reproducibility means that there's a different analyst and a different data.

76
00:08:09,760 --> 00:08:18,460
So maybe we have a dataset here from the University of Michigan looking at how does coffee drinking relate to?

77
00:08:20,140 --> 00:08:22,180
How does that relate to diabetes?

78
00:08:24,210 --> 00:08:33,360
Would a different analyst working on a data set from the University of Illinois, from the University of Illinois hospital system.

79
00:08:33,360 --> 00:08:37,410
Would they be able to find the same results?

80
00:08:38,070 --> 00:08:46,320
So I definitely think we're kind of at a point in our in science where we are worried about this reproducibility crisis.

81
00:08:46,320 --> 00:08:52,080
And I think this has become especially apparent in psychology, and I think it will be an mailbag soon, if it not already.

82
00:08:52,530 --> 00:08:53,309
But, you know,

83
00:08:53,310 --> 00:09:00,390
there's there's many studies out there which seem to be conflicting where people are not being able to confirm findings from past research.

84
00:09:00,410 --> 00:09:04,440
And, you know, this kind of goes back to one of my annoyances with this.

85
00:09:04,470 --> 00:09:11,500
So this is like. Lance. It is saying we only want to publish studies which show something different.

86
00:09:11,770 --> 00:09:14,140
But given that we have a reproducibility crisis,

87
00:09:14,410 --> 00:09:26,450
I think there is a huge impetus and there should be a huge drive to wanting to have studies confirm what's been already done before.

88
00:09:26,470 --> 00:09:31,840
So like maybe we ran a small sample at the University of Michigan and found like a

89
00:09:31,840 --> 00:09:37,900
novel way that coffee consumption could could relate to increased diabetes incidence.

90
00:09:39,250 --> 00:09:46,240
What I think would be great then is maybe to have a national study or have like a much more comprehensive database from somewhere else or whatever.

91
00:09:46,540 --> 00:09:52,480
Try to do the same thing. And, you know, from the Lancet perspective, that's not novel because already the small study group something.

92
00:09:52,750 --> 00:09:58,000
But I think that would be in line with this ethos of trying to develop a scientific system which is

93
00:09:58,000 --> 00:10:04,090
reproducible and where our results have meaning because there is some degree of consistency over time.

94
00:10:05,950 --> 00:10:11,270
I'm not going to go into this just for the sake of time, but this absolutely fascinating story.

95
00:10:11,290 --> 00:10:19,839
Brian Rand Sink, you know, I think a horrible researcher, but very charismatic and excellent at hacking.

96
00:10:19,840 --> 00:10:24,370
So if, you know, that's something that you're interested in, you can you can follow his methods.

97
00:10:26,440 --> 00:10:28,329
So going back to what we started in this class,

98
00:10:28,330 --> 00:10:33,550
I think meta analysis are really important because certainly there can be a lot of people that work in tandem.

99
00:10:33,910 --> 00:10:39,900
But if we have results from a large study which show consistency, then, you know,

100
00:10:39,910 --> 00:10:45,910
we might be less concerned about the effect of some key hacking or publication by Israelis who'd be able to quantify that.

101
00:10:48,600 --> 00:10:54,230
All right. Thanks for a wonderful half term and good luck with last assignment and with the final.

