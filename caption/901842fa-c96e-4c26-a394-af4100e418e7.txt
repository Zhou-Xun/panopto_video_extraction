1
00:00:03,260 --> 00:00:09,650
Okay. Okay.

2
00:00:09,770 --> 00:00:17,240
Let's get started. So we haven't finished.

3
00:00:18,150 --> 00:00:22,110
This Monte Carlo the integration part yet. So I just wanted to.

4
00:00:23,400 --> 00:00:31,740
Uh, follow this up. So, except to pretend political integration is slightly different.

5
00:00:33,710 --> 00:00:37,700
But nine Monte Carlo integration and.

6
00:00:38,940 --> 00:00:44,240
Multicultural integration. The sampling, distribution and important sampling is quite a lady.

7
00:00:44,250 --> 00:00:47,860
So I just wanted to. Explain what the differences are here.

8
00:00:49,990 --> 00:00:52,660
So hopefully this time it's a little bit clearer.

9
00:00:53,320 --> 00:01:02,860
So nine what they call integration is basically if your target function is if you just randomly sample from uniform distribution.

10
00:01:03,750 --> 00:01:12,090
And calculate all this f of x of I and take the average weighted by the width of this window.

11
00:01:12,090 --> 00:01:24,000
Because you are conflating the area, you need to you need to multiply multiply the widths, write this F of x, take care of height.

12
00:01:24,390 --> 00:01:34,680
Right? So this is if you add them and divide by B, this is equal to average height of integration.

13
00:01:35,400 --> 00:01:40,110
So you just need to multiply the width to get that, get the area.

14
00:01:40,110 --> 00:01:41,519
That's the basic idea.

15
00:01:41,520 --> 00:01:53,820
You know, if you have enough number of random sample, they should converge to the area under the curve so that there is a bay that is the core part.

16
00:01:54,840 --> 00:01:58,320
And when we do the sampling distribution.

17
00:02:01,230 --> 00:02:11,400
Instead of using instead of using this uniform sampling, what we are doing is we're sampling from.

18
00:02:13,130 --> 00:02:22,400
Sampling from a distribution. In this case, we know that this random variable is coming from the normal distribution.

19
00:02:22,400 --> 00:02:28,760
So if we. So that's that's the only expectation we wanted to calculate from.

20
00:02:29,480 --> 00:02:35,450
So we have an actual target function to to calculate the expectation was this.

21
00:02:36,110 --> 00:02:40,600
But we are multiplying this because it's a calculated expectation.

22
00:02:41,270 --> 00:02:46,100
So you have calculated multiplying the density and we wanted to calculate the integral.

23
00:02:46,730 --> 00:02:55,860
But if your random variable comes from this distribution P of x, then you don't need to multiply this.

24
00:02:55,880 --> 00:03:04,160
You just need to randomly sample from the sampling distribution and just calculate average.

25
00:03:04,160 --> 00:03:08,220
That's I mean, calculate the average of these whatever function you want.

26
00:03:08,340 --> 00:03:11,840
So that's, that's how you actually calculate the expectation.

27
00:03:12,030 --> 00:03:16,520
So you just use a random, random sample.

28
00:03:17,540 --> 00:03:25,070
So this should work. So this is exactly the way how you calculate the calculate the expectation.

29
00:03:25,610 --> 00:03:34,700
So in this case, instead of using. So the key part is that instead of using F of X, we are using kilo of x and what is g of x?

30
00:03:35,670 --> 00:03:44,340
So Job X is that you are actually dividing this target function into the density of your sampling distribution.

31
00:03:44,820 --> 00:03:51,690
Okay. So this G of x is an F of x divided by F of x divided by of x.

32
00:03:52,230 --> 00:03:55,350
Okay, then that's what it is. Okay.

33
00:03:57,020 --> 00:04:04,190
So the nice thing is that then you don't need to worry about setting some arbitrary boundary or something like that.

34
00:04:06,220 --> 00:04:10,120
And so the input. So then.

35
00:04:12,560 --> 00:04:21,500
This important sampling. The idea is that, well, do you read so because I actually wanted to calculate this integral in the mathematical

36
00:04:22,190 --> 00:04:26,990
formulation is that I'm I don't need to know whether this is sampled from.

37
00:04:28,450 --> 00:04:36,020
Normal distribution or whatever because of one. Once you multiply some g of x, right, this is a dysfunction you need to integrate.

38
00:04:36,040 --> 00:04:44,110
I don't actually there is no right distribution for saying, okay, so why do I have to divide by P of x?

39
00:04:45,010 --> 00:04:53,050
[INAUDIBLE] be x is a normal distribution. So that was intended sampling distribution when I the way I try to calculate this expectation.

40
00:04:53,470 --> 00:05:02,980
But you don't have to. That's the point. So one audit my original intended sampling distribution of X.

41
00:05:02,980 --> 00:05:08,420
But let's totally forget about pure x because what I care about is F of x.

42
00:05:08,440 --> 00:05:13,140
I don't care about the power of x. Okay then original.

43
00:05:13,690 --> 00:05:18,000
So if P.O. Box and Cube X is the same, this is sampling distribution.

44
00:05:18,010 --> 00:05:22,870
So you basically this cancels out and your integrating g g of X eventually.

45
00:05:22,870 --> 00:05:33,180
So you calculate your bags. And you, you take the average of this job X that's what the previous example did.

46
00:05:33,840 --> 00:05:39,600
But if you shoot, you can't choose something other than P of x, there's no problem.

47
00:05:40,290 --> 00:05:50,940
Then what? And what you end up being is that you basically calculate the average of F of x divided by Q of x rather than killed x.

48
00:05:51,860 --> 00:05:56,810
Again. What it means is that you can choose which which distribution you want to sample from.

49
00:05:58,120 --> 00:06:04,169
Okay. And if cure X becomes part of X,

50
00:06:04,170 --> 00:06:11,040
that's a special case or just that happens to be the same to the job x But because we don't even care about your of X,

51
00:06:11,040 --> 00:06:20,400
we just as long as we know what the F of X is, and if we know the what the sampling distribution is, then that's all we care about.

52
00:06:20,410 --> 00:06:26,930
So F of X divide by cure of X. That's, that's important simply in.

53
00:06:28,220 --> 00:06:32,910
So. Why? So I wanted to.

54
00:06:33,450 --> 00:06:38,400
I want you to really understand why this works better than.

55
00:06:40,440 --> 00:06:45,810
So this is a core part of why important sampling works better than.

56
00:06:49,700 --> 00:06:55,910
Then just using the sampling distribution, then in which case input to sampling would be very useful.

57
00:06:56,180 --> 00:07:01,380
Okay, so that's what I wanted to. I'm talking to you.

58
00:07:01,810 --> 00:07:26,320
Okay. So. So that this we shouldn't be worried.

59
00:07:26,530 --> 00:07:32,210
So what do you know? Distribution is you know, is this is a your banks, right?

60
00:07:33,770 --> 00:07:37,170
So this is a province. Pubic services.

61
00:07:37,510 --> 00:07:50,010
So normal sampling distribution. Right. But the quantity we wanted to integrate was exponential distribution, truncating exponential distribution.

62
00:07:50,990 --> 00:07:59,219
The quantity we wanted to integrate looks really strange, like it looks good in intake.

63
00:07:59,220 --> 00:08:05,520
It increases somewhat pretty rapidly, but this decreased it pretty rapidly too.

64
00:08:06,270 --> 00:08:17,070
So once you multiply them together, you have what we are integrating over is basically following here.

65
00:08:17,640 --> 00:08:22,800
And so well I'm going to I'm going to just write in different place.

66
00:08:23,550 --> 00:08:26,970
So this this is the quantity we want to integrate eventually.

67
00:08:26,970 --> 00:08:33,750
This is F of X. Okay. So when we when you usually get the expectation.

68
00:08:35,100 --> 00:08:44,069
Those, you know, the quantity you want it to integrate over should be very similar to this that the usual like if if I do like oh I

69
00:08:44,070 --> 00:08:50,129
wanted to cut I wanted to come the expectation of x squared that well x looks like this x still looks like this.

70
00:08:50,130 --> 00:08:55,470
So it's slightly different, but it's still all of these overall shape.

71
00:08:56,040 --> 00:09:01,990
But sometimes it doesn't, right? Then when you calculate the.

72
00:09:03,970 --> 00:09:15,330
Calculate the expectation based on the random sample of the random sample from your sampling from here just doesn't contribute to your quantity.

73
00:09:15,340 --> 00:09:24,220
So sampling from ordinal sampling distribution in this case doesn't work very well because you need to over sample here.

74
00:09:25,090 --> 00:09:31,959
The sample here doesn't contribute at all. This is an extreme example, but there are many cases like that.

75
00:09:31,960 --> 00:09:36,820
Let's say you're conflating expectation of E of x, okay?

76
00:09:37,610 --> 00:09:40,749
If X is a normal distribution, there's the same thing.

77
00:09:40,750 --> 00:09:48,760
Like if X is very, very large that you want to sample more frequently, then if that in the case X is small.

78
00:09:49,540 --> 00:09:52,810
If this wasn't like x squared is even worse, right?

79
00:09:52,960 --> 00:09:57,430
So then that actually in this case it's almost like VLANs.

80
00:09:57,430 --> 00:10:01,450
So you can accumulate that expectation properly.

81
00:10:02,050 --> 00:10:13,150
But in this case, you need to sample from extreme more and more so important sampling allows you to be weight to your observation differently.

82
00:10:13,360 --> 00:10:20,290
So that's what this is what it does so by so well because this is the function.

83
00:10:20,290 --> 00:10:24,160
I don't care about the distribution. This is the function I wanted to integrate.

84
00:10:24,670 --> 00:10:27,760
I just wanted to find a new distribution and.

85
00:10:30,250 --> 00:10:34,660
New distribution and just focus on dyscalculia the integral.

86
00:10:34,750 --> 00:10:39,130
I'll totally forget about the original this version.

87
00:10:39,970 --> 00:10:42,910
And you know, I just want to focus on something.

88
00:10:42,910 --> 00:10:48,910
So whatever whatever basic function that looks like this, this could be normal shift in normal distribution.

89
00:10:48,910 --> 00:10:52,569
You can use it or you can use a truncated normal distribution.

90
00:10:52,570 --> 00:11:04,299
You can use it whatever you use, whatever distribution you use, that you should be waiting this part more than than other parts.

91
00:11:04,300 --> 00:11:08,900
And that should give a more proper average, basically.

92
00:11:08,920 --> 00:11:12,670
So that's that's the key part.

93
00:11:13,850 --> 00:11:20,419
So with that in mind, I'm just going to go through this example.

94
00:11:20,420 --> 00:11:23,870
So. So here.

95
00:11:26,830 --> 00:11:32,470
So there was effort. So. So. And worse than 40.

96
00:11:32,580 --> 00:11:37,190
Okay, so this was already 940, right? So, uh.

97
00:11:42,260 --> 00:11:47,569
I guess it doesn't work because I didn't. Define this function.

98
00:11:47,570 --> 00:11:56,469
But this f of t will be will be defined. So this this f of t was defined.

99
00:11:56,470 --> 00:12:06,330
So I'm going to just move on. Okay. So this is so I added I added some more information to so that you can understand better.

100
00:12:06,340 --> 00:12:11,440
So here we still want to calculate this, right?

101
00:12:12,010 --> 00:12:16,510
So we're using uniformly distributed sample and try try to calculate the average here.

102
00:12:16,520 --> 00:12:26,829
So that's what this code is intended to calculate the sample you from the uniform distribution and calculate this average.

103
00:12:26,830 --> 00:12:31,600
Exactly. Because this is F of T, we could actually calculate F of T here.

104
00:12:31,810 --> 00:12:35,500
Okay. So this was the knife.

105
00:12:36,070 --> 00:12:40,020
Now you've to cut it off, right? Uh, I'm sorry.

106
00:12:40,270 --> 00:12:43,720
This is a still using this air, so it doesn't work.

107
00:12:44,140 --> 00:12:50,090
I can load this function. Uh, so that it doesn't break.

108
00:12:51,600 --> 00:12:54,780
Uh. Both of them.

109
00:12:56,320 --> 00:13:00,860
And I can. Run this again and they should work.

110
00:13:01,100 --> 00:13:12,210
Okay. So the doing the integrating of a sampling distribution is basically because a pie of polio vaccine that this is a this is normal density,

111
00:13:12,810 --> 00:13:21,760
the field is a normal density. So we're just integrating inside because we are basically dividing by the density.

112
00:13:22,060 --> 00:13:30,760
That's actually same as important sampling. But, you know, this is not obvious that you are dividing by by this distribution.

113
00:13:30,760 --> 00:13:34,110
But that that's what that's what it does. So integrate.

114
00:13:34,120 --> 00:13:41,680
So this one basically integrate instead of integrating F of T is trying to integrate g of T, right?

115
00:13:41,890 --> 00:13:46,209
Which which you actually already divide density by by the density.

116
00:13:46,210 --> 00:13:49,660
Right. So and and that's the difference.

117
00:13:50,230 --> 00:14:00,760
Okay. So. So then we already saw that this doesn't work and important sampler is basically you you divide

118
00:14:00,760 --> 00:14:08,240
by some other distribution Q objects okay so doesn't kill it doesn't matter what this cube is.

119
00:14:08,290 --> 00:14:11,860
So this is an effort F of X and this is cube x.

120
00:14:11,890 --> 00:14:17,830
Basically you are calculate calculating the average of this and you can choose any.

121
00:14:17,830 --> 00:14:23,770
Q Is it wrong as the simpler represent the density, the P sampler?

122
00:14:24,130 --> 00:14:33,010
Okay. Well, I mean, simpler is a random so random sampling function and the p simply the density function, then this should work.

123
00:14:33,020 --> 00:14:39,370
So you're just calculating basically target function divide by the density of the sample sampling function.

124
00:14:39,700 --> 00:14:47,880
Then then you are basically weighting each of the observation of this proportional to the density of the,

125
00:14:47,890 --> 00:14:54,820
of, of those the sample the value and calculating the weighted average.

126
00:14:55,950 --> 00:15:07,270
Okay. So so in this case we are we're giving our actual target function, not job t so be be mindful about that.

127
00:15:07,950 --> 00:15:15,900
So if the one one fun fact here is that if you use I you live here in the important center if you do,

128
00:15:15,910 --> 00:15:21,140
I mean, what does it mean you are using uniform distribution or.

129
00:15:21,210 --> 00:15:25,260
So basically what are dividing piece that is being constant, right?

130
00:15:26,160 --> 00:15:30,920
So that means that you are actually just using value for the convention.

131
00:15:31,260 --> 00:15:34,400
Okay. So this one is a nice multi-color method.

132
00:15:34,840 --> 00:15:39,890
Okay. So this one is nice that that multi-color method, but with a different max.

133
00:15:40,050 --> 00:15:45,270
Meaning Max. Okay. You just need to provide a matching sampler and density function.

134
00:15:46,260 --> 00:15:49,499
And this one I got a question is why?

135
00:15:49,500 --> 00:15:57,450
Why does it look like this? So it is basically what it does is that ah on its p function.

136
00:15:59,750 --> 00:16:02,880
If so, if. Punish a discipline for this. Right.

137
00:16:03,230 --> 00:16:06,710
So you want to you want to shift by t right.

138
00:16:07,100 --> 00:16:11,839
So in the random random sample of what you can do is that I'm sampling from

139
00:16:11,840 --> 00:16:17,000
the random exponential distribution and the T that's that's what then your,

140
00:16:17,150 --> 00:16:28,010
your start, your book value start from T when you calculate the density function, you need to shift to this basic function by, by t to the right.

141
00:16:28,460 --> 00:16:37,910
How do you do it. Any function tell you have ample width. If you want to shift the shift by to the x axis by t, you can you can do this.

142
00:16:37,910 --> 00:16:41,930
Then it is shift, right? So this is a translation of function.

143
00:16:42,440 --> 00:16:49,490
So that's why I did this. Okay. Also, this example is the same thing.

144
00:16:50,180 --> 00:16:55,220
I'm making random sample from just a uniform distribution.

145
00:16:55,500 --> 00:17:05,210
But. But at t so this is exactly the same as r ornament x and t, t and one.

146
00:17:05,270 --> 00:17:08,840
So which minty and minty and various one.

147
00:17:09,170 --> 00:17:14,270
These two should be the same. And this is also the same.

148
00:17:14,270 --> 00:17:27,590
So you can instead of doing this, you can do t one. If you if this is a more more intuitive to you, this should exactly the same answer, I think.

149
00:17:28,550 --> 00:17:33,290
Okay. So. What did I do wrong here?

150
00:17:35,210 --> 00:17:38,450
Do you need clean up? Oh, sorry.

151
00:17:38,560 --> 00:17:42,090
He put. I didn't. I didn't know this function. Okay.

152
00:17:43,180 --> 00:17:46,440
Right again. Good. Then you have this. Okay.

153
00:17:47,490 --> 00:17:57,750
So basically in this case, uniform distribution actually does pretty well because, you know, you're simply uniformity here.

154
00:17:58,080 --> 00:18:05,549
So it's reasonable to to integrate them actually, as long as you don't waste too much of space.

155
00:18:05,550 --> 00:18:11,010
So first one, use it the the waste of quite a bit space, right?

156
00:18:11,010 --> 00:18:18,240
Because it's going from minus w t w okay. But here it is.

157
00:18:18,690 --> 00:18:22,980
Uh. It is wasting less space, so it's more accurate.

158
00:18:23,880 --> 00:18:27,720
If we use the exponential distribution, it does work pretty well actually.

159
00:18:27,720 --> 00:18:33,210
Right, because it's a good approximation in a normal distribution.

160
00:18:34,030 --> 00:18:39,329
You're wasting half of them, right? But the rest of the half is pretty useful.

161
00:18:39,330 --> 00:18:45,530
So it does work pretty well. So if you run it again, sometimes it has variation, right?

162
00:18:46,230 --> 00:18:50,640
So sometimes it does. But so one works better or not. This is a little bit random.

163
00:18:51,180 --> 00:18:54,320
So. How do we evaluate this?

164
00:18:54,560 --> 00:19:00,110
So now we you know, you can run run multiple times and see how you behave,

165
00:19:00,110 --> 00:19:05,900
that you have some subjective feeling of how one works better than the other.

166
00:19:06,920 --> 00:19:10,280
But how do you quantify it objectively?

167
00:19:10,520 --> 00:19:15,530
Okay. So what you what you care about is that is it consistent?

168
00:19:15,860 --> 00:19:18,979
Right. So does it does it compare it to the resolution?

169
00:19:18,980 --> 00:19:22,520
And we already know that this is most of them are consistent.

170
00:19:23,150 --> 00:19:30,799
What you more what you probably want to care more about is is this how how efficient is it?

171
00:19:30,800 --> 00:19:34,760
How what is that? What is a variance, right? How precise it is.

172
00:19:34,970 --> 00:19:40,610
So so you can you can evaluate the variance to do evaluate the variance.

173
00:19:40,610 --> 00:19:43,700
You need to run this over your multiple times.

174
00:19:44,420 --> 00:19:52,100
So this is calculate the means. Kodaira. So, I mean, it's clear that it's basically bias plus variance squared.

175
00:19:52,250 --> 00:20:03,140
So you, you, you just evaluate the performance we the MSI to to collectively assess the bias and variance usually so you can do this.

176
00:20:03,710 --> 00:20:08,900
So how do you do it? I. So there is a importance set.

177
00:20:08,900 --> 00:20:11,479
So function simpler. P Simpler.

178
00:20:11,480 --> 00:20:22,190
And I'm, I'm running this a multiple times and each time I'm sampling and different samples, but I'm repeating this procedure and ramp time.

179
00:20:22,430 --> 00:20:27,950
Okay, then, then I in this case, I should know what the answer should be.

180
00:20:27,950 --> 00:20:36,470
So in this case, I'm cheating. I'm giving a transfer here and then you can provide a bias and variance and you can evaluate this.

181
00:20:36,590 --> 00:20:40,370
Okay. So that's a small function to do that. Okay.

182
00:20:40,370 --> 00:20:48,890
So if theta theta is a true answer here. So in this case, I, I pick is imply know the true answer so you can evaluate MSE.

183
00:20:50,150 --> 00:20:56,600
If you don't know the transfer, you can now evaluate the vias so you can only evaluate the variants, I think.

184
00:20:57,530 --> 00:21:00,920
Okay, so let's see how this works.

185
00:21:00,970 --> 00:21:07,740
So first one, what is this first one? First one is a nine Monte Carlo algorithm.

186
00:21:08,580 --> 00:21:15,690
That is because they using the all units and we're giving the type bound T to W.

187
00:21:15,810 --> 00:21:23,959
Okay, let's let's let's see how this works. So then bias, pretty small variance is pretty small.

188
00:21:23,960 --> 00:21:31,280
Most is pretty small. Okay, so this is good. We are only using thousand random samples, by the way.

189
00:21:32,800 --> 00:21:35,890
So this is pretty robust and.

190
00:21:37,090 --> 00:21:42,560
How about this? Okay, so this one is that I'm using exponential distribution, okay?

191
00:21:43,180 --> 00:21:48,940
And repeating thousand times, but each of them has a thousand thousand as a random sample.

192
00:21:49,570 --> 00:21:55,000
So let's use the exponential distribution encrypted by t that how this works.

193
00:21:55,180 --> 00:22:01,760
Oh. So this does have a smaller bias and smaller variance and smaller masses.

194
00:22:01,780 --> 00:22:08,349
So I would guess this wins. Why? Why does this better than the name of the conglomerate?

195
00:22:08,350 --> 00:22:09,570
And why do you think so?

196
00:22:12,470 --> 00:22:20,180
So now with the kind of method is basically uniform, you sample from here to a very, very large value and try to take the average.

197
00:22:21,770 --> 00:22:23,450
If you use exponential distribution,

198
00:22:23,810 --> 00:22:31,700
it tries to generate the most more end of samples here and the less random samples here and take the average and properly weight them.

199
00:22:32,420 --> 00:22:39,560
Okay. But if you take the more random samples here, then contribute to more of these integrated parts.

200
00:22:39,570 --> 00:22:45,260
So it's better to to randomly sample more here.

201
00:22:45,680 --> 00:22:50,120
Right. So let's say you're trying to do the integral.

202
00:22:50,150 --> 00:22:53,780
I say it by by pipeline, the rectangle.

203
00:22:54,200 --> 00:23:03,220
So the then you can you can argue that, oh, if I make a rectangle all the same width, I'll do that.

204
00:23:03,230 --> 00:23:06,920
I'll do it'll do the best you could.

205
00:23:06,920 --> 00:23:13,190
You could you could argue that. But if you have a limited number of rectangle, actually the,

206
00:23:13,610 --> 00:23:19,969
the way to calculate the more accurately is that use a smaller width for the for the

207
00:23:19,970 --> 00:23:25,130
beginning part and use the larger weights and the later part because the beginning part,

208
00:23:25,760 --> 00:23:28,770
you know, contribute the total integral more.

209
00:23:28,790 --> 00:23:33,500
So it's a, it's better to sample from here more. So that's a same kind of idea.

210
00:23:35,180 --> 00:23:45,629
So that's then. And. If you use a normal distribution, normal distribution, basically it should do pretty well.

211
00:23:45,630 --> 00:23:52,630
But the distribution probably decayed to two more rapidly here, potentially right there.

212
00:23:52,650 --> 00:23:59,309
And uh, and no, actually it's not because of the target function is still moderately decreasing.

213
00:23:59,310 --> 00:24:03,600
But yeah, so, but the bigger problem is that you're wasting half of the space.

214
00:24:04,290 --> 00:24:12,050
So you do have a larger bias and a larger variance because you're, you're simply you're wasting half of the sample.

215
00:24:12,480 --> 00:24:24,690
So the the way you can, you can prove that this would work better, is that if you just actually you're just to increase the number of random sample,

216
00:24:24,690 --> 00:24:29,550
I'm pretty sure that this will pretty much be similar to actually now.

217
00:24:30,150 --> 00:24:38,129
So let's look at this. But bias and variance is pretty, pretty good though by I mean various and MSI is a pretty good.

218
00:24:38,130 --> 00:24:43,470
Now write bias is slightly worse, but overall messages are comparable now.

219
00:24:43,890 --> 00:24:47,090
Okay. Also art.

220
00:24:47,330 --> 00:24:50,720
Sometimes when you use art, you always find some packages.

221
00:24:51,020 --> 00:24:56,570
So, oh, you know what I, what I'm doing must be worse than what is out there.

222
00:24:56,960 --> 00:25:04,220
So, like, you know, that's not always true. So you can, you can find the packaging or truncate the normal distribution, and then you can use this.

223
00:25:04,810 --> 00:25:13,520
And because they, they have a truncated normal distribution and they, they have this, uh, dysfunction.

224
00:25:13,940 --> 00:25:18,829
Well, then, if you use this, the good thing is that because this is a truncated,

225
00:25:18,830 --> 00:25:24,290
normal distribution, meaning that you're using exactly the same sampling distribution, right?

226
00:25:24,800 --> 00:25:31,270
So in this case, bias and variance, these are all better than anything else.

227
00:25:31,700 --> 00:25:34,970
And this is not surprising because you are actually doing it.

228
00:25:34,990 --> 00:25:38,299
You're doing the ideal random sampling here.

229
00:25:38,300 --> 00:25:42,950
So in this case, every observation is ideally weighted.

230
00:25:43,370 --> 00:25:50,990
Okay, because A, P and Q your sampling distribution and the importance distribution is exactly the same.

231
00:25:51,260 --> 00:25:55,610
Okay, so those cancels out so that minimize your variance.

232
00:25:55,610 --> 00:26:01,120
So I'll explain that later. But also.

233
00:26:02,430 --> 00:26:10,320
Also, you can you can do even better because you're actually you're using the appropriate normal distribution here.

234
00:26:11,820 --> 00:26:15,890
Okay. So one thing is that. So.

235
00:26:16,640 --> 00:26:20,660
What we are doing is that we are using that you can use a truncated normal distribution.

236
00:26:20,930 --> 00:26:25,580
Okay. So the continuum of this looks like this a of next.

237
00:26:26,820 --> 00:26:32,280
And so basically you're using this information as a sampling distribution.

238
00:26:32,910 --> 00:26:37,530
But what you want to continue to. GOLBERG Multiply this to the X, right?

239
00:26:38,130 --> 00:26:45,920
So then this is still not exactly proportional to the to the distribution.

240
00:26:45,930 --> 00:26:51,240
So if you make a shift of this truncated normal distribution by one,

241
00:26:51,780 --> 00:26:57,389
which is a you can rewrite this equation, you will know that this one is the ideal.

242
00:26:57,390 --> 00:27:02,220
I'm on the shift. Then I think it'll do even better, as you see.

243
00:27:02,700 --> 00:27:07,370
So much, much better. Does it make sense to?

244
00:27:10,090 --> 00:27:16,400
Let me. Let me go through the slide and I'll come back. So.

245
00:27:18,400 --> 00:27:20,590
In the in the variance of so.

246
00:27:23,640 --> 00:27:37,800
Variance in the important sampling can be minimized and can be minimized if you choose a Q of which is exactly proportional to your target function.

247
00:27:39,030 --> 00:27:43,940
Okay. What does this mean? So.

248
00:27:47,760 --> 00:27:55,200
You have. So in this example we have you have that your prior function is E to the X.

249
00:27:56,450 --> 00:28:03,580
And the eye of X is greater than T, and you have a normal density greater.

250
00:28:05,360 --> 00:28:11,000
You have some constant number of just a constant because those are those are just normalizing factor.

251
00:28:11,390 --> 00:28:17,879
And you have this part. Right. Okay. So this is a normal density, right?

252
00:28:17,880 --> 00:28:23,160
So, you know, you have it if you don't want to lose the constant you have, you have this.

253
00:28:24,210 --> 00:28:30,720
So then let's forget forget about this part, because this is this is just constant almost.

254
00:28:31,290 --> 00:28:36,230
And this one is basically minus two X squared plus x, right?

255
00:28:37,140 --> 00:28:48,120
This part can be read, rewritten, adds it to the minus 2xx minus one square.

256
00:28:48,600 --> 00:29:02,190
Okay. And you basically put the multiplied by spirit of E, then this is the this is the actual density, actual function you want to have, right?

257
00:29:03,180 --> 00:29:06,540
Does it make sense again? And these are all constant.

258
00:29:06,540 --> 00:29:12,420
I don't care about this constant because I want to find a density that is exactly proportional to this.

259
00:29:13,230 --> 00:29:23,220
What is that? This is a truncated normal because you still have this truncated effect, truncated normal, but shifted by shifted shift.

260
00:29:23,430 --> 00:29:27,990
So truncate normal. But I want to start from x equals one.

261
00:29:28,440 --> 00:29:32,700
Okay, so shift the mean. Okay. That's exactly what this has done.

262
00:29:33,360 --> 00:29:35,489
I did that. I did have truncate normal.

263
00:29:35,490 --> 00:29:46,320
So truncate normal us you know, truncated from t minus one but added one the basically mean I'm I'm making a truncated mode normal from the.

264
00:29:47,450 --> 00:29:51,769
From mean one. But the truncate truncate from key.

265
00:29:51,770 --> 00:29:59,000
So this is what what it does. So I know it's a little bit different, but you are you're making a truncated normal,

266
00:29:59,550 --> 00:30:04,310
you know, truncated t minus one and add one the same as a truncating t,

267
00:30:04,310 --> 00:30:12,310
but the making mean as one before truncation I mean and another the density should we should be looking like this.

268
00:30:12,320 --> 00:30:18,110
So this is exactly the distribution you wanted to you wanted to.

269
00:30:18,230 --> 00:30:28,670
This is an ideal distribution. You can minimize the variance amongst all possible choices based on this equation.

270
00:30:30,120 --> 00:30:34,560
Okay. So, I mean, sometimes you can find this, but sometimes you may not.

271
00:30:35,400 --> 00:30:42,959
So that the rule of thumb is that pick your works that is as close as your target distribution.

272
00:30:42,960 --> 00:30:49,440
That's always good. And in this case, the reason why this is exactly not truncated or, you know,

273
00:30:49,770 --> 00:30:54,990
number is because you multiply something on the target or the origin of distribution.

274
00:30:55,680 --> 00:30:59,490
So the target distribution is shifted slightly from your sampling distribution.

275
00:31:01,470 --> 00:31:06,380
Okay. So that's pretty much it.

276
00:31:06,560 --> 00:31:11,810
And this is a very small note of a straight fine sampling.

277
00:31:11,810 --> 00:31:17,220
So let me go through this. Okay. So stratify sampling.

278
00:31:17,240 --> 00:31:30,490
Okay. So. Strip sampling is basically another strategy too, that you can always use with some other sampling procedure.

279
00:31:30,610 --> 00:31:37,460
So stratified sampling is that, oh, when I sample something, I want the same.

280
00:31:37,480 --> 00:31:43,630
I want to divide my area and sample each area separately.

281
00:31:44,100 --> 00:31:48,090
You can. So it would a different sampling procedure.

282
00:31:48,110 --> 00:31:51,590
So if you wanted to sample a.

283
00:31:53,390 --> 00:32:01,760
Uh, fixed a number of points. Uh. The I want to am according to certain density.

284
00:32:02,570 --> 00:32:07,270
Then you can. So.

285
00:32:08,450 --> 00:32:10,730
So basically you can calculate.

286
00:32:11,090 --> 00:32:21,860
So if you have a target integral like this, you make a you know, you divide a domain into this subsets and you sample each of the subsets separate.

287
00:32:21,950 --> 00:32:33,110
So then, then what happens here is that so you have, you have some distribution like this then basically is a simple,

288
00:32:33,320 --> 00:32:38,370
simple this part separate, separate, written for this purpose, separate, this part, separate and so on.

289
00:32:39,360 --> 00:32:43,100
Then you can achieve more efficiency.

290
00:32:43,950 --> 00:32:49,220
Then what? You do it by one, but by by one batch.

291
00:32:50,390 --> 00:32:54,770
Because. Uh, because you have this theorem.

292
00:32:54,970 --> 00:33:04,940
Okay, so. So basically. So even so, to hear what you want to see sees this.

293
00:33:05,260 --> 00:33:11,030
Okay. So I think you have learned this in six one already.

294
00:33:11,870 --> 00:33:17,229
But if you so. So conditional variance,

295
00:33:17,230 --> 00:33:24,760
somewhat expected expected variance of conditioning on certain certain certain control variable

296
00:33:24,760 --> 00:33:31,390
certain certain certain certain any confounder variable or some some other controlling variable.

297
00:33:32,140 --> 00:33:36,400
Usually you can reduce the variance. You actually don't increase the variance.

298
00:33:36,400 --> 00:33:41,920
You can reduce the variance. So that's that's the rule of thumb of conditioning.

299
00:33:42,820 --> 00:33:46,320
So a pretty much every Monte Carlo estimate.

300
00:33:46,320 --> 00:33:59,320
TER If you have the ability to divide your region or into specific strata and you have a chance to reduce your variance,

301
00:33:59,320 --> 00:34:08,350
and you can even try to find the strategy to minimize your variance by using this, try to find a sampling.

302
00:34:08,560 --> 00:34:16,090
Okay. So basically immobilize the how many sample do I want to sample from each of the regions?

303
00:34:16,090 --> 00:34:24,040
So you divide by dividing the divide the region and you can calculate how many sample I need to sample from here, here and here, so on.

304
00:34:24,820 --> 00:34:38,260
Then what? What do you gain by that? The one you are gaining is that you have a control of exactly how many sample you can sample from this region,

305
00:34:38,890 --> 00:34:42,250
which you don't get in full randomized setting.

306
00:34:42,250 --> 00:34:46,300
So. So you can think about this way. Oh.

307
00:34:47,510 --> 00:34:50,960
I want to calculate the integral of some function.

308
00:34:52,380 --> 00:35:00,090
Okay. And let's say you had only ten, ten, random ten pain points you can use.

309
00:35:00,810 --> 00:35:04,160
Okay. Then you have to trust reality.

310
00:35:04,670 --> 00:35:08,320
What is that? I'm going to use that.

311
00:35:08,330 --> 00:35:11,570
Exactly regular rectangle.

312
00:35:11,760 --> 00:35:22,550
So I want you to integrate this using evenly spaced the rectangle versus randomize ten points and ten points.

313
00:35:22,580 --> 00:35:32,790
And you can do this. Which one would you do? Do you want to do that regularly spaced or done, or do you want to choose a randomly selected word?

314
00:35:32,790 --> 00:35:40,170
And what did you do? You have only ten point. Raise your hand if you want to do the legendary Spaceballs.

315
00:35:41,290 --> 00:35:45,790
And raise your hand if you have if you want to, you just randomly sample one.

316
00:35:47,480 --> 00:35:54,530
Okay. You know, even if you don't want to do any of them. So Lebanon is a place one is more guaranteed to.

317
00:35:55,010 --> 00:35:59,030
You know, you actually, you know, at least the worst case performance.

318
00:35:59,390 --> 00:36:05,680
It'll be better if you do random point you might have like we we are in case

319
00:36:05,690 --> 00:36:09,330
all ten points are located here in the European your calculations out away.

320
00:36:09,350 --> 00:36:15,680
Right. So usually interesting randomness is always not a good thing.

321
00:36:16,430 --> 00:36:21,590
You're introducing random. These can avoid by bias.

322
00:36:23,430 --> 00:36:33,690
If you are you know, if you're deterministic, everything doesn't but doesn't carefully model those those noises and biased biases.

323
00:36:33,690 --> 00:36:38,610
But it does it does introduces some in it potential inefficiencies.

324
00:36:39,480 --> 00:36:42,840
And this training for sampling is like you're doing this.

325
00:36:42,990 --> 00:36:48,420
So I'm making 10 million I, I'm sampling ten points in each of them.

326
00:36:48,420 --> 00:36:52,319
Each of the region is simply hundred points in everything.

327
00:36:52,320 --> 00:37:00,270
So this strain by sampling, just by by that concept, it reduces the risk of randomness a little bit.

328
00:37:00,270 --> 00:37:05,850
So it does reduce the bias a little bit. So that's that's what the stratified sampling is.

329
00:37:06,360 --> 00:37:08,370
And there's a lot of theories about this.

330
00:37:09,060 --> 00:37:16,650
But in fact, when you use this slide, maybe you can reduce the bias slide so you can do this a variance slightly,

331
00:37:17,280 --> 00:37:21,990
but there's a lot of things you need to take care of to make this telestrator working.

332
00:37:22,620 --> 00:37:28,410
So this is a good theory. This is a good to know. I highly doubt that you're going to use in your real setting.

333
00:37:28,650 --> 00:37:32,639
So that's that's my take. Okay. But this is a theory.

334
00:37:32,640 --> 00:37:40,500
So you have you can add you can estimate the conditional expectation for each of these a random sample.

335
00:37:41,130 --> 00:37:47,040
So you can make a weighted average for each of the thread to calculate the integral properly.

336
00:37:47,040 --> 00:37:58,530
So that's all the theory about the strip stratified sampling and that that strategy can in principle reduce your various.

337
00:37:58,680 --> 00:38:07,800
But your code might be a lot more complicated than using the more simpler strategy of the multi-cloud integration.

338
00:38:08,860 --> 00:38:09,060
Okay.

339
00:38:09,430 --> 00:38:19,270
So we're not going to do the example because the coding is obviously a mess and I don't really recommend to do it unless you really have to do it.

340
00:38:19,780 --> 00:38:29,379
Usually if you design your important sampling very systematically, you don't need to increase the amount of your coding much.

341
00:38:29,380 --> 00:38:34,740
But the important sampling is a great, great features that you want to use especially.

342
00:38:35,540 --> 00:38:42,370
I do use important sampling a lot, especially when you try when you need to sample from tail, right.

343
00:38:42,790 --> 00:38:46,630
So sometimes you need to sample from extreme right.

344
00:38:47,080 --> 00:38:54,010
In that case, if you just use a random procedure, just a typical procedure of doing something,

345
00:38:54,280 --> 00:38:57,579
you know, you're never going to reach those random sampling.

346
00:38:57,580 --> 00:39:03,670
And you need like a billion random samples, but only you can use only very small amount.

347
00:39:04,240 --> 00:39:11,410
If you do the important sampling, that usually are a lot better in many settings.

348
00:39:12,310 --> 00:39:18,549
In my area, I do that a lot. I'm pretty sure you would encounter some problems that were important.

349
00:39:18,550 --> 00:39:21,670
Sampling becomes really useful for your actual research.

350
00:39:22,550 --> 00:39:26,820
So. Yeah. So that this is the last slide.

351
00:39:27,140 --> 00:39:32,520
Any question about the Monte Carlo integration? Okay.

352
00:39:35,170 --> 00:39:41,290
Okay. So let's move on to the next section.

353
00:39:41,770 --> 00:39:48,100
Okay, next lecture. So how how how is the semester going so far, by the way?

354
00:39:49,380 --> 00:39:55,770
Is. It looks like I see a face and I see more stress in your face.

355
00:39:55,770 --> 00:40:01,380
More, more so. I hope that this class didn't. This class doesn't add too much of those stresses.

356
00:40:02,490 --> 00:40:07,140
Uh, I mean, I. I think this class is very.

357
00:40:08,360 --> 00:40:11,810
Heavy loaded. I know that in a.

358
00:40:13,350 --> 00:40:22,990
And. But. But usually the workload after midterm, assuming your final project is not too overwhelming,

359
00:40:23,350 --> 00:40:27,190
it shouldn't it shouldn't increase too much compared to other classes.

360
00:40:27,190 --> 00:40:30,340
So I hope that, you know, that that is the case for you.

361
00:40:31,870 --> 00:40:44,050
I wanted to cover the material that is more relevant to the midterm and final project earlier than later so that you can have.

362
00:40:45,300 --> 00:40:49,860
You know, you can have a more lead time to think about the problem.

363
00:40:50,490 --> 00:40:53,820
So I decided to change the order of lecture a little bit.

364
00:40:54,150 --> 00:40:58,530
Okay. So lecture 12 and 13, both of them are Monte Carlo method.

365
00:40:58,710 --> 00:41:04,860
So bootstrapping and other Monte Carlo method. So that is better in terms of connection.

366
00:41:05,460 --> 00:41:15,520
But those are not included in those are not necessary for your rest of the hallmark or or the or midterm.

367
00:41:15,540 --> 00:41:23,640
So I'm going to cover that a little later. What I am going to cover today, the rest of time is a EMR algorithm.

368
00:41:24,780 --> 00:41:33,230
And the next lecture I'm going to cover how to make our packages and how to use ICP so that you can use them for your project.

369
00:41:33,240 --> 00:41:36,030
And I'm going to move on to the the lecture 12 again.

370
00:41:36,030 --> 00:41:46,650
So I, I know it's not ideal, but this seems like the right sequence of the lecture this year given that, given the, given the current progress.

371
00:41:46,890 --> 00:41:52,680
So I hope that everyone is okay. And if you have any questions, let me know regarding that.

372
00:41:54,340 --> 00:42:00,610
So that being said, let's skip, let it ferment for 13 and then let's move on to lecture 14.

373
00:42:00,610 --> 00:42:06,260
I think lecture 14 slides should be in campus as well. Any questions so far?

374
00:42:07,360 --> 00:42:11,050
Before moving on to Yemen. Okay.

375
00:42:11,070 --> 00:42:19,649
So now midterm is out and you if you finish it your homework for you can move on to the uh, midterm.

376
00:42:19,650 --> 00:42:23,580
I one of the problem is already can become on the way to what you have learned.

377
00:42:23,580 --> 00:42:27,180
But the other problem, you need to know what the Yemeni with me is.

378
00:42:27,960 --> 00:42:31,530
So I want to cover this with enough lead time.

379
00:42:31,530 --> 00:42:36,150
And if you have any questions regarding Yemen revision, please let me know.

380
00:42:38,070 --> 00:42:46,080
So this image is not going to cover exactly the same problem that that had that was in the midterm.

381
00:42:46,080 --> 00:42:51,410
But actually the. The principle is really, really similar.

382
00:42:51,420 --> 00:43:02,840
So you should be able to. You should be able to use all of these theories in your in your midterm project.

383
00:43:04,320 --> 00:43:11,170
Okay. So. Uh, love Paul.

384
00:43:11,950 --> 00:43:16,100
How many of you heard about Yemen, William? Okay.

385
00:43:16,190 --> 00:43:23,010
Well, so majority. That's good. How many of you have used the have been any sitting.

386
00:43:24,860 --> 00:43:28,580
So there are people who have used the algorithm. That's great.

387
00:43:28,610 --> 00:43:34,190
So, yeah, algorithm is a is a one of my favorite algorithm.

388
00:43:34,460 --> 00:43:39,560
I really think this is a beautiful. Okay. And if you.

389
00:43:42,220 --> 00:43:51,000
If you wanted to know about more. There is a other variation, like my algorithm and some related algorithm you can also learn about.

390
00:43:51,010 --> 00:43:57,400
But because this is 615, we're going to be short and get to the just the core part of em.

391
00:43:59,130 --> 00:44:04,920
So yeah, my religion is a quite general organism for missing data problem.

392
00:44:05,430 --> 00:44:10,320
So if you have some missing data, Emma, wisdom can help you.

393
00:44:10,440 --> 00:44:14,220
But not all of them. Not all of the missing data. Plus some of the missing data.

394
00:44:14,740 --> 00:44:19,799
Okay. So it is not a general method for optimization,

395
00:44:19,800 --> 00:44:29,490
meaning that now that we'd all the f g p so those are the ultimate algorithm that helps you try to try to optimize a parameter.

396
00:44:29,970 --> 00:44:35,100
And EMR wisdom can do that too. It can help you find the optimal parameters,

397
00:44:35,820 --> 00:44:42,180
but it doesn't it you can that you cannot implement every major general function you

398
00:44:42,180 --> 00:44:47,790
need to implement EMR vision case by case bit depending on what the decision is.

399
00:44:48,670 --> 00:44:56,440
So that's why you need to really learn about them very carefully, because you need to implement and you cannot just plug and play here.

400
00:44:56,680 --> 00:45:03,319
Okay. So now this year my wisdom is very frequent uses.

401
00:45:03,320 --> 00:45:11,870
If you have a military disposition, this is a very, very useful and you need to specialize your problem.

402
00:45:11,960 --> 00:45:17,070
In fact, as I said. So some citation record.

403
00:45:18,540 --> 00:45:26,730
I did not take these numbers I think. But EMR with if you look at Google Scholar, I'm pretty sure that these numbers are much, much larger now.

404
00:45:27,120 --> 00:45:34,800
It has been 50 years and less than 45 years, but it's been cited a lot.

405
00:45:35,250 --> 00:45:38,730
Then that means simplex method is cited a lot, but much less.

406
00:45:39,300 --> 00:45:47,640
I think these days the citation numbers are not very impressive because like all this machine learning method, I know that the transformer,

407
00:45:47,670 --> 00:45:54,989
which was a this is a natural language processing algorithm that is really revolutionary that they're publishing for Google,

408
00:45:54,990 --> 00:46:00,270
that that is already cited over 50 to 50000 times in five years is amazing.

409
00:46:01,010 --> 00:46:06,450
Okay. So. Oh, okay.

410
00:46:06,510 --> 00:46:09,560
So a brief history. Okay.

411
00:46:09,570 --> 00:46:13,320
So I think many of you know what the Bayes theorem.

412
00:46:13,320 --> 00:46:21,479
So we there's a, you know, Bayes theorem has been out there for for the century, many centuries.

413
00:46:21,480 --> 00:46:28,200
And there is a maximum like estimate that there was that now it's a hundred years old, I think.

414
00:46:29,400 --> 00:46:37,560
So there is there is a there there was a high level at the time and the maximum like estimation could be done.

415
00:46:38,070 --> 00:46:44,219
So it was actually it was actually developed in the setting of the genetics at the time,

416
00:46:44,220 --> 00:46:50,400
although Fisher was using maximum likely estimate, ah, to solve some genetics problem.

417
00:46:51,210 --> 00:46:55,790
And in that genetic setting, someone came up with this,

418
00:46:56,100 --> 00:47:01,379
this thing called M are going to solve a specific problem and later decided wisdom

419
00:47:01,380 --> 00:47:06,870
was adapted to you sold more general problems so that that is a history of the

420
00:47:06,930 --> 00:47:13,860
argument that the first occurrences to have the 1955 that people haven't realized

421
00:47:13,860 --> 00:47:18,870
this is much more general but there is a much more general application until 1970s.

422
00:47:21,100 --> 00:47:24,580
So. So this is the formulation of BMI algorithm.

423
00:47:25,150 --> 00:47:29,750
Okay. So you have. Two sets of variables.

424
00:47:30,110 --> 00:47:36,850
Two sets of data. One is an the idea of what is an unobserved thing.

425
00:47:37,240 --> 00:47:42,430
So we say W as a combination of two stores, two complete data.

426
00:47:43,600 --> 00:47:48,730
And X is observed data, which is the notation X is already observed.

427
00:47:48,970 --> 00:47:54,720
Z is always unobserved. Okay. So this is a very generous setting.

428
00:47:54,730 --> 00:47:57,639
Doesn't have to be a distribution in the region.

429
00:47:57,640 --> 00:48:05,740
Why this works well with the mixture distribution is just that because you you it's a very well fitting with this scheme.

430
00:48:06,010 --> 00:48:09,340
So observed data is individual observation.

431
00:48:10,150 --> 00:48:14,410
There is some hidden latent variable you cannot observe from the from the data.

432
00:48:14,530 --> 00:48:21,670
So. Then MRI algorithm is basically is our incredible algorithm.

433
00:48:22,660 --> 00:48:26,230
You. You make a guess about unobserved data.

434
00:48:27,700 --> 00:48:36,880
And you you update your parameter for modeling, observe data, right.

435
00:48:37,870 --> 00:48:43,239
And you calculate the likelihood, assuming that you're your guests on these.

436
00:48:43,240 --> 00:48:47,260
Correct. And after updating every parameter.

437
00:48:48,500 --> 00:48:54,140
U. U. We estimate the missing parameter again.

438
00:48:55,280 --> 00:49:01,129
Okay. Based on all these observations and everything and keep keep doing this.

439
00:49:01,130 --> 00:49:14,170
So. So you fix so big, you keep an event X, you estimate the Z, and given Z, you estimate the distribution of X and I go, go, go back and forth.

440
00:49:14,170 --> 00:49:22,680
So the, so the step that, that you infer Z is called E X is that.

441
00:49:23,720 --> 00:49:30,410
And our expectations stem and you update the procedure top data parameter is called the maximum step tab.

442
00:49:30,420 --> 00:49:40,959
So that's why it's called E m I made them. So we're going to talk about the example of a Gaussian mixture.

443
00:49:40,960 --> 00:49:49,000
And I know that in in the example of the midterm is a binomial mixture, but I'm going to talk about the Gaussian mixture.

444
00:49:50,980 --> 00:49:54,459
So when are the EMR, which I'm useful?

445
00:49:54,460 --> 00:50:05,390
Well, EMR worries me is useful if the problem become very trivial, if you have the complete data again.

446
00:50:06,070 --> 00:50:12,850
But problem is more much more sophisticated if you don't have company, if you have a missing data, this is heart problem.

447
00:50:13,210 --> 00:50:17,950
If you don't have if you have complete data, usually you have a closed form solution.

448
00:50:18,430 --> 00:50:22,360
So that's the ideal setting to use the M strategy.

449
00:50:22,660 --> 00:50:29,170
Okay. So the mixture is a good example because you have a mixture of a Gaussian.

450
00:50:29,800 --> 00:50:37,870
So you have some data that. That looks like, you know, very, very typical.

451
00:50:40,330 --> 00:50:43,750
Very typical setting is that you have one dimensional motion picture.

452
00:50:44,500 --> 00:50:48,610
You have a you know, you have a some some distribution of this.

453
00:50:48,940 --> 00:50:56,080
So. Well, this is a. Well, this is the distribution of height of some population, I say.

454
00:50:56,590 --> 00:51:07,870
Okay. And there are many different factors, but usually the biological sex can be one of the factor.

455
00:51:07,870 --> 00:51:17,110
For example, in the case, you know, you can model as a mixture of males and females and that'll that'll probably you can model a lot cleaner.

456
00:51:17,280 --> 00:51:23,170
Okay. So then if you. So if you don't know who's male and female.

457
00:51:24,330 --> 00:51:29,190
And the modeling, these all these all this vision is kind of hard.

458
00:51:29,550 --> 00:51:36,480
Okay. But if you know oh, if you know the weather there, males and females,

459
00:51:36,480 --> 00:51:45,090
and if you assume that they they have a final distribution, given the sex information, this problem becomes very easy problem.

460
00:51:45,390 --> 00:51:57,710
Right. So. So how do we do the proper inference without knowing that missing data in this case, in this hiding generally is just a sex?

461
00:51:58,370 --> 00:52:00,560
But there might be other similar example.

462
00:52:01,050 --> 00:52:08,960
Okay, so calcium mixtures, basically, you're filling in the missing data group assignment for each observation.

463
00:52:09,830 --> 00:52:19,580
But for example, the tricky part is that, oh, someone is like, uh, somebody, someone is in this place that doesn't.

464
00:52:21,020 --> 00:52:28,490
That doesn't give you an information. Exactly. If you take information about group assignment.

465
00:52:28,550 --> 00:52:33,050
Sometimes. Sometimes. If they're in the extreme, you can.

466
00:52:34,040 --> 00:52:37,429
You can be quite confident about whether this is male or female.

467
00:52:37,430 --> 00:52:40,860
But you're going to be never a perfect.

468
00:52:40,860 --> 00:52:45,440
So you need to assign this group somewhat probabilistically.

469
00:52:45,480 --> 00:52:48,890
Okay. So that's the tricky part. Okay.

470
00:52:50,300 --> 00:52:53,480
So let's look at the formulation of the M in the Gaussian mixture.

471
00:52:54,990 --> 00:52:58,040
So we're just talking about the one dimensional Gaussian mixture.

472
00:52:58,050 --> 00:53:04,140
Super simple, but still not not very, very easy to come up with a good algorithm.

473
00:53:05,750 --> 00:53:09,920
So in the culture mixture you have a three different parameters.

474
00:53:10,700 --> 00:53:14,030
You have a PI is a mixing proportion.

475
00:53:14,900 --> 00:53:23,060
You have a meal, which is a means in the sigma, which is a standard deviation or for seamless care of various.

476
00:53:24,140 --> 00:53:29,150
So and this is the likelihood of, gosh, a mixture.

477
00:53:30,260 --> 00:53:39,620
Okay so you are we it's a weighted mean of the pdf of the normal distribution for each of the component.

478
00:53:41,140 --> 00:53:49,990
So this is the distribution. This distribution is hard to track analytically because it's the addition of exponential functions.

479
00:53:50,530 --> 00:53:56,630
Okay. And, uh.

480
00:53:57,380 --> 00:54:06,140
Yeah. And if you have a k different classes here, you're hidden variable is the class assignment.

481
00:54:06,350 --> 00:54:10,180
So you just have this observation. You don't know where.

482
00:54:10,550 --> 00:54:14,660
Oh, whether the the individual is male or female, that information is hidden.

483
00:54:14,930 --> 00:54:22,550
Okay. So then you have a two types of a likelihood.

484
00:54:23,030 --> 00:54:29,300
One is the observed data likelihood. This is real likely because they see the variable is not given.

485
00:54:29,690 --> 00:54:35,150
There's no way that you can you you can exactly know the hidden variable.

486
00:54:36,560 --> 00:54:39,620
So this is your actual likelihood.

487
00:54:39,620 --> 00:54:45,290
Theta likelihood is that probability of x given specific parameters.

488
00:54:45,500 --> 00:54:50,900
You have a specific parameters parameter here is that combination of mixing mixing proportion.

489
00:54:51,320 --> 00:54:58,309
So if you know the mixing proportion, you know the means and variances, then you can calculate the likelihood.

490
00:54:58,310 --> 00:55:01,940
There's no problem of calculating likelihood if you know the parameter.

491
00:55:01,940 --> 00:55:05,030
Exactly. The problem is that we don't know the parameter.

492
00:55:05,150 --> 00:55:08,240
Right. So. But it is. This is it. Okay.

493
00:55:09,200 --> 00:55:12,470
So this is often intractable. Okay.

494
00:55:12,650 --> 00:55:15,710
The problem is intractable. This is this is hard problem.

495
00:55:16,810 --> 00:55:23,290
Again. So how do you how do you actually make it tractable?

496
00:55:23,310 --> 00:55:35,160
So that's that is a question. But the problem can be very trivial if you if you pretend that you know all the class assignments.

497
00:55:35,430 --> 00:55:43,890
Right. Okay. So, uh, so, yeah, my wisdom is working in these two steps.

498
00:55:44,990 --> 00:55:52,060
One is. You assume that you your perimeter is correct.

499
00:55:52,080 --> 00:55:55,740
You know that parameter. You assume that you know the perimeter.

500
00:55:56,250 --> 00:56:05,880
Okay. So theta t is a in iteration t you assume that you know the perimeter all the I know all the missing proportion.

501
00:56:06,510 --> 00:56:12,020
All these. The means and variances.

502
00:56:12,030 --> 00:56:21,530
I'm going to assume that I know of and I'm going to just calculate the conditional distribution of these hidden variables.

503
00:56:22,380 --> 00:56:40,220
Okay. Okay. So. And so then I can calculate fake probably kill the z given and observe data and all this popular at the iteration.

504
00:56:40,220 --> 00:56:43,220
T So you copulate this quantity.

505
00:56:44,550 --> 00:56:53,580
If you can calculate this quantity, you can write this function is called expected local likely of data.

506
00:56:53,900 --> 00:57:03,300
Okay. So because you have a full, you know, the full distribution, you just calculate this is a complete data likelihood and you take the expectation.

507
00:57:03,800 --> 00:57:10,260
Okay. Assuming that you are getting that based on the based on the assumption that,

508
00:57:10,260 --> 00:57:15,020
you know, that you reject maybe the probability of these probabilities we should see.

509
00:57:15,030 --> 00:57:23,640
So I assume that I know the distribution of Z based on the X in the current version of data and based on the distribution of Z,

510
00:57:24,030 --> 00:57:27,510
I can calculate this expected log likelihood in.

511
00:57:30,680 --> 00:57:37,910
So once they calculate these expectations, expect a lower likelihood that my step is basically well.

512
00:57:38,150 --> 00:57:43,700
So this looks very confusing actually. Just so I need to explain what this is.

513
00:57:44,570 --> 00:57:50,990
So here theta t, theta t is not something you're trying to optimize here.

514
00:57:51,380 --> 00:57:56,030
So theta T is, you know, given parameters.

515
00:57:56,150 --> 00:58:06,470
Right. So at a step t you have some product, you know, t's a fixed value now and you have another theta, which is unknown.

516
00:58:06,770 --> 00:58:10,850
Right? So you can model the likely theta assuming.

517
00:58:11,180 --> 00:58:16,420
So even though Z is not calculated from T, okay.

518
00:58:16,640 --> 00:58:20,390
But I'm going to assume that Z is just correctly calculated.

519
00:58:21,050 --> 00:58:27,740
But you can you can calculate the likelihood of all this data as a function of theta function time.

520
00:58:29,400 --> 00:58:32,880
Then this expecting low likely this function of data.

521
00:58:33,540 --> 00:58:37,169
Okay. New data. So I'm assuming some parameters.

522
00:58:37,170 --> 00:58:43,980
True, but I'm going to explore different parameters and try to calculate the expected low likelihood.

523
00:58:44,640 --> 00:58:52,140
And what I my m step is doing is to maximize that expected low like.

524
00:58:53,600 --> 00:58:57,260
So that's what that word and that does.

525
00:58:58,450 --> 00:59:07,720
So if you look at this slide. And if you are not, if you look at this line for the first time and if you are not confused over the migrant.

526
00:59:08,010 --> 00:59:15,400
Okay, in very hard to understand what this means. So you need to interleukin actually examples that.

527
00:59:16,600 --> 00:59:22,770
So. So this is a this is a major biotech paper.

528
00:59:22,880 --> 00:59:28,060
I think this is a really good introduction of EMR.

529
00:59:28,980 --> 00:59:32,460
So I think this is a really good paper you might want to read. Okay.

530
00:59:33,620 --> 00:59:37,410
So. So why this works.

531
00:59:37,560 --> 00:59:42,180
So this this is a this is a very good illustration.

532
00:59:42,180 --> 00:59:46,919
So I'm going to explain. So usually you're your editor for logo.

533
00:59:46,920 --> 00:59:51,750
Like a function is very complicated. Like this is blue blue curve.

534
00:59:53,660 --> 01:00:00,139
So the problem. So and you obviously don't have a one dimensional parameter, you have a multidimensional parameter,

535
01:00:00,140 --> 01:00:04,130
but in this case, just one dimensional parameter for for you explain.

536
01:00:04,250 --> 01:00:16,510
Easily explain. So what Im Yamaguchi team is trying to do is basically finding the maximum global maximum well local maximum to took effect.

537
01:00:16,520 --> 01:00:22,400
But so you are starting from here and you want to climb up until here.

538
01:00:23,060 --> 01:00:26,480
Okay, but to climb up.

539
01:00:27,080 --> 01:00:30,710
Okay, I don't know what the function looks like.

540
01:00:31,070 --> 01:00:36,920
Okay. So what is what is low like expected low like function?

541
01:00:36,920 --> 01:00:46,309
This looks really complicated, but what this is trying to do is that it makes some convex function so you can,

542
01:00:46,310 --> 01:00:49,820
in this case, parabola, but it is a just visualization purpose.

543
01:00:50,450 --> 01:00:56,120
It makes the context function. This exactly look like it has a property that it crosses.

544
01:00:56,120 --> 01:01:00,590
So because of, you know, the you assume that your parameter is through here.

545
01:01:01,610 --> 01:01:05,629
So you could actually know what the true true looks like through this.

546
01:01:05,630 --> 01:01:11,840
You already calculate this, then you can this is the expected low like a function.

547
01:01:11,840 --> 01:01:18,110
The g function is expected low like function. Okay, then you're are trying to maximize it.

548
01:01:18,260 --> 01:01:21,470
Oh, this is oh, how do I maximize this? And this?

549
01:01:21,560 --> 01:01:25,610
This is a maximal point. Oh, I'm going to try this point the next time.

550
01:01:25,970 --> 01:01:33,160
Okay. So now I'm going to assume this is a total perimeter and I can look at all the distribution of Z,

551
01:01:33,170 --> 01:01:45,590
even given extent theta T again and try to, uh, now you can draw another, another distribution.

552
01:01:45,860 --> 01:01:50,540
Okay. Here in Kolkata expect a lower likelihood and try to maximize again.

553
01:01:50,960 --> 01:02:01,550
You keep doing this, okay? That is what what is doing is the East Curve is basically calculating this green curve and instead is trying to find.

554
01:02:04,170 --> 01:02:14,700
Find the maximum point and try, try, try a new, new perimeter, basically, so that that one day support will begin again in a visual illustration.

555
01:02:16,890 --> 01:02:26,560
Okay. So this is a special case of minorities, makes them a maximization algorithm that is a more general version of EMI going to them.

556
01:02:26,560 --> 01:02:34,300
And then the minority makes a maximization algorithm is really, you know, great algorithm to learn about.

557
01:02:36,330 --> 01:02:39,990
Okay. Now I'm going to show some proof.

558
01:02:40,530 --> 01:02:50,610
Okay. I know that this is still very intimidating because you probably, you know, don't know what this all.

559
01:02:51,910 --> 01:02:52,600
You're saying?

560
01:02:53,660 --> 01:03:03,230
So what I can tell you is that implementing the embargo itself is a lot easier because there is a some standard sort of a way to implement it.

561
01:03:03,920 --> 01:03:11,180
But it is important to understand this line because whenever you sometimes you can

562
01:03:12,050 --> 01:03:17,960
implement the algorithm without knowing how to calculate the expected log like you can,

563
01:03:18,800 --> 01:03:27,570
you can even know how to calculate. How to how to do that instead, because there is a everyone is doing the same thing.

564
01:03:27,570 --> 01:03:31,379
I'm just following these. I don't know what I'm doing, but that's not a good practice.

565
01:03:31,380 --> 01:03:36,510
Right? So you you need to know what you're doing and to understand what you're doing.

566
01:03:36,510 --> 01:03:40,080
You to verify that your every step is correct way to do it,

567
01:03:40,650 --> 01:03:47,670
you do need to calculate the expected level likely that make sure that your update rule is correct.

568
01:03:47,880 --> 01:03:54,570
Okay. So to understand that you need to understand this, this, this a few slides carefully.

569
01:03:55,860 --> 01:03:59,800
Okay. So what does this do?

570
01:04:01,270 --> 01:04:11,110
So these surrogate functions. Okay. So this is expecting low global likelihood is basically you're calculating the

571
01:04:11,110 --> 01:04:16,630
conditional distribution of Z given observe theta and assume the parameter.

572
01:04:17,800 --> 01:04:24,130
But you are modeling the low likelihood of low likelihood.

573
01:04:24,670 --> 01:04:27,700
Basically, this is the, the ratio.

574
01:04:28,390 --> 01:04:32,650
Uh. Between the the complete.

575
01:04:32,650 --> 01:04:37,450
This is a complete data likelihood. Complete data like.

576
01:04:37,570 --> 01:04:47,650
Sorry, this is a complete data likelihood. And this is the conditional distribution of p p of z given given everything.

577
01:04:47,860 --> 01:04:53,439
Right, then the upper part is expected in all likelihood.

578
01:04:53,440 --> 01:04:57,819
That's how we defined right this this is expected a lower likely.

579
01:04:57,820 --> 01:05:05,560
So the Q Q is here and the bottom bottom is this one, right?

580
01:05:06,790 --> 01:05:10,140
So then. So.

581
01:05:10,210 --> 01:05:13,830
So then if. So.

582
01:05:16,620 --> 01:05:19,860
So. Let's say. Okay.

583
01:05:21,110 --> 01:05:25,070
Let's say that data is exactly data.

584
01:05:25,310 --> 01:05:33,450
So let's look at this first. Okay. If they data data t then both the data is exactly the same.

585
01:05:33,470 --> 01:05:40,070
So we're calculated in P of x composite divide by p of zeek conditional on x, right.

586
01:05:40,640 --> 01:05:45,890
So this this ratio will be the same as a P of x.

587
01:05:46,010 --> 01:05:50,780
Right? So because this is conditional distribution, all parameter exactly same.

588
01:05:51,380 --> 01:05:58,220
So in this case, key of key of data of T is exactly the low likelihood.

589
01:05:58,580 --> 01:06:01,910
So that's what this is trying to say at this point.

590
01:06:02,180 --> 01:06:07,580
If the data is data T, it exactly overlaps with the actual law like in a function.

591
01:06:07,590 --> 01:06:18,380
So it overlaps here. But in other cases this g of t, you can prove that this is always below the low light.

592
01:06:18,540 --> 01:06:22,270
It can function. So this is a $0.10 inequality.

593
01:06:22,990 --> 01:06:29,620
And so it's a little, little hard to understand, I guess.

594
01:06:29,620 --> 01:06:37,290
But basically this one is that because it's a peak, because this is a low function, right?

595
01:06:37,300 --> 01:06:42,250
So you take the low, low. The function is concave.

596
01:06:42,640 --> 01:06:48,660
So you have you just to make case the make it low outside this.

597
01:06:48,670 --> 01:06:54,460
This is should be less than this because this is taking the expectation basically.

598
01:06:54,910 --> 01:07:04,450
So then if you do this, okay, then this part is the same as a logo Pete logo p of x given theta.

599
01:07:04,450 --> 01:07:09,310
So this is less than always below the logo like.

600
01:07:09,560 --> 01:07:14,490
So the that's the nice property of the geology.

601
01:07:16,750 --> 01:07:27,370
And if you. So then this function field g of t this this function is an easier to maximize because a you have.

602
01:07:28,980 --> 01:07:32,810
So you have this complete data like likely the incomplete data like this.

603
01:07:32,820 --> 01:07:38,700
Basically, you know, which one is which individual is a male, which one is a female?

604
01:07:38,710 --> 01:07:42,360
So then you can easily quite get the mean and variance in culture.

605
01:07:42,450 --> 01:07:44,669
In all the other examples, you know,

606
01:07:44,670 --> 01:07:51,300
that you'd actually if you take memberships you can calculate the parameter based on the distribution assumption usually.

607
01:07:51,840 --> 01:07:56,890
So that's, that's the setting. So you can calculate this probability easily.

608
01:07:56,900 --> 01:08:04,710
Then you can, you can get this or get this information because you also know this conditional distribution well.

609
01:08:05,280 --> 01:08:14,220
So now you just the what what matters is just maximize this parameter with respect to T.

610
01:08:14,670 --> 01:08:19,170
So with respect to data and that that's what that's what you can you can do.

611
01:08:20,250 --> 01:08:23,870
So. Then. Ah yeah.

612
01:08:23,890 --> 01:08:29,950
So you can, you can choose. Q as a as this condition distribution and that that becomes an EMI with them.

613
01:08:30,280 --> 01:08:34,730
Okay. So. Now let's.

614
01:08:35,400 --> 01:08:40,240
So this is a general principle. Again, this is just a mathematical illustration of how this works.

615
01:08:40,240 --> 01:08:45,450
So this is a this is basically always a lot smaller than this.

616
01:08:46,830 --> 01:08:50,580
The full the margin. This is a marginal over like a function, right?

617
01:08:51,480 --> 01:09:03,990
So so low like these are expected low likely is always below the actual likelihood, but it crosses the actual value at the time of data.

618
01:09:03,990 --> 01:09:08,960
T. So now. So you need to.

619
01:09:09,080 --> 01:09:16,850
So you have all the principles. But until you know, actually, you didn't you get to that actually, because it's hard to understand how this works.

620
01:09:16,940 --> 01:09:19,850
So let's look at the actual examples.

621
01:09:21,250 --> 01:09:35,350
So when you have a make mixture, there are two types of like what it is I said so there there are observed data likelihood which we saw here.

622
01:09:35,980 --> 01:09:37,780
This is observed data likelihood. Right.

623
01:09:38,800 --> 01:09:47,350
And you also have a complete data like complete data likely this a case where, you know, the membership of all observation are all individual.

624
01:09:47,350 --> 01:09:53,080
So this is much easier because of, you know, you know which individual they belong to.

625
01:09:53,770 --> 01:10:01,480
So you can just calculate that multiply, multiply that everything based on this.

626
01:10:03,420 --> 01:10:09,479
You know, based on this full likelihood, but using the actual membership action mean and standard deviation.

627
01:10:09,480 --> 01:10:12,510
So there is a no no summation here.

628
01:10:12,750 --> 01:10:17,520
Right. So in the complete data, like the original data, like a you have a summation.

629
01:10:17,520 --> 01:10:26,639
This part is this part is hard part because now you need when you multiply, you have you are multiplying the summation.

630
01:10:26,640 --> 01:10:31,080
So that's a hard to that's that's the information that is hard to track there's no summation here.

631
01:10:31,440 --> 01:10:33,150
This is a much more tractable. Right.

632
01:10:33,870 --> 01:10:43,730
So if you because this is a complete data likelihood if you take the low likelihood, look likely looks like this it's pretty straightforward.

633
01:10:43,740 --> 01:10:47,819
Every every product changes to summation.

634
01:10:47,820 --> 01:10:51,270
So this is a everything is linear, nice and easy.

635
01:10:51,630 --> 01:10:56,770
You can optimize it pretty well and. So.

636
01:10:58,580 --> 01:11:04,790
If you assume that, you know data so used to you assume that you know data right.

637
01:11:05,180 --> 01:11:13,220
The I have a best guess data I'm going to try that and I want to know what the distribution of Z looks like given excellent data.

638
01:11:13,370 --> 01:11:19,040
Okay. So then you need to calculate this expected low likely to do that.

639
01:11:20,020 --> 01:11:25,400
But because you have a full log, likelihood of what you need to do is just do this.

640
01:11:25,610 --> 01:11:29,760
Okay. Okay. And in this case.

641
01:11:29,780 --> 01:11:35,839
But the the difference here is that in the low likely, you know, the membership.

642
01:11:35,840 --> 01:11:42,250
Exactly right. But in the estab, you don't actually know the membership.

643
01:11:42,560 --> 01:11:48,230
Your calculate the expectation, you know the parameter, but you don't know the Z.

644
01:11:48,860 --> 01:11:52,370
Z is only given as a probability.

645
01:11:52,970 --> 01:11:58,010
So Che, because of you taking the expectation of this identity function,

646
01:11:58,520 --> 01:12:05,780
this becomes a probability, conditional probability that G belongs to certain component.

647
01:12:06,200 --> 01:12:13,370
Given your data and observation, this is a core part that you need to keep track of as information.

648
01:12:13,640 --> 01:12:18,620
Okay. But you can calculate this. Okay. So the other part is a straightforward.

649
01:12:19,580 --> 01:12:26,690
Okay. So. Okay, so that that's the that's the first part.

650
01:12:26,690 --> 01:12:32,060
Let's let's go over the actual example just to to have a sense.

651
01:12:32,390 --> 01:12:41,719
Okay. So. So this is the this is the part you just to get started with.

652
01:12:41,720 --> 01:12:46,580
This is not M algorithm. It's just a simulating mixture of normal.

653
01:12:46,900 --> 01:12:51,270
Okay. So. Similarly from normal.

654
01:12:51,270 --> 01:12:56,040
How do we do it? So you have we have a we're going to simulate different values.

655
01:12:56,310 --> 01:13:05,670
Okay. And this is a new the vector muse vector of sigma is and a vector mixing proposition.

656
01:13:06,420 --> 01:13:16,200
And what it does is that I'm going to sample which cluster each of observation will shoot below which component.

657
01:13:16,440 --> 01:13:16,700
Okay.

658
01:13:17,370 --> 01:13:27,870
And I'm, I'm going to sample from normal distribution, but I'm going to scale by sigma and add new for each according to the each of the components.

659
01:13:27,870 --> 01:13:30,480
So it will become a mixture of normal.

660
01:13:31,020 --> 01:13:41,100
And I'm if I throw away this cluster information, there's no way to recover that information with 100% confidence.

661
01:13:45,460 --> 01:13:53,140
Okay. And I'm going to talk about a little bit of trick or treat that you probably don't know.

662
01:13:53,710 --> 01:13:58,330
This is using a class in r. So all.

663
01:14:00,280 --> 01:14:08,950
So you don't have to use this. I'm just showing this is possible and using class in order is a little tricky.

664
01:14:09,220 --> 01:14:16,390
It's not as systematic or sound nice compared to Python class or C++ class,

665
01:14:16,930 --> 01:14:23,680
but if you if you need to maintain a very large amount of information, you can do that with a list.

666
01:14:23,920 --> 01:14:28,690
Sometimes. But the list might be too big, so you might want to organize a little bit more.

667
01:14:29,080 --> 01:14:38,889
Then you can use these kind of tricks. So basically the R class you can make a class like a this is a class is known makes M I made and

668
01:14:38,890 --> 01:14:45,820
you can keep track of bunch of member variables and you can also keep track of member functions.

669
01:14:45,820 --> 01:14:53,800
So you can you can close the class is a collection of member variable and member functions and that's that's what I'm going to implement here.

670
01:14:54,280 --> 01:14:56,829
Obviously, you don't have to do that for your homework.

671
01:14:56,830 --> 01:15:04,990
This is this is just the one way to do it in your EMR, which I'm I'm pretty sure that that can be implemented in one function if you wanted to.

672
01:15:05,850 --> 01:15:11,430
But you are welcome to use these kind of schemes to sow fear.

673
01:15:11,490 --> 01:15:15,180
What kind of information do I need to keep track of all this information?

674
01:15:15,240 --> 01:15:24,750
So K is a integer value represent the total number of components and is the integer value that represent the total number of observations.

675
01:15:25,960 --> 01:15:30,250
The vector is actual observation. That is a vector of observation.

676
01:15:31,060 --> 01:15:36,100
PY vector is a vector of the mixing proportion.

677
01:15:36,760 --> 01:15:47,140
New vector picked out means picked up sigma. These are all the parameters and the probability matrix is a matrix that I'm going to store these values.

678
01:15:48,270 --> 01:15:57,600
Okay. And a low likelihood is a value I'm going to keep track of and a polarizes convergence threshold.

679
01:15:57,870 --> 01:16:06,870
Okay. Then what I'm going to what I'm doing here is basically just to define some member functions here.

680
01:16:07,560 --> 01:16:10,770
So. You can define a by function here.

681
01:16:10,780 --> 01:16:18,520
So no. So now. No. M is a class. Or you can define a methods method and you can define a new method like this.

682
01:16:19,240 --> 01:16:23,380
Initialize is a function. So you you put a new function here.

683
01:16:23,950 --> 01:16:29,440
So I'm making a function called the initialize and the initialize based based on the input.

684
01:16:30,310 --> 01:16:35,020
I'm giving the list of the observations and the number of components.

685
01:16:35,950 --> 01:16:42,370
And one thing you need to remember is that you need to use a global assignment inside this member variable,

686
01:16:43,330 --> 01:16:49,270
because if you use the local the local assignment, everything evaporates up, the function finishes.

687
01:16:49,930 --> 01:16:53,860
So that's that's the bad part. I don't really like this.

688
01:16:54,370 --> 01:17:00,700
That's a bad part of using the the class on the in r.

689
01:17:00,910 --> 01:17:06,559
Okay. So yeah. So that that's the data vector.

690
01:17:06,560 --> 01:17:10,040
Okay. Number of components in the length your sign.

691
01:17:10,310 --> 01:17:17,660
So do this and you also in it paras is initialize the initial time.

692
01:17:18,110 --> 01:17:22,820
Okay. So to get started, we, I need to initialize a part of your in some way.

693
01:17:23,390 --> 01:17:34,070
So there are multiple different strategy here. But here I'm going to use a very small number tolerance, a 10th in most hundred and mixing proportion.

694
01:17:34,070 --> 01:17:38,180
I'm going to assume that every component is equally probable.

695
01:17:38,330 --> 01:17:41,430
I think that's reasonable. And.

696
01:17:42,780 --> 01:17:46,430
New vector. This is a little bit interesting choice in that,

697
01:17:46,980 --> 01:17:55,049
you know when you when you when you implement your all yeah my wisdom how do I select the initial popularity

698
01:17:55,050 --> 01:18:02,940
is really critical part so you have a very different strategies obviously you don't want it to be biased,

699
01:18:02,940 --> 01:18:09,500
but also you want it to be a little bit different between the components with each component, the two similar to each other.

700
01:18:10,080 --> 01:18:14,190
I'm going to take a mean of everything I'm making. Going to make a meal all the same.

701
01:18:14,520 --> 01:18:19,590
It's never going to converge because each component is actually seems so it's not going to diverge.

702
01:18:20,310 --> 01:18:22,940
So you need to make each component slightly different.

703
01:18:22,970 --> 01:18:29,670
In this case, I'm using the quantile function to Oh, I'm going to use like, you know, k k different quantile.

704
01:18:29,970 --> 01:18:34,140
So to, to use, to use them as a mean point.

705
01:18:35,070 --> 01:18:43,260
But for sigma, I'm going to use the just the variance of all the data together because I mean has to be different,

706
01:18:43,260 --> 01:18:48,839
but sigma doesn't have to be head doesn't have to be different to to model the library differently.

707
01:18:48,840 --> 01:18:53,430
So I'm going to use the same sigma and the primary metrics look like, right?

708
01:18:53,430 --> 01:18:58,170
I'm going to give some initial the initial value that is reasonable.

709
01:18:58,620 --> 01:19:04,280
Okay. Yeah. So that's their they've had one minute left.

710
01:19:04,550 --> 01:19:10,130
Okay. So I guess I need to talk about this Easter and mustard for the next class.

711
01:19:11,450 --> 01:19:14,520
Okay. So it's I think I.

