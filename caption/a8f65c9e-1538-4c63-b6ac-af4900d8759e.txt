1
00:00:00,150 --> 00:00:09,540
Two things that you might benefit from by reviewing the police say to your questions

2
00:00:09,540 --> 00:00:19,410
by Sunday and 5 p.m. so that you I can plan things accordingly like last time.

3
00:00:20,010 --> 00:00:26,850
Ideally, nobody said any questions except one person, and we kind of talked about that in the context of a problem.

4
00:00:27,690 --> 00:00:36,989
If there are more questions from from you all, then I will dedicate the one hour during the review to answer those questions.

5
00:00:36,990 --> 00:00:41,850
Otherwise, I will fall practice some of the practice problems.

6
00:00:41,930 --> 00:00:45,540
Okay, so that's the plan, really.

7
00:00:45,570 --> 00:00:51,690
Six so one week to. Well, there is no second term substitute for the one.

8
00:00:54,800 --> 00:00:57,830
So this is the only other McDermid you have like on the.

9
00:00:59,660 --> 00:01:02,720
Correct. And the Nick B, of course.

10
00:01:06,010 --> 00:01:09,910
And six one of the final exam. All right. And since we do have a flood.

11
00:01:10,120 --> 00:01:21,290
Okay. So. So that's all I wanted to say about the midterm.

12
00:01:22,850 --> 00:01:29,750
So watch out for the guidelines as well as as well as the practice.

13
00:01:30,860 --> 00:01:39,389
Exams. And for you to remember that if you do want me to review any particular topic,

14
00:01:39,390 --> 00:01:49,910
finding any sort of any slide or you have a general question, then send it email to me by 5 p.m. on Sunday.

15
00:01:50,690 --> 00:01:55,660
Okay. So we are not going to do this review jointly between the two sections,

16
00:01:55,680 --> 00:02:09,530
but each section is do any way to do that independent review because we will lecture for the first hour but the exam your day to join.

17
00:02:10,340 --> 00:02:17,480
Yes, you're. Oh, yes.

18
00:02:17,530 --> 00:02:30,239
Sorry. Thank you for that question. So everything that we covered deal today will be on the exam when I lecture on the dean's debut not being made.

19
00:02:30,240 --> 00:02:36,450
So I'll get so all that will be covered up to date will be part of the exam.

20
00:02:36,840 --> 00:02:47,100
Okay. Okay. Any questions for me on general linear hypothesis here?

21
00:02:54,790 --> 00:03:07,139
Often diagnostics or even diagnostics is a is a big point to be able to talk about the type of diagnostics you talk about.

22
00:03:07,140 --> 00:03:11,520
And I believe you can talk about influential observations.

23
00:03:12,390 --> 00:03:23,510
And then after that, it's more difficult. So you can think about the next three modules as part of the big mission agnostics.

24
00:03:23,510 --> 00:03:30,430
Although we didn't have like what they. Okay.

25
00:03:31,720 --> 00:03:41,620
Anything else? So let's pick up from where we left on on Tuesday.

26
00:03:42,580 --> 00:03:51,490
So we saw that. So we were talking about this example where I had three groups.

27
00:03:56,140 --> 00:04:05,440
26 subjects participated in a study designed to examine the effects of weight and activity level on cholesterol levels.

28
00:04:06,070 --> 00:04:13,060
So the subjects were placed in these three groups that are controlled running, running and weight training.

29
00:04:13,090 --> 00:04:20,560
The response variable was HDL and the full body expert group and and weight in pounds.

30
00:04:21,340 --> 00:04:29,020
And the question that we were trying to answer is, does the association between weight and HDL differ by group?

31
00:04:29,740 --> 00:04:34,959
So you all told me, and rightly so, that, you know, this is an interaction question.

32
00:04:34,960 --> 00:04:39,540
So we could sort of and answer this using interactive.

33
00:04:39,850 --> 00:04:46,690
But now we got stuck in the data framework and we are addressing this question.

34
00:04:48,430 --> 00:04:58,780
So basically we talked about stratified analyzes where we fit three separate regression model to each group.

35
00:04:59,920 --> 00:05:13,240
So three is a large sum. But now I convinced you that from the three SLR, you can go to one MLR model, one multiple linear regression model,

36
00:05:14,500 --> 00:05:24,760
because and the advantage of fitting one MLR is keeping the separate models does not readily allow us to test differences in parameters,

37
00:05:25,120 --> 00:05:33,850
differences in diabetes, meaning like when each group and I think three separate set out, each group has its own intercept and slope.

38
00:05:34,420 --> 00:05:40,270
And we wanted to test whether the slopes are the same across for the three groups.

39
00:05:40,570 --> 00:05:46,570
And if the slopes are same like, are the intercepts also the same?

40
00:05:46,600 --> 00:05:51,520
So in other words, we are interested in testing the hypothesis of parallelism.

41
00:05:52,000 --> 00:06:04,149
And if parallelism falls, then the next hypothesis is do the 394 overlap because we think add central to the same,

42
00:06:04,150 --> 00:06:07,630
then basically the three lines overlap. Right. This question.

43
00:06:30,200 --> 00:06:40,400
No, I'm not. That's why I said you can address it using interactions, but I'm guessing in the general linear hypothesis framework.

44
00:06:45,320 --> 00:06:55,820
No, there is no data. So I'm just guessing we in a different framework that they want to show you that sort of the advantage of of DNA.

45
00:06:56,060 --> 00:07:01,880
You can test more flexible hypotheses with DNA so that.

46
00:07:08,370 --> 00:07:12,600
They didn't give equivalent importance. Absolutely.

47
00:07:13,710 --> 00:07:18,200
But. This is another way of looking at the question.

48
00:07:19,430 --> 00:07:22,280
So I don't I. Is it clear in your mind,

49
00:07:22,280 --> 00:07:34,579
like you get an answer to this question in the interactions and casting you in the direction seem more and you can also put it in the framework.

50
00:07:34,580 --> 00:07:44,090
These new models we give you e just like many of the different cell coding versus cell means 40, you got equivalent inference.

51
00:07:44,270 --> 00:07:49,730
So think of it this way. I think this is like a semi sporting type of framework.

52
00:07:50,990 --> 00:07:55,250
You will get the equivalent inference, but the point of showing you gasping it.

53
00:07:55,400 --> 00:08:03,590
The delete framework is to store the flexibility of DNA to handle, you know, different kinds of.

54
00:08:04,780 --> 00:08:05,080
Okay.

55
00:08:06,910 --> 00:08:28,569
So we basically then showed that from the group specific laws, you can go to an MLR model and the way you go from the group specific class to MLR,

56
00:08:28,570 --> 00:08:39,100
you write the design metrics for each group, but then you are basically stacking the, the, the data for the 26 subjects.

57
00:08:39,100 --> 00:08:45,790
So like the first group had eight, the second group at the end, the third group then now you're stacking the,

58
00:08:46,270 --> 00:09:01,180
the data for the 26 subjects there and we and the MLR model then reduces or looks like this.

59
00:09:01,990 --> 00:09:07,000
So now why is a picture of 26 plus one beat beta?

60
00:09:07,180 --> 00:09:14,710
The parameter vector has six elements, so it's a six across one vector,

61
00:09:15,460 --> 00:09:27,790
three sets and three slopes for the three groups and the design matrix, we sort of stopped here.

62
00:09:28,090 --> 00:09:38,020
The design matrix basically is a 26 cross six matrix.

63
00:09:42,650 --> 00:09:51,460
So the design metrics that 26 across six metrics and the design metrics looked like this.

64
00:09:51,470 --> 00:10:05,570
So the first three columns correspond to the three intercepts and the last three columns correspond to the two the slopes, three slopes.

65
00:10:06,260 --> 00:10:14,270
So basically and the model then, you know, I mean, I wrote down the model again,

66
00:10:14,270 --> 00:10:21,690
the model is HDL is equal to beta one one plus beta two, D2 plus beta three D3 would be the one we get.

67
00:10:21,690 --> 00:10:27,620
The bigger three are the intercepts for the three groups. And then the slopes are like the,

68
00:10:27,620 --> 00:10:41,840
the part that involves the slopes is beta for D1 W plus beta five d2 w plus beta 63 W so beta for better 5 to 6 are the slopes for the three groups.

69
00:10:42,220 --> 00:10:51,560
Now g g is the indicator for group D, so take the value one did the value one.

70
00:10:51,920 --> 00:11:01,850
If Subject II is in Group G and the W's correspond to the base of the the subjects across the three groups.

71
00:11:02,420 --> 00:11:11,900
So note the above x spans the column of ones, but it does not contain an intercept.

72
00:11:14,090 --> 00:11:26,300
Okay, how does it expand the column one? Because if you take the sum of the first three follows, you get the column of 1111 126 times one.

73
00:11:26,870 --> 00:11:30,310
That column vector up once again.

74
00:11:30,480 --> 00:11:38,719
So it spans the the column of ones, but it does not contain the intercept.

75
00:11:38,720 --> 00:11:43,820
So this is kind of like I was drawing that analogy between self means and body.

76
00:11:45,020 --> 00:11:50,720
Okay? So now the first thing that we are going to do is to test the parallelism hypothesis.

77
00:11:51,200 --> 00:11:54,640
So HDL equal to beta one, divine blood B technology.

78
00:11:54,650 --> 00:11:56,059
Duplass Beta three D3.

79
00:11:56,060 --> 00:12:10,250
This is video on BW Beta three are the intercepts and then the slow is given by beta 4d1 plus beta five D2 plus with 63 times W.

80
00:12:10,250 --> 00:12:14,360
So this is the this is the part that corresponds to the flow, right?

81
00:12:16,790 --> 00:12:23,990
So now what I want to test is using the DL approach.

82
00:12:25,070 --> 00:12:29,270
I want to test whether the slopes of the group specific regression lines are equal.

83
00:12:31,670 --> 00:12:41,450
The non hypothesis then is what? Because remember, under this model, if I use this model specification for group one,

84
00:12:42,950 --> 00:13:06,590
the line is HDL equal to beat one plus beat up four times w right for root to its HDL equal to beta two plus beta five times W And for group three,

85
00:13:08,030 --> 00:13:25,350
it's HDL. Equal to beta three plus beta six times w this new member d d i's are the indicator or dummy variables by group membership.

86
00:13:26,020 --> 00:13:32,310
So to test the that the group specific regression lines are parallel.

87
00:13:32,320 --> 00:13:38,800
What do I what is the null hypothesis? The null hypothesis is bigger for the cultivator five equal to beta six.

88
00:13:39,760 --> 00:13:43,990
That would give me parallel lines, right? No zeros there.

89
00:13:46,420 --> 00:13:52,810
And what is alternative that at least one beta g would be different from the others?

90
00:13:53,200 --> 00:14:01,329
At least one line is not parallel to the others. So equivalently, how can I write this?

91
00:14:01,330 --> 00:14:11,020
I can write the null hypothesis as beta four minus beta five equal to zero and beta five minus beta six equal to zero.

92
00:14:13,240 --> 00:14:18,940
Yes. And the alternative is that either one of those is violated.

93
00:14:19,420 --> 00:14:24,910
Now, you might ask me, what about beat up four miners beat six equal to zero.

94
00:14:25,510 --> 00:14:29,350
And I also put that as part of the null hypothesis.

95
00:14:30,850 --> 00:14:35,220
So let me play it again. I saw one note and one.

96
00:14:35,230 --> 00:14:39,290
Yeah. I want to hear both of those two equations.

97
00:14:42,280 --> 00:14:47,710
Okay. So those two equations mean they're automatically beta four minus beta six equal to zero.

98
00:14:47,720 --> 00:14:53,280
So the third contrast is redundant. What happens is one.

99
00:14:53,960 --> 00:15:01,460
Does it still need? Four hypotheses see to it that for first.

100
00:15:03,680 --> 00:15:11,820
Although we don't need that. It's been a great it's been a six year.

101
00:15:13,990 --> 00:15:23,160
This is. No, no, we don't, because the third one for us will become redundant, number one.

102
00:15:23,160 --> 00:15:26,550
And number two. Let's say you put it in the in the dog,

103
00:15:26,700 --> 00:15:36,750
wake up one minus three desert sequences and you construct the the matrix that the matrix we have three rows and six columns.

104
00:15:37,230 --> 00:15:45,930
What would be the name of this we choose. It could be that the bitter miners with a 60.0.

105
00:15:51,710 --> 00:15:57,290
If I did still go ahead and added Vienna for my Las Vegas and frequent visitor, what would happen to the female prince?

106
00:15:57,350 --> 00:16:06,260
It would be three clubs across six meetings. But the last two rule that did make this is the sum of the first two rules.

107
00:16:09,350 --> 00:16:16,280
Right. So that's what we've done. Then it means that you will, you will.

108
00:16:17,060 --> 00:16:26,480
You are going to construct the test statistic based on the number of linearly independent laws of the P matrix.

109
00:16:26,840 --> 00:16:33,290
So you have to be the four minus because it's basically the third row would be the sum of the first two.

110
00:16:33,680 --> 00:16:43,070
You will still have only two linearly independent rows of one another, but the rank of D would still be two.

111
00:16:43,610 --> 00:16:48,230
So that's what redundant means. And.

112
00:16:52,770 --> 00:17:00,330
Taking it from me and use. Absolutely.

113
00:17:00,340 --> 00:17:02,560
Absolutely. So. Here is another question.

114
00:17:02,830 --> 00:17:09,370
Do I have to be married to make up for mine is because I reported it on B2C five minus because it's equal to zero than 30.

115
00:17:09,370 --> 00:17:17,560
Still I could do it does better for minus be the five equal to zero and beta four minus beta 6.20.

116
00:17:17,860 --> 00:17:27,940
Then beta five minus beta six equal to zero. Because again, bottom line is you can have only two linearly independent them that you need six.

117
00:17:29,260 --> 00:17:38,590
Okay, so to make sense. So now I'd frame each note in terms of the JLA and here is that the matrix.

118
00:17:40,300 --> 00:17:47,500
Basically the first row is 0001 minus one zero corresponding to beta four minus with a five equal to zero.

119
00:17:48,010 --> 00:17:54,430
And the second row is 00001 minus one corresponding to five, -3 to 6 equal to zero.

120
00:17:54,760 --> 00:18:02,260
So note that now if I multiply this Dmitry by.

121
00:18:07,570 --> 00:18:14,800
If I multiply this matrix by beta the vector beta bitterness.

122
00:18:15,220 --> 00:18:24,820
So beta one beta two beta three beta four beta five beta seeds by this beta vector.

123
00:18:29,120 --> 00:18:36,500
What do I get. I get the first gives me better for miners better five equal to zero.

124
00:18:39,840 --> 00:18:45,180
And the second loop gives me beta five minus beta six.

125
00:18:49,230 --> 00:18:59,430
So it said those two two said those two zero against the null hypothesis so that's my null my now is the beta equal to zero versus alternative.

126
00:18:59,430 --> 00:19:03,270
The P b does not equal to zero. Okay.

127
00:19:03,510 --> 00:19:15,950
So the test statistic is going to be de beta had transpose of that term.

128
00:19:17,730 --> 00:19:24,330
And then we we kind of conceptually give you the idea of how the best of the state

129
00:19:24,330 --> 00:19:32,879
is constructed and sandwiched in between debating transport and debating that.

130
00:19:32,880 --> 00:19:43,170
Is that kind of the variance for billions matrix of debate the height minus a sort of not minus leg bonding,

131
00:19:43,170 --> 00:19:50,700
a multiplier and that is d ex transport that single g transpose inverse of the whole thing divided by two.

132
00:19:50,700 --> 00:19:59,310
That two comes from the rank of D and in the denominator I have Sigma Head Square of the MSI.

133
00:19:59,820 --> 00:20:07,290
This has an m distribution with Degrees of Freedom two and when de the two comes from the rank of B, where does the Trinity come from?

134
00:20:07,650 --> 00:20:12,150
I have an equal to 26 and I have B.

135
00:20:12,150 --> 00:20:16,500
The dimension of the beta vector is six.

136
00:20:16,860 --> 00:20:22,709
Three intercepts and three slopes. Okay. So we will look at the r potential for.

137
00:20:22,710 --> 00:20:28,530
But the f statistic value was 2.95 with the P value of .075.

138
00:20:29,040 --> 00:20:34,800
So the data is consistent with the parallelism hypothesis, the null hypothesis.

139
00:20:35,340 --> 00:20:46,230
And we conclude that there is insufficient evidence to say that the associations between weight and HDL differ across exercise groups.

140
00:20:48,150 --> 00:20:51,660
So here is the data.

141
00:20:52,890 --> 00:21:01,379
So by reading the data in know again, 26 subjects in three groups.

142
00:21:01,380 --> 00:21:14,820
So I have my ID, subject I.D. and group weight and HDL, and here is the MLR model.

143
00:21:14,970 --> 00:21:20,630
Remember, we could fit a stratified we could do a stratified analysis, the three Islams.

144
00:21:20,640 --> 00:21:24,990
But I showed you how from the three SLR could go to one MLR.

145
00:21:25,410 --> 00:21:29,969
So that's this model here. Do you want to give three?

146
00:21:29,970 --> 00:21:38,580
Are the indicators or dummy variables for group membership and W is the way to query it?

147
00:21:40,140 --> 00:21:50,129
And I fit the model of HDL on the indicators for do you want me to do three for the four questions for

148
00:21:50,130 --> 00:21:59,340
those indicators will give me the intercepts for the three lines and then I have d one star weed d2,

149
00:22:00,120 --> 00:22:08,130
the D3 star weed and the coefficients corresponding to those three terms we give me the the slopes.

150
00:22:08,670 --> 00:22:24,930
So these are intercepts for the three groups and these are this will give me the slopes for the three groups.

151
00:22:26,610 --> 00:22:35,860
Good. And the minus one here tents are don't fit an overall intersect.

152
00:22:36,520 --> 00:22:48,520
Okay, so this is bad. It's like the sand means scalding just to bring home that point.

153
00:22:48,580 --> 00:23:00,100
Okay, so who knows? The people of the point estimates the intercept for the three groups and the slopes for the three groups,

154
00:23:00,460 --> 00:23:07,390
along with their standard error and the D statistics and the P value for each of those coefficients.

155
00:23:07,420 --> 00:23:14,220
Now, what I am interested in testing is again the parallelism hypothesis, right?

156
00:23:14,230 --> 00:23:20,980
That the slopes are the same. So here is the in r.

157
00:23:21,700 --> 00:23:24,840
Again, I sort of set the contrast me.

158
00:23:25,220 --> 00:23:28,810
So here is my D contrast matrix.

159
00:23:29,920 --> 00:23:41,320
And as you saw before, my contrast matrix has the four stro 0001 minus one zero and then the second row is 00001 minus one.

160
00:23:41,330 --> 00:23:47,470
So this is a loop cross six matrix with two, then two.

161
00:23:58,140 --> 00:24:05,850
Okay. So now I conduct the hypothesis test of parallelism.

162
00:24:07,110 --> 00:24:12,000
So here are tells me that these these are my hypothesis.

163
00:24:12,030 --> 00:24:21,690
So dubon started minor due to starboard with a chance for to go to zero to 3 to 4 -3 to 5 equal to zero.

164
00:24:22,170 --> 00:24:26,760
And the next one is G2 star rate minus G3 star rate equal to zero.

165
00:24:26,760 --> 00:24:34,200
So beta five minus beta six equal to zero. So basically I'm just mimicking what I, what I did couple slides back up.

166
00:24:35,580 --> 00:24:45,360
So again, nested models, extra self squares principle so more than one is the model on the null.

167
00:24:45,570 --> 00:24:55,110
This is the restricted model restricted because now I have three intercepts but only one slope if under the null.

168
00:24:55,200 --> 00:25:04,350
Right, the common slope. So how many parameters summit is estimating the restricted model four parameters.

169
00:25:08,410 --> 00:25:16,090
And so this is the model under null and model two is the model under alternative.

170
00:25:16,090 --> 00:25:23,140
So this is the full model, which is basically the model with three intercepts, three slopes.

171
00:25:23,530 --> 00:25:33,850
So fit those two models implicitly are fits those two models and I get the residual Somersworth for the two models.

172
00:25:34,540 --> 00:25:48,549
You know the difference and I get the final 5.25 and the extra degrees of freedom in the bigger model,

173
00:25:48,550 --> 00:25:52,490
in the full model of the model under each one is two.

174
00:25:52,570 --> 00:26:03,130
This is what corresponds to the rank of the TV grid and using the MSE from the full model,

175
00:26:04,720 --> 00:26:10,130
I construct the F statistic it's 2.95 with the P value of .75.

176
00:26:10,420 --> 00:26:14,440
This is what I was referring to in the previous slide. Okay.

177
00:26:15,760 --> 00:26:25,510
So all good with parallelism. Now suppose I want to test the equality in the in the intercepts.

178
00:26:26,890 --> 00:26:43,060
So. So basically, given that the p value was point or seven, I concluded that there is insufficient evidence to reject the null.

179
00:26:46,810 --> 00:26:51,910
So now the next step is I want to test the equality of intercepts.

180
00:26:53,980 --> 00:27:02,290
So the idea is if parallelism is not predicted, then the next question is to the line forward thinking.

181
00:27:03,100 --> 00:27:11,200
Because if the intercepts are the same or I cannot prove that there is no evidence to support that the intercepts are different,

182
00:27:11,560 --> 00:27:24,010
then basically the three dates overlap. So equality of intercepts, these correspond to the intercepts between the two later three.

183
00:27:24,460 --> 00:27:29,500
So we can also test the quality of the intercepts of the specific regression lines.

184
00:27:29,500 --> 00:27:33,370
And the null hypothesis would be better along equal to beta two.

185
00:27:33,370 --> 00:27:48,100
Equal to beta three. Yes. So I can again rewrite the hypothesis as Beethoven minus beta two equal to zero and beta two minus beta three equal to zero.

186
00:27:48,970 --> 00:27:58,000
And so the following up on Justice's question, I would write it as beta on minus beta two equal to zero and beta one minus three for two zero.

187
00:27:58,270 --> 00:28:06,550
The bottom line is that a two linearly independent one grasps.

188
00:28:07,900 --> 00:28:17,410
There may be many different ways, but we only need to specify two, so the other one will be redundant.

189
00:28:17,530 --> 00:28:23,229
So the D matrix now is this matrix here.

190
00:28:23,230 --> 00:28:30,760
This is the matrix of contrast. And the first row is one minus one 0000 corresponding to beta,

191
00:28:30,760 --> 00:28:43,950
one minus beta do equal to zero and the second row of the T matrix is zero one minus one 000 corresponding to beta two minus beta 3120.

192
00:28:43,960 --> 00:28:51,160
So once again, this is a to cross six matrix with the rank equal to two.

193
00:28:52,630 --> 00:29:00,160
So I can construct the test statistic and we'll see in the next slide.

194
00:29:01,720 --> 00:29:08,020
R gives me a death statistic like a F statistic value of 3.12,

195
00:29:08,020 --> 00:29:14,319
which under the null hypothesis has an F distribution with degrees of freedom to and 20 again

196
00:29:14,320 --> 00:29:24,910
for the same reason because in the full model you have six parameters that you are estimating.

197
00:29:25,390 --> 00:29:32,800
In the restricted model of the model on the null, you have four parameters null.

198
00:29:34,120 --> 00:29:37,250
So that's what you are now.

199
00:29:37,690 --> 00:29:44,330
So that's would the two and 20 comes from and we get a p value of point or six.

200
00:29:44,530 --> 00:29:59,889
So so basically in R again same principle, I first specify the contrast matrix and based on this contrast matrix,

201
00:29:59,890 --> 00:30:07,120
the hypothesis of interest due on minus three to equal to zero and D2 minus due to equal to the.

202
00:30:07,350 --> 00:30:15,600
Once again module under the null and alternative are nested and getting again the extra sum of

203
00:30:15,600 --> 00:30:27,989
squares principle not to fit that to construct the sum of squares due to the under each one.

204
00:30:27,990 --> 00:30:44,850
Given that that the model under null was already fitted so the so the it's distinct that I get is 3.12 with the p value of point six.

205
00:30:45,540 --> 00:31:03,600
Once again the conclusion is do not reject the null hypothesis and that is insufficient evidence to conclude that the intercepts are different object.

206
00:31:05,340 --> 00:31:11,010
So that's kind of the close of this example.

207
00:31:11,760 --> 00:31:25,620
And just to sort of, you know, I see that I give you the concept of why the delay tested this week as an investigation.

208
00:31:25,620 --> 00:31:37,010
So here are a few lines of the proof, or maybe I mean the proof, but like a sketch of the proof based on what you learned in module F.

209
00:31:37,080 --> 00:31:44,760
So recall we know from module the beta hat has a normal distribution with mean

210
00:31:44,760 --> 00:31:50,579
equal to beta and variance covariance matrix sigma squared x transpose x inverse.

211
00:31:50,580 --> 00:31:58,080
So this is from module F. Okay.

212
00:31:58,530 --> 00:32:12,510
So therefore, what happens to the debate that had a debate that we had remember we had is normal debate to have is a linear combination of normal.

213
00:32:12,660 --> 00:32:15,270
Linear combination of normal is also normal.

214
00:32:15,900 --> 00:32:28,290
So Peter had to alter the normal distribution with mean equal to the mean of Phoebe to her and I'll just show what it is and variance debate I had.

215
00:32:28,290 --> 00:32:32,670
So the linear combination of normality, the normal, that's what we are using here.

216
00:32:33,510 --> 00:32:38,880
Linear combination of normal.

217
00:32:44,760 --> 00:33:00,470
Is that okay? What is the expected value of debate that is it's equal to debater which under the null is c and what is variance of debater hat?

218
00:33:00,720 --> 00:33:09,000
This is again from what you read. It's D variance of retard b transpose.

219
00:33:09,750 --> 00:33:14,270
I know what variance of beta had. Is it sigma squared x transport x inverse.

220
00:33:14,270 --> 00:33:23,849
So I just plug in. Okay.

221
00:33:23,850 --> 00:33:29,880
So under the null hypothesis, expected value of EBIDTA at minus C is zero.

222
00:33:31,440 --> 00:33:39,790
Right? So what happens to the distribution of predictor hat minus C under the null hypothesis?

223
00:33:39,810 --> 00:33:44,730
Under H naught, it has a normal distribution,

224
00:33:44,730 --> 00:34:00,090
which means zero zero is a vector and variance covariance matrix given by this quantity here sigma squared d x transpose vaccine bursty transpose.

225
00:34:00,600 --> 00:34:04,379
So this is this looks ugly, but it's actually not.

226
00:34:04,380 --> 00:34:08,200
It's it's basically come straight from modular.

227
00:34:09,660 --> 00:34:13,620
Okay. So call this guy.

228
00:34:14,250 --> 00:34:16,950
Give this guy a name. Capital Sigma.

229
00:34:18,870 --> 00:34:30,929
So what I'm showing here, if I go to make a hat minus C under the null hypothesis is a normally distributed random variable.

230
00:34:30,930 --> 00:34:36,150
Multiply it normal, which means zero and billion for beginners matrix capital sigma.

231
00:34:37,260 --> 00:34:49,319
So I have these two results in algebra that we refer to now.

232
00:34:49,320 --> 00:34:59,879
And the first result is why if Y has a multi-billion normal distribution between zero and variance within sigma, then why transpose the main words?

233
00:34:59,880 --> 00:35:07,920
Why has a chi square with degrees of freedom equal to the rank of sigma and result?

234
00:35:07,920 --> 00:35:21,030
To see is that if you multiply a matrix by a full rank matrix, then the rank of the parent matrix does not change, does not reduce.

235
00:35:21,930 --> 00:35:31,500
So if you multiply, if you take the rank of p x transpose x inversely transpose because it's transpose x is a full rank matrix.

236
00:35:32,040 --> 00:35:41,249
So this matrix preserves the rank of D and so the rank of p x transpose that

237
00:35:41,250 --> 00:35:48,030
scene was the transpose is the same as the rank of P using these two results.

238
00:35:48,040 --> 00:35:56,920
Now applying this tool, then I can easily see that de beta had.

239
00:35:56,940 --> 00:36:03,540
So now I'm constructing a quadratic form. Remember quadratic forms in module F,

240
00:36:04,140 --> 00:36:15,750
so now I can construct a quadratic form debate I had minus C the debate I had minuses like my Y in the full plot, the y transpose C minus Y.

241
00:36:15,750 --> 00:36:19,080
So this is my y transpose in the footnote.

242
00:36:19,560 --> 00:36:25,330
This is my c, my inverse in the footnote and this is my Y in the footnotes.

243
00:36:25,330 --> 00:36:33,930
So y transpose said mine was y has a chi square distribution with a rank equal to D.

244
00:36:36,360 --> 00:36:43,890
So that's what I am using to construct the F statistic for the DNA.

245
00:36:43,920 --> 00:36:57,270
So basically we have this result and it's a non central chi square y, it's a non central score because the mean of debate I had minus C is zero.

246
00:36:58,680 --> 00:37:05,969
Okay? And in module F we had also shown that C over Sigma Square has a chi square root because of

247
00:37:05,970 --> 00:37:15,209
freedom N minus B and its central chi square didn't seem the previous one that the command said.

248
00:37:15,210 --> 00:37:22,890
No, no. They say the police officer central chi square right or debate to have minus see that the fourth bullet point

249
00:37:22,890 --> 00:37:32,130
itself by square with their B and it's a central square lambda is zero Y because the median of divided by.

250
00:37:33,540 --> 00:37:37,520
It's a new product, the same single. Anyway.

251
00:37:38,000 --> 00:37:39,350
And from what you left,

252
00:37:39,620 --> 00:37:49,310
I have already shown that this is seriously masqueraded as a central chi square with degrees of freedom in minus B under the line is uncertain.

253
00:37:50,390 --> 00:37:53,510
I have also shown that Peter had happen.

254
00:37:53,510 --> 00:37:57,250
It says c r independent. These are all from module F.

255
00:37:59,840 --> 00:38:10,160
So now the first rule, I have a case where the second rule I have a guy squid and the squid in the first rule and the second rule are independent.

256
00:38:11,150 --> 00:38:18,410
So of course I can take the ratio divided by the degrees of freedom to construct an extra distance.

257
00:38:18,410 --> 00:38:33,319
And that's exactly what I do. So the numerator in the numerator, I have this guy, this first guy, and divided by its degrees of freedom,

258
00:38:33,320 --> 00:38:39,229
which is the rank of P and in the denominator I have this says C over sigma squared

259
00:38:39,230 --> 00:38:45,140
this guy divided by its degrees of freedom and minus B so that the ratio is an F.

260
00:38:47,990 --> 00:38:57,139
And if you just simplify because there's a sigma squared inverse in the numerator and then there's a one over C must be in the denominator.

261
00:38:57,140 --> 00:39:03,350
So the sigma squared cancels out and I'm left with this one p here.

262
00:39:04,880 --> 00:39:14,000
What is sigma have squared. Sigma have squared is MSE, which is basically the ss e over n minus P from the full number.

263
00:39:15,380 --> 00:39:24,550
So under had the null hypothesis. This has an F distribution with degrees of freedom, then coffee and then minus.

264
00:39:25,160 --> 00:39:30,920
So that's the result I have been using to construct my LH test.

265
00:39:37,510 --> 00:39:46,810
Okay. So that completes the discussion on general linear hypothesis.

266
00:39:48,640 --> 00:39:52,970
Are there any questions? Yes.

267
00:39:53,120 --> 00:40:02,580
Is this something you would be expected to care? Is this something that, like, this is something.

268
00:40:14,370 --> 00:40:22,170
People like you asking about the proof or like any money, for example, where you would.

269
00:40:23,250 --> 00:40:32,510
But what that means is for you to get that pessimistic pattern, we would have to give you enough information about the beat you had about it.

270
00:40:32,730 --> 00:40:41,260
But it's important. It might require you to set up your matrix matrix of contrast.

271
00:40:41,640 --> 00:40:49,590
But we would have to give you enough information in terms of the pizza, have the emcee and so on.

272
00:40:49,870 --> 00:40:57,750
What do you construct the rest of this stage? Either that or we can give you an output from R and then.

273
00:41:00,600 --> 00:41:04,760
I'm struck down. If statistics value.

274
00:41:08,540 --> 00:41:17,930
And if you were asking about the proof, the derivation that be good because all of these concepts and all of these results

275
00:41:18,530 --> 00:41:26,000
we actually proved in modules and this was just borrowing the module results here.

276
00:41:27,180 --> 00:41:33,569
Yes. Why?

277
00:41:33,570 --> 00:41:40,510
That certain part? No, not a certain part. The sequence at square the circle part is the ratio.

278
00:41:40,830 --> 00:41:44,489
Sorry, you. I got this. Sorry.

279
00:41:44,490 --> 00:41:47,760
This is the MSE. Yeah. So I.

280
00:41:47,760 --> 00:41:52,500
The circle part is the ratio of the chi square.

281
00:41:53,340 --> 00:41:58,570
Is it as are. It's not it's it's odd exactly.

282
00:41:59,380 --> 00:42:04,840
But it's it's somewhat good if you do that contrast split.

283
00:42:10,340 --> 00:42:14,570
Okay. Any other questions? No.

284
00:42:16,160 --> 00:42:24,320
Okay. So I think that wraps up a module in D.

285
00:42:25,850 --> 00:42:29,120
No more questions on this. Everything is fine.

286
00:42:29,450 --> 00:42:33,590
Okay, then let's actually do this. Let's totally straight away.

287
00:42:34,100 --> 00:42:37,850
I'm going to diagnostics, but maybe this is a good time.

288
00:42:37,850 --> 00:42:45,350
So let's take a break for 10 minutes and let's come back sharp at nine and we'll start talking about diagnostics.

289
00:42:47,840 --> 00:42:51,560
Some of you came late. I talked about the exam.

290
00:42:53,870 --> 00:42:57,260
So please talk to your friends.

291
00:43:23,608 --> 00:43:31,618
So. So this is module key and.

292
00:43:34,598 --> 00:43:52,538
As I was mentioning that. From you from this point on board, like I think we have sort of learned from increasing importance.

293
00:43:53,528 --> 00:44:04,508
I mean, estimation in 1415 made it 15 deductions, more than 15, 15 hypotheses of the different frameworks.

294
00:44:05,108 --> 00:44:19,178
We have also talked about categorical predictors, but now we are launching in the discussion of our very important topic, which is model diagnostics.

295
00:44:19,178 --> 00:44:25,148
So you remember we started with a basic set of assumptions from the linear regression model.

296
00:44:26,138 --> 00:44:44,258
So what we are going to generally now talk about is are both assumptions where does the nonsense value constructed and how do we use our diagnostics?

297
00:44:45,488 --> 00:44:50,588
Oh, there's a time for sorting. It's not model diagnosis.

298
00:44:50,588 --> 00:44:55,327
Can you just. It's model diagnostics will be.

299
00:44:55,328 --> 00:45:03,908
They miss it. Model diagnostics.

300
00:45:07,678 --> 00:45:12,348
Okay. So we are going to basically assess.

301
00:45:16,008 --> 00:45:23,478
Whether to start with the assumptions made sense or the assumptions are valid

302
00:45:25,308 --> 00:45:32,598
in the context of the data that we are finding that the data that we have.

303
00:45:34,188 --> 00:45:43,968
So this is a long module we will talk about now the purpose of doing diagnostics.

304
00:45:43,968 --> 00:45:49,308
We talk about residuals, properties of residuals.

305
00:45:50,298 --> 00:46:00,878
Then we will review the model assumptions and talk about the impact of violating the assumptions.

306
00:46:02,868 --> 00:46:11,368
And we will use we will talk about extensively that use of residuals.

307
00:46:11,368 --> 00:46:21,078
So I kept on saying Labor all along when we were doing, you know, sort of model fitting that the residuals are the building blocks of my diagnostics.

308
00:46:21,078 --> 00:46:26,448
The residuals are really important and you see why I kept on saying that.

309
00:46:27,168 --> 00:46:32,747
So we will see the use of residuals in model diagnostics.

310
00:46:32,748 --> 00:46:44,657
We will talk about like what to do when there is some sort of indication that certain assumptions were not valid to start with.

311
00:46:44,658 --> 00:46:53,868
What what are the correction strategies? What do we do? Then we will talk about outliers and and then finish up with some examples.

312
00:46:54,588 --> 00:46:59,148
So this is chapter five and four and five from your textbook.

313
00:46:59,808 --> 00:47:03,828
But as I said, this is a this is a pretty long module.

314
00:47:04,428 --> 00:47:16,128
And we will also talk follow up. As a follow up, we talk about multiple linearity and we talk about influential diagnostics.

315
00:47:16,338 --> 00:47:23,388
So regression diagnostics includes assessment of model assumptions like the line assumptions.

316
00:47:24,028 --> 00:47:31,038
And then as I mentioned, we also talk about influential observations, outliers.

317
00:47:31,308 --> 00:47:38,718
I will define what I mean by influential observations, but just as the name suggests, for now,

318
00:47:38,988 --> 00:47:49,848
it's enough for you to know that influential observations are observations that that kind of have an undue impact on the model 50.

319
00:47:50,358 --> 00:47:55,148
So how do we assess which observations have influence?

320
00:47:55,338 --> 00:48:01,727
Then we'll talk about multiple linearity in the context of the model.

321
00:48:01,728 --> 00:48:10,968
There may be a sample before we have a really strong correlation between them and what's the implication of what they call linearity on the model.

322
00:48:11,478 --> 00:48:18,468
So these are all going to be part of my regression diagnostics lecture,

323
00:48:18,828 --> 00:48:27,428
but we will have separate modules for these two topics, much smaller but standalone modules.

324
00:48:27,678 --> 00:48:40,067
Okay. So first let's discuss the adequacy of the model assumptions, the line assumptions.

325
00:48:40,068 --> 00:48:53,328
So just as, as a as a recap, remember in the linear model Y equal to X plus epsilon, we started with these set of assumptions, this line assumptions.

326
00:48:54,138 --> 00:48:58,697
What are the line assumptions, the force, the error corresponds to linearity.

327
00:48:58,698 --> 00:49:11,468
So we assume that the the meaning of y or y is a linear function with three of of the whole periods.

328
00:49:11,628 --> 00:49:14,298
And the linearity is with respect to the betas,

329
00:49:14,538 --> 00:49:27,378
we'll get to the functional form of X and the linear functional form is often the focus of diagnostics for linearity.

330
00:49:27,948 --> 00:49:35,028
So this is the first assumption and it's probably the most important assumption, the functional form of X.

331
00:49:35,538 --> 00:49:38,838
So that corresponds to the end in the line assumption.

332
00:49:39,168 --> 00:49:42,258
The second one is the independence assumption.

333
00:49:42,258 --> 00:49:52,158
So we had assumed that the and then no matter so excellent I it's intended but I'm not because the dead are independent.

334
00:49:52,638 --> 00:49:58,078
So we have independent voters. So that's the eye of the line, as I'm sure.

335
00:49:58,188 --> 00:50:04,968
Then we have the N, which is which corresponds to the normality assumption.

336
00:50:06,018 --> 00:50:14,678
So we assume that epsilon either not random errors have a normal distribution, which means zero.

337
00:50:15,408 --> 00:50:18,768
Invariant sigma squared. So that's the end of the line.

338
00:50:19,188 --> 00:50:27,798
And then finally we have the equal variance or the homogeneity or the homo skidelsky assumption.

339
00:50:27,798 --> 00:50:34,488
And what is the equal release of something? It says that the variance of epsilon I is sigma squared for all II.

340
00:50:35,358 --> 00:50:43,248
So the variance is constant. It does not depend on the value or level of x.

341
00:50:44,898 --> 00:50:56,418
Okay. So that's those are the four assumptions in line and which we showed earlier on that for estimation,

342
00:50:56,808 --> 00:51:03,168
you don't need the distributional assumption, you don't need normality, but for inference you do.

343
00:51:03,858 --> 00:51:14,538
Right. So we are going to issue from now on work that we have assumed all the four assumptions

344
00:51:14,538 --> 00:51:24,558
on the line and we will now discuss how do we check the value of these assumptions?

345
00:51:27,158 --> 00:51:33,367
So note here that the assumptions are mainly about the Crusaders, about the epsilon.

346
00:51:33,368 --> 00:51:37,028
Ah, epsilon eyes are the true heroes.

347
00:51:40,098 --> 00:51:43,938
But the two errors are not observable, right?

348
00:51:43,968 --> 00:52:00,378
I don't know what the troops and allies are. So how do we kind of assess the the validity of the assumptions on the Twitter?

349
00:52:00,738 --> 00:52:06,918
What we are going to do is we are going to use the estimated inverse or what we call the residuals,

350
00:52:06,918 --> 00:52:13,457
the Epsilon hat, to infer the the behavior of the Crusader.

351
00:52:13,458 --> 00:52:19,578
So we will inform the behavior that we are that epsilon I had.

352
00:52:19,588 --> 00:52:25,518
So these are the estimated errors or residuals.

353
00:52:33,818 --> 00:52:44,408
The residual is a measure of the variability of the outcome that is not explained by the model because it's what y minus y you had observed,

354
00:52:44,408 --> 00:52:58,088
minus predicted. So the residuals are kind of the building blocks of my regression diagnostic.

355
00:53:22,238 --> 00:53:25,508
Yeah. That's why the residuals are so important.

356
00:53:27,158 --> 00:53:32,398
Okay. You know. What?

357
00:53:32,998 --> 00:53:47,278
So before we go into how to assess the adequacy of these model assumptions, I think we should ask the question Why do we even care?

358
00:53:49,678 --> 00:53:59,458
Why should we care? I mean, if the assumptions are valid or not in the first place and the reason why we care,

359
00:54:00,238 --> 00:54:19,788
we have to care is because there may be grave consequences to violating these assumptions and some of the potential consequences of their themselves.

360
00:54:19,858 --> 00:54:27,958
Meanwhile, it could be that, you know, the point estimates are biased.

361
00:54:30,748 --> 00:54:37,198
The standard errors would be wrong.

362
00:54:39,808 --> 00:54:47,368
The confidence intervals for the big goals could be inaccurate or wider,

363
00:54:50,998 --> 00:54:59,908
and the hypothesis tests would be valid because if the standard errors are wrong, then basically you are going to get the wrong test.

364
00:55:00,478 --> 00:55:06,898
So there are serious implications to the assumptions being violated.

365
00:55:11,068 --> 00:55:14,908
The question is how?

366
00:55:15,898 --> 00:55:19,528
What? You know, when will some of these happen?

367
00:55:19,558 --> 00:55:22,708
Under what circumstances? Under what violations?

368
00:55:23,998 --> 00:55:30,028
That depends on which assumption is violated and to what degree.

369
00:55:30,898 --> 00:55:35,798
Because, remember, this is this is real data, right?

370
00:55:35,818 --> 00:55:41,068
So it will not be like kind of a mathematical violation.

371
00:55:41,068 --> 00:55:44,098
It would be a stochastic probabilistic violation.

372
00:55:44,788 --> 00:55:58,618
So to what extent the advancements are violated, that will be fact, how the estimation would be impacted, how the inference would be impacted.

373
00:55:59,098 --> 00:56:07,708
But the bottom line is the reason we need to care is because there could be all these implications.

374
00:56:07,828 --> 00:56:16,078
We could get lost estimates, we could get wrong inference, we could get inaccurate confidence intervals,

375
00:56:16,078 --> 00:56:26,578
and B, that we don't seriously threaten our the validity of our inference and the value to all for.

376
00:56:27,418 --> 00:56:31,208
So you have to get good.

377
00:56:34,738 --> 00:56:41,248
What are the objectives here? So we re we talked about why we should care.

378
00:56:42,418 --> 00:56:49,318
Now let's talk about what are we going to do with this information.

379
00:56:50,548 --> 00:56:57,688
If we find that the assumption, some assumption in politics, like what do we do with that?

380
00:56:57,718 --> 00:57:09,328
So the objectives here in this lecture or in this module is we are going to first identify the violation of moral assumptions

381
00:57:11,608 --> 00:57:21,118
we made that may include also identifying observations that may have too much influence on the results I talked about,

382
00:57:21,118 --> 00:57:29,728
like, you know, sort of identifying influential observations that are out there, observations that are having an undue impact on the model.

383
00:57:31,828 --> 00:57:36,117
So outliers, influential observations.

384
00:57:36,118 --> 00:57:44,938
We'll talk about that aspect. And then we will understand the impact of the of these violations.

385
00:57:46,798 --> 00:57:51,508
When does it specifically the effect?

386
00:57:51,598 --> 00:57:54,998
Is it the inference? Is it the point estimate?

387
00:57:56,608 --> 00:58:01,948
And to kind of understand exactly what I said a few seconds back,

388
00:58:02,368 --> 00:58:10,798
that as these are not kind of mathematical violations, the assumptions will never hold perfectly.

389
00:58:13,228 --> 00:58:19,858
And the degree of the validity of the model results will depend on the degree to which the assumptions hold.

390
00:58:20,188 --> 00:58:22,408
They will never hold perfectly.

391
00:58:22,618 --> 00:58:36,508
So in a way, your task is to assess how much departure from an assumption is going to be problematic and how much we can ignore.

392
00:58:38,548 --> 00:58:49,158
So that's the that's that's part of the challenge and that's the the sort of experience you gather,

393
00:58:49,408 --> 00:58:58,348
you sort of develop as you as over time, as you analyze more and more data.

394
00:58:58,528 --> 00:59:09,128
Oftentimes I, I say that, you know, and you will see that when we talk about examples like there's a lot of like graphics,

395
00:59:09,148 --> 00:59:17,578
there's a lot of that is sometimes there is no formal test for assessing the adequacy of these assumptions.

396
00:59:18,148 --> 00:59:24,687
So we do a lot of plotting. We do a lot of like it's truly diagnostic.

397
00:59:24,688 --> 00:59:37,648
So there is a point where some folks might feel, oh, this is kind of like this is sort of like a little bit of science and also a little bit of art,

398
00:59:37,648 --> 00:59:45,558
like how our subject empowering in dissecting a certain diagnostic plot, oh, I see this, but you won't see this.

399
00:59:45,568 --> 00:59:50,068
So there is a little bit of subjectivity you might pick.

400
00:59:50,188 --> 00:59:54,178
But what I'm trying to deal with over time,

401
00:59:54,178 --> 01:00:05,067
what happens is you develop eyes or you develop like the the sort of the experience to know that there is that is some sort of an aberration,

402
01:00:05,068 --> 01:00:14,457
departure from normality. But what this is not going to really affect my inference.

403
01:00:14,458 --> 01:00:17,308
I mean, and this kind of departure, I can live with that.

404
01:00:17,488 --> 01:00:26,397
So it's it's like you are in the context of there's a bit there's a little bit of like subjectivity,

405
01:00:26,398 --> 01:00:39,678
but you kind of develop those set of eyes sometimes like and and at that point, keep in mind that the assumptions will never fall perfectly.

406
01:00:39,688 --> 01:00:46,468
This is real data, real life. So you cannot there will be some aberrations.

407
01:00:46,468 --> 01:00:51,568
But with the with training and with sort of like experience,

408
01:00:52,168 --> 01:01:05,158
you will know where the aberrations are going to be seriously influencing or impacting your results and when they can be considered benign.

409
01:01:05,968 --> 01:01:17,578
Okay. And the other thing that we are going to learn is, I mean, if there is volition that needs to be sort of addressed,

410
01:01:18,088 --> 01:01:23,008
then we need to be able to suggest potential corrective actions like what do we do?

411
01:01:23,998 --> 01:01:26,977
What do we do if the normality assumption seems like valid?

412
01:01:26,978 --> 01:01:33,358
And what do we do if the linearity assumption or the constant being says assumption assumption seems to.

413
01:01:33,498 --> 01:01:36,947
Violet did what kind of corrective actions we can take.

414
01:01:36,948 --> 01:01:43,488
And we've talked about some of the corrective actions like you can do transformation of response.

415
01:01:43,998 --> 01:01:49,698
You can sort of add polynomial terms.

416
01:01:50,688 --> 01:01:55,098
And and all of these will be suggested from the regression diagnostics.

417
01:01:56,568 --> 01:02:09,128
And then finally, what I want to not only mention, but I want to stress and oftentimes your job,

418
01:02:09,568 --> 01:02:18,198
oftentimes people don't realize that revision diagnostics can be an iterative process

419
01:02:18,948 --> 01:02:24,108
because violation of one assumption can mask the violation of another assumption.

420
01:02:24,378 --> 01:02:28,248
What does it mean in plain English? In plain English?

421
01:02:28,248 --> 01:02:36,588
As you work on your project, you'll see that they keep working on on on some real detail.

422
01:02:36,618 --> 01:02:44,148
And when we do six 1990, even more that you may have to you like in the final report,

423
01:02:44,148 --> 01:02:51,618
you might present one model, but that's not the model that you start to see you in the back.

424
01:02:52,728 --> 01:03:01,428
Behind the scenes, you might be creating several models, you might be doing diagnostics, you might be fixing certain aspects.

425
01:03:01,728 --> 01:03:06,708
Then you really do the more modeling, but then you find something else.

426
01:03:07,068 --> 01:03:14,658
And so it's a meditative process and it's not like you push a button and get a model and like a, like a project.

427
01:03:14,658 --> 01:03:22,668
It's not that I think oftentimes, like, you know, you will face this,

428
01:03:23,118 --> 01:03:28,848
like if you're working with collaboration collaborative in a collaborative environment where

429
01:03:29,118 --> 01:03:34,928
you are working with people from various disciplines because we have such great software,

430
01:03:35,028 --> 01:03:45,048
people might think, oh, you know, like you push a button and so far you see the results and you done no, it's not that.

431
01:03:45,858 --> 01:03:49,848
So please remember that this is an iterative process,

432
01:03:50,628 --> 01:04:02,117
and that's why it's so important to assess diagnostics at every step and finally will reach a point where,

433
01:04:02,118 --> 01:04:12,198
you know, you have satisfied you have said that that you have done to the best of your knowledge and ability, you have done all this subject.

434
01:04:12,318 --> 01:04:17,258
So we're happy that we can do with adequacy checks.

435
01:04:17,268 --> 01:04:22,668
We are happy with some of the more rapid diagnostic things before answering before that.

436
01:04:23,418 --> 01:04:29,238
But there is a process behind the scenes that is going on.

437
01:04:29,358 --> 01:04:36,738
Yes. So do you want us to present the process or you want to show you know, you can put that in the report,

438
01:04:36,858 --> 01:04:46,038
like what you hear the logic of the steps because I want because this classes have you done well you when

439
01:04:46,308 --> 01:04:55,338
you do not show the output from every step but you will have to convince me that how you reach that point.

440
01:04:56,848 --> 01:05:01,668
So it's like you start with the first thing you say, Who did this next? Yeah.

441
01:05:01,788 --> 01:05:09,948
For example, you can I'm just giving it as an example of maybe skipping some things, but jumping.

442
01:05:10,698 --> 01:05:18,527
So for example, with the first one and you do some individual diagnostics and see that based on your regular diagnostics,

443
01:05:18,528 --> 01:05:28,308
that you're often big incidences of viruses and you do something normal replacement management, something you before me the steps.

444
01:05:28,728 --> 01:05:33,828
I mean, that didn't happen because if it's a very common graph, then it goes into the main body.

445
01:05:33,828 --> 01:05:43,457
But at least in that when you saw that I did the diagnostics and look, this is how it looks and I know that there's a violation of the law.

446
01:05:43,458 --> 01:05:50,778
Something else is on compliance to see that. That's why you like you cannot suddenly say that I did this transformation.

447
01:05:51,528 --> 01:05:56,208
But tell me why you did that and give me the evidence for that.

448
01:05:57,048 --> 01:06:02,058
Okay. Okay.

449
01:06:02,148 --> 01:06:06,678
Any other questions? Okay.

450
01:06:06,678 --> 01:06:12,668
So that's that's sort of the big picture view of for this module.

451
01:06:12,678 --> 01:06:21,438
So now let's talk about residuals. The first, as I mentioned, the residuals are kind of the building blocks of regression diagnostics.

452
01:06:21,798 --> 01:06:28,367
So we'll talk about residuals first. So the first one is the conventional residuals.

453
01:06:28,368 --> 01:06:34,068
I mean, you know, this is this is more of a review. We have already seen this.

454
01:06:34,398 --> 01:06:41,807
So we assumed that the true error that and I have a normal distribution of multivariate

455
01:06:41,808 --> 01:06:47,178
normal which means zero variance will be respect sigma sweatpants identity matrix.

456
01:06:47,178 --> 01:06:53,418
Right. So this is the this is how we wrote the model in the matrix notation.

457
01:06:53,958 --> 01:07:08,508
Now epsilon is a vector and cross one vector of fluids, so the error to the model is y equal to XP de plus epsilon.

458
01:07:08,778 --> 01:07:17,238
Right. And this is the assumption that I make on on the Twitter so the I can also write.

459
01:07:17,238 --> 01:07:20,778
So under this assumption, the expected value of Y is greater.

460
01:07:22,158 --> 01:07:33,078
So I can also write the two error says Y minus the expected value of Y, because under these assumptions, expected value of y is better.

461
01:07:34,938 --> 01:07:38,028
Okay. So these are the two errors. And what about the.

462
01:07:40,938 --> 01:07:45,468
I guess he does. The residuals are the estimated errors.

463
01:07:46,518 --> 01:08:10,488
So the counterparts of each of these one victims for Epsilon Hap or the residuals, the estimated errors then are essentially y minus Y effect.

464
01:08:10,968 --> 01:08:14,478
You know, this is again, this is a review. It's nothing new.

465
01:08:15,078 --> 01:08:18,558
And I know that y you had I can write it as eight times y,

466
01:08:18,568 --> 01:08:25,938
but it is a is the projection matrix or in other words, I can write it as I minus eight times y.

467
01:08:27,648 --> 01:08:33,348
Okay. So what is expected value of epsilon hat and expected value of the residuals is

468
01:08:33,768 --> 01:08:41,328
I minus eight times expected value of y or in other words I minus h times x.

469
01:08:41,328 --> 01:08:49,618
Peter comes from here. So you can write it as exhibit minus eight x beta.

470
01:08:51,088 --> 01:08:57,238
What is 8x8 is a production matrix, so each x is equal to x.

471
01:09:04,358 --> 01:09:12,088
You can also write the full form of h h is x extractable vaccine burst x transpose time

472
01:09:12,158 --> 01:09:18,878
so it's better for these canceled your left exhibit than minus x but are equal to zero.

473
01:09:20,108 --> 01:09:29,258
So the agents have a folder that I see.

474
01:09:29,258 --> 01:09:40,928
Do others have means zero? One assumption that I need here implicitly is that going from here to here expected value

475
01:09:40,928 --> 01:09:48,817
of y equal to expect that I assume that you know the the correct model has been fitted.

476
01:09:48,818 --> 01:09:49,178
Right.

477
01:09:50,768 --> 01:10:02,108
So that's an implicit assumption that I have made and just wanted to point that out that I have fitted the correct model, correct functional form.

478
01:10:03,338 --> 01:10:07,658
And this is where you will see how the assumptions are kind of connected.

479
01:10:08,528 --> 01:10:13,868
Okay. So the residuals have mean zero, but what about the variance?

480
01:10:13,868 --> 01:10:29,138
The variance of Epsilon hat is variance of I minus six times y and we know that variance of meade breaks e times multivariate.

481
01:10:29,558 --> 01:10:36,158
Why is a variance of y times eight transpose.

482
01:10:38,648 --> 01:10:51,637
Right. So I applied that result from module D and I did this I minus eight Sigma Square dance dive into matrix time

483
01:10:51,638 --> 01:11:00,488
phi minus each transpose so this gives me sigma squared pants I minus eight square eight is symmetric.

484
01:11:00,488 --> 01:11:11,318
I been 14. And you showed we showed that I minus eight is also symmetric eight important.

485
01:11:14,108 --> 01:11:22,028
So I have variance of the Epsilon hat, the sigma four times identically minus H.

486
01:11:22,598 --> 01:11:33,278
Therefore, the residuals Epsilon hat have a normal distribution with multivariate normal with mean zero and variance for within matrix.

487
01:11:33,668 --> 01:11:38,468
Sigma squared times the identity minus h matrix.

488
01:11:40,568 --> 01:11:43,957
Why does Epsilon have a normal distribution?

489
01:11:43,958 --> 01:11:47,488
Because Epsilon Act is a linear combination of normals.

490
01:11:48,188 --> 01:11:53,668
It's y minus y that are not all of I minus eight times y.

491
01:11:53,678 --> 01:11:55,688
So it's a linear combination of normal.

492
01:11:56,048 --> 01:12:02,888
So we did normal and we have just shown that the -0 and the radians forward is the frequency must contain minus Z.

493
01:12:05,408 --> 01:12:08,828
Okay. Any questions? So far? This is all reviewed.

494
01:12:10,098 --> 01:12:15,908
So now if I ask you this. So we started with the two errors.

495
01:12:15,938 --> 01:12:27,908
The two error could mean zero, and the two errors also have a constant variance sigma squared times the identity matrix,

496
01:12:27,908 --> 01:12:31,118
constant variance, and the two errors are independent.

497
01:12:32,288 --> 01:12:36,668
What about the residuals? The residuals have mean zero.

498
01:12:36,698 --> 01:12:44,158
Are the residuals independent? Are the residuals independent?

499
01:12:44,368 --> 01:12:52,918
The true Arabs are independent and the residuals independent. No, I know.

500
01:12:58,128 --> 01:13:07,098
Yeah. He said, if we can get everybody here that institutions are not independent.

501
01:13:07,118 --> 01:13:13,738
Because, you know, I it's true that the media's coverage to see most of the times I didn't think they might in this age.

502
01:13:14,248 --> 01:13:27,598
What are the what are the, uh, uh, of definite elements of this matrix or what is poverty in Ypsilanti hat and Ypsilanti hat?

503
01:13:28,588 --> 01:13:33,338
But I'm not equal to them based on this matrix.

504
01:13:33,358 --> 01:13:42,708
It's minus sigma squared HIIT, but it aids the idea diagonal them.

505
01:13:42,838 --> 01:13:47,398
I did element of the had matrix of the each matrix.

506
01:13:49,048 --> 01:13:59,368
So, you know, it doesn't necessarily have to be zero. Um, what about the, the, do the residuals have constant variance?

507
01:14:03,538 --> 01:14:07,108
The A-roads have constant variance to the residuals of constant variance.

508
01:14:09,688 --> 01:14:14,388
So. But I think there are variances to say.

509
01:14:16,538 --> 01:14:27,128
Yeah. Yeah. So I would have thought everybody saw this slide and would be jumping and saying, no, no, no, but why am I not hearing that?

510
01:14:28,868 --> 01:14:36,248
What is the variance of epsilon I had based on this?

511
01:14:39,298 --> 01:14:44,868
It's Sigma Square one minus eight.

512
01:14:44,998 --> 01:14:48,468
I. Right.

513
01:14:51,228 --> 01:15:08,268
So the variance of the residuals depend on the diagonal elements of this production matrix, so they're not necessarily going to be the same for all.

514
01:15:10,818 --> 01:15:14,238
So the residuals do not have constant gradients.

515
01:15:16,518 --> 01:15:29,658
Okay. So summing up LA, if the two areas are normal, then the residuals are also normally distributed.

516
01:15:29,808 --> 01:15:34,908
If the curative means zero, then the residuals also have meaning zero.

517
01:15:34,908 --> 01:15:43,218
But even if the two areas are mutually independent, which is what we assume, even then,

518
01:15:43,548 --> 01:15:59,748
the residuals are not being independent because of the fact that Corbin Simpson and I had accidentally had is going to be equal minus three M.D.

519
01:15:59,748 --> 01:16:05,228
JJ And based on the readings,

520
01:16:05,238 --> 01:16:17,748
Paul begins with this expression in the previous slide and and there's no guarantee that DJT is zero or high, all a multiplicity.

521
01:16:18,348 --> 01:16:25,398
And then for this, even if the variance of the two arrows is constant for only the beginning of the

522
01:16:25,398 --> 01:16:33,797
residuals is not it's not equal to sigma squared for any in fact variance of epsilon.

523
01:16:33,798 --> 01:16:48,078
I had these as we saw sigma squared times one minus h i and since h is a positive definite matrix, AGI is greater than equal to zero.

524
01:16:50,518 --> 01:16:54,508
Good. So. So that stuff.

525
01:16:54,748 --> 01:17:00,148
Those are the properties of the residuals. And very important slide.

526
01:17:08,208 --> 01:17:17,808
Okay. So now I'm going to talk about some other versions of procedures.

527
01:17:17,808 --> 01:17:26,368
So the first thing that I'm going to talk about is a standardized version of these residuals.

528
01:17:26,548 --> 01:17:41,478
So this is basically now I think that all residuals epsilon I had and I standardize and there are residuals by dividing by sigma have.

529
01:17:46,118 --> 01:17:59,347
So the Z are iPads are Epsilon iPod divided by Sigma hat and the national is that the audacity was epsilon I had said even a dependent on.

530
01:17:59,348 --> 01:18:06,818
In other words if you change the unit supply then that would change the residuals also.

531
01:18:07,328 --> 01:18:21,307
So the division by sigma hat standardizes this no residuals and kind of brings them in the same scale across even if there is a change of that.

532
01:18:21,308 --> 01:18:25,598
Units standardizing with Sigma hat is very easy to do.

533
01:18:27,248 --> 01:18:37,807
And what will happen is, again, from a diagnostic point of view, like if you are looking at factors in the no residuals and Epsilon,

534
01:18:37,808 --> 01:18:46,568
I had more of those the fact ones will also get reflected in the Z I in the standardized residuals.

535
01:18:47,078 --> 01:19:00,758
But it might it could be easier to charge the sort of the the back of the magnitude because now these are standardized.

536
01:19:03,158 --> 01:19:11,308
So that's the whole idea of standardizing the residuals with respect to signature.

537
01:19:13,298 --> 01:19:22,238
However, I would argue that this is kind of an imperfect version.

538
01:19:22,238 --> 01:19:31,958
It's, it's, it's it's better than the residual because it is a standardized version, but it is still probably not good enough.

539
01:19:32,408 --> 01:19:39,457
Why is it not good enough? And I would argue the reason why it's not good enough, because just in the previous slide,

540
01:19:39,458 --> 01:19:50,708
we saw that the variance of Epsilon I had is actually this one quantity here, one minus the GI Times Sigma squared.

541
01:19:51,158 --> 01:20:00,578
So if I am standardizing using an estimate there for Sigma, it's kind of an imperfect standardization.

542
01:20:00,608 --> 01:20:13,748
It would be better if I could divide by the standard data or the standard deviation of the residuals.

543
01:20:14,678 --> 01:20:26,198
And the standard deviation by standard deviation of the residuals means what I have to incorporate the I'm one minus each area multiplied also again.

544
01:20:27,338 --> 01:20:37,168
So just keep that part. Let's talk a little bit more about the standardized residuals, the apex.

545
01:20:37,448 --> 01:20:50,048
Before we go to a bigger standardization, a better version of the given, given that the standardization is somewhat imperfect here.

546
01:20:50,438 --> 01:21:00,098
So the Z I had some have have an approximate mean of zero and an approximate me variance of one.

547
01:21:01,538 --> 01:21:12,248
If the model assumptions are correct, then z i head has an approximate normal distribution standard.

548
01:21:12,248 --> 01:21:15,548
Normal normal which means it obedience one.

549
01:21:17,108 --> 01:21:27,488
And that's the reason why the Z hats are sort of in the in the sort of diagnostics

550
01:21:27,488 --> 01:21:35,527
word that the hats are often looked at because they're pretty easy to interpret.

551
01:21:35,528 --> 01:21:48,468
But I mean, if they're approximately like standard normal, I can actually use normal distribution model to assess the,

552
01:21:48,708 --> 01:21:56,498
the magnitude of the Z hat and to examine if there are certain values,

553
01:21:56,678 --> 01:22:06,668
certain residual values that are too large, too large meaning like, you know, the foreign correspondent to do the observations.

554
01:22:08,048 --> 01:22:18,218
To do the observations. The basic rules are really large, meaning that somehow that observation did not fit well.

555
01:22:20,438 --> 01:22:24,698
For the morning. Well, guess what? In other words. And there's this.

556
01:22:25,838 --> 01:22:29,258
Implicitly. And we are going to talk more formally about outliers.

557
01:22:29,798 --> 01:22:37,428
Implicitly, this depends if we are really ECB of assessing outliers.

558
01:22:37,448 --> 01:22:40,638
So that's the advantage of the I we have.

559
01:22:40,638 --> 01:22:47,228
Or were the lower residuals Epsilon had is that we have a rule of thumb to identify large value.

560
01:22:47,468 --> 01:22:50,888
Oftentimes you might I mean, you may have even done it yourself.

561
01:22:50,888 --> 01:22:54,818
You might have said, oh, is it greater than two or 3 seconds between two?

562
01:22:55,658 --> 01:23:01,628
Yes. Then then, you know, that's an alpha. And the reason why that where does that to come from?

563
01:23:01,928 --> 01:23:12,818
That to come from the fact that, you know, it's 1.96 is the, you know, 95th percentile of normal zero one distribution.

564
01:23:13,028 --> 01:23:18,368
And the Z hats are approximately standard normative.

565
01:23:18,368 --> 01:23:27,498
That's relatively possible. I'm just going to close with one statement and then I'll take your question.

566
01:23:27,768 --> 01:23:31,968
So the other thing that you can do is because these are readily interpretable,

567
01:23:31,968 --> 01:23:36,978
you can construct histograms one belt, one road blocks, box whisker plots.

568
01:23:37,788 --> 01:23:42,598
All of these will have the same pattern as the law as you do else.

569
01:23:42,618 --> 01:23:47,088
And we will introduce some of these diagnostic plots later on.

570
01:23:47,478 --> 01:23:54,038
But the idea is you can use either the raw residuals or the standard solution.

571
01:23:54,288 --> 01:23:59,438
So my question is so. He's going to be testing the.

572
01:24:00,158 --> 01:24:06,687
Because. What do you mean?

573
01:24:06,688 --> 01:24:14,528
Many different points like. Yeah.

574
01:24:15,958 --> 01:24:19,438
So let's say that. We need to look out.

575
01:24:20,028 --> 01:24:23,298
To be tested for multiple testing.

576
01:24:23,658 --> 01:24:29,308
You are using the term loaded all you want to prove testing and statistics before something big.

577
01:24:32,688 --> 01:24:33,828
I won't go into that,

578
01:24:33,948 --> 01:24:43,908
but I think what you are asking is the joint influence of those three or four points versus each point we and one at find time, right?

579
01:24:45,478 --> 01:24:50,938
As well as conducting four tests to see if you are not doing any tests at all.

580
01:24:52,358 --> 01:24:58,738
You're not doing any tests here. No, no, no, no, no.

581
01:24:58,748 --> 01:25:05,818
That, that, that's why I sort of give the PM hope that you've seen the wheel of Diagnostics.

582
01:25:08,428 --> 01:25:11,067
Most of the time there is no statistical test.

583
01:25:11,068 --> 01:25:20,998
It's long since sort of these are approximate distributions and these are just giving you rules of thumb so there's no statistical test.

584
01:25:21,268 --> 01:25:27,508
And by the way, I don't want to use up class time to go into multiple testing,

585
01:25:27,508 --> 01:25:32,697
but that's a that's a very long time in a lot in this context because like in the testing,

586
01:25:32,698 --> 01:25:40,528
there is a big literature on what people testing in split statistics, but that refers to completely something completely different.

587
01:25:41,398 --> 01:25:45,328
We'll get to just just like a site aside, you know.

588
01:25:45,748 --> 01:26:36,948
Any other questions? Yes. Um, this is an approximate distribution.

589
01:26:38,388 --> 01:26:42,978
So for your part, as I said, this is an imperfect standardization.

590
01:26:42,978 --> 01:26:46,668
So hold your phone and see what's coming next. Okay.

591
01:26:50,728 --> 01:26:54,388
Okay. So following up on that question, you know,

592
01:26:54,568 --> 01:27:03,328
there was concern that Sigma had would be influenced by because Sigma had this build up of the residual site.

593
01:27:03,748 --> 01:27:08,038
So Sigma had would be influenced by this Epsilon apex.

594
01:27:09,148 --> 01:27:12,148
And we already said it's an imperfect standardization.

595
01:27:12,148 --> 01:27:15,198
So let's see what we can do.

596
01:27:15,208 --> 01:27:20,748
Can we do better? And here is a better version.

597
01:27:20,758 --> 01:27:27,687
Here is a bigger standardization. These are called internally digested words.

598
01:27:27,688 --> 01:27:37,258
And the idea is this recall that Sigma had square not not sigma had squared,

599
01:27:37,258 --> 01:27:44,578
but Sigma had squared times one minus area is an unbiased estimate of the variance of epsilon impact.

600
01:27:45,568 --> 01:27:52,668
Right? So the correct variance of excellent I had is not sigma squared, but it's sigma squared times one minus H.

601
01:27:53,728 --> 01:28:01,138
So the standardization or the deviation by just Sigma Square final for an imperfect standardization.

602
01:28:01,678 --> 01:28:06,688
So let's let's look at a better version.

603
01:28:06,698 --> 01:28:13,828
So this is called an internally student residual r I have is equal to epsilon I had.

604
01:28:14,338 --> 01:28:19,438
Now I divide by sigma had time square root of one minus h ii.

605
01:28:21,988 --> 01:28:27,777
So this is the chronic variance of epsilon impact or standard deviation of epsilon?

606
01:28:27,778 --> 01:28:38,818
I have. And what is HLA? HLA is the die the the diagonal I add diagonal element of the H matrix, the prediction matrix.

607
01:28:39,928 --> 01:28:47,518
This has a term H.I and we will talk about it in mean much more detail later on.

608
01:28:47,938 --> 01:28:56,278
But this had the name the r u diagonal element of the projection matrix is called the leverage for the IAP.

609
01:28:56,278 --> 01:29:06,628
Individual leverage is the name we give to the I of diagonal element of the production matrix,

610
01:29:06,628 --> 01:29:15,267
the hat matrix and the HHI is basically to write it in matrix notation.

611
01:29:15,268 --> 01:29:28,268
It's Excel transpose extract the vaccine bars excite as a special case if you want feel if you do kind of the algebraic simplification of this HHI,

612
01:29:28,708 --> 01:29:40,048
then in the simple linear regression model it reduces to this expression area is equal to one over in say minus x bar squared divided by SSX.

613
01:29:41,188 --> 01:29:49,408
That's the that's the other big simplification you can get being the simple linear regression situation.

614
01:29:49,648 --> 01:29:53,668
So now let's stare at this expression for a for a minute.

615
01:29:53,938 --> 01:29:57,568
It's easier to understand this in two dimensions.

616
01:29:57,678 --> 01:30:05,388
In this scenario, it's difficult to understand in in higher dimensions.

617
01:30:06,028 --> 01:30:08,188
And what, what, what am I talking about?

618
01:30:08,608 --> 01:30:23,578
So if you stare at this expression, you will see that the the leverage that each eye is tells us how far its side is from X bar.

619
01:30:26,308 --> 01:30:35,638
Right. How far the audit subject's full bodied value is from the mean for value.

620
01:30:36,508 --> 01:30:45,568
The points that are far from the spa have the potential to pool the best pipeline towards them.

621
01:30:47,458 --> 01:30:59,038
Therefore, making the epsilon i head smaller than what would be if the point had been excluded from the regression.

622
01:31:01,308 --> 01:31:05,528
So here is a picture and is going to. So here it is.

623
01:31:05,538 --> 01:31:09,698
Let's see the scatterplot of why versus it.

624
01:31:10,188 --> 01:31:16,818
And I have a point I have a point like this.

625
01:31:17,778 --> 01:31:31,517
Another point. So if this point is included in the regression, then this one will pull it.

626
01:31:31,518 --> 01:31:39,298
This point is farther from the x bar. It's outlined in the X space.

627
01:31:39,598 --> 01:31:45,418
It will pull the straight line towards it says.

628
01:31:48,508 --> 01:31:58,048
Kind of exert some point. If I did not have this point, then actually the best fit would look something like this.

629
01:32:04,448 --> 01:32:12,607
Okay. So how much and you know, by virtue of this point e being included in the regression,

630
01:32:12,608 --> 01:32:32,138
what would happen is it would actually not make the and that epsilon I had for himself or herself, it would make the epsilon I had smaller than.

631
01:32:35,428 --> 01:32:38,458
Then what it would be if.

632
01:32:47,278 --> 01:32:51,058
O the point had been excluded from the revision.

633
01:33:01,418 --> 01:33:05,347
So in other words, coming back to that point is like the outline,

634
01:33:05,348 --> 01:33:13,758
this off point sort of effects its own residual, which in turn impacts the seed might have.

635
01:33:15,038 --> 01:33:26,348
But nevertheless, this is a better standardization than when we divided just by Sigma had some properties of this internally some diversity.

636
01:33:26,588 --> 01:33:37,868
So the mean of these are you had to zero the variance is approximately one and

637
01:33:39,158 --> 01:33:46,418
the right heads having to have a have a distribution again because of now the

638
01:33:46,418 --> 01:33:53,797
standardization is with respect to the correct variance has a distribution with degrees

639
01:33:53,798 --> 01:33:57,728
of freedom equal to a C I think this is this is the point you're going to make.

640
01:33:57,968 --> 01:34:05,168
Right. Okay. So so that does it clear up now, like why this is a better standardization.

641
01:34:05,288 --> 01:34:17,108
That's exact. But still, when the degrees of freedom or the sample size is large, you can approximate the P with the standard, but the normal 2 to 1.

642
01:34:19,148 --> 01:34:21,847
And if it's moderate or small sample size,

643
01:34:21,848 --> 01:34:31,088
then you see the bottom line is in nothing in a lot of these discussions about diagnostics and you will see that we use approximate distribution,

644
01:34:31,598 --> 01:34:37,538
we use approximate knock offs. And the reason being that, you know, this, there's no formal test here.

645
01:34:38,108 --> 01:34:47,978
We are using these as rules of thumb. So that's why using the normal course make a whole lot of sense and what might they be deemed applicable?

646
01:34:47,978 --> 01:34:53,248
You can kind of close your eyes. You don't need to look at to be able to say, okay, sample,

647
01:34:53,258 --> 01:34:59,528
so glad I can approximate the P with the normal and I'm just going to look at plus minus two as my.

648
01:35:00,888 --> 01:35:03,928
Okay. So that's the idea.

649
01:35:04,008 --> 01:35:11,437
You had a question posed. How was the. I'm not going to pull that result.

650
01:35:11,438 --> 01:35:21,248
But think about it this way. You are epsilon Ipek on the top and on the bottom you have the variants of thought ingredients of Brexit.

651
01:35:21,368 --> 01:35:34,208
I that. This, like it's approximately one to the hands of four or five that is equal to the variance of that ratio,

652
01:35:36,398 --> 01:35:44,318
readings of that ratio and the variance of the call is the bottom of that's the displacement.

653
01:35:44,648 --> 01:35:48,008
That's the reason why it's not exactly.

654
01:35:48,008 --> 01:35:51,398
One is on the bottom of the sigma part of the speaker.

655
01:35:53,318 --> 01:35:56,798
Okay. So that's that's the reason. Okay.

656
01:35:58,688 --> 01:36:05,168
You know, it's it's a it's a few lines of proof, but you can even kind of mentally do it.

657
01:36:05,708 --> 01:36:14,978
And then lastly, uh, the, there is another version of the residuals.

658
01:36:14,978 --> 01:36:17,288
This is all externally standardized.

659
01:36:18,008 --> 01:36:32,318
And the idea is exactly as I was saying, that, you know, if you have a guy like this that, you know, outlined this in the X piece,

660
01:36:32,558 --> 01:36:41,558
sort of pulls the line towards C more hard and makes the Epsilon eight head smaller than what it would have been if the point were excluded.

661
01:36:41,888 --> 01:36:57,278
So in a way, this person is masking his or her own outline this by being there and by impacting the Sigma hat so much better.

662
01:36:57,668 --> 01:37:02,408
Again, another better version is the externally standardized residual,

663
01:37:02,768 --> 01:37:08,648
where again the expression is still the same epsilon I had divided by the standardization,

664
01:37:08,648 --> 01:37:20,198
but instead of using sigma ahead from the full regression, what you do is you use sigma hat from a regression that excludes the individual.

665
01:37:24,688 --> 01:37:28,168
I think I'm going to just because people may have to.

666
01:37:28,298 --> 01:37:38,188
So it excludes the individual. And the idea is this is also called leave one out or Jackknife Residuals.

667
01:37:38,578 --> 01:37:44,217
And it's basically saying that if we do regression without the individual,

668
01:37:44,218 --> 01:37:53,878
we go and then estimate the sigma from there that we that is what I put in the denominator for the.

669
01:37:54,358 --> 01:38:01,428
And I without I have the sequences are externally standardized and implicitly what are you doing

670
01:38:01,438 --> 01:38:08,008
you are actually fitting in the sample sizes and you are actually freaking in revision each time,

671
01:38:08,668 --> 01:38:14,638
deleting one observation, leaving the parts one second one out, followed by another, and so on.

672
01:38:15,208 --> 01:38:26,308
That's what is happening. And the reason why you are doing that is basically you are removing the influence of Epsilon.

673
01:38:26,308 --> 01:38:29,428
I had done the sigma squared by removing the data point.

674
01:38:29,428 --> 01:38:33,998
Exactly because of. This graph.

675
01:38:35,678 --> 01:38:41,858
Okay. So I think I need to stop here and continue from.

676
01:38:47,958 --> 01:38:58,608
Yeah. I think that's all I wanted to say about the externally sanitized residuals and basically leave it at that.

677
01:38:59,268 --> 01:39:10,448
And then we will essentially go to the the next model thing.

678
01:39:10,608 --> 01:39:18,918
The only maybe just one other point I would make is like we talked about this for different types of residuals for large samples.

679
01:39:19,098 --> 01:39:27,948
They would all provide the same information and they will also approximately you can sort of calibrated this they to a normal.

680
01:39:28,518 --> 01:39:43,097
So that simply doesn't matter what you use but for doing that moderate samples we can make a difference and the the residual that is most sensitive is

681
01:39:43,098 --> 01:39:53,808
the externally standardized because it kind of speaks of the impact of the outline of a certain observation and how we can get something like that.

682
01:39:54,118 --> 01:40:06,768
So just in closing, in most practical situations, it wouldn't matter which party you if the sample sizes too small or you know,

683
01:40:06,788 --> 01:40:12,588
like in the tens twenties, then you would see a difference.

684
01:40:13,248 --> 01:40:21,048
But otherwise you. So I'm going to sort of stop here and pick up and just thank you.

685
01:40:21,678 --> 01:40:50,428
Do send me questions if you have by sending. He just said.

686
01:40:53,638 --> 01:41:00,928
That's. Is it just.

