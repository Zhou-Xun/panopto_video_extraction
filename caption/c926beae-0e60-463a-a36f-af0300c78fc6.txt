1
00:00:05,430 --> 00:00:20,840
Okay. So, Bob, if you remember we were talking about basically I was giving you an overview of the topics that we will learn in 654.

2
00:00:20,940 --> 00:00:28,409
And we started with the simple linear regression model, which is basically the framework,

3
00:00:28,410 --> 00:00:36,450
is that the response variable Y is continuous and has a normal distribution.

4
00:00:36,450 --> 00:00:48,569
We'll talk about that in more detail later. And there is a single core variable X, so we introduce that model and I mentioned that the whole body,

5
00:00:48,570 --> 00:00:56,280
it can have any measurement scale, it can be continuous, it can be categorical, it can be nominal, ordinal, anything.

6
00:00:56,280 --> 00:01:02,010
So we talked about the simple linear regression model, then the R v dimension,

7
00:01:02,400 --> 00:01:12,389
because this is more of an overview of what's coming in 650 and then we talked about how we are going to extend that SLR model where we are

8
00:01:12,390 --> 00:01:23,790
bringing in multiple more than one covariance and it's the multiple linear regression model or the multivariable linear regression model.

9
00:01:24,180 --> 00:01:37,230
And of course it's much more flexible because now you can include several covariates, several predictors at a time simultaneously in the model.

10
00:01:37,620 --> 00:01:40,830
But at the same time it is also more complicated.

11
00:01:41,430 --> 00:01:44,270
Then we see that, well,

12
00:01:44,400 --> 00:01:56,130
there are some basic assumptions that underlie a linear regression model and these are with regards to the error term, Ypsilanti.

13
00:01:56,820 --> 00:02:07,030
And we talked about like how failure of any of these assumptions could seriously invalidate the,

14
00:02:07,350 --> 00:02:12,270
the, you know, various aspects of the analysis and influence the results.

15
00:02:12,870 --> 00:02:20,939
So we are going to also in this course talk about how to check for the adequacy of those assumptions.

16
00:02:20,940 --> 00:02:27,330
And these it's a it's a very important topic in regression.

17
00:02:27,720 --> 00:02:34,650
And typically the checking the adequacy of the assumptions is done through what are called residuals.

18
00:02:35,040 --> 00:02:43,199
So the residuals are estimated errors. And based on the model three, we are going to obtain the residuals.

19
00:02:43,200 --> 00:02:47,370
And based on these residuals, we are going to check the adequacy of the model fit.

20
00:02:48,060 --> 00:02:50,400
So this is a very important topic that you will see.

21
00:02:50,400 --> 00:03:00,270
We spend a lot of time talking about residual diagnostics and the objective is to check the validity of the assumptions after after model thinking.

22
00:03:00,270 --> 00:03:07,679
Usually this leads to kind of a iterative process and one one to do the residual diagnostics, check the adequacy.

23
00:03:07,680 --> 00:03:16,889
If the assumptions seem to be not satisfied, then you go back and do something, make some fixes,

24
00:03:16,890 --> 00:03:23,430
make some changes in the model, recreate the model, and then come back and look at the residuals again.

25
00:03:23,430 --> 00:03:35,010
And so it's kind of an iterative process. So these are all topics that we will come across or we will discuss.

26
00:03:35,640 --> 00:03:48,420
So the next one that we are going to see in the course we are going to spend quite a lot of time talking about is transformations.

27
00:03:49,530 --> 00:03:55,290
So as I mentioned, that is. Can you?

28
00:03:55,320 --> 00:03:59,890
I cannot hear you. It's better. In the.

29
00:04:05,610 --> 00:04:24,940
Oh. Sorry. Sorry. So the the second assumption, the symbol here is in case you are not familiar with the symbol, this is orthogonal.

30
00:04:24,960 --> 00:04:31,200
So Epsilon II is orthogonal to epsilon d for all i d i not equal to g.

31
00:04:31,530 --> 00:04:41,099
So when we talk about the model formally, we will we will mention what what precisely this assumption means.

32
00:04:41,100 --> 00:04:47,850
But for the time being, often banality of the error means that the error so independent or in other words,

33
00:04:48,090 --> 00:04:53,160
that in plain English what we are saying is that if we have two individuals,

34
00:04:53,730 --> 00:05:02,850
two subjects, I ended the error these, these the, the sum of the outcomes and the errors from these individuals are independent.

35
00:05:03,570 --> 00:05:07,350
So that's the kind of plain English interpretation of this disaster.

36
00:05:07,620 --> 00:05:11,370
But thank you for asking that question. Everybody clear.

37
00:05:12,120 --> 00:05:19,139
Okay. So we are going to spend quite some time.

38
00:05:19,140 --> 00:05:28,480
Yes. Is there a difference between independence? We come to that so often.

39
00:05:29,210 --> 00:05:33,220
Generally, we talk about the I do a quick answer on that.

40
00:05:33,550 --> 00:05:40,960
So there is going to be a drop in the bucket of criminality of helping people sort of project in the country.

41
00:05:42,530 --> 00:05:47,260
Accidents are random in our state, let me tell you.

42
00:05:47,360 --> 00:05:58,920
So so here we are talking about sort of random variable, Fremont, Pomona, which is basically the and of the desert.

43
00:05:59,800 --> 00:06:10,090
So so for far for the for the people like this class, when we are talking about if we go back to the victims,

44
00:06:10,090 --> 00:06:23,020
which is the usual nomenclature here, we are coupling the opportunity for the murder of the random people who think that being defendants.

45
00:06:25,660 --> 00:06:39,280
Okay. Any other questions? So we are going to spend time talking about transformations.

46
00:06:39,280 --> 00:06:48,129
Remember, as I mentioned, that once we check the adequacy of the assumptions based on residuals and surveys,

47
00:06:48,130 --> 00:06:57,760
find that certain assumptions seem like they may not be valid in the context of our data.

48
00:06:58,000 --> 00:07:09,069
And and at this point, Tom and I may be sounding like, you know, sort of vague because we haven't really talked about what these assumptions are.

49
00:07:09,070 --> 00:07:11,230
We haven't been talked about how we check.

50
00:07:11,710 --> 00:07:27,880
But but assume that based on our residual diagnostics, we have found that some of the assumptions may not forward in the context of our data.

51
00:07:28,240 --> 00:07:33,730
So as I mentioned, we will be generally what do we do? We go back, we do some fixes to the model.

52
00:07:34,360 --> 00:07:43,390
We do underestimate diagnostics again. And then we kind of like, well, you know, like when, but it's kind of an interpretive process.

53
00:07:44,290 --> 00:07:55,839
So one of the things that often comes to help is taking transformation of either the outcome or the whole body.

54
00:07:55,840 --> 00:08:03,969
It really will talk about both. So what does transformation of the outcome?

55
00:08:03,970 --> 00:08:10,330
How does it look like? So consider the following simple linear regression model Y equal to we do not.

56
00:08:10,340 --> 00:08:18,850
Plus we go on its side plus Epsilon nine. So once again, you know the same for the interpretations that we had laid out before.

57
00:08:19,630 --> 00:08:30,100
So the me, you know, like what is the sort of the premise of this model that Y is linearly related to X squared,

58
00:08:30,730 --> 00:08:47,920
but it is possible that instead of Y being linearly related to x, there is some function of y chf y that is linearly related to x.

59
00:08:47,920 --> 00:09:00,190
So instead of y being linear with x, this is a function of y that is linearly related to x, so so that.

60
00:09:00,190 --> 00:09:08,709
So for example, you know, one, one such instance will be each of y equals to be another plus to go on X plus Epsilon II.

61
00:09:08,710 --> 00:09:21,070
And there are several possible choices of each which will work in your case kind of will depend on again the data applied and then sort of,

62
00:09:21,070 --> 00:09:33,460
you know, the distribution and what the residual diagnostics tell us in terms of what was what what specific assumptions seem to be invalidated.

63
00:09:33,700 --> 00:09:45,999
So some possible choices of y that of h that often come coming to rescue are natural logarithm of y.

64
00:09:46,000 --> 00:09:57,610
I should have written ellen of y law b strain of y square root of y and one over y so like, you know, the reciprocal of y.

65
00:09:57,610 --> 00:10:05,920
So these are some very commonly implied transformations that I know that coming to rescue.

66
00:10:06,190 --> 00:10:11,740
And when we are in to this section or in this module, we talk about transformations.

67
00:10:12,130 --> 00:10:19,300
We will discuss in what context, which choice would be the best serve.

68
00:10:21,610 --> 00:10:33,460
But often times, you know, sort of the model refinement or the model sort of updating the thumb is done based on fast formation.

69
00:10:36,070 --> 00:10:46,760
We can also do transformations of the gradient. So here is again the simple linear regression model Y equal to better not excite.

70
00:10:46,830 --> 00:10:54,850
Plus now we have an extra quadratic Dominick's so beta to exercise squared plus epsilon i.

71
00:10:57,450 --> 00:11:05,969
So what's the difference between this model and the simple linear regression model that we had written earlier?

72
00:11:05,970 --> 00:11:18,560
And I don't know where the styluses. So.

73
00:11:20,270 --> 00:11:35,780
So we had this modern. So now what we have done is we have added a term like meter to Excise Square.

74
00:11:35,830 --> 00:11:43,810
So like the one he is. Is this a linear model?

75
00:11:47,730 --> 00:11:52,500
Yes. Why did the lunar module stolen urine?

76
00:11:53,250 --> 00:12:03,270
Exactly. It is delineating the perimeter. So this is a really, really important concept that you also sort of theodicy.

77
00:12:03,630 --> 00:12:14,370
So when we talk about linear regression, linear and model, the linearity refers to the linearity of the parametres or the force assurance.

78
00:12:15,630 --> 00:12:21,300
So how the politicians enter into the model, that has to be linear.

79
00:12:22,470 --> 00:12:36,210
We are adding up what, be 30 minutes but it is still 11 model because the and the parameters the fact it is still linear in the parameters.

80
00:12:37,230 --> 00:12:45,930
So that's a very important concept and that you should be aware of.

81
00:12:46,140 --> 00:12:53,650
Yeah. So I'm actually going. Expire next spring, specifically when those two are lost to.

82
00:12:55,570 --> 00:12:59,100
I for. So. Yeah, so.

83
00:13:00,140 --> 00:13:03,960
So let's we will talk about it.

84
00:13:03,990 --> 00:13:15,170
I mean, 2 to 3 sides down. Coming up, we will talk about sort of, you know, all the of the four wickets in the morning.

85
00:13:15,180 --> 00:13:18,569
The general point, if you are in the right, it's a it's a square.

86
00:13:18,570 --> 00:13:22,440
You know, it's it's not an exact function of site.

87
00:13:23,160 --> 00:13:29,400
But again, you will see, like when we look at the scatter plots, I mean, there are,

88
00:13:29,910 --> 00:13:36,330
you know, if this happens what looks like kind of an inverted elongated you,

89
00:13:36,900 --> 00:13:43,410
then basically what we are saying is that there are the recently in the company, but they've also acquired,

90
00:13:43,410 --> 00:13:54,570
for example, sort of scatter plots like those would be very, very available to like the body like this.

91
00:13:55,560 --> 00:14:02,690
Okay. So there the quality of the aspect, you know, kind of doesn't come into play.

92
00:14:02,700 --> 00:14:08,310
And we we talk about how do you have. In the Denver context.

93
00:14:09,750 --> 00:14:17,250
So the transfer flow, as you can see, the transformation of the board we need can all just as it can include a quadratic kinetics,

94
00:14:17,250 --> 00:14:22,590
it can also include, you know, log of x power suffix.

95
00:14:22,800 --> 00:14:35,640
So like, you know, excise credits IQ, which we call like in in general the polynomial so it can be a polynomial in x square root of x.

96
00:14:35,640 --> 00:14:50,190
And then finally lines. So lines are piecewise linear functions and the piecewise, you know,

97
00:14:50,190 --> 00:15:01,470
the definition of the piecewise defined functions is that there is a, there's a smoothness at the places where the there are,

98
00:15:01,650 --> 00:15:09,840
there are kind of like bricks or not these a here in this if we're in this subject expression,

99
00:15:10,140 --> 00:15:20,220
it mentions that there is a there is a split that e so you can think of and draw a picture and I don't know why can everybody see.

100
00:15:31,510 --> 00:15:35,560
So if it looks like there's been to.

101
00:15:40,000 --> 00:15:43,060
I mean, this is a school that.

102
00:15:51,420 --> 00:16:03,030
So this is called a slot slide because as you can see that the relationship between y in ex the

103
00:16:03,030 --> 00:16:17,730
smoothed solid line can be described using piecewise media sections joined at these points here.

104
00:16:18,090 --> 00:16:20,520
So these are called knots.

105
00:16:28,580 --> 00:16:44,450
So there is a piece from 0 to 8 and other pieces on A to B and then another piece from linear piece from B to C, wherever the range of the data is.

106
00:16:46,320 --> 00:16:58,290
Okay. So this is for discipline. And we could also have a transformation of the whole video that essentially involves slides.

107
00:16:58,680 --> 00:17:10,950
So we will talk about all of these different transformations in the context of both a simple and an A multiple linear regression model.

108
00:17:14,850 --> 00:17:25,079
One point of caution. You have to be very careful when using transformations and primarily with respect to the

109
00:17:25,080 --> 00:17:35,010
interpretation of the because the interpretation of the parameters changes completely.

110
00:17:38,370 --> 00:17:49,310
This happens. Because when you transform, why then?

111
00:17:52,470 --> 00:18:02,820
And then if you fit them all and then you kind of back transform to the original scale, it transforms.

112
00:18:02,820 --> 00:18:07,140
It sort of changes the assumptions regarding the error structure.

113
00:18:08,010 --> 00:18:13,320
So, for example, you have a longer log of y.

114
00:18:19,540 --> 00:18:24,069
Is equal. Do they do not love their money?

115
00:18:24,070 --> 00:18:34,360
Say, let's get to the night. So here we have the error.

116
00:18:34,600 --> 00:18:39,140
Epsilon i adjective in the lost you in everybody you see.

117
00:18:39,820 --> 00:18:44,790
Or is it? No, I don't know where the stylus finished.

118
00:18:48,570 --> 00:18:53,470
I think that all the. Oh.

119
00:18:55,300 --> 00:19:07,250
My gosh. Never mind. Okay.

120
00:19:08,660 --> 00:19:12,830
That's fine. But. So. So suppose we have this model.

121
00:19:24,320 --> 00:19:28,060
Suppose we have this wanted local foyer.

122
00:19:29,420 --> 00:19:32,660
This is natural law school is equal to better not.

123
00:19:33,680 --> 00:19:45,710
Does Beethoven excite less? It's so clear the air is additive in the locked skin.

124
00:19:59,480 --> 00:20:03,500
If you know, I'll bring it.

125
00:20:03,690 --> 00:20:08,630
So I should probably see, like, there's a step zero.

126
00:20:08,990 --> 00:20:13,210
So the the model was why you would be denied?

127
00:20:13,220 --> 00:20:16,970
Because they don't want to excite the sex in that time. If you did the model,

128
00:20:17,090 --> 00:20:27,410
we did residual diagnostics and we found that some is answer we we are not going to like the we haven't haven't

129
00:20:28,070 --> 00:20:34,459
there so at this point this you know sort of trust me when I say that based on the residual diagnostics,

130
00:20:34,460 --> 00:20:38,840
we find that a certain is unfairly invalidated.

131
00:20:39,770 --> 00:20:42,860
So we have to make the transportation. So that's what we did.

132
00:20:42,890 --> 00:20:51,490
We took the log of why that's the transformation we use and we we feed them on again.

133
00:20:51,500 --> 00:20:56,090
Everything is good, the rest does the good and we are happy with the model.

134
00:20:56,090 --> 00:20:59,600
So this is the model that we need to settle on.

135
00:21:00,410 --> 00:21:03,379
Now the question comes back really well,

136
00:21:03,380 --> 00:21:13,070
what do I mean by be careful about the interpretation now since this is the final model we are going to interpret based on this model,

137
00:21:13,070 --> 00:21:25,760
the parameters, right? So what happens now if you want to back transforming the original skin because let's say,

138
00:21:25,760 --> 00:21:35,270
you know, say like, let's see why is BMI an excuse or why is cholesterol lowering since BMI?

139
00:21:36,350 --> 00:21:48,260
And you were trying to make the inference about for every one unique increase in BMI, well, estrogen goes up by 14, such right by beta one.

140
00:21:49,910 --> 00:21:59,000
But you love it now. So now the model is fine and base, you know, according to the residual diagnosis.

141
00:21:59,240 --> 00:22:02,450
But now we want to make an interpretation of the betas.

142
00:22:02,960 --> 00:22:13,430
So in this case, the interpretation will be for every one unique increase in BMI law, cholesterol goes up.

143
00:22:13,430 --> 00:22:20,840
I'm just assuming like beta one is positive, lower cholesterol goes up by beta one units.

144
00:22:22,100 --> 00:22:25,069
Now suppose you want to say, well, you know, like lower cholesterol,

145
00:22:25,070 --> 00:22:37,070
I want to kind of back transforming the original scale and making friends based on what happens to cholesterol in its original units.

146
00:22:38,810 --> 00:22:44,540
So then you are bringing it back to the original SKU to the Y scale.

147
00:22:44,810 --> 00:22:55,280
So then you exponential both sides. What happens you have Y equal to exponential of this whole thing.

148
00:22:55,820 --> 00:23:09,680
So it basically becomes exponential beta, not times exponential beta one ixi times exponential of Ypsilanti.

149
00:23:12,000 --> 00:23:23,430
So now now you see what we have done or what what kind of came out of this that now the error becomes multiplicative in the natural skew.

150
00:23:41,500 --> 00:23:47,709
So we have to be careful about the interpretation because it's, you know,

151
00:23:47,710 --> 00:24:00,580
sort of the barometer interpretations change completely and also the transformations alter the assumption of the underlying structure.

152
00:24:00,580 --> 00:24:05,500
So just to keep these two things in mind, of course, you know, we have solutions for this,

153
00:24:05,500 --> 00:24:13,420
but I just want to make sure that you have you are aware and you would follow and of of this fact.

154
00:24:15,970 --> 00:24:20,560
Next, we are going to talk about outliers and influence diagnostics.

155
00:24:20,560 --> 00:24:35,470
So we have the fitted model. And as you can see now, I have full text on top of the Y and we've done not and we go on because based on what had done,

156
00:24:35,890 --> 00:24:39,040
we do not video on with unknown barometers.

157
00:24:48,510 --> 00:24:57,800
And Big Donald had big dough on hand on my estimates.

158
00:25:02,500 --> 00:25:09,909
I'm always going to denote that estimates by hats off to them and based on the estimates

159
00:25:09,910 --> 00:25:15,370
we do not have and we don't want the fact I bloody love them into the equation.

160
00:25:15,670 --> 00:25:23,350
I get why a hack which is the predicted value of the response for the AI itself.

161
00:25:23,620 --> 00:25:35,190
So that's the standard annotation for my via iPad are again, we are going to develop this much more in detail.

162
00:25:35,210 --> 00:25:38,440
So this is predicted response.

163
00:25:42,740 --> 00:25:46,880
Okay. So, um, so here is the fitted model.

164
00:25:47,600 --> 00:26:04,360
Now in this topic, when we come to our class in diagnostics, what we are going to ask ourselves is the object subject's data excite?

165
00:26:04,360 --> 00:26:16,219
Why? What impact does it have on the predicted value of of why?

166
00:26:16,220 --> 00:26:23,330
For the eye it's deterring other words, how much does the AI subject influence?

167
00:26:25,190 --> 00:26:34,720
The parameter estimates better not hack and beta one have another way of asking the same question only like.

168
00:26:36,320 --> 00:26:43,700
I fix this? By what amount would be the one to change the subject to delete it?

169
00:26:45,800 --> 00:26:50,750
These are all questions that needed to be said about that.

170
00:26:51,290 --> 00:26:54,709
Is there any influential subject,

171
00:26:54,710 --> 00:27:04,250
influential off the books and what does influential author we can make your own begin to develop this family when you go into the topic, is you.

172
00:27:04,280 --> 00:27:13,900
Is there an observation? You live a subject that heavily influences or unduly influenced the estimation of the benefits.

173
00:27:15,500 --> 00:27:18,920
Here is up and if it does, should it be removed?

174
00:27:19,250 --> 00:27:26,320
So here is a here is an example of. This is why this is X.

175
00:27:27,370 --> 00:27:31,210
And suppose here is the scarecrow.

176
00:27:32,350 --> 00:27:35,710
This is almost like a picture perfect scatterplot.

177
00:27:37,000 --> 00:27:43,180
Now here is a point in the scatterplot.

178
00:27:47,420 --> 00:28:04,740
So as you can see. This point is clearly the blue point is clearly different in summary.

179
00:28:06,200 --> 00:28:12,550
Right. The question is like, you know, it's it's it's far away.

180
00:28:12,560 --> 00:28:19,710
I should have made it even farther away from the, uh, you know, like the mean cloud of the,

181
00:28:19,730 --> 00:28:27,110
of the bonds or the, the sort of the Y scatter, y versus X scatter.

182
00:28:27,110 --> 00:28:37,250
And without this blue form, the best straight line for it would be this very easy.

183
00:28:38,180 --> 00:28:48,650
So the question now is this blue point should I included or excluded with the blue line?

184
00:28:49,700 --> 00:29:01,160
And does the regression line would sort of build a little bit towards this point?

185
00:29:02,330 --> 00:29:18,780
Correct. So as you can see, the question then becomes that blue point is influential because it is impacting the estimate of our estimate of beta one,

186
00:29:19,920 --> 00:29:30,800
one of the slope. The question is how do we firstly assisting first and how do we decide whether it's

187
00:29:31,550 --> 00:29:39,830
it's sort of benign or whether it is really unduly impacting the regression estimate.

188
00:29:41,210 --> 00:29:44,720
So how are these two knocked up? And in fact, how?

189
00:29:47,700 --> 00:29:51,300
When do we explore? When we still keep it?

190
00:29:51,720 --> 00:30:00,540
So these are some questions that we will tackle in the section on Outliers, and that stuck.

191
00:30:03,520 --> 00:30:13,060
Okay. But since this is a very, very interesting sort of topic that we will cover next.

192
00:30:16,440 --> 00:30:26,470
Is multiple linearity. So. There was a question just a few minutes back about bullying yet.

193
00:30:26,820 --> 00:30:40,010
We are also going to talk about linearity more in the in the regression context with multiple linearity and the sort of the idea is this,

194
00:30:40,940 --> 00:30:49,429
see, we have a linear regression model with covariates X minus two x3x up to excuse.

195
00:30:49,430 --> 00:30:58,940
Where Q is the number of whole variants or predictors we including the model to be with this is a multiple linear regression model.

196
00:30:58,940 --> 00:31:03,650
Q is greater than one.

197
00:31:05,840 --> 00:31:18,559
Okay. So the corporate are not assumed to be independent because we are measuring these from the same person like age,

198
00:31:18,560 --> 00:31:23,120
height, the letter, systolic blood pressure, diastolic blood pressure.

199
00:31:24,410 --> 00:31:27,110
So they are not going to be independent.

200
00:31:27,110 --> 00:31:41,120
There is always going to be some faulty clarity between the audience because you are measuring sort of intrinsic things about the same subject height,

201
00:31:41,120 --> 00:31:48,350
weight, systolic blood pressure, high cholesterol, BMI.

202
00:31:50,630 --> 00:31:55,130
So there is always going to be some people in addition to the build up obedience.

203
00:31:57,050 --> 00:32:11,660
However, when the populations are extreme among the whole vineyard's, then that is a problem because it interferes with the model fitting.

204
00:32:13,100 --> 00:32:17,870
And how does it show up? How does it get reflected so often?

205
00:32:17,870 --> 00:32:21,470
It may be impossible to compute the beating heart.

206
00:32:22,280 --> 00:32:30,079
And you will see, like when we introduce the matrix formulation for a multiple linear regression model,

207
00:32:30,080 --> 00:32:35,659
you would see like why it becomes impossible to estimate or compute the beta defects,

208
00:32:35,660 --> 00:32:42,500
because then that becomes the, you know, if you have extreme multiple between covariance,

209
00:32:42,500 --> 00:32:48,860
then some meet fixing versions become problematic and we can see that.

210
00:32:48,860 --> 00:32:59,150
FORMAN The other way tells us is that some of the estimates mean that we need large categories.

211
00:32:59,930 --> 00:33:07,420
And the way you would see that in your results is there have really been like y points you don't see.

212
00:33:09,050 --> 00:33:23,000
So that's also should be taken as a red flag and the fitted model may be quite honestly valuable, maybe shaky because the confidence interval is wide,

213
00:33:23,000 --> 00:33:30,290
the standard roads are really large and sometimes the even the locations, the parameter estimates, meeting signs.

214
00:33:31,220 --> 00:33:35,240
So multiple linearity can be a serious issue.

215
00:33:35,240 --> 00:33:37,190
But coming back to your question,

216
00:33:37,730 --> 00:33:47,059
some linearity is always expected and there are situations that you see that there is continue to develop really doesn't.

217
00:33:47,060 --> 00:33:51,410
It's what you call like, okay, it's there.

218
00:33:51,410 --> 00:33:56,990
But you know, it doesn't impact my morning. So I'll just leave it.

219
00:33:57,290 --> 00:34:04,309
Leave the variables. That's it. So that's so we can talk about how what are the sort of metrics,

220
00:34:04,310 --> 00:34:14,240
what kind of measures can we continue to understand the degree of holding ATP so that we can

221
00:34:14,240 --> 00:34:22,040
make a decision as to whether we can keep the water visible supporting a variable climate.

222
00:34:22,040 --> 00:34:27,410
Do you think the model is that we have to drop one? Okay.

223
00:34:27,910 --> 00:34:32,590
So we are going to talk about multiple in any billions in your question.

224
00:34:33,670 --> 00:34:42,490
Okay. Then the next topic that we are going to spend quite a lot of time on is model selection.

225
00:34:45,250 --> 00:34:55,920
And there was a question on Tuesday about a what if I started with a lot of variables and the

226
00:34:56,370 --> 00:35:03,399
whole I know you knew the inside of a sample of size 30 and you start with 60 variables.

227
00:35:03,400 --> 00:35:13,030
I mean, really that's a normal but if I still have like 20 variables, I mean, I only have a.

228
00:35:15,050 --> 00:35:20,090
One and a half beat up on Sparkle. Can I still keep that identity?

229
00:35:20,660 --> 00:35:30,230
Possibly not. So how we choose? Set up some set of people from the 22 past.

230
00:35:30,860 --> 00:35:36,260
Find the best model. So the objective is to find the best model.

231
00:35:37,310 --> 00:35:43,480
And there are several algorithms that may simplify the process.

232
00:35:43,490 --> 00:35:55,670
So we will talk about three such automated selection procedures forward selection, backward elimination and stepwise selection.

233
00:35:56,060 --> 00:36:08,450
And the idea is quite simple, but there are of course intricate, like there are technicalities,

234
00:36:09,140 --> 00:36:14,280
but with the forward selection, what we do is we start with normal.

235
00:36:14,480 --> 00:36:19,550
Maybe it's in the model. We start with only like a kind of a beta, not like a mean.

236
00:36:20,540 --> 00:36:30,480
Then we keep on adding the most significant covariance one at a time until further addition does not improve the model.

237
00:36:31,280 --> 00:36:36,260
So you start from null and you keep on adding covariates one at a time.

238
00:36:37,010 --> 00:36:49,249
So that's the forward selection. The backward elimination is when you kind of sort of the other extreme, you start with the full mode,

239
00:36:49,250 --> 00:36:57,200
lets you start with those 20 variables and you drop one variable at a time.

240
00:36:58,520 --> 00:37:02,810
And the variable, I mean, you know, based on what you,

241
00:37:03,290 --> 00:37:15,540
what you do is you successively delete the least significant over you until you come to a point where no further deletions resulting,

242
00:37:15,560 --> 00:37:21,080
you know, kind of an increase in the model accuracy.

243
00:37:22,070 --> 00:37:34,190
And the third one that we are going to talk about is stepwise selection, which is basically a combination of our selection and backward elimination.

244
00:37:34,520 --> 00:37:40,070
So we will talk about these three automatic selection procedures.

245
00:37:43,540 --> 00:37:52,550
Model validation is another important topic that will come to the end of the semester here.

246
00:37:52,570 --> 00:37:58,300
The concept is how accurate is the fitted model on new data?

247
00:37:58,360 --> 00:38:05,410
Remember we talked about the sort of broad objectives of any regression analysis.

248
00:38:05,860 --> 00:38:10,360
One has to do with inference and the other is prediction.

249
00:38:10,780 --> 00:38:19,180
And depending on what the scientific objective of the study demands, you will sort of choose one versus the other.

250
00:38:20,080 --> 00:38:27,040
But often times it's a combination of both. I mean, there is that, you know, I'm not going to do prediction.

251
00:38:27,040 --> 00:38:31,350
I'm only focusing on inference or not the other way around.

252
00:38:31,360 --> 00:38:35,440
So it's a combination of usually a combination of two.

253
00:38:36,880 --> 00:38:42,940
But model validation has to do is more important when you're in the prediction arena.

254
00:38:44,170 --> 00:38:54,070
So the idea is that you are going to take that fitted model and you are going to use it to predict for future observations.

255
00:38:55,060 --> 00:38:59,440
And the question is how accurate is the fitted model on new data?

256
00:39:01,570 --> 00:39:10,000
I've already, you know, computed the parameter estimates in the model, but now I want to know how accurate is the printed marker.

257
00:39:11,140 --> 00:39:21,900
There are various ways that you can do it. So basically the idea is you train the model on half of that.

258
00:39:21,930 --> 00:39:25,270
You you take the whole data split into two parts.

259
00:39:26,020 --> 00:39:35,770
You train the model, train the model, meaning you compute the parameter estimate based on the first half of the model, first half of the data,

260
00:39:36,220 --> 00:39:47,530
you freeze the model and then you take it to the second half of the data and you try to predict based on that fitted model.

261
00:39:48,460 --> 00:39:51,910
And now what do have you having the second half of the data?

262
00:39:52,180 --> 00:40:00,640
You actually have the actual observe Y and based on the fitted model, you have the predicted Y.

263
00:40:01,630 --> 00:40:05,620
So you can see how closely your observed y predicted y you are.

264
00:40:07,630 --> 00:40:17,240
That's sort of the idea for validation that and again, we are going to develop it much more in-depth.

265
00:40:17,260 --> 00:40:24,339
Yes. Do we have to do this for more validation?

266
00:40:24,340 --> 00:40:28,030
This has to do as we do more with prediction.

267
00:40:28,360 --> 00:40:37,630
But no, not for inference, because any premature focus is to actually, you know, sort of field the model and then get the,

268
00:40:38,140 --> 00:40:43,480
you know, boundary estimates that in certain like the confidence intervals hypothesis test.

269
00:40:43,720 --> 00:40:48,460
Right. And so the the criteria are different.

270
00:40:49,120 --> 00:40:54,159
You know, you look at our outliers, you look at diagnostics, you look at how very community this has been.

271
00:40:54,160 --> 00:40:58,050
You have actually decided that this is your fight and more.

272
00:40:59,470 --> 00:41:08,170
Now we want to validate it. If you have an external data set to validate, that's beautiful.

273
00:41:08,980 --> 00:41:12,550
That's even better. But usually you don't.

274
00:41:12,700 --> 00:41:17,280
I mean that. So this is one way that you can make it.

275
00:41:17,360 --> 00:41:28,030
You use your own sample. You are rooted in other samples and other sort of topic that we will cover.

276
00:41:28,090 --> 00:41:38,200
Like in weekly is there may be situations when not all units are sampled completely at random.

277
00:41:38,200 --> 00:41:45,430
And this often happens in surveys where you may oversample certain subpopulations.

278
00:41:45,670 --> 00:41:48,790
For example, let's say you may oversample ethnic minorities.

279
00:41:50,260 --> 00:41:58,060
So we will talk about weighted regression in that context because survey weighting can be can be quite messy.

280
00:41:59,110 --> 00:42:07,659
And we did regression. The whole idea of weighted regression is that some units are given more weight during the estimation process.

281
00:42:07,660 --> 00:42:11,440
And how do we do that? We are going to learn in, in, in.

282
00:42:11,650 --> 00:42:19,800
Then we covered this topic linear versus generalized linear model.

283
00:42:19,820 --> 00:42:27,370
So the next two slides are kind of trying to put your this for sequence.

284
00:42:27,370 --> 00:42:31,330
Six 5651 653 into perspective for you.

285
00:42:32,230 --> 00:42:39,549
So this course 650 is about linear regression and where we are at.

286
00:42:39,550 --> 00:42:47,120
You mean that y outcome variable is going to be? And Epsilon either errors are normally distributed.

287
00:42:48,440 --> 00:42:52,790
Suppose the outcome variable is an indicator.

288
00:42:54,140 --> 00:42:57,709
For example, I mentioned cancer. No cancer disease.

289
00:42:57,710 --> 00:43:01,460
No disease. So it's binary.

290
00:43:02,840 --> 00:43:06,770
Or in other words, why follow the Bernoulli distribution?

291
00:43:07,400 --> 00:43:13,790
And what you are trying to model is the probability of cancer, probability of disease.

292
00:43:15,650 --> 00:43:30,440
So if you fitted a linear model to the probability of cancer, like if you fitted a SLR where your left hand side is instead of y,

293
00:43:30,440 --> 00:43:35,750
its probability of y being equal to one, your probability of cancer.

294
00:43:35,810 --> 00:43:40,580
If you fitted an SLR model. What's the problem?

295
00:43:43,070 --> 00:43:48,530
Can somebody tell me what? Like, just from what would be the problem?

296
00:43:48,530 --> 00:43:56,860
What would be their errors or. Yeah, but the errors are not going to be long enough or even before that.

297
00:43:56,950 --> 00:44:06,530
Like then, like what would happen? Like if you used a linear model to model the problem, like, you know, when you think about.

298
00:44:09,920 --> 00:44:14,120
Exactly. So what probabilities bounded between zero and one?

299
00:44:15,500 --> 00:44:26,000
Right. But if you if you model probability as a function of X in a linear model, your predictions then go from negative, continue to be positive.

300
00:44:26,000 --> 00:44:35,030
And so again, probability will be on the zero one we didn't know.

301
00:44:35,720 --> 00:44:41,900
So that's like like you will actually not only the assumptions will be blatantly violated,

302
00:44:42,140 --> 00:44:49,670
but you'll get like sort of, you know, meaningless and sort of really wrong results.

303
00:44:51,380 --> 00:45:02,120
So the the class of models that is used to model such outcomes and in general non normal

304
00:45:02,120 --> 00:45:10,249
responses is called generalized linear model and you will learn logistic regression model,

305
00:45:10,250 --> 00:45:15,799
binary outcomes. Like we mentioned, there could also be instances,

306
00:45:15,800 --> 00:45:26,270
but you are maybe interested in modeling the incidence of new cases of cancer in a certain geographical region.

307
00:45:26,570 --> 00:45:35,129
So Y then is a count of the new cases and the regression framework that you see in such context.

308
00:45:35,130 --> 00:45:40,760
The model incidence is Poisson regression. So Poisson regression is regression from known data.

309
00:45:41,060 --> 00:45:49,820
So these are examples or, or these are sort of models that fall in the class of,

310
00:45:49,820 --> 00:45:55,970
in the family of generalized behavior models, which is what you're going to learn in 651.

311
00:45:57,260 --> 00:46:02,900
Okay. And then what about another assumption?

312
00:46:05,930 --> 00:46:14,630
And this has got to do with PubMed and also like a sort of often mistaken terminology.

313
00:46:15,320 --> 00:46:20,120
So multiple versus multivariate regression in 650.

314
00:46:21,500 --> 00:46:26,120
In this course, we are going to talk about simple linear regression where there is only one predictive variable.

315
00:46:26,120 --> 00:46:32,440
We are also going to talk about multiple linear regression where there are multiple predictor variables x.

316
00:46:34,190 --> 00:46:45,950
However, sometimes people confuse multiple linear regression and call it multivariate regression.

317
00:46:45,950 --> 00:47:02,770
Multiple regression is also sometimes referred to as multivariable, but multivariable regression is not the same as multi period condition.

318
00:47:07,250 --> 00:47:09,559
What is my favorite regression model?

319
00:47:09,560 --> 00:47:16,730
If regression is met, your outcome variable instead of being a single number, instead of being a scalar, it's a vector.

320
00:47:20,150 --> 00:47:24,560
Examples. Same outcome being measured repeatedly over time.

321
00:47:24,890 --> 00:47:33,170
So say you are measuring the systolic blood pressure on subjects on a cohort every six months.

322
00:47:33,890 --> 00:47:40,880
So for each subject. So, you know, six months, 12 months, 3 to 4 months and that's say 48 months.

323
00:47:41,210 --> 00:47:46,850
That's the follow up. So every subject has a four dimensional outcome.

324
00:47:48,020 --> 00:48:04,220
612 2448 And and that's why he's a vector and EPS who make 87% to full vs.

325
00:48:04,610 --> 00:48:11,749
But the big difference between a lot of diabetes and a lot of diabetes, definitely not everything is that in a lot of people.

326
00:48:11,750 --> 00:48:15,160
It's a lot of people. For me, it's my baby.

327
00:48:15,290 --> 00:48:19,189
It means that you're one with not a single number.

328
00:48:19,190 --> 00:48:24,440
You're why he's a bit. And this can be any distribution.

329
00:48:26,780 --> 00:48:31,490
One of the examples is like a mild, I believe, normal.

330
00:48:31,760 --> 00:48:47,640
But you will learn about multivariate regression and more in the context of fertility data or repeated measured data such as the example I gave you.

331
00:48:48,190 --> 00:48:52,130
You would study this in biostatistics 53.

332
00:48:53,300 --> 00:49:09,350
Okay. So that's how. Six 5651 and 653 are connected and 650 really forms the basis or the sort of the foundation for 651 and 652.

333
00:49:10,310 --> 00:49:13,520
Okay. So we're at 855.

334
00:49:16,610 --> 00:49:26,150
Do you mind if we sort of carry on and do this class exercise for 5 minutes and then take a break?

335
00:49:27,440 --> 00:49:31,709
Okay. So now we have this class exercise.

336
00:49:31,710 --> 00:49:41,570
We all have had this paper uploaded in a canvas.

337
00:49:42,230 --> 00:49:48,680
So what we are going to do is I'm just going to very briefly describe this study.

338
00:49:48,680 --> 00:49:53,300
This was an article that appeared in the journal Pediatrics.

339
00:49:54,680 --> 00:50:02,059
And the sort of the theme of this is this for the prospective study,

340
00:50:02,060 --> 00:50:09,170
looking at a maternal bone lad as an independent risk factor for fetal neurotoxicity.

341
00:50:09,170 --> 00:50:17,149
And here is a brief background so that we all know is a neurotoxic and it is associated with

342
00:50:17,150 --> 00:50:23,000
decreased mental development in children and accelerated mental decline in the elderly.

343
00:50:23,900 --> 00:50:31,120
There have been many studies that measure lead concentration in blood and correlate them with measurements of intelligence.

344
00:50:31,120 --> 00:50:41,900
So one index that is often used to measure IQ of children is the Baby Mental Development Index.

345
00:50:42,980 --> 00:50:45,920
And so the authors used that index here.

346
00:50:46,970 --> 00:50:57,980
And what they did is they looked at prenatal exposure measurements, which are typically assessed by the amount of leg in the umbilical cord,

347
00:50:58,250 --> 00:51:05,870
but they also introduced a new biomarker and that is born led concentration.

348
00:51:06,560 --> 00:51:14,810
So the premise is that lead leaches out of the mother's womb for pregnancy and that so you can kind of go to the fetus.

349
00:51:15,140 --> 00:51:29,310
So that's the background. And the objectives of the study was to basically look at, you know, children, mothers, peers recruited at birth.

350
00:51:29,310 --> 00:51:37,790
There were 197 such peers, and they were followed until the children were 24 months old.

351
00:51:38,570 --> 00:51:49,640
So the research question is whether lead exposure is associated with mental development, and they have two measurements for the net exposure.

352
00:51:50,810 --> 00:51:58,010
One is the concentration in the umbilical cord, umbilical blood, which is the more traditional measure.

353
00:51:58,700 --> 00:52:03,110
And then the new marker is the concentration in maternal bones.

354
00:52:03,740 --> 00:52:11,960
And then outgo is the mental. Envelopment that that was measured by Baylor's index of mental development.

355
00:52:12,420 --> 00:52:15,979
And they use the acronym MDI to refer to that.

356
00:52:15,980 --> 00:52:26,330
So MDI is my outcome. Y and my primary predictors are these lead concentration in umbilical blood and the maternal bonds.

357
00:52:26,840 --> 00:52:36,300
The statistical question that they are asking is if umbilical cord blood and bone concentration are predictors of baby's MDI.

358
00:52:36,320 --> 00:52:47,389
So that's the context of the study. One other hub sort of quickly that I'm going to mention at this point,

359
00:52:47,390 --> 00:52:53,780
and then we're going to, you know, sort of in did talk about confounding variables.

360
00:52:53,780 --> 00:53:04,370
So in this article. The authors also talk about confounders and what are confounders.

361
00:53:04,640 --> 00:53:13,580
So the idea is this up on farm rates, we have an we have an outcome and we have a primary predictor X.

362
00:53:13,760 --> 00:53:20,180
So Y versus X Association. That is what we are interested in assessing primarily.

363
00:53:20,570 --> 00:53:31,309
There could be a third variable Z that can make the observed association between Y and X either stronger

364
00:53:31,310 --> 00:53:37,490
than the true association or weaker than the true association or even reverse the two association.

365
00:53:37,500 --> 00:53:52,969
So. So the basically the third variable Z in fact, in some way the association between Y yet X a very sort of big bookish type example.

366
00:53:52,970 --> 00:54:01,490
I have a little diagram on the right basketball success and the Y axis height on the x axis.

367
00:54:01,910 --> 00:54:17,210
If you did not have red and blue colors for these different age groups, then the as you can see, the black solid line would be the best fit line.

368
00:54:18,950 --> 00:54:28,200
But that is completely sort of in inverse of the peak of of the true association.

369
00:54:28,550 --> 00:54:40,160
As soon as you bring in A's and the red dots are age 5 to 10 and blue dots are age 50 to 55, as you can see that now,

370
00:54:40,160 --> 00:54:49,490
once you have designated the age groups with two different colors, you'll see that there is a positive association.

371
00:54:49,910 --> 00:54:54,319
And so so this is an extreme example, like a kind of a textbook example.

372
00:54:54,320 --> 00:55:02,690
But generally confounders are variables that can in some way change.

373
00:55:02,690 --> 00:55:08,360
The association between Y said we will talk again more in depth in this paper.

374
00:55:09,230 --> 00:55:16,760
The confounders that they are introducing or they introduced are demographics, age, gender, education,

375
00:55:16,760 --> 00:55:27,440
marital status, breastfeeding, duration, maternal IQ and and sometimes confounders and predicted variables.

376
00:55:27,440 --> 00:55:34,700
And somebody asked me this question in the context of the regression model, because all of them are full benefits.

377
00:55:35,780 --> 00:55:41,450
There may be a primary exclusion of the primary and and testing for this quite complex,

378
00:55:41,480 --> 00:55:45,830
but the need to get into them all these are all together as sort of reasons.

379
00:55:48,730 --> 00:55:52,990
So that's the premise. What I want you to do is goal.

380
00:55:52,990 --> 00:56:00,390
And and in your canvas, there is a man.

381
00:56:00,880 --> 00:56:13,740
There is a. The paper is uploaded, so please read only the abstract of that paper.

382
00:56:14,370 --> 00:56:22,650
Okay. And then let's go for a break and when we come back.

383
00:56:22,950 --> 00:56:27,570
But before we go for the break, please have a quick glance at the abstract.

384
00:56:27,990 --> 00:56:33,660
When we come back, we are going to talk about the in-class exercise.

385
00:56:33,690 --> 00:56:41,700
It's a really cool sort of audience for you to see how the themes that we are going to

386
00:56:41,700 --> 00:56:48,960
learn in this course are you through that slide setting in the context of the story.

387
00:56:49,650 --> 00:57:07,860
So give us briefly to the abstract. Well, for a break, let's come back at 930 and we will go to that plus exercise quickly and then what's going on?

388
00:57:08,170 --> 00:58:32,094
Okay. She just moved to action.

389
00:59:03,901 --> 00:59:08,021
Okay. So welcome back from break.

390
00:59:08,801 --> 00:59:20,171
So as I mentioned that this exercise is for is for you to see the overall relevance of the class in the

391
00:59:20,171 --> 00:59:30,491
applied context and to learn like how to describe the analysis that you conduct in a research article.

392
00:59:30,851 --> 00:59:36,941
This would be extremely useful when again you write your final project report.

393
00:59:37,241 --> 00:59:48,371
So what we are going to do here is for each of the sentences below, like which are text speaking from the article,

394
00:59:50,591 --> 01:00:01,900
telling me like which step of the data analysis process and or the specific topic that we will cover in the class is been described in the class.

395
01:00:01,901 --> 01:00:07,961
When we kind of do it together, we will just say, okay, you know, referring to such and such topic.

396
01:00:08,411 --> 01:00:12,761
And when you go back at home, since there is no homework this week,

397
01:00:13,721 --> 01:00:26,561
read through the articles and maybe you can know down the slide numbers from the lecture relevant to the particular section of

398
01:00:26,561 --> 01:00:37,511
the text that we are talking about and are allowed to upload sort of load off guide on campus or do you need to break those?

399
01:00:37,931 --> 01:00:46,931
There was a question, and I just want to clarify that the topics that we talked about this course,

400
01:00:46,931 --> 01:00:53,651
just to give you a preview, I mean, I haven't told you anything about how to do each of those.

401
01:00:54,311 --> 01:01:01,061
So there is a question that how like before work selection, how do you decide which is the most significant?

402
01:01:01,121 --> 01:01:07,721
We talk about all of those things is the residual diagnostics that I kind of introduced in one slide.

403
01:01:08,081 --> 01:01:11,591
We will actually take three or four lectures to talk about that.

404
01:01:11,591 --> 01:01:16,511
The module selection that I introduced in one slide, you will take 4 to 4 lectures.

405
01:01:16,901 --> 01:01:22,271
So these topics are just to give you a preview of what's coming in the course.

406
01:01:22,491 --> 01:01:31,031
Okay. So don't get nervous that well, I don't know, I didn't follow how to do that because I didn't tell you yet how to do those.

407
01:01:32,321 --> 01:01:38,991
So let's come back to the exercise. We are going to talk about the the relevance of each of these.

408
01:01:38,991 --> 01:01:52,921
So the first section text taken from the article and this basically is the kind of to go back to the to the article,

409
01:01:52,931 --> 01:01:59,401
like if you want to see the exact section.

410
01:01:59,411 --> 01:02:07,990
So the main section that we are going to sort of refer to is the data analysis section in the methods,

411
01:02:07,991 --> 01:02:16,391
which is page one, one, two of the articles when you go back home and read the article in your leisure.

412
01:02:17,921 --> 01:02:24,641
So focus on the data analysis section. So here the author see this group of statistics,

413
01:02:24,651 --> 01:02:31,601
appropriate transformations and identification of outliers form before bivariate and multivariate analysis.

414
01:02:31,631 --> 01:02:42,040
The first thing that I'm going to highlight for you and I mentioned this in multivariate analysis.

415
01:02:42,041 --> 01:02:46,181
So again, around usage, we just talked about that.

416
01:02:46,181 --> 01:02:49,271
So they really were doing a multivariable analysis.

417
01:02:49,991 --> 01:03:00,131
So this frequently is it is used this term is used mistakenly, erroneously in clinical journals.

418
01:03:00,821 --> 01:03:07,301
And what they begin with are multiple linear regression analysis is referred to as a multivariate analysis.

419
01:03:07,691 --> 01:03:15,371
So this is I wanted to point out that, again, this is the mistake, this is the wrong terminology that they use.

420
01:03:15,371 --> 01:03:19,871
They're really doing a multivariable analysis anyway.

421
01:03:20,471 --> 01:03:30,161
So what what are they referring to here? Basically, this is the step that we talked about, like, you know, doing the data, exploratory analysis.

422
01:03:30,161 --> 01:03:35,111
They were doing descriptions some.

423
01:03:39,351 --> 01:03:47,901
When they talk about by immediate association or they're looking at scatterplot and so on,

424
01:03:47,901 --> 01:03:54,901
umbilical blood lead concentrations, but one quarter to the natural logarithm values to normalize the skewed distribution.

425
01:03:55,861 --> 01:04:02,841
So they're validating their. Data points.

426
01:04:05,951 --> 01:04:16,281
Nothing yet to do with modeling. Get get excited.

427
01:04:16,291 --> 01:04:22,020
Boone led values with measurement uncertainties exceeding ten and 15 for tibia and patella respectively,

428
01:04:22,021 --> 01:04:27,181
were excluded from the analysis as part of an established quality control procedure,

429
01:04:27,961 --> 01:04:32,131
as these values reflecting adequate sampling and a not for all related with the

430
01:04:32,131 --> 01:04:35,971
bone let concentration stem said so this is all about like knowing the data.

431
01:04:36,391 --> 01:04:50,201
Descriptive analysis, frequency distributions, mars bar graphs, histograms, perhaps scatter plots, validating the data values and so on.

432
01:04:50,731 --> 01:05:02,521
So the first step then to see the association between my sports have been for months and various measurements of NBA is for a 24 months is my outcome.

433
01:05:02,551 --> 01:05:04,681
This is the why, right?

434
01:05:05,551 --> 01:05:14,610
So they looked at associations between why and various measurements of lead exposure and other covidiots were first examined in by media done.

435
01:05:14,611 --> 01:05:22,711
And this is the once again this term is a little kind of confusing in their in their context or the way they described it.

436
01:05:22,951 --> 01:05:32,881
What are what do you think they were doing here? So they were like each of the, you know, the lead measurements,

437
01:05:32,881 --> 01:05:40,441
exposure of any of those and all the other audience like, you know, age of mother, mother, segregation and so on.

438
01:05:40,771 --> 01:05:48,871
They read, looking at the association of each of those with Y, and they said by baby data analysis, what do you think they were doing?

439
01:05:54,301 --> 01:05:59,821
Yeah so so they were actually thinking of one to think building any different models so lots

440
01:05:59,821 --> 01:06:08,561
and lots of simple linear regression model that that's what they were doing so bunch of this.

441
01:06:13,521 --> 01:06:24,621
Okay. Lots of times. Okay.

442
01:06:24,621 --> 01:06:32,600
Then this is the adjusted association between MBI and biomarkers of LED, but then assessed using multiple linear regression.

443
01:06:32,601 --> 01:06:40,401
So they use the right terminology here and initial model was fitted and included maternal IQ,

444
01:06:40,401 --> 01:06:46,130
maternal age child her gender child, gender of the child,

445
01:06:46,131 --> 01:06:50,870
maternal years of education, paternal years of education, marital status,

446
01:06:50,871 --> 01:06:54,801
duration of breastfeeding and child hospitalization during the first six months.

447
01:06:55,131 --> 01:07:02,871
So these all make up. Remember the covariates that are adjusted in the model.

448
01:07:03,831 --> 01:07:09,141
So this is basically a multiple linear regression model, but this is the base model.

449
01:07:13,121 --> 01:07:16,781
And they adjusted for all of these things.

450
01:07:26,321 --> 01:07:29,891
Okay. So all of these are cool videos.

451
01:07:45,241 --> 01:07:57,061
Okay. Then they say each of the lead biomarkers, umbilical blood lead deep on lead and patella bone leg was then added separately to this model.

452
01:07:57,721 --> 01:08:03,211
So basically, this is their this was their primary hypothesis.

453
01:08:03,221 --> 01:08:08,671
So this is that they were evaluating the primary research question.

454
01:08:29,041 --> 01:08:32,751
Okay. Then.

455
01:08:32,971 --> 01:08:41,841
So so basically we started off with the model with all those four idiots in the model simultaneously.

456
01:08:41,841 --> 01:08:51,681
Then they added the LED exposure videos and then the talk about a final model.

457
01:08:52,251 --> 01:08:56,961
So they said a final model was selected using forward,

458
01:08:57,801 --> 01:09:06,051
backward and stepwise methods to assist for the most stable and robust models very significantly associated with them.

459
01:09:06,051 --> 01:09:14,281
Developing for entering forward. Stepwise multiple regression with backward elimination.

460
01:09:14,301 --> 01:09:20,601
Da da da. So this is why they are doing the model selection that we talked about or that we will talk about.

461
01:09:22,611 --> 01:09:36,761
And they arrived at their final models. So these three steps.

462
01:09:41,491 --> 01:09:47,761
Together. I would say make up the model building phase.

463
01:10:14,461 --> 01:10:20,341
Okay. What will they do next? Then they look at regression diagnostics.

464
01:10:21,481 --> 01:10:30,421
They say regression diagnostics were performed to assess the effect of multiple linearity and potentially influential data points.

465
01:10:30,421 --> 01:10:34,831
So this is where we were doing the residual diagnostics.

466
01:10:37,521 --> 01:10:43,161
Assessing multiple linearity and form.

467
01:10:53,101 --> 01:11:02,671
The multiple distribution models were repeated after excluding potentially influential points using PDI 24 months

468
01:11:02,671 --> 01:11:13,081
as the dependent variable and evaluating the interaction between umbilical cord and blood led vent levels.

469
01:11:13,801 --> 01:11:23,141
So the fourth tool once again is, you know, what do you do or what happens if you do remove outliers?

470
01:11:23,141 --> 01:11:37,321
So like what to do after you detect like influential people and so like remove outliers and releases, that kind of stuff.

471
01:11:41,901 --> 01:11:47,321
We also looked at evaluating the interaction between umbilical cord and one leg levels.

472
01:11:47,331 --> 01:11:59,421
We haven't I haven't done a preview of interactions to come, but we will discuss interactions again in a lot of detail.

473
01:11:59,421 --> 01:12:10,850
And at this point, this small interaction means like how a variable X affects Y depends on the level of occurred.

474
01:12:10,851 --> 01:12:19,821
People seem to be very interested in assessing interactions and we will talk about in the model context how to assess this.

475
01:12:21,081 --> 01:12:28,011
Then obviously we also examine the associations between child blood lead levels at 12 and 24 months instead of four.

476
01:12:28,011 --> 01:12:35,451
In addition to a little broader one leg measurement in the multivariate model, once again the long usage of the term multivariate.

477
01:12:36,471 --> 01:12:43,041
But here they were really doing something like a bunch of sensitivity analysis.

478
01:12:48,321 --> 01:13:02,751
Are secondary to the main, main model and trying to assess like, you know, different meanings of the biomarkers and so on.

479
01:13:02,761 --> 01:13:10,130
So more in the spirit of a of to begin and this is and then finally because more blood borne blood

480
01:13:10,131 --> 01:13:14,841
lead levels do not have a reference range and do not therefore have an obvious interpretation.

481
01:13:15,201 --> 01:13:21,861
We also need an analysis stripping bone led as a categorical variable divided into quartiles.

482
01:13:22,641 --> 01:13:27,350
So sometimes or often times this kind of analysis is done.

483
01:13:27,351 --> 01:13:38,511
So you are on working up on the newest variable into a categorical variable or or actually meeting quartiles or, you know,

484
01:13:38,511 --> 01:13:44,300
ordinal capping be the variable out of this and rerunning the analysis and why is

485
01:13:44,301 --> 01:13:48,951
the dying and this is this is quite a common practice in clinical literature.

486
01:13:49,701 --> 01:13:53,831
The reason is it's it's more easy to interpret.

487
01:13:54,171 --> 01:14:00,771
So this is primarily for the sake of interpretation that they did this.

488
01:14:01,281 --> 01:14:17,951
And and we will also talk about the interpretation in the context of a regression model where a certain X is capital.

489
01:14:18,231 --> 01:14:23,391
And how do you interpret and why is it sort of easier for interpretation?

490
01:14:24,141 --> 01:14:27,921
So this last point really refers to use of interpretation.

491
01:14:28,041 --> 01:14:35,861
Yes. The last three.

492
01:14:38,031 --> 01:14:41,151
So the last one is for interpretation.

493
01:14:44,081 --> 01:14:47,501
Last. Third. Third twice. Third to last.

494
01:14:47,831 --> 01:14:51,161
Remove outliers and reassess.

495
01:14:52,151 --> 01:14:55,411
Yeah. Sort. Sorry. My. My writing is telling.

496
01:14:56,971 --> 01:15:00,821
You know, something is not with.

497
01:15:03,491 --> 01:15:08,221
So I wonder which one. So they converted to the courthouse.

498
01:15:08,231 --> 01:15:16,120
I wonder what converting is used for is also how, you know, not not from an interpretation point of view.

499
01:15:16,121 --> 01:15:19,161
No. Because it would still be continuous.

500
01:15:19,181 --> 01:15:27,761
Right. So oftentimes in the clinical literature, what happens is, let's say let's dig VMI, for example, BMI.

501
01:15:27,791 --> 01:15:31,481
And like if you were building the model as a continuous variable,

502
01:15:31,481 --> 01:15:43,691
then the interpretation is for every one you keep increasing BMI, cholesterol, both by units and exceedance.

503
01:15:46,431 --> 01:15:53,691
However, we know that there is a very standard classification of obesity based on BMI.

504
01:15:53,701 --> 01:15:58,581
So there is like a, you know, under the normal overweight or obese.

505
01:15:59,571 --> 01:16:06,731
So it would be a very natural way to and to take the continuous values of BMI, you know,

506
01:16:06,741 --> 01:16:16,431
like create a categorical variable using those factors that are established in the clinical literature and use that categorical variable,

507
01:16:16,731 --> 01:16:20,450
which is basically, you know, kind of normal, overweight or obese.

508
01:16:20,451 --> 01:16:21,231
And then, of course,

509
01:16:21,231 --> 01:16:32,301
we have an idea with that variable in the model because then the interpretation becomes much more much easier from a clinical perspective.

510
01:16:32,661 --> 01:16:44,641
And you can see, you know, compared to normal people, overweight people have on an average it's you know,

511
01:16:44,661 --> 01:16:51,531
it's higher cholesterol values are obese compared to normal weight.

512
01:16:51,861 --> 01:16:58,121
So this is done purely for the sake of interpretation.

513
01:16:58,131 --> 01:17:02,151
And it's a very pretty common practice in the clinical.

514
01:17:05,271 --> 01:17:12,081
Okay. Any other questions? No.

515
01:17:12,711 --> 01:17:19,821
Okay. So, again, as I said, this exercise was just to put in context and as I have mentioned,

516
01:17:19,821 --> 01:17:25,161
that you've got to ask yourself, why am I in this class and why should I care?

517
01:17:25,641 --> 01:17:29,511
And the reason why you forget is, you know, I told you like what is coming up,

518
01:17:29,511 --> 01:17:37,670
what you learn and you say you knew this is this exercise was to show you that what you would be learning actually is really,

519
01:17:37,671 --> 01:17:41,270
really useful, important in the applied context.

520
01:17:41,271 --> 01:17:46,731
And this is this exercise was just to put back into perspective how all of these

521
01:17:46,731 --> 01:17:54,741
things that you'd be learning kind of gets applied mean in scientific literature and,

522
01:17:54,741 --> 01:18:04,970
you know, gets supported. So you will learn how to pick up a paper, you know, from the from probably applied science.

523
01:18:04,971 --> 01:18:10,941
And then, you know, you get used to being immigration to people who interpret and.

524
01:18:13,561 --> 01:18:21,661
The results. He had actually me up be the end of his gate and designed and ran back into support

525
01:18:21,981 --> 01:18:29,671
for a group for the investigative group that is stacked up in studies like this.

526
01:18:30,001 --> 01:18:33,241
Okay. So just to give that flavor.

527
01:18:33,691 --> 01:18:41,071
So now we are going to go back and do our.

528
01:18:48,231 --> 01:18:55,641
Module, and we're going to do a brief review of basic statistics.

529
01:18:57,431 --> 01:19:03,201
And at this point, we are reviewing concepts that are required for simple linear regression.

530
01:19:03,681 --> 01:19:12,471
We will review the matrix algebra in the future. Then we are ready to talk about multiple linear regression.

531
01:19:13,401 --> 01:19:24,741
So basic statistics. We have up and then the material Y and the notation that I'm going to use here is,

532
01:19:24,921 --> 01:19:33,800
is the notation that I'm going to carry forward when we go and write the model for a simple linear regression.

533
01:19:33,801 --> 01:19:39,561
So we have a sample y i from one to N.

534
01:19:39,771 --> 01:19:44,541
So and you know, some in our context will be not the subjects.

535
01:19:46,581 --> 01:19:54,981
So we have been subject to the subscript II denotes, as I mentioned, response from the subject and so on.

536
01:19:56,721 --> 01:20:12,800
So this summation and Big Sigma I from one to n y denotes the sum of y one plus y two plus y in the product is denoted by B,

537
01:20:12,801 --> 01:20:27,831
by I from one to n of y is y one times y two times up to y in what is the expected value or mean of random variable.

538
01:20:29,601 --> 01:20:39,591
So assume that the mean a the random variable y has a density function level f of y.

539
01:20:40,101 --> 01:20:50,601
They knew why the expected value or the mean of the random variable y is denoted as expected.

540
01:20:50,601 --> 01:21:08,781
Y e within square brackets y is equal to y if y integrated over the beta line.

541
01:21:08,961 --> 01:21:19,101
Negative infinity. Positive infinity. Assuming that the random variable y takes values in, that would be a line.

542
01:21:19,461 --> 01:21:28,911
So the expected value is a measure of our central tendency reflects the center of Y's distribution.

543
01:21:32,561 --> 01:21:37,871
Loads of expected values. Expectation of some sort.

544
01:21:37,931 --> 01:21:45,581
So expected value of some is Gen Y from one to n is the sum of the expectations.

545
01:21:46,121 --> 01:21:56,770
So you can interchange the expectation and the summation signs and there is no assumption of independence required here.

546
01:21:56,771 --> 01:22:02,261
So you can interchange the expected and the summation sign.

547
01:22:03,911 --> 01:22:08,851
Let me run it to, up to and be constants.

548
01:22:08,891 --> 01:22:17,681
Known constants then expected value of little e i.e. why is it times expected value of y?

549
01:22:17,681 --> 01:22:21,761
So you can take the constant outside the expected value.

550
01:22:24,821 --> 01:22:31,181
Similarly, by extension expected value of the sum of the Y.

551
01:22:31,181 --> 01:22:35,771
Again, you can interchange, you can take the X summation sign outside.

552
01:22:36,551 --> 01:22:40,601
So it's add on to the constant outside of the expectation.

553
01:22:41,411 --> 01:22:49,871
So you get summation I from one to n, I expected value of y expected value of product.

554
01:22:50,711 --> 01:22:59,501
So expected value of y y d is now there is a condition here.

555
01:23:00,131 --> 01:23:12,581
The first results, you know, does not need any assumption, any independence assumption, but the product expected value of the product of y idea.

556
01:23:13,661 --> 01:23:21,761
Why? Why does it work? Expected value of times, the expected value of y d if y and y are independent.

557
01:23:21,821 --> 01:23:31,481
So this is a very, very crucial assumption and we will use this many times.

558
01:23:33,251 --> 01:23:38,560
So the bottom line of this slide is it does not matter whether the expectation is

559
01:23:38,561 --> 01:23:43,571
applied before or after the operations of addition and scalar multiplication.

560
01:23:43,931 --> 01:23:58,051
However, the product of two random events, the expectation of the product is the product of the expectations.

561
01:23:58,151 --> 01:24:06,311
If the random variable said independent looking variance and standard deviation.

562
01:24:06,461 --> 01:24:11,891
So what is the variance? The variance is a measure of dispersion.

563
01:24:11,891 --> 01:24:19,210
It reflects the spread of the y, y, the distribution of y or the spread of the wise.

564
01:24:19,211 --> 01:24:30,100
So variance of y is the expected value of y minus the mean of y squared of that.

565
01:24:30,101 --> 01:24:36,791
So this is the new y you remember from previous slide is expected value of life.

566
01:24:37,031 --> 01:24:52,231
Okay, so it's. Okay.

567
01:24:53,611 --> 01:25:05,551
So it again can be written as y minus b y squared if y d by the remember if if y is the density function.

568
01:25:08,961 --> 01:25:15,831
Integrated over negative infinity to positive infinity.

569
01:25:16,491 --> 01:25:25,241
So this reflects the spread of flies and that unit is of gradients is the square of the units of white.

570
01:25:26,091 --> 01:25:29,901
What is the standard deviation? Standard deviation is square root of the variance.

571
01:25:31,531 --> 01:25:42,681
Again, if you multiply y the random variable by a scalar constant here,

572
01:25:43,461 --> 01:25:52,731
then the standard deviation of y is basically equal to eight times the standard deviation of y and the standard deviation.

573
01:25:52,911 --> 01:25:59,631
Once again, it reflects the dispersion in the distribution of y, but it is measured in the same unit as Y.

574
01:26:00,741 --> 01:26:05,291
And this all should be known.

575
01:26:05,301 --> 01:26:10,291
I mean, you should all have familiarity with these results.

576
01:26:10,531 --> 01:26:16,161
It's it's a quick review, but let me know if you have questions.

577
01:26:16,181 --> 01:26:24,351
So another variance variance Y is going to go back to the first principle.

578
01:26:24,461 --> 01:26:30,591
It's by definition expected value of Y minus new y squared of the whole thing.

579
01:26:32,121 --> 01:26:46,011
You can also show algebraically again two simple steps that this is equal to expected value of y squared minus expected value of y equals square.

580
01:26:46,011 --> 01:26:58,130
So this is new y is expected value of twice its expected value of y squared minus the mean of y all squared variance of

581
01:26:58,131 --> 01:27:07,370
linear combination variance of eight plus b is equal to variance of e y y because adding a constant does not change.

582
01:27:07,371 --> 01:27:20,481
The dispersion and variance of a y is basically if you again apply the to go back to the first principle, try out the variance expression.

583
01:27:20,751 --> 01:27:28,281
You can see easily that variance of e by is equal to a square times gradients of one.

584
01:27:29,781 --> 01:27:35,151
Okay. What about obedience?

585
01:27:35,841 --> 01:27:51,441
Obedience? Suppose we have two random variables x and Y then the obedience of a y is basically a given by expected value of life.

586
01:27:51,651 --> 01:28:04,911
You take the product of the center and y makes center meaning X minus the mean of x, y minus the mean of y.

587
01:28:05,271 --> 01:28:17,391
And you take the product of those two and and the expectation of the product so that it's basically not even an expected value of y minus me y dimes,

588
01:28:17,601 --> 01:28:23,271
minus me with new x. It measures the linear association between X and Y.

589
01:28:24,531 --> 01:28:33,411
What's the interpretation? And so if obedience is greater than zero, then large values of X tend to occur with large values of Y.

590
01:28:33,891 --> 01:28:39,111
If it's less than zero, then large values of X tend to coincide with small values of five,

591
01:28:39,561 --> 01:28:46,610
and zero means that the size of X provides no information on the size of white men.

592
01:28:46,611 --> 01:28:51,291
That obedience is calculated. The data are not standardized.

593
01:28:52,071 --> 01:29:07,161
That's why you can only interpret the direction, but not the magnitude, because it is not scale invariant of more interpretable quantities.

594
01:29:07,161 --> 01:29:11,721
The coordination. Once again, X and Y to random variables.

595
01:29:12,211 --> 01:29:18,200
The correlation between X and Y is defined as the whole variance of X five divided

596
01:29:18,201 --> 01:29:23,691
by the product of the standard deviation suffix and the standard deviation of y.

597
01:29:25,041 --> 01:29:33,381
So now it's scaled. It's a scaled measure of linear association, and that's the advantage of coordination.

598
01:29:34,311 --> 01:29:37,281
It is easier to interpret than the obedience.

599
01:29:37,971 --> 01:29:46,641
You can interpret both the magnitude and the direction, and correlation always lies between negative one and positive one.

600
01:29:47,481 --> 01:29:55,491
The closer it is to one in absolute value, it means the stronger is the association linear association between X and Y.

601
01:29:56,871 --> 01:30:02,211
Okay. We are going to use both obedience and coordination in the context of estimation.

602
01:30:02,601 --> 01:30:09,591
But as I mentioned, correlation is a much more interpretable quantity needed to have covariance,

603
01:30:09,681 --> 01:30:17,231
obedience, y and x is again going back to the first principle.

604
01:30:17,331 --> 01:30:25,310
If you write it as the expectation of the product, the centered Y in X, then again two lines of algebra,

605
01:30:25,311 --> 01:30:35,331
you can show that it is equal to expectation of the product of X and Y, minus the product of the expectations of x and Y.

606
01:30:37,771 --> 01:30:38,181
Okay.

607
01:30:38,221 --> 01:30:52,051
So we are going to use this that are also a lot more variants of why which itself is variants of why if do not know you both x and y are independent.

608
01:30:52,651 --> 01:31:01,261
That means they are uncoordinated. Or in other words, the convenience between extend y is zero.

609
01:31:02,431 --> 01:31:14,781
The reverse is not necessarily true. If two random variables X and Y are unaltered does not necessarily mean that they are independent.

610
01:31:14,791 --> 01:31:25,381
The only time it holds is if the if two random variables X and y are you know, are uncorrelated meaning there covariance is zero.

611
01:31:25,771 --> 01:31:33,181
And if x y have a bivariate normal distribution, then x and y are independent.

612
01:31:33,421 --> 01:31:39,511
So this is also a very, very important result that we are going to use a lot.

613
01:31:39,991 --> 01:31:42,810
And I in the context of simple linear regression,

614
01:31:42,811 --> 01:31:53,161
I will show you an example where two variables X and Y are uncorrelated, but they are very, very dependent.

615
01:31:55,061 --> 01:32:00,081
Yes. Like, sort of like the example, but I'm.

616
01:32:00,871 --> 01:32:04,041
Approach to the X and X squared. Yes, exactly.

617
01:32:04,051 --> 01:32:11,610
That would be exciting square. Basically, you know, not only are they the bit you know,

618
01:32:11,611 --> 01:32:20,401
the the whole they are not only with the defendant, they are basically one is a function of God.

619
01:32:21,241 --> 01:32:26,041
Right. So. So there would be. And that's the.

620
01:32:26,351 --> 01:32:32,221
So in general, that is sort of the result. Well, variance is symmetric, additive and scale.

621
01:32:32,221 --> 01:32:35,731
Preserving obedience x one is equal to obedience.

622
01:32:35,731 --> 01:32:36,361
Why? Yes.

623
01:32:36,391 --> 01:32:50,521
So this is basically symmetric properties or radians x with y one plus y two is equal to obedience x with y one plus four we use x with y two.

624
01:32:50,911 --> 01:32:59,220
So this is the distributive property of obedience and finally obedience of x.

625
01:32:59,221 --> 01:33:02,071
With the Y you can take the scalar outside.

626
01:33:02,401 --> 01:33:17,610
It's equal to eight times for median sub ex wife rules a variance variance of some so variance of a song of y i from one to

627
01:33:17,611 --> 01:33:34,111
n can be written as some from I want to n g from one to n double some of covariance y with y you g if you break this up,

628
01:33:34,591 --> 01:33:45,601
sort of expand, dance and collect, then you can see that the obedience of Y with itself is obedience of fire.

629
01:33:45,961 --> 01:33:49,111
So you will have in variance dance.

630
01:33:50,971 --> 01:33:54,181
So that's what gives rise to this first part here.

631
01:33:54,271 --> 01:34:09,901
So this is some I from one plan variance of Y plus you have a bunch of all variance terms y with each of y t that js not equal blind.

632
01:34:10,771 --> 01:34:21,871
Okay, so you have double some I from one to and g from one to n, but I'm not equal to g obedience y y g.

633
01:34:22,561 --> 01:34:29,220
So now if you collect, that dance will be and by and applying the symmetric property of obedience.

634
01:34:29,221 --> 01:34:32,911
But we didn't y y is equal to convenience. Why did you buy?

635
01:34:33,781 --> 01:34:42,781
What you get is, uh, hat this is equal to summation of the variances from i from 1 to 10 plus to nine.

636
01:34:42,991 --> 01:34:52,581
You were collecting the terms obedient some of the I from one G from I plus one to end or within supply.

637
01:34:52,591 --> 01:35:01,920
I do so. And basically the variance of some can be written as some of the variances plus

638
01:35:01,921 --> 01:35:11,641
twice plus some of obedience dance between why why do that g is greater than Y.

639
01:35:13,081 --> 01:35:21,301
And a simple sort of special case of this is if you have variance, it's plus one.

640
01:35:22,171 --> 01:35:28,411
Applying this formula you have variance of x plus baby and so fine.

641
01:35:32,191 --> 01:35:43,741
Plus two times already. And so it's like, okay, if y y y do y in a mutually independent, then what happens?

642
01:35:44,101 --> 01:35:53,881
The whole thing starts all drop off because they're all zero, because independence means uncoordinated or in other words, orbiting zero.

643
01:35:53,881 --> 01:36:03,691
So the orbiting starts all drop off and the variance of the sum of Y reduces to the sum of the variances of flight.

644
01:36:04,741 --> 01:36:15,630
So again, this is a result that we're going to use very a lot estimated of mean suppose we obtained a simple random sample from

645
01:36:15,631 --> 01:36:22,651
some underlying population and we derive sample estimates of each of the population quantities defined previously.

646
01:36:23,941 --> 01:36:33,491
So we have white on white do y in an audio ID with me and me white and variance is more widespread than the estimated of the mean you have to.

647
01:36:33,531 --> 01:36:45,931
Why is the sample mean? And the sample mean is denoted by y bar and taken as the sum of the sample observations.

648
01:36:45,931 --> 01:36:50,581
Y want to weigh in divided by little n, but a little n is the sample size.

649
01:36:52,531 --> 01:36:57,451
Applying the expectation and variance formula from the previous slides.

650
01:36:57,871 --> 01:37:11,221
Then the expectation of the sample mean y bar is equal to the population mean new y and in 6 to 1 you will learn about bias.

651
01:37:11,311 --> 01:37:21,181
Often estimated are unbiased, honest, and from that respect the sample mean y bar is an unbiased estimated of the population mean

652
01:37:21,601 --> 01:37:31,291
because in expectation y bar is equal to the population mean variance of fiber turns out

653
01:37:31,291 --> 01:37:38,370
to be equal to one over n squared some of the variances and again this is based on applying

654
01:37:38,371 --> 01:37:43,471
the formula from the previous slide because what happens to the poor variance terms?

655
01:37:43,471 --> 01:37:49,800
The comedians don't all drop off because y1y2y in a random sample from a population.

656
01:37:49,801 --> 01:37:54,571
What does that mean? That you are drawing from the same population,

657
01:37:54,571 --> 01:38:04,591
but you are making independent and identical copies from that distribution to independent, identically distributed by independents.

658
01:38:05,281 --> 01:38:14,191
We are a few and a few noting that the y y are uncorrelated to the formula is done struck off.

659
01:38:14,671 --> 01:38:26,131
So applying the results in the previous slide I have variants of y bar is equal to one over and square times some of the variances.

660
01:38:26,581 --> 01:38:32,371
So you get sigma y squared, which is the variance of five divided by n.

661
01:38:34,021 --> 01:38:40,081
Okay. So I'm going to stop here and.

662
01:38:44,191 --> 01:38:51,351
And we will pick up from here. Okay.

663
01:38:51,471 --> 01:38:52,131
Thank you.

