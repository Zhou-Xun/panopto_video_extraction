1
00:00:00,570 --> 00:00:04,800
It's not as if you just started recording.

2
00:00:42,200 --> 00:00:46,320
All right. So this is our first split class.

3
00:00:46,950 --> 00:00:52,440
We seem to be missing about a third of people of a kind, about 30 people who are here.

4
00:00:53,130 --> 00:00:57,300
Hopefully the other 20 ish show up. Or are. Uh, yeah.

5
00:00:57,600 --> 00:01:03,890
So we have four things on our agenda today that I want to cover.

6
00:01:03,900 --> 00:01:09,240
So the first is following up on what Jane talked about on Tuesday.

7
00:01:09,690 --> 00:01:18,389
I want to go through their slides, which are on your left and the deconstructed report instructions on your right.

8
00:01:18,390 --> 00:01:24,090
So we're going to go through that both kind of broadly and walk through the specific ideas.

9
00:01:24,090 --> 00:01:28,620
I'll just kind of offer some commentary on the different things.

10
00:01:29,730 --> 00:01:38,610
Second is project questions. So that's what however much time we need up to about 20 minutes or so of questions from you all.

11
00:01:39,910 --> 00:01:47,140
Third is a statistical topic on transformations of data and parameters.

12
00:01:47,770 --> 00:01:56,650
And then fourth is collaborations on like general, like advice on being a statistical collaborator.

13
00:01:56,740 --> 00:02:00,620
So that's going to be about one quarter of the class for each of those. Okay.

14
00:02:00,730 --> 00:02:04,990
So that's that's our to do list today. So.

15
00:02:06,260 --> 00:02:13,830
Project one, as you found out on Tuesday, is. That the deliverable from you is going to be a little bit different.

16
00:02:13,920 --> 00:02:18,540
But the way that we presented it specifically, the way that Jane talked about it on Tuesday,

17
00:02:18,540 --> 00:02:23,580
is that the goal keep in mind that the goal is like a report.

18
00:02:23,640 --> 00:02:27,870
That's what you'll be. That's what you'll be turning in for projects two and four.

19
00:02:28,770 --> 00:02:37,469
So we presented the elements of a report first, and now thinking about that,

20
00:02:37,470 --> 00:02:41,700
we're going to show you what specifically we're going to ask you to do for Project One.

21
00:02:43,200 --> 00:02:50,460
So if you've taken a philosophy class, you read about Plato's Republic, the analogy of the cave.

22
00:02:50,770 --> 00:03:02,860
Okay, so project the report is the actual like the actual forms themselves, the deconstructed report, are the shadows being projected onto the cave?

23
00:03:02,900 --> 00:03:06,570
Okay, so keep that in mind. You're not working for the deconstructed your report.

24
00:03:06,900 --> 00:03:11,700
You're working for the actual report. Okay, so that's that's the background.

25
00:03:13,590 --> 00:03:19,420
So. In the introduction. So now it is moving through the parts in the introduction.

26
00:03:20,200 --> 00:03:27,730
We want you to have a little bit of just background on the thing, the general scientific topic.

27
00:03:28,060 --> 00:03:33,570
As Jean said, you are not the expert in this area per se.

28
00:03:33,970 --> 00:03:36,960
And so even the report is going to be a little bit artificial.

29
00:03:36,970 --> 00:03:41,950
We don't expect you to do literature reviews, even for the actual reports for Project two.

30
00:03:42,700 --> 00:03:47,979
You don't need to do extensive literature reviews. In my collaborations, I don't know if this is the case for Premier,

31
00:03:47,980 --> 00:03:54,040
but like usually it's my collaborator who is the scientist that takes the lead on writing most of the introduction.

32
00:03:55,210 --> 00:04:02,500
So I'm usually not the primary author for writing like the background, the introduction, particularly for applied papers.

33
00:04:04,390 --> 00:04:12,969
Um. I'll also note in parentheses of the deconstructed report under each header there's a number that corresponds

34
00:04:12,970 --> 00:04:22,180
to kind of how many points out of 50 that we are ascribing in terms of importance as well as for your grade.

35
00:04:22,390 --> 00:04:29,440
Okay. So for this deconstructed report, it's basically five out of 15 points are for your introduction.

36
00:04:30,400 --> 00:04:34,570
And that kind of reflects this relative importance of of it.

37
00:04:34,600 --> 00:04:41,709
It should be there, but it's not like the main thing. And just also a reminder, it's this kind of start broad and focus.

38
00:04:41,710 --> 00:04:47,830
N So that by the end of your introduction you both presented the topic as well as specifically.

39
00:04:48,280 --> 00:04:55,330
But our end in specifically naming the question that you're seeking to address in the remainder of the report.

40
00:04:56,170 --> 00:05:05,940
So one comment I want to make for the scene is in 699 many times that the last line of investigation actually already has those four questions.

41
00:05:05,950 --> 00:05:13,150
Right. So basically, there's a tendency in the introduction to just cut and piece those four questions from the investigators reservation.

42
00:05:13,450 --> 00:05:18,500
Don't do that. Rephrase it as if you were not writing the text that this is what I was.

43
00:05:22,720 --> 00:05:33,040
Okay. So the the, the, that the meat of your deconstructed report as well as your report is going to be on the methods.

44
00:05:33,730 --> 00:05:48,510
So let me go down. As Jeanne noted, there's there's generally two subsections to your methods, and this can be literal subsections if you want.

45
00:05:48,520 --> 00:05:51,900
So as we'll see in the report that we go through in a moment, um,

46
00:05:53,130 --> 00:05:59,520
this person decided to have a subsection under methods called study design and another one called statistical methodology.

47
00:06:02,290 --> 00:06:05,859
So describe the study design. Include all of these details that you see.

48
00:06:05,860 --> 00:06:09,550
Here is one question that you'll be answering.

49
00:06:10,000 --> 00:06:17,230
And just for just to clarify, like we're expecting adjusted in the examples that and project when you'll turn in a document

50
00:06:17,530 --> 00:06:21,700
that's basically this with your answers to every single one of these questions under each of them.

51
00:06:22,000 --> 00:06:29,080
Okay, that's kind of expectation here. Number seven, table one, what variables will you provides descriptors to?

52
00:06:29,260 --> 00:06:38,380
Six four is there any variable that you will split table one on this is this is you can talk about table one here,

53
00:06:38,890 --> 00:06:44,650
but in the actual report, don't say in table one we are going to put such and such just.

54
00:06:44,650 --> 00:06:48,820
You were talking generically about the variables that you're going to describe.

55
00:06:50,580 --> 00:06:54,030
Talk about missing data. If so, how will you deal with it?

56
00:06:55,950 --> 00:07:01,190
There's not a lot of missing data. Well, let me rephrase that.

57
00:07:01,200 --> 00:07:05,130
We've kind of taken care of the missing data for it. Problem for you in Project One.

58
00:07:06,570 --> 00:07:09,750
There were quite a bit of missing data, but you don't you don't see those?

59
00:07:10,440 --> 00:07:13,640
Um, we do. You know, picking the worst possible rate.

60
00:07:13,650 --> 00:07:22,290
I just want to pretend that complete this analysis may not be a good idea here, but we don't want you to worry about all of these issues continuously.

61
00:07:22,290 --> 00:07:29,260
Your first project. So this is an artifact. So here number nine.

62
00:07:29,380 --> 00:07:36,100
That's 13 points total out of the 50. So again, relative importance, this is kind of like a key part here.

63
00:07:36,640 --> 00:07:40,300
So describe the model you use to answer each of the questions.

64
00:07:40,810 --> 00:07:51,820
And so here, as Jim noted, you're going to be repeating number nine times the number of questions that that you're you're being tasked with.

65
00:07:52,180 --> 00:07:57,040
And there can be some replication, even just some copy paste, like.

66
00:07:57,430 --> 00:08:01,510
Same as question one. For some of these, you might just write that.

67
00:08:02,410 --> 00:08:11,760
And for the deconstructed report, that's acceptable. But what we're trying to get you to practice is the rhythm of laying out all of

68
00:08:11,760 --> 00:08:15,990
the things that you had to decide by asking you explicitly to address them.

69
00:08:18,990 --> 00:08:30,880
Um. We we kind of phrased it in the way that you might refer to a regression model, but as you see written here,

70
00:08:31,540 --> 00:08:39,580
not all of the things are going to be applicable if you're using something that isn't like a classical regression model.

71
00:08:39,850 --> 00:08:47,740
So for example, random forests don't have parameters in the same sense of like a linear regression does.

72
00:08:48,900 --> 00:08:59,790
And so there is some, depending on your analytic approach for Project one, you might need to fudge the answers to the questions here.

73
00:08:59,790 --> 00:09:02,400
And we're expecting that that will that will be the case.

74
00:09:03,030 --> 00:09:07,680
Don't try to force it into like a regression context if you're aren't doing a classical regression model.

75
00:09:10,420 --> 00:09:14,700
Um. I think that's what I wanted to say about this.

76
00:09:14,730 --> 00:09:21,320
Any questions before I move on to the results? Okay.

77
00:09:22,280 --> 00:09:26,120
So there's 20 points in the Deconstructed report dedicated to methods.

78
00:09:26,840 --> 00:09:35,540
There's ten points dedicated to results. If you compare that to the syllabus where we give the points for the report,

79
00:09:35,540 --> 00:09:41,569
you'll see that there's actually 25 for methods and five for like presentation of figures.

80
00:09:41,570 --> 00:09:52,960
And that's because really questions ten and 13 in this deconstructed report have to do more with like the the figures and the tables.

81
00:09:52,970 --> 00:09:59,060
So those and those points add up to five and then questions 11 and 12 here.

82
00:10:00,020 --> 00:10:03,499
Have more to do with how you're describing your results and interpreting.

83
00:10:03,500 --> 00:10:08,810
So those in the syllabus, those five points actually get lumped together with the methods.

84
00:10:09,020 --> 00:10:11,870
So that's how that's how the arithmetic works for that.

85
00:10:13,610 --> 00:10:22,429
Table one, when we go through the examples, I'll have a little bit more to say about that table one bit intermediate step,

86
00:10:22,430 --> 00:10:28,759
providing the results of fitting the models here to develop that practice.

87
00:10:28,760 --> 00:10:36,110
You can put our output as appropriate, but just pay attention to this comment that we have a question 11.

88
00:10:36,110 --> 00:10:40,430
Don't put our output, generally speaking, in a report.

89
00:10:40,460 --> 00:10:47,720
That's not really the expectation because your audience member is somebody who doesn't necessarily think like you and I

90
00:10:47,720 --> 00:10:56,210
do in terms of variables and and squiggles to denote models and all of that and ask triple asterisks and all of that.

91
00:11:03,880 --> 00:11:08,560
Using your answer. Yeah. So using one of our sentences for each question and five.

92
00:11:08,560 --> 00:11:16,090
So each of your questions that you are tasked to answer, then you're writing out kind of how you're answering that question.

93
00:11:16,360 --> 00:11:26,200
That's what a question 12 is. Discussion and conclusion based on the results section one of the main answers.

94
00:11:26,740 --> 00:11:34,059
So if it feels like you're reiterating things, you are and that's a that you're that's an accurate assessment, right?

95
00:11:34,060 --> 00:11:39,040
So like a very a very brief description of how to report is, is like,

96
00:11:39,040 --> 00:11:44,290
say the thing that you're going to do, do the thing and then say the thing that you did.

97
00:11:44,890 --> 00:11:50,080
Right. So that's kind of how a report is structured and that's how this deconstructed report is going to feel like.

98
00:11:51,030 --> 00:11:59,280
And that's that's normal because also your audience member has not you're sorry your reader in the

99
00:11:59,280 --> 00:12:04,460
real world has not been thinking as deeply about this problem as you have devoted your time to.

100
00:12:04,470 --> 00:12:08,730
And so they're going to need that repetition. That's kind of like a useful hook for them.

101
00:12:10,800 --> 00:12:14,700
Limitations of the study analysis, like Jean said on Tuesday.

102
00:12:16,640 --> 00:12:19,880
This is more than just, like, finite sample sizes.

103
00:12:20,720 --> 00:12:23,120
Like, we're never in an infinite population case.

104
00:12:23,120 --> 00:12:29,899
And so, like, it's kind of it's fine to say that, but, like, try to think more carefully and more nuanced about the limitations.

105
00:12:29,900 --> 00:12:37,850
So this also encompasses things like sampling mechanisms like target your target

106
00:12:37,850 --> 00:12:43,580
population versus your sample population and how those are different from each other,

107
00:12:44,570 --> 00:12:51,980
any sort of potential violations to modeling assumptions that you aren't able to kind of interrogate?

108
00:12:53,750 --> 00:13:00,680
Those would be examples of limitations. So both design as well as analytic limitations is what's encompassed here.

109
00:13:02,890 --> 00:13:07,060
And then. Then once you've got that done, you're ready to write the abstract,

110
00:13:07,060 --> 00:13:12,850
which is really just a distillation, kind of like condensing the things that you did in the report.

111
00:13:14,120 --> 00:13:22,670
So so I'm going to go to examples now before I do that, are there any questions generally?

112
00:13:27,550 --> 00:13:31,610
Okay. So I have linked.

113
00:13:32,390 --> 00:13:39,800
Now let me pull up. Biosafety example one let's see how this.

114
00:13:43,630 --> 00:13:47,310
For Pete's sake, move to. Window.

115
00:13:54,520 --> 00:13:58,480
Okay. So on the left is so we've kind of done things in reverse order.

116
00:13:58,540 --> 00:14:02,589
So the left is an example report that's available to you.

117
00:14:02,590 --> 00:14:06,640
It's linked in the lectures. This is an actual 699 report.

118
00:14:06,910 --> 00:14:14,800
And then we kind of reversed the process and turned it into what could be a deconstructed report, which is on the right.

119
00:14:15,820 --> 00:14:22,120
Okay. So maybe is it useful to have the boss screens up or am I just making everything small?

120
00:14:22,690 --> 00:14:27,140
I see one head now that is useful. Zoom in on the left.

121
00:14:27,150 --> 00:14:33,030
Yeah. Okay.

122
00:14:33,900 --> 00:14:41,610
So background. Um, I'm not going to dwell on that because I think that is not as important to talk about.

123
00:14:41,610 --> 00:14:46,379
But just you see like general topics focusing on down.

124
00:14:46,380 --> 00:14:52,680
So that reverse triangle is happening associated with pre perinatal morbidity and mortality.

125
00:14:52,860 --> 00:14:54,300
There's a research gap here,

126
00:14:55,590 --> 00:15:04,980
kind of past research that's relevant and then you'll see that this is probably taken directly from the scientists slides.

127
00:15:05,460 --> 00:15:10,190
These two specific questions. I just contradicted I just said that you shouldn't.

128
00:15:11,470 --> 00:15:15,850
Uh, this last part? Yeah, I think.

129
00:15:17,730 --> 00:15:26,160
I think for the deconstructive report it I think in ways that it is acceptable to do in the actual report,

130
00:15:26,170 --> 00:15:32,940
a bulleted list or like a numbered list like this is. It's kind of tacky at the least, but it's not really like.

131
00:15:35,230 --> 00:15:43,930
It's not like natural throws. So you would at least write two sentences instead of a bulleted list like that.

132
00:15:44,770 --> 00:15:47,979
So I want to mention one thing which may be confusing to you.

133
00:15:47,980 --> 00:15:52,840
For those of you who are looking at the left hand side, that the abstract itself has four sections.

134
00:15:53,350 --> 00:16:00,910
So many journals often require you to have a structured abstract, but that's not what we're looking for here.

135
00:16:01,090 --> 00:16:05,770
Right. Because this abstract is structured. Abstract? Yes.

136
00:16:07,000 --> 00:16:10,030
When I read it, I'm usually fine with either, to be honest.

137
00:16:10,630 --> 00:16:16,000
So for the Friday part, you can keep it structured like this where you've got background methods, results.

138
00:16:17,290 --> 00:16:21,280
I don't want to contradict what you're saying, but it's not needed.

139
00:16:21,290 --> 00:16:29,350
It's not needed. Correct. But if it helps you in the abstract, even for your report, you can leave it structured like this.

140
00:16:32,540 --> 00:16:40,860
Okay. So methods. So like kind of like more lettered lists here.

141
00:16:41,070 --> 00:16:46,020
You'll see that in the actual report. There aren't lettered lists like that.

142
00:16:46,020 --> 00:16:49,440
So. So this is fine for your deconstructed report,

143
00:16:52,140 --> 00:16:57,900
but you'll notice how it's like prose and kind of a little bit of a narrative in

144
00:16:57,900 --> 00:17:01,500
the actual report on your left when they when they talk about the study design.

145
00:17:04,960 --> 00:17:10,130
Uh. Okay. Table one.

146
00:17:10,850 --> 00:17:15,200
I will provide some statistics. So I think actually so. So Jane did this one.

147
00:17:15,200 --> 00:17:19,489
And I think what they did this isn't like taken from the report.

148
00:17:19,490 --> 00:17:22,940
I think they just wrote out a sentence that was like what they would have answered.

149
00:17:23,870 --> 00:17:27,430
But you'll notice that how how that manifests here.

150
00:17:27,440 --> 00:17:32,000
Where is where do they talk about summary statistics? Yeah.

151
00:17:32,090 --> 00:17:36,860
Here, under the statistical methods is where where Jane took that language from.

152
00:17:42,000 --> 00:17:44,820
Is there any missing data? Yes.

153
00:17:44,830 --> 00:17:51,540
They talk about here the student use multiple imputation, which is taken from this section here on the left is in mice.

154
00:17:51,900 --> 00:17:58,370
This. I think this is a pretty good, pretty good section on missing data.

155
00:17:59,150 --> 00:18:08,660
You'll notice that they have that extra section. They don't. They have study design, they have missing data, and then they've got statistical methods.

156
00:18:08,870 --> 00:18:15,819
And that's totally appropriate. So then here's the the main question for each of your question.

157
00:18:15,820 --> 00:18:25,460
Talk about how you're going to answer. Um, I won't get into the details here, but they they say they're using a linear regression.

158
00:18:25,850 --> 00:18:33,050
It's a special kind of linear regression that accounts for that, the multiple measurements taken throughout one person's pregnancy.

159
00:18:33,590 --> 00:18:37,310
And so they use a linear, mixed model with some random effects and so forth.

160
00:18:37,340 --> 00:18:41,590
So that's all that's all good. A little bit of math here.

161
00:18:42,160 --> 00:18:46,120
I don't recall off the top of my head. No.

162
00:18:46,120 --> 00:18:50,080
In the actual report, they don't have like a linear mixed model.

163
00:18:50,230 --> 00:18:59,260
But I think that this is from the appendix. Yeah.

164
00:18:59,560 --> 00:19:03,280
So these are screen captures from what ultimately ended up in the appendix.

165
00:19:03,700 --> 00:19:08,040
So that's that's probably the appropriate place for these. Um.

166
00:19:10,480 --> 00:19:14,170
And then same you'll notice that this person,

167
00:19:14,170 --> 00:19:20,980
like in this example deconstructed report they said same as in one and that's so that's like appropriate as time just to refer back.

168
00:19:21,550 --> 00:19:26,840
You can also just copy and paste if you feel better about that. Okay.

169
00:19:26,960 --> 00:19:35,850
So now to the results. I want to go back up to table one because I just have one minor comment about that.

170
00:19:35,850 --> 00:19:39,770
So this is the same table that you saw in Jane's presentation.

171
00:19:39,780 --> 00:19:46,950
They took it from this report. Um, just a just more of like a this is like an opinion.

172
00:19:47,400 --> 00:19:51,959
So off on that you might get different perspectives from different professors.

173
00:19:51,960 --> 00:19:57,360
And it's I think it's fine either way. When I make a table one for my papers,

174
00:19:57,360 --> 00:20:06,660
I usually exclude missing notes from the denominator because my perspective is that like when I report a percentage.

175
00:20:07,230 --> 00:20:10,410
So for example, what percentage of.

176
00:20:11,620 --> 00:20:17,620
This population has a lot of the mothers in this study had a college degree and trained it.

177
00:20:18,460 --> 00:20:22,810
And implicitly linking that back to some target population that I'm interested in.

178
00:20:23,050 --> 00:20:25,120
And in that target population, there's no vaccine.

179
00:20:25,330 --> 00:20:29,229
And so, like, whether, you know, the messiness, you can make the argument that's differential or whatever.

180
00:20:29,230 --> 00:20:33,190
But like I don't find it useful to include the messiness in the denominator.

181
00:20:33,250 --> 00:20:38,560
As I'm saying, I would just get the count and denominator would just be the sum of the observed ones.

182
00:20:39,070 --> 00:20:43,540
But that's just more of a preference thing in terms of how I do it.

183
00:20:43,540 --> 00:20:48,490
And I you're not going to get marked down for doing it one way or the other.

184
00:20:50,150 --> 00:20:54,559
One thing I want to point out is that this is something you often miss.

185
00:20:54,560 --> 00:20:59,030
Is this absolutely clear and detailed cash?

186
00:20:59,550 --> 00:21:06,350
Mm hmm. Because the tendency in your first report is usually that just put the table together and just see this.

187
00:21:06,350 --> 00:21:09,920
But look at this. Like how every abbreviation is explained.

188
00:21:09,980 --> 00:21:15,549
The units are. So the caption typically pulled up like you read in the paper,

189
00:21:15,550 --> 00:21:20,740
is that the tables and figures have to be stand alone, even if I'm not reading the whole thing.

190
00:21:20,890 --> 00:21:21,879
But looking at this,

191
00:21:21,880 --> 00:21:30,250
I should be able to pick up what is being reported so that the option of footnotes are very important or publishable or something or something.

192
00:21:34,570 --> 00:21:39,310
Any questions so far? Yeah.

193
00:21:39,340 --> 00:21:44,560
Go ahead. Well, what you said it is internal paperwork.

194
00:21:49,630 --> 00:21:52,900
I think if if you have it, I would report it.

195
00:21:53,260 --> 00:22:01,579
I mean, if it's there in any sort of meaningful way. If you have to, you know, 1% of your observations add some sporadic messiness.

196
00:22:01,580 --> 00:22:06,550
I could see how you would just say we dropped them. We dropped two people who had some business.

197
00:22:08,140 --> 00:22:14,410
But like if it's any sort of consequential business, I would report here that's where my future belongs.

198
00:22:16,380 --> 00:22:25,260
The other thing too related to that is I kind of view table one as like a chronologically as like pre the outcome.

199
00:22:25,560 --> 00:22:33,209
So I know Jane Gene had a comment they recommended not to try to like stratify your columns by now outcome.

200
00:22:33,210 --> 00:22:38,140
I actually hope not this person did or not. I think that I forget what the outcome is.

201
00:22:38,570 --> 00:22:41,720
In the new school year, it is now accountable.

202
00:22:42,200 --> 00:22:48,409
So they do what I probably would recommend against doing, and I believe that Gene said the same thing I do.

203
00:22:48,410 --> 00:22:51,200
Table one is kind of present in the starting point.

204
00:22:52,400 --> 00:22:57,740
I don't even actually think that it's important to if you're presenting the outcome in a different figure.

205
00:22:58,010 --> 00:22:59,900
To even summarize the outcome in Table one.

206
00:22:59,960 --> 00:23:10,040
Again, I think there's a little bit of perspective on that, but I definitely want it stratified by the case.

207
00:23:11,380 --> 00:23:15,070
I see. I think I was going to disagree with me on that point that this is being.

208
00:23:15,310 --> 00:23:20,020
I am because this is that you recall from the last time I blogged it's on this

209
00:23:20,210 --> 00:23:25,030
remembering all the discussions we've had in events on Iraq those continuous.

210
00:23:25,030 --> 00:23:34,270
Do you really believe in this? Because we're in this in many times, actually, the students ended up in clubbing 0 to 2 as low as three and four as I.

211
00:23:34,450 --> 00:23:41,650
And so. So the argument was that can you see a trend in terms of these covariates as you go to the upper

212
00:23:41,770 --> 00:23:48,050
right so you can observe but dropping continuous variables you see that the actions that.

213
00:23:49,170 --> 00:23:53,010
I can. Um.

214
00:23:53,940 --> 00:23:57,729
Provide a nicely formatted table. So. Right.

215
00:23:57,730 --> 00:24:03,530
So I think not to dwell too much on the Deconstructed report itself, but I think that your table, too.

216
00:24:05,440 --> 00:24:13,370
Well. Actually let me take that back. I think, yeah, if you can make a nice table like obviously this person went above and beyond,

217
00:24:13,370 --> 00:24:20,090
as I noted, as you noted on Tuesday, like these tables are pretty exquisite in terms of their detailed.

218
00:24:21,050 --> 00:24:25,129
I so like just keep that in mind.

219
00:24:25,130 --> 00:24:29,780
This is kind of like in the upper bound in terms of like. Quality of a table.

220
00:24:29,780 --> 00:24:35,600
But something like this I think is a is a really nice is considered like a nicely formatted table.

221
00:24:36,290 --> 00:24:45,800
Yeah, go ahead. Yes, you mentioned us. What if we we still.

222
00:24:49,460 --> 00:24:56,350
I. I would say no.

223
00:24:56,410 --> 00:25:00,610
So my perspective at table one is it's the data that you actually analyzed.

224
00:25:00,880 --> 00:25:08,920
And so if you're in a scenario where you have you have excluded people in the data file that you got.

225
00:25:09,850 --> 00:25:14,350
Uh, I would, I would consider table one to be the people that are left that like the,

226
00:25:14,410 --> 00:25:17,590
the observations that are left over after those exclusion criteria.

227
00:25:17,950 --> 00:25:24,130
But I probably would talk about that, at least in the text and maybe even a flowchart,

228
00:25:24,430 --> 00:25:29,950
for example, on the appendix to kind of show how people and why people dropped out of your data.

229
00:25:31,580 --> 00:25:38,450
So for example, in this particular table, we actually already excluded everyone from a completely missing computer again.

230
00:25:38,960 --> 00:25:42,950
Okay. So this is like the partial covariance that you see.

231
00:25:43,760 --> 00:25:50,580
But they were even people with missing presumably were analyzed for their observed information that they did have via that imputation.

232
00:25:50,600 --> 00:25:54,560
Right. Okay. Yeah. But if you had nothing, then they were right.

233
00:25:54,900 --> 00:25:58,160
Yeah. They would probably be not. They would not be in this table there.

234
00:25:58,220 --> 00:26:14,990
Yeah. Yeah. Go ahead. So. Yeah.

235
00:26:18,310 --> 00:26:26,000
Etiquette. Uh, I would say out of necessity.

236
00:26:26,000 --> 00:26:33,850
Probably I would. Report the union of your of your study populations.

237
00:26:35,670 --> 00:26:42,809
I don't unless you can. If you have a creative alternative that you end up doing, I'm open to it like I would.

238
00:26:42,810 --> 00:26:47,090
I would encourage you to do it. Like, you know,

239
00:26:47,090 --> 00:26:54,020
just sort of guidance and that best practices that we follow typically is the so

240
00:26:54,020 --> 00:27:00,140
typically table one is overall population if is a driving stratifying factor,

241
00:27:00,410 --> 00:27:03,680
for example, race. Ah yes that's a great.

242
00:27:03,730 --> 00:27:08,230
But that's not light qualified to see that.

243
00:27:08,500 --> 00:27:14,290
But if they're part of some groups that you're interested in on a particular particular analysis, you looked at that.

244
00:27:14,590 --> 00:27:20,870
You might think that the. I wanted to do something else, like looked into the appendix and supplementary.

245
00:27:21,850 --> 00:27:27,909
Table one is descriptive characteristics of the study population and so remember that.

246
00:27:27,910 --> 00:27:33,200
So usually it's overall if there is one factor that you are very interested in why it's not.

247
00:27:34,970 --> 00:27:48,970
I think that. I also what I think you're kind of pointing at is also like maybe for some of these questions,

248
00:27:48,970 --> 00:27:56,890
you might exclude certain blood gas measurements and for other questions you don't necessarily feel like you can or should.

249
00:27:57,580 --> 00:28:00,730
And so is that is that where you're going towards? Yeah.

250
00:28:04,320 --> 00:28:07,350
Yeah, I would probably. Yeah.

251
00:28:08,030 --> 00:28:13,660
Is where I would lead in that regard. Okay.

252
00:28:14,890 --> 00:28:22,670
And then finally set a conclusion. I don't think there's anything specifically I'm expecting that you'll probably read through this on your own.

253
00:28:22,700 --> 00:28:26,170
That's why I feel like it feels like I'm kind of jumping from topic to topic.

254
00:28:26,950 --> 00:28:34,070
Um. You know, they know that.

255
00:28:34,080 --> 00:28:44,720
They know that the measurement error as a limitation. This is a little bit tricky, just as had more of a statistical comment.

256
00:28:46,230 --> 00:28:56,370
They? It's kind of like I don't know in my mind if if they're basing this limitation on the effect sizes that they observed in this data, set it.

257
00:28:57,520 --> 00:29:00,880
It's kind of I don't know, it feels a little bit backwards to call that a limit.

258
00:29:01,970 --> 00:29:02,630
Tation.

259
00:29:03,860 --> 00:29:12,070
Because they don't know if the actual observed effect sizes are are are modest because they're modest or because of some other issue with the data.

260
00:29:12,080 --> 00:29:17,210
So it feels a little bit circular, but um, I wouldn't quibble with them putting in that.

261
00:29:17,930 --> 00:29:21,280
Yeah, go ahead. So for my part.

262
00:29:23,220 --> 00:29:37,630
Well, I should mention. Yeah.

263
00:29:38,680 --> 00:29:42,700
So I think we posted some of the answers for a bunch of them on that issue.

264
00:29:43,060 --> 00:29:47,260
And I think his recommendation was that he doesn't believe anything that you said.

265
00:29:48,120 --> 00:29:55,490
And so I think if you look at that, because that was in the discussion board and you can see that we decided them because.

266
00:29:56,360 --> 00:30:02,600
You know, beyond the realm of plausible measurements. Yeah, I think it's squarely a limitation though, because.

267
00:30:03,360 --> 00:30:10,480
Yeah. Yeah, you don't. Because it also tells me, like, you only know because they're so unbelievable.

268
00:30:10,570 --> 00:30:20,170
But it also tells me that there's probably some underlying data entry issue and that if you have a blood gas of 90 that was mistakenly entered,

269
00:30:20,170 --> 00:30:24,550
you're never going to know that it was a mistake because it's totally plausible.

270
00:30:24,580 --> 00:30:34,890
So. How deep into the. Not.

271
00:30:35,100 --> 00:30:39,580
Not super deep. No. No.

272
00:30:39,940 --> 00:30:47,179
Because ultimately there's nothing that you can do to. Follow up on it in the scope of your project.

273
00:30:47,180 --> 00:30:52,550
So I calling it out I think is the most important.

274
00:30:54,300 --> 00:31:03,060
And I think this is an important point, statistical point. And Sloan mentioned that measurement error is never really towards one end of the scale.

275
00:31:03,340 --> 00:31:11,660
So it's easy to catch it to it's one end of the scale. If there is a measurement error, it's probably a visit to the entire spectrum of measurements.

276
00:31:13,780 --> 00:31:19,520
You know, seven, ten, 14 up. But. They need to go to step.

277
00:31:23,110 --> 00:31:27,970
Okay. So in the interest of time, I'm not going to go through the second version,

278
00:31:28,750 --> 00:31:34,690
but I just want if anybody has looked at it and has a particular question on the second report.

279
00:31:34,780 --> 00:31:41,050
Sorry, that second version. The second report. I'm open to answering otherwise.

280
00:31:42,140 --> 00:31:49,720
I think we'll move on to the next topic. Where do they put the tables and figures?

281
00:31:52,840 --> 00:31:59,410
Those are primarily in questions of ten.

282
00:32:01,220 --> 00:32:03,620
So instead of saying, see table one and report,

283
00:32:03,920 --> 00:32:10,040
your version is going to have the table right there because you're not always going to have a report to point to.

284
00:32:11,030 --> 00:32:14,270
And Question 13 is the other one.

285
00:32:14,660 --> 00:32:18,620
So like, if you can just drop them in there, that would be ideal.

286
00:32:19,570 --> 00:32:22,930
I'm sure of. What happened?

287
00:32:25,860 --> 00:32:33,180
We're. It is your call.

288
00:32:34,440 --> 00:32:39,840
As long as it's clear. That there's a question in that you're answering the question right below it.

289
00:32:40,080 --> 00:32:45,790
But I think beyond that. Go with kind of what is most natural for you.

290
00:32:47,050 --> 00:32:51,240
And there will not be an appendix for this. That's a good point.

291
00:32:51,870 --> 00:32:57,140
We did not talk about that. So I think the answer is no because you write in the equations, right.

292
00:32:57,270 --> 00:33:00,540
There's some things that would go in the appendix in this. Yeah.

293
00:33:03,280 --> 00:33:06,460
Yeah. Go ahead, Claire. We're only heading in the right.

294
00:33:08,310 --> 00:33:14,340
You're only heading in the right. Correct. And you'll you'll internalize that.

295
00:33:14,340 --> 00:33:19,530
And then for a project to enter a project for you, you won't turn in the right at all.

296
00:33:20,310 --> 00:33:25,140
Maybe you'll use this as a schematic, but you'll turn in the left.

297
00:33:28,080 --> 00:33:35,520
Sorry for Mara supplementary because we said no and then basically stick to the affordable supplementary tables and figures for this.

298
00:33:36,780 --> 00:33:42,510
I guess we should ask our colleagues or talk about it ourselves. And then get back to them on that.

299
00:33:45,530 --> 00:33:48,910
I guess I want more hands. Okay.

300
00:33:54,520 --> 00:34:05,180
I think we need to do. Okay.

301
00:34:07,290 --> 00:34:14,300
We'll get back to you on that point. That's all I wanted to say on the Deconstructed report.

302
00:34:14,960 --> 00:34:18,120
So. You kind of.

303
00:34:18,120 --> 00:34:23,280
You already did. But you have our blessing to start filling this out now.

304
00:34:24,330 --> 00:34:28,250
The due date is. Is that the 24th?

305
00:34:28,850 --> 00:34:33,510
Okay. So that's less than two weeks. So.

306
00:34:34,980 --> 00:34:38,100
But we'll have a file. You'll be uploading this as a file.

307
00:34:39,210 --> 00:34:45,620
Well, pdf or word doc. Our two natural formats that I expect you you might have.

308
00:34:47,030 --> 00:34:52,700
If there's something else beyond that, maybe check with us if you think it's going to be a different file format.

309
00:34:56,340 --> 00:35:02,610
Just do more things. Be very careful about this as a decimal in your see.

310
00:35:03,060 --> 00:35:06,180
We're ready. Not cautious about that in the last.

311
00:35:08,510 --> 00:35:18,690
And also. This rubric is very good for writing the paper, and that's why we're sort of ready to do this with that.

312
00:35:19,470 --> 00:35:25,470
Instead of going to the final ultimate report, we have a structure in place and gradually build it.

313
00:35:25,770 --> 00:35:29,890
It's sort of like a scaffold. The uncertainty of the studio. That's your project.

314
00:35:30,360 --> 00:35:32,100
So think of that practice.

315
00:35:32,430 --> 00:35:40,590
And also we've done some interim project reports for this project that sort of needs you to wait till the last moment to write this up.

316
00:35:40,660 --> 00:35:44,440
But it's like, you know, please see us in our office hours.

317
00:35:44,480 --> 00:35:47,940
Please use that time. Get started early as.

318
00:35:49,980 --> 00:35:53,820
Interim presentation. Interim? Yes. Nobody has an interim report.

319
00:35:54,300 --> 00:36:02,260
This. There are, but we probably did it.

320
00:36:02,440 --> 00:36:06,640
Maybe we didn't call that out. I have one, four, 5 to 6.

321
00:36:06,910 --> 00:36:11,110
So this is the top of the syllabus. So each of us has an individual office hour.

322
00:36:12,160 --> 00:36:16,870
I've made mine virtual only just because logistically it seemed easier.

323
00:36:18,760 --> 00:36:23,020
I don't know where my readers are in person. Do you have that?

324
00:36:23,470 --> 00:36:27,070
I just moved the class and maybe make some zoom and something.

325
00:36:27,400 --> 00:36:31,180
Okay, everyone. You'll have to make a way to make that clear, though.

326
00:36:31,450 --> 00:36:36,340
Yes. And Jean Jane and Jeremy have made it hybrid.

327
00:36:37,060 --> 00:36:42,670
So you can see it's kind of spans four days of the week and then plus, we'll have the paired office hours.

328
00:36:42,730 --> 00:36:46,300
So anybody from any class is welcome to any office hour.

329
00:36:49,670 --> 00:36:52,850
Good question. Thanks for calling that out. Okay.

330
00:36:53,300 --> 00:36:59,600
So now let's go to a project questions like specific technical and statistical questions.

331
00:36:59,600 --> 00:37:06,080
Sure. Do that remark. Okay, I'll pull up this just if we need any reorient in.

332
00:37:21,620 --> 00:37:25,160
Do you want to have anything? Any follow ups? Yeah, go ahead.

333
00:37:25,940 --> 00:37:31,410
Yeah. For about. S.C.

334
00:37:36,120 --> 00:37:49,200
I'm not sure whether any questions are asked for. Like.

335
00:37:55,570 --> 00:37:58,850
So what what would be and you say you mentioned a two by two table.

336
00:37:58,870 --> 00:38:02,720
What would be the margins of that? Yeah, I mean, like so.

337
00:38:04,120 --> 00:38:07,690
So what I mean is like after.

338
00:38:08,600 --> 00:38:29,010
Based on the data. I'm not quite sure whether.

339
00:38:35,650 --> 00:38:40,150
Because it's not. Mm.

340
00:38:41,850 --> 00:38:48,210
So yeah. So I think you're drilling in on a good point, which is like what is meant by threshold.

341
00:38:48,840 --> 00:38:56,580
So what I, what I interpret threshold to mean here is just a decision based upon the measured.

342
00:38:57,790 --> 00:39:10,200
Pulse ox. For a person that probably is taking into account their self-reported race and may also take into account other factors.

343
00:39:11,310 --> 00:39:17,570
Is that how you are interpreting threshold here? So I think this is, you know.

344
00:39:18,570 --> 00:39:26,730
John definitely defined a particular goal in terms of hypoxemia like hypoxia.

345
00:39:27,150 --> 00:39:28,590
That's that's that's in school.

346
00:39:28,890 --> 00:39:36,570
And so I think that if I was a statistician on this subject, I'm going to push on this like, you know, what is right, like what metric.

347
00:39:37,350 --> 00:39:41,260
So we have to think about the best metric, even if you think about sensitivity and specificity.

348
00:39:41,280 --> 00:39:47,940
What am I going to optimize? So I would think that there are different thresholds.

349
00:39:47,950 --> 00:39:52,510
The threshold is determined by the pulse oximeter because that's what you can see get.

350
00:39:52,840 --> 00:40:02,310
But ultimately, the clinical goal is that I really want to have the right treatment for anyone who is truly below 88.

351
00:40:02,320 --> 00:40:08,430
And that's that's, I think, the ultimate goal. And it takes into account their self-reported race.

352
00:40:08,460 --> 00:40:13,480
Yes. Right. Yes. And then possibly six.

353
00:40:13,930 --> 00:40:17,049
I do not know what factors are accountable for that,

354
00:40:17,050 --> 00:40:29,980
but my goal is to make sure that I pick up everybody across all possible subgroups and not be misled by those where the true measurement is.

355
00:40:35,700 --> 00:40:39,390
Is that a measurement dispute? We up?

356
00:40:42,170 --> 00:40:45,470
That's what I'm thinking. But I didn't know. What? What.

357
00:40:45,530 --> 00:40:49,350
How do you do? I'm not gonna say how to do it.

358
00:40:49,680 --> 00:40:53,340
I was just fixating on the threshold, like, what does that mean?

359
00:40:53,880 --> 00:40:58,830
I would interpret that to mean, like, a decision for action for supplementary oxygen.

360
00:41:00,530 --> 00:41:06,280
That's how I interpret the word threshold in this. So that's why I felt 1992.

361
00:41:06,590 --> 00:41:12,470
Yeah. So at a threshold, as no matter who you are, no matter the pigment of your skin.

362
00:41:12,620 --> 00:41:16,970
92 is a decision that's a threshold, and probably not a very good one.

363
00:41:22,440 --> 00:41:25,860
Make sure we circle back to are you processing or.

364
00:41:32,970 --> 00:41:38,550
And I'm like, Should we make? So the work that.

365
00:41:41,130 --> 00:41:44,970
So after you did that. Are.

366
00:41:47,830 --> 00:41:52,680
About the stretch. You have. The wrong place.

367
00:41:55,470 --> 00:42:01,150
Yep. Because that. What is the plan for?

368
00:42:02,340 --> 00:42:12,879
For many people, you should. It is another way of what you're asking.

369
00:42:12,880 --> 00:42:16,540
How to balance between the false positives and false negatives.

370
00:42:17,260 --> 00:42:23,050
Yeah. So the false positives. So it's false positives.

371
00:42:23,970 --> 00:42:29,150
Basically based on the. I mean, you're.

372
00:42:32,130 --> 00:42:41,790
So I have of self-help. The latest data.

373
00:42:43,500 --> 00:42:46,920
Okay. But so is the data.

374
00:42:46,920 --> 00:42:51,430
And I. There are people about the threshold.

375
00:42:54,490 --> 00:42:58,310
Actually about that. Those are.

376
00:43:07,340 --> 00:43:11,480
It's using arterial blood gas as the gold standard. Um.

377
00:43:15,490 --> 00:43:20,710
You're not going to know what the right ratio is other than because we don't have any outcomes to.

378
00:43:21,670 --> 00:43:30,130
But what Tom if you that some of the responses from him subsequently suggest that.

379
00:43:30,980 --> 00:43:40,730
False positives are less detrimental, meaning it's not going to as harmful to give supplemental oxygen to somebody who doesn't need it.

380
00:43:40,940 --> 00:43:48,320
That's not as bad as failing to give supplemental oxygen to somebody who does need it.

381
00:43:49,010 --> 00:43:53,990
So like, there is a asymmetry here. We just don't know how asymmetric.

382
00:43:55,030 --> 00:43:58,450
And I think we don't. Yeah, that's not answerable with our data.

383
00:43:58,810 --> 00:44:05,900
Yes. In the session, Rodrigo said that the cost of unnecessarily treating is not known.

384
00:44:06,310 --> 00:44:11,530
Of course there is cost because intubation and other things actually can have minimal risk.

385
00:44:11,530 --> 00:44:20,190
But we do not know that lost function. I like high level question.

386
00:44:20,600 --> 00:44:23,960
Specifically around the threshold decision is that.

387
00:44:24,440 --> 00:44:32,089
Is there a principle of physician autonomy at work here, which is to say that we probably don't want to provide physicians with like 16 charts

388
00:44:32,090 --> 00:44:35,930
that are like divided by six and divided by eight divided by race and specialty,

389
00:44:35,930 --> 00:44:40,940
because we trust doctors intuition in their training to make decisions based on complex factors about patients.

390
00:44:41,270 --> 00:44:47,050
So I guess we agree that the threshold should be kind of while taking into account racism.

391
00:44:51,100 --> 00:44:57,600
I'm always happy in person. I'm happy to. I don't want to keep talking over. Okay.

392
00:44:58,290 --> 00:45:10,500
You would do that all the time when you did that. And so my response is probably like this is a statistical solution to a technical problem.

393
00:45:11,070 --> 00:45:15,780
And so I don't think in your mind's eye, even though you are asked to calculate thresholds,

394
00:45:16,470 --> 00:45:22,530
probably the goal is not to provide a new threshold at the bedside,

395
00:45:22,920 --> 00:45:29,880
because the underlying problem is that the algorithms and the pulse ox and like the light beams and stuff aren't.

396
00:45:31,200 --> 00:45:34,530
Appropriately reacting to the pigmentation of the skin. So like,

397
00:45:35,040 --> 00:45:41,639
hopefully this work makes that changes makes those changes so that like maybe

398
00:45:41,640 --> 00:45:45,360
there is a global threshold that's appropriately calibrated for all pigmentation.

399
00:45:46,140 --> 00:45:48,240
So, like, I wouldn't be so concerned.

400
00:45:49,140 --> 00:45:56,370
Like, I totally understand your point, but, like, I wouldn't be so concerned that, like, overwhelming, like, multiple thresholds is,

401
00:45:56,370 --> 00:46:03,180
like, not clinically feasible because probably this isn't the ultimate way that this issue is going to be resolved.

402
00:46:03,360 --> 00:46:10,999
It might take. So you are trying to articulate the problem that you say that you because you are

403
00:46:11,000 --> 00:46:15,800
doing a disservice to certain populace and how much of a disservice that is.

404
00:46:16,070 --> 00:46:22,190
Right. Finally, I think the decision is hopefully if and I actually I talk to somebody,

405
00:46:22,580 --> 00:46:29,209
the solution when they first were developing these instruments were quite simple to use different kinds of like instead of one.

406
00:46:29,210 --> 00:46:34,220
Like we used like two or three, but it was more expensive in terms of manufacturing.

407
00:46:34,220 --> 00:46:43,220
That's what you do. So ultimately, I think that the technical solution is the goal that you fix it so that it actually means measurement.

408
00:46:43,250 --> 00:46:49,670
Well, for. Most people want change, the medicine Card said.

409
00:46:49,920 --> 00:46:54,840
But you do not want to confuse things. You want the media to.

410
00:46:56,030 --> 00:46:59,250
Four, but even then, politicians in this world.

411
00:47:00,100 --> 00:47:03,440
So this is a there are studies on that.

412
00:47:03,470 --> 00:47:06,960
So I think you are right in the. We knew that.

413
00:47:08,600 --> 00:47:16,500
But even more troubling should. It's a great thought, though.

414
00:47:16,740 --> 00:47:22,580
That's that any time you build, like a risk prediction model, you ought to be thinking and just like you asked,

415
00:47:22,590 --> 00:47:28,170
like kind of there is a practicality component to it and doctors aren't going to be plug it in.

416
00:47:30,040 --> 00:47:34,300
You know, all the factors into a calculator to to make that decision.

417
00:47:39,550 --> 00:47:48,500
Yeah. So. All of this, I.

418
00:47:49,460 --> 00:47:59,730
Discussion of. Wondering with.

419
00:48:05,510 --> 00:48:17,720
I know you. We just.

420
00:48:26,200 --> 00:48:33,040
However, my answer to that was that's a hard one. So I do think that.

421
00:48:33,280 --> 00:48:38,930
So the question is that there is measurement data to. Which one is more harmful for the patient?

422
00:48:39,170 --> 00:48:39,460
Right.

423
00:48:39,920 --> 00:48:49,600
When you're reading the same high IQ oxygen level, it's actually you know, that's the piece we are most concerned about because it's not harmful.

424
00:48:50,120 --> 00:48:57,750
That's the assumption. But measurement is going to be everywhere because the reading could be like, you know.

425
00:48:58,810 --> 00:49:03,840
It really could be higher. That's that's what we're concerned about.

426
00:49:03,890 --> 00:49:09,350
If you're reading this law and have to laugh and gets hotter, you're less concerned about that.

427
00:49:10,070 --> 00:49:21,959
I think that's what we. And if you decide to define I think this was negative and then he's saying, I link to it saying false positives don't matter.

428
00:49:21,960 --> 00:49:28,690
In this case we want to avoid. So do we just.

429
00:49:30,000 --> 00:49:43,909
Do we just stay quiet? So if I think about the decision theoretic perspective I was just asking felt like

430
00:49:43,910 --> 00:49:48,500
an intuitive function that this is the loss corresponding to these decisions.

431
00:49:48,500 --> 00:49:51,930
And how likely is it that from a position. Right.

432
00:49:51,950 --> 00:49:56,120
So he's saying that don't matter in this case but in the.

433
00:49:58,570 --> 00:50:00,730
Everybody gets oxygen. I.

434
00:50:03,510 --> 00:50:11,410
I think obviously the thing that I'm going to do, hypothesis testing by I'm going to always reject the notion that I couldn't have followed.

435
00:50:11,430 --> 00:50:16,170
What. That's what. I think that.

436
00:50:17,650 --> 00:50:24,190
I think characterizing the false positive rate and the false negative rate would be very useful even if you did it,

437
00:50:24,730 --> 00:50:29,980
even if you're not able to explicitly like optimize with respect to some utility.

438
00:50:30,670 --> 00:50:37,989
I think by that you mean this. Like reporting you have a threshold you are able to report,

439
00:50:37,990 --> 00:50:45,070
at least within the framework of your model, what it thinks that the that two error rates are.

440
00:50:45,490 --> 00:50:50,160
So at least if you're able to report that, you've got a threshold.

441
00:50:54,280 --> 00:50:59,950
Yeah. Because it's not as simple as that. Like, it's, it's said simply, but like there's a lot that's going to go into that.

442
00:50:59,950 --> 00:51:03,340
So it's actually not going to be as trivial as it sounds.

443
00:51:03,730 --> 00:51:06,870
So that's a lot of work right there already. It might.

444
00:51:07,190 --> 00:51:12,580
Yeah. And one thing I want to mention that. Do you have enough information to talk about these?

445
00:51:12,670 --> 00:51:18,520
Absolutely. Don't forget that your goal is to provide to groups the whites.

446
00:51:19,330 --> 00:51:26,120
Are there and that there is a set benchmark and you are trying to make this fair and equitable with respect to two groups.

447
00:51:27,680 --> 00:51:34,360
Don't forget that. Breakfast. I had something in my shoe that know.

448
00:51:35,450 --> 00:51:40,960
You want to make this a race? The relative comparison is important.

449
00:51:41,280 --> 00:51:46,690
Yeah, it will be hard if like you've got 95% sensitivity and 90.

450
00:51:46,710 --> 00:51:47,760
I'm just going to make up numbers,

451
00:51:47,760 --> 00:51:56,280
92% specificity in one group and like different numbers and the other group like it will be hard to kind of compare those.

452
00:51:57,710 --> 00:52:03,390
Is that what you're getting at? Okay. The measurements are not perfect for either group.

453
00:52:03,420 --> 00:52:07,740
I don't want a particular group to get this effect because it is invented to comparison.

454
00:52:14,620 --> 00:52:20,989
That's a critical question that. Pushes that are there, which is that given that these technologies were developed,

455
00:52:20,990 --> 00:52:27,080
unlike patients and that they were relatively happy with their thresholds when they were treating mostly white patients.

456
00:52:27,320 --> 00:52:30,420
Can we make something of a principled assumption that you were happy with?

457
00:52:31,410 --> 00:52:39,640
Specificity. Like reverse engineer, like the target is based upon what you observed.

458
00:52:40,210 --> 00:52:51,010
Would that be would there be any. No, I don't have.

459
00:52:51,010 --> 00:52:56,070
And I'm pondering and processing. I feel like.

460
00:52:56,920 --> 00:53:00,440
We have to set some target and the target is not going to cost them.

461
00:53:01,720 --> 00:53:05,720
So I think that that's probably.

462
00:53:08,310 --> 00:53:11,820
Yeah. My first blush is I don't have I don't see what.

463
00:53:13,390 --> 00:53:16,120
It could be wrong. Like that seems like a sensible approach to me.

464
00:53:22,510 --> 00:53:30,220
That might be worth that comment might be worth sharing on the discussion board as like a question that was asked.

465
00:53:30,850 --> 00:53:37,330
And it seems like a good starting point to me. And also you can the.

466
00:53:48,810 --> 00:53:56,190
These are great questions. Sorry to monopolize.

467
00:53:56,720 --> 00:54:01,800
No. Anyone else don't feel the. Okay?

468
00:54:02,130 --> 00:54:08,650
Yeah. Go for it. Yeah. This is just a follow up, but one of the things that he answered, he said that in their paper, they cut off everything.

469
00:54:08,670 --> 00:54:14,520
I think below 70. Now, that paper was in specifically non ICU settings.

470
00:54:15,080 --> 00:54:21,770
And we're in ICU settings. So I guess I'd like to follow up to that.

471
00:54:30,990 --> 00:54:35,190
I o this BMJ paper was in an ICU settings.

472
00:54:40,050 --> 00:54:44,700
I am not qualified to answer that question, but I can follow up with him.

473
00:54:46,930 --> 00:54:51,140
You are absolutely right that you set the true that it would be much lower.

474
00:54:52,040 --> 00:55:10,800
That's what you. So the question is basically does does a threat like I realize he's not married to 70% here, but like if there is a threshold,

475
00:55:11,310 --> 00:55:18,300
does one expect that to kind of vary depending on if your population is like hospitalized more broadly versus ICU?

476
00:55:18,390 --> 00:55:21,850
Is that the basic? Yeah. I think that he thinks that.

477
00:55:27,500 --> 00:55:32,320
Of our group of. Mm hmm.

478
00:55:35,900 --> 00:55:42,320
Is this cut up for the defense or because from the question it's not clear, uh, the absolute minimum.

479
00:55:44,700 --> 00:55:49,169
I think it's absolute, but I'm not sure what she's referring to or both. It's cut off from the papers.

480
00:55:49,170 --> 00:56:18,610
Both. Okay.

481
00:56:20,620 --> 00:56:24,070
Should we take a. Five minute break.

482
00:56:25,520 --> 00:56:30,180
Any other. Yes.

483
00:56:30,650 --> 00:56:39,320
So. So just how they.

484
00:56:46,020 --> 00:57:03,770
Now that we know we. So it's the question.

485
00:57:03,770 --> 00:57:08,180
The question is that the decline between the two measurements, whether and how this time between.

486
00:57:09,000 --> 00:57:11,310
Affect the difference with the error that you see.

487
00:57:11,910 --> 00:57:17,510
So the question is that does that relationship could it be that there is no association in the white,

488
00:57:17,550 --> 00:57:21,090
but there is more association in other subgroups?

489
00:57:22,510 --> 00:57:27,130
So whether it's a marginal question or it's something specific.

490
00:57:31,760 --> 00:57:37,090
Hmm. Do you know? Do you know which time it ended?

491
00:57:40,020 --> 00:57:43,550
I do not think that he was thinking something.

492
00:57:44,720 --> 00:57:45,860
But having said that,

493
00:57:45,860 --> 00:57:54,470
I don't think that you have to talk too much about this question to see that maybe a race track like the Indians Association system would be.

494
00:57:54,650 --> 00:58:01,680
I do not know. We looked at the margin of error, and I looked at the margin rate.

495
00:58:01,890 --> 00:58:09,310
Mm hmm. The possibilities are endless.

496
00:58:13,040 --> 00:58:18,439
Statistician looks into. But it looks.

497
00:58:18,440 --> 00:58:21,870
That was. Now.

498
00:58:24,520 --> 00:58:28,850
So. So it says for mom that.

499
00:58:36,110 --> 00:58:40,690
Yeah. I think the logical statement, the proper logical statement is.

500
00:58:41,120 --> 00:58:46,400
And now or. So it's basically he does.

501
00:58:47,510 --> 00:58:51,650
And I don't think you have a sample size specific to each possible.

502
00:58:58,300 --> 00:59:05,920
So one interesting study that people said to do is that it appears to be sort of a measure of pigmentation.

503
00:59:06,750 --> 00:59:10,970
Then if you look at the differences across different ethnicities that multisystem.

504
00:59:12,620 --> 00:59:31,760
If there's a difference between cities. But I think we have the same. So we know.

505
00:59:38,180 --> 00:59:44,480
Let's say I want to make like. It's a great step.

506
00:59:44,810 --> 00:59:51,220
But to they go. Initiated onto.

507
00:59:52,830 --> 00:59:57,190
And they leave after that. They have been waiting for the arterial line.

508
00:59:58,690 --> 01:00:00,849
We've got a quick just reading.

509
01:00:00,850 --> 01:00:08,770
Just because I have oxygen information and I know the observations were within 10 minutes, I'm going to close this one.

510
01:00:09,070 --> 01:00:21,610
But that would be like a modified. I would.

511
01:00:21,730 --> 01:00:28,090
I. I don't know if I would call it a bias, because I think you can measure I think you have data to measure that potentially.

512
01:00:28,100 --> 01:00:29,560
I think it's a great thought, though.

513
01:00:29,590 --> 01:00:37,720
Like I think if you act on a low pulse acts and then took arterial blood gas, you would expect it to be reflected.

514
01:00:38,180 --> 01:00:43,900
But I guess I understand the bias. But, um, my question would be like would it be?

515
01:00:46,760 --> 01:00:50,110
I'm assumption is made out to be no. 88 people.

516
01:00:52,560 --> 01:00:56,010
So I think 88 and 92 are the important thresholds.

517
01:00:56,570 --> 01:00:59,920
That. And I think that's probably right.

518
01:00:59,950 --> 01:01:03,390
If. You know, 88. It would be reasonable.

519
01:01:04,200 --> 01:01:08,470
Yeah. That they received.

520
01:01:08,630 --> 01:01:12,140
Yeah. And I think the positions this like the.

521
01:01:18,410 --> 01:01:20,570
Yeah. I take that back about, you know,

522
01:01:20,690 --> 01:01:25,300
you probably don't have all of the data measure because you're saying you don't have whether they administered oxygen.

523
01:01:25,310 --> 01:01:33,750
So yeah, I would I would draw that comment. It's a good. Yeah. Okay.

524
01:01:36,130 --> 01:01:41,520
Should we come back at 205? All right.

525
01:01:51,920 --> 01:01:57,120
This is. You have to define what you're trying to get at.

526
01:01:57,300 --> 01:02:02,430
Right. What did you try to minimize for that? It was that graph.

527
01:02:02,730 --> 01:02:06,040
What percentage are below the level for blacks?

528
01:02:06,190 --> 01:02:14,850
So for me, if you go to the graph, I'm trying to what they were asking in some sense for his presentation.

529
01:02:16,820 --> 01:02:28,500
And one of the the side by side box plots that that figure to me if I am thinking I want to move that threshold of

530
01:02:28,500 --> 01:02:37,620
the blacks so that I can actually get to this the exactly the percentage that are separating the ones across this.

531
01:02:37,770 --> 01:02:41,720
That would be my goal. Right? That I cannot make it perfect.

532
01:02:41,730 --> 01:02:49,740
There will always be misclassification, but the proportion which are not getting it is much higher here than in here.

533
01:02:50,400 --> 01:02:55,530
And I want to make it similar to here across the board.

534
01:02:58,500 --> 01:03:03,740
I want to move the threshold in a variable way so that I can match.

535
01:03:03,750 --> 01:03:07,610
If somebody has that as a rule, that would be perfectly fair.

536
01:03:07,620 --> 01:03:10,860
Right, that I want this to be said.

537
01:03:10,920 --> 01:03:18,510
There is no question here on on a population level. I want to make that this this is this percentage that is below.

538
01:03:19,080 --> 01:03:23,190
I want to make that same across the two groups because that's my benchmark.

539
01:03:23,430 --> 01:03:35,040
I want equitable thing. Yeah, I should actually measure that.

540
01:03:36,000 --> 01:03:40,350
That's a really, really important one that I just include.

541
01:03:41,300 --> 01:03:49,440
Yeah. I think include an additional factor in your model for whether the skew is less.

542
01:03:49,530 --> 01:03:54,180
That's right. That's right here. Right. That's why they did not consider that here.

543
01:03:55,290 --> 01:03:59,100
Sure. Yeah. Right. In the false. It goes below.

544
01:03:59,460 --> 01:04:06,030
That's true. All you always have to do implicitly that like, yeah, there was no oxygen here.

545
01:04:08,430 --> 01:04:14,850
So I think we should point out to this and I think this is a very good goal that like, you know.

546
01:04:17,150 --> 01:04:38,040
We're going to be president on time. I'll try to be fast. That's right.

547
01:04:39,420 --> 01:04:45,850
Not just across party lines. This.

548
01:04:47,730 --> 01:04:55,340
The school, which was one of the things I don't think we're going to get too much if they don't start talking about this kid,

549
01:04:55,390 --> 01:04:59,320
they can get this one right? Yeah. Yeah. Okay.

550
01:04:59,570 --> 01:05:05,580
Yeah. And they filed episodes and so forth. One day I saw this open.

551
01:05:05,580 --> 01:05:10,770
That went up. Yeah. After. When you're done. And is their point of view.

552
01:05:10,820 --> 01:05:14,310
Oh, you know, I think Matt brings his own.

553
01:05:14,640 --> 01:05:17,670
I don't see why not. So we'll have to.

554
01:05:19,520 --> 01:05:23,480
Oh, yeah. We'll have to do it by hand, I guess, for today.

555
01:05:23,990 --> 01:05:27,190
Yeah, I have one. Yeah.

556
01:05:31,580 --> 01:05:38,730
Both. Regarding a first of all, it doesn't matter.

557
01:05:39,150 --> 01:05:42,220
We don't have to pass it along. Correct.

558
01:05:42,310 --> 01:05:46,210
So if you would just take that first, that that would be an acceptable.

559
01:05:47,110 --> 01:05:50,589
Okay. You can take first you can take this. You have to have one measurement.

560
01:05:50,590 --> 01:05:54,520
You can pick one at random. Yes.

561
01:05:54,550 --> 01:05:58,360
Yeah. I mean, you could take a range of one random. You.

562
01:05:59,980 --> 01:06:04,990
But one observation per patient. As long as you're not taking, like, an outcome dependent sample.

563
01:06:05,080 --> 01:06:10,990
Like, take the one with the largest and see to it that it is random.

564
01:06:11,530 --> 01:06:18,150
It could be random. That's not going to change the distribution of it.

565
01:06:18,460 --> 01:06:27,030
No. One thing I was thinking that even random is a little bit problematic because if a patient has these so many hours,

566
01:06:27,070 --> 01:06:35,730
such she's in the ICU and if you see one which is towards the later versus somebody who has this one that introduced some bias.

567
01:06:36,760 --> 01:06:39,890
So maybe because because the number of observations is in.

568
01:06:39,930 --> 01:06:46,059
So this is very interesting because in G we know that the cluster size is not enough for me to do that.

569
01:06:46,060 --> 01:06:49,330
But here the cost of size is implemented. Right.

570
01:06:49,840 --> 01:06:53,950
So if picking at random from a cluster may not be a good idea because you have like

571
01:06:53,950 --> 01:06:58,270
ten and you have one unequal probability that means you do some differential bias.

572
01:06:58,480 --> 01:07:02,440
I just thought about that. Yeah. Yeah, that's right. We should mention that potentially.

573
01:07:03,250 --> 01:07:08,760
Yeah. Just as soon as you get admitted if feel uniform.

574
01:07:09,350 --> 01:07:13,940
But then treatments happen and other things happen as you go. All right.

575
01:07:15,050 --> 01:07:19,220
Okay. We're going to get started. So we have two more things.

576
01:07:20,450 --> 01:07:24,829
The first thing that we're going to do is like it's just a short set of size.

577
01:07:24,830 --> 01:07:29,330
I will come I'm keeping it less than half an hour on this.

578
01:07:29,330 --> 01:07:36,469
But it's it's kind of like on statistical transformations, which you've probably seen before in various of your classes.

579
01:07:36,470 --> 01:07:39,470
But this is the sort of thing that kind of Yeah.

580
01:07:39,710 --> 01:07:47,300
Is useful to see. Again, definitely there could be some connections to your project when thinking or to future projects.

581
01:07:47,300 --> 01:07:56,090
So this isn't. There's always a risk or like an an effect that we observe in 699 where like if we lecture on a particular topic,

582
01:07:56,690 --> 01:08:03,229
I think there's that assumption that it's directly relevant to the project and it's it's just not that clean.

583
01:08:03,230 --> 01:08:05,600
There's not just like a 1 to 1 comparison like that.

584
01:08:05,600 --> 01:08:14,930
So, um, not that they're unrelated, but okay, so this is, this is taken from content that Jeremy developed.

585
01:08:15,590 --> 01:08:21,290
But like that, all this extended time professors have kind of fed into it. So what can you transform?

586
01:08:21,290 --> 01:08:28,369
You can you can do several things. But I'm talking about basically like transformations of data itself or of like summaries of data,

587
01:08:28,370 --> 01:08:32,329
which is another word, another description for statistics.

588
01:08:32,330 --> 01:08:36,200
Right. Is just just statistics is just data summarization.

589
01:08:36,570 --> 01:08:39,960
Right. That's how I, that's how I define that word. So, um.

590
01:08:41,690 --> 01:08:45,049
Thinking about a linear model and when you fed it.

591
01:08:45,050 --> 01:08:52,730
So transformations have an interesting history because it for my take they seem to have been developed

592
01:08:52,790 --> 01:08:59,419
to accommodate fitting linear models when there wasn't as much machinery for fitting fancier models.

593
01:08:59,420 --> 01:09:04,640
Right. So a lot of a lot of the papers and references in here are really old papers.

594
01:09:05,690 --> 01:09:12,349
And they are if you read them or or look at look at them at least like they all

595
01:09:12,350 --> 01:09:16,850
kind of seem to be focused on let's make this fit a linear model framework.

596
01:09:16,850 --> 01:09:25,070
Let's let's adjust our data so that we can better satisfy the assumptions of a linear model rather than today.

597
01:09:25,280 --> 01:09:29,109
We have a lot of other fields that are take a different approach,

598
01:09:29,110 --> 01:09:35,240
which is while the linear model assumptions are satisfied, let's think about bigger, more complicated sorts of models.

599
01:09:35,900 --> 01:09:47,260
So. When you transform your data or why you might transform your data, as I think these are.

600
01:09:49,340 --> 01:09:52,489
At least the first one, in my opinion, is probably the most important.

601
01:09:52,490 --> 01:09:59,149
The first and second one are kind of related, but like to to match the scientific context I think is always useful.

602
01:09:59,150 --> 01:10:08,600
That's one of our jobs as bio statisticians. Transformations can help you interpret things a little bit more naturally, and as I've mentioned already,

603
01:10:08,600 --> 01:10:13,880
there oftentimes lead to kind of better satisfying the underlying assumptions that you're making.

604
01:10:15,520 --> 01:10:21,650
So one of the classical transformations is called the Box Cox Transformation.

605
01:10:21,670 --> 01:10:30,340
It looks a little bit strange slash overwhelming at first, so it's kind of applicable to positive.

606
01:10:30,590 --> 01:10:37,080
Now it is applicable to two positive numbers, and it takes this form that you see here.

607
01:10:37,090 --> 01:10:44,350
So if it's if if lambda your parameter, your transformation parameter is not equal to zero,

608
01:10:44,770 --> 01:10:52,420
then it's Y to the lambda minus one all over lambda and to in order to make this a continuous transformation,

609
01:10:52,690 --> 01:11:02,140
if you let lambda go to zero, you can convince yourself that that transformation converges to the log of y.

610
01:11:02,650 --> 01:11:07,690
And so this is a continuously defined transformation over all values of lambda.

611
01:11:08,110 --> 01:11:13,239
The way that it's written out here use very usefully and importantly,

612
01:11:13,240 --> 01:11:18,580
this is a monotonic transformation so that if your two values of Y are ordered one way,

613
01:11:18,580 --> 01:11:21,969
then transforming them the same way is going to preserve that order.

614
01:11:21,970 --> 01:11:30,840
And that's what I mean by monotonic. I already actually gave away the punchline for why you divide by one, subtract one, and divide by lambda.

615
01:11:30,860 --> 01:11:34,730
That's what preserves this, makes it a continuous function as lambda goes to zero.

616
01:11:35,030 --> 01:11:39,189
You get the log. So here's just I'll refer to this.

617
01:11:39,190 --> 01:11:49,180
This is it. There's probably a built in function for the boxcars transformation, but this is just so I'll refer back to it in a couple slides.

618
01:11:49,510 --> 01:11:51,100
So this is what it actually looks like.

619
01:11:52,000 --> 01:12:00,050
Y is on the x axis, and I've just plotted what that transformation looks like for four different values of lambda negative one, zero one and two.

620
01:12:00,070 --> 01:12:06,430
And so it's you can see it's monotonic because these are all these are all functions still.

621
01:12:08,740 --> 01:12:16,810
And it kind of changes from concave to convex at lambda equals one, right?

622
01:12:17,650 --> 01:12:21,570
So and because one is just the straight line is like an identity transformation.

623
01:12:24,210 --> 01:12:32,550
So one of the classical uses of transformations is that they're useful for vers, for skewed variables.

624
01:12:33,390 --> 01:12:37,060
And so here's I just generated some skewed data.

625
01:12:37,080 --> 01:12:38,490
It's got a heavy right tail.

626
01:12:39,300 --> 01:12:48,450
If we take the log transform so the box backs transformation with lambda equal to zero, we kind of unscrew up and we get a little bit of left skew.

627
01:12:49,710 --> 01:12:54,930
If we take a different trace of lambda, even though as you saw there,

628
01:12:55,260 --> 01:12:59,100
the functional form looks quite different, it's a it's a continuous transformation.

629
01:12:59,100 --> 01:13:02,340
So it does something in between Lambda equal to zero,

630
01:13:02,370 --> 01:13:08,490
which was this one and the very first plot I showed you, which is essentially lambda equal to one.

631
01:13:10,730 --> 01:13:13,760
So that's just visualizing the box box transformation.

632
01:13:13,760 --> 01:13:17,510
That's all that says. There's nothing deeper in this beyond that.

633
01:13:19,860 --> 01:13:25,320
These are useful when you're in particular when you're fitting regressions.

634
01:13:25,740 --> 01:13:31,680
So basically the question that could come before you is, okay, I've got some data.

635
01:13:32,370 --> 01:13:46,560
I'm considering fitting a linear model to I to transform either my outcome Y or one or more of my predictors X as part of this model fit in process.

636
01:13:46,710 --> 01:13:57,650
So instead of this linear model that you see at the top, you might have a slightly different linear model where I change my y's or my axes or both.

637
01:13:57,660 --> 01:14:02,450
And of course that I've denoted that the parameters are going to take on different values.

638
01:14:02,460 --> 01:14:08,070
So that's why I'm using betas and alphas instead of Camus. They're not going to be the same things anymore.

639
01:14:11,520 --> 01:14:19,980
Okay. So there's there is another parameter now floating around, which is lambda that's controlling your transformation.

640
01:14:20,190 --> 01:14:26,700
Um, I guess I realize this is kind of a value statement and so this is a scientific statement,

641
01:14:26,700 --> 01:14:34,620
but typically like you're not interested primarily in choosing the very best lambda possible.

642
01:14:35,670 --> 01:14:40,350
It's obviously an important considerations and there's going to be some that are better and we'll talk about that.

643
01:14:40,350 --> 01:14:50,280
But it's it's a means to an end in that we're trying to get to a model that's better describing the the x y relationship.

644
01:14:53,110 --> 01:14:56,460
So, for example, here's some here's some made up data.

645
01:15:03,390 --> 01:15:06,720
X say is my predictor. It's on the x axis.

646
01:15:07,020 --> 01:15:11,970
Why is my outcome on the y axis? And I have shown you a scatterplot.

647
01:15:12,740 --> 01:15:16,290
Uh, there there seems to be a there's a clear relationship.

648
01:15:16,860 --> 01:15:20,240
Um. Probably nonlinear.

649
01:15:20,990 --> 01:15:25,190
And so I guess the question for you, as for you all,

650
01:15:25,190 --> 01:15:34,670
is should we should we kind of try to make this a linear relationship by stretching out acts as opposed to say, sorry?

651
01:15:34,970 --> 01:15:45,020
Should you stretch out the X axis? And hopefully you can kind of envision how that with linear is it or should we pull down the wires,

652
01:15:45,020 --> 01:15:50,600
pull down the large wires, which should also be another way to do linear as it.

653
01:15:52,080 --> 01:15:55,770
So what would you what's your initial reaction? What's your gut reaction to that?

654
01:15:57,300 --> 01:16:04,410
To those two alternatives. Say it again.

655
01:16:06,030 --> 01:16:09,540
Take the log away. So you're suggesting more generally to transform?

656
01:16:09,540 --> 01:16:13,340
Why? Maybe take the log, maybe take a different value of the box.

657
01:16:13,350 --> 01:16:19,890
COX Transformation. Do you have. It's fine if you don't, but is there a particular reason why you think you should take the log?

658
01:16:21,390 --> 01:16:27,450
Jeffrey what I do. Why is.

659
01:16:28,250 --> 01:16:33,890
Part of the. Okay.

660
01:16:34,960 --> 01:16:50,880
Fair enough. So I agree with probably I would end up some transformation of Y if I can add on to what you say.

661
01:16:51,300 --> 01:16:56,100
The reason that it strikes me as perhaps the more natural approach here is

662
01:16:56,100 --> 01:17:02,880
that if you if you can draw a line of best fit with your mind on these data,

663
01:17:03,450 --> 01:17:07,050
what do the errors seem to do as X changes?

664
01:17:10,220 --> 01:17:15,440
The variance of those errors or those residuals, I guess, increase in with X.

665
01:17:16,040 --> 01:17:20,750
And so if we're changing X, we're not going to necessarily.

666
01:17:21,770 --> 01:17:28,310
Change the scope of those individuals and the cuts and the consequential effect, whereas if we change way,

667
01:17:29,210 --> 01:17:34,310
we would also be impacting input, hopefully introducing more homogeneous variants there.

668
01:17:36,530 --> 01:17:39,690
So how would you make a distribution? Instead.

669
01:17:40,050 --> 01:17:43,260
You could. That's my. That's my, my.

670
01:17:43,470 --> 01:17:51,770
This is a hot take, I realize. But, like, if if I was writing this 100 years ago and all I knew has had an ordinary,

671
01:17:51,790 --> 01:17:55,920
least squares, it'd be much easier not to deal with changing the distribution.

672
01:17:57,200 --> 01:18:03,410
I don't know how they actually calculated boxcar transformations, though, because that's I mean, I guess you could do it, but.

673
01:18:07,570 --> 01:18:14,379
Right. So okay. So getting back to the assumptions that this isn't a simple example of how transfer are transforming your data and kind of

674
01:18:14,380 --> 01:18:22,960
thinking carefully about specifically what you're going to transform can help you better satisfy linear model assumptions.

675
01:18:23,890 --> 01:18:34,720
So these specific these are kind of like it's interesting exercise if you Google linear model assumptions like the first ten hits like

676
01:18:35,110 --> 01:18:40,870
one of them will say like the three assumptions of linear models and the next one will say the seven assumptions of linear models,

677
01:18:40,870 --> 01:18:43,240
and then the other one will say the four assumptions of linear models.

678
01:18:43,660 --> 01:18:50,440
So I think depending on how you kind of define an assumption, it varies how many we're actually making,

679
01:18:50,440 --> 01:18:57,579
but there's kind of three fundamental ones, which is the linearity between Y and X, um, the error terms.

680
01:18:57,580 --> 01:18:59,320
So not the whys themselves,

681
01:18:59,680 --> 01:19:11,680
but the error terms have a constant variance and that the errors are also Gaussian distributed and transformations are a tool in our toolbox to.

682
01:19:12,730 --> 01:19:16,120
Better satisfy these assumptions when refuting the linear model.

683
01:19:19,690 --> 01:19:25,780
Okay. So I'm also introducing another question, which is how might how do you how do you choose lambda?

684
01:19:26,530 --> 01:19:29,950
Um, some sometimes there's contextual knowledge.

685
01:19:30,250 --> 01:19:39,700
So to my thinking, it makes the most sense when we log transform some things we know like cell, like cell growth.

686
01:19:40,840 --> 01:19:44,170
Are fundamentally logarithmic sort of operations.

687
01:19:44,560 --> 01:19:50,790
And so if your outcome is like. Some sort of like abundance measure.

688
01:19:51,390 --> 01:19:57,810
Then logging that and thinking that it's a linear thing is on the log scale kind of makes scientific sense.

689
01:19:58,800 --> 01:20:09,960
Um, you could potentially just eyeball it and compare some chance comparisons of Y transform against X or against X transform.

690
01:20:10,650 --> 01:20:15,060
That's going to become difficult pretty quickly. That's not really a sustainable solution.

691
01:20:15,390 --> 01:20:22,270
There's also and this is what they talk about in the original. Tran backs Cox paper and a couple of subsequent answers.

692
01:20:22,270 --> 01:20:28,450
You could you could treat it like a parameter that you've got data on and that you can estimate.

693
01:20:29,720 --> 01:20:33,410
Using maximum likelihood. So here's our likelihood.

694
01:20:33,920 --> 01:20:44,900
It's got one extra parameter in it. So and it's got this term called J, which is a hint for you all for my question, which is like what is J what?

695
01:20:45,260 --> 01:20:50,720
Why does my likelihood which other than that looks like a normal linear model likelihood?

696
01:20:51,170 --> 01:21:01,210
Why does it have this extra j term? The U.N. recognized what that is.

697
01:21:05,120 --> 01:21:09,140
You have to go back to 601. When you're changing a variable.

698
01:21:09,170 --> 01:21:14,930
Yeah, it's a Jacobean. That's right. So we're interested if if our question is,

699
01:21:14,930 --> 01:21:22,549
let's compare the likelihood of our data for different parameter resolutions that we have to incorporate

700
01:21:22,550 --> 01:21:28,130
that Jacobi in so that the likelihoods are comparable across these different transformations.

701
01:21:29,540 --> 01:21:34,429
So that it's the Jacobean. And specifically it takes this form.

702
01:21:34,430 --> 01:21:38,690
It's the derivative of a transformation with respect to the parameter.

703
01:21:39,260 --> 01:21:44,450
Um, and so it's proportional to this y to the lambda minus one.

704
01:21:46,400 --> 01:21:57,170
So with with that extra bit of knowledge, what we can do is this cool idea that is called profile likelihood.

705
01:21:57,530 --> 01:22:07,520
So that idea of a profile likelihood, it recognizes that for a fixed lambda we have an easy task in front of us,

706
01:22:08,000 --> 01:22:16,370
which is we can calculate the maximum likelihood estimates of data and sigma that we know maximizes the likelihood surface.

707
01:22:17,700 --> 01:22:21,840
Along that fixed lambda line. So a profiling.

708
01:22:23,870 --> 01:22:29,030
At multiple slices of our complicated likelihood surface that has beta and segment and lambda,

709
01:22:29,240 --> 01:22:34,640
and we're taking maximum likelihood slices along all of the different values of lambda.

710
01:22:35,330 --> 01:22:41,750
So we're turning our likelihood surface into a one dimensional likelihood function of lambda,

711
01:22:42,650 --> 01:22:47,720
because for any given lambda we can calculate the values of beta and sigma

712
01:22:48,080 --> 01:22:51,860
that are functions of that and then plug those into our likelihood function.

713
01:22:52,310 --> 01:22:58,820
And now we've turned it into a one argument function, and that's called the profile likelihood.

714
01:22:59,300 --> 01:23:03,980
And if we look across all of the different values of Lambda for this profile likelihood,

715
01:23:04,430 --> 01:23:10,700
we can use the information in our data to suggest to us what the best value of Lambda should be.

716
01:23:13,850 --> 01:23:17,420
Any questions on this concept of profile likelihood as much?

717
01:23:18,460 --> 01:23:22,960
Like uses beyond. Box cocks, transformations.

718
01:23:24,850 --> 01:23:31,900
How many of you have seen her by that? Yeah.

719
01:23:31,910 --> 01:23:36,200
It, it it wasn't something that I encountered in my kind of like.

720
01:23:37,810 --> 01:23:44,790
Score graduate courses. Can't speak to them now, but I'm not sure what if it was covered?

721
01:23:45,480 --> 01:23:48,720
Abby, can I call on you? Was it in your GSK stuff or was it.

722
01:23:50,190 --> 01:23:55,239
Okay. Okay. Okay.

723
01:23:55,240 --> 01:24:00,610
So let's go back to these data. These are these do they simulated data that I showed you before?

724
01:24:02,050 --> 01:24:06,910
Be cognizant of the time here. I just did a simple little sorry.

725
01:24:07,520 --> 01:24:14,410
To avoid using pejorative terms. I did a little bit of our code here because we're in a pretty simple situation of.

726
01:24:14,680 --> 01:24:17,800
Of just Y against one X and.

727
01:24:19,110 --> 01:24:25,620
If you run that our code with the data, which I realize I didn't give you the data so you wouldn't be able to run the Arco directly.

728
01:24:26,670 --> 01:24:33,030
This is what that profile likelihood looks like. So now we have just one parameter that we're varying,

729
01:24:33,570 --> 01:24:43,380
and each value of lambda gives the maximum likelihood across all of the parameters because we can calculate them all as functions of lambda.

730
01:24:43,950 --> 01:24:55,709
And what we see is that the maximum profile likelihood is some value of lambda is just above zero and and far from one,

731
01:24:55,710 --> 01:25:00,750
which would be like not transforming my error. And also it's coastal America zero.

732
01:25:00,760 --> 01:25:06,300
So the intuition to log transform y, uh, makes sense here.

733
01:25:06,330 --> 01:25:09,899
Right. Like, that's just consistent with our intuition to transform.

734
01:25:09,900 --> 01:25:10,170
Why?

735
01:25:12,920 --> 01:25:27,950
So then we plug that in and we can fit our model, which gives us on this transform value of Y that the intercept is negative to the slope is three.

736
01:25:28,400 --> 01:25:34,030
And just to confirm that, we're like obviously the P values are very small.

737
01:25:34,040 --> 01:25:35,509
There's a lot of signaling data.

738
01:25:35,510 --> 01:25:43,370
It's not a particularly hard statistical problem, but I'm just showing you how I generated these data and it is indeed on a log scale, right?

739
01:25:43,370 --> 01:25:48,200
So I calculate the linear term and then I exponential it to get my Y.

740
01:25:49,300 --> 01:25:53,200
And that court, if you bring the expert to the other side, that's just a log linear model.

741
01:25:57,850 --> 01:26:04,210
I'll call out here and glossing over a little bit of a thing here that we estimated, Amanda.

742
01:26:04,810 --> 01:26:08,320
And if you do inference using this table,

743
01:26:08,320 --> 01:26:16,420
you're ignoring the fact that we have actually estimated Lambda for implicitly treating it as something that we always knew all along.

744
01:26:17,410 --> 01:26:22,210
And that's that actually introduces a little bit of an uncertainty that's not captured here.

745
01:26:25,470 --> 01:26:28,650
Okay. So the tradeoff for doing this, though,

746
01:26:28,650 --> 01:26:33,959
is that now data is a little bit harder to interpret because it's it's a

747
01:26:33,960 --> 01:26:40,110
function of lambda in the sense that if we changed a different value of lambda,

748
01:26:40,620 --> 01:26:47,010
we would have calculated a different value of data because in the wise would have changed.

749
01:26:47,190 --> 01:26:51,659
So if beta had is 2.96, which I'm taking from that previous model,

750
01:26:51,660 --> 01:27:02,610
the interpretation is that every value of we expect Y transformed according to Lambda to increase by 2.96.

751
01:27:03,880 --> 01:27:12,280
Per unit increase in ex. So this doesn't tell us anything about why directly it's this transform value of why that's changing.

752
01:27:14,810 --> 01:27:18,830
So it's a little bit more difficult to interpret.

753
01:27:19,340 --> 01:27:21,590
We do have some options available to us.

754
01:27:21,620 --> 01:27:30,740
For example, you could take you take that predictions and back transform it using like the inverse of the box box transformation.

755
01:27:31,460 --> 01:27:41,510
And so, for example, if I plug in all of that and invert it, we would get an X equals point five that Y had is predicted to be about 0.6 to 9.

756
01:27:42,730 --> 01:27:53,350
I'm plugging in all these coefficients, so. How like how what can you offer any sort of interpretation of what of what why is.

757
01:27:54,630 --> 01:27:57,860
Um, specifically. Let me get a little bit more specific.

758
01:27:57,870 --> 01:28:05,820
Can we interpret this as a conditional mean of Y given X anymore like we can do in linear regression models?

759
01:28:15,240 --> 01:28:23,970
We can't, unfortunately. Sad face because we've taken all of these nonlinear transformations to get back to the Y scale.

760
01:28:24,570 --> 01:28:30,000
And so Jansen's inequality tells us that these are not the same anymore.

761
01:28:30,690 --> 01:28:40,020
However, because we're still in a monotonic function, we can interpret this still as the predicted median of Y given x.

762
01:28:40,260 --> 01:28:46,020
So we just would have to provide a little bit more nuanced interpretation of this.

763
01:28:49,480 --> 01:28:54,670
Okay. I'm going to skip over that because that's a little bit of an aside.

764
01:28:54,670 --> 01:29:00,910
I want to make sure we have time. So that was a discussion on transforming y.

765
01:29:02,080 --> 01:29:10,180
You could also consider transforming X and and so a question for you is, do we need if we're considering.

766
01:29:11,480 --> 01:29:15,950
Only looking at the space of transforming X across different lambdas.

767
01:29:16,430 --> 01:29:22,600
Do we still need to include the Jacobean or not? I see a slight head shake.

768
01:29:23,750 --> 01:29:28,520
I agree with that. So because we're no longer transforming our outcome.

769
01:29:29,850 --> 01:29:38,460
Ah and the likelihood is, is with respect to the outcome, given the access, we don't need to incorporate this Jacobean transformation any more.

770
01:29:41,810 --> 01:29:53,750
Um, here are a couple extensions which are very similar in spirit to this concept of, of transformations along the lines of box cox.

771
01:29:54,200 --> 01:29:59,900
So there's these things called spines, basis spines or penalized regression spines.

772
01:30:00,350 --> 01:30:09,230
And if you you can have those all fall under the umbrella of generalized additive models where basically you assume that your axes.

773
01:30:09,650 --> 01:30:12,830
So they're additive in the sense of like they.

774
01:30:13,010 --> 01:30:20,299
Their effect adds up to the expected value of Y but the individual is the

775
01:30:20,300 --> 01:30:25,790
individual function which is denoted by G here is allowed to be flexibly defined.

776
01:30:25,790 --> 01:30:29,630
It can kind of be wiggly. It can go up. It can go back down again.

777
01:30:31,490 --> 01:30:34,730
And that uses these powerful things called spines to do all of that.

778
01:30:35,840 --> 01:30:47,360
There's this paper called The Box, Tidwell by Box and Tidwell that kind of automates the box COX process when you're transforming multiple axes.

779
01:30:47,360 --> 01:30:53,209
So this is really this is really kind of like a probably a paper that wouldn't be impactful

780
01:30:53,210 --> 01:31:00,740
today because we have more tools in our toolbox for complicated relationships between acts.

781
01:31:00,740 --> 01:31:07,729
But basically that the idea was when you recognize that Y given X is not like it's not linear in X,

782
01:31:07,730 --> 01:31:12,680
but probably linear in some strange combination or transformations of x.

783
01:31:12,980 --> 01:31:17,900
It's like a whole automatic process to F to calculate where all your transformations should be.

784
01:31:19,200 --> 01:31:26,130
And then there's a shifted box. COX Which just allows you to transform things that might have negative.

785
01:31:26,700 --> 01:31:32,320
You have additional location parameter involved. Okay.

786
01:31:36,760 --> 01:31:40,549
Last thing I want to. Talk about.

787
01:31:40,550 --> 01:31:44,060
I think just because this one would be more useful to you, I to skip over.

788
01:31:46,490 --> 01:31:50,780
Yeah. I'm a skip over the Fisher transformation. I'm just talk about binomial proportions.

789
01:31:51,110 --> 01:31:57,500
So this is one example of not transforming data per se, but transforming statistics of your data.

790
01:31:58,190 --> 01:32:04,940
So if we have a scenario where we've got data generated from a binomial distribution and we're

791
01:32:04,940 --> 01:32:10,280
interested in doing inference on the underlying probability in that binomial distribution,

792
01:32:12,020 --> 01:32:17,360
then some transformations might make that inference a little bit more sensible.

793
01:32:17,810 --> 01:32:28,400
So we know that p hat, this estimate of the proportion is asymptotically approaching a normal distribution with a certain large sample variance.

794
01:32:29,180 --> 01:32:35,960
And so if we have a large enough sample, we could just do the standard wald test directly on that proportion too,

795
01:32:36,170 --> 01:32:40,190
and also invert that to get a confidence interval. And we would be good.

796
01:32:41,300 --> 01:32:46,580
In smaller samples, particularly as the true proportion is close to zero or one.

797
01:32:47,090 --> 01:32:55,610
You you run into some awkward situations where first of your type one error is not nominal perhaps,

798
01:32:56,000 --> 01:33:04,430
or like equivalently, your, your confidence intervals are spanning going beyond zero or above one.

799
01:33:04,610 --> 01:33:07,340
So you run into some strange situations that way.

800
01:33:07,850 --> 01:33:15,589
And so to that end, there's a couple transformations that either speed your convergence to normality,

801
01:33:15,590 --> 01:33:20,150
so they speed you along the central limit theorem results to get to normality sooner.

802
01:33:21,150 --> 01:33:26,850
And they also make sure that your confidence bounds are within the appropriate proportions.

803
01:33:27,030 --> 01:33:31,950
So there's the arc sign. There's something that probably is more familiar to you called the logit transformation.

804
01:33:33,150 --> 01:33:39,290
And if you. Apply like a Dalton methods type argument.

805
01:33:39,350 --> 01:33:47,740
You can you can perhaps convince yourself that these transformations are also normal,

806
01:33:47,750 --> 01:33:52,280
well, are also normally distributed, and they have this particular large sample variance.

807
01:33:52,790 --> 01:33:57,079
And what's interesting, especially about the arc side and what makes it rather unique,

808
01:33:57,080 --> 01:34:03,500
is that the variance after this transformation doesn't actually depend on the true proportion.

809
01:34:03,500 --> 01:34:09,020
It's got a constant variance. So sometimes it's called a variance stabilizing transformation,

810
01:34:09,740 --> 01:34:15,620
because the variance of this transformed estimate no longer depends upon the true unknown value.

811
01:34:17,320 --> 01:34:24,450
And so I just gave a simple example that compares the three options, the wild example, the arc and the logistics.

812
01:34:24,460 --> 01:34:28,180
So if we have 25 observations draws.

813
01:34:28,180 --> 01:34:35,120
Yeah, yeah. And it's just like the beach.

814
01:34:38,740 --> 01:34:44,310
Oh, yes. Thank you. There should be a parentheses here after the midget as well.

815
01:34:44,410 --> 01:34:48,430
And also after the logo. Thank you. Yep. Yep, that's exactly right.

816
01:34:48,430 --> 01:34:54,100
The mean is budget a p and our cosine of rupee and the variance is the rest of that term.

817
01:34:56,530 --> 01:35:06,249
So I just I calculate the wald interval, I calculate the arc sign using that result that I just showed you before, but then I reverse transform it.

818
01:35:06,250 --> 01:35:15,310
So I take that inverse the sign and square it to get my confidence interval back on that probability scale.

819
01:35:15,670 --> 01:35:22,030
And similarly with the logistic, I take the logit function, calculate the world, and then I expect it.

820
01:35:22,030 --> 01:35:27,249
So I inverse logit it. And you can see that in this example which I cherry picked so that we would

821
01:35:27,250 --> 01:35:32,829
get this the confidence interval using just the naive wild drops below zero,

822
01:35:32,830 --> 01:35:36,700
which is going to be a little bit strange looking, it is a little bit strange looking,

823
01:35:37,060 --> 01:35:43,990
whereas the other two transformations, not only are they within the bounds, which you can see,

824
01:35:44,470 --> 01:35:53,520
but something that we can't see is that they approach their nominal coverage much, much faster with N than the standard wild interval.

825
01:35:53,530 --> 01:35:56,650
So that second thing I said, you can't just see from looking at these.

826
01:35:59,470 --> 01:36:00,910
So I guess the takeaway,

827
01:36:00,970 --> 01:36:11,320
a simple takeaway that I would encourage you to think about is you'll definitely be doing inference on proportions in your work.

828
01:36:12,100 --> 01:36:17,320
Think about fitting a logistic regression, and it's very helpful to calculate.

829
01:36:18,250 --> 01:36:20,200
Intervals. I like the real scale,

830
01:36:20,200 --> 01:36:27,250
so like the log odds ratio scale and then transform back to the probability scale after you calculate those confidence intervals.

831
01:36:28,060 --> 01:36:31,270
Like here. And then you're you're Garrett.

832
01:36:31,270 --> 01:36:35,500
You're kind of ensuring that you'll be within the bounds of your proportions.

833
01:36:37,590 --> 01:36:45,350
So if you got nothing else from this lecture, just. That last thing will be useful to you in your in your career.

834
01:36:49,080 --> 01:36:53,640
Any questions? Okay.

835
01:36:59,210 --> 01:37:05,090
So I just wanted to point out before we close that. Oh yeah, this is bigger.

836
01:37:05,100 --> 01:37:08,300
I think you ask this question. That's whether we are.

837
01:37:09,080 --> 01:37:12,680
Sorry. Sorry. That not that one. This way. Yeah.

838
01:37:12,710 --> 01:37:17,810
You asked, like, whether we can assume that everybody has been oxygenated below.

839
01:37:17,810 --> 01:37:23,240
So probably that's why that when people do decisions initially, probably is the pulse oximeter.

840
01:37:23,630 --> 01:37:28,340
And so you see that it's does not include anybody below that.

841
01:37:28,850 --> 01:37:33,690
And I also wanted to point out the point that I was trying to make that there are two groups, right?

842
01:37:33,710 --> 01:37:37,810
So if you are applying data, so your goal is do that.

843
01:37:37,820 --> 01:37:47,210
The actual measurement is falling 88 and still you are getting numbers who are above 88 on the measurement that you're using in the pulse oximeter,

844
01:37:47,600 --> 01:37:54,590
that proportion. You see that like you know which you are misclassifying is going to be there.

845
01:37:54,830 --> 01:37:59,330
But is it similar in blacks and whites?

846
01:37:59,480 --> 01:38:02,480
Right. So that's there is a comparison group here.

847
01:38:02,750 --> 01:38:06,170
Don't forget that. That you're trying to minimize this below.

848
01:38:06,410 --> 01:38:10,010
But you are trying to think about like, what is the comparison?

849
01:38:10,220 --> 01:38:15,570
This is a very telling thing for the problem. Yes.

850
01:38:15,780 --> 01:38:20,470
Would it be inappropriate for us to values.

851
01:38:21,200 --> 01:38:26,490
I mean, because the the.

852
01:38:28,300 --> 01:38:33,050
As always after. I'm not going to answer that question.

853
01:38:35,440 --> 01:38:42,780
Ben, do you want to answer that question? Whether they should discard anybody below it.

854
01:38:43,090 --> 01:38:47,130
The. Discard is a strong word.

855
01:38:49,450 --> 01:38:56,209
But I mean, I guess you saw that's what they did. So I feel like I can't give advice towards discarding.

856
01:38:56,210 --> 01:39:01,430
But if you decide that it's not useful for answering your question, then you could.

857
01:39:02,240 --> 01:39:12,650
Then I could see that argument being made. All right, our last thing.

858
01:39:12,780 --> 01:39:16,390
Okay. Well, we have 11 minutes for this hour.

859
01:39:16,400 --> 01:39:22,570
So just before I. So when we talk about collaboration, go ahead from our.

860
01:39:23,650 --> 01:39:32,200
Yeah. So I thought that like, you know, I guess we did that is going to take my time and so we are not going to have enough time.

861
01:39:32,200 --> 01:39:36,640
So we thought that instead of. Talking to you.

862
01:39:37,090 --> 01:39:40,990
We are going to perform a skit which we actually wrote.

863
01:39:41,320 --> 01:39:47,860
I am Professor Jimmy Herring, who is a do or in junior researcher workshop to show people that what a

864
01:39:47,860 --> 01:39:52,390
collaboration our conversation between a scientist and statistician looks like.

865
01:39:53,050 --> 01:39:58,150
So that's what we're going to do. And this so you have to pardon that.

866
01:39:58,150 --> 01:40:00,820
We have not rehearsed it before because I was out of town.

867
01:40:01,660 --> 01:40:12,100
But Phil is going to be the kind statistician and I thought is much better and we should do that.

868
01:40:12,430 --> 01:40:16,820
Well, let's. We used to use our mates.

869
01:40:20,900 --> 01:40:26,629
You can hear this, right? Even without that. No.

870
01:40:26,630 --> 01:40:30,440
No. Oh, come on. Hello, Doctor.

871
01:40:30,440 --> 01:40:35,290
Please drop. Thank you for your time. I just saw you.

872
01:40:35,480 --> 01:40:39,230
Oh, yeah, yeah, yeah. We had a good. You know, thank you for your time.

873
01:40:39,440 --> 01:40:45,379
I this is. This is a good idea. I'd like you to rescind the review, and we would like your advice.

874
01:40:45,380 --> 01:40:49,430
Really, really appreciated advice as we prepare the resubmission.

875
01:40:50,820 --> 01:40:53,850
Oh. Oh, I see. So this was. This was a grant.

876
01:40:53,920 --> 01:40:59,850
Yeah. And it was recently reviewed. Yes. Can you can you tell me a little bit more about what you propose to do?

877
01:41:00,630 --> 01:41:09,300
Oh, yes, of course. Of course. We're basically current predictors of brain injury, rely on external measurements of head motion.

878
01:41:09,720 --> 01:41:14,190
However, measurements of brain motion could predict injury more accurately.

879
01:41:14,190 --> 01:41:18,719
You understand? You have that. We have developed fantastic, fantastic novel,

880
01:41:18,720 --> 01:41:27,570
ultrasonic methods and motion tracking algorithms that can better predict brain most brain brain motion, and therefore, in turn,

881
01:41:28,170 --> 01:41:38,030
brain injury in our grant to show that these methods can be used to show for the first time shock wave propagation in the brain,

882
01:41:38,430 --> 01:41:42,239
which could transform how we view mechanism of brain trauma.

883
01:41:42,240 --> 01:41:47,340
And the whole would. Wow. So that sounds very interesting.

884
01:41:47,370 --> 01:41:51,360
Yeah. Could you tell me about your specific study, please?

885
01:41:52,140 --> 01:41:55,620
Well, we went. Yeah, I have some slides. Well, yes.

886
01:41:55,620 --> 01:41:59,550
Here. You see that? Our special instruments right here,

887
01:42:00,450 --> 01:42:09,990
we are going to very special ultrasound machine and we are going to induce transit transducer to the head and then apply some forces,

888
01:42:10,410 --> 01:42:15,060
blunt or blast forces, and then measure the response.

889
01:42:15,420 --> 01:42:20,070
That's the study. I see. So these are human heads.

890
01:42:22,410 --> 01:42:29,070
Oh, yes, yes, yes. Yes, I do. Ideally, we will have a few.

891
01:42:29,370 --> 01:42:40,260
In fact, you can see a green. You know that that's right there that we have quantifying how the waves move through this gelatinous material.

892
01:42:42,270 --> 01:42:46,140
Have you spoken with the Institutional Review Board about this study?

893
01:42:47,790 --> 01:42:51,950
Oh, no. But those are quite expensive.

894
01:42:51,970 --> 01:42:56,640
Do and we really need to conduct experiments in living tissue.

895
01:42:57,120 --> 01:43:03,870
So the bulk of the experiments will be done in pigs.

896
01:43:05,550 --> 01:43:16,350
Okay. Right. So and then so and then we have consulted with the animal subjects, research experts to ensure that we do it to protocols to fix.

897
01:43:16,670 --> 01:43:20,160
Okay. That's that that's much better. It makes a lot more sense.

898
01:43:20,430 --> 01:43:23,820
So now can you tell me a little bit about your design of your own?

899
01:43:24,060 --> 01:43:33,270
Oh, of course. Of course. Of course, yes. So each pig will surgically attach the transducers to the head and then half the

900
01:43:33,270 --> 01:43:39,270
things will get the blunt force applied and half will be subject to the breast.

901
01:43:40,570 --> 01:43:48,310
Then Admiral sacrificed and measured the damage to show that we can predict where it would.

902
01:43:49,990 --> 01:43:54,330
So what's you said blunt force and you said blast force.

903
01:43:54,340 --> 01:44:04,360
Yes. So what's the difference between these two forces? Well with the blunt force was successively applied Peter and Greta forces to the scope.

904
01:44:05,770 --> 01:44:12,630
The last was well. Okay.

905
01:44:13,470 --> 01:44:17,460
So now cutting back on, how many pigs will there be?

906
01:44:18,000 --> 01:44:25,290
Well, in that respect, we are very lucky. The budget will allow us to have eight pigs.

907
01:44:26,290 --> 01:44:29,890
Okay. So it takes per group. Oh, goodness. No.

908
01:44:29,920 --> 01:44:35,760
Total. Okay. All right, let me just write that down.

909
01:44:35,910 --> 01:44:42,300
So how how are you going to determine the damage tissue when all the pigs are injured?

910
01:44:44,900 --> 01:44:48,960
Mm. God, I don't think.

911
01:44:51,060 --> 01:44:57,030
So here's an idea. One thing that you can do is you can look across different regions of the brain.

912
01:44:57,870 --> 01:45:04,030
And, for example, um. Maybe there are some areas where you would predict there to be injury.

913
01:45:05,060 --> 01:45:08,120
And you can measure the actual injury and others you could predict.

914
01:45:08,540 --> 01:45:12,890
You might not expect there to be injury and also measure the observed injury that.

915
01:45:14,000 --> 01:45:21,990
That makes a lot of sense, Dr. Boonstra. But probably as I think about it.

916
01:45:23,510 --> 01:45:27,110
Just brain surgery is naturally traumatic to the brain.

917
01:45:28,200 --> 01:45:40,560
So what? Could you have a control pain so that you can know if the damage is just due to the surgery itself rather than the force?

918
01:45:41,430 --> 01:45:45,620
Oh, great. Yes, we love that. I'm sure this idea of a grouping.

919
01:45:45,960 --> 01:45:50,280
I'm sure the budget will allow us to add one thing for that purpose.

920
01:45:50,590 --> 01:45:56,860
What? Well, yes. Just one. All right.

921
01:45:56,860 --> 01:46:01,560
So I understand the study design. What about the outcome measures now?

922
01:46:01,570 --> 01:46:04,690
How how are you going to quantify brain injury?

923
01:46:05,290 --> 01:46:09,190
Well, we are going to use the School of Medicine, Histology, Research Group.

924
01:46:12,520 --> 01:46:15,580
Okay. Let me ask it a different way. Um.

925
01:46:17,030 --> 01:46:21,490
What are they going to measure? They're going to measure brain injury.

926
01:46:21,640 --> 01:46:25,340
What else? But how?

927
01:46:25,430 --> 01:46:28,880
Like, how? How will they quantify?

928
01:46:29,720 --> 01:46:34,370
That that degree of injury. So if they're going to look at the picture.

929
01:46:36,150 --> 01:46:40,350
Like the picture that you've got up there. How do they decide which are injured?

930
01:46:41,430 --> 01:46:44,550
Which are? How are they going to measure that degree?

931
01:46:44,580 --> 01:46:48,120
Is there going to be a score? Will there be a threshold?

932
01:46:49,860 --> 01:46:55,439
So this is this is an important part of what I'll be doing as I work with you

933
01:46:55,440 --> 01:46:59,760
as a statistician is we need to quantify what the actual outcome will be,

934
01:47:00,540 --> 01:47:07,110
because that directly impacts the analysis that we're able. So it has to be a number or a threshold or something.

935
01:47:08,360 --> 01:47:11,900
You know, I see that. I'm not really sure what the measure.

936
01:47:12,800 --> 01:47:19,190
Why don't we set up a follow up meeting with the point person in the histology research core to learn more about this.

937
01:47:19,370 --> 01:47:23,000
Yeah, that's a good that's a good idea. That would be very helpful for me.

938
01:47:23,480 --> 01:47:27,920
And then after we meet with them, then then we can draft an analysis plan together.

939
01:47:28,590 --> 01:47:34,170
Great. Thank you so much. I absolutely knew that you were totally the right person to call.

940
01:47:35,520 --> 01:47:40,200
So one more question. I saw it in your second game of your career and you were going to show.

941
01:47:40,530 --> 01:47:45,300
You said you were going to show that your method was better than current methods.

942
01:47:46,400 --> 01:47:52,220
For predicting brain injury based on where those other methods use head motion alone.

943
01:47:52,700 --> 01:47:56,749
So can you talk more about those competing methods and so that we can think about

944
01:47:56,750 --> 01:48:01,790
what sort of hypothesis we might be able to set out for this superiority test?

945
01:48:02,660 --> 01:48:14,540
Yeah. Actually all of those other methods are highly proprietary, actually, using the other methods far, far beyond the scope of this grant activity.

946
01:48:14,960 --> 01:48:24,200
My postdoc would have to go to France and study under another team for months and months just to implement one of those methods.

947
01:48:24,680 --> 01:48:27,960
So there is no way we can compare our method to other methods.

948
01:48:27,980 --> 01:48:39,430
This is out of the question. Okay. So what if what if you reword this aim so that you're not saying that you will show that the method is better?

949
01:48:40,960 --> 01:48:44,260
Because if I'm a reviewer and I'm looking at that,

950
01:48:45,010 --> 01:48:54,430
I'm going to be looking for the hypothesis test or some sort of statistical test to allow you to show that superiority.

951
01:48:54,790 --> 01:49:02,709
Could you maybe say something that you'll develop software which will allow for facilitating

952
01:49:02,710 --> 01:49:07,090
this comparison of these approaches in the future and make it more of a developmental aim?

953
01:49:08,030 --> 01:49:16,580
Oh, that's an excellent idea. Maybe that is why it's the grant that even if the statistical review are so upset.

954
01:49:17,480 --> 01:49:21,800
All right. So last thing. Let's talk timeline. When is your grant deadline?

955
01:49:22,640 --> 01:49:27,750
Well. You know, our score was in the top ten percentile with this proposal.

956
01:49:28,040 --> 01:49:33,570
So they're sort of outstanding. But they said that for the revision, we needed a statistician.

957
01:49:34,050 --> 01:49:38,140
So. So. So this grant will be funded.

958
01:49:39,280 --> 01:49:42,700
Well, you know, the lines are so tight, we're not sure.

959
01:49:42,700 --> 01:49:45,010
But of course, it's an excellent proposal.

960
01:49:45,220 --> 01:49:51,280
If it's not funded this time, we're pretty sure that it is going to fund it, get funded when we submit this in July.

961
01:49:52,250 --> 01:49:57,790
July is great. That gives us plenty of time to talk about questions that might arise.

962
01:49:57,790 --> 01:49:59,370
I'll probably have more as we get to know.

