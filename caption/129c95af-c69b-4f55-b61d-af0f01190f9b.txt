1
00:00:09,660 --> 00:00:12,780
Okay. I think we can start going over this.

2
00:00:13,740 --> 00:00:25,070
So you from last year. What is the appropriate counterfactual comparison of the effect of vaccination on measles incidence?

3
00:00:27,490 --> 00:00:33,330
So here we're looking for the counterfactual and we have one study.

4
00:00:33,340 --> 00:00:40,570
It's a longitudinal observational study, and we compare rates of measles in those who are vaccinated.

5
00:00:40,900 --> 00:00:42,460
We have a case control study.

6
00:00:43,630 --> 00:00:59,410
We have an experimental study where we randomize individuals over time and then we have an individual who we follow over time and then time traveling,

7
00:01:00,010 --> 00:01:04,660
going back to time zero because they don't get the vaccine and then see if they get the disease.

8
00:01:06,550 --> 00:01:09,880
So this is actually the counterfactual down here.

9
00:01:10,420 --> 00:01:14,659
I would say this is a counterfactual comparison, counterfactual comparisons.

10
00:01:14,660 --> 00:01:19,300
This is time travel. It obviously is counter to fact.

11
00:01:19,450 --> 00:01:22,480
So that's a new word. So it's not real.

12
00:01:23,140 --> 00:01:30,950
We have observational comparisons here and you can have an observational comparison in cross-sectional study,

13
00:01:31,010 --> 00:01:35,200
have it in a longitudinal study and in a case control study like older study design.

14
00:01:35,200 --> 00:01:40,780
So you've been learning both. And then of course, there can be experimental comparisons as well.

15
00:01:41,320 --> 00:01:45,129
So it's kind of maybe a bit of a distinction that we want to make in this class.

16
00:01:45,130 --> 00:01:52,210
As there is, there's observational comparisons, there's experimental comparisons, and then there's counterfactual comparisons.

17
00:01:54,340 --> 00:02:02,380
And again, I'll be hammering this point a lot. So, you know, if you're only slightly, slightly grasping it now or if you're still, you know,

18
00:02:03,070 --> 00:02:07,149
not really understanding it, I think there will be a lot more examples in today's class.

19
00:02:07,150 --> 00:02:13,090
And then also in future, can marginal structural models deal with unmeasured confounders?

20
00:02:13,090 --> 00:02:15,190
The answer is no.

21
00:02:15,610 --> 00:02:22,540
You kind of have to add on an additional method if you want to deal with unmeasured confounders like I think an instrumental variable analysis,

22
00:02:22,540 --> 00:02:25,840
that's a great way of dealing with unmeasured confounders.

23
00:02:27,040 --> 00:02:32,919
You know, you could do a quantitative bias analysis which you could kind of add on to a marginal structural model.

24
00:02:32,920 --> 00:02:37,450
But like at its base, a marginal structural model cannot deal with unmeasured things,

25
00:02:40,330 --> 00:02:47,590
do causal do causal inference methods, estimate per protocol effects that were intended to treat effects.

26
00:02:48,280 --> 00:02:51,640
They estimate per protocol, which is what most of you got.

27
00:02:53,290 --> 00:02:56,410
And again, the difference is per protocol is like what's actually happening,

28
00:02:56,590 --> 00:03:02,380
what are people's behaviors, what's like, what is their actual exposure levels like?

29
00:03:03,760 --> 00:03:08,890
And that is, I think what we want like in general for protocol is what we want.

30
00:03:09,430 --> 00:03:14,320
I mean tend to treat is just kind of what were they randomized to do in an experimental study?

31
00:03:14,650 --> 00:03:18,280
And we don't really care about the point of randomization per say.

32
00:03:18,460 --> 00:03:24,070
We only like the point of randomization because that theoretically should remove all confounders.

33
00:03:25,300 --> 00:03:34,690
So intent to treat is sort of we we deal with it because we get rid of confounding that way but we don't necessarily like that as an estimate,

34
00:03:34,690 --> 00:03:36,190
whereas we do like per protocol.

35
00:03:36,190 --> 00:03:46,180
Estimates suggest that in observational studies or in in in even in experimental study, we could introduce some residual confounding.

36
00:03:49,610 --> 00:03:50,839
In marginal structure models.

37
00:03:50,840 --> 00:03:57,850
Should a model accounting for confounding include inverse probability weights for variance in the model statement or both?

38
00:03:58,760 --> 00:04:03,829
And the answer is just inverse probability weights. And so this is like a question that always gets asked.

39
00:04:03,830 --> 00:04:08,540
And I feel like in some of the homework assignments in previous years, I was a bit confusing about this.

40
00:04:08,550 --> 00:04:12,590
Hopefully I changed my wording a bit, but you know,

41
00:04:12,590 --> 00:04:17,719
how you deal with confounders in a typical observational study is that you have a model

42
00:04:17,720 --> 00:04:23,810
statement and you include your confounders as covariates in a marginal structural model.

43
00:04:23,810 --> 00:04:28,790
You don't include them as covariates, but you do include them in your in your construction of weights.

44
00:04:29,960 --> 00:04:36,060
So you don't really need to do both. Okay.

45
00:04:36,060 --> 00:04:41,820
So we're developing a marginal sexual model. We're assessing the effect of X on Y.

46
00:04:42,660 --> 00:04:50,230
What are how do we develop the propensity towards our propensity score?

47
00:04:50,250 --> 00:04:56,850
Should be this one. It's the probability that we're in a certain exposure group given the confounders.

48
00:04:59,030 --> 00:05:02,470
So what if you got that again?

49
00:05:02,480 --> 00:05:07,820
The whole point of these propensity scores is to account for the confounding.

50
00:05:10,070 --> 00:05:15,320
So let me try to give an example of this.

51
00:05:16,100 --> 00:05:21,070
And I don't know if you follow with super well, but hopefully, you know, I do not have a whiteboard in here.

52
00:05:21,080 --> 00:05:30,560
So I'm here with my document camera and I will try to be let me know if this works or doesn't.

53
00:05:33,050 --> 00:05:42,200
So let's just say we have like a very simple example and we have are are dead where

54
00:05:42,200 --> 00:05:50,330
we are looking at the hydroxychloroquine we're seeing it does use of maybe like,

55
00:05:50,600 --> 00:05:57,230
you know, regular use of hydroxychloroquine. Does that affect your instance of malaria?

56
00:06:00,180 --> 00:06:08,500
But you and we have to confounder if you're just going to simplify it, we're going to say age and we are also going to say nationality,

57
00:06:08,510 --> 00:06:16,580
say we're doing like a study in Malawi and we're maybe thinking of, you know, using hydroxychloroquine as some sort of preventive.

58
00:06:18,680 --> 00:06:22,580
So age is a continuous variable. Nationality is the dichotomous variable.

59
00:06:22,940 --> 00:06:34,970
And this is an observational study. Okay, so what we can do is just do some descriptive of individuals.

60
00:06:35,390 --> 00:06:45,650
And so let's say for those who we again, this observational study for those who are not using hydroxychloroquine, if we look at age.

61
00:06:49,170 --> 00:06:58,410
And maybe, you know, people can range from like 0 to 80. In our study population, it's sort of like this.

62
00:07:01,360 --> 00:07:14,110
If we look among people who have been using hydroxychloroquine, it is more like this.

63
00:07:16,030 --> 00:07:22,630
And so we see the problem here, right? Because if age we see it's definitely related to hydroxychloroquine use.

64
00:07:22,990 --> 00:07:28,110
If age is somehow related to risk of malaria, then we have substantial amounts of confounding.

65
00:07:28,150 --> 00:07:32,860
And in a traditional approach, we would, you know, want to make sure it is included in our covariates.

66
00:07:35,720 --> 00:07:42,680
Does that make sense? Um. Let's think about nationality.

67
00:07:43,710 --> 00:07:54,360
Um. It may be just like the proportion of people who have citizenship in Malawi.

68
00:07:58,240 --> 00:08:06,780
And again, let's there's not really a bar or like a histogram like that we can do, but we could do.

69
00:08:07,660 --> 00:08:13,900
We could find out, maybe. Among those who don't use hydroxychloroquine, it's like 70%.

70
00:08:15,130 --> 00:08:26,740
In those who do, it's like 30%. So clearly people from outside of Malawi are the ones who are kind of driving people outside of Malawi.

71
00:08:26,740 --> 00:08:36,160
Yeah. Are driving hydroxychloroquine. Anything about how I portrayed things as anything be so far.

72
00:08:37,960 --> 00:08:43,120
So the lake thing about propensity scores.

73
00:08:46,480 --> 00:09:01,270
Is, you know, we will model the probability that somebody is using hydroxychloroquine based on age and nationality.

74
00:09:05,800 --> 00:09:11,860
And then we can, you know, divide it up. And what we might see is.

75
00:09:20,250 --> 00:09:23,550
Maybe we see something like this.

76
00:09:25,360 --> 00:09:31,419
Again, this propensity score is is like a made up number, but it's based on age, nationality.

77
00:09:31,420 --> 00:09:37,960
And the idea is that we're just able to kind of like remix the population and see if there's a concordance there.

78
00:09:38,590 --> 00:09:47,740
So this is sort of like ideal case because we see that there's an even distribution of people across the to different propensity to get this.

79
00:09:50,350 --> 00:09:56,440
And know that this is basically what we would want to be able to do in a randomized controlled trial,

80
00:09:56,920 --> 00:10:02,350
because in a randomized controlled trial, if you are just randomizing people into,

81
00:10:05,680 --> 00:10:10,419
you know, into using hydroxychloroquine or not, like the AIDS should be the same,

82
00:10:10,420 --> 00:10:13,630
but these ages should be the same in these nationality groups should be the same.

83
00:10:15,880 --> 00:10:21,760
So then in a way like it doesn't really matter that we do a propensity score because like these should be like equivalent to each other.

84
00:10:24,010 --> 00:10:27,100
Does this process sort of make sense?

85
00:10:27,640 --> 00:10:32,469
And I think one of the benefits of the propensity score is like, what if we did this in Iran,

86
00:10:32,470 --> 00:10:36,250
just making up a fake example, which is like a couple of different variables.

87
00:10:36,250 --> 00:10:42,730
But say we had see, what actually happened was.

88
00:10:46,970 --> 00:10:56,180
For our propensity scores if we have something like this.

89
00:10:59,230 --> 00:11:06,250
Then that wouldn't be great because then it'll be like separation of our responses and then like we,

90
00:11:06,250 --> 00:11:11,230
we just can't deal with the confounding the situation. So I think, you know,

91
00:11:11,260 --> 00:11:20,710
one of the benefits of like going through this process is that at least we're able to like see to what extent is confounding still a problem.

92
00:11:21,700 --> 00:11:25,570
And, you know, if we're just adding covariates into our model, we're really not thinking about that too much.

93
00:11:25,990 --> 00:11:30,100
But if we look at the propensity scores, we can compare them across the different treatment levels,

94
00:11:30,100 --> 00:11:36,520
across the different exposures and see how they're different. So we like this.

95
00:11:37,330 --> 00:11:44,540
We wouldn't like this. Okay.

96
00:11:45,290 --> 00:11:54,450
Um. You know, this is like a bit, you know, all new stuff.

97
00:11:54,460 --> 00:11:58,660
It's like thinking about counterfactuals is confusing.

98
00:12:00,040 --> 00:12:05,020
Any other questions before I start with the lecture for today?

99
00:12:09,130 --> 00:12:14,700
Yeah. Um, to this one.

100
00:12:15,200 --> 00:12:19,230
Okay. So you should only use the inverse probability,

101
00:12:19,830 --> 00:12:25,710
so you do not need to also like putting the covariates as confounders or within the confounders as compared to them.

102
00:12:25,830 --> 00:12:29,310
That's like our typical way that we deal with stuff in observational studies.

103
00:12:29,850 --> 00:12:35,550
You don't need to use both. Anyway, it's kind of like double dipping or, you know, like controlling for it twice.

104
00:12:42,190 --> 00:12:46,300
Okay. So here you will be here later in the class.

105
00:12:46,690 --> 00:12:49,840
We're just going to do attendance at least this week.

106
00:12:50,350 --> 00:12:56,169
We are going to do it based on like she'll have a sign in sheet so she'll go around seats to the group.

107
00:12:56,170 --> 00:13:00,160
So please do make sure that you sign that by the end of class. But you know, she's not here right now.

108
00:13:01,210 --> 00:13:10,180
Other things that we talked about, um, I appreciate that the assignments are due on Sunday.

109
00:13:10,180 --> 00:13:15,790
So many of you are just working on them over the weekend. That's probably how I would to, um.

110
00:13:16,120 --> 00:13:22,150
But I don't have like, any obligations for him to respond to your emails over the weekend.

111
00:13:22,530 --> 00:13:24,160
And like, if she's able to, that's great.

112
00:13:24,700 --> 00:13:34,329
But my recommendation is, if you had a specific question to her to email it by noon on Friday, you know, and all check my email regularly.

113
00:13:34,330 --> 00:13:38,830
But like, I also do not, you know, have a guaranteed response to you over the weekend.

114
00:13:39,540 --> 00:13:43,389
Um, but hopefully there's at least like a few days during the week that you'd be able to

115
00:13:43,390 --> 00:13:48,040
like touch on some of the homework and ask us any questions if that is necessary.

116
00:13:49,810 --> 00:13:53,970
Um. Yeah.

117
00:13:53,980 --> 00:14:06,250
And again, if you have some if you want to make up some points on your assignments, there is the makeup that's already on the canvas.

118
00:14:07,060 --> 00:14:12,010
How it works is like you don't need to do any make ups until like the last week of class.

119
00:14:12,010 --> 00:14:18,159
But if you I mean, I would recommend it once you get your homework back, if you want to do makeup, start doing it then.

120
00:14:18,160 --> 00:14:22,330
But I will not grade them until the end of this half term.

121
00:14:22,330 --> 00:14:27,340
So like if you if you wanted to talk to me about them, like, you know, feel free to do so.

122
00:14:27,340 --> 00:14:34,360
But otherwise, like if you have a make makeup, don't expect any like scores on that until the end of discussion.

123
00:14:41,300 --> 00:14:48,500
Okay. So we are talking about mediation and interaction, let's say mediation today.

124
00:14:48,500 --> 00:14:58,700
But um, confounding again, I think you're all very familiar with how to set up dags.

125
00:14:59,690 --> 00:15:02,230
Generally, we have this idea that the confounder, you know,

126
00:15:02,240 --> 00:15:09,770
the definition of a confounder is something which is causally related to the outcome, but it is not in the mediation.

127
00:15:10,220 --> 00:15:13,580
It is not a mediator of the pathway between the expert and outcome.

128
00:15:14,840 --> 00:15:21,169
So in general, that means that the confounder either causes exposure or there there's some common ancestor between exposure,

129
00:15:21,170 --> 00:15:25,100
accent, confounder, and sometimes we represent that with a double header zero.

130
00:15:28,130 --> 00:15:37,790
So one idea thinking about confounding is that it is when these extraneous factors, these third variables differ between the exposure groups.

131
00:15:37,790 --> 00:15:47,570
And that's exactly what I was showing you in in my little printout is that like if you have.

132
00:15:52,710 --> 00:15:58,500
Yeah. So this is like when things differ. So this is like confounding because there's a different distribution of,

133
00:15:58,500 --> 00:16:03,120
of like age among those with hydroxychloroquine or not or nationality, so on and so forth.

134
00:16:06,630 --> 00:16:10,680
Okay. So how can we measure confounding?

135
00:16:11,880 --> 00:16:15,210
I don't know. Have you heard of the 10% rule before?

136
00:16:15,630 --> 00:16:24,310
Okay, so the 10% rule, you know, basically what you do is you would compute an odds ratio without any confounders.

137
00:16:24,330 --> 00:16:29,280
You do it with the confounders and see, you know, does your value change where the 10% or not?

138
00:16:30,540 --> 00:16:33,240
I mean, that's very much like a rule of thumb.

139
00:16:33,960 --> 00:16:39,450
I don't think that you can like publish in the article and be like, oh, there's confounding because things change by more than 10%.

140
00:16:40,740 --> 00:16:44,190
And in fact, there was a recent article that came out in the Journal of Epidemiology,

141
00:16:44,490 --> 00:16:53,790
and they kind of had this figure that maybe 38% is a better number and they had this like formula industry for why that would be the case.

142
00:16:54,900 --> 00:17:02,760
Also, the 10% rule is a bit confusing, especially when you think of like odds ratios are like on a on a logarithmic scale.

143
00:17:02,780 --> 00:17:09,060
He probably would want to look at the beta values, but on exponential values, but still then it's like a bit confusing.

144
00:17:09,360 --> 00:17:16,049
The reason why I just like talking to you about this is like it's really, it's easy to think about confounding in a theoretical sense.

145
00:17:16,050 --> 00:17:21,390
Like what? How is the confounder theoretically related to exposure? How is that theoretically related to the outcome?

146
00:17:21,660 --> 00:17:29,070
But like when you get down to the nitty gritty, there's not like necessarily a great way of thinking about how we can measure it.

147
00:17:30,480 --> 00:17:31,560
This is just an aside.

148
00:17:31,800 --> 00:17:40,140
Well, linearity is an extreme example of confounding when there are two variables which are just really strongly related to each other,

149
00:17:40,410 --> 00:17:44,910
and so that we might have some cells which have very few.

150
00:17:46,140 --> 00:17:51,620
And by cells we kind of mean if we put together a two by two table, how many people are in each of them.

151
00:17:51,630 --> 00:17:59,410
So in some, you know, maybe we have a study, there is about 200 people in it, you know,

152
00:17:59,670 --> 00:18:04,950
and if we dichotomous education income, it just might be that people have had higher education levels.

153
00:18:04,950 --> 00:18:09,020
People are going to have more education levels and there's very few people in the offset.

154
00:18:09,090 --> 00:18:20,250
So here we say like the the cell counts are very low. And then that could be, you know, a problem in certain situations.

155
00:18:20,670 --> 00:18:25,260
How can we test for linearity? There's something called the variance inflation factor or the VIX.

156
00:18:26,490 --> 00:18:33,030
The problem with the variance inflation factor is that it really only has been operationalize well in a linear regression model.

157
00:18:33,480 --> 00:18:36,629
So here I have the SAS coding for it. You know, it's very simple.

158
00:18:36,630 --> 00:18:40,600
If you just have your product read, you can specify an option for a via.

159
00:18:42,270 --> 00:18:50,370
So what the variance inflation factor is see is like if you were to remove that variable, how much would the variance in the other variable change?

160
00:18:52,500 --> 00:18:57,330
And so, you know, here, because there's only two variables, the variance inflation factor will be the same across these two.

161
00:18:57,570 --> 00:19:01,140
And you know, what this exactly means does not really matter.

162
00:19:01,890 --> 00:19:10,500
But if you're trying to think, for instance, if there was a high degree of correlation between I believe this is looking at like a gender of a kid,

163
00:19:10,890 --> 00:19:22,710
and if they had a high respiratory rate and they think like it is, you know, like their comfort in, uh, like their degree of pain.

164
00:19:24,630 --> 00:19:29,760
This is a clinical study. I believe that I used to have a homework study based on the homework assignment based on.

165
00:19:30,180 --> 00:19:35,729
But we see variance inflation 1.2, basically low variance inflation.

166
00:19:35,730 --> 00:19:41,490
We kind of want to cut off of four or ten basically meaning that, you know,

167
00:19:41,760 --> 00:19:53,340
the variance in the other variables would change by four or ten times if we were to include versus remove this, this, this variable.

168
00:19:55,810 --> 00:20:02,590
They didn't have the whole idea behind this like linearity is that there's like a high degree of because these are highly correlated,

169
00:20:03,520 --> 00:20:08,560
they'll heavily influence each other's experiences.

170
00:20:11,710 --> 00:20:15,730
So, you know, if you have a large data set, it probably doesn't matter too much.

171
00:20:15,730 --> 00:20:21,670
So, you know, if you have 200 people, if there's only five or four people in the office, that's a big problem.

172
00:20:21,910 --> 00:20:23,620
Let's say you get 200,000 people.

173
00:20:23,620 --> 00:20:29,230
If you have five or 4000 people in the off season, that's probably not so much a problem, even proportionally, that's the same.

174
00:20:29,800 --> 00:20:34,050
But, you know, just having that extra number is useful.

175
00:20:35,530 --> 00:20:42,099
Also, sometimes, you know, there are a lot of social epidemiologist, for instance, who will recommend that you include education income,

176
00:20:42,100 --> 00:20:47,700
even though they're highly linear, just because they kind of touch on different aspects of socioeconomic status.

177
00:20:47,710 --> 00:20:54,160
So there may be a reason to include multiple variables which are highly linear in the same model,

178
00:20:54,460 --> 00:21:00,160
even if it doesn't, even if it might not make sense in other situations.

179
00:21:02,620 --> 00:21:09,399
So what do we care about linearity? We care about linearity when we're looking at the strength of association.

180
00:21:09,400 --> 00:21:13,420
So we are computing an odds ratio or risk ratio or rate difference.

181
00:21:15,180 --> 00:21:17,470
We're looking at the measure, then we care about it,

182
00:21:18,760 --> 00:21:23,920
but we do not necessarily care about linearity when we're computing some sort of propensity score.

183
00:21:24,490 --> 00:21:29,080
So, you know, for this test, over time, you have to do a propensity score.

184
00:21:29,710 --> 00:21:37,570
You wouldn't really test for COLANERI any time that you were trying to create a propensity score, but you might want to do that.

185
00:21:37,570 --> 00:21:42,130
If you you know, if you had continuous outcomes, you could do a linear regression model.

186
00:21:42,490 --> 00:21:49,480
You might want to test for linearity if you were looking to have some sort of like

187
00:21:49,490 --> 00:21:53,500
a beta estimate that once your that was the thing that you were interested in.

188
00:21:56,740 --> 00:22:04,150
So we also might like an extreme example of linearity would be positivity.

189
00:22:05,560 --> 00:22:20,620
And positivity is potentially shown by something like this where we're able to predict another variable completely based on another variable.

190
00:22:21,280 --> 00:22:28,720
So like in this situation, we're able to completely predict hydroxychloroquine use based on nationality and age.

191
00:22:29,050 --> 00:22:32,080
So we would say that there's potentially a violation of positivity here.

192
00:22:34,450 --> 00:22:38,330
Whereas this is not positive, this is like no violation of positivity.

193
00:22:38,350 --> 00:22:39,700
Here is a violation of positivity.

194
00:22:40,870 --> 00:22:55,000
Another example about this is see that you were doing a study on the effect of Medicare on somebody's health status.

195
00:22:57,580 --> 00:23:02,110
If we also included age in that model. And see, we just had like a very simple data set.

196
00:23:02,110 --> 00:23:05,530
So like we were just recognizing based on an age of 65,

197
00:23:07,000 --> 00:23:14,470
we would have a violation of positivity because everybody younger than 65 would not have Medicare in the model.

198
00:23:16,480 --> 00:23:23,860
So this is obviously like when we're including things in our model, we want to make sure that there are people like that in our exposure groups.

199
00:23:23,860 --> 00:23:31,030
There is a distribution of the confounders and you know, sometimes there might be situations where that's not the case.

200
00:23:31,990 --> 00:23:37,710
Again, for example, if you are studying Medicare and you're just, you know,

201
00:23:37,780 --> 00:23:45,400
trying to dichotomous age based on 65 above and below, everybody's going to be over 65.

202
00:23:50,460 --> 00:23:56,030
This is just like another wordy way of doing it, if what I'm saying doesn't make sense.

203
00:23:56,040 --> 00:24:08,279
This is Dr. Murray. She has a different way of talking about and again, this is a way of looking about positivity from last time.

204
00:24:08,280 --> 00:24:15,149
This is like what we want when these propensity scores are relatively prevalent between the different exposure levels.

205
00:24:15,150 --> 00:24:19,889
But here's there's like complete separation. This is a violation of positivity here.

206
00:24:19,890 --> 00:24:23,100
There's like sort of a violation of separation.

207
00:24:23,100 --> 00:24:28,140
So we we probably would have to cut off the people who are up here and the people down here.

208
00:24:28,530 --> 00:24:31,140
And then we just would have to say that's a limitation to our study,

209
00:24:31,410 --> 00:24:36,100
is that we're not able to generalize to people who have, you know, a really low propensity score.

210
00:24:36,120 --> 00:24:45,810
Really high propensity. Any questions before we dove into remediation?

211
00:24:48,840 --> 00:24:53,490
So a mediator in contrast to confounder mediators along the pathway from this merger to the outcome.

212
00:24:54,510 --> 00:24:58,410
And we can think of a few different things. There's the total effect. There's the direct effect.

213
00:24:58,710 --> 00:25:06,850
And there's the indirect effect. So the total effect is the model which does not include the leader.

214
00:25:07,740 --> 00:25:12,750
So generally in epidemiology, the total effect is kind of what we want to be estimating.

215
00:25:13,950 --> 00:25:19,050
I would say sometimes like in more of like the psychology and health behaviors, they'll often deal with mediators a lot more.

216
00:25:19,440 --> 00:25:24,600
I mean, in epidemiology, we can deal for mediators. But, you know, in many analysis, we just don't deal with mediators.

217
00:25:24,990 --> 00:25:25,740
So again, like,

218
00:25:25,740 --> 00:25:33,810
if you're I think the default is just to look at the total effects and that's what you should always be assuming unless somebody says otherwise.

219
00:25:36,830 --> 00:25:44,840
So the direct effect is when we adjust for the mediator and it's kind of like what doesn't occur through the mediator.

220
00:25:45,710 --> 00:25:49,850
So what parts of the exposure and the outcome does not occur through the mediator?

221
00:25:55,270 --> 00:25:59,530
And then the indirect effect is that halfway through the mediator.

222
00:26:02,530 --> 00:26:10,060
So the indirect effect in a way, it's sort of like a measure of the amount of mediation in your analytical sample.

223
00:26:10,510 --> 00:26:16,510
And there are two ways to calculate this, and this will be part of the homework assignment is calculating them both of these ways.

224
00:26:17,020 --> 00:26:25,840
So what you could do to get at this number, since there's not just like one estimate, there's not like one beat estimate that we can do.

225
00:26:26,440 --> 00:26:30,340
There's not one one part of your regression model that can estimate the effect.

226
00:26:30,820 --> 00:26:35,470
You can either take the total effect and then subtract out the indirect effects.

227
00:26:35,830 --> 00:26:39,070
That's one way of doing it. Or you can multiply across these arrows.

228
00:26:40,390 --> 00:26:46,900
And that's just kind of one way of thinking about a path analysis is whenever you're like going through a path, you multiply those numbers.

229
00:26:46,900 --> 00:26:50,020
It's not like you add them or what, not enough points.

230
00:26:51,220 --> 00:26:57,350
So these numbers should be relatively poor. Okay.

231
00:26:57,540 --> 00:27:05,450
So I've been talking a lot. Let me just push this back to you for small group discussion.

232
00:27:05,480 --> 00:27:17,030
As always, there should be a link to the Google Drive documents in your canvas page, but I would like you to think about this situation so often.

233
00:27:17,420 --> 00:27:23,080
Say that you're interested in the relationship between education and mortality.

234
00:27:23,090 --> 00:27:28,190
So you have this this long longitudinal study, a long term study.

235
00:27:29,690 --> 00:27:32,839
Do you think income is a mediator or a confounder?

236
00:27:32,840 --> 00:27:39,550
So, you know, groups discuss what this is and I'll give you maybe 5 minutes to discuss.

237
00:27:54,950 --> 00:28:00,650
But. Yeah.

238
00:28:13,210 --> 00:28:27,680
Also. Also.

239
00:28:42,930 --> 00:28:53,950
Funny for. And if you're anything but.

240
00:28:55,650 --> 00:29:02,510
Yes. There's more.

241
00:29:09,190 --> 00:29:20,410
The press release. So that.

242
00:29:24,840 --> 00:29:35,580
I that like. You know I mean I to have.

243
00:29:43,570 --> 00:29:49,380
33. I.

244
00:29:56,010 --> 00:30:00,530
Know. Yeah.

245
00:30:06,920 --> 00:30:34,570
Yeah. Oh.

246
00:30:45,050 --> 00:30:59,000
That can be done. But I want you to know that.

247
00:31:01,580 --> 00:31:08,300
After her company found her. I think the thing.

248
00:31:17,370 --> 00:31:20,940
Okay. Let's talk about this one for a couple of years.

249
00:31:21,540 --> 00:31:25,890
The take two areas. Do you have any thoughts about income being immediate or income?

250
00:31:28,170 --> 00:31:37,800
So actually. I'm not sure about the workings of.

251
00:31:39,970 --> 00:31:44,230
That's where all the countries. Not going.

252
00:31:52,210 --> 00:31:56,080
Yeah. I'll go to another group. How about Maboneng?

253
00:31:56,350 --> 00:32:01,310
What are your thoughts? We all?

254
00:32:03,690 --> 00:32:11,860
Just wait it out, like. Normally you might have a meeting with the last person.

255
00:32:13,470 --> 00:32:18,710
The Gulf of Mexico. Yes. So I don't necessarily have an answer for this.

256
00:32:18,730 --> 00:32:25,200
I think it kind of depends on the design of the study. And I'm being very purposely vague about like education, income.

257
00:32:25,210 --> 00:32:30,970
Are you talking about like an individual or are you talking about like their parents versus themselves?

258
00:32:30,970 --> 00:32:38,380
So like how all of these things interlink could change whether you think something is a mediator or confounder?

259
00:32:39,100 --> 00:32:45,100
The reason why I bring this up, though, is that sometimes it can be really difficult to suss out these differences.

260
00:32:45,430 --> 00:32:54,970
So I always go back to your data, sets your code books and kind of see like when were things measured that can be maybe a bit useful,

261
00:32:55,390 --> 00:33:03,070
but also, you know, having discussions with colleagues, with advisors, with other people can really help to distinguish this.

262
00:33:03,070 --> 00:33:09,790
But I think this is actually like a very common thing to think about in studies is like give something, a mediator versus a confounder.

263
00:33:09,820 --> 00:33:18,040
It can be a bit confusing and it's okay if your own data sets you are confused but just, you know, think through it with other people.

264
00:33:21,830 --> 00:33:29,420
So what do we do with mediators? Typically, we ignore them in our traditional regression analysis, and that is my recommendation to you.

265
00:33:29,690 --> 00:33:33,380
Unless you are interested in the mediator itself, just ignore it.

266
00:33:34,220 --> 00:33:38,060
If you are interested in it, that's great. Then let's do a formal mediation analysis.

267
00:33:38,060 --> 00:33:44,870
And what kind of formal mediation analysis can we do? There are a few different approaches.

268
00:33:45,710 --> 00:33:55,360
Through regression based approaches, you can use the very canny approach that was, I believe, I don't know, 30,

269
00:33:55,370 --> 00:34:04,609
40 years ago, these two psycho patricians came up with this idea of testing mediators through the difference method.

270
00:34:04,610 --> 00:34:07,519
And by difference that means this equation here.

271
00:34:07,520 --> 00:34:15,230
So basically means that you create a model with the direct effect and then you include the mediator and subtract those values.

272
00:34:17,150 --> 00:34:22,670
There's no the simple test as well, and that is the product of coefficients, methods that these numbers.

273
00:34:24,230 --> 00:34:30,560
And so some people have argued that's a more recent method and some people have argued that it's a bit more stable.

274
00:34:32,090 --> 00:34:35,600
I would say that in a lot of situations they are relatively equivalent.

275
00:34:37,310 --> 00:34:43,219
Another thing to keep in mind is that you want to keep these on like an unexplained initiated scale.

276
00:34:43,220 --> 00:34:54,530
So even if you were interested in odds ratios and risk ratios, you would first measure this through like an unexplained initiated number.

277
00:34:54,530 --> 00:35:02,150
Like you don't like subtracting the odds ratio of 1.5 from 1.3 is due to an been a number and then subtract those out.

278
00:35:02,840 --> 00:35:09,560
But I believe in the homework assignment for today. You deal with some sort of difference measure so you don't just know no exponential options.

279
00:35:12,020 --> 00:35:16,540
You could use a pattern analysis or a structural feature model and assume based approach.

280
00:35:16,550 --> 00:35:23,300
The above doing this if you have a background in psychology or health behavior and

281
00:35:23,300 --> 00:35:29,990
it's just a way of modeling a few different regression models at the same time,

282
00:35:31,220 --> 00:35:37,250
you will read a critique structural equation models by Tyler Vanderbilt as part of the homework assignment for tonight.

283
00:35:38,540 --> 00:35:43,759
I think he digs into them a lot. But I you know, they're still very popular.

284
00:35:43,760 --> 00:35:49,219
So I think this is an argument. I'm not trying to say that they're bad, my published papers using ICM.

285
00:35:49,220 --> 00:35:53,210
So I think that they're I think they're okay. But, you know, some people would argue against them.

286
00:35:53,330 --> 00:36:04,850
Some people would argue vehemently for them. So Tyler Vanderbilt, what he likes to do is he came up with this proof of like a decomposition.

287
00:36:04,850 --> 00:36:12,440
So he would decompose the total effect into direct and indirect effects and into effect modifiers as well.

288
00:36:12,980 --> 00:36:19,400
So, you know, that's like definitely in advance how both different methods will deal a bit with it.

289
00:36:19,730 --> 00:36:24,560
I think the last part of the homework assignment is looking at output from decomposition.

290
00:36:26,150 --> 00:36:34,219
And there's, you know, in more recent years there has been code which has been developed for SAS and or among other sample packages for that.

291
00:36:34,220 --> 00:36:37,880
But it's only very recently that we've been able to see this.

292
00:36:40,150 --> 00:36:47,380
And I guess the whole idea behind Tyler Vanderbilt is that he he's a bit more comprehensive in how he decomposes the total effect.

293
00:36:48,040 --> 00:36:52,119
So he also accounts for mediation, in effect modification at the same time,

294
00:36:52,120 --> 00:36:57,850
whereas in a lot of other approaches, you deal with those separately or only deal with one at a time.

295
00:37:00,410 --> 00:37:00,740
Okay.

296
00:37:00,920 --> 00:37:08,690
So if I need any questions on mediation and we'll take the last couple classes also like a very deep dove into mediation, but any questions now will.

297
00:37:12,810 --> 00:37:21,540
Okay. Effect modification. I like to think of the effect modification in terms of stratification, in terms of strength of association.

298
00:37:21,780 --> 00:37:26,970
So let's go over what those words mean. So strength of association just means like for risk ratio.

299
00:37:27,690 --> 00:37:34,320
If you have these numbers, like a risk ratio of 10.5 is a stronger association than a risk ratio of 1.5,

300
00:37:35,940 --> 00:37:39,150
whereas, you know, a risk free 4.6 is stronger than 0.9.

301
00:37:39,390 --> 00:37:44,340
So it's symmetrical around one for any difference measure, it's symmetrical around zero.

302
00:37:44,340 --> 00:37:47,460
So, you know, a risk difference of 90% is stronger than 30%.

303
00:37:47,940 --> 00:37:52,290
A risk difference of maybe 53% is greater than -25%.

304
00:37:54,300 --> 00:37:55,200
Any questions about that?

305
00:37:56,250 --> 00:38:03,520
I also want to posit that I don't think like strength in association has really much to do with whether something is causally related or not.

306
00:38:03,540 --> 00:38:07,589
I'm just saying, like me. So we use. I wouldn't use that like a 10.5.

307
00:38:07,590 --> 00:38:11,700
Doesn't mean that like there's more of a causal relationship at all.

308
00:38:12,810 --> 00:38:15,900
It's just a stronger association and that association is stronger.

309
00:38:15,900 --> 00:38:23,520
That could be due to all sorts of statistical effects. But so strength doesn't necessarily equipment, causality.

310
00:38:23,640 --> 00:38:27,630
I want to make sure with that. But I you know, I think this is a permanent record that we use.

311
00:38:29,730 --> 00:38:36,780
So how do we represent effect modifiers in Dags?

312
00:38:36,780 --> 00:38:42,390
And this is how I like to do it, but this is not what you're supposed to do.

313
00:38:43,320 --> 00:38:45,120
I just to me, this just makes sense.

314
00:38:45,870 --> 00:38:53,670
And the whole point of a directed basically graph is that your arrows only can point to two other variables and they can point to one.

315
00:38:54,090 --> 00:38:58,649
But the reason why I like it to point to the line because that kind of symbolizes how the

316
00:38:58,650 --> 00:39:05,040
effect modifier affects the strength of the association as represented by this line.

317
00:39:07,320 --> 00:39:13,170
So let's think of an example where there is a dichotomous effect modifier.

318
00:39:16,280 --> 00:39:22,099
We could think of a situation where there is a little effect and there's a strong effect.

319
00:39:22,100 --> 00:39:35,300
And let me give you an example from the vaccine world. Let's look at the effects of getting a vaccine on your incidence of of of influenza.

320
00:39:35,420 --> 00:39:41,870
So does getting a vaccine protect you against getting the flu?

321
00:39:43,370 --> 00:39:49,940
We could think of age as an effect modifier because we know if you are older, the vaccine is a lot less protected.

322
00:39:50,540 --> 00:39:53,630
So there's a lot the strength of association is a lot less.

323
00:39:54,080 --> 00:39:57,560
Whereas if you are younger, the strength of association is a lot more.

324
00:40:03,820 --> 00:40:07,719
We could think of a situation where there is actually it's not just lots of effect.

325
00:40:07,720 --> 00:40:14,050
There's no effect. An example of this would be measles vaccinations of children.

326
00:40:14,440 --> 00:40:22,300
So again, we have measles vaccination as the exposure the outcome would be, you know, does somebody generate an immunological response?

327
00:40:23,980 --> 00:40:27,190
You know, from our most children, we think that that is the case.

328
00:40:28,150 --> 00:40:38,260
But for children who do have antibodies from their mothers inside of them, so maternal antibodies can result in no effect.

329
00:40:39,160 --> 00:40:48,670
And that's just because when you're vaccinated, your antibodies from your parents are the thing which are going to deal with the

330
00:40:48,670 --> 00:40:52,420
vaccine strain instead of you developing this immunological response yourself.

331
00:40:53,500 --> 00:40:57,309
And so that's why we kind of give the measles vaccine a bit later in life at about 12 months of age,

332
00:40:57,310 --> 00:41:08,020
just because we think that kids younger than that might still have antibodies to the to measles, which come from when they were still a fetus.

333
00:41:12,180 --> 00:41:18,210
We could think of an example where there's like an entirely reversed association.

334
00:41:20,970 --> 00:41:32,880
Let me give you an example of dengue vaccination and protection against subsequent severe dengue disease.

335
00:41:34,200 --> 00:41:39,239
So what we would like is, you know, like dengue vaccination protects it.

336
00:41:39,240 --> 00:41:46,350
So there's a negative association that happens for dengue vaccine at least there's a number of different kinds of vaccines out there now.

337
00:41:46,360 --> 00:41:53,379
But for Dengvaxia, that would happen if you previously had it suddenly maybe kind of think of it like influenza.

338
00:41:53,380 --> 00:41:56,070
That changes over time. There's a number of different strains of it.

339
00:41:56,970 --> 00:42:06,450
So if you were infected as a youth and then later on you've got the vaccine, then the vaccine would have a strongly protective effect.

340
00:42:07,380 --> 00:42:16,770
But if your vaccine was the first exposure that you had to any sort of dengue antigen, what actually happens is that in the future,

341
00:42:16,770 --> 00:42:24,270
if you are subsequently infected, infected with dengue, you can have a very serious reaction, including death.

342
00:42:29,210 --> 00:42:33,550
So again, this all goes back to this idea of the effect modifier.

343
00:42:33,560 --> 00:42:37,640
I like to think of it as influencing this arrow between the exposure and outcome.

344
00:42:38,030 --> 00:42:43,310
It can mean that the arrow stronger or less strong. It can mean that there is no effect even.

345
00:42:43,640 --> 00:42:48,890
It could also mean that there's a reversal in the destiny of the defect.

346
00:42:52,480 --> 00:42:57,130
The other thing about effect modification is that it is reciprocal.

347
00:42:58,390 --> 00:43:05,350
So there is no way for our statistical program to distinguish between this situation and the situation

348
00:43:05,350 --> 00:43:12,430
where there is an arrow from the effect modifier on to the outcome with the exposure of that relationship.

349
00:43:13,600 --> 00:43:19,840
So there's reciprocity. Yeah.

350
00:43:20,650 --> 00:43:24,130
That I could just see that they're on the same page with that.

351
00:43:27,890 --> 00:43:35,670
Also, if you're viewing this on Panopto later, let me know how the camera thing works because I am not sure how Panopto deals with multiple things.

352
00:43:35,680 --> 00:43:41,330
So we have our exposure outcome and we have an effect modifier.

353
00:43:44,460 --> 00:43:54,480
This. This is the same thing as saying the effect modifier relates to outcome in ex because that creates

354
00:43:54,480 --> 00:44:02,880
that error and there could be no reasons why one of these dags makes more sense than the other.

355
00:44:05,310 --> 00:44:12,800
But we're not able to like statistically. Look and contrast these two things.

356
00:44:16,440 --> 00:44:25,760
So other names for effect modification we can think of effect measure modification we can think of interaction,

357
00:44:25,770 --> 00:44:33,149
we can think of statistical interaction. We can think of biological interaction for the purposes of this class.

358
00:44:33,150 --> 00:44:35,550
These are all kind of the same thing.

359
00:44:36,300 --> 00:44:42,090
There are some statisticians out there who will argue that there is a difference between interaction and effect provocation.

360
00:44:42,810 --> 00:44:49,170
I don't find their arguments super well, first off, super understandable, but also very persuasive.

361
00:44:49,590 --> 00:44:51,360
So I use them interchangeably.

362
00:44:51,360 --> 00:44:56,220
But you could find somebody in the future who doesn't, or you might yourself think that these are different and that's okay.

363
00:44:58,440 --> 00:45:08,370
I will say that oftentimes, especially when people think of a biological interaction, they'll want to evaluate interaction on an additive scale.

364
00:45:08,370 --> 00:45:17,490
So they'll want to evaluate interaction in terms of risk differences or rate differences more so than like risk ratios or rate ratios in people.

365
00:45:18,060 --> 00:45:24,000
So that was again, people come up with these like convoluted arguments for why that's the case.

366
00:45:24,510 --> 00:45:30,780
I will say that I once was was just presenting at this at this conference,

367
00:45:30,780 --> 00:45:37,079
and there was a panel about the use of additive versus multiplicative measures.

368
00:45:37,080 --> 00:45:40,559
And two of the panelists just like went after each other because one was

369
00:45:40,560 --> 00:45:46,230
arguing for multiplicative being like a better measure of attack modification,

370
00:45:46,230 --> 00:45:51,680
the other one additive. And then afterwards there was this like chain of emails back and forth.

371
00:45:51,690 --> 00:45:55,110
And at one point in time, like I was just roped into this, I wasn't, you know,

372
00:45:55,110 --> 00:45:59,759
part of this argument by one point time when the people said that we had to each respond to and say

373
00:45:59,760 --> 00:46:05,309
that we would never mention their names again because they thought that it was like really sensitive.

374
00:46:05,310 --> 00:46:08,790
And there's a lot of personal attacks going on. But this is all the same.

375
00:46:08,790 --> 00:46:11,220
People have, like, you know, strong feelings about this.

376
00:46:11,940 --> 00:46:18,360
So I will not mention that person's name, but they're probably somebody who you've read their works in one class or another.

377
00:46:20,010 --> 00:46:23,940
Okay. So again, effect modification is skill specific.

378
00:46:25,110 --> 00:46:34,140
So what do you mean by this? We will often find evidence of effect modification on an additive scale, but not on a multiplicative scale.

379
00:46:34,170 --> 00:46:37,770
Or we could do the reverse, or we could find them in both. Or we could find them in either.

380
00:46:37,770 --> 00:46:40,230
But there's not unless it's not neat.

381
00:46:41,100 --> 00:46:48,570
And again, by additive scale, I mean things like risk differences by multiplicative scale, I mean ratios like risk ratios, rate ratios, odds ratios.

382
00:46:54,150 --> 00:46:59,100
So, you know, my recommendation to you, similar to mediation,

383
00:47:00,060 --> 00:47:06,570
is to not include effect modifiers in your teams or in your research unless it's a key part of it.

384
00:47:06,750 --> 00:47:10,200
Like, if you want to deal with the fact modification, that's totally fine.

385
00:47:10,440 --> 00:47:11,640
But then make that a specific.

386
00:47:12,390 --> 00:47:21,420
You shouldn't just like willy nilly be deciding, oh, I want to, you know, test for effect modification because depending on what scale you use,

387
00:47:21,720 --> 00:47:25,480
you'll often find something and then, you know, you'll have to expand that.

388
00:47:26,670 --> 00:47:36,300
So how do we deal with interaction you like in in SAS, you include a term with an asterisk in between the exposure and effective modifier.

389
00:47:37,020 --> 00:47:41,520
Again, these are there's no statistical difference between the exposure and effect modifier.

390
00:47:45,000 --> 00:47:50,700
You generally you'd still include the main effects, so you'd include both the exposure and the effect modifier and both of them.

391
00:47:50,940 --> 00:47:57,660
I can't really think of a scenario where you would only include the effect, modifier and exposure, but not the effect comparison effect.

392
00:47:57,930 --> 00:48:03,110
You kind of leave them all together. Okay.

393
00:48:03,770 --> 00:48:07,190
Any questions about effectiveness or modification?

394
00:48:10,950 --> 00:48:14,130
It's a very exciting topic. I know. But again, people will argue about it.

395
00:48:15,810 --> 00:48:27,690
I will give you a I'll give you a break until 2:00 here, and I'll sit here with the sign in sheet to show off to go round in the city center.

396
00:48:27,720 --> 00:48:32,090
Or if you want, like you could just have it. I won't able to go to the break. Yeah.

397
00:48:32,460 --> 00:48:37,730
So I'll see you at. Please.

398
00:48:49,170 --> 00:48:53,440
You're not going to.

399
00:48:56,800 --> 00:49:09,990
In a way. Should they have a philosophy that they have a different.

400
00:49:28,230 --> 00:49:32,360
They learn. I word.

401
00:49:33,210 --> 00:49:38,690
Sorry. So I. I didn't know what to.

402
00:49:42,060 --> 00:49:50,410
Three question. So do you understand the excuse for this is like the number, maybe like the number.

403
00:49:50,440 --> 00:50:02,450
And they just. Okay. So obviously by people at zero 20 people over.

404
00:50:03,140 --> 00:50:07,120
Right. And clearly, there's a difference. Does that make sense here?

405
00:50:07,450 --> 00:50:10,870
I mean, does this make sense to you? It's just like.

406
00:50:10,930 --> 00:50:14,790
It's like a prison. Yeah. Yeah.

407
00:50:14,830 --> 00:50:17,940
So this is like. Yeah. This is also similarly the number.

408
00:50:18,970 --> 00:50:27,720
Okay. So maybe it's saying that, you know, there is it's pretty public.

409
00:50:27,750 --> 00:50:34,310
So maybe like 20 people at put more than 20 people up in three, 15 people at point.

410
00:50:34,470 --> 00:50:37,950
And then the first person. The first two probably.

411
00:50:41,680 --> 00:50:47,270
Yeah. So that propensity score, like if he were to I don't know if, you know, circumstantial.

412
00:50:49,640 --> 00:50:50,960
Do typically use for your fans.

413
00:50:52,940 --> 00:51:03,460
Okay, so you know how you'd be doing this if you have like a private business to do for this example and you might have like a positive,

414
00:51:04,040 --> 00:51:09,770
but then you'd have like an auto. Yeah. Oh, h c q people.

415
00:51:11,120 --> 00:51:17,800
You know, and then you would just at some point turn to.

416
00:51:19,890 --> 00:51:27,910
I think it's another thing like an ultrasound. I have to admit, it's like the year.

417
00:51:32,630 --> 00:51:36,210
So that's what it is. So basically, it's predicting what these are.

418
00:51:41,610 --> 00:51:46,530
Okay. And the whole idea is that if you only have one variable predicting,

419
00:51:46,530 --> 00:51:52,440
like if we only included age, then our like our propensity score would be pretty much the same.

420
00:51:53,490 --> 00:51:56,890
But if we put it in their finality, maybe it'll start, you know, you know.

421
00:51:57,420 --> 00:52:01,530
Okay, but maybe it won't. And this is the thing.

422
00:52:01,680 --> 00:52:06,300
This is the because I don't know if this is going to be like this is just like an alternative.

423
00:52:08,120 --> 00:52:13,140
So. Right. And then this would just indicate. Yeah, yep, yep.

424
00:52:13,160 --> 00:52:17,370
Yep, yep. Okay. Okay. Thank you. Yep. Yep, I'm sure.

425
00:52:20,760 --> 00:52:29,360
I need you to. It turns out that I should probably get one.

426
00:52:32,360 --> 00:52:45,140
Okay. So for the Zito Effect and I said X with measles vaccination rate, I said, why was measles disease?

427
00:52:45,950 --> 00:52:53,930
Yes, yes, there's infection. And then the you the exposure is the presence of material.

428
00:52:54,890 --> 00:52:58,690
I know. Okay. Okay. I heard it.

429
00:52:58,690 --> 00:53:04,510
And I don't know why. She thought it was young age like I was.

430
00:53:05,050 --> 00:53:10,330
But, I mean, like, it's related to you because, like, the younger you are, right? Anybody see right here?

431
00:53:10,980 --> 00:53:15,520
Okay, that's my speak. Okay. You're okay?

432
00:53:16,030 --> 00:53:25,260
Yeah. This past.

433
00:53:29,300 --> 00:53:35,910
His kids have been so institutionalized.

434
00:53:42,380 --> 00:54:00,250
What is? There is no turnaround in the health care system.

435
00:54:00,670 --> 00:54:06,840
That's a real. First.

436
00:54:18,420 --> 00:54:28,980
I wish I knew. Yeah. Thank you.

437
00:54:30,920 --> 00:54:36,590
Coming. The Manila Times.

438
00:54:51,690 --> 00:54:55,920
Oh, I need to start. Saw.

439
00:55:02,810 --> 00:55:07,900
How do I know that? I basically.

440
00:55:10,660 --> 00:55:14,650
Yeah. And.

441
00:55:20,560 --> 00:55:46,290
And could. I.

442
00:55:51,150 --> 00:56:00,320
Now we got. That found.

443
00:56:17,540 --> 00:56:37,340
Cannot. After the.

444
00:56:49,700 --> 00:56:55,120
Yeah. I thought.

445
00:56:55,810 --> 00:57:01,880
Yeah, I know. I think I've heard all the theories and.

446
00:57:06,120 --> 00:57:10,580
Then stand up for the people.

447
00:57:10,760 --> 00:57:19,270
Yeah. Are you going to be all right?

448
00:57:20,480 --> 00:57:23,640
We're going to get.

449
00:57:35,760 --> 00:57:39,780
Yeah. Yeah. Okay, let's go back.

450
00:57:40,710 --> 00:57:44,900
And we are going to do a deeper dove into remediation.

451
00:57:46,380 --> 00:57:51,240
This is probably one of the harder our blocks in this class.

452
00:57:51,240 --> 00:57:57,420
But like it, it kind of is done with help from here. So just stick with me over the next next hour.

453
00:57:59,130 --> 00:58:02,760
And this is really getting into this idea of counterfactuals.

454
00:58:03,870 --> 00:58:09,750
And so I think a causal in prints and counterfactuals in terms of time travel.

455
00:58:10,170 --> 00:58:14,190
Again, counterfactual means is this artifact, this is not real.

456
00:58:14,190 --> 00:58:19,020
We do not have actual real world evidence of this.

457
00:58:19,980 --> 00:58:26,639
But a lot of our thinking is based on this idea of what happens to an individual if they have

458
00:58:26,640 --> 00:58:33,060
a certain exposure and in going back in time and see if they had a different set of exposures.

459
00:58:33,840 --> 00:58:37,680
So that's kind of how I want you to think about causal inference.

460
00:58:37,980 --> 00:58:42,510
Again, of course, we don't have this in in reality, but if we start thinking this way,

461
00:58:42,510 --> 00:58:51,990
then we can start thinking if our study designs actually can live up to some assumptions that we're making as part of their causal imprint screen.

462
00:58:54,400 --> 00:58:59,020
Okay. So I am going to be drawing things out on this page.

463
00:58:59,470 --> 00:59:03,550
I also have so there will be the Panopto recording that I'm doing right now.

464
00:59:03,880 --> 00:59:08,170
There is another recording already on canvas of me doing this.

465
00:59:09,340 --> 00:59:14,440
I also have a printout of this document which I'm making.

466
00:59:14,890 --> 00:59:20,380
So hopefully through one of those ways, you will be able to review this material.

467
00:59:20,680 --> 00:59:27,910
I would say for most students, you probably have to view this at least one more time to get like a basic understanding of it.

468
00:59:28,420 --> 00:59:34,629
I will also be introducing things like control, direct effects and natural direct effects and natural indirect effects,

469
00:59:34,630 --> 00:59:39,760
and it'll all be like a bit confusing, like a bit out there.

470
00:59:40,570 --> 00:59:48,399
And when I think about like what the purpose of this class is, I don't expect that in five years without any other classwork or without any

471
00:59:48,400 --> 00:59:52,270
repeated exposure to feel like remember the difference between like a natural,

472
00:59:52,270 --> 01:00:01,569
direct effect and a controlled direct effect. But hopefully you will think back and be like, okay, so like I do remember something about that.

473
01:00:01,570 --> 01:00:08,530
So I can kind of Google the answer. And as you Google an answer, maybe you'll think back to this lecture.

474
01:00:09,340 --> 01:00:15,670
So again, I realize that some of the concepts are complicated here, but, you know, you're stuck in here at these students.

475
01:00:16,000 --> 01:00:21,910
Let's prevent let's present some advanced methods to you, and I think you'll get it.

476
01:00:24,160 --> 01:00:32,980
Okay. So we are going to go with our typical example, like our DAG here is.

477
01:00:36,950 --> 01:00:41,810
Just a simple example of one exposure, one mediator and an outcome.

478
01:00:42,410 --> 01:00:50,570
And so the exposure is coffee, like, are you a coffee drinker or not?

479
01:00:51,590 --> 01:00:56,600
The mediator is whether you have a high sugar diet or not.

480
01:00:57,380 --> 01:01:02,060
And then why is whether you have diabetes or not?

481
01:01:03,940 --> 01:01:09,340
And for all of these, there's sort of like zero means no and one means yes.

482
01:01:10,180 --> 01:01:16,360
And I'll try to represent these with like a like a building.

483
01:01:16,780 --> 01:01:20,769
Right now the circle is no, I filled in.

484
01:01:20,770 --> 01:01:25,780
That would be. Yes. So, you know, I'm not a nutritionist.

485
01:01:25,780 --> 01:01:36,849
I'm making up this that. But the idea for me is if you were a coffee drinker, then maybe you are going to start putting lots.

486
01:01:36,850 --> 01:01:43,209
If you're like me, you're going to put lots of like sugar and cream in your coffee that's going to

487
01:01:43,210 --> 01:01:47,890
increase your sugar load and then that might increase your risk of diabetes.

488
01:01:49,600 --> 01:01:54,309
Or it could be that, you know, if you drink a lot of coffee, that leads to other changes in your life.

489
01:01:54,310 --> 01:02:01,990
Like maybe you drink your coffee black, but that means you're not drinking like sugary sodas and as a result, you have a lower risk of diabetes.

490
01:02:01,990 --> 01:02:05,050
So there's sort of different pathways that this could operate under.

491
01:02:06,730 --> 01:02:11,570
But as in any mediation study, you know, you'd want to think carefully about the timing of this.

492
01:02:11,590 --> 01:02:21,530
I think back to our education income example and for this I am considering that coffee was measured before coffee was measured at one time point.

493
01:02:21,540 --> 01:02:26,349
Then we looked at the mediator a few moments later, a few years later,

494
01:02:26,350 --> 01:02:31,630
and we looked at the outcome of diabetes even after that, and the people with diabetes were excluded at baseline.

495
01:02:32,350 --> 01:02:33,670
Does this setup make sense?

496
01:02:35,020 --> 01:02:46,630
And then also are you able to like see, when I integrate on here, sort of hopefully, okay, if we all try to be bold when I can.

497
01:02:48,130 --> 01:02:56,470
So we're going to present 12 different scenarios. But these are different, different timelines.

498
01:02:56,980 --> 01:03:02,290
So let's start off with the we have three people that we're looking at.

499
01:03:02,290 --> 01:03:14,380
So we'll be looking at them repeatedly through this. And let's say that we are going to look at them in reality.

500
01:03:23,550 --> 01:03:33,450
So for respondent one at 9.1, this person does drink a lot of coffee and we follow them over time.

501
01:03:34,380 --> 01:03:37,620
And it turns out this person does end up with a high sugar diet.

502
01:03:39,870 --> 01:03:45,270
We follow them over time and then they do end up with diabetes.

503
01:03:49,550 --> 01:03:55,010
So I'm just going to say like X equals Y and equals one.

504
01:03:55,490 --> 01:04:02,180
Y equals one. For person to be third off.

505
01:04:02,180 --> 01:04:09,799
They do not drink coffee, but they do have a very sugary diet.

506
01:04:09,800 --> 01:04:17,630
Maybe they're drinking lots of coke instead to get that caffeine fix, and they do end up with diabetes.

507
01:04:19,750 --> 01:04:27,930
So again, that's x equals zero and equals one and y equals one person.

508
01:04:27,970 --> 01:04:41,020
Three does drink a lot of coffee, but, you know, it's not actually end up with a high sugar diet.

509
01:04:44,280 --> 01:04:58,140
But this person still wanted to take. Any questions about this overall setup?

510
01:04:58,350 --> 01:05:03,020
Again, this is just part of the real world, but we're following people.

511
01:05:03,300 --> 01:05:07,940
Yes. Thank you.

512
01:05:07,950 --> 01:05:12,630
Thank you. Thank you. This is a break here.

513
01:05:13,300 --> 01:05:24,170
Yes. Yes. So. 41 02.431.

514
01:05:27,540 --> 01:05:30,660
So this is what happened, because I'm also looking at that printer right now.

515
01:05:30,720 --> 01:05:33,720
So what did happen with this person?

516
01:05:33,970 --> 01:05:37,200
Let's right now think of this person as.

517
01:05:51,330 --> 01:06:01,660
Yeah. Let us think of this person as a Y equals with your wife as one.

518
01:06:01,740 --> 01:06:12,459
They have that? Yeah. Okay.

519
01:06:12,460 --> 01:06:21,190
So we could think of counterfactual situation and the situation of whether they get diabetes or not, you know, certainly matters to this individual.

520
01:06:21,190 --> 01:06:24,500
But for the sake of me, explaining things doesn't matter.

521
01:06:26,800 --> 01:06:33,460
We could think of a counterfactual reality. So counter factual.

522
01:06:38,050 --> 01:06:45,190
And we still have these three individuals, number one, number two and number three.

523
01:06:47,680 --> 01:06:52,840
And now what we do is we switch.

524
01:06:53,170 --> 01:06:58,629
We switch. So for this person, we go back in time.

525
01:06:58,630 --> 01:07:02,490
We found out they had diabetes and we go back in time to the beginning of the study and we say like,

526
01:07:02,500 --> 01:07:06,660
no, you're not going to drink in comfort for this person.

527
01:07:06,670 --> 01:07:12,910
We flip it so that they do drink coffee, and for the first time they say, no, they don't drink coffee.

528
01:07:13,480 --> 01:07:17,110
So that's the only thing that we're changing the counterfactual. So that's what a counterfactual is.

529
01:07:17,110 --> 01:07:21,250
We go back in time, we change their exposure. Does that make sense?

530
01:07:23,620 --> 01:07:26,800
But in a similar way, we are just observing what happens after this.

531
01:07:27,430 --> 01:07:34,960
So this person, you know, not drinking coffee, they end up not with a sugary diet.

532
01:07:36,220 --> 01:07:41,650
And unfortunately, though, they still end up with diabetes.

533
01:07:43,240 --> 01:07:52,930
So this person is it may be like a crude way we call them doomed.

534
01:07:52,930 --> 01:08:01,630
It like they're going to kind of get diabetes regardless of the exposure level for this person.

535
01:08:01,750 --> 01:08:09,250
They have, you know, they're a coffee drinker, as it turns out.

536
01:08:11,290 --> 01:08:15,550
They still have that they like that that sugar in their diet.

537
01:08:17,530 --> 01:08:22,450
And they also end up with diabetes.

538
01:08:26,640 --> 01:08:40,330
For this person. The. You know, they're not a happy drinker.

539
01:08:48,130 --> 01:08:57,360
But they do have a high sugar diet then and then they end up with diabetes.

540
01:09:14,090 --> 01:09:18,930
Okay. So on one hand, what actually happens to these individuals is that matter.

541
01:09:19,310 --> 01:09:24,080
On the other hand, if everyone has diabetes, then my number is like, just look really confusing over time.

542
01:09:24,080 --> 01:09:30,500
So I am going to change this person. Luckily for them, they end up with not diabetes.

543
01:09:30,800 --> 01:09:38,700
So no diabetes y equals zero. This is where it goes here.

544
01:09:39,870 --> 01:09:44,860
Thank you for. Both of you for a.

545
01:09:45,710 --> 01:09:49,100
But yeah, we're going to say this for life.

546
01:09:51,250 --> 01:09:56,560
So now what we can do is we can do an alternate counterfactual.

547
01:10:00,040 --> 01:10:10,630
So this is like an alternate counterfactual and it is kind of like a counterfactual to the counterfactual.

548
01:10:11,110 --> 01:10:29,569
So what do I mean by that? What I mean is that we start off similar to the counterfactual.

549
01:10:29,570 --> 01:10:32,990
So everything here is the same as above. So I'm just going to fill this in really quick.

550
01:10:42,980 --> 01:10:47,360
So these are the thing, you know, we're going back in time. We're switching out these people's behaviors.

551
01:10:49,220 --> 01:10:55,130
But then what we do is we do some manipulations here.

552
01:10:58,140 --> 01:11:02,070
And probably what you're thinking right now is that we.

553
01:11:04,900 --> 01:11:09,220
Flip the mediator, but don't do that.

554
01:11:09,250 --> 01:11:24,190
That's like not what you do. Instead, what you do is you set the mediator as if X.

555
01:11:25,390 --> 01:11:37,650
Was different. So as if x equals one is here, x equals zero, x equals one, x equals zero three.

556
01:11:37,750 --> 01:11:52,240
Setting the mediator as if x equals one here will set the mediator as if x equals zero, and here will set the mediator as if x equals one.

557
01:11:55,290 --> 01:11:58,930
So what do I mean by that? And then I'll give you practical examples of like what this is supposed to.

558
01:12:04,280 --> 01:12:07,400
So we don't look up here. We look here for the value.

559
01:12:07,430 --> 01:12:11,420
So here I am equals one. We have this filled in.

560
01:12:13,460 --> 01:12:17,450
Here we look the value for when x equals zero.

561
01:12:18,890 --> 01:12:25,860
So that's here. So and we'll also equal one.

562
01:12:30,740 --> 01:12:33,830
And then here we set the mediator as if X equals one.

563
01:12:34,280 --> 01:12:37,460
And so then here is where it should be like this.

564
01:12:38,000 --> 01:12:45,980
You don't have to make an edit to the to the handout as well, because this should be a separate here.

565
01:12:50,090 --> 01:12:57,079
Again, I think what people are thinking is like, oh, we manipulated in this one will also manipulate em in this one.

566
01:12:57,080 --> 01:13:02,180
And yes, we do like a bit of manipulations, but we don't just flip em from being 1 to 0 0 to 1.

567
01:13:02,600 --> 01:13:06,080
What we do is we let them naturally vary.

568
01:13:06,530 --> 01:13:10,550
And so natural variation means like, what would happen if X happens?

569
01:13:11,090 --> 01:13:20,630
So for this individual, what happens is like they drink coffee, they always put sugar in their copies, they have a high sugar diet.

570
01:13:21,650 --> 01:13:24,680
So when you don't have coffee, they have a low sugar diet.

571
01:13:25,520 --> 01:13:33,710
So then in the counter fact, in the alternative counterfactual, this person is not drinking coffee,

572
01:13:34,310 --> 01:13:39,470
but then we force them to have the sugar diet as if they were drinking coffee.

573
01:13:45,390 --> 01:13:51,480
It's the same thing with I mean, person two doesn't really vary, so let's give them so.

574
01:13:51,480 --> 01:13:55,140
Person three this is a coffee drinker.

575
01:13:55,470 --> 01:14:01,080
So then they do not drink high sugar, they do not have a lot of high sugar.

576
01:14:01,080 --> 01:14:04,650
So maybe, you know, they're able to caffeinated, they drink their coffee black.

577
01:14:05,310 --> 01:14:09,600
But if you don't let them drink coffee, they'll drink sugary substances.

578
01:14:09,600 --> 01:14:19,500
Instead, this person is going to switch over to drinking soda. So in the alternative counterfactual, what we do is we don't let them drink coffee,

579
01:14:21,570 --> 01:14:25,680
but then we think, what would their mediator be like if they did drink coffee?

580
01:14:27,390 --> 01:14:30,600
So it's not just flipping them to like, Oh, you have a low sugar diet, you have a high sugar.

581
01:14:30,690 --> 01:14:36,690
It's like, let's think of what would have happened if we changed the exposure, not if we changed to be a mediator.

582
01:14:40,510 --> 01:14:48,790
So then we're in again. We're we're in in this like crazy alternative counterfactual world.

583
01:14:48,790 --> 01:14:56,260
And let's just say this person ends up, you know, this person always is getting diabetes.

584
01:14:57,450 --> 01:15:12,970
Uh, and here, you know, we'd also say, and it goes right here and equals one, and you have this life where this person also ends up getting that he's.

585
01:15:15,260 --> 01:15:18,830
And then you have this situation.

586
01:15:21,820 --> 01:15:28,660
Where this person and the not having taken.

587
01:15:37,640 --> 01:15:40,820
And again, like what their diabetes situation is like.

588
01:15:40,820 --> 01:15:42,320
We we consider a lot of different things.

589
01:15:42,320 --> 01:15:46,820
But in this example, we're like, you know, considering a lot of like what their coffee and sugar substances are.

590
01:15:46,820 --> 01:15:52,129
But, you know, everybody also varies in terms of genetics, maybe their family predisposition,

591
01:15:52,130 --> 01:15:55,700
other behaviors that they have, other things which they're consuming.

592
01:15:56,090 --> 01:16:00,079
So there's a lot of things that can influence whether they get diabetes or not. We're just in this situation.

593
01:16:00,080 --> 01:16:04,220
The only thing we're changing is, you know, their their coffee consumption.

594
01:16:04,790 --> 01:16:11,959
And then we're letting the mediator we're letting sugar consumption levels vary based on these two examples.

595
01:16:11,960 --> 01:16:25,190
The reality of the current you. I hesitate to say either question because I'm sure there are questions.

596
01:16:27,570 --> 01:16:30,780
What might help you grasp at this point in time?

597
01:16:34,230 --> 01:16:41,010
What's to. Why?

598
01:17:02,330 --> 01:17:09,799
Yeah, great question. So what are we changing? The other thing I'm going to do, just as you ask questions like label these rows,

599
01:17:09,800 --> 01:17:17,690
just in case you have like a question about a specific row, then you can.

600
01:17:20,480 --> 01:17:23,730
Find them. Yeah.

601
01:17:23,750 --> 01:17:31,520
So what we like to do, and I think that's the thing you mentioned, is if we're looking at the effect of vaccination,

602
01:17:31,520 --> 01:17:39,209
the counterfactual for vaccination, what we want is if somebody gets a vaccine, then we travel back in time.

603
01:17:39,210 --> 01:17:42,500
And the only thing we're changing is whether they get that vaccine or not.

604
01:17:44,180 --> 01:17:50,809
But in the reality, like, obviously, there's going to be something that's like a simplified way of talking about the counterfactual framework.

605
01:17:50,810 --> 01:17:57,560
But obviously there's going to be some things which change as a result of that, because for vaccination,

606
01:17:57,560 --> 01:18:02,030
you could look at what's the immunological response to it and you could think of like,

607
01:18:02,030 --> 01:18:06,169
does somebody actually announce like an antibody response or not? Do they have a T cell response?

608
01:18:06,170 --> 01:18:11,720
Are not. And then how does that relate to infections down the road? Because even with something as simple as vaccination,

609
01:18:11,990 --> 01:18:20,510
you could develop a mediation framework where you will let the mediator being the immune response, you could let that naturally vary.

610
01:18:24,550 --> 01:18:30,250
So which is all to say, like in a traditional counterfactual approach, you only deal with exposures.

611
01:18:30,760 --> 01:18:34,420
But like, if you want to do a mediation analysis, you can add in mediation.

612
01:18:35,140 --> 01:18:40,390
And again, this is this is this is complicated. This is second year epidemiology and not your first year stuff.

613
01:18:41,890 --> 01:18:45,490
If you're somebody who had a person who you saw recently.

614
01:18:45,880 --> 01:18:51,130
But I guess I'm just not very sure. Very.

615
01:18:53,770 --> 01:19:01,960
Yeah. So it doesn't naturally bury me. Somebody just uses the word natural, like some methodologies a decade or so ago.

616
01:19:02,260 --> 01:19:08,350
30, using the word naturally with this. Does it actually have anything to do with how we use naturally in English otherwise?

617
01:19:08,740 --> 01:19:12,310
Probably not. But we just use the word natural in the way.

618
01:19:13,090 --> 01:19:19,730
So. Here is where we're talking about like this natural variation.

619
01:19:23,350 --> 01:19:31,570
Natural means that we are letting the mediator vary based on the exposure.

620
01:19:31,580 --> 01:19:34,660
So we're thinking like what naturally would happen to the mediator?

621
01:19:34,870 --> 01:19:40,149
So we're not directly influencing the mediator itself, we're not directly changing the mediator.

622
01:19:40,150 --> 01:19:49,390
We're just saying, like what? What would happen in a world where the exposure level changed with the mediator change?

623
01:19:50,050 --> 01:19:52,600
So I think that's like what the natural means.

624
01:19:52,840 --> 01:20:01,150
And so like my intuition with the natural is like we're not as human beings, we're not like directly affecting the,

625
01:20:01,380 --> 01:20:08,020
the natural variation, the mediator, but we're letting it kind of like vary a bit based on preexisting things.

626
01:20:11,050 --> 01:20:19,150
It's a great question. And then you'll also, you know, the the move talking about like pure and impure direct effects as well.

627
01:20:19,150 --> 01:20:27,640
We're not going to go over those in this class, but they'll just be like adjectives which are put in front of these words.

628
01:20:28,510 --> 01:20:31,839
And the important thing for natural direct effect is thinking about this factual

629
01:20:31,840 --> 01:20:37,090
world where the mediator is very not because we are directly impacting it.

630
01:20:37,090 --> 01:20:43,630
We're not directly changing the levels of the mediator, but we are we are letting it vary based on us.

631
01:20:44,110 --> 01:20:50,860
What we are doing is we are manipulating the exposure, but we're not directly manipulating the mediator.

632
01:20:52,770 --> 01:20:58,020
Yeah. So for me, naturally.

633
01:20:59,230 --> 01:21:04,510
He's the mediator. Always be the same as it was. When it was in the first.

634
01:21:04,640 --> 01:21:09,440
Yeah. So if you're looking at the counterfactual, then the mediator will be the same.

635
01:21:10,640 --> 01:21:17,770
Yep. That's a great segway into our second alternative.

636
01:21:24,740 --> 01:21:33,310
Where we do. Reality for X.

637
01:21:38,040 --> 01:21:41,190
And then, uh. Um.

638
01:21:45,520 --> 01:21:53,200
The counterfactual for the. The contractual fourth annual data.

639
01:21:57,060 --> 01:22:10,620
Okay. So again, we are person one, person to person three and I am just going to breathe through doing the expert of this.

640
01:22:12,420 --> 01:22:19,120
So somebody has x equals one. Griffin has x equals zero.

641
01:22:20,650 --> 01:22:32,340
Ferguson has x equals one. This is the reality.

642
01:22:36,000 --> 01:22:50,700
And then what we're going to do is we're going to, you know, let the meter vary, but we're going to, again, set the mediator as if X equals zero.

643
01:22:52,130 --> 01:23:04,650
Here is that the mediator is if x equals one and set the mediator is if x equals zero.

644
01:23:12,550 --> 01:23:26,230
So, um, you know, we, we look at that based on previously so when X equals zero is this situation right here so and would equal zero.

645
01:23:29,450 --> 01:23:33,380
So here at zero. And we have.

646
01:23:37,030 --> 01:23:45,420
Here when we have a set and as if x equals one, we're going here seven and doesn't put one.

647
01:23:50,960 --> 01:23:55,850
And then we said, as if X equals zero. So that would be here.

648
01:23:55,940 --> 01:24:10,250
That is an equal. And then from here on out, we're just viewing these people over time and seeing what happens to them.

649
01:24:10,880 --> 01:24:22,760
So this person who, you know, sort of drinking coffee and then had the media value changed the potentially well,

650
01:24:22,790 --> 01:24:35,300
you know, we're calling this person doom. So they're always going to be y equals one for this person over time.

651
01:24:37,640 --> 01:24:44,100
Let's say they also end up with diabetes. And this person.

652
01:24:47,950 --> 01:24:51,320
Also with. That means.

653
01:24:56,580 --> 01:25:03,959
So, you know, maybe the natural variation. Another way to think about it is like we have our two things which can happen with the exposure here,

654
01:25:03,960 --> 01:25:08,850
but we can change their levels and then we can look at what happens with the mediator and then

655
01:25:08,850 --> 01:25:15,600
we just like switch it so that the mediator is what would happen in the alternative situation.

656
01:25:17,640 --> 01:25:25,240
But then for all of these, like what happens at the end with getting diabetes or not is dependent on, you know,

657
01:25:25,260 --> 01:25:29,640
it's dependent on like the coffee and sugar, but it's also these other factors which are involved in their lives.

658
01:25:40,810 --> 01:25:47,500
What questions do people have? Okay.

659
01:25:47,540 --> 01:25:55,550
So now we have to get into naming these things, though, like, well, we have all these convoluted storylines.

660
01:25:56,090 --> 01:26:02,069
There's like. The of the alternative timeline.

661
01:26:02,070 --> 01:26:07,770
But then there's like the meshing of the alternative timelines and that happens like a third different way.

662
01:26:07,770 --> 01:26:14,040
So what does this all actually and this is obviously like theoretical stuff.

663
01:26:14,190 --> 01:26:19,260
We're not in reality observing this, but a causal inference is like based off of these, these things.

664
01:26:20,310 --> 01:26:28,020
So we can use these examples to calculate the natural, direct effect.

665
01:26:32,040 --> 01:26:38,460
And the natural direct effect is when we counter factually.

666
01:26:41,620 --> 01:26:54,190
Observe the difference between x equals zero and x equals one when.

667
01:26:58,230 --> 01:27:03,560
The mediator. Is that?

668
01:27:06,120 --> 01:27:14,490
To be the natural value when x equals zero.

669
01:27:18,720 --> 01:27:35,080
We will go over what this actually means. So what we are trying to do is bring these numbers.

670
01:27:43,860 --> 01:28:00,420
And we need to once mean x equals one when we set an as if x equals zero and then we have x equals zero when we set and as if x equals zero.

671
01:28:08,970 --> 01:28:14,430
So basically now we need to just like look at this table and and see where these are.

672
01:28:15,330 --> 01:28:18,060
So for person one, we're looking for when X equals one.

673
01:28:18,570 --> 01:28:24,810
And so that could be either this or it could be this, but it's when we set em as if X equals zero.

674
01:28:25,380 --> 01:28:32,060
And here is where we set them, as if X equals zero. So this is road check.

675
01:28:36,160 --> 01:28:41,050
And this would be like a one because that's what the guy is.

676
01:28:42,430 --> 01:28:50,659
So it's like, what is the. It's a wondering. So where is x equals zero?

677
01:28:50,660 --> 01:28:53,660
As if x is the zero.

678
01:28:53,690 --> 01:28:54,890
That's kind of a simple one.

679
01:28:54,900 --> 01:29:07,880
So here it's just in the in the normal counterfactual where x equals zero and we set an has effectively zero said zero D here, which is also one.

680
01:29:10,450 --> 01:29:22,390
So we do one minus one equals zero. Okay.

681
01:29:23,050 --> 01:29:26,890
So this second one is.

682
01:29:28,060 --> 01:29:33,760
So for prison two, we're looking at X equals one where M is set as if x equals zero.

683
01:29:35,410 --> 01:29:42,730
So here we have to go all the way down to H. So this is row H and it is y equals.

684
01:29:46,360 --> 01:29:53,290
And then we said zero is if X equals zero.

685
01:29:53,290 --> 01:30:03,260
And then we simply just grab. Here was.

686
01:30:10,390 --> 01:30:15,520
So I'm I'll do this. But I also want you in your heads to try to do number three.

687
01:30:15,520 --> 01:30:18,850
So I'm not going to talk to you for a moment just so that you can try to find it yourself.

688
01:30:42,970 --> 01:30:46,180
So that might equal one.

689
01:30:47,380 --> 01:30:53,090
And then I have a row and I see this one.

690
01:31:00,540 --> 01:31:04,499
So here's the NFL draft. Take us to zero, as you'd like to add all these up.

691
01:31:04,500 --> 01:31:11,810
But this is only three people obviously want to look through a lot of different people to make some sort of population level inference.

692
01:31:16,890 --> 01:31:19,980
And if that weren't enough, we also have a natural, indirect effect.

693
01:31:28,780 --> 01:31:35,230
And maybe let's let's bring this back to the natural, direct effect. You know, again, it's looking at the X to Y situation.

694
01:31:38,340 --> 01:31:43,870
This is the direct effect. And this is sort of the indirect.

695
01:31:45,670 --> 01:31:56,360
Just going through the mediator. Okay.

696
01:31:56,360 --> 01:32:02,180
So for the indirect effect, we fix X to be one.

697
01:32:05,880 --> 01:32:14,970
And then we compare the mediator will be we can hear the outcome when the mediator.

698
01:32:17,050 --> 01:32:28,800
Is that? Is if x equals zero compared to if x equals one.

699
01:32:36,200 --> 01:32:44,600
In both these situations, x will be one, but as if x equals one and set.

700
01:32:46,180 --> 01:32:58,320
If X equals zero. Uh.

701
01:32:58,970 --> 01:33:05,830
Um. It's my person to person three.

702
01:33:08,710 --> 01:33:18,700
And if everything is as it is, like said X equals one, as if the meter is set when x equals one.

703
01:33:19,000 --> 01:33:24,190
This makes things like a bit simple. Then we aren't in any of the alternative counterfactuals.

704
01:33:26,770 --> 01:33:29,860
Similar to here. You know we're not in any of the alternate constructions.

705
01:33:30,250 --> 01:33:33,400
But here x equals one.

706
01:33:34,450 --> 01:33:48,580
That's just a. So that's revivals one that's a one or two with we have to go to this row then so that is really.

707
01:33:50,300 --> 01:33:58,970
You see that person also? That's why he was born. And then for number three, we are going to here is where X equals one.

708
01:34:00,170 --> 01:34:08,730
So that would be real. New York City's version of the zero.

709
01:34:14,360 --> 01:34:19,820
And then here we go to alternative counterfactual land.

710
01:34:21,050 --> 01:34:26,120
So again, we have to look where X equals one.

711
01:34:28,340 --> 01:34:39,910
We set them as if x equals zero. That is this line. So this is where J is 51 for number two.

712
01:34:39,920 --> 01:34:44,540
Again, down here, we're looking where X equals one. So this is row H.

713
01:34:46,930 --> 01:34:54,580
Here is also where it is. And then for the third person, we are looking for where x equals one.

714
01:34:54,800 --> 01:35:12,230
So this would be real. So if you like, for a lot of this, which is like nothing, any evidence of a direct or indirect effect.

715
01:35:13,820 --> 01:35:19,340
Okay. So you've been listening to me prattle on and probably very confused, which is understandable.

716
01:35:19,910 --> 01:35:25,460
For the last 40 minutes about this.

717
01:35:25,610 --> 01:35:30,740
So thank you. Um, so why am I telling you all of this?

718
01:35:31,550 --> 01:35:41,240
One, if you do go on to a Ph.D. program, um, if you do, if you are looking to do a mediation analysis,

719
01:35:41,540 --> 01:35:46,430
there is, you know, simpler ways you can do like the symbols test or the burning canon.

720
01:35:46,730 --> 01:35:52,700
You don't need to like too much about this, but when you actually get into some of the more causal inference ways,

721
01:35:52,700 --> 01:35:57,230
like if you are using characters developed by Tyler Vanderbilt, it's based off of this paradigm.

722
01:35:57,800 --> 01:36:07,370
And so I realize this is like a lot to digest. Um, I want you to have like a very high level idea of this.

723
01:36:08,180 --> 01:36:14,419
So I'm not going to present a really complicated table in the future and have you calculate the

724
01:36:14,420 --> 01:36:19,160
natural effect in that film director but I don't think that's really appropriate for your level.

725
01:36:19,460 --> 01:36:22,750
But I do want you to be thinking about like how these time travel happened.

726
01:36:23,390 --> 01:36:29,030
And because we kind of have two different points of time travel in a way we are.

727
01:36:29,510 --> 01:36:33,270
That just means there are four timelines that we can look at in somebody's life.

728
01:36:34,370 --> 01:36:40,770
That's how it works out mathematically. The other thing is looking at this idea of natural variation of the mediator.

729
01:36:41,090 --> 01:36:47,330
Natural variation mediator is not directly changing the media because I think what a lot of people think is like this means that,

730
01:36:47,780 --> 01:36:51,709
you know, here and equals one.

731
01:36:51,710 --> 01:36:53,750
So we're going to make it m equals zero. And here it's that.

732
01:36:53,750 --> 01:37:00,860
Because once we make it because that's not the case, we look to see what happens in the alternative timeline or in the counterfactual.

733
01:37:01,100 --> 01:37:04,920
We. Okay.

734
01:37:06,210 --> 01:37:11,530
So that's what we mean by natural direct effects in natural indirect effects and natural direct natural indirect effects.

735
01:37:11,550 --> 01:37:14,940
I know that this is like really complicated thinking about conceptually,

736
01:37:15,270 --> 01:37:19,830
but apparently from like a formula information perspective, it's like very beautiful.

737
01:37:20,490 --> 01:37:27,750
So that's why people have come up with this and have done like this, which, you know, we're not going to be doing this for this class.

738
01:37:29,670 --> 01:37:40,330
But the other thing is that the natural, direct effect in the natural indirect effect sort of mimic what Sobel and Bernanke were trying to do.

739
01:37:40,350 --> 01:37:47,010
So Bernie Kenney Sobel sort of did this in a very simplistic way, but then people later on tried to decompose things.

740
01:37:47,370 --> 01:37:50,040
And this decomposition is kind of what I was showing you.

741
01:37:51,510 --> 01:37:56,820
That is where people came up with this like natural direct effect in natural indirect effects.

742
01:37:58,230 --> 01:38:11,219
So one last twist of things to deal with is I do want you to think about controlled direct effects, except for no,

743
01:38:11,220 --> 01:38:16,650
I've been harping on you for the last couple of seconds about what a natural, direct effect means.

744
01:38:17,670 --> 01:38:26,430
This is one way of writing it down. So how much would an outcome change if the treatment changes from zero to x equals one?

745
01:38:26,460 --> 01:38:28,410
Again, this is what we are forcing people to change,

746
01:38:29,670 --> 01:38:35,810
but then we're setting the mediator to naturally vary at a level that would have been taken in the absence of useful.

747
01:38:37,890 --> 01:38:41,370
A controlled direct effect is, I think, much easier to grasp.

748
01:38:42,060 --> 01:38:47,970
So a controlled direct effect is we are setting the mediator at a certain level.

749
01:38:51,560 --> 01:38:57,860
So in her example of looking at coffee and diabetes, we would have a controlled,

750
01:38:57,860 --> 01:39:04,280
direct effect for when people have a high sugar diet and we have a controlled direct effect for when people have a low sugar diet.

751
01:39:04,910 --> 01:39:12,350
So the problem with controlled direct effects is there are as many controlled direct effects as there are levels of your mediator.

752
01:39:13,460 --> 01:39:17,510
So a natural, direct effect is sort of averaging across all of those different levels.

753
01:39:19,100 --> 01:39:24,530
So in a way, a natural direct effect is an average across different controlled direct effects,

754
01:39:25,550 --> 01:39:29,600
but in actual direct effect, it's like very complicated to interpret that beautiful math.

755
01:39:29,990 --> 01:39:39,200
I think a controlled direct effect is probably kind of what we're thinking about a bit more, but the math doesn't line up very nicely all the time.

756
01:39:44,350 --> 01:39:48,550
This gets to the thing that like probably matters, like all the other stuff was, you know,

757
01:39:48,820 --> 01:39:54,630
like a bit confusing thinking about like all these odd factual, you know, different alternate timelines.

758
01:39:54,640 --> 01:40:01,180
What do we actually report in a paper? You probably either report the proportion mediated or the proportion eliminated.

759
01:40:02,020 --> 01:40:10,540
So these are sort of analogous things, but the proportion mediated kind of has to do with natural,

760
01:40:10,540 --> 01:40:15,490
direct and natural indirect effects, whereas the proportion eliminated has to do with control, direct effects.

761
01:40:15,790 --> 01:40:18,790
And for whatever reason, we just do not refer to controlled indirect effects.

762
01:40:18,790 --> 01:40:26,320
So instead of having the sort of controlled indirect effects, we are just doing a subtraction or total effect minus the control direct.

763
01:40:27,310 --> 01:40:31,600
But what different sort of reasons the proportion mediated. This is something which, you know,

764
01:40:31,600 --> 01:40:39,310
maybe long term you'd remember a person mediated that is using these natural numbers

765
01:40:39,880 --> 01:40:43,690
and that is the extent to which the total effect operates through the mediator.

766
01:40:43,690 --> 01:40:46,860
And I think this is kind of what people think of when they think of use.

767
01:40:46,870 --> 01:40:58,360
It's like if we go back to our example here, proportion mediated is like of of this total effect here,

768
01:40:58,720 --> 01:41:10,610
what proportion is going through this versus this? Proportioned eliminated is a slightly different way of thinking of things.

769
01:41:12,290 --> 01:41:23,720
The proportion eliminated is like if we can do a policy to intervene on the media, if we can get rid of any effect of the mediator.

770
01:41:26,170 --> 01:41:35,550
What would happen. So in our example of coffee and diabetes, the proportion eliminated is measuring.

771
01:41:35,850 --> 01:41:45,749
You know, if we were able to get rid of the effect of high sugar diets, if we were able to completely eliminate that,

772
01:41:45,750 --> 01:41:51,240
maybe we have some policy intervention where we, you know, force people to have low sugar diets.

773
01:41:54,060 --> 01:42:00,300
What? How much, then, of the effect of coffee on diabetes would be reduced?

774
01:42:02,620 --> 01:42:05,949
So basically I would you know, the the impact of coffee,

775
01:42:05,950 --> 01:42:12,220
would that change if we were able to do a policy decision to eliminate the effect of the media?

776
01:42:12,930 --> 01:42:14,440
So oftentimes these are very similar.

777
01:42:14,440 --> 01:42:21,940
Sometimes they're not it kind of has to do with it is the natural indirect effect is that or is the natural direct effect?

778
01:42:21,940 --> 01:42:24,760
Is that the same as the control direct effect?

779
01:42:27,550 --> 01:42:33,820
You know, that's that's a theoretical consideration that would change based on which of the your specific theory.

780
01:42:36,050 --> 01:42:42,800
Okay. So a lot of said today, I would recommend you to watch my video.

781
01:42:42,920 --> 01:42:47,060
I will upload that I have done here.

782
01:42:47,600 --> 01:42:52,399
Please make an appointment to see me if this is confusing, but I would definitely start here.

783
01:42:52,400 --> 01:42:54,200
Make sure that this sort of makes sense.

784
01:42:54,200 --> 01:43:01,400
I have the results of this article here, which explains it well and then, you know, work backwards from there.

785
01:43:01,700 --> 01:43:09,560
I think the homework assignment kind of hold your hand a bit as you go through an example of the barium penny and the symbol test approach.

786
01:43:10,460 --> 01:43:18,410
And then I just give you the results of this decomposition of like natural direct effects and indirect effects.

787
01:43:18,830 --> 01:43:22,729
So that part is, you know, just looking at the after effects of it.

788
01:43:22,730 --> 01:43:30,470
And I don't want to have to, you know, process this completely, but I wanted to give you a taste of what what caused the difference in epidemiology.

789
01:43:30,920 --> 01:43:35,660
Oh, yeah. I'm sure your brains are absolutely fried, so enjoy the rest of your.

790
01:43:44,400 --> 01:43:48,809
After a year like this.

791
01:43:48,810 --> 01:43:54,440
Why are you. To the wild.

792
01:43:54,460 --> 01:43:59,910
I just made so many things that. When we've done this before.

793
01:44:00,090 --> 01:44:05,170
Why is simply. Sometimes you got a feeling like that.

794
01:44:12,450 --> 01:44:21,950
So. Yes.

795
01:44:27,550 --> 01:44:31,480
I don't really have that kind of feel. I guess you could. You could call it that.

796
01:44:33,970 --> 01:44:39,140
Oh, yeah. Thank you very much.

