1
00:00:02,020 --> 00:00:09,680
All right. We're now recording. There's so many people that are apparently watching these.

2
00:00:09,990 --> 00:00:19,260
It's important to get that recording going. So last time we talked about risk assessment, we're going to continue on with the risk assessment here.

3
00:00:19,420 --> 00:00:22,070
There's a lot of these topics sort of bleed into each other.

4
00:00:22,080 --> 00:00:26,850
So we have like risk assessment and then later we're going to get into risk communication.

5
00:00:26,860 --> 00:00:32,790
So once we understand risk, how do we communicate to the stakeholders what the risk is?

6
00:00:32,790 --> 00:00:34,860
And we'll talk a little bit about that.

7
00:00:37,530 --> 00:00:44,519
I think this is where we left off last time, where we were talking about animal bio assays and how there's, you know,

8
00:00:44,520 --> 00:00:52,380
issues that come up in terms of using animal bio assays for risk assessment, such as limitations on the number of doses tested in some cases.

9
00:00:52,860 --> 00:00:56,190
Um, and kind of differences between species.

10
00:00:56,490 --> 00:01:05,250
The model organisms may not, you know, completely replicate the biology of humans in all cases of these types of things.

11
00:01:06,330 --> 00:01:14,700
Yeah. So. And we're trying to sort of address that and in a lot of cases,

12
00:01:14,700 --> 00:01:19,469
so there's this effort with the collaborative cross mouse models where you're

13
00:01:19,470 --> 00:01:23,670
taking instead of using the standard sort of genetically identical mouse strains,

14
00:01:23,670 --> 00:01:27,899
you're using like genetically diverse mouse strains,

15
00:01:27,900 --> 00:01:34,559
it's like breeding them together in order to generate like a genetically diverse population of mice

16
00:01:34,560 --> 00:01:40,560
that sort of more closely represent the genetic diversity that we see in actual human populations.

17
00:01:41,130 --> 00:01:48,740
So that's a pretty cool project and you'll see pretty interesting papers using that, uh, those collaborative cross mice as well.

18
00:01:48,750 --> 00:02:02,230
So. So another type of data that we use in addition to like toxicology data is like epidemiological epidemiological data.

19
00:02:03,670 --> 00:02:06,489
And this is really in terms of like human health risk assessment.

20
00:02:06,490 --> 00:02:11,440
This is going to be the most convincing because you're it's in the species of interest rate.

21
00:02:11,440 --> 00:02:20,200
It's in humans. We're looking at whether an environmental exposure is leading to a disease outcome, which is, you know,

22
00:02:20,200 --> 00:02:25,830
pretty that's pretty much, you know, what we're trying to do with our environmental risk assessment.

23
00:02:25,840 --> 00:02:32,590
So epidemiology data is very important in understanding these links between exposures and disease,

24
00:02:32,890 --> 00:02:35,920
but it can also be challenging for a number of reasons.

25
00:02:37,720 --> 00:02:42,460
So we're often, you know, doing these sort of exploratory studies with epidemiology.

26
00:02:43,750 --> 00:02:50,020
We don't have necessarily a really strong, like a mechanistic hypothesis.

27
00:02:50,020 --> 00:03:01,090
When we start to do some of these studies like we do with toxicology, in a lot of cases, measuring levels of exposure to toxicants can be a challenge.

28
00:03:01,510 --> 00:03:08,260
So if someone is exposed to, say, phthalates on the job in an occupational setting,

29
00:03:08,260 --> 00:03:15,820
how do you sort of control or understand their exposures to, say, phthalates outside of their job?

30
00:03:16,480 --> 00:03:20,470
It's sort of like modeling the exposure scenarios can be sort of challenging there.

31
00:03:21,940 --> 00:03:26,200
People are exposed to complex mixtures of chemicals and we've talked about that a few times.

32
00:03:26,680 --> 00:03:36,700
How do you isolate one particular chemical exposure if like multiple types of chemicals might be impacting one particular biological pathway or,

33
00:03:37,030 --> 00:03:40,689
you know, we may be exposed to multiple carcinogens per day.

34
00:03:40,690 --> 00:03:46,780
So if we're trying to isolate exposure to a particular one toxicant,

35
00:03:46,780 --> 00:03:52,200
it can be kind of a challenge to sort of pull that out of the mix of exposures we encounter data that.

36
00:03:55,130 --> 00:03:59,120
Yeah. And there's often like this trade off that we have to do between either having a lot of

37
00:03:59,120 --> 00:04:04,489
detailed information on like a small subset of people and shallow information on like a big,

38
00:04:04,490 --> 00:04:06,080
big group of people. Right.

39
00:04:06,470 --> 00:04:13,670
So you'll see like certain types of studies, the way they're designed, they're like really like controlling the diet of like a cohort of people,

40
00:04:15,140 --> 00:04:18,680
you know, like doing like other things like that or like, say, doing this.

41
00:04:18,680 --> 00:04:25,280
Um, uh, like I was talking about with the air pollution where you can bring people in and do like have them exercise

42
00:04:25,670 --> 00:04:30,920
in a room with like purified air versus a room with normal air to see if that impacts their health.

43
00:04:31,730 --> 00:04:38,150
But there's like a difference in health outcomes between, ah, like, you know, cardiovascular outcomes between the two air sources there.

44
00:04:38,990 --> 00:04:43,140
So that's like some pretty detailed like kind of, you know, getting into, you know,

45
00:04:43,160 --> 00:04:51,620
controlling like specific aspects of a cohort or like really well characterizing a cohort like, you know, following them very closely,

46
00:04:51,620 --> 00:04:55,609
having a lot of follow ups versus like some people they're looking and they're working

47
00:04:55,610 --> 00:05:01,100
is like the big data space are like taking like national exposure data and doing,

48
00:05:01,220 --> 00:05:07,400
you know, we've published a couple of papers on that where we take like the and Haines exposure, which is like hundreds of thousands of people.

49
00:05:07,730 --> 00:05:13,970
And so, you know, there's a limit to the amount of information you can get for that many people, right?

50
00:05:14,480 --> 00:05:21,800
Does that make sense? Has anyone does anyone have like epidemiology background or have had courses you got.

51
00:05:21,830 --> 00:05:28,879
Okay. Yeah, great. Um, so we'll talk about the CB, be able to jump in and help me out with some of these because I'm not an epidemiologist.

52
00:05:28,880 --> 00:05:32,360
So if I missed something, please feel free to let me know.

53
00:05:32,750 --> 00:05:37,610
Are you in the epidemiology program? Yeah. We're an occupational and environmental epi.

54
00:05:37,790 --> 00:05:42,310
Okay, great. Yeah, so I'll do my best, but please feel free to jump in.

55
00:05:42,350 --> 00:05:46,549
Let me. Let me know if I do something. Um, yeah.

56
00:05:46,550 --> 00:05:49,730
So you have this trade off, and then humans again are genetically diverse.

57
00:05:49,970 --> 00:05:56,750
So, as opposed to us in our nice toxicology studies are we're getting like all of the same animal we're working with in epidemiology.

58
00:05:56,750 --> 00:06:04,070
You're working with genetically diverse populations. Are you familiar with these different types of epidemiology study designs?

59
00:06:05,780 --> 00:06:10,069
So to somewhat to tell me what a cohort design is, it's a little bit up there,

60
00:06:10,070 --> 00:06:15,650
but maybe just put it in your own words, like what a cohort is, like what a cohort study design is.

61
00:06:20,000 --> 00:06:27,500
Here we go. You just take a group of people and you look at them over a period of time and you kind of track how their exposure to disease are.

62
00:06:27,830 --> 00:06:29,570
Yeah, exactly. Yeah. So.

63
00:06:29,870 --> 00:06:37,639
Well, you would typically like it in terms of like a toxicology type of study or like a like a chemical environmental exposure study.

64
00:06:37,640 --> 00:06:44,629
You're taking two groups of people, one that has been exposed to a chemical and one that's not been exposed typically.

65
00:06:44,630 --> 00:06:53,240
So you might see this designed in a way where there may have been like historical contamination of the drinking water in like this part of the town.

66
00:06:53,240 --> 00:06:57,950
But this other part of the town was served from a different drinking water source that was not contaminated.

67
00:06:58,220 --> 00:07:03,500
And so you basically can groups of people into two groups of exposed and non exposed and then you follow

68
00:07:03,500 --> 00:07:09,920
them from that point on through time to see if anyone develops an adverse health outcome in that case.

69
00:07:10,340 --> 00:07:13,490
So your sample composition, so this is like prospective, right?

70
00:07:14,150 --> 00:07:19,790
That your sample composition is going to be non disease individuals and you can see the proportions.

71
00:07:19,790 --> 00:07:25,130
So the comparisons with the proportion of exposed individuals that have the disease versus the non exposed group.

72
00:07:26,300 --> 00:07:28,000
Sounds pretty familiar right.

73
00:07:28,000 --> 00:07:35,030
To like if you've had like an epidemiology course before and then case control, can anyone tell you what case control is?

74
00:07:37,970 --> 00:07:41,600
Like usually if you have a rare disease outcome.

75
00:07:42,050 --> 00:07:46,070
And so you start with people like a sample of people who have the disease and then

76
00:07:46,070 --> 00:07:50,630
compare them to controls who are similar in other ways but don't have the disease.

77
00:07:50,900 --> 00:07:57,830
Yeah. Yeah. And then in the context of like environmental toxicology, we then want to understand,

78
00:07:57,830 --> 00:08:01,280
like if this group of people developed liver cancer and people didn't.

79
00:08:01,490 --> 00:08:06,110
Was there a difference in their exposure? And so that's looking retrospectively.

80
00:08:06,530 --> 00:08:11,329
Did these exposures that occur in the past contribute to the health outcome that you're looking at in

81
00:08:11,330 --> 00:08:18,350
the at that time among these cases that controls the proportions of cases with high levels of exposure.

82
00:08:18,890 --> 00:08:27,680
So the difference there and this will give you like the odds ratio and then cross-sectional is where you're just taking a slice of time and looking.

83
00:08:27,680 --> 00:08:30,559
You can kind of design this in a number of different ways.

84
00:08:30,560 --> 00:08:38,270
You'll see different papers, different studies done in different ways, either like defining it based on control or disease status.

85
00:08:38,750 --> 00:08:42,410
But again, it's like a slice of time. One important thing.

86
00:08:44,020 --> 00:08:48,730
That you're to to bear in mind when you're doing like a cross-sectional study is.

87
00:08:49,150 --> 00:08:52,420
This may sound kind of silly in a way, but it is important.

88
00:08:52,720 --> 00:08:56,680
You're going to be looking at survivors or people that are currently alive.

89
00:08:56,680 --> 00:09:05,810
Right. So can you could anyone hazard a guess as to why it's important that you're looking at people in terms of an environmental exposure?

90
00:09:05,830 --> 00:09:13,630
You're looking at live individuals. You heard of survivorship bias.

91
00:09:13,660 --> 00:09:19,600
So if let's say people are exposed to a chemical and a lot of them have died early,

92
00:09:20,170 --> 00:09:25,030
then they're not going to be included in that slice of that sample of people that you're looking at.

93
00:09:25,240 --> 00:09:31,110
Right. So that's something to bear in mind. And you see some of the comparisons.

94
00:09:31,120 --> 00:09:31,310
Yeah,

95
00:09:31,330 --> 00:09:42,910
this is like kind of a can be either one and then you can give you an example of like prevalence or prevalence of like health outcome risk or rates.

96
00:09:44,200 --> 00:09:47,440
So anything else there? We cover that pretty well.

97
00:09:47,890 --> 00:09:51,010
Anything to add? It's pretty. You're very good.

98
00:09:51,910 --> 00:09:52,600
I will pick on you.

99
00:09:56,080 --> 00:10:02,260
I think we could probably talk about this pretty you know, I think you guys have a good intuitive sense of this stuff at this point.

100
00:10:03,370 --> 00:10:07,270
Could we talk about the advantages and disadvantages of let's start with cohort.

101
00:10:07,750 --> 00:10:09,270
What are the advantages? Yeah.

102
00:10:09,910 --> 00:10:17,530
Oh, you get to follow people and like kind of consider times throughout because sometimes with other studies you might have like recall bias.

103
00:10:17,800 --> 00:10:26,980
Mm hmm. So if you're asking them, like, short term plan, it's pretty good you're following them, kind of you're in time going forward.

104
00:10:27,070 --> 00:10:31,629
So you're not having to reconstruct exposures. It's not retrospective, right?

105
00:10:31,630 --> 00:10:37,750
So recall bias. So people might not recall in some cases in different study designs,

106
00:10:37,750 --> 00:10:44,469
people might not be recalling their exposures or you may not be constructing reconstructing the exposure in an entirely accurate way.

107
00:10:44,470 --> 00:10:50,080
And so cohort is kind of immune to that because you're sort of incorporating

108
00:10:50,080 --> 00:10:54,760
the exposure assessment kind of from the when you design the study forward,

109
00:10:54,760 --> 00:11:00,640
right? So that would be an advantage. But more accurate exposure probably can be obtained there.

110
00:11:01,450 --> 00:11:07,480
Um, any disadvantages? They're expensive.

111
00:11:08,170 --> 00:11:13,960
Expensive. Yeah. So I hear I don't, I don't work in that area, but for sure I would imagine.

112
00:11:15,370 --> 00:11:18,700
How about like the types of diseases you can study with a cohort?

113
00:11:19,360 --> 00:11:23,590
Yeah. If the disease is rare, it can take a really long time to have enough people develop it.

114
00:11:23,890 --> 00:11:32,799
Right? Exactly. If it's a rare disease, then the chances that any one particular pool of people is going to develop enough of this disease to,

115
00:11:32,800 --> 00:11:39,400
like, make it statistically powerful is kind of not is not as great in terms of a like a cohort study.

116
00:11:40,120 --> 00:11:47,259
Yeah. So so lack of bias and exposure yields, rates of incidence and risk for advantages, disadvantages.

117
00:11:47,260 --> 00:11:50,319
Large number of subjects require long follow ups.

118
00:11:50,320 --> 00:11:53,590
So you kind of have to keep following up with these people, you know,

119
00:11:53,590 --> 00:11:58,120
because you're looking to see whether they're going to end up developing the health outcome or not.

120
00:11:58,120 --> 00:12:04,269
So there's a lot of follow up that has to be included. Um, things can change like for when you're recruiting.

121
00:12:04,270 --> 00:12:07,270
Like we've encountered this with some of my collaborators where, you know,

122
00:12:07,480 --> 00:12:12,340
you make certain decisions when you start recruiting and you kind of have to stick with those decisions going forward in some ways.

123
00:12:12,340 --> 00:12:19,180
So like different things can kind of change in terms of like individuals like dropping off or, you know,

124
00:12:19,180 --> 00:12:24,069
different types of criteria applied and different methods in terms of like exposure assessment and stuff.

125
00:12:24,070 --> 00:12:25,690
And you're trying to keep things consistent.

126
00:12:26,200 --> 00:12:32,950
And so if you're following what some of the studies are designed to follow people for years and years, and so things can change.

127
00:12:32,950 --> 00:12:42,310
And so you're kind of locked in when you like develop that cohort right off the bat can be costly and not great for rare diseases.

128
00:12:42,340 --> 00:12:46,710
Right? Case control. Advantages.

129
00:12:48,950 --> 00:12:59,510
It is the inverse of the cohort. It's better for what types of disease or rare diseases.

130
00:12:59,520 --> 00:13:02,820
Right. But then the disadvantages might be.

131
00:13:05,280 --> 00:13:09,860
On the other end of things. With the exposure.

132
00:13:10,790 --> 00:13:17,920
So you have this recall bias issue where, you know, you have a group of people that have developed cancer.

133
00:13:17,930 --> 00:13:25,100
They may be really stringently combing their life history for exposures to different chemicals.

134
00:13:25,820 --> 00:13:33,740
And if you're asking people that are essentially our healthy babies are not paying as much attention to the things that they've been exposed to.

135
00:13:34,260 --> 00:13:41,570
Um, and there's all sorts of different biases that can kind of creep in there in terms of like reconstructing the exposure retrospectively.

136
00:13:41,750 --> 00:13:47,630
So that's a disadvantage. Any other thoughts, advantages or disadvantages to case control?

137
00:13:48,470 --> 00:13:53,959
This is kind of a minor thing, but with case control, you can only see the risk.

138
00:13:53,960 --> 00:13:58,440
And sometimes odds ratios are just like difficult to interpret or understand.

139
00:13:58,470 --> 00:14:03,050
They mean. Yeah. Yeah. Risk is a little bit more intuitive, right?

140
00:14:03,060 --> 00:14:06,750
Like what's your risk of this versus like odds ratios? Kind of, yeah.

141
00:14:08,550 --> 00:14:10,680
Yeah. That's I think that might be in here actually.

142
00:14:10,680 --> 00:14:16,739
So advantages for case control inexpensive you know you can do small number of subjects because you're kind of

143
00:14:16,740 --> 00:14:23,790
selecting you can kind of construct your adverse health outcome versus like your controls in a more controlled way,

144
00:14:24,270 --> 00:14:29,600
um, more suitable for rare diseases. Uh, let's see.

145
00:14:29,610 --> 00:14:32,640
And then of course, like recall bias would be one of the disadvantages.

146
00:14:33,210 --> 00:14:40,070
Um, yeah. Yields relative risk and not rated establishing like causation, etc.

147
00:14:40,080 --> 00:14:46,469
So you could kind of read through those and then we'll just walk through the cross-sectional pretty quick.

148
00:14:46,470 --> 00:14:52,080
You know, you're just like recruiting a section of people in the moment.

149
00:14:52,650 --> 00:14:55,110
Again, you're not really able to establish causation.

150
00:14:55,140 --> 00:15:02,520
There's like an association between exposure, but you're not really able to get at that causation question.

151
00:15:03,570 --> 00:15:09,160
Um, and again, kind of inadequate for rare diseases. So you see these different models unemployed for different reasons.

152
00:15:09,480 --> 00:15:14,410
They all have their advantages and disadvantages, and they're really helpful in risk assessment,

153
00:15:14,740 --> 00:15:19,149
especially if they link up with our toxicology data and the mechanisms that we identify.

154
00:15:19,150 --> 00:15:27,970
The toxicology studies sort of fit with what we would expect in terms of a human health outcome of a chemical is damaging DNA,

155
00:15:28,300 --> 00:15:32,060
and we see people exposed to that chemical or developing cancer.

156
00:15:32,080 --> 00:15:35,050
It's like a fairly coherent biological story there.

157
00:15:38,310 --> 00:15:44,850
So, you know, and then, of course, like all epidemiology studies are not like, you know, created equal, right?

158
00:15:44,850 --> 00:15:45,920
There's like a criteria.

159
00:15:45,930 --> 00:15:52,319
And again, if you look at like an EPA document where they are doing like say, a risk assessment or like an evaluation of this phenol,

160
00:15:52,320 --> 00:15:59,820
a, for example, you'll see them talk about like the strengths and weaknesses of both toxicology studies and epidemiology studies.

161
00:16:00,120 --> 00:16:03,659
And they'll kind of get to a point where they say, okay,

162
00:16:03,660 --> 00:16:10,350
we've included like these ten studies because they we think that they were really well-designed and are controlled for all the possible confounders.

163
00:16:10,650 --> 00:16:15,959
And this is why we're setting our sort of like reference dose based on the data we pulled from these studies.

164
00:16:15,960 --> 00:16:21,480
So these are the types of documents that are pretty interesting, particularly if you want to keep doing research,

165
00:16:21,480 --> 00:16:27,360
you can kind of like understand and get some notes from regulators about like how what a

166
00:16:27,360 --> 00:16:31,829
well-designed study looks like and what they're kind of critiquing out of different studies.

167
00:16:31,830 --> 00:16:35,670
So they're pretty, pretty kind of fun documents to read in a way.

168
00:16:36,450 --> 00:16:42,810
So when we're critiquing or like, you know, understanding the criteria for like an epidemiology study,

169
00:16:43,080 --> 00:16:46,410
we might think think about things like the strength of the association.

170
00:16:47,610 --> 00:16:51,870
Some of the data that we've shown in this class, like, say, what's aflatoxin and liver cancer.

171
00:16:51,870 --> 00:16:59,820
The odds ratios were like something like 2025, you know, especially with some of these other additional factors, like huge odds ratios.

172
00:17:02,320 --> 00:17:11,690
Other studies, there's a lot of studies where the odds ratio is more like something like 1.1 to or 1.5 up to two, you know.

173
00:17:13,060 --> 00:17:19,330
So the strength of the association is one thing that, you know, if it's a really strong association with,

174
00:17:19,330 --> 00:17:23,160
like really high odds ratios, that might be something that, uh.

175
00:17:24,420 --> 00:17:30,390
You know, lead us to sort of put more weight on that epidemiologist study.

176
00:17:31,050 --> 00:17:32,100
Are these consistent?

177
00:17:32,100 --> 00:17:41,520
Are we seeing like these same epidemiological trends in multiple studies like across the world amongst different research groups?

178
00:17:43,410 --> 00:17:52,170
Specificity. So, you know, if there's a specific exposure and a specific health outcome, that's always going to be a little bit.

179
00:17:54,830 --> 00:18:01,399
Good to see her. You know something that might place more weight on if we're just looking at like, say, air pollution kind of in general,

180
00:18:01,400 --> 00:18:07,520
that makes it a little bit harder to understand what that linkage might be, you know, like what in the air pollution is causing.

181
00:18:08,520 --> 00:18:14,999
Asthma or what have you. So the specificity can be something to incorporate.

182
00:18:15,000 --> 00:18:22,440
And then temporal relationship like, um, you know, exposure really should be preceding the health outcome.

183
00:18:22,890 --> 00:18:26,459
You will see, I haven't actually personally read any of these studies,

184
00:18:26,460 --> 00:18:30,990
but apparently there are some studies where they're looking at the health outcome as a predictor

185
00:18:30,990 --> 00:18:36,780
of exposure and some of these kind of odd study designs that I'm not quite sure from,

186
00:18:36,780 --> 00:18:42,900
like sort of a toxicological standpoint, kind of like doesn't make sense. So the temporality is important to consider as well.

187
00:18:44,040 --> 00:18:51,000
Dose responsiveness are people that are exposed to higher levels of the chemical seeing, like higher rates of the health outcome.

188
00:18:54,610 --> 00:18:56,640
Biological plausibility doesn't make sense.

189
00:18:56,650 --> 00:19:03,459
Again, this coherence, if a chemical is damaging DNA and we're seeing people presenting with different types of cancers,

190
00:19:03,460 --> 00:19:06,790
that's a pretty biologically plausible link.

191
00:19:07,030 --> 00:19:11,149
It's their power of detection and how they will design studies.

192
00:19:11,150 --> 00:19:16,360
Do they have the statistical power to find what they're looking for? Appropriateness of outcomes?

193
00:19:16,610 --> 00:19:22,330
Are we looking at like self-reported data or is this like more like objectively assessed kind of stuff?

194
00:19:23,160 --> 00:19:29,830
Um, the quality of exposure assessments, like how good are the methods used for measuring the exposure levels?

195
00:19:30,670 --> 00:19:33,700
How good are confounding factors accounted for?

196
00:19:36,690 --> 00:19:42,030
Um. Yeah. And, you know, ah, can this data be applied to other at risk populations?

197
00:19:42,030 --> 00:19:47,370
So these are the types of things that we could sort of bring in when we're thinking about the criteria for,

198
00:19:47,370 --> 00:19:50,940
like judging an epidemiology study for use and risk assessment.

199
00:19:53,010 --> 00:20:02,239
Double check that I am recording. Okay, great. Kind of, you know, for a little while.

200
00:20:02,240 --> 00:20:06,590
But kind of on the recent side of things, we're seeing a lot of tech and a lot of technical,

201
00:20:06,590 --> 00:20:14,149
logical advances that are making epidemiology more quantifiable and can kind of link up the efforts

202
00:20:14,150 --> 00:20:19,550
of someone like myself who is a more of a mechanistic toxicologist with someone in epidemiology.

203
00:20:20,160 --> 00:20:30,980
So there's this field of like molecular epidemiology where we're quantifying like exposure and biomarkers of either exposure or health outcomes.

204
00:20:31,400 --> 00:20:40,700
So like certain biological markers in the blood that might increase when we know someone is like a, like a marker of inflammation, say,

205
00:20:41,330 --> 00:20:43,639
and we see like with exposure to phthalates,

206
00:20:43,640 --> 00:20:50,060
we're seeing like increased levels of certain inflammatory biomarkers in the blood of exposed individuals.

207
00:20:50,750 --> 00:20:54,709
So there's this which really piques our interest as toxicologists,

208
00:20:54,710 --> 00:21:00,110
because then we want to go back into our models and see how the phthalates might be causing inflammation or

209
00:21:00,110 --> 00:21:04,339
how this might play a role in terms of some of the health outcomes that we think are linked to phthalates.

210
00:21:04,340 --> 00:21:09,920
So there's a nice sort of back and forth there in terms of collaborations between our separate fields.

211
00:21:11,820 --> 00:21:17,640
And this allows us to understand sort of the mechanistic basis for some of these epidemiology studies.

212
00:21:18,720 --> 00:21:27,540
You know, we we predict that this biomarker is going to be increased in this population when they're exposed to phthalates, for example.

213
00:21:28,350 --> 00:21:33,390
And we're going to measure that in the blood of this cohort that we've identified this type of thing.

214
00:21:33,630 --> 00:21:46,590
So it's a nice biological linkage that helps this helps sort of the the again, the biological plausibility end of things.

215
00:21:47,800 --> 00:21:51,100
And the types of techniques that are used in this are things like, say, genomics,

216
00:21:51,100 --> 00:21:56,200
metabolomics, meaning looking at all the metabolites in someone's body,

217
00:21:56,470 --> 00:22:03,510
epigenomics, transcriptomics, all the genes that are transcribed, and then proteomics, all the proteins.

218
00:22:03,520 --> 00:22:13,120
So we're able to look at things like huge amounts of data these days, which is really exciting and kind of challenging.

219
00:22:15,160 --> 00:22:18,560
This is where we start to get into like gene environment interactions.

220
00:22:18,640 --> 00:22:27,280
You remember the data presented with chlorpyrifos where some there's this polymorphism in a gene that metabolizes a chemical.

221
00:22:28,210 --> 00:22:38,830
And we saw that the some of the toxic effects of chlorpyrifos were more pronounced when that when individuals had that polymorphism.

222
00:22:39,220 --> 00:22:46,990
So there's this interaction between an environmental exposure and the genotype of an individual causing a more pronounced effect.

223
00:22:51,880 --> 00:22:56,410
Um. Yeah. And so this is like really exciting technology.

224
00:22:56,920 --> 00:22:58,809
But for quite a while,

225
00:22:58,810 --> 00:23:10,200
the agencies have been sort of debating and like discussing how to incorporate these like relatively new technologies into risk assessment.

226
00:23:10,210 --> 00:23:16,120
Like how do you take a set of Transcriptomic data and apply that in a risk risk assessment context?

227
00:23:16,660 --> 00:23:23,950
So I remember reading documents in like 20, 2010 about how, how do we take this like these big,

228
00:23:23,950 --> 00:23:27,280
like genomic data sets and like apply them and risk assessment.

229
00:23:27,290 --> 00:23:29,110
So it's still this ongoing discussion.

230
00:23:31,140 --> 00:23:37,110
Because there's you know, it's there is a paper that just came out that talked about how there's a lot of genomics,

231
00:23:37,500 --> 00:23:45,840
genomics papers out there that have like big lists of can of genes in an Excel file.

232
00:23:46,320 --> 00:23:50,790
And inadvertently the gene needs got converted into a date.

233
00:23:51,660 --> 00:23:55,950
So anyone that's like downloading that supplementary data and like analyzing it,

234
00:23:56,790 --> 00:24:03,089
you know, there's a lot of kind of so that's why we have this sort of like trepidation,

235
00:24:03,090 --> 00:24:06,050
if you will, about incorporating these into our risk assessments,

236
00:24:06,060 --> 00:24:11,250
because it's when you have like a new technology now, maybe sometimes all the bugs haven't been worked out.

237
00:24:11,790 --> 00:24:21,090
So it's something like I want to say something like 10,000 papers were found to have like this flip of like gene in to date in the Excel file.

238
00:24:21,690 --> 00:24:25,860
So that gives you a sense of the types of issues that are being faced there.

239
00:24:26,100 --> 00:24:30,960
Really powerful technology. But you got to be careful. Any thoughts or questions there?

240
00:24:37,780 --> 00:24:41,940
Okay. So. Quantitative risk assessment. Yeah.

241
00:24:41,950 --> 00:24:47,500
So what we're trying to do is integrate dose response relationships along with the exposure.

242
00:24:49,470 --> 00:24:53,010
Trying to understand variation and susceptibility between individuals,

243
00:24:53,010 --> 00:24:59,969
whether that's because of genotype differences or because of like life stage differences or underlying health issues.

244
00:24:59,970 --> 00:25:09,330
All of those can contribute to susceptibility and then also quantify or at least understand the uncertainty inherent in some of our analysis.

245
00:25:09,450 --> 00:25:15,870
We talked about kind of sources of uncertainty is our are our methods of detecting certain

246
00:25:15,870 --> 00:25:22,190
molecules like powerful or sensitive enough in some cases like to get the true level of exposure.

247
00:25:23,190 --> 00:25:30,720
You'll see some like exposure studies where there's a lot of data that's below limit of detection, you know, these types of things.

248
00:25:30,960 --> 00:25:37,380
And the technology is always improving. So. That can help to address that uncertainty issue.

249
00:25:40,310 --> 00:25:46,430
So generally with risk assessment, you're wanting to identify like one adverse health effects.

250
00:25:47,520 --> 00:25:53,250
You know, liver cancer or neurotoxicity or something to that effect.

251
00:25:56,200 --> 00:26:01,300
So. And generally, I think this probably makes pretty intuitive sense.

252
00:26:01,660 --> 00:26:04,899
You're wanting to when you're like looking across all these different datasets that

253
00:26:04,900 --> 00:26:09,850
might be looking like conducting studies on the chemical and this health effect.

254
00:26:10,300 --> 00:26:13,270
You're going to want to choose the data sets that show this adverse effect at

255
00:26:13,270 --> 00:26:17,320
the lowest levels of exposure and through the most relevant exposure routes,

256
00:26:17,560 --> 00:26:21,700
which sort of makes intuitive sense, right? We want to be protective.

257
00:26:21,940 --> 00:26:27,250
So if effects are occurring at really low levels, that's kind of where we want to be focusing our attention.

258
00:26:29,520 --> 00:26:34,860
And then, you know, relevant exposure outs. Like we always want to be looking at data that is.

259
00:26:35,930 --> 00:26:40,700
If you have if you have a chemical where the exposure occurs through like oral routes

260
00:26:40,700 --> 00:26:45,889
and you're only looking at studies where the chemical is delivered by IV injection,

261
00:26:45,890 --> 00:26:53,210
you know, not the best it's best to keep it as relevant to the actual risk scenario that you're analyzing.

262
00:26:53,300 --> 00:27:04,250
Right. And so the critical adverse effect is defined as the adverse biological outcome that occurs at the lowest exposure level,

263
00:27:04,310 --> 00:27:07,430
which again, I think fairly intuitive. We're trying to be protective.

264
00:27:07,430 --> 00:27:13,310
So if something some health effect occurs at the lowest level, that's the one we're probably going to want to focus on first.

265
00:27:18,100 --> 00:27:22,209
So we can walk through some of the main steps in quantitative risk assessment.

266
00:27:22,210 --> 00:27:24,220
So we're going to identify the available studies.

267
00:27:25,220 --> 00:27:30,590
Evaluate the dose response relationships for all these endpoints, focusing on the one you're most interested in.

268
00:27:31,160 --> 00:27:37,310
Uh, or I mean, yeah, all the various endpoints and then identify the critical effect that occurs at that lowest level.

269
00:27:38,930 --> 00:27:42,740
And the idea is to determine a point of departure for the critical effect.

270
00:27:43,280 --> 00:27:48,020
So that would be, you know, when are we starting to see this adverse health effects?

271
00:27:48,620 --> 00:27:58,620
At what dose does that happen? Right. So examples of what a critical effect might be would be like in animal bio assays,

272
00:27:58,620 --> 00:28:04,079
like changes in weight gain, particularly like in newborn rat pups, for example.

273
00:28:04,080 --> 00:28:10,050
They're like, they stop gaining weight when they're exposed to this chemical. We can look at something like liver enzymes.

274
00:28:10,620 --> 00:28:13,440
Changes in liver enzymes could indicate damage to the liver.

275
00:28:14,190 --> 00:28:23,489
Um, decrease in the number of offspring definitely would be like a red flag for developmental toxicants and obviously,

276
00:28:23,490 --> 00:28:28,360
you know, increased rates of malformed offspring, again developmental toxicity tumors.

277
00:28:28,970 --> 00:28:34,410
Um, so the idea again is to identify the dose at which the most sensitive critical effect happens,

278
00:28:34,410 --> 00:28:36,810
and then you want to look at the most sensitive species.

279
00:28:37,440 --> 00:28:43,830
So again, the idea here is like you want to be protective, you're looking at where the most sensitive effects are occurring.

280
00:28:48,110 --> 00:28:53,419
And so we use for a lot of end points.

281
00:28:53,420 --> 00:28:57,230
For like most end points, we use what you call what's called a threshold approach,

282
00:28:57,890 --> 00:29:05,660
meaning that we essentially identify a dose that is predicted to cause and effect.

283
00:29:06,350 --> 00:29:16,390
Uh. And kind of derive our reference doses and daily intake from using that, using that dose.

284
00:29:16,400 --> 00:29:23,480
Right. So again, you remember like the Lowell, Noel Lowell is the lowest observed adverse effect level.

285
00:29:24,050 --> 00:29:27,470
And then the Noel is the known observed adverse effect level.

286
00:29:28,280 --> 00:29:37,470
But the Noel is the highest dose, which does not cause a significant change in the response that we're looking at.

287
00:29:37,490 --> 00:29:37,820
Right.

288
00:29:38,270 --> 00:29:45,060
And so you see, like once you start, once you see the significant increase in response, that's what we would call like breaking through the threshold.

289
00:29:45,110 --> 00:29:50,060
Right. Um, and so that's kind of the approach that's used for most types of toxicity.

290
00:29:53,590 --> 00:30:01,240
Yeah. So Noel is on a traditional basis for risk assessment. So you identify this one dose that you used in your study and then you.

291
00:30:04,570 --> 00:30:07,960
Do your uncertainty factors. You remember the uncertainty factors.

292
00:30:08,500 --> 00:30:18,310
So one, we divide the dose by ten to account for the species differences and then divide the dose by ten again to account for human variability.

293
00:30:19,510 --> 00:30:25,510
And so that's how we arrive at our like our reference dose or acceptable daily intake.

294
00:30:27,250 --> 00:30:34,150
Right. So like the no. Well, divided by is uncertainty factors acceptable daily intake, again altered from this Noel.

295
00:30:37,990 --> 00:30:46,370
And this is all revealed, right? And then we also talked about the benchmark dose approach,

296
00:30:46,370 --> 00:30:54,960
which allows us to expand beyond just being tied to that one dose that we've tested as our Noel.

297
00:30:55,730 --> 00:30:59,990
So in the benchmark dose, we talked about this already, but you know,

298
00:30:59,990 --> 00:31:07,520
you generate a dose response curve and then what you do is model the entire curve and you're

299
00:31:07,520 --> 00:31:11,820
going to get like confidence limits around like what you predict the curve would be.

300
00:31:11,840 --> 00:31:17,090
So you're modeling the data through these points and there's different statistical models you can use.

301
00:31:17,600 --> 00:31:21,889
And then essentially what you do is you set one particular response and say that

302
00:31:21,890 --> 00:31:29,540
response is where we think the is significant in terms of a significant impact.

303
00:31:30,600 --> 00:31:34,739
And so oftentimes you'll see 10% used as a sort of rule of thumb.

304
00:31:34,740 --> 00:31:39,780
So like a 10% increase in a particular liver enzyme is considered like significant.

305
00:31:40,410 --> 00:31:44,250
And that way you essentially are like drawing. This isn't a very technical way to describe this,

306
00:31:44,250 --> 00:31:50,730
but you're essentially drawing the line over from that 10% to where intersects with that fit and dose response curve.

307
00:31:52,120 --> 00:31:55,450
And so you could see how that's not one of the doses you tested.

308
00:31:55,750 --> 00:32:03,040
So it gives you a little bit more flexibility than with that Noel approach, because the Noel by definition has to be one of the doses you tested.

309
00:32:03,520 --> 00:32:11,980
So this really has come in to more favor because it allows people a little bit more, uh, you know, um, flexibility I guess.

310
00:32:12,700 --> 00:32:18,280
So that all make sense. Again, that's a review, but I thought it was a good time to revisit it and just make sense.

311
00:32:22,920 --> 00:32:35,420
Okay. However, in some cases, like with cancer caused by radiation, we use a non threshold approach which is not a non-controversial way to do it.

312
00:32:35,420 --> 00:32:38,450
But I'll walk through some of the thought process here.

313
00:32:40,130 --> 00:32:45,290
So one, what this is. So with the threshold approach, you're basically yeah, the threshold approach,

314
00:32:45,290 --> 00:32:52,220
you're basically saying there's no significant not expected to be any significant health impact unless you hit this particular point.

315
00:32:52,790 --> 00:33:02,390
In terms of the dose response, whether that's on your, you know, using your benchmark dose response or you're using your sorry,

316
00:33:02,720 --> 00:33:10,250
uh, your no approach, like your, your, your all of these doses are predicted to not cause a significant health impact.

317
00:33:10,250 --> 00:33:13,280
Right? However, with a not threshold response.

318
00:33:14,420 --> 00:33:23,360
Like in the case of environmental radiation, where a city, no safe dose of radiation above background is entirely safe.

319
00:33:23,360 --> 00:33:29,870
There's some risk with exposure to radiation, even if it's very low and it's a very low risk.

320
00:33:30,020 --> 00:33:36,620
But there is some risk there. So the way we model this is like.

321
00:33:37,550 --> 00:33:42,410
So the idea here is that it's sometimes called the linear non threshold approach.

322
00:33:42,950 --> 00:33:51,140
So this is basically the idea that let's say a dose of radiation causes one excess cancer in a group per a thousand people,

323
00:33:51,560 --> 00:33:59,210
then one 1000, 1/1000 of that dose will cause one excess cancer in a million per million people.

324
00:33:59,540 --> 00:34:08,659
Right. Does that make sense? So as you're taking these doses of radiation down lower and lower, there is excess cancer that you're going to see.

325
00:34:08,660 --> 00:34:12,050
But it's just a very low rate, if you will.

326
00:34:12,800 --> 00:34:16,850
Is this also called probabilistic approaches or is that something different from this?

327
00:34:17,430 --> 00:34:23,420
Gosh, that's a good question. I think this is different, but I'd have to.

328
00:34:23,450 --> 00:34:27,139
I'd have to go. I'd have to dig into that a little more. That's a good question.

329
00:34:27,140 --> 00:34:30,350
I know. I believe these probabilistic modeling for cancer, too.

330
00:34:30,350 --> 00:34:33,590
It doesn't research on that, but. Yeah.

331
00:34:34,160 --> 00:34:37,370
Yeah, that's an interesting I don't know if it's considered probabilistic.

332
00:34:37,560 --> 00:34:41,540
Yeah. Okay. Yeah. I'd like to talk to you more about that. That's yeah, that's a good question.

333
00:34:44,950 --> 00:34:50,019
Yeah. So this has like a really large effect on radiation public policy because the way it's sort of

334
00:34:50,020 --> 00:34:54,880
talked about is a little bit different than the way we talk about what the threshold response.

335
00:34:55,120 --> 00:35:02,500
Right. So you're you are like sort of acknowledging that there's going to be a risk of an impact, even low, low levels of radiation.

336
00:35:02,830 --> 00:35:06,690
And so in terms of like this will be a way to sort of transition, maybe to risk communication.

337
00:35:07,090 --> 00:35:11,590
When you talk about that, even, you know, for a lot of people, even if you're talking about a really,

338
00:35:11,590 --> 00:35:16,240
really low probability of getting cancer, you're still talking about it happening.

339
00:35:16,250 --> 00:35:26,170
So it's kind of this different context. Does that make sense? And so that that kind of drives kind of the way the policy is utilized and communicated.

340
00:35:26,900 --> 00:35:32,590
Um, you know, it's still considered precautionary, but again, it's pretty controversial.

341
00:35:33,760 --> 00:35:38,680
And the idea is that like you may be causing some psychological distress or like mental health impacts,

342
00:35:38,680 --> 00:35:44,830
which psychosocial stress, as you know, we're finding out in and of itself is sort of like an exposure in a way.

343
00:35:44,980 --> 00:35:48,730
You know, we do have changes in our biochemistry when we're under a high degree of stress.

344
00:35:49,390 --> 00:35:58,570
And so you don't want to be putting undue psychosocial stress on someone, um, in communicating some of these issues, right?

345
00:35:59,590 --> 00:36:04,830
Um, yeah. So if, you know, for instance, if someone hears that they were exposed to some, you know,

346
00:36:05,560 --> 00:36:11,980
radiation event that may be quite small, you know, are you causing extra stress to them when you're talking about this risk?

347
00:36:16,100 --> 00:36:22,700
So we can think about this, like if we have a dose response and this gets into maybe even beyond radiation and cancer,

348
00:36:22,700 --> 00:36:31,310
but how we deal with modeling dose response curves below when we don't have the the dose response data, right.

349
00:36:32,090 --> 00:36:36,560
So doses use in animals are often much higher than levels humans are exposed to.

350
00:36:37,940 --> 00:36:42,350
And so oftentimes this will require extrapolating beyond the dose response curve,

351
00:36:44,390 --> 00:36:50,870
i.e. modeling what happens at low doses, but using data from studies using higher doses.

352
00:36:51,890 --> 00:36:57,650
And so you can imagine this extrapolation. You're going to you're going to have choices in terms of the types of modeling you

353
00:36:57,650 --> 00:37:02,240
do when you're modeling the dose response curve below outside of where you tested.

354
00:37:04,350 --> 00:37:10,700
Um, yeah. So, you know, we're extrapolating to like very low doses beyond the biologically observed response rates.

355
00:37:11,630 --> 00:37:15,590
So imagine if you have doses here. Up to this point.

356
00:37:16,400 --> 00:37:23,000
If you're trying to predict a dose down here like this, how does the rest of that curve look?

357
00:37:23,420 --> 00:37:29,760
Are you going to model it in this fashion, this like super linear fashion, this or this?

358
00:37:29,780 --> 00:37:34,190
You know, there's different statistical models you can apply to sort of fill out the rest of that curve.

359
00:37:35,060 --> 00:37:38,390
And so this becomes like an area of a lot of discussion and debate.

360
00:37:41,850 --> 00:37:50,670
And again, going back to the radiation and cancer idea, a lot of this came from like survivors of Hiroshima and Nagasaki bombings.

361
00:37:51,960 --> 00:37:58,080
So they the researchers fit it like they could kind of identify people that were exposed

362
00:37:58,080 --> 00:38:02,639
to high levels of radiation versus those that are exposed to like above background,

363
00:38:02,640 --> 00:38:05,610
but fairly like relatively lower levels of radiation.

364
00:38:05,880 --> 00:38:13,780
And there appeared to be to that the researchers, a linear response between the dose of radiation and risk of cancer.

365
00:38:13,800 --> 00:38:17,100
So you could see a kind of going down here all the way.

366
00:38:17,610 --> 00:38:24,390
And I've heard I think some people might quibble with whether that's actually like a good line or fit through that data.

367
00:38:25,290 --> 00:38:28,500
But I'll just leave it there for now.

368
00:38:29,910 --> 00:38:36,750
Yeah. So the idea is that you're getting some level of risk even as you're approaching like a really low level of radiation.

369
00:38:37,170 --> 00:38:40,770
The idea would be like theoretically if you could like just decrease your sample size,

370
00:38:40,770 --> 00:38:45,030
let's say you're doing an animal biopsy of like more and more and more mice

371
00:38:45,030 --> 00:38:47,910
that you're dosing with like lower and lower and lower levels of radiation.

372
00:38:48,240 --> 00:38:56,250
You could theoretically detect like a non-zero increase in risk of cancer among those mice if you were able to just like,

373
00:38:57,060 --> 00:39:00,690
you know, pump the numbers up to get the statistical power.

374
00:39:01,170 --> 00:39:04,050
Does that make sense? So it's sort of a different idea.

375
00:39:04,310 --> 00:39:10,080
You know, the idea kind of being with this like one hit idea, we're like one like impact on DNA,

376
00:39:10,290 --> 00:39:16,200
like a change, like one DNA base could like lead to a cell becoming cancerous.

377
00:39:16,380 --> 00:39:20,130
So it's kind of different sorts of differences in terms of the mechanism at play

378
00:39:20,130 --> 00:39:27,390
here that sort of go into this way of thinking about this non threshold approach.

379
00:39:28,230 --> 00:39:39,270
I've seen some papers more recently talk about and propose taking this approach into like the endocrine disruption world of like

380
00:39:39,690 --> 00:39:50,760
any extra estrogen or estrogenic type of chemical in your body above background is potentially could cause some kind of effect.

381
00:39:51,270 --> 00:39:54,750
So again, active area of discussion and debate.

382
00:39:58,570 --> 00:40:05,590
And again, this is just this idea of like how we extrapolate from a dose where we've tested it down to like,

383
00:40:05,710 --> 00:40:07,750
you know, how we fill in the gaps, so to speak.

384
00:40:08,470 --> 00:40:19,450
So you can have like connecting this to that to the 020 dose here you have a super linear, linear model, linear quadratic.

385
00:40:21,220 --> 00:40:25,780
And there's even folks that talk about this concept of form.

386
00:40:25,780 --> 00:40:30,130
Is this has anyone ever heard of horror basis? You want to describe what it is?

387
00:40:30,250 --> 00:40:38,410
I can't remember. I've heard it before, but it's basically the idea that like a little bit of radiation is actually kind of good for you.

388
00:40:38,500 --> 00:40:43,660
Like it sort of zeros up the body's responses to, like, combat cancer.

389
00:40:44,110 --> 00:40:46,059
And so that's kind of represented here.

390
00:40:46,060 --> 00:40:53,290
We're like a little bit is actually going to decrease your cancer risk and you even get folks that it's sort of, oh, I think it's started.

391
00:40:53,560 --> 00:40:58,750
So even the folks that like go to certain like caves,

392
00:40:58,750 --> 00:41:08,650
I guess in some cases that are like higher like like above the normal background radiation levels too as an almost therapeutic type of situation.

393
00:41:09,310 --> 00:41:12,549
Not something that I would personally want to do.

394
00:41:12,550 --> 00:41:17,500
But it is an interesting thought and we do see these like, you know,

395
00:41:17,500 --> 00:41:24,910
just these like odd don't you shape those response curves for like nutrients and for certain toxicants.

396
00:41:25,390 --> 00:41:28,600
So, you know, it is what it is. So.

397
00:41:29,960 --> 00:41:38,350
But this is just to show the different ways you can model the outside of the realm of where you've tested your doses that right.

398
00:41:43,490 --> 00:41:48,080
So it's all of that. That was kind of a real quick felt kind of quick breeze through.

399
00:41:48,110 --> 00:41:58,519
Like a lot of the risk assessment process. I think there's a couple of quotes that helped us sort of illustrate the tensions or,

400
00:41:58,520 --> 00:42:04,120
you know, things in terms of how we assess environmental chemicals for their risk.

401
00:42:04,130 --> 00:42:12,680
Right. So like, for instance, there's this quote from Bradford Hill, All scientific work is incomplete, whether it be observational or experimental,

402
00:42:12,950 --> 00:42:19,159
all scientific work is liable to be upset or modified by advancing knowledge that does not confer upon us the

403
00:42:19,160 --> 00:42:24,290
freedom to ignore the knowledge we already have or postpone the action that it appears to demand at a given time.

404
00:42:25,040 --> 00:42:30,440
And so oftentimes, you know, it can be frustrating because.

405
00:42:31,510 --> 00:42:40,060
A lot of people in a lot of different stakeholders want a yes or no answer and want it to be this level of this chemical is safe.

406
00:42:40,540 --> 00:42:44,020
Done deal. But we know that science doesn't work that way.

407
00:42:44,050 --> 00:42:47,080
Right. And so risk assessment is informed by science or it should be.

408
00:42:47,410 --> 00:42:50,650
And so the risk assessment is going to be sort of colored by that.

409
00:42:51,100 --> 00:43:02,879
And so. That's a little bit of attention, right, where we're trying to bring to bear the scientific process for a lot of different stakeholders,

410
00:43:02,880 --> 00:43:06,720
for different reasons that want like a concrete answer.

411
00:43:10,080 --> 00:43:15,690
This is a I don't know if you've ever looked at this book doubt as our doubt is their product by David Michaels.

412
00:43:15,690 --> 00:43:18,749
I believe he was forgetting his exact position.

413
00:43:18,750 --> 00:43:22,560
I believe he was in the Clinton administration. Uh, I'd have to double check.

414
00:43:24,240 --> 00:43:30,420
But yeah, he wrote this book about like environmental toxicology policy, like occupational toxicology policy.

415
00:43:30,420 --> 00:43:39,299
And he pulls out this quote from the R.J. Reynolds Tobacco Company executive Daughters are products product,

416
00:43:39,300 --> 00:43:44,370
since it is the best means of competing with the body of fat that exists in the mind of the general public.

417
00:43:45,120 --> 00:43:55,019
So the idea here being that if you can, you know, put some doubt in terms of this scientific process, it really can help certain stakeholders,

418
00:43:55,020 --> 00:44:03,900
like a tobacco company, like a doubt, any degree of doubt about whether cigarets have bad health effects redounds to their benefit.

419
00:44:04,180 --> 00:44:11,950
Right. So some more talk about searching for certainty.

420
00:44:12,700 --> 00:44:16,810
Senator Edmund Muskie has called for one armed scientist who did not respond.

421
00:44:16,840 --> 00:44:20,710
On the one hand, the evidence is so. But on the other hand, etc.

422
00:44:20,920 --> 00:44:27,909
When asked about the health effects of pollutants. So again, you get this like tension of a scientist.

423
00:44:27,910 --> 00:44:33,670
We don't like to give. We give like we talk like problem. But we're 99% confident that this is the case.

424
00:44:34,060 --> 00:44:42,730
We're 99% confident the gravity is real and go on, you know, so it becomes you know, it becomes difficult in this realm.

425
00:44:44,230 --> 00:44:46,870
So any thoughts, questions, comments.

426
00:44:49,260 --> 00:44:55,470
I mean, I guess it's kind of, I guess as a precautionary principle and like, I think I lean more environmentalist way more like the.

427
00:44:55,740 --> 00:44:59,340
I think most chemicals probably until proven innocent or guilty.

428
00:44:59,670 --> 00:45:05,430
And I guess as a toxicologist, what you've seen, I guess, where do you think you stay on line,

429
00:45:05,430 --> 00:45:07,680
I guess, or I mean, obviously that's like a big question.

430
00:45:07,740 --> 00:45:14,760
But how much how much credit the are just how much leeway do you give as opposed to when you're in a regulatory context?

431
00:45:15,120 --> 00:45:23,670
Yeah, you know, I think I sort of lean I think precautionary principle is great.

432
00:45:24,090 --> 00:45:28,050
I do acknowledge that there are certain things that may be a drawback in some ways.

433
00:45:28,080 --> 00:45:31,739
Um. You know, whether those be economic or what have you.

434
00:45:31,740 --> 00:45:36,240
And to some degree, this they can get this can get into questions outside the realm of science.

435
00:45:36,250 --> 00:45:39,540
You talk about policy and things like that or.

436
00:45:39,630 --> 00:45:42,030
Yeah, so but you know,

437
00:45:42,030 --> 00:45:46,770
I think there's been some interesting stuff coming out that I've seen that I would like to follow up on when I have the time about,

438
00:45:46,980 --> 00:45:58,590
I think in particular for P files about a decision process of basically doing a cost benefit analysis for a particular chemical.

439
00:45:58,620 --> 00:46:02,880
Like what do we use this chemical for? How much of it do we make?

440
00:46:03,360 --> 00:46:07,800
How does what is the cost associated with that versus the benefit derived from it?

441
00:46:08,130 --> 00:46:14,760
I always go back to Sally's said worked a lot with phthalates and they're used in hospital settings,

442
00:46:14,760 --> 00:46:23,520
you know, in kidney dialysis, etc. for their packaging of sterile instruments, countless things.

443
00:46:24,600 --> 00:46:31,050
Um, it's hard for me, even to someone who knows all the toxic stuff associated with the Sally to say ban all use of phthalates.

444
00:46:31,350 --> 00:46:34,440
Right. I don't think I could say that. So.

445
00:46:35,790 --> 00:46:43,590
But is there a smarter way we can use them? You know, that's kind of my thought process as a toxicologist.

446
00:46:44,940 --> 00:46:49,979
I sort of lean towards trying to fully understand and improve our methods of

447
00:46:49,980 --> 00:46:54,750
fully understanding what the hazards posed by a chemical are attributable.

448
00:46:54,870 --> 00:47:01,319
Again, I can't give you the like yes or no. It's, you know, like I said, but that's kind of the way I think about it.

449
00:47:01,320 --> 00:47:07,320
But, you know, I'm certainly would be really interested to hear what what you all think.

450
00:47:07,320 --> 00:47:12,390
And, you know, what my colleagues think because it's it's something that I've been involved with for a long time and.

451
00:47:13,020 --> 00:47:25,679
Yeah. Any other thoughts if I'm not exactly on that, but just in thinking about this doubt, I think there's a lot of parallels like climate change.

452
00:47:25,680 --> 00:47:36,749
And there's another book called Merchants of Doubt that I think like integrates like talks about tobacco and like things like DDT and climate change

453
00:47:36,750 --> 00:47:48,690
and just how like the sort of purposeful campaign of various industries to sow doubt and how much impact that has on public perceptions and policies.

454
00:47:49,350 --> 00:47:57,330
Yeah, if I'm not mistaken, I think that there's like some pretty direct, like lifting of the playbook from the tobacco.

455
00:47:57,450 --> 00:48:03,030
Yeah. Case into the climate change. The same lobbyists and a lot of the same people.

456
00:48:03,120 --> 00:48:06,900
Yeah. Might be like the exact same people in some cases.

457
00:48:06,910 --> 00:48:12,149
So yeah, that's a great point. The climate change issue, I've seen a lot of stuff.

458
00:48:12,150 --> 00:48:15,320
Yeah, but yeah. And you know, I think like there's,

459
00:48:15,330 --> 00:48:20,700
I think this is where like social sciences can play an important role of like you have a chemical, this is what we know about.

460
00:48:20,700 --> 00:48:26,580
It's toxicity, but this is what it can, you know, these are the benefits and acknowledging those.

461
00:48:27,030 --> 00:48:31,710
And what's the proper given those two columns, if you will, like what's the proper way?

462
00:48:32,490 --> 00:48:37,320
Are there limitations in terms of like how much we can manufacture per year type of deal?

463
00:48:38,190 --> 00:48:42,120
That would be really something that I would favor. Uh, yeah.

464
00:48:42,120 --> 00:48:49,820
It's very interesting. But yeah, I could talk about this for, for quite a while so.

465
00:48:51,350 --> 00:48:57,320
Yeah. Oh yeah. Yeah. They on the, the issue of like banning a whole class of like phthalates or so,

466
00:48:57,340 --> 00:49:04,100
a lot of policies have started moving to the like using language of like unavoidable uses or things like that,

467
00:49:04,210 --> 00:49:07,400
like medical uses or things or we don't have something else to replace it,

468
00:49:07,640 --> 00:49:12,570
like banning the other uses where it's unnecessary, like people said, like paint them burning those things.

469
00:49:12,570 --> 00:49:17,389
That doesn't need to be like preserving. For at least for now, until we have some.

470
00:49:17,390 --> 00:49:25,670
And I think it then becomes like a great chemistry issue, like how do we create something less toxic that serves the same purpose incentivizing that?

471
00:49:25,670 --> 00:49:30,049
I don't know how to do that, but yeah, yeah. I mean that's that's why yeah.

472
00:49:30,050 --> 00:49:34,490
I guess to go on that now what do you think about like regrettable substitution where like,

473
00:49:34,520 --> 00:49:44,059
I mean w w doe and like especially when you start looking at there's thousands of people's types of chemicals and we might know one's bad for you,

474
00:49:44,060 --> 00:49:48,230
but we can kind of, I guess, assume based on the chemical structure, what other ones, I guess,

475
00:49:48,290 --> 00:49:53,660
I guess how much uncertainty do you think that really poses or how much can that's called?

476
00:49:53,690 --> 00:49:57,290
Just notice from the understanding of chemistry how similar things are, I guess.

477
00:49:57,870 --> 00:50:03,499
Yeah. No, I mean, I think I think that's right. I mean, there is this like, you know,

478
00:50:03,500 --> 00:50:08,350
you'll see just changing a couple atoms or something and then a chemical like

479
00:50:08,360 --> 00:50:12,590
reintroduced as a new a new chemical that just hasn't been thoroughly characterized,

480
00:50:12,740 --> 00:50:19,790
you know, and it's it's a real problem. I yeah. I don't know that I have a real quick comment on that, but it just, it is what it is.

481
00:50:20,270 --> 00:50:25,520
Um, it's, I mean, I hate to, I hate to put it in this way, but it certainly ensures that, like,

482
00:50:25,550 --> 00:50:30,320
I'll have plenty of research to do as a toxicologist, but I don't know if it's the best for public health.

483
00:50:30,950 --> 00:50:35,530
Um, yeah. This is a really great comments. Um, yeah, I think, you know,

484
00:50:35,540 --> 00:50:40,459
I just go back to like this idea of like the production side of like how much of certain things

485
00:50:40,460 --> 00:50:44,450
do we need to make and for what purpose is maybe a good lens to look at things through?

486
00:50:47,280 --> 00:50:52,290
Okay. And this is actually a good kind of segway into this.

487
00:50:52,980 --> 00:51:03,240
This shows you how, in a sense, like the march of science and like as we find out more about a chemical kind of reevaluating our guidelines.

488
00:51:03,870 --> 00:51:11,460
So this is the guideline level for release, a piece of poly for poly flow, a poly fluorinated chemical into the environment.

489
00:51:12,000 --> 00:51:17,340
We started out with like, I believe West Virginia had a DuPont factory that was like producing a lot of this P force.

490
00:51:17,760 --> 00:51:23,400
And so they were like jumping on, looking at this early of like how much of this stuff can be released into the environment.

491
00:51:23,910 --> 00:51:28,710
And then you see like individual states started like coming in, EPA started weighing in,

492
00:51:29,760 --> 00:51:35,550
and they were each doing like their own sort of risk assessment or like evaluation.

493
00:51:35,730 --> 00:51:40,270
And this guideline level was like dropping over the last 20 years.

494
00:51:40,290 --> 00:51:47,580
You can kind of see this trend here. EPA came out, New Jersey did like a study, you know, did an assessment and lowered their level.

495
00:51:48,000 --> 00:51:55,680
And you can kind of just like walk through as you march through time here and then in New Jersey again, like reevaluated and lowered it again.

496
00:51:55,680 --> 00:51:59,820
I think it's even lower than what you see on this now in terms of EPA.

497
00:52:00,590 --> 00:52:03,720
Uh, so this to me is actually.

498
00:52:05,200 --> 00:52:16,089
Maybe a strangely. Energizing graph to look at in a way because it shows like the application of like

499
00:52:16,090 --> 00:52:21,850
this of like what we know about a chemical and we know like what we know about.

500
00:52:21,850 --> 00:52:31,090
It's a risk we can kind of like see the science helping drive what the guidelines are in terms of what we what we set for release of this chemical.

501
00:52:31,390 --> 00:52:32,670
So you can see a change there.

502
00:52:32,680 --> 00:52:38,379
There's there's a lot of areas where, you know, things could kind of seem insurmountable and things can't change when you're,

503
00:52:38,380 --> 00:52:40,660
you know, when you're following this type of stuff.

504
00:52:40,660 --> 00:52:45,819
But in this case, you are seeing like the type of work that we do in like this school of public health,

505
00:52:45,820 --> 00:52:54,130
in this Department of Environmental Health being applied here and like causing regulators to like reassess this chemical.

506
00:52:54,730 --> 00:53:02,080
So kind of a cool graph and this is like a pretty cool if you want to learn more about the whole process of how chemicals are evaluated,

507
00:53:02,410 --> 00:53:08,350
both like nationally and like state by state. Uh, this is like a good one you could click on to read.

508
00:53:10,630 --> 00:53:16,810
Okay. So this is just showing all the different ways that people can be exposed.

509
00:53:17,320 --> 00:53:22,510
It's from your book and, you know, different ways chemicals can move through the environment.

510
00:53:22,920 --> 00:53:28,570
We need to spend too much time there. So risk communication, right.

511
00:53:29,860 --> 00:53:33,780
So what's you know, like let's say you do know what the risk of a certain chemical is like.

512
00:53:33,790 --> 00:53:38,050
How do you communicate it to the public and to other stakeholders?

513
00:53:40,390 --> 00:53:44,710
So one thing is to understand we can think about like risk perception.

514
00:53:44,740 --> 00:53:53,020
Different people perceive risk in different ways. One person may think bungee jumping looks like super fun.

515
00:53:53,020 --> 00:54:01,270
Another person might say, No, thank you. It looks too risky. So individuals respond differently to information about risk.

516
00:54:04,090 --> 00:54:11,920
And so we need to understand these responses when we're crafting effective risk communication strategies and evaluating these risk management options.

517
00:54:13,020 --> 00:54:21,120
All the things, all the the social aspects like education and cultural upbringing where one lives.

518
00:54:21,780 --> 00:54:28,770
Age, gender. These could all influence how we perceive risk because we're each individual's words going to perceive different things, different ways.

519
00:54:30,460 --> 00:54:36,610
Even toxicologists from academia, government and industry can all perceive different things.

520
00:54:36,610 --> 00:54:37,749
And you'll definitely see this.

521
00:54:37,750 --> 00:54:45,880
Like if you go to the Society of Toxicology National Meeting, you can see people are getting in discussions and debates sometimes about.

522
00:54:46,270 --> 00:54:50,680
I've had people come up and say, Do you really think this chemical is like that toxic and stuff?

523
00:54:51,040 --> 00:54:58,860
So there's like there's different perspectives in terms of the risk of chemicals even within like the group of experts.

524
00:54:58,870 --> 00:55:07,980
Right. We can think about this in terms of, um, they showed us in your book the dreads dread space.

525
00:55:08,590 --> 00:55:14,160
It's like the different things that sort of drive how we perceive different types of risks.

526
00:55:14,730 --> 00:55:19,380
And so this is laid out on a couple of different axes, one being controllable.

527
00:55:21,330 --> 00:55:30,150
So this is like not very dreadful, not global catastrophic consequences, not fatal low risk to future generations.

528
00:55:30,810 --> 00:55:36,690
Um, but like, yeah, things that one can control and that uncontrollable.

529
00:55:37,520 --> 00:55:42,210
Then you also have observable and not observable and where things are placed on this.

530
00:55:43,870 --> 00:55:49,729
Graph. Sort of. Kind of. You know, are going to depend on.

531
00:55:49,730 --> 00:55:54,350
You know. Different aspects of the risk in question.

532
00:55:55,250 --> 00:56:02,000
So you can kind of find some different fun things on here. Like, for instance, like skateboards, like fairly controllable.

533
00:56:02,570 --> 00:56:09,020
Um, something like DNA technology, kind of more of the uncontrollable side.

534
00:56:09,050 --> 00:56:13,040
You know, people don't feel like necessarily they have maybe a lot of control over.

535
00:56:13,700 --> 00:56:21,139
I don't know whether they're eating GMOs or something. In a lot of cases, uh, or and also kind of not observable, right?

536
00:56:21,140 --> 00:56:25,880
It's just sort of something that happens for most people kind of in the background. You might read a news story about it here or there.

537
00:56:26,660 --> 00:56:33,139
So that could kind of maybe foster like a high degree of dread, you know, or,

538
00:56:33,140 --> 00:56:37,160
you know, people may elevate that as a risk in terms of their perception.

539
00:56:37,370 --> 00:56:47,120
Right. Does this seem pretty intuitive? Like these things might drive like some of our chemicals can be found here as well.

540
00:56:47,570 --> 00:56:54,170
You know, a lot of these are going to be in the dread space of like not controllable, like radioactive waste or pesticides of mercury.

541
00:56:55,070 --> 00:56:59,630
We don't really it's not something you can kind of control on your day to day life, in your day to day life.

542
00:57:00,400 --> 00:57:04,639
And in many cases, I mean, there's certain ways you can mitigate your exposure to that.

543
00:57:04,640 --> 00:57:11,570
But if there's like a mercury spill or mercury in the air, like, there's not much you can do, right?

544
00:57:12,110 --> 00:57:15,560
Versus like you could maybe choose to get on a trampoline or not.

545
00:57:17,760 --> 00:57:23,969
So yeah. So in toxicology, you know, a lot of the stuff that we're looking at is for most people would be considered

546
00:57:23,970 --> 00:57:28,440
like an uncontrollable and oftentimes like not observable type of risk.

547
00:57:29,070 --> 00:57:36,210
You know, we're not really observing air pollutants and, you know, directly, we're just exposed to that.

548
00:57:36,780 --> 00:57:44,250
So that makes sense. This is kind of an interesting quote.

549
00:57:44,260 --> 00:57:49,870
If someone had evaluated the risk of fire right after it was invented, they may well have decided to eat their food raw.

550
00:57:51,120 --> 00:57:54,990
I don't know what. What do you think about that quote? Any thoughts?

551
00:58:00,850 --> 00:58:06,040
I'll take a stab. I mean, I think a lot of just I don't know where she is, but a lot of economists,

552
00:58:06,040 --> 00:58:11,979
I think sometimes I think overplay risks when it comes to looking at economic impacts.

553
00:58:11,980 --> 00:58:17,800
And I think that kind of I don't fully agree with the state, but it is kind of interesting.

554
00:58:18,220 --> 00:58:25,570
Yeah, it's an interesting thought. It's kind of an interesting example because, you know, like fire is really dangerous.

555
00:58:25,580 --> 00:58:30,890
So, you know, I mean, assessing the risk of fire seems like something you would want to do.

556
00:58:32,000 --> 00:58:40,170
And the way it's phrased, care almost seems like. Oh, well, you know, we should have, I think, the risk of fire, but in any event, it does.

557
00:58:40,170 --> 00:58:45,150
You know, it's again, it gets to this idea of like cost benefit, risk benefit type of analysis.

558
00:58:47,470 --> 00:58:56,210
And these are. Different types of risks ranked by a risk analyst.

559
00:58:56,270 --> 00:58:58,909
So I believe, like when they're saying risk analyst here,

560
00:58:58,910 --> 00:59:06,130
we're talking about someone that like maybe works for like an insurance company and it's like really well versed in the data of what causes like fate,

561
00:59:06,290 --> 00:59:09,409
fatalities, injuries, like in most people.

562
00:59:09,410 --> 00:59:14,930
Right. And so they're thinking, you know, you can kind of go through motor vehicles, you know,

563
00:59:14,930 --> 00:59:23,210
cause a lot of deaths every day, smoking, alcohol and guns on down to like nuclear power rate really low.

564
00:59:24,350 --> 00:59:29,600
This one started like back in the late seventies, so that might be something to take into account,

565
00:59:30,590 --> 00:59:38,110
but that if you take like a non risk analyst who is maybe not as well versed in the data, it's going to be kind of a scrambled compared to this.

566
00:59:38,120 --> 00:59:43,909
Right. So they're ranking this, but there are some things that I think like people sort of intuitively understand,

567
00:59:43,910 --> 00:59:46,430
even if they're not an expert in that field like motor vehicles,

568
00:59:46,430 --> 00:59:52,220
I think most people kind of do understand there's an intuitive danger to riding in a motor vehicle.

569
00:59:53,780 --> 01:00:00,050
And then nuclear power though they're ranking first versus the risk analysts break that last

570
01:00:00,740 --> 01:00:05,680
um because of you know if you're really looking at the numbers not that many people like

571
01:00:06,410 --> 01:00:12,530
have died from nuclear power accidents compared to like every year it's tens of thousands

572
01:00:12,530 --> 01:00:16,520
dying in motor vehicles and then you can kind of see how stuff kind of shakes out here.

573
01:00:17,330 --> 01:00:23,989
But the idea here is, again, this risk perception idea and this is the stuff you have to kind of take into account when you're someone who's,

574
01:00:23,990 --> 01:00:32,750
like involved with one of these public campaigns to like warn people about, say, contaminated fish in a particular state or city.

575
01:00:35,130 --> 01:00:44,860
Yeah. And then, yeah, this is just showing like risk communication is going to like, you know,

576
01:00:44,870 --> 01:00:52,639
be part of like the risk management sort of process and feed and feed into the risk manage the risk part of the risk management.

577
01:00:52,640 --> 01:00:55,900
It's going to involve risk mitigation. Here's an example.

578
01:00:55,900 --> 01:00:58,959
I believe this is from locally.

579
01:00:58,960 --> 01:01:03,970
I'm not sure exactly which body of water but o of Detroit River Fish advisory.

580
01:01:03,980 --> 01:01:09,310
Sorry. I was just looking at the pretty colors on the slide of.

581
01:01:10,700 --> 01:01:22,459
So this is an example of how MDH, as you know, is communicating to people the degree of contamination in different types of fish.

582
01:01:22,460 --> 01:01:26,450
One might, if they're going fishing, pull out of the Detroit River.

583
01:01:26,960 --> 01:01:30,650
And so it's a fairly simple gradient here.

584
01:01:30,710 --> 01:01:37,700
These fish have more chemicals. These fish have less chemicals. And you have examples along that gradient.

585
01:01:39,530 --> 01:01:43,729
How do you. Any comments? Any thoughts on this? Risk communication.

586
01:01:43,730 --> 01:01:49,490
We've talked we've gone over a couple of different risk communication scenarios.

587
01:01:49,490 --> 01:02:00,720
What do you think of this? Example. Anything you'd add, anything you'd take away.

588
01:02:02,440 --> 01:02:12,580
I think. Yeah, I think it like gets at the basis, but maybe not as indepth as it could be from my standpoint.

589
01:02:13,310 --> 01:02:18,520
You know, I did my undergrad in chemistry. I'm going to look at this and say, yeah, everything has chemicals.

590
01:02:18,520 --> 01:02:23,540
Like what chemicals are you talking about? Like, what do these chemicals do?

591
01:02:23,560 --> 01:02:28,360
Like, I think it just needs to be more specific. Maybe if it's talking about PE forcer.

592
01:02:28,480 --> 01:02:34,030
I grew up in North Carolina, so Gen-X was like the big thing that I was looking out for.

593
01:02:34,240 --> 01:02:39,879
If there was any indication of like the types of chemicals that they're talking about,

594
01:02:39,880 --> 01:02:48,040
more specifically, I think people would be more in tune with what they're trying to say.

595
01:02:48,310 --> 01:02:51,490
But like, for me, this one wouldn't really scare me.

596
01:02:51,730 --> 01:02:55,930
Like, okay, yeah. Chemical is everything is a chemical liquid. What do you mean by that?

597
01:02:56,530 --> 01:03:01,900
That's a good point, too. And it would have to have a lot of chemicals for me to not want to eat some of those guys.

598
01:03:01,930 --> 01:03:04,950
Yeah, like catfish, like. All right.

599
01:03:05,170 --> 01:03:10,750
Yeah. Yeah. No, I think that's a really good point.

600
01:03:11,050 --> 01:03:15,490
I believe this is just, like, put up, like, outside, like, by where? In areas where people would be fishing.

601
01:03:15,880 --> 01:03:21,260
So you could maybe add like something on the side that says examples of chemicals we're talking about include X, Y and Z.

602
01:03:21,650 --> 01:03:27,100
See, they're not talking about the nice omega fatty acids that are in the fish that we want.

603
01:03:27,580 --> 01:03:36,880
Right. In a lot of cases because they'd be good for us. So I think, yeah, that might be helpful, but it's like it's fairly intuitive, right?

604
01:03:36,910 --> 01:03:41,190
Like you can kind of see what they mean and it has pictures of the fish, which I like, you know,

605
01:03:41,190 --> 01:03:46,599
in case someone is like they catch something and they're like, I don't know, they could like check against this.

606
01:03:46,600 --> 01:03:50,770
The color coding, it's kind of nice, kind of catches your eye.

607
01:03:53,680 --> 01:03:59,830
One thing I kind of like about it is the part of choose wisely, eat safely.

608
01:04:00,340 --> 01:04:12,999
I think a lot of people, including myself, this idea of choose gives you a sense of empowerment where, you know, it's not just, like, dangerous.

609
01:04:13,000 --> 01:04:20,590
Don't fish here or whatever it. I mean, unfortunately, we're just living with this, you know, environmental pollution and it's just a fact of life.

610
01:04:20,600 --> 01:04:28,389
So within that context, it's nice to be able to have some kind of agency in terms of like, okay,

611
01:04:28,390 --> 01:04:32,920
well, I'm still going to fish, but I'm going to do it in a way that is healthiest for me.

612
01:04:33,820 --> 01:04:39,410
So I like that component of it. Anything else?

613
01:04:40,370 --> 01:04:43,570
I almost want it to be more actionable. Okay. Like.

614
01:04:43,580 --> 01:04:50,660
Like in me. Like a yeah. Choose wisely. You can do whatever you want. You know, if you want to eat a high chemical fish, go for it.

615
01:04:51,050 --> 01:04:56,570
But like, if you're going to have that, shoppers want, like a statement, like, maybe put this one back in the water.

616
01:04:56,680 --> 01:05:00,620
Yeah. Right. You don't eat this that often? Yeah.

617
01:05:00,770 --> 01:05:08,050
Yeah. I've seen maybe I've seen it too, like a like some of the Fisher brothers.

618
01:05:08,060 --> 01:05:12,980
I think you could see them, like, on line. I'm not sure if they have these, like, posted anywhere, but, like, you could see, like,

619
01:05:12,980 --> 01:05:21,860
how much of a particular type of fish you can eat or like how much is suggested to eat to like keep below a certain level of like mercury,

620
01:05:22,190 --> 01:05:26,599
for example. So like you can eat like I think it's like, I don't know who put it out.

621
01:05:26,600 --> 01:05:29,150
I just remember at some point it's like a state,

622
01:05:29,160 --> 01:05:37,250
maybe an EPA thing where it would tell you like you can eat x grams of catfish or this many filets of whatever,

623
01:05:38,060 --> 01:05:43,940
you know, that's what's considered safe. And then you'd see like some types of species of fish and it'd be like none at all.

624
01:05:44,150 --> 01:05:48,139
Or like you can eat like a gram or something.

625
01:05:48,140 --> 01:05:52,820
And so I think that kind of helps to, I think that's like sort of empowering.

626
01:05:52,820 --> 01:05:57,910
It allows you to make your own decisions. Yeah.

627
01:05:59,510 --> 01:06:03,100
Any other thoughts? Yeah. I was going to say context is everything.

628
01:06:03,100 --> 01:06:08,259
And obviously, like if this was a part of a campaign like this is what's at the river like pretty good,

629
01:06:08,260 --> 01:06:12,860
obviously supplementary information or like a QR code that sends you to the web site or something.

630
01:06:12,860 --> 01:06:21,190
That's always good. So that's a really good idea. I think the QR code for sure as that stuff becomes more commonplace.

631
01:06:23,900 --> 01:06:27,350
Back when I was in undergrad, the I wouldn't know what a QR code was.

632
01:06:27,680 --> 01:06:31,370
I'd have to go back to the computer lab and write down the fish.

633
01:06:31,880 --> 01:06:39,980
So its technology improves. We're more. More able to empower people in some cases, or at least inform people.

634
01:06:39,980 --> 01:06:43,090
So that's good. Okay.

635
01:06:44,410 --> 01:06:46,090
So the idea here was a miscommunication.

636
01:06:46,100 --> 01:06:55,180
We're trying to convey information about levels of health risk or like environmental risk, maybe ecological risk in some cases.

637
01:06:56,440 --> 01:06:59,889
You know, what does that mean? You know, what does it mean?

638
01:06:59,890 --> 01:07:06,820
You know, it's not enough to just give someone the numbers and say your levels of mercury are this or your levels of faster this.

639
01:07:07,210 --> 01:07:12,430
You're going to want to tell people what that means in terms of like their health risk.

640
01:07:12,610 --> 01:07:19,360
Right. It's just context. Free numbers don't really do a ton for most people that are already in that field.

641
01:07:20,890 --> 01:07:29,320
And then the decisions, actions and policies will be aimed at managing or controlling the risks based on this information.

642
01:07:30,160 --> 01:07:38,590
And then, you know, we have all these different parties that may be involved in this governments, community groups, media, etc.

643
01:07:43,640 --> 01:07:46,680
So yeah, I think this is this is a good point.

644
01:07:46,700 --> 01:07:50,390
Like, we did this like bi directional approach.

645
01:07:50,450 --> 01:07:53,839
Like we need to be listening to people and also informing people.

646
01:07:53,840 --> 01:07:59,210
It kind of can't be just all one way, just like talking at people, which I think, you know.

647
01:08:00,500 --> 01:08:08,860
Fairly straightforward there. Has anyone done any work,

648
01:08:08,860 --> 01:08:13,210
any like human subjects work or like community work where you had to sort of

649
01:08:13,240 --> 01:08:18,550
be communicating risks or kind of environmental information to a group of.

650
01:08:21,000 --> 01:08:23,650
Local people? Yeah, kind of.

651
01:08:23,730 --> 01:08:37,980
I like did work at the NHS on community health outcomes, like around COVID and preventing like compounding impacts in the community.

652
01:08:38,610 --> 01:08:45,640
So it was a little weird. We did it more like a broad sort of seminar where we were like giving information.

653
01:08:45,660 --> 01:08:52,560
It wasn't exactly one on one, but it was still like a very interesting experience.

654
01:08:52,830 --> 01:08:57,110
Yeah, yeah. That's I mean, that's I mean, COVID especially.

655
01:08:57,120 --> 01:09:00,479
I mean, jeeze, I can't believe I've even gone this far without talking about COVID.

656
01:09:00,480 --> 01:09:04,709
It really kind of like highlighted this risk communication.

657
01:09:04,710 --> 01:09:11,260
I mean, you remember the early days of COVID with all the different guidances that were coming out and we still we're still kind of living in it,

658
01:09:11,280 --> 01:09:14,730
right? So you could see how sometimes the stuff is like.

659
01:09:16,340 --> 01:09:24,580
It has to take place over like in a very rapid way, like communicating about these risks if there's a disaster, pandemic, what have you.

660
01:09:24,910 --> 01:09:30,520
So it's not trivial, you know, to think about how to communicate this stuff.

661
01:09:30,520 --> 01:09:37,900
I think I think a lot of scientists kind of take it for granted that like, oh, if I just tell people what my results are, people respond to it.

662
01:09:37,910 --> 01:09:45,100
It's like now there's the whole sort of social fabric there that you have to kind of engage with and understand to communicate effectively.

663
01:09:50,020 --> 01:09:56,020
Right. So successful engagement for communication involves, you know, providing the information and,

664
01:09:56,020 --> 01:10:01,809
you know, saying when the science might be uncertain explanation of the risk assessment process.

665
01:10:01,810 --> 01:10:06,850
People, I think in general like to look under the hood and see like, how did you arrive at this conclusion?

666
01:10:07,270 --> 01:10:11,230
What was what went into this decision or what went into your assessment of this risk?

667
01:10:12,400 --> 01:10:15,160
You'll see a lot of stuff, like I said, because like publicly access,

668
01:10:15,160 --> 01:10:19,629
like certain EPA documents that talk about the risk assessment of trichloroethylene,

669
01:10:19,630 --> 01:10:27,430
I mean, they're long to to read and they, you know, they require a certain level of technical knowledge to understand in a lot of places.

670
01:10:27,430 --> 01:10:30,520
But they are out there and accessible, which I think is a good thing.

671
01:10:32,970 --> 01:10:38,100
Accounting for differences in acceptable levels of risk. So remember, different people might have different risk perceptions.

672
01:10:38,100 --> 01:10:44,730
So if I tell one person the levels of PCBs in the river is X and I tell another,

673
01:10:44,940 --> 01:10:49,770
they might be fine with it, but I tell another person they might, you know, be enraged.

674
01:10:50,810 --> 01:10:54,160
Um. Let's see. Yeah.

675
01:10:54,170 --> 01:10:58,570
And again, providing information that can assist in personal decision making, informing policy.

676
01:10:58,840 --> 01:11:02,860
Right. So we talked about like individual choices and like helping to like hopefully

677
01:11:02,860 --> 01:11:08,110
empower people both on the policy side and in the personal decision side of things.

678
01:11:11,610 --> 01:11:18,750
So risk communication program again, you know, improve risk understanding among certain target groups.

679
01:11:18,990 --> 01:11:24,209
There might be certain groups of people that are in a different situation, like certain tribal groups,

680
01:11:24,210 --> 01:11:30,060
maybe eating on average, like much higher levels of certain fish in some cases,

681
01:11:30,060 --> 01:11:32,370
like when I was in Seattle in the Northwest,

682
01:11:32,370 --> 01:11:39,690
there was like a little bit more of a targeted risk assessment and risk communication going on for some of those populations compared to,

683
01:11:40,110 --> 01:11:41,490
you know, the broader population.

684
01:11:44,800 --> 01:11:49,440
You know, disclosing, you know, we want to tell people that are exposed to chemicals what hazards might be associated.

685
01:11:49,450 --> 01:11:54,520
You know, if you're living in an area that's contaminated, it's important to get that information out.

686
01:11:55,510 --> 01:12:01,239
And then the idea would be to help with risk communication, to explain and justify the risk management strategies.

687
01:12:01,240 --> 01:12:07,780
Why are we doing this? Why are we putting this like. You know, new containment over this like landfill.

688
01:12:11,470 --> 01:12:19,450
Let's see. Again, providing information about individual risk reduction measures, as we talked about, encouraging behavior.

689
01:12:19,450 --> 01:12:23,320
You know, we would like to encourage behavior that would improve public health.

690
01:12:24,820 --> 01:12:30,430
And then educating, you know, decision makers, politicians, etc., for, you know,

691
01:12:30,610 --> 01:12:34,330
hopefully policies that will improve public health and environmental health.

692
01:12:35,370 --> 01:12:39,870
All makes sense, right? Sounds pretty easy, right? Yeah, I just.

693
01:12:39,930 --> 01:12:48,300
I think I'm the behavior change. And it's important to acknowledge, like, who can make changes to their behavior in certain ways, like,

694
01:12:48,600 --> 01:12:55,520
who can afford to buy more expensive products that don't have built in parabens or eat foods that have less pesticides or buy organic or something?

695
01:12:55,530 --> 01:12:58,170
Like I think that's a very important component of the behavior change,

696
01:12:58,620 --> 01:13:03,270
but like it shouldn't just be on individuals and like, we live in a society where everybody can.

697
01:13:04,550 --> 01:13:07,910
Is not able to make the same changes. That's an excellent, excellent point.

698
01:13:07,910 --> 01:13:11,540
And this sort of ties into the environmental justice issues.

699
01:13:12,460 --> 01:13:14,240
You know, some people live in food deserts.

700
01:13:15,020 --> 01:13:24,650
Some people live in areas where they cannot choose to just, you know, live somewhere where there's really clean air or what have you.

701
01:13:25,070 --> 01:13:31,580
So understanding how people are constrained, people are very constrained within their environments.

702
01:13:31,670 --> 01:13:40,280
When you have a social and in the social and other sort of environments around them.

703
01:13:40,670 --> 01:13:44,120
So yeah, I think that's a really great point that, you know,

704
01:13:44,780 --> 01:13:56,359
to understand the differences in the degree to which someone can change a certain aspect of their behavior and one person who may seem intuitive like,

705
01:13:56,360 --> 01:14:01,640
Oh, just don't eat that type of fish or whatever. That might not work for someone else.

706
01:14:01,960 --> 01:14:05,660
They might, for whatever reason, that might be a staple part of their diet.

707
01:14:07,340 --> 01:14:11,360
There's all types of lived experience, and so for sure, it's a great point.

708
01:14:12,050 --> 01:14:18,980
Any other thoughts on that? I just wanted to plug H.B. six six to it's with Professor Zygmunt Fischer.

709
01:14:19,100 --> 01:14:23,120
It's Ross communication. I think it's way more complex to get into it.

710
01:14:23,330 --> 01:14:32,000
Yeah. Yeah. As with everything, a lot of this stuff, I could only touch on it for, like, one semester or one lecture, so.

711
01:14:34,950 --> 01:14:38,920
Okay. Cool. Let's see.

712
01:14:39,730 --> 01:14:47,410
So in terms of like what triggers like a big a lot of attention on some risk, you know, environmental health issue.

713
01:14:48,220 --> 01:14:52,000
There's a couple of sort of characteristics we could think about.

714
01:14:52,750 --> 01:14:59,140
So a possible risk to public health is more likely to become a major story if the following are prominent.

715
01:14:59,680 --> 01:15:06,339
So questions of blame if there's like one company to blame for like a chemical

716
01:15:06,340 --> 01:15:12,130
spill that will kind of engender a lot of like media attention in a lot of cases.

717
01:15:12,580 --> 01:15:18,490
We saw this with, like, the tobacco companies. And then let's see.

718
01:15:19,040 --> 01:15:23,740
Alleged secrets and attempted cover ups, you know, always makes for really interesting.

719
01:15:24,310 --> 01:15:27,880
Oh, shoot. We're okay. Yeah, we'll pick this up later.

720
01:15:27,910 --> 01:15:31,740
Sorry about that. Yeah, really great discussion.

721
01:15:31,750 --> 01:15:35,049
But yeah, we'll pick this up next time. We're going to be in a whirlwind.

722
01:15:35,050 --> 01:15:35,520
The last.

