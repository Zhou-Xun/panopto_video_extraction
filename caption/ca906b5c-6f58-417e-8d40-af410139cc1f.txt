1
00:00:06,990 --> 00:00:11,730
Okay. Good afternoon, everybody. Okay. Can you guys hear me in the back? I know.

2
00:00:11,790 --> 00:00:17,920
Is it through the P.A. system? All right, how about now?

3
00:00:18,430 --> 00:00:27,999
Okay, um, welcome back to the lecture, and hopefully you guys have had a good Halloween if you are celebrating.

4
00:00:28,000 --> 00:00:34,480
And kudos to everybody who has completed the midterm, and we are still in the process of grading it.

5
00:00:34,960 --> 00:00:38,760
Um. So a few announcements.

6
00:00:39,450 --> 00:00:42,450
The first one is about the Hummer, number three.

7
00:00:42,810 --> 00:00:48,600
So. It is going to be due by November 21st.

8
00:00:48,750 --> 00:00:53,010
So most materials would be based on the margin of models.

9
00:00:53,400 --> 00:01:04,590
So it is primarily focused on hand now owns only ABC and there are two questions, but primarily just one question.

10
00:01:04,890 --> 00:01:09,300
The first one is about the, you know, a data analysis.

11
00:01:09,660 --> 00:01:14,130
Okay? The number two is about project proposal.

12
00:01:14,850 --> 00:01:18,090
So, you know, everybody will get this three points.

13
00:01:18,240 --> 00:01:21,540
So I don't I can not imagine I will take any points out of this.

14
00:01:23,190 --> 00:01:29,730
But essentially it is trying to give you a second chance to organize your proposal stuff.

15
00:01:32,190 --> 00:01:37,890
So for each team, I think that we have a Google sheet where you can link to the.

16
00:01:39,950 --> 00:01:43,070
Team names, dates and the data link.

17
00:01:43,520 --> 00:01:53,590
So. I don't know whether you guys have decided the dates, but please put in the preferred presentation date.

18
00:01:53,680 --> 00:01:57,190
If not, I will just be randomly assigning the dates.

19
00:01:58,210 --> 00:02:01,960
Also the data link. So if you have decided to split, put them there.

20
00:02:03,280 --> 00:02:07,210
But I guess as long as you prepare your proposal document, I think that's okay.

21
00:02:08,320 --> 00:02:13,360
We will be providing feedback to those people or teams who have submitted proposal.

22
00:02:13,780 --> 00:02:21,090
So time for submitted proposal is going to be, you know, there's a soft deadline, which is pretty close.

23
00:02:21,100 --> 00:02:27,980
I know it is November. November 9th, basically in one week's time.

24
00:02:28,220 --> 00:02:33,560
So if you have to started early, I know a few teams have started early, so for free to submit by that time.

25
00:02:33,950 --> 00:02:41,090
But I understand that, you know, you have a lot of things going on not only in this class, we will make this a softer line.

26
00:02:41,750 --> 00:02:50,750
So if you do need more time to put things together, you have until November 21st to send in the proposal.

27
00:02:51,170 --> 00:03:00,050
Every team only needs to send in one proposal so we can designate one leader or one submitter to send in the proposal.

28
00:03:00,920 --> 00:03:05,530
So this deadline is pretty. It is hard. Okay.

29
00:03:05,640 --> 00:03:12,530
November 21st. So what are the things we need to do in the proposal?

30
00:03:13,310 --> 00:03:20,900
First, identify data set. Number two, describe basically the scientific background of the data.

31
00:03:21,440 --> 00:03:24,860
Number three, you can just try to visualize the data.

32
00:03:26,660 --> 00:03:30,020
From using some simple visualization tools.

33
00:03:30,410 --> 00:03:35,350
The goal is not really trying to conduct any analysis, but just trying to show that you can extract a data plot.

34
00:03:35,960 --> 00:03:39,950
And that data seems to be reasonable for answering your questions.

35
00:03:41,970 --> 00:03:49,049
And there are a few more questions about visualize individual level, individual people's trajectory,

36
00:03:49,050 --> 00:03:56,760
and also have you describe some potential models to deal with the data.

37
00:03:58,350 --> 00:04:03,299
And number six, it is just asking you to plan the sections.

38
00:04:03,300 --> 00:04:05,040
A proposal like introduction.

39
00:04:05,430 --> 00:04:15,120
That's a review of model formulation, exploratory data analysis, simulation, if you have any data analysis and discussion.

40
00:04:15,690 --> 00:04:16,979
So as you can see,

41
00:04:16,980 --> 00:04:30,390
this is trying to get you to a stage where you understand how to organically put together all the parts right into a report of a paper, if you will.

42
00:04:33,320 --> 00:04:37,610
Yeah. When you submit, just go to the campus site to make the proposed submission.

43
00:04:38,120 --> 00:04:43,410
So that's it for the homework. Number three one data analysis question based on motion models.

44
00:04:43,430 --> 00:04:50,090
The second one is basically a continuation of what's asked of you to prepare for the proposal.

45
00:04:52,500 --> 00:04:59,510
Yeah, I suppose. Sorry.

46
00:04:59,510 --> 00:05:02,810
I cannot hear you. I'm going. Service.

47
00:05:04,100 --> 00:05:16,240
Ready? Okay. I cannot spin this or say.

48
00:05:19,580 --> 00:05:24,740
Okay. All right. I need to end in there.

49
00:05:26,660 --> 00:05:33,800
Okay. This might be an obvious question, but for the project proposal section, it's okay if you and your teammates have exactly the same.

50
00:05:34,680 --> 00:05:39,420
Answers and text. Correct. So what are the same sections?

51
00:05:39,510 --> 00:05:45,060
The project proposal? Yeah. It's okay if you and your team have exactly the same responses.

52
00:05:45,390 --> 00:05:49,890
Or would you want us to like. Oh, you mean for two people in the same team?

53
00:05:50,550 --> 00:05:56,460
Yes. Yeah, of course. So you only need. So everybody will submit their own question one.

54
00:05:57,000 --> 00:06:05,160
Okay. And if you want, you can say question to please see the submit a proposal under the name of blah blah.

55
00:06:05,700 --> 00:06:14,009
And I will automatically try to link that to you. But you know, just to I think I'll give you everybody everybody three points.

56
00:06:14,010 --> 00:06:18,450
So pretty much our le our not try to take off any points on this proposal.

57
00:06:18,450 --> 00:06:25,080
Clearly, if nobody submitted for that team, then everybody on that team loses three points.

58
00:06:25,590 --> 00:06:31,740
But if anybody submitted, I think I'll give the three points. And I think that you just need to.

59
00:06:32,820 --> 00:06:37,440
Every team needs only to submit one proposal. Okay.

60
00:06:38,040 --> 00:06:50,880
All right. Yes. Okay.

61
00:06:50,890 --> 00:06:54,370
So the question is, do we have a separate submission link?

62
00:06:55,060 --> 00:07:03,340
Yes, there is. So if you go to go to this go this particular tab so you can submit here.

63
00:07:07,860 --> 00:07:12,530
So yeah. So the question is, do you still need to submit as part of homework three?

64
00:07:13,110 --> 00:07:21,250
You don't have to. But again, I think it's probably doable that you specify what is the project's name?

65
00:07:22,300 --> 00:07:27,420
So I know that you're part of the project when they submit a proposal.

66
00:07:27,890 --> 00:07:32,810
Know we can do the work of linking that in. Yes.

67
00:07:36,070 --> 00:07:39,790
Okay. I'm going to throw this to you. Be careful with the bubble tea.

68
00:07:43,090 --> 00:07:48,500
All right. So there's, like, one person in the team.

69
00:07:49,030 --> 00:07:54,460
This person's homework through include the question, the individual question and the proposal questions.

70
00:07:54,880 --> 00:07:58,620
So they're all including this person's homework. Three. Right.

71
00:08:00,100 --> 00:08:05,050
So the question is for each individual, what are the components that you should include?

72
00:08:05,980 --> 00:08:10,510
So my answer to that is you should include the answer to the question, number one.

73
00:08:11,050 --> 00:08:17,810
And you can say I am a team member of project blah, blah, blah.

74
00:08:18,160 --> 00:08:24,220
Okay. And if you have like three or four people in the team, you just have that lead submitter,

75
00:08:24,730 --> 00:08:28,840
send in that proposal and make sure that proposal have the same project title.

76
00:08:29,380 --> 00:08:36,700
And also, I do have these information here. Okay. So if this is accurate, I will be able to.

77
00:08:38,460 --> 00:08:42,330
Make sure that whoever said say, I don't know, I'm just going to.

78
00:08:45,340 --> 00:08:54,260
Say you shot here, submitted a proposal. Okay. And could you guys should list your names under that proposal.

79
00:08:54,280 --> 00:08:59,110
Right. And now when we are grading, we are going to see.

80
00:08:59,130 --> 00:09:03,300
Okay. Do we see a proposal that's linked to this name?

81
00:09:03,510 --> 00:09:08,100
This name, this name? This name. So pretty much that's what I'm going to do.

82
00:09:11,170 --> 00:09:15,910
Okay. So that's three points on homeworks. So that's three points on homework three.

83
00:09:16,480 --> 00:09:20,830
That part. Yeah. Is submit it was homework three.

84
00:09:21,040 --> 00:09:24,979
Right. Okay.

85
00:09:24,980 --> 00:09:28,430
So I need to show you what I should do. Let me explain this.

86
00:09:28,430 --> 00:09:35,210
I no problem confusing you guys. Okay, so when you send me home, let's do a stupid interview, okay?

87
00:09:36,200 --> 00:09:41,480
All right. So when you finish your homework anniversary, okay, you click this.

88
00:09:42,600 --> 00:09:47,720
And when you submit, I believe when you upload this thank you for homework.

89
00:09:48,510 --> 00:09:51,930
Three Question one you just do whatever you need to finish that question.

90
00:09:52,440 --> 00:09:57,020
For the second question, you just need to write one sentence. Me.

91
00:09:58,160 --> 00:10:01,500
Blah blah blah is UN project entitled blah blah blah.

92
00:10:01,940 --> 00:10:06,290
Other team members are blah blah blah. If you have that, that's okay.

93
00:10:06,830 --> 00:10:15,440
Now if you are the submitter for the team, for the proposal, if you are the person, then you have to do an additional step to go to this part.

94
00:10:17,280 --> 00:10:27,600
And then upload that proposal. But if you decided to let another person say, John, to submit a proposal for the team, then you don't have to do this.

95
00:10:28,020 --> 00:10:32,910
But John then has to do this additional step of sending that proposal for the team.

96
00:10:35,140 --> 00:10:38,860
You still look confused, but are you sure you understand it? Okay.

97
00:10:38,870 --> 00:10:50,739
Yeah, yeah. There's someone hard to understand, so. Okay, so, team member, please help me there to tell us.

98
00:10:50,740 --> 00:10:56,020
Exactly. Yeah, please. All right.

99
00:10:58,360 --> 00:11:03,700
Thanks. Yeah, please. Just as an appetizer again, if you need to.

100
00:11:04,960 --> 00:11:08,750
Or please come to me after class. I can explain to in whatever way that you feel.

101
00:11:09,380 --> 00:11:12,490
That's clear. Okay. Any other questions?

102
00:11:16,790 --> 00:11:26,000
All right. So. And by the way, if you guys have not thought about Team Name and Dead series, you guys should put in some team names.

103
00:11:27,860 --> 00:11:31,250
This, too. This is to enhance some team spirit, I guess.

104
00:11:32,030 --> 00:11:35,060
I don't know. What we see is only gets B.S.

105
00:11:38,110 --> 00:11:41,880
I don't know. I don't know what that is. What is it?

106
00:11:42,790 --> 00:11:46,780
Whatever you want it to be. I got it.

107
00:11:47,020 --> 00:11:53,510
Okay. All right, well, you guys can explain when you're presenting, but this is interesting.

108
00:11:53,530 --> 00:11:57,970
Okay. All right. So. All right, let's return to the lectures.

109
00:11:58,330 --> 00:12:01,970
So. Regarding the progress.

110
00:12:02,660 --> 00:12:07,590
We did a mid-term review. On Wednesday last week.

111
00:12:07,830 --> 00:12:17,250
And for this week we are going to finish talking about review of John and then talk about the module module introduction.

112
00:12:17,670 --> 00:12:23,130
So hopefully this can get us started on the module models.

113
00:12:23,820 --> 00:12:29,220
So let's go to the slide handout eight and let's finish that review.

114
00:12:32,900 --> 00:12:39,120
So I'm going to scroll down to. So this particular figure here.

115
00:12:41,700 --> 00:12:45,600
My claim is that if you're going to forget everything about.

116
00:12:47,730 --> 00:12:51,750
Joanne, you should not forget about this and.

117
00:12:54,020 --> 00:13:00,080
So I made this figure because this is exactly how I understood generalize and unit models.

118
00:13:00,440 --> 00:13:06,140
And I think this probably is going to be very helpful for those who are for those people who are visual.

119
00:13:06,440 --> 00:13:11,330
I'm a visual person and like to draw figures. So hopefully this helps.

120
00:13:11,570 --> 00:13:16,610
Let's go through this figure in a generalized, neater model.

121
00:13:16,970 --> 00:13:20,330
We have three components. We have talk about the names.

122
00:13:20,360 --> 00:13:25,640
The first one is called Random Component in the Orange Box.

123
00:13:27,050 --> 00:13:31,430
On the right there, is this a systematic component? It's in the green box.

124
00:13:31,730 --> 00:13:36,380
And there is this a link function that's trying to connect these two components.

125
00:13:37,520 --> 00:13:48,470
And whenever you see something like these yellowish dashed links, these are deterministic relationships.

126
00:13:48,500 --> 00:13:51,260
So there is no stochastic relationships,

127
00:13:51,770 --> 00:14:00,380
which means that if you say XY on the top right and beta I on top a bottom right, you know exactly what it is.

128
00:14:00,530 --> 00:14:04,700
So that is what we meant by deterministic transformations.

129
00:14:05,720 --> 00:14:15,590
When you see these arrows. Now these are stochastic relationships, which means that even if you know Phi and Theta, I hear I explain what they are.

130
00:14:16,190 --> 00:14:19,459
You still do not know the exact value of y, but you do.

131
00:14:19,460 --> 00:14:23,990
You know the distribution of y. So those are arrow solid arrows.

132
00:14:24,060 --> 00:14:31,870
Okay. So let me explain the components.

133
00:14:32,440 --> 00:14:37,900
First one, let's focus on the random component right here.

134
00:14:38,230 --> 00:14:44,560
Why is the response? Remember, we're doing review of generalized union models, so we are dealing with independent observations.

135
00:14:44,860 --> 00:14:54,700
I is the index for independent subjects. Fi here is the what people call over over sorry dispersion parameter.

136
00:15:01,170 --> 00:15:06,870
This person parameter set I here is called canonical parameter.

137
00:15:13,460 --> 00:15:19,700
Okay now because we assume Y is a follow a certain distribution.

138
00:15:20,060 --> 00:15:23,360
Can you guys recall what is the name of that family?

139
00:15:23,810 --> 00:15:30,390
We often assume y is drawn from. Something starting with the.

140
00:15:32,820 --> 00:15:38,030
What if family? So exponential.

141
00:15:38,300 --> 00:15:46,360
Exponential. So exponential family isn't brought family.

142
00:15:46,360 --> 00:15:52,850
It contains many special cases like Gaussian. Bernoulli.

143
00:15:55,710 --> 00:15:59,550
A person. What else?

144
00:16:03,700 --> 00:16:10,190
Binomial. There are many other possibilities.

145
00:16:10,790 --> 00:16:19,430
So in the orange dashed box, essentially it is to say we need to give certain stochastic assumptions upon Y.

146
00:16:19,670 --> 00:16:24,440
And often these assumptions are characterized by the two parameters.

147
00:16:24,740 --> 00:16:36,720
One is PHI, the other state I once we have, once we know the PHI and I value we know what's the distribution y here on the right hand side.

148
00:16:36,740 --> 00:16:43,879
This is what we call systematic component, right? These are the this is the vector of covariates.

149
00:16:43,880 --> 00:16:47,870
So xy equals x12sip.

150
00:16:48,590 --> 00:16:54,050
And I think that you have been looking at this for quite some time since 650.

151
00:16:54,500 --> 00:16:58,400
Essentially, these are p measured features on the subject, right?

152
00:16:58,670 --> 00:17:02,450
They can be intercept, weight, height, what have you.

153
00:17:03,140 --> 00:17:11,900
And here on the bottom, right, you see that this is a Greek letter which is representing the regression coefficients when you are waiting them,

154
00:17:12,140 --> 00:17:19,850
combining them together through a in a product. Basically, you're using beta to weight different features you have collected on this person.

155
00:17:20,210 --> 00:17:24,500
You produce something that's called adeyeye. So for this Adeyeye, we have a name for it.

156
00:17:24,980 --> 00:17:32,550
It is called the meaning or predictor, right? Media predictor.

157
00:17:33,570 --> 00:17:39,840
Okay. So to this point. Now, on the left hand side, you have a distribution for the outcome.

158
00:17:40,870 --> 00:17:47,560
But you are trying to explain how that distribution would change where the features obtain from each of the subject.

159
00:17:48,010 --> 00:17:52,150
On the right hand side, you do have those individual specific vectors of features.

160
00:17:52,510 --> 00:17:57,490
You hope to explain why they're right. So now there needs to be a link to connect the two.

161
00:17:57,970 --> 00:18:01,120
And this link is what we call the link function component.

162
00:18:02,170 --> 00:18:08,140
But what does it mean to link the two? So we need to discuss these two things.

163
00:18:08,680 --> 00:18:15,919
And again, I know these are Greek letters and the reason why I feel so comfortable around these Greek letters is because I'm just older.

164
00:18:15,920 --> 00:18:23,710
I have been looking at this for such a long time, but I think for your learning purpose, I want to emphasize there is a meaning,

165
00:18:24,100 --> 00:18:29,860
but if you can try to remember that they are really designated for these meanings in generalized model framework.

166
00:18:31,250 --> 00:18:39,080
So mu. I hear, as you have learned in our class, this often represents certain average or mean quantity.

167
00:18:39,110 --> 00:18:45,800
So this is the main response. In response for subject I.

168
00:18:49,230 --> 00:18:55,910
Okay. I'm going to pause for a moment. So you may be wondering, hey, Jim, this is on one subject.

169
00:18:55,920 --> 00:19:05,850
What do you mean by have a mean? Well, this is this is can be understood through the perspective that before you collect the data, right.

170
00:19:06,270 --> 00:19:10,620
For one subject, the response may still be stochastic.

171
00:19:10,620 --> 00:19:16,590
So before you collect the data, there is a kind of distribution around this value.

172
00:19:17,280 --> 00:19:21,030
So this movie is trying to say, okay, why ice?

173
00:19:21,930 --> 00:19:30,360
You know, distribution has this particular center view I hear, okay, for another person, this view may be different.

174
00:19:31,290 --> 00:19:34,980
Right. So this is how you would interpret the movie here.

175
00:19:36,730 --> 00:19:42,530
On the other hand, this eight eye is clearly just excite transpose times data.

176
00:19:44,370 --> 00:19:54,300
Now that you can see there is a yellow dashed line connecting the two that is specified through this particular link, we transform the main.

177
00:20:03,730 --> 00:20:12,130
Transform the meme. So for example, if we are talking about my being the saying the meaning.

178
00:20:13,500 --> 00:20:21,010
Of a coin flip outcome. Right.

179
00:20:21,430 --> 00:20:25,630
So one with probability pi zero with probably one minus pi.

180
00:20:26,050 --> 00:20:29,590
So the UI will be what will be pi?

181
00:20:29,620 --> 00:20:33,190
Right. So if you said G equals logit.

182
00:20:34,340 --> 00:20:38,210
Then you basically have transformed the meme.

183
00:20:41,430 --> 00:20:51,749
I'm sorry. Let me just show all this. You have transformed me into a logit of me.

184
00:20:51,750 --> 00:20:57,400
Lie here. And this is what people call log odds, right?

185
00:21:04,460 --> 00:21:08,170
It is long odds. So this is one example of transforming the.

186
00:21:09,140 --> 00:21:13,130
And after the transformation, we equate that to this need and predictor.

187
00:21:13,650 --> 00:21:17,960
Okay. So in the middle, we have bridged.

188
00:21:18,680 --> 00:21:25,520
On one hand, the distributional assumption. On the other hand, the deterministic features you have measured.

189
00:21:25,520 --> 00:21:30,350
And you have a you want to estimate how to weigh these features to predict the mean.

190
00:21:32,680 --> 00:21:39,190
So this these three components in together specifies the generalized nano model.

191
00:21:40,000 --> 00:21:45,850
And there is one additional link that's rather technical, but I will just present it.

192
00:21:45,850 --> 00:21:49,750
And because I think this is just a.

193
00:21:50,700 --> 00:21:57,139
Uh, I believe probably your step theory instructor.

194
00:21:57,140 --> 00:22:06,860
Tavis So as you recall, my eye is the meaning of a distribution, and the Theta II is the canonical parameter of a distribution.

195
00:22:07,370 --> 00:22:12,370
They are not the same thing, but there is a mathematical relationship to connect the two.

196
00:22:12,800 --> 00:22:17,120
So I'm just going to broadly present this as my equals B prime time State II.

197
00:22:17,540 --> 00:22:21,290
So for those who are interested in knowing what this is, I'm going to present this.

198
00:22:21,770 --> 00:22:26,120
But if you don't care, you can just ignore what I'm going to say next. 30 minutes, 30 seconds.

199
00:22:26,570 --> 00:22:31,610
So this is 30 seconds for sure. So so this is a derivative.

200
00:22:34,030 --> 00:22:41,210
Of all of the human and. Human function.

201
00:22:44,510 --> 00:22:47,740
Of. The distribution.

202
00:22:50,890 --> 00:22:59,970
Of what I hear. Okay, now you can refocus. So essentially what I have presented is that we link them.

203
00:23:00,220 --> 00:23:08,050
Me, which is a very interpretable quantity to certain technical parameterization of the distribution.

204
00:23:09,280 --> 00:23:16,690
The reason why I need to talk about this is because you often run into a terminology that's called a canonical link.

205
00:23:17,260 --> 00:23:18,550
Hopefully you have heard about it.

206
00:23:18,970 --> 00:23:28,810
And basically, if I challenge you to say, Hey, why do you use logic transformation when you're doing binary outcome regression?

207
00:23:29,920 --> 00:23:36,380
What's your response? So if you have not thought about that question, then this is the answer.

208
00:23:36,390 --> 00:23:43,140
So technically it is more convenient. So by canonical link and loads it is a critical link.

209
00:23:43,500 --> 00:23:48,569
It means that it just so happens that theta equals eta eight.

210
00:23:48,570 --> 00:24:01,130
I here mathematically oc. And I here, as we know, is linked to the move by this particular functional form.

211
00:24:02,240 --> 00:24:06,290
So then this dictates what is a particular form of G here.

212
00:24:06,530 --> 00:24:16,370
So let me show that. So we want see the AI equals eight AI, right?

213
00:24:16,460 --> 00:24:24,560
But we know it equals GMU ai and we also know that Mila equals B prime, said AI.

214
00:24:25,130 --> 00:24:33,860
So now tell me if I want to ask what's the G here to make this quantity equal to this quantity?

215
00:24:34,700 --> 00:24:37,820
Then G clearly should be the D prime inverse.

216
00:24:38,510 --> 00:24:47,210
So this is the reason why, for example, when you're doing binary outcome regression, you often use the logit.

217
00:24:47,690 --> 00:24:52,880
I will not derive this, but this is the fundamental approach of reasoning.

218
00:24:53,630 --> 00:24:58,460
I just want to pause here for like 30 seconds, just to have you look at it.

219
00:24:59,000 --> 00:25:03,650
And as I say that, I would say if you forget everything in 651,

220
00:25:04,010 --> 00:25:11,420
I think you should at least ask first whether you want to be able to want to be able to recall what's what you've learned in 651,

221
00:25:11,690 --> 00:25:14,750
if you want to recall. I think this figure is the only thing you need to know.

222
00:25:17,750 --> 00:25:23,450
You know, going forward. But clearly, you know, you can use this as a roadmap to learn other aspects of Julian.

223
00:26:04,370 --> 00:26:12,160
Okay. Now let me return to the previous slide, which has a lot more words, but I think it provides a good summary.

224
00:26:13,990 --> 00:26:19,569
First, in generalizing unit models, I'm just going to read them aloud because this is review.

225
00:26:19,570 --> 00:26:24,190
So the distribution response is assumed to belong to a single family of distribution.

226
00:26:24,190 --> 00:26:30,490
No one is exponential family and we have talk about that. That's the random component.

227
00:26:32,040 --> 00:26:41,700
Second, a transform of the main response is linearly related covariance the appropriate link function and number three

228
00:26:42,630 --> 00:26:48,900
regarding the estimation because we often assume particular distribution family like Gaussian a person.

229
00:26:49,120 --> 00:26:56,669
Finally, we can write down the likelihood function and hence perform the maximum like estimation and which

230
00:26:56,670 --> 00:27:02,070
gives you a lot of devices or tools to conduct inference like obtaining the standard errors.

231
00:27:03,630 --> 00:27:07,200
Now I'm going to say that although this is quite flexible,

232
00:27:07,950 --> 00:27:18,060
this is having a very important limitation because all of these formulations were based on the assumption that all the I's are independent.

233
00:27:18,450 --> 00:27:30,060
As you know, in our class, the central focus is how to extend these models to a situation where observations can be dependent to within the person.

234
00:27:32,100 --> 00:27:36,420
And in in the next handout.

235
00:27:36,520 --> 00:27:39,330
And in the old nine handout, handout ten,

236
00:27:39,690 --> 00:27:50,880
we will be providing two extensions of the different flavors so that all the technical tools you learn in Jilin may still be useful.

237
00:27:51,210 --> 00:27:57,600
And I think of, you know, they provide different flavors of extension.

238
00:27:57,930 --> 00:28:08,940
And our goal then will be try to teach you under what circumstances to choose one extension versus another in the following.

239
00:28:09,270 --> 00:28:14,810
I'm just going to provide some very quick review of how generalized meaning

240
00:28:14,820 --> 00:28:20,510
models got used and different outcomes and especially non continuous outcomes.

241
00:28:20,520 --> 00:28:26,280
After all, that's the context in which Jalan was introduced in the first place.

242
00:28:27,690 --> 00:28:38,669
First, as you know, logistic regression, this the outcome often is a binary like successive failure and you collect information about a lot of

243
00:28:38,670 --> 00:28:43,950
covers and you want to conduct a regression analysis of that binary outcome with these covariates.

244
00:28:44,490 --> 00:28:48,270
And the big here will just be the probability of success.

245
00:28:48,600 --> 00:28:55,230
And then you want to use the covers to, to describe the effects upon me.

246
00:28:57,800 --> 00:29:09,080
You know, if we're going to do this, like as we do in our models, we would do this right in bullet one, which is to directly relate men with cougars.

247
00:29:09,710 --> 00:29:19,310
And actually, this is the ongoing debate, especially among economic Christians, whether this is actually worse or better than let's just regression.

248
00:29:21,600 --> 00:29:26,640
Some people say this is better than those just regression. But that's a site that's an aside.

249
00:29:27,750 --> 00:29:32,160
I think today I'm going to say that it indeed has some drawbacks.

250
00:29:32,220 --> 00:29:43,140
Right. If you're going to have a side that's like negative or very large, then there's no guarantee that new AI will be within the range of zero one.

251
00:29:43,260 --> 00:29:49,590
After all, my here is characterizing the probability of success and that number has to be between zero one.

252
00:29:49,860 --> 00:30:03,570
So it is you know, it makes sense to first do a transformation of that zero one value into a number that's that can be within.

253
00:30:04,800 --> 00:30:10,830
The range of real life. So whenever you are constructing the Nina predictor, whatever beta you plug in,

254
00:30:10,830 --> 00:30:17,690
whatever apps you plug in, the resulting logit of is going to be on the real line.

255
00:30:17,700 --> 00:30:22,290
And then after you do the inverse transformation, you can get a view that's between zero one.

256
00:30:22,560 --> 00:30:26,670
So this is one important reason people do this situation.

257
00:30:27,880 --> 00:30:31,120
And number two is this quantity.

258
00:30:31,480 --> 00:30:34,510
Again, it's controversial.

259
00:30:34,510 --> 00:30:38,649
As I said, it's long odds, right? So what do I mean?

260
00:30:38,650 --> 00:30:44,330
If you have a coin that's even right. Half heads, half tail.

261
00:30:44,680 --> 00:30:53,190
So the odds of landing head. Had is like what, over two minus one over two.

262
00:30:53,200 --> 00:30:57,010
Right. So the odds is one this quantity to me.

263
00:30:57,460 --> 00:31:06,820
Well, I don't think it's always natural to everybody unless you play Texas Hold'em or you raise horses like excessively.

264
00:31:07,630 --> 00:31:15,520
But this quantity has been controversial. Many people say, hey, why don't we just do the risk difference?

265
00:31:15,670 --> 00:31:25,180
Why don't we just do that relative risk? But it turns out that there is an important subset of studies like case control studies,

266
00:31:25,480 --> 00:31:30,100
where at log odds ratios can approximate the relative risk pretty well.

267
00:31:30,730 --> 00:31:36,040
I will not talk too much about that. I believe your epidemiology training probably has taught you that.

268
00:31:36,820 --> 00:31:43,600
So I think there is a place for long odds and correspondingly log odds ratio in statistics.

269
00:31:44,170 --> 00:31:47,120
But in this case I want to say is that, you know,

270
00:31:47,200 --> 00:31:56,200
this happens to be the canonical link which produces the log odds interpretation and this is a visualization which I will not spend too much time on.

271
00:31:56,320 --> 00:32:01,180
Basically, as it's changes, the probability of success can go from 0 to 1.

272
00:32:04,090 --> 00:32:09,970
Interpretation. So, for example, let's do this thing here.

273
00:32:10,890 --> 00:32:22,260
So the model supposedly fit the model and the model is like beta one plus beta two times x, right and forever unit change in x.

274
00:32:23,370 --> 00:32:28,230
We can make this contrast right by setting X to one versus zero, we take the difference.

275
00:32:28,620 --> 00:32:31,830
And that's the how do we call this term?

276
00:32:34,860 --> 00:32:42,310
This is the log odds ratio, right? Log odds ratio.

277
00:32:44,930 --> 00:32:51,229
And basically you just calculate what's the log odds on x equals one and what's the log odds under x equals zero?

278
00:32:51,230 --> 00:32:54,640
And that difference is basically beta two.

279
00:32:56,350 --> 00:33:07,810
All right. And clearly, if you can exponential it and then that has the exponential beta two has an interpretation of our of odds ratio.

280
00:33:08,230 --> 00:33:11,060
So for those of you who are wondering why it's called log odds ratio, right.

281
00:33:11,080 --> 00:33:16,780
So essentially you're talking about the log of probability under X equals one.

282
00:33:20,060 --> 00:33:25,100
Right. So this is an odds under x equals one.

283
00:33:25,100 --> 00:33:31,580
Would you agree? And then you're doing this p x equals zero, p x equals zero.

284
00:33:31,760 --> 00:33:35,870
So this is odds of success on the X equals zero, right?

285
00:33:36,770 --> 00:33:41,840
But this thing can be written as something like Odds X equals one.

286
00:33:42,980 --> 00:33:46,250
Not odd odds. Odds. X equals zero.

287
00:33:46,250 --> 00:33:52,580
Right. So these two are same. Right. So when we talk about this, then this is our odds ratio.

288
00:33:55,380 --> 00:34:04,920
Hence my my claim that the saying the circle in red is called long odds ratio for every unit change in the covers.

289
00:34:05,160 --> 00:34:11,940
Everyone with me. Okay.

290
00:34:12,480 --> 00:34:15,780
So that's enough on this interpretation.

291
00:34:16,140 --> 00:34:19,560
Now, let's look at the another aspect, which is the variance.

292
00:34:19,890 --> 00:34:28,590
So this is a really subtle point. I do want to review this because in generalizing a mixed model, you will encounter this issue again.

293
00:34:29,220 --> 00:34:35,210
And so for example, if you are talking about Bernoulli, the variance is of a familiar form,

294
00:34:35,610 --> 00:34:43,440
the success probability times one minus the success probability, and it has a no one over dispersion from two fibers one.

295
00:34:45,340 --> 00:34:57,640
Okay. And when you're dealing with, you know, binomial, for example, when you have covers of the street, right, say the culvert is like age group.

296
00:34:57,760 --> 00:35:00,820
So you have many age group, many people in different age groups.

297
00:35:01,330 --> 00:35:04,400
And you ask with each age group, what's the number of events, right?

298
00:35:04,420 --> 00:35:10,750
So the number of people to start with will be, say MJ for it for the JTH cohort pattern.

299
00:35:11,090 --> 00:35:16,810
So J can go from 1 to 5 if there are five age groups and different age groups have different numbers of people.

300
00:35:17,320 --> 00:35:23,110
And y j here simply counts the number of, say, event of interest in that group.

301
00:35:23,440 --> 00:35:27,880
And there you will have a binomial distribution.

302
00:35:28,150 --> 00:35:34,780
And the meaning of binomial distribution is again very similar, just with an additional multiplier of m j here.

303
00:35:36,620 --> 00:35:40,999
Now. Importantly, when we are going from binary good binomial,

304
00:35:41,000 --> 00:35:48,830
we make an assumption that say about ten people, each one, each person is tossed in an independent coin.

305
00:35:49,040 --> 00:35:57,380
So the outcomes are independent and only under that assumption you can call the total number of success in that group to be a binomial.

306
00:35:57,680 --> 00:36:04,640
But there are situations where that independence assumption can be violated, like in the repeated measurement setting.

307
00:36:05,180 --> 00:36:11,180
So this brings us to some observed phenomena that can be explained by these lack of independence.

308
00:36:11,900 --> 00:36:15,800
First, I just want to talk about the term over dispersion.

309
00:36:16,430 --> 00:36:23,810
So over dispersion basically means that the actual observed variability way exceeds what the model predicts.

310
00:36:24,590 --> 00:36:35,470
What does that mean? For example, a binomial model predicts that if you have more people, if the success probabilities measure,

311
00:36:36,170 --> 00:36:42,980
this is the variability of the number of success you observe in J's covariance strata stratum.

312
00:36:44,120 --> 00:36:54,920
However, there are additional variabilities in empirical data and that can often that often does not match this particular prediction.

313
00:36:55,640 --> 00:37:00,770
So people would argue that, hey, we may need to put in additional over dispersion from here.

314
00:37:03,160 --> 00:37:08,080
And what are the consequences of fate of failing to account for these over dispersion?

315
00:37:09,370 --> 00:37:18,510
Primarily, it's about the standard error estimation. So certain areas could be underestimated if you do not account for over dispersion.

316
00:37:18,960 --> 00:37:22,680
And and that's usually a bad thing in terms of inference.

317
00:37:30,740 --> 00:37:36,490
So in the next few slides, I think these are very simple logistic regression examples.

318
00:37:36,500 --> 00:37:45,210
I will skip them. Now let's look at the second class model, which is called a La Nina models with count outcomes.

319
00:37:45,630 --> 00:37:54,630
So it is also referred to as Poisson regression because people often make an assumption of Pearson distributed outcomes

320
00:37:55,050 --> 00:38:05,580
and it is very useful when you outcomes are accounts that occur during a period of time or in a pre specified space.

321
00:38:06,750 --> 00:38:13,799
So for example, the response verbal can be the count of the number of epileptic seizures for particular patients in the

322
00:38:13,800 --> 00:38:20,940
four week interval or in may be account of bacteria presence in the fixed volume of bacterial suspension.

323
00:38:21,420 --> 00:38:24,270
All right. So in either cases, y is a count.

324
00:38:24,570 --> 00:38:33,840
And the goal of doing these log 90 regression is trying to use cover as you can measure your subject to explain the count.

325
00:38:36,650 --> 00:38:44,960
And whenever you have a count, you've got to ask a count in what time, during what time account, in what space.

326
00:38:45,170 --> 00:38:50,000
So this brings up a concept of race here.

327
00:38:51,780 --> 00:39:01,260
So for example, if you have a four week interval versus a 12 month interval in which you're going to count the number of seizures,

328
00:39:01,530 --> 00:39:07,140
clearly, you know, the count will have a lot to do with the duration of that of those intervals.

329
00:39:07,500 --> 00:39:11,970
So it makes sense to talk about the rate for, say, every week.

330
00:39:12,390 --> 00:39:15,540
What is the rate with which the seizure would occur?

331
00:39:16,100 --> 00:39:22,020
Right. And this is particularly important if you observe different subjects for different intervals,

332
00:39:22,530 --> 00:39:26,430
because the time and risk will not be the same across all the people.

333
00:39:26,790 --> 00:39:30,270
And there you will need to do some adjustment in these regression.

334
00:39:30,630 --> 00:39:35,730
And this is, I would say, brings in the concept of offset.

335
00:39:35,730 --> 00:39:49,310
We will talk about that. Now, just as we do in general, generalized models, we talk let's talk about the random component.

336
00:39:49,810 --> 00:40:00,460
And, you know, in here it is basic to say that we do we assume that person distribution, which is which has a mass function like this.

337
00:40:00,880 --> 00:40:07,300
And we know for some distribution, famously, the mean is going to be equal to the variance.

338
00:40:09,580 --> 00:40:19,300
And for every observation, if we have the information regarding the time at risk, say, for four weeks,

339
00:40:19,510 --> 00:40:24,570
let's observe a number of seizures for this patient, then tis for or for another person.

340
00:40:24,580 --> 00:40:30,670
If the duration of observation is much longer, say, 12 months that he is going to be 12.

341
00:40:31,060 --> 00:40:41,650
Okay, so this may be different across people. And in those cases, when the tie varies across people, it would make more sense to talk about the rate,

342
00:40:41,650 --> 00:40:47,890
which is the unit time occurrence of the event of interest which is represented by UI,

343
00:40:48,190 --> 00:40:54,040
the being over that interval divided by the time at risk or the observation interval.

344
00:40:56,760 --> 00:41:02,730
So in these cases, you know, we just put on, as you know, very simple model.

345
00:41:04,320 --> 00:41:11,970
What is in the parentheses is just the rates. What's on the right is just an intercept, plus a single scalar covered right.

346
00:41:12,300 --> 00:41:16,260
And when you move that t to the right hand side,

347
00:41:16,620 --> 00:41:29,760
you can see that this is just like you transform your by a log logarithm and then you have an offset term log ty where you put in there,

348
00:41:29,790 --> 00:41:33,119
not for the purpose of estimating its coefficient,

349
00:41:33,120 --> 00:41:41,520
but rather you put it there with a coefficient of one to account for the fact that this person have an observation interval of time.

350
00:41:42,660 --> 00:41:44,520
So this is a what people call an offset.

351
00:41:44,820 --> 00:41:55,650
And for the remaining two terms, the original intercept beta one and the slope beta two, they ought to be estimated as know, and they are unknown.

352
00:41:55,680 --> 00:42:14,990
They are not set to any particular values. Similarly, we will need to talk about the over dispersion under the La Nina model.

353
00:42:15,230 --> 00:42:24,050
And this is actually a pretty severe issue, and you can detect that again quite easily if you have a table that's like, let me think.

354
00:42:25,700 --> 00:42:26,000
Uh.

355
00:42:28,320 --> 00:42:39,360
So if you have a table with ID, be one, two, three, four, five and you have an outcome and suppose everybody has the same duration observation, right?

356
00:42:39,360 --> 00:42:48,240
Like one week or one year, what have you. And this is like ten, 15, 25 or seven.

357
00:42:48,720 --> 00:42:53,670
So clearly you can check whether the mean is equal to the variance, right?

358
00:42:53,970 --> 00:42:58,510
You can calculate the mean. You can calculate the variance.

359
00:43:00,810 --> 00:43:10,700
And ask, are they the same? Often, often in real data analyzes the variance is much bigger than the mean, and there are multiple reasons for it.

360
00:43:10,730 --> 00:43:17,540
I'll go through them. And this is a situation where you're sure that a person will not work too well

361
00:43:17,540 --> 00:43:22,220
because person distribution naturally assumes the mean and variance are equal.

362
00:43:25,380 --> 00:43:31,350
So this is the point I'm trying to make. At least it's spelled out in three.

363
00:43:31,680 --> 00:43:39,390
In many biomedical applications, count data have variability that far exceeds that's predicted by Poisson distribution.

364
00:43:39,930 --> 00:43:46,500
So it means that the variance of Y is going to be bigger than what's predicted.

365
00:43:51,190 --> 00:43:58,870
Was predicted by Sun. As you know, if you use person, the prediction is fiery.

366
00:43:59,430 --> 00:44:05,549
So what people do is that they just need to assume, say why the variance?

367
00:44:05,550 --> 00:44:10,290
Why is some factor times why here with this factor often greater than one?

368
00:44:12,990 --> 00:44:23,100
So this is the technical fix, but what are the potential reasons for this kind of over dispersion or extra person variation?

369
00:44:25,170 --> 00:44:35,100
There are at least two reasons that I know of. The first one is that we may have a huge heterogeneity in terms of the mean here.

370
00:44:35,670 --> 00:44:39,510
And also there might be a correlation or dependance.

371
00:44:43,440 --> 00:44:52,830
In specifying the model. You know, as we say, we can put in a particular over dispersion parameter.

372
00:44:53,930 --> 00:44:59,330
And we use this to fit the maximum likelihood model and.

373
00:45:00,700 --> 00:45:09,250
And we may fix the over dispersion issue. There are two more ways that we have not introduced so far.

374
00:45:10,300 --> 00:45:15,150
The second way is to use a sandwich estimate for this.

375
00:45:15,160 --> 00:45:19,270
You will come across this name quite frequently in hand only.

376
00:45:20,380 --> 00:45:31,840
So the idea is that the estimation of beta and the assessment of uncertainty of beta hat will be robust to over dispersion.

377
00:45:32,590 --> 00:45:38,950
Number three, this is actually going to be related to the generalized unit mix model where you

378
00:45:38,950 --> 00:45:44,259
just put in additional source of probability so that after you specify the model,

379
00:45:44,260 --> 00:45:47,770
you do can see that the variance of y is greater than the mean.

380
00:46:07,030 --> 00:46:16,299
Now some interpretations, just like what we did with the binary outcomes when you are interpreting the percent sorry,

381
00:46:16,300 --> 00:46:25,300
look log linear regression models, what you do again is trying to make the simple comparison by setting covariates at different levels.

382
00:46:25,540 --> 00:46:32,950
Say if it's just one single convert, you know, you're contrasting x equals one versus X equals zero.

383
00:46:33,370 --> 00:46:37,270
And you just take this difference and you can see the difference in beta two.

384
00:46:37,810 --> 00:46:42,670
Then you can interpret this as a beta two is the log.

385
00:46:44,180 --> 00:46:48,170
Rate ratio per unit change in covered.

386
00:47:09,550 --> 00:47:15,610
Okay. And clearly, if you express, you hate it, you can just interpret that as void ratio.

387
00:47:15,640 --> 00:47:19,270
So exponential beta two is a rate ratio per unit change.

388
00:47:19,270 --> 00:47:23,380
You are to interpret the intercept.

389
00:47:24,250 --> 00:47:27,940
You just need to know that in the model you have.

390
00:47:32,600 --> 00:47:38,080
You know, one plus matter to us. I just said X equals zero.

391
00:47:38,110 --> 00:47:41,350
Then you recover the invitation of beta one, which is the.

392
00:47:44,320 --> 00:47:48,970
Log of the expected rate for people who have the cover value of zero.

393
00:47:55,590 --> 00:48:01,290
So in the following few slides we just provide some illustrative model fitting for the sake of time.

394
00:48:01,740 --> 00:48:08,850
I'm just going to skip them. They provide you with some review of how to use your line.

395
00:48:08,880 --> 00:48:18,360
I hope that you're familiar with this command from 651, and there you do need to specify certain information to be offset.

396
00:48:18,990 --> 00:48:23,240
So let me. So this is the data, right?

397
00:48:23,250 --> 00:48:26,760
And the first column essentially is a tie.

398
00:48:28,770 --> 00:48:34,140
And this is exciting one excited to excite three and this is why I write.

399
00:48:34,710 --> 00:48:39,910
So why is account. But over. Different intervals.

400
00:48:41,120 --> 00:48:45,200
Okay. And you have three covariates smoking.

401
00:48:45,650 --> 00:48:54,710
How many pets? How many cigarets you smoke? What's the is a pressure, you know, abnormal, normal and what kind of behavior this person exhibit.

402
00:48:56,540 --> 00:49:07,309
So you can request the coronary heart disease event counts upon this smoking status when the offset of the observation

403
00:49:07,310 --> 00:49:16,430
interval and here you specify the model family to be person and then you've got this one and if you include more covariates,

404
00:49:16,880 --> 00:49:18,110
then this is what you get, right?

405
00:49:18,380 --> 00:49:28,100
Still, you have the outcome as the count of the coronary heart disease events, and you have the smoke, you have additional covariates.

406
00:49:28,460 --> 00:49:31,700
Again, you specify the offset to be the.

407
00:49:32,640 --> 00:49:35,700
Duration of the intervals in which you observe these events.

408
00:49:36,330 --> 00:49:40,730
Here you do the song regression. Then you specify the family persona.

409
00:49:40,800 --> 00:49:46,800
So you got these estimates. I don't think these there are anything new here compared to 651.

410
00:49:47,730 --> 00:49:52,650
So a quick summary we reviewed very quickly.

411
00:49:52,830 --> 00:50:01,260
That's the framework of generalized models and it is very useful that it provides a general

412
00:50:01,260 --> 00:50:06,270
framework to relate the meaning of a discrete or continuous response to covariates.

413
00:50:07,110 --> 00:50:12,640
And to visualize that, I would encourage you to consider those kind of, you know,

414
00:50:12,720 --> 00:50:17,520
what people call Dak's or graphs that have those three parts specified.

415
00:50:18,270 --> 00:50:22,679
And importantly, different from linear models.

416
00:50:22,680 --> 00:50:29,190
We now have this transformation called G, right? We have the g, mu i equals and I here.

417
00:50:29,700 --> 00:50:35,220
This G can be logit can be log. Depending on the outcome you are studying.

418
00:50:35,700 --> 00:50:38,910
And it is because you use different g for different outcomes.

419
00:50:39,210 --> 00:50:47,910
You need to be careful about the invitations and you need to to be aware of the G you're using in terms of estimation.

420
00:50:48,090 --> 00:50:54,030
I believe the technique you've learned in 651 is primarily maximum like estimation.

421
00:50:54,570 --> 00:51:00,570
And unfortunately, that's not enough for dealing with the extension of vigilance.

422
00:51:00,750 --> 00:51:04,740
So we will be introducing two more two more sets of techniques.

423
00:51:05,640 --> 00:51:17,350
One is called estimating equations. The other is basically to solve geo.

424
00:51:17,390 --> 00:51:22,990
And I think there are like. Penalized.

425
00:51:24,700 --> 00:51:27,790
Chris, I. Likelihood.

426
00:51:31,990 --> 00:51:35,880
Which is called peak oil. And there are other extensions, too, like.

427
00:51:38,490 --> 00:51:47,160
So my main point is that for it to deal with the extensions, not only the model formulation has to be extended,

428
00:51:47,490 --> 00:51:51,810
but also the estimation techniques have to be extended as well.

429
00:51:52,230 --> 00:52:01,620
And because this clause is really a first intro to longitudinal data analysis, I will not talk about this estimation technique in too much detail,

430
00:52:01,890 --> 00:52:11,010
but hopefully this can at least give you the note that if you late on doing these research in your own work,

431
00:52:11,490 --> 00:52:14,400
that these are the keywords you can search on.

432
00:52:14,730 --> 00:52:25,890
For some of you who desperately need a review, I would say there are two sections I highly recommend to to read in the class in the textbook.

433
00:52:26,310 --> 00:52:30,510
And I think at least to me, it was very helpful when I was preparing for this lecture.

434
00:52:30,810 --> 00:52:32,520
So hopefully that can be helpful to you as well.

435
00:52:34,350 --> 00:52:40,620
I don't say I will talk about these, but these are the summary of all the models you've learned in the 651.

436
00:52:41,130 --> 00:52:44,370
I don't know whether you guys learn this, but if not, we can just forget about this.

437
00:52:44,800 --> 00:52:48,060
But I believe primarily you guys were learning these in 651.

438
00:52:48,510 --> 00:52:57,570
So let's take a five minute break and come back at 4 p.m. and we will talk about handout or nine of the module models.

439
00:59:02,830 --> 00:59:06,700
All right, buddy. So let's get back to work.

440
00:59:06,700 --> 00:59:13,630
And actually, I'm quite excited to start talking about the most general extensions.

441
00:59:14,380 --> 00:59:21,100
So let's start with the margin models. And for this handout, we have three parts on our APC.

442
00:59:21,280 --> 00:59:28,990
So what are they are a is about introduction. So it is primarily focused on the conceptual introduction to how we do the generalization.

443
00:59:29,440 --> 00:59:35,559
As you would recall at the beginning of a class, we have spent some time introducing notations.

444
00:59:35,560 --> 00:59:40,540
Hopefully all those notation will pay off all those efforts in understanding notation.

445
00:59:41,260 --> 00:59:47,830
And the additional things we need to do is to extend them to deal with non continuous outcomes.

446
00:59:49,390 --> 00:59:52,970
Number two, it is actually about inference. So how do we do the inference?

447
00:59:52,990 --> 00:59:54,129
As I have promised,

448
00:59:54,130 --> 01:00:04,690
there is a new technique called estimating equation and they just decided to name a g generalized estimating equations you can imagine.

449
01:00:04,700 --> 01:00:09,310
So actually in this I was learning this when there is a famous I believe.

450
01:00:11,110 --> 01:00:14,710
Korean. Ben. And I think they have.

451
01:00:15,490 --> 01:00:23,290
They have a son named there. So we were making fun of this, but they probably revealed too much about how old I am.

452
01:00:24,190 --> 01:00:32,290
So this is a technique for estimating the margin models. And finally, it is the G examples.

453
01:00:32,560 --> 01:00:36,610
Okay. And there I have provided all the R code,

454
01:00:36,790 --> 01:00:43,119
the data to run our code and also additional references you can consult when

455
01:00:43,120 --> 01:00:47,700
you're trying to understand the new functions you would need to fit the G.

456
01:00:49,690 --> 01:00:54,520
Tech models. And okay, so we are going to start with the introduction.

457
01:00:59,680 --> 01:01:07,360
Again. Disclaimer This lecture does not provide any introduction about anything about estimation of the parameters.

458
01:01:07,720 --> 01:01:13,510
It is only about the specification of these models, models, assumptions and interpretations.

459
01:01:13,780 --> 01:01:21,249
So this is a more conceptual lecture. So please focus on how do we do it in the first place?

460
01:01:21,250 --> 01:01:28,960
Why do we bother to do this right? And I think I would want to challenge you to as herself, not to me, but after class.

461
01:01:29,260 --> 01:01:38,680
Have you understood the meaning of marginal? And my so my expectation is that this is this is something you should have already known.

462
01:01:38,800 --> 01:01:43,120
And this meaning is the same as you have learned these words before.

463
01:01:44,050 --> 01:01:50,680
In the first part of this lecture. So that's a little challenge to you for learning objectives.

464
01:01:52,240 --> 01:01:57,580
After the class, you want to ask yourself, Can I define the three components of the margin model specification?

465
01:02:00,310 --> 01:02:04,660
Number two, can I explain why the models define here our terms and model models?

466
01:02:06,550 --> 01:02:10,600
And what you would ask is what is non margin models, right?

467
01:02:11,080 --> 01:02:20,200
Number three, can I explain of a key assumption called for called conditional mean assumption, FCC assumption.

468
01:02:21,310 --> 01:02:26,530
Number four, can I show the framework using three examples?

469
01:02:26,830 --> 01:02:31,360
So those are the four questions I would say you can ask yourself when you're reviewing these slides.

470
01:02:31,600 --> 01:02:37,150
If all the answers are positive, then I'm very optimistic that you have grasped the central ideas.

471
01:02:37,720 --> 01:02:52,880
So let's start with the first one. So it is about why bother?

472
01:02:52,910 --> 01:03:01,070
Why do we want to do this generalization? Clearly from a from this class of all things are longitudinal or all things are repeatedly measured.

473
01:03:01,460 --> 01:03:06,930
So. There is no technique to deal with that when you have not continuous outcomes.

474
01:03:08,010 --> 01:03:16,819
But I'm going to summarize them here. So first, if you are going to use GitLab for independent observation,

475
01:03:16,820 --> 01:03:23,700
you have to here you will ignore the possible within person dependencies, and that may lead to problems with the inference.

476
01:03:24,420 --> 01:03:33,900
And number two, even if you can try to use regression models forcefully to analyze not continuous outcomes,

477
01:03:34,260 --> 01:03:44,070
that may have a lot of problems, including the outer edge problem say we're going to request a success probability upon a convert.

478
01:03:44,160 --> 01:03:49,860
So there's no guarantee that the new predictor will give you a number because you're in a one.

479
01:03:51,870 --> 01:04:01,950
And a subtle point here is that when you are dealing with media regressions, the G there is.

480
01:04:05,610 --> 01:04:12,700
So G is actually identity, right? When you are using.

481
01:04:14,930 --> 01:04:26,700
Why is calcium? And this is no longer true.

482
01:04:27,780 --> 01:04:33,120
Sorry, this g may take other form when you have done Gaussian distributions.

483
01:04:34,540 --> 01:04:46,060
And in those cases, it is especially important to recognize that calcium based aggression has a very has a unique feature.

484
01:04:46,450 --> 01:04:49,660
Let me explain. I think you guys have seen this kind of formula.

485
01:04:56,580 --> 01:05:00,060
This is the classical know mixed model formula.

486
01:05:05,440 --> 01:05:09,760
Okay. And we call this a conditional model.

487
01:05:09,790 --> 01:05:15,430
Right. But when you're trying to integrate over by.

488
01:05:29,990 --> 01:05:43,110
You will be able to derive that this whole thing is this one. So this is basically averaging over.

489
01:05:50,770 --> 01:05:54,450
Over by. And this is averaging over.

490
01:05:59,070 --> 01:06:03,210
Y i j given x i j i hear.

491
01:06:03,990 --> 01:06:16,270
Okay. So what is extraordinary with Gaussian based in your model is that regardless of whether you interpret, be to hear or interpret to hear, right?

492
01:06:18,330 --> 01:06:22,860
They're the same. They related to the men in the same way.

493
01:06:24,030 --> 01:06:28,970
But whenever when you have a you know. Noncontiguous outcome.

494
01:06:29,330 --> 01:06:33,290
This may not be the same thing. Let me show you. So.

495
01:06:37,430 --> 01:06:43,190
If you have exponential expectation that by and.

496
01:06:45,170 --> 01:06:59,670
G. Later on, you want me to deal with this kind of integration?

497
01:07:00,150 --> 01:07:03,780
So, g unlike the above, right?

498
01:07:03,810 --> 01:07:09,330
This is basically identity here. Identity here.

499
01:07:09,780 --> 01:07:12,840
So g for other, an unconscious outcome will not be identity.

500
01:07:13,140 --> 01:07:16,800
So it is not easy to integrate easily.

501
01:07:50,260 --> 01:07:55,629
And this basically creates a discrepancy that depending on whether you are doing model

502
01:07:55,630 --> 01:08:03,340
condition by or if you're doing model with integrate out when you have a non-linear link,

503
01:08:03,850 --> 01:08:08,550
these two may have very different limitations. So again, this is just a preview.

504
01:08:08,560 --> 01:08:10,389
We will talk about technical details about later,

505
01:08:10,390 --> 01:08:16,960
but all the complications come from the integration over a transformative version of the expected value here.

506
01:08:19,460 --> 01:08:24,290
So let's give them some names. For these kind of models.

507
01:08:27,670 --> 01:08:39,070
We call this population average Aboriginal. Why?

508
01:08:39,250 --> 01:08:45,000
Well, we integrate over the by. So it is a population average version.

509
01:08:45,600 --> 01:08:50,730
And in this particular model it is talking about.

510
01:08:51,960 --> 01:08:57,420
How the change in some population in terms of their covers would change the expected value of the outcome.

511
01:08:58,560 --> 01:09:03,590
For the model here, we call them subject. Specific models.

512
01:09:13,350 --> 01:09:21,840
Why do we call that subject specific models? It is because we have conditioned upon a subject specific random effect.

513
01:09:32,730 --> 01:09:35,730
And for handouts online. We are going to focus on this one.

514
01:09:39,040 --> 01:09:42,870
And these are for handouts. Ten.

515
01:09:44,350 --> 01:09:47,620
Here. You know, clearly they're just need new models.

516
01:09:47,620 --> 01:09:52,900
But we will try to do them. We will discuss them separately when we have noncontiguous outcomes.

517
01:09:53,570 --> 01:10:01,229
Okay, so let's focus on the population average version. Later on.

518
01:10:01,230 --> 01:10:06,660
We'll also talk about for Perdue, a data set. How do you choose between these two kinds of modeling approaches?

519
01:10:12,350 --> 01:10:18,410
So let's go to the part number two, which is for the margin, the model for longitudinal data.

520
01:10:19,190 --> 01:10:22,610
And here the word marginal needs some special attention.

521
01:10:22,940 --> 01:10:26,180
We're not conditioned on any subject specific random effects.

522
01:10:26,870 --> 01:10:34,280
So again, the term marginal indicates that we are only going to be modeling.

523
01:10:35,800 --> 01:10:44,620
Something like this. This is it.

524
01:10:45,250 --> 01:10:56,630
No buy involved. So the word marginal here represents the average over.

525
01:11:00,280 --> 01:11:06,100
I. Okay.

526
01:11:09,500 --> 01:11:17,510
And as you can see, that this cannot be called mixed effects model because there is a single effect of population level effect better.

527
01:11:25,150 --> 01:11:30,640
Okay. What is the general other considerations for module model?

528
01:11:31,240 --> 01:11:36,800
So first, it seems quite natural. It seems to be quite a natural extension to them.

529
01:11:40,840 --> 01:11:46,600
Why? Well, actually, Don, this is one component of the margin model specification.

530
01:11:46,990 --> 01:11:53,680
This looks exactly like the g j equals 8ij.

531
01:11:53,680 --> 01:12:01,660
Right. So this is like the double indexed extension to the single indexed dual setting.

532
01:12:02,050 --> 01:12:06,610
So I here represent the subject. I j here represents two occasions.

533
01:12:07,060 --> 01:12:11,020
So we are going to have this as a component, the margin model. So it's quite natural.

534
01:12:12,640 --> 01:12:19,030
Number two, this is something new. It does not require any distribution or assumptions for the observations.

535
01:12:19,810 --> 01:12:24,340
Only a regression model for the mean response. Let me explain.

536
01:12:24,730 --> 01:12:31,660
So whenever you are doing generalized model, right, you remember there is a random component.

537
01:12:36,860 --> 01:12:43,440
For this is specify the distribution. Of what's and why I write.

538
01:12:43,860 --> 01:12:50,460
And then you do the likelihood. You write down the likelihood function, and then you do the MLB, right?

539
01:12:50,880 --> 01:12:56,100
This is how you get the how you get the smarter data, especially.

540
01:12:58,080 --> 01:13:01,320
However, emerging models we do not.

541
01:13:02,330 --> 01:13:28,090
We do not do this. All we need to specify is this one is how the meeting would be related to the covers.

542
01:13:31,380 --> 01:13:36,810
And clearly, if we say we do not do that someone like her, we need to provide some alternative.

543
01:13:37,110 --> 01:13:40,830
And that happens to be what we call generalized estimating equation.

544
01:13:41,920 --> 01:13:59,489
And this is our IP. And because we are doing extensions.

545
01:13:59,490 --> 01:14:05,670
So it makes more sense for us to discuss the extension on more challenging outcomes like the non continuous outcomes.

546
01:14:06,030 --> 01:14:13,290
So the main focus when we're introducing the margin models will be focused on discrete data like binary responses accounts.

547
01:14:13,770 --> 01:14:19,110
And it is not surprising that for continuous outcomes there are some natural connections.

548
01:14:19,740 --> 01:14:29,460
But I will only talk about them when we encounter them. So the goal is trying to make the margin model to work for noncontiguous outcomes.

549
01:14:30,090 --> 01:14:34,440
So let's look at some notations. So this one is basically something you're very familiar with.

550
01:14:35,040 --> 01:14:40,970
Let's collect all the outcomes for subject AI. So if it's binary, that can be like 1001, right?

551
01:14:40,980 --> 01:14:45,809
So something like that. And you have capital and number of subjects for every subject.

552
01:14:45,810 --> 01:14:49,620
You may have a distinct number of observation observation occasions.

553
01:14:50,850 --> 01:14:58,150
Second, you want to relate absurd features to this person's vector of observation vector of outcomes.

554
01:14:58,530 --> 01:15:04,080
So this again, is something that's exactly the same as you have learned in linear models.

555
01:15:05,570 --> 01:15:13,040
And SGA represents the vector of P features you collected at the location for such an AI,

556
01:15:13,310 --> 01:15:19,040
and they can be a timing variant and they can be time varying converts.

557
01:15:21,520 --> 01:15:26,180
So more broadly, you can use a big matrix to represent all of these information.

558
01:15:26,620 --> 01:15:36,250
And for in longitudinal data analysis, this is called design matrix of Dimension II by P, where in is the number of occasions.

559
01:15:36,610 --> 01:15:39,780
So this one is basically what's the second occasion, right?

560
01:15:43,820 --> 01:15:50,570
And you have p columns representing the pieces of information you want to use as predictors for the outcome.

561
01:15:52,920 --> 01:16:02,250
On. When you are specifying the marginal models, it is very important to distinguish two kinds of parameters.

562
01:16:03,750 --> 01:16:09,030
The first one is called substantive parameters, which is often denoted by the Greek letter beta.

563
01:16:11,750 --> 01:16:18,350
There is another set of parameters which we call more general parameters, which are the association parameters.

564
01:16:26,200 --> 01:16:35,290
I have purposefully avoided to use the word correlation because associations are more broader is a broader concept.

565
01:16:35,500 --> 01:16:39,850
Correlation often meant and correlation, which is not what I intended.

566
01:16:41,290 --> 01:16:54,760
So margin models are primarily used to make inference about population means like, you know, movies and how they are related to the fetus and of.

567
01:16:55,920 --> 01:17:01,620
We will separate and model the meaning structure and the and structure.

568
01:17:02,580 --> 01:17:10,480
So beta often is using the ME model. Alpha is often using the association model.

569
01:17:25,350 --> 01:17:30,690
So how do we specify the model? Three components. First one, let's do the.

570
01:17:33,090 --> 01:17:39,210
A new predictive specification. So here it is exactly the same as generalized model.

571
01:17:39,480 --> 01:17:46,880
The only difference is that now we have two indices. Representing the observations nested within each subject.

572
01:17:47,390 --> 01:17:51,950
G is a link function mu AJ is the mean response for subject.

573
01:17:51,950 --> 01:18:01,580
I had occasion j and AJ. Here is a NINA predictor representing the combination of the features you measured at the JTH occasion for subject I.

574
01:18:03,550 --> 01:18:07,780
Now, after this step is done, you have created a model for the main.

575
01:18:08,650 --> 01:18:12,100
Okay. Is that all? Not yet.

576
01:18:12,910 --> 01:18:19,690
There is this association or variance part, and this part is the most tricky part.

577
01:18:19,720 --> 01:18:26,980
I was the most challenging part. When you are trying to learn these models and we will talk about them repeatedly.

578
01:18:26,980 --> 01:18:31,660
So hopefully some of them will be making sense to you. How do we specify the model?

579
01:18:32,380 --> 01:18:39,550
Well, remember, we're dealing with models of the models. So the variance we're going to specify is about the you know.

580
01:18:40,660 --> 01:18:49,270
It's about what is about the veterans condition on covers, not all any random effects, and often it's written in two parts.

581
01:18:51,840 --> 01:19:02,680
FYI the dispersion parameter. Number two, this is called variance function.

582
01:19:07,960 --> 01:19:20,890
Examples. Well, if y j is a 1 to 0 a binary outcome, then fi the variance can be was a variance can be five times in my times one minus my j.

583
01:19:20,890 --> 01:19:23,470
Sorry J. What does this mean?

584
01:19:23,650 --> 01:19:32,770
Well, you're thinking, hey, you have a variance that looks like Bernoulli, but you do believe there might be a need for a over dispersion parameter.

585
01:19:33,100 --> 01:19:37,900
So these two parts multiplied together is your model for the variance.

586
01:19:40,210 --> 01:19:46,570
And Curious Mind may ask, how do I assure that the fires are the same across occasions?

587
01:19:46,900 --> 01:19:55,270
You don't have to make them the same, but most frequently it is imposed, defies the same rigorous occasions for simplicity.

588
01:19:57,160 --> 01:20:02,170
I do believe you can make them more flexible, but I personally have not used these models in practice.

589
01:20:02,620 --> 01:20:07,830
But I guess technically you can. So this specify the variance.

590
01:20:07,840 --> 01:20:13,809
But we have we still have and talk about the association between two measurements obtaining the same person.

591
01:20:13,810 --> 01:20:19,360
Right. That's a place where we're going to talk about currencies or associations.

592
01:20:19,690 --> 01:20:26,500
So. This is a long sentence, but we're trying to model the Within Subject Association.

593
01:20:30,820 --> 01:20:37,480
For example, we may represent the pairwise correlation or log odds ratio among repeated responses.

594
01:20:38,590 --> 01:20:49,800
If you are dealing with a binary outcome. You want to ask, what's the association?

595
01:20:53,360 --> 01:20:57,670
Between. Why J.

596
01:20:57,700 --> 01:21:02,140
And why I. All right, so what is a good measure to characterize it?

597
01:21:03,760 --> 01:21:06,969
Well, you have learned to buy a few tables. Right.

598
01:21:06,970 --> 01:21:12,010
You can do y j y k and it's a120120.

599
01:21:12,040 --> 01:21:16,569
Right. What's this? What's a measure of the association? Log odds.

600
01:21:16,570 --> 01:21:38,240
Right. Log log odds. Ratio option. Which is the log of p11 times p00 divided by p10 times p01.

601
01:21:38,540 --> 01:21:46,670
And here the PS represents the proportion of people that takes a particular pattern, that cell.

602
01:21:47,510 --> 01:21:53,180
So this is a log odds ratio. And when you have multiple such pairs, right.

603
01:21:53,300 --> 01:21:59,990
You know, J and K can be any pair in the many occasions obtained from the same person.

604
01:22:00,260 --> 01:22:06,110
You have many pairs and all those log odds ratios will comprise alpha.

605
01:22:13,860 --> 01:22:18,150
So we will stop now.

606
01:22:18,570 --> 01:22:25,650
I just want to say that for marginal model specification, all you need to do is the main specification,

607
01:22:26,280 --> 01:22:35,100
the variance specification, which often involves the dispersion parameter and finally some specification about the association.

608
01:22:35,520 --> 01:22:39,450
And it is very case by case because it depends on kind of outcome.

609
01:22:39,660 --> 01:22:41,430
You may care about different association.

610
01:22:41,850 --> 01:22:50,130
So from the next Monday, we will be continuing from here and give you some examples of the model specification.

611
01:22:50,730 --> 01:22:51,600
All right. Thanks.

