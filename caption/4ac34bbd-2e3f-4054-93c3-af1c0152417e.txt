1
00:00:01,860 --> 00:00:10,000
He's close to 163.

2
00:00:10,350 --> 00:00:21,950
So yeah, you really have to go from different people thinking such a big items are really actually going to do this.

3
00:00:22,290 --> 00:00:38,520
It's nice to have some zero tolerance, not the way you can submit this, basically the way some make your homeworks,

4
00:00:38,970 --> 00:01:01,330
make sure it's clear from the answers in the comments and others are actually attached and maybe an extra final four figures.

5
00:01:01,410 --> 00:01:15,959
So there's two questions and ask you to make figures of 12 attached to extra figures

6
00:01:15,960 --> 00:01:23,470
or if you're doing something for the arms which are marked down as a single document.

7
00:01:23,880 --> 00:01:36,390
Why do you want to do something that tries to download the data to make sure?

8
00:01:36,660 --> 00:01:42,420
No problem. So the data files folder isn't it?

9
00:01:42,790 --> 00:01:54,870
So these two needs to file is completely automated saying hey, I know this assignment.

10
00:01:55,610 --> 00:02:05,009
I guess the quiz on some category, let's leave it on when you see this, that's basically in class.

11
00:02:05,010 --> 00:02:09,280
So that's basically like a shorthand.

12
00:02:09,300 --> 00:02:19,830
It's like I said, it's not you don't have to write any code to put in time.

13
00:02:22,290 --> 00:02:30,730
So it's more like the style of the online. It's more like we just have to fill in for most of the questions.

14
00:02:30,750 --> 00:02:34,470
Just like a numbers one is just a workaround.

15
00:02:36,180 --> 00:02:52,500
So it's kind of the style. So any advice for Muslims all over the world of the notes and the and these two quizzes?

16
00:02:53,430 --> 00:02:59,639
Who's the future of your question questions tonight or tomorrow?

17
00:02:59,640 --> 00:03:08,000
We get some extra samples from past answers, but I'll give you extra questions.

18
00:03:09,540 --> 00:03:17,560
This is just I was trying I didn't try to ask anything.

19
00:03:17,760 --> 00:03:29,969
Have to be super like crazy detail you just mentioned and it's like you never mentioned.

20
00:03:29,970 --> 00:03:41,700
We tried to reduce the main the main, main features of the different data structure here for this panel.

21
00:03:45,140 --> 00:03:52,230
Teach the different in writing logical expressions using it statements things like that.

22
00:03:52,230 --> 00:04:00,890
So. You know, I just want to go to lectures and probably helps to lower the homework a little bit too.

23
00:04:01,340 --> 00:04:05,730
She's definitely I think the style is kind of similar to the online courses.

24
00:04:07,180 --> 00:04:13,940
Oh, yeah. Shall we have a function topic as well as a quiz?

25
00:04:15,370 --> 00:04:21,970
Yeah. There's probably two questions on this.

26
00:04:22,730 --> 00:04:28,730
It's not a ton of. Yes, not a ton of function questions will get a little bit.

27
00:04:30,230 --> 00:04:37,250
And please, we can't. I mean, it's just I can't say closed the quiz, which we already attempted.

28
00:04:37,460 --> 00:04:42,050
We just know the score. Got to do it for each question.

29
00:04:43,440 --> 00:04:47,030
I mean, you don't have the answers. No.

30
00:04:47,120 --> 00:04:54,470
It says you have to use your last attempt last year, whereas you have to just like you have to use all three attempts before you can see it.

31
00:04:56,600 --> 00:05:02,780
Oh, you have to use. Oh and you have to wait until he releases them.

32
00:05:02,930 --> 00:05:10,370
But, you know, we want to change the subject once.

33
00:05:10,370 --> 00:05:26,360
And he took a few minutes with you on your loose change students.

34
00:05:28,230 --> 00:05:30,440
And the first one, cause one is already gone.

35
00:05:30,890 --> 00:05:58,820
You know, we tried to find solutions and he said, we have seen some of the callers and try and I'm in it.

36
00:05:59,930 --> 00:06:13,850
Yeah. Yeah. Can you just as notes can be studied, it's not visible to the students until they have submitted them last time.

37
00:06:13,910 --> 00:06:20,319
Don't. Okay.

38
00:06:20,320 --> 00:06:26,200
Maybe try try a little later or during the class.

39
00:06:26,200 --> 00:06:33,630
And if not, I'll try to try to do something else.

40
00:06:33,640 --> 00:06:38,530
Or the worst case for at least for the actual answers.

41
00:06:39,040 --> 00:06:55,419
Yeah, hopefully that works. And then I'll do that for the quiz because quiz two is due tonight, so close to tomorrow after the due date for any other.

42
00:06:55,420 --> 00:07:00,430
Oh, I forgot to mention the follow up announcement.

43
00:07:01,030 --> 00:07:10,390
So the quiz doesn't cover it doesn't cover everything in the you know, for today and tomorrow.

44
00:07:10,600 --> 00:07:21,040
I mean, on Wednesday, then it's going to cover up to basically everything up to date of visualization.

45
00:07:21,040 --> 00:07:26,860
So that's like have maybe two thirds to today's lecture.

46
00:07:28,180 --> 00:07:35,770
So we're going to be seeing up to four days of visualization is covered on the quiz.

47
00:07:37,420 --> 00:07:42,060
Yeah. Is one or two answers positive.

48
00:07:43,600 --> 00:07:47,620
No, not at I'll I'll do that. I'll do that tonight as well.

49
00:07:47,750 --> 00:08:00,030
So we'll start shipping.

50
00:08:00,040 --> 00:08:04,900
Which earnings. No. So getting close to the end.

51
00:08:06,680 --> 00:08:12,920
So I guess we basically talked about all these things.

52
00:08:16,610 --> 00:08:26,479
Okay. So merging, merging and reshaping alliances, the first two things and then we're going to start talking about data visualization.

53
00:08:26,480 --> 00:08:34,130
We'll see how much basically to do as much as we can on that topic until the end of the class.

54
00:08:35,330 --> 00:08:40,610
Okay. So merging data is the next topic.

55
00:08:40,610 --> 00:08:47,149
So I guess the it's basically when you're trying to showing two different data sets in some way.

56
00:08:47,150 --> 00:08:55,940
So you are just two examples are one example where we have two different data frames.

57
00:08:56,480 --> 00:09:00,860
So these both have three, three observations.

58
00:09:00,860 --> 00:09:08,420
It's basically you have like an IDF variable in the first data frame.

59
00:09:09,080 --> 00:09:19,760
We have kind of a name that link that's connected to the idea is let's just put this, this is actually not necessary since default.

60
00:09:19,760 --> 00:09:30,890
But I did that anyway. Okay. So the people data frame just has an ID and a name and then the second data frame has an ID and like a score.

61
00:09:30,980 --> 00:09:38,030
So we can think of this as like a dataset which links people,

62
00:09:38,630 --> 00:09:45,710
people's names to their I.D. And this is a data set that links like an exam score to the I.D. You want to merge those?

63
00:09:45,890 --> 00:09:58,010
Okay. So I guess one thing to note about this is that like the the order of the people in the two different data frames is different.

64
00:09:58,010 --> 00:10:13,340
So like the first row and the exam is the nine, 8 seconds, I mean, but it's the second observation and the and the people data frame.

65
00:10:14,120 --> 00:10:18,680
So I guess we did this as a kind of a wrong way to merge them.

66
00:10:18,690 --> 00:10:31,069
I mean, you could just do like a C bind talked about with matrices and data frame where you just stack them, you put them right next to each other.

67
00:10:31,070 --> 00:10:42,290
So that's that's basically the wrong way to do it since since the last item, the ideas are they're not matched up across rows automatically.

68
00:10:42,860 --> 00:10:53,809
So we have the mismatch between rows one to I guess the other problem is that look, this is that this kind of duplicates the,

69
00:10:53,810 --> 00:11:05,120
um, I.D. column and we have a, basically the, is the first and the third column has the same, represent the same variable.

70
00:11:05,120 --> 00:11:09,590
Usually when you merge data sets, you just want to have like one ID column.

71
00:11:09,830 --> 00:11:13,580
Okay. So that's just an example.

72
00:11:17,180 --> 00:11:22,160
Showing basically the wrong way to do it is to contrast with the the right way to do it.

73
00:11:22,170 --> 00:11:27,860
So is the I guess the right way is to use energy from your deep flower.

74
00:11:29,330 --> 00:11:34,340
Also, there is a merge function from space that you can use as well.

75
00:11:35,000 --> 00:11:44,570
But if you're kind of combining this with kind of the other in our operations, it's probably good to know about inner join.

76
00:11:44,810 --> 00:11:51,350
Okay. So we do inner join. It kind of automatically joins everything in the correct order.

77
00:11:51,350 --> 00:11:59,810
So right here we can see that the exam score for the one, two, three, four, i, i, it should be 60.

78
00:12:00,470 --> 00:12:05,790
And so it kind of does that automatically for 60 minutes.

79
00:12:06,240 --> 00:12:18,110
It kind of just puts everything since we're joining by the idea, it'll only have like one variable in the in the final data frame.

80
00:12:18,230 --> 00:12:24,830
Okay. Okay. So let's look at like a slightly more complicated example.

81
00:12:24,830 --> 00:12:37,700
So here we kind of have the same setup where we have one data frame that has IDs and names and the other data frame has IDs and exam scores.

82
00:12:37,890 --> 00:12:47,629
Okay. I guess the only difference or the only difference between kind of this example, in the last example,

83
00:12:47,630 --> 00:12:59,270
we have an I.D. and in the people data frame that's not in the exam data frame this morning is either one, one, one.

84
00:12:59,270 --> 00:13:02,480
It is not in the. Oh, it is in the.

85
00:13:04,870 --> 00:13:08,280
Oh, yeah. Oh, sorry. It is in the exam day.

86
00:13:08,290 --> 00:13:09,310
And frankly, it's nice.

87
00:13:11,970 --> 00:13:23,640
I'm sorry the 555i5 I.D. that's that's in the people line dataframe and not in the exam data frame and then kind of vice versa.

88
00:13:24,510 --> 00:13:34,650
4444 idea is in the exam data frame but it's it's not in the it's not in the people data frame.

89
00:13:34,740 --> 00:13:38,520
Okay. So that's kind of something that could happen a lot.

90
00:13:38,520 --> 00:13:43,829
So there's really there's actually different ways of merging this type of data where

91
00:13:43,830 --> 00:13:51,200
there there's not quite 100% agreement between the IDs across different data sets.

92
00:13:51,990 --> 00:13:59,969
I would say probably at least for the types of datasets I usually analyze,

93
00:13:59,970 --> 00:14:10,380
I would say inner join is probably the most common thing I do when merging data sets, but they're sending all the type of analysis that you do.

94
00:14:10,830 --> 00:14:13,520
There are quite a few cases where you might want to do a left,

95
00:14:14,160 --> 00:14:26,430
a left join or a right join one is kind of a visual representation of what you're doing, and we talk about it in more detail for inner join.

96
00:14:27,300 --> 00:14:33,180
When you merge, then you're only keeping the like the ideas that are in both datasets basically.

97
00:14:33,180 --> 00:14:34,710
So that's kind of represented here.

98
00:14:35,220 --> 00:14:44,570
You're looking at the intersection of the IDs across data sets left join and you're basically keeping everything in the mix,

99
00:14:44,790 --> 00:14:49,500
the data set and you specify to be the left data set.

100
00:14:54,580 --> 00:15:05,170
And you're kind of adding the data from the the right data set for for all those IDs in the left data side, if it's in the right dataset,

101
00:15:05,170 --> 00:15:19,659
but not in the left dataset, you don't do any any type of merging probabilities for these right join is just basically the same thing as left join it.

102
00:15:19,660 --> 00:15:32,410
Oh, all right. So in this case, is X going to be like the left and of the right, or are we supposed to like oh yeah, x to be the left one.

103
00:15:32,410 --> 00:15:35,469
Yeah. Okay. So we don't have to like define it. Yeah.

104
00:15:35,470 --> 00:15:43,360
I think the function kind of automatically treats the X argument as the left is like yeah.

105
00:15:43,750 --> 00:15:47,079
Yeah, that's right. Yeah.

106
00:15:47,080 --> 00:15:58,660
I think right join is basically what is conceptually the same thing as just switching, which one really used to include all of the items.

107
00:16:00,520 --> 00:16:08,649
So let's see. So okay, so inner join is probably the first one we'll talk about.

108
00:16:08,650 --> 00:16:17,130
So. I guess this is supposed this at least this figure is supposed to illustrate it.

109
00:16:17,140 --> 00:16:24,150
So let's say we have two data sets and we think of these as kind of like the ideas of these colored numbers.

110
00:16:28,310 --> 00:16:40,320
And we have some X data and some Y data. Actually, we do an inner join you only included the rows for the for the items that are in both datasets.

111
00:16:40,320 --> 00:16:44,640
So the, the ideas that are in both datasets are one and two basically.

112
00:16:45,630 --> 00:16:51,300
Right. So like when you do this inner join, you're only going to have two rows.

113
00:16:51,780 --> 00:16:57,230
Keeps the x data from this data for entering the Y data from this.

114
00:16:58,560 --> 00:17:02,430
I think that's what was supposed to happen for this merge.

115
00:17:02,430 --> 00:17:09,360
So you can see here when we did this inner join, we can only have three observations, right?

116
00:17:09,360 --> 00:17:20,100
So if we go look back at the original data, so there's basically these three, one, two, three, four, five, seven, six and one, one, one.

117
00:17:20,760 --> 00:17:23,610
Those are the only identities in both data.

118
00:17:23,940 --> 00:17:35,069
So when we do the merge going is those three kind of the data from those three observation are those three IDs basically.

119
00:17:35,070 --> 00:17:39,480
So when we do the inner join, when we're going to have three rows.

120
00:17:40,940 --> 00:17:46,169
So that's in our joint you should think of an inner join is basically just the intersection.

121
00:17:46,170 --> 00:17:52,350
You're only keeping the IDs which are in the intersection of two data frames.

122
00:17:53,790 --> 00:17:58,299
Okay, so the way. Here's an example of full join.

123
00:17:58,300 --> 00:18:04,720
I mean, the first thing would.

124
00:18:12,660 --> 00:18:25,730
So is the first thing which is might be confusing about a full joint how do you fill in data that's not for IDs that are not present.

125
00:18:25,740 --> 00:18:39,750
So basically the way it does that it'll it'll fill it in with within a values so you can see here I guess we're doing a full join in it's

126
00:18:39,750 --> 00:18:51,239
like basically the union IDs in the merged data set are basically like the union of the IDs and the to the to the original dataframe.

127
00:18:51,240 --> 00:18:57,450
Sophie, look here at these two examples. We have IDs one, two, three and one, two, four.

128
00:18:57,450 --> 00:19:06,600
So when we do a full join, you want data for the union of these IDs which is one, two, three, four.

129
00:19:08,130 --> 00:19:19,170
However, we don't have the X data for I.D. number four and we don't have the we don't have the Y data for I.D. number three.

130
00:19:19,170 --> 00:19:30,770
That's what this is supposed to represent. This is like the data for only three and this is like the on the left is the data for i.t four.

131
00:19:31,620 --> 00:19:41,909
So the way it fills it in, when you do the merge, it it'll you'll still have four rows for all the for Chinese but it'll just have a missing for the

132
00:19:41,910 --> 00:19:51,690
x value and I-94 and then it'll have a missing value for the the Y data and ID for sorry ID three.

133
00:19:52,120 --> 00:19:59,519
And since we don't have any of the Y data from ID three, okay, so that's a full join.

134
00:19:59,520 --> 00:20:09,130
So it's basically takes all of the it's it's trying to create a data set that has data for all of the IDs and either of the datasets.

135
00:20:09,630 --> 00:20:23,540
Okay. So here's a it, here's a left. And so the way the way this works, it basically just includes all of the ideas from the lefties.

136
00:20:23,760 --> 00:20:28,620
So this is like the way we're setting this up is the left over.

137
00:20:28,710 --> 00:20:37,830
This is the less data set. So it's basically going to include all of the IP is from here in our merged dataset.

138
00:20:41,440 --> 00:20:45,270
It won't include IDs from here if it's if it's not in here.

139
00:20:45,360 --> 00:20:52,170
Okay. So like a for before it is in the right dataset.

140
00:20:54,060 --> 00:20:57,840
So it's but it's not in the left dataset, so it's not going to include it.

141
00:20:59,400 --> 00:21:05,880
Here we have what it looks like when we do the left join type of merge.

142
00:21:06,480 --> 00:21:10,830
So here we have all the ideas for the left data side, which is 1 to 3.

143
00:21:12,060 --> 00:21:25,520
We have all the X data, but we have one missing Y value because there's no y value here for for i.t three since 83 is not in the left dataset.

144
00:21:25,560 --> 00:21:37,290
So the way we you kind of do that with the left joint function from the default our so you see that you just do the left join when people.

145
00:21:42,540 --> 00:21:47,520
So it's kind of this data frame is the left data frame when it comes first.

146
00:21:48,300 --> 00:21:56,030
So we have it's basically going to take all of the ideas from this data from because

147
00:21:56,040 --> 00:22:00,740
the left showing it's not going to include anything that's in here but not in here.

148
00:22:00,750 --> 00:22:14,570
So this is not going to include the for or for for I.D. We do have a left, right and center and exam to be like the right data set.

149
00:22:16,260 --> 00:22:32,760
But you can see that there's no there's no for for for I to do this left join when we do the first data for the left data set.

150
00:22:34,410 --> 00:22:42,209
You can see that here we have we have one missing value since this is five, five, five, five,

151
00:22:42,210 --> 00:22:59,010
five that did not have a entry in the does not have an entry in the O and in the example, there's no five five island here.

152
00:22:59,610 --> 00:23:06,340
So it's just going to fill that in as a as a as a missing value with the which I don't.

153
00:23:08,250 --> 00:23:11,610
So here's the right of.

154
00:23:11,730 --> 00:23:24,400
So we have still a. So we have a joint venture, right?

155
00:23:25,880 --> 00:23:42,320
So it's doing a variety show. And actually, again, I would say so it's it's keeping all of the data from the from the exam data from.

156
00:23:46,050 --> 00:23:49,860
So we can see that here. The falling is keeping all of the data.

157
00:23:49,870 --> 00:23:56,730
When we do it right, join, it's keeping all of the items from here. And it's not including this.

158
00:24:00,690 --> 00:24:11,700
So it's including all of the items from the exam, but not the or not the ID from a pupil, at least not get it from people.

159
00:24:11,700 --> 00:24:24,959
That is not an exam. So you can see here, actually the exam is treated as the right thing to say that people write as people then come exam.

160
00:24:24,960 --> 00:24:29,460
It's kind of automatically treating the first entry as a less data set.

161
00:24:30,000 --> 00:24:37,290
And the second argument is the right dataset first for both the left join and the right to join the function.

162
00:24:37,500 --> 00:24:42,340
Okay. Okay.

163
00:24:44,620 --> 00:24:51,160
So here's just an extra feature I guess you can use sometimes in English, but.

164
00:24:56,360 --> 00:25:06,800
In some cases you have like a you have two data frames where you have like an ID variable that you want to merge on, but you also have.

165
00:25:09,100 --> 00:25:11,560
Two of other variables which have the same name.

166
00:25:15,040 --> 00:25:23,080
So in that case, well, our has some default that it does it gives it or gives it'll give it some alternative name.

167
00:25:23,620 --> 00:25:28,750
And so you can have two different columns of the same name in your merged dataframe.

168
00:25:29,530 --> 00:25:43,930
But there you can you can force it to have a kind of a more interpretable name, if you want, by using this kind of suffix argument.

169
00:25:44,620 --> 00:25:48,339
So you can see here, if we have like two different data frames,

170
00:25:48,340 --> 00:25:59,380
which we have where we have IDs and we also have a score of these kind of represent scores on like two different classes and we want to merge them.

171
00:26:01,910 --> 00:26:12,010
And so when we merge them by ID, we want to have two separate columns for the score in Python, in the score in our.

172
00:26:13,150 --> 00:26:20,400
But they can't have the same names by default if they are scored on X or scored on Y.

173
00:26:20,410 --> 00:26:27,910
But if you want them to have informative names, you can have this you can include this kind of suffix type of thing.

174
00:26:27,910 --> 00:26:35,260
And so if there's not a common variable name between the two data frames,

175
00:26:35,260 --> 00:26:46,720
it'll give one of them kind of had this extra series of characters at the end of that, that name.

176
00:26:47,000 --> 00:26:52,690
Okay. So it's giving the score that it takes from the exam python data frame.

177
00:26:52,720 --> 00:27:00,459
It's getting the suffix python from the exam, our data frame, it will use our suffix.

178
00:27:00,460 --> 00:27:05,800
So it's just something that might be useful to be aware of something.

179
00:27:06,130 --> 00:27:15,070
I mean, another way to do it, I might just redefine the variable names like on this, rename this score,

180
00:27:15,070 --> 00:27:23,440
underscore python and the score underscore are before merging since it's also an easy and easy way to do it as well.

181
00:27:25,840 --> 00:27:31,899
Oh yeah. This was just showing what happens if you don't include that kind of extra suffix

182
00:27:31,900 --> 00:27:36,879
argument and you have two data frames with a variables with the same name.

183
00:27:36,880 --> 00:27:40,780
So like here again, these are the same data frames.

184
00:27:41,290 --> 00:27:51,939
They both have a score variable. If we do like an inner join, it needs to be in frames without adding any extra suffix argument.

185
00:27:51,940 --> 00:27:57,969
It'll just label them as scored on X and scored on these, which is fine,

186
00:27:57,970 --> 00:28:06,460
but those variable names are not as informative as if we had like score underscore python and the.

187
00:28:11,490 --> 00:28:20,530
You know, score under squirrel. Okay. So here's kind of like I just said, I think it's a little exercise with the gap in mind or data set.

188
00:28:22,030 --> 00:28:28,389
Just maybe this is kind of a slightly more realistic example of of using these merging tools.

189
00:28:28,390 --> 00:28:34,810
So let's say we want to extract, remember the cat minor data,

190
00:28:34,810 --> 00:28:46,959
let's say we want to extract the data from the two years 1952 and 2007 and then merge the data across

191
00:28:46,960 --> 00:28:55,480
two years for kind of each country dropping countries where it kind of either year has missing data.

192
00:28:55,930 --> 00:29:04,330
Since some of these countries don't have data for all of years, especially some of the kind of smaller countries.

193
00:29:07,360 --> 00:29:20,830
We also recently mentioned that right at the end we will basically create a new variable as part of this exercise.

194
00:29:22,570 --> 00:29:31,030
So I guess the first thing we wanted to do is create two data frames, which is basically only for 1950 to 2007 data.

195
00:29:31,660 --> 00:29:38,739
So kind of after reading in the data, that readout could be on the way and you could do that on a deep fly.

196
00:29:38,740 --> 00:29:45,610
R is just using filter, right?

197
00:29:45,610 --> 00:29:49,600
Filter kind of data, basically do a subset.

198
00:29:50,470 --> 00:29:57,670
So this if you do filter, this will only keep the observations that happened 1952.

199
00:29:58,480 --> 00:30:00,520
So I don't remember if you mentioned this or not,

200
00:30:00,520 --> 00:30:12,970
if we do select in a minus that basically the data frame stays mostly the same, it just removes this, this variable.

201
00:30:13,090 --> 00:30:22,690
Okay, that's all it does. So it's going to create a data frame with only the 1952 observations and then it removes the year variables.

202
00:30:24,070 --> 00:30:28,870
Basically, you might want to do that just because you're here is not informative.

203
00:30:29,080 --> 00:30:36,610
Once with once you're done filtering to 1952, you already know that all of the observations are from 1952.

204
00:30:36,610 --> 00:30:47,270
So there's no point in having a year column. And also when emerging you don't really know.

205
00:30:48,460 --> 00:31:02,700
Okay, so once you've created these two separate data sets, if you just want to do a merging of what you can do in our joint side or not.

206
00:31:02,740 --> 00:31:05,760
So I see that at the beginning of.

207
00:31:10,400 --> 00:31:24,170
Anyway, the inner join basically it's always nice to keep the countries that are they've had data from both 1952 and 1927.

208
00:31:25,250 --> 00:31:34,070
That's all the joint is doing. This suffix is going to add well, you'll see what happens in the next slide or so.

209
00:31:35,030 --> 00:31:41,989
Basically, we have we have common variables for both the 1952 and the 2007 data,

210
00:31:41,990 --> 00:31:49,310
like we have GDP per capita for both 1952 and 2007 and they have the same variable names.

211
00:31:50,030 --> 00:32:00,530
So I'm just adding the suffix. So we look at the merged data so we can see which variable came from 1952 and which one came from 2007.

212
00:32:02,330 --> 00:32:06,830
So states should be what happens. All right.

213
00:32:07,070 --> 00:32:16,640
Okay. So this is basically what it. Looks like it's actually okay.

214
00:32:18,190 --> 00:32:25,390
So what, did we merge on the way? Well, we merged on all you could do, both country and continent.

215
00:32:28,300 --> 00:32:36,070
This might be slightly. I mean, continents. We didn't talk about it in detail.

216
00:32:37,030 --> 00:32:43,930
So I guess I did both country and continent, just so it doesn't repeat it twice in the merged data.

217
00:32:43,940 --> 00:32:47,350
French But it's it's basically merging on country.

218
00:32:47,800 --> 00:32:51,310
This is kind of I guess it's a theory.

219
00:32:52,170 --> 00:32:56,100
If they have a country match there, then a continent value matches as well.

220
00:32:56,110 --> 00:32:59,860
I guess the the only reason I did this because I didn't want to.

221
00:33:01,290 --> 00:33:08,740
I didn't want it to have like a duplicate duplicate continent variable in the merge dataframe.

222
00:33:09,490 --> 00:33:16,240
Okay. So we can see that a continent is not repeated for 1952 and 27,

223
00:33:16,930 --> 00:33:21,530
but basically you should think of this, it's doing the same as far as the merging goes.

224
00:33:21,560 --> 00:33:26,290
This is doing the same thing as just merging on the country variable.

225
00:33:26,620 --> 00:33:30,720
Okay. Okay.

226
00:33:30,730 --> 00:33:39,549
So we we've merged everything by country. And so for all of the other variables except for continent, it creates we're going to have two columns.

227
00:33:39,550 --> 00:33:42,520
We have one for the 1952 data.

228
00:33:43,270 --> 00:33:56,290
And we we have everything repeated for 27, like we have population in 1952 and population 27, life expectancy 1952, life expectancy 2007.

229
00:33:56,950 --> 00:34:02,050
And the GDP per capita in 1952. And GDP per capita 27.

230
00:34:04,730 --> 00:34:08,850
Translates loosely. Okay.

231
00:34:08,850 --> 00:34:12,729
So I think that was the main goal for like merging the data.

232
00:34:12,730 --> 00:34:22,440
And we just wanted to create two data frames, one from 1952 and 21 from 27 and just merge by, by country.

233
00:34:22,890 --> 00:34:31,130
Okay. So I think the other thing we wanted to do was create new variables like ah,

234
00:34:31,200 --> 00:34:40,490
a series of new variables in this merged data frame by looking at or that measure some kind of change from 27 to 2,

235
00:34:40,590 --> 00:34:51,660
1952, since we have both the 27 data and the 1952 data in the same data frame.

236
00:34:51,690 --> 00:35:01,330
It shouldn't it should be a little bit easier to to look at comparisons between 2007 and 1952.

237
00:35:01,350 --> 00:35:07,490
So the way you can do that is with mutate.

238
00:35:07,500 --> 00:35:19,360
That's, that's a deeply ah kind of. Function so you could use mutate in a as part of like a pipe chain,

239
00:35:19,360 --> 00:35:25,450
but you can also use it by itself since we've kind of already done all of the appropriate merging,

240
00:35:25,450 --> 00:35:29,290
it doesn't need to be combined with any other operation.

241
00:35:29,300 --> 00:35:35,620
So the way you use these deploy are functions by themselves.

242
00:35:36,160 --> 00:35:39,760
Remember, you just the first argument is just the name of the data frame.

243
00:35:39,810 --> 00:35:48,230
And after that the other arguments kind of are what do the operations that you're interested in?

244
00:35:48,230 --> 00:35:55,600
And so, so mutate is basically a thing that is the function that's used for defining new variables.

245
00:35:56,200 --> 00:36:08,170
Okay. So we can define a series of new variables based on the data and gap both years by just putting in a formula based on them on the variables.

246
00:36:11,360 --> 00:36:13,189
In the gap both years dataframe.

247
00:36:13,190 --> 00:36:26,480
So the first variable here is it's like life expectancy ratio it's just the life expectancy in 27 divided by the life expectancy in 2000 at 1952.

248
00:36:29,570 --> 00:36:43,250
Think that's basically. Yeah, the other ones are basically kind of the same concept be the ratio of certain variables between 27 and 1952.

249
00:36:44,330 --> 00:36:48,860
Okay. So this is what it looks like after we do this, after we use mutate.

250
00:36:48,870 --> 00:36:57,980
Now it has how many additional variables we have to should have six additional variables.

251
00:36:59,930 --> 00:37:05,120
So those are these six out here. One, two, three, four, five, six.

252
00:37:05,900 --> 00:37:12,980
So you have the life expectancy ratio, the population ratio, the GDP per capita ratio,

253
00:37:14,150 --> 00:37:22,220
the GDP for each of the two years and then the the GDP ratio or the total GDP ratio.

254
00:37:22,990 --> 00:37:28,490
Yeah. That's how you can use mutate to define new variables.

255
00:37:32,360 --> 00:37:42,650
Oh, I guess this is just a reminder of different things you can do with air operations and pipe change.

256
00:37:42,660 --> 00:37:49,940
If you wanted to do, if you wanted to like output a data frame where we only have two variables like

257
00:37:50,360 --> 00:37:58,700
country and life expectancy ratio and then sort from I guess largest to smallest.

258
00:37:59,480 --> 00:38:10,410
So basically that the country has the largest gain in life expectancy, at least in terms of the the ratio.

259
00:38:11,000 --> 00:38:25,220
I we do it this way. This is just creates a data frame with only these two columns and it's sorted by the life expectancy ratio between 27 and 1952.

260
00:38:29,480 --> 00:38:35,300
Obviously, the smallest, it's basically the reverse, just sort of from lowest to highest.

261
00:38:37,520 --> 00:38:46,820
Here is if we look at the GDP ratio from largest to smallest, I guess these are top ten.

262
00:38:47,160 --> 00:38:52,790
I guess that's what's yeah, that's about it.

263
00:38:52,790 --> 00:39:02,230
That's just kind of an exercise for kind of that you might be interested in for this type of thing.

264
00:39:02,930 --> 00:39:13,690
There's times you have two, maybe two data sets from representing data from two different years that emerged and then define new variables.

265
00:39:15,170 --> 00:39:22,630
That's. They can compare different aspects of the two data sets.

266
00:39:23,350 --> 00:39:28,190
Okay. So here's the. All right, so let's.

267
00:39:32,580 --> 00:39:35,580
So I think that's that's basically about it for merging.

268
00:39:35,580 --> 00:39:41,160
That's really just really the thing to note is the four types of joins.

269
00:39:42,070 --> 00:39:49,440
Inner join left join right join is fraudulent and he had left join and right join her

270
00:39:49,440 --> 00:39:55,410
kind of the same thing just depends on which one you're defining as the left or right.

271
00:39:56,970 --> 00:40:06,190
All right. So the next. The next topic is I guess I'll call it reshaping data.

272
00:40:06,190 --> 00:40:16,000
This is really at least what we'll talk about is really just converting between, I would call it two different data formats.

273
00:40:18,190 --> 00:40:26,169
It's often called tall versus wide data. So I think you'll often see data stored in these two different formats and it's good to

274
00:40:26,170 --> 00:40:33,580
know what they are and then how to convert the data between one format and the other.

275
00:40:33,730 --> 00:40:41,650
Okay. All right. So let's look at the let's remind ourselves what the gap in mind your dataset looks like.

276
00:40:42,970 --> 00:40:49,299
So here is just the basically just the what it looks like.

277
00:40:49,300 --> 00:40:55,390
If you look at a subset of the data fully for the years 1952 and 2007.

278
00:40:55,390 --> 00:41:00,760
So I guess what I want to point out here is that.

279
00:41:05,780 --> 00:41:10,340
We have kind of multiple rows of data for each country.

280
00:41:11,030 --> 00:41:16,760
If we're thinking of the country as the main unit of analysis,

281
00:41:17,450 --> 00:41:24,460
we have kind of multiple observations for each country, and this data center is multiple balloons.

282
00:41:26,090 --> 00:41:33,169
So we're thinking of the country as the main unit of analysis, as often called like repeated measures.

283
00:41:33,170 --> 00:41:39,260
Data just basically means that we have kind of repeated observations for the same country,

284
00:41:39,770 --> 00:41:45,320
at least we have kind of repeated observations the way it's represented from this dataset.

285
00:41:45,890 --> 00:41:56,310
So it's this is an example of. What's often called like tall or long format.

286
00:41:57,270 --> 00:42:03,210
Basically, the reason for that is if you look at the data set, it's high,

287
00:42:04,350 --> 00:42:11,130
it's kind of long and thin, or at least compared to other ways of storing the data.

288
00:42:11,550 --> 00:42:22,950
It'll be a lot taller, especially if we looked at the we looked at the full data set, you would have six or seven observations for each country.

289
00:42:23,250 --> 00:42:31,810
Okay. So this can be a lot. A lot sooner than the alternative, what we're going to talk about.

290
00:42:34,540 --> 00:42:38,530
Okay. So it's not this sheet map, really.

291
00:42:39,040 --> 00:42:45,640
It's totally clear, but kind of the way to think about it, we're basically going to have a data set that kind of looks like this.

292
00:42:46,510 --> 00:42:52,360
This is like a taller format and it's basically going to you're going to keep all of the same data.

293
00:42:52,360 --> 00:42:53,950
It's just a different way of storing it.

294
00:42:54,370 --> 00:43:01,750
You're kind of converting it to something that looks more like this, which is what we call like a wide format.

295
00:43:04,390 --> 00:43:09,760
Okay. So I just give an example of why swimming maybe I'll just say what it is.

296
00:43:10,330 --> 00:43:16,570
So why format it? You're basically going to have one row, you're going to have one row of data, four for each country.

297
00:43:16,900 --> 00:43:21,889
Okay. So let's say we have I don't know how many countries we had in a data set.

298
00:43:21,890 --> 00:43:29,040
If we have 100 countries in the dataset, we're going to have 100 rows in the data frame.

299
00:43:29,440 --> 00:43:37,600
Okay. That's the way it works. So we're going to do wide format and have like a single row for Afghanistan.

300
00:43:41,820 --> 00:43:46,920
And then the way it would be stored is that you have life.

301
00:43:47,220 --> 00:43:56,250
You have a column for life expectancy, 1952, and then another column for life expectancy 27.

302
00:43:56,520 --> 00:44:03,659
Okay. That's kind of the way it works. And you can see why it's why especially on most of the oceans,

303
00:44:03,660 --> 00:44:12,180
because you kind of you might have to have multiple columns for kind of one variable that's stored here.

304
00:44:12,870 --> 00:44:17,460
And leave the I think the examples will make it a little bit more clear.

305
00:44:18,180 --> 00:44:28,970
So there's if you're sticking with some of the packages that are similar to decline are.

306
00:44:29,550 --> 00:44:35,940
There's the spread function. I also talk about the reshape function, which is another way of doing it.

307
00:44:37,010 --> 00:44:48,750
Yes, they do the same thing. I think it's just a way of I think what I really prefer basically be a reshape function.

308
00:44:48,780 --> 00:44:56,040
I think the syntax might be a little bit more clear for some people that I mentioned both ways of doing it.

309
00:44:56,880 --> 00:45:12,420
So the spread function is as the it's from the tidy R package, it's basically one way of converting and dataframe from tall format into into Y format.

310
00:45:12,960 --> 00:45:18,960
Okay. So the the format for this is will get the function spread.

311
00:45:19,620 --> 00:45:22,290
And then you you give the name of the dataframe.

312
00:45:22,290 --> 00:45:33,030
So this is a data frame that's stored in tall format and then you give it something called the key and something that will call the value.

313
00:45:33,870 --> 00:45:34,860
So the key.

314
00:45:39,070 --> 00:45:49,600
Is the name of the variable which will become the column names in the line format so or will be used as part of the names and the wide format.

315
00:45:49,600 --> 00:45:55,420
So it's basically like the first that that helps for the mind or data.

316
00:45:55,670 --> 00:46:06,579
So it's going to be the thing that's going to be and this becomes part of the column names when we do my format.

317
00:46:06,580 --> 00:46:09,580
So this will go back.

318
00:46:12,280 --> 00:46:25,930
So we convert this to wide format. We want something. We're going to have two columns for a life expectancy, one for 1952 and one for 2007.

319
00:46:26,410 --> 00:46:29,410
Kind of a C is going to tell us when to.

320
00:46:32,220 --> 00:46:39,300
His name, those and and what and what and whether to repeat them or not, I guess.

321
00:46:42,290 --> 00:46:53,100
And then the value is the name of the variable whose values will be spread, I guess spread across across the different levels of the keyword.

322
00:46:54,500 --> 00:47:03,590
So this is the name of the variable who's going to take different values across the different columns in your life?

323
00:47:05,990 --> 00:47:10,580
Hopefully that maybe the hopefully example makes it more clear. So let's.

324
00:47:14,340 --> 00:47:18,899
Okay. So this is the an example.

325
00:47:18,900 --> 00:47:28,590
So let's spread the gap dot tall data data frame that we had earlier using here as a key

326
00:47:29,070 --> 00:47:38,340
and life expectancy as the about basically other than life expectancy kind of everything.

327
00:47:41,800 --> 00:47:49,480
Everything doesn't have multiple columns in the in the in the wide format.

328
00:47:49,870 --> 00:47:55,330
Okay. So in the wide format, only life expectancy has multiple columns.

329
00:47:55,630 --> 00:48:06,130
Okay. So you can see that here. So these are just the two different values of life expectancy for 1952 and 2000.

330
00:48:06,520 --> 00:48:13,140
So this is and this is basically the the wide format version of the the earlier data frames.

331
00:48:13,180 --> 00:48:17,890
You can see here there's only one there's one row for each column for each country.

332
00:48:31,970 --> 00:48:36,140
So I guess this is just this is not really related to the flu.

333
00:48:36,140 --> 00:48:53,150
This is said somewhat clearly this is the year is the variable you're going to see for distinguishing different values of life expectancy,

334
00:48:54,200 --> 00:49:00,860
which is the way to think of it. And those those become the column in the wide format.

335
00:49:04,910 --> 00:49:09,170
Now, this is just another example of using mutate.

336
00:49:09,170 --> 00:49:14,810
I guess you can then take the ratio of life expectancy between the two years.

337
00:49:20,960 --> 00:49:27,220
Okay, so let's I guess that's going from tall to wide.

338
00:49:27,230 --> 00:49:32,060
I guess you can go back from tall, from Y to tall by using gather.

339
00:49:32,870 --> 00:49:57,890
So the way you you do that is. So you're name some something.

340
00:50:05,320 --> 00:50:18,810
I'm just. You shouldn't go.

341
00:50:24,010 --> 00:50:36,310
Not sure why I wrote them. I will. Okay.

342
00:50:38,660 --> 00:50:42,860
I will just fix that. I'll just send that out later. You know, it's kind of about that.

343
00:50:43,700 --> 00:50:47,650
There's a there's a little bit of.

344
00:50:55,490 --> 00:51:01,010
Oh, I see. I did. I did it correctly. Right. I guess I should have sorted by.

345
00:51:01,010 --> 00:51:09,170
This is kind of sorted by year. It's probably better is sorted by by country.

346
00:51:09,930 --> 00:51:16,190
Certain things are one clear sort of how I will fix that.

347
00:51:20,920 --> 00:51:24,489
Let's hope instead of here.

348
00:51:24,490 --> 00:51:27,910
But anyway, this is kind of the syntax.

349
00:51:27,910 --> 00:51:37,510
We kind of gather the variables that going to change over time.

350
00:51:38,200 --> 00:51:46,960
So these are the input, the variables that change over time as kind of just starting as the second argument, I gather.

351
00:51:47,410 --> 00:51:52,270
And this is an underlying level of time which you basically year.

352
00:51:53,350 --> 00:52:01,410
These are kind of the variable name that you want to place.

353
00:52:02,470 --> 00:52:05,500
The thing that varies across columns.

354
00:52:05,800 --> 00:52:14,240
So this is probably it makes sense to put years in here. But anyway, this is not a probably you probably better fix this first.

355
00:52:14,260 --> 00:52:18,910
I think this is sorted by year, so it's not clear as to what's going on.

356
00:52:20,860 --> 00:52:26,170
But anyway, I gather is used to convert from wide back to tall.

357
00:52:27,820 --> 00:52:33,969
So let's just, I think look at one other example and then I'll also use this example to talk about the reshape function,

358
00:52:33,970 --> 00:52:42,760
which in some ways I don't know, I think some people might prefer the syntax for the reshape function rather than spread versus gather.

359
00:52:44,110 --> 00:52:49,150
So here's just another example data set. This is from the faraway package.

360
00:52:51,790 --> 00:53:02,400
This is probably data set and just adding the difference in extra variables just

361
00:53:02,410 --> 00:53:07,260
to denote the and this is clear in the original data frame that this is a visit.

362
00:53:07,270 --> 00:53:16,030
So these we have multiple observations from the same idea that denotes like different visits.

363
00:53:16,900 --> 00:53:22,000
I think just to pulse are hospital visits or something like that.

364
00:53:22,730 --> 00:53:25,990
Okay. So this is actually an example of a tall data set.

365
00:53:26,470 --> 00:53:30,460
We're thinking of the unit of like analysis as the individual person.

366
00:53:31,000 --> 00:53:37,930
This is an example of a tall data set. So we have you can see we have multiple rows for the same ID.

367
00:53:38,650 --> 00:53:43,000
Okay. The first five roses are from the same person.

368
00:53:44,860 --> 00:53:52,420
So this is taller data set. We can convert this into a wide format.

369
00:53:54,040 --> 00:53:58,540
We're going to convert this into wide format. Only looking at I mean, this is weight and height.

370
00:53:59,370 --> 00:54:08,860
I think we're we're going to I mean, you could do a lot more observations, but it's too wide to print.

371
00:54:09,490 --> 00:54:16,390
So we're just going to look at the weight and height when we convert this into wide format.

372
00:54:19,600 --> 00:54:23,480
Okay. So the way you can do like multiple columns,

373
00:54:23,500 --> 00:54:37,960
I think spread works for a single column in a tall data set that you want to spread out into a wide format if you want to do kind of multiple columns.

374
00:54:42,170 --> 00:54:47,840
You can use a pivot wider. This is actually, I think only available from tiny R.

375
00:54:49,880 --> 00:54:53,290
So here the way that works is.

376
00:54:54,910 --> 00:55:00,020
Well, first of all, to point out that we're only first of all on keeping the variables.

377
00:55:00,020 --> 00:55:03,110
I'd wait. Hi. Entries. Okay.

378
00:55:05,750 --> 00:55:14,809
So you should think of the visit variable as acting kind of as the same way as here in the gap my data frame since it's it's kind of a thing

379
00:55:14,810 --> 00:55:27,710
that varies over time within the same individual is it plotted in this data set and it plays the same role as you're in the get my hands.

380
00:55:28,940 --> 00:55:33,060
So I want to convert this to the world wide format.

381
00:55:33,080 --> 00:55:44,500
Basically, we want to have in a wide format, we want to have a weight for each visit and a height for each visit is basically.

382
00:55:45,560 --> 00:55:52,370
And I think in this dataset, everybody has every person in the data frame has five visits.

383
00:55:54,930 --> 00:56:02,640
You know, except for a little bit of missing data here and there. Everybody's supposed to have five of visits.

384
00:56:03,360 --> 00:56:09,860
So when we convert it into a wide format, we have to have only one row for each person.

385
00:56:09,860 --> 00:56:18,150
And you can see that here. The first row has ID 12 0011, second row has 12 0012.

386
00:56:18,630 --> 00:56:28,680
So every row has a different I.D. and then for each we're going to have five columns for height and then five columns for weight is what we have.

387
00:56:30,810 --> 00:56:36,120
So this is basically what it does.

388
00:56:36,120 --> 00:56:39,600
We have five columns, four height, five columns, four weight.

389
00:56:39,610 --> 00:56:49,169
So height underscore one despite the height of the first visit by two is the second visit, etc. just a little bit annoying.

390
00:56:49,170 --> 00:56:56,909
This is at least in terms of slides, this is on the tip of the top about.

391
00:56:56,910 --> 00:57:04,559
It's just a tiny R kind of converts everything into a table automatically.

392
00:57:04,560 --> 00:57:13,050
It's basically a data image. It's it does everything that a dataframe does, which is sense except extra features.

393
00:57:14,100 --> 00:57:20,399
We can do everything, just about everything that you can do with the data frame.

394
00:57:20,400 --> 00:57:28,570
You could do it in a row in the tables, doesn't work. You just think you going to think of this as a data difference.

395
00:57:29,430 --> 00:57:36,390
Okay, so this so if we have multiple columns, you can use pivot, slider, okay?

396
00:57:38,340 --> 00:57:43,200
So the way that works, you use the names from the visit.

397
00:57:43,200 --> 00:57:48,020
So visit is kind of like plays the role kind of as the key in.

398
00:57:48,320 --> 00:57:55,950
And so it's the thing that you're going to take the names from to distinguish the columns.

399
00:57:56,400 --> 00:58:02,280
It's actually out of, I guess, the variables that you're spreading in the live format.

400
00:58:02,280 --> 00:58:08,580
So we're spreading both height and weight across different columns in a wide format.

401
00:58:08,940 --> 00:58:13,650
And the kind of the names that we attach are determined by the visit.

402
00:58:14,160 --> 00:58:18,270
So you can see visit here has one, two columns, one, two, three, four, five.

403
00:58:18,990 --> 00:58:26,940
So the visit is kind of what distinguishes the columns.

404
00:58:27,150 --> 00:58:33,630
Go ahead. I think I tried running this code and it says when I tried to do the pivot lighter, it says object not found for visit.

405
00:58:34,470 --> 00:58:37,710
It says object present, not felt right. We did you do this.

406
00:58:37,800 --> 00:58:43,520
What did I did? Close enough.

407
00:58:44,120 --> 00:58:57,790
What I can do is go. Is it not found because you chose to install this first or as far away?

408
00:59:02,110 --> 00:59:06,150
It was in stores and stuff.

409
00:59:13,670 --> 00:59:31,070
Move to a basic package or something. It seems like this where you're going and I'll check in like 10 seconds.

410
00:59:32,510 --> 00:59:36,500
Oh, this were basically done with this section anyway.

411
00:59:37,310 --> 00:59:41,270
Oh, I appreciate. Oh, yeah. I had a reshape example.

412
00:59:42,800 --> 00:59:46,430
I'll check back after we do the reshape slide.

413
00:59:48,020 --> 00:59:53,870
So you can do a reshape. I think some people prefer the subtitling.

414
00:59:53,900 --> 00:59:56,870
I think in some cases they actually prefer the syntax as well.

415
00:59:59,840 --> 01:00:11,629
So the way reshape works is you have to you have to give it kind of explicitly the name of a variable that kind of uniquely identifies

416
01:00:11,630 --> 01:00:27,710
the unit of analysis on calling it like basically like the country and got in mind your I.D. in I.D. and in the poly data set.

417
01:00:28,400 --> 01:00:34,219
And then I guess the thing doesn't necessarily have to be tied into exactly.

418
01:00:34,220 --> 01:00:41,660
But kind of the thing that distinguishes the observations within like the same idea,

419
01:00:41,830 --> 01:00:49,090
like, like the visit is what is that role in the poly like for the same I.D. visitors,

420
01:00:49,100 --> 01:00:59,210
which are really distinguishing the observations, which is often something that varies across time, like we visit one five for the same indecision.

421
01:00:59,840 --> 01:01:03,710
So you have to specify that and then you give it like the V,

422
01:01:04,100 --> 01:01:12,650
you give it the names of the variables that you want it to kind of spread out over or that you want it to include,

423
01:01:12,830 --> 01:01:17,989
you want to include in the wide form. And I guess you have to include direction line.

424
01:01:17,990 --> 01:01:26,000
I think there's some other things that Reshape does. So you can see here, it basically converts.

425
01:01:26,620 --> 01:01:32,239
Basically it's the same. Now the same thing that we did before.

426
01:01:32,240 --> 01:01:39,650
I guess it just has got to be five verses underscore, but it's kind of more or less the same thing.

427
01:01:41,540 --> 01:01:49,639
So I think some people like the syntax better to kind of explicitly give us an idea and we have in

428
01:01:49,640 --> 01:01:57,980
long format and the kind of the variables that distinguishes the observations within each idea,

429
01:01:58,160 --> 01:02:01,790
which is usually something that varies over time.

430
01:02:02,760 --> 01:02:07,180
Okay, that's the last thing.

431
01:02:07,200 --> 01:02:12,590
But let me see the states.

432
01:02:12,710 --> 01:02:19,140
It may not. Do you think?

433
01:02:22,150 --> 01:02:25,330
Sorry I didn't work. I don't know. Did it work? Did you try it again?

434
01:02:28,440 --> 01:02:45,700
I don't know if I'll check it later, but it should have worked at some point because I mean, I generated these slides using the actual article, so.

435
01:02:47,560 --> 01:02:52,719
Should work in some cases. Okay.

436
01:02:52,720 --> 01:02:57,520
I think that's that was it for.

437
01:02:59,410 --> 01:03:11,990
Yeah. That's it for. Reshape.

438
01:03:12,110 --> 01:03:27,410
Maybe I'll just say that I'm not ready to quit, so I don't. There's no question on the spread or gather or reshape because it's 100 and, you know.

439
01:03:29,990 --> 01:03:43,060
Yeah. No, I'm sure in the question on spread, gather or reshape, we study everything up it, but definitely no data visualization.

440
01:03:43,070 --> 01:03:56,240
Okay. I mean, data visualization is important, but I don't think we have enough time to include it on the quiz questions or come so far as we.

441
01:03:59,930 --> 01:04:07,459
So if not, we'll just we'll just do at least 15, 15 minutes of this or so.

442
01:04:07,460 --> 01:04:12,170
But it's, you know, data visualization types of questions on the quiz.

443
01:04:15,290 --> 01:04:21,620
So, yeah, let's just start out. I mean, it will hopefully help a little bit about G, g plot to later.

444
01:04:22,640 --> 01:04:31,200
But if you could just if you want to just start making graphs in R, I think it's just too easy to start the plot function problem,

445
01:04:31,640 --> 01:04:37,700
basic function for generating figures in our at least that when you're starting out.

446
01:04:41,120 --> 01:04:45,529
So I guess the most basic type of failure is just a scatterplot.

447
01:04:45,530 --> 01:04:53,600
It's just a point you're plotting points for Y versus X for for kind of two factors of data.

448
01:04:54,440 --> 01:04:58,610
And the way you do that with plot is just plot x, comma y.

449
01:04:59,310 --> 01:05:03,830
I should think of X's like just a vector, a numeric vector and y.

450
01:05:04,280 --> 01:05:10,520
I said numeric vectors may have the same the same length, x and Y have the same length.

451
01:05:11,060 --> 01:05:20,390
And you're just plotting kind of the coordinates of like x or x communication, kind of the K and your vector.

452
01:05:20,450 --> 01:05:29,659
So if you just have two vectors of like three and you just plot X and Y, it'll look like this.

453
01:05:29,660 --> 01:05:31,940
There's just you can apply these three points.

454
01:05:32,570 --> 01:05:46,790
So we have the first coordinate is one, two it plots that second coordinate is two, three and then the next coordinate is 43.034734.

455
01:05:47,540 --> 01:05:50,690
That's basically how a plot works.

456
01:05:50,750 --> 01:05:59,479
You can just plot points that you give it. This is probably a more realistic example of using plot.

457
01:05:59,480 --> 01:06:03,920
So if we think of the got minor data set,

458
01:06:03,920 --> 01:06:17,750
these these two variables are the you can think of these two variables from the data for images, vectors or numeric data.

459
01:06:18,260 --> 01:06:26,270
So we just plot life expectancy versus versus GDP per capita.

460
01:06:27,020 --> 01:06:32,750
We're just going to have life expectancy on the Y axis and GDP per capita on the X axis.

461
01:06:32,900 --> 01:06:38,180
Okay. So this is kind of what it looks like. It's just a scatterplot of those two variables.

462
01:06:40,580 --> 01:06:46,030
So I'll just go through different things you can kind of add to make this plot look nicer.

463
01:06:46,040 --> 01:06:54,139
So this looks okay, but it's not really like the X label is not probably what you would want it to be.

464
01:06:54,140 --> 01:06:57,140
If you're publishing this for yourself.

465
01:06:57,770 --> 01:07:06,890
The way you add a label to the x axis is just this argument.

466
01:07:07,100 --> 01:07:17,050
You just set it equal to something, something inside of quote that you want, quotes that you want to appear on the x axis, right?

467
01:07:17,360 --> 01:07:23,540
And this is why labs are okay so that I'll show up here and get you to give it a title if you want.

468
01:07:24,050 --> 01:07:30,170
So main equals something is the way you give a title to your figure if you're using plot.

469
01:07:34,220 --> 01:07:37,790
Okay. So I just I'm just kind of showing extra features that you can add.

470
01:07:38,240 --> 01:07:42,920
So if you have this plot here, you might want to add like a regression line to it.

471
01:07:43,520 --> 01:07:50,540
That's just the best fitting lines with a regression line or just basically going to have a intersected slope for your

472
01:07:50,540 --> 01:07:58,700
line using kind of the the points that you have from your your scatter plot to estimate that intercept and slope.

473
01:08:00,260 --> 01:08:05,420
So the way you can do that in R is this line function which stands for linear model.

474
01:08:09,470 --> 01:08:15,050
So if we want to like get a regression line between GDP per capita and our service

475
01:08:15,200 --> 01:08:21,559
life expectancy and GDP per capita where kind of life expectancy is the Y variable,

476
01:08:21,560 --> 01:08:24,560
you can use elm of life expectancy.

477
01:08:25,920 --> 01:08:31,700
So the GDP per capita. And the second argument is the data frame.

478
01:08:31,700 --> 01:08:36,830
The data is data equals and then we give it the name of the data frame that you're using.

479
01:08:37,940 --> 01:08:44,990
And actually what this rate this actually returns a bunch of other function.

480
01:08:45,530 --> 01:08:49,370
So Elm Gap is actually a list when you look at it.

481
01:08:53,210 --> 01:09:01,820
With a bunch of different components. The thing that you're interested in is really the coefficients element of this list.

482
01:09:02,840 --> 01:09:05,930
If you're looking for the best fitting regression line.

483
01:09:06,470 --> 01:09:18,410
So if we look at 11 gap line coefficients, that's going to give us the intercept and the slope of the intercept in the slope.

484
01:09:20,720 --> 01:09:27,370
Oh, yeah. I guess this is just showing an alternative way of doing the same thing.

485
01:09:27,380 --> 01:09:33,260
You can actually give the input of the the other function.

486
01:09:33,260 --> 01:09:36,230
You can just give it the actual vectors itself,

487
01:09:36,590 --> 01:09:42,830
rather than just giving the names of the variables and using it to the name of the data frame as the second argument.

488
01:09:44,300 --> 01:09:49,640
It's just it does the same thing. It's just two different ways of of it's.

489
01:09:55,480 --> 01:10:07,190
Estimating the same regression. Okay, so I just plotted the same scatter plot of data and a did that show up before.

490
01:10:07,210 --> 01:10:15,050
I don't think I. The underlying theme of.

491
01:10:17,390 --> 01:10:20,510
Oh, I guess this is the gap. This is the this is the complete gap.

492
01:10:20,510 --> 01:10:29,180
Minor data set. So what I did here is the I'm going to do the scatterplot for only the 1952 data.

493
01:10:30,320 --> 01:10:36,820
That was a regression. Okay. Okay.

494
01:10:36,830 --> 01:10:42,860
So I plotted it. I did the same scatterplot with plot and I added a regression line.

495
01:10:42,860 --> 01:10:47,900
The way to do that is with this function a B line, you give it like that.

496
01:10:49,450 --> 01:10:54,320
The first argument is like an intercept in the second arguments, like a slope and just plots it.

497
01:10:55,040 --> 01:11:00,350
It just plots a straight line with that intercept and slope.

498
01:11:00,830 --> 01:11:01,670
So you can see that here.

499
01:11:01,670 --> 01:11:11,510
I would say this looks kind of terrible, terrible at least the way you usually imagine, like a scatterplot and in a straight line going through it.

500
01:11:12,050 --> 01:11:15,990
So I think the reason for that is kind of this one way out here.

501
01:11:16,020 --> 01:11:22,370
This is just amazing. It's real. But I think this is this is pretty extreme.

502
01:11:22,820 --> 01:11:27,590
So I think that's that's where it's really messing up the regression.

503
01:11:27,710 --> 01:11:38,630
Look, this point here is really messing up with the collection estimate for the slope and and the intercept.

504
01:11:41,600 --> 01:11:44,900
So you can try to identify which observation that is.

505
01:11:45,780 --> 01:11:54,349
I did that here. There's the which I have done like which with that you could have done it another

506
01:11:54,350 --> 01:11:58,459
way without knowing the which dot max function that you could have done,

507
01:11:58,460 --> 01:12:01,650
which is of course the maximum that.

508
01:12:01,670 --> 01:12:04,670
But the easier way is just which dot max.

509
01:12:07,040 --> 01:12:15,680
So which dot max just returns the index of the observation that equals the maximum value of GDP per capita,

510
01:12:16,640 --> 01:12:22,640
so that returns the index of the observation which equals the largest GDP per capita.

511
01:12:22,640 --> 01:12:31,730
And then if you get 1952 off of the index here and then come up with nothing after that,

512
01:12:31,760 --> 01:12:44,000
just that's just going to print out the row of the data frame for that index so that you can see that, yeah, this is the coming from Kuwait data.

513
01:12:44,120 --> 01:12:54,290
Okay. So this has an extremely large student overdraft, at least compared to all of the other countries.

514
01:12:56,390 --> 01:13:02,870
I mean, I don't know. It might be an error or it might be real, but I removed it anyway.

515
01:13:06,170 --> 01:13:12,440
Just because it's kind of if you're doing a regression line, I just kind of messes everything up.

516
01:13:12,590 --> 01:13:12,960
Okay.

517
01:13:13,730 --> 01:13:25,970
So to remove the observation from a data frame, it could just be the minus of the the that index reasoning for putting it as the, the first argument.

518
01:13:26,100 --> 01:13:33,739
That'll remove that row. I can see that this is the same I guess it's sub 72.

519
01:13:33,740 --> 01:13:37,470
So this which dot max returns the number 72.

520
01:13:37,490 --> 01:13:41,810
So this is the same thing as just doing -72.

521
01:13:41,990 --> 01:13:53,660
Okay. So I just redefined the gap in the 1952 data frame as the previous data frame was just that observation, 72 removed.

522
01:13:53,960 --> 01:13:59,900
Okay. So then I can I refit the linear regression.

523
01:14:00,590 --> 01:14:03,950
So we're going to get new slope and intercept. And then I.

524
01:14:03,950 --> 01:14:15,500
Can I plot it? Okay. Okay. So this is just the same plot with the a b line function, but we've just done everything with that.

525
01:14:15,800 --> 01:14:17,540
The Kuwait observation removed.

526
01:14:18,050 --> 01:14:26,840
So here this kind of looks a lot more reasonable and it's kind of more of what you typically expect with a scatter plot and and aligned to it.

527
01:14:27,470 --> 01:14:32,940
Okay. So how are we? One time.

528
01:14:32,950 --> 01:14:35,109
So I'll just mention I think this quickly.

529
01:14:35,110 --> 01:14:43,510
I think there's one example in a final assignment where you are asked to generate a histogram or a couple of the grounds.

530
01:14:45,250 --> 01:14:50,230
So a histogram I guess is a common way of visualizing the distribution of a numeric variable.

531
01:14:50,980 --> 01:14:57,430
It's basically the way it works is you have a bunch of kind of relatively small intervals that you

532
01:14:57,430 --> 01:15:05,290
choose and you count the number of kind of observations that all and each one of those intervals.

533
01:15:05,290 --> 01:15:14,080
And then you just plot those, those counts, those count numbers in the same way you would for like a bar chart.

534
01:15:14,680 --> 01:15:17,780
Okay. That's kind of the way a histogram works.

535
01:15:18,880 --> 01:15:26,620
The way you would plot that in ours is the highest function. And you can give the input to that to the highest function.

536
01:15:27,520 --> 01:15:31,650
You can just give it a vector of numeric data is actually all you have to do.

537
01:15:31,870 --> 01:15:39,909
There are other options, other arguments that you can use to make the histogram look a little different.

538
01:15:39,910 --> 01:15:45,880
But just to get a kind of a basic histogram, all you have to give it is like a vector of numeric data.

539
01:15:46,870 --> 01:15:55,660
So we can see that here. If we look at the life expectancy data from the 1952 gap minor data set,

540
01:15:58,930 --> 01:16:07,870
all I have to do is just give it this data from that variable with HIST and it'll give us a it'll give us the output of a histogram.

541
01:16:08,590 --> 01:16:12,250
So far on the y axis is the frequency they call it.

542
01:16:12,910 --> 01:16:17,200
It's just the number of observations that fall into that kind of interval.

543
01:16:17,200 --> 01:16:21,849
So here I guess that's about 25 or 26.

544
01:16:21,850 --> 01:16:27,820
We have 26 observations that followed between 40 and 45.

545
01:16:28,480 --> 01:16:34,450
We have this is this is like 12 that fallen between 30 and 35.

546
01:16:35,020 --> 01:16:52,300
That's the way to interpret this same kind of extra arguments that you use with plot to do add different x axis labels or titles, ex-slave or man.

547
01:16:57,340 --> 01:17:01,690
Let's see what we do here. Oh yeah.

548
01:17:01,830 --> 01:17:07,959
It doesn't look that much different. I guess it's the the default is a little bit of a different color.

549
01:17:07,960 --> 01:17:18,250
Gray, but if you can change the color that it fills in these bars with, if you do this call argument arguments, I guess this is the basic gray.

550
01:17:18,250 --> 01:17:23,410
It's a little bit darker than the previous one and it's something else you can add.

551
01:17:24,040 --> 01:17:33,969
I think breaks is the argument that basically controls like the width of these intervals that you're computing the counts for.

552
01:17:33,970 --> 01:17:39,790
So if it breaks equals 30 settings in order to compute basically.

553
01:17:41,720 --> 01:17:53,570
Roughly 30 individuals. I think it depends on how it can divide things like it's going to try to do a roughly 30 intervals for this header histograms.

554
01:17:53,570 --> 01:18:05,059
Okay. So if you want your histogram to be a little bit more in order order intervals to I guess like finer resolution,

555
01:18:05,060 --> 01:18:15,860
you can increase the breaks, you'll see oh breaks equals five, it's a lot wider.

556
01:18:20,600 --> 01:18:29,780
Oh it's, that's about infinity, I guess this was the other thing some cases you want to do like a normalized this thing around

557
01:18:29,780 --> 01:18:38,810
that this basically means that the the area underneath the histogram is is equal to one.

558
01:18:38,870 --> 01:18:46,250
Okay. In that case, it's it has kind of the interpretation is more like a probability distribution.

559
01:18:47,310 --> 01:18:53,210
The appearance would be the same. Adjusted like axis is scaled differently.

560
01:18:53,450 --> 01:18:58,220
Okay. So for that, you just add this probability distribution.

561
01:18:59,460 --> 01:19:03,130
So you can see here that histogram looks basically the same.

562
01:19:03,260 --> 01:19:10,610
The, the, the y axis is a lot different since like .02.4.

563
01:19:12,080 --> 01:19:17,059
That's okay. Maybe we'll just know.

564
01:19:17,060 --> 01:19:24,970
We'll just stop there. Do enough to check if the close one showed up trying to figure out later.

565
01:19:26,000 --> 01:19:38,720
Yeah. You know, I think you said the library was messed up, so I don't care just because I'm like, okay, it doesn't show.

566
01:19:39,290 --> 01:19:51,980
Yeah, like, we have to stop fast, fast after practice, right?

567
01:19:53,660 --> 01:19:57,690
You're you're looking at. Right. At least the middle classes start.

568
01:19:57,740 --> 01:20:02,580
I don't know. You're.

569
01:20:07,020 --> 01:20:12,570
You really are putting the U.S. on my list.

570
01:20:13,930 --> 01:20:17,120
Oh, yeah. Yeah.

571
01:20:17,150 --> 01:20:21,360
Okay. Oh, I see nothing.

572
01:20:23,130 --> 01:20:38,680
I mean, I know I got to battery last night, so I was actually worried about the school battle.

573
01:20:38,680 --> 01:20:43,120
But, yeah, there's a lot of material in the community here.

574
01:20:46,850 --> 01:20:50,280
Oh, you guys. Oh, yeah.

575
01:20:50,380 --> 01:20:53,110
I mean, we must get this.

