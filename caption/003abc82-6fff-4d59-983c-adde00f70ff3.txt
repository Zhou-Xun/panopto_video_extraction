1
00:00:15,340 --> 00:00:17,450
She just, yeah that was her response.

2
00:00:17,450 --> 00:00:22,240
It's just that you idea.

3
00:00:22,240 --> 00:00:23,531
I'll talk to you.

4
00:00:24,720 --> 00:00:28,001
She said that, it's a little on half semester classes.

5
00:00:39,850 --> 00:00:43,300
>> Okay, it is 10 AM, so

6
00:00:43,300 --> 00:00:47,630
we're going to get started.

7
00:00:47,630 --> 00:00:52,574
This is introduction part two to lab three because there's some parts

8
00:00:52,574 --> 00:00:55,441
that we just couldn't cover last time.

9
00:00:56,450 --> 00:01:00,068
So just a reminder of where we are lab one is graphical analysis,

10
00:01:00,068 --> 00:01:02,154
lab two is descriptive statistics.

11
00:01:02,154 --> 00:01:03,543
We calculated table one,

12
00:01:03,543 --> 00:01:07,410
we're working on bivariate analysis with simple linear regressions.

13
00:01:07,410 --> 00:01:11,540
And we're gonna work our way up to multivariate analysis.

14
00:01:11,540 --> 00:01:16,820
So we're only looking at one X on our exposure.

15
00:01:16,820 --> 00:01:19,290
We aren't looking at confounding yet.

16
00:01:19,290 --> 00:01:25,557
We aren't looking at adjusting for multiple potential confounding variables.

17
00:01:25,557 --> 00:01:32,794
Yeah, so when we introduced lab two, we talked about the snack choice.

18
00:01:32,794 --> 00:01:36,616
That snack study where they took a bunch of undergrads, who went to the gym,

19
00:01:36,616 --> 00:01:40,470
offered them a snack either before their exercise or after their exercise.

20
00:01:40,470 --> 00:01:46,580
So the exposure was before or after.

21
00:01:46,580 --> 00:01:50,719
And then the responsive interest was if they chose a healthy snack or

22
00:01:50,719 --> 00:01:52,036
an unhealthy snack.

23
00:01:52,036 --> 00:01:56,055
And when we talked about the advance table one, one of the questions was.

24
00:01:56,055 --> 00:01:57,045
Okay, well,

25
00:01:57,045 --> 00:02:02,573
if I put everyone who got the before the workout snack in that corner of the room.

26
00:02:02,573 --> 00:02:06,263
And everyone who got the after workout snack,

27
00:02:06,263 --> 00:02:09,485
would those two groups look different.

28
00:02:09,485 --> 00:02:14,972
And it's kind of hard to say and look at this table and say, okay well,

29
00:02:14,972 --> 00:02:20,571
75% used resistance exercise and 74.5% did before and after.

30
00:02:20,571 --> 00:02:22,570
What do those percentages look like?

31
00:02:22,570 --> 00:02:24,490
Are they actually different?

32
00:02:24,490 --> 00:02:27,920
So it's a question of are my exposures and

33
00:02:27,920 --> 00:02:32,740
the way that I randomize is this actually going to affect?

34
00:02:32,740 --> 00:02:34,670
Are they different groups of people?

35
00:02:34,670 --> 00:02:36,630
Does that make sense?

36
00:02:36,630 --> 00:02:41,437
So we're trying to decide if there's any confounding based

37
00:02:41,437 --> 00:02:45,403
on if our subgroups in our sample look different.

38
00:02:45,403 --> 00:02:50,597
Okay, but we can actually perform inferential analysis to try

39
00:02:50,597 --> 00:02:57,460
to figure out and see if these groups are statistically significantly different.

40
00:02:57,460 --> 00:03:03,011
So table one was trying to formalize the exploratory analysis we did into

41
00:03:03,011 --> 00:03:08,380
the advanced part of lab three, is mathematically and statistically

42
00:03:08,380 --> 00:03:14,113
formalizing any type of difference you might see based on your quartiles.

43
00:03:14,113 --> 00:03:19,617
Okay, so there's a p-value that is calculated and

44
00:03:19,617 --> 00:03:22,629
so it's a hypothesis test.

45
00:03:22,629 --> 00:03:27,641
That the before and after conditions are the same or they're different.

46
00:03:28,860 --> 00:03:34,300
Okay, what test do you guys think that the authors were running?

47
00:03:34,300 --> 00:03:35,320
Yeah, Mirah.

48
00:03:35,320 --> 00:03:37,336
>> The parity test.

49
00:03:37,336 --> 00:03:39,700
>> Parity test, yeah.

50
00:03:39,700 --> 00:03:41,610
Close the right idea.

51
00:03:41,610 --> 00:03:47,550
So we have some numerical data and we have some categorical data.

52
00:03:47,550 --> 00:03:49,267
If you have categorical data,

53
00:03:49,267 --> 00:03:52,650
you're saying okay these people did resistance exercise.

54
00:03:52,650 --> 00:03:54,870
These people did not do resistance exercise.

55
00:03:54,870 --> 00:03:57,148
Are the two groups the same.

56
00:03:57,148 --> 00:03:59,159
What test does that make you think of?

57
00:03:59,159 --> 00:04:01,931
>> [INAUDIBLE].

58
00:04:01,931 --> 00:04:05,088
>> Not quite.

59
00:04:05,088 --> 00:04:06,277
>> Chi squared?

60
00:04:06,277 --> 00:04:09,901
>> Chi squared, we're testing for homogeneity across the groups.

61
00:04:10,910 --> 00:04:15,345
Okay, and what if we have BMI, we have a numerical and

62
00:04:15,345 --> 00:04:20,778
we're testing for homogeneity across the numerical variable?

63
00:04:20,778 --> 00:04:23,351
What other tests do we know about?

64
00:04:23,351 --> 00:04:24,215
>> ANOVA?

65
00:04:24,215 --> 00:04:27,786
>> ANOVA, good.

66
00:04:27,786 --> 00:04:32,830
Okay, so these P values are not significant.

67
00:04:32,830 --> 00:04:33,722
What does that mean?

68
00:04:38,263 --> 00:04:43,199
So what's the null hypothesis here, yeah?

69
00:04:43,199 --> 00:04:47,607
Yes, the null hypothesis is that there's no difference between our groups.

70
00:04:47,607 --> 00:04:52,380
That any variation we see is just due to random variation in our population.

71
00:04:52,380 --> 00:04:54,095
What's the alternative hypothesis?

72
00:04:54,095 --> 00:04:55,457
>> That there is a difference?

73
00:04:55,457 --> 00:04:57,030
That there's a difference, good.

74
00:04:57,030 --> 00:04:59,650
Okay, so these P values are not significant.

75
00:04:59,650 --> 00:05:02,343
What does that mean in terms of a hypothesis test?

76
00:05:02,343 --> 00:05:07,835
Just any of hypothesis test you find your P value is not significant, Mirah?

77
00:05:07,835 --> 00:05:09,750
>> I think we fail to reject the nodes.

78
00:05:09,750 --> 00:05:12,360
>> Yeah, we fail to reject the nodes,

79
00:05:12,360 --> 00:05:16,541
there's no evidence that our groups are not different.

80
00:05:17,980 --> 00:05:23,667
Okay, is that good or bad in the context of this study?

81
00:05:23,667 --> 00:05:26,935
>> That we see the difference between before and

82
00:05:26,935 --> 00:05:31,839
after and to each other and is not necessarily the real difference.

83
00:05:31,839 --> 00:05:37,480
>> So your reasoning is correct but the conclusion it's not quite, it's good.

84
00:05:37,480 --> 00:05:42,461
We don't wanna have our exposure like these other variables.

85
00:05:42,461 --> 00:05:45,661
We don't want these variables to be indicating that there's a difference.

86
00:05:45,661 --> 00:05:48,860
We want our exposure to be indicating a difference in our outcome.

87
00:05:48,860 --> 00:05:52,677
So we don't want any confounding in our exposure.

88
00:05:52,677 --> 00:05:57,651
I think an episode they call this like differential some type of information

89
00:05:57,651 --> 00:05:59,991
bias or something, I don't know.

90
00:05:59,991 --> 00:06:04,615
Exposure misclassification, that's what it is?

91
00:06:04,615 --> 00:06:08,575
Okay, so what do you think that you'll find in your data set,

92
00:06:08,575 --> 00:06:10,031
if you were to do this?

93
00:06:10,031 --> 00:06:16,270
>> [INAUDIBLE] And I guess before and

94
00:06:16,270 --> 00:06:23,208
after our significant [INAUDIBLE].

95
00:06:23,208 --> 00:06:24,104
>> If before and

96
00:06:24,104 --> 00:06:30,065
after are statistically significant that would indicate that there is confounding.

97
00:06:30,065 --> 00:06:34,025
Because if before and after a statistically significant it would

98
00:06:34,025 --> 00:06:40,240
indicate that there's something about age that's making before and after different.

99
00:06:40,240 --> 00:06:43,270
If aerobic exercise is statistically significant.

100
00:06:43,270 --> 00:06:47,446
There's something about aerobic exercise that's making before and after different.

101
00:06:47,446 --> 00:06:53,123
There's something about that extra variable that's affecting our exposures.

102
00:06:53,123 --> 00:07:01,159
>> [INAUDIBLE].

103
00:07:01,159 --> 00:07:06,610
>> It's not that we don't want something we just wanna know about it if it exists.

104
00:07:06,610 --> 00:07:09,510
Okay, so it's not necessarily a bad thing.

105
00:07:09,510 --> 00:07:13,551
It's just good to know if it does or does not happen, Capiche?

106
00:07:14,870 --> 00:07:15,431
>> Cool.

107
00:07:16,530 --> 00:07:23,040
>> Okay, so the challenge point is just to continue the advanced table one.

108
00:07:23,040 --> 00:07:26,050
So you're gonna continue along with your quartiles.

109
00:07:26,050 --> 00:07:29,956
Your quartiles are gonna be your groups instead of the before after condition that

110
00:07:29,956 --> 00:07:31,860
they did in the snack study.

111
00:07:31,860 --> 00:07:37,861
So does my first quartile for my exposure differ significantly in age,

112
00:07:37,861 --> 00:07:41,671
is age affecting that quartile difference?

113
00:07:43,590 --> 00:07:45,047
So add a p-value,

114
00:07:45,047 --> 00:07:50,800
briefly interpret the associations between exposure and each variable.

115
00:07:50,800 --> 00:07:53,401
Now, that you have a formal hypothesis test result.

116
00:07:54,460 --> 00:07:58,107
These relationships help us to understand the confounding potential of

117
00:07:58,107 --> 00:07:59,500
variables in the dataset.

118
00:07:59,500 --> 00:08:03,038
Although we might not be specifically interested in the effect of

119
00:08:03,038 --> 00:08:04,068
these variables,

120
00:08:04,068 --> 00:08:08,880
this sub analysis helps to determine how important they are to adjust for.

121
00:08:08,880 --> 00:08:12,700
So again, it's not necessarily good or bad either way that the tests come out,

122
00:08:12,700 --> 00:08:14,761
it's just good to have that information.

123
00:08:17,290 --> 00:08:19,160
Okay so we're gonna run through an example.

124
00:08:19,160 --> 00:08:24,055
There's some code that's in the lab three section in canvas that

125
00:08:24,055 --> 00:08:29,590
I'm gonna run through with you guys if you need to look back at it.

126
00:08:29,590 --> 00:08:33,997
But basically there's this medical expenditure panel survey and

127
00:08:33,997 --> 00:08:39,112
then what's happened is that the authors have categorized based on region,

128
00:08:39,112 --> 00:08:41,560
where the participants are living.

129
00:08:43,470 --> 00:08:49,450
So you might have Q1, Q2, Q3 and Q4 in your data set.

130
00:08:49,450 --> 00:08:50,950
The authors just had region.

131
00:08:50,950 --> 00:08:53,870
And so they're testing to see is there something about BMI,

132
00:08:53,870 --> 00:08:55,771
that's making these regions differ.

133
00:09:00,880 --> 00:09:03,830
So these are the key values that they calculated.

134
00:09:03,830 --> 00:09:07,320
They used ANOVA it says in the bottom for numerical variables.

135
00:09:07,320 --> 00:09:10,001
In a chi square test for categorical variables.

136
00:09:12,580 --> 00:09:17,048
All right, that is the end of the side show, we're gonna move over into our, you

137
00:09:17,048 --> 00:09:21,611
don't need to download it but if you wanna follow along with me, you totally can.

138
00:09:22,890 --> 00:09:25,466
Okay, so can you guys see that?

139
00:09:25,466 --> 00:09:27,240
Okay, if you want me to zoom in?

140
00:09:27,240 --> 00:09:33,208
>> [INAUDIBLE].

141
00:09:42,718 --> 00:09:45,589
So there we go, that's better?

142
00:09:51,253 --> 00:09:53,174
It's bigger and then zoom in again.

143
00:09:58,200 --> 00:10:01,110
Okay, library tidyverse.

144
00:10:01,110 --> 00:10:06,230
So the first two times are just getting the categories together.

145
00:10:06,230 --> 00:10:08,000
So this is the first test.

146
00:10:08,000 --> 00:10:13,287
We're gonna take a look at our data, which are the categorical

147
00:10:13,287 --> 00:10:18,091
variables here and which are the continuous variables.

148
00:10:18,091 --> 00:10:21,493
Someone other than Mirah, no offense [LAUGH].

149
00:10:21,493 --> 00:10:25,847
>> [INAUDIBLE].

150
00:10:25,847 --> 00:10:31,580
>> Nice, okay, so that means region sites in general health categorical.

151
00:10:31,580 --> 00:10:36,439
Okay, so remember that table that was in the slides were testing the difference

152
00:10:36,439 --> 00:10:37,609
between regions.

153
00:10:37,609 --> 00:10:39,186
So are not hypothesis for

154
00:10:39,186 --> 00:10:43,710
all of these tests that we're gonna run is that there is no difference.

155
00:10:43,710 --> 00:10:46,221
And the alternative is that there is a difference.

156
00:10:47,760 --> 00:10:50,504
It's the first test,

157
00:10:50,504 --> 00:10:56,900
there is a difference in age and region, right?

158
00:10:56,900 --> 00:11:01,392
And then the summary, what's the p-value here?

159
00:11:08,350 --> 00:11:15,930
Here let's see if I can make it up.

160
00:11:15,930 --> 00:11:17,271
I don't like it.

161
00:11:24,170 --> 00:11:30,201
No, okay, Okay,

162
00:11:30,201 --> 00:11:36,747
so I just ran the same things that

163
00:11:36,747 --> 00:11:41,839
you guys in the back might

164
00:11:41,839 --> 00:11:48,156
be able to see a little better.

165
00:11:48,156 --> 00:11:52,323
What is the p-value?

166
00:11:52,323 --> 00:11:53,990
>> [INAUDIBLE].

167
00:11:53,990 --> 00:11:57,160
>> Good, yeah, 6.33 times 10 to the -7.

168
00:11:57,160 --> 00:11:58,331
Is this significant?

169
00:12:00,910 --> 00:12:03,071
It's really tiny.

170
00:12:03,071 --> 00:12:05,030
So that means it is significant.

171
00:12:05,030 --> 00:12:10,930
Remember the p-value is the probability of seeing a value as extreme or more extreme?

172
00:12:10,930 --> 00:12:15,001
If the variation is solely due to chance, that probability is really tiny.

173
00:12:15,001 --> 00:12:18,680
So that tells us our differences are likely not due to chance,

174
00:12:18,680 --> 00:12:21,101
there's something more going on here.

175
00:12:22,120 --> 00:12:25,880
Okay, how many people are in this data set?

176
00:12:25,880 --> 00:12:30,830
Don't look at the data side of

177
00:12:30,830 --> 00:12:36,182
the R without looking at [LAUGH].

178
00:12:38,574 --> 00:12:43,726
>> Okay, the degrees of freedom

179
00:12:43,726 --> 00:12:50,540
is like four groups minus one, right?

180
00:12:50,540 --> 00:12:55,881
And then the residuals are supposed to be minus the.

181
00:12:58,040 --> 00:13:03,180
>> Okay, that's yeah, you're totally along the right line of thinking.

182
00:13:03,180 --> 00:13:06,521
So our degrees of freedom, tell us how many people there are.

183
00:13:06,521 --> 00:13:09,576
So it's always you sum up this degree of freedom,

184
00:13:09,576 --> 00:13:12,000
this degree of freedom and you add one.

185
00:13:12,000 --> 00:13:16,827
Like in our T test, our T test is always distributed as like T

186
00:13:16,827 --> 00:13:21,390
is approximately T with n minus one degrees of freedom.

187
00:13:21,390 --> 00:13:25,820
In our F statistic, our F statistic is approximately F distributed.

188
00:13:25,820 --> 00:13:30,010
But F distributions have two parameters, not just one.

189
00:13:30,010 --> 00:13:34,868
So it's always DF1,

190
00:13:34,868 --> 00:13:38,958
so of our region and

191
00:13:38,958 --> 00:13:42,801
N-DF-1-1.

192
00:13:44,430 --> 00:13:48,954
So to figure out the number of people, we know what DF1 is,

193
00:13:48,954 --> 00:13:54,301
we don't want is and we know what this, so n is this plus this plus one.

194
00:13:55,770 --> 00:14:00,381
So there are 9,231 people in our sample.

195
00:14:01,810 --> 00:14:10,150
Okay, yeah, I think that's all we're gonna say about the F statistic.

196
00:14:10,150 --> 00:14:14,610
Okay, and then that was and I know of a test.

197
00:14:14,610 --> 00:14:21,401
So we tested a continuous variable test, BMI p-value.

198
00:14:21,401 --> 00:14:22,961
Is it significant or not?

199
00:14:24,750 --> 00:14:25,755
At Î± is 0.05.

200
00:14:31,340 --> 00:14:33,387
Yes, what does that mean?

201
00:14:38,740 --> 00:14:41,020
>> We need freedom to reject?

202
00:14:41,020 --> 00:14:43,350
>> We reject them all.

203
00:14:43,350 --> 00:14:48,171
Okay, so significant p-values mean that we reject the null, that nothing is going on.

204
00:14:49,870 --> 00:14:52,811
All right, so those are the two continuous ones.

205
00:14:52,811 --> 00:14:54,480
Those are our ANOVA tables.

206
00:14:54,480 --> 00:14:57,070
Now, we're gonna move into categorical data.

207
00:14:57,070 --> 00:15:02,048
So just taking a look at our data, along the vertical access,

208
00:15:02,048 --> 00:15:06,068
we have the regions and then along the X axis we have

209
00:15:06,068 --> 00:15:11,350
the different values that our observations are taking.

210
00:15:11,350 --> 00:15:13,450
Just kinda looking at this.

211
00:15:13,450 --> 00:15:15,370
There's a lot of numbers going on.

212
00:15:15,370 --> 00:15:18,600
It's hard to be like yeah, clearly there's a difference.

213
00:15:18,600 --> 00:15:21,219
So that's why we're gonna run.

214
00:15:21,219 --> 00:15:24,160
You can also run it to get counts,

215
00:15:24,160 --> 00:15:30,490
to get proportions that are a little bit nicer and easier to read.

216
00:15:30,490 --> 00:15:33,328
But what we're actually gonna do is we're gonna run a chi squared test.

217
00:15:33,328 --> 00:15:40,919
Chi squared tests, take tables of counts, not tables of proportions.

218
00:15:40,919 --> 00:15:42,970
Protest square test.

219
00:15:42,970 --> 00:15:47,671
Okay, p-value for the first test that we ran.

220
00:15:48,700 --> 00:15:50,213
What is that?

221
00:15:50,213 --> 00:15:51,792
>> 174.

222
00:15:51,792 --> 00:15:53,258
>> Significant or not?

223
00:15:53,258 --> 00:15:54,087
>> Not significant.

224
00:15:54,087 --> 00:15:55,030
>> Not significant.

225
00:15:55,030 --> 00:15:55,993
What does that mean?

226
00:15:55,993 --> 00:15:58,590
We fail to reject the not good.

227
00:15:58,590 --> 00:16:02,034
Okay, 2nd test.

228
00:16:02,034 --> 00:16:03,656
What does that mean?

229
00:16:03,656 --> 00:16:05,436
>> That we reject.

230
00:16:05,436 --> 00:16:09,525
>> Good, okay, and then one other thing,

231
00:16:09,525 --> 00:16:14,335
lab three asks you to do a residual plot just to do

232
00:16:14,335 --> 00:16:18,690
some very basic checking of assumptions.

233
00:16:18,690 --> 00:16:21,970
This is not code that Matt put on canvas.

234
00:16:21,970 --> 00:16:26,320
But this is also being recorded, so you can come back to this later on canvas.

235
00:16:26,320 --> 00:16:31,279
So just for an example, I'm going to model general health with region,

236
00:16:31,279 --> 00:16:33,731
age, sex and BMI as co-variants.

237
00:16:35,180 --> 00:16:39,730
And to get the residuals there two general ways that you can extract them.

238
00:16:39,730 --> 00:16:41,888
A linear model object that LM,

239
00:16:41,888 --> 00:16:45,640
object actually has residuals contained within it.

240
00:16:45,640 --> 00:16:50,536
So you can just index your linear model object to get residuals and

241
00:16:50,536 --> 00:16:52,591
save it as another vector.

242
00:16:53,950 --> 00:16:58,788
Or you can also just do residual of your linear model vector and or

243
00:16:58,788 --> 00:17:03,716
your linear model object and it will return a vector that contains

244
00:17:03,716 --> 00:17:07,500
the residuals and they're exactly equivalent.

245
00:17:07,500 --> 00:17:11,819
Every single one of those values is equal to the other ones.

246
00:17:11,819 --> 00:17:17,201
So yeah, those are the two ways of getting residuals.

247
00:17:17,201 --> 00:17:18,579
We run over tests.

248
00:17:18,579 --> 00:17:20,329
This data is on canvas.

249
00:17:20,329 --> 00:17:23,359
So that concludes the presentation.

250
00:17:23,359 --> 00:17:24,628
Are there any questions?

251
00:17:24,628 --> 00:17:26,901
I'm gonna actually turn off on October first.

