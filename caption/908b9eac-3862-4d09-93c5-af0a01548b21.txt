1
00:00:00,840 --> 00:00:14,040
But now, even while our economy is heading off this writing, which you can only replace,

2
00:00:14,040 --> 00:00:22,290
you know what your committee is advising on this project is governed by our policy focus.

3
00:00:23,010 --> 00:00:36,910
So I'm hoping you can just give me an example that might look a stroke like the one they've got.

4
00:00:37,150 --> 00:00:43,370
They just didn't have time to to the results.

5
00:00:43,410 --> 00:00:48,160
That's the theory. I think, though, we have not heard 100 words.

6
00:00:48,330 --> 00:01:03,910
Oh, you guys need to sit here and you go back and find out what everyone told us over the past couple of months.

7
00:01:05,820 --> 00:01:18,060
Yeah, I like this movie, but I think the setting ended up being a really good point in my way.

8
00:01:18,810 --> 00:01:31,050
But it in advance. And he's like. I might go into my anger against them.

9
00:01:31,160 --> 00:01:36,940
Yeah. Oh, I know, I know.

10
00:01:37,130 --> 00:01:41,370
I knew they were going to do that.

11
00:01:45,560 --> 00:01:54,450
Yeah. Yeah. So how's tennis?

12
00:01:55,130 --> 00:01:58,960
Yeah. Just gonna kill me.

13
00:02:01,100 --> 00:02:09,320
All right, well, let's get started. And maybe the other two thirds of the class are enjoying this weather, or they just never show up.

14
00:02:09,860 --> 00:02:15,740
So today we're going to finish the adaptive futures, talk about protocol deviations,

15
00:02:16,640 --> 00:02:21,460
and then that is the end of the material for your test, which is a week from today.

16
00:02:21,470 --> 00:02:25,280
We will talk about crossover designs on Tuesday.

17
00:02:26,150 --> 00:02:30,559
But I didn't I ended up taking off any questions on the test. So the test is very similar.

18
00:02:30,560 --> 00:02:40,610
It's multiple choice and short answer focused on the material we've learned since phase two.

19
00:02:40,610 --> 00:02:41,569
Right? So phase three.

20
00:02:41,570 --> 00:02:52,729
But that includes randomization and sample size, adaptive stuff, compliance, which we'll talk about today as well, or protocol deviations.

21
00:02:52,730 --> 00:02:56,120
Any questions about the test?

22
00:03:01,570 --> 00:03:05,950
And your homework is now due Tuesday, and that's your last homework.

23
00:03:05,970 --> 00:03:10,299
And then I saw almost every group, maybe not really every group,

24
00:03:10,300 --> 00:03:15,879
but there are a lot of groups the other day at office hours, so it's good that you're working on your projects,

25
00:03:15,880 --> 00:03:23,680
continue to do that because the videos are due, I think the first so that we have a little bit of time to watch them to then have discussion on class.

26
00:03:24,960 --> 00:03:35,640
Okay. So if you remember, we were talking about adaptive features Tuesday and we mainly focus on group sequential design.

27
00:03:35,640 --> 00:03:43,350
So the fact that we can have these interim analyzes where we before the study starts, we decide that we're going to look at the data at certain times.

28
00:03:43,680 --> 00:03:51,840
And then based upon what's happening with the data and some pre-specified rules or boundaries like Pocock Boundary or O'Brien Fleming boundary,

29
00:03:52,140 --> 00:03:56,340
we can decide whether we can stop the trial early or we would continue.

30
00:03:57,180 --> 00:04:02,669
Then I asked you to remove that set of articles from clinical trials, and that's really focus on response.

31
00:04:02,670 --> 00:04:06,420
Adaptive randomization are what they call an outcome adaptive randomization.

32
00:04:06,420 --> 00:04:11,100
So we want to talk about that today. So that's one of the other types of adaptive features.

33
00:04:11,490 --> 00:04:21,540
But remember that there are a lot of different adaptive designs or ways to adapt a clinical trial besides these two that we're mainly focusing on.

34
00:04:22,350 --> 00:04:28,560
Okay. So here's an example. This was an HIV transmission trial.

35
00:04:28,800 --> 00:04:31,830
It was written up in the New England Journal of Medicine in 1994,

36
00:04:32,220 --> 00:04:39,630
and it was reporting on a trial to evaluate this drug, AZT, and reducing the transmission of HIV from mom to baby.

37
00:04:40,380 --> 00:04:48,750
And so they used a 5050 randomization or equal balance randomization between moms who got AZT and moms who got a placebo.

38
00:04:49,530 --> 00:05:00,150
What they found was that 20 out of 239 babies who were born of moms who were in the treatment group were HIV positive,

39
00:05:00,600 --> 00:05:07,200
whereas 60 out of 238 are almost the exact same number of the moms who were treated with placebo were HIV positive.

40
00:05:07,200 --> 00:05:08,429
So you can see they're right.

41
00:05:08,430 --> 00:05:20,670
There's a 3 to 1, almost 3 to 1 ratio of HIV transmission in the placebo in the placebo group are two to 2 to 1 transmission.

42
00:05:21,510 --> 00:05:29,340
And so. Right. Three times as many. So given the seriousness of this outcome of the study that these babies were HIV positive,

43
00:05:31,560 --> 00:05:35,550
there's the argument that was balanced randomization, ethical.

44
00:05:35,700 --> 00:05:45,779
Was that a good idea? And so there are designs that have used this this trial as an example of, well,

45
00:05:45,780 --> 00:05:51,000
if we actually decided to change the allocation ratio or change that probability of randomization,

46
00:05:51,300 --> 00:05:54,720
there wouldn't have been so many babies who are HIV positive.

47
00:05:54,930 --> 00:06:07,680
And so, for example, way so there was a paper written after the fact using the randomized play,

48
00:06:07,680 --> 00:06:11,489
the winner rule, which was written up in the late sixties and late seventies.

49
00:06:11,490 --> 00:06:16,620
Right? So it's definitely around when this HIV trial occurred but wasn't used,

50
00:06:16,620 --> 00:06:21,930
an association went back and said, Well, what if we actually use this in this trial?

51
00:06:22,350 --> 00:06:31,770
Then there would have been 360 patients who received AZT and only 117 women who received the placebo.

52
00:06:32,100 --> 00:06:38,780
And it would have, instead of having 80 babies, have the HIV positive, only 68 babies total.

53
00:06:38,780 --> 00:06:43,760
But at this I'd be positive, which seems valuable here, right,

54
00:06:43,770 --> 00:06:53,070
that you're treating more women with this drug that ended up being very useful and less babies who are HIV positive.

55
00:06:55,200 --> 00:07:02,670
And so a reminder, we actually talked very briefly about this play the winner role when we looked at the phase one, phase two designs.

56
00:07:04,380 --> 00:07:08,280
Now, if you don't have a randomized play, the winner role is just to play the role.

57
00:07:08,280 --> 00:07:11,880
This was what was a jasso paper in 1969.

58
00:07:12,840 --> 00:07:21,240
You would, you know, flip a coin at first and then see if the patient had a response or was had a success or not.

59
00:07:21,540 --> 00:07:25,859
And if they were successful, then the next patient gets that treatment that was successful.

60
00:07:25,860 --> 00:07:29,520
And if they were not successful, then they'd get the other treatment.

61
00:07:29,880 --> 00:07:37,290
And so you can see here that really the only flip a coin happens with the first patient and then every patient,

62
00:07:37,300 --> 00:07:41,790
every treatment assignment thereafter depends on the previous patient's response.

63
00:07:43,800 --> 00:07:50,129
That's how two properties are here, that the ratio of one treatment arm,

64
00:07:50,130 --> 00:07:57,180
the number in a treatment arm over the total number of patients treated is the I think these are the

65
00:07:57,180 --> 00:08:06,420
probability of success in like the better over the combination of the probability of success successes.

66
00:08:06,810 --> 00:08:15,690
So the downfall here is that it's not randomized beyond the first patient everybody's assigned based on the previous treatment response.

67
00:08:15,690 --> 00:08:22,920
And what's another downfall here to what would make this what could make this difficult to actually put into practice?

68
00:08:25,590 --> 00:08:30,390
We have to wait for the previous patients response before we can assign the next person.

69
00:08:30,660 --> 00:08:40,500
So if this is not a very quick outcome, right, this trial could take a very long time, this type of rule.

70
00:08:40,980 --> 00:08:45,150
Now, they didn't fix that with this next iteration.

71
00:08:45,810 --> 00:08:52,290
It took almost ten years for away in Durham to say, hey, that might be a good idea, but because it's not randomized,

72
00:08:52,290 --> 00:08:59,850
let's actually add some randomization into it so that we can keep some of the positives or advantages of randomization.

73
00:09:00,210 --> 00:09:11,160
And so they said again in a paper, let's add this randomization by saying we have this big urn of treatment,

74
00:09:11,430 --> 00:09:16,440
possibility, treatment, sign up possibilities, and we'll begin with equal.

75
00:09:17,460 --> 00:09:23,910
Balls of the two treatment options. And now every time somebody we can flip a coin, right?

76
00:09:23,940 --> 00:09:32,999
So we assign a if somebody has a success on that treatment day, then we'll add one more of that treatment, a ball into the mix.

77
00:09:33,000 --> 00:09:36,840
So now the probability of getting an A is a little bit higher.

78
00:09:37,760 --> 00:09:42,240
And if somebody has a failure, then we'll add the opposite ball.

79
00:09:42,260 --> 00:09:45,500
So now the probability of getting the other treatment is a little bit higher.

80
00:09:46,070 --> 00:09:49,910
Right. So this is a way in which we're going to change the probability of randomization,

81
00:09:49,910 --> 00:09:55,640
and it depends on what the previous person did, but we're still keeping the randomization in play.

82
00:09:55,880 --> 00:09:58,880
It's not a deterministic. Okay, there is this success.

83
00:09:58,930 --> 00:10:04,220
Now you get that treatment. And so I also mentioned this Eskimo trial.

84
00:10:04,760 --> 00:10:09,840
They actually didn't I say here that they had the randomized say next slide, these randomized play the winner.

85
00:10:09,860 --> 00:10:13,459
They actually just used play the winner for this Michigan Eskimo trial.

86
00:10:13,460 --> 00:10:22,940
So this was done here at Michigan. This guy, as of like I think he's still around as of like six, seven, eight years ago, I met I met him.

87
00:10:23,330 --> 00:10:30,440
He was here. He was quite old. I think he's still around. He is an emergency department physician.

88
00:10:31,250 --> 00:10:37,549
He's probably not practicing anymore. But anyway, he was the one who started this Eskimo study and said, hey,

89
00:10:37,550 --> 00:10:46,490
we use this this extracorporeal membrane oxygenation for people who have cardiac surgery.

90
00:10:46,490 --> 00:10:53,299
It's essentially like this machine to be your blood, your blood heart system.

91
00:10:53,300 --> 00:10:56,390
Right. It can keep your blood going around, oxygenate your blood.

92
00:10:56,810 --> 00:11:01,490
Why don't we see if we can use this in babies who might need help?

93
00:11:02,240 --> 00:11:08,149
And so the first study happened in 1985, and as we'll see, they use this play the winner approach.

94
00:11:08,150 --> 00:11:13,580
And so they didn't have very many babies in the trial and they didn't really believe that evidence.

95
00:11:13,910 --> 00:11:19,850
So they had another study in Boston in 1989 that also was relatively small.

96
00:11:19,850 --> 00:11:28,190
And so they ended up doing a full randomized trial in the U.K. in 1996 to feel like they had actual robust evidence for the use of this.

97
00:11:28,820 --> 00:11:31,370
So I say our P.W., but it was really just the play.

98
00:11:31,370 --> 00:11:39,200
The winner role wasn't randomized, and they flipped a coin essentially at first, and that patient got the conventional therapy.

99
00:11:39,590 --> 00:11:49,489
And then so then the next patient, since that was a failure, that execution got chemo and then the next ten were all successes on chemo.

100
00:11:49,490 --> 00:11:56,180
So they all just kept receiving well, so there's only one patient, there's only 12 total patients, one who had the conventional therapy.

101
00:11:56,720 --> 00:12:04,370
And as you can see, right, with 12 patients and only one who had one treatment, there was no statistical conclusion.

102
00:12:04,760 --> 00:12:08,690
Right. We can't get a variance around that one person in that arm.

103
00:12:08,840 --> 00:12:12,530
And so this lacked power. There was high variability.

104
00:12:12,770 --> 00:12:19,520
And while it looks like, oh, while it must be very useful, this is still based on a very small sample size.

105
00:12:19,520 --> 00:12:25,460
And so it could have been specifics about these specific babies which led to its success or failure.

106
00:12:27,470 --> 00:12:37,100
So eventually they had that larger chemo trial in England where they randomized 185 patients and they saw 82 deaths.

107
00:12:38,090 --> 00:12:49,250
And so they had a equal balance randomization in that UK trial with 93 infants getting Eskimo and 92 infants getting the conventional therapy.

108
00:12:49,700 --> 00:12:55,280
And they saw that there are about half as many deaths in the Echo group than the conventional therapy group.

109
00:12:55,760 --> 00:13:02,420
And so again, statisticians who came up with what they call an efficient randomized adaptive design,

110
00:13:02,780 --> 00:13:11,630
they reanalyzed or kind of took that M0 trial, the UK, M0 data and said, well, what if we use this efficient adaptive randomization?

111
00:13:12,020 --> 00:13:16,580
How would it look different? How would the outcomes play out? How many people would have gotten each treatment?

112
00:13:17,180 --> 00:13:20,870
So they applied this based on the actual data in the trial, right?

113
00:13:20,870 --> 00:13:24,410
Based on the patients who came in and what their outcomes were.

114
00:13:25,430 --> 00:13:34,790
And through simulation, they found that that they would have gotten 121 infants who had the active atmo treatment and only 64 got the conventional.

115
00:13:35,240 --> 00:13:39,380
And there would have been about equal deaths in each group.

116
00:13:39,650 --> 00:13:42,020
But because there are so many more in the atmo group.

117
00:13:42,020 --> 00:13:49,790
Right, you can see that since we didn't have as many in the conventional group, there are less total deaths across the trial.

118
00:13:52,300 --> 00:13:54,520
And so with some of these examples, right,

119
00:13:54,550 --> 00:14:00,850
you can see that perhaps there is some advantages to being able to change the randomization probability in the trial.

120
00:14:00,850 --> 00:14:05,590
It's very easy to retrospectively look at a trial and say, Oh, we could have done that better.

121
00:14:05,800 --> 00:14:10,300
Right. And so it's it was it's motivating.

122
00:14:10,300 --> 00:14:15,940
But then we also have to be careful because we never know when we actually start a trial whether the treatment's actually going to be good or not.

123
00:14:15,940 --> 00:14:22,180
And so that's some of the arguments against the adaptive randomization that we'll talk more about later.

124
00:14:22,960 --> 00:14:26,560
But essentially, you write that clinical trials are complex.

125
00:14:26,950 --> 00:14:32,980
They often have multiple objectives. So we're we're trying to maximize the power to detect the difference between the treatments.

126
00:14:33,430 --> 00:14:40,810
We're also probably trying to minimize the expected number of failures or or bad outcomes.

127
00:14:41,110 --> 00:14:44,469
We're trying to maximize the individual patients experience in the trial,

128
00:14:44,470 --> 00:14:50,740
but we're also trying to maximize getting that information fast out to the patient, the larger public.

129
00:14:51,190 --> 00:14:54,969
And as always, we have some limited resources.

130
00:14:54,970 --> 00:14:59,230
And so we're trying to minimize the total cost of the trial. So you can imagine.

131
00:14:59,230 --> 00:15:08,440
Right, that we can set apart set aside as one primary objective in terms of maximizing or minimizing one of these

132
00:15:08,440 --> 00:15:15,069
things and set forth defining that probability of randomization that maximizes or minimizes one of these.

133
00:15:15,070 --> 00:15:18,430
And it's likely. Right, that randomization probably could change.

134
00:15:18,730 --> 00:15:22,870
That probability could change based on what our actual goal is here.

135
00:15:24,250 --> 00:15:32,290
And so we have to be very careful and explicit about what the goal is and see how

136
00:15:32,290 --> 00:15:36,340
that actually affects the individuals in the trial and also outside of the trial.

137
00:15:37,900 --> 00:15:44,470
And remember that, right, we generally always want to use randomization because this removes the potential bias in a clinical trial.

138
00:15:45,850 --> 00:15:49,780
Okay. So how can we figure out if we're going to use adaptive randomization?

139
00:15:49,780 --> 00:15:52,809
So how do we adapt the randomization? Right.

140
00:15:52,810 --> 00:16:01,720
So let's pretend that we have a setting where we have two treatments, A and B, and the outcome is binaries and probability of response or success.

141
00:16:02,350 --> 00:16:05,979
And so our null hypothesis is that these the probability of response is the same.

142
00:16:05,980 --> 00:16:11,050
And all of our alternative is that it differs and we can say that it differs by some delta amount.

143
00:16:13,450 --> 00:16:19,510
And here we'll say response is good. And so let's use the normal approximation for binary data.

144
00:16:20,200 --> 00:16:23,950
So we have what we call sigma squared the variances here.

145
00:16:25,330 --> 00:16:30,549
So can we modify this allocation probability?

146
00:16:30,550 --> 00:16:30,760
Right.

147
00:16:30,760 --> 00:16:40,719
The probability that a patient gets treatment t equal to one after every individual in the trial to reflect what we're learning about in the game.

148
00:16:40,720 --> 00:16:50,470
Right. So we want to use the cumulative data in the trial to figure out can we change the probability of randomization to under some

149
00:16:51,130 --> 00:17:00,700
specific criteria of like maximizing the efficiency of the treatment effect estimate or minimizing the number of non-responders.

150
00:17:02,860 --> 00:17:08,020
And so if we start, the first person will flip the coin, the probability will be one half.

151
00:17:08,440 --> 00:17:11,889
How can we figure out how to change it moving forward? Okay,

152
00:17:11,890 --> 00:17:16,570
so this is all what response adaptive randomization or outcome adaptive randomization

153
00:17:16,570 --> 00:17:21,250
is doing is that they're trying to find an optimal allocation and that

154
00:17:21,250 --> 00:17:26,829
optimal allocation will depend upon a specific criterion for which we call the

155
00:17:26,830 --> 00:17:32,530
optimal criterion or that thing that we're trying to maximize or minimize. Then we'll use sequential estimation,

156
00:17:32,530 --> 00:17:39,370
meaning after every patient we will update the information that we have and we'll substitute our

157
00:17:39,370 --> 00:17:45,340
estimates from the data to figure out what the next what we're going to call the optimal allocation.

158
00:17:48,700 --> 00:17:56,499
And so this this using the information accrued, updating what we have, applying it, and then updating the information accrued again.

159
00:17:56,500 --> 00:18:01,360
All right. This is called the response adaptive randomization procedure because that probability of

160
00:18:01,360 --> 00:18:06,310
assignment to treatment is going to change and depend upon the previous patients responses.

161
00:18:07,120 --> 00:18:12,160
Now we're going to talk about it as if we're doing this after every patient, which you certainly could do.

162
00:18:12,190 --> 00:18:16,809
You could also have some rules that you don't do it until after every five patients or three patients.

163
00:18:16,810 --> 00:18:22,810
Right. But generally, if you have response, adaptive recommendation and you turn it on,

164
00:18:23,200 --> 00:18:27,320
you're going to update it constantly for each individual's like the serum, right?

165
00:18:27,430 --> 00:18:32,139
Think back to the continuous assessment method after every individual we'd figure out if it had

166
00:18:32,140 --> 00:18:39,220
a DLT or not and then we'd update the dose assignment based on that accumulated information.

167
00:18:40,380 --> 00:18:46,350
Okay. So here's one way in which we could think about defining the optimal criterion.

168
00:18:46,680 --> 00:18:49,050
So we could use what's called Neimann Allocation.

169
00:18:49,230 --> 00:18:58,200
And you may remember I think we talked about this earlier that the Neimann allocation is actually set to try to minimize the total

170
00:18:58,200 --> 00:19:06,810
number of patients in the trial so that we're essentially trying to minimize the variance of the treatment effect estimates.

171
00:19:06,840 --> 00:19:15,629
Okay. Or maximize the precision. So this is one criterion or goal that we could have is that we want to maximize the precision,

172
00:19:15,630 --> 00:19:18,660
precision of the treatment effect estimates or minimize the variance.

173
00:19:19,950 --> 00:19:25,259
Okay, so let's pretend that the total number of patients in our trial or those that we have that

174
00:19:25,260 --> 00:19:35,520
number and a and the number and B and we can refer to the number in B is some constant times.

175
00:19:35,520 --> 00:19:46,229
The number and a price of this R is like the ratio of how many people are if we have an eight and how many different is that?

176
00:19:46,230 --> 00:19:51,780
And B All right. So here's here's our sample size calculation for the one of the treatment arms,

177
00:19:51,780 --> 00:19:55,140
which is just based on the normal approximation for the binary outcome.

178
00:19:55,980 --> 00:20:01,110
Right? Or you could alternatively think this is just normal, but we're working in that binary outcome.

179
00:20:01,110 --> 00:20:06,600
So we have this difference between the treatments, we have their variances, we have the allocation ratio,

180
00:20:06,900 --> 00:20:11,070
and we have our critical statistics based on our type one error in our power.

181
00:20:12,420 --> 00:20:17,430
If we just let this be C, just going to do that because the algebra is easier, right?

182
00:20:17,430 --> 00:20:24,120
Or it's easier to show on one line. Right? You could then solve for the total of N right.

183
00:20:24,120 --> 00:20:28,110
And we set up to be an eight times one plus R.

184
00:20:28,650 --> 00:20:38,100
And so if I now say that A is the C squared over delta squared or sigma squared a plus thing was word V over R, right.

185
00:20:38,100 --> 00:20:46,469
I can do the algebra and figure out that this is the total n and now if I want to figure out how do I minimize this?

186
00:20:46,470 --> 00:20:51,390
N Right, I can, I can minimize with respect to r this allocation ratio.

187
00:20:51,390 --> 00:20:54,750
So I take this derivative so that equal to zero. I'm going to do it for you.

188
00:20:54,750 --> 00:21:04,860
I know you all can do the calculus, right? And you'll figure out that the optimal allocation ratio here is the square root of our

189
00:21:04,860 --> 00:21:08,790
variance and treat one treatment arm over the variance and the other treatment arm.

190
00:21:09,710 --> 00:21:13,370
And so here in Neiman Neiman allocation,

191
00:21:13,370 --> 00:21:21,859
the objective is that we're minimizing the variance of the estimate are for our population mean our population proportion right and so that would

192
00:21:21,860 --> 00:21:32,210
say that we'd allocate A to treatment A and we'd allocate and be based on what we're seeing in each treatment arm on their probability of response.

193
00:21:32,870 --> 00:21:39,360
Right? So we have a and a and then after every individual we're going to figure out, well, how many, right?

194
00:21:39,410 --> 00:21:43,460
We flip a coin. So one goes to probably flip a coin for the first few people.

195
00:21:43,820 --> 00:21:49,100
Right. We have so many and a and we have some in B now we have to figure out what comes

196
00:21:49,100 --> 00:21:54,919
next so we can figure out how many people could be in any and how they could be in.

197
00:21:54,920 --> 00:22:01,460
And B, that's going to depend on, well, how many responses do we have in a, in A and how many responses do we have in B?

198
00:22:02,960 --> 00:22:07,940
And so what would happen if we actually applied this allocation ratio?

199
00:22:09,060 --> 00:22:13,320
Say that the probability of response in a at some point is 0.5.

200
00:22:13,320 --> 00:22:20,670
50% of the people in treatment have responded and 70% in treatment B have responded with equal allocation.

201
00:22:20,670 --> 00:22:34,350
I'd have 121 patients per arm, but with name allocation, I'd have 126 and A and 116 and B, because all I did was I just took I took the PS point five.

202
00:22:34,620 --> 00:22:38,430
So this was point seven times one match point seven over point five times one minus point five.

203
00:22:39,210 --> 00:22:43,830
Multiply it by any. Okay. What, what went wrong here? What's bad with this name in allocation.

204
00:22:45,640 --> 00:22:53,540
Or is anything about it? Look at what PR and PR and then look at how many are in each arm.

205
00:22:57,860 --> 00:23:02,960
If response is good, right? Which treatment is better or be treated well?

206
00:23:03,170 --> 00:23:08,360
Are there more patients in a or indeed. So that's bad, right?

207
00:23:08,630 --> 00:23:11,630
We just put more patients with name an allocation Z.

208
00:23:11,960 --> 00:23:18,500
Even though B is better for the most efficient treatment effect estimate, we put more patients on A.

209
00:23:20,900 --> 00:23:25,670
So we're going to put more patients on the worse treatment and we're going to see more failures.

210
00:23:25,850 --> 00:23:28,640
Right. So name and allocation does not seem good, right?

211
00:23:28,940 --> 00:23:35,150
Even though statistically it would be a good idea to optimize the efficiency of our treatment effect estimates.

212
00:23:35,960 --> 00:23:40,400
This is not a good randomization tool if the outcome is really bad.

213
00:23:41,390 --> 00:23:49,670
Right? And so this is what happens. This actually used to be a common problem that I took away and just put it in the in the class.

214
00:23:49,970 --> 00:23:57,200
But if you calculate the regions of if you you get this name an allocation and then you can figure out,

215
00:23:57,200 --> 00:24:00,620
so where is arm a going to be allocated to more patients?

216
00:24:00,620 --> 00:24:03,170
That's everywhere where this graph is colored black.

217
00:24:04,100 --> 00:24:13,690
And so you can see, for example, here, if the probability of response in a is 75% in the probability response for B is like 20%.

218
00:24:13,700 --> 00:24:16,520
You get more people that that seems like a good idea.

219
00:24:17,930 --> 00:24:27,080
But if your probability of response for a is 75% and your probability of response for B is like 95%, you're still getting more people today.

220
00:24:27,320 --> 00:24:34,070
Now it's that. So there are some bad decisions being made with the name and allocation.

221
00:24:34,340 --> 00:24:43,489
Same thing here, right? The probability of response for treatment, a 75% and the probability response for B is 50%.

222
00:24:43,490 --> 00:24:48,950
You should be giving a more often, right? But yet somehow with name and allocation, we're giving V more often.

223
00:24:49,400 --> 00:24:59,450
And so when we're in this upper triangle or in this right triangle, we're making bad decisions for name and allocation and bad being unethical.

224
00:24:59,510 --> 00:25:02,540
We're giving more people the worst performing treatment.

225
00:25:02,930 --> 00:25:06,830
We're more likely to see more failures based on this.

226
00:25:07,430 --> 00:25:13,670
So they're likely ethically problematic. This would be ethically problematic allocation.

227
00:25:15,510 --> 00:25:24,570
So there's a lot of alternatives that we can instead try to optimize this allocation ratio that is ethical.

228
00:25:24,870 --> 00:25:28,679
And so one of those alternatives was like was to say, oh, well,

229
00:25:28,680 --> 00:25:34,700
then we should just be instead of trying to maximize the efficiency, we should be trying to minimize the total number of failures.

230
00:25:35,100 --> 00:25:41,370
Right? Let's minimize those that number of non responses and figure out the allocation ratio that way.

231
00:25:41,610 --> 00:25:49,110
And so that was a statistical paper and what they found that if we want to minimize the expected number of non responses,

232
00:25:49,530 --> 00:25:54,030
then our optimal allocation ratio is based on the square root of the probability

233
00:25:54,030 --> 00:25:58,850
response in R&D over this over the probability response response in ARM.

234
00:25:59,790 --> 00:26:09,359
And so we can now figure out how do we, you know, if we're going to flip a coin based on this probability of randomization,

235
00:26:09,360 --> 00:26:15,990
that probability of randomization can now depend upon the accumulated probability of response in both of the treatment arms.

236
00:26:16,350 --> 00:26:22,140
And we can allocate the the patients based on this optimal allocation rate.

237
00:26:23,560 --> 00:26:29,290
So what happens is that after every after we so usually there's what we call like a burn in period,

238
00:26:29,410 --> 00:26:32,440
maybe the first ten patients or first 50 patients depending on the size.

239
00:26:32,920 --> 00:26:43,240
We use equal allocation, but then we turn on this adaptive randomization and after each patient we're going to estimate the response probabilities.

240
00:26:43,480 --> 00:26:46,600
So what was the this was like? What was the outcome?

241
00:26:46,600 --> 00:26:52,479
Did they respond or did they not respond? And this is for treatment A versus treatment B, right?

242
00:26:52,480 --> 00:26:58,209
This is for everybody. So you just have the number of responses over the total number of people in treatment, right?

243
00:26:58,210 --> 00:27:07,660
Or the number of responses over the total number of people in treatment B And then the next patient would be assigned assigned to arm A,

244
00:27:08,020 --> 00:27:17,290
according to this square root of the probably a response, the estimated problem response in that treatment arm.

245
00:27:17,830 --> 00:27:26,170
And like I said, usually we have this burn in period so that you'd have equal randomization for a little bit and then you could turn on this

246
00:27:26,170 --> 00:27:35,340
probability of this adaptive randomization and then after and patients we can calculate our use test statistic as usual,

247
00:27:35,360 --> 00:27:40,210
compare the proportions of response and both in both arms or compare the means of both arms.

248
00:27:41,470 --> 00:27:44,740
And we still have asymptotically normal distributions for outcomes.

249
00:27:47,480 --> 00:27:53,030
Okay. So essentially, right, what's happening with the outcome adaptive randomization is that we have some

250
00:27:53,750 --> 00:27:58,879
thing that we want to optimize in most cases or in this case we're talking about,

251
00:27:58,880 --> 00:28:05,990
we want to optimize the number of responders where we're minimizing the number of non-responders.

252
00:28:05,990 --> 00:28:09,080
Right. That's what's going to be our optimality criterion.

253
00:28:09,440 --> 00:28:18,559
And that's going to then decide what is the probability of getting assigned to a treatment so that our coin can now be biased.

254
00:28:18,560 --> 00:28:23,840
So it's not 5050 necessarily. It's based on that accumulated response probability.

255
00:28:25,400 --> 00:28:30,500
Okay. So let's look at a simple simulation study to compare simple randomization,

256
00:28:30,500 --> 00:28:37,520
meaning balance task has a flip of a balance coin of a fair coin versus response adaptive randomization,

257
00:28:37,520 --> 00:28:44,510
where we're trying to minimize the number of non-responders and we'll compare type one error power treatment allocation and treatment failures.

258
00:28:45,080 --> 00:28:50,590
Now there's code. I think I still have it in canvas.

259
00:28:50,590 --> 00:28:55,520
If I did not put it in there, I'm not going to show it or go through the code, but you can look at it if you're interested.

260
00:28:55,540 --> 00:29:01,870
What I want us to focus on are the results to see like what are the differences when we use adaptive randomization versus not.

261
00:29:02,750 --> 00:29:10,380
Okay, so this is a lot of different settings. So these are each each one of these rows are like if we ran, you know,

262
00:29:10,450 --> 00:29:18,129
we had a different trial where these are the two true probabilities of response for treatment a m.

263
00:29:18,130 --> 00:29:22,240
For treatment b. Okay. So these are different, like true settings.

264
00:29:22,600 --> 00:29:27,250
And so for the first four of them, right, they're all different null settings.

265
00:29:27,640 --> 00:29:29,620
They're all where there is no treatment effect.

266
00:29:29,800 --> 00:29:36,550
But what I'm showing you is that how does it differ for if the response rate is super low versus if the response rate is high.

267
00:29:37,390 --> 00:29:43,080
Now the rest of these. So from. This this row, 1020.

268
00:29:43,680 --> 00:29:45,120
Through this row,

269
00:29:45,120 --> 00:29:58,560
1035 to these four scenarios here are where treatment A is worse than treatment B and then the last scenarios are where treatment A is better.

270
00:29:58,890 --> 00:30:04,280
Oh, no, wait. Sorry, this one. This one is where treatment is better than treatment.

271
00:30:04,290 --> 00:30:07,830
Be here it's worse again. Here it's worse. Here is worse.

272
00:30:09,060 --> 00:30:13,560
But you can see what's happening here is that there is a greater and greater difference between the two treatments.

273
00:30:14,970 --> 00:30:22,190
Okay. So if we use fixed randomization, this is the probability that we reject the null hypothesis under the null.

274
00:30:22,200 --> 00:30:28,169
What should this be? What should the probability that we reject the null under the null be 1.5.

275
00:30:28,170 --> 00:30:30,120
Right. Should be our type one error rate. Exactly.

276
00:30:30,420 --> 00:30:36,930
So what we want to see here under both of these settings, whether it's adaptive or fixed allocation ratio, is that we're.

277
00:30:38,070 --> 00:30:45,630
Upholding our type one error rate, right, that we're seeing that the type of error rate we expected is the true type one error rate.

278
00:30:46,170 --> 00:30:50,950
And you can see that it's relatively. It's going relatively well, right?

279
00:30:50,970 --> 00:30:58,930
It's a little small. That might be due to the number of simulations we ran. But this is saying that we're rejecting the null.

280
00:30:58,930 --> 00:31:03,489
We're saying that there's a treatment difference when there actually isn't 4 to 5% of the time.

281
00:31:03,490 --> 00:31:07,690
And we re set up this design such that we we want that to be 5%.

282
00:31:08,410 --> 00:31:11,739
Okay. Now for any time that there's a treatment difference, right?

283
00:31:11,740 --> 00:31:17,170
This rejection rate is what is the power now, right?

284
00:31:17,470 --> 00:31:23,060
Is the probability that we rejects the null, given that there is a true difference in the treatments, which we see right.

285
00:31:23,080 --> 00:31:28,270
There is a true difference. And so I set this up just so it always has the same sample size.

286
00:31:28,270 --> 00:31:32,440
So you can see that as the difference between the two treatments increases.

287
00:31:32,800 --> 00:31:35,620
The more power we have, right, the higher the power.

288
00:31:36,160 --> 00:31:43,440
And if we compare the adaptive design to the fixed design, they have very similar type rejection rates, right?

289
00:31:43,450 --> 00:31:49,000
They're very similar type one error rates and very similar powers. That's good, right?

290
00:31:49,180 --> 00:31:55,840
We wouldn't want to have massively different power under one design or the other.

291
00:31:55,870 --> 00:31:59,290
We would want to have similar power and we would want to update our type one error rate.

292
00:32:00,520 --> 00:32:09,280
Oops. Okay. So now let's look at this is the number of individuals on the A treatment arm.

293
00:32:09,490 --> 00:32:14,140
So this is the number of individuals who are allocated to treatment arm A and in

294
00:32:14,770 --> 00:32:20,420
not outside of the parentheses is the average number on A and inside the press.

295
00:32:20,430 --> 00:32:26,330
These is the 97.5 fifth percentile of the number of individuals who are assigned to a right.

296
00:32:26,380 --> 00:32:31,000
So we're doing this in simulation and because we're using randomization, sometimes we could have way more.

297
00:32:31,120 --> 00:32:35,580
Some are using simple randomization, right? We're not doing block randomization or ever.

298
00:32:35,620 --> 00:32:38,990
So sometimes we have way more on one or the other. All right.

299
00:32:39,010 --> 00:32:42,070
So not surprisingly, right for fixed randomization, it's fixed.

300
00:32:42,760 --> 00:32:48,280
We on average always have half of the patients or 130 assigned to a treatment arm.

301
00:32:48,940 --> 00:32:52,240
And so 130 would also similarly be assigned to treatment arm.

302
00:32:53,230 --> 00:32:55,150
Occasionally randomization gets off.

303
00:32:55,150 --> 00:33:03,160
And so we could have as many as like 146 or the 97 point fifth percentile, 646 could be an arm a and there would be.

304
00:33:06,050 --> 00:33:09,350
What, 260 -146 on B? Okay.

305
00:33:09,380 --> 00:33:11,900
Now, what happens under the adaptive optimal design?

306
00:33:12,470 --> 00:33:19,700
Ideally, right under the knoll, you're not assigning more patients to one treatment or the other because there's not actually a difference.

307
00:33:19,970 --> 00:33:25,450
So we hope to see the same under the adaptive randomization versus under fixed randomization.

308
00:33:25,460 --> 00:33:34,460
We don't want it to be making adaptations. And so you can see that on average there is equal number to both treatment arms.

309
00:33:34,970 --> 00:33:42,620
But interestingly, the 97.5 percentile right is is higher in every case.

310
00:33:42,620 --> 00:33:45,259
It's about the same here. Right. But it's much higher here.

311
00:33:45,260 --> 00:33:53,690
When the probability of response is low, there's a lot more variability in the assignment because it is trying to make the changes.

312
00:33:53,810 --> 00:33:59,299
Right. It's updating the probability of response. And so when the probability response is so low,

313
00:33:59,300 --> 00:34:05,000
it's trying to make these changes and it's getting it wrong sometimes and eventually on average, right?

314
00:34:05,000 --> 00:34:09,200
It averages back out and it figures out, oh, there's no difference between these these treatments,

315
00:34:09,980 --> 00:34:16,880
but it can make some bad decisions, particularly when there's very small probability response.

316
00:34:17,450 --> 00:34:24,700
Now, if we go down here, right, these next four scenarios, we would like to see that there are less participants on treatment arm.

317
00:34:25,130 --> 00:34:31,310
Right? So we want to see that this mean is less than one in 30 because A is worse than B and

318
00:34:31,310 --> 00:34:36,950
we're trying to optimize to minimize the number of NONRESPONDERS And so that is the case,

319
00:34:36,950 --> 00:34:49,220
right. On average, somewhere between 96 to 111 are are treated with a and you can see that, right, though it gets less people are treated on a.

320
00:34:49,250 --> 00:34:53,000
The bigger the difference between the two treatments. Right.

321
00:34:53,000 --> 00:35:05,900
As this goes from 10 to 20, from 10 to 35, we're treating more patients on B or the better treatment now that 97.5 percentile.

322
00:35:06,230 --> 00:35:12,049
Right. It's it is smaller than that of equal allocation.

323
00:35:12,050 --> 00:35:17,900
So on average we are generally treating the less individuals on a in these cases.

324
00:35:19,940 --> 00:35:27,770
Now if we look at here, right, we want to see more patients treated on A because A is better and that is the case.

325
00:35:29,660 --> 00:35:39,530
And then if we go down here again, we want to see less patients treated on a and that is the case but interesting to note right that there are.

326
00:35:41,380 --> 00:35:47,620
Times when, for example, the 97.5 percentile like here or here.

327
00:35:47,620 --> 00:35:52,210
Right, actually is greater than the optimum the fixed randomization number.

328
00:35:52,630 --> 00:35:59,200
And so if we had just run an equal there, there could be scenarios in which even though we were attempting to use adaptive

329
00:35:59,200 --> 00:36:04,719
allocation and treat more on B because view as better were to be treated,

330
00:36:04,720 --> 00:36:10,210
more patients could be treated on a an adaptive scenario than had they had fixed allocation.

331
00:36:10,720 --> 00:36:17,750
Right. Because of the variability of the randomization. All right.

332
00:36:17,830 --> 00:36:23,649
So let's look at the number of failures. Right. The adaptive allocation was to minimize the number of failures.

333
00:36:23,650 --> 00:36:30,640
So what we hope to see are that there are less failures under adaptive randomization than under fixed randomization.

334
00:36:31,060 --> 00:36:33,250
Of course, under the NOL right,

335
00:36:33,250 --> 00:36:41,320
there is no real change because we ended up essentially having similar fixed randomization in the adaptive case, which you hope.

336
00:36:41,380 --> 00:36:45,410
Right. You want to make sure that if you have adaptive randomization, but there shouldn't be,

337
00:36:45,410 --> 00:36:51,100
it shouldn't be adapting, you should get the same thing as fixed randomization here.

338
00:36:51,100 --> 00:36:57,550
Right. We saw that we were on average treating more patients with treatment B And so we're hoping that there are less.

339
00:36:59,170 --> 00:37:03,249
Non responses are less failures and on average that is the case, right?

340
00:37:03,250 --> 00:37:08,290
We will go from 221 to 2 19 to 15, 2 to 11 208 to 202.

341
00:37:08,710 --> 00:37:17,980
Now the 97 point fifth percentile shows you that sometimes we actually get more right than the average of six randomization,

342
00:37:18,760 --> 00:37:23,080
but on average, we're getting less. But but look at the difference, right?

343
00:37:23,200 --> 00:37:27,040
You have the adaptive randomization and there's a 10% difference here.

344
00:37:27,460 --> 00:37:35,650
Only two non responses are saved with the adaptive randomization here only four non responses are saved.

345
00:37:35,830 --> 00:37:44,620
Right. There's a pretty small difference here. There's a 15% difference between treatments and on average there's only a difference of saving.

346
00:37:44,620 --> 00:37:46,450
Three non responses.

347
00:37:46,810 --> 00:37:56,139
Now if non-response is something like death, and especially if this is maybe with like yo babies or whoever, right, that might be really important.

348
00:37:56,140 --> 00:38:03,070
It might be. Well, this is this is enough to input the doctor randomization because it's so important to save any amount of deaths.

349
00:38:03,220 --> 00:38:07,720
Right. But if it's not as extreme of an outcome,

350
00:38:08,650 --> 00:38:14,140
this is a lot of work and a lot of waiting to update the probabilities for very

351
00:38:15,220 --> 00:38:20,860
few differences in the actual average number of individuals with with failures.

352
00:38:21,490 --> 00:38:24,730
Right. And so we are minimizing the failures.

353
00:38:24,820 --> 00:38:31,540
But unless that difference is really large, it's really not a very big difference between the groups.

354
00:38:34,180 --> 00:38:41,950
So it's simulations like these that people did looking at different scenarios to say is this, you know, is it really worth it?

355
00:38:41,950 --> 00:38:52,029
And so again, reading that set of clinical trials articles I found very interesting, they're not super statistical, right?

356
00:38:52,030 --> 00:38:54,340
They're very editorial commentaries.

357
00:38:55,180 --> 00:39:04,209
And it was it started out with, Hey, Anne Kimmelman, writing this commentary that said there are medical ethicists and they were like,

358
00:39:04,210 --> 00:39:08,380
RB, you know, this is adaptive randomization, something we really should be doing.

359
00:39:08,380 --> 00:39:14,950
It was getting a lot of press in the early two, thousands from statisticians being like, Look at this really cool thing that we can do.

360
00:39:14,950 --> 00:39:18,040
It could save lives. You know, this is really exciting.

361
00:39:18,040 --> 00:39:26,680
Look at the the different optimality criterion we can use to do have different of annotations and hang Kimmelman came in and said, well,

362
00:39:26,860 --> 00:39:33,280
you know, if I look at your your simulations, I really only see big differences when there are massive differences in treatments.

363
00:39:33,280 --> 00:39:38,379
And how often do we really get to see that? And we don't know at the beginning of the trial if we're going to see that.

364
00:39:38,380 --> 00:39:42,580
Right. So if we start implementing this all the time might not actually be useful.

365
00:39:42,940 --> 00:39:53,299
And I said, great, if there's. If if this outcome is going to be slow to see, and especially if we have we don't accrue patients faster,

366
00:39:53,300 --> 00:39:57,440
we're actually doing a disservice to the trial because the trial is going to take way longer.

367
00:39:57,740 --> 00:40:04,610
Then how do we just use fixed randomization and been able to as soon as people come in, assign them a treatment and ultimately.

368
00:40:04,610 --> 00:40:08,480
Right, they were thinking of the collective ethics at that point saying, you know,

369
00:40:08,510 --> 00:40:12,709
we want to treat the patients in the trial as well as we can, but we have equipoise.

370
00:40:12,710 --> 00:40:19,760
We don't know it's better and it'd be better for everybody else outside of the trial to get this treatment sooner if it's really effective.

371
00:40:21,650 --> 00:40:25,870
They're also really worried about informed consent of these adaptive designs.

372
00:40:26,750 --> 00:40:31,550
And part of it was saying, you know, we were supposed to come into the trial with equipoise.

373
00:40:31,850 --> 00:40:36,319
And now all of a sudden, as we're accruing information which could be based on very little information,

374
00:40:36,320 --> 00:40:41,420
we're throwing equipoise out the door, is that really, you know, appropriate?

375
00:40:41,420 --> 00:40:45,110
And if we're doing that, then why are we still giving people the inferior treatment?

376
00:40:45,650 --> 00:40:48,860
Right. So that was part of their their argument.

377
00:40:48,860 --> 00:40:53,809
And then a lot of the arguments for adaptive randomization were saying, well,

378
00:40:53,810 --> 00:40:59,210
this seems to benefit the patient and the patient might be more willing to be in a trial if there is adaptive randomization.

379
00:40:59,600 --> 00:41:06,079
And having Kimmelman come out and say, Do patients really understand this when they're enrolling in a trial through the informed consent?

380
00:41:06,080 --> 00:41:12,130
Do they understand where, you know, they don't know where they are in the trial, and now we're going to sell them saying like,

381
00:41:12,140 --> 00:41:17,210
you can get the better treatment, but they still have probability of getting the not better treatment.

382
00:41:17,540 --> 00:41:23,780
Is that really well understood, particularly based on consent material has to be written at an eighth grade level.

383
00:41:24,410 --> 00:41:29,840
Right. How much of this detail can you really give patients to truly understand what they're consenting to?

384
00:41:31,730 --> 00:41:38,000
And so you have to make sure that subjects still understand that they're not necessarily going to get the better arm.

385
00:41:38,300 --> 00:41:43,070
And if they're under that, if they think that that's true.

386
00:41:43,070 --> 00:41:50,729
Right. If they if they figure out that they've gotten the other treatment, they might be less adherent, less compliant, drop out.

387
00:41:50,730 --> 00:41:59,660
And that would be bad. One of the biggest arguments against adaptive design is the potential shift in patients over time in a trial.

388
00:42:00,080 --> 00:42:06,739
And so if patients all of a sudden are somewhat different and or if treatment's concomitant treatments,

389
00:42:06,740 --> 00:42:11,480
other treatments that you would have for these patients change over time in this trial,

390
00:42:11,720 --> 00:42:22,190
it's going to be really difficult to figure out, you know, where are we making the right decisions based on the the the responses?

391
00:42:22,190 --> 00:42:25,460
And everybody who came later still looks like everybody who was first.

392
00:42:25,790 --> 00:42:32,870
Right? Or was it something about the how the patients change or how other treatments change that made us make those decisions to end up with this?

393
00:42:34,060 --> 00:42:36,040
This imbalance between the treatments.

394
00:42:36,730 --> 00:42:45,100
And often what happens is that if you know, you have an adaptive trial and you're a clinician or a research assistant trying to put patients on this,

395
00:42:45,760 --> 00:42:54,999
you would be more likely to put the healthy patients on first because they have a less likely probability of getting the more effective treatment.

396
00:42:55,000 --> 00:42:59,380
Right, because we haven't really adapted those updated those treatment allocation rates yet.

397
00:42:59,980 --> 00:43:02,920
And you if you if you are sorry,

398
00:43:03,250 --> 00:43:10,570
you you could what you would end up doing is putting whoever really needs treatment on the trial right away because they don't have time to wait.

399
00:43:11,080 --> 00:43:12,970
Right. But if you have time to wait.

400
00:43:13,210 --> 00:43:20,560
Right, you might say, oh, hold on, wait, wait till we get further in the trial, because then you have a better chance of getting the better treatment.

401
00:43:20,830 --> 00:43:24,370
And the patients who can actually wait are the healthier patients. Right?

402
00:43:24,430 --> 00:43:32,410
So then you have this very different looking kind of patient across time and it's going to confound the treatment effects.

403
00:43:34,990 --> 00:43:41,470
So I find the articles really interesting. And you know, Don Barry is the first commentary after Hank Kimmelman.

404
00:43:41,800 --> 00:43:49,120
And if you don't know Don Barry, he was a statistician who was an Andy Anderson and then started his own business called Barry Consultants.

405
00:43:49,780 --> 00:43:53,589
His son, Scott Barry, now I think mainly runs it.

406
00:43:53,590 --> 00:43:55,600
And they have a don't know,

407
00:43:55,600 --> 00:44:04,240
like a large cadre of statisticians who I think mostly now work remotely remotely across the U.S. on these adaptive designs.

408
00:44:04,660 --> 00:44:07,150
They are a very lucrative company.

409
00:44:08,380 --> 00:44:19,209
The Barry's particularly now Scott Barry talks at a lot of conferences and they they are sort of like having a monopoly on adaptive designs.

410
00:44:19,210 --> 00:44:22,330
If you want to have an adaptive design, you consult with the Barry's.

411
00:44:23,470 --> 00:44:27,490
So but you can see that when he wrote that, he took it like a personal attack.

412
00:44:27,550 --> 00:44:32,440
Right. The head and Kimmelman were like personally attacking him on adaptive design.

413
00:44:32,440 --> 00:44:38,450
And he said, You know, they characterize this as unethical. Well, that means I'm unethical and I'm not unethical.

414
00:44:38,470 --> 00:44:43,680
Right. That was basically like his argument, which was just kind of funny to read, right?

415
00:44:43,690 --> 00:44:47,830
You just saw this man get, like, really angry and just take it so personally.

416
00:44:48,100 --> 00:44:55,839
And instead of having really well thought out statistical responses, right?

417
00:44:55,840 --> 00:44:59,499
It was just like, well, you suck, right? You're terrible. You suck, too.

418
00:44:59,500 --> 00:45:05,140
It's not me, it's you. And I was just like, I just found that article just so funny to read because he was.

419
00:45:05,570 --> 00:45:10,120
Then you keep reading and you're like, Oh, he said that, right? Like, like the next guy, right?

420
00:45:10,120 --> 00:45:14,560
And you're like, that was a much better way of getting across probably what he wanted to say.

421
00:45:16,750 --> 00:45:20,110
And then like, Hey, Kimmelman at the end, right? Write a response to everybody.

422
00:45:20,110 --> 00:45:23,200
And they just they pointed that out. They're like, What was wrong with it?

423
00:45:23,740 --> 00:45:30,760
Like, I'm sorry you dislike this, but you could have been better in your response.

424
00:45:31,000 --> 00:45:35,440
Right. And so there were it kind of went back and forth.

425
00:45:35,470 --> 00:45:37,570
At first, it was a lot of people saying, you know,

426
00:45:37,570 --> 00:45:44,500
adaptive randomization is great and here's why or here's what was wrong with the arguments of Hey Kimmelman, or here are the holes in their arguments.

427
00:45:45,550 --> 00:45:54,970
But I think it's. You know, it's useful to also think about what are what are the issues with response, adaptive randomization.

428
00:45:55,510 --> 00:46:03,520
So one of the main advantages is that can reduce the number of patients assigned to the inferior treatment arm, which I think we would all like.

429
00:46:03,640 --> 00:46:10,820
Right. It seems like that is a good ethical thing that less people in the trial will get the worst performing treatment.

430
00:46:10,840 --> 00:46:14,140
There will be less failures potentially.

431
00:46:14,890 --> 00:46:21,310
That seems like a very good thing. However, we don't right after the trial, you can see when that would be useful.

432
00:46:21,310 --> 00:46:31,000
But before the trial, we have no idea. And so there is this issue of equipoise and that was raised and has been talked about.

433
00:46:31,960 --> 00:46:35,590
And it just says that why would we if we have equipoise,

434
00:46:35,590 --> 00:46:42,220
why are we going to start changing it in this trial where we're trying to figure out, should, you know, what is the real answer?

435
00:46:42,430 --> 00:46:44,170
Let's wait till the end to figure that out.

436
00:46:45,190 --> 00:46:52,520
Also, if we can just implement this group sequential design, early decision rules, why do we need to play with the randomization probability?

437
00:46:52,540 --> 00:46:56,770
Let's just have options right to end the trial early based on the data.

438
00:46:57,970 --> 00:47:05,080
And so if we include early stopping rules, then there's doesn't seem like there is a big benefit from response adaptive randomization.

439
00:47:07,880 --> 00:47:14,000
There is that collective versus individual ethics argument saying that most of the patients who have this disease,

440
00:47:14,000 --> 00:47:17,540
who could benefit from the treatment are not in the trial. Right. The trial is a very small.

441
00:47:17,570 --> 00:47:22,820
Usually except for in rare diseases, a very small sample of those people who would actually get the treatment.

442
00:47:23,690 --> 00:47:30,110
And so if we can try to figure out, you know, we can if we have balanced randomization,

443
00:47:30,110 --> 00:47:35,870
we can likely run the trials faster and get that information out to patients faster.

444
00:47:36,740 --> 00:47:43,610
And so, again, it's an issue of potentially waiting for those adaptations.

445
00:47:45,800 --> 00:47:53,950
And then. There are a whole host of potential biases that can come in from the adaptations.

446
00:47:53,950 --> 00:47:58,540
If things are changing outside of the trial of the patients are changing, right?

447
00:47:58,540 --> 00:48:03,429
We're potentially introducing these issues, potential biases,

448
00:48:03,430 --> 00:48:11,380
which now we're going to have to correct for in either Bayesian or frequentist analyzes that aren't necessarily going to take all of the bias away.

449
00:48:11,410 --> 00:48:14,709
Right. There's always potential for residual confounding or something that we can't

450
00:48:14,710 --> 00:48:21,400
actually statistically account for and so we can end up with biased results.

451
00:48:23,140 --> 00:48:25,870
Okay. So I think it's important for you.

452
00:48:25,870 --> 00:48:33,550
I'm not I'm not saying that I hopefully I think that there are benefits to adaptive randomization in certain situations,

453
00:48:33,880 --> 00:48:37,540
and I think that there are also disadvantages to adaptive randomization.

454
00:48:37,840 --> 00:48:46,870
And so I think that with all things, you can't just have one hammer and one nail and just for every trial decide that that's what you're doing.

455
00:48:46,870 --> 00:48:53,890
You always have to think about the setting, the outcome, the resources that you have to decide if it's appropriate or not.

456
00:48:54,130 --> 00:49:02,320
And with all trials, hopefully you're going to run simulations under different scenarios like we especially showed in CRM designs,

457
00:49:02,320 --> 00:49:07,050
phase one designs, but also showed here to figure out would we make good decisions, right?

458
00:49:07,090 --> 00:49:11,530
Would this be worth it under all these potential potential settings that could happen.

459
00:49:11,950 --> 00:49:19,900
And then going from there, right? You can provide better evidence for why you choose one design over the other.

460
00:49:20,410 --> 00:49:27,300
So I'm not like team no adaptive randomization or team Bayesian versus frequentist, right?

461
00:49:27,310 --> 00:49:35,110
Like I think for all of those things, you just really need to think critically about your setting and make sure that you've gone through the

462
00:49:35,110 --> 00:49:41,710
motions as much as you can to see how what the outcome would be under these different potential options.

463
00:49:43,420 --> 00:49:49,020
Okay. Any questions? Yes. Emphasize is fixed beforehand.

464
00:49:49,350 --> 00:49:56,310
Yes. It's just the allocation ratio changes throughout. That's right. If sample size wasn't fixed, it's a good trend.

465
00:49:56,340 --> 00:50:02,580
Infinity. Yeah. What? That allocation ratio cut like trend towards like zero or one.

466
00:50:04,290 --> 00:50:07,890
Or is it. I mean, I guess. Right. Ultimately, like.

467
00:50:07,950 --> 00:50:11,940
Yeah. If you can figure if you can figure out that you actually have a better treatment.

468
00:50:12,600 --> 00:50:15,780
Right. Then you want into treatment.

469
00:50:15,930 --> 00:50:20,220
Yeah. Or is it stopped at like is there some boundary.

470
00:50:20,280 --> 00:50:26,219
Yeah. I mean, you could probably put some threshold on it, but I think if you truly have infinite,

471
00:50:26,220 --> 00:50:32,280
you would go toward if you have enough power to see that difference, you would automatically assume everyone to the better treatment.

472
00:50:32,790 --> 00:50:35,790
And if you don't, then you would always have 50% allocation.

473
00:50:37,670 --> 00:50:45,740
This seemed odd because by the end of the trial, you're saying treatment is better, statistically better.

474
00:50:45,800 --> 00:50:49,220
So you're deterministically saying deciding people treat me after that.

475
00:50:49,250 --> 00:50:54,500
Yeah. The timing and the trial, you're only still assigning, like, 70% of people to treatment a?

476
00:50:54,680 --> 00:50:58,850
Yeah. Yeah. Like how to reconcile that? I agree.

477
00:50:59,030 --> 00:51:05,210
Yes. But we never have infinite numbers, and so we never be in that situation.

478
00:51:05,630 --> 00:51:13,700
Okay, so let's kind of change topic here and we're going to talk about the importance of a protocol where protocol looks like very briefly,

479
00:51:13,970 --> 00:51:19,940
there's a protocol template on canvas which is useful in terms of if you actually go into the clinical trial world,

480
00:51:19,940 --> 00:51:22,010
every clinical trial has to have a protocol.

481
00:51:22,340 --> 00:51:28,700
And so there's a little template that if you, for example, start working with an investigator and they have,

482
00:51:29,180 --> 00:51:36,590
you know, a template now you can use way more importantly and more statistically are the deviations to the protocol.

483
00:51:36,600 --> 00:51:41,659
We're going to talk about the effects of those and also talk about as part of deviations or

484
00:51:41,660 --> 00:51:46,940
missing data and how we can deal with that a little bit without really talking about methods.

485
00:51:48,590 --> 00:51:55,070
And very briefly, I'm probably just going to tell you to look at these slides, not really go over them, is talking about how you report the trial.

486
00:51:56,650 --> 00:52:05,860
Okay. So the clinical trial protocol is a document that includes everything important for that trial.

487
00:52:06,160 --> 00:52:11,290
So it talks all about the design of the trial, why this trial is important.

488
00:52:11,590 --> 00:52:15,190
There's a whole background information that usually has everything from like basic

489
00:52:15,190 --> 00:52:21,069
science to the point where you've got now past previous studies that are related,

490
00:52:21,070 --> 00:52:26,620
right? This is all going to be in the beginning of the protocol and then it's going to explicitly say, now here is this trial that we're doing.

491
00:52:26,620 --> 00:52:32,020
Here's the design, here is the eligibility criteria, inclusion exclusion criteria.

492
00:52:32,650 --> 00:52:36,730
Here's how we're randomizing patients. Here's how we're going to analyze the data.

493
00:52:37,090 --> 00:52:46,209
Here is the sample size or power calculation for it. So this is like the the clinical trial Bible that sets it up and is completed before

494
00:52:46,210 --> 00:52:51,160
the trial starts so that everybody knows that this is how this trial is being run.

495
00:52:51,310 --> 00:52:55,540
Bless you. So it's a scientific document.

496
00:52:55,540 --> 00:53:07,389
It's also a manual of operation. This has to be approved by the Institutional Review Board and the internal review board, the IRP also,

497
00:53:07,390 --> 00:53:15,520
if it's a study that has drugs where somebody is giving you the drugs, that sponsor has to approve it.

498
00:53:16,480 --> 00:53:22,000
If it has a drug where you're hoping to get that drug approved later, the FDA has to approve this protocol.

499
00:53:22,780 --> 00:53:29,110
And then also, if you have to have a data safety monitoring board, which basically every trial does, they also have to approve this.

500
00:53:29,440 --> 00:53:36,550
So you have to write this entire document. You have to get it approved by many different boards before you can actually start to run the trial.

501
00:53:37,000 --> 00:53:43,960
Right. So there is so much set up before a trial actually happens and then the trial runs accrues data for a while.

502
00:53:44,440 --> 00:53:48,640
Right. And then you analyze the data. So this is part of the reason why trials take so long.

503
00:53:51,850 --> 00:53:59,860
So the background is first given and this is including preclinical studies, or it might be like social studies and or animal studies.

504
00:54:00,340 --> 00:54:05,560
We have to figure out, well, what could the adverse events be in this group for this treatment?

505
00:54:06,460 --> 00:54:10,810
And then we have to explain why we chose the study design. Why is it A versus B?

506
00:54:10,810 --> 00:54:14,440
Why is it an active treatment versus active treatment or why is it versus placebo?

507
00:54:14,440 --> 00:54:19,450
Or why is this a crossover design or a smart study or a factorial design?

508
00:54:19,450 --> 00:54:26,530
Right. Of these all these different designs that you would you'll hear more about during your projects, that all has to be there.

509
00:54:27,340 --> 00:54:31,840
And then you have to say, now, what are the study's aims or goals or purpose?

510
00:54:32,320 --> 00:54:38,170
How is it going to be conducted and explicitly write out your specific aims?

511
00:54:38,470 --> 00:54:42,160
Your primary aim, right is going to power that clinical trial.

512
00:54:43,330 --> 00:54:44,680
It has to be very thorough.

513
00:54:44,710 --> 00:54:51,550
It has to anticipate problems before they occur so that you can go back to this protocol and say, oh, this happened, what do I do?

514
00:54:51,850 --> 00:54:56,350
And the reason why we do that is because that has to be consistent for everyone, right?

515
00:54:56,350 --> 00:54:58,390
In order to have unbiased data.

516
00:54:58,780 --> 00:55:04,270
It's not that, oh, my gosh, this person had this event and that doctor decided they'd do this thing or take them off or whatever.

517
00:55:04,540 --> 00:55:08,829
Right. But over here. Oh, this person did something else. Well, that's going to affect the outcome, right?

518
00:55:08,830 --> 00:55:12,640
So we need to make sure that it's consistent as possible for every individual.

519
00:55:15,390 --> 00:55:23,969
Then Section nine, at least in the template, or at least there will be one full section dedicated to the statistical and analytic plans.

520
00:55:23,970 --> 00:55:28,650
And so you will write as the statistician will write this section, right?

521
00:55:28,710 --> 00:55:35,160
The investigator often writes, the rest of the protocol you'll have say you should look at it and read it to make sure it makes sense.

522
00:55:35,610 --> 00:55:39,630
But you would explicitly write the statistical and analytic plan section.

523
00:55:41,590 --> 00:55:45,040
And that statistical download plain section will include like a sample size power

524
00:55:45,040 --> 00:55:50,140
analysis and also then your analytic plan to address all of the aims in the protocol,

525
00:55:51,010 --> 00:55:54,719
how you discuss missing data, blah blah blah. Okay.

526
00:55:54,720 --> 00:55:58,510
So. Why all this?

527
00:55:58,780 --> 00:56:04,980
This Bible, this manual of operations is written. We inevitably have violations of it.

528
00:56:05,020 --> 00:56:12,220
So we thought we had all these possibilities. You know, we thought we had thought of what could happen and how to address it.

529
00:56:12,580 --> 00:56:19,630
But usually things happen that we didn't anticipate or it was accidentally not followed well.

530
00:56:20,950 --> 00:56:24,430
And so the protocol could be not completely followed.

531
00:56:24,640 --> 00:56:28,420
Sometimes we let people into the trial and randomize them potentially,

532
00:56:28,420 --> 00:56:33,610
even though they shouldn't have been in it, but they didn't actually meet the eligibility criteria.

533
00:56:34,780 --> 00:56:40,479
And then there's human error in terms of people say that they're in the trial and they agree to be in the trial,

534
00:56:40,480 --> 00:56:46,450
but then they're they don't show up to give you like outcome data or they didn't fill in all the survey.

535
00:56:46,450 --> 00:56:54,490
So there's inevitably missing data. So at the end, there's always a question of like, so who do we include in this analysis?

536
00:56:54,820 --> 00:57:01,899
And we already talked about before, right, intent to treat analysis versus per protocol analysis versus as treated analysis.

537
00:57:01,900 --> 00:57:06,760
And that very much needs to be written in the protocol about which one of those you're doing.

538
00:57:07,810 --> 00:57:10,720
And then you also need to make sure.

539
00:57:11,020 --> 00:57:18,549
Right, that if they were randomized, that they're if you're using an analysis that you're analyzing them as analyzed,

540
00:57:18,550 --> 00:57:22,570
even if they are missing something. Right. Or if they change treatments or whatever.

541
00:57:22,570 --> 00:57:31,000
Because if we start now excluding patients because they didn't give us their final outcome or for one reason or another,

542
00:57:31,000 --> 00:57:36,450
right now we're adding our own bias or some not our own. But now we're like, the data is going to be biased, right?

543
00:57:36,460 --> 00:57:43,330
If we start picking and choosing who's in this outcome. So there are there are two different kinds of like.

544
00:57:45,310 --> 00:57:49,180
Ways in which patients can not continue to be followed up.

545
00:57:49,660 --> 00:57:58,299
So one is what we call an exclusion. And that's actually that a lot of patients are individuals for trial, are screened to be in the trial.

546
00:57:58,300 --> 00:58:02,380
And then we learn that they're not eligible for the trial, and so they're excluded from the trial.

547
00:58:02,680 --> 00:58:08,230
These people are not actually even randomized. These are just people who we thought maybe could be in the trial and then are not.

548
00:58:09,650 --> 00:58:14,890
The the other. The other kind of patients who are.

549
00:58:16,730 --> 00:58:21,200
I guess I could not have full data at the end are those who we call are withdrawals.

550
00:58:21,230 --> 00:58:32,360
Now those patients actually are randomized, but they are maybe missing their outcome or some day they withdraw their consent at some point.

551
00:58:32,820 --> 00:58:39,620
Okay. And so there is this difference between the patients who are not randomized versus the patients who are randomized and how we handle those.

552
00:58:40,710 --> 00:58:44,810
Right. Obviously, if they weren't randomized, we don't include them into our. We don't have anything on them.

553
00:58:45,170 --> 00:58:50,870
Right. And that exclusion was made prior to anything any treatment being given to them or randomized to them.

554
00:58:50,870 --> 00:58:57,920
So exclusions can be excluded and they will not introduce selection or participation bias.

555
00:58:58,370 --> 00:59:01,580
So patients who are never randomized can be excluded.

556
00:59:04,030 --> 00:59:08,079
What's that going to affect? Will Obviously, it's going to mean that we don't have those patients, right.

557
00:59:08,080 --> 00:59:12,879
So we may not have the number of screen patients.

558
00:59:12,880 --> 00:59:16,000
We have the number of randomized patients, which could affect the power of the study.

559
00:59:17,140 --> 00:59:20,830
And more importantly, what it affects is the generalizability of the study.

560
00:59:20,830 --> 00:59:31,840
Right. For whoever we exclude. Then if we exclude all of one kind of group, then we can't really say that our results are generalizable to that group.

561
00:59:34,700 --> 00:59:40,880
So that's really important in terms of developing the inclusion exclusion eligibility criteria of the trial.

562
00:59:43,160 --> 00:59:49,900
And you can almost always see figure one of every clinical trial article that exists out there.

563
00:59:49,910 --> 00:59:52,580
Figure one is what we call a consort diagram,

564
00:59:52,940 --> 01:00:00,589
and it usually always says how many patients were screened or eligible for the trial versus how many patients were actually randomized.

565
01:00:00,590 --> 01:00:08,480
And so some of the trials that I'm involved in, they screen thousands and thousands of patients for like, you know, 100 something to be randomized.

566
01:00:08,870 --> 01:00:14,360
It can take a long it can take a while to figure out who, you know, to get the eligibility criteria.

567
01:00:16,610 --> 01:00:20,690
Also, you can screen patients and they can be eligible and then they can say, no, I don't want to be in your trial.

568
01:00:20,750 --> 01:00:24,050
Right. That's another way in which those those patients would be excluded.

569
01:00:24,950 --> 01:00:28,279
There's also there's the consort diagram, FIG. one, and then there's table one,

570
01:00:28,280 --> 01:00:35,630
which is always the table of demographics, which you should look at to see how generalizable is this trial.

571
01:00:35,660 --> 01:00:41,930
Right. What do the people look like in the trial and how does that relate to the people who would get this treatment out of the trial?

572
01:00:44,060 --> 01:00:53,720
If you if people withdraw from the study, either they stop coming to the study or they've said,

573
01:00:53,960 --> 01:00:57,410
you know, I no longer want to participate in the study or whatever.

574
01:00:58,790 --> 01:01:05,090
It's now you have this loss of information from groups and it can differ between the groups.

575
01:01:05,150 --> 01:01:09,799
Right. And the proportion of people who withdraw from each group could differ, which could be a problem.

576
01:01:09,800 --> 01:01:16,250
And also the reasons for withdrawal could be differ. And so the types of people who withdraw from each group could be really different.

577
01:01:16,250 --> 01:01:24,500
And so these withdrawal groups can actually really bias the data if you fully excluded them, right?

578
01:01:25,070 --> 01:01:30,020
The investigator would then have to convince you like, oh, well, they're the same, right?

579
01:01:30,020 --> 01:01:34,159
Like they were randomly, they randomly withdrew across both.

580
01:01:34,160 --> 01:01:42,229
And then you could say, okay, that's unbiased. Some reasons for withdrawals are that after randomization we randomize them.

581
01:01:42,230 --> 01:01:47,150
We thought they were eligible, but now we look at the data and realize, oh, they actually were eligible for this study.

582
01:01:47,150 --> 01:01:51,650
I'll give you some examples because you think like, how could that happen? It unfortunately can happen.

583
01:01:52,310 --> 01:01:58,160
Noncompliance. So they were supposed to be taking their treatment, but they start taking something else, right?

584
01:01:58,160 --> 01:02:02,990
Or they're just not compliant with giving you the data. So there's poor quality or missing data.

585
01:02:03,320 --> 01:02:08,360
And then there's also the possibility that there are competing events that end in the follow up.

586
01:02:08,360 --> 01:02:15,890
So for example, somebody could die, right? And so now we don't have the endpoint of interest because they're not there to give it to us.

587
01:02:18,650 --> 01:02:28,240
So. We we what we should do is thoroughly check eligibility before a trial and try really hard not to randomize somebody who's not eligible.

588
01:02:28,930 --> 01:02:39,460
But sometimes these errors occur because we didn't get some piece of information or it required like a lab test for the eligibility.

589
01:02:39,790 --> 01:02:47,680
And either we didn't get the test back in time or we got like a wrong result on that test that said that they were eligible when they're not.

590
01:02:48,610 --> 01:02:52,300
It could require like an X-ray or some image to be read.

591
01:02:52,630 --> 01:02:55,180
And there's always human error in the reading of images.

592
01:02:55,180 --> 01:03:01,180
They could have said, Oh yeah, it does look like they had a heart attack when actually somebody else reads it and says, No, that's, that's just.

593
01:03:02,590 --> 01:03:06,760
Weird heart rate. I don't know. Right? There's misclassification.

594
01:03:06,760 --> 01:03:11,049
So they could say, oh yeah, that person is disease stage three.

595
01:03:11,050 --> 01:03:15,460
But then again, somebody else comes and looks and says, No, no, no. That should have been stage two.

596
01:03:16,750 --> 01:03:24,459
I was recently on a it was a pilot study and they were looking to randomize individuals who were supposed

597
01:03:24,460 --> 01:03:29,320
to be taking a certain treatment and so in their medical file so that they are taking this treatment.

598
01:03:29,680 --> 01:03:34,030
But then when they actually called the patient, so they randomize them and then later they called the patient.

599
01:03:34,030 --> 01:03:39,700
The patient was like not in order to take that treatment, you know, like in your medical file, it tells you every time, like, are you on these drugs?

600
01:03:39,790 --> 01:03:43,929
Like what I took out, like eight years ago. Why is this still in my file? Right, like that?

601
01:03:43,930 --> 01:03:50,110
That still happens. And if you're saying that it has to deal with like the the health record, right.

602
01:03:50,110 --> 01:03:53,649
It could be wrong. So ideally, you wouldn't have these problems.

603
01:03:53,650 --> 01:04:02,830
But sometimes it happens again, ideally below the number of people that this would occur to would be low, but it could happen even more so.

604
01:04:02,830 --> 01:04:07,150
This was that a beta blocker heart attack trial.

605
01:04:08,020 --> 01:04:17,440
They required that everybody had some defined period after a heart attack and they did try to assess eligibility by a central unit.

606
01:04:17,440 --> 01:04:23,080
So this is one way in which you can try to stop this from happening, is instead of saying like, you know, if it's multi trial,

607
01:04:23,110 --> 01:04:27,400
if it's multi-centre or every center is doing their own assessment, they said, well, send them all to us.

608
01:04:28,240 --> 01:04:37,450
But the problem was because they had the central unit, there is often a several week delay of whether they were really eligible or not.

609
01:04:37,900 --> 01:04:44,560
And so sometimes somebody would have already decided, Oh, well, everything else looked good, we'll just put you on this study.

610
01:04:45,580 --> 01:04:53,409
And so it ended up that they were assigned to treatment, even though they hadn't gone for this through this full assessment.

611
01:04:53,410 --> 01:05:02,620
And so 9% of these patients actually didn't have their heart attacks confirmed or they weren't confirmed in this defined period of time.

612
01:05:04,180 --> 01:05:08,469
And so the protocol required follow up and analysis of all randomized patients,

613
01:05:08,470 --> 01:05:13,720
including this 9% of people who were included but maybe shouldn't have been included.

614
01:05:14,740 --> 01:05:17,740
And in this case, they did sensitivity analysis. They said, well,

615
01:05:17,740 --> 01:05:22,030
we'll include them and randomize them in this intent to treat analysis and then we'll

616
01:05:22,030 --> 01:05:26,530
exclude them and random and analyze the data and see if there are differences.

617
01:05:26,530 --> 01:05:31,480
And in this case, you know, it's less than 10%. I think this is a very large trial.

618
01:05:31,780 --> 01:05:35,050
And they found that the outcomes didn't change.

619
01:05:35,710 --> 01:05:38,410
And so they said, you know, well, that wasn't great.

620
01:05:38,410 --> 01:05:45,490
But luckily, there's not a lot to to have to defend here because we get the same the same results either way.

621
01:05:47,560 --> 01:05:52,990
Here's another example, again, having to do with heart attack.

622
01:05:53,500 --> 01:06:00,670
And so this was after patients are admitted to a hospital with a possible acute myocardial infarction.

623
01:06:00,670 --> 01:06:08,469
They were randomized to one of three treatments. And then in several cases, subsequent testing could not confirm the myocardial infarction.

624
01:06:08,470 --> 01:06:11,140
So they were like, we thought we had what they had one.

625
01:06:11,500 --> 01:06:15,670
But then they had to confirm the fact that they had a heart attack and they couldn't confirm that.

626
01:06:15,790 --> 01:06:21,729
Yes, if on the previous example. So they include people, they randomized them.

627
01:06:21,730 --> 01:06:27,220
And then if you've been randomized but you aren't actually eligible, isn't that data just missing at random anyway?

628
01:06:27,880 --> 01:06:32,860
So isn't your randomization still valid? So right.

629
01:06:32,860 --> 01:06:38,809
If you if you didn't know ahead of. Treatment assignment, right?

630
01:06:38,810 --> 01:06:42,950
Your treatment assignment doesn't depend on that eligibility.

631
01:06:42,950 --> 01:06:47,209
It shouldn't matter, right? Which is part of why they were like, well,

632
01:06:47,210 --> 01:06:53,510
we'll just analyze this data because it's not like everybody who wasn't ineligible was in one group or another.

633
01:06:53,510 --> 01:06:58,100
Right? So yeah, it seems like excluding them would have been fine as well.

634
01:06:59,060 --> 01:07:02,990
But just in case they said, well, we said we were going to analyze everyone who was randomized.

635
01:07:02,990 --> 01:07:10,550
That's an intent to treat. So we'll do that. But then we think that it was random and so we'll also analyze it without.

636
01:07:15,300 --> 01:07:18,540
And so here they again.

637
01:07:18,540 --> 01:07:22,260
They didn't. Actually, they couldn't.

638
01:07:23,780 --> 01:07:34,370
They couldn't confirm this heart attack. And so some subjects either didn't have one or they could have had one may have had this

639
01:07:34,370 --> 01:07:38,270
heart attack arrested immediately by the therapy and thus didn't appear to have it.

640
01:07:38,570 --> 01:07:44,360
And there was no way to distinguish between those who actually didn't have a heart attack or those who had a heart attack but were treated well.

641
01:07:44,370 --> 01:07:45,440
So it didn't look like they did.

642
01:07:46,340 --> 01:07:56,809
And so if they took away all these patients, if they said, well, those patients who didn't fit this criteria, we just will exclude them.

643
01:07:56,810 --> 01:08:05,360
It could potentially bias the analysis because those patients who actually got the treatment and then we couldn't tell, right.

644
01:08:05,360 --> 01:08:08,930
We can't distinguish them from those who just didn't have one.

645
01:08:09,260 --> 01:08:13,069
And so there could be more patients on the treatment arm who got the treatment.

646
01:08:13,070 --> 01:08:17,690
And we couldn't tell that they had a heart attack than those on the placebo arm.

647
01:08:17,900 --> 01:08:22,580
And so since we can't tell the difference between these two types, one of which which might be because of the treatment,

648
01:08:22,880 --> 01:08:26,030
we have to be careful about just fully excluding those individuals.

649
01:08:29,550 --> 01:08:34,320
Truly ineligible patients, right? Those who just didn't have one should be equal across the arms.

650
01:08:34,650 --> 01:08:39,900
But those who got the treatment and did better right would only be in the treatment arm and not on placebo.

651
01:08:39,900 --> 01:08:43,110
And since we can't tell the difference, we have to just leave everybody in the analysis.

652
01:08:45,560 --> 01:08:50,180
So if we have if we figure out that there is that patients were ineligible.

653
01:08:50,390 --> 01:08:55,670
Right. We should make a decision as to what we're doing with that patient as soon as possible.

654
01:08:56,060 --> 01:09:03,680
And we should hopefully be blinded to treatment so that we're not making a decision based upon which treatment are in the policy for withdrawal,

655
01:09:03,680 --> 01:09:05,940
as has to be clearly stated in the protocol.

656
01:09:06,950 --> 01:09:15,799
And then even then, people could be worried about the the results because they could say, well, you know, this is what happened.

657
01:09:15,800 --> 01:09:19,430
And now you're analyzing a subset. And that subset wasn't all those who are randomized.

658
01:09:19,430 --> 01:09:23,310
So we have to be really careful here. Okay.

659
01:09:24,610 --> 01:09:29,140
So the withdrawals are due to like.

660
01:09:33,270 --> 01:09:36,780
Those withdrawals were due to post randomization detection of an eligibility.

661
01:09:36,780 --> 01:09:41,370
But there are other types of withdrawals dealing with noncompliance, right.

662
01:09:41,370 --> 01:09:46,560
And so noncompliance can look different as well. It can be that people just drop out of the study,

663
01:09:46,980 --> 01:09:51,840
are no longer interested in participating and don't want to take this treatment anymore and not doing that right.

664
01:09:51,840 --> 01:09:58,350
Or it could be like there are participants in the control arm who somehow figure out a way to get the treatment right.

665
01:09:58,680 --> 01:10:01,830
And you can imagine this is easier when it's like behavioral treatment,

666
01:10:01,920 --> 01:10:06,930
not when it's this would be harder if it's drug treatment or that drug isn't approved.

667
01:10:07,230 --> 01:10:10,290
It'd be harder to get that. But it could be possible that they could say.

668
01:10:11,600 --> 01:10:14,600
I don't know. I will no longer be a part of this study.

669
01:10:14,840 --> 01:10:18,800
Please give me that treatment or something. So they're dropouts and they're drop ins.

670
01:10:19,790 --> 01:10:23,870
And this decision can be made by the patient. It could be made by the clinician.

671
01:10:25,100 --> 01:10:28,489
Right. It could be made by somebody else involved in the trial.

672
01:10:28,490 --> 01:10:31,640
So it's not just always like a we shouldn't always blame the patient. Right.

673
01:10:31,670 --> 01:10:37,010
It's not just a patient problem. It could be the clinician guiding them in one way or another, too.

674
01:10:39,020 --> 01:10:44,120
And so often noncompliance. One of the biggest reason is like, Oh, I'm taking this treatment and this treatment sucks, right?

675
01:10:44,150 --> 01:10:46,820
Like it's giving me these extra effects. I hate these effects.

676
01:10:47,090 --> 01:10:51,919
Or I don't want to take this treatment this many times, or I don't want to go to the facility to get this treatment.

677
01:10:51,920 --> 01:10:56,270
No, thank you. I'm done. Right. That's probably the biggest noncompliance.

678
01:10:57,800 --> 01:11:02,390
Another one is just, you know, I signed up for this and now I don't want to go like.

679
01:11:02,510 --> 01:11:08,320
Right. All these 61 students signed up for this class and get myself right.

680
01:11:08,750 --> 01:11:11,990
Just like noncompliant, like it's cloudy. Not going to class.

681
01:11:12,310 --> 01:11:15,990
Right. Same thing with drugs. Oh, this is like, not. Don't want to be in this trial.

682
01:11:16,330 --> 01:11:19,850
The thing or something could happen.

683
01:11:19,850 --> 01:11:22,960
We're like, they're taking it, and now they're automatically feeling better.

684
01:11:23,330 --> 01:11:26,860
And so they're like, Oh, me this, right? I'm better. Like, this is great.

685
01:11:26,870 --> 01:11:30,020
I'm something has changed or the opposite.

686
01:11:30,320 --> 01:11:33,620
I'm taking it. And now like something with the disease went way worse.

687
01:11:33,950 --> 01:11:38,420
And so, like, maybe it's not, it's not a good idea to continue taking that treatment.

688
01:11:41,030 --> 01:11:50,059
And so if we find out that people are noncompliant, it would be quite easy to be like, well, they weren't actually taking the treatment, right?

689
01:11:50,060 --> 01:11:55,580
So let's just exclude them because they weren't doing what they were supposed to be doing in these in this arm.

690
01:11:56,480 --> 01:12:00,740
But as we know. Right. That can lead to bias. So we randomize patients.

691
01:12:00,740 --> 01:12:04,969
There is supposed to be balance across the groups on these covariates.

692
01:12:04,970 --> 01:12:09,740
And now all of a sudden, if we start cherry picking certain people out, we're messing up that balance.

693
01:12:10,280 --> 01:12:17,870
Right. And the compliance is likely often is related to the intervention.

694
01:12:17,870 --> 01:12:23,449
It doesn't have to be right. It maybe maybe you know, I'm not going to take it personally that people don't come to this class.

695
01:12:23,450 --> 01:12:27,100
I don't think it's because of me. But I know it's nice out or I know it's like a firewall.

696
01:12:27,210 --> 01:12:30,560
Right. So so it doesn't have to be related to the intervention, but it may be.

697
01:12:32,360 --> 01:12:36,919
And so if compliance is especially if it's related to the intervention, right.

698
01:12:36,920 --> 01:12:40,370
And now we start excluding those people, we definitely have biased results.

699
01:12:41,060 --> 01:12:50,930
Um. So this is just one example where we randomize to no treatment versus chemotherapy.

700
01:12:51,290 --> 01:12:58,609
And if we all now want to analyze the data and say like, well, all these people are in the chemotherapy arm,

701
01:12:58,610 --> 01:13:02,000
but a bunch of them didn't actually get the amount of chemotherapy they're supposed to get.

702
01:13:02,420 --> 01:13:06,200
Right. We only want to look at the data when they get the amount that they are supposed to get.

703
01:13:06,710 --> 01:13:12,620
Now we're all of a sudden requiring we're going to knock it down to the people who stayed with the chemotherapy,

704
01:13:12,620 --> 01:13:17,750
where they're healthy enough to continue or motivated or driven enough to get that chemotherapy,

705
01:13:17,780 --> 01:13:21,110
which are going to exclude anybody who had side effects that stopped.

706
01:13:21,470 --> 01:13:26,959
We're getting into a very specific subset of people who could take this treatment, and we can bias bias.

707
01:13:26,960 --> 01:13:35,420
The effect the more complex therapies get there is usually a greater opportunity for protocol violations.

708
01:13:35,780 --> 01:13:40,399
So this really means to take you need to take this into account before the trial starts.

709
01:13:40,400 --> 01:13:44,420
And you also need to make sure that you have, like an appropriate control group here.

710
01:13:46,670 --> 01:13:51,469
But that's why you don't often see trials where the treatment.

711
01:13:51,470 --> 01:13:57,440
Right. Is like super elaborate with lots of different lots of different pieces to it.

712
01:13:57,450 --> 01:13:58,950
Right. We usually want to figure out like,

713
01:13:58,970 --> 01:14:04,370
what's the most what's the effective pieces of this treatment so that we give as little as possible so that there's

714
01:14:04,370 --> 01:14:11,270
less opportunities for people to to take liberties into what they're getting or to violate the treatment protocol.

715
01:14:12,050 --> 01:14:15,800
Okay. So here are some examples about how compliance affects results.

716
01:14:16,310 --> 01:14:25,520
So this is the coronary drug project where they're looking at the percent of five year mortality in patients between patients who got some treatment,

717
01:14:26,450 --> 01:14:30,230
slow, separate, I don't know, versus placebo.

718
01:14:30,500 --> 01:14:38,420
Okay. And overall, these are the results that there is 18.2% mortality in the drug group and 19.4 in the placebo group.

719
01:14:38,550 --> 01:14:42,800
Right. The higher the worse. Now, if we look at compliance,

720
01:14:42,800 --> 01:14:52,570
so if we either had a way to figure out who was taking the drug or we either counted pills or they had to come to the clinic, or we ask them, right.

721
01:14:52,580 --> 01:15:01,160
We can see if they are highly compliant. There's actually equal mortality rate, even if they're a highly compliant with placebo.

722
01:15:01,340 --> 01:15:04,940
Right. They were doing better than overall.

723
01:15:05,360 --> 01:15:15,170
Now, if they weren't compliant right then we can see that there was a higher mortality rate on the placebo.

724
01:15:16,790 --> 01:15:17,989
But it's kind of interesting, right,

725
01:15:17,990 --> 01:15:24,590
part of that placebo effect that if you sometimes just complying with the treatment can result in a better outcome.

726
01:15:26,660 --> 01:15:30,440
Here's another example. This is, again, mortality. So higher is worse.

727
01:15:30,860 --> 01:15:35,960
In a three arm study of beta blockers, two of them are beta blockers versus a placebo.

728
01:15:36,290 --> 01:15:42,740
Now, if we look at compliance, right, it looks like those who take these drugs are doing way better than those in placebo.

729
01:15:42,770 --> 01:15:51,950
Right. The mortality rate is way lower. If we look at the noncompliance, the mortality rate in the drug group is higher than placebo.

730
01:15:54,250 --> 01:16:00,920
And so it's pretty interesting that the treatment noncompliance have some side,

731
01:16:01,090 --> 01:16:07,540
some sort of like worse risk of mortality than those in the placebo group.

732
01:16:07,870 --> 01:16:11,259
So if we just did an analysis based on the compliance,

733
01:16:11,260 --> 01:16:16,210
we would have a very different conclusion than if we looked at the average over

734
01:16:16,390 --> 01:16:20,380
the the treatment and or individually in the compliance versus noncompliance.

735
01:16:21,430 --> 01:16:27,220
But it's just, again, showing you, right, this this comparison, just that compliance is going to be biased of the whole effect.

736
01:16:29,740 --> 01:16:36,910
And so the problem is that because compliance can be related to the intervention, right?

737
01:16:36,970 --> 01:16:41,230
Compliance is an outcome and it's also a predictor.

738
01:16:41,740 --> 01:16:45,670
So it has the circular effects that can bias are our results.

739
01:16:48,340 --> 01:16:53,559
And while if you comply, you do great write is useful information.

740
01:16:53,560 --> 01:16:57,080
If not, a lot of people can comply because the treatment is really hard.

741
01:16:57,110 --> 01:16:58,990
Right. That's also very useful information.

742
01:17:00,130 --> 01:17:07,060
And so while it could be effective if you can't actually have a lot of people tolerate or comply with the treatment,

743
01:17:07,060 --> 01:17:12,010
it might not be something that you actually want in public for most people.

744
01:17:13,210 --> 01:17:19,360
Okay. Here's just another example. This is an antidepressant trial where poor compliance was related to side effects.

745
01:17:19,810 --> 01:17:23,530
And so this is the new drug versus like a standard of care.

746
01:17:24,430 --> 01:17:31,880
And these are numbers of individuals treated. This is. So on the new drug, eight people said it was very effective.

747
01:17:31,910 --> 01:17:41,260
Two said it was effective and two said it was ineffective. So that's 12 total people and eight people withdrew from the standard of care.

748
01:17:41,270 --> 01:17:45,170
Six people said very effective. Eight people said effective, zero said ineffective.

749
01:17:45,200 --> 01:17:54,859
That was a total of 14 people. Only one person withdrew. If I look at the compliance effect, it's this new drug.

750
01:17:54,860 --> 01:17:58,970
It has a 67% very effective rate.

751
01:17:59,090 --> 01:18:05,780
Write it out. 12. Whereas the center of care has only a 43% very effective rate.

752
01:18:06,320 --> 01:18:14,240
However, if I look at everybody who is randomized, right, if I include those people who withdrew right as they were there,

753
01:18:14,240 --> 01:18:24,230
but we assume that they would have said either effective or non effective right now that my very effective percentage is equal in both groups.

754
01:18:24,320 --> 01:18:31,970
So again, this is somewhere where like, well, clearly this drug was hard to tolerate or that wasn't something that people wanted to comply with.

755
01:18:32,600 --> 01:18:37,700
And so while it might be effective if you comply, if you can't get people to comply, this maybe isn't right.

756
01:18:37,700 --> 01:18:43,490
It's not going to have an overall good effect on most people or on many people.

757
01:18:49,010 --> 01:18:54,440
So one way to try to. Well, first, right. You should try to collect information on compliance.

758
01:18:54,470 --> 01:18:59,090
You should try to as much as possible, have it so that patients don't not comply.

759
01:18:59,100 --> 01:19:04,549
But that's sort of unavoidable. And once now, if people start not complying,

760
01:19:04,550 --> 01:19:07,940
we have this potential loss of power because we don't have as many people getting the

761
01:19:07,940 --> 01:19:11,450
treatment that we actually thought we're getting the treatment in the way we wanted them to.

762
01:19:11,930 --> 01:19:17,690
And so one way that we can account for this is that if we know that there's going to be a certain percentage of noncompliance,

763
01:19:18,080 --> 01:19:23,010
we can essentially inflate our sample size for that percentage of noncompliance.

764
01:19:23,360 --> 01:19:29,600
So we often do this with like we know that certain amount of people are going to drop out, so we'll inflate our sample size.

765
01:19:30,670 --> 01:19:41,320
Okay. That's another example. This is a saying try to minimize noncompliance and encourage compliance.

766
01:19:42,430 --> 01:19:48,670
This is another example. And then I'll talk a little bit about missing data when we start.

767
01:19:49,870 --> 01:19:53,410
Yeah, we'll still go through that when we start Tuesday.

768
01:19:53,770 --> 01:19:58,150
But I also just want to make sure that you especially look at this last slide.

769
01:19:58,540 --> 01:20:09,009
So this last slide is trying to bring your master's education all together and showing you how these how these classes are actually useful.

770
01:20:09,010 --> 01:20:12,850
So hopefully you've seen a little bit that in clinical trials, but like what we're,

771
01:20:12,970 --> 01:20:18,129
what we're trying to do here and what this lecture especially was trying to give you this like intuition of

772
01:20:18,130 --> 01:20:24,460
like bias can exist and when and how does it exist and how can we try to prevent it and how can we fix,

773
01:20:24,490 --> 01:20:27,520
you know, attempt to treat it or fix it with statistics?

774
01:20:28,930 --> 01:20:33,820
And so one way, you know, one of the ways in which we see bias is that we're missing data.

775
01:20:34,510 --> 01:20:38,660
But if we have longitudinal data, then and we assume that's missing at outcomes,

776
01:20:38,680 --> 01:20:43,210
we can apply all the things you learned in 653 and use mixed methods, right?

777
01:20:43,230 --> 01:20:49,360
Another thing is that if data is missing and we believe that we can actually impute the data,

778
01:20:49,810 --> 01:20:54,980
then we can use things that you learn from missing data, which maybe is 80.

779
01:20:55,000 --> 01:20:59,559
I think there's also a 600 level missing data classes and there are no, no, no.

780
01:20:59,560 --> 01:21:03,129
That said, well, if you stick around, you can take it any right.

781
01:21:03,130 --> 01:21:09,130
And you leave, you'll you'll learn multiple imputation which can address missing data as well.

782
01:21:09,730 --> 01:21:13,540
If you take 675 next semester, survival, right?

783
01:21:13,540 --> 01:21:21,639
If you have time to event outcomes, you can realize how we can deal with sensor data and if that censoring is dependent.

784
01:21:21,640 --> 01:21:29,530
So if we're not seeing the outcome because of something about the intervention, right, we can deal with that analytically in survival.

785
01:21:29,950 --> 01:21:34,419
So the whole goal is that you have this intuition about bias and then you can kind of

786
01:21:34,420 --> 01:21:40,120
use toolkits that you got from all of your classes here to apply to these problems.

787
01:21:40,330 --> 01:21:46,059
And you often, even if you didn't learn it in class, you're now have those skills to like go look up like,

788
01:21:46,060 --> 01:21:50,170
Oh, there's bias now I can look up, I can Google this thing and then try to figure out how to fix it.

789
01:21:51,010 --> 01:21:59,260
Okay. So it comes together here, but it's going to come together also as you go out and you don't realize how it all into relates.

790
01:21:59,920 --> 01:22:04,510
Maybe now, but you will see as you start practicing in the real world.

791
01:22:05,200 --> 01:22:13,270
Okay, we'll meet Tuesday, have finished this lecture and start crossover designs and then Thursday you have your second.

792
01:22:14,560 --> 01:22:19,570
Both of them are designed to be on the agenda. They will not go missing due to stuff.

793
01:22:19,570 --> 01:22:24,520
My feelings in that will talk about Tuesday but not. All right.

794
01:22:24,700 --> 01:22:42,770
You take Martin. Do you take it away? No, no, no, no, no, no, no, no, no, no.

795
01:22:43,120 --> 01:22:52,840
Fear makes me feel like one of the things I was doing.

796
01:22:54,580 --> 01:22:59,440
So what do you guys actually do out there?

797
01:22:59,500 --> 01:23:04,510
So I like to stay, so I don't know.

798
01:23:04,790 --> 01:23:10,360
My we are actually up with that.

799
01:23:12,190 --> 01:23:17,010
Yeah. Okay, that's pretty good if you're hiding it.

800
01:23:17,020 --> 01:23:26,560
At least it says that it needs to have an interaction to make sense to have to from 3:00.

801
01:23:26,980 --> 01:23:37,990
I don't know. I like a it is that you don't seem to be able to gain power on me.

802
01:23:38,980 --> 01:23:45,880
So because of our own interaction interactions. So I think we have to see it.

803
01:23:45,930 --> 01:23:59,290
Usually he doesn't like the way we calculate here, so it's a little bit of a social interaction,

804
01:23:59,710 --> 01:24:06,370
but generally there is an interaction still likely to have done with the game.

805
01:24:08,470 --> 01:24:15,520
So you wouldn't be our first interaction, but still might be different to each other anymore.

806
01:24:15,520 --> 01:24:26,950
But if you want Safari one, you can say, Oh, he really thought the sample size would be larger than if you did.

807
01:24:27,790 --> 01:24:34,040
In other words, if you're to.

808
01:24:35,340 --> 01:24:40,080
That's why you can talk to me.

809
01:24:42,600 --> 01:24:51,960
So that's. So that's cluster randomization. So, yeah, we didn't really think about it,

810
01:24:52,380 --> 01:25:01,050
but you can sort of randomizing patients like randomize clinicians or it doesn't exclude sites that that doesn't

811
01:25:01,050 --> 01:25:09,380
necessarily help clients because the physicians or hospitals have plans along with the physician randomization.

812
01:25:13,320 --> 01:25:16,139
So if you have a cluster, if it's cluster requisition,

813
01:25:16,140 --> 01:25:22,650
so it's at the level of clinician or outside that you have to do, that's not all you're doing with the account.

814
01:25:23,310 --> 01:25:30,040
So usually not, for example, hierarchical models really makes models to analyze the data.

815
01:25:38,660 --> 01:25:58,840
You know, it's not that it's not.

816
01:26:09,630 --> 01:26:13,040
They're going to see what goes on here.

817
01:26:15,260 --> 01:26:36,830
But it was nice. No matter how much I wasn't going to look, I'm going to tell you guys.

818
01:26:38,540 --> 01:26:42,040
They said, Oh, no, we're going to do our best to deal with it.

819
01:26:42,180 --> 01:26:51,200
You know, basically like there's a there's a clear difference between before and after.

820
01:26:51,470 --> 01:26:57,050
But that one variable that's actually significant doesn't explain all of that difference.

821
01:26:57,260 --> 01:27:00,800
The difference between for that variable is too small and close to one.

822
01:27:01,860 --> 01:27:09,980
So I need to figure out, like. Because we never interpret answer to if you want to get that useful and that.

823
01:27:10,920 --> 01:27:18,460
That interaction term should be significant. Right. But it's the only 250 miles.

824
01:27:19,220 --> 01:27:26,520
Explain the before and like the actual interpretation of the fine.

825
01:27:29,340 --> 01:27:33,060
Like it. We can wait. You can explain. Where can I go to then?

826
01:27:33,300 --> 01:27:37,950
ESP one, esp to using this one or no using this rule.

827
01:27:38,070 --> 01:27:43,740
How can not? You know, you can't you can't go the word and go all the way that way.

828
01:27:43,820 --> 01:27:47,580
No, there are set doors in this classroom.

829
01:27:47,940 --> 01:27:54,060
Maybe not this classroom. No, but this one, even though you know there's someone.

830
01:27:54,550 --> 01:27:57,840
No one. We're fully in that speech. What are you doing here?

831
01:27:59,370 --> 01:28:08,399
What am I doing here? Yeah, we're having girls talk, recognize and talk, and he thinks the position is too large.

832
01:28:08,400 --> 01:28:14,030
And I'm like, I can't talk without the most okay without them.

833
01:28:14,070 --> 01:28:17,220
But I can use all we're not thinking about like.

834
01:28:18,730 --> 01:28:27,450
Voting you out of this group? Yeah. Well, why can't I own all the band's brands?

835
01:28:28,240 --> 01:28:32,690
It's me who start the family. It's all in your name.

836
01:28:33,470 --> 01:28:36,650
What about what? The brand name is only your name.

837
01:28:36,680 --> 01:28:40,040
You should also listen or I will kick you out. Cut that.

838
01:28:41,210 --> 01:28:49,890
Cut that [INAUDIBLE]. Yes. The Kardashians, not just Kim Kardashian and the.

839
01:28:50,440 --> 01:28:57,610
No. Where are you going? I'm trying to find you and it's backward, but I can tell you this way.

840
01:28:58,130 --> 01:29:02,500
Yes, you can use it. Oh, yeah, you're right there.

841
01:29:03,550 --> 01:29:08,700
I don't. You should check the of list closer to the have.

842
01:29:09,820 --> 01:29:13,780
White and black. Like any white.

843
01:29:19,040 --> 01:29:23,600
Do you know how to tackle this virus attacks?

844
01:29:23,690 --> 01:29:31,700
It asks for logs of Bernoulli distribution and the perimeter of the brain in distribution follows another distribution.

845
01:29:32,210 --> 01:29:37,310
Then how do you calculate the variance of x? Wait, wait.

846
01:29:37,340 --> 01:29:48,200
How are our normal number? Newly collected x is for doing x distribution and the pattern either observation and distribution.

847
01:29:48,950 --> 01:29:53,150
Oh no, no, no, no, no.

848
01:29:54,230 --> 01:29:55,990
That sounds like a game.

