1
00:00:05,400 --> 00:00:08,979
I probably. Okay.

2
00:00:08,980 --> 00:00:14,650
If you have not done report card, please do click for those. Or if you just did like one per group, that's fine too.

3
00:00:14,650 --> 00:00:20,170
But. And you had a really.

4
00:00:35,640 --> 00:00:40,710
Okay. We'll start in the way, way back. I forget your group name.

5
00:00:45,210 --> 00:00:49,350
Good luck. So what do you think about this question?

6
00:00:49,380 --> 00:00:54,450
All six trials showed a significant difference between eradication therapy and control treatment.

7
00:00:54,870 --> 00:01:12,019
True or false? Yeah.

8
00:01:12,020 --> 00:01:15,770
I mean, I think all of them include one except maybe this one right here,

9
00:01:16,130 --> 00:01:20,540
but all of them do it just to kind of like orient yourself to how course plot works.

10
00:01:20,870 --> 00:01:27,169
Sometimes there's a particular order to them, sometimes not, and oftentimes it's done in order of time.

11
00:01:27,170 --> 00:01:39,200
So here they're they're doing earliest ones on the top. Generally, the smaller the like main dot is, that means that's the smaller the study.

12
00:01:39,530 --> 00:01:46,219
So this one is really big because, you know, there's thousands of people enrolled in it, whereas in these other ones there's fewer people.

13
00:01:46,220 --> 00:01:53,160
So they're smaller dots. So they could bigness of it is sort of a visual reminder of how much weight can we take to this?

14
00:01:53,180 --> 00:01:59,410
Okay. The forest plot is drawn on a logarithmic scale.

15
00:02:01,390 --> 00:02:06,340
The pieces are bigger. What did you say? We said this.

16
00:02:09,050 --> 00:02:12,290
Yeah. And why would you put it on a logarithmic scale? So what?

17
00:02:12,740 --> 00:02:19,550
Oh, why would you put it on a logarithmic scale? Because the reservation for the next year delay.

18
00:02:22,810 --> 00:02:31,480
Yup. Exactly what is in the middle. So again, whenever you use very few measures, try to include this on a logarithmic scale.

19
00:02:31,630 --> 00:02:34,510
There was something that confused me a couple of weeks ago when we looked at the value.

20
00:02:34,520 --> 00:02:38,889
They didn't have the the logarithmic scale and I was a bit confused as a result.

21
00:02:38,890 --> 00:02:42,880
But that's, that's beyond the scope of it. Um, okay.

22
00:02:43,880 --> 00:02:49,959
A risk. So I'm sorry. This is, this is Dr. Sedgwick doing weird wording for questions,

23
00:02:49,960 --> 00:02:58,930
but a risk ratio greater than one indicates an increased risk of gastric cancer with the control treatment compared with eradication therapy.

24
00:03:00,040 --> 00:03:06,230
What does this group say? That calls.

25
00:03:07,400 --> 00:03:22,660
Part of it should be. Yeah.

26
00:03:22,670 --> 00:03:25,819
So I mean, and this is this is like one of those things with sports plots,

27
00:03:25,820 --> 00:03:32,389
which are like some of the clinical wording behind this is a bit different than how I would refer to things in epidemiology.

28
00:03:32,390 --> 00:03:37,280
But, you know, with this plot they're talking about like favoring control this way just because

29
00:03:37,280 --> 00:03:42,470
there's a higher risk among those who are treated with like the special treatment.

30
00:03:42,860 --> 00:03:50,960
And then, you know, there would be a lower risk in the treatment group if it was less than one.

31
00:03:53,570 --> 00:04:02,570
Okay. Um, so d let's grow to greet group h happy students.

32
00:04:02,570 --> 00:04:31,600
What do you think? Cheerful. Okay.

33
00:04:31,720 --> 00:04:35,050
So we have this as .66.

34
00:04:36,040 --> 00:04:40,000
Um, I think they still are comparing it to the control here.

35
00:04:40,030 --> 00:04:44,500
So I would I think I would have said that.

36
00:04:48,240 --> 00:04:49,580
Yeah. I would have said true here.

37
00:04:51,240 --> 00:04:57,030
And again, just looking at this, what we're trying to say is like, is the risk higher if you're using the medication therapy or not?

38
00:04:58,230 --> 00:05:02,820
And clearly, the numbers .66. So it's just thinking about like what is the reference group?

39
00:05:02,820 --> 00:05:05,070
And I think the reference group still is control for this.

40
00:05:05,080 --> 00:05:12,420
So we're favoring eradication here because the risk of gastric cancer is point six, six times as high.

41
00:05:12,750 --> 00:05:16,830
So it's it's like less, less ha. Does that make sense?

42
00:05:17,010 --> 00:05:20,240
Or did other did other groups have confusion about this?

43
00:05:20,250 --> 00:05:23,830
I will say, yeah. What is the practice?

44
00:05:29,400 --> 00:05:33,180
It's kind of like, what is your what is your reference?

45
00:05:33,510 --> 00:05:38,880
So I wish I had a great team down.

46
00:05:43,270 --> 00:05:46,450
So I mean, the whole thing with the risk ratio is that you have like, you know,

47
00:05:46,570 --> 00:06:01,780
one value like a risk a and then you have that over risk B And so usually the risk people will consider some sort of control or placebo,

48
00:06:01,780 --> 00:06:06,249
but it doesn't need to be, especially for like considering two treatments which, you know,

49
00:06:06,250 --> 00:06:09,550
there might be clinical equipoise, like we don't know which one would be better or not.

50
00:06:11,530 --> 00:06:16,360
But yeah, so if we are comparing to see,

51
00:06:16,370 --> 00:06:23,140
like if we are comparing like a new treatment and here they're trying to compare like eradication to some sort of controlled placebo.

52
00:06:25,330 --> 00:06:38,340
In this situation, if it's less than one, then that would mean that, you know, we, we favor eradication, meaning that like eradication is protected.

53
00:06:39,580 --> 00:06:43,090
Is that what you're getting at? Yeah. I mean, it's a great question.

54
00:06:43,090 --> 00:06:49,569
And I. They include some of these questions in just cause.

55
00:06:49,570 --> 00:06:57,430
Like, you know, I think as epidemiologists, we're I would say, like we're a bit more straightforward with how we talk about exposures than outcomes.

56
00:06:57,430 --> 00:06:59,950
But you will, if you like, read the clinical literature.

57
00:07:00,250 --> 00:07:04,750
Sometimes you do need to think those questions of like what actually like what are they comparing?

58
00:07:04,750 --> 00:07:10,480
And then what does it actually mean? Is this a positive outcome or a less positive outcome?

59
00:07:11,200 --> 00:07:14,530
And those things can get confusing quickly. So I get the motivation of the question.

60
00:07:14,650 --> 00:07:20,050
It's a good question. Okay.

61
00:07:23,320 --> 00:07:30,910
Let's end up here in front. Significant heterogeneity existed between the sample estimates of the population risk ratio of gastric cancer.

62
00:07:31,390 --> 00:07:38,660
What did you all see? Because the.

63
00:07:41,980 --> 00:07:50,770
Yeah. So there's two ways of looking at this. So you can visually look at this in kind of all the confidence intervals encompass each other.

64
00:07:50,770 --> 00:08:01,810
So like it doesn't seem that any of these studies is like saying something that's incorrect or which is or like not in line with other studies.

65
00:08:02,350 --> 00:08:12,430
Um, but we can also do a formal test and that through this test for heterogeneity and as you mentioned, this test is not significant.

66
00:08:12,820 --> 00:08:19,960
It's a p value of 0.6. So that's kind of like a dichotomous test of, you know, let's look at that.

67
00:08:19,970 --> 00:08:28,510
The hypothesis testing based on an alpha level of five. I mean, if it was less than .05, we'd say there was heterogeneity and heterogeneity.

68
00:08:28,510 --> 00:08:32,200
And this situation would be like, oh, some studies found that, you know,

69
00:08:32,200 --> 00:08:38,140
eradication was like highly productive and some studies found that it was highly deleterious or whatnot.

70
00:08:39,910 --> 00:08:44,709
The Higgins I squared is more of a, you know,

71
00:08:44,710 --> 00:08:49,940
a quantitative approach with like a continuous value and it's looking at the amount of heterogeneity there is.

72
00:08:49,940 --> 00:08:55,240
So that could range from 0 to 100%. But here is the thing. There's there's absolutely no heterogeneity whatsoever.

73
00:08:59,490 --> 00:09:08,549
So this story here is that, you know, before this before this MIT analysis was conducted, there were,

74
00:09:08,550 --> 00:09:18,270
you know, four or five or six studies that had found no protective effect of this treatment of this age.

75
00:09:18,270 --> 00:09:19,320
Pylori Eradication.

76
00:09:22,180 --> 00:09:34,070
And you know, as you might have learned from other studies, there is this this causal link between each pylori infection and gastric cancer.

77
00:09:34,090 --> 00:09:42,940
So I guess the idea here is, you know, if we can eradicate each pylori from the body, shouldn't we be able to reduce the incidence of gastric cancer?

78
00:09:44,770 --> 00:09:49,479
It's all of these individual studies did not find a significant effect.

79
00:09:49,480 --> 00:09:59,650
They did not find a significant association. But if we compile all of these studies together, we do find a significant a significant relationship.

80
00:09:59,680 --> 00:10:04,750
And by doing this, Higgins, I swear, we find there's not like all the studies are saying the same thing.

81
00:10:05,080 --> 00:10:09,760
Basically, they're just underpowered to detect this difference. Does that make sense?

82
00:10:11,350 --> 00:10:18,100
So that's that's like the the benefit of doing this MIT analysis is like all these individual studies can find this significant effect.

83
00:10:18,100 --> 00:10:21,520
But like with the meta analysis, we were able to.

84
00:10:27,660 --> 00:10:32,190
Okay. Any questions about forest plots before we move on?

85
00:10:32,230 --> 00:10:35,770
Yeah. 38. Very.

86
00:10:37,330 --> 00:10:44,690
Get your 17 old. By the way, your character.

87
00:10:50,720 --> 00:10:54,950
So what null hypothesis are we talking about every time? Go for the heterogeneity or for.

88
00:10:55,460 --> 00:11:00,170
Yeah, I mean, generally if you were trying in a systematic review,

89
00:11:00,410 --> 00:11:07,520
probably what you'd want is for your results to be homogeneous, like you would not want heterogeneous results.

90
00:11:08,420 --> 00:11:11,570
I mean, it depends. And there could be situations where you do want that.

91
00:11:11,580 --> 00:11:17,389
But I would say you as a researcher like this is a very clean story where there is like

92
00:11:17,390 --> 00:11:21,460
there's no heterogeneity and there is a significant effect in the pooled results.

93
00:11:22,250 --> 00:11:31,040
If you did find that this was significant, like if Higgins I squared was like 40, 50, 60% and the P value was under .05,

94
00:11:31,430 --> 00:11:39,350
then what you'd say is that, you know, we, we, we reject the null hypothesis of homogeneity.

95
00:11:39,980 --> 00:11:45,710
Maybe there is evidence of heterogeneity across these studies, so you probably shouldn't be using your pooled result then.

96
00:11:46,040 --> 00:11:50,180
Like your pooled result should only come if there is homogeneous results leading up to it.

97
00:11:51,500 --> 00:11:54,620
And so if this is heterogeneous, maybe you need to stratify.

98
00:11:54,980 --> 00:11:57,920
Like maybe there are some studies conducted in one country and not another.

99
00:11:57,920 --> 00:12:02,040
Maybe there's like different strains of each pylori that we may be looking at.

100
00:12:02,060 --> 00:12:06,590
Maybe there's like different antibiotics they're using or, you know, there could be all sorts of other reasons.

101
00:12:10,870 --> 00:12:17,380
Okay. Let us go into two parts and then after this, we'll take a break.

102
00:12:18,580 --> 00:12:28,000
So funnel plots look like this. And so, again, in your groups, there should be like a or something for you to read.

103
00:12:28,390 --> 00:12:34,780
And then I'll go around to four different groups and ask what you think about these three questions.

104
00:15:19,990 --> 00:15:42,410
200. He said that he had a lot.

105
00:16:07,880 --> 00:16:14,080
The fact that. No. There's a lot of.

106
00:16:28,440 --> 00:16:42,430
Yeah. Families have.

107
00:16:49,840 --> 00:17:02,220
Yeah. Yeah.

108
00:17:21,310 --> 00:17:31,780
Three. Okay.

109
00:17:32,240 --> 00:17:36,650
Let us go over these questions.

110
00:17:38,780 --> 00:17:43,280
Let's stay here in the front with the type two ers.

111
00:17:43,880 --> 00:17:45,850
What do you think about this question?

112
00:17:45,860 --> 00:17:54,950
Failure to include in the meta analysis, all of the relevant trials that have been conducted may have been due to reporting bias.

113
00:18:00,400 --> 00:18:06,090
Felt. Yeah.

114
00:18:06,280 --> 00:18:10,109
So let's just, like, ponder what these parts mean for a moment.

115
00:18:10,110 --> 00:18:17,820
And I realize, you know, what you read is very small and that's why we're discussing what these are initials for, is the dialog in diastolic.

116
00:18:17,830 --> 00:18:22,700
In my mind these show similar results, although from the adverse test.

117
00:18:22,710 --> 00:18:27,810
If we go over in a moment, they show different things. But in my mind these these kind of are so different things.

118
00:18:27,810 --> 00:18:40,770
So to situate ourselves or understand this correctly, this is looking at like home testing of blood pressure versus going through a health system.

119
00:18:41,760 --> 00:18:47,190
And then I think by ten it means it's like ten points lower doing it in home versus in a health care system.

120
00:18:49,550 --> 00:18:57,410
So what we think should be the case is that these plots should be symmetrical.

121
00:19:00,530 --> 00:19:05,330
And they should be kind of symmetrical around like the top of the funnel.

122
00:19:07,280 --> 00:19:15,260
So again, this axis is about the mean difference. This is sort of the outcome in the study is this is looking at one over the standard error.

123
00:19:16,670 --> 00:19:22,760
This is really just looking at the sample size. This is like a fancy way of saying that this is a higher sample size than this

124
00:19:23,450 --> 00:19:29,909
because your standard error will be larger the the smaller your sample is.

125
00:19:29,910 --> 00:19:35,389
So if you do one over that, then you flip it. So something right here actually has a low standard error.

126
00:19:35,390 --> 00:19:39,410
So probably has a high sample size.

127
00:19:41,870 --> 00:19:48,350
Does that make sense? So, you know, we're not saying that like the one with the highest sample size must be like the most correct one.

128
00:19:48,710 --> 00:19:54,800
But it kind of is like a bit of a fulcrum to kind of view what the patterns are for some of the other studies.

129
00:19:56,420 --> 00:20:01,070
And clearly what we are missing is something like over here,

130
00:20:01,280 --> 00:20:05,750
because if something was truly symmetrical and if you look at kind of like the average values,

131
00:20:06,290 --> 00:20:09,890
we should see results kind of like in this area of the chart.

132
00:20:10,880 --> 00:20:14,780
Does that make sense? We're just looking at this visually.

133
00:20:16,340 --> 00:20:21,500
So that just means that there are some studies out there which we're not including.

134
00:20:22,160 --> 00:20:25,300
That's the idea behind this. And so why aren't we including them?

135
00:20:25,310 --> 00:20:34,970
That could be due to a reporting bias. Um, and I think the second yeah, we'll talk more about that in a moment, but let's go to the second question.

136
00:20:35,330 --> 00:20:42,920
So a funnel plot can suggest whether relevant trials were not included in the meta analysis only as a result of publication bias.

137
00:20:43,460 --> 00:20:47,730
So welcome to this table. What do you all think about professional?

138
00:20:50,130 --> 00:20:55,050
And again, you have you might not have a great will to say, well, what is it?

139
00:20:55,050 --> 00:20:58,860
What is your topic? What is your initial reaction? To answer your question.

140
00:21:00,830 --> 00:21:06,770
Or false, he said, according to the year old.

141
00:21:09,570 --> 00:21:13,180
Yeah. So you think both and that would be correct.

142
00:21:13,980 --> 00:21:19,560
So one thing, you know, this is just like a test taking strategy whenever there's I can only, you know, be that suspicious about that.

143
00:21:20,710 --> 00:21:31,110
Um, okay. So this is a point where I'm trying to talk to you about, um, reporting bias.

144
00:21:32,550 --> 00:21:39,300
So the idea of a reporting bias is like what's not included in the systematic review.

145
00:21:39,900 --> 00:21:50,220
So there is like something out there, there's some paper that is not being included in your systematic review and like we're biased as a result of it.

146
00:21:51,630 --> 00:21:58,380
You know, it's like one thing if like we missed a study which is kind of around the middle, but like it's that would be like an error.

147
00:21:58,380 --> 00:22:02,280
But that's not a bias. It's a bias if it's a study that's like over here.

148
00:22:04,480 --> 00:22:08,500
So why can this occur?

149
00:22:10,060 --> 00:22:16,780
So there are different types of there's different types of reporting biases.

150
00:22:17,260 --> 00:22:24,520
One thing that I would say is there could be a publication bias in that maybe authors are not submitting.

151
00:22:28,010 --> 00:22:32,540
And, you know, the the conspiratorial part of me is thinking like maybe if there was.

152
00:22:32,780 --> 00:22:39,169
They're seeing results down here, which might seem counter to the prevailing scientific wisdom or whatnot.

153
00:22:39,170 --> 00:22:42,770
Maybe they're just like, oh, this is a failed trial. It doesn't.

154
00:22:42,770 --> 00:22:45,260
Our results don't make sense. We're just not going to try to publish them.

155
00:22:48,500 --> 00:22:52,850
But publication bias could also refer to know there could be like journals which

156
00:22:52,850 --> 00:22:57,050
just aren't going to accept these or maybe as part of the peer review process, they're just not wanting these.

157
00:22:59,120 --> 00:23:03,230
But there could be other reasons for reporting bias. The publication bias is one of them.

158
00:23:03,470 --> 00:23:12,140
There could be citation bias. And that just means that like these studies are cited a lot, so they're just easier to find.

159
00:23:12,950 --> 00:23:15,739
Whereas some of these other studies, they might be out there, they might be published,

160
00:23:15,740 --> 00:23:22,450
but we just might not be finding them as part of our systematic review because they're not being cited as much and as part of a systematic review.

161
00:23:22,460 --> 00:23:26,260
You kind of like look through the the work cited section of each of the papers.

162
00:23:26,270 --> 00:23:31,850
So if people are not seen in your study, then they're less likely to find it as part of a systematic review.

163
00:23:32,450 --> 00:23:43,549
We could also think of there's some sort of like time like bias would maybe these studies are just like were conducted within the past year,

164
00:23:43,550 --> 00:23:46,820
but we're just not published yet.

165
00:23:47,090 --> 00:23:48,110
They haven't been published yet.

166
00:23:50,300 --> 00:23:58,460
We could also think of like a language bias, like maybe these studies are being published in Spanish or Russian or Chinese or whatever.

167
00:23:59,150 --> 00:24:04,430
So we're just not finding that as part of our systematic review if we're excluding languages outside of English.

168
00:24:07,400 --> 00:24:10,400
So this is all to say that like according to Cedric's definition,

169
00:24:10,400 --> 00:24:17,170
reporting by this includes like all of these things and not just publication bias, but probably publication bias is like a big one.

170
00:24:17,180 --> 00:24:26,690
It might even be done. Any question about like these different types of bias, these are just biases that can crop up during a systematic review.

171
00:24:32,480 --> 00:24:37,550
Okay. Let's move over to group a the disease.

172
00:24:38,540 --> 00:24:47,480
The funnel plot for systolic and diastolic blood pressure indicate that not all of the relevant trials that have been conducted were identified.

173
00:24:48,590 --> 00:24:57,140
What do you think? Trials. And why did you think you.

174
00:25:00,000 --> 00:25:04,050
Yeah. So it seems like there are some things here. And that seems like common across boxes.

175
00:25:04,800 --> 00:25:10,530
And, you know, I'm not like a cardiovascular researcher, but I would assume there's similar trends for systolic and diastolic blood pressure.

176
00:25:10,530 --> 00:25:16,270
So, you know, there's a study missing here and it's probably the same studies which are basically, um,

177
00:25:17,130 --> 00:25:24,600
okay, so and again, we haven't talked about Eggers test in this class yet, but let's just work through it.

178
00:25:24,870 --> 00:25:26,309
And what do you think is the response to this?

179
00:25:26,310 --> 00:25:35,160
The result of the test indicates that asymmetry exists in the funnel plot for the outcome of systolic blood pressure group F.

180
00:25:35,160 --> 00:25:50,160
What do you think for that? Yeah.

181
00:25:50,160 --> 00:25:54,570
I mean, I get it. Okay, so the. What is the Eggers test?

182
00:25:55,230 --> 00:25:59,400
The Eggers test is actually just a simple linear regression.

183
00:26:01,560 --> 00:26:13,680
So in an Eggers test, you would just you kind of just, um, you would just see what, like the slope of the line.

184
00:26:13,760 --> 00:26:26,460
So maybe it's like this. Um, so it's a simple linear regression of, you know, this axis here, the mean difference on the effect size.

185
00:26:27,180 --> 00:26:49,670
And then you'd have a p value for the slope. See the eggers test is the p value for testing.

186
00:26:53,210 --> 00:27:04,010
But this one. So if there is like a completely even flow, that's like thing that's symmetrical, right?

187
00:27:04,090 --> 00:27:09,969
Because like if this was like if there were dots over here, if there were studies over here,

188
00:27:09,970 --> 00:27:14,410
then this line would, you know, essentially not be deviating from a slope.

189
00:27:18,980 --> 00:27:25,580
The confusing thing with this is that they found that the P-value was significant for systolic blood pressure,

190
00:27:26,060 --> 00:27:31,010
but it was not significant for diastolic blood pressure.

191
00:27:33,080 --> 00:27:37,580
So in a way, they're saying like this line was, you know.

192
00:27:39,330 --> 00:27:49,810
This line here was actually. Yeah.

193
00:27:50,460 --> 00:27:57,000
Okay. It's not gonna. This line is straight. So here there is a slope, and I'm assuming that sort of like downward.

194
00:27:57,330 --> 00:28:00,720
Let's say in here, they think that it's kind of straight in the slope here.

195
00:28:03,240 --> 00:28:07,370
Yeah. So it's little. Yeah.

196
00:28:08,390 --> 00:28:14,690
So the null is so it's just, you know, like visually we can look at the plots and see like, oh, do we think like with our eye that they're different?

197
00:28:15,140 --> 00:28:18,230
But the average test is like a formally looking at the significance of that.

198
00:28:18,620 --> 00:28:25,060
But, you know, I would say if I were writing a systematic review of this, I would have, you know, maybe I'd report the Eggers test.

199
00:28:25,070 --> 00:28:28,310
But like I'd say, visually, you know, it's pretty obvious that there's a difference.

200
00:28:29,510 --> 00:28:33,010
So I think this is just like also a cautionary tale,

201
00:28:33,020 --> 00:28:39,950
not super relying on those p values and the significance test, like trust your intuition and trust your gut.

202
00:28:39,950 --> 00:28:44,120
And I think all of us looking at these charts would be like there's there's something missing over here.

203
00:28:44,450 --> 00:28:50,190
Even if the test says that, it's not really. Any questions about the.

204
00:28:53,250 --> 00:28:59,670
Okay. Let me give you until 215 and then we will reconvene and talk about.

205
00:29:04,210 --> 00:29:09,010
Let. Right through this last hurdle.

206
00:29:12,650 --> 00:29:15,770
So if you think this is just from the article from today.

207
00:29:18,800 --> 00:29:28,670
And. You have. So some properties have met analysis.

208
00:29:28,700 --> 00:29:32,510
One is, you know, we want to do some sort of multilevel analysis.

209
00:29:32,940 --> 00:29:35,810
And so in this paper, they mentioned that they did a random effects model.

210
00:29:36,140 --> 00:29:40,369
And that's just the type of multi-level analysis, you know, not you know, there's the fixed effects models.

211
00:29:40,370 --> 00:29:48,410
There's random effects models and mixed effects models. There are, you know, a generalized estimating equation approach for GDP.

212
00:29:48,800 --> 00:29:54,410
So if you ever take like a longitudinal data analysis class in the future, you'll learn more about those.

213
00:29:54,410 --> 00:29:57,920
But just, you know, keep this in your mind. They used a multilevel analysis.

214
00:30:00,250 --> 00:30:08,380
Okay so they mentioned that they used an iceberg statistic in the purpose was to quantify the degree of heterogeneity.

215
00:30:10,270 --> 00:30:14,140
They said they did an aggregate test. Here they say they quantified publication bias.

216
00:30:14,410 --> 00:30:18,399
I would say separate from the the articles he referred,

217
00:30:18,400 --> 00:30:24,160
the small group said you could argue that they should have just mentioned reporting bias here for the publication bias.

218
00:30:24,160 --> 00:30:31,900
But I would say a lot of the times that you see people say Eggers tests in papers, they'll just mention publication bias.

219
00:30:34,120 --> 00:30:39,430
So any other questions about kind of blocks or tests we might use for a meta analysis?

220
00:30:42,790 --> 00:30:43,240
Okay.

221
00:30:43,240 --> 00:30:54,260
So one of the benefits of a meta analysis is that we can guard or we can at least kind of quantify the degree of P hacking that might be happening.

222
00:30:54,280 --> 00:30:58,629
So let's talk about P hacking. So what is P hacking?

223
00:30:58,630 --> 00:31:05,980
P hacking is when we are somehow what's getting out there into the literature.

224
00:31:06,910 --> 00:31:19,860
There's a lot of studies which are showing low P values when in reality the association may have a higher P value or you know,

225
00:31:19,900 --> 00:31:26,799
there may be some sort of bias which is resulting in us submitting significant results in trying to make our results

226
00:31:26,800 --> 00:31:32,830
seem as significant of a p value as low as possible when we are writing a manuscript and analyzing our data.

227
00:31:35,640 --> 00:31:42,570
Okay. So just to give you a background on what these charts are talking about,

228
00:31:44,340 --> 00:31:52,319
a p curve is where we would collect all the P values for every single study which looked at associations.

229
00:31:52,320 --> 00:32:01,530
So, you know, maybe there are 200 studies out there which have looked at the association between coffee, drinking and diabetes.

230
00:32:02,460 --> 00:32:10,380
The p curve is basically a histogram of those P values. So we're just going to zoom in on the values from 0 to 0.1.

231
00:32:10,950 --> 00:32:13,290
But, you know, this should theoretically extend all the way to one.

232
00:32:14,370 --> 00:32:24,510
So in a situation where there is no effect, there should be an equal number of values for every single p value.

233
00:32:25,410 --> 00:32:32,190
So the line will be straight here. And that's like what we mean by a null hypothesis under.

234
00:32:36,470 --> 00:32:40,950
A no and null hypothesis is that, you know, there there's no effect.

235
00:32:40,950 --> 00:32:47,130
And so we might see results which have a P value under 2.05, but that still just means no effect.

236
00:32:47,940 --> 00:32:53,600
If there is an effect, then we would see, you know, a higher frequency of values under point of.

237
00:32:56,760 --> 00:33:01,490
But again, you know, we would still see CC studies which have P values above five.

238
00:33:03,510 --> 00:33:18,840
Does this does this make sense? Okay, so publication bias happens when journal editors don't want to publish studies with insignificant results.

239
00:33:20,550 --> 00:33:27,000
So this means that so publication bias depresses the values which are above 25.

240
00:33:32,910 --> 00:33:41,260
Because, you know, like the more often that you have a p value point of six or seven or nine or 0.25 or .85, like the general default.

241
00:33:45,180 --> 00:33:55,069
P. Hacking is often referring to like what are researchers doing to increase the number

242
00:33:55,070 --> 00:33:58,640
of studies that they have out there which have a p value under point of five?

243
00:33:59,030 --> 00:34:06,770
And because it's easier to kind of manipulate your data to get from like a p value of .0512.049 well,

244
00:34:06,800 --> 00:34:13,240
you kind of look for is that blip just under .05 because this is the area where it's easy to kind of like push studies,

245
00:34:13,670 --> 00:34:17,930
like relatively easy to kind of like manipulate your statistics, try different tests,

246
00:34:18,380 --> 00:34:22,160
you know, stratify your populations, whatever you need to do to get it under point of five.

247
00:34:22,430 --> 00:34:26,749
And it's easier to like just end up with a value .04 or five or something like that, you know,

248
00:34:26,750 --> 00:34:32,210
completely pushing it down to a value of like point oh or one or five to be less likely.

249
00:34:35,280 --> 00:34:45,450
So again, publication bias is what happens above a p value point or five p hacking is what happens below a value of point five.

250
00:34:47,440 --> 00:34:50,619
And this this can happen if there is actually no effect.

251
00:34:50,620 --> 00:34:58,660
In fact, you know, somehow we're God and we realize that there's no causal relationship or there could be in a scenario where there is an effect.

252
00:34:58,690 --> 00:35:03,309
So this this happens in each situation. I would say of all of these,

253
00:35:03,310 --> 00:35:13,869
this one is probably the hardest to distinguish from a curve just because these values like they're they're decreasing anyway.

254
00:35:13,870 --> 00:35:20,780
But. Okay.

255
00:35:21,370 --> 00:35:25,280
Um, so I have, uh.

256
00:35:26,900 --> 00:35:34,040
Oh. Everywhere for you. So just give you a minute or two to complete the.

257
00:37:13,130 --> 00:37:15,320
Okay. Take a look at this.

258
00:37:18,590 --> 00:37:26,270
So and of course, like through all these, we're receiving their top 11.5, which, you know, may not be true for every state, but that's in place.

259
00:37:29,250 --> 00:37:35,520
Okay. So here we have a situation where in our peak curve there's a rapid drop after point of.

260
00:37:36,570 --> 00:37:40,580
That would be an example publication bias in our hacking.

261
00:37:42,620 --> 00:37:46,610
Compassion is on this side of the upper level. Publication bias is on the side.

262
00:37:50,510 --> 00:37:58,100
Okay. This example we see like certainly there's a depression here.

263
00:37:59,300 --> 00:38:04,460
I would say that this is unclear whether there's nothing, but if there is something, it would be publication bias.

264
00:38:05,000 --> 00:38:09,860
But one of the reasons why I'm doing this is just to show you that it can be kind of complicated to interpret these.

265
00:38:11,330 --> 00:38:16,310
So this I wouldn't know necessarily if there is publication bias or not.

266
00:38:16,340 --> 00:38:23,240
There might be. There may not be. So either of these situations might be correct.

267
00:38:28,200 --> 00:38:33,000
And again, that's just hard to see. Like, is there actually a curve downward or not?

268
00:38:34,800 --> 00:38:41,190
What is this situation? I would say that this is this is definitely peaking.

269
00:38:41,190 --> 00:38:45,269
And so I think, you know, most of you are getting there because the thing is,

270
00:38:45,270 --> 00:38:53,870
when we see a lot of values just underpinning about whether there is publication bias or not is a bit tough because like,

271
00:38:53,880 --> 00:38:59,820
what would it actually look like? Again, the publication bias is the one that's hard to evaluate.

272
00:38:59,820 --> 00:39:05,010
So I would say it's not clear in this, but I'd be maybe leaning towards there not being publication bias.

273
00:39:05,010 --> 00:39:11,639
But there could be. And in this is neither publication bias nor PR hacking.

274
00:39:11,640 --> 00:39:17,040
It's just like a straight shot. Pretty, pretty clear evidence of no bias.

275
00:39:19,390 --> 00:39:23,469
So again, the publication bias is the one that can be a bit difficult to suss out.

276
00:39:23,470 --> 00:39:26,830
But any other questions on these charts?

277
00:39:26,950 --> 00:39:36,389
I think the majority of you are getting this. Okay.

278
00:39:36,390 --> 00:39:41,490
So how can we let him in on this? How can it be? How can we get our P-value lower?

279
00:39:43,080 --> 00:39:54,330
So one thing we could do is we could record many response variables and then just selectively report associations like maybe,

280
00:39:57,360 --> 00:40:01,829
you know, I just had a meeting with Kenny earlier today, so this was in my mind.

281
00:40:01,830 --> 00:40:06,990
But he has this analysis which he's looking at stunting and wasting.

282
00:40:08,190 --> 00:40:14,310
So if you did all of your analysis and you found really significant results for stunting but not for wasting,

283
00:40:14,700 --> 00:40:17,490
so you only report stunting, you just don't report wasting at all.

284
00:40:17,820 --> 00:40:24,450
That would be an example of that because we're just like not showing those results for, you know, all of our analysis that we do.

285
00:40:26,640 --> 00:40:33,060
If you're certain times when you're dropping outliers and you don't have a good motivation for that, that could be a way to kick your way.

286
00:40:34,020 --> 00:40:40,440
You know, if you're just like including or excluding different covariates and confounders and you're just if the reason why

287
00:40:40,440 --> 00:40:47,650
you're doing that is to change your P value for you'd like your main exposure outcome that that would be problematic.

288
00:40:50,480 --> 00:40:59,630
So one thing that we've realized is that you can also kind of pick your way if you stop data collection.

289
00:41:01,520 --> 00:41:05,509
You can do this if you recombine different treatment groups.

290
00:41:05,510 --> 00:41:10,760
So like if you have a randomized controlled trial and maybe you have like four acts like one control in three experimental arms.

291
00:41:12,290 --> 00:41:17,660
It could be that like maybe only one of the arms is significant, the other two are not.

292
00:41:18,770 --> 00:41:23,510
But if you like, combine them all together. Maybe it still looks like all three of them are significant, but that would be a problem.

293
00:41:25,400 --> 00:41:31,160
And thirdly, we are going to be considering about like what are we finding in a per protocol versus an intent to treat?

294
00:41:31,520 --> 00:41:32,719
And I honestly,

295
00:41:32,720 --> 00:41:39,799
I really like randomized controlled trials which report both kinds of analysis because they give you kind of different types of evidence.

296
00:41:39,800 --> 00:41:43,700
And it could be that one would have a significantly valid one. But no, and I think that's interesting.

297
00:41:43,700 --> 00:41:47,269
But isn't an example of a field trial, that's just something that, you know,

298
00:41:47,270 --> 00:41:51,290
adds some contextual information about when a certain treatment works and doesn't.

299
00:41:55,500 --> 00:41:58,740
Okay. So some further details about this.

300
00:41:59,580 --> 00:42:02,910
Say we are interested in blood pressure as an outcome.

301
00:42:03,630 --> 00:42:07,709
How do we measure it? We could measure it continuously, like just continuously.

302
00:42:07,710 --> 00:42:11,140
What are people's diastolic and systolic blood pressure?

303
00:42:11,160 --> 00:42:14,549
We could categorize it and we could categorize the different ways.

304
00:42:14,550 --> 00:42:19,560
And there's different borders we could use. We could use like above and below, like a normal border.

305
00:42:19,560 --> 00:42:24,389
We could do like an elevated number there. Stage one and stage two, hypertension.

306
00:42:24,390 --> 00:42:25,950
There's a hypertensive basis.

307
00:42:26,370 --> 00:42:36,210
So all of a sudden for this, like one measurement that you have done in your in your study, like it should be to protect healthcare systolic.

308
00:42:36,210 --> 00:42:37,620
But from those two things,

309
00:42:38,490 --> 00:42:44,740
you all of a sudden could have like four or five or maybe even six different outcomes depending on how you measure blood pressure,

310
00:42:44,760 --> 00:42:54,510
hypertension and things like that. This is all to say that it is, you know, maybe ahead of time.

311
00:42:54,510 --> 00:43:02,070
What you're interested in is hypertensive crisis and all these other things are not important to you, so you don't even analyze them.

312
00:43:03,000 --> 00:43:06,360
That that would be fine then for you just to report the hypertensive crisis.

313
00:43:06,720 --> 00:43:11,340
But in a different world where you like look at all of these and like this one is insignificant, this one is insignificant,

314
00:43:11,580 --> 00:43:16,060
and then like only this one is significant, then that's where you get into the problem of people.

315
00:43:16,410 --> 00:43:19,280
So it's kind of like, what is your process and what are you leaving coke?

316
00:43:21,150 --> 00:43:29,070
And you know, like a lot of times, like there's no one looking over your shoulder for everything you're doing in your hour or SAS or whatever.

317
00:43:29,070 --> 00:43:35,640
So we don't exactly know if you did all these other things and just ignored them.

318
00:43:36,360 --> 00:43:40,290
So this is just about being honest as a researcher and what you did or didn't do.

319
00:43:41,550 --> 00:43:46,760
And my. Yeah.

320
00:43:46,760 --> 00:43:52,579
My my own thought is that, like, if you do have things which are not significant, I think it's okay to report them,

321
00:43:52,580 --> 00:43:58,250
maybe put them in a supplementary appendix, but in your methods, the rate that you did, all these different things.

322
00:44:00,890 --> 00:44:04,370
Okay. Four timing sequels. Keep on moving on.

323
00:44:05,770 --> 00:44:08,840
Um, so dropping outliers.

324
00:44:09,080 --> 00:44:12,799
So, what do we mean by outliers? By outliers?

325
00:44:12,800 --> 00:44:17,840
You can measure them. A number of different ways. There's, like, cut distance you can kind of look at.

326
00:44:20,910 --> 00:44:25,440
Students residuals, things like that to see.

327
00:44:25,450 --> 00:44:31,530
And then maybe we could see like, oh, for these observations where there's the distance above 0.1 or something like that,

328
00:44:32,430 --> 00:44:39,560
it might be useful to look at them in the data because the whole idea behind a distance is that these are the really influential values.

329
00:44:39,570 --> 00:44:45,000
Like if we took these people from the data set and we removed them, we might get different.

330
00:44:45,120 --> 00:44:51,000
We might get different results. So I do think it's important to look at these.

331
00:44:51,240 --> 00:45:00,960
And it could be that like, oh, maybe there was clearly an error in the data collection.

332
00:45:00,970 --> 00:45:04,709
You know, there were there were values for like how old the child is that?

333
00:45:04,710 --> 00:45:10,830
It was like 100 instead of ten. You know, there are things which are very clearly things that you could go back to and,

334
00:45:10,830 --> 00:45:14,060
you know, look, to make sure that your dataset is clean, it is correct.

335
00:45:14,070 --> 00:45:23,940
I think that's an important thing for kids distance. But I would be a bit suspicious of just like eliminating these ones wholesale.

336
00:45:24,870 --> 00:45:30,870
I mean, and if you do decide to go that road, I think you just need to be honest about it in your results section and just mention like,

337
00:45:30,870 --> 00:45:38,580
oh, you know, there were two values which didn't work or that had a really high distance, so we remove them.

338
00:45:38,820 --> 00:45:44,820
But in that case, I think it would be important to kind of report maybe the results in supplementary

339
00:45:44,820 --> 00:45:49,170
appendix and maybe in that supplementary appendix you don't have significant results.

340
00:45:49,530 --> 00:45:53,939
But I think you're being honest and I think it's all about like honesty as a researcher,

341
00:45:53,940 --> 00:46:01,169
because it'd be very easy to just remove these people and, you know, probably peer reviewers aren't going to catch it like that.

342
00:46:01,170 --> 00:46:09,539
These two people were included in the study. Um, so again, it's, it's about your, your honesty and your, your being able to sleep at night.

343
00:46:09,540 --> 00:46:13,630
I think that's all about. Okay.

344
00:46:14,290 --> 00:46:23,050
Um, I'm going to kind of skip through some of these and just this is all to say

345
00:46:23,050 --> 00:46:28,750
that I think parking should be prevented because in an ideal scientific world,

346
00:46:28,750 --> 00:46:33,730
we should do a sample size calculation that we put as part of our grant application.

347
00:46:34,090 --> 00:46:43,420
And then when we get funding, we will report to our IRB that like, oh, we need 500 people, that part of the grant application,

348
00:46:43,930 --> 00:46:51,550
and then we will complete our study and lo and behold, there will be 500 people in our study and we will be good to go.

349
00:46:53,200 --> 00:46:59,290
One thing that I would caution on, and I think I was kind of giving up this idea here of.

350
00:47:04,580 --> 00:47:11,020
You should see if I can show this. Yes.

351
00:47:11,800 --> 00:47:15,040
Okay. So if we have a situation.

352
00:47:20,050 --> 00:47:25,240
And I think if I keep on going back, which is my keyboard in my handwriting, it works.

353
00:47:25,240 --> 00:47:30,910
So stay with me. Well.

354
00:47:32,110 --> 00:47:37,400
We are with the class and you're going to have you come back. And I've already said, you know, with the rest of this.

355
00:47:38,150 --> 00:47:43,910
But read the is not productive and I feel.

