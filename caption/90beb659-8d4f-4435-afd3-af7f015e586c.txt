1
00:00:02,520 --> 00:00:07,620
Okay. We can get started. I don't think there's much housekeeping.

2
00:00:07,710 --> 00:00:12,880
There is. If you haven't taken the survey, this is your last chance.

3
00:00:12,900 --> 00:00:16,140
I think almost everyone has taken the survey, so.

4
00:00:16,410 --> 00:00:20,640
Thank you, everyone. I think from the results.

5
00:00:21,600 --> 00:00:25,010
So then the biggest thing that I wanted to schedule was office hours.

6
00:00:25,020 --> 00:00:28,980
Looks like everyone can come to office hours, maybe with one exception if you need to.

7
00:00:30,360 --> 00:00:36,540
If you want to come at a time that's not my standing office hours. Just send me an email and I will make time to to talk to you.

8
00:00:36,900 --> 00:00:43,560
So I'm going to keep office hours at 3 to 4 on Friday, so I'll do that.

9
00:00:44,520 --> 00:01:00,110
The other thing that I wanted to get scheduled was our Project Review Day, which is really important that if possible, everyone can be here that day.

10
00:01:00,120 --> 00:01:01,100
That would be ideal.

11
00:01:01,110 --> 00:01:08,630
I think last year everyone showed up on that day except for one person who joined by joined their group by Zoom who was traveling.

12
00:01:08,640 --> 00:01:13,410
So we did well last year and hopefully will do well again this year.

13
00:01:13,410 --> 00:01:17,280
I think so far two people said they can't come on this day.

14
00:01:17,280 --> 00:01:23,370
I did realize I also put it I think that is in our that day now apparently not a lot of you were

15
00:01:23,370 --> 00:01:31,229
going to but so I think I'll send one more survey to ask you about like the 15th or the 22nd,

16
00:01:31,230 --> 00:01:34,950
if we can push it one way or the other to see if we can get everyone.

17
00:01:35,130 --> 00:01:41,340
If we can't get everyone will make do with what we can. I think that was it.

18
00:01:42,360 --> 00:01:48,450
Just a reminder, homework one is posted do and it was two weeks, but now it's less than two weeks because time passes.

19
00:01:50,700 --> 00:01:54,120
I don't think it's too hard. If you have trouble with it, come talk to me at office hours.

20
00:01:56,400 --> 00:02:03,150
Any logistical questions or concerns or anything else?

21
00:02:04,320 --> 00:02:07,500
Okay, we'll pick up where we left off.

22
00:02:07,500 --> 00:02:18,510
So we had just started talking about dads. On Monday, if you're also in 699, you're just all dads all week is your experience.

23
00:02:19,410 --> 00:02:22,580
So we had gotten this far right.

24
00:02:22,590 --> 00:02:32,820
We had this story about there's two possible treatments you can a doctor might determine treatment based on age or disease severity.

25
00:02:33,540 --> 00:02:39,180
Treatment can affect the outcome. And we drew this deck or you guys drew a DAG, and then I also drew a dog.

26
00:02:39,450 --> 00:02:43,079
I think a lot of you had to defer from treatment to adherence, which I think is reasonable.

27
00:02:43,080 --> 00:02:46,750
But I didn't put it in this example. Okay.

28
00:02:48,620 --> 00:02:52,729
Temporality came up in our discussion. There's a good question about temporality.

29
00:02:52,730 --> 00:03:04,580
So our definition of causality sort of definitionally requires that the thing at the end of the arrow happens after or potentially concurrently,

30
00:03:04,580 --> 00:03:12,380
but like after in some later physical nanosecond sort of sense, the thing at the beginning of the error.

31
00:03:13,040 --> 00:03:19,399
So because we have this restriction under where every DAG is consistent with

32
00:03:19,400 --> 00:03:23,840
some time ordering so you can take all of the nodes and put them in an order.

33
00:03:23,850 --> 00:03:28,700
It doesn't mean that that's exactly the right physical order.

34
00:03:28,730 --> 00:03:36,080
Right. So in this Dag, I can order I know that age and severity come before treatment and I know that

35
00:03:36,080 --> 00:03:40,190
treatment comes before outcome and I know that adherence comes before outcome.

36
00:03:40,340 --> 00:03:46,920
I can't necessarily tell you just based on the structure of the DAG, whether treatment comes before adherence or not.

37
00:03:46,940 --> 00:03:49,849
I know that it does because I know what those words mean.

38
00:03:49,850 --> 00:03:55,460
But if I just gave you the DAG structure, you wouldn't necessarily be able to say that treatment comes before adherence.

39
00:03:55,670 --> 00:04:03,380
But nevertheless, our DAG is consistent with at least one strict ordering of the nodes.

40
00:04:06,760 --> 00:04:12,700
So there's this other question of how do you represent a feedback loop if you have to have a strict ordering in a directed basically graph?

41
00:04:13,000 --> 00:04:20,460
So let's say you have. We can think of like lots of feedback loops, right?

42
00:04:20,640 --> 00:04:27,450
Like how well you're doing in a class determines how you feel about the class, which determines whether you go to class or not.

43
00:04:27,450 --> 00:04:31,410
And then that also determines like you're great, great. So there's this feedback, feedback loop there.

44
00:04:31,980 --> 00:04:37,110
So we might you might want to draw that as the tag on the left.

45
00:04:39,000 --> 00:04:46,290
And the thing to notice here is that actually you when we drew the dog on the left, this a which is not a tag.

46
00:04:46,290 --> 00:04:50,670
So graph the graph on the left is a feedback loop with Y.

47
00:04:50,910 --> 00:04:56,860
We've sort of collapsed a bunch of time points together into a and we've collapsed a bunch of time points together into why.

48
00:04:57,060 --> 00:05:05,820
So I can turn this into a drag by, like spreading out those time points so you can imagine discrete time,

49
00:05:05,940 --> 00:05:17,729
which I guess I would argue discrete time is almost always good enough, especially in like epidemiological or economic contexts.

50
00:05:17,730 --> 00:05:25,920
We can measure things that sort of discrete time points and so you can take your feedback loop and spread it out into discrete time points.

51
00:05:26,130 --> 00:05:29,910
So let's say A is how well I'm doing in the class.

52
00:05:30,720 --> 00:05:35,940
And then why is my class attendance rates or how I'm doing might influence my attendance,

53
00:05:36,180 --> 00:05:39,509
which might influence how I'm doing, but now it time to write.

54
00:05:39,510 --> 00:05:45,540
So I put a subscript on a so I have how I'm doing at time one and then I can check how am I doing at time.

55
00:05:45,780 --> 00:05:50,850
So now that I have this time feature in my graph, I'm back to having a dag,

56
00:05:51,240 --> 00:05:56,160
which makes me happy and we're going to see why we want to have tags a little bit later.

57
00:05:57,180 --> 00:06:04,350
More questions about this. This is sort of like a side about having this restriction about being a.

58
00:06:07,350 --> 00:06:14,219
Okay, so we know how to draw these pictures and we need to be able to take this picture and say,

59
00:06:14,220 --> 00:06:17,370
what mathematical properties can I assign to this picture?

60
00:06:17,370 --> 00:06:24,480
How is this going to be useful to me as a mathematical object? So the first property that we need is the causal mark of property.

61
00:06:25,170 --> 00:06:34,180
So this is how we say what is this picture telling me about the joint probability distribution of these variables?

62
00:06:34,180 --> 00:06:40,409
So the causal mark of property says that I can factor the joint probability distribution into

63
00:06:40,410 --> 00:06:47,760
a product where each product is probability of that node conditional only on its parents.

64
00:06:48,660 --> 00:06:54,020
So the. We didn't do the example of the factory until a little bit later.

65
00:06:54,630 --> 00:06:57,410
And so in this graph, right,

66
00:06:57,410 --> 00:07:09,010
this graph is telling me that I can factor the joint probability of all five of these variables as this sort of long thing on the left.

67
00:07:09,020 --> 00:07:14,389
But you'll notice that in each probability statement, I'm conditioning on the parents of that node.

68
00:07:14,390 --> 00:07:22,640
So I've got probability of outcome conditional on adherence, treatment and severity, probability of treatment, conditional on adherence and severity.

69
00:07:22,910 --> 00:07:26,810
And then the other three don't have any parents. And so they're just in there unconditional.

70
00:07:28,820 --> 00:07:36,680
So the other consequence of the causal Markov property is that it tells me about conditional independence.

71
00:07:36,680 --> 00:07:41,180
So it tells me that I make sure I say this right.

72
00:07:41,330 --> 00:07:48,650
Each node is independent of all nodes that are not direct, that are not its descendants, conditional on its parents.

73
00:07:49,190 --> 00:07:58,819
So in this DAG that we had, I can take that property and I can write a series of conditional independent statements from that property.

74
00:07:58,820 --> 00:08:03,320
So the first thing that I might do is say, okay, severity doesn't have any parents.

75
00:08:03,830 --> 00:08:08,030
And so severity is going to be conditionally independent of everything that's not.

76
00:08:09,050 --> 00:08:12,380
A descendant of severity conditional on nothing.

77
00:08:12,530 --> 00:08:17,450
So that's age and adherence. So I can say severity is conditionally independent of age.

78
00:08:17,960 --> 00:08:22,720
Severity is conditionally independent of adherence. And then I can do the next one, right?

79
00:08:22,730 --> 00:08:30,200
Age is conditionally independent of adherence. There's no parents their treatment has to parents age and severity.

80
00:08:31,250 --> 00:08:34,790
And the only other thing that's not a parent or its descendant is adherence.

81
00:08:34,940 --> 00:08:41,400
So you can say treatment is conditionally independent of adherence, conditional on age and severity.

82
00:08:41,870 --> 00:08:51,080
So the causal markup property gets me this set of conditional independence statements and it gets me this factoring of the graph.

83
00:08:51,620 --> 00:08:56,329
So in our world, a graph can mean a lot of things, right?

84
00:08:56,330 --> 00:09:01,560
You might use a graph just to represent. You might have seen a graph in like a Bayesian model, right?

85
00:09:01,610 --> 00:09:08,630
Where you have variables and you draw arrows. If like one of the variables depends on another variable or something, it's a different kind of graph.

86
00:09:08,780 --> 00:09:12,860
So this is a graph that is about probability distributions of variables.

87
00:09:14,050 --> 00:09:18,710
And the causal mark property is the thing that tells us.

88
00:09:18,730 --> 00:09:22,420
What does this graph mean about the probability distribution of the variables?

89
00:09:23,650 --> 00:09:28,780
There are questions about this. Okay.

90
00:09:29,260 --> 00:09:35,140
So the customer record properly told us how to connect the picture to probability distributions.

91
00:09:35,440 --> 00:09:43,900
We still don't know how to connect the picture to counterfactuals, so we're going to find a few ways to connect the picture to counterfactuals.

92
00:09:44,800 --> 00:09:49,120
They will end up being equivalent under some variations of different assumptions.

93
00:09:50,260 --> 00:09:55,720
So the first one is the do. Operator. So this is the way Judea Pearl likes to talk about this.

94
00:09:57,040 --> 00:10:02,440
So the do operator is this idea that I can intervene any of the nodes in the graph?

95
00:10:02,770 --> 00:10:10,960
I have a hypothetical intervention where I can intervene and set the value of that node to, let's say, little X.

96
00:10:13,280 --> 00:10:21,050
So that Judea Pearl connects this operator of setting X to little X with a graph surgery procedure.

97
00:10:21,530 --> 00:10:22,820
And we'll see what that looks like.

98
00:10:23,950 --> 00:10:34,450
So in the graph surgery procedure, Perl says, we have this dag and this dag shows you the world with no interventions.

99
00:10:34,930 --> 00:10:42,670
And then if I want to see what does the probability distribution look like if I have this intervention is do x equals x has happened.

100
00:10:42,970 --> 00:10:49,090
Then I perform graph surgery to my DAG where I take out all of the arrows that go

101
00:10:49,090 --> 00:10:53,110
into x because I intervened and I said X doesn't depend on the world anymore.

102
00:10:53,110 --> 00:10:57,670
I determine what X is. I'm sort of an omniscient God in this world.

103
00:11:00,670 --> 00:11:10,150
And that's my graph surgery. So the example here is on the left we have like the original DAG, which describes sort of the natural world.

104
00:11:10,330 --> 00:11:14,200
So in this example, this is clipped out of Judea Pearl's book.

105
00:11:14,200 --> 00:11:18,140
So we have. At the top X1 is the season.

106
00:11:19,190 --> 00:11:23,810
And then the season affects whether someone is using a sprinkler or whether it might be raining.

107
00:11:25,160 --> 00:11:29,690
If either the sprinklers on or it's raining, then the sidewalk is wet.

108
00:11:29,930 --> 00:11:32,090
And if the sidewalk is wet, then it might be slippery.

109
00:11:32,510 --> 00:11:41,150
So that's this dag on the left and that corresponds to this probability factoring on the bottom.

110
00:11:41,720 --> 00:11:47,870
So in. The interventional world or we intervene and we turn the sprinkler on.

111
00:11:49,040 --> 00:11:52,099
We've cut the dependance between X3,

112
00:11:52,100 --> 00:11:57,919
the sprinkler variable and the season because we intervened and x three is now a

113
00:11:57,920 --> 00:12:06,290
fixed node so there's no we could put probability of x three in this factorization,

114
00:12:06,290 --> 00:12:14,600
but that's one. So I didn't put probability of x three in the factorization because x three in this world only has one value and that is gone.

115
00:12:15,890 --> 00:12:23,600
So this is the intervention world where we turn to the sprinkler on and we have sort of a similar factorization,

116
00:12:23,780 --> 00:12:31,970
but you'll notice we set x three in the conditioning and we removed x three the x three term.

117
00:12:32,540 --> 00:12:37,820
Right? This term does not appear in our interventional world probability distribution.

118
00:12:39,020 --> 00:12:47,450
So this is in Judea pearls, a way of sort of putting all of this together.

119
00:12:47,450 --> 00:12:51,859
This is the connection between the dags and the counterfactuals is that I have this operation

120
00:12:51,860 --> 00:12:56,720
that I can perform to a like natural world tag and it gets me an intervention world at.

121
00:12:59,330 --> 00:13:05,530
I'm not going to spend too much more time on this because at the end of the lecture,

122
00:13:05,540 --> 00:13:12,469
the last chunk of the lecture is maybe an extension of this graph modification

123
00:13:12,470 --> 00:13:17,210
procedure that I find sort of more satisfying and easier to understand visually.

124
00:13:17,450 --> 00:13:22,520
But I think. Kind of can supersede this graft surgery procedure.

125
00:13:24,920 --> 00:13:29,540
Okay. So the second way to connect dags to counterfactuals is structural equation models.

126
00:13:31,170 --> 00:13:38,850
So a structural equation is different from a regular equation in that the thing on the left is a counterfactual.

127
00:13:41,670 --> 00:13:45,580
So. All right. So we have this example.

128
00:13:45,850 --> 00:13:50,650
We have a machine that's going to measure blood pressure. X is the true systolic blood pressure.

129
00:13:50,920 --> 00:13:56,320
Why is the measured systolic blood pressure? And then the machine has some amount of error.

130
00:13:56,500 --> 00:14:00,370
And let's say that error just doesn't depend on anything else. So my dad is really simple.

131
00:14:00,610 --> 00:14:11,590
If X causes white. So the structural equation that corresponds to that X causes Y is Y equals x plus some error.

132
00:14:11,800 --> 00:14:15,040
And when I say here Y equals X plus some error,

133
00:14:15,040 --> 00:14:23,680
I'm thinking of this as like a generative model x occurs in that I can get Y by plugging in x and then the error.

134
00:14:24,310 --> 00:14:29,500
So mathematically, right, if I want to generate let's say x and y in r,

135
00:14:29,830 --> 00:14:35,649
I could generate x and then generate y by taking x and adding something or I

136
00:14:35,650 --> 00:14:39,340
could generate y and then generate x by taking why and subtracting something.

137
00:14:39,340 --> 00:14:44,319
Right. And those would give me the same probability distribution in R right.

138
00:14:44,320 --> 00:14:51,830
So I could. Take this equation on the top, rearrange it and say X equals y minus epsilon y.

139
00:14:52,990 --> 00:14:58,720
However, the first equation is structural because the Y is a counterfactual.

140
00:14:58,930 --> 00:15:00,520
I didn't write it as a counterfactual,

141
00:15:00,520 --> 00:15:09,150
and the reason I didn't write it as a counterfactual is for some reason in structural equation land, they tend not to put that in there.

142
00:15:09,160 --> 00:15:16,090
So you just have to remember when someone says this is a structural equation, they mean the thing on the left is the counterfactual.

143
00:15:18,520 --> 00:15:21,759
So just to like really hammer this home, right?

144
00:15:21,760 --> 00:15:27,700
I could get the counterfactual of y of x equals x by plugging little x into this first equation.

145
00:15:28,150 --> 00:15:33,879
If I want the counterfactual x of y equals little y, that's just x, right?

146
00:15:33,880 --> 00:15:41,470
X doesn't change. If I change y, if I intervene and I say I'm going to make this blood pressure machine read 100.

147
00:15:41,620 --> 00:15:48,759
That doesn't change what your blood pressure is, right? So this second equation isn't a counterfactual equation.

148
00:15:48,760 --> 00:15:52,060
It's not a structural equation. The first equation is a structural equation.

149
00:15:53,940 --> 00:16:01,420
Okay. So we can link a graph to a causal model with a system of non parametric structural equations.

150
00:16:01,440 --> 00:16:04,980
We could also do it with parametric equations but would be a little bit more general.

151
00:16:05,430 --> 00:16:12,720
So let's say Curly G is a directed graph, it's a DAG, it's got nodes v one through v, n,

152
00:16:13,320 --> 00:16:19,080
and then there are errors epsilon one through epsilon n, and they're associated with each corresponding node.

153
00:16:20,430 --> 00:16:25,800
So we're going to assume that given VII with parents pay.

154
00:16:25,830 --> 00:16:30,060
So it's bold because it might be a vector, so we might have multiple parents.

155
00:16:30,990 --> 00:16:36,240
There's a Counterfactual VII of what would happen if I intervened on its parents,

156
00:16:36,630 --> 00:16:43,560
and that counterfactual is given by a structural equation f sub VII of parents and epsilon.

157
00:16:43,860 --> 00:16:53,160
So the important thing here is. It's non parametric, which means f can be anything and it only depends on the parent and this error.

158
00:16:55,420 --> 00:17:00,120
Your questions. Okay. So here's an example.

159
00:17:00,120 --> 00:17:11,609
We have this little graph. Z and Y, the corresponding structural equation set to this graph is Z is just some function of the error.

160
00:17:11,610 --> 00:17:13,880
Epsilon, z, m of Z.

161
00:17:13,890 --> 00:17:23,040
Here I did put the counterfactuals in so it's really clear m of z is a function of z and it's error and then Y is a function of Ammons Z.

162
00:17:24,690 --> 00:17:26,940
So one thing that this tells us is like.

163
00:17:29,970 --> 00:17:40,020
M of y the counterfactual m of y is just m right that if I plug y into this equation, there's nowhere to watch for y to go in.

164
00:17:40,020 --> 00:17:45,120
So I can tell from this equation that changing y does not change from.

165
00:17:48,430 --> 00:17:51,910
So a special case is linear structural equation models.

166
00:17:52,270 --> 00:17:58,600
So a linear structural equation model has this very special form, which is that all the equations are linear.

167
00:17:58,720 --> 00:18:03,880
We can take that and we can turn it into a matrix form.

168
00:18:04,210 --> 00:18:08,740
And you can do lots of stuff. Like if you know all the equations are linear,

169
00:18:08,740 --> 00:18:12,639
then you can sort of go along the path from one variable and you can figure out

170
00:18:12,640 --> 00:18:16,720
what the correlations between any pair of variables in your model should be.

171
00:18:17,500 --> 00:18:21,580
I think we might have a linear structural equation model problem on homework too, although.

172
00:18:22,970 --> 00:18:34,470
If we don't. Don't be mad. So a lot of the early work in structural equation models assume, by the way, a structural equation model.

173
00:18:34,740 --> 00:18:46,520
If that's not clear, a lot of the early work is in specifically linear schemes and it's it can be a nice model to sort of work out problems of

174
00:18:46,530 --> 00:18:52,530
like can I show confounding it's nice to start with a linear model and see if we can show confounding in a linear model.

175
00:18:54,390 --> 00:19:03,690
However, the linear model is very restrictive in a bunch of ways, so for now, we're not going to assume the linear model is true.

176
00:19:04,500 --> 00:19:07,560
When we start the modeling section,

177
00:19:08,310 --> 00:19:14,460
we'll start to see that we need to start making some modeling assumptions about the distributions of some of our variables.

178
00:19:14,640 --> 00:19:19,260
But even then, we're going to try not to limit ourselves just to simple linear models.

179
00:19:23,230 --> 00:19:27,730
Okay. So that non parametric structural equation model.

180
00:19:29,030 --> 00:19:34,669
Z as F, sub Z of Z and Epsilon Z and Z doesn't go in.

181
00:19:34,670 --> 00:19:42,140
The Z is not totally enough to get us a complete causal system.

182
00:19:42,410 --> 00:19:50,210
So I can't get from just that specification of a non parametric structural equation model to the causal Markov property,

183
00:19:50,330 --> 00:19:54,770
which I know is really important without making additional assumptions about the Epsilon.

184
00:19:55,190 --> 00:20:02,060
So one sort of like maybe obvious natural feeling assumption is to just assume that all the epsilon is are independent, right?

185
00:20:02,240 --> 00:20:07,580
If those epsilon is aren't independent, it sort of seems like maybe I left something out of the dag or something.

186
00:20:09,590 --> 00:20:16,160
So this is the. Richardson and Robbins call this the NPR IEEE model.

187
00:20:16,170 --> 00:20:22,040
So IEEE is for independent error, NPC and non parametric structural equation models.

188
00:20:23,600 --> 00:20:25,880
They actually end up showing it to me.

189
00:20:26,180 --> 00:20:31,790
I think when I first learned about non parametric structural equation models, I just learned about it with this assumption.

190
00:20:31,790 --> 00:20:34,580
Right. The first ones are independent. That's part of the definition.

191
00:20:35,000 --> 00:20:41,110
Turns out that's actually kind of a strong definition and they're going to show us why that's stronger than we need.

192
00:20:41,120 --> 00:20:44,780
And we can actually make a weaker assumption when I.

193
00:20:46,250 --> 00:20:49,729
Reference Richardson and Robbins. So this is a good reminder.

194
00:20:49,730 --> 00:20:54,080
So there's reading that I put for each of the lectures.

195
00:20:54,080 --> 00:21:02,729
Again, you don't have to. Really read any or all of it unless I tell you specifically.

196
00:21:02,730 --> 00:21:09,000
Like, please read this. So this paper, this Richardson and Robbins paper, a good chunk of this lecture,

197
00:21:09,000 --> 00:21:12,960
especially the last half, which is about swigs that we'll get to or the last.

198
00:21:14,060 --> 00:21:19,340
Some proportion is based on this paper.

199
00:21:19,340 --> 00:21:24,380
I think it's a really nice paper. It is long. It is like more than 100 pages.

200
00:21:24,680 --> 00:21:31,520
So the relevant parts to us are maybe like the first 40 pages.

201
00:21:32,870 --> 00:21:38,810
Read it if you are interested to read it or if you want like more details about the technical details of this.

202
00:21:38,820 --> 00:21:45,200
I'm again sorry that this projector is bad and it's hard to read this, but you can get to this on canvas.

203
00:21:46,590 --> 00:21:50,079
Okay. All right.

204
00:21:50,080 --> 00:21:55,180
So we're going to come back to that. Independent errors assumption all the way to the end.

205
00:21:55,510 --> 00:22:01,660
For now, we're going to turn our attention to the back door criterion. So this is going to help us say I have a DAG.

206
00:22:02,380 --> 00:22:09,730
It corresponds to a causal model. I want to use my dog to figure out what variables I should condition on in order to get that conditional

207
00:22:09,730 --> 00:22:15,610
exchange ability that I know is so important because I have so far we have this like one big result,

208
00:22:15,610 --> 00:22:19,090
right? Which is that conditional identification result.

209
00:22:19,420 --> 00:22:22,750
And in order to get that result, we need conditional exchange ability.

210
00:22:23,140 --> 00:22:27,010
And this criterion is going to tell us how to get conditional exchange ability.

211
00:22:29,110 --> 00:22:32,200
All right. So you sort of just said this.

212
00:22:32,200 --> 00:22:36,790
So I have this graph and I want to know. Is.

213
00:22:38,070 --> 00:22:46,080
O t the counterfactual. So given that I changed the treatment conditionally independent of T and if it's not.

214
00:22:46,290 --> 00:22:54,840
What do I have to condition on to make it so? Do people have opinions about this now before we launch into the rules for how to figure this out?

215
00:22:58,100 --> 00:23:01,460
Okay. Think about if you think of tea as conditionally independent of tea,

216
00:23:01,700 --> 00:23:07,310
and then we'll do a little poll for yes and no to think if you think it's yes versus no.

217
00:23:08,570 --> 00:23:12,860
All right, raise your hand if you think our tea is conditionally independent of tea.

218
00:23:14,770 --> 00:23:17,860
Raise your hand if you think it's not. Okay.

219
00:23:18,310 --> 00:23:23,540
Pretty unanimous. Most people seem to think our party is not conditionally independent of tea.

220
00:23:23,560 --> 00:23:30,450
Does someone want to tell me why it's not? You all have this opinion.

221
00:23:31,320 --> 00:23:37,620
Someone has an opinion. Why? Or. Z.

222
00:23:37,680 --> 00:23:45,970
And what do you think? So know I think what we all know is that some of.

223
00:23:49,700 --> 00:23:56,190
Sorry. That's okay. Yeah, I think her opinion was her own.

224
00:23:56,820 --> 00:24:02,070
You're right. Yeah. So she is a parent of. Which means that we know that O is not independent.

225
00:24:02,370 --> 00:24:05,370
But what about the counterfactual? Our party. Right.

226
00:24:05,370 --> 00:24:12,319
If I intervene on. T I can get the world where everyone got t is.

227
00:24:12,320 --> 00:24:16,130
Yes, in the world where everyone got t is no. If I look in this graph.

228
00:24:16,640 --> 00:24:19,750
Yes and you. But yeah.

229
00:24:20,570 --> 00:24:29,540
Severity is okay. Could be like linked to treatment and now you have the conditional, right.

230
00:24:29,540 --> 00:24:33,450
Yeah. So I think that's that's. That's the right answer.

231
00:24:33,460 --> 00:24:39,590
So. Here. We know that severity is affecting treatment and severity is affecting outcome.

232
00:24:40,130 --> 00:24:46,580
So my o of t is going to depend on severity, but the treatment also depends on the severity.

233
00:24:46,580 --> 00:24:51,170
And so o of t is not going to be independent of T unconditionally.

234
00:24:51,320 --> 00:24:56,750
So your answer sort of suggests that the answer to this second one, what do I have to control for?

235
00:24:57,080 --> 00:25:03,260
What should lbe that our answer should be? S And so we're going to find through this lecture,

236
00:25:03,260 --> 00:25:11,250
we're going to find a rule that tells us definitely how do I check what sets I can condition on and what sets I can't condition.

237
00:25:13,430 --> 00:25:18,950
Okay. So informally, there's two sources of lack of exchange ability in a dag.

238
00:25:18,950 --> 00:25:25,970
So one of them is confounders which we'll call common causes, and one of them is colliders or common effects.

239
00:25:26,120 --> 00:25:33,680
And we're going to see how each of these work. If you were in 699 yesterday, you have a head start on what a confounder in a collider is,

240
00:25:35,060 --> 00:25:37,790
or if you took it last year, you also heard me talk about it then.

241
00:25:38,750 --> 00:25:45,620
And then we're going to see how to formalize the statements about confounding and colliding to really get like a rule that works every time.

242
00:25:46,570 --> 00:25:50,090
Okay. So a common cause between.

243
00:25:51,260 --> 00:25:57,230
A common cause of aide y will cause an association between A and why that's not due to the causal effect.

244
00:25:57,300 --> 00:26:05,150
And we we saw this a little bit already. So in the disease treatment example, let's say sicker patients are more likely to receive aid will zero,

245
00:26:05,360 --> 00:26:12,200
and sicker patients are also more likely to have a poorer outcome. It might end up looking like receiving a equals zero gives you a poor outcome.

246
00:26:16,380 --> 00:26:27,840
So the term confounding is old. And actually the exactly what people mean when they say confounding has not been formally always nailed down.

247
00:26:27,840 --> 00:26:35,090
So for us in this class. Confounders are sort of defined a little bit tautological.

248
00:26:35,150 --> 00:26:40,010
So a confounder is a variable which can be used to adjust for confounding.

249
00:26:40,880 --> 00:26:47,180
So in this graph we have L one causes L two, which causes Y and L, one causes a.

250
00:26:47,900 --> 00:26:49,190
So we haven't proven this yet,

251
00:26:49,190 --> 00:26:55,970
but I could adjust for the confounding by conditioning on l one or I could condition on l two and I would do just as well.

252
00:26:56,270 --> 00:27:02,479
So even though l two isn't a common cause of a and why we might say that l two is a confounder.

253
00:27:02,480 --> 00:27:07,810
Because if I condition on it, I can get rid of the confounding. Okay.

254
00:27:08,360 --> 00:27:11,780
Common effects. So colliders, so variable.

255
00:27:12,020 --> 00:27:17,570
L is a collider relative to A and Y if it is a descendant of both A and y.

256
00:27:17,870 --> 00:27:23,750
So I'd like. Important tricky thing about colliders is it's always relative to whatever its parents are.

257
00:27:23,930 --> 00:27:28,340
So l might have like other arrows coming out of it or going into it.

258
00:27:28,520 --> 00:27:35,630
You have to look at like the path that it's on and say it's a collider relative to and y y.

259
00:27:36,530 --> 00:27:41,810
So if we condition on l will induce an association between A and y.

260
00:27:41,870 --> 00:27:47,030
So in this graph a and y are independent. If I condition on l in my.

261
00:27:48,600 --> 00:27:55,920
Conditioned subset, there's now an association between A and Y, and I think that it's not necessarily intuitive that that happens.

262
00:27:57,030 --> 00:28:01,490
So we'll go through a couple examples. So this is.

263
00:28:04,550 --> 00:28:09,220
An example that I have, like. A love hate relationship with.

264
00:28:09,250 --> 00:28:13,540
I think it's very clear. I also think it's sort of like weird an old.

265
00:28:15,770 --> 00:28:18,980
But we'll just we'll do it until I come up with another example.

266
00:28:19,310 --> 00:28:27,890
So let's say that in order to become a movie star, you must be either talented or beautiful.

267
00:28:28,100 --> 00:28:37,339
Either of them is sufficient to become a movie star. And let's also suppose that in the general population, talent and beauty are uncorrelated right.

268
00:28:37,340 --> 00:28:45,230
Looks and brains don't have much to do with each other. And let's say talent and beauty are both have a rate of 10% in the population.

269
00:28:47,960 --> 00:28:56,270
And. So in the tables below I'm showing based on what I just told you.

270
00:28:56,290 --> 00:29:04,360
Talent and beauty are uncorrelated in the population. Either of them is sufficient to help you become a movie star in the population.

271
00:29:05,350 --> 00:29:13,950
If I look at how many people are both beautiful and talented or only beautiful, only beautiful or only talented gray.

272
00:29:13,990 --> 00:29:17,610
I get this two by two table on the left.

273
00:29:17,620 --> 00:29:21,490
Right. So 81% of everyone is neither talented nor beautiful.

274
00:29:21,700 --> 00:29:27,190
We have this 1% that is both talented and beautiful and then 9% each for one of them.

275
00:29:27,910 --> 00:29:32,590
Let's say now I go to Hollywood and I sample just movie stars, right?

276
00:29:32,950 --> 00:29:36,280
All of these people have to be at least talented or beautiful.

277
00:29:37,120 --> 00:29:40,569
Let's say neither of these traits has gives you a particular advantage.

278
00:29:40,570 --> 00:29:44,560
You're just as likely to become a movie star if you're talented versus you're beautiful or you're both.

279
00:29:45,010 --> 00:29:53,770
So this graph I got to by just taking this graph, I got rid of all of the people that were not talented or beautiful,

280
00:29:53,770 --> 00:29:56,829
and then I just normalize the graph so that it all added up to one. Right?

281
00:29:56,830 --> 00:30:00,670
So these three squares add up to 19%.

282
00:30:00,670 --> 00:30:05,470
So I just took 9% divided by 19%.

283
00:30:05,510 --> 00:30:13,480
That gets you 47%. So in among our stars, 47% of them are only beautiful and not talented.

284
00:30:13,720 --> 00:30:16,720
47% of them are only talented and not beautiful.

285
00:30:16,960 --> 00:30:22,540
And then 5%, there's some rounding here. Obviously, of them are both talented and beautiful.

286
00:30:23,560 --> 00:30:27,100
So if we look at the association here, right in the population.

287
00:30:27,490 --> 00:30:33,910
Talent and beauty are independent. The probability that you're talented, given that you're beautiful is still 10%, just like it was.

288
00:30:33,910 --> 00:30:39,340
If I didn't condition, I'd beauty among our stars the probability that you're talented.

289
00:30:40,280 --> 00:30:46,499
Is. 0.1 divided by 0.19.

290
00:30:46,500 --> 00:30:54,810
Right. So I took the 0.1 is I added up the bottom row and divided by the 19% of people who are eligible to become stars.

291
00:30:55,290 --> 00:30:59,249
So 53% of stars are talented.

292
00:30:59,250 --> 00:31:02,910
Right? So that's also 47 plus five. That's how we get those numbers.

293
00:31:04,680 --> 00:31:10,830
If I condition on being beautiful, the probability of being talented goes down to 10%.

294
00:31:11,490 --> 00:31:17,790
Right? So among the stars, you might say that talent and beauty are negatively correlated, right?

295
00:31:17,790 --> 00:31:23,160
If I know that you're beautiful, it's less likely that you're talented than if I didn't know that you were beautiful.

296
00:31:25,450 --> 00:31:31,120
So that's that's what we just said. So this happens because we conditioned on becoming a star.

297
00:31:31,390 --> 00:31:38,680
Right. And that was our collider. So that just to remind us the picture here, talent and beauty both cause stardom.

298
00:31:38,890 --> 00:31:43,870
If I condition on stardom, I've induced a correlation between talent and beauty.

299
00:31:46,010 --> 00:31:49,400
I have questions about this. I think it's sort of surprising.

300
00:31:49,730 --> 00:32:01,270
Maybe you don't hear about this as much as you hear about confounding. Okay.

301
00:32:01,330 --> 00:32:07,840
So there's this feature, which is that colliders can sort of block the action of confounder.

302
00:32:08,140 --> 00:32:13,660
So in this graph, there are actually no common causes of A and Y, right?

303
00:32:13,930 --> 00:32:22,780
So we might say that this path from A to Y is blocked by the fact that this arrow points into L two instead of into Y.

304
00:32:23,140 --> 00:32:27,220
Right? So the collider l two is blocking the path from A to Y.

305
00:32:28,180 --> 00:32:33,490
So the causal marker properties in this graph, we know that A and Y are independent.

306
00:32:34,000 --> 00:32:36,399
The concept of D separation, which we're about to see,

307
00:32:36,400 --> 00:32:43,660
is going to formalize the rules for identifying whether pairs of traits in a graph are independent.

308
00:32:45,820 --> 00:32:49,090
Yeah. Right. Or why?

309
00:32:49,090 --> 00:32:54,190
For most of us, how do we build really necessary carburetors?

310
00:32:55,030 --> 00:32:58,950
Yeah. So. Are we good on?

311
00:32:59,250 --> 00:33:03,540
Are you good on? How I got from this population? Proportions to the stars.

312
00:33:03,540 --> 00:33:06,390
Proportions. Does that make sense? Yeah. Okay.

313
00:33:06,780 --> 00:33:14,549
So if I just look in this among the stars and I look at the unconditional probability of being talented,

314
00:33:14,550 --> 00:33:19,290
which is 53%, and I look at the conditional probability of being talented.

315
00:33:19,300 --> 00:33:25,530
So if I know that you're beautiful, you only have a 10% chance of being talented.

316
00:33:26,430 --> 00:33:30,450
If I don't know that you're beautiful, you've a 53% chance of being talented.

317
00:33:31,200 --> 00:33:36,570
So this is sort of this example is supposed to be like.

318
00:33:38,280 --> 00:33:49,940
It's supposed to be exploiting a like. Sort of trope of like, oh, very beautiful movie stars can't act right or like great acting movie star.

319
00:33:49,950 --> 00:33:56,000
They're like, not beautiful. I don't think that either of those is true, but it's true.

320
00:33:56,160 --> 00:34:02,100
The example is supposed to be like exploiting some intuition about that, which is,

321
00:34:02,100 --> 00:34:06,600
again, at some point I'll find an example that maybe is a little bit better.

322
00:34:06,990 --> 00:34:13,040
Does that make sense? When I say it's negatively correlated, what I mean is that conditional on being beautiful?

323
00:34:13,050 --> 00:34:19,140
Your probability of being challenged is lower, right? So if if it was higher, I would say that was positively correlated.

324
00:34:22,210 --> 00:34:25,540
The like. The other intuitive way to think about this is like.

325
00:34:26,510 --> 00:34:32,270
Sort of an Ockham's Razor argument that like we have a condition stardom and there's.

326
00:34:33,430 --> 00:34:36,430
Two explanations for why you're a star, right?

327
00:34:36,430 --> 00:34:40,420
You could be a star because you're beautiful. You could be a star because you're talented.

328
00:34:41,410 --> 00:34:47,830
The Occam's Razor argument is, if I know you're beautiful, that's enough of an explanation to tell me why you're a star.

329
00:34:47,920 --> 00:34:52,749
I don't have to add in that you're talented. So it's more likely that you just got here because you're beautiful than that.

330
00:34:52,750 --> 00:34:59,650
You got here because you're both beautiful and talented, right? In this world where that's not like.

331
00:35:00,430 --> 00:35:09,180
It's not better to have both. Okay.

332
00:35:10,140 --> 00:35:16,620
So this additional point, I think the side may have gotten moved from where it originally.

333
00:35:17,070 --> 00:35:22,380
So something to keep in mind is that there's no statistical definition of confounding.

334
00:35:22,530 --> 00:35:29,519
So something that you might hear given as a statistical definition of confounding is that a

335
00:35:29,520 --> 00:35:35,000
confounder is associated with the exposure with a it's associated with the outcome with Y,

336
00:35:37,440 --> 00:35:40,350
and it's not on the pathway of interest between the exposure and the outcome.

337
00:35:41,220 --> 00:35:48,150
So this isn't enough to tell you that it's a confounder you need to know or be able to guess at the DAG.

338
00:35:48,420 --> 00:35:53,760
Right. So in this graph, L two satisfies all of these criteria, right?

339
00:35:53,760 --> 00:35:57,570
So it's, it's associated with a it's associated with Y.

340
00:35:58,320 --> 00:36:03,360
There's it's not on a pathway from A to Y, but it's not a confounder.

341
00:36:03,660 --> 00:36:07,379
And if I condition on it, I'm going to cause myself problems.

342
00:36:07,380 --> 00:36:14,550
Right? Because we just learned that if I condition on L two, I'm going to induce an association between L one and Y,

343
00:36:14,820 --> 00:36:21,000
and that's going to give me an association between A and Y that's not reflective of the causal effect.

344
00:36:23,400 --> 00:36:29,610
So the big thing to remember is that your data alone cannot tell you if you have confounding or not, right?

345
00:36:29,620 --> 00:36:36,329
So I can't just look at like a table of values and say, well, these things are associated and if I adjust for it,

346
00:36:36,330 --> 00:36:40,950
the estimate changes or some other statistical measure and therefore I have confounding.

347
00:36:43,710 --> 00:36:50,340
All right. So back to D separation. So we want this definition that's going to tell us when two variables are independent.

348
00:36:52,870 --> 00:37:00,670
So we're going to start with a path. So a path is, as we learned, a path last week, so or last.

349
00:37:01,810 --> 00:37:06,040
Monday. It has not been a week since Monday. So just to remember what our path is, right?

350
00:37:06,040 --> 00:37:09,760
A path is a series of nodes that are connected by arrows.

351
00:37:09,940 --> 00:37:14,800
And arrows can go in any direction. So a path is blocked.

352
00:37:15,220 --> 00:37:21,970
If there's a collider in the path for it to arrows collide and you're not conditioning on the collider.

353
00:37:23,270 --> 00:37:31,700
Or it contains a non collider that is being conditioned on a path is open if it's not blocked.

354
00:37:31,730 --> 00:37:36,290
So it's just the opposite. So it doesn't contain a collider.

355
00:37:37,370 --> 00:37:40,550
And we're not conditioning on anything in the past or.

356
00:37:42,660 --> 00:37:46,200
There are conditioned colliders and we're not conditioning on non colliders,

357
00:37:46,500 --> 00:37:52,080
etc. So two variables are separated if all paths between them are blocked.

358
00:37:53,270 --> 00:38:01,670
So we're going to do some examples. And I think I decided we're just going to use the examples in this lecture as our like in class activities.

359
00:38:01,680 --> 00:38:12,350
So if you do the examples like pick a buddy that you're going to do the series of examples with and then that'll be your activity partner.

360
00:38:13,040 --> 00:38:30,649
If you need paper in your name and we're bringing in your paper, something that was very digital over there, right?

361
00:38:30,650 --> 00:38:44,510
Maybe so that I could see like Fast and Furious and you don't have a 20 combine the passive

362
00:38:45,860 --> 00:38:54,589
reaction from the come to do it so there's there's a whole bunch of examples in this lecture.

363
00:38:54,590 --> 00:38:58,459
So we're not going to take a huge amount of time on all of them.

364
00:38:58,460 --> 00:39:07,310
This one is pretty fast. We just want to know in which of these examples are A and y d separated.

365
00:39:15,840 --> 00:39:19,950
So and a flip back to the definition of D separated, right?

366
00:39:19,950 --> 00:39:24,300
So D separated there. D separated if all the paths between them are blocked.

367
00:39:25,170 --> 00:39:30,410
Otherwise, they're not d separated. Oh, I'm sorry.

368
00:39:30,430 --> 00:39:34,030
The square means we're conditioning on the variable. I realized I didn't tell you what that means.

369
00:39:34,300 --> 00:39:38,950
So if the variable is a square, we're conditioning on it. If it's a circle, we're not conditioning on.

370
00:39:39,510 --> 00:39:47,770
I want to say, this is what happens when they're not connected.

371
00:39:48,090 --> 00:39:54,840
Yes. There is no loop at the center.

372
00:39:58,210 --> 00:40:07,460
Plus, it's too much for sharing these.

373
00:40:07,540 --> 00:40:13,390
I'm really glad you mentioned a path I thought would happen, which has never happened.

374
00:40:14,890 --> 00:40:20,240
Right. I this.

375
00:40:22,690 --> 00:40:26,580
But here we have that connection or something. Yeah.

376
00:40:28,630 --> 00:40:32,970
Right now we need to. About how far?

377
00:40:33,250 --> 00:40:36,940
All right. Let's do it like it is. One and a half more minutes.

378
00:40:37,870 --> 00:40:42,790
Something like that as it is. Yeah, that's what it is now.

379
00:40:43,540 --> 00:41:01,770
Yeah. And check my definition of motion to do so.

380
00:41:02,970 --> 00:41:08,320
Me for someone who knows how to use it in there.

381
00:41:08,980 --> 00:41:14,050
All right, you guys sort of landed on your answers. Let's say yes.

382
00:41:16,450 --> 00:41:22,540
Okay. You're going to hang on your paper. I would say again, there's no points for getting it right or wrong.

383
00:41:22,540 --> 00:41:29,260
So it doesn't matter if you correct yourself on the paper, like, I'm not going to judge you for for getting it wrong.

384
00:41:31,180 --> 00:41:34,480
Okay. So let's start with this first one. So let's remember. So.

385
00:41:37,520 --> 00:41:41,210
Two variables are separated if all paths between them are blocked.

386
00:41:42,430 --> 00:41:50,290
A path is blocked if two arrowheads collide at a variable that is not conditioned on and which has no descendants in the conditioning set,

387
00:41:50,620 --> 00:41:54,070
or it contains a non collider that is being conditioned on.

388
00:41:54,490 --> 00:41:57,670
And just as a reminder. What is a path?

389
00:41:58,750 --> 00:42:03,640
Let's see if we can find what a path is. Fine.

390
00:42:03,740 --> 00:42:05,440
Slide seven. Thanks.

391
00:42:06,020 --> 00:42:13,730
So a path is a sequence of edges in which each edge contains a node from the previous edge or the yes, a node from the previous edge.

392
00:42:14,720 --> 00:42:21,080
So go back to our examples. Okay.

393
00:42:21,350 --> 00:42:25,100
So this first one. How many paths are there from eight to why?

394
00:42:27,690 --> 00:42:30,950
So you take, you say zero.

395
00:42:30,960 --> 00:42:39,040
How come? There is no arrow starting from a medium to yeah, so that's not a path.

396
00:42:39,250 --> 00:42:42,520
A path is just a series of arrows. It doesn't matter what direction they go.

397
00:42:42,730 --> 00:42:49,150
So we'll go back to Slide seven just to remind ourselves we're seven.

398
00:42:50,260 --> 00:42:57,310
Here it is, right? A path is a sequence of edges in which each edge contains a node from the previous edge, a directed path.

399
00:42:57,610 --> 00:43:01,510
All the arrows face the same direction. But a path is just a connection, right?

400
00:43:01,720 --> 00:43:06,700
Get rid of the arrows and just see the lines. That's a path. Okay, so.

401
00:43:09,350 --> 00:43:13,850
How many paths are there from A to Y in the first example? One.

402
00:43:14,000 --> 00:43:17,150
All right. Is that path open or closed?

403
00:43:20,280 --> 00:43:29,870
Okay. Why is it open? It doesn't have a collider and we're not conditioning on anything.

404
00:43:29,880 --> 00:43:33,750
So that path is open. So A and Y are not D separated.

405
00:43:35,350 --> 00:43:41,860
There. I think disconnected is the opposite, but you pretty much are always going to talk about de separation versus enacting separation.

406
00:43:42,040 --> 00:43:54,279
All right. Second example, how many paths are there for me to I still one is that path open or closed and it's closed because of this collider.

407
00:43:54,280 --> 00:43:58,320
Right. Okay. Third example, how many paths remain?

408
00:43:58,330 --> 00:44:02,620
Why one? So one is that path open or closed?

409
00:44:04,270 --> 00:44:07,990
And why is it open? Because we conditioned on the collider.

410
00:44:08,110 --> 00:44:14,890
Yes. Okay. So we are. Not separated.

411
00:44:15,260 --> 00:44:20,110
D Separated, not. D separated. All right, so there's our examples.

412
00:44:21,680 --> 00:44:24,890
Okay. So D separation in the causal mark of property.

413
00:44:25,310 --> 00:44:28,370
So let's say A and B are now sets of variables.

414
00:44:28,670 --> 00:44:36,140
So we have this nice result from Verma in Perl that if A and B are de separated given C,

415
00:44:36,350 --> 00:44:43,250
then we have this conditional independence result and you can prove this from the causal markup property.

416
00:44:43,310 --> 00:44:50,210
We're not going to prove it in class if you want to see the proof. I think that paper is linked on the recommended reading.

417
00:44:50,540 --> 00:44:56,450
So this is this result that is going to get us from d separation to the backdoor criterion.

418
00:44:56,750 --> 00:45:05,400
If a. And b r d separated given c then a is conditionally independent of be given c.

419
00:45:05,580 --> 00:45:15,110
So if we go back to our examples, right? In this one we said a and why are separated giving given nothing right.

420
00:45:15,140 --> 00:45:18,500
So A is conditionally independent of why that's what that result gets us.

421
00:45:20,560 --> 00:45:31,600
In these other cases. A is not conditionally independent of y given l two and in this type one a is not conditionally independent of y.

422
00:45:35,200 --> 00:45:38,830
All right. Faithfulness is the property.

423
00:45:39,550 --> 00:45:45,490
It's the other arrow on that proof. So this proof says A and B are separated.

424
00:45:45,640 --> 00:45:50,110
Then A conditionally independent of B, given C faithfulness is the other arrow.

425
00:45:50,110 --> 00:45:58,750
Faithfulness says if A is conditionally independent of B, given C, then A and B are separated given C.

426
00:45:59,500 --> 00:46:07,420
So faithfulness, violations of faithfulness occur when like confounding effects perfectly cancel each other out, right?

427
00:46:07,420 --> 00:46:11,709
So if you could imagine like a linear model and the confounding effect from one

428
00:46:11,710 --> 00:46:15,280
variable perfectly cancels out the confounding effect from the other variable.

429
00:46:15,910 --> 00:46:25,090
And so Perl calls these incidental cancellations and he defines stable versus unstable, unbiased.

430
00:46:25,090 --> 00:46:30,879
This so unstable, unbiased. This essentially sounds like you didn't condition on this confounder,

431
00:46:30,880 --> 00:46:34,480
but you got lucky because faithfulness doesn't hold right that this confounding

432
00:46:34,480 --> 00:46:39,220
effect was perfectly canceled out by some other effect in your in your graph.

433
00:46:39,670 --> 00:46:48,549
Generally, we're always going to assume faithfulness. So non faithfulness will occur for like very specific values of parameters.

434
00:46:48,550 --> 00:46:50,710
And then otherwise you have faithfulness,

435
00:46:50,980 --> 00:46:57,970
non faithfulness and sort of this like stopped clock is right twice a day idea that like occasionally you can like not condition

436
00:46:57,980 --> 00:47:06,430
on something and get it right because like the effects perfectly cancel out matching studies intentionally violate faithfulness.

437
00:47:06,430 --> 00:47:09,880
So we'll come back to that idea when we get to matching in like several weeks.

438
00:47:11,380 --> 00:47:18,640
Okay. Another example. So which pairs in this which pairs in this graph are separated.

439
00:47:18,990 --> 00:47:24,520
And I'll say. Is it worth it to write down all of them?

440
00:47:24,520 --> 00:47:44,920
Probably not. Pick a couple and write down a few that are separated by an agent and.

441
00:47:50,410 --> 00:47:57,069
Marriage is in the hands of humans.

442
00:47:57,070 --> 00:48:03,400
And the outcome would be if you're not internationally described, you need a partner.

443
00:48:03,520 --> 00:48:06,840
Are you just doing stuff on your own? I mean, if they're not, yes.

444
00:48:09,560 --> 00:48:13,620
Yeah. If that's fine with you, that's fine otherwise.

445
00:48:13,630 --> 00:48:19,420
But let's just take a couple of minutes on that.

446
00:48:19,430 --> 00:48:27,880
So break right down like any two pairs that you think are separated is actually one, you know?

447
00:48:29,440 --> 00:48:29,820
Yeah,

448
00:48:30,010 --> 00:48:53,020
I think people have impressed me with the treatment as treatment in severity because the brain of three people have all been subject to get trying.

449
00:48:54,460 --> 00:49:03,900
But is that the only part or you know, or is that the other part of strategy, treatment or anything else?

450
00:49:06,940 --> 00:49:10,870
Oh, yeah, exactly.

451
00:49:11,980 --> 00:49:14,460
So then this is what it is.

452
00:49:14,470 --> 00:49:33,730
It turns out it is a fundamental point because it's a narrative and you're with the number one issue there is that you have.

453
00:49:34,570 --> 00:49:38,950
All right. Let's come back together. Let's get a couple of things.

454
00:49:39,730 --> 00:49:43,310
Do you guys want, like 30 more seconds with you?

455
00:49:44,020 --> 00:49:48,249
All right. All right, let's come back. So let's have a couple volunteer pairs.

456
00:49:48,250 --> 00:49:52,670
Tell us like a couple pairs that they think are separated and.

457
00:49:56,070 --> 00:50:00,380
When you do an agent severity and agent errors.

458
00:50:00,810 --> 00:50:08,700
Okay, so why are age and severity separate? There is only one pass between here as age and there is a higher yes.

459
00:50:09,030 --> 00:50:19,920
And then the other one you had was age and adherence Asian in the areas there's are you to pass this along has 8082 has as an 80 as old 80.

460
00:50:20,820 --> 00:50:24,000
Both have a nine year old. Yep, I agree with that.

461
00:50:24,630 --> 00:50:29,720
Who has another pair. And I'll pick the same pair.

462
00:50:30,600 --> 00:50:38,630
Mary Lou, what about your group? What do you think? Oh, yeah. And I think the energy and perseverance is also a pair of these after that.

463
00:50:38,780 --> 00:50:45,950
Uh huh. Yeah. Because they have to as at0 as and t as.

464
00:50:46,460 --> 00:50:49,670
Oh, you're right. There are two paths. We only had one before.

465
00:50:49,670 --> 00:50:52,940
Right. We said ats. But you're right, there's also eight O. S.

466
00:50:53,150 --> 00:51:02,600
Mhm. And for the short term path the T is according to at0 as the O is required.

467
00:51:02,690 --> 00:51:10,220
Yeah. Yeah. So this is a good example. So if I look at this path at 0st is into collider in that path.

468
00:51:10,520 --> 00:51:14,660
So this is we were talking about the colliding is always sort of relative to the parents.

469
00:51:14,870 --> 00:51:22,320
So in this path t is not a collider, but O is because it's at the points of two eras really.

470
00:51:22,390 --> 00:51:32,860
And we try and you have want to add one thing at a time of age, an outcome.

471
00:51:32,900 --> 00:51:36,520
Yeah. Our age and outcome D separated.

472
00:51:37,330 --> 00:51:44,390
What's the walk me through it. And.

473
00:51:48,620 --> 00:51:52,129
So I think they're not de separated.

474
00:51:52,130 --> 00:51:57,560
And the reason is there's this past age treatment outcome and that car is not blocked.

475
00:51:57,920 --> 00:52:03,770
So since we didn't if we conditioned on treatment, if we conditioned on treatment, that would block that path.

476
00:52:03,830 --> 00:52:09,860
Right. What we discussed. I see. Okay. All right.

477
00:52:09,860 --> 00:52:15,679
So the one that I picked was treatment and adherence, right?

478
00:52:15,680 --> 00:52:23,360
So when we had our causal mark of property, the causal mark of property required be always to condition on the parents.

479
00:52:23,630 --> 00:52:31,610
So the causal mark of property, let me say, treatment and adherence are independent, conditional on the parents of treatment.

480
00:52:32,510 --> 00:52:40,820
But now that I have this separation property, I can say that treatment and adherence are independent unconditionally.

481
00:52:40,840 --> 00:52:49,920
Right. I have. Looks like two paths from treatment to adherence treatment t0 ad and that's blocked by the collider.

482
00:52:49,970 --> 00:52:56,360
Oh and I have ts0 ad and always also a collider on that path.

483
00:52:56,570 --> 00:53:00,710
Right. So treatment and adherence are d separated in this graph.

484
00:53:03,680 --> 00:53:10,820
There are questions about separation. After a little after you practice a few of these, you'll start to be able to like see them more naturally.

485
00:53:10,830 --> 00:53:14,720
Like if it's separated, if it's not, is in practice, in homework one.

486
00:53:15,960 --> 00:53:21,820
Okay. Back our path. So we just spent a bunch of time trying to figure out if there's paths.

487
00:53:21,850 --> 00:53:27,910
Now we need a special kind of path, which is a backdoor path. So back to our path from A to why is just a path.

488
00:53:28,270 --> 00:53:36,220
But I'm going to orient the first arrow. So the first arrow has to go into what a all of the other arrows can be pointing any direction.

489
00:53:36,670 --> 00:53:40,690
So let's just do this one as a group. Let's start with the first one.

490
00:53:40,990 --> 00:53:44,590
How many backdoor paths are there from A to Y in the top left?

491
00:53:48,340 --> 00:53:51,730
When you say one, what is it and why?

492
00:53:51,880 --> 00:53:57,150
How come the path to why is not a back door path? The first of the first age people going.

493
00:53:57,550 --> 00:54:04,300
Yes. Right. So we have this condition. The first arrow of my path has to go into a how about the top?

494
00:54:04,300 --> 00:54:08,620
Right. How many backdoor paths from ADA? Why? And you shaking your head.

495
00:54:10,700 --> 00:54:15,980
So. Zero zero. Okay. How come there's no nose leaving it there?

496
00:54:16,370 --> 00:54:19,620
Yes. Yeah. All right. Last one.

497
00:54:19,640 --> 00:54:23,210
A little bit more complicated. How many backdoor paths are there from a to why?

498
00:54:34,680 --> 00:54:37,740
All right, let's try put up the number of fingers that you think.

499
00:54:40,330 --> 00:54:43,900
Huh? And let's see if we can get more fingers in the background.

500
00:54:45,070 --> 00:54:48,550
Uh huh. So it's a mix. Some say two, some say one.

501
00:54:49,060 --> 00:54:53,379
Malcolm, you say two. What do you see? I think a new one.

502
00:54:53,380 --> 00:54:56,980
L y and au1. lu2.

503
00:54:57,250 --> 00:55:03,130
Yeah, I think that's right. I agree with that. So too. So even though these share like their beginning part, we're still going to call that two parts.

504
00:55:06,020 --> 00:55:10,460
All right, so I'm pretty about that last part. Yeah. So it's up in the air.

505
00:55:11,090 --> 00:55:17,060
Yeah. So we're trying to. We start. The one way to count the vector paths is just start with all the paths.

506
00:55:17,360 --> 00:55:20,160
So there's three paths from A to Y. There's.

507
00:55:21,050 --> 00:55:25,760
And a path is just I'm going to walk from one note to the other and I'm always going to follow in parallel direction.

508
00:55:26,270 --> 00:55:31,999
Yeah. So you don't care about direction except for that first node. So the first node I'm going to orient.

509
00:55:32,000 --> 00:55:36,799
So one way to do it, take off all the arrowheads and write down the path.

510
00:55:36,800 --> 00:55:39,620
So the path to identify the paths, you don't need to see the arrow heads,

511
00:55:40,130 --> 00:55:46,070
put the arrow heads back on and then just keep the paths where that first edge was into.

512
00:55:46,070 --> 00:55:49,400
A You don't even in fact that's the only arrow had union.

513
00:55:49,490 --> 00:56:00,200
So take the arrow heads off. Write down the paths put on just the arrow head for the the first arrow and then see if that goes into

514
00:56:00,350 --> 00:56:05,360
or out of a and then that tells you what your back to a parser we're going to see an alternative.

515
00:56:05,360 --> 00:56:09,680
If you hate the backdoor criterion, we'll have an alternative for you.

516
00:56:11,420 --> 00:56:16,690
Okay. Do you care whether it is it is closed or not?

517
00:56:17,780 --> 00:56:21,410
We're going to forget it before we care if it's closed.

518
00:56:21,500 --> 00:56:28,219
But we just have to know what a backdoor passes. But then we're going to care if it's closer open than can we traipse on that crap.

519
00:56:28,220 --> 00:56:32,570
Which are the two patsies? I can't see it. I'm just. Yeah, I got confused.

520
00:56:32,750 --> 00:56:37,100
Yeah. So the first path is this like m shaped path.

521
00:56:39,420 --> 00:56:43,600
And then the second path is this like melting mountain shaped powder?

522
00:56:45,300 --> 00:56:49,470
Yeah. It doesn't matter what it is.

523
00:56:50,370 --> 00:56:56,909
Oh, yeah. So a backdoor path in the definition of backdoor path is not whether it's closed or open.

524
00:56:56,910 --> 00:57:00,990
There's nothing about the direction of any of the arrows except for that first arrow.

525
00:57:01,230 --> 00:57:05,820
So a path is just a connection of nodes that doesn't care about the arrows.

526
00:57:06,000 --> 00:57:10,620
A backdoor path cares only about the arrow of the first the first arrow.

527
00:57:11,080 --> 00:57:14,159
So that that sorry that ends from you on.

528
00:57:14,160 --> 00:57:18,510
That is whatever path that's part of it is essentially what we're calling these assets.

529
00:57:18,690 --> 00:57:23,760
Mm hmm. Yeah. Right. So one thing you can do is just look at a and see how many entrances are there to A,

530
00:57:23,970 --> 00:57:26,580
and those are going to be the beginnings of all your background paths.

531
00:57:26,580 --> 00:57:32,970
And then if you can get to Y by starting on that edge, then you've found a backdoor path.

532
00:57:34,590 --> 00:57:41,340
Okay, so the backdoor criterion is a way to connect our graph to exchange ability.

533
00:57:41,790 --> 00:57:46,410
So if I have a set of variables, this is our like backdoor criterion theorem.

534
00:57:46,830 --> 00:57:50,340
I have a set of variables. L if.

535
00:57:51,130 --> 00:57:58,720
L blocks every backdoor path between A and Y and contains no descendants of A.

536
00:57:58,870 --> 00:58:02,650
And that's an important one. And we'll come back to why we need that.

537
00:58:03,370 --> 00:58:06,700
Then I can make this exchange ability.

538
00:58:06,940 --> 00:58:11,050
Statement Y of a is conditionally independent of a given l.

539
00:58:12,590 --> 00:58:16,490
So this is a big result for us. We're going to practice using it a little bit.

540
00:58:16,790 --> 00:58:23,299
So just to review my set of variables is sufficient to condition on to give me

541
00:58:23,300 --> 00:58:27,890
exchange ability if it blocks every back door path and contains no descendants of a.

542
00:58:31,200 --> 00:58:35,460
All right. So these are called the backdoor criteria. Okay, so.

543
00:58:37,160 --> 00:58:40,910
Through this one together. What set of variables make and why?

544
00:58:40,910 --> 00:58:45,710
Conditionally independent? Let's start with how many backdoor paths are there from A to Y?

545
00:58:46,430 --> 00:58:50,390
Hold up a number of fingers. This looks like a graph we saw before.

546
00:58:50,810 --> 00:58:54,740
Yeah. So there's one backdoor path. It's the m shaped path.

547
00:58:56,400 --> 00:59:00,240
What do I need to condition on to make a and y conditional exchangeable?

548
00:59:04,740 --> 00:59:08,490
I see lots of puzzled faces. So actually, let's take a minute to do this with your partner.

549
00:59:09,240 --> 00:59:16,260
We'll take a minute or so before we get to that.

550
00:59:19,140 --> 00:59:42,340
In terms of what I mean, you know, I think I was here to tell you, is that back door machine, you know, this is what we're saying right now.

551
00:59:42,640 --> 00:59:48,620
You know what? This you know, descendants of descendants.

552
00:59:48,960 --> 00:59:53,390
So the only this child's great if you can follow a directed passageway.

553
00:59:54,210 --> 00:59:59,070
So why is a descendant of an eagle or like Ella's a descendant of you?

554
00:59:59,070 --> 01:00:05,700
Once I saw you today.

555
01:00:07,080 --> 01:00:12,180
If you are interested in reconditioning.

556
01:00:12,930 --> 01:00:31,980
Yeah, so we don't have an excuse because there is no back to back or past or as you say, there are like multiple there's multiple answers here.

557
01:00:32,370 --> 01:00:39,720
There's multiple correct answer ones that have to be quite a lot of noise or a lot of additional information.

558
01:00:40,020 --> 01:00:43,460
So if you can go back in your life and your belongings,

559
01:00:44,460 --> 01:00:58,950
there are other things that you might not remember in the way you may need to active condition aggregate.

560
01:00:58,950 --> 01:01:03,060
Is that all right? All right, all right. Let's come back. I think it's already.

561
01:01:03,360 --> 01:01:08,240
I'm here. I was somebody have an idea that they want to share it with you.

562
01:01:08,520 --> 01:01:15,450
What set of variables can we conditioned on to make a and why in life conditionally exchangeable.

563
01:01:17,940 --> 01:01:22,830
There's there's multiple correct answers. Yeah, there's a question.

564
01:01:22,860 --> 01:01:30,420
Yes. Is Alex here? It's a collider between you one and you to write the tips of two arrows.

565
01:01:31,080 --> 01:01:34,730
Right. So is that Black Panther's name? Yeah.

566
01:01:34,740 --> 01:01:40,850
So one answer is. It sounds like you're getting towards one answer, which is, I don't think you need a commission on.

567
01:01:41,420 --> 01:01:47,270
You don't have to condition on anything. Right. A and Y are conditionally exchangeable on nothing.

568
01:01:47,300 --> 01:01:53,150
Conditional on nothing. Unconditionally. Right. And we can see that because there's one backdoor path.

569
01:01:53,950 --> 01:01:57,310
And it's blocked by Elle, so I'm good to go.

570
01:01:58,000 --> 01:02:00,130
But there's other options. What's another?

571
01:02:01,190 --> 01:02:08,660
Another option, another set of variables that I can conditioned on to make a why exchangeable besides the empty set.

572
01:02:13,050 --> 01:02:19,140
You seem like you have an idea or you nodded affirmatively, which is a good way to get called on.

573
01:02:19,350 --> 01:02:26,070
What do you think? You want me to come?

574
01:02:26,080 --> 01:02:31,590
A computer? Yeah. Yeah. I could condition on you one or you two or both of them.

575
01:02:31,740 --> 01:02:40,760
Oh. Is there another set that I could condition? What if I condition on l is that.

576
01:02:42,350 --> 01:02:48,660
A good idea about idea. That is bad, right?

577
01:02:48,670 --> 01:02:53,430
So if I condition not going to open up that path, can I save myself by also conditioning on something else?

578
01:02:54,970 --> 01:03:01,750
So you. What happens if the commissioner and all three of you are now two?

579
01:03:01,930 --> 01:03:05,500
Yeah. So if I condition on l, I'm going to open the pen.

580
01:03:06,340 --> 01:03:11,050
If I want to close the path again, I have to condition on something that's not a collider that's in the patent.

581
01:03:11,230 --> 01:03:17,200
So I could condition on U one and L or two and L or all three.

582
01:03:18,640 --> 01:03:24,940
In fact, the only the only answer that gets me not exchangeable is just condition on L.

583
01:03:25,780 --> 01:03:28,960
So every other set gives me conditional exchange ability.

584
01:03:31,600 --> 01:03:37,960
All right, so this is an example of M bias, it's called that, because that's an M shape.

585
01:03:38,860 --> 01:03:43,650
And the bias happens when you condition on the thing at the at the point of the M.

586
01:03:47,230 --> 01:03:51,830
Okay. What about this one? What set of variables makes a and why?

587
01:03:51,880 --> 01:03:55,480
Conditionally exchange of all ads? Take a couple of minutes.

588
01:03:55,870 --> 01:04:04,030
Write down your answers. And I knew that wasn't going anywhere.

589
01:04:07,600 --> 01:04:15,350
So I got to that worked out in addition to something else.

590
01:04:17,350 --> 01:04:23,650
Well, it depends on which we had anything else that was passed out in the letter.

591
01:04:24,430 --> 01:04:27,670
So it's set in the same way. You don't have to condition on anything.

592
01:04:28,300 --> 01:04:34,260
All right. Well, but you have.

593
01:04:34,400 --> 01:04:40,390
Yeah. Yeah. So in that case, it is not radically different.

594
01:04:40,480 --> 01:04:44,410
Right. So I think we have never said a word about, you know, how.

595
01:04:44,410 --> 01:04:48,540
Yeah, you or you can say that. I was going to say I don't know, but that's fine.

596
01:04:48,550 --> 01:04:51,850
I just opened it right up. Yeah.

597
01:04:53,440 --> 01:04:56,710
Because, you know, because think that that didn't happen for a while.

598
01:04:58,520 --> 01:05:01,680
You Tube. Well, don't you.

599
01:05:02,320 --> 01:05:05,559
Well, you don't want to condition that as well as we will.

600
01:05:05,560 --> 01:05:09,010
Morris, do you want to go? Could you?

601
01:05:09,010 --> 01:05:13,630
Do you want to? That's the question. Right. And probably second.

602
01:05:14,850 --> 01:05:18,810
Yeah, well, there's at least something you can do.

603
01:05:19,750 --> 01:05:23,220
Right. Right. And you've got more mission on UI.

604
01:05:23,260 --> 01:05:29,400
And then you want to know what again, you want to decide on when you do that.

605
01:05:29,470 --> 01:05:42,550
But you definitely have to condition whenever there is an indication that you can control your condition and other things.

606
01:05:43,650 --> 01:05:55,830
There was a reason you want one now and everything.

607
01:05:55,850 --> 01:06:00,940
Now we know there's nothing quite.

608
01:06:01,360 --> 01:06:05,370
You've mastered the Dag. You can solve any of these problems.

609
01:06:05,380 --> 01:06:08,580
No. I remember in six that I tried to draw that.

610
01:06:08,590 --> 01:06:14,410
Now just seven Q. Yeah. I think your guys would say I feel it definitely was.

611
01:06:14,470 --> 01:06:22,600
I didn't think I felt any of those. Yeah. You'll figure it out if you get and hopefully by the end you all that all know your way

612
01:06:22,600 --> 01:06:30,340
around it that we didn't have a real problem that we're not going to experience now.

613
01:06:31,680 --> 01:06:37,880
And and I think it was variance like I just think about it.

614
01:06:38,980 --> 01:06:44,710
All right. Let's let's come back maybe let's hear from a group that we haven't heard from.

615
01:06:45,310 --> 01:06:56,890
I think, uh, moving and your name gives you.

616
01:06:57,070 --> 01:07:00,430
Yeah, we think there are two parts from X and Y.

617
01:07:00,550 --> 01:07:10,980
Mm hmm. The first line I do want to provide is from any time you want to tell us why.

618
01:07:11,110 --> 01:07:19,030
Mm hmm. And we think that you are in some video from somewhere that you can control for you one.

619
01:07:19,240 --> 01:07:29,290
And then A and Y will be conditionally independent, conditional on one condition that I you know, I want to introduce ourselves.

620
01:07:29,740 --> 01:07:34,059
Aha. So let's, let's check you listed the back to our paths.

621
01:07:34,060 --> 01:07:38,710
So now we can just check if each factor of path is open or closed conditional on you one.

622
01:07:39,280 --> 01:07:44,650
So this first one, the m shaped path. Is that open or closed conditional on you one.

623
01:07:48,100 --> 01:07:51,219
Yeah. Yeah, I think I think, you know, I think you're on the right path.

624
01:07:51,220 --> 01:07:54,820
And then I feel like I've called it the melting mountain shaped path.

625
01:07:55,180 --> 01:08:01,089
Is that open or closed conditional on you one. GROSS okay.

626
01:08:01,090 --> 01:08:05,290
So we blocked all the backdoor paths, so you got it. So conditional on you one.

627
01:08:06,350 --> 01:08:09,980
A and why? Our conditionally exchangeable.

628
01:08:11,660 --> 01:08:17,270
Are there other sets that you could use, any of the sets that include you on any set that includes you on?

629
01:08:17,270 --> 01:08:21,770
Is there a set that doesn't include you? One that you could use and order to you, Ellen.

630
01:08:21,770 --> 01:08:25,430
You too. Right. So if we just conditioned on L.

631
01:08:26,620 --> 01:08:31,750
We can block this mountain shaped path, but we've opened up the M shaped path.

632
01:08:32,170 --> 01:08:36,400
So to close the M shaped path, we could condition on you one or you two.

633
01:08:37,300 --> 01:08:40,590
And since I've told you not to condition on you one condition on YouTube.

634
01:08:40,770 --> 01:08:46,500
Right. So, Alan, you two does it just you one does it. Any of the sets that include you want to do it, I.

635
01:08:48,390 --> 01:08:53,250
All right. And this is the sense, the answers. So.

636
01:08:54,490 --> 01:09:02,620
If you can't measure, you want in. You too. If they're unobserved, you are out of luck in this case.

637
01:09:02,770 --> 01:09:13,270
Right. You have to condition on L to block the mountain shaped path and you can't condition on L if you want the m shaped path to be blocked.

638
01:09:13,570 --> 01:09:20,590
So if both of these are unobserved and this is your dad, you can't get conditional exchange ability.

639
01:09:24,290 --> 01:09:32,390
That makes sense to everybody. Okay.

640
01:09:32,870 --> 01:09:36,980
Here's another one. What set of variables makes a and why conditionally exchangeable?

641
01:09:37,490 --> 01:09:38,600
Let's do this one together.

642
01:09:40,430 --> 01:09:46,610
Are they conditionally exchangeable without conditioning on anything unconditionally or are they unconditionally exchangeable?

643
01:09:47,390 --> 01:10:06,640
So the way that you would say that. And you would guess, are they unconditionally exchangeable?

644
01:10:11,240 --> 01:10:14,660
Okay. Let's start with an easier one. How many backdoor paths are there from eight to why?

645
01:10:15,980 --> 01:10:19,610
One. Okay, you see two.

646
01:10:20,810 --> 01:10:26,870
I forget the arrow. Okay, so it's one, right? So a u l y is our backdoor path.

647
01:10:27,260 --> 01:10:30,850
Is it open or closed? Open its open.

648
01:10:31,450 --> 01:10:35,620
So they're not conditionally exchangeable because there's an open backdoor path.

649
01:10:36,310 --> 01:10:40,170
What do I have to condition on to block the backdoor path? Yeah.

650
01:10:41,350 --> 01:10:52,549
Yeah. So I have to condition on you. I could condition on l but l is on this pathway between A and Y and L is a descendant of a.

651
01:10:52,550 --> 01:10:57,560
So l blocks the backdoor path, but it violates.

652
01:10:58,250 --> 01:11:05,570
I skipped ahead. So l violates our second condition of the backdoor criterion, which is.

653
01:11:09,530 --> 01:11:13,489
Our conditioning set needs to contain no descendants of a right.

654
01:11:13,490 --> 01:11:19,280
So we have two conditions. We have to block every backdoor path and we can't use descendants of A to do it.

655
01:11:20,180 --> 01:11:27,170
So if we look at this graph, L blocks our backdoor path, but it's a descendant of a, so I can't use L.

656
01:11:29,950 --> 01:11:39,520
Yes. The second condition deck. Could you make the second condition weaker by saying contains no descendant of a that is on any path?

657
01:11:39,820 --> 01:11:44,770
Hmm. No. Okay. Now. And we'll we'll we'll see more about that later.

658
01:11:51,150 --> 01:11:56,050
Okay. In fact, I think we're going to see more about it in this section, although.

659
01:11:58,080 --> 01:12:10,129
And my. I think I'm going to stop here so that we can do a single world intervention graphs all as one piece on the next time we meet,

660
01:12:10,130 --> 01:12:13,890
which is Wednesday, because there's Martin Luther King Day. So we're going to stop here.

661
01:12:15,020 --> 01:12:22,129
You are now armed with the backdoor criterion, which is everything that you need to do all of homework.

662
01:12:22,130 --> 01:12:28,760
One. If you run into problems, come see me in office, hours or otherwise.

663
01:12:29,840 --> 01:12:40,130
There's any questions. We can do those, or we can have eight three additional minutes of your life that you get back due to finishing early.

664
01:12:43,830 --> 01:12:52,290
Okay. Thanks, everybody.

665
01:12:52,290 --> 01:12:59,920
Enjoy your holiday on Monday. Do something nice after the break.

666
01:13:00,540 --> 01:13:04,740
I was happy to be going into the library. Oh, great.

667
01:13:05,060 --> 01:13:12,480
You're right. I mean, that's all I remember.

668
01:13:12,480 --> 01:13:15,690
Just crying and I'm so confused. Media inquiries.

669
01:13:15,990 --> 01:13:21,630
I don't think I can. That makes it clear he's not going to say that again.

670
01:13:23,310 --> 01:13:27,180
That's the biggest thing I got. He was like, you know. Yeah, yeah, yeah.

671
01:13:27,270 --> 01:13:31,320
You're trying to tell me. And I was like, I want you to.

672
01:13:31,680 --> 01:13:35,760
You got it? No, I got. I still haven't. I haven't. I got no record.

673
01:13:35,880 --> 01:13:41,130
Okay. I'll check with I'll start with Nicole. I emailed her about something else I didn't hear about.

674
01:13:41,130 --> 01:13:45,120
So maybe that something's going on with her. Sure.

675
01:13:45,990 --> 01:13:52,140
Okay. I just leave the home. Yeah, I'll take it to the campus site without it.

676
01:13:53,040 --> 01:13:56,200
Yeah. You can promise me that you will register for sure.

677
01:13:56,270 --> 01:14:04,910
Thanks to you that I got everyone that you're going to start with.

678
01:14:05,520 --> 01:14:08,920
Is there anything else you'd like us to?

679
01:14:09,630 --> 01:14:18,600
Yeah. I mean, I think I can add you in there. I think that should be enough for me that you're probably right.

680
01:14:22,680 --> 01:14:29,960
Yeah, I do hers.

681
01:14:30,180 --> 01:14:42,090
So when we come from what you said, is that like where we are also areas put in there is.

682
01:14:42,120 --> 01:14:45,370
And when we think about it. Yes, exactly right.

683
01:14:45,370 --> 01:14:50,849
So when you look at the map or whether some things are collider or not depends on the path.

684
01:14:50,850 --> 01:14:58,440
You can see we look at this past week and it's like, okay, if we look at this, how well is that?

685
01:14:58,440 --> 01:15:03,840
I heard gunshots. Yeah. Oh, yeah.

686
01:15:04,230 --> 01:15:10,200
Just now because they're like, let's see.

687
01:15:10,530 --> 01:15:18,989
So there is no thinking of me as a liar right now in this example.

688
01:15:18,990 --> 01:15:24,000
Right. Our secret, the avenue right now is a collider.

689
01:15:24,170 --> 01:15:39,120
We're going to have one. It means that there's nobody desaparecidos as long as we walk all of them right through December.

690
01:15:39,530 --> 01:15:44,580
You're going back overseas now. Back to the back door criterium.

691
01:15:46,950 --> 01:15:52,020
Oh, right. This one is telling us about conditional independence.

692
01:15:52,350 --> 01:15:55,950
In the back of a freight train, a song is about a conditional exchange.

693
01:15:55,950 --> 01:16:07,110
A village in the back of a grade hearing is about quiet, whereas the desaparecidos just about okay.

694
01:16:09,480 --> 01:16:29,160
But you get to the back from the separation also known as they did so well that says like that be separated in the 21st century.

695
01:16:29,350 --> 01:16:40,530
We didn't know how to read in this year and how we can do that with this because was emotional just just a little bit of the 19

696
01:16:40,670 --> 01:16:51,989
so yeah so the de separation do you know the separation tells us if two variables are discrete and then their conditioning,

697
01:16:51,990 --> 01:16:55,250
you know, and then they're separated by conventional.

698
01:16:56,010 --> 01:17:01,469
So here we're conditioning and nothing you we see that treatment and adherence are being separated and we know

699
01:17:01,470 --> 01:17:08,930
that they are because there is you have to have that one opportunities one and both of those reports on notice.

700
01:17:09,330 --> 01:17:19,379
So the state of our congressional district courts for example.

701
01:17:19,380 --> 01:17:29,940
Gotcha. Yeah so like why we some operational the distinction between conditional independence and conditional release is a little bit subtle.

702
01:17:30,300 --> 01:17:36,240
This one just the community distribution initial exchange guilty is okay.

703
01:17:36,240 --> 01:17:43,830
Yeah, well, thank you. And I'm also wondering what that looks like for this.

704
01:17:43,830 --> 01:17:49,250
Of course. Yes. I think I tried. Are you registered to audit?

705
01:17:49,460 --> 01:17:54,240
Oh, no, I'm not registered. I just want to take this course, which I think is very healthy.

706
01:17:55,490 --> 01:17:58,520
Yeah. Can you register to audit? Okay.

707
01:17:59,030 --> 01:18:05,310
I am supposed to. I supposed to have all of hall auditors registered as part of the.

708
01:18:05,600 --> 01:18:12,399
So they are not looking at me. Oh. But that's how the university wants to go to.

709
01:18:12,400 --> 01:18:18,020
And so I was asked to do that. And so I think if you have trouble registering.

710
01:18:18,320 --> 01:18:21,550
So I'll give you permission. Okay?

711
01:18:21,650 --> 01:18:26,540
Okay. I didn't think it was about that. Yeah. And then you can also get access to the kids, I think.

712
01:18:27,020 --> 01:18:44,150
Okay. Thank you. Yeah. So she really got me here.

713
01:18:52,070 --> 01:19:16,020
All right. One more point that I heard from is on the way I heard.

714
01:19:17,300 --> 01:19:21,740
But it is important that we do.

715
01:19:21,770 --> 01:19:50,400
A longer version of what you have is that I'm on the eastern side of the mine and I hear that.

716
01:19:51,840 --> 01:19:53,850
I don't that.

