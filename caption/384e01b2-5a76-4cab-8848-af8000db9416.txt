1
00:00:00,830 --> 00:00:03,900
You just stay here, okay?

2
00:00:04,260 --> 00:00:08,480
That sounds good. All right. Okay. All right. I'll put with you.

3
00:00:08,550 --> 00:00:15,600
Okay. All right.

4
00:00:19,670 --> 00:00:23,030
This is up to you. So if you want to wear your mask, please do so.

5
00:00:23,960 --> 00:00:42,860
It's not a requirement. All right, let's get started.

6
00:00:43,280 --> 00:00:47,270
Welcome to Biostatistics. 626.

7
00:00:47,840 --> 00:00:51,860
So this is machine learning for health sciences.

8
00:00:57,720 --> 00:01:03,930
So you feel fine here? Obviously you have. That's great.

9
00:01:04,300 --> 00:01:11,670
So the. All right, so everybody got a copy of syllabus from the canvas.

10
00:01:12,540 --> 00:01:16,260
If you have trouble, access canvas, let me know.

11
00:01:17,820 --> 00:01:21,030
If you register class, that's not supposed to be a problem.

12
00:01:21,990 --> 00:01:26,130
If you just want to sit in. That's should be okay in general.

13
00:01:26,340 --> 00:01:31,460
Okay with me. But you need to let me know so I can add you to the.

14
00:01:32,940 --> 00:01:45,390
Um, the cannabis side. Um, so first we go through the main things on the syllabus and then we start the first lecture.

15
00:01:46,200 --> 00:01:52,130
My name is William one. I'm a faculty member in the bio statistics.

16
00:01:53,130 --> 00:01:58,110
My all my office is fourth floor ESP too.

17
00:01:58,110 --> 00:02:08,670
So it's listed here. The office hall where the. I'm 4040 517 is my just opposite the men's room so you know trouble to find.

18
00:02:09,870 --> 00:02:14,580
We have a GSI is you see you shot here. Never mind.

19
00:02:15,990 --> 00:02:20,250
Sure he's not required to be here, but I think if you know.

20
00:02:21,870 --> 00:02:27,060
The person that's easier, but well, we introduce next time.

21
00:02:27,900 --> 00:02:33,150
So the lecture time is Monday, Wednesday, 830, probably first thing in the morning.

22
00:02:35,310 --> 00:02:38,580
The class probably need a little bit of attention.

23
00:02:38,580 --> 00:02:48,690
So this particular time designing should you know how in that regard the office hour is tentative.

24
00:02:50,610 --> 00:02:57,600
You know, I think especially this large class, we cannot meet everybody's need.

25
00:02:57,720 --> 00:03:08,430
So we have a in person office all wear from 10 to 11 after the class on Wednesday and then I usually host a zoom office I'll wear on Friday.

26
00:03:09,900 --> 00:03:12,990
There's a typo. It is so 10 to 11 as well.

27
00:03:13,440 --> 00:03:19,690
So the zoom office all where you could attend anywhere should be easier for I'm than

28
00:03:19,740 --> 00:03:33,569
usually for most of you a Friday is more flexible but if you can make either of these,

29
00:03:33,570 --> 00:03:41,100
you can always email me and then we can arrange a time just for your special need.

30
00:03:43,360 --> 00:03:46,450
Okay. So so those are logistics.

31
00:03:47,260 --> 00:03:52,000
Something else regarded to the. We'll talk about any questions so far.

32
00:03:52,990 --> 00:03:56,980
Good. So let me talk about the course.

33
00:03:57,100 --> 00:04:05,620
So if you are in the biostatistics department, I think there is this is our own machine learning class.

34
00:04:06,100 --> 00:04:16,749
In a way. We want you to know the the basic knowledge of the machine learning and then how you use your existing knowledge,

35
00:04:16,750 --> 00:04:21,159
especially after you have learned the mathematical statistics,

36
00:04:21,160 --> 00:04:27,850
you have learned all these statistical tools, how you approach the problem of machine learning.

37
00:04:29,140 --> 00:04:33,630
I know there are several versions of machine learning class are on campus.

38
00:04:33,640 --> 00:04:40,930
Almost every department like E C has statistics on school of information.

39
00:04:40,930 --> 00:04:45,070
They offer their own version of machine learning.

40
00:04:45,370 --> 00:04:52,080
So what's the difference here? I think we'll go through some of the the history of machine learning on that.

41
00:04:52,090 --> 00:04:59,830
I will try to emphasize something, but the important point I think I have 2.1 is this is an introductory level class.

42
00:05:00,130 --> 00:05:05,770
So don't expect you become an expert of machine learning just after this class.

43
00:05:05,800 --> 00:05:11,080
Even you get an A-plus. You probably don't. You know you don't.

44
00:05:13,490 --> 00:05:18,380
Qualify for a machine learning expert. This is not expert level class.

45
00:05:18,650 --> 00:05:28,100
All right. So that being said, I have, you know, a relatively low requirement for you to enroll this class.

46
00:05:28,970 --> 00:05:37,040
The only prerequisite is 601, meaning graduate level mathematical statistics.

47
00:05:37,350 --> 00:05:47,060
Right. So I expect you know about a single random variable, how to compute the expectation, what probability means.

48
00:05:48,680 --> 00:05:58,850
Actually, besides that, you should be familiar with the usual regression tools like linear regression, logistic regression, and so on and so forth.

49
00:05:59,030 --> 00:06:07,540
Okay. So those are not in, you know, essential to understand the course material.

50
00:06:07,540 --> 00:06:17,800
But it's really helpful for you to kind of a bridge the gap between what you have learned and then what we going to learn in this class.

51
00:06:18,120 --> 00:06:28,659
Okay. So that's actually the second point. So we trying to approach those machine learning techniques,

52
00:06:28,660 --> 00:06:35,590
especially the modern machine learning techniques from the perspective of probability or statistics.

53
00:06:37,990 --> 00:06:44,740
It's fair to say you don't have to have a statistical background to understand machine learning.

54
00:06:45,160 --> 00:06:52,750
So this is a kind of a separate if you see the course offered in other disciplines, like reading a different textbook.

55
00:06:52,750 --> 00:06:56,890
Some of the textbooks I listed here have has that type of the flavor.

56
00:06:56,900 --> 00:07:01,030
You don't need the statistics to study machine learning.

57
00:07:01,480 --> 00:07:10,180
You may come from a completely different perspective and their reach to the the probably the same type of the algorithm.

58
00:07:11,140 --> 00:07:20,860
But having a statistical background is actually, in my view obviously I'm going to be biased here is is a plus, right.

59
00:07:21,820 --> 00:07:27,129
So we could understand the, you know, the advantage and and disadvantage of certain methods,

60
00:07:27,130 --> 00:07:38,530
how we analyze the algorithm and then how do we see that certain algorithm is suitable or unsuitable for a specific type of the problem?

61
00:07:39,940 --> 00:07:45,060
Um. Yeah. So that's the two points kind of important.

62
00:07:45,510 --> 00:07:52,649
So, you know, don't be disappointed if you are right in not well I mean in the introductory level class so if you already know

63
00:07:52,650 --> 00:08:01,140
most of the things you probably should looking uh look for more advanced the topic that's completely okay with me.

64
00:08:01,590 --> 00:08:07,170
And secondly I think that we're going to use a lot of statistical thinking in this class.

65
00:08:09,000 --> 00:08:15,270
All right. So the well, we will. So because of the unique features of this class,

66
00:08:15,270 --> 00:08:24,210
we're going to be mostly using mostly use the the lecture notes, which we will distribute in the canvas.

67
00:08:24,690 --> 00:08:31,020
But there are four different textbooks I listed in the syllabus is a four references.

68
00:08:31,680 --> 00:08:39,509
The first two are highly recommended, but you don't have to read those.

69
00:08:39,510 --> 00:08:47,820
But if you have extra energy after the class and then just want to explore the topic in more depth.

70
00:08:48,690 --> 00:08:53,909
You're welcome to read this. The first one I like those.

71
00:08:53,910 --> 00:09:00,210
I like this book the most. It's called Pattern Recognition and Machine Learning.

72
00:09:00,240 --> 00:09:06,299
Um, it's a thick book. I used to have a hard copy, and then I learned to someone else, and then it got lost.

73
00:09:06,300 --> 00:09:15,600
This always happened that way. The the textbook is a very approachable, um,

74
00:09:15,990 --> 00:09:24,930
meaning you don't need to have very strong mathematical or statistical backgrounds to understand that, to understand the text.

75
00:09:24,930 --> 00:09:34,319
It's a very well written, a lot of examples, not a lot of hands on examples, but explained things extremely well.

76
00:09:34,320 --> 00:09:45,360
It's designed as a textbook. The only caveat is you can see the publication date is 22 2006, like almost two decades ago.

77
00:09:45,510 --> 00:09:54,899
It's a little bit old, but most of the material still, you know, the the theory developed they then still valid here.

78
00:09:54,900 --> 00:10:03,750
I think it's if you're just looking for some new things like deep learning, you're not going to find in this book.

79
00:10:03,750 --> 00:10:13,649
But actually most of the material we cover in this class you can find in I think this is also is because it's published by Springer.

80
00:10:13,650 --> 00:10:18,410
So you should try this take advantage. You are in the university.

81
00:10:18,410 --> 00:10:23,220
You try to Springer link, see if you can get the electronic copy of the book.

82
00:10:23,920 --> 00:10:30,180
Um, the second book is, is a introduction to statistical learning.

83
00:10:30,180 --> 00:10:33,360
So it's quite popular in statistical community.

84
00:10:33,870 --> 00:10:45,930
So this is a call the, you know, a baby version of the fourth book, the ESL, the elements of Statistical Learning, which is kind of a.

85
00:10:47,190 --> 00:10:52,170
Very thick and then very technical book.

86
00:10:53,040 --> 00:10:56,130
The ISIL is a sort of a baby version.

87
00:10:56,400 --> 00:11:05,100
So the benefit of that book is they have a lot of examples, as have a lot of articles in the book.

88
00:11:05,130 --> 00:11:08,920
So you could try by yourself again.

89
00:11:08,940 --> 00:11:13,710
So from a perspective for you, for you are trained as a statistician by far.

90
00:11:13,740 --> 00:11:17,010
So this is probably the most approachable book to you.

91
00:11:17,220 --> 00:11:22,590
And if you want to practice your programing skills, this is also a plus.

92
00:11:23,430 --> 00:11:28,590
So the the negative of this, the book is statistical, not learning.

93
00:11:28,620 --> 00:11:32,720
Technically speaking, it's not the whole machine learning field.

94
00:11:32,730 --> 00:11:40,020
So they view problems from a statistician or from a more from a probabilistic point of view.

95
00:11:40,380 --> 00:11:48,120
So it doesn't cover the full spectrum of the machine learning, but, you know, it's still helpful.

96
00:11:49,350 --> 00:11:57,720
So this is also. So the third book is purely for reference purpose, the foundation of machine learning.

97
00:11:57,990 --> 00:12:02,000
So this is what I'm talking about. This is this is a computer science book.

98
00:12:02,010 --> 00:12:05,159
So you can go through the whole book.

99
00:12:05,160 --> 00:12:10,830
Probably don't find anything about in-depth discussion on probability.

100
00:12:11,690 --> 00:12:17,310
Okay. So but still works. Why we will. Well, what we'll see throughout the class.

101
00:12:17,760 --> 00:12:25,130
As I said, you don't have to have a code, a statistical point of view to understand the the current machine learning.

102
00:12:25,140 --> 00:12:32,880
So if you want to see how other people, especially the experts in other areas, look at the same type of the problem,

103
00:12:33,330 --> 00:12:40,590
then this is probably a good book, but that does require you to understand their language.

104
00:12:40,590 --> 00:12:44,460
Like how do you analyze the complexity of the algorithm?

105
00:12:44,630 --> 00:12:50,910
Right. So those are the the main point of thinking from a computer science perspective.

106
00:12:51,180 --> 00:13:02,220
We will introduce those type off the concept, but we will now discuss those concepts or thought process in details in this course.

107
00:13:03,410 --> 00:13:09,920
All right. So all of these are for references. So you should mainly rely on the lecture notes.

108
00:13:10,990 --> 00:13:14,120
The prerequisite, as I said, I think so.

109
00:13:14,120 --> 00:13:23,719
Initially, the first offering of this class, we put a lot of prerequisites because we don't we didn't really know what we should put there.

110
00:13:23,720 --> 00:13:31,940
But we expect all work for if you a master student, we expect you probably get most if you have already.

111
00:13:32,930 --> 00:13:37,580
Take the the mathematical statistics sequence and the.

112
00:13:39,480 --> 00:13:45,570
And then the applied sequence in terms of like logistic regression, generalized linear model.

113
00:13:46,680 --> 00:13:56,560
But again, I think once I taught this class a few times, I feel like it's a little bit against the initial purpose, right?

114
00:13:56,580 --> 00:14:05,640
So, you know, you don't want. You want to see how machine learning that sort of a thought process is.

115
00:14:05,650 --> 00:14:13,170
All-region. You don't want people you don't necessarily need a fixed type of the the thinking.

116
00:14:13,170 --> 00:14:16,290
So you have to treat this problem like logistic regression.

117
00:14:16,530 --> 00:14:22,410
You have to treat this problem as a traditional statistical analysis problem.

118
00:14:22,440 --> 00:14:32,550
It's actually not really necessary. So we reduce the basically remove most of the the request.

119
00:14:32,620 --> 00:14:43,070
Now it's only 601, but that's important. The other thing is important is the we didn't list the here because there's also the prerequisite of 601.

120
00:14:43,080 --> 00:14:58,740
So you we are going to talk a lot of optimization algorithm so a little bit of review of your multi variable calculus is somewhat important.

121
00:14:58,830 --> 00:15:09,100
So something like one we get to will probably after the first a few weeks, we're going to talk about a Lagrange multiplier.

122
00:15:09,120 --> 00:15:12,850
So those things are covered in multi variable calculus.

123
00:15:12,870 --> 00:15:17,970
I'm pretty sure if you haven't been using that is going to be a little bit.

124
00:15:18,450 --> 00:15:30,089
So you need a refresher on that sort of thing. Welcome to review with this during the class, but just be aware we will need some optimization,

125
00:15:30,090 --> 00:15:36,360
convex optimization and constraint optimization algorithm.

126
00:15:36,690 --> 00:15:40,860
So if you have trouble.

127
00:15:43,030 --> 00:15:53,410
Understanding those optimization algorithms, I would strongly encourage you to refresh yourself with that material.

128
00:15:53,690 --> 00:15:57,370
Okay, so there are course goals.

129
00:15:57,970 --> 00:16:04,700
Okay. The grading. Students usually care most of all the grading.

130
00:16:05,810 --> 00:16:13,190
So there will be homework. Midterms. The form of the midterms is to be determined.

131
00:16:13,250 --> 00:16:17,870
Last year we didn't have any in-class closed book.

132
00:16:19,990 --> 00:16:26,020
Midterms. We have this sort of machine learning challenge type of the midterm,

133
00:16:26,020 --> 00:16:33,160
so it's like a project, but you do it yourself and then we compete with each other.

134
00:16:33,700 --> 00:16:37,840
Okay. It's kind of fun, but it depends on the the problem.

135
00:16:38,800 --> 00:16:46,060
So there will be two types of midterms, each for 30% of your grade.

136
00:16:46,270 --> 00:16:53,420
And then there are homework. There will be. Typically bi weekly homeworks is also 30%.

137
00:16:54,560 --> 00:17:00,280
The class participation is 10%. So. So wait.

138
00:17:00,290 --> 00:17:10,520
Will we as a department hope you to show up in we are no longer do a livestream zoom.

139
00:17:10,910 --> 00:17:14,930
I think that's my discretion. But I think to maximize your.

140
00:17:17,370 --> 00:17:27,270
You know, the what you're getting from the class, the best way is come here and ask questions and participate in discussion.

141
00:17:28,500 --> 00:17:32,260
I don't want to force you to come to class.

142
00:17:32,280 --> 00:17:40,200
I think everybody gets sick. A certain point, especially there is so many virus around us.

143
00:17:40,530 --> 00:17:45,659
And so if you are sick, that's fine. We have a recording that's automatic recording.

144
00:17:45,660 --> 00:17:49,200
The lecture will be uploaded to the the canvas.

145
00:17:49,210 --> 00:17:57,330
That's fine. But I do hope to see most of you in lecture.

146
00:17:57,420 --> 00:18:02,549
Right. So this is kind of a. You know, I don't know.

147
00:18:02,550 --> 00:18:09,750
How do I was. Well, if there is you know, you always throw up most of class or show up, then you automatically get the 10%.

148
00:18:09,750 --> 00:18:16,950
But if you know, only a few people show up, I have to reward those people constantly show up.

149
00:18:17,280 --> 00:18:21,490
Right. Kind of a carrot and.

150
00:18:22,470 --> 00:18:26,490
Stick thing. Any questions?

151
00:18:27,300 --> 00:18:33,450
We're not saying the you know, the midterm format will be close book war will be project.

152
00:18:33,600 --> 00:18:36,690
No, not yet. It's all depends on.

153
00:18:38,520 --> 00:18:42,060
You know. Well, we'll make that decision and. Okay.

154
00:18:42,270 --> 00:18:47,010
So the the the the to me terms will cover different topics.

155
00:18:47,340 --> 00:18:51,540
So the first me term will cover supervised learning.

156
00:18:51,540 --> 00:18:54,840
The second midterm will cover the topics in the unsupervised learning.

157
00:18:55,380 --> 00:19:00,300
As you can see from the schedule, those two things are very imbalanced.

158
00:19:00,870 --> 00:19:04,979
So actually the first and mid term you can expect that it's actually towards the

159
00:19:04,980 --> 00:19:12,299
end of the march and then we only have like two or three weeks for unsupervised.

160
00:19:12,300 --> 00:19:25,760
The learning is relatively. Less material so so that the to me terms potentially is give you a feeling of back

161
00:19:25,760 --> 00:19:33,370
to back is just I have to say this up front so we won't have a midterm around,

162
00:19:34,550 --> 00:19:42,379
I'd say the winter of the spring break or the winter break, but more like the first midterm it's going to be towards the end of the march.

163
00:19:42,380 --> 00:19:49,730
And then the second one is going to be probably the last week of the semester.

164
00:19:50,330 --> 00:19:56,250
Okay. So roughly speaking, any questions so far?

165
00:19:56,270 --> 00:20:01,570
Yes, please. Okay.

166
00:20:01,690 --> 00:20:10,540
We'll talk about that. Thank you. That's a good question. So the question is, I think I have to repeat with this, is there a required coding language?

167
00:20:11,050 --> 00:20:14,980
The answer is no. You can use whatever you prefer.

168
00:20:15,010 --> 00:20:18,970
Actually, I encourage you to use whatever you feel comfortable with.

169
00:20:19,330 --> 00:20:25,180
But to illustrate some of the examples I'll provide.

170
00:20:27,170 --> 00:20:37,640
You know, example those codes are in our because, you know, most people feel comfortable I have to you know.

171
00:20:39,250 --> 00:20:41,170
Accommodate the majority. So.

172
00:20:41,210 --> 00:20:51,580
So the examples are in using our B given in R, but you can easily translate that into Python or C, C++, anything you like.

173
00:20:51,970 --> 00:20:58,900
There is no restriction that you have to do your homework with or project the with certain programing language.

174
00:20:59,440 --> 00:21:04,150
Right. Good. Good question on the other questions.

175
00:21:06,390 --> 00:21:10,170
Okay. So we have a tall peak list.

176
00:21:10,260 --> 00:21:14,069
So you can see that the whole semester, probably just 14 weeks.

177
00:21:14,070 --> 00:21:22,440
So this is exclude the spring break. So not a lot of actually event in a semester you don't have a lot of.

178
00:21:22,440 --> 00:21:28,340
So it's like 28, 30 lectures if we don't cancel any.

179
00:21:28,440 --> 00:21:39,839
I haven't finalized my travel schedule for that, but I don't expect to cancel more than two lectures because of my schedule.

180
00:21:39,840 --> 00:21:47,969
So usually so those so it's going to be a little bit tied and then the material will be heavily so a little

181
00:21:47,970 --> 00:21:55,830
bit heavy on the supervise the side because there's a more theory or is more algorithm in that perspective.

182
00:21:57,450 --> 00:22:10,710
The unsupervised learning is mostly easier for statistical if you have a statistical background and the unsupervised learning is relatively easier.

183
00:22:11,850 --> 00:22:16,980
But I'm sorry, I should say that unsupervised learning is easier for.

184
00:22:18,790 --> 00:22:27,180
If you have a problematic background or you have a statistical thinking built in supervised learning is a different story.

185
00:22:27,190 --> 00:22:31,319
So we've probably spent more time on that. Okay.

186
00:22:31,320 --> 00:22:37,800
Everybody happy so far? Okay. You know, we have a lot of registered.

187
00:22:38,820 --> 00:22:46,170
You know, I think. But I think I wish you try it out, especially the first two weeks.

188
00:22:46,860 --> 00:22:54,389
Okay. Feel free to drop the class. I don't think you know, I think for us, obviously, is for me.

189
00:22:54,390 --> 00:23:00,060
And then the GSI is is easier, but it's really about your need.

190
00:23:00,750 --> 00:23:06,149
As I said, if you're come from completely different background, if you don't have that, you know,

191
00:23:06,150 --> 00:23:10,740
the the way we talk about the problem like statistics or probabilities,

192
00:23:10,740 --> 00:23:15,870
you may find other offering of the machine learning fits your background better.

193
00:23:16,350 --> 00:23:21,750
Okay. So this is mostly for if you're an undergraduate student or from a different department.

194
00:23:21,900 --> 00:23:24,930
Okay. It's not it's not discourage you.

195
00:23:24,930 --> 00:23:29,190
But I think I acknowledge there is different ways to learn things.

196
00:23:29,460 --> 00:23:36,120
And we are teaching this class in the way trying to, um.

197
00:23:36,720 --> 00:23:40,380
So to speak. Please. The statistical audience.

198
00:23:41,040 --> 00:23:44,850
Okay. Any questions? Good.

199
00:23:45,150 --> 00:23:53,220
So let's get started. I mostly use the border. There is some of the material I need to illustrate.

200
00:23:53,520 --> 00:24:02,090
Then I will put on the. There is there are prepared slides or show on the on the number on the screen.

201
00:24:03,320 --> 00:24:12,600
So. You know, I think when you first when we first approach a topic we should I think is for any topic you want,

202
00:24:12,620 --> 00:24:20,870
especially scientific, you want to look into the history of this particular discipline and develop around history.

203
00:24:21,080 --> 00:24:31,190
So that gave you a kind of understanding of the thought process, how people in different times thinking about the same scientific problem.

204
00:24:31,280 --> 00:24:34,490
Right. So we'll do the same for machine learning.

205
00:24:36,110 --> 00:24:39,590
So the first thing we see here is going to be the history.

206
00:24:43,440 --> 00:24:49,049
Oh. Without talking about what actually is a machine learning.

207
00:24:49,050 --> 00:24:56,040
So the history of machine learning will help us define what really what machine learning really is.

208
00:24:56,160 --> 00:25:00,149
Right. So the birthplace of machine learning.

209
00:25:00,150 --> 00:25:08,550
So those are the history. Why you talk about history then different people have kind of a different again have different viewpoints.

210
00:25:09,390 --> 00:25:18,000
So if you ask a computer scientist, what is the birth time of machine learning at this point, they probably give you a different.

211
00:25:20,580 --> 00:25:24,450
We're sort of now with them. I see what the first time was.

212
00:25:24,980 --> 00:25:28,450
That probably gave you a different answer than the statistics.

213
00:25:28,950 --> 00:25:38,670
But surprisingly, I think for machine learning. Everybody think this the major events in 1763.

214
00:25:39,660 --> 00:25:44,520
So what happened this year is Thomas face.

215
00:25:47,800 --> 00:25:55,540
Publish his essay. It's called An Essay Towards Solving a Problem in the Doctrine of Kansas.

216
00:25:55,970 --> 00:26:00,400
So. So you don't know that what essay of that paper is.

217
00:26:00,940 --> 00:26:05,080
That's basically the base rule. So the. This is the base.

218
00:26:11,440 --> 00:26:15,820
So the first application of the base rule. So surprisingly,

219
00:26:16,570 --> 00:26:20,690
even you are from a computer science background though they think the base

220
00:26:20,710 --> 00:26:25,330
or the purpose of the base through mark the purpose of the machine learning.

221
00:26:25,900 --> 00:26:32,320
So this is actually the the way how people using probability to do reasoning, right.

222
00:26:34,450 --> 00:26:40,659
This is also mark the first place of statistics or probability surprisingly so.

223
00:26:40,660 --> 00:26:51,400
So what I mean, this is basically saying at the first place where all converge and at the same line or we can all trace back to Thomas Base.

224
00:26:51,670 --> 00:26:54,810
Thomas base itself is a this Reverend Thomas space.

225
00:26:54,820 --> 00:27:03,219
So he's a priest. So his interest is mostly dealing with how to reasoning using a mathematical way.

226
00:27:03,220 --> 00:27:10,930
So he is mostly was inspired by Newton's way to use mathematics to derive to

227
00:27:10,930 --> 00:27:16,270
summarize that the common rules physics common rules his particular interests.

228
00:27:16,270 --> 00:27:19,600
These are trying to show I mean the theorem.

229
00:27:19,840 --> 00:27:26,860
Well here the hypothesis he has is God exists. So you have so many phenomenon, so conditional on those phenomena.

230
00:27:26,860 --> 00:27:30,219
How do you prove the God exists?

231
00:27:30,220 --> 00:27:38,560
So the final product is this a little book about time you don't have peer review

232
00:27:38,920 --> 00:27:45,190
you just so they are the so the book is written by so what I said this book is

233
00:27:45,190 --> 00:27:50,559
called I See an Essay towards solving the problem in the doctrine of Chances is

234
00:27:50,560 --> 00:27:55,780
it's written by the base and then another gentleman called the peers believe.

235
00:27:56,920 --> 00:28:01,450
So this is right in the book and then read it in the Royal Society.

236
00:28:01,450 --> 00:28:09,129
The mark the actually the the first the version of the base rule is a pretty rough version.

237
00:28:09,130 --> 00:28:19,350
So it's not really mathematical and it's not the the the formula you see nowadays we call base rule this is until.

238
00:28:20,780 --> 00:28:27,290
So I should draw the line. There's an 1812.

239
00:28:28,870 --> 00:28:35,740
So pure. Simon La Plus. So this is an interesting figure in science.

240
00:28:42,280 --> 00:28:54,650
So it's unknown. The Laplace independently derived the base, or he actually write the base book, but he never claimed that he's the first one.

241
00:28:54,670 --> 00:28:59,440
He was the first one to discover the base rule, but he formalize the base rule.

242
00:28:59,470 --> 00:29:07,930
It's it's more like what we have seen today, especially the base rule in base version.

243
00:29:08,860 --> 00:29:12,530
Um. Assume you have a default uniform prior.

244
00:29:12,550 --> 00:29:22,000
So if you don't know what that means, that's fine. But the closest version gave you some sort of flexibility to specify different types of the prior.

245
00:29:22,270 --> 00:29:25,570
So the prior is part of the base.

246
00:29:26,020 --> 00:29:30,910
The base theory. Now. And well, he's.

247
00:29:31,360 --> 00:29:36,050
His contribution is not just the base, is he?

248
00:29:36,220 --> 00:29:39,820
I wrote a book. I'm not going to butcher the French.

249
00:29:39,830 --> 00:29:45,760
I don't really know French. So the English translation is analytic theory of probability.

250
00:29:46,090 --> 00:29:49,690
So he, the Laplace, was interested in.

251
00:29:49,750 --> 00:29:56,560
So this is a more scientific driven. So he's more close to the profession, like in Newton.

252
00:29:57,010 --> 00:30:03,160
Right. At that time, he's very interested in as normal astronomy.

253
00:30:05,080 --> 00:30:09,850
So there is a lot of collection of data during the period of time.

254
00:30:09,850 --> 00:30:17,770
So people see, observe the nightly sky every day and then collect data.

255
00:30:17,770 --> 00:30:24,130
So you only heard about, you know, the prediction of different comet at around that time.

256
00:30:24,580 --> 00:30:27,320
So so there is a need of a data analysis.

257
00:30:27,340 --> 00:30:41,049
So La Plaza also feel that the data collected by Naked Eye is a scientific observation, but also there is a lot of noise in the data.

258
00:30:41,050 --> 00:30:46,180
So this is a kind of uncertainty and a noisy data type of the problem.

259
00:30:46,600 --> 00:30:48,159
So LA Plus is the first.

260
00:30:48,160 --> 00:30:58,390
The solution is actually the same as using base rule, um, by using the evidence to modify your initial belief, that sort of thing.

261
00:30:58,660 --> 00:31:03,940
But later on, the classes are really the founder of the modern statistics.

262
00:31:04,480 --> 00:31:07,600
He also discover the characteristic functions.

263
00:31:08,230 --> 00:31:14,680
He also discovers the probably the holy grail of the modern statistics central limit theorem.

264
00:31:16,270 --> 00:31:22,089
The interesting, as I said, is an interesting figure, at least in statistics, also in machine learning,

265
00:31:22,090 --> 00:31:31,329
is you actually drift away from Bayesian statistics, rely more on the central limit theorem.

266
00:31:31,330 --> 00:31:42,280
Nowadays we call the frequentist statistics. But nevertheless, this is a mark that the the data analysis at that time if you want to.

267
00:31:43,820 --> 00:31:47,959
It's basically defined by the problem they have in hind.

268
00:31:47,960 --> 00:31:51,050
Right. So they want to find the rules of all these.

269
00:31:53,530 --> 00:31:57,969
Planets and then the stars. So they collect the data.

270
00:31:57,970 --> 00:32:04,150
They want to find the the orbit, the the hidden stars, the hidden planet.

271
00:32:04,690 --> 00:32:12,060
So so those that data analysis, those type of the scientific problem drives at that time.

272
00:32:12,070 --> 00:32:22,149
So the machine learning more machine learning more statistics are more we mostly focus on using the tools we have today is either the base rule,

273
00:32:22,150 --> 00:32:30,550
the Bayesian statistic or the frequentist rule, or more specifically, the central limit theorem, right.

274
00:32:31,270 --> 00:32:41,500
So, you know, there is a lot of incremental change or while it's probably easy for me to say the incremental changes, but.

275
00:32:43,500 --> 00:32:51,000
Every changes here during. 1812 to 1950, you are important.

276
00:32:51,240 --> 00:32:57,240
So during this period of time. So this is a kind of of more than a century.

277
00:32:57,930 --> 00:33:01,620
There are a lot of things happen, but it's not that sort of a landmark thing.

278
00:33:02,080 --> 00:33:16,050
And specifically, you know, mathematicians formally define what probability is because those are the kolmogorov work that's in the early 20th century.

279
00:33:17,370 --> 00:33:20,999
They're also the development of the modern statistics.

280
00:33:21,000 --> 00:33:27,060
Right. The design of the experiment. Those those of the things that you are learning by Fisher.

281
00:33:28,740 --> 00:33:39,600
There are also other things. But in terms of computer science, that we don't consider the earth computer science in this before 1950,

282
00:33:39,840 --> 00:33:45,750
1950 is really the first place of computer science, at least the theoretical computer science.

283
00:33:46,080 --> 00:33:49,320
Well, marks the birthplace of computer science or.

284
00:33:50,750 --> 00:33:57,260
The other people say that the modern machine learning is 1950.

285
00:33:58,460 --> 00:34:03,650
Curious Essay on computing machinery and intelligence.

286
00:34:04,990 --> 00:34:25,180
Maybe I'll write this. So you are interested in this?

287
00:34:25,200 --> 00:34:28,350
There is a well, I mean, there is a movie.

288
00:34:29,130 --> 00:34:31,590
You don't need to read the the full book.

289
00:34:31,600 --> 00:34:41,370
The movie is obviously not the complete history, but it's still kind of attached to the emotional history of it's called The Imitation Game, I think.

290
00:34:41,850 --> 00:34:49,079
And pretty good movie tells you a little bit of the journey of touring itself or himself.

291
00:34:49,080 --> 00:34:57,810
It's obviously is it's about his personal life, but there's also his scientific development during his career.

292
00:34:58,080 --> 00:35:06,230
So Turing is given what's given the task of crack, the enigma of the German secret code during the World War Two.

293
00:35:07,050 --> 00:35:12,210
So those are the code used by German submarines.

294
00:35:13,350 --> 00:35:24,510
And then we could just use this tactic to disrupt the transportation between Atlantic transportation from us to to England.

295
00:35:24,840 --> 00:35:34,350
So that's kind of an important thing. If you crack the code, you know, where the submarine is, you can avoid that or try to eliminate the threat.

296
00:35:34,860 --> 00:35:46,170
So he was given the task and then he realized that computing is a big issue because if a thinking about cracking a code, it's in mathematics.

297
00:35:46,620 --> 00:35:51,230
The language is just, you know, combinatorics.

298
00:35:51,720 --> 00:35:57,090
Right. So you're trying to find different combinations, how these combinations would actually work.

299
00:35:57,090 --> 00:36:05,040
You find the right codes, you find the you you decode the the enigma in the right way.

300
00:36:05,400 --> 00:36:12,059
So they're the they find the necessary of developing this computing machinery.

301
00:36:12,060 --> 00:36:15,840
So those things are the modern day computers.

302
00:36:16,200 --> 00:36:21,929
Right. So what is a computer? So the before Turing, there is no formal definition.

303
00:36:21,930 --> 00:36:32,130
What computer is or what is a computing language is Turing is the first one to define that in a mathematical way.

304
00:36:32,490 --> 00:36:41,100
So for example, the computer, what we have nowadays, oh have this architecture, they have memories, right?

305
00:36:41,370 --> 00:36:46,470
They precise the the precise sphere, the language like a string.

306
00:36:46,720 --> 00:36:50,250
Right. So this is kind of a simplified version,

307
00:36:50,250 --> 00:36:59,070
but you need a universal version of the computer so you have limited memories and then you can process the information and then you can do logic,

308
00:36:59,820 --> 00:37:11,520
reasoning around information. So about this paper, though, so, you know, you need to use the machine to speed up the computing to crack the enigma.

309
00:37:11,520 --> 00:37:21,630
That's why Turing is the person have the opportunity to develop the, you know, realize the importance of computing.

310
00:37:21,960 --> 00:37:29,160
So the computing machinery he was referring to is it's more like what we what I just describe you have a memory,

311
00:37:29,160 --> 00:37:38,130
you have a processing unit and then you have, you know, a string of so called a different type of language, computable language.

312
00:37:38,430 --> 00:37:44,490
And then you can observe and then intelligence is refers to human intelligence.

313
00:37:44,940 --> 00:37:51,330
And then what comes all the from this essay is the question of can you tell?

314
00:37:53,020 --> 00:37:59,920
The difference between the machine and the human rights is that this is a famously known as the Turing test.

315
00:38:01,390 --> 00:38:06,070
Why this problem is important? Because they gave you an understanding so.

316
00:38:06,340 --> 00:38:15,129
Well, this sounds philosophical. It's not because technically you want to know if machine some of the task a human can do you

317
00:38:15,130 --> 00:38:21,640
think human intelligence can achieve like play the board games can they achieve the by computer?

318
00:38:21,820 --> 00:38:27,160
Nowadays we know the answer is affirmative. Right. So they they they can and then they can beat human.

319
00:38:27,470 --> 00:38:33,129
By that time, it's all clear, especially you have a machine like a simple machine.

320
00:38:33,130 --> 00:38:40,030
It's not very powerful, but that's huge. Like this room every day is just a lot of spins of the.

321
00:38:42,590 --> 00:38:48,200
The wheels of and it's not a very powerful machine about time and then they want to understand if.

322
00:38:49,230 --> 00:38:54,480
These computing machinery and intelligence can also see a radical problem.

323
00:38:55,440 --> 00:39:02,160
Can the machine computing machinery and the intelligence being indistinguishable?

324
00:39:03,500 --> 00:39:13,980
So the Turing's answer is affirmative. He believe he can achieve to the same, but they are still like thoughts in his conclusion.

325
00:39:14,520 --> 00:39:17,670
But I should say that was the last thoughts nowadays, right?

326
00:39:19,590 --> 00:39:23,190
It's it's not really asking the question.

327
00:39:23,190 --> 00:39:28,499
Can a machine think? Because that is hard to define as a scientific question.

328
00:39:28,500 --> 00:39:30,120
What is thinking? Right.

329
00:39:30,150 --> 00:39:43,500
So that's more philosophical, but rather trying to say can using this basic first off equipment with memory, with with limited memory,

330
00:39:43,500 --> 00:39:54,900
with structure in a language, and then with a logical processing unit, can the machine act indistinguishable as a rational human right?

331
00:39:54,910 --> 00:40:03,870
So if it's just the intelligence, it's not about how machines think, it's about their behavior is in this distinguishable.

332
00:40:03,870 --> 00:40:09,239
But I think it's a fascinating topic and probably not really related to this class.

333
00:40:09,240 --> 00:40:13,040
But if you're interested, it should. Well, either watch the movie.

334
00:40:13,050 --> 00:40:17,700
I don't know how much of the movie, actually, I don't remember how much the movie actually talk about this.

335
00:40:17,700 --> 00:40:21,960
But, you know, think about this type of the problem. So, you know, the Turing test, right?

336
00:40:21,990 --> 00:40:26,460
So if you can tell, you know, nowadays it's difficult.

337
00:40:27,180 --> 00:40:30,540
You tax to a service. So they respond you immediately.

338
00:40:30,540 --> 00:40:34,200
You probably think this is a machine. There is a just a robot.

339
00:40:34,710 --> 00:40:41,850
Answer my question. And then most of the time that you cannot tell is a human or a robot behind.

340
00:40:41,880 --> 00:40:45,630
So that's based on the Turing test, the setting, I should say.

341
00:40:46,530 --> 00:40:53,579
Anyway, so this mark the verse of modern computer science and modern.

342
00:40:53,580 --> 00:40:59,160
Well, you want to say that artificial intelligence, the first of these are based on.

343
00:41:00,600 --> 00:41:07,740
I should also say so. Turing's work is actually built upon those previous works.

344
00:41:08,130 --> 00:41:19,890
There is a lot of application of base rule in Turing's computing machinery, so these are part of his computing logic.

345
00:41:21,650 --> 00:41:29,600
All right. So those are the landmark events. The rest of those after 1950s, the development.

346
00:41:30,870 --> 00:41:35,570
Just mark the important inventions of algorithms.

347
00:41:38,940 --> 00:41:49,400
Machine learning and artificial intelligence. At this point, it's probably, Mark, there's the difference between computer science and statistics.

348
00:41:49,410 --> 00:41:54,569
Okay. They all have the same ancestor, which is the base rule 1950.

349
00:41:54,570 --> 00:41:59,580
They become siblings. 1952.

350
00:41:59,970 --> 00:42:10,850
So this is the author. So he saw he was a I.B.M. computer scientist.

351
00:42:12,200 --> 00:42:22,780
Samuel. So what his contribution to one is, he's the first to develop a computer system can play a human game.

352
00:42:22,790 --> 00:42:26,720
So the first game the computer can play is actually checker.

353
00:42:28,070 --> 00:42:36,750
And then and the other significance about the author, Samuel, is he actually coined the name of machine learning, right?

354
00:42:36,850 --> 00:42:43,190
So the machine learning the strategy, right at this point,

355
00:42:43,190 --> 00:42:54,110
this three things so that so A.I. and then machine learning basically consider in the same camp but already branch the from the statistics.

356
00:42:54,530 --> 00:43:10,089
Okay. So 1950 to 1967, I mentioned those because all these things we can learn, some of these inventions,

357
00:43:10,090 --> 00:43:18,790
some of the algorithms are going to be important for today's machine learning or are we going to learn that during the class?

358
00:43:19,240 --> 00:43:23,900
So, friends. Rosenblatt.

359
00:43:30,480 --> 00:43:38,610
So with the so the Turing's that there is some sort of a you know,

360
00:43:38,820 --> 00:43:44,340
the the indistinguishable between the human behavior and then the the machine behavior.

361
00:43:44,350 --> 00:43:55,860
So that actually motivated computer science to actually program or design algorithms like computer algorithms, just like human thinking.

362
00:43:56,190 --> 00:44:01,890
So what Frank Rosenblatt there is a he was in Cornell at that B so he.

363
00:44:03,800 --> 00:44:14,770
Versus the wild concept of perceptron. Um, so you're very familiar with the modern artificial intelligence, especially neural networks.

364
00:44:15,460 --> 00:44:21,110
The perceptron is the. Sorry.

365
00:44:23,280 --> 00:44:29,600
They're going to tell me if the. If they're reporting this as a problem.

366
00:44:29,720 --> 00:44:33,710
Sorry, it's not. Okay. So the perceptron is essentially the.

367
00:44:36,090 --> 00:44:42,360
The dual wrong or the very basic process for neural networks.

368
00:44:42,630 --> 00:44:52,050
Right. So this there is kind of a similar simulating human intelligence in in this kind of a computing architecture.

369
00:44:52,350 --> 00:45:00,780
So that's Mark. This is also the first place probably you want to think about that sort of the first place of neural network.

370
00:45:01,850 --> 00:45:06,960
Okay. So we're going to get there.

371
00:45:08,460 --> 00:45:12,330
So 1963 again.

372
00:45:12,330 --> 00:45:18,840
So there's just a there's this a more like I type of the the development rather than the machine learning.

373
00:45:20,640 --> 00:45:23,850
So the machine play. Exactly.

374
00:45:30,390 --> 00:45:41,760
So the significance of war machine play tic tac toe is not a really extraordinary thing, especially you think of the toys come for easy game nowadays.

375
00:45:42,210 --> 00:45:47,610
But this is a mark to the formalization of a type of the learning.

376
00:45:47,760 --> 00:45:52,020
Nowadays they call the reinforcement learning. We'll talk about what that means.

377
00:45:52,350 --> 00:46:03,540
So this is the. So that sort of idea is formally introduced into artificial intelligence and machine learning.

378
00:46:06,700 --> 00:46:13,690
By the way, we're not going to talk a lot. Well, actually, reinforcement learning at all in this class.

379
00:46:14,140 --> 00:46:17,440
I'll give you a definition later today. So we could.

380
00:46:18,280 --> 00:46:25,870
So you for an. Hopefully there I think there are classes in the around campus.

381
00:46:25,870 --> 00:46:29,180
There are just part of that reinforcement. Okay.

382
00:46:30,450 --> 00:46:33,820
So though reinforcement learning is more focused on, you know,

383
00:46:33,870 --> 00:46:39,900
human machine interaction and how you deal with a series of decision making, we'll get there.

384
00:46:41,490 --> 00:46:46,990
But that's a. Sort of it deserved landmark event.

385
00:46:48,860 --> 00:46:55,270
Um, so while I probably not write a lot, but 1967.

386
00:46:57,580 --> 00:47:06,640
Some of the algorithm and the important algorithm we still use and all the users in its development, let's call it nearest the neighbor.

387
00:47:14,190 --> 00:47:19,700
Around. So very simple.

388
00:47:19,700 --> 00:47:24,710
So if you are ever done anything. Bioinformatics or.

389
00:47:26,620 --> 00:47:35,830
In doing something, I don't know. I mean, it's it's a very cool algorithm and that can be like you don't really need to know a lot of.

390
00:47:37,720 --> 00:47:46,130
Mathematics to understand. This particular algorithm is so useful, is still used today.

391
00:47:49,170 --> 00:47:57,180
But actually everything I wrote here, you're probably not going to learn from a statistical textbook, right?

392
00:47:57,200 --> 00:48:02,660
So that's marked the divergence of, you know, the traditional statistics and then the machine learning.

393
00:48:02,670 --> 00:48:09,330
That's why we take this class. I think we want to system understand this side of the development.

394
00:48:11,500 --> 00:48:16,139
Okay. So I'm going to mark the events in that case.

395
00:48:16,140 --> 00:48:22,730
Otherwise, we're not going to finish this. There's a history lesson of history class.

396
00:48:24,060 --> 00:48:29,400
So 1978 to 1980.

397
00:48:31,290 --> 00:48:42,150
So the important thing happened in this decade is there is a separation between artificial intelligence and the machine learning.

398
00:48:42,600 --> 00:48:49,110
They will come back together. We'll see. But for now, they're kind of a two different branches of computer science.

399
00:48:49,110 --> 00:48:54,990
And all artificial intelligence becomes more into this expert system,

400
00:48:55,410 --> 00:49:03,930
actually focus more or have been focused on the reinforcement learning part of the world.

401
00:49:04,320 --> 00:49:08,340
So what they are trying to do is build this expert system.

402
00:49:09,040 --> 00:49:11,910
So we're always trying to build this kind of a machine.

403
00:49:12,840 --> 00:49:22,920
For example, a digital doctor, I say you tell him you're sick and he's going to make the machine, going to make a diagnosis.

404
00:49:23,100 --> 00:49:30,300
So that sort of thing. So they're going to ask you. So you see this kind of a tree type of architecture.

405
00:49:30,440 --> 00:49:34,730
So there's a lot of a lot of group. Those are people with a robot.

406
00:49:35,130 --> 00:49:41,520
So you tell the basic information, you've got lunch and then you tell the other information that you've got.

407
00:49:43,840 --> 00:49:51,670
Machine learning, though. Um. There is some more refined type of a problem.

408
00:49:51,820 --> 00:49:59,540
So those are. Actually, you can see, I mean, just roughly reinforcement learning.

409
00:49:59,550 --> 00:50:02,850
So this is a kind of human machine interaction.

410
00:50:08,450 --> 00:50:16,510
And then the other branch, as we call them, machine learning is nowadays machine learning is no interaction without involve.

411
00:50:16,520 --> 00:50:19,820
A lot of human machine interaction not play with the machine.

412
00:50:20,120 --> 00:50:26,220
So those are supervised. An unsupervised.

413
00:50:31,900 --> 00:50:44,740
Lines. Again, we cannot discuss what we're going to focus on this part of the branch in this class.

414
00:50:48,820 --> 00:50:51,870
Um. So 1990.

415
00:50:57,200 --> 00:51:07,220
In terms of artificial intelligence. So now we kind of talk about different type of algorithm in terms of the reinforcement learning.

416
00:51:07,940 --> 00:51:10,280
Thinking about the human machine interaction.

417
00:51:10,910 --> 00:51:19,780
Do you and any of you know, the probably the most significant event in human machine interaction during the 1990s?

418
00:51:21,440 --> 00:51:24,870
Most of you are not born. Okay.

419
00:51:25,350 --> 00:51:28,420
All right. So this is the first time.

420
00:51:28,440 --> 00:51:34,380
1997, 96. And the the IBM computer machine called The Deep Blue.

421
00:51:35,350 --> 00:51:40,180
They won the they beat the human monster in chess.

422
00:51:41,020 --> 00:51:44,440
Right? Well, I mean, that's a historical. I still remember.

423
00:51:45,190 --> 00:51:49,180
Tell me how old I am. It's exciting.

424
00:51:49,870 --> 00:51:56,210
So most of us don't believe human machine can be. Got can be human, but.

425
00:51:57,320 --> 00:52:01,270
They did. So this is kind of a historical event.

426
00:52:01,700 --> 00:52:07,810
So for supervised, unsupervised learning, so there is kind of a the new algorithms.

427
00:52:08,880 --> 00:52:12,270
Haben. You were in that area. What are we going to learn?

428
00:52:13,290 --> 00:52:17,580
Two things. Superb support factor machine. We'll talk about that.

429
00:52:19,550 --> 00:52:25,010
Okay. So this was pure computer assignment and then the boosting algorithm.

430
00:52:26,310 --> 00:52:31,980
So those two things are mostly development during the 1990s.

431
00:52:34,640 --> 00:52:39,140
Towards the 2000. Oh.

432
00:52:39,980 --> 00:52:49,580
The other thing is there is kind of a convergence between these two types of so the artificial intelligence and the unsupervised,

433
00:52:49,580 --> 00:52:55,400
unsupervised learning. So we officially call this reinforcement learning during the 1990s.

434
00:52:55,590 --> 00:53:00,559
Right. So the machine learning community again got invested in this area.

435
00:53:00,560 --> 00:53:06,410
So this is kind of a joint effort and not necessarily saying the discipline becomes the same.

436
00:53:06,410 --> 00:53:11,000
But you're still different learning scenarios as we're going to discuss later.

437
00:53:11,000 --> 00:53:14,020
But there is a common cause.

438
00:53:14,030 --> 00:53:19,189
Well, there is a consensus. There's this stream should be considered across the street.

439
00:53:19,190 --> 00:53:22,610
Different learning scenarios should be considered common scenarios in.

440
00:53:24,410 --> 00:53:28,970
Those both of these, although we are focused on two of those.

441
00:53:31,070 --> 00:53:34,760
Right. So 90. So off to the 2000.

442
00:53:37,250 --> 00:53:45,860
There is a kind of a reemergence of statistical community in back into machine learning.

443
00:53:47,150 --> 00:53:51,500
So those are, you know, the the algorithm like loss.

444
00:53:51,500 --> 00:54:02,750
So the regularization. So the problem there is motivated so that a lot of these solutions are motivated by the big data type of the problem,

445
00:54:03,110 --> 00:54:07,280
especially in health sciences, biomedical sciences.

446
00:54:07,520 --> 00:54:11,390
Now we can measure things in high dimension. Right.

447
00:54:11,560 --> 00:54:17,980
So so the high dimension we're going to talk about the curse of dimensionality is

448
00:54:17,980 --> 00:54:23,770
a statistic and realize there is a issue about traditional statistical methods,

449
00:54:24,640 --> 00:54:31,270
especially there's the philosophical aspect, you know,

450
00:54:31,270 --> 00:54:40,870
how do you find the unbiased estimate or that's no longer working or no longer valid in that type of the study?

451
00:54:40,900 --> 00:54:49,690
And then they start looking into the machine learning community, thinking about the predictive perspective of your estimate,

452
00:54:49,690 --> 00:54:55,030
and then so their re-introduce of statistics back into the machine learning.

453
00:54:55,030 --> 00:55:03,790
So there's a lot of a regularization in for high dimensional data actually evolved between the 1990 and 2000.

454
00:55:05,050 --> 00:55:12,510
So that's kind of a connection we have here. So the in the course, during the course, we're going to discuss this type of the ideas, right?

455
00:55:12,880 --> 00:55:17,920
So how machine learning type of the idea help us to solve high dimensional problems?

456
00:55:21,330 --> 00:55:25,280
So the major events during the 2200.

457
00:55:26,360 --> 00:55:29,420
We should say the meeting. 2000.

458
00:55:29,600 --> 00:55:36,140
Between 2000 to 2000, Tanner mark the two well known machine learning challenges.

459
00:55:36,800 --> 00:55:41,360
The first one I don't know if you know and everybody knows Netflix.

460
00:55:41,670 --> 00:55:46,310
Hannibal do you know and do you know what Netflix challenges?

461
00:55:47,620 --> 00:55:55,089
Anyone? Yes. Good. Because you probably this is a before and probably a little bit before, before your first time.

462
00:55:55,090 --> 00:55:56,890
Words just are wrong. Your first time.

463
00:55:57,160 --> 00:56:10,600
We used to rent DVDs and you know, there's a physical copy of DVDs where you use the the VCR tapes and then the you know,

464
00:56:11,350 --> 00:56:22,030
so the Netflix challenge is Netflix at that time is a new company is trying to challenge this traditional rental, um, video rental business.

465
00:56:22,330 --> 00:56:25,959
So they want to so there are kind of a new strategy.

466
00:56:25,960 --> 00:56:36,630
They want to predict a particular customer's preference on certain genre of videos, movies, so they can recommend.

467
00:56:36,670 --> 00:56:45,459
So this is the first place of the like recommending system, if you want to say so they gave out this one meal in challenge at that time.

468
00:56:45,460 --> 00:56:50,710
One Well, I mean, still 1 million. It's a lot of money, especially for a research community.

469
00:56:51,070 --> 00:56:56,590
So they they gave this problem. So say I have this historical data.

470
00:56:56,980 --> 00:57:07,330
Can you predict can you do the algorithm accurately predict this particular customer's preference for certain movie,

471
00:57:07,330 --> 00:57:12,670
certain pulpy custom movie, so so we can recommend it more effectively, right?

472
00:57:13,060 --> 00:57:17,620
Because they're doing a business, they'd automatically send you through email.

473
00:57:17,620 --> 00:57:25,240
Oh sorry, not email through the traditional USB mail, the hard copies of the, the, the DVDs.

474
00:57:25,570 --> 00:57:36,379
So they, if you can have accurately predict what the customer like that they're not likely to return these and then you don't waste

475
00:57:36,380 --> 00:57:44,740
that those the resources in transition right so you can charge them if they enjoy the the movie so that's the benefit.

476
00:57:45,010 --> 00:57:50,139
So Netflix challenge is a big thing because well I think just the sheer amount of

477
00:57:50,140 --> 00:57:56,830
the the reward toward the end of the there are two teams winning this challenge.

478
00:57:57,280 --> 00:58:04,510
So actually what important is they all use this so called and symbol algorithm.

479
00:58:05,470 --> 00:58:08,980
Right. So we're going to again, we're going to learn that in some BO algorithm.

480
00:58:08,980 --> 00:58:16,330
So the make and symbol type of the thinking, very popular in machine learning in terms of prediction on classification.

481
00:58:17,560 --> 00:58:23,170
The second challenge. Is the image that challenge hosted by Google.

482
00:58:23,290 --> 00:58:28,639
So those are. Pattern recognition type of the problem.

483
00:58:28,640 --> 00:58:34,510
So you are. So every year from 2009, they hosted that.

484
00:58:34,520 --> 00:58:43,249
They gave out a set of images and then using light the machine you can as a team,

485
00:58:43,250 --> 00:58:49,969
you can view the algorithm to recognize certain objects are from the image is a dog is a human,

486
00:58:49,970 --> 00:58:54,470
is a cat, is a car, something like simple those things.

487
00:58:55,130 --> 00:59:05,960
So what's significance about this particular challenge is they gave in 2012 or 2013.

488
00:59:06,320 --> 00:59:12,740
So the deep learning algorithm really distinguish itself from the traditional machine learning algorithm.

489
00:59:13,220 --> 00:59:17,150
So it get to the point. So those are academic challenges.

490
00:59:17,150 --> 00:59:21,979
There are no reward, but you know, it's just you winning the challenge,

491
00:59:21,980 --> 00:59:27,380
you feel good about it, but you know that the image that challenges is no longer exist.

492
00:59:27,740 --> 00:59:30,830
Why? Because up to 2015.

493
00:59:31,400 --> 00:59:37,070
So the machines, the algorithms, performance is as good as human.

494
00:59:37,070 --> 00:59:42,440
So we cannot really judge if she made a mistake, if if.

495
00:59:42,830 --> 00:59:49,790
Well, it's possible that, you know, this is a really get to the the point of Turing's point.

496
00:59:49,790 --> 00:59:57,740
You cannot tell if a machine is doing the the image recognition or a human doing because of the error rate is similar.

497
00:59:58,830 --> 01:00:05,070
All right. But to get to the the end point of this history reveal 2010 two wars.

498
01:00:05,100 --> 01:00:12,739
Nowadays it's kind of a. So the most popular thing I see you have known is the deep learning,

499
01:00:12,740 --> 01:00:22,040
especially the the thinking of you neural network is actually get into all of the different type of the machine learning, right?

500
01:00:22,040 --> 01:00:30,770
So the reinforcement learning take advantage of the, the deep neural network they're supervised or the unsupervised learning algorithm takes.

501
01:00:31,100 --> 01:00:39,170
So the example of the reinforcement learning type, this deep neural network architecture, everybody knows the AlphaGo.

502
01:00:39,510 --> 01:00:44,420
So the go is the most complex scheme nowadays.

503
01:00:44,600 --> 01:00:49,940
Um, it's fair to say I think the most complex human game ever invented.

504
01:00:50,240 --> 01:00:55,219
So we really, you know, even your professional go player, you don't expect that,

505
01:00:55,220 --> 01:01:02,650
you know, the machine can be human because there are so many different, but.

506
01:01:04,140 --> 01:01:08,860
Actually did. It didn't happen in 20. 16.

507
01:01:09,040 --> 01:01:12,070
The AlphaGo, they beat the gold champion.

508
01:01:14,410 --> 01:01:19,959
So that's what we are today. And all these landmark are trying to say.

509
01:01:19,960 --> 01:01:27,250
Here is the things we cannot discuss during the during the class.

510
01:01:27,250 --> 01:01:32,829
I give you a sort of a history, a quick history view of different things.

511
01:01:32,830 --> 01:01:39,870
I probably. Amidst some of the important events.

512
01:01:40,260 --> 01:01:54,030
But the point is, since 1950, if your education is in statistics, you probably feel unfamiliar with all of the things we have discussed.

513
01:01:54,390 --> 01:02:07,380
But somehow they are also connected because some of these are really similar problems that we are trying to solve in the in the statistical context.

514
01:02:08,630 --> 01:02:18,200
So where is the connection? That's the question next. So to answer that question, let's first give a sort of informal definition of machine learning.

515
01:02:19,740 --> 01:02:30,180
Okay. Any questions? What? The history. So what is most alarming?

516
01:02:34,410 --> 01:02:38,510
It's not going to be. Consensus.

517
01:02:40,470 --> 01:02:47,440
Defamation. But. So those are a security task.

518
01:02:49,140 --> 01:02:54,490
Defined. Definition. So it is a scientific study.

519
01:02:54,670 --> 01:03:00,530
So first of all, it's a scientific study. That means we have to use scientific approach, right?

520
01:03:00,580 --> 01:03:08,559
We're are going to use logical reasoning. We're going to use, you know, repeated experiments to verify the results and so on and so forth.

521
01:03:08,560 --> 01:03:14,120
So that's why it's. The scientific. A study.

522
01:03:16,610 --> 01:03:28,530
Of algorithms. It's very important that the objects where are going to learn is going to be algorithms.

523
01:03:29,440 --> 01:03:33,480
Not multiple algorithms. What's the difference?

524
01:03:33,490 --> 01:03:39,460
Hopefully. I don't know if I have time today to explain, but that will be a topic we're going to discuss.

525
01:03:39,910 --> 01:03:45,760
What's the difference between a statistical model versus what is a machine learning algorithm?

526
01:03:45,760 --> 01:03:49,400
Different things. So this is a scientific of algorithms.

527
01:03:49,420 --> 01:04:01,210
So the algorithm. Well, in the in the intuitive sense is a set of procedure, a taker input and then gives the output.

528
01:04:01,930 --> 01:04:06,910
All right. Simple as that. I think this is almost a generic description of algorithm.

529
01:04:07,600 --> 01:04:11,559
You can think of all the black box, but you could or white box.

530
01:04:11,560 --> 01:04:15,040
It doesn't matter. They all share this property.

531
01:04:15,340 --> 01:04:18,400
If you gave the same input, usually, I think also.

532
01:04:21,130 --> 01:04:27,730
Barring some randomness in the algorithm itself, it should give you the same output.

533
01:04:29,230 --> 01:04:35,230
So this is machine learning is a study of scientific algorithm to solve.

534
01:04:38,030 --> 01:04:50,130
Following problem. Through.

535
01:04:50,220 --> 01:04:56,830
So this is the really the key words and then the connection between the statistics and then, um.

536
01:04:59,100 --> 01:05:02,680
As a machine learning. So through what I saw.

537
01:05:04,950 --> 01:05:10,650
Data analysis. All right, so this is our sweet spot.

538
01:05:11,250 --> 01:05:15,960
We meaning us. The distinction. War mathematician.

539
01:05:16,440 --> 01:05:25,050
So the algorithm is designed to solve scientific problems through data analysis.

540
01:05:25,100 --> 01:05:32,640
So you are collecting data for that. So one of the things you have to acknowledge is no perfect data.

541
01:05:33,300 --> 01:05:36,830
The data is always noisy. Right. So there may be.

542
01:05:37,050 --> 01:05:40,110
So if you have an algorithm that solved the perfect.

543
01:05:42,100 --> 01:05:45,680
You already know the relationship.

544
01:05:45,710 --> 01:05:51,400
What? What is that important? I know what it's output. You probably don't, you know, don't need to study that.

545
01:05:51,880 --> 01:06:00,990
But in a lot of scientific contacts, we collect the data we trying to use algorithm to solve sue.

546
01:06:02,300 --> 01:06:08,200
So we're analyzing those data by acknowledging that data is imperfect.

547
01:06:08,230 --> 01:06:13,510
There is noise in that. There is a imperfect relationship between the input and output.

548
01:06:13,900 --> 01:06:17,110
So what kind of about the problems we're going to deal with?

549
01:06:17,800 --> 01:06:23,660
Prediction. All right.

550
01:06:23,670 --> 01:06:27,200
So this is. It doesn't. Explain.

551
01:06:27,230 --> 01:06:31,660
We're trying to predict that the outcome, for example, in the House.

552
01:06:32,360 --> 01:06:37,090
If you do. Certain exams.

553
01:06:38,560 --> 01:06:41,740
Examinations. You want to predict what?

554
01:06:42,000 --> 01:06:45,550
The patient's disease status prediction.

555
01:06:48,210 --> 01:07:00,300
Classification. Classification is not really different than pre picture classification and then prediction.

556
01:07:00,540 --> 01:07:04,589
So the way you want to think about the difference is a prediction.

557
01:07:04,590 --> 01:07:10,370
Usually you want to the output is a quantitative value, right?

558
01:07:10,380 --> 01:07:21,240
Usually it's either positive or negative. You can predict, let's say some of these H or you want to predict the sum of these blood pressure.

559
01:07:21,240 --> 01:07:28,100
So those are the the point is their quality. Classification is usually predefined categories.

560
01:07:28,230 --> 01:07:33,140
You want to put the outcome right, so exist versus none.

561
01:07:33,800 --> 01:07:39,230
So that will. Instead of calling that prediction problem technical, you would call that classification.

562
01:07:40,670 --> 01:07:46,160
One of the technologies diagnosed on a patient being having the disease or not having.

563
01:07:47,080 --> 01:07:50,950
So usually binary. So this is a binary classification you can have.

564
01:07:53,490 --> 01:07:58,700
Multiple categories. Right. So. So, you know, there's.

565
01:08:00,330 --> 01:08:12,860
Pattern recognition. Clustering.

566
01:08:19,420 --> 01:08:25,280
Ranking. So.

567
01:08:25,300 --> 01:08:32,980
So pattern recognition and clustering, usually I think we see mostly unsupervised type of the problem.

568
01:08:33,040 --> 01:08:36,450
So the example is, you know, some sort.

569
01:08:38,660 --> 01:08:48,420
Thinking back to that emotional. You see a graph like this and then, you know, if you're a human, you see it, okay?

570
01:08:48,600 --> 01:08:51,629
I mean, this is the intention. So it's easy to see.

571
01:08:51,630 --> 01:08:54,860
There are two clusters. So you see the pattern.

572
01:08:55,690 --> 01:08:59,080
They are not a homogeneous single cluster.

573
01:08:59,100 --> 01:09:08,190
So this is not considered to be easy. So you can cluster these data points in the in the points in two separate groups, although they are not really.

574
01:09:08,460 --> 01:09:12,000
So they are different recognize they are systematically different.

575
01:09:12,040 --> 01:09:18,810
This is the task of pattern recognition and then some of these are clustering.

576
01:09:19,440 --> 01:09:25,260
So ranking is actually I should write this towards the prediction.

577
01:09:27,240 --> 01:09:34,320
So ranking is kind of a unique problem. And you can think of all that's like a partial prediction problem.

578
01:09:35,520 --> 01:09:39,390
But this is the most famous example is the page ranking, right?

579
01:09:39,420 --> 01:09:47,440
When you query something I say, you say you query machine learning through Google that can give you a bunch of answers.

580
01:09:47,460 --> 01:09:51,750
But they are, I think the top rank, the ones it list.

581
01:09:51,750 --> 01:10:04,350
The first is the most irrelevant to your question. So this so the ranking itself doesn't necessarily have their output, not necessarily have.

582
01:10:06,070 --> 01:10:12,670
Meaning by itself. Right. It's only the relevance is a relative relevance towards the other.

583
01:10:14,870 --> 01:10:18,760
Things in the in the list, the relative importance.

584
01:10:19,100 --> 01:10:26,270
So like the page ranking. So that's a kind of a unique type of the problem, but it's also considered.

585
01:10:29,310 --> 01:10:37,770
So the other example in biomedical science is you want to understand which gene is relevant to certain diseases.

586
01:10:38,250 --> 01:10:44,250
So you may already know that all of the genes are going to be relevant because in the biological system,

587
01:10:44,250 --> 01:10:47,760
all the genes are going to more or less related to the disease.

588
01:10:48,000 --> 01:10:54,940
But you can still write the importance like direct the impact of certain genes to a disease.

589
01:10:54,950 --> 01:11:04,930
So that's another example for running. And then the last one I consider unsupervised learning problem is that my initial reaction?

590
01:11:10,000 --> 01:11:15,070
So those are more like data processing. You have a huge amount of data.

591
01:11:15,850 --> 01:11:26,230
For example, if you are taking, you measure the gene expression from a patient like, say, blood sample, you're going to get 20,000 genes.

592
01:11:27,220 --> 01:11:33,910
More than that, actually, nowadays, simulating not all of those are relevant to the problem you're going to.

593
01:11:36,650 --> 01:11:41,870
You're going to trying to solve. I say predict if the patient have certain disease or not.

594
01:11:42,260 --> 01:11:47,600
You only believe there is a small proportion of the genes going to kill you.

595
01:11:47,840 --> 01:11:51,850
The relevance of the disease relevance.

596
01:11:51,860 --> 01:11:59,000
So you need to do this so-called reduction to reduce to basically reduce the noise

597
01:11:59,150 --> 01:12:04,490
and then finding the relevant features from the data is that measure reduction?

598
01:12:05,600 --> 01:12:12,270
So. It's fair to say none of these are going to be a601602 tall picks.

599
01:12:14,690 --> 01:12:17,780
But nevertheless, it's an important topic. So if you're.

600
01:12:18,880 --> 01:12:21,100
You know, trying to work out a date.

601
01:12:22,000 --> 01:12:29,860
I'd say two days, but they are fall into this category of machine learning and then we cannot discuss those type of the problems.

602
01:12:31,390 --> 01:12:35,300
This is not the only inclusive list.

603
01:12:35,320 --> 01:12:38,620
There are some other things going to be there just to.

604
01:12:40,720 --> 01:12:45,390
So something like the algorithm to to do that decision making.

605
01:12:45,390 --> 01:12:53,700
So, you know, just not only include it here, but these are the typical things we're going to discuss during this class.

606
01:12:54,150 --> 01:12:57,900
So it's kind of a class specific definition for machine, right?

607
01:12:58,500 --> 01:13:02,910
Important thing. Algorithms and then data analysis.

608
01:13:03,180 --> 01:13:10,100
So statistics are still important. All right.

609
01:13:10,110 --> 01:13:13,920
So probably last topic for today.

610
01:13:14,340 --> 01:13:21,980
We're going to talk about learning scenarios. So the difference if you talk about machine learning what type of the learning we're talking about.

611
01:13:22,310 --> 01:13:30,200
So during the history review, we have already seen the three types of the machine learning scenarios.

612
01:13:30,210 --> 01:13:33,350
So those are actually the three scenarios.

613
01:13:34,000 --> 01:13:46,600
Supervised learning. So the definition of this is a learning by example.

614
01:13:52,200 --> 01:14:06,280
The earliest legal example. Let's do unsupervised.

615
01:14:19,520 --> 01:14:30,740
Learning that more. All right.

616
01:14:30,860 --> 01:14:37,760
So supervised learning is a typical type of the problems that deal with prediction classification.

617
01:14:39,440 --> 01:14:47,690
So. The most important characteristic of supervised learning.

618
01:14:47,690 --> 01:14:52,400
As we learn examples, there is a collect pre collected data.

619
01:14:52,790 --> 01:14:58,670
So you have improved that sense of label the for for example, the classification type of the problem.

620
01:14:59,030 --> 01:15:06,580
As I said, the classification of them prediction not fundamentally different, but sometimes it's easier to explain things with classification, right?

621
01:15:06,830 --> 01:15:18,540
So you already know you collect the bunch of. Some polls, which is already clearly label that patients have a disease, not having a disease.

622
01:15:18,900 --> 01:15:25,470
And then you're trying to have the poll that reads like, oh, these gene expression values measure.

623
01:15:26,760 --> 01:15:31,410
So those are the example. The existence of these example are important.

624
01:15:31,650 --> 01:15:38,580
So through this example, you're trying to find the relationship between the input and output, right?

625
01:15:38,610 --> 01:15:41,740
Simple as that. You have the examples.

626
01:15:42,030 --> 01:15:49,440
So the labels are the known output from those predictors.

627
01:15:50,100 --> 01:15:55,270
Right. And then. In the supervised learning.

628
01:15:55,270 --> 01:15:57,610
So you need to take advantage of the existing.

629
01:15:59,900 --> 01:16:06,470
Label the examples and I'm trying to find the functional relationship between the predictors and then the output.

630
01:16:06,860 --> 01:16:10,340
If you find that, you'll find the algorithm, simply put.

631
01:16:10,730 --> 01:16:15,710
But at this point, all you need to know is this is that learning by examples.

632
01:16:16,160 --> 01:16:21,020
Unsupervised learning is a learning with all examples like the clustering problem.

633
01:16:21,380 --> 01:16:25,040
Right. So there is no input output. Oh, you see? Is this.

634
01:16:26,240 --> 01:16:30,470
Right. You want to see the pattern? You want to see the cluster structure.

635
01:16:30,620 --> 01:16:34,120
There is no so so-called the learning data.

636
01:16:34,250 --> 01:16:39,980
There is just a single set. And when you are trying to classify that into.

637
01:16:40,250 --> 01:16:51,560
So if you have a case, you have learning without examples, you automatically know that is unsupervised learning.

638
01:16:52,630 --> 01:17:01,770
So the third reinforcement learning. We're not going to talk about this, but I think we have already talked about the.

639
01:17:03,930 --> 01:17:08,400
The difference or so that unique so learnings or interactions.

640
01:17:11,040 --> 01:17:14,670
It's not necessarily has to be a machine interaction.

641
01:17:18,340 --> 01:17:21,580
But interaction with environments in general.

642
01:17:23,380 --> 01:17:31,700
So one of the most important example, for example, in clinical trials is a dynamic treatment regime.

643
01:17:31,720 --> 01:17:36,340
Right. So those are kind of a situation learnings through interactions.

644
01:17:36,640 --> 01:17:49,180
So you gave a patient first a drug. So you have a different sort of treatment regimes, but depends on patients response to this, to this.

645
01:17:49,180 --> 01:17:59,390
And then you select different. It's a little bit like that. Physician trees, we initially thought in the artificial intelligence context, right?

646
01:17:59,410 --> 01:18:05,980
So you get so this is not the single answer yes or no like in the label the case.

647
01:18:06,160 --> 01:18:14,260
So those answers you get intermediate interactions between so called a player and then the

648
01:18:14,260 --> 01:18:21,879
environment and then you make the decision a dynamic physician according to that to the outcome,

649
01:18:21,880 --> 01:18:24,970
you get the overall goal, it's a little bit different.

650
01:18:24,970 --> 01:18:32,470
So you want to achieve some optimal outcome in the end, for example, that patients survive.

651
01:18:32,740 --> 01:18:35,980
You don't want to. That's the overall goal.

652
01:18:36,790 --> 01:18:43,690
Right. So so this is a little bit different than both of both of these supervised unsupervised.

653
01:18:43,900 --> 01:18:51,160
And then we're not going to discuss in this class. I'm pretty sure I think the department offer a different cost for this.

654
01:18:51,170 --> 01:18:54,430
So, no, you are. I don't know.

655
01:18:54,430 --> 01:19:01,390
I mean, the prerequisite. But you are you should be able to take this class under my understanding the difference between what we going.

656
01:19:02,600 --> 01:19:04,650
What are you're going to learn that class. Okay.

657
01:19:06,680 --> 01:19:16,870
So we're going to focus on this, too, and just realize there are something in the middle in type of the available data types, right?

658
01:19:17,180 --> 01:19:20,300
So there are online learning. We see the time.

659
01:19:26,560 --> 01:19:30,400
And Simon. Simon supervised of.

660
01:19:35,760 --> 01:19:42,250
So those are. The variance of supervised learning.

661
01:19:45,670 --> 01:19:49,770
So there is more statistical discussion on how you actually can do this.

662
01:19:49,780 --> 01:19:53,190
This is a more so the online learning is easy to understand.

663
01:19:53,200 --> 01:19:57,910
So in the supervised learning task you have a static learning.

