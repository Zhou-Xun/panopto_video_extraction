1
00:00:01,880 --> 00:00:08,210
Today we are going to talk about the statistical properties of the generalized estimating equation technique.

2
00:00:08,510 --> 00:00:13,520
And I believe this section contains a lot of new concepts and a lot of new formula.

3
00:00:13,820 --> 00:00:21,890
So I think if you have not review the slides, sorry, preview the slide before this class, you will find some new and unfamiliar concepts.

4
00:00:22,340 --> 00:00:27,110
And so that's the that's why I'm here trying to guide you through how to read this formula.

5
00:00:27,920 --> 00:00:33,560
Second, I will try to explain when.

6
00:00:34,730 --> 00:00:43,570
To use G. And when not to. So I think this will set up us very well for introducing the case study, which is,

7
00:00:43,580 --> 00:00:48,950
you know, Nancy, I probably will not have enough time to go through and out on Nancy.

8
00:00:49,550 --> 00:00:56,870
So my goal today, it's just trying to finish, hand out, be a quick review of the learning objectives.

9
00:00:57,980 --> 00:01:02,630
So first, describe the need of generalized estimating equations.

10
00:01:03,770 --> 00:01:09,830
And what is your take on that? Why do we need a generalized estimating equation?

11
00:01:13,880 --> 00:01:23,170
So in general. It is because we have to estimate.

12
00:01:25,390 --> 00:01:28,930
Estimate. Marginal models. Marginal models.

13
00:01:32,040 --> 00:01:35,430
Without a full likelihood.

14
00:01:39,390 --> 00:01:42,810
Specification. Right.

15
00:01:43,020 --> 00:01:46,390
So without a full likelihood, the maximum likely is just out of window.

16
00:01:46,470 --> 00:01:52,470
It's not possible. And we do need alternative strategies to be able to estimate the parameter of interest, which is beta.

17
00:01:53,130 --> 00:01:57,570
So G has been introduced as a way to produce those estimate.

18
00:01:58,230 --> 00:02:03,870
Um, number two, uh, clearly explain what is g and how to obtain the estimates.

19
00:02:08,720 --> 00:02:15,350
So we will be with you again. That gear essentially is just that, this whole thing.

20
00:02:21,330 --> 00:02:25,590
How do I remember this? Let's try to we'll walk through them.

21
00:02:25,600 --> 00:02:35,400
But I think this is a G, right. So we will explain that the D here is the partial of lie part should of beta.

22
00:02:37,910 --> 00:02:45,110
We'll explain that a little bit more, just to give you a general sense of what they are and VA essentially as they're working.

23
00:02:45,830 --> 00:02:49,280
Um, variance covariance.

24
00:02:49,640 --> 00:02:57,440
Okay. Um, and why is the vector of outcomes is a vector of expected outcomes.

25
00:02:58,190 --> 00:03:03,300
So. This is G. That's why you need to specify the ME model.

26
00:03:18,940 --> 00:03:27,000
And you need to specify the words in various covers. And clearly the vi it in the fact that it is being called working.

27
00:03:27,010 --> 00:03:32,770
I may not be equal to the true various governance matrix. Number three, we have not talked about it, but we will.

28
00:03:33,760 --> 00:03:38,380
So let's get back to part number three, but actually a part of it, too.

29
00:03:39,310 --> 00:03:46,460
So that's where we stopped in the last lecture. I believe it's on page.

30
00:03:47,730 --> 00:03:54,990
2021. So this is where we will start.

31
00:03:56,160 --> 00:04:05,580
Before I dive into the technical detail, I do want to do a quick review of the margin models and what we have talking about in terms of G.

32
00:04:05,910 --> 00:04:13,620
And so in the next maybe 10 minutes. Strictly no new material, just quick review and I just like anything else you learn.

33
00:04:14,280 --> 00:04:22,650
Now, on the first occasion, unless you're super familiar and we learned repeatedly, so this is a second time we're going to talk about this.

34
00:04:28,610 --> 00:04:33,960
So for GE, we know it is for estimating margin models, right?

35
00:04:33,980 --> 00:04:38,360
So we have to remember that margin, the model requires a few specifications.

36
00:04:38,960 --> 00:04:43,370
1 to 3, what are the three specifications?

37
00:04:46,330 --> 00:04:52,190
The first one is a big structure. Right.

38
00:04:52,610 --> 00:05:02,290
The second one is the marginal variances. And the third one is marginal associations.

39
00:05:09,140 --> 00:05:13,730
In terms of notation I'm going to write down. Bless you.

40
00:05:13,880 --> 00:05:20,959
So we transform. We transform the expected value of y j.

41
00:05:20,960 --> 00:05:27,930
Give an exact j. And then we link that to a vector of covariance.

42
00:05:28,470 --> 00:05:37,990
And this time. So this is what we meant by the main model specification, and we must assume this is correct.

43
00:05:49,190 --> 00:05:57,770
Okay. And this is very important. We assume we got to got it right for how the mean is related to coverage of.

44
00:05:58,920 --> 00:06:02,610
Did you still recall an assumption about the conditional?

45
00:06:04,580 --> 00:06:07,910
Conditional dependance. So there's assumption.

46
00:06:08,490 --> 00:06:11,720
It's called FCM, right? It's called full.

47
00:06:13,320 --> 00:06:15,840
What's the what's the name? What's the name of this assumption?

48
00:06:43,380 --> 00:06:53,790
Which is to say that's why J given the entire venture of entire set of it's can be simplified to just.

49
00:06:55,270 --> 00:06:59,600
The concerts you observed on the occasion.

50
00:06:59,620 --> 00:07:06,729
So the difference lies in the index here on the left hand side that is representing

51
00:07:06,730 --> 00:07:11,410
the entire set of covariance on over all the occasions on the right hand side,

52
00:07:11,800 --> 00:07:15,850
it is specifically pointing to the set of covariates.

53
00:07:15,850 --> 00:07:21,700
You obtained objectification. So under this assumption we are specifying this model.

54
00:07:22,180 --> 00:07:29,260
So that's a mean model specification. Second, we have the marginal variances.

55
00:07:29,950 --> 00:07:34,540
And in notation, we just write down this whole thing.

56
00:07:36,390 --> 00:07:39,870
As five times over in this function.

57
00:07:42,400 --> 00:07:48,500
So we called. This parameter phi to be over dispersion parameter.

58
00:07:52,440 --> 00:07:56,190
Some people or some our function calls the scale parameter.

59
00:07:56,610 --> 00:08:00,420
So when you see those names, they are referring to this file here.

60
00:08:01,500 --> 00:08:09,340
The second part, it is called a variance function. Function so.

61
00:08:10,380 --> 00:08:17,100
Again. It has various function, but not the entire marginal variance.

62
00:08:17,670 --> 00:08:25,400
It has been multiplied by the fight. And I think I want to emphasize here that because the v my j.

63
00:08:26,480 --> 00:08:32,180
It is a function. My right and my self depends on what depends on Peter.

64
00:08:36,200 --> 00:08:41,630
So it is likely that this marginal variance also depends on data.

65
00:08:41,720 --> 00:08:50,970
When you are specifying the model. Number three, the marginal associations and I have been using the word association.

66
00:08:51,690 --> 00:08:55,950
It has many different manifestations. For example, it can be Paris and correlation.

67
00:08:58,600 --> 00:09:12,690
Not despair. It can be long odds ratios.

68
00:09:17,650 --> 00:09:21,040
And many other things. And when we are considering a.

69
00:09:22,700 --> 00:09:25,730
We have talked about a particular specification.

70
00:09:26,630 --> 00:09:30,910
The unstructured one would be the covariance. Sorry, the correlation actually.

71
00:09:30,920 --> 00:09:34,650
Correlation of. Any pair of measurements.

72
00:09:38,020 --> 00:09:44,980
I. J i k. It is specified by what by row j k.

73
00:09:45,220 --> 00:09:49,480
For any j less than k greater than one good.

74
00:09:49,570 --> 00:09:52,930
Then you go to one and less than equal to I.

75
00:09:53,320 --> 00:10:01,980
Right? So this is what we call unstructured. Okay.

76
00:10:02,310 --> 00:10:05,880
Or we can say it can be can be Roe.

77
00:10:06,030 --> 00:10:12,030
So it is basically exchangeable, right? Thanks.

78
00:10:15,210 --> 00:10:18,810
Can I use x c for short? Because I don't want to write the entire thing.

79
00:10:19,560 --> 00:10:33,670
All right, now. Once we have done these three steps, we have already specified a marginal model, but usually two and three combined.

80
00:10:36,980 --> 00:10:46,050
We may specify. And G.

81
00:10:47,340 --> 00:11:02,900
Can protect against this. Okay.

82
00:11:05,080 --> 00:11:12,250
Now with a two, step two and step three, we now know how to combine the margin of variances and marginal associations.

83
00:11:14,020 --> 00:11:19,600
And in the special case of Pearson correlation, we can write down the variance covariance matrix.

84
00:11:19,810 --> 00:11:23,290
The working variance provides matrix as this one.

85
00:11:28,150 --> 00:11:45,190
Actually let me just write down. So the idea is of dimension.

86
00:11:48,580 --> 00:11:52,890
Night by night. And ah, I hear.

87
00:11:55,000 --> 00:11:58,720
I here is the person correlation matrix.

88
00:12:08,270 --> 00:12:16,760
It is specified in here, for example. You guys all know what IGG means, right?

89
00:12:18,560 --> 00:12:25,690
It's a Latin for, for example. Okay. Um. In a here is the.

90
00:12:26,730 --> 00:12:33,420
Of the form fi the new i1 that the phi new.

91
00:12:33,540 --> 00:12:42,809
Sorry the new. The new.

92
00:12:42,810 --> 00:12:47,430
I am not here. Okay. Everything off diagonal will be zero.

93
00:12:50,480 --> 00:12:54,560
Okay. So basically I hope that this.

94
00:12:57,120 --> 00:13:00,810
This part reviews the specification. So nothing new so far.

95
00:13:01,740 --> 00:13:06,510
And I do want to then bridge to the G.

96
00:13:08,340 --> 00:13:11,560
So. Again, still review. All right.

97
00:13:12,190 --> 00:13:15,740
So, G. What is G?

98
00:13:16,520 --> 00:13:19,640
G is to take. The marginal.

99
00:13:23,100 --> 00:13:31,030
Model specification. And estimates.

100
00:13:32,710 --> 00:13:36,690
Data. Okay.

101
00:13:37,170 --> 00:13:43,980
And the the reason why it is extraordinary is because it does not require us to specify the full likelihood.

102
00:13:45,480 --> 00:13:55,200
So then we have been using well, I have been using an analogy which I hope that to inspire your thinking about this technique.

103
00:13:55,560 --> 00:14:00,400
We call this some. Get the baby out of the crib.

104
00:14:00,410 --> 00:14:08,460
Right? Now we can see how I am dedicated to analogies out of crude.

105
00:14:10,200 --> 00:14:15,090
So the baby is the beta and the crib is the estimating equations.

106
00:14:21,930 --> 00:14:29,950
And I want to emphasize that. If you are hearing me saying this for a second time, a third time, it is estimating equations.

107
00:14:30,040 --> 00:14:34,300
So please bear with me. And what is the equation?

108
00:14:36,420 --> 00:14:57,530
So the equation actually is. Now we will need to dive into these annotations, um, notation.

109
00:14:57,740 --> 00:15:01,100
So you, you may feel that if you want to. Now, that's totally fine.

110
00:15:01,100 --> 00:15:10,579
But I think I just want to offer people who wants to have a firm, much firmer, more firm grasp of this equation.

111
00:15:10,580 --> 00:15:16,730
I want to explain his term in detail. So you may have the choice of deciding how much attention you pay to this.

112
00:15:18,230 --> 00:15:25,639
So first, I essentially is a vector of things like one up to me.

113
00:15:25,640 --> 00:15:28,940
I kn so it's a and I buy one vector.

114
00:15:29,390 --> 00:15:36,680
What it does is exactly as you have before. And because of the specification, we know it takes this form.

115
00:15:45,830 --> 00:15:51,290
This is one, actually. So if you disagree with anything here,

116
00:15:51,290 --> 00:15:55,820
you have to raise your hand and tell me that otherwise I assume that's technically

117
00:15:56,780 --> 00:16:00,830
you either copy in the notes correctly or you agree with the notation.

118
00:16:01,130 --> 00:16:06,590
So the bottom line is this my vector depends on the beta y.

119
00:16:06,590 --> 00:16:08,870
Clearly it is something you are very familiar with,

120
00:16:09,290 --> 00:16:19,489
the vector responses you're trying to model and it can be continuous or it can be binary, can be can can be categorical.

121
00:16:19,490 --> 00:16:36,030
What have you now? D This term is called the derivative of the vector mu with respect to beta and I think this just a.

122
00:16:37,700 --> 00:16:42,650
I had prepared for the lecture. And for those of you who are not operating.

123
00:16:45,130 --> 00:16:48,550
We're not doing like partial derivative derivatives every day.

124
00:16:48,850 --> 00:16:50,440
This can be a little bit mechanical.

125
00:16:52,180 --> 00:17:01,750
So I'm just going to say I'm going to write is to respect the notation that people have been using in multivariate calculus,

126
00:17:02,620 --> 00:17:05,980
which I hope you've learned in college. So.

127
00:17:07,120 --> 00:17:09,130
This strictly is this one.

128
00:17:14,490 --> 00:17:29,010
First Mila is what is and I buy one OC and beta is p by one, but I have intentionally put a t here it is because beta is a p by one vector.

129
00:17:29,010 --> 00:17:37,950
But when you are learning multivariate calculus, the beta you put on to the denominator of the partial derivatives is always a row vector.

130
00:17:38,370 --> 00:17:42,120
So I have to put a transpose of beta to put that into a row vector.

131
00:17:43,200 --> 00:17:49,200
So the result of this. The result of this is a nine by p matrix.

132
00:17:49,830 --> 00:17:59,850
So each element so di has a element of um, let's see, how can I represent this?

133
00:18:01,580 --> 00:18:06,020
So for the J youth. Element.

134
00:18:08,150 --> 00:18:17,120
Of the I it is essentially as partial of me i j over a partial of beta beta you.

135
00:18:19,080 --> 00:18:23,620
Okay. So hopefully this is clearer because my j is a scalar.

136
00:18:23,920 --> 00:18:30,260
You is just a number. A scalar. Right. So the idea is like this one.

137
00:18:30,270 --> 00:18:33,780
I told you, it's quite mechanical. So just bear with me for now.

138
00:18:35,220 --> 00:18:42,330
The I hear it essentially it is just the working variance covariance matrix.

139
00:18:42,360 --> 00:18:45,810
I will not repeat that again. But you have seen one example.

140
00:18:50,890 --> 00:18:59,830
So clearly it's night by night, right? Because it is characterizing how and they responses within Subject II are correlated.

141
00:19:02,660 --> 00:19:06,330
Why is that term? I have thought about it.

142
00:19:06,350 --> 00:19:12,710
So let's look at the entire dimension here. I said the AI is an AI viper.

143
00:19:12,920 --> 00:19:18,710
So if you do the transpose, this entire thing will be up by an IRA.

144
00:19:21,650 --> 00:19:29,840
Should I use another color? Let's use another color. I'm concerned that it's too much within this small space.

145
00:19:29,840 --> 00:19:33,079
So P by and I and this is and I by now.

146
00:19:33,080 --> 00:19:38,870
Right? Because a square matrix inverse is also a square of the same dimension.

147
00:19:39,960 --> 00:19:43,260
Why is and I buy one UI is in.

148
00:19:43,260 --> 00:19:49,800
I buy one. So what you. What you have will be a vector of.

149
00:19:51,350 --> 00:19:55,120
What of a P? P buy one, right?

150
00:19:56,660 --> 00:20:02,570
So because it's all compatible. So the resulting thing on the left will be a by one vector.

151
00:20:03,080 --> 00:20:08,210
And we're saying for each of these elements in that vector, we equate them to zero.

152
00:20:10,520 --> 00:20:15,860
And that is why we are calling this thing. Set of equations.

153
00:20:18,140 --> 00:20:24,290
Which terms contain beta. Which terms contain beta.

154
00:20:26,500 --> 00:20:35,500
So this term definitely contains beta because it's the main. This thing may contain beta depends on your variance function.

155
00:20:36,370 --> 00:20:40,230
Because Variantes function depends on the new and new depends on the data.

156
00:20:41,080 --> 00:20:46,450
The I here. It may depend on beta because it's a derivative of the main with respect to beta.

157
00:20:46,930 --> 00:20:52,990
If MU is nonlinear in beta, there is no way that after a partial derivative you can get rid of beta.

158
00:20:53,800 --> 00:21:02,300
But if by chance, well, if by the nature of the problem solving it is a regression, then new.

159
00:21:02,390 --> 00:21:06,700
I just need a function of beta so we can get rid of the beta so that I will have no beta in there.

160
00:21:08,550 --> 00:21:13,890
Anyway. I think that's the technical review of what we have so far.

161
00:21:14,490 --> 00:21:20,299
And I am trying to convince you is that. Mechanically.

162
00:21:20,300 --> 00:21:28,970
This is something that's doable. And I have to ask one question, is that how did we come to this?

163
00:21:29,660 --> 00:21:35,030
How do we come up with this equation? We were mimicking what technique?

164
00:21:49,080 --> 00:21:52,520
So. If we go back to this particular slide.

165
00:21:54,690 --> 00:22:03,470
Um. This is the objective function we are minimizing and this is trying to mimic what we have done in generalized at least squares.

166
00:22:03,830 --> 00:22:09,740
So if you take a derivative of this whole thing with respect to beta, you will get.

167
00:22:11,460 --> 00:22:21,330
This. Now with all the technical introduction behind us, what are the statistical probabilities we pursue?

168
00:22:21,540 --> 00:22:25,590
After all, why bother? Right. It is a lot of new details.

169
00:22:25,890 --> 00:22:33,770
It is not giving us some good statistical properties. It is not worthwhile to learn these things in general in statistics.

170
00:22:35,120 --> 00:22:46,170
We seek two things. The first one is. The beta have the estimate of p parameters well converging probability to beta.

171
00:22:46,830 --> 00:23:00,540
We call this consistency. And now I basically challenge you to tell us to verbalize what is the what to articulate, what is the meaning?

172
00:23:00,630 --> 00:23:10,940
Consistency. I know you can write down the probability of like beta had minus beta absolute value than epsilon.

173
00:23:10,940 --> 00:23:26,730
The project goes to zero. What does that mean? So basically it says that.

174
00:23:29,400 --> 00:23:32,880
The chance. Of making.

175
00:23:35,050 --> 00:23:38,080
An error. Of Epsilon.

176
00:23:40,900 --> 00:23:45,400
Goes to zero. As.

177
00:23:46,400 --> 00:23:53,090
The number of subjects. Goes to infinite.

178
00:23:58,400 --> 00:24:03,630
Infinity. Okay. So that's the meaning of. Consistency.

179
00:24:05,720 --> 00:24:11,570
Number two, it is about the distributional asymptotic distribution.

180
00:24:11,570 --> 00:24:18,980
So Peter had two times one converges and distribution to a Gaussian distribution.

181
00:24:20,280 --> 00:24:26,880
Uh. With the meaning of data and awareness concerns of data have.

182
00:24:29,990 --> 00:24:38,030
Actually, some people would in general make this arrow looking like this to indicate that it's a convergence in distribution.

183
00:24:40,830 --> 00:24:47,370
So we call this asymptotic. Asymptotic normality.

184
00:24:51,130 --> 00:24:57,370
All right. So. Question number one, this is.

185
00:24:58,310 --> 00:25:04,880
Quizzing your probability theory which form of conversions is weaker?

186
00:25:06,060 --> 00:25:11,210
Or easier to achieve. Well, I heard the right answer.

187
00:25:11,240 --> 00:25:16,130
That's true. So if you have the convergence of probability, do you have the convergence in distribution?

188
00:25:35,010 --> 00:25:43,140
In general, it's not necessarily true. So if you have conversion distribution, you can derive its conversions in probability.

189
00:25:43,830 --> 00:25:47,040
Although it's called weak conversions, it's not necessarily too weak.

190
00:25:47,280 --> 00:25:52,360
After all, it is providing you with a distribution which is not always normal.

191
00:25:52,410 --> 00:25:57,180
And if you just have a point conversions. Number two is.

192
00:26:01,310 --> 00:26:06,710
I want to say that here you don't see any. You don't see any hat here.

193
00:26:07,040 --> 00:26:11,720
So this is a theoretical. Quantity.

194
00:26:13,480 --> 00:26:24,010
Which means that it can depend on. My hand is twitching because of this angle is really weird.

195
00:26:24,910 --> 00:26:30,380
It can depend on. Unknowns.

196
00:26:33,700 --> 00:26:36,930
OC such as Sigma I, the true band school friends of Peter. Right.

197
00:26:38,130 --> 00:26:43,800
So later on, clearly you cannot use a theoretical variance to perform estimation of the variance,

198
00:26:43,800 --> 00:26:47,460
so you have to plug in the estimates of these unknowns.

199
00:26:48,960 --> 00:26:52,320
So these two things combined are what we call can.

200
00:26:54,880 --> 00:27:00,420
Consistent and a simple target normal. So we want to see can estimates.

201
00:27:04,330 --> 00:27:10,750
I just want to make sure that everybody's on the same page here. All right.

202
00:27:11,170 --> 00:27:14,560
So now all the slides that's going to follow.

203
00:27:14,620 --> 00:27:20,470
Essentially, it's trying to make these two points clear. First.

204
00:27:22,040 --> 00:27:29,090
As we say, with a very high probability, you had as close to the population regression parameter beta in large samples.

205
00:27:33,140 --> 00:27:41,360
And number two, it is important to note that this is consistent regardless of whether you have correctly specified the Within Subject Association.

206
00:27:42,380 --> 00:27:52,460
Um, so this is relevant because it does not require you to specify both the mean structure and the variance coherence model correctly.

207
00:27:53,780 --> 00:28:00,640
So it is a it is offering some protection against missed specification of the VI.

208
00:28:03,950 --> 00:28:09,820
And. To see how to see why this is true.

209
00:28:10,420 --> 00:28:17,020
So this is something I usually do. Look, you can take this equation.

210
00:28:18,380 --> 00:28:21,440
You can divide it by the number of subjects.

211
00:28:29,390 --> 00:28:33,050
You can do this, right? This is the equation you're solving.

212
00:28:35,190 --> 00:28:38,730
And now if you take the if you take the end to infinite.

213
00:28:43,760 --> 00:28:50,240
You see? Probably will limit.

214
00:28:50,330 --> 00:28:54,710
This will converge. Converges to what? As a convert to.

215
00:28:56,760 --> 00:29:28,770
It's expected value, right? And in general, the technique of the proof is to say that if you have the main structure correctly specified.

216
00:29:29,840 --> 00:29:37,280
This term will always be zero. So the beta will be estimated consistently and this technique in general is called zero.

217
00:29:37,490 --> 00:29:43,740
Sorry Z estimation. So Z is for zero.

218
00:29:44,650 --> 00:29:48,800
Because you said. A set of equations to zero.

219
00:29:50,300 --> 00:29:54,530
And this is often introduced in the semi parametric class.

220
00:29:55,760 --> 00:29:58,339
So I will not talk too much about the theory per se,

221
00:29:58,340 --> 00:30:07,520
but just to emphasize the fact that you want to specify the main structure as quickly as possible when you are doing real data application.

222
00:30:11,150 --> 00:30:14,780
Now onto the variants.

223
00:30:15,950 --> 00:30:21,200
So we are focused on essentially the same topic variants.

224
00:30:23,650 --> 00:30:30,400
Here. I'm not showing you any derivation. But I want to direct you to the form of the result.

225
00:30:31,610 --> 00:30:36,440
So this is the same topic variance covariance matrix for beta has.

226
00:30:37,700 --> 00:30:42,080
Recall better have is produced by your specification.

227
00:30:42,680 --> 00:30:46,370
You may have got a wrong balance clearance model specification.

228
00:30:48,120 --> 00:30:59,760
So it is totally reasonable that the quality or the uncertainty of beta out depends on your specification of the variance governance matrix.

229
00:31:00,180 --> 00:31:04,500
So you would expect the VI there. Let's take a close look.

230
00:31:05,430 --> 00:31:10,200
So the B is of this term. Well, you know what dis and you know what it veers.

231
00:31:11,820 --> 00:31:16,440
M You know, these two terms and this is the true various programs.

232
00:31:16,440 --> 00:31:31,740
We usually do know that by sigma, I hear. So putting together this true various covariance matrix actually is a, you know, is like a sandwich, right?

233
00:31:32,780 --> 00:31:36,110
On both sides. These are bread in the middle. It's meat.

234
00:31:36,680 --> 00:31:43,790
And why do we call it meat? It is because in the center, we include the true barons, Coburn's.

235
00:31:46,230 --> 00:31:55,530
As we say, this is a theoretical quantity. There is no way that this is an estimate because, you know, if you have de vie,

236
00:31:55,530 --> 00:32:02,080
that may depend on the beta dependent alpha and also you have the true events covariance.

237
00:32:02,100 --> 00:32:10,860
So you do have to plug in their estimate to provide an estimate of this variance now with this form.

238
00:32:10,920 --> 00:32:14,430
We have some exercise to do. I would do this with you.

239
00:32:16,780 --> 00:32:26,700
It is basically to consider, um, consider the possibility of the AI being equal to Sigma I here.

240
00:32:28,550 --> 00:32:33,740
So this is the equation we're going to be working with. Let me.

241
00:32:35,120 --> 00:32:42,030
Write down a question explicitly. So the variance actually the variance covariance of beta had.

242
00:32:44,880 --> 00:32:48,900
Is the inverse times b inverse.

243
00:32:50,100 --> 00:32:53,830
My question is. What is?

244
00:32:56,430 --> 00:33:04,740
Clarence. Peter Hart is the i e cause sigma here for all I.

245
00:33:06,560 --> 00:33:11,240
Which is to say that we have. Correctly.

246
00:33:13,580 --> 00:33:18,890
Specified. Specified the variance covariance.

247
00:33:25,840 --> 00:33:30,430
And you need to drive this. I'll give you, like, 3 minutes to do so.

248
00:33:30,940 --> 00:33:41,680
I will show the formula to top. And I think this exercise is very helpful to distinguish the roles of being a.

249
00:37:10,280 --> 00:37:16,250
Okay. So let's derive them together. I think it should be if you have the answer, congrats.

250
00:37:16,670 --> 00:37:25,910
But this should be quite straightforward. Essentially, we know that the formula is this.

251
00:37:29,200 --> 00:37:34,780
D I transpose the inverse d i and times the.

252
00:37:47,410 --> 00:37:52,750
All I'm doing is copy down everything I saw. So this one is this.

253
00:37:54,730 --> 00:37:59,790
Okay. Now with the assumption that we are equals the variance.

254
00:38:00,120 --> 00:38:03,620
So these two. Let me. Okay.

255
00:38:04,370 --> 00:38:11,710
So these two are canceled, right? And when you have what's left, you can look at this as inverse and this is not inverse.

256
00:38:12,250 --> 00:38:18,090
So these two terms are cancel. Actually we didn't have in verse here.

257
00:38:18,290 --> 00:38:22,610
So what's left is simply the bread, right? So the answer is B inverse.

258
00:38:24,240 --> 00:38:31,440
So what this exercise means is that when you have fortunately correctly specified a variance covariance matrix,

259
00:38:32,070 --> 00:38:35,130
the variance takes a non sandwich form. You just have the bread.

260
00:38:36,210 --> 00:38:46,090
Okay. And. There will be situations where you can correctly specify various governments, just like the UN structured various governments.

261
00:38:46,110 --> 00:38:54,330
You can never get wrong there. So in those cases you would want to use the B inverse as a variance coherence estimate.

262
00:38:55,640 --> 00:39:07,430
There are also other situations where it is impossible to provide or very hard to provide an estimate for the concerns of why I write.

263
00:39:07,430 --> 00:39:15,680
So you cannot include Emma there, so you have no choice but to use the B inverse as the variance coherence estimate.

264
00:39:22,840 --> 00:39:26,550
Now. This is the theoretical incoherence.

265
00:39:26,650 --> 00:39:31,540
Let's talk about assessment, which will which is related to this exercise.

266
00:39:31,540 --> 00:39:40,390
We do. So sandwich variance estimate for account for to account for potentially missed specified working within subject associations.

267
00:39:41,020 --> 00:39:44,700
So this one is of sandwich form, but it's not estimated.

268
00:39:44,700 --> 00:39:50,110
All right. So we need to understand what makes a sandwich variance estimate.

269
00:39:51,850 --> 00:39:55,660
First, B and M can be estimated by plug in.

270
00:39:56,170 --> 00:39:59,230
The Alpha Alpha remember is association parameter.

271
00:39:59,560 --> 00:40:05,590
PHI is the scale or over or over dispersion parameter beta is the regression parameter.

272
00:40:06,730 --> 00:40:18,820
You just plug in Vietnam into the equation, but there is a term sigma I that is that is a true variance coherence.

273
00:40:19,390 --> 00:40:27,640
So how can you estimate that generically you can use this particular sample variance covariance matrix.

274
00:40:30,220 --> 00:40:33,340
Basically you all have already fitted beater hats.

275
00:40:34,610 --> 00:40:42,890
And you just plug it into the movie form so you will have my hat and you take the vector residuals for subject I and then you do the across product.

276
00:40:44,980 --> 00:40:56,410
Once you have been fully had it fully estimated by the data, you will be able to produce the following sandwich variants estimate or.

277
00:41:00,410 --> 00:41:10,420
So this is the sandwich variance estimate. When I was learning this, I was saying, wow, this is really complicated.

278
00:41:13,450 --> 00:41:16,480
But it is a beauty of mathematics.

279
00:41:17,350 --> 00:41:21,210
It is just what it is. And I think to me, it.

280
00:41:22,320 --> 00:41:29,370
What's my expectation? So my expectation for you is to understand where this complicated form come from.

281
00:41:31,410 --> 00:41:36,239
Your stop your starting point can be that particular formula being vers times m and

282
00:41:36,240 --> 00:41:42,270
being vrs and then you've got to know from there how how do you come to this equation.

283
00:41:42,960 --> 00:41:48,360
And some of you may be wondering, hey, Jim, how did I know that the same toddy burns of this form?

284
00:41:49,890 --> 00:41:58,890
There are some textbooks to introduce this, but I believe that it is probably a little bit too advanced for an introduction class.

285
00:42:00,200 --> 00:42:03,499
As I said, if you're truly interested, come to my office. Our.

286
00:42:03,500 --> 00:42:08,330
I can find those references for you. So this can be a safe starting point.

287
00:42:09,530 --> 00:42:20,120
Number two is that fortunately you are only required to use our functions or SAS functions if you want to provide these estimates.

288
00:42:21,510 --> 00:42:29,970
And your task then is trying to identify. The column that correspond to this estimate, we will be providing examples.

289
00:42:32,480 --> 00:42:37,750
It is called also called empirical variance estimates.

290
00:42:38,180 --> 00:42:47,960
So some people will say, Hey, Forgey, you know, John, can you get me the empirical variance smarter than you should be able to know that is this one.

291
00:42:48,440 --> 00:42:55,610
So the reason why is called empirical is because this part is an empirical estimate of the variance.

292
00:42:55,610 --> 00:43:01,340
Covance. So people can refer to a sandwich estimate or imperial estimate.

293
00:43:02,090 --> 00:43:07,160
So you should know their the same thing. And there is a third name.

294
00:43:08,060 --> 00:43:11,270
It can also be called robust variance estimate.

295
00:43:12,330 --> 00:43:16,920
Why? Well, you do not have to specify the sigma by correctly.

296
00:43:17,760 --> 00:43:24,570
You can just specify VII and VII can be different from Sigma II, and that's why it is robust.

297
00:43:25,170 --> 00:43:28,260
The result beta better based on your VI is consistent.

298
00:43:29,300 --> 00:43:33,100
It's Barrons is correctly estimated by this one. Right.

299
00:43:33,110 --> 00:43:42,620
So it's robust. So three names, sandwich variance estimate, her empirical variance estimate, her robust variance estimates are exactly the same thing.

300
00:43:43,810 --> 00:43:50,580
Now. I don't know whether you are asking this question, if you are actually actively with me here.

301
00:43:51,060 --> 00:43:54,090
You may be wondering, Hey, Jim, isn't there a price to be paid?

302
00:43:56,140 --> 00:44:00,340
If we just get a VA grossly wrong from the truth.

303
00:44:02,430 --> 00:44:09,000
Is there a price? That's a question for you.

304
00:44:09,030 --> 00:44:14,370
Is there roughly a price there? I'm going to ask two questions.

305
00:44:14,400 --> 00:44:20,639
The first one is, what is the variance? Peter That's actually by far.

306
00:44:20,640 --> 00:44:26,760
I mean, there's clearance. If you have VII equals sigma I for eyes.

307
00:44:28,350 --> 00:44:31,760
We just arrived at. It is being inverse right?

308
00:44:32,570 --> 00:44:39,680
To what if V.I. does not equal sigma for any for some.

309
00:44:40,830 --> 00:44:44,370
I. So the appearance of Peter Hart is what?

310
00:44:45,740 --> 00:44:51,400
The sandwich form. Now by the price, I mean.

311
00:44:52,790 --> 00:44:56,990
First. Do you want a better head that's more precise or less precise?

312
00:44:57,680 --> 00:45:06,500
More precise, right. So in terms of the variance covariance matrix, what relationship do you want to have?

313
00:45:08,860 --> 00:45:14,290
Well, I can do. I can do what I can do this one.

314
00:45:16,610 --> 00:45:21,230
Minus this one. Can we possibly guess a condition?

315
00:45:24,380 --> 00:45:27,920
What can I say? Result or property of this matrix?

316
00:45:28,040 --> 00:45:37,160
That's the difference between these two. P.S.

317
00:45:40,370 --> 00:45:45,560
Positive. Semi definite.

318
00:45:47,070 --> 00:45:53,560
Right. So it basically is to say that the difference here is going to be larger than the inverse.

319
00:45:54,480 --> 00:45:59,790
But because these are matrices, so you cannot really say it's larger, you have to say it's positive, semi definite.

320
00:46:01,530 --> 00:46:11,770
So. Yeah, that's. Is is. Now returning to the question, do we have a price that's paid?

321
00:46:12,280 --> 00:46:16,540
If we did if we did not correctly specify the variance clearance matrix.

322
00:46:17,960 --> 00:46:29,180
The price is quantified here. Right. If you specify the true sigma by the resulting beta, that will have a bigger variance.

323
00:46:30,850 --> 00:46:35,540
Okay. So that's a. Let's. I'll give you 30 seconds to let that sink in.

324
00:46:35,560 --> 00:46:38,960
Okay. And this is pretty much the most important lesson.

325
00:46:40,000 --> 00:46:48,610
A message. And actually, whenever you listen to a talk or somebody do analysis,

326
00:46:48,610 --> 00:46:53,140
you can always ask this question, Hey, how did you decide the various governance structure?

327
00:46:53,860 --> 00:46:59,350
Are you sure you're getting the most efficient way to beta? And you can then observe the answer,

328
00:46:59,950 --> 00:47:08,680
and you want to see the answer roughly in the direction that I have confirmed that this variance coherence matrix pretty close to what data suggest.

329
00:47:11,150 --> 00:47:18,170
Then you probably do not pay a big price by using a little bit wrong variance clearance matrix.

330
00:47:31,380 --> 00:47:39,060
Summary beef for bread and for meat. And if we model the various class structure correctly, we have a simpler form.

331
00:47:40,890 --> 00:47:47,700
So a few remarks about G. Let's talk about sociology of G here.

332
00:47:48,120 --> 00:47:54,890
It's actually quite popular and. Why it is a popular.

333
00:47:54,920 --> 00:47:58,230
Well, you can tell from the construction.

334
00:47:58,880 --> 00:48:07,190
We did not require it for distribution or specification. And if you can get the VII to be close to Sigma I.

335
00:48:08,150 --> 00:48:13,760
You don't lose too much efficiency. Right. So that's a good thing.

336
00:48:13,910 --> 00:48:24,680
And it can be shown that the resulting estimated error from here is going to be the same thing as generalized squares under girls in assumptions.

337
00:48:25,790 --> 00:48:32,430
Okay. So these are the reason why it's popular.

338
00:48:38,030 --> 00:48:41,210
Number two. Number three, actually. Sorry. Number three.

339
00:48:44,250 --> 00:48:47,910
Beta hat is a consistent estimate here on their possible association.

340
00:48:49,050 --> 00:48:54,300
And the sandwich estimate provides the actual formula you can use for estimation of.

341
00:48:56,070 --> 00:49:00,870
And we provide the formula for the estimate for. Missing data.

342
00:49:01,830 --> 00:49:08,220
We have some students coming by and asking for their project on how to deal with missing data.

343
00:49:08,670 --> 00:49:11,880
And that's a that's something I want to teach.

344
00:49:12,420 --> 00:49:17,760
But I am just afraid that we will not have sufficient time to cover every detail.

345
00:49:18,150 --> 00:49:22,530
And fortunately in our department, I think we are pretty strong on our missing data.

346
00:49:23,670 --> 00:49:27,030
There are few people as close as you want to you want to take if you are interested in this.

347
00:49:27,690 --> 00:49:34,890
The first one is Louanne. I think she is wonderful lecture. She does a lot on missing data and clearly raw little.

348
00:49:37,240 --> 00:49:43,700
So. So when these two people offers cause and you interested in missing data, then take it.

349
00:49:44,840 --> 00:49:47,900
My role here is just trying to introduce some basic concepts.

350
00:49:48,910 --> 00:49:53,260
Um. First, does g work in the presence of missing data?

351
00:49:54,310 --> 00:50:03,220
It only works when. Well, using the original form we introduce, it only works when you assume the missing data is completely at random.

352
00:50:06,060 --> 00:50:09,150
Maybe you haven't heard about this term, but let me explain.

353
00:50:09,180 --> 00:50:14,730
It is the probability missing. This does not depend on any observed or unobserved information.

354
00:50:15,960 --> 00:50:22,720
Right. So do you think the indicator of you be in the classroom is completely random?

355
00:50:27,170 --> 00:50:36,409
I probably should not answer it, but you can think about it and think if you are doing a if you have a study that you have,

356
00:50:36,410 --> 00:50:42,650
people just naturally come back to the study site and there will be some people who just drop out.

357
00:50:43,400 --> 00:50:50,030
Do you think those people are going to be completely similar to the people who remain in the study?

358
00:50:51,510 --> 00:51:01,260
Probably not because they may be sick. They may have other, you know, more serious conditions preventing them from revisiting the study centers.

359
00:51:01,680 --> 00:51:07,480
So it is probably an overly optimistic sub cohort you will be analyzing.

360
00:51:07,500 --> 00:51:10,860
If you only focus on the subset of people who remain in the study.

361
00:51:12,150 --> 00:51:20,550
So this brings a question then. If is requiring such a strong missing data assumption, how can this be so popular?

362
00:51:21,360 --> 00:51:24,420
Well, this is the nature of a great method, right?

363
00:51:24,930 --> 00:51:28,320
You never question that girls in the inventing lives score is a dumb idea.

364
00:51:29,360 --> 00:51:36,050
You just generalize it. So people have generalized g to be to have weighted versions,

365
00:51:36,470 --> 00:51:48,200
especially inverse probability weighted versions so they can deal with missing data that are that can be classified as missing and random.

366
00:51:49,460 --> 00:51:54,710
So what does missing at random mean? Again, a whirlwind review.

367
00:51:55,130 --> 00:52:04,210
It is to say that. The probability mRNAs may depend on observed information, but not any unobserved information.

368
00:52:04,750 --> 00:52:10,690
So it means that the information that I measured upon this person can predict.

369
00:52:11,950 --> 00:52:16,270
Can predict the chance of this person being missing or not.

370
00:52:17,560 --> 00:52:27,460
Save for one particular visit. Right. But what if there are information that's not measured or obtained from a subject

371
00:52:28,030 --> 00:52:32,470
that are strongly correlated with the chance of this person missing a visit?

372
00:52:33,010 --> 00:52:36,750
Then you know, then that's not missing at random.

373
00:52:36,760 --> 00:52:40,630
It is called not missing at random. So that's the most challenging situation.

374
00:52:42,070 --> 00:52:49,030
But anyway, the summary of this slide is that if you are using the classical form of gear.

375
00:52:50,340 --> 00:52:57,240
In the presence of missing data, you are implicitly assuming the data is missing at random, completely random.

376
00:52:57,660 --> 00:53:02,820
And if you are using an inverse probably weighted version, you can deal with data that are missing in random.

377
00:53:03,720 --> 00:53:07,920
Who are the people to consult or learn from if you are dealing with this technique?

378
00:53:08,640 --> 00:53:13,180
So I think lou-ann and I think Walter Dempsey, Ron Little, Mike Elliott.

379
00:53:13,230 --> 00:53:17,330
These are people we can consult. Summary.

380
00:53:19,760 --> 00:53:24,710
So JE basically is awesome. We can have our cake and eat it.

381
00:53:24,740 --> 00:53:32,570
We can obtain a valid estimate of beta and its sampling variability, even if we have not model within subject association correctly.

382
00:53:37,000 --> 00:53:42,850
But can we? And we know that there is a price we will pay if we did not get the true events covariance correctly.

383
00:53:43,330 --> 00:53:49,900
But. But can we disregard the model for the various covenants among repeated measures for purpose of inferences about beta?

384
00:53:55,670 --> 00:54:01,460
In general, not a good idea. You have to use the sandwich variance estimate or.

385
00:54:04,760 --> 00:54:11,540
So one. So you may be wondering what are the some popular working parents covariance matrices?

386
00:54:12,620 --> 00:54:19,670
Many people use independence to say hey, let's assume we are just a diagonal with.

387
00:54:20,940 --> 00:54:26,250
Of diagonal elements being zero. That's a guess of the dependent structure.

388
00:54:26,580 --> 00:54:34,050
Could be bad guess, but that's a guess. And the sandwich bears estimate it will still rescue based on that specification.

389
00:54:34,830 --> 00:54:40,590
It is just that you probably have to pay a very high price if the data shows high dependance among the measurements.

390
00:54:45,690 --> 00:54:49,259
So this goes to this slide. Why is the choice of working?

391
00:54:49,260 --> 00:54:53,820
Coburn's will give you a higher position that better have. So that's the.

392
00:54:54,750 --> 00:54:57,750
So this is, again, the price I want to emphasize here.

393
00:55:06,540 --> 00:55:13,740
And a subtle. Reason for modeling the association.

394
00:55:15,340 --> 00:55:22,810
Is when the data shows some very irregular structure for sandwich variants estimated to work well.

395
00:55:23,860 --> 00:55:31,640
You have to have a reasonably large end. Why?

396
00:55:31,880 --> 00:55:41,610
Well. You see, we were talking about a can estimate a consistent and important analogy when you talking about steam tonic properties.

397
00:55:41,630 --> 00:55:48,330
There is always a question. I know. I always like ask the speaker, what do you mean by a large sample?

398
00:55:48,350 --> 00:55:52,250
Is a 50. Is it 51 or is it 100? So.

399
00:55:53,900 --> 00:56:01,520
It is not known if a priority before you analyze the data so that you know when the sample size is small,

400
00:56:01,940 --> 00:56:05,020
like 20, you probably is not in the same topia.

401
00:56:05,210 --> 00:56:11,650
Right. So you have to be careful there. The sandwich estimated may not be good enough.

402
00:56:11,660 --> 00:56:16,880
It can have can show high bias. Indeed, people have derived small sample corrections.

403
00:56:17,570 --> 00:56:27,320
And one of my students here, she will be I think she will be presenting some of that, at least some application that may use those techniques.

404
00:56:28,460 --> 00:56:33,800
Yes, I will probably invite two students of mine to present in the class.

405
00:56:36,530 --> 00:56:40,870
Before you guys present. So.

406
00:56:42,320 --> 00:56:46,940
So that's the situation where you have to be careful, too, when using a sandwich estimate.

407
00:56:47,150 --> 00:56:55,670
Number two, when the design is highly unbalanced, when design is highly imbalanced, what term is very hard to estimate?

408
00:56:56,690 --> 00:57:01,600
So if I go back to this formula. Just bear with me.

409
00:57:02,170 --> 00:57:08,730
So look, this y minus my hat times y minus my hat, right?

410
00:57:09,860 --> 00:57:14,720
If so, you have one set of time and I have a set of time points.

411
00:57:16,230 --> 00:57:19,410
What does this mean? It is not having any replications.

412
00:57:19,440 --> 00:57:23,610
Right. Suppose we have, say, 20 people who are measured at time.

413
00:57:23,610 --> 00:57:27,560
One, two, three, four, five, six, seven. All at the same.

414
00:57:27,590 --> 00:57:34,760
Common occasions. Then you have a replication of ten people to produce an estimate of errands here.

415
00:57:35,240 --> 00:57:40,580
But if you all ten people are measured at completely different time points, there is no replication.

416
00:57:41,670 --> 00:57:42,930
To speak with two speak.

417
00:57:43,590 --> 00:57:53,310
So that is the situation where you probably want to delve a little bit about the quality of the sandwich variance estimate her.

418
00:57:55,050 --> 00:58:00,240
And I think this is pretty much the same thing I will not elaborate on.

419
00:58:00,330 --> 00:58:09,360
So it is showing that there are some real data situations where the sandwich estimated it could be problematic.

420
00:58:10,260 --> 00:58:16,050
There are lots of work trying to rescue this. But what is the most common technique?

421
00:58:16,830 --> 00:58:22,740
Basically you just assume what you specified via VII is the true model.

422
00:58:23,580 --> 00:58:30,040
Say you specify the exchangeable correlation or you specify the exponential correlation.

423
00:58:30,060 --> 00:58:37,530
You assume that's the truth. And the reason why it helps is that first it reduced a number of parameters in various programs.

424
00:58:37,540 --> 00:58:43,450
You just need to estimate the row or the marginal variances and to treat is treated as truth.

425
00:58:43,840 --> 00:58:48,450
And then you can use just the bread. To estimate is variance, right?

426
00:58:48,690 --> 00:58:52,180
So in this you don't have any Y minus Y part, right?

427
00:58:52,240 --> 00:58:59,460
Those residuals, you don't have them here. So the resulting estimate of the covariance is called.

428
00:59:00,770 --> 00:59:03,860
Model based estimate of the covariance.

429
00:59:04,790 --> 00:59:08,720
What is a model? You assume that Sigma equals VI.

430
00:59:09,140 --> 00:59:13,730
You are modeling sigma and you believe that model is true.

431
00:59:14,510 --> 00:59:25,720
Okay. So. The reason why I mention this is because in SAS we are you will see this term and I just want to be extremely clear what that means and.

432
00:59:27,270 --> 00:59:32,130
And this is the terminology. So it's always estimated just a bread.

433
00:59:34,210 --> 00:59:37,390
Um. So. Summary.

434
00:59:37,420 --> 00:59:42,520
I promise you'll have 5 minutes break and then we can use 10 minutes of our data if you want.

435
00:59:43,750 --> 00:59:51,160
So G is a likely free estimation technique for margin models, so you have to know what is specification margin model.

436
00:59:51,940 --> 00:59:57,910
And number two, sorry. In this technique, we focus on the regression coefficient beta.

437
00:59:58,390 --> 01:00:04,180
The goal is trying to estimate beta consistently and with the same type of normality.

438
01:00:05,790 --> 01:00:14,350
This method subsumes what you learn in Gaussian based regression theory as a special case.

439
01:00:14,370 --> 01:00:18,270
So G itself generalizes the generalized squares.

440
01:00:21,180 --> 01:00:29,550
And numerically, it's often obtained iteratively, and it is rarely the case that you can solve it by hand.

441
01:00:30,420 --> 01:00:36,570
So I don't know. I think that when I was learning this, I was asked to program G from scratch.

442
01:00:38,000 --> 01:00:43,520
So it's a bit. It's very you know, it's a lot of work, but I'm not requiring of that.

443
01:00:43,640 --> 01:00:46,640
So as long as you can use. Ah, that's okay. Um.

444
01:00:49,750 --> 01:00:54,209
Number three, we emphasize that you must assume the structure is correctly specified.

445
01:00:54,210 --> 01:01:04,550
Otherwise this consistency is not guaranteed. But you do not need to specify the association structure correctly and the for the variants of G.

446
01:01:05,000 --> 01:01:12,080
S mater. Theoretically takes so much form and you can replace a theoretical form by estimating it.

447
01:01:12,740 --> 01:01:15,740
You do this by plug in all the estimated quantities.

448
01:01:17,490 --> 01:01:26,040
Number five, which is the model based variance in situations where you have small sample sizes, you have irregular timings and measurements.

449
01:01:27,900 --> 01:01:32,550
You may want to consider assuming Sigma AI is correctly specified.

450
01:01:33,030 --> 01:01:36,360
And the reason why that helps is if you have a small number of people,

451
01:01:36,660 --> 01:01:46,200
you'd better have a parsimonious model instead of relying on a totally unstructured y minus mu type of residual calculation.

452
01:01:46,890 --> 01:01:51,250
So that so there is a role for model based variance. Okay.

453
01:01:51,250 --> 01:01:55,270
So I think I decided not to continue on.

454
01:01:55,270 --> 01:02:07,150
I see. So you guys can have 8 minutes back on, but in the next lecture we will be primarily focused on some examples, theta examples for Ferengi.

455
01:02:09,220 --> 01:02:13,060
Yeah, that's what I have today. And have a good afternoon.

