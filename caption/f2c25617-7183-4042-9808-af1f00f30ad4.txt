1
00:00:09,590 --> 00:00:13,309
One of the things that you may have been surprised to have missed in my discussion

2
00:00:13,310 --> 00:00:17,209
of evaluating the evidence when looking at the literature with statistical

3
00:00:17,210 --> 00:00:25,610
significance now P values are something that are taught in all introduction to statistics courses and are highly pervasive throughout the literature.

4
00:00:26,180 --> 00:00:32,330
Yet there's a growing movement by many epidemiologists to eliminate the presentation of P values altogether.

5
00:00:34,010 --> 00:00:43,280
In fact, one of the leading minds in epidemiology, Sander Greenland, has published several scathing articles arguing against the use of P values.

6
00:00:43,940 --> 00:00:52,580
In fact, he has argued that the use of P values has led to interpretations that are simply wrong, sometimes disastrously so.

7
00:00:52,910 --> 00:00:57,260
And yet these misinterpretations dominate much of the scientific literature.

8
00:00:58,970 --> 00:01:01,670
Now, he's not the only one to share this sentiment.

9
00:01:01,880 --> 00:01:09,740
In fact, many journals have begun to ban the presentation of P values and encourage authors not to focus on statistical significance.

10
00:01:10,820 --> 00:01:16,340
Interestingly, the discussion of the debate even had the popular press with pretty exciting headlines,

11
00:01:16,340 --> 00:01:20,780
such as what you see here from Nate Silver's FiveThirtyEight and from Vox.

12
00:01:22,580 --> 00:01:25,459
Now, as a result of this bit of scientific drama,

13
00:01:25,460 --> 00:01:34,310
the American Statistical Association has published a statement to try and help clarify the purpose and proper use of P values in the literature.

14
00:01:35,180 --> 00:01:44,329
They importantly concluded that scientific conclusions and policy decisions should not be based on whether or not a P value passes.

15
00:01:44,330 --> 00:01:52,490
Some specific threshold. P values or statistical significance do not measure the importance of a result.

16
00:01:53,420 --> 00:02:00,170
So if I piqued your interest in this video, I'm going to remind you what the meaning is of a p value.

17
00:02:00,560 --> 00:02:02,630
Discuss how the commonly used.

18
00:02:03,020 --> 00:02:11,000
Describe the issue with these statistics and then talk about favorite approaches for considering the role of chance and her findings.

19
00:02:12,830 --> 00:02:21,139
Now, what is the definition of a p value? Well, p value is defined as the probability under a specified statistical model that a

20
00:02:21,140 --> 00:02:26,150
summary measure of the data would be equal to or more extreme than the observed value.

21
00:02:27,140 --> 00:02:34,549
Here, the underlying idea is that there is variation in the world and that P values are meant to help us

22
00:02:34,550 --> 00:02:41,540
understand how extreme or uncommon the data that we see is given some assumed model of the truth.

23
00:02:42,590 --> 00:02:47,059
P Values are typically used because we're collecting samples or different

24
00:02:47,060 --> 00:02:52,100
realizations of the world rather than testing all individuals in a population.

25
00:02:53,810 --> 00:02:59,900
In practice, it's very common for researchers to use some cut point, say 0.05,

26
00:03:00,200 --> 00:03:05,990
at which they magically say that the data is either inconsistent or consistent with some null hypothesis,

27
00:03:06,260 --> 00:03:17,510
which is typically specified as no association. In other words, if they detect a p value that's less than 0.5, they'll say Hooray, I see an effect.

28
00:03:18,020 --> 00:03:24,920
Whereas if they see a p value that's greater than 0.5, they're going to interpret it as nothing is there?

29
00:03:25,430 --> 00:03:28,850
This win or lose approach, however, is very misguided.

30
00:03:30,740 --> 00:03:37,760
Perhaps the use of the word significance is really what's altering people's vision of the actual importance of this measure.

31
00:03:38,420 --> 00:03:44,300
Because if you think about it, how uncommon things are is occurring on a continuum.

32
00:03:45,170 --> 00:03:50,180
Therefore, there's really nothing magical about this cut point of 0.05.

33
00:03:50,660 --> 00:03:52,250
And the truth of the matter is,

34
00:03:52,250 --> 00:04:05,330
is that something with a p value of 0.055 or even 0.07 is really not all that much more likely than something that has a p value of 0.05.

35
00:04:06,260 --> 00:04:13,970
In other words, hypotheses don't automatically become truth just because they've passed this little threshold.

36
00:04:15,740 --> 00:04:24,050
Even when we do find significant results. It doesn't always imply that those significant findings are large, important, or even accurate.

37
00:04:24,770 --> 00:04:31,219
The power that we have to detect statistically significant differences depends on so many different factors,

38
00:04:31,220 --> 00:04:35,030
including the size of the sample, the magnitude of the effect.

39
00:04:35,030 --> 00:04:41,120
We're looking at variability in the exposure as well as error that we have in our measures.

40
00:04:41,900 --> 00:04:50,000
For example, with enough samples, I can detect statistically significant differences that are teeny tiny,

41
00:04:50,450 --> 00:04:55,880
so tiny, in fact, that they literally have no clinical relevant information in them at all.

42
00:04:56,930 --> 00:05:03,020
In contrast, there are times when I have extremely large and important associations that I'm seeing in data,

43
00:05:03,530 --> 00:05:10,310
but they can appear non-significant because my sample size might be too small or the measurements are imprecise.

44
00:05:11,150 --> 00:05:18,800
And finally, statistical significance tells us absolutely nothing about the quality of the information that we're using in our test.

45
00:05:19,190 --> 00:05:23,780
So if bias exists, you know, I might have a statistically significant result,

46
00:05:23,780 --> 00:05:31,380
but it may still be wrong because journals and readers continue to place so much value on p values.

47
00:05:31,400 --> 00:05:38,120
However, we see evidence of what's called publication bias in the literature where findings with very

48
00:05:38,120 --> 00:05:42,770
strong P values and strong results were the ones that were most likely to be published.

49
00:05:43,490 --> 00:05:46,309
That's in contrast to papers with null findings,

50
00:05:46,310 --> 00:05:52,490
meaning they really didn't see associations where those results either tended to be published in lower quality journals,

51
00:05:52,850 --> 00:05:56,360
not published at all, and sometimes not even written up.

52
00:05:57,410 --> 00:06:02,840
This tendency to only publish positive findings that are consistent with what we think is

53
00:06:02,840 --> 00:06:08,930
going to be true in the world in advance of doing the study is dangerous for so many reasons,

54
00:06:09,260 --> 00:06:13,250
given that we're in a field that we're supposed to be objectively seeking the truth

55
00:06:14,390 --> 00:06:19,640
as part of synthesis of information when we're looking across different studies.

56
00:06:20,150 --> 00:06:27,200
If we don't even. And half the studies with no associations or associations that are counter to our hypothesis

57
00:06:27,200 --> 00:06:33,680
published to go off of how are we ever to get a good sense as to what the literature is telling us?

58
00:06:35,390 --> 00:06:39,620
So hopefully I've convinced you of all the perils of P values.

59
00:06:40,160 --> 00:06:43,370
So now what? What do we do and what takes its place?

60
00:06:44,060 --> 00:06:46,430
Well, the answer is truly multi-pronged.

61
00:06:46,970 --> 00:06:56,060
The first step is to really create clear data analysis plans from the get go and publish all of your planned findings,

62
00:06:56,300 --> 00:07:05,960
regardless of what those associations say. You should also be clear when you're showing results from models that you decided to run post-hoc.

63
00:07:06,410 --> 00:07:16,700
In other words, after the fact, since often these most interesting findings are ones that are likely to be either unexpected and or less reproducible.

64
00:07:17,960 --> 00:07:22,940
Finally, you should look for consistency across related findings and studies.

65
00:07:23,690 --> 00:07:32,060
For example, in my particular field, it's not uncommon for us to publish on several different air pollutants at once with one health end point.

66
00:07:32,840 --> 00:07:39,860
To be honest with you, I'm far more inclined to believe research that says there's an association between pollution health

67
00:07:40,370 --> 00:07:45,140
if all of the different related exposures have the same positive relationship with the outcome,

68
00:07:45,560 --> 00:07:50,389
but are non-significant such as what you see on the left hand side of your screen as

69
00:07:50,390 --> 00:07:54,650
compared to scenarios where I might see many statistically significant findings.

70
00:07:54,980 --> 00:07:59,930
But those findings range all over the map, like what you see on the right hand side of the screen.

71
00:08:01,730 --> 00:08:07,280
So in summary, I hope that I've convinced you that simple interpretation of P values with some

72
00:08:07,280 --> 00:08:13,340
pre-defined cutoff is highly problematic and not favored in modern epidemiology.

73
00:08:14,330 --> 00:08:22,940
Now, unfortunately, as Sander Greenland says, there's no interpretations of these concepts that are at once simple, intuitive, correct and foolproof.

74
00:08:23,870 --> 00:08:32,870
Therefore, what we must do is to be mindful of the data that's informing our estimates and look for consistency across our metrics and studies.

75
00:08:33,560 --> 00:08:40,520
Ultimately, this is going to provide us with far more confidence in the associations than any one P value can do.

