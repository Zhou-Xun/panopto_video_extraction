1
00:00:00,450 --> 00:00:06,260
All right. So let's continue our discussion on March of Change.

2
00:00:06,840 --> 00:00:13,110
So the last time we talk about Markov property, it's very simple.

3
00:00:13,380 --> 00:00:17,700
But given the present, the past and the future are independent.

4
00:00:17,710 --> 00:00:23,610
So it's really about implying a conditional independence relationship.

5
00:00:23,880 --> 00:00:30,720
Right. Some other terminology is we're going to use homogeneous Markov chain transition kernel.

6
00:00:32,310 --> 00:00:37,320
And then we demonstrate even this seemingly very simple,

7
00:00:37,740 --> 00:00:48,840
stochastic model that we can use to describe some variabilities occurred in real science, like in genetics, population, genetics.

8
00:00:52,710 --> 00:00:56,670
So next we will explore more properties of Markov chain.

9
00:00:57,750 --> 00:01:05,220
Some of the questions really related to the fundamental properties.

10
00:01:05,700 --> 00:01:13,890
For example, it looks like we're defining a homogeneous Markov chain with a one step transition kernel.

11
00:01:14,310 --> 00:01:18,990
Right. So the maybe the most obvious question is, does it.

12
00:01:21,730 --> 00:01:25,290
Describe all the properties. The Markov chain kind of.

13
00:01:25,630 --> 00:01:32,710
Right. So if you're trying to understand some very basic questions of the fundamental questions about stochastic process.

14
00:01:33,100 --> 00:01:39,310
So let's say X is this great time, discrete state space, Markov chain,

15
00:01:39,970 --> 00:01:48,000
and then everything seemingly is defined by the transition kernel hijack, right.

16
00:01:48,010 --> 00:01:56,980
So this is it tells you how the Markov chain moves in one slide from that I just j for arbitrary pairs of iron chain.

17
00:01:57,970 --> 00:02:09,190
The question is is that sufficient? And one of the the question we're going to study today is called the Chapman format growth equation.

18
00:02:26,730 --> 00:02:28,730
So Chapman called on the royal equation.

19
00:02:28,740 --> 00:02:37,040
Trying to solve this the following question with one stout translation Colonel, can we get arbitrary unstuck transitions?

20
00:02:37,740 --> 00:02:42,650
Right. So we're going to run in the Markov chain multiple steps, right?

21
00:02:42,660 --> 00:02:48,150
So in the case of either Gambler's Rule, we're not just interested in one step.

22
00:02:48,810 --> 00:02:59,280
It's a one step from accession to X. In the last one we may interested in x and two x and plus p where p is arbitrary integers.

23
00:03:00,480 --> 00:03:04,680
Sometimes we're interested in something and it's much larger.

24
00:03:04,680 --> 00:03:07,979
So if you're given the initial state, you want to understand,

25
00:03:07,980 --> 00:03:14,220
like after you're running the Markoff train long enough, what is the marginal distribution of X?

26
00:03:14,580 --> 00:03:24,160
Right. So those are seemingly requires more than just one step transitions and we need to understand transitions.

27
00:03:24,180 --> 00:03:40,380
That's a starting point. So let's define that step or in this case I understand transition as simply as the probability X and plus an equal.

28
00:03:40,860 --> 00:03:48,690
So the transition is still going to be I to j, but we know that is after m steps.

29
00:03:49,320 --> 00:03:54,360
So because well, in the in the situation we have a homogeneous.

30
00:03:56,320 --> 00:04:03,160
Tiny homogeneous Markov chain. So that that's supposed to be independent of anchor point so where you start

31
00:04:03,160 --> 00:04:14,469
counting and steps so on the plus and given at an equal to I right just to

32
00:04:14,470 --> 00:04:20,470
emphasize this is a high and homogeneous Markov chain this is probability this

33
00:04:20,830 --> 00:04:27,790
particular conditional probability is independent of the anchor point for there.

34
00:04:27,790 --> 00:04:32,800
Right? This is just x and j given x is zero.

35
00:04:34,120 --> 00:04:37,779
Right? So here I just changed our point to zero.

36
00:04:37,780 --> 00:04:42,460
So if I see if I just start from the very beginning.

37
00:04:44,890 --> 00:04:53,650
So the champion for my group B finishes is basically trying to say this particular quantity,

38
00:04:54,190 --> 00:05:04,419
this is called and step transition col can be derived directly from this the ones that transition col the pie j and then how to do that.

39
00:05:04,420 --> 00:05:15,100
It's pretty actually. I just forgot to write this for tight homogeneous Markov trace, so you probably won't get that note.

40
00:05:16,000 --> 00:05:19,180
And so how do we calculate this?

41
00:05:23,530 --> 00:05:26,890
So pie jam me, right? Yeah.

42
00:05:38,620 --> 00:05:45,580
Right. So the idea is break this star into me, the intermediate stops, right?

43
00:05:46,540 --> 00:05:53,350
And then again, if you're trying to say there is a intermediate step, as after I say.

44
00:05:55,480 --> 00:05:59,530
The certain steps, like, say, Adams steps. Oh, let's.

45
00:05:59,620 --> 00:06:03,380
Well, let's change to a different problem. I mean. Okay.

46
00:06:04,860 --> 00:06:08,950
I'm sorry. I shouldn't stick to my nails. No.

47
00:06:09,960 --> 00:06:13,870
Okay. We're going to change the problem a little bit. Just for the ease of the notation.

48
00:06:14,980 --> 00:06:19,360
Let's consider an a step in transition, Colonel.

49
00:06:19,960 --> 00:06:27,670
And this is an ambassador we provide right now, and I'm consistent.

50
00:06:28,120 --> 00:06:37,420
So this is a plus and step transition and this is an plus step transition probability from state to state.

51
00:06:37,430 --> 00:06:40,570
J As reflected by the right hand side here.

52
00:06:44,950 --> 00:06:53,440
So the idea of Chad call my proof equation is really just trying to break this down by adding in a intermediate step.

53
00:06:53,770 --> 00:06:57,490
So if you have a m and that and then you can think about this.

54
00:06:57,820 --> 00:07:04,600
Well, I mean, the intention is pretty obvious. You break this into a and step transition and then that transition.

55
00:07:04,960 --> 00:07:10,330
So the intermediate step has to occur. Well, depends on, let's say.

56
00:07:12,700 --> 00:07:17,410
So the way we do it is using this probability equation.

57
00:07:18,420 --> 00:07:27,730
Okay. So after and step.

58
00:07:34,760 --> 00:07:48,110
Right. So this is a joint conditional distribution if your consider all possible stops after and step right so there's oh possible stops on.

59
00:07:49,910 --> 00:07:56,440
All possible states. But after I'm staffed, the Markov chain can stop all that.

60
00:07:56,450 --> 00:08:04,960
You get this equation. So equivalently, this one is the same as is just now becomes a review, right?

61
00:08:04,970 --> 00:08:13,010
So the probability I am an equal to j conditional on x zero.

62
00:08:13,010 --> 00:08:17,510
Here I am.

63
00:08:19,910 --> 00:08:27,350
So these two lines are the same. So you don't have to use this line to get this line right.

64
00:08:27,350 --> 00:08:32,840
So this one is just execute the the expected value line.

65
00:08:32,900 --> 00:08:38,120
And then from here, you kind of see how this line lead to this line, right?

66
00:08:38,120 --> 00:08:43,410
So the probability here, we're going to use the Markov property first, right?

67
00:08:43,790 --> 00:08:48,610
The Markov property basically saying there. But either way, I think you can get it.

68
00:08:48,620 --> 00:08:57,079
So the conditional on the two points, one is in the far past, the one is in the near post.

69
00:08:57,080 --> 00:09:04,430
And so only the accent matters. Either way is through this two lines, I should say.

70
00:09:05,000 --> 00:09:10,790
You kind of right, either the reasoning from this line or this line, they give you the same.

71
00:09:13,600 --> 00:09:19,030
Result which we need for the Chapman chromatography equation.

72
00:09:19,300 --> 00:09:26,200
Right. So the first line is just marginalization. The second one is the total expectation loss.

73
00:09:26,350 --> 00:09:31,870
Right. As we said, the probability is the same thing as the expectation.

74
00:09:37,390 --> 00:09:43,810
So if you work on the second line, that's more straightforward or faster.

75
00:09:44,950 --> 00:09:52,870
And in either case, you should get this because the banks analyze the probabilities,

76
00:09:53,040 --> 00:10:08,070
the probability of the probability excitement, plus an equal to j conditional on x am equal to pay x is zero equal to two on.

77
00:10:09,830 --> 00:10:22,550
I multiply by the probability I can write.

78
00:10:23,380 --> 00:10:27,730
So from the this well, I mean, I'm counting the second line here.

79
00:10:28,150 --> 00:10:35,230
So this is a just turn that problem of the drawing probability into this factorization, right?

80
00:10:35,350 --> 00:10:39,070
So you get these multiplied by this, you get in the middle.

81
00:10:39,550 --> 00:10:44,600
The second line is the same. The second life for the for the for the bottom line.

82
00:10:44,620 --> 00:10:51,429
If you're trying to calculate from there, this is also the expectation taken with respect to all possible values of X.

83
00:10:51,430 --> 00:10:54,920
And so either way, take one of the your favorite.

84
00:10:55,900 --> 00:11:04,990
But anyway, we get these two lines and now we realize this is how we're going to apply the Markov property,

85
00:11:05,380 --> 00:11:13,840
given the relatively near past, the far past, and then the future becomes independent.

86
00:11:14,470 --> 00:11:22,240
This is a generalization of the Markoff property, and if you are not sure about this, you should prove it yourself.

87
00:11:22,330 --> 00:11:36,550
But that's relatively trivial that allow us to achieve simply simplification by just conditional.

88
00:11:39,880 --> 00:11:44,610
The best case scenario in the near post this.

89
00:11:53,260 --> 00:12:03,549
Okay. So we do so these tools are more like the problem where starting from the very

90
00:12:03,550 --> 00:12:09,850
beginning because if you look at this this represent unstuck translation kernel,

91
00:12:10,390 --> 00:12:18,490
right. So after. So the anchor point is time this time and then the end.

92
00:12:18,490 --> 00:12:22,330
The destination here at the end point is I'm plus end.

93
00:12:22,330 --> 00:12:25,659
So there is also that transition going on here.

94
00:12:25,660 --> 00:12:31,180
There it's amidst that transition going starting from time, zero to time.

95
00:12:31,180 --> 00:12:44,610
And so I can just simply write this assumption of, okay, now I'm going to write this in the form of an unstable transition kernel from K to J.

96
00:12:44,620 --> 00:12:53,229
So okay. J in steps multiply by here, I'm stuck in transition from it.

97
00:12:53,230 --> 00:12:59,710
Okay. So I'm starting from I write it even better.

98
00:13:03,040 --> 00:13:18,130
I just swap the two terms. Okay, so this one equals E.

99
00:13:19,120 --> 00:13:30,370
I would say that this equation is known as the Chapman parameter of equation four.

100
00:13:32,770 --> 00:13:38,979
So the derivation is quite straightforward, nothing more as well.

101
00:13:38,980 --> 00:13:46,940
You get multiple ways you can get here. We did use Markov property in a more generalized way, given the paths,

102
00:13:47,890 --> 00:13:55,330
give us a present, the possible future independent and what this conclusion tells us.

103
00:13:56,560 --> 00:14:01,630
Remember, I'm and the breakdown of ampersand it's really arbitrary.

104
00:14:01,690 --> 00:14:08,740
We didn't require any special property. What is is a full M has to be one no no such restriction.

105
00:14:09,100 --> 00:14:11,830
And as of last night under there's no such restriction.

106
00:14:12,880 --> 00:14:21,330
But the breakdown is arbitrary and what you need to do is sum over all possible state space, right?

107
00:14:21,430 --> 00:14:33,630
So it's just enumerate the state space some over all possible intermediates stocks or list states and then you get this page.

108
00:14:34,900 --> 00:14:40,710
So the equation should remind you a lot about matrix multiplication, right?

109
00:14:41,140 --> 00:14:48,880
So if you have two matrix multiplying together, you have the corresponding matrix element together.

110
00:14:48,910 --> 00:14:58,250
The product of matrix has this sort of an action in here and these are some over all possible indices.

111
00:14:58,770 --> 00:15:03,310
But just in the middle, we'll talk about this a little bit later, but for now.

112
00:15:06,570 --> 00:15:15,780
So why this result is useful in terms of deriving arbitrary unstuck from.

113
00:15:16,320 --> 00:15:29,700
So obviously because I'm I'm doesn't have any kind of strikes so from what is that transition kernel we have find this in a two step contribution.

114
00:15:31,750 --> 00:15:33,059
Right. So just apply.

115
00:15:33,060 --> 00:15:40,680
I'm equal to one and equal to one everything in the summation form it's known that's from one step transition kernel and now you get.

116
00:15:41,460 --> 00:15:49,920
So same thing you can from here you can respect transition kernel you have a choice of do what you can use either and

117
00:15:49,920 --> 00:16:00,100
step the existing to start transition and then take an equal to two and equal to one or the other way around and so on,

118
00:16:00,120 --> 00:16:09,450
so forth. So the core mineral equation basically tells you from a one step transition from this transition kernel,

119
00:16:09,720 --> 00:16:13,200
what we're saying, we can get arbitrary centralization kernel.

120
00:16:13,410 --> 00:16:21,180
So that's one. And the other thing is the connection, the this.

121
00:16:23,880 --> 00:16:31,110
The similarity between the Chapman core group equation and then the matrix multiplication.

122
00:16:32,010 --> 00:16:40,660
So we can but we don't really have a matrix transition matrix unless we have finite things to be smart culture.

123
00:16:41,100 --> 00:17:01,620
We talk about philosophy. So the property one is a generalization of this issue.

124
00:17:09,000 --> 00:17:27,120
So the second thing we're going to do is there's a corollary for a finite state Markov chain.

125
00:17:28,200 --> 00:17:42,060
And I'm. So we can if it's a finite state Markov chain, then we can arrange anything and start a transition or step into a matrix.

126
00:17:42,750 --> 00:17:46,049
That's not a very difficult argument to make. Right.

127
00:17:46,050 --> 00:17:54,390
So if you're start with state zero, then you need to end up in the all possible finite state.

128
00:17:54,720 --> 00:17:58,500
So that's the best theory of first row. This is your second row.

129
00:17:58,500 --> 00:18:03,569
And to say anything about say everything about what the probability is that if you

130
00:18:03,570 --> 00:18:10,560
start from State one and the destinations are all five of the old finite states,

131
00:18:10,920 --> 00:18:14,880
states in the state space, so on and so forth.

132
00:18:15,180 --> 00:18:24,480
So we can always organize these and stack transition.

133
00:18:25,760 --> 00:18:34,930
Col into a matrix just using the on the layout I just described.

134
00:18:34,940 --> 00:18:40,430
The first row is the state zero to the subsequent states.

135
00:18:41,510 --> 00:18:45,820
The second row is the state one to the subsequent state.

136
00:18:45,840 --> 00:18:57,890
So. So this statement is always true. So like a P.A., it's an step.

137
00:18:58,910 --> 00:19:15,970
In this case, it's transition matrix. Then it must be true.

138
00:19:16,790 --> 00:19:26,120
The P and plus and equals P and multiply right here.

139
00:19:27,500 --> 00:19:34,790
All right. So what the difference here is, I'm talking about matrix multiplication.

140
00:19:36,470 --> 00:19:39,860
Right. Let me just write those in the more obvious way.

141
00:19:40,670 --> 00:19:49,390
So if you have. If ever used that transition matrix for a finite state space mark,

142
00:19:49,750 --> 00:19:56,320
then you get a matrix form of Chapman from across the equation in this matrix form as well.

143
00:19:56,740 --> 00:20:07,390
So this matrix, the left hand side is an placeholders, that transition matrix for that very Markov chain, and then they can be computed by.

144
00:20:07,900 --> 00:20:15,670
PND Is that transition matrix that's a that transition matrix by just doing the multiplication.

145
00:20:16,540 --> 00:20:26,080
All right. So the proof is really simple because the form of the the Chapman for growth equation if your have K is finite.

146
00:20:27,220 --> 00:20:31,570
Right. So the K in this case is going to be finite, going to do the finite sum here.

147
00:20:31,990 --> 00:20:37,450
You just need to show OC 1/1 thing the the peak.

148
00:20:39,070 --> 00:20:44,120
So I k so I don't need to do that.

149
00:20:44,120 --> 00:21:00,910
So for this office. So the p like and remember this is this is the element in these I'm plus in transition matrix.

150
00:21:01,870 --> 00:21:12,850
So you just need to realize is e. I throw up in parliament?

151
00:21:15,340 --> 00:21:18,640
It's called Parliament.

152
00:21:21,430 --> 00:21:28,600
And he was right.

153
00:21:28,990 --> 00:21:36,640
So if you realize this saying so, the plea from I took according to the connection I just described this as true.

154
00:21:37,360 --> 00:21:40,930
And the same thing for the the key one.

155
00:21:40,980 --> 00:21:44,410
What you need is p j. Okay.

156
00:21:46,360 --> 00:21:51,130
So for the pic j, so on, so forth, they are all matrix elements.

157
00:21:51,370 --> 00:21:55,650
Now if you're fine, you're starting over. You're just involved.

158
00:21:55,660 --> 00:22:01,120
Chatman for one equation. All of a sudden they're responding to a matrix multiplication.

159
00:22:01,130 --> 00:22:05,650
Therefore we don't have a finite state space Markov chain.

160
00:22:05,660 --> 00:22:10,390
But that's the consequence of it. Yeah. So it's gonna be a matrix multiplication.

161
00:22:11,950 --> 00:22:16,050
Any questions? Good.

162
00:22:16,090 --> 00:22:19,700
But you can only do this for finite state space.

163
00:22:19,710 --> 00:22:23,220
Mark Right. So you cannot do this for countable.

164
00:22:23,250 --> 00:22:32,310
If my girlfriend has a comfortable, safe space, then I'm comfortable imagining that this won't work.

165
00:22:33,540 --> 00:22:43,410
The reason this the matrix multiplication doesn't really expand to the next step in space.

166
00:22:46,110 --> 00:22:56,760
All right. So another sort of a this is a I don't think it's worth a theorem, but another corollary.

167
00:22:57,780 --> 00:23:02,370
So if you have a finite state space, mark off and then you only want to calculate.

168
00:23:03,090 --> 00:23:08,230
If you want everyone to calculate, what is that position kernel?

169
00:23:08,610 --> 00:23:12,090
What you can do is the following.

170
00:23:12,390 --> 00:23:22,950
So the P in the end is just the matrix multiplied by itself and times for like a power matrix.

171
00:23:23,100 --> 00:23:27,780
So that's well defined, right? For that those matrix are.

172
00:23:30,180 --> 00:23:34,740
So first of all, for finite state space matrix, there has to be a square matrix, right?

173
00:23:35,130 --> 00:23:40,890
If a is that the states has to be an entire matrix.

174
00:23:41,250 --> 00:23:51,590
So these things are well defined. So you can just multiply to itself and times and then you get the spectral division kernel.

175
00:23:53,010 --> 00:23:59,370
Well, we can prove by induction, but I think it's pretty obvious from the above.

176
00:24:02,240 --> 00:24:08,720
Right. So we're going to talk about Chapman's former groovy creation in multiple occasions.

177
00:24:08,750 --> 00:24:19,070
Whenever we change the scenery for different types of Markov chains, so continuous time for continuous state space.

178
00:24:19,340 --> 00:24:23,840
But they all have the same implication.

179
00:24:23,840 --> 00:24:28,790
That is, if you know the one, it's sufficient to define a one step transition.

180
00:24:29,240 --> 00:24:39,760
And then you can generalize that once that transition to arbitrary step transition, um, you know, utilizing ambiguity here,

181
00:24:39,770 --> 00:24:46,070
the key thing is actually utilizing the Marcos property without mark of property, the generalization won't be true.

182
00:24:46,480 --> 00:24:51,340
Okay. So why this is useful.

183
00:24:51,350 --> 00:24:57,589
So this without discussing anything, the special properties of the states,

184
00:24:57,590 --> 00:25:07,490
the nature of the the transitions here as a general view from the stochastic process,

185
00:25:07,970 --> 00:25:19,310
given that the Chapman format will be present, we can already do something to understand this the underlying Markov chain, right?

186
00:25:20,060 --> 00:25:32,020
So this is a collection of random variables, assuming you could you only, you know, you know, the so-called initial distribution of the rough chain.

187
00:25:32,030 --> 00:25:35,690
That is how you mark the starting point.

188
00:25:36,410 --> 00:25:42,830
So the that's really talking about the the distribution of X zero.

189
00:25:44,840 --> 00:25:54,860
So we're going to have this concept of initial distribution.

190
00:26:00,780 --> 00:26:16,620
So the initial distribution is just fancy name for distribution off of one particular read the variables of the distribution of zero.

191
00:26:17,400 --> 00:26:25,000
So how you start your Markov chain? Because every things are correlated, right?

192
00:26:25,020 --> 00:26:31,890
So if you're want to know every other states, you need to know how the chain actually started.

193
00:26:31,920 --> 00:26:35,280
So we call this this initial state distribution.

194
00:26:35,740 --> 00:26:40,590
And this is it has this property because this finite state space.

195
00:26:40,830 --> 00:26:48,000
Sorry. It's a disgrace that space takes a zero equal to, I think, that kind of guy.

196
00:26:48,260 --> 00:26:56,280
Right. So that's sufficient. He finds all these obviously all five going to be between zero and one.

197
00:26:56,700 --> 00:27:01,679
If you knew some overall positive possible alpha values, they add up to one.

198
00:27:01,680 --> 00:27:05,340
So that make it a valid probability distribution.

199
00:27:05,370 --> 00:27:11,580
But nothing more than that. So if you have this initial state distribution.

200
00:27:13,230 --> 00:27:23,070
And then, you know, the transition colonel. Right. So this is known as the initial state distribution.

201
00:27:23,070 --> 00:27:27,030
Okay. So the apply now we know two things.

202
00:27:27,840 --> 00:27:33,780
One is alpha, right?

203
00:27:34,170 --> 00:27:37,470
Also for the Markoff chain, the supply chain.

204
00:27:39,390 --> 00:27:50,370
Right. So that's the one step transition kernel with this is with this to high of quantities,

205
00:27:51,390 --> 00:28:01,020
we are able to figure out the marginal distribution of any number of variables in the collection in the Markov chain.

206
00:28:02,310 --> 00:28:18,120
How. So let's say we want to compute probability X and equal to j.

207
00:28:20,490 --> 00:28:30,000
So the claim is with this two quantities, we can't figure out anything for arbitrary and for arbitrary j that is valid for our values.

208
00:28:31,830 --> 00:28:41,710
Okay. So. Again, probability or just expectation.

209
00:28:41,850 --> 00:29:04,210
So we had you the total expectation of all of them.

210
00:29:06,190 --> 00:29:10,480
So that quantity equals expected value.

211
00:29:12,950 --> 00:29:19,810
Right this way. If I say J, I'm going to conditional on x zero.

212
00:29:23,680 --> 00:29:27,579
Right. Fair enough. So that is the expectation.

213
00:29:27,580 --> 00:29:31,000
That's a probability of trying to calculate. But there's always can be a reason.

214
00:29:31,000 --> 00:29:39,390
That's a decent one. So this is all required. Now it's just taking the expectation with respect to act x and zero.

215
00:29:39,400 --> 00:29:42,790
So X zero take different values.

216
00:29:43,330 --> 00:29:50,020
Let's say it's this is going to be I so will be x0.

217
00:29:50,590 --> 00:30:00,970
I multiply by the probability, a conditional probability at zero by.

218
00:30:01,270 --> 00:30:08,690
Yeah. So this is a precise representation of this and total expectation.

219
00:30:10,600 --> 00:30:16,690
Now you see, this part can come from the initial state distribution.

220
00:30:17,500 --> 00:30:27,690
And then this is nothing more than an exact transition from the mark, which we can derive from one step transitions.

221
00:30:27,730 --> 00:30:37,240
But at the end of the day this shows this is often I am happy i j and steps.

222
00:30:38,450 --> 00:30:41,290
Okay. So this is the answer.

223
00:30:49,870 --> 00:30:59,439
This may feel trivial later on if you're trying to do this sort of a thing, trying to figure out the marginal distributions for,

224
00:30:59,440 --> 00:31:06,130
oh, the number of random variables, it can become more complicated, more challenging.

225
00:31:06,430 --> 00:31:10,150
In other contexts, we can get this simple representation.

226
00:31:10,510 --> 00:31:16,330
Or if the simple expression is just because this is really a consequence of Markov chain.

227
00:31:18,760 --> 00:31:26,710
Yes, we have we have already said that that transition can be derived from being a single step transition.

228
00:31:28,140 --> 00:31:35,630
All right. So well, I think we are have already achieved a quite a bit here, even though we haven't,

229
00:31:35,910 --> 00:31:42,600
you know, into the details about a lot of fascinating facts about Markov chain.

230
00:31:42,600 --> 00:31:45,659
But so this is the fundamental thing.

231
00:31:45,660 --> 00:31:49,980
If you gave me a transition kernel, gave me an initial state, I can have this.

232
00:31:50,400 --> 00:31:56,580
So some of the interesting question is also affiliated with this type of the equation.

233
00:31:57,210 --> 00:32:10,860
So let's say if I run the Markov chain long enough, meaning if I take and goes to infinity, is there a limit for these marginal distribution?

234
00:32:11,100 --> 00:32:15,420
Then you go to J. If there is a limit, there is a mathematical limit.

235
00:32:15,840 --> 00:32:17,970
What does that even mean? Right.

236
00:32:18,720 --> 00:32:29,760
So we're going to discuss this in the following nature, but that sort of thing we're going to be interested in essentially Markov chain is a simple,

237
00:32:30,420 --> 00:32:41,430
stochastic well, simple in the sentence is is correlated, but the dependent structure is relatively straightforward, like I should say specified.

238
00:32:41,850 --> 00:32:45,720
Now if you run the Markov chain long enough, what can that happen?

239
00:32:46,200 --> 00:32:53,000
Right. Sometimes you can relate this to the question of right feature model.

240
00:32:53,040 --> 00:32:57,390
So this is a kind of an example without or with mutation.

241
00:32:57,750 --> 00:33:03,840
So without mutation is seemingly saying some of these states where if you run long enough,

242
00:33:03,840 --> 00:33:14,490
if the accepted remember is is one of the old in the one type of the Leo in the population it's got two.

243
00:33:14,790 --> 00:33:24,500
So if it's a right fisher model without mutation so I'd say equal to J if J is not zero, I'm going to be zero.

244
00:33:24,720 --> 00:33:32,670
Right. So there is no possibility of this. It's a drift eventually get into the absorbing state that seemingly is a intuition.

245
00:33:33,060 --> 00:33:39,780
Is that true? I need to prove that's becomes less straightforward.

246
00:33:40,260 --> 00:33:43,710
And if with some mutation, if you introduce a mutation,

247
00:33:44,160 --> 00:33:50,130
what is that long term probability x that you put to J so you can calculate how many

248
00:33:50,280 --> 00:33:56,249
copies will remain in the population if you take if you run the them long enough.

249
00:33:56,250 --> 00:34:09,580
Meaning if the generation. If you have a lot of generations long enough, why this mutation and an inheritance.

250
00:34:12,760 --> 00:34:19,480
Process is going on forever. So that sort of question we need to discuss next.

251
00:34:20,800 --> 00:34:24,880
But if you have a bar code, that's for something you can do.

252
00:34:25,090 --> 00:34:30,610
So in the homework problem. There is a finite state space, Marcos chain problem.

253
00:34:31,120 --> 00:34:35,170
It requires you to do something similar to this.

254
00:34:35,440 --> 00:34:38,710
Take this into the matrix for writing for finite state.

255
00:34:39,250 --> 00:34:43,630
Again, this one looks like a matrix multiplication, right?

256
00:34:43,660 --> 00:34:49,150
It's probably one is the. It looks like a vector multiplied by a matrix.

257
00:34:49,240 --> 00:34:55,180
So. Or there's a a by one by one.

258
00:34:55,900 --> 00:34:59,620
I missed the number of the states by one. That's the initial state.

259
00:34:59,620 --> 00:35:04,690
Distributions look like a vector or a single column matrix multiplied by our transition.

260
00:35:05,080 --> 00:35:08,650
So you can write this into the matrix form as well.

261
00:35:09,250 --> 00:35:15,700
And then there are then you can ask the similar questions if you multiply this multiple times.

262
00:35:15,760 --> 00:35:24,220
What going to happen here? Okay. Do you see a so-called stationary distribution emerge from this operation?

263
00:35:26,460 --> 00:35:36,300
So far, he's just observation. Later on, we can determine that by inspect some of the properties of the one state in transition.

264
00:35:38,240 --> 00:35:42,860
Okay. Any questions before we move on to the next topic?

265
00:35:45,550 --> 00:36:03,640
Okay. So actually I can't give out the the answer to the question.

266
00:36:05,110 --> 00:36:13,600
So everything. The after the you know, the next few weeks, the conclusion will be very simple.

267
00:36:13,660 --> 00:36:18,790
Everything about the Markov chain is going to be determined by this transition kernel,

268
00:36:18,790 --> 00:36:24,790
especially the long term behavior is going to be determined by the initial state.

269
00:36:25,270 --> 00:36:29,350
Sorry, nothing is going to be determined by the transition kernel.

270
00:36:31,150 --> 00:36:34,780
The other thing is from that equation just erased.

271
00:36:34,780 --> 00:36:49,120
It seems like the initial state transition, the initial state distribution plays a fundamental role in the for the X and that means

272
00:36:49,120 --> 00:36:55,450
the after that the the distribution at the time and the random variable at times.

273
00:36:56,580 --> 00:37:04,780
There are also kind of a property we going to learn that Exxon's distribution can be well.

274
00:37:05,170 --> 00:37:12,970
In fact, it's is always going to be correlated with axis zero, but the impact can be smaller and smaller.

275
00:37:13,390 --> 00:37:19,910
Right. You can imagine that. It's all depends on the transition.

276
00:37:22,420 --> 00:37:33,400
So really, if we study Piaget, we going to know everything about the Markov chain for certain kinds of marks, which is obviously for all of them.

277
00:37:34,870 --> 00:37:47,890
And then we can take advantage of that tool for statistics to do probability modeling, to do stochastic calculations, taking advantage of the feature.

278
00:37:48,550 --> 00:37:56,170
All right. So we're going to get there. Before that, we're going to discuss the classification of space.

279
00:37:57,820 --> 00:38:06,370
So that's the next lecture. So classification of the states and the idea that is not all the states are the same.

280
00:38:06,820 --> 00:38:13,629
Right. So obviously we have already seen in the right fisher model there are sloping states that's very different than all

281
00:38:13,630 --> 00:38:22,420
the other states in the middle right is a zero states and then a state and that's different than all the others.

282
00:38:23,260 --> 00:38:33,130
There's just some connection properties between the states and then you can always ask the question like what to do next.

283
00:38:33,460 --> 00:38:45,880
But before that, we're going to discuss another important terminology slash concept starting time.

284
00:38:49,690 --> 00:38:59,330
A strong mark off the property. So this is not just Markov property.

285
00:38:59,340 --> 00:39:18,139
This is a strong Markov property. So we're going to define stopping time first and then starting time.

286
00:39:18,140 --> 00:39:36,340
It's always it's a random time defined with respect, a stochastic process, not necessarily Markov chain, but so what is a solving?

287
00:39:38,670 --> 00:39:51,079
Okay, so we need to as I said, this is stopping time is is a random time associated with a stochastic process.

288
00:39:51,080 --> 00:39:54,110
So we need a first time forecasting process.

289
00:39:54,110 --> 00:40:00,260
In this case, we can see a discrete time stochastic process.

290
00:40:04,590 --> 00:40:23,670
He is stochastic and means that a starving time.

291
00:40:25,350 --> 00:40:32,970
If you have this because that the property of the starting time cannot be defined through this underlying stochastic process.

292
00:40:32,980 --> 00:40:43,830
Okay, so you cannot talking about solving time with all without a associated stochastic process.

293
00:40:44,370 --> 00:40:57,899
So solving time, first of all, is a random variable. This runs in my original question here we say support.

294
00:40:57,900 --> 00:41:12,270
So that's the possible values is support is the Z plus you in and it takes all of these integer values that's basically

295
00:41:12,270 --> 00:41:21,480
the in the index on this one has a process can take you know addition to infinity there is an infinity you can take.

296
00:41:26,880 --> 00:41:37,410
All right. So in this case, it's just a random time, but affiliated with a stochastic prosecco, affiliated with a discrete time stochastic process.

297
00:41:38,130 --> 00:41:42,090
But this is not just any random time.

298
00:41:42,420 --> 00:42:03,060
It's the call her. You find this through the following sentence such that for every if you give me for every am for any finite values in this support,

299
00:42:03,900 --> 00:42:13,080
then the event there is a balance right t equals m.

300
00:42:14,130 --> 00:42:22,440
So I'm just saying that stopping time takes particular value is determined.

301
00:42:22,440 --> 00:42:41,910
So this is a kind of a vague mathematical language because we're not using the most rigorous definition here is determined by the values of.

302
00:42:45,590 --> 00:42:50,770
x0x1 up 2xm.

303
00:42:54,100 --> 00:43:00,260
All right. So. Hopefully this is not.

304
00:43:02,590 --> 00:43:09,550
A difficult definition to read. The point here is, first of all, the stopping time is a random time.

305
00:43:09,970 --> 00:43:13,270
So we take all these integer values, the indices.

306
00:43:14,350 --> 00:43:20,229
Secondly, the most important thing is the event tuples.

307
00:43:20,230 --> 00:43:27,220
N. So if you're like me, make a determination t equals m is true or false.

308
00:43:28,120 --> 00:43:36,160
All I need to do is looking at the values of the realization of certain realizations from the stochastic process,

309
00:43:37,120 --> 00:43:48,190
from the value is from x0 all the way up to that event, the time event in the indicator, that's the exam.

310
00:43:48,580 --> 00:43:53,650
So all the values up to x am going to determine the event.

311
00:43:53,800 --> 00:43:59,980
T equals m is truth. Right. This is a more specific than the statement here.

312
00:44:00,520 --> 00:44:06,850
So you need to make a judgment by looking at is null saying so in the way it says

313
00:44:08,170 --> 00:44:22,920
these events the 0x1 should be able to tell you if Jesus m is right or wrong.

314
00:44:23,290 --> 00:44:30,820
So if you think about all these events, it can represent that this part which is T equals AB is a part of it.

315
00:44:30,970 --> 00:44:34,840
So it can, you know, they usually don't write this way.

316
00:44:35,800 --> 00:44:40,660
Okay. So because they are not in the same sort of the sample space.

317
00:44:41,020 --> 00:44:46,370
But the idea is this C is determined by that.

318
00:44:47,500 --> 00:44:58,960
If you can write in the more general sample space, put these two things together, then, you know, this is this is a more information.

319
00:44:59,800 --> 00:45:05,080
This is a less information. So more information can determine less information.

320
00:45:05,260 --> 00:45:08,830
Write more of this. That's the idea.

321
00:45:10,570 --> 00:45:26,850
All right. So a lot of the random time we have considered are, you know, are indeed stopping time examples.

322
00:45:27,580 --> 00:45:36,730
So so in the Templars ruling or in the simple random walk and we talk about the first the passage time.

323
00:45:42,580 --> 00:45:46,670
So we know the underlying stochastic process is a markov chain.

324
00:45:46,670 --> 00:45:50,680
It doesn't really matter. There is a stochastic process if you know.

325
00:45:51,430 --> 00:45:56,620
So that's how we use Tao is the first time.

326
00:45:58,510 --> 00:46:02,830
Think how. It's just how. Okay, it's the first time.

327
00:46:05,370 --> 00:46:08,650
An excellent one.

328
00:46:10,240 --> 00:46:21,070
Right. So that's the first the passage time to the upper barrier and this is how it's a stalling time.

329
00:46:21,610 --> 00:46:31,600
Why? Because we can check this definition. So first of all, is a random time defined in the integer, not an active integer valued.

330
00:46:34,710 --> 00:46:38,460
It's a non-negative integer value, not an active integer.

331
00:46:40,830 --> 00:46:45,690
And furthermore, it satisfies the the condition.

332
00:46:46,720 --> 00:46:52,150
So if we were trying to help, it's how I remember.

333
00:46:52,420 --> 00:47:01,960
I say, How are you going to keep? All we need is the realization of x1x2 up to x, right?

334
00:47:04,960 --> 00:47:09,250
You can tell if this is true or false if you have this information.

335
00:47:09,580 --> 00:47:20,620
I see this the excuse the first time and then the only time the the random walk actually reaches to the the target one.

336
00:47:20,860 --> 00:47:30,180
Right. I need all this information from X0 two or all the way up to access.

337
00:47:30,910 --> 00:47:34,410
I need all this information to determine these two events.

338
00:47:35,160 --> 00:47:44,820
If this particular event, the statement about the random about the stopping time is true or false, it looks trivial.

339
00:47:45,090 --> 00:47:54,660
You may think every single of a lot of random times are going to be starving time.

340
00:47:56,610 --> 00:48:02,790
That's not a wrong impression. A lot of the run time we discussed all have this property.

341
00:48:03,000 --> 00:48:09,330
But it would be wrong to say all the random times are stopping time.

342
00:48:09,750 --> 00:48:15,000
Here is a counterexample. Say counterexample.

343
00:48:20,780 --> 00:48:32,970
Okay. I just need to. So I'm going to steal associate random time with this random walk and then to define the last time.

344
00:48:45,670 --> 00:48:48,999
Call this, we have a first passage time.

345
00:48:49,000 --> 00:48:52,510
I just you know, you can make something like a call.

346
00:48:52,510 --> 00:48:58,160
The loss of part time losses.

347
00:48:58,670 --> 00:49:07,950
The passage of time for. For the random walk to cross.

348
00:49:07,990 --> 00:49:11,030
Let's say only you quote 100.

349
00:49:11,900 --> 00:49:16,340
For. Right.

350
00:49:16,610 --> 00:49:23,650
So this is pretty arbitrary. Don't worry about this that exists or not.

351
00:49:24,100 --> 00:49:28,180
That's not the question. I can certainly define things like that.

352
00:49:28,540 --> 00:49:33,490
Right. So that's it's a random time. Right. So this is last passage time.

353
00:49:33,940 --> 00:49:42,089
It's defined for. The underlying rhythm walk, the simple rhythm one.

354
00:49:42,090 --> 00:49:51,810
But I'm asking, is this the last time the random walk ever cross the barrier, which is 100?

355
00:49:53,130 --> 00:49:56,250
So the question is, is this a stopping time?

356
00:49:57,990 --> 00:50:02,630
Right. So this is a sort a random time, but it's very easy to see.

357
00:50:02,640 --> 00:50:13,560
That's not that is not a stopping time, because, again, you go back to what you do, doesn't have a notation for that.

358
00:50:13,590 --> 00:50:19,530
Let's say this is lambda. Lambda you call to.

359
00:50:21,740 --> 00:50:25,670
He is that determined?

360
00:50:25,800 --> 00:50:31,610
I say you go to one so you can basically put any value on here.

361
00:50:32,030 --> 00:50:37,160
It's going to be determined by the x1x2 all the way up to.

362
00:50:41,420 --> 00:50:55,230
XD you can only by definition you can only have finite step information up to the statement here in dictated.

363
00:50:55,610 --> 00:51:01,190
That's Islam equal to tea. Is this enough information for you to determine what you find is true or false?

364
00:51:02,500 --> 00:51:12,220
Obviously not, because that's the last time you definitely need all of the subsequent realization of the stochastic process.

365
00:51:13,120 --> 00:51:21,729
Right. So that's the key. So the time all the way out, the one to t is insufficient to determine the lambda.

366
00:51:21,730 --> 00:51:25,510
You put the T in this case the lost passage time. It's true or false.

367
00:51:25,960 --> 00:51:31,910
So therefore, that's a contradiction to the definition of a stopping time.

368
00:51:31,930 --> 00:51:36,730
So in this case, this last passage, time is not a stopping time.

369
00:51:37,420 --> 00:51:41,200
Okay. So that's very important concept.

370
00:51:41,650 --> 00:51:48,880
This is almost saying make you think every random time is stopping time, but in fact, it's not.

371
00:51:49,450 --> 00:51:53,919
But you could easily check the definition by just using this type of example,

372
00:51:53,920 --> 00:52:03,190
because in the definition that determines you just means if this particular event can determine the true or false,

373
00:52:03,460 --> 00:52:08,020
then the fact about this statement equal to or something.

374
00:52:08,260 --> 00:52:11,440
Okay. So that's the definition. Just stick to the definition.

375
00:52:14,910 --> 00:52:18,570
Okay. Now we know what is a starving time.

376
00:52:19,470 --> 00:52:28,060
Why is that important? Because we're going to have a different version of the Markle provocation.

377
00:52:28,120 --> 00:52:33,540
It's not one that's the strong Markle property of you have to you know,

378
00:52:33,720 --> 00:52:41,100
we have to appreciate that this is fundamentally different than the Markle property.

379
00:52:41,100 --> 00:52:49,620
We've just discussed the second strong Markle provocation.

380
00:53:03,480 --> 00:53:09,690
So the strong Markle of property is something more than the regular Markle property.

381
00:53:09,690 --> 00:53:14,580
That's the idea. But how strong? Strong means more requirement.

382
00:53:16,470 --> 00:53:33,330
So this strong Markle problem is expected to stop any time to be a sporting kind of wasters.

383
00:53:33,330 --> 00:53:44,490
Back to an ABC Markov chain and son here.

384
00:53:46,860 --> 00:53:50,280
All right, so then it follows so strong.

385
00:53:50,700 --> 00:53:56,010
So right there are Markov property is talking about conditional independence relationship.

386
00:53:56,460 --> 00:54:00,480
The same for the strong Markle property. It follows.

387
00:54:01,570 --> 00:54:21,370
This is. We're going to have x, t plus one, x, t plus two, and so on and so forth.

388
00:54:21,430 --> 00:54:34,900
So this is a you know, I mean, first and right now this condition is independent of x P, minus one, x, t minus two, all the way up to at zero.

389
00:54:38,350 --> 00:54:47,350
Conditional on what information? Conditional on x, t and the event and t equals m.

390
00:54:51,130 --> 00:54:56,760
All right. So if it's a markov chain, this is has a lot of similarity, right?

391
00:54:56,770 --> 00:55:02,620
So the x t is present and then this is the past.

392
00:55:03,160 --> 00:55:15,250
Everything happened before x t, everything happened after x, after the time t is in the in the front.

393
00:55:15,670 --> 00:55:19,990
And then everything happens before the time T is.

394
00:55:20,170 --> 00:55:23,410
And so this looks like just Markov property.

395
00:55:24,980 --> 00:55:31,070
Except the T here is a random time, so you have to appreciate that.

396
00:55:31,070 --> 00:55:36,530
So this is not the same thing as Markov property, Markov property.

397
00:55:36,530 --> 00:55:41,600
You can talk about this for any fix the time or any preset like the time.

398
00:55:42,140 --> 00:55:51,560
But this one, if you read the notation, nothing is determined for x, t plus one if you don't know anything about the t, right?

399
00:55:53,780 --> 00:55:58,700
So this may if I tell you t pause thus makes it's trivial.

400
00:55:58,910 --> 00:56:10,310
Yes, it is tribute to true. This is true. But you still need to realize this is not the same thing as just the right area.

401
00:56:11,090 --> 00:56:24,919
Markov property is has a stronger requirement to work condition put in the T for example has to be a starting time and then if it's not stopping time,

402
00:56:24,920 --> 00:56:28,340
you can use the last passage time to try. That won't be true.

403
00:56:28,610 --> 00:56:33,710
This whole statement won't be true. And this is like a whole one.

404
00:56:33,920 --> 00:56:56,080
We can't prove this is true if he is. If he is already a starving time, then you can say the regular Markov property will reduce the strong mark.

405
00:56:56,540 --> 00:57:00,850
But if he is a starving kind.

406
00:57:03,850 --> 00:57:17,500
So we can show. Show regular markup of property.

407
00:57:22,880 --> 00:57:28,140
So it's a consequence. You don't need to do it every time you see a mark off chain.

408
00:57:28,160 --> 00:57:33,170
This is always true. But the key is you have to check the T here.

409
00:57:33,440 --> 00:57:46,550
The random time is a stopping time induces strong mark.

410
00:57:56,160 --> 00:58:09,580
Sun. So how do we do that?

411
00:58:09,680 --> 00:58:14,540
It actually is pretty trivial. So what we need to do is for a time, well,

412
00:58:14,540 --> 00:58:31,249
we can just keep the time homogeneous Markov chain and then try to prove this simple thing here x plus one equals one j and additional t equals mx0.

413
00:58:31,250 --> 00:58:40,100
You consume i0xyi1 all the way up to an equal to I.

414
00:58:41,570 --> 00:58:48,020
Right. But if we can't figure out this statement, but just look at this statement.

415
00:58:48,320 --> 00:58:51,770
This is already very different than that regular Markov chain.

416
00:58:51,950 --> 00:58:56,420
In the regular Markov chain. You don't have this T equals and here.

417
00:58:57,140 --> 00:59:06,590
Right. Because I'm going to be a face the number so you don't but the in the front T plus one equals J.

418
00:59:06,800 --> 00:59:10,580
That is a very complicated expression, right?

419
00:59:10,910 --> 00:59:15,680
Not only the value itself, the x values are random.

420
00:59:16,340 --> 00:59:19,940
The in this the index is also random.

421
00:59:20,150 --> 00:59:23,150
So think about like component variable.

422
00:59:23,150 --> 00:59:38,090
That's not trivial to make this statement, but because of the Markov property and because T is a is a is a solving time,

423
00:59:38,420 --> 00:59:48,250
we know this information T plus m is already encoded in this x zero equal to zero all the way up to excitement y.

424
00:59:48,260 --> 00:59:51,920
That's the key thing, the essence of the stopping time.

425
00:59:52,400 --> 01:00:01,550
So there are a few things. If you're already conditional on T equals M, then you can write this one as x and plus one.

426
01:00:02,330 --> 01:00:15,860
Okay, conditional off equals m x is 00x1 line one all the way up to maximum right note.

427
01:00:16,100 --> 01:00:22,520
If so, this you can do for even the t is not a solving time like last passage time.

428
01:00:22,520 --> 01:00:29,540
If I tell you the information T equals m, you can always do this substitution right because you are the conditional.

429
01:00:30,740 --> 01:00:35,870
However, even this statement is not the same as the regular Markov property.

430
01:00:36,320 --> 01:00:42,050
Right. You cannot evoke like here. Just say this is because of a mark of property.

431
01:00:42,350 --> 01:00:45,990
This is a pig. This is not your condition.

432
01:00:46,070 --> 01:00:48,350
On more things. Usually things change.

433
01:00:48,920 --> 01:00:57,860
However, because of the definition of stopping time, A says the tuples M can be fully determined by all the information.

434
01:00:57,860 --> 01:01:13,010
Here is this it becomes a sub event of all the things here that because so this is really because of the definition of starting time.

435
01:01:19,390 --> 01:01:23,300
It basically means you force this particular event.

436
01:01:23,320 --> 01:01:26,500
It's a redundant from the air conditioning so.

437
01:01:28,780 --> 01:02:00,080
That. Tells you that the probability of a conditional probability you're considering is just the same as X is zero.

438
01:02:00,090 --> 01:02:07,920
You see all the way up to Act II, and in this case, it's just a chain.

439
01:02:10,470 --> 01:02:16,680
Right? But the logical link cannot be missed here.

440
01:02:17,020 --> 01:02:24,990
Right? So every statement, every expression is different from the first line to the second line we take.

441
01:02:25,350 --> 01:02:30,930
That's true for arbitrary stopping time. From the second line to the third line, there has to be.

442
01:02:31,950 --> 01:02:38,010
There's a strong requirement that he has to be the stalking type that we got here.

443
01:02:39,660 --> 01:02:49,500
And. All right, so probably with all this kind of a discussion,

444
01:02:50,910 --> 01:03:01,590
we just want to say just one point so that not only mark of property is truly or any fixed the time arbitrary.

445
01:03:01,860 --> 01:03:13,740
So like the time is also true. You have also have a strong mark of property with respect to this for a random stopping time.

446
01:03:13,830 --> 01:03:22,920
If that random stopping time that if that random time is happened to be the stopping time, then you can also say percent future independent.

447
01:03:25,320 --> 01:03:28,320
The consequence of this we're going to see probably.

448
01:03:29,630 --> 01:03:37,100
And on one stage. But we can think about the problem like these.

449
01:03:39,370 --> 01:03:43,510
So once white is strong, our property is relevant for our discussion.

450
01:03:51,520 --> 01:04:09,210
Okay. So so the implication of strong mark on property.

451
01:04:16,100 --> 01:04:19,530
So we can define f i.

452
01:04:20,090 --> 01:04:22,820
We're going to see that later on formally defined.

453
01:04:22,830 --> 01:04:38,420
That is on our return probability to state I so you can think about in the contacts of a random walk, right?

454
01:04:38,480 --> 01:04:43,370
So this is our return probability to the B uh, to the starting point to zero.

455
01:04:43,670 --> 01:04:46,829
What is the probability we set that in the random walk.

456
01:04:46,830 --> 01:04:53,620
That depends on the, the P and Q or if you've got a tendency of keeping up or down.

457
01:04:53,990 --> 01:04:59,569
So the I've, I here. So this is a return probability the state zero.

458
01:04:59,570 --> 01:05:05,060
In that case I have zero zero as may be one and then in some cases it's not one.

459
01:05:05,690 --> 01:05:08,930
Right? So this is a kind of interesting thing to compute.

460
01:05:12,200 --> 01:05:18,730
So formally this is but you can actually define this for arbitrary Markov chain for a state line.

461
01:05:18,860 --> 01:05:25,099
If you're starting from the state high, what is the probability the Markov chain will return back to the state?

462
01:05:25,100 --> 01:05:30,620
I mean, if you think of a finite state space versus a finite state space,

463
01:05:30,710 --> 01:05:39,560
that could have very different results later on in the scene, for example, you write Fisher model.

464
01:05:39,830 --> 01:05:48,080
If you start from a state, is that always so the probability of where returned to that state is always one?

465
01:05:48,560 --> 01:05:54,290
That's not necessarily true, right? Realizing there are two absorbing states you state.

466
01:05:56,180 --> 01:05:59,509
So this is a call, a return probability to to a state high.

467
01:05:59,510 --> 01:06:07,580
So you can define this for arbitrary states rather than re arbitrary margin.

468
01:06:09,770 --> 01:06:25,760
So but t here is the first possible time I have to state i.

469
01:06:28,140 --> 01:06:34,260
Okay. Obviously I need to define this because I'm trying to illustrate the t.

470
01:06:34,260 --> 01:06:38,700
Here is a starting time just like we discussed.

471
01:06:39,090 --> 01:06:47,300
Right? If you say something about t for the hundred percent, it can give me x zero all the way up to 600.

472
01:06:47,310 --> 01:06:51,060
I can determine that you find this true or false. All right.

473
01:06:51,060 --> 01:07:09,990
So what the relationship between the two so you have is f I define as the summation and you put one within the probability t equals an axis zero one.

474
01:07:13,530 --> 01:07:16,740
Right? So I'm going to start from at zero equal to I.

475
01:07:16,890 --> 01:07:25,260
And I'm trying to account for the fact that the Markov chain ever returned back to.

476
01:07:27,090 --> 01:07:37,050
The starting state I. Right. So if t equals one, t equals two accounts for the ever return probability I need to account for.

477
01:07:37,380 --> 01:07:43,230
The first return occurs in all possible time points therefore.

478
01:07:43,230 --> 01:07:50,219
And then they are mutually exclusive, right? Because the way the T is defined is defined at the first three term times.

479
01:07:50,220 --> 01:07:59,220
They're therefore mutually exclusive. If I add those up by give me the average term probabilities I write.

480
01:07:59,970 --> 01:08:03,330
So this is a definition and then this is the t y.

481
01:08:03,330 --> 01:08:12,750
The thing is you can ask for the probability for the first return probability that yeah.

482
01:08:12,750 --> 01:08:22,229
So you ask the question, well this random walk ever return back to the starting point.

483
01:08:22,230 --> 01:08:26,280
Let's say X zero. You can do I. Right.

484
01:08:26,320 --> 01:08:33,160
So that's sci fi. We know how to calculate that. And we did calculate that in simple random walk.

485
01:08:34,060 --> 01:08:38,940
Then the question that's a little bit different. What is the probability?

486
01:08:39,810 --> 01:08:46,110
So the queue here would require us to invoke strong Markov property as the following.

487
01:08:46,920 --> 01:08:53,790
What is the probability of?

488
01:08:55,330 --> 01:09:02,460
Of returning. Exactly.

489
01:09:04,800 --> 01:09:10,270
Exactly. And times. Right.

490
01:09:10,270 --> 01:09:14,380
If I ask, what is the return probability for us?

491
01:09:15,100 --> 01:09:19,440
What is the probability of returning one time? That is if I.

492
01:09:20,770 --> 01:09:29,360
What is the probability of a return? 11 times. Any intuitive answers.

493
01:09:33,860 --> 01:09:40,160
Hindus try to use try to use the for the strong mark of problem.

494
01:09:45,700 --> 01:09:48,780
Right. You don't really need to recalculate anything.

495
01:09:48,850 --> 01:09:53,470
I think. Well, calculating is challenging, right?

496
01:09:53,500 --> 01:10:06,010
As we've shown that hope calculate a whole bunch of the using the um the recursive relationship on the side of the equations.

497
01:10:06,400 --> 01:10:12,549
But there is a single argument, there is a simple argument to calculate the return times.

498
01:10:12,550 --> 01:10:26,980
Even though what I find is. So first of all, the argument is really simple.

499
01:10:27,460 --> 01:10:34,240
If you you know, the first the return the first, the return time is a starting time.

500
01:10:34,660 --> 01:10:40,540
Then by the definition of is won by the property of the strong mark.

501
01:10:40,690 --> 01:10:43,840
And then this on the right where it defined everything on the market.

502
01:10:44,050 --> 01:10:54,670
And then by the strong mark of property, every single returns cannot be independently mutually independent with each other.

503
01:10:54,680 --> 01:11:03,280
Right? So what does it mean? So if you have a return and then you restart this process, these two events are independent.

504
01:11:03,400 --> 01:11:08,170
So the next two return probability will also be I.

505
01:11:08,350 --> 01:11:11,760
Right? So it's the same problem here.

506
01:11:11,770 --> 01:11:14,590
The strong mark of property. Just tell you they're independent.

507
01:11:14,770 --> 01:11:22,780
And then the fact that they are the dynamics of where the set ups never change tells you they are also not just independent,

508
01:11:22,880 --> 01:11:26,020
they should also be identically distributed.

509
01:11:27,400 --> 01:11:35,290
So if the return times, if you're involved correctly evoke strong mark of property is very easy to copy.

510
01:11:37,570 --> 01:11:41,730
And. That's right. So that's the key thing.

511
01:11:43,020 --> 01:11:53,040
More or less you need this. I'm not quite sure if you don't use Markel appropriate a strong market property how you arrive to this type of the

512
01:11:53,040 --> 01:12:01,710
answer but I think implicitly you're going to recycle the argument for the strong markup property one way or the other.

513
01:12:02,430 --> 01:12:07,860
So this is the key answer. We're going to use this a lot in the next.

514
01:12:08,100 --> 01:12:14,600
So this seems like a natural point to start. This is an additional property for strong Markov.

515
01:12:15,060 --> 01:12:17,190
So for for for the market property next.

516
01:12:17,190 --> 01:12:25,440
So we're going to talk about understanding the structure of the Markov chain by just inspecting the ones that transition kernel.

517
01:12:25,860 --> 01:12:27,900
So the story is very simple.

518
01:12:28,470 --> 01:12:36,450
The one step transition kernel are going to tell us everything about the underlying Markov chain of some of these things are fascinating,

519
01:12:37,110 --> 01:12:40,320
but we will start talking about these on Wednesday.

520
01:12:40,680 --> 01:12:49,409
Okay. In the meantime, expect to turn the assignment to write again.

521
01:12:49,410 --> 01:12:54,540
You will have ten days of assignment three.

522
01:12:54,540 --> 01:12:58,550
You don't need to turn in. I forgot to say. Okay, guys, you are okay.

523
01:12:58,560 --> 01:13:02,850
You don't you don't need to turn. And it's just for you to write this yourself.

524
01:13:04,210 --> 01:13:07,440
And now we say the date, the number six.

525
01:13:09,090 --> 01:13:09,290
But.

