1
00:00:07,800 --> 00:00:12,780
Good afternoon, everybody. So why don't we get started? Hopefully you have, um.

2
00:00:14,220 --> 00:00:18,840
You have had the weekend week, week so far. And let's continue to talk about.

3
00:00:19,230 --> 00:00:29,310
653 And today we are going to cover 05b and C and we will start by finishing the final four slides of oh five.

4
00:00:29,760 --> 00:00:39,960
So just a recap of where we are now. We are modeling the mean and in the first approach we talk about is analysis of response profiles.

5
00:00:40,590 --> 00:00:51,030
And there we laid out two objectives. The first one is to describe the hypotheses that we usually can test using response, profile analyzes.

6
00:00:51,420 --> 00:00:56,610
And we emphasized that it is this interruption that we care the most about,

7
00:00:56,910 --> 00:01:01,140
i.e. if you compare two groups, whether the trajectories will be parallel or not,

8
00:01:01,410 --> 00:01:11,040
right, then we use the notation to represent these hypotheses and we introduce the cell encoding and the reference cell coding.

9
00:01:11,700 --> 00:01:19,320
And we will be analyzing a real data example using our code which is in the hand.

10
00:01:19,650 --> 00:01:26,130
I will cover that in detail. So why don't we continue to finish the objective number two?

11
00:01:26,490 --> 00:01:49,290
So I'm going to scroll down here. Yes.

12
00:01:49,290 --> 00:02:01,199
I believe this is where we stopped. So let me just start from those slip up once they before here we are for meeting the model using

13
00:02:01,200 --> 00:02:07,860
the reference cell coding and the ME model part again is generically written as EXI beta.

14
00:02:08,220 --> 00:02:13,560
And yesterday I received the wonderful question at LA Clase regarding how to interpret this matrix.

15
00:02:14,040 --> 00:02:18,540
So let me find my pen. Hopefully the previous lecture did not take it away.

16
00:02:19,850 --> 00:02:31,700
I very much hope that's not the case. That's going to present some challenge.

17
00:02:41,210 --> 00:02:45,470
Just bear with me for a moment. It's on the floor.

18
00:02:48,220 --> 00:02:54,130
Watch. Oh, there we go. Thank you. This is wonderful to point this out.

19
00:02:54,460 --> 00:03:03,450
Okay. All right. So here, let's illustrate for a person in the first group.

20
00:03:03,460 --> 00:03:08,820
Right. And in this toy example, we have an equals three occasions.

21
00:03:11,300 --> 00:03:14,210
And you need to recall that for this person.

22
00:03:14,510 --> 00:03:24,020
It has y a1 y2y a3 concatenated into a vector representing the information you collect about outcome at three occasions.

23
00:03:24,020 --> 00:03:31,780
Right. And correspondingly you will have information about x one excited to an x a3.

24
00:03:31,970 --> 00:03:36,110
Right. So this is the structure we are talking about.

25
00:03:36,350 --> 00:03:42,710
And xy1 represents the covered information you collected at occasion, one for the subject.

26
00:03:42,980 --> 00:03:47,150
And that can be many information. Right. So what are these information?

27
00:03:47,180 --> 00:03:52,160
So it turns out that we can consider six pieces of information.

28
00:03:52,760 --> 00:03:59,370
Machine learning people call them features. And other people call them independent variables, what have you.

29
00:03:59,760 --> 00:04:02,860
So here. That's the first occasion.

30
00:04:02,860 --> 00:04:09,000
You can ask, hey, whether this person you can ask me, what's the information here?

31
00:04:09,010 --> 00:04:15,969
So it's intercept. So it's just one. And also, you can ask whether this person is at the second occasion, clearly not,

32
00:04:15,970 --> 00:04:19,930
because we're looking at the first row and then moving on to the third entry here.

33
00:04:20,320 --> 00:04:24,070
You're asking whether this occasion is a third occasion. No, such a zero.

34
00:04:24,400 --> 00:04:28,620
And moving on whether this person is in group one. Yes.

35
00:04:28,710 --> 00:04:33,030
So it's a one. And then moving on, is this person in group one?

36
00:04:33,060 --> 00:04:37,230
Yes. Is this person also in at the occasion? Two unfortunate.

37
00:04:37,230 --> 00:04:42,590
So it's zero. Finally, is this person group one at occasions three.

38
00:04:42,740 --> 00:04:51,270
No. We are looking at row number one. So for the first occasion, you have gone through each question or each feature and then you can coat them.

39
00:04:51,630 --> 00:05:02,100
Similarly, you can do this for the remaining two occasions, and if you do this for the second group, you will get the binary design matrix here.

40
00:05:02,940 --> 00:05:08,010
So one big thing, one big difference between two is that these two design matrices are different.

41
00:05:08,490 --> 00:05:11,580
Right. Because of the group indicator here.

42
00:05:12,860 --> 00:05:17,210
And then we can just supply the beta vector, beta one better surface.

43
00:05:17,450 --> 00:05:26,150
And upon a matrix operation you can now predict each particular outcome using XY times beta.

44
00:05:28,860 --> 00:05:32,100
So then we can work out the actual numbers here.

45
00:05:33,090 --> 00:05:41,580
Here I am using a vector. I mean two to represent the three main responses at the three occasions for people in group two.

46
00:05:42,180 --> 00:05:45,360
Here, the view is not indexed by any particular subject.

47
00:05:45,960 --> 00:05:46,680
It's independent.

48
00:05:46,690 --> 00:05:57,480
By here we are making the assumption that we are interested in the mean in the group and in the second column here we are just writing them out.

49
00:05:57,660 --> 00:06:05,160
Right? And we use using the previous specification of design matrix, we can write out the actual numbers of these.

50
00:06:06,120 --> 00:06:10,020
We can repeat this calculation, the responses for the second for the first group.

51
00:06:10,200 --> 00:06:14,250
And we got a more complicated set of formula.

52
00:06:15,630 --> 00:06:17,400
Now, returning to the hypotheses,

53
00:06:17,400 --> 00:06:24,030
we were interested in asking whether the difference in the mean between the two groups at each of the occasions will be the same.

54
00:06:24,390 --> 00:06:34,020
So without any restriction, we just calculate difference by comparing the first beta one here and the beat one plus being a four here.

55
00:06:34,050 --> 00:06:44,010
So I have plotted on the right indicating the mean level beta one for the first group, for the second group in red.

56
00:06:44,160 --> 00:06:48,900
And then we have beta one plus beta for.

57
00:06:51,850 --> 00:06:53,920
For the first group and the first occasion.

58
00:06:54,280 --> 00:07:03,430
So if you can repeat the same procedure by mapping all of these dots, you can see the differences between the levels.

59
00:07:06,310 --> 00:07:10,480
Our premiere shows by Beta for Beautiful plus beta five and beta four plus beta six.

60
00:07:10,870 --> 00:07:18,280
So the null hypothesis that the two lines are parallel would correspond to beta five equals beta six equals zero.

61
00:07:19,060 --> 00:07:21,640
This is where we stopped in the previous lecture,

62
00:07:21,940 --> 00:07:28,800
and the reason why we introduced this notation is because it's much simpler to represent the null hypothesis.

63
00:07:31,810 --> 00:07:47,110
Okay. All right.

64
00:07:48,860 --> 00:07:54,590
Before I move on, I want to just ask you to do a quick exercise here.

65
00:07:55,130 --> 00:07:59,870
Not a hard one. Just a quick conceptual understanding.

66
00:08:01,550 --> 00:08:10,160
If I go from rule one to row two here. So two minus one will be what will be beta two, right?

67
00:08:10,790 --> 00:08:18,170
So this means the change from the baseline for people in group two.

68
00:08:21,090 --> 00:08:26,970
Right at the second occasion. So Peter too is the change for that group since baseline.

69
00:08:27,930 --> 00:08:31,680
Now if you do the same thing for one prime, two prime.

70
00:08:34,740 --> 00:08:40,160
Let's see what that number is. So it turns out to be like being a two plus better five.

71
00:08:42,160 --> 00:08:51,040
So below two plus below five means the change at the second occasion since baseline for people in the first group.

72
00:08:53,350 --> 00:09:01,180
Now, if you compare these two rows, the fact that they are different indicates this model is flexible enough to capture different levels of changes.

73
00:09:01,420 --> 00:09:04,630
Right now, I'm going to give you a bigger number.

74
00:09:05,560 --> 00:09:10,719
Sorry, a specific number. Say theta is say minus beta.

75
00:09:10,720 --> 00:09:15,270
Five is say. Like -2.1.

76
00:09:15,360 --> 00:09:18,680
Right. It's a negative number.

77
00:09:19,550 --> 00:09:27,890
Then how do you interpret this? It means that for people in group one, they experienced a greater decrease.

78
00:09:29,830 --> 00:09:36,970
A greater decrease in the main response, which is to say both groups can experience decrease in the response.

79
00:09:37,420 --> 00:09:45,490
But group one here, they experienced a greater decrease because a net sign here, they dropped much faster.

80
00:09:58,340 --> 00:10:07,430
So Peter five has a temptation of what's the magnitude of the greater decrease in the response from occasion to to baseline.

81
00:10:07,850 --> 00:10:12,770
All right. We will be practicing this interpretation again when we see the actual code.

82
00:10:13,910 --> 00:10:19,490
Now, this is the exemple data from TLC trial.

83
00:10:19,880 --> 00:10:26,780
And I think I don't need to repeat this again. I'll give you 30 seconds to read it, just to familiarize yourself with the scientific context.

84
00:11:00,360 --> 00:11:10,680
So in this dataset we have. RN equals sorry and equals four, which means that which means that everybody had four measurements.

85
00:11:11,880 --> 00:11:18,960
So it is okay to talk about a four by four variance covariance matrix for people's responses.

86
00:11:22,030 --> 00:11:28,060
By not running a code, which I will do next. In that slide, I'm just going to show you the Remo estimate.

87
00:11:31,000 --> 00:11:35,470
So this is one output here. It's just one hammer this home. This is something you can look at.

88
00:11:37,210 --> 00:11:40,870
We did not place any assumptions about these various clearances.

89
00:11:41,260 --> 00:11:44,440
You don't see any quality here. They're just freely estimated.

90
00:11:45,070 --> 00:11:59,580
Each parameter by itself. We can also perform something like Walt Test.

91
00:12:01,320 --> 00:12:06,940
Remember, we call in the. NULL hypothesis.

92
00:12:09,530 --> 00:12:13,130
We have this particular format, right?

93
00:12:14,420 --> 00:12:22,560
So you can write out l. Right.

94
00:12:25,090 --> 00:12:31,390
So essentially we'll be 000010000001.

95
00:12:32,970 --> 00:12:37,160
Write to us. How do you do this test?

96
00:12:37,370 --> 00:12:43,340
Well, this test is about whether these two equations beta five equals zero and beta six equals zero.

97
00:12:43,730 --> 00:12:49,760
Jointly are jointly true. We can do all test by estimating this thing.

98
00:12:50,840 --> 00:12:53,990
And then divide it by the various covariance. Right.

99
00:12:55,790 --> 00:13:02,090
But we can now do that because l theta it's a vector, it's a two by one vector.

100
00:13:02,570 --> 00:13:10,000
So we just do the vector. Notarized version of this.

101
00:13:20,360 --> 00:13:24,500
So we call if you have a single theta.

102
00:13:25,530 --> 00:13:38,710
The way you do the test is to do this. So this would approximately follow Chi Square one.

103
00:13:40,020 --> 00:13:44,460
This is scalar. But we are in a situation that we have to deal with this.

104
00:13:45,520 --> 00:13:59,100
So by drawing the analogy. This essentially is the various covariance matrix for that l beta, which is just beta five and beta six.

105
00:14:00,120 --> 00:14:10,190
And you can work out this particular. Formula, which is a l times variance covariance of theta at times l prime.

106
00:14:12,630 --> 00:14:19,140
Which is the one you see here on the type of slide. So this conceptually is quite simple.

107
00:14:19,150 --> 00:14:25,360
You estimate elbow to hat and estimated severance and then do the standardization.

108
00:14:25,720 --> 00:14:34,510
So you compare this quantity to a Chi Square distribution with degree freedom equal to the number of rows and out which is to.

109
00:14:36,330 --> 00:14:39,570
So remember that this thing itself is a scalar.

110
00:14:39,600 --> 00:14:45,860
Okay. It's a number. So, Andre, now you may have a Chi Square distribution.

111
00:14:45,890 --> 00:14:53,060
Remember, Chi Square distribution is always positive. And you ask whether, say, this is the.

112
00:14:54,140 --> 00:14:59,690
.505 cut off and you ask, where does this W2 Square lie?

113
00:14:59,720 --> 00:15:03,500
Right. So, for example, if W2 Square is here.

114
00:15:05,990 --> 00:15:13,610
Obese represent the observed. Then you can declare that this now is rejected based on the significance level of 0.05.

115
00:15:13,910 --> 00:15:19,100
So this is general procedure. I don't think there's anything new here except the quadratic form.

116
00:15:21,810 --> 00:15:24,780
So based on this, we can do some quick analysis.

117
00:15:25,020 --> 00:15:30,630
Again, I will repeat this in the hand of I v where I show you our code, but just want to give you some sense.

118
00:15:31,680 --> 00:15:37,640
So this is wall test, but based on a different self encoding there.

119
00:15:37,950 --> 00:15:41,110
As you recall, it is much more complicated to write out.

120
00:15:41,580 --> 00:15:45,030
But still you can represent the null using later representation.

121
00:15:45,540 --> 00:15:53,350
Here we can see that we can test for the interactions.

122
00:15:53,370 --> 00:15:58,450
We can test for the may effects. And in this case, let's focus on the introduction term.

123
00:16:01,890 --> 00:16:06,840
So the wall statistic here is 107.8 with three degrees of freedom.

124
00:16:07,260 --> 00:16:13,560
And, you know, it's significant. We just reject in all that the two curves are parallel.

125
00:16:21,150 --> 00:16:27,690
And this is the reference cell coding where you would did need to focus on.

126
00:16:28,680 --> 00:16:31,800
Again the coefficients corresponding to the interactions.

127
00:16:38,100 --> 00:16:42,420
So again, here you see evidence that you still need these interaction terms.

128
00:16:46,920 --> 00:16:50,100
So to summarize what we have learned so far.

129
00:16:51,230 --> 00:16:59,750
Analysis response profiles is very straightforward and we have illustrated these examples using a very special scenario,

130
00:16:59,750 --> 00:17:08,450
which is that the design is balanced, everybody has the same number of measurements and these the measurement occasions are shared to all the people.

131
00:17:09,170 --> 00:17:14,600
And also we have focused on discussing the group by time interaction.

132
00:17:14,870 --> 00:17:19,760
So the group is discrete and we were playing around with two level groups.

133
00:17:19,850 --> 00:17:24,290
Clearly you can do three level, four level. That's conceptually similar.

134
00:17:25,470 --> 00:17:29,430
What are the main features of this problem, of this model?

135
00:17:30,270 --> 00:17:34,680
So it allows arbitrary patterns in the response over time.

136
00:17:35,080 --> 00:17:39,360
Recall for any particular group, say treated group.

137
00:17:39,540 --> 00:17:46,110
Right. We have these. Unknowns.

138
00:17:48,290 --> 00:17:52,980
So this represents four main responses for people in the treated group.

139
00:17:53,000 --> 00:17:57,650
At four occasions, they are free parameters. They do not constrain each other.

140
00:17:57,680 --> 00:18:02,600
There is no monotonic relationship. Right? So you can pretty much estimate separately.

141
00:18:02,840 --> 00:18:06,320
That's what I meant by. They have arbitrary patterns in immune response all the time.

142
00:18:06,890 --> 00:18:09,230
And this also applies to the control group.

143
00:18:11,110 --> 00:18:21,610
Second, we also said we also want to have arbitrary patterns in the of responses, which is to say sigma I here,

144
00:18:22,090 --> 00:18:28,180
which naturally needs to be sigma because everybody has the same number of measurements at the same times.

145
00:18:30,050 --> 00:18:33,320
This will need to be completely written out as.

146
00:19:02,250 --> 00:19:07,190
And so on, so forth. So essentially it's unstructured.

147
00:19:11,490 --> 00:19:21,860
I'll structure the covariance matrix. With these two, you basically cannot be wrong.

148
00:19:22,100 --> 00:19:25,220
The means are completely flexible, no constraints.

149
00:19:25,580 --> 00:19:28,910
The variance covariance matrix is completely unstructured.

150
00:19:29,150 --> 00:19:34,460
So you are being you're not wrong. So this confers certain kind of robustness.

151
00:19:35,360 --> 00:19:46,640
It minimizes the risk of model specification. It also works when certain people have missing data and originally balanced design.

152
00:19:46,970 --> 00:19:51,860
And clearly there will be question about whether the missing data are informative or not.

153
00:19:52,520 --> 00:19:59,149
But in this particular situation, we're going to assume the missing data are missing completely random,

154
00:19:59,150 --> 00:20:04,550
and then we can proceed by ignoring certain rows from design matrix to accommodate missing data.

155
00:20:07,440 --> 00:20:09,299
We will be talking about drawbacks.

156
00:20:09,300 --> 00:20:16,350
Clearly there is no free lunch and we are using a lot of parameters here and we will be counting parameters in five C.

157
00:20:16,920 --> 00:20:21,450
But a prelude to that is, you know, as you if you have more than four measurements,

158
00:20:21,450 --> 00:20:25,050
say ten measurements, you will have a lot of variance governance parameter here.

159
00:20:25,500 --> 00:20:31,440
And if you have ten occasions, you will not only have four means, but rather ten means, right?

160
00:20:31,770 --> 00:20:39,929
So these numbers goes up pretty quickly, especially if you have more than two treatment groups or, you know, different two groups.

161
00:20:39,930 --> 00:20:44,160
You have like five level groups or like ten level group.

162
00:20:44,760 --> 00:20:46,950
You will have a lot more main parameters.

163
00:20:47,490 --> 00:20:56,520
So in those cases, we will need to have some parsimonious parametric assumption to use some bias to by some statistical efficiency.

164
00:20:57,510 --> 00:20:59,580
Well, that's more high level. But I want to say.

165
00:21:02,980 --> 00:21:09,820
The analysis immune response profile has some robustness, but at the cost of too many parameters to be estimated.

166
00:21:14,150 --> 00:21:17,430
As promised, I'm going to move on to the AFib.

167
00:21:17,810 --> 00:21:22,640
And this slide is basically a slide about code example.

168
00:21:22,820 --> 00:21:32,500
I'll explain some detail about how we produced those test results using are we using SAS or any other language?

169
00:21:32,510 --> 00:21:41,180
I think you have to try out on your own, but I assume that most of us are familiar with our and also these code are fully reproducible.

170
00:21:41,180 --> 00:21:47,140
So you can copy and paste them and run them. So let's switch to Hannah.

171
00:21:47,350 --> 00:21:51,420
I'd be. This is the same data set.

172
00:21:51,630 --> 00:21:54,750
So I'm going to skip this here.

173
00:21:55,230 --> 00:22:02,810
This is data printed out in the Arkansas. This data currently is of wide format.

174
00:22:07,410 --> 00:22:14,100
So here I'd represent, you know, I did it for different people.

175
00:22:14,490 --> 00:22:17,790
And the treatment to represent the group.

176
00:22:17,970 --> 00:22:24,070
A person being randomly assigned to. So placebo means you no control group.

177
00:22:24,210 --> 00:22:25,410
Eczema means a new treatment.

178
00:22:25,770 --> 00:22:34,290
And we hope to compare whether the new treatment gives us any difference in the trajectory of the outcome, which is a lower level.

179
00:22:34,860 --> 00:22:41,830
So. Then y0 y 146 represents the level at the baseline.

180
00:22:41,860 --> 00:22:47,350
One week later, four weeks later and six weeks later, and so on and so forth for the other rows.

181
00:22:47,980 --> 00:22:53,410
And what we did here essentially is to melt this white formatted data into long format.

182
00:22:59,890 --> 00:23:09,950
It is pretty intuitive because if you're looking at the excuse me, looking at the long format data, it will be something like this.

183
00:23:11,260 --> 00:23:16,690
First, you have more rows corresponding to one subject.

184
00:23:17,080 --> 00:23:23,590
Now all the four measurements from one subject are put on top of put one on top of each other.

185
00:23:24,250 --> 00:23:31,620
And we have the measurements. A together, too.

186
00:23:32,340 --> 00:23:37,040
Finally, we have the time of the measurements. 01465.

187
00:23:38,590 --> 00:23:43,570
So this is a long format. So originally we had 100.

188
00:23:44,490 --> 00:23:57,460
Rose. In wide format. Now we would have and times end, which is 100 times four, which is 400 rows here.

189
00:23:59,910 --> 00:24:06,479
In long format. In.

190
00:24:06,480 --> 00:24:10,470
Ah. There are two functions I often use.

191
00:24:13,460 --> 00:24:17,930
So by the short but fat triangle, I mean wide format.

192
00:24:19,600 --> 00:24:23,440
And this is a long format. It's not drawn to scale.

193
00:24:25,750 --> 00:24:34,790
If you do this, you can use melt. Because you're melting down the white one right now.

194
00:24:34,790 --> 00:24:44,090
If you're trying to convert long formatted data into white format, you basically have to go up, which is the case function.

195
00:24:54,110 --> 00:24:58,310
So with the long data, long format of data, we can do some plots.

196
00:24:58,550 --> 00:25:08,630
And this is something quite simple. What I want to say is that this is the DG plot version of those two main response curves.

197
00:25:09,260 --> 00:25:14,450
So the right one is for placebo and the green one is for the the treatment group.

198
00:25:14,750 --> 00:25:22,370
As you can see, the signal is pretty strong. People roughly start at the same place because our randomization balanced out the differences

199
00:25:22,370 --> 00:25:27,650
in immune responses and people in the treatment group are benefited from the drug.

200
00:25:28,040 --> 00:25:31,850
The plot level reduced and then the lower level seemed to have increased.

201
00:25:32,240 --> 00:25:40,550
And primarily because to achieve a equilibrium, the the led in the bone will be released into the bloodstream, causing the increase.

202
00:25:41,060 --> 00:25:43,940
But still, by comparing the two trends, they were very different.

203
00:25:44,480 --> 00:25:51,440
And the task of the remaining few slides are trying to use our code to quantitatively say that this trend is different.

204
00:25:53,310 --> 00:26:02,430
So what's the model we're going to build here? This is the full model specification recall that's arisen in the most generic form.

205
00:26:02,520 --> 00:26:10,170
We just write out something like this, right? This is the most generic form.

206
00:26:10,530 --> 00:26:15,440
And often we will need to assume that if i.

207
00:26:15,450 --> 00:26:19,360
J. It's going to be independent.

208
00:26:19,610 --> 00:26:23,420
I enjoy Prime where I does not play prime.

209
00:26:24,260 --> 00:26:30,980
So in English it means that for two people the rest errors are going to be independent regardless of.

210
00:26:32,240 --> 00:26:39,410
When the occasion was right. However, i j and if i.

211
00:26:39,410 --> 00:26:42,590
J prime can be correlated.

212
00:26:46,210 --> 00:26:50,420
Positively or negatively. So these are the.

213
00:26:52,900 --> 00:26:56,100
To assumptions made on the on the errors.

214
00:26:56,110 --> 00:27:08,810
And then we can focus on the mean. So for the main part, again, I am not doing anything new here relative to what we saw earlier in The Matrix.

215
00:27:09,320 --> 00:27:17,110
So. First because we are doing a response profile analysis.

216
00:27:17,440 --> 00:27:21,100
How many parameters do we need for two groups at four occasions?

217
00:27:23,070 --> 00:27:28,080
Yeah. Don't count there. I think that's where I'm going next. Two groups, four locations.

218
00:27:28,470 --> 00:27:35,350
So two times four, there should be eight free parameters. And indeed, if you count from beta zero, beta seven, then there are eight parameters.

219
00:27:35,370 --> 00:27:42,060
So a number seems to check out. And what are the features we add into the model?

220
00:27:42,780 --> 00:27:48,960
So the first one clearly is The Intercept. And we have the time many effects.

221
00:27:49,350 --> 00:27:52,560
So we're doing the dummy variables for each of the baseline times.

222
00:27:54,030 --> 00:27:58,200
And we have a group indicator. So the reference group will be the placebo group.

223
00:27:58,830 --> 00:28:02,070
And finally, we have the interaction term between the group and time.

224
00:28:02,340 --> 00:28:10,440
And because we used dummy variables for the week, we're just going to interact the group indicator with each of the.

225
00:28:12,230 --> 00:28:18,120
Post baseline indicators. All right. So that gives us the the model.

226
00:28:25,660 --> 00:28:33,370
Okay. And our goal essentially is trying to run some code to figure out whether we have

227
00:28:33,370 --> 00:28:37,540
enough evidence against this now that all the interaction terms are equal to zero.

228
00:28:38,650 --> 00:28:46,570
So before we proceed, I do want to sort of share with you one technique I used whenever I'm looking at this ugly me model.

229
00:28:47,030 --> 00:28:53,210
To be honest, it's very tedious. So it's purely for interpretation of purposes.

230
00:28:53,230 --> 00:28:58,090
Some of you will say that this is not a real technique. This is just working out every held true.

231
00:28:58,360 --> 00:29:00,970
But I can assure you that if you follow this recipe,

232
00:29:01,420 --> 00:29:05,800
you should not encounter any problem with interpretation of better and more especially interaction terms.

233
00:29:06,520 --> 00:29:12,490
So let's get started. It's similar to the curves we saw earlier, but here I'm just presenting them in the table.

234
00:29:19,330 --> 00:29:26,660
So I'm going to draw like two by. Actually your table.

235
00:29:28,870 --> 00:29:34,300
And I'm going to use one zero. So one represents I group.

236
00:29:35,720 --> 00:29:39,130
Equals this eczema. Can I use this? Okay.

237
00:29:39,850 --> 00:29:47,580
And then we have all the occasions. One, two, three, four.

238
00:29:47,730 --> 00:30:00,200
So this represents J. So in actual calendar time, this is week zero, week one, week four and week six.

239
00:30:01,310 --> 00:30:08,180
All right. So what we do now, at least in this context, is trying to figure out, you know, what's the main responses?

240
00:30:11,560 --> 00:30:17,049
For example, if I'm going to ask you this particular cell, it is basically asking you,

241
00:30:17,050 --> 00:30:20,980
hey, what's the mean response for a person in groups of similar.

242
00:30:22,150 --> 00:30:26,800
At the first occasion, which is the baseline, and then you need to look at the model to do that.

243
00:30:27,370 --> 00:30:31,930
I'm going to only do one cell and I'm going to copy all the other answers into the table at the same time.

244
00:30:33,260 --> 00:30:36,650
So here we're talking about group one.

245
00:30:36,860 --> 00:30:42,740
So let's look at the left. So all these interaction terms should be considered.

246
00:30:44,730 --> 00:30:49,920
Actually, let's start from the first one zeros there. Right. It is not week one.

247
00:30:49,950 --> 00:30:53,250
So this whole thing will be zero. This is not week four.

248
00:30:53,370 --> 00:30:58,530
So it's zero because we were talking about week zero here and this is zero two.

249
00:30:59,310 --> 00:31:02,490
Is it in group six Sigma? Yes. So it's a one.

250
00:31:03,330 --> 00:31:08,970
And it gets in. And is it week one, week four, week six now.

251
00:31:09,270 --> 00:31:14,250
So these terms will disappear. So the mean term will only be paid a zero plus of for.

252
00:31:19,680 --> 00:31:23,400
Right. So very simple, you know, almost you can do this, you know, brain dead.

253
00:31:23,850 --> 00:31:28,760
So essentially all you do is just repeat this several times, which I would not do.

254
00:31:28,770 --> 00:31:34,320
Now. Which are not doing to tell, but I'll copy the numbers.

255
00:31:34,340 --> 00:31:50,000
So the delta here. So if you do, the a man is a prime.

256
00:31:51,050 --> 00:31:59,290
Here. The difference will be better for right? So that represents the difference between two groups.

257
00:31:59,290 --> 00:32:21,650
And I'm going to have the other numbers. Right now, if you look at the difference, I probably should just simplify this by saying this is Delta.

258
00:32:22,820 --> 00:32:30,070
Maybe that's easier for me. This difference is now beta four.

259
00:32:30,100 --> 00:32:33,660
Plus beta five. And if we repeat this.

260
00:32:46,220 --> 00:32:50,820
The difference will be. Peter four plus Peter six.

261
00:32:51,870 --> 00:32:55,340
Similarly, you have. Four plus minus seven.

262
00:33:07,470 --> 00:33:09,440
Right. So again,

263
00:33:09,800 --> 00:33:21,350
you can see that the null hypothesis of the difference being identical across time will need to be beta five equals beta cells equals beta seven.

264
00:33:21,570 --> 00:33:28,330
Right. So this is a novel of parallel curves.

265
00:33:34,540 --> 00:33:38,380
Okay. One more task. How do we interpret a coefficient bid of five?

266
00:34:16,290 --> 00:34:18,440
So this is something we talk about earlier, right?

267
00:34:18,930 --> 00:34:25,830
So the way to interpret this is a little bit tricky, but it always rely on how we do the differences.

268
00:34:26,900 --> 00:34:30,950
So I'm going to put a few numbers here. One, two.

269
00:34:31,900 --> 00:34:38,910
Three, four. So four minus three is what?

270
00:34:41,150 --> 00:34:48,830
A better one, right? So beta one is the main change in the response.

271
00:34:49,900 --> 00:34:57,030
At occasion two since baseline. Let's do two minus one.

272
00:35:00,650 --> 00:35:05,240
Two minus one. So that will be beta one plus beta five.

273
00:35:08,770 --> 00:35:18,660
Right. So beta one plus beta five. Again, it represents the a change in the mean response allocation to since baseline.

274
00:35:18,840 --> 00:35:22,410
But for the Six Sigma group, the previous one was for the control group.

275
00:35:23,940 --> 00:35:29,130
So Peter five again represents that additional change in the treated group

276
00:35:29,910 --> 00:35:37,229
education to sense baseline if better five is negative as in the case in the data

277
00:35:37,230 --> 00:35:45,420
you saw then Beta five represents the additional decrease in the response education

278
00:35:45,420 --> 00:35:50,790
to a since baseline comparing the treatment group versus the control group.

279
00:35:57,800 --> 00:36:05,959
Okay. So that's how we do the annotation. And trust me, you don't need anything complicated here.

280
00:36:05,960 --> 00:36:10,910
Just map how this table. If you're careful, I think these indentations can be done pretty easily.

281
00:36:11,480 --> 00:36:19,250
Um, if later on you become more proficient in interpreting these coefficients, I think sometimes you don't need them.

282
00:36:19,880 --> 00:36:22,940
But for me, you know, especially when I'm teaching,

283
00:36:22,940 --> 00:36:29,510
I would like to map all of these numbers pretty slowly and hopefully that's not going to cause any mistakes.

284
00:36:30,770 --> 00:36:35,930
I'm going to give you 30 seconds just to look at this and to see if you have any questions.

285
00:37:00,150 --> 00:37:03,810
Okay. So this is the technique I was mentioning.

286
00:37:03,870 --> 00:37:09,060
Now let's return to the fitting on the model here.

287
00:37:09,360 --> 00:37:12,960
Again, just remind you that in mathematical notations,

288
00:37:13,500 --> 00:37:24,000
what we did above by specifying this complicated model is just to enable us to write down the entire design matrices for each people.

289
00:37:24,150 --> 00:37:29,700
Right. So remember in the previous slide, this is a model for y j write for each occasion.

290
00:37:30,090 --> 00:37:35,220
What this slide does is that it concatenated all the main model specification.

291
00:37:36,570 --> 00:37:40,020
Occasion. One, two, three, four.

292
00:37:40,020 --> 00:37:45,260
Together. Right. And depending on the group, we have different design matrices.

293
00:37:45,270 --> 00:37:48,540
So we have seen that earlier in the handout five.

294
00:37:57,920 --> 00:38:06,910
I'm going to zoom in a little bit. So this is the I believe the first time you have seen the use of the function seals.

295
00:38:07,430 --> 00:38:16,210
So again. What does goals mean? Okay.

296
00:38:16,730 --> 00:38:19,880
So hopefully that 10 seconds of pause give you some time to think.

297
00:38:19,910 --> 00:38:24,260
If you recall, great. If you don't recall this, fine. It's called a generalized.

298
00:38:28,080 --> 00:38:31,320
There's no excuse for not knowing least. Right. So it's a little scarce.

299
00:38:40,760 --> 00:38:46,660
Side note, you do not, right? So who invented Euler's?

300
00:38:54,600 --> 00:38:57,650
There's a controversy, but one person started with the G.

301
00:39:06,190 --> 00:39:13,180
Goss. The other person, I believe, is a. I cannot write down that name.

302
00:39:13,210 --> 00:39:17,650
Legend. So they have a controversy regarding who invented this.

303
00:39:17,650 --> 00:39:23,890
But I think guys pretty much said, hey, you know the other guy, I have done this three years ago.

304
00:39:24,220 --> 00:39:27,520
So actually I am into this. But the other guy said, Really?

305
00:39:28,030 --> 00:39:30,130
But why did you why didn't you write a paper about it?

306
00:39:30,220 --> 00:39:35,950
So the other guy wrote a paper about this and God said he have been using that for ten years or longer.

307
00:39:36,460 --> 00:39:42,460
Anyway, that's just a side note. So this is generalized, at least squares.

308
00:39:43,390 --> 00:39:48,010
And in this particular formula, I'm going to reveal to you all the components in the model.

309
00:39:48,640 --> 00:39:54,070
As promised, we will be talking about me model. We will talk about variance, coherence.

310
00:39:55,290 --> 00:39:57,420
I'm going to point to you where they are.

311
00:39:58,290 --> 00:40:08,010
So for the first few part of the code, just regard them as data pre-processing, trying to assign the J to each of the measurement occasions.

312
00:40:08,340 --> 00:40:14,820
One, two, three, four. Okay. Here, I'm going to point you to the me model first.

313
00:40:17,380 --> 00:40:21,390
This part. So the outcome is blood level.

314
00:40:23,960 --> 00:40:31,540
And. The covariates are the main effects of treatment and the main effects were weak dot

315
00:40:31,600 --> 00:40:38,680
f so that means factor and this estimate means the interaction is also included.

316
00:40:39,730 --> 00:40:44,660
Clearly don't need to write down the main effects to do this. So this is the moral.

317
00:40:46,200 --> 00:40:49,230
And then we ask the function, taking the data to fit the model.

318
00:40:52,220 --> 00:41:00,420
Finally, there is something that's new. There are two pieces.

319
00:41:00,750 --> 00:41:03,810
One is called a correlation, the other called weights.

320
00:41:05,610 --> 00:41:10,180
So I explain what that means. So first, a mathematical fact.

321
00:41:13,410 --> 00:41:20,850
Okay. Any matrix variance covariance matrix can be written as the following factorization.

322
00:41:37,620 --> 00:41:46,080
Where D is the diagonal matrix comprised of all the variances at each of the occasions?

323
00:41:48,010 --> 00:42:10,210
The is the correlation matrix. So the j and j primes element will be the correlation between y j i j prime of given the covariance.

324
00:42:15,330 --> 00:42:19,700
So this decomposition is really nothing, you know?

325
00:42:22,080 --> 00:42:29,290
Special. It is just trying to take into account that veterans have balances, have skills.

326
00:42:29,310 --> 00:42:35,310
We just normalize them. So the normalized version, the is inside and those you know,

327
00:42:35,310 --> 00:42:45,840
the correlation matrix always have a have diagonal values of one right because correlation of x and X with x being any random variable that we want.

328
00:42:46,650 --> 00:42:50,160
So this is a normalized variance or people call this clearance matrix.

329
00:42:51,000 --> 00:42:55,170
In this spirit, we will only need to specify what's V and we'll see.

330
00:42:55,920 --> 00:43:00,000
So c r here is basically specifying the v.

331
00:43:02,320 --> 00:43:06,010
And the ways here is basically specifying the V sorry.

332
00:43:06,070 --> 00:43:11,380
D Oh, yes. All right, everybody with me.

333
00:43:12,950 --> 00:43:17,740
Now. Let's look at the seal r for its correlation.

334
00:43:18,010 --> 00:43:26,139
Right. So it is using seal or sem symmetry sem and with a form.

335
00:43:26,140 --> 00:43:29,950
That's till the time. Vertical bar.

336
00:43:29,960 --> 00:43:38,630
I'd. So here the till the time just indicates what are the what is the variable that indicates occasion.

337
00:43:40,150 --> 00:43:46,060
Occasion one of you on two occasions, three and four, and it will pick pairs of those occasions to calculate correlations.

338
00:43:49,610 --> 00:43:55,510
And vertical bar idea is indicate how do we group observations?

339
00:43:55,550 --> 00:43:59,660
So observations in one group will be correlated clearly.

340
00:43:59,990 --> 00:44:05,420
Measurements obtained from people with the same ID will be repeated measurements and they likely will be correlated.

341
00:44:05,450 --> 00:44:08,990
So that's the ACR apart next weights.

342
00:44:09,530 --> 00:44:17,420
So that's a D part. And you will only need to specify how these sigma one squared to sigma and squared will look like.

343
00:44:20,140 --> 00:44:23,650
There are many ways to specify this. And this is one way.

344
00:44:23,980 --> 00:44:24,820
It's a simple way.

345
00:44:25,780 --> 00:44:33,760
It is to say that first, these variances are now going to depend on other words, they are just numbers that one single number one estimate.

346
00:44:34,540 --> 00:44:38,560
What if you put something else, something else other than one?

347
00:44:40,950 --> 00:44:45,960
Then you would allow Sigma one square say to depend on additional covers.

348
00:44:46,680 --> 00:44:51,960
That's doable, but that's not often what we want to do.

349
00:44:51,990 --> 00:45:00,060
Sometimes we just want to have a simple variances in the situation where you want to replace the.

350
00:45:01,050 --> 00:45:06,330
Is this called Twiddle? Until. This one.

351
00:45:06,810 --> 00:45:12,480
Sorry. I need your help to. How is this notation called?

352
00:45:16,870 --> 00:45:23,980
Twiddle until. I think it's both okay.

353
00:45:25,170 --> 00:45:33,870
Sorry. I think it's Tilda. Tilda. Okay. All right, so I'll call that to Tilde.

354
00:45:35,010 --> 00:45:38,040
So if you replace this by some other cover, it's like.

355
00:45:40,990 --> 00:45:45,870
Time. You will be allowing Sigma Square j to depend on time.

356
00:45:49,190 --> 00:45:55,730
And that's more complicated. Here we start off again.

357
00:45:55,740 --> 00:46:05,940
This is trying to indicate how many time points we're looking at because we do need that information to tell how many signals do we need.

358
00:46:06,120 --> 00:46:13,200
If it's everybody had four measurements, so we've got F being a factor, it'll have four levels.

359
00:46:13,470 --> 00:46:25,380
So you will need sigma one squared, two sigma four squared. So taken together, what we have just talking about is to use a function called us.

360
00:46:26,830 --> 00:46:37,450
As you recall, g class is beta has OC and once you have fitted this, once you have called this function,

361
00:46:37,720 --> 00:46:41,380
essentially you will be able to get beta hat and all their likes and errors.

362
00:46:43,480 --> 00:46:48,460
Okay. And we talked about two parts of the model as we have introduced the model.

363
00:46:48,970 --> 00:46:52,450
Now, in this case, with the interaction between treatment and time.

364
00:46:53,590 --> 00:46:55,210
And also indicate what data to use.

365
00:46:55,810 --> 00:47:03,040
Finally, we introduced in two parts how to specify the variance covariance matrix and based on the mathematical fact,

366
00:47:03,520 --> 00:47:08,740
we can specify the correlation matrix V and the diagonal variances.

367
00:47:10,360 --> 00:47:21,050
And in any particular analysis, what are specified after seal are and weights can be different depending on your particular interest.

368
00:47:21,070 --> 00:47:28,059
You can make them complicated here. We illustrated one example where it's a symmetric correlation,

369
00:47:28,060 --> 00:47:32,680
which means regardless of which pair of measurements you pick, the correlation will always be row.

370
00:47:33,010 --> 00:47:39,820
And that's called symmetry or symmetric correlation or exchangeable structure or compound symmetry.

371
00:47:39,850 --> 00:47:45,530
Okay. So. We can run this code.

372
00:47:45,560 --> 00:47:48,770
Oh, by the way, this package is called Lemi. So we can call this.

373
00:47:50,010 --> 00:47:54,300
Once you run this code, you will get these data. Sorry, these are results.

374
00:47:55,750 --> 00:47:59,910
What I did is simply just do the summary. First,

375
00:48:00,660 --> 00:48:05,520
a natural question you should always ask yourself is what is the algorithm or

376
00:48:05,520 --> 00:48:09,839
what is the estimate that was used to estimate the sigma is the variances,

377
00:48:09,840 --> 00:48:16,080
right? We have talked conceptually that there are email and remote options by default.

378
00:48:17,130 --> 00:48:20,850
It is done by Remo. We call. I did not even specify this option in the function.

379
00:48:21,990 --> 00:48:25,430
The fact that it's default indicates that, you know,

380
00:48:26,100 --> 00:48:34,110
the developer of this package understands quite well that any finite samples to mitigate the bias, we want to use Remo as a default.

381
00:48:37,020 --> 00:48:40,080
As promised, we have estimated a few different parameters.

382
00:48:40,830 --> 00:48:45,390
So this one's to be. This one is the DE.

383
00:48:47,750 --> 00:48:59,480
And these are the beta zero beta for beta one beta to be the 3 to 5 better 6 to 7.

384
00:49:01,310 --> 00:49:07,530
The reason why I know I should put beta for here is that I checked the ordering of these terms.

385
00:49:07,550 --> 00:49:14,330
It just so happens that in the previous slide we put, the group may in fact in the in the fifth place.

386
00:49:14,330 --> 00:49:18,120
So I'm just going to put it back. This can't be the four, right?

387
00:49:23,400 --> 00:49:29,860
All right. So. Can you guys recall?

388
00:49:29,870 --> 00:49:39,100
What's the definition? What's the interpretation of the five? We just interpret this, right?

389
00:49:40,390 --> 00:49:45,130
So it is currently a negative number, -11.

390
00:49:46,680 --> 00:49:54,070
What does that mean? You can practice during a break, but hopefully this can give you another chance to practice invitation.

391
00:49:54,280 --> 00:49:59,799
Let's come back at 355 and we will continue to finish this hand off.

392
00:49:59,800 --> 00:50:03,310
I've been potentially start a little bit into who if I see.

393
00:50:27,280 --> 00:51:14,500
But. So it's a little bit.

394
00:51:20,820 --> 00:51:25,250
Oh, yes, I do, of course. Never.

395
00:51:40,680 --> 00:51:44,670
So. For the.

396
00:51:46,160 --> 00:51:49,590
How many? Here's what he had to say.

397
00:51:50,750 --> 00:51:53,920
Tell people these are. Occasions.

398
00:51:56,540 --> 00:52:05,790
Get to. Certain populations of.

399
00:52:12,710 --> 00:52:28,940
It took. So without time to you and say, okay, tell me what's.

400
00:52:40,870 --> 00:52:43,890
And just have to.

401
00:52:47,540 --> 00:52:52,650
I think so. I think. This.

402
00:52:56,430 --> 00:53:08,100
Saying. I just want to say. So we have seen.

403
00:53:13,250 --> 00:53:25,090
My friends. Okay.

404
00:53:32,820 --> 00:53:49,320
I did. Because this is.

405
00:53:57,970 --> 00:54:05,880
The public. Oh.

406
00:54:08,330 --> 00:54:14,910
It's a serious. So I think this is.

407
00:54:15,680 --> 00:54:21,070
Yeah, exactly. Yeah. This is supposed to be. Do.

408
00:54:24,160 --> 00:54:28,080
So I think maybe you can simplify. I think you're right.

409
00:54:30,440 --> 00:54:42,780
Requirements for. Yeah, well, I think.

410
00:54:45,060 --> 00:54:49,270
Second sort of. That's.

411
00:55:01,310 --> 00:55:05,240
So it's a situation.

412
00:55:10,050 --> 00:55:14,930
Yeah. Basically says that the Syrians.

413
00:55:21,850 --> 00:55:25,310
To say that.

414
00:55:31,780 --> 00:55:36,700
Oh, yeah.

415
00:55:37,100 --> 00:55:45,700
Yeah. So. So. Yeah, I see.

416
00:55:50,950 --> 00:55:55,880
Again. I think that we're just. I think.

417
00:55:56,740 --> 00:56:18,000
I think. I think. This represents representative.

418
00:56:21,680 --> 00:56:39,500
I just asked the question. So he asked me all this question.

419
00:56:40,810 --> 00:56:47,880
Yeah, I. How? Like this. But.

420
00:56:49,720 --> 00:56:53,280
So this year we're. Yeah.

421
00:56:55,900 --> 00:57:01,410
Okay. So suppose there's a person standing.

422
00:57:04,150 --> 00:57:07,540
So what's the value of this? Yeah.

423
00:57:10,410 --> 00:57:15,480
This week is.

424
00:57:22,690 --> 00:57:29,970
But the first real. So I ask the percentages here.

425
00:57:36,360 --> 00:57:42,620
You know, I think. Yes, I know.

426
00:57:42,670 --> 00:57:56,420
I just want you to. Yes.

427
00:57:58,400 --> 00:58:03,780
Yeah. Yes.

428
00:58:08,450 --> 00:58:13,580
But obviously. To.

429
00:58:19,390 --> 00:58:23,410
I thought I was.

430
00:58:27,100 --> 00:58:40,740
Let's go to. I think that more importantly.

431
00:58:43,200 --> 00:59:03,430
Yeah. All right.

432
00:59:03,760 --> 00:59:10,180
So let's get back to work. So here, I want to return to this particular table again.

433
00:59:10,180 --> 00:59:16,210
The question I left you before we took the break was, what's the interpretation of Beta five?

434
00:59:16,630 --> 00:59:20,530
So we return to this table and you can see that's a better five.

435
00:59:20,530 --> 00:59:27,100
Is the access or additional change of the main response or the second occasion relative to baseline?

436
00:59:27,700 --> 00:59:31,030
Right. Comparing group one versus group zero.

437
00:59:31,120 --> 00:59:35,080
Right. So that access decrease is going to be good, right?

438
00:59:35,440 --> 00:59:40,569
Because the treatment is working so hard. So reduce the block level.

439
00:59:40,570 --> 00:59:44,050
So that's good, right? So when you look back at our result.

440
00:59:46,420 --> 00:59:50,350
Here. This number is -11.4. So it's good.

441
00:59:50,350 --> 00:59:54,700
And this is what sustained error 1.12. And what's the P value?

442
00:59:55,510 --> 01:00:07,570
It's very small. So it indicates that for beta five indicates that the drug is working significantly to reduce the block level.

443
01:00:10,330 --> 01:00:15,490
At second occassions relative to baseline four for the treatment group relative to the control group.

444
01:00:16,600 --> 01:00:20,380
So you can do the similar represent a similar indentation for beta six and beta seven.

445
01:00:21,730 --> 01:00:25,540
But if you look at these numbers, you can see they are significant individually.

446
01:00:27,460 --> 01:00:34,180
Right. So looking at this table is only going to give you, um, how to say ah.

447
01:00:35,780 --> 01:00:43,240
It's only going to help you test whether beta five equals zero. All right, you've got a1p value and you repeat this for other.

448
01:00:45,320 --> 01:00:51,950
Parameters. But the test we are interested in is this joint, not whether they are equal zero.

449
01:00:53,060 --> 01:01:01,310
So there are many, many different options. One option, slightly different from what test is called like a racial test.

450
01:01:01,790 --> 01:01:05,420
I don't see any to introduce to this audience what like a racial test is.

451
01:01:06,410 --> 01:01:11,570
But the general idea is that we fit a bigger model.

452
01:01:13,130 --> 01:01:17,240
And we fit a smarter model. The bigger model contains the introduction term.

453
01:01:17,870 --> 01:01:23,720
The smaller model does not contain the introduction term. So model one is the bigger, bigger model with interruption.

454
01:01:23,930 --> 01:01:30,550
Model zero is the model without interruption. The difference between these two set them in the codes is simply this model.

455
01:01:30,680 --> 01:01:33,680
Look. The only difference is this asterisk versus this.

456
01:01:33,680 --> 01:01:41,540
Plus, if you cannot see, you've got to trust me here. So the top one is the asterisk indicating interaction is present was not in the second model.

457
01:01:42,650 --> 01:01:48,500
The larger model will have a larger model space so you can potentially obtain the higher likelihood.

458
01:01:49,010 --> 01:01:54,020
So naturally, you want to ask whether that increased likelihood is worth the while of the additional parameter.

459
01:01:54,440 --> 01:02:00,050
And here we have three additional parameter. To help us increase the likelihood.

460
01:02:00,140 --> 01:02:03,440
Right. And let's see, how much likelihood did we increase?

461
01:02:05,370 --> 01:02:09,790
So if you look at model one, there are 18 degrees of freedom.

462
01:02:09,810 --> 01:02:15,660
We will be calculating this number in a little while, but this model model has 15 parameters.

463
01:02:16,260 --> 01:02:20,340
So difference of three coming from betas five. Beta six. Beta seven.

464
01:02:26,010 --> 01:02:35,520
And the like ratio statistic is going to be 774 and the p value compared to chi square

465
01:02:35,520 --> 01:02:40,980
statistic with three degrees of freedom and that p value is going to be much smaller than .05.

466
01:02:42,180 --> 01:02:50,920
So. You know, this is going to indicate that all the we can reject and all that's all the drugs in terms of zero.

467
01:02:54,420 --> 01:02:59,610
So what's the expected value of Chi Square? P. Should be pro.

468
01:03:00,710 --> 01:03:10,620
So 74 is pretty big number relative to the main. Before I leave this slide, I wonder whether anybody.

469
01:03:11,700 --> 01:03:14,730
Who has sharp bias caught any weird.

470
01:03:15,710 --> 01:03:23,790
Options in these two functions. Not weird, but something I have been advocating not to do.

471
01:03:25,750 --> 01:03:29,550
Say it again. Emma. Excellent. Did you guys see it?

472
01:03:34,200 --> 01:03:37,350
I used ML and I forced them to use ML.

473
01:03:39,980 --> 01:03:45,350
I will be talking more about this, but I want to give you a quick reason why.

474
01:03:48,110 --> 01:03:57,490
So for Rimmel, we talk about two perspective, right? So eml is l beta sigma with data.

475
01:03:59,940 --> 01:04:04,420
We maximize this day. Right, Remo? We had.

476
01:04:06,090 --> 01:04:15,870
Two perspectives. One is that we. Use a modified like hood on the transform data so the likelihood does not depend on beta.

477
01:04:15,900 --> 01:04:22,170
Let's just stop right there. Okay. If you're going to compare two models, do you want to compare model that contains beta?

478
01:04:22,410 --> 01:04:25,500
Do you want to use a method or design criteria that contains beta or not?

479
01:04:28,170 --> 01:04:34,430
Okay. Let me say it again. If you're going to compare two models and you're going to use certain objective functions.

480
01:04:35,390 --> 01:04:41,150
Do you want to use objective functions obtained with beta in their or not in their?

481
01:04:43,460 --> 01:04:53,120
So that's the. To me. I would want to I would want the memorial selection to based on the objective function that contains better.

482
01:04:57,670 --> 01:05:05,980
Why? Well, data star, the transfer of data is explicitly transformed to remove its dependance on data.

483
01:05:06,760 --> 01:05:11,140
Hence your ability to choose the ME model because my model is completed to me by data.

484
01:05:11,830 --> 01:05:20,230
So it is just. It's not a silly idea to rely on to use Remo to do me model selection.

485
01:05:22,150 --> 01:05:26,920
Okay. So we will be talking about this again and this will be a midterm exam question.

486
01:05:27,820 --> 01:05:34,510
Okay. So so when we were choosing a model, we want to use eml.

487
01:05:39,580 --> 01:05:49,570
Okay. So hopefully this particular test of the null hypothesis of beta five equals beta six, seven equals zero.

488
01:05:50,170 --> 01:05:59,230
Give you a strong quantitative sense that we need to tell FDA that please approve this drug.

489
01:05:59,520 --> 01:06:02,380
Okay. Something to this degree.

490
01:06:02,650 --> 01:06:09,520
If you guys were at the yesterday's seminar, it's pretty much what they want to do to show that that thing works and they have some way to do it.

491
01:06:09,940 --> 01:06:13,450
This is in longitudinal setting. I guess they don't have that.

492
01:06:16,880 --> 01:06:25,370
Now with the help of this particular model, I do want to return to one point that many people were asking.

493
01:06:28,300 --> 01:06:35,240
It is about based on randomization. So here I'm going to point you to parameter beta four.

494
01:06:35,450 --> 01:06:38,510
So beta four, as I recall, is the.

495
01:06:41,610 --> 01:06:45,180
Just bear with me here. Peter. Four is the coefficient in front a group indicator.

496
01:06:45,720 --> 01:06:53,210
So how do you interpret data for. This one, right?

497
01:06:54,280 --> 01:06:58,359
You calculate the mean for the first group or the first occasion you calculate me

498
01:06:58,360 --> 01:07:01,560
for the second group and the first occasion to take different sets of data for.

499
01:07:01,960 --> 01:07:06,550
But. The first occasion is the baseline prior to realization, right?

500
01:07:07,090 --> 01:07:12,880
So Beta four is really theoretical, measuring the main difference between two groups before randomization.

501
01:07:13,360 --> 01:07:16,830
Suppose a random randomization has done its job before.

502
01:07:16,840 --> 01:07:21,610
Should be comfortably near zero. Right. At least in large samples.

503
01:07:22,450 --> 01:07:27,970
So let's see whether we can claim. That there is no evidence against Peter four zero.

504
01:07:28,390 --> 01:07:32,260
So the estimate of that is point to seven and scenario a one.

505
01:07:32,950 --> 01:07:37,990
So if you just take the ratio and clearly that's not not too big.

506
01:07:38,260 --> 01:07:43,690
If you look at the p value here, essentially that's 0.79 here.

507
01:07:45,540 --> 01:07:52,740
So not too big. So we do not have evidence to reject and all that's better for equals zero.

508
01:07:53,190 --> 01:07:58,860
In this case, we're just going to accept that the randomization worked well to balance out

509
01:07:59,640 --> 01:08:03,690
differences between the two groups so they have the same mean at baseline.

510
01:08:08,630 --> 01:08:13,520
All right. So in the final slide, these are just more interpretations.

511
01:08:14,090 --> 01:08:17,910
And here I probably will not.

512
01:08:18,530 --> 01:08:25,340
I will I will prefer you guys to read these in more detail so as so that it is a practice for you to interpret these parameters.

513
01:08:25,880 --> 01:08:32,580
I have taken the lead to interpret Beta five and you will need to interpret this particular one actually.

514
01:08:33,290 --> 01:08:38,480
Peter, what you need to ask yourself. But I want to.

515
01:08:39,580 --> 01:08:46,420
Just touch again on the fourth point is that the later increase in bottler level is likely due to

516
01:08:46,420 --> 01:08:51,790
the fact that the let story in the bill were released into the bloodstream achieving equilibrium,

517
01:08:51,790 --> 01:08:58,240
hence that upward trend for treatment group. So overall, this analysis has shown the treatment works.

518
01:09:00,220 --> 01:09:06,310
And before we move on to the next slide, I just want to return to this particular slide here, as I promised that.

519
01:09:06,310 --> 01:09:08,800
How did you come up with the number 18?

520
01:09:12,100 --> 01:09:21,040
So 18 is the model degree of freedom, which means that you used 18 parameters in a big amount and let's count the parameters.

521
01:09:21,640 --> 01:09:32,250
So when you are talking about model degree freedom. The model part naturally includes the exhibitor part, the ME model, and the Sigma II Part II.

522
01:09:33,500 --> 01:09:39,149
Now. How many parameters are there in the model? Two groups four occasions.

523
01:09:39,150 --> 01:09:43,280
So two times four eight. How about Sigma?

524
01:09:43,310 --> 01:09:54,430
If we assume it's totally unstructured. So it's end plus one times and divide by two.

525
01:09:55,910 --> 01:10:04,830
So it's ten. So this is ten.

526
01:10:05,460 --> 01:10:08,960
So in total, 18. Okay.

527
01:10:08,980 --> 01:10:18,950
Where does this come from? Well. So you have a big matrix.

528
01:10:19,490 --> 01:10:24,430
How many diagonals? You have end of them, right?

529
01:10:24,490 --> 01:10:29,360
How many of the other values? And choose to write.

530
01:10:29,450 --> 01:10:34,250
So if you do this and this will be and plus one times and divide by two.

531
01:10:36,040 --> 01:10:40,870
Anyway. This is why we got 18 here. And for the 15th, really just have three.

532
01:10:42,710 --> 01:10:49,300
You know, you just don't have the Beta Phi Beta six and Beta seven. Just remind you that we started with a zero.

533
01:10:49,310 --> 01:10:52,460
So the fact that we have better serve indicates we had eight main parameters.

534
01:10:55,100 --> 01:11:00,490
Or I'm going to switch in that slide, but I'm going to give you like 30 seconds to think about what are the question they ask here.

535
01:11:01,130 --> 01:11:09,470
Um, again, I have presented a lot of details here and hopefully these are detailed enough for you to reexamine all the code.

536
01:11:09,710 --> 01:11:18,200
And I think that the most important exercise is trying to interpret the coefficients, say like beta phi beta six, beta seven individually.

537
01:11:18,560 --> 01:11:26,210
And um, and also the final slides provide some invitations and you want to map them back to the update estimates.

538
01:11:52,600 --> 01:11:55,660
Would people be comfortable if I go to an out of five seat?

539
01:11:57,460 --> 01:12:00,740
Is that okay? All right.

540
01:12:01,280 --> 01:12:16,400
Okay. So 405c We are going to do something a little more fancier.

541
01:12:17,270 --> 01:12:21,080
Not really fancier than what you have seen in your 650 or 651,

542
01:12:21,530 --> 01:12:28,520
but fancier than the one you have seen in the previous few slides like the response profile analysis.

543
01:12:29,630 --> 01:12:34,880
Still, it falls into the subset of lectures that talks about me.

544
01:12:35,360 --> 01:12:38,540
So it is still about how do we do a good job here?

545
01:12:40,180 --> 01:12:47,530
And we were going to be, you know, criticizing the main response provider analysis a lot.

546
01:12:48,370 --> 01:12:52,480
It does not mean that being response professionals is not useful at all.

547
01:12:53,600 --> 01:12:58,190
But it is not often applicable in real data analysis.

548
01:12:59,330 --> 01:13:05,899
So I'm going to talk about a few reasons I would be very happy if I can cover those high

549
01:13:05,900 --> 01:13:12,440
level reasons in the next 7 minutes and we can dove into detail at the next lecture.

550
01:13:15,110 --> 01:13:22,790
So for this particular handout of our objectives, our three.

551
01:13:22,880 --> 01:13:28,610
First, we're going to describe the advantages and drawbacks of profile analysis.

552
01:13:29,300 --> 01:13:34,720
And second, it's not always enough to point out A is not good enough.

553
01:13:34,730 --> 01:13:41,460
You've got to provide some solutions. And and we were just going to be doing that.

554
01:13:41,670 --> 01:13:45,060
So the solution will be some parametric models.

555
01:13:45,660 --> 01:13:50,070
And finally, we like to put everything into a clean format.

556
01:13:50,160 --> 01:13:55,980
So we will be formulating those alternative solutions in mathematically rigorous way.

557
01:13:57,320 --> 01:14:01,820
And we will be just covering objective one today.

558
01:14:03,730 --> 01:14:11,200
So let's start with the relative merits and drawbacks of the main response profile analysis.

559
01:14:16,760 --> 01:14:20,210
As you have seen, it is very straightforward.

560
01:14:21,560 --> 01:14:26,060
We require the design to be balanced and the timing repeater measurements to be common

561
01:14:26,060 --> 01:14:30,620
to all individuals and require the cohorts to be discrete like the treatment group.

562
01:14:30,620 --> 01:14:36,020
Right. So that's the setting where we discussed the analysis of response profiles.

563
01:14:38,690 --> 01:14:43,250
What are its main features? It allows arbitrary patterns.

564
01:14:43,550 --> 01:14:51,140
It allows arbitrary parts in the variance and covers. It has certain robustness, and it can be applied when data are missing.

565
01:14:51,170 --> 01:15:01,040
So this is a slide from before. However, the real world is going to surprise you because there are many mis timed measurements,

566
01:15:02,240 --> 01:15:09,860
which means that we have measurements from you that are going to be obtained at completely different timings than my measurements.

567
01:15:11,270 --> 01:15:22,430
So either if you want to use the analysis of profile response profiles, you've got to be willing to round the observations to come time points.

568
01:15:23,270 --> 01:15:30,090
But if you're not willing to do that, what are the solutions? So we will be talking about that.

569
01:15:30,100 --> 01:15:40,350
So clearly the first situation where the being response profile analysis falls short is when the measurements are not obtained at the common timings.

570
01:15:41,190 --> 01:15:48,410
Second. The main response profile analysis ignores the time ordering of the repeated measurements.

571
01:15:49,730 --> 01:15:58,010
So this is a little bit nuanced to topic, but I do want to point out what information do you lose by ignoring the time ordering?

572
01:15:59,070 --> 01:16:04,260
Recall the previous lecture. So our previous slide, when you were modeling the week you just did.

573
01:16:04,260 --> 01:16:09,780
The indicators indicate a week equals one indicator weekly for indicator we peak of six.

574
01:16:10,530 --> 01:16:14,620
There's one indicator know each other which one comes before which? They don't.

575
01:16:14,980 --> 01:16:19,690
You just tell them which level they're in. As if weak is purely categorical.

576
01:16:19,740 --> 01:16:22,960
Order the variable. But that's not true in reality.

577
01:16:23,930 --> 01:16:30,840
Has orders. So.

578
01:16:33,120 --> 01:16:36,210
This brings us to two points. I want to elaborate.

579
01:16:36,510 --> 01:16:40,560
First, in response, profile analysis.

580
01:16:41,340 --> 01:16:48,750
When we were talking about that particular technique, we were saying, hey, let's measure the weight of the guinea pig four or four times.

581
01:16:49,030 --> 01:16:52,440
Right. So you're measuring the same outcome four, four times.

582
01:16:54,340 --> 01:17:01,480
So it's called repeated measurements. But indeed, if you think about it, there's no reason why those four things has to be about weight.

583
01:17:01,570 --> 01:17:08,260
Maybe I measure weight in the first week, then measure height in the second week and measure the body fat percentage in the third week.

584
01:17:08,740 --> 01:17:12,350
If I measure the VO2 max in the fourth week, right.

585
01:17:12,430 --> 01:17:15,470
These are four numbers, supposedly all continuous or maybe they're not. So.

586
01:17:15,490 --> 01:17:18,720
Suppose you can measure four measurements, four things. Totally different, right?

587
01:17:19,770 --> 01:17:22,800
Analysis. I mean, response profiles absolutely.

588
01:17:22,950 --> 01:17:25,430
Can analyze that data, right?

589
01:17:26,840 --> 01:17:34,640
So there is you know, you don't need any relationships between those outcomes conceptually or, you know, but you can still model the correlation.

590
01:17:36,970 --> 01:17:46,390
So that is what what is said here. That analysis can work even if you have a vector of outcomes that are distinct and then can measure it.

591
01:17:48,140 --> 01:17:53,270
So this brings me a bringer I want to sort of sidetrack a little bit.

592
01:17:55,400 --> 01:18:01,910
In many of the papers you're going to read in your career, hopefully you are going to be papers in a career.

593
01:18:02,420 --> 01:18:06,110
There will be a few terms like repeated outcomes.

594
01:18:11,190 --> 01:18:18,930
Cluster outcomes. And multivariate.

595
01:18:20,830 --> 01:18:30,159
Outcomes. Okay. So my English was very bad when I was first started graduate school.

596
01:18:30,160 --> 01:18:33,520
So I absolutely was very, not very sure what's the difference.

597
01:18:34,810 --> 01:18:40,060
I'm now going to offer how I how I feel, how I sort of categorize them.

598
01:18:41,340 --> 01:18:49,640
I claim that there is a Venn diagram you can draw. And all you need to do is to connect where they are.

599
01:18:59,640 --> 01:19:03,630
Okay. So here's my claim. Multivariate is most general.

600
01:19:05,330 --> 01:19:11,980
Custard is a little bit more restricted. And repeat is going to be the most restrictive.

601
01:19:15,630 --> 01:19:21,840
Repeated means that the thing that causes which causes us to have many measurements because of time.

602
01:19:24,670 --> 01:19:28,360
And they're correlated, but it's not. It's not only time that can cause correlation.

603
01:19:28,600 --> 01:19:33,220
If we are a family, our hearts, our diet, our lifestyle will be similar.

604
01:19:34,420 --> 01:19:39,040
So that's going to be more general. It's called clustered. Multivariate.

605
01:19:39,040 --> 01:19:42,700
Clearly, multivariate just means many things, right?

606
01:19:43,300 --> 01:19:46,300
So so this is how I visualize that.

607
01:19:46,390 --> 01:19:58,360
Okay. If you do need a visual cue. I'm a very visual person, so I like this.

608
01:19:58,380 --> 01:20:08,750
I like to do this. Just bear with me. Anyway.

609
01:20:11,200 --> 01:20:14,680
And returning back to the main story here.

610
01:20:15,010 --> 01:20:25,030
So if we fail to recognize ordering and clearly there may be some important biological process that you do not measure to do not model.

611
01:20:25,030 --> 01:20:28,500
Right. Whatever. It's actually a very biological process.

612
01:20:28,510 --> 01:20:34,150
By ignoring the time ordering, you're not going to use that information or you're just stretching the data to think.

613
01:20:42,470 --> 01:20:55,450
So final slide. Stop. The third complaint is that forming response profile analysis you can reject or not like we did before.

614
01:20:57,260 --> 01:21:02,180
You did the job as you interpreted it. Did the local racial test you reject or not?

615
01:21:02,300 --> 01:21:07,700
Great. Okay, now what? They have interruption, but in which way?

616
01:21:08,570 --> 01:21:17,490
So that test absolutely does not tell you. What's, you know, what you would do after you've checked it all.

617
01:21:18,060 --> 01:21:24,630
So sometimes people would want to have an alternative that's more specific, like.

618
01:21:25,880 --> 01:21:29,120
You know they are not parallel but both on senior.

619
01:21:30,770 --> 01:21:34,310
They are not parallel, but both are quadratic. They are not parallel.

620
01:21:34,850 --> 01:21:42,800
Both are qubit. You know where I'm going? So sometimes it's important to be able to detect specific trends in response to over time.

621
01:21:47,940 --> 01:21:51,570
So that's why it's worthwhile to assume some trends, be it or quadratic.

622
01:21:55,240 --> 01:22:07,130
And a final the final bullet point here is that often if you restrict yourself to the alternative that it is and most need here not other trends.

623
01:22:07,540 --> 01:22:11,260
Your test is often going to be more powerful. Think about it.

624
01:22:11,620 --> 01:22:20,589
You have restrict a space of hypotheses to be linear trends that are parallel or non parallel versus alternative hypothesis space,

625
01:22:20,590 --> 01:22:24,570
which is all the curves that are parallel and on parallel, right?

626
01:22:25,120 --> 01:22:33,220
So clearly you can use data with much more focus and use that much more efficiently by focusing on a specific alternative.

627
01:22:33,700 --> 01:22:38,870
So that's the final point. With that, I'm going to end today's lecture.

628
01:22:38,870 --> 01:22:45,350
And in the next lecture we will be talking about exactly how we are going to do this.

629
01:22:46,730 --> 01:22:49,220
Thanks, everybody. Wish you have a good day.

