1
00:00:03,680 --> 00:02:40,740
It's just. But.

2
00:02:49,020 --> 00:02:58,670
We? Other.

3
00:03:01,640 --> 00:03:08,430
So many thanks to.

4
00:03:11,670 --> 00:03:28,360
All. So.

5
00:03:30,640 --> 00:03:33,960
Thanks, everybody, for getting your musings in on the idea.

6
00:03:34,360 --> 00:03:39,850
It was never fun, but I appreciated it.

7
00:03:43,420 --> 00:03:53,530
Today we're going to talk about this new sense of this perception of having a cute little girl.

8
00:03:54,620 --> 00:03:59,950
And one of the things I want to say right at the start is that there's a tendency

9
00:04:00,880 --> 00:04:09,370
for us to sort of start to categorize people or relations as being one or the other.

10
00:04:09,820 --> 00:04:17,920
Well, this person is more emotional than this person, and I want to steer you away from that.

11
00:04:19,750 --> 00:04:26,050
I also want to say, you know, there are times in which you don't think, well, wow, that's a really emotional risk that's really come into your breath.

12
00:04:26,080 --> 00:04:33,340
Yes. There are some risks that are more than evokes strong emotions in us and others that maybe are going to do so less.

13
00:04:34,210 --> 00:04:47,530
But that shouldn't change. Are you nervous that you are fundamentally both always bringing our cognitive conscious,

14
00:04:48,280 --> 00:04:55,630
analytical about and thinking to air on any risks that we're facing?

15
00:04:56,710 --> 00:05:00,440
Yeah. You're always unconscious.

16
00:05:01,810 --> 00:05:06,100
In our experiences imagery, the.

17
00:05:11,620 --> 00:05:14,560
Connections that we've made, consciously or not.

18
00:05:15,430 --> 00:05:24,970
Meaning what is happening to us or to ones we love, or things that we are seeing in person or on the media and those risks.

19
00:05:26,050 --> 00:05:32,470
And by the way, another key piece of this is that we are not seeing not hearing about,

20
00:05:32,830 --> 00:05:41,770
not noticing is just as much part of our experience as a lot of what we're

21
00:05:41,770 --> 00:05:46,180
going to do today is to practice this thinking about lots of different risks,

22
00:05:47,070 --> 00:05:59,500
bring to bear what are the things that we're going to consciously notice, think about, attention to, and analyze what are parts of our life,

23
00:05:59,500 --> 00:06:06,520
our experiences of this that we are just not going to notice, that are going to change us the unconscious ways?

24
00:06:08,200 --> 00:06:12,550
This is the task that I'm asking you to do for your first assignment.

25
00:06:13,000 --> 00:06:24,050
Just do next week. I've given you a specific list to work with and you're going to have to think through that task.

26
00:06:24,140 --> 00:06:29,990
What are the inputs, not the output of the inputs into our risk perception systems.

27
00:06:31,280 --> 00:06:38,180
What are we gonna consciously pay attention to? And what are we going to unconsciously be affected by?

28
00:06:39,170 --> 00:06:46,700
We go through a variety of examples, and all I want you to do for that next assignment is basically do the same process on another level.

29
00:06:49,040 --> 00:06:54,500
Why am I harping on this? Ultimately, this connects to what we were talking about.

30
00:06:56,060 --> 00:07:03,469
My class and we'll talk about at the end of today in terms of the questions of thinking about the dual nature of our risk,

31
00:07:03,470 --> 00:07:11,300
perception is essential to us thinking about what our goals are as communicators, what are we trying to accomplish?

32
00:07:11,720 --> 00:07:22,610
Because we have to ask the question, what are we trying to accomplish analytically and what are we trying to accomplish experientially?

33
00:07:24,240 --> 00:07:27,600
Or the realities of that individual's experience that we have to at least.

34
00:07:29,940 --> 00:07:38,740
To recognize and address. In fact, one of the things that I want to keep going back to, as many of you touched upon in your writings for today,

35
00:07:39,850 --> 00:07:50,410
is situations in which these two things don't agree, and that misalignment ends up creating all kinds of problems.

36
00:07:51,580 --> 00:08:00,820
So so there's going to be a lot about examples. I've got some of my own personal and some research ones, etc., but that's the general theme.

37
00:08:01,870 --> 00:08:15,610
And to get us started from start to. And you were talking about a friend of yours kind of vaping.

38
00:08:15,940 --> 00:08:21,070
Yeah. So risks of vaping. Let's start thinking about what?

39
00:08:21,160 --> 00:08:22,959
First, let's share a little bit of the background.

40
00:08:22,960 --> 00:08:28,860
But you started talking about both sort of conscious things and unconscious things in the context of that.

41
00:08:28,870 --> 00:08:31,930
Basically like to think for a while.

42
00:08:32,080 --> 00:08:38,440
Yeah, she kind of knows the consequences, like her cost response, like if we have to bring it up is like, Oh, I know.

43
00:08:38,440 --> 00:08:45,690
Like I know it has these side effects. I know it comes with this. I know that I know this for all I know that deciding to do it.

44
00:08:46,240 --> 00:08:52,040
And she's like, Well. So I want to and it's almost like she measures her race did win the thing you

45
00:08:52,040 --> 00:08:57,499
do and she sees the race but she doesn't think it's like closely affected.

46
00:08:57,500 --> 00:09:01,580
So maybe yeah. Or she sees it as like a fight.

47
00:09:02,180 --> 00:09:05,940
I think we talked about this last week like but it's not immediate yet, so she doesn't like that.

48
00:09:05,970 --> 00:09:10,940
So take action versus like we people who don't actually do it, see how detrimental it give me.

49
00:09:11,300 --> 00:09:15,770
And because she's been doing it so long, I this is like more like an addiction now.

50
00:09:15,770 --> 00:09:19,100
So she's not just gonna, like, kill her own, like, she claims she will.

51
00:09:19,820 --> 00:09:24,830
So, I mean, I don't want to minimize the addiction component in baby like that's that's a real thing.

52
00:09:24,830 --> 00:09:28,610
But I'm going to put that aside for the moment because already and what you've talked about here,

53
00:09:29,000 --> 00:09:32,990
we can see both cognitive and experiential components of of risk perception.

54
00:09:33,620 --> 00:09:43,500
So let's talk about the cognitive ones. She already knows that the scientific evidence is that there are health effects associated with it.

55
00:09:43,740 --> 00:09:48,420
So if you ask what is their risk, the answer is yes.

56
00:09:50,130 --> 00:09:57,030
And that could be because she has herself, you know, done research.

57
00:09:57,030 --> 00:10:00,479
It could be because she's observed news reports or other kinds of things.

58
00:10:00,480 --> 00:10:03,900
Saying, documenting. Here is what the science says about the risks, etc.

59
00:10:07,200 --> 00:10:11,100
Maybe there are statistics involved in there. Who knows? But that's the cognitive side.

60
00:10:11,550 --> 00:10:16,350
So those what you're saying here, is it that cognition exists?

61
00:10:17,560 --> 00:10:22,360
And yet it doesn't change the behavior. So what is the experiential side of this?

62
00:10:24,750 --> 00:10:29,530
So. Key question is anything that happened to your project?

63
00:10:33,740 --> 00:10:41,540
Engaging in the behavior. What happens is we unconsciously experience either bad things happen or they don't happen.

64
00:10:41,870 --> 00:10:50,690
In the case of your friend, nothing bad has happened. So we engaged in this risky behavior, which cognitively she knows is risky.

65
00:10:51,110 --> 00:10:54,770
And I think that does occur. It's not rocket science here.

66
00:10:55,670 --> 00:10:58,670
Like we tried something risky. It didn't turn out badly.

67
00:10:58,670 --> 00:11:03,350
Hey, must not be actually as busy as I thought it was. There's no cognition there.

68
00:11:04,100 --> 00:11:12,290
This isn't conscious, but it's learning. If every day, every vape, every hour is part of that learning process.

69
00:11:14,420 --> 00:11:18,740
So that's one piece of it. What else is going on here?

70
00:11:20,330 --> 00:11:24,110
Why did you start in the first place? Yes, she's dealing with something.

71
00:11:25,250 --> 00:11:28,310
Sorry. Say it again. I was dealing with something. Yeah.

72
00:11:29,360 --> 00:11:34,640
So there are some experiential, positive things here.

73
00:11:36,050 --> 00:11:40,220
Like, remember another piece of what came up in the article that you read about?

74
00:11:40,730 --> 00:11:47,960
I think emotion is the idea that risk and benefit are not two different things that are weighed.

75
00:11:48,560 --> 00:12:00,880
There are two sides of the same coin. So vaping has positive experiences, whatever they may be, and we need to get into it.

76
00:12:01,510 --> 00:12:13,530
But those positive experiences change the emotional component so they associate positive things with that behavior.

77
00:12:14,120 --> 00:12:26,770
Now that because of the way we think about risk and the positive emotions associated with them, we tend not to think of it as having risk.

78
00:12:27,220 --> 00:12:31,060
So that means that unconsciously there's going to be this push.

79
00:12:31,060 --> 00:12:38,170
The more that there are positive associations with that behavior I enjoy, it makes me feel good.

80
00:12:38,170 --> 00:12:42,170
It helps me relax, whatever. More.

81
00:12:42,280 --> 00:12:47,080
There will be, again, an unconscious suppression of feelings of risk.

82
00:12:49,690 --> 00:12:58,990
That experience is real. So even without the addiction component and without the chemical addiction component, there is going to be a feedback loop.

83
00:13:00,600 --> 00:13:06,390
I did the thing. Not only did nothing bad happen to me, well, I got positive experiences out of it.

84
00:13:06,400 --> 00:13:15,549
I want to do that again. Part of this breakdown was the time component I and already brought this up like there may be no risks now,

85
00:13:15,550 --> 00:13:21,730
but if the risks are going to happen in the future, we have a problem. Another part of the problem here is.

86
00:13:22,750 --> 00:13:26,050
All of these things are probabilistic. So.

87
00:13:29,000 --> 00:13:35,500
It didn't happen to me, but it didn't happen to somebody else. So I happened to be the lucky one.

88
00:13:35,950 --> 00:13:41,140
Great. But that doesn't mean there's no risk there. The risk is a population construct.

89
00:13:42,940 --> 00:13:46,960
I wanted to start with your example because I think it's a really good one, very familiar.

90
00:13:47,620 --> 00:13:48,940
There's lots of these out there,

91
00:13:49,270 --> 00:14:02,020
lots of different kinds of situations where we might push back because these two different sides of us are feeling different things.

92
00:14:02,560 --> 00:14:05,040
So to go back to more examples,

93
00:14:06,430 --> 00:14:16,660
Sydney Pitbulls so you were talking about a community that had a pit bull ban and it was thinking about the risks associated with football.

94
00:14:17,890 --> 00:14:20,920
Let's break it down in the same way. So cognitive side,

95
00:14:21,500 --> 00:14:30,520
emotional stuff just tells a story because both of these things came up and you were describing like a town hall meeting or something like that.

96
00:14:31,350 --> 00:14:38,590
And both of these things came up. Yeah. So in my hometown, there's been a pit bull ban since 2002.

97
00:14:38,620 --> 00:14:46,990
So to reach the 20 year anniversary and there was a lot of momentum and push back like the summer to reform the ban and have pit bulls.

98
00:14:47,350 --> 00:14:53,320
And I should experience pit bulls and dogs that have similar characteristics to pit bulls, which is kind of a gray area.

99
00:14:54,460 --> 00:14:57,220
But to have that removed from the ban and so there was a city council meeting

100
00:14:57,700 --> 00:15:01,250
where several people and it was like Facebook Live streamed and everything.

101
00:15:01,300 --> 00:15:05,980
Several people on both sides came up to speak for the ban and also against the ban.

102
00:15:06,670 --> 00:15:10,989
But a lot of people talking about their personal experiences with pit bulls, either people,

103
00:15:10,990 --> 00:15:15,160
owner or they had like an encounter with a pit bull that they felt like their life was in danger.

104
00:15:15,280 --> 00:15:25,140
It was scary. There was like, you know, pro pit bull people were talking about kind of like how strong pit bulls are and like these,

105
00:15:25,360 --> 00:15:32,649
the potential for harming humans, socialization issues versus an Typekit bull ban.

106
00:15:32,650 --> 00:15:38,200
People talking about this is further stereotyping couples and it's not like a breed of pit bulls.

107
00:15:38,200 --> 00:15:40,240
This can happen like, you know, any large dog.

108
00:15:41,950 --> 00:15:52,270
And also the issue of like having a dog with similar characteristics and what that brings up and how they're fearful of their like dog safety.

109
00:15:52,930 --> 00:15:57,159
And ultimately the ban, it was voted to stay in place like 4 to 1.

110
00:15:57,160 --> 00:16:03,580
Decisions are still still in place. Yeah. So I want to again unpack the different components of what you brought up.

111
00:16:04,180 --> 00:16:11,560
So on the analytical side, we have various forms of logic and statistics.

112
00:16:12,550 --> 00:16:15,220
So we've got statistics working on both sides here.

113
00:16:15,640 --> 00:16:25,120
So we've got statistics being brought up of the number of incidents that occurred or the severity of those incidents we have descriptions of.

114
00:16:28,160 --> 00:16:36,290
You know, patterns of behavior of a population, but you also have logic and sort of questions of symbols.

115
00:16:36,290 --> 00:16:41,450
And then I'm going to I'm intentionally sort of oversimplifying, but that's the way this works.

116
00:16:41,870 --> 00:16:46,939
Like pit bulls are bred to be aggressive. Kinds of statements are logical statements.

117
00:16:46,940 --> 00:16:48,380
That's analytical thinking.

118
00:16:48,770 --> 00:16:59,600
I'm making an analytical argument out of a belief, right or wrong, about how these dogs were bred, what kind of temperament they have, etc.

119
00:16:59,900 --> 00:17:04,729
By the way, on the opposite side you have the same analytical thinking as if for somebody who comes up and says,

120
00:17:04,730 --> 00:17:07,790
No, no, no, it's not about the genetics. It's about how they were raised.

121
00:17:07,790 --> 00:17:10,550
It's about the environment they were in. So one can say, look,

122
00:17:11,930 --> 00:17:19,370
we don't need to ban is what we need to do is to make laws to make sure that they're treated appropriately and when treated appropriately,

123
00:17:19,370 --> 00:17:28,380
they are safe. That is another analytical, logical argument to say in this environment, under these circumstances, that the risk is much less good.

124
00:17:28,760 --> 00:17:35,930
I'm not arguing the truth or or untruth of these statements, but notice that you can make analytical arguments on both sides.

125
00:17:37,310 --> 00:17:45,170
Grounded in logic, facts, or at least believed blacks, Hispanics, those kind of thing.

126
00:17:46,100 --> 00:17:51,440
But also, Sidney, you brought up different kinds of areas that are much more experiential.

127
00:17:52,190 --> 00:18:00,379
So people who have had dog attacks, whether they were pit bulls or not, and the emotions of that and the experiences of that,

128
00:18:00,380 --> 00:18:08,980
etc., people have had on pit bulls for years and had wonderful experiences with all the cancer loving experiences.

129
00:18:09,010 --> 00:18:19,080
Great their experiences. We need them to have the belief that they're generalizing.

130
00:18:19,090 --> 00:18:26,469
They haven't had a one off or maybe whatever their experiences with this small set of dogs are leading them to generalize,

131
00:18:26,470 --> 00:18:29,980
to believe that these dogs are not dangerous.

132
00:18:30,940 --> 00:18:37,690
Other people may never have interacted with a pit bull in their life except for one moment where maybe one of them,

133
00:18:38,230 --> 00:18:43,630
and all of a sudden that's their only experience. I think they're taking that experience perception of risk.

134
00:18:46,240 --> 00:18:51,100
So. Okay. I'm not dictating what the answer is here.

135
00:18:51,280 --> 00:18:59,319
I just want you to notice both the analytical and the experiential side of being brought up in this wrestling with.

136
00:18:59,320 --> 00:19:04,360
Is this an appropriate risk to take or not? Which is essentially what the policy decision was.

137
00:19:06,110 --> 00:19:10,520
That kind of discussion happens within each one of us for all of these kinds of things.

138
00:19:11,660 --> 00:19:16,400
And then the question becomes, well, okay. Where we're going to go is okay, if this is what we are?

139
00:19:17,750 --> 00:19:25,910
What is our job as communicators? How do we interface with people who are both cognitive emotional?

140
00:19:26,990 --> 00:19:34,530
Experiential. Analytical. All of these things. All of us. Next words.

141
00:19:36,260 --> 00:19:40,310
Isabella, are you okay? All right.

142
00:19:40,360 --> 00:19:46,080
I'm trying to get the faces with the Malcolm. So you were talking about how I understood this, right?

143
00:19:46,100 --> 00:19:55,410
Your Facebook child support group. So what's it really like to thousands and the different experiences and the things that came up in that?

144
00:19:55,940 --> 00:20:03,110
It's interesting. We think about that as a location where people are intentionally coming together to talk about the risks.

145
00:20:04,040 --> 00:20:07,820
So I'm curious, talk a little bit more about what you observed there. Yeah.

146
00:20:07,830 --> 00:20:14,780
So back when COVID started early in the summer of 2020, it was getting pretty bad in my hometown.

147
00:20:15,770 --> 00:20:17,690
So like a lot of people were getting sick that I knew.

148
00:20:18,290 --> 00:20:27,230
And so my mom invited me to this Facebook support group just to, like, get information on how people heal faster, like to use herbs, stuff like that.

149
00:20:29,330 --> 00:20:36,200
And then a couple of weeks later, my parents got sick and I kept going on this page to see what other people's experiences were.

150
00:20:36,680 --> 00:20:44,000
And it was pretty bad, actually. Every time that I logged on, there was someone either dying or in the hospital on a ventilator.

151
00:20:44,300 --> 00:20:52,710
So, like, a lot of bad experiences. And so the longer I stayed on this group, the more I believed that this was going to happen to my parents also.

152
00:20:52,970 --> 00:20:57,170
I was like, I need to get out of this group, cause it's like that energy. So yeah.

153
00:20:58,440 --> 00:21:02,880
So notice what happened here. What did we do? I faced a risk.

154
00:21:04,290 --> 00:21:07,560
Went to find others who had faced that risk.

155
00:21:08,310 --> 00:21:15,270
The fact that they weren't family matters is irrelevant. They were people who were going through the process of facing this particular risk.

156
00:21:16,140 --> 00:21:19,320
This is very different than large scale statistics.

157
00:21:19,470 --> 00:21:24,270
This is about public health representative samples. This is whoever happens to be on the Facebook group.

158
00:21:26,170 --> 00:21:30,730
We're drawn towards that. What happened to you kind of reaction?

159
00:21:31,330 --> 00:21:37,990
And as you were talking about, you keep observing negative outcomes occurring to the people who are associated with this group.

160
00:21:38,170 --> 00:21:42,639
What does that do to your risk perceptions? You perceive that the risk is severe.

161
00:21:42,640 --> 00:21:45,910
You perceive that it's more likely that your parents are going to die from this.

162
00:21:46,780 --> 00:21:55,870
Is that an accurate perception of the overall likelihood? You know, looks like it depends upon how representative the group is.

163
00:21:56,260 --> 00:22:00,370
People who are actually talking in this Facebook group, maybe they're all super healthy people,

164
00:22:00,370 --> 00:22:04,330
in which case it wouldn't be representative in one direction. Maybe it's the most sickest people.

165
00:22:04,900 --> 00:22:13,090
Either way, you're generalizing from this small sample of individual experiences and using the experience over here.

166
00:22:13,090 --> 00:22:17,170
But because this is not cognitive, this is not you saying, well, of course,

167
00:22:17,530 --> 00:22:20,940
because this person looks like my dad, they're going to have the same thing happen to them.

168
00:22:20,950 --> 00:22:24,669
No, no, no, no, no, no. This is that experiential learning.

169
00:22:24,670 --> 00:22:30,460
Except if it doesn't happen to us, the next generation of experiential learning is it happens to somebody who I know individually.

170
00:22:32,500 --> 00:22:39,489
So this is the well, I won't do that surgery because my aunt friend's, you know, sister had that surgery and it turned out poorly.

171
00:22:39,490 --> 00:22:50,140
So I definitely can't do that. I have my own story as I went through a transplant.

172
00:22:51,730 --> 00:22:55,559
This was free Facebook. Yes.

173
00:22:55,560 --> 00:22:59,700
I'm that old. So we had listservs.

174
00:23:00,670 --> 00:23:10,750
Of patients and we enjoyed what the follow the life experiences of other patients who were going through the same process.

175
00:23:11,950 --> 00:23:17,379
And it's not there's no learning that can happen there. But it's not representative.

176
00:23:17,380 --> 00:23:20,380
And it's very much of this experiential learning, not according to.

177
00:23:22,160 --> 00:23:29,520
So notice this isn't just that you learn experientially, General.

178
00:23:30,590 --> 00:23:36,110
It's that we want to learn that we seek out other people who are in the same situation.

179
00:23:36,920 --> 00:23:43,220
And we think they're like me, and therefore what happens to them might happen to me, good or bad.

180
00:23:47,270 --> 00:23:50,660
It's another form of this experiential learning that's a really powerful one,

181
00:23:51,080 --> 00:24:00,080
because what that means is if I see you and I see you as different from me, then your experiences may not inform my risk perceptions.

182
00:24:00,080 --> 00:24:08,930
But if I see a similar to me in whatever way that is, that might be surface features,

183
00:24:09,710 --> 00:24:16,190
that might be diagnoses, that might be location, that might be job, whatever.

184
00:24:18,020 --> 00:24:22,310
I see you as like me that I'm going to expect what happened to you is going to happen to me.

185
00:24:25,420 --> 00:24:32,770
And sometimes that's a very powerful good thing. Sometimes it's an awful thing from the standpoint of motivating, productive,

186
00:24:33,130 --> 00:24:38,950
creative that we are aware of, that similarities can be a powerful influence on our behavior.

187
00:24:44,320 --> 00:24:49,310
Let's do one more show.

188
00:24:49,380 --> 00:24:53,710
When? We're talking about antibiotics.

189
00:24:54,770 --> 00:25:02,629
Overuse of antibiotics. And there's a similar story we can tell about cognition and emotion when we think about the risks of antibiotics.

190
00:25:02,630 --> 00:25:07,760
And I want to actually extend what you're talking about to another example, but I'll talk about what you were talking about first.

191
00:25:08,690 --> 00:25:11,930
I come from a different background. I am Chinese.

192
00:25:12,020 --> 00:25:18,610
So back in, I remember when I was a kid, when I might have said or have like four.

193
00:25:18,620 --> 00:25:24,200
When I go to the hospital, I almost always I get antibiotics, injections.

194
00:25:25,040 --> 00:25:27,140
So the light after a while,

195
00:25:27,470 --> 00:25:37,850
there's more studies coming out saying that if we keep getting like broad spectrum antibiotics for small little diseases such as flu,

196
00:25:38,240 --> 00:25:41,660
it might build up like extra resistance.

197
00:25:42,590 --> 00:25:52,280
So since then, people started like the doctors start to like decrease prescribing or be more cautious about prescribing antibiotics.

198
00:25:53,720 --> 00:25:58,380
So. Think about the risk. The risks associated with.

199
00:25:59,580 --> 00:26:03,540
Antibiotic use notices. Actually two critical risks here.

200
00:26:03,900 --> 00:26:15,100
There is the risk of whatever the infection is that the patient is facing, which is immediate and symptomatic usually.

201
00:26:15,120 --> 00:26:17,280
So people are feeling it.

202
00:26:18,180 --> 00:26:28,170
And now we're talking about the risks associated with antibiotic overuse, which is diffuse across many different people in future,

203
00:26:29,460 --> 00:26:32,970
something that's not affecting us now but may affect us greatly in the future.

204
00:26:34,530 --> 00:26:41,500
And. That means that we're likely to pay more attention to the antibiotic risk now than we are to the future.

205
00:26:42,940 --> 00:26:49,629
But analytically, the scientific studies, we find out, Hey, there is this resistance building up.

206
00:26:49,630 --> 00:26:52,570
Hey, we don't we shouldn't be overusing broad spectrum antibiotics.

207
00:26:53,590 --> 00:27:01,239
Additively We have one set of learning statistics, science studies saying, hey, antibiotic overuse is something that we need to avoid,

208
00:27:01,240 --> 00:27:08,410
etc. And then we have the experience of why did anything bad happen to you when you had those broad spectrum antibiotics?

209
00:27:08,860 --> 00:27:12,270
No. So why not keep doing it right?

210
00:27:12,510 --> 00:27:16,770
That is the level of the experiential learning. You did the behavior.

211
00:27:17,070 --> 00:27:23,950
It turned out great. You got better. Whatever it happened to you because of your initial prescription went fine.

212
00:27:24,780 --> 00:27:30,990
Why not keep engaging in this behavior? Now we have an experiential learning of this is a positive thing.

213
00:27:31,380 --> 00:27:34,920
We put it in contrast to an analytical belief that, Hey, we shouldn't do this.

214
00:27:37,370 --> 00:27:42,100
That's a story we're going to hear a lot. I think that happened.

215
00:27:42,120 --> 00:27:48,040
Why can't I keep doing this? I can't keep driving my gas guzzling car when I have broad spectrum antibiotics.

216
00:27:48,060 --> 00:27:54,670
Why can't I do X? And so.

217
00:27:57,290 --> 00:28:07,460
Notice that our role in the communication context is usually we're the one championing the analytical side.

218
00:28:11,000 --> 00:28:12,890
In contrast to people's experience.

219
00:28:15,120 --> 00:28:29,200
And if you remember reading through the article about analytical, emotional learning, you put these things in contrast who usually wins in emotion?

220
00:28:30,800 --> 00:28:37,520
You're fighting an uphill battle because we can make cognitive arguments all day long.

221
00:28:37,520 --> 00:28:41,270
But if the experience has come back, we have exactly the experience of science print.

222
00:28:43,610 --> 00:28:51,890
We know that antibiotic overuse as it doesn't mean it's easy for us to go ahead and not do much good.

223
00:28:52,460 --> 00:28:58,910
By the way, this isn't just a case thing. I remember getting broad spectrum antibiotics when I was younger, much more commonly than we do now.

224
00:29:00,860 --> 00:29:04,250
This has been a shift globally in terms of the way in which we deal with this risk.

225
00:29:05,480 --> 00:29:10,570
Know, let's let's generalize this to something even more salient than the public health world right now.

226
00:29:11,880 --> 00:29:20,130
Think about your own personal perception of the likelihood of becoming addicted to opioid painkillers if you had painkillers after surgery.

227
00:29:22,120 --> 00:29:29,290
This is a conversation that happens every day and it goes in for an appendectomy or tonsillectomy or whatever,

228
00:29:29,650 --> 00:29:34,810
and they're going to get prescribed a surgery or operation, rather.

229
00:29:35,890 --> 00:29:39,040
How do we think about this? What do we feel about it?

230
00:29:41,100 --> 00:29:44,670
So analytical side. What do we know about opioids?

231
00:29:48,190 --> 00:29:53,040
Highly addictive. Highly addictive. What else is patent?

232
00:29:53,490 --> 00:29:57,510
Sorry. Relieve pain. They are effective at relieving pain.

233
00:29:57,540 --> 00:30:02,130
So notice already we have benefit and risk.

234
00:30:03,870 --> 00:30:11,100
Okay. What else? The prescribed a lot like they're commonly used so as a statistic in terms of the public health standpoint.

235
00:30:11,310 --> 00:30:20,040
They are commonly used. They are regular. They are many, many people are potentially affected by the controlled substances.

236
00:30:20,280 --> 00:30:24,300
So there are laws, rules in place for how they are used.

237
00:30:24,540 --> 00:30:28,770
Because. And what do we learn from that? They must be risky if we have to have rules about them.

238
00:30:30,930 --> 00:30:35,550
Once they're part of an epidemic, they're part of an epidemic.

239
00:30:35,580 --> 00:30:39,700
So they are. I'm going to reframe this.

240
00:30:40,840 --> 00:30:44,230
They are a attribute.

241
00:30:45,150 --> 00:30:53,280
Because of the societal impacts that we see in terms of the epidemic to the specific drugs that have been prescribed.

242
00:30:53,880 --> 00:30:59,100
We don't attribute that epidemic to social factors. We don't attribute it to income disparities.

243
00:30:59,100 --> 00:31:07,320
We don't attribute it to lots of other things. We attribute it to this causal factors like causal belief that causes be.

244
00:31:08,900 --> 00:31:13,400
Analytical thinking is there's a risk of failure.

245
00:31:14,630 --> 00:31:21,610
Right. That's. What do you believe?

246
00:31:21,640 --> 00:31:30,820
Rightly or wrongly, once. Like what do you believe, rightly or wrongly about if you were to start taking?

247
00:31:31,510 --> 00:31:38,050
How quickly would you become effective? How likely would you be become adept at.

248
00:31:42,590 --> 00:31:46,070
I mean, the studies show you are much more likely to have a chronic issue.

249
00:31:46,440 --> 00:31:50,480
And so you have a factual belief. Yes, truth here is not the issue.

250
00:31:50,780 --> 00:31:56,749
You have a logical belief that taking a long period of time or for a chronic condition

251
00:31:56,750 --> 00:32:01,670
is different in terms of individual risk taking in the first sort of acute way,

252
00:32:01,670 --> 00:32:05,240
say, post-surgical, where you're going to get off of it very quickly. All right.

253
00:32:05,810 --> 00:32:12,530
Do you have, for example, a belief about whether or not your addiction risk is different if.

254
00:32:13,500 --> 00:32:18,690
A year ago you had a surgery and you were prescribed opioids and were on it, and then you got off of it.

255
00:32:18,690 --> 00:32:24,610
And now you're starting to get. Now go back to work.

256
00:32:25,480 --> 00:32:35,620
My dad actually, um, had a major surgery in 2008 where he still, to this day has chronic pain, and they describe him of opioids for that.

257
00:32:35,620 --> 00:32:46,329
And I don't think that I look at it. They think because I saw my dad have a chronic issue for years and he now doesn't even take me and he's fine.

258
00:32:46,330 --> 00:32:49,960
I've never seen no notice. And our store here is really powerful.

259
00:32:50,260 --> 00:32:53,409
Like, here's a person who is engaging the behavior.

260
00:32:53,410 --> 00:33:00,820
In fact, not even just engaging in behavior, but behaviors that we cognitively think of as being potentially higher risk, chronic use.

261
00:33:01,870 --> 00:33:06,760
And yet I think that's changed your perception.

262
00:33:06,800 --> 00:33:08,190
That's the experiential side.

263
00:33:10,440 --> 00:33:17,520
I was just going to speak to my grandparents who have knee surgeries and they refuse opioids because of the side effects.

264
00:33:17,720 --> 00:33:22,480
Yeah, they're very sensitive now. I want to separate.

265
00:33:22,660 --> 00:33:28,810
Just so we're clear, there is the short term side effects of these medications, and then there's the long term addiction.

266
00:33:29,350 --> 00:33:33,220
And one may refuse to use them for either reason.

267
00:33:36,910 --> 00:33:45,700
You probably have very different causal beliefs about how likely it is that somebody has to have the short term side effects versus the long term.

268
00:33:47,200 --> 00:33:50,740
Whatever those beliefs are, they're analytical, they're cognitive.

269
00:33:51,460 --> 00:33:55,210
They are associations between I believe I have X Factor.

270
00:33:56,770 --> 00:34:02,620
I or I am young or I have this particular gene or I have this particular behavior, whatever it is,

271
00:34:03,460 --> 00:34:09,370
that's an analytical belief that says I am more likely or less likely to face this.

272
00:34:10,780 --> 00:34:15,970
Which, by the way, is another. Another example of this is people who go, oh, I'm not going to die from COVID because I'm young.

273
00:34:16,720 --> 00:34:30,640
That's an analytical belief about safety. So now we get into the you brought up marijuana the issue of refusal.

274
00:34:32,230 --> 00:34:43,210
It is the transition that I was moving toward. This is a situation in which a person is choosing to not engage in a behavior.

275
00:34:44,980 --> 00:34:50,080
Which they know is placing them at greater risk of one thing here pain.

276
00:34:52,300 --> 00:34:56,930
While avoiding another. Is that appropriate?

277
00:34:58,480 --> 00:35:02,570
I'm going to flag for the moment appropriate. Here is a value judgment.

278
00:35:05,510 --> 00:35:11,960
One person might say, yes, that is appropriate. I am willing to tolerate this degree of pain to avoid this risk.

279
00:35:11,990 --> 00:35:18,710
Another person might say it's not appropriate, especially if in the context of the likelihood of that negative event.

280
00:35:19,740 --> 00:35:23,340
As small. As small as a judgment. It's always a value.

281
00:35:25,930 --> 00:35:29,620
10% small enough is 1% small enough as one in a million small enough.

282
00:35:33,310 --> 00:35:37,920
Yeah. Could be what? Really? When I'm a millionaire. If you're that one, you're still addicted.

283
00:35:37,930 --> 00:35:44,440
You still have all these negative things happen to you. So we're facing that that alignment between.

284
00:35:44,800 --> 00:35:51,070
Is this something I'm willing to accept or not? So this is I wanted to this is a great transition to where I want to go next.

285
00:35:51,190 --> 00:35:59,550
I want to give you guys an example. This is a scenario that I have used in research.

286
00:36:01,350 --> 00:36:03,960
We've probably given this to 10,000 people.

287
00:36:04,110 --> 00:36:13,770
I know exactly what's going to happen right now, but it's a really good illustration of the issues that come up when we think of politics.

288
00:36:14,810 --> 00:36:24,290
I want you to imagine for the moment that you have been diagnosed with a fairly aggressive form of colon cancer.

289
00:36:25,650 --> 00:36:29,180
I. Absent surgery.

290
00:36:30,820 --> 00:36:36,010
Colon cancer will continue to grow and will likely lead to your death in about two years.

291
00:36:38,880 --> 00:36:44,490
However, there are two surgeries available to you, each of which.

292
00:36:46,100 --> 00:36:50,540
Greatly increases your survival. The question I'm going to give you is which one do you want to pick?

293
00:36:55,720 --> 00:37:18,610
Surgery one. Has an 80% chance that it can cure, you know, long term complications of.

294
00:37:28,140 --> 00:37:32,220
However, it is not perfect. And there was a 20% chance.

295
00:37:33,510 --> 00:37:40,950
That it will not actually completely get rid of the cancer and will continue to grow and if you will, in fact, die.

296
00:37:40,950 --> 00:37:45,240
Two years is better than not doing the surgery.

297
00:37:45,420 --> 00:37:49,980
There's no question about this. Or you can do surgery to.

298
00:37:56,380 --> 00:38:05,500
Surgery to. Also. Had an 80% chance of curing you with no complications.

299
00:38:17,150 --> 00:38:23,930
It has a 1% chance it will cure you, but it will leave you with chronic diarrhea for the rest of your life.

300
00:38:39,310 --> 00:38:45,760
It's got another 1% chance it will cure you, but it's more complicated surgery and therefore it has a chance,

301
00:38:46,000 --> 00:38:49,480
a 1% chance it will leave you with a chronic wound infection.

302
00:38:49,900 --> 00:38:52,960
You will have to be managing for at least a year.

303
00:39:04,220 --> 00:39:07,490
The 1% chance that it will cure, you know, cancer will be gone,

304
00:39:07,940 --> 00:39:12,350
but you will then suffer a not from diarrhea, the opposite problem from chronic bowel obstructions.

305
00:39:12,680 --> 00:39:16,700
Where bowel becomes blocked, it becomes very painful.

306
00:39:17,720 --> 00:39:21,770
You have to do lots of things to try and move the stool through, etc.

307
00:39:28,340 --> 00:39:32,890
And 1% chance. Yes.

308
00:39:36,280 --> 00:39:42,580
If you have cured of the cancer, we can't really answer, but you will have to have a colostomy.

309
00:39:42,880 --> 00:39:50,980
Now, for those of you unfamiliar with a colostomy, a colostomy is when you have surgery to divert the gastrointestinal tract.

310
00:39:51,580 --> 00:39:55,520
So it does not, in fact, flow through the colon. Instead, it comes out of a hole.

311
00:39:55,990 --> 00:40:01,690
You have to wear a pouch. All of the stool comes out of the pouch. You change the pouch on a regular basis.

312
00:40:02,080 --> 00:40:05,530
It's smelly. It's you have to handle this on a daily basis.

313
00:40:07,480 --> 00:40:12,980
Wear it under your clothes, sir. And in this case, this would be a permanent colostomy.

314
00:40:13,000 --> 00:40:25,880
You would have the clock for the rest of your life. And then there is a 16% chance the surgery doesn't work.

315
00:40:26,660 --> 00:40:33,680
It does not cure cancer. And instead, you would be left with the cancer still growing and you would die within two years.

316
00:40:34,630 --> 00:40:40,360
That's your trade. If you want surgery, what do you want? Surgery to solve it.

317
00:40:41,410 --> 00:40:47,350
I'll give you that choice. And let's be really clear right up front, both of these are better than nothing.

318
00:40:48,190 --> 00:40:52,630
Like you're way better off picking either one of these surgeries and not having surgery.

319
00:40:54,240 --> 00:41:00,000
Which in question is this something or is this imagine like right now, are we going to be older, like in the future?

320
00:41:00,720 --> 00:41:04,560
What's going on right now? You had your.

321
00:41:04,680 --> 00:41:08,900
Imagine your face it. Yeah. Okay. That's why we get into off into the weeds.

322
00:41:08,910 --> 00:41:14,600
But are both of these surgeries very similar or is surgery one a guarantee colostomy?

323
00:41:14,610 --> 00:41:18,390
You just put it on the board like is it the same surgery?

324
00:41:19,050 --> 00:41:21,480
They are two variants of the surgery.

325
00:41:22,290 --> 00:41:30,660
These are the chances of within the population, like out of 100 people who have surgery, one 80%, 80 of them are survive and 20 of them die.

326
00:41:31,770 --> 00:41:34,830
100 people who have surgery to have this breakdown.

327
00:41:34,860 --> 00:41:39,920
80%, one, one, one, one and 16. You don't know what's going to happen to you.

328
00:41:39,930 --> 00:41:44,610
Those are those are accurate representations of the population level outcomes from these surgeries.

329
00:41:47,940 --> 00:41:52,590
Don't overthink it. What do you want? Comprehensive show hands.

330
00:41:52,620 --> 00:41:57,800
How many of you want surgery? One. And how many of you want surgery to?

331
00:41:59,990 --> 00:42:04,540
But 6040. Yeah. It's about right.

332
00:42:05,410 --> 00:42:10,930
Usually like somewhere in that room that I'm not going to make any value judgments here.

333
00:42:11,740 --> 00:42:18,020
I want to make an observation. And which one are you more likely to be alive?

334
00:42:19,650 --> 00:42:30,200
To check to. So simple question which would you rather have to be alive the rest of your life with a colostomy?

335
00:42:30,560 --> 00:42:36,890
Would it be dead 100% chance. And so you're going to say that you're going to say you choose to be dead.

336
00:42:38,390 --> 00:42:42,260
I'm not arguing with you. I've done this many times. I understand how this is going to play out.

337
00:42:42,530 --> 00:42:48,079
There's always some people who say that let's engage in what that means.

338
00:42:48,080 --> 00:42:52,129
Like that means all of the parts of your positive life, all of your social engagements,

339
00:42:52,130 --> 00:42:57,110
all the things that you could do, oh, you can go out to dinner with a colostomy bag.

340
00:42:57,120 --> 00:43:01,310
You can have sex with a colostomy bag. You can do lots of things. You can have friendships, you can play games.

341
00:43:01,410 --> 00:43:04,730
All of these positive things that you could do, you can still do.

342
00:43:05,720 --> 00:43:11,320
But these conditions. Maybe your value structure is.

343
00:43:11,330 --> 00:43:14,620
It doesn't matter. The class. That's so awful. I don't want to face it.

344
00:43:14,920 --> 00:43:23,950
Fine. But also engage with the possibility that you might adapt to it, that you might learn to live with these conditions,

345
00:43:24,640 --> 00:43:27,850
and that that would enable you to do all of these other positive things.

346
00:43:29,200 --> 00:43:36,240
Yeah, I got something that I'm thinking of is that wound infection risk with the second one because then that brings its own risk.

347
00:43:36,250 --> 00:43:43,360
If you happen to get that 1%, then now you have to face that percentage of people who passed away from wound infections.

348
00:43:43,970 --> 00:43:49,220
I could artificially shape this and say the wound infection is never fatal because

349
00:43:49,220 --> 00:43:53,180
you just have to manage it for a long period if that doesn't really matter. I mean, this is an artificial scenario.

350
00:43:53,180 --> 00:44:01,370
It illustrates the underlying point here. If we were to make this an analytical, purely analytical argument.

351
00:44:02,700 --> 00:44:07,620
What is the public health justification for picking surgery to?

352
00:44:07,950 --> 00:44:11,550
You're going to live more likely period. End of conversation.

353
00:44:12,210 --> 00:44:17,340
We value mortality really life over death. You're going to be more likely to live with surgery to.

354
00:44:18,910 --> 00:44:26,470
And yet. Every time we do this, at least 40, 50% of people take surgery.

355
00:44:26,750 --> 00:44:33,500
By the way, I have done this. I've published this many doctors as the subject.

356
00:44:34,250 --> 00:44:37,640
And the rates do change a little bit, but not that much.

357
00:44:38,780 --> 00:44:44,130
You still get. I think we had 38% of our doctors saying, yeah, I'll take surgery.

358
00:44:44,140 --> 00:44:48,310
What? This is not about training.

359
00:44:48,340 --> 00:44:51,880
This isn't about logic. This is about the fundamental fact.

360
00:44:52,990 --> 00:44:57,970
What did I do? I made you think about gross stuff.

361
00:45:00,580 --> 00:45:07,340
I need you engage emotionally. With outcomes that you didn't want.

362
00:45:08,770 --> 00:45:12,670
Notice I did not spend my time talking about death.

363
00:45:14,470 --> 00:45:19,660
It blew right past that one. What do they spend time talking about?

364
00:45:20,710 --> 00:45:24,110
Diarrhea. Blasphemy.

365
00:45:24,700 --> 00:45:29,140
Human infections. What did I do? I played to your emotional side.

366
00:45:30,370 --> 00:45:34,390
I made you engage with all of the emotions you have and the grossness.

367
00:45:35,380 --> 00:45:39,250
What did you do in return? You said honestly or unconsciously.

368
00:45:39,880 --> 00:45:43,740
Well, you know, 16% and those are pretty close.

369
00:45:43,820 --> 00:45:51,280
That's probably not that big of a deal. I just want to avoid this crap, literally, and I get to avoid it over here.

370
00:45:56,520 --> 00:46:01,440
Same play tension, by the way. Those are again, there's no value judgment here.

371
00:46:02,280 --> 00:46:08,550
I know this rule. I still have the same gut reaction of the avoidance reaction, even though I know it.

372
00:46:09,440 --> 00:46:12,880
So we are all both of these? Yeah.

373
00:46:13,350 --> 00:46:18,870
So it's kind of a philosophical question, but why do we think of death as such a bad outcome?

374
00:46:18,870 --> 00:46:29,190
Like you talk about euthanasia, like there are people who would be, in their own opinion, much happier if not suffering.

375
00:46:29,310 --> 00:46:32,630
Yes. Than being alive. So I guess the question is like.

376
00:46:33,820 --> 00:46:41,060
I know there's no right answer. Right? Like, why do we have to assume that having that 4% chance is the.

377
00:46:42,480 --> 00:46:46,290
I don't want to use the right decision, but it's a bad outcome.

378
00:46:46,500 --> 00:46:50,670
It isn't. And so I look at this from the standpoint of this is what I why I went through this.

379
00:46:50,910 --> 00:47:01,060
How did you face this choice? Right now you're up to this and this is irrelevant because it's the same.

380
00:47:03,680 --> 00:47:08,290
On both choices. And down here.

381
00:47:11,750 --> 00:47:15,410
16% of this 20 is irrelevant because it's the same over here.

382
00:47:16,620 --> 00:47:23,100
The only thing that's different between this is something that will happen to 4% of people

383
00:47:23,370 --> 00:47:27,510
because 96% of people are going to get exactly the same outcome no matter what they pick.

384
00:47:30,790 --> 00:47:37,810
What you're basically saying is, look. Why don't we assume that death is worse than this?

385
00:47:38,150 --> 00:47:46,450
And I want to say I don't assume. If. And you know, the question of where we are in our life that Sidney raised is important.

386
00:47:46,840 --> 00:47:53,980
Like, if you were talking to someone who, for whatever reason, in their stage of life has.

387
00:47:57,120 --> 00:48:00,330
Circumstances that lead them to actually prefer.

388
00:48:02,340 --> 00:48:08,310
The short, well defined. Mortality that would come from this versus the long term experience.

389
00:48:08,880 --> 00:48:14,310
I'm not going to make a value judgment and say that's wrong. We could have the philosophical argument, but that's not where I'm going right now.

390
00:48:14,970 --> 00:48:20,040
What I want you to notice is and what we see when we do this in a research.

391
00:48:20,980 --> 00:48:28,240
Is that we ask people bluntly, which would you rather be alive with the colostomy or dead?

392
00:48:28,630 --> 00:48:34,960
And 90 plus percent of them say I'd rather be alive. And then they turn around and pick surgery one.

393
00:48:36,320 --> 00:48:52,809
And that incongruity is a problem. Because it's saying that the emotionality of these things is tipping us in a way that goes in contrast

394
00:48:52,810 --> 00:48:57,010
to the preferences that I'm willing to turn right around and tell you that this is what I care about.

395
00:48:57,280 --> 00:49:02,070
This is what is more important to. Yeah.

396
00:49:02,640 --> 00:49:11,880
I just share a great example. So my significant other is a bone marrow transplant nurse and he has been for seven or eight years.

397
00:49:12,660 --> 00:49:21,720
You been this example like makes this risk like decision making look easy comparison.

398
00:49:21,900 --> 00:49:25,440
Yeah. And you'd also be surprised.

399
00:49:25,440 --> 00:49:28,860
And bone marrow transplant. The side effects are horrific.

400
00:49:29,340 --> 00:49:38,010
I mean, like, you have lesions on your skin in your throat and texture, salivary glands, and people still choose to get the transplant all the time.

401
00:49:40,260 --> 00:49:49,709
And I think it says a lot about the risk communication happening around cancer and like the fear of cancer in our society and the fact that

402
00:49:49,710 --> 00:49:56,850
people would rather do anything to get cancer or live with cancer even though they could die from the side effects of the treatment that easily.

403
00:49:58,620 --> 00:50:02,460
And I'll just expand out the cool problem.

404
00:50:03,090 --> 00:50:13,770
Okay. Caroline was, say, the core problem. We live in a society that prioritizes survival at any cost.

405
00:50:16,730 --> 00:50:24,920
We don't talk about the human cost, the physical costs, the pain, the financial cost of what we do to survive very often.

406
00:50:27,680 --> 00:50:31,190
And. That is not universally true.

407
00:50:31,710 --> 00:50:41,180
When I said I would be very clear about that, the perceptions of the value of extending life are not.

408
00:50:43,540 --> 00:50:46,630
Seen the same way in every culture around the globe.

409
00:50:48,000 --> 00:50:53,920
There certainly are many cultures for whom a shorter life,

410
00:50:53,920 --> 00:51:02,610
but one which is seen as more peaceful and more safe and more comfortable and connected with one's family is much more highly valued than,

411
00:51:03,450 --> 00:51:07,140
Oh, wait, you could get six more months if you put yourself through this health kind of experience.

412
00:51:09,530 --> 00:51:16,909
Again, that's a value judgment. I'm not going to choose to say one way or another what your values should be,

413
00:51:16,910 --> 00:51:21,110
but let's recognize that we're we're manifesting our values when we make these trade offs.

414
00:51:21,470 --> 00:51:28,800
We're saying this is more valuable to me than. Why am I emphasizing?

415
00:51:30,900 --> 00:51:39,670
I want us to pay attention to. One, the idea that we're making these trade offs to them, we're making them not very consciously meant at the time.

416
00:51:39,670 --> 00:51:43,120
We're sort of focusing on certain risks and not other ones.

417
00:51:43,540 --> 00:51:48,700
And that's a problem. And I want to go back to talking about your musings a little bit, because there are so many more examples of it.

418
00:51:49,960 --> 00:51:55,020
But. Notice. Also, Andrew's question is important.

419
00:51:58,050 --> 00:52:01,590
We are communicating with somebody and doing so for a purpose.

420
00:52:03,370 --> 00:52:09,040
What is my purpose in the way in which I chose to present this to you?

421
00:52:09,360 --> 00:52:15,190
I artificially distorted this by highlighting the emotional aspects of it.

422
00:52:15,820 --> 00:52:27,310
I knew that by having you have this visceral picture in your mind of diarrhea or colostomy, that it was going to push you away from surgery to.

423
00:52:30,560 --> 00:52:37,490
I could have changed the way in which I presented still told you those risks,

424
00:52:37,490 --> 00:52:44,570
but change the way in which I presented it to decrease the emotionality of those side effects and increase the emotionality of mortality.

425
00:52:44,900 --> 00:52:49,430
And you would have at least some of you probably would have changed your choices that way.

426
00:52:49,430 --> 00:52:56,700
They very scared the side effects of the drug commercial. Supergrass Yes, yes, yes.

427
00:52:57,860 --> 00:53:04,910
That's precisely why they do that. I notice that, by the way, the imagery, as always happens, is always positive imagery.

428
00:53:05,030 --> 00:53:09,070
Yeah, exactly. Why? Exactly?

429
00:53:09,080 --> 00:53:12,080
Because. What are you doing? You're having that positive, emotional.

430
00:53:12,080 --> 00:53:17,780
Oh, look at them frolicking on the beach while this risk information is provided, therefore can't be that bad.

431
00:53:19,760 --> 00:53:22,850
Absolutely. This is what I want us to be aware of.

432
00:53:23,000 --> 00:53:31,070
Known as the patterns of behavior in the doctors, conversations in drug ads, etc. Everybody knows this, consciously or not.

433
00:53:31,460 --> 00:53:36,050
This is the way we change what people do.

434
00:53:39,490 --> 00:53:42,730
It also means if you are going to be risk communicators.

435
00:53:43,900 --> 00:53:50,980
You are going to be manipulating. People's minds for your own purposes.

436
00:53:53,740 --> 00:54:03,680
It. We're going to be doing it on its own, what you're willing to do on what you're not willing to do.

437
00:54:06,020 --> 00:54:12,440
But I can make anybody afraid of or relieved by any number without changing the number.

438
00:54:17,250 --> 00:54:21,930
That's the power. What we're talking. All right.

439
00:54:21,990 --> 00:54:24,710
So let's go back to some examples,

440
00:54:24,720 --> 00:54:30,360
because there's lots of good examples in which these sort of tensions between cognition and emotion lead us to do different behaviors.

441
00:54:33,110 --> 00:54:38,000
Where do I want to go next? Better.

442
00:54:41,640 --> 00:54:45,030
Yeah. Thank you.

443
00:54:45,350 --> 00:54:56,750
I'm going to remember that you were talking about nuclear power plant and people who live around it, in particular contrasting it with pesticides.

444
00:54:57,090 --> 00:55:03,329
Yes. Let's talk about that. Oh, I live about five miles away from a nuclear plant.

445
00:55:03,330 --> 00:55:09,020
And it's also a big farming community. And the nuclear plant has been operating for the last 40 years.

446
00:55:09,030 --> 00:55:11,370
So it's not really anything that we ever think about anymore.

447
00:55:11,790 --> 00:55:21,390
I was asking my grandparents about it because they would have been in their twenties or thirties when it was being built and then fully operational.

448
00:55:21,930 --> 00:55:26,430
And there's another power plant that's actually about 20 miles north of where we are.

449
00:55:26,430 --> 00:55:34,739
So we're, you know, if anything bad happens, we're all done. But my grandma was talking about how around that time,

450
00:55:34,740 --> 00:55:39,809
a lot of people that they knew were coming down with like cancer diagnoses or like their

451
00:55:39,810 --> 00:55:44,700
parents were getting different diagnoses of neurological disorders like Parkinson's,

452
00:55:45,090 --> 00:55:51,900
M.S. and that sort of thing, and that they were all convinced that it was because this power plant was being built and then,

453
00:55:52,290 --> 00:55:59,700
you know, about to be fully operational. But they did not necessarily take into account the fact that all the pesticides they were using,

454
00:56:00,350 --> 00:56:04,499
you know, when they were doing their farming, because that was the primary kind of in common.

455
00:56:04,500 --> 00:56:12,000
You know, almost everyone was a farmer at that time. So they more attributed these negative effects with the nuclear plant,

456
00:56:12,000 --> 00:56:15,989
which was new and being built rather than pesticides in their farming practices,

457
00:56:15,990 --> 00:56:20,460
which they'd been doing, would lead to long term outcomes like cancer.

458
00:56:21,300 --> 00:56:30,060
So well, let's snap back to the dread and unknown known stuff like pesticides, super unknown, familiar part of their daily life.

459
00:56:30,570 --> 00:56:40,590
Lots of knowledge about them, risks, but dealt with sort of as part of the daily experience and that this is what is through their livelihood.

460
00:56:40,860 --> 00:56:49,860
Like you can't farm or they they believe that you can't far without these pesticides you had positive associations of this can't be

461
00:56:49,860 --> 00:56:58,890
that bad we've been doing this for ages verses nuclear plant new unusual thread all of those things that we talked about before.

462
00:56:59,490 --> 00:57:02,940
And the other point I want to bring up that Becca brought up very nicely.

463
00:57:04,200 --> 00:57:09,360
People get cancer, people get Parkinson's disease. It happens randomly within society.

464
00:57:10,370 --> 00:57:14,670
What happened? You have this new thing. You have this bad outcome.

465
00:57:16,220 --> 00:57:19,340
Consciously or unconsciously, we're going to associate these things.

466
00:57:20,600 --> 00:57:28,010
We're going to develop causal beliefs that say, Oh, well, it must be the nuclear power plant that's causing the Parkinson's disease.

467
00:57:29,750 --> 00:57:33,230
Even though we may not have any scientific reason to believe that.

468
00:57:34,430 --> 00:57:39,300
And. Why is that one particularly important to bring up?

469
00:57:42,170 --> 00:57:48,030
Anybody ever think about? Measles vaccination and autism.

470
00:57:51,360 --> 00:57:59,380
Same problem. Autism happens when autism happens when at the same time that kids get vaccines,

471
00:57:59,860 --> 00:58:06,729
you're gonna get just through random chance a set of people who will take their kids

472
00:58:06,730 --> 00:58:11,770
in to get vaccinated and a bad thing like autism or whatever an allergic reaction.

473
00:58:11,770 --> 00:58:15,160
But it won't matter what this is. Even if that didn't happen right after vaccination.

474
00:58:17,510 --> 00:58:22,250
What can happen to them. They're going to build the same cause, all these same work that he's talking about.

475
00:58:24,480 --> 00:58:29,740
Because it's true. That. They took their kid in.

476
00:58:30,130 --> 00:58:37,320
They did this behavior. The bad thing happened that says that truth of their life cannot be argued against.

477
00:58:37,330 --> 00:58:41,230
We can only argue about it at the population level. They experience the negative thing.

478
00:58:43,120 --> 00:58:47,860
So that's another form of this experiential learning that's very difficult for us to face.

479
00:58:48,610 --> 00:58:51,370
But it happens all the time because we look at the population.

480
00:58:53,070 --> 00:59:04,110
An individual farmer only looks at What do I do on my farm if I get another one I want to bring up in a similar vein.

481
00:59:05,160 --> 00:59:13,830
Audrey. You were talking about, like gun purchases and.

482
00:59:14,980 --> 00:59:19,240
Similar kinds of cognitions were popping up. So I want you to share your discussion.

483
00:59:20,530 --> 00:59:26,380
So I related it to the discussion and how like after the January six insurrection,

484
00:59:26,890 --> 00:59:31,170
a lot of my family was like, oh, you know, like purchase guns to protect ourselves.

485
00:59:31,720 --> 00:59:40,690
My whole thing is like, I learned that, like, having a gun is more prone to like gun violence in the home and actually, like, we're not in the class.

486
00:59:40,690 --> 00:59:42,240
So I kind of had to. Yeah.

487
00:59:42,910 --> 00:59:50,560
And another thing, it was more like, what do you think is going to happen type of question where it's like, do you think they're going to come here?

488
00:59:50,710 --> 00:59:54,580
Like, I was in California, so like, do you think they're going to come from D.C.?

489
00:59:55,930 --> 00:59:59,500
Right. So what do we have here?

490
00:59:59,740 --> 01:00:04,240
We have a visceral, emotional event occurring.

491
01:00:05,350 --> 01:00:07,750
Triggering perceptions of vulnerability.

492
01:00:10,070 --> 01:00:21,750
We have a behavior in this case buying a gun, which is marketed as and certainly can understandably be connected.

493
01:00:22,370 --> 01:00:26,000
This is a thing which creates a sense of safety.

494
01:00:29,370 --> 01:00:34,960
You have the factual knowledge that we know from a population level standpoint,

495
01:00:34,980 --> 01:00:41,280
the presence of gun within a household is associated with lots of risks, suicide risks, gun violence, gross domestic violence, risk on people.

496
01:00:41,280 --> 01:00:47,940
This goes on and on and on. That's a scientific fact which fundamentally changes your perception of the safety of that behavior.

497
01:00:48,990 --> 01:00:54,420
You perceive that behavior analytically driven by that factual knowledge to be a dangerous thing.

498
01:00:58,060 --> 01:01:03,410
We also have. The experiential learning.

499
01:01:04,690 --> 01:01:08,190
So. Family have guns.

500
01:01:08,880 --> 01:01:18,810
History of guns, use guns safely, etc. versus does this person have for their family have a history of being affected by gun violence?

501
01:01:20,860 --> 01:01:26,970
Either way, that kind of experiential learning is going to tilt them as to whether that is a safe or a not safe thing to do.

502
01:01:28,760 --> 01:01:29,900
Tension is, again,

503
01:01:30,380 --> 01:01:41,360
the analytical knowledge of the risks that come with having the gun in the house versus the sort of positive or negative mental associations,

504
01:01:41,360 --> 01:01:45,260
the emotions you have with the idea of having good enough.

505
01:01:50,790 --> 01:01:55,640
Any more. I want to hear. There's a couple of more I want to make sure we hit before we run out of time.

506
01:01:59,040 --> 01:02:03,570
I mean, you made a comment about.

507
01:02:04,530 --> 01:02:08,510
The comic is in the spotlight.

508
01:02:08,520 --> 01:02:16,780
Article The Street Capital is comic book and I wanted to make sure come back to that because it's something worth making sure that we all talk about.

509
01:02:16,800 --> 01:02:19,410
So if you could just share your thoughts. I wanted to comment on. Yeah,

510
01:02:19,800 --> 01:02:27,180
so I kind of related the comics sculpture calculus where there is like a white male and

511
01:02:27,180 --> 01:02:32,070
a black male and they're in a little bubble and they had destructiveness on each other.

512
01:02:32,310 --> 01:02:37,650
And also the little giving factors for each other, for the right look,

513
01:02:37,650 --> 01:02:45,440
the details are definitely upwards of kind of speaking about my experience as a black woman navigating

514
01:02:45,660 --> 01:02:52,440
private spaces and then also growing up with me and like wondering if any throughout my lifetime,

515
01:02:52,710 --> 01:03:02,310
if there's been any time where someone else supplement was white or just like fantasy, like saw me walking on the street or in class or whatever.

516
01:03:02,670 --> 01:03:08,220
And I kind of did that kind of mental street calculus and kind of figured out,

517
01:03:08,370 --> 01:03:17,120
did the comparison of the receptors or talking to me or acknowledging me and the mitigating factors and compare that and figure it out,

518
01:03:17,400 --> 01:03:22,170
came to a conclusion of whether you want to talk to me or like sit next to me or whatever.

519
01:03:23,370 --> 01:03:25,709
And I basically said,

520
01:03:25,710 --> 01:03:36,780
how about like the street calculus was basically the receptors in the body factors were based on stereotypes and racial stereotypes.

521
01:03:36,780 --> 01:03:47,340
So it has to be more specifically and because receptors are like, oh, the guy was wearing his cap backwards and his lower neighborhood and or the.

522
01:03:48,510 --> 01:03:53,990
I think for the Latinos, like something like, oh, like I forgot the details.

523
01:03:54,420 --> 01:04:05,790
That doesn't matter. Yeah, it was. So I yeah, I saw how it was based off of racial stereotypes with the factors or kind of just and also sexist too,

524
01:04:05,790 --> 01:04:09,060
like, oh over 40 or is this person a female or not?

525
01:04:09,300 --> 01:04:17,040
So things that are seamless with you, you're bringing to the surface something that's important,

526
01:04:17,610 --> 01:04:20,760
which is that when we're talking about unconscious associations.

527
01:04:23,680 --> 01:04:30,190
The potential for those associations to be driven by all kinds of stereotypes is enormous.

528
01:04:32,810 --> 01:04:36,889
That is so I mean, the answer to your question is, did anybody like.

529
01:04:36,890 --> 01:04:51,080
The answer is yes. Everyone to some degree is going through that process unconsciously because we have those unconscious associations,

530
01:04:51,590 --> 01:05:01,040
positive or negative, with the features that we notice, which are often surface features of individuals or situations, etc.

531
01:05:03,010 --> 01:05:07,030
Notice that this means that to some degree. What's the counter for that?

532
01:05:07,030 --> 01:05:14,620
Like when we raise that queerness to our certain consciousness, then we can say analytically, Hey, that's a stereotype.

533
01:05:14,620 --> 01:05:17,620
I'm not going to choose to behave that way. I'm going to choose to be differently.

534
01:05:18,220 --> 01:05:22,990
We're using the analytical side of our risk. Perceptions counterbalance against that.

535
01:05:24,880 --> 01:05:28,480
But we have to own that, that stereotypes. We have to own that.

536
01:05:28,750 --> 01:05:32,319
These unconscious associations come from our lived experiences.

537
01:05:32,320 --> 01:05:43,870
They come from the stories we've been told, the culture that we're immersed in, etc., and they embody all of the biases that we're talking about.

538
01:05:48,000 --> 01:05:54,080
I wanted I wanted to make sure we got to that because. On the one hand.

539
01:05:56,150 --> 01:06:02,110
We never want to say, Oh, well, that's just the way it is. And say, okay, we're just going to have to live with that.

540
01:06:02,110 --> 01:06:04,990
No, we don't have to live with that. We can do things to try and change that.

541
01:06:05,800 --> 01:06:15,370
On the other hand, we are all carrying with us the historical experiences that we've had, the association and I'll give you my example.

542
01:06:17,060 --> 01:06:20,330
Before I I tired of academics.

543
01:06:20,330 --> 01:06:27,050
We moved around the country multiple times. I grew up in elementary school.

544
01:06:29,070 --> 01:06:35,910
Hyde Park in Chicago. A very racially segregated space.

545
01:06:38,700 --> 01:06:46,200
The racial makeup of the neighborhood in which my house was was probably 80 or 90% white.

546
01:06:46,800 --> 01:06:49,170
Six blocks away was the exact reverse.

547
01:06:51,420 --> 01:07:00,930
I have I carry with me the historical associations on that point in my life of the experiences, of the connections.

548
01:07:02,330 --> 01:07:11,240
I can raise that. The conscious now is out now and say, Hey, consciously or unconsciously, I need to push back against that.

549
01:07:11,270 --> 01:07:21,790
But you're right, that street calculus occurs and when we want to talk about any kind of risk in this particular one was about violence risk.

550
01:07:21,800 --> 01:07:28,820
But if we want to talk about even something as simple as likelihood of being diagnosed with particular conditions,

551
01:07:28,820 --> 01:07:35,090
why do racial biases exist to some degree in the diagnosis processes of doctors?

552
01:07:35,360 --> 01:07:42,170
Because they carry with them the unconscious associations from that history, from their life.

553
01:07:43,310 --> 01:07:50,450
That lead to certain associations which are often incorrect, and we have to guard against that.

554
01:07:51,820 --> 01:07:56,530
We are all over this. And I want him to make sure that that point came up today.

555
01:07:58,210 --> 01:08:01,660
It's not that some people carry stereotypes and others don't know we all have them.

556
01:08:02,080 --> 01:08:07,900
It's just a question of how do we let those play out in the context of our speaking up?

557
01:08:09,400 --> 01:08:15,640
I already talked briefly about the idea that people are not necessarily emotional versus cognitive.

558
01:08:16,750 --> 01:08:20,470
That was another thing that came up in a couple of people's musings, but I really want to make sure that gets talked.

559
01:08:20,650 --> 01:08:24,309
You know, we raise this awareness. It's not that, oh yeah, so-and-so is really emotional.

560
01:08:24,310 --> 01:08:35,650
Therefore, they're no, no, no, no, no. We're both. And then you've also already talked to Max.

561
01:08:35,650 --> 01:08:43,270
I know you were talking in your musing about sort of like when we think about emotions, there's a lot of potential for people to be manipulated.

562
01:08:44,510 --> 01:08:47,270
And. Yeah, that's right.

563
01:08:49,040 --> 01:08:56,810
And that's another piece of this puzzle is that when we own how much of us is driven, our risk perceptions are driven by emotion.

564
01:08:57,380 --> 01:09:01,250
Some of you wrote about fear messaging, health messaging, political messaging.

565
01:09:01,250 --> 01:09:02,840
Yes, same problem.

566
01:09:04,060 --> 01:09:13,450
Like when we have political messaging that creates an association of particular groups as being people to be afraid of for whatever reason that is,

567
01:09:13,450 --> 01:09:19,240
manipulating our risk perceptions, elevating the experiential side of our risk perceptions,

568
01:09:19,600 --> 01:09:23,320
pushing back against analytical beliefs to create a sense of fear.

569
01:09:25,460 --> 01:09:28,760
We'll talk later on this course about when that's valuable and what it's not.

570
01:09:28,820 --> 01:09:32,750
And that's a real dangerous to try and focus on fear as a messaging strategy.

571
01:09:34,310 --> 01:09:37,550
But all of this ties in to this idea of we are both.

572
01:09:38,810 --> 01:09:44,660
So it's we've got 10 minutes left. Let's keep failing to say, you know, it's 930.

573
01:09:44,660 --> 01:09:49,190
It's time to go back to tape. Turn to your neighbor.

574
01:09:50,800 --> 01:09:54,070
It's not about the question of the day. So we are both.

575
01:09:54,370 --> 01:10:04,990
We are analytical. We are emotional. What are you taking away from this knowledge as we think about what our goals need to be as communicators?

576
01:10:07,090 --> 01:10:10,630
So think about this. Should we only be trying to work on emotion?

577
01:10:10,660 --> 01:10:14,920
Should we only be trying to give people facts? How do we do both of these?

578
01:10:14,920 --> 01:10:18,610
If not, how do we do both of these to talk about this in the terms of goal?

579
01:10:19,960 --> 01:10:23,140
Just share your thoughts for a few more minutes and then I'll pull it back. Okay.

580
01:10:32,880 --> 01:10:48,750
Yeah. Yeah, I guess it's not going to be like he's going to be like it kind of like, is this going to work in this audience?

581
01:10:49,350 --> 01:11:11,230
I think it's not like they're like you're just trying to find out whether or not know was looking at

582
01:11:13,930 --> 01:11:54,840
me like I used to work at Facebook groups and it's like I'm going to hear that body like immediately.

583
01:11:54,900 --> 01:12:14,760
Like, like like I don't understand why I collect like this because I don't I don't think that.

584
01:12:15,720 --> 01:12:29,900
That's right. I think that the other side I don't know, I guess that sort of thing, you know,

585
01:12:30,370 --> 01:12:54,540
I think for a lot of our business, I think you're just like, oh, come on, snap back and go.

586
01:12:57,420 --> 01:13:11,590
Yeah, sorry, I didn't like that.

587
01:13:13,440 --> 01:13:17,230
I don't know.

588
01:13:17,520 --> 01:13:23,260
I don't like my time. Like, here's what I want my help manipulate right now.

589
01:13:23,400 --> 01:13:26,610
Like, would you like to help? Yeah. Would we know?

590
01:13:26,610 --> 01:13:43,890
Like, whether actual, you know, purely like it's really not that bad.

591
01:13:45,810 --> 01:13:54,390
A lot of times like this, like constantly.

592
01:13:58,440 --> 01:14:08,520
I just imagine what it's like to be out there.

593
01:14:08,750 --> 01:14:38,260
And I think it's usually just like, you know, I mean,

594
01:14:39,530 --> 01:14:56,510
I don't like it's like I just feel like it's like the energy's dying down a little bit of all that.

595
01:14:57,940 --> 01:15:01,240
So what's your takeaway from this?

596
01:15:01,660 --> 01:15:07,540
What do you think this tells us about our holes as a community making the kind of movie?

597
01:15:14,420 --> 01:15:20,900
But a lot of times it's the emotional sides to trump the cognitive ones or that sort of messaging.

598
01:15:21,290 --> 01:15:25,279
I was mentioning just our conversation about smoking, um, and the like.

599
01:15:25,280 --> 01:15:28,880
You could skew numbers. People like their percentage chance of being lung cancer a lifetime.

600
01:15:29,330 --> 01:15:32,299
Um, once like, however, they put images of some of the effects.

601
01:15:32,300 --> 01:15:38,090
The stuff of ask people would die on cigaret boxes like what their lungs look like after they have lung cancer.

602
01:15:38,440 --> 01:15:41,780
Um, so they get a more visceral response from people.

603
01:15:41,930 --> 01:15:49,969
Yeah. So, I mean, there's two pieces there. One is the is the fear messaging, like those visceral pictures going to be valuable?

604
01:15:49,970 --> 01:15:52,850
Maybe, maybe not. There's real questions there.

605
01:15:54,020 --> 01:15:59,629
But the key points that you're raising that I really want to highlight here is just simply throwing analytical facts at

606
01:15:59,630 --> 01:16:06,410
people is not likely to be very effective in that kind of a situation where there's a lot of power in the emotions,

607
01:16:06,410 --> 01:16:09,530
both positive associations with the behavior, etc.

608
01:16:10,580 --> 01:16:17,659
Unfortunately, a lot of times we in public health, especially in health communications,

609
01:16:17,660 --> 01:16:23,780
get tasked with jobs that are basically let's throw facts at people.

610
01:16:26,320 --> 01:16:32,890
If you are feeling discouraged at this moment, like why are we being told to do this thing that is unlikely to be effective?

611
01:16:33,610 --> 01:16:37,060
Unfortunately, that's the reality that we need to deal with.

612
01:16:38,530 --> 01:16:42,669
So yes, that's a key piece like, okay, we can't just ignore the emotions.

613
01:16:42,670 --> 01:16:49,140
What are we going to do about that? What's all this stuff that came up that you your talking.

614
01:16:51,180 --> 01:16:58,350
I talked a little bit too about and when it comes sometimes we have to focus on the emotion even more like in my experience,

615
01:16:59,850 --> 01:17:06,180
the patients were trying to leave and they are against medical advice like that tends to be like a very emotional decision.

616
01:17:06,480 --> 01:17:11,520
And like a lot of times when you go in there, you're trying to talk to them to be like lost.

617
01:17:11,520 --> 01:17:15,480
And these are the chances of like the bad things that can happen if you leave.

618
01:17:15,660 --> 01:17:17,729
Like that's not the things they want to hear sometimes.

619
01:17:17,730 --> 01:17:22,230
Like they just want to be able to like talk and communicate and like say what their concerns are.

620
01:17:22,530 --> 01:17:25,050
And sometimes their concerns are like completely more like,

621
01:17:25,350 --> 01:17:29,429
I need to leave because I have my dog at home and like you have to try to figure out these things

622
01:17:29,430 --> 01:17:35,010
that are unrelated to what you think you're handling that are like impacting their emotions.

623
01:17:35,130 --> 01:17:45,930
Yeah. And I mean, so there's the there's my dog at home and then there's the I am so trying to avoid the negative emotions

624
01:17:45,930 --> 01:17:49,950
I am feeling right now that I have this belief that if I can get myself out of this physical space,

625
01:17:49,950 --> 01:17:56,909
that they will go away. I definitely see that this is this is a cognitive rational.

626
01:17:56,910 --> 01:18:00,060
This is driven by this space is freaking me out.

627
01:18:02,010 --> 01:18:04,889
And so maybe if I leave this space, then I will feel better.

628
01:18:04,890 --> 01:18:09,330
But of course, addressing that as let's deal with the emotions that you're feeling about the space,

629
01:18:09,330 --> 01:18:14,970
not try and pretend that we can do an analytical argument about it's going to be more risky for you to be here versus there.

630
01:18:16,150 --> 01:18:21,940
You want? Yeah, I wanted to say that with emotions like when talking with cognitive versus emotional,

631
01:18:21,940 --> 01:18:28,270
I was hearing that emotions are considered more of like this irrational thing and we talked about how it really is.

632
01:18:28,660 --> 01:18:33,850
There's always a basis for why it's a needs that you had or somebody had the experience.

633
01:18:35,860 --> 01:18:41,680
So just bringing that up, notions have a basis in reality, not just random thing happening.

634
01:18:41,950 --> 01:18:49,630
So and actually it's evolved, right? You extend your idea to something that we're talking about later in this course.

635
01:18:50,770 --> 01:18:56,830
Not only is it the case that emotions come from somewhere, they have some original source that is real.

636
01:18:57,850 --> 01:19:04,060
But emotion serves some really critical purposes in the way in which we navigate information about risk.

637
01:19:07,030 --> 01:19:18,400
A critical example. So I already raised this thing. But think about this. Death and diarrhea are two radically different things.

638
01:19:18,940 --> 01:19:24,580
And yet somehow we're supposed to be, like, doing calculus on them and combining them.

639
01:19:24,910 --> 01:19:31,110
How do we do that? I can't you know, I can't measure this in terms of our is dead.

640
01:19:31,150 --> 01:19:34,630
I know that doesn't work like that. There's no performance calculus.

641
01:19:35,350 --> 01:19:39,520
When we start to put together different health states, they are different.

642
01:19:40,330 --> 01:19:45,250
Pain is not the same thing as itching is not the same thing as physical.

643
01:19:45,250 --> 01:19:47,840
So physical ability is not the same thing.

644
01:19:48,280 --> 01:19:57,340
Like we live in a space in which we're supposed to be doing tabulations and analytical comparisons of things which are fundamentally incomparable.

645
01:20:00,200 --> 01:20:08,630
Except they all evoked emotions. Emotions are the common currency of disparate outcomes.

646
01:20:10,170 --> 01:20:18,440
And so part of the purpose of emotions is they allow us to do things that allow us to say, well, how bad with this one and how bad would that be?

647
01:20:18,450 --> 01:20:21,299
And I get the feel of something and this doesn't feel that bad.

648
01:20:21,300 --> 01:20:28,470
Then maybe I'll pick that one instead of this one where it spent a lot of time talking about How do people translate?

649
01:20:29,750 --> 01:20:38,990
Risk information data statistics into common currency emotions because they have purposes of being able to say,

650
01:20:39,700 --> 01:20:42,820
Oh, what do I feel about the blood pressure?

651
01:20:42,860 --> 01:20:51,740
140 over 90. Oh, but if you tell me that that blood pressure is borderline high,

652
01:20:52,070 --> 01:20:59,710
I have an emotion and that emotion motivates behavior, or at least something that I can balance that has value.

653
01:21:01,250 --> 01:21:08,690
So that's it for the that's that's the particularly about experience of risk.

654
01:21:10,370 --> 01:21:14,660
And I really want to dove into the experience of whether you feel at risk.

655
01:21:16,250 --> 01:21:18,490
Please feel free to bring in the knowledge story.

656
01:21:18,530 --> 01:21:30,740
These situations, if you have felt either a sense of safety or were risk senses of risk, when you do perhaps that you work on the customer.

657
01:21:31,730 --> 01:21:52,390
Talk about that experience with other thoughts about being able to access the virus in the same program.

658
01:21:53,900 --> 01:22:24,160
I mean, know, I've told you that most people I know when I get my oh yeah, I think it goes on the record.

659
01:22:24,530 --> 01:22:27,049
Like I was feeling I'm just double up here.

660
01:22:27,050 --> 01:22:40,350
I'll take and take them with some the other stuff another I thought I was but I went through for another 4 hours from like 60.

661
01:22:40,730 --> 01:22:51,040
You think about all the machines in my room and I'm the older you get, the more that you see.

662
01:22:51,080 --> 01:23:04,280
So women don't feel it will help us to help you just so you're like I'm just like, oh, but whatever the calculation is, it's we exit versus.

663
01:23:05,960 --> 01:23:12,230
So I think that it has it in a particular form a lot more.

664
01:23:13,830 --> 01:23:22,970
You know, there I see a lot of positives and a lot of I'm saying, hey, I have cancer is a positive answer.

665
01:23:23,510 --> 01:23:33,560
That's right. No idea because I don't know what your next class will be.

666
01:23:35,270 --> 01:23:44,030
I guess I can talk about different behaviors, behavior or like full of feelings.

667
01:23:44,300 --> 01:23:56,930
I think I feel like my in my emails like that I mentioned I don't think you mentioned this one.

668
01:23:57,050 --> 01:24:04,670
Obviously it's with you need to stay in good shape those value structures he had just talked about us

669
01:24:04,670 --> 01:24:10,940
reading that article that you're saying the kids that you and I like before because we couldn't go to that.

670
01:24:11,650 --> 01:24:15,170
And then he attached a very powerful other thing.

671
01:24:15,170 --> 01:24:21,860
I thought it was the chapter thing at first, but there are personal beliefs that may or may not be aligned with.

672
01:24:22,130 --> 01:24:26,510
High society poses its own values structures. So just read one of Bloomberg.

673
01:24:26,840 --> 01:24:30,250
They also create example or pages.

674
01:24:30,740 --> 01:24:41,840
In the larger society industry, pricing is generally framed as negative in certain subgroups, not defined by all the fighting for an individual think.

675
01:24:42,100 --> 01:24:50,630
I don't think this thing was negative. I don't think I don't have strong I want to separate it when we're talking about it here because.

676
01:24:51,620 --> 01:24:58,539
One of the key things that really struck a lot of viewers is not to assume that we know the value that somebody.

