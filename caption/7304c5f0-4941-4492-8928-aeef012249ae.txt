1
00:00:01,500 --> 00:00:06,920
I think it's like everything I'm asking you.

2
00:00:08,940 --> 00:00:15,000
What about you? I like that you had one more day.

3
00:00:15,450 --> 00:00:29,290
Oh, great. There is probably when she looks at me like I know the reason I sit in the front of all the clubs

4
00:00:29,700 --> 00:00:49,110
generally is that I was with my father and I know it's like it's not good for us to do the same thing.

5
00:00:50,550 --> 00:01:01,950
I can kind of see what it's like kind of adjustment focused to really try to do what's right.

6
00:01:01,980 --> 00:01:17,580
I think anyway, it's just the way I prefer to be because without my mom I know that there's nothing going forward,

7
00:01:17,580 --> 00:01:23,489
but I can't see you really read the book in front of me.

8
00:01:23,490 --> 00:01:42,350
And I don't know if you would like to have like a family near and far, mostly just like, oh, such an extent that goes as far as like that.

9
00:01:43,080 --> 00:02:08,960
And she actually makes me sound like something other than and then like, I know he's doing things like, I don't know, that's what it was like.

10
00:02:10,470 --> 00:02:14,470
Yeah. I didn't have anything to do.

11
00:02:18,550 --> 00:02:29,340
Yeah. I think that stuff, I mean I literally come through, you know, I've been doing it for 20 plus years.

12
00:02:29,550 --> 00:02:33,490
I just have a feeling like I do notice a joke.

13
00:02:33,790 --> 00:02:39,560
Oh, my God. Yeah. I'm trying to get the ball from something.

14
00:02:39,860 --> 00:02:44,910
So between the central campus, more and more interested in talking about that,

15
00:02:45,420 --> 00:02:53,520
there's like a slight difference in the center campus for I noticed because I get the ones I hear

16
00:02:53,540 --> 00:03:04,930
that I specifically and it's like I can't talk to me because I really look for the blackboard.

17
00:03:05,700 --> 00:03:13,770
Like a big place on the bottom page.

18
00:03:14,670 --> 00:03:17,700
Yeah, we're on it.

19
00:03:17,700 --> 00:03:29,160
That might have a lot of like, oh, oh, oh, like, messing everything to be able to see me now.

20
00:03:29,180 --> 00:03:42,060
I can't wait to see those last hour.

21
00:03:42,570 --> 00:03:47,610
Something in your heart. Can you tell us from Lake Michigan?

22
00:03:49,110 --> 00:04:04,380
Oh, yeah. Oh, yeah, yeah, yeah.

23
00:04:09,630 --> 00:04:17,410
Oh. All right, let's get started now.

24
00:04:18,360 --> 00:04:22,589
I suspect people will be struggling in traffic and certainly worse than I have been previously.

25
00:04:22,590 --> 00:04:40,020
So before we get into today's discussion through some of the questions on the covers and times of possibility versus probability communication,

26
00:04:40,710 --> 00:04:48,480
I want to take a second and go back to your assignment to put up a couple examples from previous years.

27
00:04:48,480 --> 00:04:54,150
This one, all of these are anonymous. There's some from students in different years, etc.

28
00:04:54,780 --> 00:04:59,820
I just want to take a couple of minutes and walk through them just so that you get a chance to see.

29
00:04:59,910 --> 00:05:10,980
My thinking for what it is that I was really noticing in terms of different ways of accomplishing the goals of these test results communications.

30
00:05:11,820 --> 00:05:15,720
Again, I wasn't happy to talk with people about your own examples.

31
00:05:15,730 --> 00:05:20,370
I've already had a couple of these conversations. So if you do this and you're like, Well, wait, didn't I do that?

32
00:05:20,370 --> 00:05:23,160
Or What about this? Etc., I'm happy to engage in that conversation.

33
00:05:24,030 --> 00:05:28,280
But this is a skill set I really want to make sure you have and you feel comfortable with.

34
00:05:29,250 --> 00:05:37,440
It will come up again and again and again. In practice, you're going to be communicating numbers, whatever your audience is they're not familiar with.

35
00:05:37,440 --> 00:05:50,300
And I want to be make sure that you're really feeling comfortable with this. Here's the first one.

36
00:05:51,650 --> 00:05:56,570
And one of the things I want to note here is there's not tons of text.

37
00:05:57,560 --> 00:06:01,129
You don't actually have to have lots of information to accomplish.

38
00:06:01,130 --> 00:06:13,970
The core things I like about what math for reference levels that are clearly marked.

39
00:06:15,680 --> 00:06:20,220
If you actually want to make it bigger. Yeah. I would be happy to.

40
00:06:28,780 --> 00:06:35,560
The BPA action level at 15, which many people used in their graphics on their phones in some form.

41
00:06:36,430 --> 00:06:45,610
And you the FDA limit for bottled water. Here is five. What I appreciate about this is the calibration of the color coding to these.

42
00:06:46,950 --> 00:06:51,470
So the EPA limit for bottled water is yellow.

43
00:06:52,150 --> 00:06:55,500
It's not good. Okay. I'm fine.

44
00:06:55,650 --> 00:07:00,299
When you're at five grams, this is the maximum amount that you're allowed to have in bottled water.

45
00:07:00,300 --> 00:07:04,900
But we actually really don't want to have that. And so even though it isn't labeled, I mean,

46
00:07:04,900 --> 00:07:11,350
the only thing I would nitpick on this is I would love to have sort of like gold level or safe level or something down here at zero.

47
00:07:11,770 --> 00:07:14,920
It really reinforces the idea that no level of planning is safe.

48
00:07:16,330 --> 00:07:23,110
But as simple as this is, it's relying upon that red, yellow, green color scheme to communicate most of it to me.

49
00:07:25,000 --> 00:07:31,960
Witness your bright yellow. And if you're colorblind, this would be weaker.

50
00:07:32,500 --> 00:07:36,480
Although the positioning is not bad. There is a lot of redundancy here.

51
00:07:36,490 --> 00:07:39,880
You could still get most of it. That's that's one of the weaknesses of it.

52
00:07:40,950 --> 00:07:44,700
But. I like this. It's got an arrow at the top.

53
00:07:44,700 --> 00:07:49,800
It shows that you can go forward. Actually orienting vertically is not crazy in this context.

54
00:07:51,890 --> 00:07:56,210
Like Moore here is bad and that's really clearly conveyed.

55
00:07:58,160 --> 00:08:16,970
So this is simple, but it accomplishes most of its goals. I make this even bigger.

56
00:08:26,170 --> 00:08:30,990
So what I like about this one is. First of all, the scale.

57
00:08:33,080 --> 00:08:38,840
It would be nice if they actually said this is zero, although it's pretty clear that it must be zero given this basic.

58
00:08:40,000 --> 00:08:49,690
The spacing is perfect. The spacing up here, 200, 500, and maybe this needs to be a little bit longer, but it's close.

59
00:08:50,050 --> 00:08:56,550
I really pay attention to the scaling here. You have an arrow up at the top.

60
00:08:57,060 --> 00:08:58,230
What I like is that this?

61
00:08:59,590 --> 00:09:06,520
It's not just leave a normal range labeled range of a healthy immune system, which is the underlying idea that I want to get across.

62
00:09:11,340 --> 00:09:14,970
Different people labeled things at the lower end of the queue for different ways.

63
00:09:17,690 --> 00:09:21,950
The critical point language is actually pretty good from the standpoint of management.

64
00:09:21,950 --> 00:09:25,610
It's not saying you're instantaneously going to buy a fax that you're like.

65
00:09:25,820 --> 00:09:30,770
It's just saying that this is a critical point at which a lot of that your immune system is going to be.

66
00:09:33,580 --> 00:09:36,730
And you've got your values in there. And this is not perfect.

67
00:09:36,750 --> 00:09:41,550
I can I can depict this, but from a conceptual level.

68
00:09:44,540 --> 00:09:48,840
I like the way you've got focus here and you're very clear.

69
00:09:48,890 --> 00:09:52,640
I don't want to be here. I do want to pursue a kind of stuff.

70
00:09:56,190 --> 00:10:06,590
Yes. I would just want to just struggling and feeling like I had that under 200 with like a diagnosis.

71
00:10:08,280 --> 00:10:16,930
So there's a couple of them. So the question here is Kissinger labeling that under 200 level, should it be labeled as diagnosis?

72
00:10:16,950 --> 00:10:23,609
Should we believe that some people want to sort of there's a there's a staging structure for how you know,

73
00:10:23,610 --> 00:10:27,840
what stage of disease progression for someone who's HIV positive, etc.

74
00:10:29,040 --> 00:10:35,579
My concern about that is that's actually stepping away from things like what does this

75
00:10:35,580 --> 00:10:40,500
value mean for your risk to the question of the labels that we attach to that condition.

76
00:10:42,840 --> 00:10:53,270
At its heart. This should be about is your value normal or representing some escalating degree of risk,

77
00:10:53,270 --> 00:10:59,810
whether that's degree of infection risk, disease progression, etc. What we choose to label that is irrelevant.

78
00:11:01,040 --> 00:11:05,600
But the more that it's conveying a sense of danger, probably the better.

79
00:11:07,040 --> 00:11:11,239
So, again, we can argue about this. This is why I say this is an art, not a pure science.

80
00:11:11,240 --> 00:11:17,120
But the more you step away from communicating risk and more sort of like.

81
00:11:19,260 --> 00:11:25,700
It's information about the disease. You're actually adding in an extra step you're going from well, because your risk is this.

82
00:11:25,710 --> 00:11:32,190
And we have rules that say once you have passed that threshold, now we're going to call you as having AIDS or something else that's going.

83
00:11:36,150 --> 00:11:45,820
What else? It's hard for me to describe guys who might be attached to our vehicles.

84
00:11:46,400 --> 00:11:53,740
Again, these are different variants. This again, is simple.

85
00:11:54,820 --> 00:11:58,300
You could do more here. I would actually like it to do a little bit more here.

86
00:11:59,200 --> 00:12:06,790
But the spacing is very clear. Like each one of those step points is one where at seven, there's at 15.

87
00:12:06,790 --> 00:12:10,060
It is, in fact, at the precise location where it needs to be.

88
00:12:12,820 --> 00:12:16,540
And. This analysis isn't labeled actionable.

89
00:12:16,540 --> 00:12:20,140
It's basically an instruction need for action. And my weakness of this.

90
00:12:21,870 --> 00:12:27,920
Yes. The water systems need for action.

91
00:12:28,160 --> 00:12:30,740
It's not the same thing as a homeowner's need for action.

92
00:12:32,260 --> 00:12:39,040
So the biggest problem I have with this one is this person is going to walk away thinking I don't need to do anything.

93
00:12:41,020 --> 00:12:46,240
And that concerns me. Because what's not as clear here?

94
00:12:47,290 --> 00:12:50,350
Is the I want to be like.

95
00:12:53,540 --> 00:12:57,830
This isn't the normal range. It doesn't mean that anything below from 0 to 15 is fine.

96
00:12:58,520 --> 00:13:01,870
I actually want you to be lower. If you're in seven, I want you to be at three.

97
00:13:01,880 --> 00:13:05,680
If you're a three, I want you to be at one, etc. That's not being all clear.

98
00:13:06,320 --> 00:13:08,210
Communicated here as a quickie.

99
00:13:09,320 --> 00:13:20,990
So your parts you may see different be calling out things where is it is not reinforced that in this case lower is always better.

100
00:13:22,010 --> 00:13:26,450
That's going to be a problem. That's a huge, huge problem. Like this is not bad, don't get me wrong.

101
00:13:27,890 --> 00:13:46,680
But that's the probably the biggest complaint I have. Well, that upper end isn't an arrow, but it's clearly different than the bottom here.

102
00:13:46,680 --> 00:13:52,410
You've got a red, yellow, green color, but it's being reinforced by high risk, low risk levels.

103
00:13:54,110 --> 00:13:58,780
We find once again, spacing is perfectly proportionate.

104
00:13:59,800 --> 00:14:06,760
There is that when you go up to 15 now, even that 375 looks low because way over on the left side,

105
00:14:07,790 --> 00:14:11,380
that's part of the cue that people are picking up on unconsciously.

106
00:14:12,040 --> 00:14:18,670
If you just cut off the graphic up here now, all of a sudden this point is above the midpoint.

107
00:14:19,870 --> 00:14:29,200
So that spacing becomes a cue that matters. Now, this is a case in which we can argue about whether the top half should be in 1200 or 1500, whatever.

108
00:14:29,500 --> 00:14:35,130
This light grid, I think, does a good job of sort of saying, Wow, most people are down here, but this isn't a problem, whatever.

109
00:14:36,700 --> 00:14:40,900
But I really like the spacing on this one. Clean. Yeah.

110
00:14:42,340 --> 00:14:47,200
We add more labels and maybe could those labels be risk of what could we get into?

111
00:14:47,200 --> 00:14:52,220
Sort of, you know, question of this is immune health, not just this isn't death.

112
00:14:53,210 --> 00:15:03,350
Fine. Those are judgment calls. And the way this is, but the way this is framed, I think pretty clearly drops you into a middle category.

113
00:15:04,010 --> 00:15:07,930
That is. Understood pretty clearly.

114
00:15:10,390 --> 00:15:13,510
Any questions on any of these? None of them are perfect.

115
00:15:14,880 --> 00:15:21,010
Yeah. How do you feel about not using, like the red yellow green color scheme since it is like, such a common scheme?

116
00:15:21,010 --> 00:15:27,510
But you mentioned the colorblind. Yeah. So let's think about this one if you strip out the colors here.

117
00:15:29,060 --> 00:15:32,120
Would you be able to figure out pretty much everything I just said? Yeah, you would.

118
00:15:33,880 --> 00:15:37,480
Could you use a different color scheme?

119
00:15:37,510 --> 00:15:46,750
Absolutely. And what's tricky in this context is let's say you wanted to use a grayscale, so just black versus white.

120
00:15:47,530 --> 00:15:51,550
How would you do that here? What would you want to be black and what would you want to be white?

121
00:15:52,680 --> 00:15:57,090
So these questions would have to be answered about that. If it's is it.

122
00:15:58,100 --> 00:16:07,270
That you would make it. The black part is good when you make the black part bad and then this would be white.

123
00:16:08,140 --> 00:16:20,320
I mean, you could. What I have seen more often, John, is stripping down to one color, so only using a red gradient, for example,

124
00:16:20,560 --> 00:16:25,270
because then if you have and if you can see the red and you have an association of it, you associate it with danger.

125
00:16:25,270 --> 00:16:30,849
That's good. And you have the color intensity to provide gradients.

126
00:16:30,850 --> 00:16:35,980
An average I've ever done and I know several of you did that and that was totally fine in general.

127
00:16:38,770 --> 00:16:43,090
So that's my quick thoughts. But there's nothing there's no single answer for what the right policy meant.

128
00:16:44,500 --> 00:16:52,210
The main question is redundancy. Like you can't depend on the color if you're going to have somebody who is colorblind.

129
00:16:56,760 --> 00:17:01,760
I. A quick question.

130
00:17:01,890 --> 00:17:07,610
Yeah, right. Can you just can you talk a little bit more about your thoughts about the high risk?

131
00:17:07,610 --> 00:17:10,760
Low risk, because we've talked a lot about how that's like.

132
00:17:11,920 --> 00:17:20,650
It's very imprecise. Yes, it is. And so would you feel in this example, it's okay to do.

133
00:17:20,660 --> 00:17:25,150
But there are other cases where it might not feel like transition to today's conversation,

134
00:17:26,710 --> 00:17:38,040
because you're highlighting the issue of what happens if I tell you your risk is low risk or your risk is high risk here.

135
00:17:38,380 --> 00:17:45,400
Those terms are being applied to spaces. The label here is not your risk.

136
00:17:45,850 --> 00:17:49,990
It's not labeling at this point. It's labeling this range or this range.

137
00:17:50,470 --> 00:17:59,770
And so the assumption here. Right, maybe not always true, but the assumption here is the hope is that somebody then says, oh, okay.

138
00:18:00,860 --> 00:18:11,450
Green means low, but they're also picking up on the spatial cues so that they will feel differently if they're like right here versus in here.

139
00:18:12,320 --> 00:18:14,570
And that's where that extra precision comes in,

140
00:18:14,570 --> 00:18:21,020
is that the label is being complemented by the other information that somebody can pick up from the graph.

141
00:18:22,040 --> 00:18:28,850
Where we run into more concerning problems is when we are only using verbal terms without other data.

142
00:18:29,600 --> 00:18:34,190
So there's no ability for the person to determine. And I read at the edge of normal.

143
00:18:34,190 --> 00:18:40,100
Am I in the middle of normal? How close to my to like all of that is gone because all we have is the verbal term.

144
00:18:43,220 --> 00:18:47,540
Now, part of the point of this and this is why this is a transition for today,

145
00:18:48,200 --> 00:18:52,610
is the question we want to be asking ourselves is when do we need precision?

146
00:18:54,230 --> 00:19:00,200
When is that important for what a person or an organization needs to do with this information?

147
00:19:00,440 --> 00:19:06,440
And when actually is it better and less complicated, simpler to communicate,

148
00:19:06,440 --> 00:19:13,080
more likely to be understood accurately, to take out the numbers and just rely upon the label to motivate behavior?

149
00:19:14,250 --> 00:19:22,440
Or not behavior, etc. So that's kind of the question I want to go out for today, which is when are the situations in which.

150
00:19:23,190 --> 00:19:32,790
Yeah, I don't really need you to be precise. I just need to know you need to act or you don't need to act or why you should feel something about this.

151
00:19:33,360 --> 00:19:37,030
And when are there situations when. No, no, no. The number is not.

152
00:19:37,230 --> 00:19:42,700
You know, a label is not this. It's not enough to tell you you're at risk.

153
00:19:42,700 --> 00:19:44,590
It's not even enough to tell you you're at high risk.

154
00:19:44,620 --> 00:19:50,590
You need those numbers in order to derive the level of precision that matters for what you need to do.

155
00:19:53,280 --> 00:19:56,350
So. I got a few examples.

156
00:19:58,860 --> 00:20:10,570
And I think I want to start with. You were talking in your musing about remember it was your dad's gallbladder surgery or had gallbladder problems.

157
00:20:11,640 --> 00:20:20,370
And I want to pick out the way the risks and benefit of that was presented because it's a nice I think it's a good place to start this conversation.

158
00:20:20,690 --> 00:20:27,720
Yeah. When I was younger, I actually was having like a lot of like abdominal pain.

159
00:20:27,990 --> 00:20:36,150
So he drove himself to the hospital and when he got there were like, Oh, your gallbladder is acting up.

160
00:20:37,350 --> 00:20:44,670
And it was like, Oh, you mean like an emergency, like removal, like this has to happen in the next 24 hours.

161
00:20:45,990 --> 00:20:49,889
And so there wasn't like a really long conversation about risk.

162
00:20:49,890 --> 00:21:00,030
It was really kind of communicated, I guess, and like more like general statements, you are at high risk of like pancreas, like what?

163
00:21:00,060 --> 00:21:05,879
Like chronic pancreas swelling. You are at low risk of complications from surgery.

164
00:21:05,880 --> 00:21:09,660
And that kind of went very quickly and mostly with general statements.

165
00:21:11,200 --> 00:21:17,109
So the thing I remember and this is maybe this was just the the way you wrote

166
00:21:17,110 --> 00:21:23,020
it up was it sounded as though the benefits of surgery were communicated,

167
00:21:23,350 --> 00:21:30,020
at least to some degree, with numbers sort of like your risk of pancreatitis,

168
00:21:30,020 --> 00:21:33,370
that your chronic pancreatitis is going from X to Y or something like that.

169
00:21:33,390 --> 00:21:42,010
It was purely in the verbal terms because I think that they were there was like some like treadmill like numbers going on.

170
00:21:42,010 --> 00:21:49,800
But it was. There's more, I guess these like high risk shrapnel, like statements like this.

171
00:21:49,830 --> 00:21:57,990
Something that has to happen fast. Yeah. So notice that a key piece of the story here is high speed.

172
00:21:59,200 --> 00:22:04,660
I think there's a tradeoff here when we step away from precision or we step away from numbers.

173
00:22:04,990 --> 00:22:10,180
We're also stepping away from complexity, which means we can get things done a heck of a lot more simply and quickly.

174
00:22:12,160 --> 00:22:16,840
So what could go wrong? Let's unpack.

175
00:22:17,530 --> 00:22:21,370
God knows, through this process they talking about you have a high risk of X, you have a lower.

176
00:22:21,550 --> 00:22:26,110
Why? It's low risk of surgical complications. Where can this process break?

177
00:22:28,100 --> 00:22:32,510
What would. I mean, imagine, like, what would happen that would be like, whoa, whoa.

178
00:22:32,840 --> 00:22:38,430
That was not enough. So if something happened in surgery.

179
00:22:40,020 --> 00:22:45,330
But we didn't mention surgical. So it's not as though we didn't have that possibility talked about.

180
00:22:47,660 --> 00:22:55,700
We might be more concerned if something happens in surgery that is of a magnitude that they didn't really engage with.

181
00:22:57,580 --> 00:23:01,060
So we're probably more worried in this context about.

182
00:23:02,340 --> 00:23:06,810
Rare but catastrophic things that the patient has not engaged with.

183
00:23:07,140 --> 00:23:12,680
Then the more common stuff. You could also have misdiagnosis.

184
00:23:12,700 --> 00:23:21,100
I mean, it's rare, but it does happen that you think one thing is a certain condition and it's actually a factor of something else entirely going on.

185
00:23:21,760 --> 00:23:24,940
So there's a diagnostic uncertainty here.

186
00:23:24,970 --> 00:23:31,600
So another possibility is and this is worth talking about, and we're going to get more and more into this over the next couple of weeks.

187
00:23:32,770 --> 00:23:42,370
The assumption here is that there must be actually that there is a high enough risk of harm if no surgery takes place.

188
00:23:43,620 --> 00:23:47,950
That action must be taken. We don't actually know that.

189
00:23:48,900 --> 00:23:54,300
We don't know for your dad's case. If he had had the surgery, what would have happened?

190
00:23:54,810 --> 00:23:57,630
And it is possible that the surgery was unnecessary.

191
00:24:00,530 --> 00:24:04,610
That could be a misdiagnosis or it could be it's an accurate diagnosis, but it doesn't progress kind of thing.

192
00:24:05,690 --> 00:24:10,520
But in a way, it's an error of the of the estimate of the of the belief that this must occur.

193
00:24:12,610 --> 00:24:18,190
Why am I raising this one? This is I want to I want to emphasize this point, because this is going to come up over and over and over again.

194
00:24:18,700 --> 00:24:27,060
We have a system. And emphasizes if it is going to make overemphasize the risk of.

195
00:24:30,570 --> 00:24:37,610
Feeling the that. And we don't talk very much about unnecessary procedures or unnecessary tests.

196
00:24:39,620 --> 00:24:48,450
We're going to keep coming back to this question. We actually don't know. Whether Emily's dad needed to have surgery, I do not know.

197
00:24:49,920 --> 00:24:57,319
Whether I would be alive today if I hadn't had my transmission. I have strong beliefs about that.

198
00:24:57,320 --> 00:25:01,070
I believe the risk estimates that I was given, but I don't know.

199
00:25:04,450 --> 00:25:09,390
And that's part of the, you know, the questions we have to wrestle with, you know.

200
00:25:11,300 --> 00:25:20,360
So anyway, this is this is all a big set up to the question of like if they went through a process, it was some form of both nudging.

201
00:25:20,750 --> 00:25:27,320
There was no question they wanted him to have served. Yes. But also some form of informed consent.

202
00:25:27,380 --> 00:25:30,530
Awareness of the trade offs and the risks that might come along with that.

203
00:25:30,920 --> 00:25:34,070
And yet, for the most part, it was done, not quantitative.

204
00:25:36,990 --> 00:25:40,020
Let's just sit with that. What, are we gonna be okay with that?

205
00:25:40,020 --> 00:25:45,510
And what are we not? In similar domains.

206
00:25:46,140 --> 00:25:52,290
Elizabeth, you were talking about the degrees of which you've been exposed to messages

207
00:25:52,290 --> 00:25:56,670
about being at high risk for this or high risk for that and the multiple things.

208
00:25:57,900 --> 00:26:03,180
So that was a you know, another reason why we often step away from numbers is just volume.

209
00:26:05,530 --> 00:26:08,920
Because I was just having part of having a condition like diabetes.

210
00:26:09,250 --> 00:26:16,810
There's tons of things that that's listed as you are at a high risk for blink because of your condition.

211
00:26:16,840 --> 00:26:25,510
Yeah. And one of the examples I was talking about is all the recommendations of these supplements that you can take to supposedly decrease the risks.

212
00:26:25,990 --> 00:26:30,040
And but that's when I want to have actual numbers of.

213
00:26:30,190 --> 00:26:32,740
Okay. How much of a difference is this going to make?

214
00:26:33,820 --> 00:26:42,640
Is it if it decreases something by a minuscule amount but has a bunch of other side effects, is that really helpful or worthwhile?

215
00:26:43,180 --> 00:26:48,430
And so I'm saying that's what it helps to not just have a possibility communication,

216
00:26:48,430 --> 00:26:56,620
but when you actually want more precise information, sometimes you need more than just knowing there's a possibility.

217
00:26:57,100 --> 00:27:05,530
And speaking I want to highlight from Elizabeth's story is notice the contrast in goals you have versus the supplement producer has.

218
00:27:09,020 --> 00:27:15,500
Supplement producer wants to nudge, wants to convey the fact that benefit exists,

219
00:27:16,190 --> 00:27:23,000
that there is a possibility that it will reduce your risk and give you a qualitative sense of benefit.

220
00:27:24,160 --> 00:27:27,370
And that has the persuasive impact that you're describing.

221
00:27:28,720 --> 00:27:37,270
You want. Precision because you want to be able to make your own decisions about whether this benefit is worth it or not.

222
00:27:38,590 --> 00:27:43,120
And and I'm going to even make it bigger, like you talked about. You know, maybe there are side effects associated with it.

223
00:27:43,390 --> 00:27:48,340
Maybe it's just money and time and having to remember to take a pill that's just as important as anything else.

224
00:27:48,340 --> 00:27:55,310
Serious, like we all ideally should be making choices about the risks that we accept or don't accept.

225
00:27:55,330 --> 00:28:00,040
And you want detail to understand the magnitude of the risks that you might be accepting.

226
00:28:02,840 --> 00:28:08,000
That's another piece. So part of the problem here is if we gave you quantitative information,

227
00:28:08,420 --> 00:28:12,770
let's think about how many the volume of quantitative information that would be involved here.

228
00:28:12,950 --> 00:28:18,709
It would be huge because we're talking about marginal risk reduction for all of these different potential interventions.

229
00:28:18,710 --> 00:28:24,530
For all of these different potential. I. But a difficult task for a low numerate person.

230
00:28:28,920 --> 00:28:34,590
So we don't. But I wanted to have you bring up this issue because there is a cost to this,

231
00:28:35,190 --> 00:28:39,930
both in terms of it makes the experience more persuasive for right and wrong.

232
00:28:40,800 --> 00:28:45,480
And because it means that what you're left with is making decisions where, you know,

233
00:28:45,480 --> 00:28:50,580
you don't know enough, you know, okay, maybe this does make a difference.

234
00:28:50,580 --> 00:28:59,680
Maybe it doesn't. But I actually don't know whether I want it or not. In a similar domain.

235
00:29:01,890 --> 00:29:13,580
Let's see. I emailed you this morning and wasn't able to come today.

236
00:29:14,240 --> 00:29:19,080
We should share a little bit of what she was talking about. Emily.

237
00:29:19,080 --> 00:29:25,950
I musing wrote about a kindred spirit about. Like the way risks are talked about at the dentist.

238
00:29:28,170 --> 00:29:37,170
She has a cavity. You can have a nice conversation and that looks like they have identified a thing that is associated with risk.

239
00:29:40,380 --> 00:29:48,330
Does your dentist talk to you about probability that your cavity will lead to meaning to remove your truth or.

240
00:29:48,930 --> 00:29:54,300
I don't think I have heard a quantitative risk communication in however long I've been going to the dentist.

241
00:29:55,200 --> 00:30:03,040
Right. It's all framed in basically mental model terms, like you have a cavity or, you know,

242
00:30:03,060 --> 00:30:08,490
this decay is going to keep getting worse until eventually we have to replace this crown type of stuff.

243
00:30:09,510 --> 00:30:14,820
I causal models explanations of I see this thing and this thing will lead to bad things.

244
00:30:16,730 --> 00:30:25,070
But not improbable. Now, the closest I've ever had a dentist come to that is not actually the savages, but.

245
00:30:25,300 --> 00:30:29,720
All right. And that's another reason I wanted to bring up this example, because we can quantify risks,

246
00:30:29,720 --> 00:30:34,670
certainly certain types of risk, not by whether it will happen, but how quickly it will happen.

247
00:30:35,860 --> 00:30:43,829
So. I have the unfortunate experience of having had to have some crowds relatively early on.

248
00:30:43,830 --> 00:30:47,340
So I now, I think having conversations about, you know, how long will this last?

249
00:30:48,750 --> 00:30:54,210
Is this I'm going to have to replace this in next year? Or is this something that replaces five years or is this I'm going to replace it in ten years.

250
00:30:55,020 --> 00:30:58,310
Okay. That is risk communication. But it's it's free.

251
00:30:58,320 --> 00:31:01,350
It's not. Yes. No, it's time.

252
00:31:02,130 --> 00:31:06,120
And there's plenty of other health issues where time shows up as critical values.

253
00:31:06,660 --> 00:31:14,130
So joint replacement, for example, is a classic what like people need to replace their have loved me or hip replacements.

254
00:31:14,700 --> 00:31:17,189
They're going to last for ten years or 20 years or whatever.

255
00:31:17,190 --> 00:31:24,200
But there's questions about their ability that is at least as important as anything about symptom,

256
00:31:24,750 --> 00:31:33,930
how many chronic diseases you're talking about time to progression that you may not be able to prevent progression.

257
00:31:34,910 --> 00:31:42,730
But you could talk about delay of progression. So to take an example, ripped from the headlines.

258
00:31:44,800 --> 00:31:55,060
How do you help the controversial medication that was recently approved for people with early stages of Alzheimer's?

259
00:31:57,440 --> 00:32:03,580
It is not a presentation in any way claiming to prevent or stop the disease.

260
00:32:03,590 --> 00:32:07,100
Its benefit is entirely described in terms of time.

261
00:32:07,980 --> 00:32:11,760
DeLay of onset slow of symptoms, etc.

262
00:32:15,180 --> 00:32:21,210
So. Again, we're stuck in this question of, you know, is it enough to say that?

263
00:32:22,790 --> 00:32:30,380
And artificially describe this. Yeah. This drug will slow the onset of dementia symptoms.

264
00:32:34,420 --> 00:32:38,980
I don't know. Meaning what? Meaning by a week.

265
00:32:39,700 --> 00:32:43,720
By a year? Like it actually matters a lot.

266
00:32:44,230 --> 00:32:47,290
How much? How quickly or slowly?

267
00:32:48,100 --> 00:32:49,630
What's the difference that we're talking about?

268
00:32:52,540 --> 00:33:00,340
So that's another case in which I would at least feel if I don't have some sense of quantity, I'm not necessarily for.

269
00:33:08,010 --> 00:33:12,110
Oh, yeah. But.

270
00:33:16,300 --> 00:33:20,550
Like a. You're talking about communications.

271
00:33:21,690 --> 00:33:26,340
And again, the question of like, when are we getting numbers about code communications and when are we not?

272
00:33:26,610 --> 00:33:29,610
Yeah. So I'm trying to remember the specific examples you talked about.

273
00:33:29,730 --> 00:33:33,780
Amusing, but you weren't the only person to bring up COBRA communications on this question.

274
00:33:34,470 --> 00:33:44,190
Yeah. So I was talking about how I was working as a case investigator at the health department when the COVID vaccine was first coming out and so on.

275
00:33:44,310 --> 00:33:52,740
A lot of times my conversations with people on the phone call to check in on them, the vaccine would come up or right after it had been rolled out.

276
00:33:52,740 --> 00:33:57,450
And I think there was a lot of confusion when people were still getting COVID who

277
00:33:57,450 --> 00:34:02,819
had been vaccinated about kind of what success means in terms of the vaccine.

278
00:34:02,820 --> 00:34:06,809
Because you had people talking on the news about like just like during all these numbers out,

279
00:34:06,810 --> 00:34:12,930
people from the pharmaceutical companies would come on and from the store sent out and say, you know, it's effective, it's working.

280
00:34:12,930 --> 00:34:20,190
But then people who maybe weren't as kind of up to date on what those numbers meant when

281
00:34:20,400 --> 00:34:25,750
they'd see people still getting sick with companies like like this is you guys buying it.

282
00:34:25,800 --> 00:34:31,170
And it was a kind of a disconnect that the efficacy and effectiveness of the vaccine,

283
00:34:31,830 --> 00:34:36,360
according to the public health and also like the pharmaceutical companies,

284
00:34:36,360 --> 00:34:40,530
would be that less people would be getting like seriously ill and dying from COVID.

285
00:34:40,980 --> 00:34:48,389
Whereas the general public, from what I noticed, seemed to understand it as more prevention of people even getting COVID in the first place.

286
00:34:48,390 --> 00:34:55,049
Right. So. Three things I want to say here.

287
00:34:55,050 --> 00:35:02,610
Human trafficking, which I would do first. First of all, I want to note that much of what you just described is in many ways mental models.

288
00:35:02,610 --> 00:35:09,480
Problems like your mental model of a vaccine is it will prevent me from getting sick, period.

289
00:35:10,560 --> 00:35:14,280
Then when somebody gets sick, that little model is broken. You have a trust problem.

290
00:35:14,670 --> 00:35:21,990
If, however, the experts model is not in line with the goal of a vaccine more than anything else is to prevent you from being hospitalized.

291
00:35:22,530 --> 00:35:25,640
Patients with severe disease, hospitalization and death.

292
00:35:25,650 --> 00:35:31,020
How are we going to measure that? The infection is not actually a relevant marker.

293
00:35:32,690 --> 00:35:34,190
And that's just a completely secondary thing.

294
00:35:34,190 --> 00:35:41,210
And that was one of the big problems that we had in the COVID vaccine rollout was that most people's mental models was, I'm not going to get sick.

295
00:35:41,220 --> 00:35:46,310
And frankly, much of the conversation from the political leaders was about, you're not going to get sick.

296
00:35:48,980 --> 00:35:53,000
What's the actual thing that was measured?

297
00:35:53,180 --> 00:36:01,940
More so to measure of efficacy. And the argument for vaccination in the first place is prevention of severe disease to.

298
00:36:03,520 --> 00:36:10,410
Let's talk about measurement of effectiveness. How do you measure whether a vaccine is effective?

299
00:36:16,310 --> 00:36:19,310
Yeah. My body's so.

300
00:36:20,650 --> 00:36:24,130
It's an interesting question. Antibodies are a biomarker.

301
00:36:24,280 --> 00:36:29,860
We presume or have a mental model that says the higher your antibodies,

302
00:36:30,160 --> 00:36:35,620
the more likely you are to avoid getting sick and the more likely it is your body will be able to fight off infection.

303
00:36:37,020 --> 00:36:40,740
That is true, but not perfectly true.

304
00:36:40,960 --> 00:36:47,190
There is certainly variability between the mapping of amount of antibodies and the degree of protection that's actually observed.

305
00:36:48,480 --> 00:36:54,570
So, yes, but that's not the externally visible outcome.

306
00:36:56,310 --> 00:37:02,430
So what if when we you do vaccine trials and I have seen the original vaccine trials data

307
00:37:02,430 --> 00:37:08,460
on the COVID vaccines because at the time that these things were rolling out in late 2020,

308
00:37:09,000 --> 00:37:15,300
I got tapped to be sit on a committee at Michigan Medicine for reviewing that safety data as it was being processed.

309
00:37:17,490 --> 00:37:21,730
What is the measurement? We think about efficacy.

310
00:37:25,820 --> 00:37:30,400
Fatalities in what? And so what's the reference class?

311
00:37:30,410 --> 00:37:37,540
What's the denominator here? So you take a population.

312
00:37:38,570 --> 00:37:42,020
You randomize people whether they get the vaccine or not.

313
00:37:44,880 --> 00:37:48,420
You follow, you observe outcomes.

314
00:37:49,430 --> 00:37:52,459
Outcomes will include things like Fort Collins.

315
00:37:52,460 --> 00:37:59,210
It will include hospitalizations. It will include. Essentially measures of symptoms.

316
00:37:59,720 --> 00:38:03,260
You will not actually generally include whether or not you are infected,

317
00:38:03,260 --> 00:38:07,730
because we're not actually going around to every one of those people are drawing blood to see whether or not they've been infected.

318
00:38:08,270 --> 00:38:17,540
What we measure is what we can observe, which is part of the reason why the COVID vaccines were so difficult, because certainly initially.

319
00:38:19,450 --> 00:38:23,530
How do you measure the impact when you have asymptomatic infections?

320
00:38:25,520 --> 00:38:31,100
You're not actually seeing the outcome, which is in fact, something that is being changed, we hope, by the vaccine.

321
00:38:34,910 --> 00:38:41,719
So. Anybody.

322
00:38:41,720 --> 00:38:44,180
Remember, this is like two years ago.

323
00:38:44,360 --> 00:38:58,549
I don't know what the efficacy statistics that were being bandied around when vaccines and vaccines first rolled out before we had all the variants,

324
00:38:58,550 --> 00:39:05,530
like we're just talking about the original strains of COVID and the original vaccines which.

325
00:39:12,070 --> 00:39:15,170
I thought it was around like 80% or something.

326
00:39:15,200 --> 00:39:19,500
Which one's for the. Oh, gosh. I guess the visor was like 80.

327
00:39:19,530 --> 00:39:22,730
The modiano was a little bit higher, like many eighties low nineties.

328
00:39:22,940 --> 00:39:33,830
So I don't remember perfectly. My recollection was that the Verna was around I think 92 visor.

329
00:39:35,190 --> 00:39:47,880
Was somewhere around 98. And JMJ vaccine, the non RNA vaccine was 66.

330
00:39:53,990 --> 00:39:57,140
Let's think about this. What does this actually mean?

331
00:39:57,740 --> 00:40:05,000
Does this mean that if you get vaccinated with Johnson Johnson, you had a 33% chance or 34% chance of getting COVID?

332
00:40:06,630 --> 00:40:15,220
No. The baseline rates are here are quite small if we're talking about tens of thousands of people for which.

333
00:40:16,470 --> 00:40:19,700
A very small fraction of any of them got covered in the first place.

334
00:40:21,790 --> 00:40:27,430
These are relative statistics. So this is saying essentially.

335
00:40:30,320 --> 00:40:34,380
Compared to the control group. You were.

336
00:40:35,610 --> 00:40:40,080
There were only about a third as many people who got.

337
00:40:42,050 --> 00:40:47,150
If they had been vaccinated with Jay versus the rate of people who got COVID in the control room.

338
00:40:48,110 --> 00:41:01,750
And here it's about a 10th. It also means putting the positive rating on more than half the people.

339
00:41:02,500 --> 00:41:06,430
More than half of the risk is removed, even by the least effective vaccine.

340
00:41:09,980 --> 00:41:16,370
Here's the third thing I want. You guys may know this. You may not. What's the average effectiveness rate?

341
00:41:16,910 --> 00:41:20,720
Calculating exactly the same kinds of ways for flu vaccine.

342
00:41:21,350 --> 00:41:24,450
Seasonal flu vaccines. No.

343
00:41:25,810 --> 00:41:29,799
I don't know exactly. I know it's a lot lower, though. Yeah. It varies from year to year.

344
00:41:29,800 --> 00:41:35,530
Yeah. Compared to different strains, etc. Yeah, I know. It's not as good as I looked up the statistics.

345
00:41:35,680 --> 00:41:41,950
I think a year ago I looked up the statistics over like a decade's worth of flu vaccines and the range was.

346
00:41:43,880 --> 00:41:52,970
Was about 20 to 60. Even the years when the flu vaccine matches well and we're talking about it's a great match,

347
00:41:52,970 --> 00:41:56,970
etc., it didn't actually match the effectiveness include vaccination.

348
00:41:58,540 --> 00:42:06,370
That's how effective these things are. Now, obviously, this has been changed with the advent of new variants.

349
00:42:07,640 --> 00:42:13,360
So these are not matched to the different variants, which is why we have time specific booster now, etc.

350
00:42:14,470 --> 00:42:20,200
But actually, one of the more interesting conversations that unfolded at that point in time was.

351
00:42:21,350 --> 00:42:28,250
At the trials that Moderna and Pfizer did were about a couple of months ahead of the trials for JNJ.

352
00:42:30,310 --> 00:42:36,490
All of their data was collected before even Phi Beta variant were started to circulate while.

353
00:42:37,980 --> 00:42:42,340
And J was later. And so they were starting to deal with some of the other variants.

354
00:42:43,170 --> 00:42:48,829
So the denominator was not actually I don't know. Nobody knows whether had J.

355
00:42:48,830 --> 00:42:55,440
J been launched at the same time as the other ones, what its efficacy statistics would look, guesses, they would be higher.

356
00:42:55,830 --> 00:42:59,070
And we don't know because the denominator is different.

357
00:42:59,580 --> 00:43:02,130
Now, why am I spending all this time talking about this?

358
00:43:03,200 --> 00:43:09,200
Because these numbers got thrown out in the public risk communication about the vaccines all the time.

359
00:43:09,950 --> 00:43:21,679
So much so that. There was a significant number of people who intentionally pivoted away from the JNJ vaccine because they saw 66 as less than 90.

360
00:43:21,680 --> 00:43:27,740
And that's not good. That was only the level at which this was kind of, you know, they were processing it.

361
00:43:28,310 --> 00:43:35,060
And so there was this tension in the public health messaging at that point in time between one group of messages that were basically saying,

362
00:43:35,060 --> 00:43:39,580
okay, here's the efficacy statistics. Maybe even medicine is better than Pfizer.

363
00:43:39,580 --> 00:43:42,990
I mean, really? Why do we think that that's a statistically reliable difference?

364
00:43:43,020 --> 00:43:44,360
I certainly don't know.

365
00:43:47,520 --> 00:43:55,080
But really asking people, giving people numbers and basically saying, we want you to pay attention to these as a marker of which one is better.

366
00:43:56,540 --> 00:44:00,890
Versus a whole nother set of communications as well as all of the vaccines are good.

367
00:44:00,920 --> 00:44:04,130
All of the vaccines will protect you, get whatever you can get.

368
00:44:04,310 --> 00:44:07,610
Let's not stress about these numbers. They just go. They're good.

369
00:44:12,040 --> 00:44:20,360
And my personal perspective is. It would have been better had we not actually talked about the efficacy of.

370
00:44:21,560 --> 00:44:27,080
I actually believe the rollout would have been better without, but we didn't have that control.

371
00:44:28,160 --> 00:44:32,750
As soon as the studies came out, news grabbed them. We were desperate for news and talk about.

372
00:44:34,020 --> 00:44:39,040
But in terms of goals. Our goal in public health was to get everybody vaccinated.

373
00:44:39,070 --> 00:44:42,680
I don't actually believe the statistics supported that death.

374
00:44:44,880 --> 00:44:53,400
With, like because there was so much hesitancy around the vaccine, whether it like being a new technology or a new way to do it.

375
00:44:53,940 --> 00:45:04,049
I think I know if we hadn't known what that what the efficacy rates were for certain ones and maybe

376
00:45:04,050 --> 00:45:09,000
a lot of other people wouldn't have gotten it because they were concerned about this new type of.

377
00:45:09,480 --> 00:45:15,420
So this is a very fair question. I have my own personal opinion.

378
00:45:15,430 --> 00:45:19,660
It is a expert opinion, you know, but I certainly can't prove it.

379
00:45:22,120 --> 00:45:25,780
I don't think most of the people who were.

380
00:45:26,860 --> 00:45:30,249
Often to any vaccine because it's quality. Some people were never open.

381
00:45:30,250 --> 00:45:36,400
They like it didn't. Who open to a vaccine were really making a decision based upon efficacy.

382
00:45:37,450 --> 00:45:49,239
They were making their decision based on fear. And the novelty of the RNA vaccines is a risk question as a side effect.

383
00:45:49,240 --> 00:45:52,230
QUESTION Is this going to do something terrific to me?

384
00:45:52,240 --> 00:45:59,050
QUESTION Even if it had worked perfectly, I suspect most of those people would still have had those.

385
00:46:00,430 --> 00:46:03,970
So, yes, risks and benefits are not.

386
00:46:05,020 --> 00:46:08,530
Separate. Right. You're talking about the ethnic touristic that they're linked.

387
00:46:09,250 --> 00:46:13,120
So when you had that fear of.

388
00:46:14,910 --> 00:46:23,640
Novel vaccine. I don't know if it's going to cause me harm. That also impacted people's perceptions of benefits in a very unconscious, emotional way.

389
00:46:24,780 --> 00:46:28,750
I'm not convinced that throwing more numbers at them would have actually changed anything.

390
00:46:30,830 --> 00:46:36,590
What I am convinced of is that for highly numerate, desperate people,

391
00:46:37,370 --> 00:46:45,379
we had a lot of conversations about numbers that probably had no reliability in the first place, to the point where and I think I've said this before,

392
00:46:45,380 --> 00:46:54,110
but I'll say it again, there was a time in the rollout when when the Detroit mayor basically at least publicly talked

393
00:46:54,110 --> 00:46:57,860
about essentially refusing a shipment of the JNJ vaccine because it wasn't the good one.

394
00:47:01,210 --> 00:47:05,220
And it turned out that was a, you know, probably an exaggeration of how that unfolded.

395
00:47:06,070 --> 00:47:13,540
But that story came out and everybody went, well, I understand why people were reacting that way because they're looking at this and.

396
00:47:15,270 --> 00:47:20,850
If you take away from these numbers, JJ is less effective than the other two vaccines.

397
00:47:22,100 --> 00:47:25,280
In a meaningful way, then I don't want I want the bad stuff.

398
00:47:25,280 --> 00:47:36,430
I want the good stuff. If you're really different than just sort of saying these are all effective vaccines, we've got to get them out, etc.

399
00:47:37,600 --> 00:47:41,559
Yeah, what might have been the consequences of not discussing the efficacy?

400
00:47:41,560 --> 00:47:48,560
And then once we get boosters and like today kind of. The J vaccine isn't used for these things.

401
00:47:48,560 --> 00:47:55,320
And so could that have created mistrust that we said all of these are good and then down the line we're not recommending it anymore as well.

402
00:47:56,450 --> 00:48:00,450
So. Unpacking. I mean, there's many, many things that have happened here.

403
00:48:01,590 --> 00:48:07,800
We have evolving understandings of what the underlying rare risks are.

404
00:48:10,840 --> 00:48:19,000
We have greater confidence that the technology is actually working like the many vaccines have achieved their goals instead of.

405
00:48:20,830 --> 00:48:25,930
But this is a I'm going to bookmark this conversation and then we'll come back to it in like three weeks.

406
00:48:26,290 --> 00:48:32,680
Because what you're really talking about here is. How we managed uncertainty.

407
00:48:35,750 --> 00:48:43,760
If we basically had started the conversation by saying we have these different vaccines, as far as I can tell, they're all effective.

408
00:48:43,760 --> 00:48:49,910
As far as we can tell, they're all pretty safe. But we've just got to act now because we can't afford to wait.

409
00:48:51,240 --> 00:48:56,940
And then six months later we said, yeah, you know what? We learned a lot more or maybe less emphasize these ones and less that one.

410
00:48:56,940 --> 00:49:04,020
I don't think anybody would have had a problem because the initial conversation had a degree of precision like,

411
00:49:04,050 --> 00:49:08,970
Oh, these vaccines are so effective, etc.

412
00:49:10,970 --> 00:49:15,770
Now it looks like we're a little bit we're going back on our word like this was a good vaccine, but now it's not.

413
00:49:16,190 --> 00:49:19,670
I don't think we ever actually knew. I didn't feel like I knew.

414
00:49:21,170 --> 00:49:24,680
As I was guessing my own choices about which vaccine to get.

415
00:49:28,360 --> 00:49:31,780
And there was very definitely a moment when it was sort of like, where can I get an appointment?

416
00:49:31,790 --> 00:49:40,239
I don't care. And that's a very different kind of framing of this than ones in which we want people to

417
00:49:40,240 --> 00:49:44,360
be engaging with the numbers because we want them to be nuanced in their decision making.

418
00:49:44,380 --> 00:49:48,060
Much like Elizabeth was talking about earlier, right. There are situations in which this really matters.

419
00:49:48,070 --> 00:49:51,230
The numbers do matter. I might not.

420
00:49:51,770 --> 00:50:01,040
I might take or not take this medication because it has a marginally better efficacy rate is a conversation that happens all the time.

421
00:50:03,670 --> 00:50:08,000
But I'm not sure it serves the public interest that we will have that type of conversation around the Kodak.

422
00:50:11,580 --> 00:50:17,059
And that's this tension that I want us to wrestle with. So what?

423
00:50:17,060 --> 00:50:24,740
I want to spend our real time on this conversation in different ways.

424
00:50:24,740 --> 00:50:31,129
Three different people talk about the same themes that probably butcher your your spins on it,

425
00:50:31,130 --> 00:50:34,160
but hopefully I will get close enough so that everybody can see how they're connected.

426
00:50:37,720 --> 00:50:42,709
I mean, I hear that you were talking about the question of.

427
00:50:42,710 --> 00:50:49,170
Okay. Is this a question of can providers figure out the number or capabilities of the person they're seeing?

428
00:50:50,100 --> 00:50:55,920
Like. Should we behave differently when we've got somebody who wants more numbers or doesn't want more numbers, etc.?

429
00:50:59,360 --> 00:51:04,190
You were talking about something similar in terms of know kind of provider.

430
00:51:04,580 --> 00:51:10,320
Should we be giving you differently? Like I'll get the high number of people, this idea of a low number of people, that kind of thing.

431
00:51:12,570 --> 00:51:19,800
You were talking, Harry, about the shared decision making grounded in having that kind of conversation.

432
00:51:20,280 --> 00:51:27,500
And has that changed the way, you know, how we have sort of the dynamic between providers and patients in Egypt?

433
00:51:27,510 --> 00:51:33,480
You had this other question of should we be doing different things for different people and what's the consequences of that?

434
00:51:35,870 --> 00:51:40,670
I actually don't care where we start because we're going to connect all the dots. Anybody want to go first?

435
00:51:47,640 --> 00:51:51,600
Say something. Well, um, yeah.

436
00:51:51,600 --> 00:51:55,920
So I was kind of thinking about and you kind of summed it up well.

437
00:51:56,550 --> 00:52:06,690
And providers should give like, kind people like more numbers or if they should give low number of people more of like a just impression.

438
00:52:06,690 --> 00:52:15,420
But then I was thinking like the low number of people will have to kind of depend on the provider and their interpretation of their risk.

439
00:52:15,690 --> 00:52:21,950
And if that is that really giving the patient autonomy or informed consent when they don't have like a they don't have like a,

440
00:52:21,990 --> 00:52:27,060
I guess a personal interpretation of that risk. They're just depending on their providers interpretations.

441
00:52:27,150 --> 00:52:32,400
Now, having already had the conversations about the fact that even when we do give people numbers,

442
00:52:32,400 --> 00:52:36,299
we are also manipulating and persuaded by the way in which we do that.

443
00:52:36,300 --> 00:52:38,160
It's not like we're completely separating.

444
00:52:38,610 --> 00:52:44,960
You know, giving people numbers, gives them autonomy and don't give them numbers means that we were shaping their their reactions.

445
00:52:45,390 --> 00:52:52,700
But you're raising a really important point here, which is this is really intimately linked with the question of how much of the recipient.

446
00:52:52,730 --> 00:52:53,850
We're talking about patients here.

447
00:52:53,850 --> 00:53:00,690
But the same thing applies when we step out of the medical consultation context, the recipient, the ability to do more,

448
00:53:01,020 --> 00:53:06,030
to make their own judgment, to change, to see something differently than the way the computer is catered.

449
00:53:08,810 --> 00:53:13,639
I mean, I go to you because this question of sort of like you're alluding to like essentially

450
00:53:13,640 --> 00:53:17,330
autonomy as it's linked to the kind of interaction that somebody is going to have,

451
00:53:17,330 --> 00:53:19,850
that whether they're going to be able to be a participant in this process.

452
00:53:21,290 --> 00:53:25,700
Yes, I was kind of talking about like when you're when I was trained and said shared decision making,

453
00:53:25,880 --> 00:53:32,210
being told to sort of start with the question of like how many what level of detail do you want, where how exact you want me to be?

454
00:53:32,960 --> 00:53:38,510
And I feel like sometimes you're trying to have this conversation and really lay things out as transparently as possible.

455
00:53:38,510 --> 00:53:44,390
And people are like, just get to the point you kind of forced. I sometimes feel like I'm kind of forced to share decision making on people.

456
00:53:45,680 --> 00:53:55,970
And I do think there's an element of sort of trying to gauge that level and letting people choose what level of.

457
00:53:57,150 --> 00:54:01,290
Shared decision making they actually want. I think it's where I run into trickiness.

458
00:54:01,290 --> 00:54:04,380
There is like people feeling like.

459
00:54:05,540 --> 00:54:09,090
How much autonomy do people actually feel like? Why are they choosing the different things?

460
00:54:09,410 --> 00:54:14,120
That gets more complicated, but I think some people say like, give me all the numbers laid out, let's talk to it.

461
00:54:14,570 --> 00:54:18,770
And some people are very much just like a glaze over so that when you're trying to give

462
00:54:18,770 --> 00:54:24,139
them numbers and trying to sort of like force that shared decision making on them, almost that is like counterproductive.

463
00:54:24,140 --> 00:54:27,610
And then they completely check out from what you're talking about. Yeah.

464
00:54:27,650 --> 00:54:29,440
So I want to flag something that's important here.

465
00:54:29,600 --> 00:54:34,339
A way of framing this is you can do share decision making with numbers and you can't do it if you want to.

466
00:54:34,340 --> 00:54:38,360
And I'm not saying that that's, that mischaracterize it.

467
00:54:38,810 --> 00:54:44,500
Um, I mean, I think I, that was not my intention, but this is an important nuance.

468
00:54:44,540 --> 00:54:50,479
I want to make sure we unpack what, what is necessary from your perspective for sure.

469
00:54:50,480 --> 00:54:56,120
This usually I mean, I think we talk, by the way, about shared decision making in like of two classes from now in much greater detail.

470
00:54:56,120 --> 00:55:01,909
So we're going to come back to this. I mean, I think ultimately, like patients need to have like if you're if you're talking about a choice,

471
00:55:01,910 --> 00:55:07,129
patients need to have information about both the benefits and negatives of both choices

472
00:55:07,130 --> 00:55:11,060
for them and in concordance with their value system and what their priorities are.

473
00:55:12,140 --> 00:55:14,480
So typically trying to like start with what their priorities are.

474
00:55:14,480 --> 00:55:20,450
But I think being able to lay that out for them and lay that out in a way that they can connect with and they can actually

475
00:55:20,450 --> 00:55:25,580
then judge for themselves how that lines up with their value system numbers either does or doesn't come into it.

476
00:55:26,270 --> 00:55:29,480
And for different people that can help or I think actually be damaging.

477
00:55:30,970 --> 00:55:35,530
So a couple of things I want to bring up here. Jump in if there's anything that you want to you want to add.

478
00:55:35,530 --> 00:55:40,230
I have already talked about of.

479
00:55:44,000 --> 00:55:47,270
While we may be tempted to say.

480
00:55:48,080 --> 00:55:52,700
I want to communicate this way to somebody who is a numbers person.

481
00:55:53,110 --> 00:55:59,120
And that way that somebody who is not a numbers person. The only way that works will.

482
00:56:00,370 --> 00:56:09,029
If. One, we are accurate who is a numbers person and who is not a scientist?

483
00:56:09,030 --> 00:56:13,290
Because, one, we're not necessarily very accurate from the outside.

484
00:56:13,290 --> 00:56:18,299
And judging people's character abilities is not perfectly correlated with education.

485
00:56:18,300 --> 00:56:20,970
It's certainly not well correlated with visible demographics.

486
00:56:22,410 --> 00:56:31,050
And even if it is recognized that to some degree, tailoring the degree of precision or details we provide to communication,

487
00:56:31,350 --> 00:56:39,690
different groups based upon the correlations that exist within society is structurally reinforcing

488
00:56:39,690 --> 00:56:44,190
those disparities in terms of who gets the right to more detailed information versus not.

489
00:56:45,180 --> 00:56:49,440
So there is a risk here that and I, by the way, I've heard this from many clinicians,

490
00:56:49,440 --> 00:56:52,920
is sort of like, well, I'm not going to bother this type of person. Like, Whoa, hold on.

491
00:56:54,270 --> 00:56:57,630
Why do we feel confident that that's ethically okay?

492
00:56:58,290 --> 00:57:03,770
You're changing the level of detail that we're delivering to somebody. Because of who we think they are.

493
00:57:04,920 --> 00:57:07,600
On, whatever. Yeah.

494
00:57:09,740 --> 00:57:21,110
Well, and any talked about here in terms of making sure somebody knows that there are both risks and benefits, that there exists trade offs.

495
00:57:22,100 --> 00:57:28,679
That there is a role for somebody whose values is different than whether or not you need to know

496
00:57:28,680 --> 00:57:33,560
the magnitude of the difference of risk for each of those things or the magnitude of the benefit,

497
00:57:33,570 --> 00:57:38,100
much like what we're talking about with Elizabeth and whether that level of precision is

498
00:57:38,100 --> 00:57:43,370
necessary for them to begin to understand what the tradeoff is to making good decisions.

499
00:57:44,950 --> 00:57:48,520
So on. One extreme is our situations like where.

500
00:57:50,400 --> 00:57:55,590
I mean, the example I used in the paper was some situations like additive therapies for for cancer treatment,

501
00:57:56,070 --> 00:58:00,120
where you've already had your primary cancer treatment, you've already had surgery to remove the cancer.

502
00:58:00,750 --> 00:58:04,350
The only reason you're going to take this medication is to prevent a cancer from coming back.

503
00:58:05,490 --> 00:58:10,280
Well, is that you're going to prevent. If I'm ever coming back.

504
00:58:10,280 --> 00:58:14,180
No, they're not perfect. So how much risk reduction are you actually going to get?

505
00:58:14,630 --> 00:58:19,940
Is the only reason to take the medication in the first place. There's no other benefit you're going to get.

506
00:58:21,050 --> 00:58:26,300
That's a case in which quantifying that benefit seems like it's kind of important, like if you're only going to prevent.

507
00:58:28,660 --> 00:58:33,760
One out of a thousand women are going to actually prevent the cancer from coming back by taking this medication.

508
00:58:33,790 --> 00:58:36,969
That doesn't seem like a very effective medication.

509
00:58:36,970 --> 00:58:44,170
And I might not be very motivated if 600 out of 1000 women would have prevented have their cancers prevented from coming back.

510
00:58:44,320 --> 00:58:49,540
That seems like a good deal. So quantification becomes far more important.

511
00:58:49,570 --> 00:58:52,740
I can't just say you have a chance of this coming back.

512
00:58:52,750 --> 00:58:59,469
I can't even just say you have a high chance of this coming back as high here might be ten out of a thousand or 500 out of a thousand.

513
00:58:59,470 --> 00:59:03,670
And that does matter. On the other hand, if what we're talking about is.

514
00:59:06,030 --> 00:59:13,080
Let's say, different types of side effects, you know? You know, and this goes back to the verbal terms, which we talked about, the weaknesses of them.

515
00:59:13,110 --> 00:59:17,940
If I say to you, you know, you have a high risk of experiencing headaches if you take this medication.

516
00:59:19,350 --> 00:59:22,350
But if you take this other medication, you don't get the headaches risk.

517
00:59:22,350 --> 00:59:25,560
It's much lower, but you do have some fatigue risk.

518
00:59:28,310 --> 00:59:34,780
Is more detail necessary. I mean, this is where this gets complicated.

519
00:59:34,790 --> 00:59:37,910
Like, how is this a situation in which the number is necessary?

520
00:59:39,590 --> 00:59:45,620
I asked you guys to read the paper on the quantitative imperative because I want you

521
00:59:45,620 --> 00:59:49,070
to understand something which may not be obvious from the public health perspective,

522
00:59:49,490 --> 01:00:00,260
which is how much there's an assumption in many health communication context that our job is to find the number and to provide the number,

523
01:00:00,260 --> 01:00:10,440
and then our job is done. And I really like Peter Schwartz perspective on this because he says, wait, why?

524
01:00:12,370 --> 01:00:16,990
Why do we assume that the number is not percent? Why do we assume that that's our goal?

525
01:00:19,120 --> 01:00:22,480
And he's not arguing that you should never production numbers, quite the contrary.

526
01:00:23,290 --> 01:00:29,740
But he does push this question of, well, wait a second, do we need the number to accomplish our goal?

527
01:00:31,060 --> 01:00:38,080
And that is anything else that I want you to take away from today. That is the question I want you to take away, to always ask yourself.

528
01:00:40,000 --> 01:00:42,990
Leave it up. Can we accomplish our goal?

529
01:00:43,000 --> 01:00:48,640
Because honestly, if our goal was to get people vaccinated for COVID 19, I don't think the numbers were actually helpful.

530
01:00:48,970 --> 01:00:53,170
I think we could have accomplished that goal through a variety of verbal terms and other ways of communicating.

531
01:00:55,090 --> 01:00:59,649
But there are clearly other situations in which there is absolutely are necessary.

532
01:00:59,650 --> 01:01:04,940
And we wouldn't feel like somebody has informed consent or truly understands the choices they're making without that kind of.

533
01:01:11,010 --> 01:01:18,850
So the last thing I wanted to touch upon today, which Andrew you brought up and Audrey brought.

534
01:01:19,870 --> 01:01:23,380
It's sort of the larger. Is it okay to be judging people?

535
01:01:24,250 --> 01:01:36,040
It's. So. Audrey, why don't you start and just I mean, I think it's ethical personally, because people can still decide not to do it.

536
01:01:36,190 --> 01:01:39,700
So if you're like, hey, you should do this, they still have that opportunity to do that.

537
01:01:40,270 --> 01:01:44,680
So I was saying this in like reference to chavs or like you don't have any opportunities.

538
01:01:44,740 --> 01:01:50,480
Like cigarets. If you take them off the market, you don't have any chance versus like, hey, don't do that.

539
01:01:50,500 --> 01:01:54,760
Based on the data. And.

540
01:01:56,680 --> 01:02:03,840
You know. So what's the problem mean? I'm going to be contrary to what's the problem with simply showing people a lot of information that says,

541
01:02:03,930 --> 01:02:10,200
hey, there's lots of risk associated with cigarets or alcohol use, etc., and.

542
01:02:11,900 --> 01:02:20,080
Leaving it at that. I think it gets back to the autonomy where it is up to the person at the end of the day.

543
01:02:22,570 --> 01:02:25,630
You can't force them to do something if they don't want to do it.

544
01:02:28,500 --> 01:02:32,760
If I call you at high risk of dying from lung cancer.

545
01:02:34,990 --> 01:02:43,150
I could not do it. But I can't say that you haven't changed my experience by throwing that label at.

546
01:02:46,240 --> 01:02:50,430
Now we work with them. And then maybe.

547
01:02:50,520 --> 01:02:55,620
I mean, certainly in the context of smoking, my answer is often, yes, I will pull out all the stops on that.

548
01:02:59,700 --> 01:03:07,470
But it doesn't quite question like this is a space in which, again, we're being very explicit about how much are we providing information.

549
01:03:07,750 --> 01:03:12,270
And again, this goes back to the very first reading I had to do is through our article about sharing information,

550
01:03:12,450 --> 01:03:17,400
about care, about changing behavior, like how are we going to measure our success?

551
01:03:18,670 --> 01:03:22,170
I can measure success by how many people get vaccinated or how many people quit smoking,

552
01:03:22,170 --> 01:03:29,670
or am I going to measure success by can they return to me the information that I just gave them in an accurate and complete way?

553
01:03:30,750 --> 01:03:34,980
And if they say that I understood this and I'm going to go ahead and keep smoking.

554
01:03:36,090 --> 01:03:43,780
Am I okay with that? Doing that also brings up the factor of culture, too.

555
01:03:43,780 --> 01:03:48,159
Like if I were to tell someone, Hey, you should do this and be slightly disruptive,

556
01:03:48,160 --> 01:03:53,260
even though they technically by definition could say no, that culturally isn't okay.

557
01:03:55,350 --> 01:03:58,540
Yeah. Okay. So I really I take issue with that.

558
01:03:58,540 --> 01:04:03,520
I actually think it's important to nudge people because actions have externalities.

559
01:04:03,530 --> 01:04:08,259
So you can say, oh, you should, you shouldn't smoke that or you should stop smoking.

560
01:04:08,260 --> 01:04:14,230
That's your decision. Or when you smoke, you're putting out secondhand smoke, which affects people around you.

561
01:04:14,770 --> 01:04:21,069
So it's not just about the individual and autonomy, and I think this is what they were in.

562
01:04:21,070 --> 01:04:28,299
SUNSTEIN Really talk about with libertarian paternalism is that like it might actually be okay to nudge people

563
01:04:28,300 --> 01:04:35,560
in a specific direction because your choices have external externalities and they do affect other people.

564
01:04:35,800 --> 01:04:45,010
And so if we know that telling you to do something is better on the whole for entire outcome utility,

565
01:04:45,010 --> 01:04:49,959
whatever you want to call it, that's at least in my opinion, I agree with them.

566
01:04:49,960 --> 01:04:54,670
I think that's completely fine. So where do we draw the line?

567
01:04:56,400 --> 01:04:59,550
Because it's not the case. I mean, on the most extreme side here.

568
01:05:00,840 --> 01:05:04,510
We could take cigarets off of our. We could.

569
01:05:08,600 --> 01:05:10,580
On the first day of the semester.

570
01:05:11,690 --> 01:05:20,390
Line, each one of you up at the door to the School of Public Health with vaccine shots and not let you in the building until we had administered it.

571
01:05:24,310 --> 01:05:27,360
You don't do that. The military does.

572
01:05:33,130 --> 01:05:37,720
And that's one of the distinctions between we hold between a civilian and someone who's

573
01:05:37,720 --> 01:05:42,370
in the military is the degree to which we have the right to refuse an order like that.

574
01:05:47,170 --> 01:05:53,970
We could. Take cheeseburgers off every menu in the country.

575
01:05:56,220 --> 01:06:01,610
And not healthy. We don't.

576
01:06:03,930 --> 01:06:09,500
There's no simple answer to this. I don't want to be.

577
01:06:10,490 --> 01:06:18,500
I mean, I've already put out that everything that we do in this class, every moment we're communicating risk is to some degree nudging.

578
01:06:20,770 --> 01:06:29,680
Because we're making choices about how much context we're providing, how much we are using terms or colors that we know are going to evoke emotions.

579
01:06:30,740 --> 01:06:33,840
And the point you're bringing up is important.

580
01:06:33,940 --> 01:06:39,220
Like there is. Situations culturally where choice doesn't exist.

581
01:06:40,460 --> 01:06:44,920
I fully believe that many doctor patient communications.

582
01:06:45,940 --> 01:06:51,640
The patient does not believe they have a choice, even if the doctor believes they have engaged in shared decision making.

583
01:06:52,940 --> 01:06:57,380
Because culturally we have expectations of deferral to authority.

584
01:06:58,220 --> 01:07:04,340
We have confusion. It was obvious to me you had a choice, but it wasn't necessarily obvious in the many ways in which that can break down.

585
01:07:04,880 --> 01:07:07,040
Part of where we're going, this is why I'm having this conversation.

586
01:07:07,060 --> 01:07:16,310
Part of what we're going is one of the things that we can do as communicators is make it clear when someone has a choice.

587
01:07:17,620 --> 01:07:27,250
That there is a trade off, that there is a possibility that one might choose A or B and to show that trade off now.

588
01:07:30,610 --> 01:07:34,330
There's everything here from choices where.

589
01:07:35,730 --> 01:07:38,940
Well, you could not do that, and this would be the consequences of that.

590
01:07:38,950 --> 01:07:48,839
But boy, it would be really helpful. I mean, so anything that comes into that cockpit, he could have not had surgery, but it was medically advisable.

591
01:07:48,840 --> 01:07:51,899
And that was very clearly communicated that it was medically advisable.

592
01:07:51,900 --> 01:07:56,560
And so like all and he also he was in so much pain.

593
01:07:56,850 --> 01:07:58,620
Great. Take it out. Right.

594
01:07:58,740 --> 01:08:05,210
But I mean, there are also plenty of other situations in which somebody doesn't actually have symptoms that we we bring the same level of,

595
01:08:05,730 --> 01:08:09,540
hey, like this is an obvious one. We got to do something about this.

596
01:08:11,830 --> 01:08:15,370
That's. And I would never want to back away from that.

597
01:08:17,560 --> 01:08:22,700
Have I talked with you guys about stroke? No.

598
01:08:23,630 --> 01:08:30,080
Now's a good time for that story. Urgent.

599
01:08:30,720 --> 01:08:38,220
You're out in the world and you guys are probably too young for it to have a stroke risk.

600
01:08:38,250 --> 01:08:46,680
Think of it as a personal risk. But somebody my age and starts showing stroke symptoms rushing to the hospital.

601
01:08:47,920 --> 01:08:51,159
And. Yeah. Stroke.

602
01:08:51,160 --> 01:08:58,240
There's a clot. What we do now. We have these medications often referred to as clot busters.

603
01:08:59,900 --> 01:09:03,470
They break up the clock, they speed.

604
01:09:04,710 --> 01:09:10,170
Recovery because they limit the degree of that bleeding is, you know, is expanding in the brain.

605
01:09:11,490 --> 01:09:19,080
They also, as a primary side effect, they cause bleeding because they're disrupting your ability to clot.

606
01:09:20,970 --> 01:09:29,460
So the same drug, it's benefit is also related to questions of causing bleeding by itself.

607
01:09:34,000 --> 01:09:41,840
Should you have? Quantitative informed consent for risk benefit tradeoff discussions before

608
01:09:41,840 --> 01:09:45,710
administering these kinds of medications for somebody who is experiencing stroke.

609
01:09:51,320 --> 01:09:55,040
If you just look at this from a risk standpoint, the answer has got to be yes.

610
01:09:55,400 --> 01:10:01,520
These things cause serious bleeding like there is no question there is serious risk associated with use of these medications.

611
01:10:02,390 --> 01:10:05,780
There's also no question that there are serious benefit associated with their use.

612
01:10:06,760 --> 01:10:10,780
So this is a classic like let's have some sort of decision making situation, right?

613
01:10:13,080 --> 01:10:16,649
Except. That's true.

614
01:10:16,650 --> 01:10:22,010
You administer these drugs that. And we're not talking days.

615
01:10:22,010 --> 01:10:31,260
We're not talking hours. We're talking politics. Every 5 minutes changes the risk benefit ratio to a significant degree.

616
01:10:33,230 --> 01:10:37,960
How long is it going to take to have that conversation? And is it worth it?

617
01:10:40,190 --> 01:10:43,850
So I got approached about a year or two ago by a.

618
01:10:45,590 --> 01:10:50,510
Somebody deals with stroke on a regular basis. I don't actually remember whether they're in the emergency department or neurology, but either way,

619
01:10:52,040 --> 01:10:56,390
who basically wanted to write a paper and I ended up being a coauthor of this paper arguing

620
01:10:57,230 --> 01:11:03,470
that this is one of these cases in which a detailed number conversation is counterproductive.

621
01:11:05,210 --> 01:11:08,600
That the right conversation acknowledges that there is a trade off.

622
01:11:09,730 --> 01:11:15,790
Acknowledges that in the benefits outweigh the risks, especially the faster you act.

623
01:11:17,680 --> 01:11:26,950
And since this are purely at the qualitative level and not at point, is that doing so speed delivery which would improve overall outcomes.

624
01:11:28,350 --> 01:11:35,580
Is it a nudge? Absolutely. Is it full autonomy in the sense of can someone say no?

625
01:11:35,610 --> 01:11:41,220
Well, I mean, this is why this cultural issue is important. Like, boy, it would be hard to say no in that context.

626
01:11:42,630 --> 01:11:44,730
And we're kind of okay with that.

627
01:11:45,670 --> 01:11:54,250
But it's an ethical argument because we are removing from the conversation the quantification of how much risk somebody put in themselves,

628
01:11:54,670 --> 01:12:00,750
which is not small. But not acting is also putting themselves at risk in a not small way.

629
01:12:01,810 --> 01:12:06,670
Well, interesting papers I've I've had a chance to work on because it was really an ethical argument of.

630
01:12:07,880 --> 01:12:12,460
The qualitative understanding is so much more important to that moment than the quantitative.

631
01:12:14,880 --> 01:12:20,100
But it does change the fact it's not fully informed consent in the way we might

632
01:12:20,100 --> 01:12:24,090
think about for detailed quantitative understandings of what the tradeoffs.

633
01:12:24,450 --> 01:12:30,919
It is good enough. So if somebody has a bleeding event happen, they said yes, you told me about that.

634
01:12:30,920 --> 01:12:35,780
I understood there was a possibility that I might have a bleeding effect because of taking this drug.

635
01:12:38,900 --> 01:12:46,020
But they understood what the tradeoff was. There are no simple answers for this for today.

636
01:12:46,110 --> 01:12:53,160
Today is a set up essentially for where we're going over the next couple of weeks, which is about guiding decisions and shared decision making.

637
01:12:53,170 --> 01:13:00,420
And what is is what we think we need to be communicating with people when there is choices to be made and what we don't need to communicate.

638
01:13:01,410 --> 01:13:07,710
But I wanted to engage in these ethical conversations right up front. Because we are energy people.

639
01:13:07,890 --> 01:13:12,510
And sometimes I'm with you, Andrew. I'm like, Look, we know this makes a difference, and so let's do it.

640
01:13:13,730 --> 01:13:17,209
And there are other times when I'm less comfortable with that.

641
01:13:17,210 --> 01:13:25,670
So the last thing I want to say today is I want to let this discussion range fairly widely, but I want to own my own personal position.

642
01:13:25,670 --> 01:13:29,360
And this is where I'm stepping out of and just describing what the literature says.

643
01:13:29,780 --> 01:13:32,060
And this is my personal perspective on this.

644
01:13:35,780 --> 01:13:41,930
Several people only talk for a while about, you know, do react differently for certain people than for others.

645
01:13:42,730 --> 01:13:52,940
For me. That's the wrong question. The right question is, does this task that a user needs to do?

646
01:13:53,900 --> 01:14:05,580
Require numbers. So Elizabeth was talking about like, I want to know how much of a reduction in risk this would have if I take this medication.

647
01:14:06,480 --> 01:14:11,700
That's a task requires some measurement, some quantification.

648
01:14:13,060 --> 01:14:14,110
So I'm unhappy.

649
01:14:14,320 --> 01:14:22,330
Even if says I don't want to help give me details, I still will push to some degree of gratification in that task because they need it.

650
01:14:23,870 --> 01:14:27,080
In the stroke cases, the opposite for strokes.

651
01:14:27,860 --> 01:14:34,280
Their task is to understand the fact that a trade off exists, that there is a possibility of harm,

652
01:14:34,280 --> 01:14:38,450
but there is a even bigger possibility of harm if they don't act.

653
01:14:39,770 --> 01:14:43,670
And once they get that possibility to understanding.

654
01:14:44,650 --> 01:14:48,670
I want to move them towards taking that medication as quickly as possible.

655
01:14:49,270 --> 01:14:56,440
So quantification is not necessary to their task. We?

656
01:14:58,070 --> 01:15:03,700
Communicators, the experts. We know the tasks better than the people we communicate.

657
01:15:06,100 --> 01:15:10,360
We really pause and acknowledge what is it that we are asking people to do?

658
01:15:11,670 --> 01:15:13,770
That could be vaccines. I just want them to get vaccinated.

659
01:15:13,800 --> 01:15:20,700
I don't want them to be making policy decisions about what vaccines we should be offering, that the average consumer just needs to go get vaccinated.

660
01:15:20,940 --> 01:15:29,849
That's not their task. But if I was creating a risk communication about COVID vaccines to the policymakers who are

661
01:15:29,850 --> 01:15:34,380
going to be making choices about which vaccines to be putting out and which ones to buy,

662
01:15:34,740 --> 01:15:42,210
then that detail is best. So I would have a different conversation about efficacy with that audience than I would for the average consumer.

663
01:15:49,000 --> 01:15:53,770
I want to I'm talking to a breast cancer patient about should you be having surgery?

664
01:15:54,400 --> 01:15:58,450
No. The conversation is fairly qualitative. You're going to need to have surgery.

665
01:15:58,450 --> 01:16:02,529
You're at high risk. The cancer is going to keep growing. You need we need to get it out.

666
01:16:02,530 --> 01:16:06,880
We might have some choices about how to do that, but we're not arguing about whether or not you're having surgery.

667
01:16:08,700 --> 01:16:14,380
We have that same woman now, hopefully cancer free after her surgery.

668
01:16:15,160 --> 01:16:20,760
Asking whether or not to take these hormone suppressing medications to reduce the chances of recurrent cancer is going to come back.

669
01:16:21,830 --> 01:16:26,780
Yeah, we're going to be talking numbers because simply saying you're at low risk of having

670
01:16:26,780 --> 01:16:33,200
the cancer come back or high risk is so washing out the level of detail is important.

671
01:16:33,620 --> 01:16:37,370
If I say you're at high risk of recurrence, you're going to take the medication no matter what else I tell you.

672
01:16:38,330 --> 01:16:43,340
If I tell you you have a low risk of recurrence, you're not going to take it. No matter what I tell you, that's not good enough.

673
01:16:44,850 --> 01:16:51,240
Numbers matter. So when I hope I'm going to leave you with here today is two things.

674
01:16:51,300 --> 01:16:54,750
One, always ask the question Are numbers necessary?

675
01:16:56,770 --> 01:17:00,280
To. This kind of task orientation.

676
01:17:00,340 --> 01:17:05,470
And it's not about whether a person wants numbers or not, although that's can be relevant, how you present it.

677
01:17:06,720 --> 01:17:15,540
It's about your understanding of what the task is when you want to be persuasive, when you want to say, I have a budget and I own it, etc.

678
01:17:16,980 --> 01:17:21,570
And when precision matters for guided behavior.

679
01:17:22,500 --> 01:17:28,770
When I gave you those two test result letters, I intentionally gave you two borderline cases.

680
01:17:30,720 --> 01:17:34,140
They were not lead levels of three. There's no arguments for that.

681
01:17:34,560 --> 01:17:38,150
Go get a filter. Don't drink the water. We're done. I did.

682
01:17:38,210 --> 01:17:48,260
It wasn't one. It was seven. You're seeing four accounts work by 100 and they weren't 100 in the model.

683
01:17:48,410 --> 01:17:52,700
So that was a space in which numbers mattered because nuance mattered.

684
01:17:53,950 --> 01:17:57,120
But not everything is like that. And that's why.

685
01:17:58,500 --> 01:18:05,800
There are times in which we have essentially warning systems to actually go back to where we write backed up.

686
01:18:07,080 --> 01:18:10,830
I got into this stuff in part because of the way in which we designed.

687
01:18:11,860 --> 01:18:15,280
Airplane cockpit, which is another risk communication space.

688
01:18:15,280 --> 01:18:18,910
If you think about it, airplane cockpits are designed to be.

689
01:18:19,900 --> 01:18:27,400
There are things which are gauges and then there are things which are warning

690
01:18:27,400 --> 01:18:31,180
lights now that bother you until you hit the point in which of the warning.

691
01:18:32,020 --> 01:18:39,490
And there are things which shake the thing you're holding so and say warning, warning, warning so that you can't possibly ignore them.

692
01:18:42,340 --> 01:18:47,260
Because one of those things requires immediate action. One of those things requires to notice it.

693
01:18:47,650 --> 01:18:52,780
And one of those things you're going to check when you feel like it. Tasks are different.

694
01:18:52,870 --> 01:18:58,140
The communications are different. That mindset is what I want you to hold on to from there.

695
01:18:58,890 --> 01:19:01,380
Now we're going to go forward from here towards choice.

696
01:19:04,100 --> 01:19:14,660
Next class, you're going to read a an article, which is one of the more influential articles in medical risk communication, drug facts, boxes.

697
01:19:15,590 --> 01:19:23,299
Pay attention to this one. It is a template that we will come back to over and over again and that you should be looking at and saying, Oh,

698
01:19:23,300 --> 01:19:26,270
I need to make something that looks like this for my next assignment,

699
01:19:26,720 --> 01:19:30,170
because for your next assignment, you need to make something that looks kind of like that.

700
01:19:32,390 --> 01:19:38,690
Next assignment is not about creativity. Next assignment about taking some good templates and using them in new ways.

701
01:19:39,750 --> 01:19:41,230
We're not reinventing the wheel.

702
01:19:41,250 --> 01:19:45,240
So pay attention to the stuff for next week because they're going to be some really good examples and I'll talk it through.

703
01:19:46,380 --> 01:19:52,870
All right. So let's wrap up here and I will see you next week, not on Tuesday, but on Tuesday.

704
01:19:53,240 --> 01:20:00,600
Tuesday for free. I have a question that might be like a whole nother tangent.

705
01:20:02,760 --> 01:20:10,820
So I was thinking about we're talking about it before that. Great. So I wait 30 years, some of your ideas and they can't communicate.

706
01:20:10,840 --> 01:20:25,180
You have to presume to say something. And so for somebody who is on this show, I think it's I think there's a common theme here.

707
01:20:26,000 --> 01:20:34,980
I think it seems weird that there is like a typical politico and everybody would be like, wow.

708
01:20:35,460 --> 01:20:42,720
Yeah. I mean, I'm like, do nothing for me.

709
01:20:42,990 --> 01:20:54,970
You know, it's not like you're like, I don't know, something that to do if you're sort of right.

710
01:20:55,070 --> 01:20:55,330
Yeah.

711
01:20:55,350 --> 01:21:18,140
I mean, is this like, what about like about whenever something that I don't really want to talk about early Saturday shopping and swimming lessons.

712
01:21:18,900 --> 01:21:21,980
More likely for political reasons.

713
01:21:22,530 --> 01:21:31,590
But this was not even communicated last time, which is why is limited partnership meetings about how to live.

714
01:21:33,090 --> 01:21:49,049
And then also when you're looking like you can make it like it's so weird.

715
01:21:49,050 --> 01:21:56,310
But why? Why is that? I guess the question is, though, and why is that okay in that circumstance,

716
01:21:56,820 --> 01:22:03,390
in every other circumstance outside it means and I don't know, I guess I just don't I don't see.

717
01:22:04,860 --> 01:22:08,900
So I think like, yeah, no,

718
01:22:09,270 --> 01:22:16,260
I think we're pretty similar situation where there life is also in peril where you want to have to get involved in the case.

719
01:22:16,280 --> 01:22:37,139
It was actually question one you can go wrong and you have other people have those rare situations that would look like X but it's like,

720
01:22:37,140 --> 01:22:41,190
yeah, it's like they're in a situation, you know what the closest analogy is?

721
01:22:41,190 --> 01:22:49,410
Somebody who is practicing diving deep in their are, you know, like and if you get in etc. like it's not that big of a deal, so you do it anyway.

722
01:22:49,600 --> 01:22:53,070
Bo. Bo negative consequences, I guess. What if.

723
01:22:54,410 --> 01:23:03,110
I guess the analogy here is if somebody was going to be doing that, you would prefer them to go to you before they start doing it and say, Hey,

724
01:23:03,110 --> 01:23:07,940
I'm an expert diver and I'm going to be practicing this and I'm going to be underwater for like a minute or two at a time,

725
01:23:08,300 --> 01:23:11,510
so don't worry about me and then you won't worry about that.

726
01:23:11,510 --> 01:23:17,500
And that would be a good thing. And that's essentially the equivalent of what we do in any other context and catch those very.

727
01:23:20,030 --> 01:23:33,850
Part of it is also. More subtlety in terms of preferences, like drowning is kind of a no brainer in the sense that nobody wants to drown.

728
01:23:33,880 --> 01:23:37,810
We assume that, okay, but then we're getting into it like suicide.

729
01:23:37,940 --> 01:23:42,700
It's like. It's. There are nuances of things like.

730
01:23:45,690 --> 01:23:55,870
Why do we have DNR orders? There are people for whom there and it is stated that a rifle isn't the answer.

731
01:23:56,400 --> 01:24:02,160
They would not make it legitimately and appropriately, would not want you to jump in and be doing CPR on them.

732
01:24:02,290 --> 01:24:08,220
Yes. And we can catch that when we engage in those kinds of conversations ahead of us.

733
01:24:08,400 --> 01:24:14,840
I'm not I mean, again, it's. There's an ethical line here.

734
01:24:16,220 --> 01:24:26,510
Everybody draws it in slightly different places. Of my personal way of thinking, it is very much grounded in.

735
01:24:30,530 --> 01:24:32,360
Let's sort of say, what do you want?

736
01:24:32,570 --> 01:24:39,350
Because I think there's a problems with going down that pathway, both because we're bad at guessing that and because people are bad at knowing that.

737
01:24:39,360 --> 01:24:44,780
Yeah. But we are experts in what the decisions are.

738
01:24:45,280 --> 01:24:51,969
And just like a lifeguard is trained to jump in. There are times when, as communicators, we should be trained to jump in and knowledge.

739
01:24:51,970 --> 01:24:56,390
And there are times when we should be trained to jump out and not jump yet.

740
01:24:56,470 --> 01:24:58,530
Yeah, and we can.

