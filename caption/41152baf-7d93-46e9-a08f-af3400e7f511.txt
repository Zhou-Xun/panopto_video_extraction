1
00:00:00,240 --> 00:00:06,420
A quick announcement for the exams.

2
00:00:06,420 --> 00:00:11,930
If you have any of your grade requests, they are due by next the start of next class period, so on.

3
00:00:11,940 --> 00:00:20,760
And as a reminder, it needs to be typed up unless it's a simple like addition of your points issue and turned into.

4
00:00:22,170 --> 00:00:27,959
And as always, you can actually lose points on a regrade request rather than gain points.

5
00:00:27,960 --> 00:00:36,330
So just make sure you consult the rubric and make sure that you're actually were miss graded and aren't just trying to

6
00:00:36,330 --> 00:00:41,580
see if you could possibly get some more points here and there because students are sometimes unhappy with their exams,

7
00:00:41,580 --> 00:00:51,400
get returned to them with fewer points. But if the answer was right, you know, and it was Ms. graded by all means, submit the exam for a grade.

8
00:00:52,110 --> 00:00:55,589
All right. So today we're going to talk about information bias.

9
00:00:55,590 --> 00:00:59,790
I will mention I have requested yet again that the temperature be fixed in this room.

10
00:01:00,210 --> 00:01:06,890
I have escalated it now. So I put in the request to various you know, I called several emails.

11
00:01:06,900 --> 00:01:13,139
I will like contact the dean's office if necessary, because we do need to get some ventilation in this room.

12
00:01:13,140 --> 00:01:17,549
And I actually think the issue is that there's no ventilation or very little in this room at the moment.

13
00:01:17,550 --> 00:01:21,840
So there you go. Okay. So information bias.

14
00:01:23,690 --> 00:01:29,879
Uh oh. And one more thing. Sorry about missing class last Thursday or rescheduling it, right?

15
00:01:29,880 --> 00:01:35,460
We just move things around. We were going to do the recorded lecture today with like an in-class activity,

16
00:01:36,600 --> 00:01:42,780
but I had a very sick child who like when five and six year olds are sick in that way.

17
00:01:43,140 --> 00:01:46,530
Right. She had a very upset stomach, but they need kind of constant attention.

18
00:01:46,560 --> 00:01:51,750
So I figured there was probably no way I could actually give you my full attention at that time.

19
00:01:51,750 --> 00:01:55,110
So it was better to to plot the classes around.

20
00:01:55,890 --> 00:02:05,760
All right. So lecture outline. And I think actually that's the first time I've missed class ever or since teaching at U of M, I am recording.

21
00:02:05,820 --> 00:02:09,090
Thank you. So there we go.

22
00:02:09,120 --> 00:02:12,719
All right, so we're lecture outlined today.

23
00:02:12,720 --> 00:02:17,070
We'll do basics of information, bias. We're going to do sensitivity and specificity.

24
00:02:17,610 --> 00:02:24,089
I talk about non differential and differential misclassification will then go into non

25
00:02:24,090 --> 00:02:28,650
differential misclassification in detail and differential misclassification in detail.

26
00:02:29,070 --> 00:02:33,600
We'll talk about some specific information bias types that you need to be familiar with.

27
00:02:35,070 --> 00:02:42,000
And we will discuss missing data as well as specific biases in ecologic studies and in equivalence studies,

28
00:02:42,000 --> 00:02:47,310
because there are some concerns that are specific to those study designs.

29
00:02:48,000 --> 00:02:56,280
So information bias is when the quality or the completeness of measurement or data collection distorts the effect estimate, right?

30
00:02:58,380 --> 00:03:03,240
It can be misclassification of exposure and or disease.

31
00:03:03,600 --> 00:03:09,640
It can also be misclassification of other factors, confounding factors, which we'll talk more about confounders next.

32
00:03:09,660 --> 00:03:13,920
We're missing data on certain variables for some subjects.

33
00:03:16,630 --> 00:03:21,790
So sources of information bias. We have measurement error, which we've talked a little bit about already.

34
00:03:22,450 --> 00:03:28,210
Missing data on disease status, exposure, status or covariance for some subjects in the study.

35
00:03:28,840 --> 00:03:33,250
Right. And this is can be missing at random. It could be not missing at random.

36
00:03:34,180 --> 00:03:39,909
There's also a tendency for the value of a continuous failed variable that is measured with error and is extreme

37
00:03:39,910 --> 00:03:45,610
on its first measurement to be closer to the center center of the population distribution on a later measurement.

38
00:03:48,670 --> 00:03:54,560
You can also have missing information on the joint distribution of exposure and disease when groups are treated as the unit of analysis.

39
00:03:54,570 --> 00:04:02,469
So that would be in the ecologic study design. So for information bias types, we've got misclassification bias.

40
00:04:02,470 --> 00:04:07,750
So two types there, right? Differential misclassification and non differential misclassification.

41
00:04:08,740 --> 00:04:13,060
And then we have specific types of bias that result in misclassification that we'll talk about.

42
00:04:13,450 --> 00:04:17,980
Recall bias. INTERVIEWER Bias. Respondent bias and observer bias.

43
00:04:18,340 --> 00:04:23,980
And we'll talk about some ways to try to deal with those biases as well, or to prevent that from occurring or at least lessen them.

44
00:04:25,270 --> 00:04:29,590
So just a quick reminder, a quick note about sensitivity and specificity.

45
00:04:30,040 --> 00:04:36,460
These will be covered in great depth in the clinical epi lecture.

46
00:04:36,760 --> 00:04:43,180
However, there's no way to really talk about misclassification if we don't talk a little bit about sensitivity and specificity.

47
00:04:43,750 --> 00:04:52,990
So I'm also going to briefly cover those here. So validity of a test, a test, ability to distinguish between who has a disease and who does not.

48
00:04:53,470 --> 00:04:56,920
There are two components to that. Sensitivity and specificity.

49
00:04:57,610 --> 00:05:02,290
So sensitivity is the ability of a test to correctly identify those that have disease.

50
00:05:02,890 --> 00:05:15,010
Sensitivity is going to be the true positives over all of those with the disease or A over eight plus C, right.

51
00:05:17,650 --> 00:05:27,180
So it's here that the individuals who are correctly marked as being a true positive, divided by everybody with the disease.

52
00:05:27,190 --> 00:05:29,710
So that's the true positives, plus the false negatives.

53
00:05:31,980 --> 00:05:39,720
So calculate sensitivity was is the sensitivity of this screening test or physical examination and mammography.

54
00:05:46,290 --> 00:06:42,180
When you go in, calculate it quickly. All right.

55
00:06:42,190 --> 00:06:49,000
So it's a over eight plus C, right? So 132 over 177, which is 74.6%.

56
00:06:50,290 --> 00:06:55,600
All right. So specificity is the ability of a test to correctly identify those that do not have the disease.

57
00:06:56,170 --> 00:07:01,330
So it's going to be the true negatives divided by all of those without the disease.

58
00:07:01,840 --> 00:07:11,730
So D over B plus B. Can you calculate specificity for the screening test?

59
00:07:46,250 --> 00:07:55,640
All right. So we get 63,650 divided by 64,633, which is equal to 98.5%.

60
00:07:57,410 --> 00:08:04,250
So for sensitivity and specificity, ideally we want something that's perfectly sensitive and and perfectly specific, right?

61
00:08:05,270 --> 00:08:12,140
So 100% sensitivity, 100% specific. But generally, there's going to be a tradeoff between sensitivity and specificity.

62
00:08:12,860 --> 00:08:17,090
Meaning as you push sensitivity, higher specificity is going to go down or vice versa.

63
00:08:17,690 --> 00:08:21,950
So for most test results, there are some people who are really clearly positive.

64
00:08:22,430 --> 00:08:29,120
There are some people who are really clearly negative. And then there's going to be a group of people who are kind of in the Green Zone, right?

65
00:08:31,040 --> 00:08:36,290
The cutoff between normal and abnormal is going to be determined by a variety of factors.

66
00:08:36,770 --> 00:08:44,510
And we will talk about those factors in the clinical lecture when we discuss picking cut offs and such.

67
00:08:45,350 --> 00:08:50,930
All right. So that's sensitivity and specificity, and we will use them throughout the rest of the lecture.

68
00:08:52,430 --> 00:09:00,620
So non differential and differential misclassification. So these are there are two types of misclassification.

69
00:09:00,920 --> 00:09:03,530
So you can have misclassification of the outcome, right?

70
00:09:03,770 --> 00:09:11,690
For non differential misclassification of the outcome is when outcome misclassification is not related to exposure status.

71
00:09:12,470 --> 00:09:18,950
Differential misclassification is going to be when the outcome misclassification is related to the exposure status.

72
00:09:21,920 --> 00:09:25,760
We can also have misclassification of our exposure variables.

73
00:09:25,760 --> 00:09:33,139
Right. And that will be non differential in differential as well. So non differential misclassification is when exclude exposure.

74
00:09:33,140 --> 00:09:38,870
Misclassification is not related to the outcome status and differential misclassification

75
00:09:38,870 --> 00:09:43,430
is when exposure exposure misclassification is related to the outcome status.

76
00:09:48,760 --> 00:09:49,050
Okay.

77
00:09:49,070 --> 00:09:58,000
So for now, differential and differential compared to differential misclassification, non differential misclassification typically leads to bias.

78
00:09:58,840 --> 00:10:03,010
Right. The expected direction of the bias is towards the null value.

79
00:10:03,880 --> 00:10:11,590
But that's a general principle. It's not always true. We'll go over some examples for when that's not true for some situations when that's not true.

80
00:10:12,550 --> 00:10:20,470
Differential misclassification, on the other hand, can bias towards away from or beyond the null value.

81
00:10:20,950 --> 00:10:25,560
Right. I'm. So kind of in any direction, essentially.

82
00:10:29,530 --> 00:10:34,840
So here is a scenario from Morgan Stern's notes.

83
00:10:35,110 --> 00:10:41,720
So here we've got the correctly classified table. What you're going to notice is there's little arrows between the cells, right?

84
00:10:41,740 --> 00:10:47,270
That means that people are being misclassified. And we'll go through two different scenarios here.

85
00:10:47,270 --> 00:10:52,659
A scenario one is where 10% of oral contraceptive users are misclassified as

86
00:10:52,660 --> 00:11:00,110
non users and 10% of oral contraceptive nonusers are misclassified as users.

87
00:11:00,130 --> 00:11:04,360
Both misclassification probabilities apply equally to cases of non cases.

88
00:11:04,690 --> 00:11:10,600
Is this non differential or differential? Scenario one What do you think?

89
00:11:14,430 --> 00:11:24,190
Yeah, that's non differential because the exposure misclassification that's occurring here is regardless of outcome status, right?

90
00:11:24,520 --> 00:11:28,830
It's happening in both equally to both cases and non cases.

91
00:11:29,280 --> 00:11:39,450
Okay. So scenario two is 10% of exposed non cases are misclassified as unexposed and 10% of the unexposed cases are misclassified as exposed.

92
00:11:42,030 --> 00:11:57,989
Is this differential or non differential misclassification? Uh, is it dependent is we're talking about exposure classification.

93
00:11:57,990 --> 00:12:02,580
Is it dependent on outcome? It is here.

94
00:12:02,910 --> 00:12:13,649
Actually, if you reread it because unexposed non cases are not being misclassified right and exposed cases are not being misclassified,

95
00:12:13,650 --> 00:12:18,510
it's only excludes non cases and unexposed cases that are being misclassified.

96
00:12:18,900 --> 00:12:27,000
So it's dependent on the outcome status here. So that's going to be what type of misclassification differential.

97
00:12:27,090 --> 00:12:31,440
Yeah. All right. So scenario one, and we'll go through this a little bit.

98
00:12:31,450 --> 00:12:41,280
We've got 200 people in the cell, so a and we've got 10% of people being misclassified between among the disease here.

99
00:12:41,730 --> 00:12:52,890
So 10% of 20 are going to come down into this cell and 10% of 150 up into the cells before cell A, we get 200 -10% of 200.

100
00:12:53,650 --> 00:13:00,150
Right. I said 20, but I meant 200 plus 10% of 150.

101
00:13:00,150 --> 00:13:00,389
Right.

102
00:13:00,390 --> 00:13:12,120
So that's those people switching up and 10% of these people moving down, we end up with 195 people in cell A, in cell B, we start with 131 people.

103
00:13:12,450 --> 00:13:14,909
10% of 131 are moving down.

104
00:13:14,910 --> 00:13:25,890
So we're going to subtract out point one times 131 and then we're going to add in in 10% of the to 19 are moving from unexposed to exposed.

105
00:13:27,030 --> 00:13:35,850
So point one we're going to add in 0.1 times to 19 and you end up with 139.8, even though we can't have point eight of a person.

106
00:13:36,210 --> 00:13:40,230
Right. We will do that for these problems. Well, you'll get partial people.

107
00:13:40,890 --> 00:13:44,670
All right. So for C, we've already calculated these out up here.

108
00:13:44,670 --> 00:13:53,760
So it ends up being 150 plus feet. 10% of the 200 that move down to plus 20 -15 gives us 155 people.

109
00:13:54,210 --> 00:14:02,340
And then for D, 219 people plus 13.1 -21.9.

110
00:14:02,340 --> 00:14:06,900
So 10% of each of those cells and we end up with 210.2.

111
00:14:07,440 --> 00:14:10,620
All right. And this is are correctly classified.

112
00:14:10,650 --> 00:14:16,930
Odds ratio of 2.23. So we do the same thing for scenario two, right.

113
00:14:16,950 --> 00:14:24,380
Once again, just multiplying it out to move people. However, this time we have 200 people here.

114
00:14:24,390 --> 00:14:29,730
Right. And we only have unexposed people moving to B being exposed.

115
00:14:30,480 --> 00:14:39,059
Right. So we're going to take 150, 10% of 150 and and move them to that start with 200.

116
00:14:39,060 --> 00:14:44,820
So we end up with 215 here, subtracting it from cell C, we end up with 135.

117
00:14:45,510 --> 00:14:54,060
And then the same thing here. But this time we have 10% of the controls, the the not disease individuals.

118
00:14:54,720 --> 00:14:57,990
Right. Are being misclassified as unexposed.

119
00:14:58,530 --> 00:15:05,160
So 131 minus point one times, 131, we end up with one 17.9.

120
00:15:06,030 --> 00:15:09,659
Right. And then to 19 plus 13.

121
00:15:09,660 --> 00:15:17,219
This is actually should be two 32.1. So slight error in the slide, but it doesn't it doesn't make a big difference.

122
00:15:17,220 --> 00:15:23,280
But. All right. Okay, so step one, right.

123
00:15:23,280 --> 00:15:25,080
Is non differential misclassification.

124
00:15:25,440 --> 00:15:35,550
And what we see here when we know that we have those tables and we calculate the odds ratio right eight times D over B times C,

125
00:15:36,680 --> 00:15:41,639
you should be able to repeat that in your sleep by now. Or certainly we'll be able to by the end of episode six.

126
00:15:41,640 --> 00:15:45,420
So one, we end up with one 81.89.

127
00:15:45,750 --> 00:15:49,020
So we biased it towards the null.

128
00:15:49,200 --> 00:15:53,040
Right. So that too is differential misclassification.

129
00:15:53,370 --> 00:15:57,060
And in this particular case, we biased it away from the null.

130
00:15:57,360 --> 00:16:06,360
But remember, it could go in any direction. There any questions there?

131
00:16:08,660 --> 00:16:13,969
There are some good examples of these tables and calculating them out in the assignment.

132
00:16:13,970 --> 00:16:21,620
So make sure you go through that in detail because this is one area where people tend to struggle a little bit.

133
00:16:21,620 --> 00:16:28,910
It looks kind of obvious until you start to try to do it, and then you may find that it's a little bit harder than you thought it was.

134
00:16:29,600 --> 00:16:40,080
All right. Okay. So non differential misclassification is when misclassification of the exposure is unrelated to the disease status, right.

135
00:16:40,100 --> 00:16:45,530
Or misclassification of a disease status is unrelated to the exposure status tends to bias towards one.

136
00:16:45,980 --> 00:16:49,070
Remember, that's a general rule isn't always true.

137
00:16:49,070 --> 00:16:57,140
So when is that not true? That's not true when the amount of misclassification of exposure or disease is extreme.

138
00:16:58,250 --> 00:17:05,840
So if you sum up sensitivity and specificity and it's equal to less than one that is extreme misclassification.

139
00:17:09,900 --> 00:17:15,510
This could also happen with misclassification of exposure. Status and disease status are not independent.

140
00:17:15,900 --> 00:17:18,540
That is when the errors are correlated between the variables.

141
00:17:19,440 --> 00:17:26,160
You might bias away from the null when the exposure variable in question is probably too much.

142
00:17:26,280 --> 00:17:34,560
So when it has more than two groups of three or four or five right can occur when the sum of census sensitivity and specificity is greater than one.

143
00:17:34,830 --> 00:17:42,180
So you can actually have pretty good test performance and still bias away from the null in this situation when you have three or more groups.

144
00:17:42,180 --> 00:17:53,899
Right. When you don't have a dichotomous. Exposure and then an excellent ecologic studies on differential exposure

145
00:17:53,900 --> 00:17:59,120
misclassification with within the groups can cause it to bias it away from mental.

146
00:18:00,800 --> 00:18:05,600
So here we have an example of non differential misclassification.

147
00:18:07,550 --> 00:18:14,360
One thing to note, how do we know this is non differential? Look at the sensitivity and specificity in the cases and the controls.

148
00:18:15,080 --> 00:18:18,080
Right. They're the same. Okay.

149
00:18:18,080 --> 00:18:21,290
So that tells you right away that that's non differential misclassification.

150
00:18:23,870 --> 00:18:30,190
And in this case, we've biased the odds ratio towards the null, you know, practice, move,

151
00:18:30,210 --> 00:18:35,480
move the cells around yourselves so that you come up with these the the new numbers.

152
00:18:37,150 --> 00:18:47,490
All right. Here we have the same sensitivity and specificity as we did last time.

153
00:18:47,910 --> 00:18:52,740
But the big difference here is that the prevalence of the exposure is really low on the controls.

154
00:18:53,160 --> 00:18:59,000
Right? So notice here it was 5050, right?

155
00:18:59,910 --> 00:19:03,600
50% of the controls had the exposure here.

156
00:19:05,340 --> 00:19:12,899
Here it's much lower. And the thing to note here is that it's much more misclassified, right?

157
00:19:12,900 --> 00:19:14,910
Instead, the true odds ratio was 4.0.

158
00:19:15,330 --> 00:19:26,400
When the prevalence of the exposure is fairly low in the controls, we end up with a 1.3 odds ratio, which is is, you know, almost nothing at all.

159
00:19:26,790 --> 00:19:32,280
Whereas when the prevalence was higher, about 50% in the controls, we ended up with 2.6.

160
00:19:32,640 --> 00:19:41,520
So the degree of bias is not only going to be controlled or determined by the sensitivity and specificity rate,

161
00:19:41,550 --> 00:19:45,330
but it's also going to be determined by the prevalence here.

162
00:19:46,440 --> 00:19:50,810
And in in any case, it would be. All right.

163
00:19:52,400 --> 00:20:01,100
So cohort studies and case control studies are a bit different in the effect that sensitivity and specificity,

164
00:20:01,100 --> 00:20:07,130
or at least sensitivity is going to have on on the amount of misclassification.

165
00:20:07,140 --> 00:20:10,400
So here we have a large closed cohort study.

166
00:20:11,630 --> 00:20:19,970
50% of people are exposed. The two true risk in the exposed is 30% and the unexposed is 5%.

167
00:20:20,270 --> 00:20:28,349
So this is going to be a relative risk of what? Pleasure.

168
00:20:28,350 --> 00:20:34,870
Right here. Six, right. That's the risk ratio.

169
00:20:36,820 --> 00:20:43,240
So here we see this is right here across the bottom specificity right in here.

170
00:20:43,240 --> 00:20:50,980
Each line is sensitivities of the top ones. One, we've got set 90%, 75%, 50%.

171
00:20:51,610 --> 00:20:57,700
Right. And what you'll notice is up here, right. We're getting very close to the true the correct risk ratio.

172
00:20:58,870 --> 00:21:11,140
But of course, as we drop in sensitivity and specificity, sensitivity and specificity, we get further and further away from the true risk ratio.

173
00:21:11,860 --> 00:21:20,420
One of the big things to notice is that sensitivity doesn't matter nearly as much as specificity.

174
00:21:20,920 --> 00:21:28,420
Right. So here, if we have almost perfect specificity, doesn't really matter what our sensitivity is, right?

175
00:21:28,720 --> 00:21:37,210
It matters a little bit. Right. It is bias. But sensitivity is not having much of an effect.

176
00:21:37,870 --> 00:21:42,880
Right. As specificity drops, we see more of an effect from sensitivity.

177
00:21:43,240 --> 00:21:52,600
But it's still not nearly as good as not having nearly as large of effect on nearly as much bias as a lower specificity.

178
00:21:52,990 --> 00:21:58,299
Right. So here, specificity of 50%, we end up with a one.

179
00:21:58,300 --> 00:22:05,530
Right. Saying that there's no relationship with the lower sensitivity or it's a pretty low relative risk,

180
00:22:05,530 --> 00:22:09,640
some somewhere between 1.5 and one, depending on the sensitivity.

181
00:22:10,780 --> 00:22:20,109
But if we have a relatively good specificity, right, there's not much difference that we see dependent on this sensitivity.

182
00:22:20,110 --> 00:22:29,110
Right. So you still end up with something much closer to two, the true value, right?

183
00:22:29,110 --> 00:22:37,419
So maybe is right here. For example, if we took a specificity of that's probably like 96, 97%, right?

184
00:22:37,420 --> 00:22:42,610
You're going to end up with a relative risk of somewhere around for two or three and a half to four.

185
00:22:45,520 --> 00:22:55,690
So you can have pretty low sensitivity and still get less bias than if you have perfect sensitivity and low to moderate specificity.

186
00:22:55,990 --> 00:22:58,149
And this is one of the things I always find really interesting,

187
00:22:58,150 --> 00:23:03,340
because at least among infectious disease people, people are super zoned in on that sensitivity.

188
00:23:03,340 --> 00:23:09,040
But I want to capture all my cases. Right? Like, I'm really concerned about misclassifying my cases.

189
00:23:09,850 --> 00:23:12,900
But you're actually much better off, right?

190
00:23:12,910 --> 00:23:16,719
Misclassifying cases.

191
00:23:16,720 --> 00:23:26,110
This is then and having a higher specificity and more sensitivity.

192
00:23:28,150 --> 00:23:32,680
So there you go. But this is a cohort study. Now, in a case control study, it looks a little bit different.

193
00:23:32,980 --> 00:23:39,340
So kind of generally true, but higher sensitivity is having a much larger effect than we've seen a cohort study.

194
00:23:40,180 --> 00:23:44,620
So here we have a large number of cases with an equal number of non cases.

195
00:23:44,620 --> 00:23:50,590
In this example, 40% exposure prevalence among the cases, 10% among non cases.

196
00:23:51,520 --> 00:23:54,820
Odds ratio again is six here.

197
00:23:54,880 --> 00:23:58,120
Right. Which we see when sensitivity is one and specificity is one.

198
00:24:00,340 --> 00:24:07,690
Here we see substantial bias even for modest amounts of misclassification.

199
00:24:08,500 --> 00:24:15,490
Right. And the pattern is differing from a cohort. Study sensitivity is having a bigger effect than we see in a cohort study,

200
00:24:17,290 --> 00:24:24,339
although once again with really high specificity, the sensitivity doesn't matter quite, quite as much.

201
00:24:24,340 --> 00:24:29,350
Right. All right.

202
00:24:31,400 --> 00:24:35,660
So non differential disease misclassification in the case control study problems of

203
00:24:35,660 --> 00:24:39,500
disease misclassification and subject selection are going to be interrelated related.

204
00:24:39,530 --> 00:24:43,249
Why is that? Because we're selected. We're selecting based on our outcome.

205
00:24:43,250 --> 00:24:45,920
Right. And we might have misclassified our outcome.

206
00:24:48,260 --> 00:24:53,240
So this is happening because cases and controls are selected from a misclassified based population.

207
00:24:54,230 --> 00:24:59,090
The amount of disease misclassification in the study population is does not equal the amount

208
00:24:59,090 --> 00:25:05,500
of disease misclassification in the base population from which the true study cases are rows.

209
00:25:07,160 --> 00:25:14,480
So this means, unfortunately, even if we know sensitivity and specificity in our base population, we're going to not going to know.

210
00:25:14,780 --> 00:25:17,810
Sensitivity and specificity in our study population.

211
00:25:20,120 --> 00:25:27,480
For disease misclassification. So what does that mean?

212
00:25:27,990 --> 00:25:31,030
I will show you what that means. Okay. Okay.

213
00:25:31,110 --> 00:25:34,679
So problems of disease, misclassification and subject selection are interrelated, right?

214
00:25:34,680 --> 00:25:37,710
That's because we're misclassifying people in their disease status.

215
00:25:38,580 --> 00:25:43,590
And then we're selecting from those misclassified people our cases and controls.

216
00:25:46,080 --> 00:25:49,770
Here we've got sensitivity and specificity in the base.

217
00:25:49,770 --> 00:25:53,159
Population is equal to 90% exposure.

218
00:25:53,160 --> 00:26:00,450
Prevalence is 60% in cases, but only 23.3% in the misclassified tables.

219
00:26:00,810 --> 00:26:04,290
So this is a large number of cases, 1 to 1 ratio.

220
00:26:05,990 --> 00:26:13,370
We. That's not right.

221
00:26:13,390 --> 00:26:20,060
It is not a 1 to 1 ratio for. Controls, right?

222
00:26:20,090 --> 00:26:25,720
No. Much. Thousand people, right?

223
00:26:25,750 --> 00:26:28,780
Ten cases, 990 controls here.

224
00:26:31,000 --> 00:26:34,629
All right. So we've misclassified people here.

225
00:26:34,630 --> 00:26:39,010
We still have a thousand people, but we're now moving people between tables.

226
00:26:39,490 --> 00:26:45,550
Low number of cases, though, and we end up with an odds ratio which is strongly biased towards mental.

227
00:26:49,030 --> 00:26:58,090
So suppose you now conduct a cumulative population based case control study by selecting all of the observed cases in a random sample of 100 controls.

228
00:26:58,790 --> 00:27:04,270
Right. So now we're not going to select from the thousand. So this is the base population risk classification.

229
00:27:09,720 --> 00:27:16,320
So for the cases right, we're going to take all of our cases that arise because we don't have that many cases.

230
00:27:17,820 --> 00:27:23,640
Note here, because there were actually very few cases where the truly crook classified,

231
00:27:23,880 --> 00:27:28,830
we end up with a lot of misclassified people in the in the case category.

232
00:27:29,610 --> 00:27:36,210
All right. So for controls, we're going to take 100 controls out of 892.

233
00:27:36,660 --> 00:27:43,470
So we're going to multiply it by 100 divided by 892, which is our sampling fraction.

234
00:27:44,310 --> 00:27:58,260
And this is what we will get here. So 20.045 for our exposed controls and 79.955 for our unexposed controls.

235
00:27:59,550 --> 00:28:05,870
All right. So that's just taking these numbers down here and multiplying by 100 divided by 892.

236
00:28:05,880 --> 00:28:11,040
Right. Because we're not going to take all the possible controls in our base population, just a sample of them.

237
00:28:13,570 --> 00:28:15,550
All right. So this is our table.

238
00:28:16,930 --> 00:28:25,810
So we've got our misclassified table and our correctly classified table with the same 208 people if they were correctly classified.

239
00:28:26,290 --> 00:28:30,780
So remember, sensitivity and specificity in the base population was 90%.

240
00:28:30,790 --> 00:28:40,210
That's what we had multiplied out originally. Right. And remember, sensitivity is those classified as disease positives divided by the truly disease.

241
00:28:40,280 --> 00:28:47,260
Individual specificity is those that are classified as negative divided by the the the all the real negatives.

242
00:28:48,640 --> 00:28:57,280
Okay. So in our study population, we end up with 504 plus 3.6 divided by 9.112.

243
00:28:57,880 --> 00:29:01,900
So a sensitivity of 0.98.

244
00:29:07,440 --> 00:29:11,129
Specificity in the study population. This is that they're correctly classified.

245
00:29:11,130 --> 00:29:25,050
Remember 19.978 plus 79.9910 divided by one 98.880.

246
00:29:25,050 --> 00:29:30,090
So we end up with a specificity of 50% in the study population.

247
00:29:30,480 --> 00:29:36,690
And this is because. Right. Our base population was misclassified.

248
00:29:41,470 --> 00:29:54,490
All right. So non differential disease mask misclassification in a case control study as a major issue or can be the

249
00:29:54,490 --> 00:30:00,550
validity of disease classification may be quite different in a case control study than in the base population.

250
00:30:00,820 --> 00:30:03,610
So that means our sensitivity and specificity may be quite different.

251
00:30:04,540 --> 00:30:11,620
Knowing sensitivity and specificity in the base population would not allow us to correct the odds ratio estimate.

252
00:30:12,820 --> 00:30:21,370
However, there are a few things to remember here. Generally, disease misclassification is not going to be as extensive as illustrated here.

253
00:30:22,300 --> 00:30:27,370
Most non cases can be eliminated from the case series by diagnostic confirmation.

254
00:30:27,700 --> 00:30:34,600
So you could require a second test, right? Which would help you identify who was truly deceased.

255
00:30:35,890 --> 00:30:47,740
So if you were to eliminate the nine cases from the Case series, we end up with an odds ratio of 5.98, which is almost unbiased from the last example.

256
00:30:50,090 --> 00:30:55,670
And the reason for this is that eliminating the nine cases from the case series is really increasing the specificity.

257
00:30:56,360 --> 00:31:04,850
Right. And specificity is going to induce more bias than sensitivity that's at least at similar lower levels.

258
00:31:08,530 --> 00:31:12,610
And here because we had really high sensitivity already anyways where we've almost eliminated.

259
00:31:15,020 --> 00:31:18,910
That bias. All right. Okay.

260
00:31:18,920 --> 00:31:22,820
So we can correct for non differential misclassification under certain circumstances.

261
00:31:23,990 --> 00:31:28,130
That is if the sensitivity and specificity for disease and exposure tests are known.

262
00:31:29,960 --> 00:31:33,260
Yes. So if it was.

263
00:31:36,140 --> 00:31:54,180
But it's not just control. Could you still not be able to you would still want to go would probably with the second follow up test if

264
00:31:54,180 --> 00:32:03,780
you were using a test with such low specificity to start but with the base population you would still get.

265
00:32:06,830 --> 00:32:12,380
You are sure that wasn't true? For what it's worth.

266
00:32:15,030 --> 00:32:18,030
And dead today. Yeah, I believe it would still be true.

267
00:32:21,810 --> 00:32:27,180
Let me think about Kate's career. Definitely an identity. It would still be true, but I think this cohort would still be true as well.

268
00:32:31,240 --> 00:32:38,590
All right. All right. So under certain circumstances we can correct for non differential misclassification does

269
00:32:38,590 --> 00:32:44,580
require two assumptions misclassification errors for errors for both variables are independent,

270
00:32:44,590 --> 00:32:50,230
but as they're not correlated and misclassification of each variable is non differential by the other variable.

271
00:32:51,070 --> 00:32:58,300
So to correct for non differential misclassification, you're going to use these formulas which you must memorize.

272
00:32:59,140 --> 00:33:03,880
No, I'm kidding you. Just seeing if you all were paying attention.

273
00:33:04,630 --> 00:33:09,100
We will never make you memorize these formulas. You'll be provided with the formulas, but there you go.

274
00:33:11,830 --> 00:33:16,030
Yeah. So here they are right here.

275
00:33:16,570 --> 00:33:20,770
The capital E and capital s p are sensitivity and specificity for the disease,

276
00:33:20,770 --> 00:33:29,649
misclassification and s little E and big s little p are sensitivity and specificity of the exposure misclassification.

277
00:33:29,650 --> 00:33:33,040
So we've got both disease and exposure misclassification here.

278
00:33:33,520 --> 00:33:42,579
Right? So we will just take this formula used these here write substitute demand.

279
00:33:42,580 --> 00:33:51,370
So for a C there for substituting that in.

280
00:33:51,850 --> 00:33:56,950
Right. And you end up with this formula.

281
00:33:57,340 --> 00:34:01,989
Okay. And that's how you would then correct in a cohort study.

282
00:34:01,990 --> 00:34:09,790
If you know the sensitivity and specificity of both, however your term determine the outcome and the exposure status, right?

283
00:34:10,060 --> 00:34:14,890
You can then correct for that misclassification with those assumptions I mentioned before,

284
00:34:17,320 --> 00:34:22,150
corrected estimates of other measures of association such as risk difference can also be derived,

285
00:34:23,020 --> 00:34:29,980
and note that the corrected relative risk does not depend on sensitivity of the outcome here.

286
00:34:30,290 --> 00:34:35,890
E And that's because it's in the numerator and the denominator, so it actually cancels out.

287
00:34:38,770 --> 00:34:44,730
All right. So that's sensitivity of the disease classification or alcohol.

288
00:34:45,360 --> 00:34:51,120
All right. So here we have an example with a fixed cohort with possible misclassification of exposure,

289
00:34:51,570 --> 00:34:56,040
exposure and for disease, we're going to look at both the effect on the relative risk and the risk difference.

290
00:34:56,730 --> 00:35:01,040
Right. So none of these are a perfect test, right?

291
00:35:01,170 --> 00:35:03,690
Or at least perfect test for both disease and exposure here.

292
00:35:03,690 --> 00:35:11,990
We've got a perfect test for disease, but you can see that the exposure, misclassification,

293
00:35:12,000 --> 00:35:19,830
8.9 and point nine rates are 90% for each for sort of for sensitivity and specificity.

294
00:35:21,030 --> 00:35:25,920
And we end up with a relative risk of 1.67.

295
00:35:27,990 --> 00:35:33,270
One of the things to kind of look at here is to notice that non differential misclassification of both

296
00:35:33,270 --> 00:35:41,100
variables so both the disease and the exposure is going to result in more bias than of just one or the other.

297
00:35:42,780 --> 00:35:49,560
So for example, we can compare rows three and four, right?

298
00:35:50,070 --> 00:35:53,460
So we get more bias. This is the corrected estimate, right?

299
00:35:53,490 --> 00:36:03,510
This is what we got when we we observed we get more bias here in row four when we've got non differential

300
00:36:03,510 --> 00:36:09,120
misclassification of both of our R does outcome of interest and of our exposure of interest.

301
00:36:09,930 --> 00:36:13,049
You can also compare you see a much bigger difference.

302
00:36:13,050 --> 00:36:23,730
I think it's row six and each year a large difference and you'll see that this occurs for both our relative risk and our risk difference.

303
00:36:25,850 --> 00:36:29,690
Anything else I want to point out here? One other thing to kind of point out here.

304
00:36:29,810 --> 00:36:38,510
Right. So the only thing that is misclassified here is we've got sensitivity in disease in individuals is lowered to 80%.

305
00:36:38,840 --> 00:36:41,990
Right. Notice that there is no bias.

306
00:36:42,680 --> 00:36:49,610
What we observed up here, 1.5 in our corrected estimate, or 1.5 for our relative risk rate.

307
00:36:49,610 --> 00:36:51,469
And that's because, remember, it cancels out.

308
00:36:51,470 --> 00:36:58,850
So the sensitivity of the outcome doesn't matter for the relative risk, but it still does matter for the risk difference.

309
00:36:58,850 --> 00:37:13,060
And when we get a little bit of bias there. Oh, this is a slide for me not to forget to point that out to you.

310
00:37:13,070 --> 00:37:16,130
But I remember to point out at this point. So. Okay.

311
00:37:18,380 --> 00:37:18,650
All right.

312
00:37:18,650 --> 00:37:26,360
So a correlation of misclassification, variable variables biasing towards the null is dependent on the misclassification not being correlated.

313
00:37:27,080 --> 00:37:31,010
When misclassification is correlated, that is when it's not independent.

314
00:37:31,370 --> 00:37:34,670
Right? Then a positive bias is possible.

315
00:37:35,360 --> 00:37:41,509
Okay, so remember, we could if we've got a risk factor, we're going to be biasing away from the null.

316
00:37:41,510 --> 00:37:49,370
Then positive correlation is likely to arise when the measurements of both variables are based on subjective reports by the subjects themselves.

317
00:37:54,220 --> 00:37:59,530
So problems of correcting misclassification. It ignores other sources of bias.

318
00:37:59,770 --> 00:38:04,540
So selection, bias and confounding might be difficult to determine whether classification errors

319
00:38:04,540 --> 00:38:08,920
are independent or correlated or whether they are non differential or differential.

320
00:38:10,870 --> 00:38:14,709
Sensitivity and specificity may not be known and then you can't really correct for it,

321
00:38:14,710 --> 00:38:20,770
although you could still do sensitivity analyzes using different sensitivity and specificity.

322
00:38:22,090 --> 00:38:27,070
This is one of those things where like when I first learned this many years ago,

323
00:38:27,490 --> 00:38:32,860
I thought I had never used that I wouldn't want I used like kind of going back in and trying to correct,

324
00:38:33,220 --> 00:38:38,080
you know, but you'll see in a lot of papers, maybe less so in newer papers.

325
00:38:38,080 --> 00:38:41,200
But that may not actually be true when people say like, Oh,

326
00:38:41,200 --> 00:38:46,900
but there was lower sensitivity of the test and that might have bias or specificity or whatever.

327
00:38:47,110 --> 00:38:51,850
So they're kind of guess that what would have happened to it when you could actually figure it out?

328
00:38:52,690 --> 00:39:02,680
And when you're comparing your outcomes, which maybe, you know, if you're if you're doing that or your findings sorry, which you're using now,

329
00:39:02,890 --> 00:39:06,570
maybe the test is much more sensitive than specific then test, you know,

330
00:39:06,580 --> 00:39:11,800
from 20 years ago and you're trying to, to explain why your findings might differ from that old study.

331
00:39:12,130 --> 00:39:18,010
You could actually then if you have some information on sensitivity and specificity of that test,

332
00:39:18,010 --> 00:39:21,100
or at least a range for it, you could adjust their estimates.

333
00:39:21,100 --> 00:39:25,030
Right. And and use that in your discussion to talk about rather than just kind of

334
00:39:25,030 --> 00:39:29,260
speculating on which way it would go and how much of an effect it would have.

335
00:39:31,450 --> 00:39:34,870
But remember, this is kind of this does ignore all other sources of bias.

336
00:39:34,870 --> 00:39:41,740
This is just dealing with information bias and and specifically my own differential misclassification.

337
00:39:42,010 --> 00:39:46,060
All right. Any questions about non differential misclassification?

338
00:39:51,420 --> 00:39:58,380
Okay. We will take a ten minute break and start back up at 1055 just so I don't cut down on differential misclassification and have.

339
00:40:22,886 --> 00:40:27,146
Okay, let's get started back up. So we're going to talk about differential misclassification.

340
00:40:28,436 --> 00:40:34,436
So differential misclassification is when the degree of misclassification differs between the groups being compared.

341
00:40:35,756 --> 00:40:39,085
So this could be among the cases in controls, right?

342
00:40:39,086 --> 00:40:41,065
Or the exposed in the unexposed,

343
00:40:41,066 --> 00:40:48,776
depending on we're talking whether we're talking about differential misclassification of the outcome or differential misclassification of exposure.

344
00:40:51,746 --> 00:40:54,625
So this will happen, for example,

345
00:40:54,626 --> 00:41:01,316
when the sensitivity and or the specificity of the classification of exposure status are different between cases and controls.

346
00:41:02,426 --> 00:41:10,975
It would also happen when the sensitivity of the specificity of the outcome status is different by exposed or not exposed.

347
00:41:10,976 --> 00:41:15,656
Right. It can bias the association in either direction.

348
00:41:16,736 --> 00:41:22,406
So in pictures here we have a case control study. Right here is our reference population.

349
00:41:22,676 --> 00:41:31,256
We're selecting our cases and controls in. And what we see is more cases who are not exposed are being misclassified as exposed.

350
00:41:31,676 --> 00:41:35,515
Right. So we have some general misclassification occurring, right.

351
00:41:35,516 --> 00:41:42,716
That's what those arrows are representing. But it's much more likely that not exposed cases will be misclassified as

352
00:41:42,716 --> 00:41:47,396
exposed cases than exposed cases being misclassified as not exposed cases or,

353
00:41:48,236 --> 00:41:56,306
you know or controls. Right. This area right here really exposed controls being misclassified as non-experts controls.

354
00:41:58,906 --> 00:42:05,535
Okay. So for differential misclassification, as an example from SQL, right, we've got in this example,

355
00:42:05,536 --> 00:42:11,956
the sensitivity for cases is greater than controls, but the specificity for cases and controls is the same, right?

356
00:42:11,956 --> 00:42:19,966
So here we see sensitivity for cases is 0.96, but the sensitivity for controls is 0.7.

357
00:42:21,136 --> 00:42:27,196
Right? So our true odds ratio here is for that same example, kind of before different scenario.

358
00:42:27,556 --> 00:42:32,746
Now we've misclassified them and we end up with an odds ratio of 5.7.

359
00:42:32,746 --> 00:42:39,346
So we are biased away from the null. It's differential, right?

360
00:42:39,346 --> 00:42:51,676
Because the sensitivity of the outcome of the exposure status is differing by cases and controls.

361
00:42:52,966 --> 00:42:59,326
All right. Here, both the sensitivity and specificity of the cases is greater than controls.

362
00:43:01,516 --> 00:43:07,606
And in this particular case. Right, that the bias is that the odds ratio towards the null.

363
00:43:07,606 --> 00:43:14,296
So in the opposite direction from the last example go through and kind of work through these because

364
00:43:14,296 --> 00:43:21,346
I find that really helpful myself for understanding which way it's going in specific directions.

365
00:43:24,346 --> 00:43:27,616
All right. Any questions about differential misclassification?

366
00:43:30,896 --> 00:43:35,326
All right. So specific information bias types, recall bias play.

367
00:43:35,326 --> 00:43:38,445
A lot of you've heard about recall bias before, right? It's inaccurate.

368
00:43:38,446 --> 00:43:44,296
Recall of past exposure, a form of exposure, identification bias.

369
00:43:45,466 --> 00:43:50,806
One example of that I think I've mentioned it before is tan and ability and melanoma in the nurses health study.

370
00:43:53,206 --> 00:43:57,646
So in that study cases tended to overall report low tanning ability when post

371
00:43:57,646 --> 00:44:02,685
diagnostic nurses interviews were compared to pre diagnosis diagnosis interviews.

372
00:44:02,686 --> 00:44:08,326
So they asked for both before and after they were diagnosed.

373
00:44:09,016 --> 00:44:18,046
The relationship was not seen in control. So when they were asking earlier or later, right, so they then said, okay,

374
00:44:18,046 --> 00:44:26,626
well that therefore is not a reflection of like greater education around tanning and tanning ability, etc.

375
00:44:29,216 --> 00:44:35,306
Because certainly attitudes towards panning have changed over time, right, as we've realized.

376
00:44:35,306 --> 00:44:39,536
But that's a very big risk factor for skin cancer, for melanoma.

377
00:44:42,296 --> 00:44:47,366
So how do we control recall bias one way?

378
00:44:47,456 --> 00:44:52,256
The best way probably is to verify participant interviews by checking pharmacy records,

379
00:44:52,256 --> 00:44:56,096
hospital charts or other sources, if possible, not always possible.

380
00:44:57,806 --> 00:45:06,206
Use use of a disease control group can also be effective for for helping to deal with recall bias.

381
00:45:07,166 --> 00:45:12,206
And that's because disease persons are more likely to have ruminated on past exposures.

382
00:45:13,136 --> 00:45:19,586
So they'll at least be less likely to report themselves as unexposed if they were truly exposed.

383
00:45:20,096 --> 00:45:25,946
May in fact be more likely to also misclassified themselves, but probably depends on the exposure.

384
00:45:28,316 --> 00:45:35,456
Recall bias objective markers of exposure susceptibility are less prone to recall bias, right?

385
00:45:35,786 --> 00:45:45,266
So an example in the same study, the nurses health study, they asked about hair color and people were less likely to misreport their hair color.

386
00:45:46,616 --> 00:45:58,866
Their natural hair color. Right. Recall bias is more a more common problem in case control studies, but certainly can occur in cohort studies as well.

387
00:46:00,786 --> 00:46:07,896
So some strategies to prevent or reduce recall is to assess exposure by using objective data collected reported prior to disease onset,

388
00:46:08,256 --> 00:46:09,365
some medical records,

389
00:46:09,366 --> 00:46:17,406
employment records, etc. Use controls, as I said that that may have similar record rates for typically another group with a different disease.

390
00:46:19,866 --> 00:46:24,746
One thing to note here, though, is that both of those strategies can actually induce selection bias.

391
00:46:24,756 --> 00:46:29,436
So then you may be trading an information bias for a selection bias.

392
00:46:32,296 --> 00:46:38,296
And that's because you would only be able to use information from people that had these medical records or employment records.

393
00:46:38,566 --> 00:46:43,366
So we're kind of you know, those are the only people that are eligible to be selected.

394
00:46:44,116 --> 00:46:48,946
And here, right, we're now basing selection to the study on having that other disease.

395
00:46:49,616 --> 00:46:53,356
We talked about that during the case control lecture.

396
00:46:54,106 --> 00:47:00,435
Sure. All right. So response bias one surveys on there, several types of response bias.

397
00:47:00,436 --> 00:47:05,446
One, surveys participants have a tendency to have a tendency to agree with all statements.

398
00:47:06,446 --> 00:47:10,246
That's because people tend to want to please people that they're talking to.

399
00:47:13,786 --> 00:47:17,296
They may alter response or behavior because they are part of a study.

400
00:47:18,136 --> 00:47:23,056
Right. You can also have a bias towards only responding in the extremes of a scale.

401
00:47:23,446 --> 00:47:28,656
So on a 1 to 5 scale, you might get, you know, only ones and fives, but mostly ones and fives.

402
00:47:28,666 --> 00:47:32,356
You can also have a bias towards only giving values in the middle of the scale.

403
00:47:33,136 --> 00:47:35,596
So usually 1 to 5 scale and you get lots of threes.

404
00:47:35,596 --> 00:47:43,336
I will say that these tend to be pretty cultural, but may also depend on the question you're asking.

405
00:47:43,666 --> 00:47:50,716
Right. So they'll vary a lot by country. I remember when I first started working in Nicaragua, although now people are really used to scales.

406
00:47:51,106 --> 00:47:54,286
People were not used to scales of like, I really like this too.

407
00:47:54,286 --> 00:48:01,566
I really dislike it. Right? Would be like on a scale of 1 to 7, then I get the data is like why is it all ones in seven, right?

408
00:48:01,576 --> 00:48:06,286
Like, like they weren't like it was just like they either liked it or they didn't like it, right?

409
00:48:07,276 --> 00:48:16,576
Whereas in the, in the US, at least in certain populations, you'll have this more of this tendency towards going towards the middle of the scale.

410
00:48:17,326 --> 00:48:23,716
And so it could vary by country, it can vary by populations inside of a country.

411
00:48:24,976 --> 00:48:32,926
So just something to be kind of aware of. And social desirability bias is also another bias that you can deal with on the surveys.

412
00:48:32,926 --> 00:48:39,856
So people knowing or, you know, are thinking, are they going to think down on me if I answer this way or that way?

413
00:48:40,726 --> 00:48:45,916
Right. So there is that bias. All right. So we've got.

414
00:48:45,916 --> 00:48:49,426
INTERVIEWER bias is another information bias that can come into effect.

415
00:48:49,426 --> 00:48:53,776
It's a bias introduced by the interviewer due to the outcome to knowledge of the outcome.

416
00:48:54,916 --> 00:49:00,726
It often results as a result of a clarifying question, you know.

417
00:49:00,766 --> 00:49:05,506
So they might say, well, I mean, are you sure you didn't take that medication?

418
00:49:05,506 --> 00:49:14,506
You know, the congenital condition that your child was born is strongly tied to that medication, you know, and the person says, well, maybe I did.

419
00:49:14,506 --> 00:49:18,766
You know, actually, I think I think you're right. I think that's what I took. Right, etc.

420
00:49:20,206 --> 00:49:29,416
It can happen in the opposite way as well. So ways to reduce interviewer bias or having defined protocols with training for the interviewers so

421
00:49:29,416 --> 00:49:36,946
they understand that they're not supposed to ask those clarifying questions or do extra probing.

422
00:49:37,306 --> 00:49:39,496
And you can also blind the interviewers, right?

423
00:49:39,496 --> 00:49:48,016
If they don't know the outcome status, then they're not likely to then, you know, probe or by outcome status or they can't.

424
00:49:48,766 --> 00:49:53,176
Okay. Respondent bias is occurs when the outcome is assessed by the participant.

425
00:49:54,016 --> 00:49:59,776
It's a form of outcome ID bias. An example of that would be migraine headache episodes.

426
00:50:01,756 --> 00:50:09,646
So whenever possible, confirm information given by a participant using more objective means hospital records, clinical records, pharmacy records, etc.

427
00:50:12,836 --> 00:50:18,716
Observer bias is a decision of whether an outcome is present, is affected by knowledge of exposures.

428
00:50:19,166 --> 00:50:22,166
This is more likely to happen with soft outcomes.

429
00:50:23,096 --> 00:50:31,196
It's a form of outcome ID bias. An example of this is a pathologist is more likely to give a diagnosis of alcoholic cirrhosis.

430
00:50:31,976 --> 00:50:35,546
Right. So that would be of the liver, right?

431
00:50:35,846 --> 00:50:38,636
If he or she knows that the patient was an alcoholic.

432
00:50:40,946 --> 00:50:48,686
This is the same causes diagnostic bias, but it's occurring when participants have already been enrolled.

433
00:50:49,166 --> 00:50:53,576
So rather than being a selection bias, it's now an information bias.

434
00:50:55,046 --> 00:50:58,285
And it doesn't it's not impacting their selection into the study.

435
00:50:58,286 --> 00:51:01,886
Right. Just their classification of whether or not they have the outcome.

436
00:51:03,836 --> 00:51:09,446
So observer bias or methods to control or to mask observers to the exposure status of to blind them.

437
00:51:09,656 --> 00:51:12,656
Yes. Just to clarify, diagnostic bias.

438
00:51:14,426 --> 00:51:18,935
A person would be involved in the study. Yeah.

439
00:51:18,936 --> 00:51:21,756
So diagnostic biases and case control studies.

440
00:51:22,056 --> 00:51:28,445
If you remember I said, well, but if this cause happens in cohort studies, it will be an information bias.

441
00:51:28,446 --> 00:51:36,816
And we'll talk about that later. So. It would almost be easier just to call it the same thing.

442
00:51:36,816 --> 00:51:40,176
And. Just classify it.

443
00:51:40,176 --> 00:51:45,826
Right. But there you go. This is what it's normally called.

444
00:51:46,156 --> 00:51:50,076
Okay. So mask observers to exposure status of linemen.

445
00:51:50,806 --> 00:51:56,056
You can use multiple observers so the presence or outcome must be agreed upon by two out of three observers.

446
00:51:56,056 --> 00:52:03,916
Sometimes this is done with all three initially reading something radiographs or a big one where you get multiple people reading them,

447
00:52:03,916 --> 00:52:11,446
particularly for pneumonia, because you would think pneumonia would be pretty easy to classify, but actually it isn't.

448
00:52:12,736 --> 00:52:17,986
And so what they'll typically do there is they have two people read the x ray, right?

449
00:52:18,436 --> 00:52:25,006
Two different radiologists. And if there is disagreement, then a third person also reads the x ray, right?

450
00:52:28,576 --> 00:52:37,515
Whether you start with three or start with two, it depends a bit on study resources as well as how much disagreement you see and the cost of people.

451
00:52:37,516 --> 00:52:43,666
Radiologists are actually pretty highly paid individuals, right, at least in the US.

452
00:52:44,656 --> 00:52:51,586
And so for those then you'd start with two and then add a third one if you need it, essentially.

453
00:52:52,306 --> 00:52:54,496
Okay, so performance bias in our cities,

454
00:52:55,726 --> 00:53:01,786
it's a bias that arises due to the knowledge of the allocated interventions by participants and personnel during the study.

455
00:53:03,136 --> 00:53:08,086
You can deal with this bias through blinding if it's possible for blind individuals.

456
00:53:09,226 --> 00:53:17,445
And there are many like interventions where people think it's not possible to blind and they have actually

457
00:53:17,446 --> 00:53:25,546
done blinded studies like I don't know if Abraham brought it up in the intervention trial lecture,

458
00:53:25,906 --> 00:53:32,355
but for example, like you would think with surgeries, you know, people you can be blind blinded to whether or not you had back surgery.

459
00:53:32,356 --> 00:53:38,296
Right. But they actually do follow surgery despite what they they put people under.

460
00:53:38,296 --> 00:53:43,396
They have in their teams. Right. They put people under they go in, they make an incision.

461
00:53:43,756 --> 00:53:50,656
Right. You get stitches. But you didn't actually get the surgery done, obviously, since there are potential harms to that.

462
00:53:51,076 --> 00:53:54,705
Right. Like because putting anybody under right, there's always the chance.

463
00:53:54,706 --> 00:53:57,886
But you don't you don't actually wake up or it induces other problems.

464
00:54:00,256 --> 00:54:05,506
A surgical incision can always get infected. Obviously it's painful, etc.

465
00:54:05,776 --> 00:54:09,916
Right. So you'd have to have a really good reason for doing that study.

466
00:54:11,296 --> 00:54:15,516
Presumably you'd have to have a really good reason.

467
00:54:16,126 --> 00:54:22,606
This study. They, you know, they, they have done it in certain spinal surgeries, for example,

468
00:54:23,386 --> 00:54:30,226
just because people became convinced that perhaps the spinal surgery wasn't actually helping anybody and might be harming people.

469
00:54:30,826 --> 00:54:35,686
Right. And they're really not sure of whether or not it's helping, potentially harming.

470
00:54:36,586 --> 00:54:43,155
And so and then probably even given that if you put it presented it to six different IRBs,

471
00:54:43,156 --> 00:54:46,666
there would be some IRBs that said it was ethical and other.

472
00:54:46,666 --> 00:54:50,805
I agree with that. You knew where you were not going to. You know, we're not possibly going to touch that.

473
00:54:50,806 --> 00:54:54,876
So I'm. No.

474
00:54:55,366 --> 00:55:00,046
Yes. Oh, they would be told they would absolutely have to be informed.

475
00:55:00,346 --> 00:55:04,636
So they would know there was potential for that type of action. Yes.

476
00:55:09,906 --> 00:55:15,636
That if they did not know that, that would be very unethical, particularly for something that might harm them.

477
00:55:16,176 --> 00:55:24,096
So because we know that they're you know, although there's a there is a placebo effect, that's one thing that they saw actually,

478
00:55:24,096 --> 00:55:28,745
that they felt like that a lot of the effect of of of having the surgery was actually

479
00:55:28,746 --> 00:55:35,226
a placebo effect because people who had the incision had basically the same effect.

480
00:55:35,296 --> 00:55:46,146
So anyways, remember, assessment of misclassification is subjective because there's often no way to go back and obtain the true data.

481
00:55:46,776 --> 00:55:52,416
Right. No. Study data are perfect, so every study is going to have something misclassified.

482
00:55:53,286 --> 00:55:57,066
There's always some amount of this classification probably of everything in the study.

483
00:55:57,966 --> 00:56:03,365
And usually you'll use kind of common sense or clinical epidemiological sense to examine

484
00:56:03,366 --> 00:56:08,196
the nature and the magnitude of the misclassification that might have occurred.

485
00:56:09,126 --> 00:56:18,276
Right. And then we'll do a sensitivity analysis to talk about the effects we think it would have on our, in fact, estimate.

486
00:56:19,146 --> 00:56:23,526
Right. So there you go. All right.

487
00:56:24,936 --> 00:56:31,416
Basic questions to ask when assessing misclassification bias in studies based on the data collection methods,

488
00:56:31,416 --> 00:56:35,796
what study elements may have been misclassified? So you want to think about that right away.

489
00:56:37,296 --> 00:56:41,946
Is this misclassification likely to be differential or non differential?

490
00:56:43,206 --> 00:56:47,736
And then what is the expected impact of misclassification on the study results?

491
00:56:53,036 --> 00:57:00,596
Summary on information bias. Information bias arises from how information is collected from participants in the study,

492
00:57:01,256 --> 00:57:04,946
and then misclassification can be non differential or differential.

493
00:57:08,066 --> 00:57:12,116
All right. So now we're going to are there any questions about that, about missing data?

494
00:57:15,046 --> 00:57:18,046
All right. So missing data.

495
00:57:18,636 --> 00:57:26,356
Missing data on certain variables in some subjects represents both measurement and selection problems that can produce bias in effect estimation.

496
00:57:27,766 --> 00:57:30,696
There are different ways that people deal with missing data. Right.

497
00:57:30,706 --> 00:57:36,956
One one way that you'll see that people deal with missing data a lot is you just exclude subjects with the missing data.

498
00:57:37,816 --> 00:57:43,096
Right. You can treat missing covariance as another category of that variable.

499
00:57:44,176 --> 00:57:49,035
So what would that look like? Smoked, you know, smoker.

500
00:57:49,036 --> 00:57:52,696
Non smoker. Unknown. Missing would be an example.

501
00:57:54,846 --> 00:58:01,085
Inverse probability weighting IP w is another way and we'll talk about that a little bit more.

502
00:58:01,086 --> 00:58:04,895
Multiple imputation and then full likelihood.

503
00:58:04,896 --> 00:58:07,786
I will not talk about the likelihood any more than this side.

504
00:58:08,096 --> 00:58:12,096
It's beyond the scope of the course, but it's one way to deal with missing data from the question.

505
00:58:12,186 --> 00:58:16,286
I feel like in a lot of surveys you'll see a response like prefer not to answer,

506
00:58:16,326 --> 00:58:20,196
don't know, but then you don't see those responses like in the analysis.

507
00:58:20,946 --> 00:58:25,986
So I just wondered like, I guess how should you treat those types of responses?

508
00:58:26,796 --> 00:58:32,896
Um, it depends on how common they are and your abilities, right?

509
00:58:32,916 --> 00:58:40,206
You, like I said, you could treat them as another covariate and separate that out, so prefer not to respond.

510
00:58:40,656 --> 00:58:50,106
Um, you could use ip w and we'll talk about that in a minute to actually kind of update people who are similar to that person who did answer.

511
00:58:52,956 --> 00:58:57,646
Or multiple invitation to guess at what their answer would have been to that question had the answer.

512
00:58:58,896 --> 00:59:06,695
Um, all right. So excluding subjects with missing data, um, you see, there's a lot, right?

513
00:59:06,696 --> 00:59:10,685
Particularly in older papers, but all over the place it reduces the sample size.

514
00:59:10,686 --> 00:59:13,776
So that's one issue, right? So that's going to broaden our confidence interval.

515
00:59:15,636 --> 00:59:19,086
It can introduce bias from the data are not missing completely at random.

516
00:59:20,886 --> 00:59:25,025
The assumption of missing it completely at random is likely to be violated.

517
00:59:25,026 --> 00:59:32,855
Right. It's usually not missing at random. Uh, because a lot of the missing this tends to be because of human behavior.

518
00:59:32,856 --> 00:59:36,816
So on a survey, if they say prefer not to answer, that is missing.

519
00:59:37,026 --> 00:59:48,786
That is not missing at random. If you did a Redcap survey and somehow failed to put in a page of your questions right at

520
00:59:48,786 --> 00:59:53,945
the beginning and then caught it and added it back in as long as when people filled out,

521
00:59:53,946 --> 00:59:58,746
the survey was not related to what their responses might have been, that that would be like an example of,

522
00:59:59,226 --> 01:00:03,816
in a way, missing at random, or if you wouldn't expected those answers to change over time.

523
01:00:05,916 --> 01:00:09,086
But most of the time it's not going to be missing at random, right?

524
01:00:09,606 --> 01:00:17,076
Or at least not completely at random. All right. So you can treat missing covariate variables as another category of the variable.

525
01:00:18,126 --> 01:00:21,785
One plus of this is it's not going to result in a loss of subject's right.

526
01:00:21,786 --> 01:00:24,906
You're going to keep everybody in the study.

527
01:00:25,866 --> 01:00:32,976
So an example, right? I already mentioned smoker, non smoker and then missing unknown as a third category.

528
01:00:35,106 --> 01:00:42,276
This once again can lead to bias when estimating the exposure effect, and this is when data are not missing completely at random.

529
01:00:43,476 --> 01:00:54,605
So people, you know, here, maybe people who were smokers are going to be more likely to kind of put missing unknown than people who are non smokers.

530
01:00:54,606 --> 01:01:01,116
And this may be related to the outcome. You're interested in looking at inverse probability weighting.

531
01:01:02,676 --> 01:01:07,926
So for complete cases that is complete participant files.

532
01:01:07,926 --> 01:01:12,626
That is, those with all information are weighted by their probability of being complete.

533
01:01:14,166 --> 01:01:20,196
And this method is also used to correct for bias in surveys and that's where we use sampling fractions and weights.

534
01:01:20,886 --> 01:01:24,336
Right. So what does this look like here?

535
01:01:24,966 --> 01:01:29,496
We have some missing data, right? In these little kind of not filled in.

536
01:01:33,686 --> 01:01:43,406
Uh, triangles and boxes and whatnot. So down here, there's a line there where the line is here.

537
01:01:43,796 --> 01:01:47,786
Right. You see what would happen if you just used the data as it was.

538
01:01:48,686 --> 01:01:55,726
But then we've got these two individuals here right where we can up wait their data, right?

539
01:01:55,796 --> 01:02:07,886
Because they were unlikely to have completely full data, but they did. And then that then corrects the the relationship between X and Y, given Z.

540
01:02:09,476 --> 01:02:14,606
Okay. So it's just these like people who are less likely to be complete cases, but they happen to be complete cases.

541
01:02:14,996 --> 01:02:22,256
You're going to update their data if you can be used with repeated measures.

542
01:02:24,926 --> 01:02:31,196
Sensitivity analysis you allow for the use of IP w with data that's not missing at random.

543
01:02:33,266 --> 01:02:39,506
IP w requires that all conditional probabilities not be and must be non zero.

544
01:02:39,806 --> 01:02:42,326
If that's not true, then you can use g estimation.

545
01:02:43,526 --> 01:02:49,546
An example of a zero probability would be for example, if you're looking at an occupational exposure,

546
01:02:49,566 --> 01:02:55,346
people not working would have a zero probability because they couldn't possibly have that occupational exposure.

547
01:02:57,596 --> 01:03:03,205
IP W is also used for causal inference. Write inverse probability of treatment weighting.

548
01:03:03,206 --> 01:03:14,096
IP t w. Multiple imputation is a statistical technique for dealing with incomplete datasets.

549
01:03:14,666 --> 01:03:17,846
Basically involves three steps, right?

550
01:03:17,876 --> 01:03:22,355
Imputation values are filled in multiple times for in data sets.

551
01:03:22,356 --> 01:03:27,686
So for some number of datasets. So you impute the values, then you do the analysis.

552
01:03:28,556 --> 01:03:35,456
Each dataset will be analyzed separately and then you pull it together to integrate the analysis results.

553
01:03:37,476 --> 01:03:45,576
So in comparing IP versus multiple reputation, IP requires a model for the probability that an individual is a complete case,

554
01:03:47,016 --> 01:03:50,976
whereas multiple imputation requires a model for the distribution of missing data.

555
01:03:51,096 --> 01:04:00,425
Given observed data, there are some advantages of wealth purification over IP w generally IP w can only use fully observed

556
01:04:00,426 --> 01:04:05,336
variables where multiple imputation can use data from individuals with only partially missing data.

557
01:04:05,346 --> 01:04:11,536
So that's one benefit. And multiple imputation is generally more efficient than IP w.

558
01:04:15,756 --> 01:04:23,946
All right. Are there any questions about that? So that's how we deal with missing data or some ways that we deal with missing data.

559
01:04:24,996 --> 01:04:36,666
All right. Bias and ecologic studies. So we've got the ecologic fallacy, right, that you learned about in in that lecture.

560
01:04:37,056 --> 01:04:40,836
EcoLogic studies have the potential for substantial bias, in effect,

561
01:04:40,836 --> 01:04:46,726
estimation results from making causal inferences about individual effects on the basis of group data.

562
01:04:46,746 --> 01:04:52,386
Right. That's the ecologic fallacy. And did you use the example of Prussia, the Prussians?

563
01:04:52,716 --> 01:04:59,525
Yeah. So if you remember that example. So bias in ecologic studies.

564
01:04:59,526 --> 01:05:02,556
There are three types of bias that you need to be familiar with.

565
01:05:03,006 --> 01:05:07,536
We've got aggregation bias, specification bias and cross level bias.

566
01:05:08,676 --> 01:05:12,486
Aggregation bias is bias due to grouping or aggregating individuals.

567
01:05:13,236 --> 01:05:18,756
The association that is the core correlation coefficients or the relative risk obtained

568
01:05:18,756 --> 01:05:24,785
from group data may not be the same as what would be obtained or is not the same sorry,

569
01:05:24,786 --> 01:05:28,056
as what would be obtained from individual data. That's aggregation bias.

570
01:05:30,136 --> 01:05:34,096
There's specification bias bias due to the confounding effect of the group.

571
01:05:35,056 --> 01:05:41,596
It's essentially confounding by group. And we'll talk more about confounding in the next lecture sort of occurs when some

572
01:05:41,596 --> 01:05:45,856
property of the group is a risk factor for the outcome that you're interested in.

573
01:05:46,786 --> 01:05:54,616
Extraneous factors are distributed differently across the groups, and then there's cross level bias,

574
01:05:54,946 --> 01:05:59,386
which is basically the joint effect of aggregation bias and specification bias.

575
01:06:00,226 --> 01:06:10,486
So it's the result of the group being a confounder and the risk difference, the risk being different, risk difference being different across groups.

576
01:06:14,296 --> 01:06:23,566
There's also pure specification bias that arises when the model selected is not the correct functional form of the relationship.

577
01:06:23,956 --> 01:06:29,956
So here, right, we've got use of a linear model when a non-linear risk function would be the

578
01:06:29,956 --> 01:06:36,016
correct form to use might lead to a little bias when estimating small effects.

579
01:06:36,946 --> 01:06:45,856
However, in summary, ecologic rate ratios may be extremely sensitive to the choice of model form, so you'll need to pay attention to that.

580
01:06:47,536 --> 01:06:50,686
All right. Last but not least, bias in equivalence studies.

581
01:06:51,406 --> 01:06:57,866
You remember a couple of studies. Intervention Trials Lecture right?

582
01:06:59,826 --> 01:07:07,476
Tobias and superiority and equivalence studies and superiority trials lost to follow up and compliance issues issues tend to bias towards binaural.

583
01:07:08,226 --> 01:07:11,436
Remember, if we're going to have a bias, we want a bias towards men.

584
01:07:11,446 --> 01:07:16,926
Also, we're not as concerned about that. We, of course, want to keep lost to follow up in compliance.

585
01:07:17,196 --> 01:07:20,286
Lost follow up as low as possible in compliance as high as possible,

586
01:07:21,906 --> 01:07:28,476
but in equivalent study trials lost to follow up in compliance issues tend to increase the likelihood the trial will find equivalence.

587
01:07:29,526 --> 01:07:33,786
That is, it increases the likelihood of accepting the alternate hypothesis.

588
01:07:33,876 --> 01:07:38,856
And if you remember, the alternative hypothesis here is that there is no difference between your two treatments.

589
01:07:41,976 --> 01:07:48,636
That's one of the big issues with, you know, the criticisms people have of equivalent studies, right?

590
01:07:48,966 --> 01:07:53,316
If you do an equivalent study poorly, you're more likely to find equivalence.

591
01:07:58,956 --> 01:08:04,236
All right. So just a quick example of bias.

592
01:08:04,416 --> 01:08:07,856
This comes from some materials adopted from Dubai.

593
01:08:08,346 --> 01:08:16,416
You know, I went to grad school together, actually. So here's an example of estrogen and endometrial cancer.

594
01:08:17,226 --> 01:08:20,886
So it was actually on it. Just finish.

595
01:08:21,216 --> 01:08:27,996
I'm not saying that. Right. An opposed estrogen, i.e., without progestin, is known to increase the risk of endometrial cancer.

596
01:08:29,736 --> 01:08:33,756
So that's true now. But in the 1970s and 1980s, that was controversial.

597
01:08:34,656 --> 01:08:38,826
And so a series of case control studies were performed to examine the association.

598
01:08:40,956 --> 01:08:46,296
So here is one of those studies. So estrogens and endometrial cancer in a retirement community.

599
01:08:47,286 --> 01:08:54,876
And they concluded that the risk ratio for any estrogen use was estimated from all available evidence to be 8.0,

600
01:08:55,416 --> 01:08:59,586
and that for conjugated estrogen used to be 5.6.

601
01:09:00,216 --> 01:09:08,496
Right. So there were a number of studies that were performed, such as this one using community based control groups.

602
01:09:09,366 --> 01:09:13,146
Hospital based control groups yielded similar estimates.

603
01:09:17,586 --> 01:09:23,226
However, there was another paper that came out which said, like, while this might be a detection bias, right?

604
01:09:24,476 --> 01:09:30,966
There was a paper published in the New England Journal of Medicine indicating that detection bias may have led to an overestimate of the effect.

605
01:09:31,476 --> 01:09:38,855
You're like eight. That's really high. You know, they proposed an alternate method of control selection by selecting cases and controls

606
01:09:38,856 --> 01:09:43,656
from the same group that present for an intra endometrial diagnostic procedure.

607
01:09:45,876 --> 01:09:49,086
Controls are being worked out for benign conditions.

608
01:09:49,986 --> 01:09:54,246
Reasoning was that the benign conditions also would be subject to detection bias.

609
01:09:54,246 --> 01:09:57,606
Right, because they're undergoing more testing and exams.

610
01:09:58,776 --> 01:10:02,176
So by using their alternate control group, they showed.

611
01:10:02,196 --> 01:10:09,276
So they did a study use this alternate control group. They showed that the effect estimate which much was much closer to the null 1.7.

612
01:10:11,646 --> 01:10:21,906
Right. That's quite different from 8.0. And obviously, there are a lot of benefits right now to estrogen replacement therapy as well.

613
01:10:23,106 --> 01:10:29,046
So what Atchison and Rothman then came in and they had their own kind of take on this.

614
01:10:29,436 --> 01:10:34,026
So that paper was accompanied by an editorial by Hutchison and Rothman.

615
01:10:34,876 --> 01:10:38,886
You know, in this often happens when the reviewers really don't agree.

616
01:10:38,896 --> 01:10:43,236
So they get like two reviewers who say, like, this is fantastic, amazing.

617
01:10:43,506 --> 01:10:46,805
And the third reviewer was like, what are they doing?

618
01:10:46,806 --> 01:10:50,106
This is, you know, like this is potentially terrible, etc., etc.

619
01:10:50,316 --> 01:10:54,185
So then the editor says, like, would you like to write an editorial about your position?

620
01:10:54,186 --> 01:11:00,336
We're going to publish this anyways because we're not really clear on where where, you know, what the true answer is.

621
01:11:00,336 --> 01:11:03,876
So we're going to publish this paper, but we'll give you the opportunity to write an editorial.

622
01:11:04,686 --> 01:11:10,536
So they write an editorial and they proposed that rather than removing detection bias,

623
01:11:10,536 --> 01:11:16,476
they were introducing bias because some of the benign conditions that cause bleeding could be induced by estrogen use.

624
01:11:17,526 --> 01:11:22,566
And so that would result in a falsely high frequency of estrogen use in the control group.

625
01:11:23,346 --> 01:11:29,396
So then you'd be selecting on exposure. Right. So what does that look like?

626
01:11:29,406 --> 01:11:36,516
Here is our DAG. Right. So we've got estrogen use. We don't know for sure if it's related to uterine cancer,

627
01:11:37,536 --> 01:11:44,586
but we've got this benign condition is caused by estrogen use and that results in selection into the study,

628
01:11:44,586 --> 01:11:47,706
as does whether or not you have uterine cancer. Right.

629
01:11:47,736 --> 01:11:55,306
So this is a case control study. So you here, you've addressed detection bias.

630
01:11:55,606 --> 01:12:00,856
Right. But we've now introduced what a selection bias.

631
01:12:03,026 --> 01:12:10,416
If you believe the stag. So what ended up happening with this kind of story?

632
01:12:10,436 --> 01:12:15,256
Well, people tried. Attempted using various control groups.

633
01:12:15,266 --> 01:12:19,736
You know, there was no perfect control group detection bias.

634
01:12:20,396 --> 01:12:24,596
How much that was impacting things was examined using an autopsy series.

635
01:12:25,856 --> 01:12:32,096
In the end, evidence accumulated showing a strong, positive relationship between estrogen use and inter mutual cancer.

636
01:12:37,046 --> 01:12:40,646
And you know, this does teach an important lesson about bias.

637
01:12:42,626 --> 01:12:46,465
Okay. And so with that. Any questions for today.

