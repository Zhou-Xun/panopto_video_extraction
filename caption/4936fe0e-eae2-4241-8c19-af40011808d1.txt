1
00:00:01,990 --> 00:00:13,080
The court has had a reason for doing that, which is a good thing.

2
00:00:13,090 --> 00:00:20,940
It was a great meeting because I never met her in person.

3
00:00:21,750 --> 00:00:29,920
What is the best way to communicate with her? My lab, which is a biotech audience, is already I'm giving it to someone in biological.

4
00:00:29,940 --> 00:00:36,360
Do not know what I'm going to say, how to use it.

5
00:00:36,360 --> 00:00:43,680
And I know it takes everything you have to give you.

6
00:00:43,980 --> 00:00:48,090
Well, if that becomes a thing I'd have to work with in that.

7
00:00:53,880 --> 00:01:00,600
Okay, I finished grading your homework. Number two, I'd like to talk a little bit about your homework.

8
00:01:00,630 --> 00:01:03,810
Number two, people start lecture today. So the.

9
00:01:03,840 --> 00:01:11,760
Okay, she's not here. Oh, you're not?

10
00:01:13,560 --> 00:01:18,630
Misclassification is very important to you.

11
00:01:18,660 --> 00:01:27,770
And she. You're here through your set.

12
00:01:27,860 --> 00:01:32,670
I mean, no, we're not here. Ben Smith.

13
00:01:33,800 --> 00:01:38,640
Someone's here and it's here. You don't see.

14
00:01:42,780 --> 00:01:46,860
Ro. Yo, yo, yo.

15
00:01:54,360 --> 00:02:00,820
What's this? Not here. One.

16
00:02:01,160 --> 00:02:11,300
Case number one. It's not the union.

17
00:02:11,330 --> 00:02:19,020
Right. I think has got to wear a face mask because so often the ones.

18
00:02:23,820 --> 00:02:31,930
Okay. Cool.

19
00:02:33,340 --> 00:02:38,670
Okay. You know when.

20
00:02:43,230 --> 00:03:02,700
Okay. More people coming. Yeah, I mean, it also, you know, this is all natural, you know, like your handwriting.

21
00:03:10,020 --> 00:03:15,409
Getting ever more comfortable or getting into. Okay.

22
00:03:15,410 --> 00:03:22,940
Let's talk a little bit of homework. Number two. I think you do the well for problem one.

23
00:03:23,210 --> 00:03:26,900
I don't have any concerns. So that you were able to derive this.

24
00:03:27,080 --> 00:03:31,700
That means you get good training from six or one or six or two,

25
00:03:31,700 --> 00:03:39,110
but you are able to do that and most of them are able to identify over dispersion and so on.

26
00:03:39,690 --> 00:03:44,360
There's no problem. Okay. Problem two.

27
00:03:46,370 --> 00:03:59,650
So. So some some some of you just train all they are squibs and I'll pull them pages, please.

28
00:04:00,250 --> 00:04:04,990
I spent quite a bit of time in trying to identify spots.

29
00:04:05,020 --> 00:04:06,550
Where are your answers?

30
00:04:07,090 --> 00:04:23,460
And I really that your your your solutions are buried in those solved software or software generated volumes of computation result and so on.

31
00:04:25,270 --> 00:04:34,270
I feel that to you you have talked about the, the observations and so on, so forth and.

32
00:04:38,030 --> 00:04:49,480
That's your copy. One thing I think is what I was looking for is both your point of views.

33
00:04:49,690 --> 00:04:58,540
I know that you're trying to explain things. I mean, that I mean, I did not ask you to really do something.

34
00:04:58,540 --> 00:05:07,020
But when when you do statistics, right. I really want to you, you know, to to express your own opinions.

35
00:05:07,030 --> 00:05:11,530
I don't. Some people did that, but most people do not do that.

36
00:05:11,530 --> 00:05:17,740
I really want to you told me your very unique lens.

37
00:05:17,790 --> 00:05:21,430
Use your very unique lens to tell me something. Right.

38
00:05:21,700 --> 00:05:26,020
It's not just say, okay, I say four picks. I say I say four picks.

39
00:05:26,110 --> 00:05:33,880
You don't need to tell me. I know you'll say four picks in the playoffs, but tell me why, what, what, what, what, what?

40
00:05:34,210 --> 00:05:38,820
What are the things behind the four picks? That's what I wanted to read through.

41
00:05:38,940 --> 00:05:44,409
Right. So, I mean, I did not I did at the point. But I just see that as a Boston rehashing.

42
00:05:44,410 --> 00:05:47,920
Right. You tell like you go presentation.

43
00:05:47,920 --> 00:05:51,999
Here's the Paul you can say four piece. That's the end of your presentation.

44
00:05:52,000 --> 00:05:56,740
I don't think so. Right. You need to tell people when that happens, why that happens.

45
00:05:57,040 --> 00:06:01,710
What what are your lens of focus of explaining it?

46
00:06:01,720 --> 00:06:06,940
Why don't you believe that people people are not really data measurement error.

47
00:06:07,420 --> 00:06:11,590
There is a what are the possible differences there?

48
00:06:12,100 --> 00:06:17,740
Right. Couple of sentences there. I understand your idea of thoughts.

49
00:06:18,070 --> 00:06:21,990
It's not just tell me what you see. Right. I know I can see it.

50
00:06:22,000 --> 00:06:29,740
I have no problem to say that. But I want to you tell me something a little bit behind what you see.

51
00:06:30,310 --> 00:06:32,860
I mean, some people do. Most people did not. Okay.

52
00:06:33,970 --> 00:06:43,330
Another thing is that when you generate futures, the the one part that you likely to make are you can, you know,

53
00:06:43,900 --> 00:06:55,420
be sloppy is labeled because the one that once somebody says that one figure were solved, in some words, I mean, there can be confusion.

54
00:06:55,420 --> 00:07:00,850
If you don't give enough for accurate labeling, you have to label things correctly.

55
00:07:01,090 --> 00:07:08,260
Right. So I know it takes a lot of time to work on the labeling because you need to write one more line of coding.

56
00:07:09,010 --> 00:07:19,480
And I think accurately and precisely and informative, you know, put your formatted labels there, you know,

57
00:07:19,510 --> 00:07:26,290
that is kind of self people when they read the figure, you know, sometimes we will give presentation.

58
00:07:26,620 --> 00:07:31,179
You have lines and lines of scripts and you quickly talk about it.

59
00:07:31,180 --> 00:07:36,200
But when you present a figure, people really look at the figure very carefully.

60
00:07:36,230 --> 00:07:40,690
Right. So that it's very useful.

61
00:07:40,690 --> 00:07:46,450
You have kind of a self-contained sort of future where people can understand by just reading figures.

62
00:07:46,450 --> 00:07:51,850
So to, you know, follow your your presentation, your argument.

63
00:07:51,870 --> 00:07:56,620
So labeling is very important. I think it's something that you need to pay attention.

64
00:07:56,620 --> 00:08:00,190
I think you are able to do it is sometimes just your, you know,

65
00:08:01,060 --> 00:08:08,560
did not pay much attention to that in the labeling part of the and the captions

66
00:08:08,560 --> 00:08:13,570
can be a little bit more I know most of people write confused by captions.

67
00:08:14,230 --> 00:08:23,560
It's a good place where you can explain to them more than something that is hard to be labeled in future.

68
00:08:23,650 --> 00:08:35,800
Right. So so you can explain a little bit more their party figure I think that's remember that figure should be to a large extent, self content.

69
00:08:36,020 --> 00:08:43,330
Okay. So that so that's something I want you to improve if you can.

70
00:08:45,490 --> 00:08:51,399
Problem three is the first time to see, you know, your, your last skill data analysis.

71
00:08:51,400 --> 00:08:55,840
And I saw a word emotional you couple of emails that I.

72
00:08:56,860 --> 00:09:06,060
I feel that the people trying to explain them all of these aren't I noticed that there's a unit behind this person.

73
00:09:06,180 --> 00:09:06,850
Right. Right.

74
00:09:07,180 --> 00:09:18,280
So, so we have this problem all of is whatever the cognitive model put into the model, there must be a unit associate you're talking about per minute.

75
00:09:18,370 --> 00:09:25,540
You are talking about per ten people per one, meaning people there must be per unit there.

76
00:09:25,990 --> 00:09:40,510
So, so, so something related to the unit of that problem rate has to be something of, you know, you should consider for the offset term.

77
00:09:40,680 --> 00:09:44,880
So because across different age groups there are different population size.

78
00:09:45,500 --> 00:09:52,280
This offshoot term is certainly a very critical for you to build up a interpretive model.

79
00:09:53,150 --> 00:09:58,280
So I point out that in the midterm I gave you the population size.

80
00:09:58,290 --> 00:10:04,670
And so that's a good thing to notice to learn from this project.

81
00:10:04,700 --> 00:10:08,600
Right? Otherwise, the higher interpretive estimation result.

82
00:10:13,290 --> 00:10:20,310
So a lot of people called it the H variable as a, you know, dummy variable like the use factor.

83
00:10:20,400 --> 00:10:25,799
As I said that I want you to call this as a ordinal categorical variable.

84
00:10:25,800 --> 00:10:31,020
Namely, you need to call them 012 rather than 0101.

85
00:10:31,200 --> 00:10:44,490
Okay. So, so if you call the age variable as by two dummy variables, you basically lose this originality, the ordering of the categories.

86
00:10:44,880 --> 00:10:49,680
The way to call to a ordering categorical is call them as 012.

87
00:10:50,400 --> 00:10:55,620
Okay. Or you use the middle age of the interval. But either way, it's fine.

88
00:10:55,650 --> 00:11:06,790
You can use the the middle point of the interval as one variable to coat your ordinal age variable or you use zero 1 to 2.

89
00:11:06,870 --> 00:11:14,370
Now call the last two dummy variables in that case that you basically make this two variables exchangeable.

90
00:11:15,090 --> 00:11:19,710
Do you do now that you force this ordinary T in the encoding variable?

91
00:11:20,850 --> 00:11:25,080
So if you say 012, do you mean like coded numerically?

92
00:11:25,200 --> 00:11:28,620
Yeah. Numerically? Yes. Numerically? Yeah.

93
00:11:30,510 --> 00:11:34,780
I have a question. If you got it numerically, then you're saying that the step service thing is.

94
00:11:34,920 --> 00:11:41,610
I don't think that's the case because. Yeah, that's if you don't feel comfortable, that's the one way to call the ordinary categorical,

95
00:11:41,610 --> 00:11:45,150
where if you don't feel comfortable, you can't use the middle age.

96
00:11:45,990 --> 00:11:52,530
Right. What I'm saying is that if you coded as numeric, you're saying that I'm not predicting like zero one and they're not treated as dummy variable.

97
00:11:52,540 --> 00:11:57,090
I'm thinking there's a relationship. I can also predict virus like the age of three or four.

98
00:11:57,570 --> 00:12:04,440
And if you don't coded as factors that may vary marked regression, you're treated as continuum.

99
00:12:04,890 --> 00:12:11,130
Yeah, it is truly a continuous variable because you can say that when you move one category from one to another.

100
00:12:11,610 --> 00:12:14,910
So what's the change of this? That's right.

101
00:12:15,420 --> 00:12:20,840
Right. Of a you're making a very strong assumption saying that's the change from the 18 to

102
00:12:20,840 --> 00:12:30,010
16 from below 18 is the same as the change from 65% is a six time Kramer to 18 to 65.

103
00:12:30,010 --> 00:12:33,220
Because of that assumption, I think it needs to be justified.

104
00:12:35,300 --> 00:12:40,160
Yeah. Well, then, do you have a better solution than this?

105
00:12:41,420 --> 00:12:49,940
If you suppose you want to take care of this already narrative, the ordering of a category, what would be your solution for that?

106
00:12:50,350 --> 00:13:02,310
Oh, I think I think just just treat them as factor, because if you treat a factor, then you lose the ordinary order of the order of the categories.

107
00:13:02,340 --> 00:13:05,419
Right. Okay. So you think order. No. Okay.

108
00:13:05,420 --> 00:13:10,160
So you're saying that Oriental didn't need to be the ordinal is a not really to reflect this.

109
00:13:10,820 --> 00:13:15,730
You're the already ordering of the categories. So. Yeah.

110
00:13:17,290 --> 00:13:27,100
Just like if you if you the age is older right you want to really incorporate is older or younger as part of the coding.

111
00:13:28,060 --> 00:13:39,549
Right. If you do, you know, you treat the 17 or younger as the reference, the other two category as a sort of another exchangeable category.

112
00:13:39,550 --> 00:13:43,060
You don't have any older, younger consent.

113
00:13:43,150 --> 00:13:47,320
Right. We just see. Well, here's my reference. I compare the reference group.

114
00:13:47,320 --> 00:13:52,400
What is the the other group would look like, right?

115
00:13:52,420 --> 00:14:01,510
There's just no ordering of a category. Right. So but, you know, that's the middle age group is younger than the 65 older group.

116
00:14:01,810 --> 00:14:08,190
Right. So. So in categorical variable analysis, people just simply call 012.

117
00:14:08,200 --> 00:14:13,300
If you don't feel comfortable, you can use the middle point of the H interval as the variable.

118
00:14:13,990 --> 00:14:20,500
Every group presented a variable in that. There's no treating as a numerical variable.

119
00:14:20,560 --> 00:14:30,020
Yeah. Okay. Yeah.

120
00:14:30,230 --> 00:14:34,730
So most of you just interpret result for significance.

121
00:14:35,030 --> 00:14:41,420
Do you see significance or not? You do not see significance using, you know, the p value.

122
00:14:41,840 --> 00:14:51,080
But I but you know the magnitude also is part of this model output that you should be able to interpret that.

123
00:14:51,110 --> 00:14:57,009
Right. So one thing is about this over dispersion.

124
00:14:57,010 --> 00:15:04,300
And so I don't know what you learn from 651.

125
00:15:05,500 --> 00:15:09,280
So, you know, there are two types.

126
00:15:09,670 --> 00:15:13,080
That's a number. That's a sample you show for over dispersion.

127
00:15:13,100 --> 00:15:17,470
Right. So some of you did disappear some type of test.

128
00:15:18,190 --> 00:15:27,990
So basically, you you are assured that the variance of your of the column pass this form sequence were over,

129
00:15:28,000 --> 00:15:32,580
you realize is the meaning of of the variable.

130
00:15:32,620 --> 00:15:37,780
Right. So this morning later the form is called Pearson type over dispersion.

131
00:15:38,470 --> 00:15:44,980
Some of you did this test. Okay, so test the water and obviously, come on, score equal to one.

132
00:15:47,170 --> 00:15:51,550
Go to test that one or not. The Bears is same as the mean.

133
00:15:51,670 --> 00:15:55,479
Okay, so that's that's the one thing people did.

134
00:15:55,480 --> 00:15:58,900
This is called Pearson type of over dispersion.

135
00:15:59,390 --> 00:16:08,500
Okay. So but if you really want to test for everything, then that's the model of negative binomial and that's.

136
00:16:20,020 --> 00:16:26,950
So. So there is not only to test the over dispersing of possible distribution nested in that negative binomial right.

137
00:16:27,250 --> 00:16:35,410
So in this case, the reverse of negative binomial negative binomial is new times.

138
00:16:36,670 --> 00:16:39,970
You were over doing that.

139
00:16:40,810 --> 00:16:53,110
So we around rather negative binomial regression, you run in it like all of you have done this use this theorem negative by normal distribution.

140
00:16:53,110 --> 00:16:57,430
You estimate we say the parameter, write this, this is out.

141
00:16:58,060 --> 00:17:02,890
So here, of course, the larger say that they're more or less over dispersion.

142
00:17:03,130 --> 00:17:09,310
Okay. So. So that there is a test to test the one.

143
00:17:09,400 --> 00:17:12,940
Now, this one over Sycamore seen that equal to zero?

144
00:17:13,270 --> 00:17:20,110
Or you feel like you can rewrite this as more of stimulus for this square where Sigma

145
00:17:20,110 --> 00:17:26,050
Square is dispersion from you can trace test of the sigma score equal to zero.

146
00:17:26,680 --> 00:17:35,530
Okay. So in our there is a test that allow you to test this over dispersion equal to zero.

147
00:17:36,070 --> 00:17:39,520
Okay. Now test this for one. This test is equal to zero.

148
00:17:40,810 --> 00:17:53,200
So. So in this case, because this this parameter is on the boundary of parameter space, because the Sigma Square has to be zero bigger.

149
00:17:53,230 --> 00:17:58,300
You cannot test this to be, but this cannot be negative.

150
00:17:58,870 --> 00:18:07,659
So the null value of not hypothesis B now valid prime for about three which is sigma squared

151
00:18:07,660 --> 00:18:12,310
dispersion parameter equal to zero is right on the boundary of the parameter space.

152
00:18:13,180 --> 00:18:21,040
So that the like a ratio test, the test does not follow Chi Square distribution.

153
00:18:21,790 --> 00:18:30,940
It follows a mixture of Chi Square. This is half of zero degrees of freedom of Chi Square, just ranging with zero degree spread.

154
00:18:31,480 --> 00:18:35,680
And the problem of Chi Square distribution is one degrees of freedom.

155
00:18:37,360 --> 00:18:47,530
Okay. So, so, so that's that's basically the the issue where you have the this test positive for Sigma equal to one.

156
00:18:48,250 --> 00:18:53,559
This one is not the boundary because the sigma squared in this person.

157
00:18:53,560 --> 00:18:56,860
Time can be easily smaller while bigger than one.

158
00:18:57,100 --> 00:19:05,470
Okay. But if you test this lasting situation between positive and DEC is by no means the parameter

159
00:19:05,800 --> 00:19:11,830
becomes you tested this longer theta equal to zero or sigma squared equal to zero.

160
00:19:12,280 --> 00:19:19,870
This prime dom parameter parameter value break in the DOM hypothesis is right on the boundary of the prime space.

161
00:19:20,140 --> 00:19:28,990
So like innovation test that is no longer follow chi square student but follows a measure of Chi Square powerful coming from

162
00:19:29,320 --> 00:19:35,230
Chi Square distribution with zero degrees freedom and half coming from the Chi Square distribution is one degrees free.

163
00:19:38,050 --> 00:19:41,770
You will learn as we walked, when you'll take it all to.

164
00:19:41,800 --> 00:19:44,830
Okay, so there is kind of nonstandard condition,

165
00:19:44,830 --> 00:19:52,530
like a recent test because you learn like a recent testing six or two where basically you test number within the parameter space,

166
00:19:52,810 --> 00:19:58,840
but if you want to test it, you know, do the like a region test on the boundary of the parameter space,

167
00:19:58,840 --> 00:20:02,860
then you end up as the Chi Square distribution. Make sure Chi Square.

168
00:20:06,040 --> 00:20:11,800
So there's a story about this thing. And besides.

169
00:20:12,320 --> 00:20:14,830
Right. So, you know, you have useless leaks.

170
00:20:18,060 --> 00:20:25,540
This bizarre product mix that basically is very important, sourced from, you know, analyzed the longitudinal data using genomics.

171
00:20:25,540 --> 00:20:36,000
The fact from all right, the first version I remember was the first release that the version of 8.50 I think many years ago by SAS company.

172
00:20:36,690 --> 00:20:46,830
And then in the in the product mix the test of these, you know, the second parameter of Ranbaxy, the various components.

173
00:20:47,340 --> 00:20:51,930
Okay so you have brand effect smaller we have fixed it that you have the brand effects.

174
00:20:55,160 --> 00:21:02,299
So. So you have random facts following almost universally one dimensional situations where of course,

175
00:21:02,300 --> 00:21:06,470
you confess that these segments would be equal to zero.

176
00:21:06,500 --> 00:21:11,390
This is various components in Iran. In fact, small, basic wonder and out there as a random facts.

177
00:21:12,470 --> 00:21:17,629
So the first version, I think this 58.55 or 8.51.

178
00:21:17,630 --> 00:21:22,130
I don't remember exactly the use libration pass.

179
00:21:22,130 --> 00:21:23,810
There's Chi Square distribution,

180
00:21:23,810 --> 00:21:31,550
which is wrong because this parameter seems to be square is on the boundary because you cannot have a negative variance parameter.

181
00:21:32,750 --> 00:21:42,590
So so the thing like a recent test or for testing the various components parameter mixed fact model actual

182
00:21:42,590 --> 00:21:51,350
follow a chi square distribution of this mixture half of chi square zero degree and half of one degree.

183
00:21:52,280 --> 00:21:55,489
So why? Why use this product?

184
00:21:55,490 --> 00:22:04,050
I noticed that they made a mistake and rule emailed to the company then to correct this distribution

185
00:22:04,100 --> 00:22:12,280
to to to to calculate a p value for the for this test and I think starting version of 9.0.

186
00:22:12,290 --> 00:22:19,160
So they made a right correction of their they are the test for the various components.

187
00:22:19,580 --> 00:22:24,649
So same situation for the plotting dispersion parameter testing.

188
00:22:24,650 --> 00:22:29,180
If you're looking at the nesting sort of situation problem with being negative by now,

189
00:22:29,180 --> 00:22:37,460
you face the same situation that your DOM parameter is right on the boundaries parameter space and then you no longer have this.

190
00:22:37,880 --> 00:22:42,090
A single chi square distribution has to be a big sort of suspicion.

191
00:22:45,100 --> 00:22:57,940
Okay. So one thing that I really want to see at the very end of your problem three is I really want to see defeated minor sporting figure number.

192
00:22:58,000 --> 00:23:02,530
Nobody proposed that to me. And they say, okay, here is my estimate better.

193
00:23:02,530 --> 00:23:05,049
And so I really want to see.

194
00:23:05,050 --> 00:23:15,570
I think I'm very a picture driven person to understand that as a result, I always encourage my student to show me a figure, to show me a plot.

195
00:23:15,580 --> 00:23:28,150
I feel that I am more sort of sort of I can appreciate a love more when I see a figure then.

196
00:23:28,540 --> 00:23:37,299
So by now it was not required. But anyway, so when you see your result, I know that you're trying to explain, you know,

197
00:23:37,300 --> 00:23:44,200
the p value or everything to the magnitude that I really want to see because there's a sign change.

198
00:23:44,200 --> 00:23:47,330
Some sometimes go the negative, the positive negative.

199
00:23:47,950 --> 00:23:55,629
I if you want to see the entirety of this twin, I really want to see that how this changes over time.

200
00:23:55,630 --> 00:24:01,810
Show me the picture that that old little more convincing appealing to me to understand the entire point

201
00:24:02,050 --> 00:24:08,980
because the universe you just assume a nine twin which is easier to imagine it going down over time.

202
00:24:09,490 --> 00:24:17,110
But when you have this supply of is different points and then I really want to see how that

203
00:24:17,110 --> 00:24:27,460
changes and that that's a good place that you can get some more sort of explanation about this,

204
00:24:28,660 --> 00:24:37,420
you know, the four peaks you observe in the problem to install and yeah,

205
00:24:37,750 --> 00:24:46,510
so problem four is sort of you continue to incorporate the vaccination into it and so on.

206
00:24:47,650 --> 00:25:03,010
I think it's okay that no, no, the serious concerns there, but I really think that some some discussion on confounding factor will be very useful.

207
00:25:03,010 --> 00:25:06,790
That really is part of something.

208
00:25:06,790 --> 00:25:12,519
I like to talk to you about this the group project.

209
00:25:12,520 --> 00:25:22,030
So now we spend quite the time on the homework to to get that the mortality data backs in the data.

210
00:25:22,390 --> 00:25:28,510
But we we know that there is a strong confounding factor about the number of confirmed cases.

211
00:25:29,320 --> 00:25:42,219
Right. So the mortality is know driven by the population size, but also is largely related to the number of confirmed cases.

212
00:25:42,220 --> 00:25:48,700
If you have more people get infected, of course you are more likely to have a higher.

213
00:25:49,390 --> 00:25:57,340
That's true. So I think there are a couple of confounding factors that should be related to this study of mortality.

214
00:25:57,340 --> 00:26:07,090
And so in there and of of course, that people talk about that this political segregation and also some some people argue in the.

215
00:26:12,960 --> 00:26:21,150
Leave the in the first homework about the weather like we have look for full peaks in the homework, too.

216
00:26:21,180 --> 00:26:30,540
Some people say, oh, the summertime because, you know, the temperature rises, then, you know, the somehow helps to kill the bugs somehow.

217
00:26:30,870 --> 00:26:40,590
So so some people have the hypothesis to look at the the association between the test read versus the mythological variables.

218
00:26:41,610 --> 00:26:50,939
So I'm thinking that our next project, we should try to get some of the important confounding factors as group of objects to expand our analysis,

219
00:26:50,940 --> 00:26:58,050
not just looking at deaths read and the vaccination, but looking at some potential confounding factors.

220
00:26:58,170 --> 00:27:01,590
Right. So, so that's why I'm trying to design project.

221
00:27:01,590 --> 00:27:09,990
I will give you all the details. I already have some primary thoughts about which group, what kind of what kind of confounding factors.

222
00:27:10,450 --> 00:27:14,190
Then on Thursday, I will give you a very specific task.

223
00:27:14,460 --> 00:27:24,720
Let's focus on the expansion or data in the group project so that we have more choices on the table for your final project to write.

224
00:27:24,720 --> 00:27:29,370
A nice final project to include more confounding factors.

225
00:27:29,370 --> 00:27:35,760
Scene analysis. Yes. So and this is a question I had that I want to ask on.

226
00:27:36,480 --> 00:27:43,710
I sort of understand the difference between the first project and the final project, given we have only a month left of class.

227
00:27:43,780 --> 00:27:48,420
Yeah, I just wanted to sound like a timeline of when which is due.

228
00:27:49,200 --> 00:27:56,429
So yeah, I think the of this pool project mostly it will be a data capture or data cleaning and

229
00:27:56,430 --> 00:28:03,690
do some preliminary data analysis that will be due sometime the later this month.

230
00:28:03,930 --> 00:28:15,320
Right. Okay. So the final project will be due anytime of middle late December before the university closed.

231
00:28:16,280 --> 00:28:20,030
So for the final project, are representing anything.

232
00:28:20,480 --> 00:28:29,940
Well, yeah. So. So I think that the presentation I think is more useful to give the presentation about your data the first time first.

233
00:28:30,200 --> 00:28:37,310
So the midterm, I don't know the midterm project or first project is what we should be presenting or not the final project.

234
00:28:37,370 --> 00:28:42,620
Right. And is that different than the data capture the midterm project?

235
00:28:44,600 --> 00:28:50,570
Yeah. So you will capture a different type of data. Data. Right. So I don't know which group you are behind to capture data.

236
00:28:50,810 --> 00:28:53,960
So after you get data, you could draw some preliminary results.

237
00:28:54,080 --> 00:28:56,660
Right. Okay. Because I think I'm just confused.

238
00:28:56,780 --> 00:29:04,550
And there's a lot of things in the syllabus listed and I don't know which of them are which which is which.

239
00:29:04,580 --> 00:29:05,370
What do you mean by that?

240
00:29:05,400 --> 00:29:18,350
Well, because there's a there's a midterm, there's a data capture assignment, and there's midterm project and there's a research paper presentation.

241
00:29:18,530 --> 00:29:22,520
Then there's a final project with presentation listed in the room.

242
00:29:23,250 --> 00:29:30,680
Okay. Okay. I don't know what else to do to worry about that.

243
00:29:32,420 --> 00:29:35,420
So. Yeah. So, yes, that.

244
00:29:36,080 --> 00:29:45,120
And then, you know. Okay.

245
00:29:45,420 --> 00:29:49,470
So, research paper presentation. Okay. That's the one.

246
00:29:58,540 --> 00:30:06,280
We I mean so we have a group project and this is mostly about the the.

247
00:30:17,260 --> 00:30:22,930
So the Blue Project and the final project, the final budget is the continuation of the project authorities are referring to.

248
00:30:22,990 --> 00:30:27,550
I think they should be continued the continuation of the group project.

249
00:30:28,030 --> 00:30:34,630
I'm just trying to simplify a little bit some of this items because maybe it's too much to work for you guys.

250
00:30:34,720 --> 00:30:37,780
I saw your recently paper for you to give her annotation.

251
00:30:37,880 --> 00:30:43,060
Well, I would if you could make a final i.

252
00:30:43,540 --> 00:30:50,410
Yeah, I. I certainly feel that you spent a quite a bit time working on the homework too, because you need to do a lot of data combing.

253
00:30:51,040 --> 00:31:01,330
And so I feel that, you know, we need to do a little bit more data capturing work to do the final project.

254
00:31:01,930 --> 00:31:07,120
Otherwise we will recycle the same data set, which is a little bit boring to me.

255
00:31:07,600 --> 00:31:13,060
So we need to do a little bit more because you have only outlined some of the scientific

256
00:31:13,360 --> 00:31:17,590
goals that you want to achieve for this course that suddenly we need more data.

257
00:31:17,800 --> 00:31:24,310
So I want just combine this data capture with the you have done data capture for homework too.

258
00:31:24,640 --> 00:31:28,480
But we need to do a little bit more on that to be combined.

259
00:31:28,480 --> 00:31:30,850
This a group project that's currently empty.

260
00:31:31,690 --> 00:31:42,460
So for the presentation research paper presentation and you have done the data presentation for that data we use for midterm,

261
00:31:42,880 --> 00:31:48,100
we could have a data presentation.

262
00:31:48,400 --> 00:31:53,610
Right. So just go to one class for data presentation after you've finished your group project,

263
00:31:53,640 --> 00:31:58,360
talk about your data before people move on to the final project that people share,

264
00:31:58,360 --> 00:32:08,530
that what data I have captured and what I see and how I think that this data may be relevant as a variable to to be put in the,

265
00:32:09,230 --> 00:32:12,300
you know, prediction model or the model that that brings.

266
00:32:12,580 --> 00:32:16,480
So my question is, so you are saying something like group Y is like data.

267
00:32:17,050 --> 00:32:20,380
Yeah, I have done that already. I have to do a little bit.

268
00:32:20,440 --> 00:32:26,679
Oh yeah. I already have this Google sheet where I sign groups and of all the data,

269
00:32:26,680 --> 00:32:31,240
I have to organize that little bit more to talk about it specifically on Thursday.

270
00:32:31,390 --> 00:32:38,830
So we captured those data. Then you will use basically group voice goes directly to the group project and to find out what did that is done.

271
00:32:39,220 --> 00:32:50,620
So I want to capture data and then to, for example, make some of the plot of your mortality and your Google this whatever like for example,

272
00:32:50,830 --> 00:32:54,850
political statement, segregation data and see how that looks like.

273
00:32:55,540 --> 00:33:00,630
And you may use now we only work on the Michigan data.

274
00:33:01,000 --> 00:33:04,090
Do you want to work on the national data?

275
00:33:04,210 --> 00:33:10,120
Right. So so we want to expand that to to national data.

276
00:33:10,570 --> 00:33:17,290
That's something we can talk about, about your scope of your presentation.

277
00:33:18,840 --> 00:33:26,730
So is the final project individual or it says individual or groups, or is it the same groups as the group project?

278
00:33:26,970 --> 00:33:31,049
So I think probably we can go for group project.

279
00:33:31,050 --> 00:33:37,580
If you want to stick out the same group you can, you know, you just stick out the same group, right?

280
00:33:38,550 --> 00:33:43,220
And that time out, I was not so sure what your ambition would be.

281
00:33:43,230 --> 00:33:49,950
You do want to do what? Individual project want to project. So final project, the presentation.

282
00:33:51,030 --> 00:34:02,250
Yeah. I don't know if we'll have time. If we have time, we can make a poster presentation, invite your friends over and yeah.

283
00:34:02,490 --> 00:34:13,570
To, to present your results. But if not then we can put this, find a term presentation part of your final project.

284
00:34:15,300 --> 00:34:21,540
And so for the presentation or project in general, would we,

285
00:34:21,540 --> 00:34:31,320
would there be some component where we would need to be here during finals week or would everything we do before.

286
00:34:32,340 --> 00:34:38,340
So, so that because we do not have final exam, we don't have very fixed date for that.

287
00:34:38,340 --> 00:34:43,770
I, I'd certainly like to see I don't know you are going to follow your findings

288
00:34:43,770 --> 00:34:49,620
and that certainly this final project is in terms of due date is flexible.

289
00:34:50,160 --> 00:34:53,890
I mean, depends on when you are able to finish it up.

290
00:34:53,910 --> 00:35:05,379
Right. So. I mean, some of the you are sort of they are going to have some of the fine exams.

291
00:35:05,380 --> 00:35:10,150
All right. So concentrate on the exams. Right. So, yeah.

292
00:35:14,210 --> 00:35:18,800
So just to summarize, so we are going to do is first of all,

293
00:35:18,830 --> 00:35:28,220
to do the capture by our group and then probably do some spark three analysis for us and give a presentation.

294
00:35:28,490 --> 00:35:38,060
Yeah. And then we're just to go over some detailed analysis for the final product and we will submit the report.

295
00:35:38,330 --> 00:35:42,610
Yeah. Okay. So the presentation always.

296
00:35:42,620 --> 00:35:48,930
Yeah, I don't know, like, you know, where is feasible to have a project presentation and,

297
00:35:49,200 --> 00:35:54,979
and I don't know your how some you are able to finish up the final project okay.

298
00:35:54,980 --> 00:35:59,360
So you know, so I can put this five person into the final term project.

299
00:36:00,740 --> 00:36:06,380
So we decided to go for a group project or you can stick out the this end group.

300
00:36:07,100 --> 00:36:14,180
Do you do it for the group project. That's reasonable you think is doable.

301
00:36:14,840 --> 00:36:19,280
Yeah. So the, the group project is to some late this month.

302
00:36:19,520 --> 00:36:27,200
Right. This is just the first day of this month or so. So we still have a couple of homeworks to worry about.

303
00:36:27,330 --> 00:36:29,490
Right. So in this month. Right.

304
00:36:30,920 --> 00:36:38,899
But in the meantime, on Thursday, after assigning the specified tasks of data capture and then you can start to work on that,

305
00:36:38,900 --> 00:36:46,340
you can allocate your workload based in that group and see how you're going to more efficient in the way to finish out that data capture.

306
00:36:49,850 --> 00:36:53,050
Sounds good. Reasonable. Okay.

307
00:37:00,280 --> 00:37:10,120
Okay. Let me back to the not sure to the.

308
00:37:17,060 --> 00:37:26,630
So I've been talking about the estimation of the parameters in the mode compartment modeling of setting,

309
00:37:27,470 --> 00:37:35,959
and so mostly coming from like the frequentist method and sort of the cost of the likelihood

310
00:37:35,960 --> 00:37:41,990
method based on the composite like here and so on and or estimating functions and so on.

311
00:37:43,100 --> 00:37:52,430
But there is a very powerful statistic method to estimate the parameters in a motor compartment model.

312
00:37:53,180 --> 00:38:03,470
This is the one that we did a couple of years ago to essentially train a motor confirmed model to make a prediction.

313
00:38:04,340 --> 00:38:08,690
And I think this is quite a useful sort of approach.

314
00:38:09,440 --> 00:38:17,600
Mark Campbell Carl is basically trying to use computer power to simulate data from a

315
00:38:17,610 --> 00:38:27,530
underlying model so that you can generate the distribution for parameters given your data.

316
00:38:28,490 --> 00:38:37,430
So this idea is very powerful and it's widely used in the study where you are easily generate data.

317
00:38:37,430 --> 00:38:46,339
So you deep learning, there's a generative modeling, generative machine machine of the machinery.

318
00:38:46,340 --> 00:38:51,889
And basically the the philosophy is very simple.

319
00:38:51,890 --> 00:39:00,650
You want to use powerful computing power to generate data and then use simulated data to learn the underlying

320
00:39:01,160 --> 00:39:11,870
sort of mechanism of your data structure where you some build up some learning processes so won't mar.

321
00:39:12,410 --> 00:39:20,870
Mark Colvin And Samsara is kind of that idea particularly useful in the case of the hierarchical model we look at,

322
00:39:20,930 --> 00:39:26,390
okay, we are now changing our model or just changing the way of estimating parameters.

323
00:39:28,370 --> 00:39:32,210
So I will give a quick review about my same method.

324
00:39:32,960 --> 00:39:39,500
I don't have time to cover and same see this mainly the entire course to cover.

325
00:39:39,500 --> 00:39:44,569
Mm. Same, same method which is quite a essential sort of method.

326
00:39:44,570 --> 00:39:57,500
As I said that MSNBC is one of the biggest some of the significant advances in statistics in PATH for decades.

327
00:39:58,010 --> 00:40:03,319
So it deserves an entire course to cover if you want to out a lot of details.

328
00:40:03,320 --> 00:40:05,480
But I want to quickly walk you over.

329
00:40:06,200 --> 00:40:14,480
And now we do have several are sort of packages that you can run this method directly with minimal understanding of this method.

330
00:40:14,930 --> 00:40:25,910
So I use our jacks and this is a advanced version from the on the previous CMC sort of

331
00:40:26,180 --> 00:40:33,440
program and it becomes a lot of simpler and more powerful to do same scene estimation.

332
00:40:33,920 --> 00:40:39,650
I cover a but deep sampling because that's the basically the first version of ways of

333
00:40:40,310 --> 00:40:48,889
simulating data from high dimension of distribution cover the convergence diagnosis,

334
00:40:48,890 --> 00:40:58,520
which is quite a important thing to do in the same C method and touch base, a little bit more chocolate hosting algorithm,

335
00:40:58,940 --> 00:41:07,130
which is the place where you are now able to use skip sampling that you can do the beat up algorithm.

336
00:41:08,330 --> 00:41:17,430
And finally talk a little bit to the of the icy criterion to do model selection this deviance information criterion.

337
00:41:18,210 --> 00:41:23,900
I just like this Bayesian version in the context of AMC to do model selection,

338
00:41:24,500 --> 00:41:29,690
something like your ACL back in the front in this context for model selection.

339
00:41:30,200 --> 00:41:40,879
Now I talked about how this we we rather our package based on the our Ajax to estimate

340
00:41:40,880 --> 00:41:47,750
the mode compare model especially extend the SDR model to make a prediction.

341
00:41:47,930 --> 00:41:54,680
Okay. So that's something I want to cover. After I finished this topic, we'll move on to spatial data analysis.

342
00:41:58,030 --> 00:42:01,680
So simulation based approach becomes very popular.

343
00:42:01,690 --> 00:42:03,880
I mean, particularly in the, you know,

344
00:42:03,890 --> 00:42:17,050
the situation of a complex data analysis where it's hard to specify parametric distributions when when your underlying problem is very complex,

345
00:42:17,350 --> 00:42:27,870
it's hard to use a simple, pragmatic distribution to, you know, specify it's sort of random law or some kind of uncertainty quantification.

346
00:42:28,570 --> 00:42:35,920
Lot of time people just want to learn the knowledge, generate new knowledge from data is solved by simulation.

347
00:42:36,160 --> 00:42:49,450
Okay, so simulation based approach is become as becomes more and more popular nowadays and can be easily formulated in the Bayesian sort of paradigm.

348
00:42:50,080 --> 00:42:55,450
Because if you look at this philosophically for the basic method,

349
00:42:55,480 --> 00:43:02,020
it's really how you're going to integrate the current data versus some of the existing knowledge.

350
00:43:02,020 --> 00:43:11,080
So this integration, it's quite essential to produce the oh, so your knowledge.

351
00:43:11,290 --> 00:43:17,730
Okay, so the machine that I haven't seen method allows you to do that is quite a popular, powerful tool.

352
00:43:17,740 --> 00:43:21,160
This high dimension of numerical problems is to the inference,

353
00:43:22,630 --> 00:43:29,590
including evaluation of how the machine integrals require in the complex likelihood function.

354
00:43:31,910 --> 00:43:40,069
So, so same same method is established in the base inference paradigm where all

355
00:43:40,070 --> 00:43:44,240
the model parameters assumed to be random variable so that you can simulate.

356
00:43:44,570 --> 00:43:54,320
Right. So, so you, if you have, if you want to do simulation, including simulating random parameters,

357
00:43:54,560 --> 00:43:57,590
you have to make trend parameters to the random variable.

358
00:43:58,010 --> 00:44:11,540
We are willing to do that. You basically start to thinking basically because basically the basic statistic, all the parameters are random variables.

359
00:44:11,600 --> 00:44:16,130
Right. So we some prior distribution of to do so.

360
00:44:16,940 --> 00:44:22,950
So you need to specify some prior distribution for parameters representing come existing

361
00:44:22,970 --> 00:44:30,410
knowledge of all your parameters so that you can work out the posterior density of parameters.

362
00:44:30,410 --> 00:44:35,629
And posterior densities are essentially conditional distribution of your primary,

363
00:44:35,630 --> 00:44:43,940
giving your data and this post der distribution will be used to make inference or estimation inference.

364
00:44:44,120 --> 00:44:55,070
Okay. So here in the following presentation for the review of CMC method algorithm, the beta is a generic notation for set of all model parameters.

365
00:44:56,060 --> 00:44:59,690
In our case, we have this horrible hierarchical model result.

366
00:44:59,960 --> 00:45:06,320
We still have this, you know, the hierarchical model y t okay, see that?

367
00:45:06,350 --> 00:45:13,250
T And then you have this see that he was one that this famous constructor.

368
00:45:13,370 --> 00:45:19,640
Right. So, so I still have this kind of hierarchical.

369
00:45:20,960 --> 00:45:34,430
So here under underlying process there we have this mode comp hard model to describe the underlying fascist dynamics of evolution in the population.

370
00:45:34,430 --> 00:45:44,600
And then you have the surveillance data, data capture to, to, you know, analyze the underlying in fascist evolution.

371
00:45:44,710 --> 00:45:53,900
Okay. So that's the hierarchical model when I'm talking about particularly here where they say that he will follow a multi compartment model.

372
00:45:54,420 --> 00:46:07,500
Okay. So for the technical convenience, they lead in process, which is typically tedious, and recent data can also be part of the parameters.

373
00:46:08,220 --> 00:46:12,150
Okay, so let's talk about a base this base model.

374
00:46:12,490 --> 00:46:16,710
Okay. So, so y t I have a hierarchy model.

375
00:46:16,770 --> 00:46:23,580
Okay. So why t is the number of, you know, the confirmed cases?

376
00:46:23,610 --> 00:46:31,440
This could be a vector, right? Number of confirmed cases. The number of the recoveries and number of the, you know, deaths.

377
00:46:31,540 --> 00:46:34,410
Right. So so whichever the data, you can collect it.

378
00:46:34,800 --> 00:46:44,730
See that t will be a latent process that follows a most compound model could be SARS, R or other type of mode.

379
00:46:44,730 --> 00:46:51,870
Conform all of that to values of the underlying evolution of your infectious disease.

380
00:46:52,080 --> 00:46:56,100
Okay. So is also a vector and follows sort of distribution.

381
00:46:56,400 --> 00:47:01,320
In our case, we have the possible distribution or beta distribution.

382
00:47:01,320 --> 00:47:09,660
If you work on the Y t as a proportion, right. Working on the Y as the proportion rather than counts directly.

383
00:47:09,750 --> 00:47:15,600
Okay. So that will give you some kind of a distribution you specified.

384
00:47:15,600 --> 00:47:20,880
And then you have this Markov process for this mode computer model.

385
00:47:21,480 --> 00:47:27,660
And then you have another set of parameter B to, to, to, you know, describe,

386
00:47:27,990 --> 00:47:36,960
to characterize the underlying transmission from past to the current of this evolution of the infectious disease.

387
00:47:37,470 --> 00:47:42,780
And then you have the prior distribution beta follows some prior distribution,

388
00:47:42,780 --> 00:47:51,000
and you have this sort of orthogonal prior distributions with independence of the parameters.

389
00:47:51,120 --> 00:48:03,179
Just for simplicity, the parameters can be correlated, can be related, but for the simplicity we just assumed that is, you know, under the parameter,

390
00:48:03,180 --> 00:48:13,759
the prior distribution are specified that for individual parameters and under the independence of those parameter distributions.

391
00:48:13,760 --> 00:48:19,079
So we can write the joint distribution as the product of marginal distribution.

392
00:48:19,080 --> 00:48:25,980
That's the typical way people specify the prior distribution for multidimensional parameters.

393
00:48:26,070 --> 00:48:26,310
Right.

394
00:48:28,200 --> 00:48:37,050
So now if you want to work on the posterior distribution, basically it's a conditional distribution of the all model parameters given your data.

395
00:48:37,080 --> 00:48:42,930
Right. Then this will be proportional to the joint distribution of your beta and the Y,

396
00:48:43,860 --> 00:48:51,510
because this conditional distribution, the posterior we call posterior right s.

397
00:48:53,740 --> 00:49:00,820
So this is something you want to learn. Right. So given the data, what is the distribution of my parameter?

398
00:49:00,850 --> 00:49:01,899
And this, of course,

399
00:49:01,900 --> 00:49:11,500
can be reached in a joint distribution of this over the margin distribution of Y for marginal distribution Y doesn't depend on beta.

400
00:49:12,190 --> 00:49:20,350
So is is a constant with respect to the parameter beta so that you can easily ignore this normalizing constant.

401
00:49:21,370 --> 00:49:30,610
Okay. I just use this spatial notation to denote that you are subject to a normalized and constant that's independent of beta.

402
00:49:31,090 --> 00:49:37,630
This conditional distribution we call posterior posterior of theta is, as you know,

403
00:49:38,710 --> 00:49:45,100
you call to this joint density subject to a constant that is independent of your parameter.

404
00:49:45,760 --> 00:49:56,230
Okay? Now in that is for this joint distribution you can folder, right, of using this comp structure.

405
00:49:56,260 --> 00:50:08,079
Right? So what you're trying to do here is you, you have this beta of one, right equal to read out y you add all the theta.

406
00:50:08,080 --> 00:50:13,420
Of course theta is later process. That's one of these, you know,

407
00:50:13,420 --> 00:50:22,120
observe that and just integrate alpha state in the press so the marginal distribution can be written as this augmentation.

408
00:50:22,840 --> 00:50:31,990
So the outcome in your distribution by is is inserting this theta process, but you interpret of that are equal mathematically.

409
00:50:32,500 --> 00:50:40,629
So after you you you do that, you can work out this joint distribution using this comp structure, right?

410
00:50:40,630 --> 00:50:52,030
That can be easily written as this so that your observed data could be either accounts or be proportion of certain

411
00:50:52,030 --> 00:51:00,459
compartment given the current status of compartment will follow is or possible distribution negative binomial distribution.

412
00:51:00,460 --> 00:51:08,500
If you think a whites can't or you u this will follow a beta distribution if if wife is

413
00:51:08,650 --> 00:51:14,590
proportional or simplex distribution depends on how you want to formulate your problem.

414
00:51:14,590 --> 00:51:22,090
You are a package esr. We use proportions like the incidence rate.

415
00:51:22,630 --> 00:51:29,680
Okay, so so now you you can write this joint distribution conditional theta and then you have the

416
00:51:29,680 --> 00:51:35,590
distribution of theta inside the integral and you have the Markov process to write this.

417
00:51:36,370 --> 00:51:45,190
So theta one is the parameter socialist, the E meeting probability, the T from the related process to observe the process.

418
00:51:45,640 --> 00:51:54,350
And this beta two is the parameter in this lethal process, like the transmission rate, recover rate.

419
00:51:54,490 --> 00:51:58,299
Okay. So, you know, some of you work on other parameters.

420
00:51:58,300 --> 00:52:06,850
So at Delta there's incubation rate. So those beta two are the parameter related to multicam palm model describing what's

421
00:52:06,850 --> 00:52:13,510
going on in that latent process and times actually the the pull of the part of density.

422
00:52:13,660 --> 00:52:26,440
Okay so here denominator in constant namely the margin distribution of F of Y is the independent parameter theta and bleeding processes so that,

423
00:52:26,920 --> 00:52:36,730
you know, the estimate of theta such as poster mode or poster being can be derived directly from there, the joint distribution.

424
00:52:36,880 --> 00:52:49,000
Okay. So here, after you figure out this posterior distribution of your parameter, you can either get in the mode of this density,

425
00:52:49,240 --> 00:52:55,030
which is equal to the most probable value of your parameter, giving your data worried.

426
00:52:55,120 --> 00:53:05,890
Take the mean of this distribution, which is called base estimate or the the expectation of the posterior distribution is named,

427
00:53:06,850 --> 00:53:11,200
but base estimate that's the best optimal estimate under error to loss.

428
00:53:11,500 --> 00:53:19,900
So that's something you you know it. So you can either use the mode or use the mean to be the estimate or of your parameter,

429
00:53:20,710 --> 00:53:26,590
of course that this distribution allows you to construct confidence interval not confidence is a credible interval.

430
00:53:27,070 --> 00:53:38,080
So in Bayesian statistic, we name this sort of interval estimation as credible interval because there has no large sample interpretation.

431
00:53:38,590 --> 00:53:41,800
Everything is based on the data you have. Okay.

432
00:53:42,220 --> 00:53:47,980
So, so you can say that this poster distribution is a distribution condition of what you observe.

433
00:53:48,370 --> 00:53:52,240
There is no argument would be sample size scale.

434
00:53:52,390 --> 00:53:59,500
Infinity. What's going to happen is everything is based on inference of the this finite sample collected.

435
00:54:00,910 --> 00:54:06,700
But this posterior distribution can be updated if you have more data point come in.

436
00:54:06,950 --> 00:54:11,180
And that's basically something related to online learning.

437
00:54:11,200 --> 00:54:19,840
But anyway, so, so after you figure out the posterior distribution, you calculate the demand of this or the meaning of this to get your estimate.

438
00:54:20,560 --> 00:54:30,430
Okay. And here the the the prior distribution also involving part of this joint distribution.

439
00:54:30,940 --> 00:54:35,230
And then you need to really deal with it in practice.

440
00:54:35,890 --> 00:54:39,700
If the prior for the beta here is typo.

441
00:54:40,060 --> 00:54:48,160
Okay. Is constant. Here is the beta is a constant that we call this flat or non-uniform priors.

442
00:54:48,550 --> 00:54:53,710
The posture above is a factor for the proportional to likely the function.

443
00:54:53,740 --> 00:55:00,310
Okay. So if this pi beta becomes a just a constant.

444
00:55:00,550 --> 00:55:06,760
Right, it does not really contribute anything to chance this integral.

445
00:55:07,840 --> 00:55:13,600
Then if there's a constant or non-uniform to a flat, flat fire or non-uniform prior,

446
00:55:14,350 --> 00:55:20,980
then this whole thing, this integral piece of this integral piece is equivalent to likelihood function.

447
00:55:21,610 --> 00:55:28,450
So basically the mode of postural in this case is equivalent to your MLC.

448
00:55:28,840 --> 00:55:36,580
Okay. So that's the connection between your base estimation and maximum likely estimation.

449
00:55:36,970 --> 00:55:45,010
If the pie beta this power distribution becomes a constant doesn't depend on your parameter,

450
00:55:45,010 --> 00:55:52,300
then this integral piece is exactly seen numerically as your likely to function.

451
00:55:52,630 --> 00:56:00,180
So maximizing this piece is equivalent to find the posture mode of of this.

452
00:56:00,190 --> 00:56:05,560
And so that numerically I have to emphasize numerically.

453
00:56:05,710 --> 00:56:13,030
Okay. So from Clinton, this inference from Fisher and Bayesian inference from, you know,

454
00:56:13,130 --> 00:56:19,900
the basic statistics fundamentally are in a different school of thinking.

455
00:56:20,470 --> 00:56:30,620
But numerically, I'm numerically they can't have the identical solution, but they have a different ways to process the data.

456
00:56:31,150 --> 00:56:35,260
You know, to to come up with different object function in the optimization.

457
00:56:35,500 --> 00:56:39,190
Okay. But numerically, they can reach the same solution.

458
00:56:39,700 --> 00:56:49,149
Okay. So for this step left prior so now you from program for beta and are usually preferred in practice.

459
00:56:49,150 --> 00:56:55,110
And so of course in practice sometimes we will use context parameters as well.

460
00:56:55,120 --> 00:57:05,890
But you know, so this is for the sake of interpretation and and people just want to use knowing form prior.

461
00:57:06,490 --> 00:57:13,890
If you don't have very good firsthand information about your parameter that you,

462
00:57:14,230 --> 00:57:28,880
you just take a uniform distribution or some constant like, oh Jeffery prior to start is your sort of Bayesian method okay in most cases.

463
00:57:28,900 --> 00:57:33,820
Normal distribution is assumed as prior for each of us.

464
00:57:34,060 --> 00:57:42,220
You know, the association parameters like parameter with some hyper parameters.

465
00:57:42,250 --> 00:57:46,690
So when you specify a prior distribution, for example.

466
00:57:47,290 --> 00:57:53,830
So if you have our fourth one and this is your vaccination, right?

467
00:57:54,610 --> 00:58:03,820
So, so this is like in your specification, your life in either model over whatever picture and model you want to study the association,

468
00:58:04,210 --> 00:58:09,550
how the vaccination, the cumulative vaccination affect your mortality rate.

469
00:58:09,910 --> 00:58:19,840
You have a corp. So so in the Bayesian context, you need to specify this association parameter and prior distribution.

470
00:58:19,840 --> 00:58:33,340
Then you say, okay, this follows normal distribution, this means zero and the number variance one or something else, you know, maybe seek a mask.

471
00:58:34,810 --> 00:58:39,030
Let's see how square for now.

472
00:58:39,880 --> 00:58:48,640
So we you when you specify the the mean and what the variance prime for for this parameter.

473
00:58:48,650 --> 00:58:52,050
So you have parameters used in this.

474
00:58:52,270 --> 00:59:03,610
Supply and distribution of printer. So those printers I call hyper printers, they are not estimable, they're just something to pre-specified.

475
00:59:04,210 --> 00:59:10,420
Well, your question here is, how do I know that if I choose the right fry the hypo primers?

476
00:59:10,690 --> 00:59:14,020
Well, that's the open probably Bayesian statistics, right?

477
00:59:14,590 --> 00:59:21,070
So if you want to use the metal, you have to attack. You have to agree with this specification.

478
00:59:21,490 --> 00:59:33,850
So you have hyper parameters. Those are the parameters in the prior distribution for the parameters that you'd like to to work on in the base,

479
00:59:33,850 --> 00:59:41,110
in content being set as some values according to some existing empirical knowledge you learn from a pilot study.

480
00:59:41,620 --> 00:59:43,370
What are you if you don't have a host of anyway?

481
00:59:43,390 --> 00:59:51,430
Suppose you have something you you guess some values or reasonable values or similar in facilities occurred in the past.

482
00:59:51,550 --> 00:59:59,560
So you fashion disease situation and maybe we have this privilege that that we could learn from some of the similar

483
00:59:59,560 --> 01:00:08,190
diseases in the history and then we can come up with some reasonable hyper prime pairs in our in this specification.

484
01:00:08,200 --> 01:00:15,700
That's what we did for this ESR package where we use the code corral.

485
01:00:15,730 --> 01:00:23,350
There's a source of errors there that you can come to actually specify the deprived distribution,

486
01:00:23,470 --> 01:00:29,490
the hyper parameters in the prior to prior distribution for the parameter indic mode compartment model.

487
01:00:29,500 --> 01:00:38,110
So we use some historical infectious data with similar disease type of various type virus type that that so so

488
01:00:38,110 --> 01:00:49,690
we could or some people say that if I want to you know study the of the infectious disease full year 2022.

489
01:00:50,020 --> 01:01:00,640
I can use the data from year 2021 to learn something and use that kind of data to estimate my parameter so that it can,

490
01:01:01,180 --> 01:01:07,110
you know, come up with some informative, reasonable prior distributions for current to you.

491
01:01:07,130 --> 01:01:15,790
So there are some ways that you can find a useful, relevant data to learn those hyper parameters in the specification.

492
01:01:15,790 --> 01:01:29,229
But when you do not have these good resources, historical data or some kind of existing relevant data for you to learn high parameters,

493
01:01:29,230 --> 01:01:33,700
then you could consider use of flat prior or diffuse prior.

494
01:01:33,820 --> 01:01:43,150
Okay. So that's the of the the thing we did in the implementation of this SBIR after

495
01:01:43,330 --> 01:01:49,420
algorithm in February 2020 when very little data is available for COVID 19 pandemic.

496
01:01:50,020 --> 01:01:56,890
So in our paper, right, so this is one of the earliest the paper to implement a multicam PA model.

497
01:01:57,670 --> 01:02:07,530
And so from that we used the data collected from this similar cloud, various SaaS from Hong Kong surveillance program.

498
01:02:07,570 --> 01:02:15,760
So, so that's what we did to learn something about the high parameter to specify prior distribution.

499
01:02:17,740 --> 01:02:29,800
Okay. So in a study of COVID 19 pandemic in 2022, one may use the data from year 2020, year 2022, year 21 to generate relevant hyper trackers.

500
01:02:29,860 --> 01:02:35,260
But those hyper primers must be specified in advance.

501
01:02:35,860 --> 01:02:38,320
In some way, you have to find something useful.

502
01:02:39,220 --> 01:02:49,120
In the case of no existing relevant data, a uniform distribution is used to specify the prior distribution, which is no unknown informative prior.

503
01:02:50,020 --> 01:02:58,419
So you don't know when you have normal distribution, you'll basically build up something, right?

504
01:02:58,420 --> 01:03:01,900
So so this is a normal distribution.

505
01:03:02,620 --> 01:03:05,139
If you have this as a prior distribution,

506
01:03:05,140 --> 01:03:16,360
basically you believe that the new data are more likely to be the case than the two values on the on the other tell the distribution.

507
01:03:16,600 --> 01:03:26,230
Right. So, so you impose this belief as part of your prior information in the same serum and basic method.

508
01:03:26,800 --> 01:03:36,570
But if you do not believe that any of that is would be more and more likely to to occur than other values, you can use this uniform distribution.

509
01:03:36,580 --> 01:03:44,370
Yes. So I do need to use a truncated name because I will talk about diffuse prior next.

510
01:03:44,680 --> 01:03:51,990
Yeah, yeah. Yes, you in normal is otherwise you don't get density.

511
01:03:52,000 --> 01:03:55,240
Right. Interesting. So let me just.

512
01:03:57,650 --> 01:04:08,660
Okay. Maybe I just answer the question right now. If you want to use uniform that the if you know the range of the parameter, that's no problem.

513
01:04:09,170 --> 01:04:21,499
But if you you don't know the range of parameter, then you cannot put the uniform on the entire space for a -22,000.

514
01:04:21,500 --> 01:04:28,550
Infinity is not possible. So you can all have uniform distributed, defined from minus component.

515
01:04:29,360 --> 01:04:38,120
So what you should do here is that we still use the normal and put a very small or large events or small percentage.

516
01:04:38,270 --> 01:04:47,410
Right. So if you can think that if I increase the appearance of the normal distribution, this becomes very flat rate.

517
01:04:48,150 --> 01:04:56,870
So this Sigma Square goes to infinity, then the variance becomes larger and more and more spread out.

518
01:04:57,440 --> 01:05:04,340
And this is still well defined, but it becomes less favorable to the point underneath.

519
01:05:04,370 --> 01:05:14,540
So this called diffuse is imperfect. Just like if I don't want to touch that improper prior.

520
01:05:14,570 --> 01:05:20,210
Yes, you can use that. Sorry. I guess my question is because alpha and this is related to alpha.

521
01:05:20,600 --> 01:05:25,610
Okay. The positive rate is I think why? Why, why has to be positive.

522
01:05:25,630 --> 01:05:30,090
Why given the positive distribution and this parameter.

523
01:05:30,110 --> 01:05:36,260
So the parameter is not this sort of positive because you use exponential transformation.

524
01:05:37,060 --> 01:05:42,790
Oh, right. So you have half of the vaccination. I don't need that.

525
01:05:43,060 --> 01:05:47,600
That's the beauty of the function here. So why do you need to do a function?

526
01:05:47,990 --> 01:05:54,920
People say, Oh, well, I don't know. So people, you start 36, 51.

527
01:05:54,930 --> 01:05:59,300
The instructor or the professor told me that you the function well in function.

528
01:05:59,580 --> 01:06:06,400
Exactly serve the purpose. You just said you basically want to relax the constraint.

529
01:06:07,940 --> 01:06:13,610
But if you go have exponential, this has to be somewhat constrained so that the mean becomes positive.

530
01:06:13,610 --> 01:06:16,940
Right. So. So think function is.

531
01:06:17,300 --> 01:06:23,840
So for this understanding why you need to use log now use the other functions or you can use order from functions.

532
01:06:23,840 --> 01:06:29,020
Why use lock? Because lock link function is chronically right.

533
01:06:29,540 --> 01:06:38,270
You can make your estimation procedure simpler. It doesn't mean this is the only choice and addicts mark the most.

534
01:06:38,330 --> 01:06:43,160
Okay, Mark. Right. So it's a German statistician who said that.

535
01:06:43,230 --> 01:06:47,440
Why I need to, you know, use just one function.

536
01:06:47,450 --> 01:06:52,160
Can I do optimization for a set of doing functions?

537
01:06:52,400 --> 01:06:56,870
Okay. They are Alex Monk from Gottingen, Germany.

538
01:06:57,320 --> 01:07:02,590
He had this work about 30 years ago. And see, I have a possible of functions.

539
01:07:02,620 --> 01:07:06,109
Let's pick up one that is most supported by data.

540
01:07:06,110 --> 01:07:09,440
But I've never done popular in so.

541
01:07:09,460 --> 01:07:13,220
So theoretically you can analyze talk about that.

542
01:07:13,220 --> 01:07:19,730
But in people just from prime to point of view, people pick up link function for convenience, right?

543
01:07:20,690 --> 01:07:25,669
Or if there are software available, people just use whatever available in their software.

544
01:07:25,670 --> 01:07:35,030
So creating a solver gave you a chance to dominate in a few because people will say, Hey, I just go for whatever is available in solver.

545
01:07:35,090 --> 01:07:38,450
I know your theory is beautiful, but you don't have software.

546
01:07:38,780 --> 01:07:40,370
I don't. I'm not going to use it.

547
01:07:40,940 --> 01:07:48,559
So in this case, that use lock or lock ordering function will be fine as long as the server for the purpose of relax,

548
01:07:48,560 --> 01:07:55,700
there's constraints on the parameters and but which way ever gives you the, you know,

549
01:07:55,710 --> 01:08:02,390
be solved or it will be eventually of the winning situation for people to adopt.

550
01:08:02,630 --> 01:08:12,800
Okay. So typically we space of a large variance or small precision parameter specific precision parameters is reciprocal of variance parameter.

551
01:08:12,800 --> 01:08:18,920
Right. You'll know this. Okay. So, so to the power of ten minus four, a massive five.

552
01:08:18,920 --> 01:08:22,520
Why you do this? That's by convention.

553
01:08:22,550 --> 01:08:26,570
There is no argument. Why not? I choose 10 to -8.

554
01:08:26,900 --> 01:08:32,910
Well, you can do that. But so typically, I mean, you practice, people use 10 to -4, ten them as well.

555
01:08:32,930 --> 01:08:36,980
There is no theory to say that this is the right or wrong.

556
01:08:37,010 --> 01:08:42,139
It's just empirical experience or a convention.

557
01:08:42,140 --> 01:08:52,610
People do it. Okay. So the result in prior, as I do, all there is kind of diffused in sense that is less concentrated around the prior mean parameter.

558
01:08:53,210 --> 01:08:57,170
So this called diffuse power, which is close to. Oh, now you've.

559
01:08:57,250 --> 01:09:02,700
Alternative prior is not exactly uniform distribution, but is close to that.

560
01:09:02,710 --> 01:09:08,620
So is the calls to them for prayer. So you sum up our approach.

561
01:09:08,650 --> 01:09:16,300
In fact, this is a normal prior view, some large appearance. So that prior tense is probably flat the a reasonable range of parameter space.

562
01:09:16,900 --> 01:09:22,870
This is typical way to specify approximately frac prior only unbounded parameter space.

563
01:09:22,870 --> 01:09:26,799
So it's usually called diffuse prior. Okay.

564
01:09:26,800 --> 01:09:33,730
That's the for the RFA. So if you want to work on some distributions like in our case,

565
01:09:34,600 --> 01:09:39,340
if you want to work at negative binomial server, we need to worry about over dispersion parameter.

566
01:09:40,030 --> 01:09:45,249
If you want to work out, you know, better distribution, you have two parameters, right?

567
01:09:45,250 --> 01:09:52,210
So not only the in the mean but also you have the variance or dispersion parameter and so on.

568
01:09:52,750 --> 01:10:05,770
So the in the M same C people always need to deal with this prior distribution for variance parameter or this dispersion parameter.

569
01:10:06,190 --> 01:10:12,760
What people usually do is use inverse comma distribution as the prior for variance parameter.

570
01:10:12,930 --> 01:10:18,940
Okay, so let's suppose suppose Sigma Square denotes appearance parameter.

571
01:10:19,420 --> 01:10:26,290
And what we should do is set up a universe comma as the prior distribution for this

572
01:10:26,290 --> 01:10:33,400
variance parameter or equivalent to one over Sigma Square follows this comma distribution.

573
01:10:33,640 --> 01:10:40,959
Okay. So Sigma Square follow inverse comma is equivalent to one over second square follows gamma distribution.

574
01:10:40,960 --> 01:10:45,420
So both our phi beta are set as small vowels.

575
01:10:46,050 --> 01:10:51,640
Know that's once again is based on in purple convention.

576
01:10:52,420 --> 01:10:58,149
The choice of inverse comma is due to the fact that universe comma is a conjugate prior meaning that the

577
01:10:58,150 --> 01:11:04,360
posterior of the parameter Sigma Square will have the same distribution as type of the prior distribution.

578
01:11:04,750 --> 01:11:13,090
This is due to the analytic sort of convenience because conjugate prior gives you this very

579
01:11:13,090 --> 01:11:22,110
nice property that is this distribution only closed so that if you start this normal again,

580
01:11:22,150 --> 01:11:25,750
normally if you start this inverse combat, inverse comma.

581
01:11:25,750 --> 01:11:34,120
So, so that's something for the convenience. So now what if you have many, many variance parameters?

582
01:11:35,260 --> 01:11:49,059
So, so in the case, for example, in the theta here is theta is a like three dimensional four dimensional vector where if you want you can work

583
01:11:49,060 --> 01:11:57,010
on the Corvus part of this of state of vector process because think about the more command model you have.

584
01:11:57,010 --> 01:12:03,060
S confirm if you have a compartment, we have our compartment.

585
01:12:03,070 --> 01:12:12,370
This is two dimensional vector in this to describe this evolution of infectious disease.

586
01:12:12,790 --> 01:12:24,880
So if you believe those Thetas are of course supposed to be correlated across different compartments, then you need to work on the covariance matrix.

587
01:12:25,450 --> 01:12:34,359
Okay. So covariance matrix for one Sigma Square that you work on the for our distribution based on inverse gamma.

588
01:12:34,360 --> 01:12:41,860
If you work on covariance matrix, you can specify this as a diagonal matrix.

589
01:12:43,150 --> 01:12:52,510
So, so diagonal matrix you need only specify the prior for each diagonal element in by the inverse comma.

590
01:12:52,900 --> 01:13:02,920
Okay. Or if you want to, you know, work on the a general covariance matrix with nonzero off diagonal elements, then you need the we shot distribution.

591
01:13:03,640 --> 01:13:09,970
So this is how people work, how this covariance matrix prior distribution specification.

592
01:13:10,240 --> 01:13:24,760
Okay. So the assumed that the precision matrix, namely the inverse matrix of the covariance matrix follows a we start distribution with R a Q by Q,

593
01:13:24,760 --> 01:13:29,680
a symmetric announcing or a matrix and copper, which is the degrees of freedom of it.

594
01:13:29,680 --> 01:13:34,570
We shot distribution, so we showed distribution is generalization of Chi Square distribution.

595
01:13:34,690 --> 01:13:45,489
Okay, if you do multivariate analysis, then you know that we start distribution plays a very important role in multivariate analysis.

596
01:13:45,490 --> 01:13:54,680
Just like Chi Square distribution plays a central role in the analysis of variance in one dimensional situation.

597
01:13:54,760 --> 01:14:03,239
Okay. So if you want to specify some. Kind of prior we start distribution, like the way I describe in the normal distribution case,

598
01:14:03,240 --> 01:14:10,050
like make it diffuse in some sense, then you can control the hyper parameters are in copper.

599
01:14:10,620 --> 01:14:16,710
You can set the copper small and the matrix to be set up to be diagonal matrix.

600
01:14:17,130 --> 01:14:23,760
Okay. With the diagonal elements equal to the estimate, prior distribution, assume a diagonal matrix.

601
01:14:24,030 --> 01:14:28,769
Okay. So that's something you don't need to necessarily follow this.

602
01:14:28,770 --> 01:14:41,730
But you know, and lot of time what people you should do here is to look at how other people have down for days prior specification.

603
01:14:42,330 --> 01:14:48,390
Sometimes it's is hard to guess what values to use for the high parameters.

604
01:14:48,900 --> 01:15:01,229
So many times when I was a student I always look at the other people have specified those hyper primers and put into my model to see how,

605
01:15:01,230 --> 01:15:08,430
how, how the the result would look like and if those specifications are reasonable.

606
01:15:08,760 --> 01:15:15,420
And then I start to, to my hyper prime for a bit here there and to really, you know,

607
01:15:15,480 --> 01:15:26,480
make some kind of reasonable choices of my hyper parameters, having some references from the, the values specified in the existing literature.

608
01:15:26,820 --> 01:15:32,490
Our previous work will be very convenient way to determine the high parameters.

609
01:15:32,490 --> 01:15:38,910
And because there's no wrong and right is, it's really something you have to find reasonable, hyper prime,

610
01:15:38,910 --> 01:15:47,430
dirty work to present some kind of prior information so that your posterior works in a reasonable way to generate the result.

611
01:15:49,510 --> 01:15:55,150
Okay. The posteriors of latent process condition, namely conditions of speaking of yours,

612
01:15:55,690 --> 01:16:02,170
theta and given theta our primary interest in them, all universities are used.

613
01:16:02,860 --> 01:16:12,580
So in the MSM scene you do not need to actually work on this sort of common filter coming smoother any way.

614
01:16:13,090 --> 01:16:18,880
This can be easily generated from your same C sampling.

615
01:16:19,180 --> 01:16:28,270
Okay, so so that's why people like MSM say because you can put your model and data into a black box and they're wrong,

616
01:16:28,420 --> 01:16:36,100
you know, and the computer will, you know, run up to two or random samples from the distributions.

617
01:16:36,760 --> 01:16:45,270
And if you work on different quintessence stuff like you need to worry about computing speed or you work out come a filter and come a smoother,

618
01:16:45,350 --> 01:16:58,690
have a better, you know, involvement of analytical derivations to see how you're going to implement this latent process estimation of,

619
01:16:59,230 --> 01:17:01,990
well, it really depends on your comfort level.

620
01:17:01,990 --> 01:17:13,270
Some people like me, I, I like to do comment first because I've found us more elegant for me to understand how things is this,

621
01:17:13,270 --> 01:17:23,230
you know, I'm for it and, and I like to know how I'm going to modify things to improve my estimation procedure.

622
01:17:23,590 --> 01:17:28,990
Some people feel that it's too much computer I that people feel that computers are

623
01:17:29,500 --> 01:17:33,970
too much and they don't want to do you can't do it I'm sensing instead that's fine.

624
01:17:34,990 --> 01:17:44,530
And the problem here is I think that, you know, it's just that, you know, as I said,

625
01:17:45,100 --> 01:17:51,700
one of the two regret I had was I did not put the computer smooth as our package.

626
01:17:51,700 --> 01:17:58,270
If if so, that will be different story. At the my time, I didn't realize that having our package is so crucial.

627
01:17:58,900 --> 01:18:00,190
And yeah,

628
01:18:00,310 --> 01:18:13,840
but I think that's something I if I look at everything research respectively now I should have a very powerful ah package also people can use to,

629
01:18:13,960 --> 01:18:17,740
to, to, to do this analysis at this way.

630
01:18:17,860 --> 01:18:18,070
Okay,

631
01:18:20,380 --> 01:18:31,510
so I'm saying say allow us to use simulated data to estimate is a posterior distribution rather than doing some sophisticated analytic derivations.

632
01:18:33,400 --> 01:18:40,750
In particular, the mean mode variance quantile of the positive distribution can be estimated by the simulated data.

633
01:18:40,750 --> 01:18:43,959
Right. So this is very easy to understand.

634
01:18:43,960 --> 01:18:52,140
Right. So. So suppose this is the poster of distribution.

635
01:18:52,440 --> 01:19:02,220
A great idea for my favor. Right. So let's suppose this is a poster distribution like I have.

636
01:19:02,460 --> 01:19:07,800
And what I want here is, of course, to get the meaning of load.

637
01:19:08,790 --> 01:19:16,709
The load here is the different aspect of the estimate that I want, because that would be, you know,

638
01:19:16,710 --> 01:19:25,410
the mode of the posterior distribution, which will be a estimate, right, of the most probable value given the knowledge of I have.

639
01:19:26,130 --> 01:19:30,740
L or you look at expectation of this poster distribution as base estimate, right.

640
01:19:30,780 --> 01:19:42,509
So that under an error two walls. So instead of doing this sort of analytical calculation, you can just simulate this.

641
01:19:42,510 --> 01:19:51,780
I don't know this distribution. If I'm able to find machinery that allows me to simulate from this distribution, I simply would want to get it right.

642
01:19:54,770 --> 01:20:00,740
So if I simulate pencil the data point from this, this movie is able to do that.

643
01:20:01,240 --> 01:20:04,790
No I can use just sample mean to estimate.

644
01:20:04,800 --> 01:20:10,820
I can use the order statistic to figure out the my mode of this distribution.

645
01:20:10,850 --> 01:20:15,150
Right. So I can have a empirical solution or sample estimate.

646
01:20:15,170 --> 01:20:22,310
I don't need to analytic solution directly from this density function to figure out my estimate.

647
01:20:22,460 --> 01:20:33,830
I just use a set of simulated data from this cluster distribution and then calculate my sample being a sample load or sample quantile of.

648
01:20:33,830 --> 01:20:43,130
The question here is are we able to generate the simulated data, always able to generate the data from the positive distribution?

649
01:20:43,910 --> 01:20:47,950
The answer is yes. That's the same thing. Okay.

650
01:20:48,460 --> 01:20:54,310
So this is a very popular idea. You just need to simulate the from the poster distribution.

651
01:20:54,310 --> 01:20:57,310
How are you going to do that? That's basic sampling.

652
01:20:58,030 --> 01:21:05,740
I'm going to talk about Thursday. So you can simulate from a poster.

653
01:21:06,160 --> 01:21:15,790
Now, you have long probably have a complex system like Theta T and all the model parameters are have a high dimensional parameter.

654
01:21:16,810 --> 01:21:21,430
Right. How do you going to simulate data from a dimension space?

655
01:21:21,760 --> 01:21:31,540
Okay. That's what I'm trying to tackle and give you an empirical solution and then implementation software, then you can do this.

656
01:21:34,030 --> 01:21:39,790
So sorry that I have to tell you that because of this job candidate coming for job interview today,

657
01:21:40,480 --> 01:21:46,780
I have to cancel my office hour where I can be out of my office after the seminar.

658
01:21:46,780 --> 01:21:51,600
If you want, have a talk to me about your work.

659
01:21:52,480 --> 01:21:57,880
The seminar finished at 430, so I'll be back tomorrow if you want to help me.

660
01:21:59,230 --> 01:22:13,870
That's it. The question should be possible to kind of give us a calendar of approximate due dates for things like this Thursday just so.

