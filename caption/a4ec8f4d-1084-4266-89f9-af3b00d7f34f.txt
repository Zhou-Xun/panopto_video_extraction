1
00:00:00,360 --> 00:00:07,140
Part of this is just recognizing that not all variables are or are going to be the same.

2
00:00:07,150 --> 00:00:11,910
Right. They're all going to follow some sort of sampling distribution, which we talked about earlier on this semester.

3
00:00:12,210 --> 00:00:16,350
And so it's just recognizing that if you don't have to memorize, okay,

4
00:00:16,350 --> 00:00:19,800
I have this kind of variable, therefore I need to have this kind of sampling distribution.

5
00:00:19,980 --> 00:00:26,640
I don't think that's quite as important as just realizing that you can't take a count and treat it like a continuous variable.

6
00:00:26,880 --> 00:00:30,690
Right. You can't take a dichotomy and treat it like a categorical right.

7
00:00:30,720 --> 00:00:35,040
You have to have some discernment in terms of what kind of variable.

8
00:00:35,220 --> 00:00:38,520
And I might look at questions about one here.

9
00:00:39,210 --> 00:00:42,150
But the sampling model is or the sampling distribution is.

10
00:00:46,060 --> 00:00:52,960
That's the most technical part of this week, but there's enough out there to say, okay, I have a variable with two levels.

11
00:00:53,470 --> 00:00:59,140
I'm going to use logistic regression. I have a categorical variable. It's not only a regression, I have a variable, etc.

12
00:00:59,680 --> 00:01:06,280
So that part's a little bit easier. And then there's some built in commands with an R, a lot of defaults that we can just follow.

13
00:01:06,580 --> 00:01:11,290
So it's just the recognition that my alpha variable is not this continuous normally distributed variable.

14
00:01:11,710 --> 00:01:18,540
How about that link function? What's that? Yes.

15
00:01:20,190 --> 00:01:23,400
That's where you essentially born.

16
00:01:25,550 --> 00:01:30,470
Maybe Category two?

17
00:01:31,970 --> 00:01:37,700
Yeah, absolutely. So at the end of the day, are going to try to make this variable, whatever it looks like if it's a two.

18
00:01:37,910 --> 00:01:41,690
If it's a category, if it's a count or whatever, it's going to look like,

19
00:01:41,900 --> 00:01:48,500
we want to use some sort of transformation to make it look like a continuous variable.

20
00:01:48,500 --> 00:01:52,280
Something orange is all the way from negative infinity to positive infinity.

21
00:01:52,670 --> 00:01:57,620
Such that we can ultimately regress that on our our predictor variables.

22
00:01:57,980 --> 00:02:03,860
Okay. So all this is, is some sort of mathematical transformation to make, number one,

23
00:02:04,580 --> 00:02:08,330
look like a continuous variable, like we've been using all semester long.

24
00:02:08,870 --> 00:02:15,349
And in fact, we've been using only function all semester long of the identity length and just means we're not touching it.

25
00:02:15,350 --> 00:02:18,350
All right. Because it's already a continuous variable.

26
00:02:19,520 --> 00:02:24,320
So your Z scores, you know, that have been of zero range from -3550.

27
00:02:24,680 --> 00:02:27,830
Those are kind of what we're trying to make all the other variables look like.

28
00:02:28,430 --> 00:02:42,380
Right. And then finally, structural, all of that. This is also something you've been doing all semester, at least the last four or five weeks.

29
00:02:44,280 --> 00:02:51,010
Whereas. Camilla predictors.

30
00:02:51,380 --> 00:02:56,060
Yes. Yeah. Yeah. It's the combination of linear predictors.

31
00:02:56,060 --> 00:02:59,510
Right. So these are all things that we think are going to be related to the outcome variable.

32
00:02:59,780 --> 00:03:03,050
Right. That's our structure model. When we say regression model.

33
00:03:03,290 --> 00:03:04,840
That's our structural model. Right.

34
00:03:04,850 --> 00:03:10,070
It's how we think the outcome variable is related to a whole bunch of covariance or a whole bunch of predictor variables.

35
00:03:10,640 --> 00:03:16,790
So all we've really done here is try to make that left side of the equation, the outcome side of the equation.

36
00:03:17,270 --> 00:03:22,010
So it's something that's applicable to our standard linear regression or multiple regression model.

37
00:03:22,700 --> 00:03:25,730
That's all this really is. All right.

38
00:03:26,120 --> 00:03:30,860
So there is a fair bit of math this week. There's a fair bit of new terminology and jargon.

39
00:03:31,130 --> 00:03:34,010
But at the end of the day, that's all we're doing. All right.

40
00:03:34,250 --> 00:03:39,510
And what's great about these structural models is they're the exact same kinds of interpretation of unit change,

41
00:03:39,510 --> 00:03:44,330
and our predictor variable equals that much change in our outcome variable, holding everything else constant.

42
00:03:44,750 --> 00:03:50,420
Right. The interaction terms are going to work. Similarly, the variables that we create and plug into a logistic regression, percent regression,

43
00:03:50,660 --> 00:03:55,640
they're going to work similarly because they're all part of the same family models, which is awesome.

44
00:03:56,040 --> 00:03:59,960
Good questions about number whatever one here.

45
00:04:04,340 --> 00:04:09,979
All right. How about number two? First off, is number two related?

46
00:04:09,980 --> 00:04:24,300
So one, two or three. True?

47
00:04:24,870 --> 00:04:29,110
Yeah, it is actually kind of how we're taking a variable.

48
00:04:29,180 --> 00:04:33,080
It has a defined range of 0 to 1 a problem.

49
00:04:33,510 --> 00:04:39,890
Right. Folks are familiar with the concept of probability. Six in ten chance that something is going to happen versus not going to happen.

50
00:04:40,370 --> 00:04:45,859
Right. To creative to make a variable that's prior to a probability that can only

51
00:04:45,860 --> 00:04:49,700
range from 0 to 1 into something that we can plug into our structural model.

52
00:04:49,910 --> 00:04:52,800
We're going to have to make some sort of transformation, right?

53
00:04:52,820 --> 00:04:58,790
We have to do something to that variable so that it fits what our conception is of a structural model needs to be.

54
00:04:59,300 --> 00:05:04,310
So the way we do that is two steps. We take a probability that ranges from 0 to 1,

55
00:05:04,430 --> 00:05:10,250
and we divide it by the probability of something happening divided by the probability of something not happening.

56
00:05:10,550 --> 00:05:16,810
And then we get the odds. So the probability of success divided by the probability of not success, the probability of getting sick first,

57
00:05:17,750 --> 00:05:22,730
divided by the probability of not being sick, etc. etc. etc. that ranges from what?

58
00:05:26,130 --> 00:05:29,460
Some people think zero zero infinity.

59
00:05:29,640 --> 00:05:35,700
We're almost there. We're almost there. Right. So we still want to have the full range and know the real number line.

60
00:05:35,910 --> 00:05:41,520
So we take a little longer than that. For a logarithm of that, we get to the log of the ads for low.

61
00:05:42,060 --> 00:05:49,020
We now have a variable that looks and feels and copy tastes like a linear regression outcome,

62
00:05:49,380 --> 00:05:55,840
a continuous maybe not necessarily find it completely normally distributed, but it's close enough, right?

63
00:05:56,130 --> 00:06:00,930
None of these sampling models, some of the sampling distributions are ever perfect in practice.

64
00:06:01,470 --> 00:06:05,370
But what we do is we try to approximate if we look at a distribution of an outcome variable,

65
00:06:05,580 --> 00:06:11,160
if it looks normal enough, then we can use our kind of linear regression models here.

66
00:06:11,190 --> 00:06:14,250
So we take something that started from 0 to 1.

67
00:06:14,580 --> 00:06:20,160
Now we have negative infinity all the way to positivity. This is the essence of only function.

68
00:06:22,760 --> 00:06:27,710
Questions about this. Do you really need to memorize this?

69
00:06:29,060 --> 00:06:34,520
Probably not, but it might help when we start to get to the interpretability part down here.

70
00:06:35,060 --> 00:06:38,360
Right. Interpreting illusions is not straightforward.

71
00:06:38,390 --> 00:06:42,500
People don't think about the world and logic. They don't think about numbers and the law of the odds.

72
00:06:43,220 --> 00:06:49,670
So we use things like odds ratios or we can even convert back to a probability because it is more familiar, it's more accessible for a lot of folks.

73
00:06:50,120 --> 00:06:54,260
So understanding how we use this can maybe be helpful from time to time.

74
00:06:54,440 --> 00:07:01,610
But if you only ever got to looking at the output from a logistic regression analysis and said positive association,

75
00:07:02,030 --> 00:07:06,410
positive association or negative association, you're probably still in pretty good shape.

76
00:07:08,900 --> 00:07:15,620
And last one here. I wonder that.

77
00:07:25,980 --> 00:07:33,440
You think that people who reported having LGBT socialist countries to.

78
00:07:34,020 --> 00:07:38,790
Horses have 26, six odds of reporting.

79
00:07:41,190 --> 00:07:44,360
I guess that's it, huh?

80
00:07:44,890 --> 00:07:48,220
Um, so that's kind of where we, we, we get to, right?

81
00:07:48,380 --> 00:07:56,930
You know, point six, 65. I mean, if a Saturday drinking episode can be and potentially a more difficult for folks to make sense around.

82
00:07:57,270 --> 00:07:58,309
Um, but I mean,

83
00:07:58,310 --> 00:08:07,670
in the sense it's giving you a relative likelihood of something happening versus not happening given a unit change and our particular variable.

84
00:08:07,910 --> 00:08:08,180
Right.

85
00:08:08,180 --> 00:08:14,060
So when you see a logistic regression output, you're going to have some independent variable, you're going to have some sort of outcome variable.

86
00:08:14,300 --> 00:08:19,610
And the way that we relate these to each other is often through analogies or odds ratios.

87
00:08:20,150 --> 00:08:23,209
You can take the inverse of this if it helps to make it a little you know,

88
00:08:23,210 --> 00:08:28,670
we have one point whatever times the odds of something happening or not happening.

89
00:08:28,970 --> 00:08:36,230
Question of I suppose going to ask and you get flip it and say people without this of course have one over six.

90
00:08:36,350 --> 00:08:41,780
Yeah, exactly. Yeah. And so if that's a little easier to think of it in one kind of a more positive direction,

91
00:08:41,780 --> 00:08:51,580
at least in terms of the association, that's that's completely fine. What would be the difference in saying like times and more likely, uh.

92
00:08:55,210 --> 00:09:00,430
I mean, I guess if I was your queen more likely to time here instead of something happening, that's.

93
00:09:00,730 --> 00:09:11,740
That's what you're. You know, that would be that. Yeah. Yeah, I live, I guess I'm a little confused because I thought less than one was less like.

94
00:09:12,340 --> 00:09:18,980
Yes. So could you interpret this as individuals with so of support resources?

95
00:09:19,560 --> 00:09:25,210
Uh. With 66 times less likely to participate at the summit.

96
00:09:26,680 --> 00:09:34,870
Okay, so if the odds ratio is one, there would be no more or less likely to report episodic drinking.

97
00:09:34,870 --> 00:09:37,959
If it's another issue over one, they're more likely to report drinking.

98
00:09:37,960 --> 00:09:50,930
If they're less than one, it's less likely. So you could say there's 66% less likely to have had the episode.

99
00:09:53,970 --> 00:09:58,450
Drinking in the. The ones that had the social support.

100
00:09:58,490 --> 00:10:11,690
And um, so there is a, where it gets a little tricky is that it's not a straight linear, you know, 66%.

101
00:10:11,750 --> 00:10:16,850
It's like where when you change and if you have to you and a change would be like 123% or something like that, right?

102
00:10:17,180 --> 00:10:22,040
That 66% and 66% and six. So it's always kind of multiplied by a fraction or a ratio.

103
00:10:23,030 --> 00:10:26,800
So it's a not a it's not a constant change throughout.

104
00:10:26,810 --> 00:10:33,350
That's part of the hard part of the logistic distribution. You have more change that happens at the extremes are tailwinds.

105
00:10:33,830 --> 00:10:37,580
So for this very specific 012 change.

106
00:10:37,580 --> 00:10:43,670
Yes, but then for subsequent change afterwards, we will be able to use the same 66% reduction.

107
00:10:46,320 --> 00:10:57,880
34% reduction. There are questions about any of this.

108
00:10:59,170 --> 00:11:03,610
Conceptually, again, if you're here and I do have my responses to these things,

109
00:11:03,610 --> 00:11:07,150
too, for folks to reference if you want, were you able to see the answers? Yes.

110
00:11:07,300 --> 00:11:11,890
Okay. Here. Yeah. So if you want to see kind of how I talk about them, those are always there.

111
00:11:11,890 --> 00:11:12,879
But then bring these up.

112
00:11:12,880 --> 00:11:19,020
So try to make sure that you're feeling comfortable with why we're doing some things rather than just, Oh, okay, it's my script too.

113
00:11:19,030 --> 00:11:22,030
So we ran this analysis and that's what's that's what's going to happen.

114
00:11:22,510 --> 00:11:30,460
All right. So I'm going to skip these parts. And this is this is that sampling, modeling function, structural model,

115
00:11:31,060 --> 00:11:38,590
just understanding that this part of it is really just a reflection of what you've identified, your outcome variables going to look like.

116
00:11:39,430 --> 00:11:48,400
Try not to worry so much about this, what my distribution should be, because usually you're going to get away with the defaults that are sets for you.

117
00:11:48,730 --> 00:11:53,110
Now, there's always a little bit of nuance and there are going to be caveats or special cases where you're not going to want to use,

118
00:11:53,110 --> 00:11:59,590
for example, a vanilla distribution that's well in advance of what we need to consider for most of the work that we do.

119
00:11:59,860 --> 00:12:06,040
So we're talking about the 5% of analyzes that don't kind of map on to a distribution like this.

120
00:12:06,040 --> 00:12:09,490
And that's going to be true of all the general any amount of stuff that we do this week.

121
00:12:09,820 --> 00:12:19,000
Okay. But if you want to impress your friends, there you go on here, folks, remember this stuff.

122
00:12:19,870 --> 00:12:27,760
Not that it matters too much here, but looks the same thing with the link function.

123
00:12:28,210 --> 00:12:33,130
Your link function is going to be something that is going to be more or less prepackaged for you.

124
00:12:33,670 --> 00:12:37,809
But we can talk about it a lot more specifics if you want to. This is just a slide.

125
00:12:37,810 --> 00:12:41,440
I think I'll do all this in the video service going the wrong way of converting.

126
00:12:41,740 --> 00:12:51,430
We just did this together as a group that you can convert from a probability to a logit in our logit back to a probability which you just did this to.

127
00:12:56,410 --> 00:13:03,069
We'll talk about this in practice. I need you to remember is that we're moving away from ordinarily squares.

128
00:13:03,070 --> 00:13:07,930
We don't have to calculate this by hand any more. Go blue, so it doesn't really matter to us.

129
00:13:07,930 --> 00:13:12,100
But we're going to be using a different computational method now.

130
00:13:12,280 --> 00:13:17,380
So where we try on a bunch of different parameter values to see which fits the data best.

131
00:13:17,740 --> 00:13:23,740
This is calculus. This is also calculus, but it's rote calculus.

132
00:13:23,750 --> 00:13:24,950
It's just trying to undo it. Why?

133
00:13:24,950 --> 00:13:30,160
I shouldn't I mean, it's just trying tons and tons of different values to see which one maximizes the likelihood function.

134
00:13:32,590 --> 00:13:38,800
That's what this is here. Believe or not, all the distributions that we play with in this class all have a likelihood function.

135
00:13:39,130 --> 00:13:45,130
It just basically tells us how likely is an observation based on the parameters or the values, like the means.

136
00:13:45,130 --> 00:13:51,760
They're deviation probabilities. So really all this slide is telling us is that we're going to try tons and tons of different probabilities.

137
00:13:52,330 --> 00:13:55,870
So that's the that's what drives our entire logistic regression equation.

138
00:13:56,110 --> 00:14:03,520
What is the true underlying probability that someone is ill or someone is present or whatever?

139
00:14:03,760 --> 00:14:09,280
Right. That's what we care about. That's the whole point. Give me that number based on that number.

140
00:14:09,520 --> 00:14:16,060
All of this stuff sort of manifests. What this computer is going to do is it's going to try thousands,

141
00:14:16,330 --> 00:14:23,890
thousands and thousands and thousands of different probabilities and it's going to plug in all the data based on a given probability.

142
00:14:24,130 --> 00:14:30,070
So let's just say it's 0.7. We're going to take that guess. It's going to plug in 0.7 to this equation.

143
00:14:30,370 --> 00:14:35,740
It's going to map all the data onto an equation like this with a probability of 0.7.

144
00:14:36,100 --> 00:14:37,720
And we're going to see how well that fits.

145
00:14:38,590 --> 00:14:45,940
And it's going to do the same thing with .70001 and .7002, and then all the way up to two one or all the way down to zero.

146
00:14:46,210 --> 00:14:49,560
And it's going to figure out which of those probabilities is his best fit.

147
00:14:49,610 --> 00:14:54,400
So it's trying on tons and tons of different hats. That's essentially what maximum likelihood is doing for us.

148
00:14:55,300 --> 00:14:58,330
So it's good to know. That's a little bit of what's happening behind the scenes.

149
00:15:01,590 --> 00:15:10,590
Does the data fit the model? Just like we do with ANOVA and linear regression.

150
00:15:10,830 --> 00:15:15,290
We are going to look at that question of does does our model fit the data, right?

151
00:15:15,300 --> 00:15:22,260
And so we'll have models that we can compare. We will use a slightly different sort of comparison than a nova.

152
00:15:22,670 --> 00:15:27,330
It's called a likelihood ratio test because we're now not using well, let's regression.

153
00:15:27,570 --> 00:15:33,600
We're using that log likelihood that I just talked about, which again, might seem a little nebulous right now,

154
00:15:33,600 --> 00:15:38,400
a little abstract, but it's just really some sort of estimate of how well the model fits the data.

155
00:15:39,030 --> 00:15:43,560
And then we're going to compare that with successive models, which are the models fits the data best.

156
00:15:43,570 --> 00:15:51,240
And if we want to go over this in a lot more depth, we can do that. But we're trying to get to an example here just to clarify things give you a

157
00:15:51,240 --> 00:15:56,610
statistic is just is just a transformation of our log likelihood or kind of fit.

158
00:15:56,850 --> 00:16:01,770
It's just taking that log likelihood when we have multiple models.

159
00:16:02,190 --> 00:16:09,180
We can look at the change in our log likelihood, which this is our likelihood.

160
00:16:09,720 --> 00:16:13,530
We're just taking the log of the likelihood right there.

161
00:16:15,900 --> 00:16:19,799
You don't need to memorize that just quite yet. But if if it's helpful,

162
00:16:19,800 --> 00:16:27,270
we can talk a lot more or more detail about we'll look at how the log likelihood or model fit changes between

163
00:16:27,270 --> 00:16:32,970
successive models and we can operationalize that as the deviant statistic which we can test with the Chi Square.

164
00:16:33,810 --> 00:16:41,880
That was three slides and a lot of jargon to say we can use a chi square test instead of a nova to see whether model one fits better than model two.

165
00:16:42,720 --> 00:16:44,640
Right. That's what we're really getting down here.

166
00:16:45,120 --> 00:16:52,320
So if we did it in multiple regression over the last few weeks, there's an analog for it with logistic regression.

167
00:16:53,220 --> 00:16:59,549
Okay. So everything that we did before, how we build the models, how we interpret the coefficients,

168
00:16:59,550 --> 00:17:03,690
all that kind of stuff, there is a similar thing in logistic regression.

169
00:17:03,870 --> 00:17:09,090
We're just going to have to point out the differences so that we use the right test when when the time comes.

170
00:17:10,260 --> 00:17:13,830
So just some things about the deviance. It's our model fit measure.

171
00:17:14,040 --> 00:17:18,659
We're going to be able to compare models, including against our worst possible model.

172
00:17:18,660 --> 00:17:23,750
We did this with linear regression to our worst possible model was the mean in this case,

173
00:17:23,760 --> 00:17:29,550
our worst possible model is just going to be if it's above 0.5, the probability everybody is a yes.

174
00:17:29,820 --> 00:17:34,920
If it's below 0.5, everybody's a no. Right. So it's just going to say, what's the most likely thing to happen?

175
00:17:35,160 --> 00:17:37,110
And we're just going to assume that's true for everybody.

176
00:17:37,500 --> 00:17:45,060
That's essentially the worst guess that we could have if six out of ten people are going to watch the Michigan State game on Saturday,

177
00:17:45,360 --> 00:17:47,730
I'm just going to say randomly pick somebody.

178
00:17:48,120 --> 00:17:54,200
You're also going to watch the Michigan State game on Saturday y because six out of ten, more likely than not that you are going to watch that game.

179
00:17:54,780 --> 00:17:58,380
So many people will watch that game and say go boom, you go blue.

180
00:17:58,440 --> 00:18:06,299
This are your models miss is values fit the data better so that's just kind of as we

181
00:18:06,300 --> 00:18:11,190
as we look at nested models those where the deviance value is smaller that's better.

182
00:18:11,370 --> 00:18:14,760
But again, we'll use the chi square test to test that empirically.

183
00:18:17,620 --> 00:18:21,610
And the bad part is we don't have any more R-squared percent of variability explained.

184
00:18:21,970 --> 00:18:25,780
You can't use that with logistic regression. There they have.

185
00:18:26,200 --> 00:18:32,170
They've created a couple of different analogs to it, but they're not well-accepted or broadly accepted.

186
00:18:32,560 --> 00:18:36,130
I look at them as kind of a, you know, just a face validity check.

187
00:18:37,000 --> 00:18:43,540
But really, we are going to go to a model by model comparison rather than a strict.

188
00:18:43,900 --> 00:18:47,320
This is how much variability in the outcome we're explaining. Okay.

189
00:18:47,500 --> 00:18:53,860
So it's more of a question of which is my best fitting model, not a global question of I'm explaining about 30% of the variability.

190
00:18:56,700 --> 00:19:04,349
All right. So with that crash course of theory and again, let's go back to that.

191
00:19:04,350 --> 00:19:07,910
If folks are interested in some of those pieces or if you want to see a little bit more,

192
00:19:07,920 --> 00:19:12,240
I can always try to expand on this particular slide or something if a question popped up.

193
00:19:12,570 --> 00:19:17,010
But I wanted to walk through an example and then in particular,

194
00:19:17,010 --> 00:19:23,400
if you then ask or then you have some questions about some of the theoretical theory or abstract part of it, we can go back to it.

195
00:19:24,330 --> 00:19:30,750
So this is our new function. Remember, we've been using Linear Elm Linear Model all semester long.

196
00:19:30,990 --> 00:19:39,360
We're just now going to use the general linear model. The nice thing is the default Gaussian is just a fancy word for normal.

197
00:19:40,050 --> 00:19:44,040
The default for this whole thing is just your linear regression model.

198
00:19:44,670 --> 00:19:52,050
So it'd be the exact same thing as using Elm if you didn't change any of this stuff, if you just went through the defaults.

199
00:19:52,830 --> 00:19:56,490
Now, there are a ton of arguments that I have listed up here.

200
00:19:56,940 --> 00:20:02,520
The ones that are highlighted, whatever color that is, are the ones that you have to worry about, right?

201
00:20:03,000 --> 00:20:07,290
Yeah. If you get all fancy and you want to, you can start to play around with some of these additional ones.

202
00:20:07,590 --> 00:20:12,810
But again, 95 times out of 100, all you have to worry about are these things okay.

203
00:20:13,470 --> 00:20:17,850
Formula happily is just the outcome.

204
00:20:17,850 --> 00:20:22,470
Variable tilde predictor variables. Exact same from the rest of the semester.

205
00:20:22,790 --> 00:20:26,310
That same you're already halfway done. The next big one.

206
00:20:26,790 --> 00:20:30,060
This is that sampling model, that sampling distribution.

207
00:20:30,990 --> 00:20:37,710
Okay. What is your outcome variable look like? That's this is the only mistake you're going to make with logistic regression.

208
00:20:38,670 --> 00:20:43,830
You need to make sure that the family is appropriate for your outcome variables.

209
00:20:45,210 --> 00:20:48,660
So if you have a dichotomy, it's not Gaussian or normal.

210
00:20:49,650 --> 00:20:53,250
If you have a count, it's not Gaussian or normal, right?

211
00:20:53,610 --> 00:20:58,290
So it's trying to map on the family to the appropriate distribution for your outcome.

212
00:20:58,740 --> 00:21:09,960
Pause questions about that. Good.

213
00:21:11,100 --> 00:21:15,660
So really mind reduce down to this. This is not too bad, right?

214
00:21:15,670 --> 00:21:21,750
This looks an awful lot like what we've been doing all semester long. The only big difference here is this family thing that we have to remember.

215
00:21:22,080 --> 00:21:25,650
So I'm going to say it as an object. Life is good here, right? This is not all that.

216
00:21:25,990 --> 00:21:30,480
We look at this. Look at this. This is easy peasy, right?

217
00:21:30,690 --> 00:21:34,560
We're happy with this. We're very happy with this. Very comfortable with this.

218
00:21:35,430 --> 00:21:40,320
So I'm going to save a model. This whatever you want to call it. I'm using zero instead of just l.

219
00:21:40,770 --> 00:21:48,390
I had my outcome variable, which I'm forgetting right now is that I forget.

220
00:21:48,400 --> 00:21:53,370
I think it's likelihood of sex initiation or something like that. Thank you.

221
00:21:53,790 --> 00:21:57,410
That was size it. So now I'm just looking at the content now.

222
00:21:57,840 --> 00:22:02,489
All right. Thank you. Goodbye. So this is like the end of sex initiation, so.

223
00:22:02,490 --> 00:22:06,540
Yes, now. Right. So by the time they're in real estate, they're about 14 or 15 or something like that.

224
00:22:06,780 --> 00:22:10,350
Just a question of whether or not they're they're sexually active or having another thing in their life.

225
00:22:11,460 --> 00:22:14,910
We're going to start with just a base base model.

226
00:22:15,600 --> 00:22:21,390
We don't have to run this model because it's going to be embedded with actually all the different models that will run with the GLM.

227
00:22:22,020 --> 00:22:27,270
But this is our worst guess kind of model. It's going to assume that there's some underlying probability.

228
00:22:28,440 --> 00:22:32,070
It's going to estimate that just based on the strict proportion.

229
00:22:32,460 --> 00:22:36,030
So how many people in this sample have had sex in their lifetime?

230
00:22:36,240 --> 00:22:39,360
Right. That's basically what this model's going to tell us.

231
00:22:39,690 --> 00:22:43,590
But but it has illustrated for two different reasons.

232
00:22:43,620 --> 00:22:45,960
One, it tells us what's our function.

233
00:22:46,710 --> 00:22:55,710
Now, you know this term logit log of the odds are linked function here or is going to it's part of a binomial distribution.

234
00:22:56,280 --> 00:23:00,840
Binomial is what the family that we're using here.

235
00:23:01,560 --> 00:23:05,610
There are multiple links that we could use within the binomial distribution.

236
00:23:05,940 --> 00:23:10,620
And I want you to immediately forget that I just mentioned that it's here for you, for you to know.

237
00:23:11,100 --> 00:23:15,780
But this is what you're going to use. No reason really to stray beyond this unless you have some really funky data.

238
00:23:16,440 --> 00:23:20,400
Okay. Again, now we're talking like 99 times out of 100.

239
00:23:20,770 --> 00:23:24,990
You're going to be happy with your login link. Recognize the family.

240
00:23:25,590 --> 00:23:30,150
The link I actually think is. It might be default, but put it in there just in case.

241
00:23:32,870 --> 00:23:36,310
Questions about this initial syntax. Yeah, exactly.

242
00:23:36,800 --> 00:23:46,400
Your family moved here for this is going to be something like, I don't know, county, basically.

243
00:23:46,820 --> 00:23:52,790
Yeah. So and next week I'll show you so like this. Like if this is a facade family or multi nominal family or whatever.

244
00:23:53,090 --> 00:23:57,590
Right. And like the link actually is going to be, I think the same across all of them.

245
00:23:58,250 --> 00:24:01,950
So you can use a logic, you know, link for category outcomes, for account outcomes.

246
00:24:02,600 --> 00:24:11,440
It's pretty standard to use. What kind of question would do we ask here before I keep going down this path?

247
00:24:11,450 --> 00:24:16,580
Do we know what question we're asking here? What's a logical extension of this model to start asking questions?

248
00:24:18,740 --> 00:24:22,850
Outcome variables. Sex initiation by age 1415. What kind of questions are we asking here?

249
00:24:32,080 --> 00:24:38,170
You know, the first did at this point. What do we ask him? I threw some variables into this model.

250
00:24:38,170 --> 00:24:47,990
What do we know? What kind of question are we asking? How did we interpret those odds ratios?

251
00:24:48,020 --> 00:24:52,070
That's what's coming down the line, right? We're going to get a whole bunch of allergists are going to get some odds ratios.

252
00:24:52,400 --> 00:24:56,330
And what we're going to relate this outcome variable to some combination of predictors.

253
00:24:56,840 --> 00:25:00,260
Right. And we want to see what those relations are associations look like,

254
00:25:00,800 --> 00:25:09,650
what variables within our dataset predict or associated with the likelihood of initiating sex or not.

255
00:25:09,920 --> 00:25:15,739
Right. That's what this question is all about. Don't forget those things as we're moving through our code here.

256
00:25:15,740 --> 00:25:19,220
I know you can do this code. That's no problem. I just send it to you.

257
00:25:19,430 --> 00:25:25,490
You just type it all in. Not a big deal, but at least have some sense of what this model is supposed to be doing for us.

258
00:25:26,000 --> 00:25:33,530
Right. I'm going to pick out some variables that I think are related to sex initiation and figure out whether or not they do a good job of predicting.

259
00:25:33,650 --> 00:25:39,500
From a sample of 820 real people out in Flint, Michigan, back in 1994 were sexually active or not.

260
00:25:39,920 --> 00:25:45,260
Yeah. So the predictor we're having is we're saying yes to sex.

261
00:25:45,590 --> 00:25:50,210
Want to like one is they said yes to sex. No, no, no.

262
00:25:50,720 --> 00:25:59,650
This is basically just say an intercept only model. This is run a model where we have beta zero outcome variable baseline model.

263
00:26:00,200 --> 00:26:06,560
Okay. Okay. You're probably thinking more along the lines of something like like this model.

264
00:26:08,630 --> 00:26:16,850
Okay. So basically when we, when we just have regressed on one, we're just talking about a single value.

265
00:26:17,300 --> 00:26:25,160
This is our worst possible guess, right? It's basically going to use the proportion as the predictor variable.

266
00:26:25,790 --> 00:26:32,630
Right? Again, what's the likelihood or what's the proportion of students that have had sex at age 14 or 15?

267
00:26:33,050 --> 00:26:36,080
Right. It's going to give us some there's some true value in our in our sample.

268
00:26:36,740 --> 00:26:45,320
I figure what it is like 40%, 60% on with that, based on that count as our only parameter or only information,

269
00:26:46,190 --> 00:26:51,560
the model will now try to make some predictions about whether a specific individual has had sex.

270
00:26:52,270 --> 00:26:59,270
All right. The same thing happened in linear regression where we use just the mean based on the mean.

271
00:26:59,570 --> 00:27:01,010
If we had no other information,

272
00:27:01,250 --> 00:27:09,740
we just kind of predict the mean for everybody based on knowing how many or what proportion of people have had sex in the sample.

273
00:27:10,220 --> 00:27:13,550
We are just going to use that information for our predictions.

274
00:27:13,820 --> 00:27:18,370
And again, logistic regression is kind of funny because you can't just, you know, guess the meaner camp.

275
00:27:18,620 --> 00:27:22,760
Well, there's a probably they've had 6.6 times now that doesn't work.

276
00:27:23,180 --> 00:27:27,080
If it's above 0.5, they just going to say everybody in the samples said sex.

277
00:27:27,380 --> 00:27:30,260
If it's below point five, nobody in the sample has had sex.

278
00:27:30,710 --> 00:27:38,210
I can tell you and you can tell me that's a terrible model to predict because we know that at least whatever 30 or 40% ever have had enough.

279
00:27:38,870 --> 00:27:48,350
Right. But it starts it establishes some level of model fit from which we can now compare models where we have new information,

280
00:27:48,710 --> 00:27:51,890
new variables to see if we can do a better job of predicting.

281
00:27:51,890 --> 00:27:58,430
Specifically, does student one, three, four or five two what is their activity?

282
00:28:01,790 --> 00:28:09,540
That makes sense. Isn't that amazing? Let's keep working to the example and let's see if we can get there.

283
00:28:09,600 --> 00:28:15,719
All right. But I am I do want to push back and I just try to come at these analogies with something in your head.

284
00:28:15,720 --> 00:28:19,200
You're trying to glean some information from this data. Right.

285
00:28:19,200 --> 00:28:23,459
It's not just reproducing the code. Right. That was my stats training.

286
00:28:23,460 --> 00:28:28,260
It sucked because then I didn't know how to do it. Well, it does raise a whole bunch of code.

287
00:28:29,880 --> 00:28:33,330
All right, so here's my model. We have just one parameter.

288
00:28:33,330 --> 00:28:37,469
We've really just estimated one thing. And this is the intercept. We can use this.

289
00:28:37,470 --> 00:28:45,150
This is a loaded this is all a log of the odds to figure out exactly what proportion of people in the sample were sexually active.

290
00:28:45,360 --> 00:28:51,570
Now, we could have just done this descriptively, but the reason why we want to run this model is because of this.

291
00:28:52,170 --> 00:28:57,090
This is our model fit information. You're used to looking down here and seeing an over the test.

292
00:28:57,380 --> 00:28:59,370
There's no ANOVA and there's no F test.

293
00:28:59,370 --> 00:29:06,660
But but there is this deviants information which we know is going to relate to each other based on kind of a chi square test.

294
00:29:07,170 --> 00:29:19,170
Right. Right now, the no deviance baseline or no no zero predictors in the model is the same as the residual deviance,

295
00:29:19,170 --> 00:29:31,230
which we can think of as the alternative model. Every logistic regression you run with this data should give you the exact same no deviance, no zero.

296
00:29:31,620 --> 00:29:37,800
No predictors compared to a model that has not no predictors, right?

297
00:29:38,040 --> 00:29:39,210
That's where we're going to get to.

298
00:29:39,690 --> 00:29:48,000
But we want essentially this value to help us start to make some decisions about as we add predictors, does our model fit improve.

299
00:29:49,080 --> 00:29:52,730
I clause now make makes sense.

300
00:29:52,740 --> 00:30:00,010
No make sense accent. They're looking at.

301
00:30:00,020 --> 00:30:07,940
There's just no range. But what's better? Higher or lower? No shortage.

302
00:30:08,610 --> 00:30:14,630
Models, martinis, values. Better thing. Okay. Could you repeat again why we're comparing them to residual?

303
00:30:15,050 --> 00:30:21,500
So this is this this is our this whole model is just where this value this line.

304
00:30:21,770 --> 00:30:29,030
That's where this comes from. It is this model that gets us to this no deviance when we do, when we run.

305
00:30:29,180 --> 00:30:38,060
Now, this model, no deviance is the same, but the residual deviance has changed.

306
00:30:38,480 --> 00:30:43,860
That's where we're getting to. Now, there was you are not getting that.

307
00:30:44,300 --> 00:30:50,390
So in this model and no models were predicting 48% at this track.

308
00:30:50,930 --> 00:30:54,810
That's the log of the odds. So. Right. We have to convert back to.

309
00:30:55,220 --> 00:31:04,730
Aha. So we take, we take our largest right logit is the log of the natural log of the odds,

310
00:31:05,300 --> 00:31:09,950
which we got from the probability by taking a probability of success provided by probability, not success.

311
00:31:10,220 --> 00:31:14,150
So we've got to do a little bit work. We got to do a little bit algebra to get ourselves back.

312
00:31:14,810 --> 00:31:18,980
So this is the way that we started. We started with eight. Where is our probability?

313
00:31:19,850 --> 00:31:25,030
Probability or not? Probability. I would go from A to theta or five.

314
00:31:26,120 --> 00:31:32,200
So getting to our log of our log of the odds gives us our loads, right?

315
00:31:33,590 --> 00:31:42,080
We can reverse this process by taking the exponential of eight earth divided by one plus the exponential of error.

316
00:31:43,040 --> 00:31:46,220
Right. So that's what this value is.

317
00:31:46,790 --> 00:31:50,540
Right. We take our logic and we plug it into this equation.

318
00:31:51,890 --> 00:31:57,620
Right. And that's going to help us work backwards to a probability when I plug in my logit.

319
00:31:57,770 --> 00:32:04,880
And don't forget that there is a Oh, I'm using this one right here, so I'm using this data here.

320
00:32:06,020 --> 00:32:19,310
So one divided by one, plus the exponential of -8.48 gives me a probability of 0.61.

321
00:32:20,450 --> 00:32:28,160
Right. I've taken a logit of 0.48 and I've converted it back to a probability.

322
00:32:29,720 --> 00:32:36,530
I took a logit, converted it back to a probability before we ever in the analysis, we started with a probability we do.

323
00:32:36,530 --> 00:32:40,450
We create the odds. We did the long of the odds. That's why with all logit, right?

324
00:32:40,460 --> 00:32:44,840
But we can work backwards. It's perfectly acceptable. What I think is really, really neat about this.

325
00:32:45,500 --> 00:32:49,310
There are 30 people who didn't respond to this question, so we had 820 people in the sample.

326
00:32:50,000 --> 00:32:58,970
This is the exact same thing as our sample proportion. 507 of the 820 people reported being sexually active, about 61%.

327
00:33:00,230 --> 00:33:04,060
That's what this model is telling us. It is giving us the probability of the outcome.

328
00:33:05,150 --> 00:33:09,350
Right. It's estimating the probability for us. Now, this is just the estimate.

329
00:33:09,920 --> 00:33:16,310
The predictive model that kind of also happens behind the scenes is where it says right now, based on this parameter,

330
00:33:16,880 --> 00:33:22,460
that an error that we're thinking about, the true population of people who have had sex in the sample is about 60%.

331
00:33:23,210 --> 00:33:28,600
Right. What do I do with individuals? And this is where that funky thing happens, where it says, alright,

332
00:33:28,610 --> 00:33:34,160
well if I give an individual and I know that there's a better chance that they have had sex versus not,

333
00:33:34,460 --> 00:33:37,820
I'm just going to guess the same thing for everybody. Right.

334
00:33:37,940 --> 00:33:41,930
But we know that 38% of the people in the sample had not had sex.

335
00:33:42,620 --> 00:33:46,640
So we know our prediction models can be off and probably off by a fair amount.

336
00:33:47,120 --> 00:33:53,690
That's why this is the worst possible guess. And we should start to see improvement as we include additional information.

337
00:33:54,590 --> 00:33:58,130
What else do we know about folks that might be related to the sexual worse behavior or whatever?

338
00:33:58,430 --> 00:34:04,880
Our sex initiation sees me with partners, and let's see if we can do a better job of predicting because we know.

339
00:34:05,180 --> 00:34:11,240
Yes, no people have initiated sex. We're trying to do are trying to create a model that predicts that quite well.

340
00:34:12,110 --> 00:34:19,640
I think this is nicer than than a model just like your linear regression, because there is that tangible outcome.

341
00:34:19,970 --> 00:34:23,030
Right. We know it's yes or no and we have that data.

342
00:34:23,450 --> 00:34:29,809
So we're just trying to figure out a model that categorizes people well, which is a little different when we have some continuous outcome.

343
00:34:29,810 --> 00:34:36,530
Variable level 2.6. Right. Well, how do you relate these variables to about like a mean value of 2.6 or something?

344
00:34:37,160 --> 00:34:40,700
There is something nice about public health variables that have countable outcomes.

345
00:34:44,520 --> 00:34:47,550
All right. So this is what we do is this demographic models.

346
00:34:48,540 --> 00:34:52,560
So we have lots of sex, race and age.

347
00:34:53,850 --> 00:35:00,320
Notice up here, the only difference is now instead of a one, this one is a still under than underneath there.

348
00:35:00,330 --> 00:35:04,440
That's the intercept. Right. So anytime we regress on one, it's just give me the intercept.

349
00:35:05,610 --> 00:35:12,180
So we will have an intercept value. But then here we just plug it in our values, just like we did the rest of the semester.

350
00:35:13,620 --> 00:35:19,349
And I even know that I'm even lazier. And I didn't even put the link in there. I just said, Give me the binomial and it's going to default.

351
00:35:19,350 --> 00:35:26,900
The logic like, yeah, what do we need to do?

352
00:35:26,910 --> 00:35:34,260
We need a variable. Oh, yeah.

353
00:35:34,650 --> 00:35:38,889
Oh, yeah. If this is not a factor or if there are, these are all factors in this.

354
00:35:38,890 --> 00:35:44,700
The continuous, you know, you still need to make sure. So all the steps that we did, all the measurement stuff that we did, all still applies.

355
00:35:45,240 --> 00:35:50,130
Right. It's the exact it is almost the exact same thing as linear regression.

356
00:35:50,760 --> 00:35:56,820
Right. All we've done is we've just changed a little bit of the meaning of these coefficients.

357
00:35:57,180 --> 00:36:03,810
We've just taken a variable that wasn't continuous and normal, and we're using a two level outcome.

358
00:36:04,200 --> 00:36:09,240
So we had to do all that manipulation, but everything on the right side of the equation, just like with before.

359
00:36:09,930 --> 00:36:17,850
Okay. Just like before. Jen, I think you might have that answer.

360
00:36:18,090 --> 00:36:32,820
But just to clarify the difference between generalized neurons and just logistic regression and using a non normal distributed variable.

361
00:36:34,320 --> 00:36:41,190
No, not continuous normally distributed variable. Yeah. So the continuous part is the real key, but the normal do normally distribute.

362
00:36:41,220 --> 00:36:46,620
Is that what's one of those underlying assumptions. Right. So that's why it's better to think about it as both together.

363
00:36:48,640 --> 00:36:52,840
I mean, that's really the only difference is we have a new kind of outcome variable.

364
00:36:53,110 --> 00:36:58,270
It violates like a two level outcome variable. If you look at a histogram, it's just one or zero.

365
00:36:58,600 --> 00:37:03,100
Right. And that violates the linearity assumption. It violates the homogeneity assumption.

366
00:37:03,310 --> 00:37:12,040
So a lot of those linear assumptions that we covered in week four or whatever, five are going to be violated here until we make some of these changes.

367
00:37:12,400 --> 00:37:18,490
By making this a logit, we can get away with running this kind of standard regression modeling.

368
00:37:22,610 --> 00:37:27,620
This room feels deflated and it makes me very sad. I don't want to feel deflated or defeated or anything.

369
00:37:27,800 --> 00:37:34,600
Why? You're only tested on this. So go blue. But it is more of just one.

370
00:37:34,610 --> 00:37:39,110
Let's try to keep going until folks are feeling comfortable. And there's nothing wrong with asking questions.

371
00:37:39,170 --> 00:37:42,740
Am I doing this for a a few more weeks? And when you do, this is.

372
00:37:42,830 --> 00:37:51,520
This is probably more relevant to folks in this building than many other places on campus, because we are so interested in observable outcomes.

373
00:37:51,530 --> 00:37:56,300
Countable outcomes. Yes. No, present at present outcomes. So this is good stuff to learn.

374
00:37:58,610 --> 00:38:04,159
We plug these values in. These are no longer our standard units.

375
00:38:04,160 --> 00:38:08,890
Right? These are not the original units. These are now what? These are all in what unit?

376
00:38:11,180 --> 00:38:14,959
Long odds, logics, whatever you want to call them. That's huge,

377
00:38:14,960 --> 00:38:24,950
because we can't say like a unit change in our going from male to female or whatever is associated with a 0.55 change in our outcome variable.

378
00:38:25,100 --> 00:38:32,720
Right. That's not the that's not the interpretation where we're talking about now changes in the law of violence, which is okay.

379
00:38:33,260 --> 00:38:35,210
It helps us get to associations.

380
00:38:35,540 --> 00:38:40,880
The folks remember from the videos, how can we tell whether or not something is positively associated or negatively associated?

381
00:38:46,980 --> 00:38:55,870
Easier with allergies. How can we tell if these variables are so age that positively or negatively associated with sex initiation?

382
00:39:00,880 --> 00:39:04,900
How do you know that it's greater zero?

383
00:39:05,390 --> 00:39:08,840
Right. Yeah, sure. Right.

384
00:39:09,430 --> 00:39:19,060
So when we are in low diets, when we eat a lot of the odds units, anything that's over zero is positively associated with the outcome occurring.

385
00:39:19,810 --> 00:39:24,370
Anything is less than zero is negatively associated with the outcome occurring.

386
00:39:24,670 --> 00:39:31,360
So the more I have of something, the less likely it is that they're going to initiate sex.

387
00:39:31,680 --> 00:39:35,229
Right. So now health services, it looks like.

388
00:39:35,230 --> 00:39:42,910
So our female is our referent category. So as we go from female to male, the likelihood of.

389
00:39:47,050 --> 00:39:51,010
Sex initiation increases. Right.

390
00:39:51,310 --> 00:39:57,190
This is just another way of saying that males are more likely to report sex initiation in females at whatever age, 14 or 15.

391
00:39:59,560 --> 00:40:06,580
Similarly, age the older I am, the more likely than to report sex initiation.

392
00:40:08,470 --> 00:40:11,910
So in this case, you look at it from, like, negative infinity.

393
00:40:13,300 --> 00:40:16,690
Hmm. And when we're looking at our table, then this way.

394
00:40:17,170 --> 00:40:21,460
Are we just looking related to each other? So can we say that?

395
00:40:25,120 --> 00:40:29,020
You are. White House have more.

396
00:40:31,170 --> 00:40:37,350
Slightly more likely than you'd like, you know.

397
00:40:37,350 --> 00:40:42,300
So they saying, remember that Syria is a good point. So this is that we always start to think about back to the record category.

398
00:40:42,510 --> 00:40:45,629
So our referent category here is going to be African-Americans.

399
00:40:45,630 --> 00:40:53,310
So relative to African-Americans, all white participants in this in the sample were more or less likely to initiate sex.

400
00:40:53,880 --> 00:41:01,370
That's like. And that's that's kind of the extent that you're going to get from logic terminology.

401
00:41:01,640 --> 00:41:07,790
It's they're not directly comparable. They're a little bit like beta coefficients and the understand on standardized data coefficients.

402
00:41:07,790 --> 00:41:14,899
And we can't say like, oh, well .55 is half of minus absolute value.

403
00:41:14,900 --> 00:41:20,600
It's half of 1.0.10. Right. That's no, they're not effect sizes in that way.

404
00:41:21,290 --> 00:41:25,790
Odds ratios are like those standardized coefficients in that they're an effect size in

405
00:41:25,790 --> 00:41:30,710
that they're standardized and be comparable across not just variables within an analysis,

406
00:41:30,950 --> 00:41:40,969
but other and other analysis as well. An odds ratio of 0.1 to five is the same in this study or any other variables relative to other studies.

407
00:41:40,970 --> 00:41:45,160
It's still 1.5 times the odds. Yeah.

408
00:41:46,400 --> 00:41:49,460
So this is great for just trying to figure general direction.

409
00:41:50,270 --> 00:41:57,890
Right now is more age, more what are white relative to males, relative females, white or relative to black?

410
00:41:58,040 --> 00:42:01,430
Right. Those are the same kinds of interpretations we've been using all semester long.

411
00:42:04,160 --> 00:42:10,040
This is the other piece of it. Now we're going to if you'd want to take from this study or sorry, from this analysis.

412
00:42:10,220 --> 00:42:14,180
We started with a note that we saw a slide and a half ago.

413
00:42:15,800 --> 00:42:21,730
We have now seen a decrease in deviance, which means this model fits the data better.

414
00:42:22,970 --> 00:42:27,710
Right. We can't say that with certainty. We're going to run a test.

415
00:42:27,890 --> 00:42:34,670
The analog you want to know. I mean, we're on a liquid end ratio test basically to see whether this change in deviance is worth

416
00:42:34,670 --> 00:42:39,680
the cost of losing four degrees of freedom or degrees of freedom because we estimated one,

417
00:42:39,950 --> 00:42:44,740
two, three, four. Framers.

418
00:42:48,940 --> 00:42:54,160
Political issues. Values. Yes.

419
00:42:54,480 --> 00:42:59,290
I'd be backtracking a little bit. Not really related to this, but.

420
00:43:01,130 --> 00:43:06,680
To get like very variable, say like w on a three way or Caucasian.

421
00:43:06,680 --> 00:43:10,130
Is that because of the a factor? Yeah. Yeah.

422
00:43:10,190 --> 00:43:14,240
This is all just default from the, um, from the program.

423
00:43:14,660 --> 00:43:18,340
But I would have read the script.

424
00:43:18,350 --> 00:43:22,850
I was up on channels, but I probably read these in as valuable as equals.

425
00:43:22,850 --> 00:43:29,870
True. Um, I used a little cheat because I don't like having a capital as everything, so that's why it's just wave 183 is all of our case.

426
00:43:30,200 --> 00:43:33,200
These are the same variables we've been using from earlier this semester.

427
00:43:35,700 --> 00:43:39,080
All right. Bear with me just last slide, and then I will let you go.

428
00:43:39,090 --> 00:43:43,290
We can do more of this on Thursday, as well as give you some time to practice.

429
00:43:44,610 --> 00:43:49,980
Two more slides. Sorry. First off, we can take we can exponentially.

430
00:43:50,730 --> 00:43:59,130
All right. Take to the power or whatever the coefficients and convert them from their original to odds ratios.

431
00:43:59,490 --> 00:44:05,670
And we all do this as a group where we can think about now this, this kind of interpretation.

432
00:44:05,670 --> 00:44:10,590
So it was point five, five and logit, we can use the XP function.

433
00:44:11,100 --> 00:44:17,190
We are exponentially 8.55. We get an odds ratio of 1.75.

434
00:44:17,660 --> 00:44:22,440
All right. So we can use language like, you know.

435
00:44:24,880 --> 00:44:30,280
Where's the e-mails? Perfect. Perkins Right. So we saw that there was a positive association mails where you can treatment relative

436
00:44:30,280 --> 00:44:38,050
to referent males had a higher odds of engaging in sexual sexual activity than females.

437
00:44:38,710 --> 00:44:48,330
In this particular sample. Right. 1.57 And again, all we've done is take the XP of this value to get to here.

438
00:44:48,720 --> 00:44:54,060
These are all very comparable. These were not this is not one half of this.

439
00:44:54,540 --> 00:45:02,220
But if we had our energy ratio that was point or 3.5, that would be twice.

440
00:45:02,560 --> 00:45:10,940
Sorry, this. Harder to think about things that are over one versus less than one.

441
00:45:10,960 --> 00:45:16,150
That's why you can take like the inverse. One over one divided by .334335.

442
00:45:16,390 --> 00:45:21,610
And that will give you a positive value that you can compared directly to 1.75, for example.

443
00:45:22,730 --> 00:45:28,360
Okay. These are effect sizes.

444
00:45:28,540 --> 00:45:37,480
These are standardized. These are comparable. And the ratings say the confidence intervals.

445
00:45:37,540 --> 00:45:44,259
What is that? Whatever it was. What are we looking for in a confidence interval that anybody remember or anybody know with odds ratios?

446
00:45:44,260 --> 00:45:47,440
It's not zero. Is the hint? What is a null effect?

447
00:45:47,650 --> 00:45:54,400
One odds ratios greater than one or positive less than one are negative associations.

448
00:45:55,000 --> 00:45:57,160
Odds ratios of one. No association.

449
00:45:57,820 --> 00:46:06,280
So with our confidence intervals, we're looking for any sort of any interval that contains one which suggests that there's no effect, basically zero.

450
00:46:09,020 --> 00:46:12,180
So this one should contain one. Right.

451
00:46:12,230 --> 00:46:16,160
It's minus. So this goes over 2.24 through 1.39.

452
00:46:16,340 --> 00:46:20,750
Not significant. This one is all over 11.46, up to 2.4.

453
00:46:21,110 --> 00:46:25,190
So it gives us an idea of the range of the effect, and we're looking to see if it contains one.

454
00:46:27,020 --> 00:46:32,149
This is we're all in. So in that slide, we saw that no deviance.

455
00:46:32,150 --> 00:46:39,860
This was what we got from the very first model. And we saw the change by adding these new predictors went down to ten, ten, 15.3.

456
00:46:40,340 --> 00:46:45,530
We literally subtract the new value from the old value from the null.

457
00:46:46,460 --> 00:46:49,130
We see that that value is 74.6.

458
00:46:49,580 --> 00:46:56,540
We literally subtract the changes in the degrees of freedom, and then we can plug these two values into a chi square test.

459
00:46:56,990 --> 00:47:01,760
And that will tell us if it's the null hypothesis of that test is these models

460
00:47:01,760 --> 00:47:08,360
fit the same based on this value of seven 74.6 with four degrees of freedom.

461
00:47:09,020 --> 00:47:12,050
The computer is going to tell us, do these models fit the same?

462
00:47:19,100 --> 00:47:23,720
Very good. You don't go home. Have a good weekend.

463
00:47:24,590 --> 00:47:31,190
Whatever you do, you're in for questioning tomorrow.

464
00:47:32,030 --> 00:47:35,479
Yeah, yeah. No, I'm. I guess shoot me nuts.

465
00:47:35,480 --> 00:47:39,049
If you have questions about the midterm Monday, I should also be around.

466
00:47:39,050 --> 00:47:43,820
But don't be sending it to me like 7:00 at night. I'll do my best.

467
00:47:43,820 --> 00:47:47,870
But yeah, yeah. The sooner you get there, the better.

468
00:47:48,590 --> 00:47:52,070
But for third four, next. Oh, well, next Tuesday.

469
00:47:54,350 --> 00:48:02,930
Next Tuesday, we can continue this example and then give some folks time to practice a logistic regression on your own.

470
00:48:03,140 --> 00:48:09,860
So then in the next continuous example, what we're going to do is add one more variable and show you how you can now take this

471
00:48:09,860 --> 00:48:14,180
model one compared to one that has additional predictors and does that for the data better.

472
00:48:17,710 --> 00:48:18,740
Yeah. Yeah.

