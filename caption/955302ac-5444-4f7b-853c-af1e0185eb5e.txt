1
00:00:09,830 --> 00:00:17,610
The Introduction video for this particular unit. I talked about the story of kids knowing who to ask in order to get the answer they want.

2
00:00:18,270 --> 00:00:28,170
In epidemiology, we're obviously trying to get the answer of the full population and not distort our answer based on what we think should be there.

3
00:00:28,980 --> 00:00:39,840
Unfortunately, the way in which we select or recruit people can sometimes introduce this type of selection bias without people really realizing it.

4
00:00:40,830 --> 00:00:47,910
In this particular video, what I'm going to talk through are common sources of selection bias so that you'll know how to recognize them.

5
00:00:48,660 --> 00:00:54,570
We'll also practice predicting the directionality of selection bias when these common situations occur.

6
00:00:56,330 --> 00:01:01,640
So the three that I'm going to focus on today are inappropriate selection of participants.

7
00:01:02,060 --> 00:01:05,780
Selective loss to follow up and self-selection bias.

8
00:01:07,510 --> 00:01:16,000
So let's start out revisiting our old example of whether or not motorcycle helmet reduced serious injury among people in crashes.

9
00:01:17,740 --> 00:01:25,960
You recall that the problem was that we did not sample people with serious injuries that led to death, in our case population.

10
00:01:26,590 --> 00:01:31,120
And because helmet use also reduced the severity of injuries in a crash,

11
00:01:31,540 --> 00:01:37,810
we had relationships of our selection process to both our exposure and our outcome.

12
00:01:39,420 --> 00:01:49,770
This selection bias was introduced by missing people in cells A and cell C, especially people in cell C,

13
00:01:49,770 --> 00:01:54,810
because those are the ones who are not wearing helmets and who also had a serious injury.

14
00:01:56,040 --> 00:02:08,190
As you recall, people who are in cell C are part of the population that supports protective associations between the exposure and the outcome.

15
00:02:09,090 --> 00:02:16,770
By missing a huge fraction of that, what we are doing is actually inflating the true association or really blunting

16
00:02:16,770 --> 00:02:21,120
the protective association that we expect to see in this particular scenario.

17
00:02:22,860 --> 00:02:29,820
Now, that was an example of how we can have problems through the selection of our cases in a case control study.

18
00:02:30,630 --> 00:02:38,160
Even more commonly, we often have issues with selection bias when we select our controls for study.

19
00:02:39,120 --> 00:02:46,770
Now let's take a scientific question of looking at whether or not alcohol use is related with more diabetic shock.

20
00:02:47,370 --> 00:02:51,210
Again, imagine that we're going to conduct a case control study.

21
00:02:52,080 --> 00:03:00,629
We first go to the E.R. and we select people who've been admitted for diabetic shock as our cases investigators might think.

22
00:03:00,630 --> 00:03:06,660
Well, how are we going to get people who are similar to those cases but without diabetic shock?

23
00:03:07,170 --> 00:03:12,479
Well, it sort of makes sense that people who are admitted to the hospital and people admitted to the

24
00:03:12,480 --> 00:03:20,340
emergency room without any relationship to diabetes should probably be fine to use as controls,

25
00:03:20,340 --> 00:03:29,220
right? Let's think, for example, about people who've been admitted to the emergency room with injuries, falls, car crashes, things like that.

26
00:03:30,960 --> 00:03:35,280
On first glance, it seems like this would be a good control population.

27
00:03:35,970 --> 00:03:46,650
However, this can introduce problems because patients in the emergency room with injuries are also more likely to have alcohol

28
00:03:46,650 --> 00:03:55,170
exposure than the general population because of car crashes and falls and other things that happen with alcohol use.

29
00:03:57,020 --> 00:04:04,040
Therefore, the way in which we have sampled controls from the emergency room is likely to

30
00:04:04,040 --> 00:04:10,160
introduce more exposure in that population than in the general population.

31
00:04:10,610 --> 00:04:18,950
You can see that here by an overestimation of the sampling frequency in Selby as compared to the other cells.

32
00:04:19,880 --> 00:04:27,380
In other words, what we've done is we've oversampled people who are drinkers in our control population.

33
00:04:28,100 --> 00:04:35,690
This provides evidence that drinking would not be associated with diabetic shock because we have so much drinking in our

34
00:04:35,690 --> 00:04:44,390
control population and it results in an underestimation of the true association between drinking and diabetic shock.

35
00:04:44,900 --> 00:04:49,170
Again, if it's more intuitive for you to see in the graphic, you should use that.

36
00:04:49,190 --> 00:04:54,530
But you can also see those estimations in the odds ratios mathematically shown below.

37
00:04:55,490 --> 00:05:04,580
I wanted to raise this particular issue as one of our most common sources of selection bias, and in fact, it even has a name because it is so common.

38
00:05:05,000 --> 00:05:11,700
It's labeled. BERKES And Bias in the next kind of example that I'm going to talk about.

39
00:05:11,720 --> 00:05:17,120
We're looking at something called Lost to follow up. So here, let's look at Lost,

40
00:05:17,120 --> 00:05:24,860
a follow up under a scenario where we're interested in whether or not some new treatment is associated with increased survival.

41
00:05:26,660 --> 00:05:35,510
Now, we could have a problem in this particular type of study if the medication that we're studying makes people have side effects.

42
00:05:35,600 --> 00:05:43,070
Let's imagine that the side effects of that drug impact those with the worse health status the most.

43
00:05:43,490 --> 00:05:48,620
And what that means is that people who have the side effects effectively drop out of our study.

44
00:05:48,620 --> 00:05:52,279
They no longer want to participate. They no longer take those drugs.

45
00:05:52,280 --> 00:05:58,280
They just disappear from our study population. Now, if that occurs, right.

46
00:05:58,280 --> 00:06:06,830
So those people who are on the medicine with a worse survival are most likely to be undercounted in our study.

47
00:06:07,250 --> 00:06:11,900
What we're going to see is an underestimation of people in our cell a.

48
00:06:12,530 --> 00:06:21,470
You can see that here on the graphic again because people who are in cell A both have the exposure and the outcome.

49
00:06:21,980 --> 00:06:27,260
They are the ones who support the fact that our exposure leads to more outcome.

50
00:06:27,590 --> 00:06:30,020
Therefore, by undercounting them,

51
00:06:30,230 --> 00:06:38,360
we are going to get an underestimate of the true association between medication use and death in our sample population.

52
00:06:40,010 --> 00:06:44,239
All right. Let's take one more example. In this particular case,

53
00:06:44,240 --> 00:06:49,100
I want to walk you through what's called self-selection or do that in a scenario where we're

54
00:06:49,100 --> 00:06:55,220
interested in whether or not more pesticide use is associated with greater risks of prostate cancer.

55
00:06:56,750 --> 00:06:58,430
In this particular scenario,

56
00:06:58,430 --> 00:07:05,780
let's imagine that people who are exposed to pesticides are concerned about the potential health effects and so are more likely to participate.

57
00:07:06,500 --> 00:07:13,190
Similarly, those who have a family history of prostate cancer might also be driven to participate,

58
00:07:13,520 --> 00:07:16,640
even if they themselves are unexposed to pesticides.

59
00:07:17,060 --> 00:07:23,800
This scenario makes the unexposed population more likely to get cancer than the general population.

60
00:07:25,480 --> 00:07:32,680
In effect, what we are doing is we're inflating the population that's in cell C, those without exposure,

61
00:07:32,680 --> 00:07:41,800
and yet still likely to get prostate cancer themselves because this inflate cell C meaning those without exposure.

62
00:07:42,070 --> 00:07:51,970
But with prostate cancer, what we're going to wind up doing is underestimating the true association between pesticide use and prostate cancer.

63
00:07:52,780 --> 00:08:00,610
You can again see that either intuitively through those cells or if it helps you to see it mathematically at the bottom of the slide.

64
00:08:02,380 --> 00:08:09,040
I hope that those few examples have helped you to see how selection bias can introduce a different

65
00:08:09,040 --> 00:08:15,010
association in the study population then that what we would have observed in the full population.

66
00:08:15,430 --> 00:08:24,250
Some key takeaways for you are that there's many different causes of selection bias, either due to retention or recruitment into the study.

67
00:08:24,760 --> 00:08:33,130
Biases can make the observed associations appear too large or too small with respect to the true associations in the full population.

68
00:08:33,820 --> 00:08:38,440
And the use of two by two tables can be helpful to predict the directionality, the bias.

69
00:08:39,550 --> 00:08:47,260
Finally, you can see that careful thought is really needed to identify and potentially correct for any

70
00:08:47,260 --> 00:08:52,900
imbalances in our sampling frequencies that occur based on both exposure and our outcome.

