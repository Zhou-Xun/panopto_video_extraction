1
00:00:01,230 --> 00:00:07,260
All right. Good afternoon, everybody. Thanks for coming to the guest lecture by two talented student.

2
00:00:07,260 --> 00:00:14,520
One has been mainly the others. However, I should, and I know that it takes a lot of effort to walk to this building, and I really appreciate that.

3
00:00:14,910 --> 00:00:19,950
And we will try to adjust the final score based on that. And remember our names.

4
00:00:20,400 --> 00:00:23,940
Okay. All right. So let's let's do that first.

5
00:00:24,560 --> 00:00:28,670
First off. Recitals.

6
00:00:30,560 --> 00:00:36,950
Oh, hi, everyone. Thanks a lot for coming hard this holiday.

7
00:00:37,910 --> 00:00:48,020
So I'm going to like talk about some extension of the topic that was covered in this course and this about marginal models.

8
00:00:48,650 --> 00:00:53,299
But specifically, we're going to focus on marginal models with time bearing, covariates.

9
00:00:53,300 --> 00:01:00,620
And there will be some some notes about the causal interpretation on the marginal models.

10
00:01:02,240 --> 00:01:10,219
So I think in the previous lecture gym class already explained there is a key assumption of the marginal model,

11
00:01:10,220 --> 00:01:14,450
which is colorful, contrary, conditional, mean assumption and arguing.

12
00:01:14,450 --> 00:01:22,189
I'm going to describe the situations where when this assumption may be violated and are also going to

13
00:01:22,190 --> 00:01:29,630
talk about how and when we can correctly interpret regression coefficients in the marginal model.

14
00:01:32,900 --> 00:01:36,650
So just to give you a quick recap.

15
00:01:36,920 --> 00:01:45,380
So this marginal model and that she was first proposing the Long and Ziggler 1986 paper.

16
00:01:45,650 --> 00:01:50,290
So it's a distribution free model on repeated measurements.

17
00:01:50,330 --> 00:01:54,920
It's also called a population outreach model because two regression coefficients

18
00:01:55,280 --> 00:02:01,310
has a population average interpretation as compared to the conditional model,

19
00:02:01,910 --> 00:02:07,250
where you have to do a conditional on the later random effects for each individual.

20
00:02:07,610 --> 00:02:17,930
So this picture was taken actually here at U Mesh about that I think in like three years ago, like during the rally or lectureship.

21
00:02:17,960 --> 00:02:31,520
So Scott Z girl was invited to give a lecture and so his peers like my academic grandpa because his eyes are off by the visor and.

22
00:02:31,910 --> 00:02:38,820
Yeah. So just to give you a quick review of the marginal model.

23
00:02:40,410 --> 00:02:47,219
So we're considering a and I by one letter of outcome of each individual I and

24
00:02:47,220 --> 00:02:56,820
this is a vector y and the covariates we have a and i times p dimensional matrix

25
00:02:56,820 --> 00:03:04,469
of covariates of each individual and where each exile j denote for the chase for

26
00:03:04,470 --> 00:03:11,040
the outcome of the chase location is a p dimensional covariate denoted as x.

27
00:03:11,340 --> 00:03:17,280
I want to excite j p and the marginal model has three key components.

28
00:03:17,490 --> 00:03:29,969
The first one is the mean model, where it assumes that the surrounding function of the conditional mean the conditional mean of y j given

29
00:03:29,970 --> 00:03:39,690
x how j is parameterized by a linear function which is x transpose times coefficient beta and for

30
00:03:39,690 --> 00:03:45,819
the variance assumption it assumes that the condition of our ends can be captured by some parameter

31
00:03:45,820 --> 00:03:53,910
of five and the function of the mean new and also requires to model for within subject covariance,

32
00:03:53,910 --> 00:03:58,170
which is the correlation structure we typically talk about in this course.

33
00:03:59,100 --> 00:04:10,380
So this can be a function of the mean, the mean at the gate and k time points and also an additional vector of parameters alpha.

34
00:04:10,980 --> 00:04:13,620
So the beta here in the first component,

35
00:04:14,370 --> 00:04:21,810
the regression coefficient has a population average interpretation and thus white of this model is a marginal model.

36
00:04:24,000 --> 00:04:28,680
So for it to make inference on a marginal model,

37
00:04:28,830 --> 00:04:37,290
we can estimate a coefficient and the like using this generalize estimate equation

38
00:04:37,560 --> 00:04:44,190
which which targets to minimize the following objective function to solve for theta.

39
00:04:44,940 --> 00:04:48,090
So these this list is also like, you know,

40
00:04:48,150 --> 00:04:56,490
I think in lecture nine or ten and I'm not going to explain like all the notation here because this just a quick overview, quick review.

41
00:04:58,660 --> 00:05:04,880
She should. This is.

42
00:05:05,880 --> 00:05:09,770
This is sound. Can everybody hear me? Okay, good.

43
00:05:10,700 --> 00:05:14,479
So why do we want to use marginal models?

44
00:05:14,480 --> 00:05:17,680
Because they have so or it has their advantages.

45
00:05:19,250 --> 00:05:27,710
The marginal, almost like level three. So we don't have to worry about how we parameter or how we captured the data generating process.

46
00:05:28,280 --> 00:05:34,729
And in addition, like in a marginal model framework, in fact,

47
00:05:34,730 --> 00:05:45,420
any generative model that satisfy the three specification requirements that we just talk about in the previous slide can be estimated at using G.

48
00:05:46,250 --> 00:05:57,420
So G is just a method for estimating the coefficients in a in the model that satisfied the marginal model specifications.

49
00:05:57,950 --> 00:06:08,420
So for example, if we consider a random intercept model here where Y equals zero transpose times fadeout

50
00:06:08,690 --> 00:06:14,780
plus a random intercept by at some individual and some residual that has mean zero.

51
00:06:15,200 --> 00:06:27,090
Then we can check that this conditional mean if the conditional expectation of x given exi actually equals to the conditional expectation of exact j,

52
00:06:27,170 --> 00:06:31,730
given exact j which occurs here x transpose taxpayer.

53
00:06:32,120 --> 00:06:38,260
And we may actually estimate this data using general using G.

54
00:06:38,840 --> 00:06:50,810
And that's the advantage of G is that it's a very fast and flexible way to produce a consistent estimate of the regression coefficients.

55
00:06:52,350 --> 00:06:56,850
And of course there are some disadvantages of the marginal model.

56
00:06:57,900 --> 00:07:07,980
The first most significant disadvantage is that the main requirement essentially of first requirement can be hard to satisfy in a real data analysis.

57
00:07:08,340 --> 00:07:16,560
And the second disadvantage is that the marginal model is actually a little bit and big ambiguous about conditioning.

58
00:07:16,710 --> 00:07:19,320
And of course, there are others, but I'm going to.

59
00:07:20,460 --> 00:07:28,230
So in the following size, I'm going to specifically talk about the first disadvantage and are also briefly mentioned the second disadvantage.

60
00:07:29,810 --> 00:07:33,510
So. As we mentioned before,

61
00:07:33,510 --> 00:07:43,830
the marginal model has to fork over a conditional model assumption where we assume that the conditional mean of the response

62
00:07:44,040 --> 00:07:52,620
given or given the covariates measure at all to all the occasions only depends on the covariate measure at the j occasions.

63
00:07:53,010 --> 00:08:02,460
So essentially it assumes that the conditional is back taken off by j given all the exercise from one to excite and i.

64
00:08:04,440 --> 00:08:14,459
You're close to the conditional expectation of what i j given only x out j so we can like job or the x i i case where it k doesn't.

65
00:08:14,460 --> 00:08:29,310
You quoted j. And the marginal model actually also has a conditional covariance assumption because we assumed that the within subject correlation can

66
00:08:29,310 --> 00:08:40,469
be captured through a function of some parameter alpha and maybe also some maybe also a function of the mean measured at occasions.

67
00:08:40,470 --> 00:08:46,170
J On K the input so implicit we have assumed that the pairwise correlation

68
00:08:46,170 --> 00:08:53,219
between the outcome at a j and the K occasions only depends on the covariates.

69
00:08:53,220 --> 00:08:54,570
And these two occasions,

70
00:08:54,870 --> 00:09:06,780
which is essentially means that a correlation between the Y gen like you when you close to the correlation between y gen y i k given Excedrin exec.

71
00:09:07,350 --> 00:09:11,980
But if you are like paying enough attention,

72
00:09:12,000 --> 00:09:23,309
you may notice that in the previous studies where we have assumed that the conditional mean only depends on the covariance measure at occasion.

73
00:09:23,310 --> 00:09:34,440
J So what does this correlation actually mean when a mean model is conditional on a different sets of covariates than the correlation model?

74
00:09:34,800 --> 00:09:40,350
So if you have taken like the values that eight 4842 which is the,

75
00:09:40,410 --> 00:09:49,500
I think the seminal papers in statistics taught by RAD and in that class, he's going to suggest that when you are formulating models.

76
00:09:51,160 --> 00:09:56,230
Be sure to be very careful about what you're actually conditioning upon.

77
00:09:59,560 --> 00:10:04,240
So but now I'm going to only talk about the FCC assumption here.

78
00:10:05,200 --> 00:10:12,880
Again, this assumption assumes that the conditional mean of a fly on it depends on the covariates, measure and education.

79
00:10:14,080 --> 00:10:25,629
So. So now what if? So if the x arch is our time invariant, which means that at all occasions this year would take the same value.

80
00:10:25,630 --> 00:10:31,060
For example, your gender, it's going to be the same at all time points.

81
00:10:31,750 --> 00:10:39,850
If this extra is its time invariant, then this assumption is easily satisfied.

82
00:10:41,150 --> 00:10:48,020
And another situation is that if X how the searches are Tom varying coverage.

83
00:10:48,740 --> 00:10:51,920
But there are also two situations.

84
00:10:52,250 --> 00:10:57,560
The first situations is that it can be time varying but it's it is fixed by study design.

85
00:10:58,010 --> 00:11:05,210
For example, light if say a treatment group indicator in a crossover child,

86
00:11:05,660 --> 00:11:15,650
the x the value of the z Jews take different value at different time points, but they are pre defined late in the study design.

87
00:11:15,980 --> 00:11:22,500
So the values of these JS vary in a manner all related to the longitudinal outcomes.

88
00:11:22,880 --> 00:11:27,950
So in this case, this FCC m assumption is also satisfied.

89
00:11:27,980 --> 00:11:39,230
So this is okay, but if this z j very, very randomly or at home, this would get much more complicated.

90
00:11:39,230 --> 00:11:49,460
And we're going to talk about this. So let me give you an example when this assumption may be violated.

91
00:11:50,060 --> 00:11:57,020
So so this is when the the values of the trees are governed by a random mechanism.

92
00:11:57,140 --> 00:12:04,370
And in this case, Discovery's cart is stochastic covariate, and the system assumption may be violated.

93
00:12:04,760 --> 00:12:10,550
For example, if we consider an observational study of diabetes, that diabetes.

94
00:12:11,720 --> 00:12:16,070
So we measure the physical activity of these.

95
00:12:17,600 --> 00:12:21,649
So we observe the physical activity levels of the individuals.

96
00:12:21,650 --> 00:12:26,389
At. At. Many different occasions.

97
00:12:26,390 --> 00:12:32,390
And we also observe the blood glucose levels of these individuals at these occasions.

98
00:12:32,750 --> 00:12:41,240
So our goal here is to determine the relationship between the physical activity and our outcome, which is blood glucose level.

99
00:12:42,020 --> 00:12:49,040
So now if we further suppose that the subjects in our study,

100
00:12:49,760 --> 00:13:01,790
the subjects with high or elevated blood glucose level at at pump j, which means that their their why OJ has high value.

101
00:13:02,150 --> 00:13:07,400
These people subsequently realized that I have very high level.

102
00:13:07,400 --> 00:13:10,100
I need to exercise more to get healthier.

103
00:13:10,460 --> 00:13:20,210
So then at the next timepoint they increase their activity level so they are x i j plus one picks higher value then than before.

104
00:13:20,960 --> 00:13:29,720
And on the other hand, subjects who have normal glucose levels will just like keep their usual activity level because they are already pretty healthy.

105
00:13:29,720 --> 00:13:33,620
I don't think they don't really care about that. So in this case,

106
00:13:34,220 --> 00:13:43,520
this assumption would not hold because the value of our the blood glucose level at time j actually

107
00:13:44,090 --> 00:13:53,210
can predict the amount of physical activity at a next time point given the current coverage.

108
00:13:53,600 --> 00:14:03,650
Because if I have if I have high y y values at home J, then I'm going to have high,

109
00:14:04,820 --> 00:14:14,420
high value for J plus one, which means that YJ can predict X plus one.

110
00:14:15,050 --> 00:14:24,500
And therefore, if the conditional expectation of y i j given how j and zero plus one does not equal to the conditional expectation of x.

111
00:14:25,310 --> 00:14:28,580
And in this case, this assumption does not hold.

112
00:14:30,930 --> 00:14:34,740
So what's the consequence of violating this assumption?

113
00:14:35,280 --> 00:14:43,499
So if so, we call it out. We have a working on LEGO working correlation structure for the within.

114
00:14:43,500 --> 00:14:53,280
Such a correlation. So if in the margin in the margin model, if we let the working correlation be known diagonal,

115
00:14:53,490 --> 00:14:58,650
then the g estimate of the regression parameters may be biased and inconsistent.

116
00:14:59,040 --> 00:15:03,750
But on the other hand, the good news is that if the working correlation is diagonal,

117
00:15:03,990 --> 00:15:12,750
which means that we assume that later on within subject observations are conditional independent upon the converse,

118
00:15:13,080 --> 00:15:16,980
then the g estimates of the regression coefficient is still consistent.

119
00:15:20,120 --> 00:15:30,170
The lie. Even if this assumption hos there we are, we will face like a lot of issues with the interpretation of the regression parameters.

120
00:15:30,710 --> 00:15:38,210
In particular, this data may not have to cause the interpretation that we want it to be so.

121
00:15:39,650 --> 00:15:44,570
So now so so again, in this assumption,

122
00:15:45,260 --> 00:15:57,709
each coverage can be either a coverage measure at a jth occasion or a covariate measure at or preceding and preceding the JTH occasion,

123
00:15:57,710 --> 00:16:02,450
for example, like a cumulative exposure up to the JTH occasion.

124
00:16:03,310 --> 00:16:15,440
So and in the following, I'm going to focus on the second case and also going to give an example where the exact JS measure as a cumulative exposure.

125
00:16:16,040 --> 00:16:22,700
So now I'm going to introduce a new notation which is like X bar in YJ bar.

126
00:16:22,970 --> 00:16:31,030
So these, these notation means like a, like a history of a coverage up to a J.

127
00:16:31,580 --> 00:16:38,640
So it's essentially Z part you close to x high one X to up to X out.

128
00:16:39,110 --> 00:16:43,220
And similarly, you have like a history of outcomes. Do you notice what i j.

129
00:16:44,000 --> 00:16:47,780
So this is like YJ one what y one up to YJ.

130
00:16:49,370 --> 00:17:01,720
So if we view the z j bar sc covariate, then the FCC assumption of the marginal model becomes the conditional expectation of y j.

131
00:17:01,730 --> 00:17:16,430
Given all the x idea or the exact JS depends only on the history up to occasion j which is like the conditional citation of y jake x high j bar.

132
00:17:16,850 --> 00:17:24,470
So if j. So if this covariate stochastic, then this covariate history is also a stochastic covariate.

133
00:17:26,760 --> 00:17:30,630
So. So again, back to the blood glucose example.

134
00:17:30,840 --> 00:17:38,700
Suppose that we are measuring the blood glucose level for for our study subjects at two occasions.

135
00:17:38,790 --> 00:17:45,360
So why one and why to step back glucose levels at baseline and a follow up location

136
00:17:45,720 --> 00:17:50,730
and the one excitement in those two physical activity level at these two occasions.

137
00:17:51,360 --> 00:18:02,939
So our goal here is to determine the relationship between a cumulative amount of physical activity where we have a new covariate denoted as x I start,

138
00:18:02,940 --> 00:18:07,020
which is the sum of excellent x how 1x2.

139
00:18:08,010 --> 00:18:13,370
And our outcome is going to be the cost level at a completion of the study, which is why.

140
00:18:13,380 --> 00:18:21,510
Two. So if we use a marginal model as the following where we assume that the conditional expectation

141
00:18:21,510 --> 00:18:28,920
of Y to give an excise star because you've been a 1% beta two times X how excise star.

142
00:18:29,670 --> 00:18:40,620
So what is the interpretation of beta two here? So so out of at the first side you might guess so.

143
00:18:41,300 --> 00:18:46,370
Better to close to can be a written assertion as this difference.

144
00:18:46,640 --> 00:18:58,760
Where is the difference between the conditional mean of y to give an x star equals to x plus one minus x y start equals to x equals to x.

145
00:18:59,240 --> 00:19:02,209
So if this is the case,

146
00:19:02,210 --> 00:19:12,320
we're going to say like beta two captures the effect of a unit increase in a cumulative amount of physical activity on a mean blood glucose level.

147
00:19:12,320 --> 00:19:15,890
A follow up. But is this actually the case?

148
00:19:17,510 --> 00:19:22,730
So. So if this is life, we're only talking about the association, then.

149
00:19:22,940 --> 00:19:26,690
Yes, of course. Is this correct? But it's not a cult.

150
00:19:27,140 --> 00:19:30,260
It doesn't carry over a causal interpretation.

151
00:19:32,740 --> 00:19:40,450
The reason is that the blood glucose level at our baseline may confound the relationship

152
00:19:40,450 --> 00:19:46,300
between the blood glucose level at a follow up and our covariate of interest.

153
00:19:46,720 --> 00:19:54,550
So here confounding loosely means that y one, which is the blood glucose level at the baseline,

154
00:19:54,760 --> 00:20:08,350
may distort the association of the scientific interest between our our our interest the outcome follow up and the and our covariate excise star.

155
00:20:10,110 --> 00:20:18,630
So there are two potential situation that confounding may arise in this case, for example.

156
00:20:20,010 --> 00:20:27,089
So given the given the physical activity levels at both baseline and follow up,

157
00:20:27,090 --> 00:20:36,690
which is one side to why two may be predicted by why one, because these are longitudinal responses.

158
00:20:36,840 --> 00:20:40,710
Of course we we may expect that the.

159
00:20:41,630 --> 00:20:47,270
There would be a positive correlation between the longitudinal responses.

160
00:20:47,840 --> 00:20:51,600
So which means that we can predict the Black Caucus level.

161
00:20:51,600 --> 00:20:59,960
A follow up based on a suggested Black Caucus level at baseline and also his or her physical activity level at both occasions.

162
00:21:01,250 --> 00:21:05,150
And the second situation is that given the.

163
00:21:07,210 --> 00:21:11,320
BLOCK given the physical activity level at baseline,

164
00:21:11,950 --> 00:21:21,430
the physical activity level at follow up may actually be predicted by the blood glucose level at at baseline.

165
00:21:21,820 --> 00:21:28,390
So this happen so this happens exactly like in the situation I just described before.

166
00:21:29,450 --> 00:21:37,390
So so if subjects with elevated blood glucose level at baseline increase their physical activity subsequently,

167
00:21:37,660 --> 00:21:42,879
but the subjects with normal blood glucose level at baseline maintain their usual activity

168
00:21:42,880 --> 00:21:49,930
level then actually two maybe predicted by y one given the baseline activity level.

169
00:21:50,880 --> 00:21:52,710
So in both situations.

170
00:21:55,410 --> 00:22:07,350
The Why I won the outcome at the first point actually distorts the relationship between the outcome at the next town point and our coverage.

171
00:22:07,860 --> 00:22:14,220
And therefore this better to does not quantify the causal effect of this exercise.

172
00:22:14,490 --> 00:22:24,030
X I start on y two because it infest effect from what I won on y two and strictly speaking,

173
00:22:24,030 --> 00:22:29,730
if taken light on or having any previous knowledge about, say, causal inference.

174
00:22:30,090 --> 00:22:37,140
So while one is both a confounder and a mediator on a causal path between Y two and exercise star.

175
00:22:39,920 --> 00:22:49,010
So. So. Under what situations is this better to have a valid causal interpretation?

176
00:22:50,120 --> 00:22:55,280
So this actually requires requires some additional assumptions.

177
00:22:55,880 --> 00:23:03,830
If either of these two assumptions is satisfied, then beta beta two will have a valid causal interpretation.

178
00:23:04,340 --> 00:23:11,430
So here the first assumption says that given the covariate history, our outlook,

179
00:23:11,450 --> 00:23:16,790
the outcome, the current outcome is not predicted by the previous outcomes.

180
00:23:18,150 --> 00:23:27,420
Or the second assumption states that given the previous coverage, our current coverage is not predicted by the previous outcome.

181
00:23:27,720 --> 00:23:37,290
So essentially these two assumptions are just like the negative of our of the previous situation we just talk about.

182
00:23:38,340 --> 00:23:44,909
So. But notice that because we are considering longitudinal data on the first assumption,

183
00:23:44,910 --> 00:23:53,310
light really hovers in longitudinal study because the repeated responses are usually positively correlated given the covariates.

184
00:23:53,460 --> 00:24:00,180
So we will likely expect that our future outcomes can be predicted by our previous outcomes.

185
00:24:00,780 --> 00:24:09,360
And therefore in a longitudinal study, the causal interpretation primarily relies on the validity of the second assumption.

186
00:24:09,660 --> 00:24:22,170
So when and in fact there is a term for this covariate when the second assumption hos and this so late and this if

187
00:24:22,170 --> 00:24:29,880
this assumption host that is covariates called external or external combat with respect to the response variable,

188
00:24:30,120 --> 00:24:42,120
which essentially says that like my future covariate value cannot be predicted by my previous outcomes given all my covariate history.

189
00:24:44,550 --> 00:24:53,210
So so so privacy. You have consider like an external covariate when there are only two timepoints.

190
00:24:54,690 --> 00:24:57,900
Now I'm going to like give you a.

191
00:24:58,820 --> 00:25:02,600
Like a external covariate when there are multiple timepoint.

192
00:25:03,350 --> 00:25:10,850
So formerly a covariance external ah is also called exogenous if this equation hosts

193
00:25:11,090 --> 00:25:18,080
so this essentially asset my next outcome the distribution of my next outcome.

194
00:25:18,110 --> 00:25:21,439
Given all my covariate history and my outcome.

195
00:25:21,440 --> 00:25:32,040
History. Only it depends on the cover of history, which essentially means that the outcome here can be jump from this distribution.

196
00:25:32,930 --> 00:25:35,780
So let me give you a quick example here.

197
00:25:36,200 --> 00:25:45,830
So suppose that we are considering a study of air pollution on children's lung function growth, then the outdoor levels of the air pollutants.

198
00:25:46,040 --> 00:25:52,790
So this is a so this is obviously stochastic because you cannot fix it by any study design,

199
00:25:53,870 --> 00:25:59,660
but like conditional on the past values of these levels of air pollutants,

200
00:25:59,870 --> 00:26:08,030
the future values are not predicted by the lung function responses of the subjects and therefore it is external coverage assumption.

201
00:26:10,760 --> 00:26:17,120
However, the children's personal exposure to air pollution is not an external coverage

202
00:26:17,800 --> 00:26:26,270
is if the children's poor lung function and growth subsequently altered light,

203
00:26:26,330 --> 00:26:33,350
changed your daily activity behavior in order to avoid exposure to a high level of air pollution.

204
00:26:33,620 --> 00:26:39,440
And in this case, the children's personal exposure is not external.

205
00:26:41,980 --> 00:26:45,250
So how do we chat with it? Covariates.

206
00:26:45,820 --> 00:26:49,690
Covariates, external. So this is pretty simple.

207
00:26:50,830 --> 00:26:56,020
So essentially we want to chat this equation so we can actually like just perform a simple

208
00:26:56,260 --> 00:27:01,960
so linear regression if you have like continuous outcome or like continuous coverage.

209
00:27:02,320 --> 00:27:13,480
So we can simply like regress the XY, then covariates and the next time points upon like the covariate history and outcome history.

210
00:27:13,910 --> 00:27:17,230
So here, like all my other.

211
00:27:18,330 --> 00:27:21,990
Regression parameters for the outcome. History are denoted by alpha.

212
00:27:22,260 --> 00:27:31,740
So if all these alpha are jointly zero, then if this assumption if this null hypothesis is rejected,

213
00:27:32,010 --> 00:27:35,250
then the external covariate assumption may not hold.

214
00:27:37,830 --> 00:27:46,710
So. So all we've talk about like the causal interpretation of the regression parameters, but all these,

215
00:27:47,490 --> 00:27:57,300
all the above discussions are actually depen are dependent on the premise of something called a no, a measure confounder.

216
00:27:57,630 --> 00:28:04,970
So this is a very commonly. Very common to see an assumption in causal inference in literature.

217
00:28:05,630 --> 00:28:09,140
But I'm not going to spend more details about it here.

218
00:28:09,380 --> 00:28:13,370
If you're interested, you can look it up in some literature.

219
00:28:13,910 --> 00:28:19,230
So. So when there are potential time dependent confounders,

220
00:28:19,950 --> 00:28:29,400
that causal interpretation of recoveries are definitely much more complicated than simply applying a regression model, as we just shown there.

221
00:28:30,270 --> 00:28:39,690
The reason that we can check the external coverage by doing a regression is because we assume there is no unmeasured confounder.

222
00:28:41,440 --> 00:28:52,239
So if you are interested, you can like go to some reference paper like this 1999 paper by Robbins and they have a more formal

223
00:28:52,240 --> 00:28:59,890
discussion about estimating the causal effect of time coefficient covariance in the in the marginal models.

224
00:29:01,340 --> 00:29:10,520
So to give you a quick summary, the marginal models increase and we make the FCC m assumption,

225
00:29:10,520 --> 00:29:20,600
which assumes that the conditional expectation of the outcome at time j only depends on the current covariates.

226
00:29:21,170 --> 00:29:27,440
So in marginal models, if this covariates are time varying and stochastic,

227
00:29:27,650 --> 00:29:32,480
we need to be careful about the causal interpretation of this regression parameters.

228
00:29:32,990 --> 00:29:41,740
And in terms of when the progression coefficient can have a causal interpretation, it is under two assumptions.

229
00:29:41,750 --> 00:29:48,740
The first one is no unmeasured confounder, and the second one is that given the current preceding covariates,

230
00:29:49,040 --> 00:29:57,290
the current and preceding responses do not confound the relationship between the covariate and subsequent responses.

231
00:29:57,770 --> 00:30:04,550
And in this case, we have a we may have a valid causal interpretation of the regression coefficients.

232
00:30:06,290 --> 00:30:18,100
So if you're interested, why lie either of those two assumption hold light implies that external covariate may refer to this slice.

233
00:30:18,210 --> 00:30:24,470
I'm not going to talk about this here. So, yeah, my part is finished.

234
00:30:24,590 --> 00:30:52,460
I'm going to get this to. Thank you again for everyone coming.

235
00:30:53,120 --> 00:31:02,630
Let me introduce myself first. My name is Hera and I'm a third year Ph.D. in the department, and this is my fifth year in the department.

236
00:31:03,590 --> 00:31:08,120
And so I'm working with Dr. Jenko and Walter Dempsey.

237
00:31:08,480 --> 00:31:14,840
So my topic today is time varying causal effect estimation in mobile health studies.

238
00:31:15,740 --> 00:31:18,980
So just a little bit background.

239
00:31:20,510 --> 00:31:30,290
So Mon mean, just talk about how to build models and estimate marginal mean and the causal effect is basically built on that concept.

240
00:31:30,560 --> 00:31:36,800
So if we have a really good marginal model, then we will have a better understanding of the causal effect.

241
00:31:37,190 --> 00:31:42,140
And of course, mobile health study, as you can imagine, it should be something longitudinal.

242
00:31:42,380 --> 00:31:47,930
So this is my that's why I like I bring this topic here today.

243
00:31:48,590 --> 00:31:53,420
So first, I want to give you a brief introduction on what is mobile health.

244
00:31:53,810 --> 00:32:02,420
So it's pretty self-explanatory. So mobile health refers to the use of tools and platforms for health research and health

245
00:32:02,420 --> 00:32:08,090
care delivery and is also called telemedicine digital health or digital medicine.

246
00:32:08,450 --> 00:32:15,920
So if you actually have a Fitbit or Apple Watch on your hand, then that's what's I'm referring here.

247
00:32:16,220 --> 00:32:24,050
So you have a wearable device and it just records your skin temperature, your heart rate and step counts,

248
00:32:24,440 --> 00:32:28,910
and sometimes it will send you notifications to ask you to walk a little bit more.

249
00:32:29,450 --> 00:32:32,780
And software support for mHealth has greatly improved.

250
00:32:33,050 --> 00:32:40,460
And there are, as what I mentioned, there are Fitbit and Apple Watch and they have their corresponding research platform.

251
00:32:40,760 --> 00:32:45,740
And for Apple Watch is called Researchkit or recently named HealthKit.

252
00:32:46,070 --> 00:32:55,430
And the other one is for Google. So in AMP Health, one thing we're really interested and interested in is called adaptive intervention.

253
00:32:56,000 --> 00:33:01,190
And the more smart version is called Just in Time Adaptive Intervention.

254
00:33:01,520 --> 00:33:11,090
So we're at using contextual data from wearable devices to deliver tailored support to users at the right time and right location.

255
00:33:11,510 --> 00:33:15,200
So that's very ideal and we're working towards that.

256
00:33:15,440 --> 00:33:25,730
So for example, if you have an Apple Watch and when it's raining outside, a smartwatch shouldn't ask you to go outside to exercise.

257
00:33:26,750 --> 00:33:29,540
So that's called Just in time adaptive intervention.

258
00:33:29,720 --> 00:33:35,600
So how we can make the Apple Watch or make any wearable devices that smart to actually know everything?

259
00:33:36,770 --> 00:33:41,120
So that's what I'm going to talk about. Hopefully that's interesting enough for you.

260
00:33:42,740 --> 00:33:52,790
A very popular model or study design called Micro Randomized Trials is introduced to model what happened in the mobile health world.

261
00:33:53,180 --> 00:34:02,720
So as I am, our team consists of a sequence of within subject decision times at which treatment options may be randomly assigned,

262
00:34:03,170 --> 00:34:09,250
and the resulting longitudinal data for any subject can be summarized as such things.

263
00:34:09,260 --> 00:34:19,230
So for everyone you have O, which has all the information stored between two adjacent time point and then you have a a is the action.

264
00:34:19,250 --> 00:34:26,210
So if you know a little bit about reinforcement learning, this notation said is really similar to that.

265
00:34:26,510 --> 00:34:33,740
So you have the O and a just happening or like recording sequentially one after another.

266
00:34:34,490 --> 00:34:41,630
And the notation here we use another letter which is age to denote the history,

267
00:34:41,810 --> 00:34:48,410
which is basically the same thing as what we just mentioned about the over bar notation.

268
00:34:48,770 --> 00:35:00,290
So the age t is said that contains all the information, so including action and also other things you're measured along the way onto time.

269
00:35:00,300 --> 00:35:07,430
T But one thing I want to mention here is the history actually doesn't include current action.

270
00:35:07,730 --> 00:35:14,940
So current action, like everything happen before 80 is called h t.

271
00:35:15,980 --> 00:35:22,640
So for simplicity here, 80, we only assume it's a binary treatment, so you either get a notification or not.

272
00:35:23,210 --> 00:35:33,050
So and a little bit more on this. So the randomization probability which I just introduced will happen at every time point.

273
00:35:33,410 --> 00:35:36,860
So it depends on the complete observed history.

274
00:35:37,580 --> 00:35:41,150
So that's what I'm saying. Like to make the notifications smart enough.

275
00:35:41,660 --> 00:35:48,940
That means like your current action should contain all the previous history into account and actually make a.

276
00:35:49,010 --> 00:35:55,880
Decision. And saying how much of a probability I should be using to do the randomization.

277
00:35:56,420 --> 00:36:02,480
So if weather is actually part of the history information and the weather says it's raining,

278
00:36:02,900 --> 00:36:09,500
then this probability of taking an action to ask you to walk outside should be really, really low.

279
00:36:09,500 --> 00:36:13,010
So this should be small. If that's the saying,

280
00:36:13,010 --> 00:36:23,630
then we can say this policy actually makes sense and the treatment options are designed to impact a proximal or possibly lagged response.

281
00:36:23,900 --> 00:36:31,430
So y t plus delta on the proximal means like immediate happening right after.

282
00:36:31,880 --> 00:36:39,470
So if I send you a notification now, a proximal outcome I'm interested in might be the step count in the like.

283
00:36:39,470 --> 00:36:43,430
Right following the 30 minutes. Right following the notification is randomized.

284
00:36:44,390 --> 00:36:53,810
So in this way, we can actually evaluate whether the notification is helpful in terms of like the proximal outcome you're interested in.

285
00:36:54,560 --> 00:36:58,730
And another thing is called availability. So here is just a side note.

286
00:36:59,420 --> 00:37:06,890
So for mobile notifications, sometimes it's not really safe or useful to send it.

287
00:37:07,070 --> 00:37:09,380
So for example, if a person is driving,

288
00:37:09,710 --> 00:37:17,480
it's probably not a good idea to send a notification on his phone right now because the notification may be distracting.

289
00:37:17,870 --> 00:37:24,290
So this availability measure is happened before the treatment randomization.

290
00:37:24,620 --> 00:37:29,240
So this is the extra piece of information we need to take into account.

291
00:37:30,610 --> 00:37:40,180
So to make it like super mathematical, what is the causal effect we're really interested in to estimate?

292
00:37:40,540 --> 00:37:49,989
So again, all the randomization happens along the line and since we're having like a wearable device actually randomized to send a notification,

293
00:37:49,990 --> 00:37:58,000
it's not very costly. So when you think about a mobile house or Marty, this T actually can be very large.

294
00:37:58,330 --> 00:38:06,070
So for example, some app may actually have like for notification and randomization every day.

295
00:38:06,310 --> 00:38:11,830
And if you observe it for half a year, that's 180 times four.

296
00:38:11,840 --> 00:38:18,460
So it's large. So every time point we have a treatment randomization.

297
00:38:19,150 --> 00:38:26,920
So eight can be zero or one. And right after 80, we observe a proximal outcome white t plus one.

298
00:38:27,250 --> 00:38:33,670
So it's a little bit different from some notations or like just the mummy's notation here.

299
00:38:33,670 --> 00:38:38,110
We kind of like really emphasize what happens before and what happens right after.

300
00:38:38,410 --> 00:38:46,020
So we have this white T plus one. But you can just like think of it as what happens after your treatment at eight.

301
00:38:46,720 --> 00:38:51,070
You observe an outcome that's white plus one.

302
00:38:51,550 --> 00:38:57,260
And also, sometimes we're interested in what happens a little bit like longer afterwards.

303
00:38:57,280 --> 00:39:07,299
So that's why the data kicks in. So sometimes we want to see whether send you a notification in the morning and just like not

304
00:39:07,300 --> 00:39:12,940
sending you at noon and not sending you had afternoon and how it will impact your night outcome.

305
00:39:13,450 --> 00:39:24,460
So that's something I introduced here, but I won't be talking about it too much later because it will just like complicate complicate everything.

306
00:39:25,270 --> 00:39:32,499
So if we have an estimate of this type of causal effect, well, we can answer for scientific question.

307
00:39:32,500 --> 00:39:40,870
What B, what is the effect of delivering a treatment, especially for a company like Apple Watch and Fitbit?

308
00:39:41,140 --> 00:39:50,680
They are really interested in whether pushing notifications will actually bring them any like profit or like more things,

309
00:39:50,680 --> 00:39:57,160
like whether sending a notification will actually improve your physical activity, physical activity or mental health sometimes.

310
00:39:58,600 --> 00:40:10,600
Um, so this is the, uh, the how does it like the formal definition of a causal excursion effect?

311
00:40:10,990 --> 00:40:18,220
And of course, there are assumptions. So under the assumption of positivity, consistency and enjoyment or ability,

312
00:40:18,730 --> 00:40:24,010
we have the causal excursion effects for a continuous outcome defined as a linear contrast.

313
00:40:24,460 --> 00:40:25,540
And as you can see,

314
00:40:26,770 --> 00:40:36,489
this expectation of Y given A and T is what not being just introduced is a marginal model and the other one is also a marginal ME model.

315
00:40:36,490 --> 00:40:40,990
And the difference is whether at this time point you're treated or not.

316
00:40:41,560 --> 00:40:47,980
And this as t is something you're interested in and you do a conditional expectation.

317
00:40:48,190 --> 00:40:54,240
And that's the thing we want to know. To begin with, we make a parametric model assumption.

318
00:40:54,250 --> 00:40:57,850
Of course, we need to have estimate and confidence interval, right?

319
00:40:57,910 --> 00:41:02,950
And this beta t as can be modeled as a linear model.

320
00:41:03,190 --> 00:41:09,340
And here the linear what I mean about linear is not a straight line, it's just linear in coefficient.

321
00:41:09,640 --> 00:41:15,459
So this av-test you can actually take, for example, one square.

322
00:41:15,460 --> 00:41:20,110
So technically it's a quadratic function, but it's linear in coefficient.

323
00:41:20,710 --> 00:41:27,400
For example, if this s t is day in study and I only include the first order term, so is one.

324
00:41:27,400 --> 00:41:37,690
And as t the thing I can estimate here is how the causal effect actually varies along the time linearly.

325
00:41:37,900 --> 00:41:42,070
So I can get a straight line. That's how this works.

326
00:41:42,070 --> 00:41:45,880
And our goal now is really just to estimate what beta is.

327
00:41:46,150 --> 00:41:50,059
Beta Star is. Here.

328
00:41:50,060 --> 00:41:55,430
I want to introduce like one of the most popular model or estimation method

329
00:41:55,430 --> 00:42:00,290
we're currently using in mobile health is called Weighted and Central Square.

330
00:42:00,800 --> 00:42:10,370
And for everyone who are currently taking 653, this is pretty much the same with a g objective function.

331
00:42:10,730 --> 00:42:15,530
So you minimize this and then get a consistent estimate of theta.

332
00:42:16,490 --> 00:42:24,380
There are some special things, of course, so but I just want to say this thing you should look like familiar.

333
00:42:25,370 --> 00:42:32,520
So you have the outcome y t plus one and the control variable just added in some other information

334
00:42:32,550 --> 00:42:38,630
you observed to help reduce noise and potentially construct the more powerful test statistics.

335
00:42:39,200 --> 00:42:42,919
And the action is centered by a propensity score.

336
00:42:42,920 --> 00:42:50,270
If you have heard this name before and then the FTSE T theta is the causal part we're interested in.

337
00:42:50,270 --> 00:42:55,550
So the model assumption I just introduced and our goal is minimizing this and get a beta.

338
00:42:56,690 --> 00:43:00,620
So P and here is an operator denoting the sample average.

339
00:43:00,620 --> 00:43:05,090
So it's just right like this. So it's kind of comparable to an expectation.

340
00:43:05,390 --> 00:43:14,720
So it's really just short for one over n sigma I from one to n, so it's calculating for everyone and then taking an average.

341
00:43:15,500 --> 00:43:21,170
And this w t here I want to mention a little bit more because this is special.

342
00:43:21,650 --> 00:43:28,880
So as Momi just said, if we want to actually have a causal interpretation of the coefficient,

343
00:43:29,210 --> 00:43:39,140
we need to satisfy to like either one of those to our assumptions which just recall, I just copy paste here.

344
00:43:39,410 --> 00:43:44,420
So the first one she just said longitudinal data barely hold.

345
00:43:44,420 --> 00:43:53,900
So we just check the second one. So the second one is saying future covariate cannot be predict by previous outcome.

346
00:43:54,470 --> 00:44:04,730
But what we have here about 88 is future covariate and h t actually contains previous outcome.

347
00:44:05,390 --> 00:44:10,879
So how should we deal with this and then still make our beta actually has a causal

348
00:44:10,880 --> 00:44:16,610
interpretation and the solution is just using this weight and if you are interested,

349
00:44:16,610 --> 00:44:23,810
you can actually just change this to an expectation and write out conditioning on a course to one and zero.

350
00:44:24,170 --> 00:44:32,990
And you can see this W2, you actually transfer the policy from depending on the whole history to only depending on s t,

351
00:44:33,230 --> 00:44:36,260
which actually doesn't include the previous outcome anymore.

352
00:44:36,800 --> 00:44:42,980
So in this way, we guarantee the beta, we estimate it actually has a causal interpretation.

353
00:44:44,590 --> 00:44:46,990
So I hope that's not too complicated.

354
00:44:48,520 --> 00:44:56,920
So the previous method WC allows actually construct a linear contrast treated minus on treated, which is pretty straightforward.

355
00:44:57,310 --> 00:45:04,960
But that's only for a continuous outcome. Continuous outcome, including like your step count, your heart rate, or some other things.

356
00:45:05,560 --> 00:45:16,389
But sometimes you have a binary outcome. So if you're interested in like step counts, increase or decrease, that's like two directions.

357
00:45:16,390 --> 00:45:19,430
You can code it as one zero at this time.

358
00:45:19,460 --> 00:45:23,470
A linear contrast wouldn't be that of scientific interest.

359
00:45:23,860 --> 00:45:32,200
And if we write it into a log relative risk, if you take any AP class or I think some biotech class actually have this.

360
00:45:32,620 --> 00:45:38,410
So if you write it into a log relative risk, that's what we are aiming for.

361
00:45:39,430 --> 00:45:47,440
So a consistent estimate here of marginal excursion effect can be obtained by solving the following estimating equation.

362
00:45:47,710 --> 00:45:53,020
And I won't talk too much about it because this is probably really hard to grab.

363
00:45:53,800 --> 00:46:02,260
But just saying the causally excluded effects need to be constructed in terms of what your outcome looks like.

364
00:46:04,480 --> 00:46:10,930
So I will talk about a case study which should be more interesting, I guess.

365
00:46:11,770 --> 00:46:16,390
So this is the my area work I'm currently working on.

366
00:46:16,810 --> 00:46:18,820
So it's about cardiac rehab.

367
00:46:19,150 --> 00:46:29,350
So we have a group of people who are currently in the cardiac rehab program, not currently before they were in, but now they graduated from it.

368
00:46:29,680 --> 00:46:33,160
So first, I want to introduce what is a cardiac rehab.

369
00:46:33,520 --> 00:46:41,950
So cardiac rehab is an evidence based, multidisciplinary secondary prevention program for patients with cardiovascular disease.

370
00:46:42,250 --> 00:46:51,760
So they had their surgery and then they are admitted to a cardiac rehab program and then they graduate from it.

371
00:46:52,030 --> 00:46:53,440
So that's where they're at.

372
00:46:53,950 --> 00:47:04,700
So in this cardiac rehab, they do exercise training and some other cardiac cardiovascular disease risk factor modification.

373
00:47:04,720 --> 00:47:12,940
So part of the education to tell you not to drink too much, not to just sit at one place for the whole day.

374
00:47:13,330 --> 00:47:14,670
Exercise a little bit more.

375
00:47:14,680 --> 00:47:24,880
So education's aspect and cardiac rehab is helpful to reduce mortality and hospital readmission and to improve quality of life.

376
00:47:25,360 --> 00:47:29,840
So this is. What is cardiac rehab?

377
00:47:30,170 --> 00:47:33,540
And there are some concerns. So for those who do.

378
00:47:33,560 --> 00:47:40,790
Participant in cardiac rehab activity levels may be suboptimal, optimal and decline over time.

379
00:47:40,970 --> 00:47:48,110
Once patients graduate from the program, placing them at risk for recurrent cardiovascular disease events.

380
00:47:48,530 --> 00:47:52,850
So once they get out of the program, they're totally on their own.

381
00:47:52,940 --> 00:48:03,200
And they may kind of like start to go back to their bad habits, which increase the risk of cardiovascular events and how we can actually help them.

382
00:48:03,800 --> 00:48:09,050
This is the motivation. So how we can address this gaps in care delivery.

383
00:48:09,500 --> 00:48:14,900
Of course, mobile health and the advantages are as follows, and I believe there are more.

384
00:48:15,140 --> 00:48:18,860
So the first is improve the range of cardiac rehab services.

385
00:48:19,160 --> 00:48:26,000
So if you have a phone and it just keeps telling you to walk a little bit more, eventually you will walk a little bit more, hopefully.

386
00:48:26,420 --> 00:48:31,340
And achieving them, maintaining lifestyle behaviors such as increased the physical activity,

387
00:48:31,670 --> 00:48:37,610
and especially provide unique insight into patients home based physical activity levels,

388
00:48:38,300 --> 00:48:43,550
which is basically the theme for the past three years work activity at home.

389
00:48:43,580 --> 00:48:48,530
So this study is called the Volunteer Study.

390
00:48:48,800 --> 00:48:52,050
So the full name is pretty long like over there.

391
00:48:52,310 --> 00:48:56,420
But it's there is a short abbreviation called Ballantyne Study.

392
00:48:56,870 --> 00:49:01,890
So the goal is you can only you can just read the highlighted part.

393
00:49:01,910 --> 00:49:10,040
So the goal is to test the hypothesis that a smartwatch or notifications could augment physical

394
00:49:10,040 --> 00:49:16,040
activity levels and support patients in maintaining increased physical activity levels over time.

395
00:49:16,460 --> 00:49:23,990
So basically saying like send you notifications could help you improve or at least maintain your physical activity level.

396
00:49:25,130 --> 00:49:28,760
And. The trial design is as follows.

397
00:49:28,760 --> 00:49:40,820
So it is an MD micro randomized trial and participants have a 25% probability of receiving an activity notification at four timepoints every day,

398
00:49:41,210 --> 00:49:47,390
morning, lunch time, mid-afternoon evening. And if you are lucky enough, you will have four notifications on your phone every day.

399
00:49:48,290 --> 00:49:55,520
And the proximal outcome is the step count to 6 minutes after a notification is randomized to deliver.

400
00:49:56,540 --> 00:50:00,140
And because the study is pretty long, it's about six months.

401
00:50:00,440 --> 00:50:04,130
So we kind of just like divide the study into three phases.

402
00:50:04,490 --> 00:50:13,280
So the break down is month, one is phase one months, 2 to 4 is phase two and three months, 5 to 6 is phase three.

403
00:50:14,060 --> 00:50:18,800
And for some reason, there are two types of devices in the study,

404
00:50:19,070 --> 00:50:27,350
which is actually not very helpful in terms of doing analysis because you need to account that Apple Watch and Fitbit actually count.

405
00:50:27,350 --> 00:50:33,410
Step counts differently, but unluckily that's what they did the study.

406
00:50:33,710 --> 00:50:43,790
So again, just to put it into this content, it is the notification which is the step count 60 minute,

407
00:50:43,790 --> 00:50:52,700
one hour after the notification is randomized and we are interested at how a t will impact why t plus one.

408
00:50:54,150 --> 00:51:01,020
And put it here. Again, we're interested in the causal effect and the model we're going to built here.

409
00:51:01,230 --> 00:51:11,310
So the FTSE t actually has one, which is the intercept and device, which is the Apple Watch and Fitbit.

410
00:51:11,760 --> 00:51:14,730
And the face is just what I introduced. One, two, three.

411
00:51:15,270 --> 00:51:25,440
And for The Intercept, we can see marginally how this notification works in terms of improving step counts, right?

412
00:51:25,440 --> 00:51:32,160
Following a notification and device, we can see whether the Fitbit and Apple Watch users behave similarly,

413
00:51:33,060 --> 00:51:37,560
like how they respond to the treatment and face.

414
00:51:37,560 --> 00:51:45,030
We can see if the person reaction to this treatment actually decays or increase over time.

415
00:51:45,450 --> 00:51:51,600
So for every coefficient it has really interesting and useful practical interpretation.

416
00:51:53,190 --> 00:51:58,680
And here is a disclaimer. This is a subset of 30 participants is not everyone.

417
00:51:58,710 --> 00:52:05,910
We have over 100 people. So this doesn't really represent any complete results for the Ballantine analysis.

418
00:52:06,240 --> 00:52:19,020
But I would just introduce some like and interpret some of the coefficient so as what I just said, the notification coefficient here is oh.

419
00:52:19,020 --> 00:52:24,150
So all the step counts are log transformed. So that's why like the coefficient are kind of small.

420
00:52:24,570 --> 00:52:33,300
So the notification here, the coefficient, this is positive and it has significant meaning for Apple Watch users.

421
00:52:34,410 --> 00:52:41,730
The treatment effect is actually significant and the meaning if you have an Apple Watch and they send you a notification along the way,

422
00:52:41,910 --> 00:52:45,390
it actually helpful to improve your physical activity.

423
00:52:45,960 --> 00:52:50,010
And here there are some other things that's pretty interesting.

424
00:52:50,310 --> 00:52:55,410
So this is an interaction between notification and device.

425
00:52:55,740 --> 00:53:06,000
So you can see compared to Apple Watch users, Fitbit users are actually not that like responding to the notification very well and the interaction

426
00:53:06,000 --> 00:53:16,200
between notification and face the coefficients are all negative are both negative and significant meaning.

427
00:53:16,530 --> 00:53:24,600
When time goes by, people tend to just like not care about the notification anymore, which is basically every one of us.

428
00:53:24,750 --> 00:53:30,480
So you just get tired of the notification they are to habitual eyes to the notification.

429
00:53:30,870 --> 00:53:33,870
So I think that's everything I have.

430
00:53:34,020 --> 00:53:41,160
Thank you. All right.

431
00:53:41,210 --> 00:53:50,780
You have any questions? No questions.

432
00:53:54,070 --> 00:53:58,629
All right. Thanks all for the attention and physically being here.

433
00:53:58,630 --> 00:54:05,320
And I think that means a lot to me. I believe and I believe you probably won the same thing in the future of your presenting.

434
00:54:05,320 --> 00:54:11,590
So I think that that speaks this speaks volumes about your sincerity and participate in this class.

435
00:54:11,980 --> 00:54:16,690
And I hope this is the final lecture we will have of this format.

436
00:54:16,690 --> 00:54:21,319
So for next Monday we will be doing team presentations.

437
00:54:21,320 --> 00:54:27,040
So six teams will be there. I will be releasing the presentation order via the email and on Wednesday next

438
00:54:27,040 --> 00:54:31,420
week we will have another set of six teams presenting then the randomized order.

439
00:54:32,290 --> 00:54:35,470
The seed will be the. Will be the.

440
00:54:36,790 --> 00:54:40,540
The day of presentation. So that's hopefully it's going to be fair.

441
00:54:42,130 --> 00:54:47,980
All right. So I'm going to release you all and wish you all a warm evening and see you next Monday.

442
00:54:48,880 --> 00:54:49,930
All right. Thanks.

