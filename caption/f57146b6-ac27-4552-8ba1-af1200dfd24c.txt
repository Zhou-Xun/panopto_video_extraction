1
00:00:02,660 --> 00:00:11,270
Actually, I will say that reading the discussion board, I thought you guys kind of pretty into it for about that stuff.

2
00:00:11,270 --> 00:00:16,490
So I did kind of get the sense that you liked the papers or were interested in the papers.

3
00:00:22,150 --> 00:00:26,370
So tell me what you think, Nick.

4
00:00:33,590 --> 00:00:37,130
So for those of you who are nodding your head who said, yeah, I like to do three things.

5
00:00:37,550 --> 00:00:42,660
What was it that you liked about them? And I'm curious.

6
00:00:44,560 --> 00:00:49,000
Yeah, that's. Well, just like.

7
00:00:49,240 --> 00:00:52,360
What did you what did you find interesting about the reading?

8
00:00:52,810 --> 00:00:54,400
If you found anything interesting about it?

9
00:00:54,700 --> 00:01:02,180
I felt like from the discussion that people kind of got into that I was just reading her more about what you liked about them.

10
00:01:05,140 --> 00:01:09,380
Different models for different concepts. No.

11
00:01:12,330 --> 00:01:17,890
Some of the material was 3600.

12
00:01:18,020 --> 00:01:22,710
Yeah, it was meant to be relevant.

13
00:01:23,550 --> 00:01:27,990
Yeah. Yeah.

14
00:01:28,210 --> 00:01:32,670
So maybe part of it is that you've seen this before. And so then when you see it again,

15
00:01:32,670 --> 00:01:44,639
you get a little more ahead of time and you're seeing that in the context of social epidemiology rather than just as a generic tool.

16
00:01:44,640 --> 00:01:47,790
So like, how would it apply to our research function that you might care about?

17
00:01:48,090 --> 00:01:54,110
Yeah. Anything else that people kind of got out of those readings.

18
00:01:59,660 --> 00:02:05,640
Did you think that there would be any of this stuff? Well, I hope that this is true, and maybe more so by the time we finish today.

19
00:02:05,660 --> 00:02:11,930
But do you feel like you could apply any of this stuff to things that you're working on right now or getting ready to work on?

20
00:02:14,630 --> 00:02:18,590
RB Are you thinking like dissertation or your master's project?

21
00:02:20,270 --> 00:02:27,020
Uh, I have, I have too many friends specifically.

22
00:02:27,020 --> 00:02:31,070
Like one thing, my role is more as like a quantitative researcher,

23
00:02:31,080 --> 00:02:37,190
and it's really helpful to think carefully about the ways that bias can come up and find that research.

24
00:02:37,190 --> 00:02:41,959
And then also like, yeah, yeah, for sure.

25
00:02:41,960 --> 00:02:48,980
And hopefully this will help you. I mean, obviously you're going to be doing a project for this class where you're going to be thinking about a

26
00:02:48,980 --> 00:02:55,370
research question and aims and how you would go about designing a study to do that for your final project.

27
00:02:55,370 --> 00:03:02,220
We'll talk more about that in just a little bit. But for many of you, if you're sticking your master's students home,

28
00:03:02,330 --> 00:03:06,660
then you're getting ready to start your own project, or maybe you've already started it.

29
00:03:06,710 --> 00:03:11,900
And so keeping this kind of thing in mind would be helpful when you get started,

30
00:03:11,900 --> 00:03:22,490
because you'll see how quickly that a set of five variables can become incredibly complicated and

31
00:03:22,490 --> 00:03:29,299
that there can be so many ways that you can analyze the data so that if you don't have something,

32
00:03:29,300 --> 00:03:35,090
some kind of conceptual model or framework going into that analysis,

33
00:03:35,450 --> 00:03:39,440
you'll easily get confused and just start running hundreds and hundreds of models.

34
00:03:40,310 --> 00:03:44,030
We've all done well. I've done it. Maybe you haven't done it yet.

35
00:03:44,030 --> 00:03:48,500
You'll probably do it. Olivia, have you done it? Yes, it happens.

36
00:03:48,890 --> 00:03:54,890
So this can be really helpful for us. All right. So I do want to remind you that next week you have a one paragraph description

37
00:03:54,890 --> 00:04:00,220
of your research proposal topic do this doesn't have to be super fleshed out.

38
00:04:00,260 --> 00:04:06,500
Right? Just think about what is it that you want to spend the semester kind of exploring and thinking about.

39
00:04:07,340 --> 00:04:12,980
And I will give you feedback on that. You can also remember you can meet with me during office hours.

40
00:04:13,730 --> 00:04:18,410
I can do them on Zoom or in the office and we can talk about it.

41
00:04:18,410 --> 00:04:24,290
So Bella and I met last week and talked about her research question and it doesn't take that long.

42
00:04:24,290 --> 00:04:28,790
I mean, I think we spent the 15 minutes. So if you want to do that, feel free to sign up.

43
00:04:29,570 --> 00:04:31,430
It can sometimes be helpful to talk it through.

44
00:04:31,430 --> 00:04:38,419
We'll also use some time at the end of today's class for you to get in your groups and start kind of brainstorming,

45
00:04:38,420 --> 00:04:44,180
research questions and talking to each other about it and getting any questions about that.

46
00:04:48,130 --> 00:04:57,670
All right. I know this isn't the first time that you've heard about correlation versus causation.

47
00:04:57,670 --> 00:05:02,740
Right. But what does it mean to say that correlation does not imply causation?

48
00:05:10,630 --> 00:05:18,960
It's not a trick question. Anna?

49
00:05:19,080 --> 00:05:22,350
Yes. Okay. Now it's just hard to see that.

50
00:05:22,380 --> 00:05:28,400
Cheers. Cover up the ones in the back. So even. We didn't want to go to the other.

51
00:05:30,130 --> 00:05:40,990
Yeah, exactly. So if two things are related to each other or cauvery, it doesn't mean that one necessarily causes the other.

52
00:05:41,740 --> 00:05:50,590
Right. You learned this from your very first statistics class or methods that I'm sure now I'm going to show you some some funny examples,

53
00:05:52,020 --> 00:06:02,500
but I don't know if they're funny. Some of them are a little dark, but this person's put together this website on spurious correlations just to show

54
00:06:02,710 --> 00:06:08,320
how easy it is to find things that are very highly correlated with one another,

55
00:06:08,950 --> 00:06:13,240
but in no way makes sense as like a causal relationship.

56
00:06:14,230 --> 00:06:17,860
So let's scroll down to the margarine one because it's my favorite.

57
00:06:18,820 --> 00:06:23,890
The divorce rate in Maine correlates with per capita consumption of margarine.

58
00:06:24,580 --> 00:06:29,170
Okay, you can see that the correlation coefficient is 0.99.

59
00:06:31,980 --> 00:06:35,070
Could these things be? Do you guys know what margarine is?

60
00:06:37,020 --> 00:06:40,810
It's fake butter. It's like butter made of oil.

61
00:06:40,830 --> 00:06:43,379
No one eats it anymore. It was, like, popular in the eighties.

62
00:06:43,380 --> 00:06:48,570
They used to tell us that it was better for your heart, but it was actually terrible that trans fat anyway.

63
00:06:48,960 --> 00:06:53,460
So these two things absolutely go away.

64
00:06:53,730 --> 00:06:58,080
That one is causing the other. Right, but strongly, strongly correlated.

65
00:06:58,860 --> 00:07:02,130
And so this is obvious, right?

66
00:07:02,520 --> 00:07:07,410
Because you're using logic. You're applying logic and you're saying, no, I'm not buying it.

67
00:07:08,280 --> 00:07:21,000
But when you get into a real data set and you've got hundreds of variables and you run like a correlation matrix on those hundreds of variables,

68
00:07:21,780 --> 00:07:25,890
you're going to find dozens and dozens and dozens of significant correlations.

69
00:07:27,210 --> 00:07:32,460
And you have to figure out what they mean, and they're not going to be as obvious as this.

70
00:07:32,550 --> 00:07:37,080
So you could have a lot of spurious correlations in your data set,

71
00:07:37,470 --> 00:07:43,890
but let's say you're using those correlations to decide what to control for in a model.

72
00:07:45,150 --> 00:07:51,050
People do that right. What's the problem there?

73
00:08:02,740 --> 00:08:07,450
These things may be spurious that you're seeing, and if you start putting them in a model,

74
00:08:07,450 --> 00:08:15,279
it's going to start affecting your results in all kinds of weird ways that you can't necessarily anticipate without

75
00:08:15,280 --> 00:08:23,080
having gone into it with a strong idea of the causal relationships between the variables that you're you're looking at.

76
00:08:23,830 --> 00:08:32,590
And it's not enough to say, Oh, I have a cross-sectional data set, so I can't look at causation anyway.

77
00:08:33,130 --> 00:08:36,370
So I'm just going to look at the correlations, right?

78
00:08:36,370 --> 00:08:40,480
Like I think sometimes we might be tempted to do that. It's still a bad idea.

79
00:08:40,690 --> 00:08:45,129
We'll keep going through why that's a bad idea and what you can do differently.

80
00:08:45,130 --> 00:08:51,250
But I just want to emphasize that point that just looking at correlations isn't

81
00:08:51,250 --> 00:08:54,520
the way you want to go because you'll find lots of things that are correlated,

82
00:08:54,520 --> 00:09:00,610
but there's no causal relationship there. Okay, so why are do we even care?

83
00:09:01,870 --> 00:09:04,870
Why would we care if a relationship is causal?

84
00:09:08,980 --> 00:09:13,720
We can probably publish it even if it's not. So why do we care?

85
00:09:15,810 --> 00:09:21,709
Yeah. Okay.

86
00:09:21,710 --> 00:09:26,170
So we might want to understand how strongly these things are related.

87
00:09:41,110 --> 00:09:48,010
Yeah. So maybe we want to intervene. Maybe our intention is to be able to improve health and we want to intervene.

88
00:09:48,580 --> 00:09:56,740
So if something isn't causally related and we intervene on the exposure or what happens.

89
00:10:02,620 --> 00:10:08,169
Oh, yeah. Like nothing will happen, right? Because it wasn't causally associated with that outcome.

90
00:10:08,170 --> 00:10:14,050
So we can intervene on it, but because it's not the cause, then we're not going to improve the outcome.

91
00:10:14,500 --> 00:10:19,809
And so we want to understand this because exactly.

92
00:10:19,810 --> 00:10:23,469
Because we want to be able to like potentially do interventions.

93
00:10:23,470 --> 00:10:31,000
Or we could be I mean, that's more typical in public health or it could be that we just want to understand the causal nature of something.

94
00:10:31,000 --> 00:10:39,670
Maybe we can't intervene, but we want to understand the true causal relationship between something we don't really care.

95
00:10:39,670 --> 00:10:49,640
And two things are it. So probably in any work that you're going to do, unless it's purely descriptive,

96
00:10:50,900 --> 00:10:55,430
you're probably going into it thinking about it from a causal framework.

97
00:10:56,300 --> 00:11:05,030
You most likely aren't doing rigorous causal analysis in your in your like actual data analysis,

98
00:11:05,030 --> 00:11:08,689
probably because you don't have the available data that you would need or whatever.

99
00:11:08,690 --> 00:11:15,710
You really are probably mostly looking at association. But you're going into it with a causal framework in your mind.

100
00:11:16,130 --> 00:11:19,500
Otherwise, who cares? Right.

101
00:11:20,860 --> 00:11:26,550
Okay. So that leads us to talking about bias.

102
00:11:33,100 --> 00:11:36,280
So I thought it was interesting that in the discussion board there were kind

103
00:11:36,280 --> 00:11:43,590
of two sometimes intersecting conversations going or going on around bias.

104
00:11:43,600 --> 00:11:47,450
And so some people were kind of talking about these different forms of bias,

105
00:11:47,470 --> 00:11:53,200
or those of you who ask questions about what is the specific kind of bias, I probably don't know.

106
00:11:53,200 --> 00:12:00,010
So I serve you. There's some really obscure ones in that article, right?

107
00:12:00,030 --> 00:12:04,149
And if you wanted it, we could we could look it up and try to find out more about those types of bias.

108
00:12:04,150 --> 00:12:15,070
But some people were talking about those kind of epidemiologic biases, like Ferguson's bias or obsequiousness.

109
00:12:15,070 --> 00:12:27,219
Bias, I think was one of the ones that they put in there. And then other people were kind of thinking about investigator bias in a broader sense,

110
00:12:27,220 --> 00:12:33,790
about how we come to the research questions, but we study and how we interpret data.

111
00:12:34,480 --> 00:12:39,610
So I thought that was really interesting that we have these two kinds of bias and but we can think

112
00:12:39,610 --> 00:12:48,130
about them as both being kind of subsets of things that mess up our understanding of something,

113
00:12:48,250 --> 00:12:54,010
right? So if we just say bias and general language, what do we mean?

114
00:12:55,690 --> 00:13:09,610
What is what does it mean to be bias? Yeah.

115
00:13:09,720 --> 00:13:17,180
Yeah. Okay.

116
00:13:17,220 --> 00:13:28,140
So someone might have opinions or a thought process that's influencing their judgment of that.

117
00:13:28,140 --> 00:13:38,910
Kind of like looking at two things and making a determination that's sort of based on what's true versus what they're almost like misperceived.

118
00:13:41,040 --> 00:13:46,929
Does that sound right? Yeah.

119
00:13:46,930 --> 00:13:55,690
So. So a dictionary definition is prejudice in favor of or against one thing person or group compared with another.

120
00:13:56,070 --> 00:14:00,310
You get the two, usually in a way considered to be unfair.

121
00:14:01,180 --> 00:14:12,130
But you can also think of it as like because of this bias, they're not accurately perceiving the like what's happening, right?

122
00:14:13,270 --> 00:14:20,590
And so that's, you know, kind of what bias is and epidemiologic studies as well.

123
00:14:21,010 --> 00:14:34,690
So it's a kind of error. Prevents you from seeing what's really happening.

124
00:14:40,390 --> 00:14:47,770
And so bias which we'll talk about some different forms of bias results in a lack of internal validity.

125
00:14:58,730 --> 00:15:07,760
So if we have bias or not correctly assessing the association between an exposure and an outcome in our target population.

126
00:15:08,570 --> 00:15:17,360
It essentially just means we got it wrong. What we think the association is in the target population.

127
00:15:18,170 --> 00:15:25,120
We got the wrong answer. Does anyone want that to happen to them?

128
00:15:26,470 --> 00:15:30,129
No, we're definitely trying to avoid this.

129
00:15:30,130 --> 00:15:38,890
Right. That's why we care so much about bias. We want to make sure that whatever we're describing, that what we're seeing reflects reality is true.

130
00:15:44,160 --> 00:15:53,700
And so in another reason this is important is because internal validity is necessary for external validity or generalizability.

131
00:15:55,020 --> 00:16:02,220
But of course it's not sufficient for that. But if we don't have internal validity, we can never have external validity.

132
00:16:09,500 --> 00:16:17,649
Does that make sense? Now most.

133
00:16:17,650 --> 00:16:25,060
Unfortunately for us, bias can yield estimates toward or away from the norm.

134
00:16:29,690 --> 00:16:34,190
It would be a whole lot easier if it always went in one direction.

135
00:16:34,190 --> 00:16:44,600
Right. Because we could say, well, if we have bias, we think this estimate is is overly conservative.

136
00:16:46,010 --> 00:16:49,579
We could say that. But we can't always know that.

137
00:16:49,580 --> 00:16:54,020
Right. Because bias can yield estimates toward or away from the norm.

138
00:16:54,560 --> 00:16:59,300
So it can cause us to say that there's not a relationship there when there really is.

139
00:17:00,380 --> 00:17:07,370
Or it can cause us to say that there is a relationship that really isn't there.

140
00:17:08,030 --> 00:17:14,710
But actually both of those are. Did I say the same thing twice? I'll say it again.

141
00:17:15,760 --> 00:17:25,990
It can cause us to say that there is a relationship when there really isn't, or it can cause us to say there's no relationship when there really is.

142
00:17:28,870 --> 00:17:40,169
Both wrong answers, right? And maybe you haven't gotten this far in your projects where you're doing data analysis.

143
00:17:40,170 --> 00:17:42,149
But but think about it for a minute.

144
00:17:42,150 --> 00:17:50,290
Would you rather say that there is an association when there isn't or that there's not an association than there really is?

145
00:17:50,310 --> 00:17:57,959
Do you have a preference? Okay.

146
00:17:57,960 --> 00:18:14,840
Depends on what it is. Say more about that. Yeah.

147
00:18:14,840 --> 00:18:22,970
So maybe if you're studying like let's say you're studying some dietary factor as a risk factor for cancer and you say,

148
00:18:23,840 --> 00:18:28,700
oh, um, I'm thinking about the research on alcohol at home.

149
00:18:28,760 --> 00:18:35,930
Right. Like you say, there's no association between alcohol consumption and cancer.

150
00:18:36,770 --> 00:18:43,000
Okay. So then that could be guidelines to tell people, don't worry about how much you drink.

151
00:18:43,010 --> 00:18:49,730
It's not going to increase your risk for cancer. But if there really is and is a true association there and you say there is,

152
00:18:49,970 --> 00:18:55,560
then now we've missed the opportunity to prevent cancer by telling people that for.

153
00:18:59,460 --> 00:19:07,470
Yeah. On the flip side, you say, like, if you're looking at some sort of innovative intervention and saying that like there

154
00:19:07,470 --> 00:19:10,650
is a relationship between that and before then reflecting on that just right now,

155
00:19:10,920 --> 00:19:16,830
then you're giving people an unnecessary reason. Yeah, absolutely.

156
00:19:16,860 --> 00:19:23,399
Yeah. So like, if you say there is this association between these two things and then and there really isn't,

157
00:19:23,400 --> 00:19:28,680
and you impose this intervention, it can impose hardship on people.

158
00:19:28,680 --> 00:19:33,900
It could actually have unintended consequences that negatively impact their health in some other way.

159
00:19:35,460 --> 00:19:39,670
I mean, I'm thinking about. Gosh.

160
00:19:40,730 --> 00:19:53,139
Well. Some of our interventions with COVID had turned out to not work right.

161
00:19:53,140 --> 00:19:59,200
Remember when playgrounds got shut down? There was fear about people like kids touching playground equipment.

162
00:19:59,230 --> 00:20:04,880
And so, like in my neighborhood, we have a little playground and it literally had like police around go.

163
00:20:05,320 --> 00:20:07,930
The kids couldn't go out there. Right.

164
00:20:07,930 --> 00:20:18,009
And so that didn't prevent people from getting COVID, even though we thought it would and could have these negative impacts,

165
00:20:18,010 --> 00:20:22,030
because it meant that like kids could play, they couldn't be outside.

166
00:20:22,030 --> 00:20:27,100
Like there were these kind of other consequences that came about as a result of that.

167
00:20:28,400 --> 00:20:39,550
Um, so yeah, we sometimes I think researchers would rather make the error of saying there's not an association when there really is,

168
00:20:40,120 --> 00:20:46,360
because that feels more conservative. But both types of errors are problematic.

169
00:20:47,150 --> 00:20:51,990
Alec, did you want to say something? Okay. Isn't this really someone?

170
00:21:02,210 --> 00:21:04,290
Yeah. I mean, it could be. Yeah, sort of. Really.

171
00:21:04,290 --> 00:21:09,629
I was thinking more of like type one here and type two where like you learn about that in statistics.

172
00:21:09,630 --> 00:21:11,640
That's like more of what I was thinking of.

173
00:21:12,960 --> 00:21:18,930
But yeah, you could, you could also think about it in the sense of like a positive or a false negative, which would then be kind of.

174
00:21:27,390 --> 00:21:32,880
Okay. So we're going to talk about like I said, the article talks about millions of kinds of bodies,

175
00:21:34,320 --> 00:21:39,720
but we're going to focus on just a few that are really important in social epidemiology.

176
00:21:40,830 --> 00:21:45,600
So I want to talk about some different kinds of selection bias to start.

177
00:21:46,860 --> 00:21:54,330
And so you have a selection bias when your study population does not represent your target population.

178
00:22:01,200 --> 00:22:07,800
Do you think this is a big issue in epidemiology? Yes.

179
00:22:08,230 --> 00:22:11,270
Yes, yes. Yeah, I'm seeing a lot.

180
00:22:11,510 --> 00:22:14,840
Yes. And the answer is absolutely.

181
00:22:15,730 --> 00:22:22,390
Um, think about the study that we read last week with the cardiac study.

182
00:22:22,400 --> 00:22:31,260
Do you remember that one with the social relationships and how, um, what do you think their target population was?

183
00:22:31,300 --> 00:22:35,660
I know I'm asking you to remember back, but what do you think their target population was?

184
00:22:36,260 --> 00:22:44,020
Who were they trying to make an inference about? Yeah, Ali.

185
00:22:46,710 --> 00:22:55,710
Yeah. It was young at all right. And maybe you could go as far as to say, black and white young adult because they were limited to those groups.

186
00:22:55,740 --> 00:22:58,890
Okay. So that's who they're trying to, like, make appearances about.

187
00:22:59,910 --> 00:23:08,100
Um, do you think that their sample of black and white young adults was representative of the target population?

188
00:23:08,370 --> 00:23:21,080
Why or why not? Yeah.

189
00:23:21,120 --> 00:23:30,299
That's just not. Yeah, exactly.

190
00:23:30,300 --> 00:23:40,310
So that's part of it, right. They sampled from two urban areas in the US for that, for this sub sample that they actually used for their analysis.

191
00:23:40,880 --> 00:23:45,200
So they didn't have people kind of from everywhere. It was from two specific areas.

192
00:23:46,580 --> 00:23:54,890
I don't think they mentioned this in the article, but most of these cardiovascular cohort studies are basically convenience samples.

193
00:23:55,400 --> 00:23:58,790
They like to put up posters and see who signs on.

194
00:23:59,090 --> 00:24:04,340
So even within those two geographic areas that they used, they didn't randomly sample.

195
00:24:04,340 --> 00:24:07,490
They didn't do like random digit dialing or anything like that.

196
00:24:08,000 --> 00:24:13,760
So the people who are in the sample aren't necessarily representative of the target population.

197
00:24:13,760 --> 00:24:20,060
And that's the case for many, many, many studies in epidemiology and other disciplines.

198
00:24:21,320 --> 00:24:26,629
Um, we do have some important examples of studies that are representative.

199
00:24:26,630 --> 00:24:34,910
So if you think about NHANES, the National Health and Nutrition Examination Survey, that is a nationally representative sample.

200
00:24:35,360 --> 00:24:42,950
We have purpose the behavioral risk factors, surveillance system, those are representative of the populations within states.

201
00:24:43,820 --> 00:24:50,780
So in epidemiology we do have some representative studies, but we have a lot more that aren't.

202
00:24:51,950 --> 00:24:56,420
So that's kind of the first the first one,

203
00:24:57,710 --> 00:25:04,550
the first thing that can contribute to selection bias is when we have a nonrandom sample and we have this all the time.

204
00:25:05,750 --> 00:25:12,950
So if we could get a random sample one, why would we choose not to?

205
00:25:20,330 --> 00:25:26,820
Katia. So you have a lot of money? Yeah. Yeah. Okay, so the money could be part of it.

206
00:25:26,840 --> 00:25:30,590
It might be more expensive. And.

207
00:25:30,590 --> 00:25:34,900
Yeah, time. Yeah, yeah, yeah, time.

208
00:25:34,910 --> 00:25:39,550
It's difficult to do. Yeah.

209
00:25:40,060 --> 00:25:43,860
And I was like, Wait till run up something, if you like.

210
00:25:43,890 --> 00:25:49,810
When you sample somebody or whatever, it will be hard to contact them.

211
00:25:50,260 --> 00:25:57,990
Yeah, it can be like very difficult to like people who live in more remote areas.

212
00:25:58,000 --> 00:26:01,629
Either you have to go to them or they have to come to you.

213
00:26:01,630 --> 00:26:08,110
And that can be hard either way. Right. It is much more time consuming.

214
00:26:08,890 --> 00:26:12,460
You know, you have to go to the stuff and you have to have a sampling frame.

215
00:26:13,090 --> 00:26:19,990
Um, you have to recruit people into the study through that, um,

216
00:26:21,040 --> 00:26:27,969
if you want to do things like they did in the cardio study where they're measuring cortisol and they're drawing

217
00:26:27,970 --> 00:26:34,330
blood and they're doing all of these things that can be really challenging to do outside of a clinical setting.

218
00:26:34,540 --> 00:26:39,189
There are some studies that collect biomarkers that are kind of population studies

219
00:26:39,190 --> 00:26:43,149
where they go into people's houses and take blood samples and things like that.

220
00:26:43,150 --> 00:26:52,290
But as you can imagine, that's really challenging because you have to train the people who are doing the data collection and do all that stuff.

221
00:26:52,300 --> 00:26:58,240
You have to have protocols that work in the context of someone's house versus being in a doctor's office.

222
00:26:58,630 --> 00:27:06,000
So there's all sorts of reasons why we would use convenience samples instead of random samples.

223
00:27:06,010 --> 00:27:10,200
But you think back to your statistics, your basic statistics classes.

224
00:27:11,200 --> 00:27:17,620
What is the assumption of all of the models that they teach you how to run everything?

225
00:27:26,890 --> 00:27:33,400
The assumption is that you have a simple random sample, not just a random sample, but a simple random sample.

226
00:27:33,400 --> 00:27:40,840
That was the assumption of all men. And we violate that pretty much 100% of the time.

227
00:27:41,680 --> 00:27:48,310
All right. Another thing that can lead to selection bias.

228
00:27:48,350 --> 00:27:55,680
Maybe you start with a random sample. But then you're following people up over time.

229
00:27:57,420 --> 00:28:03,170
Do people in your study have to stay in your study? No.

230
00:28:03,180 --> 00:28:08,490
Right. We can't make people stay in our studies sometimes that's because they die, right?

231
00:28:08,510 --> 00:28:10,370
So we can have lots of follow up because death,

232
00:28:11,360 --> 00:28:15,590
sometimes it's because they simply don't want to be in your study anymore, because you ask too many questions.

233
00:28:17,000 --> 00:28:23,030
It can be because they moved away and you can't find that. There's all sorts of reasons why we might lose people to follow up.

234
00:28:23,030 --> 00:28:27,830
And then even when we had initially a random sample, we no longer do.

235
00:28:28,070 --> 00:28:33,660
And so this can create bias. Now.

236
00:28:35,200 --> 00:28:40,240
What if people. What if you had 100 people in your sample and you lost ten?

237
00:28:42,190 --> 00:28:47,140
And those ten people that you lost are in no way different than the 90 people that you cut.

238
00:28:48,520 --> 00:28:52,770
Would that be a problem? No.

239
00:28:53,080 --> 00:28:59,410
Right. But are the people who drop out of studies ever the same as the people who stay?

240
00:28:59,680 --> 00:29:05,200
How do you know? So that's what creates the problem there.

241
00:29:07,030 --> 00:29:15,250
Same thing with missing data. So just like we can't make people stay in our studies, we can't make people answer all of our.

242
00:29:19,430 --> 00:29:25,250
It's not uncommon in a survey for 20% of data on income to be missing.

243
00:29:26,570 --> 00:29:30,110
You know, one in five people won't tell you how much they made.

244
00:29:32,810 --> 00:29:35,900
But income is a really important variable in your model.

245
00:29:35,900 --> 00:29:46,420
You've got to control for it. So what do you do? What happens when you put income in your model and 20% of the people didn't answer the question.

246
00:29:56,130 --> 00:30:03,820
Have you tried that before? Yeah.

247
00:30:03,830 --> 00:30:09,810
You might have to impute the data because if you don't, they drop out of your sample, right?

248
00:30:10,490 --> 00:30:19,310
So you automatically lose 20% of your sample when you run a regression model that includes income of 20% of people that answer the question.

249
00:30:20,690 --> 00:30:30,140
And so, again, even if your original sample was a random sample, but 20% of people didn't answer income,

250
00:30:30,290 --> 00:30:35,180
then you're only you're doing a complete case analysis and now you've only got 80% of your sample.

251
00:30:36,740 --> 00:30:45,290
If that data isn't missing at random, which it probably isn't, then you've now got bias.

252
00:30:45,830 --> 00:30:53,000
So you might do something like a multiple imputation to retain those cases in your in your analysis.

253
00:30:59,990 --> 00:31:08,950
And then the last thing that can happen here. We're one of the other things that can happen is non-response bias.

254
00:31:11,430 --> 00:31:16,110
So people don't have to stay in your study. They don't have to answer your question.

255
00:31:16,500 --> 00:31:21,210
And they don't even have to be in your study. They can tell you no.

256
00:31:27,240 --> 00:31:33,210
And when they tell, you know, it messes up the representativeness of your sample.

257
00:31:37,130 --> 00:31:45,390
And so. Some studies I know enhance.

258
00:31:45,390 --> 00:31:46,740
I'm not sure about purpose,

259
00:31:47,400 --> 00:32:01,170
but there we account for non-response bias and things like that to try to make your sample more like the population that it's trying to to represent.

260
00:32:02,010 --> 00:32:11,880
So, for example, uh, men are more likely to refuse to participate in a study than women.

261
00:32:12,720 --> 00:32:19,650
You might have to update your male respondents in order to say that you have representative data that makes sense.

262
00:32:25,250 --> 00:32:30,620
All right. So that selection bias is bad but sort of ever present.

263
00:32:30,620 --> 00:32:33,049
There were there was a lot of discussion, I think,

264
00:32:33,050 --> 00:32:42,230
on the board about can you ever not have bias or there so many types of bias that you can never actually have an unbiased study?

265
00:32:42,860 --> 00:32:50,840
I think the answer to that is absolutely yes. We're never probably going to have no bias in your study.

266
00:32:54,420 --> 00:32:58,950
All right. So another type of bias that's important is information bias.

267
00:33:03,770 --> 00:33:07,400
And so this comes in during data collection.

268
00:33:20,410 --> 00:33:23,290
So we can have bias due to misclassification.

269
00:33:32,140 --> 00:33:42,490
Just meaning that whatever procedure we're using to detect the exposure or detect it to detect the outcome is flawed in some way.

270
00:33:56,010 --> 00:34:02,670
So we could have examples of the classification can include recall by us where

271
00:34:02,970 --> 00:34:09,210
having a disease may influence your recall of possible causes under study.

272
00:34:11,400 --> 00:34:15,990
We can have reporting bias or social desirability.

273
00:34:16,320 --> 00:34:24,490
Somebody who is writing about that in their comments. Andrew Was it you were talking about social desirability bias and your internship that you.

274
00:34:29,430 --> 00:34:39,299
Yeah. But can can you say a little bit about what that might be like when you're collecting data and you might have social desirability bias?

275
00:34:39,300 --> 00:34:56,070
Like, what would that? Yeah, but you make sure they give you the answers that make them look better.

276
00:34:56,640 --> 00:35:01,290
Or they might try to give you the answer that they think you want for their study.

277
00:35:02,850 --> 00:35:08,520
And we don't want that. Right. Because then that creates bias in our study.

278
00:35:10,140 --> 00:35:19,050
And somebody else was talking about how, you know, in a time maybe you as you were you talking about like, okay, go ahead.

279
00:35:20,550 --> 00:35:37,500
Something to. Okay. You're going.

280
00:35:49,000 --> 00:35:59,150
During the past couple days. One of the participants. That's three days.

281
00:36:03,740 --> 00:36:07,270
Right. Part of it is.

282
00:36:17,240 --> 00:36:19,910
Yeah. That's a really interesting point that you had in your study.

283
00:36:19,910 --> 00:36:25,100
There were some things that you could objectively measure, so you're less worried about this.

284
00:36:25,800 --> 00:36:36,160
But they're then when you have enough people to report being themselves that you can objectively measure and you could get into all sorts of.

285
00:36:37,130 --> 00:36:40,580
You don't understand the question. The question is, why is that?

286
00:36:41,270 --> 00:36:46,849
Maybe they don't remember, but you know how to report that.

287
00:36:46,850 --> 00:36:52,290
So there's all kinds of reasons, even though when you're measuring things more objectively, right?

288
00:36:52,310 --> 00:36:55,700
Like let's say you're measuring blood pressure.

289
00:36:57,740 --> 00:37:03,140
Can you measure blood pressure wrong? Absolutely right.

290
00:37:03,320 --> 00:37:09,860
So we can have we can get mass classification even when we're measuring kind of objective things,

291
00:37:09,860 --> 00:37:15,610
if there's error in our instrument or in our the way that we're reading it or something like that.

292
00:37:15,620 --> 00:37:20,930
So this kind of error is always kind of a possibility.

293
00:37:22,100 --> 00:37:27,260
Another type of information bias is the ecological fallacy,

294
00:37:28,430 --> 00:37:35,750
and that's just when we do an analysis of the group level and use that to make an inference at the individual level.

295
00:37:37,670 --> 00:37:41,150
And we might do that because we don't have individual level data.

296
00:37:45,960 --> 00:37:56,740
But it can be a problem. So I'll give you an example.

297
00:37:58,210 --> 00:38:04,030
I'm interested in understanding unemployment as a risk factor for early mortality.

298
00:38:05,740 --> 00:38:13,180
Okay. And I look at the relationship between state level, the state level, unemployment rate and the mortality rate in the state.

299
00:38:13,990 --> 00:38:24,050
And I say there's an association there. Where might I go wrong if I commit the ecologic fallacy?

300
00:38:26,240 --> 00:38:36,600
Where would I go wrong in my logic? Culinary thinking about it.

301
00:38:58,330 --> 00:39:00,970
Probably not. That is the consequence.

302
00:39:01,180 --> 00:39:11,860
But if I say that across the state, at the state level, higher levels of unemployment are associated with higher mortality risk.

303
00:39:12,550 --> 00:39:20,800
If I then go on to say that being unemployed means that person increases a person's risk for death,

304
00:39:21,610 --> 00:39:28,630
then I've committed that because I haven't shown that with the way with the area level data.

305
00:39:29,260 --> 00:39:33,040
So it's not that ecologic studies are wrong.

306
00:39:33,070 --> 00:39:39,760
I think sometimes we get the impression that ecologic studies are bad or wrong and that we shouldn't, that that's not true.

307
00:39:40,390 --> 00:39:47,500
What's a problem is when we take studies that are at the group level and we try to apply it to the individual level,

308
00:39:47,800 --> 00:39:55,300
that's where we go wrong because that relationship may be true at the aggregate level and yet not true at the individual level.

309
00:39:57,800 --> 00:40:03,740
So it's just a matter of making sure that you're collecting data at the level at which you want to make an inference.

310
00:40:09,310 --> 00:40:11,950
We can also have temporal ambiguity.

311
00:40:15,930 --> 00:40:23,730
So it's not always clear that the exposure came before the outcome, particularly if we're doing a cross-sectional study.

312
00:40:24,060 --> 00:40:42,310
But it can even be true in to data. All right.

313
00:40:42,320 --> 00:40:48,000
So the third one. Yes. Sasha. Yes.

314
00:40:52,610 --> 00:40:56,500
I wanted to help out front to make sure that like.

315
00:41:01,360 --> 00:41:08,379
Yeah. I just wondered. It's just like I thought. So I'm thinking about.

316
00:41:08,380 --> 00:41:11,180
Yeah, I can just study design. You should be fine, right?

317
00:41:11,360 --> 00:41:19,240
No, people don't have a condition at the start of the study and then you follow them until they develop their condition.

318
00:41:19,780 --> 00:41:23,980
You should be able to kind of have that established.

319
00:41:24,070 --> 00:41:33,160
Now, it would be difficult, though, if they just don't have a clinical manifestation of the disease at the start of the study.

320
00:41:33,160 --> 00:41:40,300
But the disease process has already begun. And so that could be issues.

321
00:41:40,750 --> 00:41:45,549
I'm also thinking about studies where, you know, you start off,

322
00:41:45,550 --> 00:41:51,070
I did my dissertation with the National Longitudinal Study of adolescents who adults on the adult study.

323
00:41:51,460 --> 00:41:58,390
They started that study when people were in seventh through 12th grade and then followed them over time.

324
00:41:59,050 --> 00:42:07,600
And so, you know, I might have a question about what's happening within that area,

325
00:42:07,600 --> 00:42:12,880
but I don't know what happened for like I don't know what happened to them in childhood.

326
00:42:13,330 --> 00:42:22,510
And so it could be that there's something there that's actually like already present that we don't observe until they're in adolescence.

327
00:42:22,510 --> 00:42:26,300
And we assume that we know when it happened, but it could have been before.

328
00:42:26,320 --> 00:42:35,410
I mean, it's kind of the same as the example that we find that subclinical that's already present before the initiation of the study.

329
00:42:44,460 --> 00:42:52,600
Okay. So don't look at the definition again. This is the biggie, right?

330
00:42:52,630 --> 00:42:55,870
We worry about confounding a lot. Yes. Okay.

331
00:42:55,870 --> 00:42:59,620
So confounding is our third kind of source of bias.

332
00:42:59,620 --> 00:43:04,810
But the article talks about this is the article's definition of confounding.

333
00:43:09,590 --> 00:43:13,430
It's just too many words. Words.

334
00:43:22,060 --> 00:43:24,940
I'm going to try to say it in a way that's a little simpler,

335
00:43:25,300 --> 00:43:33,820
although I do want to say that confounding is one of those things, just like all of these other concepts.

336
00:43:35,790 --> 00:43:40,080
That are very abstract concepts.

337
00:43:41,560 --> 00:43:47,370
And you could read five different authors or ten different authors who would

338
00:43:47,380 --> 00:43:53,320
give you ten different ten slightly different takes on what these things mean.

339
00:43:57,780 --> 00:44:07,379
So this offers us. But confounding occurs when a variable is a risk factor for an effect among non exposed persons and is associated with the

340
00:44:07,380 --> 00:44:14,040
exposure of entries in the population from which the effect derives without being affected by the exposure or the disease,

341
00:44:14,050 --> 00:44:18,960
in particular, without being an intermediate step in the causal pathway between the exposure and the effect.

342
00:44:22,320 --> 00:44:31,980
Okay. A simpler definition, because it's a third variable that distorts the relationship between two other variables.

343
00:44:43,560 --> 00:44:53,070
So it can either falsely obscure a relationship that's really there or possibly accentuate a relationship.

344
00:45:03,930 --> 00:45:09,360
And so then there's a couple of different ways that epidemiologists talk about confounders.

345
00:45:09,930 --> 00:45:18,870
So you probably learned a definition that something like a variable that's associated with the exposure and a risk

346
00:45:18,870 --> 00:45:29,850
factor for the outcome that's unfamiliar and without being on the causal pathway from the exposure to the outcome.

347
00:45:30,150 --> 00:45:34,860
So that's that piece here in particular without being an immediate stat.

348
00:45:36,060 --> 00:45:43,110
So it can't be a mediator. Basically, if it's on the causal pathway from exposure to outcome, it's not a confounder.

349
00:45:43,650 --> 00:45:50,760
But if it's if that's not true and it's associated with the exposure and a risk factor for the outcome, that it's a confounder.

350
00:45:51,510 --> 00:45:59,880
Also, the way that you think about confounders and dags, which the glamor article that you read I think talks about this.

351
00:46:01,440 --> 00:46:06,120
A confounder is a common cause of exposure and outcome.

352
00:46:08,370 --> 00:46:12,480
So those are slightly different definitions.

353
00:46:13,400 --> 00:46:18,960
So if we think about exposure, that's.

354
00:46:26,950 --> 00:46:43,540
We think about exposure and our. That's the first way of defining the confounder that we just talked about that's associated with the exposure.

355
00:46:43,540 --> 00:46:46,780
And it's a risk factor, meaning it causes the outcome.

356
00:46:47,260 --> 00:46:54,700
But the other way of thinking about it in the dark perspective is that it has to be like that.

357
00:46:55,480 --> 00:47:08,010
It has to be a common cause. And I think we talked last time about how it's really difficult to know if it's this or if it's this.

358
00:47:08,010 --> 00:47:15,660
Right. So that's it seems simple here, but it's actually more it's hard to know which is actually going on.

359
00:47:17,530 --> 00:47:27,639
Okay. So to deal with confounding, we can match on potential confounders and the design phase of the study or we can control for them.

360
00:47:27,640 --> 00:47:33,910
And the analysis which pretty much every paper we're going to read this semester is

361
00:47:33,910 --> 00:47:39,310
going to take the approach of trying to control for confounders and the analysis stage.

362
00:47:44,080 --> 00:47:49,190
Okay. Any questions about that?

363
00:47:53,390 --> 00:47:58,520
So let's talk about different kinds of third variable effects.

364
00:47:58,550 --> 00:48:06,980
And this is I'm drawing on the macKinnon paper for this part of the lecture, and I love this paper.

365
00:48:10,590 --> 00:48:14,490
So they talk about mediation, confounding and suppression.

366
00:48:18,150 --> 00:48:25,680
Now, the McKinnon paper didn't talk about it, but we could also talk about effect modification as another type of third variable effect,

367
00:48:25,680 --> 00:48:30,150
and we can talk about that if you have questions about how it's different from these other things.

368
00:48:34,280 --> 00:48:46,090
So let's start with mediation. So in this drawing, X is our exposure.

369
00:48:46,390 --> 00:48:49,630
Y is our outcome. M is our mediator.

370
00:48:51,490 --> 00:48:57,850
So a mediator is just a third variable that's intermediate between exposure and outcome.

371
00:49:07,890 --> 00:49:15,140
So this piece. The X to Y piece is what's called the direct effect.

372
00:49:16,700 --> 00:49:20,780
And then this piece from X and Y is the indirect effect.

373
00:49:21,590 --> 00:49:25,940
And we add up the direct effect and the indirect effect, and it gives us the total effect.

374
00:49:29,490 --> 00:49:32,340
So we say that we have complete mediation.

375
00:49:33,000 --> 00:49:38,909
If after controlling for the mediator, there is no longer any association between the exposure and the outcome,

376
00:49:38,910 --> 00:49:43,350
we would say we've completely explained why this exposure is related to this outcome.

377
00:49:44,220 --> 00:49:52,800
More often we have partial mediation where some of this relationship that we observe is explained by the mediator or mediators that were looking at.

378
00:49:58,150 --> 00:50:07,060
So let's think about an example. So if I am interested in understanding income as a predictor of blood pressure.

379
00:50:09,270 --> 00:50:13,680
I want to know if people who have lower income have higher blood pressure.

380
00:50:14,880 --> 00:50:44,010
What might be some potential mediators there? Yeah.

381
00:50:45,150 --> 00:50:53,300
Say that again. Race. Race. Like, for example, you use the color, experience or stress, which also means you are above pressure.

382
00:50:53,670 --> 00:50:59,470
Okay, so let's talk about that. Let's talk about race. And stress.

383
00:51:00,460 --> 00:51:04,240
So what would be the relationship between race and income?

384
00:51:07,520 --> 00:51:11,380
Correlated. Right.

385
00:51:12,100 --> 00:51:17,110
And then stress. What's the relationship between income and stress?

386
00:51:23,930 --> 00:51:28,430
I think maybe I would say that low income can cause stress.

387
00:51:29,000 --> 00:51:37,400
So I would put stress here. So I would say that race is a confounder.

388
00:51:39,830 --> 00:51:42,860
Here and the stress is a potential mediator.

389
00:51:43,350 --> 00:51:49,220
So, I mean, does that sound right? Okay. Any other potential mediators?

390
00:51:50,120 --> 00:51:53,870
Yes. Next. Exercise.

391
00:51:53,900 --> 00:52:00,750
Okay, so we got the physical activity. Okay, Tiger.

392
00:52:03,930 --> 00:52:07,030
You have the option of smoking.

393
00:52:15,570 --> 00:52:18,620
Oh. But you also said maybe by conditions in the.

394
00:52:19,520 --> 00:52:22,620
Yeah, yeah, yeah. Right. Conditions.

395
00:52:23,820 --> 00:52:28,770
And then of course, this is related to right.

396
00:52:28,770 --> 00:52:32,669
Which is also kind of important for mediation.

397
00:52:32,670 --> 00:52:44,500
We don't have to go too much into that, but. Yes.

398
00:52:45,730 --> 00:52:55,020
It's not 100 and. I felt like.

399
00:52:58,730 --> 00:53:02,000
So the difference is entirely conceptual.

400
00:53:02,990 --> 00:53:06,020
It's entirely up to you to decide.

401
00:53:07,040 --> 00:53:10,580
I'm sorry. It was the nature of the relationship is.

402
00:53:10,670 --> 00:53:14,980
Okay, so let's take. So race is actually a good one for us to think about.

403
00:53:14,990 --> 00:53:20,390
So for something to be a mediator, the exposure has to cause it.

404
00:53:21,230 --> 00:53:25,430
Can income cause race? No, that's easy, right?

405
00:53:25,460 --> 00:53:33,080
You know that income does not cause race. So we know racism. We know race isn't a mediator, but we know they're associated.

406
00:53:33,230 --> 00:53:36,650
So then we would say it's a confounder and we need to adjust for it.

407
00:53:36,890 --> 00:53:41,750
Okay. But something like, um.

408
00:53:42,530 --> 00:53:49,700
Something like smoking. What we would have to think about.

409
00:53:49,700 --> 00:53:52,730
If we want to propose that smoking is a mediator.

410
00:53:53,360 --> 00:54:03,020
We have to know a couple of things, right? We have to know if we're what we're saying is that income, low income causes people to smoke.

411
00:54:04,740 --> 00:54:07,980
What would be an explanation for why that would be the case?

412
00:54:08,760 --> 00:54:13,250
Some causal reasoning for why low income would cause people to smoke.

413
00:54:20,170 --> 00:54:24,690
Yeah. Okay.

414
00:54:24,880 --> 00:54:31,990
It could be that there's targeted advertising for people at low income, which then causes them to be more likely to smoke.

415
00:54:32,650 --> 00:54:43,170
Any other reason? Yeah. It. Okay.

416
00:54:49,080 --> 00:54:57,150
Yeah. So it might be an accessible way to reduce the stress and a lot of it from your.

417
00:55:05,520 --> 00:55:09,290
Yeah. So you might be in areas where there's a high concentration of outlets for

418
00:55:09,420 --> 00:55:14,850
the whole cigarets and other unhealthy kinds of items that you want to more.

419
00:55:19,270 --> 00:55:26,470
Yeah. Yeah.

420
00:55:26,480 --> 00:55:34,640
So if, if where you're buying food is in a gas station versus a grocery store or higher access to gas right now.

421
00:55:42,480 --> 00:55:46,590
Sure, but the conditions could negatively impact the display.

422
00:55:49,040 --> 00:55:53,690
Yeah. You are in a large area.

423
00:55:58,840 --> 00:56:08,600
I. Yeah, I don't know, but possibly it depends if it's like I think generally in more densely populated areas.

424
00:56:11,310 --> 00:56:15,700
But I'm not sure. I.

425
00:56:18,340 --> 00:56:22,629
Say that again. Just like that. Okay.

426
00:56:22,630 --> 00:56:28,480
So your income could be associate of the type of occupation you have, which could be, um,

427
00:56:28,840 --> 00:56:33,940
if it operated by increasing strife that caused you to use cigarets or something like that.

428
00:56:34,960 --> 00:56:40,200
Yeah, we can. Yeah. No, I'm sorry. I'm just thinking, like along that same line if you're not working.

429
00:56:42,410 --> 00:56:45,910
Yeah. Smoke now. Mhm. Yeah. Absolutely.

430
00:56:46,000 --> 00:56:57,460
You could also think about there could be worms in your social group that vary by income around smoking and you could make arguments for how,

431
00:56:57,790 --> 00:57:01,360
um, having low income could cause people to smoke more.

432
00:57:02,410 --> 00:57:10,840
But, um, what if I told you that a lot of people start smoking before they have a job?

433
00:57:13,270 --> 00:57:19,330
A smart start smoking an adolescent. So what is the time order here?

434
00:57:19,350 --> 00:57:24,160
Is that right? Right. Like, what if I'm measuring someone's income?

435
00:57:24,790 --> 00:57:35,850
My sample is 18 to 34. And I measure an income at 18 to 34 and I'm noticing smoking at 18 to 34.

436
00:57:36,840 --> 00:57:41,430
Could I have this wrong? Could I have the temporal order of smoking at home?

437
00:57:42,030 --> 00:57:45,630
I could. You could argue that.

438
00:57:47,550 --> 00:57:59,990
Parental income. Both causes a person's own income and young adulthood and influence smoking initiation.

439
00:58:01,310 --> 00:58:06,260
And then we feel we'd be safe, but we probably don't really know that.

440
00:58:08,930 --> 00:58:18,980
So this is just an example of how if you thought that income didn't actually cause someone to smoke,

441
00:58:19,520 --> 00:58:26,540
but it was just associated through some other mechanism, then you'd put smoking here.

442
00:58:29,350 --> 00:58:30,220
You, wouldn't it, here.

443
00:58:33,350 --> 00:58:42,380
And that's why I'm saying it's completely up to how you're thinking about where it falls in all of these relationships that you're interested in such.

444
00:58:50,100 --> 00:58:56,640
I mean, it could also be that people who I'm just trying to think of another one that would, um.

445
00:58:57,240 --> 00:59:04,680
Well, you could also think about someone's neighborhood conditions as actually being closely related to their ability to earn income.

446
00:59:05,040 --> 00:59:12,540
So if you live in a neighborhood where there's few job opportunities you'll have, you'll have a lower income.

447
00:59:12,900 --> 00:59:16,590
But maybe the neighborhood condition you're talking about here.

448
00:59:17,010 --> 00:59:23,670
Um, I think this is why. Sure. Yeah, I think what you were saying, like, maybe this is about neighborhood safety.

449
00:59:26,690 --> 00:59:31,070
So maybe you have low income, you're more likely to live in a neighborhood that's less safe.

450
00:59:32,180 --> 00:59:37,940
Therefore, you're less likely to engage in physical activity, which then affects your blood pressure.

451
00:59:45,590 --> 00:59:48,770
He then we have to put this in a regression model. Right.

452
00:59:53,510 --> 00:59:59,040
So we're going to talk about that in just a minute. All right.

453
00:59:59,040 --> 01:00:02,130
So there are some examples of media. These are an example of decent.

454
01:00:05,770 --> 01:00:17,700
Now we've got new founding. This is the third variable that distorts, falsely obscures or accentuates the relationship between two other variables.

455
01:00:19,500 --> 01:00:23,010
A relationship between an exposure and an outcome. A spurious.

456
01:00:23,430 --> 01:00:30,060
Yeah. After controlling for this third factor, there's no longer an association between the exposure and the outcome.

457
01:00:31,260 --> 01:00:39,480
If we had some kind of significant relationship and we just found her and it's gone, we know it was a spurious relationship.

458
01:00:42,490 --> 01:00:50,680
Okay. So classic example. Among children, shoes, shoe size predicts math ability.

459
01:00:52,220 --> 01:00:55,629
Why? Yeah.

460
01:00:55,630 --> 01:00:58,690
Ali, it is age. Yeah.

461
01:00:58,840 --> 01:01:02,750
So older kids have bigger feet. They also know more math.

462
01:01:03,830 --> 01:01:10,470
Right. So that was silly. But we have you in kind of questions that we care about you.

463
01:01:10,480 --> 01:01:18,070
So we already kind of started down this path of thinking about what the potential confounders are between income and blood pressure.

464
01:01:18,370 --> 01:01:22,120
Are there others that you can think of besides race and maybe smoking?

465
01:01:30,460 --> 01:01:37,710
Yeah, it's. So older people.

466
01:01:38,480 --> 01:01:42,000
Well, up to a point in this age range and 18 to 34.

467
01:01:42,020 --> 01:01:46,610
Older people tend to have higher incomes because they have more work experience.

468
01:01:47,840 --> 01:01:52,490
But becoming older doesn't cause you to have higher income.

469
01:01:52,790 --> 01:02:04,159
Right. Because if you didn't have if you weren't working and getting more work experience that increased your income,

470
01:02:04,160 --> 01:02:08,430
then there would be no relationship between aging. Yes.

471
01:02:08,560 --> 01:02:15,810
Oh, yeah. Who makes more money.

472
01:02:21,190 --> 01:02:26,570
I was going to say. You're to say gender to you. Yeah. Also we might.

473
01:02:26,790 --> 01:02:39,050
Oh, yeah. Well, I don't know. Maybe, like, if someone's in town, it's like somewhere, like, they're, like, chronically know.

474
01:02:40,190 --> 01:02:44,089
So if they have low income, chronically, that would be.

475
01:02:44,090 --> 01:02:47,629
Are you thinking that if someone lost their job and they're thinking in the survey of like,

476
01:02:47,630 --> 01:02:52,520
yeah, my income is $15,000 right now, but like most other times in their life closer.

477
01:02:53,420 --> 01:02:56,860
So that would be misclassification. Right.

478
01:02:56,870 --> 01:03:03,769
Because what we're trying to understand is kind of not their exact current income.

479
01:03:03,770 --> 01:03:09,110
We're trying to understand in general their income and the money that they have access to.

480
01:03:09,380 --> 01:03:15,740
And if they're very temporarily unemployed and we measure income and we say this

481
01:03:15,740 --> 01:03:19,160
is the income that this person has been exposed to for some period of time,

482
01:03:19,430 --> 01:03:26,170
then we just misclassified them, I guess, like something that's like a chronic health outcome that.

483
01:03:30,400 --> 01:03:37,330
Yeah. And it could be. So we have another potential problem with this model that has nothing to do with confounding or mediation.

484
01:03:37,750 --> 01:03:41,410
And that is what if this area actually goes that way?

485
01:03:43,150 --> 01:03:46,240
What if having having chronic hypertension.

486
01:03:48,230 --> 01:03:51,830
Reduces your income because you're less able to work.

487
01:03:52,730 --> 01:03:55,910
Or you have high blood pressure.

488
01:03:56,390 --> 01:04:03,200
This led to a stroke, led you to be unemployed or on disability or something like that.

489
01:04:03,230 --> 01:04:11,060
Right. This is probably unlikely in our 18 to 34 age range.

490
01:04:12,290 --> 01:04:15,890
But what if we were looking at people who are 65 plus.

491
01:04:19,320 --> 01:04:25,220
Which gets us to another question. But this even makes sense as an exposure and people who are 65 five.

492
01:04:26,520 --> 01:04:30,120
Olivia, you say no way. Looks like you're going to be retiring.

493
01:04:32,120 --> 01:04:32,500
Yeah.

494
01:04:32,510 --> 01:04:45,020
So income after 65 doesn't necessarily tell us that much about a person's material resources because they likely have wealth at that point or don't,

495
01:04:45,020 --> 01:04:55,610
which is more important. Like if they have an hour.

496
01:04:58,080 --> 01:05:07,600
Yeah. Yeah. They could be. So, like, once you get once you get over 65, like once you have a lot of people who are retired.

497
01:05:08,140 --> 01:05:15,820
If what you really want to understand is what is the level of material resources that this person has?

498
01:05:16,240 --> 01:05:22,540
Only measuring income is a problem because a lot of times older adults are on fixed incomes.

499
01:05:22,540 --> 01:05:25,600
But it doesn't necessarily mean that they don't have access to material.

500
01:05:27,560 --> 01:05:34,720
So their income may be sort of relatively low, but their wealth may be high, which is promoting.

501
01:05:34,730 --> 01:05:41,850
So you might want to. You might also want to control for wealth here.

502
01:05:55,560 --> 01:06:02,320
Now. You all look sad. Yeah. McKayla. Okay.

503
01:06:03,490 --> 01:06:07,160
So you could have more? Yeah.

504
01:06:07,180 --> 01:06:10,260
Or, like household size, right?

505
01:06:10,270 --> 01:06:19,510
Because household size could be related to your income and your blood pressure through some different pathway.

506
01:06:19,720 --> 01:06:27,459
You can also do if you wanted to deal with that a different way, you could you can have adjusted for household size.

507
01:06:27,460 --> 01:06:33,820
So you could do per capita income or something like that to adjust for differences in household size.

508
01:06:36,100 --> 01:06:39,800
Yeah. I think you all look sad because it's really complicated.

509
01:06:41,300 --> 01:06:48,590
Okay, that must be right, because you laugh. Maybe you're sad because it's Friday and that can be part of that year.

510
01:06:48,890 --> 01:06:56,240
But this does it does kind of make you, like, think, oh, my God, that's a really easy research question.

511
01:06:57,050 --> 01:07:03,200
And even it is really, really messy. So, yeah, this stuff is it's hard.

512
01:07:04,400 --> 01:07:09,320
Okay. And I already talked about two ways to think about confounders, right?

513
01:07:09,320 --> 01:07:14,120
So whether it's associated or whether it's a common cause.

514
01:07:15,380 --> 01:07:25,500
So. If we think that this may be getting too much into the weeds events, but like,

515
01:07:25,550 --> 01:07:34,879
let's say we think that age is merely associated with income and not a and not a cause of income.

516
01:07:34,880 --> 01:07:39,830
Then we might not put it in our bag because we don't think it's closely related.

517
01:07:40,070 --> 01:07:43,640
Anyway, don't worry about that. Let's move on. Okay.

518
01:07:45,920 --> 01:07:52,330
So what I did want to say also is that usually usually adjusting for this,

519
01:07:52,340 --> 01:07:57,950
adjusting for a confounder well, attenuate the association between exposure and outcome.

520
01:07:57,950 --> 01:08:04,490
Right. Move it closer to the null. Make it smaller, but not always.

521
01:08:06,620 --> 01:08:09,140
And so that's where we get suppression.

522
01:08:10,520 --> 01:08:18,050
So suppression is where you add a third variable and it actually increases the association between the exposure and the outcome.

523
01:08:22,120 --> 01:08:25,960
And so this may be referred to as negative, confounding.

524
01:08:27,070 --> 01:08:34,030
If you're thinking about confounding or inconsistent mediation, they talk about a little bit.

525
01:08:38,240 --> 01:08:45,770
In the McKinnon caper. And I'm going to try to show you an example of this.

526
01:09:19,140 --> 01:09:22,320
That red marker is intensive, does not want to come off. Okay.

527
01:09:43,150 --> 01:09:56,060
Okay. Want us to. All right.

528
01:09:56,980 --> 01:10:03,100
So I'm doing an analysis and I want to understand gender differences in Self-Rated health.

529
01:10:04,660 --> 01:10:10,630
I actually did this paper a number of years ago. So what I'm going to show you is, like, is real, what we really found.

530
01:10:11,320 --> 01:10:19,210
Okay. So we were looking and this analysis was done in South Korea, not in the US.

531
01:10:19,990 --> 01:10:22,990
And I'll just tell you what. The relations well, maybe I'll let you guys.

532
01:10:23,290 --> 01:10:28,210
How do you think gender is related to Self-Rated health in Korea?

533
01:10:31,110 --> 01:10:35,339
It was more likely to report fair or poor self-rated health versus good.

534
01:10:35,340 --> 01:10:41,070
Very good or excellent women or men. What do you think?

535
01:10:47,070 --> 01:10:50,510
No idea. Women. Okay.

536
01:10:50,520 --> 01:10:54,690
So women report poorer health than men in the US.

537
01:10:54,720 --> 01:10:57,840
That's not that's not always the case a lot of times. Self-Rated health.

538
01:10:57,840 --> 01:11:04,980
We'll talk about that when we talk about gender. But but in Korea, women report worse self-rated health than men.

539
01:11:05,550 --> 01:11:12,150
Okay, now, how do you think that gender is related to income in Korea?

540
01:11:16,070 --> 01:11:19,810
Do you want to make more money or less money? Yes, I want.

541
01:11:23,570 --> 01:11:28,080
Yeah. Much love. Even more than in the U.S. Yeah. Okay.

542
01:11:30,490 --> 01:11:35,980
Was. And how do you think that income is related to Self-Rated health?

543
01:11:40,540 --> 01:11:45,370
Yeah. If I was lower income, who'd want to wait to report?

544
01:11:45,390 --> 01:11:48,820
So. Oh, for software. Yeah.

545
01:11:48,940 --> 01:11:54,700
Exactly. So higher income people are less likely to report fear of poor self-rated health.

546
01:11:55,120 --> 01:11:59,650
Yep. Okay, so now we got that pathway. All right, what about smoking?

547
01:11:59,680 --> 01:12:08,530
Again, this is different in the US. In Korea. What do you think the relationship is here?

548
01:12:10,300 --> 01:12:14,340
Are women more or less likely to smoke than men? Yeah.

549
01:12:14,530 --> 01:12:17,680
Victoria is far less likely to smoke.

550
01:12:18,070 --> 01:12:21,220
Yeah, there is in the US somewhere.

551
01:12:21,700 --> 01:12:29,260
All right. And how is smoking related to health? This is true everywhere, right?

552
01:12:29,800 --> 01:12:34,060
More smoking, worse health. Right next to smoking.

553
01:12:34,720 --> 01:12:38,740
If you smoke, you're more likely to report fair or poor self-rated health.

554
01:12:39,980 --> 01:12:53,900
Okay. So now one of these is what is a confounder, positive, confounder or a mediator?

555
01:12:53,900 --> 01:12:57,500
I mean, I'm showing this like a mediator because I'm showing this is causal,

556
01:12:57,500 --> 01:13:01,700
but you could often have it be like that if you just think they're associated.

557
01:13:02,180 --> 01:13:10,370
All right. So. Let's say one of them is a confounder and one is a suppressor.

558
01:13:10,910 --> 01:13:18,970
Do you know which is the confounder? Which is going to attenuate this association when I adjust for it.

559
01:13:34,650 --> 01:13:37,970
You have a 5050 chance. Yeah.

560
01:13:38,060 --> 01:13:42,080
Patricia? Income. Yes. And I'll show you why.

561
01:13:42,830 --> 01:13:47,330
And this is this is also in the McKenna paper.

562
01:13:47,360 --> 01:13:53,230
Okay. So if I take negative times.

563
01:13:53,300 --> 01:13:58,860
Negative. Times positive.

564
01:14:00,840 --> 01:14:04,470
What do I get? Positive.

565
01:14:05,710 --> 01:14:13,250
Okay. So positive, confounding or mediation.

566
01:14:15,370 --> 01:14:21,880
And then if I take negative times positive.

567
01:14:24,410 --> 01:14:28,310
I'm positive. Why do I get negative?

568
01:14:30,050 --> 01:14:34,790
I get negative, confounding or inconsistent mediation.

569
01:14:39,690 --> 01:14:43,680
So like so let's talk about what that means. Like how you would interpret that.

570
01:14:46,220 --> 01:14:58,880
So if we're looking at this one through income, gender differences in income help explain why women report worse off than men.

571
01:15:01,720 --> 01:15:06,910
If women had higher income, they would likely report self-rated health.

572
01:15:06,910 --> 01:15:10,450
That was more similar to that. So that makes sense. Okay.

573
01:15:11,620 --> 01:15:16,180
But smoking is actually working in the opposite direction.

574
01:15:17,170 --> 01:15:24,400
So women are less likely than men to smoke if they smoked at similar levels.

575
01:15:24,910 --> 01:15:28,330
They would have even worse self-rated health than men.

576
01:15:32,230 --> 01:15:36,640
Does that make sense? Usually what we see is this.

577
01:15:37,300 --> 01:15:41,090
This is almost always what we see. We rarely see suppression effects.

578
01:15:41,110 --> 01:15:51,550
They're uncommon. Because remember, what you're thinking about is if I hold this constant, what happens to this relationship?

579
01:15:56,480 --> 01:16:02,270
You want me to say it again or not say one more time.

580
01:16:02,570 --> 01:16:16,340
Okay. So part of the reason why women report we're self-treat at home in Korea than men is because they have lower income and income promotes health.

581
01:16:17,270 --> 01:16:18,830
So that's part of the explanation.

582
01:16:19,160 --> 01:16:28,610
So from the perspective of an intervention, what might I do if I wanted to improve women's self-rated health relative to men?

583
01:16:32,840 --> 01:16:38,090
They might increase their income. Right. And if I did that, I would expect to see that if this is truly causal,

584
01:16:38,330 --> 01:16:41,930
I would expect to see that their self-rated health would improve relative to men.

585
01:16:42,500 --> 01:16:50,780
All right. So then this one is saying that women don't have it doesn't make sense to say that women have

586
01:16:51,950 --> 01:16:56,450
are more likely to have fair or poor self-rated health than men because they smoke more.

587
01:16:56,840 --> 01:17:01,520
Because they don't. Right. They smoke less, which actually protects their health.

588
01:17:02,240 --> 01:17:11,000
So if I did a hypothetical intervention and I made women smoke at the same level as men, I increase women's smoking.

589
01:17:11,720 --> 01:17:17,180
What would happen here when women and men have more similar self-rated health or less similar?

590
01:17:20,180 --> 01:17:23,600
It would become less similar. It would diverge even more.

591
01:17:24,830 --> 01:17:30,500
So that's what the suppression effect is. Now, what's.

592
01:17:33,730 --> 01:17:42,120
Super fascinating to me anyway. Well, we're going to come back to that.

593
01:17:42,120 --> 01:17:49,170
But the point that McKinnon makes is that statistically these are all equivalent, right?

594
01:17:50,040 --> 01:17:53,580
Mediation, confounding suppression. We're all looking at them.

595
01:17:54,210 --> 01:18:02,100
We're looking at all the same. So then the question is, which one is it?

596
01:18:03,210 --> 01:18:09,240
And particularly when we're talking about whether you have a confounder or a mediator, how do you know?

597
01:18:19,430 --> 01:18:27,740
In this example, if we're trying to figure out if income is a mediator, which would look like this or a confounder.

598
01:18:30,000 --> 01:18:36,360
Can I after I adjust for income in the model is going to do exactly the same thing.

599
01:18:36,360 --> 01:18:42,810
Whether I say it's immediate or whether it's going to attenuate the association and that's what it's going to do.

600
01:18:44,960 --> 01:18:52,040
But that doesn't tell me if it's a mediator or founder. I have to decide if it's a mediator or confounder.

601
01:18:52,460 --> 01:19:04,100
Do I think that there's something about gender or gender roles that's causally related to income that I'm going to say it's a mediator?

602
01:19:04,280 --> 01:19:08,810
If I think these two things just happen to be correlated, then I'm going to say it's a confounder.

603
01:19:12,890 --> 01:19:14,000
So you have to decide.

604
01:19:14,000 --> 01:19:23,930
But that interpretation depends on the interpretation of the result, depends on whether you think it's a mediator or a confounder.

605
01:19:24,980 --> 01:19:29,270
So remember my explanation before was a mediation explanation.

606
01:19:29,270 --> 01:19:37,999
I said The reason women are more likely to have fear for Self-Rated health is because they have less income.

607
01:19:38,000 --> 01:19:45,399
And I'm saying in this like causal way. And so when this relationship is attenuated,

608
01:19:45,400 --> 01:19:56,560
I'm saying that this is real and this is why if I say it's a confounder and this gets attenuated, I'm saying it's not real.

609
01:19:57,400 --> 01:20:02,080
This isn't a real relationship because it's just being distorted by this correlation.

610
01:20:02,410 --> 01:20:10,120
So it's not really positive. So we already answered this question.

611
01:20:10,120 --> 01:20:13,220
Can statistics tell you if it's a mediator or a confounder?

612
01:20:13,240 --> 01:20:26,730
No. So you have to rely on theory and accumulated knowledge to tell you about the direction and the causal nature of relationships in your model.

613
01:20:27,570 --> 01:20:30,450
So I can tell from data the direction.

614
01:20:31,440 --> 01:20:41,090
I can just use the data to know that this is negative, but I can't use data to tell me whether it's merely correlated or causal that I can't do.

615
01:20:47,940 --> 01:20:55,800
So I have to understand the nature of this relationship in the social context that I'm studying.

616
01:21:03,360 --> 01:21:09,030
So I know that I'm in a context where there's a lot of gender discrimination in the workplace.

617
01:21:09,810 --> 01:21:13,980
So I know that this is probably possible, and that's how I'm going to interpret it.

618
01:21:16,840 --> 01:21:20,650
Okay. So then the question is.

619
01:21:22,500 --> 01:21:34,760
What should you control for? Should you only control for things that you think are confounders?

620
01:21:37,080 --> 01:21:46,890
Should you control for confounders and mediators? Should you control for things that aren't either a confounder or a mediator?

621
01:21:56,610 --> 01:22:00,390
What do you think? Yes, I sure.

622
01:22:03,940 --> 01:22:09,140
I mean if the research study. With the leader that is already.

623
01:22:10,450 --> 01:22:20,270
I like going off. Like the kids down here and not like the kids on the show.

624
01:22:26,110 --> 01:22:32,530
Not so. So you're going to want to control for confounders.

625
01:22:32,540 --> 01:22:37,420
If you want to understand the unbiased relationship between your exposure and your outcome.

626
01:22:37,810 --> 01:22:42,700
Yeah, that's when you should just control for confounders.

627
01:22:43,780 --> 01:22:50,940
When might you want to control for mediators? Or should you never control for meat eaters?

628
01:22:53,770 --> 01:23:00,000
Yes. I have often said that when you want to study the directive for.

629
01:23:02,200 --> 01:23:06,400
Are you okay? I just for a minute. Oh, just Florida A&M, right?

630
01:23:07,060 --> 01:23:07,690
Exactly.

631
01:23:08,080 --> 01:23:17,230
So if you're interested in decomposing that total effect into the direct effect and the indirect effect, because you have a question about mediation.

632
01:23:17,660 --> 01:23:22,300
Right. What you want to understand why this relationship is here.

633
01:23:22,960 --> 01:23:29,620
Then you can then you adjust for mediators, but you do it in the context of understanding, direct and indirect.

634
01:23:31,950 --> 01:23:35,430
Yeah. Does that make sense to everybody?

635
01:23:36,270 --> 01:23:46,480
Sometimes I feel like I've had years where students were taught never control for a mediator, and that's not true.

636
01:23:46,740 --> 01:23:51,780
Sometimes you have a mediation question you should control through a mediator,

637
01:23:51,780 --> 01:23:56,040
and all you're trying to do is find the unbiased association between exposure and outcome.

638
01:23:56,040 --> 01:23:59,760
That's absolutely right. You should not be controlling for things that are in the causal pathway.

639
01:24:00,090 --> 01:24:04,380
But if you want to understand the causal pathway, then you do it.

640
01:24:05,310 --> 01:24:09,780
So it's all about how your research question and how you interpret your findings.

641
01:24:10,710 --> 01:24:15,630
So what about things that aren't potential confounders or mediators?

642
01:24:15,630 --> 01:24:20,040
They're just things that are associated with the outcome. Should you adjust for those?

643
01:24:22,550 --> 01:24:28,100
Olivia, you might y think it's still important to understand it's still influencing that outcome.

644
01:24:28,100 --> 01:24:32,300
So you want to know how to understand your answer to my question?

645
01:24:32,990 --> 01:24:39,290
Yeah. So you might want to control for something that's just associated with the outcome because it increases the precision of your estimates.

646
01:24:39,620 --> 01:24:50,719
So for example, like in our example before where we had income and we had blood pressure and you might want to control for some

647
01:24:50,720 --> 01:24:55,700
genetic risk score for blood pressure because we know that there's some genetic component of blood pressure,

648
01:24:56,000 --> 01:25:01,860
but we have no reason to believe that that genetic risk for blood pressure is any way associated with income.

649
01:25:01,920 --> 01:25:04,999
Now, maybe you have a hypothesis that it is and you'd have to think through that.

650
01:25:05,000 --> 01:25:13,670
But we're going to for this example, we're going to say, no, this isn't really related in any way, so we don't technically have to adjust for it.

651
01:25:13,670 --> 01:25:20,030
It's not a confounder by either definition that we've considered, but we do know it's related to blood pressure.

652
01:25:20,030 --> 01:25:23,490
And so we might want to adjust for this just to have a more precise measure.

653
01:25:31,360 --> 01:25:36,490
Okay. So the big question, should you control for everything just to be on the safe side?

654
01:25:39,820 --> 01:25:45,550
Now everybody say, no, no, this is very tempting.

655
01:25:45,790 --> 01:25:50,170
When you first start doing research and why is it tempting? Because everything is correlated.

656
01:25:51,520 --> 01:25:58,360
And so if you go into your analysis thinking, I'm going to control for everything that's correlated with my exposure and my outcome,

657
01:25:58,900 --> 01:26:07,990
you are going to end up with a crap tastic model that possibly is going to induce bias actually, and make it worse.

658
01:26:08,890 --> 01:26:16,300
So you do not want to control for every one of these very carefully about what it is that you control for in your model.

659
01:26:17,590 --> 01:26:22,780
And one of the reasons we want to control for everything is because we don't know which things to control for.

660
01:26:23,590 --> 01:26:28,270
And so we might think if I don't know if I'm not sure it's safer to put it in.

661
01:26:29,440 --> 01:26:32,710
So that's not necessarily true. So.

662
01:26:33,850 --> 01:26:41,830
What happened there is weird. This is where Dags can come in and be helpful.

663
01:26:43,270 --> 01:26:47,049
I'm not going to say very much about them. I know you've learned about them in other classes.

664
01:26:47,050 --> 01:26:48,670
I'm sure you know more about them than I do.

665
01:26:49,990 --> 01:26:58,040
But essentially, they're used to depict statistical associations implied by a given set of assumptions about the causal structure among variables.

666
01:26:58,070 --> 01:27:00,910
That's what we've been doing here today with these examples.

667
01:27:01,330 --> 01:27:06,040
We've been thinking about the causal relationship between the set of variables, and that's what you're doing.

668
01:27:06,040 --> 01:27:11,080
When you create a DAG, you're drawing arrows that imply causal relationships.

669
01:27:13,280 --> 01:27:19,280
And what a dog is very useful for is identifying a sufficient sort of covariance

670
01:27:19,610 --> 01:27:23,990
that allow you to estimate the uncanny IT association between exposure and outcome.

671
01:27:25,340 --> 01:27:28,550
So you can put 30 variables in your day.

672
01:27:29,390 --> 01:27:36,530
You can draw all of the relationships between them. And then it can help you figure out which of those things can you draw?

673
01:27:36,530 --> 01:27:41,330
Which of those things do you not have to control for? Which of those things should you not control for?

674
01:27:42,200 --> 01:27:47,270
And so it's very helpful in that way. Yeah, that's the last point.

675
01:27:47,280 --> 01:27:51,050
They can help you determine what not to control more, which is very, very helpful.

676
01:27:51,530 --> 01:27:56,959
So it can become extremely complicated to make a DAG.

677
01:27:56,960 --> 01:28:02,870
If you have more than three or four variables for four variables, you can do it by hand and figure it out.

678
01:28:02,870 --> 01:28:06,920
But if you start getting a lot of variables, it becomes extremely complicated to do.

679
01:28:07,190 --> 01:28:10,669
And that's where these kinds of like computer programs can be helpful.

680
01:28:10,670 --> 01:28:16,100
So if you've never tried Dag, it's free. It's worth playing around with.

681
01:28:17,700 --> 01:28:25,850
And also, the website has some more resources, you know, about tags, the terminology, applications, functions.

682
01:28:27,450 --> 01:28:35,430
So the drugs are powerful and helpful in epidemiology, but there are things that they can't do for you.

683
01:28:38,500 --> 01:28:44,620
So they cannot tell you whether your assumptions about causal structure are correct.

684
01:28:52,810 --> 01:28:58,690
If you said that something is a common cause, but it's actually a mediator, the DAG won't tell you you're wrong.

685
01:28:59,410 --> 01:29:07,270
It just accepts whatever you tell it. It can't tell you whether you include at all possible common causes.

686
01:29:07,810 --> 01:29:11,170
Again, it's only operating on the information that you're providing it.

687
01:29:11,530 --> 01:29:18,220
So if there's an unmeasured confounder that you haven't included in the DAG, it can't tell you what to do about that.

688
01:29:19,870 --> 01:29:27,820
I can't tell you whether an effect is big or small, whether it has clinical significance or not that.

689
01:29:33,290 --> 01:29:37,580
And it can't tell you whether your concepts were operationalized correctly.

690
01:29:38,900 --> 01:29:44,360
So here's where we get back into that, the misclassification bias kind of thing.

691
01:29:44,870 --> 01:29:47,960
So a drag can't overcome measurement problems.

692
01:29:48,230 --> 01:29:51,890
So this is a real example from a paper that I used to assign in a different class.

693
01:29:52,820 --> 01:29:57,920
There was this paper where they wanted to study the effects of short birth interval on pregnancy

694
01:29:57,920 --> 01:30:03,740
outcomes under the hypothesis that if you had a very short amount of time between births,

695
01:30:04,100 --> 01:30:07,790
that that would be associated with worse outcomes in that second pregnancy.

696
01:30:08,660 --> 01:30:13,820
But they defined short birth interval as less than ten years between births.

697
01:30:15,140 --> 01:30:20,550
I'm not making this up. This is an actual published paper. It's absurd, right?

698
01:30:20,570 --> 01:30:24,500
Like that doesn't make any biological sense or social sense.

699
01:30:24,980 --> 01:30:33,379
Right. So maybe their dad was right. But they're operationalizing short birth interval in this way.

700
01:30:33,380 --> 01:30:38,150
That makes absolutely no sense. And so this isn't going to help you avoid that.

701
01:30:39,730 --> 01:30:41,560
Okay. So last thing before we take a break,

702
01:30:42,940 --> 01:30:51,560
can we ever say that our exposure caused our outcome in observational research, which most of social biology has?

703
01:30:51,580 --> 01:30:55,270
There's some natural experiments and things, but not many.

704
01:30:58,760 --> 01:31:05,630
Can we ever say that our exposure caused our outcome? Let's see a couple of those.

705
01:31:07,340 --> 01:31:20,960
Yeah, we, we, we shouldn't, um, we can never really be sure that we've accounted for all confounders.

706
01:31:22,090 --> 01:31:31,340
Um, so in this case, I'm saying genetic factors may be a confounder not of that, not of this particular one, but they could be for another example.

707
01:31:32,840 --> 01:31:37,790
But it could be other things besides genetic factors. There could just be unmeasured things we haven't.

708
01:31:38,090 --> 01:31:41,360
So it could be things we haven't even thought of to include.

709
01:31:41,360 --> 01:31:45,589
It could be things that we want to include, but we literally don't have a measure of it.

710
01:31:45,590 --> 01:31:54,070
We don't have genetic data in our study. We can never be sure that we've identified all the potential confounders.

711
01:31:54,640 --> 01:31:57,129
We can also have residual confounding, right?

712
01:31:57,130 --> 01:32:03,070
So maybe we did control for something, but because we measured it poorly, we still have residual confounding.

713
01:32:03,760 --> 01:32:13,450
So let's say the age is the confounder and we measure age as under 50 or 50 and that may not be

714
01:32:13,450 --> 01:32:18,250
sufficient to account for the confounding by age because we've measured age really important.

715
01:32:18,850 --> 01:32:21,850
So we can have unmeasured confounding, we can have residual confounding.

716
01:32:22,060 --> 01:32:25,930
And so what do we do? We avoid using causal language.

717
01:32:27,160 --> 01:32:34,450
So we don't say cause we don't see influence, we don't see change, we don't say promote.

718
01:32:34,660 --> 01:32:41,290
And if we do, we get in trouble from our advisor or from the journal editor or the reviewer.

719
01:32:45,680 --> 01:32:49,790
So instead we we use this language like association.

720
01:32:52,230 --> 01:32:59,490
Right. And I think one of the unintended consequences of this I mean, this is why we do it, right?

721
01:32:59,640 --> 01:33:01,290
There's a legitimate reason for doing that.

722
01:33:01,920 --> 01:33:07,379
But I think sometimes it makes us forget that what we how we started the class, what do we actually care about?

723
01:33:07,380 --> 01:33:11,730
We actually care about causes. We don't actually care about associations.

724
01:33:12,330 --> 01:33:16,910
But we can't ever say that something caused something else, which is just frustrating, right?

725
01:33:16,920 --> 01:33:22,410
When that's what you care about. So you've just got to keep this tension in your mind.

726
01:33:22,740 --> 01:33:27,210
But yes, we care about causes. That's what we want to understand in epidemiology.

727
01:33:27,480 --> 01:33:34,139
But we're limited in observational studies, in our ability to say that something caused something else.

728
01:33:34,140 --> 01:33:37,200
And so we have to always have that level of caution here.

729
01:33:37,590 --> 01:33:45,540
And what we're trying to do when we want to eliminate bias, all these forms of bias that we talked about,

730
01:33:45,840 --> 01:33:53,760
we want to make sure that that association that we're describing is right and that if indeed it is causal,

731
01:33:53,760 --> 01:34:00,240
which we can never probably really know that, then it would make sense to like to intervene upon that or act upon it.

732
01:34:01,440 --> 01:34:08,159
So it's just a little bit of a tricky thing that we have to hold in our head, our heads as epidemiologists, that we care about causes,

733
01:34:08,160 --> 01:34:15,140
but we can't always really or we probably can never identify them through most of the types of methods that we use.

734
01:34:20,640 --> 01:34:28,560
Okay. Let's take a break and then come back and we're going to talk about we're going to talk about conceptual model.

735
01:34:32,040 --> 01:34:35,490
Let's see. How about it's 1109? How about we come back at 1115?

736
01:34:54,870 --> 01:35:06,840
That's why. Names.

737
01:35:18,300 --> 01:35:29,570
No. It's lucky.

738
01:35:30,980 --> 01:35:39,020
I think what I am trying to say, I feel, is.

739
01:35:41,480 --> 01:35:48,240
I. Oh, it's awful.

740
01:35:56,100 --> 01:36:01,330
Because I don't about.

741
01:36:05,530 --> 01:36:09,630
Yeah. Yeah. But. I know, I.

742
01:36:22,370 --> 01:36:26,610
Yeah, yeah, yeah, yeah, yeah.

743
01:36:38,310 --> 01:36:43,050
Yeah. Right. Yeah, I know.

744
01:36:47,970 --> 01:36:51,660
And I decided to put.

745
01:36:59,870 --> 01:37:05,720
Things like that. Yeah. Yeah, like I said.

746
01:37:05,750 --> 01:37:13,120
So we had a. I this one.

747
01:37:51,480 --> 01:38:04,800
Monday. Anyway.

748
01:38:18,080 --> 01:38:28,850
Right here in. I.

749
01:38:33,680 --> 01:38:36,710
I have a meeting.

750
01:38:41,270 --> 01:39:33,060
Okay. They?

751
01:39:49,730 --> 01:39:56,320
You're going to. 6.3.

752
01:40:57,080 --> 01:41:00,530
Actually. I think.

753
01:41:14,080 --> 01:41:17,710
Yeah. You were there before.

754
01:41:32,610 --> 01:41:45,710
Okay. Let's talk about conceptual model. What do you think of when you think of a conceptual model?

755
01:41:45,740 --> 01:42:01,760
Like if you had to define it, what would you say it is? Is it the same thing as a DAG?

756
01:42:08,660 --> 01:42:14,810
Not exactly. But you think of it, maybe that is a type of conceptual model.

757
01:42:15,710 --> 01:42:22,640
Yeah, I think so. I mean, there are definitely like there's overlap between that and conceptual models.

758
01:42:23,780 --> 01:42:29,030
A conceptual model doesn't follow formal rules,

759
01:42:29,390 --> 01:42:38,299
whereas a dog follows formal rules and is intended to help you identify like what to adjust

760
01:42:38,300 --> 01:42:43,700
for in your model in terms of to address bias due to confounding like that's its purpose.

761
01:42:44,270 --> 01:42:50,840
A conceptual model doesn't have a purpose as specific as that, and it doesn't have rules.

762
01:42:51,260 --> 01:42:59,899
But the thing that's kind of that they share in common is that they're about helping you think through the constructs that you're interested

763
01:42:59,900 --> 01:43:07,040
in studying and the relationships between all of those like concepts are constructs that you're that you're interested in studying.

764
01:43:07,040 --> 01:43:17,869
So they share that in common. So in the Capilano and Daley article, they define a conceptual model as a tool for theory.

765
01:43:17,870 --> 01:43:24,739
Explication developed and used to make specific assumptions about a limited set of parameters and variables.

766
01:43:24,740 --> 01:43:33,170
So that sounds very much like a DAG, right? You're taking a sort of relatively small number of variables and you're sort of under

767
01:43:33,170 --> 01:43:37,490
you're making assumptions about the causal relationships between those variables.

768
01:43:38,570 --> 01:43:40,550
And they say that in a conceptual model,

769
01:43:40,640 --> 01:43:48,600
the assumptions that you make are then systematically explored and tested on a limited set of outcomes by a particular method or methods.

770
01:43:53,120 --> 01:43:59,449
And then Krieger says models attempt to portray how connections occur and are always

771
01:43:59,450 --> 01:44:04,310
constructed with elements and relationships specified by particular theories.

772
01:44:04,670 --> 01:44:11,420
Remember, we found that like when we're doing a DAG, or if we're doing a conceptual model that,

773
01:44:11,870 --> 01:44:17,029
um, that we're, we're talking about how these things are causally related to one another.

774
01:44:17,030 --> 01:44:23,480
We're not like the nature of the relationship between these concepts and that we're,

775
01:44:24,530 --> 01:44:32,120
that we're doing this based not just on what we see in the paper, but on the theory.

776
01:44:36,390 --> 01:44:42,870
So you can do a conceptual model without having a single data point.

777
01:44:43,680 --> 01:44:46,680
You don't need data to make a conceptual model.

778
01:44:54,620 --> 01:45:00,049
So then you might wonder, how do I use a theory?

779
01:45:00,050 --> 01:45:03,110
Or Here is to develop a conceptual model.

780
01:45:04,190 --> 01:45:09,960
You're going to do this for your your final assignment. You're going to have a conceptual model that you present.

781
01:45:20,000 --> 01:45:26,630
And this is I suppose you could have a conceptual model that only works, but I would just encourage that.

782
01:45:27,980 --> 01:45:33,860
You know, a conceptual model is really a picture, some kind of drawing that helps you understand the relationships.

783
01:45:35,270 --> 01:45:40,190
So the first thing you're going to do is identify the important constructs that you're interested in.

784
01:45:40,760 --> 01:45:47,660
So like, let's imagine that we're using social stress theory to develop a conceptual model.

785
01:45:50,360 --> 01:45:54,470
Some of the constructs in there might be things like social status.

786
01:45:55,540 --> 01:45:58,750
Stressors. Stress buffers.

787
01:45:59,260 --> 01:46:07,770
Health outcomes. Right. We're probably for a conceptual model, going to get a little more specific than that.

788
01:46:07,840 --> 01:46:11,829
Right. So we might have a specific social status that we're interested in.

789
01:46:11,830 --> 01:46:16,130
And let's say we're interested in race.

790
01:46:24,950 --> 01:46:30,620
Was the specific stressor that we might be interested in studying for a stressor.

791
01:46:35,750 --> 01:46:39,100
Yeah. Francis. Say it again.

792
01:46:39,100 --> 01:46:45,710
Racism. Racism. Okay. Discrimination.

793
01:46:51,360 --> 01:46:57,650
What's the stress buffer we might have to look at? Yeah.

794
01:46:58,680 --> 01:47:05,490
Social support. And what the health outcome that we might want to look at.

795
01:47:13,950 --> 01:47:19,120
Yeah. Depression. That's a great. Okay.

796
01:47:19,930 --> 01:47:23,530
So we've identified our important constructs, right? We took a theory.

797
01:47:23,770 --> 01:47:30,400
We identified the important constructs. Now we want to detail the causal flow.

798
01:47:32,110 --> 01:47:35,980
All right. So what comes first? In this example.

799
01:47:51,380 --> 01:47:54,980
Grace. So Grace comes first.

800
01:47:59,130 --> 01:48:03,780
What does race cause was so complicated that we're just going to go with it?

801
01:48:04,020 --> 01:48:13,230
What does race cause? Racism cause racism, but race discrimination.

802
01:48:19,290 --> 01:48:23,850
You might be exposed to discrimination because of your race or people's perception of your race.

803
01:48:25,050 --> 01:48:38,600
Okay. Where does social support come in? Permanent your social support.

804
01:48:48,790 --> 01:48:54,040
Yeah. I think maybe race can also relate to social support and.

805
01:49:03,690 --> 01:49:06,920
And the support also. No word.

806
01:49:11,770 --> 01:49:13,960
If you had more social support,

807
01:49:14,980 --> 01:49:25,620
but it lower your exposure to social support or lower your vulnerability to maybe lower no one ability to get depression.

808
01:49:27,540 --> 01:49:36,690
Yeah. Okay. So we in this model race leads to discrimination, leads to depression.

809
01:49:37,470 --> 01:49:44,640
Race can influence social support. And social support can modify the effect of discrimination on depression.

810
01:49:47,320 --> 01:49:51,340
Does that makes sense? So that would be our this would be.

811
01:49:51,490 --> 01:49:56,830
This is not very pretty. But you can make this prettier and this would be your conceptual model.

812
01:49:58,330 --> 01:50:02,590
So what's your exposure here?

813
01:50:14,320 --> 01:50:20,410
If this was your model and you now want to move toward analyzing it, what's your exposure here?

814
01:50:23,380 --> 01:50:30,850
Okay. So it could be discrimination. What's your outcome?

815
01:50:35,230 --> 01:50:46,850
Sorry. Social support is an effective modifier or moderator in this case.

816
01:50:48,880 --> 01:51:04,640
And so what is race? What do you think of.

817
01:51:12,760 --> 01:51:19,090
Okay. So if if discrimination is our exposure, then this might be a confounder.

818
01:51:19,750 --> 01:51:23,350
If we had a if we had an arrow to depression to.

819
01:51:23,350 --> 01:51:28,060
Right. Which we probably would. So this could be a confounder.

820
01:51:30,590 --> 01:51:33,680
It could also be an exposure. Right.

821
01:51:34,190 --> 01:51:38,570
In that case, this race could be your exposure. What would discrimination be?

822
01:51:39,950 --> 01:51:45,960
A mediator. You can do it either way.

823
01:51:46,260 --> 01:51:49,680
Right. Depends on how you're asking the question.

824
01:51:56,590 --> 01:52:04,240
So we already did. Step three, we detailed the causal relation using arrows that these things were causal.

825
01:52:09,110 --> 01:52:16,980
And then we might illustrate the direction of the so we might say higher discrimination associated with more depression.

826
01:52:18,830 --> 01:52:31,160
We have to be more specific here about what we mean by race and discrimination, but we might argue minoritized race.

827
01:52:32,430 --> 01:52:34,710
Positively associated discrimination.

828
01:52:34,980 --> 01:52:45,160
So now we're saying that these are the like these are the kind of not just the direction but the nature that they're not just the causal relationship,

829
01:52:45,160 --> 01:52:52,830
but the direction of association that we see. We might say less, although actually than people might say more.

830
01:52:53,590 --> 01:53:00,030
So you'd have to understand the literature on this to really, like, identify that one very well.

831
01:53:03,450 --> 01:53:09,569
So Pearland, based on theory, this is a great example.

832
01:53:09,570 --> 01:53:12,000
Based on theory. We would say this, right?

833
01:53:12,750 --> 01:53:20,820
Perlin said that low social status is associated with less access to coping social buffers, social stress buffers.

834
01:53:21,510 --> 01:53:26,790
But it could be that there is data that suggests that's not actually true in this case.

835
01:53:27,270 --> 01:53:32,280
And then we'd have to think through what that meant, that implications of that for for our model and for testing the theory.

836
01:53:33,890 --> 01:53:38,810
Okay. So that's how you would use a theory to develop a conceptual model.

837
01:53:41,840 --> 01:53:46,280
Here's another example. So what we do here in our example, this was the theory.

838
01:53:49,070 --> 01:53:53,540
This was the conceptual model and at that level, on that level.

839
01:53:53,540 --> 01:53:58,130
So we took the kinds of things, really big concepts from the theory,

840
01:53:58,370 --> 01:54:04,820
and then we brought them down to a slightly lower level here in our conceptual model.

841
01:54:06,740 --> 01:54:14,570
And so this image is just showing that there's these different levels going from the highest and most general framework.

842
01:54:14,750 --> 01:54:20,850
And there example all the things in the social environment impact individual responses,

843
01:54:20,900 --> 01:54:27,320
which then impacts health and function framework example extremely general down to a theory

844
01:54:27,320 --> 01:54:32,300
that's more specific that resources in the social environment influence health behaviors,

845
01:54:32,690 --> 01:54:36,610
which then influence health outcomes and then down to a conceptual model,

846
01:54:37,010 --> 01:54:43,940
more specific and measurable neighborhoods with quality effects, dietary practices which affect obesity.

847
01:54:43,940 --> 01:54:48,110
So these are all examples of mediation kind of models.

848
01:54:51,160 --> 01:54:54,940
And do you see how they're all kind of the same thing? They're just different levels of specificity.

849
01:54:59,750 --> 01:55:07,430
So now I want to talk about the difference between a research question, a study and a hypothesis,

850
01:55:08,270 --> 01:55:16,280
because I want you to be thinking about this kind of framework when you're working on your paper, your kind of grant proposal.

851
01:55:16,730 --> 01:55:22,200
So a research question. It's just it's very broad.

852
01:55:22,200 --> 01:55:25,440
It's a general question that you're interested in studying.

853
01:55:33,150 --> 01:55:38,520
So maybe in this case, if this is your exposure and this is your outcome,

854
01:55:38,520 --> 01:55:43,800
your research question might be, I want to understand how race is related to depression.

855
01:55:46,740 --> 01:55:53,310
I want to know if some race groups have higher or lower levels of depression than other groups.

856
01:55:55,990 --> 01:56:02,380
Or your question may really be, I want to know how discrimination is related to depression, if that's your exposure.

857
01:56:03,280 --> 01:56:06,910
So you can have slightly different research questions for that same conceptual model.

858
01:56:09,240 --> 01:56:14,940
Your aim is a research objective based on the research question.

859
01:56:17,900 --> 01:56:25,550
Aims are what are included in the NIH grant applications and other types of grant applications.

860
01:56:28,770 --> 01:56:33,250
So an aim usually has action words and it's.

861
01:56:36,610 --> 01:56:39,790
Olivia, you've been working on your cave proposal, so you know. That's right.

862
01:56:40,150 --> 01:56:43,930
Thinking about the importance of specific themes and how to phrase those.

863
01:56:44,170 --> 01:56:48,970
I'm going to give you examples, too, so that this is more concrete than a hypothesis.

864
01:56:51,760 --> 01:57:01,870
Is a provisional conjecture or tentative assumption about the causes or relation between phenomena and they can be directional or not directional.

865
01:57:06,160 --> 01:57:13,130
So I can say just discrimination is positively associated with symptoms of depression.

866
01:57:13,420 --> 01:57:17,620
Or I can just say my hypothesis is that discrimination is associated with depression.

867
01:57:18,070 --> 01:57:35,230
It's not directional. Now for your project, you may have all of you may have a research question.

868
01:57:35,830 --> 01:57:43,959
You may have and you may have hypotheses or you may only have a research question.

869
01:57:43,960 --> 01:57:51,660
And even there may not be enough evidence for you to have a hypothesis yet.

870
01:57:51,670 --> 01:57:57,549
So you may be doing work that's more exploratory. If it's a more established area of research,

871
01:57:57,550 --> 01:58:05,860
you might be able to generate a hypothesis based on previous studies so you don't have to have all of them for a study.

872
01:58:06,750 --> 01:58:10,200
Okay. So we'll show you some examples.

873
01:58:14,260 --> 01:58:18,640
Research questions, aims and hypotheses, and we can draw these out if it's helpful.

874
01:58:29,200 --> 01:58:36,070
So my first question is do adverse childhood experiences affect physical health in midlife?

875
01:58:43,190 --> 01:58:47,570
So that's the general question I'm interested in. And then I have an aim.

876
01:58:48,020 --> 01:58:52,309
My aim, which is more specific than my question,

877
01:58:52,310 --> 01:59:00,560
is to examine the association between adverse childhood experiences and blood pressure among adults age 40 to 65.

878
01:59:03,960 --> 01:59:13,230
Do you notice that I use causal language in my quest, but in my aim, I move to, like, association, right?

879
01:59:13,230 --> 01:59:20,210
Because this is becoming more closer to what I'm going to actually do.

880
01:59:23,900 --> 01:59:31,950
Then I have these hypotheses that say what I expect the nature of this relationship between adverse childhood experiences and blood pressure to beat.

881
01:59:32,360 --> 01:59:38,419
So I say adults who report a greater number of adverse childhood experiences will have higher systolic blood

882
01:59:38,420 --> 01:59:44,180
pressure and higher diastolic blood pressure than those who report fewer adverse childhood experiences.

883
01:59:44,780 --> 01:59:50,239
So you're in fact, I hope that you're starting to see how this is totally testable.

884
01:59:50,240 --> 02:00:00,350
Right. Like, once I get down to the hypothesis part, it's really clear what my exposure and my outcomes are then hypothesis too.

885
02:00:00,860 --> 02:00:09,290
Okay. So I said aces, systolic blood pressure, diastolic blood pressure.

886
02:00:09,680 --> 02:00:13,190
And I said more is higher blood pressure.

887
02:00:13,520 --> 02:00:24,380
Okay. And then my second hypothesis is that this association will be partially mediated by symptoms of depression.

888
02:00:28,850 --> 02:00:36,559
That would imply this that aces are associated with increased symptoms of depression

889
02:00:36,560 --> 02:00:40,670
and increased subtypes of depression are associated with higher blood pressure.

890
02:00:42,430 --> 02:00:50,710
But make sense. Okay. So then a second example got a little wordy on this one.

891
02:00:51,400 --> 02:00:54,580
Does the death of a spouse affect mental health?

892
02:00:56,460 --> 02:01:02,910
So now, instead of having one aim with two hypotheses, I have two aims and three hypotheses.

893
02:01:02,910 --> 02:01:07,570
So again, this can vary depending on your question, what you're interested in.

894
02:01:07,590 --> 02:01:13,440
So my first aim would be to determine whether symptoms of depression increase following the death of a spouse.

895
02:01:13,440 --> 02:01:17,580
So now you can see that I'm probably looking at longitudinal data here.

896
02:01:19,440 --> 02:01:24,329
And so in my hypothesis I see married respondents who experience the death of a spouse between

897
02:01:24,330 --> 02:01:29,790
baseline and follow up will report a greater increase in symptoms of depression over time,

898
02:01:29,790 --> 02:01:34,620
compared to married respondents who do not experience the death of his term in the study period.

899
02:01:36,000 --> 02:01:45,090
Oftentimes these are awkwardly they don't sound pretty because you have to be really specific, right?

900
02:01:45,150 --> 02:01:48,690
You've got to make sure that you're saying like, what is the reference for you?

901
02:01:51,670 --> 02:01:55,900
So it's okay for these to be kind of wordy and clunky,

902
02:01:56,500 --> 02:02:01,270
and you probably will have to play around with them a little bit to get them to really convey what it is you're trying to do.

903
02:02:01,810 --> 02:02:08,950
Second aim of this study to determine whether the association between spousal loss and symptoms of depression depends on gender and age.

904
02:02:09,340 --> 02:02:17,979
So here I'm hypothesizing effect modification. So I'm saying that the association between death of a spouse and change in depressive symptoms will be

905
02:02:17,980 --> 02:02:24,639
greater for men and for women and greater for respond for younger respondents versus older respondents,

906
02:02:24,640 --> 02:02:36,969
those under 65 versus those 65 plus. And all of these would be based on theory, and there's plenty of evidence in this field.

907
02:02:36,970 --> 02:02:43,400
So I would also use previous research to help me think through why I might expect this type of prediction.

908
02:02:45,870 --> 02:02:49,980
Okay. Last one, I promise. A third example.

909
02:02:50,220 --> 02:02:55,050
Do differences in diet help explain socioeconomic health disparities?

910
02:02:56,690 --> 02:03:02,600
This is a mediation question, right? Like you can already tell from just the question that this is going to be mediation.

911
02:03:03,260 --> 02:03:10,240
So my first aim is to document differences and fresh fruit and vegetable consumption among residents of Ann Arbor, Michigan.

912
02:03:12,580 --> 02:03:18,520
And so I hypothesize that respondents in the bottom quartile of the income distribution will report consuming fewer

913
02:03:18,520 --> 02:03:23,980
servings of fresh fruits and vegetables per week than respondents in the top quartile of the income distribution.

914
02:03:25,960 --> 02:03:33,160
And then my second aim would be to examine the association between fresh fruit and vegetable consumption and the odds of beer for self-rated health.

915
02:03:33,520 --> 02:03:38,319
You can see the hypothesis Do I ever invest?

916
02:03:38,320 --> 02:03:44,139
Example Am I ever calculating, direct and indirect effects?

917
02:03:44,140 --> 02:03:47,170
And I'm really testing mediation in this example.

918
02:03:52,580 --> 02:04:04,340
Not even though I have what is ultimately a question about the mission, and there may be various reasons why I would do that.

919
02:04:07,460 --> 02:04:13,100
It could be that I don't have data that allow me to meet the assumptions of formal mediation models.

920
02:04:13,820 --> 02:04:27,140
And so what I'm trying to show. The US diet.

921
02:04:29,790 --> 02:04:32,960
The disparities. How?

922
02:04:35,450 --> 02:04:39,769
And 71 fresh fruits and vegetables.

923
02:04:39,770 --> 02:04:47,280
So this is in one. I've been too busy suffering.

924
02:04:49,200 --> 02:04:55,390
They have to. So in this example, I never look at this.

925
02:04:58,240 --> 02:05:01,870
And I never actually calculate like I never calculate the total effect.

926
02:05:01,870 --> 02:05:10,779
I never calculate the direct and indirect effect. But if these are true, then it would suggest that there is mediation.

927
02:05:10,780 --> 02:05:19,260
And so that could be a future analysis. Okay.

928
02:05:19,350 --> 02:05:23,730
So now I want you to try it in your small group.

929
02:05:26,650 --> 02:05:31,360
Get together and just talk about your initial idea for your research proposal.

930
02:05:31,720 --> 02:05:37,060
You know, it doesn't have to be perfect. Just talk to each other about what it is that you think you want to do.

931
02:05:37,570 --> 02:05:43,030
And after you've had a few minutes to think through and talk to each other about your idea,

932
02:05:43,320 --> 02:05:50,350
to write a draft campaign based on your research question and try to draw a really basic conceptual

933
02:05:50,350 --> 02:05:55,220
model that shows the relationships between the variables that you kind of have in your.

934
02:05:59,270 --> 02:06:03,589
Okay. So I didn't put the group 10th out because I messed it up every time.

935
02:06:03,590 --> 02:06:08,180
So I'm going to let you do it. So I left them here. So group one, two, three, four, five, six.

936
02:06:08,750 --> 02:06:16,060
Somebody come get your ten and then go where there's room because it'll probably work better than what I.

937
02:06:32,960 --> 02:07:02,750
Yeah. Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah.

938
02:07:03,680 --> 02:07:15,340
I was just like. You are just like that.

939
02:07:15,790 --> 02:07:29,340
Sorry. I suppose I have.

940
02:07:46,710 --> 02:07:54,220
Yeah. So yeah, I'm glad I'm saying this now because I'm going to be gone forever.

941
02:07:55,550 --> 02:08:00,100
Okay, I have a trailer and. Okay, let me go ahead and write it on your thing.

942
02:08:00,100 --> 02:08:10,720
So I don't have no problem, I think is the one that I know that we're all very focused on the start of the party.

943
02:08:15,910 --> 02:08:26,290
Yeah, yeah, yeah, yeah, yeah, yeah, yeah.

944
02:08:27,440 --> 02:08:36,870
Oh, yeah, yeah. No, no, no, no, no.

945
02:08:38,140 --> 02:08:45,950
Okay. That was the only.

946
02:08:45,950 --> 02:08:50,430
How do you guys do something when you're required?

947
02:08:50,680 --> 02:09:04,260
Yeah. That's.

948
02:09:13,000 --> 02:09:42,180
Here. You know, nymphomania or whatever you call them, then I think, you know.

949
02:09:45,250 --> 02:09:49,150
You know, not even the. Stuff.

