1
00:00:35,750 --> 00:00:44,420
Okay. So right before I start lecture today, I just wanted a couple a couple of quick little things.

2
00:00:45,380 --> 00:00:49,670
One is hopefully some people will be watching this on video since classes than today.

3
00:00:49,670 --> 00:00:52,700
But please respond to class evaluations.

4
00:00:54,200 --> 00:00:58,819
You know, I it's kind of important. Many of you said you're probably sitting here probably that people have responded.

5
00:00:58,820 --> 00:01:06,110
But if you haven't or maybe you can trade your friends because other classmates because I,

6
00:01:06,620 --> 00:01:10,939
I will try to bring in some goodies and maybe I'll make it extra special.

7
00:01:10,940 --> 00:01:23,830
If we get to 50% response rate, we're now at 17. So I will have some food and drink on on Wednesday for the presentations.

8
00:01:24,830 --> 00:01:30,740
And with regards to the presentations, basically we have about 10 minutes to present.

9
00:01:31,010 --> 00:01:35,870
So that's basically probably just enough time to sort of give a brief overview and your results.

10
00:01:36,290 --> 00:01:38,960
You probably can't get too much into the depths of what you're doing.

11
00:01:39,830 --> 00:01:47,719
That would be a couple of minutes for maybe some brief discussion and switching around so that, you know,

12
00:01:47,720 --> 00:01:52,130
we'll try to get right started it right at 1130 and probably go to those close to one as we can.

13
00:01:53,900 --> 00:01:58,790
And along those lines of five headed when issue was the number was sort of small

14
00:01:58,790 --> 00:02:03,050
enough that taking two classes seemed excessive and one class is a bit tight.

15
00:02:03,560 --> 00:02:07,610
If anybody wants to volunteer to go on Monday, get it out of the way.

16
00:02:08,960 --> 00:02:13,550
Just the presentation. The paper would not be due until the end of the day.

17
00:02:13,550 --> 00:02:16,880
Wednesday, then that would be fine.

18
00:02:18,450 --> 00:02:24,530
I'll sit on that one out. Any takers? But if you do the advantage of that, when you have a little more time, we will have a little more time.

19
00:02:25,220 --> 00:02:32,740
And. People are stressed and if you're close to being done, then maybe atropine on Monday.

20
00:02:34,600 --> 00:02:39,730
Okay. So any questions about that presentation?

21
00:02:42,130 --> 00:02:50,830
We'll do it here. You can post stuff up onto campus and should be ready to, you know, sort of pick up and go.

22
00:02:51,960 --> 00:02:57,500
And so I think hopefully it'll be fairly. Fairly easy.

23
00:02:58,210 --> 00:03:02,770
And I guess given the time,

24
00:03:05,590 --> 00:03:12,180
I'll leave it up to you as to how you want to have how people if you want to just have 1% or if you want to have several people present,

25
00:03:14,050 --> 00:03:20,190
having a little bit of turnover would probably be good. But I also recognize that it's hard times.

26
00:03:22,130 --> 00:03:25,820
Okay. One in the order of 1020.

27
00:03:26,270 --> 00:03:30,830
Oh, yes, I will. I will produce a random order.

28
00:03:32,720 --> 00:03:46,260
That's a good point, though. So you will be will be given an order to go back.

29
00:03:47,750 --> 00:03:51,540
So. And.

30
00:03:53,320 --> 00:04:00,580
And if we have more than if anybody what chance we have more than one group volunteer for Monday local randomize that as well.

31
00:04:01,270 --> 00:04:04,400
So. Okay. All right.

32
00:04:04,990 --> 00:04:09,110
Anything else? All right.

33
00:04:09,120 --> 00:04:18,709
So we'll finish up our discussion of causal inference and time to event modeling.

34
00:04:18,710 --> 00:04:22,970
Today in particular is further discussion of hazard ratios.

35
00:04:23,960 --> 00:04:35,490
And this is just to review from from where we finished back on back on Monday that this argument in the

36
00:04:35,490 --> 00:04:44,030
Hearn and some of the Foley papers argued against the use of causal inference for a couple of reasons,

37
00:04:44,030 --> 00:04:47,599
one of which is the proportional hazard ratio will not be constant.

38
00:04:47,600 --> 00:04:51,050
So this sort of causal interpretation becomes particularly tricky there,

39
00:04:51,590 --> 00:04:56,750
although there are sort of easy, some sense, fairly straightforward fixes for that.

40
00:04:57,710 --> 00:05:01,260
Well, trickier is this change of the risk set required to estimate the hazard ratio.

41
00:05:01,290 --> 00:05:07,009
A given point doesn't account for the counterfactual survival risk. So while there has been some more advanced work,

42
00:05:07,010 --> 00:05:11,600
including some stuff that I work with student on this to try and deal with

43
00:05:11,600 --> 00:05:16,490
that to sort of bring hazard ratios back into back into a meaningful approach.

44
00:05:17,390 --> 00:05:29,270
Another approach is to simply focus on on the actual survival curve under the counterfactual treatment assignments,

45
00:05:29,690 --> 00:05:39,200
basically doing prediction out for each individual had they been assigned and had they not been assigned to the treatment based on their covariance.

46
00:05:40,170 --> 00:05:49,250
So to try to deal with the fact that that if you're sort of looking at observed survivors any point in time, the sort of the sort of.

47
00:05:53,430 --> 00:06:03,510
Exposure exposure effect. Where were the if they if they see the treatment or exposure is is negative,

48
00:06:04,470 --> 00:06:12,690
then the pool of survivors in the treated untreated group aren't really going to be comparable.

49
00:06:13,470 --> 00:06:16,709
And of course, the same thing is true if it's positive.

50
00:06:16,710 --> 00:06:23,460
Right. Just the other correction. So this is even if treatments randomize, you know, as long as there's been a treatment effect itself.

51
00:06:24,900 --> 00:06:35,230
So. So as I mentioned, their suggestion is to actually work on survival curves.

52
00:06:36,100 --> 00:06:40,770
So to get away from this sort of conditioning of this post-treatment variable, you know,

53
00:06:40,990 --> 00:06:45,820
by the time t t itself is actually the kind of the post-treatment version here.

54
00:06:46,630 --> 00:06:57,370
So by studying the street time hazard model, you know, possibly just adjusting for coverage.

55
00:06:58,420 --> 00:07:05,860
This is actually kind of important because to the extent this sort of measures, sort of intrinsic differences and fragility,

56
00:07:06,490 --> 00:07:14,110
these this is sort of where you get the ability to to account for this this this correction.

57
00:07:15,280 --> 00:07:22,179
And this this is just the sort of baseline hazard of the sort of change in

58
00:07:22,180 --> 00:07:27,820
risk that we're going to just estimate if over all the culverts here for zero,

59
00:07:27,970 --> 00:07:32,530
basically. So and then, of course, the the treatment itself.

60
00:07:36,100 --> 00:07:46,810
So basically fit this model and we can go back and get the prediction of survival up to time point T as a function of.

61
00:07:53,620 --> 00:08:03,160
Failure at time t given survival. T Must one have to match one given zero two months to all the way down to survival through the first time period.

62
00:08:03,940 --> 00:08:09,820
So effectively that's that's given by this.

63
00:08:13,690 --> 00:08:20,950
Product of one minus the risk of failure up to time, t minus one and the risk of failure 20.

64
00:08:22,120 --> 00:08:25,159
So essentially the denominator, right?

65
00:08:25,160 --> 00:08:29,950
And these long odds models would convert it back to probability.

66
00:08:29,950 --> 00:08:43,720
This is going to be one plus. E to when you're combination of coverage over the entire time frame and this one just comes into last time point.

67
00:08:50,950 --> 00:09:02,890
So we essentially go back and predict survival up to 20 based on covariates and whether or not they were in in the treatment or control group.

68
00:09:04,210 --> 00:09:09,460
Basically just didn't appear to had a component.

69
00:09:11,530 --> 00:09:15,220
And then you can then marginalize this by averaging over the sample.

70
00:09:17,090 --> 00:09:23,750
So this last step is basically standardizing the curves to the empirical distribution of the two countries in the study.

71
00:09:24,530 --> 00:09:29,360
So you get a martian survival curve in exposure. Another one under no exposure.

72
00:09:29,900 --> 00:09:36,920
Treatment and no treatment. You know, obviously, this could be extended if you have multiple treatments, right?

73
00:09:37,490 --> 00:09:45,440
Yes. You know, additional variables for the type of treatment they received and could do prediction for that.

74
00:09:47,750 --> 00:10:02,670
So. So time very hazard ratios can be accommodated by adding product terms between exposure and time of follow up,

75
00:10:03,810 --> 00:10:12,480
and various estimates can be to be obtained by by bootstrap, by making sure that you can keep the propensity score piece and the re sample data.

76
00:10:13,660 --> 00:10:20,170
I'm changed. And I guess I'm at a step ahead here because I got pretty scores.

77
00:10:20,190 --> 00:10:22,070
But. But right.

78
00:10:22,150 --> 00:10:35,139
If you have randomization that we don't have to worry about imbalance in baseline covariates but if we do we do have if we don't have that,

79
00:10:35,140 --> 00:10:39,520
then we can just go back to our usual inverse probability weighting. So.

80
00:10:40,560 --> 00:10:45,540
Yeah. To look at the probability of treatment versus control.

81
00:10:47,370 --> 00:10:51,060
Org scale, linear and covariates.

82
00:10:51,900 --> 00:11:03,090
Go there to get predictors of being in one treatment or the other and then take the reciprocal of that to get the weights.

83
00:11:05,250 --> 00:11:10,680
So basically then you use these weights back and.

84
00:11:14,620 --> 00:11:18,910
Fitting the screen time hazard model and also in the survival model.

85
00:11:19,210 --> 00:11:24,340
You don't need to worry about that here because this is an individual level measure. Right.

86
00:11:24,340 --> 00:11:30,040
So basically, you're looking at the prediction for an individual.

87
00:11:30,040 --> 00:11:33,190
So there's no you're not averaging across individual.

88
00:11:33,200 --> 00:11:36,680
So there's no need to be using weights and. Step.

89
00:11:38,160 --> 00:11:41,860
He didn't use them. They were just cancel. So.

90
00:11:45,950 --> 00:11:52,430
Okay. So questions about that before we look at the example.

91
00:11:57,780 --> 00:12:04,500
So barring a little recent example that I was involved in with some collaborators way back in Pennsylvania,

92
00:12:05,670 --> 00:12:13,140
that was 17 years ago when I still work with people there once in a while. So it's really a study of novice drivers in Ohio.

93
00:12:13,410 --> 00:12:23,580
These are people that work at newly licensed sort of just pre-COVID time here between July and December of 2019, under the age of 25.

94
00:12:24,180 --> 00:12:30,780
And they were followed for just under three, up to up to almost three years, in some cases less.

95
00:12:31,110 --> 00:12:34,230
And when they came in, basically in the follow period ended.

96
00:12:35,250 --> 00:12:40,650
So so this it's also just mentioning this is administrative censoring here.

97
00:12:40,860 --> 00:12:47,460
Right? We basically stopped calling people to a point in time because they want it to go on with the analysis and probably also ran out of money.

98
00:12:48,750 --> 00:12:54,090
So they so there's no reason to think about it forward of censoring being a problem here.

99
00:12:55,430 --> 00:13:05,130
So so this wasn't one of the sort of secondary issues was to sort of assess the age of of kind of licensure I'm sorry,

100
00:13:05,160 --> 00:13:10,080
the effective age at time of licensure. It wasn't like one of the main points of this.

101
00:13:10,080 --> 00:13:19,200
But for this example, I think focusing on that and and just looking at.

102
00:13:24,420 --> 00:13:32,100
60 to 70 versus 17 to 25. And I guess I could mention here the outcome is crash.

103
00:13:32,430 --> 00:13:37,680
It's the first crash that occurred. So we should make that clear.

104
00:13:43,620 --> 00:13:48,420
So we're following them until they get a police report. A crash? Not until to die.

105
00:13:49,110 --> 00:13:56,489
I don't know. Actually, there are a handful of deaths in this group, but the vast majority of crashes are not fatal or even serious injury.

106
00:13:56,490 --> 00:14:00,810
But. But they are serious enough to be reported to police.

107
00:14:00,930 --> 00:14:05,790
We. We got them. Is it any crash or is it just a crash that they're responsible for?

108
00:14:06,060 --> 00:14:10,590
No, as any crash. So the responsibility is initially questioned.

109
00:14:11,550 --> 00:14:17,100
There are ways of assigning that sometimes that are not always accurate.

110
00:14:18,730 --> 00:14:22,770
But we did we did look at it in any crash.

111
00:14:23,170 --> 00:14:28,320
If it had been police reported. So if there were crashes that were reported to police, we don't have them.

112
00:14:28,590 --> 00:14:33,760
But generally, if there's enough damage to involve an insurance company involved.

113
00:14:35,670 --> 00:14:48,810
So I'm like, in order to kind of keep this manageable, I just focused on a handful of covariates, basically gender.

114
00:14:49,380 --> 00:14:58,620
And then we had some, some very detailed measures about the sort of local environment with respect to socioeconomic status,

115
00:14:59,130 --> 00:15:03,960
transportation, urban history. So this is Ohio. So some places are rural, some places are urban.

116
00:15:05,790 --> 00:15:13,830
So, you know, there are some poor areas both in urban and rural areas and areas that are more middle class or upper class.

117
00:15:14,410 --> 00:15:18,959
In some places that have more there's a lot of car use everywhere,

118
00:15:18,960 --> 00:15:27,750
but there's some places where it's more and more reliant on cars and others usually associated with how urban areas.

119
00:15:28,620 --> 00:15:32,080
So we had a bunch of census tract measures on this one.

120
00:15:32,100 --> 00:15:34,680
We knew where these people lived so we can assign them with census tract.

121
00:15:35,190 --> 00:15:40,380
A census tract, by the way, is sort of an area that contains about five or 10,000 people.

122
00:15:40,860 --> 00:15:47,490
You can roughly think of it as a neighborhood or or maybe a Cook County or part of the county in a smaller and a more rural area.

123
00:15:50,100 --> 00:15:55,310
So so we basically have some summary measures of this.

124
00:15:55,320 --> 00:16:01,110
We we have sort of the top three principal components and some of you see have or haven't seen,

125
00:16:01,110 --> 00:16:10,380
but it's basically a way of sort of taking a linear combination of these of these measures and getting a summary value that sort of highlights.

126
00:16:12,360 --> 00:16:17,730
Certain where the differences might be the greatest. So you're sort of trying to explain the variance in these things.

127
00:16:18,480 --> 00:16:24,840
So so we might have a sort of a high socioeconomic status component of a low and then

128
00:16:24,840 --> 00:16:28,890
maybe one where you're sort of a intermediate or mixed sort of leadership distribution.

129
00:16:29,490 --> 00:16:31,410
So I want to get to the details now.

130
00:16:31,410 --> 00:16:39,630
You can just sort of think of it as a way of measuring these these features of where people are and isn't necessarily assigned to that individual.

131
00:16:39,660 --> 00:16:43,310
Right, because you can have poor people in rich areas and vice versa.

132
00:16:43,350 --> 00:16:47,429
But but it does say something about the environment and to some extent,

133
00:16:47,430 --> 00:16:53,760
probably with some probability of higher probability about them, about the nature of the young driver.

134
00:16:54,990 --> 00:17:06,750
So all right. So the bottom line on this was the probability of crash was lower among the younger novice drivers,

135
00:17:07,770 --> 00:17:11,280
which actually was interesting because historically that's not been true.

136
00:17:11,280 --> 00:17:17,820
But in recent years it has become more so. But the probability is is far from significant any point during follow up.

137
00:17:18,720 --> 00:17:23,610
So some of this could be due to unmeasured confounding.

138
00:17:23,790 --> 00:17:39,359
There's definitely a tendency for sort of more upper income teenagers to get to have access to to being able to get training,

139
00:17:39,360 --> 00:17:44,940
to get a license when they're when they're 16 versus versus at older ages.

140
00:17:46,110 --> 00:17:52,680
But there's also things that go other directions, because sometimes in more suburban urban areas, you may have access to public transportation.

141
00:17:53,340 --> 00:17:55,920
So certainly you don't bother the beginning license.

142
00:17:56,700 --> 00:18:02,220
So there are some differences in these groups with attempted to sort of sort that out mostly through these census tract measures.

143
00:18:04,170 --> 00:18:13,350
But there does seem to be some tendency possibly due to better training for the younger drivers because they in order to get a license at this age,

144
00:18:13,350 --> 00:18:22,580
you have to you have to get of some type of of training program.

145
00:18:22,620 --> 00:18:26,430
Whereas if you get to be older, particularly over over 18 years of age,

146
00:18:26,850 --> 00:18:32,550
you can there's no requirement that you just pass the driving test and then you get a license.

147
00:18:33,640 --> 00:18:38,460
So so there are three lines for each car.

148
00:18:38,790 --> 00:18:46,110
So, right. So this was an idea of what the dark line is, 16 to 17.

149
00:18:46,200 --> 00:18:51,680
So this is the rate at which they get crashes, right? And the green line is 17 to 25.

150
00:18:52,350 --> 00:18:56,610
And these are confidence intervals is a point most confidence intervals of the dotted line.

151
00:18:57,030 --> 00:19:04,859
It happens to be that that the the point estimate for the 16 to 17 year olds

152
00:19:04,860 --> 00:19:16,350
kind of covers the upper bound on sort of survival for the 1725 year olds.

153
00:19:17,160 --> 00:19:25,920
So that's a little a little confusing. I probably could have done this with some color coloring in right at the time, but.

154
00:19:27,340 --> 00:19:34,030
But I can kind of see is the the red line being the 16 and 17 year olds and the speed.

155
00:19:36,220 --> 00:19:40,260
For the 17 to 25 year olds. So.

156
00:19:41,220 --> 00:19:44,520
No, nothing close to significance. Any of these.

157
00:19:44,520 --> 00:19:48,050
So. So that.

158
00:19:51,150 --> 00:19:55,110
That suggests we're not seeing strong age effects per se.

159
00:19:57,610 --> 00:20:05,140
Okay. So I just posted some code here. Some of this you've seen before.

160
00:20:06,250 --> 00:20:10,960
But there is one little library I use called Discrete Survey. Discrete Survival.

161
00:20:11,020 --> 00:20:15,290
Sorry. So this actually is used to fit discrete time survival models.

162
00:20:15,310 --> 00:20:23,299
I didn't use all of the. Functions in that in that package.

163
00:20:23,300 --> 00:20:28,850
But there were a couple that I pulled out that were particularly easy to use the actual data set.

164
00:20:28,850 --> 00:20:34,130
I wish I could supply it, but it's it's kind of somewhat embargoed.

165
00:20:35,540 --> 00:20:40,670
So I didn't post it up. I had posted the code so you could conceivably use this for your own purposes.

166
00:20:41,780 --> 00:20:47,870
This is just sort of list of the variables. So this was just an idea.

167
00:20:48,740 --> 00:20:55,730
This is the either the follow up time, if it's censored or the time of crash.

168
00:20:56,510 --> 00:21:00,230
If it's not, I could have included location.

169
00:21:00,260 --> 00:21:02,659
A lot of that is picked up in these measures here.

170
00:21:02,660 --> 00:21:17,390
But I didn't just make my age group gender and then these these nine principal components, three for a second and third for each of these.

171
00:21:18,550 --> 00:21:27,130
Sort of measure types. And it's not, of course, age is.

172
00:21:30,630 --> 00:21:38,540
You know, I mean, it's not it's not in some sense. Actually, this sort of brings to a point this.

173
00:21:38,960 --> 00:21:45,710
So, I mean, we can't sort of think about manipulate years. We could imagine manipulating when you get your license, though.

174
00:21:46,130 --> 00:21:52,410
Right. So basically, you should think of this not this serious age, but it's when you got the license.

175
00:21:52,490 --> 00:22:05,150
Right. So probably getting a license when you're 16 or 17 versus 18, 16 under 17, 16 to 17 because you have to be 16 versus 17 or older.

176
00:22:06,610 --> 00:22:16,040
So so we just plugged in gender and then all of our census tract measures.

177
00:22:16,610 --> 00:22:19,950
I didn't get super fancy here to look for interactions and so forth.

178
00:22:19,970 --> 00:22:27,080
People could do that. And so that just gives me my usual first probability weights.

179
00:22:31,100 --> 00:22:39,680
So to compare the data for the discrete survival analysis, this is actually rated use, the discrete survival package.

180
00:22:41,570 --> 00:22:51,320
So I did a first of all, I wanted to describe my limits on this so I would do it every month.

181
00:22:52,280 --> 00:22:55,850
So I actually have time and days. So I'm doing it well.

182
00:22:56,360 --> 00:23:01,850
Under 30 pretty much is 30 days. So I had up to 33 months of follow up.

183
00:23:02,960 --> 00:23:16,100
So basically, I'm telling I want to have a No. 1 to 30, 31 to 60, 61 to 90 and so on and so forth, up to whatever 960 what I do.

184
00:23:19,250 --> 00:23:33,470
So, so this continuous discrete function, I tell it the object list or matrix and tell it which one of these is the time column.

185
00:23:34,190 --> 00:23:41,090
And then I tell it how I want the limits and it will then convert this in days to months.

186
00:23:44,510 --> 00:23:54,800
And then. Then I create a little thing here that adds on.

187
00:23:58,180 --> 00:24:02,760
The. All right.

188
00:24:03,510 --> 00:24:17,830
When? When the event. This. You're.

189
00:24:23,860 --> 00:24:28,160
When the event occurred. I knew that. I'm sorry.

190
00:24:30,140 --> 00:24:35,250
Huh? So this is this is just describing whether this is censored.

191
00:24:35,430 --> 00:24:40,410
Right. So I need I need this to say whether the event actually occurred or whether it was censored.

192
00:24:41,100 --> 00:24:45,270
So I put in a censoring variable. So I'm just switching now to becoming an event variable.

193
00:24:47,370 --> 00:24:58,050
And then. I am going to bring down my weight variable here because what I want to do is do this conversion now.

194
00:24:58,070 --> 00:25:09,170
So basically, if I were doing discrete kind event models, like so am I, right now I have this variable form.

195
00:25:12,150 --> 00:25:16,520
See, I have my time, I have my event.

196
00:25:19,510 --> 00:25:24,380
So. So.

197
00:25:28,300 --> 00:25:41,170
It's something like this. So I need to convert this to and then my new type I measure.

198
00:25:49,920 --> 00:26:00,240
Because it's a00. You know, because this was not censored.

199
00:26:00,410 --> 00:26:03,840
And no one. I have driven so.

200
00:26:05,890 --> 00:26:11,210
Actually, it's. So something might come.

201
00:26:15,940 --> 00:26:21,410
It's. So.

202
00:26:22,550 --> 00:26:36,700
All right. So I'm going to go and fit this now with my. Personalized lives or ordinary log log like generalizing her model where I have my.

203
00:26:39,960 --> 00:26:46,950
Little. My windows of follow up.

204
00:26:47,820 --> 00:26:51,240
And then we know whether an event occurred.

205
00:26:51,250 --> 00:26:55,940
And and by definition, it's a it's going to be at the last ballot period.

206
00:26:55,950 --> 00:27:01,470
If it occurs at all twice, it'll just be zero. So that's essentially what this.

207
00:27:04,530 --> 00:27:07,950
Did a long thing is doing here.

208
00:27:09,510 --> 00:27:15,060
So I've got to tell it like times are. And I'm going to tell it like that variable.

209
00:27:16,860 --> 00:27:31,880
And then I'm going to get out this. So.

210
00:27:33,240 --> 00:27:38,410
So I'm going to call this now. So this is what I'm going to plug into my.

211
00:27:41,830 --> 00:27:54,940
Logistic regression mentioned. And then I just converted this to two dummy variables for my exposure and.

212
00:27:56,970 --> 00:28:06,300
Gender and then I census variables here and I brought along this weight variable because I also needed it to be.

213
00:28:20,180 --> 00:28:26,749
To be included along with all the other exits here, France and. And I just make that point.

214
00:28:26,750 --> 00:28:35,129
So. All right.

215
00:28:35,130 --> 00:28:42,380
So this would be just carried over to attorney general because they are very.

216
00:28:53,690 --> 00:29:05,630
All right. But I wanted to convert this into long form, so.

217
00:29:09,160 --> 00:29:16,000
So this all this stuff is just pointing out the little fiddly thing here.

218
00:29:16,630 --> 00:29:22,180
I only had one crash from 31, so on.

219
00:29:23,230 --> 00:29:28,900
So in order to get these dummy variables to not act weird and when I get to logistic

220
00:29:28,900 --> 00:29:33,610
regression because I recently have no outcomes in that for that particular dummy,

221
00:29:34,030 --> 00:29:40,720
I just collapsed them into a single time period. So once through three are basically all true as a single block.

222
00:29:43,630 --> 00:29:51,170
So. And then finally it.

223
00:29:52,310 --> 00:30:07,040
Right. So this spits out. This is time variable D and I just noted how long that know to get my end for the long day.

224
00:30:09,160 --> 00:30:16,729
So I do want to put this in the survey. So I've got to have my survey designed, so I need to tell it again.

225
00:30:16,730 --> 00:30:21,950
There's no clustering here. It's actually meeting a point, these discrete time magic models,

226
00:30:21,950 --> 00:30:29,450
even though these IDs are repeated because this is basically a fake out the likelihood which doesn't include any clustering.

227
00:30:30,080 --> 00:30:33,990
So treat these as all independent to get the likelihood to work.

228
00:30:34,010 --> 00:30:40,850
Right. So when this idea is first around, it's actually a bit of an argument on that.

229
00:30:41,420 --> 00:30:45,800
It took a while to sort of sell that. Yeah, but but it's pretty clear.

230
00:30:46,730 --> 00:30:49,639
So no, definitely.

231
00:30:49,640 --> 00:30:57,680
But the variables in here, I'm just listing them in a data frame and then I'll tell them my weights because I don't have assignment of.

232
00:30:58,010 --> 00:31:03,110
We didn't randomly assigned people to wait for or get their license at age 16.

233
00:31:04,490 --> 00:31:18,680
So then I fit. I'm just you're going to use the logistic regression model of the logit link.

234
00:31:22,030 --> 00:31:27,310
And now he's just zero one, so I can put them in without any fuss.

235
00:31:27,790 --> 00:31:34,210
These are continuous variables I'm just going to put in. Well, again, this is a class sample.

236
00:31:34,510 --> 00:31:41,229
You could try to get fancier to make sure that you've got forms that could be, you know, polynomials and so forth.

237
00:31:41,230 --> 00:31:44,709
And here you could explore as well as interactions.

238
00:31:44,710 --> 00:31:49,450
But I'm not going to be going to let that go for now. Okay.

239
00:31:49,450 --> 00:31:54,010
So once I've got that fit, put it in this object here.

240
00:31:56,380 --> 00:32:10,600
I want to compute these, these CDF. So the failure probabilities to get my survival function so so I just put together my X matrix of of.

241
00:32:14,980 --> 00:32:18,340
But what I should hope and I am putting these in is dummy variables.

242
00:32:27,070 --> 00:32:31,270
So I'm going to start off with just my my age.

243
00:32:32,360 --> 00:32:44,770
So this piece here. I used the census tract and then I'm just creating some holders here.

244
00:32:45,550 --> 00:32:51,640
So basically my denominators for.

245
00:32:56,070 --> 00:32:59,360
I multiply these pieces together here?

246
00:33:01,230 --> 00:33:06,290
It's going to be putting them on one product after the other of this of this piece here.

247
00:33:13,080 --> 00:33:17,520
And then the actual. I'm sorry.

248
00:33:19,670 --> 00:33:22,990
But I'm actually going to hold them to the.

249
00:33:25,930 --> 00:33:30,700
Survivals are the probabilities of failure at a given time point. The whole product of this thing.

250
00:33:38,310 --> 00:33:41,560
These other matrices are obviously different ways to do this.

251
00:33:45,170 --> 00:33:49,479
I'm doing it if I want to create my little dummy variables here, right?

252
00:33:49,480 --> 00:34:02,379
So basically each per each time point, I'm going to create a column off of ones, right?

253
00:34:02,380 --> 00:34:11,050
Because this is essentially. And so when I would have computers to be the dummy girls, right?

254
00:34:11,050 --> 00:34:16,410
I sort of have this function there which. Right.

255
00:34:16,440 --> 00:34:27,710
So it's basically. I have. Five four to say hello to.

256
00:34:34,260 --> 00:34:56,870
So basically this would be. Right.

257
00:34:57,420 --> 00:35:12,700
And if. Right.

258
00:35:12,750 --> 00:35:18,690
So don't matrix in there. But now I want to compute this right now between predictions.

259
00:35:19,200 --> 00:35:24,150
So when I said everybody to be equal, one t equal.

260
00:35:27,500 --> 00:35:38,300
Well, there's a there's an intercept term, too. So my intercept terms always one and then for my dummy variable.

261
00:35:43,210 --> 00:35:50,320
That's right, because I think there's 31 of these all together, so on.

262
00:35:51,540 --> 00:35:54,880
So I end up with love computing these.

263
00:35:58,000 --> 00:36:01,390
Right. I want to set these. Don't be one, two, three, four. So one.

264
00:36:01,400 --> 00:36:05,330
So I just fill in these columns saying that I wanted to leave the rest to be zero.

265
00:36:06,740 --> 00:36:09,740
And then I put that all together.

266
00:36:19,110 --> 00:36:32,580
They are here to check. For now.

267
00:36:32,600 --> 00:36:36,260
My alpha testers for this session. I didn't do this stuff in the last time I taught the class.

268
00:36:38,840 --> 00:36:41,840
So. Right.

269
00:36:41,840 --> 00:36:49,909
So I want to combine my variables with my with my predictors,

270
00:36:49,910 --> 00:36:59,540
and I want to set treatment to be one because I want to do something to generate the survival function under those that are 16 to 17.

271
00:37:01,130 --> 00:37:05,930
So I compute the logit, right?

272
00:37:07,110 --> 00:37:14,130
Just as this piece here. And then I want one.

273
00:37:17,850 --> 00:37:25,020
Theologian Peace right is in this denominator part, and I keep ratcheting that up, so I save it.

274
00:37:25,620 --> 00:37:31,020
So the next time I'm going to multiply to one and 2 to 20, 23 and so on and so forth,

275
00:37:31,920 --> 00:37:40,140
and then for the actual probability, I want to take that denominator and plug e to the logic that.

276
00:37:43,250 --> 00:37:47,570
And the same thing down here. But now I'm starting treatment to zero.

277
00:37:50,610 --> 00:37:56,570
So that gives me essentially these. Computing.

278
00:37:59,930 --> 00:38:06,090
Each one of these. For an individual.

279
00:38:08,080 --> 00:38:23,780
Under treatment or under control. Starting point. So I didn't want to flip this around to be its final curve.

280
00:38:24,560 --> 00:38:29,780
So I want to take one minus that probability and compute that at each one of these times.

281
00:38:30,830 --> 00:38:40,610
So I start off with one. I keep multiplying by whatever my current value is, and I'm just saving these aside.

282
00:38:40,700 --> 00:38:50,180
Again, this is in each individual. So I'm going to put together survivals for each individual time, 1 to 2 and three that becomes this big.

283
00:38:52,050 --> 00:38:59,100
You see by just putting those columns across. So at the last step, I want to average.

284
00:39:03,200 --> 00:39:06,650
So that's this. This piece here.

285
00:39:08,180 --> 00:39:18,630
But I want to do weighted means. So.

286
00:39:21,440 --> 00:39:29,259
So I'm basically just to the quick trick of using matrix multiplication and then dividing by the sum of the weights,

287
00:39:29,260 --> 00:39:33,470
because I want to use that sort of stabilized estimate.

288
00:39:35,710 --> 00:39:41,580
And then for the plot, there's actually a little you know, you can use a step function option.

289
00:39:41,620 --> 00:39:44,880
I should have done know that for this.

290
00:39:45,520 --> 00:39:48,830
So that's how I got started having the lines at that little step functions.

291
00:39:50,450 --> 00:40:00,090
It actually looks like a survival curve. So then to get the confidence intervals, basically, I just rebooted everything.

292
00:40:03,510 --> 00:40:10,200
I mean, I just bootstrapped everything so that you compute the weights.

293
00:40:10,200 --> 00:40:16,530
I just bootstrap them and then plug it in and we compute everything all the way through.

294
00:40:19,680 --> 00:40:32,490
Right. And then basically get. Each one of these curves now, but at a particular point.

295
00:40:33,630 --> 00:40:37,890
So then I go through it each time.

296
00:40:37,890 --> 00:40:43,920
Point. Take all my bootstrap averages and take the 2.75, and that is the percentile approach.

297
00:40:44,940 --> 00:40:56,400
And then I save those percentiles so that this is the actual curve that I apply.

298
00:40:58,630 --> 00:41:02,020
But the confidence in the messenger.

299
00:41:03,880 --> 00:41:10,030
Okay. So questions about that and the three of them step by step enough to see.

300
00:41:14,050 --> 00:41:17,930
Okay. So.

301
00:41:25,400 --> 00:41:28,700
So apologies that were some errors on this last one.

302
00:41:28,700 --> 00:41:36,529
I just reposted it. So I've tried to make notes where I made some differences, but if somebody looks different,

303
00:41:36,530 --> 00:41:45,200
if you downloaded this last night rather than this morning, then you can let me know.

304
00:41:45,290 --> 00:41:56,850
I'll try to. Sure I catch that. So, okay, so for this last part of our discussion,

305
00:41:57,510 --> 00:42:03,070
we're going to pull back and we sort of touched on this a little bit in the multiple media setting, right?

306
00:42:03,420 --> 00:42:06,780
We allowed for this case for one meter to impact the other.

307
00:42:07,830 --> 00:42:17,790
So this sort of confounder mediator, confounder by indication. So if we have multiple treatments over time.

308
00:42:18,600 --> 00:42:21,839
So this is a little different this time to do that piece, because in the time to that piece,

309
00:42:21,840 --> 00:42:25,170
we still just have a single treatment or exposure which is following people over time.

310
00:42:25,620 --> 00:42:30,330
But now if we're taking an individual and we're hitting them with multiple treatments over time,

311
00:42:31,740 --> 00:42:35,819
then the measures of this intermediate outcome can essentially sort of take on this rule because

312
00:42:35,820 --> 00:42:41,190
the treatment at time t plus one may depend on the time on the outcome at time T plus one,

313
00:42:41,190 --> 00:42:45,690
which is the term based on the treatment of time teams, based on the outcome of 20 and so on and so forth.

314
00:42:45,690 --> 00:42:48,750
So I don't see the banks here.

315
00:42:51,240 --> 00:42:55,280
A lot of arrows. So this was sort of a single five point situation.

316
00:42:55,750 --> 00:43:09,220
And so I may have. Hopefully they have a clean situation where they don't get each other because of some of my family history in the first place.

317
00:43:10,330 --> 00:43:27,430
So. So in this case. I sort of think about some situation going along here.

318
00:43:33,950 --> 00:43:38,630
So this is some kind of equivalent to our outcome of real health and interest.

319
00:43:39,350 --> 00:43:54,630
This is the sort of interest. But if we have intervening treatments along the way.

320
00:43:57,250 --> 00:44:01,910
Right. So. Treating a patient.

321
00:44:02,000 --> 00:44:10,370
Come in. They come in and may decide that any treatments will continue the same treatment based on whatever we have here.

322
00:44:13,170 --> 00:44:18,930
And that could go on the effect, too. And of course, you might have some visual piece of that as well.

323
00:44:19,530 --> 00:44:34,620
Right. So. You know, just keep on going.

324
00:44:34,760 --> 00:44:38,640
Right. So by the time he gets out here.

325
00:45:06,080 --> 00:45:09,890
There could even be an impact on that decision.

326
00:45:11,630 --> 00:45:16,650
I want to hear your. So.

327
00:45:20,660 --> 00:45:28,790
So basically. If you just sort of start conditioning all these treatments without worrying about the fact

328
00:45:28,790 --> 00:45:34,760
that these intermediate outcomes or even earlier treatments could impact these assignments,

329
00:45:35,660 --> 00:45:46,220
then you basically have trouble, some sort of conditioning on on post treatment covariance that has been sort of the major focus of the class.

330
00:45:47,740 --> 00:45:57,590
So so we're going to try to come up with discussions of the causal inference, potential outcome setting in this more more complex setting.

331
00:45:57,740 --> 00:46:05,059
So and by the way, so I think a real driver example is actually somebody driving general proposal inference in water

332
00:46:05,060 --> 00:46:16,280
tanks started back to AIDS when people were sort of physically trying to find drugs or so on.

333
00:46:17,390 --> 00:46:25,130
So they weren't bothered some point between political pressure and just sort of common sense that we should be doing randomized trials for everything,

334
00:46:26,090 --> 00:46:36,020
given the desperate situation they left the medical community with, basically just start treatment and then let's see how some is doing.

335
00:46:36,680 --> 00:46:41,569
And they would try to change it. They would sort of fiddle with the treatments and so on and so forth.

336
00:46:41,570 --> 00:46:48,500
And one of the. So you can imagine a situation where maybe you don't associate with these drugs, but it's literally like this could have.

337
00:46:48,800 --> 00:46:52,580
And so we might withhold treatment until somebody got very sick.

338
00:46:53,660 --> 00:46:59,810
So or of the levels of treatment in if they were not responding.

339
00:47:00,470 --> 00:47:03,050
So you can imagine you can crudely look at this, right?

340
00:47:03,650 --> 00:47:08,390
These drugs are going to look terrible because they're basically being used on people that are sicker.

341
00:47:10,040 --> 00:47:14,770
So sort of the initial attempts to deal with this were like it was very hard to figure out.

342
00:47:14,960 --> 00:47:18,080
The people know that it's happening, obviously, but how do we account for that?

343
00:47:19,130 --> 00:47:32,380
And it actually drove a lot of sort of early. Focus in on, first of all, bringing the potential outcome framework and a clear vision,

344
00:47:32,770 --> 00:47:37,780
and then more generally about how to how do we sort of think about using this

345
00:47:38,530 --> 00:47:43,510
sort of prediction approaches that we can talk about to really come up with?

346
00:47:46,330 --> 00:47:51,490
So okay, so a little bit of notation here.

347
00:47:52,900 --> 00:47:57,990
So they tend to use bars in some of the papers here.

348
00:47:58,000 --> 00:47:58,809
It's a little confusing.

349
00:47:58,810 --> 00:48:10,210
It looks like a mean, but it actually means like essentially a vector of assignments or a vector of of variables more generally.

350
00:48:11,290 --> 00:48:15,660
So a treatment assignment then because it's variable over time, right?

351
00:48:16,060 --> 00:48:26,830
We have to we can actually talk about indexing this by T as well as I and then using the bar to point out its, you know, possible set covariance,

352
00:48:26,830 --> 00:48:32,320
which might be counterfactual here because we're going to allow sort of these early ys sometimes to be thought of as Xs.

353
00:48:33,670 --> 00:48:42,910
So we might have some sort of potential outcome or counterfactuals for these axes,

354
00:48:43,660 --> 00:48:47,260
but they're usually going to be sort of one step behind in terms of the treatment assignments,

355
00:48:47,260 --> 00:48:51,760
because we're assuming that the outcome can only be based on treatment.

356
00:48:54,520 --> 00:49:02,679
And then finally, our ultimate potential outcome of interest at time t is is going to be based on on these sort of treatment assignments.

357
00:49:02,680 --> 00:49:06,670
So you can see there's going to be more than two often potentially, maybe not.

358
00:49:07,030 --> 00:49:11,290
So depending what you want to focus on, at least in the general, most general case, always more than two.

359
00:49:12,910 --> 00:49:16,210
So as a matter of fact. Right.

360
00:49:16,540 --> 00:49:28,370
If we have. Two treatments at each time point we can have to the t choose to eat.

361
00:49:28,380 --> 00:49:32,040
This is like the simplest parts. The same sort of t is only to use it to follow ups.

362
00:49:32,040 --> 00:49:35,340
You actually end up with six. Right? So we could think of.

363
00:50:13,060 --> 00:50:14,830
So we can think about comparing outcome.

364
00:50:16,270 --> 00:50:37,030
If you receive treatment at both times versus only time, one of times versus any time to both times versus not at all.

365
00:50:38,980 --> 00:50:52,450
And of course, you could also use first versus second versus versus not at all.

366
00:50:58,000 --> 00:51:09,250
Second versus not at all. So so there are some favorites in here.

367
00:51:09,430 --> 00:51:16,540
One is, you know, sort of this can be thought of as sort of the most like if I treat that for the full battery treatments versus none.

368
00:51:18,450 --> 00:51:26,010
You know, sort of often maybe sort of comparing back to the control cases it's pretty common to so.

369
00:51:27,850 --> 00:51:34,390
And as we've seen a minute, if this gets to be three or four or five, you know, quickly this this quickly grows out of control.

370
00:51:35,110 --> 00:51:37,990
So you often going to be doing some kind of reduction.

371
00:51:38,920 --> 00:51:44,600
It might be that you assume that there's some sort of, you know, sort of linear, effective treatments.

372
00:51:44,770 --> 00:51:50,950
Right. So basically just doing linear regression in the number of treatments, possibly, maybe some sort of functions.

373
00:51:53,020 --> 00:51:58,090
But you have to sort of reduce this in some manner unless you have very large sample.

374
00:51:58,540 --> 00:52:06,730
And because if nobody falls into one of these categories, you're going to feel less than let's discuss what happens if the treatment is continuous,

375
00:52:06,820 --> 00:52:09,880
like in the example that people are getting like small amounts. Yeah.

376
00:52:10,630 --> 00:52:15,160
So often that would be that would be sort of binary ized.

377
00:52:15,850 --> 00:52:21,790
Otherwise you could, you could set up some kind of linear model and then you would sort of fix the value here for this.

378
00:52:22,570 --> 00:52:27,190
Right. So again, if you're sort of thinking about it as being possibly some linear function of that,

379
00:52:27,700 --> 00:52:30,880
then you could sort of describe it in a kind of more general fashion.

380
00:52:32,350 --> 00:52:35,470
So. So that you have a question.

381
00:52:35,710 --> 00:52:45,010
Okay. Yeah. Just on top of that, what why do we have some sort of continuous or in general a differentiation framework in calls in this case,

382
00:52:45,010 --> 00:52:49,420
why do we consider quite, quite useful?

383
00:52:49,870 --> 00:53:03,430
Well, very often they are, but it's also it's a way of of sort of being able to define the the potential outcome in a in a sort of simple fashion.

384
00:53:04,180 --> 00:53:09,800
I haven't gone into it too much, but it certainly is possible. But these we talk about a versus a prime.

385
00:53:09,820 --> 00:53:13,510
Right. Right. So you could set those to be a continuous measure.

386
00:53:15,580 --> 00:53:21,309
And the principle stratification approach, it's perhaps a bit trickier. Although, again, you can sort of imagine.

387
00:53:21,310 --> 00:53:26,310
You sort of. You could kind of sort of try to stratify on.

388
00:53:27,060 --> 00:53:35,790
But it becomes very counterfactual then because you're sort of doing a prediction sort of prediction space that you're that you're looking at there.

389
00:53:36,630 --> 00:53:43,790
So but but, yes, you can't you can't do continuous, continuous treatments.

390
00:53:45,240 --> 00:53:57,210
So. But in the end, the case often might have been that zero one like they were doing some watchful waiting and they kicked in.

391
00:53:58,140 --> 00:54:02,160
But there was no specific protocol for that. So it varied by doctor.

392
00:54:03,000 --> 00:54:07,770
And so you had some variability to look at when you were looking at these these estimates.

393
00:54:09,440 --> 00:54:20,610
So. All right.

394
00:54:20,620 --> 00:54:24,490
So there are several approaches. I'm going to sort of work for the two simplest here.

395
00:54:27,880 --> 00:54:31,940
One is basically inverse probability weighting. It's a very similar rule we've seen before.

396
00:54:31,960 --> 00:54:38,380
It is basically we sort of now have to extend this to the problem, look at the probability of treatment at each time,

397
00:54:38,380 --> 00:54:46,600
conditional on the previous observed data and then g estimation, which is a little bit more like the marginal structural model approach.

398
00:54:47,260 --> 00:54:51,580
Basically, you're now going to model in the speed outcome and each time conditional on the previous observed data.

399
00:54:52,810 --> 00:54:59,800
And so so it's basically more of an imputation or prediction type approach.

400
00:55:01,420 --> 00:55:05,560
So, okay, I'm doing time. All right.

401
00:55:05,650 --> 00:55:11,320
So I would just get through it first, probably limiting today, getting a couple of both.

402
00:55:11,320 --> 00:55:15,310
But I'll postpone the example I think. Okay.

403
00:55:15,550 --> 00:55:20,440
So her interest probability burning, basically we need to compute.

404
00:55:29,540 --> 00:55:35,580
Right. I just want to follow up a little bit on this. So I was sort of talking with the modeling before, but but you know,

405
00:55:35,600 --> 00:55:44,600
ultimately we'd probably be doing some kind of modeling of the long lines of of, you know, we want to look at the.

406
00:56:14,930 --> 00:56:24,530
And so this sort of very gentle face, you just fit a model with very little.

407
00:56:33,270 --> 00:56:41,810
So you wanted to do it in the interaction. So, right.

408
00:56:41,920 --> 00:56:45,010
So my expected values of why what would that be? What?

409
00:56:53,120 --> 00:57:02,540
Consumers will see. They deserve this big one.

410
00:57:10,420 --> 00:57:19,080
1110. I'm going to be doing.

411
00:57:19,420 --> 00:57:24,510
You know why?

412
00:57:25,830 --> 00:57:32,020
Zero one. They're in the village.

413
00:57:32,760 --> 00:57:37,690
It's an 1000.

414
00:57:42,080 --> 00:57:51,379
Right. So. So when I want to consider things like this, I would just do everything.

415
00:57:51,380 --> 00:57:55,070
Just the beat of zeroes get subtracted off and, you know, just sort of have this peace here.

416
00:57:56,230 --> 00:58:00,560
Look at this little one. This would be, too.

417
00:58:01,970 --> 00:58:05,180
So. Pretty straightforward.

418
00:58:06,690 --> 00:58:15,060
And I guess we're at the point that these are continuous sort of thing, you know, some sort of military one that's sweeping accounts of these.

419
00:58:15,690 --> 00:58:21,590
So for some. You could obviously explore this data.

420
00:58:22,580 --> 00:58:37,030
Okay. But in first probability, we. So basically we're sort of looking to see the probability being whatever the observed vector of treatments are.

421
00:58:37,990 --> 00:58:47,530
So we start out with that last treatment assignment, given the vector of previous treatments and covariates and the next to last treatment assignment.

422
00:58:47,690 --> 00:58:51,370
And previous on the way down to the probability of first treatment assignment.

423
00:58:53,320 --> 00:59:01,510
And so and of course, these, of course, could be considered plenary outcomes or any kind of mediator involved in this process.

424
00:59:04,960 --> 00:59:08,980
And so we then go ahead and compute.

425
00:59:19,530 --> 00:59:27,389
It's expected value is a weighted regression of Y on this function of Z where the weights are given by the product of the waist,

426
00:59:27,390 --> 00:59:31,710
the final time point times one minus the final time point back to the baseline piece.

427
00:59:33,360 --> 00:59:42,780
And again it's sort of consider all these interactions if TS Low dimension, if it's higher dimension,

428
00:59:42,780 --> 00:59:53,190
you might see what led you to drop some of the interactions or do some kind of more data, some sort of dimension reduction in this estimation.

429
01:00:03,300 --> 01:00:07,950
So we should mention, of course, there is usually assumptions that are running around here.

430
01:00:08,880 --> 01:00:15,510
Right. And so they're basically extensions of the assumptions that we use in a single time point for a propensity score.

431
01:00:16,800 --> 01:00:32,100
So namely, basically our observed final outcome matches this potential outcome given the observed set of treatment assignments.

432
01:00:32,100 --> 01:00:38,880
And the same thing is true for these intermediate covariates positivity.

433
01:00:39,540 --> 01:00:46,500
Again, the probability of getting assigned,

434
01:00:47,520 --> 01:00:58,620
it's got to be zero for all values and all the possible treatments at any time point given previous treatment history and covered history,

435
01:01:00,870 --> 01:01:06,749
regardless of whatever that is. And finally, importantly, sequential, more ability, right?

436
01:01:06,750 --> 01:01:21,090
That essentially disease or randomly assigned with respect to to the current Y and X is given previous exigencies.

437
01:01:22,530 --> 01:01:25,740
So sometimes this can be right, maybe a bit of a stretch.

438
01:01:26,910 --> 01:01:30,390
This also can be a stretch move more as we get these higher dimension problems.

439
01:01:31,260 --> 01:01:36,419
So it may not be that you can really consider all possible treatment assignments that

440
01:01:36,420 --> 01:01:42,000
may only be a subset of things that are realistic to suit for may or may not be.

441
01:01:42,030 --> 01:01:49,260
Hopefully this this is usually pretty typical. And, you know, independence may or may not be the case.

442
01:01:50,190 --> 01:01:58,870
So. Questions about that.

443
01:02:03,300 --> 01:02:10,800
All right. So various estimates can be computed using the sort of survey inference extended to regression settings.

444
01:02:12,630 --> 01:02:27,740
So. So.

445
01:02:31,070 --> 01:02:41,580
Right. I'm looking at this. It's.

446
01:02:45,730 --> 01:02:49,210
People to setting. All right. It's going to be with some of these betas.

447
01:02:54,910 --> 01:03:01,930
And so the variance that is estimated is fact.

448
01:03:03,010 --> 01:03:22,640
And this is going to be right. So I just get my my usual DJ sandwich estimate her.

449
01:03:32,140 --> 01:03:37,180
This sort of. Whiskey.

450
01:03:37,180 --> 01:03:47,370
Wine. Sorry. Survey package.

451
01:03:53,250 --> 01:03:57,750
You just use this eBay feeling to get this and then I just compute.

452
01:04:01,090 --> 01:04:07,000
Sure. Delta method, which is little simple here because it's just a linear combination.

453
01:04:09,040 --> 01:04:12,470
So, okay.

454
01:04:13,840 --> 01:04:18,580
So one issue here is weights can be really unstable. We have these positivity problems.

455
01:04:19,420 --> 01:04:21,790
So there's a whole it's actually, I think,

456
01:04:22,210 --> 01:04:27,760
still a fairly open area for work as sort of trying to deal with this one issue is to sort of slice things back.

457
01:04:28,000 --> 01:04:33,760
Right. Sort of don't worry about some treatment assignments and just give up on them.

458
01:04:35,260 --> 01:04:40,780
So you sort of take a smaller subset of the data or maybe there's some cover in space that you can't really look at.

459
01:04:41,710 --> 01:04:53,090
So you sort of pack that piece out. But that's it's a bit of a trade off for this, otherwise somewhat on parametric approach.

460
01:04:57,250 --> 01:05:05,760
Okay. So g estimation. So this is kind of similar to the structural mean model.

461
01:05:07,560 --> 01:05:15,629
It it relies on imputation and in particular relies on this factorization that I want

462
01:05:15,630 --> 01:05:21,030
to look at the expected value of my potential outcome under some sort of treatments,

463
01:05:21,030 --> 01:05:25,649
assignments under sood for positivity, sequential and or ability.

464
01:05:25,650 --> 01:05:43,320
I could break this down to basically conditioning on my X's and average over all the possible set of treatment assignments of my axes.

465
01:05:44,730 --> 01:05:50,190
Looking at the expected value of Y, given a particular x, t and and, and, you know,

466
01:05:50,190 --> 01:05:57,660
fixing this and whatever my treatment assignment I'm going to worry about and then multiply that by the probabilities of each one of these.

467
01:05:59,600 --> 01:06:06,860
Coming right down to the basic averaging over the baseline covariance.

468
01:06:09,350 --> 01:06:12,970
So the trick for this basically is, you know,

469
01:06:12,980 --> 01:06:21,320
we're going to fix this in advance and then you can just start predicting now what my set of covenants would look like.

470
01:06:21,830 --> 01:06:25,700
You can sort of think about me, I guess this about from A to Z here.

471
01:06:31,040 --> 01:06:58,510
Just. So we're going to fix this.

472
01:07:03,970 --> 01:07:12,640
And essentially you're going to start predicting now and there will be other explanations here.

473
01:07:14,880 --> 01:07:18,660
It's very clear that nothing in here.

474
01:07:21,280 --> 01:07:32,440
So from this I'm going to predict this and this, this I'm going to predict this, and then I'll have the three fixed.

475
01:07:33,910 --> 01:07:39,130
And so this one doesn't change. So predictive three and so on and so forth.

476
01:07:43,830 --> 01:07:49,870
And then when I'm all done that now, then I'll predict what my final outcome will.

477
01:07:55,640 --> 01:08:00,150
Are we not worried about, like why was going to all the way through?

478
01:08:01,070 --> 01:08:08,450
Well, they they they're sort of embedded in this. So this.

479
01:08:10,940 --> 01:08:15,690
This can take a few months.

480
01:08:23,510 --> 01:08:36,520
Generalized notation. And.

481
01:08:44,550 --> 01:08:50,460
So I just do this over and over again. I'm sorry.

482
01:08:50,730 --> 01:08:51,570
I do this one time,

483
01:08:52,470 --> 01:09:08,250
and then I'm going to compute the mean of my predicted wise under a given treatment assignment and that I just repeat that many times.

484
01:09:10,590 --> 01:09:15,810
And then I would estimate this difference. Then I just take the mean of these.

485
01:09:17,460 --> 01:09:22,570
Of these boots, these sort of. Bootstrap.

486
01:09:22,570 --> 01:09:33,400
But we we sampled what's called Monte Carlo expectations of some of these under a fixed set of treatment assignments.

487
01:09:33,910 --> 01:09:37,820
And I want to compare it against another treatment assignment. So I do the same thing for a different set of treatments.

488
01:09:38,050 --> 01:09:46,210
So I could set everything here to assume treated the whole way through particular outcomes for everybody based on that,

489
01:09:47,140 --> 01:09:50,380
and then do the same thing for no treatment.

490
01:09:50,590 --> 01:09:54,190
And I would compare those and that would give me an estimate of this.

491
01:09:55,510 --> 01:09:58,750
Conversely, if I just have two treatment assignments, I could do the same thing.

492
01:10:01,260 --> 01:10:08,600
This for this. You get all four possibilities put together.

493
01:10:08,600 --> 01:10:17,950
Whatever I. So. Okay.

494
01:10:19,000 --> 01:10:25,950
So variance estimation can be done per bootstrap here. Somewhat time consuming if you like.

495
01:10:25,950 --> 01:10:30,340
I sort of. We didn't I was running last night at 11:00 when I went in there.

496
01:10:30,400 --> 01:10:40,660
My point estimate. So basically just we sample all the data and you also here you do and re sample X it's all different the

497
01:10:40,660 --> 01:10:46,720
regression setting because you really you want to account for the variability the whole way through including.

498
01:10:50,770 --> 01:11:01,459
Cool. Next one. So right to do the oversampling.

499
01:11:01,460 --> 01:11:07,310
So then you can just bootstrap and you get each one of these bootstraps gives a new estimate of this

500
01:11:08,030 --> 01:11:17,840
and you can look at the variability of that to get assess variability in this in this causal effect,

501
01:11:18,590 --> 01:11:21,800
different speeds that you're looking at. Okay.

502
01:11:22,460 --> 01:11:28,090
So step two and step three can be a little complicated, right?

503
01:11:28,100 --> 01:11:30,560
Particularly maybe step two or maybe a whole bunch of axes here.

504
01:11:31,550 --> 01:11:37,610
So an easy way to deal with this is using the imputation software, especially nowadays with things like mice.

505
01:11:38,360 --> 01:11:45,860
It's fairly it's somewhat automated, basically handed a data set with holes and it fills it in.

506
01:11:47,030 --> 01:11:50,110
Now it's issues about how it's doing it and you need to think about.

507
01:11:50,270 --> 01:11:54,790
But but it's a little perhaps a little easier to me.

508
01:11:54,800 --> 01:11:59,030
So we first developed an example, so it looks at a somewhat simple piece.

509
01:12:01,640 --> 01:12:05,299
Okay. So. All right.

510
01:12:05,300 --> 01:12:13,160
So some issues here. You need sort of a large number of maybe increasingly high dimensional outcome models, right.

511
01:12:13,460 --> 01:12:21,350
To lose get to be two tiers, three, four, five or more than that.

512
01:12:21,360 --> 01:12:28,550
Maybe maybe you don't start worrying about all of the previous axes, maybe just the most recent one.

513
01:12:29,240 --> 01:12:33,080
Right? So you don't have to sort of think about the whole chain here.

514
01:12:33,410 --> 01:12:36,490
BUSH You are fixing this to whatever it is. So that's that's that.

515
01:12:40,520 --> 01:12:45,290
So the other thing that the, of course, positivity problems show up a little differently here.

516
01:12:45,920 --> 01:12:48,920
We're going to show up as limited or no data to estimate this. Right.

517
01:12:48,980 --> 01:12:52,820
Once you fix this, maybe there's nobody that has this big reset.

518
01:12:54,230 --> 01:12:59,630
So or maybe there's it's very difficult to find individuals that sort of had,

519
01:12:59,870 --> 01:13:04,410
you know, imputed values that look look similar here can be estimated here.

520
01:13:04,430 --> 01:13:11,510
So, again, some reduction might be a little in this case, probably.

521
01:13:15,830 --> 01:13:19,490
Again, you could possibly fit if you're doing some kind of.

522
01:13:22,580 --> 01:13:29,120
Outcome or intermediate outcomes, a as a as a linear function of some continuous treatment or,

523
01:13:29,600 --> 01:13:40,160
or may be a function of the number of treatments you've received the of as models for these immediate steps and manage that.

524
01:13:40,760 --> 01:13:45,500
So. Okay, so.

525
01:13:47,940 --> 01:13:51,450
I'm going to stop here because example's going to take a little bit time to get through.

526
01:13:54,600 --> 01:14:02,260
Any questions at this point? All right.

527
01:14:04,300 --> 01:14:08,860
All right. So you get an extra 5 minutes today. Have no office hours to honor as usual.

528
01:14:10,420 --> 01:14:15,250
And I be honest, I class can be somewhat abbreviated on Monday if people don't sign up.

529
01:14:15,880 --> 01:14:21,460
I decided if I want to throw a little more in on this or not and otherwise.

530
01:14:22,570 --> 01:14:27,840
Have a good weekend. But we can.

