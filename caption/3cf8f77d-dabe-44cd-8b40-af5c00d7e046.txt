1
00:00:01,420 --> 00:00:10,300
Sure. So today we are going to talk about the influence of politics.

2
00:00:12,070 --> 00:00:18,459
So this adds to the role and alliance influence and connection to leverage and

3
00:00:18,460 --> 00:00:25,480
then the impact of this alliance of evolution observations with radical leaders.

4
00:00:25,480 --> 00:00:38,830
And we are as. So, you know, in an ideal world, in a perfect world, if that's if we want to fit a regression model Y on X,

5
00:00:39,820 --> 00:00:48,640
and if we observe such or these data points, then that would be great, right?

6
00:00:48,730 --> 00:00:53,020
Because the see that these data point, they roughly fall along the same line.

7
00:00:53,560 --> 00:00:59,860
And of course there is some division but not too much. And so we are just looking at this plot.

8
00:00:59,860 --> 00:01:08,260
It's so clear that there is a strong, very, very strong with one X and that we can see that this model for.

9
00:01:09,550 --> 00:01:12,550
So this is what we would like to have.

10
00:01:12,820 --> 00:01:16,640
However, in reality, when you click on our data,

11
00:01:17,170 --> 00:01:27,250
likely that you will have some observations that are kind of far away from the general trend from at least these three points.

12
00:01:27,610 --> 00:01:31,660
You're kind of far away from the rest of these. And this is not uncommon.

13
00:01:33,190 --> 00:01:42,600
And this this lecture we're going to talk about how all of those how to tell when some of these points that are,

14
00:01:42,880 --> 00:01:46,150
you know, that are outliers where they are in usual observations.

15
00:01:46,150 --> 00:01:58,160
And how do men in public health, if they are if large observations, but then how to deal with that?

16
00:01:58,180 --> 00:02:01,600
That's actually really a case by case.

17
00:02:02,320 --> 00:02:09,520
So here, if we look out of the the red light sorry that this red dot and this blue dot,

18
00:02:10,000 --> 00:02:18,940
we're going to see that although these two points, they are kind of far away from the rest points, but they are far away.

19
00:02:18,940 --> 00:02:23,170
What? Sorry, something. So these these two points there are far away.

20
00:02:23,410 --> 00:02:25,900
First of all, they are far away in terms of the X.

21
00:02:26,260 --> 00:02:34,000
So if you look at the X range, most of the data points, they fall within this X reach for this red dot.

22
00:02:34,060 --> 00:02:40,090
And Budapest, they are far away in terms of X. And so EPS values are quite different from others.

23
00:02:41,920 --> 00:02:46,780
In this case, we say they are they have high leverage.

24
00:02:47,050 --> 00:02:51,910
There are high leverage points. That means that they have extreme X values.

25
00:02:53,990 --> 00:03:02,389
On the whole, everyone, if they have a certain X values, typically this will impact these points.

26
00:03:02,390 --> 00:03:08,990
They will impact the virus as major, because if you recall, one of the viruses,

27
00:03:09,500 --> 00:03:17,120
the virus of beta had, that's equal to Sigma Square Times, the inverse of this matrix.

28
00:03:18,230 --> 00:03:29,330
And then if the value for particular X Y is quite large, that's going to affect this matrix because this depends on X from every subject,

29
00:03:30,860 --> 00:03:40,850
depends on each enzyme, so that if you have a very extreme X higher than that x on that, it will in fact impact the or a bad value of this matrix.

30
00:03:41,000 --> 00:03:43,700
Now, of course you take the immersed in fact,

31
00:03:44,150 --> 00:03:54,830
in the end a fact about how you know the estimated variance and it may impact better hat it doesn't have to but it may have been have

32
00:03:54,920 --> 00:04:04,850
so for example this red dot just by by intuition this red dot doesn't seem to affect beta much because it just fall along that line,

33
00:04:04,880 --> 00:04:08,450
although it's far away in terms of X, but it's still fall on their line.

34
00:04:08,720 --> 00:04:18,470
So it doesn't affect beta by this blue dot because it's while here if you have this pull data points in the data set,

35
00:04:18,470 --> 00:04:23,090
then it would actually shift or or, you know,

36
00:04:23,130 --> 00:04:30,620
several later this this line a bit if you had this still pinpoint does that mean

37
00:04:30,620 --> 00:04:38,809
in the dataset so it may or may not impact beta hat and then these two dots,

38
00:04:38,810 --> 00:04:45,720
this green and this blue dots. These two days are the so-called outliers.

39
00:04:46,220 --> 00:04:55,440
The but in the sense that they have extreme emphasis on that we have extreme, absolute hat because this line here is this line.

40
00:04:55,460 --> 00:05:00,290
This is the finish line. And then the residual the residual.

41
00:05:00,320 --> 00:05:09,410
Abdulwahab, that is the difference between where the distance between the point and corresponding point and a very light blue points.

42
00:05:10,250 --> 00:05:14,150
Also, this is additional I get from from this life.

43
00:05:14,960 --> 00:05:19,760
And so these two boys, they kind of have extreme effect.

44
00:05:20,900 --> 00:05:28,010
So this is what we call the outliers. And outliers will impact see must wear hat.

45
00:05:28,460 --> 00:05:36,440
Well, the reason is that even because he must wear a hat is she must wear hat is actually equal to A minus P.

46
00:05:39,940 --> 00:05:47,940
That's it. So again. So then, of course, if you if you have an extreme arsenal hatch,

47
00:05:48,450 --> 00:05:58,140
then that will impact the the sea level of steam has cleared that eventually will impact the bearers of their hat.

48
00:06:00,940 --> 00:06:07,090
And this agreement I was from I wanted to have dinner speaking you it does impact better hat.

49
00:06:09,580 --> 00:06:15,640
And then if we look out of the Blue Dog, if you look at the blue dot,

50
00:06:16,540 --> 00:06:28,900
the Blue Dog is actually far away from the rest of the data points in both X and in the absolute result of of of in terms of x,

51
00:06:28,930 --> 00:06:34,150
most of the X values they are here. So this broadcast is far away in terms of X.

52
00:06:34,630 --> 00:06:38,380
So in terms of the absolute, of course is far away from this fairly light.

53
00:06:38,560 --> 00:06:41,590
Of course, it's also far away in terms of epsilon.

54
00:06:41,890 --> 00:06:51,370
So in this case, we call it visual point. So it is extreme in terms of both X and absolute evolution.

55
00:06:51,370 --> 00:06:54,530
One or two impacts both beta have what?

56
00:06:54,610 --> 00:06:59,679
Sorry, enhanced beta hat and impacts done several years as near as well.

57
00:06:59,680 --> 00:07:07,000
So the evolution point of you or this is the most worrisome data that we should pay very close attention to.

58
00:07:09,710 --> 00:07:17,300
Okay. So now what? Yeah, this slides. So we have sort of the distinction between a high level report outlier as it

59
00:07:17,300 --> 00:07:23,510
was on point and of most us that the one that has the most severe consequence,

60
00:07:23,510 --> 00:07:28,640
that's not a good report. Now, let's take a look at them one by one.

61
00:07:29,270 --> 00:07:30,470
So first, outliers.

62
00:07:30,770 --> 00:07:45,110
And so in our data that we have the response Y and a bunch of X coherence time and then the value of the A's individual can be extreme pain

63
00:07:46,610 --> 00:08:02,690
can easily be a while the y the response or the outcome can be extreme compared last for the x can easily get lots of and that's involved.

64
00:08:02,960 --> 00:08:14,030
I mean either one can eventually lead to a very large have some left and thus impact a data analysis of results.

65
00:08:15,950 --> 00:08:23,239
So outliers again here it refers to observation with a large residual.

66
00:08:23,240 --> 00:08:26,690
So in terms of that in terms of that get in terms of a plot here,

67
00:08:27,050 --> 00:08:34,620
the outlier would be in this regard and this would not because they have relatively extreme patterns.

68
00:08:35,390 --> 00:08:40,850
This is the so-called outliers in For Outliers era.

69
00:08:40,850 --> 00:08:49,940
It's kind of difficult to divide outliers. If you just look at the X space, this is very easy to see from this plot.

70
00:08:50,120 --> 00:08:56,390
So for example, let's say if we just look at a like for example, for the for the green dot,

71
00:08:56,750 --> 00:09:04,020
for this green dot, then if we just look at the X value in the space now, there is no Y space rising.

72
00:09:04,020 --> 00:09:11,330
B, just consider this so that obviously the point is on the x axis that all of these data points, they have the value.

73
00:09:13,530 --> 00:09:20,070
You're right. And this green dodge adds value is just fall right in this range.

74
00:09:20,460 --> 00:09:28,680
And if you look at start up and you want to be able to tell this is actually an out and so so just by looking at it adds value,

75
00:09:28,890 --> 00:09:40,500
typically we are not able to tell if the datapoint is in line because our is actually are the ones that have extremely heavy.

76
00:09:44,730 --> 00:09:51,450
Now that's. Now the leverage the data points or the high leverage.

77
00:09:53,100 --> 00:10:01,919
Not actually leverage is a concept that is used to reflect how far away the subject is from the rest of us objects.

78
00:10:01,920 --> 00:10:09,899
In terms of X we can look at to see how far away the X value is and the leverage exaggerated.

79
00:10:09,900 --> 00:10:14,490
Well, this is the definition of leverage H II and this quantifies leverage.

80
00:10:14,490 --> 00:10:18,300
This is how we define that reach the leverage for the individual.

81
00:10:18,690 --> 00:10:28,650
This is the on ice element or the afterglow element of this parametric recall that has had metrics is equal to x.

82
00:10:32,340 --> 00:10:42,010
Is equal to X. It's principles and this is what are what have matrixes.

83
00:10:42,310 --> 00:10:48,640
And at that age, ice just is just the ice bag element of this matrix.

84
00:10:50,080 --> 00:11:01,870
In a special case, in a symbol year we're going in case it's high leverage has this very simple expression and this is very then very easy

85
00:11:01,870 --> 00:11:10,950
to see that it does quantify how far the particular X is from the average value at average adds value in the data.

86
00:11:11,680 --> 00:11:21,280
So indeed it is a measure of how far away the particular x i use is from the the other or from the average x.

87
00:11:23,680 --> 00:11:36,070
So the high library points of high level two points may have a disproportionate impact on the highest.

88
00:11:37,750 --> 00:11:42,819
So here I want to point out this. It may have, but not necessary.

89
00:11:42,820 --> 00:11:53,080
So for example, if again, if we look at this plot here, as we mentioned, this red dot from the very end here and this red dot,

90
00:11:53,110 --> 00:12:03,160
this is of a point has high leverage because these X value is so far, so far away from the rest of the data points.

91
00:12:03,490 --> 00:12:14,680
However, just having this red on it or not will not impact the visual feeder too much because it's far along this line.

92
00:12:15,220 --> 00:12:18,880
But it may affect the virus as an agent for for data.

93
00:12:18,910 --> 00:12:23,680
So that may be more precise if you have this wrap document while we will talk about that later.

94
00:12:28,180 --> 00:12:34,480
So here, since the virus of epsilon I had is equal to this guy.

95
00:12:38,030 --> 00:12:46,940
Is equal to this God. We have shown that something. Previously we have shown that this is actually equal to our minus h four sigma squared minus H.

96
00:12:47,930 --> 00:12:52,909
So that's why if you look at variance of absolute ah ha, that's absolute one minus h.

97
00:12:52,910 --> 00:12:54,020
I hadn't seen the square.

98
00:12:54,380 --> 00:13:03,950
So if the average is large, I mean if this is large, then the corresponding variance for this residual is going to be smaller.

99
00:13:04,430 --> 00:13:10,400
So you are forcing the theory larger to be closer to the truth.

100
00:13:11,690 --> 00:13:19,300
So so that's why high leverage points may have a disproportionate impact on balance.

101
00:13:21,290 --> 00:13:24,799
Now for leverage, the leverage it has.

102
00:13:24,800 --> 00:13:28,220
Obama So the leverage I always saw between zero and one.

103
00:13:28,790 --> 00:13:36,349
Well, the reason is that actually here the note that's the slides is not non-ideal

104
00:13:36,350 --> 00:13:40,760
because this is not simply because age is symmetric and positive dominated.

105
00:13:41,030 --> 00:13:43,460
So for any symmetrical, positive, definite metrics,

106
00:13:43,880 --> 00:13:51,060
the guideline which could be any any party number but for the reason that this is always between zero

107
00:13:51,090 --> 00:14:00,470
one is actually because h because h is equal to x times at transpose x inverse times x transpose.

108
00:14:00,950 --> 00:14:03,710
So because of this particular, you know,

109
00:14:03,770 --> 00:14:12,560
we have this inverse in the middle because of this particular structure of the target ahead of us is always are always between zero and one.

110
00:14:13,340 --> 00:14:17,900
This is a fact for our matrix algebra, but let's not worry too much about that.

111
00:14:18,350 --> 00:14:27,229
So then the mean of the leverage is actually just the symbol simply of this, right?

112
00:14:27,230 --> 00:14:32,660
Over a sum of over the end individuals and event is one over ten times the twist

113
00:14:32,660 --> 00:14:39,620
because I are the guy element and because H is symmetric a positive dominant.

114
00:14:40,040 --> 00:14:43,710
So the twist is actually equal to the rec right?

115
00:14:44,570 --> 00:14:51,410
And this is actually h rec of H the run of H now the rank of H.

116
00:14:51,410 --> 00:14:56,810
We know that this matrix part of Alice, the rec of H is actually equal to cheat.

117
00:14:57,500 --> 00:15:00,770
This is not because we have to covariance.

118
00:15:00,770 --> 00:15:10,580
So the reality is equal to p and so the h bar, the average of the leverage is actually equal P over here.

119
00:15:11,360 --> 00:15:12,910
So this is one of the averages.

120
00:15:14,300 --> 00:15:29,660
And then the criterion then to tell whether a particular data point I is an outlier in terms of F space where it has a relatively large leverage.

121
00:15:30,560 --> 00:15:41,840
Well, we have two criteria to use. One is, well, if I is larger than two times the average, two times the ME or larger than this value,

122
00:15:42,260 --> 00:15:46,310
well, typically we say, oh, this point has a higher leverage.

123
00:15:47,600 --> 00:15:57,499
Or I mean, the another rule people typically use is that, well, if I it is between 2.5 here we say, oh,

124
00:15:57,500 --> 00:16:07,220
this has a moderate average, but if you I above point five, we say this has large leverage because high speed between zero and one.

125
00:16:10,160 --> 00:16:14,300
So different people, they may apply different they may follow different rules.

126
00:16:16,040 --> 00:16:20,239
So you will see that for model diagnostics, there are many rules like this.

127
00:16:20,240 --> 00:16:25,240
There is no like hard cut off. So generally we have this our rule.

128
00:16:25,250 --> 00:16:30,650
Some people follow this, but but you can definitely following the rules as well.

129
00:16:31,790 --> 00:16:37,670
So that's the reason why I just want to make this clear that in the final product,

130
00:16:38,510 --> 00:16:42,410
if you do model diagnosis, there is no unique way of doing market analysis.

131
00:16:42,650 --> 00:16:47,450
There is no no unique model in the end. So it is totally fine.

132
00:16:47,600 --> 00:16:54,740
Different groups come out with different models in the end, but you need to be able to define the value or not.

133
00:16:54,980 --> 00:16:59,270
For example, if you think about the data point that is and has a high leverage,

134
00:16:59,510 --> 00:17:05,749
well then you could either apply this and say, oh, it has a high rate or either,

135
00:17:05,750 --> 00:17:08,460
well, if you see the state of Islam at the bottom,

136
00:17:08,540 --> 00:17:15,380
sees it is a high level report and these two roles may not necessarily agree with each other all the time.

137
00:17:15,600 --> 00:17:27,140
So sometimes using while it tells you about that, it is a high leverage one by using the other, maybe say oh is not,

138
00:17:27,410 --> 00:17:33,830
which is fine, but as long as you go to clear which rule you are following and you are able to to defend your your statement.

139
00:17:34,850 --> 00:17:46,020
Okay, so. And also another common practice people use is to read eyes in addition to apply these criteria.

140
00:17:46,290 --> 00:17:50,110
So for example, ranking that data points, we have data points ranking,

141
00:17:50,120 --> 00:18:00,060
but it is a course from the artist to hide from the smallest and see whether special art is run by their or any concerns.

142
00:18:00,560 --> 00:18:07,510
And then once you identify those objects, you take a closer look at those objects and to see whether they're a data entry errors, for example.

143
00:18:07,540 --> 00:18:12,330
And that leads to a higher average. Okay.

144
00:18:12,690 --> 00:18:24,240
So that's leverage. And then the outliers record on Outliers and they have a3xr over the absolute absolute hat.

145
00:18:24,840 --> 00:18:33,660
These are outliers. Of course, having outliers in the data will change the asymmetric w the sigma half square.

146
00:18:33,960 --> 00:18:38,070
The reason is that CMI has square is calculated based on the absolute.

147
00:18:38,400 --> 00:18:42,900
Of course, then if you have a very large epsilon hat, it is going to change the value.

148
00:18:43,650 --> 00:18:51,870
So you might have squared and the outlier can distort the energies as well.

149
00:18:52,260 --> 00:19:09,400
So the reason is that beta j, if you recall one beta test theta is actually equal to it is equal to this type of principle.

150
00:19:09,750 --> 00:19:12,120
Yes. So better Z will do that.

151
00:19:12,970 --> 00:19:20,010
Now that means each component of beta is actually the linear combination of y as we measured this as we have talked about this.

152
00:19:20,190 --> 00:19:25,950
So it's a linear combination. So there is a way that a sample mean of these y ice.

153
00:19:28,800 --> 00:19:31,830
What do we know that a sample mean is very sensitive to our.

154
00:19:33,340 --> 00:19:41,379
Well, for example, if I ask you to calculate the mean and saying this is different, five different points as well as the academy.

155
00:19:41,380 --> 00:19:44,490
Let me. These five boys are the meaning is probably somewhere here.

156
00:19:45,240 --> 00:19:51,510
But if I add one point, which is the highlight over here that it would dramatic to a fact and I mean if you can

157
00:19:51,510 --> 00:19:57,600
tell me in the mean it's probably somewhere over the maybe somewhere maybe here or here.

158
00:19:57,910 --> 00:20:01,850
So outlier the mean he's very sensitive to outliers.

159
00:20:02,280 --> 00:20:07,980
Now the big J has these are weighted average or weighted mean of the wise.

160
00:20:08,220 --> 00:20:14,490
So once you have alignment of outliers large in terms of absolute line.

161
00:20:15,360 --> 00:20:19,890
And also of course most likely is in terms of extreme amount of what.

162
00:20:20,130 --> 00:20:24,390
So you have a value of what then they're likely to have.

163
00:20:25,020 --> 00:20:28,620
Well, it will have an impact on a had.

164
00:20:33,830 --> 00:20:41,600
Okay. Now, one for outliers in high inflation points.

165
00:20:42,920 --> 00:20:45,500
It's not like previously, you know,

166
00:20:45,530 --> 00:20:58,759
when we when we recall that it when we talk about hypotheses when we finally decided the question of interest in true hypothesis and about we have

167
00:20:58,760 --> 00:21:07,370
about how others we construct either f so those date were t statistic and we'd ask a little hypothesis and we decide whether to reject or not.

168
00:21:08,360 --> 00:21:13,670
So here are four outliers for inverse observations as well as being very nascent.

169
00:21:13,700 --> 00:21:20,479
We are not so interested in proposing a test of this really.

170
00:21:20,480 --> 00:21:25,550
We just see that a people carried out a formal test it whether observation is like a or not.

171
00:21:26,360 --> 00:21:34,370
Well, the reason is that, well, there are many reasons, but one particular reason is that the power of these tests is driven by the sample size.

172
00:21:35,690 --> 00:21:44,120
So if you have large sample size, you know there are sample size library analyzes to follow.

173
00:21:44,360 --> 00:21:50,870
This is this is just the truth. So in this class, we are we don't have time to worry too much about that.

174
00:21:50,870 --> 00:21:58,430
But you will see that in six or so sample size, if you have a very large sample size likely in order to reject and it does not exist.

175
00:21:59,870 --> 00:22:09,709
So the power is actually driven by sample size. Now, when you have a very small sample size, then you have little power off of the test.

176
00:22:09,710 --> 00:22:11,360
You are correct.

177
00:22:11,810 --> 00:22:20,960
However, it is when you have a small sample size that you'd worry more about the so-called outliers if you have a very large sample size.

178
00:22:21,440 --> 00:22:28,520
But having one outlier impact is relatively mild, small, mild.

179
00:22:28,580 --> 00:22:36,950
You probably don't need to worry too much, but when you have a small sample size, having an outlier might dramatically change your analysis results.

180
00:22:39,080 --> 00:22:44,170
So it is in a small sample size that we should learn more about our lives.

181
00:22:44,210 --> 00:22:50,450
But when we have a small sample size in our test, if we propose, the test has to have little power.

182
00:22:50,730 --> 00:22:55,370
So so that's one reason. Of course, there are many other, but that's one reason why we are here.

183
00:22:55,370 --> 00:23:03,800
We do not worry too much and we will be ordered to not carry off their formal test, to test whether a point is an outlier or influential observation.

184
00:23:05,270 --> 00:23:10,220
And another thing that is worth mentioning is that once you think once you, you know,

185
00:23:10,520 --> 00:23:15,350
based on looking at on the leverage and also later level of how about other positives

186
00:23:15,650 --> 00:23:21,950
whether other data point is a virtual observation was you think a data point is

187
00:23:21,950 --> 00:23:28,250
English observations you are we do not remove that directly from the data such that

188
00:23:28,280 --> 00:23:32,870
that's really about to produce if you directly remove it even if you see an outlier.

189
00:23:33,440 --> 00:23:41,089
And for example, here again in the very first block, we see this green are three dots, right?

190
00:23:41,090 --> 00:23:44,960
Red wing and blue dots. They are kind of far away from the rest of the data points.

191
00:23:45,530 --> 00:23:52,730
So we would not really remove these data points because they might be really modestly data points.

192
00:23:53,210 --> 00:23:56,150
So there is no reason to think that a be unless I mean,

193
00:23:56,150 --> 00:24:03,080
unless you have strong reason you you can with little voice and you'll see oh there they are a data entry error.

194
00:24:03,080 --> 00:24:07,650
Somebody must have read the data about it so that you have these you know that in that case you well,

195
00:24:07,970 --> 00:24:14,060
you might throw them away or you might crack those values. Other than that, if you think this is a legitimate data point,

196
00:24:14,330 --> 00:24:22,399
then then we wouldn't throw those will remove those data points is that we

197
00:24:22,400 --> 00:24:26,060
would really want to go back to those data point to see exactly what happened,

198
00:24:26,330 --> 00:24:30,500
whether we did a little bit value. Sure, there is some data, Andrew, error.

199
00:24:30,510 --> 00:24:43,040
So for example, it's very common for people to code this value as either 99 or 99, like using revised code missing out on.

200
00:24:43,100 --> 00:24:47,750
In that case, of course, we would treat that as a numerical value. That might be easy.

201
00:24:47,960 --> 00:24:56,150
For example, if you have a missing edge now, if you call that as not alive, that's what it is that I solved.

202
00:24:56,570 --> 00:25:10,580
But that's a data entry thing. So one of the single hazards refers to make sure that there doing lot of that line and then we

203
00:25:10,580 --> 00:25:16,880
could actually carry out analysis work without that performance and see how sensitive the results,

204
00:25:16,910 --> 00:25:20,210
how much of results in. Okay.

205
00:25:20,780 --> 00:25:27,740
So that's how I is in the high leverage points now, the influential observations.

206
00:25:27,920 --> 00:25:32,330
These are the boys that both high leverage and also high residual.

207
00:25:34,050 --> 00:25:38,060
In Harlem rage. This is in terms of acts space, right?

208
00:25:38,070 --> 00:25:49,440
Acts low and high crime is the fact that because of that, the essence that's moving out in the residuals and in this case,

209
00:25:50,790 --> 00:25:56,880
this gridlocks, this is the value that has high influence because they have full trial.

210
00:25:57,970 --> 00:26:09,020
What I have in terms of absolute hat and here on.

211
00:26:12,430 --> 00:26:18,170
So the info about the data points with high leverage but not all the evidence base.

212
00:26:18,730 --> 00:26:25,450
Now they could influence the Libyans as leader, but not how they thought they have a seamless Ferhat.

213
00:26:25,720 --> 00:26:31,180
This is quite easy to see. Again, we take this rent boys as an example.

214
00:26:31,180 --> 00:26:37,690
So the red read Dodge. It has high leverage, but very low abstract.

215
00:26:38,800 --> 00:26:41,950
So in this case, if you look at this red dot,

216
00:26:43,210 --> 00:26:53,620
it will not impact the as Major Peter had by too much because with or without at this point, the line roughly, I mean is just this line.

217
00:26:54,440 --> 00:26:58,300
So this red off for almost exactly on this line.

218
00:26:58,510 --> 00:27:01,450
So having this red out or not, we will have roughly the same.

219
00:27:01,600 --> 00:27:10,210
Of course, there is some numerical value change, but it's not going to be too much, however, and it will impact the various as nature.

220
00:27:12,360 --> 00:27:19,850
Impact awareness has made her the reason. Well, the reason is that it varies depends on X value.

221
00:27:20,390 --> 00:27:30,880
And now if you have an extreme X value impact reader and the outliers in terms of the absolute catch.

222
00:27:35,390 --> 00:27:40,910
But for those who have lower, large, absolute values, but a small average,

223
00:27:41,270 --> 00:27:47,810
they could imagine the intercept and see much square, but not the slope this is.

224
00:27:50,070 --> 00:27:55,530
For example, if you look at a green dot, the green dot has a rather large absolute hat.

225
00:27:56,640 --> 00:28:02,460
But in terms of X value, it has to have extreme x x value.

226
00:28:03,180 --> 00:28:14,010
So in this case, if you look at the impact of this on, a lot of it will pull the line up by a little bit, but it will not change the slope.

227
00:28:14,580 --> 00:28:18,809
It could pull the line up a little bit with or without.

228
00:28:18,810 --> 00:28:26,430
I mean, when comparing the analysis with a list of beautiful and 2000 without this datapoint, something will change that.

229
00:28:26,430 --> 00:28:33,770
And also of course it will influence the to see my hat a square because the CMA had to square this answer.

230
00:28:33,810 --> 00:28:42,060
It depends on the x capsule I had about it. Right. This is the one over amnesty, the sum of x absolute I squared.

231
00:28:42,480 --> 00:28:46,350
So of course if you have a large epsilon square, you will have a while.

232
00:28:46,380 --> 00:28:53,520
This similar kind of square would change. So this is not different scenarios.

233
00:28:54,850 --> 00:29:00,069
No. But these two under the radar.

234
00:29:00,070 --> 00:29:05,830
This is the so-called high leverage point. And this green dot was the so-called outlier.

235
00:29:06,760 --> 00:29:11,379
And this Blue Dog, this is the so-called influential observation.

236
00:29:11,380 --> 00:29:17,180
This this is this has a set of models in both accidental and absolute.

237
00:29:17,260 --> 00:29:21,660
So. That involves observation.

238
00:29:22,170 --> 00:29:26,820
These like the the blue dots in the previous speaker.

239
00:29:27,340 --> 00:29:35,920
It will have impact on all of these values and your your estimated beta hash for various as major of beta hat.

240
00:29:36,100 --> 00:29:44,710
And then the idea that is the reason is that here this blue dot over here well this old arch

241
00:29:45,010 --> 00:29:53,400
it will pull this line this way here so and so it will change the slope of the graph maybe,

242
00:29:53,410 --> 00:29:57,550
but it may not be this dramatic. I'm I'm sure I'm just exaggerating.

243
00:29:58,990 --> 00:30:05,590
So this one single data point may not have such a dramatic impact, but it will pull the line towards the point a little bit.

244
00:30:06,640 --> 00:30:13,600
And also, of course, because of the extreme adds value and also extreme absolute amount of it, it will it will affect the awareness as major.

245
00:30:14,650 --> 00:30:28,240
So basically, it'll affect almost everything in our analysis now for if we want to tell whether a data point is the higher influential observation,

246
00:30:29,230 --> 00:30:41,860
one way that people typically do is to look at so look how it has as a result without missing a point and a compared to the the

247
00:30:41,880 --> 00:30:49,990
analysis what including this data points and to see how different these results are and if the results are quite different than this.

248
00:30:50,080 --> 00:30:55,899
While we say this is an influential observation, even if the results are very similar, then that's okay.

249
00:30:55,900 --> 00:30:59,740
So this is not a is observation.

250
00:31:00,490 --> 00:31:03,580
So now let's take a look at some criterion.

251
00:31:03,580 --> 00:31:12,210
So we will use this notation minus R subscript to denote that this this is the analysis by removing just

252
00:31:12,340 --> 00:31:18,370
this just one single data point for high suitable orbit from the data as that isn't fair to follow.

253
00:31:18,790 --> 00:31:26,380
So there are four commonly used criteria to tell if the observation is a visual observation.

254
00:31:26,830 --> 00:31:33,489
So the first one is that this is the difference in the field value and this

255
00:31:33,490 --> 00:31:39,130
is the standardized standardized difference of and the names actually quite.

256
00:31:41,610 --> 00:31:48,210
Judy. So there's a difference in the feeling that at the center, a student centered on standardized difference, they feel about it.

257
00:31:48,540 --> 00:31:56,520
This shows the influence on the feet of what I had and in certain ways, the difference in the balance.

258
00:31:57,140 --> 00:32:01,570
Plus, this is still nice to a standardized influence, all the better.

259
00:32:03,000 --> 00:32:11,310
And then we have the so-called cooks distance. This is the influence of the overall Peter Hatch, not a nuclear Peter.

260
00:32:11,580 --> 00:32:17,090
How to keep an overall beater hat and also that we have the the cobras region.

261
00:32:17,820 --> 00:32:23,950
This is the influence of the bearers of Peter Hatch and each of this matter.

262
00:32:23,980 --> 00:32:30,480
It reflects the difference between while you basically essentially you run two sets of results.

263
00:32:31,080 --> 00:32:36,390
So the results were the observation and the results without this particular observation.

264
00:32:36,510 --> 00:32:40,920
And then you compare while the truth as a result and see how close they are.

265
00:32:43,890 --> 00:32:47,050
Now, let's take a look at each of these one by one.

266
00:32:47,100 --> 00:32:50,760
So the difference now, the difference in theory, the value.

267
00:32:52,890 --> 00:32:56,520
This is as a matter of influence on the field about it.

268
00:32:57,510 --> 00:33:05,580
So what it does is we will say to the model by removing by removing this RS individual.

269
00:33:05,760 --> 00:33:13,830
So if we want to see how influential the eyes individual is on the results, then we will remove this individual and the related model.

270
00:33:14,340 --> 00:33:16,620
So this here, this has been a minus.

271
00:33:16,620 --> 00:33:25,080
I have and this is not estimated by the head by removing this individual from the data about of course, it has the same formula as before.

272
00:33:25,200 --> 00:33:32,040
This just does not have the IV data point. And then the field model is actually equal to this.

273
00:33:33,060 --> 00:33:38,970
And that if you look at how that different in theory that the student has how is defined,

274
00:33:39,590 --> 00:33:46,750
essentially it looks at the difference in the field value with and without the body.

275
00:33:47,670 --> 00:33:51,570
And again it tries to centralize it as a standardized.

276
00:33:54,430 --> 00:34:01,660
So this is how this criterion is defined in the numerator in the denominator here.

277
00:34:01,780 --> 00:34:03,250
That's the standardization.

278
00:34:03,280 --> 00:34:13,250
This is actually why we have this generalization, because the variance of why half the value is equal to y hat is equal to had a matrix.

279
00:34:13,300 --> 00:34:23,290
That's why this is this is the answer that equal to age times there is of white pants, exact times transpose.

280
00:34:24,370 --> 00:34:30,490
But this is actually equal to see among the square. So it's equal to see my square times.

281
00:34:32,140 --> 00:34:36,730
Times age because age is symmetric and age is ages and important.

282
00:34:37,030 --> 00:34:46,089
So is equal to this. So. So then what we centralized does we just take about the variance of either individual that component.

283
00:34:46,090 --> 00:34:49,690
Let's see what's where it comes. H i and the ISO elements.

284
00:34:49,960 --> 00:34:54,700
And this is just a and as nature of this kind of this virus.

285
00:34:55,840 --> 00:35:00,880
So this is how we how we define the difference in the feel of that.

286
00:35:01,420 --> 00:35:09,790
And then typically people consider the absolute value of the difference if the model is larger than this.

287
00:35:10,240 --> 00:35:14,320
While this is already considered a highly influential observation.

288
00:35:14,950 --> 00:35:18,280
So again, this is not a hard threshold.

289
00:35:18,310 --> 00:35:28,090
This is just people typically follow. I think people typically use and based on experience, this gives us quite a good criterion.

290
00:35:28,450 --> 00:35:38,200
So then if you see a value like if the, the, the feeling value change by standardized state of our change by this much well,

291
00:35:38,260 --> 00:35:43,560
and without that individual, then we say, oh, this individual is observation.

292
00:35:44,830 --> 00:35:53,260
So that's the difference in feeling about it. And of course, if you look at this, how this value is calculated,

293
00:35:53,260 --> 00:36:00,340
it seems it will seem that we have to fit the model twice in order to tell whether either individual is equal.

294
00:36:00,700 --> 00:36:07,510
So we first need a model for this as individual, and then we fit the model without the size individual that we compare.

295
00:36:08,530 --> 00:36:12,830
Now, if you want to look at each data point, whether it's evolution,

296
00:36:12,890 --> 00:36:17,310
observation or not, that means you have to fit in the model repeated many, many times.

297
00:36:18,160 --> 00:36:26,030
So that's of course not ideal, but it turns out that we do not need to fit a model exclusively so many times to turn it up.

298
00:36:26,050 --> 00:36:30,550
Turns out that we can provide a one and then we can calculate all this difference in event.

299
00:36:30,880 --> 00:36:36,370
So this is because of the particular feature of linear regression model, because it's linear.

300
00:36:36,370 --> 00:36:41,200
So when there is algebra that can simplify the result.

301
00:36:41,210 --> 00:36:48,880
So it turns out that here we will not talk about the details, but it turns out that of the difference in fit can be calculated in this particular way.

302
00:36:49,480 --> 00:36:59,980
Now this r half minus I, you know, this is the eternal is still nice residual and this h I here this is the elements of that matrix.

303
00:37:00,430 --> 00:37:06,530
Then by this formula you only need a fair amount of once.

304
00:37:06,550 --> 00:37:11,780
There's really no need to feed the model many, many times. So then on.

305
00:37:12,250 --> 00:37:18,610
But anyway, so the we do not have to calculate this not only ourself.

306
00:37:18,890 --> 00:37:23,420
You use us. Are you not only less than zero?

307
00:37:24,040 --> 00:37:28,180
It's not. But here are just 1.4 now, right?

308
00:37:29,500 --> 00:37:38,020
Although, I mean, essentially from the very beginning, this is defined as, you know, comparing the model with and without this observation.

309
00:37:39,010 --> 00:37:46,360
Now essentially, you need to fit four models, but as with the elderly population, there is no need.

310
00:37:47,200 --> 00:37:56,570
So and also while our insights we will have for this that okay, so that's difference in the field about it.

311
00:37:56,590 --> 00:38:03,100
Now look at the difference in the patterns is based on almost exactly the same idea.

312
00:38:03,820 --> 00:38:07,090
So we look at the difference of Peter K, right?

313
00:38:07,120 --> 00:38:17,320
This is the case component of Peter been okay with and without this RS individual in our dataset, we look at how different they are.

314
00:38:18,370 --> 00:38:30,159
And also we standardized this difference. We standardized this difference and thus the summarization comes from the fact that the variance of better

315
00:38:30,160 --> 00:38:38,500
hat what Peter K happened there is a better kind of hat is as with the case diagonal element of this guy.

316
00:38:39,670 --> 00:38:43,299
And then we just while we standardize this difference,

317
00:38:43,300 --> 00:38:50,170
we just take the square root of the estimate here of the merits of the case diagonal element of this matrix.

318
00:38:52,070 --> 00:38:56,210
That's how we standardized down the defensive.

319
00:38:57,590 --> 00:39:02,780
But essentially, again, you can see that the idea is almost exactly the same as difference in field about it,

320
00:39:02,780 --> 00:39:08,210
which is look at how much [INAUDIBLE] there is. Well, then without this particular data point.

321
00:39:08,960 --> 00:39:12,350
And then if the change is very large, we say, I see.

322
00:39:12,480 --> 00:39:15,800
The point is if literal. Otherwise it is not.

323
00:39:16,450 --> 00:39:23,360
Now, the criteria typically used is while we look at an absolute value, whether that's larger than this particular rally,

324
00:39:23,720 --> 00:39:30,540
the content of this if it's larger than this or we consider the points to be influential.

325
00:39:30,770 --> 00:39:35,150
I. And a similarity.

326
00:39:36,050 --> 00:39:44,870
While we do not need to expose the fate of these different models for free to models for each data point after removing each of their point.

327
00:39:45,470 --> 00:39:58,490
So it turns out that now if we use this W to denote this matrix, this matrix is actually better had a single event, not even double times.

328
00:39:58,490 --> 00:40:06,860
What? So if we denote w to be this matrix then beta how to k can be expressed as this particular linear combination of the y.

329
00:40:08,330 --> 00:40:19,130
And then it turns out that this criterion, the difference in beta, a certain size difference in beta, it it can be calculated in this particular way.

330
00:40:20,300 --> 00:40:24,770
So it depends on W and depends on the externally that residual.

331
00:40:24,770 --> 00:40:35,149
It depends on the patch matrix. So again, this, this means that we do not need to fit explicitly many, many different linear regression models.

332
00:40:35,150 --> 00:40:39,530
We can just say the model was and then calculate the difference in betas.

333
00:40:41,360 --> 00:40:47,570
So again, if you ask SAS in R appropriate event, they will output these values.

334
00:40:57,390 --> 00:41:01,080
Okay. Okay. Any questions so far about these different criteria?

335
00:41:07,320 --> 00:41:11,610
And then we have the so-called cooks distance the cooks.

336
00:41:11,610 --> 00:41:20,160
This is this now the different while this also looks at the influence of the hat on the hat.

337
00:41:20,760 --> 00:41:29,909
Well, the difference between cooks this is and this difference in us is that a difference that made us look at each individual

338
00:41:29,910 --> 00:41:38,940
being called out for while we have feet of actor Peter Factories is equal to Peter Carver's lovely little intersections.

339
00:41:38,940 --> 00:41:42,060
Peter one and up two. Peter P minus one.

340
00:41:42,960 --> 00:41:52,870
So we have to convey that is in total and this difference in Peter it looks at each component of make it the slope of gender,

341
00:41:52,870 --> 00:41:58,110
a slope of age, the slope raised a slope of height, weight and so forth.

342
00:42:00,840 --> 00:42:04,830
But this cooks distance is sort of overall quite here.

343
00:42:05,730 --> 00:42:12,810
So it looks at an influence on the whole beta hat rather than looking at the influence on each individual with a hat.

344
00:42:15,570 --> 00:42:20,490
So the distance is defined in the following way.

345
00:42:21,270 --> 00:42:31,259
So here it looks that to compare the Peter Hat that is derive by including all the data points to the

346
00:42:31,260 --> 00:42:39,510
beta hat that is derived by removing the people are data points and look at how different they are.

347
00:42:40,350 --> 00:42:47,429
And then it looks at the quadratic flaw I see here you have this difference transpose,

348
00:42:47,430 --> 00:42:52,320
here you have the difference and in the middle you have the inverse of the various matrix.

349
00:42:52,920 --> 00:42:59,430
So this is sort of to standardize the community for the difference, for the squared difference.

350
00:43:01,140 --> 00:43:04,049
So now, of course, the, the virus,

351
00:43:04,050 --> 00:43:14,010
we know that the virus this is equal to the virus is equal to see my square for seamless we're ahead of times x transpose x inverse.

352
00:43:14,790 --> 00:43:17,790
And if you plug this in then you will have this.

353
00:43:18,360 --> 00:43:25,740
Then there is another inverse over here and then you will have x transpose times that's divided by similar square patch.

354
00:43:27,720 --> 00:43:33,360
So this is how the distance is defined essentially.

355
00:43:33,360 --> 00:43:37,790
Again, it looks that how different these two estimated values are.

356
00:43:38,070 --> 00:43:43,860
The other words, how different your results are worth and without this particular bit of points.

357
00:43:44,430 --> 00:43:50,430
But this difference is defined in terms of this quadratic for you look at the quadratic

358
00:43:51,210 --> 00:43:56,310
or the squared difference but is but it is sort of standardized squared difference.

359
00:43:57,480 --> 00:44:06,030
Okay. So then of course, the lower larger the cooks distance is the larger this difference is,

360
00:44:06,540 --> 00:44:10,950
then the larger the influence of the highest point on the results.

361
00:44:11,400 --> 00:44:16,140
Right. So the distance is always positive.

362
00:44:16,920 --> 00:44:25,950
The reason is that the reason is that here this is a quantitative floor and in the matrix in the middle, this is a positive gravity matrix.

363
00:44:26,400 --> 00:44:30,000
So that's why this on any form, of course, distance is always positive.

364
00:44:34,110 --> 00:44:36,740
And then for good citizens while.

365
00:44:36,750 --> 00:44:47,490
Tippett Well, again, there are different criteria that people follow in order to, to, to, to tell whether observation is evolution.

366
00:44:47,670 --> 00:44:50,740
So people could compare this to one of.

367
00:44:52,440 --> 00:44:59,130
And also some people just know f f does fusion with p m on this particular freedom.

368
00:44:59,460 --> 00:45:09,040
And then you compare it to, for example, not if not if not if it's the principle of this religion and to look at whether these he's larger than that.

369
00:45:09,630 --> 00:45:10,140
And also,

370
00:45:10,140 --> 00:45:20,790
some people might have considered by to use the criteria then to look at whether the eyes larger than three times bar which is the average of the.

371
00:45:21,570 --> 00:45:28,350
And also while in south in ah while the criterion is used is is rather die is larger than four over again.

372
00:45:29,130 --> 00:45:39,480
So this is criteria used by sense in art. So again, you can see that there is no but unique criteria here.

373
00:45:41,040 --> 00:45:50,399
Different people may follow different criteria. And then in the end it means that people will probably make lively, make different statements,

374
00:45:50,400 --> 00:45:56,810
whether regarding to a particular bit of one, whether that's our line or whether we're not.

375
00:45:57,450 --> 00:46:07,830
But that's okay. As long as you are clear which and this is our general speaking, this should be in which will differ from each other by too much.

376
00:46:08,290 --> 00:46:11,820
And then, of course, you will see some difference if you apply different criteria.

377
00:46:17,870 --> 00:46:25,660
Okay. So I think let's let's take a five minute, eight minute break and then we will resume production.

378
00:46:35,485 --> 00:46:39,605
So I think that the course of evolution now is available, right?

379
00:46:39,745 --> 00:46:46,165
So you guys probably have received the email that's to fill out an application.

380
00:46:46,165 --> 00:46:49,404
So if you think I spend I don't know how long it takes.

381
00:46:49,405 --> 00:46:52,675
Maybe 10 minutes. Around 10 minutes. I don't know.

382
00:46:52,915 --> 00:46:57,835
So but if you could stand around 10 minutes, try to fill that out, and that would be great.

383
00:46:58,345 --> 00:47:05,785
Would it helpful to know that how we did this semester and how I was thinking about this course?

384
00:47:07,105 --> 00:47:12,415
So I, I would really appreciate if you spend 10 minutes filling that out.

385
00:47:14,245 --> 00:47:14,635
Okay.

386
00:47:14,695 --> 00:47:26,905
So now let's continue talking about the course distance now for as we mentioned, the distance again, it's up if you look out of the definition itself.

387
00:47:28,345 --> 00:47:36,525
It was about to seem that you had referred to the model many, many times because you're comparing for each data point.

388
00:47:36,535 --> 00:47:43,075
I've managed to find this debate to have months, but I wouldn't qualify as individual.

389
00:47:44,695 --> 00:47:48,595
But again, we don't have to.

390
00:47:48,895 --> 00:47:50,334
There is alternative calculation,

391
00:47:50,335 --> 00:47:59,965
so we can be sure that VI is equal to this guy and then we just need to model was to be able to calculate of course distance.

392
00:48:02,555 --> 00:48:11,665
So that's cooks distance. And then we have another criterion that is still called coverage of region.

393
00:48:12,635 --> 00:48:22,265
This matters how much influence observation it is, all the various covariance matrix or various estimates of data.

394
00:48:23,765 --> 00:48:27,695
So we define a generalize the variance in this way.

395
00:48:27,725 --> 00:48:33,005
So recall that this varies evader hat. This is a matrix.

396
00:48:33,185 --> 00:48:37,505
This is the variance covariance matrix that is equal to this.

397
00:48:38,315 --> 00:48:44,435
Now this is a matrix. Now we want to summarize this matrix into a single point in your scale.

398
00:48:44,975 --> 00:48:51,455
So that based on that scalar, we can tell whether the observation has been has strong influence or not.

399
00:48:52,445 --> 00:48:55,715
So how to how to summarize the matrix into a screen.

400
00:48:56,165 --> 00:49:00,875
So we take the so-called determinant and this is the content of this matrix.

401
00:49:02,135 --> 00:49:07,955
And so the generalized variance is simply the determinant of appearance covariance matrix.

402
00:49:09,395 --> 00:49:18,785
And then, of course, larger virus is what leads to larger generalized variance or some other determinant of the matrix.

403
00:49:19,715 --> 00:49:26,945
And then we summarize the subject's eyes influenced by looking at the Converse region.

404
00:49:27,335 --> 00:49:33,835
This is the ratio comparing with generalized comparative generalized variance without

405
00:49:33,855 --> 00:49:39,275
ISO observation to the generalized virus that includes the highest observation.

406
00:49:39,785 --> 00:49:43,355
And we look at this region and see how how large the region is.

407
00:49:47,515 --> 00:49:52,185
So the generalized well, this this is the determinant of this guy.

408
00:49:52,205 --> 00:49:59,225
And this the numerator is the determinant of the same virus estimate, but just without the highest observation.

409
00:50:00,965 --> 00:50:09,045
And then this when the value is larger than one values, larger one what that means.

410
00:50:09,095 --> 00:50:17,615
Well, if this is more than one, this is the same as GV Hedge minus rather than GV.

411
00:50:21,065 --> 00:50:25,985
So when this is larger than one or one, this is larger than this.

412
00:50:26,015 --> 00:50:35,055
This means that removing removing the eyes individual will actually increase the generalizing worse.

413
00:50:35,425 --> 00:50:40,235
Oh, sorry. So the generalized variance.

414
00:50:40,715 --> 00:50:48,695
Right. So removing this will increase. So including the highest observation will decrease the generalized generalized variance.

415
00:50:48,965 --> 00:50:55,225
So in other words, the individual reduce the variance or increase precision.

416
00:50:55,415 --> 00:50:59,254
So in other words, in observation.

417
00:50:59,255 --> 00:51:04,085
With this, we shall learn a lot what it means. This observation is kind of a good observation.

418
00:51:04,405 --> 00:51:10,275
So modeling one is means, well, this is a good one.

419
00:51:10,385 --> 00:51:13,715
Good observation. Smaller than one. It means a lot.

420
00:51:13,745 --> 00:51:20,405
This is a back which the patient we're learning has high evolution about what's good or bad.

421
00:51:20,445 --> 00:51:24,725
Well, how? Do not know the ideal exclusion? How do you get an idea?

422
00:51:25,205 --> 00:51:28,955
So if you larger than one and you are, we don't need to worry too much about it,

423
00:51:28,955 --> 00:51:34,055
but it is smaller than one that we either pay close attention to that particular data point.

424
00:51:35,465 --> 00:51:38,874
And usually people follow this criteria.

425
00:51:38,875 --> 00:51:43,655
So if the original larger than this guy was like this guy and you are,

426
00:51:43,655 --> 00:51:49,775
we need to pay closer attention to this particular individual and see whether there is something going on,

427
00:51:50,415 --> 00:51:54,245
for example, entry error or including error or something like that.

428
00:51:57,485 --> 00:52:01,385
So the KUMARIS regional, it has another flaw.

429
00:52:02,675 --> 00:52:06,875
It can be expressed as a product of this to the product.

430
00:52:08,915 --> 00:52:12,034
Now, once we express it as the product, that is true,

431
00:52:12,035 --> 00:52:23,135
we have we can actually have a better idea how the leverage and the residual actually might impact the contribution to the influence.

432
00:52:24,735 --> 00:52:32,675
So now so for now, suppose first let's suppose that the virus global change, let's see, my square does not change much.

433
00:52:34,175 --> 00:52:40,795
So this is sort of the. So.

434
00:52:55,415 --> 00:52:58,955
We tried to actually connect this to the telegraph.

435
00:52:58,965 --> 00:53:03,365
We have. We haven't. Well, we use it now in this plot here.

436
00:53:03,965 --> 00:53:07,595
Let's now focus on this rat point.

437
00:53:08,345 --> 00:53:14,975
Rat data point. Now, this one, the residual, if you look at the residuals of the actual hat.

438
00:53:15,375 --> 00:53:22,565
So I have as well having this pause or not will not change so much because see, my square depends.

439
00:53:22,565 --> 00:53:27,014
I have some hat, right? But I've still got a hat. In this case, it's almost zero.

440
00:53:27,015 --> 00:53:36,605
So. So it will not affect. It will not affect the cinema.

441
00:53:37,445 --> 00:53:40,924
The sea must work. So with and without this it applies.

442
00:53:40,925 --> 00:53:43,415
The sea must have does not will not change by March.

443
00:53:43,955 --> 00:53:51,905
Or you can consider this case as to the rat the read the radical change on this on that and a few other logic.

444
00:53:53,465 --> 00:54:01,595
So in that case now a high leverage the red dot a red dot as far away in terms of X.

445
00:54:01,715 --> 00:54:08,135
So it has a high that reach those. The high leverage is a high leverage, H is large.

446
00:54:09,305 --> 00:54:11,825
So then this will make well,

447
00:54:11,945 --> 00:54:20,295
if I have a large H that it will make this whole thing large and the average as then you will have the covariance ratio increase

448
00:54:20,295 --> 00:54:34,475
increased and it was this increase it it means that what was this increase because this covariance ratio is defined as this guy.

449
00:54:34,475 --> 00:54:43,505
So it was this increase that means having or removing this actually will increase the variance

450
00:54:44,435 --> 00:54:50,764
for including basketball and will decrease the barriers for decreasing the variance.

451
00:54:50,765 --> 00:54:57,365
That means increasing the precision. So in other words that are the high average will increase the precision.

452
00:54:59,165 --> 00:55:03,515
So this density agrees very well with a graphical illustration.

453
00:55:03,545 --> 00:55:06,065
Now let's go back to the figures is much easier to see.

454
00:55:06,665 --> 00:55:14,165
So this round out of here now, you know, has higher average because these x value is far away from the rest,

455
00:55:14,345 --> 00:55:18,945
but is absolute value is very close to zero for almost all of my life.

456
00:55:18,945 --> 00:55:29,885
So Epsilon I have is is almost zero. Then if you look at this guy having this data point it it will not change the estimate of beta by much,

457
00:55:30,395 --> 00:55:34,415
but it will increase the precision estimate of better.

458
00:55:34,715 --> 00:55:38,795
The reason is that now imagine if we do not have this data points.

459
00:55:38,855 --> 00:55:44,825
Imagine there's no variable. And then when you finish the line for the rest of the data points.

460
00:55:45,185 --> 00:55:49,625
Now if you change the line by quite a little bit.

461
00:55:49,955 --> 00:55:58,274
By a little bit. Well, there's no reason to tell which, at least graphically.

462
00:55:58,275 --> 00:56:04,455
There is no way to tell. Like if you switch gears, if you're slightly rotated as it becomes better or worse.

463
00:56:04,905 --> 00:56:12,825
Right. However, with this data point now and there's been a point well fitted this model not because of this particular data point,

464
00:56:13,245 --> 00:56:16,635
the line has to most like it build this line.

465
00:56:17,265 --> 00:56:24,525
It cannot be, for example, if you slightly rotate this line, that it'll become far away from this red data point.

466
00:56:25,135 --> 00:56:33,605
And so that means when you ask me to back it up with this red dot in it, it actually will increase the precision.

467
00:56:33,615 --> 00:56:43,095
It will because I mean, it just make this line more easily to what it is, more easily to determine what line it is.

468
00:56:44,475 --> 00:56:50,535
So this as well agree with our intuition and this one here we're gonna make.

469
00:56:50,955 --> 00:56:52,635
It's just precisely to that point.

470
00:56:52,965 --> 00:57:06,675
So for that red dot, I mean having a higher average but but a little absolute and without having that really increased precision on the other hand.

471
00:57:07,005 --> 00:57:13,595
Now if leverage is equal to zero, never able to zero means that the x value.

472
00:57:13,605 --> 00:57:19,385
Well, this means that this means that x value is just equal to the average.

473
00:57:19,395 --> 00:57:22,395
X is on average equal to zero.

474
00:57:22,965 --> 00:57:34,745
So it's just a the average average X that if leverage is equal to zero, while that means here this is equal to zero, then this is equal to that.

475
00:57:34,755 --> 00:57:37,935
This would be equal to one, this is equal to one.

476
00:57:38,325 --> 00:57:47,354
And then if the individual is an outlier, that means as large epsilon hat one has more have.

477
00:57:47,355 --> 00:57:55,675
So I had them than including.

478
00:57:56,055 --> 00:58:03,915
Oh, sorry. If we can just launch Epsilon ahead then of course including the data points will increase.

479
00:58:03,915 --> 00:58:07,754
C must wear by quite a bit because too must worry the son of the capsule.

480
00:58:07,755 --> 00:58:13,545
I have. So if what you do as large as I have that it will increase c must grip.

481
00:58:13,875 --> 00:58:22,455
So in that case the sigma without lies individual divided by sigma square will the size individual will be less than one,

482
00:58:23,145 --> 00:58:27,225
less than one, and then here less than one.

483
00:58:27,285 --> 00:58:32,745
And that if you read this to power P, it will become much less than one, much less than one.

484
00:58:33,015 --> 00:58:45,495
So if this is much, lot less than one, so if you look at if you look at how this is defined, how this is defined, if this is much less than one,

485
00:58:46,665 --> 00:59:01,245
is much less of one, then that means this is so with the data point, this DV becomes much larger, a generalized variance become much larger.

486
00:59:01,695 --> 00:59:12,605
So that means having this size imager will decrease precision because it increases the generalized virus or or for various essentials.

487
00:59:13,455 --> 00:59:19,245
So this actually corresponds to the the Green Point on that plot.

488
00:59:20,445 --> 00:59:23,535
Now, if we go back to the to the plot, we see.

489
00:59:26,875 --> 00:59:34,935
Let's take a look at this Green Point. Now, this green point, you see that is adds value is almost like, wow, this is not perfect.

490
00:59:34,945 --> 00:59:43,645
But let's just imagine, pretend that this x value does exactly equal to the average of x as an average equal to zero.

491
00:59:44,065 --> 00:59:49,015
But also by hand, this is kind of large compared to other data points.

492
00:59:49,885 --> 00:59:53,615
And in this case, having this data point.

493
00:59:53,635 --> 00:59:54,985
So we now have this data point.

494
00:59:55,015 --> 01:00:04,705
So if we don't have this data points, then the line would look roughly look like this, but it would instead of data points.

495
01:00:05,065 --> 01:00:14,874
Now, of course, having this red green data points at a real high resolution will pull the line up a

496
01:00:14,875 --> 01:00:21,085
little bit and then it will decrease the precision estimate better because of the line,

497
01:00:21,205 --> 01:00:28,555
because of the large epsilon hat, because large runs in terms of content by this data point.

498
01:00:29,035 --> 01:00:40,975
So this is precisely the point that is we are trying to illustrate here, sad but important.

499
01:00:41,935 --> 01:00:45,325
So in other words, that in summary, this common visual,

500
01:00:45,355 --> 01:00:54,495
this is actually a matter measuring how much of the influence is of the AIS observation on the various estimates of data.

501
01:00:55,915 --> 01:01:03,025
Okay. So and finally, let's summarize what we have so far.

502
01:01:03,505 --> 01:01:11,185
So for this influence diagnostics, it's better to use them as descriptive tools.

503
01:01:11,545 --> 01:01:15,355
There's we do not have formal pass. If you look at this, we do not have formal text.

504
01:01:15,835 --> 01:01:25,585
We do not rely on P value, for example, to tell whether that observation is good or not is merely based on by looking at the plot and also by

505
01:01:25,585 --> 01:01:32,295
looking at by calculating the relative this different to different criteria and then use that rule of thumb,

506
01:01:32,305 --> 01:01:37,825
use the cutoff. So that's the cut off. Do we tell the truth or not?

507
01:01:39,565 --> 01:01:50,604
And well, again, of course, then people may use different criteria when defining the observation.

508
01:01:50,605 --> 01:01:59,395
So like then if you look at the same datasets, if you apply all the criteria in high now, we may not have exactly the same in conclusion.

509
01:01:59,455 --> 01:02:04,745
And so some observations. You may think they are highly influential or you may think we are influential.

510
01:02:04,825 --> 01:02:10,935
And I, based on the criteria I use, I may think they are okay, but it is not about them.

511
01:02:10,945 --> 01:02:17,055
This is not to say that, you know, a, you are tracking I'm wrong or I am crying, you are wrong.

512
01:02:17,065 --> 01:02:23,604
It's just that we are using these criteria and if observation is indeed is very influential,

513
01:02:23,605 --> 01:02:36,225
to then use an even criterion should reach from the same conclusion and for that very extreme of and when possible.

514
01:02:36,235 --> 01:02:40,134
Now we need to was we see these highly influential observations.

515
01:02:40,135 --> 01:02:45,475
We need to actually investigate their value and see whether they are indeed the true values.

516
01:02:46,075 --> 01:02:55,275
And usually we do not directly exclude the highly influential media from their houses.

517
01:02:55,285 --> 01:03:04,225
We need to go back to the data voice and see whether there is an error before we take and edit.

518
01:03:06,295 --> 01:03:14,065
Okay, so that's this module. Any questions before we move on to the next one?

519
01:03:14,215 --> 01:03:28,014
What are or are there situations where we would favor one measure over another one matter?

520
01:03:28,015 --> 01:03:35,035
You mean like we have different these different matters? Yeah, we have all these different ways to measure influence.

521
01:03:35,215 --> 01:03:45,535
And so I'm wondering, you know, are there certain questions or contexts where we that would push us to use one over another?

522
01:03:46,855 --> 01:03:53,445
Well, that's a really good question. Now, this measure is this sort of measure of different things.

523
01:03:53,455 --> 01:04:05,865
So go back to this address. So the first one many focused on the feeling that and the second one better focus on the individual better.

524
01:04:06,345 --> 01:04:13,945
So now here, while the second one will lead through and into several additional criteria for each.

525
01:04:14,295 --> 01:04:19,845
Right. And again, the third one was discussed. This is this is looking at the overall banner.

526
01:04:20,235 --> 01:04:26,925
And last one is look at the various has made on banner. So each matter actually focus on different things.

527
01:04:27,645 --> 01:04:35,955
And of course, if you talk about them, then applying this different matters may actually leads to a different conclusion in the end.

528
01:04:36,345 --> 01:04:41,025
So it depends on which quality you're more interested in.

529
01:04:41,775 --> 01:04:45,105
You may want to rely more on one than the other.

530
01:04:46,695 --> 01:04:56,585
But yeah, there's no hard rule saying that you have to look at, you know, you have to make your decision based on the,

531
01:04:56,595 --> 01:05:00,845
you know, for example, the first the very first one, there is no such sort of thing.

532
01:05:00,855 --> 01:05:05,535
And you just take a look at them and then like overall and make an assessment overall.

533
01:05:07,875 --> 01:05:17,595
So this is so this is this is a place where the conclusion could be a little bit subjective depending on which criteria you are looking at.

534
01:05:23,855 --> 01:05:30,165
Any other questions? Okay.

535
01:05:30,185 --> 01:05:34,235
And then, if not, then let's move on to the next module.

536
01:05:34,265 --> 01:05:38,375
This is multicolored energy. This is also a very important topic.

537
01:05:39,815 --> 01:05:43,384
We have seen this in at the beginning of the semester.

538
01:05:43,385 --> 01:05:55,475
And hopefully, I mean, you guys I think you guys probably should to check this in your your name as a for the final product,

539
01:05:55,475 --> 01:06:00,845
because this indeed is a step that cannot be skipped when doing of analysis.

540
01:06:02,525 --> 01:06:13,355
So Michael, in the energy, this is a coalescing and without paying much attention to it, it really is too problematic conclusion in the end.

541
01:06:14,615 --> 01:06:15,455
So Michael, in error,

542
01:06:15,455 --> 01:06:25,145
it means that these are matrix recall that we have to do that matrix has four columns and corresponding to different communities like age,

543
01:06:25,145 --> 01:06:30,844
gender, height, weight for example, and money.

544
01:06:30,845 --> 01:06:39,545
Clean energy means that some columns, maybe one or maybe multiple, they are highly correlated with the other columns.

545
01:06:40,745 --> 01:06:47,655
And this could be that, you know, one particular column is equal to another column times the constant, right?

546
01:06:47,785 --> 01:06:55,995
This could be one cause. For example, let's say you have weight, you have height both in the DeRozan's and weight in height.

547
01:06:56,195 --> 01:07:01,225
Let's say that you know the part where I say for.

548
01:07:01,535 --> 01:07:10,115
Yeah. So when I have height and let's just say that they have a very strong linear association and then this tool will be highly rated.

549
01:07:11,015 --> 01:07:18,515
So that's one case. So another thing is that one parallel column is roughly equal to a linear combination of a few other columns.

550
01:07:20,195 --> 01:07:28,025
So it may not be equal to one, but you got a column was positive, but it may be equal to a linear accommodation while other columns.

551
01:07:29,965 --> 01:07:40,825
So yeah both both ads we can these two done but equally narrowed the problem so when this occurs but mathematically if you think about

552
01:07:40,825 --> 01:07:50,635
this math it means that X matrix there are columns that are linear combinations of others so that this matrix is not a full rack anymore,

553
01:07:51,275 --> 01:07:57,095
not a full rack and a some columns. They are redundant because some column they are linear combination of.

554
01:07:57,325 --> 01:08:13,584
It's also that. And then because when we calculate the as major both as major that happens equal to this times and beta have y is

555
01:08:13,585 --> 01:08:21,115
equal to a sorry the variance of beta had a zero so both as were involved currently the inverse of this matrix.

556
01:08:22,225 --> 01:08:30,165
But when x is not a full rank, this in the matrix is not a vertical, so that mathematically there will be a problems.

557
01:08:30,925 --> 01:08:32,335
So this is a mathematical problem.

558
01:08:33,265 --> 01:08:41,245
But in practice, even if we do not see perfect body economy, continuity, a perfect linear association, but even the correlation is very high.

559
01:08:41,845 --> 01:08:48,255
Then this matrix is the so-called E0 collision, so that although mathematically it may still be vertical,

560
01:08:48,265 --> 01:08:53,695
but a very common inverse inverse matrix will have very large values.

561
01:08:54,865 --> 01:08:58,194
Some Andries and this will blow out the ligaments.

562
01:08:58,195 --> 01:09:04,165
So for example, we carry the inverse of this, they will have some very large values along.

563
01:09:04,165 --> 01:09:07,405
But I know that there is a way to hack.

564
01:09:07,435 --> 01:09:18,365
We'll have more planets. So this is the problem of medical genealogy and there are many things that occur least dramatically.

565
01:09:18,895 --> 01:09:28,134
And if you are about this, just a few example, by no means is this an exclusive list of reasons, just a few examples.

566
01:09:28,135 --> 01:09:33,444
So, for example, inadequate assumption in adequate assembly.

567
01:09:33,445 --> 01:09:39,955
What we mean is that you want a sample, you want to have a sample that is pure genius enough.

568
01:09:39,985 --> 01:09:45,445
You want to include, for example, both males and females and both younger people and older people.

569
01:09:45,835 --> 01:09:52,335
And it goes in a couple of different, different reserves and a cover, you know, people will differ different way.

570
01:09:52,495 --> 01:09:56,365
So in other words, you want to know your sample to be if you were genius enough.

571
01:09:57,805 --> 01:10:08,425
However, if you from the very beginning, if you did a kind of about a sample as a sample, a very homogeneous sample, and that has only,

572
01:10:08,575 --> 01:10:17,695
they say, for example, females and 20 years old and then, you know, like a survey with a certain sort of a certain height, a certain weight.

573
01:10:18,205 --> 01:10:25,885
In that case, very likely you have or likely you will have this medical inherited problem because you are covariates.

574
01:10:25,885 --> 01:10:30,415
They are they are they're so, so homogeneous that they are highly correlated.

575
01:10:31,345 --> 01:10:37,585
So that's one possible reason. Another possible reason is the poor choice of model covariate.

576
01:10:38,065 --> 01:10:43,855
So there might be cases where the variables they are married the same thing.

577
01:10:44,275 --> 01:10:58,795
So for example BMI and weight. Right. Or the a way matter blood pressure you have or you have different different ways of measuring blood plasma.

578
01:10:58,795 --> 01:11:03,165
You have systolic blood pressure, you have high quality.

579
01:11:03,805 --> 01:11:13,735
Another good example now about a oh, let's say the the the most study on a la la la la la.

580
01:11:13,885 --> 01:11:16,794
I mean, we saw that a very example.

581
01:11:16,795 --> 01:11:23,185
And I think, you know, there are first lecture so you have the blood level and then you have the bone that resolves.

582
01:11:23,215 --> 01:11:28,195
So they both matter the level in your body, but they are highly correlated.

583
01:11:28,585 --> 01:11:32,635
So if you throw both variables into the model, then they are highly correlated.

584
01:11:32,875 --> 01:11:39,115
So this is the poor choice of moral covariance and another one is E0 collision the covariance.

585
01:11:39,415 --> 01:11:47,125
This mostly occur when for a categorical variable, if for one particular category there are few subjects.

586
01:11:47,425 --> 01:11:54,485
So for example, let's say you have. You take samples?

587
01:11:56,195 --> 01:12:00,305
I say, well, we consider the sex negative.

588
01:12:00,455 --> 01:12:09,635
We have males and females. But let's say your sample somehow that you you can go there because it's somebody you you have to feel.

589
01:12:10,055 --> 01:12:17,735
Yes. And most of them are females. And well, in that case, you might have also have this medical narrative problem,

590
01:12:17,735 --> 01:12:26,895
because most of because of a large proportion of this is because most of the values are either was or zero.

591
01:12:26,925 --> 01:12:29,855
So if you think in terms of time variables and then of course,

592
01:12:29,855 --> 01:12:34,805
this will be highly correlated with at least the intercept because the intercept is a column once.

593
01:12:35,165 --> 01:12:41,885
And if you if you have the categorical variable, then only for most of the subjects, they will fall within one particular category.

594
01:12:42,095 --> 01:12:46,095
Then that variable is either just a column was for column zeros.

595
01:12:46,315 --> 01:12:53,645
Right. So, so that also leads to medical and the consequence of medical problem.

596
01:12:53,655 --> 01:13:02,105
So the major issue is while we have meter issues, when we we are more interested in admission or inverse.

597
01:13:02,375 --> 01:13:08,495
This is just like what we have just mentioned.

598
01:13:15,755 --> 01:13:22,655
The reason is that not for isolation. And if we're both assumption inference, we have to cover the inverse of this matrix.

599
01:13:23,825 --> 01:13:31,625
If there is not a clue in energy, this matrix is almost like but in the movie word event in right or.

600
01:13:31,985 --> 01:13:41,915
So this is so the molecule inanity is most trouble is most troubles solved when we are more interested in estimation or inference.

601
01:13:43,535 --> 01:13:47,645
So it becomes less worrisome if we are.

602
01:13:47,765 --> 01:13:51,005
What we are interested in is making a prediction.

603
01:13:51,605 --> 01:13:55,865
If you just want to predict what a y y value is.

604
01:13:55,985 --> 01:13:59,165
So in other words, you just want to calculate this y hat.

605
01:14:00,965 --> 01:14:09,515
This is less worrisome because as long as you get your data and then you can use that data to predict this y hat.

606
01:14:11,045 --> 01:14:19,444
However, this is less worrisome. MARTIN Or this less worrisome.

607
01:14:19,445 --> 01:14:24,755
But still, there are scenarios where this might be probable problematic.

608
01:14:25,145 --> 01:14:30,724
One scenario is that this is the new observation you are trying to make a prediction.

609
01:14:30,725 --> 01:14:36,485
So, for example, let's say you kind of beat us at the the the most of the individual we're all on one

610
01:14:36,485 --> 01:14:40,835
individual in your data sat there between you know 20 years old you are 30 years old.

611
01:14:41,645 --> 01:14:49,535
And then you build your model and then you're trying to use your model result to predict the subject that is, you know, six years old.

612
01:14:50,135 --> 01:14:56,555
And because that age is so outside the range of the data set and collected, that is there is medical.

613
01:14:56,555 --> 01:15:04,445
And, you know, the problem is you want to make a prediction for that to six years old, then there may be some some issues.

614
01:15:06,275 --> 01:15:10,145
So you are the worse the Y you try to make extrapolation.

615
01:15:10,595 --> 01:15:18,155
What we mean by extrapolation is that your data that only has age, for example, between 30, 20 and 30,

616
01:15:18,365 --> 01:15:25,355
but a you are trying to make a conclusion for some somebody because with age 60 you are in line, but then you are at four.

617
01:15:25,685 --> 01:15:30,215
As for relating to world region where you didn't observe any data points,

618
01:15:30,725 --> 01:15:37,445
in that case you might have some trouble seeing whether somebody from the narrative problem,

619
01:15:38,885 --> 01:15:43,084
but generally speaking, generally speaking is less worrisome if you are interested.

620
01:15:43,085 --> 01:15:51,275
It is in making predictions, but it is definitely, definitely a problem if you are interested in estimation and inference.

621
01:15:54,995 --> 01:15:59,045
So one calls it was one very easy way and it was very easy.

622
01:15:59,165 --> 01:16:03,184
Of course, we are going to talk about a lot more on that. Well, very easy way to look at it.

623
01:16:03,185 --> 01:16:07,925
Whether you have a medical analogy is to look at a general area.

624
01:16:09,275 --> 01:16:16,505
If you see an unusually large center out here as much then as you are, and that's a sign of modularity.

625
01:16:16,805 --> 01:16:25,095
That's the reason is that if you have years. If you take the inverse, the inverse that I want out of this will be huge.

626
01:16:25,635 --> 01:16:28,935
And then if you cover the parents various, it will be huge.

627
01:16:29,145 --> 01:16:35,315
It will give you huge center out. So if you look at our output, look over the tables and look at a center out.

628
01:16:35,955 --> 01:16:43,545
If you see you in the large center errors, that's usually a sign that you have no problem.

629
01:16:46,975 --> 01:16:54,415
Now let's take a look at some more formal ways of determining whether there is some medical immunity.

630
01:16:55,285 --> 01:17:02,125
So the virus of beta j can be theorized can be written in this funny way.

631
01:17:02,125 --> 01:17:07,995
So we have seen ones where there's variability absent times one over as as j.

632
01:17:08,155 --> 01:17:11,215
This variability in X and then perhaps this guy.

633
01:17:12,755 --> 01:17:18,915
So the last guy here, this is referred to as the various inflation factor we've been.

634
01:17:22,025 --> 01:17:29,075
The Origins Square and exactly the artworks.

635
01:17:30,235 --> 01:17:39,775
By feeding the model, treating it straight as the outcome, and treating the rest of the rest as covariates.

636
01:17:40,615 --> 01:17:42,535
So this is just our square for this moment.

637
01:17:43,255 --> 01:17:54,905
This becomes very intuitive that because if there is high commonality, particularity that it means and state actually is strongly,

638
01:17:55,225 --> 01:18:01,225
quite strongly associated with the other states and that if you run a linear regression model,

639
01:18:02,005 --> 01:18:07,795
the square of a linear regression model should be relatively high with a large proportion.

640
01:18:09,145 --> 01:18:13,945
Either variation of that state can be explained by the rest the other x.

641
01:18:15,595 --> 01:18:20,385
So then the orange square that is just taken to be the square for this model.

642
01:18:21,985 --> 01:18:39,915
The large part is where indicates that this actually has a very strong linear association of the rest of S and then it will have that large R square.

643
01:18:39,925 --> 01:18:47,555
If you have a large R a square, then you have a large of the spot F and then you will have a large beta chain packed.

644
01:18:48,305 --> 01:18:59,455
And so this is what about how the vif is defined is defined as just one over one minus RJ squared.

645
01:19:03,865 --> 01:19:05,694
So if you look at this guy,

646
01:19:05,695 --> 01:19:13,524
this does not depend on what if you look at how how this defined it was only independent on our case with one artist where exactly

647
01:19:13,525 --> 01:19:22,975
the square for this model by treating it straight as the response resonates as the covariate so it has nothing to do with why.

648
01:19:23,965 --> 01:19:28,095
So he either was while this valve does not been a lot.

649
01:19:28,115 --> 01:19:35,725
So this intuitively makes sense because because we're looking at multiple the energy of all this x, so this shouldn't depend on what.

650
01:19:36,595 --> 01:19:46,075
So we just look at whether there's clean energy of a moment X and also it is invariant under a century and a screening of covariate.

651
01:19:46,085 --> 01:19:52,135
So it doesn't matter whether you send her covariate and skew your covariate, the BNF will stay the same.

652
01:19:54,715 --> 01:19:58,465
So. So for example,

653
01:19:58,465 --> 01:20:08,275
if you standardize and say you send her extra and you send her outside and then you call a new covariance z j this is a center bastardized end state.

654
01:20:08,875 --> 01:20:13,795
Then the VII f remain the same. There's no change by that.

655
01:20:15,895 --> 01:20:21,315
And another thing about virus is that VM is always larger than we're able to.

656
01:20:21,385 --> 01:20:24,435
And the reason is that this is defining this way. Right.

657
01:20:24,445 --> 01:20:28,645
But but we know that R2 square, this is always between zero and one.

658
01:20:29,275 --> 01:20:33,895
So that's why this VHF is always larger than one.

659
01:20:35,335 --> 01:20:39,745
So we all have equal to one means that all is equal to one.

660
01:20:39,745 --> 01:20:50,695
When our gain is equal to zero, what it means, it's true, has nothing, has no linear association at all with the rest of x of the resonance.

661
01:20:51,085 --> 01:20:55,465
So then that means and today is uncorrelated with other covariance.

662
01:20:55,885 --> 01:21:00,025
So this is just this is one extreme case you have equal to one.

663
01:21:00,355 --> 01:21:06,025
You know, generally speaking, people use ten as sort of a cutoff.

664
01:21:06,025 --> 01:21:19,075
So if we see a VR half larger than ten, you're we think there is a large amount of medical energy between this particular act state and the rest of X.

665
01:21:21,895 --> 01:21:25,405
So this is, again, the one criterion that people use.

666
01:21:26,005 --> 01:21:35,215
They are. Okay.

667
01:21:35,215 --> 01:21:39,925
So here is another interpretation of VHF.

668
01:21:42,085 --> 01:21:45,445
So we have if we look at before this is.

669
01:21:47,445 --> 01:21:53,155
This is actually the formula we just throw down. Oh, it's this guy over here.

670
01:21:53,175 --> 01:21:56,175
This is just a well, this is just a mathematical fact.

671
01:21:56,205 --> 01:22:01,335
Okay. So there is no nothing more nothing too much to to say here.

672
01:22:01,485 --> 01:22:06,795
So the virus of the hand is equal to the day of diagonal elements of this matrix.

673
01:22:07,215 --> 01:22:11,655
And of that it can be mathematically, it can be written in this particular way.

674
01:22:12,585 --> 01:22:18,105
So this is the virus of beta j hat. That is what we have here.

675
01:22:18,135 --> 01:22:24,325
So this is the weirdness of being hacked. Now compare.

676
01:22:24,375 --> 01:22:31,815
Well, now consider a case where this X chain has no linear association.

677
01:22:32,295 --> 01:22:38,295
Well, the rest of it acts at all. Now it is uncorrelated with all the rest of X.

678
01:22:38,985 --> 01:22:42,854
In that case, the r j is equal.

679
01:22:42,855 --> 01:22:52,335
R2 square is equal to zero. That's because our square is actually the square by regressing x ray on the rest of x.

680
01:22:53,745 --> 01:23:03,195
Now, if that state has no linear association at all with the rest of x, then of course this R square for this model is equal to zero.

681
01:23:03,775 --> 01:23:09,275
There are days where it's equal to zero. Okay, so what are J square root zero?

682
01:23:09,285 --> 01:23:16,485
That means if x j is completely unrelated to the other unrelated to the other x,

683
01:23:17,175 --> 01:23:24,045
then the variance of better j becomes becomes less because artist square is equal to zero.

684
01:23:25,095 --> 01:23:27,345
So then we are this becomes this.

685
01:23:28,035 --> 01:23:40,605
So then we are f can be considered as the ratio between this guy out of this class, which now is actually this of the F and this is VII F.

686
01:23:40,905 --> 01:23:55,485
So the value of is equal to the variance of J and divided by Levinas of the G where this is this binary hat.

687
01:23:55,785 --> 01:24:09,595
And this is to be that we had a denominator. And looking at it this way, it's just help us to understand the virus better, hopefully.

688
01:24:10,165 --> 01:24:16,465
So what it means is that we compare the virus estimate for better day.

689
01:24:18,625 --> 01:24:27,835
Well, in our in our data set to a case, we're assuming there is no correlation at all between our handshake and all the other apps.

690
01:24:28,495 --> 01:24:37,045
And we see how much the virus got inflated, how we see how much, you know, there is got it inflated by a lot.

691
01:24:38,155 --> 01:24:48,985
Not only is there is medical analogy going on, if the virus is not inflated by much pretty much similar to that,

692
01:24:49,255 --> 01:24:52,375
what real world we are, we are half is close to one.

693
01:24:52,765 --> 01:24:54,845
That means there is not much air.

694
01:24:57,235 --> 01:25:08,225
So this is another just another way of interpreting the r f as the ratio, comparing the case in our dataset and also the like,

695
01:25:08,295 --> 01:25:15,385
the real case and a case where you we're assuming they're pretending there is we're assuming there's no correlation.

696
01:25:25,675 --> 01:25:35,785
So. If I ask you if you can help on behalf.

697
01:25:36,265 --> 01:25:37,935
Now again, this is the Viagra.

698
01:25:38,275 --> 01:25:49,195
If you come to me, I have what we have can tell us which reach state because you cannot is for it straight for each extra for each contact.

699
01:25:49,765 --> 01:25:59,425
So it can count. It can tell us which covariate, which exchange has a strong linear association with the rest of x.

700
01:26:00,105 --> 01:26:17,785
So, for example, it tells you that, you know, let's say if you included both, both in college age education level and include a few other variables,

701
01:26:17,785 --> 01:26:25,315
digital data that and this might tell you old age actually has a strong linear association with the other covariates.

702
01:26:26,035 --> 01:26:33,265
So it tells you which one has strong linear associated with all the covariates because the corresponding various will be large.

703
01:26:34,945 --> 01:26:45,785
However, the v i have two now tells you oh, which of the two remaining covariates are the ones that explain the variance of x j.

704
01:26:46,015 --> 01:26:56,125
So for example, again, maybe let's be more specific, let's say you are trying to model a particular response.

705
01:26:59,145 --> 01:27:11,525
I have a super good example, let's say. London counting of a very good continuous lesson about this.

706
01:27:11,675 --> 01:27:16,595
So let's say blood pressure. So you are you are modeling people's blood pressure.

707
01:27:16,655 --> 01:27:28,305
So that's the response. And the cool areas that I'm considering include people's income, age, gender allocation level and so forth.

708
01:27:28,335 --> 01:27:40,465
Right. So well, in that case, if we consider this this few Kumaris like income, age, gender allocation level and then if you calculated our J Square,

709
01:27:40,475 --> 01:27:48,615
let's say for income let's say income strongly depends off of age, gender and let's say educated now.

710
01:27:48,905 --> 01:27:58,045
So then you will see that. Well, tell me about the I have for income ratio rather large B I am for income that indicates that.

711
01:27:58,055 --> 01:28:03,815
Okay, so the income variable this covariate may be strongly associated with others.

712
01:28:03,815 --> 01:28:12,665
There may be some modicum narrative. However, it doesn't tell you which of the other three their age gender allocation model,

713
01:28:13,025 --> 01:28:19,795
which one of those or which combination of those might be strongly associated with Lagos,

714
01:28:20,075 --> 01:28:27,665
which just tells you that income is strongly, strongly associated with others, but it doesn't tell you which one it is strongly associated with.

715
01:28:28,925 --> 01:28:31,505
So this is what this list is trying to say.

716
01:28:37,615 --> 01:28:46,375
So in order to find out which covariates might be strongly associated with with income, now we need to do a further investigation.

717
01:28:52,375 --> 01:28:56,025
So the remedies are what what we want to do if we have political party.

718
01:28:56,305 --> 01:29:00,715
Well, we do. Typically, we would remove. I mean, this is simple and also very effective.

719
01:29:00,925 --> 01:29:07,015
We will remove one or more collieries involved in a coordinated problem.

720
01:29:08,965 --> 01:29:17,845
So by removing this, usually the regression, some the square or the fit mechanism theater of the model usually does not change by much.

721
01:29:18,175 --> 01:29:23,575
The reason is that, you know these acts because a lot of narratives, they provide redundant information.

722
01:29:24,145 --> 01:29:29,544
So if we remove one of these x while the information they provide state remained

723
01:29:29,545 --> 01:29:34,675
pretty much the same does not change to match the information that they provide.

724
01:29:34,705 --> 01:29:40,675
So in the end, the policy regarding the central square remain pretty much the same.

725
01:29:41,635 --> 01:29:46,594
Or if that's not, it's not reduced by too much. You know what?

726
01:29:46,595 --> 01:29:50,245
We remove those variable, we might remove one at a time.

727
01:29:50,255 --> 01:29:54,065
We do not want to remove too many at once.

728
01:29:55,265 --> 01:30:03,105
And also, we could actually look at the that will remove the one with the largest VEGF and then look at that.

729
01:30:03,155 --> 01:30:10,595
Take another look to see whether the the remaining covariate is still have the third part and then decide whether to enforce.

730
01:30:11,425 --> 01:30:16,115
Of course, this removing colvera also needs to depend on your scientific interest.

731
01:30:16,385 --> 01:30:24,515
So let's say you want to study the relationship between blood pressure and people's weight ranges,

732
01:30:24,815 --> 01:30:28,505
but in the commentary you consider what color is weight and height?

733
01:30:28,895 --> 01:30:36,995
And they are highly correlated. You want to remove one of those, but it becomes all your interest is in association between laboratory and weight.

734
01:30:37,175 --> 01:30:44,825
So you definitely want to keep wetting the model by removing height so you wouldn't really more weight because that's your collateral interest.

735
01:30:45,155 --> 01:30:52,025
So, so removing which which variable usually also depends on the question of interest.

736
01:30:55,845 --> 01:30:59,085
Okay. That's my peculiarity. So any questions?

737
01:31:00,525 --> 01:31:06,584
Yes. To avoid a multi calendar event result before even feeding the model,

738
01:31:06,585 --> 01:31:14,235
can we identify this problem just by looking at the correlation matrix between x?

739
01:31:14,535 --> 01:31:22,925
Well, yeah, definitely. Yes. So the recall that if there's one one model, I think before Thanksgiving, we look at, you know,

740
01:31:23,625 --> 01:31:31,215
a feed figure, a plot, it shows the the collisions, among the correlations among the covariates.

741
01:31:31,575 --> 01:31:35,835
And then we pointed out there was a particular value that is value, large point.

742
01:31:37,575 --> 01:31:41,325
That would be the around 4.8 of correlation. That's a relatively high correlation.

743
01:31:41,745 --> 01:31:49,725
And if you see such a correlation between two marathons, then you might want to be a little more careful because that might be used for medical.

744
01:31:58,025 --> 01:32:05,425
Okay. So then just a very quickly, I'll finish the another this is another remedy.

745
01:32:05,435 --> 01:32:12,245
This is more complex than removing a and we will covariance probably the simplest solution.

746
01:32:12,245 --> 01:32:15,425
Right? It is very effective. Simplest solution.

747
01:32:15,785 --> 01:32:18,815
No alternative solution is the so-called called regression.

748
01:32:20,715 --> 01:32:31,845
Rita regression is based on the idea that now for a simple linear regression this about even in the present model inheritance,

749
01:32:32,295 --> 01:32:38,325
the simple linear regression to beta hat is still unbiased because as we mentioned, beta hat is equal to this.

750
01:32:41,345 --> 01:32:48,575
So it doesn't matter whether we have multiple inanity or not. As long as this is still a convertible, then you are still able to calculate that.

751
01:32:49,055 --> 01:32:56,464
And then while these valuations to zero so some always unbiased but is very stable

752
01:32:56,465 --> 01:33:03,485
because the virus because the virus virus is actually equal to which is equal to this.

753
01:33:03,725 --> 01:33:13,175
So if this is if there is multiple narrative, then you take the inverse, you will see there are a lot of variants of the virus become very large.

754
01:33:13,505 --> 01:33:18,785
So that asymmetry is not a stable one because you were two people have these.

755
01:33:19,295 --> 01:33:28,055
Okay so what if I combine take a both bias in a virus equal consideration and this is a so-called and I see that that's

756
01:33:28,055 --> 01:33:35,075
not to worry too much about it this year because in six or two we we'll have a more detailed coverage of of Amazon.

757
01:33:35,315 --> 01:33:40,835
But this is sort of a balance between bias and ignorance by taking both into account

758
01:33:42,035 --> 01:33:46,625
so that you can control both bias and awareness and neither one become too large.

759
01:33:47,915 --> 01:33:51,335
So this is a contract compromise between bias and violence.

760
01:33:52,625 --> 01:33:59,315
So we might actually introduce some bias. So significant would be ours.

761
01:33:59,675 --> 01:34:07,084
So the idea of regression is that, okay, so this matrix is not a vertical or we are not a vertical.

762
01:34:07,085 --> 01:34:15,785
So when can words you have problems then what I do is I will simply add to this causal k times.

763
01:34:16,115 --> 01:34:21,965
I don't know the matrix. If I enter this, then this matrix now becomes inverted.

764
01:34:23,105 --> 01:34:31,205
Mathematically becomes inverted so that I can infer this makes it easy and that I can calculate the so-called rich regression asymmetric.

765
01:34:32,955 --> 01:34:39,105
End of the year, people are taking this to be a very small constant and there are different ways of capital in this case.

766
01:34:39,285 --> 01:34:40,905
This is just what they would do next.

767
01:34:42,165 --> 01:34:54,195
And and on the advantage of this of of this approach is that we do not need to remove any coherence so we can still keep all the covariance,

768
01:34:54,705 --> 01:34:58,815
because sometimes, even if there is not a clean energy person,

769
01:34:58,815 --> 01:35:06,374
typically what means is less appealing to remove covariance because they are there, they are all important.

770
01:35:06,375 --> 01:35:16,245
What are really interesting seem very fat. So then in order to keep all these coherence in the model that you could actually

771
01:35:16,245 --> 01:35:19,665
follow this so-called reader machine and it doesn't remove any covariance.

772
01:35:20,415 --> 01:35:25,485
But a disadvantage is that by calculating this asymmetric limits,

773
01:35:26,445 --> 01:35:33,254
it makes things more complex because while the standard error of this as matrix and another

774
01:35:33,255 --> 01:35:37,125
corresponding inverse like constraint in common using intervals and a kind of have

775
01:35:37,215 --> 01:35:42,284
this we have a loss has to not to become a little bit that's clear like how because

776
01:35:42,285 --> 01:35:48,645
that then that all depends on this work you use it so that makes things more complex.

777
01:35:49,365 --> 01:35:52,935
So yeah so you pay something you get something that you pay a price.

778
01:35:54,885 --> 01:36:00,255
Okay, so that's regression is it's a couple of use happening to deal with that molecule.

779
01:36:02,595 --> 01:36:11,055
Okay. So we are a little bit over time. So we will stop here and on Thursday we will talk about models that action that is forward, backward.

780
01:36:11,205 --> 01:36:20,475
So it's an action. And again, so if you guys haven't done so, please stand up a minute,

781
01:36:20,595 --> 01:36:29,865
a few minutes to fill out all the evaluation that will be extremely helpful to know, to get some feedback and know how well we're going to discuss.

