1
00:00:00,390 --> 00:00:05,350
We could scream at the top of his lungs. Swine flu. Like swine flu.

2
00:00:05,490 --> 00:00:10,620
This seems like what you. And if you like, left in the mornings, like, you know, go to work.

3
00:00:10,620 --> 00:00:16,410
He would, like, stop your car. And he'd be like, what are you going to do?

4
00:00:16,530 --> 00:00:20,370
Work. Good. I worked in the factory for 15 years. But if you.

5
00:00:20,370 --> 00:00:25,790
All right. Good afternoon, everyone.

6
00:00:26,480 --> 00:00:31,340
Welcome back for welcome to the next day. And again we should all bring.

7
00:00:35,330 --> 00:00:40,640
All right. There are 25 of you here. There's 30 of you who are going to watch this later.

8
00:00:45,690 --> 00:00:53,420
We are having a conversation in this department. I think it's fine that you all manage your time as you need to.

9
00:00:53,430 --> 00:01:02,190
I think it's a little troubling when every class has 50% attendance, so I'm not going to force you guys to come.

10
00:01:03,860 --> 00:01:11,840
But I like to believe that coming here actually has a benefit besides watching me on your computer at home with your phone out and the internet.

11
00:01:12,230 --> 00:01:20,150
But anyway, I hope you use these as supplements, not as an investment for the charm that I bring to this room.

12
00:01:20,180 --> 00:01:21,320
I know this room is horrible.

13
00:01:21,350 --> 00:01:31,190
I don't want to be in this room here anyways, but I do like the topic and I do like to teach and I do hope that more of you can come.

14
00:01:32,390 --> 00:01:35,110
There is no class on Friday to make that clear.

15
00:01:35,120 --> 00:01:44,280
Again, I think everyone knows this, but I have an all day retreat at the Botanical Gardens, so I don't think we're going to be in the gardens.

16
00:01:45,600 --> 00:01:47,230
That not going to be in a room in there.

17
00:01:48,080 --> 00:01:57,770
But anyway, so there is no class on Friday and fortunately you are done with the data sets that you have been working with so far this semester.

18
00:01:58,280 --> 00:02:03,110
So if you're sick of whatever data set you had, you are free.

19
00:02:05,330 --> 00:02:12,139
We have done everything we want to do with continuous data and so now we're going to

20
00:02:12,140 --> 00:02:16,970
take everything we did with continuous data and we're going to apply it to groups.

21
00:02:17,870 --> 00:02:26,000
So if you don't understand girls putting a correlation matrix as a weight in a regression formula for the parameter estimates,

22
00:02:26,360 --> 00:02:31,400
if you're still not grasping that, try and grasp it because we're going to do it again.

23
00:02:31,520 --> 00:02:38,660
If you don't understand what a sandwich variance estimate does, we're going to talk about it a lot more in G in the next couple of days.

24
00:02:39,410 --> 00:02:44,960
Try and get your head around what we're doing there. If you don't understand what a random intercept does for you,

25
00:02:45,830 --> 00:02:50,600
we got to work on that because you're going to hit random intercepts in a couple weeks with not normal data.

26
00:02:51,230 --> 00:02:56,210
So the more you understood so far, the rest, the better the rest of the semester.

27
00:02:57,650 --> 00:03:06,379
Homework number four is currently due on November 2nd and it looks just like all the other homework assignments that you've done so far,

28
00:03:06,380 --> 00:03:09,620
except now you're going to download us.

29
00:03:10,790 --> 00:03:18,560
So you're going to be given one count data set and you have one binary outcome data set longitudinal.

30
00:03:19,460 --> 00:03:22,940
And so you're going to be asked to analyze both of those data sets in this homework assignment.

31
00:03:23,840 --> 00:03:28,399
And so for the count data set, you're going to do what you did before exploratory data analysis.

32
00:03:28,400 --> 00:03:31,940
Are there group differences? Is there a pattern over time? If so, what is that?

33
00:03:32,420 --> 00:03:38,600
What's the within person correlation? All those questions we asked before going to ask you to do that with a count dataset.

34
00:03:38,600 --> 00:03:47,630
In a binary dataset, binary data are really challenging to look at visually because everybody's a zero or a one.

35
00:03:48,470 --> 00:03:54,890
So think of what a spaghetti plot looks like for binary outcomes which right pretty useless.

36
00:03:56,450 --> 00:04:06,169
So exploratory data was binary is really you know set on mostly what are the proportions over time in each group sort of plot.

37
00:04:06,170 --> 00:04:12,350
But anyways, so once you do some exploratory data analysis for each those datasets and then fit three.

38
00:04:14,260 --> 00:04:17,469
Models. And they're all glum.

39
00:04:17,470 --> 00:04:19,860
So we're going to preview James.

40
00:04:19,980 --> 00:04:28,080
If you need a brush up on that from last year and we're going to insert correlation matrices into an analysis, one that assumes independence.

41
00:04:28,080 --> 00:04:31,920
So again, swimming independence is doing what you would have done in 651.

42
00:04:33,680 --> 00:04:40,380
We're going to fit and exchangeable correlation matrix and we're going to fit an auto regressive one and correlation structure.

43
00:04:40,650 --> 00:04:47,879
So fitting three different models to the data deciding which one fits the model the best you're going to get model based, standard error.

44
00:04:47,880 --> 00:04:50,940
You're going to get sandwich estimates for the variances, senators,

45
00:04:50,940 --> 00:05:01,950
everything that you did before but with two new datasets and again, depending on whether you want to use my code or not for that one.

46
00:05:01,950 --> 00:05:11,669
First on the cloud in our studio there is a homework for folder and I have a count dataset and a

47
00:05:11,670 --> 00:05:18,210
binary dataset and I have already done analyzes for my homework for assignment if I were doing it.

48
00:05:19,230 --> 00:05:25,200
So you can see again, you are welcome to use my code as much as you want or as little as you want.

49
00:05:25,200 --> 00:05:29,309
I know you guys. I like to use juju platter. A lot of you do. So maybe the planning is not useful for you.

50
00:05:29,310 --> 00:05:31,380
But anyways it's there.

51
00:05:32,160 --> 00:05:44,879
Even though we haven't talked about G quite yet, it's available to you questions on homework for you and have a cover their homework for once.

52
00:05:44,880 --> 00:05:48,930
We're done with G. That is on the exam that's coming up right before Thanksgiving.

53
00:05:48,930 --> 00:05:59,900
So Thanksgiving the exam before Thanksgiving will cover linear mixed models with continuous data and g with.

54
00:06:00,180 --> 00:06:07,710
With. G With no normal data, you can use G with normal data.

55
00:06:08,010 --> 00:06:14,550
And now you're going to see that I'm election months but she was normal data is equivalent to what we've already been doing.

56
00:06:15,060 --> 00:06:20,490
G. Normal data is jealous. So just keep that in mind.

57
00:06:20,520 --> 00:06:25,379
Just like an LM is just one example of a GLM, right?

58
00:06:25,380 --> 00:06:30,840
Where you broaden the family. It's the same way g g ls with normal data.

59
00:06:31,020 --> 00:06:35,070
Is G just got broadened later.

60
00:06:35,550 --> 00:06:41,280
So that's there for you. Last week I came up with a poll and it's still open.

61
00:06:42,360 --> 00:06:48,090
And I do want to emphasize I got seven responses so far.

62
00:06:48,570 --> 00:06:58,600
Every one of them different. Some of them have caused me to lose sleep, not because they're bad or they're good, or they're just.

63
00:06:58,750 --> 00:07:02,650
I was kind of like someone hit me in the head with something that I just didn't realize.

64
00:07:05,920 --> 00:07:07,380
These are very important comments.

65
00:07:07,390 --> 00:07:16,030
If you don't feel comfortable expressing what makes you feel less successful than you can be in classes because of anxiety, stress, whatever.

66
00:07:17,410 --> 00:07:21,340
Please type them in the survey. It's still open. I will leave it open this week.

67
00:07:23,470 --> 00:07:29,470
Am not sharing these with the department quite yet unless I see something that is is very, very troubling.

68
00:07:31,330 --> 00:07:35,980
And again, if at any time you want to talk more about what you wrote, I don't know who wrote what.

69
00:07:37,480 --> 00:07:49,300
I am happy to do so preferentially, but I would like to know. There are things in here that I never realized folks would want from me as a professor.

70
00:07:49,780 --> 00:07:53,169
And you might say, how could you possibly not know by now that I want that?

71
00:07:53,170 --> 00:07:57,880
But. Anyway. So please respond to that if you have ideas.

72
00:07:58,720 --> 00:08:03,700
Um, because I think the attendance is an indication of a bigger problem.

73
00:08:03,880 --> 00:08:08,650
How many times can I talk about attendance without telling you that you have to come anyway?

74
00:08:09,940 --> 00:08:14,470
I love that there's now a video of you shaking your finger. Yes. People out to look at it and.

75
00:08:16,440 --> 00:08:21,580
I'm talking to more of them than I am to you right now. So now some of you will probably watch this again.

76
00:08:24,190 --> 00:08:30,820
All right. We want to talk about G. G is something that I never learned in grad school.

77
00:08:31,510 --> 00:08:40,180
It was so new at that point that I heard about this mystical approach, but we weren't at a point where computers could do it.

78
00:08:41,020 --> 00:08:48,730
We didn't even have R When I was in grad school, we had sass and plus and it wasn't really ready for prime time.

79
00:08:48,970 --> 00:08:55,450
So lucky, you know, two points where we are ready to be able to do all these things.

80
00:08:57,460 --> 00:09:05,380
Again, it's not as mystical. I think perhaps once you see it again, I don't mean to trivialize some of these methods.

81
00:09:05,380 --> 00:09:15,370
The difficulty of getting a proving they work asymptotically, getting a computer to do what we want to do has taken people their entire career.

82
00:09:17,450 --> 00:09:24,460
But hopefully you'll see that if you understand genomes, G is kind of like, yeah, that, that's jealous.

83
00:09:26,340 --> 00:09:29,340
It's just the same tactics and the estimation become very difficult.

84
00:09:29,670 --> 00:09:31,440
So let's talk about allowance.

85
00:09:31,650 --> 00:09:38,340
Let's review everything you talked about last year that I think is important for knowing what we're going to talk about in the next lecture.

86
00:09:39,390 --> 00:09:43,830
So you guys are now experts with longitudinal, continuous outcomes.

87
00:09:44,670 --> 00:09:49,140
We use generalized squares. So again, using a correlation matrix for the errors,

88
00:09:49,860 --> 00:09:55,980
or we could use a linear mix model where we partition the error into between or within components of variability.

89
00:09:56,760 --> 00:10:01,200
And now we want to take both those ideas and apply them to not normal outcomes.

90
00:10:01,200 --> 00:10:05,279
And we're going to focus on binomial one. And so we've got again,

91
00:10:05,280 --> 00:10:10,889
generalized estimating equations is the extension of goals and a generalized linear

92
00:10:10,890 --> 00:10:15,270
makes models of course is the generalization of how about linear mixed models.

93
00:10:16,050 --> 00:10:20,130
So glm with random effects. That's what I'll generalize under big smiles.

94
00:10:21,090 --> 00:10:30,450
And I want to start with G and in a couple of lectures we'll get to July now, but let's again matrix algebra.

95
00:10:30,450 --> 00:10:34,830
If it is uncomfortable for you, you're going to see a lot more of it.

96
00:10:35,190 --> 00:10:42,510
Please ask me questions. Try to clarify what exactly is in my election notes if you're not interested in what we're doing here.

97
00:10:43,110 --> 00:10:48,720
But again, let's go back to independent data. One outcome per person, one continuous outcome.

98
00:10:49,800 --> 00:10:53,670
Two 650 So the mean is a linear combination of covariates.

99
00:10:53,670 --> 00:10:57,120
Times are regression parameter. Right. We read that as exit data.

100
00:10:58,170 --> 00:11:02,280
And if you write down the normal distribution and the mean is a function of this thing,

101
00:11:02,280 --> 00:11:08,040
then the log likelihood is simply that component right there proportional to other stuff that we get rid of.

102
00:11:09,480 --> 00:11:15,030
And we estimate the beta parameters by taking the derivative of the log likelihood, right.

103
00:11:15,030 --> 00:11:18,300
Setting it equal to zero and it comes up really nice here.

104
00:11:19,950 --> 00:11:24,120
We get what we call a score equation or a score function for the aggression parameters.

105
00:11:24,390 --> 00:11:32,520
And in more general terms, we call that an estimating equation. That is an equation there in which we can solve for the parameters and theta.

106
00:11:33,240 --> 00:11:39,660
And this is where a generalized estimating equations comes from because you can think of them as an estimating equation approach.

107
00:11:42,360 --> 00:11:46,830
So Sigma Squared came out of the sum here because everybody had the same variance,

108
00:11:47,040 --> 00:11:52,499
constant variance variance did not change with I was individual right and so we could pull it out.

109
00:11:52,500 --> 00:11:57,880
But if we wanted to have non constant variance so everybody has their own variance that just

110
00:11:57,900 --> 00:12:02,910
comes back in the sum and we have again that's a weight one over sigma squared is a weight.

111
00:12:03,360 --> 00:12:06,960
We have weight of these squares here and the same thing happens here.

112
00:12:07,440 --> 00:12:14,590
But that equation right there. I could use for anything that I really wanted to try and fit.

113
00:12:15,160 --> 00:12:23,140
So to estimate the regression parameters beta, all I need is a model for the mean and a model for the variance.

114
00:12:24,160 --> 00:12:26,770
I didn't need to say anything more than that to get that.

115
00:12:26,800 --> 00:12:31,900
If I start with that equation right there, that equation doesn't have to be the derivative of a likelihood.

116
00:12:31,930 --> 00:12:36,010
It could be, but it doesn't have to be. It's just an estimating equation.

117
00:12:37,690 --> 00:12:46,270
And this is the concept of quasi likelihood that I hope you saw in 651 plays a role in geoscience and of course also energy.

118
00:12:46,720 --> 00:12:54,220
So again, we can call that a quasi score. It comes from a quasi likelihood, something that looks like a likelihood but isn't.

119
00:12:54,220 --> 00:13:01,810
And therefore we can use likelihood theory to talk about the asymptotic distribution or the distribution of the regression parameters.

120
00:13:02,800 --> 00:13:06,430
So for example, let me change the variance model.

121
00:13:06,430 --> 00:13:12,190
So the mean model still says that the mean a z times beta linear combination of covariates, age, gender,

122
00:13:12,910 --> 00:13:19,480
smoking, etc. but the variance is not constant for each individual is proportional to their mean.

123
00:13:20,880 --> 00:13:25,860
So someone with a larger mean has larger variance, right in picture that sort of funnel funnel shape.

124
00:13:26,700 --> 00:13:29,720
And so the estimating equation is just what we had before.

125
00:13:30,180 --> 00:13:34,530
It's the difference between each observation and it's mean divided by the variance.

126
00:13:35,580 --> 00:13:40,800
And there's a derivative there, a derivative of the mean with respect to data, which is always the design matrix.

127
00:13:41,400 --> 00:13:46,830
And we get that estimate equation right there. And there is a way to solve that equation for beta.

128
00:13:47,700 --> 00:13:51,720
I didn't say anything about normality. All I said was that was the mean and that was the variance.

129
00:13:54,150 --> 00:13:58,260
If something is proportional to the effects, if a random variable is something,

130
00:13:58,260 --> 00:14:04,590
if random variable has its variance proportional to its mean, that is something Poisson like.

131
00:14:06,240 --> 00:14:10,240
So it looks like it was on model. Again.

132
00:14:10,240 --> 00:14:18,229
What seems square is one, but it's not quite. Because the mean is exhibit A and the mean can go from negative infinity to infinity.

133
00:14:18,230 --> 00:14:25,280
That linear predictor goes from negative infinity to infinity. So we can't use that word, that estimate equation with Y's that are Poisson.

134
00:14:26,300 --> 00:14:31,010
Because the regression parameters might lead to a mean that makes no sense with baseline data to be negative.

135
00:14:32,030 --> 00:14:37,340
So instead of modeling the mean, we're going to model a function of the mean.

136
00:14:37,390 --> 00:14:40,880
And that's where the idea of a link function came into place in 651.

137
00:14:41,780 --> 00:14:46,940
So if we want a mean that has to be strictly positive, we pick a log link.

138
00:14:47,510 --> 00:14:55,160
And with binomial data, there's even more possibilities. So again, Poisson data, the variance is the mean.

139
00:14:56,390 --> 00:15:00,680
But if the variance is larger than just the mean, that is proportional to the mean.

140
00:15:00,980 --> 00:15:06,470
Then you talked about over dispersion when that factor was bigger than one over dispersion.

141
00:15:07,610 --> 00:15:15,440
651 Okay. So this is exactly what generalized linear models were doing.

142
00:15:15,620 --> 00:15:19,880
We take a model. We model a function of the mean. Again, for normal data.

143
00:15:20,450 --> 00:15:27,650
The identity is that function and a linear combination of covariates, and we talk about the variance being proportional to some function of the mean.

144
00:15:28,370 --> 00:15:31,810
And if you have normal data, you've got a link that is the identity.

145
00:15:31,820 --> 00:15:35,750
Like I just said, GMU is the meta is the mean for Poisson.

146
00:15:35,750 --> 00:15:40,729
We typically use a log link. That's the length that makes everything from the positive realign.

147
00:15:40,730 --> 00:15:44,410
Go back to the go to the non-negative support.

148
00:15:45,800 --> 00:15:54,140
And for binomial, we have three possibilities. The most common is the logit link, which is the log of the ads.

149
00:15:54,680 --> 00:16:00,800
Log of again new is usually p. There's usually probability, but it's the log of the mean over one minus the mean.

150
00:16:01,670 --> 00:16:04,190
There are again, we're just trying to come up with a function.

151
00:16:04,190 --> 00:16:11,570
You could use any function you want that takes muli which goes from 0 to 1 for binomial data and was it to the real line.

152
00:16:12,380 --> 00:16:17,390
So there's the log of that ratio. There is the inverse CDF of the normal distribution.

153
00:16:18,500 --> 00:16:23,479
Right? So if A is a is a real number in the normal distribution,

154
00:16:23,480 --> 00:16:29,000
there's a cumulative probability that corresponds to any real number and the number goes from 0 to 1 because it's a cumulative probability.

155
00:16:29,870 --> 00:16:35,580
We call that a private function. I don't know where Probit came from, but that's what it is.

156
00:16:35,600 --> 00:16:38,570
And then there is another one called the complementary log lock.

157
00:16:39,680 --> 00:16:45,950
Again, I can't believe anybody picked that other than it goes from zero one to the real line.

158
00:16:46,940 --> 00:16:51,440
I don't know why anybody would want the log of the negative log or something, but it does.

159
00:16:51,500 --> 00:17:02,959
It does work. It is a possible link function. Why do we use logit links when we talk about logistic regression nine times out of ten instead?

160
00:17:02,960 --> 00:17:08,610
Sort of probit regression. Anything special?

161
00:17:08,620 --> 00:17:12,160
A theologian. Has a nice interpretation.

162
00:17:12,370 --> 00:17:16,390
It has a nice interpretation. What is that interpretation? That's the log ons.

163
00:17:16,630 --> 00:17:20,820
Okay. Is that a nice interpretation? No.

164
00:17:23,020 --> 00:17:27,040
When I work with individuals, they don't understand what an odds ratio is.

165
00:17:27,610 --> 00:17:31,520
Your odds ratios to. Right.

166
00:17:33,170 --> 00:17:36,800
If I say your risk ratio is to oh, I'm twice as likely.

167
00:17:37,990 --> 00:17:41,290
I have twice the odds. Is that. Is that a lot?

168
00:17:42,550 --> 00:17:46,380
Um, so it has an interpretation. I don't know.

169
00:17:46,390 --> 00:17:52,600
That's a nice one. Um, I don't really like to report odds ratios for a lot of individuals.

170
00:17:52,600 --> 00:17:57,760
And in logistic regression, again, if I report an odds ratio of 1.3.

171
00:17:59,000 --> 00:18:10,670
That's 30% more ads. But everybody I work with, oh, 30% more likely know now, 30% more probability, 30% more ads.

172
00:18:11,210 --> 00:18:15,720
And again, you've learned in Europe classes, if PE is small, right,

173
00:18:15,720 --> 00:18:20,930
then the odds and the risk ratios are the same thing as ratios using inverse ratios.

174
00:18:21,290 --> 00:18:27,049
But so the logic does have I mean, it does have a algo.

175
00:18:27,050 --> 00:18:30,110
They have a nice, nice interpretation, if you like, odds ratios.

176
00:18:30,910 --> 00:18:36,830
Any other reason we would use the logit? He's very read my notes.

177
00:18:37,460 --> 00:18:41,150
Yes, it's the canonical. It's the canonical link. Right.

178
00:18:41,310 --> 00:18:45,020
And log is the canonical and just happens to be the canonical links for poisson.

179
00:18:45,020 --> 00:18:48,950
But it also has a nice interpretation. Nice. You can exponentially.

180
00:18:48,980 --> 00:18:53,930
Again, most people who don't like math don't like E. The heck is E, right?

181
00:18:55,970 --> 00:19:02,930
It has some nice computational benefits to us. So we use logistic regression a lot.

182
00:19:04,080 --> 00:19:11,120
I think for no reason other than the math is nice. Not because anybody is interested in an odds ratio, but anyway,

183
00:19:11,900 --> 00:19:18,960
it does have an interpretation if you can get your head around what an odds ratio is for these three families or distributions.

184
00:19:18,980 --> 00:19:24,410
There is also this fee parameter, which is the variability beyond what's explained.

185
00:19:24,680 --> 00:19:28,909
So that sigma squared and normal data was Sun Bernoulli data.

186
00:19:28,910 --> 00:19:35,900
We assumed that parameter is one. We don't estimate it. It is set to one unless we want over dispersion.

187
00:19:37,940 --> 00:19:40,680
And I'm also going to define oops.

188
00:19:40,700 --> 00:19:48,290
And then so the variance function again is one for normal data because it's one time sigma squared for Poisson data.

189
00:19:48,290 --> 00:19:58,070
The variance is the is the mean. And again for binary data, the variance is p times one minus p new times one minus, right?

190
00:19:59,180 --> 00:20:04,100
So we have a variance function, we have a mean function. Those are the two things we need to do.

191
00:20:04,100 --> 00:20:11,240
Quasi likelihoods, use an estimating equation. And so let us define the inverse of the link function again.

192
00:20:11,750 --> 00:20:18,380
I remember taking James and I remember there was a link function invariance function managed inverse linked function.

193
00:20:19,880 --> 00:20:23,030
It gets a little bit much, but all of this helps with the computation.

194
00:20:23,030 --> 00:20:26,660
Again, if you think of the inverse linked function as being the mean.

195
00:20:27,880 --> 00:20:29,650
Then you return to the estimating equation.

196
00:20:29,650 --> 00:20:39,640
This was the estimating equation I just showed a few slides ago and you just took in F of x I for you and you put in the variance function.

197
00:20:39,910 --> 00:20:43,780
So again, I tried to highlight the blue with the blue and the orange with the orange.

198
00:20:44,650 --> 00:20:48,940
And so we've now generalized our estimating equation. Again, this is not correlated data.

199
00:20:48,940 --> 00:20:54,459
This is one observation per person, this is glimpse. And we generalize it to this right here.

200
00:20:54,460 --> 00:20:59,890
Again, we're trying to estimate the beta parameters that make this equation zero.

201
00:21:00,550 --> 00:21:05,950
Right. This isn't a maximum likelihood estimate. It is an estimate from this equation.

202
00:21:08,050 --> 00:21:16,240
Now, as was just said, we know that the derivative using the change earlier of the mean written as the inverse of the length,

203
00:21:17,140 --> 00:21:22,600
is just the derivative of that function, Times XIV. And why do I care about that?

204
00:21:23,470 --> 00:21:30,040
Because for certain length functions, it turns out that the variance is equal to the derivative of the inverse function.

205
00:21:30,640 --> 00:21:34,270
And this is the canonical. This is where canonical comes from.

206
00:21:35,080 --> 00:21:40,240
So the log link is canonical for Poisson and the logic link is canonical for binomial data.

207
00:21:40,630 --> 00:21:48,510
Why do I care about a canonical link? For for no reason that I know of, other than it makes the estimating equation easier.

208
00:21:49,140 --> 00:21:52,410
Because if I go back to my original estimating equation.

209
00:21:53,400 --> 00:21:59,290
Right. This thing right here is the various function times X, right?

210
00:21:59,310 --> 00:22:02,520
There's the derivative. Times X, the derivative is the variance function.

211
00:22:02,970 --> 00:22:12,090
There's a variance function in the denominator here. So those two things cancel out and you have something that looks like this, which looks very,

212
00:22:12,090 --> 00:22:20,310
very much like ordinarily squares each observation minus its mean times, the design matrix equals zero.

213
00:22:20,550 --> 00:22:24,540
That's what we had before. So this is what we do with glance.

214
00:22:24,690 --> 00:22:31,500
We have something that looks very similar. So what we do is ordinarily squares, but right.

215
00:22:31,530 --> 00:22:35,570
Those parameters beat ahead, don't have a close firm solution.

216
00:22:35,580 --> 00:22:41,490
I can't just write a formula for beta hat equals write some sort of combination of x and wise.

217
00:22:42,420 --> 00:22:50,760
And this is what I asked last week. The way to solve for the beta parameters in groups is through iterated and weighted squares.

218
00:22:50,790 --> 00:23:00,030
Again, there's a weighted least squares component here. There's a weight at least squares flavor here, and we have to iterate when we do this.

219
00:23:00,480 --> 00:23:08,790
So we select starting values for the weights. The variance variance is different if the variance is a function of the mean.

220
00:23:10,110 --> 00:23:13,980
Then we have non-cancer variants where every person has a different mean, they have a different variance.

221
00:23:14,430 --> 00:23:19,350
So we have to pick the weights. We put those weights into the estimating equation to get better.

222
00:23:19,420 --> 00:23:23,040
Let's go back to data to get the weights to go back and forth.

223
00:23:23,430 --> 00:23:28,770
And it converges quite quickly for most datasets. If you have enough observations.

224
00:23:29,430 --> 00:23:37,820
All right, that's iterated weight of these scores. And this is all totally review, right?

225
00:23:39,290 --> 00:23:48,530
I'm like wait at least squares. The resulting estimate or beat ahead from iteratively squares is not a linear combination of normal random variables.

226
00:23:49,940 --> 00:23:54,590
And so, again, we can't just say that the parameter estimate, therefore, is normal.

227
00:23:54,900 --> 00:23:58,180
The linear combination is normal while their combination of normals is normal.

228
00:23:58,670 --> 00:24:02,540
Here we don't have that. So it is consistent.

229
00:24:02,540 --> 00:24:07,790
It's not necessarily in biased. The bias goes away as you get larger, more and more data.

230
00:24:07,970 --> 00:24:13,550
So it is consistent. Again, that's not a trivial proof, but it is true.

231
00:24:14,150 --> 00:24:23,390
And it can also be shown that beta has a limiting normal distribution with mean beta and it has a variance that looks like that thing right there.

232
00:24:24,560 --> 00:24:31,610
And if you have normal data, this is simply sigma squared inverse times x transpose x inverts the usual formula.

233
00:24:31,610 --> 00:24:40,070
Right, because the variance function is one. The Sigma squared and the derivative IMU with respect to beta is at the ex matrix design matrix.

234
00:24:40,430 --> 00:24:46,010
So you get X, transpose x and sometimes sigma squared. So this is again just the generalization here.

235
00:24:51,470 --> 00:24:58,430
So again, the variance of beta hence is a function of fee.

236
00:24:58,430 --> 00:25:02,620
We don't know what fee is, we don't know what the betas of the beta here.

237
00:25:02,710 --> 00:25:07,460
We don't have beta in that formula. So we're going to put in beta that we're going to estimate fee.

238
00:25:08,120 --> 00:25:11,810
I've already shown you how to estimate beta as iteratively squares.

239
00:25:12,860 --> 00:25:17,090
We use fee from the residuals. We get the last over variability from the residuals.

240
00:25:18,260 --> 00:25:21,649
However, for percent and binomial models, we generally don't estimate it.

241
00:25:21,650 --> 00:25:24,770
We fix it to be one unless we want to account for over dispersion.

242
00:25:25,990 --> 00:25:30,670
Again, if you have binary data, there is no such thing as over dispersed binary data.

243
00:25:31,670 --> 00:25:33,470
You either are a zero or a one.

244
00:25:34,480 --> 00:25:41,940
You can't have more dispersion than that, you know, in a binomial model, but you can have more than the typical Poisson variation with count data.

245
00:25:42,660 --> 00:25:50,069
If you want to estimate, deal with some data that I said, you're going to again put that into the formula and account for over dispersion.

246
00:25:50,070 --> 00:25:53,730
So for normal outcomes, that C parameter is sigma squared, right?

247
00:25:54,900 --> 00:25:58,500
The difference between the fitted values and the observed values squared divided by and

248
00:25:58,500 --> 00:26:02,400
minus p and is the number of people P is the number of parameters in the regression model.

249
00:26:03,090 --> 00:26:08,190
And for Pearson outcomes it looks very similar, except you have to scale by the fit it means.

250
00:26:09,690 --> 00:26:13,049
And I assume that, you know, you looked at the deviance,

251
00:26:13,050 --> 00:26:19,530
we're concerned about deviance and those sorts of concepts from Glimpse, but it's all part of deviance.

252
00:26:19,530 --> 00:26:23,549
Again, deviance is just a difference between observed and fitted, but there it is.

253
00:26:23,550 --> 00:26:30,480
This is all done in general as you're doing the homework and I edited these slides.

254
00:26:30,720 --> 00:26:31,540
This is what I was talking about.

255
00:26:31,560 --> 00:26:40,709
I changed a few things and out of these slides it's really important as a practicing statistician to never give the regression coefficients

256
00:26:40,710 --> 00:26:49,140
from Poisson regression and binomial regression because we don't interpret and want you to change on a log scale or a log odds scale.

257
00:26:50,340 --> 00:26:54,360
We try to get back to the original for an odds ratio or something or a risk ratio.

258
00:26:54,750 --> 00:27:02,610
So with a log, like if you have cosine outcomes, each parameter quantifies the additive change in the log of the mean with one unit change.

259
00:27:02,610 --> 00:27:08,970
Right. That is not very helpful. People do not know what a log log of 2.3.

260
00:27:09,870 --> 00:27:15,209
What is that. I don't even know. And so natural log write natural logs.

261
00:27:15,210 --> 00:27:19,020
Oh my god, that's too busy. What is easy.

262
00:27:20,460 --> 00:27:25,170
So we exponential eight we take a parameter and say e to that parameter.

263
00:27:26,040 --> 00:27:31,530
And so that describes a multiplicative change in the mean with one unit change in the covariance.

264
00:27:31,950 --> 00:27:36,450
So again, if I get a coefficient estimate for Poisson regression of zero point -4.75,

265
00:27:37,230 --> 00:27:45,900
then a one in a change in a covariate leads to a 47% reduction in the mean because e to that regression parameter is pointing to force of two,

266
00:27:46,740 --> 00:27:52,910
so don't report -0.75. Report e to that number and tell me what it is.

267
00:27:53,270 --> 00:28:00,810
It's a multiplicative adjustment to the mean. It's an additive with a logic linked logics.

268
00:28:00,830 --> 00:28:04,129
Ah yeah. Those are hard. Those are not nice functions.

269
00:28:04,130 --> 00:28:11,750
But each parameter quantifies an additive change in the logit, which is the log odds with a one unit change in the covariate.

270
00:28:11,780 --> 00:28:17,570
So again, we exponential parameters to get to an odds ratio instead of a log odds ratio.

271
00:28:19,210 --> 00:28:22,990
So we exponentials parameter describe the multiplicative change in the odds.

272
00:28:23,950 --> 00:28:26,620
Odds ratio one odds versus the other.

273
00:28:27,040 --> 00:28:36,580
So if the coefficient estimate is .34, then a one year change in the covariance leads to a 40% increase in odds because eight of that number is 1.4.

274
00:28:38,900 --> 00:28:46,340
So it isn't 40% greater risk. It's 40% greater odds, which again, is close.

275
00:28:46,400 --> 00:28:51,830
If we have a small incidence of whatever we're trying tomorrow. But I try to be really picky.

276
00:28:53,150 --> 00:28:56,950
How many of you are taking a survival analysis semester?

277
00:28:57,710 --> 00:29:02,210
So friend this semester? Next semester? Who plans on taking it next semester?

278
00:29:02,240 --> 00:29:07,790
Just curious because there you're going to learn a hazard ratio and if you want to watch people's heads explode,

279
00:29:08,270 --> 00:29:14,330
let's try to explain them what a hazard ratio is. Again, they want to interpret it as an increase in risk.

280
00:29:15,170 --> 00:29:19,340
It's an increase in the hazard. What is a hazard? All right.

281
00:29:19,940 --> 00:29:24,950
So it's our job. I was just saying to my wife, I am not training you to be computers.

282
00:29:25,370 --> 00:29:28,910
I'm training you to be communicators. We got lots of computers out there.

283
00:29:29,600 --> 00:29:35,420
We don't have a lot of good communicators, but good communication goes a long way in our field.

284
00:29:35,840 --> 00:29:40,040
So work on doing that. So you have fit a model.

285
00:29:41,630 --> 00:29:45,830
And you get coefficient estimates, right? Standard errors. Everything really nice.

286
00:29:46,280 --> 00:29:51,830
You get the standard error of your coefficient estimate. How do you get the standard error of the log of the estimate?

287
00:29:52,340 --> 00:29:59,780
Assertions say log of your speed of the estimate. And this.

288
00:29:59,780 --> 00:30:03,880
And I thought I was being so smart and I got a backward step.

289
00:30:04,250 --> 00:30:10,950
Don't. Word.

290
00:30:15,500 --> 00:30:19,870
So you did some regression and you got to the point for two.

291
00:30:20,710 --> 00:30:25,480
I'm sorry beta at this point 42 and I just told you I don't want to know what that is.

292
00:30:26,410 --> 00:30:32,970
And it has a standard error. Right.

293
00:30:34,040 --> 00:30:39,570
So I told you to report it at the beginning. And so that speed of I should have picked tennis ahead of time is some number.

294
00:30:41,360 --> 00:30:45,290
What's the standard error of that thing? What's the standard error of the meter ahead?

295
00:30:49,400 --> 00:30:58,220
How did you get that done? Nelson I think one of the best things you learned in 601, right, this is where we teach is 601.

296
00:30:59,090 --> 00:31:02,630
So if you have the standard error of the thing you got is the Delta method, right?

297
00:31:03,050 --> 00:31:09,480
So you got the variance. A bit of at times the derivative was.

298
00:31:10,600 --> 00:31:14,830
I mean, is there another need in the data? So Delta method.

299
00:31:16,860 --> 00:31:21,440
Art doesn't do that for you unless someone has someone from your library. There's probably a library out there.

300
00:31:22,830 --> 00:31:26,040
But what does it say? What does it say?

301
00:31:30,480 --> 00:31:35,640
The data from the data. The values in the data.

302
00:31:36,930 --> 00:31:41,370
So it's the variance of data at times of the data.

303
00:31:44,730 --> 00:31:53,690
Thank you. Yes. Much better. You can do that an hour.

304
00:31:53,720 --> 00:31:59,040
Just keep this stuff in mind. And then how do you compute a 95% confidence interval for the rate ratio?

305
00:31:59,900 --> 00:32:02,120
We you put that back up first. I sure can.

306
00:32:02,780 --> 00:32:10,260
I was going to go back to it with this next question, but I understand there should be some benefit of being here, right?

307
00:32:11,130 --> 00:32:15,350
It is. Is it the standard error or is it. Oh, that's the variance.

308
00:32:15,380 --> 00:32:20,260
Thank you. That's the question. Yes.

309
00:32:20,560 --> 00:32:30,370
I'm really good spirits. I just said I'm just going that way.

310
00:32:31,090 --> 00:32:35,460
There we go. This is the variance. Is that the question?

311
00:32:36,870 --> 00:32:43,530
The variance of that. And so the standard error of data hands is the standard error.

312
00:32:46,070 --> 00:32:51,080
I hear a bit of at times either being hit.

313
00:32:52,110 --> 00:32:56,220
Or two square root. I could be wrong.

314
00:32:56,640 --> 00:32:59,940
Is it the derivative squared? I'm not remembering this correctly. You are.

315
00:33:00,270 --> 00:33:04,050
Thank you very much. I knew there wasn't a square root here.

316
00:33:04,170 --> 00:33:09,660
That's why. And thank you. So, justice squared.

317
00:33:10,590 --> 00:33:16,440
Some area. So the variance of each of the data set is the variance times.

318
00:33:16,440 --> 00:33:22,630
The derivative squared. That's the delta method. And then check the square root of that to get the standard error.

319
00:33:22,650 --> 00:33:29,970
So much this. So is the standard error proportional to the risk ratio itself for the rate ratio.

320
00:33:35,760 --> 00:33:41,480
Now I'm going to close it or at least minimize it. All right.

321
00:33:44,330 --> 00:33:48,770
And then how do you computer 95% confidence interval. So you've computed each to the ahead for me.

322
00:33:49,760 --> 00:34:08,230
How do you give me a 95% confidence interval for that? Do either the beta plus or minus.

323
00:34:10,430 --> 00:34:15,560
I did it. And it's the right thing to.

324
00:34:17,220 --> 00:34:30,070
So again, once again, this was my third. 95% confidence interval.

325
00:34:31,140 --> 00:34:34,650
For each of the rate of rate for the risk ratio, stick with my son.

326
00:34:35,340 --> 00:34:44,970
And so we could say that is either the better at plus or minus to standard errors of eight of the better helps.

327
00:34:47,410 --> 00:34:57,880
And that doesn't look like an experiment. Does everybody think of that as an all in exponent?

328
00:34:57,900 --> 00:35:03,970
Right. But set on the exponent. Well in the exponent even.

329
00:35:09,250 --> 00:35:21,819
So let me read this first. I want to be explicit. People think you can do something plus or minus two senators?

330
00:35:21,820 --> 00:35:25,030
That's what I'm reading right now. Is that what you meant? Somebody said that?

331
00:35:25,030 --> 00:35:28,300
No. So what's what's wrong with what's wrong with that?

332
00:35:28,630 --> 00:35:31,920
What are the limitations of doing this? Right.

333
00:35:31,930 --> 00:35:36,850
This is a normal 95% confidence interval. Right. Estimate plus or minus two senators.

334
00:35:39,650 --> 00:35:45,680
What problems could arise from this confidence interval. You might end up with a negative man with a negative value, right?

335
00:35:45,680 --> 00:35:49,550
So we don't typically take that plus or minus two standard errors.

336
00:35:51,930 --> 00:36:00,820
So I think in the. We do this gross exponential of data at plus or minus to standard errors that beat ahead.

337
00:36:01,790 --> 00:36:09,820
I think that's what someone was saying. Well putting it all in the exponent so a computer 95% confidence intervals for the parameter itself.

338
00:36:10,970 --> 00:36:18,560
And then do the transformation that will get you to stay on the right scale and it has better coverage.

339
00:36:19,640 --> 00:36:22,670
We want 95% coverage of the true values.

340
00:36:23,780 --> 00:36:28,670
So I just want to reinforce this. This is the kind of stuff you should be doing when you fit these deals.

341
00:36:29,940 --> 00:36:35,030
You can report risk ratios, you can find the standard error of the ratio.

342
00:36:35,040 --> 00:36:40,140
You can put it at 95% confidence interval. You don't need a library to do this.

343
00:36:41,160 --> 00:36:51,690
All this information comes from from the model itself. So I just wanted to emphasize that as I was doing the homework assignment and.

344
00:36:53,510 --> 00:37:01,969
I think there was a lot more stuff that was that other things about games that I just said or

345
00:37:01,970 --> 00:37:10,280
you remember from 651 kind of scratching your head or you got to know about league functions.

346
00:37:10,280 --> 00:37:15,820
You got to know about variance functions. Interpretation of coefficient estimates.

347
00:37:18,640 --> 00:37:22,490
Yeah. Again model comparison.

348
00:37:22,820 --> 00:37:26,750
I didn't go through that here in Geelong, but, you know, model comparison was the same as it was before.

349
00:37:27,350 --> 00:37:36,010
You compare it to nascent models. I'm looking at a test, usually a change in deviance, just like a like literature test and so forth.

350
00:37:36,030 --> 00:37:39,680
So those things are the same. All right.

351
00:37:40,940 --> 00:37:45,980
So that was my whirlwind tour of an entire semester of your life, right?

352
00:37:50,530 --> 00:37:56,620
Now let's move on. So now that we all are comfortable again with gloves.

353
00:38:00,700 --> 00:38:03,130
Let's talk about what we're going to do now applied to the letter.

354
00:38:03,910 --> 00:38:11,080
And again, the notation here is trying to get into Matrix to look like what we were doing before with gels.

355
00:38:11,350 --> 00:38:16,210
So with one observation per individual, again, we haven't gotten the correlated data yet.

356
00:38:17,380 --> 00:38:21,610
There's the estimating function I showed you just a few slides ago in the previous set of slides.

357
00:38:22,840 --> 00:38:26,560
Some function of the mean is a linear combination of covariance,

358
00:38:27,040 --> 00:38:33,970
and we have to wait each observation by the inverse of its variance, which also changes with the mean.

359
00:38:34,890 --> 00:38:40,480
Right. So betas in both pieces there, the transpose part and the denominator.

360
00:38:40,960 --> 00:38:48,760
So in preparation for multiple observations for individuals, I'm going to rewrite that equation up there in this form right here.

361
00:38:49,150 --> 00:38:53,710
And so we've got Delta I and I wish I had called that Sigma inverse.

362
00:38:57,560 --> 00:39:03,120
Presenter is that? No capital.

363
00:39:03,120 --> 00:39:07,350
The capital in the Delta II lambda i inverse times.

364
00:39:07,560 --> 00:39:13,190
The difference between the mean and the observed values. So again, we have a vector of derivatives.

365
00:39:13,200 --> 00:39:18,600
Delta stands for derivatives. Right. So again, each new I.

366
00:39:19,880 --> 00:39:22,880
There's a bunch of regression parameters in it.

367
00:39:23,300 --> 00:39:27,200
So again, Demir, why debate or not is what?

368
00:39:31,460 --> 00:39:41,870
If you look at the regression. It's related to the intercept rate data, not as multiplied times the intercept.

369
00:39:42,380 --> 00:39:50,210
So it's related to the first column of the design matrix. Again, it's MMU isn't x data anymore, it's g of new is x beta.

370
00:39:51,110 --> 00:39:54,520
But again, that first component is related to the intercept.

371
00:39:54,530 --> 00:39:59,810
The second component is related to the first covariate. The next one is related to the next covariate and so forth.

372
00:40:01,850 --> 00:40:13,880
That's again, if if we have an identity, if MU is exactly x beta, then this is one and this is the first covariate and the second cover and so forth.

373
00:40:14,450 --> 00:40:23,090
But again, how are you doing, James? And again, in Lambda, I is is dealing with the variance.

374
00:40:23,120 --> 00:40:24,770
Again, we're going to have a weight.

375
00:40:25,370 --> 00:40:31,820
Every person has a weight applied to the difference between what we see and what the model says it should be on average.

376
00:40:32,420 --> 00:40:41,720
And that weight is the inverse of the variance. She has, again, a constant fee, usually one times the variance function.

377
00:40:44,060 --> 00:40:49,880
So with that matrix sort of representation, now we're going to start bold facing things.

378
00:40:50,150 --> 00:40:53,930
So we're going to assume that each person has a set of observations.

379
00:40:53,940 --> 00:41:06,049
Am I? It could be different for every person such that that vector of outcomes y has a vector of means that again is set of regression parameters,

380
00:41:06,050 --> 00:41:16,340
times the design matrix. If there's one row of x per person, there's one row of access for every outcome over time, usually time in this class.

381
00:41:17,420 --> 00:41:22,640
The variance of someone's set of observations is not a scalar.

382
00:41:22,760 --> 00:41:29,930
It is a matrix because our observations in the same person may be correlated.

383
00:41:31,310 --> 00:41:40,990
And so the variance of y this matrix, which again is lambda i boldfaced, we're going to split it apart into the two pieces.

384
00:41:41,000 --> 00:41:47,299
There's a variance component and a correlation component. So again, the speed parameter is always floating around.

385
00:41:47,300 --> 00:41:51,170
It's usually one unless we're talking about normal data, which is sigma squared.

386
00:41:52,160 --> 00:41:59,030
There's this very inverse limit and I t is via inverse.

387
00:42:00,270 --> 00:42:05,280
What am I trying to do here? I'm trying to get a variance for each person.

388
00:42:05,490 --> 00:42:09,240
And I do that by taking the square root and having these two matrices in either site.

389
00:42:09,570 --> 00:42:13,950
So when I do the matrix multiplication, I end up with a V of new Y for each person,

390
00:42:15,600 --> 00:42:18,750
and then there's a correlation matrix, which is the part in the middle.

391
00:42:20,280 --> 00:42:25,830
So. This looks like a sandwich simulator type thing, but it's not.

392
00:42:25,840 --> 00:42:30,810
It looks like a sandwich type thing. But again, don't get confused. That's not where the sandwich this is going to come in yet.

393
00:42:33,720 --> 00:42:42,000
This correlation matrix are just like the correlation matrices we used in GLS, so everything's a little bit differently.

394
00:42:42,870 --> 00:42:48,180
But I'm going to assume a model. There's going to be a model that says the variance covariance matrix of

395
00:42:48,210 --> 00:42:52,950
someone's measurements is related to the variance and the correlation structure.

396
00:42:54,450 --> 00:42:59,730
What kind of correlation structures are we going to use? The same ones we used in GLC.

397
00:43:00,870 --> 00:43:06,870
Now in G nomenclature, people call this correlation structure a working correlation.

398
00:43:07,670 --> 00:43:13,860
I don't know where that came from, why it couldn't be modeled, but it's the working correlation.

399
00:43:14,160 --> 00:43:20,040
So when you fit G, you have to tell the folks, just like you do with Gladys.

400
00:43:20,040 --> 00:43:23,760
What correlation structure did you use in your estimation?

401
00:43:24,330 --> 00:43:30,610
This is going to be part of the estimation. This thing right here was the thing that there is going to be part of the estimation.

402
00:43:31,360 --> 00:43:34,840
We could have independence. This could be an identity matrix.

403
00:43:36,760 --> 00:43:41,739
If I if that thing is an identity matrix and I take this times and ideally

404
00:43:41,740 --> 00:43:46,540
times that I essentially get this squared and this squared is just a diagonal,

405
00:43:46,540 --> 00:43:52,839
a variances. That's what independence is. I could use exchangeable correlation.

406
00:43:52,840 --> 00:43:55,570
We all know what that is by now. Or I could use another one.

407
00:43:57,140 --> 00:44:03,530
You can use any working correlation structure you want there's TOEPLITZ There's you know that weird exponential one all different kinds of

408
00:44:03,530 --> 00:44:12,049
decay you can fit in unstructured correlation matrix so that every observation has a different correlation with the other observations.

409
00:44:12,050 --> 00:44:18,590
Possibly, again, that that gets problematic as the number of observations is a number of people excuse me,

410
00:44:18,590 --> 00:44:24,260
a small of the number of observations is big. So we typically don't fit a.

411
00:44:25,590 --> 00:44:37,120
Correlation structure like that. So again, we have some simply specified a vector of means for the observations.

412
00:44:38,130 --> 00:44:41,850
And the correlation variance covariance structure of the observations.

413
00:44:42,270 --> 00:44:51,070
I haven't said anything about distributions. I got the first two moments and the correlation of the observations.

414
00:44:51,820 --> 00:44:54,880
All of this leads to a generalized estimating equation.

415
00:44:55,750 --> 00:45:02,380
This is the same estimating equation I showed you before for one observation, but now we have multiple observations per person.

416
00:45:02,950 --> 00:45:06,520
So we take how different someone's observations are from the mean.

417
00:45:07,510 --> 00:45:10,510
Let's see, we multiply that by a weight matrix.

418
00:45:11,590 --> 00:45:16,270
And there's also that derivative component, which was X when we had normal data.

419
00:45:16,810 --> 00:45:21,220
Now it's a little more complex and all of that again, each person.

420
00:45:22,960 --> 00:45:27,730
It has matrices here, and then we add them all up because people are independent of each other.

421
00:45:29,200 --> 00:45:33,840
And as I just said, none of this came from a normal likelihood or any kind of likelihood.

422
00:45:34,420 --> 00:45:38,020
I just specified the mean and the variance governance structure.

423
00:45:38,590 --> 00:45:42,250
That was all I did. And I said, Here's an estimating equation we could use.

424
00:45:43,750 --> 00:45:52,300
Is it a good equation? I don't know. We're going to find out. But I can find estimates for beta, the regression parameters in the mean model,

425
00:45:53,170 --> 00:45:57,430
and I can also find an estimate for the correlation parameter in the weak matrix.

426
00:45:58,840 --> 00:46:02,950
This model does not come from a joint distribution of all the elements of Y.

427
00:46:02,980 --> 00:46:09,280
I have not expressed to the entire multivariate distribution of why I have simply said

428
00:46:09,280 --> 00:46:15,190
what the marginal means with the marginal variances and possibly some correlation.

429
00:46:15,820 --> 00:46:20,800
So we call G a marginal model. We're modeling the marginal mean of each individual.

430
00:46:21,940 --> 00:46:29,920
And that's going to be different from the subject specific, conditional kind of approach that we have already random effects, right?

431
00:46:30,100 --> 00:46:38,440
So you might hear someone saying a marginal model interpretation, and that's what G.E. is, because we are not specifying a joint distribution.

432
00:46:38,950 --> 00:46:42,310
We are specifying the marginal distributions of each observation.

433
00:46:43,480 --> 00:46:50,230
More specifically, if we further assume normality each Y is marginally normal,

434
00:46:51,340 --> 00:46:56,560
then we can show that the collection of wise have a multivariate normal distribution.

435
00:46:57,220 --> 00:47:05,230
This is why we like normal distributions. A multivariate normal distribution produces marginal normals, right?

436
00:47:05,500 --> 00:47:09,550
And so estimation comes directly from that normal likelihood.

437
00:47:09,760 --> 00:47:12,910
So beta looks like that looking in now we're using the equation,

438
00:47:13,180 --> 00:47:18,940
the notation that's in G, but beta hat looks like that and that's the variance formula.

439
00:47:19,360 --> 00:47:22,930
And again, what's normal data? This is X.

440
00:47:24,980 --> 00:47:29,390
The derivatives. This ends up being that x transpose sigma inverse.

441
00:47:29,690 --> 00:47:37,070
Just what you saw from glass. This is the glass solution to the hat that you saw a couple of weeks ago.

442
00:47:37,460 --> 00:47:45,050
And this is the model based variance formula x transpose sigma inverse x transpose all inverse.

443
00:47:46,610 --> 00:47:49,760
So you can fit each a normal data.

444
00:47:52,210 --> 00:47:58,440
But if you do, you are doing glass. And again, I welcome you to try that out.

445
00:47:58,560 --> 00:48:02,190
You're going to learn how to use the G function and are fed g to homework.

446
00:48:02,190 --> 00:48:05,500
Number three. Number two, wherever you get.

447
00:48:05,590 --> 00:48:10,350
Yeah. Number two. And then feta cheese model using a different function that the other library,

448
00:48:10,860 --> 00:48:16,110
they should produce exactly the same answers, except for maybe the algorithm that's used for the estimation of things.

449
00:48:16,560 --> 00:48:19,290
But within of many, many decimal points, this should be identical.

450
00:48:19,950 --> 00:48:28,290
So G is usually presented as something you use with no normal data, simply because fitting it to normal data is goals.

451
00:48:29,820 --> 00:48:38,660
So you can achieve in normal data. There's no problem there at all for other marginal distributions, however,

452
00:48:39,450 --> 00:48:47,700
and you probably haven't noticed this in your theoretic classes, but there is no multivariate Poisson distribution.

453
00:48:48,730 --> 00:48:58,010
There's a multivariate normal. There is no multivariate binary multi meal or multi multivariate Bernoulli distribution.

454
00:48:59,600 --> 00:49:10,430
We can think about variants of that, but we can't take a multivariate Poisson distribution and produce for some originals, which is no such thing.

455
00:49:11,060 --> 00:49:19,490
The normal is really the only distribution that has that nice property, so we can't use likelihood methods to find the distribution of data at.

456
00:49:20,800 --> 00:49:23,260
So again, we have a salute. We have a value for ahead.

457
00:49:24,310 --> 00:49:29,650
I want to know what its distribution of sampling distribution is so that I can do inference and likelihood.

458
00:49:29,650 --> 00:49:33,040
Theory is not going to help me here because this isn't a maximum likelihood estimate.

459
00:49:34,750 --> 00:49:39,250
There are vast, vast amounts of theory and estimating equations.

460
00:49:39,430 --> 00:49:44,739
So there's a whole body of research in estimating equations that I don't know is that fruitful anymore?

461
00:49:44,740 --> 00:49:49,750
But it was huge in the nineties or maybe the early 2000s.

462
00:49:50,470 --> 00:49:58,330
It seems like an age ago, but estimating equation theory gets us basically the same answer we saw with maximum length.

463
00:49:58,330 --> 00:50:05,950
It estimates that beta hat asymptotically has a normal distribution centered at the right thing and it has a variance

464
00:50:05,950 --> 00:50:13,630
covariance matrix that looks just like the x transpose sigma inverse x transpose model based to variance estimate.

465
00:50:13,780 --> 00:50:19,000
Right? So this estimate error looks the same as it did for normal data.

466
00:50:19,900 --> 00:50:26,590
It's just that the inference behind that estimate takes a little more machinery which has been solved for us.

467
00:50:28,300 --> 00:50:36,970
So we have an estimate of our regression parameters from a longitudinal set of non normal data and we get a variance covariance

468
00:50:36,970 --> 00:50:45,010
matrix right in the diagonal of this thing gives me the variances in the standard errors that we produce and in the R output,

469
00:50:45,010 --> 00:50:48,920
for example. This is the model based.

470
00:50:49,070 --> 00:50:56,780
This is why I tried to teach this earlier in the semester. If my model for the variance covariance structure is correct.

471
00:50:57,910 --> 00:51:01,060
This is the model based variance of the regression parameters.

472
00:51:02,700 --> 00:51:06,680
If this is wrong. This variance is wrong.

473
00:51:10,050 --> 00:51:16,629
And if I assumed independence. When I fit everything.

474
00:51:16,630 --> 00:51:21,520
So there's an identity matrix in here and there truly was correlation in the data.

475
00:51:22,650 --> 00:51:25,870
This is wrong, and therefore the variance is wrong.

476
00:51:27,630 --> 00:51:30,780
So we need to somehow get a better variance estimate.

477
00:51:34,320 --> 00:51:38,200
And so that's what I'm getting to here. But let's first get to that ahead.

478
00:51:38,620 --> 00:51:46,450
So the estimating equations again written right there, we view that thing right there is a weight matrix that is the variance covariance.

479
00:51:46,870 --> 00:51:55,239
Again, the V is the variance part and the left eye is the the correlation part with normal outcomes.

480
00:51:55,240 --> 00:51:58,780
Recall that beta hat is unbiased. Right? I said this earlier.

481
00:51:59,170 --> 00:52:02,230
It doesn't matter what the weight matrix was.

482
00:52:02,950 --> 00:52:11,100
I still got a consistent estimate. The unbiased estimate excuse me of beta had an unbiased estimate of beta beta.

483
00:52:11,110 --> 00:52:17,170
That is an unbiased estimate of beta regardless of I have if I have the correlation structure right or not.

484
00:52:18,510 --> 00:52:22,400
The same concept carries over from not normal outcomes. So Engie.

485
00:52:23,810 --> 00:52:25,930
Peter Hatton is consistent. So I don't buy a strike.

486
00:52:25,940 --> 00:52:32,930
We're not talking about obviousness here, but some tactically better hat is getting close to the right thing.

487
00:52:33,170 --> 00:52:37,190
It's consistent for any weight matrix, any working correlation structure.

488
00:52:39,080 --> 00:52:44,900
So again, if it's consistent for any form of correlation, why wouldn't I just use independence?

489
00:52:46,620 --> 00:52:51,960
That's a simple correlation structure. The problem is that the inference is messed up.

490
00:52:53,810 --> 00:52:58,060
And you ask if you ever do a search on the Internet and this sort of stuff.

491
00:52:58,070 --> 00:53:05,060
G.E. has a long foundation in econometrics. I despise statistics and econometrics.

492
00:53:05,630 --> 00:53:10,459
They really don't care about inference. They just want a model. I mean, they have a lot of data.

493
00:53:10,460 --> 00:53:16,970
They just want to get the mean. And so you'll hear a lot of you'll read a lot of reports that say, who cares?

494
00:53:17,270 --> 00:53:21,290
Who cares what the way it matrix says? Just use independence. It's consistent.

495
00:53:22,610 --> 00:53:28,520
But if you're interested in doing inference, then we have a problem if we don't get the correlation matrix right.

496
00:53:28,790 --> 00:53:41,350
So what happens if we have the wrong form? So again, the working correlation produces a model based variance, which I just said a second ago.

497
00:53:42,250 --> 00:53:50,740
It's consistent. It's got the right value. Only if I have direct correlation structure and you keep saying this over and over and over.

498
00:53:52,850 --> 00:53:55,280
And again, we saw this with glass.

499
00:53:55,700 --> 00:54:04,940
If we do not want to rely upon our model for how I think the correlation should be, we can empirically estimated from the residuals.

500
00:54:05,570 --> 00:54:14,810
If I assume that my outcomes are independent of each other and that residuals have correlation in them, I need to use that information.

501
00:54:15,800 --> 00:54:20,870
And that leads to what we call the empirical or the sandwich variance estimate or.

502
00:54:23,670 --> 00:54:27,480
Again. It's just like what we had in jails. This is the model based variance.

503
00:54:30,540 --> 00:54:36,400
It's no. There we go. That's the bread. And then there's all this stuff in the middle of the sandwich.

504
00:54:38,400 --> 00:54:44,010
And that again, RSI is the variance covariance in the residuals.

505
00:54:44,850 --> 00:54:50,230
If I got the model right. This should be pretty close to independence.

506
00:54:50,740 --> 00:54:56,170
If I took care of the correlation in my model already, what's left in the residuals should be nothing but independence.

507
00:54:56,710 --> 00:55:01,750
And so this is close to an identity matrix and things start to simplify.

508
00:55:03,270 --> 00:55:07,020
So we talked about this with us.

509
00:55:07,080 --> 00:55:15,670
And the question is, is why? Why would I use a sandwich system, which, again, in G lingo is often referred to as an empirical variance estimated.

510
00:55:17,930 --> 00:55:23,350
Why do we do that? Because if the mean model is correct, then again the residuals.

511
00:55:23,350 --> 00:55:28,620
Tell us what structure for variance is still needed and that the variance will be corrected.

512
00:55:28,630 --> 00:55:34,750
The variance estimate from the sandwich estimate gets closer to the right answer and it's nearly unbiased for large.

513
00:55:34,750 --> 00:55:47,590
And even if I have the model wrong, if I have the model wrong that is here, the wrong part is here, the wrong part is here, the right part is here.

514
00:55:47,920 --> 00:55:53,410
So it corrects for the incorrectness of in the variance of y that you assumed.

515
00:55:59,230 --> 00:56:11,560
BNR And in most packages, when you get a report of results from a G.E. algorithm, it will call the empirical variance estimate a robust estimate.

516
00:56:12,490 --> 00:56:16,180
And that term came about from the creators of G.

517
00:56:16,990 --> 00:56:20,860
It has stuck around for as long as is as I've been doing this stuff.

518
00:56:21,790 --> 00:56:25,270
I had professors who kind of said, stay away from that.

519
00:56:26,350 --> 00:56:29,740
What is it robust for? Robust. Sounds like that really cool thing.

520
00:56:30,040 --> 00:56:31,890
Right. And robust.

521
00:56:33,160 --> 00:56:42,550
And again, if you're not quantitatively trained, you might think that that is an adjective that says wonderful, perfect as no problems.

522
00:56:42,650 --> 00:56:50,340
Right. It still has problems. And so folks have kind of said, well, do we really want to say robust?

523
00:56:50,350 --> 00:56:53,469
Right. So I don't ever I try not to call it a robust estimate anywhere.

524
00:56:53,470 --> 00:56:58,570
I talked about sandwich estimation a lot with goals or an empirical variance estimate.

525
00:56:59,540 --> 00:57:03,660
So just again, be aware of this terminology.

526
00:57:03,920 --> 00:57:09,290
Again, what is it robust to? It's robust to getting the correlation structure wrong.

527
00:57:09,710 --> 00:57:20,220
It isn't robust to anything else. It's not robust to, you know, having the wrong distribution or other issues such as cognitive sound system error.

528
00:57:23,900 --> 00:57:31,040
When you are reporting GDP, there we go. If you ever fit G beyond this class, what do you need to tell people?

529
00:57:32,870 --> 00:57:36,260
Right. You need to tell them the marginal mean. What is the regression model?

530
00:57:36,890 --> 00:57:47,210
What are the covariance? Right. What's the link? We have count data, we're going to fit post in regression with a log link with these covariates.

531
00:57:48,510 --> 00:57:51,660
You know, you don't need to specify the marginal variance outside of this class.

532
00:57:52,050 --> 00:57:55,740
You do need to specify the working correlation structure.

533
00:57:55,740 --> 00:58:06,630
You can't just say, I analyze these data using G, I analyze these data using G with a working, exchangeable correlation matrix.

534
00:58:08,040 --> 00:58:09,810
And then you have one more decision to make.

535
00:58:10,140 --> 00:58:17,970
Will you use the standard errors assuming exchangeable or a will you then use the sandwich variance estimated for inference?

536
00:58:18,150 --> 00:58:23,250
Which one will you use? You don't want to report both. If they're different.

537
00:58:24,300 --> 00:58:27,390
If they're different, I'm trying to teach you which one is right.

538
00:58:28,700 --> 00:58:32,850
Um, a lot of other folks don't care which ones, right? They're just going to pick the one that's smaller.

539
00:58:34,320 --> 00:58:37,710
So you're the communicator. You're not just the computer.

540
00:58:38,430 --> 00:58:44,350
So what do I do? What is Brian's method here? Whenever I use G, I assume independence.

541
00:58:44,370 --> 00:58:49,730
I just finished glm. Because all I want is a regression parameter estimate.

542
00:58:50,120 --> 00:58:54,140
And then I'll fix the inference later with with a sandwich variance estimate.

543
00:58:56,880 --> 00:59:02,460
That can be an efficient. If you know the right structure, you should use it.

544
00:59:02,470 --> 00:59:09,820
You should use it in the weight matrix for estimation, right? The weight the weight will change your specific estimate.

545
00:59:10,570 --> 00:59:13,960
Both of them are consistent, but they're different.

546
00:59:15,520 --> 00:59:18,700
So but in large samples, does it really matter if you have enough data?

547
00:59:19,000 --> 00:59:23,799
Does all this stuff really matter? That's the eternal question I think you should ever ask yourself.

548
00:59:23,800 --> 00:59:33,470
And what is large? I have no idea. Before we move on to random effects models, I intimated this.

549
00:59:33,490 --> 00:59:38,410
I think last week G is not a random effects model.

550
00:59:39,820 --> 00:59:44,480
G is a marginal model. We use a weight matrix to account for the correlation.

551
00:59:44,500 --> 00:59:48,840
There is nothing random effect d about this. That's a new word and a result before.

552
00:59:49,480 --> 00:59:57,640
Note that G models the between individual variation through the correlation matrix and within variation through the variance function.

553
00:59:59,160 --> 01:00:08,190
There are no random effects. And she. And as we have learned now with normal outcomes, we had two equivalent methods.

554
01:00:09,420 --> 01:00:13,080
We could fit GLC with an exchangeable correlation matrix.

555
01:00:13,440 --> 01:00:17,670
And now we know that GLS on normal data is G with normal data.

556
01:00:20,210 --> 01:00:27,550
Exchangeable correlation is the same as a random intercept. Randomness that produces constant correlation within a person.

557
01:00:29,260 --> 01:00:32,980
This idea does not carry over to non normal outcomes.

558
01:00:34,270 --> 01:00:38,980
So again, I will read papers. Well, they will say we said gee with a random intercept.

559
01:00:39,760 --> 01:00:47,709
You did not. You figure with an exchangeable correlation matrix and you have confused a concept

560
01:00:47,710 --> 01:00:51,940
with what's normal data that should not be carried here is not carry over any longer.

561
01:00:51,970 --> 01:00:55,690
Right. So we're going to fit a random intercept model to non normal data.

562
01:00:56,170 --> 01:00:59,800
It is not the same as g with exchangeable correlation.

563
01:01:00,700 --> 01:01:07,030
They are not going to be different for reasons that have to do with the nonlinear link function.

564
01:01:09,100 --> 01:01:13,300
And it is five after four. All right. Oh, I forgot my red X's.

565
01:01:13,390 --> 01:01:17,100
Here we go. They are not to the same rate.

566
01:01:17,380 --> 01:01:23,300
Not to the same. I want to stop here.

567
01:01:23,330 --> 01:01:29,270
I feel like I've just flown through a ton of information. I am not here on Friday.

568
01:01:30,020 --> 01:01:34,069
That means we don't see each other till next Wednesday. Next Wednesday.

569
01:01:34,070 --> 01:01:44,840
I want to review this again. Again, I just showed you a decade's worth of research for The Onion Ziegler in a set of PowerPoint slides.

570
01:01:47,060 --> 01:01:50,170
And then I want to go through our code, which I normally would do on a Friday.

571
01:01:50,180 --> 01:01:54,140
But since I'm not here on Friday, we're going to do it on Wednesday to help you with homework number four.

572
01:01:56,060 --> 01:02:02,450
And then I want to do some stimulating again with nominal data as I ponder as I discuss all this.

573
01:02:04,160 --> 01:02:09,440
Sandwich variance estimates are used all the time and G.

574
01:02:11,080 --> 01:02:15,020
It always has become a mantra. I think everyone fits G.E. They get a robot standard error.

575
01:02:15,400 --> 01:02:21,100
It's not robust, empirical or sandwich based variance estimate smarter and they're done.

576
01:02:21,790 --> 01:02:24,850
Why don't we do this with GLS? Why don't we do it with normal data?

577
01:02:25,390 --> 01:02:32,430
Why don't we just set a model, get a working correlation structure, and then report them with senators with us?

578
01:02:32,440 --> 01:02:37,510
We tend to report the model based industries more. I don't have an answer for that.

579
01:02:38,940 --> 01:02:41,730
So what I want to do via simulation is actually see what happens.

580
01:02:42,810 --> 01:02:50,250
Is there something about not normal data that the sandwich estimate gives us some sort of benefit that it doesn't?

581
01:02:50,250 --> 01:02:55,380
What's normal data? I don't know. Know it's lots of questions to to address.

582
01:02:55,410 --> 01:02:58,980
So who had a test today?

583
01:03:00,110 --> 01:03:04,730
I had to go with the stochastic was a stochastic or what.

584
01:03:04,750 --> 01:03:09,870
Don't know. Causal. That's right. So I need to take a causal inference class.

585
01:03:10,860 --> 01:03:16,800
You guys can teach me. I am doing research right now that blends into that field and I'm scratching my head.

586
01:03:18,030 --> 01:03:21,330
You made me think I missed my 680 exam just there. What's that?

587
01:03:22,000 --> 01:03:26,280
The stochastic. You missed the concept of the whole class, huh?

588
01:03:28,210 --> 01:03:34,430
Oh, all right. Well, I hope it went okay. Noble. Okay. Any more exams on Monday?

589
01:03:34,440 --> 01:03:39,830
801 on Monday. Oh, got it. Oh, those days.

590
01:03:39,840 --> 01:03:44,160
Yes. Good luck on that. I'll see you guys next week.

591
01:03:45,420 --> 01:03:58,260
And please fill out the form. I really do want to know if we can get the attendance up and people are feeling better about that walking around.

