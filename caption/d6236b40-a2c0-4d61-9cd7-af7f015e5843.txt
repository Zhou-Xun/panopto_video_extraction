1
00:00:00,570 --> 00:00:15,700
Over and over and over and over and over and over again.

2
00:00:27,010 --> 00:00:34,040
I thought that was great. Okay. Okay.

3
00:00:34,180 --> 00:00:39,100
So I think we'll get started. So I'm like, announce stuff at the beginning.

4
00:00:39,100 --> 00:00:43,030
So homework one is up. Some of you might have seen it.

5
00:00:46,240 --> 00:00:52,720
Just to introduce you a little bit to it, it's six questions.

6
00:00:54,190 --> 00:00:58,600
It's got instructions that say what to do if there's code. There's no code in this homework, so don't worry about that.

7
00:01:00,820 --> 00:01:05,440
The first three questions are just math problems.

8
00:01:05,440 --> 00:01:09,640
And I think that you should, after the first lecture, know everything that you need to know to do them.

9
00:01:12,190 --> 00:01:17,890
Question four I think that we'll learn about D separation today, so you should be able to do that after today.

10
00:01:18,610 --> 00:01:27,550
Question five uses material that we probably won't get to today, and then six is more of a thinking and writing question.

11
00:01:28,330 --> 00:01:31,420
And you, gosh, this projector is really just not very good.

12
00:01:31,810 --> 00:01:41,560
Can't really read this. So question six You should be able to do most of with what we did in the first class and the slides and so on.

13
00:01:41,860 --> 00:01:51,200
So it's. It's long because I had to write a lot of words, but I don't think it's a very long homework,

14
00:01:51,200 --> 00:01:54,799
so don't be intimidated by the fact that it is multiple pages.

15
00:01:54,800 --> 00:02:01,160
It's really just six problems. Let's see, there is one other thing, right?

16
00:02:01,160 --> 00:02:06,290
So I'm going to do one of the in-class activities and see how that goes.

17
00:02:06,500 --> 00:02:11,780
I just wanted to clarify. So I realized I wrote in the syllabus.

18
00:02:12,890 --> 00:02:17,180
That you could miss, like up to 20% of them. But you guys don't know what 20% is going to be.

19
00:02:17,180 --> 00:02:25,040
And so it might be mysterious how much that is. So I went back and I wrote four or 20%, whichever is more.

20
00:02:25,040 --> 00:02:28,460
I think that will probably do about 20 of these over the semester.

21
00:02:28,760 --> 00:02:36,620
So that should be about right. If 20% is more, I'll even do like ceiling or something then that's fine.

22
00:02:36,620 --> 00:02:44,449
So if you have to miss a couple of classes because you're out of town or you're sick, it's probably don't worry about it.

23
00:02:44,450 --> 00:02:48,790
It's probably fine. So I think so.

24
00:02:48,790 --> 00:02:57,360
We'll. Do the first activity. So the first activity is really just reviewing stuff that we went over in the first class.

25
00:02:57,370 --> 00:02:59,799
So I'm going to say pair up with someone.

26
00:02:59,800 --> 00:03:07,570
If you weren't in the first class, pair up with someone who was in the first class will probably be more fun.

27
00:03:07,720 --> 00:03:15,760
And I'm going to say, let's do this without looking at the slides because otherwise it's like doesn't require you to rattle your memory at all.

28
00:03:15,760 --> 00:03:24,220
If you can just read it off the slides. And just as a reminder, the in class activities are rated as either you did it or you didn't do it.

29
00:03:24,430 --> 00:03:26,230
There's no grade for if you got it right.

30
00:03:26,350 --> 00:03:34,060
It's just a chance for you to have to recall something that you learned in the past, and then you get to find out whether you recalled it or not.

31
00:03:34,360 --> 00:03:39,400
And also, I get a little bit of data about whether the things I said were memorable.

32
00:03:40,300 --> 00:03:45,879
If you need paper, there's paper here. Grab a partner and a pen and some paper.

33
00:03:45,880 --> 00:03:48,070
If you have paper, you're welcome to use your own.

34
00:03:48,820 --> 00:03:56,090
I think when you and Brian were both not here, maybe in the exam and you were in that direction with you,

35
00:03:56,090 --> 00:04:07,250
you were carrying around you and you compare during each turn in a separate put your name on it.

36
00:04:12,110 --> 00:04:17,330
Good to see them again, Brad. Rather than events.

37
00:04:17,540 --> 00:04:21,410
In the meantime, if there needs to be three, that's fine.

38
00:04:23,420 --> 00:04:29,120
And I think we should doing okay. I don't mind investing in the cemetery.

39
00:04:31,820 --> 00:04:35,120
Sensitive to post-production. So.

40
00:04:35,120 --> 00:04:38,540
Yeah. Yeah. So grab a pair.

41
00:04:38,960 --> 00:04:42,570
Basically, each pair is going to try to write down a definition of something we learned in class.

42
00:04:42,590 --> 00:04:47,150
Or close your laptop so that you aren't tempted. And I'll.

43
00:04:47,240 --> 00:04:52,040
I'll just, like, count numbers, and then you'll do the number that you got.

44
00:04:52,040 --> 00:04:56,270
So I'll wait for all the papers to be settled and then we'll talk about them in class.

45
00:04:56,720 --> 00:05:13,540
So let's say Andy and Brad, you're one Malcolm and you two, three for Dan, Lou and Taylor for you and the Yang, which I should know, because I.

46
00:05:13,580 --> 00:05:17,150
You were in my class. Okay, well, my. At five.

47
00:05:18,300 --> 00:05:20,800
Five. And there's five of these. And then we start over.

48
00:05:20,820 --> 00:05:29,400
So let's say one guy, Chris, and then you, RU, and then our person who just walked in in the gray.

49
00:05:29,460 --> 00:05:34,230
Remind me your name. Sorry. I'm leaving.

50
00:05:35,190 --> 00:05:39,090
Maybe join. Join the group on your right or left.

51
00:05:39,180 --> 00:05:46,800
One of those. Everyone has a number. Okay, so just try to try to generate from your memory a definition of.

52
00:05:47,730 --> 00:05:53,190
I realize these aren't numbered, but you can count to five of the the term here.

53
00:05:53,190 --> 00:05:59,130
You can write it in words. You can write it in mathematical symbols, whatever, whatever works for you.

54
00:06:00,900 --> 00:06:05,300
Put your names on it and put the thing that you're trying to define on there so I know what you're going for.

55
00:06:06,090 --> 00:06:11,830
Let's give it like. Five ish, some 10 minutes until people look satisfied.

56
00:06:11,860 --> 00:06:16,510
I guess we have everything okay. I don't think I have a chance.

57
00:06:16,610 --> 00:06:34,080
Maybe it's because you have up explosions and it is what I want to say about the average person.

58
00:06:34,810 --> 00:06:46,740
So we were evacuated the previous year. It's not really assuming everyone is the whistler is counter-factual or not.

59
00:06:49,580 --> 00:06:59,590
Yes, I think actually drive is an explanation of why we would see argumentation.

60
00:06:59,640 --> 00:07:03,130
Why I do not remember it all. That's right. Yeah, right.

61
00:07:03,790 --> 00:07:09,740
So it's changeability, I think. I think.

62
00:07:10,690 --> 00:07:13,870
Okay, what happened? So I heard from your grandmother, right?

63
00:07:14,140 --> 00:07:28,200
Oh, so if you remember, then that would be the end of the internet in terms of differing opinions.

64
00:07:28,270 --> 00:07:31,430
And we should have that, right?

65
00:07:31,660 --> 00:07:44,709
Yes. This is what a patient war and health care unit is, that, you know,

66
00:07:44,710 --> 00:07:53,920
the comforting commentary from all of you is that it could also be a school that know if it was okay.

67
00:07:53,920 --> 00:07:57,070
I think that's what the exaggerate. That's quite a hospital.

68
00:07:58,570 --> 00:08:10,790
This was a year ago and a new fancy convention in notation that the mission has got to reduce the difference.

69
00:08:10,830 --> 00:08:27,750
So I just need to work on a Saturday night and I think this is why I shrunk.

70
00:08:27,930 --> 00:08:50,920
And I know we can, but I don't I don't think I think there that's one patient becomes more and more like why yeah I never know until I was okay.

71
00:08:51,120 --> 00:09:05,020
Going to do all the same observation and interview and you have just nothing on anymore.

72
00:09:05,060 --> 00:09:09,120
It was and it's I think now we don't need a cruise.

73
00:09:09,130 --> 00:09:16,690
It can feel done. You can bring your your paper out the front door where where would you like?

74
00:09:17,290 --> 00:09:28,240
And that seems to be located at least in Seattle on Christmas.

75
00:09:28,790 --> 00:09:33,519
I don't remember exactly the same or that it's like you didn't get that one.

76
00:09:33,520 --> 00:09:46,050
And the difference between that's that's a good spot that occurred to me 107. appearance was very interesting

77
00:09:46,120 --> 00:09:54,370
that that doesn't or interference would be treatment of one unit doesn't be thank you I'm going to someone else

78
00:09:55,030 --> 00:10:02,830
and look at the stage of military action network so many times and never do we have a one on one is extraordinary

79
00:10:02,860 --> 00:10:14,740
that incredible because we don't do so many different why do I remember the exchange of all that writing?

80
00:10:14,770 --> 00:10:22,890
Is that little pilot or all the work that I do?

81
00:10:23,500 --> 00:10:37,640
I just realized I'm in second grade students bicycling.

82
00:10:40,270 --> 00:10:44,110
If you don't know, it's okay to write. I don't remember.

83
00:10:44,110 --> 00:10:49,509
Or, you know, something to that effect. That's okay. All right.

84
00:10:49,510 --> 00:11:19,780
Last three groups, let's say 1.5 more minutes. I think there's a whole pile here because it wasn't quite right.

85
00:11:22,150 --> 00:11:38,090
But there is an older folk hero in Lou.

86
00:11:38,210 --> 00:11:45,080
Let's let's wrap it up. There's no no consequences of getting an incomplete or losing her.

87
00:11:46,320 --> 00:11:52,350
Yeah, well.

88
00:12:08,830 --> 00:12:13,910
Yeah. All right. Good job, everyone. Okay, so now.

89
00:12:13,980 --> 00:12:16,150
Now I think we'll go through these.

90
00:12:16,150 --> 00:12:23,210
And the reason that we did a like definition review on the first day is these terms are going to they should really sink into your brain.

91
00:12:23,230 --> 00:12:28,450
These are going to be your friends by hopefully the next couple of weeks and certainly by the end of the semester.

92
00:12:29,710 --> 00:12:34,920
Which also reminds me that I added. To the candidates page.

93
00:12:35,460 --> 00:12:37,920
This is sort of an aspirational thing, so we'll see how it goes.

94
00:12:38,160 --> 00:12:46,440
So I added this glossary and review page so that when you forget a term and you don't want to rifle through the slides,

95
00:12:46,830 --> 00:12:55,410
you have like a quick a quick reminder. There's not all the details about the nuances here, just the like sort of the flash card definitions are here.

96
00:12:55,710 --> 00:13:04,050
If I manage to keep up with it, that'll be great. If I don't, maybe I'll ask someone enterprising to add to it.

97
00:13:06,000 --> 00:13:12,510
Okay. So back to let's do a review of these terms that I asked you to define.

98
00:13:12,510 --> 00:13:16,740
So I'm going to let's see if I can find it. Okay.

99
00:13:16,800 --> 00:13:23,400
So we have counterfactual, average treatment, in fact, consistency, exchange, ability and interference.

100
00:13:25,800 --> 00:13:32,030
So let's go through those. Briefly, and then that'll set us up for where we're getting here.

101
00:13:32,140 --> 00:13:42,620
Okay. So counterfactual is the thing that we started with. Counterfactual is the thing that we started with at the beginning of the lecture last time.

102
00:13:42,620 --> 00:13:47,329
And it's sort of the thing that like is going to hold a lot of this class together.

103
00:13:47,330 --> 00:13:50,810
Right. So the counterfactual. Let's see if we can get there.

104
00:13:51,680 --> 00:13:56,360
Oh, that's a tip if you're looking through slides. Oh, we'll make it do this.

105
00:13:57,770 --> 00:14:03,170
So that will make it faster for both you and I to get around.

106
00:14:03,180 --> 00:14:07,100
Right. So the counterfactual is this hypothetical value.

107
00:14:07,370 --> 00:14:09,080
What would unit I,

108
00:14:09,570 --> 00:14:18,680
what would I have observed from unit I there outcome if they had had this intervention of setting their exposure to too little a right.

109
00:14:18,680 --> 00:14:29,760
So we're going to use the notation Y of little A in the slides in the book they use y superscript little okay.

110
00:14:29,990 --> 00:14:33,799
So that we did counterfactual average treatment effect.

111
00:14:33,800 --> 00:14:39,740
This is just our difference in mean counterfactuals, right so average treatment effect.

112
00:14:41,300 --> 00:14:47,780
Let's see if we can get there. Great.

113
00:14:47,840 --> 00:14:53,660
Average treatment effects just difference in ear y of one minus e of y of zero.

114
00:14:56,770 --> 00:15:02,620
Consistency. Right. So I heard someone mention, oh, I'm glad I didn't get consistency because I don't remember what it is.

115
00:15:02,770 --> 00:15:13,000
And it's kind of it's like a weird, subtle idea the first couple times that you hear it because it sounds almost like tautological consistency is.

116
00:15:15,000 --> 00:15:22,920
Consistency is the property that if I have a unit and I observe they got exposure little I and I saw

117
00:15:22,980 --> 00:15:30,210
outcome why I then if I had intervened and forced them to get the exposure that they actually got,

118
00:15:30,690 --> 00:15:35,820
they would have had the same outcome as what I saw. Right. So this is saying essentially that my.

119
00:15:37,050 --> 00:15:41,310
My intervention is not in some way fundamentally different from.

120
00:15:43,070 --> 00:15:48,700
The world that I'm observing. Right. So you could imagine cases where that that wouldn't be true.

121
00:15:48,710 --> 00:15:57,410
I think one like silly example that one of my my past advisors used to use is let's say I intervene

122
00:15:58,040 --> 00:16:04,160
to increase your BMI and I do it by like injecting a bunch of metal into your shoes or something.

123
00:16:04,580 --> 00:16:16,010
Right. So. The intervention approach might lead to a different outcome in this case because it's a silly intervention.

124
00:16:16,280 --> 00:16:25,819
Then my natural observation of what if your BMI just is 22 versus What if I forced your BMI to be 22 by doing something that doesn't make sense,

125
00:16:25,820 --> 00:16:27,110
like putting metal in your shoes.

126
00:16:29,420 --> 00:16:36,979
So consistency is sort of necessary to make any of our causal questions make sense, and it's necessary for our framework to hold together.

127
00:16:36,980 --> 00:16:41,000
And we generally just assume consistency, but we have to remember that it's always under there.

128
00:16:42,110 --> 00:16:47,399
Okay. Exchange ability. This is another really important one.

129
00:16:47,400 --> 00:16:50,880
And we're going to spend a lot of time on our exchange ability in the next like three or four days.

130
00:16:51,420 --> 00:16:59,380
So exchange ability. Is this conditional independence or in this case, just an independence assumption?

131
00:16:59,390 --> 00:17:07,720
So we're assuming that. If if if we have exchange ability, then y of a is independent of capital.

132
00:17:08,290 --> 00:17:11,560
So that saying essentially exchange ability is saying.

133
00:17:13,380 --> 00:17:15,300
My data looks like a randomized trial.

134
00:17:15,390 --> 00:17:24,180
We're going to talk about a randomized trial and we're also going to talk about conditional versions of this exchange ability statement today.

135
00:17:24,480 --> 00:17:32,470
And then finally, interference. This was that second condition in setback, the stable unit treatment value assumption.

136
00:17:32,490 --> 00:17:35,810
So. SAT was this two part.

137
00:17:37,640 --> 00:17:41,080
Let's see if I can. Here we go.

138
00:17:43,950 --> 00:17:51,310
Right. So interference is this phenomenon when the treatment of those around you affects your outcome.

139
00:17:51,330 --> 00:17:55,410
So the biggest I think the easiest example to think about for interference is.

140
00:17:58,380 --> 00:18:07,680
Immunization rates. So this herd immunity idea, if everyone in my town gets immunized, I sort of get a little bit of effect from that, right.

141
00:18:07,680 --> 00:18:12,750
I'm protected because the people around me don't have measles or COVID or etc.

142
00:18:15,000 --> 00:18:21,110
Okay. So that's our little review. And I think that brings us right up to where we left off.

143
00:18:21,120 --> 00:18:28,980
So we got all the way up to this ID theorem, and the ID theorem told us that if we have consistency,

144
00:18:29,280 --> 00:18:35,489
the stable unit, treatment, value, assumption and exchange ability, then my counterfactual value.

145
00:18:35,490 --> 00:18:45,030
The thing that I would like to ask questions about has the same distribution as the conditional Y, given a, which is the thing that I get to see.

146
00:18:45,090 --> 00:18:47,970
So this is like the magical theorem that says.

147
00:18:49,190 --> 00:18:56,500
I can observe properties of the thing I want to measure y of a from the sample that I have where I only get to see y given a.

148
00:18:56,780 --> 00:19:02,120
So this theorem is sort of the. Underlies a lot of the stuff that we're going to do.

149
00:19:02,120 --> 00:19:06,860
And versions of this theorem are going to be things that we work with for the entire semester.

150
00:19:08,040 --> 00:19:11,840
Okay. So we got through that. This.

151
00:19:14,660 --> 00:19:20,690
Okay. So randomized experiments, I think most people are familiar with the idea of a randomized experiment.

152
00:19:20,700 --> 00:19:26,749
So we have units, they're assigned into a treatment value by some sort of random procedure, right?

153
00:19:26,750 --> 00:19:32,989
You generate a random number in a computer, you flip a coin, etc., and for now we're going to assume full compliance, right?

154
00:19:32,990 --> 00:19:38,180
So if you were assigned to have treatment equals zero, you always get treatment equals zero.

155
00:19:38,180 --> 00:19:42,290
If you're assigned to have treatment equals one, you always have treatment equals one.

156
00:19:42,710 --> 00:19:51,020
So in this design exchange, ability holds because we controlled everything about how the exposure is handed out.

157
00:19:51,410 --> 00:19:57,710
So exchange ability hold by design. This means it's actually easy for us to measure that average treatment effect.

158
00:19:57,920 --> 00:20:04,580
So as long as consistency in sector holds, so as long as I've defined my intervention logically,

159
00:20:04,610 --> 00:20:09,290
which it's kind of hard not to when you're actually doing a trial. Right, because you have to execute the trial.

160
00:20:09,290 --> 00:20:16,920
So it's harder to get away with thinking about interventions that are not as specific as you thought they might be.

161
00:20:16,940 --> 00:20:25,190
So usually consistency in sight hold. I can just estimate the average treatment effect as the difference of two averages.

162
00:20:25,820 --> 00:20:29,420
They'll do a little bit of algebra with this difference of two at averages.

163
00:20:29,420 --> 00:20:34,999
Estimate here in homework one. So this is why we like randomized experiments.

164
00:20:35,000 --> 00:20:40,800
They're hard to do. They're hard to organize and, like, fund and pull off.

165
00:20:40,820 --> 00:20:45,040
What? Once you did it, they're easy to analyze. All right.

166
00:20:45,040 --> 00:20:50,170
So the next, like, most complicated thing that I could do is a conditionally randomized experiment.

167
00:20:50,200 --> 00:20:59,170
So let's say let's say I had some feature l and I want to assign treatment based on your value of L.

168
00:21:00,010 --> 00:21:08,110
So if L and y of A are not independent that I just tanked exchange ability, I don't have exchange ability anymore.

169
00:21:08,980 --> 00:21:12,150
I have a sort of like created confounding for myself.

170
00:21:12,160 --> 00:21:15,340
We're going to talk about what confounding mean formally later today.

171
00:21:15,850 --> 00:21:24,310
However, if I know how I made the assignment, which I do, because I'm doing a trial, I can still identify the 80.

172
00:21:24,520 --> 00:21:28,420
And I all I need is this like conditional version of the ID theorem.

173
00:21:30,080 --> 00:21:34,190
So the idea here, let's say we're doing a trial for a new surgery option.

174
00:21:34,370 --> 00:21:40,370
So say L equals zero. I think we had this surgery example last year.

175
00:21:40,410 --> 00:21:44,240
Yeah. So l equals zero. A patient is less sick.

176
00:21:44,420 --> 00:21:47,569
L equals one. A patient is more sick. So I think the surgery is good.

177
00:21:47,570 --> 00:21:53,780
And I want to assign my more sick patients to get the new surgery more often because I think it's going to help them.

178
00:21:54,710 --> 00:22:03,170
So I decide that I'm going to randomize so that my more sick patients get the surgery 75% of the time.

179
00:22:03,530 --> 00:22:06,650
Am I less sick patients get the new surgery only half the time.

180
00:22:09,390 --> 00:22:14,300
Okay. So this trial that I've done. If we just split it on.

181
00:22:14,310 --> 00:22:22,550
L It looks like two fully randomized trials where I just conditioned on L as a like inclusion criteria into the trial.

182
00:22:23,560 --> 00:22:30,490
So that should give me a hint that I should be able to estimate the 80 because I could do it in each of those sub trials.

183
00:22:30,490 --> 00:22:38,510
Right. So conditional exchange ability captures the idea that the data are randomized within levels of so conditional exchange ability.

184
00:22:38,510 --> 00:22:44,479
You can think of as saying this looks like a randomized trial on some level of granularity,

185
00:22:44,480 --> 00:22:50,510
like if I divide my patients in on enough things, then I have a randomized trial within those groups.

186
00:22:52,030 --> 00:22:59,080
So conditional exchange ability is just y of a is independent of a conditional on L.

187
00:22:59,200 --> 00:23:06,780
So it is what you thought it was going to be. OC stratum specific causal effects and standardization.

188
00:23:06,790 --> 00:23:09,820
So let's say we have conditional exchange ability,

189
00:23:11,350 --> 00:23:19,060
then we know that e of y of a conditional on l is identifiable because we can just use the ID theorem that we used before.

190
00:23:19,090 --> 00:23:28,180
Right. So we just have this. Why are they conditional on l is the same in distribution as Y given a and l.

191
00:23:29,440 --> 00:23:34,569
All right. So now I want to estimate the non conditional average treatment effect rates

192
00:23:34,570 --> 00:23:39,639
of the population level marginal counterfactual value of Y under treatment.

193
00:23:39,640 --> 00:23:47,350
A So all I'm going to do is I'm going to do a weighted average. So in this scenario, I'm imagining that all is something that has discrete levels.

194
00:23:47,620 --> 00:23:52,650
If L doesn't have discrete levels, I can take this sum and turn it into an integral right.

195
00:23:52,660 --> 00:23:56,470
It's not. Nothing too bad happens in that case.

196
00:23:57,250 --> 00:24:01,959
I do at that point need to know the density of L, but we'll come back to that.

197
00:24:01,960 --> 00:24:09,160
So for now, let's say L has discrete levels so I can just take my e of y of a that I did in each

198
00:24:09,460 --> 00:24:15,730
sublevel of l and just average it by the population frequency of that level of l.

199
00:24:17,780 --> 00:24:24,050
So this is called the standardized mean and it's standardized by the population frequencies of ell and you might like.

200
00:24:24,980 --> 00:24:28,940
Make choices about what you use as these population frequencies.

201
00:24:28,940 --> 00:24:34,730
Right. So it might be that like in your hospital setting, l equals one is like pretty common.

202
00:24:34,940 --> 00:24:39,770
40% of the patients have this like more sick characteristic, but.

203
00:24:40,970 --> 00:24:47,630
In the larger patient population that you think the surgery is going to be applied to l equals one is only 10%.

204
00:24:47,900 --> 00:24:55,880
So in that case, you might use L equals one. When you calculate or I'm sorry, 10% point one, you might use l.

205
00:24:56,120 --> 00:25:02,570
P of l equals one as 0.1. Even though that's not the population that you have, it's the population that you're trying to match.

206
00:25:06,090 --> 00:25:14,399
Okay. So the standardization trick we can't use if one of those p m l equals l terms is zero.

207
00:25:14,400 --> 00:25:20,760
Right? So if. Within one level of l patients.

208
00:25:20,770 --> 00:25:28,240
I'm sorry, not PML equals out. If I can't estimate e of y given an l because some people never.

209
00:25:28,570 --> 00:25:35,410
If some combination of A's and ls that never exist in my population, I can't do the standardization trick.

210
00:25:35,410 --> 00:25:39,879
I need to have an estimate for e of y given a and l for every level of a.

211
00:25:39,880 --> 00:25:42,580
And so this is called positivity, right?

212
00:25:42,580 --> 00:25:52,150
It's just this property that for every combination of A's and L's, there are some people who have that combination in my study.

213
00:25:53,560 --> 00:25:56,950
So in our conditionally randomized experiment, positivity holds.

214
00:25:56,950 --> 00:26:02,110
And of course you would never design a conditionally randomized experiment where positivity doesn't hold.

215
00:26:02,710 --> 00:26:10,330
So this brings us to our conditional ID theorem, and this just says if we've got the assumptions we had before,

216
00:26:10,330 --> 00:26:15,969
it's or consistency set by and now conditional exchange, ability and positivity.

217
00:26:15,970 --> 00:26:17,560
So we need to add positivity here.

218
00:26:17,920 --> 00:26:28,150
Then we have this conditional ID theorem which says that Y of a given l is equal in distribution to y given l and a.

219
00:26:31,470 --> 00:26:36,570
So this is the same proof that we used for the non conditional version of this theorem.

220
00:26:37,320 --> 00:26:41,850
So we do so the same steps, but we just conditioned on L in every step.

221
00:26:41,860 --> 00:26:46,290
So the first step we're going to use conditional exchange ability to conditional on a.

222
00:26:46,560 --> 00:26:51,450
In the second step, we just use consistency to replace Y of A with y.

223
00:26:52,140 --> 00:27:02,200
There are questions here. I think to me this result is like intuitive with some of what you've learned in statistics so far.

224
00:27:02,210 --> 00:27:09,970
You might not have like called it exchange ability, but you have looked at things like confounders and thought I should adjust for those.

225
00:27:09,970 --> 00:27:14,350
Right? So that's that's what's going on here. We're going to talk about what confounding means.

226
00:27:16,600 --> 00:27:20,049
Okay. Inverse probability weighting. So there's another option.

227
00:27:20,050 --> 00:27:23,290
If I didn't want to do the standardization trick, there's another thing that I could do.

228
00:27:23,650 --> 00:27:30,550
So the way that you get to inverse probability weighting, or at least the mental exercise that we'll do to get ourselves there.

229
00:27:30,790 --> 00:27:33,520
And so let's picture our trial that we took.

230
00:27:34,240 --> 00:27:45,070
75% of our sick patients got the new surgery, 50% of the less sick patients got the new surgery, and now take each of those patients and clone them.

231
00:27:45,400 --> 00:27:51,160
So everyone has a body who is identical to them on the features that matter.

232
00:27:51,160 --> 00:27:54,550
In this case, it's just l but in some other case, it might be something else.

233
00:27:54,730 --> 00:28:02,980
Okay, so now we can imagine this super trial that includes all two times and people in that trial.

234
00:28:03,400 --> 00:28:10,000
We have a fully randomized trial and half of all the people got the treatment.

235
00:28:10,000 --> 00:28:14,350
So we were fully randomized trial with p of equals.

236
00:28:14,710 --> 00:28:18,880
A given l equals 0.5 for every level of L because those are all the same.

237
00:28:19,120 --> 00:28:22,270
I can just forget about l in the pseudo population.

238
00:28:23,740 --> 00:28:29,649
So that means that if I could get the estimate that I would have gotten in the pseudo population that I'm good to go.

239
00:28:29,650 --> 00:28:34,420
So in the pseudo population e of y given a estimates.

240
00:28:34,630 --> 00:28:38,580
E of y of a. Okay.

241
00:28:38,590 --> 00:28:44,739
So now I imagine how did I get from this pseudo population, this population of two times,

242
00:28:44,740 --> 00:28:51,460
and people who are all clones of someone who's in my my actual trial?

243
00:28:51,490 --> 00:28:56,290
How did I get from there to the actual trial that I did? So I can imagine a sampling mechanism.

244
00:28:56,350 --> 00:29:06,370
In the sampling mechanism, I take the subset of that pseudo population that has equal zero, and I sample half of those participants into my trial.

245
00:29:06,850 --> 00:29:11,110
And I take the subset of the trial that has L equals one,

246
00:29:11,350 --> 00:29:18,880
and I sample a quarter of the people who got a equals zero and I sample three quarters of the people who got a good one.

247
00:29:19,570 --> 00:29:30,160
So that procedure should give me the same trial that I actually have with my, my stratified treatment assignment.

248
00:29:32,170 --> 00:29:37,140
So I can take that sampling procedure from the pseudo population and turn it into a weight.

249
00:29:37,150 --> 00:29:41,260
And I just do this by looking at what is the probability that each person would

250
00:29:41,260 --> 00:29:45,280
have gotten sampled from this two times in population into my actual trial.

251
00:29:45,550 --> 00:29:52,420
So if you had l equals zero, regardless of your value, your chance of getting sampled was 0.5.

252
00:29:52,430 --> 00:29:56,110
So we can say each of those people represents two people.

253
00:29:56,110 --> 00:29:59,770
So they represent two people in the pseudo population, so their weight is two.

254
00:30:00,580 --> 00:30:06,630
If you had A equals zero and L equals one, I only took a quarter of those people.

255
00:30:06,640 --> 00:30:11,770
So each one of those people represents four people in the pseudo population, so their weight is four.

256
00:30:12,250 --> 00:30:17,040
And then if you had equals one and L equals zero, we took three quarters of those people.

257
00:30:17,170 --> 00:30:24,459
So each of those people is really only representing one in a third. So which I got by just dividing one by 0.75.

258
00:30:24,460 --> 00:30:31,450
So that's everyone's weight. And if I weight everybody by that, I should get back to this two times in population.

259
00:30:33,210 --> 00:30:36,240
And if I do that, then I should be able to estimate of y of a.

260
00:30:36,750 --> 00:30:45,210
So formally the way that we set this up, let's say f sub a given l is the conditional pdf of a given l.

261
00:30:46,680 --> 00:30:49,920
We're going to assume that F of a given l is greater than zero for all eight.

262
00:30:49,950 --> 00:30:55,380
So that's positivity there. So we're going to need that. So we're going to put it on the bottom of this fraction.

263
00:30:58,180 --> 00:31:01,960
Then the IP waiting for an individual. W so i.

264
00:31:02,440 --> 00:31:06,800
A is just one over. After being given out.

265
00:31:08,140 --> 00:31:19,180
So now all I do is I multiply everybody by their weight and I calculate each of y given a and so that turns out to be just the expected value.

266
00:31:19,190 --> 00:31:30,070
This is an indicator. Did you actually get treatment a and then you're y divided by F of a given l so that's our IP weighting estimate or for what?

267
00:31:30,130 --> 00:31:36,270
E Of why are they? Questions here. Okay.

268
00:31:37,390 --> 00:31:41,560
So satisfyingly we have these two ways of doing it. We have standardization.

269
00:31:41,740 --> 00:31:50,620
We have IP weighting in this regime where A and L are both discrete and I can measure all of these probabilities.

270
00:31:50,830 --> 00:31:54,580
These are exactly equivalent, right? They're not just like asymptotically equivalent.

271
00:31:54,580 --> 00:31:58,989
They're numerically exactly equivalent, which is nice because now we don't have to make decisions.

272
00:31:58,990 --> 00:32:02,740
We can just do one of them. So the.

273
00:32:03,890 --> 00:32:11,240
Way that we find out that IP weighting and standardization are the same is we just apply our iterated expectation formula.

274
00:32:11,480 --> 00:32:14,570
And here I've done it very explicitly in the book.

275
00:32:14,570 --> 00:32:19,040
I think they skip like. Two and a half of these four steps.

276
00:32:20,900 --> 00:32:22,300
But you can sort of work through this, right?

277
00:32:22,310 --> 00:32:30,050
So if we start with we take this expectation and we just spread it out expectation with respect to l with respect to and with respect to Y.

278
00:32:30,080 --> 00:32:40,819
So this this middle part expectation of indicator of equals a y over F of a given l and then we're taking the expectation just with respect to Y,

279
00:32:40,820 --> 00:32:49,190
so I can just replace this Y with expectation of y given A and L, and now I'm going to take the expectation with respect to A.

280
00:32:49,550 --> 00:32:56,690
So this becomes just the probability that A equals l given l this part stays the same,

281
00:32:56,690 --> 00:33:02,120
except now we have a little a inside our conditioning and we put a little a on the bottom.

282
00:33:02,990 --> 00:33:12,920
And then finally we use our fact that f of a given l is equal to probability of equals, n given l equals l, which is going to be this top part.

283
00:33:12,920 --> 00:33:19,490
Once we condition on l those cancel out and we just get our standardization estimate at the bottom.

284
00:33:20,270 --> 00:33:24,450
So. Okay.

285
00:33:24,660 --> 00:33:31,350
So this proof has a straight forward extension to continuous l by just replacing the sum in our.

286
00:33:33,460 --> 00:33:39,700
The sun gets replaced with an integral. It doesn't extend to continue as a.

287
00:33:39,910 --> 00:33:43,840
So for continuous a we have to do a different kind of waiting.

288
00:33:44,410 --> 00:33:48,900
We're going to talk about continuous a in more detail in the future.

289
00:33:48,910 --> 00:34:00,190
For now, in this like more conceptual part, we're going to stick with categorical A where we have positive probability on any particular value of a.

290
00:34:04,270 --> 00:34:11,020
Okay. So we did all of that in the context of a randomized trial and then a conditionally randomized trial.

291
00:34:11,380 --> 00:34:19,870
None of our ID theorem identification conditions, which are consistency, setback, conditional exchange, ability and positivity.

292
00:34:19,870 --> 00:34:25,720
None of these required that I did an experiment, right? None of them said, and you did an experiment.

293
00:34:27,460 --> 00:34:30,700
So I can do this in observational data if I can meet these assumptions.

294
00:34:31,030 --> 00:34:38,860
The issue is that there are much stronger assumptions in observational data, and we have to think really hard about do they fit?

295
00:34:39,040 --> 00:34:42,790
So we're going to sort of like walk through these and think about what does it mean to

296
00:34:42,790 --> 00:34:47,980
believe this in observational data versus what does it mean to believe this in a trial?

297
00:34:49,000 --> 00:34:55,660
So consistency, just to remind us again, it says that for unit I who receive treatment little AA,

298
00:34:56,110 --> 00:35:02,590
the thing I observe y is the same as what I would have observed if I had intervened on them.

299
00:35:02,800 --> 00:35:08,260
I know this is like repetitive that we just talked about it, but these these conditions are.

300
00:35:09,070 --> 00:35:14,180
These are like the. Yellow Brick Road that you will walk on to get to Cleveland in France.

301
00:35:14,180 --> 00:35:19,219
So we're going to, like, really hammer them in. Okay.

302
00:35:19,220 --> 00:35:22,220
So in a trial with an intervention.

303
00:35:23,290 --> 00:35:26,030
Worrying about consistency just doesn't make any sense, right?

304
00:35:26,080 --> 00:35:33,970
I don't have to worry about if what they what happened to them is what would have happened them if I'd intervened because I did intervene.

305
00:35:34,000 --> 00:35:39,970
Right. So it's like tautological in an observational study that's not always the case.

306
00:35:39,980 --> 00:35:46,260
Right? So people can change their behavior if they receive an intervention versus if they choose a behavior independently.

307
00:35:46,270 --> 00:35:47,860
We talked about this some last time, right?

308
00:35:47,860 --> 00:35:55,090
So if I have an app on my phone that buzzes and it says, go for a walk, it's time for you to go for a walk.

309
00:35:56,460 --> 00:35:58,630
You know, some people will say, that's great.

310
00:35:58,650 --> 00:36:03,960
I'm going to go for a walk and I'm going to have exactly the same experience as if I decided to go for a walk on my own.

311
00:36:04,260 --> 00:36:07,050
Me personally, I'm going to look at my phone and say, You know what?

312
00:36:07,290 --> 00:36:10,950
I might go for a walk, but I'm going to have a bad time because I don't like being told what to do.

313
00:36:11,880 --> 00:36:17,040
I'm going to go for that walk and I'm just going to think. I don't know if this is the right time for me to go for a walk.

314
00:36:17,070 --> 00:36:18,959
I don't like people making decisions for me.

315
00:36:18,960 --> 00:36:23,520
And then I'm going to come back and I'm going to be more grumpy than I would have been if I decided to do it on my own.

316
00:36:24,420 --> 00:36:28,379
The window into how I am. All right.

317
00:36:28,380 --> 00:36:29,790
Conditional exchange ability.

318
00:36:30,240 --> 00:36:36,660
So in a trial, we guarantee exchange ability with our study design, either conditional or unconditional exchange ability.

319
00:36:36,840 --> 00:36:44,380
So we have total control over what treatment everyone gets in an observational study.

320
00:36:44,760 --> 00:36:49,950
Treatment assignment occurs, let's say, naturally, right? So people just get the treatment that they get.

321
00:36:50,970 --> 00:36:59,250
So identifying a set of variables to give us conditional exchange ability requires a bunch of assumptions and knowledge about how the system works.

322
00:37:00,900 --> 00:37:05,370
It also may not be possible to measure all of the necessary variables.

323
00:37:05,370 --> 00:37:15,120
So we're going to talk in the next slide deck about this project of how do I identify the set of variables that I need to condition on?

324
00:37:17,320 --> 00:37:22,110
All right, positivity. So in a trial, I'm never going to design a trial that doesn't have positivity.

325
00:37:22,150 --> 00:37:25,180
That would be silly in observational data.

326
00:37:25,600 --> 00:37:33,140
It's very easy to violate positivity rate. So especially in cases where you have big divisions between groups of people, right?

327
00:37:33,160 --> 00:37:37,810
So there could be levels of AML where there's just nobody in that level.

328
00:37:37,820 --> 00:37:41,920
So some examples are like, let's say you're looking at.

329
00:37:44,820 --> 00:37:53,130
Like race and neighborhood, right? So in some there are some neighborhoods where that neighborhood is 98%, 99%.

330
00:37:53,580 --> 00:38:00,330
One race. You may have nobody in your trial who is, you know, from each of the race categories that you're trying to look at.

331
00:38:00,330 --> 00:38:08,520
And it also lives in that neighborhoods you you can't condition on in that case, neighborhood, because you don't have positivity.

332
00:38:08,790 --> 00:38:12,540
And this this comes up much more frequently than you think it does.

333
00:38:12,540 --> 00:38:14,700
So positivity seems like sort of a trivial one.

334
00:38:14,910 --> 00:38:24,660
And then it turns out to actually be hard to meet, especially in social science situations, but also sometimes in medical situations.

335
00:38:26,250 --> 00:38:28,590
All right. So this brings us to the idea of the target trial.

336
00:38:28,800 --> 00:38:36,000
So Hernan and Robbins are textbook authors, argue that to identify a causal effect in observational data,

337
00:38:36,390 --> 00:38:41,400
you want to have in mind a target trial that would identify the same parameter if it was feasible, right?

338
00:38:41,400 --> 00:38:45,030
So it might be impossible to do a trial for the thing that you're thinking about.

339
00:38:45,030 --> 00:38:53,219
It's in ethical, unethical or impractical, etc. But you have to have something that you can imagine doing right.

340
00:38:53,220 --> 00:38:55,170
If there's no trial that you can imagine doing,

341
00:38:55,380 --> 00:39:05,910
it kind of tells you that you're like maybe violating one of those assumptions that are about the specificity of your of your intervention.

342
00:39:06,360 --> 00:39:10,610
So. They're really pushing us to.

343
00:39:11,780 --> 00:39:19,350
Clearly specify a hypothetical intervention so that we have that satisfied assumption.

344
00:39:19,370 --> 00:39:22,840
Satisfied. However. Right.

345
00:39:22,850 --> 00:39:26,290
This intervention doesn't necessarily need to be possible.

346
00:39:26,300 --> 00:39:30,290
Right. So either it could be something that like we don't even know how to do that.

347
00:39:30,290 --> 00:39:35,449
Right. So gene therapy is not gene therapy is sort of becoming something that we do.

348
00:39:35,450 --> 00:39:40,130
But currently you can't just like take an embryo and change a single base pair.

349
00:39:40,280 --> 00:39:47,330
But I can still talk about what is the causal effect of changing this base pair on, say, your.

350
00:39:48,670 --> 00:39:56,200
LDL cholesterol levels when you're 50. That still makes sense because I can imagine having some technology that flips that base pair.

351
00:39:58,090 --> 00:40:01,480
Okay. So we're going to do like a group discussion where we're.

352
00:40:02,590 --> 00:40:05,020
Don't go into pairs, but we'll just discuss together.

353
00:40:05,020 --> 00:40:13,420
So I think about the question, does getting a college degree increase earnings and then try to think about.

354
00:40:15,010 --> 00:40:18,430
How would I make a target trial out of this question?

355
00:40:18,610 --> 00:40:25,749
And do I need to change anything about this question in order to make it something that conforms to this target trial idea?

356
00:40:25,750 --> 00:40:32,920
And think about like your assumptions here. Right? So we're going to look. It does help to like tick through the assumptions and say, like what?

357
00:40:33,920 --> 00:40:37,040
What would I need to do to satisfy that in a trial?

358
00:40:37,110 --> 00:40:38,780
Maybe take some time and think.

359
00:40:38,780 --> 00:40:46,160
Or if you have ideas, please raise your hand and share them things that you're thinking about and take a couple of minutes and think.

360
00:40:51,700 --> 00:40:54,819
We can also start like. Brainstorming, right?

361
00:40:54,820 --> 00:41:03,340
So the first thing that you come up with doesn't necessarily have to be the last thing. So, again, it is possible to randomize education in reality.

362
00:41:03,460 --> 00:41:06,730
Mm hmm. It is begun. You do that? Oh, no.

363
00:41:06,880 --> 00:41:09,880
Right. So you. I don't think that you could do a trial of this, right?

364
00:41:09,910 --> 00:41:13,180
You can't tell someone they have to go to college or not.

365
00:41:13,300 --> 00:41:19,630
But remember, our target trial didn't recur, didn't require that we can do it.

366
00:41:20,110 --> 00:41:28,660
It just requires that we can imagine it. So if you can imagine, you have godlike powers and no ethical concerns.

367
00:41:29,290 --> 00:41:39,519
What trial would you do? To test this question, does a college degree increase your answer experiment where some people get yes,

368
00:41:39,520 --> 00:41:50,140
get lucky and then go to college and some people are unfortunately getting lucky and they didn't go to school and we compendia.

369
00:41:50,350 --> 00:41:54,130
Yeah. I'm going to encourage you to use your godlike powers more.

370
00:41:54,380 --> 00:41:58,200
Oh, yeah. Because we're not going to do this.

371
00:41:58,200 --> 00:42:02,830
So you don't have to worry about hurting the imaginary people that you would never do this to.

372
00:42:03,340 --> 00:42:08,229
Also, I don't think I can afford to send a randomized set of people to college.

373
00:42:08,230 --> 00:42:11,530
Right. Right.

374
00:42:11,530 --> 00:42:13,030
So we're kind of getting that right.

375
00:42:13,030 --> 00:42:18,720
So we have this idea of like, okay, we have to randomize some people to go to college and some people not to go to college.

376
00:42:18,730 --> 00:42:22,080
That's like part of this. How? How would you do that?

377
00:42:22,110 --> 00:42:26,520
Who would you randomize? How would you. And what do you mean by.

378
00:42:27,710 --> 00:42:30,860
Go to college. What happens to them after that? What college do they go to?

379
00:42:34,350 --> 00:42:36,860
Everyone goes to Harvard. Everyone goes to Harvard. Okay.

380
00:42:36,870 --> 00:42:44,670
So Harvard now has like many satellites and okay, so everyone is going to get not just a college education, but a Harvard education.

381
00:42:44,880 --> 00:42:48,510
This is Andy's trial. Other people can have have different trials.

382
00:42:48,870 --> 00:42:52,410
So some people go to Harvard and then what happens to the other people?

383
00:42:56,320 --> 00:43:02,680
They graduate high school and they go get jobs. They do like whatever they would have done otherwise.

384
00:43:03,810 --> 00:43:07,560
As long as that's not going, as long as it's not going to college.

385
00:43:07,690 --> 00:43:10,800
Okay. But people are dropping out.

386
00:43:11,310 --> 00:43:16,650
Okay. So the people who went to Harvard, they're required to finish going to Harvard.

387
00:43:16,860 --> 00:43:20,430
We don't let them drop out. But like I said, I'm finishing.

388
00:43:21,180 --> 00:43:25,740
Okay. And how are they going to pay for going to Harvard?

389
00:43:26,600 --> 00:43:30,300
Oh, it's free tuition. They're very generous. I see.

390
00:43:30,480 --> 00:43:35,830
So now in our trial, we have. In our trial, we have free tuition.

391
00:43:36,340 --> 00:43:39,670
But when? Let's say.

392
00:43:40,720 --> 00:43:47,860
Andy walks into a college admissions, let's say Andy in high school goes to the college admissions advisor,

393
00:43:48,040 --> 00:43:52,150
and they say, Andy, if you go to college, your earnings will be higher.

394
00:43:53,050 --> 00:43:57,040
Andy's tuition isn't free. Right? So now our trial doesn't quite look.

395
00:43:57,880 --> 00:44:03,790
It doesn't quite look like the world that we had in mind when some well-meaning soul gave you that advice.

396
00:44:03,790 --> 00:44:08,420
Right. All right, so what else can we do?

397
00:44:12,140 --> 00:44:16,220
Force them to take on the average amount of debt that you would normally get from college, I guess.

398
00:44:16,310 --> 00:44:19,610
Uh huh. Right. So kind of there's an issue of.

399
00:44:22,800 --> 00:44:27,060
I guess in economics they call this. I'll come to you in a minute.

400
00:44:27,930 --> 00:44:31,170
There's an opportunity cost rate. So each person.

401
00:44:33,110 --> 00:44:41,570
Let's say Hanyu is thinking about whether to go to college and one possibility is go to college and, like, pay a bunch of money.

402
00:44:42,520 --> 00:44:46,389
Or go into debt. Right. So some people are going to pay a bunch of money.

403
00:44:46,390 --> 00:44:49,600
Some people are going to pay a bunch of money, but their parents have gobs of money.

404
00:44:49,600 --> 00:44:52,149
And so it's a tiny drop in their in their bucket.

405
00:44:52,150 --> 00:44:58,030
Some people are going to, you know, clear out their bank accounts to go to college and then other people are going to take out a bunch of loans.

406
00:44:58,030 --> 00:45:01,450
Right. Those are like different financial choices that might influence.

407
00:45:02,810 --> 00:45:08,150
This question of what are your later earnings? And then on the other side of not going to college.

408
00:45:08,150 --> 00:45:14,030
Right. Some people maybe have skills and they're going to get a job right away doing something.

409
00:45:15,070 --> 00:45:24,630
That pays well and some other people aren't right. And the decisions people make naturally might relate to those counter.

410
00:45:24,680 --> 00:45:29,620
Like people might have a sense of what their counterfactuals are, right? Like back to high school, Andy.

411
00:45:29,620 --> 00:45:34,900
Andy might be able to imagine what is my life going to be like if I don't go to college?

412
00:45:35,140 --> 00:45:43,300
What if it is? What will it be like if I do go to college? And Amy might make a decision based on that about whether to go to college.

413
00:45:44,200 --> 00:45:48,790
So now we have I think we're sort of running into like. This.

414
00:45:50,420 --> 00:45:56,300
Multiple versions of treatment problem, right? So, like, if I randomize someone to go to college.

415
00:45:58,430 --> 00:46:05,480
They might have choices about how to do that. So do I want to specify in the trial what constrains their choices?

416
00:46:06,140 --> 00:46:11,830
T Y You add? A Yeah, so. How do we control or control them?

417
00:46:11,830 --> 00:46:21,700
Because the people go to God and innate ability that will make them successful and not even without college degrees.

418
00:46:21,850 --> 00:46:29,290
Right. They have to get admitted into Harvard. Right. So in in Andy's trial, Andy, I think you're overwriting the admissions committee, is that right?

419
00:46:29,440 --> 00:46:33,250
Oh, yeah. Okay. Every university is Harvard. Okay?

420
00:46:33,640 --> 00:46:40,780
Every university every university offers the same. Is it just quality of education or like also flavor?

421
00:46:40,780 --> 00:46:47,540
Right. Someone who goes to like. Caltech, I would argue, is not.

422
00:46:49,040 --> 00:46:53,390
They've got a different like they're going to be like an engineer. This is Harvard having an engineering program.

423
00:46:53,630 --> 00:46:57,350
Now we're getting beat by a Canadian.

424
00:46:57,590 --> 00:47:02,750
Maybe I'll change the central Florida's. And it's like the largest university I've seen as the OC.

425
00:47:03,950 --> 00:47:14,360
Any other ideas about how you would design this trial? Who would want to randomly assign majors.

426
00:47:15,920 --> 00:47:22,370
So I would argue that you could do that trial rate of like you can imagine doing a trial of.

427
00:47:23,730 --> 00:47:27,900
Randomly assigning everyone to go to Central Florida, assigning majors.

428
00:47:28,230 --> 00:47:31,260
But at that point, it doesn't look like the world that we're considering.

429
00:47:31,260 --> 00:47:38,280
Right. It no longer looks like. Like Dan's counterfactuals of go to college or not go to college.

430
00:47:39,580 --> 00:47:45,220
Dan never has available the counterfactual of go to a uniform central Florida,

431
00:47:45,490 --> 00:47:49,930
which I'm assigned a random major which maybe you would choose not to do.

432
00:47:49,930 --> 00:47:50,530
I'm not sure.

433
00:47:52,900 --> 00:48:03,700
So now our our intervention is so far from the world that we're considering that it maybe is not measuring the thing that we want to measure.

434
00:48:03,700 --> 00:48:07,180
Right. So I think the.

435
00:48:08,710 --> 00:48:14,980
Kind of where I'm going with this is it's actually, I think, very hard to design a trial that looks like.

436
00:48:16,800 --> 00:48:21,270
When let's say someone gives this advice of getting a college degree, it's going to increase your earnings.

437
00:48:21,450 --> 00:48:30,330
You're imagining like your counterfactuals. It's hard to design a trial that actually mimics that, that choice.

438
00:48:30,540 --> 00:48:36,850
Hey, why you at an all ages one study? You'd be in an actually in study.

439
00:48:37,560 --> 00:48:43,540
Exactly. Yeah. The dynamic matters that you use in Vietnam.

440
00:48:44,490 --> 00:48:49,260
Right. And so there are like people I've tried to study this several ways.

441
00:48:49,260 --> 00:48:52,470
One of them is that you can use like.

442
00:48:53,500 --> 00:49:00,040
Cutoffs, right? So in some places you get into college or you get to go to some particular college.

443
00:49:00,040 --> 00:49:03,369
If you're above a cutoff on a test and you don't if you're below the cutoff.

444
00:49:03,370 --> 00:49:11,859
And so you compare people who are like just above and just below under the idea that like those people are comparable.

445
00:49:11,860 --> 00:49:16,960
Right? That like whether you got a 503 or 497 on your test is like random chance.

446
00:49:18,670 --> 00:49:24,489
And that does answer this question, but it only answers this question for those people that were in that like boundary region.

447
00:49:24,490 --> 00:49:29,590
Right. It doesn't answer the question for the people who got a 900 or the people who got a 20.

448
00:49:32,050 --> 00:49:35,080
So there's sort of I think there's limits here.

449
00:49:35,080 --> 00:49:38,920
And I would argue like this seems like a straightforward counterfactual to me.

450
00:49:38,920 --> 00:49:48,479
Like I think probably heard people say this or at least think. If you're in, people have given out advice, maybe not that specific advice,

451
00:49:48,480 --> 00:49:52,889
but they they hand out this advice that are sort of proposing a counterfactual.

452
00:49:52,890 --> 00:49:57,930
And it turns out it's actually hard to imagine a trial that that sits well with this.

453
00:50:01,530 --> 00:50:08,969
Okay. So that's one side is can we even imagine a trial that measures this proposal or do

454
00:50:08,970 --> 00:50:13,770
we need to modify our question so that it is actually something that's testable?

455
00:50:13,770 --> 00:50:21,329
So I think I would argue that as phrased right, does getting a college degree increase earnings is not testable?

456
00:50:21,330 --> 00:50:28,260
But we could modify that by saying, does getting a college degree increase earnings for people in a particular group?

457
00:50:28,260 --> 00:50:31,290
Maybe, right. Like maybe people at that like boundary of the.

458
00:50:34,790 --> 00:50:38,989
Of this score distribution or for people who have like particular financial choices.

459
00:50:38,990 --> 00:50:47,690
Right. We might be able to say, like, if you're choosing between these specific choices, then we can imagine casting this question.

460
00:50:50,210 --> 00:50:59,840
Okay. So then there's this other issue of can we even think about counterfactuals for something that is like a non modifiable exposure?

461
00:51:00,230 --> 00:51:03,440
So. What do we mean by non modifiable exposure?

462
00:51:03,510 --> 00:51:08,069
So there are some features which I'll say like my best examples of these are race,

463
00:51:08,070 --> 00:51:15,209
sex and BMI that are really hard to imagine modifying or it's hard to imagine specifically

464
00:51:15,210 --> 00:51:18,960
how you're going to modify that and what features of that you're going to change.

465
00:51:19,260 --> 00:51:28,589
So a reason why this is. Hard to imagine is that these features are very multifaceted, right.

466
00:51:28,590 --> 00:51:37,650
So they describe whole groups of both like physical features and social features and like life history features.

467
00:51:38,640 --> 00:51:46,950
And it's not necessarily clear when you say is there an effect of sex on X or gender on X?

468
00:51:50,050 --> 00:51:56,000
Which of those features do you mean? Right. Is it physical features?

469
00:51:56,020 --> 00:51:59,810
Is it social discrimination? Is it like history?

470
00:52:01,210 --> 00:52:05,620
And because it's hard to if you haven't nailed down what you mean by that,

471
00:52:05,920 --> 00:52:09,880
it's hard to imagine, like specifically what the intervention is going to be here.

472
00:52:10,270 --> 00:52:17,320
That makes sense. There is a workaround that's like very popular, so it's good to know.

473
00:52:17,590 --> 00:52:23,760
So in the case of discrimination studies, so we want to know, is there housing discrimination?

474
00:52:23,770 --> 00:52:27,070
Is there discrimination in who gets hired?

475
00:52:27,940 --> 00:52:33,339
There's a very convenient workaround, which is that rather than thinking about intervening on the applicant,

476
00:52:33,340 --> 00:52:42,370
let's say we're looking at housing discrimination. I don't imagine like magically turning this apartment applicant from one race into the other.

477
00:52:42,610 --> 00:52:46,450
I imagine intervening on the perceptions of the person giving out the apartment.

478
00:52:46,720 --> 00:52:50,230
So do they perceive this person to be black or white or something else?

479
00:52:51,100 --> 00:52:55,270
So the these studies have been done actually in real life.

480
00:52:55,270 --> 00:53:00,130
And the way they often do these is by like sending around applications that are the same.

481
00:53:01,110 --> 00:53:07,080
The same, but have different names. So either like male or female coded names or like different race coded names.

482
00:53:07,530 --> 00:53:14,939
I mean, housing discrimination law actually send human actors to apartments who will pretend

483
00:53:14,940 --> 00:53:18,719
to want an apartment and they have like a specific script that they go through.

484
00:53:18,720 --> 00:53:22,860
So they know that like a white couple in a black couple said the same thing and

485
00:53:22,860 --> 00:53:26,700
then they just compare the rates at which those people were given the apartment.

486
00:53:26,700 --> 00:53:36,210
So that is sort of a convenient workaround to an otherwise like sort of the sticky philosophical and scientific issue.

487
00:53:36,870 --> 00:53:46,649
We're going to come back to specifically this idea of race and sex and gender and BMI later in the semester.

488
00:53:46,650 --> 00:53:53,280
And we'll take like a break from theory and talk about philosophy a little bit and execution of studies.

489
00:53:53,850 --> 00:54:00,839
For now, I will just leave it at I'm like Hernan and Robyn's advice to think about the target trial.

490
00:54:00,840 --> 00:54:06,180
So I think that that's like a good place to start if you're trying to understand what actually would the counterfactual here be.

491
00:54:06,960 --> 00:54:12,810
And so that is that is the end of. I realize I called these lectures and none of them fitness single day.

492
00:54:12,810 --> 00:54:15,710
So please don't be intimidated when you see like that.

493
00:54:15,720 --> 00:54:21,270
There's 70 some slides in a lecture that's just those topics together and talk about them for a period of time.

494
00:54:21,690 --> 00:54:26,370
Are there questions before we move on to. Or call a lecture to.

495
00:54:30,240 --> 00:54:38,290
Okey dokey. Oh, well, we're switching.

496
00:54:38,310 --> 00:54:50,430
I'll just note. So if you didn't notice the slides are hosted on GitHub and if you just want to see the like list of slides there, they're here.

497
00:54:51,300 --> 00:54:54,750
So you can maybe put a link on the canvas page or something.

498
00:54:56,130 --> 00:54:59,610
This doesn't have anything that you won't see on canvas, but these will be here.

499
00:55:03,110 --> 00:55:06,040
I can't guarantee that they'll be there, like if I'm not alive.

500
00:55:06,050 --> 00:55:11,450
But I will maintain my GitHub account so that you should be able to find them in the future if you

501
00:55:11,450 --> 00:55:17,300
want to come back and you can print them if you want to with the print function in your browser.

502
00:55:19,370 --> 00:55:22,580
Okay. Dags and confounding actually.

503
00:55:25,190 --> 00:55:28,940
All right, so there's four ish parts to this lecture.

504
00:55:29,270 --> 00:55:33,170
I have, like, 20 minutes, so we'll get through that first one maybe.

505
00:55:34,580 --> 00:55:39,410
So we're going to start with how do I represent causal relationships in a directory, a cyclic graph?

506
00:55:40,610 --> 00:55:45,200
How do I connect dags to counterfactuals? In a couple of different ways?

507
00:55:45,200 --> 00:55:49,999
We're going to see like three different ways to connect dags to counterfactuals, talk about the backdoor criterion,

508
00:55:50,000 --> 00:55:56,780
which is this like very powerful way for looking at a graph and figuring out conditional exchange ability.

509
00:55:57,260 --> 00:56:00,830
And then we'll talk about single world intervention graphs which are sort of like nice,

510
00:56:01,760 --> 00:56:06,980
I'll call it like extension of dags and kind of an alternative to the backdoor criterion.

511
00:56:11,150 --> 00:56:14,210
Okay. Representing Causal Relationships Index.

512
00:56:15,680 --> 00:56:19,940
So the idea here is pretty straightforward and I think it's fairly intuitive to you.

513
00:56:20,690 --> 00:56:24,799
The idea is you take all of your variables and you make them known.

514
00:56:24,800 --> 00:56:32,810
So you write them down on a piece of paper and then you connect them with arrows when you think there's a causal effect between between variables.

515
00:56:33,380 --> 00:56:37,880
So in this case, our edges are the arrows and they represent direct causal effects.

516
00:56:37,880 --> 00:56:45,860
So like not mediated by other variables in the graph and, and an absence of an edge is informative, right?

517
00:56:45,860 --> 00:56:49,310
So we only have edges or not edges and there's no like.

518
00:56:50,800 --> 00:56:54,970
In dogs, there's not really ambivalence. You can't say.

519
00:56:56,830 --> 00:57:04,710
I don't know. It's either there or it's not there. You can draw an arrow and then say this effect might be zero.

520
00:57:04,720 --> 00:57:12,130
I'm testing this effect, but generally an absence of an edge means there's no causal effect there.

521
00:57:13,060 --> 00:57:18,129
So a couple examples. The one on the left is like the simplest dag that we could have.

522
00:57:18,130 --> 00:57:22,780
We have exposure A and it causes outcome Y and there's nothing else going on.

523
00:57:23,020 --> 00:57:28,929
So we also mean when we draw this dag on the left, I also mean that there's no confounding, right?

524
00:57:28,930 --> 00:57:35,830
So the dag on the right, it causes y and there's a common cause l that causes a and y.

525
00:57:36,070 --> 00:57:39,850
This dag is like fundamentally different from the dag on the left.

526
00:57:39,940 --> 00:57:45,639
The dag on the left. Both says it causes y and it says. And there's nothing else going on, right?

527
00:57:45,640 --> 00:57:50,130
There's no L. Okay.

528
00:57:50,170 --> 00:57:53,860
So why would you put your causal effects into a graph?

529
00:57:54,310 --> 00:57:58,430
So one reason is that it's just a convenient way to think about the world.

530
00:57:58,450 --> 00:58:04,390
Right. So they're pretty intuitive I think, even to non statisticians.

531
00:58:04,420 --> 00:58:09,190
I think like lots of scientists already, like draw dogs in their life.

532
00:58:09,190 --> 00:58:13,509
Like if you're working on enzymes or something and you say this enzyme interacts with that enzyme,

533
00:58:13,510 --> 00:58:16,810
you draw an arrow, you sort of mean the same thing that we mean in a dog.

534
00:58:18,220 --> 00:58:21,070
This makes it easy to communicate across domains.

535
00:58:21,070 --> 00:58:27,820
So I've had definitely times when I sat down with a collaborator and I said, This is how I'm thinking about the system.

536
00:58:27,820 --> 00:58:32,230
I've drawn this picture. I think that this causes this and this doesn't cause that.

537
00:58:32,440 --> 00:58:35,980
And they say, Oh, yes, that's correct, or No, that's not how we're thinking about it.

538
00:58:37,990 --> 00:58:42,430
And then a nice thing is, so we have this way to take our scientific knowledge,

539
00:58:42,700 --> 00:58:48,700
put it into a picture, and then the nice extension is that if I make some assumptions,

540
00:58:48,940 --> 00:58:55,239
I can then take this picture and use it to solve a math problem so I can use properties of the picture that I've

541
00:58:55,240 --> 00:59:00,460
just used to like represent my knowledge about the world and actually make like mathematical conclusions from it.

542
00:59:00,730 --> 00:59:04,300
So that's sort of the the golden promise of Dex.

543
00:59:05,360 --> 00:59:10,280
All right. We're going to start with some definitions we haven't even really gotten to.

544
00:59:10,330 --> 00:59:19,180
There's no counterfactuals here. We're just drawing a graph. So a graph is a set V of nodes e edges.

545
00:59:19,450 --> 00:59:23,020
So nodes also vertices.

546
00:59:23,140 --> 00:59:35,560
Same thing. I might use both. So let's say we have j vertices and then we have a capital K edges and edges, just an ordered pair of vertices.

547
00:59:35,560 --> 00:59:43,540
Right. So the first one that starts, the second one it ends. A graph generally can be directed or undirected.

548
00:59:43,810 --> 00:59:46,960
Our graphs are always directed, so we're working with directed graphs.

549
00:59:49,560 --> 00:59:52,950
Two nodes are called adjacent if they're connected by an edge.

550
00:59:54,270 --> 00:59:56,670
Doesn't matter what direction the edge goes, they're adjacent.

551
00:59:58,020 --> 01:00:09,300
If the edge is directed, the node at the tip of the arrow at the point of the arrow is the child and the node at the butt of the arrow is the parent.

552
01:00:09,360 --> 01:00:21,340
Right? So the parent causes the child. A path is a sequence of edges in which each edge contains one node from the previous edge, a directed path.

553
01:00:21,820 --> 01:00:29,580
All of the edges go the same direction. So in this graph, if I want to find paths from A to Y, there's two.

554
01:00:29,590 --> 01:00:33,760
I can go a, l y or just a y.

555
01:00:34,960 --> 01:00:38,260
This is a directed path. This is not a directed path.

556
01:00:40,120 --> 01:00:44,050
There's a directed path from all the Y right questions here.

557
01:00:47,140 --> 01:00:50,860
I think most of the graph definitions are like powerfully intuitive.

558
01:00:50,860 --> 01:00:57,790
Nothing too sneaky is going to happen. So if there's no paths between two nodes, they are called disconnected.

559
01:00:58,510 --> 01:01:04,220
Otherwise they're connected. So. This is a fully connected graph.

560
01:01:04,910 --> 01:01:15,340
There's no disconnected pairs there. Node J is a descendant of Node K if there's a directed path from J to VK.

561
01:01:18,970 --> 01:01:23,740
If a graph contains known cycles, it is a cyclic that should be hopefully.

562
01:01:26,580 --> 01:01:28,889
Hopefully intuitive from the name.

563
01:01:28,890 --> 01:01:35,790
And then so a directed acyclic graph is a directed graph with no cycles, and that's going to be what we're working with.

564
01:01:37,170 --> 01:01:44,399
All right. So we're going to do a little exercise, so you'll want to write for this.

565
01:01:44,400 --> 01:01:47,670
And I'd say, let's do it in pairs because it's more fun.

566
01:01:48,510 --> 01:01:52,080
You can either grab the same pair that you had last time or a different pair fact,

567
01:01:52,350 --> 01:01:57,270
grab a different pair, work with the person behind you instead of next to you. All right.

568
01:01:57,270 --> 01:02:01,200
So in this story, we have two treatments for a disease.

569
01:02:01,200 --> 01:02:06,570
So this sounds like similar to the surgery example. So we can have a equals one equals zero.

570
01:02:08,940 --> 01:02:12,200
Equals one is more effective, but it has more side effects.

571
01:02:12,210 --> 01:02:20,850
So doctors like to give equal zero to patients who are older or they have more mild disease patients outcome,

572
01:02:20,970 --> 01:02:23,910
which is do they go into remission or do they not go into remission,

573
01:02:24,180 --> 01:02:32,340
is going to be affected by the initial severity of their disease treatment and treatment adherence.

574
01:02:32,820 --> 01:02:40,560
And then conditional on everything else. Let's say, ed, age has no effect on patient outcome or disease severity and.

575
01:02:41,990 --> 01:02:47,389
And then work with your neighbor to put these variables into a dag.

576
01:02:47,390 --> 01:02:53,960
If you want to add more variables to the DAG, you can. There's more than one right answer to this.

577
01:02:54,350 --> 01:03:03,380
I will say. I put my answer on Slide ten, so just don't flipped upside ten so that you had a chance to do it for yourself.

578
01:03:04,340 --> 01:03:10,880
And as you draw the dog, think about like, you know, why each area we're not each arrow, right?

579
01:03:11,300 --> 01:03:22,880
Grab a partner and if anyone needs paper, raise your hand and I will bring you paper and you can go underground.

580
01:03:27,530 --> 01:03:38,020
And the healthcare industry, I guarantee you had worked in the trio again.

581
01:03:40,690 --> 01:03:44,390
Okay, so why are you here?

582
01:03:45,620 --> 01:04:06,550
When I first heard about people that we think the reason we had our initial disease free and then we have our treatment.

583
01:04:07,420 --> 01:04:12,140
And so I can confirm that.

584
01:04:12,800 --> 01:04:24,200
Where do we say I'm very familiar with the impact of this doesn't interest me actually.

585
01:04:24,200 --> 01:04:31,340
No. Yeah, yeah, yeah.

586
01:04:31,730 --> 01:04:50,040
And you also have the fact that no one has tried to know risks.

587
01:04:50,360 --> 01:04:53,419
What do you mean by appearances?

588
01:04:53,420 --> 01:04:54,920
Like whether you actually took the treatment?

589
01:04:54,920 --> 01:05:04,520
Your daughter has more side effects and then you're inside the home because of Katrina was worse than regular school.

590
01:05:04,520 --> 01:05:08,750
Or you're trying to say that this is more side effects.

591
01:05:09,290 --> 01:05:18,610
Yeah. So let's see. Here in Brooklyn, patient out there is no security strategy.

592
01:05:19,430 --> 01:05:48,470
You know, I think these units that they say no right now, they are now beginning to introduce you to a patient.

593
01:05:52,640 --> 01:06:07,990
You are the kind that this is your whole life is so many doctors.

594
01:06:11,390 --> 01:06:34,160
So I what I wanted to hear that should be older hover over an area I have not seen anything at all because we have a great N.A.,

595
01:06:38,300 --> 01:06:55,760
N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A.,

596
01:06:58,040 --> 01:07:29,480
N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A.,

597
01:07:33,020 --> 01:07:52,760
N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A., N.A. Refreshing.

598
01:07:57,520 --> 01:08:16,800
Do you think there are a lot of young people throughout the world that are angry with a particular.

599
01:08:17,520 --> 01:08:26,910
I would hope so. There may be something like that.

600
01:08:27,550 --> 01:08:36,420
That's why I really do like two and a half more minutes.

601
01:08:37,020 --> 01:08:43,900
So I feel like I think that we could be like, okay.

602
01:08:45,020 --> 01:08:51,360
And also we want to react later.

603
01:08:52,860 --> 01:09:03,980
Yeah, because it just sounds interesting and I don't know how I figure it out.

604
01:09:04,460 --> 01:09:16,870
This is about the direction I would suggest is take that strength and just go with it.

605
01:09:18,270 --> 01:09:26,940
And although, you know, yeah, I think I was like the best we can do on television,

606
01:09:26,970 --> 01:09:39,710
you know, I don't really have a plan, so it's only a quiet, you know, careful.

607
01:09:39,780 --> 01:09:46,910
And so I was the only one who was very involved.

608
01:09:49,770 --> 01:09:57,660
Okay. Do people feel approximately resolved or would you rather interview one more time?

609
01:09:59,610 --> 01:10:03,299
Okay. So I can't I have no idea how to get this project to go up.

610
01:10:03,300 --> 01:10:10,330
So I'm going to go right on the whiteboard in the back, which is also then I can see, you know.

611
01:10:12,390 --> 01:10:22,890
So I think I will just write each of these variables up and then you guys can tell me what areas are consensus and what areas we are painting.

612
01:10:24,750 --> 01:10:36,390
We have our contractor use the call line. We have treatments, team treatment, adherence.

613
01:10:37,170 --> 01:10:45,900
I call that a D adherence and severity and initial disease severity.

614
01:10:46,130 --> 01:10:57,060
Right. Give it to me.

615
01:11:00,090 --> 01:11:06,930
Okay, so these are our variables. They range them approximately in time order to help us.

616
01:11:08,610 --> 01:11:13,510
Somebody shout out an arrow that actually draws. Age treatment.

617
01:11:15,750 --> 01:11:23,040
And how come some doctors are going to be more akin to not assign treatment based off of it?

618
01:11:23,550 --> 01:11:30,690
Yeah, this is something that the actual doctors were going to gauge when they decided to treat a different group.

619
01:11:33,070 --> 01:11:39,140
And that's something that we've learned from the story treatment outcome.

620
01:11:40,350 --> 01:11:43,800
And so this is the one that we want to know about.

621
01:11:43,820 --> 01:11:50,980
Sometimes I like to talk about the customer. So we're going to ask, how come?

622
01:11:56,010 --> 01:12:12,700
How come? Well, like the initial days like this, it is pretty severe to know and it makes sense.

623
01:12:12,720 --> 01:12:17,340
Great. If your disease is worse at the beginning, you might have worse outcomes regardless of what we.

624
01:12:17,550 --> 01:12:23,070
Right. What else? Treatment is the treatment to adherence.

625
01:12:24,660 --> 01:12:27,989
Treatment home. The treatment is costly along with much truth.

626
01:12:27,990 --> 01:12:32,700
It all ends with a new drug and we have all potential danger.

627
01:12:33,930 --> 01:12:38,700
Right? So or we have this fact that like one of the treatments has more side effects, right?

628
01:12:38,700 --> 01:12:43,350
You might not be as good about taking your treatment as giving a side effect.

629
01:12:45,060 --> 01:13:02,440
What else appears to. In the age groups with maybe younger children, they don't quite follow what's best for us.

630
01:13:02,680 --> 01:13:07,020
Yeah, we didn't. And that's that sort of thing that we don't actually really get in our story.

631
01:13:07,030 --> 01:13:11,260
What is your graduation rate? Is it adults?

632
01:13:11,530 --> 01:13:18,820
And when we say older women over 80, really, do we mean children?

633
01:13:19,180 --> 01:13:23,920
Right. And you can imagine age affecting appearance on either of those scales.

634
01:13:24,820 --> 01:13:31,600
And then how about adherence to why way we have that era? I believe that to be true.

635
01:13:32,890 --> 01:13:39,890
Yeah. And the other Arabs. Severity was.

636
01:13:41,650 --> 01:13:47,230
This is that every day on Wall Street, we may have an idea.

637
01:13:47,300 --> 01:13:51,410
They know that somehow treatment will overcome the disease on their second

638
01:13:51,460 --> 01:13:55,660
trip seconds they treat them and they have accepted more adventurous people.

639
01:13:55,670 --> 01:13:59,650
Hospital makes sense. How do you people put that arrow on their day?

640
01:14:02,460 --> 01:14:07,080
Gregory. So, like I said, there's, like, plausible, right?

641
01:14:07,110 --> 01:14:14,250
And the great secret. Depends on what the disease is, right? You might think that this is the case, or like your collaborators might think.

642
01:14:14,700 --> 01:14:19,410
We don't really see that true. And the other areas.

643
01:14:21,810 --> 01:14:32,500
And a question we have. Some people say you have to do the left or right or the bottom three times or in the very important time is a big deal.

644
01:14:32,500 --> 01:14:38,050
And we'll talk about time in like a couple slides.

645
01:14:38,440 --> 01:14:46,270
I think it is convenient to order in time because it helps you not send arrows backwards in time.

646
01:14:46,700 --> 01:14:55,269
Right. So you wouldn't say treatment affects disease severity because I put disease severity here to remember that I mean,

647
01:14:55,270 --> 01:14:59,200
pre treatment and I can even label that for treatment severity.

648
01:14:59,590 --> 01:15:07,480
But ordering them in time helps you have like a slightly for your graph and helps you not not be.

649
01:15:10,430 --> 01:15:13,750
Accidentally time and reality.

650
01:15:13,850 --> 01:15:23,120
The question, I imagine that any other year was actually a question how do you interpret the interaction?

651
01:15:23,540 --> 01:15:25,130
Rather because, for example,

652
01:15:25,280 --> 01:15:36,169
I'm treating there are only four other people with some new treatment for my treatment and who accept not the direction I want to go.

653
01:15:36,170 --> 01:15:42,290
How should I work? Yeah, that's a great question. The short answer is there's not a convenient way to show, right?

654
01:15:42,290 --> 01:15:46,879
So we know that in our graph, our graph says treatment affect outcome.

655
01:15:46,880 --> 01:15:51,530
And what are the two things things that might your act treatment and treatment page.

656
01:15:51,610 --> 01:15:55,250
Okay so let's say treatment and age offense interacts,

657
01:15:55,940 --> 01:16:03,319
then we would have an arrow from age ten to Y and we'd have an arrow from treatment to why we can't see that.

658
01:16:03,320 --> 01:16:07,790
That is an interaction. We'll talk more about what the interactions are.

659
01:16:07,940 --> 01:16:16,819
I think in lecture three and it turns out that like when an interaction is certain, certain matters, what scale you measure stuff on, right?

660
01:16:16,820 --> 01:16:20,930
You can have an interaction on the linear scale, but not on the scale.

661
01:16:21,370 --> 01:16:31,220
And there's no convenient way if you're thinking specifically about a linear model and no convenient way to put an interaction on a die.

662
01:16:31,580 --> 01:16:40,160
So we're going to stay tuned for a lecture room which is not actually on the grid there like any other arrows.

663
01:16:41,360 --> 01:16:47,900
Okay, so you draw on arrow from each of those is very dependent or not.

664
01:16:48,500 --> 01:16:54,200
Yeah. And it could be right. Like it might be either being older through disease, worse or better, right?

665
01:16:54,200 --> 01:16:57,919
That's like with chicken pox is like that, right?

666
01:16:57,920 --> 01:17:02,090
If you get the chicken pox when you're 30, it's horrible.

667
01:17:02,090 --> 01:17:07,670
But if you get it when you're in friendships, I think unpleasant but not as bad.

668
01:17:08,600 --> 01:17:15,400
Right. So we could imagine that there was one here. Sometimes I'll put out an arrow, then I'm thinking about something to bring a arrow her age to.

669
01:17:15,410 --> 01:17:22,610
Why? Yeah.

670
01:17:22,830 --> 01:17:27,990
This one. I told you specifically not to put conditioner with everything else it has no effect on.

671
01:17:28,650 --> 01:17:33,540
So it has no effect.

672
01:17:36,000 --> 01:17:41,129
So let me out of here. All right.

673
01:17:41,130 --> 01:17:45,990
So I think that I think that's a pretty good graph. I think if you draw across, it doesn't look exactly like this.

674
01:17:45,990 --> 01:17:54,780
It's also probably pretty good, right. So there is like information missing in the story about like should I draw error from seems very triggering.

675
01:17:54,780 --> 01:18:00,359
I'm not sure I draw an arrow. That might be the biggest question.

676
01:18:00,360 --> 01:18:04,770
Arrow. Right. Um. Maybe age to appearance moving.

677
01:18:04,870 --> 01:18:15,030
Also wonder if we should draw or not. I don't think my sort of wrap up here can see what graph I drew.

678
01:18:17,670 --> 01:18:23,909
Yeah. So I tried to keep it simple for future for future use.

679
01:18:23,910 --> 01:18:35,760
So I didn't draw any of the arrows into it here. And so I didn't even draw the treatment to adherence arrow, although mostly for didactic use later.

680
01:18:36,240 --> 01:18:40,690
But I think it makes sense to draw the treatment to adherence. Zero. So this is the graph that I drew.

681
01:18:40,720 --> 01:18:45,959
We're going to work with this graph going forward in this in this lecture,

682
01:18:45,960 --> 01:18:55,830
and I'll just pick it up here on Wednesday, bring me your nametags and I will bring them back to you.

683
01:18:55,980 --> 01:19:01,170
And then, yeah, really, your group can bring me your your graph.

684
01:19:01,170 --> 01:19:04,560
I don't need everyone else's graph, but if you want to give me your graph, you can.

685
01:19:15,560 --> 01:19:18,790
Beautiful. Thank you, sir. Okay.

686
01:19:18,900 --> 01:19:25,630
Yeah. I understand how great they are.

687
01:19:35,120 --> 01:19:50,690
And the conspiracy thing I think you can hear me talk about.

688
01:19:51,330 --> 01:19:58,770
Yes. So tell me what you remember hearing in a period of American economy of North America.

