1
00:00:18,840 --> 00:00:22,499
We're a little bit late today. Hopefully you have good time.

2
00:00:22,500 --> 00:00:29,309
And we had a great homecoming game. So it was a little bit now nerve racking.

3
00:00:29,310 --> 00:00:33,090
And so finally we got away and so that's great.

4
00:00:33,090 --> 00:00:41,610
So it's time to look at your homework one and read your AC and you had a lot of great ideas.

5
00:00:41,610 --> 00:00:53,879
And so yeah, I would think that as if some of the ideas finally sort of implemented and they are the projects

6
00:00:53,880 --> 00:01:05,490
and I'll be very fascinating sort of set of projects or findings that I look forward that.

7
00:01:06,570 --> 00:01:11,520
So yeah, I will return your first homework as long as possible.

8
00:01:11,970 --> 00:01:22,590
I think that it's quite a nice have this time reading your projects like give me give myself a peace of mind.

9
00:01:22,950 --> 00:01:26,280
And so that's really the part I enjoyed the most.

10
00:01:26,580 --> 00:01:33,090
Okay. So anyway, so on to the homework.

11
00:01:33,090 --> 00:01:38,250
Number two, I, I really want to you start to look at data, okay?

12
00:01:38,400 --> 00:01:42,660
So I don't know what's the best way to process the data.

13
00:01:42,660 --> 00:01:48,450
I download two big data set from CTC Data Portal Portal.

14
00:01:48,990 --> 00:01:54,030
Why is the weekly mortality incidences?

15
00:01:54,540 --> 00:02:02,669
So I think that now because of the availability of a home testing kit, like a lot of you know,

16
00:02:02,670 --> 00:02:09,030
the confirmed cases are not accurate because it's so severely underreported.

17
00:02:09,540 --> 00:02:18,750
That's well known results so far the Omar had the of the number of deaths is relatively

18
00:02:18,990 --> 00:02:25,070
trustworthy so we can look at the the weekly number of deaths in the analysis.

19
00:02:25,080 --> 00:02:33,660
So so I download this to big data. And first I want you to sort of look at the data.

20
00:02:35,010 --> 00:02:45,780
So for example, that some of you would want to study vaccination and in this data file they have different maybe 12 or ten.

21
00:02:45,780 --> 00:02:56,850
And two variables are to to summarize the vaccination, compliance and different scenarios of vaccination and improvement in population.

22
00:02:57,660 --> 00:03:00,030
So I think that that this weekly percent of.

23
00:03:01,850 --> 00:03:08,959
One dose of vaccination worthy of vaccination would be something like I would be interested in looking at.

24
00:03:08,960 --> 00:03:18,100
So you can, you know, I'll use other types of variable about the vaccination sort of implementation scenarios in the population.

25
00:03:18,110 --> 00:03:23,530
But this are sort of to sort of variables a in the.

26
00:03:25,770 --> 00:03:32,759
But the most is that really that the most data that give you the age group because we know that older people,

27
00:03:32,760 --> 00:03:42,910
senior people tend to have higher risk of mortality or severe illness after infection.

28
00:03:42,950 --> 00:03:51,839
The college so the age is very important risk factor so that I want to use study that whether or not the age

29
00:03:51,840 --> 00:04:06,959
is a really severe risk factor in in this is associated with the death right so the mentality so that's the

30
00:04:06,960 --> 00:04:14,130
problem three I want to you on delivered the possum regression model or negative binomial regression model to

31
00:04:14,130 --> 00:04:22,440
see to assess whether or not age is a really a risk factor because we read this from newspapers all the time,

32
00:04:22,470 --> 00:04:28,350
right. I'm older people tend to have higher risk, but we don't know what I mean.

33
00:04:28,350 --> 00:04:33,870
If that's true, that only tells significance, doesn't tell you the magnitude.

34
00:04:34,320 --> 00:04:40,760
So now using the data, you can see that if they are one year older, how much additional risk people could have, right?

35
00:04:40,770 --> 00:04:46,620
So you have a little bit more precise sort of quantification of risk, right?

36
00:04:46,680 --> 00:04:49,920
Rather than just saying, oh, yes, it's a risk factor.

37
00:04:50,190 --> 00:04:53,969
So so that's something we we can use statistical model to do that.

38
00:04:53,970 --> 00:05:07,560
And also I want to use to do the first sort of analysis about the potential protection so the effectiveness of protection using vaccination.

39
00:05:08,100 --> 00:05:13,770
So on the top of the age, a, we look hard of vaccination because, you know,

40
00:05:13,770 --> 00:05:24,750
so we just do the simple you know this neither volcanism or regression model from jr m is our consider C or dependance.

41
00:05:25,290 --> 00:05:35,189
But the following project I will ask you to build upon this data and also the the models like this this analysis

42
00:05:35,190 --> 00:05:45,540
more like something you generate for you initial first you need to build a little bit more structures like this,

43
00:05:46,440 --> 00:05:52,349
you know, the hierarchy model for this dynamic sort of temple structure.

44
00:05:52,350 --> 00:06:01,860
And I didn't ask you to make a prediction, but after we have this common filter come as models are implemented in the next work,

45
00:06:01,860 --> 00:06:06,930
that you can use this sort of as your starting point to make a prediction.

46
00:06:07,140 --> 00:06:12,810
Okay, but here I just want to you have a chance to practice to look at a data,

47
00:06:13,320 --> 00:06:24,000
extract this data set and then, you know, look at the the simple analysis, sort of a preliminary analysis,

48
00:06:24,450 --> 00:06:29,099
very simple analysis to see at the first hand of the evidence what's going on,

49
00:06:29,100 --> 00:06:34,200
because you really need to see the data, feel the data before you really go something deeper.

50
00:06:34,200 --> 00:06:44,339
Right. So analysis. But later on we will apply the model like the one I did for the poly of mostly positive sort of analysis here.

51
00:06:44,340 --> 00:06:55,050
We can use this structure and see what additional sort of things like prediction were, efficiency improvement or something like that.

52
00:06:55,380 --> 00:07:00,000
We can game out from the more surface of the modeling structure.

53
00:07:00,000 --> 00:07:06,180
Okay, that's the first step. I think you'll hopefully you enjoy doing this.

54
00:07:06,240 --> 00:07:10,740
Yeah. So do on. Are we prepared.

55
00:07:12,520 --> 00:07:14,530
Or is there something we're still learning?

56
00:07:14,530 --> 00:07:20,740
And the next you are ready to do this analysis because this analysis is basically using the knowledge of your group.

57
00:07:20,920 --> 00:07:29,370
651. Mm hmm. Yeah. Right. This is the one that you you are able to do this because you just you for the tempo,

58
00:07:29,380 --> 00:07:33,440
correlation and treat the data are somehow independent sample, but they are not.

59
00:07:33,520 --> 00:07:39,610
But what I think what I said in the here is that you still get valid point estimation.

60
00:07:40,060 --> 00:07:43,230
Okay. Well you can already see our dependance.

61
00:07:43,240 --> 00:07:52,570
You still get the valid point estimation, but your statistical inference is underpowered because you did not consider the temple sort of correlation.

62
00:07:52,630 --> 00:07:58,710
That's the component that we need to like, you know, get something done first.

63
00:07:58,810 --> 00:08:08,500
I have to, you know, ask you to program common field or come in smoother than we can improve this analysis.

64
00:08:08,740 --> 00:08:19,480
Okay. This may be just iteration zero, where iteration one we have more iterations, but this is the, you know, the beginning of the whole analysis.

65
00:08:22,320 --> 00:08:26,190
So the question is, is there a typo? So yeah.

66
00:08:26,580 --> 00:08:31,290
So the sentence to last sentence of May 22nd.

67
00:08:31,620 --> 00:08:37,100
20 oh. Oh, yeah.

68
00:08:37,160 --> 00:08:40,370
I already look forward to the holiday time next year.

69
00:08:41,110 --> 00:08:44,440
No, that's me too. Is the 2022?

70
00:08:44,450 --> 00:08:47,930
Yes, it is. Clear title.

71
00:08:47,960 --> 00:09:01,430
I'm sorry. Okay. So those are the big sort of times that we know that maybe a sociopath is this regime change in this faction.

72
00:09:01,700 --> 00:09:14,509
So December 14, which is very, very big day in the world that we have the vaccine, the MMR vaccine available to general population,

73
00:09:14,510 --> 00:09:23,060
the medical staff and senior people with high risk medical condition are eligible for the first, you know,

74
00:09:24,800 --> 00:09:31,940
chance of receiving the vaccination that we have the following year that, you know, delta very becomes dominant.

75
00:09:31,940 --> 00:09:37,489
Is that change the whole dynamic from the alpha variant to Delta then we have

76
00:09:37,490 --> 00:09:42,080
the omega and coming that's basically which I mean according to report the

77
00:09:42,080 --> 00:09:48,530
news media and somehow challenge the vaccination people receive and adverse a

78
00:09:48,770 --> 00:09:53,560
higher infection rate for the people who even received the food vaccination.

79
00:09:53,570 --> 00:09:58,040
I mean I always say the data I really want to say I never analyzed the data before.

80
00:09:58,040 --> 00:10:11,659
I will be very interested to see that why do not only convert is really challenging or cause some damage on the vaccinated people.

81
00:10:11,660 --> 00:10:19,040
Right. So I think we will look at a data that's over 29th the uncovered become spread in this also

82
00:10:19,040 --> 00:10:26,210
Africa that in following a few months that was quite quickly spread in the US population.

83
00:10:26,750 --> 00:10:30,680
Thus 2020. Okay. Okay.

84
00:10:30,680 --> 00:10:36,050
So I will ask you to do a little bit of exercise on looking at this.

85
00:10:37,160 --> 00:10:40,820
Not the time so small, but pro-social gala distribution,

86
00:10:41,730 --> 00:10:50,990
the hierarchal model that I sort of introduced to model the plot to do that in the next homework or project, I would ask you to do that.

87
00:10:51,170 --> 00:11:00,230
Right. So but anyway, that's the very basic understanding about this possum garment distribution that I think is essential

88
00:11:00,410 --> 00:11:09,320
for people to understand the sort of the operation or some basic properties of this hierarchy model,

89
00:11:09,650 --> 00:11:13,730
because this is the model that we're going to use for prediction or forecast.

90
00:11:14,000 --> 00:11:22,820
Okay. So that's my problem. One, I want to do a little bit this derivation that's from Essential six or one, right.

91
00:11:23,420 --> 00:11:28,340
Okay. Couple of distributions questions clear.

92
00:11:29,240 --> 00:11:31,880
I think if you want to have some, you know,

93
00:11:33,320 --> 00:11:42,799
teamwork on the the data cleaning and data processing you work on to organize your your sort of collaboration.

94
00:11:42,800 --> 00:11:47,690
Because I think the data process of data preparation will take time.

95
00:11:47,690 --> 00:11:53,840
For example, you have a team that say, okay, you do the vaccination data, I do the mortality data.

96
00:11:54,110 --> 00:12:01,489
You don't need to spend time to or someone in your team developed the code for to process the vaccination data.

97
00:12:01,490 --> 00:12:04,910
You can have it process data in your project.

98
00:12:04,940 --> 00:12:10,160
I don't want to spend too much time to sort of independent develop this data.

99
00:12:10,490 --> 00:12:12,800
So extraction process if you want.

100
00:12:12,980 --> 00:12:21,379
But teamwork to save that time, usually it takes quite the time to look at a file and find it right a variable and process it.

101
00:12:21,380 --> 00:12:26,720
And I, I think that give a permission to have open discussion among your.

102
00:12:28,290 --> 00:12:32,730
So too, to say how you're going to attract the right variables.

103
00:12:32,910 --> 00:12:43,650
And so I wanted to put this two big data files in the canvas so you don't need to download yourself unless later on you want to.

104
00:12:44,430 --> 00:12:50,610
I think the data is up to September 27, 21st or 27.

105
00:12:50,730 --> 00:12:57,930
I don't remember. Which I think is towards September 27th, so on or 21st.

106
00:12:58,680 --> 00:13:09,240
But if later on you in November want to do the same thing, we can download, you know, most of the data because CDC updates the data weekly.

107
00:13:09,450 --> 00:13:12,689
Okay. Or we have a model that we do not open the data.

108
00:13:12,690 --> 00:13:21,030
We can make prediction. Then what? Download data. So if your predictions are okay with whatever observed.

109
00:13:21,210 --> 00:13:25,860
Okay. So, so that's something we can look at a later.

110
00:13:26,550 --> 00:13:30,480
But now you have a data, you have the task of the data analysis.

111
00:13:30,480 --> 00:13:37,830
And we will see. Okay.

112
00:13:38,610 --> 00:13:44,200
I did have a quick question for. Class.

113
00:13:44,220 --> 00:13:47,420
When should we expect the first midterm to be about?

114
00:13:47,590 --> 00:13:50,730
Sometime in October. Late October.

115
00:13:51,600 --> 00:13:55,660
Right. It's okay. You are you.

116
00:13:58,160 --> 00:14:01,309
I thought it would be all right.

117
00:14:01,310 --> 00:14:05,390
I think we have a full break. Maybe after a full.

118
00:14:08,720 --> 00:14:15,140
We don't. If at that time you feel ready to do that.

119
00:14:15,190 --> 00:14:20,940
Right. Okay.

120
00:14:24,700 --> 00:14:33,860
Lecture for. We'll provide.

121
00:14:51,680 --> 00:15:05,460
Okay. So. You don't.

122
00:15:06,650 --> 00:15:11,800
Then you open. Uh.

123
00:15:24,660 --> 00:15:36,120
So let me talk about some parametric models or distribution before random variable and proportion random variable because as I said,

124
00:15:36,120 --> 00:15:43,650
in the in facilities model, we constantly give this number of something, number of confirmed cases.

125
00:15:43,680 --> 00:15:47,430
Number of number of recoveries.

126
00:15:47,460 --> 00:15:48,340
Number of deaths.

127
00:15:48,360 --> 00:15:57,780
So the kind certainly a very primary type of random variable we deal with, of course, that you have time series that have the spatial data.

128
00:15:58,200 --> 00:16:07,409
But marginally, we if you look at that, the random variable is a random variable like non negative integer value, random variable.

129
00:16:07,410 --> 00:16:13,260
So that's something we need to deal with for the distributions available in

130
00:16:13,260 --> 00:16:18,600
the literature that we could use to do this count random variable tomorrow.

131
00:16:20,330 --> 00:16:24,590
This will look at prevalence were incidence rate or prevalence rate.

132
00:16:24,590 --> 00:16:27,379
Then we look at the proportions, right?

133
00:16:27,380 --> 00:16:38,180
So pull the number of infections divide by total number of, you know, individuals living in certain counties of region.

134
00:16:38,660 --> 00:16:44,500
Or we look at, you know, the number of confirmed cases over total number of test.

135
00:16:44,510 --> 00:16:48,319
We look at, you know, this positive rate, right?

136
00:16:48,320 --> 00:16:57,320
So so how many people who, you know, received the COVID test and how many people turned out to be positive?

137
00:16:57,350 --> 00:17:02,749
You have this positive rate. So a lot of time or do you feel this percentage right.

138
00:17:02,750 --> 00:17:08,930
Given proportions? Right. So what kind of random variables are variable or distribution over a variable that

139
00:17:08,930 --> 00:17:13,970
for us to model that kind of random variable that's confined between zero or one,

140
00:17:14,390 --> 00:17:18,590
you don't have chance to have anything be smaller or bigger than one.

141
00:17:18,590 --> 00:17:24,880
This is a proportional. So I just give the review.

142
00:17:24,890 --> 00:17:34,240
I think, you know, distribution that in the six or one that you have learn a lot of descriptions, particularly in the modeling of infectious disease.

143
00:17:35,110 --> 00:17:43,360
We look at this two types of random variables. Essentially, it's really modeling the observed a time series.

144
00:17:43,360 --> 00:17:50,990
Like you basically see how you're going to specify a premature model for the emission probability law from.

145
00:17:53,560 --> 00:18:06,070
The underlying sort of infectious dynamics that you cannot derived directly observe and then, you know, send a signal to a process that you observe.

146
00:18:06,100 --> 00:18:09,850
That's you. Wait, what? How do you model the distribution?

147
00:18:12,300 --> 00:18:18,840
If you observe the process, given the latent processes of the infection at a given time.

148
00:18:18,960 --> 00:18:26,650
So basically working on this distribution. But it's.

149
00:18:27,660 --> 00:18:31,650
All this late in process, but you need another distribution for the emission problem.

150
00:18:33,150 --> 00:18:36,990
What you see is this very small. We?

151
00:18:38,280 --> 00:18:47,790
So we reveal this to random variable and then we will see how we're going to deal with it.

152
00:18:47,880 --> 00:18:54,210
First, of course, is the Colorado variables. So this is something you are all very familiar with.

153
00:18:54,270 --> 00:19:03,750
I just walk you very quickly about this discrete random variable y that takes non-negative integers, namely y.

154
00:19:05,090 --> 00:19:09,230
Human, which is defined by non-negative integer values.

155
00:19:09,680 --> 00:19:12,950
So no cases. One case.

156
00:19:13,220 --> 00:19:16,290
Two cases of. No. No. Something.

157
00:19:16,310 --> 00:19:32,680
No. If you observe. But in those tests, according to our Fisher Fisher set distribution of the most important all the squid probability distributions.

158
00:19:32,930 --> 00:19:43,139
I, I of agree with him or disagree because I in the policy in oh five statistic,

159
00:19:43,140 --> 00:19:47,250
I think the logistic regression like logistic regression were binomial distribution.

160
00:19:48,500 --> 00:20:01,070
It's probably most the more often used the propensity score model, the probability of being factored versus fact affected.

161
00:20:06,200 --> 00:20:10,850
By now, the distribution can be generated.

162
00:20:13,700 --> 00:20:18,210
So basically you have in some sense that fossil distribution is more fungible.

163
00:20:21,320 --> 00:20:24,350
But argument to support a fishers point of view.

164
00:20:24,410 --> 00:20:28,910
But anyway, Fisher said that possibly this is. Oh.

165
00:20:32,480 --> 00:20:40,010
Of course not everyone is negative binomial distribution is also a distribution we use to model non-negative integers.

166
00:20:41,840 --> 00:20:45,980
So then probably never heard of this generalized poisson distribution.

167
00:20:46,790 --> 00:20:54,109
But I. I know this distribution because I work in this sort of year and field generalizing

168
00:20:54,110 --> 00:20:59,290
year round field and there wasn't one qualified exam that many years ago.

169
00:20:59,350 --> 00:21:05,360
Our the question question was made by me of course is that that is the generalized problem distribution.

170
00:21:05,930 --> 00:21:10,100
And so so now not many people know know this.

171
00:21:10,430 --> 00:21:17,870
But anyway, I will tell you, because this distributing has been widely used in our insurance exercise.

172
00:21:18,230 --> 00:21:23,809
Okay. So so that that's a distribution that I will introduce today.

173
00:21:23,810 --> 00:21:32,360
I mean, has been very investigated in by like biomedical field, but it's important distribution.

174
00:21:34,530 --> 00:21:40,020
Well, not thing, of course. How do you create quarterly random variables?

175
00:21:40,960 --> 00:21:46,840
Well, that's something we need. We revealed possum of species.

176
00:21:46,890 --> 00:21:56,890
I mean, this is very trivial thing to so everybody knows this expression of this basic coming from Taylor expansion of this exponential function of.

177
00:21:58,810 --> 00:22:02,730
Well, E2 minus is the normalized constant right there.

178
00:22:03,520 --> 00:22:09,480
This. It's actually the the probability mass.

179
00:22:09,750 --> 00:22:13,500
But of course this this all of this over y you can do zero to.

180
00:22:17,280 --> 00:22:21,089
Coming from the expansion of to the last meal.

181
00:22:21,090 --> 00:22:24,690
This is essentially all new, right? If.

182
00:22:26,090 --> 00:22:32,530
So this is a well. Okay, no problem. Probability of massive problem.

183
00:22:33,800 --> 00:22:43,730
A set of non integer values. Why you into this naturally.

184
00:22:45,190 --> 00:22:53,350
Natural express your family. So from six or two you know that natural expand the family distribution to express.

185
00:22:55,940 --> 00:23:04,040
And also distribution is one of the essential sort of grim mauled generals.

186
00:23:04,100 --> 00:23:08,810
The log possum regression is very popular and in.

187
00:23:10,910 --> 00:23:25,720
Something called divas, right? Divas. Right.

188
00:23:25,810 --> 00:23:28,900
So that you know you.

189
00:23:34,110 --> 00:23:39,550
What? Right. Ad hoc function because you.

190
00:23:42,120 --> 00:23:47,800
Property that this assembles to a distance either in a.

191
00:23:49,780 --> 00:23:52,820
What? Yeah.

192
00:23:56,080 --> 00:23:59,100
The situation of normal distribution, right?

193
00:23:59,800 --> 00:24:15,380
Well, normal distribution is. You know wall to Wall Square of why my son you wear this the famous coliseum distribution you know.

194
00:24:16,700 --> 00:24:29,490
Well, this one. Experiment.

195
00:24:30,000 --> 00:24:33,570
And the mule is the model that you going to specify, right?

196
00:24:34,170 --> 00:24:40,980
So so they are determined in a regression model, you set out this, you simply take handfuls of data.

197
00:24:41,580 --> 00:24:47,250
This is your model of how you are going to land the the mean.

198
00:24:49,490 --> 00:24:52,970
But why my. You learn to fly so distant. Right.

199
00:24:53,480 --> 00:25:00,140
Is it? Literally is signed excited by this piece of property that.

200
00:25:04,080 --> 00:25:11,490
So. So if you want to interpret this parcel distribution merry to normal, then this one does not.

201
00:25:22,130 --> 00:25:25,220
In this, the function we call device user. You've heard this.

202
00:25:25,460 --> 00:25:30,830
You have humans residual from the output to measure that.

203
00:25:31,350 --> 00:25:34,610
So so the purpose of device here.

204
00:25:44,480 --> 00:25:49,430
It's a general. Are just.

205
00:25:55,700 --> 00:26:07,999
The. Right. But. And in closing distribution, if you think this is the distribution,

206
00:26:08,000 --> 00:26:12,430
how do you turn the distribution in a way that you can have similar interpretation?

207
00:26:12,440 --> 00:26:21,950
I know what you're trying to do is forcefully create this function to put a distance between what?

208
00:26:23,530 --> 00:26:26,410
And your model in such a way.

209
00:26:26,980 --> 00:26:33,110
So there's a well known established in the literature of the generalized in either models or condition called humans function.

210
00:26:33,110 --> 00:26:36,160
You can check that why you could do milder you go to zero.

211
00:26:37,000 --> 00:26:42,070
Okay so. So you can see that this function is.

212
00:26:45,230 --> 00:26:48,680
Oh, that's right. So why do you get to meet?

213
00:26:52,180 --> 00:26:59,470
But first to your Business Square. So when you create this property, that is why do you build it?

214
00:27:14,930 --> 00:27:21,710
Optimization in the. Regression analysis.

215
00:27:21,920 --> 00:27:26,270
Okay. So the first thing to realize once your arm is really.

216
00:27:32,660 --> 00:27:41,880
And then the. The. Very, very, very similar to your.

217
00:27:47,900 --> 00:27:52,460
You know, some have scored residuals in the theater and you have some motives.

218
00:27:52,480 --> 00:27:57,170
Defense analysis of events in the Irish model have analysis.

219
00:27:58,500 --> 00:28:03,800
Well, they're very similar. Also, I don't see you create a defense function that.

220
00:28:06,100 --> 00:28:09,840
Why am I a small square? But in other distribution, eye popping distribution,

221
00:28:10,150 --> 00:28:18,670
you have a nonlinear distance function called defense functions that help you to this y maximum of square.

222
00:28:19,180 --> 00:28:28,180
Okay, so, so distant. The distance between data, observation and mall is measured by humans.

223
00:28:28,180 --> 00:28:33,730
This function in the percent is because you won't be here anymore.

224
00:28:34,090 --> 00:28:41,440
Every generalized we have a people's function. That's a way that you can have that.

225
00:28:41,990 --> 00:28:45,980
Okay. So this is a very basic way of doing care.

226
00:28:46,000 --> 00:28:56,290
And so, as a matter of fact, my dissertation on the visor by George Benson, he has a paper 1978,

227
00:28:56,290 --> 00:29:03,170
in Jarvis's view, is a discussion paper that has profound inference in here in general.

228
00:29:03,260 --> 00:29:10,840
And. Okay. This is very, very well known paper very creative paper.

229
00:29:10,930 --> 00:29:13,700
Obviously it's a discussion paper.

230
00:29:13,720 --> 00:29:24,940
There are about 20 famous statistician, including like Peter McCullough and Dr. Cox, and now there are a lot of Grimm guys like Adam Acquisti.

231
00:29:24,940 --> 00:29:32,919
And so it's so they all go there to discuss this. So he got this birth model is that it's fine to do there.

232
00:29:32,920 --> 00:29:36,210
It's okay to do that all the time. Generalized.

233
00:29:36,250 --> 00:29:40,030
Neither model error distribution can be read in this form.

234
00:29:42,020 --> 00:29:46,160
Well, this is very, very similar to normal. You have no.

235
00:29:52,920 --> 00:29:57,840
That's from this fossil distribution and that you have normalized the constant.

236
00:29:59,450 --> 00:30:03,410
Well here the Devens function is certainly nothing negative.

237
00:30:04,600 --> 00:30:13,540
Isn't this the theta y and your model nu and is normalizing the constant as thus now depend on meu?

238
00:30:15,750 --> 00:30:19,709
Very, very, very important property. This novena is in constant.

239
00:30:19,710 --> 00:30:26,730
Doesn't depend on me. Very, very similar to. Oh.

240
00:30:30,600 --> 00:30:40,470
So because of this poverty, the normalizing term here, that little term doesn't depend on.

241
00:30:47,450 --> 00:30:51,110
Or any separately from the square.

242
00:30:51,120 --> 00:31:00,350
You know, you have this experience, right? We will do you know regression your first estimate beta result as the Sigma Square after you finish.

243
00:31:02,450 --> 00:31:09,010
Submit your second score using this sort of residual sum of salt.

244
00:31:09,560 --> 00:31:15,770
So you know that you perform your 602 that we go to emulate, right?

245
00:31:17,890 --> 00:31:24,430
Normal distribution, you have data x one, two x and I be normal.

246
00:31:24,580 --> 00:31:34,010
You think I'm a square? You'll know that the employee of you is what I.

247
00:31:35,160 --> 00:31:41,850
And we would drive this. You can clearly see that it has nothing to do with there's no involvement of signal software.

248
00:31:42,390 --> 00:31:53,190
Okay. So you derive this email solution without the estimation, you can be done separately from estimate Sigma Square.

249
00:31:54,610 --> 00:31:59,090
So so this gives rise a grid is in the prime.

250
00:32:00,060 --> 00:32:04,800
So that's why we like GM. GM is very, very numerically stable.

251
00:32:05,220 --> 00:32:16,120
Okay, just slightly score. So I think that our goals made it to outstanding contributions to statistics.

252
00:32:16,140 --> 00:32:24,930
Sometimes people saw Fisher as the fodder. Sometimes I think Gauss should be because he invented two most important things two statistics.

253
00:32:25,200 --> 00:32:28,440
One is Gaussian distribution. Otherwise, at least the square.

254
00:32:29,070 --> 00:32:33,720
Least square is the most stable numerical optimization.

255
00:32:34,140 --> 00:32:39,450
Not all the non parametric semi-automatic series all built at least a square.

256
00:32:39,810 --> 00:32:42,720
We have to be square to deliver extension of that.

257
00:32:43,230 --> 00:32:51,900
So these these are the square is very, very powerful, stable, robust numerical optimization framework.

258
00:32:52,440 --> 00:32:54,840
So so because of this that.

259
00:32:56,060 --> 00:33:09,140
The DRM has a one step like small one step extension from the square using the divisions and so that it also enjoys this numerical stability.

260
00:33:09,150 --> 00:33:12,350
So that's why DRM is very popular in practice for us to.

261
00:33:13,780 --> 00:33:19,100
But. In the literature, like Nancy Reid and D.R. Cordes.

262
00:33:19,400 --> 00:33:24,860
They wrote a paper a long time ago named this poverty as like a novelty.

263
00:33:26,360 --> 00:33:32,330
So you have the likelihood involves both parameter meat parameters that are separable.

264
00:33:32,690 --> 00:33:41,540
When you ask the mean you don't need to estimate the sigma squared can be separate estimate from estimating Sigma Square.

265
00:33:42,230 --> 00:33:54,120
So Derek Hobson nice read the name of Likelihood of a Banality because this normalized and constant does not depend on me.

266
00:33:56,120 --> 00:34:07,580
So I b the distribution is not of my fair distribution because the distribution, this issue denominator harmonizing constant involves new to me.

267
00:34:09,060 --> 00:34:23,420
You know. Okay. So because of this spatial structure that assembles very, very, very like normal distribution and a bent Jordan.

268
00:34:24,950 --> 00:34:35,810
It's personal. And this is essentially the distribution structure that we use here and also all the software packages.

269
00:34:37,400 --> 00:34:43,250
So and parcel distribution is one of this case.

270
00:34:43,250 --> 00:34:47,900
When you choose a space, you. So.

271
00:34:48,020 --> 00:34:51,380
So we are talking about possible distribution if you feel space.

272
00:34:57,240 --> 00:35:07,170
Does not involve the mule. Okay. Well, you know that in the in the possible distribution dispersion parameters one from your team.

273
00:35:07,320 --> 00:35:16,740
Okay. So, so the IMU is the.

274
00:35:22,370 --> 00:35:26,780
Is your best friend, right? When these two women.

275
00:35:28,940 --> 00:35:33,470
Anyway. So that's the E.T. model.

276
00:35:34,730 --> 00:35:40,010
So from E.T. model that you can take the first order, drop the second derivative.

277
00:35:40,430 --> 00:35:45,980
So what you see here, if you take the first of all, the derivative of the devgn's.

278
00:35:47,260 --> 00:35:52,210
Function with respect to the new parameter. This is why most new overview.

279
00:35:54,100 --> 00:36:03,120
So why am I this meal over a meal? This is essentially the the possum statistic, right?

280
00:36:03,130 --> 00:36:08,410
So you know that the meal actually is the variance of the possum distribution.

281
00:36:09,070 --> 00:36:18,720
And then another insight here is really that the, uh, the defense function, Selva, is nonlinear.

282
00:36:18,730 --> 00:36:25,640
It can clear see that? But first we're going to run to the defense fund function this.

283
00:36:27,760 --> 00:36:34,430
In your function. You? Why? Well, this is quite interesting to see this.

284
00:36:35,750 --> 00:36:40,880
Well, this first order of derivative, the defense function with risk of lives and function.

285
00:36:41,450 --> 00:36:51,200
And this helps us to really create a of a good score function to solve because you

286
00:36:51,200 --> 00:36:56,570
know that in the grammar we always use Emily to find the parameter estimation.

287
00:36:57,110 --> 00:37:00,150
And when you do the neural ration search, the,

288
00:37:00,870 --> 00:37:10,910
the score equation is linear in your data that's very friendly for us to find the right of search algorithm to find the solution.

289
00:37:11,240 --> 00:37:18,900
Right. The. A meal is to fly over New Square and that.

290
00:37:28,160 --> 00:37:30,410
Value add the meal and if you.

291
00:37:33,830 --> 00:37:43,819
There is one case that's well known in the Jeremy you have various function which basically the function according to this definition,

292
00:37:43,820 --> 00:37:51,720
you can see that. Of your difference function rate around the mean parameter or.

293
00:37:58,420 --> 00:38:02,980
The second derivative of your demon's function, which tells you.

294
00:38:06,250 --> 00:38:18,350
Right at the minute. So the various function which you know, and this has been sort of well-studied in the literature.

295
00:38:20,640 --> 00:38:30,740
So so of course that if you go to dispersion model theory, you know that we would choose different d we use binomial distribution.

296
00:38:30,740 --> 00:38:55,660
You have a different. Obviously supply lines new square that's normal distribute this family distribution in close post

297
00:38:55,680 --> 00:39:05,670
continues of the distribution and discrete distribution and also negative binomial binomial so.

298
00:39:09,310 --> 00:39:15,220
As part of our US special cases, elements of this Femi Oke, you can study this.

299
00:39:16,000 --> 00:39:19,400
And so I like the etymology.

300
00:39:20,050 --> 00:39:25,900
It gives you a lot of the important distributions, the distribution that we use to do a generalized new model.

301
00:39:29,260 --> 00:39:36,100
Pretty surprising distribution which are quite essential like a well-known so mean it's mule which.

302
00:39:40,100 --> 00:39:43,730
This is only distribution where the mean and variance are. You call. Okay.

303
00:39:43,850 --> 00:39:47,270
I see. I don't know. Okay. Of course, if you have. Yeah.

304
00:39:48,180 --> 00:39:53,080
Huh? Oh. So when the virus is spreading.

305
00:39:55,330 --> 00:40:00,390
There is. I.

306
00:40:02,190 --> 00:40:05,910
Right. So. So. So either is over dispersion and or dispersion.

307
00:40:06,300 --> 00:40:13,000
We can now follow partial distribution because forcing distribution guarantees that various.

308
00:40:14,590 --> 00:40:20,170
Severance is bigger than meaning that this verbal cannot follow a dispute,

309
00:40:20,410 --> 00:40:29,350
possibly equal GAAP and variance is characterizations of problem distribution making the omission little football its first commentator.

310
00:40:29,620 --> 00:40:35,079
So that's the place where you want bring negative binomial distribution in the model

311
00:40:35,080 --> 00:40:40,700
because you know the possible distribution requires that the data has to have you know,

312
00:40:40,720 --> 00:40:46,480
population meaning and population variance equal but which may not be true for the data.

313
00:40:49,480 --> 00:40:53,580
So the postal distribution has this, you know.

314
00:40:57,970 --> 00:41:03,580
Random variables disarm also follow pos and random variable with the disarm of the mean.

315
00:41:04,000 --> 00:41:10,450
Right. So that's the it's quite quite a well known property to independent person.

316
00:41:10,690 --> 00:41:14,530
Put them together. Disarm is also allow you integer random.

317
00:41:17,740 --> 00:41:24,540
I mean, some of it. I'm in here as independence.

318
00:41:26,350 --> 00:41:35,590
And so. So this is the property that I mentioned that the Y Palsson distribution is more fundamental distribution than binomial.

319
00:41:35,680 --> 00:41:38,800
Okay. Here is the fact.

320
00:41:39,110 --> 00:41:46,140
You have two independent property distribution. This follows a possum distribution.

321
00:41:47,160 --> 00:41:52,649
When. Of independent person random variables.

322
00:41:52,650 --> 00:41:57,290
The disarm is also some random variable is the mean you call to view one

323
00:41:57,300 --> 00:42:04,980
possible to know if your conditional on this why one condition the told whole.

324
00:42:05,790 --> 00:42:09,970
Okay then this. This false binomial distribution.

325
00:42:15,080 --> 00:42:24,440
Oh. The farm, which is fixed and the probability is the ratio of new album one plus new to.

326
00:42:25,170 --> 00:42:38,820
So basically plus binomial in the random number generator people sometimes start replacing random variable simulate possum numbers by oh I'm sorry.

327
00:42:39,510 --> 00:42:46,860
So when people say that binomial random variable simulate binomial data by simulated independent possum.

328
00:42:47,250 --> 00:42:52,230
Okay. So if you want the total to be a and you are right.

329
00:42:55,880 --> 00:43:03,170
A coin. Two, two, three, two, two. Awesome distribution.

330
00:43:04,760 --> 00:43:20,990
Okay. So, so, so that sometimes people are also doing this sort of grim analysis by following the sort of the the apostle models,

331
00:43:21,440 --> 00:43:27,230
because sometimes it's easier to model the problem data than binomial data.

332
00:43:27,560 --> 00:43:36,950
So they, they say that if I can model the, the plus one data in such a way, I can always come up with a model for binomial.

333
00:43:37,370 --> 00:43:46,910
So I see some publications where people say, oh, if I have ran effects model because you know that there are tremendous difficulty to deal with.

334
00:43:48,110 --> 00:44:01,250
Now in your ran fixed model, this the binomial distribution will come that using possible distribution because of this connection.

335
00:44:02,020 --> 00:44:17,460
Okay. Anyway. So so so the point of here is that you can run logistic regression by using this commit are in the possible with parsing distribution.

336
00:44:19,390 --> 00:44:25,450
So how do you create a core data processing brand? Here is the way the cost will cast the presentation.

337
00:44:26,520 --> 00:44:35,640
So. So you have three. Then you know you at.

338
00:44:37,640 --> 00:44:45,850
The first one. Third one to get her. You get the death. Oh. But Zeta is the way to pass away one too.

339
00:44:45,880 --> 00:44:49,240
You have three person, right? Who are independent.

340
00:44:49,660 --> 00:44:55,030
You at. We want you to share this.

341
00:44:57,930 --> 00:45:01,860
And Z one to Z to our correlated. Okay.

342
00:45:05,050 --> 00:45:11,830
Correlation for a pair of random variables because the here same problem.

343
00:45:13,300 --> 00:45:16,800
So Mark. Mustapha Colucci, closure.

344
00:45:17,590 --> 00:45:24,400
They want you to follow. And because why?

345
00:45:24,450 --> 00:45:33,549
One more. There's a11.

346
00:45:33,550 --> 00:45:38,000
One two is the. Also has the same meaning.

347
00:45:38,460 --> 00:45:42,600
The various opossum is same as. The.

348
00:45:45,860 --> 00:45:54,190
But this approach of success or representation enables us to generate many positive quality random variables with.

349
00:45:57,590 --> 00:46:01,750
Roll the package how to generate the core frosting using this kind of stuff has.

350
00:46:04,910 --> 00:46:11,920
And we also learn generate sort of ultra regressive tempo structure for corporate profit.

351
00:46:11,930 --> 00:46:15,320
And this is basically through the binomial thingy. Right.

352
00:46:15,620 --> 00:46:26,240
So I have a slice and lecture three where I prove that if you do binomial thing, you generate ultra regressive Poisson process.

353
00:46:26,720 --> 00:46:35,630
So this one allows you to generate the correlated portion of this comp on symmetry or unstructured fossil.

354
00:46:37,660 --> 00:46:47,890
But also regressive type of correlation that you can use the binomial setting which has been covered in the previous lecture notes.

355
00:46:50,590 --> 00:47:03,100
Okay. So this is all the distribution we know and very likely that the negative binomial distribution is also a dispersion model.

356
00:47:03,340 --> 00:47:14,150
Okay, so, so well, we know that negative binomial distribution is also a distribution to model counts.

357
00:47:15,080 --> 00:47:22,940
Not. Dispersion with proportional distribution fails to do that.

358
00:47:22,950 --> 00:47:25,959
We always have the alternative.

359
00:47:25,960 --> 00:47:39,180
The goal for know this is why did use in genetics or genomics because people have the number reason even from sequencing everything that people use.

360
00:47:39,180 --> 00:47:48,480
Think of the binomial it becomes a sort of a natural alternative to fossil when your face over dispersion.

361
00:47:48,630 --> 00:47:53,250
Okay so negative binomial is has to be. Why do you see genetics and genome?

362
00:47:54,370 --> 00:48:01,550
When you want to do this over at dispersion data. Well in the in the context of.

363
00:48:05,700 --> 00:48:11,759
So as I said, that the negative binomial is also a special member of this birth model.

364
00:48:11,760 --> 00:48:18,520
So you can write in that form where. This.

365
00:48:23,080 --> 00:48:31,070
Okay. So the last point that's actually the point that I give you in the homework.

366
00:48:31,170 --> 00:48:35,920
Right. So like, if by no means may arise from a plus and comma mixture,

367
00:48:36,730 --> 00:48:45,220
that is you you have y conditional meaning follows partial distribution and mean false comma distribution that marginally while fun negative binomial.

368
00:48:45,550 --> 00:48:50,140
Okay, so that's the homework. The first problem of all three.

369
00:48:50,140 --> 00:48:53,230
I want to. You really see that? Okay,

370
00:48:53,320 --> 00:49:02,650
that Harold come up here I come all of that you see marginally in that you know states with model I used to analyze

371
00:49:02,660 --> 00:49:09,820
polio data we basically use the negative binomial distribution as margin distribution in outcome construct.

372
00:49:10,360 --> 00:49:15,250
Of course, go one step further to build up some temple dependance.

373
00:49:17,450 --> 00:49:25,550
Oh, next one. This is probably a very unknown sort of distribution that generates pleasant distribution.

374
00:49:25,790 --> 00:49:29,329
And if you want to, you can do this.

375
00:49:29,330 --> 00:49:33,020
And to replace negative binomial and.

376
00:49:34,390 --> 00:49:37,000
Such why used in not to?

377
00:49:37,870 --> 00:49:49,510
I have never seen any publication in genomics genetics to use in genes which is also a distribution can be used to deal with over dispersion.

378
00:49:49,870 --> 00:49:59,490
Okay. Let me just explain to you. Well, this work was started by two actual scientists studying in Canada.

379
00:49:59,520 --> 00:50:04,710
I think the council is a person who works in exercise.

380
00:50:05,040 --> 00:50:09,870
It's an exercise they always knew to model the data number of plants.

381
00:50:09,880 --> 00:50:14,010
Right. So so for a company that, you know, pay the price,

382
00:50:14,460 --> 00:50:24,170
they always want to know how many cases that this year the company has processed and the number of claims or had the individual right.

383
00:50:24,180 --> 00:50:30,059
They always say, you know, is this individual worthy of a good, safe, safe driver?

384
00:50:30,060 --> 00:50:38,010
And the word this individual has a lot of claims over here. And of course, most of time, good drivers have zero of zero claims.

385
00:50:38,430 --> 00:50:46,350
So that's one way to generalize from this has better capacity to do this zero because both parcel distribution,

386
00:50:46,350 --> 00:50:57,329
negative binomial distribution, generalized problem distribution have have this zero as one of the possible value.

387
00:50:57,330 --> 00:50:59,610
Right? But they have different distributions.

388
00:50:59,610 --> 00:51:06,630
Sometimes you have zero inflated situation so that you need an additional component to model the zero counts.

389
00:51:06,750 --> 00:51:14,040
Right. This, this distribution has better capacity to deal with zero counts.

390
00:51:16,000 --> 00:51:22,180
The situation in the actual size than when people model the number of claims in data.

391
00:51:22,260 --> 00:51:35,919
Okay. I think that this also B issue in many applications, including sequencing data where you have many zeros at the chance of occurrence of zero,

392
00:51:35,920 --> 00:51:40,120
is a lot of time beyond what percent distribution can describe.

393
00:51:40,130 --> 00:51:44,380
So people add additional component to deal with the inflated distribution.

394
00:51:44,380 --> 00:51:54,910
But the generalized problem distribution is the one that gives a little bit more flexibility than person to give is the chance of zero.

395
00:51:55,270 --> 00:51:59,920
Okay, here is that is a live for me expression of that.

396
00:52:01,360 --> 00:52:10,250
So we can say when lambda zero. A regular possum distribution.

397
00:52:10,760 --> 00:52:16,280
Okay. So so that's. This is called generalized processing with a zero.

398
00:52:16,280 --> 00:52:20,700
Then this. Yeah.

399
00:52:22,730 --> 00:52:30,860
Oh. Well, that's the distribution.

400
00:52:30,860 --> 00:52:35,510
You can prove this meeting is equal to theta times one minus lambda.

401
00:52:36,440 --> 00:52:41,140
And you. One over one master lambda by.

402
00:52:43,710 --> 00:52:47,670
And Placide is a parameter. You call. You put one bigger.

403
00:52:48,330 --> 00:52:53,250
Then you can calculate the variance of this jar plus one equal to.

404
00:52:55,400 --> 00:53:05,180
And you can see that. Is the over dispersion parameter because 1% equal to one that.

405
00:53:06,550 --> 00:53:09,880
So soon when you go to one. That's the case that.

406
00:53:11,780 --> 00:53:21,410
Well, that three returns the puzzle. Okay, no problem. But when the number is not equal to one or two, then you'll have over dispersion.

407
00:53:21,740 --> 00:53:28,010
So generous profit distribution is another distribution that you use to view.

408
00:53:31,700 --> 00:53:46,160
So that's. Something we and I say I would I would make a comma here be the advertisement.

409
00:53:50,670 --> 00:53:53,940
They are very, very.

410
00:53:55,700 --> 00:53:59,360
In form. So you have this dispersion from.

411
00:54:02,940 --> 00:54:07,930
Elected by an all new right. You have the new equal to one fourth.

412
00:54:30,790 --> 00:54:34,870
There are some new times. One wants a stranger there.

413
00:54:35,050 --> 00:54:37,420
Okay, that's short term negative. I know.

414
00:54:38,080 --> 00:54:48,400
And this is nice because it gives multiple people it's much cleaner than the actually the form of over dispersion suggested by negative binomial.

415
00:54:48,630 --> 00:54:54,780
Okay. So, but people haven't really make a good use of generalize passive distribution.

416
00:54:54,840 --> 00:55:02,350
Yeah. Okay. So if you want to write this little bit in the way of the switching of middle and disparate.

417
00:55:04,390 --> 00:55:11,000
So now you can rewrite this in this way. And then so.

418
00:55:11,160 --> 00:55:18,980
So you have all the nice interpretation and. So this will give you the way to test it over this.

419
00:55:25,390 --> 00:55:28,510
Who works on a lot of popular modeling and.

420
00:55:33,210 --> 00:55:45,180
Your job is to make people know that they are largely similar but different defer for their skewness and under a large fraction of zero cuts.

421
00:55:45,720 --> 00:55:52,800
So essentially what this said there is it's, you know, generalized question and make it binomial are very, very similar.

422
00:55:53,280 --> 00:56:03,180
But, you know, the general problem gives you more flexibility to deal with skewness and to do this large, fractional zero count.

423
00:56:03,600 --> 00:56:09,900
Well, essentially, the journalist principle isn't slightly better than English by not what you need.

424
00:56:10,410 --> 00:56:15,300
What do you want? Type of data at the moment. But you know.

425
00:56:18,840 --> 00:56:21,930
As possible. So but I think that's.

426
00:56:22,200 --> 00:56:28,800
I don't I haven't seen anything really seriously developed in the context of, you know.

427
00:56:31,140 --> 00:56:37,310
Goodbye, Norman. Yeah, but it's a room to do something and make this available.

428
00:56:37,670 --> 00:56:45,770
No difficult thing to do, but certainly is something useful to be added into our toolbox.

429
00:56:48,280 --> 00:56:53,169
So John departments rarely know and studying the literature because this had been way to

430
00:56:53,170 --> 00:56:59,500
use the actual signs and insurance company and first day that in more practice you should.

431
00:57:02,630 --> 00:57:13,040
On the wall of my piece student of mine about ten years ago she wonderful job interview in a bank.

432
00:57:15,460 --> 00:57:20,200
To work in Cincinnati. And she went to fifth, fifth and third back.

433
00:57:20,320 --> 00:57:26,260
I want to work for. Is there financial sort of a no, no, the risk management team in the bank.

434
00:57:27,220 --> 00:57:41,170
And then the feedback is that, oh, you know, they are dispersed more because this kind of wall is really widely used in finance.

435
00:57:46,930 --> 00:57:52,720
So that's what I want to see. And you can see there is something you can think about it.

436
00:57:54,860 --> 00:57:58,570
It's all proportion, random variable proportions.

437
00:57:59,780 --> 00:58:09,469
It's the number one choice that people are to have devoted so many, you know, sort of a statistic method algorithm around beta distribution.

438
00:58:09,470 --> 00:58:16,240
This is a random variable defined. Interval between universe.

439
00:58:18,370 --> 00:58:21,400
Right? Right. So this cannot happen.

440
00:58:21,760 --> 00:58:27,700
There is zero probability. I would be young. Zero one so is a proportion, right?

441
00:58:27,710 --> 00:58:35,930
You define that all the time. People just treat zero one to be normally distributed, which is not true because normal distribution,

442
00:58:36,560 --> 00:58:40,070
you cannot really come far unless you do truncated normal.

443
00:58:40,400 --> 00:58:45,830
You cannot guarantee that you have zero chance to have probably be the certain interval.

444
00:58:46,130 --> 00:58:54,050
I mean, technically speaking, you have probably chance 0 to -8 or beyond certain range, but you still have certain non-zero probability.

445
00:58:54,290 --> 00:59:01,370
If you really want to have zero probability, you have to think about something different distribution.

446
00:59:01,730 --> 00:59:06,260
But anyway, better distribution is the one we use a lot in practice too.

447
00:59:06,260 --> 00:59:10,280
You have the regular distribution for most of the popular distribution.

448
00:59:10,730 --> 00:59:18,440
But anyway, this is something that we use. But I work on quite a bit on the simplex distribution, I would tell.

449
00:59:19,880 --> 00:59:24,890
And my first feature student who worked on a pitch dissertation on simplex distribution.

450
00:59:26,950 --> 00:59:32,410
But simplest distribution is better than better distribution, according to my experience.

451
00:59:32,770 --> 00:59:37,230
But this distribution is not for one. Okay.

452
00:59:37,410 --> 00:59:40,860
So when you have proportions like methylation. Okay.

453
00:59:40,860 --> 00:59:45,020
Methylation data is a proportion of, you know, this or I guess you look at that,

454
00:59:45,030 --> 00:59:54,710
this relative expression of c c nucleotide over the sum of the order of nucleotides.

455
00:59:54,720 --> 01:00:03,510
So you basically have proportions of there, there you feel you generalize this to multivariate.

456
01:00:03,930 --> 01:00:07,680
So a version you basically have compositional data.

457
01:00:08,180 --> 01:00:16,800
Right. Compositional data is also a multi dimensional version of multi dimensional proportional data.

458
01:00:17,160 --> 01:00:26,910
You split the pie into pieces. You look at what's the percentage occupation of one piece over entire arm pie.

459
01:00:26,910 --> 01:00:34,160
Suppose that this the pie the pie has 100% you look at, but you can't apply it to three pieces.

460
01:00:34,210 --> 01:00:35,820
You're looking at compositional.

461
01:00:36,720 --> 01:00:47,910
Oh so the proportion of each pie amount the entire pie so so that's what microbiome data Michael behind it because you cannot count exactly the number

462
01:00:47,910 --> 01:01:00,120
of the the various or whatever if you count the you instead of you look at what's the proportion of that one type in the taxa among all entire thing.

463
01:01:00,130 --> 01:01:08,010
So, so you have one dimensional situation where you look out of proportion, you have multi-dimensional, you have compositional data.

464
01:01:08,190 --> 01:01:14,330
Okay, what composition of distributions? Here is you just.

465
01:01:16,580 --> 01:01:20,390
Emoji transformation. So why over a woman's why and.

466
01:01:21,450 --> 01:01:24,569
Relaxed this boundary condition from zero one.

467
01:01:24,570 --> 01:01:30,930
So when you do this transformation, just like launch your transformation from probability parameter to regression.

468
01:01:31,320 --> 01:01:34,370
This will vary from minus infinity. Positive infinity.

469
01:01:34,380 --> 01:01:39,660
Right. So you. You know best and. The one.

470
01:01:43,060 --> 01:01:48,760
And as people know, often due to, you know, just, you know, sort.

471
01:01:51,240 --> 01:01:55,190
For easy sort of statistical analysis to.

472
01:02:05,770 --> 01:02:10,860
The the of this as a. Oh.

473
01:02:11,670 --> 01:02:25,700
So so we I think this model it's sometimes hard to interpret this model time and we really want to evaluate the direct association between the need.

474
01:02:29,070 --> 01:02:35,360
And coveted specs. We're not interested in evaluating the new.

475
01:02:39,240 --> 01:02:44,530
This is the difference. If you want. You.

476
01:02:46,730 --> 01:02:55,290
Right. And the expectation of trance. Device sell directly.

477
01:02:55,620 --> 01:02:59,070
If this is the philosophy you use, why we need DRM.

478
01:02:59,370 --> 01:03:05,790
We all need to grow. We can just as well mention of your data and then all of this transformed data.

479
01:03:06,030 --> 01:03:13,960
Why do we need to grab the. So, Jeremy, is that you want to model the meaning of the rent.

480
01:03:16,100 --> 01:03:25,440
Directly as a function of covers water to transform the form to to do that this this is the difference.

481
01:03:25,790 --> 01:03:35,780
Do you do. And they transform the variable or.

482
01:03:37,600 --> 01:03:43,530
And the corvids the. Right.

483
01:03:44,730 --> 01:03:48,950
Okay. So. So that's. That's the choice, right?

484
01:03:49,230 --> 01:03:59,400
Which way you want to go. If you want lots of the of to your end that is we directly model the relationship between the meaning of

485
01:03:59,400 --> 01:04:06,790
your data or your random variable rather than the meaning of transformed variable as a function of cause.

486
01:04:06,900 --> 01:04:13,310
Then this is not your first. So another thing we did.

487
01:04:14,280 --> 01:04:17,560
Um, yeah.

488
01:04:17,570 --> 01:04:21,590
So another point is that they were used to this normal times.

489
01:04:28,120 --> 01:04:31,220
Women that transmit data for normal distribution to variants that.

490
01:04:36,580 --> 01:04:41,770
A problem because you'll know that the jitter and all the distribution except normal distribution.

491
01:04:43,210 --> 01:04:47,410
That the viewers depend on mean you have various function, right?

492
01:04:47,980 --> 01:04:53,410
So the variance of your random variable always is dispersed in prime time.

493
01:04:53,410 --> 01:05:01,960
So various functions so mean always affects your various except normal because in the normal case, very function is one constant.

494
01:05:02,710 --> 01:05:11,500
So now you are trying to when you do this modeling, you basically say that, oh, there is no longer a fax, it's no longer a function of me,

495
01:05:11,500 --> 01:05:22,239
which you can but doesn't tell you that simply this will doesn't tell you that there's no way you can really then provide that.

496
01:05:22,240 --> 01:05:26,080
But people do this anyway. So it's long time.

497
01:05:26,400 --> 01:05:38,530
The inference may be problematic because the do not model the second moment as function, but intrinsically in principle it is should depend on me.

498
01:05:38,890 --> 01:05:42,850
But because of normal assumption you ignore that connection.

499
01:05:51,140 --> 01:05:54,950
Oh. Piece of paper that the.

500
01:06:00,600 --> 01:06:04,080
When Y is very close to zero or close to one.

501
01:06:04,560 --> 01:06:08,580
This one artificially create a lot of homebuyers because of.

502
01:06:10,690 --> 01:06:15,519
So sometimes if you give this to a practitioner to use the data.

503
01:06:15,520 --> 01:06:21,110
Right, they just do of transformation. Oh. It's not easy.

504
01:06:21,530 --> 01:06:29,659
Okay, let's find another model. But that did not available in your data.

505
01:06:29,660 --> 01:06:36,140
Could be all that could be a troublemaker to change your analysis result quite a bit.

506
01:06:36,680 --> 01:06:44,620
It's not this. It's all the. That's when y equals wow equal to zero.

507
01:06:46,530 --> 01:06:54,300
And the magnitude quite of that is some artificial layers of layers created by the transformation.

508
01:06:54,890 --> 01:06:58,080
Okay. So that can change whole analysis quite a bit.

509
01:06:58,530 --> 01:07:04,770
So people don't like this transformation, although it is so widely used practice.

510
01:07:05,010 --> 01:07:08,830
Okay. I.

511
01:07:09,770 --> 01:07:14,180
I'll model the algorithm for them to use. Oh.

512
01:07:14,820 --> 01:07:21,030
So that's why people use paper. And I think people need to take a training course.

513
01:07:21,840 --> 01:07:27,310
You worked. I'll normally distribute it later.

514
01:07:27,530 --> 01:07:34,980
You mean you'll make a. This whole transformation may not make sense.

515
01:07:35,090 --> 01:07:41,850
I think maybe I'll give the wrong generate a lot of false positives or false negatives.

516
01:07:42,870 --> 01:07:46,500
Okay. So if you want to really impose a theorem time following.

517
01:07:51,570 --> 01:07:55,740
As people use data distribution as the first choice.

518
01:07:55,950 --> 01:08:07,790
Okay. So here's the fader distribution. And this a proportion this is density of you know that.

519
01:08:09,560 --> 01:08:15,220
Then you have uniform distribution. So uniform is a special case of data anyway.

520
01:08:16,360 --> 01:08:24,000
So if you have the meaning and veterans. The standard sort of thing.

521
01:08:24,610 --> 01:08:34,810
You. We? Our transition.

522
01:08:35,560 --> 01:08:40,090
So, one, I want to work more than me. It's a dispersed model.

523
01:08:40,090 --> 01:08:49,340
You always want to introduce me and this. Or disbursement.

524
01:08:49,760 --> 01:08:58,090
So just do reprioritisation. Okay. So. You did not aafa over afro queda from.

525
01:09:00,330 --> 01:09:06,900
It's into a very function as one of the side mealtimes when new.

526
01:09:09,210 --> 01:09:16,350
Well, size one plus anyway. So there's a 1 to 1 correspondence between these two types of primary citations.

527
01:09:16,710 --> 01:09:25,790
So anyway, this is the one I like. I always want to write the mean as mean so I can write the function of my covers.

528
01:09:26,160 --> 01:09:29,800
And I have. So.

529
01:09:34,390 --> 01:09:39,490
This reprioritisation period is to build density in this.

530
01:09:40,950 --> 01:09:54,790
Then the fight is over. This version premature. Easily to do and people to model this here is there to be a distribution.

531
01:09:58,500 --> 01:10:04,230
So. So, yeah. Well so when this dispersion parameter.

532
01:10:12,490 --> 01:10:21,000
When they when this this person from this fight becomes larger and larger than the distribution will climb more and more.

533
01:10:22,440 --> 01:10:26,429
Yeah. So. So what is this version?

534
01:10:26,430 --> 01:10:33,660
Params r one minus the reciprocal of the size dispersion parameter because the larger.

535
01:10:38,310 --> 01:10:42,300
Of the dispersion parameter, as I say that you have.

536
01:10:47,400 --> 01:10:56,639
A square of various functions and this is called dispersion prime in this case is one over one plus size.

537
01:10:56,640 --> 01:11:01,110
So this one over one plus size dispersion property.

538
01:11:01,410 --> 01:11:10,620
But simply if you ignore that thing, you can roughly think that a five over one over five is the dispersion parameter.

539
01:11:11,130 --> 01:11:19,860
So this has to improve in the tier m that when dispersion parameter, this goes to zero.

540
01:11:21,900 --> 01:11:29,990
Well. The distribution will become more and more like normal.

541
01:11:30,570 --> 01:11:34,090
The. A lot of assembled.

542
01:11:34,090 --> 01:11:38,130
Some products you always let to ensure the mystery of angles.

543
01:11:38,150 --> 01:11:43,520
Infinity. Right. That's that's sort of the typical.

544
01:11:44,880 --> 01:11:50,790
But but in the theater and they're not way doing some products that costs more dispersion of

545
01:11:50,790 --> 01:11:55,710
some products it's more dispersion products and products basically is that if you make it.

546
01:11:57,960 --> 01:12:04,140
But over to the side because arm goes smaller, smaller and distribution will look.

547
01:12:09,600 --> 01:12:13,500
Meaning distribution with no equal or better distribution.

548
01:12:15,380 --> 01:12:22,940
And this is how many people are involved in this more dispersion of some products.

549
01:12:23,360 --> 01:12:27,120
This is something to do with a set sign to point of approximation.

550
01:12:27,560 --> 01:12:40,920
In the problem in Laplace. Approximation. Well. The.

551
01:12:44,500 --> 01:12:48,360
You're. A lot of times people are.

552
01:12:51,900 --> 01:13:05,210
This is something to do model for. For. But we look at why most of us resist you as the way to measure the goodness of feet.

553
01:13:05,330 --> 01:13:14,600
Right. But, you know, the this distant we know understand now is only good for you.

554
01:13:14,610 --> 01:13:19,340
Clean a space where you clean space where we're this normal defense function.

555
01:13:19,910 --> 01:13:25,890
But if you move out of the normality, then this distance is not right.

556
01:13:25,940 --> 01:13:29,720
You shouldn't use the given function to measure the distance.

557
01:13:30,170 --> 01:13:39,130
But people say when when this approximately good to measure the difference because this is easier to compute.

558
01:13:39,140 --> 01:13:39,380
Right.

559
01:13:39,620 --> 01:13:51,200
When this one this typical the residual we use in they need a regression model be relatively okay to measure the distance for non normal distribution.

560
01:13:51,770 --> 01:13:56,840
This this basically is the smallest person's high so we have the solution look like a no.

561
01:13:57,990 --> 01:14:04,590
Then this y minus you will be approximately okay to make sure the Afghan is fit.

562
01:14:06,420 --> 01:14:15,850
So that's small thought. So we have one thing to point out that like.

563
01:14:21,860 --> 01:14:26,629
So it does not belong to the family of giraffe in this year.

564
01:14:26,630 --> 01:14:31,880
And we do not have a distribution. You know why? Well, that's it.

565
01:14:32,120 --> 01:14:45,230
That's because the normal life and constant thought that you don't have a likelihood of other knowledge often involves this.

566
01:14:45,790 --> 01:14:56,890
Involves. Correct. Almost it involves the mean parameter so that it does not satisfy the likelihood of a penalty.

567
01:14:57,280 --> 01:15:01,240
So the inference, an estimation inference is the more.

568
01:15:04,150 --> 01:15:07,620
Side two parameters 17. Okay.

569
01:15:07,630 --> 01:15:14,200
So this is something people. You can't do it nowadays because you have power computer.

570
01:15:14,590 --> 01:15:24,820
But technically speaking, the model structure is not a bit unpleasant in the way that, hey, we can do better, can we?

571
01:15:25,030 --> 01:15:31,250
Or if. We don't have the like to offload them out.

572
01:15:38,470 --> 01:15:42,400
Edge the range from zero one to A to B, that's understandable.

573
01:15:42,940 --> 01:15:46,540
And then the payload distribution is actually.

574
01:15:47,480 --> 01:15:51,390
Gallery from Ghana distribution. Okay.

575
01:15:54,380 --> 01:15:58,340
That reissue of this drama is Spader.

576
01:15:58,790 --> 01:16:02,419
So there is a similar poverty for this.

577
01:16:02,420 --> 01:16:08,770
If you have to plus independent person, the ratio will be what will be fine.

578
01:16:09,570 --> 01:16:16,510
Okay. Some of the ratio was in favor of it because of this property.

579
01:16:17,260 --> 01:16:24,240
You can generate more gamma distributions to have a multiverse version of, you know,

580
01:16:24,280 --> 01:16:31,380
that's called the ratio distribution that people use to model compositional things like Michael.

581
01:16:32,260 --> 01:16:36,960
But. It.

582
01:16:39,510 --> 01:16:44,040
They don't you and you can say that the money is the ratio of this.

583
01:16:44,060 --> 01:16:53,860
Right. Me. It's the ratio of this this is between zero one doesn't sample because Y varies between zero one.

584
01:16:53,980 --> 01:16:59,980
Right the being has to be between zero one so that you can do a true set.

585
01:17:00,150 --> 01:17:08,600
Logit. By a logic how you think function.

586
01:17:08,600 --> 01:17:19,910
And in our gear and contacts there is our participatory rec that is all estimation and inference.

587
01:17:20,420 --> 01:17:26,659
Okay, so one thing I should point out, because you do not have the likelihood of functionality,

588
01:17:26,660 --> 01:17:37,280
you need to ask them the regression parameter and the the skill parameter or dispersion parameter, which is whatever site to get her.

589
01:17:42,490 --> 01:17:46,660
Well, at that time I would talk about simply distribution. That's the one I like.

590
01:17:47,350 --> 01:17:52,900
I work quite a bit on that. So I'll tell you why this is important in my view.

591
01:17:53,410 --> 01:17:56,500
Okay. Thanks for the lecture today.

592
01:17:56,500 --> 01:18:02,560
I have officer and 330 this afternoon and finally I can come back to have my office.

593
01:18:02,820 --> 01:18:06,160
So thanks for your patience. That's it.

