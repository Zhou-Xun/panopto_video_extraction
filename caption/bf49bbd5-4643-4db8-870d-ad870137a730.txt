1
00:00:04,560 --> 00:00:11,040
Ananda Sen: Great so and also talking about recording I did.

2
00:00:11,040 --> 00:00:19,650
Ananda Sen: Release both days recordings on canvas So those are accessible not.

3
00:00:19,650 --> 00:00:22,680
Ananda Sen: Good so let's see.

4
00:00:22,680 --> 00:00:27,450
Ananda Sen: Is there anything else I need to mention before we get started.

5
00:00:27,450 --> 00:00:35,370
Ananda Sen: I don't think so um so yeah so any questions.

6
00:00:35,370 --> 00:00:42,360
Christie Flanagan: I have a quick question as a student who's taking the course for a grade I wondered when the other assignments would be available.

7
00:00:42,360 --> 00:00:51,420
Ananda Sen: So I will assign the second assignment tonight, and that will be due Friday Okay, and the third assignment will be assigned on Friday will be due on Sunday.

8
00:00:51,420 --> 00:00:52,500
Christie Flanagan: Okay, thanks.

9
00:00:52,500 --> 00:00:56,040
Ananda Sen: So yeah so that's that's the way and design it so.

10
00:00:56,040 --> 00:00:58,680
um.

11
00:00:58,680 --> 00:01:03,450
Ananda Sen: yeah.

12
00:01:03,450 --> 00:01:05,460
Ananda Sen: So my dad.

13
00:01:05,460 --> 00:01:10,710
Ananda Sen: You should have gotten a message about the recordings.

14
00:01:10,710 --> 00:01:26,610
Ananda Sen: That, if you have not then then we'll have to check into that but it automatically scan we're supposed to actually notify the students when the videos that are released, so you have not got the videos you haven't got the recording.

15
00:01:26,610 --> 00:01:36,840
Mehrdad Motamed: um so I got a message from you last night, it was around tool am but in canvas I cannot find the files, you can.

16
00:01:36,840 --> 00:01:38,700
Ananda Sen: Go to zoom.

17
00:01:38,700 --> 00:01:45,090
Ananda Sen: And cloud recordings under zoom there is something called cloud recordings.

18
00:01:45,090 --> 00:01:46,020
Ananda Sen: let's do that right now.

19
00:01:46,020 --> 00:01:53,160
Mehrdad Motamed: Yes, and they are there, I guess, against me please yes awesome Thank you.

20
00:01:53,160 --> 00:02:06,870
Ananda Sen: yeah so under zoom if it did not send you an automated message I thought that they would, but if it did not then go under zoom and go on to the cloud recordings, you should see day one, and day two.

21
00:02:06,870 --> 00:02:11,550
Mehrdad Motamed: recordings in there, so yes.

22
00:02:11,550 --> 00:02:15,480
Ananda Sen: All right, um I think that's.

23
00:02:15,480 --> 00:02:17,100
Okay.

24
00:02:17,100 --> 00:02:19,710
Ananda Sen: Yes.

25
00:02:19,710 --> 00:02:26,370
Ananda Sen: Christie sorry, did you ask me, oh yeah you asked me about the assignments now, am I yeah all right.

26
00:02:26,370 --> 00:02:52,200
Ananda Sen: Okay let's see.

27
00:02:52,200 --> 00:02:59,280
Ananda Sen: So I want to spend a little bit more time on the hypothesis testing, because it is a little I think I.

28
00:02:59,280 --> 00:03:10,050
Ananda Sen: did a little abruptly and I didn't do the whole thing we talked about this very briefly, also for linear model so generally sorry linear regression.

29
00:03:10,050 --> 00:03:25,710
Ananda Sen: But for logistic regression it's basically re emphasizing the same concept but but let's do that in the context in this context, so that we have a more consolidated.

30
00:03:25,710 --> 00:03:31,950
Ananda Sen: idea of how this this is works this works, so what is hypothesis testing hypothesis testing is everything.

31
00:03:31,950 --> 00:03:47,760
Ananda Sen: In statistics if we're not doing hypothesis testing or not brings distance so basically you are all this testing, where there is an association between X variable and why whether there is, there are two groups which are different, whether certain.

32
00:03:47,760 --> 00:03:53,790
Ananda Sen: Certain categories do not exist in the model and so on and so forth, or you're doing other types of.

33
00:03:53,790 --> 00:04:00,690
Ananda Sen: Things any genes have significant in a genetic experiment things like this, anything that hypothesis testing without hypothesis testing, you cannot.

34
00:04:00,690 --> 00:04:15,720
Ananda Sen: cannot do a full statistical analysis okay so that's it, what are the different types of tests, which are available in the context of prop logistic or broad general so logistic regression.

35
00:04:15,720 --> 00:04:23,430
Ananda Sen: That are essentially what we'll concentrate ourselves on is to look at the parameters of the model.

36
00:04:23,430 --> 00:04:36,150
Ananda Sen: and try to see if either those parameters are zero effectively, which means that the X variable is not predictive or some of the parameters are equal.

37
00:04:36,150 --> 00:04:46,290
Ananda Sen: which can happen if you have a race category where black and white and other three categories anyone to say with a black and the white are the same.

38
00:04:46,290 --> 00:04:52,290
Ananda Sen: In in in regards to the outcome.

39
00:04:52,290 --> 00:04:54,810
Ananda Sen: Okay, so.

40
00:04:54,810 --> 00:05:19,560
Ananda Sen: So that's one type of test or the type of test is that some some X variables one or more of experience, may not even exist in the moment so that's the basic premise, knowing that what type, how do we do these testings and and the way to test that out is in.

41
00:05:19,560 --> 00:05:27,690
Ananda Sen: All of these, there are three different ways, as I mentioned briefly before we are only going to concentrate on Walden likelihood ratio.

42
00:05:27,690 --> 00:05:36,210
Ananda Sen: Wild is more of a statistical test it's it's obviously something which comes with almost every procedure by default.

43
00:05:36,210 --> 00:05:52,410
Ananda Sen: So i'm not going to spend any time on the technical aspect of it likelihood ratio test is an easier to understand this, of course, again suppress the technical details, but let's see at least in prop logistic in Prague gen mode how we.

44
00:05:52,410 --> 00:05:57,540
Ananda Sen: are able to do those type of test, so I.

45
00:05:57,540 --> 00:06:01,230
Ananda Sen: Suppose my my.

46
00:06:01,230 --> 00:06:13,290
Ananda Sen: test of interest, and this is a small model where the low birth rate is is a likelihood of low birth birth rate is.

47
00:06:13,290 --> 00:06:22,560
Ananda Sen: used or or predicted by the hypertension status smoke status smoking status and age.

48
00:06:22,560 --> 00:06:30,240
Ananda Sen: And in this case you notice something both hypertension status and the smoking status are dichotomous I have not.

49
00:06:30,240 --> 00:06:40,560
Ananda Sen: declared them as class variable, and the reason for doing that is that for Class variable if I if I presented as a class variable then.

50
00:06:40,560 --> 00:07:03,150
Ananda Sen: i'm, then I do get that extra indicator setting now, that is fine, but when there are only two categories, which means just yes, no or one zero that sort of thing then setting it up as class variable or not.

51
00:07:03,150 --> 00:07:08,760
Ananda Sen: Is the same thing.

52
00:07:08,760 --> 00:07:12,750
Ananda Sen: In fact, setting it up as class variable.

53
00:07:12,750 --> 00:07:20,730
Ananda Sen: Sometimes, can be a little confusing i'll show you why but but let's first go through this.

54
00:07:20,730 --> 00:07:29,640
Ananda Sen: This exercise so so I want to test whether ages at all predictive so ages in the model.

55
00:07:29,640 --> 00:07:37,440
Ananda Sen: And you look at the basically what you do is you look at the coefficient corresponding to age and look at the p value, corresponding to age.

56
00:07:37,440 --> 00:07:52,560
Ananda Sen: And the p value, corresponding to ages 0.1206 that's it that basically says that age is not predictive in this model and and the test, that is, that is printed here the the.

57
00:07:52,560 --> 00:08:02,820
Ananda Sen: The the wild test, this is the world Chi square that actually you can calculate that, based on the other things here so it's, for example, minus 0.0504.

58
00:08:02,820 --> 00:08:13,050
Ananda Sen: If you divide that by that standard error point 03 to five and squared it that's exactly what you get 2.41 okay so that's how you get that.

59
00:08:13,050 --> 00:08:17,490
Ananda Sen: And, and then it's then it's tested against the Chi square with one degree of freedom.

60
00:08:17,490 --> 00:08:30,210
Ananda Sen: And and that's that's how you get the wii balance that's the technical part of it, but really as a practitioner, this is what you look at point 1206 and say that, but then he is not significant.

61
00:08:30,210 --> 00:08:38,970
Ananda Sen: Even if it is a negative sign, and you can interpret the coefficient everything but but really at the end of the day, if it is not significant why bother.

62
00:08:38,970 --> 00:08:41,250
Ananda Sen: Okay.

63
00:08:41,250 --> 00:08:51,960
Ananda Sen: So, so that is a wall test, there is a different type of test which I will come to in a moment.

64
00:08:51,960 --> 00:08:59,670
Ananda Sen: And this, I will explain this in the context of another problem in this, in this case so suppose the same model.

65
00:08:59,670 --> 00:09:20,310
Ananda Sen: And I want to say that well let's not worry about just a single variable let's do all of them at once, age, hypertension status smoking status, all of them equal to zero so let's try to figure out what that means, really, that means.

66
00:09:20,310 --> 00:09:28,110
Ananda Sen: The model really doesn't have any predictive X okay now.

67
00:09:28,110 --> 00:09:38,820
Ananda Sen: How are you how would I set that up the H not is there, which says that the beta coefficients corresponding to all three variables are equal to zero simultaneously.

68
00:09:38,820 --> 00:09:56,400
Ananda Sen: Now every hypothesis comes with an alternative, what will be the alternative what will be the compliment of this compliment will be at least one of them is non zero at least one is predictive I don't know which one, but at least one is predict.

69
00:09:56,400 --> 00:10:11,790
Ananda Sen: that's what I wrote in each one just in mathematical terms but but really what I mean there is something like.

70
00:10:11,790 --> 00:10:16,050
Ananda Sen: At least one.

71
00:10:16,050 --> 00:10:19,470
Is predictive.

72
00:10:19,470 --> 00:10:23,820
Ananda Sen: that's what each one's.

73
00:10:23,820 --> 00:10:31,470
Ananda Sen: um how do I do that in proper logistic it's actually there is nothing to do.

74
00:10:31,470 --> 00:10:42,660
Ananda Sen: You just you just run it and and and that will print these these tests see it says global null hypothesis beta equals zero global means everything is equal to zero, the same time.

75
00:10:42,660 --> 00:10:45,840
Ananda Sen: likelihood ratio.

76
00:10:45,840 --> 00:10:57,930
Ananda Sen: test score test walters all three tests, we talked about it says what is the conclusion, this is the important point, what is the conclusion point 009 8.009 5.01 for it.

77
00:10:57,930 --> 00:11:09,870
Ananda Sen: They all say that the null hypothesis rejected, which means there is at least one significant cooperate, maybe, maybe two, but at least one.

78
00:11:09,870 --> 00:11:12,720
Ananda Sen: that's the interpretation.

79
00:11:12,720 --> 00:11:14,490
Ananda Sen: That makes sense.

80
00:11:14,490 --> 00:11:16,800
Ananda Sen: Okay.

81
00:11:16,800 --> 00:11:18,390
Ananda Sen: Now.

82
00:11:18,390 --> 00:11:23,190
Ananda Sen: One thing I want to point out here is that.

83
00:11:23,190 --> 00:11:26,580
Ananda Sen: This particular test.

84
00:11:26,580 --> 00:11:32,910
Ananda Sen: Which is the global test is usually comes, by default, with any procedure.

85
00:11:32,910 --> 00:11:44,640
Ananda Sen: If you have to do us a test of a subset of variables like we did for age well single variables and all variables are available without having to do any other.

86
00:11:44,640 --> 00:11:54,840
Ananda Sen: Additional thing but it's the setup for various is when things are different so so, for example.

87
00:11:54,840 --> 00:12:09,510
Ananda Sen: Suppose a want to test oh one other thing I just wanted to make sure they understand this somebody asked me about the degrees of freedom so here's the degrees of freedom is three.

88
00:12:09,510 --> 00:12:16,560
Ananda Sen: Why is it three.

89
00:12:16,560 --> 00:12:18,780
Theresa Marie Kowalski-Dobson: Because you have three variables.

90
00:12:18,780 --> 00:12:20,340
Ananda Sen: Because you have three variables.

91
00:12:20,340 --> 00:12:22,680
Ananda Sen: And you're you're.

92
00:12:22,680 --> 00:12:36,300
Ananda Sen: you're just setting it up as as three separate tests essentially essentially three separate tests, but you're combining everything yes that's why so so that's something which.

93
00:12:36,300 --> 00:12:38,490
Ananda Sen: Every Member all right now.

94
00:12:38,490 --> 00:12:45,240
Yoonhee Ryder: i'm sorry, can I ask one question, so how did you know again that one of them at least is predictive just by looking at the.

95
00:12:45,240 --> 00:12:45,810
Yoonhee Ryder: value.

96
00:12:45,810 --> 00:12:47,760
Ananda Sen: Now, yes okay.

97
00:12:47,760 --> 00:12:51,360
Ananda Sen: This be very significant okay.

98
00:12:51,360 --> 00:12:57,060
Thank you yeah.

99
00:12:57,060 --> 00:13:08,370
Ananda Sen: Okay, so if the p value is less than point five, then the null hypothesis is rejected and rejected means that at least one of them is.

100
00:13:08,370 --> 00:13:16,770
Ananda Sen: Okay, so suppose I want to do a likelihood ratio test for a set of covariance and let's take an example that.

101
00:13:16,770 --> 00:13:36,360
Ananda Sen: let's say I wanted to test beta smoke equal to zero and beta hd equal to zero, but I don't care about age adjust for each so the age will always be in the model and beta smoking article Zealand or beta hd not equal to zero again adjusting for age.

102
00:13:36,360 --> 00:13:44,040
Ananda Sen: So what is likelihood ratio test, so the likelihood ratio test is essentially.

103
00:13:44,040 --> 00:13:53,130
Ananda Sen: So again, we very briefly talked about the light the likelihood is that the data it's a way of the data telling.

104
00:13:53,130 --> 00:14:02,130
Ananda Sen: The model that given this data How likely is this model, going to be the right model.

105
00:14:02,130 --> 00:14:05,700
Ananda Sen: So, based on the likelihood of the data.

106
00:14:05,700 --> 00:14:13,380
Ananda Sen: You run the model and that's that's and the log of that is is basically.

107
00:14:13,380 --> 00:14:19,560
Ananda Sen: Your your likelihood of the model.

108
00:14:19,560 --> 00:14:22,320
Ananda Sen: Based on this data.

109
00:14:22,320 --> 00:14:36,360
Ananda Sen: Now there are two models, we are actually looking at one is the null hypothesis which says that neither smoke nor he is in the in the model, which is the first model model zero.

110
00:14:36,360 --> 00:14:46,500
Ananda Sen: See, there is no smokes noise he just these these two it's a simple logistic regression model not linear but simple logistic okay.

111
00:14:46,500 --> 00:15:01,260
Ananda Sen: That this model H1 basically says, well, I I don't know either of them could be nonzero book could be non zero so the full of model model, one model under each one is the full model.

112
00:15:01,260 --> 00:15:03,990
Ananda Sen: Eight smoke hd.

113
00:15:03,990 --> 00:15:08,190
Ananda Sen: So if there is a model that has a likelihood.

114
00:15:08,190 --> 00:15:19,560
Ananda Sen: And you can run this twice Brock logistic actually prints out the likelihood, so the log likelihood of the model lock likelihood of the model.

115
00:15:19,560 --> 00:15:41,790
Ananda Sen: So this is the model that intercept plus age, this is the model with with the full model, and if you if you if you read out those likelihoods a log like minus to log likelihoods from the from the output, you would get these two numbers and those to give you 8.631 that difference.

116
00:15:41,790 --> 00:15:53,460
Ananda Sen: What should you compare that with Chi square with two degrees of freedom, why two degrees of freedom because, again, there are two variables we're testing beta smoking aged.

117
00:15:53,460 --> 00:16:06,060
Ananda Sen: So if you compare that with bicycle two degrees of freedom, you get a P value of point 0134 which basically says one of the smoker hd at least one of the smoker hd versus significant.

118
00:16:06,060 --> 00:16:07,680
Ananda Sen: Okay.

119
00:16:07,680 --> 00:16:10,500
Ananda Sen: Now, obviously.

120
00:16:10,500 --> 00:16:13,560
Ananda Sen: This is something which.

121
00:16:13,560 --> 00:16:15,240
Ananda Sen: Is.

122
00:16:15,240 --> 00:16:27,360
Ananda Sen: The mechanism, how the likelihood ratio test works, but it would be so nice if this is done nicely directly in SAS and, indeed, you can do that.

123
00:16:27,360 --> 00:16:31,140
Ananda Sen: But for that you need to use prop general.

124
00:16:31,140 --> 00:16:46,740
Ananda Sen: it's just a it's just a some kind of idiosyncrasy with with with the software's there these procedures in all software's and generated by different groups, and they have different purposes.

125
00:16:46,740 --> 00:16:50,520
Ananda Sen: Brock logistic.

126
00:16:50,520 --> 00:17:13,860
Ananda Sen: Typically it's it's very easy to generate walls type of test problem gen mode it's not it's also dangerous wall type of test but they're the default is likelihood ratio test, you can get wild type of test there too, but, like you, Richard is default, so what I do here is I you see.

127
00:17:13,860 --> 00:17:21,630
Ananda Sen: I use this statement, this is an important statement.

128
00:17:21,630 --> 00:17:26,520
Ananda Sen: And this essentially.

129
00:17:26,520 --> 00:17:39,180
Ananda Sen: makes the coding so just just use that whenever you need to do any any type of testing if you don't do any testing then don't have to worry about this, but if you do any type of testing, use that.

130
00:17:39,180 --> 00:18:02,040
Ananda Sen: parameter parameter ization is effect pyramidal effect, and then the model function, the model statement is the same as before, but you add a contrast statement that contrast statement says well i'm testing both hypertension and and smoke and that one one essentially says that.

131
00:18:02,040 --> 00:18:11,160
Ananda Sen: i'm just testing it to zero that's that's the that's the format of using this now um.

132
00:18:11,160 --> 00:18:23,130
Ananda Sen: whenever you use a contrast statement, you are going to get a likelihood ratio test and the likelihood ratio test turns out to be in this case exactly 8.63 which I got before.

133
00:18:23,130 --> 00:18:28,950
Ananda Sen: But I got using the two likelihoods and.

134
00:18:28,950 --> 00:18:34,890
Ananda Sen: And the p value is point 0134.

135
00:18:34,890 --> 00:18:38,760
Okay.

136
00:18:38,760 --> 00:18:51,540
Any question.

137
00:18:51,540 --> 00:18:56,700
Ananda Sen: So this is a test, this is the likelihood ratio test for.

138
00:18:56,700 --> 00:19:03,630
Ananda Sen: hypertension and smoke to be simultaneously equal to zero vs one of them is non zero.

139
00:19:03,630 --> 00:19:14,310
Ananda Sen: And previously the test was everything is zero including age, so this is agent justin comparison between not comparison age adjusted.

140
00:19:14,310 --> 00:19:18,870
Ananda Sen: test of hd and smoke to equal to zero.

141
00:19:18,870 --> 00:19:21,720
Ananda Sen: is how you do the lakers.

142
00:19:21,720 --> 00:19:24,540
Okay.

143
00:19:24,540 --> 00:19:38,700
Ananda Sen: All right, and while test, if you have to do, while just right here ah after hd one you just throw in walled slash world.

144
00:19:38,700 --> 00:19:42,990
Ananda Sen: Right.

145
00:19:42,990 --> 00:19:53,460
Ananda Sen: Right, I want to do a small exercise this is less SAS but more common application type of thing.

146
00:19:53,460 --> 00:20:00,750
Ananda Sen: see in this I have used age of the continuous variable what does it mean.

147
00:20:00,750 --> 00:20:22,740
Ananda Sen: We don't use something as a continuous variable my basic assumption not ages, of course, insignificant but but let's say i'm I have a I have a case where it's possible that I, I have a nonlinear structure and the linear.

148
00:20:22,740 --> 00:20:31,830
Ananda Sen: structure for each the continuous structure for is a single variable for age actually does not capture the change.

149
00:20:31,830 --> 00:20:43,410
Ananda Sen: Now it's possible So how do you check for that the way to check, for that is well what not only way to check for that, but, but one of the ways to check, for that is.

150
00:20:43,410 --> 00:21:02,250
Ananda Sen: How about divide up age, whatever age, you have God in your in your data divided up into groups and have separate parameters for each of the groups and see if the if the parameters either increase or decrease, you know what I mean so Basically, there is a trend.

151
00:21:02,250 --> 00:21:06,270
Ananda Sen: In those in those parameters so.

152
00:21:06,270 --> 00:21:10,380
Ananda Sen: So this is what I do now.

153
00:21:10,380 --> 00:21:13,080
Ananda Sen: So what you can do I mean this is just an example.

154
00:21:13,080 --> 00:21:24,600
Theresa Marie Kowalski-Dobson: Okay, can I ask you a question yeah is this, I know you've just started getting into it, but i'm trying to frame it my mind, is this only for continuous variables, or would this be okay.

155
00:21:24,600 --> 00:21:25,410
Theresa Marie Kowalski-Dobson: Yes, yeah.

156
00:21:25,410 --> 00:21:37,230
Ananda Sen: that's a BMI blood pressure, you want it, you want to make sure that you're not missing any pattern, maybe, maybe i'm maybe the.

157
00:21:37,230 --> 00:21:46,170
Ananda Sen: outcome is on increasing function of age up until the point, and then it flattens out.

158
00:21:46,170 --> 00:21:54,780
Ananda Sen: You know, then trying to fit a linear function over the whole space range of age is not correct.

159
00:21:54,780 --> 00:22:01,860
Ananda Sen: So what if I got what if I got a pattern which looks like this.

160
00:22:01,860 --> 00:22:05,910
Ananda Sen: say this is the probability.

161
00:22:05,910 --> 00:22:08,880
This is age.

162
00:22:08,880 --> 00:22:15,270
Ananda Sen: If I feed a linear linear pattern then it'll go all the way and I.

163
00:22:15,270 --> 00:22:35,460
Ananda Sen: I will have no way of knowing that it's it's it's a it's a pattern like so it's it's actually not a bad idea to do to try out, especially when you see that there is insignificance trying out small little checks like this.

164
00:22:35,460 --> 00:22:38,760
Ananda Sen: So how do you do that.

165
00:22:38,760 --> 00:22:54,510
Ananda Sen: quarters of age distribution is is is reasonable place to start so basically divide up your age in your data into for equal groups, according to the values ordered values.

166
00:22:54,510 --> 00:23:02,430
Ananda Sen: And if the same model, but now, with three different parameters for the three dummy variables for the groups.

167
00:23:02,430 --> 00:23:16,200
Ananda Sen: And if linear consumption is reasonable, then you expect the beta had to have jumps and going up or going down in some some order.

168
00:23:16,200 --> 00:23:24,720
Ananda Sen: So so that's what we tried here and and there is a simple way of.

169
00:23:24,720 --> 00:23:28,500
Ananda Sen: Creating creating.

170
00:23:28,500 --> 00:23:39,420
Ananda Sen: What files so so our program is a way to create for equal groups, and this is the age is the variable I do that and then.

171
00:23:39,420 --> 00:23:55,080
Ananda Sen: I set out a data all data out to out to which uses out one and then H quantel equal to zero, H quantel equal to one all of those are dummy variables and I have created for dummy variables i'm not going to use all four.

172
00:23:55,080 --> 00:24:10,860
Ananda Sen: i'm going to use three of them, but I just created all of them, so so that there's no problem and then instead of continuous age run the model runner gen mode with those dummy variables and if you do that, then this is what I get.

173
00:24:10,860 --> 00:24:25,170
Ananda Sen: So look at the age coefficients point 251 6.6236 minus 0.4652 so clearly, this is not incremental it goes up and then comes down.

174
00:24:25,170 --> 00:24:28,740
Ananda Sen: Obviously, this is all.

175
00:24:28,740 --> 00:24:41,520
Ananda Sen: With comparison to the baseline age, one which is the lowest quarter quarter quarter of age, but but nonetheless it is, it is definitely not a linear pattern.

176
00:24:41,520 --> 00:24:45,660
Ananda Sen: So you know I mean I mean that's a very easy quick check.

177
00:24:45,660 --> 00:24:59,640
Ananda Sen: And and seeing this case, nothing is significant so So if you look at the p values in it whatsoever, and this is, this is a moot exercise because it's not it's not going to significant anyway so.

178
00:24:59,640 --> 00:25:15,090
Ananda Sen: But nonetheless it's it's it's a it's in general, a good example of how to look at a continuous variable and and check linearity, and this is true not only for logistic regression for integration.

179
00:25:15,090 --> 00:25:25,920
Theresa Marie Kowalski-Dobson: And what wouldn't something looks like if it was like very it's like H3 was obviously much different than the others, would it be like just a much higher number or much lower number.

180
00:25:25,920 --> 00:25:27,180
Ananda Sen: If it is linear.

181
00:25:27,180 --> 00:25:37,140
Ananda Sen: Yes, well, first of all, if it is linear, then you would expect that the age variable would actually come out to be significant the trend so so so when.

182
00:25:37,140 --> 00:25:38,430
Ananda Sen: When you.

183
00:25:38,430 --> 00:25:48,270
Ananda Sen: yeah that would be the first thing this is basically trying to see if I did not see linearity but is there a pattern i'm missing.

184
00:25:48,270 --> 00:25:51,030
Ananda Sen: And and one could say that while there may be a.

185
00:25:51,030 --> 00:25:58,080
Ananda Sen: You know it's it's probably something like pattern like this, but.

186
00:25:58,080 --> 00:26:16,380
Ananda Sen: But, but nothing is significant here so so you don't really want to go farther into into this or deep dive into this other than said oh yeah there is some change in the second quarter, that is, that seems like that's farthest apart from from the others, but.

187
00:26:16,380 --> 00:26:18,390
Ananda Sen: Still it's not.

188
00:26:18,390 --> 00:26:21,570
Thanks.

189
00:26:21,570 --> 00:26:25,080
Ananda Sen: So.

190
00:26:25,080 --> 00:26:30,990
Ananda Sen: One last type of test which is useful.

191
00:26:30,990 --> 00:26:46,620
Ananda Sen: So we are in this scenario of age to age age age is a categorical variable four categories SUP only want to test tool, the categories, I want to say that, by don't think that four categories is good, I think, two of the categories could be collapsed.

192
00:26:46,620 --> 00:27:03,570
Ananda Sen: Then he would test something like beta H2 and H3 let's H2 and H3 those to be that should be the same, I can do something like that now, if you want to do something like that, then there is the estimate statement which were used before you could use it here.

193
00:27:03,570 --> 00:27:15,270
Ananda Sen: Proper logistic did attempt model low event one I hd smoke H2 H3 H4 and an estimate statement it's very similar to Prague glm estimate statement.

194
00:27:15,270 --> 00:27:31,500
Ananda Sen: H 2183 minus one, so one and minus one i'm just treating the difference to be equal to zero and, and the reason it is one and minus one is that you can also read right ah, not as this.

195
00:27:31,500 --> 00:27:47,490
Ananda Sen: beta age to minus beta age three equals zero C, so one minus one just look at the coefficient of beta H, two and three days and that's what shows up there.

196
00:27:47,490 --> 00:27:58,770
Ananda Sen: And if you just use it that way you would end up getting the test for that and the test of course shows that this is they're not they're not.

197
00:27:58,770 --> 00:28:06,660
Ananda Sen: they're not different so the the larger P value larger than point 5.5 means that.

198
00:28:06,660 --> 00:28:12,240
Ananda Sen: Each node is not rejected, so you could treat them as the same.

199
00:28:12,240 --> 00:28:28,470
Ananda Sen: So that's I just want to point out something, this is a little different than saying that the age coefficient of zero it's just saying that age question may or may not be zero you're not testing that what you're testing is be the two.

200
00:28:28,470 --> 00:28:31,320
Ananda Sen: categories of the same.

201
00:28:31,320 --> 00:28:37,500
Ananda Sen: Okay, and this.

202
00:28:37,500 --> 00:28:44,280
Ananda Sen: But particular output does not show that but can you.

203
00:28:44,280 --> 00:29:07,980
Ananda Sen: guess what the degrees of freedom for this one.

204
00:29:07,980 --> 00:29:13,350
Christie Flanagan: Would you count the age core tiles that you did as as their own variable.

205
00:29:13,350 --> 00:29:15,390
Yes.

206
00:29:15,390 --> 00:29:18,180
Ananda Sen: Yes, so sorry good.

207
00:29:18,180 --> 00:29:29,370
Christie Flanagan: I didn't say looking at this, you have the hypertension smoking age 234 but then there's also the kotel that's not present this was our six.

208
00:29:29,370 --> 00:29:32,610
Ananda Sen: Well, actually actually just count the variables.

209
00:29:32,610 --> 00:29:36,090
really why.

210
00:29:36,090 --> 00:29:39,300
Christie Flanagan: The willingness to be three then.

211
00:29:39,300 --> 00:29:47,760
Ananda Sen: No it's a No six is fine, because what you don't see here is there is another variable which is called the intercept you don't see that here.

212
00:29:47,760 --> 00:29:48,870
Christie Flanagan: Okay, so sick.

213
00:29:48,870 --> 00:29:56,670
Ananda Sen: So it is six yeah Okay, and then that six variables are not in the entire model.

214
00:29:56,670 --> 00:30:00,300
Ananda Sen: But what is, what is the test degrees of freedom.

215
00:30:00,300 --> 00:30:03,900
Christie Flanagan: I thought you said that the degrees of freedom or equal to the number of variables.

216
00:30:03,900 --> 00:30:08,340
Ananda Sen: That we were testing number of very mature testing.

217
00:30:08,340 --> 00:30:13,230
Ananda Sen: So so here.

218
00:30:13,230 --> 00:30:16,740
Ananda Sen: The degrees of freedom was three because you're testing three variables.

219
00:30:16,740 --> 00:30:20,430
Ananda Sen: In age equal to zero beta H equals zero beta smoke equal to zero.

220
00:30:20,430 --> 00:30:21,870
Ananda Sen: There are three tests.

221
00:30:21,870 --> 00:30:23,190
Christie Flanagan: I see.

222
00:30:23,190 --> 00:30:25,830
Ananda Sen: So here, given that.

223
00:30:25,830 --> 00:30:35,370
Ananda Sen: What do you think the degrees of freedom for this one.

224
00:30:35,370 --> 00:30:39,180
Christie Flanagan: Is it too.

225
00:30:39,180 --> 00:30:41,160
Christie Flanagan: I.

226
00:30:41,160 --> 00:30:42,270
Ananda Sen: Would.

227
00:30:42,270 --> 00:30:54,600
Christie Flanagan: Just i'm looking at the the beta age two equals beta age three verses are way oh wait no no i'm confusing myself, because those are the two hypotheses.

228
00:30:54,600 --> 00:31:01,080
Ananda Sen: How many how many hypothesis are here, is it too, or is it one.

229
00:31:01,080 --> 00:31:04,470
Christie Flanagan: You have your nolan your alternative so that would be.

230
00:31:04,470 --> 00:31:12,120
Ananda Sen: No, no, no, just the not just the not just how many tests are there is it just one test or this.

231
00:31:12,120 --> 00:31:14,070
Christie Flanagan: one.

232
00:31:14,070 --> 00:31:16,350
Ananda Sen: Is one.

233
00:31:16,350 --> 00:31:18,660
Ananda Sen: degree of freedom should be one.

234
00:31:18,660 --> 00:31:20,400
Christie Flanagan: Okay, so it is just one test.

235
00:31:20,400 --> 00:31:21,750
Ananda Sen: yeah.

236
00:31:21,750 --> 00:31:22,740
Ananda Sen: he's very good.

237
00:31:22,740 --> 00:31:30,240
Christie Flanagan: You don't count your know as a as a hypothesis, and your alternative hypothesis, you would just count those as one hypothesis.

238
00:31:30,240 --> 00:31:45,270
Ananda Sen: Yes, because the alternative is always determined by the now whatever is is the compliment of now is the alternative you you just look count the number of tests number of independent tests you're doing so what I wanted to point out is that.

239
00:31:45,270 --> 00:32:09,180
This is different.

240
00:32:09,180 --> 00:32:17,190
Ananda Sen: On the left hand side, what I what I wrote our beta to equal to beta three equals zero that is actually two tests.

241
00:32:17,190 --> 00:32:23,220
Ananda Sen: Because it's saying both of them are equal to zero, that is different from beta H2 equal to be dates three.

242
00:32:23,220 --> 00:32:27,810
Ananda Sen: Because, maybe it's too little bit so just means that you don't care whether or not.

243
00:32:27,810 --> 00:32:29,880
they're equal.

244
00:32:29,880 --> 00:32:35,250
Ananda Sen: To see the distinction, so this one will have one degree of freedom.

245
00:32:35,250 --> 00:32:40,920
Ananda Sen: And this one will have two degrees of freedom.

246
00:32:40,920 --> 00:32:46,980
Ananda Sen: Okay.

247
00:32:46,980 --> 00:32:56,790
Okay.

248
00:32:56,790 --> 00:33:06,840
Ananda Sen: So.

249
00:33:06,840 --> 00:33:21,030
Ananda Sen: Can we do a small exercise before we move on to the third let's do this small exercise in fact I don't know how many of you are able to pull it up now, but let me.

250
00:33:21,030 --> 00:33:24,030
Ananda Sen: Go to my.

251
00:33:24,030 --> 00:33:28,470
Ananda Sen: i'm.

252
00:33:28,470 --> 00:33:45,000
Ananda Sen: SAS on demand.

253
00:33:45,000 --> 00:34:08,070
Ananda Sen: Okay, where is my success on demand.

254
00:34:08,070 --> 00:34:13,170
Ananda Sen: All right.

255
00:34:13,170 --> 00:34:17,040
Sure screen.

256
00:34:17,040 --> 00:34:19,710
Ananda Sen: Can you see my.

257
00:34:19,710 --> 00:34:29,790
Ananda Sen: screen yeah okay good all right so let's see.

258
00:34:29,790 --> 00:34:38,310
Geehan Suleyman: I went to the website and I can't download the slides for some reason.

259
00:34:38,310 --> 00:34:40,440
Ananda Sen: You still can download the slides.

260
00:34:40,440 --> 00:34:45,120
Geehan Suleyman: And looks completely different than yesterday um.

261
00:34:45,120 --> 00:34:48,540
Ananda Sen: Did you go to files.

262
00:34:48,540 --> 00:34:52,830
Ananda Sen: lecture yes, I actually removed all the links from homepage.

263
00:34:52,830 --> 00:34:57,990
Ananda Sen: Okay yeah that's what I mentioned in the beginning.

264
00:34:57,990 --> 00:35:02,340
Ananda Sen: Okay yeah you have to go to the file.

265
00:35:02,340 --> 00:35:05,400
Ananda Sen: yeah.

266
00:35:05,400 --> 00:35:24,390
Ananda Sen: because that was the source of confusion or problem alright so i'm signing in and.

267
00:35:24,390 --> 00:35:27,330
Ananda Sen: And let's do this.

268
00:35:27,330 --> 00:35:31,350
Ananda Sen: I have the data set already in so all I need is the.

269
00:35:31,350 --> 00:35:38,910
Ananda Sen: coordinates so.

270
00:35:38,910 --> 00:35:42,780
Ananda Sen: I have a.

271
00:35:42,780 --> 00:35:55,830
Software codes lab to.

272
00:35:55,830 --> 00:36:02,130
Ananda Sen: All right.

273
00:36:02,130 --> 00:36:05,100
Ananda Sen: that's my name.

274
00:36:05,100 --> 00:36:12,090
Geehan Suleyman: So I will go.

275
00:36:12,090 --> 00:36:34,860
Ananda Sen: Okay.

276
00:36:34,860 --> 00:36:40,230
Ananda Sen: Alright, so I think I have read the beta.

277
00:36:40,230 --> 00:36:57,720
Ananda Sen: So the questions were How do you assess the overall race effect in a model containing age hd smoking race Okay, so I was trying to do something like that here.

278
00:36:57,720 --> 00:37:06,330
Ananda Sen: And then, in the second question was how do you test whether the effect of white is the same as the effect of black adjusting for age hd and small.

279
00:37:06,330 --> 00:37:18,960
Ananda Sen: suppose you want to understand, ask the first question and not worry about the rest at this point so SAS, this is the way to comment anything out so.

280
00:37:18,960 --> 00:37:21,810
Ananda Sen: let's not worry about that for now.

281
00:37:21,810 --> 00:37:24,570
Sorry.

282
00:37:24,570 --> 00:37:41,550
Ananda Sen: So I just do have the hd smoke age and race, in the in the proper logistic command and I have the hd small Chris all of them declare it class variables I don't really have to do that, but well race, I do because race, there are three races.

283
00:37:41,550 --> 00:37:46,440
Ananda Sen: So I just would.

284
00:37:46,440 --> 00:37:56,070
Ananda Sen: run this.

285
00:37:56,070 --> 00:37:59,340
Why does it not like it.

286
00:37:59,340 --> 00:38:09,210
Ananda Sen: Take to me.

287
00:38:09,210 --> 00:38:25,290
For some reason it didn't like.

288
00:38:25,290 --> 00:38:29,850
Okay.

289
00:38:29,850 --> 00:38:37,560
Ananda Sen: Oh, that was genuine nevermind i'm sorry, so this was Mrs.

290
00:38:37,560 --> 00:38:45,180
Ananda Sen: Okay, so.

291
00:38:45,180 --> 00:38:48,780
Ananda Sen: I.

292
00:38:48,780 --> 00:39:14,160
Ananda Sen: Have in the middle, I have got hd smoke agent race all of those separate analysis so point 057 1.003 1.2860 and point 02 to three so apart from age, everything is significant hd is borderline using using walled Chi Square.

293
00:39:14,160 --> 00:39:19,980
Ananda Sen: That okay.

294
00:39:19,980 --> 00:39:26,760
Ananda Sen: So you do get the likelihood ratio test, I do have to use Prague gen mode but i'm not worried about that right now.

295
00:39:26,760 --> 00:39:29,310
Ananda Sen: So.

296
00:39:29,310 --> 00:39:47,400
Ananda Sen: It does the does print out the global tests, all three types likelihood ratio and score and world and that's just above that this is the global global means what everything is zero apart from the intercept.

297
00:39:47,400 --> 00:39:50,970
Ananda Sen: And these are the individual variables.

298
00:39:50,970 --> 00:39:59,280
Ananda Sen: So those two are fine Now I want to know whether white and black are different, are you with me up to this point, is there any question.

299
00:39:59,280 --> 00:40:13,230
Ananda Sen: So this table testing global null hypothesis that test everything in and look at the degrees of freedom five, why is it five because if you go back.

300
00:40:13,230 --> 00:40:21,330
Ananda Sen: To the to the cord there hd smoke age race.

301
00:40:21,330 --> 00:40:30,930
Ananda Sen: Why why, why is it okay let's let's do this exercise, why is the degree of freedom five.

302
00:40:30,930 --> 00:40:37,140
Ananda Sen: it's a good exercise actually.

303
00:40:37,140 --> 00:40:39,360
Ananda Sen: What are we testing.

304
00:40:39,360 --> 00:40:43,500
Ananda Sen: When we go here.

305
00:40:43,500 --> 00:40:46,050
Ananda Sen: This one.

306
00:40:46,050 --> 00:40:48,840
Ananda Sen: Well, first of all yeah sorry.

307
00:40:48,840 --> 00:41:03,780
Yoonhee Ryder: i'm just gonna say if it's a global test, it would be like H equals zero is one test smoke zero second test age equals zero is the third test and then.

308
00:41:03,780 --> 00:41:07,230
Yoonhee Ryder: there's two races so it'd be.

309
00:41:07,230 --> 00:41:16,290
Yoonhee Ryder: Three races yeah three races within that would be six so.

310
00:41:16,290 --> 00:41:18,990
That one i'm not sure.

311
00:41:18,990 --> 00:41:19,830
Ananda Sen: You.

312
00:41:19,830 --> 00:41:22,380
Ananda Sen: know you you actually.

313
00:41:22,380 --> 00:41:24,300
Ananda Sen: got.

314
00:41:24,300 --> 00:41:35,310
Ananda Sen: This great wall, one of the hidden says that if you look at the title and less effects and add up the degrees of freedom here that should add up to five so that's one thing, so why is raised two and not three.

315
00:41:35,310 --> 00:41:44,880
Ananda Sen: that's the question it's not three because remember the way we do this way, we have devised this it's always being checked.

316
00:41:44,880 --> 00:41:46,680
Ananda Sen: With.

317
00:41:46,680 --> 00:41:48,510
Ananda Sen: With the.

318
00:41:48,510 --> 00:41:52,260
Ananda Sen: What would say, with the baseline the baseline is other.

319
00:41:52,260 --> 00:42:04,260
Ananda Sen: So, so one of the tests is white equal to other the other test is black equal to other that's what the models of the that's what the beta is for the model is.

320
00:42:04,260 --> 00:42:25,530
Ananda Sen: Okay, and and other is actually intercept intercept is always in the model you're not going to remove that so race is actually whether there is a race when we say there is no race effect, all we mean is that there is that the races don't have any difference.

321
00:42:25,530 --> 00:42:32,790
Ananda Sen: The model always will include the three races one to the entire seven intersect will always stay.

322
00:42:32,790 --> 00:42:48,600
Ananda Sen: In the model so that's that's how to look at look if there if there is a categorical variable with five levels, then the degrees of freedom would be, for, if you want to test that to be equal to the no effect for that same is true.

323
00:42:48,600 --> 00:42:55,320
Ananda Sen: Set point clear.

324
00:42:55,320 --> 00:42:57,540
Ananda Sen: that's why we have.

325
00:42:57,540 --> 00:43:01,530
Ananda Sen: Five this.

326
00:43:01,530 --> 00:43:07,350
Ananda Sen: or five degrees of i'm sorry.

327
00:43:07,350 --> 00:43:09,780
Ananda Sen: Okay.

328
00:43:09,780 --> 00:43:11,760
Ananda Sen: Okay.

329
00:43:11,760 --> 00:43:25,470
Ananda Sen: That gives with a test for overall race effect in the model containing age hd smoking race and indeed the the The conclusion is that there is.

330
00:43:25,470 --> 00:43:33,540
It.

331
00:43:33,540 --> 00:43:47,250
Ananda Sen: Overall yeah and and the conclusion is that there is indeed summaries effect because it is point 02 to three is there be then okay now.

332
00:43:47,250 --> 00:43:55,320
Ananda Sen: I want to dig deeper, I want to know whether white race and the black race will have the same predicted probability.

333
00:43:55,320 --> 00:44:05,010
Ananda Sen: Alright So how do I do that and the way to do that would be actually Invoking this command, which I.

334
00:44:05,010 --> 00:44:06,720
Ananda Sen: Sorry.

335
00:44:06,720 --> 00:44:09,030
Ananda Sen: which I just.

336
00:44:09,030 --> 00:44:15,510
Ananda Sen: commented out.

337
00:44:15,510 --> 00:44:19,320
Ananda Sen: So this is the one estimate statement.

338
00:44:19,320 --> 00:44:27,180
Ananda Sen: This is very, very similar to what we have for log prob glm, so there is no change Ray says three categories.

339
00:44:27,180 --> 00:44:38,850
Ananda Sen: And we are looking at white versus black, so why does one black is two other is three, so I will just use it at a string one minus one zero.

340
00:44:38,850 --> 00:44:47,040
Ananda Sen: All right, that's it.

341
00:44:47,040 --> 00:44:50,730
Ananda Sen: So let's see what did we get anything new.

342
00:44:50,730 --> 00:45:06,870
Ananda Sen: Yes indeed, so white versus black the estimate is negative 0.9539 and and probability and the p value is just borderline point oh five six.

343
00:45:06,870 --> 00:45:22,560
Ananda Sen: By the way, when I say estimate remember this is not linear model, this is a logistic regression model so that's a difference in log log odds not not odds, but law gods so.

344
00:45:22,560 --> 00:45:27,450
Ananda Sen: So in this case, they would say that the white.

345
00:45:27,450 --> 00:45:36,930
Ananda Sen: Mothers have a slightly lower law gods than black mothers to have.

346
00:45:36,930 --> 00:45:43,050
Ananda Sen: To have a little bit.

347
00:45:43,050 --> 00:45:52,050
Ananda Sen: Okay.

348
00:45:52,050 --> 00:45:58,110
Ananda Sen: Any question on this, I will now move to the third lab or the third.

349
00:45:58,110 --> 00:46:07,080
Ananda Sen: Collective sensor.

350
00:46:07,080 --> 00:46:09,240
Ananda Sen: Okay.

351
00:46:09,240 --> 00:46:13,500
Ananda Sen: So let's stop this Shirin.

352
00:46:13,500 --> 00:46:54,450
Ananda Sen: And let's move back to the sharing from the iPad.

353
00:46:54,450 --> 00:47:06,150
Ananda Sen: Day three.

354
00:47:06,150 --> 00:47:10,470
Ananda Sen: So let me start with this.

355
00:47:10,470 --> 00:47:14,850
Ananda Sen: um.

356
00:47:14,850 --> 00:47:30,750
Ananda Sen: You come from different backgrounds, so this is, this is why i'm asking you this, and some of you have worked with the applications which might use this so how many of you.

357
00:47:30,750 --> 00:47:34,560
Ananda Sen: How many of you have.

358
00:47:34,560 --> 00:47:41,160
Ananda Sen: heard about sensitivity and specificity.

359
00:47:41,160 --> 00:47:45,930
Ananda Sen: yeah so pretty much everybody.

360
00:47:45,930 --> 00:48:06,900
Ananda Sen: So this is this is kind of the bread and butter of any diagnostic tests right, so you define you define a new test and you want to whether it's a whether it's for detecting a condition, whether it's for detecting a particular disease.

361
00:48:06,900 --> 00:48:19,890
Ananda Sen: Whatever it is, it is essentially the property of a test diagnostic test and the sensitivity is true positive rate, which is the proportion of effective people.

362
00:48:19,890 --> 00:48:32,550
Ananda Sen: With positive test results specificity, is the proportion of non effective people with negative test results and and and the predictive values are the flips so.

363
00:48:32,550 --> 00:48:43,890
Ananda Sen: The difference between sensitivity and positive predictive value, and this is something which is very important to understand when a test is being developed.

364
00:48:43,890 --> 00:48:48,390
Ananda Sen: The researchers are the ones who are who are.

365
00:48:48,390 --> 00:48:56,730
Ananda Sen: chemists and biologists and experimenters and clinical scientists, they are more.

366
00:48:56,730 --> 00:49:03,510
Ananda Sen: interested in the sensitivity of the test, what is the sensitivity specificity, so what is the sensitivity of the test.

367
00:49:03,510 --> 00:49:22,020
Ananda Sen: If you find a disease person or a person with a condition or disease and you do this test on that person you want a high likelihood of the person to be detected having the disease.

368
00:49:22,020 --> 00:49:32,070
Ananda Sen: Similarly for specificity to negative rate if the person does not have the disease, you want the test to say no, I cannot take the disease.

369
00:49:32,070 --> 00:49:37,380
Ananda Sen: You want that to be a high probability to or high chances to.

370
00:49:37,380 --> 00:49:39,960
Ananda Sen: Now.

371
00:49:39,960 --> 00:49:45,180
Ananda Sen: Think of it from a patient's point of view.

372
00:49:45,180 --> 00:49:54,840
Ananda Sen: A patient has got some report God the test result done this is, this is the reality right patient has got the test result done.

373
00:49:54,840 --> 00:50:05,760
Ananda Sen: goes to consult his or her physician and say that while this is my testers and it's true and positive, what are the chances, I have the disease.

374
00:50:05,760 --> 00:50:08,790
Ananda Sen: that's a flipped question.

375
00:50:08,790 --> 00:50:11,070
Ananda Sen: You run the test.

376
00:50:11,070 --> 00:50:21,390
Ananda Sen: The test comes out positive, what are the chances, you have the disease or you have the condition negative predictive value similar.

377
00:50:21,390 --> 00:50:24,030
Ananda Sen: The result is negative.

378
00:50:24,030 --> 00:50:29,490
Ananda Sen: What are the chances, you are truly negative in the sense that you don't have these.

379
00:50:29,490 --> 00:50:33,270
Ananda Sen: that's that's a different.

380
00:50:33,270 --> 00:50:44,700
Ananda Sen: type of probability arm and, although they look very similar, they are not the same, and I.

381
00:50:44,700 --> 00:51:01,710
Ananda Sen: We, this is a small schematic table, which shows status, which is the truth and the positive and negative on the columns and then on the roads, you actually have the test results well, there are two cases when they match.

382
00:51:01,710 --> 00:51:12,930
Ananda Sen: And the two other cases where they do not match, so the status could be negative and the test result could be positive status could be positive and the test result could be negative so false negative and false positive.

383
00:51:12,930 --> 00:51:29,580
Ananda Sen: So that this page, other than the fact that is just got some calculations, to show you what exactly specificity sensitivity and priority a predictive value in it, and then you break the value is in regards to this simple two by two table.

384
00:51:29,580 --> 00:51:46,530
Ananda Sen: Also, the prevalence is is is divided by and getting that number tp plus a fan the true positives and false negatives and add them up and divide that by the population size that's your prevalence okay.

385
00:51:46,530 --> 00:51:48,270
Ananda Sen: So.

386
00:51:48,270 --> 00:51:53,430
Ananda Sen: These are four different ways of thinking of the test.

387
00:51:53,430 --> 00:52:02,130
Ananda Sen: What Should I be worried about or what Should I be knowing what should they actually focus on.

388
00:52:02,130 --> 00:52:07,260
Ananda Sen: sensitivity and specificity, it turns out to be properties of the test.

389
00:52:07,260 --> 00:52:25,710
Ananda Sen: Now the predictive that they are important to the to the to the subject to the patient, but unfortunately the dependent prevalence if you have a rare disease, you can ignore the the actual mathematical formula it's a formula which connects.

390
00:52:25,710 --> 00:52:39,930
Ananda Sen: The positive predictive value with with the prevalence and the sensitivity, but specificity, but the real thing is that, if it is a rare disease probably that the positive predictive value is typically quite low.

391
00:52:39,930 --> 00:52:55,320
Ananda Sen: On the other hand, the negative predictive value is going to be high What that means is predictive values, and not just a function of the test this performance depends a lot on how rare diseases.

392
00:52:55,320 --> 00:53:08,940
Ananda Sen: Now, does that mean that it should not be used ever well the the negative predictive value actually has a meaning because they're getting the value could have a cost consideration why because say.

393
00:53:08,940 --> 00:53:27,150
Ananda Sen: You are getting test results which are which are positive, which are false positives with the hydrate and given that that you are getting that false positive and getting the test, you are sending.

394
00:53:27,150 --> 00:53:39,150
Ananda Sen: The the patient for further tests, which is a huge cost implication, and if you do have a test with a loan negative predictive value that means.

395
00:53:39,150 --> 00:53:51,720
Ananda Sen: A lot of times, even if the test comes out positive, you are actually that the patient is actually negative then your unnecessarily doing a lot of costly expensive tests on this person, a second second test.

396
00:53:51,720 --> 00:54:06,660
Ananda Sen: To see the mean that the importance of negative thoughts so negative predictive value, basically, is something which can determine the healthcare costs to some extent in this in this context.

397
00:54:06,660 --> 00:54:31,410
Ananda Sen: Right so so yeah that is also important, but when you are discovering a test when you're actually trying to make sure the test is good, the detective detecting probability of the test is good discriminatory power of test is good that's when you continuance it sensitivity and specificity.

398
00:54:31,410 --> 00:54:37,890
Ananda Sen: So.

399
00:54:37,890 --> 00:54:42,360
Ananda Sen: The these are some of the other popular measures i'll i'll just skip that.

400
00:54:42,360 --> 00:54:46,650
Ananda Sen: But, but the question.

401
00:54:46,650 --> 00:54:51,210
Ananda Sen: is how do I determine.

402
00:54:51,210 --> 00:54:54,750
Ananda Sen: Whether it's good or not.

403
00:54:54,750 --> 00:54:58,920
Ananda Sen: let's think about specificity and sensitivity for a moment.

404
00:54:58,920 --> 00:55:09,210
Ananda Sen: What is how our sensitivity and specificity, are they related to each other, are the two separate properties of the test which we can.

405
00:55:09,210 --> 00:55:11,520
Ananda Sen: which we can.

406
00:55:11,520 --> 00:55:13,860
Ananda Sen: Study independently.

407
00:55:13,860 --> 00:55:16,680
Ananda Sen: And unfortunately they're there they're.

408
00:55:16,680 --> 00:55:22,080
Ananda Sen: extremely closely related and i'll try to give you this example.

409
00:55:22,080 --> 00:55:27,270
Ananda Sen: You know sort of like a broad generally.

410
00:55:27,270 --> 00:55:32,310
Ananda Sen: So what is done in this case.

411
00:55:32,310 --> 00:55:37,470
Ananda Sen: These are these tests often are made of essays.

412
00:55:37,470 --> 00:55:51,330
Ananda Sen: essays have certain certain markers those markers or obtainable from serum that serum or what tissues are things like those.

413
00:55:51,330 --> 00:56:00,030
Ananda Sen: And when you do the assay you try to find that mark the presence of that marker and imagine it as a continuous cut off.

414
00:56:00,030 --> 00:56:14,430
Ananda Sen: If that continuous measures above a certain threshold, you might call that a positive test, and then you try to say that while the test is positive, so i'm pretty sure that the person has the disease now.

415
00:56:14,430 --> 00:56:17,190
Ananda Sen: What is the cutoff.

416
00:56:17,190 --> 00:56:23,820
Ananda Sen: If that cutoff is too high, which means basically that you don't.

417
00:56:23,820 --> 00:56:27,750
Ananda Sen: you're pretty conservative you don't want to.

418
00:56:27,750 --> 00:56:46,170
Ananda Sen: Say somebody is positive, unless you go pretty high, what are you doing what you're doing is you are actually there are a lot of false negatives in this because, maybe, by increasing the threshold too high, you are.

419
00:56:46,170 --> 00:56:56,730
Ananda Sen: missing out on a lot of folks who actually are positive, but because your test is looking at a very high threshold, you are not detecting them.

420
00:56:56,730 --> 00:57:03,180
Ananda Sen: Right so sensitivity is compromised specificity is great.

421
00:57:03,180 --> 00:57:15,300
Ananda Sen: Because, because when it is something is negative, then you know that is a that is truly negative, or at least sorry if somebody is negative, then you're.

422
00:57:15,300 --> 00:57:38,520
Ananda Sen: you're not you're pre pre highly likely that you're not going to call that person positive specificity high sensitivity low, you can do the other other side to to lower threshold you're calling everybody a positive are almost everybody a positive, that means a lot of false positives.

423
00:57:38,520 --> 00:57:41,490
Ananda Sen: Now the sensitivity is very high.

424
00:57:41,490 --> 00:57:50,520
Ananda Sen: Because you're capturing everybody who is positive, almost but the specificity is low, so what i'm trying to say is that.

425
00:57:50,520 --> 00:57:58,860
Ananda Sen: sensitivity and specificity, have a balance if one is high, the other is low.

426
00:57:58,860 --> 00:58:01,380
Okay.

427
00:58:01,380 --> 00:58:06,480
Ananda Sen: So what would be a good test.

428
00:58:06,480 --> 00:58:08,700
Ananda Sen: So.

429
00:58:08,700 --> 00:58:12,600
Ananda Sen: I just mentioned, about the sense that threshold.

430
00:58:12,600 --> 00:58:19,950
Ananda Sen: If you carry on this over the entire range of the cutoffs.

431
00:58:19,950 --> 00:58:30,660
Ananda Sen: you'd actually trace out a curve what curves the curve is something where, for every threshold, they have got a pair of sensitivity and specificity.

432
00:58:30,660 --> 00:58:35,340
Ananda Sen: You can plot that as a function of the threshold.

433
00:58:35,340 --> 00:58:44,400
Ananda Sen: And the plot out the curve that curve is called risk receiver operating characteristic curve RC curb.

434
00:58:44,400 --> 00:58:52,110
Ananda Sen: So there's a little bit of a history there, but let me just go straight to the.

435
00:58:52,110 --> 00:59:06,120
Ananda Sen: picture, what is this picture so picture has to to access one of them is false positive rate, which is hundred minus specificity, the other one is true positive rate with your sensitivity.

436
00:59:06,120 --> 00:59:13,770
Ananda Sen: So for each and the core and the end the end the line that dashed line is kind of the.

437
00:59:13,770 --> 00:59:24,000
Ananda Sen: The 45 degree line each point on this curve is really a pair of specificity and sensitivity that or one month specificity and sensitivity.

438
00:59:24,000 --> 00:59:41,280
Ananda Sen: And the way this is designed, is that, if your specificity increases, which means one minus specificity hundred minutes, because if it decreases then your sensitivity should all should decrease, so it should be an increasing curve.

439
00:59:41,280 --> 00:59:53,940
Ananda Sen: Now 45 degree line actually presents a useless test that means it's completely random you want to stay high above the 45% or 45 degree line.

440
00:59:53,940 --> 00:59:59,640
Ananda Sen: And what you do is you actually.

441
00:59:59,640 --> 01:00:10,380
Ananda Sen: try to find the area under this curve.

442
01:00:10,380 --> 01:00:13,560
Ananda Sen: Okay, this this entire area.

443
01:00:13,560 --> 01:00:18,330
Ananda Sen: that's called area under.

444
01:00:18,330 --> 01:00:21,630
Ananda Sen: receiver operating characteristic.

445
01:00:21,630 --> 01:00:35,460
Ananda Sen: How you find the area, there are different methods there's a Crevasse idol method and so on and so, how much would the area be well you know they'll never get 100% 90% is fabulous fantastic.

446
01:00:35,460 --> 01:00:45,930
Ananda Sen: 80% is considered mostly pretty good 70% or more is kind of like the 70% 75% is kind of minimum you would want to do.

447
01:00:45,930 --> 01:00:49,680
Ananda Sen: or a diagnostic test.

448
01:00:49,680 --> 01:00:54,150
Ananda Sen: And and that's that's how you go about it so.

449
01:00:54,150 --> 01:00:57,300
Ananda Sen: um.

450
01:00:57,300 --> 01:00:58,620
Okay.

451
01:00:58,620 --> 01:01:05,310
Ananda Sen: hey, this is a good time to do a quick stop why don't we come back.

452
01:01:05,310 --> 01:08:25,140
Ananda Sen: At 245 will start from this page.

453
01:08:25,140 --> 01:08:27,600
This.

454
01:08:27,600 --> 01:08:33,180
Ananda Sen: So.

455
01:08:33,180 --> 01:08:42,990
Ananda Sen: The rsc curve, then, is.

456
01:08:42,990 --> 01:08:46,470
Ananda Sen: Is is essentially.

457
01:08:46,470 --> 01:08:48,960
Ananda Sen: A by various curve.

458
01:08:48,960 --> 01:08:51,930
Ananda Sen: of sensitivity and specificity.

459
01:08:51,930 --> 01:09:01,500
Ananda Sen: But it actually covers the threshold of here and there, and the reason it is done that way is because.

460
01:09:01,500 --> 01:09:17,850
Ananda Sen: He it, it is important when you when you design two tests, for example, two markers or a panel of markers and you want to find out which one does the best job what what ends up happening is.

461
01:09:17,850 --> 01:09:28,230
Ananda Sen: it's possible that, for one, cut off, you might find one test which is better than for the other cut off, you might find the other test is doing better so.

462
01:09:28,230 --> 01:09:48,570
Ananda Sen: So you want to you want to do an overall comparison, so you so this this area under the rsc curve is basically an overall discriminatory power for the test right, so you you compare you can compare the two diagnostic measures based on.

463
01:09:48,570 --> 01:09:52,530
Ananda Sen: Based on these two these two tests.

464
01:09:52,530 --> 01:10:10,560
Ananda Sen: Any question on this i'm i'm not sure if if some of you actually have heard of rsc before if you have done sensitivity specificity, the likelihood is that you probably have heard some amount of pharmacy so.

465
01:10:10,560 --> 01:10:16,260
Ananda Sen: So let me ask that question posed that question, have you done RC.

466
01:10:16,260 --> 01:10:23,640
Ananda Sen: Or have you seen rsc curves the news yeah to some extent, maybe.

467
01:10:23,640 --> 01:10:38,970
Ananda Sen: So, whether you've seen it or not, we're going to you're going to now know how to do this actually in in SAS anyway, one of the software's allow it to do this, but.

468
01:10:38,970 --> 01:10:44,220
Ananda Sen: But, in particular in SAS so.

469
01:10:44,220 --> 01:10:50,670
Ananda Sen: So before we go there, I want to.

470
01:10:50,670 --> 01:10:56,430
Ananda Sen: Start are talking about our third study.

471
01:10:56,430 --> 01:11:11,040
Ananda Sen: Which is called Google it's the it's the global longitudinal study of osteoporosis in women, and this is a, this is a data collected from an international consortium.

472
01:11:11,040 --> 01:11:25,290
Ananda Sen: I think there were about 50 countries who participated in this and and the data we are going to use which I collected from this book by Horsemen limitations to prevent.

473
01:11:25,290 --> 01:11:42,540
Ananda Sen: It it's it's got 500 records for participants from us alone and and and and really the main outcome here is this, this is a salon to study so women were enrolled and they were followed up.

474
01:11:42,540 --> 01:12:01,230
Ananda Sen: Over a period of years and and the question was these were women, of course, have osteoporosis with with osteoporosis, so one of the main questions was to see whether this suffered a bone fracture of any kind, within the next year.

475
01:12:01,230 --> 01:12:05,670
Ananda Sen: So one thing I want to point out is that.

476
01:12:05,670 --> 01:12:20,670
Ananda Sen: Obviously the the records have participants from us, and those who suffered fractures were over samples, so that their fraction is exactly like quarter of the data.

477
01:12:20,670 --> 01:12:28,590
Ananda Sen: Which is much larger than the actual fraction which is 4% in the larger population.

478
01:12:28,590 --> 01:12:37,620
Ananda Sen: So what are the variables well there are several variables, so this is a larger data set in terms of variables compared to the low birth with data we have.

479
01:12:37,620 --> 01:12:50,730
Ananda Sen: borrowed from the standard identification and studies say we are the physician ID we have a history of pride fracture age of enrollment weight height body mass.

480
01:12:50,730 --> 01:12:54,660
Ananda Sen: Men or boys, yes, no then.

481
01:12:54,660 --> 01:12:59,760
Ananda Sen: There is famous family history variable mother and hip fracture not.

482
01:12:59,760 --> 01:13:14,700
Ananda Sen: Then arm assist as an interesting variable that's basically basically asked about the strength of the woman arms and needed to stand from a cheer yes or no, then smoking status.

483
01:13:14,700 --> 01:13:19,410
Ananda Sen: And then self reported risk of fracture.

484
01:13:19,410 --> 01:13:29,850
Ananda Sen: it's just it's just a self reported risk it's not the actual fracture thing and then fracture risk score.

485
01:13:29,850 --> 01:13:40,200
Ananda Sen: And then the the the 15th variable the variable number 15 is the one which is the main outcome and the fractal in first year.

486
01:13:40,200 --> 01:13:52,740
Ananda Sen: That that that frack of our frat score or factory score is actually a composite variable a composite score, which is comprised of that equal, which is basically.

487
01:13:52,740 --> 01:13:59,370
Ananda Sen: constructed using that equation so in the equation, you would you would notice that there are several variables and several.

488
01:13:59,370 --> 01:14:18,780
Ananda Sen: coefficient associated with it, those are the weights, essentially, and you see those variables in parentheses, that means that's a grouping variable so zero times age less than equal to 60 basically means if if the woman is less than at age 60 then it doesn't get any weight in fact score.

489
01:14:18,780 --> 01:14:33,840
Ananda Sen: It gets a weight of one if the woman is between 60 and 65 we'd have to if it is between 65 and 70 and the weight of three and so on, so forth Okay, and then prior fracture.

490
01:14:33,840 --> 01:14:39,480
Ananda Sen: mother had a fracture Martin and so on and so forth.

491
01:14:39,480 --> 01:14:41,970
Ananda Sen: it's it's.

492
01:14:41,970 --> 01:14:45,300
Ananda Sen: It, these are all.

493
01:14:45,300 --> 01:15:03,270
Ananda Sen: done as a categorical variable so it's it's it's even if he was continuous, it is actually broken up into into groups so so that's the frack score so that's a that's kind of a risk score similar to framingham risk or if you if you're familiar with their.

494
01:15:03,270 --> 01:15:08,460
Ananda Sen: job.

495
01:15:08,460 --> 01:15:15,870
Ananda Sen: what's the model fit to the glow steady and this is using some of these.

496
01:15:15,870 --> 01:15:18,330
Ananda Sen: Some of these important.

497
01:15:18,330 --> 01:15:29,610
Ananda Sen: Factors now notice I didn't put in the risk frank frank score there because frack score is actually based on some of these variables, so will will not.

498
01:15:29,610 --> 01:15:55,410
Ananda Sen: Double double adjust this so but age height brad fracture mom fractured and so on and so forth, there are quite a few, then there are some interaction variables there too, so it's it's it's a model which was developed, after some analysis re analysis massaging the data and songs.

499
01:15:55,410 --> 01:16:03,360
Ananda Sen: These almost all of them, except for the actually all of them are.

500
01:16:03,360 --> 01:16:15,540
Ananda Sen: I guess rate risk three is is 0.051 and that's close rate risk.

501
01:16:15,540 --> 01:16:21,390
Ananda Sen: yeah I got I don't remember what the retreats, but that was a derived variable.

502
01:16:21,390 --> 01:16:24,810
Ananda Sen: That much I remember.

503
01:16:24,810 --> 01:16:29,850
Ananda Sen: Okay.

504
01:16:29,850 --> 01:16:31,920
Ananda Sen: All right.

505
01:16:31,920 --> 01:16:39,030
Ananda Sen: So this is the area under the curve in the rsc curve now, one might say that.

506
01:16:39,030 --> 01:16:46,020
Ananda Sen: What, how are we, what does it mean what does the end of our sick or mean.

507
01:16:46,020 --> 01:16:56,340
Ananda Sen: Because there is no no marker there well, you have to think of it this way our oC is really.

508
01:16:56,340 --> 01:17:05,010
Ananda Sen: Essentially, is a discriminatory power of a model it doesn't have to be just as just a marker.

509
01:17:05,010 --> 01:17:16,560
Ananda Sen: It can be a combination of demographic variables other variables it's basically a model of power, logistic regression models power to discriminate.

510
01:17:16,560 --> 01:17:31,380
Ananda Sen: or discriminate between the yeses and the nose, and this is a yes and noise fracture in next year are fracture know fraction next year, something like that.

511
01:17:31,380 --> 01:17:44,520
Ananda Sen: So if you use that we use that model, the one I just showed you and and put that in the logistic regression and try to get the area, the rsc it turns out, it was 73%.

512
01:17:44,520 --> 01:17:58,230
Ananda Sen: So the question now is we know how to do it for a single marker when I have when I have a bunch of variables, how does, how is this generated that's the question.

513
01:17:58,230 --> 01:18:02,790
Ananda Sen: And that's what I will explain next so.

514
01:18:02,790 --> 01:18:08,850
Ananda Sen: it's a logistic regression model so based on the logistic regression model, you will get predicted probabilities.

515
01:18:08,850 --> 01:18:16,830
Ananda Sen: So for each subject if I use the subjects use the moms are the woman's.

516
01:18:16,830 --> 01:18:27,270
Ananda Sen: On data, then I will get a predicted law of odds and predicted odds and predicted probability from that right.

517
01:18:27,270 --> 01:18:29,640
Ananda Sen: Suppose.

518
01:18:29,640 --> 01:18:41,940
Ananda Sen: We predict the outcome to be positive or negative, depending on whether that predicted property is greater than or less than or for a fixed very fixed fixed threshold see.

519
01:18:41,940 --> 01:18:58,530
Ananda Sen: Again, each threshold gives us sensitivity specificity and and you slide see any good, a new value of sensitivity specificity, as seen greater sensitivity decreases and specificity increases okay so.

520
01:18:58,530 --> 01:19:21,240
Ananda Sen: I can do this not just based on a single marker button, based on a model which has many, many, many, many variables and that's that's how it is done in the context of a multiple logistic regression so to put this in perspective i've got a couple of pictures, so the first picture is this.

521
01:19:21,240 --> 01:19:24,180
Ananda Sen: First picture is where the.

522
01:19:24,180 --> 01:19:28,560
Ananda Sen: We, the same data and.

523
01:19:28,560 --> 01:19:53,400
Ananda Sen: And suppose the cutoff for the predicted probabilities taken to be 0.50 okay if that's the case if it is taking 0.50, then I have got a true positive rate of 17.6% and true negative rate of 94.9%.

524
01:19:53,400 --> 01:19:56,070
Ananda Sen: So curve is point five.

525
01:19:56,070 --> 01:20:15,990
Ananda Sen: For the predicted probability If so, what does that mean that means if for a for a given subject if the predicted properties above 50% I would say that person is positive positive for pretty good fracture and if it's less than 50% of call it negative.

526
01:20:15,990 --> 01:20:33,060
Ananda Sen: yeah So if I do that, then I get a certain sensitivity and certain specificity, so the sensitivity is, in this case 17.6% pretty low and and specificity is.

527
01:20:33,060 --> 01:20:37,710
Ananda Sen: it's a two negative rate 94.9%.

528
01:20:37,710 --> 01:20:40,080
up.

529
01:20:40,080 --> 01:20:44,010
Ananda Sen: Suppose a change that cut off 2.3.

530
01:20:44,010 --> 01:20:49,770
Ananda Sen: That is anybody was about 30% greater probability will be called positive.

531
01:20:49,770 --> 01:20:53,310
Ananda Sen: Otherwise, the person will be terminated.

532
01:20:53,310 --> 01:21:11,160
Ananda Sen: Before going to the next page and don't be don't cheat, what do you think is going to happen if I change the cut off to 0.30 is sensitivity going to increase or decrease.

533
01:21:11,160 --> 01:21:24,420
Ananda Sen: Suppose they change the cut off from 0.5 or 0.3 is the true positive rate, which is the sensitivity going to increase or decrease.

534
01:21:24,420 --> 01:21:28,290
Ananda Sen: Think it's increase why.

535
01:21:28,290 --> 01:21:34,800
Yoonhee Ryder: Because you're catching more people changing it to.

536
01:21:34,800 --> 01:21:37,650
Ananda Sen: Lower a lower threshold yes.

537
01:21:37,650 --> 01:21:39,900
Ananda Sen: that's exactly right so.

538
01:21:39,900 --> 01:21:48,420
Ananda Sen: that's revealed answers, yes, so that's exactly what is happening so when you when you when you change the cut off to a lower value.

539
01:21:48,420 --> 01:21:53,490
Ananda Sen: Then the true positive rate increases because you're catching more people.

540
01:21:53,490 --> 01:22:08,460
Ananda Sen: With with possible fracture and and the true positive rate now jumps from 18% to 62% and true negative rate, not surprisingly came down.

541
01:22:08,460 --> 01:22:17,790
Ananda Sen: So so that's the idea so change the color of the sensitivity specificity will will vary, and if you go through this.

542
01:22:17,790 --> 01:22:25,350
Ananda Sen: over the whole thing over the whole threshold, then you trace out a curve, the blue curve is.

543
01:22:25,350 --> 01:22:36,600
Ananda Sen: A receiver operating characterising garden and and and the area, and that is the 0.73 now the current doesn't look smooth and that's because.

544
01:22:36,600 --> 01:22:49,800
Ananda Sen: Remember, this predicted problems are based on the data, so there are holes, there are gaps and and those in those gaps so so the lowest rated property is 20%.

545
01:22:49,800 --> 01:22:58,080
Ananda Sen: The next one is 25% there is nothing between 20 20% 25% so what's going to happen well in between 20 and 25 5%.

546
01:22:58,080 --> 01:23:13,200
Ananda Sen: The values don't change the the sensitivity and specificity don't change because they are you're not getting anybody there in the data for that so, so it is a non smooth curve it's it's going to going to have this little.

547
01:23:13,200 --> 01:23:28,530
Ananda Sen: nothing happening range and then jumping again so a typical typical case would be something here so flat and then jumps sounds.

548
01:23:28,530 --> 01:23:30,030
Ananda Sen: That.

549
01:23:30,030 --> 01:23:33,540
Ananda Sen: You understand the principle.

550
01:23:33,540 --> 01:23:37,770
Ananda Sen: it's it's actually.

551
01:23:37,770 --> 01:23:40,170
Ananda Sen: Very.

552
01:23:40,170 --> 01:23:45,720
Ananda Sen: I mean, this is very practical it's It is this is how this is entirely done.

553
01:23:45,720 --> 01:23:50,640
That.

554
01:23:50,640 --> 01:24:09,000
Ananda Sen: And again, you don't have to worry about this last statement, the area under the receiver operating characteristic curve is going to be estimated for you don't have to worry about about that.

555
01:24:09,000 --> 01:24:11,640
Okay.

556
01:24:11,640 --> 01:24:23,370
Ananda Sen: let's see how to implement this in.

557
01:24:23,370 --> 01:24:26,880
Ananda Sen: So they said, is the glow steady.

558
01:24:26,880 --> 01:24:37,050
Ananda Sen: So I said it wrong, so it is 10 countries not 50 times 10 countries or 60,000 60,000 women.

559
01:24:37,050 --> 01:24:41,580
Ananda Sen: at the table again.

560
01:24:41,580 --> 01:24:52,140
Ananda Sen: just wanted to see how the data is that I created variables and then interaction term so basically.

561
01:24:52,140 --> 01:24:56,580
Ananda Sen: that's your rate risk and rate risk.

562
01:24:56,580 --> 01:25:02,010
Ananda Sen: equal to three and 123 and so on and so forth.

563
01:25:02,010 --> 01:25:16,380
Ananda Sen: Some basic this This is something all of you i'm sure when you do the analysis you do some of these things in the beginning, just to see how the data is how it looks like and different types of variables and so on and so what.

564
01:25:16,380 --> 01:25:20,520
Ananda Sen: Age height weight at three examples.

565
01:25:20,520 --> 01:25:26,520
Ananda Sen: I also looked at the histogram of the screen and variable called frack score.

566
01:25:26,520 --> 01:25:36,210
Ananda Sen: It that looks not so normal it's actually a little bit of a little bit of a stillness there.

567
01:25:36,210 --> 01:25:44,970
Ananda Sen: it's it's content on the left more and then it goes down.

568
01:25:44,970 --> 01:25:49,590
Ananda Sen: You can actually check normality of the data using.

569
01:25:49,590 --> 01:25:59,220
Ananda Sen: The this sub command normal and then this is asking for the histogram so some of those things are available in proper unitarian.

570
01:25:59,220 --> 01:26:12,210
Ananda Sen: rock means we have seen the application off it's just brings out the means standard deviation and meaning.

571
01:26:12,210 --> 01:26:16,890
Ananda Sen: You can also do cross classification, the way we did before.

572
01:26:16,890 --> 01:26:24,600
Ananda Sen: So this is fracture prior fracture and trying to remember.

573
01:26:24,600 --> 01:26:29,700
Ananda Sen: Right fracture is.

574
01:26:29,700 --> 01:26:32,790
What was brave fracture.

575
01:26:32,790 --> 01:26:38,460
Ananda Sen: So fracture pride fracture.

576
01:26:38,460 --> 01:26:46,620
Ananda Sen: So history of pride factor just that it's not not, there is no time bound thing if there is a fracture or not before.

577
01:26:46,620 --> 01:26:51,330
Ananda Sen: So that if you look at.

578
01:26:51,330 --> 01:27:00,570
Ananda Sen: The 00 is the most populated one, so no brad fracture know fractures that's 300 out of the 500 that 60%.

579
01:27:00,570 --> 01:27:18,570
Ananda Sen: And then there was a private fracture and there was a fracture which also happen next year, that is, about 10% of the people had that 52 out of the 500 and the remaining are somewhat evenly distributed.

580
01:27:18,570 --> 01:27:37,620
Ananda Sen: Within the group, there was a prior fracture but no fractal in the next year, there was no pride fractured but a fracture in the next year, so so so that's kind of evenly distributed in this those two columns on it and there's two categories, but otherwise it's a.

581
01:27:37,620 --> 01:27:42,780
Ananda Sen: Most more concentrated on the diagonals.

582
01:27:42,780 --> 01:27:46,410
Okay.

583
01:27:46,410 --> 01:27:51,270
Ananda Sen: And you can do all sorts of Chi Square and all of those tests here.

584
01:27:51,270 --> 01:28:06,750
Ananda Sen: If you if you wanted like we did before so coming to the arrow see analysis, so one question is is frack score a good marker for fracture.

585
01:28:06,750 --> 01:28:11,730
Ananda Sen: Okay now how do I get that so.

586
01:28:11,730 --> 01:28:23,460
Ananda Sen: You can use the audience device again because you do get a lot of plots based on that, especially the rsc plots.

587
01:28:23,460 --> 01:28:35,520
Ananda Sen: So again, proper logistic is the way to go about this, this one that one has rotc inbuilt you could use Prague gen mode but.

588
01:28:35,520 --> 01:28:43,590
Ananda Sen: Doing gen rfc for gen mode is much more more pain properties tickets, is there.

589
01:28:43,590 --> 01:28:48,000
Ananda Sen: So you could see I do a request for a plot.

590
01:28:48,000 --> 01:28:54,300
Ananda Sen: For the rsc and then model fracture.

591
01:28:54,300 --> 01:29:08,610
Ananda Sen: as a function of frack score that's just a single marker and the only reason i'm using frack scores markers effects for is already a function of several variables, we use the model.

592
01:29:08,610 --> 01:29:13,470
Ananda Sen: i'm out putting.

593
01:29:13,470 --> 01:29:17,970
Ananda Sen: In two variables out equal to diag.

594
01:29:17,970 --> 01:29:36,480
Ananda Sen: And and an article to die is and predicted equal to score one So those are the Those are two values two variables, I have created diagonal and score one and then.

595
01:29:36,480 --> 01:29:51,480
Ananda Sen: running the property just again then wrapping it up with audience graphics off so that's the estimate correct score and that's the Chi square 32.447.

596
01:29:51,480 --> 01:30:01,650
Ananda Sen: And then the odds ratio estimate so that's The other thing about, but just a proper logistic is automatically creates the odds ratio, which is, which is nice.

597
01:30:01,650 --> 01:30:14,250
Ananda Sen: Point estimate the confidence limits again, if you look at the confidence limits of odds ratio, you see 0.718 to 0.851.

598
01:30:14,250 --> 01:30:19,260
Ananda Sen: Okay, so there are a couple of things to.

599
01:30:19,260 --> 01:30:22,500
Ananda Sen: See here so first of all.

600
01:30:22,500 --> 01:30:30,870
Ananda Sen: This is the odds ratio estimate let's first try to interpret the odds ratio estimate of point seven.

601
01:30:30,870 --> 01:30:34,740
Ananda Sen: So what does that mean.

602
01:30:34,740 --> 01:30:37,440
Ananda Sen: Can anybody tell.

603
01:30:37,440 --> 01:31:21,300
Ananda Sen: This is going back to what we talked about yesterday.

604
01:31:21,300 --> 01:31:27,780
Christie Flanagan: i'll be looking at the estimate the negative 0.2466 or the point estimate of the fact score.

605
01:31:27,780 --> 01:31:40,860
Ananda Sen: Oh well, this is the law gods, and this is the odds so either one is fine, I think all does it easier way to interpret this so let's come to the odds odds ratio point 71.

606
01:31:40,860 --> 01:31:44,130
Ananda Sen: The point so.

607
01:31:44,130 --> 01:31:59,490
Ananda Sen: What does that mean.

608
01:31:59,490 --> 01:32:06,570
Christie Flanagan: i'm not hundred percent sure is the point ESTA is out of one so it's saying that.

609
01:32:06,570 --> 01:32:15,360
Christie Flanagan: That facts score is given that it's closer to one that would be a good marker it's estimating that it would be.

610
01:32:15,360 --> 01:32:19,380
Ananda Sen: 00 right.

611
01:32:19,380 --> 01:32:21,480
Ananda Sen: anybody else.

612
01:32:21,480 --> 01:32:25,260
Marcelo Galafassi: Like.

613
01:32:25,260 --> 01:32:29,220
Marcelo Galafassi: more likely to be correct i'm.

614
01:32:29,220 --> 01:32:33,690
not sure.

615
01:32:33,690 --> 01:32:51,150
Ananda Sen: So it's good to get back to things we talked about yesterday, so the odds ratio is less than one, what does it mean.

616
01:32:51,150 --> 01:32:59,580
Ananda Sen: or equivalent the logo is less than zero.

617
01:32:59,580 --> 01:33:02,580
Ananda Sen: frack effects if given this.

618
01:33:02,580 --> 01:33:08,430
Theresa Marie Kowalski-Dobson: Maybe it's like and it doesn't predict, it is that what it's saying if it's negative.

619
01:33:08,430 --> 01:33:10,710
Ananda Sen: A.

620
01:33:10,710 --> 01:33:14,970
Ananda Sen: doesn't predict will come from the p that does or does not predict.

621
01:33:14,970 --> 01:33:17,370
Ananda Sen: But i'm talking about the direction of.

622
01:33:17,370 --> 01:33:19,470
Association.

623
01:33:19,470 --> 01:33:20,040
Ananda Sen: So.

624
01:33:20,040 --> 01:33:25,140
Ananda Sen: My first question let's let's let's let's make this question a little bit more pointed.

625
01:33:25,140 --> 01:33:27,390
To my first question.

626
01:33:27,390 --> 01:33:41,070
Ananda Sen: If rack score increases, do you expect based on what we found do we expect a higher likelihood of fracture or lower liking.

627
01:33:41,070 --> 01:33:43,620
Higher.

628
01:33:43,620 --> 01:33:48,150
Ananda Sen: Higher somebody said higher.

629
01:33:48,150 --> 01:33:52,770
Ananda Sen: Higher.

630
01:33:52,770 --> 01:34:01,560
Ananda Sen: I think that's why i'm getting confused because it looks like that is lower, so that doesn't make sense, why does it not not doesn't make sense.

631
01:34:01,560 --> 01:34:13,680
Yoonhee Ryder: Because you would expect it to be the odds, to be greater than one because the frak score should predict who's going to have a fracture.

632
01:34:13,680 --> 01:34:28,410
Ananda Sen: yeah okay so so you have said two things which are related but not exactly the same okay predictive means whether it's significantly predicts either higher or.

633
01:34:28,410 --> 01:34:31,080
lower.

634
01:34:31,080 --> 01:34:39,780
Ananda Sen: But who said that frack score higher means that the higher likelihood affected, I never said that practical reviews that.

635
01:34:39,780 --> 01:34:53,250
Ananda Sen: I mean it's it sounds like it, it sounds like it, but, but in this case, what we are seeing is that the frack score is actually protective, which means higher the score.

636
01:34:53,250 --> 01:35:02,250
Ananda Sen: Lower is the likelihood of having a future fracture.

637
01:35:02,250 --> 01:35:05,520
Okay.

638
01:35:05,520 --> 01:35:16,980
Ananda Sen: I don't know if that is I don't know enough of this data to know whether that is actually i'm counterintuitive.

639
01:35:16,980 --> 01:35:21,330
Ananda Sen: But what i'm trying to say is that.

640
01:35:21,330 --> 01:35:33,060
Ananda Sen: This is this is at least what the data is one and and and and, and this is where I have made sure that because proper logistic also has that tendency of.

641
01:35:33,060 --> 01:35:44,370
Ananda Sen: modeling zero so I have made sure that it's what it's doing is it's modeling event equal to one, which means it is actually modeling fracture.

642
01:35:44,370 --> 01:35:45,840
Ananda Sen: But.

643
01:35:45,840 --> 01:35:54,390
Ananda Sen: without knowing the clinical thing if I just have to base it on the data I would actually say.

644
01:35:54,390 --> 01:36:00,600
Ananda Sen: Higher the frack score lower the chances of a future fracture.

645
01:36:00,600 --> 01:36:05,130
Ananda Sen: That may be completely clinically wrong.

646
01:36:05,130 --> 01:36:19,230
Ananda Sen: But the data is showing that.

647
01:36:19,230 --> 01:36:34,230
Christie Flanagan: And doctor said just to be clear that's because, for the odds ratio if it's less than one your odds are less likely greater than one your odds increase that's okay thank.

648
01:36:34,230 --> 01:36:34,560
Christie Flanagan: You.

649
01:36:34,560 --> 01:36:39,030
So.

650
01:36:39,030 --> 01:36:44,400
Ananda Sen: So, if it is less than one.

651
01:36:44,400 --> 01:36:47,340
Ananda Sen: Then hire.

652
01:36:47,340 --> 01:36:52,650
Ananda Sen: The frack score lower are the odds of fractured now.

653
01:36:52,650 --> 01:37:03,990
Ananda Sen: Is it significance as many of you mentioned with is predictive is it significantly less than one, yes, because if you're looking at the confidence intervals, it does not cover one.

654
01:37:03,990 --> 01:37:15,420
Ananda Sen: Which means it remains significant this one actually is conform firmed by the p value, which is less than point 0001.

655
01:37:15,420 --> 01:37:21,000
Ananda Sen: that's the people that that that P value corresponds to the.

656
01:37:21,000 --> 01:37:27,210
Ananda Sen: Confidence limit.

657
01:37:27,210 --> 01:37:28,920
Ananda Sen: Okay.

658
01:37:28,920 --> 01:37:31,650
Ananda Sen: More.

659
01:37:31,650 --> 01:37:34,980
Ananda Sen: So.

660
01:37:34,980 --> 01:37:36,870
Ananda Sen: Not only.

661
01:37:36,870 --> 01:37:48,780
Ananda Sen: You can say more basically so it's not this point seven eight basically says, not only does the odds go down, it goes down by 22%.

662
01:37:48,780 --> 01:38:58,110
Ananda Sen: So it's 22% lower odds of having a fracture for each unit increase in frack score so let's write that down.

663
01:38:58,110 --> 01:39:07,950
Ananda Sen: Press corps predicted doesn't necessarily mean that that.

664
01:39:07,950 --> 01:39:12,510
Ananda Sen: is positively predictive but it's predicting nonetheless.

665
01:39:12,510 --> 01:39:20,940
Ananda Sen: Because the p value is less than and also the fact that the conference limits don't include one.

666
01:39:20,940 --> 01:39:29,850
Marcelo Galafassi: So just to make sure my mistake was if there's the French girls one point 78 then he will be 78% more likely.

667
01:39:29,850 --> 01:39:34,980
Ananda Sen: Right, yes, yes, yes yeah if it is 1.78 benny would say.

668
01:39:34,980 --> 01:39:36,450
Ananda Sen: 70% more likely.

669
01:39:36,450 --> 01:39:43,500
Ananda Sen: To be 0.78, then you would actually subtracted from one, and since 22%.

670
01:39:43,500 --> 01:39:45,930
Ananda Sen: OK.

671
01:39:45,930 --> 01:39:50,580
OK.

672
01:39:50,580 --> 01:39:55,470
Ananda Sen: So that say how is the model.

673
01:39:55,470 --> 01:40:01,320
Ananda Sen: Where do I get the E area and REC well there is a little bit.

674
01:40:01,320 --> 01:40:04,770
Ananda Sen: Which to.

675
01:40:04,770 --> 01:40:06,960
Ananda Sen: tiny little.

676
01:40:06,960 --> 01:40:17,790
Ananda Sen: See it's actually sometimes called the coefficient of concordance that's why the name see label see.

677
01:40:17,790 --> 01:40:29,730
Ananda Sen: it's not great it's a great market 67.6% or 60.676 that's the URL see.

678
01:40:29,730 --> 01:40:32,730
Ananda Sen: It comes as an.

679
01:40:32,730 --> 01:40:37,260
Ananda Sen: As an offshoot of logistics.

680
01:40:37,260 --> 01:40:40,410
Ananda Sen: So, this would have been there.

681
01:40:40,410 --> 01:40:49,980
Ananda Sen: Even if I did not do this audience and all of those things, but what we would not have gotten is the is the picture.

682
01:40:49,980 --> 01:40:52,710
Ananda Sen: So the picture basically.

683
01:40:52,710 --> 01:40:56,550
Ananda Sen: is associated with the rsc.

684
01:40:56,550 --> 01:41:11,340
Ananda Sen: And it comes up like this one minute specificity insensitivity.

685
01:41:11,340 --> 01:41:36,330
question.

686
01:41:36,330 --> 01:41:42,690
Ananda Sen: suppose you want to compare two markers.

687
01:41:42,690 --> 01:41:59,430
Ananda Sen: And I can use frack score as one of the markers and perhaps derive a marker myself based on data and see if the two markers defer.

688
01:41:59,430 --> 01:42:19,680
Ananda Sen: to score one is constructed as the predicted probability from a logistic regression of fracture on track score and score to is constructed as a predictive programming from a logistic regression of fracture on age BMI rate risk factor in and pre mandibles premenopausal status status.

689
01:42:19,680 --> 01:42:32,880
Ananda Sen: And the question the research question i'm asking is is the difference between the EU RC based on score one and score to statistical significance.

690
01:42:32,880 --> 01:42:37,410
Ananda Sen: Okay, now, this is very common.

691
01:42:37,410 --> 01:42:42,750
Ananda Sen: So essentially what it what it what it's doing is that it's.

692
01:42:42,750 --> 01:42:44,760
Ananda Sen: Creating.

693
01:42:44,760 --> 01:42:56,880
Ananda Sen: Two different set of markers or you can be more in a typical diagnostic scenario often what you have so.

694
01:42:56,880 --> 01:43:03,870
Ananda Sen: Though one of one of the areas where which i'm quite.

695
01:43:03,870 --> 01:43:06,930
Ananda Sen: vested in is is.

696
01:43:06,930 --> 01:43:10,890
Ananda Sen: GA cancer and in GI cancer.

697
01:43:10,890 --> 01:43:33,540
Ananda Sen: Investigation people often do and colonoscopies the gold standard but do detect any activity but people's want to do, prevention, so they are always trying to come up with markers there some of some of the markers and but based on the market that tissue based some of the markers are.

698
01:43:33,540 --> 01:43:39,600
Ananda Sen: plasma based all of those things, and also, you know lifestyle and all sorts of things.

699
01:43:39,600 --> 01:43:44,610
Ananda Sen: So there are what are called marker panels.

700
01:43:44,610 --> 01:43:55,560
Ananda Sen: Not a single variable but but a complex very complex set of variables and and they are all these different combinations of these many things.

701
01:43:55,560 --> 01:44:09,390
Ananda Sen: And sometimes people come up with these things in a lab and and this say that well let's use these genes that says these genetic marker is use let's use these.

702
01:44:09,390 --> 01:44:15,390
Ananda Sen: I don't know how many of you have heard of things like apologetic risk score and.

703
01:44:15,390 --> 01:44:37,080
Ananda Sen: and other other types of score so so all of these are markers and the question is, do they do, they do anything do they do anything as far as the as the as far as the discrimination is concerned, or if there is an existing existing marker does it actually increasing.

704
01:44:37,080 --> 01:44:44,280
Ananda Sen: In terms of the discrimination, so this is a fairly simple almost a toy exercise to do a similar thing.

705
01:44:44,280 --> 01:44:57,810
Ananda Sen: most often you will not when you when you compare markers you will not necessarily use the same variables what you'll do is you would actually use other variables and and come up with a different market.

706
01:44:57,810 --> 01:45:00,150
Ananda Sen: But the question remains the same.

707
01:45:00,150 --> 01:45:09,060
Ananda Sen: Are there discrimination powers, do they do, they have changed this condition powers, and to do that.

708
01:45:09,060 --> 01:45:14,400
Ananda Sen: This is the series of things are Sir or a long line of.

709
01:45:14,400 --> 01:45:35,880
Ananda Sen: sass codes, you would go through, so the what is the first block in the first block you would create the logistic regression model based on frack score which I already did so we just rerun that output out equal to directly to the score one run.

710
01:45:35,880 --> 01:45:46,740
Ananda Sen: And then you take that SAS data set which is called diet you're just great we just created that.

711
01:45:46,740 --> 01:45:48,390
Ananda Sen: So.

712
01:45:48,390 --> 01:45:55,950
Ananda Sen: Take that take that says data set diet and then in that says data set.

713
01:45:55,950 --> 01:46:00,690
Ananda Sen: You you want to model, you want to.

714
01:46:00,690 --> 01:46:20,880
Ananda Sen: construct the other models So what does diet consists of now diet consists of all the data, you had in your database plus what is created, based on the model statement in the first statement first set of statement, so it has all the original variable so you just.

715
01:46:20,880 --> 01:46:32,430
Ananda Sen: construct a logistic regression with age BMI rate risk right fracture and and menopausal status right so that's the second.

716
01:46:32,430 --> 01:46:43,740
Ananda Sen: So you do both of those and then you again create another SAS data set which is called RC compare.

717
01:46:43,740 --> 01:46:57,930
Ananda Sen: And here, you throw in another predicted value, which is called score too so so I mean score two is another another variable you created, so what is score one score one is the predicted.

718
01:46:57,930 --> 01:47:16,860
Ananda Sen: probabilities from using model, one score two is the predicted probability from using model to model to being the model with the five variables and score one is just a single variable model.

719
01:47:16,860 --> 01:47:25,560
Ananda Sen: Then you have created those predicted variables and now you want to do a comparison of our scenes.

720
01:47:25,560 --> 01:47:30,180
Ananda Sen: So that's the third step, so what is the third step the third step.

721
01:47:30,180 --> 01:47:49,530
Ananda Sen: You use rock compare Arosa compared the why the reason you use RSA compare is back, because obviously it's all incremental you're you're creating the previous data set put that in the in the new data set a new database and carry on so rsc compare.

722
01:47:49,530 --> 01:47:59,910
Ananda Sen: In that database, you have everything, but you also, most importantly, you have scored one and score two.

723
01:47:59,910 --> 01:48:02,760
Ananda Sen: So those predicted probabilities.

724
01:48:02,760 --> 01:48:18,720
Ananda Sen: Okay now let's see, this is a, this is an interesting way of doing this, so you again use a logistic model, but this time your variables are score one and score to.

725
01:48:18,720 --> 01:48:23,910
Ananda Sen: But you don't want fit you just want to do the rsc is based on this.

726
01:48:23,910 --> 01:48:30,630
Ananda Sen: So I use that option called no fit and just don't worry about the.

727
01:48:30,630 --> 01:48:38,370
Ananda Sen: You know the the coefficient or anything like that i've already got the predicted probabilities i'm just going to do this.

728
01:48:38,370 --> 01:48:41,370
Ananda Sen: Do this rfc comparison.

729
01:48:41,370 --> 01:48:44,820
Ananda Sen: So.

730
01:48:44,820 --> 01:48:54,960
Ananda Sen: Those anything within parentheses, and the next two statements RC score one score to those are the names of labels, but the variables her score one and score two.

731
01:48:54,960 --> 01:49:01,620
Ananda Sen: And then the last statement is the important one, I want to compare them, I want to do a test.

732
01:49:01,620 --> 01:49:05,310
Ananda Sen: Between the two horses.

733
01:49:05,310 --> 01:49:08,850
Ananda Sen: and provide an estimate.

734
01:49:08,850 --> 01:49:14,310
Ananda Sen: Do you see the sequence of.

735
01:49:14,310 --> 01:49:17,970
Ananda Sen: So all of those things which.

736
01:49:17,970 --> 01:49:27,120
Ananda Sen: created this soul run the models separately London logistic models separately for each of the markers.

737
01:49:27,120 --> 01:49:36,030
Ananda Sen: Then, create a final data set which creates all those predicted probabilities which combines all those bigger problems and then do.

738
01:49:36,030 --> 01:49:43,440
Ananda Sen: A second it's actually not that bad I mean it's it's it's very logical, the way things are happening, the one thing to be careful about.

739
01:49:43,440 --> 01:49:53,460
Ananda Sen: In the first logistic run you have an outage equal to dialogue and that direct that data sitting have created that you carry over to the next one.

740
01:49:53,460 --> 01:50:05,160
Ananda Sen: And then the data said you created their out equal to rock compare that is a data you create you carry over to the third step so it's it's it's creating sequential.

741
01:50:05,160 --> 01:50:06,510
Okay.

742
01:50:06,510 --> 01:50:11,310
Ananda Sen: So that the final data actually contains everything.

743
01:50:11,310 --> 01:50:15,150
Ananda Sen: And, in particular, both discourse.

744
01:50:15,150 --> 01:50:20,640
Ananda Sen: Any question on this.

745
01:50:20,640 --> 01:50:24,000
Ananda Sen: Certainly, a bit longer than.

746
01:50:24,000 --> 01:50:30,180
Ananda Sen: than the other things you have done, but it's also, I think this.

747
01:50:30,180 --> 01:50:34,890
Ananda Sen: Pretty logical if you follow the steps.

748
01:50:34,890 --> 01:50:45,390
Ananda Sen: Once you have done that, what do you get well you get a pretty picture like this, so the red one is or two that's based on the.

749
01:50:45,390 --> 01:51:06,360
Ananda Sen: Much more elaborate model and and the new one is the the first score and we already know what the uc for the first one is point 6763 the second one is point seven so it's a little little more, but not a whole lot more.

750
01:51:06,360 --> 01:51:20,460
Ananda Sen: And then on the right hand side you have another table, which shows all the different things and i'm not going to go over those much the summers D and gamma and tower and all of those things but.

751
01:51:20,460 --> 01:51:30,900
Ananda Sen: Ultimately, what you want to do is to compare them did did the second score do a better job over the first score it's it's numerically more.

752
01:51:30,900 --> 01:51:49,530
Ananda Sen: There you see our ERC is more but does it actually do a significantly better job, and the answer is no, the Chi square is only point two, five, the p value is only point two, four, so, so the conclusion is.

753
01:51:49,530 --> 01:51:51,780
Ananda Sen: Core to.

754
01:51:51,780 --> 01:51:54,810
Ananda Sen: does not.

755
01:51:54,810 --> 01:51:58,500
Ananda Sen: improve.

756
01:51:58,500 --> 01:52:05,520
Ananda Sen: discrimination.

757
01:52:05,520 --> 01:52:14,760
significantly.

758
01:52:14,760 --> 01:52:18,570
Compared school.

759
01:52:18,570 --> 01:52:21,480
Ananda Sen: Because this is.

760
01:52:21,480 --> 01:52:31,980
Ananda Sen: More than point of.

761
01:52:31,980 --> 01:52:49,500
Ananda Sen: Any questions.

762
01:52:49,500 --> 01:52:55,320
Ananda Sen: Okay let's go back to the.

763
01:52:55,320 --> 01:53:00,180
Ananda Sen: lecture set which talks about goodness of fit.

764
01:53:00,180 --> 01:53:13,020
Ananda Sen: So the two things I wanted to do today are talking about the receiver operating characteristics which is basically a discrimination problem and the goodness of fit is how good the model, it is.

765
01:53:13,020 --> 01:53:19,800
Ananda Sen: And that will go through some concepts which we have not talked about before, so I want to.

766
01:53:19,800 --> 01:53:21,870
Do it slowly.

767
01:53:21,870 --> 01:53:33,150
Ananda Sen: But before there any question.

768
01:53:33,150 --> 01:53:45,570
Sorry.

769
01:53:45,570 --> 01:53:51,510
Ananda Sen: So we briefly visited this idea.

770
01:53:51,510 --> 01:53:54,420
Ananda Sen: When we're talking about linear regression.

771
01:53:54,420 --> 01:54:06,540
Ananda Sen: you're fitting a model to the end doesn't matter whether it's a linear regression or a logistic regression or a person regression or whatever it is, when you're fitting model fitting a model for the data.

772
01:54:06,540 --> 01:54:20,580
Ananda Sen: That should be the first thing you should be looking at, you know how good is the fit you can you can just fit a lousy model and and and say the value and the model did.

773
01:54:20,580 --> 01:54:41,460
Ananda Sen: I mean model Ram but that doesn't mean that it's a good fit I mean you one very crude check would be if you ran a model and saw that 80% of the of the variables are insignificant that that's a good indication that the the fit was not that great.

774
01:54:41,460 --> 01:54:54,600
Ananda Sen: But association actually can be misnomer, because you could have a model where there are several significant variables, yet the overall fit may not be that great.

775
01:54:54,600 --> 01:54:59,700
Ananda Sen: So what are the ways of.

776
01:54:59,700 --> 01:55:01,170
Ananda Sen: Of.

777
01:55:01,170 --> 01:55:23,460
Ananda Sen: finding out whether the whether the model was good or not, then generally two different ways, where the fit can be assessed and one of the residual based approach the other one is a predictive power based approach so so let's first talk about a concept called.

778
01:55:23,460 --> 01:55:29,940
Ananda Sen: concept of deviance yes, what is it deviance.

779
01:55:29,940 --> 01:55:39,240
Ananda Sen: Well, at any stage of any model feeling, you have a model with a certain number of covariance.

780
01:55:39,240 --> 01:55:49,800
Ananda Sen: Okay, so say you have six score variants in a model and and that's your current model.

781
01:55:49,800 --> 01:55:57,300
Ananda Sen: Well, there are two other models one model is the know which doesn't have any cool very.

782
01:55:57,300 --> 01:56:04,200
Ananda Sen: And, and any or your covariance is.

783
01:56:04,200 --> 01:56:09,960
Ananda Sen: I mean basically basically that's that's that's that has only intercepting the model.

784
01:56:09,960 --> 01:56:12,060
Ananda Sen: that's another model.

785
01:56:12,060 --> 01:56:22,590
Ananda Sen: And then there is also the saturated model what is a saturated model, the saturated model is the model which does the best job.

786
01:56:22,590 --> 01:56:32,730
Ananda Sen: Giving your data or giving you a covariance that's it, I mean you cannot do any better that's the that's the achievable that's the that's the limit.

787
01:56:32,730 --> 01:56:42,660
Ananda Sen: Okay, so the objective is to get the current model close to the saturated model your current model will not be equal to the saturated model because saturated model.

788
01:56:42,660 --> 01:56:54,690
Ananda Sen: is a model which doesn't allow you do to do anything with the model of your fit fitted perfectly no no allowance for air editor.

789
01:56:54,690 --> 01:57:13,680
Ananda Sen: So the idea is get your current model close to the close to the saturated model and and and if if the difference between the saturation and the current model is relatively high, then he would say that.

790
01:57:13,680 --> 01:57:26,280
Ananda Sen: Sorry relatively low, then, then he would say that the current model is doing a pretty decent job in terms of fee okay that's how that's how it is.

791
01:57:26,280 --> 01:57:34,140
Ananda Sen: It is done so it's it's a formalized that it's a long likelihood of saturated model minus log like that.

792
01:57:34,140 --> 01:57:35,640
Ananda Sen: But.

793
01:57:35,640 --> 01:57:43,980
Ananda Sen: But that's more of a mathematical thing, but the idea is actually depicted in the picture okay.

794
01:57:43,980 --> 01:57:49,110
Ananda Sen: All right.

795
01:57:49,110 --> 01:58:04,590
Ananda Sen: So let's understand try to understand what the saturated model so saturated model has a separate parameter for each predicted probability.

796
01:58:04,590 --> 01:58:15,510
Ananda Sen: Each predicted probability and and produces a perfect fit so so basically.

797
01:58:15,510 --> 01:58:25,650
Ananda Sen: When I see, is a perfect fit the individual with individual level data, the log likelihood for the saturated model is zero.

798
01:58:25,650 --> 01:58:42,900
Ananda Sen: arm and is somewhat useless because for each data point if you have a parameter so let's say there are 500 data points you have parameter for each data point then it's it's useless model because because you're filling the data perfectly sure.

799
01:58:42,900 --> 01:58:57,210
Ananda Sen: But it's a but the but, but that has a log likelihood of of zero and it's it's it's not it's it's not possible to do anything with that it's an all parameters model, I mean imagine.

800
01:58:57,210 --> 01:59:06,690
Ananda Sen: A data set with tight 500 people or 500 subjects and you're fitting 500 parameter model, yes, of course, it fits everything.

801
01:59:06,690 --> 01:59:22,110
Ananda Sen: But it's it's an useless model so so what what we can do, we can also even defining saturated model we can come up with some kind of compromise so.

802
01:59:22,110 --> 01:59:36,360
Ananda Sen: And this is meaningful only when models have categorical variables, if you want to have models with continuous variables some of this is breaking down now, this does break down but let's try to understand this from the.

803
01:59:36,360 --> 01:59:43,020
Ananda Sen: categorical categorical variables standpoint so.

804
01:59:43,020 --> 01:59:46,740
Ananda Sen: For categorical variables.

805
01:59:46,740 --> 01:59:53,250
Ananda Sen: let's say i've got gender, race, in each category, these are, these are the three things i'm interested in.

806
01:59:53,250 --> 01:59:55,320
Ananda Sen: So i've got.

807
01:59:55,320 --> 02:00:01,560
Ananda Sen: two categories of gender three categories of race and five categories of each.

808
02:00:01,560 --> 02:00:10,500
Ananda Sen: So I will fit one parameter to each covariance pattern So what do you mean by corporate pattern.

809
02:00:10,500 --> 02:00:17,160
Ananda Sen: So corporate parenting something like this male black.

810
02:00:17,160 --> 02:00:22,230
Ananda Sen: The smallest age group that's a cool video pattern.

811
02:00:22,230 --> 02:00:30,390
Ananda Sen: and other bad and would be males white middle age group.

812
02:00:30,390 --> 02:00:33,180
Okay.

813
02:00:33,180 --> 02:00:39,660
Ananda Sen: All together, there are 30 possibilities 30 covariance patterns.

814
02:00:39,660 --> 02:00:46,650
Ananda Sen: Because for each gender and each race, you have five categories of age.

815
02:00:46,650 --> 02:00:54,150
Ananda Sen: And then each of those five categories can be attached to each of the races for each gender so that's 15.

816
02:00:54,150 --> 02:00:55,800
Ananda Sen: And then.

817
02:00:55,800 --> 02:01:02,400
Ananda Sen: You have this for males you have this for females and that's that's terrific so it's two times three times five.

818
02:01:02,400 --> 02:01:06,420
Ananda Sen: that's the number of cool video patterns.

819
02:01:06,420 --> 02:01:16,290
Ananda Sen: If you're fitting a model to each comedy pattern at the total number of Kobe.

820
02:01:16,290 --> 02:01:27,090
Ananda Sen: Okay, and the best possible fit using these variables fits the 30 cents per se perfectly using 30 parameters.

821
02:01:27,090 --> 02:01:37,350
Ananda Sen: Now what we compare is the fit of the reduced model to the saturated model reduce model means the current model.

822
02:01:37,350 --> 02:01:39,330
Okay.

823
02:01:39,330 --> 02:01:53,130
Ananda Sen: Why do I say reduce model, I mean look at the covariance parent, yet there are 30 to 30 parameters, if I use this for our logistic regression, how many how many parameters, would I use gender to.

824
02:01:53,130 --> 02:02:07,530
Ananda Sen: race so gender one actually one race to one plus two three age category four, so one plus two three plus four seven.

825
02:02:07,530 --> 02:02:25,980
Ananda Sen: Even if there are some interactions I will hardly come up with more than 10 or 15 variables 15 parameters, but that's not the saturated model, the question is the reduced model which I have now how close it comes to become.

826
02:02:25,980 --> 02:02:31,140
Ananda Sen: It gets to the century model remember a new smalls will always have words fit.

827
02:02:31,140 --> 02:02:42,960
Ananda Sen: But it's a difference between that and the best possible is what we're looking.

828
02:02:42,960 --> 02:02:59,400
Ananda Sen: So full full model which is a saturated model all variation and data is explained by signal, because it's all your parameters are explaining that now model, there is nothing and its current model P parameters.

829
02:02:59,400 --> 02:03:05,310
Ananda Sen: The current model and the question is, is a satisfactory fit.

830
02:03:05,310 --> 02:03:11,190
Ananda Sen: So deviance should be small if model fits well now how long how small.

831
02:03:11,190 --> 02:03:26,490
Ananda Sen: Now, if all kept covariance of categorical you can actually do this do this, using a Chi square distribution and the guys great degrees of freedom would be number of coverage parents minus speech therapies, the number of parameters, including intersect.

832
02:03:26,490 --> 02:03:28,290
Okay.

833
02:03:28,290 --> 02:03:34,710
Ananda Sen: Now I have some covert is a continuous, then the deviance is not Chi square that's the problem.

834
02:03:34,710 --> 02:03:52,380
Ananda Sen: So P values are really not appropriate, but when people still calculate the deviant and look at it and what what they do, then, is that they look at the scale deviance divided by degrees of freedom and it's pretty close to one, then they sell it well.

835
02:03:52,380 --> 02:04:03,840
Ananda Sen: That, that is, that is fine, so it's more of an ad hoc thing, rather than a test, but if it is a categorical you can actually do the test.

836
02:04:03,840 --> 02:04:15,060
Ananda Sen: there's also the idea of Pearson Chi Square, which is an alternative statistic for testing goodness of fit and the same recommendations work.

837
02:04:15,060 --> 02:04:19,320
Ananda Sen: So how How did this work, and again I.

838
02:04:19,320 --> 02:04:36,450
Ananda Sen: Am i'm asking you not to pay too much attention to the actual formula, but but let's understand the way this works so suppose that a J unit observed cooperate patterns, like in the previous example we have 30.

839
02:04:36,450 --> 02:04:48,060
Ananda Sen: Now, consider the zero and one outcomes for each of these parents so so some of them will have zero outcome, some of them will have one outcome, so there are actually two times J cells.

840
02:04:48,060 --> 02:04:49,590
Okay.

841
02:04:49,590 --> 02:04:52,080
Ananda Sen: So.

842
02:04:52,080 --> 02:04:59,640
Ananda Sen: orgy and EG are observing expected frequencies in cell J each of those cells.

843
02:04:59,640 --> 02:05:02,040
Ananda Sen: Now.

844
02:05:02,040 --> 02:05:07,080
Ananda Sen: EG is derived from the fitted model.

845
02:05:07,080 --> 02:05:12,420
Ananda Sen: Some model I don't care it's a logistic regression model anyway.

846
02:05:12,420 --> 02:05:15,630
Ananda Sen: And OJ is the observed one.

847
02:05:15,630 --> 02:05:19,470
Ananda Sen: observe frequency and celgene.

848
02:05:19,470 --> 02:05:23,970
Ananda Sen: So let's let's try to understand this in the context of.

849
02:05:23,970 --> 02:05:25,740
Ananda Sen: All we had.

850
02:05:25,740 --> 02:05:28,680
Ananda Sen: main black.

851
02:05:28,680 --> 02:05:32,970
Ananda Sen: lowest age group.

852
02:05:32,970 --> 02:05:44,370
Ananda Sen: Some of the male black lowest age group will have zero outcomes, some of the male black lowest age group will have one outcome in your data.

853
02:05:44,370 --> 02:05:48,060
Ananda Sen: Right so that's observed.

854
02:05:48,060 --> 02:06:05,550
Ananda Sen: Then, based on your model, what do you expect how many people would be there in any of those cells.

855
02:06:05,550 --> 02:06:11,190
Ananda Sen: Anyone can do this for each of those two times J cells.

856
02:06:11,190 --> 02:06:18,810
Ananda Sen: That make sense.

857
02:06:18,810 --> 02:06:22,710
Ananda Sen: Okay.

858
02:06:22,710 --> 02:06:24,690
Ananda Sen: So, again.

859
02:06:24,690 --> 02:06:32,910
Ananda Sen: They are both guys square approximately as long as we have categorical cooperates.

860
02:06:32,910 --> 02:06:42,840
Ananda Sen: If you have continuous covariance then it doesn't work that doesn't work in the sense that it doesn't give you guys square but that's that's more of a technical issue, so.

861
02:06:42,840 --> 02:06:53,760
Ananda Sen: always remember that you can calculate the deviance even with continuous forwards, but but it's it's it's it's easier to understand what this What this really means.

862
02:06:53,760 --> 02:07:06,510
Ananda Sen: One more thing I want to point out deviants versus global Chi square I didn't I didn't fill out the gap in the lower half or the lower part of this of this graph.

863
02:07:06,510 --> 02:07:20,160
Ananda Sen: So his current model better than our model that's a Chi square distribution that's a Chi square test the global Chi square that you can do always and you might come up with with a significance.

864
02:07:20,160 --> 02:07:25,170
Ananda Sen: The question is, is it the best you can get.

865
02:07:25,170 --> 02:07:29,730
Ananda Sen: that's what is measured by deviance.

866
02:07:29,730 --> 02:07:31,350
Ananda Sen: So.

867
02:07:31,350 --> 02:07:46,560
Ananda Sen: This is this is slightly different for deviance non significance is good, because we want to get close to the to the top line.

868
02:07:46,560 --> 02:07:55,530
Ananda Sen: For global guys square the significance is good.

869
02:07:55,530 --> 02:08:07,410
Ananda Sen: You see, the contrast you want that this distance distance between now and the current model to be high, you want the distance between the current the saturated model to be low.

870
02:08:07,410 --> 02:08:18,630
Ananda Sen: So P value larger than point of five for deviance is good P value less than Point two five for the current model is good.

871
02:08:18,630 --> 02:08:25,980
Ananda Sen: For the for the global Chi square is good.

872
02:08:25,980 --> 02:08:27,750
Ananda Sen: A question, yes.

873
02:08:27,750 --> 02:08:36,810
Yoonhee Ryder: I guess I maybe I missed this is beginning but i'm just trying to see what the differences between the deviants or.

874
02:08:36,810 --> 02:08:45,810
Yoonhee Ryder: The deviance and if you are doing the area under the curve like because to me they're both comparing like a gold standard to your test.

875
02:08:45,810 --> 02:08:56,220
Yoonhee Ryder: So is there, like a difference in the type of information that you're getting there are they both just kind of showing how good your test is compared to another test.

876
02:08:56,220 --> 02:08:59,070
So.

877
02:08:59,070 --> 02:09:04,800
Ananda Sen: So I would say they are they had they're very similar in in.

878
02:09:04,800 --> 02:09:07,020
Ananda Sen: In.

879
02:09:07,020 --> 02:09:14,700
Ananda Sen: I guess one way I would think about this is that they're approaching the problem from two different angles.

880
02:09:14,700 --> 02:09:19,320
Ananda Sen: They will not necessarily.

881
02:09:19,320 --> 02:09:26,460
Ananda Sen: come up with the same type of answer but it's but.

882
02:09:26,460 --> 02:09:40,140
Ananda Sen: Our oC is predicting are actually looking at the predictive capability, based on the prediction power right.

883
02:09:40,140 --> 02:09:53,760
Ananda Sen: I mean that's that's what I was trying to mention, in a few few slides back that it's it's really the the prediction of of the event is important there, and there is a measure which says that how good your prediction is.

884
02:09:53,760 --> 02:09:58,380
Ananda Sen: damian's is is more like.

885
02:09:58,380 --> 02:10:00,570
Ananda Sen: here's a model.

886
02:10:00,570 --> 02:10:12,960
Ananda Sen: Here is what I can, what I can do, based on prediction how how good how small is a residual So this is the residual kind of.

887
02:10:12,960 --> 02:10:15,270
Ananda Sen: The difference.

888
02:10:15,270 --> 02:10:18,150
Ananda Sen: You are you can see.

889
02:10:18,150 --> 02:10:23,010
Ananda Sen: That essentially you're trying to do the same thing and I completely agree with you on that.

890
02:10:23,010 --> 02:10:34,890
Ananda Sen: That that the the the ultimate end goal is the same you're trying to find a model which does the best job of prediction but.

891
02:10:34,890 --> 02:10:41,520
Ananda Sen: it's it's it's a little different, though, in the sense that.

892
02:10:41,520 --> 02:10:49,080
Ananda Sen: it's one of the approaches are different, one of them abroad, yes it on the basis of.

893
02:10:49,080 --> 02:11:02,400
Ananda Sen: How close it can get to the maximum, if you can get to so, so it is a concept of the best you can do and how how close you get to the best you can do for.

894
02:11:02,400 --> 02:11:16,620
Ananda Sen: For our oC there is no best you can do 100%, of course, but but you're not really comparing it with any any any bar or anything like that you're just measuring that and say that well I have.

895
02:11:16,620 --> 02:11:28,020
Ananda Sen: I have a 70% RC RC and that's my that's my measure of fitness if I include more variables, can I actually get it.

896
02:11:28,020 --> 02:11:42,630
Ananda Sen: get it bigger and that 70% actually goes higher that's not necessarily true, though I mean it can actually go lower depending on what you get so to answer your question they're not the same things but.

897
02:11:42,630 --> 02:11:56,370
Ananda Sen: The background they're trying to do the same thing they approach they're trying to do the same thing with different approaches yes Okay, thank you yeah.

898
02:11:56,370 --> 02:12:02,100
Ananda Sen: Other questions.

899
02:12:02,100 --> 02:12:13,860
Ananda Sen: Alright there's one more test which is done in this context called whole smart lemme shot test, and that is actually another test which comes about.

900
02:12:13,860 --> 02:12:16,110
Ananda Sen: Very.

901
02:12:16,110 --> 02:12:27,660
Ananda Sen: it's a similar to the deviance there's except the nice thing about horseman lemme short test is that he doesn't care whether the covert it is continuous categorical, which is why this has got a lot more.

902
02:12:27,660 --> 02:12:30,090
Ananda Sen: usability.

903
02:12:30,090 --> 02:12:41,070
Ananda Sen: So, so what it does, is it just fits the model and then fits the end and then calculate the probabilities and then looked at the record properties and just creates propensity.

904
02:12:41,070 --> 02:12:58,980
Ananda Sen: Core tiles of one tiles for does break the probabilities decides is actually quite common So those are the groups and within each group they expected frequency is obtained by adding them the predicted probabilities for each of the individuals.

905
02:12:58,980 --> 02:13:02,250
Ananda Sen: And then you have the observed.

906
02:13:02,250 --> 02:13:23,760
Ananda Sen: thing also so, then they just do the Pearson statistic remember that observed minus expected square by expected that's what it calculates for each of those groups and then that's a Chi square in general, not necessarily just for continuous covariance or or.

907
02:13:23,760 --> 02:13:43,200
Ananda Sen: categorical various the degrees of freedom is number of groups minus two that's that's the theoretical part of it, but this is most general and this is, this does not care whether the character, the covariance of continuous or categorical.

908
02:13:43,200 --> 02:13:49,170
Okay.

909
02:13:49,170 --> 02:13:57,960
Ananda Sen: So goodness of fit statistics, maybe non significant which basically means the current model is good.

910
02:13:57,960 --> 02:14:07,980
Ananda Sen: It is, however, in comparison with other models, using the same covariance just probably adding interactions or something like that, using the same code.

911
02:14:07,980 --> 02:14:21,660
Ananda Sen: Now, if you have a new possibility which you want to add that may give us significantly improve it just just a just a caution about interpreting goodness of fit test.

912
02:14:21,660 --> 02:14:25,140
Ananda Sen: Alright, I think we're.

913
02:14:25,140 --> 02:14:28,770
Ananda Sen: at a point where we can take a break.

914
02:14:28,770 --> 02:14:34,230
Ananda Sen: it's a little bit left in the.

915
02:14:34,230 --> 02:23:21,330
Ananda Sen: In the lecture set, and then we will we will look at the lab part, so why don't we come back at four and finish up the rest.

916
02:23:21,330 --> 02:23:26,340
Ananda Sen: alright.

917
02:23:26,340 --> 02:23:32,010
Ananda Sen: To the.

918
02:23:32,010 --> 02:23:38,250
Ananda Sen: last session of day three.

919
02:23:38,250 --> 02:23:41,130
Ananda Sen: Any other questions.

920
02:23:41,130 --> 02:23:46,470
Ananda Sen: about what we had been talking about, so the goodness of fit.

921
02:23:46,470 --> 02:23:59,160
Ananda Sen: And we're a little bit left yet, but.

922
02:23:59,160 --> 02:24:03,210
Ananda Sen: All right.

923
02:24:03,210 --> 02:24:14,370
Ananda Sen: So let me explain another another concept which does in another way, and goodness a fifth.

924
02:24:14,370 --> 02:24:17,310
Ananda Sen: Goodness of fit.

925
02:24:17,310 --> 02:24:19,140
Ananda Sen: calculation.

926
02:24:19,140 --> 02:24:24,330
Ananda Sen: So it's a it's a measure of explained variation so.

927
02:24:24,330 --> 02:24:36,570
Ananda Sen: You know the way we do it for linear regression, we talked a little bit about R squared R squared is is like the proportion of variation explained.

928
02:24:36,570 --> 02:24:53,490
Ananda Sen: In by using the X variables in the model so that's very well defined for linear regression, if I have a continuous variable and have a linear regression R squared is very well defined and it has got nice properties and it's it's a nice.

929
02:24:53,490 --> 02:25:05,670
Ananda Sen: thing to look at that's not true for other aggression, not for logistic regression, not for counter aggression and for other months, however, there is something which is, which is close.

930
02:25:05,670 --> 02:25:12,720
Ananda Sen: it's called generalized liver R squared, which is also used for.

931
02:25:12,720 --> 02:25:16,440
Ananda Sen: assessing how good the model is.

932
02:25:16,440 --> 02:25:19,140
Ananda Sen: So.

933
02:25:19,140 --> 02:25:39,480
Ananda Sen: Again, not going into the details of the technicality what it does, essentially, is it calculates the likelihood of the fitness model and compares it with the now so essentially it is doing something like a global Chi square except it's doing it through a slightly different.

934
02:25:39,480 --> 02:25:52,560
Ananda Sen: Calculation but it, but it also adjust for the fact that we cannot do this, our square calculation up 200% for linear regression that's possible.

935
02:25:52,560 --> 02:26:01,500
Ananda Sen: linear regression, you can go up 200% it doesn't happen it doesn't happen it's it's it's the theoretical maximum you can get.

936
02:26:01,500 --> 02:26:14,760
Ananda Sen: But for for the logistic regression you actually can calculate how much you can do, how good you can do it's it's, the idea is like deviance there the maximum.

937
02:26:14,760 --> 02:26:36,060
Ananda Sen: maximum possible R squared can do it normalizes by that so that's why it's called maximum reach scale R square or generalized R squared which does not once you divide by that, then you kind of know that that the maximum scale, one can actually go from zero to 100%.

938
02:26:36,060 --> 02:26:42,450
Ananda Sen: It is true in a similar to the traditional R squared in in the interpretation.

939
02:26:42,450 --> 02:26:49,680
Ananda Sen: it's very, very generally applicable doesn't have to be just logistic regression.

940
02:26:49,680 --> 02:27:07,110
Ananda Sen: And it is also something which increases when you add variable to the model so, so it is in something which can aid in assessing how much you added in terms of contribution when you add different variables.

941
02:27:07,110 --> 02:27:14,220
Ananda Sen: And again it's in very into grouping some bad same bellwether one uses group or individual data.

942
02:27:14,220 --> 02:27:20,100
Ananda Sen: As long as cases that only group together if they're identical and all independent variable.

943
02:27:20,100 --> 02:27:23,790
Okay.

944
02:27:23,790 --> 02:27:26,220
All right.

945
02:27:26,220 --> 02:27:28,980
Ananda Sen: So so that's another measure.

946
02:27:28,980 --> 02:27:40,290
Ananda Sen: Are there other measures, well, there is one or there are some more which are based on model comparison.

947
02:27:40,290 --> 02:27:52,920
Ananda Sen: So, ultimately, if you think of this problem of goodness of fit it's always comparing models right either another models your current model current model to a saturated model.

948
02:27:52,920 --> 02:28:07,410
Ananda Sen: You have got a bunch of covariance we you, you want to see whether the AUC is better than another model with the lesser coverage it's always comparing models.

949
02:28:07,410 --> 02:28:18,060
Ananda Sen: So, so if you have a measure which is based on an overall model fit that that is actually going to do the job all right.

950
02:28:18,060 --> 02:28:32,130
Ananda Sen: So, based on that concept, there are there are model comparisons former model comparisons if the models are nested, then you can do a Chi square What do you mean by nested models nested model means.

951
02:28:32,130 --> 02:28:41,520
Ananda Sen: You have a bunch of variables you fit a model then take out some of the variables basically say, well, I think I don't need that many models, I think.

952
02:28:41,520 --> 02:28:57,600
Ananda Sen: Maybe these three variables are not important just take those three variables out just refit the model that's a nested model because one model is embedded in the other model, you have the larger model in a smaller model.

953
02:28:57,600 --> 02:28:59,280
Ananda Sen: Right.

954
02:28:59,280 --> 02:29:01,320
Ananda Sen: Well, it doesn't have to be that.

955
02:29:01,320 --> 02:29:06,270
Ananda Sen: You can have two separate group of models altogether.

956
02:29:06,270 --> 02:29:12,420
Ananda Sen: Which are called non nested if this is non nested model, then you cannot do this Chi square type of thing.

957
02:29:12,420 --> 02:29:20,430
Ananda Sen: But then there are other criteria, like a Chi key and Beijing information criteria, which are all used in the.

958
02:29:20,430 --> 02:29:29,670
Ananda Sen: In almost all software's these are based again on likelihood likelihood, and the number of parameters, you have fitted in the model.

959
02:29:29,670 --> 02:29:34,500
Ananda Sen: The sample size and all of those things.

960
02:29:34,500 --> 02:29:49,530
Ananda Sen: Again, you don't need to worry about the actual formula that the interpretation, the smaller the EIC or bi see the better remember he ICB ic is always for model comparison.

961
02:29:49,530 --> 02:29:56,130
Ananda Sen: If you are just filling one single model than a ice ice is is meaningless.

962
02:29:56,130 --> 02:30:11,970
Ananda Sen: Because, because the ICB is is designed as a model competitive comparison criteria so unless you have at least two models, then you have no use of a CBS.

963
02:30:11,970 --> 02:30:21,150
Ananda Sen: One thing about Ai CBS is, though, there is no, there is no P value here it's all ad hoc.

964
02:30:21,150 --> 02:30:29,340
Ananda Sen: So that's goodness of fit and model comparison a slight slant to this procedure is.

965
02:30:29,340 --> 02:30:41,160
Ananda Sen: Variable selection, this is something we talked about on the first day we talked a little bit about it, or maybe yesterday in the beginning.

966
02:30:41,160 --> 02:30:50,970
Ananda Sen: You have a whole bunch of covert rates as whole slew of covariance you want to know which one does the best job or which group does the best job.

967
02:30:50,970 --> 02:31:03,480
Ananda Sen: There are three selection procedures people use a stepwise backward and forward so the software selection is what I described before, let me just go over it one more time.

968
02:31:03,480 --> 02:31:31,980
Ananda Sen: We have an available pool of independent variables, each of which is considered.

969
02:31:31,980 --> 02:31:34,500
Okay.

970
02:31:34,500 --> 02:31:41,580
Ananda Sen: So so so that one is.

971
02:31:41,580 --> 02:32:00,270
Ananda Sen: You have this all these all the different variables and you just want to know which group subgroup of those P variables are going to do the best job of of the art or, in other words, do you need to use all the variables, or you can do a subset.

972
02:32:00,270 --> 02:32:05,610
Ananda Sen: So how do you do that, how do you figure out which ones are the best of the p.

973
02:32:05,610 --> 02:32:11,250
Ananda Sen: So again, you start with the simplest model, what is the simplest model is a single covidien more.

974
02:32:11,250 --> 02:32:18,840
Ananda Sen: And you compare that with the novel model which is no cody to knock over the verses one coffee.

975
02:32:18,840 --> 02:32:34,560
Ananda Sen: come up with the best in that what is the best in there when you do a like your ratio test and calculate the p value and choose the one with the minimum just a minimum be whether even if there are others which are significant just choose the minimum fee that.

976
02:32:34,560 --> 02:32:37,590
Ananda Sen: Okay.

977
02:32:37,590 --> 02:32:40,110
Ananda Sen: So you do.

978
02:32:40,110 --> 02:32:56,430
Ananda Sen: Have a pre determined thresholds for P value, and if you need think of xe one to be do the most important P value if you if you do a forward selection or considered the most important variable tend to the model.

979
02:32:56,430 --> 02:33:09,720
Ananda Sen: On the other hand, if the minimum P value is larger than the E larger than the entry that stop and conclude that there is no predictive independent variable.

980
02:33:09,720 --> 02:33:11,580
Right.

981
02:33:11,580 --> 02:33:14,220
Ananda Sen: So.

982
02:33:14,220 --> 02:33:20,610
Ananda Sen: You can stop in the first stage stage and say that why this is this is done I don't have to.

983
02:33:20,610 --> 02:33:26,700
Ananda Sen: Go any farther but, hopefully, you have at least one variable for which.

984
02:33:26,700 --> 02:33:29,220
Ananda Sen: You have.

985
02:33:29,220 --> 02:33:36,120
Ananda Sen: You can enter, along with the intersect now, what are the next step, once you have that.

986
02:33:36,120 --> 02:33:39,690
Ananda Sen: You have the first variable in the model.

987
02:33:39,690 --> 02:33:53,340
Ananda Sen: You try to add a second variable in the model after you have the first variable so so your model would be let's say i'm hd turns out to be the best model in your in your own.

988
02:33:53,340 --> 02:33:57,600
Ananda Sen: or in this case actually we're talking about a different data set now.

989
02:33:57,600 --> 02:33:59,970
Ananda Sen: let's say.

990
02:33:59,970 --> 02:34:15,420
Ananda Sen: Prior fracture is the best variable in the model based on area so keeping prior factor in the model, what would be the second model second variable which will do the best job in the sense that that has.

991
02:34:15,420 --> 02:34:20,550
Ananda Sen: The most significance and you any any calculate that.

992
02:34:20,550 --> 02:34:22,410
Ananda Sen: and

993
02:34:22,410 --> 02:34:28,410
Ananda Sen: suppose you find something which is X to to be the most important.

994
02:34:28,410 --> 02:34:43,290
Ananda Sen: Additional variable which does the fastest the entry criteria if there is none then stop it, the first variable and that's it, but if there is one, then go to the next stage.

995
02:34:43,290 --> 02:34:47,070
Ananda Sen: Now here is a twist to the problem.

996
02:34:47,070 --> 02:35:00,330
Ananda Sen: This could have happened you just kept on adding variables adding variables until you, you find you come to a stage where there is nothing which is significant, if that happened.

997
02:35:00,330 --> 02:35:06,570
Ananda Sen: Then you would you would you would say that.

998
02:35:06,570 --> 02:35:19,140
Ananda Sen: That that's the that's the best model, but that that is called forward selection for selection because you're you're only adding on variables you're not dropping ever.

999
02:35:19,140 --> 02:35:30,750
Ananda Sen: Now, since stepwise selection, you could do the do the reverse also so once you have a model which is X X one X two X one X two.

1000
02:35:30,750 --> 02:35:40,380
Ananda Sen: Now you compare it with the universe that model using X one X two alone.

1001
02:35:40,380 --> 02:35:49,620
Ananda Sen: And if the maximum P value from those two comparisons is less than a predetermined threshold.

1002
02:35:49,620 --> 02:36:03,810
Ananda Sen: Then you retain both variables in the model and move to the stage for searching the best, but if it is not, you know, in one of the one of those P value of comparisons actually larger than we are.

1003
02:36:03,810 --> 02:36:13,830
Ananda Sen: Then you remove that variable so essentially what am I what I mean by that is a variable that enter the previous in the previous stage can be dropped.

1004
02:36:13,830 --> 02:36:24,210
Ananda Sen: This is called backward elimination, it might sound a little confusing but, but the whole idea is you want to.

1005
02:36:24,210 --> 02:36:36,030
Ananda Sen: construct a model which does the best job in terms of predicting and each of them is significant.

1006
02:36:36,030 --> 02:36:42,300
Ananda Sen: sufficiently significant to stay in the model.

1007
02:36:42,300 --> 02:36:51,930
Ananda Sen: So you continue doing this until you come to a stage where, then you can either do forward selection not do backward elimination.

1008
02:36:51,930 --> 02:36:55,140
Ananda Sen: And that's your final mom.

1009
02:36:55,140 --> 02:37:13,080
Ananda Sen: What about the threshold, the threshold at 20% to 35% both P value thresholds for both entry animal.

1010
02:37:13,080 --> 02:37:27,990
Ananda Sen: One final remark about this is the ones that variables are selected, one can explore the interaction high order terms all fit diagnostics, the ones we talked about should still be carried out on the final model.

1011
02:37:27,990 --> 02:37:38,490
Ananda Sen: And, and a new remember the variable selection is basically about coming out coming up with.

1012
02:37:38,490 --> 02:37:45,600
Ananda Sen: A set of variables, which are most predictive but that doesn't mean that it does a great job of fitting.

1013
02:37:45,600 --> 02:37:52,560
Ananda Sen: Most predictive doesn't necessarily mean that it's it's I mean they're related but they're not exactly the same.

1014
02:37:52,560 --> 02:38:10,080
Ananda Sen: When when are the cases when this is an important way of doing this, so, so why can't they just fit a model and and and just retain the ones which are less than point 05, the problem is, if you do it that way, sometimes you.

1015
02:38:10,080 --> 02:38:19,470
Ananda Sen: Well, first of all, you might end up getting values or variables which.

1016
02:38:19,470 --> 02:38:28,230
Ananda Sen: which you just chose by chance, it can happen in a in a problem in a genetic problem where.

1017
02:38:28,230 --> 02:38:44,640
Ananda Sen: You have say 500 genes and only thousand observations were 500 genes is 500 variables so so just retaining those that's doing a quick.

1018
02:38:44,640 --> 02:38:49,560
Ananda Sen: model using those 500 might give you some.

1019
02:38:49,560 --> 02:38:55,590
Ananda Sen: False significance it's like it's like false type it's like Type one error right.

1020
02:38:55,590 --> 02:39:01,980
Ananda Sen: So that's why something like this is very, very useful this variable selection method.

1021
02:39:01,980 --> 02:39:10,920
Ananda Sen: let's let's try to see this is the, this is just the concept so let's try to see how this translates to.

1022
02:39:10,920 --> 02:39:17,010
Ananda Sen: Our.

1023
02:39:17,010 --> 02:39:20,040
Ananda Sen: Practice the goodness of fit.

1024
02:39:20,040 --> 02:39:24,990
Ananda Sen: So.

1025
02:39:24,990 --> 02:39:33,090
Ananda Sen: The first thing we want to do is to look at the goodness of fit.

1026
02:39:33,090 --> 02:39:35,430
Ananda Sen: And the goodness of fit.

1027
02:39:35,430 --> 02:39:59,910
Ananda Sen: I have a model which is again factory vertical to one and I have got what smoking status brad fracture mom frack rate risk and arm assist many of these are one zero variables, but i'm not designating them as class, partly because, if it is a one zero variable I don't really need.

1028
02:39:59,910 --> 02:40:23,550
Ananda Sen: And then I have those statements aggregate scale equal to none, and our rescue and lack feed our rescue is very skilled our square lack of faith is host mar limit show and aggregate a believe does the deviance criteria.

1029
02:40:23,550 --> 02:40:28,740
Ananda Sen: Okay, so um.

1030
02:40:28,740 --> 02:40:44,190
Ananda Sen: So let's see the deviant at Pearson goodness of fit statistics deviance value is 21.6633 it was a freedom is 32 and the p value is point 9161 that is good.

1031
02:40:44,190 --> 02:40:46,500
Ananda Sen: This is good.

1032
02:40:46,500 --> 02:40:54,660
Ananda Sen: This basically says there is not a significant lack of fit we are certain is point 9682.

1033
02:40:54,660 --> 02:40:56,550
Ananda Sen: that's also good.

1034
02:40:56,550 --> 02:41:06,600
Ananda Sen: This is this is pretty high P value which basically indicates.

1035
02:41:06,600 --> 02:41:17,370
Ananda Sen: So this is.

1036
02:41:17,370 --> 02:41:26,970
Ananda Sen: No significant.

1037
02:41:26,970 --> 02:41:31,020
Ananda Sen: degrees of freedom is 32.

1038
02:41:31,020 --> 02:41:35,820
Ananda Sen: Why.

1039
02:41:35,820 --> 02:41:43,740
Ananda Sen: Number of unique profiles is 38 here, well, I said 38 here because.

1040
02:41:43,740 --> 02:41:49,170
Ananda Sen: Not all of them is continuous so, even if there are.

1041
02:41:49,170 --> 02:41:55,920
Ananda Sen: Six variables or five variables, it is.

1042
02:41:55,920 --> 02:42:28,200
Ananda Sen: 38, why is it degrees of freedom 32 can anybody tell me.

1043
02:42:28,200 --> 02:42:31,830
Ananda Sen: Number 32 comes from somewhere.

1044
02:42:31,830 --> 02:42:36,720
Ananda Sen: I already told you how many unique profiles that are.

1045
02:42:36,720 --> 02:43:31,410
Ananda Sen: that's 38 but the degrees of freedom is six less.

1046
02:43:31,410 --> 02:43:39,900
Ananda Sen: You go back to the other slides sealed see this, this is number of profiles.

1047
02:43:39,900 --> 02:43:42,570
Ananda Sen: minus.

1048
02:43:42,570 --> 02:43:45,390
Ananda Sen: Number of.

1049
02:43:45,390 --> 02:43:49,710
Ananda Sen: variables.

1050
02:43:49,710 --> 02:43:53,430
In model.

1051
02:43:53,430 --> 02:43:56,640
Ananda Sen: And how many variables are there.

1052
02:43:56,640 --> 02:44:01,980
Ananda Sen: 12345 and intersect.

1053
02:44:01,980 --> 02:44:13,500
six.

1054
02:44:13,500 --> 02:44:15,420
Ananda Sen: So clear.

1055
02:44:15,420 --> 02:44:22,380
Ananda Sen: So number of profiles is 38, then you subtract number of parameters, you have fit in the model current model.

1056
02:44:22,380 --> 02:44:43,050
Ananda Sen: Which is all those variables plus they are separate you do not see.

1057
02:44:43,050 --> 02:44:44,520
Okay.

1058
02:44:44,520 --> 02:44:58,140
Ananda Sen: So what I did say is that the p value, although I was looking at the p value and say it is no significant lack of it strictly Chi square is not um.

1059
02:44:58,140 --> 02:45:02,010
Ananda Sen: I mean get a gentleman a job surgeon.

1060
02:45:02,010 --> 02:45:06,990
Ananda Sen: Proper logistic print this out, but Chi square is not a valid.

1061
02:45:06,990 --> 02:45:16,530
Ananda Sen: Conclusion here because rate risk is a continuous variable I think that's only continues.

1062
02:45:16,530 --> 02:45:23,940
Ananda Sen: Yes, I think so, but that that is a continuous variable so that's not that's not.

1063
02:45:23,940 --> 02:45:25,650
Ananda Sen: strictly.

1064
02:45:25,650 --> 02:45:34,200
Ananda Sen: gay Square, so what you should be looking at is something like the value over def.

1065
02:45:34,200 --> 02:45:38,460
Ananda Sen: That should be less than one.

1066
02:45:38,460 --> 02:45:56,340
Ananda Sen: And for both of those deviance and Pearson criteria they are less than one that's actually a good sign, and that doesn't have a female associated with it, but the fact that this is less than one.

1067
02:45:56,340 --> 02:46:07,110
Ananda Sen: is a good thing.

1068
02:46:07,110 --> 02:46:13,710
Ananda Sen: Maximum reach scale R squared That is point 1124.

1069
02:46:13,710 --> 02:46:16,230
Ananda Sen: that's a terrible fit.

1070
02:46:16,230 --> 02:46:20,730
Ananda Sen: Only like 11% of the variation explain.

1071
02:46:20,730 --> 02:46:24,690
Ananda Sen: Now, does that really contradict the deviance.

1072
02:46:24,690 --> 02:46:32,700
Ananda Sen: not really the deviant says, this is the best you can do with these variables.

1073
02:46:32,700 --> 02:46:42,120
Ananda Sen: R squared says well these variables and not enough, I mean there are other variables, which you have not measured.

1074
02:46:42,120 --> 02:46:44,940
Ananda Sen: and see the difference.

1075
02:46:44,940 --> 02:46:52,200
Ananda Sen: So R squared means there is a lot of leftover variability in the data which is not measured by these.

1076
02:46:52,200 --> 02:46:59,130
Ananda Sen: deviant says, given that these are the variables I should consider only.

1077
02:46:59,130 --> 02:47:05,580
Ananda Sen: The best you can do is not too far away.

1078
02:47:05,580 --> 02:47:10,230
Theresa Marie Kowalski-Dobson: So for an example like this, where the.

1079
02:47:10,230 --> 02:47:21,720
Theresa Marie Kowalski-Dobson: deviance is high looks good R squared looks like you can add something more which one do you value more in terms of how you lead the model.

1080
02:47:21,720 --> 02:47:27,330
Ananda Sen: Oh, I would go for go for other variables.

1081
02:47:27,330 --> 02:47:35,040
Ananda Sen: which I would then go ahead and an add on and the question is, of course, that.

1082
02:47:35,040 --> 02:47:45,570
Ananda Sen: Where you should stop because you can add on variables and you would increase the R squared a little bit a little bit and by every time.

1083
02:47:45,570 --> 02:47:51,720
Ananda Sen: You are never going to get an R squared adjusted maximum risk alert square of.

1084
02:47:51,720 --> 02:47:54,030
Ananda Sen: 50%.

1085
02:47:54,030 --> 02:47:57,390
Ananda Sen: You probably.

1086
02:47:57,390 --> 02:48:03,630
Ananda Sen: 30% is all you can get given all the variables, you have measured see that's The other thing.

1087
02:48:03,630 --> 02:48:11,700
Ananda Sen: it's also the, the question is also about what you have measured, it may be, there may be a variable which you have not measured.

1088
02:48:11,700 --> 02:48:23,040
Ananda Sen: Which is not collected data, which has been not been collected, you have no way of doing that maybe based on what you have that's the best you can do now.

1089
02:48:23,040 --> 02:48:36,060
Ananda Sen: Can you do have the luxury of going out in the field and measuring other things other biologically plausible things sure if you have the luxury then go ahead and do that, but often.

1090
02:48:36,060 --> 02:48:40,650
Ananda Sen: And most cases, you would you would have.

1091
02:48:40,650 --> 02:48:51,930
Ananda Sen: an experiment or or a trial done or an observational study done where you have certain amount of information, not more than that everything is.

1092
02:48:51,930 --> 02:48:54,360
Ananda Sen: Based on that right.

1093
02:48:54,360 --> 02:49:01,170
Ananda Sen: But to go back to your question about which one should you trust more.

1094
02:49:01,170 --> 02:49:19,590
Ananda Sen: Well, I think I think that's where the practical aspect comes in it's not just the statistical things anymore what R squared is not statistical anyway, and neither is deviance because it's it's just looking at things in the morning, more adequate.

1095
02:49:19,590 --> 02:49:33,510
Ananda Sen: If I have more variables, to consider, I will all this tribals The other thing I, which I have not indicated here is that you're always look at interactions to.

1096
02:49:33,510 --> 02:49:37,650
Ananda Sen: That could increase our Square.

1097
02:49:37,650 --> 02:49:42,240
Ananda Sen: Somewhat moderately in many cases.

1098
02:49:42,240 --> 02:49:53,700
Ananda Sen: deviance probably will not change much but it's the R square in this case at least R squared is the one which you can which you can change.

1099
02:49:53,700 --> 02:49:56,190
Ananda Sen: The bottom line is.

1100
02:49:56,190 --> 02:49:59,190
Ananda Sen: These are different.

1101
02:49:59,190 --> 02:50:03,510
Ananda Sen: These have different meanings and and.

1102
02:50:03,510 --> 02:50:06,090
Ananda Sen: You can.

1103
02:50:06,090 --> 02:50:22,740
Ananda Sen: You should not be looking at just one criteria and you should be looking at many of these things, and ultimately as a practitioner, what do you do you end up with a model and say look, this is the best I can do, given what I have, I have.

1104
02:50:22,740 --> 02:50:39,930
Ananda Sen: Yes, if I have infinite resources and could collect more data I would have collected more data but, given what I have this is the best we can, with the best compromise, we can end with so that that's kind of their take on there.

1105
02:50:39,930 --> 02:50:45,720
Ananda Sen: there's a good question, it is, it is a question which we.

1106
02:50:45,720 --> 02:50:55,380
Ananda Sen: Often, are confronted with in most cases, because all these different criteria might lead lead you to.

1107
02:50:55,380 --> 02:51:03,690
Ananda Sen: I wouldn't call them contradictory conclusion but conclusions, which are.

1108
02:51:03,690 --> 02:51:05,220
Ananda Sen: In.

1109
02:51:05,220 --> 02:51:07,260
Ananda Sen: In appetite and.

1110
02:51:07,260 --> 02:51:28,380
Ananda Sen: discrepancy but they're not discrepant things that doing different things R squared is trying to look for how much of the data variability can I explain using the excess I have the deviance is looking at well, given the access, you have Is this the best to have done or can you do more.

1111
02:51:28,380 --> 02:51:32,100
Ananda Sen: So to a different things, essentially so.

1112
02:51:32,100 --> 02:51:40,230
Ananda Sen: So yeah so I know I think I think you're right, I mean what What do you do and what do you do depends a lot on what is.

1113
02:51:40,230 --> 02:51:47,010
Ananda Sen: feasible nothing.

1114
02:51:47,010 --> 02:51:57,990
Ananda Sen: Great question more questions.

1115
02:51:57,990 --> 02:52:08,580
Ananda Sen: So the other thing which comes out of this is the horseman limit short test, and this is a nice stable, because it does show how.

1116
02:52:08,580 --> 02:52:21,960
Ananda Sen: How this is how this is set up, so in this case that eight groups, I think it is the default or something like that I can't remember now but, but their age groups.

1117
02:52:21,960 --> 02:52:27,690
Ananda Sen: And each group that are a certain number of subjects.

1118
02:52:27,690 --> 02:52:36,390
Ananda Sen: Remember, how are these groups formed these groups were formed by filling a logistic regression, the one you just saw and then.

1119
02:52:36,390 --> 02:52:48,240
Ananda Sen: predicting the probabilities and the smallest progress with you properties is group one, and then the second smallest, then the third smallest all the way up to the highest.

1120
02:52:48,240 --> 02:52:52,680
Ananda Sen: One each of them.

1121
02:52:52,680 --> 02:53:04,650
Ananda Sen: will have some observed and some expected based on fracture equal to one and X equal to zero so So this is the this is that thing we were talking about, so we have.

1122
02:53:04,650 --> 02:53:15,330
Ananda Sen: Eight groups based on the predicted probability within each of those groups, some of them were fracture one, some of them are fracture zero these are observed.

1123
02:53:15,330 --> 02:53:26,040
Ananda Sen: So there are 92 people in the 92 women in the first group 10 of them had fracture 82 of them did not have fracture.

1124
02:53:26,040 --> 02:53:40,620
Ananda Sen: Based on the model, what is the expected number of women to have fracture in that 11.18 what is expected number of women who did not ever would not have been predicted to have fracture at point eight two.

1125
02:53:40,620 --> 02:53:43,980
Ananda Sen: And this goes on for all the age groups.

1126
02:53:43,980 --> 02:53:57,690
Ananda Sen: Okay, you have an observed, you have an expected observed expected now what you do is you create the Pearson Chi square statistic, based on the observed unexpected do this for all 16 of the cells.

1127
02:53:57,690 --> 02:54:00,360
Ananda Sen: And then do a Chi Square.

1128
02:54:00,360 --> 02:54:08,790
Ananda Sen: Chi square is number of groups minus two number of groups is eight minus two is six.

1129
02:54:08,790 --> 02:54:10,740
that's.

1130
02:54:10,740 --> 02:54:12,810
Ananda Sen: again.

1131
02:54:12,810 --> 02:54:17,220
Ananda Sen: Huge P value.

1132
02:54:17,220 --> 02:54:19,020
Ananda Sen: No.

1133
02:54:19,020 --> 02:54:29,250
lack.

1134
02:54:29,250 --> 02:54:33,480
Ananda Sen: So, for lack of.

1135
02:54:33,480 --> 02:54:44,370
Ananda Sen: assessment, you would be looking at you should be looking at large.

1136
02:54:44,370 --> 02:54:48,240
Ananda Sen: Is the goodness of fit criteria and clear.

1137
02:54:48,240 --> 02:54:54,330
yeah.

1138
02:54:54,330 --> 02:55:05,580
Ananda Sen: All right.

1139
02:55:05,580 --> 02:55:09,480
Ananda Sen: let's do some variable selection.

1140
02:55:09,480 --> 02:55:16,950
Ananda Sen: And this time I have made the model, a little bit more involved so.

1141
02:55:16,950 --> 02:55:24,240
Ananda Sen: A very a set of variables, as well as some.

1142
02:55:24,240 --> 02:55:28,980
Ananda Sen: interactions and a a.

1143
02:55:28,980 --> 02:55:40,440
Ananda Sen: True in also the frack score, which is actually derived using age and some other criteria.

1144
02:55:40,440 --> 02:55:44,190
Ananda Sen: And they ran the stepwise selection.

1145
02:55:44,190 --> 02:55:50,850
Ananda Sen: Entry P value is point two and staying P value is point two five.

1146
02:55:50,850 --> 02:55:57,360
Ananda Sen: And when I ran stepwise selection look look at what happened, this is the final model.

1147
02:55:57,360 --> 02:56:05,700
Ananda Sen: frack score rate risk height prior frack mom frack it's interesting that nothing was removed.

1148
02:56:05,700 --> 02:56:15,120
Ananda Sen: It was only entered every time.

1149
02:56:15,120 --> 02:56:20,160
Ananda Sen: it's also quite interesting that the last two.

1150
02:56:20,160 --> 02:56:28,080
Ananda Sen: variables brad frank and mom friends, they turned out to have P value less more than point 05.

1151
02:56:28,080 --> 02:56:32,010
Ananda Sen: But that's because you wanted SL entry.

1152
02:56:32,010 --> 02:56:36,840
Ananda Sen: 2.2 so it's all less than Point two.

1153
02:56:36,840 --> 02:56:47,610
Ananda Sen: Okay.

1154
02:56:47,610 --> 02:57:03,030
Ananda Sen: So when you ran it I think mom frank is only one no mom freakin prayer for the two which came out to be non significant, which is expected.

1155
02:57:03,030 --> 02:57:08,220
Ananda Sen: But notice one thing ages gone.

1156
02:57:08,220 --> 02:57:17,700
Ananda Sen: age is no longer in never in the model and that's partly because I think frack score is there.

1157
02:57:17,700 --> 02:57:28,920
Ananda Sen: Because frack score is involves age in a very specific manner.

1158
02:57:28,920 --> 02:57:37,110
Ananda Sen: So the selected model does not necessarily have all significant terms.

1159
02:57:37,110 --> 02:57:54,450
Ananda Sen: But that brings a question to mind that, should we change the selection strategy and see if anything changing and indifferent happens so let's see support they didn't do stepwise and did his forward selection just for.

1160
02:57:54,450 --> 02:57:59,490
Ananda Sen: What do I get frack score rate risk.

1161
02:57:59,490 --> 02:58:01,200
Ananda Sen: And height.

1162
02:58:01,200 --> 02:58:05,670
Ananda Sen: frack score rate risk and height.

1163
02:58:05,670 --> 02:58:12,480
Ananda Sen: Sure enough, the mom fracking tracks mom track and pride fractured gone.

1164
02:58:12,480 --> 02:58:18,690
Ananda Sen: So those three are the ones which are coming up quite.

1165
02:58:18,690 --> 02:58:22,650
Ananda Sen: Quite clearly.

1166
02:58:22,650 --> 02:58:28,350
Ananda Sen: How about backward.

1167
02:58:28,350 --> 02:58:32,190
Ananda Sen: Oh, but backward retains a lot.

1168
02:58:32,190 --> 02:58:41,010
Ananda Sen: But it removed frack score see it's a very interesting, depending on what procedure you choose you you'd end up in a different type of model.

1169
02:58:41,010 --> 02:58:55,650
Ananda Sen: But that's not the that's not surprising because tracks for involved many of these variables so that's the one which has been removed the other ones stayed.

1170
02:58:55,650 --> 02:59:05,400
Ananda Sen: Including the interactions.

1171
02:59:05,400 --> 02:59:18,480
Ananda Sen: Remember backward means once the variables are dropped, it cannot re enter and forward means once a variable is included, it cannot be dropped.

1172
02:59:18,480 --> 02:59:22,380
Ananda Sen: So i've got three different models.

1173
02:59:22,380 --> 02:59:27,270
Ananda Sen: Using either stepwise or backwards or forwards.

1174
02:59:27,270 --> 02:59:30,870
Ananda Sen: Which is the better model.

1175
02:59:30,870 --> 02:59:44,100
Ananda Sen: The best thing to do is to look at all of these things a you see deviance Pearson postmark lemme show maximum rescaling R Square.

1176
02:59:44,100 --> 02:59:50,730
Ananda Sen: A I see all these different criteria.

1177
02:59:50,730 --> 02:59:53,910
Ananda Sen: So he compares.

1178
02:59:53,910 --> 03:00:02,700
Ananda Sen: The stepwise and backward are similar in fact the backward has a slightly higher, you see, but only marginally higher.

1179
03:00:02,700 --> 03:00:10,200
Ananda Sen: The deviance that about the same Pearson is about the same.

1180
03:00:10,200 --> 03:00:20,190
Ananda Sen: host more lemme Shaw, these are be validated but i'm pretty sure the say can't remember now but.

1181
03:00:20,190 --> 03:00:42,690
Ananda Sen: yeah they are the fourth selection one is is different, but the stepwise in the backward basically the same maximum rescheduled R square backward is also fine and EIC backwards so backward in the model is is actually by far the best model of the three.

1182
03:00:42,690 --> 03:00:50,610
Ananda Sen: that's the model, which does not use the derived score that's the model which uses the actual variables.

1183
03:00:50,610 --> 03:00:57,930
Ananda Sen: it's not a bad model.

1184
03:00:57,930 --> 03:01:36,780
oops sorry.

1185
03:01:36,780 --> 03:02:15,840
Ananda Sen: questions.

1186
03:02:15,840 --> 03:02:29,610
Right.

1187
03:02:29,610 --> 03:02:35,910
Ananda Sen: So, in the context of logistic regression.

1188
03:02:35,910 --> 03:02:43,980
Ananda Sen: and think the better model to use is proc logistic.

1189
03:02:43,980 --> 03:02:51,870
Ananda Sen: When we are dealing with, maybe counter aggression, or something like that it is better to use.

1190
03:02:51,870 --> 03:02:56,640
Ananda Sen: Proper Jenny mode and project might also has.

1191
03:02:56,640 --> 03:03:01,080
Ananda Sen: One additional advantage of.

1192
03:03:01,080 --> 03:03:10,500
Ananda Sen: Being able to use repeated measures data in the context of general linear models that proper logistic is not.

1193
03:03:10,500 --> 03:03:13,800
Ananda Sen: So good in in doing that, so.

1194
03:03:13,800 --> 03:03:20,430
Ananda Sen: So I would stick with Brock logistic for most purposes for logistic regression.

1195
03:03:20,430 --> 03:03:25,680
Ananda Sen: But when i'm dealing with generalized linear model, then I will.

1196
03:03:25,680 --> 03:03:28,290
Ananda Sen: Do it.

1197
03:03:28,290 --> 03:03:44,730
Ananda Sen: And do a Jen Jen Monterey county regression models elder Jen.

1198
03:03:44,730 --> 03:03:46,440
Ananda Sen: Okay.

1199
03:03:46,440 --> 03:03:52,140
Ananda Sen: we're a little bit of time, so let's.

1200
03:03:52,140 --> 03:03:56,370
discuss.

1201
03:03:56,370 --> 03:04:13,380
alright.

1202
03:04:13,380 --> 03:04:24,660
Ananda Sen: So.

1203
03:04:24,660 --> 03:04:29,790
Ananda Sen: Tell me about.

1204
03:04:29,790 --> 03:04:36,480
Ananda Sen: Your take on this so so so one thing I want to point out is that.

1205
03:04:36,480 --> 03:04:42,570
Ananda Sen: Whether it's logistic regression with as linear regression or count regression, which will do next.

1206
03:04:42,570 --> 03:04:44,640
Ananda Sen: tomorrow.

1207
03:04:44,640 --> 03:04:47,880
Ananda Sen: it's.

1208
03:04:47,880 --> 03:04:55,650
Ananda Sen: It uses different procedures uses either project monitor perak logistics from glm proper direction.

1209
03:04:55,650 --> 03:05:02,520
Ananda Sen: But they're essentially generally doing the same type of things.

1210
03:05:02,520 --> 03:05:09,930
Ananda Sen: We did not talk about multipolarity in the context logistic regression, because that is not an issue.

1211
03:05:09,930 --> 03:05:17,010
Ananda Sen: For multipolarity for for logistic regression model is not going to be nish.

1212
03:05:17,010 --> 03:05:26,490
Ananda Sen: i'm typically not because the model is non linear model is nothing if it is Dave the model is native the.

1213
03:05:26,490 --> 03:05:29,580
Ananda Sen: outcome.

1214
03:05:29,580 --> 03:05:32,970
Ananda Sen: To the directly to the.

1215
03:05:32,970 --> 03:05:45,900
Ananda Sen: observed outcome to the to the covariance is linear then magnetic could be a problem.

1216
03:05:45,900 --> 03:05:47,610
alright.

1217
03:05:47,610 --> 03:05:52,020
Ananda Sen: So.

1218
03:05:52,020 --> 03:05:56,700
Ananda Sen: i'll welcome questions at this point.

1219
03:05:56,700 --> 03:06:01,980
Ananda Sen: about any of these things.

1220
03:06:01,980 --> 03:06:08,400
Christie Flanagan: I don't have any right now but i'm sure once I review the homework, I will have plenty.

1221
03:06:08,400 --> 03:06:08,850
Ananda Sen: yeah.

1222
03:06:08,850 --> 03:06:14,190
Ananda Sen: i'm sure.

1223
03:06:14,190 --> 03:06:26,340
Ananda Sen: Since we have a little bit of time, why don't we I was originally planning to do this, why don't we go through the SAS proceeded and just slow on us small.

1224
03:06:26,340 --> 03:06:35,040
Ananda Sen: toy example and see.

1225
03:06:35,040 --> 03:06:37,440
Ananda Sen: If we can.

1226
03:06:37,440 --> 03:06:52,980
Ananda Sen: make some sense out of this and try to interpret some of these so let's see.

1227
03:06:52,980 --> 03:07:04,980
Ananda Sen: Okay.

1228
03:07:04,980 --> 03:07:27,900
yeah.

1229
03:07:27,900 --> 03:07:51,720
Ananda Sen: Alright let's bring in the third day.

1230
03:07:51,720 --> 03:08:36,360
Ananda Sen: Oh baby.

1231
03:08:36,360 --> 03:08:40,950
Ananda Sen: do not have the data oh and.

1232
03:08:40,950 --> 03:08:42,450
Then.

1233
03:08:42,450 --> 03:09:24,990
Ananda Sen: Globe data.

1234
03:09:24,990 --> 03:09:41,310
Ananda Sen: Okay that's better.

1235
03:09:41,310 --> 03:09:44,610
Ananda Sen: So this has got everything.

1236
03:09:44,610 --> 03:09:56,100
Ananda Sen: just want to make sure to them.

1237
03:09:56,100 --> 03:10:02,910
Ananda Sen: Why is it, why am I.

1238
03:10:02,910 --> 03:10:57,960
Ananda Sen: Why don't I do this, why don't we just remove this.

1239
03:10:57,960 --> 03:11:25,980
Ananda Sen: Alright let's.

1240
03:11:25,980 --> 03:11:30,000
Ananda Sen: Think there's something going on i'm not sure what.

1241
03:11:30,000 --> 03:11:39,600
But.

1242
03:11:39,600 --> 03:11:46,560
Christie Flanagan: i'm not sure if this thing's it different or not, but it could it be, because you don't have the slash sign before home because on mine it.

1243
03:11:46,560 --> 03:11:54,780
Ananda Sen: I have that and oh good bit oh you're right that's right everybody, thank you.

1244
03:11:54,780 --> 03:11:57,240
Christie Flanagan: notice there.

1245
03:11:57,240 --> 03:11:59,190
Ananda Sen: Yes.

1246
03:11:59,190 --> 03:12:04,590
Ananda Sen: That is definitely true.

1247
03:12:04,590 --> 03:12:06,000
Ananda Sen: Good catch.

1248
03:12:06,000 --> 03:12:08,820
Very good catch yes.

1249
03:12:08,820 --> 03:12:13,050
Ananda Sen: Alright, so.

1250
03:12:13,050 --> 03:12:15,960
Ananda Sen: So yeah so i'm reading the data now now.

1251
03:12:15,960 --> 03:12:18,840
Ananda Sen: We could.

1252
03:12:18,840 --> 03:12:24,840
Ananda Sen: Try to run something where.

1253
03:12:24,840 --> 03:12:27,840
Ananda Sen: um.

1254
03:12:27,840 --> 03:12:29,250
let's see.

1255
03:12:29,250 --> 03:12:32,580
Ananda Sen: let's try to use.

1256
03:12:32,580 --> 03:12:54,420
Ananda Sen: Some simple model.

1257
03:12:54,420 --> 03:12:59,400
Ananda Sen: I do not necessarily want.

1258
03:12:59,400 --> 03:13:05,790
Ananda Sen: Any of these.

1259
03:13:05,790 --> 03:13:09,630
Ananda Sen: So I want to run.

1260
03:13:09,630 --> 03:13:24,870
Ananda Sen: Brock logistic or with a model of smoke brad frack mom frack rate risk and arm assist so actually Let me take out rate risk also.

1261
03:13:24,870 --> 03:13:29,280
could just this.

1262
03:13:29,280 --> 03:13:37,860
Ananda Sen: And let's see what happens.

1263
03:13:37,860 --> 03:13:42,420
Ananda Sen: Convergence satisfy this is also another thing you have to you have to look.

1264
03:13:42,420 --> 03:13:54,750
Ananda Sen: and see it does give you all these different and different things so so the aisense already printed ignore the sc then negative to log likelihood is printed.

1265
03:13:54,750 --> 03:14:06,480
Ananda Sen: And then there is a test of global null hypothesis, and then the all those estimates now it obviously does walled based on this so.

1266
03:14:06,480 --> 03:14:18,330
Ananda Sen: let's try to understand, well, these are the odds ratios let's let's try to understand these point estimates, so what is what is prior frack.

1267
03:14:18,330 --> 03:14:25,920
Ananda Sen: 2.635 that's a that's a significant odds ratio because it's more than one.

1268
03:14:25,920 --> 03:14:48,390
Ananda Sen: How would you interpret this.

1269
03:14:48,390 --> 03:14:51,930
Ananda Sen: That particular.

1270
03:14:51,930 --> 03:14:55,440
Ananda Sen: odds ratio.

1271
03:14:55,440 --> 03:15:06,120
Theresa Marie Kowalski-Dobson: Would it be that if you've had a prior fracture you're almost three times likely to get another fracture than someone who has not had a prior fracture.

1272
03:15:06,120 --> 03:15:07,800
Ananda Sen: Correct.

1273
03:15:07,800 --> 03:15:13,290
Ananda Sen: yeah up two things I would point out here.

1274
03:15:13,290 --> 03:15:18,060
Ananda Sen: It is very common to call these more likely.

1275
03:15:18,060 --> 03:15:26,070
Ananda Sen: more likely is typically means risk ratio.

1276
03:15:26,070 --> 03:15:29,640
Ananda Sen: So, not so much.

1277
03:15:29,640 --> 03:15:46,230
Ananda Sen: The odds are so I will I will stick to the odds interpretation so so like you said everything you said is right, but almost three times higher odds, is what i'm going to use in this case, the second thing I want to point out is that.

1278
03:15:46,230 --> 03:15:49,500
Ananda Sen: everything you said is right with.

1279
03:15:49,500 --> 03:15:52,950
Ananda Sen: But I would still use the phrase.

1280
03:15:52,950 --> 03:16:01,350
Ananda Sen: Given that the smoking status moms practice data as an armistice are our constant are fixed.

1281
03:16:01,350 --> 03:16:07,350
Ananda Sen: So it's the same interpretation like multiple linear regression where.

1282
03:16:07,350 --> 03:16:15,090
Ananda Sen: You fix all the other variables status, before you talk about the breasts also it will be.

1283
03:16:15,090 --> 03:16:25,890
Ananda Sen: up for a given smoking status mom for X date as an armistice status the prior frank, the ones who had a prior fracture.

1284
03:16:25,890 --> 03:16:35,640
Ananda Sen: At 2.6 times higher than the ones who have who did not have a bad practice to have a huge effect yeah.

1285
03:16:35,640 --> 03:16:42,780
Ananda Sen: they've been clear.

1286
03:16:42,780 --> 03:16:46,920
Ananda Sen: All right.

1287
03:16:46,920 --> 03:16:56,550
Ananda Sen: um the other question I have in this regard is that suppose.

1288
03:16:56,550 --> 03:17:00,330
Ananda Sen: The value of the odds is.

1289
03:17:00,330 --> 03:17:19,650
Ananda Sen: knocking let's let's do something.

1290
03:17:19,650 --> 03:18:12,180
Ananda Sen: Okay let's see how this how this looks in Prague gen one project mode is the other one which we talked about.

1291
03:18:12,180 --> 03:18:22,230
Ananda Sen: So what it does, is it does the same thing, but it doesn't do the odds ratios, which you can ask for, but it, but it automatically does all the.

1292
03:18:22,230 --> 03:18:33,000
Ananda Sen: Other estimates and so on and so forth, it also provides the Ai CBS it also gives you because it's Jen Mon it automatically gives you the demons.

1293
03:18:33,000 --> 03:18:47,820
Ananda Sen: So what is the value over deviance quite quite a quite less than 160 point 1756 even scale events is not that bad pearson's Chi square is also small.

1294
03:18:47,820 --> 03:18:49,890
Ananda Sen: So all of those at their.

1295
03:18:49,890 --> 03:19:07,680
Ananda Sen: AC be icy at both both and algorithms converge so yeah so this, this is very similar except it gives the information slightly differently.

1296
03:19:07,680 --> 03:19:10,140
Ananda Sen: um.

1297
03:19:10,140 --> 03:19:15,900
Ananda Sen: One thing somebody pointed out, I think christy you did.

1298
03:19:15,900 --> 03:19:23,700
Ananda Sen: So there is.

1299
03:19:23,700 --> 03:19:30,270
Ananda Sen: yeah so let's let's do something here, and it is true for.

1300
03:19:30,270 --> 03:19:42,210
Ananda Sen: Both for linear model as well, so if I did project my data glug glug if I did the same thing, but now, with the class variable.

1301
03:19:42,210 --> 03:19:44,520
Ananda Sen: So glass.

1302
03:19:44,520 --> 03:19:48,480
Ananda Sen: smoke.

1303
03:19:48,480 --> 03:20:10,530
Ananda Sen: Gross right.

1304
03:20:10,530 --> 03:20:22,800
Ananda Sen: So now, I see it, does it does make this.

1305
03:20:22,800 --> 03:20:32,490
Ananda Sen: Class variable so here's here's something I want to point out, this is, this is, this is a little strange for.

1306
03:20:32,490 --> 03:20:43,560
Ananda Sen: For for says, but says, does it this way, you see what it made the baseline or reference it always makes the one that reference.

1307
03:20:43,560 --> 03:20:45,690
Ananda Sen: So.

1308
03:20:45,690 --> 03:20:55,110
Ananda Sen: In the case of smoke it's one is that reference, so what changes so smoke is point 08 to nine.

1309
03:20:55,110 --> 03:21:00,390
Ananda Sen: So let's oh.

1310
03:21:00,390 --> 03:21:13,320
Ananda Sen: I have to do both of the citizens, the annoying part I should be able to do both at the same time let's do this same time.

1311
03:21:13,320 --> 03:21:18,480
Okay.

1312
03:21:18,480 --> 03:21:24,180
Ananda Sen: So this is when I don't use the class variable negative.

1313
03:21:24,180 --> 03:21:34,470
Ananda Sen: Positive positive positive 0.989680 and so on and so forth, when I when I do this.

1314
03:21:34,470 --> 03:21:37,770
Ananda Sen: With this everything changes.

1315
03:21:37,770 --> 03:21:42,030
Ananda Sen: And it looks very strange.

1316
03:21:42,030 --> 03:21:47,520
Ananda Sen: Smoke point 0829.

1317
03:21:47,520 --> 03:21:57,570
Ananda Sen: Smoke minus 0.457 almost looks like a different model.

1318
03:21:57,570 --> 03:22:10,590
Ananda Sen: Do you see what i'm saying they are running the same model, but because they reference categories that changed, you should they are they're doing it different than you it looks like it's different.

1319
03:22:10,590 --> 03:22:12,600
Ananda Sen: So.

1320
03:22:12,600 --> 03:22:15,090
Ananda Sen: One way.

1321
03:22:15,090 --> 03:22:43,950
Ananda Sen: To get rid of that is the last thing i'm going to mention oh sorry Where do they go.

1322
03:22:43,950 --> 03:23:10,050
yeah.

1323
03:23:10,050 --> 03:23:12,900
Ananda Sen: Reference equal to first.

1324
03:23:12,900 --> 03:23:20,580
Ananda Sen: How does it change things let's run all of them and.

1325
03:23:20,580 --> 03:23:37,470
Ananda Sen: So the first one is without specifying a class just keep an eye on these things minus 0.45 0.96 0.6 seconds on the second one is regular everything changed.

1326
03:23:37,470 --> 03:23:41,280
Ananda Sen: Third, one.

1327
03:23:41,280 --> 03:23:45,780
Ananda Sen: is changing it to zero back again.

1328
03:23:45,780 --> 03:23:59,790
Ananda Sen: And now I have got all those smoking variables and the pride frag variables and the mom track the are Mrs they're all all different, they are.

1329
03:23:59,790 --> 03:24:08,340
Ananda Sen: What 0.10 0.1296 actually hang on one SEC.

1330
03:24:08,340 --> 03:24:13,320
Ananda Sen: So this is matching up to that.

1331
03:24:13,320 --> 03:24:21,810
Ananda Sen: it's not matching up with it.

1332
03:24:21,810 --> 03:24:23,640
Oh.

1333
03:24:23,640 --> 03:24:36,900
Ananda Sen: I should not have happened anyway, I will I will explain this to more I think there is one thing I want to point out is that if you run it using a class statement versus not it does change things so.

1334
03:24:36,900 --> 03:24:40,920
Ananda Sen: We have to, we have to be careful about that.

1335
03:24:40,920 --> 03:24:44,880
Ananda Sen: All right, I want to stop here.

1336
03:24:44,880 --> 03:25:02,580
Ananda Sen: But if somebody has any question wants to hang back, I am more than happy to stay back, and I will I will finish my thought here tomorrow I, there is something I wanted to point out, which can be confusing, especially if you're doing it for the first time.

1337
03:25:02,580 --> 03:25:10,560
Yoonhee Ryder: So basically we should only use class if there's more than two.

1338
03:25:10,560 --> 03:25:13,980
Ananda Sen: options for the various.

1339
03:25:13,980 --> 03:25:19,260
Christie Flanagan: doctor said, I think I just shared in the chat how you had me write it when we spoke.

1340
03:25:19,260 --> 03:25:21,390
Christie Flanagan: And I don't really that will be helpful.

1341
03:25:21,390 --> 03:25:26,880
Ananda Sen: yeah I don't know if.

1342
03:25:26,880 --> 03:25:36,690
Ananda Sen: I will check it I don't want to keep everybody waiting today i'll check it and make sure that it's right and then i'll start off with it tomorrow.

1343
03:25:36,690 --> 03:25:44,190
Ananda Sen: yeah but I, but I, but it is confusing without a class statement with the class statement it's still running the same model.

1344
03:25:44,190 --> 03:25:58,650
Ananda Sen: Especially if you have if they have the same number of so dichotomous categories, but it's about it, the coefficients come out different and they come out different because they're filling in different ways, so that's something to.

1345
03:25:58,650 --> 03:26:00,870
Ananda Sen: Be careful.

1346
03:26:00,870 --> 03:26:14,520
Ananda Sen: Okay wonderful, so I will see you tomorrow and but, again, if anybody has any question want to hang hang back i'll be happy to answer.

1347
03:26:14,520 --> 03:26:16,530
Ananda Sen: But otherwise.

1348
03:26:16,530 --> 03:26:25,680
Christie Flanagan: You Thank you tonight, yes.

1349
03:26:25,680 --> 03:26:30,660
Ananda Sen: Thank you.

1350
03:26:30,660 --> 03:26:32,880
Ananda Sen: So josh.

1351
03:26:32,880 --> 03:26:36,330
Ananda Sen: looks like we have.

1352
03:26:36,330 --> 03:26:44,190
Ananda Sen: More or less solve the problem I figured out what was happening, it was.

1353
03:26:44,190 --> 03:26:47,070
Ananda Sen: So I think when they were.

1354
03:26:47,070 --> 03:26:54,420
Ananda Sen: Trying to set up the course I don't know if it's one I did there or somebody else did that, but she they had.

1355
03:26:54,420 --> 03:26:58,530
Ananda Sen: copied everything from last year and put it on the home page.

1356
03:26:58,530 --> 03:27:00,600
Ananda Sen: And the links.

1357
03:27:00,600 --> 03:27:06,690
Ananda Sen: were broken, because I was actually I deleted everything from last year.

1358
03:27:06,690 --> 03:27:16,470
Ananda Sen: And then, and then put things back again new things, but those were the those links were not automatically generated reconnecting so.

1359
03:27:16,470 --> 03:27:32,130
Ananda Sen: gotcha so that was that was the reason I actually did not realize that from the homepage it's it's a nice feature, but the problem is that you have to you have to remember, every time if you if you change that original file then also.

1360
03:27:32,130 --> 03:27:38,460
Joshua Bosman: Okay gotcha yeah I think the explanation you gave at the beginning was was good, I think people are on the same page.

1361
03:27:38,460 --> 03:27:46,110
Ananda Sen: yeah yeah and I think I think I think now should they should not be any problem everybody should be able to upload things away so.

1362
03:27:46,110 --> 03:27:49,770
Ananda Sen: awesome okay fantastic Thank you again and.

1363
03:27:49,770 --> 03:27:56,280
Ananda Sen: How are you are keeping track of the person who was doing it for see me right, I think I think it was be on it.

1364
03:27:56,280 --> 03:27:58,800
Joshua Bosman: yep she's three for three oh good yeah.

1365
03:27:58,800 --> 03:28:00,480
Ananda Sen: All right, very good Thank you.

1366
03:28:00,480 --> 03:28:01,230
Joshua Bosman: No problem.

1367
03:28:01,230 --> 03:28:02,520
Ananda Sen: good night.

1368
03:28:02,520 --> 03:28:05,250
Joshua Bosman: You two should I hit stop recording or should you.

1369
03:28:05,250 --> 03:28:06,840
Ananda Sen: yeah yeah yeah oh.

1370
03:28:06,840 --> 03:28:09,030
Ananda Sen: yeah sure I can.

1371
03:28:09,030 --> 03:28:11,840
Joshua Bosman: still want to mess anything up, no, no, no.

