1
00:00:01,950 --> 00:00:06,290
That's what you don't know that.

2
00:00:06,510 --> 00:00:09,670
I don't need to use anything. I don't mind.

3
00:00:09,900 --> 00:00:18,150
I needed the homework while the first homework. I'm not sure if you guys saw that it's on her files.

4
00:00:18,180 --> 00:00:33,989
I agreed to this assignment folder, and here we have weekly homework, and so that must be posted every Thursday.

5
00:00:33,990 --> 00:00:46,799
And then do the math the following Thursday. The first one I supposed to do today and was due next Thursday by 9 p.m. the later this hour is as eyes.

6
00:00:46,800 --> 00:00:52,980
They are great. We have this assignment. Oh, okay, great.

7
00:00:55,200 --> 00:00:59,190
So this is where you submit your homework assignment?

8
00:01:01,110 --> 00:01:08,129
Yes, I can. Grade operations. Okay, so that's the homework.

9
00:01:08,130 --> 00:01:11,940
And then let's get back to our course materials.

10
00:01:11,940 --> 00:01:15,690
So we have been talking about. So these are the statistics, right?

11
00:01:16,110 --> 00:01:23,340
And let's just get a very quick refresher memory about a three, four, three, three.

12
00:01:23,370 --> 00:01:26,879
What kind of three things about? So here are the statistics.

13
00:01:26,880 --> 00:01:31,020
So first of all, we define what sufficient statistics are.

14
00:01:31,080 --> 00:01:36,210
And so what you have the definition and then we talk about that.

15
00:01:36,220 --> 00:01:39,960
Then there was 106.2.2.

16
00:01:41,100 --> 00:01:44,280
That is essentially the definition. Still,

17
00:01:44,280 --> 00:01:53,910
the definition just tells us how to check is a statistic equal give a statistic

18
00:01:54,150 --> 00:01:59,520
is sufficient result to check if a gives disease treatment and you just

19
00:01:59,520 --> 00:02:09,420
mutual funds are drawn because you have the drawn distinction and then have the description or the idea of the statistic of the previous statistic,

20
00:02:09,750 --> 00:02:13,710
and then you can cover the mission and the result is positive.

21
00:02:14,780 --> 00:02:21,540
Go further then that is the so that is the key is a subjective statistic.

22
00:02:22,650 --> 00:02:28,020
Okay. So then we looked at a sort of the major result.

23
00:02:42,020 --> 00:02:47,959
Cross our the fact that this is very. So further,

24
00:02:47,960 --> 00:02:57,740
I think that there is the one that is probably the most the easiest to use and also be the most widely used result when it comes

25
00:02:57,740 --> 00:03:08,600
to either to tell whether given that is a sufficient or low to to identify to find it more than 5% loss of vision statistics.

26
00:03:09,440 --> 00:03:21,020
So this is the erm says that well if you are able to fact rise the joint pdf or enough in this particular way so that you know the first part,

27
00:03:21,440 --> 00:03:27,800
it is a function of the statistic that you are looking at and then this may depend on the data.

28
00:03:28,310 --> 00:03:33,800
And then the second part of the piece has nothing to do with theta, so it doesn't depend on theta at all.

29
00:03:34,010 --> 00:03:38,000
But you are able to characterize the PDF in this particular way.

30
00:03:38,360 --> 00:03:42,170
Then this T here is a visual, this is an opening.

31
00:03:42,890 --> 00:03:46,920
So then this theorem can be used in two ways.

32
00:03:47,120 --> 00:03:54,980
One way is, well, if we are given a statistic and then you are asked, well, whether this gives the disease so vision or not,

33
00:03:55,370 --> 00:03:59,599
then you can try to write to the draw the PDF in this particular way and to see

34
00:03:59,600 --> 00:04:04,190
whether you know you can write it as a as a function of this given statistic.

35
00:04:05,600 --> 00:04:10,630
Another way of applying this is that, well, give a pdf.

36
00:04:10,640 --> 00:04:16,400
I mean, we don't know what a state is, what a surveillance state might be, and we are not given any potential candidates.

37
00:04:16,470 --> 00:04:19,130
Right. But it doesn't matter too much.

38
00:04:19,460 --> 00:04:29,080
Then you can still fact rise the drawing pdf and as long as you can fact rise in this way, then this t here appears here.

39
00:04:29,090 --> 00:04:37,280
That's that's one sufficient statistic. And so in this sense that this theory is quite general.

40
00:04:37,280 --> 00:04:45,310
It can be used both to attack a given statistic, sufficient or not, or to construct or to identify instances.

41
00:04:46,460 --> 00:04:52,370
And that we looked at a few we look at truly and also we look at examples.

42
00:04:55,250 --> 00:05:00,380
And today we are going to look at more examples before we move on to a new concept.

43
00:05:03,890 --> 00:05:19,640
So all the previous examples, I think if we look at, for example, let's say let's say this one I saw this example here,

44
00:05:19,940 --> 00:05:26,600
I'd sample uniform from uniform distribution uniform of what is supported 1%.

45
00:05:27,380 --> 00:05:32,700
In this case, the civilian is the receipt. And we found that this again,

46
00:05:32,750 --> 00:05:41,180
the maximum sorry that the largest understood is this is a some difference that is so we have theta is one measure the

47
00:05:41,200 --> 00:05:50,570
scale of civilian statistics also of scale one dimension of significance that is kind of there for normal distribution.

48
00:05:50,780 --> 00:05:56,080
Now with both, median estimates were unknown. So theta has two components.

49
00:05:56,090 --> 00:05:59,420
So we have to add all boundaries in this case.

50
00:06:00,230 --> 00:06:05,180
Well, we see that go through some calculation.

51
00:06:05,750 --> 00:06:09,930
We see that. The conditions,

52
00:06:10,040 --> 00:06:16,519
the density also has to be measured as one is the sort of what when you sample average the other

53
00:06:16,520 --> 00:06:21,530
one sort of better the spread of the data that's related to that to the variance of the data.

54
00:06:23,030 --> 00:06:32,680
Now let's take a look at the example where we only have one parameter, but the vision statistics has two dimensional.

55
00:06:34,070 --> 00:06:42,680
So this is, again, a uniform distribution, assuming that we have an I.D. data from uniform.

56
00:06:43,220 --> 00:06:47,150
But now the uniform is on the inner vote, theta, theta plus one.

57
00:06:48,110 --> 00:06:55,459
So both ends depends on theta at both ends of the theta and not all defined appearance.

58
00:06:55,460 --> 00:07:02,150
That is true for theta. So we are not a given any potential candidate to consider.

59
00:07:02,270 --> 00:07:07,970
We're just we're just asked to find a serial statistic, but it doesn't matter much.

60
00:07:08,210 --> 00:07:13,790
So we, we always would always start with drawing a PDF.

61
00:07:15,350 --> 00:07:25,760
So in this case, for a single x, if x followed this uniform, this version, then the pdf file because it's universal.

62
00:07:26,090 --> 00:07:33,170
So it's this one over theta plus one minus theta.

63
00:07:34,100 --> 00:07:42,830
If X is between theta instead of plus one, so which is equal to one, and otherwise this is equal to zero.

64
00:07:43,040 --> 00:07:51,490
If you have is just equal to zero. And it's a lot easier if we write this as a as any gender object.

65
00:07:53,640 --> 00:07:57,379
The last, last lecture, we actually used a white example.

66
00:07:57,380 --> 00:08:05,260
We introduced the negative function. So this pdf is equal to one, equal to one if x falls between the two values.

67
00:08:05,270 --> 00:08:14,090
Otherwise that pdf is equal to zero. So this is not a single a single PDF for a single x.

68
00:08:21,300 --> 00:08:28,790
You not to fund the surveillance that is based on fact isn't zero.

69
00:08:28,810 --> 00:08:32,650
We have to look out of a drawer on a PDF based on this random sample.

70
00:08:33,370 --> 00:08:38,050
So now let's try to work on the strategy here.

71
00:08:40,660 --> 00:08:48,459
That's simply the paradox of all the PDFs, right?

72
00:08:48,460 --> 00:08:56,560
So that's it. You get over X, I mean, in theater and film plus one.

73
00:09:05,820 --> 00:09:12,469
So it's equal to. Well. X1 between theater and film.

74
00:09:12,470 --> 00:09:19,190
Plus, one man playing up to that skin is between theater and theater.

75
00:09:28,300 --> 00:09:39,400
And from the property of the inaugural function, we know that this is assumed as the individual theater is while X1 is between theater and at one.

76
00:09:41,270 --> 00:10:00,330
The. It's indisputable theater. And so the drawing, the PDF is equal to one only when all these axes fall between say.

77
00:10:01,190 --> 00:10:09,880
Otherwise, it's a zero. It takes zero. Now, all these exercises, they fall between theater and theater one.

78
00:10:10,210 --> 00:10:15,670
This is the same as saying that the smallest needs to be larger than theta.

79
00:10:16,060 --> 00:10:18,850
The largest needs to be smaller than theta plus one.

80
00:10:19,450 --> 00:10:32,440
So this is then the immediate theta is less than the smallest of these values, and the largest is less than theta plus one.

81
00:10:44,870 --> 00:10:47,900
And then this one, we can write it in a.

82
00:10:50,230 --> 00:10:55,480
Well, we can stop here or we can begin right writing further as the product of two indicators.

83
00:10:55,690 --> 00:11:07,980
One is that these are less than one times the indicator in less than one.

84
00:11:10,390 --> 00:11:15,190
But so far, I mean we just apply the property of indicators we are dealing with.

85
00:11:15,190 --> 00:11:20,740
The indicators indicator of actions of all these calculations here is based on the property you get.

86
00:11:21,370 --> 00:11:28,839
So after we got here, now if we stop here for a minute and if I asked you guys now,

87
00:11:28,840 --> 00:11:36,430
are you guys able to tell what might be a sufficient statistic in this case after we got here?

88
00:11:39,070 --> 00:11:42,190
I see some of you nodding so good. Some of you count.

89
00:11:44,080 --> 00:11:48,130
Yes, the minimum and the maximum. The minimum and maximum.

90
00:11:48,580 --> 00:11:51,850
So together, this, this. Yeah. Okay.

91
00:11:52,810 --> 00:12:02,050
So after getting here now if we compare this experience, so now we have reached fast the drawing a PDF in this particular way.

92
00:12:02,460 --> 00:12:10,120
Now if you recall the expression, well what are the standards and theorems test.

93
00:12:15,810 --> 00:12:21,600
So as long as we are able to in fact rise to the drawing board in this particular way, then here is for surveillance.

94
00:12:22,900 --> 00:12:33,030
Okay. So now if we compare this expression tool to this guy over here, now if we just define the T of X to be X,

95
00:12:33,030 --> 00:12:43,470
one and X in the smaller and larger order statistics, then this whole thing here, this product of this toy negate or I can call the g function.

96
00:12:45,090 --> 00:12:50,610
And this age, while I simply define age to be one in HD could be one.

97
00:12:51,540 --> 00:13:07,590
So in this case, then, if I take h x to be 1 to 1, x one and t2x1 to be x1 and x if.

98
00:13:18,560 --> 00:13:37,090
And then see function. To be the product of this training, you can say to less than two one and the indicator key to this lesson, say, plus one.

99
00:13:43,360 --> 00:13:59,680
So if I take H and GMT to be these values, then you see that the this guy is precisely the g of t, which also depends on theta then times h.

100
00:14:00,280 --> 00:14:07,419
Now that stuff. Now, if we apply this fact right there, then clearly this is a sufficient statistic.

101
00:14:07,420 --> 00:14:14,890
So. So in this this implies that if I consider t not she has two dimension t is also a vector.

102
00:14:34,830 --> 00:14:42,580
This is a of. Okay.

103
00:14:44,390 --> 00:14:52,020
You see? So this is no big deal.

104
00:14:56,260 --> 00:15:04,810
She Oh, you mean this doesn't have to be an estimate because it's the estimate of Stacy's life.

105
00:15:05,330 --> 00:15:08,430
Is that true? Yes.

106
00:15:09,420 --> 00:15:20,490
Oh, okay. Nice. Your question. Now, the question is whether subregions the does seems to be an estimate for theta.

107
00:15:20,850 --> 00:15:29,940
So no doubt as to no. So here, if you look at what we have been doing, we haven't even thought about it as measure of theta.

108
00:15:30,900 --> 00:15:35,520
So we never we haven't of course, later on, how about estimation of theta?

109
00:15:35,520 --> 00:15:39,540
But so far we haven't started talking about an aspirational theta.

110
00:15:40,620 --> 00:15:48,450
So what we are doing here is just that. Now, of course, your goal, your final goal is well, part of your goal is to estimate theta.

111
00:15:49,350 --> 00:15:55,950
Now you don't know what theta is, you can't data you're trying to use your data to ask me to theta that that's one of the ultimate

112
00:15:55,950 --> 00:16:04,919
goals for what we are doing now is kind of we haven't started constraint as matter for theta yet,

113
00:16:04,920 --> 00:16:07,980
but we're just trying to summarize the information in the data.

114
00:16:08,610 --> 00:16:17,010
We're just trying to see using what we're using, what statistics are we able to summarize all the information available in theta?

115
00:16:17,850 --> 00:16:21,270
The information about theta? Okay.

116
00:16:22,960 --> 00:16:34,480
Just that's like if to say I would to to take like the max of X order, the minimum or the maximum minus one.

117
00:16:36,510 --> 00:16:39,940
Uh, yes, you can construct even an estimate for theta later.

118
00:16:39,940 --> 00:16:44,730
We're gonna talk. Well, maybe this. You mentioned this. Maybe it's worthwhile to to to talk.

119
00:16:45,580 --> 00:16:54,100
So to estimate a theta, you can well, you can construct numerous estimate or so estimate the theta.

120
00:16:54,100 --> 00:17:01,210
So in this case, let's say now you're here, the theta is unknown.

121
00:17:02,020 --> 00:17:09,280
Let's say out of the goal is to claim the data, the kind of sound data and to estimate the based on this data you collect,

122
00:17:09,280 --> 00:17:13,569
you can construct whatever, estimate who like you can cause.

123
00:17:13,570 --> 00:17:19,680
Well, you can even for example, you can even use the sample average to estimate, you can call it A s meter, right?

124
00:17:19,750 --> 00:17:22,479
So he's probably a really bad as major,

125
00:17:22,480 --> 00:17:30,160
but there is nothing preventing you from defining sample average as an estimate of theta and you could even just use

126
00:17:30,160 --> 00:17:38,920
the first value to estimate the theta estimate of there were you could use x two minus x one to estimate a theta.

127
00:17:39,190 --> 00:17:48,280
So you can define your own ways of estimating theta is just whether that asymmetry is a good estimate or that has meter, but we are not there yet.

128
00:17:48,340 --> 00:17:58,450
So currently our the current task is not to estimate the Zeta but a true now because there there is information in the data,

129
00:17:59,110 --> 00:18:00,940
there is information about the theta.

130
00:18:01,630 --> 00:18:09,640
And currently what we are trying to do is just to try to summarize the information from the data, the information about about it.

131
00:18:11,110 --> 00:18:21,969
You know, we're trying to see using what ways are we able to try to use the lowest the division statistics to summarize

132
00:18:21,970 --> 00:18:29,460
all the information about well what is the what is a sufficient way to summarize all the information about the.

133
00:18:31,970 --> 00:18:42,860
So if you look at this example, what this example says here is that now you gave me this data, which has been a point.

134
00:18:42,860 --> 00:18:49,550
You know what I'm saying is that while you actually don't have to give me this and then two points,

135
00:18:50,180 --> 00:18:58,280
if you just provide me the smallest down to the last, that of that content, all the information about data in the original data.

136
00:19:00,850 --> 00:19:03,909
And but how to construct an estimate or.

137
00:19:03,910 --> 00:19:08,890
We haven't talked about it yet. We haven't we haven't started that yet.

138
00:19:09,460 --> 00:19:15,190
And so far, though, we're just looking at how to summarize the information in a effective way.

139
00:19:19,080 --> 00:19:22,120
Does that answer your question? Okay.

140
00:19:22,500 --> 00:19:32,690
So I have a follow up question. And so we will use this official statistic to will use this sufficient statistic to estimate a data like this.

141
00:19:33,090 --> 00:19:36,860
The sufficient data will work as they like to use to.

142
00:19:38,370 --> 00:19:45,720
Okay. So here I want to sever this to saying this two things that are not necessarily connected to sort

143
00:19:47,700 --> 00:19:53,980
of consortium or funding or identifying something in a statistic and an estimation of data.

144
00:19:54,000 --> 00:20:02,880
These are not necessarily related. So as maintenance data, you can construct different studies do to ask me if they're different as a meters.

145
00:20:03,060 --> 00:20:06,450
As we measure this in order to estimate theta, you could have construct,

146
00:20:06,450 --> 00:20:09,900
you could just take a single value, even take a single value out of your data.

147
00:20:09,960 --> 00:20:12,020
Point to ask me to say this.

148
00:20:12,120 --> 00:20:19,620
For example, let's say you try to estimate the population age for the average age and as the average age of the population,

149
00:20:20,670 --> 00:20:31,320
you collect age from 100 individuals. Let's say, you know, age like a 31, you know, a 45 and a 51, for example, and B, values.

150
00:20:32,100 --> 00:20:40,260
And of course, you could have a very natural as major would be take the average sample, average estimate of theta and the population average age.

151
00:20:40,560 --> 00:20:42,090
However, you could just simply say,

152
00:20:42,300 --> 00:20:48,240
I'm going to use the very first value I see as the estimate of the population average and the first value I see is 31.

153
00:20:48,240 --> 00:20:53,520
So I'm going to use 31 to estimate of the population average age of causes of bad as major.

154
00:20:53,760 --> 00:21:01,760
Right. But I mean, mathematically, there is nothing preventing you from using this defining this first value estimate of average.

155
00:21:02,490 --> 00:21:09,460
So that's estimation. And later, we're going to talk about how to construct a good estimate for theta one here.

156
00:21:09,480 --> 00:21:14,340
Currently, we are focusing on the problem of you have data, you have 100 data points.

157
00:21:15,090 --> 00:21:23,850
Now, what is a way to summarize the information in this 100 data points, the information about of the population average age.

158
00:21:25,410 --> 00:21:33,820
So of course, then when you try to summarize information well whenever ways I simply call the central average and then as a summary, well,

159
00:21:33,930 --> 00:21:39,930
that should summarize all the information about age and we see that indeed that's the case for normal those children,

160
00:21:39,930 --> 00:21:42,980
because the sample average is a sufficient statistical analysis.

161
00:21:44,480 --> 00:21:48,210
But I mean, there are other ways of summarizing the information as well.

162
00:21:48,990 --> 00:21:55,890
So so these are two different things like constructing a vision, etc., and consulting as leaders.

163
00:21:56,490 --> 00:22:02,340
So you can therefore use a variance statistic to, as you can call it, an estimate or to estimate theta.

164
00:22:03,330 --> 00:22:11,489
But it may not be a very good ask major. So so yeah, that's right.

165
00:22:11,490 --> 00:22:18,360
So, so again, I want to emphasize these are two different things we have in the reach the the the task

166
00:22:18,370 --> 00:22:23,100
of assumption yet card is that we are just trying to summarize the information in a.

167
00:22:26,320 --> 00:22:32,080
So does that actually have to clarify what we are doing now?

168
00:22:34,150 --> 00:22:35,310
And so that excellent,

169
00:22:35,590 --> 00:22:48,729
excellent idea both jointly is sufficient for our economic security because they value the X one and they went to get out of that because like I said,

170
00:22:48,730 --> 00:22:56,980
we wouldn't be sufficient with either o to be surveyed is the best example here because we have shown this is a severe statistic.

171
00:22:57,460 --> 00:23:07,150
So that means, well, if you provided these two values, then these two values content information about theta and the original data together.

172
00:23:07,360 --> 00:23:14,680
Yeah, they were. Yeah. If you only give me this value, it doesn't contain all that information about theta like a you know,

173
00:23:15,310 --> 00:23:19,280
the same for this one if you're only viewing the largest order. So it doesn't contain all the information.

174
00:23:27,880 --> 00:23:37,670
Okay. That's. This example, in this example shows that here we only have one theta.

175
00:23:37,940 --> 00:23:43,930
It's a skill of the civilians that is the as well as two dimensional simulations.

176
00:23:46,750 --> 00:23:58,930
So if you want to summarize all the information in the data about theta, then you'll need to provide these two values rather than a single map.

177
00:24:00,080 --> 00:24:05,560
Or are we going to see an example where there's more than one possible sufficient statistic?

178
00:24:06,280 --> 00:24:11,170
Example of what? Where there's more than one possible sufficient disorder?

179
00:24:11,920 --> 00:24:20,890
Oh yeah. Later we're going to see examples. And it turns out that if you have a severe stress, then any extra function is going to be a statistic.

180
00:24:24,070 --> 00:24:36,930
Yeah. Okay. So just like that for given any random sample, like the whole stuff that the original simple attention is sufficient.

181
00:24:37,480 --> 00:24:40,180
Oh yeah, absolutely. So we are, we're gonna talk about.

182
00:24:45,130 --> 00:24:53,860
Okay, now let's take a look at another example, this example here we suppose we have Ida a sample, right?

183
00:24:54,310 --> 00:25:03,639
And here we want to argue that we want to show that of the order statistics of our answer they are superior statistics.

184
00:25:03,640 --> 00:25:11,920
So. So suppose we have this idea sample and then this is the draw and the pdf and then other

185
00:25:12,010 --> 00:25:20,040
that the you guys have seen other statistics of interest on the order of these values.

186
00:25:20,050 --> 00:25:24,130
Then the corresponding the corresponding run, the variable we call order statistics.

187
00:25:25,090 --> 00:25:29,790
And then the question is, is older statistics sufficient to perfect?

188
00:25:31,420 --> 00:25:36,670
And the answer is yes. So this is quite intuitive.

189
00:25:36,760 --> 00:25:43,270
Well, even before we look actually in the form of a this this is quite intuitive because the

190
00:25:43,270 --> 00:25:49,000
original data of course contain all the information about saying right in the data.

191
00:25:49,900 --> 00:25:57,070
Now if you define understood as you are just reordering all the data points, you're not losing any information, right?

192
00:25:57,070 --> 00:26:01,600
So you still keep all the data points, but you are just ordering them from the smallest or largest.

193
00:26:01,960 --> 00:26:05,620
Now, of course, intuitively so you can tell the information about theta.

194
00:26:05,830 --> 00:26:19,540
So you should be I submitted a statistic. This is very easy tool to see because it's just a rewriting of what, Judith?

195
00:26:42,630 --> 00:27:56,240
I just decided this would make things easier. This is a very straightforward application of the Factorization Theorem.

196
00:27:56,600 --> 00:28:06,169
So because the draw in a pdf, you can simply writing about the paradox of all the pdf and x.

197
00:28:06,170 --> 00:28:12,260
Y values, of course, is the same as the PDF evaluated at just all these other statistics.

198
00:28:12,680 --> 00:28:16,690
So it's the same function. Let's look then. Then, based on fact resident theorem.

199
00:28:17,060 --> 00:28:20,750
Then the other statistic is a sufficient statistic.

200
00:28:23,680 --> 00:28:35,740
Question. Yeah. So I'm getting this, but I've got sort of a sufficient statistic with dimension reduction, I guess.

201
00:28:37,210 --> 00:28:41,620
Is that accomplished here by clicking the product? Yes.

202
00:28:42,110 --> 00:28:48,970
Oh, so. Okay. Here, I think.

203
00:28:50,170 --> 00:28:55,780
Well, maybe. Maybe it's better if I explain this way. So sufficiently.

204
00:28:55,780 --> 00:29:02,050
This statistic is introduced for the sake of reducing the data.

205
00:29:02,680 --> 00:29:11,590
There is a reduction. However, it doesn't mean that without data reduction can help you find sufficient statistics.

206
00:29:11,800 --> 00:29:13,360
I mean the original data.

207
00:29:13,600 --> 00:29:22,740
Well, if you look at the original data itself, it is it should be a civilian statistic because it does convey all the information about the right.

208
00:29:23,170 --> 00:29:32,469
So but it has no data reduction. So the idea of surveillance, that is, of course, we want to have an idea of surveillance.

209
00:29:32,470 --> 00:29:35,650
That is, we will have to have some dimension of a dual impact.

210
00:29:36,310 --> 00:29:41,650
But it does mean that, you know, every surveillance analyst has to use the data.

211
00:29:42,580 --> 00:29:44,050
Okay. I have another question.

212
00:29:44,230 --> 00:29:51,550
Is there a way to tell how many statistics you need in your sufficient system for it to be sufficient, if that makes sense?

213
00:29:52,090 --> 00:29:59,410
Like in our last, last example, we had the men and trees that because there's two unknown parameters.

214
00:29:59,920 --> 00:30:06,970
You know what? I don't think there is a general like a general way of telling match.

215
00:30:07,630 --> 00:30:15,910
So which would be result. For example, I found I didn't zero just identify the corresponding machines.

216
00:30:17,200 --> 00:30:23,769
Generally speaking, it's really hard to tell because there is no leader.

217
00:30:23,770 --> 00:30:29,260
One will see one example. Maybe after seeing that things become a lot, a lot clearer, more clear.

218
00:30:31,150 --> 00:30:34,510
I'll try to carve out an example to go faster to on that example.

219
00:30:39,680 --> 00:30:49,970
Okay. So this is the the example that shows that in order statistics are absolute sufficient and which is quite intuitive.

220
00:30:50,240 --> 00:30:56,680
So again because the other to keep all the information originally.

221
00:30:59,120 --> 00:31:03,350
Okay, so that's the end of this slide. So visual statistics.

222
00:31:04,190 --> 00:31:11,749
Now let's look at a minimal sufficient statistic and lots of your many of the

223
00:31:11,750 --> 00:31:20,870
questions that you guys asked are actually like must have had of our lack or not.

224
00:31:21,080 --> 00:31:25,880
So when we were talking about a surveillance statistics and so some of the questions you asked,

225
00:31:26,460 --> 00:31:30,590
they were related to the so-called minimum minimum sufficient statistics.

226
00:31:30,590 --> 00:31:33,980
So let's try to start about the next slide.

227
00:31:34,280 --> 00:31:38,600
Hopefully, it will answer give a better answer to some of the questions.

228
00:31:55,810 --> 00:32:09,690
Much. Okay.

229
00:32:09,700 --> 00:32:16,090
So the idea behind a minimum of industries is also quite, quite simple.

230
00:32:17,080 --> 00:32:22,510
Well, first of all, now sufficient statistics. We are not a unique republic.

231
00:32:22,630 --> 00:32:27,910
By now, we should have. Maybe you guys have already realized this.

232
00:32:28,210 --> 00:32:32,290
So these businesses are not a union, for example, they are regional data.

233
00:32:32,800 --> 00:32:34,660
If we do nothing about their real name,

234
00:32:34,660 --> 00:32:40,900
the regional data should be a sufficient statistic because the regional data content all the information about data.

235
00:32:41,910 --> 00:32:45,610
There's no data reduction, but that's up to be a studies.

236
00:32:45,880 --> 00:32:49,550
That doesn't have to be accurate. So there are regional data.

237
00:32:49,570 --> 00:32:56,350
They regional sample. This is a sufficient statistic. But this is a sort of a trivial, sufficient statistic.

238
00:32:56,740 --> 00:33:02,170
And it really doesn't help us because because there is no data reduction out of.

239
00:33:02,750 --> 00:33:08,080
And then we have seen that an order statistic is also a surveillance statistic.

240
00:33:08,710 --> 00:33:13,750
Right. But this one doesn't help too much either, because, again, there is no data reduction.

241
00:33:16,090 --> 00:33:22,900
And also for any surveillance that SD is 1 to 1 function is also sufficient statistical.

242
00:33:23,170 --> 00:33:26,860
So we are now going to approve this statement at this moment,

243
00:33:26,860 --> 00:33:34,899
but later we're going to prove a slightly more advanced version of this make a

244
00:33:34,900 --> 00:33:40,290
minimal for a minimal surveillance of anyone who should use a minimum statistics.

245
00:33:40,570 --> 00:33:42,430
But for this one for this statement here.

246
00:33:42,970 --> 00:33:48,370
Well, first of all, if you want to prove this, you can definitely prove this yourself by using the factorization theorems.

247
00:33:48,580 --> 00:33:53,230
That's totally doable. And but this is also very intuitive as well.

248
00:33:54,010 --> 00:33:58,950
If you think about this. So if you give me a provisional statistic, that means is it a content?

249
00:33:58,960 --> 00:34:07,860
All the information about theta. Now, if I have 1 to 1 transformation, one function of the sufficient statistic, none of these.

250
00:34:09,370 --> 00:34:12,670
So there's a 1 to 1 correspondence between those two.

251
00:34:13,150 --> 00:34:18,720
Then, of course, the one one function should also contain all the information about saying because I mean,

252
00:34:18,880 --> 00:34:23,320
this wonderful function doesn't really have what has nothing to do with theta.

253
00:34:23,500 --> 00:34:30,360
So if I make one transformation, then shouldn't either increase or decrease the information about theta.

254
00:34:30,370 --> 00:34:37,600
So. So in other words, if you give me a, a statistic, then I can construct almost numerously,

255
00:34:38,500 --> 00:34:43,150
even in many instances, because any one one function is a severe mistake.

256
00:34:44,680 --> 00:34:54,790
Now, the question is, can we find a sufficient statistic that achieve the maximum data reduction, including other words, you give me data.

257
00:34:56,260 --> 00:35:11,750
Well, let's say that now. That's one example, which I think is a lot easier to understand.

258
00:35:11,990 --> 00:35:21,560
So suppose we have this fertility trials, this three bedroom house now while later rubble out of the three questions.

259
00:35:22,550 --> 00:35:25,880
So is t one as severe? That is for Pete.

260
00:35:26,870 --> 00:35:30,080
And well, we have already seen that of this law.

261
00:35:30,100 --> 00:35:35,209
Some of the three this is us and we have seen this example quite a few times using

262
00:35:35,210 --> 00:35:41,180
different ways of proving this and later to see whether this is a loss of innocence.

263
00:35:41,270 --> 00:35:44,540
But anyway, so let's look at these two.

264
00:35:46,410 --> 00:35:50,120
Let's let's look at this. So this is a surveillance statistic.

265
00:35:52,040 --> 00:35:59,269
And this one over here, the first one, you can very, very easily show that while later we're going to talk about this.

266
00:35:59,270 --> 00:36:07,940
This is also a surveillance statistic. However, the difference between these two is that the first one measure is two.

267
00:36:09,500 --> 00:36:16,430
So in other words, if you want to give me this this statistic, then you need to give me two values.

268
00:36:16,790 --> 00:36:28,700
Well, now two is X one plus. The other one is Act three to tell me what are the the sum of the two is and a lot of the last the third one is.

269
00:36:29,570 --> 00:36:33,660
Whereas if you provided this, if you want to provide this value to me, this,

270
00:36:33,930 --> 00:36:36,920
it would mean that you just need to tell me, my son, the sum of the three.

271
00:36:37,880 --> 00:36:42,260
Of course, from a data reduction perspective, this one has a further deterioration.

272
00:36:42,470 --> 00:36:49,580
You only need to summarize the data using a scalar, just a single about it, and it has a sufficient statistic.

273
00:36:50,660 --> 00:36:59,090
The first one is also surveillance studies. So in this sense, then the latter one, the second one is better because it asks for the data reduction.

274
00:36:59,870 --> 00:37:05,479
You only need to use a scalar to summarize all the data without losing any information about this,

275
00:37:05,480 --> 00:37:12,889
because that's probably that's actually the idea behind the so-called concept of minimal surveillance studies.

276
00:37:12,890 --> 00:37:18,110
So we are looking at a surveillance strategy that achieves the maximum data reduction.

277
00:37:21,900 --> 00:37:26,370
So this is the formal definition of minimal sufficient statistical.

278
00:37:26,640 --> 00:37:36,550
So a sufficient statistic is called a minimal sufficient or minimal sufficient if for any other studies on this front.

279
00:37:37,700 --> 00:37:44,700
And this is actually a function of price is a identified.

280
00:37:46,500 --> 00:37:54,840
Now, let's take a look take this one as an example. So this is a sufficient statistic.

281
00:37:55,650 --> 00:38:03,930
So we are going to show this one is sufficient and we are going to show this one is minimal, sufficient or for now, let's just take these facts.

282
00:38:04,650 --> 00:38:12,430
And then you can see that this is actually the second one is actually the function of the first one let.

283
00:38:12,640 --> 00:38:16,530
Because once we know the first one well, we definitely know the second one,

284
00:38:16,530 --> 00:38:20,939
the value of the cycle one, because the second one is simply, you know, the sum of this this guy.

285
00:38:20,940 --> 00:38:24,570
And so the second one is a function of the first one.

286
00:38:25,170 --> 00:38:27,630
And for us, it has to be minimal sufficient.

287
00:38:28,290 --> 00:38:39,240
Then it has to be the function like for any surveillance that you give me, then I can write this one as a function of some solution.

288
00:38:39,600 --> 00:38:44,490
So that's the the definition of the minimal civilian statistic.

289
00:38:45,420 --> 00:38:51,930
So this t is called a minimum. If for any sufficient statistic, you give me p prime.

290
00:38:52,740 --> 00:38:55,500
I can write this T as a function of t prime.

291
00:38:57,270 --> 00:39:06,090
So in other words, this T represents a further theta reduction compared to any other sufficient statistics.

292
00:39:09,390 --> 00:39:11,010
Yeah. Well, hear this.

293
00:39:11,070 --> 00:39:24,390
If you voice the interest of another way of understanding what I described so from a central point, from a small space perspective.

294
00:39:26,250 --> 00:39:35,430
So recall that a a statistic is can be interpreted as a way of partitioning the sample space.

295
00:39:35,940 --> 00:39:42,210
And so depending on different values of similarity, then similar spaces partition different regions, for example.

296
00:39:46,110 --> 00:39:52,110
And this is the sample space where you have different of different possible outcomes.

297
00:39:52,500 --> 00:39:57,410
And then if you give me a statistic, then based on value of the statistic I have, you know,

298
00:39:57,930 --> 00:40:03,420
I can partition this small space into different regions depending on different values of the studies.

299
00:40:03,630 --> 00:40:10,050
I've seen this example. So this is for any statistic.

300
00:40:10,050 --> 00:40:14,760
Okay, sorry. This is first and then you view me.

301
00:40:14,760 --> 00:40:25,649
I can based on these different values, I can partition thus the sample space into different regions like in the Met and the minimum observations that

302
00:40:25,650 --> 00:40:33,479
is is achieved when the number of regions of this partition for the number of regions is minimum is minimal.

303
00:40:33,480 --> 00:40:39,240
So so I have the least number of partition of the sample space.

304
00:40:42,960 --> 00:40:47,760
So, so if for any statistic,

305
00:40:47,790 --> 00:41:00,030
if I look at its different values and look at these different values for all the possible outcomes in the sample space, if any, for any statistic,

306
00:41:00,150 --> 00:41:09,299
the number of regions of the partition is actually no less than the partition of this t we are looking at that

307
00:41:09,300 --> 00:41:16,650
here is the so-called minimum sufficient stability and this may be a slightly more mathematical interpretation,

308
00:41:16,650 --> 00:41:21,600
but again, intuitively, that's just a minimum sufficient stress test.

309
00:41:23,400 --> 00:41:31,230
So you have deferred sufficient statistics. And this of statistics do not necessarily correspond to the maximum data reduction.

310
00:41:33,120 --> 00:41:40,499
And but if I can find a sufficient statistic that has the maximum maximum amount of data reduction,

311
00:41:40,500 --> 00:41:45,989
that is which means that it is a factor, it can be expressed as a function of any sufficient studies.

312
00:41:45,990 --> 00:41:50,100
So you give me that it is the minimum sufficient statistic.

313
00:41:58,480 --> 00:42:11,889
Okay. Another way of looking at this, I mean, I'm not sure if this how much this helps,

314
00:42:11,890 --> 00:42:18,190
but we can definitely look at this and hopefully it helps to clarify a little bit further.

315
00:42:23,620 --> 00:42:31,510
So suppose that this is the sample space where we have, you know, different outcomes, different from possible outcomes.

316
00:42:32,320 --> 00:42:40,180
If you give me a sufficient statistic, let's try any so we can do that.

317
00:42:40,180 --> 00:42:44,140
It corresponds to a partition of the samples. Let's let's say that.

318
00:42:45,690 --> 00:42:55,500
It corresponds to a partition of the small space into this low key one prime, t2 prime, t3 prime, T4 prime.

319
00:42:56,130 --> 00:42:59,280
These are different values of this statistic.

320
00:43:00,180 --> 00:43:09,380
If you are balancing so many of the different values in a partition, assemble space into this four regions so that the pulp,

321
00:43:09,390 --> 00:43:16,230
the, you know, the outcome within each region that corresponds to the same value for this difference.

322
00:43:17,400 --> 00:43:30,960
Now, the minimal sufficient statistic is a statistic that can be expressed as a function of any T prime.

323
00:43:31,230 --> 00:43:36,330
So in other words, it corresponds to last number of partition of the central space.

324
00:43:36,690 --> 00:43:40,590
So it has in other words, it has less values.

325
00:43:42,780 --> 00:43:52,139
And at last balance. So that's why I mean, here we say this team is actually a function of few primes.

326
00:43:52,140 --> 00:43:56,000
So whenever you give me two prime, then this team is determined.

327
00:43:56,010 --> 00:44:09,130
This is a function of two prime. You're talking about this Texas occasion, what you're in simple speech.

328
00:44:10,140 --> 00:44:19,650
Yes. I mean, in the last like uniformly some say the smallest of the states and the margins were

329
00:44:19,660 --> 00:44:27,310
the minimum and max and like how I understand the minimum and that's segregation.

330
00:44:29,770 --> 00:44:32,800
Okay. That's a very good question.

331
00:44:35,440 --> 00:44:40,900
So let's not try to use that example as an example for Canadians.

332
00:44:41,020 --> 00:44:55,360
The reason is that this partition of the simple space, this is an idea that is in a discrete case.

333
00:44:55,390 --> 00:45:04,770
It's relatively easy to explain, but in the community's case that it corresponds with the matter of the of the of the different, different subsets.

334
00:45:04,780 --> 00:45:09,520
I mean, you have all the interval then you have you have a narrow define on that.

335
00:45:09,740 --> 00:45:13,090
So that so that many things are really, really complicated.

336
00:45:13,420 --> 00:45:21,370
So that's the reason why here, if you look at the party on here, we only talk about discrete case.

337
00:45:21,370 --> 00:45:26,540
I mean, the example that we use is actually burning five, the three running three numerals.

338
00:45:26,920 --> 00:45:35,680
And then we use that as a example to illustrate the partition of assemblies based on let's not worry too much about this.

339
00:45:35,680 --> 00:45:41,020
I mean, here. Okay, so talking about the minimal surveillance that is here,

340
00:45:41,020 --> 00:45:47,680
I have provided several ways of interpreting or understanding minimal sufficient statistic,

341
00:45:48,850 --> 00:45:53,490
and it's totally fine if you find out some ways they are less transparent.

342
00:45:53,500 --> 00:45:57,400
I mean, they are they are harder to understand compared to others.

343
00:45:57,670 --> 00:46:02,649
That's totally fine at this moment.

344
00:46:02,650 --> 00:46:12,070
You do not have to really to understand every I got people every single way I just described.

345
00:46:17,920 --> 00:46:25,000
I mean, the slides tried to explain those in different ways, but but sometimes I guess these probably make things more complicated.

346
00:46:25,450 --> 00:46:28,389
Now, let's just try to interpret minimum provisions in this way.

347
00:46:28,390 --> 00:46:35,110
So again, let's just consider just intuitively, without looking out of the mathematical notation, let's just think about it.

348
00:46:35,110 --> 00:46:41,350
So if you have surveillance, you can have different visual statistics and this difference.

349
00:46:41,860 --> 00:46:54,579
So being a statistic that corresponds with different levels of data reduction, just like, yeah, let's use this example to use this example.

350
00:46:54,580 --> 00:47:06,129
So maybe let's use this. So x1x to everyone has a history and then x one plus.

351
00:47:06,130 --> 00:47:10,260
X two plus. Okay.

352
00:47:10,260 --> 00:47:15,510
So now I have three statistics.

353
00:47:16,560 --> 00:47:21,840
First of all, while the first one is just the original data, and the second one is what has to remain.

354
00:47:22,350 --> 00:47:27,750
The third one is just a scale, and only three are sufficient statistics.

355
00:47:29,550 --> 00:47:33,630
So the first one is this is a sufficient because it's just original data.

356
00:47:33,840 --> 00:47:38,820
So it's some vision. In the second one, we are going to show that it is a statistic.

357
00:47:39,180 --> 00:47:43,140
And the third one we have already shown some is actually the surveys.

358
00:47:43,560 --> 00:47:49,560
So all these are to statistics, but they correspond to different levels of data reduction.

359
00:47:49,680 --> 00:47:54,420
The first one has no data reduction. I hope it just gives you the original data.

360
00:47:55,140 --> 00:48:01,850
The second one has some level of data reduction. But not to the maximum reduction.

361
00:48:02,180 --> 00:48:07,670
The third one has the maximum because you summarize the data as a as a scalar.

362
00:48:08,960 --> 00:48:13,840
So among these three, the last one corresponds to the maximum data reduction.

363
00:48:13,880 --> 00:48:18,200
And it turns out that the last one is the so-called minimal sufficient statistic,

364
00:48:18,920 --> 00:48:24,470
because among all these sufficient statistics, the last one corresponds to the maximum data reduction.

365
00:48:26,540 --> 00:48:34,880
And this is not the intuition or the logic behind or the rationale behind minimal sufficient statistics.

366
00:48:35,300 --> 00:48:40,160
So it is so variance the risk and also in a corresponds to the maximum data reduction.

367
00:48:41,900 --> 00:48:49,370
This is probably the the easiest way to understand the rationale behind minimal surveillance statistic.

368
00:48:49,970 --> 00:48:52,250
I mean, the other way is we are you covered.

369
00:48:52,640 --> 00:48:57,620
There are all kinds of mathematical you specifically, for example, that, you know, like notation like this.

370
00:48:58,190 --> 00:49:04,010
This is mathematical. But if you look at if you understand the mathematics like in notations,

371
00:49:04,160 --> 00:49:14,030
you will see that that is actually what this says, is simply that the idea we explain using this example.

372
00:49:14,580 --> 00:49:21,440
It's just that we try to reduce the data to the maximum, but we still want to keep all the information in the data.

373
00:49:24,140 --> 00:49:26,240
So I had a question between one and D2.

374
00:49:26,240 --> 00:49:34,490
So in D2, is there any data reduction happening because the number of partitions in D one and D2, I bought six.

375
00:49:36,050 --> 00:49:43,640
So there is data reduction because here you only have one dimension.

376
00:49:44,570 --> 00:49:49,879
But if you look at the number of partitions on partition is because here you have I

377
00:49:49,880 --> 00:49:56,820
believe eight different combinations there on this here is probably only six one,

378
00:49:57,740 --> 00:50:03,620
then it's six in both. That's why we're looking at zero one, two, three, four.

379
00:50:03,680 --> 00:50:08,050
Those are going to give 012. The first one can be zero or one. So that's two.

380
00:50:08,100 --> 00:50:12,640
Yes. So indeed it corresponds with the introduction. Yeah.

381
00:50:17,390 --> 00:50:20,630
I hope this clarified things a little bit.

382
00:50:20,750 --> 00:50:24,100
I mean, sometimes they're small.

383
00:50:24,590 --> 00:50:30,860
There's too much mathematical notation. You know, sometimes we lose that in the bigger picture here.

384
00:50:31,220 --> 00:50:42,650
But I think that it is helpful if we just for use this as an example to first to understand the idea behind the rationale,

385
00:50:42,650 --> 00:50:48,650
behind the loss of being a statistic. And then you can take on more detail, look out of the mathematical equation.

386
00:50:49,200 --> 00:50:54,530
And because I know he's just a more rigorous way of expressing this idea.

387
00:50:58,390 --> 00:51:03,380
Okay. So that's a minimal sufficient statistic.

388
00:51:06,020 --> 00:51:15,197
Okay. Now let's take, you know, 8 minutes break and then we will come up you a short cut.

389
00:51:26,408 --> 00:51:33,558
We have the following the following result for us to count whether somebody has the disease,

390
00:51:33,578 --> 00:51:40,417
minimal surveillance studies that corresponds to this, I suppose.

391
00:51:40,418 --> 00:51:43,658
Now we have the PDA for PM out for some.

392
00:51:45,458 --> 00:51:54,618
I suppose there exists a function in this country that's an interesting as we mentioned, any arbitrary function of the data is so.

393
00:51:55,288 --> 00:52:03,698
So will this function satisfy that for any two values, like for any values X and Y,

394
00:52:04,778 --> 00:52:20,558
this ratio is a constant as a function theta even only t x is equal to P1, then t is a minimal sufficient statistic for you.

395
00:52:23,228 --> 00:52:27,488
So this again, of course, all the theorems have to be a little bit mathematical, right?

396
00:52:27,488 --> 00:52:36,248
So but what this means is that now in order to tell whether a statistic is minimal, sufficient or not,

397
00:52:37,178 --> 00:52:46,628
while we need to look at two things, one thing is that we look out of the statistical value at a two different samples, X and Y.

398
00:52:48,278 --> 00:52:56,018
And if this T this if this statistic takes at an equal value and this X and Y values are the same,

399
00:52:57,248 --> 00:53:06,068
this implies that this ratio that the ratio of this to disease, if this result does not even consider anymore.

400
00:53:07,148 --> 00:53:11,438
And this is one direction and also if.

401
00:53:15,518 --> 00:53:23,878
This ratio is a concern of theta. That implies that the statistical value out of this tool will add to two samples.

402
00:53:23,888 --> 00:53:31,208
And what if there are equal and if they are both true, then this t is minimal sufficient statistic.

403
00:53:32,828 --> 00:53:36,548
So in order to apply this theorem now, we need to check two things.

404
00:53:36,878 --> 00:53:41,078
One thing is this and one thing is this.

405
00:53:43,628 --> 00:53:50,267
So we are not going to prove this result, but we are going to see how to apply this sort of warping,

406
00:53:50,268 --> 00:53:54,938
which will argue that in our given studies, these minimum are not sufficient statistical.

407
00:54:01,988 --> 00:54:05,647
Okay. So this is again, in terms of the parties and assemble space.

408
00:54:05,648 --> 00:54:11,588
I mean, this is one thing I don't like so much about.

409
00:54:11,678 --> 00:54:19,568
You know, the the slides emphasize too much on the original assemble space.

410
00:54:19,928 --> 00:54:22,388
Assemble space. Sometimes it's got to get really confusing.

411
00:54:23,078 --> 00:54:34,028
So now in terms of the parties of the single space, it it means that now if we put X and Y into the same class.

412
00:54:35,708 --> 00:54:46,628
Well, this means is that if X and Y, if X and Y, they gave the same value for that statistic, that means we put X and Y into the same class.

413
00:54:47,558 --> 00:54:56,738
So if we put X and Y into the same class, even only if this ratio is independent of theta.

414
00:54:58,328 --> 00:55:05,888
So that's my interpretation. Minima. So because that is from the sample space partitioning for some space on A or B.

415
00:55:08,538 --> 00:55:13,098
And any studies corresponding to this partition is the single minimum sufficient statistic.

416
00:55:14,298 --> 00:55:20,478
But this and yeah, so the interpretation from partition of single spaced point of view is not very intuitive.

417
00:55:21,108 --> 00:55:22,048
It's not very intuitive.

418
00:55:22,068 --> 00:55:29,928
Again, I personally, I would suggest interpreting the minimum superior statistic as again from a data reduction point of view,

419
00:55:30,168 --> 00:55:37,247
although these two are interconnected. So we have several statistics that the studies suggest that is the content,

420
00:55:37,248 --> 00:55:44,718
all the information about theta in the original data and that they correspond to different levels of data reduction.

421
00:55:45,558 --> 00:55:50,778
Now the minimal Sabrina statistic is the one that corresponds to the maximum data reduction.

422
00:55:51,818 --> 00:55:55,608
So minimal surveillance there is still content.

423
00:55:55,878 --> 00:56:00,648
While keep all the information in the original data I think corresponds to the maximum reduction.

424
00:56:03,468 --> 00:56:09,818
Now let's take a look at this example that we have now be using as illustration.

425
00:56:12,468 --> 00:56:15,618
So it's approved. You have 332 files.

426
00:56:16,998 --> 00:56:22,968
And of course, this P, the success probability P is the parameter that we are interested in.

427
00:56:24,108 --> 00:56:31,978
And in the question one well is well ask is t one for two.

428
00:56:32,928 --> 00:56:37,458
Now let's look at question one. The answer is yes.

429
00:56:38,058 --> 00:56:41,648
Well, actually, before we look at that.

430
00:56:43,908 --> 00:56:51,438
So before even carrying out the formal calculation, it's very intuitive to see that if this is a sufficient statistic.

431
00:56:54,118 --> 00:56:59,068
Because the information that we want to have is the information about success probability.

432
00:57:00,178 --> 00:57:09,958
So if we look out of this system over here and so it tells us the sum of the first two, and then that tells us the outcome of the third file.

433
00:57:10,648 --> 00:57:14,248
Of course, knowing these truths, what is true values?

434
00:57:16,228 --> 00:57:19,708
We should know all the information about that. Because if we.

435
00:57:20,788 --> 00:57:24,988
Because if we trade with some of these, that's that's out of the three files.

436
00:57:24,988 --> 00:57:28,048
How many has we got? How many failures, how many tests we get?

437
00:57:28,678 --> 00:57:33,298
And that number actually corresponds to the has all the information about about Pete.

438
00:57:33,308 --> 00:57:37,617
So in other words, intuitively, this should be a sufficient statistic.

439
00:57:37,618 --> 00:57:41,038
It should contain all the information about Pete in the original data.

440
00:57:41,728 --> 00:57:51,718
So because it doesn't matter whether this individual X one an individual extra while they are no matter whether x one is tell we're x two is a town.

441
00:57:51,988 --> 00:58:00,928
As long as I know that it's one class x two among these two years there's a to have an intel.

442
00:58:02,248 --> 00:58:09,178
So now let's formally from that so we prove that again by factorization theorem.

443
00:58:09,238 --> 00:58:15,248
So this is always the theorem that we use to prove a statistic is sufficient statistics.

444
00:58:15,508 --> 00:58:25,528
So this is the joint PDF and this is my drawing because we have a Bernoulli distribution.

445
00:58:25,528 --> 00:58:32,878
So this is not just the preferred October three Bernoulli diffusion and then we write it in terms of the statistic.

446
00:58:32,878 --> 00:58:41,188
So this is X1 plus X2, this corresponds to, you know, the first couple in here and then we re right at three separately.

447
00:58:41,968 --> 00:58:49,768
So that's X3 and then we have X1 sort of X1 plus X to appear in the X3.

448
00:58:50,878 --> 00:58:51,778
So many other words,

449
00:58:52,678 --> 00:59:10,378
we can write the drawing a pdf in the following way as a function of g times h g tabs age where g is x while g is a function of x1 x2 so an x three.

450
00:59:10,378 --> 00:59:17,698
So this is not the true component of these statistics x1, plastics and x3.

451
00:59:18,568 --> 00:59:23,638
And then we have the age function. In fact, I was simply taking it would be equal to one.

452
00:59:25,708 --> 00:59:35,458
So after writing the drawing a PDF in this particular way, then we realized that if Occupy fact theorem is t, one must be a sufficient statistic.

453
00:59:38,908 --> 00:59:42,837
Okay. So this shows the obvious answer to the first question.

454
00:59:42,838 --> 00:59:52,468
So indeed, yes, this is a severe statistic. And the question two is, is key is the sum of the three a minimal.

455
00:59:52,468 --> 01:00:01,428
So visual statistics and we have already shown that the sum of the three is sufficient and that's what we have shown.

456
01:00:02,728 --> 01:00:09,208
Now, the question is whether this is minimal sufficient, and the answer is yes.

457
01:00:09,238 --> 01:00:15,747
Well, this very intuitive because because in this case we have one parameter, right.

458
01:00:15,748 --> 01:00:19,468
And this key who has only one dimension.

459
01:00:19,468 --> 01:00:27,738
So in other words, I mean, we summarize the data to the maximum so that in the end there's only one data point left and there's only one quality left.

460
01:00:27,958 --> 01:00:34,108
One was scalar and after you just neutral reporting the sum of that for the three variables,

461
01:00:34,318 --> 01:00:42,358
of course there cannot be further data reduction and if you further do data, then there is nothing left.

462
01:00:42,778 --> 01:00:49,538
So so of course, intuitively this is a minimal sufficient statistic, but now let's try to use that.

463
01:00:50,028 --> 01:00:57,718
Oh yeah. We just done covered try to show that indeed this is a minimal sufficient statistic.

464
01:00:59,938 --> 01:01:08,458
So in order to look at well in order to show this is minimal. So business statistic, if we apply this result, we need to show that.

465
01:01:10,768 --> 01:01:16,258
We need to show that this ratio is a constant as a function theta if and only if

466
01:01:16,948 --> 01:01:22,408
the statistic and at two different x and y values are the same that are equal.

467
01:01:25,288 --> 01:01:32,858
But let's see how to apply that result. So first, let's start with the ratio of this to testing.

468
01:01:33,538 --> 01:01:40,018
So that does the evaluating that an X can evaluate and what two different samples X and one.

469
01:01:42,678 --> 01:01:45,438
And if we just threw some very simple algebra.

470
01:01:45,438 --> 01:01:52,608
So because both the numerator and denominator are these are strong intensities for family run variables.

471
01:01:52,938 --> 01:01:56,898
So we're ready to ask the P to power.

472
01:01:57,048 --> 01:02:05,988
This is nice to know that we're looking at some of these exercise and some exercise and some old wise and some more wise,

473
01:02:07,398 --> 01:02:10,678
and then we can further write it as this.

474
01:02:13,878 --> 01:02:24,468
And then from here, it's really been easy to see that this ratio is a constant that is independent of theta or independent of height.

475
01:02:25,038 --> 01:02:31,097
If in only this power function or this power is equal to zero, I guess otherwise.

476
01:02:31,098 --> 01:02:37,278
I mean, if this is not equal to zero, then this ratio is never a constant of P or custom of theta.

477
01:02:38,688 --> 01:02:45,888
So in other words, what we can look at this two direction.

478
01:02:45,898 --> 01:02:50,568
So if t2x is equal to two Y,

479
01:02:50,688 --> 01:02:55,458
that means if the sum of exercise equal to some of why I this power is equal

480
01:02:55,458 --> 01:03:02,628
to zero about first the ratio is constant in the ratio does not depend on t.

481
01:03:04,008 --> 01:03:10,308
On the other hand, if the ratio is constant, then that means the power must be equal to zero.

482
01:03:12,198 --> 01:03:15,558
That means the sum of EXI must be equal to the sum of what?

483
01:03:17,448 --> 01:03:22,398
So that then t two at exi must be equal to t2.

484
01:03:22,458 --> 01:03:25,758
At what if you add x must be equal to t2 one.

485
01:03:26,508 --> 01:03:43,548
So what we have shown is that the ratio is a constant which.

486
01:03:50,258 --> 01:04:07,278
Of t what does not matter if and only if t2 x is equal to t2 y and then by the theorem we just hold about, right?

487
01:04:07,348 --> 01:04:12,718
So that t2 x which is the sum of x eyes.

488
01:04:12,898 --> 01:04:20,548
This is a bit of observation, a statistic. So this shows that this view is minimal sufficient.

489
01:04:22,558 --> 01:04:27,928
So this is how we applied that particular theorem to show that it is minimal sufficient.

490
01:04:32,128 --> 01:04:36,058
So this is this is minimal sufficient. And then let's look at a question.

491
01:04:36,058 --> 01:04:44,818
Three plus the three R's is T one, which has two components, one plus X, two and three.

492
01:04:45,298 --> 01:04:48,478
Is this what a minimal sufficient statistical work?

493
01:04:51,148 --> 01:04:54,448
And then the answer is a pattern. I no.

494
01:04:55,498 --> 01:04:59,848
Well, even without going through the calculation, without a plan that zero.

495
01:04:59,848 --> 01:05:05,208
Right. Because this is because of the definition of minimal sufficient studies.

496
01:05:05,218 --> 01:05:12,418
Women of sufficient means, because based on question two, we already show that this is a minimal sufficient statistic.

497
01:05:12,988 --> 01:05:16,138
So this corresponds to the maximum reduction of the data.

498
01:05:16,948 --> 01:05:28,648
But this that is still content. All the information about theta three now two two has higher dimension, two has two components.

499
01:05:30,508 --> 01:05:35,098
So of course we have already found of. So we just minimize evidence.

500
01:05:35,098 --> 01:05:43,888
That is the last dimension one. So now, of course, any evidence that is to have higher dimension cannot be minimal sufficient statistics.

501
01:05:45,088 --> 01:05:48,838
So it shows you that this is not a minimal severe instance.

502
01:05:50,218 --> 01:05:58,468
But now let's still apply that a theorem, you know, 6.2 2.13 to show that this is not a minimal sufficient statistic.

503
01:05:58,498 --> 01:06:01,768
Just as a decision of how to apply the theorem.

504
01:06:04,408 --> 01:06:07,647
So now we are considering a statistic that has two components.

505
01:06:07,648 --> 01:06:11,757
Let's say as one and as two. As one is the sum of X.

506
01:06:11,758 --> 01:06:20,828
One adds to its choice at three. So let's apply that to that fear, that result.

507
01:06:20,858 --> 01:06:28,298
So in this case, the drawing, the PDF, we need to write of the drawing a pdf as a function of S1 and S2.

508
01:06:30,388 --> 01:06:43,138
So this is fairly easy. We have already done this. So we group x one and x two together and then x three and then as x one plus x two.

509
01:06:43,138 --> 01:06:47,448
This is s one, right? And this is what this is asked to.

510
01:06:48,208 --> 01:06:52,848
So that's why here, if in combination we have p to power s one plus has two.

511
01:06:53,278 --> 01:06:56,848
Now Y minus p two power three minus x one has two.

512
01:06:57,328 --> 01:07:05,308
So so far is just algebra. We just write. We just write this drawing a PDF in terms of the AC one as two, as one as two.

513
01:07:06,808 --> 01:07:13,738
Now let's look at the ratio of this that's at different values, X and Y.

514
01:07:13,918 --> 01:07:22,818
That's what these are maps. And let's see whether this is a concept, whether this is equivalent of t one.

515
01:07:22,828 --> 01:07:26,638
And if you want the axes assigned to actually see or if you want.

516
01:07:28,528 --> 01:07:35,068
So if we look out of the ratio of this to now, because we have already surpassed the density this way,

517
01:07:35,068 --> 01:07:41,458
so then the top and bottom we just write them and then we just up here.

518
01:07:41,488 --> 01:07:46,018
Now this we can combine this two and we can combine this two.

519
01:07:46,048 --> 01:07:54,008
So we have p p over one minus p to the power as one plus as to the minus as one, minus as two.

520
01:07:54,628 --> 01:07:59,218
This is an X. And what so far is, again, just algebra.

521
01:07:59,488 --> 01:08:10,528
So we're trying to reorganize for the term. And now let's see whether this is a constant, whether that implies s one.

522
01:08:11,518 --> 01:08:18,408
What that implies as one is equal to as one absolutely zero as true.

523
01:08:20,578 --> 01:08:27,538
So if this is a constant if this is constant, then it means the power must be equal to zero.

524
01:08:28,138 --> 01:08:33,628
Otherwise, we do not have a constant function. So the power equal to zero.

525
01:08:35,728 --> 01:08:41,788
That does mean that, as one must be, you control this AC one and this astral must be the census.

526
01:08:42,718 --> 01:08:50,098
Because as long as as one person asks who is equal to this as one plus as two, then we have power equal to zero.

527
01:08:52,768 --> 01:08:59,458
So we are the worst having this tool of whatever the reason has caused and doesn't

528
01:08:59,458 --> 01:09:04,108
imply that no as one is able to answer because one has to be able to have two.

529
01:09:04,678 --> 01:09:13,438
So this means that by using this result, this means that this as well as two is not a sufficient statistic.

530
01:09:15,418 --> 01:09:19,408
It's not a separate statistic. Now, of course, the other duration is cracked.

531
01:09:19,408 --> 01:09:27,328
So now if as one is equal to as one and as two is equal to two, now, of course, the power is equal to zero.

532
01:09:29,538 --> 01:09:35,168
And which means that the ratio is a constant, but at the other direction he's not correct.

533
01:09:35,178 --> 01:09:38,238
So the power is caused doesn't mean that these two.

534
01:09:38,418 --> 01:09:48,788
Okay. So then because of this condition of 6.2.61 2.13, the collision does not hold.

535
01:09:48,798 --> 01:09:52,428
So that is not a minimal sufficient statistic.

536
01:09:53,388 --> 01:10:00,558
So this is how we applied this result to show that this statistic is not a minimum

537
01:10:00,558 --> 01:10:06,138
sufficient statistic in this recall to agree with our with our intuition.

538
01:10:06,438 --> 01:10:12,948
So again, because we have a plan of action, this is minimal sufficient,

539
01:10:13,608 --> 01:10:19,158
and this one cannot be minimal sufficient anymore because it has higher dimensional.

540
01:10:23,268 --> 01:10:27,318
Shh, shh, shh, shh. Okay. Any questions so far?

541
01:10:27,338 --> 01:10:34,958
So hopefully by using this example rather than actually, you know, this example was moved earlier.

542
01:10:37,298 --> 01:10:45,428
So by using this example, hopefully if these you guys have a better understanding of the mental loss of a statistic.

543
01:10:45,638 --> 01:10:47,728
And then when you read these, you know,

544
01:10:47,738 --> 01:10:57,578
these more mathematical statements gives you I have an easier time really understanding the logic behind this mathematical statement.

545
01:11:06,908 --> 01:11:11,548
Good because I was falling asleep. No.

546
01:11:11,898 --> 01:11:15,108
Yes. Yes. Yes.

547
01:11:15,718 --> 01:11:19,388
The average. Yes. Yes. Oh, yeah. Definitely so.

548
01:11:19,418 --> 01:11:27,138
Well, that's a very good question. So here now we have shown that a this the sum is minimal, sufficient a statistic.

549
01:11:28,238 --> 01:11:33,418
Then of course, intuitively, the average should also be a minimum of studies,

550
01:11:33,638 --> 01:11:44,208
because if we I consider the average the average is is 1% to pass through line by three.

551
01:11:44,228 --> 01:11:48,398
Right. And this is also is also minimal sufficient.

552
01:11:56,498 --> 01:12:02,138
Is also minimal surveillance. But for now, let's just look at this intuitively.

553
01:12:02,618 --> 01:12:10,208
Intuitively, this should be the case. Right? Because whatever information, you know, the sum, the contents, well,

554
01:12:10,478 --> 01:12:15,847
the information is as carried over to the sample region because they they're dividing by

555
01:12:15,848 --> 01:12:21,808
by sample size by 3000 either increase the information about P or because of information.

556
01:12:23,018 --> 01:12:26,708
So that's why this is also minimal diffusion. And later on.

557
01:12:26,838 --> 01:12:30,008
Well, I'm going to show you very quickly later at the very end,

558
01:12:30,008 --> 01:12:37,168
we have a result we will not cover in this lecture, but I will cover it in the next lecture.

559
01:12:37,628 --> 01:12:42,788
So if t is minimal sufficient, then it's one to function is also minimum sufficient.

560
01:12:43,868 --> 01:12:48,868
And of course, then taking the sample average, I'm just divided by by three.

561
01:12:48,878 --> 01:12:52,838
This is a 1 to 1 function. That's why it's also minimal.

562
01:12:53,058 --> 01:12:58,028
Sufficient emotional.

563
01:12:58,808 --> 01:13:02,558
Minimal is sufficient statistics. It's always greater.

564
01:13:02,828 --> 01:13:11,338
It's always equal summation. Not necessarily.

565
01:13:11,708 --> 01:13:20,598
Not necessary. Well, we can we can use the van.

566
01:13:20,638 --> 01:13:27,868
We can use this one as an example. I mean, this is not the best example, but it's sort of probably hopefully out of the question.

567
01:13:28,588 --> 01:13:33,838
So we cover this example. So this example, there's only one theta.

568
01:13:35,788 --> 01:13:37,258
So if you only have a school of theta,

569
01:13:37,888 --> 01:13:48,027
but when you try to find a sufficient statistic and also various studies as to dimension, you can choose to like, oh, well,

570
01:13:48,028 --> 01:13:50,458
the thing is that if you further reduce the dimension,

571
01:13:50,728 --> 01:13:56,328
then if you consider any one dimensional quality, it does not contain all the information out there.

572
01:13:57,958 --> 01:14:04,138
What about, for example, the meaning of the meaning of mathematics or like the max of them?

573
01:14:07,078 --> 01:14:17,538
Okay. So that's the reason why I didn't think this is the ideal example because, well, let's say you consider you consider this, let's consider this.

574
01:14:17,548 --> 01:14:25,258
Is that right? And first of all, while this may not even be a sufficient statistic, we haven't shown this as a surveillance.

575
01:14:27,028 --> 01:14:35,848
What we have shown is that only this is sufficient. But this is actually indeed not a surveillance because it doesn't contain information about theta.

576
01:14:39,268 --> 01:14:43,348
So in other words, I guess let's not worry too much about this concrete example.

577
01:14:43,528 --> 01:14:49,188
So piano words, the point I'm trying to make is that so the minimal surveillance studies,

578
01:14:49,258 --> 01:14:53,608
that dimension does not have to match the dimension of theta itself.

579
01:14:55,408 --> 01:15:04,508
Okay, so if you have only a scalar theta, it does mean that at the minimal sufficient studies, this has to be just a one dimension statistic.

580
01:15:04,998 --> 01:15:12,928
You might be able to construct examples where the minimum of instability that you measure these as a higher than what are the.

581
01:15:18,248 --> 01:15:22,598
So we just need to be more cost efficient.

582
01:15:22,648 --> 01:15:30,558
It's sort of like Kuwait directly to calculate anything sufficient to not just a compare.

583
01:15:30,838 --> 01:15:47,188
We we are given three simulations, so we compare each other to it's it's kind of hard, generally speaking.

584
01:15:47,548 --> 01:15:54,808
So if you look at the result we have, I mean this particular is out like the theory for minimum civilian statistical,

585
01:15:55,438 --> 01:16:04,898
this result is sort of a result of that allows you to to check if a device that is a small something didn't work.

586
01:16:05,708 --> 01:16:12,268
So suppose you are to have a candidate in mind and then resolve to check by checking whether

587
01:16:12,478 --> 01:16:18,178
this tool there even only if you can check whether this one has been investigated or not.

588
01:16:18,928 --> 01:16:24,238
But you sort of needs to have a candidate in mind. So this there is a theorem me to solve.

589
01:16:24,238 --> 01:16:28,618
It doesn't tell us your construct. I'd made lots of statistics.

590
01:16:31,058 --> 01:16:36,997
And actually there is no such general result that tells you how do you generally

591
01:16:36,998 --> 01:16:41,168
construct a minimal subordinate statistic for different distributions?

592
01:16:41,828 --> 01:16:50,708
So these men are most often a statistic, usually based on their experiences with a different experience based or knowledge based on prior knowledge.

593
01:16:50,978 --> 01:16:53,348
Some of those come out with some reasonable statistics.

594
01:16:53,618 --> 01:17:00,578
You think that might summarize all the information in the data, and that might corresponds to the maximum data reduction.

595
01:17:00,818 --> 01:17:03,838
And then you can use this one to check. Okay.

596
01:17:11,568 --> 01:17:21,147
Any other questions? Okay.

597
01:17:21,148 --> 01:17:30,078
So this slide here, again, it tries to explain this using the partition or the symbol space polynomial.

598
01:17:30,088 --> 01:17:41,338
So now the original sample space, as we mentioned, has this a defined accomodations or in other words, a, the possible outcomes.

599
01:17:41,518 --> 01:17:47,768
If you throw this coin three times, you will see this well, you can possibly see a different angle.

600
01:17:48,868 --> 01:17:54,177
Now this the density. We are assured that this is sufficient, a statistic, not minimal.

601
01:17:54,178 --> 01:18:02,908
So visited by so various statistic. And this statistic you can see that it has one, two, three, four, five, six, six different values.

602
01:18:04,168 --> 01:18:07,708
So in a partition the sample space in the 64 regions.

603
01:18:09,358 --> 01:18:19,858
And so for example, these two outcomes may fall into the same region and of these two outcomes, they fall into the same region.

604
01:18:20,698 --> 01:18:28,648
Right. So in other words, now this this one indeed the corresponds to a reduction of the sample space reduction of that of the original data.

605
01:18:30,508 --> 01:18:34,138
But it is not a minimum sufficient of it.

606
01:18:34,708 --> 01:18:40,318
But this one is what it means is that this one corresponds to the maximum reduction.

607
01:18:40,558 --> 01:18:45,298
And indeed you can see that for this one it has only one, two, three, 444 knowledge.

608
01:18:46,138 --> 01:18:55,527
And actually for these two values and these two values corresponds to only those that corresponds to the same

609
01:18:55,528 --> 01:19:05,338
value one for T2 key to a statistic in a similar and this two out of this car goes to this model for this team.

610
01:19:05,938 --> 01:19:08,728
So this T2 represents a further data reduction.

611
01:19:09,508 --> 01:19:17,668
And it turns out that while there is no further data reduction compared to a T2 corresponds to the maximum reduction.

612
01:19:18,418 --> 01:19:22,888
Now still keep all the information in the original and all the information about the success.

613
01:19:25,498 --> 01:19:28,768
And this is also what we mean by T2.

614
01:19:28,768 --> 01:19:34,558
The minimal sufficient statistic is actually a function of any sufficient statistic.

615
01:19:34,918 --> 01:19:41,158
So t y is sufficient but not a minimal sufficient key to is minimal sufficient and

616
01:19:41,158 --> 01:19:46,438
t2 is a function t one in the sense that once you tell me the value of key one,

617
01:19:46,828 --> 01:19:54,918
then I know the corresponding value of T2. So for example, if you tell me T1 one takes this value, then I know t true.

618
01:19:54,928 --> 01:20:00,868
Must take this. Now if you tell me T1 one take this value, then I know that T2 must take this value.

619
01:20:01,678 --> 01:20:04,678
But the the other way is not a cracked right.

620
01:20:05,158 --> 01:20:09,238
So that's what we mean by T2 is as a function of T1.

621
01:20:11,668 --> 01:20:18,148
So T2 can be determined by T1. So this is just another way of looking at.

622
01:20:18,148 --> 01:20:24,538
It may have also been a statistic from a simple legal provision, the simple space of view.

623
01:20:32,578 --> 01:20:44,738
Any questions before we move on to the next topic? Okay.

624
01:20:44,798 --> 01:20:54,548
Now, before we look at an additional example, let's take a look at some background knowledge because Theorem 6.0,

625
01:20:54,548 --> 01:21:00,628
1.4 and 2.13, we need to improve if and only if I.

626
01:21:00,638 --> 01:21:07,747
So the ratio is constant. I'll say to even only if X is equal to Y.

627
01:21:07,748 --> 01:21:11,168
So these are even only condition that we need to prove.

628
01:21:12,808 --> 01:21:22,258
So here are some very useful facts that are very useful in improving this even only condition.

629
01:21:23,788 --> 01:21:29,998
So. Well, first of all, the first fact is that now if we have a quadratic, a function of data,

630
01:21:31,438 --> 01:21:38,458
then this is equal to zero for any theta if and only if all these three coefficients are equal to zero.

631
01:21:39,268 --> 01:21:45,838
And this is relatively easy to see, first of all, because on one hand,

632
01:21:45,988 --> 01:21:50,908
if all these three are equal to zero, then of course, this should be equal to zero.

633
01:21:52,168 --> 01:22:02,368
This is very easy to see. Now, when this is equal to zero for any theta, then I can take different theta values, for example.

634
01:22:02,738 --> 01:22:06,658
Now, here. Now we look at a fact one.

635
01:22:06,658 --> 01:22:13,888
So if I take theta equal to zero, this implies that I take the zero.

636
01:22:13,918 --> 01:22:18,508
Then what is left is just the seat. This implies C's equal to zero.

637
01:22:19,678 --> 01:22:29,668
And if I take thena equal to 41, this implies that I have a plus b equal to zero.

638
01:22:29,698 --> 01:22:35,198
Now because C is our name, we already know C must be zero and theta equal to negative one.

639
01:22:35,228 --> 01:22:38,608
This implies that A minus B is equal to zero.

640
01:22:38,938 --> 01:22:46,108
And these two together implies that A is equal to 0p00 as all the other words.

641
01:22:46,138 --> 01:22:57,868
Now, if this is true for any theta, then by taking theta, by taking a three specific set of values, I can show that C and B, C must all be present.

642
01:22:59,098 --> 01:23:03,538
So this shows that here we have a F in all these.

643
01:23:06,318 --> 01:23:09,738
And the second the fact is the generalization of the first fact.

644
01:23:09,948 --> 01:23:18,438
So if you have a polynomial function of theta and equal to C or minus C zero,

645
01:23:19,068 --> 01:23:26,388
then that's even only if all the coefficients including C are equal to zero.

646
01:23:27,228 --> 01:23:30,588
This is just a generalization of the first first fact.

647
01:23:32,318 --> 01:23:34,608
A similar to the third one here.

648
01:23:36,458 --> 01:23:46,418
The third fact here, though, this is equal to zero for anything I want to value even olives and B and C aren't equal to zero.

649
01:23:46,898 --> 01:23:55,028
So again, this while this direction is relatively easy to see because when A-B-C are all equal to zero.

650
01:23:55,448 --> 01:24:00,728
Of course this will be equal to zero for any thing about what's even to happen.

651
01:24:02,168 --> 01:24:06,548
But I this but this is equal to zero for anything I want to.

652
01:24:07,448 --> 01:24:10,758
Now, again, I can take different things. I want the total value.

653
01:24:10,778 --> 01:24:17,768
So if I take a theta one equal to, say, a two equal to zero, this implies that a C must be equal to zero.

654
01:24:20,888 --> 01:24:33,558
And then if I take a seat, a one equal to one third or two equal to zero, this implies a one able to one that means a threat to equal to zero.

655
01:24:33,578 --> 01:24:39,067
That's that's that's b so B disappear and c we our relations from C zero.

656
01:24:39,068 --> 01:24:47,958
So this implies a zero. And then if I take C theta one equal to zero at all, equal to one, this implies p zero.

657
01:24:51,558 --> 01:25:01,668
So in other words, I mean, we have this event and these are just some you know, some well known facts.

658
01:25:01,998 --> 01:25:14,508
And for regarding polynomial functions later we're going to use and a similar but we have the following facts we're not going to show this in details,

659
01:25:14,508 --> 01:25:18,078
but first of all, they should be straightforward and assign a role.

660
01:25:18,078 --> 01:25:22,278
I mean, you guys can you can show these yourself if you want to.

661
01:25:23,078 --> 01:25:29,688
If we have this ratio, the ratio is how long a function like an Apollo moon function case.

662
01:25:30,648 --> 01:25:36,558
Now, if the ratio is constant, while the ratio is constant if and only if,

663
01:25:37,908 --> 01:25:44,778
all these coefficients are the same, all these collisions, all these collisions are.

664
01:25:49,748 --> 01:25:55,268
And notice that this is not true if we don't have the cost of one in effect.

665
01:25:56,498 --> 01:26:00,428
The reason is that now, without a cause caused in a frown,

666
01:26:00,428 --> 01:26:06,788
if we consider this two polynomials, then it could be the ratio could be a constant without.

667
01:26:07,238 --> 01:26:13,808
Here apparently the covariance here the colonies here are not equal, but there is.

668
01:26:14,078 --> 01:26:16,478
There is a constant. Then the ratio is.

669
01:26:17,978 --> 01:26:27,337
The ratio is in event of a figure, even only for all the other coefficients of the polynomials are the same as here.

670
01:26:27,338 --> 01:26:33,938
If we have indicator functions, then your margin is a cause of theta.

671
01:26:34,328 --> 01:26:47,048
Even only if the even only of these two indicators they are all the same interval is equal to be as are a is equal to C and A b is equal.

672
01:26:50,108 --> 01:26:55,988
Otherwise, this is not a constant affair and also that this is the power function.

673
01:26:56,168 --> 01:27:04,298
So theto the power team and this is called head of theta does not depend on theta, even only GC zero power is equal to zero.

674
01:27:06,608 --> 01:27:13,868
So all these facts are are very simple and we just don't go over the very quickly

675
01:27:13,868 --> 01:27:18,548
because they are needed for some of the examples that we're going to see next.

676
01:27:21,968 --> 01:27:32,258
And now we are going to try to find a minimal sufficient statistic for a few of these fusions.

677
01:27:32,828 --> 01:27:37,968
First, let's look at the uniform distribution, the uniform decision.

678
01:27:39,008 --> 01:27:42,338
I think actually this answers your question.

679
01:27:43,058 --> 01:27:46,238
So here we have uniform decision. We have just one.

680
01:27:46,268 --> 01:27:52,628
This a school or theta theta. And now we are trying to find a minimum sufficient strategy for theta.

681
01:27:52,648 --> 01:28:01,448
And we will see that this minimal civilian statistic has fully few dimensions higher than theta.

682
01:28:05,498 --> 01:28:17,798
So to find the minimal civilian a statistic. Now again, our only pool regarding women have also been a statistic is 06. 2.30, 13 or 43.

683
01:28:20,548 --> 01:28:28,058
So the number of the theorem. But anyway, if you go back to the theorem because live since the 13th.

684
01:28:29,708 --> 01:28:36,518
So the way to apply this theorem is because the theorem involves the ratio of you draw density.

685
01:28:37,658 --> 01:28:44,008
So we will start that. That's where we will start. That's our starting point.

686
01:28:44,458 --> 01:28:50,398
So we will look at the results of this to evaluate it at an X and evaluate that one.

687
01:28:54,118 --> 01:28:58,438
And then we will see when it has caused the function of theta.

688
01:29:00,958 --> 01:29:09,648
So we will need to start with the joint and the George does the in this case, he's just a product of individual speeds.

689
01:29:11,248 --> 01:29:15,508
We have already the individual PDF using any given function.

690
01:29:15,928 --> 01:29:26,128
So this is drawn to PDF. Now let's try to work out all the details.

691
01:29:32,658 --> 01:30:02,868
We are looking at the racial. So again, the first few steps we will need to apply to the properties of any get or vanishing.

692
01:30:03,158 --> 01:30:10,468
Just some algebra we need to. We are going to start in terms that helps us to identify when this ratio is a constant.

693
01:30:25,948 --> 01:30:34,468
Because we have the Prime Dog years, so we can write it as an indicator that all these exercises are between theta and theta plasma.

694
01:30:34,798 --> 01:30:40,378
And similar to the denominator is all of these y is probably true theta instead of plus one.

695
01:30:55,218 --> 01:31:03,258
And then now if we look at the numerator, then all of these x i's are between the theta and a fatal plus one.

696
01:31:03,768 --> 01:31:11,268
This is equivalent to saying that the minimum of this exercise needs to be larger than theta and the maximum.

697
01:31:11,268 --> 01:31:22,278
All of this exercise needs to be smaller than theta plus one. So theta then less than the minimum existing values and.

698
01:31:25,648 --> 01:31:30,438
And the largest one has to be less than half.

699
01:31:30,658 --> 01:31:34,767
What? And a similarity?

700
01:31:34,768 --> 01:31:38,458
The denominator split are less than one one.

701
01:31:39,008 --> 01:31:42,298
And you must have theta plus one.

702
01:31:50,808 --> 01:31:57,538
And then we can rewrite this in terms of love as an indicator for theater in itself.

703
01:31:57,908 --> 01:32:01,818
For theater. You know, again, let's focus on the top of the numerator.

704
01:32:01,828 --> 01:32:07,478
So say the less than well, first of all, we have theater it not excellent.

705
01:32:10,788 --> 01:32:14,148
And also we have theater must be larger than X in minus one.

706
01:32:14,628 --> 01:32:22,097
So it must be less than a logarithm axiom.

707
01:32:22,098 --> 01:32:28,538
That's one that's not. Numerator. And denominator similarity.

708
01:32:28,538 --> 01:32:32,098
We can write it as white in minus one less and fade out.

709
01:32:32,108 --> 01:32:44,308
That's not why. Okay.

710
01:32:47,188 --> 01:32:51,748
And now we need to see when this ratio is a constant of three.

711
01:32:53,248 --> 01:33:04,198
So this ratio is a constant. Is a constant in theta is a constant, Athena.

712
01:33:04,228 --> 01:33:15,928
In other words, it does not depend on theta. If and only if they using the one fact we just went over the top and bottom, they are both initiators.

713
01:33:16,258 --> 01:33:25,258
So in order for this region to be colored in yellow theta, now the two intervals we are looking at should be exactly the same.

714
01:33:26,138 --> 01:33:32,518
And so, in other words, even only if x, n is equal to Y.

715
01:33:35,588 --> 01:33:59,678
This one is equal to what one? And that, in other words, now, if we look at two statistics, it's one of the smallest and largest smile.

716
01:33:59,878 --> 01:34:09,268
And so, therefore, if we. Defy a statistic to be.

717
01:34:11,218 --> 01:34:18,537
It expands the smallest and hardest. And then this.

718
01:34:18,538 --> 01:34:35,828
Why should be? And many of us are hearing the statistics based on the previous zero.

719
01:34:35,838 --> 01:34:51,818
Right. The reason is that the theory says that while t here this t is minimal sufficient when well, under the condition that this ratio is constant.

720
01:34:52,058 --> 01:34:54,688
If and only if he x is equal.

721
01:34:54,848 --> 01:35:05,318
You are right now and we have shown that this is positive, even only if this tool is the same, which means t1at vaccine is equal to you.

722
01:35:06,158 --> 01:35:10,768
So that's that's the reason if you look at us, this is most of us a statistic.

723
01:35:12,038 --> 01:35:21,698
And this also answers the question that, well, this in either corresponds to the case where we have a single theta school or theta just one dimension.

724
01:35:22,088 --> 01:35:27,178
But the minimal sufficient statistic has to imagine we cannot further reduce that dimension.

725
01:35:27,398 --> 01:35:36,968
So in other words, whatever one dimensional studies do you construct is not sufficient, a statistic that's not so based.

726
01:35:39,038 --> 01:35:45,038
So in other words, whatever one dimensional studies you construct, it does not contain all that information.

727
01:35:45,038 --> 01:35:55,798
In fact, you lose some information. Okay.

728
01:35:55,798 --> 01:35:59,638
So that's one example. Let's take a look at another example.

729
01:36:00,508 --> 01:36:04,138
This example is also a universal decision.

730
01:36:05,758 --> 01:36:10,168
But now it's a uniform decision. From negative theta to theta.

731
01:36:12,208 --> 01:36:21,738
And we want to find a minimal sufficient statistic. Okay.

732
01:36:21,828 --> 01:36:29,528
Again, I mean, here we need to start with drawing a PDF because that's what we have, right?

733
01:36:29,538 --> 01:36:32,598
So and it's based on 06. 2.13.

734
01:36:32,838 --> 01:36:36,348
We need to look at again, the ratio of the to densities is a constant.

735
01:36:36,618 --> 01:36:42,238
So that's where we need to get started. So we look at the drawing, a PDF in this case the drawing.

736
01:36:42,238 --> 01:36:52,307
The PDF is just the product of the individual PDFs, not because we have uniform, this fusion uniform, this very so we have one over two theta,

737
01:36:52,308 --> 01:37:00,618
the length of the interval raised here in then the product that each x must fall between that two if they're not an audience data

738
01:37:00,888 --> 01:37:14,928
and this is the drawing pdf and now we need to find when the pdf the ratio of the PDF and x and Y when the ratio is a constant.

739
01:37:16,398 --> 01:37:20,628
So now let's again you need some, some algebra or some calculation.

740
01:38:05,538 --> 01:38:16,288
Okay. So now we can cancel apparently we can cancel this tour for guys and then for the for the entertainer.

741
01:38:17,028 --> 01:38:23,178
But I'm going to parking, so the elevator will simplify a little bit.

742
01:38:23,238 --> 01:38:30,228
So let's look at the top policies that all these add signs might as far be inactive, theta and theta,

743
01:38:30,978 --> 01:38:38,068
which means that the smallest must all be true 90%, and say that an alarm is must be two 1950 to have that.

744
01:38:39,138 --> 01:38:56,788
So this is that the same indicator as x, one and x and must fall must right?

745
01:38:59,718 --> 01:39:04,188
So x one must be larger than negative theta and actually must be smaller than theta.

746
01:39:29,088 --> 01:39:32,208
And a further simplification of the indicator.

747
01:39:32,448 --> 01:39:45,168
So the top again, if we look at the top left, have a sense that negative theta less than x one, that means theta must be larger than 90 x one.

748
01:39:48,218 --> 01:39:58,708
And also theta must be larger than x in. So that to me is theater must be larger than the maximum of this technology.

749
01:39:58,718 --> 01:40:06,368
So in other words, theater must be larger than the maximum of negative x one and x in.

750
01:40:11,598 --> 01:40:18,088
That's the part of the numerator. And similarly, the denominator is the same thing.

751
01:40:18,108 --> 01:40:25,098
Theta must be larger than the maximum between 1991 and life.

752
01:40:34,048 --> 01:40:39,718
Okay. Now, this is the ratio. This is the ratio of two indicators.

753
01:40:39,748 --> 01:40:43,948
Now, let's take a look at when this ratio is constant in theta.

754
01:40:44,278 --> 01:41:00,688
So this is a constant if and only if we have to integrate our functions both on top and bottom, then even only the two intervals are the same.

755
01:41:02,158 --> 01:41:12,928
Even only maximum negative x one an x n is equal to maximum negative y one.

756
01:41:13,558 --> 01:41:30,107
And what in? And after here, after reaching here, then if we just consider this statistic,

757
01:41:30,108 --> 01:41:41,628
if we consider t x to be the maximum of negative of the smallest utter statistic and of the largest order,

758
01:41:41,628 --> 01:41:54,348
statistically, if we consider t to be this high, then what we have shown is that the ratio is constant,

759
01:41:54,978 --> 01:42:02,958
even only if t x is equal to t, y, i, t and x is equal to one.

760
01:42:04,698 --> 01:42:12,978
So this implies that based on what the burn from.

761
01:42:34,208 --> 01:42:37,268
Is a minimum more sufficient still hurting for data.

762
01:42:44,018 --> 01:42:51,518
So in this case, indeed, we can see that, well, the minimal surveillance state is the has the same dimension as theta, the minimal.

763
01:42:51,518 --> 01:42:58,568
So it is also scalar, but it does have kind of not so straightforward fought.

764
01:42:58,568 --> 01:43:10,988
Right. Very not not super intuitive for. So we found this based on some calculation based on some somehow looking.

765
01:43:11,008 --> 01:43:20,497
So any quick questions? And if not, then we will have to stop your arm.

766
01:43:20,498 --> 01:43:23,608
You are you are not going to.

