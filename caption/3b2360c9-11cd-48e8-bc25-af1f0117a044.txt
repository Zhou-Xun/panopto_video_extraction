1
00:00:25,500 --> 00:00:31,860
You're bursting. This month.

2
00:00:31,890 --> 00:00:38,300
It was on the second. So. If the August birthday was your.

3
00:00:41,560 --> 00:00:44,620
In classification. Yeah.

4
00:00:44,830 --> 00:00:51,370
Near. Finally.

5
00:00:51,370 --> 00:00:54,760
Thanks for the birthday of We Can Have a cupcake.

6
00:00:58,250 --> 00:01:01,540
And then months from last September, I.

7
00:01:04,970 --> 00:01:10,730
Last September cupcakes. They were like this much like this much crashing into the atmosphere with this much.

8
00:01:11,600 --> 00:01:16,249
There were gigantic. Our group our group academy, our cohort were like, be careful what you eat.

9
00:01:16,250 --> 00:01:19,520
The cupcake is like too much. You might feel bad if you eat.

10
00:01:20,120 --> 00:01:27,770
What? Oh.

11
00:01:29,490 --> 00:01:38,760
Oh, and sometimes I wish I had those for them. Yeah. Oh, yeah.

12
00:01:40,050 --> 00:01:55,170
Yeah. Oh. So.

13
00:02:45,210 --> 00:02:56,310
Let's get started. When someone asks the question about a scheduled midterm I did went back to I do go back to look at my calendar.

14
00:02:56,310 --> 00:03:03,420
I found that I have this h study panel on October 26 and 27.

15
00:03:04,350 --> 00:03:10,590
Technically that I cannot teach because it's a two day study panel.

16
00:03:10,860 --> 00:03:14,110
There are over 100 proposals to evaluate.

17
00:03:14,700 --> 00:03:25,390
So, so I mean, this sh2 is socio environment determined house full house study panel.

18
00:03:25,460 --> 00:03:33,070
So anyway, so, so I have to use this opportunity to schedule midterm.

19
00:03:33,090 --> 00:03:36,450
I can check, you know, your emails.

20
00:03:36,750 --> 00:03:44,550
This is a take home midterm, but I just cannot come to teach you at a certain time without participating in discussion.

21
00:03:45,690 --> 00:03:50,040
So anyway, I will use this Thursday for the midterm.

22
00:03:50,040 --> 00:03:53,840
That's a perfect sort of time for me to do this.

23
00:03:54,040 --> 00:03:57,810
And so, yeah, that's the announcement I made. Yeah.

24
00:03:58,110 --> 00:04:05,429
So this is like a poll. Like, do we have the I can give you the homework of the homework the midterm on 830

25
00:04:05,430 --> 00:04:09,910
in the morning and then collect enough enough to know five and we're next.

26
00:04:10,050 --> 00:04:15,930
I don't want to be denied Thursday, so maybe collect that at 6 p.m. or something like that.

27
00:04:16,710 --> 00:04:21,780
Or if you need a longer time to do some research, I feel like.

28
00:04:24,680 --> 00:04:29,090
Three lectures on Wednesday. This is Thursday. This Thursday.

29
00:04:29,720 --> 00:04:35,560
Okay. I have. I lost three glasses.

30
00:04:35,990 --> 00:04:39,520
I just. I didn't know that. Oh, right, right, right, right, right.

31
00:04:40,030 --> 00:04:43,780
So you're in. The other instructors have no knowledge panels.

32
00:04:43,780 --> 00:04:50,030
And so. So what do you want to do?

33
00:04:51,050 --> 00:04:56,160
I give you a 30, but click later the next day or something like that.

34
00:04:56,330 --> 00:05:04,580
You know, like our or for like I mean, depends on how fast or how much you want to Google, right?

35
00:05:05,180 --> 00:05:08,360
So that is an average.

36
00:05:08,380 --> 00:05:11,720
It needed to do that in novels and stuff like that.

37
00:05:11,720 --> 00:05:19,820
Maybe takes 3 hours to complete. But if you want to Google a lot and probably you take a little bit more than that.

38
00:05:20,000 --> 00:05:24,230
So maybe I click next morning like 830 Friday morning.

39
00:05:24,770 --> 00:05:33,440
Yeah. Yeah. Okay. So so you can I can create this submission function in the summit over there.

40
00:05:40,030 --> 00:05:43,860
Okay. What am I going to do?

41
00:05:45,930 --> 00:05:54,930
Oh. So. What are the other options like?

42
00:05:55,680 --> 00:05:59,400
I don't know. Like, maybe we can discuss that later.

43
00:05:59,820 --> 00:06:19,950
Yeah. Another thing I want to talk about is that we have six groups and and I want the first group to jump in to do a little bit data capture,

44
00:06:20,730 --> 00:06:23,959
because I said that there's 5% of what they capture.

45
00:06:23,960 --> 00:06:32,860
Right. So one thing I read your I, I will return your homework one next week before I almost stop.

46
00:06:32,860 --> 00:06:39,110
But some talk about like vaccine hesitancy, right?

47
00:06:39,150 --> 00:06:42,240
So I talk about some of the environmental,

48
00:06:42,240 --> 00:06:47,520
social and things and but I thought that could be a very interesting project

49
00:06:47,520 --> 00:06:53,390
because I work with a couple of people from UC Davis in the past couple of years,

50
00:06:53,430 --> 00:07:05,159
how to analyze the presidential election data and how to use pollsters to forecast, Oh, what?

51
00:07:05,160 --> 00:07:15,930
Who is the winner about? And turns out to be a very interesting analysis of the paper has been revised twice for Amazon applied statistics.

52
00:07:16,410 --> 00:07:29,810
So I know there is a data about country level of 2020, so potentially election data because if you open the data, right, the link.

53
00:07:30,630 --> 00:07:39,540
But anyway, this is where you can find the breakdown of what's the percentage of residents in a county voting for

54
00:07:39,870 --> 00:07:46,740
Democratic presidential candidate and who what's the percentage of the Republican candidates essentially,

55
00:07:47,760 --> 00:07:53,870
so that you can see how this composition of of the political point of views or

56
00:07:54,030 --> 00:08:02,040
their or party affiliations would affect the sort of the vaccination hesitance.

57
00:08:02,040 --> 00:08:15,570
Right. So so we know that there's quite a bit hegerty narratives in terms of how how people of how much trust people will have of the effect.

58
00:08:18,070 --> 00:08:22,690
It's a nation to protect. So, you know, people take a very different point of view.

59
00:08:22,750 --> 00:08:28,030
But in the same time, people believe that this is somewhat related to their political point.

60
00:08:28,030 --> 00:08:31,300
But who knows? But that's something we can figure out. Right.

61
00:08:31,810 --> 00:08:36,760
So we have the vaccination data. You see home birth number two.

62
00:08:36,850 --> 00:08:42,520
Now, I give you a new data like this. So I will assign one group of this to country update.

63
00:08:42,520 --> 00:08:47,050
I put that as part of this 5% of your data capture.

64
00:08:47,650 --> 00:08:51,309
And we could capture more data from,

65
00:08:51,310 --> 00:09:02,080
like the prime agriculture to see the urban and the rural population and unemployment rate and so

66
00:09:02,080 --> 00:09:10,210
on as part of the other type of data that we can put into this sort of model for the analysis.

67
00:09:10,720 --> 00:09:17,140
Okay. So now we only have the vaccine data and demographic data like sex age and so on,

68
00:09:17,140 --> 00:09:24,280
but we can add more environmental variables and social variables like this, political.

69
00:09:24,520 --> 00:09:33,520
So the point of view variables and as many other variables that we can, you know, analyze the data in a more interesting way.

70
00:09:33,700 --> 00:09:37,560
Okay. So that's the one I put down there.

71
00:09:37,570 --> 00:09:41,380
And so essentially what you're trying to do here is that.

72
00:09:43,400 --> 00:09:46,710
Yeah, but over time.

73
00:09:47,000 --> 00:09:51,650
What have you observed then is it could be the number of confirmed cases or not.

74
00:09:54,480 --> 00:10:03,930
That state of brutality data is more cost to us to fall because of the underreporting issue and could be, you know, the count.

75
00:10:03,930 --> 00:10:07,919
And now you have, you know, something going on in the population.

76
00:10:07,920 --> 00:10:21,950
Either, you know, it's the only gas prices or we just made up a lot of other covariance right that our time during and you want to do us so.

77
00:10:26,100 --> 00:10:34,230
One too many times for it's on the line. And for example, this could be the coverage of vaccination and so on.

78
00:10:34,290 --> 00:10:39,300
So you're doing a sort of the correlation of two times for this data.

79
00:10:39,450 --> 00:10:44,380
Of course, this can be extended to spatial. Yeah.

80
00:10:45,720 --> 00:10:52,480
Or I could drive you out. Perfect location. So so you can think about that.

81
00:10:52,630 --> 00:11:01,300
And there's another layer of complicity where you have of both parties with the country,

82
00:11:01,300 --> 00:11:07,540
and then you can bring that kind of spatial genealogy as part of it.

83
00:11:08,050 --> 00:11:17,150
That's the second part of my lecture I will cover. How to do spatial at this moment are looking at tempo data.

84
00:11:17,950 --> 00:11:23,839
Okay. So. It's something like we can do.

85
00:11:23,840 --> 00:11:30,620
And and so see how this political point of view or their affiliation to party could or

86
00:11:30,620 --> 00:11:40,640
their vote result can be related to vaccination hesitancy or some of the this situation.

87
00:11:40,850 --> 00:11:42,770
Who knows? We will look at it.

88
00:11:48,250 --> 00:11:58,090
So above this proportions, as I said, that when we run this kind of analysis instead of looking at the absolute number of.

89
00:12:02,980 --> 00:12:11,500
That's where a number of confirmed cases that we can model the prevalence or incidence rate where we basically work on the proportion data.

90
00:12:11,710 --> 00:12:23,140
Okay. So you sort of scale the number of incidents by the population size so that you come up with a sort of proportion as your observed data.

91
00:12:23,710 --> 00:12:26,990
So I recovered this data distribution.

92
00:12:27,040 --> 00:12:33,800
I said that it is a very popular distribution people use to model proportion to proportional data.

93
00:12:33,820 --> 00:12:37,030
Right. But the one thing that people don't like,

94
00:12:37,030 --> 00:12:45,520
a person that I don't like is that the data distribution is not a dispersed model, is now that in their family of zero,

95
00:12:45,610 --> 00:12:53,350
it's not really a generalized newer model because the normalized and constant depend on meaning and dispersion parameter,

96
00:12:53,590 --> 00:13:02,260
which is some kind of complicity or numerical instability where you do this numerical calculation for maximum like of estimate.

97
00:13:04,540 --> 00:13:12,370
So so that there's an alternative to the beta distribution is called simplex distribution.

98
00:13:13,570 --> 00:13:20,210
This is the one that, you know, in the in the family of the generalized in your model.

99
00:13:20,230 --> 00:13:27,060
Okay. So the simplex distribution is denoted by s o subscript dash.

100
00:13:27,370 --> 00:13:30,429
There are many version of the simplex distribution.

101
00:13:30,430 --> 00:13:40,360
This is the one that I use, which is very much the, you know, the distribution.

102
00:13:40,360 --> 00:13:49,630
We need to handle the proportion of data. It has this exact density function that looks probably the first time you see this distribution.

103
00:13:51,530 --> 00:13:55,580
And so you have the Sigma Square, which is the dispersion parameter.

104
00:13:55,730 --> 00:14:01,490
And then you have this explicit close form expression of the density.

105
00:14:01,880 --> 00:14:06,140
Okay. So the defense function here is a text.

106
00:14:06,140 --> 00:14:14,890
This form is, as I said, that is a proportional distribution for proportional random variable so that I must be between zero one.

107
00:14:14,960 --> 00:14:19,440
Okay. So. So it looks like this.

108
00:14:19,440 --> 00:14:22,620
And then the meaning of why is business here or what?

109
00:14:22,750 --> 00:14:25,380
Okay, so that's the the meaning. Okay.

110
00:14:25,710 --> 00:14:33,390
So you can see that in this expression, the normalizing constant here doesn't depend on mu, only depends on Sigma Square.

111
00:14:33,750 --> 00:14:39,090
So you they simply distribution has the property of likelihood of of the novelty.

112
00:14:39,100 --> 00:14:43,580
So it will give you some kind of numerical stability.

113
00:14:43,590 --> 00:14:47,130
Yeah. So this was like. Yeah.

114
00:14:48,030 --> 00:14:52,250
Yeah. Is it related to simplex? If you look at three dimension, right.

115
00:14:54,090 --> 00:15:04,470
Oh, this will give you this. So you have this structure like simplex structure because it is essentially this is zero for compositional data.

116
00:15:05,250 --> 00:15:12,810
So so this is really in simplex because they are text mining in the text mining you.

117
00:15:14,050 --> 00:15:17,440
So simplex structurally more like in this topic.

118
00:15:17,440 --> 00:15:32,080
More fully remodeled. Uh. Oh, so you tax mining when you give a piece of article which has too many words there.

119
00:15:32,310 --> 00:15:41,830
But we will see what kind of topics write or some of the ideas presented by this piece of, you know, article.

120
00:15:42,190 --> 00:15:49,030
So sometimes you will work on this sort of you'll have total number of words like say you have 1000 words.

121
00:15:49,450 --> 00:15:59,530
How the words are distributed or are displaying a compositional fashion is basically percent that this simple structure.

122
00:15:59,920 --> 00:16:05,110
So if you if you look at the text mining, you need sort of the context of topic modeling.

123
00:16:05,470 --> 00:16:07,830
The always present is the simple structure.

124
00:16:07,840 --> 00:16:18,040
Okay, so sometimes this grouping is a distribution that gives you this sort of distribution and the nodes on the simplex structure.

125
00:16:18,160 --> 00:16:27,250
But if you haven't, you know, only a two variable like a, you know, the Y was the one dimensional case, right?

126
00:16:27,250 --> 00:16:34,740
So that you basically have a proportion, right? Either you are in, in fact the case or not if you factor cases.

127
00:16:34,840 --> 00:16:39,400
Right. So, so in for example, if you look at a vaccination, right.

128
00:16:40,030 --> 00:16:44,559
So if you look at a vaccination that you could have simply get into this sort of

129
00:16:44,560 --> 00:16:50,139
simplex structure because you have people have never received a vaccination.

130
00:16:50,140 --> 00:16:55,180
You have people receive full vaccination. People will only save one dose of vaccination.

131
00:16:55,450 --> 00:17:01,120
So you have this multi structure and new proportions so you can have.

132
00:17:03,990 --> 00:17:07,140
Very different proportions that always equal to one.

133
00:17:08,550 --> 00:17:11,550
So you have a key dimension in this case will be three dimensional.

134
00:17:11,700 --> 00:17:18,540
Right. So so that this forms a, you know, this simplex structure.

135
00:17:18,930 --> 00:17:23,580
But in this case, I only have two cases either, you know, this will not.

136
00:17:23,730 --> 00:17:27,060
So you have this sort of better distribution situation.

137
00:17:28,060 --> 00:17:32,260
Oh. So that's basically the name of this distribution.

138
00:17:33,640 --> 00:17:44,380
So this is a dispersion model and the normalizing constant doesn't depend on much, which is really a advantage in comparison to beta distribution.

139
00:17:45,010 --> 00:17:58,000
So we ask them the new parameter if they are regression models is because you are going to look at that logit think function and transpose, you know.

140
00:17:58,750 --> 00:18:06,460
So beta is the, the regression coefficients that you would like to estimate in your model that load your model for this new,

141
00:18:07,600 --> 00:18:16,550
then you don't need to really worried about estimation of dispersion parameter because of this likelihood of outcome now.

142
00:18:17,150 --> 00:18:24,490
Okay. So here is the the various function of the simplex distribution is sigma squared,

143
00:18:24,490 --> 00:18:30,790
which is dispersion parameter times this form new to the cubic one must mean to cube.

144
00:18:31,030 --> 00:18:39,510
Okay. So this is the very nice distribution, but not very well known to many people.

145
00:18:39,520 --> 00:18:44,430
You practice, you will always use beta. Run a simplex.

146
00:18:44,700 --> 00:18:48,400
So here is the shape of the the simplex distribution.

147
00:18:48,420 --> 00:18:55,710
Okay. So if you assign the setting of your mule and Sigma Square.

148
00:18:56,130 --> 00:18:59,730
Most of the time you get the union model distribution. Okay.

149
00:18:59,970 --> 00:19:03,010
So either skewed to the left or skewed to the right.

150
00:19:03,030 --> 00:19:11,970
This is very much like your methylation data. So you very like you are of have low methylation where you have high methylation.

151
00:19:12,540 --> 00:19:19,190
And anyway, sometimes you have this sort of bimodal situation where the mass solution were,

152
00:19:19,590 --> 00:19:26,550
you know, somehow distributed according to something very close to the left end or to right ends.

153
00:19:26,910 --> 00:19:30,510
So this is if you look at my situation, that looks very much like this.

154
00:19:31,080 --> 00:19:34,400
So sometimes I like the distribution shape.

155
00:19:34,560 --> 00:19:40,560
Sometimes I think that maybe simplex this would be can model methylation do that better than be that distribution.

156
00:19:41,370 --> 00:19:46,050
So you can see that we you reduce the dispersion parameter.

157
00:19:46,140 --> 00:19:53,970
Okay. So from like six, four, six, four and one, the distribution looks more and more like normal.

158
00:19:54,480 --> 00:19:59,700
Right. So this distribution becomes more about shaped and symmetric around the mean.

159
00:20:01,080 --> 00:20:05,450
So. So this is called small dispersion loss in politics.

160
00:20:05,530 --> 00:20:10,020
Right. So I, I talk about this in the.

161
00:20:13,040 --> 00:20:17,780
I read cases. So this is sort of so-called small dispersing of some parts.

162
00:20:28,480 --> 00:20:36,310
Well basically the the density function in this case as you supply and you.

163
00:20:38,140 --> 00:20:45,580
We're a convert to normal distribution when when the dispersion parameter goes to zero.

164
00:20:46,030 --> 00:20:50,830
Okay. What? What dispersant are some products?

165
00:20:51,520 --> 00:21:03,489
One thing I mention is that you look at how close devgn's function to the Fisher residual sort of of Pearson residuals or how

166
00:21:03,490 --> 00:21:14,020
the defense receipts you how close the defense research you is to the piercing procedure because a person's residual is with.

167
00:21:15,270 --> 00:21:25,980
To the normal distribution. Yara We're talking about events each year, and so this is a new normalization of the defense function.

168
00:21:26,400 --> 00:21:38,340
So people are thinking like how close this to this? And smartest person could could I give you some sort of evidence in terms of this approximation?

169
00:21:40,300 --> 00:21:47,170
So so this is not new because I and I also talk about the fossil distribution, right?

170
00:21:48,010 --> 00:21:53,250
This goes to normal distribution when milk goes to infinity.

171
00:21:53,260 --> 00:22:01,120
So when fossil possum milk parameter becomes too much or larger, an impossible distribution becomes more and more like a normal distribution.

172
00:22:01,510 --> 00:22:10,180
So similar situation happens before the durum distribution when dispersion primer goes more, you know.

173
00:22:11,880 --> 00:22:16,950
Decreasing to zero, then dissolution looks more. More exactly like what you see here.

174
00:22:17,760 --> 00:22:26,250
Right. So if you if you decrease the dispersion parameter to something smaller and then that distribution becomes more, more like normal.

175
00:22:26,590 --> 00:22:31,650
Okay. So this is some property we know that in.

176
00:22:37,310 --> 00:22:40,520
So I worked quite a bit on the simplex distribution.

177
00:22:41,150 --> 00:22:51,440
As I say that this distribution was proposed by my own advisor and he signed advisor bundle from there and only found enough nursing.

178
00:22:51,890 --> 00:22:58,900
So, so I was proposed by, you know, basically this pattern.

179
00:23:00,120 --> 00:23:06,210
I get them apparently. And so they work all this distribution with no application, actually.

180
00:23:06,540 --> 00:23:18,660
So I was trying to see work. I mean, driven mostly by the curiosity, how much extra stress and convenience that that the simplex distribution would,

181
00:23:19,170 --> 00:23:23,130
you know, offered in addition to what being the distribution had.

182
00:23:23,310 --> 00:23:27,360
So that was mostly about the, the curiosity.

183
00:23:27,870 --> 00:23:35,999
So I work on this and the first of these maybe ten and we had the parameters paper.

184
00:23:36,000 --> 00:23:46,170
We see how this sort of simplified distribution can be applied to study longitudinal data, longitude and proportional data.

185
00:23:46,740 --> 00:23:57,930
So Dr. Tao would say, Oh, I'm not sure about statistical prom at Georgetown University,

186
00:23:58,260 --> 00:24:05,639
but at that time he was a senior biostatistician at the Cleveland Clinic Foundation in Cleveland.

187
00:24:05,640 --> 00:24:11,610
And they have this eye surgery data from ophthalmology where they want to really

188
00:24:11,610 --> 00:24:18,900
look at how fast this sort of kind of recovery healing from the eye surgery.

189
00:24:19,230 --> 00:24:25,290
And they measure very, very interestingly, a proportional, longitudinal proportional data.

190
00:24:26,220 --> 00:24:30,990
And then then when we met and he asked me how to analyze data.

191
00:24:30,990 --> 00:24:36,070
Right. And I use normal this year, they use that typical normal distribution to analyze data.

192
00:24:36,090 --> 00:24:43,620
There was no power. But clearly that the data has kind of shape that it's not really super for normal.

193
00:24:44,580 --> 00:24:47,580
I said, I don't know what it is called. Simplest distribution.

194
00:24:48,300 --> 00:24:56,810
Of course I can do better distribution for TV, but at that time I said I have a simplex distribution that we can give a try.

195
00:24:56,820 --> 00:25:04,770
Then we just built up the generalized estimating equation for longitude proportional data using the simplex distribution.

196
00:25:04,770 --> 00:25:08,190
And we did find that very interesting, you know,

197
00:25:08,190 --> 00:25:15,960
this sort of healing rate for after patients received eye surgery and that was the the first paper then.

198
00:25:16,590 --> 00:25:25,440
So my first Q when he started work with me, I said, Hey, we need to really do something a little bit more and run around doing the,

199
00:25:26,250 --> 00:25:38,790
you know, the type of modeling for a longitudinal proportion that I advised the first author and to do this mix effect.

200
00:25:40,920 --> 00:25:46,340
So she was part of this sort of conversation.

201
00:25:46,350 --> 00:25:50,520
And she only who was a faculty in this department has always been at Harvard.

202
00:25:50,880 --> 00:25:57,390
So so we are in this conversation how to develop this generalized need to make small talk, not repeat.

203
00:26:01,300 --> 00:26:06,960
And my first Ph.D. student work on this kind of ran effects model.

204
00:26:06,970 --> 00:26:11,650
He now is a senior pastor in the Alberta Cancer Board in Canada.

205
00:26:11,650 --> 00:26:21,190
But anyway, he worked on this quite a bit. One student there for using ran effects model so so finally that.

206
00:26:23,840 --> 00:26:32,569
You know, folks, 80 years ago, I encourage I had my first article when I was junior faculty.

207
00:26:32,570 --> 00:26:36,290
Of course, I got kids in 1986, four years after my piece.

208
00:26:36,450 --> 00:26:44,240
I certainly wrote my article myself first while I was not really ready for our package.

209
00:26:44,780 --> 00:26:50,930
So but when you start to work on ran effect small, I say we should come by to hold together.

210
00:26:50,940 --> 00:26:58,579
And so to create a package. We've been talking about this all the time until about eight or eight years ago or so.

211
00:26:58,580 --> 00:27:08,510
We create our package simplex req. So this is like quite a popular package nowadays to nice proportional data,

212
00:27:08,840 --> 00:27:14,540
particularly the particularly when the of resolution data become available.

213
00:27:14,540 --> 00:27:21,340
So a lot of people want to analyze methylation data, use this package, defeating this proposal.

214
00:27:21,650 --> 00:27:25,560
So they package allows you to do typical jiram.

215
00:27:25,760 --> 00:27:29,570
So cross-sectional data Rotterdam analyzing repeat measurement.

216
00:27:30,170 --> 00:27:36,910
Well this package allow you to do this gram like generalize a new model full cross-section of data

217
00:27:36,920 --> 00:27:44,480
basically 651 type of analysis where y is proportion now you're taking 653 repeated measurement.

218
00:27:44,600 --> 00:27:51,800
So they package also allow you to do longitudinal proportion of data using either G or ran effects model.

219
00:27:53,670 --> 00:28:00,280
So yeah. So that's a very interesting journey working on this proportion of data.

220
00:28:00,300 --> 00:28:06,550
I didn't realize that. Of course, I did not see that nowadays.

221
00:28:06,600 --> 00:28:19,080
Right. So so methylation, that becomes a very important type of, you know, omics data, people measured to, you know, essentially proportional data.

222
00:28:19,440 --> 00:28:22,830
Now you have microbiome data, which is the simple data.

223
00:28:22,890 --> 00:28:24,240
So the compositional data,

224
00:28:24,720 --> 00:28:36,700
so that before this packet you will only work on this proportion that we did not have a development in to simple data like compositional data.

225
00:28:36,720 --> 00:28:40,910
That's certainly something worth a future investigation.

226
00:28:40,920 --> 00:28:45,600
And there's a lot of organization in Europe. People work on it.

227
00:28:45,600 --> 00:29:00,480
So compositional data, they have annual meeting somewhere each year to really discuss the events statistics method and to analyze compositional data.

228
00:29:01,290 --> 00:29:09,660
Yeah. So because of this, with the lot of investigation, what's the difference between the distribution as simply distribution?

229
00:29:10,020 --> 00:29:15,030
The way they've actually used the distribution is such a popular distribution.

230
00:29:15,210 --> 00:29:21,540
Everybody uses why we need to convince people to use a different distribution, right?

231
00:29:22,920 --> 00:29:33,360
So we did to make a common that so simplex distribution this packaged simple simplex is not to more due to the poverty of

232
00:29:33,360 --> 00:29:40,230
likelihood of the inability to understand the dispersion parameter can be seen separately from the regression parameter.

233
00:29:40,590 --> 00:29:43,590
So. So this advantage is really something.

234
00:29:46,980 --> 00:29:53,800
Yeah you lot of data analysis with in violation of human technology.

235
00:29:53,820 --> 00:30:01,640
Right. So in the settlement we see that the dispersion parameter.

236
00:30:03,130 --> 00:30:09,580
Because of that, your data is dispersed and parameter times you write this.

237
00:30:09,580 --> 00:30:13,000
This is the actually the M type of thing.

238
00:30:13,000 --> 00:30:19,840
Every year G our model has this right. So the dispersion parameter times, the variance function normal.

239
00:30:19,840 --> 00:30:24,230
Normal. Right, normal. Errands.

240
00:30:24,240 --> 00:30:28,710
Function of various function. Various function is one.

241
00:30:30,160 --> 00:30:34,690
Also write is new karma is mule square.

242
00:30:35,440 --> 00:30:39,390
Negative binomial is new. And so I'm quite small.

243
00:30:41,200 --> 00:30:45,280
Binomial is new times, one minus new.

244
00:30:46,870 --> 00:30:50,340
Right now you have this simplex and you have more.

245
00:30:51,100 --> 00:30:58,540
And you have moved cubit or minus New Cubit, or if you have numerous Gaussian, you have you covered.

246
00:30:59,830 --> 00:31:07,660
So you have all the different models that you have specific distributions depends on what kind of distribution that you use.

247
00:31:07,960 --> 00:31:21,500
My point here. I've loved your link.

248
00:31:21,500 --> 00:31:27,800
You have lovely and you have whatever you feel comfortable to model your generalized model.

249
00:31:28,310 --> 00:31:33,440
But people never work out this we all talk about over dispersing everything.

250
00:31:34,280 --> 00:31:42,169
But you know, there are some people working on modeling of this as a function of your course because people believe

251
00:31:42,170 --> 00:31:49,220
that the variability is not only controlled by the wheel but also controlled by the sickness wear.

252
00:31:49,370 --> 00:31:52,550
Okay, so. So some people have a model.

253
00:31:54,970 --> 00:32:09,940
On the dispersion properties. Well, okay. So Gordon Smith from Australia now, he's very well known in the bioinformatics or human genetics.

254
00:32:09,940 --> 00:32:18,009
Gordon Smith is in the one hospital in Melbourne and he worked quite a bit when he was young.

255
00:32:18,010 --> 00:32:21,520
He worked on a bit on the volume dispersion parameter.

256
00:32:23,200 --> 00:32:27,250
So if you are in, you're very interesting on that.

257
00:32:27,550 --> 00:32:34,990
Okay. Then dispersed model simply smaller distribution certainly US advantage because we do estimation this to powder

258
00:32:34,990 --> 00:32:41,170
separately estimate it you disputing distribution this they are all needed to be estimated together right.

259
00:32:41,650 --> 00:32:47,020
So this you have this component complicity of this to regression model if you have not

260
00:32:47,020 --> 00:32:53,290
or model like Z transforms are formed to specify how you model dispersion parameter.

261
00:32:53,290 --> 00:32:58,899
Then you have RFA and be the sort of together to deal with in the beta distribution.

262
00:32:58,900 --> 00:33:06,400
But you don't have this issue in the simply because these two factors are likelihood of satisfy

263
00:33:06,410 --> 00:33:12,670
the likelihood also denial to that you can really just do them separately and you do not do not.

264
00:33:14,150 --> 00:33:18,830
Lose the power and and efficiency.

265
00:33:19,360 --> 00:33:25,540
So Gordon Smith and work on quite a bit the modeling that this version important is young.

266
00:33:26,200 --> 00:33:35,000
Yeah I can see that when you have this someplace you might offer some kind of flexibility and and freedom to add more.

267
00:33:35,660 --> 00:33:42,010
So modeling to the second moment of your distribution when you see some hits were scattered

268
00:33:42,500 --> 00:33:48,830
in the data or are you one model of the over dispersion are to explain data better.

269
00:33:49,940 --> 00:33:56,360
So so that's one advantage that you can see in the use of simplex distribution.

270
00:33:57,260 --> 00:34:03,380
So better distribution is linked to the gamma distribution and simplex distribution linked to inverse Gaussian.

271
00:34:04,490 --> 00:34:15,620
Okay. So that's. So one time I had a conversation with D.R Cox and he told me that inverse golf

272
00:34:15,920 --> 00:34:26,329
has been sort of ignored by the community a lot in the many literary people,

273
00:34:26,330 --> 00:34:31,340
folks, a lot of karma disputing because this is really the Chi Square distribution and so on.

274
00:34:31,970 --> 00:34:38,100
But he believes that inverse Gaussian distribution is not one that should be seriously misguided.

275
00:34:38,120 --> 00:34:46,340
But so many people were counting numerous thousand distribution, but that offers a different type of thing.

276
00:34:47,450 --> 00:34:53,490
So that's right. Skewed distribution and. Okay.

277
00:34:53,520 --> 00:35:00,010
So extension to the compositional data is interesting, but we haven't done that.

278
00:35:00,540 --> 00:35:07,470
I don't know how people can develop this stuff in the Michael Bond detached homes early from UPN.

279
00:35:08,070 --> 00:35:14,129
And I had some conversation about this possible work because when he started working on the Michael Bond data,

280
00:35:14,130 --> 00:35:18,540
like ten years ago or so, he approached me and said,

281
00:35:18,810 --> 00:35:26,280
if you look at this sort of this simplex distribution work on and we had some discussion about the compositional data,

282
00:35:26,730 --> 00:35:37,230
but later on he'd work on a lot of it based on the beta distribution and the racial distribution rather than the simplex distribution.

283
00:35:37,240 --> 00:35:46,650
But based on my experience that if we can work out some stuff in the composition of data for using the simplex distribution,

284
00:35:47,010 --> 00:35:50,670
it could be a, you know, alternative toolbox.

285
00:35:53,120 --> 00:36:01,190
And. So that's sort of something we work on.

286
00:36:01,370 --> 00:36:13,580
We did a lot of simulation and you can find our sort of published papers and in general feel that some flexible space is slightly better and better.

287
00:36:14,990 --> 00:36:21,920
And so we believe a lot. So but that's just the way we discover.

288
00:36:23,510 --> 00:36:27,710
Like it, if you will use it, certainly you can use it.

289
00:36:27,720 --> 00:36:37,840
And so that's what we do. So far.

290
00:36:37,840 --> 00:36:42,310
I talk about this space model, this stationary late in process.

291
00:36:43,370 --> 00:36:50,200
Okay. So back to. So calm structure.

292
00:36:54,840 --> 00:36:57,880
Right. Okay.

293
00:36:58,900 --> 00:37:05,920
Calm structure. Right. So remember that I have this state space model, so I have my leading process,

294
00:37:05,920 --> 00:37:14,079
see the T which describe general population level infection and now I have my

295
00:37:14,080 --> 00:37:19,320
emission probability distribution to generate theta of either number of deaths.

296
00:37:19,480 --> 00:37:22,930
This can be factor number. That's number of confirmed cases.

297
00:37:23,350 --> 00:37:31,270
Now, I filled out a mock of answers. Well, this ladies and.

298
00:37:32,950 --> 00:37:40,479
But this process, because I believe that what happens today must be really that what happened yesterday at the population of all that,

299
00:37:40,480 --> 00:37:50,470
I captured the data from my surveillance system and then I'm trying to figure out what's going on in the population level.

300
00:37:50,620 --> 00:38:00,190
Okay. Which parameters will which covers that really are factors, you know, the population evolution of the infectious disease.

301
00:38:00,700 --> 00:38:01,930
That's something I'm trying to do.

302
00:38:02,530 --> 00:38:12,460
But in order to answer that question, we need to really put some kind of mathematical model words, process or structure on the rating process.

303
00:38:12,820 --> 00:38:20,140
So far, I put, you know, there the station or places like it's these are T of BTC, the T minus.

304
00:38:22,690 --> 00:38:27,900
You're. So this follows karma distribution.

305
00:38:28,380 --> 00:38:38,340
And then this has, you know, this payroll distribution thing so that you can generate that the the stationary are one process stationary.

306
00:38:41,310 --> 00:38:46,860
You know, a long process, you know, with this this sort of the exponential decay.

307
00:38:50,580 --> 00:38:53,850
Correlation function. That's that's basically what you're trying to model.

308
00:38:54,240 --> 00:39:04,950
But people may say that y y you want to you know, of folks are only at station are to process this in process can be no stationary.

309
00:39:05,640 --> 00:39:16,950
Okay so the in in this small context of this whole thing can be generalized to another station or process.

310
00:39:16,950 --> 00:39:22,140
Then this is the slides that just talk about how this can be done.

311
00:39:22,170 --> 00:39:29,760
Okay. So basically I'm talking about now station erm t on the base in parentheses to make it more flexible.

312
00:39:31,920 --> 00:39:37,020
Well I work on this non stationary process for this air pollution thing.

313
00:39:37,230 --> 00:39:40,830
This is a hard data I work on.

314
00:39:40,910 --> 00:39:47,010
I a huge when people see each hard data, I said okay, I work on this data.

315
00:39:47,010 --> 00:39:50,450
What was the p g like 30 years ago.

316
00:39:50,460 --> 00:40:01,670
Okay. Anyway, so. So what we want to do here is like the whole purpose here is really just want to estimate the Association to time series.

317
00:40:01,680 --> 00:40:08,220
Right. So you have observed the time stress in my case is number of hospital admissions.

318
00:40:08,570 --> 00:40:16,620
So for particular disease and you look at the number of hospitalizations for emergency room visit.

319
00:40:20,160 --> 00:40:27,390
When tempers change or something, government change that people tend to have, you know, that triggers their attacks to their,

320
00:40:27,920 --> 00:40:35,070
you know, respiratory system that they need to go to hospital to have some emergency treatment on their.

321
00:40:36,980 --> 00:40:42,110
They can they can be short of breath. Right. So so you have a small problem.

322
00:40:44,890 --> 00:40:50,590
We have this daily number of hospital admissions in the category of.

323
00:40:53,830 --> 00:40:58,950
Then you have your air pollution the in time that you answer the question.

324
00:40:58,960 --> 00:41:08,800
But it is to process it. Okay. We're not looking at causalities too much to us, but at least you will see whether or not these two things are.

325
00:41:10,120 --> 00:41:11,650
They teach our associate.

326
00:41:14,420 --> 00:41:23,270
So now let's talk about extend the framework space based model allowing to study facts of time during careers such as vaccination.

327
00:41:23,270 --> 00:41:32,960
And this is important to, you know, our understanding of the effectiveness of vaccination on a population level.

328
00:41:33,260 --> 00:41:39,049
You have this study of efficacy of vaccination through clinical trials,

329
00:41:39,050 --> 00:41:45,170
but efficacy may not be truly reflected the, you know, the population effectiveness of the vaccinated.

330
00:41:46,550 --> 00:41:58,610
But anyway, so, so it's very, very important in the fascist study is that you look at the population level effect of certain policy intervention.

331
00:41:58,760 --> 00:42:03,350
Okay, so you need to model that into your model.

332
00:42:04,460 --> 00:42:08,810
So now I want to talk about this data analysis.

333
00:42:08,810 --> 00:42:18,880
The case study I did for my dissertation, I studied quite a bit outdoor of asthma and like so that's why I'm Janet Jan interested

334
00:42:18,920 --> 00:42:26,510
environmental sciences I think probably or in orange it originated from my my

335
00:42:27,290 --> 00:42:36,559
dissertation okay so so there are I work on this your chart data from Prince George air

336
00:42:36,560 --> 00:42:43,310
pollution study the Prince George is a city in the northern part of British Columbia.

337
00:42:43,790 --> 00:42:51,409
Okay. I will tell you a little bit background there, but this is a study of so we look at daily time.

338
00:42:51,410 --> 00:42:58,940
So is a hospital emergency room admissions due to different respiratory disease categories.

339
00:42:59,120 --> 00:43:10,250
Okay. So this is my why why is number of hospital emergency visit in the different categories of respiratory disease.

340
00:43:11,630 --> 00:43:18,620
So why do we want to you know do here is to you know study the association between the

341
00:43:18,620 --> 00:43:27,230
air pollution and the the incidence rate of the respiratory disease in this region.

342
00:43:27,470 --> 00:43:33,170
Okay. Of course that is similar structure can be applied to develop a concrete case study.

343
00:43:33,170 --> 00:43:41,630
We think, you know, this sort of sort of analysis of vaccination for protective effect of COVID infection of this.

344
00:43:42,410 --> 00:43:47,820
So very similar structure. But okay, here's the background.

345
00:43:48,420 --> 00:43:55,260
So I work on this air pollution study when I was a teacher student with my adviser, Ben Jorgensen.

346
00:43:56,400 --> 00:44:02,520
This is a proposal from the health department from British Columbia.

347
00:44:03,240 --> 00:44:13,590
So the suck lung sort of there's a public public health concern in the Prince George this city.

348
00:44:14,270 --> 00:44:21,420
But the air quality in this city may be adversely, if adversely, affect the health of residents in that city.

349
00:44:22,260 --> 00:44:27,719
So Prince George is famous for his pulp fiction industry where the cottage trees

350
00:44:27,720 --> 00:44:32,580
and make those the pulp fiction too is a sort of mature that you make paper right.

351
00:44:33,040 --> 00:44:38,969
So also this so because of the richness of forest in that region and then the you know,

352
00:44:38,970 --> 00:44:47,400
they cannot produce high quality paper because they use trees to, you know, to make the Pulp Fiction.

353
00:44:47,410 --> 00:44:52,710
And so in that region, that that's a major industry in that region.

354
00:44:53,220 --> 00:45:00,600
And then the residents in our region complain that, you know, the air quality is not very good.

355
00:45:00,600 --> 00:45:05,670
That affects their sort of health then.

356
00:45:06,660 --> 00:45:11,580
So people are trying to understand the whether or not this is the costs or this there's

357
00:45:11,580 --> 00:45:18,800
a association of the air quality with there are respiratory incidents in the city.

358
00:45:18,810 --> 00:45:22,740
That's something the B.C. government wants.

359
00:45:23,400 --> 00:45:34,440
The British Columbia government want to understand and see if you can really, like make a change of,

360
00:45:34,740 --> 00:45:42,330
you know, there are the limitations of this waste material and release to the air or whatever.

361
00:45:42,330 --> 00:45:51,410
They have to do some kind of policy change if there is a proven association for the air pollution part fund industry.

362
00:45:51,420 --> 00:45:55,380
So. So that's quite a challenge project.

363
00:45:55,410 --> 00:46:02,810
I was you know at your age and getting this kind of serious, very serious project,

364
00:46:02,820 --> 00:46:11,639
I need to really find a very justified, justifiable statistic toolbox that can really answer the question.

365
00:46:11,640 --> 00:46:24,090
And so, you know, group of people from public health in the university, British Columbia, the U.S. estimate to do this thing.

366
00:46:24,090 --> 00:46:29,830
But there are. It doesn't like that kind of analysis.

367
00:46:30,040 --> 00:46:36,830
Okay. It is bad. It's just like they said that you have really long time stories.

368
00:46:37,090 --> 00:46:41,380
So you'll have data from one city and several hospitals.

369
00:46:41,830 --> 00:46:47,200
You had very long times that, like, you have like three, two years of data, right?

370
00:46:47,590 --> 00:46:52,840
Three years of data. Two years. 84, 85.

371
00:46:53,950 --> 00:46:57,200
86, I think. Three years. Well.

372
00:46:59,860 --> 00:47:08,220
So you don't have many. It's not really longitudinal data because for longitudinal data you have many short time, right?

373
00:47:08,650 --> 00:47:15,219
So you have each individual that has a 510 visit, but now you have like a long time daily data.

374
00:47:15,220 --> 00:47:21,190
For two years you have like more than 700 measurements from hospitals.

375
00:47:21,820 --> 00:47:31,180
And it's not a like a longitudinal, then longitudinal that typically is kind of a collection of many short time series.

376
00:47:31,600 --> 00:47:40,810
Okay. So, so do they say, okay, that's good analysis, but what they want, so they to find someone,

377
00:47:41,320 --> 00:47:49,000
someone who is able to analyze their data in a way that they think they should be denied multiple times through small.

378
00:47:51,530 --> 00:47:56,290
So my goal was of of my future study.

379
00:47:57,370 --> 00:48:02,110
I was trying to do some dumb chromatic smoothing, splicing, kernel smoothing.

380
00:48:02,120 --> 00:48:08,120
I never saw that I could be involved in this kind of project. That probably changed my career trajectory.

381
00:48:08,120 --> 00:48:14,060
And I thought, okay, let's give a try. Nobody knows how to do that, so let's give it a try.

382
00:48:14,070 --> 00:48:17,100
And that's the whole thing started.

383
00:48:17,120 --> 00:48:23,900
So basically they have the monitoring of air pollution in the city, Prince George, British Columbia.

384
00:48:24,710 --> 00:48:34,940
And then they showed that some frequent excursions about provincial air quality standards and the ESR data of emergency

385
00:48:34,940 --> 00:48:45,350
room visit due to respiratory disease from local hospital collected from April 1984 to March 1986 for the study.

386
00:48:46,430 --> 00:48:57,020
Well, basically in this study that we have time varying hold very process including daily average temperature, daytime max and minimum humidity,

387
00:48:58,100 --> 00:49:07,819
daily sulfur compound concentration until the suspended fine particles PM 2.5 at PM10 concentration because

388
00:49:07,820 --> 00:49:16,340
this risk for diseases like asthma bronchitis are very sensitive to temperature change and and some you know,

389
00:49:16,880 --> 00:49:23,650
the air quality so like humidity right so so that so they collect all this data

390
00:49:23,840 --> 00:49:28,579
by the B.C. government and they want to see the fundamental question here.

391
00:49:28,580 --> 00:49:40,070
It's really is the the emergency room visit associated with the sulfur compound concentration.

392
00:49:41,200 --> 00:49:48,950
So this this is basically a question to ask, whether or not there is a association between, you know,

393
00:49:49,060 --> 00:49:56,500
the rate of the average, you know, emergency room visit and the software company concentration.

394
00:49:56,530 --> 00:50:02,250
This is collusion, right? So so here is the Prince George.

395
00:50:02,260 --> 00:50:05,300
He's Vancouver. Down here is Seattle. Right.

396
00:50:05,330 --> 00:50:17,709
So it's really in the very awesome part of the University of British Columbia is and I did not get a chance to have a field trip there.

397
00:50:17,710 --> 00:50:24,910
But, you know, but I want to visit that city to really look at what's going on there.

398
00:50:24,910 --> 00:50:28,870
And I didn't get a chance to to visit that place.

399
00:50:28,870 --> 00:50:34,120
But I am I analyzed the data from this city. Okay. Data.

400
00:50:34,570 --> 00:50:45,640
Okay. So you have asthma counts, basically counts of daily counts of hospital, hospital admissions, emergency room visits.

401
00:50:45,800 --> 00:50:55,240
Okay. So from April to 1985 to the end of March, 1986.

402
00:50:55,570 --> 00:51:06,340
So this two year day that. So this. So you have low counts in this city with about 100,000 residents.

403
00:51:06,730 --> 00:51:12,880
Okay. And you have bronchitis. Also the number of visit as a slightly larger number of.

404
00:51:15,210 --> 00:51:19,590
Emergency. You can see there are some surge here, like here.

405
00:51:20,400 --> 00:51:24,170
And then you can see there's a surge here a little bit.

406
00:51:24,180 --> 00:51:29,900
And then, you know, you can see there are a little bit bumpy here and so and so forth.

407
00:51:29,910 --> 00:51:39,270
And they also have the ear infection. So here you can see that your infection and the bronchitis are somewhat correlated.

408
00:51:39,690 --> 00:51:46,260
So when you know they are, they rise almost the same time here compared to the average above them.

409
00:51:46,630 --> 00:51:55,170
Okay. So here as well, your infection and bronchitis that are have a slightly higher level than the average.

410
00:51:55,650 --> 00:52:00,390
So there are some correlation among those emergency room visits.

411
00:52:01,350 --> 00:52:11,129
And there's other categories that either not related to the respiratory symptoms ordered and asthma, bronchitis, infection.

412
00:52:11,130 --> 00:52:20,880
So you can see that there's also some kind of surge of the number of hospital visits, but essentially are low counts in general.

413
00:52:21,630 --> 00:52:26,490
So this is your white teeth. So which can be a vector.

414
00:52:26,700 --> 00:52:32,180
Okay. It could be the in my case, I have four dimensional vector, a white t.

415
00:52:33,660 --> 00:52:45,120
So that that's basically a daily kinds of emergency room visit for, you know, for categories of respiratory disease in this city.

416
00:52:45,600 --> 00:52:52,770
You can think about this as number of confirmed cases, the number of deaths or the number of hospitalizations in the facilities.

417
00:52:52,800 --> 00:52:57,540
But they have similar data structure is just in a different field.

418
00:52:57,930 --> 00:53:03,480
Okay. So now here is air pollution data. You have the daily average temperature.

419
00:53:03,780 --> 00:53:08,000
Okay. So this certainly is a confounding factor, right?

420
00:53:08,020 --> 00:53:11,720
So that you should put this time series into the model.

421
00:53:11,730 --> 00:53:19,800
So as I said, that this covered you can call it functional corporates is is now the age is age you know,

422
00:53:20,040 --> 00:53:25,500
a race something something fixed is a time during.

423
00:53:27,420 --> 00:53:32,760
So you you want to see if these temperatures are related to the number of hospital visits.

424
00:53:33,260 --> 00:53:38,430
Okay. So you have humidity here. You can maximum humidity or minimal humidity.

425
00:53:38,550 --> 00:53:47,270
And if whether or not the gap of this to would be dramatic, change of humidity would trigger an asthma attack or something like that.

426
00:53:47,280 --> 00:53:59,550
So. So something like you look at also, of course, the most important part is the one that the B.C. government is interested in is really the sulfur.

427
00:53:59,680 --> 00:54:07,200
Right. So they. This sort of pollutant release from the industry factories to the air.

428
00:54:07,200 --> 00:54:17,249
And you can see that here. Yeah, there is kind of a more higher amount of the sulfur in this around this time,

429
00:54:17,250 --> 00:54:23,550
which is somewhat related to the rise of the of this hospital visit.

430
00:54:23,880 --> 00:54:30,390
Would this be a would there's a sufficient statistic evidence to prove this association?

431
00:54:30,420 --> 00:54:37,500
Right. And also here you have 2.5 and 2 p.m., 2.5 particles.

432
00:54:38,040 --> 00:54:44,790
So this data is the fine particles. It's collected from the Air Quality Monitoring Center.

433
00:54:45,150 --> 00:54:49,920
Unfortunately, we only have weekly data. We don't do anything there.

434
00:54:50,430 --> 00:55:01,920
So every week we get one measurement, basically the average sort of thing from the air monitoring center.

435
00:55:02,430 --> 00:55:12,060
So what we did there is in order to create same level of resolution of the other data, uh, variables.

436
00:55:13,160 --> 00:55:18,260
We did these sort of enter into a nine year interpolation.

437
00:55:18,830 --> 00:55:23,060
So essentially we just, you know, create.

438
00:55:23,330 --> 00:55:38,510
So you have a PM 2.5 concentration on day one week of your week or week, and then you have another one that's not a week, right?

439
00:55:40,790 --> 00:55:49,370
So you just use another line to link them. This is called an interpolation and maybe you can have a better imputation method.

440
00:55:49,400 --> 00:55:55,470
We just take something very simple to impute the missing vowels to.

441
00:55:55,500 --> 00:56:04,760
You were to create a daily data of these particles if you were to have this similar level of resolution of the other data.

442
00:56:06,180 --> 00:56:13,110
So now. So the scientific questions are very clear whether the risk of culpability of respiratory

443
00:56:13,110 --> 00:56:17,790
disease associated with air pollutants in this case is sulfur and particles.

444
00:56:19,740 --> 00:56:25,890
So so that's the one that we need to provide statistical analysis to the ABC Department of Health.

445
00:56:28,710 --> 00:56:34,820
And we were right of the very interesting story when we when we were writing this paper.

446
00:56:34,830 --> 00:56:38,460
Right. And we received an email from Ron Top.

447
00:56:39,240 --> 00:56:51,240
It was a sort of added spit for environmental sciences for this is the medicine.

448
00:56:51,930 --> 00:56:56,200
And that was the first time we heard them. Right.

449
00:56:56,820 --> 00:57:06,120
And he invited us to submit a paper to that special issue for School of Medicine that, you know, all the data analysis of this project was there.

450
00:57:06,140 --> 00:57:10,050
And for that, you can find that paper in student medicine.

451
00:57:12,420 --> 00:57:19,350
So what we're trying to do here is really to address statistical needs to come forward, post social correlation,

452
00:57:19,510 --> 00:57:26,219
us seroquel issue and cross compound course and cross component correlation meaning that the correlation

453
00:57:26,220 --> 00:57:33,990
across different categories of the disease because you can see that the times are somewhat correlated.

454
00:57:34,200 --> 00:57:38,490
Okay. So. So that that's something we like to do.

455
00:57:39,470 --> 00:57:48,250
Mm hmm. So now I just want to talk about the consideration of Mall and how we specify the malls to address this.

456
00:57:49,150 --> 00:57:58,060
So modern needs distinguish different ways of the metrological variables and air pollution very affects the risk for mobility.

457
00:57:58,960 --> 00:58:10,390
And because you know the air pollution and are all of your variables may have different ways to effect, you know, respiratory symptoms.

458
00:58:11,420 --> 00:58:21,840
Okay. So here so our understanding by talking the whole project with, you know, specialists in that field, in fact,

459
00:58:22,230 --> 00:58:28,080
of metrology for variables like temperature and humidity and respiratory mobility,

460
00:58:28,080 --> 00:58:33,880
it tends to be more accurate than that of air pollution variable, which usually appears lack in time.

461
00:58:34,490 --> 00:58:43,390
So today the temperature is high. With humidity is high, people tend to have immediate sort of accurate symptoms.

462
00:58:43,540 --> 00:58:46,900
They immediately react to have some symptoms.

463
00:58:47,350 --> 00:58:56,230
But air pollution tends to have a long term slow sort of long term effect on the of the symptoms.

464
00:58:56,970 --> 00:59:05,530
Okay. So the survey of sulfur level in November of 1985 since responsive for the peak of air

465
00:59:05,530 --> 00:59:12,280
visit by patients of bronchitis ear infection and after of course in December of 1985.

466
00:59:12,290 --> 00:59:18,210
So we can look at data here. So you have the search here and there.

467
00:59:18,250 --> 00:59:21,940
You can see that there's a search afterwards.

468
00:59:21,940 --> 00:59:26,860
That's that's exactly the sort of lack, in fact, the observe.

469
00:59:28,560 --> 00:59:35,820
So. So from looking at those plots, that's why I want to you to make time as part of your homework,

470
00:59:35,820 --> 00:59:47,210
to really look at time storage data, to see in which way that the vaccination or other time between can fact the observed process.

471
00:59:47,220 --> 00:59:54,210
Right in this case is number of deaths in your user and data analysis here.

472
00:59:54,990 --> 01:00:04,200
You know so something happened that may not have immediate effect on the outcome but may be of fact the outcome in one months,

473
01:00:04,200 --> 01:00:08,640
in two months or even a few weeks. So there is a lack of facts.

474
01:00:08,880 --> 01:00:12,480
Okay, so so what do we see here in the air pollution is similar thing.

475
01:00:12,480 --> 01:00:24,300
So you have a surge of air pollution that may not have, you know, immediate effect on the visit, but maybe one month later it.

476
01:00:27,350 --> 01:00:35,390
Well, there are, you know, symptoms. So in this kind of analysis, lack, in fact, is quite important to model.

477
01:00:38,310 --> 01:00:41,860
So so we have this sort of two things going on.

478
01:00:41,880 --> 01:00:45,690
One is the acute intervention, a lack of intervention,

479
01:00:45,720 --> 01:00:51,780
the appear in the daily number of confirmed cases of daily number hospitalization and daily number of deaths.

480
01:00:51,780 --> 01:01:01,739
Or I'm talking about COVID situation. You'll give a vaccination that may not immediately slow down the infection.

481
01:01:01,740 --> 01:01:08,760
Maybe, you know, a pulse later you can start to see the drop of the confirmed cases of deaths.

482
01:01:09,090 --> 01:01:13,560
So there are always some kind of lag, sort of effect of sort of indifference.

483
01:01:14,220 --> 01:01:16,620
You ask people to stay in the home.

484
01:01:16,620 --> 01:01:25,150
You know, these sort of stay shelter order in the state of Michigan may not as you see immediately next day you have.

485
01:01:28,900 --> 01:01:36,790
See the drop of their the number of the confirmed cases and and hospitalizations.

486
01:01:36,790 --> 01:01:43,449
So this lack the fact is always a it's something we're looking at in this kind of analysis.

487
01:01:43,450 --> 01:01:54,350
We want associate to time source in your you know okay so some general advice is that we read time stories plots.

488
01:01:54,460 --> 01:02:03,100
That's why in the second problem of homework, too, I want to you look at the data closely and see what do you see there?

489
01:02:03,550 --> 01:02:10,150
And by looking at the data, you can really see what kind of variables that you can model.

490
01:02:10,150 --> 01:02:16,000
You know, industries, careers, something may now be able to to do that.

491
01:02:16,330 --> 01:02:29,050
Look at data data that times this data and that will basically give you the clues which variables you should be should incorporate into your models.

492
01:02:29,470 --> 01:02:32,890
And so that's very important.

493
01:02:34,060 --> 01:02:40,240
And so what we're trying to do here is so we first think that, okay,

494
01:02:40,600 --> 01:02:49,510
the metrological variables tend to have the accurate impact on the hospital, the er visit.

495
01:02:50,080 --> 01:02:53,260
So we call this short term effects. Okay.

496
01:02:53,560 --> 01:03:00,610
So that similarly maybe we can think about, you know, the, the, the sort of short term effect of vaccination.

497
01:03:01,120 --> 01:03:15,129
Okay. So, so basically you can model this short term the kind of this covers into this sort of intervention in this Pawsome Mm.

498
01:03:15,130 --> 01:03:20,830
Parameter right through that is the live in process that your culverts cannot explain,

499
01:03:21,460 --> 01:03:29,230
but you can put your intervention variable in a could be time change could be other you know,

500
01:03:29,230 --> 01:03:37,990
age or racial whatever the coverage you feel comfortable that would be help to to to explain the variability

501
01:03:37,990 --> 01:03:46,270
of you why but maybe the intervention is the one that you want to model as a short term effect.

502
01:03:46,540 --> 01:03:56,650
Okay. And the number of death. Okay, so you can put the vaccination on day and also like p day before.

503
01:03:56,980 --> 01:04:05,660
Okay so you could use a lax of this covered the lacked VTI as you covered I'm just giving example

504
01:04:05,660 --> 01:04:11,560
I'm not saying this is the right model specific I'm just saying that how do you at lacked cover.

505
01:04:13,070 --> 01:04:18,890
Suppose this is observed. Proportion of people fully vaccinated.

506
01:04:19,850 --> 01:04:22,730
Right. You can put like p equal to 14.

507
01:04:22,760 --> 01:04:34,010
You can look at that kind of proportion of people fully vaccinated in a window of 14 days that you'll see whether or not the vaccination, you know.

508
01:04:37,090 --> 01:04:42,710
You know, before, what really affects is the number of the deaths today.

509
01:04:42,910 --> 01:04:48,709
Right. So that's basically the this if P is 14, that's this variable 20 to tell.

510
01:04:48,710 --> 01:04:57,220
You can put 30 days, whichever you think is something like you want to analyze in terms lack of facts.

511
01:04:58,670 --> 01:05:01,159
So in terms of the long term thing, right.

512
01:05:01,160 --> 01:05:07,010
So you say that, oh, I think that the vaccination may have a long term effect rather than a short term effect.

513
01:05:07,580 --> 01:05:14,930
Then you can't allow this covers to entered the late process.

514
01:05:17,110 --> 01:05:24,249
I want to talk about, because I said that we want to relax this this Nazi assumption and they are in

515
01:05:24,250 --> 01:05:28,260
the station our policies mean is always constant doesn't change over time.

516
01:05:28,270 --> 01:05:33,840
Right this is the station now. You'll want women to be time dependent.

517
01:05:34,710 --> 01:05:46,080
So you do not want this to be a constant anymore because you believe that they live in process also driven by some covariates.

518
01:05:46,470 --> 01:05:55,650
Okay. Oh, so now if you want to model this latent process as a function of certain COVID like vaccination.

519
01:05:56,130 --> 01:06:08,610
So here is the way to look at. Okay, if you can specify the the V t as a function of the increment of the vaccination difference.

520
01:06:09,120 --> 01:06:18,600
So so v t is proportion of the individual who are fully vaccinated in state of Michigan, for example,

521
01:06:18,930 --> 01:06:26,100
v t minus one is the proportion of people in Michigan who are fully vaccinated yesterday.

522
01:06:26,430 --> 01:06:30,450
So what's the chance of that? How many additional people who have.

523
01:06:32,870 --> 01:06:40,430
So this is increment. Right. So this differ some days that, you know, more people go to receive vaccination.

524
01:06:42,130 --> 01:06:49,060
Okay. So this increments tells you the the change of the the intervention.

525
01:06:49,240 --> 01:06:53,559
Okay. In this process. Okay. Now, if you do this,

526
01:06:53,560 --> 01:07:07,330
then you can easily to show mathematically the marginal mean or the population average of the the population mean of the of C that will be equal to,

527
01:07:08,350 --> 01:07:12,460
you know, this C has zero times B1. Right.

528
01:07:14,510 --> 01:07:22,020
So this is easy to prove. How do you prove that? Well, you basically prove this by this double expectation argument.

529
01:07:22,040 --> 01:07:25,760
Right. But anyway, this you can you can prove it. That's quite straightforward.

530
01:07:26,240 --> 01:07:35,840
So now you have this sort of the the baseline mean and depends on how you define time zero.

531
01:07:36,380 --> 01:07:42,380
But anyway, you have the average number of the incidents and then you, you,

532
01:07:42,380 --> 01:07:49,550
you because this is multiplicative structure, then you can have exponential function that gives you this.

533
01:07:50,120 --> 01:07:53,330
Then you, you know, add it up.

534
01:07:53,630 --> 01:08:02,780
Okay. Beta is a constant. Then this sum will cancel out all the intermediate terms that you have, basically difference of VTI and V zero.

535
01:08:04,010 --> 01:08:07,130
So that's why we call this cumulative effect.

536
01:08:07,160 --> 01:08:17,600
Basically, you have increments that could be accumulated over time to create this big difference at a time.

537
01:08:17,600 --> 01:08:28,580
T and and the baseline. So so what you're trying to model here, of course, this is one way to model in our case that we model the air pollution.

538
01:08:29,000 --> 01:08:43,670
So so here that you have air pollution. And January 1st, 1951, looking at the Air Force, you know, air pollution at, uh, April 1st.

539
01:08:45,800 --> 01:08:49,340
What is the cumulative change of this? Okay.

540
01:08:49,760 --> 01:08:54,200
So that how that would affect your leatham process.

541
01:08:54,390 --> 01:09:05,240
Okay. So this is cumulative we call long term effect because every day you have a little bit change and gradually what's the total change that can,

542
01:09:05,810 --> 01:09:13,720
you know, basically a factor system. Okay. So this is different way compared to the first one, right?

543
01:09:13,790 --> 01:09:21,080
First, the one you are covered has immediate effect to the Y.

544
01:09:21,830 --> 01:09:28,940
Okay. So this is called the short term accurate effect of your intervention on the Y here.

545
01:09:28,940 --> 01:09:33,500
You know, the fact is sum of increments.

546
01:09:33,920 --> 01:09:42,590
Well, that it represents can the cumulative effect of your intervention where some air pollution in my case to affect your life.

547
01:09:42,840 --> 01:09:55,570
Okay so that's. Well, this acute effect is really directly on the 80 and my cue minuted that the fact is and I see that.

548
01:09:56,500 --> 01:09:58,720
Okay so that's a little bit different.

549
01:09:59,050 --> 01:10:13,000
But anyway, you put some kind of covers into the dating process if you believe that this effect is gradual, is cumulative, is long term.

550
01:10:13,750 --> 01:10:20,440
That you can build that into your latent process to model the fact of that variable of interest.

551
01:10:21,890 --> 01:10:35,080
But that's an actual fact that you can direct the built into your awesome distribution here directly involved the observed outcomes.

552
01:10:35,560 --> 01:10:39,760
So that's really the the difference how you do that.

553
01:10:39,910 --> 01:10:50,830
Okay. Of course. Now if you. The report says that basically there's a relatively stable sort of process going on in the

554
01:10:51,580 --> 01:10:58,810
population so that everything that would be really in there from from deleting process to wiki.

555
01:10:59,170 --> 01:11:04,270
So the emission probability is the place that you want to place your model,

556
01:11:04,690 --> 01:11:12,760
but you can extend by adding some model T into the covers into the latent process to model the cumulative effect.

557
01:11:13,690 --> 01:11:23,050
That's something for both cases, either stationary, although stationary like this case, the common filter and patterns most of work.

558
01:11:23,700 --> 01:11:34,540
Okay. So you can do prediction. And in my dissertation I also work out which model is like you do model selection.

559
01:11:34,540 --> 01:11:38,620
Right. So which model is more appropriate for the for the data.

560
01:11:39,040 --> 01:11:49,240
Because if all that kind of report that you submit to government, you have to have some kind of readers analysis in part of the data analysis.

561
01:11:49,900 --> 01:11:56,830
So, so I did a lot of model diagnosis. I developed quite a bit model diagnosis into the data analysis.

562
01:11:57,850 --> 01:12:03,280
Also do this model selection and which model is more of.

563
01:12:06,750 --> 01:12:19,700
So I did some actual working there. My diagnosis not just estimating inference in I work we do not have the sort of problem of prediction.

564
01:12:19,710 --> 01:12:25,200
We only want to look at associations, but in fact it is case like you are looking at.

565
01:12:25,380 --> 01:12:29,580
Of course we are interested full cost, right? So so that's slightly different.

566
01:12:30,540 --> 01:12:36,090
But I did not investigate that forecast in part in this model very much.

567
01:12:36,390 --> 01:12:45,360
But anyway, that's the Prince George Air Pollution Study, which is very similar to what you hear.

568
01:12:50,260 --> 01:12:59,440
Just come back to revisit this numerical solution, ordinary differential equation, because we need to move down to the estimation part.

569
01:12:59,650 --> 01:13:05,650
We need to know how to solve that. You already use the R pack d solve to get a solution.

570
01:13:05,650 --> 01:13:07,480
But I want to tell you a little bit,

571
01:13:07,600 --> 01:13:16,220
the technique to tell how the coach could solve a problem is approximation is of developed or how that looks like.

572
01:13:16,840 --> 01:13:21,340
So basically it's numerical solution of ordinary differential equation.

573
01:13:22,870 --> 01:13:26,640
So I will cover three since I give you the introduction of the problem.

574
01:13:26,650 --> 01:13:33,070
And first talk about Euler. First of all, our approximation, the logical de force order approximation.

575
01:13:34,880 --> 01:13:42,530
Okay. So the basic idea here is really just do the descriptive numerical solution to more to confirm all

576
01:13:42,530 --> 01:13:52,190
by discrete position and Euler first order approximations always do one to begin with and I will.

577
01:13:52,700 --> 01:13:59,269
Oh the most popular one right. You could have false order approximation which is most widely used in practice.

578
01:13:59,270 --> 01:14:05,809
I mean post approximation and arcane approximation are available in our package.

579
01:14:05,810 --> 01:14:15,200
Do solve. Okay. But after we understand older first order approximation, the first order approximation is easy to understand.

580
01:14:15,900 --> 01:14:24,889
Okay, here's a setup. This first order, ordinary differential equation, dui, t, t and f, t, y.

581
01:14:24,890 --> 01:14:31,310
And this boundary point in this case, the boundary point is place at initial time t zero.

582
01:14:32,060 --> 01:14:39,620
So here to zero is known. Maybe the t zero is today in the September 29.

583
01:14:40,130 --> 01:14:46,370
That's T zero. Y zero is no so maybe y000 is ten.

584
01:14:46,400 --> 01:14:50,000
Depends on what you're talking about. The Y zero is no.

585
01:14:50,300 --> 01:14:59,000
Okay. So basically is the boundary put positions in this case is the initial position of the of the dynamics.

586
01:15:00,110 --> 01:15:11,870
So Y is a function of T and it's first order to rotate you call to f t y is not only the function of t but also the function y.

587
01:15:12,230 --> 01:15:17,390
Okay? So y is a function of t. Okay? So this is a compound function.

588
01:15:18,020 --> 01:15:23,990
So f f is not okay. So F is that that satisfies this boundary condition.

589
01:15:26,540 --> 01:15:32,780
So also we require that F is the function that gives you the unique solution.

590
01:15:33,770 --> 01:15:41,650
Okay. So. So that's. The mathematical condition that.

591
01:15:43,030 --> 01:15:47,110
Unless you knew, Murtha's solution will be undefined.

592
01:15:47,800 --> 01:16:00,650
So. So that could be a. So our solution were talking about if this has multiple solutions.

593
01:16:01,630 --> 01:16:08,030
Okay. A lie could be that vector, not necessarily a scholar.

594
01:16:08,170 --> 01:16:17,350
So. So that's why it could be a vector of multiple compartments or a number of confirmed cases,

595
01:16:17,350 --> 01:16:22,300
number of susceptible cases, number of tests, number of vaccinations.

596
01:16:22,960 --> 01:16:27,190
So that's the vector that the. You.

597
01:16:27,240 --> 01:16:31,920
How did you do solve the problem? Okay. So this figure is very clear.

598
01:16:32,280 --> 01:16:38,770
Let me just explain this, then we can see the figure. Hmm. So what you're trying to do here, is that okay?

599
01:16:39,180 --> 01:16:45,030
I know. What's the value of yt0 at time t zero.

600
01:16:45,030 --> 01:16:49,810
The initial position. And. I really have a solution for that.

601
01:16:50,110 --> 01:17:04,800
So. Well, the problem here is that.

602
01:17:07,420 --> 01:17:16,070
Well here is my t0. I see. And I notice zero value is y zero which is my Q zero.

603
01:17:16,150 --> 01:17:25,880
This is not right. So I know this sort of starting position and I'm looking at a curve, whatever.

604
01:17:25,900 --> 01:17:31,600
This is my f t 23.

605
01:17:32,810 --> 01:17:36,800
The. See the function you need to figure out.

606
01:17:36,810 --> 01:17:41,160
Of course, this is not arbitrary. Function is a function that size of five.

607
01:17:41,970 --> 01:17:53,200
This equation. This ordinary differential equation basically says that the derivative of this function is no.

608
01:17:54,340 --> 01:18:00,250
Okay. I want to figure all the functions solved. Given that I know the derivative for this function.

609
01:18:00,910 --> 01:18:06,370
Okay. The first order of derivative of this. Basically, I know the dynamics of.

610
01:18:06,610 --> 01:18:09,940
I want to figure out the curve itself. That's essentially the problem.

611
01:18:10,810 --> 01:18:20,310
So what they're trying to do here is that you just do need norteno space is supposed to wise is the time that you want to figure out what it's like,

612
01:18:20,590 --> 01:18:24,590
what you want. Okay. This.

613
01:18:26,880 --> 01:18:33,220
I'll see the curve. Right. The power of that as exist uniquely.

614
01:18:33,430 --> 01:18:40,660
But you'll see that what you see here is really just the the the first order of drivers.

615
01:18:41,110 --> 01:18:44,440
It's given by their function. Okay. So you wanna figure out this?

616
01:18:45,130 --> 01:18:50,140
So what is this? Okay. So all I said well is just to tailor expensive.

617
01:18:52,660 --> 01:18:58,180
That's okay. You'll see. Say okay. I want to figure out this.

618
01:18:58,360 --> 01:19:04,620
Now, what I'm supposed to do here is that I find that t1ty is your choice, right?

619
01:19:04,660 --> 01:19:07,810
T one can be something very, very close to zero.

620
01:19:08,440 --> 01:19:11,740
Right. Somewhere. I mean, t y is something you choose.

621
01:19:12,580 --> 01:19:19,810
What do the values on figure out? So t one could be very near the top of this.

622
01:19:21,190 --> 01:19:29,710
Okay. So if I look at that, the the conditions I know is that I know the first position.

623
01:19:30,040 --> 01:19:33,100
Also, I know the tangent at this position. Right?

624
01:19:33,460 --> 01:19:39,180
The tangent position. I. T i. Why?

625
01:19:39,190 --> 01:19:43,990
Zero. Because that's the first order derivative of the curve, right?

626
01:19:44,350 --> 01:19:53,079
I know the first order derivative at two zero, even by my differential equation, so that I know the tangent of this.

627
01:19:53,080 --> 01:19:59,450
And so I can do all kinds of line. This this slope of FTB, their y0.

628
01:19:59,450 --> 01:20:05,950
That's what this differential equation gives me so that I can do a tighter expansion on this tangent.

629
01:20:05,950 --> 01:20:10,229
Right? So here is my first. That's the value.

630
01:20:10,230 --> 01:20:10,950
I won't freak out,

631
01:20:10,950 --> 01:20:23,849
but I can use this slope the tangent give my by my differential equation to create the tangent line so that will be you by y t of y t0.

632
01:20:23,850 --> 01:20:38,040
That's the Territory expansion, right, so that I have my f t0 y z of this affordable older derivative of that, then I have my T minus t one.

633
01:20:39,480 --> 01:20:43,000
But you. Spend is t zero.

634
01:20:48,910 --> 01:20:52,330
Well, this will deter expansion. This is your tangent line.

635
01:20:53,470 --> 01:20:59,020
Now, if you want to get to the answer on this, whether you.

636
01:21:00,450 --> 01:21:14,130
It's talking about this ten year function and you want rain, which will be your way to zero and 20y0 times t one minus two zero.

637
01:21:16,630 --> 01:21:22,330
That's all only solution. Right. Well, this is actually the solution.

638
01:21:24,670 --> 01:21:27,640
Of course it is an approximation. This is not where.

639
01:21:29,040 --> 01:21:38,979
This is not exactly Sufi this approximation your you know, the first position you know the ten the derivative of the function.

640
01:21:38,980 --> 01:21:45,750
You add this position, you create a tangent line and then use the inner function tangent line to approximate

641
01:21:45,750 --> 01:21:51,420
the value of this course that people find that approximation is not very accurate.

642
01:21:52,650 --> 01:22:01,620
So that's what I draw here. So the tension line you're trying to figure, find out this I mean, this value you really want is under this correct curve.

643
01:22:01,620 --> 01:22:07,110
But what you get estimate approximation is given on the tangent line.

644
01:22:10,340 --> 01:22:27,319
The how much of person error you have. What depends on you talking about the the the actually the this sorry this gap or you have a core error because

645
01:22:27,320 --> 01:22:32,810
if you are not only approximate this solution on one point because you want to recover entire function,

646
01:22:32,820 --> 01:22:39,460
right, and you have a sequence of great points each one, you want to do this, okay.

647
01:22:39,800 --> 01:22:50,000
So you have a human error, which is h which is the family is basically is now stepwise is the distance between t one and two zero.

648
01:22:50,300 --> 01:22:54,050
As I said T once you choose how big that the gap you want to.

649
01:22:55,950 --> 01:22:59,040
If I go out all the time now.

650
01:23:01,300 --> 01:23:09,220
So to German mathematician Roger Coulthard, they said, oh, this proximity is great, but it's not very accurate.

651
01:23:09,250 --> 01:23:13,210
And they had the force over of approximation. That's probably the.

652
01:23:15,360 --> 01:23:23,280
It's better once, but feels like a false order approximation is good enough as very simple and so people don't bother it.

653
01:23:23,280 --> 01:23:25,740
You can go something side better. Okay.

654
01:23:26,130 --> 01:23:36,780
So I will tell you how this AAC approximation is constructed next time after we understand the general formulation of EULEX prosecution.

655
01:23:49,840 --> 01:23:53,079
But I will email you the first group.

656
01:23:53,080 --> 01:24:05,050
And really I'd like to have a set up a couple of more data catchers so people can work collectively, create more interesting as a best.

