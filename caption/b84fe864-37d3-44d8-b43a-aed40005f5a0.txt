1
00:00:08,970 --> 00:00:09,690
Michael Akira Lee Hayashi: First off.

2
00:00:10,980 --> 00:00:21,270
Michael Akira Lee Hayashi: First off i'm going to go ahead and put the beginning to know you sheet from yesterday in chat and what i'd like you to do is just going to modify it really quick.

3
00:00:22,500 --> 00:00:27,840
Michael Akira Lee Hayashi: we're going to put today's date next to it and just stick an x next to your name, so this is.

4
00:00:29,370 --> 00:00:31,050
Michael Akira Lee Hayashi: To 712.

5
00:00:33,150 --> 00:00:37,230
Michael Akira Lee Hayashi: we're supposed to take attendance, so this is a easier way to ease your way to do it.

6
00:00:38,850 --> 00:00:43,950
Michael Akira Lee Hayashi: You could fill that out, hopefully, you should all have access to it, let me know if.

7
00:00:45,840 --> 00:00:46,320
Not.

8
00:01:08,820 --> 00:01:15,720
Michael Akira Lee Hayashi: And for anyone who missed yesterday's session we use this form, just to kind of get a feel for everyone's.

9
00:01:16,620 --> 00:01:24,750
Michael Akira Lee Hayashi: background why they're interested in the class things like that, so if you want to take a moment and fill out the rest of it, that would be awesome that helps us get a sense for what kind of.

10
00:01:25,770 --> 00:01:34,680
Michael Akira Lee Hayashi: What kind of technical background, people are coming in with and then that can help us kind of tailor things this year and and the next time we do this.

11
00:01:46,290 --> 00:01:58,740
Michael Akira Lee Hayashi: Right um so all that's kind of wrapping up, I wanted to go around and see if anyone has questions or anything from yesterday's lectures and labs that focused on deterministic go D type models.

12
00:02:06,930 --> 00:02:10,680
Caroline Godfrey: And from one of the lab problems, I was having trouble getting it to actually like.

13
00:02:11,400 --> 00:02:13,200
Caroline Godfrey: Give me the output.

14
00:02:13,800 --> 00:02:16,800
Michael Akira Lee Hayashi: Okay um which one you don't.

15
00:02:17,190 --> 00:02:23,730
Caroline Godfrey: know and, if you want, I can just share my screen of what I did that easier.

16
00:02:24,720 --> 00:02:25,170
yeah.

17
00:02:26,400 --> 00:02:28,500
Caroline Godfrey: One with weight and age.

18
00:02:28,980 --> 00:02:29,430
Okay.

19
00:02:30,810 --> 00:02:31,530
Michael Akira Lee Hayashi: Let me.

20
00:02:32,160 --> 00:02:33,840
Michael Akira Lee Hayashi: Let me also pull up the.

21
00:02:35,220 --> 00:02:42,480
Caroline Godfrey: I looked at the key and I it looked like I typed something pretty similar emily's it slightly different words for like the name of the function.

22
00:02:44,190 --> 00:02:48,030
Caroline Godfrey: For similar just didn't give me an answer it didn't say error just said.

23
00:02:49,890 --> 00:02:52,800
Michael Akira Lee Hayashi: Okay yeah let's let's take a quick look that.

24
00:02:54,570 --> 00:02:55,200
Michael Akira Lee Hayashi: Also.

25
00:02:56,340 --> 00:03:02,280
Michael Akira Lee Hayashi: pull that up i'm going to download the files from a slab so I have them handy.

26
00:03:09,180 --> 00:03:15,480
Michael Akira Lee Hayashi: Okay, so this one is off of which which particular lab problem which number.

27
00:03:16,260 --> 00:03:16,980
Caroline Godfrey: Let me.

28
00:03:26,280 --> 00:03:27,240
Caroline Godfrey: problem to.

29
00:03:28,140 --> 00:03:28,560
Have.

30
00:03:29,970 --> 00:03:30,750
Caroline Godfrey: options.

31
00:03:31,290 --> 00:03:31,650
Michael Akira Lee Hayashi: got it.

32
00:03:32,430 --> 00:03:33,300
um.

33
00:03:34,680 --> 00:03:43,980
Caroline Godfrey: So this is how I coded the function is to do sequels function of common eight.

34
00:03:46,110 --> 00:03:48,540
Caroline Godfrey: Sorry, and then, if.

35
00:03:53,790 --> 00:04:05,760
Caroline Godfrey: If if age is less than or equal to five then dosage equals wait times five else dosage equals wait times 10 and then I did Joseph 17 comma for.

36
00:04:06,900 --> 00:04:07,230
Caroline Godfrey: Saying.

37
00:04:09,210 --> 00:04:09,420
Caroline Godfrey: Ah.

38
00:04:09,630 --> 00:04:12,390
Michael Akira Lee Hayashi: OK so um so when you call a function.

39
00:04:13,470 --> 00:04:15,780
Michael Akira Lee Hayashi: Within a function returns a value which.

40
00:04:17,430 --> 00:04:24,180
Michael Akira Lee Hayashi: Which in in general they do you need to assign that value to another variable so you'd want to do something like.

41
00:04:25,620 --> 00:04:27,660
Michael Akira Lee Hayashi: I don't know tests, those people.

42
00:04:29,370 --> 00:04:37,350
Michael Akira Lee Hayashi: So 17 four so something like this and then that variable test dose will then show up in the air.

43
00:04:38,370 --> 00:04:41,700
Michael Akira Lee Hayashi: And it can be used for other sorts of purposes.

44
00:04:43,590 --> 00:04:45,630
Michael Akira Lee Hayashi: You may also need to.

45
00:04:47,220 --> 00:04:53,670
Michael Akira Lee Hayashi: I can't remember are implicitly returns, but I like to do explicit returns in in.

46
00:04:54,870 --> 00:05:05,970
Michael Akira Lee Hayashi: In our functions, so at the end of your function code what you probably want to do is something like whatever whatever you named the variable that storing the dose you'll want to add a statement that's like return.

47
00:05:09,600 --> 00:05:10,770
Michael Akira Lee Hayashi: Return dose.

48
00:05:12,900 --> 00:05:13,380
Michael Akira Lee Hayashi: So.

49
00:05:14,850 --> 00:05:19,110
Michael Akira Lee Hayashi: Let me, let me switch over to my own screen shared here and i'll write a.

50
00:05:22,590 --> 00:05:23,190
Michael Akira Lee Hayashi: So, like.

51
00:05:25,740 --> 00:05:28,350
Michael Akira Lee Hayashi: myself how our functions look.

52
00:05:32,910 --> 00:05:33,150
Michael Akira Lee Hayashi: To.

53
00:05:35,100 --> 00:05:37,860
Michael Akira Lee Hayashi: fix arguments call this.

54
00:05:42,630 --> 00:05:42,990
Michael Akira Lee Hayashi: later.

55
00:05:44,040 --> 00:05:47,820
Michael Akira Lee Hayashi: we're going to get this function this place right.

56
00:05:52,890 --> 00:05:53,700
Michael Akira Lee Hayashi: And then.

57
00:05:55,650 --> 00:05:59,970
Michael Akira Lee Hayashi: we're going to do something like I don't know this isn't real but joe's equals wait.

58
00:06:03,930 --> 00:06:04,470
Michael Akira Lee Hayashi: And then.

59
00:06:06,300 --> 00:06:17,370
Michael Akira Lee Hayashi: So what this does is this just tells that reminds the function what's actually supposed to come out so if you do something like even 17 type equals four.

60
00:06:24,810 --> 00:06:31,770
Michael Akira Lee Hayashi: Then this should all get this should be saved in the environment so that's us the usable I should save this.

61
00:06:49,200 --> 00:06:49,650
Michael Akira Lee Hayashi: So.

62
00:06:53,130 --> 00:07:02,070
Michael Akira Lee Hayashi: So you can see that, after calling this function and assigning value to a variable you get that value shown in the in the environment viewer.

63
00:07:03,180 --> 00:07:07,830
Michael Akira Lee Hayashi: And if you wanted to see anything about it, I think it's friends.

64
00:07:09,090 --> 00:07:11,640
Michael Akira Lee Hayashi: might be disk can't remember our print.

65
00:07:12,930 --> 00:07:17,520
Michael Akira Lee Hayashi: So there's a few things about this overall flow that I want to point out.

66
00:07:19,140 --> 00:07:28,110
Michael Akira Lee Hayashi: One is that and this might be redundant to things yesterday by default when you're running a programming language, like our and when you're writing a script.

67
00:07:28,860 --> 00:07:36,990
Michael Akira Lee Hayashi: The default behavior is not necessarily to give you any output, so if I don't have this printed statement and I run the script.

68
00:07:37,440 --> 00:07:46,620
Michael Akira Lee Hayashi: Then, my expectation is not necessarily going to be that i'm going to get any console up, but the way our studio works when it runs a script is it does tell you the lines that it.

69
00:07:47,040 --> 00:07:53,580
Michael Akira Lee Hayashi: ran so it tells me I read this line this line this line this line this line this line, but it doesn't tell me what came out.

70
00:07:54,390 --> 00:08:00,540
Michael Akira Lee Hayashi: It does store those values, because our stores any variable value in its global environment.

71
00:08:00,840 --> 00:08:14,430
Michael Akira Lee Hayashi: So our studio provides a way to view everything that's in the global environment, but none of this is necessarily coming out in like printed console out that like if I ran the script from from the command line or whatever, if I can make it do that.

72
00:08:16,050 --> 00:08:26,820
Michael Akira Lee Hayashi: terminal facility here I probably don't worry about it, if I ran this from the command line, then it wouldn't give me any up, but like i'd run the script and it just give me kind of a.

73
00:08:27,810 --> 00:08:34,500
Michael Akira Lee Hayashi: Blank next cursor that's actually fine and that's real to what I told the code to do right now, if I gave it a print statement.

74
00:08:36,270 --> 00:08:40,710
Michael Akira Lee Hayashi: What this print statement is doing is it's telling the script after you've done all this calculation.

75
00:08:41,940 --> 00:08:58,260
Michael Akira Lee Hayashi: Send whatever is in the variable dose to the console So that is where we actually see something get returned out of this or something get printed or sent back out to console from this function that we happen to right so.

76
00:08:59,460 --> 00:09:02,250
Michael Akira Lee Hayashi: One one kind of general tip is that.

77
00:09:02,700 --> 00:09:14,430
Michael Akira Lee Hayashi: When you're writing code, I like to study my code when i'm when i'm developing a function or anything like that, with print statements, the reason being it enables me to check my work at different stages along the way, so even things like.

78
00:09:14,730 --> 00:09:22,020
Michael Akira Lee Hayashi: In this dose calculator function suppose I want to make sure that the weight and height arguments are getting past correct like do something like print late.

79
00:09:25,170 --> 00:09:28,140
Michael Akira Lee Hayashi: And so now my expectation is when I run this code.

80
00:09:29,580 --> 00:09:32,520
Michael Akira Lee Hayashi: The only things that I should see are the values of weight and height.

81
00:09:34,260 --> 00:09:40,770
Michael Akira Lee Hayashi: Notably, though I will only see those values printed out if I call the dose calculator function if I don't.

82
00:09:43,170 --> 00:09:50,670
Michael Akira Lee Hayashi: Then, nothing gets printed the reason for this being that the print statements are encapsulated within the dose calculator function, therefore, they are only called.

83
00:09:50,970 --> 00:10:01,920
Michael Akira Lee Hayashi: When the dose calculator function gets called if I wanted to know what the weight and height values were without calling the dose calculator function them I would need to do something like this.

84
00:10:07,410 --> 00:10:13,560
Michael Akira Lee Hayashi: And now, these get printed just kind of in the when when the script executes after after these lines.

85
00:10:15,360 --> 00:10:25,320
Michael Akira Lee Hayashi: another couple of things that I will point out sort of about a little bit about coding architecture and coding design I think it's I think it's often important to.

86
00:10:26,130 --> 00:10:34,890
Michael Akira Lee Hayashi: Put a small amount of thought into the names that you give your variables and your functions, so when I write a function I usually give it a name that's.

87
00:10:35,940 --> 00:10:51,540
Michael Akira Lee Hayashi: descriptive sometimes to the point of of a lengthy enos about what that function does so in this case for that particular problem in the lab this thing is designed to this function is intended to calculate the dose or some drug so i've named the function dose calculate.

88
00:10:52,770 --> 00:10:57,300
Michael Akira Lee Hayashi: The function takes two arguments called weight and height, so I call those arguments weight and height.

89
00:10:57,660 --> 00:11:07,200
Michael Akira Lee Hayashi: And it is supposed to calculate a thing it's supposed to calculate while a dose of some drug, so I call that thing dose and this just gives me shorthand to make sure that.

90
00:11:07,680 --> 00:11:14,130
Michael Akira Lee Hayashi: it's easier to read my code, it can help kind of make my code self documenting in some sense, because when I read this code.

91
00:11:14,490 --> 00:11:21,840
Michael Akira Lee Hayashi: it's fairly straightforward to pick up on this sort of human readable words in here that make it easier to parse what this thing's actually doing.

92
00:11:22,200 --> 00:11:33,630
Michael Akira Lee Hayashi: In the same way that when i'm writing the rest of the script and when i'm when i'm writing variables in the rest of the script I usually want to give them names that are somewhat descriptive to their function, and not just like.

93
00:11:34,830 --> 00:11:41,970
Michael Akira Lee Hayashi: A and B, so you might have seen, and this is, this is a personal pet peeve of mine.

94
00:11:43,650 --> 00:11:51,660
Michael Akira Lee Hayashi: myself have seen in the code DEMO files that the variables renamed like thing and thing, two and three don't do this do not do this when you write your own code.

95
00:11:53,040 --> 00:11:53,850
Michael Akira Lee Hayashi: This is lazy.

96
00:11:55,440 --> 00:12:02,670
Michael Akira Lee Hayashi: And even if the variables are kind of trivial things try to at least give them some sort of name which reflects what they're doing.

97
00:12:02,970 --> 00:12:12,930
Michael Akira Lee Hayashi: or better yet give them a name that also reflects what kind of type they have, so this is an integer right five five presumably is being stored as an integer so we could call this.

98
00:12:14,340 --> 00:12:15,120
Michael Akira Lee Hayashi: I don't know.

99
00:12:19,770 --> 00:12:21,840
Michael Akira Lee Hayashi: Something like that or.

100
00:12:28,980 --> 00:12:35,550
Michael Akira Lee Hayashi: But, in general, you want to try to give yourself as much information as you can, or maybe the way I should put this as you should give.

101
00:12:35,760 --> 00:12:45,090
Michael Akira Lee Hayashi: yourself in two months as much information as you can to figure out what your self right now actually meant at the time, because I don't know about I don't know about you, but.

102
00:12:45,990 --> 00:13:01,950
Michael Akira Lee Hayashi: me two months ago was a giant moron and I don't understand the way he thought about anything so if you're remotely like that then do yourself a favor and give things names that are somewhat helpful to remember and that tell you something about what the code is doing.

103
00:13:04,050 --> 00:13:06,720
Michael Akira Lee Hayashi: There are a couple of other things kind of generally about.

104
00:13:08,160 --> 00:13:17,820
Michael Akira Lee Hayashi: kind of about our but that sort of general programming language things one is a little bit specific to the way that we're working with our through our studio so.

105
00:13:18,180 --> 00:13:25,950
Michael Akira Lee Hayashi: Again this This may, this may be review, but I think it's kind of worth repeating, even if it is our studio is a thing called an integrated development environment.

106
00:13:26,760 --> 00:13:41,370
Michael Akira Lee Hayashi: And that is a bunch of words that mean it encapsulates both a text editor that lets you write code some facilities to execute your code, like the the run button this run button that you shouldn't use the source button, which you should.

107
00:13:43,380 --> 00:13:50,640
Michael Akira Lee Hayashi: But also things like a console that runs an active our session so you can test stuff So if I wanted to assign variables.

108
00:13:52,890 --> 00:14:00,510
Michael Akira Lee Hayashi: I can do that, using the console, it also has stuff like will package management file management, whatever.

109
00:14:01,980 --> 00:14:08,400
Michael Akira Lee Hayashi: But this environment viewer and the environment viewer is something that I wanted to talk a little bit about because.

110
00:14:09,240 --> 00:14:26,430
Michael Akira Lee Hayashi: This is a thing that kind of indicates the way that our studio runs so normally when you run a script from the command line if I if I were running a Python script I would invoke that script it would run and it would clean itself up and eggs, and by that I mean.

111
00:14:27,630 --> 00:14:35,400
Michael Akira Lee Hayashi: The program would create all the variables, it needs it would calculate stuff based off of them, it would it would update those variables.

112
00:14:35,940 --> 00:14:39,690
Michael Akira Lee Hayashi: But then it would delete them all, when it's done it wouldn't it wouldn't keep those in my computer member of.

113
00:14:40,590 --> 00:14:48,480
Michael Akira Lee Hayashi: Our studio as long as in our studio session is running and as long as you haven't cleaned out the global environment holds every variable that you create.

114
00:14:48,960 --> 00:15:02,310
Michael Akira Lee Hayashi: In memory in the our studio session so as long as this our studio window is running and I don't clear it these variables are stored in my computer's memory there they're going to SIP it for for me to use However, I want.

115
00:15:03,630 --> 00:15:17,520
Michael Akira Lee Hayashi: This is nice in one sense, because that means that I can manipulate or or test things about the variables that are stored in the global environment, so if I wanted to do something in the console to test my dose calculator function, I might do something like print.

116
00:15:19,590 --> 00:15:20,460
Michael Akira Lee Hayashi: Time times.

117
00:15:22,020 --> 00:15:23,400
Michael Akira Lee Hayashi: I divided by 100.

118
00:15:24,750 --> 00:15:36,300
Michael Akira Lee Hayashi: And it's going to it's going to give me a value, but the important thing is that it was able to understand that the variable weight in the variables are stored in the global environment and are still stored in memory on my computer, so it can access them.

119
00:15:37,860 --> 00:15:43,590
Michael Akira Lee Hayashi: This is cool, but this is also dangerous because suppose I do suppose, this is what my code looks like now.

120
00:15:44,670 --> 00:15:47,880
Michael Akira Lee Hayashi: i'm no longer define a variable called weight or height anywhere.

121
00:15:49,140 --> 00:15:51,930
Michael Akira Lee Hayashi: So if I were to run this code.

122
00:15:53,790 --> 00:16:01,110
Michael Akira Lee Hayashi: I shouldn't expect it to work right because I didn't define a variable called waiter height, but i'm asking for those things to be printed.

123
00:16:01,770 --> 00:16:08,400
Michael Akira Lee Hayashi: And yet, when I ran this bit of code it did it printed a value for waiting for new devalued for height.

124
00:16:09,030 --> 00:16:14,850
Michael Akira Lee Hayashi: The reason for this is that those variables are still stored in the global environment, because our studio.

125
00:16:15,300 --> 00:16:27,480
Michael Akira Lee Hayashi: hasn't cleaned up, yet it nobody's told it to clean itself up so if I want to work from kind of a proper clean test environment, I have to actually go and clean out.

126
00:16:28,380 --> 00:16:37,050
Michael Akira Lee Hayashi: The global environment now if I try to run this thing it should complain it me because neither weight nor height exist anywhere so.

127
00:16:38,130 --> 00:16:50,280
Michael Akira Lee Hayashi: One thing that you want to keep in mind when you're working with our studio especially or, more generally, any idea that kind of keeps a session of its program running in memory wallets going is that.

128
00:16:51,300 --> 00:16:59,160
Michael Akira Lee Hayashi: Each time, you run your thing unless you're very confident with the variables you're working with you probably want to clean out your global environment, so that.

129
00:16:59,970 --> 00:17:08,970
Michael Akira Lee Hayashi: vestigial bits don't hang around essentially because they will if you're if you're not careful, like like here if I change this back to.

130
00:17:12,240 --> 00:17:12,900
Michael Akira Lee Hayashi: weight and height.

131
00:17:14,010 --> 00:17:21,270
Michael Akira Lee Hayashi: And I try to calculate using the old values variables that I put in because i'd forgotten, which happens.

132
00:17:23,010 --> 00:17:36,870
Michael Akira Lee Hayashi: This function still runs, even though it probably shouldn't because well A and B are still stored in memory in the act of our studio session so this will continue to run until I clear out my.

133
00:17:37,560 --> 00:17:43,290
Michael Akira Lee Hayashi: Global Environment try to run it again, and then it complains that it doesn't have a or B, because I didn't change the variable so.

134
00:17:44,730 --> 00:17:54,510
Michael Akira Lee Hayashi: So this is something to be a little bit careful with when you're when you're writing a script, especially when you're kind of testing stuff because that's the point where you're often just kind of.

135
00:17:54,780 --> 00:17:59,820
Michael Akira Lee Hayashi: Throwing stuff at the wall to see what sticks to try to get something to work you're creating different variables you're.

136
00:18:00,120 --> 00:18:07,710
Michael Akira Lee Hayashi: deciding that you don't like their names you're naming them something else you're calling functions in different ways and that's often more stuff will get lost in that shuffle so.

137
00:18:08,280 --> 00:18:12,930
Michael Akira Lee Hayashi: I think in one of the code examples, Murcia also included a line at the top, this thing rm.

138
00:18:13,860 --> 00:18:25,020
Michael Akira Lee Hayashi: list equals Ls i'm pretty sure what this thing does is killed is clears out the global environment, so if you always make sure to run this thing at the top of the script then well you'll you'll accomplish that function.

139
00:18:26,250 --> 00:18:31,740
Michael Akira Lee Hayashi: The other thing i'd like to point out, is why was it that I said there's this run button that you shouldn't use and a source button, that you should.

140
00:18:32,490 --> 00:18:40,920
Michael Akira Lee Hayashi: The reason for that is that this run buttons default behavior is to execute the current line of your of whatever your cursor happens to be on So if I run this here.

141
00:18:41,370 --> 00:18:50,790
Michael Akira Lee Hayashi: It reads this slide executes it and move the cursor down the line firearms again that's the next line and so on, until until I get bored of things.

142
00:18:51,600 --> 00:19:03,030
Michael Akira Lee Hayashi: Why don't I like because, when a computer runs a program, generally speaking, the computer executes code in that program starting from the top going down to the bottom sequential.

143
00:19:04,950 --> 00:19:22,650
Michael Akira Lee Hayashi: So as much as possible, you want to write code that is able to do that, that starts from the top goes to the bottom, runs every single line doesn't doesn't throw an error along the way this button lets you not do that this button lets you run your code in arbitrary order so I could.

144
00:19:23,670 --> 00:19:32,220
Michael Akira Lee Hayashi: I could, for example, run this line, then run this stuff then run this and then run this.

145
00:19:33,540 --> 00:19:40,680
Michael Akira Lee Hayashi: And that doesn't seem like it was particularly dangerous right now what it does allow, though, is something like.

146
00:19:42,960 --> 00:19:52,890
Michael Akira Lee Hayashi: This so again, this also tends to be a particular interaction between the way that some of the functionality of the studio it works and and like.

147
00:19:54,450 --> 00:20:07,440
Michael Akira Lee Hayashi: Well it's just interactions between particular functionality there there's not more to it so right now, if I if I were to look at this particular script or someone handed me this script i'd be like this isn't going to work right because.

148
00:20:09,210 --> 00:20:18,780
Michael Akira Lee Hayashi: Nobody defined this function anywhere so i'm not gonna be able to do, but if someone told me well, but when I ran it myself in an earlier version it worked My suspicion would be.

149
00:20:19,530 --> 00:20:26,910
Michael Akira Lee Hayashi: They probably ran it in in our studio session, in which the variables and the function that we're actually needed for this thing.

150
00:20:27,720 --> 00:20:36,420
Michael Akira Lee Hayashi: still existed, where, if I try to run this thing right well if I tried to run this thing right now, it will work right because, again, all that stuff is sick if I.

151
00:20:37,080 --> 00:20:47,460
Michael Akira Lee Hayashi: Clean my environment and try to run it it won't so by forcing yourself to run code fully sequentially in order and never jumping around.

152
00:20:48,030 --> 00:21:02,940
Michael Akira Lee Hayashi: You force yourself to write your code in a more repeatable way and in a way that you can hand it off to someone else they could run the whole thing, however, they want, and it will be guaranteed to produce the same result or near enough as possible.

153
00:21:06,120 --> 00:21:14,790
Michael Akira Lee Hayashi: There was one other point i'd wanted to make and it's kind of slipping my mind, and if I think about it, and my brain oh Oh, I remember the other thing I wanted to mention is.

154
00:21:15,570 --> 00:21:24,240
Michael Akira Lee Hayashi: You might notice that I use the same name to the arguments that get passed into this function as the variables that get passed as those arguments in the function.

155
00:21:24,570 --> 00:21:34,620
Michael Akira Lee Hayashi: This sometimes causes a certain amount of confusion, largely due to the fact that there's kind of two ways to use variable names one is when you're defining a function.

156
00:21:34,920 --> 00:21:48,540
Michael Akira Lee Hayashi: And you tell the function which arguments it's going to take another way of saying that is what variables will be passed into that function you usually want to give those arguments names that will be helpful for you to use in the code of the function.

157
00:21:49,680 --> 00:21:53,400
Michael Akira Lee Hayashi: In the same way that when you're defining variables just an open code.

158
00:21:54,540 --> 00:22:07,950
Michael Akira Lee Hayashi: it's helpful to give yourself names for those variables that are helpful for you to read and sometimes there's only so many ways to say wait right what else would I picture like I could use w I suppose going to use shorthand.

159
00:22:09,630 --> 00:22:19,860
Michael Akira Lee Hayashi: And, and sometimes little things like this can help you keep straight like what is an argument defined within the scope of a function versus what is a variable defined within the scope of the script.

160
00:22:21,780 --> 00:22:32,730
Michael Akira Lee Hayashi: But notice that this code doesn't actually really care what I call the variables in the script itself, I could actually call this dumb stupid useless.

161
00:22:39,480 --> 00:22:43,950
Michael Akira Lee Hayashi: useless name to, and then I could pass those things here.

162
00:22:46,200 --> 00:23:00,060
Michael Akira Lee Hayashi: This will actually work fine because the function does not actually care what name, you gave the variables that were passed to it, it just cares that they behave the way that it requires them to behave in the code itself so when I run this thing.

163
00:23:01,380 --> 00:23:06,960
Michael Akira Lee Hayashi: This well would work by from those out this works.

164
00:23:08,370 --> 00:23:24,450
Michael Akira Lee Hayashi: Because these variables w H their scope is restricted to within the function, where these variables, these are global variables that that are usable throughout the entire thing, and they can be passed into functions, however, you like so.

165
00:23:24,510 --> 00:23:31,260
Caroline Godfrey: Eric problem with just putting the numbers like directly in so like dose calculator you know 17 come up for.

166
00:23:32,250 --> 00:23:34,230
Caroline Godfrey: Rather variables.

167
00:23:34,860 --> 00:23:42,510
Michael Akira Lee Hayashi: So this is fine, this kind of thing is fine right and this this works too, and I wouldn't have had to define any of this.

168
00:23:44,730 --> 00:24:00,900
Michael Akira Lee Hayashi: So this behaves in exactly the same way, the one caveat, I would say is that this works well, if you don't have to call this function, a lot of times, or if you're very confident that you're going to remember what.

169
00:24:03,120 --> 00:24:16,170
Michael Akira Lee Hayashi: What each of these things means so if this were if this were one function call in a really, really big script that's doing a lot of different things I would generally default to actually defining.

170
00:24:18,270 --> 00:24:19,350
Michael Akira Lee Hayashi: Variable names.

171
00:24:21,690 --> 00:24:33,120
Michael Akira Lee Hayashi: Such that when I go back to read the script I get a reminder of what's actually being passed in because, if I saw 17 and four in a really lengthy script I probably wouldn't know off the top of my head what those mean.

172
00:24:33,360 --> 00:24:38,250
Michael Akira Lee Hayashi: So it's more of a kind of personal design decision than an operational one.

173
00:24:39,570 --> 00:24:48,390
Michael Akira Lee Hayashi: The one case where it can matter is if I need to count to call this function on a range of values like I need to use a for loop to call this thing on like every weight between.

174
00:24:48,960 --> 00:24:59,910
Michael Akira Lee Hayashi: 60 and 100, for example, then I would definitely want to use variables variable names, because I need to do something like that I need to pass the variable into an insight before.

175
00:25:03,060 --> 00:25:03,750
Michael Akira Lee Hayashi: This might be.

176
00:25:04,830 --> 00:25:11,850
Michael Akira Lee Hayashi: laughs and i'm going to write it as though it's it's matlab syntax because most recently writing that lap and I forgot how to write it.

177
00:25:14,340 --> 00:25:16,320
Michael Akira Lee Hayashi: Are for loop oh it's in.

178
00:25:21,360 --> 00:25:22,080
Michael Akira Lee Hayashi: it's just.

179
00:25:23,250 --> 00:25:32,280
Michael Akira Lee Hayashi: Ours for loop syntax is just matlab like and just Python like to confuse me so it's one of those things, who syntax after look up all the time.

180
00:25:33,750 --> 00:25:38,940
Michael Akira Lee Hayashi: So, this would be a case where I very much would have wanted to use a variable.

181
00:25:40,320 --> 00:25:54,930
Michael Akira Lee Hayashi: Generally speaking, I tend to favor using variable names, as opposed to just kind of directly sticking a value in just because I I like to write my code to be myself proof.

182
00:25:56,130 --> 00:26:12,930
Michael Akira Lee Hayashi: And that's that's a way to do it, but no there's not there's not like if the won't break anything if you don't what happens if you don't write return inside of function definition um I think and and I will test this here we can we can see actually.

183
00:26:14,640 --> 00:26:24,390
Michael Akira Lee Hayashi: I think that our will implicitly returned, the last variable calculated, but we can see about this so i'm going to do a couple things.

184
00:26:27,930 --> 00:26:30,180
Michael Akira Lee Hayashi: To kind of test this so then we're going to print.

185
00:26:31,530 --> 00:26:33,990
Michael Akira Lee Hayashi: going to see what comes out of our function when we run this.

186
00:26:38,580 --> 00:26:47,070
Michael Akira Lee Hayashi: So this seems to suggest that it is returning whatever got calculated here, I can also confirm that by doing something like this.

187
00:26:51,210 --> 00:26:58,440
Michael Akira Lee Hayashi: yeah so our does indeed implicitly return, whatever the last variable created it in kind of linear order was.

188
00:26:59,790 --> 00:27:04,530
Michael Akira Lee Hayashi: Which means that you don't technically need a return statement but, like many of the.

189
00:27:05,700 --> 00:27:07,080
Michael Akira Lee Hayashi: Many of the kind of.

190
00:27:08,250 --> 00:27:21,600
Michael Akira Lee Hayashi: coding style things that I will say I would highly recommend that you use a return statement, so that you don't accidentally do something like this, when you're writing a function and then perpetually get the wrong answer out of it so.

191
00:27:23,070 --> 00:27:32,610
Michael Akira Lee Hayashi: it's another case of you have the flexibility to write your code in a couple different ways, but sometimes one of those ways will do you more or less favors in the future.

192
00:27:33,960 --> 00:27:43,560
Michael Akira Lee Hayashi: Oh, I don't think actually like this, the source button this thing runs your entire script as though you had done this.

193
00:27:49,110 --> 00:27:52,710
Michael Akira Lee Hayashi: Oh i'm probably not the right working directory on my.

194
00:27:54,390 --> 00:27:54,750
Michael Akira Lee Hayashi: sister.

195
00:27:58,140 --> 00:27:58,470
Michael Akira Lee Hayashi: There we go.

196
00:27:59,610 --> 00:28:08,130
Michael Akira Lee Hayashi: So this basically is a way of forcing our to run your program a little bit more like it would, if you were running it from like a terminal instance.

197
00:28:08,610 --> 00:28:18,120
Michael Akira Lee Hayashi: It literally runs everything from line one to whatever your last line is in sequential order it doesn't stop it just goes, all the way, so this.

198
00:28:18,840 --> 00:28:25,350
Michael Akira Lee Hayashi: I I would I would highly recommend getting in the habit of just using this to run your script every single time you run it.

199
00:28:25,740 --> 00:28:35,250
Michael Akira Lee Hayashi: Even if you're only testing a small change, and if you are testing a small change to your script, then I would recommend that you studied your thing with print statements, so that you can see what comes out when you run it.

200
00:28:36,810 --> 00:28:42,900
Michael Akira Lee Hayashi: This just forces you to get in the habit of writing code that can run all the way through without without breaking somewhere.

201
00:28:44,310 --> 00:29:01,140
Michael Akira Lee Hayashi: Any other questions from yesterday, or about about our about programming in general, I have a lot of strong opinions about about programming um what is the one at the beginning of each resolving I think this is it's just enumerating the lines of the result itself.

202
00:29:03,510 --> 00:29:11,010
Michael Akira Lee Hayashi: Like if it were a multi line string it would print, it would have additional lines, let me see if I can force it to do something that would have.

203
00:29:22,650 --> 00:29:25,050
Michael Akira Lee Hayashi: doesn't it doesn't understand formatted strings.

204
00:29:31,260 --> 00:29:35,460
Michael Akira Lee Hayashi: i'd wager that this is probably just going to enumerate the lines of the.

205
00:29:36,870 --> 00:29:42,360
Michael Akira Lee Hayashi: Of the print statement if I gave it okay i'm curious now so i'm going to do something slightly done.

206
00:29:49,110 --> 00:29:50,280
Michael Akira Lee Hayashi: And going to.

207
00:29:54,930 --> 00:29:59,190
Michael Akira Lee Hayashi: win to feed the thing the first little bit of pride and prejudice we'll see what comes out.

208
00:30:00,720 --> 00:30:01,560
Michael Akira Lee Hayashi: So here we go.

209
00:30:03,930 --> 00:30:06,750
Michael Akira Lee Hayashi: Oh, this is going to have weird quotes in it, oh no that's bad.

210
00:30:08,070 --> 00:30:18,930
Michael Akira Lee Hayashi: Let me see if I can get on that quotation marks for the record, are occasionally a bit of a bugbear in in strings and we'll see why, if this okay so.

211
00:30:19,440 --> 00:30:31,200
Michael Akira Lee Hayashi: Oh no it doesn't automatically delineate so I don't actually know what that one means at the end of each result I think if I told it if I if I were doing a more elaborate print maybe I could get it to do some other stuff but for the time being, I think.

212
00:30:32,250 --> 00:30:34,740
Michael Akira Lee Hayashi: The best principle here is don't worry about it it'll be fine.

213
00:30:37,590 --> 00:30:48,570
Michael Akira Lee Hayashi: don't worry preferred ID for Python I actually don't I I I use visual studio code or vs code to write Python actually is vs code to write pretty much everything that's not our.

214
00:30:49,680 --> 00:30:54,300
Michael Akira Lee Hayashi: it's a it's a Microsoft product, but it is surprisingly good.

215
00:30:56,340 --> 00:31:08,580
Michael Akira Lee Hayashi: it's it's it's free it's open source, you can run it on any operating system you like it has just enough nice features, to be helpful, without having enough to make it kind of get in the way.

216
00:31:09,090 --> 00:31:20,820
Michael Akira Lee Hayashi: It integrates nicely with things like github it can even do remote connections like it's really, really powerful but it's not like overbearing so that's my favorite thing to write.

217
00:31:22,320 --> 00:31:27,090
Michael Akira Lee Hayashi: When programming or do I prefer to use the available package or to write my own functions my.

218
00:31:29,100 --> 00:31:34,440
Michael Akira Lee Hayashi: My instinct is usually to write my own function unless I need performance.

219
00:31:36,750 --> 00:31:40,530
Michael Akira Lee Hayashi: So it sometimes depends on what i'm doing to if I.

220
00:31:40,860 --> 00:31:54,330
Michael Akira Lee Hayashi: If I were writing an odd model, then I would definitely use D solve, and I would use the numerical integrator available to me and D so because, while I can write a numerical integrator myself, it would be slower than the ones that are in D solve because.

221
00:31:55,140 --> 00:32:04,710
Michael Akira Lee Hayashi: I don't want to write a fortran or a C plus plus numerical integrator and then spend the rest of my life debugging segments, so if it's something that.

222
00:32:06,390 --> 00:32:10,920
Michael Akira Lee Hayashi: If it's something where I don't think performance is crucial and or there just isn't an existing.

223
00:32:11,550 --> 00:32:24,000
Michael Akira Lee Hayashi: If there isn't an existing well maintained well documented package, then i'll write it myself, but if there is such a package than I will for pedagogical purposes, I operate a little bit differently, based on whether i'm.

224
00:32:24,570 --> 00:32:30,930
Michael Akira Lee Hayashi: Teaching or writing my own stuff if i'm writing my own stuff I will do whatever gets me.

225
00:32:32,010 --> 00:32:35,670
Michael Akira Lee Hayashi: A right answer with reasonably little hassle.

226
00:32:36,750 --> 00:32:45,450
Michael Akira Lee Hayashi: Based on based on what I know about the lunch and that may or may not mean that I default to writing my own function versus use something pre existing if i'm writing an r.

227
00:32:45,720 --> 00:32:51,780
Michael Akira Lee Hayashi: i'm a little more likely to write my own functions, because I don't remember the our libraries out there if i'm writing in Python.

228
00:32:52,020 --> 00:33:00,210
Michael Akira Lee Hayashi: i'll rely fairly heavily on things like the compliance side by libraries and pandas because I do use those fairly frequently so it.

229
00:33:01,050 --> 00:33:15,540
Michael Akira Lee Hayashi: It depends a little bit on what i'm doing, I think I think there's a lot of cases where like if you're if you're doing a data processing operation, it can be helpful to write your own function, because that will also ensure that you.

230
00:33:16,590 --> 00:33:24,240
Michael Akira Lee Hayashi: You know that you understand what's going on inside that function if you're doing like I don't know like a linear regression or some kind of statistical model.

231
00:33:24,510 --> 00:33:35,460
Michael Akira Lee Hayashi: You should probably use a package function for it, because those packages hopefully have been validated by some actual statisticians to make sure that the functions work properly, where, if I wrote, for example.

232
00:33:36,390 --> 00:33:45,600
Michael Akira Lee Hayashi: I don't know i'm a linear mixed model or something like that I wouldn't necessarily be confident but i'd have written, one which is provably correct and that's a problem.

233
00:33:47,250 --> 00:33:53,760
Michael Akira Lee Hayashi: Where there's there's other cases where I would be confident that i've written something provably correct so i'm okay writing my own function.

234
00:33:55,980 --> 00:34:03,750
Michael Akira Lee Hayashi: A good question, though, I think, unfortunately, a lot of my answers for this sort of stylistic thing, are you kind of.

235
00:34:04,920 --> 00:34:12,900
Michael Akira Lee Hayashi: You develop your own instincts as you go, can I typed in numerical integration package for our functions, it is de Sol.

236
00:34:15,360 --> 00:34:17,640
Michael Akira Lee Hayashi: That one will be useful.

237
00:34:19,950 --> 00:34:33,300
Michael Akira Lee Hayashi: are also oh I don't know if I don't know if mercy mentioned this, but if you've worked with our at all in the past you've probably interacted with a thing, called the tidy verse and you've probably seen our code that's written using the tidy verse functions.

238
00:34:33,720 --> 00:34:45,480
Michael Akira Lee Hayashi: This is an interesting thing to me because this creates a certain amount of divergence in the way our programmers work as as people in programs, I mean not not like.

239
00:34:46,590 --> 00:34:54,690
Michael Akira Lee Hayashi: Internally, although maybe maybe our programmers are wired early they certainly seem to be because they tolerate the way that our works anyway um.

240
00:34:55,620 --> 00:35:13,620
Michael Akira Lee Hayashi: So the Tigers is a huge package of functions basically intended to smooth over bumps in ours handling of a lot of common input output operations so things like string processing I mentioned yesterday that I do most of like, if I have to parse pride and prejudice, for example.

241
00:35:15,780 --> 00:35:27,750
Michael Akira Lee Hayashi: I would do that in Python because I do not like parsing strings and are, and I find it a lot more straightforward student paper, the tiny verse paves over some of this somewhat, because it has a bunch of functions built in.

242
00:35:28,950 --> 00:35:36,630
Michael Akira Lee Hayashi: That make processing strings much less painful it also introduces some really weird coding practice like.

243
00:35:37,650 --> 00:35:47,280
Michael Akira Lee Hayashi: You probably saw yesterday that are has two ways to assign variables as n equals, like every other same language, and it has this idiotic little thing the Left arrow.

244
00:35:50,310 --> 00:35:56,310
Michael Akira Lee Hayashi: But then the the tidy verse introduces effectively a new assignment thing called a pipe.

245
00:35:58,230 --> 00:35:58,980
Michael Akira Lee Hayashi: I think that's right.

246
00:36:00,090 --> 00:36:12,000
Michael Akira Lee Hayashi: And what a pipe does is it allows you to chain functional operations together, such that all of those functions get called in sequence, and then the result gets rolled all the way back up to one variable.

247
00:36:12,750 --> 00:36:20,190
Michael Akira Lee Hayashi: I don't use case I don't use left it but, but this is this is because i'm not a tidy verse program i'm i'm a.

248
00:36:21,750 --> 00:36:32,670
Michael Akira Lee Hayashi: Like like I think of myself as sort of a general purpose programmer, so I do not like some of the language specific stuff so if, but if you see code that's written with like pipes like that.

249
00:36:34,200 --> 00:36:43,950
Michael Akira Lee Hayashi: Actually, you know what it's it's this isn't that that's right there it is alright tidy verse so little that that I have a hard time remembering sometimes, but if you see stuff like this.

250
00:36:45,300 --> 00:36:46,710
Michael Akira Lee Hayashi: That is.

251
00:36:49,020 --> 00:36:55,020
Michael Akira Lee Hayashi: that's indicating a pipe which is likely chaining together multiple function calls that tends to be its use case.

252
00:36:57,060 --> 00:37:02,400
Michael Akira Lee Hayashi: For those For those of you who would have learned, are you probably did learn this.

253
00:37:05,310 --> 00:37:14,610
Michael Akira Lee Hayashi: If you'd like to know the actual reason I don't like it, it is functionally equivalent to the equal sign there's almost no meaningful case where it will do something different, why don't I like it.

254
00:37:15,840 --> 00:37:19,740
Michael Akira Lee Hayashi: Someone told me, the number of key presses that you have to take to do that thing.

255
00:37:25,650 --> 00:37:32,520
Michael Akira Lee Hayashi: Three it takes three heat presses to do a single assignment operation, and this is really stupid when you can do one.

256
00:37:33,720 --> 00:37:46,920
Michael Akira Lee Hayashi: And i'm late late like I will be the first to admit that i'm a very lazy programmer and, frankly, I am a computer programmer in the first place, because I would much rather a computer do menial grunt work like arithmetic or or.

257
00:37:47,970 --> 00:37:59,820
Michael Akira Lee Hayashi: or reading and things like that, then for me to do it myself, and so, under no circumstances, am I gonna waste extra keystrokes pressing shift comma.

258
00:38:01,230 --> 00:38:04,590
Michael Akira Lee Hayashi: bar that's ridiculous historically.

259
00:38:05,760 --> 00:38:12,960
Michael Akira Lee Hayashi: I know this is off topic we're supposed to talk about to cast panels today, but I figure it's it's, we might as well talk a bit about programming because it's.

260
00:38:13,590 --> 00:38:26,310
Michael Akira Lee Hayashi: it's part and parcel of this class historically keyboards actually had a single button for that they had like natural button to create that arrow which tells you how old R is that it was built around that but as assignment operator.

261
00:38:27,690 --> 00:38:38,940
Michael Akira Lee Hayashi: But we don't do that anymore keyboards don't have that, so why would you why would you use anyway that's that's my equals restart things, how do I decide which programming language, like our or Python to use for a given application.

262
00:38:39,780 --> 00:38:45,750
Michael Akira Lee Hayashi: This is usually a combination of familiarity with the language and familiarity with a feel for what the language is good at.

263
00:38:47,340 --> 00:38:50,370
Michael Akira Lee Hayashi: And this takes different forms, so this can take the form of.

264
00:38:51,450 --> 00:39:04,740
Michael Akira Lee Hayashi: trying to figure out which language performs best for a specific operation, so if I want to write, something which is fast and I don't care about banging my head against the debugging wall for a while i'll write something in c++ because that's going to be really fast.

265
00:39:07,170 --> 00:39:17,640
Michael Akira Lee Hayashi: If I want to do a statistical analysis all use our because it's statistical analysis packages are numerous and generally well validated by actual statisticians who.

266
00:39:18,390 --> 00:39:36,210
Michael Akira Lee Hayashi: Who, I trust, not matter if I want to do kind of if I want to do most of the rest of my work i'll use Python because that's the language with which i'm most familiar, it has broad package, support for scientific computing and related tasks, and I find it easy to read.

267
00:39:38,490 --> 00:39:43,020
Michael Akira Lee Hayashi: So it's very much down to what do I think a language is good at.

268
00:39:44,850 --> 00:39:52,980
Michael Akira Lee Hayashi: Like Python does actually have facilities to do data analysis or like statistical analysis in ways that it claims are equivalent to our.

269
00:39:53,250 --> 00:40:06,420
Michael Akira Lee Hayashi: I actually don't think the functions are as good I don't think the statistical methods implemented in Python are as well, validated are ones, so I just wouldn't use Python for that purpose in the same way that I wouldn't bother to use our for string processing because.

270
00:40:07,350 --> 00:40:11,700
Michael Akira Lee Hayashi: I don't have to I can use Python it's easier for me and it requires a little bit less.

271
00:40:12,990 --> 00:40:29,160
Michael Akira Lee Hayashi: Less stuff us piping in unix is this bad, perhaps, ironically, do I quite like painting, can you fix which which makes me makes me a bit hypocritical Maybe my dislike of types of ours just that i'm intrinsically suspicious of anything vaguely heterodox that are does.

272
00:40:30,840 --> 00:40:42,000
Michael Akira Lee Hayashi: But now I quite like pipes newness although, to be honest, some of this because a lot of what I do if i'm if i'm writing a Shell script which i'm not intrinsically very good at, to begin with, but.

273
00:40:43,650 --> 00:40:53,790
Michael Akira Lee Hayashi: But stuff like that, if I need to pipe commands in unix It really is kind of for one offs like I need a pipe the result of a particular output to.

274
00:40:54,360 --> 00:40:59,490
Michael Akira Lee Hayashi: To something that's subsets that output to give me the actual output and it's usually kind of a one liner like i'm not.

275
00:40:59,910 --> 00:41:11,160
Michael Akira Lee Hayashi: i'm not writing a big long thing if I was writing a longer Shell script and unix I might actually not use pipes as much so that, when I go back to read it, I have an easier time of it but.

276
00:41:11,580 --> 00:41:23,400
Michael Akira Lee Hayashi: No it's not that pipes are intrinsically bad practice in any context, I just have personal preferences over what I like to use and what I like to see us for the purposes of like.

277
00:41:24,600 --> 00:41:30,030
Michael Akira Lee Hayashi: readability and and stuff like that it's Python better for data, cleaning and ricotta.

278
00:41:31,650 --> 00:41:33,060
Michael Akira Lee Hayashi: Sometimes.

279
00:41:34,830 --> 00:41:37,590
Michael Akira Lee Hayashi: I think Python makes like.

280
00:41:39,300 --> 00:41:51,150
Michael Akira Lee Hayashi: parsing text files much easier than our and most of the reason it does, that is, that it's very agnostic about what it reads in like if I give it this this snippet of pride and prejudice.

281
00:41:51,420 --> 00:42:02,070
Michael Akira Lee Hayashi: it's just going to see that as one big long string when it reads that file and and or a big long string with some new line character space throughout it somewhere.

282
00:42:03,150 --> 00:42:10,470
Michael Akira Lee Hayashi: And that actually is in some ways, easier to work with them a language that tries to tries to parse a data file already like.

283
00:42:11,220 --> 00:42:24,150
Michael Akira Lee Hayashi: This is me this is not a great example, but if I had gotten a csv file and I needed to do some some parsing and and maybe a little bit of cleaning on that csv file, I might actually start in Python because it's not gonna.

284
00:42:25,080 --> 00:42:38,370
Michael Akira Lee Hayashi: I don't necessarily want my language, to try to act like that csv file as a data table yet i'll want to get there, eventually, but at first, I might just want to do a little bit of validation to make sure that, like the road lengths are equivalent or that.

285
00:42:40,470 --> 00:42:59,190
Michael Akira Lee Hayashi: or I might need to record some stuff or or things like that, and on a lot of those cases, I will tend to default to Python Is it better well if you're really familiar with tidy verse code, not necessarily because tidy verse functions do give you that kind of string processing stuff.

286
00:43:01,230 --> 00:43:01,620
Michael Akira Lee Hayashi: But.

287
00:43:03,480 --> 00:43:10,650
Michael Akira Lee Hayashi: Again this comes down to kind of the balance of experiencing familiarity with the language versus functionality built in I.

288
00:43:11,280 --> 00:43:16,950
Michael Akira Lee Hayashi: I also tend to be a little bit more in the line of uses few packages as possible in order to do a task and so.

289
00:43:17,310 --> 00:43:22,530
Michael Akira Lee Hayashi: I have to use very like almost no packages to do kind of string parsing in.

290
00:43:22,740 --> 00:43:33,840
Michael Akira Lee Hayashi: Python or like CS initial csv parsing in Python where I would have to use packages to do that commensurate tasks are, and that makes me think that Python is built a little bit better for it but.

291
00:43:34,320 --> 00:43:44,160
Michael Akira Lee Hayashi: Again that's that's a meeting and as as adamant about my own preferences, as I am at the end of the day, this is, this is the sort of thing where.

292
00:43:44,700 --> 00:43:55,350
Michael Akira Lee Hayashi: As programmers you need to develop your own style that works for you, if you really like pipes and left arrows fine just don't tell me about it, please, are all i'll get spice.

293
00:43:56,820 --> 00:44:00,810
Michael Akira Lee Hayashi: But I recognize the technically, this is a valid point of view.

294
00:44:01,830 --> 00:44:07,260
Michael Akira Lee Hayashi: Sorry, no, I really I recognize the technically, this is a point of view that someone might have that we've.

295
00:44:09,810 --> 00:44:11,370
Michael Akira Lee Hayashi: Any other questions I think i'll.

296
00:44:12,780 --> 00:44:16,410
Michael Akira Lee Hayashi: start to move into our lecture material, if not.

297
00:44:18,930 --> 00:44:24,150
Michael Akira Lee Hayashi: i'm always happy to to express my opinions about programming, as I have many of them and they play strong.

298
00:44:29,910 --> 00:44:38,010
Michael Akira Lee Hayashi: Oh say also really quickly you might know so you will notice that marissa and I write code in different ways marissa marissa is primary language was matlab.

299
00:44:38,310 --> 00:44:42,750
Michael Akira Lee Hayashi: So you might notice little water for code looks kind of matlab me if you've ever written matlab.

300
00:44:43,110 --> 00:44:58,470
Michael Akira Lee Hayashi: My code will often look a little Python because I primarily write Python neither of us will tend to write code that looks really are like like if you've if you've worked with an our program or they probably are using pipes and and.

301
00:44:59,340 --> 00:45:08,310
Michael Akira Lee Hayashi: And the Left arrow and a lot of built in tidy verse functions and things like that and they'll put dots there variable names which also don't do that I don't care if our accepts it don't do it.

302
00:45:09,690 --> 00:45:13,800
Michael Akira Lee Hayashi: Our verses are for machine learning versus data or SAS.

303
00:45:15,660 --> 00:45:16,770
Michael Akira Lee Hayashi: whoo that's the thing.

304
00:45:17,790 --> 00:45:21,210
Michael Akira Lee Hayashi: I would never used a disaster anything ever they have.

305
00:45:22,320 --> 00:45:26,100
Michael Akira Lee Hayashi: No valid purpose if you're comfortable with are.

306
00:45:27,180 --> 00:45:28,440
Michael Akira Lee Hayashi: largely because.

307
00:45:31,170 --> 00:45:42,960
Michael Akira Lee Hayashi: State in SAS our statistical programming like just their job is to do statistical analysis and pretty much nothing else, and if you try to do anything else with SAS is going to make you angry, really, really fast.

308
00:45:43,890 --> 00:45:48,690
Michael Akira Lee Hayashi: Or at least that's how I feel when every SAS are can do everything stayed at or SAS camp.

309
00:45:50,310 --> 00:45:58,350
Michael Akira Lee Hayashi: And the language is shockingly better in terms of readability and and accessibility for machine learning specifically.

310
00:46:00,300 --> 00:46:11,370
Michael Akira Lee Hayashi: I mean, in general, if it's our or seder SAS data or SAS bar because state or SAS are like the moment you can kick those things to the curb please do they're also.

311
00:46:11,610 --> 00:46:14,700
Michael Akira Lee Hayashi: Really, expensive and fairly invasive to your system.

312
00:46:17,160 --> 00:46:23,790
Michael Akira Lee Hayashi: yeah data cleaning is really annoying in languages like that, especially because they do implicit like.

313
00:46:24,300 --> 00:46:37,110
Michael Akira Lee Hayashi: file parsing So if you give them a data file it's going to implicitly parse that into some kind of structure and then you're going to have to deal with it, and if you if you need to write a procedural loop like a for loop to iterate ugly clean your data.

314
00:46:38,340 --> 00:46:43,170
Michael Akira Lee Hayashi: That sucks because those languages were not well intended to do that.

315
00:46:44,820 --> 00:46:45,510
Michael Akira Lee Hayashi: And so.

316
00:46:48,420 --> 00:46:57,720
Michael Akira Lee Hayashi: our capacity to do machine learning is heavily dependent on the taxes available to it, of which there are many and of which they're quite powerful now personally.

317
00:46:58,260 --> 00:47:10,080
Michael Akira Lee Hayashi: Here again I would default to Python for machine learning because machine learning is hard from a from kind of a overall work pipeline framework.

318
00:47:12,510 --> 00:47:12,810
Michael Akira Lee Hayashi: Like.

319
00:47:14,310 --> 00:47:26,640
Michael Akira Lee Hayashi: it's seldom a totally plug and play operation and if anyone tells you it is then don't listen to them about anything else, because that is fully wrong machine learning is a really fiddly thing to do that.

320
00:47:27,150 --> 00:47:35,250
Michael Akira Lee Hayashi: That we often don't think enough about kind of the mechanics of like it's really easy to get a bad answer out of naively applying machine learning.

321
00:47:36,690 --> 00:47:45,990
Michael Akira Lee Hayashi: And some of what that means is that you have to think more carefully about what is your data look like going in how have I.

322
00:47:46,680 --> 00:47:58,050
Michael Akira Lee Hayashi: How have I, for example, assigned I don't know distance metrics between between observations in the data so that a clustering algorithm can try to cluster them and.

323
00:47:58,530 --> 00:48:04,770
Michael Akira Lee Hayashi: All of that kind of interstitial stuff The pre processing the validation steps and all of that.

324
00:48:05,190 --> 00:48:15,330
Michael Akira Lee Hayashi: A lot of that stuff I feel like often falls into kind of a general purpose programming realm that I tend to think a language like Python is better at, not to mention Python has really good.

325
00:48:15,930 --> 00:48:24,510
Michael Akira Lee Hayashi: machine learning packages through like psychic learn and things like that, and I think even Google has Python packages out there, or heck I might even default to like.

326
00:48:26,100 --> 00:48:29,820
Michael Akira Lee Hayashi: Go Google has a language that's intended for machine learning or something like that.

327
00:48:30,870 --> 00:48:42,600
Michael Akira Lee Hayashi: So, like, I might actually seek out something that I think implements the most cutting edge well validated machine learning algorithms as opposed to either our or Python in that case so.

328
00:48:45,000 --> 00:48:55,380
Michael Akira Lee Hayashi: That one and I don't do a whole lot of machine learning myself so there's a limited amount which I can speak to this, I might even be tempted to write my own code like if I was implementing hierarchical clustering.

329
00:48:55,650 --> 00:49:04,680
Michael Akira Lee Hayashi: i'll probably write my own function for that, because machine learning is not performant anyway it's really, really slow so it's fine if I use a slow language.

330
00:49:09,120 --> 00:49:14,130
Michael Akira Lee Hayashi: Any other questions or comments or thoughts and take this as an opportunity to drink some coffee.

331
00:49:16,470 --> 00:49:21,150
Michael Akira Lee Hayashi: yeah so Sasha still very common in public health, especially.

332
00:49:22,230 --> 00:49:25,710
Michael Akira Lee Hayashi: The historical free the historical reason for this is actually partly that.

333
00:49:26,910 --> 00:49:39,570
Michael Akira Lee Hayashi: All the CDC uses SAS pretty extensively or used its transitioning one of the other historical reasons is that it was felt that, for whatever reason, results that come out of a.

334
00:49:42,570 --> 00:49:55,950
Michael Akira Lee Hayashi: proprietary software like SAS are more are more likely to hold up in court, then results that come out of an open source language, like our but attitudes are changing on this so.

335
00:49:58,110 --> 00:50:06,000
Michael Akira Lee Hayashi: So I think we're seeing less and less use of sass To be honest, what I would say is like my general advice on.

336
00:50:06,690 --> 00:50:15,840
Michael Akira Lee Hayashi: On SAS is to get through your SAS class as quickly as possible and with as minimal of intellectual effort as possible to avoid engaging with it.

337
00:50:16,200 --> 00:50:24,240
Michael Akira Lee Hayashi: and try to quickly move on to to a real language, because you will not need to use SAS again for.

338
00:50:24,930 --> 00:50:36,180
Michael Akira Lee Hayashi: Except like okay that's not you may need to use SAS again but, frankly, while it is a horribly frustrating language once you've learned a real language it's not hard to go back and fill in the gaps and then get angry.

339
00:50:36,510 --> 00:50:47,880
Michael Akira Lee Hayashi: At SAS at everything that it's bad at, so I would I would not expand an enormous amount of intellectual effort on SAS if I if I were in that position, I know I didn't know semester students.

340
00:50:51,810 --> 00:51:04,470
Michael Akira Lee Hayashi: All right, I would like to get on to our next bit So if you if you do have other questions feel free to put them in chat and I will or unmute mask I will.

341
00:51:05,610 --> 00:51:07,500
Michael Akira Lee Hayashi: I will try to get to them as I noticed them.

342
00:51:10,620 --> 00:51:13,890
Michael Akira Lee Hayashi: And let's talk a little bit about stochastic models.

343
00:51:14,910 --> 00:51:30,300
Michael Akira Lee Hayashi: So yesterday marissa introduced deterministic ones, one of the interesting things about a lot of the deterministic departmental models that you will have seen things like si our models, or even the drug kinetics models farm code dynamic models.

344
00:51:32,100 --> 00:51:45,690
Michael Akira Lee Hayashi: These are considered to be they're essentially modeling the average behavior of a stochastic system so for each of those models, you could think about them as having a stochastic model equivalent out there that.

345
00:51:46,230 --> 00:51:56,580
Michael Akira Lee Hayashi: That this model is just kind of simplifying this is nice because that means that you can work between the two, and you can you can make inferences about the behavior one to the other, to a certain extent.

346
00:51:58,500 --> 00:52:06,120
Michael Akira Lee Hayashi: So today i'm going to try to walk through a number of kind of simple classes of stochastic models.

347
00:52:07,770 --> 00:52:14,310
Michael Akira Lee Hayashi: Later on we're going to spend more time with Markov models and simulation of continuous time Markov chain specifically.

348
00:52:15,210 --> 00:52:27,930
Michael Akira Lee Hayashi: This section is where for folks who are interested in tobacco modeling these might be of particular interest to you things like Markov chains Markov models continuous time Markov chains and some of the simulation routines for those.

349
00:52:30,570 --> 00:52:31,230
Michael Akira Lee Hayashi: chat back.

350
00:52:34,740 --> 00:52:39,120
Michael Akira Lee Hayashi: So this is going to be a slightly notation heavy lecture so bear with me here.

351
00:52:40,590 --> 00:52:52,380
Michael Akira Lee Hayashi: Broadly speaking, a stochastic process is a mathematical description of the time evolution of some system that's subject to randomness So when I say stochastic basically, I mean there's randomness in the thing.

352
00:52:54,510 --> 00:53:08,550
Michael Akira Lee Hayashi: Formally we're going to say that a stochastic process is represented by a collection of random variables we're going to denote this collection bold X of tea and each of those random variables is going to be X one X two of tea and so on.

353
00:53:10,020 --> 00:53:20,880
Michael Akira Lee Hayashi: And each of these is really just characterizing the trajectory of some part of the system, so if this were an si our model, for example, is to cast the cast our model, this might be s T is T an rfp.

354
00:53:22,800 --> 00:53:32,040
Michael Akira Lee Hayashi: But we also need the probability distributions characterizing the chance of changing the state of your system over time.

355
00:53:33,090 --> 00:53:37,980
Michael Akira Lee Hayashi: So call these distributions P and they're going to tell us how we go from.

356
00:53:39,270 --> 00:53:42,570
Michael Akira Lee Hayashi: Time one time to time three two time for and so.

357
00:53:43,740 --> 00:53:54,720
Michael Akira Lee Hayashi: We can model stochastic processes either industry or continuous time, so what I mean by that is discrete time would be day one day two day three day four year one year two year three year for continuous time would mean.

358
00:53:55,320 --> 00:54:06,420
Michael Akira Lee Hayashi: Something can change at any arbitrary time so something could change right now at 9:27am something could change at 928 and 45 seconds and 6700.

359
00:54:07,740 --> 00:54:09,630
Michael Akira Lee Hayashi: Events can happen whenever.

360
00:54:11,340 --> 00:54:16,440
Michael Akira Lee Hayashi: We often think of real complex systems as being stochastic to one degree or another.

361
00:54:17,220 --> 00:54:25,470
Michael Akira Lee Hayashi: Some of what this is is us kind of a simplifying though the enormous amount of variation and things that happened in the world.

362
00:54:26,310 --> 00:54:44,250
Michael Akira Lee Hayashi: down to a sense of well, maybe, maybe at the end of the day, the world is actually fully deterministic and there is no actual like there's no real randomness in the world, but the things that happen are so complex that we can't really tell like we don't have a way to.

363
00:54:45,750 --> 00:54:55,890
Michael Akira Lee Hayashi: To describe every single mechanism of everything that drives the real world, and so, for all intents and purposes, things appear somewhat random.

364
00:54:58,170 --> 00:55:15,510
Michael Akira Lee Hayashi: So this can be a way of kind of encapsulating our uncertainty over certain aspects of the mechanism where like, if I had if I were all knowing then maybe I could treat the system i'm interested in as deterministic but i'm not So there we go, this is what I get.

365
00:55:17,340 --> 00:55:29,370
Michael Akira Lee Hayashi: let's take a look at a really, really simple stochastic process, the newly process is a discrete times to cast a process, so this is going to operate in like a day one day two day three.

366
00:55:30,570 --> 00:55:44,100
Michael Akira Lee Hayashi: kind of kind of style and we're going to say that in a newly process, you have some events and every time you every day or every time you flip a coin or roll a die there's some probability of that event happen.

367
00:55:45,360 --> 00:55:53,370
Michael Akira Lee Hayashi: So a newly process fundamentally is a coin flipping process right if I flip a coin there's some chance it comes up heads some chance it comes up tails.

368
00:55:54,360 --> 00:56:09,690
Michael Akira Lee Hayashi: The next time I flip a coin is the next event that process, and it has some chance of coming up ads in some chance of coming up tips, so we can describe the newly process as a sequence of independent random variables X one X two X three and so on, up to X T.

369
00:56:10,800 --> 00:56:15,750
Michael Akira Lee Hayashi: Where the value of the random variable at any of these times steps is either going to be zero or one.

370
00:56:16,140 --> 00:56:28,410
Michael Akira Lee Hayashi: Another way we think about this is zeros heads one is tales or zero means fail one means pass zero means fail one means success, we can think about a renewal process essentially as a fail success process.

371
00:56:29,910 --> 00:56:42,990
Michael Akira Lee Hayashi: Every time I go to the grocery store do they have the weird small bananas, that I like that answer is binary there's a yes or no, there so each of those times can be thought of as a trial, each time.

372
00:56:44,940 --> 00:56:52,020
Michael Akira Lee Hayashi: Each time, you give someone a drug, they either do or do not have adverse side effects from it, that would be the result of that specific trial.

373
00:56:54,240 --> 00:56:54,600
Michael Akira Lee Hayashi: and

374
00:56:56,040 --> 00:57:11,790
Michael Akira Lee Hayashi: Exit T can take the value of one or it can be a success with probability P now this probability P, and a newly processes is what's called time homogenous it doesn't matter whether it's today, tomorrow, the next day or five years from now, the probability is still the same.

375
00:57:12,900 --> 00:57:23,220
Michael Akira Lee Hayashi: This would be a coin right a coin that I might flip does not care if it's being flipped today or 10 years in the past or 10 years in the future the probability that comes up heads is just the same.

376
00:57:24,900 --> 00:57:38,610
Michael Akira Lee Hayashi: So, here again, to reiterate, we could think about this as a sequence of coin flips success or failures doesn't event happen or not, or one of my favorites arrivals this comes from queueing theory, so this would be like.

377
00:57:40,470 --> 00:57:44,790
Michael Akira Lee Hayashi: Every time someone could did someone show up at the hospital admissions desk yes or no.

378
00:57:46,590 --> 00:57:52,080
Michael Akira Lee Hayashi: there's one kind of subtle interesting thing about really processes with regard to time, which is that.

379
00:57:52,260 --> 00:58:06,810
Michael Akira Lee Hayashi: We don't necessarily specify the amount of time that you lapses between coin fruits, so all that matters for newly process is like the clock of the newly process in some sense is just whenever we flip a coin.

380
00:58:07,980 --> 00:58:16,020
Michael Akira Lee Hayashi: But I could flip two coins in rapid succession and then wait a little while and then flip another coin and those would be X one X two X three.

381
00:58:16,590 --> 00:58:25,380
Michael Akira Lee Hayashi: And we don't we don't know and we don't care for the purpose of the burden of the process, how much time elapsed between those events they could be evenly spaced they might not be.

382
00:58:25,830 --> 00:58:32,910
Michael Akira Lee Hayashi: This actually ends up being kind of helpful mathematically but, but that is a feature of these things yeah question.

383
00:58:33,780 --> 00:58:52,590
Caroline Godfrey: um the distinction between this and say, like a simple decision tree where at each point you have two options is that this you're just tracking like how individuals move through it and then like something that rather than a decision tree is really like basically averaging.

384
00:58:53,970 --> 00:58:54,690
Michael Akira Lee Hayashi: So.

385
00:58:54,750 --> 00:58:59,070
Caroline Godfrey: Being averages, because it seems like I mean a decision tree you're kind of just going through.

386
00:59:00,870 --> 00:59:04,740
Michael Akira Lee Hayashi: Right, so a decision tree can be thought of as being made.

387
00:59:04,800 --> 00:59:05,130
Caroline Godfrey: Up of.

388
00:59:05,190 --> 00:59:06,480
Michael Akira Lee Hayashi: Her newly processes.

389
00:59:06,600 --> 00:59:25,500
Michael Akira Lee Hayashi: Where it there there's not necessarily a really clean correspondence between the two like a decision tree may reflect a particular type of stochastic process but it doesn't necessarily have to be a new process like you could think of a decision tree as a newly process if.

390
00:59:27,930 --> 00:59:32,340
Michael Akira Lee Hayashi: If you think of it, such that, at every stage at every node.

391
00:59:32,940 --> 00:59:37,110
Michael Akira Lee Hayashi: That is a trial and something happens and you go down another note, so that would.

392
00:59:37,350 --> 00:59:55,110
Michael Akira Lee Hayashi: That would be a decision tree is kind of a newly process it's a little weirder because you potentially have different probability distributions over the outcomes that each node and tip for a pure for newly process you just have one probability distribution that tells you like.

393
00:59:56,790 --> 00:59:57,270
Caroline Godfrey: anything.

394
00:59:58,920 --> 00:59:59,220
Michael Akira Lee Hayashi: yeah.

395
01:00:00,030 --> 01:00:05,700
Michael Akira Lee Hayashi: So you can think of a decision tree is essentially being newly like but, having sort of a more complex.

396
01:00:06,780 --> 01:00:17,160
Michael Akira Lee Hayashi: probability distribution because, like the the sequence of events that happens that what what would effectively be the sequence of variables which correspond to the sequence of decisions taking that each node.

397
01:00:17,520 --> 01:00:30,420
Michael Akira Lee Hayashi: The probabilities of those things is governed by the product of the the probabilities at each node as opposed so it's it would no longer be time homogenous in some sense, it would be.

398
01:00:32,340 --> 01:00:37,800
Michael Akira Lee Hayashi: If you think about each node as being a time step that would actually be a time in homogenous process.

399
01:00:39,900 --> 01:00:43,980
Michael Akira Lee Hayashi: So that I think is that's the best way I can come up with.

400
01:00:45,360 --> 01:00:51,030
Michael Akira Lee Hayashi: To kind of correspond to to similar but not quite sort of sort of cousins I guess.

401
01:00:51,300 --> 01:00:51,630
Okay.

402
01:00:55,560 --> 01:00:55,770
Caroline Godfrey: well.

403
01:00:57,510 --> 01:01:07,350
Michael Akira Lee Hayashi: So, because of her newly processes awfully simple that doesn't mean that there's properties about a stick about a newly process that we just know mathematical so.

404
01:01:08,190 --> 01:01:25,080
Michael Akira Lee Hayashi: If we wanted to know the number of events or the number of heads or the number of successes, essentially, the number of time that variable is one in T timestamps this number of events is distributed according to a binomial distribution with unknown mean so T times the.

405
01:01:26,280 --> 01:01:44,550
Michael Akira Lee Hayashi: probability of that effect, so if I have a fair coin that that's got a 5050 chance of flipping heads or tails and I want to know the the average number of heads in 10 timestamps well that's just 10 times Point five or an expectation five heads the distribution is by no meal.

406
01:01:46,080 --> 01:01:46,500
Michael Akira Lee Hayashi: With.

407
01:01:48,330 --> 01:01:51,480
Michael Akira Lee Hayashi: With with its first parameter being T in the second parameter big p.

408
01:01:52,140 --> 01:02:00,930
Michael Akira Lee Hayashi: We can we know the probability of seeing exactly K events and tea time sepsis suppose I want to know the chance that I get exactly three heads and tea time steps.

409
01:02:01,440 --> 01:02:10,500
Michael Akira Lee Hayashi: Then that's given by this expression, so this first term is the convenient or extreme, so this is to choose K, so what this is saying is.

410
01:02:11,520 --> 01:02:19,350
Michael Akira Lee Hayashi: How many different combinations of K events can I get in T timestamps So if I want three.

411
01:02:19,740 --> 01:02:24,930
Michael Akira Lee Hayashi: In 10 times steps, then how many different combinations of three things can I get out of 10 timestamps.

412
01:02:25,290 --> 01:02:36,780
Michael Akira Lee Hayashi: This is actually kind of a cumbersome thing to calculate computationally so this, this is a really slow calculation so whenever there's one of these terms in a mathematical equation.

413
01:02:37,770 --> 01:02:51,180
Michael Akira Lee Hayashi: You may often find that people trying to implement this in a computer have tried to find some way to not have to calculate this term, to use an equivalency that gets around having to calculate it will see one of those very short.

414
01:02:53,070 --> 01:03:02,310
Michael Akira Lee Hayashi: Otherwise it's teach us K times P, to the K times one minus P, to the quantity T minus K you don't necessarily need to belabor what this is.

415
01:03:04,440 --> 01:03:10,110
Michael Akira Lee Hayashi: As some of this essentially is this bit here, Peter the cake, this is sort of the.

416
01:03:11,160 --> 01:03:26,460
Michael Akira Lee Hayashi: The the joint probability of K events right, so if if an event has a probability Point five and i'm looking for three of them, then the probability of three of those is point five to three, and this is the probability of not not that thing.

417
01:03:27,570 --> 01:03:30,600
Michael Akira Lee Hayashi: or not getting not getting K events.

418
01:03:33,960 --> 01:03:41,550
Michael Akira Lee Hayashi: We can also we also have the result that the number of time steps that you need in order to see some number of events.

419
01:03:41,970 --> 01:03:52,590
Michael Akira Lee Hayashi: So the number of times 60 required to see our events which we might call the waiting time is is distributed according to a negative binomial distribution with a known meet.

420
01:03:54,870 --> 01:04:05,100
Michael Akira Lee Hayashi: This is this is handy from kind of that queueing theory arrivals at a clinic thing like suppose I want to know how long it is before three people show up at the clinic.

421
01:04:05,490 --> 01:04:08,940
Michael Akira Lee Hayashi: Well, the probability distribution, for that is known it's negative binomial.

422
01:04:09,390 --> 01:04:17,640
Michael Akira Lee Hayashi: And I can calculate the average waiting time, so if the probability at any given time step if someone showing a clinic is 10%.

423
01:04:18,030 --> 01:04:32,760
Michael Akira Lee Hayashi: And I want to know how long it is until I see 10 people show up to the clinic then it's going to be point one times 10 so that's one divided by point nine so one divided by point nine didn't make my calculator do it because I can't do that.

424
01:04:34,950 --> 01:04:36,960
Michael Akira Lee Hayashi: For about 1.1 time steps.

425
01:04:40,860 --> 01:04:41,220
Michael Akira Lee Hayashi: All right.

426
01:04:44,310 --> 01:04:44,760
Michael Akira Lee Hayashi: I think so.

427
01:04:48,060 --> 01:04:57,390
Michael Akira Lee Hayashi: So this correspondence is is particularly helpful because these three things together are often important things that we want to know about a stochastic process in general.

428
01:04:57,600 --> 01:05:06,420
Michael Akira Lee Hayashi: How many events are going to happen in some time interval how long is it between events or how long is it between a certain number of events.

429
01:05:06,870 --> 01:05:14,010
Michael Akira Lee Hayashi: How likely, are we to see a certain number of events these often our kind of core questions that we'd ask if it's the casting process, but another way.

430
01:05:16,050 --> 01:05:22,830
Michael Akira Lee Hayashi: Seeing K events this might be, what is the chance that we see, for example, five cases of covert in the next day.

431
01:05:23,130 --> 01:05:33,420
Michael Akira Lee Hayashi: Or how long is it an expectation, but before we see 100 new cases of Kobe, these are the sorts of questions that we're likely to ask of our models, regardless of what their model and.

432
01:05:34,080 --> 01:05:42,390
Michael Akira Lee Hayashi: For for things that come from simpler stochastic models, we often just have kind of expressions to calculate these explicitly.

433
01:05:44,610 --> 01:06:02,730
Michael Akira Lee Hayashi: Oh, I will note that i'm using T to represent time steps in the process or the number of trials you'll often see an used because because, like I said in a newly process, you are just counting the number of like heads or the number of coin flips not necessarily the explicit time.

434
01:06:05,220 --> 01:06:21,150
Michael Akira Lee Hayashi: So here's an infectious disease example suppose we have a susceptible person who's going to encounter X infectious people each time they bump into one of those X infectious people there's some probability P that they might become infected.

435
01:06:22,680 --> 01:06:35,970
Michael Akira Lee Hayashi: And K is going to be the number of times contact with an infected resulted in an infection, so we might want to know them what's the probability that this person got infected given contact with these X infectious people.

436
01:06:38,160 --> 01:06:47,580
Michael Akira Lee Hayashi: So how do we, how do we get this well one thing about this problem is that it's actually kind of awkward to calculate directly, why is that because.

437
01:06:49,290 --> 01:07:00,030
Michael Akira Lee Hayashi: there's a bunch of different ways this person could be infected they could have been infected because they contacted this one didn't get infected contacted this one didn't get infected then contacted this one and got infected.

438
01:07:00,720 --> 01:07:05,820
Michael Akira Lee Hayashi: or because they contacted this first person and got immediately infected and then the other context it matter.

439
01:07:06,060 --> 01:07:18,810
Michael Akira Lee Hayashi: or it could have been that they contacted all three of these people didn't get infected then contacted this one and did get effect, so what happens here, essentially, is that the this what we need to know is.

440
01:07:19,890 --> 01:07:22,890
Michael Akira Lee Hayashi: what's the probability first became effect, but to do that.

441
01:07:23,400 --> 01:07:29,190
Michael Akira Lee Hayashi: We might need to calculate all the different ways that the person could have become infected, and that is really tedious.

442
01:07:29,460 --> 01:07:35,100
Michael Akira Lee Hayashi: And we'd like to avoid that friction and this gets worse, the more infectious people this person.

443
01:07:35,430 --> 01:07:42,600
Michael Akira Lee Hayashi: can possibly encounter because that means more combinations more combinations means more calculations more calculations means.

444
01:07:43,020 --> 01:07:55,290
Michael Akira Lee Hayashi: We take what should be a fairly straightforward thing to estimate and turn it into a gigantic computational problem that takes a lot of time, how do we get around this well we do a trick that is often fairly common and working with probability.

445
01:07:56,790 --> 01:08:13,710
Michael Akira Lee Hayashi: we're going to calculate the the logical conference what's the probability that the susceptible does not become infected and it turns out there's really only one way that this can happen the the susceptible person doesn't become infected if none of their contacts results in an infection.

446
01:08:14,790 --> 01:08:19,320
Michael Akira Lee Hayashi: And there's literally only one way, this can happen, so this is cool we're going to calculate that probability so.

447
01:08:20,400 --> 01:08:33,780
Michael Akira Lee Hayashi: Out of all the of all the contacts we're going to do X tues zero, which is great because that's very simple and we're going to use this equation here with K equals zero.

448
01:08:34,920 --> 01:08:38,160
Michael Akira Lee Hayashi: Because K is the number of contacts that did result in an infection.

449
01:08:40,740 --> 01:08:52,110
Michael Akira Lee Hayashi: And we're just going to solve this out this simplifies really nice because anything to the zero is one so, and this is one, so we have a one on the front of this thing which goes away.

450
01:08:53,280 --> 01:09:04,260
Michael Akira Lee Hayashi: X minus zero is X, so we arrived at this expression one minus P, to the X or one minus the probability of infection, to the power of the number of infectious people contacted.

451
01:09:05,640 --> 01:09:10,770
Michael Akira Lee Hayashi: Why is this Nice because now we can actually solve the original problem we know the chance that you didn't get infected.

452
01:09:11,580 --> 01:09:19,830
Michael Akira Lee Hayashi: The probability that you did get infected it's just one minus that so the probability that you got infected now is one minus one minus P, to the X.

453
01:09:20,370 --> 01:09:30,600
Michael Akira Lee Hayashi: And we've answered our original question, a whole lot easier than trying to answer it directly, which I think is, which I think is fun that's that's a rather clever way to go about.

454
01:09:31,740 --> 01:09:32,430
Michael Akira Lee Hayashi: questions.

455
01:09:42,330 --> 01:09:51,540
Michael Akira Lee Hayashi: um for for the record this kind of newly model of infection does kind of secretly underlie some of the notion of mass action transmission like.

456
01:09:51,810 --> 01:10:03,270
Michael Akira Lee Hayashi: In disease transmission models, we tend to assume that every time a person contacts and infectious person there's some probability of infection by that contact, and so we may We may want to know things like.

457
01:10:04,440 --> 01:10:14,610
Michael Akira Lee Hayashi: If I know how many people you contact over over there, infectious variants what's the total chance that you became infected and so you can actually use this simple model to do things like that, like if you wanted.

458
01:10:16,920 --> 01:10:21,570
Michael Akira Lee Hayashi: So, so if you're if you're neurotic and a little bit.

459
01:10:23,490 --> 01:10:26,610
Michael Akira Lee Hayashi: More than a little bit German phobic like I am you might want to know.

460
01:10:27,000 --> 01:10:37,230
Michael Akira Lee Hayashi: If you're sitting on a bus next to someone who could be potentially infectious what's the chance that I could become infectious or that I could become infected due to contact with that person, and I might do something like this, where.

461
01:10:37,590 --> 01:10:50,280
Michael Akira Lee Hayashi: X is the total number of times that I inhale each of those installations is a chance for me to get infected and so, if I know the dose response relationship of the pathogen that's being emitted by the person sitting next to.

462
01:10:51,540 --> 01:10:55,500
Michael Akira Lee Hayashi: Then I could actually calculate the probability that I get infected on the bus.

463
01:10:57,000 --> 01:11:01,950
Michael Akira Lee Hayashi: Which is, which is always fun to do when when you're sitting on the bus in the winter.

464
01:11:06,390 --> 01:11:17,640
Michael Akira Lee Hayashi: So let's move on to a related and slightly more complex process, and then these come up all the time for some processes are really useful because they underlie a lot of other things.

465
01:11:18,600 --> 01:11:29,790
Michael Akira Lee Hayashi: So, unlike a renewal process a facade process is a continuous time stochastic process now we're thinking about something where events happen at random, according to some rate so.

466
01:11:30,390 --> 01:11:40,440
Michael Akira Lee Hayashi: What that rate governs basically, is how frequently the event happens so if I go to the grocery store once a week that's a rate i'm expressing how frequently.

467
01:11:40,800 --> 01:11:49,710
Michael Akira Lee Hayashi: On average, I go to the grocery store now I may go to the grocery store on the same day, every week I may go on different days each week but, on average, I go about once a week.

468
01:11:50,670 --> 01:11:58,530
Michael Akira Lee Hayashi: So here we're going to define the facade processes being comprised of a sequence of random variables s one through St.

469
01:11:59,760 --> 01:12:00,120
Michael Akira Lee Hayashi: Where.

470
01:12:02,400 --> 01:12:05,520
Michael Akira Lee Hayashi: The sub the the subscript for each of these so.

471
01:12:06,570 --> 01:12:10,860
Michael Akira Lee Hayashi: has to be less than the subsequent one because the.

472
01:12:12,270 --> 01:12:15,240
Michael Akira Lee Hayashi: The events course, what are the variables correspond to.

473
01:12:16,350 --> 01:12:20,550
Michael Akira Lee Hayashi: times at which some event happened or the arrival times in the system.

474
01:12:23,100 --> 01:12:34,770
Michael Akira Lee Hayashi: We can also define a related sequence of variables that are going to call X one directs t of waiting times, so what these tell us is how far apart did these events happen.

475
01:12:36,030 --> 01:12:46,440
Michael Akira Lee Hayashi: So this sequence of random variables tells me when each event happens so some event happened at time to some event happened at time 2.7 some event happened to time 15.

476
01:12:48,150 --> 01:12:57,720
Michael Akira Lee Hayashi: This sequence here the entire arrival times tells us how long it was between events, so the first one starts at at the first time, so.

477
01:12:58,440 --> 01:13:02,850
Michael Akira Lee Hayashi: X one is equal to whenever the first event happened because that was the waiting time for that first event.

478
01:13:03,210 --> 01:13:18,150
Michael Akira Lee Hayashi: And then subsequent ones are the difference between subsequent events so event to it well, if the next event happened at time 2.5 the first event happened at time one than X two would be equal to 1.5 because that's the duration elapsed between.

479
01:13:19,590 --> 01:13:21,990
Michael Akira Lee Hayashi: And we're going to say that the event happens with rate lamp.

480
01:13:23,370 --> 01:13:30,270
Michael Akira Lee Hayashi: So the core of the system really is that now we're thinking about when events happen so.

481
01:13:31,290 --> 01:13:43,080
Michael Akira Lee Hayashi: So now we can we can think about this in the same coin flip way where the event is me flipping a coin, but now the thing that we really care about is how long is it between those coin flips what is the rate, with which I flip a coin.

482
01:13:44,100 --> 01:13:56,880
Michael Akira Lee Hayashi: So some of the things that tend to make sense to model with this sort of process are well like events, for which the time between occurrences of them matters a lot like.

483
01:13:58,620 --> 01:14:04,800
Michael Akira Lee Hayashi: We might want to know, for example, how frequently does a person socialize with people outside of their household.

484
01:14:05,220 --> 01:14:14,730
Michael Akira Lee Hayashi: That would that we could think of as a facade process where a person has a given rate of interacting with people outside their household maybe it's once a day, maybe it's twice a day, and so on.

485
01:14:15,180 --> 01:14:26,640
Michael Akira Lee Hayashi: And that rate is essentially going to tell us how many times that person interact with people outside their household it can we can make a stochastic system to represent when those events happen.

486
01:14:26,970 --> 01:14:35,190
Michael Akira Lee Hayashi: So that we can essentially generate a synthetic time series, and then we can do things like figure out how long it was between events and do some other analytics on it.

487
01:14:37,080 --> 01:14:56,610
Michael Akira Lee Hayashi: i'm like like a newly process there's a lot that is sort of provably known about it for some process which is great, so the number of events between in some time intervals so between times s&t or SS before T are distributed according to a plus on distribution and surprisingly.

488
01:14:58,770 --> 01:15:05,490
Michael Akira Lee Hayashi: Where the person distributions characteristic variable is equal to the rate times the time interval.

489
01:15:06,750 --> 01:15:14,520
Michael Akira Lee Hayashi: So this thing in here is what we might call or what well what we're going to call the total rate.

490
01:15:16,380 --> 01:15:26,730
Michael Akira Lee Hayashi: So the rate Lambda is is more or less the instantaneous rate of an event where multiplying Lambda by the time interval tells us like the.

491
01:15:27,360 --> 01:15:38,010
Michael Akira Lee Hayashi: The rate of the event, given the amount of time elapsed so it's a scaling factor sexually and when you when you use it for some distribution, you need to give it this total rate.

492
01:15:39,660 --> 01:15:45,990
Michael Akira Lee Hayashi: The probability of seeing K events in some time interval is given by this expression, and if this.

493
01:15:46,620 --> 01:15:54,660
Michael Akira Lee Hayashi: If this sort of thing the probability of seeing K events in some time interval sounds familiar, this is the analog to that operation for newly process.

494
01:15:55,230 --> 01:16:03,330
Michael Akira Lee Hayashi: And while this equation looks more complicated than the corresponding one for the newly equation, or the newly process this one here.

495
01:16:03,870 --> 01:16:13,020
Michael Akira Lee Hayashi: It turns out that the one for the facade process is actually easier for a computer to calculate because it doesn't have that convenient oryx term on the front of it so.

496
01:16:13,560 --> 01:16:23,610
Michael Akira Lee Hayashi: Either the whatever is relatively quick like exponential is relatively quick for a computer to do a factorial is even relatively quick for a computer to do, thanks to fast algorithms for that so.

497
01:16:25,020 --> 01:16:38,790
Michael Akira Lee Hayashi: Why is this helpful, this means that there are some cases where well we'll see this, but there are some cases where a boolean a facade processor roughly equivalent to each other and if you're in one of those cases, you can use the the.

498
01:16:39,660 --> 01:16:51,300
Michael Akira Lee Hayashi: The equation for calculating the number of the probability of seeing K events from a person from for some process instead of the newly one and save yourself computing time and we like.

499
01:16:52,890 --> 01:17:08,430
Michael Akira Lee Hayashi: Waiting times in a plus on process or distributed according to a negative exponential distribution this matters a lot, we will see this sort of concept come up quite a bit i'll talk about why in a little bit.

500
01:17:10,320 --> 01:17:18,120
Michael Akira Lee Hayashi: So, in a puts on process and we do have to make a certain number of assumptions if we're going to represent some real thing is for some process.

501
01:17:18,510 --> 01:17:25,170
Michael Akira Lee Hayashi: we're going to say that events occur independently, so the fact that I go to the grocery store today has no bearing on.

502
01:17:25,860 --> 01:17:39,300
Michael Akira Lee Hayashi: When I go to the grocery store next particularly it doesn't change that rate, which means that the event rate is constant so this this thing is also time homogenous this isn't going to change over the course of the simulation at all.

503
01:17:41,070 --> 01:17:47,790
Michael Akira Lee Hayashi: Then the final assumption is that only one event can happen at a given time, I can only go to the grocery store once at any given.

504
01:17:48,240 --> 01:17:54,360
Michael Akira Lee Hayashi: Specific time this might sound like it's kind of restrictive, but fortunately, because we're working in continuous time.

505
01:17:54,600 --> 01:18:07,530
Michael Akira Lee Hayashi: It is almost mathematically impossible for two events to happen at the same time in general right because continuous time means that even if one event happens at a time 1.0001 and the other one happens at 1.00011.

506
01:18:08,010 --> 01:18:15,000
Michael Akira Lee Hayashi: Those are different times and we haven't violated our our one event at any given time assumption for the song process.

507
01:18:17,340 --> 01:18:28,380
Michael Akira Lee Hayashi: So some examples of these are i've alluded to this kind of queueing theory notion arrivals at a clinic so instead of treating that like a newly process and discrete time we might think about this happening in continuous time.

508
01:18:29,040 --> 01:18:37,200
Michael Akira Lee Hayashi: Someone shows up at the clinic every hour or so, on average, and that would be a plus on process where the rate parameter is one over one hour.

509
01:18:38,730 --> 01:18:39,990
Michael Akira Lee Hayashi: Or the rate is one per hour.

510
01:18:41,580 --> 01:18:51,540
Michael Akira Lee Hayashi: progression of stuff through stages in the compartments a model is actually modeled using a person process, so when we have our comfort mental model that shifts.

511
01:18:52,080 --> 01:19:07,050
Michael Akira Lee Hayashi: stuff from A to B or a bulk flow of material from A to B this flow process, this is a person process, so the waiting time for any given bit of stuff here to move to here.

512
01:19:08,250 --> 01:19:12,600
Michael Akira Lee Hayashi: is governed by a person process, which means that waiting time is exponentially distributed.

513
01:19:14,340 --> 01:19:20,670
Michael Akira Lee Hayashi: Which means that anytime you have one of those so in this process will call this K, so you might have something like.

514
01:19:22,650 --> 01:19:28,740
Michael Akira Lee Hayashi: da is equal to negative K a plus i'll give it back pathway.

515
01:19:30,030 --> 01:19:32,520
Michael Akira Lee Hayashi: l plus i'll be.

516
01:19:34,140 --> 01:19:41,130
Michael Akira Lee Hayashi: This term here is essentially describing a person process for flow of stuff in one direction, as is this one.

517
01:19:42,990 --> 01:19:55,290
Michael Akira Lee Hayashi: We can also think about the occurrence of health outcomes in the cohort study there's some waiting time between when a person is enrolled and when they are diagnosed with cancer and the rate with which people.

518
01:19:56,100 --> 01:20:12,960
Michael Akira Lee Hayashi: are diagnosed with cancer can be thought of as the rate parameter in a plus on process, so if that is on average one every every 60 years, then we have a person process with a Lambda equals one over 60 years and that process gives us all of the the same.

519
01:20:14,910 --> 01:20:22,020
Michael Akira Lee Hayashi: fun results that we have here we can calculate the expected number of people who are going to have cancer.

520
01:20:22,350 --> 01:20:36,990
Michael Akira Lee Hayashi: or who are going to be diagnosed with cancer over some time interval like if we can use this in fact to do things like estimate if we run our study for a certain amount of time how how many people would we expect to actually have cancer in that study.

521
01:20:39,540 --> 01:20:47,040
Michael Akira Lee Hayashi: And, and if we model this using it for some process then we know the distribution of waiting times in that in that process.

522
01:20:47,910 --> 01:20:58,410
Michael Akira Lee Hayashi: Now the exponential distribution is kind of a lattice so a negative exponential distribution looks like this, this is this is probability mass function roughly so.

523
01:21:00,240 --> 01:21:11,520
Michael Akira Lee Hayashi: So what this means is if we think about this as waiting times right, so this is the amount of time that you might have to wait for someone shows up the clinic notice that the negative exponential distribution a it's, not a single peaks distribution.

524
01:21:13,980 --> 01:21:22,080
Michael Akira Lee Hayashi: And it's not really a modal distribution particularly right there's no bump in here anywhere but what there is is quite a lot of probability mass here.

525
01:21:22,860 --> 01:21:35,460
Michael Akira Lee Hayashi: So what does that mean well, that means that a lot of events, if we are a lot of events in a plus on process happen really fast right a lot of the time people show up to the clinic really quick.

526
01:21:36,690 --> 01:21:46,320
Michael Akira Lee Hayashi: If we're modeling it using the facade process but there's also this long tail out here, which means that some people take a really, really long time to show up.

527
01:21:47,790 --> 01:21:52,260
Michael Akira Lee Hayashi: And so we might ask ourselves is this actually realistic like.

528
01:21:54,090 --> 01:22:06,360
Michael Akira Lee Hayashi: Do people tend to show up according to this sort of distribution for something like queuing arrivals at a clinic it's a little harder I don't know off the top of my head I My suspicion is this probably is not super realistic but.

529
01:22:08,070 --> 01:22:10,350
Michael Akira Lee Hayashi: Maybe it is under certain circumstances, because.

530
01:22:10,770 --> 01:22:23,850
Michael Akira Lee Hayashi: arrivals at a clinic tend to be subject to certain scheduling dynamics where this starts to get weird is what if I told you that I want to use a facade process to represent the transition between infectiousness and recovery in a disease model.

531
01:22:27,000 --> 01:22:36,300
Michael Akira Lee Hayashi: You might find this a little suspicious, I certainly would because, in general, the amount of time, a person takes to recover from an infectious disease is.

532
01:22:37,200 --> 01:22:42,090
Michael Akira Lee Hayashi: kind of single peaked right like it kind of looks sort of this like.

533
01:22:43,080 --> 01:22:51,000
Michael Akira Lee Hayashi: Right, a lot of people are going to recover in kind of a similar amount of time, some will recover pretty quick some will kind of drag it out to be miserable for a while.

534
01:22:51,960 --> 01:22:58,650
Michael Akira Lee Hayashi: But you tend to have a lot of mass kind of in the middle, as opposed to an exponential distribution, which has a lot of mass over to the left.

535
01:22:59,640 --> 01:23:03,600
Michael Akira Lee Hayashi: So why is this a little wacky well because I mentioned that.

536
01:23:04,020 --> 01:23:14,490
Michael Akira Lee Hayashi: didn't models like si our models are fundamentally using pusan processes to represent transitions between states and the model that goes for the transition between susceptible and infected.

537
01:23:14,730 --> 01:23:23,940
Michael Akira Lee Hayashi: And infected and recovered or infected in a different stage of infected so that means that the amount of time, on average, that people spend infectious.

538
01:23:24,720 --> 01:23:36,180
Michael Akira Lee Hayashi: is drawn from a negative exponential distribution, which is one of those cases where we know this is wrong, this this isn't how that waiting time looks in the real world, it turns out that that.

539
01:23:36,750 --> 01:23:53,400
Michael Akira Lee Hayashi: The model still works pretty well, in spite of this, and there are ways to get around this particular problem like one really convenient result is that if you chain, a bunch of posts on processes together, so you have something that looks like this.

540
01:23:54,930 --> 01:24:01,680
Michael Akira Lee Hayashi: Where you go from A to B to C to D and each of these transition is governed by some.

541
01:24:03,270 --> 01:24:07,620
Michael Akira Lee Hayashi: Some facade process, the time taken to go through this entire chain.

542
01:24:08,790 --> 01:24:14,220
Michael Akira Lee Hayashi: is actually governed by gamma distribution, why is that Nice because the gamma distribution.

543
01:24:15,570 --> 01:24:26,940
Michael Akira Lee Hayashi: has two parameters, one is a shape parameter and one is a rate from the shape parameter, basically, as the number of stages in a corresponding chained exponential process and the larger that number of stages.

544
01:24:27,390 --> 01:24:39,060
Michael Akira Lee Hayashi: A gamma distribution will go from something like this, when there's one stage to something like this, when there's many stages, so you can actually create a single peak distribution, out of a chain of.

545
01:24:39,510 --> 01:24:48,720
Michael Akira Lee Hayashi: exponentially distributed pusan processes which enables you to do a better job potentially of approximating waiting times in a system that you know.

546
01:24:49,920 --> 01:24:58,650
Michael Akira Lee Hayashi: This is a bad approximation to well, we can talk about this more later because I know this this section of the lecture tends to be kind of information dumpy so.

547
01:24:59,460 --> 01:25:06,480
Michael Akira Lee Hayashi: i'm going to pause for now, before we move on, are there any questions at this point about some of the stochastic processes we've seen so far.

548
01:25:08,520 --> 01:25:11,340
Rishi Chanderraj: I just got one question um i've seen.

549
01:25:13,380 --> 01:25:21,720
Rishi Chanderraj: gamma distributions parameter eyes in different ways, with like rates and shapes and i've always had a hard time.

550
01:25:23,640 --> 01:25:35,370
Rishi Chanderraj: Like knowing which way to parameter eyes for particular problems is there, like some guidance around that or or like an optimal way for in this particular case.

551
01:25:36,060 --> 01:25:45,420
Michael Akira Lee Hayashi: Yes, that is a good question and and this this this happens, a lot, so let me, let me Wikipedia gamma distribution really quick.

552
01:25:49,410 --> 01:25:51,150
Michael Akira Lee Hayashi: i'll move my screen show over to that.

553
01:25:55,380 --> 01:26:04,290
Michael Akira Lee Hayashi: Okay, so here's a look at media has to say, but again, this is a, this is a distribution with the scale parameter or a shape parameter K and a scale parameter theta.

554
01:26:05,400 --> 01:26:19,650
Michael Akira Lee Hayashi: So the shape parameter fortunately is fairly straightforward, we can think about this, as the number of change exponential processes that generated that gamma distribution, so if K is one, then we have a single process.

555
01:26:20,010 --> 01:26:26,640
Michael Akira Lee Hayashi: Okay, is to, then we have to process these changes, the scale parameters, where things get weird.

556
01:26:28,560 --> 01:26:44,700
Michael Akira Lee Hayashi: So the scale parameter i've been talking about things in a plus on distribution, based on a rate right Lambda this is in this is expressed in units of inverse time oh that's that's wrong so time now that's right time to the minus one right so per hour per year per whatever.

557
01:26:46,440 --> 01:26:50,160
Michael Akira Lee Hayashi: The scale parameter is just the inverse of this.

558
01:26:51,480 --> 01:26:58,950
Michael Akira Lee Hayashi: So, usually the way that I think about these things is to start from the rate parameter for a system that i'm looking at and then, if.

559
01:26:59,310 --> 01:27:10,080
Michael Akira Lee Hayashi: The gamma distribution thing that i'm using requires a scale parameter i'll just take the inverse of that rate parameter to give myself the scale prep they have an exact correspondence that way so.

560
01:27:11,100 --> 01:27:23,640
Michael Akira Lee Hayashi: It the scale parameter doesn't have in as entirely useful and interpretation here so it's not bad right like if if we're talking about a rate of like.

561
01:27:27,180 --> 01:27:32,670
Michael Akira Lee Hayashi: 60 events per hour, for example, that's our rate, that means that our scale would be one over six.

562
01:27:34,320 --> 01:27:48,660
Michael Akira Lee Hayashi: And so you can convert back and forth, as you want when i'm working with models generally i'll start from a rate parameter, because that tends to be one of the easier things for me to find like how frequently an event happens and then i'll.

563
01:27:49,830 --> 01:27:59,970
Michael Akira Lee Hayashi: i'll flip that to a scale parameter if the particular gamma distribution implementation, I want to work with requires scale prep so basically my my.

564
01:28:00,600 --> 01:28:10,050
Michael Akira Lee Hayashi: Well, the tl Dr is first when if you're calling a gamma distribution from say are or whatever programming language you want to use it in it heck even excel if you want to do that.

565
01:28:10,620 --> 01:28:16,080
Michael Akira Lee Hayashi: Read the documentation for the gamma distribution, to see if it asks for a scale parameter or rate parameter.

566
01:28:16,830 --> 01:28:30,540
Michael Akira Lee Hayashi: And then figure out if the thing that you have is a rate parameter or a scale parameter and the easiest way to do that is to be like Is this a thing in units of inverse time is it like per hour if so it's a rate parameter, and you may have to convert accordingly.

567
01:28:33,660 --> 01:28:34,980
Rishi Chanderraj: Thanks yeah.

568
01:28:35,310 --> 01:28:40,530
Michael Akira Lee Hayashi: we'll we'll practice this one we do some of the lab segments, when we come back from break, but right now I will let you all.

569
01:28:40,770 --> 01:28:41,160
Caroline Godfrey: get up.

570
01:28:41,190 --> 01:28:45,210
Michael Akira Lee Hayashi: get a drink use the bathroom and go outside whatever whatever you want to do and i'll See you in.

571
01:28:45,930 --> 01:28:47,550
Michael Akira Lee Hayashi: 15 I think is are breaking.

572
01:28:49,650 --> 01:28:50,190
Michael Akira Lee Hayashi: sounds right.

573
01:28:50,850 --> 01:28:52,680
Michael Akira Lee Hayashi: Right yeah seen a bit.

574
01:28:53,550 --> 01:28:54,150
Michael Akira Lee Hayashi: bit here.

575
01:28:56,070 --> 01:29:05,640
Michael Akira Lee Hayashi: And then we will do a little bit of programming to kind of break up the lecture and see a little bit of the implementation for the things that we've started to take a look at.

576
01:29:08,520 --> 01:29:15,420
Michael Akira Lee Hayashi: Before we get started, are there any other questions from before the break and I didn't have a whole lot of time to address them.

577
01:29:31,140 --> 01:29:34,770
Michael Akira Lee Hayashi: Alright hearing none we're going to move right along.

578
01:29:35,850 --> 01:29:43,380
Michael Akira Lee Hayashi: Okay, so I alluded to the fact that there is some degree of correspondence between posts on processes and for newly processes.

579
01:29:45,090 --> 01:29:57,030
Michael Akira Lee Hayashi: In particular, we can think about a person distribution as a special case of a binomial distribution in the limit as the number of trials and that binomial distribution goes to infinity.

580
01:29:57,810 --> 01:30:05,250
Michael Akira Lee Hayashi: Provided the probability of success in any given trials fairly small, so why is that important well because.

581
01:30:06,180 --> 01:30:12,570
Michael Akira Lee Hayashi: A brilliantly process essentially uses a binomial distribution is kind of its fundamental probability distribution for stuff.

582
01:30:13,020 --> 01:30:18,990
Michael Akira Lee Hayashi: And I put on process uses a facade distribution is its kind of fundamental probability distribution for stuff.

583
01:30:19,830 --> 01:30:36,540
Michael Akira Lee Hayashi: So that means that we could think about a person process as an approximation of a new lead process as long as that newly process runs for a sufficiently long amount of time or sufficiently large number of trials with a relatively small p.

584
01:30:38,550 --> 01:30:49,500
Michael Akira Lee Hayashi: So you wouldn't want to do this if the probability of an event is fairly high like I don't know 25% 50% or so on if it's smaller than you can usually get away pretty well.

585
01:30:51,600 --> 01:30:57,540
Michael Akira Lee Hayashi: And i'll i'll talk a moment about how you can kind of how you can sometimes kind of budgets that you arrive at that.

586
01:30:59,610 --> 01:31:02,670
Michael Akira Lee Hayashi: Why would you ever want to do this, though well like I mentioned.

587
01:31:05,490 --> 01:31:13,980
Michael Akira Lee Hayashi: This is just not super fun to calculate on a computer because of the convenience oryx term it's quite slow and this is worse if, like I mentioned.

588
01:31:14,220 --> 01:31:19,410
Michael Akira Lee Hayashi: You have a large number of time steps and kind of a medium number of events in that large number of times steps.

589
01:31:19,710 --> 01:31:27,390
Michael Akira Lee Hayashi: This thing gets really cumbersome to calculate because that committed to our X term just has to iterate through a huge number of combinations in order to actually.

590
01:31:28,290 --> 01:31:30,120
Michael Akira Lee Hayashi: calculate the number of combinations.

591
01:31:30,960 --> 01:31:44,430
Michael Akira Lee Hayashi: By contrast, the corresponding equation for the probability of K events in some time interval for a person process is a lot easier for a computer to calculate because the worst of it really is the factorial term down here and that.

592
01:31:44,730 --> 01:31:57,420
Michael Akira Lee Hayashi: they're known pretty good algorithms to do so, nothing else here is nearly so computationally cumbersome as a competitor X term in the in the newly process equivalent.

593
01:32:00,210 --> 01:32:00,660
Michael Akira Lee Hayashi: So.

594
01:32:02,100 --> 01:32:07,080
Michael Akira Lee Hayashi: What does that mean in practice well if I wanted to model say.

595
01:32:08,880 --> 01:32:20,040
Michael Akira Lee Hayashi: Suppose we're looking at health outcomes in this study and I want to choose between using a newly process in the facade process well both of these would probably be appropriate, because in either case, we have.

596
01:32:21,000 --> 01:32:26,250
Michael Akira Lee Hayashi: A situation where there's some amount of time until somebody gets sick in a study.

597
01:32:27,270 --> 01:32:28,770
Michael Akira Lee Hayashi: In a cohort study I should say.

598
01:32:30,540 --> 01:32:32,040
Michael Akira Lee Hayashi: or clinical trial or whatever.

599
01:32:34,770 --> 01:32:43,230
Michael Akira Lee Hayashi: And, and we could say that the study is conducted with discrete observation points at which there's some chance that a person shows up.

600
01:32:43,650 --> 01:32:56,430
Michael Akira Lee Hayashi: as having a disease or we could think about it happening in continuous time where we don't care so much about the discrete time intervals, all we care about is how long it was, until that person was diagnosed with the particular outcome that we care about.

601
01:32:58,770 --> 01:33:10,020
Michael Akira Lee Hayashi: Often, this particular approximation works well, in those cases, because say for a chronic disease, the probability that a given person develops that disease and the given here is pretty small.

602
01:33:11,970 --> 01:33:21,660
Michael Akira Lee Hayashi: And so, our assumption is reasonably well maintained, what if it is what if there's a decent probability of an event happening in a given year well.

603
01:33:21,960 --> 01:33:28,980
Michael Akira Lee Hayashi: Sometimes we can kind of cheat if the probability of an event happening in the years fairly big What about the probability of an event happening in a day.

604
01:33:29,670 --> 01:33:44,190
Michael Akira Lee Hayashi: That smaller and so now we have something that looks a little bit more like at Hughes to our our assumption of when the newly and put some processes correspond, so we can sometimes do scaling tricks like that.

605
01:33:44,760 --> 01:33:54,660
Michael Akira Lee Hayashi: To make it so that we can use the the persona approximation to a newly process in order to make our life easier when we go to calculate things like.

606
01:33:55,290 --> 01:34:02,730
Michael Akira Lee Hayashi: what's the probability of a certain number of events happening in that thing which is useful for kind of calculating other outcomes from a from a model like that.

607
01:34:04,620 --> 01:34:06,360
Michael Akira Lee Hayashi: This is one of those things where.

608
01:34:08,730 --> 01:34:13,620
Michael Akira Lee Hayashi: The way I would think about this and, to be honest, the way I would think about a lot of the material from things up to here is kind of.

609
01:34:13,950 --> 01:34:24,750
Michael Akira Lee Hayashi: keep some of the sort of buzzword correspondences in your brain like, if you take nothing else away if you, if you remember a little ways down the line, the waiting times in a facade model are exponentially distributed.

610
01:34:25,140 --> 01:34:34,260
Michael Akira Lee Hayashi: That probably should stick in your brain similarly a pusan process approximates for newly process when there's a large number of trials and a small probability.

611
01:34:34,680 --> 01:34:46,110
Michael Akira Lee Hayashi: Those are like I would kind of keep those in your brain and maybe they maybe they'll stick now maybe they'll stick later, maybe the reasoning for them doesn't quite stick now, but there are things that will be useful if you come back to them later.

612
01:34:48,960 --> 01:34:50,430
Michael Akira Lee Hayashi: So let's move on a little bit.

613
01:34:52,680 --> 01:35:01,500
Michael Akira Lee Hayashi: And i'm going to cover next our our kind of a broad category of things, called random walks and these both encompass.

614
01:35:01,890 --> 01:35:11,100
Michael Akira Lee Hayashi: And kind of it's kind of a superset on the stochastic processes that we've seen so far so for newly processes facade process, these can all be thought of as random walks.

615
01:35:11,370 --> 01:35:18,990
Michael Akira Lee Hayashi: Really all we're talking about here is some stochastic process that describes the movement of something through some mathematical space.

616
01:35:19,230 --> 01:35:26,970
Michael Akira Lee Hayashi: These can be indiscreet or continuous time and that space is really arbitrary, so we can think about this is being physical space or Euclidean space.

617
01:35:27,570 --> 01:35:35,520
Michael Akira Lee Hayashi: We could also think about this as moving through some kind of arbitrary state space, instead of a physical space or or a space of the physical analog.

618
01:35:36,720 --> 01:35:44,670
Michael Akira Lee Hayashi: What this means is that we gain a lot of flexibility and what we can think about as a rapid walk process, so this is where so things like.

619
01:35:45,360 --> 01:35:52,110
Michael Akira Lee Hayashi: The polio is earned model that we saw yesterday I would think about that as a random walk but not quite a newly process similarly.

620
01:35:52,500 --> 01:36:07,200
Michael Akira Lee Hayashi: stochastic decision trees, we could think about as random walks but not quite newly processing, so this is this is kind of the overarching framework that we will tend to think of is encompassing a lot of other classes of stochastic model.

621
01:36:12,000 --> 01:36:18,540
Michael Akira Lee Hayashi: One One example that I like because it showcases the number of kind of interesting properties of of random walks.

622
01:36:20,040 --> 01:36:25,410
Michael Akira Lee Hayashi: Is is the little toy model where you stick a person or an answer, whatever you want, on a little plateau.

623
01:36:26,340 --> 01:36:39,240
Michael Akira Lee Hayashi: And and we're going to make this a district time model so every second they either take a one meter step to the right or to the left and there's a probability of moving right and then one minus that probability is the probability moving left.

624
01:36:40,530 --> 01:36:48,600
Michael Akira Lee Hayashi: If that thing in this case and undergrad who's drunk on spring break or whatever if they hit the edge of the plateau they fall off on either side.

625
01:36:51,480 --> 01:36:56,940
Michael Akira Lee Hayashi: Why is this interesting at all well because we might want to ask ourselves some questions about a model like this.

626
01:36:57,960 --> 01:37:11,160
Michael Akira Lee Hayashi: what's the probability that this undergrad at some point is going to fall off this plateau either to the right or the left side, we might also want to know how long are they going to on average wander around that plateau before they fall off in some direction or another.

627
01:37:12,810 --> 01:37:22,980
Michael Akira Lee Hayashi: This is a dumb example, although from my observations I do actually think that undergrads move largely by random walk process, so I think it's realistic in that sense.

628
01:37:24,570 --> 01:37:24,990
Michael Akira Lee Hayashi: But.

629
01:37:26,100 --> 01:37:38,040
Michael Akira Lee Hayashi: What might this represent the real world, well, we could think about this representation process like like a within host process it takes a lot of mutations before you actually start to show.

630
01:37:38,610 --> 01:37:48,300
Michael Akira Lee Hayashi: functional differences in an organism, so this could mean the number of mutations that have to happen to a virus before you get the evolution of a meaningfully different strain.

631
01:37:49,470 --> 01:37:58,020
Michael Akira Lee Hayashi: This could also be the number of mutations that take place within a person's body before they actually start to get cancer before before their cells become properly malignant.

632
01:37:59,010 --> 01:38:08,430
Michael Akira Lee Hayashi: So what we're thinking about here is the amount of time that, whatever our thing is in the system occupies some space before it falls off one way or another.

633
01:38:08,820 --> 01:38:23,640
Michael Akira Lee Hayashi: Is that time when it's normal right so that's the amount of time spent with a cell being normal kind of mutating in different ways, before something happens that we notice and that something in this model is, will you fall off one side or the other.

634
01:38:24,750 --> 01:38:30,150
Michael Akira Lee Hayashi: So something like this model can act as a stochastic random walk model of.

635
01:38:30,810 --> 01:38:37,500
Michael Akira Lee Hayashi: Like within host processes or mutational processes, where you don't necessarily know which way a mutation is going to pull.

636
01:38:37,680 --> 01:38:47,280
Michael Akira Lee Hayashi: The phenotype of an organism necessarily because a lot of them are just going to hit useless stuff or maybe one thing will move something a little more in one direction or another or move it back and so on, so.

637
01:38:48,330 --> 01:38:58,950
Michael Akira Lee Hayashi: We can think about it as representing those kinds of processes, and then we can use it to ask ourselves questions like how long is it until an expectation, we see a meaningful change in that organism.

638
01:38:59,280 --> 01:39:06,900
Michael Akira Lee Hayashi: Or we see the onset of disease and an organism because because essentially we fallen off one side of the other of our plateau.

639
01:39:07,890 --> 01:39:17,490
Michael Akira Lee Hayashi: So this is one kind of random walk it turns out that solving for either of these things is not immediately straightforward, this is kind of a tricky thing to solve for and.

640
01:39:18,480 --> 01:39:21,960
Michael Akira Lee Hayashi: practically speaking I won't I won't demonstrate this but.

641
01:39:22,800 --> 01:39:26,880
Michael Akira Lee Hayashi: practically speaking, we often have to use a thing called a recursive solution.

642
01:39:27,090 --> 01:39:38,760
Michael Akira Lee Hayashi: to figure out any of these answers so break a recursive solutions, one where we break the problem down into sub problems solve the sub problem move on to the next problem move on to the next problem and eventually we've solved the whole problem.

643
01:39:40,830 --> 01:39:51,300
Michael Akira Lee Hayashi: I won't I won't belabor that what I am going to do is switch over really quickly to a little bit of coding to demonstrate a really simple random walk and Euclidean space, so this is.

644
01:39:51,810 --> 01:40:07,590
Michael Akira Lee Hayashi: If you want, I can code this thing also but we'll start with a fairly simple like I don't know two dimensional random walk because that has a nice physical analog so let's move over to our studio for a bit.

645
01:40:08,880 --> 01:40:13,470
Michael Akira Lee Hayashi: And and i've wrote some template functions for random walks.

646
01:40:15,090 --> 01:40:32,130
Michael Akira Lee Hayashi: You can find these on canvas i've named this file lab to template functions, so there is a template function for a Doc product which is just sort of a thing if you want to calculate it, but the one we're going to focus on right now is this one step walk and then this random or function.

647
01:40:33,450 --> 01:40:34,050
Michael Akira Lee Hayashi: So.

648
01:40:35,430 --> 01:40:41,700
Michael Akira Lee Hayashi: i've written in the lab document which we're not going to hew to exactly but which can provide some guidance.

649
01:40:43,650 --> 01:40:53,400
Michael Akira Lee Hayashi: So if you want to pull that up, you may find that helpful but but i've also largely documented the tasks in the code itself so here, our task is as follows, we want to.

650
01:40:53,790 --> 01:41:04,620
Michael Akira Lee Hayashi: Ultimately, what we want to do is write, something which enables us to model, a random walk in two dimensional space, so we can think about this as an ant exploring for food that starts off somewhere.

651
01:41:05,130 --> 01:41:14,100
Michael Akira Lee Hayashi: And then, it takes a step, and then it takes another step and takes another step and it keeps taking steps and it just kind of meandering around the space and it and it doesn't have any particular.

652
01:41:15,330 --> 01:41:18,090
Michael Akira Lee Hayashi: rhyme or reason to its course, but this is how it does.

653
01:41:21,390 --> 01:41:31,380
Michael Akira Lee Hayashi: So that's our overall goal we want to model that whole process what i'm gonna do is i'm going to i'm going to implement this by breaking the process down into to kind of discrete little steps.

654
01:41:31,980 --> 01:41:41,850
Michael Akira Lee Hayashi: i'm going to start by writing a function that enables me to take one step just a single step in Euclidean space from some starting position to some ending position.

655
01:41:42,480 --> 01:41:52,710
Michael Akira Lee Hayashi: Why am I going to do that, because this is going to make it easier to implement the rest of the process, because the rest of the process, then, is largely just looping that single step.

656
01:41:54,060 --> 01:41:55,440
Michael Akira Lee Hayashi: And this makes my life, a little cleaner.

657
01:41:56,460 --> 01:42:06,330
Michael Akira Lee Hayashi: So what do we have to do we have a little bit more specifications on this project, so this isn't just going to be any random walk process we're going to suppose that it's specified in a particular way.

658
01:42:07,260 --> 01:42:19,230
Michael Akira Lee Hayashi: we're going to suppose that the amount of movement in the X and y direction is given by a normal distribution for each of those steps so your next step.

659
01:42:19,650 --> 01:42:31,770
Michael Akira Lee Hayashi: is normally distributed with me and and various and your wife step is normally distributed with me and and various and these can be the same, they can be different, they can be whatever you want So how do we do this well um.

660
01:42:33,570 --> 01:42:43,560
Michael Akira Lee Hayashi: let's break this thing down a little bit we start off at our starting location, so this is our initial position and we're going to need to return some new position, which is where we end up.

661
01:42:44,100 --> 01:42:52,350
Michael Akira Lee Hayashi: That new position is going to be the starting position plus x deviation and a y deviation or adding an x deviation on to the next component of that.

662
01:42:52,620 --> 01:42:58,920
Michael Akira Lee Hayashi: initial point and adding a wide deviation onto the white component of that point so we're going to do a little bit of vector operations here.

663
01:42:59,970 --> 01:43:06,480
Michael Akira Lee Hayashi: So the first thing we're going to need to do here is no it's like category that's nice that's good.

664
01:43:08,520 --> 01:43:16,800
Michael Akira Lee Hayashi: perfect the right person okay So the first thing we're going to need to do is draw some normally distributed random variables.

665
01:43:17,250 --> 01:43:30,540
Michael Akira Lee Hayashi: Because that's what we're going to use as kind of the the main probability distribution for a random walk process, so I think that the random normal function in ours called our norm, which is help.

666
01:43:33,120 --> 01:43:33,690
Michael Akira Lee Hayashi: Here it is.

667
01:43:34,860 --> 01:43:37,080
Michael Akira Lee Hayashi: And i'm going to remind myself how to.

668
01:43:38,340 --> 01:43:46,380
Michael Akira Lee Hayashi: How to access a for how to how to generate a random variable in in are using it from a normal distribution.

669
01:43:48,330 --> 01:43:57,000
Michael Akira Lee Hayashi: So we have this our norm function which generates n random numbers, with a mean and standard deviation.

670
01:43:58,590 --> 01:44:10,410
Michael Akira Lee Hayashi: Now note that in my function here I am passing the variants of a normal distribution, not the standard deviation, so this is one of those cases where a probability distribution.

671
01:44:10,710 --> 01:44:15,120
Michael Akira Lee Hayashi: May in some implementations or another be specified, using a different names parameter.

672
01:44:16,020 --> 01:44:16,830
Michael Akira Lee Hayashi: You may see.

673
01:44:17,220 --> 01:44:27,750
Michael Akira Lee Hayashi: Normal distribution specified using standard deviations you may see the specified using variants and it's important to make sure that you don't mix them up because you'll get very different random number draws on the basis of each of those.

674
01:44:27,960 --> 01:44:40,680
Michael Akira Lee Hayashi: So here are expects a standard deviation in my code, I expect a variance which means i'm going to need to do a little bit of conversion and because I can never remember these things i'm going to Google of variance standard.

675
01:44:41,850 --> 01:44:42,450
Michael Akira Lee Hayashi: ation.

676
01:44:43,770 --> 01:44:46,230
Michael Akira Lee Hayashi: And figure out what the correspondence between those.

677
01:44:47,250 --> 01:45:04,710
Michael Akira Lee Hayashi: So the very the standard deviation is the square root of the variance, which means that that's going to be my my conversion, so this experience this wide variance i'm going to need to convert to sex in St white and needs to do that by.

678
01:45:07,290 --> 01:45:09,690
Michael Akira Lee Hayashi: reminding myself of square root function.

679
01:45:11,130 --> 01:45:12,540
Michael Akira Lee Hayashi: spirit Var X.

680
01:45:15,030 --> 01:45:15,720
Michael Akira Lee Hayashi: or y.

681
01:45:17,760 --> 01:45:27,780
Michael Akira Lee Hayashi: And this should get me some traction so now, I have some of the parameters that I need in order to draw my random numbers i'm going to write some tests stuff here because I think.

682
01:45:29,790 --> 01:45:43,620
Michael Akira Lee Hayashi: It is helpful to kind of see how each stage of the function works so i'm going to define my start point as 00 going to make this a vector going to make our new X is going to be.

683
01:45:46,530 --> 01:46:02,580
Michael Akira Lee Hayashi: called out one mean by one are Var X is going to be no to our bar why it's going to be three so I can see myself getting different things out of this and then i'm going to call my one step walk function.

684
01:46:08,250 --> 01:46:14,010
Michael Akira Lee Hayashi: And i'm going to remind myself that this takes my starting point, meaning variance meaning variance so.

685
01:46:19,260 --> 01:46:30,000
Michael Akira Lee Hayashi: Far why and we're just going to leave it at that, because this thing isn't going to generate anything, particularly exciting, but what I do want to see is what's coming out in the intermediate steps.

686
01:46:36,420 --> 01:46:37,500
Michael Akira Lee Hayashi: that's the y.

687
01:46:38,610 --> 01:46:40,140
Michael Akira Lee Hayashi: Right, so this is.

688
01:46:41,580 --> 01:46:48,690
Michael Akira Lee Hayashi: This is just me double checking that these operations behave properly and that i'm not going to get something stupid outfit so let's source or.

689
01:46:51,090 --> 01:46:56,910
Michael Akira Lee Hayashi: No name something wrong of Roi is missing, with no default.

690
01:47:00,870 --> 01:47:01,770
Michael Akira Lee Hayashi: Oh, I love something.

691
01:47:05,790 --> 01:47:12,240
Michael Akira Lee Hayashi: This is, this is a reminder to make sure that you pass the right number of arguments into a function and in the right order that function is going to complain.

692
01:47:17,010 --> 01:47:17,670
Michael Akira Lee Hayashi: Oh yeah.

693
01:47:26,820 --> 01:47:38,460
Michael Akira Lee Hayashi: Just to kind of walk through what's happened here when I write template functions I usually write the name of the function of the arguments that needs to take and it'll make a trivial little return here, just so that I remind myself what actually needs to come out the back end.

694
01:47:39,660 --> 01:47:46,530
Michael Akira Lee Hayashi: I didn't assign anything to that variable before I tried to execute function, so it failed so here i'm just making something trivial.

695
01:47:47,190 --> 01:47:57,900
Michael Akira Lee Hayashi: That i'll change, but which will enable the function to run, so this is me writing just enough stuff to make the function go without kind of writing the rest of everything.

696
01:47:59,100 --> 01:48:10,140
Michael Akira Lee Hayashi: will do this again and here we go this looks right this looks like skirted to in this that's like square two three some confidence this works now I can move on a little bit do my random number draws so.

697
01:48:11,820 --> 01:48:18,390
Michael Akira Lee Hayashi: delta X is going to be my deviation in the X direction, so this is going to be our norm.

698
01:48:19,470 --> 01:48:27,450
Michael Akira Lee Hayashi: I only want one I want that to be my mean, and now I have my standard deviation going to do the same thing.

699
01:48:28,890 --> 01:48:30,480
Michael Akira Lee Hayashi: Why are.

700
01:48:33,210 --> 01:48:40,050
Michael Akira Lee Hayashi: You why St why i'm going to make very sure that I use the right variable names heroes i'm going to get myself into trouble.

701
01:48:40,320 --> 01:48:47,700
Michael Akira Lee Hayashi: notice that i'm using a combination of passing variables that were passed as arguments and ones which I created within the scope of the function itself.

702
01:48:47,940 --> 01:49:02,100
Michael Akira Lee Hayashi: Based on what I need to write my function, so I didn't need to do any operations to transform these means, so I just use them as past I didn't need to transform the variances in standard deviations so I did that and assign them out.

703
01:49:02,940 --> 01:49:08,910
Michael Akira Lee Hayashi: So now let's take a look at what we got for our delta X and delta why so print.

704
01:49:13,470 --> 01:49:21,840
Michael Akira Lee Hayashi: With respect, actually going to see what happens by Prince things as a factor, because I have a suspicion, for what that bracket ID number in the print statement is and i'm going to see if i'm right.

705
01:49:23,340 --> 01:49:25,620
Michael Akira Lee Hayashi: No it's not okay that's my.

706
01:49:26,730 --> 01:49:30,330
Michael Akira Lee Hayashi: thought it might have been implicit like enumeration it was not.

707
01:49:31,950 --> 01:49:33,750
Michael Akira Lee Hayashi: Okay So here we go.

708
01:49:36,360 --> 01:49:40,110
Michael Akira Lee Hayashi: here's our deviation in the X and y direction.

709
01:49:42,450 --> 01:49:43,020
Michael Akira Lee Hayashi: and

710
01:49:45,000 --> 01:49:53,880
Michael Akira Lee Hayashi: One one annoying thing about stochastic models and simulating them is that it might not be immediately obvious to me, if this is right, if these makes sense, so.

711
01:49:54,300 --> 01:50:10,260
Michael Akira Lee Hayashi: How can I test this little bit more well I could right now, these have very similar means and, frankly, the variances are pretty similar so a two for the amount of change in either the X or y direction it's not crazy let's make them mean why like five.

712
01:50:12,810 --> 01:50:14,580
Michael Akira Lee Hayashi: And we'll do this a few times.

713
01:50:16,650 --> 01:50:20,520
Michael Akira Lee Hayashi: And it will do some stuff and now this gives me a little more confidence that.

714
01:50:23,040 --> 01:50:29,190
Michael Akira Lee Hayashi: That this thing is working as intended, because we tend to see broadly lower values for the.

715
01:50:29,670 --> 01:50:34,830
Michael Akira Lee Hayashi: For the deviation and the X direction, then we do for the deviation and the y direction, which is, as we would hope.

716
01:50:35,100 --> 01:50:45,810
Michael Akira Lee Hayashi: Given that the average deviation in the y direction is bigger than the average deviation in the extraction, so this is to say, when you implement stochastic models it's worth doing a bit of.

717
01:50:46,800 --> 01:50:52,830
Michael Akira Lee Hayashi: testing of your intermediate stages to make very sure that you're convinced that they're behaving as they should.

718
01:50:53,130 --> 01:51:01,080
Michael Akira Lee Hayashi: Now, with with quite a lot of practice, you might be confident at that point that these are behaving right just on the basis of the function definitions like.

719
01:51:01,860 --> 01:51:07,170
Michael Akira Lee Hayashi: Once you're confident you know how to read the documentation for the function and you're confident you've given it the arguments that wants them.

720
01:51:07,590 --> 01:51:19,020
Michael Akira Lee Hayashi: Fine, you probably don't have to do this amount of checking but it's never a bad thing, and it doesn't take you a whole lot of time, so better safe than sorry, especially because bugs and stochastic models are a lot harder to find.

721
01:51:20,910 --> 01:51:28,860
Michael Akira Lee Hayashi: We can talk a little bit about ways to debug in a little bit so we're almost there right, we now have a way to.

722
01:51:30,750 --> 01:51:35,790
Michael Akira Lee Hayashi: We have a way to get a deviation in the X and y direction.

723
01:51:37,140 --> 01:51:41,100
Michael Akira Lee Hayashi: And we have we have a starting point, so what do we do now.

724
01:51:42,450 --> 01:51:44,970
Michael Akira Lee Hayashi: Well, now let's let's make our.

725
01:51:48,600 --> 01:51:49,800
Michael Akira Lee Hayashi: But stick our.

726
01:51:51,480 --> 01:52:01,560
Michael Akira Lee Hayashi: delta X and delta y into a vector so that we can calculate our new vector fairly straightforward and we're going to do it as follows point plus Delta.

727
01:52:03,960 --> 01:52:16,440
Michael Akira Lee Hayashi: Why does this work or why do I hope this works, mostly because our is a very vector heavy legs are matlab are kind of similar to that way and that, fundamentally, they really like to think about things as vectors or more broadly.

728
01:52:17,010 --> 01:52:23,580
Michael Akira Lee Hayashi: Technically arrays really vectors is what they're thinking about So if I make a test factor that is just going to be.

729
01:52:25,500 --> 01:52:26,880
Michael Akira Lee Hayashi: One one and then.

730
01:52:31,170 --> 01:52:35,880
Michael Akira Lee Hayashi: They do that's right i'm going to make some tests factors that hold two elements, what happens if I add.

731
01:52:37,650 --> 01:52:54,000
Michael Akira Lee Hayashi: Plus test out well what this is done conveniently is element wise edition or vector edition right it's out of the two onto the one in the five onto the one and given me three six, this is wonderful, because this means that I can do the.

732
01:52:55,500 --> 01:53:07,800
Michael Akira Lee Hayashi: I can essentially translate my starting point, using the deviation in the X and y directions in essentially one edition operation as opposed to what I could also have done, which would be.

733
01:53:12,930 --> 01:53:17,910
Michael Akira Lee Hayashi: You X equals.

734
01:53:20,370 --> 01:53:23,970
Michael Akira Lee Hayashi: Point two plus delta X.

735
01:53:25,500 --> 01:53:27,870
Michael Akira Lee Hayashi: y why and then stuff these things together.

736
01:53:33,900 --> 01:53:42,270
Michael Akira Lee Hayashi: And that works, this is legal, this is fine there's there's literally nothing wrong with this does take a few more lines, but it works.

737
01:53:43,320 --> 01:53:44,580
Michael Akira Lee Hayashi: I was just able to.

738
01:53:46,350 --> 01:53:53,010
Michael Akira Lee Hayashi: This so this should work in exactly the same way and actually let me, let me do a quick Google to see if I can.

739
01:53:54,930 --> 01:53:55,410
Michael Akira Lee Hayashi: Are.

740
01:54:02,460 --> 01:54:04,140
Michael Akira Lee Hayashi: think there is.

741
01:54:07,560 --> 01:54:11,220
Michael Akira Lee Hayashi: All explain what i'm doing in a moment after after i've done a bit of googling.

742
01:54:18,420 --> 01:54:21,030
Michael Akira Lee Hayashi: Typically, the documentation is bad.

743
01:54:29,070 --> 01:54:29,280
Michael Akira Lee Hayashi: Okay.

744
01:54:30,660 --> 01:54:34,050
Michael Akira Lee Hayashi: And I do a thing, then I will talk through why I did that thing.

745
01:54:38,400 --> 01:54:41,580
Michael Akira Lee Hayashi: test this see if it actually the face.

746
01:54:49,890 --> 01:54:57,510
Morgan Caitlin Byrd: Factor hey Ashley there's a comment in the chat about could you possibly make your console a little bit smaller so they can see more of the code.

747
01:54:58,020 --> 01:55:00,240
Michael Akira Lee Hayashi: Oh sorry I missed please chat.

748
01:55:03,750 --> 01:55:05,790
Michael Akira Lee Hayashi: Yes, no better.

749
01:55:15,540 --> 01:55:19,080
Michael Akira Lee Hayashi: Okay, so let me, let me back up a little bit and talk through what i've done.

750
01:55:20,340 --> 01:55:24,690
Michael Akira Lee Hayashi: um we take for granted that most programming languages can generate random numbers.

751
01:55:25,830 --> 01:55:33,060
Michael Akira Lee Hayashi: might not be immediately obvious how they're doing this and it turns out that they're not actually generating real revenue number so they're not literally rolling a dice.

752
01:55:33,630 --> 01:55:44,670
Michael Akira Lee Hayashi: What they're doing instead is they're using a complex mathematical function that takes some starting number or some starting state that it gets from something in the system like the time of day, or the weather or whatever.

753
01:55:45,180 --> 01:55:58,530
Michael Akira Lee Hayashi: And then that that initialize is what's called a pseudo random number generator that pseudo random number generator uses a complex mathematical algorithm to generate a sequence of numbers that looks for all intents and purposes brand.

754
01:55:58,950 --> 01:56:06,870
Michael Akira Lee Hayashi: It is not actually random if you know, the starting state of that random number generator and you know which random number generation algorithm was used.

755
01:56:07,140 --> 01:56:14,880
Michael Akira Lee Hayashi: You can generate the same sequence of random numbers every single time so that starting state is called the seed of the random number generator.

756
01:56:15,690 --> 01:56:26,640
Michael Akira Lee Hayashi: So what i've done here is i've forced the seed of the random number generator are to just be this this sequence of numbers, which means that every time I run my code.

757
01:56:27,480 --> 01:56:45,510
Michael Akira Lee Hayashi: it's going to generate the same outcome from these random number draws because well the random number generator is set to the same thing when it starts off so every random number generator random number that it generates subsequently is from that same sequence if I didn't do this.

758
01:56:47,400 --> 01:56:57,870
Michael Akira Lee Hayashi: The random number generator were just use whatever it intrinsically uses and you can see that it's giving me different answers for the output here, which is maybe more like what we would expect from a random number generator similarly.

759
01:56:58,920 --> 01:57:02,820
Michael Akira Lee Hayashi: recall that this is the output from this particular seed if I change the seed.

760
01:57:03,390 --> 01:57:07,980
Michael Akira Lee Hayashi: i'll get different answers, but as long as that see to set and the same it'll give me the same answers.

761
01:57:08,400 --> 01:57:23,220
Michael Akira Lee Hayashi: This is really helpful for debugging stochastic models, because this gets around the problem of not knowing whether a change in your result is due to randomness or well the random number generator or some kind of bug somewhere else along the way.

762
01:57:24,240 --> 01:57:31,560
Michael Akira Lee Hayashi: So i'm using this for reproducibility while i'm testing my code right, so I can run the thing a few times and make sure that we're okay.

763
01:57:33,300 --> 01:57:46,830
Michael Akira Lee Hayashi: And it looks kind of like we are so right now, this thing has let me, let me introduce a little bit of extra printing so that we can confirm so here's what we're gonna do we're gonna print delta X.

764
01:57:49,560 --> 01:57:49,980
Michael Akira Lee Hayashi: factor.

765
01:57:56,520 --> 01:58:00,420
Michael Akira Lee Hayashi: comic is there you go or on this again.

766
01:58:03,270 --> 01:58:04,020
Michael Akira Lee Hayashi: i'm.

767
01:58:05,100 --> 01:58:08,550
Michael Akira Lee Hayashi: Not printing what I wanted to print delta X delta why.

768
01:58:12,120 --> 01:58:12,780
Michael Akira Lee Hayashi: edition.

769
01:58:15,450 --> 01:58:19,650
Michael Akira Lee Hayashi: Somewhere here, something is no it's probably stuffed the print somewhere annoying as.

770
01:58:20,880 --> 01:58:21,450
Michael Akira Lee Hayashi: For me.

771
01:58:24,060 --> 01:58:25,770
Michael Akira Lee Hayashi: see if I can find where my.

772
01:58:28,680 --> 01:58:29,490
Michael Akira Lee Hayashi: thing is going.

773
01:58:32,220 --> 01:58:39,870
Michael Akira Lee Hayashi: finds this tough should be somewhere in work called function that's a little bit suspicious to me that we are getting something a little bit weird.

774
01:58:42,120 --> 01:58:43,530
Michael Akira Lee Hayashi: Point scale to.

775
01:58:45,060 --> 01:58:47,430
Michael Akira Lee Hayashi: put my prints in the wrong places that's possible.

776
01:58:48,990 --> 01:58:53,070
Michael Akira Lee Hayashi: Oh, I know why my answers and helpful because I started a 600.

777
01:58:54,300 --> 01:58:58,140
Michael Akira Lee Hayashi: let's not do that, so I don't confuse myself okay that's better.

778
01:58:59,190 --> 01:59:04,890
Michael Akira Lee Hayashi: Right, so now we can see the amount that we're supposed to move and where we ended up and i'm actually going to print the.

779
01:59:05,970 --> 01:59:06,900
Michael Akira Lee Hayashi: starting point to.

780
01:59:10,380 --> 01:59:14,760
Michael Akira Lee Hayashi: And we can see a little bit more of what's happened so we started, here we moved this much we ended up here.

781
01:59:15,600 --> 01:59:22,980
Michael Akira Lee Hayashi: And so, this suggests that our function is working correctly, because we could we could do the addition ourselves right, we could add.

782
01:59:23,640 --> 01:59:35,040
Michael Akira Lee Hayashi: Well, we could subtract Point seven or seven from one we can add point 54821 and we basically get this so now, I can also confirm the both implementations are identical So when I do this.

783
01:59:39,420 --> 01:59:40,110
Michael Akira Lee Hayashi: and

784
01:59:43,410 --> 01:59:44,040
Michael Akira Lee Hayashi: This.

785
01:59:46,710 --> 02:00:02,040
Michael Akira Lee Hayashi: We get the same answer right using vector addition, as opposed to explicitly adding each component, we still wind up at the same point, so this convinces me that both of those ways of implementing this particular function are identical and should be correct.

786
02:00:03,510 --> 02:00:05,970
Michael Akira Lee Hayashi: i'm going to use the vector one, because that is.

787
02:00:06,990 --> 02:00:15,750
Michael Akira Lee Hayashi: A tiny amount faster I think it's a little hard for me to evaluate because I don't know how fast the concatenate operation is, but I think it's a hair.

788
02:00:18,600 --> 02:00:25,140
Michael Akira Lee Hayashi: This is another one of those cases where if either of these ways of performing this operation work equally well.

789
02:00:27,930 --> 02:00:43,320
Michael Akira Lee Hayashi: But there are some circumstances where I would definitely prefer vector addition over component wise addition and then concatenate into a vector can anyone think of a case where that might be so where i'd find component wise addition tedious are like.

790
02:00:59,220 --> 02:01:02,400
Michael Akira Lee Hayashi: are doing this, is there a circumstance, where this might start to get clunky.

791
02:01:09,870 --> 02:01:12,390
Rishi Chanderraj: If you have more than two dimensions, I guess, like.

792
02:01:12,540 --> 02:01:20,460
Michael Akira Lee Hayashi: yeah yeah that is absolutely what, if I were in like high dimensional Euclidean space and had to do something like.

793
02:01:23,430 --> 02:01:28,770
Michael Akira Lee Hayashi: I don't know and so on, right and and at that point vector wise addition is clearly simple.

794
02:01:30,360 --> 02:01:34,830
Michael Akira Lee Hayashi: Though I could write a for loop I supposed to procedurally do this, which is a whole separate way.

795
02:01:36,750 --> 02:01:39,210
Michael Akira Lee Hayashi: But yeah, so this is an example of a case where.

796
02:01:39,210 --> 02:01:40,110
Michael Akira Lee Hayashi: Sometimes.

797
02:01:40,170 --> 02:01:53,850
Michael Akira Lee Hayashi: What works in small scale starts to get cumbersome, as you deal with a bigger and bigger problem or, if you have to deal with bigger and bigger types of input, or if you expect your code to be able to scale across a range of potential inputs.

798
02:01:55,920 --> 02:02:06,330
Michael Akira Lee Hayashi: So we've written away, to take one random step in our random walk process right that's good that's nice so now let's finish this out.

799
02:02:08,190 --> 02:02:12,930
Michael Akira Lee Hayashi: Now we want to actually write the rent the walk so let's.

800
02:02:15,120 --> 02:02:17,550
Michael Akira Lee Hayashi: let's take a look at it, so now we need to.

801
02:02:19,410 --> 02:02:29,460
Michael Akira Lee Hayashi: We need to generate a full random walk for some number of steps and steps so let's let's start by writing some of our preliminaries.

802
02:02:31,710 --> 02:02:45,330
Michael Akira Lee Hayashi: And i'm actually going to start writing these preliminaries in a way that is a little bit more queuing to the way that I tend to write code in general, you don't have to do this, but you can, if you very much wanted.

803
02:02:48,600 --> 02:02:51,270
Michael Akira Lee Hayashi: to write a thing called a main function and then.

804
02:02:54,120 --> 02:03:02,970
Michael Akira Lee Hayashi: If you've written c++ code or stuff like that you've probably used main functions before oh tech, this is not how you write these things and.

805
02:03:06,780 --> 02:03:08,550
Michael Akira Lee Hayashi: it's not how you read functions in art, that is.

806
02:03:10,530 --> 02:03:22,020
Michael Akira Lee Hayashi: So what i'm doing is i'm containing all of the operations in my script within a function that i'm calling name and then, when I call that function it fires, the script Why do I prefer this well because.

807
02:03:24,540 --> 02:03:40,920
Michael Akira Lee Hayashi: notice that when i've run my scripts now there's a lot less than the global environment it's only containing the function definitions, the only things now that come out after I call the main function, are going to be things that I explicitly print or output through.

808
02:03:42,060 --> 02:03:50,010
Michael Akira Lee Hayashi: This function and what this does is it forces me to keep the global environment, clean, I can no longer rely on like.

809
02:03:50,520 --> 02:03:54,690
Michael Akira Lee Hayashi: Like running specific lines and having them hang around in the global environment in our studio.

810
02:03:55,020 --> 02:04:07,620
Michael Akira Lee Hayashi: This basically causes are to clean up a little bit more, because once the main function finishes it cleans up everything that it created within itself again you don't have to do this, you can write your code just an open code, but.

811
02:04:08,880 --> 02:04:11,880
Michael Akira Lee Hayashi: Because I am the way that I am with my programming style.

812
02:04:12,240 --> 02:04:22,740
Michael Akira Lee Hayashi: I like to keep variable scoped very tight I don't like stuff to step into the global environment because that's a good way for you to misplace variables or to change them or use what you mean to so.

813
02:04:23,730 --> 02:04:31,710
Michael Akira Lee Hayashi: For the purpose of of of kind of demonstrating what I think is best practices i'm going to write my script using the main function.

814
02:04:33,210 --> 02:04:33,810
Michael Akira Lee Hayashi: So.

815
02:04:35,400 --> 02:04:43,830
Michael Akira Lee Hayashi: So just kind of demonstrate what I mean by you have to tell it to output stuff if I want to know what actually what the value of this new test point is.

816
02:04:44,880 --> 02:04:46,350
Michael Akira Lee Hayashi: Now I have no choice.

817
02:04:47,670 --> 02:04:57,600
Michael Akira Lee Hayashi: But, to use a print stick, because nothing else is going to come out of this function right, and here we have that this also forces me to documents, a little bit better, so I might be like Prince.

818
02:05:01,080 --> 02:05:03,210
Michael Akira Lee Hayashi: Final location and now.

819
02:05:04,680 --> 02:05:08,760
Michael Akira Lee Hayashi: Now i'm telling myself a bit more about what's going on here so Prince.

820
02:05:19,410 --> 02:05:22,740
Michael Akira Lee Hayashi: So i'm forcing my code to kind of document itself, while it runs.

821
02:05:24,900 --> 02:05:35,520
Michael Akira Lee Hayashi: This again like there's not a huge operational difference between doing this and just writing and kind of you know, writing in the global part of the script but.

822
02:05:36,000 --> 02:05:44,400
Michael Akira Lee Hayashi: I think sometimes enforcing practices like documentation and commenting and and printing and stuff like that it's not a terrible thing.

823
02:05:45,960 --> 02:05:49,080
Michael Akira Lee Hayashi: It forces yourself to stay honest about that.

824
02:05:50,400 --> 02:05:56,310
Michael Akira Lee Hayashi: Anyway, let's set some things up, so that we can move on to the next part of this little bit so.

825
02:05:57,300 --> 02:06:08,280
Michael Akira Lee Hayashi: we're pretty convinced that are one step walk function works so we're not going to need to run this anymore, we are going to need to test our next thing so we're going to want a number of steps so step.

826
02:06:09,750 --> 02:06:11,550
Michael Akira Lee Hayashi: Take 10 steps for.

827
02:06:14,460 --> 02:06:19,800
Michael Akira Lee Hayashi: And we're not going to do anything else, and our main function for the book.

828
02:06:21,660 --> 02:06:32,880
Michael Akira Lee Hayashi: So let's do a random walk function needs to do well, it needs to start from somewhere and then it needs to do some number of calls to this function in order to actually do the red and walk.

829
02:06:35,340 --> 02:06:44,790
Michael Akira Lee Hayashi: And we need to do that end steps number of times so seeing this, the thing that's probably going to come into my brain is a for loop or some kind of that might be a for loop, it might be a while loop.

830
02:06:45,120 --> 02:06:54,660
Michael Akira Lee Hayashi: Generally i'll default to a for loop when possible, because they're a little safer than while loops they will terminate or a while loop might not stop depending on how you define your conditions so.

831
02:06:55,890 --> 02:07:07,110
Michael Akira Lee Hayashi: How do we need to define our for loop Well, we know that we need to take an steps number of steps so that should give us some starting trash let's do this so for.

832
02:07:09,210 --> 02:07:16,140
Michael Akira Lee Hayashi: I in one two steps do some stuff now let's just print it.

833
02:07:17,880 --> 02:07:19,230
Michael Akira Lee Hayashi: And we're gonna.

834
02:07:21,660 --> 02:07:23,910
Michael Akira Lee Hayashi: we're gonna just do something dumb for our.

835
02:07:25,680 --> 02:07:30,960
Michael Akira Lee Hayashi: For outputs function works in the first place, and now, if I call random walk.

836
02:07:32,550 --> 02:07:32,940
Michael Akira Lee Hayashi: With.

837
02:07:44,160 --> 02:07:55,110
Michael Akira Lee Hayashi: Justice notice that i'm not necessarily when I when I test code I don't necessarily assign the value of a function somewhere I don't have to I could, if I want to so.

838
02:08:02,460 --> 02:08:09,570
Michael Akira Lee Hayashi: I can do this it doesn't really matter because right now that trajectory is entirely trivial, but you can you can kind of do this either way.

839
02:08:10,500 --> 02:08:22,620
Michael Akira Lee Hayashi: So what if I got when I call my main function, it does all of this stuff and calling this random walk function executes everything within the function body which is right now literally just running affordable.

840
02:08:24,240 --> 02:08:27,180
Michael Akira Lee Hayashi: So i'm convinced that I can take some number of steps so we're good.

841
02:08:29,460 --> 02:08:31,560
Michael Akira Lee Hayashi: That is kind of the meat of our thing.

842
02:08:33,480 --> 02:08:37,590
Michael Akira Lee Hayashi: What I want to do at each step, though, is called the one step walk function.

843
02:08:40,380 --> 02:08:45,480
Michael Akira Lee Hayashi: In order to do that i'm probably going to need to know where i'm starting right and i'm going to need to know.

844
02:08:45,780 --> 02:08:54,900
Michael Akira Lee Hayashi: Not only where i'm starting in general, but where i'm starting on any given iteration of this for loop, so if i'm on step one obviously I should be going off of the the total initial condition.

845
02:08:55,290 --> 02:09:04,650
Michael Akira Lee Hayashi: If i'm on step two though i'm moving from wherever I moved after step one so we're going to need to do a little bit to set this up here so.

846
02:09:05,370 --> 02:09:12,480
Michael Akira Lee Hayashi: recall from yesterday, but often when we write code for a thing we, we need to do a little bit of setup for that code i'm going to do that here so.

847
02:09:13,800 --> 02:09:19,950
Michael Akira Lee Hayashi: i'm going to need something to store my data in at the end of the day, right, this is going to be kind of dumb so I don't want this anymore.

848
02:09:21,030 --> 02:09:32,370
Michael Akira Lee Hayashi: If i'm thinking and kind of a matlab slash numb pie away what I probably want to do is pre allocate a matrix to store all of my data so let's do that i'm going to want a factory.

849
02:09:33,600 --> 02:09:36,930
Michael Akira Lee Hayashi: equal to matrix we're going to fill it with zeros and it's going to be.

850
02:09:38,070 --> 02:09:47,640
Michael Akira Lee Hayashi: Well i'm going to remind myself how to build a matrix because I can't remember which dimension is which So here we go I want a number of rows which is going to be n steps.

851
02:09:48,750 --> 02:09:52,110
Michael Akira Lee Hayashi: Actually i'm going to do this explicitly so that I remind myself happens.

852
02:09:53,970 --> 02:09:58,230
Michael Akira Lee Hayashi: And then m call is equal to or the number of columns is equal to.

853
02:10:00,840 --> 02:10:05,130
Michael Akira Lee Hayashi: 123 i'll show you why I want to do three so.

854
02:10:06,240 --> 02:10:07,800
Michael Akira Lee Hayashi: let's do a little bit more printing here.

855
02:10:10,020 --> 02:10:28,260
Michael Akira Lee Hayashi: To see what what i've built for my initial thing there's her for loop here's here's the matrix that I built it has 10 rows and three columns what i'm going to do is supposed to call them one is the timestamp column to as the exposition and column three is the y position.

856
02:10:29,640 --> 02:10:35,490
Michael Akira Lee Hayashi: could have done this, using a data frame, yes, but matrices are slightly less bulky so we're going to use them.

857
02:10:38,730 --> 02:10:47,760
Michael Akira Lee Hayashi: So that's our start, and we also know where are we what our initial step is in the model so let's do that let's let's say that trajectory.

858
02:10:50,310 --> 02:10:51,090
Michael Akira Lee Hayashi: or directory.

859
02:10:52,950 --> 02:10:56,640
Michael Akira Lee Hayashi: I want row one.

860
02:10:58,980 --> 02:11:07,890
Michael Akira Lee Hayashi: column all of its columns are going to be equal to the following they're going to be equal to time step one.

861
02:11:10,080 --> 02:11:20,760
Michael Akira Lee Hayashi: Point i'm going to have to test this line because i'm a little bit suspicious of how this is going to behave, so what I want to do is basically stuff a one on the front of this.

862
02:11:21,420 --> 02:11:30,780
Michael Akira Lee Hayashi: This to element vectors so that I end up with three element factor, which is the which fills in the road but let's test that so i'm going to do test is equal to.

863
02:11:32,130 --> 02:11:32,490
Michael Akira Lee Hayashi: That.

864
02:11:33,660 --> 02:11:45,030
Michael Akira Lee Hayashi: And then, if I do one test was that Okay, so this has behaved as I want, and it has indeed upended the second vector on the first one, and we should be okay, so let me again.

865
02:11:47,670 --> 02:11:53,370
Michael Akira Lee Hayashi: print statements here is that I can see what happens when I set the first row of my trajectory.

866
02:11:55,470 --> 02:12:01,110
Michael Akira Lee Hayashi: That looks Okay, because we started at one one, and this is time one, so I think we're okay.

867
02:12:05,520 --> 02:12:15,540
Michael Akira Lee Hayashi: There is something else that I may have to change in here, particularly that I may have to change in the for loop, but for the moment let's not worry about it.

868
02:12:17,670 --> 02:12:22,950
Michael Akira Lee Hayashi: Actually, Canadian spot the thing that I think i'm going to need to change either about the output matrix or within the for loop.

869
02:12:24,600 --> 02:12:26,610
Michael Akira Lee Hayashi: i'll give you a hint it's a size problem.

870
02:12:28,950 --> 02:12:33,090
Michael Akira Lee Hayashi: size of one thing that i've made right now is possibly not the size that I want.

871
02:12:48,750 --> 02:12:54,390
Caroline Godfrey: Is it that, if you take 10 steps you'll end up needing a little.

872
02:12:55,680 --> 02:12:57,840
Caroline Godfrey: Like you'll need the number of steps plus one.

873
02:12:59,070 --> 02:12:59,670
Caroline Godfrey: rose.

874
02:13:00,570 --> 02:13:07,320
Michael Akira Lee Hayashi: yeah and we can see this if I try to actually assign the value of what's happening in the for loop somewhere in the matrix if I do.

875
02:13:12,180 --> 02:13:21,720
Michael Akira Lee Hayashi: In particular it's a problem that arises because, if I start here, then the thing that fills in the next or this matrix is going to be where I moved to from here so.

876
02:13:22,680 --> 02:13:33,210
Michael Akira Lee Hayashi: What I probably want to do is make my matrix and steps plus one, so that when i'm iterating through here I don't run off the end of the input matrix.

877
02:13:34,200 --> 02:13:48,450
Michael Akira Lee Hayashi: So now we have some stuff in place to actually execute our random walk to implement it, so let us say that our new points is equal to one step walk.

878
02:13:50,550 --> 02:13:56,310
Michael Akira Lee Hayashi: And we're gonna say old points new X bar X.

879
02:13:58,080 --> 02:13:59,430
Michael Akira Lee Hayashi: Y or y.

880
02:14:00,510 --> 02:14:07,980
Michael Akira Lee Hayashi: And that should be what is old point going to be well it's going to be the previous row in our matrix so old.

881
02:14:09,450 --> 02:14:11,190
Michael Akira Lee Hayashi: Point there's going to be equal to.

882
02:14:13,200 --> 02:14:15,330
Michael Akira Lee Hayashi: Sorry, I.

883
02:14:20,880 --> 02:14:27,840
Michael Akira Lee Hayashi: To onward I think that this legal syntax will find out really promptly if this is legal syntax or not.

884
02:14:31,140 --> 02:14:34,830
Michael Akira Lee Hayashi: And this should be something we can go off so let's print.

885
02:14:38,010 --> 02:14:39,900
Michael Akira Lee Hayashi: Point and then we'll print.

886
02:14:43,410 --> 02:14:45,900
Michael Akira Lee Hayashi: And we'll see what we get from him some things complaining.

887
02:14:48,090 --> 02:14:51,150
Michael Akira Lee Hayashi: This might not be proper syntax let me do a quick Google.

888
02:14:57,450 --> 02:14:59,550
Michael Akira Lee Hayashi: subset columns in matrix.

889
02:15:11,910 --> 02:15:20,700
Michael Akira Lee Hayashi: Oh, I think it's just wants to know what it probably wants like a an end or something like that to to end it's not a keyword I think it is let's see if that works.

890
02:15:26,790 --> 02:15:27,390
Michael Akira Lee Hayashi: Are.

891
02:15:41,070 --> 02:15:47,790
Michael Akira Lee Hayashi: Some some languages have clean ways to do this, some of less clean ways to do this.

892
02:15:49,800 --> 02:15:52,230
Michael Akira Lee Hayashi: yeah so it's going to make me do it this way, so to to.

893
02:16:00,780 --> 02:16:03,720
Michael Akira Lee Hayashi: Actually, you can do an end call thing.

894
02:16:09,360 --> 02:16:13,380
Michael Akira Lee Hayashi: let's try that see if that works i'll explain what I did here so.

895
02:16:16,110 --> 02:16:32,880
Michael Akira Lee Hayashi: First of all, what I need to do in order to make my one step walk work is figure out where I started, where I started from is i'm going to assume is the current row of my trajectory matrix how do I get that well, I want to get the idea throw.

896
02:16:34,050 --> 02:16:35,640
Michael Akira Lee Hayashi: Of the trajectory matrix.

897
02:16:38,430 --> 02:16:48,750
Michael Akira Lee Hayashi: And then I need the second through last columns of that matrix which means i'm going to need to through however many columns are in that matrix and.

898
02:16:49,770 --> 02:16:53,550
Michael Akira Lee Hayashi: That should be that should be enough to work with so.

899
02:16:55,650 --> 02:17:03,270
Michael Akira Lee Hayashi: See how this goes well notice that it's it's giving different values for where we're starting and ending.

900
02:17:04,920 --> 02:17:12,000
Michael Akira Lee Hayashi: Because each time this function fires, it draws new random numbers notice also that, when that after the first step.

901
02:17:12,600 --> 02:17:19,260
Michael Akira Lee Hayashi: When it when it tells me what my old point was it's just 00, why is this because currently.

902
02:17:19,740 --> 02:17:36,450
Michael Akira Lee Hayashi: Only the first point is defined in this matrix I said it equal to the starting point, nothing else is being updated yet so now, now that we're reasonably convinced that we can generate a new point now we have to actually update the matrix here so i'm going to do.

903
02:17:37,710 --> 02:17:40,080
Michael Akira Lee Hayashi: trajectory I plus one.

904
02:17:42,570 --> 02:17:43,890
Michael Akira Lee Hayashi: Two and call.

905
02:17:45,990 --> 02:17:51,030
Michael Akira Lee Hayashi: This is going to be equal to new point and i'm also going to set the time part of it.

906
02:17:54,060 --> 02:17:54,480
Michael Akira Lee Hayashi: one.

907
02:17:56,430 --> 02:18:00,450
Michael Akira Lee Hayashi: is equal to I plus one, which is going to be our timestamp.

908
02:18:03,000 --> 02:18:07,740
Michael Akira Lee Hayashi: think this should work and we'll see what comes out the end of this thing, so now i'm going to print the.

909
02:18:12,570 --> 02:18:13,920
Michael Akira Lee Hayashi: thing that comes out here.

910
02:18:15,570 --> 02:18:16,140
Michael Akira Lee Hayashi: So.

911
02:18:17,400 --> 02:18:18,450
Michael Akira Lee Hayashi: This looks good.

912
02:18:19,650 --> 02:18:32,280
Michael Akira Lee Hayashi: We start here we move to here we start here move to here, and so on, one thing I could maybe do to test this a little bit more is to change the mean and variants of my.

913
02:18:33,630 --> 02:18:41,130
Michael Akira Lee Hayashi: of my walk things just to confirm that I am getting kind of sensible values but.

914
02:18:42,000 --> 02:18:47,970
Michael Akira Lee Hayashi: One thing that gives me some confidence in the random walk process working is that notice the y component is getting bigger.

915
02:18:48,390 --> 02:19:03,780
Michael Akira Lee Hayashi: And this should make sense, given that i'm drawing a random number from a normal distribution with mean five so on average this thing is going to be both positive but reasonably more than positive, so we should be kind of moving up in our thing so.

916
02:19:05,190 --> 02:19:08,220
Michael Akira Lee Hayashi: I might be able so so from here.

917
02:19:09,660 --> 02:19:18,630
Michael Akira Lee Hayashi: We have some sense that our random walk processes work now notice if I did something like this if I set the meaning of one of these things to zero.

918
02:19:20,070 --> 02:19:36,300
Michael Akira Lee Hayashi: Now, what we should see as a little bit more kind of oscillation closer to well, we should see negative numbers front and we may see a little bit more oscillation about zero from that thing and we might not because well random trajectories are.

919
02:19:37,350 --> 02:19:43,860
Michael Akira Lee Hayashi: Random and so sometimes they'll they'll do what we expect and sometimes they'll do stuff that we don't.

920
02:19:45,180 --> 02:19:51,570
Michael Akira Lee Hayashi: here's one where we kind of bounced around some of them kind of tend to run away some of them bounce around zero and so on.

921
02:19:53,070 --> 02:20:05,250
Michael Akira Lee Hayashi: Where, for example, if they set the average Why change to be really big then, on balance, we should be seeing trajectories that end up way higher in why space in the y coordinate.

922
02:20:05,700 --> 02:20:16,590
Michael Akira Lee Hayashi: than they do in the experts Similarly, if I had a negative average change for the X direction we should, on average, see ourselves kind of walking left.

923
02:20:20,070 --> 02:20:21,420
Michael Akira Lee Hayashi: Through this entire trajectory.

924
02:20:23,970 --> 02:20:25,230
Michael Akira Lee Hayashi: Any questions at this point.

925
02:20:46,260 --> 02:20:46,620
Michael Akira Lee Hayashi: Sorry.

926
02:20:46,740 --> 02:21:02,340
Adriana Perez: The question is by doing all of these programming, we are heading into what the election, how is that can you give me the big picture I am following you what you're doing, but I am trying to link what's the big picture here.

927
02:21:03,240 --> 02:21:04,200
Michael Akira Lee Hayashi: yeah so.

928
02:21:04,710 --> 02:21:06,390
Michael Akira Lee Hayashi: What we've done here is we've written.

929
02:21:06,660 --> 02:21:12,510
Michael Akira Lee Hayashi: we've written some functions which enable us to implement a particular kind of random walk in two dimensional space.

930
02:21:12,810 --> 02:21:19,710
Michael Akira Lee Hayashi: But what this also demonstrates, is that we could write functions to generate random walks through fairly arbitrary spaces as well.

931
02:21:20,070 --> 02:21:31,860
Michael Akira Lee Hayashi: We don't have to use this particular mechanism to move around our space right we don't have to draw our changes from normal distributions we don't have to make it so that the changes in our.

932
02:21:32,730 --> 02:21:39,870
Michael Akira Lee Hayashi: In our system are fully unbiased right like right now, this is an unbiased random walk were equally likely to go right, as we are left.

933
02:21:40,260 --> 02:21:45,840
Michael Akira Lee Hayashi: So if we wanted to implement some other kind of random walk system like a mark off model or something like that.

934
02:21:46,050 --> 02:21:54,330
Michael Akira Lee Hayashi: We would actually use some fairly similar principles so what's being demonstrated here in a lot of ways, is a simple very, very simple random walk model.

935
02:21:54,660 --> 02:22:04,920
Michael Akira Lee Hayashi: But the process of writing that model actually tells us some things about how we'd write other types of models so, for example, there are a lot of cases where when we write a model.

936
02:22:05,160 --> 02:22:09,930
Michael Akira Lee Hayashi: But we basically do is we instantiate the data structure to hold the model, and then we update.

937
02:22:10,470 --> 02:22:22,290
Michael Akira Lee Hayashi: rows of that data structure as we, as we call some function that changes the state of the model, this is actually going to happen over and over and over and over again when we implement models so.

938
02:22:22,980 --> 02:22:33,720
Michael Akira Lee Hayashi: So by writing this very simple random walk model, hopefully we've we've convinced ourselves a that it works, be that we have some kind of.

939
02:22:34,320 --> 02:22:40,680
Michael Akira Lee Hayashi: bits and pieces of code in hand that we can use if we needed to write a bigger more complicated model because.

940
02:22:40,860 --> 02:22:50,730
Michael Akira Lee Hayashi: At the end of the day, a more complicated stochastic model in a lot of ways, is going to be built from coding bits that came from simpler models, like all of this stuff updating.

941
02:22:51,210 --> 02:22:57,780
Michael Akira Lee Hayashi: In that initializing updating some kind of output matrix you're going to do this again and again and again again doing things like.

942
02:22:58,620 --> 02:23:10,890
Michael Akira Lee Hayashi: Breaking down you're breaking down your model simulation routine into smaller functions that performed smaller chunks of that simulation you're going to do that again and again and again and.

943
02:23:11,280 --> 02:23:14,100
Michael Akira Lee Hayashi: Even if you use a pre built function to do that.

944
02:23:14,640 --> 02:23:27,300
Michael Akira Lee Hayashi: it's probably doing a similar thing so so part of this pedagogically is to get a feel for what's going on under the Hood in any given simulation routine some of it also is just to give you practice writing the code that.

945
02:23:27,690 --> 02:23:40,740
Michael Akira Lee Hayashi: performs all of that work and kind of building yourself a cookbook of Okay, if I need to assign elements of an output matrix in a particular way from a for loop, how do I do it well this code is going to tell me how to do that.

946
02:23:42,420 --> 02:23:46,950
Michael Akira Lee Hayashi: Similarly, if I want to organize some of my code, this is going to tell me how to do that.

947
02:23:47,130 --> 02:24:01,350
Michael Akira Lee Hayashi: How do I instantiate all my variables, here it is so there's equal parts like taking away a little bit about how general like how a fairly naive two dimensional random walk works, as well as bits about how to write an implement.

948
02:24:01,620 --> 02:24:05,520
Michael Akira Lee Hayashi: A broader class of models, because one thing that I do like to emphasize is that.

949
02:24:07,890 --> 02:24:18,090
Michael Akira Lee Hayashi: One of the core things about mathematical modeling is that at the end of the day, inevitably you're going to have to code that model up and implemented in a computer there's very few models, where you don't have to do that.

950
02:24:18,390 --> 02:24:24,900
Michael Akira Lee Hayashi: And so the more tools and the more like little recipe chunks that you have in your brain or that you have.

951
02:24:25,320 --> 02:24:36,930
Michael Akira Lee Hayashi: easy to access that let you write pieces of your model, the easier it is to write a bigger more complicated model because all it is is built, out of little building blocks that help.

952
02:24:41,010 --> 02:24:41,700
Adriana Perez: US thanks.

953
02:24:42,870 --> 02:24:43,110
Michael Akira Lee Hayashi: Great.

954
02:24:45,090 --> 02:24:58,020
Michael Akira Lee Hayashi: Alright let's let's switch gears a little bit to take a look at some of the more something something maybe more immediately applicable from a modeling perspective i'm gonna i'm going to switch back to my lecture notes here.

955
02:24:59,550 --> 02:25:05,130
Michael Akira Lee Hayashi: And, of course, if you have questions about how i've written anything or anything like that, please don't hesitate to keep asking.

956
02:25:06,450 --> 02:25:07,290
Michael Akira Lee Hayashi: The chat backup.

957
02:25:08,700 --> 02:25:16,290
Michael Akira Lee Hayashi: So i'm going to skip a little bit of the preliminaries here and i'll come back to them if I need them let's talk for a moment about mark of months.

958
02:25:17,400 --> 02:25:31,050
Michael Akira Lee Hayashi: For one, why did they, why did they bring up random walks at all well because a mark of model which is a classic model that likely you're going to find useful in number of cases is a random walk.

959
02:25:31,590 --> 02:25:37,680
Michael Akira Lee Hayashi: All it is, though, is a random walk that's moving through a different kind of space state space so.

960
02:25:39,060 --> 02:25:52,920
Michael Akira Lee Hayashi: let's define a market model, it is a stochastic model of a system that changes over time that assumes, in particular, that the current state of the system only depends on the immediate prior state with this is known as the memory list assumption so.

961
02:25:54,720 --> 02:26:08,190
Michael Akira Lee Hayashi: You can either specify a market model than indiscreet or continuous time so as kind of a conceptual example let's suppose I want to make a mark off model of my emotional state at any given time I have essentially three emotions i'm either fine hungry or tired.

962
02:26:09,390 --> 02:26:12,900
Michael Akira Lee Hayashi: And at any given time i'm going to be one of those right now and.

963
02:26:14,940 --> 02:26:17,340
Michael Akira Lee Hayashi: For that I was fine and before that I was tired.

964
02:26:18,570 --> 02:26:28,080
Michael Akira Lee Hayashi: But what I feel right now only depends on what I felt immediately prefer so being hungry now only depends on what I was before that, and by the same token.

965
02:26:28,260 --> 02:26:40,440
Michael Akira Lee Hayashi: What I feel that the next time I reevaluate my emotions only depends on me being hungry now it doesn't matter that I was normal before i'm tired before that it only matters that i'm hungry at this moment now.

966
02:26:41,760 --> 02:26:50,490
Michael Akira Lee Hayashi: This is a fairly strong assumption in Markov models, because there are a lot of processes in the real world that do care about their entire history, they care about what happened 10 years ago.

967
02:26:51,030 --> 02:26:55,710
Michael Akira Lee Hayashi: There are some ways to kind of shim this into a mark of models, such that it's less of a problem but.

968
02:26:56,010 --> 02:27:05,610
Michael Akira Lee Hayashi: Note that this is kind of a fundamental strong assumption of the market model, but they are always memory of this, the next state depends on the current state, the current state only depends on the previous state.

969
02:27:06,360 --> 02:27:17,280
Michael Akira Lee Hayashi: And when we think about Markov models conceptually, the most important thing to think of is, how can we break the complex system we want model down into states, so if I want to model.

970
02:27:17,730 --> 02:27:36,210
Michael Akira Lee Hayashi: Emotions each of those emotions is a state, if I want to model tobacco usage, for example, which I suspect may be of relevance to some of you, then I need to think about what kinds of states can a person occupy in there in kind of their usage process as a consumer of tobacco.

971
02:27:37,290 --> 02:27:49,290
Michael Akira Lee Hayashi: So am I never user, am I a current user and my a foreign former user each of those things current never former those can be thought of as states in our market model and.

972
02:27:49,530 --> 02:27:57,150
Michael Akira Lee Hayashi: More broadly states in our state space that our system is going to randomly walk through we think about this as a random walk.

973
02:27:59,490 --> 02:28:14,580
Michael Akira Lee Hayashi: So again, bear with me while I go through some definitions here if we think about a discrete time Markov chain, in which we evaluate we change the system at fixed time intervals we're going to say that we have sorry.

974
02:28:16,860 --> 02:28:19,500
Michael Akira Lee Hayashi: I suspect that a package i'm not gonna worry about that, though.

975
02:28:21,510 --> 02:28:27,690
Michael Akira Lee Hayashi: I have some states to find us one through SK so in the previous model those states should be fine hungry and tired.

976
02:28:30,120 --> 02:28:36,630
Michael Akira Lee Hayashi: will call cutie the current state of the system so that would be me being hungry right now at time T, which is the current time.

977
02:28:37,710 --> 02:28:41,940
Michael Akira Lee Hayashi: we're also going to define a thing called a State distribution fact this is going to be.

978
02:28:44,040 --> 02:28:44,580
Michael Akira Lee Hayashi: On SEC.

979
02:29:00,420 --> 02:29:02,310
Michael Akira Lee Hayashi: sorry about that hopefully that's the thing.

980
02:29:05,250 --> 02:29:17,940
Michael Akira Lee Hayashi: So the state distribution vector tells us the probability at any given time, that the system occupies one of the case states, so the state distribution vector for this model is going to tell me what probability.

981
02:29:18,720 --> 02:29:25,740
Michael Akira Lee Hayashi: With which probability i'm in each state at any given time, so the way to think about Markov models is kind of twofold.

982
02:29:26,160 --> 02:29:31,530
Michael Akira Lee Hayashi: They if we simulate a mark of model then it occupies a sequence of discrete states.

983
02:29:31,890 --> 02:29:37,620
Michael Akira Lee Hayashi: Over time right the manifestation of a mark of model in the real world is a discrete sequence of states.

984
02:29:37,860 --> 02:29:47,550
Michael Akira Lee Hayashi: But we can also think about it as being a sequence of probability distributions describing the chance that the system will occupy a specific state at any given time.

985
02:29:48,390 --> 02:29:57,450
Michael Akira Lee Hayashi: And this can be helpful in thinking about the mark of model from both an individual and population perspective or from a number of different sort of analytical perspective.

986
02:29:59,520 --> 02:30:08,760
Michael Akira Lee Hayashi: The other really core part of the mark of model is the transition matrix this thing to find the probability that we go from one state to another state.

987
02:30:09,300 --> 02:30:21,450
Michael Akira Lee Hayashi: When we change states in the system, so this matrix is going to be composed of elements Ai J, where the probability that our state at the next time step is equal to is equal to state by.

988
02:30:22,440 --> 02:30:34,140
Michael Akira Lee Hayashi: Depending on the probability that the system was in state J at the current time is given by this element, so all I think I have an example, the next slide to make that a little more concrete so.

989
02:30:35,070 --> 02:30:48,360
Michael Akira Lee Hayashi: here's my system here's my model i'm fine i'm hungry i'm tired Those are three states, the starting probability distribution let's say is 70% fine 20% hungry and 10% tired.

990
02:30:49,560 --> 02:31:02,670
Michael Akira Lee Hayashi: Note that the number of elements here corresponds to the order in which they're defined in the state vector, so this is normal, this is normal, this is hungry, this is tired.

991
02:31:03,990 --> 02:31:12,570
Michael Akira Lee Hayashi: And then the transition matrix is going to be this thing here that tells me the probability of going from one to another, so this is going to be normal.

992
02:31:13,680 --> 02:31:14,310
Michael Akira Lee Hayashi: hungry.

993
02:31:15,540 --> 02:31:17,340
Michael Akira Lee Hayashi: Tired that he looks like an end.

994
02:31:18,420 --> 02:31:22,650
Michael Akira Lee Hayashi: There we go normal hungry talk.

995
02:31:23,730 --> 02:31:26,760
Michael Akira Lee Hayashi: The way to read this is that we go from column.

996
02:31:27,570 --> 02:31:43,530
Michael Akira Lee Hayashi: To rope so, generally speaking, when we look at matrix operations from a probability perspective we're going from column to a row right so i'm starting normal what's the chance that I go to normal or i'm starting hungry what's the chance that I go to normal so.

997
02:31:45,330 --> 02:31:46,830
Michael Akira Lee Hayashi: So if I say a.

998
02:31:48,420 --> 02:31:49,050
Michael Akira Lee Hayashi: h.

999
02:31:50,940 --> 02:31:51,420
Michael Akira Lee Hayashi: and

1000
02:31:52,470 --> 02:32:00,930
Michael Akira Lee Hayashi: Let me reflect back to the previous slide with this thing defined is the probability that i'm in I, given that I started from Jay so.

1001
02:32:01,380 --> 02:32:13,290
Michael Akira Lee Hayashi: This is going to row I from column J, which is a slightly confusing thing about matrix notation So if I have an let me do this again so a I J, this is the row.

1002
02:32:15,690 --> 02:32:16,680
Michael Akira Lee Hayashi: This is the column.

1003
02:32:17,760 --> 02:32:22,470
Michael Akira Lee Hayashi: But the way to read this from the perspective of a transition matrix is from.

1004
02:32:23,760 --> 02:32:30,330
Michael Akira Lee Hayashi: column two row so from Jay to I right so from.

1005
02:32:31,560 --> 02:32:35,880
Michael Akira Lee Hayashi: This thing means i'm going from an to ah.

1006
02:32:38,970 --> 02:32:45,990
Michael Akira Lee Hayashi: That makes sense any questions this the notation I find often like that that I lose track of a lot.

1007
02:32:55,650 --> 02:33:06,210
Rishi Chanderraj: So the output after multiply the starting state and then transition matrix is where you're at after one step.

1008
02:33:07,200 --> 02:33:17,700
Michael Akira Lee Hayashi: Correct yes so we'll we'll see this in a moment, but if I were to multiply my starting probability my starting stage distribution vector that pie, so if I did, eight times pie.

1009
02:33:18,870 --> 02:33:24,480
Michael Akira Lee Hayashi: But this is going to give me, basically, is pi let's call this pi zero is going to be the next step.

1010
02:33:27,090 --> 02:33:34,380
Michael Akira Lee Hayashi: And for refresher of what this looks like this is a matrix vector multiplication operation so it's basically going to be taking.

1011
02:33:35,700 --> 02:33:47,220
Michael Akira Lee Hayashi: Each let's suppose there's A, B and C it's going to take this multiply it by this plus this multiplied by this plus this multiplied by this, and this then.

1012
02:33:48,780 --> 02:33:50,040
Michael Akira Lee Hayashi: it's going to fill in.

1013
02:33:53,400 --> 02:34:00,360
Michael Akira Lee Hayashi: This element here then we're going to move down a row, do the same thing fill in the element here move down a row fill in this element here i'm going to.

1014
02:34:00,630 --> 02:34:08,760
Michael Akira Lee Hayashi: i'll step back and walk through this again because sometimes this This bears repeating but yes so long story short, if you multiply your.

1015
02:34:09,930 --> 02:34:15,960
Michael Akira Lee Hayashi: Your transition matrix by your state distribution vector to given time, you will get the state distribution vector for the next time.

1016
02:34:18,990 --> 02:34:23,340
Michael Akira Lee Hayashi: And this too will hopefully be made more clear as we work through this so.

1017
02:34:24,480 --> 02:34:38,640
Michael Akira Lee Hayashi: there's a lot that you can do with just these little bits for one if we want to know what the state distribution is after s time steps to suppose I want to know what the state district distribution will be five steps in the future.

1018
02:34:39,690 --> 02:34:57,660
Michael Akira Lee Hayashi: All I have to do is take the transition matrix exponential rate it to the raise it to the power s and then multiply it by mistake distribution factor so for five that would basically be a times a times a times a times a and.

1019
02:34:58,290 --> 02:35:05,640
Michael Akira Lee Hayashi: After screaming a bit we multiply by pie, and this gives pie of see this 012345.

1020
02:35:07,710 --> 02:35:19,440
Michael Akira Lee Hayashi: business is really cool because this lets us essentially look into the future of our system with a really straightforward mathematical operation we we don't even really have to do what I think of as simulation to do this right like.

1021
02:35:19,800 --> 02:35:31,320
Michael Akira Lee Hayashi: This is really powerful, although matrix multiplication is a slightly computationally intensive operation, so if we needed to do this out to like 100 steps we might be waiting for a little bit.

1022
02:35:32,760 --> 02:35:43,200
Michael Akira Lee Hayashi: Another thing Oh well, what this also means is that kind of the the case where we just need to advance one step is just a special case where s equals one.

1023
02:35:44,280 --> 02:35:48,600
Michael Akira Lee Hayashi: So that's kind of Nice to like we get the special case in the more general case.

1024
02:35:49,530 --> 02:36:02,730
Michael Akira Lee Hayashi: there's another special thing that we call the stationary distribution, you might wonder what happens if you start somewhere in your market model and then you you take a large number of steps do you tend to end up anywhere.

1025
02:36:03,660 --> 02:36:15,330
Michael Akira Lee Hayashi: The stationary distribution is that, where you tend to end up thing, so this is defined as some probability distribution vector some state distribution vectors such that if you multiply.

1026
02:36:16,020 --> 02:36:30,090
Michael Akira Lee Hayashi: The transition matrix by that vector you get that same effect what this means that that you don't go anywhere from your from your stationary distribution right if I start here after the next time step i'm still here.

1027
02:36:32,070 --> 02:36:42,330
Michael Akira Lee Hayashi: You can think about this as equivalent to the equilibrium in a compartmentalize one OD model right the equilibrium is a state where that models, no longer changing all the time, derivatives or zero.

1028
02:36:42,570 --> 02:36:50,310
Michael Akira Lee Hayashi: So if you start at that equilibrium you stay at that equilibrium in the same way if you start at the stationary distribution in the market model.

1029
02:36:50,580 --> 02:37:00,120
Michael Akira Lee Hayashi: You stay at the stationary distribution, so this can be a way of characterizing the long term behavior of that model so if my state distribution tells me the fraction of people.

1030
02:37:00,300 --> 02:37:12,420
Michael Akira Lee Hayashi: Who are current form or never smokers the stationary distribution model tells me in the fullness of time what fraction of the population are going to be current former and never smoked so this can be really powerful.

1031
02:37:13,320 --> 02:37:22,110
Michael Akira Lee Hayashi: There is a caveat to it, which means that, in the same way that an equilibrium happens after a long time, and that long time could be a truly long time.

1032
02:37:23,250 --> 02:37:31,650
Michael Akira Lee Hayashi: This is this X in that same way, it might take you a while to reach that stationary distribution so when you're interpreting the results for the model.

1033
02:37:32,130 --> 02:37:46,440
Michael Akira Lee Hayashi: Sometimes you have to be a little bit careful about the amount of time taken to reach station or distribution, so if i'm doing a tobacco usage model and it takes me 100 years to reach a stationary distribution well.

1034
02:37:48,060 --> 02:37:55,320
Michael Akira Lee Hayashi: chances are the things that made people either start or stop smoking changed over that time and so.

1035
02:37:56,070 --> 02:38:03,600
Michael Akira Lee Hayashi: While my model would have been time homogenous the actual world is not, and so my answers might not actually hold up.

1036
02:38:03,870 --> 02:38:09,630
Michael Akira Lee Hayashi: Because the assumption I made about time or margin at the model doesn't hold up over the timeframe of the month.

1037
02:38:09,990 --> 02:38:21,330
Michael Akira Lee Hayashi: On the other hand, if any club rates within like three years and we get a stationary distribution within about three years, we might think that's Okay, because not quite enough has changed for me to find my stationary distribution unreliable.

1038
02:38:21,930 --> 02:38:24,960
Michael Akira Lee Hayashi: Nevertheless, it can still be kind of a helpful tool to have in hand.

1039
02:38:26,040 --> 02:38:29,940
Michael Akira Lee Hayashi: For for thinking about the longer term behavior of market models generally.

1040
02:38:31,440 --> 02:38:33,870
Michael Akira Lee Hayashi: You can solve this analytical oh sorry go ahead.

1041
02:38:34,920 --> 02:38:42,360
Rishi Chanderraj: Is there a way to tell, like other than simulation is there a way to like determine like what.

1042
02:38:43,440 --> 02:38:58,680
Rishi Chanderraj: The I don't know the like is there, like a the if you were to simulate a bunch of different types of be like a distribution of times to stationary like what to it, where you're stable, is there a way to figure that out beforehand, other than simulation.

1043
02:39:02,760 --> 02:39:11,190
Michael Akira Lee Hayashi: I think there is an off the top of my head I don't know the reason I suspect that this is so, is that you're kind of.

1044
02:39:19,470 --> 02:39:23,580
Michael Akira Lee Hayashi: Thinking live is always dangerous because I may or may not come to the right ends.

1045
02:39:24,810 --> 02:39:31,110
Michael Akira Lee Hayashi: The way I would put it, is that it is somewhat initial condition dependent, because you may or may not start closer to the stationary distribution.

1046
02:39:31,800 --> 02:39:46,800
Michael Akira Lee Hayashi: So, from a given starting distribution, it is probably possible to essentially solve for s such that that's the number of steps taken to get you to hear from some pi zero.

1047
02:39:47,670 --> 02:39:52,830
Michael Akira Lee Hayashi: This i'm pretty sure to solve why i'm i'm relatively certain there's some like recursive methods, you could use to do it.

1048
02:39:53,880 --> 02:39:58,980
Michael Akira Lee Hayashi: i'm a better programmer them in the name mathematician so off the top of my head I don't know how to do this, but.

1049
02:39:59,370 --> 02:40:05,430
Michael Akira Lee Hayashi: This would be I bet marissa would come with either know or could come up with a way to do this to solve for that particular thing.

1050
02:40:05,730 --> 02:40:12,270
Michael Akira Lee Hayashi: It strikes me as the kind of problem for which there is a solution because we've kind of got all the bits of it here and it's probably some.

1051
02:40:12,720 --> 02:40:22,230
Michael Akira Lee Hayashi: I don't know it's probably some determinant dragon value or some such thing like there's almost certainly a sequence of linear algebra theorems you can string together solve for that the alternatively.

1052
02:40:23,760 --> 02:40:30,030
Michael Akira Lee Hayashi: What I would the way I would do that as as a computational person is I would.

1053
02:40:30,870 --> 02:40:41,970
Michael Akira Lee Hayashi: Seed myself at a bunch of different randomly generated initial probability distributions and then use the iterative solution method basically to figure out how many times steps per starting state I take to get to.

1054
02:40:42,420 --> 02:40:51,420
Michael Akira Lee Hayashi: To station or distribution and then calculate the average out of that and it'd be slow, but I know how to do that one, as opposed to yeah.

1055
02:40:52,020 --> 02:40:58,800
Rishi Chanderraj: Sorry, I sorry that's a distraction if you're if I feel like if you probably don't encounter at that wedge then it's probably not super important.

1056
02:40:58,920 --> 02:41:04,980
Michael Akira Lee Hayashi: So it's it's a good question, and this is the sort of thing where like, if you want to know, like is my stationary distribution or reasonable.

1057
02:41:05,190 --> 02:41:15,720
Michael Akira Lee Hayashi: answer you probably would actually want to do this, like if I build it tobacco model I probably would actually do that, to give myself a feel for well if it's equal upgrading to.

1058
02:41:16,770 --> 02:41:21,330
Michael Akira Lee Hayashi: To my station or distribution 800 years that's pretty dumb and that's not going to help me at all.

1059
02:41:22,350 --> 02:41:30,480
Michael Akira Lee Hayashi: One nice thing is that you don't necessarily have to engage with the entire scope of computational complexity for this, if you want to look at kind of a more.

1060
02:41:31,020 --> 02:41:41,520
Michael Akira Lee Hayashi: specific circumstance so suppose I want to know suppose I build a tobacco use model, and I want to simulate starting through the current prevalence of usage now.

1061
02:41:42,690 --> 02:41:54,600
Michael Akira Lee Hayashi: Well, I don't care about the other state distributions that might be out there, I just care about the one we have now so that means that all I really need to do is figure out, relatively speaking, what the.

1062
02:41:55,140 --> 02:41:56,670
Michael Akira Lee Hayashi: What the time what the.

1063
02:41:56,730 --> 02:42:06,690
Michael Akira Lee Hayashi: What the number of iterations until convergence to stationary distribution is for this current state distribution vector and that's a lot easier to do so.

1064
02:42:07,800 --> 02:42:09,330
Michael Akira Lee Hayashi: Then and and that.

1065
02:42:09,450 --> 02:42:11,190
Michael Akira Lee Hayashi: That I do absolutely do.

1066
02:42:11,400 --> 02:42:17,340
Michael Akira Lee Hayashi: Because that tells me kind of whether my whether my convergence timescale was halfway reasonable based on.

1067
02:42:18,390 --> 02:42:31,860
Michael Akira Lee Hayashi: What I think might change about kind of the process regulations are almost certainly going to change in 800 years we might we might have gone extinct in that time, and so the, the process is going to change rather substantially and things like that.

1068
02:42:33,510 --> 02:42:42,630
Michael Akira Lee Hayashi: There is a way to you can actually solve for the stationary distribution analytically by solving system of equations given by this expression here it's.

1069
02:42:43,890 --> 02:42:49,650
Michael Akira Lee Hayashi: I forgot the precise solution method, but it is another one of those like determinant eigenvalue style things where.

1070
02:42:50,490 --> 02:42:57,210
Michael Akira Lee Hayashi: It is solvable if I were doing it analytically I go and find my my matrix math book to find out how to do it.

1071
02:42:58,020 --> 02:43:08,880
Michael Akira Lee Hayashi: Because I do not remember off the top of my head, if you do want to know, an analytical solution ask marissa and she could probably dig it up a lot faster actually could derive it where I would not remember how to drive it.

1072
02:43:11,580 --> 02:43:11,910
Michael Akira Lee Hayashi: Did.

1073
02:43:13,110 --> 02:43:19,560
Michael Akira Lee Hayashi: Different sorry as as a little digression, I often tend to think about people in kind of the modeling spaces being.

1074
02:43:20,580 --> 02:43:29,970
Michael Akira Lee Hayashi: More Matthew or more computation brained and we will tend to come at problems from different angles, based on the way that we think about things marissa marissa.

1075
02:43:30,960 --> 02:43:42,630
Michael Akira Lee Hayashi: In her heart of hearts as a mathematician and she's a really good one, so that means that she has the capacity to derive a lot of stuff that I can't like I fully couldn't do that right, I don't have that talk.

1076
02:43:43,680 --> 02:43:47,430
Michael Akira Lee Hayashi: i'm a pretty good program or in a pretty good computational thinker, which means that.

1077
02:43:48,150 --> 02:43:56,130
Michael Akira Lee Hayashi: I can almost certainly come to a solution that involves like numerical solution methods and iterative methods to come to a solution.

1078
02:43:56,400 --> 02:44:01,620
Michael Akira Lee Hayashi: And those are both totally valid right like if you are if you're better at math than you are programming.

1079
02:44:01,860 --> 02:44:12,150
Michael Akira Lee Hayashi: Then lean on that if you're better programming than you are math lean on that and it can it can help you kind of figure out where do I feel like i'm going to fit in sort of this broader realm.

1080
02:44:12,960 --> 02:44:22,890
Michael Akira Lee Hayashi: that's, not to say that someone who's more who has more background and computers and numerical methods can't pick up the math skills it's just that sometimes you know.

1081
02:44:23,730 --> 02:44:32,370
Michael Akira Lee Hayashi: real life constraints mean we have fixed time to learn things so sometimes it's helpful to have a feel for where is my effort best spent and which routes are.

1082
02:44:32,820 --> 02:44:40,530
Michael Akira Lee Hayashi: best suited for me to go like I could probably learn the math, but it would take me more effort than learning numerical methods for example that's why I am the way.

1083
02:44:44,130 --> 02:44:53,730
Michael Akira Lee Hayashi: So for a concrete example we could ask what the probability is that i'm going to be tired after one hour, given that i've started in some initial state so.

1084
02:44:55,740 --> 02:44:56,850
Michael Akira Lee Hayashi: So, given our.

1085
02:44:58,380 --> 02:45:03,570
Michael Akira Lee Hayashi: Starting distribution here of 70% fine 20% hungry 10% tired.

1086
02:45:04,290 --> 02:45:16,260
Michael Akira Lee Hayashi: What I need to do to solve this problem is multiplying my transition matrix by my state distribution vector and then look for the element that corresponds to tired in the results in better that's going to be the third element that vectors.

1087
02:45:16,590 --> 02:45:24,870
Michael Akira Lee Hayashi: So that's why here when I say I want the probability that the state in a time to is equal to stage three, which is tired.

1088
02:45:25,320 --> 02:45:41,520
Michael Akira Lee Hayashi: i'm going to take the third element of the result of multiplying my matrix a by vector pie that happens to be about Point two one, so this notation here is saying what's the probability that at the next time step, I will be tired.

1089
02:45:43,050 --> 02:45:51,420
Michael Akira Lee Hayashi: This denotes the third element, so this thing is going to yield some vector of length three, and this is just going to be the third element that.

1090
02:45:53,520 --> 02:46:08,910
Michael Akira Lee Hayashi: I wanted step back for a moment to do a really quick refresher on matrix multiplication in case anyone has forgotten or if you're like me, you forget the visualization of how this works, so when you multiply two matrices together and this works for vectors two.

1091
02:46:10,140 --> 02:46:23,460
Michael Akira Lee Hayashi: When you multiply two matrices together you start by taking the first row of the Left matrix and the first column of the right matrix and you perform what's called a Doc product multiplication or the some product, so I take.

1092
02:46:24,450 --> 02:46:38,610
Michael Akira Lee Hayashi: This element times this one add on the next element times the next one plus the next one times the next one, and so, and the result of this, some goes in the first row of the first column of the output matrix.

1093
02:46:39,960 --> 02:46:44,580
Michael Akira Lee Hayashi: And then I moved to the next column, and I do the same process we missed.

1094
02:46:46,710 --> 02:46:51,030
Michael Akira Lee Hayashi: And this thing goes here and I just keep going until I fill in this row.

1095
02:46:52,080 --> 02:46:57,750
Michael Akira Lee Hayashi: And once I finished this row now I moved down a row in the Left matrix.

1096
02:46:58,860 --> 02:47:06,750
Michael Akira Lee Hayashi: And carriage return, all the way to the left in the right matrix start with this column and do this, and this and make myself a bunch of spaghetti.

1097
02:47:08,160 --> 02:47:14,640
Michael Akira Lee Hayashi: To fill this in and I keep going so what you're doing when you're calculating matrix multiplication is you're basically going like.

1098
02:47:14,880 --> 02:47:28,590
Michael Akira Lee Hayashi: you're doing kind of a typewriter carriage return that you're like this times this this times this this times this this times this character term drop down multiply this temps this this times this this times this this tips this carriage return drop down and so.

1099
02:47:29,730 --> 02:47:30,660
Michael Akira Lee Hayashi: that's all the tapping.

1100
02:47:32,310 --> 02:47:45,480
Michael Akira Lee Hayashi: I recommend looking up like a little animation of how matrix multiplication works, because that is what helps me, remember, more than anything else in the world what this means to is that matrix vector multiplication fortunately is very straightforward.

1101
02:47:47,010 --> 02:48:04,650
Michael Akira Lee Hayashi: like this is just the case where you only have one clear this where you only have this one column really so you do your matrix you do this times this this times this distance this this and this and because you don't have any other columns you move down a row, and then you do this.

1102
02:48:06,120 --> 02:48:09,540
Michael Akira Lee Hayashi: And this result fills in this element of the resulting effect.

1103
02:48:10,290 --> 02:48:25,860
Michael Akira Lee Hayashi: So that's that's matrix multiplication and matrix vector multiplication in a very short amount of time again, it is often helpful that either write up an example for yourself for look up animations to kind of see how how that process works, but that's that's what's going on here.

1104
02:48:28,320 --> 02:48:38,580
Michael Akira Lee Hayashi: Alright, another use a concrete usage here, I can ask myself what's the probability that i'm going to be hungry in two hours, given that I know that i'm fine now.

1105
02:48:39,030 --> 02:48:49,140
Michael Akira Lee Hayashi: So this is a slightly different formulation the problem now we're no longer working from a starting probability distribution, which would, which would be equivalent to saying I don't know what state i'm in.

1106
02:48:49,560 --> 02:49:01,440
Michael Akira Lee Hayashi: But I want to know what the chances that i'll be in different states later, given that I don't know exactly what state i'm in that now we're going to look at something a little different we're going to say I know exactly what state i'm in that i'm fine.

1107
02:49:03,330 --> 02:49:11,520
Michael Akira Lee Hayashi: So what we're doing is we're constraining the probabilities in our state distribution vector so the way we express this is the probability.

1108
02:49:11,820 --> 02:49:23,160
Michael Akira Lee Hayashi: The state at time three is going to be hungry, given that I know that the state at time one is equal to normal, so now we have this conditional probability.

1109
02:49:24,240 --> 02:49:32,130
Michael Akira Lee Hayashi: The way that we can do this is by relying on our method for calculating the state distribution factor, after some number of time steps.

1110
02:49:32,460 --> 02:49:37,950
Michael Akira Lee Hayashi: and exploiting the fact that there's a very special state distribution vector that represents us knowing.

1111
02:49:38,370 --> 02:49:45,570
Michael Akira Lee Hayashi: The state at a given time and that's the one that puts a one in the state that we know we're in and a zero everywhere else.

1112
02:49:45,930 --> 02:49:54,900
Michael Akira Lee Hayashi: that's cool that makes things easy because, frankly, this is a very easy vector to work with multiplying it by stuff is extremely simple and that's nice so.

1113
02:49:55,740 --> 02:50:05,370
Michael Akira Lee Hayashi: What do we do here, well, we want to look ahead to our so we're going to take our matrix and square it so multiply the state the transition matrix by itself.

1114
02:50:06,120 --> 02:50:14,670
Michael Akira Lee Hayashi: And then take the result of that and multiply it by this special state vector that that that exactly represents me, knowing that i'm normal now.

1115
02:50:15,150 --> 02:50:27,600
Michael Akira Lee Hayashi: And then we look for the second elements of the resulting vector which is going to correspond to the probability that I will be hungry into hours, given that I was normal now and that's about 45%.

1116
02:50:29,130 --> 02:50:34,260
Michael Akira Lee Hayashi: So being able to work between these kind of forms like.

1117
02:50:34,830 --> 02:50:46,650
Michael Akira Lee Hayashi: Being able to work between representing a State distribution as a probability distribution over the states that the system could occupy to given time, but also understanding that if you know the state that system is in right now.

1118
02:50:47,370 --> 02:50:56,430
Michael Akira Lee Hayashi: Then you can create this special kind of degenerate probability distribution vector degenerate state distribution vector that just has a one and a bunch of zeros.

1119
02:50:57,120 --> 02:51:06,630
Michael Akira Lee Hayashi: That gets you a lot of traction in working with and simulating and analyzing Markov models, because that covers a range of cases that you might care about it covers the case where.

1120
02:51:08,070 --> 02:51:19,740
Michael Akira Lee Hayashi: You don't you don't know what state the systems in it also covers the case where you do know the state assistance in the other kinds of secret case it covers, which is sort of a stealth application of Markov models is.

1121
02:51:20,880 --> 02:51:25,290
Michael Akira Lee Hayashi: You can think about the probability distribution vector over states as.

1122
02:51:25,560 --> 02:51:39,060
Michael Akira Lee Hayashi: The population frequency of each State, if you think about this model is representing a population, instead of just one person So what if I wanted to know what the distribution of emotions, is in a clone army of myself.

1123
02:51:39,660 --> 02:51:51,270
Michael Akira Lee Hayashi: Well, this state distribution vector would tell me that 70% or suggest that 70% of those clones are fine 20% of them are hungry and 10% of them are tired and.

1124
02:51:51,900 --> 02:52:03,810
Michael Akira Lee Hayashi: So this is kind of a like a frequency interpretation of that state distribution vector that we can think about the frequency of the relative frequency of people in a population occupy and given state.

1125
02:52:04,140 --> 02:52:15,330
Michael Akira Lee Hayashi: As being roughly equivalent to the probability that any given person in that population occupies that state, assuming that we draw the people uniformly rent that broadly makes sense.

1126
02:52:23,250 --> 02:52:31,080
Michael Akira Lee Hayashi: The reason that this is so convenient is that this lets us essentially simulate a population model really simply because.

1127
02:52:31,410 --> 02:52:37,080
Michael Akira Lee Hayashi: Again suppose, this was a tobacco model suppose I wanted suppose I knew that right now the prevalence of.

1128
02:52:37,710 --> 02:52:46,080
Michael Akira Lee Hayashi: Never smokers was like 30% the influence of former smokers was 40% and the prevalence of current smokers is 30%.

1129
02:52:46,470 --> 02:52:59,430
Michael Akira Lee Hayashi: Then I could just make a mark off model like this and I wouldn't need any more stuff to make it a population model I could then figure out what proportion of the population is a current former never smoked or in 10 years by.

1130
02:53:01,170 --> 02:53:04,050
Michael Akira Lee Hayashi: Doing this and iterating the model 10 steps into the future.

1131
02:53:06,300 --> 02:53:17,820
Michael Akira Lee Hayashi: And that would give me the state distribution vector which is equivalent to the population frequency distribution factor of people occupying those states 10 years in the future 10 steps.

1132
02:53:19,530 --> 02:53:20,220
Michael Akira Lee Hayashi: questions.

1133
02:53:22,080 --> 02:53:23,970
Benjamin Cristol: Are there any like.

1134
02:53:25,020 --> 02:53:36,210
Benjamin Cristol: kind of precision estimates, around the final probabilities and the final vector did you use you use it i'm saying, because it's kind of like the probability is kind of like a prevalence almost.

1135
02:53:37,920 --> 02:53:43,740
Benjamin Cristol: If it's like you know ever smokers or something like that or never smokers at the end of the state.

1136
02:53:44,820 --> 02:53:49,170
Benjamin Cristol: So is there, like a some sort of confidence interval or way to like determine precision.

1137
02:53:51,090 --> 02:53:51,990
Michael Akira Lee Hayashi: So.

1138
02:53:56,580 --> 02:54:00,570
Michael Akira Lee Hayashi: yeah Well, yes, there, there are.

1139
02:54:00,930 --> 02:54:10,530
Michael Akira Lee Hayashi: The is the short answer the long answer, is the way of determining them may or may not be somewhat elaborate, so the.

1140
02:54:14,760 --> 02:54:30,510
Michael Akira Lee Hayashi: Again, maybe you start The short answer for the precision of estimates that you get after iterating the model after simulation model out after some time dependence on the precision of the initial estimate, so if you have some if you have some variance about, about which you like.

1141
02:54:32,100 --> 02:54:41,100
Michael Akira Lee Hayashi: Sorry i'm making a mess there's going to be some range, in which you think you're starting population distribution estimates length.

1142
02:54:42,330 --> 02:54:54,480
Michael Akira Lee Hayashi: That range is then going to determine kind of the range of the output estimates after you simulate your model now again there's almost certainly a way to.

1143
02:54:55,350 --> 02:55:04,860
Michael Akira Lee Hayashi: kind of analytically determine an expression floor what that what those kind of variants balance are in the final thing.

1144
02:55:05,190 --> 02:55:16,560
Michael Akira Lee Hayashi: Compared to the starting when another way of putting this is there's almost certainly a way to analytically calculate how the uncertainty about your starting conditions propagate to uncertainty about your final state.

1145
02:55:17,490 --> 02:55:25,920
Michael Akira Lee Hayashi: Again, because i'm not a mathematician, but I am a pretty okay computational scientists, what I would do is I would run a range of simulations.

1146
02:55:26,550 --> 02:55:41,130
Michael Akira Lee Hayashi: sampling from the range of values that could have generated the starting distribution, so if my initial state distribution is like generally kind of a central point seven plus or minus point one.

1147
02:55:42,210 --> 02:55:43,770
Michael Akira Lee Hayashi: And then, like.

1148
02:55:45,300 --> 02:55:51,360
Michael Akira Lee Hayashi: Point 3.2 plus or minus point oh five I don't know point, point one plus or minus.

1149
02:55:52,380 --> 02:56:08,400
Michael Akira Lee Hayashi: Point no one, what I would do is I would I would draw random samples from state vectors consistent with this with this range and i'd have to be a little bit careful because state vectors have to some to one, so I couldn't give myself a State vector that you know.

1150
02:56:10,410 --> 02:56:23,760
Michael Akira Lee Hayashi: picked values that were like Point seven 1.25 or 3.8 point two, five and point one one, because that would add up to more than one, but if I sampled cleverly from the range of possible state vectors that I could get out of this.

1151
02:56:24,240 --> 02:56:39,690
Michael Akira Lee Hayashi: And then simulated my model for each of those samples and then visualize the range of state distribution vectors that I got out of the simulations That would be my numerical way of giving myself kind of a.

1152
02:56:40,170 --> 02:56:46,740
Michael Akira Lee Hayashi: estimate for the propagation of uncertainty about the starting distribution into the final simulation results.

1153
02:56:50,220 --> 02:56:58,050
Benjamin Cristol: yeah I think yeah, the reason I asked that question is, I was kind of trying to get at like the point you earlier made about the.

1154
02:56:58,440 --> 02:57:05,040
Benjamin Cristol: This Markov model being used in a case like a tobacco study case where you don't have any you know.

1155
02:57:05,760 --> 02:57:11,490
Benjamin Cristol: Information at the start, about you know the percentages of those three categories in the population.

1156
02:57:11,790 --> 02:57:28,410
Benjamin Cristol: So I just because that just made me think of the uncertainty, you had starting off with this model or in that scenario, so I was that made me think of like Okay, then, if you go from that starting point and use that a the transitional matrix times pie, or whatever.

1157
02:57:30,030 --> 02:57:36,480
Benjamin Cristol: How how like how precise is your final you know estimate going to be that's that's kind of where I was coming at it from.

1158
02:57:37,140 --> 02:57:38,760
Michael Akira Lee Hayashi: yeah yeah so.

1159
02:57:41,070 --> 02:57:54,810
Michael Akira Lee Hayashi: So, generally speaking, the the result of applying this procedure to get a while to get a result is going to be accurate to within with whatever machine precision, you have available to you.

1160
02:57:55,440 --> 02:58:08,550
Michael Akira Lee Hayashi: Given the starting state that you put in, so it will be the proper result of starting from wherever you started and going so that being said, if you have uncertainty over where you started, then.

1161
02:58:11,460 --> 02:58:16,860
Michael Akira Lee Hayashi: The result of a specific starting point might not fully represent the uncertainty about where you started.

1162
02:58:17,400 --> 02:58:31,620
Michael Akira Lee Hayashi: In general, in almost every case that I can think of the way that I would tend to handle uncertainty about the starting state in a simulation approach is more simulations and and that manifest in a bunch of different ways, like it might be more simulation sampling from the initial states.

1163
02:58:33,150 --> 02:58:36,780
Michael Akira Lee Hayashi: It might be more simulations to the actual process if it's a process that I can't.

1164
02:58:38,460 --> 02:58:39,690
Michael Akira Lee Hayashi: analytically solve.

1165
02:58:41,100 --> 02:58:43,980
Michael Akira Lee Hayashi: or that's a continuous time process or something but.

1166
02:58:44,700 --> 02:58:51,570
Michael Akira Lee Hayashi: Broadly speaking, like suppose you don't know very much at all about your starting state what you might just do is suppose.

1167
02:58:51,900 --> 02:58:57,390
Michael Akira Lee Hayashi: that you have a wide range of initial states that you could come from and then sample from that wide range.

1168
02:58:57,840 --> 02:59:08,490
Michael Akira Lee Hayashi: Do this process for each of those samples see what each of those gets and then visualize the range of places that you got from this procedure, since this process is initial condition dependent.

1169
02:59:08,850 --> 02:59:17,610
Michael Akira Lee Hayashi: You can do this from any arbitrary starting state which you could generate by sampling a bunch of starting states from the range of values that you find halfway plausible.

1170
02:59:25,050 --> 02:59:26,070
Michael Akira Lee Hayashi: So yeah that's a good question.

1171
02:59:26,100 --> 02:59:27,180
Benjamin Cristol: There yeah.

1172
02:59:29,880 --> 02:59:32,610
Michael Akira Lee Hayashi: yeah there's a lot of there's a lot of kind of interesting operational.

1173
02:59:32,790 --> 02:59:43,740
Michael Akira Lee Hayashi: Aspects like that once you once you get around to trying to use models like this with known data, especially, especially with the understanding the data comes with a lot of uncertainty, even if you have.

1174
02:59:43,980 --> 02:59:49,680
Michael Akira Lee Hayashi: A fairly good corpus to estimate things like a population distribution so yeah so it's.

1175
02:59:51,090 --> 02:59:56,490
Michael Akira Lee Hayashi: it's helpful, I think, to have some of this stuff kind of in your back pocket for when you bumped into one of those situations.

1176
02:59:59,070 --> 02:59:59,940
Michael Akira Lee Hayashi: that's where was.

1177
03:00:02,610 --> 03:00:10,740
Michael Akira Lee Hayashi: there's another feature of Markov models, especially discrete time Markov models that's really nice from a statistical perspective so.

1178
03:00:13,110 --> 03:00:15,510
Michael Akira Lee Hayashi: We were called from from applied statistics.

1179
03:00:15,870 --> 03:00:27,600
Michael Akira Lee Hayashi: When you fit a statistical model to your data you do that by using a procedure typically well unless you're doing bayesian statistics and even then you're using the likelihood summer you'll typically do something called maximum likelihood estimation.

1180
03:00:27,870 --> 03:00:39,960
Michael Akira Lee Hayashi: And what that's doing is it's trying to is trying to calibrate the coefficients of the model, the parameters of the statistical model, such that they maximize the likelihood function.

1181
03:00:40,470 --> 03:00:45,870
Michael Akira Lee Hayashi: That is that corresponds the data to the model likelihood is kind of a strange.

1182
03:00:46,350 --> 03:00:55,740
Michael Akira Lee Hayashi: thing, because when we are fitting the model to data what we'd like is the probability of a model, given the data another way of putting this is.

1183
03:00:56,190 --> 03:01:12,660
Michael Akira Lee Hayashi: How probable is it that our model is a good one for a given data, but we don't have that like we can't calculate that, in general, so instead we calculate this odd backwards thing the likelihood, which is the probability of seeing the data given the model that we have.

1184
03:01:13,710 --> 03:01:28,530
Michael Akira Lee Hayashi: Right that's it's one of the it's one of the core kind of odd counterintuitive things about frequencies statistics that you're assuming that the likelihood is proportional to the actual conditional probability that you care about and then in general that is so that's good.

1185
03:01:30,420 --> 03:01:40,800
Michael Akira Lee Hayashi: But there's another thing about likelihood, which is nice, from our perspective, again, by definition, the light the the likelihood is defined as the probability of data given a model.

1186
03:01:41,280 --> 03:01:50,730
Michael Akira Lee Hayashi: Well, a mark of model can actually give you that directly, because you have a model which essentially generates a synthetic time series so.

1187
03:01:51,240 --> 03:02:04,320
Michael Akira Lee Hayashi: You actually know exactly the probability of generating a given data series given a specified Markov model and the reason for this is that a specified Markov model is a transition matrix essentially.

1188
03:02:04,710 --> 03:02:17,190
Michael Akira Lee Hayashi: Plus potentially a starting state distribution vector and suppose that you're supposed to your data is cohort data for cancer again something like that, and then your cohort data is basically or suppose that your.

1189
03:02:19,020 --> 03:02:29,790
Michael Akira Lee Hayashi: Cancer skin cancer spine um what that data essentially is is a sequence of a time series of variables that's like, no, no, no, no, yes, no, no right.

1190
03:02:30,780 --> 03:02:41,100
Michael Akira Lee Hayashi: But your model generates that exact same sequence right your model can tell you, we started without cancer than at the next step still didn't have cancer, no cancer, no cancer, no cancer cancer.

1191
03:02:41,880 --> 03:02:49,500
Michael Akira Lee Hayashi: And we know what the probability of making those transitions is because we have the transition matrix and we have the starting state so.

1192
03:02:50,400 --> 03:02:56,610
Michael Akira Lee Hayashi: This means that all you have to do to calculate the likelihood in a market model given some time series data.

1193
03:02:57,240 --> 03:03:06,420
Michael Akira Lee Hayashi: is to calculate the probability that the Markov model itself generates the state that is observed in the data at a given observation point.

1194
03:03:06,750 --> 03:03:12,990
Michael Akira Lee Hayashi: And then take the take the product of all of those and, if you want the log likelihood, then you.

1195
03:03:13,620 --> 03:03:21,840
Michael Akira Lee Hayashi: Take the sum of the log of those because sometimes that's easy, so we actually get likelihood for a markup model directly out of the model itself right so.

1196
03:03:22,080 --> 03:03:38,910
Michael Akira Lee Hayashi: This here this expression, the product over all of your time steps of the probability that you're that you move to state X I in the data, given that the state of the model was something at your previous time step.

1197
03:03:41,550 --> 03:03:49,380
Michael Akira Lee Hayashi: You just take the product of all of your of all of the states that the model transits through in a given in a given run and banks are likely.

1198
03:03:49,950 --> 03:03:56,370
Michael Akira Lee Hayashi: By the same token, the log likelihood is directly calculated, and so what this means is that it is very easy.

1199
03:03:56,700 --> 03:04:12,210
Michael Akira Lee Hayashi: In principle, to fit a market model to your time series data, because what you're changing in the market model essentially are the elements of your transition matrix a right, so if I want to know what is the best fit model for this data well that's equivalent to saying.

1200
03:04:14,370 --> 03:04:16,440
Michael Akira Lee Hayashi: A such that.

1201
03:04:17,460 --> 03:04:20,820
Michael Akira Lee Hayashi: We maximize the likelihood.

1202
03:04:22,140 --> 03:04:26,310
Michael Akira Lee Hayashi: Of well of a given X.

1203
03:04:28,290 --> 03:04:31,950
Michael Akira Lee Hayashi: And this is extremely easy to calculate.

1204
03:04:34,200 --> 03:04:44,670
Michael Akira Lee Hayashi: Because, let me, let me, maybe make this more even more concrete so suppose I wanted to use my previous model to calculate likelihood suppose I have a time series which is.

1205
03:04:45,090 --> 03:04:56,700
Michael Akira Lee Hayashi: Normal hungry normal and it's a rather dumb time series But OK suppose, what I do here is, I know that X zero is n so.

1206
03:04:57,720 --> 03:05:09,540
Michael Akira Lee Hayashi: Well what's the probability that my model is here that's one what's the probability that I go from n to ah, this is an to he is point for that probability is point four.

1207
03:05:11,130 --> 03:05:11,760
Michael Akira Lee Hayashi: And then.

1208
03:05:14,250 --> 03:05:15,360
Michael Akira Lee Hayashi: That would give me a.

1209
03:05:16,920 --> 03:05:32,910
Michael Akira Lee Hayashi: Then suppose I go from age to end with the probability of that oops writing this all over the place, he and his than point one, so the total probability of seeing this series is one times point four times point one out of the model.

1210
03:05:34,320 --> 03:05:40,890
Michael Akira Lee Hayashi: And that is point oh four that's about point oh four, so this is the likelihood of our given model.

1211
03:05:42,210 --> 03:05:44,400
Michael Akira Lee Hayashi: For this particular time series data.

1212
03:05:45,420 --> 03:05:59,850
Michael Akira Lee Hayashi: And all we had to do was find the corresponding elements in our transition matrix that told us the probability of transiting states, as specified in the data and then multiplying those together and boom we're done now it's not it is a little harder than that to.

1213
03:06:00,960 --> 03:06:07,260
Michael Akira Lee Hayashi: Change values in the transition matrix in order to do the optimization process but not very much so, like.

1214
03:06:07,890 --> 03:06:11,700
Michael Akira Lee Hayashi: That is actually a fairly straightforward process to, so now we have a way.

1215
03:06:11,940 --> 03:06:21,870
Michael Akira Lee Hayashi: To estimate a best fit discrete time Markov chain for a given time series process with the caveat, of course, that if we did a bad job of specifying the states in the model then.

1216
03:06:22,200 --> 03:06:29,760
Michael Akira Lee Hayashi: No best fit is going to do us any good but that's a typical caveat of modeling if you specify your model wrong garbage in garbage out.

1217
03:06:30,210 --> 03:06:42,300
Michael Akira Lee Hayashi: nothing's going to work, but if you have reasonable confidence in the specification of your model, then you can actually calculate a best fit model for your data really easily and without doing any kind of weird mathematical tricks, it is literally the season.

1218
03:06:44,250 --> 03:06:55,800
Michael Akira Lee Hayashi: sort of my favorite things about markup models, with regard to estimation, like they're one of the weird cases where likelihood is actually intuitive because likelihood is actually something that you can directly extract from the model.

1219
03:06:58,470 --> 03:07:05,340
Michael Akira Lee Hayashi: we've got about five minutes left in this section, and when we when we come back from lunch, I would like to i'd like to have us kind of.

1220
03:07:06,300 --> 03:07:22,320
Michael Akira Lee Hayashi: build a simple tobacco simulation model and then we're going to move into continuous time Markov chains for the afternoon and we'll have some well we'll do some extra stuff and then we'll we'll do a bit of coding in there to any questions, while we're wrapping up for lunch.

1221
03:07:36,570 --> 03:07:41,370
Rishi Chanderraj: I feel like i'm just gonna have to go, I like I like I like I totally get that.

1222
03:07:42,390 --> 03:07:53,280
Rishi Chanderraj: Well, I mean I like it seems like it would be intuitive that the likelihood like you just pull the likelihood from the transition matrix, but I think i'm just going to use a finger about it.

1223
03:07:54,030 --> 03:07:56,040
Michael Akira Lee Hayashi: But yeah but yeah.

1224
03:07:56,160 --> 03:07:59,820
Rishi Chanderraj: For a few minutes before it like totally grasp it so.

1225
03:08:00,510 --> 03:08:04,620
Benjamin Cristol: yeah I have a question so not to get too far off track, but.

1226
03:08:06,780 --> 03:08:27,180
Benjamin Cristol: I mean I a lot of times to help myself like position is new stuff that you know, like kind of get it straight in my head, I think a lot about my son, you know mike's like practical work with like using various forms for regression and so is there a way to kind of.

1227
03:08:28,680 --> 03:08:37,890
Benjamin Cristol: adjust your control for like covariance in this sort of analysis piece of Markov models over here or is that too far afield.

1228
03:08:39,120 --> 03:08:40,560
Michael Akira Lee Hayashi: yeah so i'm.

1229
03:08:42,690 --> 03:08:52,590
Michael Akira Lee Hayashi: Working in between statistical and mechanistic models is always a little bit tricky because it kind of makes my brain do like a right angle turn to to make the correspondence but.

1230
03:08:53,190 --> 03:08:55,200
Michael Akira Lee Hayashi: long story short, yes, essentially so.

1231
03:08:56,190 --> 03:09:07,350
Michael Akira Lee Hayashi: The function kind of kind of kind of to be redundant, the function of covariance in a statistical model is essentially to explain variance in your data that's caused by or attributable to.

1232
03:09:07,590 --> 03:09:17,520
Michael Akira Lee Hayashi: different levels of those covariance I quite like the econometrics framing of statistical models essentially as explaining variants right how much variance point but whatever anyway um.

1233
03:09:18,660 --> 03:09:34,530
Michael Akira Lee Hayashi: So the the rough equivalent in mechanistic modeling often is sort of population stratification of the model itself, such that if I think there's some say demographic factor that influences how people.

1234
03:09:35,310 --> 03:09:38,550
Michael Akira Lee Hayashi: interact with the system like suppose that I think that.

1235
03:09:38,940 --> 03:09:52,710
Michael Akira Lee Hayashi: suppose that I suspect that there's another human out there, whose emotional system doesn't look exactly like this, or whose emotional system transits through these states in a different way, like suppose suppose someone else is much more likely to become tired.

1236
03:09:53,790 --> 03:10:02,370
Michael Akira Lee Hayashi: As opposed to Hungary tonight, then what I could do to represent people like that is essentially to create another transition matrix.

1237
03:10:03,030 --> 03:10:14,160
Michael Akira Lee Hayashi: That uses the same States but as different values in it, and then, when I do my simulations simulations for people who are who have my emotional configuration would use this one.

1238
03:10:14,370 --> 03:10:24,840
Michael Akira Lee Hayashi: simulations for people who have another emotional configuration would use this one, and my overall results would be some composite of people simulated using these different.

1239
03:10:25,410 --> 03:10:33,540
Michael Akira Lee Hayashi: These different transition matrices this can get as complicated as suppose there are people who have more than three emotions, which I have heard is possible.

1240
03:10:34,350 --> 03:10:36,870
Michael Akira Lee Hayashi: Allegedly those people would have.

1241
03:10:37,710 --> 03:10:47,880
Michael Akira Lee Hayashi: More states in their system potentially different ways of translating those States and therefore a different shaped mark off matrix or a different shaped transition matrix of bigger one.

1242
03:10:48,240 --> 03:10:53,220
Michael Akira Lee Hayashi: that's fine too, you can again make a simulated cohort.

1243
03:10:53,910 --> 03:11:03,750
Michael Akira Lee Hayashi: Where some members of that cohort are simulated using one particular model structure and others are simulated using another one that's entirely legal this is this is fine right.

1244
03:11:04,050 --> 03:11:21,570
Michael Akira Lee Hayashi: it's just that it's not necessarily that we would do sort of covariance in a in a mechanistic model in exactly the same way, where it's usually that what those covariance do is they change something about the structure of the model, such that some people in the model are subject to.

1245
03:11:22,770 --> 03:11:29,250
Michael Akira Lee Hayashi: different factors that change their speed or probability of transiting through that system.

1246
03:11:29,550 --> 03:11:38,880
Michael Akira Lee Hayashi: Where others would not or would be subject to different ones, we could also make this a little more unified such that maybe we think that there's some baseline transition matrix and then.

1247
03:11:39,210 --> 03:11:46,080
Michael Akira Lee Hayashi: When we want to simulate people have different demographic configurations we'd multiply a scaling factor onto some of the.

1248
03:11:46,620 --> 03:11:55,020
Michael Akira Lee Hayashi: onto some of the transition probabilities in order to change the transition probabilities for some people with those with those different factors so.

1249
03:11:55,740 --> 03:12:11,640
Michael Akira Lee Hayashi: that's how we tend to address it it's not conceptually I think it is a little bit different because we're less than the business of say just fitting a line to the data, like all statistics is doing is here's my data, how can I do this.

1250
03:12:12,690 --> 03:12:17,700
Michael Akira Lee Hayashi: We kind of do this, but what we care, a lot more about is how can we make something that.

1251
03:12:18,120 --> 03:12:26,850
Michael Akira Lee Hayashi: That represents the processes under the hood that actually made these points in the first place, and that tends to be why we are often more concerned with.

1252
03:12:27,150 --> 03:12:40,890
Michael Akira Lee Hayashi: How do those covariance actually changed the mechanisms in the model, such that people with one or another level of those covariance will will undergo a different mechanistic process than people who don't have those lumps.

1253
03:12:41,220 --> 03:12:42,660
Michael Akira Lee Hayashi: it's not that kind of get at it.

1254
03:12:42,990 --> 03:12:46,650
Benjamin Cristol: yeah no it really does help, and I think that's what you made me.

1255
03:12:47,520 --> 03:12:59,550
Benjamin Cristol: Think of also is kind of this like systems thinking approach where you're kind of taking that throughout the process of developing the Markov model framework, I guess, rather than like kind of.

1256
03:13:00,030 --> 03:13:14,610
Benjamin Cristol: You know one dimension or one or two dimensional or just you know yeah focusing on the whole state kind of throughout the process, and I think I was also thinking about like some causal inference techniques like dogs and.

1257
03:13:15,570 --> 03:13:24,300
Benjamin Cristol: Just like Multifactorial causality and ideology and stuff because when you look at a lot of those various kinds of models, they have like.

1258
03:13:24,630 --> 03:13:33,420
Benjamin Cristol: The web's with all the different arrows going in different directions and all these different factors leading to like the outcome right so that's that's kind of where I was.

1259
03:13:34,050 --> 03:13:42,150
Benjamin Cristol: Like going back in my head with question about the covariance so yeah so I see a link to this stuff a little bit I guess.

1260
03:13:42,690 --> 03:13:45,990
Michael Akira Lee Hayashi: cool yeah when we come back from lunch if anyone's interested.

1261
03:13:46,230 --> 03:13:48,990
Michael Akira Lee Hayashi: I can spend a little bit more time talking about kind of.

1262
03:13:50,160 --> 03:14:01,620
Michael Akira Lee Hayashi: correspondences and distinctions between sort of a statistical modeling or like a like a statistical inference framework, and what I tend to think of is like a systems thinking mechanistic modeling framework, because I think some.

1263
03:14:02,430 --> 03:14:08,670
Michael Akira Lee Hayashi: We often tend to come from one side or the other, on that so if that's interesting I can I can spend a little bit more time on that.

1264
03:14:12,120 --> 03:14:15,150
Benjamin Cristol: yeah that sounds interesting great.

1265
03:14:15,810 --> 03:14:16,230
Welcome

1266
03:14:17,790 --> 03:14:26,490
Michael Akira Lee Hayashi: cool well at the moment, we are at lunchtime, and my emotional state is transmitted to hungry so i'm gonna let you all go and I will see you back here again at.

1267
03:14:27,450 --> 03:14:29,460
Michael Akira Lee Hayashi: 130, I think, is when we finish lunch so.

1268
03:14:31,170 --> 03:14:32,160
Michael Akira Lee Hayashi: get something good deed.

1269
03:14:33,330 --> 03:14:36,270
Michael Akira Lee Hayashi: taken out do whatever you need to do, and I will see you back a little bit.

