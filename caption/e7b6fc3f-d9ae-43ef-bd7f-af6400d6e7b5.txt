1
00:00:01,800 --> 00:00:05,850
Okay. So today we're going to be doing a final quiz review.

2
00:00:06,960 --> 00:00:13,030
So. If you haven't already, go ahead and download that handout.

3
00:00:15,100 --> 00:00:21,300
Well, let's get started. Okay.

4
00:00:21,310 --> 00:00:28,790
So. Oh, I should probably mention, actually, it's back up a second here.

5
00:00:28,790 --> 00:00:39,380
So on the course page. I've made sure that the quiz opening time and closing time are clearly marked.

6
00:00:39,390 --> 00:00:44,690
I think I only had the opening time before. So it's a it's going to be a two hour quiz.

7
00:00:46,410 --> 00:00:50,050
Same kind of same length as the the first quiz.

8
00:00:50,310 --> 00:00:54,210
Just so you get an idea and I'm running it the same way. So.

9
00:00:55,370 --> 00:01:05,270
The the official time is 1030. So 1230, I think if you pay attention to the registrar's office and when they assign you to do finals,

10
00:01:05,630 --> 00:01:12,770
but I'm going ahead and opening it early and letting it close late, just in case you have technical difficulties.

11
00:01:12,770 --> 00:01:19,110
One student last time, for instance, had a refill and all the questions at the last minute because it didn't.

12
00:01:19,130 --> 00:01:28,700
Something weird happened. And so this allows you a lot more protection against weird stuff happening and being able to do everything.

13
00:01:29,540 --> 00:01:37,129
So I recommend that you take notes as you're doing the exam just in case this happens to you,

14
00:01:37,130 --> 00:01:42,080
as it did to this one student, so that if you have to refill in answers the second time,

15
00:01:42,080 --> 00:01:48,920
it's very, very quick based on the notes that you take down so you're allowed to have ahead of time,

16
00:01:48,920 --> 00:01:53,420
you're allowed to bring in two pieces of paper, front and back, kind of a cheat sheet.

17
00:01:55,010 --> 00:02:00,650
Materials and, you know, as many blank pieces of paper as you desire for taking notes.

18
00:02:01,130 --> 00:02:05,900
And you'll need a calculator. Probably so.

19
00:02:05,900 --> 00:02:09,260
But otherwise it's going to be run in the same way it was last time.

20
00:02:09,650 --> 00:02:16,340
And there's there should be a placeholder now if you open up your quizzes option in.

21
00:02:17,940 --> 00:02:22,050
In canvas. You should see a placeholder now for that quiz.

22
00:02:22,700 --> 00:02:28,110
It doesn't actually have any content, but there's like a placeholder for it.

23
00:02:29,210 --> 00:02:35,270
So you should see when your exam opens and closes to check if that looks right to you.

24
00:02:37,770 --> 00:02:42,210
And let me know if that's not the case. On my end, it looks okay.

25
00:02:43,410 --> 00:02:46,780
All right. Okay. Now to the review.

26
00:02:51,520 --> 00:03:01,809
All right. So the majority of the exam or the focus of the exam, this is really everything since we you know,

27
00:03:01,810 --> 00:03:09,550
after the first quiz, you know, how these ideas build up and the same techniques come up over and over again.

28
00:03:10,060 --> 00:03:16,959
So I can't necessarily make a claim that nothing we learned in the first part of the

29
00:03:16,960 --> 00:03:20,290
course is covered in the last part of the course just because there's a lot of overlap.

30
00:03:20,710 --> 00:03:21,970
But I'm not, for instance,

31
00:03:21,970 --> 00:03:30,280
going to go back in time and focus on RC curves or some random thing that doesn't correlate to what we've been doing lately,

32
00:03:30,790 --> 00:03:38,499
but the ability to interpret interaction terms and bills, loan terms, all that stuff is covered for these other modeling techniques as well.

33
00:03:38,500 --> 00:03:45,190
So I, I think you can focus on studying the material that's new since the last quiz,

34
00:03:45,580 --> 00:03:52,450
knowing that the skills about model building the general skills for model building, those are fair game.

35
00:03:52,450 --> 00:04:00,549
I think we've. Been pretty consistent at covering the same types of skills for every model, but interaction terms,

36
00:04:00,550 --> 00:04:06,250
supplying terms, all that stuff you studied for Quiz one and how to deal with estimate statements and everything.

37
00:04:06,880 --> 00:04:12,490
It's the same for this quiz. Those are general skills, regardless of the model you're learning.

38
00:04:13,610 --> 00:04:18,379
All right. So I'm going to briefly review what we learned about survival analysis,

39
00:04:18,380 --> 00:04:22,760
kind of the world where an approach and also regression methods for dependent outcomes.

40
00:04:23,190 --> 00:04:31,940
And then I have some example questions towards the end helping you think about, you know, when do you use which model?

41
00:04:32,300 --> 00:04:38,390
And then a little bit more practice with interpreting output from mixed models because that's

42
00:04:38,390 --> 00:04:43,100
the most recent topic and any extra experience you get with that is probably helpful,

43
00:04:43,100 --> 00:04:48,420
I thought. So first, looking at census survival outcomes.

44
00:04:48,870 --> 00:04:59,130
So this slide is something that we looked at so many times during our work with censored survival data.

45
00:04:59,140 --> 00:05:06,120
So there are two event times working kind of behind the scenes, the event time that we want to.

46
00:05:07,110 --> 00:05:12,899
Right about T.I. and the censoring time, the potential follow up time.

47
00:05:12,900 --> 00:05:20,750
C.I. And we only get to see. Either the end of follow up due to censoring or the end of follow up due to the event.

48
00:05:21,980 --> 00:05:25,340
That's all we get to see the minimum of those two.

49
00:05:26,750 --> 00:05:35,540
And so we call that observable time X II and we have an event indicator that tells us if

50
00:05:35,780 --> 00:05:42,200
this EXI was an observation time that ended due to a death or due to a censoring event.

51
00:05:44,730 --> 00:05:49,559
And just as a reminder in this topic, the field is called survival analysis.

52
00:05:49,560 --> 00:05:53,850
So I might use the word survival endpoint.

53
00:05:54,420 --> 00:06:04,410
And it's it's meant to be a generic term for any type two event, even if it's like timed to a recurrent event or time to.

54
00:06:06,360 --> 00:06:12,719
Exacerbation. It's it's all it's all kind of covered by this generalized term survival.

55
00:06:12,720 --> 00:06:15,810
So it doesn't necessarily mean mortality.

56
00:06:15,810 --> 00:06:21,870
It's the type of analysis. So just be aware, I might lean into that.

57
00:06:22,380 --> 00:06:29,100
And if you have questions about what I if I mean mortality, not just chime in and I'll let you know.

58
00:06:29,880 --> 00:06:35,460
Okay. So excited Delta I that is what appears in your dataset and what appears in all of your code.

59
00:06:36,900 --> 00:06:41,580
But there behind the scenes are these variables that we would prefer to talk about.

60
00:06:41,600 --> 00:06:45,210
Try not exi when we're writing our papers. Right.

61
00:06:45,390 --> 00:06:50,790
But we just don't get to see it on every person due to limited time to watch for those events.

62
00:06:52,690 --> 00:06:57,040
There. There certainly covers are going to come up, especially as we do to sample test.

63
00:06:57,040 --> 00:07:07,330
There's usually a group covariate. And when we do modeling for the Cox model, we have covariates that were used to help predict these event times.

64
00:07:07,810 --> 00:07:14,320
And the key assumption throughout this material is that t i the thing you want to talk

65
00:07:14,320 --> 00:07:19,510
about in your papers is independent of the censoring mechanism for all individuals.

66
00:07:19,510 --> 00:07:29,330
Given whatever covariates are used in your model and in at least one handout that is not covered on the exam, i.

67
00:07:29,560 --> 00:07:34,570
I relaxed this condition using an inverse weight method, but that is not covered on the exam.

68
00:07:35,380 --> 00:07:38,770
So everything. All the material. Kaplan-meier log rank.

69
00:07:38,770 --> 00:07:46,009
Cox model. Those are valid only under this assumption that the thing you want to write about

70
00:07:46,010 --> 00:07:52,459
this event time is independent of the censoring mechanism for all individuals,

71
00:07:52,460 --> 00:08:04,540
given the cover rates that are used in the model. So the typical outcomes we've looked at in the course or time to death or time to recurrence,

72
00:08:04,540 --> 00:08:09,100
but you can imagine a lot of time to events that you might be interested in.

73
00:08:09,530 --> 00:08:14,860
Did we do the prisoner data set in one of our examples like time to re incarceration, I think.

74
00:08:15,700 --> 00:08:19,780
So there's lots you can imagine a lot of different times to event that could be of interest.

75
00:08:21,670 --> 00:08:29,110
The Cox proportional hazards regression model is the the main go to regression model that was covered in the course.

76
00:08:29,140 --> 00:08:39,340
If you went to one of the extra sessions that wasn't required, then you also learned how to do a restricted mean survival time regression models.

77
00:08:39,340 --> 00:08:42,909
But again, that's not covered on the test or the quiz I keep.

78
00:08:42,910 --> 00:08:45,010
I should call it a quiz. It's only 15%.

79
00:08:45,970 --> 00:08:54,520
And so the Cox proportional hazard model is modeling the hazard function at, you know, over the follow up times t.

80
00:08:55,580 --> 00:08:58,620
And the reference group.

81
00:08:58,640 --> 00:09:02,120
Is that just an intercept like it would be for linear regression?

82
00:09:02,120 --> 00:09:05,720
It's actually a whole curve called the baseline hazard curve.

83
00:09:06,200 --> 00:09:10,580
So this kind of replaces the intercept concept that we had from linear regression.

84
00:09:11,000 --> 00:09:16,340
There's no intercept into this exponent. It's kind of all absorbed by this reference curve.

85
00:09:17,480 --> 00:09:24,080
So this is going to be the linear component of your model that has the covariates and the parameters that you estimate.

86
00:09:24,590 --> 00:09:31,100
And the parameters end up having some interpretation corresponding to hazard ratios.

87
00:09:32,860 --> 00:09:41,710
So if you don't have any covariates in your model and you try to estimate the Cox proportional hazards regression model with no covariates,

88
00:09:42,100 --> 00:09:50,800
the only thing you get is kind of like the baseline hazard function, and you can ask for the corresponding baseline survival function.

89
00:09:51,070 --> 00:09:54,220
And if there's no covariates in your model statement,

90
00:09:54,550 --> 00:10:03,040
then this baseline hazard baseline survival function is very similar to what you would get just doing the Kaplan-meier survival estimate.

91
00:10:04,480 --> 00:10:11,170
If you have just a single binary covariate, so z one is just a yes, no kind of a variable.

92
00:10:11,680 --> 00:10:19,059
Then the score test for that parameter estimate say beta one that corresponds to that binary

93
00:10:19,060 --> 00:10:25,090
variable with nothing else in the model is going to be equivalent to the log rank test.

94
00:10:25,090 --> 00:10:29,350
If there's no ties in the event times, it'll be very close otherwise.

95
00:10:29,740 --> 00:10:35,740
So the the Cox model kind of has two of the.

96
00:10:37,020 --> 00:10:46,350
Earlier analysis that we learned survival estimation via kaplan-meier and two sample tests based on the log rank test are kind of nested within.

97
00:10:46,590 --> 00:10:52,740
I shouldn't use the word nested because it's so loaded, but they're kind of analysis that you can replicate.

98
00:10:53,990 --> 00:10:56,560
With within the Cox model framework.

99
00:10:56,570 --> 00:11:03,799
Now, this doesn't let you do things like the gay and wilcoxon test or the gay test, but it does let you do the log rank test.

100
00:11:03,800 --> 00:11:09,500
And this has nothing to do with restricted means, you know, so you can't get to sample test for restricted means,

101
00:11:09,860 --> 00:11:15,590
but you can get at least one of the two sample tests we covered in the course by just having a single binary covariate.

102
00:11:17,750 --> 00:11:23,240
Now, if you additionally have no censoring and you have no covariates in your model,

103
00:11:23,660 --> 00:11:30,469
then the kaplan-meier estimate at any point in time t could be estimated just

104
00:11:30,470 --> 00:11:35,030
with the sample proportion counting the number of people who lived t or longer,

105
00:11:35,030 --> 00:11:39,770
you know, over the total. And so you can.

106
00:11:40,870 --> 00:11:47,620
Really. You don't need to use these models to estimate interest in quantities unless there's censoring.

107
00:11:47,620 --> 00:11:51,519
There's alternative ways to look at the data. If you don't have censoring.

108
00:11:51,520 --> 00:11:56,620
So censoring is the key thing that pushes us towards this whole separate class of analyzes.

109
00:11:58,450 --> 00:12:02,350
So if you do have censoring and you try to do sample proportions,

110
00:12:02,740 --> 00:12:08,850
then you have to make some assumptions that are incorrect about what would have happened after censoring.

111
00:12:08,860 --> 00:12:18,129
So if you assume that at the censoring time they were never going to have the event, after that you'll have a biased, high survival probability.

112
00:12:18,130 --> 00:12:22,030
You're kind of giving them more credit for being alive after the censoring time.

113
00:12:22,030 --> 00:12:32,380
Then that is fair. And if you assume they would have had the event right away, you're, you know, going to have biased low.

114
00:12:32,950 --> 00:12:37,389
That's a little too critical, assuming that the moment they were lost if all they would have had the event.

115
00:12:37,390 --> 00:12:47,320
That's too harsh. So you have to use some method that accounts for censoring correctly and the kaplan-meier does that.

116
00:12:47,320 --> 00:12:51,610
Well, if you don't have any covariance, then you have this independent censoring assumption that's valid.

117
00:12:54,730 --> 00:12:58,420
So we we got to know this dataset pretty well when we're doing survival.

118
00:12:58,420 --> 00:13:06,010
So here it is again. We're looking at the outcome is time in months from diagnosis to death.

119
00:13:06,580 --> 00:13:12,880
And so the X variable is the time from diagnosis to either death or censoring in these patients.

120
00:13:13,600 --> 00:13:19,839
And then the status indicator is telling us if the follow up for them ended because

121
00:13:19,840 --> 00:13:24,540
they died at that time or they were still at risk and alive when they were.

122
00:13:24,550 --> 00:13:31,570
The follow up time measured by X was was was collected and they have a lot of covariates here.

123
00:13:31,570 --> 00:13:37,810
And the one that we focused on the most was the platelet covariate, which is just a binary.

124
00:13:38,780 --> 00:13:49,400
And so it's one normal zero abnormal. So to do things like getting kaplan-meier plots and to get to sample tests,

125
00:13:49,910 --> 00:13:54,230
you're looking for a life test if you're a SAS user and I'll have our code as well.

126
00:13:54,800 --> 00:13:59,400
And so you can get plots. Oh, excuse me, Ed.

127
00:13:59,400 --> 00:14:07,620
Coffee. But it hasn't kicked in yet. I don't know why. I maybe need a double dose so you can get the plots, the kaplan-meier plots with the S,

128
00:14:07,620 --> 00:14:11,770
and then log minus log survival is a diagnostic that is very common.

129
00:14:11,790 --> 00:14:18,300
Not my favorite, I think I've mentioned, but it's a plot that helps you diagnose whether you've got proportional hazards or not.

130
00:14:19,330 --> 00:14:32,350
And the straight up platelet is going to give you test statistics, seeing if there are differences in survival, the gender survival here,

131
00:14:32,350 --> 00:14:39,850
it is actually the actual survival to see if there are differences across these categorical covariates in terms of survival.

132
00:14:41,670 --> 00:14:45,899
Open here, all the little bubbles. All right.

133
00:14:45,900 --> 00:14:52,620
And then our code. You're looking at a surfeit to get these same quantities.

134
00:14:54,060 --> 00:15:03,810
And here's you know, you would be looking for a log on his log survival code like this and kaplan-meier code like like so.

135
00:15:05,530 --> 00:15:16,150
The log rank test that this is going to be a challenge for people who are studying and who are users for the log rank versus the gain wilcoxon test.

136
00:15:16,510 --> 00:15:21,579
You need to pay attention to this row parameter and I'm not necessarily going to be telling

137
00:15:21,580 --> 00:15:26,050
you what's going on with comments the way I do when I'm teaching you how to do stuff.

138
00:15:26,080 --> 00:15:30,879
You're going to have to know because I'm not going to be there with you analyzing your data in the future.

139
00:15:30,880 --> 00:15:37,000
Right. So you're going to have to know that if you put row equal to zero, that's the log rank test that's requested.

140
00:15:37,390 --> 00:15:42,670
If you're an R user and you have to know that if you put row equals one,

141
00:15:42,670 --> 00:15:48,670
that's the gain wilcoxon test that's requested or does it mark the output in a clear way?

142
00:15:49,650 --> 00:15:53,340
So you're going to have to know this is like maybe a cheat sheet thing.

143
00:15:53,340 --> 00:15:58,799
Note if you're an R user to look for that now SAS users, everything is marked in the output.

144
00:15:58,800 --> 00:16:04,350
You don't have to know anything like this. So our users make a note.

145
00:16:04,350 --> 00:16:07,409
This is something you need to keep track of than in your future.

146
00:16:07,410 --> 00:16:08,580
I won't be there to tell you.

147
00:16:08,580 --> 00:16:17,130
Oh remember r equals zero this and that's a you need to maybe make a note of that if you go through the R output if you're an R user.

148
00:16:19,670 --> 00:16:25,249
Here is the. Here are the plots from either SAS or from.

149
00:16:25,250 --> 00:16:31,670
Ah. This isn't quite what SAS gives you, because I put better labels of what the curves are.

150
00:16:32,090 --> 00:16:39,800
But in both of these, the normal platelet group is on top and the abnormal platelet group is on the bottom.

151
00:16:41,130 --> 00:16:46,709
And so being able to interpret various things about this curve that that's important, right?

152
00:16:46,710 --> 00:16:53,970
This is one of the main things that you would put in a paper, these kaplan-meier curves showing you what's going on for various subgroups of interest.

153
00:16:54,450 --> 00:17:06,380
And so let's just sort of remember how this goes. So here is the output from proc life test for the platelet equals zero group.

154
00:17:06,390 --> 00:17:11,400
So this is the abnormal platelet group.

155
00:17:12,090 --> 00:17:22,440
And so the output kind of looks like it'll show you times that were in your data set and it'll show you these are the kaplan-meier.

156
00:17:23,220 --> 00:17:29,070
Yeah, this is life test that these are the KAPLAN-MEIER estimates and standard errors and so on.

157
00:17:29,430 --> 00:17:39,030
And so you need to be able to use this output to find things like probability of survival at some point in time,

158
00:17:39,780 --> 00:17:45,090
because you can see it on the curve, but you're not going to get the exact value unless you look at this output.

159
00:17:45,930 --> 00:17:52,230
And you're going to need to be able to figure out things like median survival time from this kind of output.

160
00:17:53,340 --> 00:18:01,410
And so let's say we wanted to know the survival probability at 24 months for this platelet equals zero group.

161
00:18:01,830 --> 00:18:04,980
So how do we find that value?

162
00:18:06,360 --> 00:18:10,080
And I can't remember if I hid this on your slides or not.

163
00:18:12,960 --> 00:18:18,090
So when I ask you these questions, you're not going to answer me because you don't want to be that person that just reads the slide back to me.

164
00:18:18,420 --> 00:18:24,390
Okay. Yeah. I hate that about teaching. I can't quite make that mastery happen in a seamless way.

165
00:18:24,810 --> 00:18:33,840
So the trick is, you know, the kaplan-meier curve starts off at one at times zero,

166
00:18:33,840 --> 00:18:40,709
and it stays there until an event happens and then it drops and then it is stays at that same value.

167
00:18:40,710 --> 00:18:44,100
Flat, flat, flat, flat. And then it drops the next time there's an event.

168
00:18:44,820 --> 00:18:49,260
So between zero and two, the kaplan-meier curve is one.

169
00:18:50,190 --> 00:18:58,950
And this has two values of two. I think that there were two events, two deaths at this set, this two time point.

170
00:18:59,490 --> 00:19:04,559
And so they are showing us two rows for that at time.

171
00:19:04,560 --> 00:19:09,030
Two, the kaplan-meier survival curve becomes this point seven, seven, seven, eight,

172
00:19:09,630 --> 00:19:16,770
and then between two and six, it stays there until you get to six where there was another event and it dropped.

173
00:19:17,160 --> 00:19:20,790
So if we want to know the survival probability at 24 months,

174
00:19:21,390 --> 00:19:30,480
we need to find the smallest time point that's less than 24 months because that's the last time there was a drop before 24 months.

175
00:19:30,810 --> 00:19:38,610
So between 1935, the same survival estimate is in play all the way through, including the 24 months.

176
00:19:39,450 --> 00:19:44,130
And that survival estimate is going to be the 0.2963.

177
00:19:45,590 --> 00:19:47,090
All right. So that's the trick.

178
00:19:47,090 --> 00:19:55,340
Whatever this survival time point we're looking at, we look to the largest time point that's less than or equal to that time, if you're lucky enough.

179
00:19:55,340 --> 00:19:58,460
And it's in the output because someone had an event exactly.

180
00:19:58,490 --> 00:20:03,889
At that time in the data set, it'll be easy to find, but otherwise you're looking for the the, you know,

181
00:20:03,890 --> 00:20:11,660
the last time there was an event because that's going to stay at the same probability all the way until the next event.

182
00:20:13,640 --> 00:20:25,490
So if I ask for the survival probability at 30 months, it's the same answer because there wasn't an event, an additional event between 19 and 30.

183
00:20:25,730 --> 00:20:29,540
So it's still going to be the same answer if I said 30, 31, 32.

184
00:20:29,960 --> 00:20:33,080
Only when you get to 35 does it drop again and you have a different answer.

185
00:20:36,120 --> 00:20:44,180
Now, if you incorrectly assumed that this person here is this person 13 they were sensors.

186
00:20:44,190 --> 00:20:49,469
If you incorrectly had assumed that they didn't have any risk of death past 13,

187
00:20:49,470 --> 00:20:59,460
that they were going to live forever, then the number of people who survived, you know, past 24 months, you would.

188
00:21:03,910 --> 00:21:08,500
If you assume that they all survive past 24 months, then the.

189
00:21:10,110 --> 00:21:16,620
Mm hmm. How many is this? Uh. One, two, three, four, five, six, seven, eight, nine.

190
00:21:16,920 --> 00:21:25,920
So there's nine people in the dataset. And if you assumed that, uh, this person who.

191
00:21:27,100 --> 00:21:28,600
Was censored here lived.

192
00:21:28,780 --> 00:21:37,920
Then you would have 1 to 3 people who lived past 24 months and you would have had a survival estimate of point three, three, three, three.

193
00:21:38,540 --> 00:21:43,089
Right? And so that's higher than the Kaplan-meier estimate.

194
00:21:43,090 --> 00:21:48,370
So this is just an example of how that would have a biased high survival estimate.

195
00:21:48,640 --> 00:21:56,170
If you assume that this person at 13 would have lived even after their censoring time.

196
00:21:58,800 --> 00:22:02,440
Okay. So and incest.

197
00:22:02,460 --> 00:22:06,150
You'll see those censoring times because they'll just show up with this little asterisks.

198
00:22:06,720 --> 00:22:16,140
And you're supposed to know that if there's an asterisks here, that this information is just supposed to be carried down.

199
00:22:21,620 --> 00:22:28,670
All right. So here's just the same output. But now we're looking for reminding ourselves of the trick of finding median survival time.

200
00:22:29,270 --> 00:22:35,150
So the median survival time is the first time the survival curve drops below 0.5.

201
00:22:36,020 --> 00:22:44,209
And so looking at this data set, we are where we're looking at here, the estimates here.

202
00:22:44,210 --> 00:22:47,750
And when's the first time it dropped below point five?

203
00:22:48,350 --> 00:23:01,890
Well, it's at time 13. And so the general trick is to look to the smallest timepoint where the KAPLAN-MEIER estimates are less than 0.5.

204
00:23:02,220 --> 00:23:07,140
So all the way between seven and 13, the KAPLAN-MEIER estimate is above 0.5.

205
00:23:07,140 --> 00:23:13,530
Is that 0.5556? And so the first time you're below point five is 13.

206
00:23:13,530 --> 00:23:18,360
And so that's the convention that is used for defining median survival time.

207
00:23:20,800 --> 00:23:30,820
Okay. And just so our users get to participate in seeing what their output would look like, this is what the output looks like when you call surfaced.

208
00:23:31,360 --> 00:23:34,780
This is the Kaplan-meier estimate column and they have the time points.

209
00:23:35,170 --> 00:23:39,980
They don't necessarily have the censored values with their own separate row.

210
00:23:40,000 --> 00:23:45,469
They just kind of. You know. Don't show you those arrows.

211
00:23:45,470 --> 00:23:50,380
They just report it for the event times. And so the same trick, though,

212
00:23:50,390 --> 00:23:59,360
the median survival time you look at the first time that this these numbers drop below 0.5 and you pull that time out 13 months and

213
00:23:59,360 --> 00:24:08,990
for the probability of surviving at 24 months same trick you're looking to the largest time point that is less than or equal to 24,

214
00:24:09,230 --> 00:24:14,110
and that's where you're getting your estimate from. This is all coming back, right?

215
00:24:15,630 --> 00:24:20,490
Okay. So these are the log minus log survival curves.

216
00:24:20,490 --> 00:24:24,580
And I have to tell you, there are some people who are in love with these plots for diagnostics.

217
00:24:24,600 --> 00:24:29,280
They're not my favorite just because I think they're like, what does it work?

218
00:24:30,000 --> 00:24:33,630
Rorschach test, the inkblot test. I never remember how to say the name.

219
00:24:34,110 --> 00:24:42,570
And I think you see what you want to see here. So for this particular data set, if you're comparing the abnormal, the normal platelet group,

220
00:24:42,960 --> 00:24:48,360
the way you're supposed to use this diagnostic is to visualize, visualize these vertical lines,

221
00:24:48,840 --> 00:24:56,100
you know, going through and that the distance between the red and the blue at each of the vertical lines,

222
00:24:56,100 --> 00:24:58,980
if there's proportional hazards, those are supposed to be the same length.

223
00:24:59,700 --> 00:25:04,950
And if you want proportional hazards to be okay, you're going to say that's close.

224
00:25:05,190 --> 00:25:10,320
And if you don't want proportional has to be okay, you're going to say, Oh, this vertical distance over here is way bigger.

225
00:25:10,680 --> 00:25:14,940
So I really hate these diagnostic tests. Some people just love them and will ask for them.

226
00:25:14,940 --> 00:25:20,759
If you are, you know, submitting to a journal, you might be asked, so I need to teach you these diagnostics.

227
00:25:20,760 --> 00:25:30,389
But from your homework, I think you probably figured out that my favorite diagnostic was adding in something like log little tee times,

228
00:25:30,390 --> 00:25:31,980
the cover you were concerned about.

229
00:25:32,280 --> 00:25:42,630
So for this example, if I had added in to a Cox model, a covariant like platelet times log little T and created a time covariant,

230
00:25:42,930 --> 00:25:48,540
I would have had a p value to help me see if this is really an important problem or not.

231
00:25:48,540 --> 00:25:59,580
It would give me a much better sense of that a diagnostic than just my eyeballs telling me what I wanted to conclude before I even did the plot.

232
00:26:04,450 --> 00:26:12,280
Oh, and if hazards are proportional, one of the tech homes that we learn is that the log write test performs very, very well.

233
00:26:12,550 --> 00:26:18,070
It actually also performs very, very well. If you have kaplan-meier, curves are getting further and further apart.

234
00:26:18,400 --> 00:26:22,809
It does not do well at all if your curves are getting closer together.

235
00:26:22,810 --> 00:26:28,810
And that's where we did sort of restricted mean testing. All right.

236
00:26:28,810 --> 00:26:33,520
So here's output from SAS and R for the the log rank.

237
00:26:34,300 --> 00:26:38,260
So we know that this is the log rank because it says row equals zero here.

238
00:26:38,260 --> 00:26:41,469
That's the only thing that they show you. They do not tell you.

239
00:26:41,470 --> 00:26:47,680
This is the log rank, which I think is not nice, but.

240
00:26:48,630 --> 00:26:53,010
The sass output will really make it glaringly obvious.

241
00:26:53,010 --> 00:26:56,309
So maybe an AR user wants to use SAS output for this question.

242
00:26:56,310 --> 00:27:02,040
If you're a little worried about remembering row, remember you'll have both of the outputs in front of you.

243
00:27:02,910 --> 00:27:13,739
And so when the hazards aren't crossing and they're acting proportional to the power of the log rank test is really quite good.

244
00:27:13,740 --> 00:27:19,620
It's supposed to be a more powerful test than, say, the wilcoxon of the game wilcoxon test.

245
00:27:20,130 --> 00:27:26,940
And so if you look at the output from the game Wilcoxon test, the p value is a bit higher in both of these situations.

246
00:27:27,180 --> 00:27:32,670
Again, we have to just in r you have to know row equals one is giving you the gain wilcoxon test.

247
00:27:36,970 --> 00:27:41,440
Okay. So we also learned on Tuesday, April 10th,

248
00:27:41,440 --> 00:27:47,680
that's kind of looking at the area between the survival curves and using that to tell if there are differences in survival.

249
00:27:48,250 --> 00:27:58,720
And so for this data set, both of the kaplan-meier curves, both both of those groups, the last event was a death time.

250
00:27:59,020 --> 00:28:04,179
And so we can actually do that for that very reason.

251
00:28:04,180 --> 00:28:11,410
We can actually do a Tuesday, April test comparing the means, not the restricted means, but the means all the way through time.

252
00:28:11,860 --> 00:28:18,190
Since we have kaplan-meier curves that go all the way down to zero, the area under the kaplan-meier curve all the way.

253
00:28:18,460 --> 00:28:23,380
If you have estimates all the way where it drops down to zero, it's actually an estimate of the mean survival time.

254
00:28:24,270 --> 00:28:27,950
And so you can request that with these are as two things.

255
00:28:27,960 --> 00:28:32,070
Actually, I never even use the plot. I don't know why keep on putting our t in the plots.

256
00:28:32,310 --> 00:28:36,750
I never use them. But this armistice over here is giving us what we need to do.

257
00:28:36,750 --> 00:28:40,620
Inference for restricted means survival times for the two different groups.

258
00:28:43,130 --> 00:28:51,500
And so you can specify a total restricted mean by putting something like Tao equals some number after the parentheses.

259
00:28:51,500 --> 00:28:55,220
You probably saw that when you were working on your homework.

260
00:28:56,670 --> 00:28:59,880
But that number has to be within the range of the observed data.

261
00:29:00,570 --> 00:29:11,900
So it you know, we in this example, we calculate the restricted mean by platelet group and put the output into the test cystic by hand.

262
00:29:11,910 --> 00:29:15,810
Our users have a way to get the test to be done for you.

263
00:29:16,110 --> 00:29:20,550
So our users get a little extra edge for this particular test.

264
00:29:21,330 --> 00:29:28,430
So the our code. Putting in a tower of 92, which was, you know,

265
00:29:28,480 --> 00:29:43,690
pretty much the last observed event time in the dataset there's this library package serve aam two with an arm as to function this was shown in

266
00:29:43,690 --> 00:29:52,210
lab I think in my in my hand and I just had this as a footnote of code you could use but this was shown in lab and it will do the test for you.

267
00:29:53,730 --> 00:29:57,120
So for Sass users,

268
00:29:57,360 --> 00:30:06,389
you're going to see output that kind of looks like this and you have to know how to put these estimates and standard errors into a test statistic.

269
00:30:06,390 --> 00:30:12,930
To perform the test, you have to kind of do that by hand. So maybe you want to make a note just to make sure you know how to do that by hand.

270
00:30:14,250 --> 00:30:19,950
This is what the R output looks like if you used the function that I had in my main handout.

271
00:30:21,170 --> 00:30:25,430
And so if that's what you have,

272
00:30:25,430 --> 00:30:34,310
then you have to kind of create this test just using these various means and standard errors and look up your p value in some table.

273
00:30:35,860 --> 00:30:42,050
The manuscript worthy sentence would be something like the average lifetime from multiple.

274
00:30:42,070 --> 00:30:49,450
My global diagnosis was 34.5 versus 18.9 months.

275
00:30:49,450 --> 00:30:54,790
For patients with normal versus abnormal platelets respectively, p equals up about okay.

276
00:30:55,330 --> 00:31:01,510
And you can always add in confidence limits because you've got the standard errors for these things.

277
00:31:01,900 --> 00:31:07,300
And so you can come up with confidence limits like estimate plus or -1.9, six times the standard error.

278
00:31:07,660 --> 00:31:13,300
If you wanted to put confidence limits after these things to improve your manuscript where this sentence.

279
00:31:16,030 --> 00:31:20,589
And notice I didn't have anything about restricted lifetime of lifetime lived

280
00:31:20,590 --> 00:31:25,880
during X months of follow ups because all of these curves went down to zero.

281
00:31:25,900 --> 00:31:31,000
So I'm talking about average lifetime rather than the lifetime lived over some follow up period.

282
00:31:33,750 --> 00:31:47,190
So the lab code that was taught was our EMC2, and I made a note to myself to include this code next time I teach because it's so much nicer.

283
00:31:47,190 --> 00:31:53,820
I mean, not only do you get the estimates in the standard errors, but you get confidence limits for those separately.

284
00:31:54,090 --> 00:32:01,020
And the the test statistic is here or down over here and you get confidence limits

285
00:32:01,020 --> 00:32:08,729
for the estimated difference between the two groups and the p value as well.

286
00:32:08,730 --> 00:32:20,430
So this is this is the function that wins the game within R and SAS, that this will give you anything you need for a manuscript worthy sentence.

287
00:32:20,430 --> 00:32:27,739
And so I think this is my new favorite. And this is just repeating the sentence from before.

288
00:32:27,740 --> 00:32:32,150
But now if you're an R user, when you see this 34.5,

289
00:32:32,630 --> 00:32:42,200
you can add in that parent that it confidence limit so easily with this function and this 18.9, you know, we can get that confidence limit here.

290
00:32:42,200 --> 00:32:46,640
And so it just makes the manuscript a little bit richer and easy to make richer.

291
00:32:50,440 --> 00:32:55,989
All right. So if you look at proportional hazards models, we're kind of moving into the modeling situation and,

292
00:32:55,990 --> 00:33:07,780
you know, have no covariates, but you use this stratified baseline hazard option by the platelet group.

293
00:33:08,290 --> 00:33:14,950
This is what that code would look like. So no covariates means there's nothing over here in our model statement.

294
00:33:16,050 --> 00:33:19,450
This is Greg kind of here.

295
00:33:19,980 --> 00:33:22,350
Cox will be for hour. We'll show that. Cut a minute.

296
00:33:22,560 --> 00:33:31,830
So if you put in this straight a platelet, you're basically saying I don't want to assume proportional hazards for the platelet group.

297
00:33:32,400 --> 00:33:37,710
And in this particular situation, if you also add this method equals PRL,

298
00:33:38,010 --> 00:33:43,319
you're kind of asking for a baseline survival estimation so that you'll get

299
00:33:43,320 --> 00:33:47,400
exactly the KAPLAN-MEIER estimate for the two platelet groups in this case.

300
00:33:48,710 --> 00:33:50,750
I think if you don't use this particular method,

301
00:33:50,750 --> 00:33:56,750
you'll get you'll get still get survival curves that are kind of similar to the kaplan-meier, but it won't be algebraically the same.

302
00:33:57,170 --> 00:34:02,250
So this is kind of a nice option to use. Um, and here's the R code.

303
00:34:02,270 --> 00:34:05,390
So in our code, when they have straight and a variable,

304
00:34:05,840 --> 00:34:12,560
it means that they're stratifying the estimation of the baseline hazard in a way that doesn't require proportional hazards.

305
00:34:12,860 --> 00:34:21,560
And to get the version of the baseline estimates that correspond to Kaplan-meier, you use this as type equals one.

306
00:34:23,840 --> 00:34:30,500
All right. And so and here is the output for just the platelet equals zero group.

307
00:34:30,920 --> 00:34:34,220
So this is how you tell, in fact, that that is what you're looking at.

308
00:34:34,580 --> 00:34:42,320
And this is how you tell an R. That's what you're looking at. And this is basically the baseline survival estimate from the Cox model.

309
00:34:42,830 --> 00:34:49,670
All right. Because if you don't have any covariates in your model Y, so this is only one of them.

310
00:34:49,670 --> 00:34:57,770
This is the baseline survival for group four. For this this you have to actually baseline survival curves when you use the straight statement.

311
00:34:59,830 --> 00:35:05,200
All right. So this is going to be the same as the kaplan-meier for the legal serogroup that we saw earlier because we stratified.

312
00:35:06,950 --> 00:35:11,269
And that's different from the baseline survival curve you get if you instead put

313
00:35:11,270 --> 00:35:15,290
platelet in your model statement and assume that they have proportional hazards.

314
00:35:16,130 --> 00:35:21,020
So if you do that, so there's no straight a statement here if you do that.

315
00:35:21,470 --> 00:35:27,440
And I'm and this time I actually am asking for the baseline survival.

316
00:35:27,950 --> 00:35:32,180
Remember that trick where you have the my Corvair kind of risk profile?

317
00:35:32,490 --> 00:35:35,990
Also, I'm asking for the risk profile where platelet equals zero.

318
00:35:36,410 --> 00:35:41,149
And for this model, that is the model where all the covariates are set to zero.

319
00:35:41,150 --> 00:35:47,120
So this is going to be asking for the baseline survival that I'm printing out here.

320
00:35:47,450 --> 00:35:49,040
And so if you print that out,

321
00:35:49,490 --> 00:35:59,240
same kind of idea here in R where you have a kind of my cover asking for the risk profile that has zero for platelet and putting it out.

322
00:35:59,900 --> 00:36:05,650
So this is what you get for the platelet equals zero group.

323
00:36:05,660 --> 00:36:12,320
So now that we have a model state with covariates in it, just one covariate, this is now the baseline survival.

324
00:36:14,310 --> 00:36:16,740
And it's if you if you have it in your output,

325
00:36:16,740 --> 00:36:22,260
it's nice to know you have it because if you have the baseline survival and you have the parameter estimate table,

326
00:36:22,560 --> 00:36:26,160
you can get the survival estimate for any risk profile by hand.

327
00:36:26,190 --> 00:36:31,140
I'll show you. I remind you about that trick in a minute. But so this is a lot more.

328
00:36:32,180 --> 00:36:35,629
Numbers than just the kaplan-meier curve we saw for this group.

329
00:36:35,630 --> 00:36:43,310
Those tables were very small. When you assume proportional hazards, the baseline survival curve is changing at death times.

330
00:36:45,720 --> 00:36:47,910
Where the death could have happened in either group.

331
00:36:48,030 --> 00:36:53,760
So for either group, if there was a death time, you suddenly have a baseline survival curve that drops.

332
00:36:55,460 --> 00:36:59,570
Whereas for the Kaplan-meier curve, if you were looking at the platelet zero group,

333
00:36:59,570 --> 00:37:03,800
it would only drop when someone with platelet equals zero had a death.

334
00:37:04,490 --> 00:37:10,729
But in proportional hazards, they're using information from both groups and leaning on this proportional hazards assumption

335
00:37:10,730 --> 00:37:14,810
and letting the curve drop even if the death was from the platelet equals one group.

336
00:37:15,500 --> 00:37:20,540
So that's being built into the estimation process. So this is quite different.

337
00:37:22,430 --> 00:37:28,820
Whether you put the couvert in the model statement or you put the cover it in the straight statement, you're doing two quite different things.

338
00:37:29,270 --> 00:37:32,179
And in the model statement, you're assuming proportional hazards,

339
00:37:32,180 --> 00:37:37,160
and you're going to get this weirdo curve that looks like you had a lot more data in that group than you really did.

340
00:37:42,160 --> 00:37:52,960
Okay. And so when you're looking at the beta coefficient and looking at test statistics related to that beta coefficient for the platelet variable,

341
00:37:53,470 --> 00:37:57,940
the score test in the output is going to look very much like the log rank test.

342
00:37:58,360 --> 00:38:06,030
The it's not identical in this particular case because some of the death times were tied in the data set.

343
00:38:06,040 --> 00:38:13,090
If there were no ties of death times in the data set, the score test would be algebraically equal to the log road test.

344
00:38:13,360 --> 00:38:17,590
It's close, but not the same when there ties in the death times for the data set.

345
00:38:18,760 --> 00:38:21,900
All right. So and so that's the score test.

346
00:38:21,910 --> 00:38:26,140
You don't usually see the score test in your in your output when you're looking.

347
00:38:26,350 --> 00:38:32,380
This is where I usually look for my test statistics and P values because it's so easy and given to you by SAS.

348
00:38:32,710 --> 00:38:38,350
So this is based on the world statistic and it's not going to be the same as the score of the log rank.

349
00:38:38,350 --> 00:38:45,330
But, you know, they're sort of scientifically close. They're both kind of marginally significant here.

350
00:38:47,150 --> 00:38:54,590
And so the hazard ratio that you're seeing in the output is coming from E to whatever the parameter estimate of over here.

351
00:38:54,590 --> 00:38:59,300
So each of the beta in these models, they tend to be some hazard ratio of interest.

352
00:38:59,840 --> 00:39:08,030
If you have interaction terms in your model statement, you may or may not be given E to the betas for everything,

353
00:39:08,030 --> 00:39:12,769
but I think SAS is smart enough not to call them hazard ratios anymore because a lot of the has,

354
00:39:12,770 --> 00:39:19,040
if you have interaction terms, the hazard ratios might involve more than one parameter and they try to avoid giving them to

355
00:39:19,040 --> 00:39:23,570
you for that reason are we'll just give you each the beta no matter what you have to know.

356
00:39:23,780 --> 00:39:26,810
Those aren't really hazard ratios when you have interaction terms.

357
00:39:27,890 --> 00:39:31,340
But here it works just fine because we just have one convert that's binary.

358
00:39:31,340 --> 00:39:36,829
And so that's how you get the hazard ratio of 0.5 and you can, you know,

359
00:39:36,830 --> 00:39:44,420
work out the confidence limits by hand using the usual tricks of E to the estimate, plus or -1.96 on the standard error.

360
00:39:44,720 --> 00:39:53,150
Or just look over here. If, if I'm programing and I'm asking about these things, I'm probably just going to program to show you this there.

361
00:39:53,660 --> 00:39:58,340
So but you can always you can always do it by hand if you need to.

362
00:40:00,370 --> 00:40:07,029
And so for our output, there was this really nice, pretty function that we could use to make the output pretty.

363
00:40:07,030 --> 00:40:10,190
And it gives you everything you need for a manuscript worthy sentence right there.

364
00:40:10,230 --> 00:40:15,400
So easy to find. Whereas in SAS you kind of have to hunt it down a little bit more, you know?

365
00:40:17,850 --> 00:40:24,179
And so the manuscript where the sentence would be that those with normal platelets at diagnosis were observed with marginal

366
00:40:24,180 --> 00:40:32,460
statistical significance to have half the hazard of those with abnormal platelets hazard ratio point five Confidence Interval P-value.

367
00:40:32,730 --> 00:40:40,110
And of course, there's more than one way to write these sentences, but this this is, you know, this is a perfectly lovely sentence.

368
00:40:44,960 --> 00:40:49,880
So estimating survival when you've got non-zero covariance.

369
00:40:49,880 --> 00:40:53,330
So there was we had a homework problem when we did this in several ways.

370
00:40:53,330 --> 00:40:58,790
And so one way is to do it by hand. If the baseline survival curve is in your output,

371
00:40:59,150 --> 00:41:10,190
then you can find the time point that you need and plug in all the parameter estimates that are in your model times the risk profile values for Z one,

372
00:41:10,190 --> 00:41:13,280
Z two, and so on. And you can do that by hand.

373
00:41:13,280 --> 00:41:18,200
So for Platelet Group one, for example, which is a simple binary covariate,

374
00:41:20,000 --> 00:41:26,750
I can get the survival time at 24 months for the platelet equals one group just by taking that baseline

375
00:41:26,750 --> 00:41:31,640
survival at 24 months for the platelet equals zero group and raising it to either the beta one.

376
00:41:32,780 --> 00:41:37,370
And so in SAS, R and R, this is what that would look like.

377
00:41:37,370 --> 00:41:41,840
So remember, the hazard ratio is 0.5. So that's the beta one that's just here and here.

378
00:41:42,140 --> 00:41:51,050
And then this part was in the output for the platelet equals zero survival at 24 months.

379
00:41:54,120 --> 00:42:02,040
So. Right. So remember when you're looking for that in your output, you're going to do what's the largest time point?

380
00:42:02,070 --> 00:42:06,480
Oh, and look, we have 24 months. Exactly. We just have to pull it right there.

381
00:42:06,780 --> 00:42:14,099
If 24 months hadn't been there, you're looking for the the the time that's less than or equal to that time point of interest.

382
00:42:14,100 --> 00:42:21,840
And here you're just lucky enough that 24 is actually in this output because at least one of the people in the data set had a death at 24.

383
00:42:26,180 --> 00:42:34,759
Okay. So you can do this by hand. Or if you're programing, there's a way to program it more directly.

384
00:42:34,760 --> 00:42:44,780
And so that you have kind of a mike over star style data set where you put in the risk profile for the cover you want to look at.

385
00:42:44,810 --> 00:42:55,640
So here I'm putting plate equals one. And then covariates equals whatever your makeover dataset looks like here and then proc print.

386
00:42:56,150 --> 00:43:00,170
Our code has kind of similar corresponding tricks.

387
00:43:01,270 --> 00:43:04,810
And this is what it will look like in your output.

388
00:43:04,820 --> 00:43:09,230
So here in SAS it'll tell you what the cover it was, you know.

389
00:43:09,240 --> 00:43:14,620
So that's kind of nice so that you know, that the survival time for the plate equals one group is over here.

390
00:43:15,770 --> 00:43:21,430
And for our. It doesn't necessarily tell you what you're covering its orbit.

391
00:43:21,610 --> 00:43:25,330
So you have to know what you asked for in the program. But you get some similar numbers here.

392
00:43:27,550 --> 00:43:31,090
So there's the backhand and then there's the in your output.

393
00:43:31,090 --> 00:43:39,550
If you see like a little data set that looks like a risk profile of covariance and then it's put into the, uh, the code.

394
00:43:40,990 --> 00:43:45,490
You know, asking for some survival stuff here.

395
00:43:46,990 --> 00:43:54,940
Or it's put in here. Then you'll know that some of the output is is related to that particular risk profile.

396
00:43:57,830 --> 00:44:01,969
Okay. So confounders the Cox model, confounders.

397
00:44:01,970 --> 00:44:07,250
So Cox model has two ways to adjust for confounders.

398
00:44:07,250 --> 00:44:10,010
So this is a bit different from every other model we learned.

399
00:44:10,550 --> 00:44:17,960
So one way to adjust for confounder is to do it the way we did for every other model in the class,

400
00:44:18,860 --> 00:44:21,920
and that is to put the confounder in the model statement.

401
00:44:22,250 --> 00:44:28,910
But Cox model also lets you adjust for confounders by stratifying the baseline hazard for those values.

402
00:44:29,240 --> 00:44:35,990
So suppose the confounder C has three categories then what can you do?

403
00:44:35,990 --> 00:44:40,880
The Cox models the method. One, you can use indicator variables for C in the regression model.

404
00:44:41,060 --> 00:44:47,060
So this is like putting C in the class statement for SAS or an R saying as don't factor in the variable

405
00:44:47,240 --> 00:44:53,810
in the model statement for R and that will adjust for it just like you usually would in your classes.

406
00:44:54,290 --> 00:45:01,790
When you do it this way, you're assuming proportional hazards is in effect for different levels of these categories.

407
00:45:01,790 --> 00:45:07,850
So the shapes of the hazards are assumed to be the same within each level of the confounder.

408
00:45:08,600 --> 00:45:17,060
And so remember, for male, if you put in your homework, you remember we were looking at male and proportional hazards for male.

409
00:45:17,510 --> 00:45:23,030
So if males in the model statement, you're assuming that the hazard for male and female is the same over time.

410
00:45:23,030 --> 00:45:28,760
They're having the same peaks and the same values of bad times for survival in each of those groups.

411
00:45:29,090 --> 00:45:37,830
That's. That's what you're assuming when you put. This categorical variable in your model statement that you're okay with that assumption?

412
00:45:38,130 --> 00:45:44,460
It's the same assumption you make for all the terms in your model statement for the the Cox model.

413
00:45:47,150 --> 00:45:55,130
Well, you know, if you're willing to make that assumption, you get all this nice stuff so you can interpret and tabulate the hazard ratios

414
00:45:55,460 --> 00:45:59,570
that are related to this confounder and get confidence intervals for those.

415
00:45:59,570 --> 00:46:04,130
And so you get to improvise. You get to interpret those really nicely.

416
00:46:05,720 --> 00:46:09,650
But you're leaning on proportional hazards assumptions to make those inferences.

417
00:46:09,980 --> 00:46:20,240
So the Cox model also has a second way to adjust for the confounder, and that's to stratify the baseline hazard by the three levels of the confounder.

418
00:46:20,240 --> 00:46:30,110
So you have, you know, strata male or something, and then it's estimating different baseline hazards for each of the levels of the confounder.

419
00:46:31,510 --> 00:46:38,399
Okay. And so. The shapes of the hazards are allowed to be different across the sea levels.

420
00:46:38,400 --> 00:46:49,100
If you're estimating different hazards for those groups and it might be necessary to do this if you've got some non proportional hazards, you know,

421
00:46:49,560 --> 00:46:56,670
but the downside is that you don't have hazard ratios to put in your manuscript's table

422
00:46:56,970 --> 00:47:02,280
and you don't have a p value for that particular confounder in your manuscript table.

423
00:47:02,520 --> 00:47:05,970
You just have less ability to describe what's going on with the confounder.

424
00:47:06,300 --> 00:47:11,970
So if the confounder doesn't have any intrinsic interest that you want to discuss, that's fine.

425
00:47:12,150 --> 00:47:17,639
It's kind of a nuisance variable, and stratifying by that nuisance categorical variable is fine.

426
00:47:17,640 --> 00:47:22,560
You've adjusted for its effects on the exposure that you really want to talk about in your manuscript.

427
00:47:24,360 --> 00:47:32,310
But if you really need to talk about the confounder the and you want to have it adjusted for and all that other good stuff,

428
00:47:34,020 --> 00:47:41,430
so you want to put it in the model statement. There's one other way that you can deal with this and have stuff to talk about in your manuscript.

429
00:47:42,000 --> 00:47:45,750
Do you do you want to think about that for a second?

430
00:47:45,960 --> 00:47:54,300
So if you don't think hazards are proportional over time, but you still want to have p values and ways to talk about it in your paper?

431
00:47:54,840 --> 00:47:59,160
What's one method we've looked at where you could do that?

432
00:48:01,790 --> 00:48:07,520
And my big hint is that this was one way we tested for proportional hazards in homework.

433
00:48:10,500 --> 00:48:17,850
Any thoughts? Okay. Maybe this was a little bit nuanced question, but if you want to talk about, say,

434
00:48:17,850 --> 00:48:29,009
mail and the risk according to your your sex, then you could actually model mail times,

435
00:48:29,010 --> 00:48:36,659
some log tea or some other time dependent thing and talk about how the hazard ratio for mail changes over time increases,

436
00:48:36,660 --> 00:48:37,860
decreases and all that stuff.

437
00:48:38,160 --> 00:48:48,420
You'd have p values to have that conversation if you put mail time something in the model like log t that let you describe hazard for males.

438
00:48:48,420 --> 00:48:52,380
But if I'm not interested in that particular variable being a story in my paper,

439
00:48:52,740 --> 00:49:03,420
stratifying it in the strata statement is fine and it'll still give you what you want to talk about for your exposure, for your paper.

440
00:49:03,810 --> 00:49:08,310
So it all kind of depends on how important is the confounder as a story in and of itself.

441
00:49:09,000 --> 00:49:16,469
You know, if you want to go to the extra trouble of modeling it as a time dependent variable, you know, mail times log t is the example.

442
00:49:16,470 --> 00:49:23,810
I keep coming to you because you did that in your homework. So diagnostics for assessing proportionality.

443
00:49:23,810 --> 00:49:27,799
We've already talked about plots of log minus log survival versus two year log

444
00:49:27,800 --> 00:49:34,070
t and being a constant distance apart for categorical or continuous predictors.

445
00:49:35,990 --> 00:49:41,420
My preferred method is to check models with interactions between the predictor that you're worried about and time

446
00:49:41,420 --> 00:49:50,120
dependent covariates such as log time or even linear spline representations of time or time interval indicators.

447
00:49:50,120 --> 00:49:54,610
Oh, I thought I uploaded a version of this handout where this soon didn't appear.

448
00:49:55,970 --> 00:50:03,470
Maybe I didn't do that. Okay. I thought I did. I'll. I'll do it after class because I'm not talking about linear spline representations any time soon.

449
00:50:04,100 --> 00:50:08,479
So X over the word soon. So. But not to distract.

450
00:50:08,480 --> 00:50:19,860
So the. If you have a significant p value with any covariate interacting with log little T in these time dependent kind of covariates,

451
00:50:20,190 --> 00:50:25,080
then that's kind of strength of evidence that they're not proportional hazards because the hazard

452
00:50:25,950 --> 00:50:30,120
when you write out the formula for the hazard if it still has little T and the expression.

453
00:50:30,980 --> 00:50:34,940
For the hazard ratio, then it's not a proportional hazard situation anymore.

454
00:50:38,790 --> 00:50:46,769
So all of these terms of a predictor interactive with little T will allow for the hazard ratios depend on time if

455
00:50:46,770 --> 00:50:54,389
the corresponding parameters nonzero so that's a diagnostic there's also a residual that you learn how to do.

456
00:50:54,390 --> 00:50:59,850
The Schoenfeld residuals plotted against the corresponding observed Death Times shouldn't show a trend and

457
00:50:59,850 --> 00:51:03,810
that's another trick that you did in your homework to kind of look to see if there was proportional hazards.

458
00:51:04,200 --> 00:51:07,740
And you can do a low s line through that to kind of set get a sense.

459
00:51:08,160 --> 00:51:16,020
But but honestly, for me, any plot where I'm supposed to make a judgment call based on a visual, I don't trust myself.

460
00:51:16,380 --> 00:51:23,360
So this is my favorite way to assess whether there's something important going on in the data that value violates proportional hazards.

461
00:51:23,370 --> 00:51:26,680
The middle one. That's for me to you.

462
00:51:27,140 --> 00:51:31,600
Reviewers are going to ask you for what they're going to ask you for, and they could ask you for any of these diagnostics.

463
00:51:31,900 --> 00:51:40,049
So you need to know how to do them. So other diagnostics for the Cox model are martingale residuals that are helpful for checking

464
00:51:40,050 --> 00:51:45,120
the functional form of covariates and even giving you ideas of what you should look at there.

465
00:51:46,050 --> 00:51:55,500
Deviance residuals are useful to check for outliers and if parties are measuring influence of an observation on the parameter,

466
00:51:55,500 --> 00:51:58,920
estimate how much it moves those or other useful diagnostics.

467
00:51:59,700 --> 00:52:04,889
And this is a good breakpoint because I think I'm going to review dependent outcomes regression next.

468
00:52:04,890 --> 00:52:08,580
So let's take you weren't 510 minutes what do you want.

469
00:52:11,950 --> 00:52:17,830
5 minutes. Okay. So let's meet back up roughly 9 a.m.

470
00:52:30,390 --> 00:52:33,500
So you remember survival analysis now? Yes.

471
00:52:33,510 --> 00:57:43,239
It's been a while, right? Okay.

472
00:57:43,240 --> 00:57:48,860
Let's go ahead and get back to work. All right so dependent outcomes regression.

473
00:57:48,860 --> 00:57:55,489
So I've put I've put together. Is a bit of a flowchart to help us think about what we've learned.

474
00:57:55,490 --> 00:58:04,400
You've learned so much in this course that just sort of keeping track of what is going on with all these things you've learned,

475
00:58:05,240 --> 00:58:09,260
I think should be helpful. So this is a flowchart that takes three slides.

476
00:58:09,260 --> 00:58:13,370
So just know there's a lot of next slide. Next slide in this whole deal.

477
00:58:14,360 --> 00:58:22,700
All right. So first question on flowchart, can research question be answered by combining independent outcomes into a single independent outcome?

478
00:58:23,480 --> 00:58:30,049
That's always a valid question, right? Can you move it from a dependent outcome choice to a single outcome choice?

479
00:58:30,050 --> 00:58:33,170
This might be particularly helpful when you're analyzing counts.

480
00:58:33,710 --> 00:58:37,760
Do you have to have counts that are dependent in your analysis and answer the question,

481
00:58:38,150 --> 00:58:43,730
or can you combine the total counts and make it a single outcome?

482
00:58:46,260 --> 00:58:52,120
So if yes, that is, explore the analysis that are appropriate for that independent outcome if desired.

483
00:58:52,140 --> 00:58:57,060
You know, it's a it's always a possibility that you, you know, can try this.

484
00:58:57,060 --> 00:59:06,040
And sometimes it works really well. If there's not a way to do it, if there's not a sensible combined variable that captures your research question,

485
00:59:06,490 --> 00:59:09,100
then you need to kind of go to the next part of the flowchart and say,

486
00:59:09,100 --> 00:59:15,010
are outcomes either well balanced or following a compound symmetry correlation pattern?

487
00:59:15,440 --> 00:59:18,729
So what do I mean by well balanced? So what I'm saying is,

488
00:59:18,730 --> 00:59:26,320
are do you have like measures where the first outcome and the second outcome in

489
00:59:26,320 --> 00:59:31,180
the third outcome are all measured in a way that's comparable across individuals.

490
00:59:31,810 --> 00:59:37,210
So if they're measured at time points is everybody got to measure at times zero eight and 16.

491
00:59:37,630 --> 00:59:42,430
And so that you can group and look at columns of numbers and think about correlations,

492
00:59:42,850 --> 00:59:48,850
or are these just not really well balanced at all, not measured in the same way for various people in your data set?

493
00:59:50,240 --> 00:59:53,610
So if if they're well-balanced,

494
00:59:53,630 --> 00:59:59,780
like measuring the same way for the people in your data set or they're following a compound

495
00:59:59,780 --> 01:00:05,269
symmetry correlation pattern that also works because then it doesn't matter if they were measured,

496
01:00:05,270 --> 01:00:06,049
kind of different.

497
01:00:06,050 --> 01:00:13,100
If we think that any arbitrary pair of observations has the same correlation, we don't really care if they were measured or we're different times.

498
01:00:13,100 --> 01:00:17,059
We're just going to be using all of the data, any pair of data, get it?

499
01:00:17,060 --> 01:00:22,280
That correlation if you believe it's compound symmetry or exchangeable correlation.

500
01:00:23,950 --> 01:00:33,640
So if the answer is no that you don't have this, then the next question is, you know, is your outcome a continuous like approximately normal outcome?

501
01:00:34,770 --> 01:00:38,639
And if yes, then we've got an approach we can use.

502
01:00:38,640 --> 01:00:44,820
We can use the random effects model or the mixed effects model to look at dependance and outcomes.

503
01:00:45,240 --> 01:00:52,720
And we don't need the the measures to be really well balanced and similar for everybody because we describe variability by, you know,

504
01:00:52,740 --> 01:01:00,480
how much higher or lower to the population mean or is that person's data and maybe they have a different slope from the population means data,

505
01:01:00,720 --> 01:01:06,210
but we don't care how many times they were measured or if they made their visit appointment exactly at the same time as everybody else.

506
01:01:06,630 --> 01:01:14,550
We can use the mixed model to talk about variability related to trajectories of you know, compared to the population mean.

507
01:01:16,020 --> 01:01:27,780
Now, if the answer is no, you don't have APR or normal outcomes and they're not in this situation where we can talk about correlation matrices,

508
01:01:28,260 --> 01:01:35,100
then you're in the situation of, gosh, we didn't learn this in 523, you know, so there's more methods yet to be learned.

509
01:01:35,310 --> 01:01:42,990
There are other courses within the school where you can learn more about how to deal with dependent outcomes than we covered in this course.

510
01:01:43,830 --> 01:01:51,090
And so the thing to Google, if you're stuck and you don't have time to figure out a course like right this minute, you need it right now.

511
01:01:51,660 --> 01:01:55,229
The Google term is generalized, linear, mixed model.

512
01:01:55,230 --> 01:02:04,170
So it's like the extension of logistic regression or the extension of negative binomial regression, all of these other kind of outcomes.

513
01:02:04,500 --> 01:02:10,800
There are mixed models, generalized linear mixed models that can be done.

514
01:02:11,130 --> 01:02:14,550
We just we just didn't get to that in the course.

515
01:02:14,730 --> 01:02:22,140
So in another course, you would be able to spend more time on all of these ideas and add in a few extra new tools.

516
01:02:23,280 --> 01:02:27,299
But if the outcome is approximately normal, we're set mixed models.

517
01:02:27,300 --> 01:02:35,670
And I would have to say over my many years of performing analyzes, this is the one I use the most.

518
01:02:37,290 --> 01:02:44,070
I think recently for the first time I've had to use a generalized or mixed model for a particular weird dataset.

519
01:02:44,460 --> 01:02:48,270
But it's happened like one. And I'm an old person, so.

520
01:02:49,270 --> 01:02:57,290
Since 1990. I've used it once. So it's but for you, maybe you need to use it all the time.

521
01:02:57,500 --> 01:02:59,450
I don't know. It depends on what you work on.

522
01:03:03,400 --> 01:03:09,250
And okay, so if your outcomes are either well-balanced or following some compound symmetry correlation pattern,

523
01:03:09,790 --> 01:03:13,930
then and it's a yes, then we go to the next page for our flowchart.

524
01:03:16,800 --> 01:03:20,280
All right. So this is just keeping track of where we are in the flow chart.

525
01:03:20,760 --> 01:03:25,590
Two outcomes are either well balanced or following a compound symmetry correlation pattern.

526
01:03:26,430 --> 01:03:28,350
So then we're looking at the outcome type.

527
01:03:28,350 --> 01:03:35,220
So if you have a continuous outcome type that's approximately normal, you have a lot of choices now that you can use.

528
01:03:35,580 --> 01:03:39,960
And we've looked at nearly all of them. So you can use mixed models with random effects.

529
01:03:40,440 --> 01:03:48,600
You can use a G model where you specify that your outcomes are normally distributed and you even pick a specified correlation matrix,

530
01:03:48,840 --> 01:03:53,969
whether it's unstructured or ar1 or whatever.

531
01:03:53,970 --> 01:03:57,600
I mean, you can all of your measures, well balanced,

532
01:03:57,600 --> 01:04:03,990
can be put in columns and you can think about what the correlation between any two columns is and put some structure on that.

533
01:04:06,150 --> 01:04:11,910
We also once but not enough to have it, maybe have made an impression.

534
01:04:11,910 --> 01:04:18,569
We also used sass mixed with repeated statement in our first handout when we were looking at paired t test.

535
01:04:18,570 --> 01:04:25,470
That's another way you can do this. You can use proc mixed and a covariance matrix to model dependance and outcomes.

536
01:04:25,980 --> 01:04:37,860
It's, you know, a possibility that you could that you could have and both SAS and are are okay when you have these normally distributed outcomes.

537
01:04:37,860 --> 01:04:41,880
They they both both work pretty well.

538
01:04:46,200 --> 01:04:52,890
And so if your outcome type of account or account per person year or something along those lines,

539
01:04:53,250 --> 01:04:57,840
then you have to ask this next question of are excess zero counts an issue?

540
01:04:59,670 --> 01:05:09,810
And if no access to your accounts are not an issue, then you can use a SAS or r g model.

541
01:05:11,140 --> 01:05:21,160
Actually, I have a little bit of remark here about ah so ah is okay for dependent posts on outcomes that the sample size is large,

542
01:05:21,160 --> 01:05:26,110
but SAS is kind of the winner for this area for what you can do.

543
01:05:26,950 --> 01:05:33,040
So the SAS model with a specified correlation structure to model dependance and counts is available.

544
01:05:33,040 --> 01:05:36,220
You can use Poisson or negative binomial distribution.

545
01:05:36,550 --> 01:05:40,540
There's this weird thing that SAS does for you. I think I call it Big Brother or Big Sister.

546
01:05:40,540 --> 01:05:44,080
They kind of have this adjustment that they do.

547
01:05:44,170 --> 01:05:52,960
If your Poisson and distribution doesn't quite fit the variability of the data, they have some adjustment due to the scale factor.

548
01:05:53,410 --> 01:06:01,510
And so this is so in G, it almost doesn't make a difference, if you guess correctly, between Poisson and negative binomial Big Brother.

549
01:06:01,510 --> 01:06:05,230
Big Sister is fixing up your assumption for you behind the scenes.

550
01:06:05,620 --> 01:06:11,170
And so model building in this G world isn't that, you know,

551
01:06:11,440 --> 01:06:17,710
you can almost relax a little bit because no matter what you put it, they're going to try to fix it for you.

552
01:06:18,820 --> 01:06:23,980
So we're not in this situation where we're trying to, you know, compare models in real, careful ways,

553
01:06:24,190 --> 01:06:28,870
which is a good thing because G doesn't give you a lot of model building tool to make these decisions.

554
01:06:30,570 --> 01:06:44,770
Okay. So again, ah, I in general for g e r really is only showing you results for robust sandwich empirical estimates.

555
01:06:44,770 --> 01:06:48,730
They make it very difficult to find model based results.

556
01:06:48,940 --> 01:06:52,930
Sass will show you both and you can kind of see what's going on a little bit more clearly.

557
01:06:53,560 --> 01:06:59,500
But if you have a large sample size, then those results are always going to be good.

558
01:06:59,860 --> 01:07:04,540
Robust sandwich estimates are always going to be good and so are fine in those cases.

559
01:07:06,290 --> 01:07:09,350
But they don't even try for a negative binomial, you know.

560
01:07:11,570 --> 01:07:18,820
Now, if zero counts are an issue you've already ruled out combining it into a single outcome for independent analysis.

561
01:07:18,830 --> 01:07:24,080
So if you have dependent counts and excess zeros, then you're in this.

562
01:07:24,560 --> 01:07:28,190
SAS can't model this situation.

563
01:07:28,200 --> 01:07:31,729
Are you sure you can't combine dependent counts to a single outcome for analysis?

564
01:07:31,730 --> 01:07:40,220
So a zero way to count model can be used. And so there's not software available for this that I'm aware of at this time.

565
01:07:40,520 --> 01:07:48,349
And but there is a paper with some student graduate student who has their own Frankenstein code.

566
01:07:48,350 --> 01:07:55,310
Probably if you get desperate, you can email this, the author of a paper, I, I think I might have put that paper up on canvas.

567
01:07:55,790 --> 01:08:00,859
And if you're desperate, you can say, yeah, can I get your software and you can do it.

568
01:08:00,860 --> 01:08:09,380
But I think it's much easier if you're in this situation to try to think of some creative way to make the dependent count a single outcome

569
01:08:10,580 --> 01:08:17,299
and write about that if if it's at all possible to answer the research question with a single independent outcome that's combined,

570
01:08:17,300 --> 01:08:22,250
like combining counts across even a long period of time or doing counts per person.

571
01:08:22,250 --> 01:08:28,010
Year two to account for the fact that you have some people who are watched longer than others.

572
01:08:28,010 --> 01:08:35,330
Something some trick like that might be an advantage as opposed to finding code from a graduate student.

573
01:08:35,690 --> 01:08:38,690
Now, in five years, this could probably be in software.

574
01:08:39,920 --> 01:08:45,800
Because that's between five and 20 years or where things get into software packages, depending how interesting they are.

575
01:08:46,430 --> 01:08:49,670
So eventually this will be available probably in the code, but it's not now. So.

576
01:08:52,650 --> 01:08:56,550
Now if you have a Bernoulli outcome, see next page.

577
01:08:58,320 --> 01:09:03,290
All right. So I think this might be the last phase of the of the flow chart that we'll see.

578
01:09:03,300 --> 01:09:12,000
Maybe I have another next page. So if you have dependent Bernoulli or binomial outcomes, you want to first ask,

579
01:09:12,000 --> 01:09:16,560
are these either well balanced or following a compound symmetry, a correlation pattern?

580
01:09:17,100 --> 01:09:22,590
Right. So you can kind of figure out why I'm asking that now, because we've seen it for other stuff.

581
01:09:23,040 --> 01:09:27,599
And then you also need to say, is this a matched case control study predicting case status?

582
01:09:27,600 --> 01:09:38,240
Case control? If it is, yes, it is a case control study, then you need to use conditional logistic regression in both SAS and ah, do this pretty well.

583
01:09:40,260 --> 01:09:42,870
If it is not a match case control study,

584
01:09:43,170 --> 01:09:50,730
then you can use a GE model with a specified correlation structure to model DEPENDANCE and potentially outcomes in both SAS and r.

585
01:09:51,030 --> 01:09:59,630
R. Okay. All right, I catch it all here.

586
01:10:02,400 --> 01:10:04,440
Oh, I don't have my I don't.

587
01:10:04,830 --> 01:10:12,860
So there actually is if they're not well balanced and they don't follow a compound symmetric correlation pattern, I think I have another guy moment.

588
01:10:12,870 --> 01:10:15,410
We haven't learned that. So.

589
01:10:15,770 --> 01:10:24,229
So if you don't have a way to talk about a covariance matrix because these outcomes were measured at different time points or whatnot,

590
01:10:24,230 --> 01:10:30,530
then I think you can't use G, so you'll have to use that generalized linear mixed effects model that we didn't learn.

591
01:10:32,030 --> 01:10:37,110
So there's probably a. There's probably.

592
01:10:37,120 --> 01:10:40,150
Oh, wait, did we cover it earlier? I think we did.

593
01:10:42,720 --> 01:10:46,190
It did appear in the flow chart. It's if.

594
01:10:46,240 --> 01:10:49,480
No, it's not approximately normal. No. Yeah.

595
01:10:49,600 --> 01:11:01,180
So it was covered on this first page. There's a lot of times I come up with gods in my real life, like, how do I do that?

596
01:11:01,570 --> 01:11:04,120
Well, there's a few that now you're bumping into.

597
01:11:04,120 --> 01:11:09,580
Now that you're getting really good with statistics and you're bumping into the current edge of what you can do.

598
01:11:09,910 --> 01:11:22,150
You're going to have those. All right. So. Just a reminder of what these different correlation matrices look like and kind of what they assume.

599
01:11:23,440 --> 01:11:31,060
This is probably a little bit fresher in your mind. So the common correlation structures and I'm just showing them for three outcomes,

600
01:11:31,660 --> 01:11:35,020
uh, they extend over whether you have three or four or five outcomes.

601
01:11:35,020 --> 01:11:37,840
It's just the matrices get larger for these storage systems.

602
01:11:38,200 --> 01:11:48,640
So independence implies that all any two of the outcomes in these correlated settings are going to be uncorrelated.

603
01:11:48,670 --> 01:11:56,889
So the independent correlation matrix kind of looks like this where if you if it's the correlation with that measure in itself,

604
01:11:56,890 --> 01:12:00,220
that's kind of the diagonal here of ones.

605
01:12:00,550 --> 01:12:09,640
But otherwise, any pair of outcomes in a group that that is grouped in a cluster has zero correlation.

606
01:12:10,150 --> 01:12:21,460
And so that option is Indian SAS and are and unstructured is is kind of an extreme case where

607
01:12:21,790 --> 01:12:26,680
everything's correlated and you don't have any simplification of how that correlation looks.

608
01:12:27,370 --> 01:12:34,840
So unstructured implies that all pairs of outcomes from subject II or cluster I have potentially different correlation terms.

609
01:12:35,200 --> 01:12:40,320
And so that is an incessant R for that option when you're asking for that.

610
01:12:40,330 --> 01:12:47,080
So this is kind of like the correlation between the first and the second measure for that cluster,

611
01:12:47,080 --> 01:12:50,230
the correlation between the first and the third measure for that cluster.

612
01:12:50,440 --> 01:12:56,649
And so you can kind of see how you really need to have a good definition for what is the

613
01:12:56,650 --> 01:13:03,940
first measure and what is the second measure within a cluster to get at that correlation.

614
01:13:05,930 --> 01:13:14,750
So you need kind of. Either regularly measured things where people can be grouped into columns that time, one time, eight times 16.

615
01:13:15,470 --> 01:13:18,950
Or you have to have this situation where it doesn't matter.

616
01:13:19,460 --> 01:13:27,560
Anybody in the cluster has the same correlation and that is the compound cemetery assumption or the exchange.

617
01:13:27,650 --> 01:13:31,969
It's also called exchangeable, so it implies all pairs of outcomes from subject.

618
01:13:31,970 --> 01:13:42,080
I had the same correlation. And so this is kind of nice because even if you have weird measurement, weird measurements within an individual,

619
01:13:42,080 --> 01:13:49,250
if they're equally correlated, then, you know, they don't have to be done at the same time or in the same way.

620
01:13:49,250 --> 01:14:00,329
Exactly. If you believe the correlation to be the same. So more remarks on correlation structures and variance estimation.

621
01:14:00,330 --> 01:14:10,000
So for large samples, the takeaway message, I think from this time this part of the course was if you have a large sample,

622
01:14:10,000 --> 01:14:16,319
a large dataset, then you can go straight to the empirical, robust sandwich standard error estimates.

623
01:14:16,320 --> 01:14:25,830
So whatever that table is, however your package describes it, are it this is the go to stuff that they put in the are output.

624
01:14:25,830 --> 01:14:29,130
And in SAS you would look for this empirical kind of output.

625
01:14:29,940 --> 01:14:38,729
Those results are reasonable and it doesn't matter what correlation type you designated, this robust step is fixing it up for you.

626
01:14:38,730 --> 01:14:48,420
So even if you go and say, I think independence, correlation structure, the empirical, robust thing which estimate our results will say,

627
01:14:48,420 --> 01:14:55,110
well, that's nice, but I'm going to see I'm going to fix whatever the assumption is and your standard errors will be correct.

628
01:14:55,410 --> 01:15:01,350
So this is really a powerful thing that she does for you that is not available in other package.

629
01:15:01,380 --> 01:15:07,800
You know, other approaches like mixed model won't do that for you makes model will do what you tell it to do.

630
01:15:07,830 --> 01:15:12,360
But she says, no, I'm going to go ahead and fix this up for you.

631
01:15:14,290 --> 01:15:17,919
Okay. And if you're a large enough sample, you can do that without penalty.

632
01:15:17,920 --> 01:15:18,309
Really,

633
01:15:18,310 --> 01:15:29,620
it doesn't really cost much because there's plenty of data to estimate this thing and so it provides a lot of protection there for smaller samples.

634
01:15:29,620 --> 01:15:33,819
You don't quite get that same protection because the empirical standards are more

635
01:15:33,820 --> 01:15:40,510
variable than a model that correctly specifies the correlation matrix structure.

636
01:15:40,900 --> 01:15:48,790
And we saw this firsthand when we looked at the pancreatic enzyme study data that only had six people in it.

637
01:15:49,960 --> 01:15:53,320
It was very early in in the course,

638
01:15:53,320 --> 01:16:00,010
maybe the second hand or independent outcomes where we're looking at crossover trials and with there were only six people in the dataset.

639
01:16:00,010 --> 01:16:12,219
And if we attempted to ask for an unstructured covariance matrix, both SAS and ah just went Nope and gave you really weird results.

640
01:16:12,220 --> 01:16:15,940
It just couldn't estimate that many parameters based on six people's measures.

641
01:16:16,890 --> 01:16:21,209
And so for smaller sample sizes coming, you know,

642
01:16:21,210 --> 01:16:32,100
assuming a similar correlation structure like exchangeable or even an R one, it uses a small number of parameters.

643
01:16:32,370 --> 01:16:39,659
You need to kind of think about that model a bit more and try to get it right, because the empirical standard errors aren't great.

644
01:16:39,660 --> 01:16:43,890
When you have small samples, it needs that extra sample size to get those done well.

645
01:16:47,250 --> 01:16:49,409
Even if you're in the small, stable situation,

646
01:16:49,410 --> 01:16:58,020
it's kind of useful to still look at the empirical standard errors because you can rule out poor choices for the correlation structure that way.

647
01:16:58,050 --> 01:17:01,740
So if you look at the empirical standard errors and the model base standard

648
01:17:01,740 --> 01:17:06,390
errors that at least it looks like they're not terribly far from one another,

649
01:17:06,870 --> 01:17:09,269
it can rule out really poor choices.

650
01:17:09,270 --> 01:17:16,020
And that's actually how we figured out in SAS that unstructured wasn't possible because the standard errors between

651
01:17:16,020 --> 01:17:23,100
the empirical and the model base were so far off that there was no way unstructured was doing the right thing.

652
01:17:27,080 --> 01:17:37,670
So if you if you can model the correlation and you do it well, it can increase the estimation efficiency for your parameters.

653
01:17:37,670 --> 01:17:41,149
This might give you a little extra power. If you really can do that.

654
01:17:41,150 --> 01:17:45,590
Well, you don't lose much power if you have a large sample size.

655
01:17:45,590 --> 01:17:53,030
It could do this empirical, but you can gain a little if you have a good handle on what the the model base correlation should be,

656
01:17:53,030 --> 01:18:01,150
if you think it's really exchangeable or whatnot. Gee doesn't give you a lot of tools for choosing between these things.

657
01:18:01,750 --> 01:18:07,750
They have this thing called QIC. And it's is it's not as good as.

658
01:18:07,990 --> 01:18:11,920
I see I see is so nice for all these models where we have it.

659
01:18:12,220 --> 01:18:17,650
QIC was meant to be like the HD for g, but it's really not great.

660
01:18:17,650 --> 01:18:21,070
It's not great at all. And so you can kind of look at it.

661
01:18:21,070 --> 01:18:23,740
It's the only game in town really to help you.

662
01:18:23,740 --> 01:18:30,459
But it's it's like asking that person for directions on the street and they might not really know where you're supposed to go and just point any way.

663
01:18:30,460 --> 01:18:38,330
That's. Q I see. In my opinion, Sterling is the only hope there is, but it's not great.

664
01:18:42,400 --> 01:18:46,540
And currency values are not comparable between different distribution options.

665
01:18:46,550 --> 01:18:49,150
So it can't advise you on the best distribution,

666
01:18:49,160 --> 01:18:54,970
so you can't get advice for whether you should be doing this on versus negative binomial, for example.

667
01:18:55,450 --> 01:19:02,640
So often when I'm in this situation, if I had to say choose really imposed on a negative binomial and I had to use g.

668
01:19:02,920 --> 01:19:12,340
I would just go ahead and use negative binomial because it's the more conservative case that models the variability with a little extra parameter and.

669
01:19:13,660 --> 01:19:18,170
Actually, honestly, big sister and brother fixing up that problem for you anyway, right.

670
01:19:18,190 --> 01:19:26,709
Because if you pick Poisson, it's using the scale parameter to fix your variability up so you don't have a good model building choice.

671
01:19:26,710 --> 01:19:31,090
But Big Brother, Big Sister, that's scale parameter being used in the background to keep you from,

672
01:19:31,090 --> 01:19:36,310
you know, protect you against yourself for making a weird call.

673
01:19:39,090 --> 01:19:43,649
Okay. So interpretation of parameters from dependent outcomes regression in bio 23.

674
01:19:43,650 --> 01:19:48,330
So parameter interpretations from generalized estimating equations and mixed models

675
01:19:48,330 --> 01:19:52,050
follow the same conventions as the corresponding independent outcome models.

676
01:19:52,410 --> 01:19:55,560
So once you get your parameter tables,

677
01:19:56,610 --> 01:20:05,909
it's not too bad you you've already learned how to estimate parameter estimate tables for these various things in the Independent's case,

678
01:20:05,910 --> 01:20:11,910
so it pretty much carries through. So for normally distributed outcomes in G or mixed models,

679
01:20:12,630 --> 01:20:20,550
the the parameter to J is the increase in the mean outcome for one unit increase in the JTH predictor adjusted for other variables in the model.

680
01:20:21,360 --> 01:20:25,940
And if you're looking at Poisson or negative binomial outcomes and you're doing G,

681
01:20:26,430 --> 01:20:30,959
then you either the beta j is the multiplicative effect on the mean count.

682
01:20:30,960 --> 01:20:35,340
If there's not an offset term in your model or the rate of the outcome,

683
01:20:35,340 --> 01:20:42,000
if there is an offset term in your model associated with the one unit increase in the predictor adjusted for other model variables.

684
01:20:42,420 --> 01:20:48,090
So this part carries forward as long as you know how to do the correlation structure part correctly.

685
01:20:48,540 --> 01:20:53,910
Interpreting your results is going to feel like you're back in the independence case for the most part,

686
01:20:55,410 --> 01:20:59,069
when you're doing G for Binomial or Bernoulli outcomes,

687
01:20:59,070 --> 01:21:03,900
it's just like doing an interpretation for logistic regression once you get the right P values.

688
01:21:04,320 --> 01:21:08,219
So Edith badges the odds ratio for having the outcome based on a one year increase

689
01:21:08,220 --> 01:21:12,030
in the JTH predictor versus no increase adjusted for other variables in the model.

690
01:21:13,210 --> 01:21:15,610
That's why you were able to learn these models so quickly,

691
01:21:16,030 --> 01:21:22,690
because you were just attacking the variability part of it and getting the P values to be correct when there's dependance in the outcomes.

692
01:21:24,110 --> 01:21:27,200
All right. So now that was that was the review.

693
01:21:27,500 --> 01:21:34,639
So now we're going to play this game called Which Model. And I think I hid the answers from you, but I'm not positive I did that well.

694
01:21:34,640 --> 01:21:40,600
So. But you try to play along and not peek ahead if you can.

695
01:21:40,670 --> 01:21:49,430
So this is the skill that I want to be a take away, that if you're put into a situation and give it a dataset, you know which model to use.

696
01:21:50,440 --> 01:21:56,230
All right. So for the following settings, please indicate which analysis would be appropriate from the following choices.

697
01:21:57,540 --> 01:21:59,639
And so this is going to be for the next few slides.

698
01:21:59,640 --> 01:22:05,850
So one of the choices is a generalized estimating equation model with an appropriate correlation specified for the outcomes.

699
01:22:05,850 --> 01:22:09,780
And please specify the distribution, whether it's normal binomial and so on.

700
01:22:10,890 --> 01:22:14,100
A random effects model is another one of the choices that we've learned.

701
01:22:15,520 --> 01:22:20,110
And repeated measures model using SAS product mix with the repeated statement.

702
01:22:20,110 --> 01:22:23,709
We saw that once with paired T tests. That's the only time we ever saw that.

703
01:22:23,710 --> 01:22:31,930
But it is an option. A conditional logistic regression model specifying the correlation being a straight a statement account

704
01:22:31,930 --> 01:22:38,080
model for independent rates counts poisson negative binomial with or without zero inflation component.

705
01:22:38,710 --> 01:22:41,470
A logistic regression analysis for independent outcomes.

706
01:22:41,530 --> 01:22:46,270
Even though that was really curved on the first exam, it's still a choice for which model, right?

707
01:22:46,960 --> 01:22:49,990
Or GA. We haven't learned that. That is also a choice.

708
01:22:50,770 --> 01:22:57,150
All right. And please only say that if I didn't teach it because I'll feel really bad if.

709
01:22:57,160 --> 01:23:01,200
If I taught it and he didn't learn it. So please be kind.

710
01:23:01,210 --> 01:23:04,300
Okay. And some settings. More than one choices.

711
01:23:04,300 --> 01:23:08,800
Correct. So these are all going to be the possible answers that you can chime in with.

712
01:23:08,830 --> 01:23:11,950
All right. So here's our first which model?

713
01:23:12,760 --> 01:23:16,120
So in an obesity study like this, I feel like Jeopardy!

714
01:23:16,120 --> 01:23:24,880
Right now in an obesity study, weight is measured for each patient at weeks zero eight, 16, 24 and 32.

715
01:23:25,570 --> 01:23:31,500
You want to model weight trajectories over time.

716
01:23:31,510 --> 01:23:36,640
That is what you and your collaborators want to do. You want to describe weight trajectories over time.

717
01:23:38,000 --> 01:23:42,980
All right, so which model? There might be more than one correct answer here.

718
01:23:45,600 --> 01:23:52,170
So how do you go about this? Yes, go. Yes.

719
01:23:52,520 --> 01:23:57,510
Yeah. Repeated measures model would be one way to go. What is your assumed distribution for the outcome?

720
01:24:02,360 --> 01:24:14,690
Anybody can help. Normally by meal with all those negative you negative binomial.

721
01:24:16,240 --> 01:24:20,190
That those are the only ones we've learned. Right. So. Yes.

722
01:24:21,760 --> 01:24:22,050
Yeah.

723
01:24:22,060 --> 01:24:30,130
I would have used normally distributed outcomes, although I have had a student in the past faced with this same question say isn't weight account?

724
01:24:30,520 --> 01:24:33,729
Like I'm adding the pounds over time and I'm like, rats.

725
01:24:33,730 --> 01:24:43,570
You really probably could do that. I mean, I don't think of weight as account outcome, but I would I can't say that's a wrong choice.

726
01:24:44,320 --> 01:24:47,650
But yeah, I usually treat weight as a continuous, normally distributed outcome.

727
01:24:49,300 --> 01:24:55,000
Okay. And so. So the repeated measures with the testicles normal.

728
01:24:55,360 --> 01:24:59,840
Yeah, that can be done. And so this is kind of the answer.

729
01:24:59,860 --> 01:25:06,940
So one of them you could do a model for normally distribute outcomes with the specification

730
01:25:07,030 --> 01:25:12,280
specification of some correlation matrix for the weight measures over time in person I.

731
01:25:12,850 --> 01:25:17,680
So that would be one way to do it. Or you could do a random effects model.

732
01:25:17,850 --> 01:25:23,340
And this requires thought about whether a random intercept or random slope or other random effects are helpful,

733
01:25:23,350 --> 01:25:26,730
accounting for the dependance between weights measured over time in the same individual.

734
01:25:26,740 --> 01:25:31,260
So that's a choice or and this is what you had said.

735
01:25:31,270 --> 01:25:37,569
I repeated measures model versus product mix with a repeated statement and that requires

736
01:25:37,570 --> 01:25:41,160
the specification of a covariance matrix for the weight measures over time in person.

737
01:25:41,320 --> 01:25:48,790
So that's this is like almost like g except it's really leaning more on the normally distributed assumption than the G model does.

738
01:25:49,920 --> 01:25:56,640
And in each case, the model will need to include time as a predictor if weight significantly changes over time.

739
01:25:57,390 --> 01:26:05,640
All right. And so you did well, you you identified an appropriate analysis, the 18.

740
01:26:07,270 --> 01:26:11,680
And so remember you should consider that are spline terms for the time variable

741
01:26:11,770 --> 01:26:16,299
to see if that weight trajectory changes over time and not just a straight line.

742
01:26:16,300 --> 01:26:20,290
Maybe it might be put into blind terms allowing for bends over time,

743
01:26:20,770 --> 01:26:26,829
and you might want to consider threshold terms and choosing how time best predicts weight profiles over time.

744
01:26:26,830 --> 01:26:31,000
Because, you know, maybe at a certain point in time people start changing for weight.

745
01:26:31,000 --> 01:26:39,610
I mean, that's always optimistic, right? In my case, it's still going like this, but it could be flattened out over time at some time threshold.

746
01:26:41,200 --> 01:26:48,979
All right. Which model? This is our next one. And I stole this one from my favorite textbook.

747
01:26:48,980 --> 01:26:54,469
For this kind of dependent outcomes, material fits more layered and where I really like that book.

748
01:26:54,470 --> 01:26:57,800
If you need to read more about this topic and you don't want to take in a whole other course.

749
01:26:58,580 --> 01:27:04,410
So here's which model led exposed children. Were randomized to.

750
01:27:05,970 --> 01:27:11,790
I want to say some grammar therapy, but I don't know. I think it might be a suit.

751
01:27:11,790 --> 01:27:15,150
It might be a ten like suit, Jim, or I don't know. Anyway, forgive me.

752
01:27:16,530 --> 01:27:22,770
So they randomized to some therapy like treatment one, treatment zero and weeks one,

753
01:27:22,860 --> 01:27:30,360
four and six blood lead levels were checked to see if they fell below 21.

754
01:27:30,360 --> 01:27:40,280
Is that micrograms per deciliter? And so that was recorded in this variable low let equals one versus low that equals zero.

755
01:27:40,880 --> 01:27:43,040
And this is what the first child's data looks like.

756
01:27:43,040 --> 01:27:55,700
So ideally same child that the time they were on the placebo and at week one they had low light equals zero weak for they had low let equals zero.

757
01:27:55,760 --> 01:27:59,150
We had low. That equals zero. So.

758
01:28:00,960 --> 01:28:12,010
Which model? You want to understand if this treatment helped them in terms of avoiding.

759
01:28:13,380 --> 01:28:18,870
Led problems, blood lead level problems with this low blood variable.

760
01:28:19,320 --> 01:28:24,800
Yes. Z is a very good answer.

761
01:28:24,810 --> 01:28:34,130
So we have a yes no outcome that ruled out all the normally distributed methods and we had dependent outcomes.

762
01:28:34,140 --> 01:28:44,940
So we can't just use logistic. So the only method we've learned is G e and we can do that because all of the kids are measured at similar time points.

763
01:28:44,940 --> 01:28:47,280
So we can put in a column of data for week one,

764
01:28:47,550 --> 01:28:54,750
column of data for week for column of data for week six and actually sensibly talk about correlations between measures at those time points.

765
01:28:54,990 --> 01:29:01,840
G lets us do that. And so the distribution option that we choose is what?

766
01:29:05,710 --> 01:29:09,140
Because. Yeah. Bernoulli or binomial?

767
01:29:09,160 --> 01:29:15,720
Yep. Yes. And why did you not say conditional logistic regression?

768
01:29:16,170 --> 01:29:19,270
You were correct, by the way. But the other choice.

769
01:29:19,290 --> 01:29:24,510
Yes. It is not a case control study.

770
01:29:24,960 --> 01:29:29,470
In case control studies we would just like all the outcomes would be the same.

771
01:29:29,490 --> 01:29:36,480
It would be like 101010, you know, and it would be the covariates that are really different between people.

772
01:29:36,720 --> 01:29:41,250
So this was not a matched control case control study.

773
01:29:41,520 --> 01:29:45,090
Any child could have had any combination of these outcomes over time.

774
01:29:45,120 --> 01:29:50,190
They weren't pre-specified outcomes based on the design. JE Correct answer.

775
01:29:50,490 --> 01:29:58,950
Very well done. Yay, team. Okay. So and here it is g model for dependent binary outcomes is appropriate for

776
01:29:58,950 --> 01:30:04,050
modeling the odds of low led by treatment week and a treatment week interaction,

777
01:30:04,590 --> 01:30:14,700
whatever the data indicates. It requires additional model exploration for an appropriate correlation structure, particularly if the dataset is small.

778
01:30:15,300 --> 01:30:19,140
If the data sets large, then empirical standard error tables are reasonable choice.

779
01:30:19,500 --> 01:30:23,690
Instead of using an assumed correlation structure. All right.

780
01:30:24,140 --> 01:30:29,690
And we should be explored as a categorical or a class variable in addition to an ordinal variable,

781
01:30:29,690 --> 01:30:39,320
to see if you know that we if those outcomes kind of don't have a linear trend by week or kind of bounce around.

782
01:30:40,310 --> 01:30:47,120
So a week is another cover. You have to find the functional form for, you know, categorical ordinal.

783
01:30:51,190 --> 01:30:54,940
All right, here's another one. Do you like these? Which model? I think they're fun for me anyway.

784
01:30:55,540 --> 01:30:59,440
All right. Which model? In a crossover study.

785
01:31:00,040 --> 01:31:05,470
Time to pain relief for migraine sufferers is of interest.

786
01:31:06,220 --> 01:31:10,990
Outcomes or time to pain relief in the to treatment periods.

787
01:31:10,990 --> 01:31:15,340
And there's no censoring of outcomes. So let's take all censored models off the table here.

788
01:31:16,240 --> 01:31:21,690
Okay. Predictors are treatment group the period.

789
01:31:22,830 --> 01:31:29,460
And the randomization sequence of, you know, whether they got the treatment in period one, period two.

790
01:31:30,900 --> 01:31:36,840
We also want to test for treatment interactions with period and randomization sequence.

791
01:31:38,270 --> 01:31:50,130
So which model? So think about the flowchart here.

792
01:31:50,160 --> 01:31:53,280
So the first thing is figuring out what's the distribution of the outcome.

793
01:31:53,290 --> 01:32:00,370
So the outcome is tied to pain relief. So what distribution you want to be thinking about for that one?

794
01:32:03,730 --> 01:32:11,170
And our only choices we've learned our normal binomial slash Bernoulli puts on zero inflated.

795
01:32:11,440 --> 01:32:17,080
I mean, we haven't learned those. That's it. You have to choose. Those are our choices of what tools we know.

796
01:32:18,060 --> 01:32:23,959
Time to pain relief. What do you think? Normally distribute.

797
01:32:23,960 --> 01:32:27,230
Yeah, that's that's the one that is probably the most reasonable.

798
01:32:27,350 --> 01:32:33,440
It's definitely not a yes no outcome. Right. So it's not really binomial and it's not really a cow.

799
01:32:34,560 --> 01:32:38,060
So it's not going to be like Poisson negative binomial.

800
01:32:38,550 --> 01:32:44,970
So it's probably going to be this continuous type of pain relief, normally distributed kind of looking outcome.

801
01:32:46,100 --> 01:32:49,370
All right, so that's the outcomes.

802
01:32:49,820 --> 01:32:54,320
What model? Which model? You've already got a lot.

803
01:32:54,860 --> 01:32:59,570
You've got a lot through this flowchart already. Just identifying normally distributed outcomes.

804
01:33:00,350 --> 01:33:19,850
So which model might be more than one correct answer? Let's go to another one of the flowchart questions.

805
01:33:20,120 --> 01:33:27,840
Is this balanced? Is this well balanced? So you can think about a correlation matrices and that sort of thing.

806
01:33:29,860 --> 01:33:47,100
Yes or no? She is a very reasonable guess because you can talk about the correlation between time, pain relief and period one and period two.

807
01:33:47,460 --> 01:33:50,850
Everybody gets a period one measurement. Everybody gets period to measurement.

808
01:33:51,150 --> 01:33:54,510
You can talk about the correlation between those.

809
01:33:54,870 --> 01:33:58,950
And so a method that uses a correlation or covariance matrix is okay.

810
01:33:59,370 --> 01:34:06,270
So you can use G. You can also use just about everything else we've learned for knowledge about outcomes.

811
01:34:06,720 --> 01:34:11,620
So but your first guess was my first guess as well.

812
01:34:11,640 --> 01:34:15,690
Right. So a generalized estimating equation for dependent normally disparate outcomes.

813
01:34:16,440 --> 01:34:22,250
Compound symmetry isn't like the correlation structure, though only one correlation term requires estimation.

814
01:34:22,260 --> 01:34:28,049
So I mean, as long as you use any correlation structure, they're probably just going to give you that one correlation.

815
01:34:28,050 --> 01:34:38,100
It doesn't matter. Or you could do a random index model and you could use the random intercept term to describe the

816
01:34:38,100 --> 01:34:43,110
variability that's likely sufficient to capture the correlation of outcomes from the same individual.

817
01:34:43,500 --> 01:34:48,900
And I think that we actually did some version that was kind of like this with paired T tests.

818
01:34:49,440 --> 01:34:58,680
And this random effects model with a random intercept is going to be very similar to a G model that assumes compound symmetry.

819
01:34:59,930 --> 01:35:07,090
Because they're both sort of saying those whatever connect the two dots between the lines and the

820
01:35:07,310 --> 01:35:13,970
random intercept is saying that each person is parallel to the population mind when you say that.

821
01:35:14,360 --> 01:35:16,280
And that turns out to be compound symmetry.

822
01:35:16,520 --> 01:35:22,040
So there was a kind of a note we had in our very first handout where we looked at various ways to look at parity tests.

823
01:35:23,800 --> 01:35:31,300
You can also do a repeated measures model via SAS product mix with repeated statement, comments, symmetry, a lot like likely covariance structure.

824
01:35:31,540 --> 01:35:37,300
So again, because we can talk about a covariance matrix in a sensible way,

825
01:35:37,840 --> 01:35:44,110
we can use this repeated statement within product mix as well and sort of doing random intercept, random stuff like that.

826
01:35:45,910 --> 01:35:49,000
So the resulting analysis will be essentially the same,

827
01:35:49,000 --> 01:35:54,250
since a random intercept of mixed model assumes the same correlation structure as a compound symmetry.

828
01:35:54,250 --> 01:35:58,450
Correlation structure, which is something we talked about in Handout 15 way back when.

829
01:35:59,950 --> 01:36:03,220
Seems like a long time ago. All right. You ready for the next one?

830
01:36:04,540 --> 01:36:08,290
Which model? In a retrospective study of sudden cardiac death,

831
01:36:08,290 --> 01:36:16,120
survivors case control pairs of siblings are studied to assess whether the presence of previously implicated genes

832
01:36:16,450 --> 01:36:23,710
linked to inherited arrhythmic diseases are associated with sudden cardiac death after adjusting for other factors.

833
01:36:24,930 --> 01:36:37,160
So what's the outcome here? The cardiac.

834
01:36:37,710 --> 01:36:41,310
Yes. Sudden cardiac death. It's sudden.

835
01:36:41,310 --> 01:36:45,719
So, I mean, we don't right away we don't have censoring, we don't have time to event.

836
01:36:45,720 --> 01:36:51,060
It's just. Yes, no. Sudden cardiac death. So distribution of that outcome is.

837
01:36:54,810 --> 01:36:58,080
It's like a Bernoulli slash binomial. Yes. Okay.

838
01:36:58,530 --> 01:37:02,280
So that reduces the number of models that you should be considering. So which model?

839
01:37:02,280 --> 01:37:05,970
Now for this scenario? You've got two choices.

840
01:37:06,360 --> 01:37:10,530
One of them is G e, one of them is conditional logistic regression.

841
01:37:10,530 --> 01:37:13,530
Although I suppose we haven't learned that is an option always.

842
01:37:13,530 --> 01:37:17,670
But I'm going to just give you a hint. We learned this one so you don't hurt my feelings.

843
01:37:21,050 --> 01:37:24,830
Yes. This is.

844
01:37:24,830 --> 01:37:30,230
Yes, correct. This is conditional logistic regression because the outcomes are all case control pairs.

845
01:37:31,010 --> 01:37:34,340
And so one is always a case outcome will be a one.

846
01:37:34,850 --> 01:37:37,700
One is always control. Outcome is zero.

847
01:37:37,970 --> 01:37:46,960
And when you have that structure of every single payer has one one and one zero and it looks the same, then it's really hard to do a g.

848
01:37:47,030 --> 01:37:53,410
We kind of looked at that when we were learning which to use and G doesn't know how to handle that situation.

849
01:37:53,420 --> 01:38:00,320
She really needs to have the possibility that you have 202110 and either direction, I mean,

850
01:38:00,530 --> 01:38:05,630
it needs to have variable in the outcomes to even talk about correlation structures, you know.

851
01:38:06,470 --> 01:38:12,620
So yes, conditional logistic regression is the most appropriate for modeling the odds ratio of while accounting for

852
01:38:12,620 --> 01:38:17,830
the paired nature of the case control study design be a straighter for the matched case control pairs.

853
01:38:17,840 --> 01:38:22,219
Oh, and I have like a little link to the article that inspired that.

854
01:38:22,220 --> 01:38:25,220
Which model question. If you're interested.

855
01:38:25,790 --> 01:38:26,870
All right. Which model?

856
01:38:28,610 --> 01:38:40,040
So cystic fibrosis patients are randomized to treatment and yearly exacerbation counts are collected on each patient over a four year period.

857
01:38:41,400 --> 01:38:45,780
There are many, many patients who experience one or more years without exacerbations.

858
01:38:47,190 --> 01:38:53,250
The goal is to estimate the treatment effect, adjusting for other predictors measured at baseline.

859
01:38:54,950 --> 01:39:07,230
So what is the outcome here? The exacerbation counts.

860
01:39:07,410 --> 01:39:10,460
That's the outcome that's being modeled. Okay.

861
01:39:10,470 --> 01:39:14,760
So what distributions are on the table for counts?

862
01:39:18,340 --> 01:39:24,330
Porcine negative binomial. Zero inflated versions of these.

863
01:39:25,050 --> 01:39:32,040
Okay. So the goal is to estimate the treatment effect, adjusting for other predictors measured at baseline.

864
01:39:33,840 --> 01:39:45,549
Which model? Yearly exacerbation counts over a four year period.

865
01:39:45,550 --> 01:39:51,450
So. You have four counts.

866
01:39:51,750 --> 01:39:55,980
One per year. So you've got dependent counts.

867
01:39:58,460 --> 01:40:04,090
That's one feature. And here's another feature.

868
01:40:04,510 --> 01:40:08,290
There are many patients who experience one or more years without exacerbation.

869
01:40:09,470 --> 01:40:14,110
So we have a zero inflation problem. Yes.

870
01:40:16,560 --> 01:40:19,850
This is the guy. We haven't learned how to do this. Yes.

871
01:40:19,860 --> 01:40:25,140
And actually, you didn't hurt my feelings cause I didn't teach you that. Yay! Yeah, we don't have a way to do this.

872
01:40:26,310 --> 01:40:31,860
So there are no g models that can handle dependent zero of weight accountants and software packages.

873
01:40:31,860 --> 01:40:39,930
Yet, and fortunately, count models are either independent outcomes or dependent outcomes with no excess zeros.

874
01:40:40,380 --> 01:40:45,959
If zeros are an issue in the data analysis in the save is choices to sum over the number

875
01:40:45,960 --> 01:40:49,860
of exacerbations across the four years of follow up to create a single count outcome.

876
01:40:50,220 --> 01:40:59,910
So instead of four yearly counts have one for year count and analyze that that way that avoids this issue.

877
01:41:00,480 --> 01:41:07,440
But as I wrote, the problem there was yeah, we haven't learned how to do this was the correct response.

878
01:41:09,240 --> 01:41:21,020
All right. I think this one is going to be familiar seeming which model study collects information on C4 accounts over time.

879
01:41:21,530 --> 01:41:30,110
But despite the plan to collect data eight intervals for each patient, data is measured at irregularly spaced intervals with missing visits as well.

880
01:41:30,530 --> 01:41:34,520
It's anticipated that measures taken further apart will be less correlated.

881
01:41:34,850 --> 01:41:38,540
The measures taken close together in this data, the outcome,

882
01:41:38,540 --> 01:41:44,420
as long as the four plus one predictors are weak of measure and treatment group triple equals one or zero.

883
01:41:44,660 --> 01:41:54,240
Which model? You should know at least one appropriate model for this dataset, because I have not.

884
01:41:55,830 --> 01:41:59,460
Done anything tricky to change the data from what you've seen.

885
01:41:59,730 --> 01:42:05,680
So which model? What's the outcome here?

886
01:42:08,920 --> 01:42:17,200
The CD four counts over time. And can we assume a distribution for these log CD4 counts over time?

887
01:42:20,060 --> 01:42:25,130
So we've been assuming normally distributed outcomes for the log of these.

888
01:42:26,650 --> 01:42:32,140
When you take the log, it's not really account anymore. It's like some number that's not an integer necessarily.

889
01:42:32,920 --> 01:42:38,950
So we've been using normally disturbed outcomes. So which model for normal distribute outcomes would work for this?

890
01:42:47,240 --> 01:42:52,190
Homework. Six people. Are you just not telling me? What did you do in homework?

891
01:42:52,190 --> 01:42:59,280
Six for this. Oh, you guys, did you skip homework?

892
01:42:59,280 --> 01:43:03,450
Six. Is that the one you want to dropped for the cause?

893
01:43:04,200 --> 01:43:07,320
Oh. So this is going to.

894
01:43:07,590 --> 01:43:10,980
This is going to mean you have to throw this extra hard. So this we.

895
01:43:11,100 --> 01:43:17,100
You're used to the mix model for this. I was afraid people wouldn't do the last homework with that option.

896
01:43:17,700 --> 01:43:23,850
So this might hurt. You're going to have to study this extra hard. So the mixed model is is the one.

897
01:43:24,270 --> 01:43:30,690
And we would need to use random effects to model bullpens and city for counts over time.

898
01:43:31,170 --> 01:43:35,309
Mainly because people were measured at regular intervals where you could put

899
01:43:35,310 --> 01:43:39,750
columns that made sense and look at correlation between columns of these numbers.

900
01:43:39,990 --> 01:43:46,320
People came in at all kinds of wacky times. And so putting them into columns that were similar for reading the dataset just wasn't possible.

901
01:43:46,740 --> 01:43:51,630
So the only model is the mixed model that we learned how to do this.

902
01:43:52,440 --> 01:43:55,820
The random intercept is not sufficient to model the variable in the counts.

903
01:43:57,100 --> 01:44:05,260
If compound symmetry can't be assumed and the statement of the problem did say that measures further apart would be less correlated.

904
01:44:05,290 --> 01:44:12,640
So that rules out the compound symmetry or exchangeable correlation structure.

905
01:44:13,120 --> 01:44:17,319
So you should try a random intercept and slope model with additional random effect spline terms

906
01:44:17,320 --> 01:44:22,990
explored as needed and options requiring specification of the correlation or covariance matrix for the

907
01:44:22,990 --> 01:44:28,780
outcomes is not possible since compound symmetry again it was ruled out from the problem statement and

908
01:44:28,780 --> 01:44:33,520
the outcomes cannot otherwise be put into groups measured at similar times with similar covariance as.

909
01:44:34,920 --> 01:44:38,850
And so it's probably a good thing.

910
01:44:38,850 --> 01:44:46,110
I have a few more example questions on this kind of homework six data set, because you're going to need to practice this.

911
01:44:46,110 --> 01:44:51,780
Okay. So we've got outcomes logs three, four, which is actually the log of the four count plus one.

912
01:44:52,980 --> 01:44:56,670
Mainly that one is so that you don't hit the lock of zero, which is undefined.

913
01:44:56,670 --> 01:44:59,280
You know, the predictors are just triple and weak.

914
01:44:59,280 --> 01:45:06,000
So triple is one of the we were taking a triple therapy zero otherwise and we corresponds to the timing of the outcome measures.

915
01:45:06,390 --> 01:45:10,799
So the data for the first person looks kind of like this and I tend to tell you

916
01:45:10,800 --> 01:45:15,570
kind of what the data for the first person looks like in these output files,

917
01:45:15,570 --> 01:45:22,200
just because it helps you figure out what's going on. So for the first person they were on triple while.

918
01:45:22,200 --> 01:45:30,150
This is like crooked. Sorry about that. And the here are the weeks that they came in and had their logs three four count measured.

919
01:45:30,600 --> 01:45:35,879
All right. So that's what it looks like. And so consider the following code and output.

920
01:45:35,880 --> 01:45:45,120
And there's what it looks like for SAS, what it looks like for R, you know, what's the estimated population mean model for the triple therapy group.

921
01:45:47,050 --> 01:45:55,630
So how do you. How do you write out the population model for the triple therapy group?

922
01:46:03,900 --> 01:46:08,160
So is the intercept in the model. Yes.

923
01:46:09,280 --> 01:46:13,480
Is weak in the model. Yes.

924
01:46:14,550 --> 01:46:20,060
So we had this 2.9928 minus this number over here times week.

925
01:46:20,970 --> 01:46:24,480
What do we do for this one is triple in this model.

926
01:46:25,720 --> 01:46:32,620
Okay. How do we put it in? It's the parameter estimate.

927
01:46:32,620 --> 01:46:36,490
And for the triple equals one group, it's like you're multiplying at times a one.

928
01:46:37,660 --> 01:46:45,690
Right. What about the interaction term? Weak times, and then you plug in one four triple times this parameter estimate.

929
01:46:46,350 --> 01:46:50,550
So it's going to look like this. You know,

930
01:46:50,940 --> 01:46:56,309
so you're you're plugging in one every where you see triple here and it ends up

931
01:46:56,310 --> 01:47:04,379
looking like this and you can group terms according to whether we quizzed it or not,

932
01:47:04,380 --> 01:47:16,860
so that you can simplify it to look like this one overall intercept that comes from this intercept term in the triple term and one week.

933
01:47:18,210 --> 01:47:28,830
Term where you've got the everything that involves we kind of those parameter estimates are combined to get this -0.0005 times a week.

934
01:47:29,590 --> 01:47:29,960
Okay.

935
01:47:30,270 --> 01:47:40,200
So this is like one line for the triple equals zero group over time where, you know, they're kind of decreasing very slowly in this triple therapy.

936
01:47:40,680 --> 01:47:42,960
You know, according to this parameter over here.

937
01:47:45,300 --> 01:47:52,830
R and R are the numbers wiggle a little bit because the algorithms or the rounding or I don't know something's making them slightly different here,

938
01:47:53,130 --> 01:47:59,790
but similar. Okay.

939
01:47:59,820 --> 01:48:05,390
Next question is I think this is useful studying.

940
01:48:05,400 --> 01:48:10,530
Is it okay? Do you have time to go over because we're at 950 and usually I would stop.

941
01:48:11,720 --> 01:48:16,370
You want to keep going, though? Okay.

942
01:48:18,160 --> 01:48:21,700
I can keep recording it until someone boots me.

943
01:48:22,120 --> 01:48:26,230
But if you have to get up and leave, that's fine, too. So I'm just going to keep going.

944
01:48:26,260 --> 01:48:29,350
If you have to leave, I'll try not to, like, steer and point.

945
01:48:30,910 --> 01:48:34,300
Sorry. Bad jokes are being recorded as we speak.

946
01:48:34,310 --> 01:48:41,470
So the next one. What is the estimated population mean for the large city for count at week eight?

947
01:48:41,950 --> 01:48:47,230
If taking one of the dual therapies that equals zero groups, how do we get at that?

948
01:48:49,380 --> 01:48:52,910
So we have a we have a week to plug in now. We have eight for week.

949
01:48:53,550 --> 01:48:58,230
We're not looking for the general formula and we're plugging in triple equals zero.

950
01:48:58,980 --> 01:49:08,070
So we're going to have The Intercept eight times this week parameter and then nothing else because 000.

951
01:49:08,070 --> 01:49:12,150
So both of these terms kind of vanish. So it's going to look like this.

952
01:49:12,930 --> 01:49:17,430
And so if you were trying to do an estimate statement, you know, you know, you need one intercept,

953
01:49:17,430 --> 01:49:22,140
but you need eight for the weak variable to be plugged into the estimate statement.

954
01:49:24,780 --> 01:49:33,719
All right. Next one. It is the amount of change in log 34 per eight weeks depend on treatment group.

955
01:49:33,720 --> 01:49:37,500
Please provide a relevant p value available from the output above.

956
01:49:38,970 --> 01:49:43,560
So how can I? Is there a p value that helps me answer this question?

957
01:49:44,730 --> 01:49:51,810
Four. Does the amount of change in city four per eight weeks depend on treatment group?

958
01:50:09,080 --> 01:50:19,790
Yeah. So the P value for the week by triple interaction is is getting at this so the change over time involves these week terms.

959
01:50:20,860 --> 01:50:29,769
And if the change over time depends on group the triple, then this interaction would have to be significant.

960
01:50:29,770 --> 01:50:33,130
Otherwise everybody would have the same trajectory over time.

961
01:50:33,940 --> 01:50:38,230
So we're looking for the p value that goes with the weak times triple interaction.

962
01:50:38,470 --> 01:50:41,700
That's the only term that says the change over time.

963
01:50:41,710 --> 01:50:44,860
Eight weeks was kind of a red herring. It didn't matter what number of weeks.

964
01:50:44,860 --> 01:50:50,770
Was there any change over time? What if it depends on treatment group?

965
01:50:50,790 --> 01:50:56,160
It would have to be because this term was significant. So that's the p value that you are.

966
01:50:57,590 --> 01:51:08,260
Pulling out of the output. Good. Oh, and by the way, if these aren't showing up for you, it's because I put little text boxes over the PowerPoint.

967
01:51:08,260 --> 01:51:14,020
So you can just remove the text boxes forevermore and not have to go through the animations to see the answers.

968
01:51:14,020 --> 01:51:17,260
If you want to do that, you can do that live. Just find the text button.

969
01:51:18,320 --> 01:51:21,440
Yank it off. I was just trying to make it interesting for us so you wouldn't see the answers.

970
01:51:21,830 --> 01:51:25,370
All right, next one, consider the following code and output.

971
01:51:25,380 --> 01:51:32,450
So what we've got here is still a weak triple, triple by weak, random intercept and weak.

972
01:51:33,200 --> 01:51:38,780
And I've got it in south and ah so our treat I think this I don't know if I change it or not honestly.

973
01:51:38,990 --> 01:51:44,450
Our treatment differences in baseline CD4 Count six statistically significant.

974
01:51:44,990 --> 01:51:48,800
Please provide a relevant p value if available from the output above.

975
01:51:50,990 --> 01:51:58,040
So what do I mean by baseline? What does it mean when I say baseline study for?

976
01:52:01,330 --> 01:52:05,800
Hmm. The very first measurement. So this is like week equals zero.

977
01:52:07,280 --> 01:52:15,830
And so do we have a P value for whether treatment differences in baseline for cancer a statistically significant.

978
01:52:20,380 --> 01:52:25,060
Yes. So when we can zero these two terms go away.

979
01:52:25,330 --> 01:52:31,720
And the treatment difference at week zero is this p value over here, 0.8191.

980
01:52:32,830 --> 01:52:40,450
Yes. So that's what you it's there's not a significant difference since the triple parameter is not significant.

981
01:52:40,450 --> 01:52:47,570
P equals 0.8. But and this one is is one you haven't seen exactly before.

982
01:52:47,580 --> 01:52:51,170
So this might be a perk up moment to think about how to do this.

983
01:52:51,180 --> 01:52:59,670
So what is the estimated absolute change in laxity for over the 40 weeks of follow up if on triple therapy.

984
01:53:00,940 --> 01:53:05,819
So what's the absolute change if you're in triple therapy? Over 40 weeks.

985
01:53:05,820 --> 01:53:10,560
How much is your outcome changing? How do you get at that from this output?

986
01:53:19,670 --> 01:53:21,590
So I've got change here.

987
01:53:22,490 --> 01:53:30,709
And so kind of when you have something like change in the outcome, you're going to have to estimate the outcome at two times to figure out the change.

988
01:53:30,710 --> 01:53:35,390
Right. So you need to estimate what's the outcome at time zero.

989
01:53:35,720 --> 01:53:40,100
What's the outcome at time 40 weeks. And look at the difference.

990
01:53:41,830 --> 01:53:49,300
And so you're really just plugging in stuff into the model once fourth week equals zero and once with week equal 40.

991
01:53:50,220 --> 01:53:53,670
But assuming they're on the triple therapy group. So what does that look like?

992
01:53:54,790 --> 01:54:00,219
So. I had to fit this all in one slide, so I'm not sure that this is the best way to do it.

993
01:54:00,220 --> 01:54:05,830
But we already figured out what the regression model looked like for triple therapy people.

994
01:54:06,790 --> 01:54:16,630
So this was Slide 48 where we came up with the we kind of simplified terms that this is the equation for the mean log C4 count for the triple therapy.

995
01:54:17,800 --> 01:54:24,400
And so we need to figure out what it is at 40 weeks and subtract off what it is zero weeks.

996
01:54:25,210 --> 01:54:32,140
And when you do that, the intercept cancels because it's in both the 40 week and the zero week estimate.

997
01:54:32,470 --> 01:54:37,690
And so everything's going depend on this parameter and the number of weeks difference.

998
01:54:37,700 --> 01:54:42,520
So it's 40 times this parameter and this is your result.

999
01:54:43,120 --> 01:54:47,709
So that's how you would go about getting it. So this kind of happened in two stages.

1000
01:54:47,710 --> 01:54:57,730
One, you had to write out what the model said the trajectory was for the triple therapy group, and then you had to look at what it said it would be.

1001
01:54:57,730 --> 01:55:02,410
The outcome would be at 40 weeks and subtract off what the outcome would be at zero weeks.

1002
01:55:03,310 --> 01:55:08,020
So there's several moving parts here, but you can get all of them done.

1003
01:55:14,420 --> 01:55:19,140
All right. Is that the last one or do I have another one? Oh, I do have another one.

1004
01:55:19,170 --> 01:55:22,710
Okay. If the random affect for week.

1005
01:55:23,690 --> 01:55:31,580
Is removed here. And here, what does that assume about the individual level?

1006
01:55:31,610 --> 01:55:41,120
I mean, model. So remember, for these models, there's the population model and the individual level mean model.

1007
01:55:41,840 --> 01:55:45,950
So just as a reminder there, if you think about two different.

1008
01:55:47,120 --> 01:55:51,490
Things, how the mind behaves and how individuals behave with respect to the mean.

1009
01:55:52,340 --> 01:55:55,370
So the random statement is about the individual level model.

1010
01:55:55,670 --> 01:56:00,860
So if we remove weak from this, what does that assume about how individuals are behaving?

1011
01:56:03,520 --> 01:56:07,180
They're parallel to the population main. That's correct.

1012
01:56:07,720 --> 01:56:11,440
So if the weakest if the intercept is the only thing in the model,

1013
01:56:11,680 --> 01:56:18,460
you're assuming that all of the trajectories for all the individual level mean models are parallel to the population me model.

1014
01:56:18,820 --> 01:56:22,120
When the random weak is in there, you're letting people do this.

1015
01:56:24,130 --> 01:56:32,980
Okay. And if we did, I think we're coming to the end here.

1016
01:56:32,980 --> 01:56:42,220
If weak is removed because they want to start their class, probably if weakness removed from the random statement that I see increases.

1017
01:56:42,670 --> 01:56:45,850
What does this imply about the individual level versus the population?

1018
01:56:45,850 --> 01:56:49,810
ME models and I'm going to rush to the end because they want to come in.

1019
01:56:50,320 --> 01:57:00,070
And so if we if the AIC is increasing it, it means that we really need to keep weak in the model because we want the lowest AIC model.

1020
01:57:00,460 --> 01:57:02,110
So that's the one with weak in there.

1021
01:57:02,620 --> 01:57:09,429
And so the model improves when assuming individual level mean models over time can have different slopes from the population mean and from each other.

1022
01:57:09,430 --> 01:57:14,910
That's what that is. And I am afraid we are out of time.

1023
01:57:14,920 --> 01:57:24,900
So as you're going through, he's just remember to remove the text boxes and you'll get my answers for these last ones.

1024
01:57:25,170 --> 01:57:26,750
And good luck.

